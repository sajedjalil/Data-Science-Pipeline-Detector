{"cells":[{"metadata":{},"cell_type":"markdown","source":"# The beginning"},{"metadata":{},"cell_type":"markdown","source":"\n\n![Until I find something else better ;)](https://www.walmartbrandcenter.com/content/walmartbrandcenter/home/walmart-brand-center-0/_jcr_content/par/columns/column_2/image.img.png/1578505229420.png)\n\n\nAlright, as requested by https://www.kaggle.com/tarique7, here is a quick implementation of a [**seq2seq**](https://en.wikipedia.org/wiki/Seq2seq) model. \n\nFull disclosure before I start, most of the code here is adapted from the sources listed below. Most of the praise goes https://www.kaggle.com/aquatic to and https://www.kaggle.com/asuilin. That being out of the way, let's start. \n\nThe main sources are the following: \n\n- The rabbit hole starts from this blog post: https://jeddy92.github.io/JEddy92.github.io/ts_seq2seq_conv2/\n- From there, I went straight to the github repo: https://github.com/JEddy92/TimeSeries_Seq2Seq\n- Particularly this notebook: https://github.com/JEddy92/TimeSeries_Seq2Seq/blob/master/notebooks/TS_Seq2Seq_Conv_Full_Exog.ipynb \n(check the other notebooks as well if you want more details) \n\n\n![](https://storage.googleapis.com/kaggle-competitions/kaggle/6768/logos/header.png)\n\n- Then of course to the associated Kaggle competition: https://www.kaggle.com/c/web-traffic-time-series-forecasting/data \n\n- From there, getting to the first place discussion is only a few steps away: https://www.kaggle.com/c/web-traffic-time-series-forecasting/discussion/43795\n- And of course the first place github repo which was the inspiration for the blog post (first link): https://github.com/Arturus/kaggle-web-traffic. \n\nFull circle!\n\nBy the way, I am still reading the first place solution and trying to understand the subtleties. ;)  \n\nSo don't thank me, thank them instead. ;)\n\n\nWhat's next then? \n\nI will try to improve this work over time. Some of the things that need more work:\n\n- Better code: some refactoring, some tests?, some type hinting?\n- Better validation schema: for now it is based on the item_id.\n- Implementing the challenge metric\n- More exogenous features\n- Other feature scaling procedures: have tried [standardization](https://en.wikipedia.org/wiki/Standard_score) but gave very bad results so sticking to removing the mean only for now...\n- Dropping some of the 0s and/or doing some \"clever\" data downsampling\n- Integrating sales hierarchies (the aggregation levels) into the model architecture \n- Add some illustrations? The whole thing feels a little dry for now...\n\n\nIf you want to go further, here are other sources to read and study: \n\n- Wavenet blog post: https://deepmind.com/blog/article/wavenet-generative-model-raw-audio \n- Wavenet keras sample implementation: https://github.com/basveeling/wavenet/ \n- A great keras kernel if you want to get more details about embeddings: https://www.kaggle.com/mayer79/m5-forecast-keras-with-categorical-embeddings-v2 \n\nLet the fun begin!"},{"metadata":{},"cell_type":"markdown","source":"# Some imports (as usual)"},{"metadata":{"trusted":true},"cell_type":"code","source":"import datetime\nfrom datetime import timedelta\nimport matplotlib.pylab as plt\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow.keras.layers import (Activation, Add, Concatenate, Conv1D,\n                                     Dense, Dropout, Input, Lambda, Multiply, Embedding, Flatten, \n                                     concatenate, TimeDistributed, Reshape)\n\nfrom sklearn.preprocessing import StandardScaler\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tqdm import tqdm\nfrom pathlib import Path\nimport gc\n%matplotlib inline\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Some constants "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Some paths\n\n\nBASE_PATH = Path(\"../input/m5-forecasting-accuracy/\")\nDIMENSION_PATH = \"../input/m5-sales-hierarchy-dataset/dimension.parquet\"\nSALES_TRAIN_VALIDATION_PATH = BASE_PATH / \"sales_train_validation.csv\"\nSAMPLE_SUBMISSION_PATH = BASE_PATH / \"sample_submission.csv\"\nSUBMISSION_PATH = \"submission.csv\"\n\n# Some dates\n\nTRAIN_START_DATE = pd.to_datetime(\"2011-01-29\")\nTRAIN_END_DATE = pd.to_datetime(\"2016-04-24\")\n\n# Try more history later? One year more, let's see... Trying less...\n# Due to memory issues, the start date is only 2014...\nDATA_START_DATE = \"2015-01-01\"\nDATA_END_DATE = \"2016-04-24\"\nPRED_STEPS = 28\n\n# Will drop these dates\nCHRISTMAS_DATES = [\"2012-12-25\", \"2013-12-25\", \"2014-12-25\", \"2015-12-25\"]\n\n\n# Change these to be in different \"run\" modes.\n# TODO: Improve the conf values mngt...\nDEBUG = False\nLOAD = False\nSUBMISSION = True \n# TODO: Is this the correct one? Seems to be...\nFIRST_N_SAMPLES = 30940\n\n\n\n# Model HP\n\nBATCH_SIZE = 2**2\nEPOCHS = 5","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data processing functions "},{"metadata":{},"cell_type":"markdown","source":"Not much organized for now but I will work on this next. ;) "},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"\n# Finish implementing this...\nclass DataProcessing:\n\n    \n    pass \n\n\n\ndef get_time_block_series(series_array, date_to_index, start_date, end_date):\n\n    inds = date_to_index[start_date:end_date]\n    return series_array[:, inds]\n\n\ndef transform_series_encode(series_array):\n    # Should there be scale transformation?\n    series_array = np.log1p(np.nan_to_num(series_array)) # filling NaN with 0\n    series_mean = series_array.mean(axis=1).reshape(-1,1)\n    series_std = series_array.std(axis=1).reshape(-1,1)\n    # TODO: What about std? Should the mean be saved? Use these for predicting...\n    # Transform these into a tf.Tensor?\n    # Was 0.01. There seems to be an impact on the error function...\n    series_array = (series_array - series_mean) # To avoid NaN? Changed to smaller.\n    series_array = series_array.reshape((series_array.shape[0],series_array.shape[1], 1))\n    return series_array, series_mean, series_std\n\ndef transform_series_decode(series_array, encode_series_mean, encode_series_std):\n\n    # Should there be scale transformation?\n    series_array = np.log1p(np.nan_to_num(series_array)) # filling NaN with 0\n    series_array = (series_array - encode_series_mean) # To avoid NaN?\n    series_array = series_array.reshape((series_array.shape[0], series_array.shape[1], 1))\n\n    return series_array\n\ndef untransform_series_decode(series_array, encode_series_mean, encode_series_std):\n    series_array = series_array.reshape(series_array.shape[0], series_array.shape[1])\n    series_array = series_array + encode_series_mean\n    # unlog the data, clip the negative part if smaller than 0\n    series_array = np.expm1(series_array)\n    series_array = np.clip(series_array, 0.0, None)\n    return  series_array\n\n\ndef predict_sequences(input_sequences, batch_size):\n    history_sequences = input_sequences.copy()\n    # initialize output (pred_steps time steps)\n    pred_sequences = np.zeros((history_sequences.shape[0], pred_steps, 1))   \n\n    # TODO: pred_steps should be capitalized.\n    for i in tqdm(range(pred_steps), \"Predicting\"):\n\n        # record next time step prediction (last time step of model output)\n        last_step_pred = model.predict(history_sequences, batch_size)[:, -1, 0]\n        # For debug, remove later...\n        pred_sequences[:, i, 0] = last_step_pred\n\n        # add the next time step prediction along with corresponding exogenous features\n        # to the history tensor\n        # TODO: Why from one? So that it can be concatenated... Alright.\n        last_step_exog = input_sequences[:, [(-pred_steps+1)+i], 1:]\n        last_step_tensor = np.concatenate([last_step_pred.reshape((-1,1,1)), \n                                           last_step_exog], axis=-1)\n        history_sequences = np.concatenate([history_sequences, last_step_tensor], axis=1)\n        del last_step_pred\n        del last_step_tensor\n        del last_step_exog\n        gc.collect()\n\n    return pred_sequences\n\n\n\ndef get_data_encode_decode(series_array, exog_array, first_n_samples,\n                           date_to_index, enc_start, enc_end, pred_start=None, pred_end=None, pred=False):\n\n    exog_inds = date_to_index[enc_start:pred_end]\n\n    # sample of series from enc_start to enc_end  \n    encoder_input_data = get_time_block_series(series_array, date_to_index, \n                                               enc_start, enc_end)[:first_n_samples]\n    encoder_input_data, encode_series_mean, encode_series_std = transform_series_encode(encoder_input_data)\n\n\n    if not pred:\n        # sample of series from pred_start to pred_end \n        decoder_target_data = get_time_block_series(series_array, date_to_index, \n                                                    pred_start, pred_end)[:first_n_samples]\n        decoder_target_data = transform_series_decode(decoder_target_data, encode_series_mean, encode_series_std)\n\n        # we append a lagged history of the target series to the input data, \n        # so that we can train with teacher forcing\n        lagged_target_history = decoder_target_data[:,:-1,:1]\n        encoder_input_data = np.concatenate([encoder_input_data, lagged_target_history], axis=1)\n\n    # we add the exogenous features corresponding to day after input series\n    # values to the input data (exog should match day we are predicting)\n    if not pred:\n        # Why does that start 1?\n        exog_input_data = exog_array[:first_n_samples, exog_inds, :][:, 1:,:]\n    else:\n        exog_input_data = exog_array[:first_n_samples, exog_inds, :][:, :,:]\n    # encoder_input_data = np.concatenate([encoder_input_data, exog_input_data], axis=-1)\n\n    if not pred:\n        return encoder_input_data, exog_array, decoder_target_data\n    else:\n        return encoder_input_data, encode_series_mean, encode_series_std\n\ndef predict_and_save(encoder_input_data, sample_ind, batch_size=2**4, enc_tail_len=30 * 6, decoder_target_data=1):\n\n    label = ids[sample_ind]\n    encode_series = encoder_input_data[sample_ind:sample_ind+1,:,:]\n    pred_series = predict_sequences(encode_series, batch_size)\n\n    encode_series = encode_series.reshape(-1,1)\n    pred_series = pred_series.reshape(-1,1)  \n\n    if isinstance(decoder_target_data, np.ndarray):\n        target_series = decoder_target_data[sample_ind, :, :1].reshape(-1, 1)\n        encode_series_tail = np.concatenate([encode_series[-enc_tail_len:], target_series[:1]])\n    else:\n        encode_series_tail = encode_series[-enc_tail_len:]\n\n\n    x_encode = encode_series_tail.shape[0]\n\n    fig = plt.figure(figsize=(10,6))   \n\n    plt.plot(range(1,x_encode+1),encode_series_tail)\n    plt.plot(range(x_encode,x_encode+pred_steps),pred_series,color='teal',linestyle='--')\n\n    plt.title('Encoder Series Tail of Length %d, Target Series, and Predictions' % enc_tail_len)\n    plt.legend(['Encoding Series','Target Series',f'Predictions for {label}'])\n\n    if isinstance(decoder_target_data, np.ndarray):\n        plt.plot(range(x_encode, x_encode + pred_steps), target_series, color='orange')\n        plt.legend(['Encoding Series', 'Target Series', 'Predictions'])\n    else:\n        plt.legend(['Encoding Series', 'Predictions'])\n\n    fig.savefig(f\"predictions_{sample_ind}.png\")\n\n\n    # To clear the figure\n    plt.clf()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Numerical data processing "},{"metadata":{"trusted":true},"cell_type":"code","source":"# TODO: Move these into a function/class and some refactoring...\n\ndf = pd.read_csv(SALES_TRAIN_VALIDATION_PATH)\n\nids = df[\"id\"]\n\n\n\n\npred_length= timedelta(PRED_STEPS)\n\nfirst_day = pd.to_datetime(DATA_START_DATE) \nlast_day = pd.to_datetime(DATA_END_DATE)\n\nval_pred_start = last_day - pred_length + timedelta(1)\nval_pred_end = last_day\n\ntrain_pred_start = val_pred_start - pred_length\ntrain_pred_end = val_pred_start - timedelta(days=1)\n\n\nenc_length = train_pred_start - first_day\n\ntrain_enc_start = first_day\ntrain_enc_end = train_enc_start + enc_length - timedelta(1)\n\nval_enc_start = train_enc_start + pred_length\nval_enc_end = val_enc_start + enc_length - timedelta(1)\n\nprint('Train encoding:', train_enc_start, '-', train_enc_end)\nprint('Train prediction:', train_pred_start, '-', train_pred_end, '\\n')\nprint('Val encoding:', val_enc_start, '-', val_enc_end)\nprint('Val prediction:', val_pred_start, '-', val_pred_end)\n\nprint('\\nEncoding interval:', enc_length.days)\nprint('Prediction interval:', pred_length.days)\n\ncolumns = df.columns\nids = df[\"id\"]\ndate_columns = columns[columns.str.contains(\"d_\")]\ndates_s = pd.date_range(TRAIN_START_DATE, TRAIN_END_DATE, freq=\"1d\")\ndate_to_index = pd.Series(index=dates_s, \n                          data=range(len(date_columns)))\n\nchristmas_index = [date_to_index[d] for d in CHRISTMAS_DATES]\ndate_columns = list(set(date_columns) - set(date_columns[christmas_index]))\nseries_array = df[date_columns].values","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Adding categorical features "},{"metadata":{"trusted":true},"cell_type":"code","source":"# More features \n\n# DOW encoded => try categorical embedding later?\ndow_ohe = pd.get_dummies(dates_s.dayofweek)\n\n\n# TODO: Other later???\ndow_array = np.expand_dims(dow_ohe.values, axis=0) # add sample dimension\ndow_array = np.tile(dow_array, (df.shape[0], 1,1)) # repeat OHE array along sample dimension\n\n\nmonth_ohe = pd.get_dummies(dates_s.month)\nmonth_ohe = np.expand_dims(month_ohe.values, axis=0) # add sample dimension\nmonth_ohe = np.tile(month_ohe, (df.shape[0],1,1)) # repeat OHE array along sample dimension\n\n# Sales hierarchy \ndimension_df = pd.read_parquet(DIMENSION_PATH, columns=[\"location\", \"department\", \"category\"]) # TODO: Try more later...\ndimension_array = pd.get_dummies(dimension_df).values\ndimension_array = np.expand_dims(dimension_array, axis=1) # add timesteps dimension\ndimension_array = np.tile(dimension_array, (1, dow_array.shape[1], 1)) # repeat OHE array along timesteps dimension \n\n\nyear_array = pd.get_dummies(dates_s.year).values\nyear_array = np.expand_dims(year_array, axis=0)  # add timesteps dimension\nyear_array = np.tile(year_array, (df.shape[0],1,1))\n\n# Add the other ones later, for now memory issue...\nexog_array = np.concatenate([dow_array, dimension_array], axis=-1)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dow_array.shape[1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Cleanup\n\ndel df\ndel dimension_df\ndel year_array\ndel dimension_array\ndel month_ohe\ndel dow_array\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"embedding_df = pd.read_parquet(DIMENSION_PATH, columns=[\"location\", \"department\", \"category\"])\nlocation_s = embedding_df[[\"location\"]].values\nlocation_s = np.expand_dims(location_s, axis=1)\nlocation_s = np.tile(location_s, (1, 1913, 1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in [\"location\", \"department\", \"category\"]:\n    print(col, embedding_df[col].nunique())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def embedding_mapping(df):\n    \n    res = []\n    for col in [\"location\", \"department\", \"category\"]:\n        raw_vals = embedding_df[col].unique()\n        val_map = {}\n        for i in range(len(raw_vals)):\n            val_map[raw_vals[i]] = i     \n        mapping = df[col].map(val_map).values\n        res.append(mapping)\n    return res","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"res = embedding_mapping(embedding_df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model and training loop"},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_model(pred_steps=PRED_STEPS):\n    n_filters = 16 # 32 \n    filter_width = 2 # Was 2\n    # Maybe tree less? Was 8,  tried 4, trying 16... Overfitting for sure...\n    # Due to limited memory, will be restrained to 4\n    dilation_rates = [2**i for i in range(4)] * 2 \n    \n    \n    # Numerical input\n    history_seq = Input(shape=(None, 1))\n                        \n                        \n    # Embeddings inputs\n    # dow_input = Input(shape=(None, ), name='dow')\n    location_input = Input(shape=(None, ), name='location')\n    # department_input = Input(shape=(None, ), name='department')\n    # category_input = Input(shape=(None, ), name='category')\n    # Simple heuristic for embedding dimensions: number of unique categories divided by 2.\n\n    # dow_emb = (Embedding(7, 7 // 2)(dow_input))\n    location_emb = (Embedding(3, 3//2)(location_input))    \n    # department_emb = (Embedding(7, 7 // 2)(department_input))                        \n    # category_emb =(Embedding(3, 3 // 2)(category_input))                        \n    \n\n    # x = concatenate([history_seq, dow_emb, location_emb, department_emb, category_emb])\n    x = concatenate([history_seq, location_emb])\n    print(x.shape)\n\n    skips = []\n    for dilation_rate in dilation_rates:\n        \n        # preprocessing - equivalent to time-distributed dense\n        x = Conv1D(32, 5, padding='same', activation='relu')(x) \n        \n        # filter convolution\n        x_f = Conv1D(filters=n_filters,\n                    kernel_size=filter_width, \n                    padding='causal',\n                    dilation_rate=dilation_rate)(x)\n        \n        # gating convolution\n        x_g = Conv1D(filters=n_filters,\n                    kernel_size=filter_width, \n                    padding='causal',\n                    dilation_rate=dilation_rate)(x)\n        \n        # multiply filter and gating branches\n        z = Multiply()([Activation('tanh')(x_f),\n                        Activation('sigmoid')(x_g)])\n        \n        # postprocessing - equivalent to time-distributed dense\n        z = Conv1D(32, 1, padding='same', activation='relu')(z)\n        \n        # residual connection\n        x = Add()([x, z])    \n        \n        # collect skip connections\n        skips.append(z)\n\n    # add all skip connection outputs \n    out = Activation('relu')(Add()(skips))\n\n    # final time-distributed dense layers \n    out = Conv1D(128, 1, padding='same')(out)\n    out = Activation('relu')(out)\n    out = Dropout(.2)(out)\n    out = Conv1D(1, 1, padding='same')(out)\n\n    def slice(x, seq_length):\n        return x[:, -seq_length:, :]\n\n    pred_seq_train = Lambda(slice, arguments={'seq_length': pred_steps})(out)\n\n    model = Model([history_seq, location_input], pred_seq_train)\n    model.compile(Adam(), loss='mse', metrics=[tf.keras.metrics.RootMeanSquaredError()])   \n    return model\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = build_model()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# TODO: Break this into a separate model and train loop\n\ndef train_model(model, batch_size=BATCH_SIZE, epochs=EPOCHS):\n\n\n\n    encoder_input_data, _location_s, decoder_target_data = \\\n        get_data_encode_decode(series_array, location_s, FIRST_N_SAMPLES, date_to_index, \n                               train_enc_start, train_enc_end, train_pred_start, train_pred_end)\n\n    reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n                                                     patience=3, min_lr=0.001)\n\n    checkpoint = tf.keras.callbacks.ModelCheckpoint(\"model.h5\")\n\n    print(encoder_input_data.shape)\n    %pdb\n    history = model.fit([encoder_input_data, _location_s], decoder_target_data,\n                        batch_size=BATCH_SIZE,\n                        epochs=EPOCHS),\n                        \n                        # validation_split=0.2,\n                        # callbacks=[checkpoint, reduce_lr])\n    # Plot the model's architecture\n    tensorflow.keras.utils.plot_model(model, 'model_architecture.png', show_shapes=True) \n    return model, encoder_input_data, decoder_target_data\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if LOAD:\n    model = tf.keras.models.load_model(\"model.h5\")\nelse:\n    model = build_model()\n    model, encoder_input_data, decoder_target_data = train_model(model)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Make predictions"},{"metadata":{"trusted":true},"cell_type":"code","source":"if not submission:\n\n    if debug:\n        predict_and_save(encoder_input_data, 2486, decoder_target_data=decoder_target_data, batch_size=1)\n\n    else:\n        # TODO: Plot more?\n        for i in range(0, first_n_samples, 1000):\n            predict_and_save(encoder_input_data, i, decoder_target_data=decoder_target_data, batch_size=1)\n\nif submission:\n    # Two years was too much for the RAM...\n    cmp_enc_start = TRAIN_END_DATE - timedelta(days=28 * 3)\n\n    cmp_enc_end = TRAIN_END_DATE\n\n\n    encoder_input_data, encode_series_mean, encode_series_std = get_data_encode_decode(\n                            series_array, exog_array, first_n_samples, date_to_index, \n                            cmp_enc_start, cmp_enc_end, pred=True)\n\n    # TODO: Change how many batches are predicted once this works...\n    pred_series = predict_sequences(encoder_input_data, batch_size=2**4)\n\n    # visualize one sample to check the prediction\n    predict_and_save(encoder_input_data, 100)\n\n\n    # reverse the transformation\n    pred_series_transformed = untransform_series_decode(pred_series, encode_series_mean, encode_series_std)\n\n    # check the time frame\n    print('encode_input_first_day:', cmp_enc_start.date())\n    print('encode_input_last_day:', cmp_enc_end.date())\n\n    columns = [f\"F{id}\" for id in range(1, 29)]\n    sumbmission_df = pd.DataFrame(pred_series_transformed, columns=columns)\n    sumbmission_df[\"id\"] = ids\n    del pred_series_transformed\n    gc.collect()\n\n    # Read samples_df and make the submission_df final DataFrame.\n\n    samples_df = pd.read_csv(SAMPLE_SUBMISSION_PATH)\n    sumbmission_df = pd.concat([samples_df.loc[lambda df: df[\"id\"].str.contains(\"eval\")], sumbmission_df], sort=False)\n    del samples_df\n    gc.collect()\n    # Removed compression so that I can make a submission from the kernel\n    sumbmission_df.to_csv(SUBMISSION_PATH, index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sumbmission_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# The end"},{"metadata":{},"cell_type":"markdown","source":"If you made it so far, congratulation. I hope this was useful. \n\nThat's it for now! \n\nAs mentionned in the introduction, improvements will be made over time, so check this notebook often.\n\nAnd if you see an obvious bug and/or a way to improve, please share in the comments. :)"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}