{"cells":[{"metadata":{},"cell_type":"markdown","source":"# M5 Forecasting Encoder Decoder Model with Attention using Pytorch\n* ## Teaching an AI to Forecast\n* ## It has achieved a score of 0.603 in just 16 epochs( It has figured out exponential smoothening i guess)\n* ## It took 9 hours to complete 16 epochs though XD ( Slow learner)\n* ## It is performing well but is so scared of overfitting XD\n* ## Help my network only 16 days left!!\n* ## Run the last cell to visualize the predictions of the network and give your input\n\nUpcoming Developments stay tuned :\n1. Implementing multiple layer bidirectional LSTM\n2. Implementing custom loss based on negative binomial distributions ( Lets see if we can get uncertainity)\n3. Visualizing attention mechanism ( going to be difficult)\n4. Trying dropout layers\n5. Refining the input features\n6. Any of your suggestions ( Top priority!!! :D)\n\nDisclaimer : I am a simple guy trying complex things forgive me if I get some of the technicalities wrong.\nThis is my first kaggle notebook :). And I am a mechanical engineer with and MBA in analytics tryin to get a grip on coding","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# **A Gentle Introduction**\nTime Series forecasting has always been a challenge it has evolved from simple statistical techniques like exponential smoothening, ARIMA , SARIMA, SARIMAX to using machine learning regression models based on algorithms like Xgboost , LGBM , linear regression, RandomForest etc.\n\nThe earlier statistical techniques like exponential smoothening,ARIMA and SARIMA had a limitation as it was designed on a single feature that is the time series itself as the input. To put it simply we could not feed it explicit features like the weather , holidays, or for example in our M5 forecasting dataset the selling price of the item. This limitation was overcome by machine learning models which could also include these explicit features to train the model.\nBut as the features started increasing the Machine learning models started overfitting also they faced a problem of autocorrelation causing them to act funnily in some cases.\n\nNeural networks trie to address these limitations of machine learning solutions but at the cost of the difficulty of training such networks.\n\nIn the below solution we adopted a novel approach by using an encoder decoder model to solve the M5 Forecasting problem\nThe solution below assumes that in the rawest form times series is just a pattern and when we do forecasting we look at the input pattern which is a sequence of numbers and accordingly predict the future pattern/sequence.\nWhile making the predictions we also use additional inputs such as date features ( is it a holiday? which day is  it? which month is it) we also keep in mind the amount to variation in the time series ( is it fluctuating a lot? is there seasonality? is it stagnant?) and we also ask questions about the underlying item ( Which store is it kept? what type of item is it etc)\n\n**Static Features** : If you give it a thought properties of the underlying item like the store in which its kept , the type of the item, are static features that dont change with each timestep.( we will call them static inputs)\n\n**Dynamic features** :The other inputs like selling price and date features change with each time step. ( We will call them dynamic inputs)\n\nThe ability of a encoder decoder model to capture both these static and dynamic features and treat them accordingly gives it the edge over machine learning solutions","execution_count":null},{"metadata":{},"cell_type":"markdown","source":" ## The Encoder Decoder Design\n*People not familiar with it can think of it as models that take in a sequence/pattern and based on it generates a sequence/pattern and hence they are also know as seq2seq models*\n\nIn our approach we will use a simple neural network to capture these static features and feed them in the hidden state of the encoder. The encoder will then go across the timeseries (1913 time steps) taking the demand concatenated with the selling price and date features as the input. Finally the decoder will make predictions step by step the input will be the ouput from previous state ( or ground truth in case of teacher enforcing) , concatenated with the date features and sale prices. We will also concat attention to the hidden state before we make the output predictions ( 28 time steps).","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# The Code\n\nComponents:\n* The Model Class : M5_Encoder_Decoder\n* Transformer Class : MinMaxTransformer\n* Dataloader Class : M5dataloader ( Given a batch of ids loads the required tensors for the ANN , encoder and decoder respectively)\n(*Yup it's a batched implementation!! Vectorization for the win!! I also wanted to include padding but couldnt due to Pytorch not supporting forward padding.\nI could manually do it though. Lets see*)\n* Batch trainer function : train_batch\n* Training helper function : train_setup\n* Plotting function : get_plots\n* Prediction function : Helps in postprocessing of decoder outputs to a usable dataframe format\n* Validation score fucntion\n\nI'll add more details but thats a brief on the content of the code\nPeople interested can through the code I would also be happy to provide any clarifications\n\n*Lets jump down to the interesting part for now!!!*\n**Go to model training those who dont want to see copious amounts of code!!! XD**\n\n\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom IPython.display import clear_output\nimport numpy as np\nimport matplotlib.pyplot as plt\ndevice= torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nimport random","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/m5-forecasting-accuracy/sales_train_evaluation.csv')\ndate_features = pd.read_csv('/kaggle/input/m5features/date_features.csv')\nsales_prices = pd.read_csv('/kaggle/input/m5-forecasting-accuracy/sell_prices.csv')\nann_features = pd.read_csv('/kaggle/input/m5features/ann_features.csv')\ndf['id'] = df['id'].str[:-11]\nsales_prices['sell_price'] = MinMaxScaler().fit_transform(sales_prices['sell_price'].to_numpy().reshape(-1,1))\nsales_prices['id'] = sales_prices['item_id'] + \"_\"  + sales_prices['store_id'] \nsales_prices.drop(['item_id','store_id'], axis =1, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# A look at the Neural Network(ANN) features and date features","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"date_features.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ann_features.head()\n#std_x , mean_x ....skew_x are at the item level\n#std_y , mean_y and skew_y are at the category level we can add more such encodings if needed at the store level and state level\n#Help : Suggest me more aggregate properties of the time series","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Transformer :\nThe purpose of the below transformer is to scale **each** of the time series based on the max and min values from d_1 to d_1913 we also later use this transformer to reverse transform the output values. We could have used a minmaxtransformer from sklearn but then I would have to individually apply it on each of the time series and also store the objects in a list fo future use.\n\nNote : Though a min max transformer scales from 0-1 there can be greater than 1 values between dates d_1914 and d_1941 as the demand may increase with time.\nThis is important to note as we cannot apply sigmoidal function to the output.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"class MinMaxtransformer():\n    ''' A class to scale the time series data for each item_id'''\n    def __init__(self,d_x,d_y, info = None):\n        self.d_x = d_x\n        self.d_y = d_y\n        if info is None :\n            self.info = pd.DataFrame({'id': [],'min':[],'max':[]})\n        else :\n            self.info = info\n    \n    def fit(self, df):\n        '''Will store in min and max values of the rows in a info dataframe'''\n        self.info['id'] = df['id']\n        self.info['max']= df.loc[:,self.d_x:self.d_y].max(axis=1)\n        self.info['min']= df.loc[:,self.d_x:self.d_y].min(axis=1)\n        self.info['maxdiffmin'] = self.info['max'] - self.info['min']\n    \n    def transform(self , df, d_x = None ,d_y = None):\n        if d_x == None or d_y == None :\n            d_x = self.d_x\n            d_y = self.d_y\n        df = pd.merge(df,self.info, on ='id', how ='left')\n        for col in df.loc[:,d_x:d_y].columns:\n            df[col] = (df[col] - df['min'])/(df['maxdiffmin'])\n        df.drop(['min','max', 'maxdiffmin'],axis =1, inplace = True)\n        return df\n    \n    def reverse_transform(self, df, d_x =None,d_y = None, round_ = False):\n        df = pd.merge(df,self.info, on ='id', how ='left')\n        if d_x == None or d_y == None :\n            d_x = self.d_x\n            d_y = self.d_y\n        for col in df.loc[:,d_x:d_y].columns:\n            df[col] = df[col] * df['maxdiffmin'] + df['min']\n            if round_ :\n                df[col] = round(df[col])\n        df.drop(['min','max', 'maxdiffmin'],axis =1, inplace = True)\n        return df\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Fitting the transformer we will use it in the data loader**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"mmt  = MinMaxtransformer('d_1','d_1913')\nmmt.fit(df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Train Validation Split\n\nIts important to understand for our model our model is predicting an output sequence given an input sequence hence the below split","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\ntrainids, testids = train_test_split(df.loc[:,['id']], train_size = 0.8, random_state = 1234)\ntrainids = trainids['id'].to_list()\ntestids = testids['id'].to_list()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(trainids)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(testids)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# The Dataloader :\nI think showing the working will be better than explaining.\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"class M5dataloader():\n    def __init__(self, ids, batch_size):\n        '''IDs to be passed in list format'''\n        self.ids = ids\n        self.iteration = 0\n        self.batch_size = batch_size\n        \n        \n    def get_data(self, df,date_features,sales_prices, ann_features):\n        start = (self.iteration * self.batch_size)% len(self.ids)\n        end = start + self.batch_size\n        self.iteration +=1\n        if( end < len(self.ids)):\n            batchidlist = self.ids[start:end]\n        else :\n            end = end%len(self.ids)\n            batchidlist = [id for id in self.ids[start:]] + [ id for id in self.ids[:end]]\n        filt = df['id'].isin(batchidlist)\n        batch_data = df.loc[filt,:].drop(['item_id','dept_id','cat_id', 'store_id','state_id'], axis = 1 )\n        batch_data =  mmt.transform(batch_data,'d_1', 'd_1941')\n        encoder_data = batch_data.loc[:,'id':'d_1913']\n        decoder_data = batch_data.loc[:,'d_1914':'d_1941']\n        decoder_data['id'] = encoder_data['id']\n        decoder_data = pd.concat([batch_data.loc[:,['id']], batch_data.loc[:,'d_1914':'d_1941']], axis=1 )\n        encoder_data = encoder_data.melt(id_vars =['id'], value_vars = encoder_data.columns.to_list()[1:],var_name ='d', value_name ='count')\n        decoder_data = decoder_data.melt(id_vars =['id'], value_vars = decoder_data.columns.to_list()[1:],var_name ='d', value_name ='count')\n        encoder_data = pd.merge(encoder_data,date_features,how = 'left',on ='d').drop(['Unnamed: 0'], axis =1)\n        decoder_data = pd.merge(decoder_data, date_features, how = 'left' , on = 'd').drop(['Unnamed: 0'], axis =1)\n        encoder_data = pd.merge(encoder_data,sales_prices,how = 'left',on =['id','wm_yr_wk']).drop(['date','wm_yr_wk'], axis =1)\n        decoder_data = pd.merge(decoder_data,sales_prices,how = 'left',on =['id','wm_yr_wk']).drop(['date','wm_yr_wk'], axis =1)\n        encoder_data['id'] = encoder_data['id'] + encoder_data['d']\n        decoder_data['id'] = decoder_data['id'] + decoder_data['d']\n        encoder_data.set_index('id', inplace = True)\n        decoder_data.set_index('id', inplace = True)\n        encoder_data.drop('d', axis = 1, inplace = True)\n        ground_truth = decoder_data.loc[:,['count']]\n        decoder_data.drop(['d', 'count'], axis = 1, inplace = True)\n        encoder_data = encoder_data.fillna(value = 0)\n        encoder_data = torch.tensor(encoder_data.to_numpy().reshape(int(encoder_data.shape[0]/self.batch_size),self.batch_size,encoder_data.shape[1]),dtype=torch.float32)\n        decoder_data = torch.tensor(decoder_data.to_numpy().reshape(int(decoder_data.shape[0]/self.batch_size),self.batch_size,decoder_data.shape[1]),dtype=torch.float32)\n        ground_truth = torch.tensor(ground_truth.to_numpy().reshape(int(ground_truth.shape[0]/self.batch_size),self.batch_size,ground_truth.shape[1]),dtype=torch.float32)\n        filt2 = ann_features['id'].isin(batchidlist)\n        ann_data = ann_features.loc[filt2,:]\n        ann_data.set_index('id', inplace = True)\n        ann_data = torch.tensor(ann_data.to_numpy().reshape(self.batch_size, ann_data.shape[1]),dtype=torch.float32)\n        return (encoder_data, decoder_data,ground_truth,ann_data)\n       \n    \n        #return decoder_input, encoder_input , ANN_input , ground_truth","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Given a set of ids it will load the tensors needed for the neural network training or evaluation\nEverytime the get_data function is called it loads a different batch from the givenids till it goes through the entire dataset","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"dataloader = M5dataloader(trainids,10) # initialize it","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"encoder_data, decoder_data, ground_truth,ann_data = dataloader.get_data(df,date_features,sales_prices,ann_features) \n# getting input dimensions of encoder , decoder and ann and testing the data loader\nann_input_size = ann_data.shape[1] \nenc_input_size= encoder_data.shape[2]\ndec_input_size= decoder_data.shape[2]+1 # we add one to include output from previous state as input","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Lets look at the shapes of the tensor\nprint('\\n ANN tensor shape :', ann_data.shape)\nprint('\\n encoder tensor shape :', encoder_data.shape)\nprint('\\n decoder tensor shape :', decoder_data.shape)\nprint('\\n ground_truth tensor shape :', ground_truth.shape)\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **The Model**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"class M5_EncoderDecoder(nn.Module):\n    \n    def __init__(self, ann_input_size,enc_input_size, dec_input_size, hidden_size, output_size = 1, verbose=False):\n        super(M5_EncoderDecoder, self).__init__()\n        self.ann_input_size = ann_input_size\n        self.enc_input_size = enc_input_size\n        self.dec_input_size = dec_input_size + hidden_size\n        self.hidden_size = hidden_size\n        self.output_size = output_size\n        \n        self.s2h = nn.Sequential(nn.Linear(ann_input_size,96), \n                                 nn.ReLU(),\n                                 nn.Linear(96,hidden_size))\n        \n        self.encoder_rnn_cell = nn.GRU(enc_input_size, hidden_size)\n        self.decoder_rnn_cell = nn.GRU(dec_input_size+hidden_size, hidden_size)\n        \n        self.h2o = nn.Linear(hidden_size, output_size)\n        self.verbose = verbose\n        self.U = nn.Linear(self.hidden_size, self.hidden_size)\n        self.W = nn.Linear(self.hidden_size, self.hidden_size)\n        self.V = nn.Linear(self.hidden_size,1)\n    \n        \n    def forward(self, ann_data, encoder_data, decoder_data, ground_truth = None, steps = 28 , device = device):\n        \n        # encoder\n        ann_data =ann_data.to(device)\n        encoder_data =encoder_data.to(device)\n        decoder_data = decoder_data.to(device)\n        if ground_truth is not None:\n            ground_truth = ground_truth.to(device)\n        batch_size = encoder_data.shape[1]\n        hidden = self.s2h(ann_data.float())\n        encoder_outputs,hidden = self.encoder_rnn_cell(encoder_data.float(),hidden.view(1,batch_size,self.hidden_size).float())\n        \n        U = self.U(encoder_outputs)\n        initial_ground = encoder_data[encoder_data.shape[0]-1,:,0].view(1,encoder_data.shape[1],1).to(device)\n        # getting the last known decoder count from encoder data (in most cases it is initialized randomly but in this case we know what the firs output to the decoder should be)\n        flag = 1 # some stupid coding XD \n        if ground_truth is None:\n            ground_truth = initial_ground\n            flag = 0\n        else :\n            ground_truth = torch.cat((initial_ground,ground_truth),0)\n        \n        outputs = []\n        for i in range(steps) :\n            W  = net.W(hidden.repeat(encoder_data.shape[0],1,1))\n            V= net.V(torch.tanh(U+W))\n            alpha = F.softmax(V, dim=0)\n            attn_applied = torch.bmm(alpha.T.transpose(0,1),encoder_outputs.transpose(0,1))\n            if (i == 0):\n                decoder_input = torch.cat((ground_truth[i,:,:].float(),decoder_data[i,:,:].float(),attn_applied.transpose(0,1)[0,:,:].float()),1)\n            \n            if(i > 0):\n                if flag != 0 :\n                    decoder_input = torch.cat((ground_truth[i,:,:].float(),decoder_data[i,:,:].float(),attn_applied.transpose(0,1)[0,:,:].float()),1)\n                    \n                else:\n                    decoder_input = torch.cat((out.float(),decoder_data[i,:,:].float(),attn_applied.transpose(0,1)[0,:,:].float()),1) \n                    # no need to use i-1 as we have added a timestep inthe form of initial ground\n            \n            _ ,hidden = self.decoder_rnn_cell(decoder_input.view(1,decoder_input.shape[0],decoder_input.shape[1]).float(), hidden)\n            out = self.h2o(hidden) \n            out = out.view(out.shape[1],1)\n            outputs.append(out) # verify dimensions\n        return outputs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_batch(net, batch,batch_size,opt,criterion,device, teacher_force = False):\n    net.train().to(device)\n    opt.zero_grad()\n    encoder_data, decoder_data, ground_truth,ann_data = batch\n    ground_truth = ground_truth.to(device)\n    if teacher_force :\n        outputs = net.forward(ann_data, encoder_data, decoder_data, ground_truth)\n    else :\n        outputs = net.forward(ann_data,encoder_data,decoder_data)\n    \n    loss = torch.zeros(1,1).float().to(device)\n    for i, output in enumerate(outputs):\n        loss += criterion(outputs[i], ground_truth[i,:,:]).float()\n    loss.backward()\n    opt.step()\n    \n    return loss/batch_size","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def prediction(testids, net, df, sales_prices, date_features, ann_features,round_ = False, steps = 28,idstart = 1914):\n    testloader = M5dataloader(testids,len(testids))\n    encoder_data, decoder_data, ground_truth,ann_data = testloader.get_data(df,date_features,sales_prices,ann_features)\n    outputs = net.forward(ann_data,encoder_data,decoder_data)\n    pred= pd.DataFrame({'id' : testids})\n    for i, output in enumerate(outputs):\n        pred['d_' + str(idstart + i)] = output.cpu().data.numpy()\n    \n    start = 'd_' + str(idstart)\n    end = 'd_' + str(idstart + 27) \n    pred = mmt.reverse_transform(pred,start,end, round_ = round_)\n    pred.set_index('id', inplace = True)\n    return pred","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def actual_values(testids,df,steps = 28, idstart = 1914):\n    df.set_index('id',inplace = True)\n    start = 'd_' + str(idstart)\n    end = 'd_' + str(idstart + steps -1) \n    act = df.loc[testids,start:end]\n    df.reset_index(inplace = True)\n    return act","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def validation_error(pred,act):\n    return np.square(pred.to_numpy() - act.to_numpy()).sum()/act.to_numpy().size","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_plots(ids, net):\n    if len(ids) > 25:\n        return \"the number of ids in the list exceeds the limit of 25\"\n    \n    trainhead = actual_values(ids, df,steps = 100 , idstart = 1814).T\n    testvalues = actual_values(ids, df,steps = 28 , idstart = 1914).T\n    predictions = prediction(ids, net, df, sales_prices, date_features, ann_features, round_ = False).T\n    b = ['_encoder','_actual', '_pred']\n    for i , x in enumerate([trainhead, testvalues, predictions]):\n        x.columns = [x for x in map(lambda x: x + b[i],x.columns.to_list())]\n        x.reset_index(inplace = True)\n        x.rename(columns={'index' : 'Days'}, inplace = True)\n        x['Days'] = x['Days'].str[2:].apply(int)\n        \n    fig, ax = plt.subplots(nrows = len(ids), ncols = 1,figsize=(25,4 * len(ids)))\n    for i in range(len(ids)):\n        if len(ids) == 1:\n            trainhead.plot(x = 'Days',y=[i+1],ax=ax);\n            testvalues.plot(x = 'Days',y=[i+1],ax=ax);\n            predictions.plot(x = 'Days',y=[i+1],ax=ax);\n        else :\n            trainhead.plot(x = 'Days',y=[i+1],ax=ax[i]);\n            testvalues.plot(x = 'Days',y=[i+1],ax=ax[i]);\n            predictions.plot(x = 'Days',y=[i+1],ax=ax[i]);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_submission(idlist,net, batch = 200):\n    \n    submission = []\n    for i in range(len(idlist)//batch + int(len(idlist)%batch !=0)):\n        print(\"Iteration \", i, \"/\", len(idlist)//batch + int(len(idlist)%batch !=0))\n        start = i * batch\n        end = start + batch\n        if(end > len(idlist)): \n            end = len(idlist)\n        batchidlist = idlist[start:end] \n        pred = prediction(batchidlist, net, df, sales_prices, date_features, ann_features, round_ = False)\n        submission.append(pred) \n    return pd.concat(submission, axis =0)","execution_count":null,"outputs":[]},{"metadata":{"id":"Rjto129ssrpr","trusted":true},"cell_type":"code","source":"def train_setup(net,trainids,testids,validation = False,plots = False, lr = 0.01, n_batches = 1000, batch_size = 200, momentum = 0.9, display_freq=5, device = device,test_batch_size = 100):\n    \n    net = net.to(device)\n    criterion = nn.MSELoss()\n    opt = optim.Adam(net.parameters(), lr=lr)\n    teacher_force_upto = n_batches//3\n    trainloader = M5dataloader(trainids,batch_size)\n    loss_arr = np.zeros(n_batches + 1)\n    if validation :\n        valid_error = []\n        valid_xaxis =[]\n    for i in range(n_batches):\n        batch = trainloader.get_data(df, date_features, sales_prices, ann_features) \n        loss_arr[i+1] = (loss_arr[i]*i + train_batch(net,batch,batch_size, opt, criterion, device = device, teacher_force = False ))/(i + 1)\n        \n        if i%display_freq == display_freq-1:\n            if plots :\n                clear_output(wait=True)\n            \n            if validation :\n                ids = random.sample(testids,test_batch_size)\n                pred = prediction(ids, net, df, sales_prices, date_features, ann_features, round_ = False)\n                act = actual_values(ids, df,steps = 28 , idstart = 1914)\n                valid_xaxis.append(i)\n                v = validation_error(pred,act)\n                valid_error.append(v)\n                print('Validation error  ', v, \" per observation as tested on \", test_batch_size, \" random samples from test set\")\n            print('Epoch ',(i*batch_size)/len(trainids),' Iteration', i, 'Loss ', loss_arr[i])\n            \n            if plots:\n                fig, axes = plt.subplots(nrows =1, ncols=2 , figsize=(20,6))\n                axes[0].set_title('Train Error')\n                axes[0].plot(loss_arr[2:i], '-*')\n                axes[0].set_xlabel('Iteration')\n                axes[0].set_ylabel('Loss')\n\n                if validation :\n                    axes[1].set_title('Validation error')\n                    axes[1].plot(valid_xaxis, valid_error,'-*')\n                    axes[1].set_xlabel('Iteration')\n                    axes[1].set_ylabel('validation error')\n                plt.show()\n                print('\\n\\n')\n        if(i%100 ==0):    \n            torch.save(net.state_dict(), 'model.pt')\n        if(i%500 ==0):\n            filename = str(i) + 'model.pt'\n            torch.save(net.state_dict(), filename)\n    filename = str(i) + 'model_4K_iter.pt'\n    torch.save(net.state_dict(), filename)    \n    return (loss_arr,net)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model Training :","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Training cell\n\n#net = M5_EncoderDecoder(ann_input_size = ann_data.shape[1] ,enc_input_size= encoder_data.shape[2], dec_input_size= decoder_data.shape[2]+1, hidden_size=126)\n#losses,net = train_setup(net,trainids,testids, lr = 0.01)\n\n\n# I have already trained the model batch_size = 200 , iterations = 4000, learning_rate = 0.01 , teacher enforced upto 2k iterations\n# the train_setup function supports plots to monitor loss and also validation feel free to play with it","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We will load my pretrained model\nnet = M5_EncoderDecoder(ann_input_size = ann_data.shape[1] ,enc_input_size= encoder_data.shape[2], dec_input_size= decoder_data.shape[2]+1, hidden_size=126)\nnet.load_state_dict(torch.load('../input/m5models-2/model_4K_iter.pt', map_location = device))\nnet.eval()\nnet.to(device)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model Performance on Test Data\nLets look at 20 random plots of the 4000 iteration model\nThe plot is of 128 days\nThis is where I need your help !!\nPlease suggest techniques to improve the performance :\n* Should i decrease encoder timesteps ? Is 1913 steps huge?\n* Should I replace GRU cell with LSTM cell?\n* Should I add more layers to the LSTM cell?\n* Should I decrease size of input features?\n\nKaggle Army help me in this time of peril so that we can revolutionize forecasting !!!!!\n\n15 Days to go...!!!!\n\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"get_plots(random.sample(trainids,20),net)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"idlist = df['id'].to_list()\nsubmission_validation = get_submission(idlist,net)\nsubmission_validation.to_csv(\"submission.csv\")\n#cant submit this submission file coz its not in submission format I appended evaluation with 0's to it and submitted to get an WRMSE of 0.603","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}