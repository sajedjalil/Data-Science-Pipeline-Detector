{"cells":[{"metadata":{},"cell_type":"markdown","source":"#  **REDUCING MEMORY USAGE OF DATASETS BY MORE THAN 60%**"},{"metadata":{},"cell_type":"markdown","source":"*In this example we will be using the M5 Forecasting Accuracy dataset by walmart which consists of a collection of datasets which make up to nearly 1.5GB in total.*"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"price = pd.read_csv('/kaggle/input/m5-forecasting-accuracy/sell_prices.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"price.info(memory_usage = 'deep')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"But if you look close enough most of the column has a dtype \"Object\" type so lets explore the memory usage of different datatypes before we jump into the solution "},{"metadata":{"trusted":true},"cell_type":"code","source":"price.memory_usage(deep = True) * 1e-6    #for MB representation","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"price.head(2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This data clearly shows that the columns \"store_id\" and \"item_id\" occupy the most amount of memory. Looking into these columns we can clearly see that these are categorical data and hence converting them to their relevent dtype will efficiently reduce the memory usage."},{"metadata":{"trusted":true},"cell_type":"code","source":"price[['store_id','item_id']] = price[['store_id','item_id']].astype('category')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"price.memory_usage(deep = True) * 1e-6   #for MB representation","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Clearly we can see that converting the dtype \"Object\" to \"Categorical\" has reduced the memory size by a whopping 60 times. Similarly converting the dtypes from from higher bit representation to their lower bit counterparts will hugely leverage our goal to decrease the memory usage of the dataset by more that 60%. To achieve this I will be using a function that I learnt from https://guillaume-martin.github.io/ which efficiently changes the dtypes of a whole dataframe"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\ndef reduce_mem_usage(props):\n    start_mem_usg = props.memory_usage().sum() / 1024**2 \n    print(\"Memory usage of properties dataframe is :\",start_mem_usg,\" MB\")\n    NAlist = [] # Keeps track of columns that have missing values filled in. \n    for col in props.columns:\n        if props[col].dtype != object:  # Exclude strings\n            \n            # Print current column type\n            print(\"******************************\")\n            print(\"Column: \",col)\n            print(\"dtype before: \",props[col].dtype)\n            \n            # make variables for Int, max and min\n            IsInt = False\n            mx = props[col].max()\n            mn = props[col].min()\n            \n            # Integer does not support NA, therefore, NA needs to be filled\n            if not np.isfinite(props[col]).all(): \n                NAlist.append(col)\n                props[col].fillna(mn-1,inplace=True)  \n                   \n            # test if column can be converted to an integer\n            asint = props[col].fillna(0).astype(np.int64)\n            result = (props[col] - asint)\n            result = result.sum()\n            if result > -0.01 and result < 0.01:\n                IsInt = True\n\n            \n            # Make Integer/unsigned Integer datatypes\n            if IsInt:\n                if mn >= 0:\n                    if mx < 255:\n                        props[col] = props[col].astype(np.uint8)\n                    elif mx < 65535:\n                        props[col] = props[col].astype(np.uint16)\n                    elif mx < 4294967295:\n                        props[col] = props[col].astype(np.uint32)\n                    else:\n                        props[col] = props[col].astype(np.uint64)\n                else:\n                    if mn > np.iinfo(np.int8).min and mx < np.iinfo(np.int8).max:\n                        props[col] = props[col].astype(np.int8)\n                    elif mn > np.iinfo(np.int16).min and mx < np.iinfo(np.int16).max:\n                        props[col] = props[col].astype(np.int16)\n                    elif mn > np.iinfo(np.int32).min and mx < np.iinfo(np.int32).max:\n                        props[col] = props[col].astype(np.int32)\n                    elif mn > np.iinfo(np.int64).min and mx < np.iinfo(np.int64).max:\n                        props[col] = props[col].astype(np.int64)    \n            \n            # Make float datatypes 32 bit\n            else:\n                props[col] = props[col].astype(np.float32)\n            \n            # Print new column type\n            print(\"dtype after: \",props[col].dtype)\n            print(\"******************************\")\n    \n    # Print final result\n    print(\"___MEMORY USAGE AFTER COMPLETION:___\")\n    mem_usg = props.memory_usage().sum() / 1024**2 \n    print(\"Memory usage is: \",mem_usg,\" MB\")\n    print(\"This is \",100*mem_usg/start_mem_usg,\"% of the initial size\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"prices = pd.read_csv('/kaggle/input/m5-forecasting-accuracy/sell_prices.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"prices.info(memory_usage = 'deep')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"reduce_mem_usage(prices)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"prices.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"prices[['store_id','item_id']] = prices[['store_id','item_id']].astype('category')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"prices.info(memory_usage = 'deep')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that we managed to reduce the memory usage of \"sell_prices.csv\" from 957.5MB to 59MB with all its data unchanged and untouched"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}