{"cells":[{"metadata":{"_cell_guid":"d5ca927a-7442-4173-87fc-aa954a58c221","_uuid":"b315f4821bd700bf869a48f0838c15e18a89ea20","id":"axI4u1EkzGmT"},"cell_type":"markdown","source":"# Convolutional Neural Networks with Data Augmentation using Keras\n\n---\n\n\n## In this tutorial I will be using Keras with TensorFlow as backend to calssify digits from the MNIST Dataset","execution_count":null},{"metadata":{"_cell_guid":"e6de9fd1-9a50-4e58-a489-e69928b3c4b5","_uuid":"96e2d0d769ff90a4d74a218c8fe61317518960be","id":"3icmSjgt1VLP"},"cell_type":"markdown","source":"We will start by importing Keras.","execution_count":null},{"metadata":{"_cell_guid":"eedc5d0b-2754-4615-b49a-b656afad0675","_uuid":"77521748ee2a2edb717ad68b9f96bc5b841a5ebc","id":"96_Dh7GYzFwv","trusted":true},"cell_type":"code","source":"import numpy\nfrom keras import backend as K\nfrom keras.datasets import mnist\nfrom keras.utils import np_utils","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"243ce545-305f-4a62-bb26-ab783972c70e","_uuid":"804355b0c5d871f360a84b0ab704226ee7664214","id":"E5tzF9kr10GX"},"cell_type":"markdown","source":"We then import the layers of the convolutional neural network.The network consists of two main components :\n\n1. Convolutional layers : the convolutional layer is responsible for the convolutional operation in which feature maps identifies features in the images.\nand is usually followed by two types of layers which are :\n>*   **Dropout** : Dropout is a regulization technique where you turn off part of the network's layers randomally to increase regulization and hense decrease overfitting. We use when the training set accuracy is muuch higher than the test set accuracy.\n>*   **Max Pooling** : The maximum output in a rectangular neighbourhood. It is used to make the network more flexible to slight changes and decrease the network computationl expenses by extracting the group of pixels that are highly contributing to each feature in the feature maps in the layer.\n2. Dense layers : The dense layer is a fully connected layer that comes after the convolutional layers and they give us the output vector of the Network.\n\nAs a convention in Convolutional Neural Network we decrease the dimensions of the layers as we go deeper and increase the number of feature maps to make it detect more features and decrease the number of computational cost.\n\n![alt text](https://raw.githubusercontent.com/MoghazyCoder/Machine-Learning-Tutorials/master/assets/Untitled.png)\n\n ","execution_count":null},{"metadata":{"_cell_guid":"c615e843-f880-409a-9701-6ed334496e06","_uuid":"a5e95affe023678faaa7e2c84797bb43fafa3fce","id":"EfQa_UJZ5wsR","trusted":true},"cell_type":"code","source":"from keras.layers import Dense, Dropout,Flatten\nfrom keras.layers.convolutional import Conv2D, MaxPooling2D","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"76d4c502-5b22-41c9-907d-9b96a45e31d7","_uuid":"2fe3233cbe5b534701aa8d50fb5fe67c6691b7d8","id":"nlAMFOZO9feO"},"cell_type":"markdown","source":"Sequential layers are stacked such that every layer passes its output to the next layer without you specifying extra information so we import Sequential from models","execution_count":null},{"metadata":{"_cell_guid":"32118274-f023-4a7f-8d62-fe3a3796ff7b","_uuid":"b6edf1f142a5195b829bc541bf42da023b5aeed4","id":"T71utABW92J8","trusted":true},"cell_type":"code","source":"from keras.models import Sequential\nimport pandas as pd","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"e952e8ad-e51c-4e2d-8163-6d86273f7f17","_uuid":"27f3a5ae3ce5a87240cab3b9ba1f3f678830eb5a","id":"ja6xaTVOELGP"},"cell_type":"markdown","source":"We must specify which data format convention Keras will follow using the following line of code. Keras can accept the number of channels before other dimensions or after it but here we have to specify which convention we will use. We will use channels last which is Tensorflow's convention .","execution_count":null},{"metadata":{"_cell_guid":"56fdb1f4-94be-4fdf-b072-03519b2b8da9","_uuid":"b1a4dc85a2ee874f428d51cf736fe01c2de77a90","id":"_zASxWJTEJGr","trusted":true},"cell_type":"code","source":"K.set_image_data_format('channels_last')\nnumpy.random.seed(0)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"405477f2-49a2-4a82-933a-69f389a92ad8","_uuid":"a4172fbcf2867bc9c3ce51ece2b79b26de64f894","id":"DBqW5EfhZ9q1"},"cell_type":"markdown","source":"There are two csv files that contain the data for the training set and the test set when combined they form the mnist Dataset of 60,000 28x28 grayscale images of the 10 digits, along with a test set of 10,000 images.  when we use the read_csv() function to read the dataset csv files.","execution_count":null},{"metadata":{"_cell_guid":"898289f7-4acb-4343-8ef0-19aba44bfe8f","_uuid":"66fbfe867582338a953b00b7fe2d7a8056d32818","executionInfo":{"elapsed":1701,"status":"ok","timestamp":1516128115363,"user":{"displayName":"Abd El Rhman ElMoghazy","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"112798608963931725749"},"user_tz":-120},"id":"hAEk70qRbUs8","outputId":"674d2997-9754-4989-e278-3a515fb465bd","trusted":true},"cell_type":"code","source":"X = pd.read_csv('../input/train.csv')\ntest = pd.read_csv('../input/test.csv')\n\ny = X[\"label\"]\nX.drop([\"label\"], inplace = True, axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e0e70d0813cd19bbee9022eb9a3ea51b8103d024","trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X.values, y.values, test_size=0.2 , random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2bcc3bcdedcdc1b50aa3582ecdb51a71af7e904f"},"cell_type":"markdown","source":"We then reshape the samples according to TensorFlow convention which we chosed previously using \"K.set_image_data_format('channels_last')\" samples,rows,columns,channels as we are using channels_last if you are using channels_first you will need to change the order to samples,channels,rows,column and here we have only one channel because we are using the image in grayscale not RGB. We should also make the output in the form of one vs all (aka one hot encoding) which means that we will have 10 calsses from 0 to 9 one class for each number from 0 to 9\n\n","execution_count":null},{"metadata":{"_uuid":"e856a81b5c44ed2b8232f4125a4a34e7ba758314","trusted":true},"cell_type":"code","source":"X_train = X_train.reshape(X_train.shape[0], 28, 28 , 1).astype('float32')\nX_test = X_test.reshape(X_test.shape[0], 28, 28 , 1).astype('float32')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a8e432a179ef833e69bb5158d96f16e4f990d38f"},"cell_type":"markdown","source":"<h1>Data Exploration</h1>\nLet's explore the data we have as this will give us a hint on the algorithm we will use if we have to choose. Exploring data is also very important because it will tell you which accuracy metric you are going to use, if the data is balanced which means all the classes have fair contribution in the dataset regarding its numbers then we can easily use accuracy, But if the data is skewed then we won't be able to use accurace as it's results will be misleading and we may use F-beta score instead.","execution_count":null},{"metadata":{"_uuid":"8ee50b8d8d9187615aceb089d6af67cc89a8bb5f","trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nprint(\"the number of training examples = %i\" % X_train.shape[0])\nprint(\"the number of classes = %i\" % len(numpy.unique(y_train)))\nprint(\"Dimention of images = {:d} x {:d}  \".format(X_train[1].shape[0],X_train[1].shape[1])  )\n\n#This line will allow us to know the number of occurrences of each specific class in the data\nunique, count= numpy.unique(y_train, return_counts=True)\nprint(\"The number of occuranc of each class in the dataset = %s \" % dict (zip(unique, count) ), \"\\n\" )\n \nimages_and_labels = list(zip(X_train,  y_train))\nfor index, (image, label) in enumerate(images_and_labels[:12]):\n    plt.subplot(5, 4, index + 1)\n    plt.axis('off')\n    plt.imshow(image.squeeze(), cmap=plt.cm.gray_r, interpolation='nearest')\n    plt.title('label: %i' % label )","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2d6c72777dffe414b82987d970eaf60a5b08c1a8"},"cell_type":"markdown","source":"From the previous results we can see that the dataset consists of 60000 training example each is an image of dimention 28 * 28. We can see that the number of occurances of each class is almost balanced and based on that it is safe to use accuracy as our metric later.","execution_count":null},{"metadata":{"_cell_guid":"7bc85401-5205-4891-837e-ed87f809af10","_uuid":"5a9a1f8ead473922206ecef2aeef24ea7837205e","id":"dr-O3FdEjZMG"},"cell_type":"markdown","source":"<h1>  Model Design and Achitecture</h1>\n\nNow lets implement the first layer of the convolutional network as shown in the schema below and i will use a simple archicture simillar to LeCun's network a .\n![alt text](https://raw.githubusercontent.com/MoghazyCoder/Machine-Learning-Tutorials/master/assets/Layer.png)\nFor the sequential model you just stack the layers and only specify the image input dimensions in the first layer.\nOur first layer will be a convolutional layer Conv2D() where we specify the number of feature maps , the input shape and the activation function which is here relu .The relu activation function is represented mathematically by max(0,X).\nWe then add the max pooling layer (which is the most common kind of pooling) with a kernel of dimensions 2 * 2 .\n","execution_count":null},{"metadata":{"_cell_guid":"4191bd33-3a05-44fb-9d04-230069de90b0","_uuid":"32c0f10501334b47d509d1071ef4d43a10c4af03","id":"SHZ0G1fykEPM","trusted":true},"cell_type":"code","source":"from keras.layers import Dropout\n\nmodel = Sequential()\n\nmodel.add(Conv2D(100, kernel_size=3, padding=\"valid\", input_shape=(28, 28, 1), activation = 'relu'))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"4ec94ca1-9486-4a06-a93d-ed684c6f9200","_uuid":"da8f3eb98b3db49f376611070f585941e934ecc2","id":"Jnn9RQ92lh7L"},"cell_type":"markdown","source":"Lets add the 2nd layer but this time we increase the feature maps .","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"model.add(Conv2D(100, kernel_size=3, padding=\"valid\", activation = 'relu'))\nmodel.add(Conv2D(100, kernel_size=3, padding=\"valid\", activation = 'relu'))\n\nmodel.add(MaxPooling2D(pool_size=(2, 2), strides=(1, 1)))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"8195dc3b-a5c0-470b-8e91-57dd81f17940","_uuid":"5dd8d45e683e99a8366b0a83a63e93b7ffa9a1fb","id":"7Zd7f3cel8A5"},"cell_type":"markdown","source":"Now we add a flatten layer that takes the output of the CNN and flattens it and passes it as an input to the Dense Layers which passes it to the output layer.\nwe have used number of classes = 10 because we have 10 numbers from 0 to 9 .\nevery dense layer contains 300 neurons except for the output layer.\nWe use Softmax with the output layer to output estimated probability vector for  multi-class classification .","execution_count":null},{"metadata":{"_cell_guid":"a50cea93-2f1c-4d80-a9df-e7e426037f2e","_uuid":"7f430854d3a13b976815d7f0697a079e1b4c958f","id":"lieaM8XWl8kX","trusted":true},"cell_type":"code","source":"from keras.layers.core import Activation\n\nmodel.add(Flatten())\nmodel.add(Dense(units= 500, activation='relu'  ))\nmodel.add(Dropout(0.5))\n\nmodel.add(Dense(10))\nmodel.add(Activation(\"softmax\"))\n\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"6e51b494-f2ad-4dc1-8d00-73c91004800e","_uuid":"8bca856fd30e1e9e4871323a3d6e366f81e43c87","id":"l5wJGng0p6-B"},"cell_type":"markdown","source":"We have to compile the model and then try training it using the fit() function which fits the training data and labels , the number of epochs and the batch_size which is the number of photos per training cycle.\nThe last thing that we are going to do is to evaluate the model to ensure that it doesn't overfit the trainig data .Evaluating the model is done by using the weights that resulted from the training step and using it to estimate the value of the test data that the model haven't seen before to estimate how well the model will perform in the future on new data.\n\nif you are using cross-validation split then the convention is to split the data by 60% training set , 20% validation set and 20% test set but in the era of big data this ratio may vary according to the amount of data you have.","execution_count":null},{"metadata":{"_cell_guid":"df82157c-45c5-414c-9aae-ba9670e5f1e4","_uuid":"5edc4cb02f9d6f3f8ae7ae93d5299d49f4c8b935","id":"96_UW0g1hMDF"},"cell_type":"markdown","source":"We have used categorical_crossentropy as the cost function for that model but what does we mean by **cost function**\n\n#### Cost function : It is a measure of the overall loss in our network after assigning values to the parameters during the forward phase so it indicates how well the parameters were chosen during the forward probagation phase.\n\n#### Optimizer : It is the gradiant descent algorithm that is used. We use it to minimize the cost function to approach the minimum point. We are using adam optimizer which is one of the best gradient descent algorithms. You can refere to this paper to know how it works https://arxiv.org/abs/1412.6980v8\n\nYou can use other metrics to measure the performance other than accuracy as precision or recall or F1 score. the choice depends on the problem itself. Where high recall means low number of false negatives , High precision means low number of false positives and     F1 score is a trade off between them. You can refere to this article for more about precision and recall http://scikit-learn.org/stable/auto_examples/model_selection/plot_precision_recall.html","execution_count":null},{"metadata":{"_uuid":"16f39f73d888f276d572b8e73fd39dafdb2d0a49","trusted":true},"cell_type":"code","source":"y_train = np_utils.to_categorical(y_train).astype('int32')\ny_test = np_utils.to_categorical(y_test)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5ed2cd184b2673e92e157f1f76bc2a38c5f679c5"},"cell_type":"markdown","source":"I will use ImageDataGenerator from keras to augment the images. Augmenting the images makes the model more robust and more generalizable when using newly unseen data like the data in the test set of the competition. There are many ways to augment the images like centering the images, normalization, rotation, shifting, and flipping and i will use some of them [here](http://) .","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow import keras\n\ncallbacks = [\n    keras.callbacks.EarlyStopping(\n        # Stop training when `val_loss` is no longer improving\n        monitor='val_loss',\n        # \"no longer improving\" being defined as \"no better than 1e-2 less\"\n        min_delta=1e-3,\n        # \"no longer improving\" being further defined as \"for at least 2 epochs\"\n        patience=25,\n        verbose=1)\n]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"812b3e24c4c5b9c767261cfdb50f2839e3f3e024","trusted":true},"cell_type":"code","source":"model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n\nX_train = X_train.reshape(-1,28,28,1)\nX_test = X_test.reshape(-1,28,28,1)\n\nfrom keras.preprocessing.image import ImageDataGenerator\n\ndatagen = ImageDataGenerator(\n    featurewise_center=True,\n    featurewise_std_normalization=True,\n    rotation_range=10,\n    fill_mode='nearest',\n    validation_split = 0.2\n    )\n\ndatagen.fit(X_train)\n\ntrain_generator = datagen.flow(X_train, y_train, batch_size=60, subset='training')\n\nvalidation_generator = datagen.flow(X_train, y_train, batch_size=60, subset='validation')\n\n\n# fits the model on batches with real-time data augmentation:\nhistory = model.fit_generator(generator=train_generator,\n                    validation_data=validation_generator,\n                    use_multiprocessing=True,\n                    steps_per_epoch = len(train_generator) / 60,\n                    validation_steps = len(validation_generator) / 60,\n                    epochs = 300,\n                    workers=-1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"acc = history.history['acc']\nval_acc = history.history['val_acc']\n\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nplt.figure(figsize=(8, 8))\nplt.subplot(2, 1, 1)\nplt.plot(acc, label='Training Accuracy')\nplt.plot(val_acc, label='Validation Accuracy')\nplt.legend(loc='lower right')\nplt.ylabel('Accuracy')\nplt.ylim([-1,1])\nplt.title('Training and Validation Accuracy')\n\nplt.subplot(2, 1, 2)\nplt.plot(loss, label='Training Loss')\nplt.plot(val_loss, label='Validation Loss')\nplt.legend(loc='upper right')\nplt.ylabel('Cross Entropy')\nplt.ylim([-1,1.0])\nplt.title('Training and Validation Loss')\nplt.xlabel('epoch')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"ff1d0906-5cdb-4bc7-b1cf-67bcc4266c80","_uuid":"a578dfdf36a8734c102954e722d78251d4f0c36d","executionInfo":{"elapsed":610614,"status":"ok","timestamp":1516130986715,"user":{"displayName":"Abd El Rhman ElMoghazy","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"112798608963931725749"},"user_tz":-120},"id":"XhwUKUarp80w","outputId":"2c577b4a-6d5f-4077-defb-ae45fb7a5716","scrolled":false,"trusted":true},"cell_type":"code","source":"# model.fit(X_train, y_train, epochs= 32 , batch_size=200, validation_split = 0.2)\nscores = model.evaluate(X_test, y_test, verbose = 10 )\nprint ( scores )\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"56c100a2800ed9b4973926206fe8e215e81c4729","trusted":true},"cell_type":"code","source":"test_set = (test.values).reshape(-1, 28, 28 , 1).astype('float32')\n\nres = model.predict(test_set)\nres = numpy.argmax(res,axis = 1)\nres = pd.Series(res, name=\"Label\")\nsubmission = pd.concat([pd.Series(range(1 ,28001) ,name = \"ImageId\"),   res],axis = 1)\nsubmission.to_csv(\"cnn_mnist_datagen.csv\",index=False)\nsubmission.head(10)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"36f8febd-6203-4551-abb4-36964a434417","_uuid":"2fb0407cc6ae233656c13198a6db319a119768b5"},"cell_type":"markdown","source":"Note that these results can be further optimized and regulized but i will leave that for you. Knowing how to optimize the results is very important and can be done using error analysis techniques.","execution_count":null},{"metadata":{"_cell_guid":"ad354ed9-e997-4048-ba23-52858b34d74c","_uuid":"8f692682b13b39316b9de77c41f4f2cbaf4cf7f3","id":"NYNAQuZxtHIQ"},"cell_type":"markdown","source":"### This tutorial is written by AbdElRhman ElMoghazy.\n\n### Refrences ,Textbooks and Tutorials :\nHands on machine learning with scikit-learn and TensorFlow by Aurélien Géron\n\nPyhron machine learning 2nd edition by Sebastian Raschka ,Vahid Mirjalili\n\nhttp://www.deeplearningbook.org/\n\nhttps://keras.io/\n\nhttps://machinelearningmastery.com/handwritten-digit-recognition-using-convolutional-neural-networks-python-keras/\n\nhttps://codelabs.developers.google.com/codelabs/cloud-tensorflow-mnist/index.html?index=..%2F..%2Findex#0","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}