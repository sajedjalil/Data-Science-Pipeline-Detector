{"cells":[{"metadata":{},"cell_type":"markdown","source":"# 1. Introduction\n\nIn this notebook, I have tried 2 models. First Model is skip connection neural network using Functional API and then Second Model is a CNN Model. Both of the model achienved approximately same result which is discernible in confusion matrix below after every model. I have used ImageDataGenerator function for Image Augmentation. Scale, Zoom, rotation,shift are some properties that has been changes for image augmentation. Both model performance is also compared at the end of notebook.\n\n1. Model 1 : Functional Model with Skip Connection \n2. Model 2 : Sequential CNN Model","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.layers import Conv2D,Activation,Lambda,BatchNormalization,Dropout,Flatten,MaxPooling2D,Input,Dense,MaxPool2D\nfrom tensorflow.keras.models import Model,Sequential\nfrom tensorflow.keras.datasets import mnist\nfrom tensorflow.keras.utils import plot_model\nfrom tensorflow.keras.losses import binary_crossentropy,mse\nfrom tensorflow.keras import backend as K \nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator,array_to_img,img_to_array\nfrom tensorflow.keras.utils import to_categorical\n\nfrom tensorflow.keras.callbacks import (\n    ReduceLROnPlateau,\n    EarlyStopping,\n    ModelCheckpoint,\n    TensorBoard\n)\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train = pd.read_csv(\"../input/digit-recognizer/train.csv\")\nsample,features = train.shape[0],train.shape[1]\nprint(f\"Train data has {sample} rows and {features} columns\")\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test = pd.read_csv(\"../input/digit-recognizer/test.csv\")\nsample,features = test.shape[0],test.shape[1]\nprint(f\"Test data has {sample} rows and {features} columns\")\ntest.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's see the distribution of classes\nplt.xkcd()\nplt.figure(figsize = (16,12))\nax = sns.barplot(x = train['label'].value_counts().index,y = train['label'].value_counts().values)\n\nrects = ax.patches\n\nfor rect in rects:\n    height = rect.get_height()\n    ax.text(rect.get_x() + 1,height + 20, str (np.round(height,0)) + \" ( \" + str (np.round(height/60000,4)) + \" )\", ha = 'center',va = 'center_baseline')\n\n\n\nplt.xlabel(\"Handwritten Digits\",fontsize = 20)\nplt.ylabel(\"Frequency\",fontsize = 20)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From above bar plot we can definitely concludes that this is not imbalanced dataset. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# how our input data looks\nfrom matplotlib import pyplot\nplt.figure(figsize = (16,12))\nfor i in range(0, 9):\n    pyplot.subplot(330 + 1 + i)\n    r = np.random.choice(10000)\n    pyplot.imshow(train.drop(['label'],axis=1).iloc[r].values.reshape(28,28), cmap=pyplot.get_cmap('gray'))\n    plt.grid(False)\n    plt.axis(False)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def generate_data(df,label = 0):\n    X = train[train['label']==label]\n    datagen = ImageDataGenerator(featurewise_center=False,featurewise_std_normalization=False,\n                                zca_whitening=False,rotation_range = 20,\n                                width_shift_range = 0.2,height_shift_range = 0.2,\n                                shear_range = 0.2,zoom_range = 0.2,horizontal_flip = True,\n                                vertical_flip = True,fill_mode = 'nearest'\n                            )\n    X = X.drop(['label'],axis=1).values.reshape((X.shape[0], 28, 28, 1))\n    datagen.fit(X)\n    for x_batch,y_batch in datagen.flow(X,[label]*len(X),batch_size = 9):\n        plt.figure(figsize = (16,12))\n        for i in range(0,9):\n            pyplot.subplot(330 + 1 + i)\n            pyplot.imshow(X[i].reshape(28,28), cmap=pyplot.get_cmap('gray'))\n        # show the plot\n        pyplot.show()\n        break","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## let's generate augmented image of label 5\ngenerate_data(train,label = 5)\n# same can be done using above function for label between 0 to 9\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Y_train = train[\"label\"]\nX_train = train.drop(labels = [\"label\"],axis = 1)\nX_train = X_train / 255.0\nX_test = test / 255.0\nX_train = X_train.values.reshape(-1,28,28,1)\nX_test = X_test.values.reshape(-1,28,28,1)\nY_train = to_categorical(Y_train, num_classes = 10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For the data augmentation, i choosed to :\n\n1. Randomly rotate some training images by 10 degrees\n2. Randomly Zoom by 10% some training images\n3. Randomly shift images horizontally by 10% of the width\n4. Randomly shift images vertically by 10% of the height\n\nI did not apply a vertical_flip nor horizontal_flip since it could have lead to misclassify symetrical numbers such as 6 and 9.\n\nOnce our model is ready, we fit the training dataset .","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# CREATE MORE IMAGES VIA DATA AUGMENTATION\ndatagen = ImageDataGenerator(\n        rotation_range=10,  \n        zoom_range = 0.10,  \n        width_shift_range=0.1, \n        height_shift_range=0.1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Split the train and the validation set for the fitting\nfrom sklearn.model_selection import train_test_split\nX_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size = 0.1, random_state=42,stratify = Y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def ConvNet(x,filters,size,strides=1,batch_norm = True):\n    if strides==1:\n        padding = 'same'\n    else:\n        padding = 'valid'\n    x = Conv2D(filters = filters,kernel_size = size,strides = strides,padding = padding\n              ,use_bias = not batch_norm,kernel_regularizer = tf.keras.regularizers.l2(0.005))(x)\n    if batch_norm:\n        x = BatchNormalization()(x)\n        x = tf.keras.layers.LeakyReLU(alpha=0.1)(x)\n    return(x)\n\ndef NetResidual(x,filters):\n    prev = x\n    x = ConvNet(x,filters//2,1)\n    x = ConvNet(x,filters,3)\n    x = tf.keras.layers.Add()([prev,x])\n    return(x)\n\ndef NetBlock(x,filters,blocks):\n    x = ConvNet(x,filters,3,strides=2)\n    for _ in range(blocks):\n        x = NetResidual(x,filters)\n    return(x)\n        \ndef Network(filters,x_input,name = 'Res_Con'):\n    x = inputs = Input(x_input.shape[1:])\n    x = ConvNet(x,32,3)\n    x = NetBlock(x,64,3)\n    x = Flatten()(x)\n    x = Dense(256,activation = 'relu')(x)\n    x = Dropout(0.5)(x)\n    x = Dense(10,activation = 'softmax')(x)\n    return(Model(inputs,x,name = name))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model1 = Network(32,X_train)\nmodel1.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4)\nlearning_rate_reduction = ReduceLROnPlateau(monitor='val_acc', \n                                            patience=3, \n                                            verbose=1, \n                                            factor=0.5, \n                                            min_lr=0.00001)\nckpt = ModelCheckpoint('res_model.h5',\n                            verbose=1, save_weights_only=True,save_best_only = True)\nepochs = 50\nbatch_size = 86\n\nmodel1.compile(optimizer = optimizer , loss = \"categorical_crossentropy\", metrics=[\"accuracy\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In order to avoid overfitting problem, we need to expand artificially our handwritten digit dataset. We can make your existing dataset even larger. The idea is to alter the training data with small transformations to reproduce the variations occuring when someone is writing a digit.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"datagen = ImageDataGenerator(\n        rotation_range=10,  \n        zoom_range = 0.10,  \n        width_shift_range=0.1, \n        height_shift_range=0.1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history1 = model1.fit_generator(datagen.flow(X_train,Y_train, batch_size=batch_size),\n                              epochs = 30, validation_data = (X_val,Y_val),\n                              verbose = 1, steps_per_epoch=X_train.shape[0] // batch_size\n                              , callbacks=[learning_rate_reduction,ckpt])\n\n#model1.load_weights(\"../input/digit-recognizer-model/res_model.h5\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (16,8))\nplt.subplot(1,2,1)\nplt.plot(history1.history['loss'], color='b', label=\"Training loss\")\nplt.plot(history1.history['val_loss'], color='r', label=\"validation loss\",)\nplt.legend(loc='best', shadow=True)\nplt.subplot(1,2,2)\nplt.plot(history1.history['accuracy'], color='b', label=\"Training accuracy\")\nplt.plot(history1.history['val_accuracy'], color='r',label=\"Validation accuracy\")\nplt.legend(loc='best', shadow=True)\nplt.suptitle(\"Model 1 Performance\",fontsize = 30)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\nimport itertools\n\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                              cmap=plt.cm.Paired):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    plt.figure(figsize=(16,8))\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    if normalize:\n        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n\n    thresh = cm.max() / 2.\n    \n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, cm[i, j],\n                 horizontalalignment=\"center\",\n                 color=\"red\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n\n# Predict the values from the validation dataset\nY_pred = model1.predict(X_val)\n# Convert predictions classes to one hot vectors \nY_pred_classes = np.argmax(Y_pred,axis = 1) \n# Convert validation observations to one hot vectors\nY_true = np.argmax(Y_val,axis = 1) \n# compute the confusion matrix\nconfusion_mtx = confusion_matrix(Y_true, Y_pred_classes) \n# plot the confusion matrix\nplot_confusion_matrix(confusion_mtx, classes = range(10)) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here we can see that our CNN performs very well on all digits with few errors considering the size of the validation set (4 200 images).\n\nHowever, it seems that our CNN has some little troubles with the 4 digits, hey are misclassified as 9. Sometime it is very difficult to catch the difference between 4 and 9 when curves are smooth.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Set the CNN model \n# my CNN architechture is In -> [[Conv2D->relu]*2 -> MaxPool2D -> Dropout]*2 -> Flatten -> Dense -> Dropout -> Out\n\nmodel2 = Sequential()\n\nmodel2.add(Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same', \n                 activation ='relu', input_shape = (28,28,1)))\nmodel2.add(Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same', \n                 activation ='relu'))\nmodel2.add(MaxPool2D(pool_size=(2,2)))\nmodel2.add(Dropout(0.25))\n\n\nmodel2.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', \n                 activation ='relu'))\nmodel2.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', \n                 activation ='relu'))\nmodel2.add(MaxPool2D(pool_size=(2,2), strides=(2,2)))\nmodel2.add(Dropout(0.25))\n\n\nmodel2.add(Flatten())\nmodel2.add(Dense(256, activation = \"relu\"))\nmodel2.add(Dropout(0.5))\nmodel2.add(Dense(10, activation = \"softmax\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4)\nlearning_rate_reduction = ReduceLROnPlateau(monitor='val_acc', \n                                            patience=3, \n                                            verbose=1, \n                                            factor=0.5, \n                                            min_lr=0.00001)\nckpt = ModelCheckpoint('cnn_model.h5',\n                            verbose=1, save_weights_only=True,save_best_only = True)\nepochs = 30 \nbatch_size = 86\n\nmodel2.compile(optimizer = optimizer , loss = \"categorical_crossentropy\", metrics=[\"accuracy\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history2 = model2.fit_generator(datagen.flow(X_train,Y_train, batch_size=batch_size),\n                              epochs = 30, validation_data = (X_val,Y_val),\n                              verbose = 1, steps_per_epoch=X_train.shape[0] // batch_size\n                              , callbacks=[learning_rate_reduction,ckpt])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (16,8))\nplt.subplot(1,2,1)\nplt.plot(history2.history['loss'], color='b', label=\"Training loss\")\nplt.plot(history2.history['val_loss'], color='r', label=\"validation loss\",)\nplt.legend(loc='best', shadow=True)\nplt.subplot(1,2,2)\nplt.plot(history2.history['accuracy'], color='b', label=\"Training accuracy\")\nplt.plot(history2.history['val_accuracy'], color='r',label=\"Validation accuracy\")\nplt.legend(loc='best', shadow=True)\nplt.suptitle(\"Model 2 Performance\",fontsize = 30)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\nimport itertools\n\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                              cmap=plt.cm.coolwarm_r):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    plt.figure(figsize=(16,8))\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    if normalize:\n        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n\n    thresh = cm.max() / 2.\n    \n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, cm[i, j],\n                 horizontalalignment=\"center\",\n                 color=\"red\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n\n# Predict the values from the validation dataset\nY_pred = model1.predict(X_val)\n# Convert predictions classes to one hot vectors \nY_pred_classes = np.argmax(Y_pred,axis = 1) \n# Convert validation observations to one hot vectors\nY_true = np.argmax(Y_val,axis = 1) \n# compute the confusion matrix\nconfusion_mtx = confusion_matrix(Y_true, Y_pred_classes) \n# plot the confusion matrix\nplot_confusion_matrix(confusion_mtx, classes = range(10)) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"1. The model2 reaches almost 99.30% accuracy on validation dataset after 30 epochs, could train for more epochs with decrease in learning rate. \n\n2. Train and validation accuracy is almost close so there is no overfitting definitely.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sub = pd.read_csv(\"../input/digit-recognizer/sample_submission.csv\")\ndef predict_test(data,model):\n    data = data / 255.0\n    data = data.values.reshape(-1,28,28,1)\n    return(model.predict(data))\n\nres = predict_test(test,model1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_result = []\nfor i in range(len(res)):\n    final_result.append(np.argmax(res[i]))\nsub['Label'] = final_result","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Performance Comparison & Erros For Each Model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# skip connection network\nmodel1.load_weights(\"../input/digit-recognizer-model/res_model.h5\")\n\n# cnn model\nmodel2.load_weights(\"../input/digit-recognizer-model/cnn_model.h5\")\n\ndef display(mode1,model2,data):\n    # (batch_size,img_dim,img_dim,filters)\n    # model 1 prediction\n    p1 = np.argmax(model1.predict(data.values.reshape(-1,28,28,1)),axis=1)\n    # model 2 prediction\n    p2 = model2.predict_classes(data.values.reshape(-1,28,28,1))\n    plt.figure(figsize = (24,10))\n    for i in range(8):\n        plt.subplot(2,4,i+1)\n        r = np.random.choice(20000)\n        plt.title(f\"Model1 Predicted {p1[r]}\\nModel2 Predicted {p2[r]}\")\n        plt.imshow(data.iloc[r].values.reshape(28,28),cmap = 'gray')\n        plt.grid(False)\n        plt.axis(False)\n    plt.show()\n        \n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"display(model1,model2,test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Let's investigate for errors.(Curious!!! Then let's dive in)\n\n#### I want to see the most important errors . For that purpose i need to get the difference between the probabilities of real value and the predicted ones in the results.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def error(model,data):\n    try:\n        # this is for funtional model\n        p1 = np.argmax(model.predict(data.drop(['label'],axis=1).values.reshape(-1,28,28,1)),axis=1)\n    except:\n        # this is for sequential model\n        p1 = model.predict_classes(data.drop(['label'],axis=1).values.reshape(-1,28,28,1))\n    \n    df = data[p1-data['label'].values!=0]\n    plt.figure(figsize = (24,10))\n    i=0\n    for indx in np.random.choice(df.index.tolist(),8):       \n        plt.subplot(2,4,i+1)\n        #r = np.random.choice(20000)\n        plt.title(f\"Actual Label {data.iloc[indx]['label']}\\nModel1 Predicted {p1[indx]}\")\n        plt.imshow(train.drop(['label'],axis=1).iloc[indx].values.reshape(28,28),cmap = 'gray')\n        plt.grid(False)\n        plt.axis(False)\n        i+=1\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Model1 Error","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"error(model1,train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Model2 Error","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"error(model2,train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### If you had a great time reading this notebook, do some upvotes and comments too wherever you think there is need of some improvement.\n\n#### If you have more intuitive ideas to improve the accuracy pls do comment, it will be much appreciated...","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}