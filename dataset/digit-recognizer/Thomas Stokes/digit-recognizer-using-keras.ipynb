{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Digit Recognizer CNN\n\nThis is my first attempt at a CNN and using keras. I am using a popular notebook by Poonam Ligade as a guide: https://www.kaggle.com/poonaml/deep-neural-network-keras-way. I am also using the documentation for Keras which can be found here: https://keras.io/about/ .This notebook is heavily annotated, because I want this to be a clear notebook with which I (and hopefully others) can learn from. I got a score of 0.98575 from this CNN, I didn't put a lot of time into the finetuning parameters, so this score can easily be improved. Feel free to use this as a template and tweak the parameters to get a better result.","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 enle competitions download -c digit-recognizervironment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n\n# Here are all the packages that I will be using\nimport numpy as np # Linear algebra\nimport pandas as pd # Data processing \nimport matplotlib.pyplot as plt # Data visualistaion\nfrom sklearn.model_selection import train_test_split # Creating the training for the neural network\nfrom keras.models import Sequential # The structure of the neural network\nfrom keras import backend as K # Output layer of the neural network\nfrom keras.layers import Dense , Dropout , Lambda, Flatten # Functions for the inner layers of the neural network\nfrom keras.optimizers import Adam ,RMSprop # Optimizing the network \nfrom keras.preprocessing.image import ImageDataGenerator # Preprocessing the images\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Inspecting the Data\n\nFirst I want to look at how the images are represented, so I know how to reshape them into a form that my CNN can use.","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Let's look at the training data\ntrain = pd.read_csv(\"/kaggle/input/digit-recognizer/train.csv\")\nprint(train.shape)\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's get an overview of the training data\ntrain.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's now look at the test data\ntest = pd.read_csv(\"/kaggle/input/digit-recognizer/test.csv\")\nprint(test.shape) \ntest.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's get an overview of the test data\ntest.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As expected the tables for the train and test data are very similar. The labels are a value from 0-9 corresponding to the digit that the image represents. The pixels have a spectrum of values depending on how black or white they are, (0 for completely black, 255 for completely white).  ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Now I want to split up the train data into the images (X) and the labels (y).\nX_train = (train.iloc[:,1:].values).astype('float32') # Images (represented as pixel values)\ny_train = train.iloc[:,0].values.astype('int32') # Labels (numbers represented by the images)\nX_test = test.values.astype('float32')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# View X_train\nX_train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# View y_train\ny_train","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The labels contain multiple values (integers from 0-9). Problems of this sought (with multiple outputs) are called multiclass classification problems. This will be important later because the sequential model (our input layer for the neural network), only works for problems with one input tensor and one output tensor. For more information go and read the documentation for sequential models in Keras which can be found here: https://keras.io/guides/sequential_model/.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Visualising the Images\n\nBefore making the neural network, visualising the data can be a useful way for us to get an intutition for what our neural network should look like.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Reshape the data so that into a 28x28 grid so that the pixels form the corresponding original image\nX_train = X_train = X_train.reshape(X_train.shape[0], 28, 28)\nX_test = X_test.reshape(X_test.shape[0], 28, 28)\n\n# Generate the images\nfor i in range(20,23):\n    plt.figure(figsize=(14,14)) # Scale up the image (to make it easier to see)\n    plt.subplot(500 + (i+1)) # Creates the suplot\n    plt.imshow(X_train[i], cmap=plt.get_cmap('gray')) # Displays the image using a greyscale colour map\n    plt.title(y_train[i]) # Title for our image","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Reshaping the data so it adds in a greyscale dimension\nX_train = X_train.reshape(X_train.shape[0], 28, 28, 1)\nX_test = X_test.reshape(X_test.shape[0], 28, 28, 1)\nX_train.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Preprocessing the Data\n\nThe purpose of this step is to get the data into a form where it can be inputed into the neural network. Remember from earlier that sequential models can only take one input tensor and one output tensor. The Keras documentation of preprocessing data can be found here: https://keras.io/guides/preprocessing_layers/.   ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Normalising the training data\n# Normalising formula: Z = (X-mean)/std, this changes the distrubtion of the data to N~(mean=0, var=1)\nmean_px = X_train.mean().astype(np.float32)\nstd_px = X_train.std().astype(np.float32)\n\ndef standardize(x): \n    return (x-mean_px)/std_px","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now I would like to change the labels from being a digit from 0-9 to a one-hot vector. Which is a 10x1 vector that is zero in all the entries apart from the entry corresponding to the number, where it is 1. For example 0 is [1,0,0,0,0,0,0,0,0,0], 1 is [0,1,0,0,0,0,0,0,0,0] etc... The reason for doing this is because the neural network can output results in this form but not in the current form of the labels.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.utils.np_utils import to_categorical # Changes the data to categorical data (the type we want)\ny_train= to_categorical(y_train)\nnum_classes = y_train.shape[1] # Number to columns in our new y vector\nnum_classes","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Linear Model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fix the random seed for reproducibility\nseed = 37\nnp.random.seed(seed)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.models import  Sequential # Linear model\nfrom keras.layers.core import  Lambda , Dense, Flatten, Dropout # Functions for manipulating the inner layers of the neural network\nfrom keras.callbacks import EarlyStopping # Checking the models accuracy whilst training\nfrom keras.layers import BatchNormalization, Convolution2D , MaxPooling2D # Inner layers of the neural network","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* **Lambda** performs basic arithmetic operations on the inputs (sum, mean etc...)\n* **Flatten** transforms the data into a 1D array\n* **Dense** connects all the neurons in one layer to all the neurons in the next layer\n* **Dropout** randomly sets input units to zero to help prevent overfitting","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"model= Sequential()\nmodel.add(Lambda(standardize,input_shape=(28,28,1)))\nmodel.add(Flatten())\nmodel.add(Dense(10, activation='softmax')) # softmax function converts the inputs to a vector with elements in (0,1) with the sum of the elements = 1\nprint(\"input shape \",model.input_shape)\nprint(\"output shape \",model.output_shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are a few more things to add before we can train the neural network.\n1. A loss function to measure to good the networks guesses are, https://keras.io/api/losses/\n2. An optimiser to update the network everytime it sees a new piece of data, https://keras.io/api/optimizers/\n3. Metrics to montior the performance of the neural network, https://keras.io/api/metrics/","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.optimizers import RMSprop # RMSprop uses the moving mean squared average of the gradients to optimise the model \nmodel.compile(optimizer=RMSprop(lr=0.001), # lr is the learning rate (default is 0.001)\n loss='categorical_crossentropy', # categorical_crossentropy is used for labels that are one-hot vectors\n metrics=['accuracy']) # Calculates how often the neural networks prediction matches the label","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.preprocessing import image\ngen = image.ImageDataGenerator() # Generates batches of tensor image data ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split # Splits the training data to help avoid overfitting\nX = X_train\ny = y_train\nX_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=37)\nbatches = gen.flow(X_train, y_train, batch_size=64)\nval_batches=gen.flow(X_val, y_val, batch_size=64)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Train the model\nhistory=model.fit_generator(generator=batches, steps_per_epoch=batches.n, epochs=1, \n                    validation_data=val_batches, validation_steps=val_batches.n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Fully Connected Model\n\nFully connected models are models where all the neurons in one layer are fully connected to all the neurons in the next layer.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creates a Fully connected model\ndef get_fc_model():\n    model = Sequential([\n        Lambda(standardize, input_shape=(28,28,1)),\n        Flatten(),\n        Dense(512, activation='relu'), # relu (rectified linear unit) is the max(x,0), https://keras.io/api/layers/activations/#relu-function\n        Dense(10, activation='softmax')\n        ])\n    model.compile(optimizer='Adam', # Adam is also known as Stochastic gradient descent, https://keras.io/api/optimizers/adam/ \n                  loss='categorical_crossentropy',\n                  metrics=['accuracy'])\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Train the model\nfc = get_fc_model()\nfc.optimizer.lr=0.01\nhistory=fc.fit_generator(generator=batches, steps_per_epoch=batches.n, epochs=1, \n                    validation_data=val_batches, validation_steps=val_batches.n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Convolution Neural Networks (CNN)\n\nCNN's differ from regular neural networks in that they look for features in the input data independent of where they are on the input data. They are especially good at classifying images for this reason.\n\nDocumentation:\n* **Convolution2D**- https://keras.io/api/layers/convolution_layers/convolution2d/\n* **MaxPooling2D**- https://keras.io/api/layers/pooling_layers/max_pooling2d/","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating the CNN\nfrom keras.layers import Convolution2D, MaxPooling2D\n\ndef get_cnn_model():\n    model = Sequential([\n        Lambda(standardize, input_shape=(28,28,1)),\n        Convolution2D(32,(3,3), activation='relu'),# Convolution2D(filters (specifies the number of pieces the data is divided into), strides (how many neighbouring inputs the layer considers), activation)\n        Convolution2D(32,(3,3), activation='relu'),\n        MaxPooling2D(), # Compacts the layer by taking the max value in a given window (default is 2x2)\n        Convolution2D(64,(3,3), activation='relu'),\n        Convolution2D(64,(3,3), activation='relu'),\n        MaxPooling2D(),\n        Flatten(),\n        Dense(512, activation='relu'),\n        Dense(10, activation='softmax')\n        ])\n    model.compile(Adam(), \n                  loss='categorical_crossentropy',\n                  metrics=['accuracy'])\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Train the CNN\nmodel= get_cnn_model()\nmodel.optimizer.lr=0.01\nhistory=model.fit_generator(generator=batches, steps_per_epoch=batches.n, epochs=1, \n                    validation_data=val_batches, validation_steps=val_batches.n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Finetuning\n\nData augmentation is creating new data for the model by modifying existing data. This is done to avoid overfitting. For images this may include: rotating, cropping, translating, different brightness etc... ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Augment the data\ngen =ImageDataGenerator(rotation_range=8, width_shift_range=0.09, shear_range=0.3,\n                               height_shift_range=0.09, zoom_range=0.04)\nbatches = gen.flow(X_train, y_train, batch_size=64) # Create the batches\nval_batches = gen.flow(X_val, y_val, batch_size=64) # Create the labels for the batches","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Train the model with the augmented data\nmodel.optimizer.lr=0.001\nhistory=model.fit_generator(generator=batches, steps_per_epoch=batches.n, epochs=1, \n                    validation_data=val_batches, validation_steps=val_batches.n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Batch Normalisation (BN) fine tunes the parameters to aid in training the neural network","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create the BN model\nfrom keras.layers.normalization import BatchNormalization\n\ndef get_bn_model():\n    model = Sequential([\n        Lambda(standardize, input_shape=(28,28,1)),\n        Convolution2D(32,(3,3), activation='relu'),\n        BatchNormalization(axis=1),\n        Convolution2D(64,(3,3), activation='relu'),\n        MaxPooling2D(),\n        BatchNormalization(axis=1),\n        Convolution2D(32,(3,3), activation='relu'),\n        BatchNormalization(axis=1),\n        Convolution2D(64,(3,3), activation='relu'),\n        MaxPooling2D(),\n        Flatten(),\n        BatchNormalization(),\n        Dense(512, activation='relu'),\n        BatchNormalization(),\n        Dense(10, activation='softmax')\n        ])\n    model.compile(Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Update the Model with BN\nmodel= get_bn_model()\nmodel.optimizer.lr=0.01\nhistory=model.fit_generator(generator=batches, steps_per_epoch=batches.n, epochs=1, \n                    validation_data=val_batches, validation_steps=val_batches.n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Train the model on the full dataset\nmodel.optimizer.lr=0.01\ngen = image.ImageDataGenerator()\nbatches = gen.flow(X, y, batch_size=64)\nhistory=model.fit_generator(generator=batches, steps_per_epoch=batches.n, epochs=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Kaggle Predictions\npredictions = model.predict_classes(X_test, verbose=0)\n\nsubmissions=pd.DataFrame({\"ImageId\": list(range(1,len(predictions)+1)),\n                         \"Label\": predictions})\nsubmissions.to_csv(\"DRC.csv\", index=False, header=True)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}