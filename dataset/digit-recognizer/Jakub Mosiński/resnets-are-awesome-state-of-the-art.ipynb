{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Intro\n\nThe notebook contains an implementation of a residual neural network in Keras. If you read the references and study the code, you should have an understanding of ResNets. By the way, this method achieved above 99.75%, this is a state-of-the-art solution, hope you like it!  \n\n![soluions](http://playagricola.com/Kaggle/KaggleMNISThist3.png)"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport time\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom keras.models import Model\nfrom keras.layers import Input, Conv2D, BatchNormalization, Activation\nfrom keras.layers import Add, Flatten, AveragePooling2D, Dense, Dropout\nfrom keras.callbacks import ModelCheckpoint, LearningRateScheduler\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.utils import plot_model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Load the data\ntrain = pd.read_csv(\"../input/digit-recognizer/train.csv\")\ntest = pd.read_csv(\"../input/digit-recognizer/test.csv\")\n\n# Reshape and normalize\nX = train.drop(columns=['label']).values.reshape(-1, 28, 28, 1) / 255\ny = train['label'].values\n\ntest = test.values.reshape(-1, 28, 28, 1) / 255\n\n# Get training and testing datasets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# ResNets\n\n<img src=\"https://i.stack.imgur.com/msvse.png\" alt=\"Drawing\" style=\"width: 400px;\"/>\n\nIn short, residual neural networks introduce a shortcut connection to prevent vanishing gradients problem. Therefore, enabling to train much deeper models. I won't focus here on how ResNets work as there is a lot of good material available. I want to provide you with an easy to follow implementation that you can study and later use, modify as you wish.\n\nIf you want to know more about ResNets I highly recommend that you read this article  \nhttps://towardsdatascience.com/introduction-to-resnets-c0a830a288a4. \n  \nMore on ConvNets  \nhttps://medium.com/zylapp/review-of-deep-learning-algorithms-for-image-classification-5fdbca4a05e2"},{"metadata":{"trusted":true},"cell_type":"code","source":"def residual_block(inputs, filters, strides=1):\n    \"\"\"Residual block\n    \n    Shortcut after Conv2D -> ReLU -> BatchNorm -> Conv2D\n    \n    Arguments:\n        inputs (tensor): input\n        filters (int): Conv2D number of filterns\n        strides (int): Conv2D square stride dimensions\n\n    Returns:\n        x (tensor): input Tensor for the next layer\n    \"\"\"\n    y = inputs # Shortcut path\n    \n    # Main path\n    x = Conv2D(\n        filters=filters,\n        kernel_size=3,\n        strides=strides,\n        padding='same',\n    )(inputs)\n    x = BatchNormalization()(x)\n    x = Activation('relu')(x)\n    \n    x = Conv2D(\n        filters=filters,\n        kernel_size=3,\n        strides=1,\n        padding='same',\n    )(x)\n    x = BatchNormalization()(x)\n    \n    # Fit shortcut path dimenstions\n    if strides > 1:\n        y = Conv2D(\n        filters=filters,\n        kernel_size=3,\n        strides=strides,\n        padding='same',\n        )(y)\n        y = BatchNormalization()(y)\n    \n    # Concatenate paths\n    x = Add()([x, y])\n    x = Activation('relu')(x)\n    \n    return x\n    \n    \ndef resnet(input_shape, num_classes, filters, stages):\n    \"\"\"ResNet \n    \n    At the beginning of each stage downsample feature map size \n    by a convolutional layer with strides=2, and double the number of filters.\n    The kernel size is the same for each residual block.\n    \n    Arguments:\n        input_shape (3D tuple): shape of input Tensor\n        filters (int): Conv2D number of filterns\n        stages (1D list): list of number of resiual block in each stage eg. [2, 5, 5, 2]\n    \n    Returns:\n        model (Model): Keras model\n    \"\"\"\n    # Start model definition\n    inputs = Input(shape=input_shape)\n    x = Conv2D(\n        filters=filters,\n        kernel_size=7,\n        strides=1,\n        padding='same',\n    )(inputs)\n    x = BatchNormalization()(x)\n    x = Activation('relu')(x)\n    \n    # Stack residual blocks\n    for stage in stages:\n        x = residual_block(x, filters, strides=2)\n        for i in range(stage-1):\n            x = residual_block(x, filters)\n        filters *= 2\n        \n    # Pool -> Flatten -> Classify\n    x = AveragePooling2D(4)(x)\n    x = Flatten()(x)\n    x = Dropout(0.3)(x)\n    x = Dense(int(filters/4), activation='relu')(x)\n    outputs = Dense(num_classes, activation='softmax')(x)\n    \n    # Instantiate model\n    model = Model(inputs=inputs, outputs=outputs)\n    return model    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Now lets see how our implementation looks like"},{"metadata":{"trusted":true},"cell_type":"code","source":"simple_model = resnet(\n    input_shape=X[0].shape, \n    num_classes=np.unique(y).shape[-1], \n    filters=64, \n    stages=[2]\n)\nsimple_architecture = plot_model(simple_model, show_shapes=True, show_layer_names=False)\nsimple_architecture.width = 600\nsimple_architecture","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Keep in mind that to match the dimensions of the tensors in Add() layer, in the shortcut path there is also a Conv2D layer with strides equal to the strides in the main path. What is more, the AveragePooling2D kernel size should satisfy $\\frac{prevKernelSize}{avgPoolKernelSize}={1, 2, 3, ...}$"},{"metadata":{},"cell_type":"markdown","source":"## How to win?\n\nIf you want to win competitions, it is a good practice to train several networks independently and average the predictions (ensemble). It produces more robust results so you don't have to make several submissions and hope to be lucky. The code below is meant for that purpose. Though, here I train just a single network but feel free to change the number of iterations. One more thing, for the submission I used the whole dataset (X, y) instead of (X_train, y_train) for 40 epochs and ensembled 5 models - 99.757% accuracy."},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_model(epochs, filters, stages, batch_size, visualize=False):\n    \"\"\"Helper function for tuning and training the model\n    \n    Arguments:\n        epoch (int): number of epochs\n        filters (int): Conv2D number of filterns\n        stages (1D list): list of number of resiual block in each stage eg. [2, 5, 5, 2]\n        batch_size (int): size of one batch\n        visualize (bool): if True then plot training results \n    \n    Returns:\n        model (Model): Keras model\n    \"\"\"\n    # Create and compile model\n    model = resnet(\n        input_shape=X[0].shape,\n        num_classes=np.unique(y).shape[-1],\n        filters=filters, \n        stages=stages\n    )\n    model.compile(\n        loss='sparse_categorical_crossentropy',\n        optimizer='adam',\n        metrics=['accuracy']\n    )\n\n    # Define callbacks\n    checkpoint = ModelCheckpoint(\n        filepath=f'resnet-{int(time.time())}.dhf5',\n        monitor='loss',\n        save_best_only=True\n    )\n\n    annealer = LearningRateScheduler(lambda x: 1e-3 * 0.8**x)\n\n    callbacks = [checkpoint, annealer]\n\n    # Define data generator\n    datagen = ImageDataGenerator(  \n        rotation_range=10,  \n        zoom_range=0.1, \n        width_shift_range=0.1, \n        height_shift_range=0.1\n    )\n    datagen.fit(X)\n\n    # Fit model\n    history = model.fit_generator(\n        datagen.flow(X_train, y_train, batch_size=batch_size),\n        validation_data=(X_test, y_test),\n        epochs=epochs, \n        verbose=2, \n        workers=12,\n        callbacks=callbacks\n    )\n    \n    if visualize:\n        fig, axarr = plt.subplots(1, 2, figsize=(16, 8))\n        # Plot training & validation accuracy values\n        axarr[0].plot(history.history['accuracy'])\n        axarr[0].plot(history.history['val_accuracy'])\n        axarr[0].set_title('Model accuracy')\n        axarr[0].set_ylabel('Accuracy')\n        axarr[0].set_xlabel('Epoch')\n        axarr[0].legend(['Train', 'Test'], loc='upper left')\n        # Plot training & validation loss values\n        axarr[1].plot(history.history['loss'])\n        axarr[1].plot(history.history['val_loss'])\n        axarr[1].set_title('Model loss')\n        axarr[1].set_ylabel('Loss')\n        axarr[1].set_xlabel('Epoch')\n        axarr[1].legend(['Train', 'Test'], loc='upper left')\n\n        plt.show()\n\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Train models\nmodels = []\nfor i in range(1):\n    print('-------------------------')\n    print('Model: ', i+1)\n    print('-------------------------')\n    model = train_model(\n        epochs=10,\n        filters=64,\n        stages=[3, 3, 3],\n        batch_size=128,\n        visualize=True\n    )\n    models.append(model)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get predictions, ensemble and create submission csv\npredictions = []\nfor model in models:\n    predictions.append(model.predict(test))\npredictions = np.sum(predictions, axis=0)\npredictions = np.argmax(predictions, axis=1)\nsubmission = pd.DataFrame({'ImageId': np.arange(1, 28001, 1), 'Label': predictions})\nsubmission.to_csv('mnist_resnet_submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now that you know a thing or two about ResNets, I encourage you to play with the model yourself. Try various configurations, implement a different ResNet (FYI this is ResNet V1). Maybe you will reach 99.79%, best of luck.\n\n### Please upvote if you found this notebook useful, thank you :)"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}