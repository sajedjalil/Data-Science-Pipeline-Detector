{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"**Dimensionality Reduction :**\n\n* Dimensionality reduction is a technique to visualize data that has more dimension (typically more than 3 dimension)\n\n* There are two methods that widely used for reduction one is PCA(principal component analysis) and other is t-SNE (t-distributed stochastic neighbourhood embedding)\n","metadata":{"_kg_hide-input":false}},{"cell_type":"markdown","source":"Preprocessing :\n\n* Before doing dimensionalty reduction , It is important to normalize data\n\n* Normalization is done by below formula , \n\n  $A_i = (A_i - min(A_i))\\hspace{0.2cm} / \\hspace{0.2cm} (max(A_i) - min(A_i)) $\n\n  $      where$ $A_i = i^{th} column$\n\n* By doing normalization we're putting every data point on the same scale. More formally After normalization all data points relies between [0,1]\n\n* There is also another technique known as Standardization (It is more used then normalization)\n\n* Standardization is done by below formula ,\n\n  $A_i = ( A_i - \\mu_i ) \\hspace{0.2cm} / \\hspace{0.2cm} \\sigma_i $\n  \n  $      where$ $A_i = i^{th} column$\n  \n  $\\hspace{1cm}  \\mu_i = mean$ $of$ $i^{th}$ $column$\n  \n  $\\hspace{1cm} \\sigma_i = standard$ $deviation $ $of$ $i^{th}$ $column$\n  \n* After Standardization, mean of data becomes 0 and standard deviation of data becomes 1, More Formally $\\mu_i = 0$ and $\\sigma_i = 1$.\n\n* Another thing that we have to know before starting Dimensionality reduction is co-variance matrix.\n\n* Co-variance matrix $(S)$ of data is $d * d$ dimensional matrix , where $d =$ number of feature(column)\n\n  $ S_{ij} = Co-variance(A_{i} , A_{j}) $\n  \n  where $S_{ij} =$ $element$ $of$ $i^{th}$ $row$ $and$ $j^{th}$ $column$ \n  \n  $ \\hspace{1cm} A_{i} = i^{th} column $\n  \n  $ \\hspace{1cm} A_{j} = j^{th} column $\n  \n  $ \\hspace{1cm} Co-variance(X , Y) = (1/n) * \\sum_{i=0}^{i=n} (x_{i} - \\mu_{x}) * (y_{i} - \\mu_{y})$\n  \n  $ \\hspace{1cm} Co-variance(X , X) = variance(X)$\n  \n  So diagonal of co-variance matrix(S) will be variance and S will be symmetric as well.\n  \n  After Standardization mean $(\\mu)$ of column becomes $0$ so,\n  \n  $ \\hspace{1cm} Co-variance(X , Y) = (1/n) * \\sum_{i=0}^{i=n} ( x_{i} * y_{i} )$ and this can be written as,\n  \n  $ \\hspace{1cm} Co-variance(X , Y) = (1/n) * (X^{T} \\cdot Y)$ \n  \n  and we can proove that, After Standardization $S = 1/(n-1) * (X^T \\cdot X)$\n  \n  In above equation we divide $S$ by $(n-1)$ cause we do not have whole data(so mean of whole data is unknown)\n  \n  here's [link](https://en.wikipedia.org/wiki/Covariance#Calculating_the_sample_covariance) for when to use $n$ and when to use $n-1$\n","metadata":{}},{"cell_type":"markdown","source":"# **1. Principal Component Analysis(PCA)**","metadata":{}},{"cell_type":"markdown","source":"Geometric intuition of PCA :\n\n* Let's say we have 2 dimensional graph and we want to convert it into 1 dimensional graph.\n\n* e.g. if we have a graph like below, then we'll use f2 as a main feature and get rid of f1 because f2 has a more spread so we'll loose very small amount of information. \n\n![](https://i.ibb.co/6wsvx8D/dr.png)\n\n* Let's look at another complicated example, In below graph, spread in f1 and f2 direction are same, so In such case we'll rotate f1 and f2 and select f1' as a pricipal direction\n\n![](https://i.ibb.co/4mYY9B2/dr2.png)\n\n* To conclude , we want to find direction f1' such that the variance of $x_i$'s (data point) projected onto f1' is maximized\n\nMathematical Objective function of PCA (variance maximization):\n\n* Let's call $u_1$ = f1' and ||$u_1$|| = 1 (cause $u_1$ is just a direction)\n\n* $x_i$' = $proj_{u_1} x_i$ now we know that,\n\n  $x_i$' = $u_1 \\cdot x_i$ / ||$u_1$|| = $u_1^T * x_i$\n  \n  $\\bar{x_i}' = u_1^T * \\bar{x_i}$   where $\\bar{x_i} =$ mean of $x_i$\n  \n* So our objective is to find $u_1$ such that $variance(Proj_{u_1} x_i)_{i=1}^{n}$ is maximized.\n\n  $var(u_1^T * x_i)_{i=1}^n$ = $(1/n) * \\sum_{i=1}^n (u_1^T * x_i - u_1^T * \\bar{x_i})^2$\n  \n  But we know that after standardization $\\bar{x_i} = 0$ so,\n  \n  $var(u_1^T * x_i)_{i=1}^n$ = $(1/n) * \\sum_{i=1}^n (u_1^T * x_i)^2$\n  \n* Let's rewrite our objective more formally,\n\n  Objective = $\\underset{u_1}{max}$ $(1/n) * \\sum_{i=1}^n (u_1^T * x_i)^2$ such that ||$u_1$|| = 1 ......................(1)\n  \n  Objective = $\\underset{u_1}{max}$ $(1/n) * \\sum_{i=1}^n (u_1^T * x_i) * (x_i^T * u_1)$\n  \n  Objective = $\\underset{u_1}{max}$ $ u_1^T * [ (1/n) * \\sum_{i=1}^n (x_i * x_i^T) ] * u_1$\n  \n  Objective = $\\underset{u_1}{max}$ $ u_1^T * [ (1/n) * (X^T \\cdot X) ] * u_1$\n  \n  Objective = $\\underset{u_1}{max}$ $ u_1^T * S * u_1$ ........................(2)\n  \n   where $S$ = Covariance matrix\n   \n  we can solve eq. (2) using lagrangian multiplier and get the $u_1$.\n   \nAnother formulation for PCA (distance minimization) :\n\n![](https://i.ibb.co/12NnCMk/dr3.png)\n\n* In above image red line is a distance of data point ($x_i$) to line $u_1$,\n\n* so in this case our aim is to find $u_1$ such that sum of distance of data point is minimized. In more formally,\n\n  $ Aim = \\underset{u_1}{min}$ $d_i^2 $ \n  \n  where $d_i$ = distance of $x_i$ to $u_1$\n  \n![](https://i.ibb.co/pQ85sFs/dr4.png)\n\n  By looking at above picture we can say that,\n\n  $d_i^2 = (||x_i||)^2 - (proj_{u_1}x_i)^2$\n  \n  $d_i^2 = x_i^T \\cdot x_i - (u_1^T \\cdot x_i)^2 $\n  \n  $ Aim = \\underset{u_1}{min}$ $ (x_i^T \\cdot x_i - (u_1^T \\cdot x_i)^2)$ ........................(3)\n  \n  By looking at both equation (1) and (3) we can say both are same after doing some maths.\n  \n* Solution of our main objective(aim) is,\n\n  Let's first define some of the terminology, \n  \n  Eigen values of covariance matrix ($S$) are : $\\lambda_1 , \\lambda_2 , ... \\lambda_d$\n\n  Eigen vector correspond to eigen values are : $v_1 , v_2 , ... v_d$\n  \n  and also assume that : $\\lambda_1 \\geq \\lambda_2 \\geq ... \\geq \\lambda_d $\n  \n  So Our $u_1$ = $v_1$ = eigen vector correspond to maximum eigen values\n  \n  We can get this by solving eq. (1) or (3). You can check out this [video](https://www.youtube.com/watch?v=MUENAuYkgmI) for proof.\n\n  By using ($\\lambda_1 , \\lambda_2 ... \\lambda_i $) we can preserve = ($ \\lambda_1 + \\lambda_2 + ... + \\lambda_i $)*100 /($\\sum_{j=1}^d \\lambda_j$)\n  percent information.\n  \n  \nNow Let's Use all this info. to do dimensionality reduction :\n\n* Let's say our data is $X$ of $(n*d)$ dimension and we want to convert this data set into $(n*2)$ dimension,\n\n  First we'll calculate covariance matrix $(S)$ that is equal to $X^T \\cdot X$\n  \n  Second step is to calculate eigen values of $S$ and also find corresponding eigen vector $(v_1,v_2 ...v_d)$\n  \n  Then our transformed data $X$' will be $(n*2)$ dimensional and first column of $X$ will be $X \\cdot v_1$ and second column of $X$ will be $X \\cdot v_2$.","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\n# for ploting\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# for linear algebra\nfrom scipy import linalg\n\n# for PCA and scaling\nfrom sklearn import decomposition\nfrom sklearn import preprocessing","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-08-23T16:26:22.330222Z","iopub.execute_input":"2021-08-23T16:26:22.330608Z","iopub.status.idle":"2021-08-23T16:26:22.335535Z","shell.execute_reply.started":"2021-08-23T16:26:22.330572Z","shell.execute_reply":"2021-08-23T16:26:22.334559Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = pd.read_csv('/kaggle/input/digit-recognizer/train.csv')\ndata.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-23T16:26:22.336776Z","iopub.execute_input":"2021-08-23T16:26:22.337302Z","iopub.status.idle":"2021-08-23T16:26:25.471233Z","shell.execute_reply.started":"2021-08-23T16:26:22.337256Z","shell.execute_reply":"2021-08-23T16:26:25.470359Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"label = data.label\ndata = data.drop('label' , axis = 1)","metadata":{"execution":{"iopub.status.busy":"2021-08-23T16:26:25.472631Z","iopub.execute_input":"2021-08-23T16:26:25.473033Z","iopub.status.idle":"2021-08-23T16:26:25.769488Z","shell.execute_reply.started":"2021-08-23T16:26:25.472997Z","shell.execute_reply":"2021-08-23T16:26:25.768559Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"one_image = data.loc[3,:]\nplt.figure(figsize = (7,7))\none = np.array(one_image).reshape(28,28)\nplt.imshow(one , cmap = 'gray')","metadata":{"execution":{"iopub.status.busy":"2021-08-23T16:26:25.770737Z","iopub.execute_input":"2021-08-23T16:26:25.771167Z","iopub.status.idle":"2021-08-23T16:26:25.952587Z","shell.execute_reply.started":"2021-08-23T16:26:25.771116Z","shell.execute_reply":"2021-08-23T16:26:25.951543Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's do code\n# Preprocessing\n# we can do standardization using scikit-learn as well\n\nscaler = preprocessing.StandardScaler()\nscaler.fit(data)\ns_data = scaler.transform(data)","metadata":{"execution":{"iopub.status.busy":"2021-08-23T16:26:25.954022Z","iopub.execute_input":"2021-08-23T16:26:25.954346Z","iopub.status.idle":"2021-08-23T16:26:27.934563Z","shell.execute_reply.started":"2021-08-23T16:26:25.954316Z","shell.execute_reply":"2021-08-23T16:26:27.933585Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# standardization\nfor i in range(784):\n    s = f\"pixel{i}\"\n    if np.std(data[s]) != 0:\n        data[s] = (data[s] - np.mean(data[s])) / np.std(data[s])","metadata":{"execution":{"iopub.status.busy":"2021-08-23T16:26:27.935983Z","iopub.execute_input":"2021-08-23T16:26:27.93643Z","iopub.status.idle":"2021-08-23T16:27:07.456831Z","shell.execute_reply.started":"2021-08-23T16:26:27.936387Z","shell.execute_reply":"2021-08-23T16:27:07.455711Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = s_data\n\n# create covariance matrix\nS = np.matmul(data.T , data)\n\n# compute eigen vector\n# this function return eigen values and vectors in ascending order\n# through eigvals we're selecting 2 eigen vector with heighest eigen values\nvalues , vectors = linalg.eigh(S , eigvals = (782 , 783))\n\nprint(vectors.shape)\n\n# so we need to convert vector to transpose\nvectors = vectors.T","metadata":{"execution":{"iopub.status.busy":"2021-08-23T16:27:07.458172Z","iopub.execute_input":"2021-08-23T16:27:07.458505Z","iopub.status.idle":"2021-08-23T16:27:08.093427Z","shell.execute_reply.started":"2021-08-23T16:27:07.458474Z","shell.execute_reply":"2021-08-23T16:27:08.092389Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# now let's project original data point on vectors\nnew_data = np.matmul(vectors , data.T)\nprint(new_data.shape)","metadata":{"execution":{"iopub.status.busy":"2021-08-23T16:27:08.096097Z","iopub.execute_input":"2021-08-23T16:27:08.096557Z","iopub.status.idle":"2021-08-23T16:27:08.148671Z","shell.execute_reply.started":"2021-08-23T16:27:08.096513Z","shell.execute_reply":"2021-08-23T16:27:08.145702Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_data = np.vstack((new_data , label)).T\nreduced_data = pd.DataFrame(\n    data = new_data,\n    columns = ['1st_principle' , '2nd_principle' , 'label']\n)\nreduced_data.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-23T16:27:08.150814Z","iopub.execute_input":"2021-08-23T16:27:08.151306Z","iopub.status.idle":"2021-08-23T16:27:08.177094Z","shell.execute_reply.started":"2021-08-23T16:27:08.151259Z","shell.execute_reply":"2021-08-23T16:27:08.175747Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# now let's plot data\nsns.FacetGrid(reduced_data , hue = 'label' , height = 7).map(plt.scatter , '1st_principle' , '2nd_principle').add_legend()","metadata":{"execution":{"iopub.status.busy":"2021-08-23T16:27:08.17887Z","iopub.execute_input":"2021-08-23T16:27:08.179337Z","iopub.status.idle":"2021-08-23T16:27:09.321742Z","shell.execute_reply.started":"2021-08-23T16:27:08.179291Z","shell.execute_reply":"2021-08-23T16:27:09.321025Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Now let's use scikit learn for PCA\npca = decomposition.PCA()\n\npca.n_components = 2\npca_data = pca.fit_transform(s_data)\n\npca_data = np.vstack((pca_data.T , label)).T\n\npca_df = pd.DataFrame(\n    data = pca_data,\n    columns = ['1st_principle' , '2nd_principle' , 'label']\n)\n\npca_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-23T16:27:09.322704Z","iopub.execute_input":"2021-08-23T16:27:09.323147Z","iopub.status.idle":"2021-08-23T16:27:11.743652Z","shell.execute_reply.started":"2021-08-23T16:27:09.323105Z","shell.execute_reply":"2021-08-23T16:27:11.742636Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.FacetGrid(pca_df , hue = 'label' , height = 7).map(plt.scatter , '1st_principle' , '2nd_principle').add_legend()","metadata":{"execution":{"iopub.status.busy":"2021-08-23T16:27:11.744902Z","iopub.execute_input":"2021-08-23T16:27:11.745253Z","iopub.status.idle":"2021-08-23T16:27:12.873544Z","shell.execute_reply.started":"2021-08-23T16:27:11.745221Z","shell.execute_reply":"2021-08-23T16:27:12.872429Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Limitations of PCA :\n\n![](https://i.ibb.co/w6DvrzS/dr5.png)\n\n* In Graph 1 we can see that cluster 1 and cluster 2 will be projected on the same place so we'll not able distinguish data specifically.\n\n* In Graph 2 we can see that data is spreaded in every direction equally in this case if we do dimensionality reduction then we'll loose lots of data.\n\n* there are many cases in which PCA won't work or work very badly.","metadata":{}}]}