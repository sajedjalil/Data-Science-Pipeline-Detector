{"cells":[{"metadata":{},"cell_type":"markdown","source":"\n\n\n# Convolutional Neural Networks .\n\n<center><img src = \"https://adeshpande3.github.io/assets/Cover.png\"><img></center>\n\nIn this notebook, I'm willing to use a Convolutional Neural Network to solve this problem.\nConvolutional Neural Networks are very powerful at detecting features, like edges, parts of objets, \nand even complete objects. The more deep the network is, the more the performance will increase, but\nyou may face the problem of vanishing and exploding gradients.\n\nVanishing and exploding gradients. When you go deeper and deeper by stacking layers, the network learns intricate functions. Although this type of model building might be benign, and aids us in increasing the accuracy, it fails to learn identity functions. Theoretically, machine learning practitioners have established that as the number of layers increase, the accuracy increases. Empirically, it has been shown that this statement is far from the truth. In fact, as the layers kept increasing, the gain in accuracy was diminishing. The culprit, here, is the gradients. Succumbing to the depth of the layers, the gradients either vanished, i.e. became too small for the update to make some worthwhile progress, or exploded, i.e., became too big for the update to overshoot the minima.\n\n**Dataset description:**\n<div class=\"alert-warning\">\nThe data files train.csv and test.csv contain gray-scale images of hand-drawn digits, from zero        through nine. Each image is 28 pixels in height and 28 pixels in width, for a total of 784 pixels in total. Each pixel has a single pixel-value associated with it, indicating the lightness or darkness of that pixel, with higher numbers meaning darker. This pixel-value is an integer between 0 and 255, inclusive.\n    \nThe training data set, (train.csv), has 785 columns. The first column, called \"label\", is the digit that was drawn by the user. The rest of the columns contain the pixel-values of the associated image.    \nThe test data set, (test.csv), is the same as the training set, except that it does not contain the \"label\" column.        \n</div><br><br>    \n\nDeep Learning applied to computer vision now is exploding. \n<center><img src=\"https://encrypted-tbn0.gstatic.com/images?q=tbn%3AANd9GcRoqxX1AkrwR2-P5u31R4OtlWFcfKG2dJ9kXQ&usqp=CAU\"><img></center>"},{"metadata":{},"cell_type":"markdown","source":"Shall we start now? Ok, let's first load some useful packages.\nHere are some definitions of them not all and refrences for these packages.\n\n   - [**Numpy**](https://numpy.org/) : is a library for the Python programming language, adding support for large, multi-dimensional arrays and matrices.\n   - [**Pandas**](https://pandas.pydata.org/) :  is a software library written for the Python programming language for data manipulation and analysis. \n   - [**Matplotlib**](https://matplotlib.org/) : Python library for plotting graphs, that is data visualization.\n   - [**Pyplot**](https://matplotlib.org/api/pyplot_api.html) : is a Matplotlib module which provides a MATLAB-like interface.\n   - [**TensorFlow**](https://www.tensorflow.org/) : Deep Learning framework created by Google Brain.\n   - [**Keras**](https://keras.io/) : is an open-source neural-network library written in Python. It is capable of running on top of TensorFlow\n   \nIf you want to learn more about CNNs, there a great material [here](https://www.youtube.com/watch?v=ArPaAX_PhIs&list=PLkDaE6sCZn6Gl29AoE31iwdVwSG-KnDzF).   "},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"from matplotlib import pyplot as plt \nimport os \nimport scipy\nimport numpy as np\nimport pandas as pd\nimport IPython\nimport tensorflow as tf\nimport keras \nimport seaborn as sns\nimport warnings as w\nimport sklearn.metrics as Metric_tools\nfrom sklearn.model_selection import train_test_split\nimport cv2\n\n%load_ext autoreload\n%autoreload 2\n\nnp.random.seed(1)\nw.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Loading the dataset .\n\nLet's now load the two csv files under the input folder. `test.csv` & `train.csv`"},{"metadata":{"trusted":true},"cell_type":"code","source":"main_path = r\"../input/digit-recognizer\"\nprint(\"Files  : \\n\\t {} \".format(os.listdir(main_path)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_file = pd.read_csv(os.path.join(main_path, \"train.csv\"))\ntest_file  = pd.read_csv(os.path.join(main_path, \"test.csv\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Training file : \")\ntrain_file.head(3).iloc[:,:17]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Testing file : \")\ntest_file.head(3).iloc[:,:17]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In the train file, the first column is the label (`0..9, output`).\nYou can change .iloc[:, `this`] to see more columns. Each column has a max value of 255 and a min value of 0. That is each pixel has values between 0..255. \n\n<center>\n<img src=\"https://seis.bristol.ac.uk/~ggjlb/teaching/ccrs_tutorial/tutorial/graphics/content/pixel.gif\"></img></center>"},{"metadata":{},"cell_type":"markdown","source":"Let's now see the description of both these two files.\nDescritption is a method with dataframes, it allows us to see the statistical values of a dataset.\nLike for example, the mean, the std (`standard deviation`), the max and min values etc,..."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Description of the training : \")\ndisc_train = train_file.describe().T\ndisc_train.iloc[1:10, :]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Description of the testing : \")\ndisc_test = test_file.describe().T\ndisc_test.iloc[:10, :]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's visualize the mean of each pixel in a bar plot. \n\n**Before Scaling :**\n   - The content in the following plots represents the data before scaling, you can see that the features vary. The first features have 0 in the mean value, that's because all the values in the first features are zeros (`Black background`)."},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax_arr = plt.subplots(1, 2, figsize=(14, 4))\nfig.subplots_adjust(wspace=0.25, hspace=0.025)\n\nax_arr = ax_arr.ravel()\n\nsets = iter([(disc_train, \"training\"), (disc_test, \"testing\")])\nfor i, ax in enumerate(ax_arr):\n    set_ = next(sets)\n    ax.plot(set_[0].loc[:, \"mean\"], label=\"Mean\")\n    ax.set_title(\"Mean of the {} features.\".format(set_[1]))\n    ax.set_xlabel('Pixels')\n    ax.set_ylabel('Mean')\n    ax.set_xticks([0, 120, 250, 370, 480, 600, 720])\n    ax.legend(loc=\"upper left\", shadow=True, frameon=True, framealpha=0.9)\n    ax.set_ylim([0, 150])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Normalization :**\n   - Normalizing the data helps with converging to the global minima, instead of having a lot of local minima.\n   - Similarly, the goal of normalization is to change the values of numeric columns in the dataset to a common scale, without distorting differences in the ranges of values. For machine learning, every dataset does not require normalization. It is required only when features have different ranges.For example, consider a data set containing two features, age, and income(x2). Where age ranges from 0–100, while income ranges from 0–100,000 and higher. Income is about 1,000 times larger than age. So, these two features are in very different ranges. When we do further analysis, like multivariate linear regression, for example, the attributed income will intrinsically influence the result more due to its larger value. But this doesn’t necessarily mean it is more important as a predictor. So we normalize the data to bring all the variables to the same range."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_file_norm = train_file.iloc[:, 1:] / 255.0\ntest_file_norm = test_file / 255.0","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Describing the normalized dataset again.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"disc_train = train_file_norm.describe().T\ndisc_test = test_file_norm.describe().T","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Plotting the mean to see what's the difference.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax_arr = plt.subplots(1, 2, figsize=(14, 4))\nfig.subplots_adjust(wspace=0.25, hspace=0.025)\n\nax_arr = ax_arr.ravel()\n\nsets = iter([(disc_train, \"training\"), (disc_test, \"testing\")])\nfor i, ax in enumerate(ax_arr):\n    set_ = next(sets)\n    ax.plot(set_[0].loc[:, \"mean\"], label=\"Mean\")\n    ax.set_title(\"Mean of the {} features.\".format(set_[1]))\n    ax.set_xlabel('Pixels')\n    ax.set_ylabel('Mean')\n    ax.set_xticks([0, 120, 250, 370, 480, 600, 720])\n    ax.legend(loc=\"upper left\", shadow=True, frameon=True, framealpha=0.9)\n    ax.set_ylim([0, 150])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<div class=\"alert-warning\" style=\"background-color:lightblue ; color:black; padding:5px; border-radius:2px\">\nAs you see above, the mean of all the features is close to zero, that means all of the features have\na similar mean. This will help increasing the performance of course.\n</div>"},{"metadata":{},"cell_type":"markdown","source":"### Displaying some examples .\n\nAfter doing this important preprocessing step, we're going to display 64 randomly chosen examples in \na nice grid."},{"metadata":{"trusted":true},"cell_type":"code","source":"rand_indices = np.random.choice(train_file_norm.shape[0], 64, replace=False)\nexamples = train_file_norm.iloc[rand_indices, :]\n\nfig, ax_arr = plt.subplots(8, 8, figsize=(6, 5))\nfig.subplots_adjust(wspace=.025, hspace=.025)\n\nax_arr = ax_arr.ravel()\nfor i, ax in enumerate(ax_arr):\n    ax.imshow(examples.iloc[i, :].values.reshape(28, 28), cmap=\"gray\")\n    ax.axis(\"off\")\n    \nplt.show()    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Let's now see how the values in the output target are distributed.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10, 5))\nplt.hist(train_file.iloc[:, 0], bins=10, edgecolor=\"black\", facecolor=\"lightblue\")\nplt.xlabel('Number in the output.')\nplt.ylabel('Frequency.')\nplt.title('Distribution of numbers.')\nplt.xticks([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\nplt.xlim([0, 9])\npass","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Preparing the inputs : \n\nWe're going to prepare the input images, and put them in the correct shape.\nThe shapes should be (`num_examples`, $n_h, n_w, n_c$).\n\n$n_c$ = Number of channels (1 Gray-scale).\n\n$n_h$ = Height of images.\n\n$n_w$ = Width of images."},{"metadata":{"trusted":true},"cell_type":"code","source":"num_examples_train = train_file.shape[0]\nnum_examples_test = test_file.shape[0]\nn_h = 32\nn_w = 32\nn_c = 3","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Train_input_images = np.zeros((num_examples_train, n_h, n_w, n_c))\nTest_input_images = np.zeros((num_examples_test, n_h, n_w, n_c))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for example in range(num_examples_train):\n    Train_input_images[example,:28,:28,0] = train_file.iloc[example, 1:].values.reshape(28,28)\n    Train_input_images[example,:28,:28,1] = train_file.iloc[example, 1:].values.reshape(28,28)\n    Train_input_images[example,:28,:28,2] = train_file.iloc[example, 1:].values.reshape(28,28)\n    \nfor example in range(num_examples_test):\n    Test_input_images[example,:28,:28,0] = test_file.iloc[example, :].values.reshape(28,28)\n    Test_input_images[example,:28,:28,1] = test_file.iloc[example, :].values.reshape(28,28)\n    Test_input_images[example,:28,:28,2] = test_file.iloc[example, :].values.reshape(28,28)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for example in range(num_examples_train):\n    Train_input_images[example] = cv2.resize(Train_input_images[example], (n_h, n_w))\n    \nfor example in range(num_examples_test):\n    Test_input_images[example] = cv2.resize(Test_input_images[example], (n_h, n_w))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Train_labels = np.array(train_file.iloc[:, 0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Shape of train input images : \", Train_input_images.shape)\nprint(\"Shape of test input images : \", Test_input_images.shape)\nprint(\"Shape of train labels : \", Train_labels.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Data augmentation : \n\nImage data augmentation is a technique that can be used to artificially expand the size of a training dataset by creating modified versions of images in the dataset.\n\nTraining deep learning neural network models on more data can result in more skillful models, and the augmentation techniques can create variations of the images that can improve the ability of the fit models to generalize what they have learned to new images.\n\nThe Keras deep learning neural network library provides the capability to fit models using image data augmentation via the ImageDataGenerator class.\n\n<center><img src=\"https://nanonets.com/blog/content/images/2018/11/1_dJNlEc7yf93K4pjRJL55PA--1-.png\"><img></center>"},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.preprocessing.image import ImageDataGenerator\n\ntrain_datagen = ImageDataGenerator(\n    rotation_range=27,\n    width_shift_range=0.3,\n    height_shift_range=0.2,\n    shear_range=0.3,\n    zoom_range=0.2,\n    horizontal_flip=False)\n\nvalidation_datagen = ImageDataGenerator()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Building the structure of a CNN .\n\nIn deep learning, a convolutional neural network (CNN, or ConvNet) is a class of deep neural networks, most commonly applied to analyzing visual imagery. They are also known as shift invariant or space invariant artificial neural networks (SIANN), based on their shared-weights architecture and translation invariance characteristics. They have applications in image and video recognition, recommender systems, image classification, medical image analysis, natural language processing, and financial time series."},{"metadata":{"trusted":true},"cell_type":"code","source":"pretrained_model = keras.applications.resnet50.ResNet50(input_shape=(n_h, n_w, n_c),\n                                                        include_top=False, weights='imagenet')\n\nmodel = keras.Sequential([\n    pretrained_model,\n    keras.layers.Flatten(),\n    keras.layers.Dense(units=60, activation='relu'),\n    keras.layers.Dense(units=10, activation='softmax')\n])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Compile the model."},{"metadata":{"trusted":true},"cell_type":"code","source":"Optimizer = 'RMSprop'\n\nmodel.compile(optimizer=Optimizer, \n                loss='sparse_categorical_crossentropy',\n                metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Adding the development set."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_images, dev_images, train_labels, dev_labels = train_test_split(Train_input_images, \n                                                                      Train_labels,\n                                                                      test_size=0.1, train_size=0.9,\n                                                                      shuffle=True,\n                                                                      random_state=44)\ntest_images = Test_input_images","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class myCallback(keras.callbacks.Callback):\n    def on_epoch_end(self, epoch, logs={}):\n        if (logs.get('accuracy') > 0.999999):\n            print(\"Stop training!\")\n            self.model.stop_training = True","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"callbacks = myCallback()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"EPOCHS = 5\nbatch_size = 212\n\nhistory = model.fit_generator(train_datagen.flow(train_images,train_labels, batch_size=batch_size),\n                         steps_per_epoch=train_images.shape[0] / batch_size, \n                         epochs=EPOCHS,   \n                         validation_data=validation_datagen.flow(dev_images,dev_labels,\n                                                                 batch_size=batch_size),\n                         validation_steps=dev_images.shape[0] / batch_size,\n                         callbacks=[callbacks])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.style.use('ggplot')  \n \nacc = history.history['accuracy']\nval_acc = history.history['val_accuracy']  \nloss = history.history['loss'] \nval_loss = history.history['val_loss'] \n\nepochs = range(len(acc))\n\nfig, ax = plt.subplots(1, 2, figsize=(15, 5))\nfig.subplots_adjust(wspace=0.15, hspace=0.025)\nax = ax.ravel()\n\nax[0].plot(epochs, acc, 'r', label='Training accuracy')\nax[0].plot(epochs, val_acc, 'b', label='Validation accuracy')\nax[0].set_title('Training and validation accuracy')\nax[0].legend(loc=\"upper left\", shadow=True, frameon=True, fancybox=True, framealpha=0.9)\n\nax[1].plot(epochs, loss, 'r', label='Training Loss')\nax[1].plot(epochs, val_loss, 'b', label='Validation Loss')\nax[1].set_title('Training and validation loss')\nax[1].legend(loc=\"upper right\", shadow=True, frameon=True, fancybox=True, framealpha=0.9)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##  Submitting the prediction.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.read_csv('../input/digit-recognizer-submission/submission.csv')\nsubmission.to_csv('digit_submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Conclusion : \n\n### ConvNets history : \n\nSince the 1950s, the early days of artificial intelligence, computer scientists have been trying to build computers that can make sense of visual data. In the ensuing decades, the field, which has become known as computer vision, saw incremental advances. In 2012, computer vision took a quantum leap when a group of researchers from the University of Toronto developed an AI model that surpassed the best image recognition algorithms by a large margin.\n\nThe AI system, which became known as AlexNet (named after its main creator, Alex Krizhevsky), won the 2012 ImageNet computer vision contest with an amazing 85 percent accuracy. The runner-up scored a modest 74 percent on the test.\n\nAt the heart of the AlexNet was a convolutional neural network (CNN), a specialized type of artificial neural network that roughly mimics the human vision system. In recent years, CNNs have become pivotal to many computer vision applications. Here’s what you need to know about the history and workings of CNNs.\n<center>\n<img src=\"https://encrypted-tbn0.gstatic.com/images?q=tbn%3AANd9GcTc9CsPOtPBGGTm8zz-mIKtxBkGEHllr3VkEA&usqp=CAU\"><img></center>\n\nConvolutional neural networks, also called ConvNets, were first introduced in the 1980s by Yann LeCun, a postdoctoral computer science researcher. LeCun had built on the work done by Kunihiko Fukushima, a Japanese scientist who, a few years earlier, had invented the neocognitron, a very basic image recognition neural network.\n\n"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}