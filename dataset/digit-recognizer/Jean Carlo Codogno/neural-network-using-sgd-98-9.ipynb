{"metadata":{"language_info":{"version":"3.6.1","nbconvert_exporter":"python","name":"python","file_extension":".py","mimetype":"text/x-python","pygments_lexer":"ipython3","codemirror_mode":{"version":3,"name":"ipython"}},"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"}},"cells":[{"source":"import numpy as np\nimport pandas as pd\nimport sys\n\nimport matplotlib.pyplot as plt\nnp.random.seed(1070)","metadata":{"_cell_guid":"86990557-b034-c88c-bc3e-0e0bb666606b","_uuid":"7bcae0a7c40933c487ffa53fd08aabc8071b20ce","_active":false,"_execution_state":"idle","trusted":false},"cell_type":"code","execution_state":"idle","execution_count":null,"outputs":[]},{"source":"df = pd.read_csv(\"../input/train.csv\")\ndata = df.as_matrix()\nnp.random.shuffle(data)\ndata_y = data[:,0].astype('float32')\ndata_x = data[:,1:].astype('float32')","metadata":{"_active":false,"_cell_guid":"48c61204-4638-a851-06da-2e07208e7b35","_uuid":"6c7855986fa68686cc3ef7317881a7d4b023ca0c","collapsed":false,"_execution_state":"idle","trusted":false},"cell_type":"code","execution_state":"idle","execution_count":null,"outputs":[]},{"cell_type":"code","outputs":[],"source":"data_y = pd.get_dummies(data_y).as_matrix()\n\nmeio = 255/2\ndata_x = (data_x-meio)/meio","execution_count":null,"metadata":{"_uuid":"9d98b46d9d70692be12f54ca56901fda9c2b4d38","collapsed":false,"_execution_state":"idle"}},{"cell_type":"code","metadata":{"_cell_guid":"3e8a4950-709d-42da-87c0-44c66304a482","_execution_state":"idle","collapsed":false,"_uuid":"ac332dc6b38ba3479ec0f4ad18bfb4cdba8d67e6","trusted":false},"source":"VALID_SIZE = round(data_x.shape[0]*0.15)\n\n\nx_train = data_x[VALID_SIZE:]\nx_valid = data_x[:VALID_SIZE]\n\nd_train = data_y[VALID_SIZE:]\nd_valid = data_y[:VALID_SIZE]\n\ndata_x = None\ndata_y = None\n\nx_train.shape\n","outputs":[],"execution_count":7},{"source":"def initializationWeights():\n    ##Initialization of the Weights and the Biases with the random gaussian function with mean zeron, and variance between 1/sqtr(num_inputs_layer)\n    \n    ninputs = 784\n    wl1 = 128\n    wl2 = 64\n    nclass = 10\n    \n    mean = 0\n    \n    #layer1\n    variance = 1.0/np.sqrt(ninputs)\n    w1 = np.random.normal(mean, variance, [ninputs,wl1])\n    b1 = np.random.normal(mean, variance, [1,wl1])\n    dw1 = np.zeros([ninputs,wl1])\n    db1 = np.zeros([1,wl1])\n    \n    #Layer2\n    variance = 1.0/np.sqrt(wl1)\n    w2 = np.random.normal(mean, variance, [wl1,wl2])\n    b2 = np.random.normal(mean, variance, [1,wl2])\n    dw2 = np.zeros([wl1,wl2])\n    db2 = np.zeros([1,wl2])\n\n    #Layer3\n    variance = 1.0/np.sqrt(wl2)\n    w3 = np.random.normal(mean, variance, [wl2,nclass])\n    b3 = np.random.normal(mean, variance, [1,nclass])\n    dw3 = np.zeros([wl2,nclass])\n    db3 = np.zeros([1,nclass])\n    \n    return w1,w2,w3,b1,b2,b3,dw1,dw2,dw3,db1,db2,db3","metadata":{"_cell_guid":"8b6b4180-318c-ce22-54e6-570055cb7777","_uuid":"f76da0094629441448dfbd0db7a14888b564f0ed","_active":false,"_execution_state":"idle","trusted":false},"cell_type":"code","execution_state":"idle","execution_count":null,"outputs":[]},{"source":"##Activation Function's and Cross-entropy Function\n\ndef ReLu(x, derivative=False):\n    if(derivative==False):\n        return x*(x > 0)\n    else:\n        return 1*(x > 0)\n    \ndef LReLu(x, derivative=False):\n    if(derivative==False):\n        return x*(x > 0) + 0.1*x*(x<0)\n    else:\n        return 1*(x > 0) + 0.1*(x<0)\n\ndef sigmoid(x, derivative=False):\n    if(derivative==False):\n        return 1/(1+np.exp(-x))\n    else:\n        return x*(1-x)\n       \ndef softmax(x):\n        if(x.ndim==1 or x.ndim==0):\n            e_x = np.exp(x - np.max(x))\n            return e_x/e_x.sum(axis=0)\n        else:\n            k = 0\n            x3 = np.empty((0,0))\n            for x2 in x:\n                if(k==0):\n                    x3 = np.array([softmax(x2)])\n                    k=1\n                else:\n                    x4 = softmax(x2)\n                    x3 = np.concatenate((x3,[x4]), axis=0)\n            return np.array(x3)\n\ndef cost(Y_predict, Y_right):\n    Loss = -np.mean(Y_right*np.nan_to_num(np.log(Y_predict)),keepdims=True)\n    return Loss","metadata":{"_cell_guid":"7f2c05ac-da5c-2513-fde6-27d85ffc3956","_uuid":"23dd420a9a47d17072c6724b869dbf6b05527c26","_active":false,"_execution_state":"idle","trusted":false},"cell_type":"code","execution_state":"idle","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"_cell_guid":"eca5943a-a56b-4b79-8572-9ae9906c28f3","_execution_state":"idle","collapsed":false,"_uuid":"3e6e26acabfad2028a19c585a79b9dc053112c5a","trusted":false},"source":"def accuracy(output, y):\n    hit = 0\n    output = np.argmax(output, axis=1)\n    y = np.argmax(y, axis=1)\n    for y in zip(output, y):\n        if(y[0]==y[1]):\n            hit += 1\n\n    p = (hit*100)/output.shape[0]\n    return p","outputs":[],"execution_count":10},{"source":"def run(x_train, y_train, x_valid, y_valid, epochs = 10, nbatchs=25, alpha = 1e-3, decay = 0, momentum = 0, l2 = 0.001, DROPOUT = 0):\n  \n    pross = x_train.shape[0]*0.05\n    w1,w2,w3,b1,b2,b3,dw1,dw2,dw3,db1,db2,db3  = initializationWeights()\n    index = np.arange(x_train.shape[0])\n    \n    print(\"Train data: %d\" % (x_train.shape[0]))\n    print(\"Validation data: %d \\n\" % (x_valid.shape[0]))\n    \n    for j in range(epochs):\n        np.random.shuffle(index)\n        t = 0\n        iterations = round(x_train.shape[0]/nbatchs)\n        prog = \"\"\n        sacurr = 0\n        sloss = 0\n        sys.stdout.write(\"\\nEpochs: %2d \\ %2d \\n\"% (j+1,epochs))\n        for i in range(iterations):\n         \n            f = i*nbatchs\n            l = f+nbatchs\n            \n            if(l>(x_train.shape[0]-1)):\n                l = x_train.shape[0]\n                \n            x = x_train[index[f:l]]\n            y = y_train[index[f:l]]\n\n            #1-Hidden Layer\n       \n            first = ReLu(x.dot(w1)+b1)\n            if(DROPOUT!=1):\n                first *= np.random.binomial([np.ones_like(first)],1-DROPOUT)[0]  /(1-DROPOUT)\n            #2-Hidden Layer\n            second = ReLu(first.dot(w2)+b2)\n            if(DROPOUT!=1):\n                second *= np.random.binomial([np.ones_like(second)],1-DROPOUT)[0] / (1-DROPOUT)\n            #Output Layer\n            output = softmax(second.dot(w3)+b3)\n         \n            loss = cost(output, y)\n            \n            error = y-output\n            \n            accuracy_t = accuracy(output, y)\n            \n            sacurr += accuracy_t\n            sloss += loss\n            \n            accuracy_train = sacurr/(i+1)\n            loss_train = sloss/(i+1)\n            \n            w3_delta = error\n            \n            w2_error = w3_delta.dot(w3.T)\n            w2_delta = w2_error * ReLu(second,derivative=True)\n            \n            w1_error = w2_delta.dot(w2.T)\n            w1_delta = w1_error * ReLu(first,derivative=True)\n            \n            mew3 = np.mean(w3)\n            meb3 = np.mean(b3)\n            mew2 = np.mean(w2)\n            meb2 = np.mean(b2)\n            mew1 = np.mean(w1)\n            meb1 = np.mean(b1)\n        \n            w3 += alpha * (momentum*w3 + second.T.dot(w3_delta)) - l2 * mew3\n            b3 += alpha * (momentum*b3 + second.T.dot(w3_delta).sum(axis=0)) - l2 * meb3\n            w2 += alpha * (momentum*w2 + first.T.dot(w2_delta)) - l2 * mew2\n            b2 += alpha * (momentum*b2 + first.T.dot(w2_delta).sum(axis=0)) - l2 * meb2\n            w1 += alpha * (momentum*w1 + x.T.dot(w1_delta)) - l2 * mew1\n            b1 += alpha * (momentum*b1 + x.T.dot(w1_delta).sum(axis=0)) - l2 * meb1\n            \n            \n            \n            t+= x.shape[0]\n            \n            qtd = round(t/pross)\n            prog = \"[\"\n            for p in range(20):\n                if(p<qtd-1):\n                    prog += \"-\"\n                elif(p==qtd-1):\n                    prog += \">\"\n                else:\n                    prog += \" \"\n            prog += \"]\"\n\n           \n            sys.stdout.write(\"\\r%5d : %5d %s Train Acc.: %.4f - Train Loss: %.4f\\n\" % (x_train.shape[0],t, prog, accuracy_train, loss_train))\n        \n        alpha = alpha - (alpha*decay)\n        #1-Hidden Layer\n        first = ReLu(x_valid.dot(w1)+b1)\n        #2-Hidden Layer\n        second = ReLu(first.dot(w2)+b2)\n        #Output Layer\n        output = softmax(second.dot(w3)+b3)\n        \n        loss_valid = cost(output, y_valid)\n        accuracy_valid = accuracy(output, y_valid)\n        \n        sys.stdout.write(\"\\r%5d : %5d %s  Train Acc: %.4f - Train Loss: %.4f - Valid Acc: %.4f - Valid Loss: %.4f\\n\" % ( x_train.shape[0],t, prog, accuracy_train, loss_train, accuracy_valid, loss_valid))","metadata":{"_cell_guid":"c421dda0-0e37-9d62-8a8d-7660befa4bdd","_uuid":"be8bea97cddcadbc2ee018101373ff2937943a88","_active":false,"_execution_state":"idle","trusted":false},"cell_type":"code","execution_state":"idle","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"_cell_guid":"8935ff94-fd2a-4e19-9d4e-c31a47fda938","_execution_state":"busy","collapsed":false,"_uuid":"5cb3502552ff369cb0870625d4c11cd5282bfc7a","trusted":false},"source":"alpha = 1e-3\nepochs = 10\nrun(x_train, d_train, x_valid, d_valid, epochs = epochs, nbatchs=25, alpha = alpha, decay = alpha/epochs, momentum = 1e-9, l2 = 0.01, DROPOUT = 0.25)","outputs":[],"execution_count":null},{"cell_type":"code","metadata":{"_cell_guid":"1d4f8fc7-1ae7-4e7a-b91f-a48ba27f5ea5","_execution_state":"idle","collapsed":false,"_uuid":"cad2c3ae5c3e9cc397c3d86704efbb17409996e4","trusted":false},"source":"","outputs":[],"execution_count":null}],"nbformat_minor":0,"nbformat":4}