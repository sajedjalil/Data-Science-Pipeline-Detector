{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## INTRODUCTION\n- Itâ€™s a Python based scientific computing package targeted at two sets of audiences:\n    - A replacement for NumPy to use the power of GPUs\n    - Deep learning research platform that provides maximum flexibility and speed\n- pros: \n    - Interactively debugging PyTorch. Many users who have used both frameworks would argue that makes pytorch significantly easier to debug and visualize.\n    - Clean support for dynamic graphs\n    - Organizational backing from Facebook\n    - Blend of high level and low level APIs\n- cons:\n    - Much less mature than alternatives\n    - Limited references / resources outside of the official documentation\n- I accept you know neural network basics. If you do not know, you should learn the basics first of all.  Because I will not explain neural network concepts detailed, I only explain how to use pytorch for neural network\n<br>\n<br> **Content:**\n1. [Logistic Regression](#1)\n1. [Import necessary Libraries](#2)\n1. [Prepare Dataset](#3)\n1. [Create Logistic Regression Model](#4)\n1. [Traning the Model](#5)\n1. [Visualization&Conclusion](#6)","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-10-15T12:30:29.012147Z","iopub.execute_input":"2021-10-15T12:30:29.012632Z","iopub.status.idle":"2021-10-15T12:30:29.027747Z","shell.execute_reply.started":"2021-10-15T12:30:29.01258Z","shell.execute_reply":"2021-10-15T12:30:29.026437Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"1\"></a> <br>\n### Logistic Regression\n- Linear regression is not good at classification.\n- We use logistic regression for classification.\n- linear regression + logistic function(softmax) = logistic regression\n\n- **Steps of Logistic Regression**\n    1. Import Libraries\n    1. Prepare Dataset\n        - We use MNIST dataset.\n        - There are 28*28 images and 10 labels from 0 to 9\n        - Data is not normalized so we divide each image to 255 that is basic normalization for images.\n        - In order to split data, we use train_test_split method from sklearn library\n        - Size of train data is 80% and size of test data is 20%.\n        - Create feature and target tensors. At the next parts we create variable from these tensors. As you remember we need to define variable for accumulation of gradients.\n        - batch_size = batch size means is that for example we have data and it includes 1000 sample. We can train 1000 sample in a same time or we can divide it 10 groups which include 100 sample and train 10 groups in order. Batch size is the group size. For example, I choose batch_size = 100, that means in order to train all data only once we have 336 groups. We train each groups(336) that have batch_size(quota) 100. Finally we train 33600 sample one time.\n        - epoch: 1 epoch means training all samples one time.\n        - In our example: we have 33600 sample to train and we decide our batch_size is 100. Also we decide epoch is 29(accuracy achieves almost highest value when epoch is 29). Data is trained 29 times. Question is that how many iteration do I need? Lets calculate: \n            - training data 1 times = training 33600 sample (because data includes 33600 sample) \n            - But we split our data 336 groups(group_size = batch_size = 100) our data \n            - Therefore, 1 epoch(training data only once) takes 336 iteration\n            - We have 29 epoch, so total iterarion is 9744(that is almost 10000 which I used)\n        - TensorDataset(): Data set wrapping tensors. Each sample is retrieved by indexing tensors along the first dimension.\n        - DataLoader(): It combines dataset and sample. It also provides multi process iterators over the dataset.\n        - Visualize one of the images in dataset\n    1. Create Logistic Regression Model\n        - Same with linear regression.\n        - However as you expect, there should be logistic function in model right?\n        - In pytorch, logistic function is in the loss function where we will use at next parts.\n    1. Instantiate Model\n        - input_dim = 28*28 # size of image px*px\n        - output_dim = 10  # labels 0,1,2,3,4,5,6,7,8,9\n        - create model\n    1. Instantiate Loss \n        - Cross entropy loss\n        - It calculates loss that is not surprise :)\n        - It also has softmax(logistic function) in it.\n    1. Instantiate Optimizer \n        - SGD Optimizer\n    1. Traning the Model\n    1. Prediction\n- As a result, as you can see from plot, while loss decreasing, accuracy(almost 85%) is increasing and our model is learning(training). ","metadata":{}},{"cell_type":"markdown","source":"<a id=\"2\"></a> <br>\n### Import Necessary Libraries\n","metadata":{}},{"cell_type":"code","source":"# Import Libraries\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\nfrom torch.utils.data import DataLoader\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split","metadata":{"execution":{"iopub.status.busy":"2021-10-15T12:30:36.652699Z","iopub.execute_input":"2021-10-15T12:30:36.653071Z","iopub.status.idle":"2021-10-15T12:30:38.765224Z","shell.execute_reply.started":"2021-10-15T12:30:36.653043Z","shell.execute_reply":"2021-10-15T12:30:38.764314Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"3\"></a> <br>\n### Prepare Dataset","metadata":{}},{"cell_type":"code","source":"# Prepare Dataset\n# load data\ntrain = pd.read_csv(r\"/kaggle/input/digit-recognizer/train.csv\",dtype = np.float32)\n\n# split data into features(pixels) and labels(numbers from 0 to 9)\ntargets_numpy = train.label.values\nfeatures_numpy = train.loc[:,train.columns != \"label\"].values/255 # normalization\n\n# train test split. Size of train data is 80% and size of test data is 20%. \nfeatures_train, features_test, targets_train, targets_test = train_test_split(features_numpy,\n                                                                             targets_numpy,\n                                                                             test_size = 0.2,\n                                                                             random_state = 42) \n\n# create feature and targets tensor for train set. As you remember we need variable to accumulate gradients. Therefore first we create tensor, then we will create variable\nfeaturesTrain = torch.from_numpy(features_train)\ntargetsTrain = torch.from_numpy(targets_train).type(torch.LongTensor) # data type is long\n\n# create feature and targets tensor for test set.\nfeaturesTest = torch.from_numpy(features_test)\ntargetsTest = torch.from_numpy(targets_test).type(torch.LongTensor) # data type is long\n\n# batch_size, epoch and iteration\nbatch_size = 100\nn_iters = 10000\nnum_epochs = n_iters / (len(features_train) / batch_size)\nnum_epochs = int(num_epochs)\n\n# Pytorch train and test sets\ntrain = torch.utils.data.TensorDataset(featuresTrain,targetsTrain)\ntest = torch.utils.data.TensorDataset(featuresTest,targetsTest)\n\n# data loader\ntrain_loader = DataLoader(train, batch_size = batch_size, shuffle = False)\ntest_loader = DataLoader(test, batch_size = batch_size, shuffle = False)\n\n# visualize one of the images in data set\nplt.imshow(features_numpy[10].reshape(28,28))\nplt.axis(\"off\")\nplt.title(str(targets_numpy[10]))\nplt.savefig('graph.png')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-10-15T12:32:22.513782Z","iopub.execute_input":"2021-10-15T12:32:22.514258Z","iopub.status.idle":"2021-10-15T12:32:26.443957Z","shell.execute_reply.started":"2021-10-15T12:32:22.514215Z","shell.execute_reply":"2021-10-15T12:32:26.443089Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"4\"></a> <br>\n### Create Logistic Regression Model","metadata":{}},{"cell_type":"code","source":"# Create Logistic Regression Model\nclass LogisticRegressionModel(nn.Module):\n    def __init__(self, input_dim, output_dim):\n        super(LogisticRegressionModel, self).__init__()\n        # Linear part\n        self.linear = nn.Linear(input_dim, output_dim)\n        # There should be logistic function right?\n        # However logistic function in pytorch is in loss function\n        # So actually we do not forget to put it, it is only at next parts\n    \n    def forward(self, x):\n        out = self.linear(x)\n        return out\n\n# Instantiate Model Class\ninput_dim = 28*28 # size of image px*px\noutput_dim = 10  # labels 0,1,2,3,4,5,6,7,8,9\n\n# create logistic regression model\nmodel = LogisticRegressionModel(input_dim, output_dim)\n\n# Cross Entropy Loss  \nerror = nn.CrossEntropyLoss()\n\n# SGD Optimizer \nlearning_rate = 0.001\noptimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)","metadata":{"execution":{"iopub.status.busy":"2021-10-15T12:32:42.154428Z","iopub.execute_input":"2021-10-15T12:32:42.155076Z","iopub.status.idle":"2021-10-15T12:32:42.177706Z","shell.execute_reply.started":"2021-10-15T12:32:42.155037Z","shell.execute_reply":"2021-10-15T12:32:42.17635Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"5\"></a> <br>\n### Traning the Model","metadata":{}},{"cell_type":"code","source":"# Traning the Model\ncount = 0\nloss_list = []\niteration_list = []\nfor epoch in range(num_epochs):\n    for i, (images, labels) in enumerate(train_loader):\n        \n        # Define variables\n        train = Variable(images.view(-1, 28*28))\n        labels = Variable(labels)\n        \n        # Clear gradients\n        optimizer.zero_grad()\n        \n        # Forward propagation\n        outputs = model(train)\n        \n        # Calculate softmax and cross entropy loss\n        loss = error(outputs, labels)\n        \n        # Calculate gradients\n        loss.backward()\n        \n        # Update parameters\n        optimizer.step()\n        \n        count += 1\n        \n        # Prediction\n        if count % 50 == 0:\n            # Calculate Accuracy         \n            correct = 0\n            total = 0\n            # Predict test dataset\n            for images, labels in test_loader: \n                test = Variable(images.view(-1, 28*28))\n                \n                # Forward propagation\n                outputs = model(test)\n                \n                # Get predictions from the maximum value\n                predicted = torch.max(outputs.data, 1)[1]\n                \n                # Total number of labels\n                total += len(labels)\n                \n                # Total correct predictions\n                correct += (predicted == labels).sum()\n            \n            accuracy = 100 * correct / float(total)\n            \n            # store loss and iteration\n            loss_list.append(loss.data)\n            iteration_list.append(count)\n        if count % 500 == 0:\n            # Print Loss\n            print('Iteration: {}  Loss: {}  Accuracy: {}%'.format(count, loss.data, accuracy))","metadata":{"execution":{"iopub.status.busy":"2021-10-15T12:32:51.341191Z","iopub.execute_input":"2021-10-15T12:32:51.341476Z","iopub.status.idle":"2021-10-15T12:33:16.94364Z","shell.execute_reply.started":"2021-10-15T12:32:51.341445Z","shell.execute_reply":"2021-10-15T12:33:16.94272Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"6\"></a> <br>\n### Visualization&Conclusion","metadata":{}},{"cell_type":"code","source":"# visualization\nplt.plot(iteration_list,loss_list)\nplt.xlabel(\"Number of iteration\")\nplt.ylabel(\"Loss\")\nplt.title(\"Logistic Regression: Loss vs Number of iteration\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-10-15T12:33:25.567126Z","iopub.execute_input":"2021-10-15T12:33:25.567705Z","iopub.status.idle":"2021-10-15T12:33:25.766081Z","shell.execute_reply.started":"2021-10-15T12:33:25.567666Z","shell.execute_reply":"2021-10-15T12:33:25.765091Z"},"trusted":true},"execution_count":null,"outputs":[]}]}