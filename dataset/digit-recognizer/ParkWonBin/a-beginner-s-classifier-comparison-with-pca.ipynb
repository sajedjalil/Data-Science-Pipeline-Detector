{"cells":[{"metadata":{},"cell_type":"markdown","source":"# A Beginner's Classifier Comparison & PCA\n\n\n1. import data set  \n    1.1.split data set  \n\n2. Definition  \n    2.1. Define custom functions  \n    2.2. Define simple classifiers  \n\n3. Report  \n    3.1. Confusion matrix  \n    3.2. ROC curve  \n\n4. PCA  \n    4,1 Define custom function  \n    4.2 Plot PCA reduced picture  \n    4.3 Train with PCA reduced data  \n    4.4 Compare Ordinal and PCA   \n\n5. Save Output  ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# 1. import data set","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\ntrain_set = pd.read_csv(\"../input/digit-recognizer/train.csv\")\n\nX = train_set.drop('label', axis=1)\ny = train_set['label']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 1.1.split data set\n\nSplit the train set into two.  \ntrain set for learning and a validation set for improving accuracy.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\n\n# Split the train set into two.\nnum = int(len(X)*(3/5))\nX_train, X_valid = X[:num],X[num:]\ny_train, y_valid = y[:num],y[num:]\n\nprint(\"X_train :\",len(X_train))\nprint(\"y_train :\",len(y_train))\nprint(\"X_valid :\",len(X_valid))\nprint(\"y_valid :\",len(y_valid))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Calculation takes too long, reduce size of data set","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calculation takes too long, reduce size of data set\n# X_train, X_valid = X_train[:500], X_valid[:200]\n# y_train, y_valid = y_train[:500], y_valid[:200]\n\nprint(\"X_train :\",len(X_train))\nprint(\"y_train :\",len(y_train))\nprint(\"X_valid :\",len(X_valid))\nprint(\"y_valid :\",len(y_valid))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2. Definition\n## 2.1. Define custom functions","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import time\nimport pandas as pd\nfrom tqdm.notebook   import tqdm\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\nfrom sklearn.model_selection import cross_val_predict\n\ndef train(*models,dataset=(X_train,y_train,X_valid, y_valid)):\n    columns = [\"Name\", \"Time(sec)\",\"accuracy(%)\", \"precision(%)\",\"recall(%)\",\"f1-score\",\"confusion\" ,\"model\"]\n    df = pd.DataFrame(columns=columns)\n    \n    X_train,y_train,X_valid,y_valid = dataset\n\n    for model in tqdm(models) :\n        model_name = str(model.__class__.__name__)\n        print(model_name, end=\"...\")\n        \n        # Time measurement\n        start = time.time()\n        \n        # Trainning start\n        model.fit(X_train,y_train)\n        \n        # report\n        y_pred     = cross_val_predict(model, X_valid, y_valid, cv=3)     \n        clf_report = classification_report(y_valid,y_pred, output_dict =True)\n        \n        accuracy   = clf_report[\"accuracy\"]                # accuracy\n        precision  = clf_report['macro avg']['precision']  # precision\n        recall     = clf_report['macro avg']['recall']     # recall\n        f1_score   = clf_report['macro avg']['f1-score']\n        confusion  = confusion_matrix(y_valid, y_pred)     # confusion_matrix\n        \n        accuracy,precision,recall = [round(100*x,2) for x in [accuracy,precision,recall]]\n        \n        train_time = round(time.time() - start,2)\n\n        # save data\n        new_row = {f\"{columns[0]}\":model_name, # name\n                   f\"{columns[1]}\":train_time, # training time\n                   f\"{columns[2]}\":accuracy,   # accuracy\n                   f\"{columns[3]}\":precision,  # precision\n                   f\"{columns[4]}\":recall,     # recall \n                   f\"{columns[5]}\":f1_score,   # f1_score \n                   f\"{columns[6]}\":confusion,  # confusion_matrix \n                   f\"{columns[7]}\":model       # clf model\n                  }\n        \n        df = df.append(new_row,ignore_index=True)    \n        df = df.drop_duplicates([\"Name\"],keep=\"last\")\n        print(\"complite..!\")\n    return df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2.2. Define simple classifiers","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble     import ExtraTreesClassifier\nfrom sklearn.ensemble     import RandomForestClassifier\nfrom sklearn.tree         import DecisionTreeClassifier\nfrom sklearn.naive_bayes  import GaussianNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.svm          import SVC\nfrom sklearn.neighbors    import KNeighborsClassifier\n\n# Random Seed\nrandom_state = 20142927\n\n# Definition of Classifiers\next_clf = ExtraTreesClassifier(n_estimators=20,random_state=random_state)\ndet_clf = det_clf = DecisionTreeClassifier(splitter=\"random\",criterion='entropy',random_state=random_state) # splitter=\"random\" 빠름\nrdf_clf = RandomForestClassifier(n_estimators=15, random_state=random_state)\nknn_clf = KNeighborsClassifier(n_neighbors=20,leaf_size=50)\ngnb_clf = GaussianNB()\nlog_clf = LogisticRegression()\nsgd_clf = SGDClassifier(random_state=random_state)\nsvc_clf = SVC() \n\n\n# train and save classifiers\nclf_data = train( \n    ext_clf, \n    det_clf, \n    rdf_clf, \n    knn_clf\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble     import VotingClassifier\nfrom sklearn.ensemble     import BaggingClassifier\nfrom sklearn.ensemble     import AdaBoostClassifier\nfrom sklearn.ensemble     import GradientBoostingRegressor\n\nbag_clf = BaggingClassifier(\n    ExtraTreesClassifier(n_estimators=20,random_state=random_state),\n    n_jobs=-1,\n    n_estimators=5,\n    random_state=random_state\n)\n\nada_clf = AdaBoostClassifier(\n    ExtraTreesClassifier(n_estimators=20,random_state=random_state), \n    n_estimators=50,\n    learning_rate=0.2, \n    algorithm=\"SAMME.R\", \n    random_state=random_state\n)\n\n\nvot_clf = VotingClassifier(\n    estimators= [        \n        (\"ext_clf\",ext_clf),\n        (\"rdf_clf\",rdf_clf),\n#         (\"knn_clf\",knn_clf), # Accurate, but takes long time\n#         (\"det_clf\",det_clf), # The accuracy is too low\n#         (\"svc_clf\",svm_clf), # The accuracy is too low\n#         (\"sgd_clf\",sgd_clf), # Takes too much time\n        (\"bag_clf\",bag_clf),\n        (\"ada_clf\",ada_clf)\n    ] , voting='soft'\n)\n\n\nclf_data = clf_data.append(\n     train(bag_clf, ada_clf, vot_clf),ignore_index=True\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf_data.iloc[:,[0,1,2,5,6]]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3. Report\n## 3.1. Confusion matrix\n\n| | Predicted NO | Predicted Yes|\n|:---:|:---:|:---:|\n|Actual No| TN | FP|\n|Actual Yes | FN |TP|\n\n\n\n$$ Accuracy = {{TP + TN} \\over {TP + TN + FP + FN}}$$\n\n$$Recall = {TP \\over {TP + FN}}$$\n\n$$Precision = {TP \\over {TP + FP}}$$\n\n$$F1 Score = \\frac{2}{\\frac{1}{Recall} +  \\frac{1}{Presision}}$$","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(len(clf_data)) : \n    print(clf_data[\"Name\"][i], end=\"\\t\")\n    print(clf_data[\"accuracy(%)\"][i], end=\"(%) \\n  f1-score : \")\n    print(clf_data[\"f1-score\"][i],)\n    print(clf_data[\"confusion\"][i])\n    print(\"\\n\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3.2. ROC curve\n\n[MNIST ROC](https://davidburn.github.io/notebooks/mnist-numbers/MNIST%20Handwrititten%20numbers/)\n- y_pred : prediction by classifier\n- y_prob : The probability that the data will fall into the classification\n- y_true : Check if the classifier made the correct answer \n- y_scores : Confidence of the n value when the correct answer is n\n\n[how to select rows](https://stackoverflow.com/questions/17071871/how-to-select-rows-from-a-dataframe-based-on-column-values)\n\n- .loc  : Select a subset of DataFarame as conditional statements\n- .iloc : Select value at specific position in DataFrame\n\n```python\nmodel_name = str(models[i].__class__.__name__) \nclf_data.loc[clf_data[\"Name\"] == model_name]\nROC_dataset.loc[ROC_dataset[\"Name\"] == model_name]\n```\n[merging-dataframes](http://hleecaster.com/python-pandas-merging-and-concatenating-multiple-dataframes/)\n```python\npd.merge(clf_data, ROC_dataset, how='outer')\n```","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import roc_curve, auc\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\ndef show_auc(y_true,y_score):\n    fpr, tpr, _ = roc_curve(y_true, y_score)\n    roc_auc = auc(fpr, tpr)\n    plt.plot(fpr, tpr, label='ROC curve (area = %0.2f)' % roc_auc)\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')    \n    plt.legend(loc=\"lower right\")\n    \n    \ndef Pretreatment_ROC(model, X, y):\n    y_prob = model.predict_proba(X)\n    y_pred = cross_val_predict(model, X, y)\n    y_true = np.array(y == y_pred)\n    y_score = [y_prob[i][y.iloc[i]] for i in range(len(y_prob))]\n                        \n    return y_true, y_score\n\n    \n\ndef ROC_data(*models) :\n    columns = [\"Name\",\"test_time\",\"y_true\",\"y_score\"]\n    df = pd.DataFrame()\n    y_true, y_score =[], []\n  \n    for i in tqdm(range(len(models)))  :\n        model = models[i]\n        model_name = str(model.__class__.__name__) \n#         print(model_name, end=\"...\")\n\n       # Time measurement\n        start = time.time()\n        \n        y_true, y_score = Pretreatment_ROC(model,X_valid,y_valid)\n        \n        test_time = round(time.time() - start,2)\n\n        # data save\n        new_row = {f\"{columns[0]}\":model_name,\n                   f\"{columns[1]}\":test_time,  \n                   f\"{columns[2]}\":y_true,  \n                   f\"{columns[3]}\":y_score,  \n                  }\n        \n        df = df.append(new_row,ignore_index=True)\n        df = df.drop_duplicates([\"Name\"],keep=\"last\")\n#         print(\"complite..!\")\n    return df\n\n\ndef add_data(model,dataset):\n    clf_data = train(model)\n    ROC_dataset = ROC_data(model)\n    new_row = pd.merge(clf_data, ROC_dataset, how='outer')\n    return dataset.append(new_row)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"models = [    \n    ext_clf, \n    det_clf, \n    rdf_clf, \n    knn_clf, \n    bag_clf, \n    ada_clf, \n    vot_clf]\n\n\n# Train and store data in 'dataset'\n# clf_data = train(*models)\nROC_dataset = ROC_data(*models)\ndataset = pd.merge(clf_data, ROC_dataset, how='outer')\n\ndataset.iloc[:,[0,2,8,9,10]] ","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,10))   \nfor i in tqdm(range(len(models)))  :\n  \n    # Positioning ==================\n    col=2\n    row = len(models)//col + 1\n    plt.subplot(row,col,i+1)\n    \n    # plot ROC curve ===============\n    y_true  = dataset.iloc[i,10]\n    y_score = dataset.iloc[i,9]\n    show_auc(y_true, y_score)\n    \n    # Get data ====================\n    clf_name = dataset.iloc[i,0]\n    accuracy = dataset.iloc[i,2]\n    \n    # Display Name and Accuracy =====\n    tp = (0.2, 0.7) # Text Position\n    text = \"Accuracy : \"+ str(accuracy)+\"%\"\n    plt.text(tp[0],tp[1], clf_name, fontsize=15)\n    plt.text(tp[0],tp[1]-0.1, text, fontsize=13)\n    \nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 4. PCA\n## 4.1. Define custom function","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\nimport random\n\nclass MNIST:        \n    def __init__(self, X, y,pca):\n        self.pca = pca\n        self.X = X\n        self.y = y\n        self.ordinal = self.X\n        self.reduced = self.pca.fit_transform(self.ordinal)\n        self.recovered = self.pca.inverse_transform(self.reduced)\n        \n    def show(self,digit=None):\n        if not digit : index = random.randint(0,len(self.X))\n        else : index = int(digit)\n        \n        # Image preprocessing        \n        image_ord = np.array(self.ordinal.iloc[index]).reshape(28, 28)\n        image_rcd = np.array(self.recovered[index]).reshape(28, 28)\n        \n        # Plot Image\n        plt.figure(figsize=(7, 4))\n        pos = 121\n        for img in [image_ord, image_rcd] :\n            plt.subplot(pos)\n            plt.title(f\"y = {self.y[index]}\",fontsize = 15)\n            plt.imshow(img, cmap = matplotlib.cm.binary,interpolation=\"nearest\")\n            plt.axis(\"off\"); pos += 1    \n        plt.tight_layout()\n        print(self.pca)\n        \n        \npca = PCA(n_components=0.8,whiten=True)\npca_train= MNIST(X_train,y_train,pca)\npca_valid = MNIST(X_valid,y_valid,pca)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4.2. Plot PCA reduced picture","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"pca_train.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4.3. Train with PCA reduced data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"models = [    \n    ext_clf, \n    det_clf, \n    rdf_clf, \n    knn_clf, \n    bag_clf, \n    ada_clf, \n    vot_clf,\n]\n\n\nprint(\"train ordinal data\")\nclf_ord = train(*models)\n\nprint(\"train redused data\")\nclf_pca = train(*models, dataset=(\n    pca_train.reduced, pca_train.y,\n    pca_valid.reduced,  pca_valid.y\n))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf_ord.iloc[:,[5]] =  round(100* clf_ord.iloc[:,[5]],2)\nclf_ord_2 = clf_ord.iloc[:,[1,2,3,4,5]]\nclf_ord_2.index = clf_ord[\"Name\"]\nclf_ord_2.columns = [\"time\",\"accuracy\",\"percision\",\"recall\",\"f1-score\"]\n\nclf_pca.iloc[:,[5]] =  round(100* clf_pca.iloc[:,[5]],2)\nclf_pca_2 = clf_pca.iloc[:,[1,2,3,4,5]]\nclf_pca_2.index = clf_ord[\"Name\"]\nclf_pca_2.columns = [\"time\",\"accuracy\",\"percision\",\"recall\",\"f1-score\"]\n\nclf_ord_2,clf_pca_2","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4.4. Compare Ordinal and PCA ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"label= [x.replace('Classifier', '') for x in clf_pca_2.index]\n\nindex = np.arange(len(label))\n\n# plot Graph  ==================\nfor key in clf_pca_2.keys():\n    plt.figure(figsize=(15,4))   \n    w = 0.4\n    plt.title(key, fontsize=15)\n\n    plt.bar(index-w/2, clf_ord_2[key], width=w, label = \"ord\")\n    plt.bar(index+w/2, clf_pca_2[key], width=w, label = \"pca\")\n    \n    # Axis setting\n    y = [*clf_ord_2[key], *clf_pca_2[key]]\n    plt.axis([-w, len(label)-1+w, min(y)*0.98, max(y)+3])\n      \n    # display Value\n    for i in index:\n        fs = 12\n        dx = w*5/6\n        dy = 0.8\n        if key == \"time\" : d =\"s\";\n        else : d = \"%\"\n        plt.text(i-dx, clf_ord_2[key][i]+dy, str(clf_ord_2[key][i])+d,fontsize=fs)\n        plt.text(i-dx+w, clf_pca_2[key][i]+dy, str(clf_pca_2[key][i])+d,fontsize=fs)\n\n    # Displayed x-axis label\n    plt.xticks(index, label, fontsize=14)\n    plt.ylabel(key, fontsize=20)\n    plt.legend(loc=6,fontsize=15)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 5. Save Output","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv(\"../input/digit-recognizer/train.csv\")\ntest  = pd.read_csv(\"../input/digit-recognizer/test.csv\")\n\nX = train_set.drop('label', axis=1)\ny = train_set['label']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def nameof(obj): return [name for name in globals()  if  globals()[name] is obj][0]\n\n\nsave = False\nsave = True\n\nif save : \n    for i in range(len(models)) : \n        models[i].fit(X,y)\n        models[i].predict(test)\n        pd.DataFrame(models[i].predict(test)).to_csv(f\"{nameof(models[i])}.csv\")\nelse : \n    print(\"if you want to stroe result, set save to True\")","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}