{"cells":[{"metadata":{"_uuid":"4662cd27cca4127d6d7e8ac40d71cd29d8cd6f2d"},"cell_type":"markdown","source":"# **FastAI 1.0 with classic MNIST dataset **"},{"metadata":{"_uuid":"527a3be5219449a0bdcc21c08a20cb3fd5dc8682"},"cell_type":"markdown","source":"Kaggle provides MNIST dataset as csv file, that means standard fastai API won't work in this case, the three factory methods(for vision)  .from_folder, from_csv, from_df all asking a filename (fn) with label pair to get your databunch for training. \n\nOne solution we see in class is that you can write your own dense neural nets, but if you want to use CNN, or fastai Resnet layer functions, you will have to preprocess your data. Either use fastai provided called Lambda layer or preprocess your data as numpy array that has dimension (m, c, h, w) \n\nWhat if we want to use fastai API with transfer learning?  \n\nIn the following section, I would like to share how to customize a simple ItemList with datablock API so we can go back to fastai routine. "},{"metadata":{"_uuid":"a5bb6be8b41c19af6d1a356d4e50edbb97150931"},"cell_type":"markdown","source":"# Loading related library"},{"metadata":{"trusted":true,"_uuid":"43948420564f6cd5d13ac8b2f2ee2476730a4c5b"},"cell_type":"code","source":"# !conda install -c fastai fastai --yes #using latest 1.0.48 as 1.0.46 learner will have read-only issue","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7f5ab96048f641202178aecc8e117c5921716c8b"},"cell_type":"code","source":"%reload_ext autoreload\n%autoreload 2\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"from fastai import *\nfrom fastai.vision import *\nimport numpy as np\nimport pandas as pd\nimport re","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bfd6e650a61ed33fb4a24a470cf04a1377c64a11"},"cell_type":"markdown","source":"# Pre-Porcessing"},{"metadata":{"_uuid":"706c3cb6b6783d250a673271756cb47011bac75f"},"cell_type":"markdown","source":"The train.csv file contains **Label** column with pixel values. If we think it as **filename, label** pair, all we need is a filename for each of our data\n\nLet's create a filename for each of the training data, I used index of each example as filename. The idea here is besides label, we need to prepare each data with a name\n\nFor example, training set item 0, we name it 0. filename = 0, label =1"},{"metadata":{"trusted":true,"_uuid":"1ca08c7a82ef63f04a006b28c9eef7b3070eeec2"},"cell_type":"code","source":"df_train = pd.read_csv('../input/train.csv')\ndf_train['fn'] = df_train.index\ndf_train.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5039a0563d98308b91b361ee862863bee84070c0"},"cell_type":"markdown","source":"# Customized ItemList for pixel values"},{"metadata":{"_uuid":"377621acf747c68933ca43950f7130deb6a1fcae"},"cell_type":"markdown","source":"Since we already load the data to dataframe, we can use from_df() API to load the data. \nHowever, simply calling ImageItemList.from_df() won't work, we need to find a way to overide the default behavior : Instead of loading img files, it needs to load dataframe values, and make it RGB channels. \n\nFor example, from 784 to (28,28) to (3,28,28)"},{"metadata":{"_uuid":"5cf6a0156a6faa0d34c9d40c1bf1033a83629d04"},"cell_type":"markdown","source":"Custom ItemList provides a great way to override the standard behavior. You can check fastai doc tutorials for more [Docs](https://docs.fast.ai/tutorial.itemlist.html) \n\nMost importantly, since our custom ItemList will be similar to the ImageItemList, we only need to tell the library how to read our pixel value data.\n\nIt turns out that we only need to change the open() method, which get() will call to get data. \n\nSo fastai vision is following this path\n\n**from_csv() / from_folder() / from_df() --> eventually calls get() ---> eventually calls open()**\n\nThis is why in the custom ItemList tutorial get() is most important function to override. \n\n\n"},{"metadata":{"_uuid":"129c5368e3f83f473257db729a5cf662668f2777"},"cell_type":"markdown","source":"FInally,\nfrom_df() will pass **path/filename** to get image file, we need to properly open the img when fn is passed. \n\n1. get fn\n2. according to the fn, gets the pixel value from dataframe, this is internally saved in the self.xtra **(for 1.0.4x version, self.xtra renamed to self.inner_df)**\n3. reshape the img, stack the gray channel make it RBG\n4. fastai provides a API called **pil2tensor() ** which takes npdarry and return pytorch tensor\n5. return vision.Image() as Image class takes pixel"},{"metadata":{"trusted":true,"_uuid":"3a6ddc0b3e6c8b22ba7c594ec0ea15fb12a74146"},"cell_type":"code","source":"class PixelImageItemList(ImageList):\n    def open(self,fn):\n        regex = re.compile(r'\\d+')\n        fn = re.findall(regex,fn)\n        df = self.inner_df[self.inner_df.fn.values == int(fn[0])]\n        df_fn = df[df.fn.values == int(fn[0])]\n        img_pixel = df_fn.drop(labels=['label','fn'],axis=1).values\n        img_pixel = img_pixel.reshape(28,28)\n        img_pixel = np.stack((img_pixel,)*3,axis=-1)\n        return vision.Image(pil2tensor(img_pixel,np.float32).div_(255))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"56bc804292eb7f3f86a457a5a8972148f4ca641c"},"cell_type":"markdown","source":"# FastAI datablock API"},{"metadata":{"_uuid":"f5e84eb47f208cfa47a59bb1fbba7a780bc5efc7"},"cell_type":"markdown","source":"We are ready to go, now we can use standard fastai datablock API to create databunch\n\n1. get data, since we are calling from_df, we will pass dataframe with col that tags our data. The path passed is './fn' \n2. random split, 80-20 train - valid split\n3. get labels from df, where we pass col = 'label'.\n4. add transform (optional), we are only zero pad and random zoom in this case. calling get_transform() with flipping / lighting wont do much good for the 28*28 grey imgs(even though is RGB now)\n5. create databunch, and normalize data using pretrained model stats \n"},{"metadata":{"trusted":true,"_uuid":"c085c52969e43acb5998608471debbb0c0483c9e"},"cell_type":"code","source":"src = (PixelImageItemList.from_df(df_train,'.',cols='fn')\n      .split_by_rand_pct()\n      .label_from_df(cols='label'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0573e33d3641d033e6a0a5da45c63b9c9818fdc7"},"cell_type":"code","source":"data = (src.transform(tfms=(rand_pad(padding=5,size=28,mode='zeros'),[]))\n       .databunch(num_workers=2,bs=128)\n       .normalize(imagenet_stats))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9686c1721bc27ad3ff186e1d91398183c6eca075"},"cell_type":"markdown","source":"Lets take a look of our data, notice that the grey img now is turned to RGB(3 channels) with size (3,28,28). "},{"metadata":{"trusted":true,"_uuid":"e1450faac0f84454cd40f8f76d4f1579f2b7a37d"},"cell_type":"code","source":"data.show_batch(rows=3,figsize=(10,7))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7f7b8130b251d7e38c1f30d3c06eb0eda47e7365"},"cell_type":"markdown","source":"The label and data is correct, and we can see the data is randomly pad and zoomed \n\nwe can further test the shape of our data to make sure it is correct"},{"metadata":{"trusted":true,"_uuid":"f697c33d7cf53b7a9f57b07b3f28a9d54d39f3ea"},"cell_type":"code","source":"print(data.train_ds[0][1]) #label\ndata.train_ds[0][0] #img","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"10eaddb7536df35033ef5e4c92c8de5d7c737f85"},"cell_type":"code","source":"data.train_ds[0][0].data,data.train_ds[0][0].data.shape,data.train_ds[0][0].data.max()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2c50fe83d905e36a7b7031f343ced0deb907cf24"},"cell_type":"markdown","source":"Start training using standard fastai "},{"metadata":{"trusted":true,"_uuid":"248eb39ef36bfa8f6949726e0e9cf27bb251d1d3"},"cell_type":"code","source":"learn = cnn_learner(data,models.resnet50,metrics=accuracy,model_dir='/kaggle/model')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8d8107fba1420fbf2662774ab9920f8836a0fd05"},"cell_type":"code","source":"learn.lr_find(end_lr=100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4ba509546340e539e0e4b8c74a86893926b28024"},"cell_type":"code","source":"learn.recorder.plot()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1e27aec960ceb55dc8fc8b3a591faf9f6b3b1b07"},"cell_type":"code","source":"lr = 1e-2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6bcee6a79867ba3587847843757ca9cac103abd0"},"cell_type":"code","source":"learn.fit_one_cycle(5,slice(lr))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bec3df86c426569b8d90e37e5500d61740a1dc80"},"cell_type":"markdown","source":"Unfreeze the model and train a little bit more"},{"metadata":{"trusted":true,"_uuid":"439bff05b35f7ef0769a7a1b07f31377439e0ca6"},"cell_type":"code","source":"learn.unfreeze()\nlearn.lr_find()\nlearn.recorder.plot()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b2b5ad4af27219b711f8c9cb473d52ddb29a6705"},"cell_type":"code","source":"learn.fit_one_cycle(8,slice(2e-5,lr/5))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e003d8093daf7f3bf39c44f51e8ee53056c8d207"},"cell_type":"markdown","source":"**Updated: Using resnet 50 gives a boost of performance to 0.994 LB score**\n\n99.2 is not bad, consider we are only training with resnet 34 and train loss is still higher than valid loss, which means there is still room for you to train longer.\n\nAlso, we can simply load a resnet 50 with little bit more data argumentation, such as rotate image -10 to 10 degrees. (Which can be easily done by fastai framework)\n\nLet's take a look of some errors. "},{"metadata":{"trusted":true,"_uuid":"127c8c7d5cd291d66d5b4cb03bd880e41a6b1622"},"cell_type":"code","source":"interp = ClassificationInterpretation.from_learner(learn)\ninterp.plot_top_losses(9,figsize=(6,6))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"89f0da632393df7ecb85cfb05c9037df88fbb940"},"cell_type":"markdown","source":"From the top 9 losses, we can try the following\n\n1. remove some 'mislabeled' data? \n2. rotate the trainting set a bit should help \n3. we can train longer\n\nSo the model still got bit space to improve. "},{"metadata":{"trusted":true,"_uuid":"34d3bea6af1834f1d1317f5b14fb0f82d085a69d"},"cell_type":"code","source":"learn.show_results()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b2aa23df9a3959b4e5c6210bb88d4b523a599fd7"},"cell_type":"markdown","source":"# fin"},{"metadata":{"_uuid":"b626671db62af06226f8280f27abf419c09abf4a"},"cell_type":"markdown","source":"Pack the test set as what we did for the training set."},{"metadata":{"trusted":true,"_uuid":"9bdcfd9eee4437d79299deb9de562da8c7775e83"},"cell_type":"code","source":"df_test = pd.read_csv('../input/test.csv')\ndf_test['label'] = 0\ndf_test['fn'] = df_test.index\ndf_test.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"99e72b1827663e135bc864f80b8c1cbb742155bb"},"cell_type":"markdown","source":"It turns out that you dont need to override add_test() to work. \n\nThe default library takes ItemList to add as test set\n\nWe can simply create another PixelImageItemList and add it to the model."},{"metadata":{"trusted":true,"_uuid":"7d56b9d57d592868e0825262482f5a308e76b057"},"cell_type":"code","source":"learn.data.add_test(PixelImageItemList.from_df(df_test,path='.',cols='fn'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"74fb4c8935bed181ee00cefd209fc808af0edff0"},"cell_type":"code","source":"pred_test = learn.get_preds(ds_type=DatasetType.Test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7e214f6fed267e5e40d4a889a3716d719d79d20d"},"cell_type":"code","source":"test_result = torch.argmax(pred_test[0],dim=1)\nresult = test_result.numpy()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"94f1ecaf07473317be139b35c7ed8603a793906f"},"cell_type":"markdown","source":"Or you can take advantage of the fastai TTA (test time argumentation, which act like ensemble way of predicting. averages regular prediction and test time argumentated predication)\n\nHowever, in this model, we didn't apply transfroms to vaildation set, therefore we can skip this part and submit our predication base on no TTA"},{"metadata":{"trusted":true,"_uuid":"81c6bc29aba79b8e60a25aef336340f7e2fa0906"},"cell_type":"code","source":"# preds = learn.TTA(ds_type=DatasetType.Test)\n# pred = torch.argmax(preds[0],dim=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fa9e26013aae31e90153654563e06bccf77ccf9c"},"cell_type":"code","source":"final = pd.Series(result,name='Label')\nsubmission = pd.concat([pd.Series(range(1,28001),name='ImageId'),final],axis=1)\nsubmission.to_csv('fastai-res34-0.992.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"19d5ee05d52bbd83fb8f05db33e1e02fa3b05504"},"cell_type":"code","source":"submission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"30914404e9c27e47b7a4545d9eccd542f31174b4"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}