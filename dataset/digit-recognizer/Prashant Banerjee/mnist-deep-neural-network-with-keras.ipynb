{"cells":[{"metadata":{},"cell_type":"markdown","source":"<a class=\"anchor\" id=\"0\"></a>\n# MNIST - Deep Neural Network with Keras","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Hello friends,\n\n\nIn this notebook, I have built a deep neural network on MNIST handwritten digit images to classify them. \n\n\nMNIST is called Hello World of Deep Learning.\n\n\nSo, it is actually an image recognition task.\n\n\nIt helps me to understand and built a deep neural network with Keras. \n\n\nI hope that this notebook will help to understand the same.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"This kernel is based on the following Kaggle kernel:\n\n- [My previous kernel on Artificial Neural Network](https://www.kaggle.com/prashant111/comprehensive-guide-to-ann-with-keras)\n\n- [Deep Neural Network Keras way by Poonam Ligade](https://www.kaggle.com/poonaml/deep-neural-network-keras-way)\n\n- [MNIST with Keras for beginners by Aditya Soni](https://www.kaggle.com/adityaecdrid/mnist-with-keras-for-beginners-99457)\n\n\nI thank the authors (Poonam Ligade and Aditya Soni) for their excellent work.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**I hope you find this kernel useful and some <font color=\"red\"><b>UPVOTES</b></font> would be very much appreciated**.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<a class=\"anchor\" id=\"0.1\"></a>\n# Table of contents\n\n\n1.\t[Objective of the notebook](#1)\n1.\t[Import necessary libraries](#2)\n1.\t[MNIST dataset](#3)\n1.\t[Data visualization](#4)\n1.\t[Designing model architecture using Keras](#5)\n    - 5.1 [Import Keras layers](#5.1)\n    - 5.2 [Compute the number of labels](#5.2)\n    - 5.3 [One-Hot Encoding](#5.3)\n    - 5.4 [Data Preprocessing](#5.4)\n    - 5.5 [Setting network parameters](#5.5)\n    - 5.6 [Designing the model architecture](#5.6)\n    - 5.7 [View model summary](#5.7)\n1.\t[Implement MLP model using Keras](#6)\n    - 6.1 [Compile the model with compile() method](#6.1)\n    - 6.2 [Train the model with fit() method](#6.2)\n    - 6.3 [Evaluate model performance with evaluate() method](#6.3)\n1.\t[Overfitting and Regularization](#7)\n1.\t[Results and Conclusion](#8)\n1.\t[References](#9)\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# 1. Objective of the notebook <a class=\"anchor\" id=\"1\"></a>\n\n[Back to Table of Contents](#0.1)\n\n- In this notebook, we create a **Multilayer Perceptron (MLP)** model of the MNIST dataset. \n\n- **Multilayer Perceptrons (MLPs)** usually mean fully connected networks, that is, each neuron in one layer is connected to all neurons in the next layer. The \"fully-connectedness\" of these networks makes them prone to overfitting the data. \n\n- These MLP models are also referred to as either **deep feedforward networks** or **feedforward neural networks**. MLPs are common in simple logistic and linear regression problems.\n\n- So, the objective is to create a neural network for identifying numbers based on handwritten digits. For example, when the input to the network is an image of a handwritten number 8, the corresponding prediction must also be the digit 8. \n\n- To both train and validate a neural network, there must be a sufficiently large dataset of handwritten digits.\n\n- The Modified National Institute of Standards and Technology dataset or MNIST dataset for short, is often considered as the Hello World! of deep learning and is a suitable dataset for handwritten digit classification.\n\n- MNIST is used to explain and validate deep learning theories because the 70,000 samples it contains are small, yet sufficiently rich in information (MNIST dataset is described later).","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"So, we will start by importing the necessary libraries:","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# 2. Import necessary libraries <a class=\"anchor\" id=\"2\"></a>\n\n[Back to Table of Contents](#0.1)","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n\nimport matplotlib.pyplot as plt # plotting library\n%matplotlib inline\n\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense , Activation, Dropout\nfrom keras.optimizers import Adam ,RMSprop\nfrom keras import  backend as K\n\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n\n# Any results you write to the current directory are saved as output.\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3. MNIST dataset <a class=\"anchor\" id=\"3\"></a>\n\n[Back to Table of Contents](#0.1)\n\n\n- MNIST is a collection of handwritten digits ranging from the number 0 to 9. \n\n- It has a training set of 60,000 images, and 10,000 test images that are classified into corresponding categories or labels. \n\n- To use the MNIST dataset in Keras, an API is provided to download and extract images and labels automatically. \n\n- The following Keras code shows how to access MNIST dataset, plot 25 random samples, and count the number of labels for train and test datasets.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# import dataset\nfrom keras.datasets import mnist\n\n\n# load dataset\n(x_train, y_train),(x_test, y_test) = mnist.load_data()\n\n\n\n# count the number of unique train labels\nunique, counts = np.unique(y_train, return_counts=True)\nprint(\"Train labels: \", dict(zip(unique, counts)))\n\n\n\n# count the number of unique test labels\nunique, counts = np.unique(y_test, return_counts=True)\nprint(\"\\nTest labels: \", dict(zip(unique, counts)))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 4. Data visualization <a class=\"anchor\" id=\"4\"></a>\n\n[Back to Table of Contents](#0.1)\n\n\n- The following code will help to sample the 25 random MNIST digits and visualize them.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# sample 25 mnist digits from train dataset\nindexes = np.random.randint(0, x_train.shape[0], size=25)\nimages = x_train[indexes]\nlabels = y_train[indexes]\n\n\n# plot the 25 mnist digits\nplt.figure(figsize=(5,5))\nfor i in range(len(indexes)):\n    plt.subplot(5, 5, i + 1)\n    image = images[i]\n    plt.imshow(image, cmap='gray')\n    plt.axis('off')\n    \nplt.show()\nplt.savefig(\"mnist-samples.png\")\nplt.close('all')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 5. Designing model architecture using Keras <a class=\"anchor\" id=\"5\"></a>\n\n[Back to Table of Contents](#0.1)\n\n\n- The MLP model, discussed above can be used for MNIST digits classification. \n\n- When the units or perceptrons are exposed, the MLP model is a fully connected network.\n\n- The following code shows how to design the MLP model architecture using Keras.\n\n- The first step in designing the model architecture is to import the Keras layers. This can be done as follows:\n\n\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## 5.1 Import Keras layers <a class=\"anchor\" id=\"5.1\"></a>","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import Dense, Activation, Dropout\nfrom keras.utils import to_categorical, plot_model","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 5.2 Compute the number of labels <a class=\"anchor\" id=\"5.2\"></a>\n\n- Now, the data must be in the correct shape and format. \n\n- After loading the MNIST dataset, the number of labels is computed as:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# compute the number of labels\nnum_labels = len(np.unique(y_train))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 5.3 One-Hot Encoding <a class=\"anchor\" id=\"5.3\"></a>\n\n- At this point, the labels are in digits format, 0 to 9. \n\n- This sparse scalar representation of labels is not suitable for the neural network prediction layer that outputs probabilities per class. \n\n- A more suitable format is called a one-hot vector, a 10-dim vector with all elements 0, except for the index of the digit class. \n\n- For example, if the label is 2, the equivalent one-hot vector is [0,0,1,0,0,0,0,0,0,0]. The first label has index 0.\n\n- The following lines convert each label into a one-hot vector:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# convert to one-hot vector\ny_train = to_categorical(y_train)\ny_test = to_categorical(y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 5.4 Data Preprocessing <a class=\"anchor\" id=\"5.4\"></a>\n\n\n- In deep learning, data is stored in tensors. The term tensor applies to a scalar (0D tensor), vector (1D tensor), matrix (2D tensor), and a multi-dimensional tensor.\n\n- The rest code computes the image dimensions, input_size of the first Dense layer and scales each pixel value from 0 to 255 to range from 0.0 to 1.0. Although raw pixel values can be used directly, it is better to normalize the input data as to avoid large gradient values that could make training difficult. \n\n- The output of the network is also normalized. After training, there is an option to put everything back to the integer pixel values by multiplying the output tensor by 255.  \n\n- The proposed model is based on MLP layers. Therefore, the input is expected to be a 1D tensor. So, x_train and x_test are reshaped to [60000, 28 * 28] and [10000, 28 * 28], respectively.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# image dimensions (assumed square)\nimage_size = x_train.shape[1]\ninput_size = image_size * image_size\ninput_size","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# resize and normalize\nx_train = np.reshape(x_train, [-1, input_size])\nx_train = x_train.astype('float32') / 255\nx_test = np.reshape(x_test, [-1, input_size])\nx_test = x_test.astype('float32') / 255","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 5.5 Setting network parameters <a class=\"anchor\" id=\"5.5\"></a>\n\n- Now, we will set the network parameters as follows:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# network parameters\nbatch_size = 128\nhidden_units = 256\ndropout = 0.45","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- The **batch_size** argument indicates the number of data that we will use for each update of the model parameters.\n\n- **Hidden_units** shows the number of hidden units.\n\n- **Dropout** is the dropout rate (more on this in section 7 - **Overfitting and Regularization**).","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## 5.6 Designing the model architecture <a class=\"anchor\" id=\"5.6\"></a>\n\n\n- The next step is to design the model architecture. The proposed model is made of three MLP layers. \n\n- In Keras, an MLP layer is referred to as Dense, which stands for the densely connected layer. \n\n- Both the first and second MLP layers are identical in nature with 256 units each, followed by relu activation and dropout. \n\n- 256 units are chosen since 128, 512 and 1,024 units have lower performance metrics. At 128 units, the network converges quickly, but has a lower test accuracy. The added number units for 512 or 1,024 does not increase the test accuracy significantly.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"- The main data structure in Keras is the Sequential class, which allows the creation of a basic neural network.\n\n- The Sequential class of the Keras library is a wrapper for the sequential neural network model that Keras offers and can be created in the following way:\n\n`from keras.models import Sequential`\n\n`model = Sequential()`\n\n- The model in Keras is considered as a sequence of layers and each of them gradually “distills” the input data to obtain the desired output.\n\n- In Keras, we can add the required types of layers through the **add()** method.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# model is a 3-layer MLP with ReLU and dropout after each layer\nmodel = Sequential()\nmodel.add(Dense(hidden_units, input_dim=input_size))\nmodel.add(Activation('relu'))\nmodel.add(Dropout(dropout))\nmodel.add(Dense(hidden_units))\nmodel.add(Activation('relu'))\nmodel.add(Dropout(dropout))\nmodel.add(Dense(num_labels))\nmodel.add(Activation('softmax'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Since a Dense layer is a linear operation, a sequence of Dense layers can only approximate a linear function. \n\n- The problem is that the MNIST digit classification is inherently a non-linear process. Inserting a relu activation between Dense layers will enable MLPs to model non-linear mappings. \n\n- relu or Rectified Linear Unit (ReLU) is a simple non-linear function. It allows positive inputs to pass through unchanged while clamping everything else to zero.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## 5.7 View model summary <a class=\"anchor\" id=\"5.7\"></a>\n\n- Keras library provides us **summary()** method to check the model description.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"model.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- The above listing shows the model summary of the proposed network. It requires a total of 269,322 parameters.\n\n- This is substantial considering that we have a simple task of classifying MNIST digits. So, MLPs are not parameter efficient. \n\n- The total number of parameters required can be computed as follows:\n\n  - From input to Dense layer: 784 × 256 + 256 = 200,960. \n  \n  - From first Dense to second Dense: 256 × 256 + 256 = 65,792. \n  \n  - From second Dense to the output layer: 10 × 256 + 10 = 2,570. \n  \n  - The total is 200,690 + 65,972 + 2,570 = 269,322.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"- Another way of verifying the network is by calling the **plot_model()** method as follows:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_model(model, to_file='mlp-mnist.png', show_shapes=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 6. Implement MLP model using Keras <a class=\"anchor\" id=\"6\"></a>\n\n\n[Back to Table of Contents](#0.1)\n\n\n- The implementation of MLP model in Keras comprises of three steps:-\n\n  - Compiling the model with the compile() method.\n  \n  - Training the model with fit() method.\n  \n  - Evaluating the model performance with evaluate() method.\n  \n  \n- For detailed discussion on implementation, please refer to my previous kernel [Comprehensive Guide to ANN with Keras](https://www.kaggle.com/prashant111/comprehensive-guide-to-ann-with-keras)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## 6.1 Compile the model with compile() method <a class=\"anchor\" id=\"6.1\"></a>\n\n\n- Compilation of model can be done as follows:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"model.compile(loss='categorical_crossentropy', \n              optimizer='adam',\n              metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Loss function (categorical_crossentropy)\n\n- How far the predicted tensor is from the one-hot ground truth vector is called **loss**.\n\n- In this example, we use **categorical_crossentropy** as the loss function. It is the negative of the sum of the product of the target and the logarithm of the prediction. \n\n- There are other loss functions in Keras, such as mean_absolute_error and binary_crossentropy. The choice of the loss function is not arbitrary but should be a criterion that the model is learning. \n\n- For classification by category, categorical_crossentropy or mean_squared_error is a good choice after the softmax activation layer. The binary_crossentropy loss function is normally used after the sigmoid activation layer while mean_squared_error is an option for tanh output.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Optimization (optimizer adam)\n\n- With optimization, the objective is to minimize the loss function. The idea is that if the loss is reduced to an acceptable level, the model has indirectly learned the function mapping input to output.\n\n- In Keras, there are several choices for optimizers. The most commonly used optimizers are; **Stochastic Gradient Descent (SGD)**, **Adaptive Moments (Adam)** and **Root Mean Squared Propagation (RMSprop)**. \n\n- Each optimizer features tunable parameters like learning rate, momentum, and decay. \n\n- Adam and RMSprop are variations of SGD with adaptive learning rates. In the proposed classifier network, Adam is used since it has the highest test accuracy.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Metrics (accuracy)\n\n- Performance metrics are used to determine if a model has learned the underlying data distribution. The default metric in Keras is loss. \n\n- During training, validation, and testing, other metrics such as **accuracy** can also be included. \n\n- **Accuracy** is the percent, or fraction, of correct predictions based on ground truth.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## 6.2 Train the model with fit() method <a class=\"anchor\" id=\"6.2\"></a>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"model.fit(x_train, y_train, epochs=20, batch_size=batch_size)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 6.3 Evaluating model performance with evaluate() method <a class=\"anchor\" id=\"6.3\"></a>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"loss, acc = model.evaluate(x_test, y_test, batch_size=batch_size)\nprint(\"\\nTest accuracy: %.1f%%\" % (100.0 * acc))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 7. Overfitting and Regularization <a class=\"anchor\" id=\"7\"></a>\n\n[Back to Table of Contents](#0.1)\n\n\n- A neural network has the property to memorize the characteristics of training data. This is called **overfitting**.\n\n- In such a case, the network fails to generalize when subject to the test data.\n\n- To avoid this tendency, the model uses a regularizing layer or function. A commonly used regularizing layer is referred to as a **Dropout layer**.\n\n- Given a dropout rate (dropout=0.45), the **Dropout layer** randomly removes the fraction of units from participating in the next layer. For example, if the first layer has 256 units, after dropout=0.45 is applied, only (1 - 0.45) * 256 units = 140 units from layer 1 participate in layer 2.\n\n- The Dropout layer makes neural networks robust to unforeseen input data because the network is trained to predict correctly, even if some units are missing. \n\n- The dropout is not used in the output layer and it is only active during training. Moreover, dropout is not present during prediction.\n\n- There are regularizers that can be used other than dropouts like l1 or l2. In Keras, the bias, weight and activation output can be regularized per layer. - l1 and l2 favor smaller parameter values by adding a penalty function. Both l1 and l2 enforce the penalty using a fraction of the sum of absolute (l1) or square (l2) of parameter values. \n\n- So, the penalty function forces the optimizer to find parameter values that are small. Neural networks with small parameter values are more insensitive to the presence of noise from within the input data.\n\n- So, the l2 weight regularizer with fraction=0.001 can be implemented as:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.regularizers import l2\nmodel.add(Dense(hidden_units,\n                kernel_regularizer=l2(0.001),\n                input_dim=input_size))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- No additional layer is added if l1 or l2 regularization is used. The regularization is imposed in the Dense layer internally. For the proposed model, dropout still has a better performance than l2.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# 8. Results and Conclusion <a class=\"anchor\" id=\"8\"></a>\n\n[Back to Table of Contents](#0.1)\n\n\n- In this kernel, I build a dense neural network model to classify the MNIST digits and predict accuracy.\n\n- We get the test accuracy of 98.3%.\n\n- The accuracy tells us that our ANN model classifies the digits 98.3% of time correctly.\n\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# 9. References <a class=\"anchor\" id=\"9\"></a>\n\n\n[Back to Table of Contents](#0.1)\n\n\nThis kernel is based on the following Kaggle kernels and books:\n\n- [My previous kernel on Artificial Neural Network](https://www.kaggle.com/prashant111/comprehensive-guide-to-ann-with-keras)\n\n- [Deep Neural Network Keras way by Poonam Ligade](https://www.kaggle.com/poonaml/deep-neural-network-keras-way)\n\n- [MNIST with Keras for beginners by Aditya Soni](https://www.kaggle.com/adityaecdrid/mnist-with-keras-for-beginners-99457)\n\n-\tDeep Learning with Python by Francois Chollet\n\n-\tAdvanced Deep Learning with Keras by Rowel Atienza\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Now we will come to the end of this kernel.\n\nI hope you find this kernel useful and enjoyable.\n\nThank you\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"[Go to Top](#0)","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}