{"cells":[{"metadata":{},"cell_type":"markdown","source":"<h1 style=\"text-align: center;\" class=\"list-group-item list-group-item-action active\">Table of Contents</h1>\n\n<a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href = \"#1\" role=\"tab\" aria-controls=\"settings\">1. Introduction<span class=\"badge badge-primary badge-pill\">1</span></a>\n<a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href = \"#2\" role=\"tab\" aria-controls=\"settings\">2. Accuracy Metrics<span class=\"badge badge-primary badge-pill\">2</span></a>\n<a class=\"list-group-item list-group-item-action\"  data-toggle=\"list\" href=\"#3\" role=\"tab\" aria-controls=\"settings\">3. Precision Score<span class=\"badge badge-primary badge-pill\">3</span></a>\n   <a class=\"list-group-item list-group-item-action\"  data-toggle=\"list\" href=\"#4\" role=\"tab\" aria-controls=\"settings\">4. Recall Score<span class=\"badge badge-primary badge-pill\">4</span></a>\n  <a class=\"list-group-item list-group-item-action\"  data-toggle=\"list\" href=\"#5\" role=\"tab\" aria-controls=\"settings\">5. F1 Score<span class=\"badge badge-primary badge-pill\">5</span></a>\n  <a class=\"list-group-item list-group-item-action\"  data-toggle=\"list\" href=\"#6\" role=\"tab\" aria-controls=\"settings\">6. Area under the ROC curve<span class=\"badge badge-primary badge-pill\">6</span></a>\n  <a class=\"list-group-item list-group-item-action\"  data-toggle=\"list\" href=\"#7\" role=\"tab\" aria-controls=\"settings\">7. Confusion Matrix<span class=\"badge badge-primary badge-pill\">7</span></a>"},{"metadata":{},"cell_type":"markdown","source":"<h1  style=\"text-align: center\" class=\"list-group-item list-group-item-action active\">Introduction</h1><a id = \"1\" ></a>\n\nWhen it comes to machine learning problems, there are lot of different types of metrics in the real world. We will see some of the most common metrics that we can use when starting with our projects.\n\nIf we talk about classification problems, the most common metrics used are:\n- Accuracy\n- Precision (P)\n- Recall (R)\n- F1 score (F1)\n- Area under the ROC (Receiver Operating Characteristic) curve or simply AUC (AUC)\n\n![](https://blog.skyl.ai/hs-fs/hubfs/Evaluating%20a%20Machine%20Learning%20Model-min.jpg?width=1425&name=Evaluating%20a%20Machine%20Learning%20Model-min.jpg)"},{"metadata":{},"cell_type":"markdown","source":"**In this notebook my focus will be on Multiclass classification problems means how we can use above mentioned evaluation metrics for Multiclass classification problems**"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom sklearn import metrics\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_wine = pd.read_csv('../input/red-wine-quality-cortez-et-al-2009/winequality-red.csv')\ndf_wine.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_wine.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df_wine.drop('quality', axis = 1)\ny = df_wine['quality']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 893)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's create a common trained model which we can use to illustrate various evaluation metrics"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Common Trained Model\n\nmodel = RandomForestClassifier()\n\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h1  style=\"text-align: center\" class=\"list-group-item list-group-item-action active\">Accuracy metrics</h1><a id = \"2\" ></a>\n\n- **[Accuracy](https://blog.exsilio.com/all/accuracy-precision-recall-f1-score-interpretation-of-performance-measures/)** : It is one of the most straightforward metrics used in machine learning. It defines how accurate your model is. For example, if you build a model that classifies 90 samples accurately, your accuracy is 90% or 0.90. If only 83 samples are classified correctly, the accuracy of your model is 83% or 0.83. Simple.\n             \n"},{"metadata":{"trusted":true},"cell_type":"code","source":"def accuracy(y_true, y_pred):\n    \n    \"\"\"\n    Function to calculate accuracy\n    -> param y_true: list of true values\n    -> param y_pred: list of predicted values\n    -> return: accuracy score\n    \n    \"\"\"\n    \n    # Intitializing variable to store count of correctly predicted classes\n    correct_predictions = 0\n    \n    for yt, yp in zip(y_true, y_pred):\n        \n        if yt == yp:\n            \n            correct_predictions += 1\n    \n    #returns accuracy\n    return correct_predictions / len(y_true)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"accuracy(y_test, y_pred)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can also calculate accuracy using scikit-learn.\n\n**[Scikit-learn user guide for accuracy](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html)**"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(metrics.accuracy_score(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h1  style=\"text-align: center\" class=\"list-group-item list-group-item-action active\">Precision</h1><a id = \"3\" ></a>\n\n\n- **[Precision](https://blog.exsilio.com/all/accuracy-precision-recall-f1-score-interpretation-of-performance-measures/)** :  Precision is the ratio of correctly predicted positive observations to the total predicted positive observations. The question that this metric answer is of all passengers that labeled as survived, how many actually survived? High precision relates to the low false positive rate. We have got 0.788 precision which is pretty good\n\n     **[True Positives (TP)](https://blog.exsilio.com/all/accuracy-precision-recall-f1-score-interpretation-of-performance-measures/)** - These are the correctly predicted positive values which means that the value of actual class is yes and the value of predicted  class is also yes. E.g. if actual class value indicates that this passenger survived and predicted class tells you the same thing.\n\n     **[True Negatives (TN)](https://blog.exsilio.com/all/accuracy-precision-recall-f1-score-interpretation-of-performance-measures/)** - These are the correctly predicted negative values which means that the value of actual class is no and value of predicted class is also no. E.g. if actual class says this passenger did not survive and predicted class tells you the same thing.\n\n     False positives and false negatives, these values occur when your actual class contradicts with the predicted class.\n\n     **[False Positives (FP)](https://blog.exsilio.com/all/accuracy-precision-recall-f1-score-interpretation-of-performance-measures/)** – When actual class is no and predicted class is yes. E.g. if actual class says this passenger did not survive but predicted class tells you that this passenger will survive.\n\n     **[False Negatives (FN)](https://blog.exsilio.com/all/accuracy-precision-recall-f1-score-interpretation-of-performance-measures/)** – When actual class is yes but predicted class in no. E.g. if actual class value indicates that this passenger survived and predicted class tells you that passenger will die.\n     \n\n<h2 style = \"text-align: center\"> Precision = TP / (TP + FP) </h2>\n\nIf we have to define accuracy using the terms described above, we can write:\n\n<h2 style = \"text-align: center\">Accuracy Score = (TP + TN) / (TP + TN + FP + FN) </h2>\n"},{"metadata":{},"cell_type":"markdown","source":"Let's Create functions to compute True Positives, True Negatives, False Positives and False Negatives"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Functions to compute True Positives, True Negatives, False Positives and False Negatives\n\ndef true_positive(y_true, y_pred):\n    \n    tp = 0\n    \n    for yt, yp in zip(y_true, y_pred):\n        \n        if yt == 1 and yp == 1:\n            tp += 1\n    \n    return tp\n\ndef true_negative(y_true, y_pred):\n    \n    tn = 0\n    \n    for yt, yp in zip(y_true, y_pred):\n        \n        if yt == 0 and yp == 0:\n            tn += 1\n            \n    return tn\n\ndef false_positive(y_true, y_pred):\n    \n    fp = 0\n    \n    for yt, yp in zip(y_true, y_pred):\n        \n        if yt == 0 and yp == 1:\n            fp += 1\n            \n    return fp\n\ndef false_negative(y_true, y_pred):\n    \n    fn = 0\n    \n    for yt, yp in zip(y_true, y_pred):\n        \n        if yt == 1 and yp == 0:\n            fn += 1\n            \n    return fn","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Approch To compute precision of multi class classification problem**\n\nThere are two different ways to calculate this which might get confusing from time to time. We know that precision depends on true positives and false positives.\n\n- **Macro averaged precision**: calculate precision for all classes individually and then average them\n- **Micro averaged precision**: calculate class wise true positive and false positive and then use that to calculate overall precision\n"},{"metadata":{},"cell_type":"markdown","source":"Let’s see how macro-averaged precision is implemented."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Computation of macro-averaged precision\n\ndef macro_precision(y_true, y_pred):\n\n    # find the number of classes\n    num_classes = len(np.unique(y_true))\n\n    # initialize precision to 0\n    precision = 0\n    \n    # loop over all classes\n    for class_ in list(y_true.unique()):\n        \n        # all classes except current are considered negative\n        temp_true = [1 if p == class_ else 0 for p in y_true]\n        temp_pred = [1 if p == class_ else 0 for p in y_pred]\n        \n        \n        # compute true positive for current class\n        tp = true_positive(temp_true, temp_pred)\n        \n        # compute false positive for current class\n        fp = false_positive(temp_true, temp_pred)\n        \n        \n        # compute precision for current class\n        temp_precision = tp / (tp + fp + 1e-6)\n        # keep adding precision for all classes\n        precision += temp_precision\n        \n    # calculate and return average precision over all classes\n    precision /= num_classes\n    \n    return precision\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f\"Macro-averaged Precision score : {macro_precision(y_test, y_pred) }\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's see how we can implement marco-averaged precision using sklearn\n\n**[Scikit-learn user guide for Precision ](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html#sklearn.metrics.precision_score)**"},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"macro_averaged_precision = metrics.precision_score(y_test, y_pred, average = 'macro')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f\"Macro-Averaged Precision score using sklearn library : {macro_averaged_precision}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let’s see how micro-averaged precision is implemented."},{"metadata":{"trusted":true},"cell_type":"code","source":"def micro_precision(y_true, y_pred):\n\n\n    # find the number of classes \n    num_classes = len(np.unique(y_true))\n    \n    # initialize tp and fp to 0\n    tp = 0\n    fp = 0\n    \n    # loop over all classes\n    for class_ in y_true.unique():\n        \n        # all classes except current are considered negative\n        temp_true = [1 if p == class_ else 0 for p in y_true]\n        temp_pred = [1 if p == class_ else 0 for p in y_pred]\n        \n        # calculate true positive for current class\n        # and update overall tp\n        tp += true_positive(temp_true, temp_pred)\n        \n        # calculate false positive for current class\n        # and update overall tp\n        fp += false_positive(temp_true, temp_pred)\n        \n    # calculate and return overall precision\n    precision = tp / (tp + fp)\n    return precision","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f\"Micro-averaged Precision score : {micro_precision(y_test, y_pred)}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's see how we can implement mirco-averaged precision using sklearn\n\n**[Scikit-learn user guide for Precision ](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html#sklearn.metrics.precision_score)**"},{"metadata":{"trusted":true},"cell_type":"code","source":"micro_averaged_precision = metrics.precision_score(y_test, y_pred, average = 'micro')\nprint(f\"Micro-Averaged Precision score using sklearn library : {micro_averaged_precision}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h1  style=\"text-align: center\" class=\"list-group-item list-group-item-action active\">Recall</h1><a id = \"4\" ></a>\n\n\n- **[Recall(Sensitivity)](https://blog.exsilio.com/all/accuracy-precision-recall-f1-score-interpretation-of-performance-measures/)** : Recall is the ratio of correctly predicted positive observations to the all observations in actual class - yes. The question recall answers is: Of all the passengers that truly survived\n\n<h2 style = \"text-align: center\"> Recall = TP / (TP + FN) </h2>\n\n\n\n**Approch To compute recall of multi class classification problem**\n\nThere are two different ways to calculate this which might get confusing from time to time. We know that recall depends on true positives and false negatives.\n\n- **Macro averaged recall**: calculate recall for all classes individually and then average them\n- **Micro averaged recall**: calculate class wise true positive and false negative and then use that to calculate overall recall\n"},{"metadata":{},"cell_type":"markdown","source":"Let’s see how macro-averaged recall is implemented."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Computation of macro-averaged recall\n\ndef macro_recall(y_true, y_pred):\n\n    # find the number of classes\n    num_classes = len(np.unique(y_true))\n\n    # initialize recall to 0\n    recall = 0\n    \n    # loop over all classes\n    for class_ in list(y_true.unique()):\n        \n        # all classes except current are considered negative\n        temp_true = [1 if p == class_ else 0 for p in y_true]\n        temp_pred = [1 if p == class_ else 0 for p in y_pred]\n        \n        \n        # compute true positive for current class\n        tp = true_positive(temp_true, temp_pred)\n        \n        # compute false negative for current class\n        fn = false_negative(temp_true, temp_pred)\n        \n        \n        # compute recall for current class\n        temp_recall = tp / (tp + fn + 1e-6)\n        \n        # keep adding recall for all classes\n        recall += temp_recall\n        \n    # calculate and return average recall over all classes\n    recall /= num_classes\n    \n    return recall\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f\"Macro-averaged recall score : {macro_recall(y_test, y_pred)}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's see how we can implement macro-averaged recall using sklearn\n\n **[Scikit-learn user guide for Recall](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.recall_score.html#sklearn.metrics.recall_score)**\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"macro_averaged_recall = metrics.recall_score(y_test, y_pred, average = 'macro')\nprint(f\"Macro-averaged recall score using sklearn : {macro_averaged_recall}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let’s see how micro-averaged recall is implemented."},{"metadata":{"trusted":true},"cell_type":"code","source":"def micro_recall(y_true, y_pred):\n\n\n    # find the number of classes \n    num_classes = len(np.unique(y_true))\n    \n    # initialize tp and fp to 0\n    tp = 0\n    fn = 0\n    \n    # loop over all classes\n    for class_ in y_true.unique():\n        \n        # all classes except current are considered negative\n        temp_true = [1 if p == class_ else 0 for p in y_true]\n        temp_pred = [1 if p == class_ else 0 for p in y_pred]\n        \n        # calculate true positive for current class\n        # and update overall tp\n        tp += true_positive(temp_true, temp_pred)\n        \n        # calculate false negative for current class\n        # and update overall tp\n        fn += false_negative(temp_true, temp_pred)\n        \n    # calculate and return overall recall\n    recall = tp / (tp + fn)\n    return recall","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f\"Micro-averaged recall score : {micro_recall(y_test, y_pred)}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's see how we can implement micro-averaged recall using sklearn\n\n **[Scikit-learn user guide for Recall](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.recall_score.html#sklearn.metrics.recall_score)**\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"micro_averaged_recall = metrics.recall_score(y_test, y_pred, average = 'micro')\nprint(f\"Micro-Averaged recall score using sklearn library : {micro_averaged_recall}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h1  style=\"text-align: center\" class=\"list-group-item list-group-item-action active\">F1 score</h1><a id = \"5\" ></a>\n\n\n- **[F1 score (F1)](https://blog.exsilio.com/all/accuracy-precision-recall-f1-score-interpretation-of-performance-measures/)** : F1 Score is the weighted average of Precision and Recall. Therefore, this score takes both false positives and false negatives into account. Intuitively it is not as easy to understand as accuracy, but F1 is usually more useful than accuracy, especially if you have an uneven class distribution. Accuracy works best if false positives and false negatives have similar cost. If the cost of false positives and false negatives are very different, it’s better to look at both Precision and Recall. In our case, F1 score is 0.701.\n\nF1 score is a metric that combines both precision and recall. It is defined as a simple weighted average (harmonic mean) of precision and recall. If we denote precision using P and recall using R, we can represent the F1 score as:\n\n<h2 style = \"text-align: center\"> F1 = 2PR / (P + R) </h2>\n\n\n\n**Approch To compute F1 Score of multi class classification problem**\n\nThere are two different ways to calculate this which might get confusing from time to time. We know that F1 Score depends on precision and recall.\n\n- **Macro averaged F1 Score**: calculate f1 score of every class and then average them\n- **Micro averaged F1 Score**: calculate macro-averaged precision score and macro-averaged recall score and then take there harmonic mean\n"},{"metadata":{},"cell_type":"markdown","source":"Let’s see how macro-averaged f1 score is implemented."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Computation of macro-averaged fi score\n\ndef macro_f1(y_true, y_pred):\n\n    # find the number of classes\n    num_classes = len(np.unique(y_true))\n\n    # initialize f1 to 0\n    f1 = 0\n    \n    # loop over all classes\n    for class_ in list(y_true.unique()):\n        \n        # all classes except current are considered negative\n        temp_true = [1 if p == class_ else 0 for p in y_true]\n        temp_pred = [1 if p == class_ else 0 for p in y_pred]\n        \n        \n        # compute true positive for current class\n        tp = true_positive(temp_true, temp_pred)\n        \n        # compute false negative for current class\n        fn = false_negative(temp_true, temp_pred)\n        \n        # compute false positive for current class\n        fp = false_positive(temp_true, temp_pred)\n        \n        \n        # compute recall for current class\n        temp_recall = tp / (tp + fn + 1e-6)\n        \n        # compute precision for current class\n        temp_precision = tp / (tp + fp + 1e-6)\n        \n        \n        temp_f1 = 2 * temp_precision * temp_recall / (temp_precision + temp_recall + 1e-6)\n        \n        # keep adding f1 score for all classes\n        f1 += temp_f1\n        \n    # calculate and return average f1 score over all classes\n    f1 /= num_classes\n    \n    return f1\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f\"Macro-averaged f1 score : {macro_f1(y_test, y_pred)}\")\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's see how we can implement micro-averaged F1 score using sklearn\n\n**[Scikit-learn user guide for F1 score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html)**"},{"metadata":{"trusted":true},"cell_type":"code","source":"macro_averaged_f1 = metrics.f1_score(y_test, y_pred, average = 'macro')\nprint(f\"Macro-Averaged F1 score using sklearn library : {macro_averaged_f1}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let’s see how micro-averaged f1 score is implemented."},{"metadata":{"trusted":true},"cell_type":"code","source":"def micro_f1(y_true, y_pred):\n\n\n    #micro-averaged precision score\n    P = micro_precision(y_true, y_pred)\n\n    #micro-averaged recall score\n    R = micro_recall(y_true, y_pred)\n\n    #micro averaged f1 score\n    f1 = 2*P*R / (P + R)    \n\n    return f1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f\"Micro-averaged recall score : {micro_f1(y_test, y_pred)}\")\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's see how we can implement micro-averaged F1 score using sklearn\n\n**[Scikit-learn user guide for F1 score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html)**"},{"metadata":{"trusted":true},"cell_type":"code","source":"micro_averaged_f1 = metrics.f1_score(y_test, y_pred, average = 'micro')\nprint(f\"Micro-Averaged F1 score using sklearn library : {micro_averaged_f1}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h1  style=\"text-align: center\" class=\"list-group-item list-group-item-action active\">Area under the ROC curve</h1><a id = \"6\" ></a>\n\n\n- **[Area under the ROC (Receiver Operating Characteristic) curve](https://machinelearningmastery.com/roc-curves-and-precision-recall-curves-for-classification-in-python/)** : AUC - ROC curve is a performance measurement for the classification problems at various threshold settings. ROC is a probability curve and AUC represents the degree or measure of separability. It tells how much the model is capable of distinguishing between classes. Higher the AUC, the better the model is at predicting 0s as 0s and 1s as 1s. By analogy, the Higher the AUC, the better the model is at distinguishing between patients with the disease and no disease.\n\n     **[Scikit-learn user guide for AUC under the ROC curve](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html)**\n     \n ![](https://glassboxmedicine.files.wordpress.com/2019/02/roc-curve-v2.png?w=576)"},{"metadata":{},"cell_type":"markdown","source":"**Approch to compute AUC score of multi class classification problem**\n\n**One vs All** : It involves splitting the multi-class dataset into multiple binary classification problems. A binary classifier is then trained on each binary classification problem and predictions are made using the model that is the most confident."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import roc_auc_score\n\ndef roc_auc_score_multiclass(actual_class, pred_class, average = \"macro\"):\n    \n    #creating a set of all the unique classes using the actual class list\n    unique_class = set(actual_class)\n    roc_auc_dict = {}\n    for per_class in unique_class:\n        \n        #creating a list of all the classes except the current class \n        other_class = [x for x in unique_class if x != per_class]\n\n        #marking the current class as 1 and all other classes as 0\n        new_actual_class = [0 if x in other_class else 1 for x in actual_class]\n        new_pred_class = [0 if x in other_class else 1 for x in pred_class]\n\n        #using the sklearn metrics method to calculate the roc_auc_score\n        roc_auc = roc_auc_score(new_actual_class, new_pred_class, average = average)\n        roc_auc_dict[per_class] = roc_auc\n\n    return roc_auc_dict","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"roc_auc_dict = roc_auc_score_multiclass(y_test, y_pred)\nroc_auc_dict","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h1  style=\"text-align: center\" class=\"list-group-item list-group-item-action active\">Confusion Matrix</h1><a id = \"7\" ></a>\n\n- **[Confusion Matrix](https://www.geeksforgeeks.org/confusion-matrix-machine-learning/)** : A much better way to evaluate the performance of a classifier is to look at the confusion matrix. The general idea is to count the number of times instances of class A are classified as class B. For example, to know the number of times the classifier confused images of 5s with 3s, you would look in the 5th row and 3rd column of the confusion matrix.\n\n![](https://2.bp.blogspot.com/-EvSXDotTOwc/XMfeOGZ-CVI/AAAAAAAAEiE/oePFfvhfOQM11dgRn9FkPxlegCXbgOF4QCLcBGAs/s1600/confusionMatrxiUpdated.jpg)     \n\n\n    **[Scikit-Learn user guide for Confusion Matrix](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html)**"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_test.unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (18,8))\nsns.heatmap(metrics.confusion_matrix(y_test, y_pred), annot = True, xticklabels = y_test.unique(), yticklabels = y_test.unique(), cmap = 'summer')\nplt.xlabel('Predicted Labels')\nplt.ylabel('True Labels')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}