{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"\n# MNIST Dataset: 99.55% Accuracy. Step by Step tutorial to tackle any Machine Learning Classification Problem.\n<hr>\n\n### About the Notebook\n* In this notebook, I have covered the necessary steps to approach any Machine Learning Classification Problem.\n* Included Image Visualization for better understanding.\n* Quick Links to the functions I have used to explore it in depth.\n* Basic techniques such as Confusion Matrix, Image Augmentation, etc.\n* I have also compared the results of Model without using CNN and CNN.\n\nI have tried to make this notebook as simple as possible, along with covering the basic approach to tackle any classification task. \n### Task\nThe task is to classify the images in 10 class, i.e., [0-9], inclusively.  \n<hr>\n\n<font style=\"color:Red;font-size:18px\">If you find this notebook helpful, Please UPVOTE. Also optimizing to become Kaggle expert. Support if you learnt something from the notebook.</font>\n\n### Follow me:\n\n* <a href=\"https://github.com/ds-souvik\"><img src=\"data:image/svg+xml;base64,PHN2ZyBlbmFibGUtYmFja2dyb3VuZD0ibmV3IDAgMCAyNCAyNCIgaGVpZ2h0PSI1MTIiIHZpZXdCb3g9IjAgMCAyNCAyNCIgd2lkdGg9IjUxMiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj48cGF0aCBkPSJtMTIgLjVjLTYuNjMgMC0xMiA1LjI4LTEyIDExLjc5MiAwIDUuMjExIDMuNDM4IDkuNjMgOC4yMDUgMTEuMTg4LjYuMTExLjgyLS4yNTQuODItLjU2NyAwLS4yOC0uMDEtMS4wMjItLjAxNS0yLjAwNS0zLjMzOC43MTEtNC4wNDItMS41ODItNC4wNDItMS41ODItLjU0Ni0xLjM2MS0xLjMzNS0xLjcyNS0xLjMzNS0xLjcyNS0xLjA4Ny0uNzMxLjA4NC0uNzE2LjA4NC0uNzE2IDEuMjA1LjA4MiAxLjgzOCAxLjIxNSAxLjgzOCAxLjIxNSAxLjA3IDEuODAzIDIuODA5IDEuMjgyIDMuNDk1Ljk4MS4xMDgtLjc2My40MTctMS4yODIuNzYtMS41NzctMi42NjUtLjI5NS01LjQ2Ni0xLjMwOS01LjQ2Ni01LjgyNyAwLTEuMjg3LjQ2NS0yLjMzOSAxLjIzNS0zLjE2NC0uMTM1LS4yOTgtLjU0LTEuNDk3LjEwNS0zLjEyMSAwIDAgMS4wMDUtLjMxNiAzLjMgMS4yMDkuOTYtLjI2MiAxLjk4LS4zOTIgMy0uMzk4IDEuMDIuMDA2IDIuMDQuMTM2IDMgLjM5OCAyLjI4LTEuNTI1IDMuMjg1LTEuMjA5IDMuMjg1LTEuMjA5LjY0NSAxLjYyNC4yNCAyLjgyMy4xMiAzLjEyMS43NjUuODI1IDEuMjMgMS44NzcgMS4yMyAzLjE2NCAwIDQuNTMtMi44MDUgNS41MjctNS40NzUgNS44MTcuNDIuMzU0LjgxIDEuMDc3LjgxIDIuMTgyIDAgMS41NzgtLjAxNSAyLjg0Ni0uMDE1IDMuMjI5IDAgLjMwOS4yMS42NzguODI1LjU2IDQuODAxLTEuNTQ4IDguMjM2LTUuOTcgOC4yMzYtMTEuMTczIDAtNi41MTItNS4zNzMtMTEuNzkyLTEyLTExLjc5MnoiIGZpbGw9IiMyMTIxMjEiLz48L3N2Zz4=\" width=\"22\" align=\"left\" style=\"margin-right:10px\"/> GitHub</a>\n* <a href=\"https://medium.com/@souvik.ganguly.ds\"><img src=\"data:image/svg+xml;base64,PHN2ZyBlbmFibGUtYmFja2dyb3VuZD0ibmV3IDAgMCAyNCAyNCIgaGVpZ2h0PSI1MTIiIHZpZXdCb3g9IjAgMCAyNCAyNCIgd2lkdGg9IjUxMiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj48cGF0aCBkPSJtMjIuMDg1IDQuNzMzIDEuOTE1LTEuODMydi0uNDAxaC02LjYzNGwtNC43MjggMTEuNzY4LTUuMzc5LTExLjc2OGgtNi45NTZ2LjQwMWwyLjIzNyAyLjY5M2MuMjE4LjE5OS4zMzIuNDkuMzAzLjc4M3YxMC41ODNjLjA2OS4zODEtLjA1NS43NzMtLjMyMyAxLjA1bC0yLjUyIDMuMDU0di4zOTZoNy4xNDV2LS40MDFsLTIuNTItMy4wNDljLS4yNzMtLjI3OC0uNDAyLS42NjMtLjM0Ny0xLjA1di05LjE1NGw2LjI3MiAxMy42NTloLjcyOWw1LjM5My0xMy42NTl2MTAuODgxYzAgLjI4NyAwIC4zNDYtLjE4OC41MzRsLTEuOTQgMS44Nzd2LjQwMmg5LjQxMnYtLjQwMWwtMS44Ny0xLjgzMWMtLjE2NC0uMTI0LS4yNDktLjMzMi0uMjE0LS41MzR2LTEzLjQ2N2MtLjAzNS0uMjAzLjA0OS0uNDExLjIxMy0uNTM0eiIgZmlsbD0iIzIxMjEyMSIvPjwvc3ZnPg==\"  width=\"22\" align=\"left\" style=\"margin-right:10px\"/> Medium</a>\n* <a href=\"https://www.linkedin.com/in/souvik-ganguly-4a9924105/\"><img src=\"data:image/svg+xml;base64,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\" width=\"22\" align=\"left\" style=\"margin-right:10px\"/>Linkedin</a>\n\n\n\n<hr>\n\n# Content:\n\n* [Handwritten Digit Recognition (MNIST Dataset)](#Handwritten-Digit-Recognition-(MNIST-Dataset))\n* [Required Imports](#Required-Imports)\n* [Loading and Visualizing Dataset](#Loading-and-Visualizing-Dataset)\n* [Visualize Digits dataset](#Visualize-Digits-dataset)\n* **[Buliding Model](#Buliding-Model)**\n    * [Model Using Keras](#Model-Using-Keras)\n    * [Compiling Model](#Compiling-Model)\n    * [Training](#Training)\n    * [Training Performance](#Training-Performance.)\n    * [Confusion Matrix](#Confusion-Matrix)\n* **[Improving Result by Image Augmentation](#Improving-Result-by-Image-Augmentation)**\n    * [Augmentation Using Keras](#Augmentation-Using-Keras)\n    * [Learning Rate](#Learning-Rate)\n* [Visualizing Result](#Visualizing-Result)\n* [Predict on Testset](#Predict-on-Testset)\n* **[Understand the Intermediate Layers of the Model](#Lets-understand-the-intermediate-layers-of-the-model)**\n    * [Compare the layer 1 output](#Try-to-compare-the-layer-1-output)\n    * [Visualizing All Intermediate Activation Layer](#Visualizing-All-Intermediate-Activation-Layer)","metadata":{}},{"cell_type":"markdown","source":"# Required Imports","metadata":{}},{"cell_type":"markdown","source":"**Imports:**\n* pandas : For handeling csv dataset\n* numpy : Support for Pandas and calculations\n* Matplotlib - For visualization (Plotting graphs)\n*  keras - Prediction Models","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\nimport matplotlib.pyplot as plt \nimport cv2 as cv\n\nfrom keras.layers import Conv2D, Input, LeakyReLU, Dense, Activation, Flatten, Dropout, MaxPool2D\nfrom keras import models\nfrom keras.optimizers import Adam,RMSprop \nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.callbacks import ReduceLROnPlateau\n\nimport pickle\n\n%matplotlib inline","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Loading and Visualizing Dataset\n<hr>\n\n## About Dataset\nMNIST dataset has the following features:\n* Dataset size 60,000 samples of handwritten images.\n* The size of each image is 28x28 pixels.\n* Each image has only 1 color channel, i.e., grayscale image.\n* Each pixel has value in the range of [0,255] where 0 represents black, and 255 represents white.\n* Each image has labeled from 0-9.\n\n\n## Loading train.csv\n\nThis will loads the data from Kaggle dataset train.csv into a **Dataframe**. As file type is CSV, I am loading it using [read_csv](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html) of **pandas**. Then using NumPy for **random permutation** of the training dataset. I am using seed to regenerate the same permutation every time. Change seed value to get different permutation. \n<br>\n\n**Check out these functions for more info:**\n* [df.iloc[]](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.iloc.html)\n* [np.random](https://docs.scipy.org/doc/numpy-1.14.0/reference/routines.random.html)","metadata":{}},{"cell_type":"code","source":"np.random.seed(1) # seed\ndf_train = pd.read_csv(\"../input/digit-recognizer/train.csv\") # Loading Dataset\ndf_train = df_train.iloc[np.random.permutation(len(df_train))] # Random permutaion for dataset (seed is used to resample the same permutation every time)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.head(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Training set has 42,000 images. And has 785 columns, 1st coloumn is label for the image and rest 784 are the pixel values. <br>**Remember it is flattened. I will reshape it latter.**","metadata":{}},{"cell_type":"markdown","source":"### Preparing Training and Validation data \n**It requires a few steps:**\n* Assuming the validation set size. I am taking it 10% of the training set.  \n* Splitting training set into a training set (90% original training set) and validation set (10% original training set) from the training dataset.\n* Reshaping both sets into (sample size,28,28,1) where sample size represents the size of the train or validation set.\n* Splitting the labels for both training set and validation set.\n\nUsing df.iloc of pandas for slicing the data frame(read more link below), then converting into NumPy array and finally reshaping NumPy array in required shape.<br>\n\n**Check out these functions for more info:**\n* [df.iloc[]](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.iloc.html)\n* [np.reshape](https://docs.scipy.org/doc/numpy/reference/generated/numpy.reshape.html)\n* [np.asarray](https://docs.scipy.org/doc/numpy/reference/generated/numpy.asarray.html)","metadata":{}},{"cell_type":"code","source":"sample_size = df_train.shape[0] # Training set size\nvalidation_size = int(df_train.shape[0]*0.1) # Validation set size \n\n# train_x and train_y\ntrain_x = np.asarray(df_train.iloc[:sample_size-validation_size,1:]).reshape([sample_size-validation_size,28,28,1]) # taking all columns expect column 0\ntrain_y = np.asarray(df_train.iloc[:sample_size-validation_size,0]).reshape([sample_size-validation_size,1]) # taking column 0\n\n# val_x and val_y\nval_x = np.asarray(df_train.iloc[sample_size-validation_size:,1:]).reshape([validation_size,28,28,1])\nval_y = np.asarray(df_train.iloc[sample_size-validation_size:,0]).reshape([validation_size,1])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Shape of training set","metadata":{}},{"cell_type":"code","source":"train_x.shape,train_y.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Loading test.csv\nThis will load the test.csv. It has 18,000 images and no label is there. Prediction need to be done on these images. After loading converting the data in form of numpy array and reshaping it.","metadata":{}},{"cell_type":"code","source":"df_test = pd.read_csv(\"../input/digit-recognizer/test.csv\")\ntest_x = np.asarray(df_test.iloc[:,:]).reshape([-1,28,28,1])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Normalize Pixel Data\nEach pixel values lies between [0,255]. This value range is too high and it will be difficult for any model to learn. The best approach is **normalize** the data. In this case, as the pixel value is in the known range it sufficient to **scale the pixel values** in range [0,1] by simply dividing the array by **255**. ","metadata":{}},{"cell_type":"code","source":"# convirting pixel values in range [0,1]\ntrain_x = train_x/255\nval_x = val_x/255\ntest_x = test_x/255","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Visualize Digits dataset\n<hr>\n\nThe first and fundamental thing to check is the frequency of the classes in the dataset, as the balanced dataset is always good to start. But this is not always true, and there are several supervised learning tasks in which the classes are not balanced, also in case of anomaly detection, there is a large difference between the positive and negative class. But that is out of the scope of this notebook.\n\n### 1. Frequency plot for the training set","metadata":{}},{"cell_type":"code","source":"# Cheacking frequency of digits in training and validation set\ncounts = df_train.iloc[:sample_size-validation_size,:].groupby('label')['label'].count()\n# df_train.head(2)\n# counts\nf = plt.figure(figsize=(10,6))\nf.add_subplot(111)\n\nplt.bar(counts.index,counts.values,width = 0.8,color=\"orange\")\nfor i in counts.index:\n    plt.text(i,counts.values[i]+50,str(counts.values[i]),horizontalalignment='center',fontsize=14)\n\nplt.tick_params(labelsize = 14)\nplt.xticks(counts.index)\nplt.xlabel(\"Digits\",fontsize=16)\nplt.ylabel(\"Frequency\",fontsize=16)\nplt.title(\"Frequency Graph training set\",fontsize=20)\nplt.savefig('digit_frequency_train.png')  \nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2. Frequency plot for the validation set","metadata":{}},{"cell_type":"code","source":"# df_train.iloc[sample_size-validation_index:,1:]\n# Cheacking frequency of digits in training and validation set\ncounts = df_train.iloc[sample_size-validation_size:,:].groupby('label')['label'].count()\n# df_train.head(2)\n# counts\nf = plt.figure(figsize=(10,6))\nf.add_subplot(111)\n\nplt.bar(counts.index,counts.values,width = 0.8,color=\"orange\")\nfor i in counts.index:\n    plt.text(i,counts.values[i]+5,str(counts.values[i]),horizontalalignment='center',fontsize=14)\n\nplt.tick_params(labelsize = 14)\nplt.xticks(counts.index)\nplt.xlabel(\"Digits\",fontsize=16)\nplt.ylabel(\"Frequency\",fontsize=16)\nplt.title(\"Frequency Graph Validation set\",fontsize=20)\nplt.savefig('digit_frequency_val.png')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It seems that both the training and validation set has a good balance between the classes, so let's move on and see a few of the digits.\n\n### Visualizing the digits by plotting Images\n<hr>\nThis will plot the first 30 images of digits with the label.","metadata":{}},{"cell_type":"code","source":"rows = 5 # defining no. of rows in figure\ncols = 6 # defining no. of colums in figure\n\nf = plt.figure(figsize=(2*cols,2*rows)) # defining a figure \n\nfor i in range(rows*cols): \n    f.add_subplot(rows,cols,i+1) # adding sub plot to figure on each iteration\n    plt.imshow(train_x[i].reshape([28,28]),cmap=\"Blues\") \n    plt.axis(\"off\")\n    plt.title(str(train_y[i]), y=-0.15,color=\"green\")\nplt.savefig(\"digits.png\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Buliding Model\n\n<hr>\n\n![model](https://miro.medium.com/max/1000/1*4OUonEDfZwCfR4Y-G-h1fw.jpeg)","metadata":{}},{"cell_type":"code","source":"# # Loading pickled resources\n# !wget https://www.kaggleusercontent.com/kf/31703703/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..f6l0FhjgIxCd7bfZA3MF_A.oHtR56AzfPZhslGD-R2Uca6Kce1yVCG805lWUk25H5T-MQciLjAjmoTd9RPGh1zdUlwtbMZPuAi9_1BrHNZfFlJ5duoYajHON-Sk_mMy7OIePjqNRqo8vkEnTHmbV6Oj1Z6MR9dGzT2Gch2soeaLaZnIjxJt5e8DsFaia6dTjxRzzzrKaQDWLikdsjo2xwbQp9yo4-8htw6adclSbtnXsMV4kJNBs25d-qqRLuUSuhqxxCbJMkuuZgPjnEzqO7aLU0zqcGYUXDDdx1O-oU2ncMAYpXYqssqzQgD6-t4Fl83XWQnNqRv5wec5cdD-7IF9cbjyD_CE-Ib863pPJ9RJc-IYypbUvvKfMQuhahe9NiuRGNSNodVlSiuSzk0nudl5uHqf7V7_1h_juPPVj8mUUOqleLye9_ZtJ2S8pD6hUXT9p7kPy6v6RdoaE_LgkrijyvmJhmS-yMETpazrlQlKp96A3W0EVdhtVxmW7QUwbjIlzdEs7whAe4EcqQIzd4H69TR6hLCqVlaZkMBBPvBWr_dCTxu6htDP8qE2QCH08H1VPXyZLERTMH1SRENnwa_BxMTVkc_pP70tkvGA2xtgoJHzAlcOZZfqsa5fmCa8tqIOkME1hn88Xgm5eh58JXT2ZXyA7hxfpzKP3UQXegeBnPaEPOLa5eEoND9E9ypyi5I1QWa7QTMe9ruEbXr6DUbJ.GvQemyhjKibaOnfNrnRUXQ/model.h5\n# !wget https://www.kaggleusercontent.com/kf/31703703/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..yUikFJ_W78H0Zfo7LsQ-2g.1-XOTVZwAUnN9uzbSyaPEuXbdS_5mlLvYRUOaFjMZewxYpcCFASz2fROWEIHjC-KHCOOO-DYjLY7pWHvN81cOW25n28C2mT6aFnWkObrkUYXlbZW0sS7iaKwxUw4a_XwYYUmMfeNuOpKf5OsBdv-L2y0GOLb-fT8hDqteihh6qP97VOT1fmvdv2gYcG0WqKw4mcWwtWAYyqQAScE_knALnyTvWZTF73LR99gaxy_w41t3PHo0rfHy037Cll0lWznkf6ppfAIv_CKr_v2HfpB9go0DJbXKqD87LkcnNF94e_b95i0UYfS2pMugXS4ob1WnSblE34Df_n9rB3pWMXn9DPnqlwDqa1snJOc3CLeK9MTsPQ9NuzEFIjxMnWXQhNGQ7sXYX8qAPqanQ3-OQSZHHD8lufpcJEguVqOuOWY5qZlKe4ZTzn3g8fhbssulphnajM4ZThC5pceWnVT2rowvERPtlvijy6AdAGEy57BxN7FeweBVZAR1Zv5RA_wE2qo4DFmyK36j8p_bVZHTnVvqFFWz0SAimJqzMmzxNrZY7_NGuqDy5rjUmoa2y0e0-qPFBcfFWvT2Pe_RKW21dBOKH9HbI1j0WS_Ua72FI1-MToX8DUHRdN9UNLJhnugwWY_lWvppU-fRCJFhkSPIYyfzbx1cH2ykLLZRz4-sjCegE78I03ue3096Q7kUMw9-ZGd.a3Ge-GJWsUMNPuyQ0gpMfg/model_img_augmentation.h5\n# !wget https://www.kaggleusercontent.com/kf/31703703/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..Hq9sgF7KDfRSLRlO3zlQSg.dQKW1YCUjhwAZbX-54g1ygj0qTWSH6egywwOdIqm-hwUxTWnj6L9EkNK8qncdH4FeNbEtAjYrQ2oStYGIVVJzsqFDQwaXBsnzvKQx2icckW3ww-aNtDxATdkVY2ZiqMLAMIMyy8_nFoFvt2tQm48XeIUecn3OTL6Te3VfRr3OXQ1QWU7lbY-8BetDVy0tbrLV_vIkV-fUy_FLGsa7QgH4Xerxi7xXoItAmJCbEIXBt5pR-_frNz0rdDoj30e-xGdUkLRiBNe1Nk9_1EFacGwTwhM3KqE8SDF5CtVqP7XsFhFIxVal-lmUy1hzsPi0xZt_ikRIbmfTb6K5HmQ97Jzm4nd7YdQ_u2ScbhLLFy2pj0n4XapGZgMYE5o_2_Cv2c_3uquTGDTpjiOAA25ylrFypwZGmenAeCSrJZIto0ta_onqVPz_euNQY4PHJ5P3aJi2KWHJzNl3LxBU2u-LfkfOzwYXu1DIOOqkzSSLWraxZoleean-9oawdqq0doQhogfx2wBr844ApCpVYDxEEn8CxAwfK-RoiE7MdEasDYtWujlmcOnMQsQjUohnJcOWwQSIOIEpqevcXBU4PPoiw-ex-vEUtflupnNRtlwwLRzjyrV_6VJbdQkus1F0HfToHEBjB6tkZyydreIK3Zi8u0l1YvT3VRtWMw3amc3s7f86ilZZ_uoGhW8fc7k14JmubAX.FiOfKdbaYf0TStGFpmhDSw/history_1.hs\n# !wget https://www.kaggleusercontent.com/kf/31703703/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..IWrH9XKSk0oDOCNg7wTlbg.A49rZubI479_e2da4l7JhZBGm7YdJ87UviDnyAP6924Kez0dRIPjzw-67wCI_iMtJGQQJrnn74pVwzt0cPa8CVTJIApHs2t4qZd-7dlWDcOqcwlC8zaLECXT836Jey6auUY1JE1C8YApSHBfDr-WB_KO75JQRzDdXV6LFEdPfzTyoXWf0zmuyg4bi41rPhH2UkhEfPowmze9G_nOWg64WHfJTBCcWzMDf56GbyNmp46RtdKuqriMH0sAHHFrz94DQkHnnz0U149yYC6oUkqlvty-l7jvEGM8Nc1_-LZbptB_8cZxu9gOFYprjdUyAerb5Izz-J1nkextV-0OUe8SUYj8XwG5pF6pNDbZrkoDTurkkORlMIEAKNhl6SZvPjmb1Mv-owxnrpSdHnHejp61kFp-FOi_oFFzAJiW5NVm8pmMln7RP3JgcuCfWKQUEHlrsC5NaMvsFG9CNlXeul9E4PXS6ycuRzflz5uWxvoe1Dt-Tm4Pnoaa-CvJ2hsLKsX-oH66LE6sWQm8UrDTJ0eBxaWG8QAbJ6WeOICv9zykRx8ro5j18RWgnOLK2x9PLJmYA7rL6Zm7kxavLQMTovSg-IZ9EyuTuSmGv-0Rd_0Gmb5-2qvBsYKGcn8lbJMmOXfeDWEqTT0XBK-VfVcDNxX8EZd2iYD1c3nJa6R324v4JGTOLHZJbL7UNtGXtQn-3WVc.ShxYOf_zson_2CAlaDOT8w/history_2.hs\n!wget https://www.kaggleusercontent.com/kf/32045042/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..LcCpstRicdTOqMGGjh1wmg.LJi-FKBMbLQL5cc2XHtVyNpT--iVgBeKlbby9C_YjBjm7dSLbbqY4F27EpcgOWZYo22EOWWwyVMAd7VsYOzQkUGK-sj6gvWswv9CTc8I8ZDMkZvGdRRy3COevzGk21yXTlbRG1D--BVHlwCZVaYSMBKt9gIeCELgl-ZcFKoOjZSUpcZXBrmfuBA_OK63fc_olldErb5p5S_qWVecdJ1r49anGVs-x682Q0y5cCs9lr7q1n2B2saWkiC4d8tMGRMyUTCt1xXl6nENaYNh4C4u0DGOBfT14jEuWfyzFH3obgs_TtD4Bqg2zzxjjUwtJd7ymBNCVq7td-_chLlds-lps5uq4BLckniKl9QgbJ7ZcL6qoMonLzPn5VUE-vlIfUCfBieYOHKkWb3Buz557IrG4mk9GqAvOPp_Vzd5n9h9SGEvOFCsiG-_IinSIA7a2NxSPvtNEKmYcG37CS0xLfEV8copUh3pIHdVVLn9SXd2r5Wg2Q5q8zUnEvnHSxcPNg7LheviS4NMX18bsL9Wqm2PP_WDd7QkO_wJ0AsGyDveLWKebHB26wS_iz3w7X-EAheRmBrCyiUfJWHSM-IJPAJk9EIhRH1zr8WVtwsc19PRevFcY4RDHx7fyvPFVrJ2fFsicZMaDfJchFR_lP_uYDs6NvPu5AXXEkXnFMsrJ0IAICiEDwadnj0eV5w5b-DozhrP.8pmfOFY56dYt_Z2UVg-UWA/model_img_augmentation.h5","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Model Using Keras\n\nThere are two different ways of defining the Model in Keras:\n* Sequential Model\n* Function API\n\nFunctional API is used to build a more complicated Model such as for multi-output Models, directed acyclic graphs, or models with shared layers. I am using the Sequential Model in this notebook to keep things simple. \n<br>\nIn Sequential Model, you can add each layer sequentially.<br>\n\n**Description of Model:**\n* 2 Convolutional Blocks\n    > Each block consists of 2 Conv2D layers with LeakyRelU activation layers. Then a MaxPool2D layer and finally a Dropout Layer. \n* Then Dense Layers and Output layer after Flatten layer.\n* MaxPool2D layer is used to reduce the size of the image. Pool size `(2,2)` means reducing the image from `(28,28)` to `(14,14)`. Reducing the features.\n* Dropout layer drops the few activation nodes while training, which acts as regularization. Do let the model to over-fit.\n* Output layer has 10 nodes with sigmoid activation.\n\n**Check out these functions for more info:**\n* [Conv2D](https://keras.io/layers/convolutional/#conv2d)\n* [LeakyReLU](https://keras.io/layers/advanced-activations/#leakyrelu)\n* [MaxPool2D](https://keras.io/layers/pooling/#maxpool2d)\n* [Dropout](https://keras.io/layers/core/#dropout)\n* [Flatten](https://keras.io/layers/core/#flatten)\n* [Dense](https://keras.io/layers/core/#dense)","metadata":{}},{"cell_type":"code","source":"model = models.Sequential()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Block 1\nmodel.add(Conv2D(32,3, padding  =\"same\",input_shape=(28,28,1)))\nmodel.add(LeakyReLU())\nmodel.add(Conv2D(32,3, padding  =\"same\"))\nmodel.add(LeakyReLU())\nmodel.add(MaxPool2D(pool_size=(2,2)))\nmodel.add(Dropout(0.25))\n\n# Block 2\nmodel.add(Conv2D(64,3, padding  =\"same\"))\nmodel.add(LeakyReLU())\nmodel.add(Conv2D(64,3, padding  =\"same\"))\nmodel.add(LeakyReLU())\nmodel.add(MaxPool2D(pool_size=(2,2)))\nmodel.add(Dropout(0.25))\n\nmodel.add(Flatten())\n\nmodel.add(Dense(256,activation='relu'))\nmodel.add(Dense(32,activation='relu'))\nmodel.add(Dense(10,activation=\"sigmoid\"))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Compiling Model\nModel compilation required the selection of optimizer and loss function. Let me discuss a few important things to avoid confusion.\n\n**Optimizers:** Keras provides several optimizers that can be used by importing the optimizers and passing in compile function.\n* SGD (Stochastic gradient descent optimizer)\n* RMSprop \n* Adam\n* Adamax\n\nThere are several more check it out [here](https://keras.io/optimizers/). \n<br>\n\nOne of the important things is the selection of the **learning rate**. If the learning rate is too high, the loss may not converge, and if it is too low, the training will be slow. So it is important to select the reasonably fair value of learning rate. One of the good value to start with is 0.001. If it doesn't work, then other higher or lower values can be tried. The rest of the parameters generally works well and need not be defined. The default value works most of the time.\nHere, I am using **Adam optimizer**, but **RMSprop** can also be used.\n<br>\n\n**Loss Functions:** Keras provides all of the well-known loss functions which work well for most of the time. But if you need to define a custom loss function, you can. Defining a custom function is out of the scope of this notebook. Let's understand the loss function for the classification tasks. I am discussing 3 loss function here. There are several more check it out [here](https://keras.io/losses/). \n* **binary_crossentropy**: This loss function is used for the binary classification task. The single-node output layer is required. 0 and 1 is used for classification.\n* **categorical_crossentropy**:  Used for Used for Multi-class classification \n* **sparse_categorical_crossentropy:** Used for Multi-class classification. <br>\n\n**Difference between categorical_crossentropy and sparse_categorical_crossentropy:**\n* If your targets are one-hot encoded, use categorical_crossentropy.\n    Examples of one-hot encodings (for three classes):\n    * [1,0,0]\n    * [0,1,0]\n    * [0,0,1]\n* But if your targets are integers, use sparse_categorical_crossentropy.\n    Examples of integer encodings (for the sake of completion):\n    * 1\n    * 2\n    * 3\n    \nHere, I am using **sparse_categorical_crossentropy**, as the target values are integer not **one-hot vector**.\n","metadata":{}},{"cell_type":"code","source":"initial_lr = 0.001\nloss = \"sparse_categorical_crossentropy\"\nmodel.compile(Adam(lr=initial_lr), loss=loss ,metrics=['accuracy'])\nmodel.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Training \n[model.fit()](https://keras.io/models/sequential/#fit) is used to train the model. It takes training data, batch_size, no of epochs, validation data. There are several more parameters, and you can check the documentation [here](https://keras.io/models/sequential/#fit).\nI am taking the epochs = 20 and batch size = 256. It will return the history of training, which later can be used to analyze the performance.","metadata":{}},{"cell_type":"code","source":"epochs = 20\nbatch_size = 256\nhistory_1 = model.fit(train_x,train_y,batch_size=batch_size,epochs=epochs,validation_data=(val_x,val_y))","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Training Performance.\nLet's see how the training goesâ€”plotting the accuracy and loss of both training and validation set with each epoch. In the accuracy graphs, there is clearly a difference in the training and validation set. The model is more accurate on the training set. It seems that the model is a little bit **overfit**. So can we do better?","metadata":{}},{"cell_type":"code","source":"# Diffining Figure\nf = plt.figure(figsize=(20,7))\n\n#Adding Subplot 1 (For Accuracy)\nf.add_subplot(121)\n\nplt.plot(history_1.epoch,history_1.history['accuracy'],label = \"accuracy\") # Accuracy curve for training set\nplt.plot(history_1.epoch,history_1.history['val_accuracy'],label = \"val_accuracy\") # Accuracy curve for validation set\n\nplt.title(\"Accuracy Curve\",fontsize=18)\nplt.xlabel(\"Epochs\",fontsize=15)\nplt.ylabel(\"Accuracy\",fontsize=15)\nplt.grid(alpha=0.3)\nplt.legend()\n\n#Adding Subplot 1 (For Loss)\nf.add_subplot(122)\n\nplt.plot(history_1.epoch,history_1.history['loss'],label=\"loss\") # Loss curve for training set\nplt.plot(history_1.epoch,history_1.history['val_loss'],label=\"val_loss\") # Loss curve for validation set\n\nplt.title(\"Loss Curve\",fontsize=18)\nplt.xlabel(\"Epochs\",fontsize=15)\nplt.ylabel(\"Loss\",fontsize=15)\nplt.grid(alpha=0.3)\nplt.legend()\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Confusion Matrix\nIn the field of **machine learning** and specifically the problem of **statistical classification**, a confusion matrix, also known as an **error matrix**, is a specific table layout that **allows visualization of the performance of an algorithm**, typically a supervised learning one (in unsupervised learning it is usually called a matching matrix). Each row of the matrix represents the instances in a predicted class, while each column represents the instances in an actual class (or vice versa). The name stems from the fact that it makes it easy to see if the system is confusing two classes (i.e. commonly mislabeling one as another). [Source](https://en.wikipedia.org/wiki/Confusion_matrix)\n\nLet's try to see how well the model is performing on the validation set.","metadata":{}},{"cell_type":"code","source":"val_p = np.argmax(model.predict(val_x),axis =1)\n\nerror = 0\nconfusion_matrix = np.zeros([10,10])\nfor i in range(val_x.shape[0]):\n    confusion_matrix[val_y[i],val_p[i]] += 1\n    if val_y[i]!=val_p[i]:\n        error +=1\n        \nprint(\"Confusion Matrix: \\n\\n\" ,confusion_matrix)\nprint(\"\\nErrors in validation set: \" ,error)\nprint(\"\\nError Persentage : \" ,(error*100)/val_p.shape[0])\nprint(\"\\nAccuracy : \" ,100-(error*100)/val_p.shape[0])\nprint(\"\\nValidation set Shape :\",val_p.shape[0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f = plt.figure(figsize=(10,8.5))\nf.add_subplot(111)\n\nplt.imshow(np.log2(confusion_matrix+1),cmap=\"Reds\")\nplt.colorbar()\nplt.tick_params(size=5,color=\"white\")\nplt.xticks(np.arange(0,10),np.arange(0,10))\nplt.yticks(np.arange(0,10),np.arange(0,10))\n\nthreshold = confusion_matrix.max()/2 \n\nfor i in range(10):\n    for j in range(10):\n        plt.text(j,i,int(confusion_matrix[i,j]),horizontalalignment=\"center\",color=\"white\" if confusion_matrix[i, j] > threshold else \"black\")\n        \nplt.xlabel(\"Predicted\")\nplt.ylabel(\"Actual\")\nplt.title(\"Confusion Matrix\")\nplt.savefig(\"Confusion_matrix1.png\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Improving Result by Image Augmentation\n\n<hr>\n\n![augmentation](https://cdn-images-1.medium.com/max/1000/1*C8hNiOqur4OJyEZmC7OnzQ.png)\n\nA deep network requires extensive data to achieve decent performance. To build a good classifier with small training data, image augmentation can solve the problem to a greater extend. Image augmentation generates images by different ways of processing, such as random shift, rotation, flips, etc. \n<br>\n\nYou can check great python package for data [augmentation](https://github.com/albumentations-team/albumentations).\n\n![Augmentation](https://camo.githubusercontent.com/43d652646b37ef66762212c0e0d3150ba481347c/68747470733a2f2f686162726173746f726167652e6f72672f776562742f73752f77612f6e702f737577616e70656f36777737777077746f6274727a645f636732302e6a706567)*Image [source](https://github.com/albumentations-team/albumentations)* \n\n\n### Augmentation Using Keras\n\n<hr>\n\nHere I am using the ImageDataGenerator() function of Keras for Image augmentation. Parameters to use:\n\n* **rotation_range:**   randomly rotate images in the range (degrees, 0 to 180)\n* **zoom_range:**  Randomly zoom image \n* **width_shift_range:**  randomly shift images horizontally (fraction of total width)\n* **height_shift_range:**   randomly shift images vertically (fraction of total height)\n* **horizontal_flip:**   randomly flip images (Can't be used in this case as it changes the digit)\n* **vertical_flip:**  randomly flip images (Can't be used in this case as it changes the digit)\n\nRead more about [ImageDataGenerator()](https://keras.io/preprocessing/image/)\n\nAfter the creation and configuration of the ImageDataGenerator, you must fit it on the data, which calculates any statistics required to perform the transformation on the data. This can be done by calling the fit() function on datagen.","metadata":{}},{"cell_type":"code","source":"datagen = ImageDataGenerator(\n        featurewise_center=False,  # set input mean to 0 over the dataset\n        samplewise_center=False,  # set each sample mean to 0\n        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n        samplewise_std_normalization=False,  # divide each input by its std\n        zca_whitening=False,  # apply ZCA whitening\n        rotation_range=10,  # randomly rotate images in the range (degrees, 0 to 180)\n        zoom_range = 0.1, # Randomly zoom image \n        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n        horizontal_flip=False,  # randomly flip images\n        vertical_flip=False)  # randomly flip images\n\ndatagen.fit(train_x)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Learning Rate\n\n<hr>\n\nReduceLROnPlateau() is a callback function provided by Keras, which is used to reduce the learning rate if when a metric has stopped improving.\n\n<br>\n\n**Parameters:**\n\n* **monitor:**  takes the metric to observe (In this case val_accuracy)\n* **patience:** waits for that much epochs for the improvements, if not, then decrease the learning rate. (here `2`)\n* **factor:** factor by which the learning rate will be reduced. new_lr = lr * factor (here `0.5`)\n* **min_lr:** lower bound on the learning rate. (here `0.00001`)\n\n**Read More:**\n[Read more](https://keras.io/callbacks/#reducelronplateau)\n\n","metadata":{}},{"cell_type":"code","source":"lrr = ReduceLROnPlateau(monitor='val_accuracy',patience=2,verbose=1,factor=0.5, min_lr=0.00001)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Further Traning\n[model.fit_generator()](https://keras.io/models/sequential/#fit_generator) is used to train the model on data generated batch-by-batch by image augmentation. The data generator is an iterator that generates and provides data as per request by fit_generator(). We can configure the batch size and get the batches by calling the flow() function.\n\n","metadata":{}},{"cell_type":"code","source":"epochs = 20\nhistory_2 = model.fit_generator(datagen.flow(train_x,train_y, batch_size=batch_size),steps_per_epoch=int(train_x.shape[0]/batch_size)+1,epochs=epochs,validation_data=(val_x,val_y),callbacks=[lrr])","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Training Performance.\nNow we can see that after further training, the accuracy of training and validation set almost converges with high accuracy. It seems that the model has been significantly improved after image augmentation.","metadata":{}},{"cell_type":"code","source":"# Diffining Figure\nf = plt.figure(figsize=(20,7))\nf.add_subplot(121)\n\n#Adding Subplot 1 (For Accuracy)\nplt.plot(history_1.epoch+list(np.asarray(history_2.epoch) + len(history_1.epoch)),history_1.history['accuracy']+history_2.history['accuracy'],label = \"accuracy\") # Accuracy curve for training set\nplt.plot(history_1.epoch+list(np.asarray(history_2.epoch) + len(history_1.epoch)),history_1.history['val_accuracy']+history_2.history['val_accuracy'],label = \"val_accuracy\") # Accuracy curve for validation set\n\nplt.title(\"Accuracy Curve\",fontsize=18)\nplt.xlabel(\"Epochs\",fontsize=15)\nplt.ylabel(\"Accuracy\",fontsize=15)\nplt.grid(alpha=0.3)\nplt.legend()\n\n\n#Adding Subplot 1 (For Loss)\nf.add_subplot(122)\n\nplt.plot(history_1.epoch+list(np.asarray(history_2.epoch) + len(history_1.epoch)),history_1.history['loss']+history_2.history['loss'],label=\"loss\") # Loss curve for training set\nplt.plot(history_1.epoch+list(np.asarray(history_2.epoch) + len(history_1.epoch)),history_1.history['val_loss']+history_2.history['val_loss'],label=\"val_loss\") # Loss curve for validation set\n\nplt.title(\"Loss Curve\",fontsize=18)\nplt.xlabel(\"Epochs\",fontsize=15)\nplt.ylabel(\"Loss\",fontsize=15)\nplt.grid(alpha=0.3)\nplt.legend()\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Confusion Matrix","metadata":{}},{"cell_type":"code","source":"val_p = np.argmax(model.predict(val_x),axis =1)\n\nerror = 0\nconfusion_matrix = np.zeros([10,10])\nfor i in range(val_x.shape[0]):\n    confusion_matrix[val_y[i],val_p[i]] += 1\n    if val_y[i]!=val_p[i]:\n        error +=1\n        \nconfusion_matrix,error,(error*100)/val_p.shape[0],100-(error*100)/val_p.shape[0],val_p.shape[0]\n\nprint(\"Confusion Matrix: \\n\\n\" ,confusion_matrix)\nprint(\"\\nErrors in validation set: \" ,error)\nprint(\"\\nError Persentage : \" ,(error*100)/val_p.shape[0])\nprint(\"\\nAccuracy : \" ,100-(error*100)/val_p.shape[0])\nprint(\"\\nValidation set Shape :\",val_p.shape[0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f = plt.figure(figsize=(10,8.5))\nf.add_subplot(111)\n\nplt.imshow(np.log2(confusion_matrix+1),cmap=\"Reds\")\nplt.colorbar()\nplt.tick_params(size=5,color=\"white\")\nplt.xticks(np.arange(0,10),np.arange(0,10))\nplt.yticks(np.arange(0,10),np.arange(0,10))\n\nthreshold = confusion_matrix.max()/2 \n\nfor i in range(10):\n    for j in range(10):\n        plt.text(j,i,int(confusion_matrix[i,j]),horizontalalignment=\"center\",color=\"white\" if confusion_matrix[i, j] > threshold else \"black\")\n        \nplt.xlabel(\"Predicted\")\nplt.ylabel(\"Actual\")\nplt.title(\"Confusion Matrix\")\nplt.savefig(\"Confusion_matrix2.png\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Visualizing Result\n\n<hr>\n\n### All Errors in the Validation set\nLet's see all the errors in the validation set. It seems that in most of the cases, the recognition of digits is difficult for even humans. So we can say that our model is performing well.","metadata":{}},{"cell_type":"code","source":"rows = 4\ncols = 9\n\nf = plt.figure(figsize=(2*cols,2*rows))\nsub_plot = 1\nfor i in range(val_x.shape[0]):\n    if val_y[i]!=val_p[i]:\n        f.add_subplot(rows,cols,sub_plot) \n        sub_plot+=1\n        plt.imshow(val_x[i].reshape([28,28]),cmap=\"Blues\")\n        plt.axis(\"off\")\n        plt.title(\"T: \"+str(val_y[i])+\" P:\"+str(val_p[i]), y=-0.15,color=\"Red\")\nplt.savefig(\"error_plots.png\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Predict on Testset","metadata":{}},{"cell_type":"code","source":"test_y = np.argmax(model.predict(test_x),axis =1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rows = 5\ncols = 10\n\nf = plt.figure(figsize=(2*cols,2*rows))\n\nfor i in range(rows*cols):\n    f.add_subplot(rows,cols,i+1)\n    plt.imshow(test_x[i].reshape([28,28]),cmap=\"Blues\")\n    plt.axis(\"off\")\n    plt.title(str(test_y[i]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Lets understand the intermediate layers of the model\n\n<hr>\n\n![hidden layers](https://i.stack.imgur.com/axn7z.jpg)\n\nTo visualize the output of each layer, we need to create a model to take input tensor and gives the list of output tensor, each representing the corresponding intermediate layers. To that, we need to create a multi-output model in which the input will be the image, and the output will be the list of intermediate layers.\n\n<br>\n\nI will use the Functional API of Keras to do so. When fed an image input, this model returns the values of the layer activations in the original model. \n<br>\nI am taking all the intermediate layers except the Flatten and Dense layers. \n<br>\n\n**model.layers** returns the list of layers of the model. Selecting all the layers except the last four layers. Then passing to **models.Model()** as a list of output layers and the input layer of the original model. It will return a new model having the input and output layers of the original model. \n\n\n\n**Check out these functions for more info:**\n* [Model()](https://keras.io/models/model/)","metadata":{}},{"cell_type":"code","source":"# Extracts the outputs of all layers except Flatten and Dense layers\noutput_layers = [layer.output for layer in model.layers[:-4]]\n# Creates a model that will return these outputs, given the model input (This is multi output model)\nactivation_model = models.Model(inputs=model.input, outputs=output_layers)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Try to compare the layer 1 output \n<br>","metadata":{}},{"cell_type":"code","source":"# predicting the output of each layers\nactivations_2  = activation_model.predict(val_x[2].reshape([1,28,28,1]))\nactivations_6  = activation_model.predict(val_x[7].reshape([1,28,28,1]))\nfirst_activation_layer  = activations_2[0]\nfirst_activation_layer.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**activation_2** and **activation_6** are the output of two different images. **first_activation_layer** represents the output of the first layer of the original model. The shape is (1,28,28,32) where 32 represent the number of **channels**. Let's compare the four different channels of the first layer of two different images.","metadata":{}},{"cell_type":"code","source":"rows = 4\ncols = 2\n\nf = plt.figure(figsize=(2*cols,2*rows))\n\nfor i in range(4):\n    f.add_subplot(rows,cols,2*i+1)\n    plt.imshow(activations_2[0][0,:,:,i].reshape([28,28]),cmap=\"Blues\")\n    plt.axis(\"off\") \n\n    f.add_subplot(rows,cols,2*i+2)\n    plt.imshow(activations_6[0][0,:,:,i].reshape([28,28]),cmap=\"Blues\")\n    plt.savefig(\"layer_output_comparision\"+str(i)+\".png\")\n    plt.axis(\"off\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It seems that each channel is trying to depict something in both images, a kind of similar features. This can be concluded that each channel has been trained to find some specific features of the input images.\n\n## Visualizing All Intermediate Activation Layer","metadata":{}},{"cell_type":"code","source":"def plot_layer(layer,i,layer_name = None):\n    rows = layer.shape[-1]/16\n    cols = 16\n\n    f = plt.figure(figsize=(1*cols,1*rows))\n    # plt.imshow(first_activation_layer[0,:,:,:].reshape([14*4,14*16]),cmap=\"Blues\")\n    for i in range(layer.shape[-1]):\n        f.add_subplot(rows,cols,i+1)\n        plt.imshow(layer[0,:,:,i].reshape([layer.shape[2],layer.shape[2]]),cmap=\"Blues\")\n        plt.axis(\"off\")\n    f.suptitle(layer_name,fontsize=14)\n    plt.savefig(\"intermidiate_layers\"+str(i)+\".png\")\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Visualising each layers\nfor i,layer in enumerate(activation_model.predict(val_x[6].reshape([1,28,28,1]))):\n    plot_layer(layer,i,output_layers[i].name)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So, try to understand what's happening inside the hidden layers:\n* The first few layers retaining the shape of image but trying to get the very low-level features such as different types edges.\n* As we go into the deeper layers, the activations are more abstract and less visually interpretable. These layers are trying to encode high-level features such as corners, angles, selective borders, etc.\n* In the last few layers, We can't visually interpret anything; this is because layers are now encoding even more complex features, or we can say more information about the classes.\n\nI hope you understand the basics of CNN after the visualization of intermediate layers even more.","metadata":{}},{"cell_type":"markdown","source":"# Creating submisson","metadata":{}},{"cell_type":"code","source":"df_submission = pd.DataFrame([df_test.index+1,test_y],[\"ImageId\",\"Label\"]).transpose()\ndf_submission.to_csv(\"submission.csv\",index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# About Me\n<hr>\nI am Souvik Ganguly from India.<br>\nTechnology Analyst(Data Science) at Infosys.<br>\nPre-Covid Standup Comedian. <br>\nMachine Learning and Data Science. \n<br>\n<br>\n\n### Follow me:\n\n* <a href=\"https://github.com/ds-souvik\"><img src=\"data:image/svg+xml;base64,PHN2ZyBlbmFibGUtYmFja2dyb3VuZD0ibmV3IDAgMCAyNCAyNCIgaGVpZ2h0PSI1MTIiIHZpZXdCb3g9IjAgMCAyNCAyNCIgd2lkdGg9IjUxMiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj48cGF0aCBkPSJtMTIgLjVjLTYuNjMgMC0xMiA1LjI4LTEyIDExLjc5MiAwIDUuMjExIDMuNDM4IDkuNjMgOC4yMDUgMTEuMTg4LjYuMTExLjgyLS4yNTQuODItLjU2NyAwLS4yOC0uMDEtMS4wMjItLjAxNS0yLjAwNS0zLjMzOC43MTEtNC4wNDItMS41ODItNC4wNDItMS41ODItLjU0Ni0xLjM2MS0xLjMzNS0xLjcyNS0xLjMzNS0xLjcyNS0xLjA4Ny0uNzMxLjA4NC0uNzE2LjA4NC0uNzE2IDEuMjA1LjA4MiAxLjgzOCAxLjIxNSAxLjgzOCAxLjIxNSAxLjA3IDEuODAzIDIuODA5IDEuMjgyIDMuNDk1Ljk4MS4xMDgtLjc2My40MTctMS4yODIuNzYtMS41NzctMi42NjUtLjI5NS01LjQ2Ni0xLjMwOS01LjQ2Ni01LjgyNyAwLTEuMjg3LjQ2NS0yLjMzOSAxLjIzNS0zLjE2NC0uMTM1LS4yOTgtLjU0LTEuNDk3LjEwNS0zLjEyMSAwIDAgMS4wMDUtLjMxNiAzLjMgMS4yMDkuOTYtLjI2MiAxLjk4LS4zOTIgMy0uMzk4IDEuMDIuMDA2IDIuMDQuMTM2IDMgLjM5OCAyLjI4LTEuNTI1IDMuMjg1LTEuMjA5IDMuMjg1LTEuMjA5LjY0NSAxLjYyNC4yNCAyLjgyMy4xMiAzLjEyMS43NjUuODI1IDEuMjMgMS44NzcgMS4yMyAzLjE2NCAwIDQuNTMtMi44MDUgNS41MjctNS40NzUgNS44MTcuNDIuMzU0LjgxIDEuMDc3LjgxIDIuMTgyIDAgMS41NzgtLjAxNSAyLjg0Ni0uMDE1IDMuMjI5IDAgLjMwOS4yMS42NzguODI1LjU2IDQuODAxLTEuNTQ4IDguMjM2LTUuOTcgOC4yMzYtMTEuMTczIDAtNi41MTItNS4zNzMtMTEuNzkyLTEyLTExLjc5MnoiIGZpbGw9IiMyMTIxMjEiLz48L3N2Zz4=\" width=\"22\" align=\"left\" style=\"margin-right:10px\"/> GitHub</a>\n* <a href=\"https://medium.com/@souvik.ganguly.ds\"><img src=\"data:image/svg+xml;base64,PHN2ZyBlbmFibGUtYmFja2dyb3VuZD0ibmV3IDAgMCAyNCAyNCIgaGVpZ2h0PSI1MTIiIHZpZXdCb3g9IjAgMCAyNCAyNCIgd2lkdGg9IjUxMiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj48cGF0aCBkPSJtMjIuMDg1IDQuNzMzIDEuOTE1LTEuODMydi0uNDAxaC02LjYzNGwtNC43MjggMTEuNzY4LTUuMzc5LTExLjc2OGgtNi45NTZ2LjQwMWwyLjIzNyAyLjY5M2MuMjE4LjE5OS4zMzIuNDkuMzAzLjc4M3YxMC41ODNjLjA2OS4zODEtLjA1NS43NzMtLjMyMyAxLjA1bC0yLjUyIDMuMDU0di4zOTZoNy4xNDV2LS40MDFsLTIuNTItMy4wNDljLS4yNzMtLjI3OC0uNDAyLS42NjMtLjM0Ny0xLjA1di05LjE1NGw2LjI3MiAxMy42NTloLjcyOWw1LjM5My0xMy42NTl2MTAuODgxYzAgLjI4NyAwIC4zNDYtLjE4OC41MzRsLTEuOTQgMS44Nzd2LjQwMmg5LjQxMnYtLjQwMWwtMS44Ny0xLjgzMWMtLjE2NC0uMTI0LS4yNDktLjMzMi0uMjE0LS41MzR2LTEzLjQ2N2MtLjAzNS0uMjAzLjA0OS0uNDExLjIxMy0uNTM0eiIgZmlsbD0iIzIxMjEyMSIvPjwvc3ZnPg==\"  width=\"22\" align=\"left\" style=\"margin-right:10px\"/> Medium</a>\n* <a href=\"https://www.linkedin.com/in/souvik-ganguly-4a9924105/\"><img src=\"data:image/svg+xml;base64,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\" width=\"22\" align=\"left\" style=\"margin-right:10px\"/>Linkedin</a>\n\n\n<hr>\n\n# Feedback\n* **Your feedback is much appreciated**\n* **Please UPVOTE if you LIKE this notebook**\n* **Comment if you have any doubts or you found any errors in the notebook**","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}