{"cells":[{"metadata":{},"cell_type":"markdown","source":"Contents\n==\n\n[Introduction to The Kernel](#intro)<br/>\n[1. Preprocessing Data](#pre) <br/>\n&emsp;&emsp; [1.1 Reshaping Data](#reshape) <br/>\n&emsp;&emsp; [1.2 Normalizing Data](#norm) <br/>\n&emsp;&emsp; [1.3 Train-Test Split](#split) <br/>\n&emsp;&emsp; [1.4 Augmenting Data](#augment)<br/>\n[2. Building The Model](#model) <br/>\n&emsp;&emsp; [2.1 Model Architecture](#arch)<br/>\n&emsp;&emsp; [2.2 Hyperparameters Tuning](#params)<br/>\n[3. Model Evaluation](#eval) <br/>\n&emsp;&emsp; [3.1 Confusion Matrix](#metrics)<br/>\n[4. Error Analysis](#error)<br/>\n[5. General Notes](#notes)<br/>\n[6. Submission](#submit)"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"intro\"></a>\n# Introduction to The Kernel\n"},{"metadata":{},"cell_type":"markdown","source":"Instead of using a MLP, I'll preprocesss the data from MNIST to attain the original images from the unravelled pixels and use a CNN as my model.\nCNNs are the best choice when it comes to image classification or any computer vision task, as well as providing relatively small number of weight parameters compared to MLP\n"},{"metadata":{},"cell_type":"markdown","source":"## Steps of The Kernel\n1. Preprocessing the data, which includes reshaping, normalization and images augmentation\n2. Building the model, trying different architectures and hyperparameters tuning\n3. Model Evaluation and calculating the metrics of the model's performance\n4. Error analysis and see how to improve furture performace\n5. General notes on the model and MNIST dataset"},{"metadata":{},"cell_type":"markdown","source":"## Importing Python Modules\n* **Data Handling:** numpy, pandas\n* **DL Model Building:** keras \n* **Preprocessing:** keras, sklearn\n* **Evaluation Metrics:** sklearn\n* **Visualization:** matplotlib, seaborn, IPython.display"},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"# import NumPy and Panas\nimport numpy as np \nimport pandas as pd\n\n# import Keras and its layers\nimport keras\nfrom keras import Model\nfrom keras.layers import Dense, Conv2D, pooling, Activation, BatchNormalization, Input, Flatten, Dropout, MaxPooling2D\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.callbacks import ReduceLROnPlateau\n\n# import preprocessing functions and metrics\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, f1_score\n\n# import matplotlib and seaborn for visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom IPython.display import clear_output\n\n# import time for checkpoint saving\nimport time","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Firstly, let's use pandas to load the data and see how is it structured"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"train_data = pd.read_csv('../input/train.csv')\ntest_data = pd.read_csv('../input/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.set_option('display.max_columns', 6)\nprint(train_data.head(10), '\\n')\nprint(test_data.head(10), '\\n')\n\nprint(f'train data shape: {train_data.shape}')\nprint(f'test data shape: {test_data.shape}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The test dataset here is unlabelled and can only be used for the kernel submission, so we can only judge the model's performance on it manually."},{"metadata":{},"cell_type":"markdown","source":"<a id=\"pre\"></a>\n# 1. Preprocessing Data\n"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"reshape\"></a>\n## 1.1 Reshaping Data\n\nThe first step to do is to reshape the data to obtain the original images\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# storing the labeled dataset in numpy arrays\ndataset_x = np.array(train_data.iloc[:, 1:]).reshape(42000, 28, 28, 1)\ndataset_y = np.array(train_data.iloc[:, 0]).ravel()\n\n# one-hot encoded labels instead of digits to use in categorical/multi-class classification\ndataset_one_hot = keras.utils.to_categorical(dataset_y, num_classes=10)\n\n# storing the unlabelled dataset in a numpy array for manual model testing\nunlabelled_test_x = np.array(test_data.iloc[:, :]).reshape(28000, 28, 28, 1)\n\n# make sure that the number of examples equals the number of labels after reshaping\nassert(dataset_x.shape[0] == dataset_y.size)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"To make sure that our dataset has almost equal representation (equal number of examples) for all classes so that the model doesn't train on one class over another, we'll plot a countplot of the labels in the dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"# To get more control over visualization we'll define our figure instead of simply using plt.\nfig = plt.figure(figsize=(8, 8))  # create figure\nax = fig.add_subplot(111)  # add subplot\n\nsns.countplot(dataset_y);\nax.set_title(f'Labelled Digits Count ({dataset_y.size} total)', size=20);\nax.set_xlabel('Digits');\n\n# writes the counts on each bar\nfor patch in ax.patches:\n        ax.annotate('{:}'.format(patch.get_height()), (patch.get_x()+0.1, patch.get_height()-150), color='w')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"norm\"></a>\n## 1.2 Normalizing Data\n\nNext step is to normalize the data, so that they get a value between 0 and 1.\n\nThis will make the model train more efficiently as it will reduce the variance in the dataset distribution, so the activations of different examples won't have big differences.\n\nThink about it as if the image lies in 784-dim space (Features Space) and by normalizing we are making all the images bound by a hypersphere of radius 1"},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset_x = dataset_x / 255.0\nunlabelled_test_x = unlabelled_test_x / 255.0","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's see how the images look after reshaping."},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axs = plt.subplots(1, 5, figsize=(25, 5))\nindexes = np.random.choice(range(dataset_x.shape[2]), size=5)  # returns random 5 indexes\n\nfig.suptitle('Original Images of MNIST', size=32)\nfor idx, ax in zip(indexes, axs):\n    ax.imshow(dataset_x[idx,:, :, 0], cmap='gray');\n    ax.set_title(f'Label: {dataset_y[idx]}', size= 20);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"split\"></a>\n## 1.3 Train-Test Split\n\nSince the test dataset is unlabelled we'll be taking a portion of the labelled dataset to use as a test set and to evaluate the model's metrics on."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_x, test_x, train_y, test_y = train_test_split(dataset_x, dataset_one_hot, test_size=0.1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since the dataset size is small we'll use an augmented images generator to give images from the same distribution as the examples in the train dataset but not necessairly the original examples."},{"metadata":{},"cell_type":"markdown","source":"<a id=\"augment\"></a>\n## 1.4 Augmenting Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Augmentation Ranges\n# Large values might lead to insignificant outliers or noisy examples\ntransform_params = {\n    'featurewise_center': False,\n    'featurewise_std_normalization': False,\n    'samplewise_center': False,\n    'samplewise_std_normalization': False,\n    'rotation_range': 10, \n    'width_shift_range': 0.1, \n    'height_shift_range': 0.1,\n#     'shear_range': 0.15, \n    'zoom_range': 0.1,\n    'validation_split': 0.15\n}\nimg_gen = ImageDataGenerator(**transform_params) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The augmentation generator will only use the train portion we got from the split.\nThe model must never learn from the test images or even their augmentations, to have a meaningful performance and error analysis."},{"metadata":{"trusted":true},"cell_type":"code","source":"# used only to visualize the augmentations\nvisualizaion_flow = img_gen.flow(train_x, train_y, batch_size=1, shuffle=False)\n\n# used to feed the model augmented training data\ntrain_gen = img_gen.flow(x=train_x, y=train_y, subset='training', batch_size=96)\n\n# used to feed the model augmented validation data\nvalid_gen = img_gen.flow(x=train_x, y=train_y, subset='validation', batch_size=96)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The valid_gen used for validation will also use the training dataset from the split, as we said before that the model must never learn from the test data.\n\nSo, we are performing another split of the labelled dataset."},{"metadata":{},"cell_type":"markdown","source":"Now, let's visualize how the generator augments the original images"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axs = plt.subplots(2, 4, figsize=(20,10))  # let's see 4 augmentation examples\nfig.suptitle('Augmentation Results', size=32)\n\nfor axs_col in range(axs.shape[1]):\n    idx = np.random.randint(0, train_x.shape[0])  # get a random index\n    img = train_x[idx,:, :, 0]  # the original image\n    aug_img, aug_label = visualizaion_flow[idx]  # the same image after augmentation\n    \n    axs[0, axs_col].imshow(img, cmap='gray');\n    axs[0, axs_col].set_title(f'example #{axs_col} - Label: {np.argmax(train_y[idx])}', size=20)\n    \n    axs[1, axs_col].imshow(aug_img[0, :, :, 0], cmap='gray');\n    axs[1, axs_col].set_title(f'Augmented example #{axs_col}', size=20)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see the new images are very similar but not quite the original ones.\nThe model augments data randomly each time the training requires a new batch of images, so the model isn't training on the same examples exactly at each epoch which is the case if we have simply fitted the model using the original images. "},{"metadata":{},"cell_type":"markdown","source":"<a id=\"model\"></a>\n# 2. Building The Model"},{"metadata":{},"cell_type":"markdown","source":"Let's first define a Convolution Block to avoid repition while building our model.\n\nIt consists of\n1. 2D Convolution - extracts features\n2. BatchNormalization - enhances training efficiency and reduces Vanishing/Exploding Gradients effect\n3. ReLU Activation Function\n4. 2D Max Pool - optional, reduces overfitting and reduces the shape of the tensor "},{"metadata":{},"cell_type":"markdown","source":"<a id=\"arch\"></a>\n## 2.1 Model Architecture"},{"metadata":{"trusted":true},"cell_type":"code","source":"def conv_block(x, filters, kernel_size, strides, layer_no, use_pool=False, padding='same'):\n    x = Conv2D(filters=filters, kernel_size=kernel_size, strides=strides, name=f'conv{layer_no}',\n               padding=padding)(x)\n    \n    x = BatchNormalization(name=f'bn{layer_no}')(x)\n    x = Activation('relu', name=f'activation{layer_no}')(x)\n    if use_pool:\n        x = MaxPooling2D(pool_size=[2, 2], strides=[2, 2], name=f'pool{layer_no}', padding='same')(x)\n    return x","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After trying multiple architectures, the final one is\n\n\n<li> Input Layer </li>\n|\n<li> Series of ConvBloccks (conv1 to conv6)  </li>\n| \n<li> Flatten Layer </li>\n<li> Dropout  </li>\n|\n<li> Fully-Connected Layer  </li>\n<li> BatchNormalization  </li>\n<li> Dropout  </li>\n<li> ReLU Activation Function  </li>\n| \n<li> Fully-Connected Layer  </li>\n<li> BatchNormalization  </li>\n<li> Softmax Activation Function </li>"},{"metadata":{"trusted":true},"cell_type":"code","source":"def conv_model(X):\n    h, w, c = X.shape[1:]  # get shape of input (height, width, channels)\n    X = Input(shape=(h, w, c))\n    conv1 = conv_block(X, filters=8, kernel_size=[3, 3], strides=[1, 1], layer_no=1)\n    conv2 = conv_block(conv1, filters=16, kernel_size=[2, 2], strides=[1, 1], layer_no=2)\n    conv3 = conv_block(conv2, filters=32, kernel_size=[2, 2], strides=[1, 1], layer_no=3, use_pool=True)\n    \n    conv4 = conv_block(conv3, filters=64, kernel_size=[3, 3], strides=[2, 2], layer_no=4)\n    conv5 = conv_block(conv4, filters=128, kernel_size=[2, 2], strides=[1, 1], layer_no=5)\n    conv6 = conv_block(conv5, filters=256, kernel_size=[2, 2], strides=[1, 1], layer_no=6, use_pool=True)\n    \n    flat1 = Flatten(name='flatten1')(conv6)\n    drop1 = Dropout(0.35, name='Dopout1')(flat1)\n    \n    dens1 = Dense(128, name='dense1')(drop1)\n    bn7 = BatchNormalization(name='bn7')(dens1)\n    drop2 = Dropout(0.35, name='Dopout2')(bn7)\n    relu1 = Activation('relu', name='activation7')(drop2)\n    \n    dens1 = Dense(256, name='dense01')(relu1)\n    bn7 = BatchNormalization(name='bn07')(dens1)\n    drop2 = Dropout(0.5, name='Dopout02')(bn7)\n    relu1 = Activation('relu', name='activation07')(drop2)\n    \n    dens2 = Dense(10, name='dense2')(relu1)\n    bn8 = BatchNormalization(name='bn8')(dens2)\n    output_layer = Activation('softmax', name='softmax')(bn8)\n    \n    model = Model(inputs=X, outputs=output_layer)\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# create the model\nmodel = conv_model(train_x)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"params\"></a>\n## 2.2 Hyperparameters Tuning"},{"metadata":{},"cell_type":"markdown","source":"One of the hyperparameters to tune is the learning rate and the decay.\nFunnily enough, I found a decay rate equal to half of the learning rate that I used works pretty well.\nIf you want, I suggest taking a look at keras annealers to raise the efficiency of training even more."},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_model(learning_rate=0.00065):\n    optimizer = keras.optimizers.RMSprop(lr=learning_rate, decay=learning_rate/2)\n    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# not used - add to callbacks if wanted\nplateau_reduce = ReduceLROnPlateau(monitor='val_acc', factor=0.5,\n                              patience=6, min_lr=0.0000001)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# build the model computational graph and print out its summary\nbuild_model()\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"To have an updated visualization of the training process to see how it's doing over ep-ochs instead of just looking at values prointed out each epoch, we'll build a Plotter class to serve as a keras callback which keras executes it's methods on different milestones in the training."},{"metadata":{"trusted":true},"cell_type":"code","source":"class Plotter(keras.callbacks.Callback):\n    def plot(self):  # Updates the graph\n        clear_output(wait=True)\n        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 8))\n        fig.suptitle('Training Curves', size=32)\n        \n        # plot the losses\n        ax1.plot(self.epochs, self.losses, label='train_loss')\n        ax1.plot(self.epochs, self.val_losses, label='val_loss')\n        \n        # plot the accuracies\n        ax2.plot(self.epochs, self.acc, label='train_acc')\n        ax2.plot(self.epochs, self.val_acc, label='val_acc')\n    \n        ax1.set_title(f'Loss vs Epochs')\n        ax1.set_xlabel(\"Epochs\")\n        ax1.set_ylabel(\"Loss\")\n        \n        ax2.set_title(f'Accuracy vs Epochs')\n        ax2.set_xlabel(\"Epochs\")\n        ax2.set_ylabel(\"Accuracy\")\n        \n        ax1.legend()\n        ax2.legend()\n        plt.show()\n        \n        # print out the accuracies at each epoch\n        print(f'Epoch #{self.epochs[-1]+1} >> train_acc={self.acc[-1]:.5f}, val_acc={self.val_acc[-1]:.5f}')\n    \n    def on_train_begin(self, logs={}):\n        # initialize lists to store values from training\n        self.losses = []\n        self.val_losses = []\n        self.epochs = []\n        self.batch_no = []\n        self.acc = []\n        self.val_acc = []\n    \n    def on_epoch_end(self, epoch, logs={}):\n        # append values from the last epoch\n        self.losses.append(logs.get('loss'))\n        self.val_losses.append(logs.get('val_loss'))\n        self.acc.append(logs.get('acc'))\n        self.val_acc.append(logs.get('val_acc'))\n        self.epochs.append(epoch)\n        self.plot()  # update the graph\n        \n    def on_train_end(self, logs={}):\n        self.plot()\n               \nplotter = Plotter()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"use_generator = True  # toggles between using the augmentation generator or the original images","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The learning rate and the batch size are already tuned, the only one that remains is the number of epochs."},{"metadata":{"trusted":true},"cell_type":"code","source":"callbacks = [plotter]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if use_generator:\n    model.fit_generator(train_gen, validation_data=valid_gen, epochs=120, \n                        steps_per_epoch=train_x.shape[0]*0.85//96, \n                        validation_steps=train_x.shape[0]*0.15//96, callbacks=callbacks)\nelse:\n    model.fit(x=train_x, y=train_y, epochs=80, batch_size=32, callbacks=callbacks,\n              validation_split=0.15)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# save model\nkeras.models.save_model(model, f'model_{time.time()}.h5')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since we're using Dropout, it's not surprising to see the training accuracy less than the validation accuracy.\nThat's because Dropout is used only in training to prevent overfitting, then disabled in both validation and testing.\n\nDropout shuts down some neurons randonly each training step, so it's like using multiple weaker models during training."},{"metadata":{},"cell_type":"markdown","source":"<a id=\"eval\"></a>\n# 3. Model Evaluation"},{"metadata":{},"cell_type":"markdown","source":"The calculate_performace function will calculate the accuracy and gives back data about the falses of the model's predictions to visualize and analyze them"},{"metadata":{"trusted":true},"cell_type":"code","source":"def calculate_performance(labels, pred, dataset):\n    pred_cat = np.argmax(pred, axis=1)  # categorical predictions 0-9\n    labels_cat = np.argmax(labels, axis=1)  # categorical labels 0-9\n    \n    # a boolean vector of element-wise comparison between prediction and label\n    corrects = (pred_cat == labels_cat)\n    \n    # get the falses data\n    falses = dataset[~corrects]  # the falses images\n    falses_labels = labels_cat[~corrects]  # true labels of the falsely classified images - categorical\n    falses_preds = pred[~corrects]  # the false predictions of the images - 10-dim prediction\n     \n    examples_num = labels.shape[0]  # total numbers of examples\n    accuracy = np.count_nonzero(corrects) / examples_num\n\n    return accuracy, [falses, falses_labels, falses_preds], [labels_cat, pred_cat]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Even though we used augmented versions of the training dataset while training we can't use it to get intuition of the model's performance or metrics because it in a way learnt from these images.\n\nNote that augmentation would give an augmented image, in the Features Space, very close to the original image so the hyperplane which is our model would very likely classify the original image correctly."},{"metadata":{},"cell_type":"markdown","source":"We'll only test our model on the original training examples to make sure it's performace is very high as expected and to work as a debug point if we notrice that there's something off about it's performance."},{"metadata":{},"cell_type":"markdown","source":"<a id=\"metrics\"></a>\n## 3.1 Confusion Matrix"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_pred = model.predict(train_x)  # predict the whole training dataset\n\n# calculate the accuracy over the whole dataset and get information about falses\ntrain_accuracy, train_falses_data, (true_labels, pred_labels) = calculate_performance(train_y, train_pred ,train_x)\n\nprint(f'Don\\'t use as a metric - Original Training Dataset Accuracy: {np.round(train_accuracy*100, 3)}%')\n\nplt.figure(figsize=(10, 10))\n\n# Calculate the confusion matrix and visualize it\ntrain_matrix = confusion_matrix(y_pred=pred_labels, y_true=true_labels)\nsns.heatmap(data=train_matrix, annot=True, cmap='Blues', fmt=f'.0f')\n\nplt.title('Confusion Matrix - Training Dataset', size=24)\nplt.xlabel('Predictions', size=20);\nplt.ylabel('Labels', size=20);\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We'll gain insight from the model evaluation on the test dataset.\nFor exapmle, we can find out if a false classifications to a certain digit is biased towards another specific digit."},{"metadata":{"trusted":true},"cell_type":"code","source":"test_pred = model.predict(test_x)  # predict the whole test dataset\n\n# calculate the accuracy over the whole dataset and get information about falses\ntest_accuracy, test_falses_data, (true_labels, pred_labels) = calculate_performance(test_y, test_pred, test_x)\n\nprint(f'Test Dataset Accuracy: {np.round(test_accuracy*100, 3)}%')\n\nplt.figure(figsize=(10, 10))\n\n# Calculate the confusion matrix and visualize it\ntest_matrix = confusion_matrix(y_pred=pred_labels, y_true=true_labels)\nsns.heatmap(data=test_matrix, annot=True, cmap='Blues', fmt=f'.0f')\n\nplt.title('Confusion Matrix - Test Dataset', size=24)\nplt.xlabel('Predictions', size=20);\nplt.ylabel('Labels', size=20);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**You should be getting a model's accurace around 99.5% on the test dataset.** <br/>\n** Note that the accuracy will be different for each run of the kernel, but all of them would almost be equal.**\n\n\nOur model has better acuracy on both Training and Test Datasets than the generated images meaning that the generated images were harder for the model, but they are all very close as they all fall into the same distribution."},{"metadata":{},"cell_type":"markdown","source":"Let's visualize random examples predictions from the test dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"random_idx = np.random.choice(range(test_y.shape[0]), size=10)  # get random 10 indexes\n\nexamples = test_x[random_idx]  # the images\npreds = model.predict(examples)  # the predictions - 10-dim probabilities\nlabels = np.argmax(test_y[random_idx], axis=1)  # the labels - categorical\npreds_cat = np.argmax(preds, axis=1)  # the predictions - categorical","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axs = plt.subplots(2, 5, figsize=(28, 12));\nfig.suptitle('Model Predictions - Test Dataset', size=32)\n\nfor ax, pred, pred_prob, label, example in zip(axs.ravel(), preds_cat, preds, labels, examples):\n    ax.imshow(example[:, :, 0] ,cmap='gray');\n    ax.set_title(f'Label: {label}\\nPrediction: {pred} with {np.round(np.max(pred_prob)*100, 3)}% Confidence.',\n                size = 15)\n    ax.axis('off')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Very Good!\nWe can see that the model classifies the imgaes not only correctly but with a great confidence.\nIt's acceptable if there is some of examples that the model clssifies correctly but with a confidence/probability less than 0.5 on hard examples \nTo judge the model fairly if it's doing well or not on a certain image, we should ask ourselves 2 questions:\n\n**1. Is a human being able to classify the image corerectly with great confidence?**\n    Maybe the image is a very hard example (blurry, unclear...etc)\n\n\n**2. How does the prediction's probability fare when compared to random chance?** Each class has a 10% probability if chosen randomly, so if the model's probability of a correctly classified image is above 10% then it has some knowledge about classifying the class correctly, not a lot but some."},{"metadata":{},"cell_type":"markdown","source":"Let's visualize random examples from the unlablled dataset and see how the model performs"},{"metadata":{"trusted":true},"cell_type":"code","source":"random_idx = np.random.choice(range(test_y.shape[0]), size=10)  # get random 10 indexes\n\nexamples = unlabelled_test_x[random_idx]  # the images\npreds = model.predict(examples)  # the predictions - 10-dim probabilities\npreds_cat = np.argmax(preds, axis=1)  # the predictions - categorical","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axs = plt.subplots(2, 5, figsize=(30, 12));\nfig.suptitle('Predictions of Unlabelled Images', size=32)\n\nfor ax, pred, pred_prob, example in zip(axs.ravel(), preds_cat, preds, examples):\n    ax.imshow(example[:, :, 0] ,cmap='gray');\n    ax.set_title(f'Prediction: {pred} with {np.round(np.max(pred_prob)*100, 3)}% Confidence.', size = 15)\n    ax.axis('off')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"error\"></a>\n# 4. Error Analysis"},{"metadata":{},"cell_type":"markdown","source":"Analyzing Falses and Errors to find out where the model fails and hopefully gain an intuition on how to enhance it.\n"},{"metadata":{},"cell_type":"markdown","source":"Checking the error of the test dataset as the model hasn't seen its examples or augmented versions of them before, so they would serve as a fair test to help us find out where our model fails."},{"metadata":{"trusted":true},"cell_type":"code","source":"# checking the falses of the test dataset\nfalses_data = test_falses_data # set the dataset to check\nfalses_examples, falses_labels, falses_preds = falses_data\nfalses_idx = np.argmax(falses_preds, axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"By plotting a countbar of the labels of the images that were misclassified, we can see if there's any Digit that the model needs to train on more.\n\n\nThe confusion matrix would help us find if the misclassifications of one digit are heavily biased towards another digit.\nThis can happen to similar digits like 4 and 9"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 8))\nfig.suptitle('False Classifications Plots', size=32)\n\nsns.countplot(falses_labels, ax=ax1);\nax1.set_title(f'Falses ({falses_labels.size} total)',size=20);\nax1.set_xlabel('Digits', size=15);\nax1.set_ylabel('Count', size=15);\n\nfor patch in ax1.patches:\n    bar_height = patch.get_height()\n    ax1.annotate(f'{bar_height}', (patch.get_x()+0.25, bar_height-0.2), color='w', size=15);\n    \n\nfalses_matrix = confusion_matrix(y_pred=falses_idx, y_true=falses_labels)\nsns.heatmap(data=falses_matrix, annot=True, cmap='Blues', fmt=f'.0f')\n\nplt.xlabel('Predictions', size=20);\nplt.ylabel('Labels', size=20);\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see, the number of the misclassified images is rather small compared to the dataset size.\nThe misclassifications aren't exclusive to one digit in particular even if some digits have more number of falses than other ones.\n\n\nThe confusion matrix displays an almost equal spread which means there's no falses that are biaseds towards a certain digit."},{"metadata":{},"cell_type":"markdown","source":"Let's visualize some of the flase predictions, to try to get more of an understanding of the model's misclassifications."},{"metadata":{"trusted":true},"cell_type":"code","source":"random_falses = np.random.choice(range(falses_labels.size), size=np.min((10, falses_labels.size)), replace=False)\n\nexamples = falses_examples[random_falses]\npreds_probs = falses_preds[random_falses]\nlabels = falses_labels[random_falses]\npreds_binary = np.argmax(preds_probs, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axs = plt.subplots(2, 5, figsize=(30, 12));\nfig.suptitle('Misclassified Images', size=32)\n\nfor ax, pred, pred_prob, label, example in zip(axs.ravel(), preds_binary, preds_probs, labels, examples):\n    ax.imshow(example[:, :, 0] ,cmap='gray');\n    ax.set_title(f'Label: {label}\\nPrediction: {pred} with {np.round(np.max(pred_prob)*100, 3)}% Confidence.',\n                size = 15)\n    ax.axis('off')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As seen above, the model struggles with hard examples that humans will struggle with some of them as well.\nThere are mislabelled images in the MNIST that serve as noise or an outlier for the model."},{"metadata":{},"cell_type":"markdown","source":"<a id=\"notes\"></a>\n# 5. General Notes"},{"metadata":{},"cell_type":"markdown","source":"* This dataset has a relatively small size, that's why we used augmentation to reduce the chance of overfitting while remaining in the overall distributioin of the dataset.\n\n\n* MNIST is a pre-cleaned dataset and used as a models' performaces benchmarks. A more uncurated dataset would need more cleaning and preprocessing like checking if there are any labels missing or if the dataset require feature engineering like Dimensionality Reduction.\n\n\n* This model suffers from a fatal flaw that is it will give a prediction of a digit to any image it sees, so if we give it a picture of a human, dog, cat...etc with the correct shape it will classify it as a number. This is discussed in detail in this informative post. I highly recommend reading it. [https://emiliendupont.github.io/2018/03/14/mnist-chicken/](http://)\n\n\n* We can add another class to the dataset called \"Not-a-Digit\" and contains random images of non digits, but the dataset needs to be larger so that the dataset isn't inproportionate to one of its classes or add an appropriate number of examples for this new class, but because of the random nature of the class, its examples won't have consistient features nor distribution thus the model won't do well in case of unseen images of this new model.\n\n\n* We can only use this model to classify hand-written images (same distribution) if we want it to have the same accuracy with the new data, and the images must be of the same shape and be preprocessed the same way the training data were before being given to the model.\n\n\n* We can use this model as an initial model for recognizing digits outside this distribution but with training on the new digits distribution, its weights will be fine-tuned to be able to classify them, or we can use transfer learning to freeze some of the model's layers weights and change the architecture a bit then retrain it for a whole new problem that doesn't have anything to do with digits.\n\n\n* Finally, I hope this kernel explained a lot of tools used in preprocessing, model training, evaluation and error analyzing. You're welcome to fork it and change it as you like to try your own concepts. Feel free to use the visualiztion in this kernel to save some time while building your own kernels. "},{"metadata":{},"cell_type":"markdown","source":"<a id=\"submit\"></a>\n# 6. Submission"},{"metadata":{"trusted":true},"cell_type":"code","source":"# cateforical predictions of the dataset\npred = model.predict(unlabelled_test_x)\npred_cat = np.argmax(pred,axis = 1)\n\npred_series = pd.Series(pred_cat,name='Label')  # pandas series of predictions\nid_series = pd.Series(range(1, unlabelled_test_x.shape[0]+1), name='ImageId') # pandas series of IDs\n\nsubmission = pd.concat((id_series, pred_series), axis=1)\n\nprint(submission.head(5))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# save to csv\nsubmission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"}},"nbformat":4,"nbformat_minor":4}