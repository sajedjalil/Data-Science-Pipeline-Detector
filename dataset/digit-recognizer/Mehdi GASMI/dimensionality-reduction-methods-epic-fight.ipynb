{"cells":[{"metadata":{},"cell_type":"markdown","source":"<img src=\"https://i.postimg.cc/52kKwKCx/illustration1.jpg\" alt=\"Epic Fight\">","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"In this notebook i'll experiment many different dimension reduction algorithms on mnist dataset and see if they can handle data authenticity when they reduce progressively dimensions.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<img src=\"https://i.postimg.cc/bwqyWT3c/pic-illustrated.png\" align=\"right\" width=\"800\" height=\"400\">\n\nMNIST Dataset contains 28 * 28 pixels for each digit which makes in total 784 dimensions.\n\nI'll apply this algorithms to reduce dimensions progressively and train neural network on them using 10 Kfold cv and see how redimentioning affect model accuracy, as showed in next illustration.\n\nthe aim is to have intuition about each algorithme performance.\n\n<BR CLEAR=”left” />","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"ps: I could not test TSNE because badly manages more than 4 dimensions.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Lets begin !","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Contents\n\n* [<font size=4>Libraryes for fun</font>](#1)\n* [<font size=4>PCA</font>](#2)\n* [<font size=4>IncrementalPCA</font>](#3)\n* [<font size=4>NMF</font>](#4)\n* [<font size=4>KernelPCA</font>](#5)\n* [<font size=4>Isomap</font>](#6)\n* [<font size=4>TruncatedSVD</font>](#7)\n* [<font size=4>Gaussian Random Projection</font>](#8)\n* [<font size=4>FastICA</font>](#9)\n* [<font size=4>MiniBatch Dictionary Learning</font>](#10)\n* [<font size=4>Sparse Random Projection</font>](#11)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Libraryes for fun <a id=\"1\"></a>","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"# !jupyter labextension install jupyterlab-plotly\n\nimport numpy as np \nimport pandas as pd \n\nfrom tensorflow.keras import Sequential\n\nfrom sklearn.model_selection import cross_val_score\nfrom collections import defaultdict\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Activation\nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom sklearn.model_selection import StratifiedKFold\nfrom tqdm.notebook import tqdm \nfrom keras.utils.np_utils import to_categorical\nimport seaborn as sns\nfrom sklearn.linear_model import LogisticRegression\n\nimport plotly.express as px\nimport matplotlib.pyplot as plt\nimport plotly.offline as pyo\n\nfrom sklearn.manifold import LocallyLinearEmbedding\nfrom sklearn.decomposition import PCA\nfrom sklearn.decomposition import IncrementalPCA\nfrom sklearn.decomposition import KernelPCA\nfrom sklearn.decomposition import SparsePCA\nfrom sklearn.decomposition import NMF\nfrom sklearn.manifold import Isomap\nfrom sklearn.manifold import TSNE\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.random_projection import GaussianRandomProjection\nfrom sklearn.decomposition import FastICA\nfrom sklearn.decomposition import MiniBatchDictionaryLearning\nfrom sklearn.random_projection import SparseRandomProjection\n\nfrom sklearn.datasets import make_classification\nfrom xgboost import XGBClassifier\nimport lightgbm as lgb\n\nimport gc\n\npyo.init_notebook_mode()\n\ntrain = pd.read_csv('../input/digit-recognizer/train.csv')\n\ntrain = train.sample(10000).reset_index(drop=True) #i pick sample of 10k digit to increase speed\n\ntrain.loc[:,'pixel0':] = train.loc[:,'pixel0':]/255\n\nX_ = train.loc[:,'pixel0':]\ny = train['label']\n\ncomponents = [784 ,int(785/2) ,int(785/4) ,int(785/8), int(785/16), int(785/32), int(785/64), int(785/128), int(785/256), 2]\n# components = components[::-1]\nbatch_size = 501\nepochs = 17\nneurons = 958\noptimizer = 'Adam'\nrandom_state=42","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"def digit_show (df_train, number, name) :\n    plt.figure(figsize=(20, 15))\n    j = 1\n    for i in range(10) :\n        if number == 784 :\n            plt.subplot(1,10,j)\n            plt.gca().set_title('Image Reconstruction from Compressed Representation', fontsize=16)\n        else :\n            plt.subplot(1,10,j)\n        j +=1\n        plt.imshow(df_train[df_train['label'] == i].head(1).drop(labels = [\"label\"],axis = 1).values.reshape(28, 28), cmap='gray', interpolation='none')\n        if number == 784 :\n            plt.title(\"Original : {}\".format(i))\n        else :\n            plt.title(\"{} {} Digit: {}\".format(name, number, i))\n    plt.tight_layout()\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"# this funcion take Algorithme in entry, use it for dimention reduction, train NN with results, plus 2D and 3D result, and accuracy performance.\n\ndef dimensionality_reduction_octagone(alg, name) :\n    dim2 = []\n    dim3 = []\n    result = []\n    names = []\n    results = defaultdict(list)\n    \n    for i in tqdm(components) :\n        if i == 784 :\n            X= X_.values\n        else :\n            if name == 'KernelPCA' :\n                alg_ = alg(n_components=i, fit_inverse_transform = True).fit(X_)\n                X = alg_.transform(X_)\n            else :\n                alg_ = alg(n_components=i).fit(X_)\n                X = alg_.transform(X_)\n            \n        if i == 2 :\n            dim2 = X\n        elif i ==3 :\n            dim3 = X\n            \n        kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=random_state)\n        cvscores = []\n        for train, test in kfold.split(X, y):\n            model = Sequential()\n            model.add(Dense(neurons, input_dim=i, activation='relu'))\n            model.add(Dropout(0.2))\n            model.add(Dense(neurons, activation='relu'))\n            model.add(Dropout(0.2))\n            model.add(Dense(10, activation='softmax'))\n            model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n            model.fit(X[train], to_categorical(y[train],num_classes = 10), epochs=epochs, batch_size=batch_size, verbose = 0)\n            scores = model.evaluate(X[test], to_categorical(y[test],num_classes = 10), verbose=0)\n            cvscores.append(scores[1] * 100)\n        results[name + ' ' + str(i)].append(cvscores)\n        \n        if name not in ('Isomap', 'GaussianRandomProjection', 'MiniBatchDictionaryLearning', 'SparseRandomProjection') :\n            if i == 784 :\n                digit = pd.merge(pd.DataFrame(X), pd.DataFrame(y), left_index=True, right_index=True)\n                digit_show(digit, i, name)\n            else :\n                digit = pd.merge(pd.DataFrame(alg_.inverse_transform(X)), pd.DataFrame(y), left_index=True, right_index=True)\n                digit_show(digit, i, name)\n            \n    \n    plt.figure(figsize=(30,10))\n    \n    for key, value in results.items():\n        if key == name + ' ' + '784' :\n            names.append('Original')\n        else :\n            names.append(key)\n            \n        result.append(value)\n\n    plt.xticks(rotation=45)\n    ax = sns.boxplot(x=names, y= result)\n    ax.set(xlabel= name + ' Components (Dimmentions)', ylabel='Accuracy %')\n    ax.set_title('Accuracy Progression from '+ name + ' Compressed Representation')\n   \n\n        \n    final_2D = pd.merge(pd.DataFrame(dim2), pd.DataFrame(y), left_index=True, right_index=True)\n    final_2D.columns = ['X','Y','Label']\n    final_2D.Label = final_2D.Label.astype('str')\n\n    fig1 = px.scatter(final_2D, x='X', y='Y', color=\"Label\", title= name + \" 2 Components\")\n#     fig1.show()\n    \n    final_3D = pd.merge(pd.DataFrame(dim3), pd.DataFrame(y), left_index=True, right_index=True)\n    final_3D.columns = ['X','Y','Z','Label']\n    final_3D.Label = final_3D.Label.astype('str')\n\n    fig2 = px.scatter_3d(final_3D, x='X', y='Y', z= 'Z', color=\"Label\", size_max=0.2, title= name + \" 3 Components\")\n#     fig2.update_traces(marker=dict(size=2,\n#                               line=dict(width=0,\n#                                         color='DarkSlateGrey')),\n#                   selector=dict(mode='markers'))\n#     fig2.show()\n    \n    \n    fig1.show()\n    fig2.show()\n    gc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# PCA <a id=\"2\"></a>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<img src=\"https://i.postimg.cc/WzsdN5C9/Principal-Component-Analysis-second-principal.gif\" align=\"right\" width=\"500\" height=\"400\">\n\n<a href=\"https://builtin.com/data-science/step-step-explanation-principal-component-analysis\">Principal Component Analysis</a>, or PCA, is a dimensionality-reduction method that is often used to reduce the dimensionality of large data sets, by transforming a large set of variables into a smaller one that still contains most of the information in the large set.\n\nReducing the number of variables of a data set naturally comes at the expense of accuracy, but the trick in dimensionality reduction is to trade a little accuracy for simplicity. Because smaller data sets are easier to explore and visualize and make analyzing data much easier and faster for machine learning algorithms without extraneous variables to process.\n\nSo to sum up, the idea of PCA is simple — reduce the number of variables of a data set, while preserving as much information as possible.\n\n<BR CLEAR=”left” />","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"dimensionality_reduction_octagone(PCA, 'PCA')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* PCA decrease performances in 392 components but it increase it from 196 until 24 and that is interesting, that mean PCA can increase performances after reducing number of components.\n\n* we can easily recognize digit after 49 components.\n\n* 2D and 3D representation is not that bad.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# IncrementalPCA <a id=\"3\"></a>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<a href=\"https://scikit-learn.org/stable/modules/decomposition.html#incrementalpca\">IncrementalPCA</a>, <p>The PCA object is very useful, but has certain limitations for\nlarge datasets. The biggest limitation is that PCA only supports\nbatch processing, which means all of the data to be processed must fit in main\nmemory. The IncrementalPCA object uses a different form of\nprocessing and allows for partial computations which almost\nexactly match the results of while processing the data in a\nminibatch fashion. IncrementalPCA makes it possible to implement\nout-of-core Principal Component Analysis either by:</p>\n\n<blockquote>\n<div><ul class=\"simple\">\n<li><p>Using its partial_fit method on chunks of data fetched sequentially\nfrom the local hard drive or a network database.</p></li>\n<li><p>Calling its fit method on a sparse matrix or a memory mapped file using\nnumpy.memmap</p></li>\n</ul>\n</div></blockquote>\n\nIncrementalPCA only stores estimates of component and noise variances,\nin order update explained_variance_ratio_ incrementally. This is why\nmemory usage depends on the number of samples per batch, rather than the\nnumber of samples to be processed in the dataset.</p>\n\nAs in PCA IncrementalPCA centers but does not scale the\ninput data for each feature before applying the SVD.</p>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"dimensionality_reduction_octagone(IncrementalPCA, 'IncrementalPCA')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Same as PCA, IncrementalPCA decrease performances in 392 components but it increase it from 196 until 24 and that is interesting, that mean IncrementalPCA can increase performances after reducing number of components, buy generally there is more variance in folds.\n\n* we can easily recognize digit after 49 components.\n\n* 2D and 3D representation is not that bad same as PCA.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# NMF <a id=\"4\"></a>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<a href=\"https://en.wikipedia.org/wiki/Non-negative_matrix_factorization#:~:text=Non%2Dnegative%20matrix%20factorization%20(NMF,matrices%20have%20no%20negative%20elements.\">Non-negative matrix factorization</a>, <p><b></b>  (<b>NMF</b> or <b>NNMF</b>), also <b>non-negative matrix approximation</b><sup id=\"cite_ref-dhillon_1-0\" class=\"reference\"></sup> is a group of algorithms in multivariate analysis and linear algebra where a matrix <span class=\"texhtml\"><b>V</b></span> is factorized into (usually) two matrices <span class=\"texhtml\"><b>W</b></span> and <span class=\"texhtml\"><b>H</b></span>, with the property that all three matrices have no negative elements. This non-negativity makes the resulting matrices easier to inspect. Also, in applications such as processing of audio spectrograms or muscular activity, non-negativity is inherent to the data being considered. Since the problem is not exactly solvable in general, it is commonly approximated numerically.\n</p>\n<br />\n<img src=\"https://i.postimg.cc/qRLpWxJs/wiki.png\" width=\"500\" height=\"400\">\n<br />\n<br />\n<p>NMF finds applications in such fields as astronomy, computer vision, document clustering, chemometrics, audio signal processing, recommender systems, and bioinformatics.</p>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"dimensionality_reduction_octagone(NMF, 'NMF')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* After 24 components accuracy decrease slowly and globaly it perform bad.\n\n* we can badly recognize digit after 12 components.\n\n* Strange 2D and 3D representation, look like they are stuck at the border.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# KernelPCA <a id=\"5\"></a>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<a href=\"https://en.wikipedia.org/wiki/Kernel_principal_component_analysis\">kernel principal component analysis (kernel PCA)</a>\n<p>In the field of multivariate statistics <b></b> \n<sup id=\"cite_ref-1\" class=\"reference\"></sup>\nis an extension of principal component analysis (PCA) using techniques of kernel methods. Using a kernel, the originally linear operations of PCA are performed in a reproducing kernel Hilbert space.\n</p>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"dimensionality_reduction_octagone(KernelPCA, 'KernelPCA')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* KernelPCA is very similar to PCS in performances, the progression of accuracy is nearly the same.\n\n* Digit are all clear but i think thr is a bug in inverse_transform function, in addition to reverse it adds the fit inverse_transform attribute to the fit function, it's a bit awkward.\n\n* Nearly same 2D and 3D representation to PCA.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Isomap <a id=\"6\"></a>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<p> <a href=\"https://en.wikipedia.org/wiki/Isomap\">Isomap</a> is a nonlinear dimensionality reduction method. It is one of several widely used low-dimensional embedding methods. Isomap is used for computing a quasi-isometric, low-dimensional embedding of a set of high-dimensional data points.  The algorithm provides a simple method for estimating the intrinsic geometry of a data manifold based on a rough estimate of each data point’s neighbors on the manifold. Isomap is highly efficient and generally applicable to a broad range of data sources and dimensionalities.\n</p>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"dimensionality_reduction_octagone(Isomap, 'Isomap')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Isomap take long time to  finish.\n\n* It perform well in 2D and 3D\n\n* No possibility to reconstruct digit.\n\n* relativly big varience in accuracy and big drop between 6 and 3 componenets but globaly it performe well. nearly same as PCA.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# TruncatedSVD <a id=\"7\"></a>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<p>In linear algebra</p>, the <b>singular value decomposition</b> (<b>SVD</b>) is a factorization of a realor complex matrix that generalizes the eigendecomposition of a square normal matrix to any \n<semantics>\n<mrow class=\"MJX-TeXAtom-ORD\">\n<mstyle displaystyle=\"true\" scriptlevel=\"0\">\n<mi>m</mi>\n<mo>×<!-- × --></mo>\n<mi>n</mi>\n</mstyle>\n</mrow>\n<annotation encoding=\"application/x-tex\">{\\displaystyle m\\times n}</annotation>\n</semantics>\n</math></span><img src=\"https://wikimedia.org/api/rest_v1/media/math/render/svg/12b23d207d23dd430b93320539abbb0bde84870d\" class=\"mwe-math-fallback-image-inline\" aria-hidden=\"true\" style=\"vertical-align: -0.338ex; width:6.276ex; height:1.676ex;\" alt=\"m\\times n\"></span> matrix via an extension of the polar decomposition.\n</p>\n<p>Specifically, the singular value decomposition of an <span class=\"mwe-math-element\"><span class=\"mwe-math-mathml-inline mwe-math-mathml-a11y\" style=\"display: none;\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" alttext=\"{\\displaystyle m\\times n}\">\n<semantics>\n<mrow class=\"MJX-TeXAtom-ORD\">\n<mstyle displaystyle=\"true\" scriptlevel=\"0\">\n<mi>m</mi>\n<mo>×<!-- × --></mo>\n<mi>n</mi>\n</mstyle>\n</mrow>\n<annotation encoding=\"application/x-tex\">{\\displaystyle m\\times n}</annotation>\n</semantics>\n</math></span><img src=\"https://wikimedia.org/api/rest_v1/media/math/render/svg/12b23d207d23dd430b93320539abbb0bde84870d\" class=\"mwe-math-fallback-image-inline\" aria-hidden=\"true\" style=\"vertical-align: -0.338ex; width:6.276ex; height:1.676ex;\" alt=\"m\\times n\"></span> real or complex matrix <span class=\"mwe-math-element\"><span class=\"mwe-math-mathml-inline mwe-math-mathml-a11y\" style=\"display: none;\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" alttext=\"{\\displaystyle \\mathbf {M} }\">\n<semantics>\n<mrow class=\"MJX-TeXAtom-ORD\">\n<mstyle displaystyle=\"true\" scriptlevel=\"0\">\n<mrow class=\"MJX-TeXAtom-ORD\">\n<mi mathvariant=\"bold\">M</mi>\n</mrow>\n</mstyle>\n</mrow>\n<annotation encoding=\"application/x-tex\">{\\displaystyle \\mathbf {M} }</annotation>\n</semantics>\n</math></span><img src=\"https://wikimedia.org/api/rest_v1/media/math/render/svg/e499ae5946af9c09777ada933051b3669d3372c2\" class=\"mwe-math-fallback-image-inline\" aria-hidden=\"true\" style=\"vertical-align: -0.338ex; width:2.537ex; height:2.176ex;\" alt=\"\\mathbf {M} \"></span> is a factorization of the form <span class=\"mwe-math-element\"><span class=\"mwe-math-mathml-inline mwe-math-mathml-a11y\" style=\"display: none;\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" alttext=\"{\\displaystyle \\mathbf {U\\Sigma V^{*}} }\">\n<semantics>\n<mrow class=\"MJX-TeXAtom-ORD\">\n<mstyle displaystyle=\"true\" scriptlevel=\"0\">\n<mrow class=\"MJX-TeXAtom-ORD\">\n<mi mathvariant=\"bold\">U</mi>\n<mi mathvariant=\"bold\">Σ<!-- Σ --></mi>\n<msup>\n<mi mathvariant=\"bold\">V</mi>\n<mrow class=\"MJX-TeXAtom-ORD\">\n<mo mathvariant=\"bold\">∗<!-- ∗ --></mo>\n</mrow>\n</msup>\n</mrow>\n</mstyle>\n</mrow>\n<annotation encoding=\"application/x-tex\">{\\displaystyle \\mathbf {U\\Sigma V^{*}} }</annotation>\n</semantics>\n</math></span><img src=\"https://wikimedia.org/api/rest_v1/media/math/render/svg/b696ac58329349e430962ce8fa94b50a60ea30a5\" class=\"mwe-math-fallback-image-inline\" aria-hidden=\"true\" style=\"vertical-align: -0.338ex; width:7.185ex; height:2.343ex;\" alt=\"{\\displaystyle \\mathbf {U\\Sigma V^{*}} }\"></span>, where <span class=\"mwe-math-element\"><span class=\"mwe-math-mathml-inline mwe-math-mathml-a11y\" style=\"display: none;\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" alttext=\"{\\displaystyle \\mathbf {U} }\">\n<semantics>\n<mrow class=\"MJX-TeXAtom-ORD\">\n<mstyle displaystyle=\"true\" scriptlevel=\"0\">\n<mrow class=\"MJX-TeXAtom-ORD\">\n<mi mathvariant=\"bold\">U</mi>\n</mrow>\n</mstyle>\n</mrow>\n<annotation encoding=\"application/x-tex\">{\\displaystyle \\mathbf {U} }</annotation>\n</semantics>\n</math></span><img src=\"https://wikimedia.org/api/rest_v1/media/math/render/svg/b2141bec2344e3dc5241ff50b0fd366755e00223\" class=\"mwe-math-fallback-image-inline\" aria-hidden=\"true\" style=\"vertical-align: -0.338ex; width:2.057ex; height:2.176ex;\" alt=\"\\mathbf {U} \"></span> is an <span class=\"mwe-math-element\"><span class=\"mwe-math-mathml-inline mwe-math-mathml-a11y\" style=\"display: none;\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" alttext=\"{\\displaystyle m\\times m}\">\n<semantics>\n<mrow class=\"MJX-TeXAtom-ORD\">\n<mstyle displaystyle=\"true\" scriptlevel=\"0\">\n<mi>m</mi>\n<mo>×<!-- × --></mo>\n<mi>m</mi>\n</mstyle>\n</mrow>\n<annotation encoding=\"application/x-tex\">{\\displaystyle m\\times m}</annotation>\n</semantics>\n</math></span><img src=\"https://wikimedia.org/api/rest_v1/media/math/render/svg/367523981d714dcd9214703d654bfdedbe58d44a\" class=\"mwe-math-fallback-image-inline\" aria-hidden=\"true\" style=\"vertical-align: -0.338ex; width:6.921ex; height:1.676ex;\" alt=\"m\\times m\"></span> real or complex unitary matrix, <span class=\"mwe-math-element\"><span class=\"mwe-math-mathml-inline mwe-math-mathml-a11y\" style=\"display: none;\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" alttext=\"{\\displaystyle \\mathbf {\\Sigma } }\">\n<semantics>\n<mrow class=\"MJX-TeXAtom-ORD\">\n<mstyle displaystyle=\"true\" scriptlevel=\"0\">\n<mrow class=\"MJX-TeXAtom-ORD\">\n<mi mathvariant=\"bold\">Σ<!-- Σ --></mi>\n</mrow>\n</mstyle>\n</mrow>\n<annotation encoding=\"application/x-tex\">{\\displaystyle \\mathbf {\\Sigma } }</annotation>\n</semantics>\n</math></span><img src=\"https://wikimedia.org/api/rest_v1/media/math/render/svg/90f99b56fe6ada781ecd0f8a45b6e787b6dfed56\" class=\"mwe-math-fallback-image-inline\" aria-hidden=\"true\" style=\"vertical-align: -0.338ex; width:1.931ex; height:2.176ex;\" alt=\"\\mathbf{\\Sigma}\"></span> is an <span class=\"mwe-math-element\"><span class=\"mwe-math-mathml-inline mwe-math-mathml-a11y\" style=\"display: none;\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" alttext=\"{\\displaystyle m\\times n}\">\n<semantics>\n<mrow class=\"MJX-TeXAtom-ORD\">\n<mstyle displaystyle=\"true\" scriptlevel=\"0\">\n<mi>m</mi>\n<mo>×<!-- × --></mo>\n<mi>n</mi>\n</mstyle>\n</mrow>\n<annotation encoding=\"application/x-tex\">{\\displaystyle m\\times n}</annotation>\n</semantics>\n</math></span><img src=\"https://wikimedia.org/api/rest_v1/media/math/render/svg/12b23d207d23dd430b93320539abbb0bde84870d\" class=\"mwe-math-fallback-image-inline\" aria-hidden=\"true\" style=\"vertical-align: -0.338ex; width:6.276ex; height:1.676ex;\" alt=\"m\\times n\"></span> rectangular diagonal matrix with non-negative real numbers on the diagonal, and <span class=\"mwe-math-element\"><span class=\"mwe-math-mathml-inline mwe-math-mathml-a11y\" style=\"display: none;\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" alttext=\"{\\displaystyle \\mathbf {V} }\">\n<semantics>\n<mrow class=\"MJX-TeXAtom-ORD\">\n<mstyle displaystyle=\"true\" scriptlevel=\"0\">\n<mrow class=\"MJX-TeXAtom-ORD\">\n<mi mathvariant=\"bold\">V</mi>\n</mrow>\n</mstyle>\n</mrow>\n<annotation encoding=\"application/x-tex\">{\\displaystyle \\mathbf {V} }</annotation>\n</semantics>\n</math></span><img src=\"https://wikimedia.org/api/rest_v1/media/math/render/svg/c0048514530d0c0fb8a7beb795110815a818784d\" class=\"mwe-math-fallback-image-inline\" aria-hidden=\"true\" style=\"vertical-align: -0.338ex; width:2.019ex; height:2.176ex;\" alt=\"\\mathbf {V} \"></span> is an <span class=\"mwe-math-element\"><span class=\"mwe-math-mathml-inline mwe-math-mathml-a11y\" style=\"display: none;\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" alttext=\"{\\displaystyle n\\times n}\">\n<semantics>\n<mrow class=\"MJX-TeXAtom-ORD\">\n<mstyle displaystyle=\"true\" scriptlevel=\"0\">\n<mi>n</mi>\n<mo>×<!-- × --></mo>\n<mi>n</mi>\n</mstyle>\n</mrow>\n<annotation encoding=\"application/x-tex\">{\\displaystyle n\\times n}</annotation>\n</semantics>\n</math></span><img src=\"https://wikimedia.org/api/rest_v1/media/math/render/svg/59d2b4cb72e304526cf5b5887147729ea259da78\" class=\"mwe-math-fallback-image-inline\" aria-hidden=\"true\" style=\"vertical-align: -0.338ex; width:5.63ex; height:1.676ex;\" alt=\"n\\times n\"></span> real or complex unitary matrix.  If <span class=\"mwe-math-element\"><span class=\"mwe-math-mathml-inline mwe-math-mathml-a11y\" style=\"display: none;\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" alttext=\"{\\displaystyle \\mathbf {M} }\">\n<semantics>\n<mrow class=\"MJX-TeXAtom-ORD\">\n<mstyle displaystyle=\"true\" scriptlevel=\"0\">\n<mrow class=\"MJX-TeXAtom-ORD\">\n<mi mathvariant=\"bold\">M</mi>\n</mrow>\n</mstyle>\n</mrow>\n<annotation encoding=\"application/x-tex\">{\\displaystyle \\mathbf {M} }</annotation>\n</semantics>\n</math></span><img src=\"https://wikimedia.org/api/rest_v1/media/math/render/svg/e499ae5946af9c09777ada933051b3669d3372c2\" class=\"mwe-math-fallback-image-inline\" aria-hidden=\"true\" style=\"vertical-align: -0.338ex; width:2.537ex; height:2.176ex;\" alt=\"\\mathbf {M} \"></span> is real, <span class=\"mwe-math-element\"><span class=\"mwe-math-mathml-inline mwe-math-mathml-a11y\" style=\"display: none;\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" alttext=\"{\\displaystyle \\mathbf {U} }\">\n<semantics>\n<mrow class=\"MJX-TeXAtom-ORD\">\n<mstyle displaystyle=\"true\" scriptlevel=\"0\">\n<mrow class=\"MJX-TeXAtom-ORD\">\n<mi mathvariant=\"bold\">U</mi>\n</mrow>\n</mstyle>\n</mrow>\n<annotation encoding=\"application/x-tex\">{\\displaystyle \\mathbf {U} }</annotation>\n</semantics>\n</math></span><img src=\"https://wikimedia.org/api/rest_v1/media/math/render/svg/b2141bec2344e3dc5241ff50b0fd366755e00223\" class=\"mwe-math-fallback-image-inline\" aria-hidden=\"true\" style=\"vertical-align: -0.338ex; width:2.057ex; height:2.176ex;\" alt=\"\\mathbf {U} \"></span> and <span class=\"mwe-math-element\"><span class=\"mwe-math-mathml-inline mwe-math-mathml-a11y\" style=\"display: none;\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" alttext=\"{\\displaystyle \\mathbf {V^{T}} =\\mathbf {V^{*}} }\">\n<semantics>\n<mrow class=\"MJX-TeXAtom-ORD\">\n<mstyle displaystyle=\"true\" scriptlevel=\"0\">\n<mrow class=\"MJX-TeXAtom-ORD\">\n<msup>\n<mi mathvariant=\"bold\">V</mi>\n<mrow class=\"MJX-TeXAtom-ORD\">\n<mi mathvariant=\"bold\">T</mi>\n</mrow>\n</msup>\n</mrow>\n<mo>=</mo>\n<mrow class=\"MJX-TeXAtom-ORD\">\n<msup>\n<mi mathvariant=\"bold\">V</mi>\n<mrow class=\"MJX-TeXAtom-ORD\">\n<mo mathvariant=\"bold\">∗<!-- ∗ --></mo>\n</mrow>\n</msup>\n</mrow>\n</mstyle>\n</mrow>\n<annotation encoding=\"application/x-tex\">{\\displaystyle \\mathbf {V^{T}} =\\mathbf {V^{*}} }</annotation>\n</semantics>\n</math></span><img src=\"https://wikimedia.org/api/rest_v1/media/math/render/svg/e368902fa17988abd314f42286e859f0dc88207b\" class=\"mwe-math-fallback-image-inline\" aria-hidden=\"true\" style=\"vertical-align: -0.338ex; width:9.862ex; height:2.676ex;\" alt=\"{\\displaystyle \\mathbf {V^{T}} =\\mathbf {V^{*}} }\"></span> are real orthogonal matrices. \n</p>\n<p>Mathematical applications of the SVD include computing the pseudoinverse, matrix approximation, and determining the rank, range, and null space of a matrix.  The SVD is also extremely useful in all areas of science, engineering, and \">statistics, such as signal processing, least squares fitting of data, and process control.\n</p>\n\n<br />\n<img src=\"https://i.postimg.cc/9QWByG06/1024px-Singular-Value-Decomposition-svg.png\" width=\"500\" height=\"400\">\n<br />\n<br />\n\n\n<br />\n<br />\n<a>Truncated_SVD</a> \n\n<dl><dd><span class=\"mwe-math-element\"><span class=\"mwe-math-mathml-inline mwe-math-mathml-a11y\" style=\"display: none;\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" alttext=\"{\\displaystyle {\\tilde {\\mathbf {M} }}=\\mathbf {U} _{t}{\\boldsymbol {\\Sigma }}_{t}\\mathbf {V} _{t}^{*}}\">\n  <semantics>\n    <mrow class=\"MJX-TeXAtom-ORD\">\n      <mstyle displaystyle=\"true\" scriptlevel=\"0\">\n        <mrow class=\"MJX-TeXAtom-ORD\">\n          <mrow class=\"MJX-TeXAtom-ORD\">\n            <mover>\n              <mrow class=\"MJX-TeXAtom-ORD\">\n                <mi mathvariant=\"bold\">M</mi>\n              </mrow>\n              <mo stretchy=\"false\">~<!-- ~ --></mo>\n            </mover>\n          </mrow>\n        </mrow>\n        <mo>=</mo>\n        <msub>\n          <mrow class=\"MJX-TeXAtom-ORD\">\n            <mi mathvariant=\"bold\">U</mi>\n          </mrow>\n          <mrow class=\"MJX-TeXAtom-ORD\">\n            <mi>t</mi>\n          </mrow>\n        </msub>\n        <msub>\n          <mrow class=\"MJX-TeXAtom-ORD\">\n            <mi mathvariant=\"bold\">Σ<!-- Σ --></mi>\n          </mrow>\n          <mrow class=\"MJX-TeXAtom-ORD\">\n            <mi>t</mi>\n          </mrow>\n        </msub>\n        <msubsup>\n          <mrow class=\"MJX-TeXAtom-ORD\">\n            <mi mathvariant=\"bold\">V</mi>\n          </mrow>\n          <mrow class=\"MJX-TeXAtom-ORD\">\n            <mi>t</mi>\n          </mrow>\n          <mrow class=\"MJX-TeXAtom-ORD\">\n            <mo>∗<!-- ∗ --></mo>\n          </mrow>\n        </msubsup>\n      </mstyle>\n    </mrow>\n    <annotation encoding=\"application/x-tex\">{\\displaystyle {\\tilde {\\mathbf {M} }}=\\mathbf {U} _{t}{\\boldsymbol {\\Sigma }}_{t}\\mathbf {V} _{t}^{*}}</annotation>\n  </semantics>\n</math></span><img src=\"https://wikimedia.org/api/rest_v1/media/math/render/svg/db110a4ccfa4cf33905c27cf163bca3941c516d2\" class=\"mwe-math-fallback-image-inline\" aria-hidden=\"true\" style=\"vertical-align: -0.838ex; width:14.349ex; height:3.176ex;\" alt=\"{\\tilde {\\mathbf {M} }}=\\mathbf {U} _{t}{\\boldsymbol {\\Sigma }}_{t}\\mathbf {V} _{t}^{*}\"></span></dd></dl>\n\n<p>Only the <i>t</i> column vectors of <i>U</i> and <i>t</i> row vectors of <i>V*</i> corresponding to the <i>t</i> largest singular values Σ<sub><i>t</i></sub> are calculated. The rest of the matrix is discarded. This can be much quicker and more economical than the compact SVD if <i>t</i>≪<i>r</i>. The matrix <i>U</i><sub><i>t</i></sub> is thus <i>m</i>×<i>t</i>, Σ<sub><i>t</i></sub> is <i>t</i>×<i>t</i> diagonal, and <i>V</i><sub><i>t</i></sub>* is <i>t</i>×<i>n</i>.\n</p>\n\n<p>Of course the truncated SVD is no longer an exact decomposition of the original matrix <i>M</i>, but as discussed above, the approximate matrix <span class=\"mwe-math-element\"><span class=\"mwe-math-mathml-inline mwe-math-mathml-a11y\" style=\"display: none;\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" alttext=\"{\\displaystyle {\\tilde {\\mathbf {M} }}}\">\n  <semantics>\n    <mrow class=\"MJX-TeXAtom-ORD\">\n      <mstyle displaystyle=\"true\" scriptlevel=\"0\">\n        <mrow class=\"MJX-TeXAtom-ORD\">\n          <mrow class=\"MJX-TeXAtom-ORD\">\n            <mover>\n              <mrow class=\"MJX-TeXAtom-ORD\">\n                <mi mathvariant=\"bold\">M</mi>\n              </mrow>\n              <mo stretchy=\"false\">~<!-- ~ --></mo>\n            </mover>\n          </mrow>\n        </mrow>\n      </mstyle>\n    </mrow>\n    <annotation encoding=\"application/x-tex\">{\\displaystyle {\\tilde {\\mathbf {M} }}}</annotation>\n  </semantics>\n</math></span><img src=\"https://wikimedia.org/api/rest_v1/media/math/render/svg/8e71771306bd814a99b8329565c30f30212dc5e6\" class=\"mwe-math-fallback-image-inline\" aria-hidden=\"true\" style=\"vertical-align: -0.338ex; width:2.537ex; height:2.676ex;\" alt=\"{\\tilde {\\mathbf {M} }}\"></span> is in a very useful sense the closest approximation to <i>M</i> that can be achieved by a matrix of rank&nbsp;<i>t</i>.\n</p>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"dimensionality_reduction_octagone(TruncatedSVD, 'TruncatedSVD')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* After 12 components accuracy decrease slowly and big variance at the end.\n\n* we can badly recognize digit below 49 components.\n\n* Relatively bad representation in 2D and 3D seems intermingling between digits.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Gaussian Random Projection <a id=\"8\"></a>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"In mathematics and statistics, random projection is a technique used to reduce the dimensionality of a set of points which lie in Euclidean space. Random projection methods are known for their power, simplicity, and low error rates when compared to other methods. According to experimental results, random projection preserves distances well, but empirical results are sparse, They have been applied to many natural language tasks under the name random indexing.\n\n<a href=\"https://en.wikipedia.org/wiki/Random_projection#Gaussian_random_projection\">Gaussian_random_projection</a>\n\n<p>The random matrix R can be generated using a Gaussian distribution. The first row is a random unit vector uniformly chosen from <span class=\"mwe-math-element\"><span class=\"mwe-math-mathml-inline mwe-math-mathml-a11y\" style=\"display: none;\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" alttext=\"{\\displaystyle S^{d-1}}\">\n  <semantics>\n    <mrow class=\"MJX-TeXAtom-ORD\">\n      <mstyle displaystyle=\"true\" scriptlevel=\"0\">\n        <msup>\n          <mi>S</mi>\n          <mrow class=\"MJX-TeXAtom-ORD\">\n            <mi>d</mi>\n            <mo>−<!-- − --></mo>\n            <mn>1</mn>\n          </mrow>\n        </msup>\n      </mstyle>\n    </mrow>\n    <annotation encoding=\"application/x-tex\">{\\displaystyle S^{d-1}}</annotation>\n  </semantics>\n</math></span><img src=\"https://wikimedia.org/api/rest_v1/media/math/render/svg/c3e0d6d177d799d8dd1333f8da7dd117fc9e499f\" class=\"mwe-math-fallback-image-inline\" aria-hidden=\"true\" style=\"vertical-align: -0.338ex; width:4.714ex; height:2.676ex;\" alt=\"{\\displaystyle S^{d-1}}\"></span>. The second row is a random unit vector from the space orthogonal to the first row, the third row is a random unit vector from the space orthogonal to the first two rows, and so on. In this way of choosing R, R is an orthogonal matrix (the inverse of its transpose), and the following properties are satisfied:\n</p>\n\n<ul><li>Spherical symmetry: For any orthogonal matrix <span class=\"mwe-math-element\"><span class=\"mwe-math-mathml-inline mwe-math-mathml-a11y\" style=\"display: none;\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" alttext=\"{\\displaystyle A\\in O(d)}\">\n  <semantics>\n    <mrow class=\"MJX-TeXAtom-ORD\">\n      <mstyle displaystyle=\"true\" scriptlevel=\"0\">\n        <mi>A</mi>\n        <mo>∈<!-- ∈ --></mo>\n        <mi>O</mi>\n        <mo stretchy=\"false\">(</mo>\n        <mi>d</mi>\n        <mo stretchy=\"false\">)</mo>\n      </mstyle>\n    </mrow>\n    <annotation encoding=\"application/x-tex\">{\\displaystyle A\\in O(d)}</annotation>\n  </semantics>\n</math></span><img src=\"https://wikimedia.org/api/rest_v1/media/math/render/svg/025daba5089f54a521a6e10b7433081fda69502c\" class=\"mwe-math-fallback-image-inline\" aria-hidden=\"true\" style=\"vertical-align: -0.838ex; width:9.382ex; height:2.843ex;\" alt=\"{\\displaystyle A\\in O(d)}\"></span>, RA and R have the same distribution.</li>\n<li>Orthogonality: The rows of R are orthogonal to each other.</li>\n<li>Normality: The rows of R are unit-length vectors.</li></ul>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"dimensionality_reduction_octagone(GaussianRandomProjection, 'GaussianRandomProjection')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Performe relativly bad, after 49 components accuracy decreace dramaticly.\n\n* no waye to see digit reconstruction in sklearn lib.\n\n* Relatively bad representation in 2D and 3D seems intermingling between digits.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# FastICA <a id=\"9\"></a>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<a href=\"https://en.wikipedia.org/wiki/FastICA\">FastICA</a> is an efficient and popular algorithm for independent component analysis invented by Aapo Hyvärinen at Helsinki University of Technology. Like most ICA algorithms, FastICA seeks an orthogonal rotation of prewhitened data, through a fixed-point iteration scheme, that maximizes a measure of non-Gaussianity of the rotated components. Non-gaussianity serves as a proxy for statistical independence, which is a very strong condition and requires infinite data to verify. FastICA can also be alternatively derived as an approximative Newton iteration.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"dimensionality_reduction_octagone(FastICA, 'FastICA')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Average separation in 2D and 3D.\n\n* Hardly reconise digits after 49 componenets.\n\n* Big accuracy drop at 392 but it went back up and after it decrease hardly, globaly bad performances.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# MiniBatch Dictionary Learning <a id=\"10\"></a>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.MiniBatchDictionaryLearning.html\">MiniBatch Dictionary Learning</a>\n\nMiniBatchDictionaryLearning implements a faster, but less accurate\nversion of the dictionary learning algorithm that is better suited for large\ndatasets.</p>\n<p>By default, MiniBatchDictionaryLearning divides the data into\nmini-batches and optimizes in an online manner by cycling over the mini-batches\nfor the specified number of iterations. However, at the moment it does not\nimplement a stopping condition.</p>\n<p>The estimator also implements partial_fit, which updates the dictionary by\niterating only once over a mini-batch. This can be used for online learning\nwhen the data is not readily available from the start, or for when the data\ndoes not fit into the memory.</p>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"dimensionality_reduction_octagone(MiniBatchDictionaryLearning, 'MiniBatchDictionaryLearning')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Flat representation 2D and 3D digits but still bad.\n\n* No way to have digit reconstruction in sklearn lib.\n\n* Big accuracy drop at 196 and it went back up good and after it decrease hardly, globaly bad performances.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Sparse Random Projection <a id=\"11\"></a>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.random_projection.SparseRandomProjection.html\">Sparse Random Projection</a>\n<p>Sparse random matrix is an alternative to dense random\nprojection matrix that guarantees similar embedding quality while being\nmuch more memory efficient and allowing faster computation of the\nprojected data.</p>\n\n<p>If we note s = 1 / density the components of the random matrix are\ndrawn from:</p>\n\n<blockquote>\n<div><ul class=\"simple\">\n<li><p>-sqrt(s) / sqrt(n_components)   with probability 1 / 2s</p></li>\n<li><p>0                              with probability 1 - 1 / s</p></li>\n<li><p>+sqrt(s) / sqrt(n_components)   with probability 1 / 2s</p></li>\n</ul>\n</div></blockquote>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"dimensionality_reduction_octagone(SparseRandomProjection, 'SparseRandomProjection')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Bad representation in 2D and 3D digits.\n\n* No way to have digit reconstruction in sklearn lib.\n\n* Big accuracy at the begining and then big decrease at 49 componenets and very bad performances at 3 and 2 componenets.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Conclusion <a id=\"12\"></a>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"We can clearly distinct good approaches from bad even if they are all nearly same but there is some subtleties, if i have to classify from best to worst i do like this :\n\n1. Isomap\n2. truncatedSVD\n3. Kernel PCA\n4. PCA \n5. NMF\n6. Mini Batch Dictionary Learning\n7. FastICA\n8. Gaissian Random Projection\n9. Sparce Random Projection\n\n\nIt is true that they are tested in mnist data uniquely and with only one rigid neural network but i have beed tested them on xgb and lgbm and the result is nearly the same and mnist digits of are distinct enough from each other to evaluate an algorithm on them in my opinion.\n\nWhat do you think ?\n\nIn my opinion we can extrapolate to fight and trust it to choose an algorithm in the future.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"I hope you enjoyed it, please leave me an **upvote** I will greatly appreciate it.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<img src=\"https://i.postimg.cc/6QsZhjs3/Getty-Images-630157424.jpg\" alt=\"Epic Fight\">","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}