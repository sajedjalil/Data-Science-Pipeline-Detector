{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Upgrade to PyTorch 1.6","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"! pip install torch==1.6.0+cu101 torchvision==0.7.0+cu101 -f https://download.pytorch.org/whl/torch_stable.html","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install timm\n# PyTorch Image model from Ross Wightman\nimport timm\n# All these models can be used with this code.\n# print(timm.list_models())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\nimport torch\nimport torch.nn as nn\nimport time\nimport torch.optim as optim\nfrom torch.autograd import Variable\nfrom torch.utils.data import DataLoader\nimport pandas as pd\nfrom collections import OrderedDict\nfrom sklearn.model_selection import train_test_split\nimport os","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Load the Dataset, Create Dataloader","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Thanks to user Manav Kapadnis. \n\nI used his boiler plate to load the data !!! It became much easier for me to complete the script","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv(\"/kaggle/input/digit-recognizer/train.csv\",dtype = np.float32)\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Split data into features(pixels) and labels(numbers from 0 to 9)\n\nWe normalize the values of feature so that it becomes easier for processing\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"targets_numpy = train.label.values\nfeatures_numpy = train.loc[:,train.columns != \"label\"].values/255 # normalization","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features_train, features_test, targets_train, targets_test = train_test_split(features_numpy,targets_numpy,\n                                                                              test_size = 0.2,random_state = 2) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Creating Train and test dataset\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train=torch.from_numpy(features_train)\ny_train=torch.from_numpy(targets_train).type(torch.LongTensor)\n\nX_test = torch.from_numpy(features_test)\ny_test = torch.from_numpy(targets_test).type(torch.LongTensor)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = X_train.reshape(-1, 1, 28, 28)\nX_test = X_test.reshape(-1, 1, 28, 28)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Getting the Train and Test sets for model to operate on\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"We need to create PyTorch native Dataset so that we can train our model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"batch_size = 256\n\ntrain=torch.utils.data.TensorDataset(X_train,y_train)\ntest=torch.utils.data.TensorDataset(X_test,y_test)\n\n\n# data loader\ntrain_loader = DataLoader(train, batch_size = batch_size, shuffle = False)\ntest_loader = DataLoader(test, batch_size = batch_size, shuffle = False)\n\n# visualize one of the images in data set\nplt.imshow(features_numpy[8].reshape(28,28))\nplt.axis(\"off\")\nplt.title(str(targets_numpy[8]))\nplt.savefig('graph.png')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Create the model, loss and optimizer","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from torch.cuda import amp","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"MODEL_NAME = \"efficientnet_b3\"\nNUM_ClASSES = 10\nIN_CHANNELS = 1\nPRETRAINED = True  # If True -> Fine Tuning else Scratch Training\nEPOCHS = 3\nEARLY_STOPPING = True  # If you need early stoppoing for validation loss\nSAVE_PATH = \"{}.pt\".format(MODEL_NAME)\nSEED = 42\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Creating Model\")\n\nmodel = timm.create_model(MODEL_NAME, num_classes=NUM_ClASSES, in_chans=IN_CHANNELS, pretrained=True)\nif torch.cuda.is_available():\n    print(\"Model Created. Moving it to CUDA\")\nelse:\n    print(\"Model Created. Training on CPU only\")\n_ = model.to(device)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creates a GradScaler once at the beginning of training.\n# This is the different step while defining the model\nscaler = amp.GradScaler()\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=1e-3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Some utility functions","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"class AverageMeter(object):\n    \"\"\"Computes and stores the average and current value\"\"\"\n\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def accuracy(output, target, topk=(1,)):\n    \"\"\"Computes the accuracy over the k top predictions for the specified values of k\"\"\"\n    maxk = max(topk)\n    batch_size = target.size(0)\n    _, pred = output.topk(maxk, 1, True, True)\n    pred = pred.t()\n    correct = pred.eq(target.view(1, -1).expand_as(pred))\n    return [correct[:k].view(-1).float().sum(0) * 100.0 / batch_size for k in topk]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Training Loop","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"\n\nEarlier we required to do this with NVIDIA Apex.\nNow it is native in PyTorch\n\nAMP allows users to easily enable automatic mixed precision training enabling higher performance and memory savings of up to 50% on Tensor Core GPUs.\n\nUsing the natively supported torch.cuda.amp API, AMP provides convenience methods for mixed precision, where some operations use the torch.float32 (float) datatype and other operations use torch.float16 (half).\n\nSome ops, like linear layers and convolutions, are much faster in float16. Other ops, like reductions, often require the dynamic range of float32. Mixed precision tries to match each op to its appropriate datatype.\n\nYou can find more details in PyTorch [blog](https://pytorch.org/blog/accelerating-training-on-nvidia-gpus-with-pytorch-automatic-mixed-precision/) post which compares the performances and benhmarks some algorithms\n\nHere is An example to show how this works for CNNs.\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Changes in training loop to include Mixed Precision Training feature\n\n    Simply create a GradScaler from torch.cuda.amp\n    Change the losses and optimizer with scaler()\n\nThat's it it should support mixed precision training !!!\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_step(\n    model,\n    train_loader,\n    criterion,\n    device,\n    optimizer,\n    scheduler=None,\n    num_batches: int = None,\n    log_interval: int = 100,\n    grad_penalty: bool = False,\n    fp16_scaler=None,\n):\n    \"\"\"\n    Performs one step of training. Calculates loss, forward pass, computes gradient and returns metrics.\n    Args:\n        model : A pytorch CNN Model.\n        train_loader : Train loader.\n        criterion : Loss function to be optimized.\n        device : \"cuda\" or \"cpu\"\n        optimizer : Torch optimizer to train.\n        scheduler : Learning rate scheduler.\n        num_batches : (optional) Integer To limit training to certain number of batches.\n        log_interval : (optional) Defualt 100. Integer to Log after specified batch ids in every batch.\n        grad_penalty : (optional) To penalize with l2 norm for big gradients.\n        fp16_scaler: (optional) If True uses PyTorch native mixed precision Training.\n    \"\"\"\n\n    start_train_step = time.time()\n\n    model.train()\n    last_idx = len(train_loader) - 1\n    batch_time_m = AverageMeter()\n    # data_time_m = utils.AverageMeter()\n    losses_m = AverageMeter()\n    top1_m = AverageMeter()\n    top5_m = AverageMeter()\n    cnt = 0\n    batch_start = time.time()\n    # num_updates = epoch * len(loader)\n\n    for batch_idx, (inputs, target) in enumerate(train_loader):\n        last_batch = batch_idx == last_idx\n        # data_time_m.update(time.time() - batch_start)\n        inputs = inputs.to(device)\n        target = target.to(device)\n\n        # zero the parameter gradients\n        optimizer.zero_grad()\n\n        if fp16_scaler is not None:\n            with amp.autocast():\n                output = model(inputs)\n                loss = criterion(output, target)\n                # Scale the loss using Grad Scaler\n\n            if grad_penalty is True:\n                # Scales the loss for autograd.grad's backward pass, resulting in scaled grad_params\n                scaled_grad_params = torch.autograd.grad(\n                    fp16_scaler.scale(loss), model.parameters(), create_graph=True\n                )\n                # Creates unscaled grad_params before computing the penalty. scaled_grad_params are\n                # not owned by any optimizer, so ordinary division is used instead of fp16_scaler.unscale_:\n                inv_scale = 1.0 / fp16_scaler.get_scale()\n                grad_params = [p * inv_scale for p in scaled_grad_params]\n                # Computes the penalty term and adds it to the loss\n                with amp.autocast():\n                    grad_norm = 0\n                    for grad in grad_params:\n                        grad_norm += grad.pow(2).sum()\n\n                    grad_norm = grad_norm.sqrt()\n                    loss = loss + grad_norm\n\n            fp16_scaler.scale(loss).backward()\n            # Step using fp16_scaler.step()\n            fp16_scaler.step(optimizer)\n            # Update for next iteration\n            fp16_scaler.update()\n\n        else:\n            output = model(inputs)\n            loss = criterion(output, target)\n\n            if grad_penalty is True:\n                # Create gradients\n                grad_params = torch.autograd.grad(\n                    loss, model.parameters(), create_graph=True\n                )\n                # Compute the L2 Norm as penalty and add that to loss\n                grad_norm = 0\n                for grad in grad_params:\n                    grad_norm += grad.pow(2).sum()\n                grad_norm = grad_norm.sqrt()\n                loss = loss + grad_norm\n\n            loss.backward()\n            optimizer.step()\n\n        if scheduler is not None:\n            scheduler.step()\n\n        cnt += 1\n        acc1, acc5 = accuracy(output, target, topk=(1, 5))\n\n        top1_m.update(acc1.item(), output.size(0))\n        top5_m.update(acc5.item(), output.size(0))\n        losses_m.update(loss.item(), inputs.size(0))\n\n        batch_time_m.update(time.time() - batch_start)\n        batch_start = time.time()\n        if last_batch or batch_idx % log_interval == 0:  # If we reach the log intervel\n            print(\n                \"Batch Train Time: {batch_time.val:.3f} ({batch_time.avg:.3f})  \"\n                \"Loss: {loss.val:>7.4f} ({loss.avg:>6.4f})  \"\n                \"Top 1 Accuracy: {top1.val:>7.4f} ({top1.avg:>7.4f})  \"\n                \"Top 5 Accuracy: {top5.val:>7.4f} ({top5.avg:>7.4f})\".format(\n                    batch_time=batch_time_m, loss=losses_m, top1=top1_m, top5=top5_m\n                )\n            )\n\n        if num_batches is not None:\n            if cnt >= num_batches:\n                end_train_step = time.time()\n                metrics = OrderedDict(\n                    [(\"loss\", losses_m.avg), (\"top1\", top1_m.avg), (\"top5\", top5_m.avg)]\n                )\n                print(\"Done till {} train batches\".format(num_batches))\n                print(\n                    \"Time taken for train step = {} sec\".format(\n                        end_train_step - start_train_step\n                    )\n                )\n                return metrics\n\n    metrics = OrderedDict(\n        [(\"loss\", losses_m.avg), (\"top1\", top1_m.avg), (\"top5\", top5_m.avg)]\n    )\n    end_train_step = time.time()\n    print(\n        \"Time taken for train step = {} sec\".format(end_train_step - start_train_step)\n    )\n    return metrics\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def val_step(\n    model, val_loader, criterion, device, num_batches=None, log_interval: int = 100\n):\n    \"\"\"\n    Performs one step of validation. Calculates loss, forward pass and returns metrics.\n    Args:\n        model : A pytorch CNN Model.\n        val_loader : Validation loader.\n        criterion : Loss function to be optimized.\n        device : \"cuda\" or \"cpu\"\n        num_batches : (optional) Integer To limit validation to certain number of batches.\n        log_interval : (optional) Defualt 100. Integer to Log after specified batch ids in every batch.\n    \"\"\"\n    start_test_step = time.time()\n    last_idx = len(val_loader) - 1\n    batch_time_m = AverageMeter()\n    # data_time_m = utils.AverageMeter()\n    losses_m = AverageMeter()\n    top1_m = AverageMeter()\n    top5_m = AverageMeter()\n    cnt = 0\n    model.eval()\n    batch_start = time.time()\n    with torch.no_grad():\n        for batch_idx, (inputs, target) in enumerate(val_loader):\n            last_batch = batch_idx == last_idx\n            inputs = inputs.to(device)\n            target = target.to(device)\n\n            output = model(inputs)\n            if isinstance(output, (tuple, list)):\n                output = output[0]\n\n            loss = criterion(output, target)\n            cnt += 1\n            acc1, acc5 = accuracy(output, target, topk=(1, 5))\n            reduced_loss = loss.data\n\n            losses_m.update(reduced_loss.item(), inputs.size(0))\n            top1_m.update(acc1.item(), output.size(0))\n            top5_m.update(acc5.item(), output.size(0))\n            batch_time_m.update(time.time() - batch_start)\n\n            batch_start = time.time()\n\n            if (\n                last_batch or batch_idx % log_interval == 0\n            ):  # If we reach the log intervel\n                print(\n                    \"Batch Inference Time: {batch_time.val:.3f} ({batch_time.avg:.3f})  \"\n                    \"Loss: {loss.val:>7.4f} ({loss.avg:>6.4f})  \"\n                    \"Top 1 Accuracy: {top1.val:>7.4f} ({top1.avg:>7.4f})  \"\n                    \"Top 5 Accuracy: {top5.val:>7.4f} ({top5.avg:>7.4f})\".format(\n                        batch_time=batch_time_m, loss=losses_m, top1=top1_m, top5=top5_m\n                    )\n                )\n\n            if num_batches is not None:\n                if cnt >= num_batches:\n                    end_test_step = time.time()\n                    metrics = OrderedDict(\n                        [\n                            (\"loss\", losses_m.avg),\n                            (\"top1\", top1_m.avg),\n                            (\"top5\", top5_m.avg),\n                        ]\n                    )\n                    print(\"Done till {} validation batches\".format(num_batches))\n                    print(\n                        \"Time taken for validation step = {} sec\".format(\n                            end_test_step - start_test_step\n                        )\n                    )\n                    return metrics\n\n        metrics = OrderedDict(\n            [(\"loss\", losses_m.avg), (\"top1\", top1_m.avg), (\"top5\", top5_m.avg)]\n        )\n        print(\"Finished the validation epoch\")\n\n    end_test_step = time.time()\n    print(\n        \"Time taken for validation step = {} sec\".format(\n            end_test_step - start_test_step\n        )\n    )\n    return metrics\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for epoch in tqdm(range(EPOCHS)):\n    print()\n    print(\"Training Epoch = {}\".format(epoch))\n    train_metrics = train_step(model, train_loader, criterion, device, optimizer, fp16_scaler=scaler)\n    print()\n    print(\"Validating Epoch = {}\".format(epoch))\n    valid_metrics = val_step(model, test_loader, criterion, device)\n    validation_loss = valid_metrics[\"loss\"]\n\n    print(\"Done Training, Model Saved to Disk\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Final Word\n\n- We see that training is very fast, we get more advantages of accuracy.\n- Especially CNNs are benifitted the most.\n- I leave the submission to competition part as an exercise to the reader.\n- Trainer properly and submitting from this kernel will give you a very good LB Score","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}