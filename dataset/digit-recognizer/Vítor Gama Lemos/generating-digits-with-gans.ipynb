{"cells":[{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\n\nimport torch\nimport torch.nn as neural_network\nimport torch.nn.functional as functions\nimport torchvision.transforms as transforms\nimport torch.nn as model\nimport torch.nn.functional as functions\nimport torch.optim as optim\n\nfrom torchvision import datasets","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Generating Digits using Gans\n\nA generative adversarial network (GAN) is a class of machine learning frameworks designed by Ian Goodfellow and his colleagues in 2014. Two neural networks contest with each other in a game (in the form of a zero-sum game, where one agent's gain is another agent's loss). \n\n- More about GAN: https://en.wikipedia.org/wiki/Generative_adversarial_network\n- More about PyTorch: https://pytorch.org/\n\n\n## About this notebook\nThis notebook kernel was created to help you understand more about machine learning. I intend to create tutorials with several machine learning algorithms from basic to advanced. I hope I can help you with this data science trail. For any information, you can contact me through the link below.\n\n**Contact me: https://www.linkedin.com/in/vitorgamalemos/**\n\nOther noteboks about neural networks:\n\n- Simple Perceptron: https://www.kaggle.com/vitorgamalemos/neural-network-01-simple-perceptron\n- Multilayer Perceptron: https://www.kaggle.com/vitorgamalemos/neural-network-02-multilayer-perceptron\n- Convolutional neural network: https://www.kaggle.com/vitorgamalemos/object-recognition-using-convolutional-network"},{"metadata":{},"cell_type":"markdown","source":"## Download Data Digits and Transform"},{"metadata":{"trusted":true},"cell_type":"code","source":"def download_data_digits():\n    return datasets.MNIST(root='data', train=True, download=True, transform=transforms.ToTensor())\n\ntrain_data = download_data_digits()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Train Data Loader"},{"metadata":{"trusted":true},"cell_type":"code","source":"train = torch.utils.data.DataLoader(train_data, batch_size=64, num_workers=2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Visualizating Samples Test"},{"metadata":{"trusted":true},"cell_type":"code","source":"def show_samples_test(length=10):\n    images, labels = iter(train).next()\n    images = images.numpy()\n    \n    indexes = 0\n    for k in range(0, 5):\n        fig, axs = plt.subplots(1, length, figsize=(10, 10))\n\n        for i in range(0, length):\n            axs[i].imshow(np.squeeze(images[i + indexes]), label=labels[i + i + indexes])\n            axs[i].set_title('Y: {}'.format(labels[i + indexes]))\n            leg = axs[i].legend()\n            \n        indexes += 10\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"show_samples_test(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Discriminator Network\n\nDiscriminator Training Data\nThe discriminator's training data comes from two sources:\n\n- Real data instances, such as real pictures of people. The discriminator uses these instances as positive examples during training.\n- Fake data instances created by the generator. The discriminator uses these instances as negative examples during training.\n\n\n**Reference: https://developers.google.com/machine-learning/gan/discriminator**\n\n<img src=\"https://sthalles.github.io/assets/dcgan/GANs.png\">"},{"metadata":{"trusted":true},"cell_type":"code","source":"class Discriminator(neural_network.Module):\n\n    def __init__(self):\n        super(Discriminator, self).__init__()\n        \n        self.input_layer    = neural_network.Linear(784, 128)\n        self.hidden_layer_1 = neural_network.Linear(128, 64)\n        self.hidden_layer_2 = neural_network.Linear(64, 32)\n        self.output_layer   = neural_network.Linear(32, 1)\n\n        self.dropout = neural_network.Dropout(0.3)\n        \n        \n    def forward(self, img_input):\n        img_input = img_input.view(-1, 28 * 28)\n        img_input = functions.leaky_relu(self.input_layer(img_input), 0.2) \n        img_input = self.dropout(img_input)\n        img_input = functions.leaky_relu(self.hidden_layer_1(img_input), 0.2)\n        img_input = self.dropout(img_input)\n        img_input = functions.leaky_relu(self.hidden_layer_2(img_input), 0.2)\n        img_input = self.dropout(img_input)\n        \n        return self.output_layer(img_input)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Generator Network\n\nThe generator part of a GAN learns to create fake data by incorporating feedback from the discriminator. It learns to make the discriminator classify its output as real.\n\nGenerator training requires tighter integration between the generator and the discriminator than discriminator training requires. The portion of the GAN that trains the generator includes:\n\n- random input\n- generator network, which transforms the random input into a data instance\n- discriminator network, which classifies the generated data\n- discriminator output\n- generator loss, which penalizes the generator for failing to fool the discriminator\n\n\n**Reference: https://developers.google.com/machine-learning/gan/discriminator**\n   \n<img src=\" https://www.researchgate.net/publication/310610555/figure/fig4/AS:668380134662151@1536365650654/Architecture-of-the-generator-a-and-discriminator-b-of-our-cGAN-model-The-generator.ppm\">"},{"metadata":{"trusted":true},"cell_type":"code","source":"class Generator(neural_network.Module):\n\n    def __init__(self):\n        super(Generator, self).__init__()\n\n        self.input_layer    =  neural_network.Linear(100, 32)\n        self.hidden_layer_1 =  neural_network.Linear(32, 64)\n        self.hidden_layer_2 =  neural_network.Linear(64, 128)\n        self.output_layer   =  neural_network.Linear(128, 784)\n        \n        self.dropout = neural_network.Dropout(0.3)\n\n    def forward(self, img_input):\n        img_input = functions.leaky_relu(self.input_layer(img_input), 0.2)\n        img_input = self.dropout(img_input)\n        img_input = functions.leaky_relu(self.hidden_layer_1(img_input), 0.2)\n        img_input = self.dropout(img_input)\n        img_input = functions.leaky_relu(self.hidden_layer_2(img_input), 0.2)\n        img_input = self.dropout(img_input)\n        \n        return functions.tanh(self.output_layer(img_input))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"D = Discriminator()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"G = Generator()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def real_loss(D_out):\n    batch_size = D_out.size(0)\n    criterion = neural_network.BCEWithLogitsLoss()\n    return criterion(D_out.squeeze(), torch.ones(batch_size) * 0.9)\n    \n\ndef fake_loss(D_out):\n    criterion = neural_network.BCEWithLogitsLoss()\n    return criterion(D_out.squeeze(), torch.zeros(D_out.size(0)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"d_optimizer = optim.Adam(D.parameters(), 0.002)\ng_optimizer = optim.Adam(G.parameters(), 0.002)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# GAN Training\nBecause a GAN contains two separately trained networks, its training algorithm must address two complications:\n\n- GANs must juggle two different kinds of training (generator and discriminator).\n- GAN convergence is hard to identify.\n\nAlternating Training\nThe generator and the discriminator have different training processes. So how do we train the GAN as a whole?\n\nGAN training proceeds in alternating periods:\n\n- The discriminator trains for one or more epochs.\n- The generator trains for one or more epochs.\n- Repeat steps 1 and 2 to continue to train the generator and discriminator networks.\n\n**Reference: https://developers.google.com/machine-learning/gan/training**\n\n## Train Discriminator\n\n<img src=\"https://www.researchgate.net/publication/333831200/figure/fig5/AS:782113389412353@1563481771648/GAN-framework-in-which-the-generator-and-discriminator-are-learned-during-the-training.png\">"},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_discriminator(G, D, optimizer, real_data, batch_size, size):\n    real_data = scale(real_data.view(batch_size, -1))\n\n    optimizer.zero_grad()\n    D.train()\n\n    fake_data = G.forward(random_vector(batch_size, size))\n    total_loss = real_loss(D.forward(real_data)) + fake_loss(D.forward(fake_data))\n    total_loss.backward()\n    optimizer.step()\n    \n    return total_loss","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Train Generator"},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef train_generator(G, D, optimizer, batch_size, size):\n    optimizer.zero_grad()\n    G.train()\n    fake_data = G.forward(random_vector(batch_size, size))\n    loss = real_loss(D.forward(fake_data))\n    loss.backward()\n    optimizer.step()\n    return loss","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def random_vector(batch_size, lenx):\n    return torch.randn(batch_size, lenx).float()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def show_training_sample(data):\n    fig, axes = plt.subplots(figsize=(7,7), nrows=4, ncols=4)\n    for ax, img in zip(axes.flatten(), data[-1]):\n        img = img.detach()\n        ax.imshow(img.reshape((28,28)))\n    plt.show()\n    \n    \ndef show_training_loss(d_loss_, g_loss):\n    plt.plot(d_loss_arr, label=\"N.Discriminator\", alpha=0.5)\n    plt.plot(g_loss_arr, label=\"N.Generator\", alpha=0.5)\n    plt.title(\"Trainings loss\")\n    plt.legend()\n    plt.show()\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pickle as pkl\n\nmax_epochs = 100\n\nsamples = list()\nlosses =  list()\nd_loss_arr = list()\ng_loss_arr = list()\n\nZ = torch.from_numpy(np.random.uniform(-1, 1, size=(16, 100))).float()\n\nD.train()\nG.train()\n                           \nfor epoch in range(1, max_epochs):\n    for index, (img, l) in enumerate(train):\n        img = img * 2 - 1 \n        \n        d_optimizer.zero_grad()\n        \n        fake_images = G(torch.from_numpy(np.random.uniform(-1, 1, size=(img.size(0), 100))).float())\n\n        d_loss = (real_loss(D(img))) + fake_loss(D(fake_images))\n        d_loss.backward()\n        d_optimizer.step()\n        \n        g_optimizer.zero_grad()\n\n        fake_images = G(torch.from_numpy(np.random.uniform(-1, 1, size=(img.size(0), 100))).float())\n\n        g_loss = real_loss(D(fake_images)) \n        \n       \n        g_loss.backward()\n        g_optimizer.step()\n        \n        d_loss_arr.append(d_loss.item())\n        g_loss_arr.append(g_loss.item())\n        \n      \n        if index % 10 == 0 or index == 1:\n            print(f'[{epoch}] -- discriminator_loss: {d_loss.item()}  -- generator_loss: {g_loss.item()}')\n        \n\n    losses.append((d_loss.item(), g_loss.item()))\n\n    G.eval() \n    samples.append(G(Z))\n    G.train()\n    \n    show_training_sample(samples)\n    show_training_loss(d_loss_arr, g_loss_arr)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}