{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<h1 style = \"text-align :center; color:white; background-image:url(https://cdn.pixabay.com/photo/2014/06/16/23/40/blue-370128__340.png)\"> About Competition </h1>","metadata":{}},{"cell_type":"markdown","source":"<h2 style=\"border-style: outset;border-color: red;text-align: center;\">SIIM-FISABIO-RSNA COVID-19 Study Level Predictions</h2>\n\n<img src=\"https://content.presspage.com/uploads/2110/gettyimages-1214942330.jpg\" height=\"500\" width=\"500\" style=\"display: block;margin-left: auto;margin-right: auto;\"> \n\n<h2 style=\"text-align: center;border-style: double;text-align: center;border-color: red; \">About SIIM</h2>\n<img src=\"https://siim.org/resource/resmgr/SIIM_logo-600x315.png\" width=\"200\" style=\"display: block;margin-left: auto;margin-right: auto;\">\n<p> <b>Society for Imaging Informatics in Medicine</b> (<a href=\"https://siim.org/\">SIIM</a>) is the leading healthcare professional organization for those interested in the current and future use of informatics in medical imaging. The society's mission is to advance medical imaging informatics across the enterprise through education, research, and innovation in a multi-disciplinary community.</p>\n\n<h3 style = \"text-align :center; color:red; background-color: yellow; \">This is my first Data Visualization Notebook. Kindly comment if there are any mistakes ðŸ™‚</h3>\n\n<a href = \"https://www.kaggle.com/shanmukh05/siim-covid19-dataset-256px-jpg\" style=\"font-weight:'bold'; color:blue; font-family:monospace; \"><h3>My Dataset</h3></a>\n\n<a href = \"https://www.kaggle.com/shanmukh05/siim-covid-19-data-preparation-for-detectron2\" style=\"font-weight:'bold'; color:blue; font-family:monospace; \"><h3>My Data Preparation Notebook</h3></a> \n\n<a href = \"https://www.kaggle.com/shanmukh05/siim-covid-19-yolo-v5-image-level-predictions/output\" style=\"font-weight:'bold'; color:blue; font-family:monospace; \"><h3>YOLO v5 Image Level Training Notebook</h3></a> \n\n<a href = \"https://www.kaggle.com/shanmukh05/siim-covid-19-detection-detectron2-training\" style=\"font-weight:'bold'; color:blue; font-family:monospace; \"><h3>My Training Notebook</h3></a> \nTo be updated","metadata":{}},{"cell_type":"markdown","source":"<h2 style = \"text-align :center; font-family:verdana; color:red; background-image: url(https://hookagency.com/wp-content/uploads/2015/11/miracle-grow-light-green-gradient.jpg); \">Install Requirements</h2>","metadata":{}},{"cell_type":"code","source":"!/opt/conda/bin/python3.7 -m pip install --upgrade pip\n! pip install -q efficientnet","metadata":{"execution":{"iopub.status.busy":"2021-05-28T10:27:57.986018Z","iopub.execute_input":"2021-05-28T10:27:57.986711Z","iopub.status.idle":"2021-05-28T10:28:28.563336Z","shell.execute_reply.started":"2021-05-28T10:27:57.9866Z","shell.execute_reply":"2021-05-28T10:28:28.562101Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2 style = \"text-align :center; font-family:verdana; color:black; background-image: url(https://i.pinimg.com/originals/dd/f9/97/ddf997d65d8b92a3d7e085d6bf1cc484.jpg); \">Importing Dependencies</h2>","metadata":{}},{"cell_type":"code","source":"#-------------------\n# importing libraries\n#-------------------\nimport tensorflow as tf\nfrom tensorflow.keras import layers\nimport efficientnet.tfkeras as efn\nfrom sklearn.model_selection import KFold,StratifiedKFold\n\nfrom kaggle_datasets import KaggleDatasets\n\n\nimport pandas as pd\nimport numpy as np\n\nimport os\nimport shutil\nimport csv\n\nimport matplotlib.pyplot as plt\nimport PIL","metadata":{"execution":{"iopub.status.busy":"2021-05-28T10:28:28.565848Z","iopub.execute_input":"2021-05-28T10:28:28.566163Z","iopub.status.idle":"2021-05-28T10:28:34.354792Z","shell.execute_reply.started":"2021-05-28T10:28:28.566132Z","shell.execute_reply":"2021-05-28T10:28:34.353765Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2 style = \"text-align :center; font-family:verdana; color:red; background-image: url(https://hookagency.com/wp-content/uploads/2015/11/miracle-grow-light-green-gradient.jpg); \">Checking TPU access</h2>","metadata":{}},{"cell_type":"code","source":"AUTO = tf.data.experimental.AUTOTUNE\n\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  \n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    strategy = tf.distribute.get_strategy()\n\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)","metadata":{"execution":{"iopub.status.busy":"2021-05-28T10:28:34.356642Z","iopub.execute_input":"2021-05-28T10:28:34.357005Z","iopub.status.idle":"2021-05-28T10:28:40.729587Z","shell.execute_reply.started":"2021-05-28T10:28:34.35697Z","shell.execute_reply":"2021-05-28T10:28:40.728657Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"GCS_DS_PATH = KaggleDatasets().get_gcs_path(\"siim-covid19-dataset-256px-jpg\")\nprint(GCS_DS_PATH)\n\nTRAIN_PATH = GCS_DS_PATH + \"/256px/train/train/\"\nTEST_PATH = GCS_DS_PATH + \"/256px/test/test/\"\n\nNUM_CLASSES = 4\nHEIGHT,WIDTH = 256,256\nCHANNELS = 3\nBATCH_SIZE = 8 * strategy.num_replicas_in_sync\nSEED = 143\n\n\nclasses_dict = {\n    \"Negative for Pneumonia\" : 0,\n    \"Typical Appearance\" : 1,\n    \"Indeterminate Appearance\" : 2,\n    \"Atypical Appearance\"  : 3\n}\n\nstudy_df = pd.read_csv(\"../input/siim-covid19-detection/train_study_level.csv\")\ntrain_df = pd.read_csv('../input/siim-covid19-dataset-256px-jpg/train.csv')\ntrain_df = train_df[[\"ImageInstanceUID\",\"StudyInstanceUID\",\"label_id\",\"study_label\"]]","metadata":{"execution":{"iopub.status.busy":"2021-05-28T10:28:40.731824Z","iopub.execute_input":"2021-05-28T10:28:40.732269Z","iopub.status.idle":"2021-05-28T10:28:41.126409Z","shell.execute_reply.started":"2021-05-28T10:28:40.732238Z","shell.execute_reply":"2021-05-28T10:28:41.125382Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.head(3)","metadata":{"execution":{"iopub.status.busy":"2021-05-28T10:28:41.12756Z","iopub.execute_input":"2021-05-28T10:28:41.127833Z","iopub.status.idle":"2021-05-28T10:28:41.146818Z","shell.execute_reply.started":"2021-05-28T10:28:41.127807Z","shell.execute_reply":"2021-05-28T10:28:41.14568Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"study_df.head(3)","metadata":{"execution":{"iopub.status.busy":"2021-05-28T10:28:41.148227Z","iopub.execute_input":"2021-05-28T10:28:41.148661Z","iopub.status.idle":"2021-05-28T10:28:41.1602Z","shell.execute_reply.started":"2021-05-28T10:28:41.148588Z","shell.execute_reply":"2021-05-28T10:28:41.158997Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2 style = \"text-align :center; font-family:verdana; color:black; background-image: url(https://i.pinimg.com/originals/dd/f9/97/ddf997d65d8b92a3d7e085d6bf1cc484.jpg); \">Utility Functions for Data Preprocessing</h2>","metadata":{}},{"cell_type":"code","source":"def process_img(filepath,label):\n    image = tf.io.read_file(filepath)\n    image = tf.image.decode_jpeg(image, channels=CHANNELS)\n    image = tf.image.convert_image_dtype(image, tf.float32) \n    #image = tf.image.resize(image, [HEIGHT,WIDTH])\n    return image,label\n\n\ndef data_augment(image, label):\n    p_spatial = tf.random.uniform([], 0, 1.0, dtype=tf.float32)\n    p_rotate = tf.random.uniform([], 0, 1.0, dtype=tf.float32)\n    p_pixel_1 = tf.random.uniform([], 0, 1.0, dtype=tf.float32)\n    p_pixel_2 = tf.random.uniform([], 0, 1.0, dtype=tf.float32)\n    p_pixel_3 = tf.random.uniform([], 0, 1.0, dtype=tf.float32)\n    p_crop = tf.random.uniform([], 0, 1.0, dtype=tf.float32)\n            \n    # Flips\n    image = tf.image.random_flip_left_right(image)\n    image = tf.image.random_flip_up_down(image)\n    if p_spatial > .75:\n        image = tf.image.transpose(image)\n        \n    # Rotates\n    if p_rotate > .75:\n        image = tf.image.rot90(image, k=3) \n    elif p_rotate > .5:\n        image = tf.image.rot90(image, k=2) \n    elif p_rotate > .25:\n        image = tf.image.rot90(image, k=1) \n        \n    \n    if p_pixel_1 >= .4:\n        image = tf.image.random_saturation(image, lower=.7, upper=1.3)\n    if p_pixel_2 >= .4:\n        image = tf.image.random_contrast(image, lower=.8, upper=1.2)\n    if p_pixel_3 >= .4:\n        image = tf.image.random_brightness(image, max_delta=.1)\n        \n    \n    if p_crop > .7:\n        if p_crop > .9:\n            image = tf.image.central_crop(image, central_fraction=.7)\n        elif p_crop > .8:\n            image = tf.image.central_crop(image, central_fraction=.8)\n        else:\n            image = tf.image.central_crop(image, central_fraction=.9)\n    elif p_crop > .4:\n        crop_size = tf.random.uniform([], int(HEIGHT*.8), HEIGHT, dtype=tf.int32)\n        image = tf.image.random_crop(image, size=[crop_size, crop_size, CHANNELS])\n    \n    image = tf.image.resize(image, [HEIGHT,WIDTH])\n    return image,label\n\ndef get_dataset(filenames,labels, training=True):\n    dataset = tf.data.Dataset.from_tensor_slices((filenames,labels))\n    dataset = dataset.map(process_img,num_parallel_calls=AUTO)\n    dataset = dataset.map(data_augment,num_parallel_calls=AUTO)\n    dataset = dataset.cache()\n    dataset = dataset.repeat()\n    if training:\n        dataset = dataset.shuffle(2048)\n    dataset = dataset.batch(BATCH_SIZE)\n    dataset = dataset.prefetch(AUTO)\n    return dataset","metadata":{"execution":{"iopub.status.busy":"2021-05-28T10:28:41.16198Z","iopub.execute_input":"2021-05-28T10:28:41.162404Z","iopub.status.idle":"2021-05-28T10:28:41.182195Z","shell.execute_reply.started":"2021-05-28T10:28:41.162359Z","shell.execute_reply":"2021-05-28T10:28:41.181165Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2 style = \"text-align :center; font-family:verdana; color:black; background-image: url(https://i.pinimg.com/originals/dd/f9/97/ddf997d65d8b92a3d7e085d6bf1cc484.jpg); \">Model Function</h2>","metadata":{}},{"cell_type":"code","source":"def create_model():\n    \n    pretrained = efn.EfficientNetB4(include_top=False, weights='noisy-student',input_shape=[HEIGHT,WIDTH, 3])\n            \n    x = pretrained.output\n    x = tf.keras.layers.GlobalAveragePooling2D() (x)\n    outputs = tf.keras.layers.Dense(NUM_CLASSES,activation=\"softmax\", dtype='float32')(x)\n        \n    model = tf.keras.Model(pretrained.input, outputs)\n    return model\n\nmodel = create_model()\n#model.summary()","metadata":{"execution":{"iopub.status.busy":"2021-05-28T10:28:41.184475Z","iopub.execute_input":"2021-05-28T10:28:41.184782Z","iopub.status.idle":"2021-05-28T10:28:52.820421Z","shell.execute_reply.started":"2021-05-28T10:28:41.184753Z","shell.execute_reply":"2021-05-28T10:28:52.819385Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#chxnet model\n\nwith strategy.scope():\n    model = tf.keras.applications.DenseNet121(weights= \"imagenet\",\n                                    include_top=False,\n                                    input_shape=(HEIGHT,WIDTH,CHANNELS), pooling=\"avg\")\n    predictions = tf.keras.layers.Dense(14, activation='sigmoid', name='predictions')(model.output)\n\n    model = tf.keras.Model(inputs=model.input, outputs=predictions)   \n    model.load_weights(\"../input/pneumonia-classification-challenge/pretrained.h5\")\n    model = tf.keras.Model(model.input, model.layers[-2].output) \n    x = tf.keras.layers.Dense(512, activation = \"relu\")(model.output)\n    x = tf.keras.layers.Dropout(0.3)(x)\n    x = tf.keras.layers.Dense(128, activation = \"relu\")(x)\n    x = tf.keras.layers.Dense(64, activation = \"relu\")(x)\n    outputs = tf.keras.layers.Dense(NUM_CLASSES, activation = \"softmax\", dtype = tf.float32)(x)\n\n    chxnet = tf.keras.Model(model.input,outputs)\n\n    for layer in chxnet.layers[:-15]:\n        layer.trainble = False","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2 style = \"text-align :center; font-family:verdana; color:black; background-image: url(https://i.pinimg.com/originals/dd/f9/97/ddf997d65d8b92a3d7e085d6bf1cc484.jpg); \">Compiling the Model</h2>","metadata":{}},{"cell_type":"code","source":"import tensorflow_addons as tfa\n\ndef compile_model(model, lr=0.001):\n    \n    optimizer = tf.keras.optimizers.Adam(lr=lr)\n    \n    loss = tf.keras.losses.CategoricalCrossentropy()\n   \n    metrics = [\n       tfa.metrics.F1Score(num_classes = NUM_CLASSES,average = \"macro\", name = \"f1_score\"),\n       tf.keras.metrics.CategoricalAccuracy(name='acc')\n    ]\n\n    model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n\n    return model","metadata":{"execution":{"iopub.status.busy":"2021-05-28T10:28:52.821914Z","iopub.execute_input":"2021-05-28T10:28:52.822209Z","iopub.status.idle":"2021-05-28T10:28:52.955686Z","shell.execute_reply.started":"2021-05-28T10:28:52.822181Z","shell.execute_reply":"2021-05-28T10:28:52.954595Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2 style = \"text-align :center; font-family:verdana; color:black; background-image: url(https://i.pinimg.com/originals/dd/f9/97/ddf997d65d8b92a3d7e085d6bf1cc484.jpg); \">Model Callbacks</h2>","metadata":{}},{"cell_type":"code","source":"METRIC = \"val_acc\"\n\ndef create_callbacks(kfold,metric = METRIC):\n    \n    cpk_path = f'./best_model_{kfold}.h5'\n    \n    checkpoint = tf.keras.callbacks.ModelCheckpoint(\n        filepath=cpk_path,\n        monitor= metric,\n        mode='max',\n        save_best_only=True,\n        verbose=1,\n    )\n\n    reducelr = tf.keras.callbacks.ReduceLROnPlateau(\n        monitor= metric,\n        mode='max',\n        factor=0.1,\n        patience=3,\n        verbose=0\n    )\n\n    earlystop = tf.keras.callbacks.EarlyStopping(\n        monitor= metric,\n        mode='max',\n        patience=10, \n        verbose=1\n    )\n    \n    callbacks = [checkpoint, reducelr, earlystop]         \n    \n    return callbacks","metadata":{"execution":{"iopub.status.busy":"2021-05-28T10:28:52.956863Z","iopub.execute_input":"2021-05-28T10:28:52.957131Z","iopub.status.idle":"2021-05-28T10:28:52.964214Z","shell.execute_reply.started":"2021-05-28T10:28:52.957106Z","shell.execute_reply":"2021-05-28T10:28:52.96302Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2 style = \"text-align :center; font-family:verdana; color:black; background-image: url(https://i.pinimg.com/originals/dd/f9/97/ddf997d65d8b92a3d7e085d6bf1cc484.jpg); \">Training</h2>","metadata":{}},{"cell_type":"code","source":"files_ls = tf.io.gfile.glob(TRAIN_PATH + \"*.jpg\" )\nfiles_df = pd.DataFrame(files_ls, columns = [\"filepath\"])\n\nlabels = np.zeros((len(files_ls),NUM_CLASSES))\ntmp_labels = np.zeros((len(files_ls)))\n\ndef get_id(filepath):\n    tmp = filepath.split(\"/\")[-1]\n    tmp = tmp.split(\".\")[0]\n    tmp = tmp.split(\"_\")[-1]\n    return tmp\n\nfor i in range(len(files_ls)):\n    image_id = get_id(files_ls[i])\n    label_id = train_df[train_df[\"ImageInstanceUID\"] == image_id][\"study_label\"]\n    labels[i][label_id] = 1\n    tmp_labels[i] = label_id\n    \nprint(\"Labels shape: \",labels.shape)","metadata":{"execution":{"iopub.status.busy":"2021-05-28T10:32:15.024592Z","iopub.execute_input":"2021-05-28T10:32:15.024988Z","iopub.status.idle":"2021-05-28T10:32:16.191009Z","shell.execute_reply.started":"2021-05-28T10:32:15.024956Z","shell.execute_reply":"2021-05-28T10:32:16.19021Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"EPOCHS = 30\nVERBOSE = 1\nN_SPLITS = 5\n\nkfold = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\nhistory = {}\n\n\nfor fold,(tID,vID) in enumerate(kfold.split(files_ls,tmp_labels)):\n    tFiles, tLabels = list(files_df.iloc[tID][\"filepath\"]) , labels[tID]\n    vFiles, vLabels = list(files_df.iloc[vID][\"filepath\"]) , labels[vID]\n    print(\"Number of Training Images: \",len(tID))\n    print(\"Number of Validation Images: \",len(vID))\n    \n    STEPS_PER_EPOCH  = len(tID)//BATCH_SIZE\n    VALID_STEPS = len(vID)//BATCH_SIZE\n    \n    tf.keras.backend.clear_session()\n    \n    train_ds = get_dataset(tFiles,tLabels, training = True)\n    val_ds = get_dataset(vFiles, vLabels, training = False)\n    \n    with strategy.scope():\n        #model = create_model()\n        model = chxnet\n        model = compile_model(model, lr=0.0001)\n        callbacks = create_callbacks(kfold = fold)\n    \n        print(\"------------------Fold - \",fold+1,\" --------------------------\")\n        history[fold] = model.fit(\n                            train_ds,\n                            epochs=EPOCHS,\n                            callbacks=callbacks,\n                            validation_data = val_ds,\n                            verbose=VERBOSE,\n                            steps_per_epoch = STEPS_PER_EPOCH,\n                            validation_steps=VALID_STEPS\n                           )","metadata":{"execution":{"iopub.status.busy":"2021-05-28T10:32:23.889864Z","iopub.execute_input":"2021-05-28T10:32:23.890429Z","iopub.status.idle":"2021-05-28T10:53:38.886033Z","shell.execute_reply.started":"2021-05-28T10:32:23.890396Z","shell.execute_reply":"2021-05-28T10:53:38.884997Z"},"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2 style = \"text-align :center; font-family:verdana; color:black; background-image: url(https://i.pinimg.com/originals/dd/f9/97/ddf997d65d8b92a3d7e085d6bf1cc484.jpg); \">History Plotting</h2>","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(8*N_SPLITS,24))\n\nfor i in range(N_SPLITS):\n    acc = history[i].history['acc']\n    val_acc = history[i].history['val_acc']\n    f1 = history[i].history['f1_score']\n    val_f1 = history[i].history['val_f1_score']\n    loss = history[i].history['loss']\n    val_loss = history[i].history['val_loss']\n    epochs_range = range(len(history[i].history['val_loss'])) \n    \n    plt.subplot(N_SPLITS, 3,i*3+1)\n    plt.plot(epochs_range, acc, label='Training Accuracy')\n    plt.plot(epochs_range, val_acc, label='Validation  Accuracy')\n    plt.legend(loc='lower right')\n    plt.title(f'FOLD:{str(i)} Training and Validation  Accuracy')\n    \n    plt.subplot(N_SPLITS, 3,i*3+2)\n    plt.plot(epochs_range, f1, label='Training F1 score')\n    plt.plot(epochs_range, val_f1, label='Validation  F1 score')\n    plt.legend(loc='lower right')\n    plt.title(f'FOLD:{str(i)} Training and Validation  F1 score')\n    \n    plt.subplot(N_SPLITS, 3, i*3+3)\n    plt.plot(epochs_range, loss, label='Training Loss')\n    plt.plot(epochs_range, val_loss, label='Validation Loss')\n    plt.legend(loc='upper right')\n    plt.title(f'FOLD:{str(i)} Training and Validation Loss')\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-05-28T10:53:38.890379Z","iopub.execute_input":"2021-05-28T10:53:38.89078Z","iopub.status.idle":"2021-05-28T10:53:41.459106Z","shell.execute_reply.started":"2021-05-28T10:53:38.890741Z","shell.execute_reply":"2021-05-28T10:53:41.458051Z"},"trusted":true},"execution_count":null,"outputs":[]}]}