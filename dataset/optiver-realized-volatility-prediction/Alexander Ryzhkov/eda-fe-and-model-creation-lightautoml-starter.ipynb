{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Offline LightAutoML installation ","metadata":{}},{"cell_type":"code","source":"!cp ../input/lightautoml-framework-lama/PyMeeus-0.5.11.tar.gz.txt PyMeeus-0.5.11.tar.gz && pip install PyMeeus-0.5.11.tar.gz \n!cp ../input/lightautoml-framework-lama/efficientnet_pytorch-0.7.1.tar.gz.txt efficientnet_pytorch-0.7.1.tar.gz && pip install efficientnet_pytorch-0.7.1.tar.gz\n!cp ../input/lightautoml-framework-lama/json2html-1.3.0.tar.gz.txt json2html-1.3.0.tar.gz && pip install json2html-1.3.0.tar.gz\n!cp ../input/lightautoml-framework-lama/log_calls-0.3.2.tar.gz.txt log_calls-0.3.2.tar.gz && pip install log_calls-0.3.2.tar.gz\n!cp ../input/lightautoml-framework-lama/pyperclip-1.8.2.tar.gz.txt pyperclip-1.8.2.tar.gz && pip install pyperclip-1.8.2.tar.gz\n!rm -rf *.tar.gz\n!pip install --no-index --find-links=../input/lightautoml-framework-lama lightautoml","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-07-25T17:30:03.177308Z","iopub.execute_input":"2021-07-25T17:30:03.178088Z","iopub.status.idle":"2021-07-25T17:32:46.357999Z","shell.execute_reply.started":"2021-07-25T17:30:03.177954Z","shell.execute_reply":"2021-07-25T17:32:46.356586Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Libraries imports","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\npd.set_option('max_rows', 300)\npd.set_option('max_columns', 300)\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport os\nfrom collections import ChainMap\nfrom joblib import Parallel, delayed\nfrom tqdm.notebook import tqdm\nfrom sklearn.model_selection import KFold, train_test_split\n\n# LightAutoML presets, task and report generation\nfrom lightautoml.automl.presets.tabular_presets import TabularAutoML, TabularUtilizedAutoML\nfrom lightautoml.tasks import Task\nfrom lightautoml.report.report_deco import ReportDeco\n\n%matplotlib inline\nfrom matplotlib import pyplot as plt, rcParams\nrcParams.update({'font.size': 22})","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.status.busy":"2021-07-25T17:32:52.931237Z","iopub.execute_input":"2021-07-25T17:32:52.931747Z","iopub.status.idle":"2021-07-25T17:33:07.56007Z","shell.execute_reply.started":"2021-07-25T17:32:52.931705Z","shell.execute_reply":"2021-07-25T17:33:07.558474Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Global constants ","metadata":{}},{"cell_type":"code","source":"INPUT_PATH = '../input/optiver-realized-volatility-prediction/'\nN_THREADS = 4","metadata":{"execution":{"iopub.status.busy":"2021-07-25T17:33:07.562016Z","iopub.execute_input":"2021-07-25T17:33:07.562365Z","iopub.status.idle":"2021-07-25T17:33:07.567277Z","shell.execute_reply.started":"2021-07-25T17:33:07.562334Z","shell.execute_reply":"2021-07-25T17:33:07.566034Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Functions for preprocess","metadata":{}},{"cell_type":"code","source":"def calc_wap(df):\n    wap = (df['bid_price1'] * df['ask_size1'] + df['ask_price1'] * df['bid_size1'])/(df['bid_size1'] + df['ask_size1'])\n    return wap\n\ndef calc_wap2(df):\n    wap = (df['bid_price2'] * df['ask_size2'] + df['ask_price2'] * df['bid_size2'])/(df['bid_size2'] + df['ask_size2'])\n    return wap\n\ndef calc_mean_price(df):\n    mp = (df['bid_price1'] + df['ask_price1']) / 2\n    return mp\n\ndef log_return(list_stock_prices):\n    return np.log(list_stock_prices).diff() \n\ndef realized_volatility(series):\n    return np.sqrt(np.sum(series**2))\n\ndef count_unique(series):\n    return len(np.unique(series))","metadata":{"execution":{"iopub.status.busy":"2021-07-25T17:33:11.370414Z","iopub.execute_input":"2021-07-25T17:33:11.370859Z","iopub.status.idle":"2021-07-25T17:33:11.381354Z","shell.execute_reply.started":"2021-07-25T17:33:11.370822Z","shell.execute_reply":"2021-07-25T17:33:11.379736Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def calc_array_feats(arr, prefix, q_cnt = 10, diff = True):\n    if diff:\n        arr = np.diff(np.array(arr))\n    percs = np.linspace(0, 100, q_cnt + 1).astype(int)\n    cols = [prefix + '__P' + str(p)  for p in percs]\n    if len(arr) > 0:\n        vals = np.percentile(arr, percs)\n    else:\n        vals = [np.nan] * len(cols)\n    res = dict(zip(cols,vals))\n    return res","metadata":{"execution":{"iopub.status.busy":"2021-07-25T17:33:12.404641Z","iopub.execute_input":"2021-07-25T17:33:12.405077Z","iopub.status.idle":"2021-07-25T17:33:12.412178Z","shell.execute_reply.started":"2021-07-25T17:33:12.40504Z","shell.execute_reply":"2021-07-25T17:33:12.411166Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def rmspe(y_true, y_pred, **kwargs):\n    return (np.sqrt(np.mean(np.square((y_true - y_pred) / y_true))))","metadata":{"execution":{"iopub.status.busy":"2021-07-25T17:33:13.987637Z","iopub.execute_input":"2021-07-25T17:33:13.988225Z","iopub.status.idle":"2021-07-25T17:33:13.994075Z","shell.execute_reply.started":"2021-07-25T17:33:13.988189Z","shell.execute_reply":"2021-07-25T17:33:13.992954Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_targ_enc_feature(col, targ_col, transform_func, tr_data, te_data, n_folds = 20, n_runs = 1):\n    # Test col is transformed by the whole train set aggregation\n    stock_id_target_trans = tr_data.groupby(col)[targ_col].agg(transform_func) \n    te_col_transformed = te_data[col].map(stock_id_target_trans)\n\n    # Train col can be transformed only inside CV not to overfit\n    # New values imputed with global train values\n    glob_val = transform_func(tr_data[col].values)\n    tr_col_transformed = np.repeat(0.0, tr_data.shape[0])\n    for i in range(n_runs):\n        kf = KFold(n_splits = n_folds, shuffle = True, random_state = 13)\n        for idx_train, idx_val in kf.split(tr_data):\n            target_trans = tr_data.iloc[idx_train].groupby(col)[targ_col].agg(transform_func) \n            tr_col_transformed[idx_val] += tr_data[col].iloc[idx_val].map(target_trans).fillna(glob_val) / n_runs\n        \n    return tr_col_transformed, te_col_transformed","metadata":{"execution":{"iopub.status.busy":"2021-07-25T17:33:15.274809Z","iopub.execute_input":"2021-07-25T17:33:15.2752Z","iopub.status.idle":"2021-07-25T17:33:15.283934Z","shell.execute_reply.started":"2021-07-25T17:33:15.275161Z","shell.execute_reply":"2021-07-25T17:33:15.282538Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# EDA and data visualization ","metadata":{}},{"cell_type":"code","source":"def create_plot(stock_id, time_id, book_train, trade_train):\n    # Select time_id\n    bt = book_train.query('time_id == {}'.format(time_id))\n    bt['wap'] = calc_wap(bt)\n\n    trades = trade_train.query('time_id == {}'.format(time_id))\n    trades['seconds_in_bucket'] = np.maximum(trades['seconds_in_bucket'] - 1, 0)\n\n    # Combine trades prices/timestamps with book prices/timestamps\n    diffs = []\n    times = set(bt['seconds_in_bucket'].values)\n    for t in trades['seconds_in_bucket'].values:\n        d = 0\n        while t >= 0:\n            if t in times:\n                diffs.append(d)\n                break\n            else:\n                t -= 1\n                d += 1\n        if t == -1:\n            print('Negative!')\n\n    trades['seconds_in_bucket'] -= diffs\n\n    # Merged and calc the color (buy/sell)\n    merged = pd.merge(bt, trades[['seconds_in_bucket', 'price', 'size']], on = 'seconds_in_bucket', how = 'left').dropna()\n    merged['diff_with_bid'] = merged['price'] - merged['bid_price1']\n    merged['diff_with_ask'] = merged['ask_price1'] - merged['price']\n    merged['color'] = (merged['diff_with_bid'] < merged['diff_with_ask']).astype(int).map({0: 'green', 1: 'red'})\n\n    fig = plt.figure(figsize = (60, 20))\n    plt.plot(bt['seconds_in_bucket'].values, bt['ask_price2'].values, 'b--', linewidth = 1, label = 'Ask price 2')\n    plt.plot(bt['seconds_in_bucket'].values, bt['ask_price1'].values, 'b', linewidth = 2, label = 'Ask price 1')\n    plt.plot(bt['seconds_in_bucket'].values, bt['wap'].values, 'm', linewidth = 1, label = 'WAP')\n    plt.plot(bt['seconds_in_bucket'].values, bt['bid_price1'].values, 'g', linewidth = 2, label = 'Bid price 1')\n    plt.plot(bt['seconds_in_bucket'].values, bt['bid_price2'].values, 'g--', linewidth = 1, label = 'Bid price 2')\n\n    fig.axes[0].fill_between(bt['seconds_in_bucket'].values, \n                             bt['bid_price1'].values, \n                             bt['ask_price1'].values, \n                             color = 'orange', \n                             alpha = 0.15)\n\n    fig.axes[0].fill_between(bt['seconds_in_bucket'].values, \n                             bt['bid_price2'].values, \n                             bt['bid_price1'].values, \n                             color = 'green', \n                             alpha = 0.25)\n\n    fig.axes[0].fill_between(bt['seconds_in_bucket'].values, \n                             bt['ask_price1'].values, \n                             bt['ask_price2'].values, \n                             color = 'blue', \n                             alpha = 0.15)\n\n\n    mask = (merged['color'] == 'green').values\n    plt.scatter(merged['seconds_in_bucket'].values[mask], \n                merged['price'].values[mask], \n                marker = '*', color = merged['color'].values[mask], \n                s = 500, label = 'Buy trades')\n    \n    mask = (merged['color'] == 'red').values\n    plt.scatter(merged['seconds_in_bucket'].values[mask], \n                merged['price'].values[mask], \n                marker = '*', color = merged['color'].values[mask], \n                s = 500, label = 'Sell trades')\n\n    plt.grid(True)\n    plt.legend()\n    plt.title('Stock_id = {}, time_id = {}'.format(stock_id, time_id))\n    plt.xlabel('seconds_in_bucket')\n    plt.ylabel('Price')\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-25T17:33:18.010602Z","iopub.execute_input":"2021-07-25T17:33:18.011079Z","iopub.status.idle":"2021-07-25T17:33:18.035871Z","shell.execute_reply.started":"2021-07-25T17:33:18.011039Z","shell.execute_reply":"2021-07-25T17:33:18.03414Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for stock_id in [0,1,2]:\n    # Read data\n    bt = pd.read_parquet(INPUT_PATH + \"book_train.parquet/stock_id={}\".format(stock_id))\n    tt = pd.read_parquet(INPUT_PATH + \"trade_train.parquet/stock_id={}\".format(stock_id))\n    \n    time_ids = bt['time_id'].value_counts().index.values[[0, -1]]\n    for time_id in time_ids:\n        create_plot(stock_id, time_id, bt, tt)","metadata":{"execution":{"iopub.status.busy":"2021-07-25T17:33:20.75495Z","iopub.execute_input":"2021-07-25T17:33:20.755393Z","iopub.status.idle":"2021-07-25T17:33:27.316552Z","shell.execute_reply.started":"2021-07-25T17:33:20.755343Z","shell.execute_reply":"2021-07-25T17:33:27.315423Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Feature engineering ","metadata":{}},{"cell_type":"code","source":"def create_features(stock_id, time_id, bt, trades, last_s):\n    q_cnt = 5\n    \n    bt = bt[bt['seconds_in_bucket'] >= last_s]\n    trades = trades[trades['seconds_in_bucket'] > bt['seconds_in_bucket'].min() + 1]\n    \n    # BOOK PART    \n    features_arr = [{\n        'rv_1': realized_volatility(bt['log_return']),\n        'rv_2': realized_volatility(bt['log_return2']),\n        'rv_mp': realized_volatility(bt['log_return_mean_price'])\n    }]\n    \n    for col in ['abs_wap_balance', 'wap_balance', 'price_spread', 'bid_spread', 'ask_spread',\n               'total_volume', 'abs_volume_imbalance', 'volume_imbalance']:\n        features_arr.append(calc_array_feats(bt[col].values, 'B_' + col, q_cnt, False))\n        \n    for col in ['seconds_in_bucket', 'bid_volume', 'ask_volume']:\n        features_arr.append(calc_array_feats(bt[col].values, 'B_' + col, q_cnt, True))\n    # ==========================================\n    \n    # TRADES PART ==========================================\n    trades['seconds_in_bucket'] = np.maximum(trades['seconds_in_bucket'] - 1, 0)\n\n    # Combine trades prices/timestamps with book prices/timestamps\n    diffs = []\n    times = set(bt['seconds_in_bucket'].values)\n    for t in trades['seconds_in_bucket'].values:\n        d = 0\n        while t >= 0:\n            if t in times:\n                diffs.append(d)\n                break\n            else:\n                t -= 1\n                d += 1\n\n    trades['seconds_in_bucket'] -= diffs\n    \n    features_arr.append(calc_array_feats(np.array(diffs), 'T_diffs', q_cnt, False))\n    vc_diffs = pd.Series(diffs).value_counts()\n    features_arr.append(calc_array_feats(vc_diffs.values, 'T_vc_diffs_values', q_cnt, False))\n    features_arr.append(calc_array_feats(vc_diffs.index.values, 'T_vc_diffs_index', q_cnt, False))\n    features_arr.append({'T_len_vc_diffs': len(vc_diffs)})\n    \n    for col in ['size', 'order_count']:\n        features_arr.append(calc_array_feats(trades[col].values, 'T_' + col, q_cnt, False))\n        \n    for col in ['seconds_in_bucket', 'price']:\n        features_arr.append(calc_array_feats(trades[col].values, 'T_' + col, q_cnt, True))\n    # ==========================================\n\n    # MERGED PART\n    merged = pd.merge(bt, trades[['seconds_in_bucket', 'price', 'size']], on = 'seconds_in_bucket', how = 'left').dropna()\n    merged['diff_with_bid'] = merged['price'] - merged['bid_price1']\n    merged['diff_with_ask'] = merged['ask_price1'] - merged['price']\n    merged['side'] = (merged['diff_with_bid'] < merged['diff_with_ask']).astype(int)\n    side = merged['side'].values\n    merged['diff_with_side_volume1'] = np.array([row[s] for row, s in zip(merged[['ask_size1', 'bid_size1']].values, side)]) - merged['size']\n    merged['diff_with_side_full_volume'] = np.array([row[s] for row, s in zip(merged[['ask_volume', 'bid_volume']].values, side)]) - merged['size']\n    \n    features_arr.append({\n        'M_cnt': len(merged),\n        'M_mean_side': np.mean(side)\n    })\n    \n    cside = np.cumsum(2 * side - 1)\n    features_arr.append(calc_array_feats(cside, 'M_cside', q_cnt, False))\n    \n    for col in ['diff_with_side_volume1', 'diff_with_side_full_volume', 'diff_with_bid', 'diff_with_ask']:\n        features_arr.append(calc_array_feats(merged[col].values, 'M_' + col, q_cnt, False)) \n      \n    # ==========================================\n    features = dict(ChainMap(*features_arr))\n    features = {str(last_s) + '_' + k: features[k] for k in features}\n    \n    features['#stock_id'] = stock_id\n    features['#time_id'] = time_id\n    features['#row_id'] = '{}-{}'.format(stock_id, time_id)\n\n    return features","metadata":{"execution":{"iopub.status.busy":"2021-07-25T17:33:33.309272Z","iopub.execute_input":"2021-07-25T17:33:33.309691Z","iopub.status.idle":"2021-07-25T17:33:33.339435Z","shell.execute_reply.started":"2021-07-25T17:33:33.309655Z","shell.execute_reply":"2021-07-25T17:33:33.33781Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def calc_features_dataset_for_stock(stock_id, test_flg = False, debug = False):\n    part = 'train'\n    if test_flg:\n        part = 'test'\n        \n    bt = pd.read_parquet(INPUT_PATH + \"book_{}.parquet/stock_id={}\".format(part, stock_id))\n    bt['wap'] = calc_wap(bt)\n    bt['log_return'] = log_return(bt['wap'])\n    bt['wap2'] = calc_wap2(bt)\n    bt['log_return2'] = log_return(bt['wap2'])\n    bt['mean_price'] = calc_mean_price(bt)\n    bt['log_return_mean_price'] = log_return(bt['mean_price'])\n    \n    bt['abs_wap_balance'] = abs(bt['wap'] - bt['wap2'])\n    bt['wap_balance'] = bt['wap'] - bt['wap2']\n    \n    bt['price_spread'] = 2 * (bt['ask_price1'] - bt['bid_price1']) / (bt['ask_price1'] + bt['bid_price1'])\n    bt['bid_spread'] = (bt['bid_price1'] - bt['bid_price2']) / bt['bid_price1']\n    bt['ask_spread'] = (bt['ask_price1'] - bt['ask_price2']) / bt['ask_price1']\n    \n    bt['total_volume'] = (bt['ask_size1'] + bt['ask_size2']) + (bt['bid_size1'] + bt['bid_size2'])\n    bt['bid_volume'] = bt['bid_size1'] + bt['bid_size2']\n    bt['ask_volume'] = bt['ask_size1'] + bt['ask_size2']\n    bt['abs_volume_imbalance'] = abs(bt['bid_volume'] - bt['ask_volume'])\n    bt['volume_imbalance'] = bt['bid_volume'] - bt['ask_volume']\n    \n    book_groups = bt.groupby('time_id')\n    trade_groups = pd.read_parquet(INPUT_PATH + \"trade_{}.parquet/stock_id={}\".format(part, stock_id)).groupby('time_id')\n\n    feats_arr = []\n    bg_keys = book_groups.groups.keys()\n    tr_keys = set(trade_groups.groups.keys())\n    sample_trades_df = pd.DataFrame(columns = ['time_id', 'seconds_in_bucket', 'price', 'size', 'order_count'])\n    for time_id in tqdm(bg_keys):\n        arr = []\n        b_gr = book_groups.get_group(time_id)\n        for last_s in [600, 300]:\n            if time_id in tr_keys:\n                t_gr = trade_groups.get_group(time_id)\n            else:\n                t_gr = sample_trades_df.copy()\n            arr.append(create_features(stock_id, time_id, b_gr, t_gr, 600 - last_s))\n        feats_arr.append(dict(ChainMap(*arr)))\n        if debug:\n            break\n       \n    df = pd.DataFrame(feats_arr)\n    df = df[sorted(df.columns)].rename({'#stock_id': 'stock_id', '#time_id': 'time_id', '#row_id': 'row_id'}, axis = 1)\n    print('Stock {} ready'.format(stock_id))\n    return df","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-07-25T17:33:34.064199Z","iopub.execute_input":"2021-07-25T17:33:34.064651Z","iopub.status.idle":"2021-07-25T17:33:34.076774Z","shell.execute_reply.started":"2021-07-25T17:33:34.064614Z","shell.execute_reply":"2021-07-25T17:33:34.07571Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\ndf = calc_features_dataset_for_stock(stock_id = 0, test_flg = False, debug = True)\ndf","metadata":{"execution":{"iopub.status.busy":"2021-07-25T17:33:34.984201Z","iopub.execute_input":"2021-07-25T17:33:34.985057Z","iopub.status.idle":"2021-07-25T17:33:35.81743Z","shell.execute_reply.started":"2021-07-25T17:33:34.985011Z","shell.execute_reply":"2021-07-25T17:33:35.815792Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Multiprocessed preprocessor wrapper","metadata":{}},{"cell_type":"code","source":"def multiprocessed_df_creation(stock_ids, n_jobs = 4, test_flg = False, debug = False):\n    res_df = Parallel(n_jobs=n_jobs, verbose=1)(\n        delayed(calc_features_dataset_for_stock)(stock_id, test_flg, debug) for stock_id in stock_ids\n    )\n\n    res_df = pd.concat(res_df).reset_index(drop = True)\n    return res_df\n","metadata":{"execution":{"iopub.status.busy":"2021-07-25T17:33:42.362108Z","iopub.execute_input":"2021-07-25T17:33:42.362572Z","iopub.status.idle":"2021-07-25T17:33:42.368809Z","shell.execute_reply.started":"2021-07-25T17:33:42.362533Z","shell.execute_reply":"2021-07-25T17:33:42.367814Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"stock_ids = [0,1,2,3,4,5]\nmultiprocessed_df_creation(stock_ids = stock_ids, \n                           n_jobs = N_THREADS, \n                           test_flg = False, \n                           debug = True)","metadata":{"execution":{"iopub.status.busy":"2021-07-25T17:33:43.299551Z","iopub.execute_input":"2021-07-25T17:33:43.300092Z","iopub.status.idle":"2021-07-25T17:33:47.742441Z","shell.execute_reply.started":"2021-07-25T17:33:43.300041Z","shell.execute_reply":"2021-07-25T17:33:47.741229Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Generate full train","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv(INPUT_PATH + 'train.csv')\ntrain.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-25T17:34:04.316692Z","iopub.execute_input":"2021-07-25T17:34:04.31722Z","iopub.status.idle":"2021-07-25T17:34:04.593725Z","shell.execute_reply.started":"2021-07-25T17:34:04.317169Z","shell.execute_reply":"2021-07-25T17:34:04.592428Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_stock_ids = train.stock_id.unique()\ntrain_stock_ids","metadata":{"execution":{"iopub.status.busy":"2021-07-25T17:34:04.750181Z","iopub.execute_input":"2021-07-25T17:34:04.7506Z","iopub.status.idle":"2021-07-25T17:34:04.761866Z","shell.execute_reply.started":"2021-07-25T17:34:04.750566Z","shell.execute_reply":"2021-07-25T17:34:04.760747Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\ntrain_data = multiprocessed_df_creation(train_stock_ids, \n                                        n_jobs = N_THREADS, \n                                        test_flg = False, \n                                        debug = False)","metadata":{"execution":{"iopub.status.busy":"2021-07-25T17:34:18.721849Z","iopub.execute_input":"2021-07-25T17:34:18.722327Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data = pd.merge(train, train_data, on = ['stock_id', 'time_id'], how = 'left')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Generate full test","metadata":{}},{"cell_type":"code","source":"test = pd.read_csv(INPUT_PATH + 'test.csv')\ntest.head()","metadata":{"execution":{"iopub.execute_input":"2021-07-13T22:41:55.348219Z","iopub.status.busy":"2021-07-13T22:41:55.347936Z","iopub.status.idle":"2021-07-13T22:41:55.360331Z","shell.execute_reply":"2021-07-13T22:41:55.35947Z","shell.execute_reply.started":"2021-07-13T22:41:55.348191Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"DEBUG = (test.shape[0] == 3)\nDEBUG","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_stock_ids = test.stock_id.unique()\ntest_stock_ids","metadata":{"execution":{"iopub.execute_input":"2021-07-13T22:41:55.361643Z","iopub.status.busy":"2021-07-13T22:41:55.361357Z","iopub.status.idle":"2021-07-13T22:41:55.367361Z","shell.execute_reply":"2021-07-13T22:41:55.366149Z","shell.execute_reply.started":"2021-07-13T22:41:55.361617Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\ntest_data = multiprocessed_df_creation(test_stock_ids, \n                                       n_jobs = N_THREADS, \n                                       test_flg = True, \n                                       debug = False)","metadata":{"execution":{"iopub.execute_input":"2021-07-13T22:41:55.370898Z","iopub.status.busy":"2021-07-13T22:41:55.370425Z","iopub.status.idle":"2021-07-13T22:41:55.534687Z","shell.execute_reply":"2021-07-13T22:41:55.533989Z","shell.execute_reply.started":"2021-07-13T22:41:55.37086Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data = pd.merge(test, test_data, on = ['stock_id', 'time_id', 'row_id'], how = 'left')\ntest_data","metadata":{"execution":{"iopub.execute_input":"2021-07-13T22:41:55.53624Z","iopub.status.busy":"2021-07-13T22:41:55.535959Z","iopub.status.idle":"2021-07-13T22:41:55.547343Z","shell.execute_reply":"2021-07-13T22:41:55.546089Z","shell.execute_reply.started":"2021-07-13T22:41:55.536213Z"}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Create LightAutoML model","metadata":{}},{"cell_type":"code","source":"N_FOLDS = 10\nTIMEOUT = 24 * 3600\n\n# Default params for LGBM models\nlgbm_params = {\n      \"objective\": \"rmse\", \n      \"metric\": \"rmse\", \n      \"boosting_type\": \"gbdt\",\n      'early_stopping_rounds': 30,\n      'learning_rate': 0.01,\n      'lambda_l1': 1.0,\n      'lambda_l2': 1.0,\n      'feature_fraction': 0.8,\n      'bagging_fraction': 0.8,\n}","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_additional_feats(tr_data, te_data):\n    for t_col in ['target', '0_rv_1', '300_rv_1']:\n        print(t_col)\n        for name, func in [('mean', np.mean), ('min', np.min), ('max', np.max)]:\n            print('\\t', name)\n            tr_col, te_col = create_targ_enc_feature('stock_id', t_col, func, tr_data, te_data, 20, 3)\n            tr_data['stock_id_enc_{}_{}'.format(name, t_col)] = tr_col\n            te_data['stock_id_enc_{}_{}'.format(name, t_col)] = te_col\n            \n    for d in [tr_data, te_data]:\n        d['0rv1_diff_300rv1'] = d['0_rv_1'] - d['300_rv_1']\n        d['0rv1_del_300rv1'] = d['0_rv_1'] / d['300_rv_1']\n    ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if DEBUG:\n    tr_data, te_data = train_test_split(train_data, \n                                         test_size = 0.2, \n                                         random_state = 42)\n    print('Data splitted. Parts sizes: tr_data = {}, te_data = {}'\n                  .format(tr_data.shape, te_data.shape))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_and_train_model(tr_data, te_data):\n    # Task setup - mse loss and mse metric. To optimize rmspe we use object weights for the loss (weight column)\n    task = Task('reg',)\n    tr_data['weight'] = 1 / tr_data['target'] ** 2\n    \n    # Columns roles setup\n    roles = {\n        'target': 'target',\n        'drop': ['row_id', 'time_id'],\n        'category': 'stock_id',\n        'weights': 'weight'\n    }\n    \n    # Train LightAutoML model\n    automl = TabularAutoML(task = task, \n                           timeout = TIMEOUT,\n                           cpu_limit = N_THREADS,\n                           general_params = {'use_algos': [['lgb', 'lgb_tuned', 'cb_tuned']]},\n                           reader_params = {'n_jobs': N_THREADS, 'cv': N_FOLDS},\n                           tuning_params = {'max_tuning_time': 600},\n                           lgb_params = {'default_params': lgbm_params, 'freeze_defaults': True},\n                           verbose = 3\n                           )\n\n    oof_pred = automl.fit_predict(tr_data, roles = roles)\n    print('OOF prediction for tr_data:\\n{}\\nShape = {}'.format(oof_pred, oof_pred.shape))\n    \n    # Fast feature importances calculation\n    fast_fi = automl.get_feature_scores('fast')\n    fast_fi.set_index('Feature')['Importance'].head(100).plot.bar(figsize = (50, 10), grid = True)\n    \n    # Let's see how the final model looks like\n    print(automl.create_model_str_desc())\n    \n    # Test data prediction\n    te_pred = automl.predict(te_data)\n    print('Prediction for te_data:\\n{}\\nShape = {}'.format(te_pred, te_pred.shape))\n    \n    return oof_pred.data[:, 0], te_pred.data[:, 0], automl","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if DEBUG:\n    create_additional_feats(tr_data, te_data)\n    \n    oof_pred, valid_pred, automl = create_and_train_model(tr_data, te_data)\n    \n    # Check scores\n    print('OOF RMSPE score = {:.5f}'.format(rmspe(tr_data['target'], oof_pred)))\n    print('TEST RMSPE score = {:.5f}'.format(rmspe(te_data['target'], valid_pred)))\n    \n    create_additional_feats(tr_data, test_data)\n    \n    test_pred = automl.predict(test_data)\n    submission = test_data[['row_id']]\n    submission['target'] = test_pred.data[:, 0]\n    submission.to_csv('submission.csv', index = False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if not DEBUG:\n    create_additional_feats(train_data, test_data)\n    \n    oof_pred, test_pred, automl = create_and_train_model(train_data, test_data)\n    \n    # Check scores\n    print('OOF RMSPE score = {:.5f}'.format(rmspe(train_data['target'], oof_pred)))\n    \n    submission = test_data[['row_id']]\n    submission['target'] = test_pred\n    submission.to_csv('submission.csv', index = False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Bonus. Feature importances and model structure ","metadata":{}},{"cell_type":"code","source":"# Fast feature importances calculation\nfast_fi = automl.get_feature_scores('fast')\nfast_fi.set_index('Feature')['Importance'].head(100).plot.bar(figsize = (50, 10), grid = True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's see how the final model looks like\nprint(automl.create_model_str_desc())","metadata":{},"execution_count":null,"outputs":[]}]}