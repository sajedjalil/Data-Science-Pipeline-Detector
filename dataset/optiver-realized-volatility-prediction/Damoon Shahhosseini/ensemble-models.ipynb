{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Strategy\n\nTrying out different models then aggregating the results:\n- XGBoost\n- LightBGM\n- Neural Network","metadata":{}},{"cell_type":"code","source":"# Getting the directories\nimport glob \n\nBASE_DIR = '/kaggle/input/optiver-realized-volatility-prediction/'\n\n# Paths to book and trade data\nTRAIN_BOOK_PATHS  = glob.glob(f'{BASE_DIR}book_train.parquet/*')\nTEST_BOOK_PATHS   = glob.glob(f'{BASE_DIR}book_test.parquet/*')\nTRAIN_TRADE_PATHS = glob.glob(f'{BASE_DIR}trade_train.parquet/*')\nTEST_TRADE_PATHS  = glob.glob(f'{BASE_DIR}trade_test.parquet/*')\n\n# Plotting\nimport matplotlib.pyplot as plt \n\n# Basic Data Wrangling utilites\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_absolute_error as mae, r2_score as r2\nfrom sklearn.model_selection import KFold\n\n# Xgboost\nimport xgboost as xgb\n\n# LightGBM\nfrom lightgbm import LGBMRegressor, plot_tree, plot_importance, plot_metric, plot_split_value_histogram\nimport lightgbm as lgb\n\n# Working with dataframes and sequences\nimport numpy as np\nimport pandas as pd \n\ntrain = pd.read_csv(f'{BASE_DIR}train.csv')\nsub   = pd.read_csv(f'{BASE_DIR}sample_submission.csv')\n\n# Some helper functions\n\ndef submit(prediction):\n    \"\"\" Submition process for the competition. \"\"\"\n    sub.drop(sub.index, inplace=True)                         # Remove values in the sample submission file\n    sub['row_id'] = test_data['row_id']                       # Get the row_id for each test_data \n    sub['target'] = prediction                                # Getting the prediction\n    sub.to_csv('/kaggle/working/submission.csv', index=False) # Writting out the .csv file\n    \ndef rmspe(y_true, y_pred):\n    return np.sqrt(np.nanmean(np.square(((y_true - y_pred) / y_true))))\n\ndef validate(model, Return=False):\n    \"\"\"Validates the model for differnt metrics. \"\"\"\n    val_data = dval if type(model) == xgb.core.Booster else X_val\n    y_pred = model.predict(val_data)\n    print(f' MAE: {mae(y_pred, y_val)}, R2: {r2(y_pred, y_val)}, RMSPE: {rmspe(y_val, y_pred)}')\n    if Return: return r2(y_pred, y_val), rmspe(y_val, y_pred)\n    \ndef log_return(stock_prices):\n    return np.log(stock_prices).diff()\n\ndef xgb_importance_plot(xgb_model):\n    xgb.plot_importance(xgb_model, max_num_features=18, importance_type='gain'  , show_values=False)\n    xgb.plot_importance(xgb_model, max_num_features=18, importance_type='cover' , show_values=False)\n    xgb.plot_importance(xgb_model, max_num_features=18, importance_type='weight', show_values=False)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-07-26T16:36:04.391133Z","iopub.execute_input":"2021-07-26T16:36:04.391531Z","iopub.status.idle":"2021-07-26T16:36:07.671955Z","shell.execute_reply.started":"2021-07-26T16:36:04.391426Z","shell.execute_reply":"2021-07-26T16:36:07.671085Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Questions\n- Is it a good idea to augment features and then run a Gradient Boosting model to see which features tend to be more important?\n- Should I trim the features in each iteration? Then add new ones and repeat?","metadata":{}},{"cell_type":"markdown","source":"## Data Deneration\n\nThis part is only ran when a new data version is being worked on. Then the outpued file is uploaded [here](https://www.kaggle.com/damoonshahhosseini/processedbooktrade).","metadata":{}},{"cell_type":"code","source":"class DataManager:\n    \"\"\" Used for processing the input data so the model can be fitted on it. \"\"\"\n    def __init__(self, train=True):\n        self._train = train\n        self._book_file_list = TRAIN_BOOK_PATHS if train else TEST_BOOK_PATHS\n        self._trade_file_list = TRAIN_TRADE_PATHS if train else TEST_TRADE_PATHS\n        self.measures_list = []\n    \n    def _traverse_book(self):\n        \"\"\" Goes through each of the training files. \"\"\"\n        for book_file_path, trade_file_path in zip(self._book_file_list, self._trade_file_list):\n            stock_id = book_file_path.split(\"=\")[1] # Getting the stock_id\n            \n            # Reading the book info and preparing it for aggregation\n            book = pd.read_parquet(book_file_path)\n            \n            book.sort_values(by=['time_id', 'seconds_in_bucket'])\n            book['wap1'] = (book['bid_price1'] * book['ask_size1'] + book['ask_price1'] * book['bid_size1']) / (book['bid_size1']+ book['ask_size1'])\n            book['log_return1'] = book.groupby(['time_id'])['wap1'].apply(log_return)\n            book = book[~book['log_return1'].isnull()]\n            \n            book['wap2'] = (book['bid_price2'] * book['ask_size2'] + book['ask_price2'] * book['bid_size2']) / (book['bid_size2']+ book['ask_size2'])\n            book['log_return2'] = book.groupby(['time_id'])['wap2'].apply(log_return)\n            book = book[~book['log_return2'].isnull()]\n            \n            # Different spreads: Get the max of these for each time_id\n            book['h_spread_l1'] = book['ask_price1'] - book['bid_price1']\n            book['h_spread_l2'] = book['ask_price2'] - book['bid_price2']\n            book['v_spread_b']  = book['bid_price1'] - book['bid_price2']\n            book['v_spread_a']  = book['ask_price1'] - book['bid_price2']\n            \n            book.loc[:, 'bas'] = (book.loc[:, ('ask_price1', 'ask_price2')].min(axis = 1) / book.loc[:, ('bid_price1', 'bid_price2')].max(axis = 1) - 1) \n            \n            # Reading the trade info\n            trade = pd.read_parquet(trade_file_path)\n            \n            # Slicing the train data based on stock_id\n            book_stock_slice = train[train['stock_id'] == int(stock_id)]\n            \n            for time_id in book['time_id'].unique():\n                book_slice = book[book['time_id'] == time_id] # Slicing based on time_id\n                # Features\n                dic = {\n                    'row_id': f\"{stock_id}-{time_id}\", # Fixing row-id from here\n                    \n                    'wap1_mean': book_slice['wap1'].mean(),\n                    'wap1_std':book_slice['wap1'].std(),\n                    'wap1_max':book_slice['wap1'].max(),\n                    \n                    'wap2_mean': book_slice['wap2'].mean(),\n                    'wap2_std':book_slice['wap2'].std(),\n                    'wap2_max':book_slice['wap2'].max(),\n\n                    'h_spread_l1_mean': book['h_spread_l1'].mean(),\n                    'h_spread_l1_std': book['h_spread_l1'].std(),\n                    'h_spread_l1_std': book['h_spread_l1'].max(),\n                    \n                    'h_spread_l2_mean': book['h_spread_l2'].mean(),\n                    'h_spread_l2_std': book['h_spread_l2'].std(),\n                    'h_spread_l2_max': book['h_spread_l2'].max(),\n                    \n                    'v_spread_b_mean': book['v_spread_b'].mean(),\n                    'v_spread_b_std': book['v_spread_b'].std(),\n                    'v_spread_b_max': book['v_spread_b'].max(),\n                    \n                    'v_spread_a_mean': book['v_spread_a'].mean(),\n                    'v_spread_a_std': book['v_spread_a'].std(),\n                    'v_spread_a_max': book['v_spread_a'].max(),\n                    \n                    'log_return1_mean': book_slice['log_return1'].mean(),\n                    'log_return1_std':book_slice['log_return1'].std(),\n                    'log_return1_max':book_slice['log_return1'].max(),\n                    \n                    'log_return2_mean': book_slice['log_return2'].mean(),\n                    'log_return2_std':book_slice['log_return2'].std(),\n                    'log_return2_max':book_slice['log_return2'].max(),\n                    \n                    'bas_mean': book_slice['bas'].mean(),\n                    'bas_std': book_slice['bas'].std(),\n                    'bas_max': book_slice['bas'].max(),\n                    \n                    'ask_size_mean': book_slice['ask_size1'].mean(),\n                    'ask_size_std': book_slice['ask_size1'].std(),\n                    \n                    'ask_price_mean': book_slice['ask_price1'].mean(),\n                    'ask_price_std': book_slice['ask_price1'].std(),\n                    \n                    'bid_size_mean': book_slice['bid_size1'].mean(),\n                    'bid_size_std': book_slice['bid_size1'].std(),\n                    \n                    'bid_price_mean': book_slice['bid_price1'].mean(),\n                    'bid_price_std': book_slice['bid_price1'].std(),\n                    \n                    'actual_price_mean': trade['price'].mean(),\n                    'actual_price_std': trade['price'].std(),\n                    'actual_price_max': trade['price'].max(),\n                    \n                    'size_mean': trade['size'].mean(),\n                    'size_std': trade['size'].std(),\n                    \n                    'order_count_mean': trade['order_count'].mean(),\n                    'order_count_std': trade['order_count'].std(),\n                }\n                \n                # Note: When getting the test_data ready, there is no target column.\n                if self._train: dic['target'] = book_stock_slice[book_stock_slice['time_id'] == time_id]['target'].values[0]\n                \n                self.measures_list.append(dic)\n    \n    def get_processed(self):\n        \"\"\" Returns the processed the data. \"\"\"\n        self._traverse_book() \n        \n        return pd.DataFrame(self.measures_list)","metadata":{"execution":{"iopub.status.busy":"2021-07-26T16:36:07.673536Z","iopub.execute_input":"2021-07-26T16:36:07.673866Z","iopub.status.idle":"2021-07-26T16:36:07.696671Z","shell.execute_reply.started":"2021-07-26T16:36:07.673831Z","shell.execute_reply":"2021-07-26T16:36:07.69555Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# book = DataManager().get_processed()\n# book.to_csv('/kaggle/working/train_v3.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2021-07-22T22:31:44.305181Z","iopub.execute_input":"2021-07-22T22:31:44.305851Z","iopub.status.idle":"2021-07-22T22:31:44.313971Z","shell.execute_reply.started":"2021-07-22T22:31:44.30581Z","shell.execute_reply":"2021-07-22T22:31:44.313075Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Importing Data\n- Training data is imported from the preprocessed dataset.\n- Test data is generated here useing DataManager class.\n- Both training and test datasets are being normalized so the metrics in different columns are close to one another.","metadata":{}},{"cell_type":"code","source":"# Importing trainind data from the input folder, and generating test data with the same schema\ndata = pd.read_csv('/kaggle/input/processedbooktrade/train_v3.csv')\ntest_data = DataManager(train=False).get_processed()\n\n# Min-Max Scaling the data for better models\nfor col_name in data.columns[1:-1]:\n    test_data[col_name] = (test_data[col_name] - data[col_name].min()) / (data[col_name].max() - data[col_name].min())\n    data[col_name] = (data[col_name] - data[col_name].min()) / (data[col_name].max() - data[col_name].min())\n\n# Training Data\nX, y = data.iloc[:,1:-1], data['target']\n\n# Test Data\nX_test = test_data.iloc[:,1:]\n\n# Getting training and validations plits to check for overfitting\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2021-07-26T16:36:10.469942Z","iopub.execute_input":"2021-07-26T16:36:10.470298Z","iopub.status.idle":"2021-07-26T16:36:18.063046Z","shell.execute_reply.started":"2021-07-26T16:36:10.470265Z","shell.execute_reply":"2021-07-26T16:36:18.061932Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train_preds, test_preds = pd.DataFrame(), pd.DataFrame()","metadata":{"execution":{"iopub.status.busy":"2021-07-23T02:35:32.66191Z","iopub.execute_input":"2021-07-23T02:35:32.662515Z","iopub.status.idle":"2021-07-23T02:35:32.667006Z","shell.execute_reply.started":"2021-07-23T02:35:32.662468Z","shell.execute_reply":"2021-07-23T02:35:32.666149Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# XGBoost","metadata":{}},{"cell_type":"code","source":"# dtrain = xgb.DMatrix(X_train, label=y_train)\n# dval   = xgb.DMatrix(X_val, label=y_val)\n# dtest  = xgb.DMatrix(X_test)\n\n# X_xgb  = xgb.DMatrix(X)","metadata":{"execution":{"iopub.status.busy":"2021-07-23T02:35:35.898113Z","iopub.execute_input":"2021-07-23T02:35:35.898472Z","iopub.status.idle":"2021-07-23T02:35:36.470976Z","shell.execute_reply.started":"2021-07-23T02:35:35.89844Z","shell.execute_reply":"2021-07-23T02:35:36.470088Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def run_xgb(params, n):\n#     xgb_model = xgb.train(\n#         params, \n#         dtrain, \n#         num_boost_round=1500, \n#         early_stopping_rounds=20, \n#         evals=[(dtrain, 'train'), (dval, 'eval')],\n#     )\n    \n#     r2, rp = validate(xgb_model, True)\n    \n#     xgb_model.save_model(f'xgb_v3_{n}_1.model')\n    \n#     train_preds[f'xgb_{n}'] = xgb_model.predict(X_xgb).tolist()\n#     test_preds[f'xgb_{n}'] =  xgb_model.predict(dtest).tolist()\n    \n# #     return pred, r2, rp","metadata":{"execution":{"iopub.status.busy":"2021-07-23T02:35:38.14151Z","iopub.execute_input":"2021-07-23T02:35:38.141867Z","iopub.status.idle":"2021-07-23T02:35:38.148676Z","shell.execute_reply.started":"2021-07-23T02:35:38.141832Z","shell.execute_reply":"2021-07-23T02:35:38.146772Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xgb_param_1 = {\n    'eta': 1e-1,\n    'max_depth': 12,\n    'objective': 'reg:squarederror',\n    'booster': 'gbtree',\n    'colsample_bytree': 0.9,\n    'sampling_method': 'gradient_based',\n    'subsample': 0.6, # Avoiding overfitting\n    'tree_method': 'gpu_hist'\n}\n\n# xgb1, xgb1_r1, xgb1_rp = \n# run_xgb(xgb_param_1, 1)","metadata":{"_kg_hide-input":false,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-07-23T02:35:40.718484Z","iopub.execute_input":"2021-07-23T02:35:40.718803Z","iopub.status.idle":"2021-07-23T02:35:41.812729Z","shell.execute_reply.started":"2021-07-23T02:35:40.718772Z","shell.execute_reply":"2021-07-23T02:35:41.811855Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xgb_param_2 = {\n    'eta': 1e-1,\n    'max_depth': 5,\n    'eval_metric': 'mape',\n    'objective': 'reg:squarederror',\n    'booster': 'gbtree',\n    'lambda': 0.9,\n    'colsample_bytree': 0.5,\n    'sampling_method': 'gradient_based',\n    'subsample': 0.9, # Avoiding overfitting\n    'tree_method': 'gpu_hist'\n}\n\n# xgb2, xgb2_r2, xgb2_rp = \n# run_xgb(xgb_param_2, 2)","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-07-22T22:31:54.015465Z","iopub.execute_input":"2021-07-22T22:31:54.017069Z","iopub.status.idle":"2021-07-22T22:31:54.144256Z","shell.execute_reply.started":"2021-07-22T22:31:54.017034Z","shell.execute_reply":"2021-07-22T22:31:54.143602Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xgb_param_3 = {\n    'eta': 8e-1,\n    'max_depth': 15,\n    'verbosity': 0,\n    'eval_metric': 'mape',\n    'objective': 'reg:squarederror',\n    'booster': 'dart',\n    'tree_method': 'gpu_hist',\n    'sample_type': 'weighted',\n    'rate_drop': 0.4,\n    'max_leaves': 30,\n    'alpha': 9e-4,\n    'seed':10,\n#     'min_child_weight': 1e-3 \n}\n\n# xgb3, xgb2_r3, xgb3_rp =\n# run_xgb(xgb_param_3, 3)","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-07-23T02:38:35.965006Z","iopub.execute_input":"2021-07-23T02:38:35.966619Z","iopub.status.idle":"2021-07-23T02:38:36.375407Z","shell.execute_reply.started":"2021-07-23T02:38:35.966585Z","shell.execute_reply":"2021-07-23T02:38:36.374536Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# f, axs = plt.subplots(1,1, figsize=(100,100))\n\n# xgb.plot_tree(xgb_model, num_trees=75, ax=axs)","metadata":{"execution":{"iopub.status.busy":"2021-07-22T22:31:54.503986Z","iopub.execute_input":"2021-07-22T22:31:54.504364Z","iopub.status.idle":"2021-07-22T22:31:54.508343Z","shell.execute_reply.started":"2021-07-22T22:31:54.504326Z","shell.execute_reply":"2021-07-22T22:31:54.507344Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import gc\n\n# gc.get_count()","metadata":{"execution":{"iopub.status.busy":"2021-07-22T22:31:54.509712Z","iopub.execute_input":"2021-07-22T22:31:54.510368Z","iopub.status.idle":"2021-07-22T22:31:54.51946Z","shell.execute_reply.started":"2021-07-22T22:31:54.510327Z","shell.execute_reply":"2021-07-22T22:31:54.518663Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# LightGBM","metadata":{}},{"cell_type":"code","source":"# Reference https://www.kaggle.com/yus002/realized-volatility-prediction-lgbm-train\ndef my_metrics(y_true, y_pred):\n    return np.sqrt(np.mean(np.square((y_true - y_pred) / y_true)))\n\ndef lgbm_rmspe(y_true, y_pred):  \n    output = my_metrics(y_true, y_pred)\n    return 'rmspe', output, False\n\ndef run_lgbm(params, n):\n    lgbm_model = LGBMRegressor(**params)\n    \n    lgbm_model.fit(\n        X_train, y_train,\n        eval_set=[(X_train, y_train), (X_val, y_val)],\n        eval_metric = lgbm_rmspe,\n        verbose=300,\n        early_stopping_rounds=100\n    )\n    \n    r2, rp = validate(lgbm_model, True)\n    \n#     lgbm_model.save_model(f'lgbm_v3_{n}_1')\n    train_preds[f'lgbm_{n}'] = lgbm_model.predict(X).tolist()\n    test_preds[f'lgbm_{n}'] = lgbm_model.predict(X_test).tolist()","metadata":{"execution":{"iopub.status.busy":"2021-07-23T02:35:56.827243Z","iopub.execute_input":"2021-07-23T02:35:56.827576Z","iopub.status.idle":"2021-07-23T02:35:56.834333Z","shell.execute_reply.started":"2021-07-23T02:35:56.827545Z","shell.execute_reply":"2021-07-23T02:35:56.833275Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lgbm_param_1 = {\n    'objective':'rmse', \n    'metric': 'rmse',\n    \"boosting_type\": \"gbdt\",\n    'device_type': 'gpu',\n    'num_iterations': 5000,\n    'early_stopping_rounds': 30,\n      'learning_rate': 0.01,\n      'lambda_l1': 1,\n      'lambda_l2': 1,\n      'feature_fraction': 0.8,\n      'bagging_fraction': 0.8,\n    'num_leaves': 50,\n    'max_depth': 5,\n    'seed': 11,\n}\n\n# lgbm_pred_1 = \n# run_lgbm(lgbm_param_1, 1)","metadata":{"execution":{"iopub.status.busy":"2021-07-23T02:35:57.836993Z","iopub.execute_input":"2021-07-23T02:35:57.83736Z","iopub.status.idle":"2021-07-23T02:36:06.543832Z","shell.execute_reply.started":"2021-07-23T02:35:57.837329Z","shell.execute_reply":"2021-07-23T02:36:06.542675Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lgbm_param_2 = {\n    'objective':'mean_squared_error', \n    'metric': 'rmse',\n    'device_type': 'gpu',\n    'num_iterations': 5000,\n    'num_leaves': 100,\n    'learning_rate': 0.1,\n    'max_depth': 8,\n    'colsample_bytree': 0.85,\n    'subsample': 0.8,   \n    'seed': 11,\n    'tree_learner': 'feature'\n}\n\n# lgbm_pred_2 = run_lgbm(lgbm_param_2, 2)","metadata":{"execution":{"iopub.status.busy":"2021-07-23T02:38:33.334195Z","iopub.execute_input":"2021-07-23T02:38:33.334541Z","iopub.status.idle":"2021-07-23T02:38:35.961554Z","shell.execute_reply.started":"2021-07-23T02:38:33.334505Z","shell.execute_reply":"2021-07-23T02:38:35.960851Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# gc.collect()","metadata":{"execution":{"iopub.status.busy":"2021-07-22T22:32:06.303692Z","iopub.execute_input":"2021-07-22T22:32:06.305355Z","iopub.status.idle":"2021-07-22T22:32:06.444642Z","shell.execute_reply.started":"2021-07-22T22:32:06.305322Z","shell.execute_reply":"2021-07-22T22:32:06.443843Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# CatBoost","metadata":{}},{"cell_type":"markdown","source":"# Neural Network","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nimport tensorflow.keras.backend as K\nfrom tensorflow.keras import Sequential\nfrom tensorflow.keras.layers import Dense, BatchNormalization\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.initializers import TruncatedNormal\nfrom tensorflow.keras.losses import mean_absolute_error as MAE, mean_squared_error as MSE\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau as RLP, EarlyStopping as ES\n\nimport random\nimport copy","metadata":{"execution":{"iopub.status.busy":"2021-07-26T16:36:18.064579Z","iopub.execute_input":"2021-07-26T16:36:18.064924Z","iopub.status.idle":"2021-07-26T16:36:22.727378Z","shell.execute_reply.started":"2021-07-26T16:36:18.064884Z","shell.execute_reply":"2021-07-26T16:36:22.726531Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def nn_rmspe(y_true, y_pred):\n    return tf.sqrt(tf.experimental.numpy.nanmean(tf.square(((y_true - y_pred) / y_true))))\n\ndef nn_seq_model(layers, n):\n    model = Sequential(copy.deepcopy(layers))\n\n    model.compile(\n        optimizer=Adam(2e-3),\n        loss=nn_rmspe,\n    )\n    \n    model.fit(\n        x=X_train, y=y_train, \n        batch_size=256, \n        epochs=500,\n        verbose=False,\n        callbacks=[\n            RLP(monitor='val_loss', factor=0.98, patience=15, verbose=1), \n            ES(monitor='val_loss', patience=50, verbose=1, restore_best_weights=True)\n        ], \n        validation_data=(X_val, y_val),\n        shuffle=True,\n    )\n    \n    path = f'./nn_v3_{n}_1.h5'\n    \n    model.save(path)\n    \n    train_preds[f'nn_{n}'] = model.predict(X)\n    test_preds[f'nn_{n}']  = model.predict(X_test)","metadata":{"execution":{"iopub.status.busy":"2021-07-26T16:36:22.72896Z","iopub.execute_input":"2021-07-26T16:36:22.729286Z","iopub.status.idle":"2021-07-26T16:36:22.738135Z","shell.execute_reply.started":"2021-07-26T16:36:22.72925Z","shell.execute_reply":"2021-07-26T16:36:22.737163Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"layers_1 = [\n    Dense(32, kernel_initializer=TruncatedNormal(0, 1, 11), bias_initializer=TruncatedNormal(1e-1, 1e-7, 11)),\n    Dense(64, kernel_initializer=TruncatedNormal(0, 2, 11), bias_initializer=TruncatedNormal(0, 5e-7, 11)),\n    Dense(128, kernel_initializer=TruncatedNormal(0, 1, 161), bias_initializer=TruncatedNormal(0, 1e-7, 51)),\n    BatchNormalization(),\n     Dense(32, kernel_initializer=TruncatedNormal(0, 1e-1, 11), bias_initializer=TruncatedNormal(1e-1, 1e-7, 11)),\n    Dense(64, kernel_initializer=TruncatedNormal(0, 2e-1, 11), bias_initializer=TruncatedNormal(0, 5e-7, 11)),\n    Dense(128, kernel_initializer=TruncatedNormal(0, 1e-1, 161), bias_initializer=TruncatedNormal(0, 1e-7, 51)),\n    BatchNormalization(),\n     Dense(32, kernel_initializer=TruncatedNormal(0, 1e-2, 11), bias_initializer=TruncatedNormal(1e-1, 1e-7, 11)),\n    Dense(64, kernel_initializer=TruncatedNormal(0, 2e-2, 11), bias_initializer=TruncatedNormal(0, 5e-7, 11)),\n    Dense(128, kernel_initializer=TruncatedNormal(0, 1e-2, 161), bias_initializer=TruncatedNormal(0, 1e-7, 51)),\n    BatchNormalization(),\n     Dense(32, kernel_initializer=TruncatedNormal(0, 1e-2, 51), bias_initializer=TruncatedNormal(1e-1, 1e-7, 11)),\n    Dense(64, kernel_initializer=TruncatedNormal(0, 2e-2, 1), bias_initializer=TruncatedNormal(0, 5e-7, 11)),\n    Dense(128, kernel_initializer=TruncatedNormal(0, 1e-2, 11), bias_initializer=TruncatedNormal(0, 1e-7, 51)),\n    BatchNormalization(),\n     Dense(32, kernel_initializer=TruncatedNormal(0, 1e-3, 11), bias_initializer=TruncatedNormal(1e-1, 1e-7, 11)),\n    Dense(64, kernel_initializer=TruncatedNormal(0, 2e-3, 11), bias_initializer=TruncatedNormal(0, 5e-7, 11)),\n    Dense(128, kernel_initializer=TruncatedNormal(0, 1e-3, 161), bias_initializer=TruncatedNormal(0, 1e-7, 51)),\n    BatchNormalization(),\n    Dense(64, kernel_initializer=TruncatedNormal(0, 1e-2, 71), bias_initializer=TruncatedNormal(1e-1, 1e-7, 11)),\n    Dense(128, kernel_initializer=TruncatedNormal(0, 2e-2, 51), bias_initializer=TruncatedNormal(0, 5e-7, 11)),\n    Dense(256, kernel_initializer=TruncatedNormal(0, 1e-2, 61), bias_initializer=TruncatedNormal(0, 1e-7, 51)),\n    BatchNormalization(),\n    Dense(256, kernel_initializer=TruncatedNormal(0, 1e-3, 11), bias_initializer=TruncatedNormal(1e-1, 1e-7, 11)),\n    Dense(64, kernel_initializer=TruncatedNormal(0, 2e-3, 11), bias_initializer=TruncatedNormal(0, 5e-7, 11)),\n    Dense(32, kernel_initializer=TruncatedNormal(0, 1e-3, 161), bias_initializer=TruncatedNormal(0, 1e-7, 51)),\n    BatchNormalization(),\n    Dense(1, kernel_initializer=TruncatedNormal(0, 1, 11), bias_initializer=TruncatedNormal(0, 1e-7, 32)),\n]\n\n# nn_seq_model(layers_1, 1)","metadata":{"execution":{"iopub.status.busy":"2021-07-26T17:07:36.510097Z","iopub.execute_input":"2021-07-26T17:07:36.510423Z","iopub.status.idle":"2021-07-26T17:07:36.541079Z","shell.execute_reply.started":"2021-07-26T17:07:36.510391Z","shell.execute_reply":"2021-07-26T17:07:36.540049Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":" model = Sequential(layers_1)\n\nmodel.compile(\n        optimizer=Adam(1e-3),\n        loss=nn_rmspe,\n    )\n    \nhist = model.fit(\n        x=X_train, y=y_train, \n        batch_size=512, \n        epochs=1000,\n        verbose=True,\n        callbacks=[\n            RLP(monitor='val_loss', factor=0.98, patience=15, verbose=1), \n            ES(monitor='val_loss', patience=200, verbose=1, restore_best_weights=True)\n        ], \n        validation_data=(X_val, y_val),\n        shuffle=True,\n    )","metadata":{"execution":{"iopub.status.busy":"2021-07-26T17:07:37.190498Z","iopub.execute_input":"2021-07-26T17:07:37.190858Z","iopub.status.idle":"2021-07-26T17:21:30.590814Z","shell.execute_reply.started":"2021-07-26T17:07:37.190827Z","shell.execute_reply":"2021-07-26T17:21:30.587597Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.save('/kaggle/working/nnv3_03')","metadata":{"execution":{"iopub.status.busy":"2021-07-26T05:48:27.565332Z","iopub.execute_input":"2021-07-26T05:48:27.565697Z","iopub.status.idle":"2021-07-26T05:48:32.065586Z","shell.execute_reply.started":"2021-07-26T05:48:27.565636Z","shell.execute_reply":"2021-07-26T05:48:32.064445Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"layers_2 = [\n    Dense(16, activation='relu', kernel_initializer=TruncatedNormal(0, 1, 11), bias_initializer=TruncatedNormal(1e-1, 1e-3, 11)),\n    Dense(16, activation='relu', kernel_initializer=TruncatedNormal(0, 2e-3, 11), bias_initializer=TruncatedNormal(0, 5e-3, 11)),\n    BatchNormalization(),\n    Dense(16, activation='relu', kernel_initializer=TruncatedNormal(0, 1, 161), bias_initializer=TruncatedNormal(0, 1e-2, 151)),\n    Dense(16, activation='relu', kernel_initializer=TruncatedNormal(0, 1e-1, 61), bias_initializer=TruncatedNormal(0, 1e-2, 151)),\n    BatchNormalization(),\n    Dense(1, activation='sigmoid', kernel_initializer=TruncatedNormal(0, 1e-1, 11), bias_initializer=TruncatedNormal(0, 1e-4, 32)),\n]\n\n# nn_seq_model(layers_2, 2)","metadata":{"execution":{"iopub.status.busy":"2021-07-23T02:37:27.032885Z","iopub.execute_input":"2021-07-23T02:37:27.033242Z","iopub.status.idle":"2021-07-23T02:37:49.679295Z","shell.execute_reply.started":"2021-07-23T02:37:27.033209Z","shell.execute_reply":"2021-07-23T02:37:49.678356Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"layers_3 = [\n    Dense(32, activation='relu', kernel_initializer=TruncatedNormal(0, 1, 11), bias_initializer=TruncatedNormal(1e-1, 1e-3, 11)),\n    Dense(32, activation='relu', kernel_initializer=TruncatedNormal(0, 2, 11), bias_initializer=TruncatedNormal(0, 5e-2, 11)),\n    Dense(32, activation='relu', kernel_initializer=TruncatedNormal(0, 1, 161), bias_initializer=TruncatedNormal(0, 1e-3, 151)),\n    Dense(32, activation='relu', kernel_initializer=TruncatedNormal(0, 1e-1, 61), bias_initializer=TruncatedNormal(0, 1e-4, 11)),\n    Dense(32, activation='relu', kernel_initializer=TruncatedNormal(0, 1, 161), bias_initializer=TruncatedNormal(0, 1e-1, 101)),\n    Dense(32, activation='relu', kernel_initializer=TruncatedNormal(0, 1e-1, 61), bias_initializer=TruncatedNormal(0, 1e-2, 51)),\n    Dense(32, activation='relu', kernel_initializer=TruncatedNormal(0, 1, 161), bias_initializer=TruncatedNormal(0, 3e-1, 11)),\n    Dense(32, activation='relu', kernel_initializer=TruncatedNormal(0, 1e-1, 61), bias_initializer=TruncatedNormal(0, 1, 151)),\n    BatchNormalization(),\n    Dense(1, activation='sigmoid', kernel_initializer=TruncatedNormal(0, 1e-1, 11), bias_initializer=TruncatedNormal(0, 1e-1, 32)),\n]\n\n# nn_seq_model(layers_3, 3)","metadata":{"execution":{"iopub.status.busy":"2021-07-23T02:37:49.680928Z","iopub.execute_input":"2021-07-23T02:37:49.681267Z","iopub.status.idle":"2021-07-23T02:38:12.670543Z","shell.execute_reply.started":"2021-07-23T02:37:49.681231Z","shell.execute_reply":"2021-07-23T02:38:12.669734Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"layers_4 = [\n    BatchNormalization(),\n    Dense(32, activation=None, kernel_initializer=TruncatedNormal(0, 1e-1, 161), bias_initializer=TruncatedNormal(0, 1, 151)),\n    Dense(32, activation='relu', kernel_initializer=TruncatedNormal(0, 1e-1, 61), bias_initializer=TruncatedNormal(0, 1, 151)),\n    BatchNormalization(),\n    Dense(64, activation=None, kernel_initializer=TruncatedNormal(0, 5e-2, 11), bias_initializer=TruncatedNormal(0, 1, 151)),\n    Dense(128, activation='relu', kernel_initializer=TruncatedNormal(0, 5e-1, 161), bias_initializer=TruncatedNormal(0, 1, 151)),\n    Dense(256, activation=None, kernel_initializer=TruncatedNormal(0, 8e-1, 161), bias_initializer=TruncatedNormal(0, 1, 151)),\n    Dense(1, activation=None, kernel_initializer=TruncatedNormal(0, 1e-1, 11), bias_initializer=TruncatedNormal(0, 1e-1, 32)),\n]\n\n# nn_seq_model(layers_4, 4)","metadata":{"execution":{"iopub.status.busy":"2021-07-23T02:38:12.672238Z","iopub.execute_input":"2021-07-23T02:38:12.672588Z","iopub.status.idle":"2021-07-23T02:38:33.332762Z","shell.execute_reply.started":"2021-07-23T02:38:12.672558Z","shell.execute_reply":"2021-07-23T02:38:33.331929Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train_preds = pd.DataFrame(train_preds)\n# test_preds = pd.DataFrame(test_preds)","metadata":{"execution":{"iopub.status.busy":"2021-07-22T22:33:47.154084Z","iopub.execute_input":"2021-07-22T22:33:47.154444Z","iopub.status.idle":"2021-07-22T22:33:48.465319Z","shell.execute_reply.started":"2021-07-22T22:33:47.15441Z","shell.execute_reply":"2021-07-22T22:33:48.464442Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Ensembling\n\n","metadata":{}},{"cell_type":"code","source":"# X_train, X_val, y_train, y_val = train_test_split(train_preds, y, test_size=0.2, random_state=23)","metadata":{"execution":{"iopub.status.busy":"2021-07-23T02:38:47.492229Z","iopub.execute_input":"2021-07-23T02:38:47.492605Z","iopub.status.idle":"2021-07-23T02:38:47.557663Z","shell.execute_reply.started":"2021-07-23T02:38:47.492571Z","shell.execute_reply":"2021-07-23T02:38:47.556789Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ens_layers = [\n#     Dense(8, activation=None, kernel_initializer=TruncatedNormal(0, 2, 11), bias_initializer=TruncatedNormal(0, 1, 151)),\n#     Dense(8, activation='relu', kernel_initializer=TruncatedNormal(0, 5e-1, 11), bias_initializer=TruncatedNormal(0, 1, 11)),\n#     Dense(8, activation='relu', kernel_initializer=TruncatedNormal(0, 8e-1, 61),bias_initializer=TruncatedNormal(0, 1, 51)),\n#     BatchNormalization(),\n#     Dense(1, activation='sigmoid', kernel_initializer=TruncatedNormal(0, 1e-1, 11),\n#         bias_initializer=TruncatedNormal(0, 1e-1, 32), kernel_regularizer=None,\n#         bias_regularizer=None, activity_regularizer=None),\n# ]\n\n# model = Sequential(ens_layers)\n\n# model.compile(\n#     optimizer=Adam(5e-3),\n#     loss=nn_rmspe,\n# )\n\n# model.fit(\n#         x=X_train, y=y_train, \n#         batch_size=256, \n#         epochs=1000,\n#         verbose=False,\n#         callbacks=[\n#             RLP(monitor='val_loss', factor=0.98, patience=15, verbose=1), \n#             ES(monitor='val_loss', patience=10, verbose=1, restore_best_weights=True)\n#         ], \n#         validation_data=(X_val, y_val),\n#         shuffle=True,\n#     )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submit(model.predict(X_test))","metadata":{"execution":{"iopub.status.busy":"2021-07-26T05:48:47.377292Z","iopub.execute_input":"2021-07-26T05:48:47.377708Z","iopub.status.idle":"2021-07-26T05:48:47.597467Z","shell.execute_reply.started":"2021-07-26T05:48:47.377675Z","shell.execute_reply":"2021-07-26T05:48:47.596198Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.read_csv('/kaggle/working/submission.csv')","metadata":{"execution":{"iopub.status.busy":"2021-07-26T05:48:49.241209Z","iopub.execute_input":"2021-07-26T05:48:49.241656Z","iopub.status.idle":"2021-07-26T05:48:49.25847Z","shell.execute_reply.started":"2021-07-26T05:48:49.241622Z","shell.execute_reply":"2021-07-26T05:48:49.256932Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Reference (s)\nhttps://www.kaggle.com/konradb/we-need-to-go-deeper-and-validate#Model","metadata":{}}]}