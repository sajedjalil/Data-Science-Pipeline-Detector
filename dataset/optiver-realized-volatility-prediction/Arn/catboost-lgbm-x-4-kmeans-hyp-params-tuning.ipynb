{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Hey,It was really great to be part of this competition. Learnt a lot of new stuff.\n\n1. I have reffred a lot from existing public notebooks. Lovely Thanks for that.**\n\n2. Used CatBoost and 2 LGBM models, which is trained through 5 GroupKFold by time_id. \n\n3. I have used very few feature, only about 30-40.\n\n4. Optuna was my library for Hyperparameter tuning. \n\n5. Also used Kmeans Algo on columns which cor-related highly with target column.\n\n7. Had other stuff going on, I was able to put very little effort on this competition(Some where about 4-5 days). I wish I had more time to be spent on this competition, I could have learnt more\n\n8. **With this Notebook I was promoted to notebook expert. Thank You for your support.**\n\n**All the Best**","metadata":{}},{"cell_type":"markdown","source":"# Imports","metadata":{}},{"cell_type":"code","source":"from IPython.core.display import display, HTML\n\nimport pandas as pd\nimport numpy as np \nimport pandas as pd\nimport glob\nimport os\nimport gc\n\nfrom joblib import Parallel, delayed\n\nfrom sklearn import preprocessing, model_selection\nfrom sklearn.cluster import KMeans\nimport lightgbm as lgb\n\nfrom sklearn.metrics import r2_score\n\nimport matplotlib.pyplot as plt \nimport seaborn as sns\n\npath_data = '../input/optiver-realized-volatility-prediction/'\n\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-09-27T15:49:51.979956Z","iopub.execute_input":"2021-09-27T15:49:51.980248Z","iopub.status.idle":"2021-09-27T15:49:51.9876Z","shell.execute_reply.started":"2021-09-27T15:49:51.980216Z","shell.execute_reply":"2021-09-27T15:49:51.986575Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Dataset Pre-load","metadata":{}},{"cell_type":"code","source":"%%time\nStockID = True\nif StockID:\n    df_training = pd.read_pickle('../input/optiver-dataset-version-2/training.pkl')\nelse:\n    df_training = pd.read_pickle('../input/optiver-dataset-version-2/training_no_stock_id.pkl')","metadata":{"execution":{"iopub.status.busy":"2021-09-27T15:32:22.018286Z","iopub.execute_input":"2021-09-27T15:32:22.018584Z","iopub.status.idle":"2021-09-27T15:32:25.191478Z","shell.execute_reply.started":"2021-09-27T15:32:22.018557Z","shell.execute_reply":"2021-09-27T15:32:25.190538Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_training = df_training.drop(['wap2_mean', 'wap2_std'], axis = 1)","metadata":{"execution":{"iopub.status.busy":"2021-09-27T15:32:25.193339Z","iopub.execute_input":"2021-09-27T15:32:25.193707Z","iopub.status.idle":"2021-09-27T15:32:25.281714Z","shell.execute_reply.started":"2021-09-27T15:32:25.193666Z","shell.execute_reply":"2021-09-27T15:32:25.280947Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Kmeans","metadata":{}},{"cell_type":"code","source":"%%time\ncols_kmeans = ['log_return1_realized_volatility', 'log_return1_realized_absvar',\n    'log_return1_realized_quarticity','log_return1_realized_tripower_quarticity',\n    'log_return1_realized_quadpower_quarticity', 'log_return1_realized_1',\n    'log_return1_realized_2']\nclustering = False\nif clustering:\n    SSE = []\n    for cluster in range(1,11):\n        print(f'Done Cluster: {cluster}')\n        kmeans = KMeans(n_init = 8, n_clusters = cluster, init='k-means++')\n        kmeans.fit(df_training[cols])\n        SSE.append(kmeans.inertia_)\n    # Elbow Method\n    frame = pd.DataFrame({'Cluster':range(1,11), 'SSE':SSE})\n    plt.figure(figsize=(12,6))\n    plt.plot(frame['Cluster'], frame['SSE'], marker='o')\n    plt.xlabel('Number of clusters')\n    plt.ylabel('Inertia')","metadata":{"execution":{"iopub.status.busy":"2021-09-27T15:32:25.282848Z","iopub.execute_input":"2021-09-27T15:32:25.283203Z","iopub.status.idle":"2021-09-27T15:32:25.29109Z","shell.execute_reply.started":"2021-09-27T15:32:25.283175Z","shell.execute_reply":"2021-09-27T15:32:25.289948Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time \n\nKMEAN = True\nif KMEAN:\n    kmeans = KMeans(n_init = 20, n_clusters = 8, init='k-means++')\n\n    kmeans.fit(df_training[cols_kmeans].fillna(0))\n        \n    df_training['kmeans'] = kmeans.labels_","metadata":{"execution":{"iopub.status.busy":"2021-09-27T15:32:25.293353Z","iopub.execute_input":"2021-09-27T15:32:25.293663Z","iopub.status.idle":"2021-09-27T15:32:43.212454Z","shell.execute_reply.started":"2021-09-27T15:32:25.293634Z","shell.execute_reply":"2021-09-27T15:32:43.211836Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_training['kmeans']","metadata":{"execution":{"iopub.status.busy":"2021-09-27T15:32:43.213903Z","iopub.execute_input":"2021-09-27T15:32:43.214381Z","iopub.status.idle":"2021-09-27T15:32:43.224745Z","shell.execute_reply.started":"2021-09-27T15:32:43.214346Z","shell.execute_reply":"2021-09-27T15:32:43.223213Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#%%time\n#df_testing = pd.read_pickle('../input/optiver-dataset-version-1/testing.pkl')","metadata":{"execution":{"iopub.status.busy":"2021-09-27T15:32:43.227336Z","iopub.execute_input":"2021-09-27T15:32:43.228784Z","iopub.status.idle":"2021-09-27T15:32:43.238062Z","shell.execute_reply.started":"2021-09-27T15:32:43.228744Z","shell.execute_reply":"2021-09-27T15:32:43.237306Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Preprocessing && Test Data ","metadata":{}},{"cell_type":"code","source":"def log_return(list_stock_prices):\n    return np.log(list_stock_prices).diff() \n\ndef realized_volatility(series_log_return):\n    return np.sqrt(np.sum(series_log_return**2))\n\ndef rmspe(y_true, y_pred):\n    return  (np.sqrt(np.mean(np.square((y_true - y_pred) / y_true))))\n\ndef calc_wap1(df):\n    wap = (df['bid_price1'] * df['ask_size1'] + df['ask_price1'] * df['bid_size1']) / (df['bid_size1'] + df['ask_size1'])\n    return wap\n\ndef calc_wap2(df):\n    wap = (df['bid_price2'] * df['ask_size2'] + df['ask_price2'] * df['bid_size2']) / (df['bid_size2'] + df['ask_size2'])\n    return wap\n\ndef realized_absvar(series):\n    return np.sqrt(np.pi/(2*series.count()))*np.sum(np.abs(series))\n\n# Calculate integrated quarticity\ndef realized_quarticity(series):\n    return (series.count()/3)*np.sum(series**4)\n\ndef calc_depth(df):\n    depth = df['bid_price1'] * df['bid_size1'] + df['ask_price1'] * df['ask_size1'] + df['bid_price2'] * df[\n               'bid_size2'] + df['ask_price2'] * df['ask_size2']\n    return depth\n\ndef realized_3(series):\n    from scipy.special import gamma\n    numerator = (gamma(1/2)**3)*np.sum((abs(series)**(4/3)).rolling(window=3).apply(np.prod))\n    denominator = 8 * (gamma(7/6)**3)*np.sum(series**2)\n    return np.sqrt(numerator/denominator)\n\ndef realized_2(series):\n    return np.sqrt(((np.pi**2)*np.sum(abs(series.rolling(window=4).apply(np.product, raw=True))))/(8*np.sum(series**2)))\n\ndef realized_1(series):\n    return np.sqrt(np.sum(series**4)/(6*np.sum(series**2)))\n\ndef realized_tripower_quarticity(series):\n    from scipy.special import gamma\n    series = series ** (4/3)\n    series = abs(series).rolling(window=3).apply(np.prod, raw=True)\n    return series.shape[0]*0.25*((gamma(1/2)**3)/(gamma(7/6)**3))*np.sum(series)\n\ndef realized_quadpower_quarticity(series):\n    series = abs(series.rolling(window=4).apply(np.product, raw=True))\n    return (np.sum(series) * series.shape[0] * (np.pi**2))/4\n\ndef processing_book_data(file_path):\n    \n    df = pd.read_parquet(file_path)\n    # Wap 1\n    df['wap1'] = calc_wap1(df) \n    df['log_return1'] = df.groupby('time_id')['wap1'].apply(log_return)\n    # Wap 2 \n    df['wap2'] = calc_wap2(df)\n    df['log_return2'] = df.groupby('time_id')['wap2'].apply(log_return)\n    \n    df['wap_balance'] = abs(df['wap1'] - df['wap2'])\n    df['price_spread'] = (df['ask_price1'] - df['bid_price1']) / ((df['ask_price1'] + df['bid_price1'])/2)\n    df['bid_spread'] = df['bid_price1'] - df['bid_price2']\n    df['ask_spread'] = df['ask_price1'] - df['ask_price2']\n    df['total_volume'] = (df['ask_size1'] + df['ask_size2']) + (df['bid_size1'] + df['bid_size2'])\n    df['volume_imbalance'] = abs((df['ask_size1'] + df['ask_size2']) - (df['bid_size1'] + df['bid_size2']))\n    df['depth'] = calc_depth(df)\n    \n    create_feature_dict = {\n        'log_return1':[realized_volatility,realized_absvar,realized_quarticity,realized_tripower_quarticity,realized_quadpower_quarticity,realized_1,realized_2],\n        'log_return2':[realized_volatility,realized_absvar,realized_quarticity,realized_tripower_quarticity,realized_quadpower_quarticity,realized_1,realized_2],\n        'wap_balance':[np.mean],\n        #'price_spread':[np.mean],\n        'bid_spread':[np.mean,np.sum],\n        'ask_spread':[np.mean, np.max],\n        'volume_imbalance':[np.mean],\n        #'total_volume':[np.mean],\n        'wap1':[np.mean,np.std],\n        #'wap2':[np.mean,np.std]\n    }\n    create_feature_dict_time = {\n        'log_return1': [realized_volatility],\n        'log_return2': [realized_volatility]\n    }\n    def get_stats_window(fe_dict,seconds_in_bucket, add_suffix = False):\n        # Group by the window\n        df_feature = df[df['seconds_in_bucket'] >= seconds_in_bucket].groupby(['time_id']).agg(fe_dict).reset_index()\n        # Rename columns joining suffix\n        df_feature.columns = ['_'.join(col) for col in df_feature.columns]\n        # Add a suffix to differentiate windows\n        if add_suffix:\n            df_feature = df_feature.add_suffix('_' + str(seconds_in_bucket))\n        return df_feature\n    # 300 and 400 \n    df_feature_400 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 400, add_suffix = True)\n    df_feature_300 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 300, add_suffix = True)\n    # 600\n    df_feature = pd.DataFrame(df.groupby(['time_id']).agg(create_feature_dict)).reset_index()\n    df_feature.columns = ['_'.join(col) for col in df_feature.columns]\n    # merge\n    df_feature = df_feature.merge(df_feature_400, how = 'left', left_on = 'time_id_', right_on = 'time_id__400')\n    df_feature = df_feature.merge(df_feature_300, how = 'left', left_on = 'time_id_', right_on = 'time_id__300')\n    df_feature.drop(['time_id__400', 'time_id__300'], axis = 1, inplace = True)\n    \n    stock_id = file_path.split('=')[1]\n    df_feature['row_id'] = df_feature['time_id_'].apply(lambda x:f'{stock_id}-{x}')\n    df_feature = df_feature.drop(['time_id_'],axis=1)\n    \n    return df_feature\n\ndef processing_trade_data(file_path):\n    \n    df = pd.read_parquet(file_path)\n    df['log_return'] = df.groupby('time_id')['price'].apply(log_return)\n    \n    aggregate_dictionary = {\n        'log_return':[realized_volatility],\n    }\n    \n    create_feature_dict_time = {\n        'log_return':[realized_volatility],\n    }\n    \n    def get_stats_window(fe_dict,seconds_in_bucket, add_suffix = False):\n        # Group by the window\n        df_feature = df[df['seconds_in_bucket'] >= seconds_in_bucket].groupby(['time_id']).agg(fe_dict).reset_index()\n        # Rename columns joining suffix\n        df_feature.columns = ['_'.join(col) for col in df_feature.columns]\n        # Add a suffix to differentiate windows\n        if add_suffix:\n            df_feature = df_feature.add_suffix('_' + str(seconds_in_bucket))\n        return df_feature\n    \n    df_feature = df.groupby('time_id').agg(aggregate_dictionary)\n    df_feature = df_feature.reset_index()\n    df_feature.columns = ['_'.join(col) for col in df_feature.columns]\n    df_feature_400 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 400, add_suffix = True)\n    df_feature_300 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 300, add_suffix = True)\n    \n    df_feature = df_feature.merge(df_feature_400, how = 'left', left_on = 'time_id_', right_on = 'time_id__400')\n    df_feature = df_feature.merge(df_feature_300, how = 'left', left_on = 'time_id_', right_on = 'time_id__300')\n    \n    df_feature.drop(['time_id__400', 'time_id__300'], axis = 1, inplace = True)\n    df_feature = df_feature.add_prefix('trade_')\n    stock_id = file_path.split('=')[1]\n    df_feature['row_id'] = df_feature['trade_time_id_'].apply(lambda x:f'{stock_id}-{x}')\n    df_feature.drop(['trade_time_id_'], axis = 1, inplace = True)\n\n    return df_feature\n\ndef get_time_stock(df):\n    vol_cols = ['log_return1_realized_volatility',  \n                'log_return1_realized_volatility_400',\n                'log_return2_realized_volatility_400', \n                'log_return1_realized_volatility_300',\n                'log_return2_realized_volatility_300',\n                'trade_log_return_realized_volatility', \n                'trade_log_return_realized_volatility_400',\n                'trade_log_return_realized_volatility_300']\n\n    valo_cols1 = ['log_return1_realized_volatility','trade_log_return_realized_volatility_400']\n    # Group by the stock id\n    df_stock_id = df.groupby(['stock_id'])[vol_cols].agg(['mean', 'max', 'min' ]).reset_index()\n    # Rename columns joining suffix\n    df_stock_id.columns = ['_'.join(col) for col in df_stock_id.columns]\n    df_stock_id = df_stock_id.add_suffix('_' + 'stock')\n\n    feature_column_needed = [ 'stock_id__stock' ,'log_return1_realized_volatility_mean_stock', \n       'log_return1_realized_volatility_max_stock',\n       'log_return1_realized_volatility_min_stock',\n       'log_return1_realized_volatility_400_max_stock',\n       'log_return1_realized_volatility_400_min_stock',\n       'log_return2_realized_volatility_400_min_stock',\n       'log_return1_realized_volatility_300_min_stock',\n       'log_return2_realized_volatility_300_min_stock',\n       'trade_log_return_realized_volatility_mean_stock',\n       'trade_log_return_realized_volatility_max_stock',\n       'trade_log_return_realized_volatility_400_max_stock',\n       'trade_log_return_realized_volatility_400_min_stock',\n       'trade_log_return_realized_volatility_300_min_stock',]\n    \n    df_stock_id_column = pd.DataFrame(df_stock_id, columns = feature_column_needed)\n    \n    # Group by the stock id\n    df_time_id = df.groupby(['time_id'])[valo_cols1].agg(['mean', 'min' ]).reset_index()\n    # Rename columns joining suffix\n    df_time_id.columns = ['_'.join(col) for col in df_time_id.columns]\n    df_time_id = df_time_id.add_suffix('_' + 'time')\n    \n    feature_column_needed = ['time_id__time',\n        'log_return1_realized_volatility_mean_time',\n       'trade_log_return_realized_volatility_400_min_time']\n    \n    df_time_id_column = pd.DataFrame(df_time_id, columns= feature_column_needed)\n    \n    \n    # Merge with original dataframe\n    df = df.merge(df_stock_id_column, how = 'left', left_on = ['stock_id'], right_on = ['stock_id__stock'])\n    df = df.merge(df_time_id_column, how = 'left', left_on = ['time_id'], right_on = ['time_id__time'])\n    df.drop(['stock_id__stock', 'time_id__time'], axis = 1, inplace = True)\n    \n    return df\n\ndef read_train_test():\n    #train = pd.read_csv('../input/optiver-realized-volatility-prediction/train.csv')\n    test = pd.read_csv('../input/optiver-realized-volatility-prediction/test.csv')\n    # Create a key to merge with book and trade data\n    #train['row_id'] = train['stock_id'].astype(str) + '-' + train['time_id'].astype(str)\n    test['row_id'] = test['stock_id'].astype(str) + '-' + test['time_id'].astype(str)\n    #print(f'Our training set has {train.shape[0]} rows')\n    return test\n\ndef preprocessor(list_stock_ids, is_train = True):\n    from joblib import Parallel, delayed # parallel computing to save time\n    df = pd.DataFrame()\n    \n    def for_joblib(stock_id):\n        if is_train:\n            file_path_book = path_data + \"book_train.parquet/stock_id=\" + str(stock_id)\n            file_path_trade = path_data + \"trade_train.parquet/stock_id=\" + str(stock_id)\n        else:\n            file_path_book = path_data + \"book_test.parquet/stock_id=\" + str(stock_id)\n            file_path_trade = path_data + \"trade_test.parquet/stock_id=\" + str(stock_id)\n            \n        df_tmp = pd.merge(processing_book_data(file_path_book),processing_trade_data(file_path_trade),on='row_id',how='left')\n     \n        return pd.concat([df,df_tmp])\n    \n    df = Parallel(n_jobs=-1, verbose=1)(\n        delayed(for_joblib)(stock_id) for stock_id in list_stock_ids\n        )\n\n    df =  pd.concat(df,ignore_index = True)\n    return df","metadata":{"_kg_hide-input":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2021-09-27T15:32:43.23986Z","iopub.execute_input":"2021-09-27T15:32:43.240548Z","iopub.status.idle":"2021-09-27T15:32:43.318281Z","shell.execute_reply.started":"2021-09-27T15:32:43.240507Z","shell.execute_reply":"2021-09-27T15:32:43.317319Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\ntest = read_train_test()\n\ntest_ids = test.stock_id.unique()\n\ndf_test = preprocessor(list_stock_ids= test_ids, is_train = False)\ndf_testing = test.merge(df_test, on = ['row_id'], how = 'left')\ndf_testing['kmeans'] = kmeans.predict(df_testing[cols_kmeans].fillna(0))\nif StockID:\n    df_testing = get_time_stock(df_testing)\n    df_testing.drop(['trade_log_return_realized_volatility_400', 'log_return2_realized_volatility_300', 'log_return1_realized_volatility_300', 'trade_log_return_realized_volatility_300', 'log_return2_realized_volatility_400','log_return1_realized_volatility_400'], axis = 1, inplace = True)\nelse:\n    pass","metadata":{"execution":{"iopub.status.busy":"2021-09-27T15:32:43.324648Z","iopub.execute_input":"2021-09-27T15:32:43.327756Z","iopub.status.idle":"2021-09-27T15:32:44.82478Z","shell.execute_reply.started":"2021-09-27T15:32:43.327707Z","shell.execute_reply":"2021-09-27T15:32:44.823995Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_testing.shape","metadata":{"execution":{"iopub.status.busy":"2021-09-27T15:32:44.826057Z","iopub.execute_input":"2021-09-27T15:32:44.826682Z","iopub.status.idle":"2021-09-27T15:32:44.833004Z","shell.execute_reply.started":"2021-09-27T15:32:44.826631Z","shell.execute_reply":"2021-09-27T15:32:44.832208Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Correlation ","metadata":{}},{"cell_type":"code","source":"#import seaborn as sns\n#plt.figure(figsize=(30,30))\n#corr = df_training.corr()\n#sns.heatmap(corr,cmap='Blues',linewidth=0.5,annot=True)","metadata":{"execution":{"iopub.status.busy":"2021-09-27T15:32:44.835261Z","iopub.execute_input":"2021-09-27T15:32:44.835563Z","iopub.status.idle":"2021-09-27T15:32:44.84452Z","shell.execute_reply.started":"2021-09-27T15:32:44.835533Z","shell.execute_reply":"2021-09-27T15:32:44.843407Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model && Tuning ","metadata":{}},{"cell_type":"code","source":"import optuna \nfrom sklearn.model_selection import GroupKFold\nimport lightgbm as lgb\n\ndef rmspe(y_true, y_pred):\n    return (np.sqrt(np.mean(np.square((y_true - y_pred) / y_true))))\n\ndef rmspe_xgb(pred, dtrain):\n    y = dtrain.get_label()\n    return 'rmspe', rmspe(y, pred)\n\ndef feval_rmspe(y_pred, lgb_train):\n    y_true = lgb_train.get_label()\n    return 'RMSPE', rmspe(y_true, y_pred), False","metadata":{"execution":{"iopub.status.busy":"2021-09-27T15:32:44.845765Z","iopub.execute_input":"2021-09-27T15:32:44.84604Z","iopub.status.idle":"2021-09-27T15:32:44.859711Z","shell.execute_reply.started":"2021-09-27T15:32:44.846006Z","shell.execute_reply":"2021-09-27T15:32:44.858803Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# LGBM Tuning","metadata":{}},{"cell_type":"code","source":"num_folds = 5\ngroup_fold = GroupKFold( n_splits = num_folds )\n\nfeatures = [col for col in df_training.columns if col not in { \"target\", \"row_id\"}]\nX = df_training[features]\ny = df_training['target']\n\ndef objective(trial):\n\n    # Optuna suggest params\n    seed = 1111\n    params = {\n        'learning_rate': trial.suggest_uniform('learning_rate', 0.01, 0.2),        \n        'lambda_l1': trial.suggest_float('lambda_l1', 0.1, 1),\n        'lambda_l2': trial.suggest_float('lambda_l2', 0.1, 1),\n        'num_leaves': trial.suggest_int('num_leaves', 400, 800),\n        'feature_fraction': trial.suggest_uniform('feature_fraction', 0.50, 1),\n        'bagging_fraction': trial.suggest_uniform('bagging_fraction', 0.50, 1),\n        'bagging_freq': trial.suggest_int('bagging_freq',1, 2),\n        'min_data_in_leaf':trial.suggest_int('min_data_in_leaf',400,700),\n        'max_depth': trial.suggest_int('max_depth', 6 , 13),\n        'seed': seed,\n        'objective': 'rmse',\n        'boosting': 'gbdt',\n        'verbosity': -1,\n        'n_jobs': -1,\n        #'device': 'gpu','gpu_platform_id': 0,\n        #'gpu_device_id': 0\n    }  \n\n    \n    rmspe_list = []\n\n    for fold, (trn_ind, val_ind) in enumerate(group_fold.split(X, y, groups = X['time_id'])):\n        print(f'Training fold {fold + 1}')\n        x_train, x_val = X.iloc[trn_ind], X.iloc[val_ind]\n        y_train, y_val = y.iloc[trn_ind], y.iloc[val_ind]\n        # Root mean squared percentage error weights\n        train_weights = 1 / np.square(y_train)\n        val_weights = 1 / np.square(y_val)\n        train_dataset = lgb.Dataset(x_train, y_train, weight = train_weights)\n        val_dataset = lgb.Dataset(x_val, y_val, weight = val_weights)\n        model = lgb.train(params = params,\n                          num_boost_round=5000,\n                          train_set = train_dataset, \n                          valid_sets = [train_dataset, val_dataset], \n                          verbose_eval = 250,\n                          early_stopping_rounds=20,\n                          feval = feval_rmspe)\n        \n        #preds = model.predict(d_val)\n        preds = model.predict(x_val[features])\n        score = rmspe(y_val, preds)\n        rmspe_list.append(score)\n    \n    print(f'Trial done: rmspe values on folds: {np.mean(rmspe_list)}')\n    return np.mean(rmspe_list)","metadata":{"_kg_hide-input":false,"execution":{"iopub.status.busy":"2021-09-27T15:32:44.861141Z","iopub.execute_input":"2021-09-27T15:32:44.861781Z","iopub.status.idle":"2021-09-27T15:32:44.933231Z","shell.execute_reply.started":"2021-09-27T15:32:44.861743Z","shell.execute_reply":"2021-09-27T15:32:44.932275Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"n_trials = 10\n\nFIT_LGB = False\n\nif FIT_LGB:\n    study = optuna.create_study(direction=\"minimize\",study_name = 'LGB')\n    study.optimize(objective)\n\n    print(\"Number of finished trials: {}\".format(len(study.trials)))\n\n    print(\"Best trial:\")\n    trial = study.best_trial\n\n    print(\"  Value: {}\".format(trial.value))\n\n    print(\"  Params: \")\n    for key, value in trial.params.items():\n        print(\"    {}: {}\".format(key, value))","metadata":{"execution":{"iopub.status.busy":"2021-09-27T15:32:44.93433Z","iopub.execute_input":"2021-09-27T15:32:44.93508Z","iopub.status.idle":"2021-09-27T15:32:44.94169Z","shell.execute_reply.started":"2021-09-27T15:32:44.935045Z","shell.execute_reply":"2021-09-27T15:32:44.940927Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# CatBoost Tuning ","metadata":{}},{"cell_type":"code","source":"from catboost import CatBoostRegressor, Pool\nnum_folds = 5\ngroup_fold = GroupKFold( n_splits = num_folds )\n\nfeatures = [col for col in df_training.columns if col not in { \"target\", \"row_id\"}]\nX = df_training[features]\ny = df_training['target']\ndef objective(trial):\n\n    params = {\n        #\"colsample_bylevel\": trial.suggest_float(\"colsample_bylevel\", 0.5, 1),\n        \"depth\": trial.suggest_int(\"depth\", 6, 11),\n        \"learning_rate\": trial.suggest_uniform(\"learning_rate\",0.01,0.2),\n        \"l2_leaf_reg\": trial.suggest_float(\"l2_leaf_reg\",0.5,1),\n        \"random_seed\": 42,\n        \"iterations\": 5000,\n        \"grow_policy\": trial.suggest_categorical(\"grow_policy\", [\"SymmetricTree\", \"Lossguide\", \"Depthwise\"]),\n        \"bootstrap_type\":\"Bayesian\", \n        \"bagging_temperature\": trial.suggest_float(\"bagging_temperature\", 0, 5),\n        \"loss_function\" : 'RMSE',\n        \"eval_metric\":\"RMSE\",\n        \"has_time\": True\n        #\"task_type\":'GPU'\n    }\n    \n    if params[\"grow_policy\"] == \"Depthwise\" or params[\"grow_policy\"] == \"Lossguide\":\n        params[\"min_data_in_leaf\"] = trial.suggest_int(\"min_data_in_leaf\", 400, 700)\n    if params[\"grow_policy\"] == \"Lossguide\":\n        params[\"max_leaves\"] = trial.suggest_int(\"max_leaves\", 10, 20)\n    \n    #\n    rmspe_list = []\n\n    for fold, (trn_ind, val_ind) in enumerate(group_fold.split(X, y, groups = X['time_id'])):\n        print(f'Training fold {fold + 1}')\n        x_train, x_val = X.iloc[trn_ind], X.iloc[val_ind]\n        y_train, y_val = y.iloc[trn_ind], y.iloc[val_ind]\n        # Root mean squared percentage error weights\n        \n        train_weights = 1 / np.square(y_train)\n        val_weights = 1 / np.square(y_val)\n        \n        train_pool = Pool(x_train, y_train,weight=train_weights)\n        val_pool = Pool(x_val, y_val,weight=val_weights)\n        \n        model = CatBoostRegressor(**params)\n        \n        model.fit(train_pool,\n                  use_best_model = True,\n                  eval_set = val_pool, \n                  verbose = 250,\n                  early_stopping_rounds=10,\n                  plot =True)\n\n        preds = model.predict(x_val[features])\n        score = rmspe(y_val, preds)\n        rmspe_list.append(score)\n    \n    print(f'Trial done: rmspe values on folds: {np.mean(rmspe_list)}')\n    return np.mean(rmspe_list)","metadata":{"execution":{"iopub.status.busy":"2021-09-27T15:32:44.943133Z","iopub.execute_input":"2021-09-27T15:32:44.943513Z","iopub.status.idle":"2021-09-27T15:32:45.220985Z","shell.execute_reply.started":"2021-09-27T15:32:44.943472Z","shell.execute_reply":"2021-09-27T15:32:45.22002Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"n_trials = 10\n\nFIT_CAT = False\n\nif FIT_CAT:\n    study = optuna.create_study(direction=\"minimize\",study_name = 'CAT')\n    study.optimize(objective, n_trials=n_trials)\n\n    print(\"Number of finished trials: {}\".format(len(study.trials)))\n\n    print(\"Best trial:\")\n    trial = study.best_trial\n\n    print(\"  Value: {}\".format(trial.value))\n\n    print(\"  Params: \")\n    for key, value in trial.params.items():\n        print(\"    {}: {}\".format(key, value))","metadata":{"execution":{"iopub.status.busy":"2021-09-27T15:32:45.222182Z","iopub.execute_input":"2021-09-27T15:32:45.222426Z","iopub.status.idle":"2021-09-27T15:32:45.229627Z","shell.execute_reply.started":"2021-09-27T15:32:45.222398Z","shell.execute_reply":"2021-09-27T15:32:45.228479Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# CAT Boost Training","metadata":{}},{"cell_type":"code","source":"cat_params = {'depth': 6, 'learning_rate': 0.08814388269284729, 'l2_leaf_reg': 0.863393460851281, \n              'grow_policy': 'Lossguide', 'bagging_temperature': 0.9196224418501292, 'min_data_in_leaf': 443, \n              'max_leaves': 20,\"random_seed\": 42,\"iterations\": 5000,\n              \"bootstrap_type\":\"Bayesian\", \"loss_function\" : 'RMSE',\"eval_metric\":\"RMSE\",\"has_time\": True}\n\ncat_params_1 = {'depth': 6, 'learning_rate': 0.02562650627754423, 'l2_leaf_reg': 0.6658610403321601,\n                'grow_policy': 'SymmetricTree', 'bagging_temperature': 4.585387000263623,\"random_seed\": 42,\"iterations\": 5000,\n                \"bootstrap_type\":\"Bayesian\", \"loss_function\" : 'RMSE',\"eval_metric\":\"RMSE\",\"has_time\": True}","metadata":{"execution":{"iopub.status.busy":"2021-09-27T15:32:45.231272Z","iopub.execute_input":"2021-09-27T15:32:45.231503Z","iopub.status.idle":"2021-09-27T15:32:45.242587Z","shell.execute_reply.started":"2021-09-27T15:32:45.231472Z","shell.execute_reply":"2021-09-27T15:32:45.241692Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from catboost import CatBoostRegressor\n\nimport joblib\ndef train_and_evaluate_cat(train, test, params, boost=5000):\n    test = test.drop('time_id',axis=1)\n    features = [col for col in train.columns if col not in { \"target\", \"row_id\", \"time_id\" }]\n    #features = [col for col in train.columns if col not in {\"target\", \"row_id\"}]\n    X = train[features] \n    y = train['target']\n    # Create out of folds array\n    oof_predictions = np.zeros(train.shape[0])\n    \n    # Create test array to store predictions\n    test_predictions = np.zeros(test.shape[0])\n    \n    # Create a KFold object\n    num_folds = 5\n    # Group Fold\n    group_fold = GroupKFold( n_splits = num_folds )\n    \n    # Iterate through each fold\n    for fold, (trn_ind, val_ind) in enumerate(group_fold.split(X, y, groups = train['time_id'])):\n        print(f'Training fold {fold + 1}')\n        x_train, x_val = X.iloc[trn_ind], X.iloc[val_ind]\n        y_train, y_val = y.iloc[trn_ind], y.iloc[val_ind]\n        # Root mean squared percentage error weights\n        train_weights = 1 / np.square(y_train)\n        val_weights = 1 / np.square(y_val)\n        train_pool = Pool(x_train, y_train,weight=train_weights)\n        val_pool = Pool(x_val, y_val,weight=val_weights)\n        model = CatBoostRegressor(**params)\n        \n        model.fit(train_pool,use_best_model =True, eval_set = val_pool, \n                  verbose = 250,early_stopping_rounds=20,plot =True)\n        oof_predictions[val_ind] = model.predict(x_val[features])\n        \n        # Predict the test set\n        test_predictions += model.predict(test[features]) / num_folds\n    rmspe_score = rmspe(y, oof_predictions)\n    print(f'Our out of folds RMSPE is {rmspe_score}')\n    print(30*'-')\n    # Return test predictions\n    return test_predictions","metadata":{"execution":{"iopub.status.busy":"2021-09-27T08:29:34.77472Z","iopub.execute_input":"2021-09-27T08:29:34.775633Z","iopub.status.idle":"2021-09-27T08:29:34.789646Z","shell.execute_reply.started":"2021-09-27T08:29:34.775586Z","shell.execute_reply":"2021-09-27T08:29:34.788673Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"CAT_TRAIN = False\nif CAT_TRAIN:\n    predictions_cat_1 = train_and_evaluate_cat( df_training, df_testing, cat_params_1 )","metadata":{"execution":{"iopub.status.busy":"2021-09-27T15:49:34.159719Z","iopub.execute_input":"2021-09-27T15:49:34.160098Z","iopub.status.idle":"2021-09-27T15:49:34.164797Z","shell.execute_reply.started":"2021-09-27T15:49:34.160056Z","shell.execute_reply":"2021-09-27T15:49:34.164086Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# LGB Training ","metadata":{}},{"cell_type":"code","source":"new_params = {'learning_rate': 0.04617790260722144, 'lambda_l1': 0.7812638405539516, \n              'lambda_l2': 0.10048137762832532, 'num_leaves': 305, 'feature_fraction': 0.7620458604190743,\n              'bagging_fraction': 0.8666298803788923, 'bagging_freq': 2, 'min_data_in_leaf': 574, 'max_depth': 11,'seed': 1111,\n              'objective': 'rmse','boosting': 'gbdt','verbosity': -1,'n_jobs': -1,} # 0.2288\n\nnew_params_1 = {'learning_rate': 0.12100470091570181, 'lambda_l1': 0.9484882120442775, \n                'lambda_l2': 0.18592387603709012, 'num_leaves': 747, 'feature_fraction': 0.5464453484366312, \n                'bagging_fraction': 0.8852154214540782, 'bagging_freq': 2, 'min_data_in_leaf': 533, 'max_depth': 9,\n               'seed': 1111,'objective': 'rmse','boosting': 'gbdt','verbosity': -1,'n_jobs': -1} # 0.22913\n\nnew_params_2 = {'learning_rate': 0.0798333323013617, 'lambda_l1': 0.931510376585135, \n                'lambda_l2': 0.9836497055219714, 'num_leaves': 750, 'feature_fraction': 0.5975156297650424, \n                'bagging_fraction': 0.8211686012186546, 'bagging_freq': 1, 'min_data_in_leaf': 494, 'max_depth': 12,\n                 'seed': 1111,'objective': 'rmse','boosting': 'gbdt','verbosity': -1,'n_jobs': -1} # 0.2290\nnew_params_3 = {'learning_rate': 0.035210179568149945, 'lambda_l1': 0.2785515724790116, 'lambda_l2': 0.1344375389299167, \n                'num_leaves': 420, 'feature_fraction': 0.7424685714197569, \n                'bagging_fraction': 0.7501140603166803, 'bagging_freq': 1, 'min_data_in_leaf': 639, 'max_depth': 11,\n               'seed': 1111,'objective': 'rmse','boosting': 'gbdt','verbosity': -1,'n_jobs': -1} # 0.2287","metadata":{"execution":{"iopub.status.busy":"2021-09-27T15:32:45.244234Z","iopub.execute_input":"2021-09-27T15:32:45.244828Z","iopub.status.idle":"2021-09-27T15:32:45.262159Z","shell.execute_reply.started":"2021-09-27T15:32:45.244793Z","shell.execute_reply":"2021-09-27T15:32:45.261208Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import joblib\ndef train_and_evaluate_lgb(train, test, params, boost=5000):\n    test['stock_id'] = test['stock_id'].astype('category')\n    test = test.drop('time_id',axis=1)\n    features = [col for col in train.columns if col not in { \"target\", \"row_id\", \"time_id\" }]\n    X = train[features]\n    X['stock_id'] = X['stock_id'].astype('category')\n    y = train['target']\n    # Create out of folds array\n    oof_predictions = np.zeros(train.shape[0])\n    \n    # Create test array to store predictions\n    test_predictions = np.zeros(test.shape[0])\n    \n    # Create a KFold object\n    num_folds = 5\n    # Group Fold\n    group_fold = GroupKFold( n_splits = num_folds )\n    \n    # Iterate through each fold\n    for fold, (trn_ind, val_ind) in enumerate(group_fold.split(X, y, groups = train['time_id'])):\n        print(f'Training fold {fold + 1}')\n        x_train, x_val = X.iloc[trn_ind], X.iloc[val_ind]\n        y_train, y_val = y.iloc[trn_ind], y.iloc[val_ind]\n        # Root mean squared percentage error weights\n        train_weights = 1 / np.square(y_train)\n        val_weights = 1 / np.square(y_val)\n        train_dataset = lgb.Dataset(x_train, y_train, weight = train_weights)\n        val_dataset = lgb.Dataset(x_val, y_val, weight = val_weights)\n        model = lgb.train(params = params,\n                          num_boost_round=boost,\n                          train_set = train_dataset, \n                          valid_sets = [train_dataset, val_dataset], \n                          verbose_eval = 250,\n                          early_stopping_rounds=20,\n                          feval = feval_rmspe)\n        \n        # Add predictions to the out of folds array\n        oof_predictions[val_ind] = model.predict(x_val[features])\n        \n        # Predict the test set\n        test_predictions += model.predict(test[features]) / num_folds\n        \n        # Save model\n        joblib.dump(model, f'model_fold{fold}.pkl')\n        print(f'Model {fold + 1} Saved...')\n        \n    rmspe_score = rmspe(y, oof_predictions)\n    print(f'Our out of folds RMSPE is {rmspe_score}')\n    print(30*'-')\n    lgb.plot_importance(model,max_num_features=20)\n    \n    # Return test predictions\n    return test_predictions","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-09-27T15:49:56.96105Z","iopub.execute_input":"2021-09-27T15:49:56.961343Z","iopub.status.idle":"2021-09-27T15:49:56.977295Z","shell.execute_reply.started":"2021-09-27T15:49:56.961311Z","shell.execute_reply":"2021-09-27T15:49:56.976164Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"LGB_TRAIN = True\nif LGB_TRAIN:\n    predictions_lgb_1 = train_and_evaluate_lgb( df_training, df_testing, new_params )\n    predictions_lgb_2 = train_and_evaluate_lgb( df_training, df_testing, new_params_1 )\n    predictions_lgb_3 = train_and_evaluate_lgb( df_training, df_testing, new_params_2 )\n    predictions_lgb_4 = train_and_evaluate_lgb( df_training, df_testing, new_params_3 )","metadata":{"execution":{"iopub.status.busy":"2021-09-27T15:49:57.596624Z","iopub.execute_input":"2021-09-27T15:49:57.596909Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# LGBM and CatBoost Ensemble\ndf_testing['target'] = ((predictions_lgb_1 * 0.35) + (predictions_lgb_2 * 0.15) + (predictions_lgb_3 * 0.15) + (predictions_lgb_4 * 0.35))# * 0.95 + (predictions_cat_1) * 0.05\ndf_testing[['row_id', 'target']].to_csv('submission.csv',index = False)","metadata":{"execution":{"iopub.status.busy":"2021-09-27T15:44:37.458969Z","iopub.execute_input":"2021-09-27T15:44:37.459452Z","iopub.status.idle":"2021-09-27T15:44:37.47877Z","shell.execute_reply.started":"2021-09-27T15:44:37.459407Z","shell.execute_reply":"2021-09-27T15:44:37.477682Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Submission CSV","metadata":{}},{"cell_type":"code","source":"pd.read_csv(\"submission.csv\")","metadata":{"execution":{"iopub.status.busy":"2021-09-27T15:44:40.008267Z","iopub.execute_input":"2021-09-27T15:44:40.009154Z","iopub.status.idle":"2021-09-27T15:44:40.029154Z","shell.execute_reply.started":"2021-09-27T15:44:40.009108Z","shell.execute_reply":"2021-09-27T15:44:40.027994Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}