{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### Try to generate as much features as possible and leverage the best ones.","metadata":{}},{"cell_type":"markdown","source":"#### Helpers","metadata":{}},{"cell_type":"code","source":"import os\nimport glob\nfrom functools import reduce\nfrom multiprocessing import Pool\n\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-07-25T13:39:20.446798Z","iopub.execute_input":"2021-07-25T13:39:20.447278Z","iopub.status.idle":"2021-07-25T13:39:20.459513Z","shell.execute_reply.started":"2021-07-25T13:39:20.447197Z","shell.execute_reply":"2021-07-25T13:39:20.458314Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"N_CPUS = 4","metadata":{"execution":{"iopub.status.busy":"2021-07-25T13:39:20.465578Z","iopub.execute_input":"2021-07-25T13:39:20.466235Z","iopub.status.idle":"2021-07-25T13:39:20.475581Z","shell.execute_reply.started":"2021-07-25T13:39:20.466188Z","shell.execute_reply":"2021-07-25T13:39:20.474388Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_wap(bid_price, ask_price, bid_size, ask_size):\n    return (bid_price * ask_size + ask_price * bid_size) / (\n        bid_size + ask_size\n    )\n\n\ndef log_return(list_stock_prices):\n    return np.log(list_stock_prices).diff()\n\n\ndef realized_volatility(series_log_return):\n    return np.sqrt(np.sum(series_log_return ** 2))\n\n\ndef get_realized_vol(book: pd.DataFrame):\n    \"\"\"Mutable method to get past realized volatility\"\"\"\n\n    book[\"wap1\"] = get_wap(\n        book.bid_price1, book.ask_price1, book.bid_size1, book.ask_size1\n    )\n\n    book[\"wap2\"] = get_wap(\n        book.bid_price2, book.ask_price2, book.bid_size2, book.ask_size2\n    )\n\n    a1 = (\n        book['bid_price1'] * book['ask_size1'] +\n        book['ask_price1'] * book['bid_size1']\n    )\n    a2 = (\n        book['bid_price2'] * book['ask_size2'] +\n        book['ask_price2'] * book['bid_size2']\n    )\n    b = (\n        book['bid_size1'] + book['ask_size1'] +\n        book['bid_size2']+ book['ask_size2']\n    )\n    book['wap3'] = (a1 + a2)/ b\n\n    book['wap4'] = (book['wap1'] + book['wap2']) / 2\n\n    book.loc[:, \"log_return1\"] = log_return(book[\"wap1\"])\n    book.loc[:, \"log_return2\"] = log_return(book[\"wap2\"])\n    book.loc[:, \"log_return3\"] = log_return(book[\"wap3\"])\n    book.loc[:, \"log_return4\"] = log_return(book[\"wap4\"])\n\n    # book = book[~book['log_return'].isnull()]\n    # to not remove rows both on train and test set\n    book = book.fillna(0)\n\n    book = book.merge(\n        book[[\"time_id\", \"log_return1\"]]\n        .rename({\"log_return1\": \"vol1\"}, axis=1)\n        .groupby(\"time_id\")\n        .agg(realized_volatility),\n        how=\"inner\",\n        on=\"time_id\",\n    ).merge(\n        book[[\"time_id\", \"log_return2\"]]\n        .rename({\"log_return2\": \"vol2\"}, axis=1)\n        .groupby(\"time_id\")\n        .agg(realized_volatility),\n        how=\"inner\",\n        on=\"time_id\",\n    ).merge(\n        book[[\"time_id\", \"log_return3\"]]\n        .rename({\"log_return3\": \"vol3\"}, axis=1)\n        .groupby(\"time_id\")\n        .agg(realized_volatility),\n        how=\"inner\",\n        on=\"time_id\",\n    ).merge(\n        book[[\"time_id\", \"log_return4\"]]\n        .rename({\"log_return4\": \"vol4\"}, axis=1)\n        .groupby(\"time_id\")\n        .agg(realized_volatility),\n        how=\"inner\",\n        on=\"time_id\",\n    )\n\n    book['wap_diff12'] = book['wap2'] - book['wap1']\n    book['wap_diff13'] = book['wap3'] - book['wap1']\n    book['wap_diff14'] = book['wap4'] - book['wap1']\n\n    book['vol_rate'] = (book['vol1'] / book['vol2']).fillna(0)\n    book['vol_diff'] = (book['vol1'] - book['vol2'])\n\n    return book\n\n\n# Price features\ndef get_stats(df, ind:str, column:str):\n    \"\"\"Get aggregated features from the column provided\"\"\"\n    stats = pd.merge(\n        df[[ind, column]].groupby(ind).mean()\n        .rename({column: f'{column}_mean'}, axis=1),\n        df[[ind, column]].groupby(ind).median()\n        .rename({column: f'{column}_median'}, axis=1),\n        how='inner', left_index=True, right_index=True\n    ).merge(\n        df[[ind, column]].groupby(ind).count()\n        .rename({column: f'{column}_count'}, axis=1),\n        how='inner', left_index=True, right_index=True\n    ).merge(\n        df[[ind, column]].groupby(ind).min()\n        .rename({column: f'{column}_min'}, axis=1),\n        how='inner', left_index=True, right_index=True\n    ).merge(\n        df[[ind, column]].groupby(ind).max()\n        .rename({column: f'{column}_max'}, axis=1),\n        how='inner', left_index=True, right_index=True\n    ).merge(\n        df[[ind, column]].groupby(ind).std()\n        .rename({column: f'{column}_std'}, axis=1),\n        how='inner', left_index=True, right_index=True\n    ).merge(\n        df[[ind, column]].groupby(ind).quantile(0.25)\n        .rename({column: f'{column}_q25'}, axis=1),\n        how='inner', left_index=True, right_index=True\n    ).merge(\n        df[[ind, column]].groupby(ind).quantile(0.75)\n        .rename({column: f'{column}_q75'}, axis=1),\n        how='inner', left_index=True, right_index=True\n    ).merge(\n        df[[ind, column]].groupby(ind).nunique()\n        .rename({column: f'{column}_unique'}, axis=1),\n        how='inner', left_index=True, right_index=True\n    ).merge(\n        df[[ind, column]].groupby(ind).mad()\n        .rename({column: f'{column}_mad'}, axis=1),\n        how='inner', left_index=True, right_index=True\n    ).merge(\n        df[[ind, column]].groupby(ind).first()\n        .rename({column: f'{column}_first'}, axis=1),\n        how='inner', left_index=True, right_index=True\n    ).merge(\n        df[[ind, column]].groupby(ind).last()\n        .rename({column: f'{column}_last'}, axis=1),\n        how='inner', left_index=True, right_index=True\n    )\n\n    stats[f'{column}_delta'] = (\n        stats[f'{column}_last'] - stats[f'{column}_first']\n    )\n    stats[f'{column}_delta_abs'] = (\n        stats[f'{column}_last'] - stats[f'{column}_first']\n    ).abs()\n    stats[f'{column}_unique_pct'] = (\n        stats[f'{column}_unique'] / stats[f'{column}_count']\n    )\n\n    del stats[f'{column}_count']\n    del stats[f'{column}_unique']\n    del stats[f'{column}_first']\n    del stats[f'{column}_last']\n\n    return stats\n\n\ndef get_book_features(files):\n    pieces = []\n\n    for f in tqdm(files):\n        book = pd.read_parquet(f)\n\n        book = get_realized_vol(book)\n\n        # price\n        book['spread1'] = book['ask_price1'] - book['bid_price1']\n        book['spread2'] = book['ask_price2'] - book['bid_price2']\n        book['ask_spread'] = book['ask_price2'] - book['ask_price1']\n        book['bid_spread'] = book['bid_price1'] - book['bid_price2']\n        book['cross_spread1'] = book['ask_price1'] - book['bid_price1']\n        book['cross_spread2'] = book['ask_price2'] - book['bid_price2']\n        book['bas'] = (\n            book[['ask_price1', 'ask_price2']].min(axis = 1) /\n            book[['bid_price1', 'bid_price2']].max(axis = 1) -\n            1\n        )\n\n        # size\n        book['skew1'] = book['ask_size1'] - book['bid_size1']\n        book['skew2'] = book['ask_size2'] - book['bid_size2']\n        book['cross_skew1'] = book['ask_size1'] - book['bid_size2']\n        book['cross_skew2'] = book['ask_size2'] - book['bid_size1']\n        book['ask_sum'] = book['ask_size1'] - book['ask_size2']\n        book['bid_sum'] = book['bid_size1'] - book['bid_size2']\n        book['skew_whole'] = book['ask_sum'] - book['bid_sum']\n\n        # price - size combinations\n        book['bid_volume1'] = book['bid_price1'] * book['bid_size1']\n        book['bid_volume2'] = book['bid_price2'] * book['bid_size2']\n        book['ask_volume1'] = book['ask_price1'] * book['ask_size1']\n        book['ask_volume2'] = book['ask_price2'] * book['ask_size2']\n\n        # sum of volumes\n        book_sums = (\n            book[['time_id',\n                'bid_volume1', 'bid_volume2',\n                'ask_volume1','ask_volume2']]\n            .groupby('time_id').sum()\n            .rename({\n                'bid_volume1': 'bid_volume1_sum',\n                'bid_volume2': 'bid_volume2_sum',\n                'ask_volume1': 'ask_volume1_sum',\n                'ask_volume2': 'ask_volume2_sum'\n            }, axis=1)\n        )\n\n        features_to_get_stats = [\n            'seconds_in_bucket',\n            'wap1', 'wap2', 'wap3', 'wap4',\n            'log_return1', 'log_return2', 'log_return3', 'log_return4',\n            'vol1', 'vol2', 'vol3', 'vol4', 'vol_rate', 'vol_diff',\n            'wap_diff12', 'wap_diff13', 'wap_diff14',\n            'bid_price1', 'bid_price2', 'ask_price1', 'ask_price2',\n            'spread1', 'spread2', 'ask_spread', 'bid_spread', 'cross_spread1', 'cross_spread2', 'bas',\n            'bid_size1', 'bid_size2', 'ask_size1', 'ask_size2',\n            'skew1', 'skew2', 'cross_skew1', 'cross_skew2', 'ask_sum', 'bid_sum', 'skew_whole',\n            'bid_volume1', 'bid_volume2', 'ask_volume1', 'ask_volume2',\n        ]\n\n        pool = Pool(N_CPUS)\n        stats_df = pool.starmap(\n            get_stats,\n            zip(\n                [book] * len(features_to_get_stats),\n                ['time_id'] * len(features_to_get_stats),\n                features_to_get_stats\n            )\n        )\n        pool.close() \n        pool.join()\n\n        # do not merge with not aggregated book\n        dfs = [\n            book_sums,\n            *stats_df\n        ]\n\n        df_stats = reduce(\n            lambda left, right: pd.merge(\n                left, right,\n                how='inner', left_index=True, right_index=True\n            ),\n            dfs\n        )\n\n        df_stats[\"stock_id\"] = int(f.split(\"=\")[-1])\n\n        pieces.append(df_stats)\n\n    dataset_new = pd.concat(pieces).reset_index()\n    \n    features = list(dataset_new.keys())\n\n    dataset_new[\"row_id\"] = [\n        f\"{stock_id}-{time_id}\"\n        for stock_id, time_id in zip(\n            dataset_new[\"stock_id\"], dataset_new[\"time_id\"]\n        )\n    ]\n\n    \n    return dataset_new, features\n\n\ndef get_trade_features(files):\n    \"\"\"Getting features from trading history\"\"\"\n\n    pieces = []\n\n    for f in tqdm(files):\n        trades = pd.read_parquet(f)\n\n        trades['trade_volume'] = trades['price'] * trades['size']\n        trades['trade_size_per_order'] = (\n            trades['size'] / trades['order_count']\n        )\n        trades['trade_volume_per_order_mean'] = (\n            trades['trade_volume'] / trades['trade_size_per_order']\n        )\n\n        trades = trades.rename(\n            {\n                'price': 'trade_price',\n                'order_count': 'trade_order_count',\n                'seconds_in_bucket': 'trade_seconds_in_bucket'\n            },\n            axis=1\n        )\n        \n        # sum of volumes, orders and sizes\n        trades_sums = (\n            trades[['time_id',\n                'size', 'trade_order_count', 'trade_volume'\n            ]]\n            .groupby('time_id').sum()\n            .rename({\n                'size': 'size_sum',\n                'trade_order_count': 'trade_order_count_sum',\n                'trade_volume': 'trade_volume_sum'\n            }, axis=1)\n        )\n\n        # volatility of trades\n        trades.loc[:, \"trade_log_return\"] = log_return(trades[\"trade_price\"])\n\n        trades_vol = (\n            trades[[\"time_id\", \"trade_log_return\"]]\n            .rename({\"trade_log_return\": \"trade_vol\"}, axis=1)\n            .groupby(\"time_id\")\n            .agg(realized_volatility)\n        )\n\n        features_to_get_stats = [\n            'trade_seconds_in_bucket',  # where operations are located in the bucket\n            'trade_price', 'size', 'trade_order_count',\n            'trade_volume', 'trade_size_per_order', 'trade_volume_per_order_mean',\n            'trade_log_return'\n        ]\n\n        pool = Pool(N_CPUS)\n        stats_df = pool.starmap(\n            get_stats,\n            zip([trades] * len(features_to_get_stats), ['time_id'] * len(features_to_get_stats), features_to_get_stats)\n        )\n        pool.close() \n        pool.join()\n\n        # do not merge with not aggregated trades\n        dfs = [\n            trades_sums,\n            trades_vol,\n            *stats_df\n        ]\n\n        dfs.append(\n            trades[['time_id', 'trade_seconds_in_bucket']]\n            .groupby('time_id').count()\n            .rename({'trade_seconds_in_bucket': 'n_trades'}, axis=1)\n        )\n\n        df_stats = reduce(\n            lambda left, right: pd.merge(\n                left, right, how='inner', left_index=True, right_index=True\n            ),\n            dfs\n        )\n\n\n        df_stats[\"stock_id\"] = int(f.split(\"=\")[-1])\n        pieces.append(df_stats)\n\n    dataset_new = pd.concat(pieces).reset_index()\n    \n    features = list(dataset_new.keys())\n\n    return dataset_new, features\n\n\ndef mean_encoding(\n    dataset: pd.DataFrame,\n    means: dict = None,\n    stds: dict = None,\n    medians: dict = None\n):\n    \"\"\"Dataset with stock_id and target columns\"\"\"\n    if means is None:\n        means = (\n            dataset[[\"stock_id\", \"target\"]]\n            .groupby(\"stock_id\").mean()\n        )\n        means = means['target'].to_dict()\n\n        stds = (\n            dataset[[\"stock_id\", \"target\"]]\n            .groupby(\"stock_id\").std()\n        )\n        stds = stds['target'].to_dict()\n\n        medians = (\n            dataset[[\"stock_id\", \"target\"]]\n            .groupby(\"stock_id\").median()\n        )\n        medians = medians['target'].to_dict()\n\n    dataset[\"stock_id_mean\"] = dataset[\"stock_id\"].apply(lambda x: means[x])\n    dataset[\"stock_id_std\"] = dataset[\"stock_id\"].apply(lambda x: stds[x])\n    dataset[\"stock_id_median\"] = dataset[\"stock_id\"].apply(lambda x: medians[x])\n    del dataset['stock_id']\n\n    return dataset, (means, stds, medians)","metadata":{"execution":{"iopub.status.busy":"2021-07-25T13:39:20.477333Z","iopub.execute_input":"2021-07-25T13:39:20.477639Z","iopub.status.idle":"2021-07-25T13:39:20.635877Z","shell.execute_reply.started":"2021-07-25T13:39:20.477609Z","shell.execute_reply":"2021-07-25T13:39:20.63462Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Dataset building","metadata":{}},{"cell_type":"code","source":"# target\ndataset = pd.read_csv(\"../input/optiver-realized-volatility-prediction/train.csv\")","metadata":{"execution":{"iopub.status.busy":"2021-07-25T13:39:20.637709Z","iopub.execute_input":"2021-07-25T13:39:20.638186Z","iopub.status.idle":"2021-07-25T13:39:20.806751Z","shell.execute_reply.started":"2021-07-25T13:39:20.638139Z","shell.execute_reply":"2021-07-25T13:39:20.805813Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# book data\nfiles = glob.glob(\n    \"../input/optiver-realized-volatility-prediction/book_train.parquet/*\"\n)\nbooks, features_book = get_book_features(files)\n\ndataset_new = pd.merge(\n    books,\n    dataset[[\"time_id\", \"stock_id\", \"target\"]],\n    how=\"inner\",\n    on=[\"time_id\", \"stock_id\"],\n)","metadata":{"execution":{"iopub.status.busy":"2021-07-25T13:39:20.808175Z","iopub.execute_input":"2021-07-25T13:39:20.808613Z","iopub.status.idle":"2021-07-25T13:40:31.203471Z","shell.execute_reply.started":"2021-07-25T13:39:20.808569Z","shell.execute_reply":"2021-07-25T13:40:31.198973Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# trade data\nfiles_trade = glob.glob(\n    \"../input/optiver-realized-volatility-prediction/trade_train.parquet/*\"\n)\ntrade_stats, features_trade = get_trade_features(files_trade)\n\n# merging dataset\ndataset_new = pd.merge(\n    dataset_new,\n    trade_stats,\n    how='inner', on=['time_id', 'stock_id'],\n    left_index=False, right_index=False\n)","metadata":{"execution":{"iopub.status.busy":"2021-07-25T13:40:31.20529Z","iopub.status.idle":"2021-07-25T13:40:31.20598Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"Number of features genereated: {dataset_new.shape[1] - 2}\")","metadata":{"execution":{"iopub.status.busy":"2021-07-25T13:40:31.207378Z","iopub.status.idle":"2021-07-25T13:40:31.208068Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset_new.to_csv('dataset_new.csv', index=False, header=True)","metadata":{"execution":{"iopub.status.busy":"2021-07-25T13:40:31.209324Z","iopub.status.idle":"2021-07-25T13:40:31.209993Z"},"trusted":true},"execution_count":null,"outputs":[]}]}