{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Models:\n### - 501: MLP / 1dCNN / TabNet / UNet / WaveNet\n### - 601: LGB / Catboost / DeepForest(CFR) / MLP / 1dCNN / TabNet","metadata":{}},{"cell_type":"code","source":"# install DeepForest\n\n!mkdir -p /tmp/pip/cache/\n\nimport os\nfrom shutil import copyfile\nfrom tqdm.notebook import tqdm\n\nsrc = '../input/deep-forest-files/'\ndst = '/tmp/pip/cache/'\nfor filename in tqdm(os.listdir(src)):\n    if '.xyz' in filename:\n        f = filename.split('.xyz')[0]\n        copyfile(src + filename, dst + f + '.tar.gz')\n    else:\n        copyfile(src + filename, dst + filename)\n        \n!pip install --no-index --find-links /tmp/pip/cache/ deep-forest\n\nfrom deepforest import CascadeForestRegressor as CFR\n\n# install TabNet\n!pip -q install ../input/pytorchtabnet/pytorch_tabnet-3.1.1-py3-none-any.whl","metadata":{"execution":{"iopub.status.busy":"2021-09-25T21:19:29.290446Z","iopub.execute_input":"2021-09-25T21:19:29.290836Z","iopub.status.idle":"2021-09-25T21:20:08.404663Z","shell.execute_reply.started":"2021-09-25T21:19:29.290801Z","shell.execute_reply":"2021-09-25T21:20:08.403139Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np \nimport numpy.matlib\nimport datatable as dt\nimport pandas as pd\nimport glob\nimport gc\nimport pickle\nfrom collections import defaultdict\nfrom tqdm.auto import tqdm\nfrom numba import njit\nfrom joblib import Parallel, delayed, dump, load\n\nfrom sklearn.preprocessing import StandardScaler, PowerTransformer\nfrom sklearn.preprocessing import MinMaxScaler, QuantileTransformer, LabelEncoder\n\n\nfrom numba_functions import *\n\n# TF\nimport tensorflow as tf\nimport tensorflow.keras.backend as K\nfrom tensorflow.keras.models import Model\n\n# Torch and TabNet\nfrom pytorch_tabnet.metrics import Metric\nfrom pytorch_tabnet.tab_model import TabNetRegressor\n\nimport torch\nfrom torch.optim import Adam, SGD\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau, CosineAnnealingWarmRestarts","metadata":{"execution":{"iopub.status.busy":"2021-09-25T21:20:08.409272Z","iopub.execute_input":"2021-09-25T21:20:08.409616Z","iopub.status.idle":"2021-09-25T21:20:16.09693Z","shell.execute_reply.started":"2021-09-25T21:20:08.409584Z","shell.execute_reply":"2021-09-25T21:20:16.095842Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"N_MINS = 5\nMIN_SIZE = 600 // N_MINS\nDEBUG = 0\nDEBUG_STOCK_ID = 31\n\n# CONSTANT\nMEAN = -5.762330803300896\nSTD = 0.6339307835941186\nEPS = 1e-9\n\n# path\npreprocessor_path = '../input/optiver-final-preprocessors'","metadata":{"execution":{"iopub.status.busy":"2021-09-25T21:20:16.101193Z","iopub.execute_input":"2021-09-25T21:20:16.101515Z","iopub.status.idle":"2021-09-25T21:20:16.106036Z","shell.execute_reply.started":"2021-09-25T21:20:16.101485Z","shell.execute_reply":"2021-09-25T21:20:16.105245Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if DEBUG:\n    df_result = pd.read_csv('../input/optiver-realized-volatility-prediction/train.csv')\n    df_result['row_id'] = [f'{x[0]}-{x[1]}' for x in df_result[['stock_id', 'time_id']].values]\nelse:\n    df_result = pd.read_csv('../input/optiver-realized-volatility-prediction/test.csv')\ndf_result.head()","metadata":{"execution":{"iopub.status.busy":"2021-09-25T21:20:16.107176Z","iopub.execute_input":"2021-09-25T21:20:16.10744Z","iopub.status.idle":"2021-09-25T21:20:16.154305Z","shell.execute_reply.started":"2021-09-25T21:20:16.107414Z","shell.execute_reply":"2021-09-25T21:20:16.153101Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if DEBUG:\n    list_train_book_data = glob.glob('../input/optiver-realized-volatility-prediction/book_train.parquet/*')\n    list_stock_id = sorted([int(path.split('=')[1]) for path in list_train_book_data])\n#     list_stock_id = [0, 31]\nelse:\n    list_test_book_data = glob.glob('../input/optiver-realized-volatility-prediction/book_test.parquet/*')\n    list_stock_id = sorted([int(path.split('=')[1]) for path in list_test_book_data])","metadata":{"execution":{"iopub.status.busy":"2021-09-25T21:20:16.155608Z","iopub.execute_input":"2021-09-25T21:20:16.155946Z","iopub.status.idle":"2021-09-25T21:20:16.165403Z","shell.execute_reply.started":"2021-09-25T21:20:16.155914Z","shell.execute_reply":"2021-09-25T21:20:16.164419Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Functions","metadata":{"_kg_hide-input":true}},{"cell_type":"code","source":"def calc_log_return(prices):\n    return np.log(prices).diff()\n\n\ndef transform_target(target):\n    return (np.log(target + EPS) - MEAN) / STD\n\n\ndef inverse_target(target):\n    return np.exp(MEAN + STD * target) - EPS\n\n\ndef rmspe(y_true, y_pred):\n    return np.sqrt(np.mean(np.square((y_true - y_pred) / y_true)))\n\n\ndef mspe_loss(y_true, y_pred):\n    y_true = K.exp(MEAN + STD * y_true) - EPS\n    y_pred = K.exp(MEAN + STD * y_pred) - EPS\n    return K.sqrt(K.mean(K.square((y_true - y_pred) / y_true)))\n\n\ndef feval_rmspe(y_pred, lgb_train):\n    y_true = lgb_train.get_label()\n    return 'RMSPE', rmspe(y_true, y_pred), False","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-09-25T21:20:16.16697Z","iopub.execute_input":"2021-09-25T21:20:16.167346Z","iopub.status.idle":"2021-09-25T21:20:16.179406Z","shell.execute_reply.started":"2021-09-25T21:20:16.167316Z","shell.execute_reply":"2021-09-25T21:20:16.178285Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def load_pickle(path):\n    with open(path, 'rb') as f:\n        data = pickle.load(f)\n    return data\n\n\ndef gen_row_id(df):\n    df['row_id'] = [f'{x[0]}-{x[1]}' for x in df[['stock_id', 'time_id']].values]\n    return df\n\n\ndef get_path_by_id(data_type, stock_id):\n    if DEBUG:\n        return f'../input/optiver-realized-volatility-prediction/{data_type}_train.parquet/stock_id={stock_id}'\n    return f'../input/optiver-realized-volatility-prediction/{data_type}_test.parquet/stock_id={stock_id}'","metadata":{"execution":{"iopub.status.busy":"2021-09-25T21:20:16.180814Z","iopub.execute_input":"2021-09-25T21:20:16.181305Z","iopub.status.idle":"2021-09-25T21:20:16.191308Z","shell.execute_reply.started":"2021-09-25T21:20:16.181252Z","shell.execute_reply":"2021-09-25T21:20:16.190356Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 501","metadata":{}},{"cell_type":"markdown","source":"## Feature Generation","metadata":{}},{"cell_type":"code","source":"def unstack_agg(df, agg_col):\n    df = df.unstack(level=1)\n    df.columns = [f'{agg_col}_{k}' for k in df.columns]\n    return df.reset_index()\n\ndef init_feature_df(df_book, stock_id):\n    df_feature = pd.DataFrame(df_book['time_id'].unique())\n    df_feature['stock_id'] = stock_id\n    df_feature.columns = ['time_id', 'stock_id']\n    return df_feature[['stock_id', 'time_id']]\n\ndef add_stats(df, cols, data_name, suffix='', axis=0):\n    unwrap = lambda x: x.item() if len(cols) == 1 else x\n    df[f'{data_name}_{suffix}_mean'] = unwrap(df[cols].mean(axis=axis).values)\n    df[f'{data_name}_{suffix}_std'] = unwrap(df[cols].std(axis=axis).values)\n    df[f'{data_name}_{suffix}_skew'] = unwrap(df[cols].skew(axis=axis).values)\n    df[f'{data_name}_{suffix}_min'] = unwrap(df[cols].min(axis=axis).values)\n    df[f'{data_name}_{suffix}_q1'] = unwrap(df[cols].quantile(q=0.25, axis=axis).values)\n    df[f'{data_name}_{suffix}_q2'] = unwrap(df[cols].quantile(q=0.50, axis=axis).values)\n    df[f'{data_name}_{suffix}_q3'] = unwrap(df[cols].quantile(q=0.75, axis=axis).values)\n    df[f'{data_name}_{suffix}_max'] = unwrap(df[cols].max(axis=axis).values)\n    return df\n\ndef add_feature_min(df_feature, df, configs):\n    df['min_id'] = df['seconds_in_bucket'] // MIN_SIZE\n    df_gb_min = df.groupby(['time_id', 'min_id'])\n    for data_col, agg_func, agg_col in configs:\n        # agg by min\n        df_ = df_gb_min[data_col].agg(agg_func, engine='numba')\n        df_ = unstack_agg(df_, agg_col)\n        df_feature = df_feature.merge(df_, on=['time_id'], how='left')\n        # gen stats by min and by time\n        cols = [f'{agg_col}_{k}' for k in range(N_MINS)]\n        for c in cols:\n            if c not in df_feature:\n                df_feature[c] = 0\n        df_feature = add_stats(df_feature, cols=cols, data_name=agg_col, suffix='min', axis=1)\n    return df_feature.fillna(0.0)\n\ndef add_feature_time(df_feature, df, configs):\n    df_gb_time = df.groupby(['time_id'])\n    for data_col, agg_func, agg_col in configs:\n        # agg by time\n        df_ = df_gb_time[data_col].agg(agg_func, engine='numba')\n        df_.name = f'{agg_col}_time'\n        df_feature = df_feature.merge(df_, on=['time_id'], how='left')\n    return df_feature.fillna(0.0)\n\ndef ffill_book(df_book):\n    list_time_id_book = df_book.time_id.unique()\n    df_ = pd.DataFrame()\n    df_['time_id'] = np.matlib.repeat(list_time_id_book, 600)\n    df_['seconds_in_bucket'] = np.matlib.repmat(range(600), 1, len(list_time_id_book)).ravel()\n    df_book = df_.merge(df_book, on=['time_id', 'seconds_in_bucket'], how='left')\n    df_book = df_book.set_index('time_id').groupby(level='time_id').ffill().bfill().reset_index() \n    return df_book","metadata":{"execution":{"iopub.status.busy":"2021-09-25T21:20:16.19418Z","iopub.execute_input":"2021-09-25T21:20:16.194595Z","iopub.status.idle":"2021-09-25T21:20:16.217556Z","shell.execute_reply.started":"2021-09-25T21:20:16.194556Z","shell.execute_reply":"2021-09-25T21:20:16.216481Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Preprocessing - LGB","metadata":{}},{"cell_type":"code","source":"book_configs = [\n    ('log_return1', rv_numba, 'B_RV1'),\n    ('log_return2', rv_numba, 'B_RV2'),\n    ('seconds_in_bucket', count_numba, 'B_NROW'),\n    ('bid_vol1', sum_numba, 'B_BVOL1'),\n    ('bid_vol2', sum_numba, 'B_BVOL2'),\n    ('ask_vol1', sum_numba, 'B_AVOL1'),\n    ('ask_vol2', sum_numba, 'B_AVOL2'),\n]\n\nbook_configs_ffill = [\n    ('bid_price1', mean_numba, 'B_BP1'),\n    ('bid_price2', mean_numba, 'B_BP2'),\n    ('ask_price1', mean_numba, 'B_AP1'),\n    ('ask_price2', mean_numba, 'B_AP2'),\n    ('bid_size1', mean_numba, 'B_BS1'),\n    ('bid_size2', mean_numba, 'B_BS2'),\n    ('ask_size1', mean_numba, 'B_AS1'),\n    ('ask_size2', mean_numba, 'B_AS2'),\n    # new features\n    ('price1_diff', mean_numba, 'Z_P1-DIFF'),\n    ('price2_diff', mean_numba, 'Z_P2-DIFF'),\n    ('price1_dabs', mean_numba, 'Z_P1-DABS'),\n    ('price2_dabs', mean_numba, 'Z_P2-DABS'),\n    ('price_spread1', mean_numba, 'Z_SPREAD1'),\n]\n\ntrade_configs = [\n    ('vol', sum_numba, 'T_VOL'),\n    ('order_count', sum_numba, 'T_OC'),\n    ('size', sum_numba, 'T_SIZE'),\n    ('seconds_in_bucket', count_numba, 'T_NROW'),\n]","metadata":{"execution":{"iopub.status.busy":"2021-09-25T21:20:16.219427Z","iopub.execute_input":"2021-09-25T21:20:16.219869Z","iopub.status.idle":"2021-09-25T21:20:16.230468Z","shell.execute_reply.started":"2021-09-25T21:20:16.219836Z","shell.execute_reply":"2021-09-25T21:20:16.229575Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def gen_df_feature(stock_id):\n    # -----------------------------------------------------------------\n    # Book data (no ffill)\n    book_parquet_path = get_path_by_id('book', stock_id)\n    df_book = pd.read_parquet(book_parquet_path)\n    df_book.iloc[:, 2:] = df_book.iloc[:, 2:].astype('float64')\n    df_book_ff = df_book.copy()\n    df_feature = init_feature_df(df_book, stock_id)\n    # add wap and log_return\n    df_book['wap1'] = calc_wap_njit(\n        df_book.bid_price1.values,\n        df_book.ask_price1.values,\n        df_book.bid_size1.values,\n        df_book.ask_size1.values\n    )\n    df_book['wap2'] = calc_wap_njit(\n        df_book.bid_price2.values,\n        df_book.ask_price2.values,\n        df_book.bid_size1.values + df_book.bid_size2.values,\n        df_book.ask_size1.values + df_book.ask_size2.values\n    )\n    df_book['log_return1'] = df_book.groupby(['time_id'])['wap1'].apply(calc_log_return).fillna(0)\n    df_book['log_return2'] = df_book.groupby(['time_id'])['wap2'].apply(calc_log_return).fillna(0)\n    # add vols\n    df_book['bid_vol1'] = prod_njit(df_book['bid_price1'].values, df_book['bid_size1'].values)\n    df_book['bid_vol2'] = prod_njit(df_book['bid_price2'].values, df_book['bid_size2'].values)\n    df_book['ask_vol1'] = prod_njit(df_book['ask_price1'].values, df_book['ask_size1'].values)\n    df_book['ask_vol2'] = prod_njit(df_book['ask_price2'].values, df_book['ask_size2'].values)\n    # generate book features\n    df_feature = add_feature_min(df_feature, df_book, book_configs)\n    df_feature = add_feature_time(df_feature, df_book, book_configs)\n\n\n    # -----------------------------------------------------------------\n    # Book data (ffill) \n    df_book_ff = ffill_book(df_book_ff)\n    # new features\n    df_book_ff['price1_diff'] = df_book_ff['ask_price1'] - df_book_ff['bid_price1']\n    df_book_ff['price2_diff'] = df_book_ff['ask_price2'] - df_book_ff['bid_price2']\n    df_book_ff['price1_dabs'] = df_book_ff['price1_diff'].abs()\n    df_book_ff['price2_dabs'] = df_book_ff['price2_diff'].abs()\n    df_book_ff['price_spread1'] = (df_book_ff['ask_price1'] - df_book_ff['bid_price1']) / (df_book_ff['ask_price1'] + df_book_ff['bid_price1'])\n    # generate book features\n    df_feature = add_feature_min(df_feature, df_book_ff, book_configs_ffill)\n    df_feature = add_feature_time(df_feature, df_book_ff, book_configs_ffill)\n    \n\n    # -----------------------------------------------------------------\n    # Trade data\n    trade_parquet_path = get_path_by_id('trade', stock_id)\n    df_trade = pd.read_parquet(trade_parquet_path)\n    df_trade.iloc[:, 2:] = df_trade.iloc[:, 2:].astype('float64')\n    # add vol\n    df_trade['vol'] = prod_njit(df_trade['price'].values, df_trade['size'].values)\n    # generate trade features\n    df_feature = add_feature_min(df_feature, df_trade, trade_configs)\n    df_feature = add_feature_time(df_feature, df_trade, trade_configs)\n\n\n    # -----------------------------------------------------------------\n    # Combined feature\n    log_return = df_trade.merge(df_book, on=['time_id', 'seconds_in_bucket'], how='left').groupby('time_id')['log_return1'].agg(lambda x: np.sum(np.square(x)))\n    total_log_return = df_book.groupby('time_id')['log_return1'].agg(lambda x: np.sum(np.square(x)))\n    df_feature['Z_RATIO'] = (log_return / total_log_return).values\n    df_feature['Z_RATIO'] = df_feature['Z_RATIO'].fillna(0.0)\n    return df_feature","metadata":{"execution":{"iopub.status.busy":"2021-09-25T21:20:16.231642Z","iopub.execute_input":"2021-09-25T21:20:16.23213Z","iopub.status.idle":"2021-09-25T21:20:16.250451Z","shell.execute_reply.started":"2021-09-25T21:20:16.232085Z","shell.execute_reply":"2021-09-25T21:20:16.249648Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"list_dfs = Parallel(n_jobs=-1)(delayed(gen_df_feature)(stock_id) for stock_id in tqdm(list_stock_id))\ndf_train = pd.concat(list_dfs).reset_index(drop=True)\ndf_train = df_train.sort_values(['stock_id', 'time_id']).reset_index(drop=True)\ndf_train_nn = df_train.copy() # prepare for NN\n\nfea_cols = [c for c in df_train.columns if c.startswith('B_') or c.startswith('T_') or c.startswith('Z_')]\nfea_cols_TA = [f for f in fea_cols if 'min_' not in f]\ndf_time_mean = df_train.groupby('time_id')[fea_cols_TA].mean()\ndf_time_mean.columns = [f'{c}_TA_mean' for c in df_time_mean.columns]\ndf_time_mean = df_time_mean.reset_index()\ndf_train = df_train.merge(df_time_mean, on='time_id', how='left')\n\n# Save data for LGB\n# dt.Frame(df_train).to_csv('test_501_LGB.csv')","metadata":{"execution":{"iopub.status.busy":"2021-09-25T21:20:16.251457Z","iopub.execute_input":"2021-09-25T21:20:16.251862Z","iopub.status.idle":"2021-09-25T21:20:21.83243Z","shell.execute_reply.started":"2021-09-25T21:20:16.251833Z","shell.execute_reply":"2021-09-25T21:20:21.831103Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Preprocessing - NN","metadata":{}},{"cell_type":"code","source":"def add_time_stats(df_train):\n    time_cols = [f for f in df_train.columns if f.endswith('_time')]\n    df_gp_stock = df_train.groupby('stock_id')\n    #\n    df_stats = df_gp_stock[time_cols].mean().reset_index()\n    df_stats.columns = ['stock_id'] + [f'{f}_mean' for f in time_cols]\n    df_train = df_train.merge(df_stats, on=['stock_id'], how='left')\n    #\n    df_stats = df_gp_stock[time_cols].std().reset_index()\n    df_stats.columns = ['stock_id'] + [f'{f}_std' for f in time_cols]\n    df_train = df_train.merge(df_stats, on=['stock_id'], how='left')\n    #\n    df_stats = df_gp_stock[time_cols].skew().reset_index()\n    df_stats.columns = ['stock_id'] + [f'{f}_skew' for f in time_cols]\n    df_train = df_train.merge(df_stats, on=['stock_id'], how='left')\n    #\n    df_stats = df_gp_stock[time_cols].min().reset_index()\n    df_stats.columns = ['stock_id'] + [f'{f}_min' for f in time_cols]\n    df_train = df_train.merge(df_stats, on=['stock_id'], how='left')\n    #\n    df_stats = df_gp_stock[time_cols].max().reset_index()\n    df_stats.columns = ['stock_id'] + [f'{f}_max' for f in time_cols]\n    df_train = df_train.merge(df_stats, on=['stock_id'], how='left')\n    #\n    df_stats = df_gp_stock[time_cols].quantile(0.25).reset_index()\n    df_stats.columns = ['stock_id'] + [f'{f}_q1' for f in time_cols]\n    df_train = df_train.merge(df_stats, on=['stock_id'], how='left')\n    #\n    df_stats = df_gp_stock[time_cols].quantile(0.50).reset_index()\n    df_stats.columns = ['stock_id'] + [f'{f}_q2' for f in time_cols]\n    df_train = df_train.merge(df_stats, on=['stock_id'], how='left')\n    #\n    df_stats = df_gp_stock[time_cols].quantile(0.75).reset_index()\n    df_stats.columns = ['stock_id'] + [f'{f}_q3' for f in time_cols]\n    df_train = df_train.merge(df_stats, on=['stock_id'], how='left')\n    return df_train.fillna(0.0)","metadata":{"execution":{"iopub.status.busy":"2021-09-25T21:20:21.833924Z","iopub.execute_input":"2021-09-25T21:20:21.834237Z","iopub.status.idle":"2021-09-25T21:20:21.850481Z","shell.execute_reply.started":"2021-09-25T21:20:21.834206Z","shell.execute_reply":"2021-09-25T21:20:21.849387Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Power transformation\ndf_train = df_train_nn\npipe = load_pickle(f'../input/{preprocessor_path}/pipe.pkl')\ndf_train[fea_cols] = pipe.transform(df_train[fea_cols].values)\ndf_train[fea_cols] = df_train[fea_cols].clip(-10, 10, axis=1)\n\n# Add groupby time_id features\nfea_cols_TA = [f for f in fea_cols if 'min_' not in f]\ndf_time_mean = df_train.groupby('time_id')[fea_cols_TA].mean()\ndf_time_mean.columns = [f'{c}_TA_mean' for c in df_time_mean.columns]\ndf_time_mean = df_time_mean.reset_index()\ndf_train = df_train.merge(df_time_mean, on='time_id', how='left')\n\n# Add groupby stock_id features\ndf_train = add_time_stats(df_train)\n\n# Save data for NN\ndt.Frame(df_train).to_csv('test_501_NN.csv')","metadata":{"execution":{"iopub.status.busy":"2021-09-25T21:20:21.851901Z","iopub.execute_input":"2021-09-25T21:20:21.852269Z","iopub.status.idle":"2021-09-25T21:20:22.146198Z","shell.execute_reply.started":"2021-09-25T21:20:21.852234Z","shell.execute_reply":"2021-09-25T21:20:22.145125Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del df_time_mean, list_dfs, df_train, df_train_nn\nx = gc.collect()","metadata":{"execution":{"iopub.status.busy":"2021-09-25T21:20:22.149342Z","iopub.execute_input":"2021-09-25T21:20:22.149807Z","iopub.status.idle":"2021-09-25T21:20:22.385654Z","shell.execute_reply.started":"2021-09-25T21:20:22.149759Z","shell.execute_reply":"2021-09-25T21:20:22.384466Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Prediction - LGB","metadata":{}},{"cell_type":"code","source":"# df_test = dt.fread('test_501_LGB.csv').to_pandas()\n# fea_cols = ['stock_id'] + [f for f in df_test.columns if f.startswith('B_') or f.startswith('T_') or f.startswith('Z_')]\n# X_test = df_test[fea_cols].values\n# for i in range(N_FOLD):\n#     lgb_model_path = f'{model_path}/lgb_501_{i}.pkl'\n#     lgb_model = load_pickle(lgb_model_path)\n#     df_test[f'lgb_{i}'] = lgb_model.predict(X_test)\n# df_test['pred_501-lgb'] = df_test[[f'lgb_{i}' for i in range(N_FOLD)]].mean(axis=1)\n# df_result = df_result.merge(df_test[['stock_id', 'time_id', 'pred_501-lgb']], on=['stock_id', 'time_id'])\n# df_result.head()","metadata":{"execution":{"iopub.status.busy":"2021-09-25T21:20:22.387427Z","iopub.execute_input":"2021-09-25T21:20:22.387888Z","iopub.status.idle":"2021-09-25T21:20:22.399442Z","shell.execute_reply.started":"2021-09-25T21:20:22.387831Z","shell.execute_reply":"2021-09-25T21:20:22.397865Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# del df_test, X_test\n# x = gc.collect()","metadata":{"execution":{"iopub.status.busy":"2021-09-25T21:20:22.401029Z","iopub.execute_input":"2021-09-25T21:20:22.401355Z","iopub.status.idle":"2021-09-25T21:20:22.415179Z","shell.execute_reply.started":"2021-09-25T21:20:22.401323Z","shell.execute_reply":"2021-09-25T21:20:22.413771Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Prediction - 501 - MLP","metadata":{}},{"cell_type":"code","source":"N_SEED = 5\nN_FOLD = 10\n\ndf_test = dt.fread('test_501_NN.csv').to_pandas()\nfea_cols = [f for f in df_test.columns if f.startswith('B_') or f.startswith('T_') or f.startswith('Z_')]\nX_test = df_test[fea_cols].values\nmodel_folder = '../input/optiver-final-501-mlp'\nfor i_seed in range(N_SEED):\n    for i_fold in range(N_FOLD):\n        model_path = f'{model_folder}/model_{i_seed}_{i_fold}.hdf5'\n        model = tf.keras.models.load_model(model_path, custom_objects={'mspe_loss': mspe_loss})\n        df_test[f'pred_{i_seed}_{i_fold}'] = model.predict(X_test, batch_size=1024)\n\ndf_test['pred_501_mlp'] = df_test[[f'pred_{i_seed}_{i_fold}' for i_seed in range(N_SEED) for i_fold in range(N_FOLD)]].mean(axis=1)\ndf_test['pred_501_mlp'] = inverse_target(df_test['pred_501_mlp'])\ndf_result = df_result.merge(df_test[['stock_id', 'time_id', 'pred_501_mlp']], on=['stock_id', 'time_id'])\ndf_result.head()","metadata":{"execution":{"iopub.status.busy":"2021-09-25T21:20:22.416759Z","iopub.execute_input":"2021-09-25T21:20:22.417245Z","iopub.status.idle":"2021-09-25T21:20:39.575272Z","shell.execute_reply.started":"2021-09-25T21:20:22.417205Z","shell.execute_reply":"2021-09-25T21:20:39.57425Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Prediction - 501 - 1dCNN","metadata":{}},{"cell_type":"code","source":"N_SEED = 5\nN_FOLD = 10\n\ndf_test = dt.fread('test_501_NN.csv').to_pandas()\nfea_cols = [f for f in df_test.columns if f.startswith('B_') or f.startswith('T_') or f.startswith('Z_')]\nX_test = df_test[fea_cols].values\nmodel_folder = '../input/optiver-final-501-1dcnn'\nfor i_seed in range(N_SEED):\n    for i_fold in range(N_FOLD):\n        model_path = f'{model_folder}/model_{i_seed}_{i_fold}.hdf5'\n        model = tf.keras.models.load_model(model_path, custom_objects={'mspe_loss': mspe_loss})\n        df_test[f'pred_{i_seed}_{i_fold}'] = model.predict(X_test, batch_size=1024)\n\ndf_test['pred_501_1dcnn'] = df_test[[f'pred_{i_seed}_{i_fold}' for i_seed in range(N_SEED) for i_fold in range(N_FOLD)]].mean(axis=1)\ndf_test['pred_501_1dcnn'] = inverse_target(df_test['pred_501_1dcnn'])\ndf_result = df_result.merge(df_test[['stock_id', 'time_id', 'pred_501_1dcnn']], on=['stock_id', 'time_id'])\ndf_result.head()","metadata":{"execution":{"iopub.status.busy":"2021-09-25T21:20:39.576846Z","iopub.execute_input":"2021-09-25T21:20:39.577221Z","iopub.status.idle":"2021-09-25T21:21:03.283845Z","shell.execute_reply.started":"2021-09-25T21:20:39.577187Z","shell.execute_reply":"2021-09-25T21:21:03.282807Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Prediction - 501 - TabNet","metadata":{}},{"cell_type":"code","source":"N_SEED = 3\nN_FOLD = 5\n\ndf_test = dt.fread('test_501_NN.csv').to_pandas()\nfea_cols = [f for f in df_test.columns if f.startswith('B_') or f.startswith('T_') or f.startswith('Z_')]\nX_test = df_test[fea_cols].values\nmodel_folder = '../input/optiver-final-501-tabnet'\nfor i_seed in range(N_SEED):\n    for i_fold in range(N_FOLD):\n        model_path = f'{model_folder}/model_{i_seed}_{i_fold}.xyz'\n        model = TabNetRegressor()\n        model.load_model(model_path)\n        df_test[f'pred_{i_seed}_{i_fold}'] = model.predict(X_test)\n\ndf_test['pred_501_tabnet'] = df_test[[f'pred_{i_seed}_{i_fold}' for i_seed in range(N_SEED) for i_fold in range(N_FOLD)]].mean(axis=1)\ndf_test['pred_501_tabnet'] = inverse_target(df_test['pred_501_tabnet'])\ndf_result = df_result.merge(df_test[['stock_id', 'time_id', 'pred_501_tabnet']], on=['stock_id', 'time_id'])\ndf_result.head()","metadata":{"execution":{"iopub.status.busy":"2021-09-25T21:21:03.285465Z","iopub.execute_input":"2021-09-25T21:21:03.286073Z","iopub.status.idle":"2021-09-25T21:21:06.171365Z","shell.execute_reply.started":"2021-09-25T21:21:03.286001Z","shell.execute_reply":"2021-09-25T21:21:06.170519Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Prediction - 501 - UNet","metadata":{}},{"cell_type":"code","source":"N_SEED = 3\nN_FOLD = 5\n\ndf_test = dt.fread('test_501_NN.csv').to_pandas()\nfea_cols = [f for f in df_test.columns if f.startswith('B_') or f.startswith('T_') or f.startswith('Z_')]\nX_test = df_test[fea_cols].values\nmodel_folder = '../input/optiver-final-501-unet'\nfor i_seed in range(N_SEED):\n    for i_fold in range(N_FOLD):\n        model_path = f'{model_folder}/model_{i_seed}_{i_fold}.hdf5'\n        model = tf.keras.models.load_model(model_path, custom_objects={'mspe_loss': mspe_loss})\n        df_test[f'pred_{i_seed}_{i_fold}'] = inverse_target(model.predict(X_test, batch_size=1024))\n\ndf_test['pred_501_unet'] = df_test[[f'pred_{i_seed}_{i_fold}' for i_seed in range(N_SEED) for i_fold in range(N_FOLD)]].mean(axis=1)\ndf_result = df_result.merge(df_test[['stock_id', 'time_id', 'pred_501_unet']], on=['stock_id', 'time_id'])\ndf_result.head()","metadata":{"execution":{"iopub.status.busy":"2021-09-25T21:21:06.172719Z","iopub.execute_input":"2021-09-25T21:21:06.173078Z","iopub.status.idle":"2021-09-25T21:21:54.618434Z","shell.execute_reply.started":"2021-09-25T21:21:06.173043Z","shell.execute_reply":"2021-09-25T21:21:54.617383Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Prediction - 501 - WaveNet","metadata":{"execution":{"iopub.status.busy":"2021-09-25T21:13:41.503104Z","iopub.execute_input":"2021-09-25T21:13:41.503813Z","iopub.status.idle":"2021-09-25T21:13:41.509585Z","shell.execute_reply.started":"2021-09-25T21:13:41.503694Z","shell.execute_reply":"2021-09-25T21:13:41.508277Z"}}},{"cell_type":"code","source":"N_SEED = 3\nN_FOLD = 5\n\ndf_test = dt.fread('test_501_NN.csv').to_pandas()\nfea_cols = [f for f in df_test.columns if f.startswith('B_') or f.startswith('T_') or f.startswith('Z_')]\nX_test = df_test[fea_cols].values\nmodel_folder = '../input/optiver-final-501-wavenet'\nfor i_seed in range(N_SEED):\n    for i_fold in range(N_FOLD):\n        model_path = f'{model_folder}/model_{i_seed}_{i_fold}.hdf5'\n        model = tf.keras.models.load_model(model_path, custom_objects={'mspe_loss': mspe_loss})\n        df_test[f'pred_{i_seed}_{i_fold}'] = inverse_target(model.predict(X_test, batch_size=1024))\n\ndf_test['pred_501_wavenet'] = df_test[[f'pred_{i_seed}_{i_fold}' for i_seed in range(N_SEED) for i_fold in range(N_FOLD)]].mean(axis=1)\ndf_result = df_result.merge(df_test[['stock_id', 'time_id', 'pred_501_wavenet']], on=['stock_id', 'time_id'])\ndf_result.head()","metadata":{"execution":{"iopub.status.busy":"2021-09-25T21:21:54.619996Z","iopub.execute_input":"2021-09-25T21:21:54.620462Z","iopub.status.idle":"2021-09-25T21:22:36.867554Z","shell.execute_reply.started":"2021-09-25T21:21:54.620418Z","shell.execute_reply":"2021-09-25T21:22:36.866267Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del df_test, X_test\nx = gc.collect()","metadata":{"execution":{"iopub.status.busy":"2021-09-25T21:22:36.869178Z","iopub.execute_input":"2021-09-25T21:22:36.869607Z","iopub.status.idle":"2021-09-25T21:22:37.338591Z","shell.execute_reply.started":"2021-09-25T21:22:36.869558Z","shell.execute_reply":"2021-09-25T21:22:37.337582Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 601","metadata":{}},{"cell_type":"markdown","source":"## Feature Generation","metadata":{"execution":{"iopub.status.busy":"2021-08-29T17:52:06.933743Z","iopub.execute_input":"2021-08-29T17:52:06.934164Z","iopub.status.idle":"2021-08-29T17:52:06.93842Z","shell.execute_reply.started":"2021-08-29T17:52:06.934128Z","shell.execute_reply":"2021-08-29T17:52:06.937183Z"}}},{"cell_type":"code","source":"# for pandas aggregation\n@njit\ndef rv_fast(x): return np.sqrt(np.nansum(x**2))\nfast_rv = lambda x: rv_fast(x.values)\nfast_rv.__name__ = 'realized_volatility'\n\n@njit\ndef sum_fast(x): return np.sum(x)\nfast_sum = lambda x: sum_fast(x.values)\nfast_sum.__name__ = 'sum'\n\n@njit\ndef mean_fast(x): return np.mean(x)\nfast_mean = lambda x: mean_fast(x.values)\nfast_mean.__name__ = 'mean'\n\n# @njit\ndef std_fast(x): return np.std(x, ddof=1)\nfast_std = lambda x: std_fast(x.values)\nfast_std.__name__ = 'std'\n\n@njit\ndef min_fast(x): return np.min(x)\nfast_min = lambda x: min_fast(x.values)\nfast_min.__name__ = 'min'\n\n@njit\ndef max_fast(x): return np.max(x)\nfast_max = lambda x: max_fast(x.values)\nfast_max.__name__ = 'max'\n\n@njit\ndef count_fast(x): return len(np.unique(x))\nfast_count = lambda x: count_fast(x.values)\nfast_count.__name__ = 'count_unique'","metadata":{"execution":{"iopub.status.busy":"2021-09-25T21:22:37.344897Z","iopub.execute_input":"2021-09-25T21:22:37.345256Z","iopub.status.idle":"2021-09-25T21:22:37.36072Z","shell.execute_reply.started":"2021-09-25T21:22:37.345223Z","shell.execute_reply":"2021-09-25T21:22:37.359606Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_dir = '../input/optiver-realized-volatility-prediction/'\n\n# Function to calculate first WAP\ndef calc_wap1(df):\n    wap = (df['bid_price1'] * df['ask_size1'] + df['ask_price1'] * df['bid_size1']) / (df['bid_size1'] + df['ask_size1'])\n    return wap\n\n# Function to calculate second WAP\ndef calc_wap2(df):\n    wap = (df['bid_price2'] * df['ask_size2'] + df['ask_price2'] * df['bid_size2']) / (df['bid_size2'] + df['ask_size2'])\n    return wap\n\ndef calc_wap3(df):\n    wap = (df['bid_price1'] * df['bid_size1'] + df['ask_price1'] * df['ask_size1']) / (df['bid_size1'] + df['ask_size1'])\n    return wap\n\ndef calc_wap4(df):\n    wap = (df['bid_price2'] * df['bid_size2'] + df['ask_price2'] * df['ask_size2']) / (df['bid_size2'] + df['ask_size2'])\n    return wap\n\n# Function to calculate the log of the return\n# Remember that logb(x / y) = logb(x) - logb(y)\ndef log_return(series):\n    return np.log(series).diff()\n\n# Function to read our base train and test set\ndef read_train_test():\n    train = pd.read_csv(f'{data_dir}/train.csv')\n    test = pd.read_csv(f'{data_dir}/test.csv')\n    # Create a key to merge with book and trade data\n    train['row_id'] = train['stock_id'].astype(str) + '-' + train['time_id'].astype(str)\n    test['row_id'] = test['stock_id'].astype(str) + '-' + test['time_id'].astype(str)\n    print(f'Our training set has {train.shape[0]} rows')\n    return train, test\n\n# Function to preprocess book data (for each stock id)\ndef book_preprocessor(file_path):\n    df = pd.read_parquet(file_path)\n    df.iloc[:, 2:] = df.iloc[:, 2:].astype('float64')\n    # Calculate Wap\n    df['wap1'] = calc_wap1(df)\n    df['wap2'] = calc_wap2(df)\n    df['wap3'] = calc_wap3(df)\n    df['wap4'] = calc_wap4(df)\n    # Calculate log returns    \n    df['log_return1'] = df.groupby(['time_id'])['wap1'].apply(log_return)\n    df['log_return2'] = df.groupby(['time_id'])['wap2'].apply(log_return)\n    df['log_return3'] = df.groupby(['time_id'])['wap3'].apply(log_return)\n    df['log_return4'] = df.groupby(['time_id'])['wap4'].apply(log_return)\n    # Calculate wap balance\n    df['wap_balance'] = abs(df['wap1'] - df['wap2'])\n    # Calculate spread\n    df['price_spread'] = (df['ask_price1'] - df['bid_price1']) / ((df['ask_price1'] + df['bid_price1']) / 2)\n    df['price_spread2'] = (df['ask_price2'] - df['bid_price2']) / ((df['ask_price2'] + df['bid_price2']) / 2)\n    df['bid_spread'] = df['bid_price1'] - df['bid_price2']\n    df['ask_spread'] = df['ask_price1'] - df['ask_price2']\n    df[\"bid_ask_spread\"] = abs(df['bid_spread'] - df['ask_spread'])\n    df['total_volume'] = (df['ask_size1'] + df['ask_size2']) + (df['bid_size1'] + df['bid_size2'])\n    df['volume_imbalance'] = abs((df['ask_size1'] + df['ask_size2']) - (df['bid_size1'] + df['bid_size2']))\n    \n    # Dict for aggregations\n    create_feature_dict = {\n        'wap1': [fast_sum, fast_std],\n        'wap2': [fast_sum, fast_std],\n        'wap3': [fast_sum, fast_std],\n        'wap4': [fast_sum, fast_std],\n        'log_return1': [fast_rv],\n        'log_return2': [fast_rv],\n        'log_return3': [fast_rv],\n        'log_return4': [fast_rv],\n        'wap_balance': [fast_sum, fast_max],\n        'price_spread':[fast_sum, fast_max],\n        'price_spread2':[fast_sum, fast_max],\n        'bid_spread':[fast_sum, fast_max],\n        'ask_spread':[fast_sum, fast_max],\n        'total_volume':[fast_sum, fast_max],\n        'volume_imbalance':[fast_sum, fast_max],\n        \"bid_ask_spread\":[fast_sum, fast_max],\n    }\n    create_feature_dict_time = {\n        'log_return1': [fast_rv],\n        'log_return2': [fast_rv],\n        'log_return3': [fast_rv],\n        'log_return4': [fast_rv],\n    }\n    \n    # Function to get group stats for different windows (seconds in bucket)\n    def get_stats_window(fe_dict,seconds_in_bucket, add_suffix = False):\n        # Group by the window\n        df_feature = df[df['seconds_in_bucket'] >= seconds_in_bucket].groupby(['time_id']).agg(fe_dict).reset_index()\n        # Rename columns joining suffix\n        df_feature.columns = ['_'.join(col) for col in df_feature.columns]\n        # Add a suffix to differentiate windows\n        if add_suffix:\n            df_feature = df_feature.add_suffix('_' + str(seconds_in_bucket))\n        return df_feature\n    \n    # Get the stats for different windows\n    df_feature = get_stats_window(create_feature_dict,seconds_in_bucket = 0, add_suffix = False)\n    df_feature_500 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 500, add_suffix = True)\n    df_feature_400 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 400, add_suffix = True)\n    df_feature_300 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 300, add_suffix = True)\n    df_feature_200 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 200, add_suffix = True)\n    df_feature_100 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 100, add_suffix = True)\n\n    # Merge all\n    df_feature = df_feature.merge(df_feature_500, how = 'left', left_on = 'time_id_', right_on = 'time_id__500')\n    df_feature = df_feature.merge(df_feature_400, how = 'left', left_on = 'time_id_', right_on = 'time_id__400')\n    df_feature = df_feature.merge(df_feature_300, how = 'left', left_on = 'time_id_', right_on = 'time_id__300')\n    df_feature = df_feature.merge(df_feature_200, how = 'left', left_on = 'time_id_', right_on = 'time_id__200')\n    df_feature = df_feature.merge(df_feature_100, how = 'left', left_on = 'time_id_', right_on = 'time_id__100')\n    # Drop unnecesary time_ids\n    df_feature.drop(['time_id__500','time_id__400', 'time_id__300', 'time_id__200','time_id__100'], axis = 1, inplace = True)\n    \n    \n    # Create row_id so we can merge\n    stock_id = file_path.split('=')[1].split('/')[0]\n    df_feature['row_id'] = df_feature['time_id_'].apply(lambda x: f'{stock_id}-{x}')\n    df_feature.drop(['time_id_'], axis = 1, inplace = True)\n    return df_feature\n\n# Function to preprocess trade data (for each stock id)\ndef trade_preprocessor(file_path):\n    df = pd.read_parquet(file_path)\n    df.iloc[:, 2:] = df.iloc[:, 2:].astype('float64')\n    df['log_return'] = df.groupby('time_id')['price'].apply(log_return)\n    df['amount']=df['price']*df['size']\n    # Dict for aggregations\n    create_feature_dict = {\n        'log_return':[fast_rv],\n        'seconds_in_bucket':[fast_count],\n        'size':[fast_sum, fast_max, fast_min],\n        'order_count':[fast_sum,fast_max],\n        'amount':[fast_sum,fast_max,fast_min],\n    }\n    create_feature_dict_time = {\n        'log_return':[fast_rv],\n        'seconds_in_bucket':[fast_count],\n        'size':[fast_sum],\n        'order_count':[fast_sum],\n    }\n    # Function to get group stats for different windows (seconds in bucket)\n    def get_stats_window(fe_dict,seconds_in_bucket, add_suffix = False):\n        # Group by the window\n        df_feature = df[df['seconds_in_bucket'] >= seconds_in_bucket].groupby(['time_id']).agg(fe_dict).reset_index()\n        # Rename columns joining suffix\n        df_feature.columns = ['_'.join(col) for col in df_feature.columns]\n        # Add a suffix to differentiate windows\n        if add_suffix:\n            df_feature = df_feature.add_suffix('_' + str(seconds_in_bucket))\n        return df_feature\n    \n\n    # Get the stats for different windows\n    df_feature = get_stats_window(create_feature_dict,seconds_in_bucket = 0, add_suffix = False)\n    df_feature_500 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 500, add_suffix = True)\n    df_feature_400 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 400, add_suffix = True)\n    df_feature_300 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 300, add_suffix = True)\n    df_feature_200 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 200, add_suffix = True)\n    df_feature_100 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 100, add_suffix = True)\n    \n    def tendency(price, vol):    \n        df_diff = np.diff(price)\n        val = (df_diff/price[1:])*100\n        power = np.sum(val*vol[1:])\n        return(power)\n    \n    lis = []\n    for n_time_id in df['time_id'].unique():\n        df_id = df[df['time_id'] == n_time_id]        \n        tendencyV = tendency(df_id['price'].values, df_id['size'].values)      \n        f_max = np.sum(df_id['price'].values > np.mean(df_id['price'].values))\n        f_min = np.sum(df_id['price'].values < np.mean(df_id['price'].values))\n        df_max =  np.sum(np.diff(df_id['price'].values) > 0)\n        df_min =  np.sum(np.diff(df_id['price'].values) < 0)\n        # new\n        abs_diff = np.median(np.abs( df_id['price'].values - np.mean(df_id['price'].values)))        \n        energy = np.mean(df_id['price'].values**2)\n        iqr_p = np.percentile(df_id['price'].values,75) - np.percentile(df_id['price'].values,25)\n        \n        # vol vars\n        \n        abs_diff_v = np.median(np.abs( df_id['size'].values - np.mean(df_id['size'].values)))        \n        energy_v = np.sum(df_id['size'].values**2)\n        iqr_p_v = np.percentile(df_id['size'].values,75) - np.percentile(df_id['size'].values,25)\n        \n        lis.append({'time_id':n_time_id,'tendency':tendencyV,'f_max':f_max,'f_min':f_min,'df_max':df_max,'df_min':df_min,\n                   'abs_diff':abs_diff,'energy':energy,'iqr_p':iqr_p,'abs_diff_v':abs_diff_v,'energy_v':energy_v,'iqr_p_v':iqr_p_v})\n    \n    df_lr = pd.DataFrame(lis)\n        \n   \n    df_feature = df_feature.merge(df_lr, how = 'left', left_on = 'time_id_', right_on = 'time_id')\n    \n    # Merge all\n    df_feature = df_feature.merge(df_feature_500, how = 'left', left_on = 'time_id_', right_on = 'time_id__500')\n    df_feature = df_feature.merge(df_feature_400, how = 'left', left_on = 'time_id_', right_on = 'time_id__400')\n    df_feature = df_feature.merge(df_feature_300, how = 'left', left_on = 'time_id_', right_on = 'time_id__300')\n    df_feature = df_feature.merge(df_feature_200, how = 'left', left_on = 'time_id_', right_on = 'time_id__200')\n    df_feature = df_feature.merge(df_feature_100, how = 'left', left_on = 'time_id_', right_on = 'time_id__100')\n    # Drop unnecesary time_ids\n    df_feature.drop(['time_id__500','time_id__400', 'time_id__300', 'time_id__200','time_id','time_id__100'], axis = 1, inplace = True)\n    \n    \n    df_feature = df_feature.add_prefix('trade_')\n    stock_id = file_path.split('=')[1].split('/')[0]\n    df_feature['row_id'] = df_feature['trade_time_id_'].apply(lambda x:f'{stock_id}-{x}')\n    df_feature.drop(['trade_time_id_'], axis = 1, inplace = True)\n    return df_feature\n\n# Function to get group stats for the stock_id and time_id\ndef get_time_stock(df):\n    vol_cols = ['log_return1_realized_volatility', 'log_return2_realized_volatility', 'log_return1_realized_volatility_400', 'log_return2_realized_volatility_400', \n                'log_return1_realized_volatility_300', 'log_return2_realized_volatility_300', 'log_return1_realized_volatility_200', 'log_return2_realized_volatility_200', \n                'trade_log_return_realized_volatility', 'trade_log_return_realized_volatility_400', 'trade_log_return_realized_volatility_300', 'trade_log_return_realized_volatility_200']\n\n    # Group by the stock id\n    df_stock_id = df.groupby(['stock_id'])[vol_cols].agg(['mean', 'std', 'max', 'min', ]).reset_index()\n    # Rename columns joining suffix\n    df_stock_id.columns = ['_'.join(col) for col in df_stock_id.columns]\n    df_stock_id = df_stock_id.add_suffix('_' + 'stock')\n\n    # Group by the stock id\n    df_time_id = df.groupby(['time_id'])[vol_cols].agg(['mean', 'std', 'max', 'min', ]).reset_index()\n    # Rename columns joining suffix\n    df_time_id.columns = ['_'.join(col) for col in df_time_id.columns]\n    df_time_id = df_time_id.add_suffix('_' + 'time')\n    \n    # Merge with original dataframe\n    df = df.merge(df_stock_id, how = 'left', left_on = ['stock_id'], right_on = ['stock_id__stock'])\n    df = df.merge(df_time_id, how = 'left', left_on = ['time_id'], right_on = ['time_id__time'])\n    df.drop(['stock_id__stock', 'time_id__time'], axis = 1, inplace = True)\n    return df\n    \n\n# Funtion to make preprocessing function in parallel (for each stock id)\ndef preprocessor(list_stock_ids, is_train = True):\n    \n    # Parrallel for loop\n    def for_joblib(stock_id):\n        # Train\n        if is_train:\n            file_path_book = glob.glob(f'../input/optiver-realized-volatility-prediction/book_train.parquet/stock_id={stock_id}/*')[0]\n            file_path_trade = glob.glob(f'../input/optiver-realized-volatility-prediction/trade_train.parquet/stock_id={stock_id}/*')[0]\n        # Test\n        else:\n            file_path_book = glob.glob(f'../input/optiver-realized-volatility-prediction/book_test.parquet/stock_id={stock_id}/*')[0]\n            file_path_trade = glob.glob(f'../input/optiver-realized-volatility-prediction/trade_test.parquet/stock_id={stock_id}/*')[0]\n    \n        # Preprocess book and trade data and merge them\n        df_tmp = pd.merge(book_preprocessor(file_path_book), trade_preprocessor(file_path_trade), on = 'row_id', how = 'left')\n        \n        # Return the merge dataframe\n        return df_tmp\n    \n    # Use parallel api to call paralle for loop\n    df = Parallel(n_jobs = -1, verbose = 1)(delayed(for_joblib)(stock_id) for stock_id in tqdm(list_stock_ids))\n    # Concatenate all the dataframes that return from Parallel\n    df = pd.concat(df, ignore_index = True)\n    return df","metadata":{"execution":{"iopub.status.busy":"2021-09-25T21:22:37.363386Z","iopub.execute_input":"2021-09-25T21:22:37.36373Z","iopub.status.idle":"2021-09-25T21:22:37.430087Z","shell.execute_reply.started":"2021-09-25T21:22:37.363692Z","shell.execute_reply":"2021-09-25T21:22:37.428838Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Read train and test\ntrain, test = read_train_test()\n\n# Get unique stock ids \n# train_stock_ids = train['stock_id'].unique()\n# Preprocess them using Parallel and our single stock id functions\n# train_ = preprocessor(train_stock_ids, is_train = True)\n# train = train.merge(train_, on = ['row_id'], how = 'left')\n\n# Get unique stock ids \ntest_stock_ids = test['stock_id'].unique()\n# Preprocess them using Parallel and our single stock id functions\ntest_ = preprocessor(test_stock_ids, is_train = False)\ntest = test.merge(test_, on = ['row_id'], how = 'left')\n\n# # Get group stats of time_id and stock_id\n# train = get_time_stock(train)\ntest = get_time_stock(test)","metadata":{"execution":{"iopub.status.busy":"2021-09-25T21:22:37.431401Z","iopub.execute_input":"2021-09-25T21:22:37.431769Z","iopub.status.idle":"2021-09-25T21:22:43.91956Z","shell.execute_reply.started":"2021-09-25T21:22:37.431731Z","shell.execute_reply":"2021-09-25T21:22:43.918523Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# replace by order sum (tau)\n# train['size_tau'] = np.sqrt( 1/ train['trade_seconds_in_bucket_count_unique'] )\ntest['size_tau'] = np.sqrt( 1/ test['trade_seconds_in_bucket_count_unique'] )\n# train['size_tau_400'] = np.sqrt( 1/ train['trade_seconds_in_bucket_count_unique_400'] )\ntest['size_tau_400'] = np.sqrt( 1/ test['trade_seconds_in_bucket_count_unique_400'] )\n# train['size_tau_300'] = np.sqrt( 1/ train['trade_seconds_in_bucket_count_unique_300'] )\ntest['size_tau_300'] = np.sqrt( 1/ test['trade_seconds_in_bucket_count_unique_300'] )\n# train['size_tau_200'] = np.sqrt( 1/ train['trade_seconds_in_bucket_count_unique_200'] )\ntest['size_tau_200'] = np.sqrt( 1/ test['trade_seconds_in_bucket_count_unique_200'] )","metadata":{"execution":{"iopub.status.busy":"2021-09-25T21:22:43.920933Z","iopub.execute_input":"2021-09-25T21:22:43.921255Z","iopub.status.idle":"2021-09-25T21:22:43.932251Z","shell.execute_reply.started":"2021-09-25T21:22:43.921223Z","shell.execute_reply":"2021-09-25T21:22:43.930941Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train['size_tau2'] = np.sqrt( 1/ train['trade_order_count_sum'] )\ntest['size_tau2'] = np.sqrt( 1/ test['trade_order_count_sum'] )\n# train['size_tau2_400'] = np.sqrt( 0.33/ train['trade_order_count_sum'] )\ntest['size_tau2_400'] = np.sqrt( 0.33/ test['trade_order_count_sum'] )\n# train['size_tau2_300'] = np.sqrt( 0.5/ train['trade_order_count_sum'] )\ntest['size_tau2_300'] = np.sqrt( 0.5/ test['trade_order_count_sum'] )\n# train['size_tau2_200'] = np.sqrt( 0.66/ train['trade_order_count_sum'] )\ntest['size_tau2_200'] = np.sqrt( 0.66/ test['trade_order_count_sum'] )\n\n# delta tau\n# train['size_tau2_d'] = train['size_tau2_400'] - train['size_tau2']\ntest['size_tau2_d'] = test['size_tau2_400'] - test['size_tau2']","metadata":{"execution":{"iopub.status.busy":"2021-09-25T21:22:43.933873Z","iopub.execute_input":"2021-09-25T21:22:43.934249Z","iopub.status.idle":"2021-09-25T21:22:43.958575Z","shell.execute_reply.started":"2021-09-25T21:22:43.934215Z","shell.execute_reply":"2021-09-25T21:22:43.957685Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = dt.fread('../input/optiver-501-train/public_train_601.csv').to_pandas()\ntrain.head()","metadata":{"execution":{"iopub.status.busy":"2021-09-25T21:22:43.959664Z","iopub.execute_input":"2021-09-25T21:22:43.960125Z","iopub.status.idle":"2021-09-25T21:22:52.051237Z","shell.execute_reply.started":"2021-09-25T21:22:43.960077Z","shell.execute_reply":"2021-09-25T21:22:52.050152Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.head()","metadata":{"execution":{"iopub.status.busy":"2021-09-25T21:22:52.052506Z","iopub.execute_input":"2021-09-25T21:22:52.052809Z","iopub.status.idle":"2021-09-25T21:22:52.081462Z","shell.execute_reply.started":"2021-09-25T21:22:52.052781Z","shell.execute_reply":"2021-09-25T21:22:52.080344Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"colNames = [col for col in list(train.columns)\n            if col not in {\"stock_id\", \"time_id\", \"target\", \"row_id\"}]\nlen(colNames)","metadata":{"execution":{"iopub.status.busy":"2021-09-25T21:22:52.082844Z","iopub.execute_input":"2021-09-25T21:22:52.083169Z","iopub.status.idle":"2021-09-25T21:22:52.092581Z","shell.execute_reply.started":"2021-09-25T21:22:52.083137Z","shell.execute_reply":"2021-09-25T21:22:52.091482Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Kmean for LGB","metadata":{}},{"cell_type":"code","source":"from sklearn.cluster import KMeans\n# making agg features\n\ntrain_p = pd.read_csv(f'{data_dir}/train.csv')\ntrain_p = train_p.pivot(index='time_id', columns='stock_id', values='target')\n\ncorr = train_p.corr()\n\nids = corr.index\n\nkmeans = KMeans(n_clusters=7, random_state=0).fit(corr.values)\nprint(kmeans.labels_)\n\nl = []\nfor n in range(7):\n    l.append ( [ (x-1) for x in ( (ids+1)*(kmeans.labels_ == n)) if x > 0] )\n\n\nmat = []\nmatTest = []\n\nn = 0\nfor ind in l:\n    print(ind)\n    newDf = train.loc[train['stock_id'].isin(ind) ]\n    newDf = newDf.groupby(['time_id']).agg(np.nanmean)\n    newDf.loc[:,'stock_id'] = str(n)+'c1'\n    mat.append ( newDf )\n    \n    newDf = test.loc[test['stock_id'].isin(ind) ]    \n    newDf = newDf.groupby(['time_id']).agg(np.nanmean)\n    newDf.loc[:,'stock_id'] = str(n)+'c1'\n    matTest.append ( newDf )\n    \n    n+=1\n    \nmat1 = pd.concat(mat).reset_index()\nmat1.drop(columns=['target'],inplace=True)\n\nmat2 = pd.concat(matTest).reset_index()","metadata":{"execution":{"iopub.status.busy":"2021-09-25T21:22:52.094496Z","iopub.execute_input":"2021-09-25T21:22:52.094791Z","iopub.status.idle":"2021-09-25T21:22:53.895478Z","shell.execute_reply.started":"2021-09-25T21:22:52.094762Z","shell.execute_reply":"2021-09-25T21:22:53.894459Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mat2 = pd.concat([mat2,mat1.loc[mat1.time_id==5]])\nmat1 = mat1.pivot(index='time_id', columns='stock_id')\nmat1.columns = [\"_\".join(x) for x in mat1.columns.ravel()]\nmat1.reset_index(inplace=True)\n\nmat2 = mat2.pivot(index='time_id', columns='stock_id')\nmat2.columns = [\"_\".join(x) for x in mat2.columns.ravel()]\nmat2.reset_index(inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-09-25T21:22:53.896985Z","iopub.execute_input":"2021-09-25T21:22:53.897419Z","iopub.status.idle":"2021-09-25T21:22:53.982042Z","shell.execute_reply.started":"2021-09-25T21:22:53.897376Z","shell.execute_reply":"2021-09-25T21:22:53.981053Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nnn = ['time_id',\n     'log_return1_realized_volatility_0c1',\n     'log_return1_realized_volatility_1c1',     \n     'log_return1_realized_volatility_3c1',\n     'log_return1_realized_volatility_4c1',     \n     'log_return1_realized_volatility_6c1',\n     'total_volume_sum_0c1',\n     'total_volume_sum_1c1', \n     'total_volume_sum_3c1',\n     'total_volume_sum_4c1', \n     'total_volume_sum_6c1',\n     'trade_size_sum_0c1',\n     'trade_size_sum_1c1', \n     'trade_size_sum_3c1',\n     'trade_size_sum_4c1', \n     'trade_size_sum_6c1',\n     'trade_order_count_sum_0c1',\n     'trade_order_count_sum_1c1',\n     'trade_order_count_sum_3c1',\n     'trade_order_count_sum_4c1',\n     'trade_order_count_sum_6c1',      \n     'price_spread_sum_0c1',\n     'price_spread_sum_1c1',\n     'price_spread_sum_3c1',\n     'price_spread_sum_4c1',\n     'price_spread_sum_6c1',   \n     'bid_spread_sum_0c1',\n     'bid_spread_sum_1c1',\n     'bid_spread_sum_3c1',\n     'bid_spread_sum_4c1',\n     'bid_spread_sum_6c1',       \n     'ask_spread_sum_0c1',\n     'ask_spread_sum_1c1',\n     'ask_spread_sum_3c1',\n     'ask_spread_sum_4c1',\n     'ask_spread_sum_6c1',   \n     'volume_imbalance_sum_0c1',\n     'volume_imbalance_sum_1c1',\n     'volume_imbalance_sum_3c1',\n     'volume_imbalance_sum_4c1',\n     'volume_imbalance_sum_6c1',       \n     'bid_ask_spread_sum_0c1',\n     'bid_ask_spread_sum_1c1',\n     'bid_ask_spread_sum_3c1',\n     'bid_ask_spread_sum_4c1',\n     'bid_ask_spread_sum_6c1',\n     'size_tau2_0c1',\n     'size_tau2_1c1',\n     'size_tau2_3c1',\n     'size_tau2_4c1',\n     'size_tau2_6c1'] \n\ntrain = pd.merge(train,mat1[nnn],how='left',on='time_id')\ntest = pd.merge(test,mat2[nnn],how='left',on='time_id')","metadata":{"execution":{"iopub.status.busy":"2021-09-25T21:22:53.983342Z","iopub.execute_input":"2021-09-25T21:22:53.983649Z","iopub.status.idle":"2021-09-25T21:22:54.762826Z","shell.execute_reply.started":"2021-09-25T21:22:53.983621Z","shell.execute_reply":"2021-09-25T21:22:54.761785Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if DEBUG:\n    dt.Frame(train).to_csv(f'test_601_LGB.csv')    \nelse:\n    dt.Frame(test).to_csv(f'test_601_LGB.csv')","metadata":{"execution":{"iopub.status.busy":"2021-09-25T21:22:54.764377Z","iopub.execute_input":"2021-09-25T21:22:54.76474Z","iopub.status.idle":"2021-09-25T21:22:54.791539Z","shell.execute_reply.started":"2021-09-25T21:22:54.764695Z","shell.execute_reply":"2021-09-25T21:22:54.790386Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Kmean + Quantile transformation for NN","metadata":{}},{"cell_type":"code","source":"train.replace([np.inf, -np.inf], np.nan,inplace=True)\ntest.replace([np.inf, -np.inf], np.nan,inplace=True)\nqt_train = load_pickle(f'{preprocessor_path}/qt_train.pkl')\ntrain_nn=train[colNames].copy()\ntest_nn=test[colNames].copy()\nfor i, col in enumerate(colNames):\n    #print(col)\n    qt = qt_train[i]\n    train_nn[col] = qt.transform(train_nn[[col]])\n    test_nn[col] = qt.transform(test_nn[[col]])","metadata":{"execution":{"iopub.status.busy":"2021-09-25T21:22:54.793664Z","iopub.execute_input":"2021-09-25T21:22:54.794044Z","iopub.status.idle":"2021-09-25T21:23:19.513419Z","shell.execute_reply.started":"2021-09-25T21:22:54.793993Z","shell.execute_reply":"2021-09-25T21:23:19.51222Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_nn[['stock_id','time_id','target']]=train[['stock_id','time_id','target']]\ntest_nn[['stock_id','time_id']]=test[['stock_id','time_id']]","metadata":{"execution":{"iopub.status.busy":"2021-09-25T21:23:19.514789Z","iopub.execute_input":"2021-09-25T21:23:19.515126Z","iopub.status.idle":"2021-09-25T21:23:19.527936Z","shell.execute_reply.started":"2021-09-25T21:23:19.515094Z","shell.execute_reply":"2021-09-25T21:23:19.527016Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Kmean: making agg features\nfrom sklearn.cluster import KMeans\ntrain_p = pd.read_csv(f'{data_dir}/train.csv')\ntrain_p = train_p.pivot(index='time_id', columns='stock_id', values='target')\n\ncorr = train_p.corr()\n\nids = corr.index\n\nkmeans = KMeans(n_clusters=7, random_state=0).fit(corr.values)\nprint(kmeans.labels_)\n\nl = []\nfor n in range(7):\n    l.append ( [ (x-1) for x in ( (ids+1)*(kmeans.labels_ == n)) if x > 0] )\n    \n\nmat = []\nmatTest = []\n\nn = 0\nfor ind in l:\n    print(ind)\n    newDf = train_nn.loc[train_nn['stock_id'].isin(ind) ]\n    newDf = newDf.groupby(['time_id']).agg(np.nanmean)\n    newDf.loc[:,'stock_id'] = str(n)+'c1'\n    mat.append ( newDf )\n    \n    newDf = test_nn.loc[test_nn['stock_id'].isin(ind) ]    \n    newDf = newDf.groupby(['time_id']).agg(np.nanmean)\n    newDf.loc[:,'stock_id'] = str(n)+'c1'\n    matTest.append ( newDf )\n    \n    n+=1\n    \nmat1 = pd.concat(mat).reset_index()\nmat1.drop(columns=['target'],inplace=True)\n\nmat2 = pd.concat(matTest).reset_index()\nmat2 = pd.concat([mat2,mat1.loc[mat1.time_id==5]])","metadata":{"execution":{"iopub.status.busy":"2021-09-25T21:23:19.529224Z","iopub.execute_input":"2021-09-25T21:23:19.529533Z","iopub.status.idle":"2021-09-25T21:23:21.533377Z","shell.execute_reply.started":"2021-09-25T21:23:19.529495Z","shell.execute_reply":"2021-09-25T21:23:21.532316Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nnn = ['time_id',\n     'log_return1_realized_volatility_0c1',\n     'log_return1_realized_volatility_1c1',     \n     'log_return1_realized_volatility_3c1',\n     'log_return1_realized_volatility_4c1',     \n     'log_return1_realized_volatility_6c1',\n     'total_volume_sum_0c1',\n     'total_volume_sum_1c1', \n     'total_volume_sum_3c1',\n     'total_volume_sum_4c1', \n     'total_volume_sum_6c1',\n     'trade_size_sum_0c1',\n     'trade_size_sum_1c1', \n     'trade_size_sum_3c1',\n     'trade_size_sum_4c1', \n     'trade_size_sum_6c1',\n     'trade_order_count_sum_0c1',\n     'trade_order_count_sum_1c1',\n     'trade_order_count_sum_3c1',\n     'trade_order_count_sum_4c1',\n     'trade_order_count_sum_6c1',      \n     'price_spread_sum_0c1',\n     'price_spread_sum_1c1',\n     'price_spread_sum_3c1',\n     'price_spread_sum_4c1',\n     'price_spread_sum_6c1',   \n     'bid_spread_sum_0c1',\n     'bid_spread_sum_1c1',\n     'bid_spread_sum_3c1',\n     'bid_spread_sum_4c1',\n     'bid_spread_sum_6c1',       \n     'ask_spread_sum_0c1',\n     'ask_spread_sum_1c1',\n     'ask_spread_sum_3c1',\n     'ask_spread_sum_4c1',\n     'ask_spread_sum_6c1',   \n     'volume_imbalance_sum_0c1',\n     'volume_imbalance_sum_1c1',\n     'volume_imbalance_sum_3c1',\n     'volume_imbalance_sum_4c1',\n     'volume_imbalance_sum_6c1',       \n     'bid_ask_spread_sum_0c1',\n     'bid_ask_spread_sum_1c1',\n     'bid_ask_spread_sum_3c1',\n     'bid_ask_spread_sum_4c1',\n     'bid_ask_spread_sum_6c1',\n     'size_tau2_0c1',\n     'size_tau2_1c1',\n     'size_tau2_3c1',\n     'size_tau2_4c1',\n     'size_tau2_6c1'] ","metadata":{"execution":{"iopub.status.busy":"2021-09-25T21:23:21.53465Z","iopub.execute_input":"2021-09-25T21:23:21.534964Z","iopub.status.idle":"2021-09-25T21:23:21.542328Z","shell.execute_reply.started":"2021-09-25T21:23:21.534934Z","shell.execute_reply":"2021-09-25T21:23:21.541574Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mat1 = mat1.pivot(index='time_id', columns='stock_id')\nmat1.columns = [\"_\".join(x) for x in mat1.columns.ravel()]\nmat1.reset_index(inplace=True)\n\nmat2 = mat2.pivot(index='time_id', columns='stock_id')\nmat2.columns = [\"_\".join(x) for x in mat2.columns.ravel()]\nmat2.reset_index(inplace=True)\n\ntrain_nn = pd.merge(train_nn,mat1[nnn],how='left',on='time_id')\ntest_nn = pd.merge(test_nn,mat2[nnn],how='left',on='time_id')","metadata":{"execution":{"iopub.status.busy":"2021-09-25T21:23:21.543485Z","iopub.execute_input":"2021-09-25T21:23:21.543783Z","iopub.status.idle":"2021-09-25T21:23:22.586951Z","shell.execute_reply.started":"2021-09-25T21:23:21.543755Z","shell.execute_reply":"2021-09-25T21:23:22.585878Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# fill mean\nfeatures_to_consider = list(train_nn)\n\nfeatures_to_consider.remove('time_id')\nfeatures_to_consider.remove('target')\ntry:\n    features_to_consider.remove('pred_NN')\nexcept:\n    pass\n\ntrain_nn[features_to_consider] = train_nn[features_to_consider].fillna(train_nn[features_to_consider].mean())\ntest_nn[features_to_consider] = test_nn[features_to_consider].fillna(train_nn[features_to_consider].mean())\n\nif DEBUG:\n    dt.Frame(train_nn).to_csv(f'test_601_NN.csv')    \nelse:\n    dt.Frame(test_nn).to_csv(f'test_601_NN.csv')","metadata":{"execution":{"iopub.status.busy":"2021-09-25T21:23:22.588445Z","iopub.execute_input":"2021-09-25T21:23:22.58873Z","iopub.status.idle":"2021-09-25T21:23:26.118219Z","shell.execute_reply.started":"2021-09-25T21:23:22.588703Z","shell.execute_reply":"2021-09-25T21:23:26.117359Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del mat1, mat2, train, test, train_nn, test_nn\n# del train,test\nx = gc.collect()","metadata":{"execution":{"iopub.status.busy":"2021-09-25T21:23:26.119764Z","iopub.execute_input":"2021-09-25T21:23:26.120094Z","iopub.status.idle":"2021-09-25T21:23:26.382995Z","shell.execute_reply.started":"2021-09-25T21:23:26.120062Z","shell.execute_reply":"2021-09-25T21:23:26.382118Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Prediction - 601 - LGB","metadata":{}},{"cell_type":"code","source":"N_SEED = 5\nN_FOLD = 10\n\ndf_test = dt.fread('test_601_LGB.csv').to_pandas()\nfea_cols = ['stock_id'] + [f for f in df_test if f not in ['time_id', 'target', 'stock_id', 'row_id']]\nX_test = df_test[fea_cols].values\nmodel_folder = '../input/optiver-final-601-lgb'\nfor i_seed in range(N_SEED):\n    for i_fold in range(N_FOLD):\n        model_path = f'{model_folder}/LGB_{i_seed}_{i_fold}.pkl'\n        model = load_pickle(model_path)\n        df_test[f'pred_{i_seed}_{i_fold}'] = model.predict(X_test)\ndf_test['pred_601_lgb'] = df_test[[f'pred_{i_seed}_{i_fold}' for i_seed in range(N_SEED) for i_fold in range(N_FOLD)]].mean(axis=1)\ndf_result = df_result.merge(df_test[['stock_id', 'time_id', 'pred_601_lgb']], on=['stock_id', 'time_id'])\ndf_result.head()","metadata":{"execution":{"iopub.status.busy":"2021-09-25T21:23:26.384516Z","iopub.execute_input":"2021-09-25T21:23:26.385222Z","iopub.status.idle":"2021-09-25T21:23:31.705719Z","shell.execute_reply.started":"2021-09-25T21:23:26.38507Z","shell.execute_reply":"2021-09-25T21:23:31.704877Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Prediction - 601 - Catboost","metadata":{}},{"cell_type":"code","source":"N_SEED = 5\nN_FOLD = 10\n\ndf_test = dt.fread('test_601_LGB.csv').to_pandas()\nfea_cols = ['stock_id'] + [f for f in df_test if f not in ['time_id', 'target', 'stock_id', 'row_id']]\nX_test = df_test[fea_cols]\nmodel_folder = '../input/optiver-final-601-cat'\nfor i_seed in range(N_SEED):\n    for i_fold in range(N_FOLD):\n        model_path = f'{model_folder}/CAT_{i_seed}_{i_fold}.pkl'\n        model = load_pickle(model_path)\n        df_test[f'pred_{i_seed}_{i_fold}'] = model.predict(X_test)\ndf_test['pred_601_cat'] = df_test[[f'pred_{i_seed}_{i_fold}' for i_seed in range(N_SEED) for i_fold in range(N_FOLD)]].mean(axis=1)\ndf_result = df_result.merge(df_test[['stock_id', 'time_id', 'pred_601_cat']], on=['stock_id', 'time_id'])\ndf_result.head()","metadata":{"execution":{"iopub.status.busy":"2021-09-25T21:23:31.70696Z","iopub.execute_input":"2021-09-25T21:23:31.707256Z","iopub.status.idle":"2021-09-25T21:23:33.87533Z","shell.execute_reply.started":"2021-09-25T21:23:31.707228Z","shell.execute_reply":"2021-09-25T21:23:33.874341Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Prediction - 601 - DeepForest (CFR)","metadata":{}},{"cell_type":"code","source":"N_FOLD = 5\n\ndf_test = dt.fread('test_601_LGB.csv').to_pandas()\nfea_cols = ['stock_id'] + [f for f in df_test if f not in ['time_id', 'target', 'stock_id', 'row_id']]\nX_test = df_test[fea_cols].values\nmodel_folder = '../input/optiver-601-cfr'\nfor i_fold in tqdm(range(N_FOLD)):\n    model_path = f'{model_folder}/cfr_601_{i_fold}.pkl'\n    model = load_pickle(model_path)\n    df_test[f'pred_{i_fold}'] = model.predict(X_test)\ndf_test['pred_601_cfr'] = df_test[[f'pred_{i_fold}' for i_fold in range(N_FOLD)]].mean(axis=1)\ndf_result = df_result.merge(df_test[['stock_id', 'time_id', 'pred_601_cfr']], on=['stock_id', 'time_id'])\ndf_result.head()","metadata":{"execution":{"iopub.status.busy":"2021-09-25T21:23:33.878789Z","iopub.execute_input":"2021-09-25T21:23:33.879146Z","iopub.status.idle":"2021-09-25T21:24:42.632954Z","shell.execute_reply.started":"2021-09-25T21:23:33.879109Z","shell.execute_reply":"2021-09-25T21:24:42.63175Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Prediction - 601 - MLP","metadata":{}},{"cell_type":"code","source":"N_SEED = 5\nN_FOLD = 10\n\ndf_test = dt.fread('test_601_NN.csv').to_pandas()\nfea_cols = [f for f in df_test if f not in ['time_id', 'target', 'pred_NN', 'stock_id', 'row_id']]\nS_test = df_test['stock_id'].values\nX_test = df_test[fea_cols].values\nmodel_folder = '../input/optiver-final-601-mlp'\nfor i_seed in range(N_SEED):\n    for i_fold in range(N_FOLD):\n        scaler = load_pickle(f'{model_folder}/minmax_scaler_{i_seed}_{i_fold}.pkl')\n        X_test_scaled = scaler.transform(X_test)\n        model_path = f'{model_folder}/model_{i_seed}_{i_fold}.hdf5'\n        model = tf.keras.models.load_model(model_path, custom_objects={'mspe_loss': mspe_loss})\n        df_test[f'pred_{i_seed}_{i_fold}'] = model.predict([S_test, X_test_scaled], batch_size=1024)\n\ndf_test['pred_601_mlp'] = df_test[[f'pred_{i_seed}_{i_fold}' for i_seed in range(N_SEED) for i_fold in range(N_FOLD)]].mean(axis=1)\ndf_test['pred_601_mlp'] = inverse_target(df_test['pred_601_mlp'])\ndf_result = df_result.merge(df_test[['stock_id', 'time_id', 'pred_601_mlp']], on=['stock_id', 'time_id'])\ndf_result.head()","metadata":{"execution":{"iopub.status.busy":"2021-09-25T21:24:42.634775Z","iopub.execute_input":"2021-09-25T21:24:42.635241Z","iopub.status.idle":"2021-09-25T21:25:01.293978Z","shell.execute_reply.started":"2021-09-25T21:24:42.635193Z","shell.execute_reply":"2021-09-25T21:25:01.292995Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Prediction - 601 - 1dCNN","metadata":{}},{"cell_type":"code","source":"N_SEED = 5\nN_FOLD = 10\n\ndf_test = dt.fread('test_601_NN.csv').to_pandas()\nfea_cols = [f for f in df_test if f not in ['time_id', 'target', 'pred_NN', 'stock_id', 'row_id']]\nS_test = df_test['stock_id'].values\nX_test = df_test[fea_cols].values\nmodel_folder = '../input/optiver-final-601-1dcnn'\nfor i_seed in range(N_SEED):\n    for i_fold in range(N_FOLD):\n        scaler = load_pickle(f'{model_folder}/minmax_scaler_{i_seed}_{i_fold}.pkl')\n        X_test_scaled = scaler.transform(X_test)\n        model_path = f'{model_folder}/model_{i_seed}_{i_fold}.hdf5'\n        model = tf.keras.models.load_model(model_path, custom_objects={'mspe_loss': mspe_loss})\n        df_test[f'pred_{i_seed}_{i_fold}'] = model.predict([S_test, X_test_scaled], batch_size=1024)\n\ndf_test['pred_601_1dcnn'] = df_test[[f'pred_{i_seed}_{i_fold}' for i_seed in range(N_SEED) for i_fold in range(N_FOLD)]].mean(axis=1)\ndf_test['pred_601_1dcnn'] = inverse_target(df_test['pred_601_1dcnn'])\ndf_result = df_result.merge(df_test[['stock_id', 'time_id', 'pred_601_1dcnn']], on=['stock_id', 'time_id'])\ndf_result.head()","metadata":{"execution":{"iopub.status.busy":"2021-09-25T21:25:01.295151Z","iopub.execute_input":"2021-09-25T21:25:01.295444Z","iopub.status.idle":"2021-09-25T21:25:26.37478Z","shell.execute_reply.started":"2021-09-25T21:25:01.295416Z","shell.execute_reply":"2021-09-25T21:25:26.373647Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Prediction - 601 - TabNet","metadata":{}},{"cell_type":"code","source":"N_SEED = 3\nN_FOLD = 5\n\ndf_test = dt.fread('test_601_NN.csv').to_pandas()\ndf_test['stock_id'] = df_test['stock_id'].astype('int64')\nfea_cols = [f for f in df_test if f not in ['time_id', 'target', 'pred_NN', 'stock_id', 'row_id']]\nS_test = df_test['stock_id'].values\nX_test = df_test[['stock_id']+fea_cols].values\nmodel_folder = '../input/optiver-final-601-tabnet'\nfor i_seed in range(N_SEED):\n    for i_fold in range(N_FOLD):\n        scaler = load_pickle(f'{model_folder}/minmax_scaler_{i_seed}_{i_fold}.pkl')\n        X_test_scaled = X_test.copy()\n        X_test_scaled[:, 1:] = scaler.transform(X_test[:, 1:])\n        model_path = f'{model_folder}/model_{i_seed}_{i_fold}.xyz'\n        model = TabNetRegressor()\n        model.load_model(model_path)\n        df_test[f'pred_{i_seed}_{i_fold}'] = model.predict(X_test_scaled)\n\ndf_test['pred_601_tabnet'] = df_test[[f'pred_{i_seed}_{i_fold}' for i_seed in range(N_SEED) for i_fold in range(N_FOLD)]].mean(axis=1)\ndf_test['pred_601_tabnet'] = inverse_target(df_test['pred_601_tabnet'])\ndf_result = df_result.merge(df_test[['stock_id', 'time_id', 'pred_601_tabnet']], on=['stock_id', 'time_id'])\ndf_result.head()","metadata":{"execution":{"iopub.status.busy":"2021-09-25T21:25:26.376354Z","iopub.execute_input":"2021-09-25T21:25:26.376775Z","iopub.status.idle":"2021-09-25T21:25:28.329773Z","shell.execute_reply.started":"2021-09-25T21:25:26.376729Z","shell.execute_reply":"2021-09-25T21:25:28.328628Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del df_test, X_test\nx = gc.collect()","metadata":{"execution":{"iopub.status.busy":"2021-09-25T21:25:28.331341Z","iopub.execute_input":"2021-09-25T21:25:28.331761Z","iopub.status.idle":"2021-09-25T21:25:28.766165Z","shell.execute_reply.started":"2021-09-25T21:25:28.331717Z","shell.execute_reply":"2021-09-25T21:25:28.765184Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Public - KFold","metadata":{}},{"cell_type":"code","source":"def read_train_test():\n    # Function to read our base train and test set\n    \n    train = pd.read_csv('../input/optiver-realized-volatility-prediction/train.csv')\n    test = pd.read_csv('../input/optiver-realized-volatility-prediction/test.csv')\n\n    # Create a key to merge with book and trade data\n    train['row_id'] = train['stock_id'].astype(str) + '-' + train['time_id'].astype(str)\n    test['row_id'] = test['stock_id'].astype(str) + '-' + test['time_id'].astype(str)\n    print(f'Our training set has {train.shape[0]} rows')\n    print(f'Our test set has {test.shape[0]} rows')\n    print(f'Our training set has {train.isna().sum().sum()} missing values')\n    print(f'Our test set has {test.isna().sum().sum()} missing values')\n    \n    return train, test","metadata":{"execution":{"iopub.status.busy":"2021-09-25T21:25:28.775075Z","iopub.execute_input":"2021-09-25T21:25:28.775863Z","iopub.status.idle":"2021-09-25T21:25:28.784071Z","shell.execute_reply.started":"2021-09-25T21:25:28.775808Z","shell.execute_reply":"2021-09-25T21:25:28.782735Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train, test = read_train_test()","metadata":{"execution":{"iopub.status.busy":"2021-09-25T21:25:28.785831Z","iopub.execute_input":"2021-09-25T21:25:28.786235Z","iopub.status.idle":"2021-09-25T21:25:30.200208Z","shell.execute_reply.started":"2021-09-25T21:25:28.786198Z","shell.execute_reply":"2021-09-25T21:25:30.199124Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# data directory\ndata_dir = '../input/optiver-realized-volatility-prediction/'\n\ndef calc_wap1(df):\n    # Function to calculate first WAP\n    wap = (df['bid_price1'] * df['ask_size1'] + df['ask_price1'] * df['bid_size1']) / (df['bid_size1'] + df['ask_size1'])\n    return wap\n\ndef calc_wap2(df):\n    # Function to calculate second WAP\n    wap = (df['bid_price2'] * df['ask_size2'] + df['ask_price2'] * df['bid_size2']) / (df['bid_size2'] + df['ask_size2'])\n    return wap\n\ndef log_return(series):\n    # Function to calculate the log of the return\n    return np.log(series).diff()\n\ndef realized_volatility(series):\n    # Calculate the realized volatility\n    return np.sqrt(np.sum(series**2))\n\ndef count_unique(series):\n    # Function to count unique elements of a series\n    return len(np.unique(series))\n\ndef book_preprocessor(file_path):\n    # Function to preprocess book data (for each stock id)\n    \n    df = pd.read_parquet(file_path)\n    \n    # Calculate Wap\n    df['wap1'] = calc_wap1(df)\n    df['wap2'] = calc_wap2(df)\n    \n    # Calculate log returns\n    df['log_return1'] = df.groupby(['time_id'])['wap1'].apply(log_return)\n    df['log_return2'] = df.groupby(['time_id'])['wap2'].apply(log_return)\n    \n    # Calculate wap balance\n    df['wap_balance'] = abs(df['wap1'] - df['wap2'])\n    \n    # Calculate spread\n    df['price_spread'] = (df['ask_price1'] - df['bid_price1']) / ((df['ask_price1'] + df['bid_price1']) / 2)\n    df['price_spread2'] = (df['ask_price2'] - df['bid_price2']) / ((df['ask_price2'] + df['bid_price2']) / 2)\n    df['bid_spread'] = df['bid_price1'] - df['bid_price2']\n    df['ask_spread'] = df['ask_price1'] - df['ask_price2']\n    df[\"bid_ask_spread\"] = abs(df['bid_spread'] - df['ask_spread'])\n    df['total_volume'] = (df['ask_size1'] + df['ask_size2']) + (df['bid_size1'] + df['bid_size2'])\n    df['volume_imbalance'] = abs((df['ask_size1'] + df['ask_size2']) - (df['bid_size1'] + df['bid_size2']))\n    \n    # Dict for aggregations\n    create_feature_dict = {\n        'wap1': [np.sum, np.mean, np.std],\n        'wap2': [np.sum, np.mean, np.std],\n        'log_return1': [np.sum, realized_volatility, np.mean, np.std],\n        'log_return2': [np.sum, realized_volatility, np.mean, np.std],\n        'wap_balance': [np.sum, np.mean, np.std],\n        'price_spread':[np.sum, np.mean, np.std],\n        'price_spread2':[np.sum, np.mean, np.std],\n        'bid_spread':[np.sum, np.mean, np.std],\n        'ask_spread':[np.sum, np.mean, np.std],\n        'total_volume':[np.sum, np.mean, np.std],\n        'volume_imbalance':[np.sum, np.mean, np.std],\n        \"bid_ask_spread\":[np.sum, np.mean, np.std],\n    }\n    \n    def get_stats_window(seconds_in_bucket, add_suffix = False):\n        # Function to get group stats for different windows (seconds in bucket)\n        \n        # Group by the window\n        df_feature = df[df['seconds_in_bucket'] >= seconds_in_bucket].groupby(['time_id']).agg(create_feature_dict).reset_index()\n        \n        # Rename columns joining suffix\n        df_feature.columns = ['_'.join(col) for col in df_feature.columns]\n        \n        # Add a suffix to differentiate windows\n        if add_suffix:\n            df_feature = df_feature.add_suffix('_' + str(seconds_in_bucket))\n        return df_feature\n    \n    # Get the stats for different windows\n    df_feature = get_stats_window(seconds_in_bucket = 0, add_suffix = False)\n    df_feature_400 = get_stats_window(seconds_in_bucket = 400, add_suffix = True)\n    df_feature_300 = get_stats_window(seconds_in_bucket = 300, add_suffix = True)\n    df_feature_200 = get_stats_window(seconds_in_bucket = 200, add_suffix = True)\n    \n    # Merge all\n    df_feature = df_feature.merge(df_feature_400, how = 'left', left_on = 'time_id_', right_on = 'time_id__400')\n    df_feature = df_feature.merge(df_feature_300, how = 'left', left_on = 'time_id_', right_on = 'time_id__300')\n    df_feature = df_feature.merge(df_feature_200, how = 'left', left_on = 'time_id_', right_on = 'time_id__200')\n\n    # Drop unnecesary time_ids\n    df_feature.drop(['time_id__400', 'time_id__300', 'time_id__200'], axis = 1, inplace = True)\n    \n    \n    # Create row_id so we can merge\n    stock_id = file_path.split('=')[1]\n    df_feature['row_id'] = df_feature['time_id_'].apply(lambda x: f'{stock_id}-{x}')\n    df_feature.drop(['time_id_'], axis = 1, inplace = True)\n    \n    return df_feature\n\n\ndef trade_preprocessor(file_path):\n    # Function to preprocess trade data (for each stock id)\n    \n    df = pd.read_parquet(file_path)\n    df['log_return'] = df.groupby('time_id')['price'].apply(log_return)\n    \n    # Dict for aggregations\n    create_feature_dict = {\n        'log_return':[realized_volatility],\n        'seconds_in_bucket':[count_unique],\n        'size':[np.sum, realized_volatility, np.mean, np.std, np.max, np.min],\n        'order_count':[np.mean,np.sum,np.max],\n    }\n    \n    def get_stats_window(seconds_in_bucket, add_suffix = False):\n        # Function to get group stats for different windows (seconds in bucket)\n        \n        # Group by the window\n        df_feature = df[df['seconds_in_bucket'] >= seconds_in_bucket].groupby(['time_id']).agg(create_feature_dict).reset_index()\n        \n        # Rename columns joining suffix\n        df_feature.columns = ['_'.join(col) for col in df_feature.columns]\n        \n        # Add a suffix to differentiate windows\n        if add_suffix:\n            df_feature = df_feature.add_suffix('_' + str(seconds_in_bucket))\n        return df_feature\n    \n    # Get the stats for different windows\n    df_feature = get_stats_window(seconds_in_bucket = 0, add_suffix = False)\n    df_feature_400 = get_stats_window(seconds_in_bucket = 400, add_suffix = True)\n    df_feature_300 = get_stats_window(seconds_in_bucket = 300, add_suffix = True)\n    df_feature_200 = get_stats_window(seconds_in_bucket = 200, add_suffix = True)\n    \n    def tendency(price, vol):    \n        df_diff = np.diff(price)\n        val = (df_diff/price[1:])*100\n        power = np.sum(val*vol[1:])\n        return(power)\n    \n    lis = []\n    for n_time_id in df['time_id'].unique():\n        df_id = df[df['time_id'] == n_time_id]        \n        tendencyV = tendency(df_id['price'].values, df_id['size'].values)      \n        f_max = np.sum(df_id['price'].values > np.mean(df_id['price'].values))\n        f_min = np.sum(df_id['price'].values < np.mean(df_id['price'].values))\n        df_max =  np.sum(np.diff(df_id['price'].values) > 0)\n        df_min =  np.sum(np.diff(df_id['price'].values) < 0)\n        abs_diff = np.median(np.abs( df_id['price'].values - np.mean(df_id['price'].values)))        \n        energy = np.mean(df_id['price'].values**2)\n        iqr_p = np.percentile(df_id['price'].values,75) - np.percentile(df_id['price'].values,25)\n        abs_diff_v = np.median(np.abs( df_id['size'].values - np.mean(df_id['size'].values)))        \n        energy_v = np.sum(df_id['size'].values**2)\n        iqr_p_v = np.percentile(df_id['size'].values,75) - np.percentile(df_id['size'].values,25)\n        \n        lis.append({'time_id':n_time_id,'tendency':tendencyV,'f_max':f_max,'f_min':f_min,'df_max':df_max,'df_min':df_min,\n                   'abs_diff':abs_diff,'energy':energy,'iqr_p':iqr_p,'abs_diff_v':abs_diff_v,'energy_v':energy_v,'iqr_p_v':iqr_p_v})\n    \n    df_lr = pd.DataFrame(lis)\n        \n   \n    df_feature = df_feature.merge(df_lr, how = 'left', left_on = 'time_id_', right_on = 'time_id')\n    \n    # Merge all\n    df_feature = df_feature.merge(df_feature_400, how = 'left', left_on = 'time_id_', right_on = 'time_id__400')\n    df_feature = df_feature.merge(df_feature_300, how = 'left', left_on = 'time_id_', right_on = 'time_id__300')\n    df_feature = df_feature.merge(df_feature_200, how = 'left', left_on = 'time_id_', right_on = 'time_id__200')\n\n    # Drop unnecesary time_ids\n    df_feature.drop(['time_id__400', 'time_id__300', 'time_id__200','time_id'], axis = 1, inplace = True)\n    df_feature = df_feature.add_prefix('trade_')\n    stock_id = file_path.split('=')[1]\n    df_feature['row_id'] = df_feature['trade_time_id_'].apply(lambda x:f'{stock_id}-{x}')\n    df_feature.drop(['trade_time_id_'], axis = 1, inplace = True)\n    \n    def order_sum(df, sec:str):\n        new_col = 'size_tau' + sec\n        bucket_col = 'trade_seconds_in_bucket_count_unique' + sec\n        df[new_col] = np.sqrt(1/df[bucket_col])\n        \n        new_col2 = 'size_tau2' + sec\n        order_col = 'trade_order_count_sum' + sec\n        df[new_col2] = np.sqrt(1/df[order_col])\n        \n        if sec == '400_':\n            df['size_tau2_d'] = df['size_tau2_400'] - df['size_tau2']\n        \n\n    \n    for sec in ['','_200','_300','_400']:\n        order_sum(df_feature, sec)\n        \n    df_feature['size_tau2_d'] = df_feature['size_tau2_400'] - df_feature['size_tau2']\n    \n    return df_feature\n\n\ndef get_time_stock(df):\n    # Function to get group stats for the stock_id and time_id\n    \n    # Get realized volatility columns\n    vol_cols = ['log_return1_realized_volatility', 'log_return2_realized_volatility', 'log_return1_realized_volatility_400', 'log_return2_realized_volatility_400', \n                'log_return1_realized_volatility_300', 'log_return2_realized_volatility_300', 'log_return1_realized_volatility_200', 'log_return2_realized_volatility_200', \n                'trade_log_return_realized_volatility', 'trade_log_return_realized_volatility_400', 'trade_log_return_realized_volatility_300', 'trade_log_return_realized_volatility_200']\n\n    # Group by the stock id\n    df_stock_id = df.groupby(['stock_id'])[vol_cols].agg(['mean', 'std', 'max', 'min', ]).reset_index()\n    \n    # Rename columns joining suffix\n    df_stock_id.columns = ['_'.join(col) for col in df_stock_id.columns]\n    df_stock_id = df_stock_id.add_suffix('_' + 'stock')\n\n    # Group by the stock id\n    df_time_id = df.groupby(['time_id'])[vol_cols].agg(['mean', 'std', 'max', 'min', ]).reset_index()\n    \n    # Rename columns joining suffix\n    df_time_id.columns = ['_'.join(col) for col in df_time_id.columns]\n    df_time_id = df_time_id.add_suffix('_' + 'time')\n    \n    # Merge with original dataframe\n    df = df.merge(df_stock_id, how = 'left', left_on = ['stock_id'], right_on = ['stock_id__stock'])\n    df = df.merge(df_time_id, how = 'left', left_on = ['time_id'], right_on = ['time_id__time'])\n    df.drop(['stock_id__stock', 'time_id__time'], axis = 1, inplace = True)\n    \n    return df\n\ndef create_agg_features(train, test):\n\n    # Making agg features\n\n    train_p = pd.read_csv('../input/optiver-realized-volatility-prediction/train.csv')\n    train_p = train_p.pivot(index='time_id', columns='stock_id', values='target')\n    corr = train_p.corr()\n    ids = corr.index\n    kmeans = KMeans(n_clusters=7, random_state=0).fit(corr.values)\n    l = []\n    for n in range(7):\n        l.append ( [ (x-1) for x in ( (ids+1)*(kmeans.labels_ == n)) if x > 0] )\n\n    mat = []\n    matTest = []\n    n = 0\n    for ind in l:\n        newDf = train.loc[train['stock_id'].isin(ind) ]\n        newDf = newDf.groupby(['time_id']).agg(np.nanmean)\n        newDf.loc[:,'stock_id'] = str(n)+'c1'\n        mat.append ( newDf )\n        newDf = test.loc[test['stock_id'].isin(ind) ]    \n        newDf = newDf.groupby(['time_id']).agg(np.nanmean)\n        newDf.loc[:,'stock_id'] = str(n)+'c1'\n        matTest.append ( newDf )\n        n+=1\n\n    mat1 = pd.concat(mat).reset_index()\n    mat1.drop(columns=['target'],inplace=True)\n    mat2 = pd.concat(matTest).reset_index()\n    \n    mat2 = pd.concat([mat2,mat1.loc[mat1.time_id==5]])\n    \n    mat1 = mat1.pivot(index='time_id', columns='stock_id')\n    mat1.columns = [\"_\".join(x) for x in mat1.columns.ravel()]\n    mat1.reset_index(inplace=True)\n    \n    mat2 = mat2.pivot(index='time_id', columns='stock_id')\n    mat2.columns = [\"_\".join(x) for x in mat2.columns.ravel()]\n    mat2.reset_index(inplace=True)\n    \n    prefix = ['log_return1_realized_volatility', 'total_volume_mean', 'trade_size_mean', 'trade_order_count_mean','price_spread_mean','bid_spread_mean','ask_spread_mean',\n              'volume_imbalance_mean', 'bid_ask_spread_mean','size_tau2']\n    selected_cols=mat1.filter(regex='|'.join(f'^{x}.(0|1|3|4|6)c1' for x in prefix)).columns.tolist()\n    selected_cols.append('time_id')\n    \n    train_m = pd.merge(train,mat1[selected_cols],how='left',on='time_id')\n    test_m = pd.merge(test,mat2[selected_cols],how='left',on='time_id')\n    \n    # filling missing values with train means\n\n    features = [col for col in train_m.columns.tolist() if col not in ['time_id','target','row_id']]\n    train_m[features] = train_m[features].fillna(train_m[features].mean())\n    test_m[features] = test_m[features].fillna(train_m[features].mean())\n\n    return train_m, test_m\n    \n    \ndef preprocessor(list_stock_ids, is_train = True):\n    # Funtion to make preprocessing function in parallel (for each stock id)\n    \n    # Parrallel for loop\n    def for_joblib(stock_id):\n        # Train\n        if is_train:\n            file_path_book = data_dir + \"book_train.parquet/stock_id=\" + str(stock_id)\n            file_path_trade = data_dir + \"trade_train.parquet/stock_id=\" + str(stock_id)\n        # Test\n        else:\n            file_path_book = data_dir + \"book_test.parquet/stock_id=\" + str(stock_id)\n            file_path_trade = data_dir + \"trade_test.parquet/stock_id=\" + str(stock_id)\n    \n        # Preprocess book and trade data and merge them\n        df_tmp = pd.merge(book_preprocessor(file_path_book), trade_preprocessor(file_path_trade), on = 'row_id', how = 'left')\n        \n        # Return the merge dataframe\n        return df_tmp\n    \n    # Use parallel api to call paralle for loop\n    df = Parallel(n_jobs = -1, verbose = 1)(delayed(for_joblib)(stock_id) for stock_id in list_stock_ids)\n    \n    # Concatenate all the dataframes that return from Parallel\n    df = pd.concat(df, ignore_index = True)\n    \n    return df","metadata":{"execution":{"iopub.status.busy":"2021-09-25T21:25:30.201815Z","iopub.execute_input":"2021-09-25T21:25:30.202147Z","iopub.status.idle":"2021-09-25T21:25:30.276092Z","shell.execute_reply.started":"2021-09-25T21:25:30.202114Z","shell.execute_reply":"2021-09-25T21:25:30.274983Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get unique stock ids \ntrain_stock_ids = train['stock_id'].unique()\n\n# Preprocess them using Parallel and our single stock id functions\n# train_ = preprocessor(train_stock_ids, is_train = True)\ntrain_ = pd.read_csv('../input/optiver-public-kfold-train/kfold_train_.csv')\ntrain = train.merge(train_, on = ['row_id'], how = 'left')\n\n# Get unique stock ids \ntest_stock_ids = test['stock_id'].unique()\n\n# Preprocess them using Parallel and our single stock id functions\ntest_ = preprocessor(test_stock_ids, is_train = False)\ntest = test.merge(test_, on = ['row_id'], how = 'left')\n\n# Get group stats of time_id and stock_id\ntrain = get_time_stock(train)\ntest = get_time_stock(test)\n\n# Fill inf values\ntrain.replace([np.inf, -np.inf], np.nan,inplace=True)\ntest.replace([np.inf, -np.inf], np.nan,inplace=True)\n\n# Aggregating some features\ntrain, test = create_agg_features(train,test)\ntest = test.loc[test['row_id'].isin(df_result['row_id'])]","metadata":{"execution":{"iopub.status.busy":"2021-09-25T21:25:30.277392Z","iopub.execute_input":"2021-09-25T21:25:30.277733Z","iopub.status.idle":"2021-09-25T21:26:31.315992Z","shell.execute_reply.started":"2021-09-25T21:25:30.2777Z","shell.execute_reply":"2021-09-25T21:26:31.314911Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = train.drop(['row_id', 'target', 'time_id'], axis = 1)\ny = train['target']\nX_test=test.copy()\nX_test.drop(['time_id','row_id'], axis=1,inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-09-25T21:26:31.317589Z","iopub.execute_input":"2021-09-25T21:26:31.31792Z","iopub.status.idle":"2021-09-25T21:26:31.709673Z","shell.execute_reply.started":"2021-09-25T21:26:31.317886Z","shell.execute_reply":"2021-09-25T21:26:31.708877Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nunique = X.nunique()\ntypes = X.dtypes\n\ncategorical_columns = []\ncategorical_dims =  {}\n\nscalers = dict()\nfor col in X.columns:\n    if col == 'stock_id':\n        l_enc = LabelEncoder()\n        X[col] = l_enc.fit_transform(X[col].values)\n        X_test[col] = l_enc.transform(X_test[col].values)\n        categorical_columns.append(col)\n        categorical_dims[col] = len(l_enc.classes_)\n        dump(l_enc, 'l_enc')\n    else:\n        scaler = StandardScaler()\n        X[col] = scaler.fit_transform(X[col].values.reshape(-1, 1))\n        X_test[col] = scaler.transform(X_test[col].values.reshape(-1, 1))\n        scalers[col] = scaler\n\ndump(scalers, 'scalers')\ncat_idxs = [ i for i, f in enumerate(X.columns.tolist()) if f in categorical_columns]\ncat_dims = [ categorical_dims[f] for i, f in enumerate(X.columns.tolist()) if f in categorical_columns]","metadata":{"execution":{"iopub.status.busy":"2021-09-25T21:26:31.710768Z","iopub.execute_input":"2021-09-25T21:26:31.711189Z","iopub.status.idle":"2021-09-25T21:26:43.794594Z","shell.execute_reply.started":"2021-09-25T21:26:31.711158Z","shell.execute_reply":"2021-09-25T21:26:43.793466Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Prediction - KFold - TabNet","metadata":{}},{"cell_type":"code","source":"N_FOLD = 5\n\ntest_predictions = np.zeros(X_test.shape[0])\nmodel_folder = '../input/optiver-public-kfold-tabnet'\nfor i_fold in range(N_FOLD):\n    model_path = f'{model_folder}/fold{i_fold}.xyz'\n    model = TabNetRegressor()\n    model.load_model(model_path)\n    test_predictions += model.predict(X_test.values).flatten() / N_FOLD\n\ndf_result['pred_kfold_tabnet'] = test_predictions\ndf_result.head()","metadata":{"execution":{"iopub.status.busy":"2021-09-25T21:26:43.796192Z","iopub.execute_input":"2021-09-25T21:26:43.796528Z","iopub.status.idle":"2021-09-25T21:26:44.517224Z","shell.execute_reply.started":"2021-09-25T21:26:43.796494Z","shell.execute_reply":"2021-09-25T21:26:44.516131Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Make submission","metadata":{}},{"cell_type":"code","source":"pred_cols = ['pred_501_mlp', 'pred_501_wavenet', 'pred_501_1dcnn', 'pred_501_tabnet', 'pred_501_unet']\ncoef_ = [ 0.4, 0, 0.4, 0.1, 0.1 ]\ndf_result[pred_cols] = transform_target(df_result[pred_cols])\ndf_result['fpred_501'] = np.sum(coef_ * df_result[pred_cols].values, axis=1)\ndf_result['fpred_501'] = inverse_target(df_result['fpred_501'])\n\npred_cols = ['pred_601_tabnet', 'pred_601_mlp', 'pred_601_lgb', 'pred_601_cat', 'pred_601_1dcnn', 'pred_601_cfr']\ncoef_ = [ 0.2, 0.4, 0.15, 0, 0.25, 0 ]\ndf_result[pred_cols] = transform_target(df_result[pred_cols])\ndf_result['fpred_601'] = np.sum(coef_ * df_result[pred_cols].values, axis=1)\ndf_result['fpred_601'] = inverse_target(df_result['fpred_601'])\n\npred_cols = ['fpred_501', 'fpred_601', 'pred_kfold_tabnet']\ncoef_ = [ 0.6, 0.3, 0.1 ]\ndf_result[pred_cols] = transform_target(df_result[pred_cols])\ndf_result['target'] = np.sum(coef_ * df_result[pred_cols].values, axis=1)\ndf_result['target'] = inverse_target(df_result['target'])\n\ndf_result.head()","metadata":{"execution":{"iopub.status.busy":"2021-09-25T21:26:44.526358Z","iopub.execute_input":"2021-09-25T21:26:44.526868Z","iopub.status.idle":"2021-09-25T21:26:44.57286Z","shell.execute_reply.started":"2021-09-25T21:26:44.526817Z","shell.execute_reply":"2021-09-25T21:26:44.571912Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Make submission\ndf_submission = df_result[['row_id', 'target']]\ndf_submission.to_csv('submission.csv', index=False)\nprint(df_submission.shape)\ndf_submission.head()","metadata":{"execution":{"iopub.status.busy":"2021-09-25T21:26:44.576105Z","iopub.execute_input":"2021-09-25T21:26:44.576424Z","iopub.status.idle":"2021-09-25T21:26:44.596402Z","shell.execute_reply.started":"2021-09-25T21:26:44.576395Z","shell.execute_reply":"2021-09-25T21:26:44.595303Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}