{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from IPython.core.display import display, HTML\n\nimport pandas as pd\nimport numpy as np # linear algebra\nimport glob\nimport os\nimport gc\n\nfrom joblib import Parallel, delayed\n\nfrom sklearn import preprocessing, model_selection\nimport lightgbm as lgb\n\nfrom sklearn.metrics import r2_score\n\nimport matplotlib.pyplot as plt \nimport seaborn as sns\n\nfrom scipy.stats import binned_statistic\n\nfrom functools import partial\nfrom numba import jit\n\nimport tensorflow as tf \nfrom tensorflow import keras\nfrom keras import layers\n\npath_root = '../input/optiver-realized-volatility-prediction'\npath_data = '../input/optiver-realized-volatility-prediction'\npath_submissions = '/'\n\ntarget_name = 'target'\nscores_folds = {}","metadata":{"execution":{"iopub.status.busy":"2021-10-13T20:12:44.576392Z","iopub.execute_input":"2021-10-13T20:12:44.576811Z","iopub.status.idle":"2021-10-13T20:12:53.11982Z","shell.execute_reply.started":"2021-10-13T20:12:44.576721Z","shell.execute_reply":"2021-10-13T20:12:53.118717Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Time Binned Data Gen","metadata":{}},{"cell_type":"code","source":"def log_return(list_stock_prices):\n    return np.log(list_stock_prices).diff() \n\ndef diff(list_stock_prices):\n    return list_stock_prices.diff() \n\ndef realized_volatility(series_log_return):\n    return np.sqrt(np.sum(series_log_return**2))\n\ndef realized_absvol(series_log_return):\n    return np.sum(np.abs(series_log_return))\n\n@jit(nopython=True)\ndef finish_bin(bin_width, bin_sum:float, bin_weight:float, last_val, last_weight, last_time)->float: \n    dt = bin_width - last_time%bin_width\n    \n    bin_weight += 1.0*last_weight*dt\n    bin_sum    += 1.0*last_weight*dt*last_val\n    \n    return float(bin_sum/(bin_weight + 1e-8))\n    \nimport typing\n\n\n\n@jit()\ndef binned_time_weighted_mean_stat(t, tid, x, w, bin_width, n_bins, n_rows):\n    #bin_list = [[]]\n    z = np.zeros( (n_rows,n_bins) )\n    \n    prev_time   = 0\n    prev_weight = 0.0\n    prev_val    = 0.0\n    \n    bin_sum = 0.0\n    bin_weight = 0.0\n\n    zk  = 0\n    zkk = 0\n    for k in range(t.shape[0]):\n        \n        if tid[k] != tid[max(k-1,0)]:\n            z[zk, zkk] = float(finish_bin(bin_width, bin_sum, bin_weight, prev_val, prev_weight, prev_time))\n            zkk += 1\n            \n            while zkk < z.shape[1]:\n                z[zk, zkk] = prev_val\n                zkk += 1\n            zk += 1\n            zkk = 0\n            \n            prev_time  = 0\n            bin_sum    = 0.0\n            bin_weight = 0.0            \n            \n        if int(t[k]//bin_width) != int(prev_time//bin_width):\n            \n            z[zk, zkk] = float(finish_bin(bin_width, bin_sum, bin_weight, prev_val, prev_weight, prev_time))\n            zkk += 1\n            \n            while zkk < t[k]//bin_width:\n                z[zk, zkk] = prev_val\n                zkk += 1\n            \n            prev_time  = bin_width*(t[k]//bin_width)\n            bin_sum    = 0.0\n            bin_weight = 0.0\n        \n        bin_sum    += prev_val*prev_weight*(t[k] - prev_time)\n        bin_weight +=          prev_weight*(t[k] - prev_time)\n        \n        prev_time   = t[k]\n        prev_val    = x[k]\n        prev_weight = w[k]\n            \n    z[zk, zkk] = finish_bin(bin_width, bin_sum, bin_weight, prev_val, prev_weight, prev_time)\n    \n    for zkk in range(zkk+1, z.shape[1]):\n        z[zk, zkk] = prev_val\n                             \n    return z\n\n\n\n@jit()\ndef binned_sum_impulse(t, tid, x, bin_width, n_bins, n_rows):\n    z = np.zeros( (n_rows,n_bins) )\n\n    zk  = 0\n    for k in range(t.shape[0]):\n        \n        if tid[k] != tid[max(k-1,0)]:\n            zk = zk + 1\n            \n        z[zk, int(t[k]//bin_width)] += x[k]            \n    \n    return z\n\n\n\ndef full_book_wap(p0, df, lvl, n_step=62, step_size=1/9):\n    p = np.array(p0)\n    la2 = np.array(df['log_ask2'])\n    la1 = np.array(df['log_ask1'])\n    lb1 = np.array(df['log_bid1'])\n    lb2 = np.array(df['log_bid2'])\n    as2 = np.array(df['ask_size2'])\n    as1 = np.array(df['ask_size1'])\n    bs1 = np.array(df['bid_size1'])\n    bs2 = np.array(df['bid_size2'])\n    \n    \n    s = (-1)**lvl\n    for k in range(64):\n        ixa2 = 1.0/( 1000*( p - la2 ) )\n        ixa1 = 1.0/( 1000*( p - la1 ) )\n        ixb1 = 1.0/( 1000*( p - lb1 ) )\n        ixb2 = 1.0/( 1000*( p - lb2 ) )\n\n        p = p + step_size * (\n              (   ( bs2*ixb2**(lvl+1) + bs1*ixb1**(lvl+1) )\n              + s*( as1*ixa1**(lvl+1) + as2*ixa2**(lvl+1) ) )\n            / (   ( bs2*ixb2**(lvl+2) + bs1*ixb1**(lvl+2) )\n              + s*( as1*ixa1**(lvl+2) + as2*ixa2**(lvl+2) ) ) \n            )/( (lvl + 1) * 1000 )\n    return p\n   \n\ndef full_book_wap_bisect(df, lvl, n_step=22):\n    la2 = np.array(df['log_ask2'])\n    la1 = np.array(df['log_ask1'])\n    lb1 = np.array(df['log_bid1'])\n    lb2 = np.array(df['log_bid2'])\n    \n    as2 = np.array(df['ask_size2'])\n    as1 = np.array(df['ask_size1'])\n    bs1 = np.array(df['bid_size1'])\n    bs2 = np.array(df['bid_size2'])\n    \n    ub = la1\n    lb = lb1\n    s0 = ub - lb\n    \n    s = (-1)**lvl\n    for k in range(n_step):\n        p = (ub + lb)/2.0\n        ixa2 = 1.0/( 1000*( p - la2 ) )\n        ixa1 = 1.0/( 1000*( p - la1 ) )\n        ixb1 = 1.0/( 1000*( p - lb1 ) )\n        ixb2 = 1.0/( 1000*( p - lb2 ) )\n\n        f  = -(   ( bs2*ixb2**(lvl+1) + bs1*ixb1**(lvl+1) )\n              + s*( as1*ixa1**(lvl+1) + as2*ixa2**(lvl+1) ) )\n        \n\n        dub = - (ub-lb)/2.0*(f>=0)\n        dlb =   (ub-lb)/2.0*(f< 0)\n        \n        \n        ub = ub + dub\n        lb = lb + dlb\n\n    p = (ub + lb)/2.0    \n        \n    return p\n   \n\n\n\nfrom time import time\n\n\n\ndef get_stock_stat(stock_id : int, dataType = 'train'):\n    key = ['stock_id', 'time_id', 'seconds_in_bucket']\n    \n    #Book features\n    df_book = pd.read_parquet(os.path.join(path_data, 'book_{}.parquet/stock_id={}/'.format(dataType, stock_id)))\n    \n    df_book['stock_id'] = stock_id\n        \n    cols = key + [col for col in df_book.columns if col not in key]\n    df_book = df_book[cols]\n    \n    \n    df_book['ask_vol1'] = df_book['ask_price1']*df_book['ask_size1']\n    df_book['ask_vol2'] = df_book['ask_price2']*df_book['ask_size2']\n    df_book['bid_vol1'] = df_book['bid_price1']*df_book['bid_size1']\n    df_book['bid_vol2'] = df_book['bid_price2']*df_book['bid_size2']\n    \n    df_book['log_ask1'] = np.log( df_book['ask_price1'] )\n    df_book['log_ask2'] = np.log( df_book['ask_price2'] )\n    df_book['log_bid1'] = np.log( df_book['bid_price1'] )\n    df_book['log_bid2'] = np.log( df_book['bid_price2'] )\n    \n    df_book['wap1'] = ( df_book['log_bid1'] * df_book['ask_size1'] + df_book['log_ask1'] * df_book['bid_size1'] ) / (df_book['bid_size1'] + df_book['ask_size1'])\n    df_book['wap2'] = ( df_book['log_bid2'] * df_book['ask_size2'] + df_book['log_ask2'] * df_book['bid_size2'] ) / (df_book['bid_size2'] + df_book['ask_size2'])\n    \n    df_book['wapq0'] = full_book_wap_bisect( df_book, lvl=0)\n    df_book['wapq1'] = full_book_wap_bisect( df_book, lvl=1)\n    df_book['wapq2'] = full_book_wap_bisect( df_book, lvl=2)\n    df_book['wapq3'] = full_book_wap_bisect( df_book, lvl=3)\n    df_book['wapq4'] = full_book_wap_bisect( df_book, lvl=4)\n    \n\n    df_book['liquidity0'] = (\n                  df_book['bid_vol1']/( 1000*(df_book['wapq0'] - df_book['log_bid1']) )\n                + df_book['bid_vol2']/( 1000*(df_book['wapq0'] - df_book['log_bid2']) )\n                - df_book['ask_vol1']/( 1000*(df_book['wapq0'] - df_book['log_ask1']) )\n                - df_book['ask_vol2']/( 1000*(df_book['wapq0'] - df_book['log_ask2']) )\n    )\n    \n    df_book['liquidity1'] = (\n                  df_book['bid_vol1']/( 1000*(df_book['wapq1'] - df_book['log_bid1']) )\n                + df_book['bid_vol2']/( 1000*(df_book['wapq1'] - df_book['log_bid2']) )\n                - df_book['ask_vol1']/( 1000*(df_book['wapq1'] - df_book['log_ask1']) )\n                - df_book['ask_vol2']/( 1000*(df_book['wapq1'] - df_book['log_ask2']) )\n    )\n    \n    \n    df_book['liquidity2'] = (\n                  df_book['bid_vol1']/( 1000*(df_book['wapq2'] - df_book['log_bid1']) )**2\n                + df_book['bid_vol2']/( 1000*(df_book['wapq2'] - df_book['log_bid2']) )**2\n                + df_book['ask_vol1']/( 1000*(df_book['wapq2'] - df_book['log_ask1']) )**2\n                + df_book['ask_vol2']/( 1000*(df_book['wapq2'] - df_book['log_ask2']) )**2\n    )\n    df_book['liquidity2f1'] = (\n                  df_book['bid_vol1']/( 1000*(df_book['wap1'] - df_book['log_bid1']) )**2\n                + df_book['ask_vol1']/( 1000*(df_book['wap1'] - df_book['log_ask1']) )**2\n    )\n    \n    \n    df_book['liquidity3'] = (\n                  df_book['bid_vol1']/( 1000*(df_book['wapq3'] - df_book['log_bid1']) )**3\n                + df_book['bid_vol2']/( 1000*(df_book['wapq3'] - df_book['log_bid2']) )**3\n                - df_book['ask_vol1']/( 1000*(df_book['wapq3'] - df_book['log_ask1']) )**3\n                - df_book['ask_vol2']/( 1000*(df_book['wapq3'] - df_book['log_ask2']) )**3\n    )\n    \n    df_book['liquidity4'] = (\n                  df_book['bid_vol1']/( 1000*(df_book['wapq4'] - df_book['log_bid1']) )**4\n                + df_book['bid_vol2']/( 1000*(df_book['wapq4'] - df_book['log_bid2']) )**4\n                + df_book['ask_vol1']/( 1000*(df_book['wapq4'] - df_book['log_ask1']) )**4\n                + df_book['ask_vol2']/( 1000*(df_book['wapq4'] - df_book['log_ask2']) )**4\n    )\n    \n    df_book['liquidity6'] = (\n                  df_book['bid_vol1']/( 1000*(df_book['wapq4'] - df_book['log_bid1']) )**6\n                + df_book['bid_vol2']/( 1000*(df_book['wapq4'] - df_book['log_bid2']) )**6\n                + df_book['ask_vol1']/( 1000*(df_book['wapq4'] - df_book['log_ask1']) )**6\n                + df_book['ask_vol2']/( 1000*(df_book['wapq4'] - df_book['log_ask2']) )**6\n    )\n    df_book['liquidity10'] = (\n                  df_book['bid_vol1']/( 1000*(df_book['wapq4'] - df_book['log_bid1']) )**10\n                + df_book['bid_vol2']/( 1000*(df_book['wapq4'] - df_book['log_bid2']) )**10\n                + df_book['ask_vol1']/( 1000*(df_book['wapq4'] - df_book['log_ask1']) )**10\n                + df_book['ask_vol2']/( 1000*(df_book['wapq4'] - df_book['log_ask2']) )**10\n    )\n    \n    df_book['spread']     = df_book['log_ask1'] - df_book['log_bid1']\n    df_book['inv_spread'] = (df_book['log_ask1'] - df_book['log_bid1'])**-2\n    df_book['log_spread'] = np.log(df_book['log_ask1'] - df_book['log_bid1'])\n    df_book['log_spread2'] = np.log(df_book['log_ask2'] - df_book['log_bid2'])\n\n    \n    df_book['book_size1'] = df_book['ask_vol1'] + df_book['bid_vol1']\n    df_book['book_size'] = df_book['ask_vol1'] + df_book['bid_vol1'] + df_book['ask_vol2'] + df_book['bid_vol2']\n\n    \n    \n\n    df_book['askliq1bal'] = (\n                  df_book['ask_vol1']/( 1000*(df_book['wapq1'] - df_book['log_ask1']) )**1\n               -  df_book['ask_vol2']/( 1000*(df_book['wapq1'] - df_book['log_ask2']) )**1\n    )\n    \n    df_book['bidliq1bal'] = (\n                  df_book['bid_vol1']/( 1000*(df_book['wapq1'] - df_book['log_bid1']) )**1\n               -  df_book['bid_vol2']/( 1000*(df_book['wapq1'] - df_book['log_bid2']) )**1\n    )\n\n    \n    \n    df_book['lr1' ] = df_book.groupby(by = ['time_id'])['wap1' ].apply(diff).fillna(0)\n    df_book['lr2' ] = df_book.groupby(by = ['time_id'])['wap2' ].apply(diff).fillna(0)\n    df_book['lrq0'] = df_book.groupby(by = ['time_id'])['wapq0'].apply(diff).fillna(0)\n    df_book['lrq1'] = df_book.groupby(by = ['time_id'])['wapq1'].apply(diff).fillna(0)\n    df_book['lrq2'] = df_book.groupby(by = ['time_id'])['wapq2'].apply(diff).fillna(0)\n    \n    df_book['lr2e'] = df_book['lr2' ]*(df_book['lr1' ]==0)\n    \n    \n    df_book['log_liquidity1'] = np.log(df_book['liquidity1'])\n    df_book['log_liquidity2'] = np.log(df_book['liquidity2'])\n    df_book['log_liquidity3'] = np.log(df_book['liquidity3'])\n    \n    \n    df_book['liqret1'] = df_book.groupby(by = ['time_id'])['log_liquidity1'].apply(diff).fillna(0)\n    df_book['liqret2'] = df_book.groupby(by = ['time_id'])['log_liquidity2'].apply(diff).fillna(0)\n    df_book['liqret3'] = df_book.groupby(by = ['time_id'])['log_liquidity3'].apply(diff).fillna(0)\n    df_book['lsret'] = df_book.groupby(by = ['time_id'])['log_spread'].apply(diff).fillna(0)\n    \n    \n    \n    df_book['lrq0_lp'] = (df_book['liqret1']>0)*df_book['lr1']\n    df_book['lrq0_lm'] = (df_book['liqret1']<0)*df_book['lr1']\n\n    \n    \n    ##################\n    trade_stat =  pd.read_parquet(os.path.join(path_data,'trade_{}.parquet/stock_id={}'.format(dataType, stock_id)))\n    trade_stat['volume'] = trade_stat['size']*trade_stat['price']\n    ###########################\n    \n    ids = np.array(df_book[['stock_id', 'time_id']])\n    \n    ids = np.unique(ids, axis=0)\n    \n    stats = {}\n    \n    #stats['stock_id'] = ids[:,0:1]\n    stats[ 'time_id'] = ids[:,1:2]\n    \n    stats['avol1'] = binned_sum_impulse(np.array(df_book['seconds_in_bucket']),\n                                       np.array(df_book['time_id']),\n                                       np.abs(np.array(df_book['lr1'])),\n                                       20, 30, ids.shape[0])\n    stats['avol2'] = binned_sum_impulse(np.array(df_book['seconds_in_bucket']),\n                                       np.array(df_book['time_id']),\n                                       np.abs(np.array(df_book['lr2'])),\n                                       20, 30, ids.shape[0])\n\n    stats['lr1'] = binned_sum_impulse(np.array(df_book['seconds_in_bucket']),\n                                       np.array(df_book['time_id']),\n                                       np.array(df_book['lr1']),\n                                       20, 30, ids.shape[0])\n    \n    \n    stats['qvol1'] = binned_sum_impulse(np.array(df_book['seconds_in_bucket']),\n                                       np.array(df_book['time_id']),\n                                       np.array(df_book['lr1'])**4,\n                                       20, 30, ids.shape[0])**0.25\n    \n    stats['vol1'] = binned_sum_impulse(np.array(df_book['seconds_in_bucket']),\n                                       np.array(df_book['time_id']),\n                                       np.array(df_book['lr1'])**2,\n                                       20, 30, ids.shape[0])**0.5\n    \n    stats['vol2'] = binned_sum_impulse(np.array(df_book['seconds_in_bucket']),\n                                       np.array(df_book['time_id']),\n                                       np.array(df_book['lr2'])**2,\n                                       20, 30, ids.shape[0])**0.5\n    stats['vol2e'] = binned_sum_impulse(np.array(df_book['seconds_in_bucket']),\n                                       np.array(df_book['time_id']),\n                                       np.array(df_book['lr2e'])**2,\n                                       20, 30, ids.shape[0])**0.5\n    \n    stats['vol1_2'] = binned_sum_impulse(np.array(df_book['seconds_in_bucket']),\n                                       np.array(df_book['time_id']),\n                                       np.array(df_book['lr1'])**2,\n                                       20, 30, ids.shape[0])    \n    \n    stats['vol2_2'] = binned_sum_impulse(np.array(df_book['seconds_in_bucket']),\n                                       np.array(df_book['time_id']),\n                                       np.array(df_book['lr2'])**2,\n                                       20, 30, ids.shape[0])\n    \n    stats['avolq0'] = binned_sum_impulse(np.array(df_book['seconds_in_bucket']),\n                                       np.array(df_book['time_id']),\n                                       np.abs(np.array(df_book['lrq0'])),\n                                       20, 30, ids.shape[0])\n    \n    stats['volq0'] = binned_sum_impulse(np.array(df_book['seconds_in_bucket']),\n                                       np.array(df_book['time_id']),\n                                       np.array(df_book['lrq0'])**2,\n                                       20, 30, ids.shape[0])**0.5\n    \n    stats['volq0_lp'] = binned_sum_impulse(np.array(df_book['seconds_in_bucket']),\n                                       np.array(df_book['time_id']),\n                                       np.array(df_book['lrq0_lp'])**2,\n                                       20, 30, ids.shape[0])**0.5\n    \n    stats['volq0_lm'] = binned_sum_impulse(np.array(df_book['seconds_in_bucket']),\n                                       np.array(df_book['time_id']),\n                                       np.array(df_book['lrq0_lm'])**2,\n                                       20, 30, ids.shape[0])**0.5    \n\n    \n    \n    \n    stats['volq1'] = binned_sum_impulse(np.array(df_book['seconds_in_bucket']),\n                                       np.array(df_book['time_id']),\n                                       np.array(df_book['lrq1'])**2,\n                                       20, 30, ids.shape[0])**0.5\n    stats['volq2'] = binned_sum_impulse(np.array(df_book['seconds_in_bucket']),\n                                       np.array(df_book['time_id']),\n                                       np.array(df_book['lrq2'])**2,\n                                       20, 30, ids.shape[0])**0.5\n\n    stats['cvol1'] = binned_sum_impulse(np.array(df_book['seconds_in_bucket']),\n                                       np.array(df_book['time_id']),\n                                       np.array((df_book['liqret2']*df_book['lrq1'])**2 ),\n                                       20, 30, ids.shape[0])\n    \n    stats['evol1'] = binned_sum_impulse(np.array(df_book['seconds_in_bucket']),\n                                       np.array(df_book['time_id']),\n                                       np.array(( np.exp(df_book['liqret1'])*df_book['lrq1'])**2 ),\n                                       20, 30, ids.shape[0])\n    stats['e2vol1'] = binned_sum_impulse(np.array(df_book['seconds_in_bucket']),\n                                       np.array(df_book['time_id']),\n                                       np.array(( np.exp(df_book['liqret1'])*df_book['lrq1'])**2 ),\n                                       20, 30, ids.shape[0])\n    \n    \n    stats['svol1'] = binned_sum_impulse(np.array(df_book['seconds_in_bucket']),\n                                       np.array(df_book['time_id']),\n                                       np.array(( df_book['lr1']/df_book['spread'])**2 ),\n                                       20, 30, ids.shape[0])\n    stats['lvol1'] = binned_sum_impulse(np.array(df_book['seconds_in_bucket']),\n                                       np.array(df_book['time_id']),\n                                       np.array( ( df_book['lr1'])**2/df_book['liquidity2'] ),\n                                       20, 30, ids.shape[0])\n    \n    \n    stats['liqvol1'] = binned_sum_impulse(np.array(df_book['seconds_in_bucket']),\n                                       np.array(df_book['time_id']),\n                                       np.array(df_book['liqret1'])**2,\n                                       20, 30, ids.shape[0])\n    stats['liqvol2'] = binned_sum_impulse(np.array(df_book['seconds_in_bucket']),\n                                       np.array(df_book['time_id']),\n                                       np.array(df_book['liqret2'])**2,\n                                       20, 30, ids.shape[0])\n    stats['liqvol3'] = binned_sum_impulse(np.array(df_book['seconds_in_bucket']),\n                                       np.array(df_book['time_id']),\n                                       np.array(df_book['liqret3'])**2,\n                                       20, 30, ids.shape[0])\n    stats['lsvol'] = binned_sum_impulse(np.array(df_book['seconds_in_bucket']),\n                                       np.array(df_book['time_id']),\n                                       np.array(df_book['lsret'])**2,\n                                       20, 30, ids.shape[0])\n    \n    stats['book_delta_count'] = binned_sum_impulse(np.array(df_book['seconds_in_bucket']),\n                                       np.array(df_book['time_id']),\n                                         np.array(df_book['lr1']*0+1.0),\n                                         20, 30, ids.shape[0])\n    \n    stats['volume'] = binned_sum_impulse(np.array(trade_stat['seconds_in_bucket']),\n                                       np.array(trade_stat['time_id']),\n                                         np.array(trade_stat['volume']),\n                                         20, 30, ids.shape[0])\n    \n    stats['root_volume'] = binned_sum_impulse(np.array(trade_stat['seconds_in_bucket']),\n                                       np.array(trade_stat['time_id']),\n                                         np.array(trade_stat['volume']**.5),\n                                         20, 30, ids.shape[0])\n    \n    stats['cube_root_volume'] = binned_sum_impulse(np.array(trade_stat['seconds_in_bucket']),\n                                       np.array(trade_stat['time_id']),\n                                         np.array(trade_stat['volume']**(1/3)),\n                                         20, 30, ids.shape[0])\n\n    \n    stats['volume_p2/3'] = binned_sum_impulse(np.array(trade_stat['seconds_in_bucket']),\n                                       np.array(trade_stat['time_id']),\n                                         np.array(trade_stat['volume']**(2/3)),\n                                         20, 30, ids.shape[0])\n    \n    \n    stats['quart_root_volume'] = binned_sum_impulse(np.array(trade_stat['seconds_in_bucket']),\n                                       np.array(trade_stat['time_id']),\n                                         np.array(trade_stat['volume']**.25),\n                                         20, 30, ids.shape[0])\n    \n    \n    stats['trade_count'] = binned_sum_impulse(np.array(trade_stat['seconds_in_bucket']),\n                                       np.array(trade_stat['time_id']),\n                                         np.array(trade_stat['volume']**0),\n                                         20, 30, ids.shape[0])\n\n    \n    \n    stats['wap1'] = binned_time_weighted_mean_stat(np.array(df_book['seconds_in_bucket']),\n                                       np.array(df_book['time_id']),\n                                                   np.array(df_book['wap1']),\n                                                   np.ones((df_book.shape[0])),\n                                                   20, 30, ids.shape[0])\n    \n    stats['wap2'] = binned_time_weighted_mean_stat(np.array(df_book['seconds_in_bucket']),\n                                       np.array(df_book['time_id']),\n                                                   np.array(df_book['wap2']),\n                                                   np.ones((df_book.shape[0])),\n                                                   20, 30, ids.shape[0])\n    \n    stats['wapq1'] = binned_time_weighted_mean_stat(np.array(df_book['seconds_in_bucket']),\n                                       np.array(df_book['time_id']),\n                                                    np.array(df_book['wapq1']),\n                                                    np.ones((df_book.shape[0])),\n                                                    20, 30, ids.shape[0])\n        \n    stats['wapq0'] = binned_time_weighted_mean_stat(np.array(df_book['seconds_in_bucket']),\n                                       np.array(df_book['time_id']),\n                                                    np.array(df_book['wapq0']),\n                                                    np.ones((df_book.shape[0])),\n                                                    20, 30, ids.shape[0])\n\n    stats['wap1smx'] = np.log( binned_time_weighted_mean_stat(np.array(df_book['seconds_in_bucket']),\n                                       np.array(df_book['time_id']),\n                                                   np.exp( 4000*np.array(df_book['wap1'])),\n                                                   np.ones((df_book.shape[0])),\n                                                   20, 30, ids.shape[0]) )/4000\n    \n    stats['wap1smn'] = -np.log( binned_time_weighted_mean_stat(np.array(df_book['seconds_in_bucket']),\n                                       np.array(df_book['time_id']),\n                                                   np.exp(-4000*np.array(df_book['wap1'])),\n                                                   np.ones((df_book.shape[0])),\n                                                   20, 30, ids.shape[0]) )/4000\n    stats['wap1hl']  = np.exp(stats['wap1smx'] - stats['wap1smn'])\n    \n    \n    \n    \n    stats['wapq0smx'] = np.log( binned_time_weighted_mean_stat(np.array(df_book['seconds_in_bucket']),\n                                       np.array(df_book['time_id']),\n                                                   np.exp( 4000*np.array(df_book['wapq0'])),\n                                                   np.ones((df_book.shape[0])),\n                                                   20, 30, ids.shape[0]) )/4000\n    \n    stats['wapq0smn'] = -np.log( binned_time_weighted_mean_stat(np.array(df_book['seconds_in_bucket']),\n                                       np.array(df_book['time_id']),\n                                                   np.exp(-4000*np.array(df_book['wapq0'])),\n                                                   np.ones((df_book.shape[0])),\n                                                   20, 30, ids.shape[0]) )/4000\n    stats['wapq0hl']  = stats['wapq0smx'] - stats['wapq0smn']\n    \n    del stats['wapq0smx'], stats['wapq0smn']\n    del stats['wap1smx'], stats['wap1smn']\n\n        \n    stats['liquidity1'] = binned_time_weighted_mean_stat(np.array(df_book['seconds_in_bucket']),\n                                       np.array(df_book['time_id']),\n                                         np.array((df_book['liquidity1'])),\n                                         np.ones((df_book.shape[0])),\n                                         20, 30, ids.shape[0])\n    \n    stats['liquidity2'] = binned_time_weighted_mean_stat(np.array(df_book['seconds_in_bucket']),\n                                       np.array(df_book['time_id']),\n                                         np.array((df_book['liquidity2'])),\n                                         np.ones((df_book.shape[0])),\n                                         20, 30, ids.shape[0])\n    \n    stats['liquidity2f1'] = binned_time_weighted_mean_stat(np.array(df_book['seconds_in_bucket']),\n                                       np.array(df_book['time_id']),\n                                         np.array((df_book['liquidity2f1'])),\n                                         np.ones((df_book.shape[0])),\n                                         20, 30, ids.shape[0])\n    \n    \n    stats['root_liquidity2'] = binned_time_weighted_mean_stat(np.array(df_book['seconds_in_bucket']),\n                                       np.array(df_book['time_id']),\n                                         np.array((df_book['liquidity2']))**0.5,\n                                         np.ones((df_book.shape[0])),\n                                         20, 30, ids.shape[0])\n    \n    stats['liquidity3'] = binned_time_weighted_mean_stat(np.array(df_book['seconds_in_bucket']),\n                                       np.array(df_book['time_id']),\n                                         np.array((df_book['liquidity3'])),\n                                         np.ones((df_book.shape[0])),\n                                         20, 30, ids.shape[0])\n\n    \n    stats['liquidity3_root'] = binned_time_weighted_mean_stat(np.array(df_book['seconds_in_bucket']),\n                                       np.array(df_book['time_id']),\n                                         np.array((df_book['liquidity3']**0.5)),\n                                         np.ones((df_book.shape[0])),\n                                         20, 30, ids.shape[0])\n    \n    stats['liquidity3_4th_root'] = binned_time_weighted_mean_stat(np.array(df_book['seconds_in_bucket']),\n                                       np.array(df_book['time_id']),\n                                         np.array((df_book['liquidity3']**0.25)),\n                                         np.ones((df_book.shape[0])),\n                                         20, 30, ids.shape[0])\n    \n                         \n    stats['liquidity4'] = binned_time_weighted_mean_stat(np.array(df_book['seconds_in_bucket']),\n                                       np.array(df_book['time_id']),\n                                         np.array((df_book['liquidity4'])),\n                                         np.ones((df_book.shape[0])),\n                                         20, 30, ids.shape[0])\n    \n    stats['liquidity4_root'] = binned_time_weighted_mean_stat(np.array(df_book['seconds_in_bucket']),\n                                       np.array(df_book['time_id']),\n                                         np.array((df_book['liquidity4']))**.5,\n                                         np.ones((df_book.shape[0])),\n                                         20, 30, ids.shape[0])\n    \n    stats['liquidity6_root'] = binned_time_weighted_mean_stat(np.array(df_book['seconds_in_bucket']),\n                                       np.array(df_book['time_id']),\n                                         np.array((df_book['liquidity6']))**.5,\n                                         np.ones((df_book.shape[0])),\n                                         20, 30, ids.shape[0])\n    \n    stats['liquidity10_root'] = binned_time_weighted_mean_stat(np.array(df_book['seconds_in_bucket']),\n                                       np.array(df_book['time_id']),\n                                         np.array((df_book['liquidity10']))**.5,\n                                         np.ones((df_book.shape[0])),\n                                         20, 30, ids.shape[0])\n    \n    stats['spread'] = binned_time_weighted_mean_stat(np.array(df_book['seconds_in_bucket']),\n                                       np.array(df_book['time_id']),\n                                         np.array((df_book['spread'])),\n                                         np.ones((df_book.shape[0])),\n                                         20, 30, ids.shape[0])\n    stats['inv_spread'] = binned_time_weighted_mean_stat(np.array(df_book['seconds_in_bucket']),\n                                       np.array(df_book['time_id']),\n                                         np.array((df_book['spread']))**-1,\n                                         np.ones((df_book.shape[0])),\n                                         20, 30, ids.shape[0])\n    stats['log_spread'] = binned_time_weighted_mean_stat(np.array(df_book['seconds_in_bucket']),\n                                       np.array(df_book['time_id']),\n                                         np.log(np.array((df_book['spread']))),\n                                         np.ones((df_book.shape[0])),\n                                         20, 30, ids.shape[0])\n    \n    stats['log_spread2'] = binned_time_weighted_mean_stat(np.array(df_book['seconds_in_bucket']),\n                                       np.array(df_book['time_id']),\n                                         np.array((df_book['log_spread2'])),\n                                         np.ones((df_book.shape[0])),\n                                         20, 30, ids.shape[0])\n    \n    stats['book_size1'] = binned_time_weighted_mean_stat(np.array(df_book['seconds_in_bucket']),\n                                       np.array(df_book['time_id']),\n                                         np.array((df_book['book_size1'])),\n                                         np.ones((df_book.shape[0])),\n                                         20, 30, ids.shape[0]) \n    stats['book_size'] = binned_time_weighted_mean_stat(np.array(df_book['seconds_in_bucket']),\n                                       np.array(df_book['time_id']),\n                                         np.array((df_book['book_size'])),\n                                         np.ones((df_book.shape[0])),\n                                         20, 30, ids.shape[0]) \n    \n    \n    stats['tvpl1'] = stats['volume']/stats['liquidity1']\n    stats['tvpl2'] = stats['volume']/stats['liquidity2']\n    stats['tvpl3'] = stats['volume']/stats['liquidity3']\n    stats['tvpl4'] = stats['volume']/stats['liquidity4']\n    \n    \n    stats['askliq1bal'] = binned_time_weighted_mean_stat(np.array(df_book['seconds_in_bucket']),\n                                                   np.array(df_book['time_id']),\n\n                                         np.array((df_book['askliq1bal'])),\n                                         np.ones((df_book.shape[0])),\n                                         20, 30, ids.shape[0])\n    \n    stats['bidliq1bal'] = binned_time_weighted_mean_stat(np.array(df_book['seconds_in_bucket']),\n                                            np.array(df_book['time_id']),\n\n                                         np.array((df_book['bidliq1bal'])),\n                                         np.ones((df_book.shape[0])),\n                                         20, 30, ids.shape[0])\n    \n    return stats\n\n\n\n\n@jit\ndef index_into_set(ids_left, ids_right):\n    j = 0\n    z = np.zeros(  ids_right.shape[0])\n    for i in range(ids_right.shape[0]):\n        while ids_left[j] != ids_right[i]:\n            j = j+1\n            if j >= ids_left.shape[0]:\n                return z\n        z[i] = j\n    return z\n            \n    \n@jit\ndef index_into_set(ids_left, ids_right):\n    j = 0\n    z = 1 == np.zeros(  ids_left.shape[0])\n    for i in range(ids_right.shape[0]):\n        while ids_left[j] != ids_right[i]:\n            z[j] = False \n            j = j+1\n            if j >= ids_left.shape[0]:\n                return z\n        z[j] = True\n        j = j+1\n    return z\n            \n    \n    \n\ndef get_dataSet(stock_ids : list, dataType = 'train'):\n    t = time()\n    stock_ids= sorted(stock_ids)\n    \n    print(stock_ids)\n    \n    stock_stats = Parallel(n_jobs=3)(\n        delayed(get_stock_stat)(stock_id, dataType) \n        for stock_id in stock_ids\n    )\n    \n    print('stock_stats time:', time()-t)\n    \n    data = {}\n        \n    time_ids = sum([list(ss['time_id']) for ss in stock_stats], [] )\n    time_ids = list(np.unique(time_ids))\n    \n    n_bins = 30\n    n_time_ids = len(time_ids)\n    n_stock_ids = len(stock_ids)\n    \n    data['time_ids' ] = np.array( time_ids)\n    data['stock_ids'] = np.array(stock_ids)\n    \n    for key in stock_stats[0].keys():\n        if key == 'time_id':\n            continue\n        \n        Z = np.zeros(( n_time_ids, n_stock_ids, n_bins))\n        \n        for k in range(n_stock_ids):\n            ss = stock_stats[k]\n\n            #ts = index_into_set(np.array(time_ids), ss['time_id']).astype(int)\n\n            b = index_into_set(np.array(time_ids), ss['time_id'])\n\n            #print(b)\n            #print(b.shape)\n            \n            Z[ b, k, :] = ss[key]\n            \n            Z[~b, k, :] = np.nanmean(ss[key])\n            \n            Z[:,k,:][np.isnan(Z[:,k,:])] = np.nanmean(Z[:,k,:])\n                                              \n            #del ss[key]\n        \n        data[key] = Z\n        \n        gc.collect()\n        \n        \n    del stock_stats\n    gc.collect()\n    \n    print('full time:', time()-t)\n    \n    data['vol1'] = ( data['vol1']**2 + .25*data['vol2']**2)**0.5\n\n    return data","metadata":{"execution":{"iopub.status.busy":"2021-10-13T20:12:53.121791Z","iopub.execute_input":"2021-10-13T20:12:53.122084Z","iopub.status.idle":"2021-10-13T20:12:53.24502Z","shell.execute_reply.started":"2021-10-13T20:12:53.122055Z","shell.execute_reply":"2021-10-13T20:12:53.243411Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv(os.path.join(path_data, 'train.csv'))\ntest = pd.read_csv(os.path.join(path_data, 'test.csv'))\n\ntest_binned  = get_dataSet(stock_ids = list(np.unique(test[ 'stock_id'])), dataType = 'test')\ntrain_binned = get_dataSet(stock_ids = list(np.unique(train['stock_id'])), dataType = 'train')\n\n\nIS_GOOD_TEST = test_binned['vol1'].shape[1] > 100","metadata":{"execution":{"iopub.status.busy":"2021-10-13T20:12:53.24746Z","iopub.execute_input":"2021-10-13T20:12:53.24793Z","iopub.status.idle":"2021-10-13T20:37:08.165459Z","shell.execute_reply.started":"2021-10-13T20:12:53.247881Z","shell.execute_reply":"2021-10-13T20:37:08.16445Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Clustering","metadata":{}},{"cell_type":"code","source":"from sklearn.decomposition import PCA\nfrom sklearn.cluster import KMeans\nfrom sklearn.cluster import AgglomerativeClustering\nfrom sklearn.mixture import GaussianMixture\nimport scipy as sp\n\ndef get_cut(X):\n\n    y = X\n    X = np.hstack((X, np.ones((X.shape[0], 1))))\n    \n    y2 = np.sum(y**2, axis=1, keepdims=True)/2\n\n    y = np.hstack((y, y2, 0*y+1, y2 + 1))\n    \n    \n    yX = np.matmul(y.T, X)\n\n    \n    minkowski = -np.ones((y.shape[1],1))\n    minkowski[-1,0] = +1.0\n \n    A = np.matmul(yX.T, minkowski*yX)\n    \n    Ao = np.expand_dims(A[:-1,-1],1)\n    Aoo = A[-1,-1]\n    A = A[:-1,:-1] - np.matmul(Ao, Ao.T)/Aoo\n    \n    \n    lam, v = sp.linalg.eigh(A, eigvals=(0,0))\n    v = np.vstack((v, -np.dot(Ao[:,0],v)/Aoo))\n    \n    \n    c = np.matmul(X, v) > 0.0\n    \n    return c[:,0]\n\n\ndef otree_clusters(X, max_depth):\n    \n    sets = np.zeros((X.shape[0])).astype(int)\n\n    for k in range(max_depth):\n        new_sets = 0*sets\n        \n        for kk in range(np.max(sets)+1):\n            b = sets==kk\n            \n            Xs = X[b, :]\n            \n            \n            c = get_cut(Xs)\n            \n            \n            new_sets[b] = 2*sets[b] + c\n        sets = new_sets\n        \n    return sets\n\n\n\ndef get_clusters(x, depth=3):  \n    x[~np.isfinite(x)] = np.mean(x[np.isfinite(x)])\n\n    x = x - np.nanmean(x, 0)\n    x = x / (np.nanstd(x, 0)+1e-20)\n    \n    C = np.corrcoef(x.T)\n    C = np.arctanh(C)\n    a = np.arange(C.shape[0])\n    C[a,a] = 0\n        \n    m = np.mean(C, 1, keepdims=True)\n    C = C - (m + m.T)\n    C[a,a] = np.mean(C)\n\n    pca = PCA(x.shape[1])\n    \n    pca.fit(x)\n    c = pca.components_ * pca.singular_values_[:,np.newaxis]**.5\n    \n    c = c.T\n    \n    c[:,0] = c[:,0]\n\n    p = otree_clusters(c, depth)\n    \n    means = []    \n    for k in range(np.max(p)+1):\n        mn = np.mean(c[p==k, :], 0)\n        means.append(mn)\n    means= np.vstack(means)\n    \n    gm = GaussianMixture(means.shape[0], means_init=means, weights_init=np.ones(means.shape[0])/means.shape[0], covariance_type='full', \n                         precisions_init=np.repeat( np.eye(means.shape[1])[np.newaxis,:,:]/means.shape[1], repeats=means.shape[0], axis=0) )\n                             \n    p = gm.fit_predict(c)\n    \n    print(p)    \n    _, cnts = np.unique( p, return_counts=True)\n    print(cnts)\n\n    score=0\n    nc= np.max(p)+1\n    for k in range(nc):\n        Q =  C[p==k,:][:,p==k]\n        score += np.mean(Q)/nc\n    print('SCORE', score)\n    \n    \n    k = np.argsort(p)\n    \n    plt.imshow( (C[k,:][:,k]) )\n    plt.show()\n\n    return p\n\ndef high_corr_set(x, q):  \n    x[~np.isfinite(x)] = np.mean(x[np.isfinite(x)])\n\n    x = x - np.nanmean(x, 0)\n    x = x / np.nanstd(x, 0)\n    \n    C = np.corrcoef(x.T)\n    c = np.median(C, 1)\n    \n    b = c > np.quantile(c,q)\n    \n    idxs =  np.arange(c.shape[0])[b]\n\n    return idxs\n\n\nrvol   = train_binned['root_volume']\nvol    = train_binned['volume']\nvol1   = train_binned['vol1']\nlr1   = train_binned['lr1']\nlsprd  = train_binned['log_spread']\nr4liq3 = train_binned['liquidity3_4th_root']\nliq2   = train_binned['liquidity2']\n\nlr1 = np.mean(lr1,2)\nvol1 = np.mean(vol1**2, 2)**.5\ntvpl3 = np.log(np.mean( rvol, 2)**(1/2)/np.mean( (r4liq3[:,:, 0:]), 2)**(4/3))\nrvol = np.log(np.mean(rvol**.5,2))\nlsprd = (np.mean(lsprd,2))\nvol1 = np.log(vol1)\nvol = (np.mean(vol,2) )\n\n\nlr1_clusters1 = get_clusters(lr1, depth=1)\nlr1_clusters2 = get_clusters(lr1, depth=2)\nlr1_clusters3 = get_clusters(lr1, depth=3)\nlr1_clusters4 = get_clusters(lr1, depth=4)\nlr1_clusters5 = get_clusters(lr1, depth=5)\n\nlr1_hc = high_corr_set(lr1, .9)\n\ntvpl3_clusters1 = get_clusters(tvpl3, depth=1)\ntvpl3_clusters2 = get_clusters(tvpl3, depth=2)\ntvpl3_clusters3 = get_clusters(tvpl3, depth=3)\ntvpl3_clusters4 = get_clusters(tvpl3, depth=4)\n\nvol1_clusters1 = get_clusters(vol1, depth=1)\nvol1_clusters2 = get_clusters(vol1, depth=2)\nvol1_clusters3 = get_clusters(vol1, depth=3)\nvol1_clusters4 = get_clusters(vol1, depth=4)\n\nvol1_hc = high_corr_set(vol1, .95)\n\nvol_clusters1 = get_clusters(rvol, depth=1)\nvol_clusters2 = get_clusters(rvol, depth=2)\nvol_clusters3 = get_clusters(rvol, depth=3)\nvol_clusters4 = get_clusters(rvol, depth=4)\n\n\ndel rvol, vol, vol1, lr1, lsprd, r4liq3, liq2\n\n","metadata":{"execution":{"iopub.status.busy":"2021-10-13T20:41:05.026639Z","iopub.execute_input":"2021-10-13T20:41:05.027013Z","iopub.status.idle":"2021-10-13T20:41:13.829743Z","shell.execute_reply.started":"2021-10-13T20:41:05.026982Z","shell.execute_reply":"2021-10-13T20:41:13.828768Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Features","metadata":{}},{"cell_type":"code","source":"\ndef nancorr(a, b):\n    v = np.isfinite(a)*np.isfinite(b) > 0\n    return np.corrcoef(a[v], b[v])[0,1]\n\n\ndef plot_by_sid(train, x, y):\n    for u in np.unique(train['stock_id']):\n        b = train['stock_id']==u\n        plt.plot(x[b], y[b], '.', markersize=1)\n    plt.show()\n    \n\n\n\ndef cluster_agg(x, clusters, agg_fun):\n    r = 0*x\n    \n    for k in range(np.max(clusters)+1):\n        z = agg_fun(x[:,clusters==k,:], 1, keepdims=True)\n        r[:,clusters==k,:] = np.repeat(z, repeats=int(np.sum(clusters==k)), axis=1)\n    return r\n    \n\ndef keys_to_array(d, features):\n    \n    sh = np.max( [d[f].shape[1] for f in features] )\n    sq = np.max( [d[f].shape[0] for f in features] )\n    \n    for f in features:\n        if d[f].shape[1]==1:\n            d[f] = np.repeat(d[f], repeats=sh, axis=1)\n        if d[f].shape[0]==1:\n            d[f] = np.repeat(d[f], repeats=sq, axis=0)\n        \n    \n    stack = [d[feature] for feature in features]\n    \n    X = np.dstack(stack)\n    \n    return X\n\n\ndef merge_features_to_df(fdict, df, features):\n    sids = df['stock_id'].unique()\n\n    T = fdict['time_ids'][:,:,0]\n    S = fdict['stock_ids'][:,:,0]\n\n    T = np.reshape(T, [T.shape[0]*T.shape[1]])\n    S = np.reshape(S, [S.shape[0]*S.shape[1]])\n    \n    sh = np.max( [fdict[f].shape[1] for f in features] )\n    sq = np.max( [fdict[f].shape[0] for f in features] )\n    \n    for f in features:\n        if fdict[f].shape[1]==1:\n            fdict[f] = np.repeat(fdict[f], repeats=sh, axis=1)\n        if fdict[f].shape[0]==1:\n            fdict[f] = np.repeat(fdict[f], repeats=sq, axis=0)\n        \n    \n    reshaped_features = [np.reshape( fdict[feature], [fdict[feature].shape[0]*fdict[feature].shape[1]] )\n                         for feature in features]\n\n\n    \n    dfz = pd.DataFrame(data=np.vstack([S,T] + reshaped_features  ).T, columns=['stock_id', 'time_id']+features)\n    dfz['time_id' ] = dfz['time_id' ].astype(int)\n    dfz['stock_id'] = dfz['stock_id'].astype(int)\n\n    dfz = df.merge(dfz, on=['stock_id', 'time_id'], how='left')\n    return dfz\n\n\n\ndef get_simple_features(binned_features, final_features, name, ffrom=0):\n    v = binned_features[name]\n\n    suffix = f'_from_{ffrom}'\n\n    s  = np.mean(np.mean(v**2, (2), keepdims=True)**.5, 0, keepdims=True)\n\n    final_features[name + suffix] =            np.log(             np.mean(v[:,:,ffrom:]**2, 2, keepdims=True)**0.5/final_features['vol1'])\n    final_features[name+'stock_mean'+suffix] = np.log(s*np.median( np.mean( v[:,:,ffrom:]/s, 2, keepdims=True), 1, keepdims=True)/final_features['vol1'])\n\n    \n\ndef get_cohesion_features(binned_features, final_features, ffrom=0):\n    lr1 = binned_features['lr1']\n\n    if ffrom > 0:\n        suffix = f'_from_{ffrom}'\n    else:\n        suffix = ''\n    \n    s = np.mean( np.var(lr1, 0,keepdims=True), 2, keepdims=True)**0.5\n    qs = lr1[:,:,ffrom:]/s\n\n    final_features['tbin_var'               + suffix] = s*np.mean(  np.var( qs, 1, keepdims=True)         , 2, keepdims=True)**0.5/final_features['vol1']\n    final_features['market_var'             + suffix] = s*np.mean( (np.mean(qs, 1, keepdims=True)     )**2, 2, keepdims=True)**0.5/final_features['vol1']\n    final_features['deviations_from_market' + suffix] = s*np.mean( (np.mean(qs, 1, keepdims=True) - qs)**2, 2, keepdims=True)**0.5/final_features['vol1']\n    \n    \n\n\n\ndef get_misc_features(binned_features, final_features):\n    vol    = binned_features['volume']\n    vol1    = binned_features['vol1']\n\n    rvol   = binned_features['root_volume']\n\n    volcr  = binned_features['cube_root_volume']\n    liq2   = binned_features['liquidity2']\n    liq3   = binned_features['liquidity3']\n    r4liq3 = binned_features['liquidity3_4th_root']\n    lsprd  = binned_features['log_spread2']\n    bsz  = binned_features['book_size']\n\n    \n    s  = np.nanmean( vol, (0,2), keepdims=True)\n    sr = np.nanmean(rvol, (0,2), keepdims=True)\n    l2 = np.nanmean(liq2, (0,2), keepdims=True)\n    l3 = np.nanmean(liq3, (0,2), keepdims=True)\n\n    v1 = np.nanmean(vol1**2, 2, keepdims=True)**.5\n    \n    final_features['soft_stock_mean_tvpl2'    ] = np.log( (s/l2*np.nanmean( (vol[:,:, 0:]/liq2[:,:, 0:]*l2/s)**(1/8), (1,2), keepdims=True)**8)**.5/v1)\n    final_features['soft_stock_mean_tvpl2_f10'] = np.log( (s/l2*np.nanmean( (vol[:,:,10:]/liq2[:,:,10:]*l2/s)**(1/8), (1,2), keepdims=True)**8)**.5/v1)\n    final_features['soft_stock_mean_tvpl2_f20'] = np.log( (s/l2*np.nanmean( (vol[:,:,20:]/liq2[:,:,20:]*l2/s)**(1/8), (1,2), keepdims=True)**8)**.5/v1)\n    \n    final_features['soft_stock_mean_tvpl3'    ] = np.log( (sr/l3*np.nanmean( (rvol[:,:, 0:]/liq3[:,:, 0:]*l3/sr)**(1/8), (1,2), keepdims=True)**8)**(1/3)/v1)\n    final_features['soft_stock_mean_tvpl3_f10'] = np.log( (sr/l3*np.nanmean( (rvol[:,:,10:]/liq3[:,:,10:]*l3/sr)**(1/8), (1,2), keepdims=True)**8)**(1/3)/v1)\n    final_features['soft_stock_mean_tvpl3_f20'] = np.log( (sr/l3*np.nanmean( (rvol[:,:,20:]/liq3[:,:,20:]*l3/sr)**(1/8), (1,2), keepdims=True)**8)**(1/3)/v1)\n    \n    final_features['soft_stock_mean_tvpl2_liqf'       ] = np.log( (s/l2*np.nanmean( (vol[:,:, 0:]/liq2[:,:,-1:]*l2/s)**(1/8), (1,2), keepdims=True)**8)**.5/v1)\n    final_features['soft_stock_mean_tvpl2_liqf_volf10'] = np.log( (s/l2*np.nanmean( (vol[:,:,10:]/liq2[:,:,-1:]*l2/s)**(1/8), (1,2), keepdims=True)**8)**.5/v1)\n    final_features['soft_stock_mean_tvpl2_liqf_volf20'] = np.log( (s/l2*np.nanmean( (vol[:,:,20:]/liq2[:,:,-1:]*l2/s)**(1/8), (1,2), keepdims=True)**8)**.5/v1)\n    \n    final_features['soft_stock_mean_tvpl3_liqf'       ] = np.log( (sr/l3*np.nanmean( (rvol[:,:, 0:]/liq3[:,:,-1:]*l3/sr)**(1/8), (1,2), keepdims=True)**8)**(1/3)/v1)\n    final_features['soft_stock_mean_tvpl3_liqf_volf10'] = np.log( (sr/l3*np.nanmean( (rvol[:,:,10:]/liq3[:,:,-1:]*l3/sr)**(1/8), (1,2), keepdims=True)**8)**(1/3)/v1)\n    final_features['soft_stock_mean_tvpl3_liqf_volf20'] = np.log( (sr/l3*np.nanmean( (rvol[:,:,20:]/liq3[:,:,-1:]*l3/sr)**(1/8), (1,2), keepdims=True)**8)**(1/3)/v1)\n    \n    \n\n    tvpl3     = np.log( np.mean( rvol, 2, keepdims=True)**(1/2)/np.mean( (r4liq3[:,:, 0:]), (2), keepdims=True)**(4/3)/v1)\n    tvpl3lf20 = np.log( np.mean( rvol, 2, keepdims=True)**(1/2)/np.mean( (r4liq3[:,:, 20:]), (2), keepdims=True)**(4/3)/v1)\n    \n    final_features['tvpl3_rmed2v1']     = np.log( np.median(tvpl3/v1,1, keepdims=True)    )\n    final_features['tvpl3_rmed2v1lf20'] = np.log( np.median(tvpl3lf20/v1,1, keepdims=True))\n    \n    \n    \n    final_features['v1proj_25_15'] = np.log( np.nanmedian( np.mean(vol1[:,:,25:]**2,2,keepdims=True)\n                                                         / np.mean(vol1[:,:,:15]**2,2,keepdims=True),1,keepdims=True)**.5 )\n    \n    \n    final_features['v1proj_25_15_lr1_hc'] = np.log( np.nanmedian( np.mean(vol1[:,lr1_hc,25:]**2,2,keepdims=True)\n                                                                / np.mean(vol1[:,lr1_hc,:15]**2,2,keepdims=True),1,keepdims=True)**.5 )\n    \n    final_features['v1proj_25_15_vol1_hc'] = np.log( np.nanmedian( np.mean(vol1[:,vol1_hc,25:]**2,2,keepdims=True)\n                                                                 / np.mean(vol1[:,vol1_hc,:15]**2,2,keepdims=True),1,keepdims=True)**.5 ) \n    final_features['v1proj_25_lr1_hc']     = np.log( np.nanmedian( np.mean(vol1[:,lr1_hc,25:]**2,2,keepdims=True)\n                                                             / np.mean(vol1[:,lr1_hc,:]**2,2,keepdims=True),1,keepdims=True)**.5 )\n    final_features['v1proj_25_vol1_hc']    = np.log( np.nanmedian( np.mean(vol1[:,vol1_hc,25:]**2,2,keepdims=True)\n                                                              / np.mean(vol1[:,vol1_hc,:]**2,2,keepdims=True),1,keepdims=True)**.5 )\n    \n    \n    final_features['lsvol'] = np.log( np.nanmean(binned_features['lsvol'], 2, keepdims=True))    \n\n    final_features['liqvol1'] = np.log( np.nanmean(binned_features['liqvol1'], 2, keepdims=True))    \n    final_features['liqvol1_smean'] = np.log( np.nanmean(binned_features['liqvol1'], (1,2), keepdims=True))    \n\n    final_features['liqvol1_smean_c3'] = np.log( cluster_agg(np.nanmean(binned_features['liqvol1'], 2, keepdims=True), lr1_clusters3,np.nanmedian))    \n\n    \n    \n    final_features['liqvol2'] = np.log( np.nanmean(binned_features['liqvol2'], 2, keepdims=True))    \n    final_features['liqvol3'] = np.log( np.nanmean(binned_features['liqvol3'], 2, keepdims=True))    \n\n    \n    final_features['liqvol1_15_15'] = np.log( np.nanmean(binned_features['liqvol1'][:,:,15:  ], 2, keepdims=True)\n                                            /np.nanmean(binned_features['liqvol1'][:,:,  :15], 2, keepdims=True))    \n\n    final_features['trade_count']      = np.log( np.nanmean(binned_features['trade_count']    , 2, keepdims=True))    \n    final_features['root_trade_count'] = np.log( np.nanmean(binned_features['trade_count']**.5, 2, keepdims=True))    \n    final_features['root_book_delta_count'] = np.log( np.nanmean(binned_features['book_delta_count']**.5, 2, keepdims=True))    \n\n    final_features['root_trade_count_smean'] = np.log( np.nanmean(binned_features['trade_count']**.5, (1,2), keepdims=True))    \n\n    final_features['root_trade_count_smean_c1'] = np.log( cluster_agg(np.nanmean(binned_features['trade_count']**0.5, 2, keepdims=True),\n                                                                     lr1_clusters1,np.nanmean))\n    final_features['root_trade_count_smean_c2'] = np.log( cluster_agg(np.nanmean(binned_features['trade_count']**0.5, 2, keepdims=True),\n                                                                     lr1_clusters2,np.nanmean))\n    final_features['root_trade_count_smean_c3'] = np.log( cluster_agg(np.nanmean(binned_features['trade_count']**0.5, 2, keepdims=True),\n                                                                     lr1_clusters3,np.nanmean))    \n    final_features['root_trade_count_var'] = np.log( np.nanvar(binned_features['trade_count']**.5, 2, keepdims=True))    \n\n    \n    final_features['root_book_delta_count_smean'] = np.log( np.nanmean(binned_features['book_delta_count']**.5, (1,2), keepdims=True))    \n\n    final_features['root_book_delta_count_smean_c1'] = np.log( cluster_agg(np.nanmean(binned_features['book_delta_count']**0.5, 2, keepdims=True),\n                                                                     lr1_clusters1,np.nanmean))\n    final_features['root_book_delta_smean_c2'] = np.log( cluster_agg(np.nanmean(binned_features['book_delta_count']**0.5, 2, keepdims=True),\n                                                                     lr1_clusters2,np.nanmean))\n    final_features['root_book_delta_count_smean_c3'] = np.log( cluster_agg(np.nanmean(binned_features['book_delta_count']**0.5, 2, keepdims=True),\n                                                                     lr1_clusters3,np.nanmean))    \n    final_features['root_book_delta_count_var'] = np.log( np.nanvar(binned_features['book_delta_count']**.5, 2, keepdims=True))    \n\n    \n    \n    final_features['trade_count_15_15']      = np.log( np.nanmean(binned_features['trade_count'][:,:,15:  ], 2, keepdims=True)/\n                                                      np.nanmean(binned_features['trade_count'][:,:,  :15], 2, keepdims=True))    \n    final_features['root_trade_count_15_15'] =  np.log( np.nanmean(binned_features['trade_count'][:,:,15:  ]**.5, 2, keepdims=True)/\n                                                       np.nanmean(binned_features['trade_count'][:,:,  :15]**.5, 2, keepdims=True)) \n    \n    \n    final_features['v1proj_29_15'] = np.log( np.nanmedian( np.mean(vol1[:,:,29:  ]**2,2,keepdims=True)\n                                                        / np.mean(vol1[:,:,  :15]**2,2,keepdims=True),1,keepdims=True)**.5)\n    final_features['v1proj_20']    = np.log( np.nanmedian( np.mean(vol1[:,:,20:  ]**2,2,keepdims=True)\n                                                        / np.mean(vol1[:,:,  :  ]**2,2,keepdims=True),1,keepdims=True)**.5 )   \n    final_features['v1proj_25']    = np.log( np.nanmedian( np.mean(vol1[:,:,25:  ]**2,2,keepdims=True) \n                                                        / np.mean(vol1[:,:,  :  ]**2,2,keepdims=True),1,keepdims=True)**.5 )\n    final_features['v1proj_29']    = np.log( np.nanmedian( np.mean(vol1[:,:,28:  ]**2,2,keepdims=True)\n                                                        / np.mean(vol1[:,:,  :  ]**2,2,keepdims=True),1,keepdims=True)**.5 )\n    \n    final_features['v1proj_29_q1'] = np.log( np.quantile( np.mean(vol1[:,:,28:]**2,2,keepdims=True)\n                                                        / np.mean(vol1[:,:,:  ]**2,2,keepdims=True), 0.25, 1,keepdims=True)**.5 )\n    final_features['v1proj_29_q3'] = np.log( np.quantile( np.mean(vol1[:,:,28:]**2,2,keepdims=True)\n                                                        / np.mean(vol1[:,:,:  ]**2,2,keepdims=True), 0.75, 1,keepdims=True)**.5 )\n    final_features['v1proj_25_q1'] = np.log( np.quantile( np.mean(vol1[:,:,25:]**2,2,keepdims=True)\n                                                        / np.mean(vol1[:,:,:  ]**2,2,keepdims=True), 0.25, 1,keepdims=True)**.5 )\n    final_features['v1proj_25_q3'] = np.log( np.quantile( np.mean(vol1[:,:,25:]**2,2,keepdims=True)\n                                                        / np.mean(vol1[:,:,:  ]**2,2,keepdims=True), 0.75, 1,keepdims=True)**.5 ) \n    \n    final_features['v1proj_29_15_q1'] = np.log( np.quantile( np.mean(vol1[:,:,28:  ]**2,2,keepdims=True)\n                                                           / np.mean(vol1[:,:,  :15]**2,2,keepdims=True), 0.25, 1,keepdims=True)**.5 )\n    final_features['v1proj_29_15_q3'] = np.log( np.quantile( np.mean(vol1[:,:,28:  ]**2,2,keepdims=True)\n                                                           / np.mean(vol1[:,:,  :15]**2,2,keepdims=True), 0.75, 1,keepdims=True)**.5 )\n    final_features['v1proj_25_15_q1'] = np.log( np.quantile( np.mean(vol1[:,:,25:  ]**2,2,keepdims=True)\n                                                           / np.mean(vol1[:,:,  :15]**2,2,keepdims=True), 0.25, 1,keepdims=True)**.5 )\n    final_features['v1proj_25_15_q3'] = np.log( np.quantile( np.mean(vol1[:,:,25:  ]**2,2,keepdims=True)\n                                                           / np.mean(vol1[:,:,  :15]**2,2,keepdims=True), 0.75, 1,keepdims=True)**.5 )\n    \n    \n      \n    \n    \n    \n    \n    final_features['v1proj_25_15_std'] = np.log( np.nanstd( np.log( np.mean(vol1[:,:,25:]**2,2,keepdims=True)\n                                                               / np.mean(vol1[:,:,:15]**2,2,keepdims=True)),1,keepdims=True)**.5 )\n    final_features['v1proj_29_15_std'] = np.log( np.nanstd( np.log( np.mean(vol1[:,:,29:]**2,2,keepdims=True)\n                                                               / np.mean(vol1[:,:,:15]**2,2,keepdims=True)),1,keepdims=True)**.5 )\n    \n    final_features['v1proj_20_std'] = np.log( np.nanstd( np.log(np.mean(vol1[:,:,20:]**2,2,keepdims=True)\n                                                            /np.mean(vol1[:,:,:  ]**2,2,keepdims=True))\n                                                    ,1,keepdims=True) )\n    final_features['v1proj_25_std'] = np.log( np.nanstd( np.log(np.mean(vol1[:,:,25:]**2,2,keepdims=True)\n                                                            /np.mean(vol1[:,:,:  ]**2,2,keepdims=True))\n                                                    ,1,keepdims=True) )\n    final_features['v1proj_29_std'] = np.log( np.nanstd( np.log(np.mean(vol1[:,:,28:]**2,2,keepdims=True)\n                                                            /np.mean(vol1[:,:,:  ]**2,2,keepdims=True))\n                                                    ,1,keepdims=True) )\n    \n    \n    final_features['v1proj_29_q3q1'] = np.log(np.quantile( np.log(np.mean(vol1[:,:,28:]**2,2,keepdims=True)\n                                                            /np.mean(vol1[:,:,:  ]**2,2,keepdims=True))\n                                                    ,0.75, axis=1,keepdims=True)\n                                                -\n                                                np.quantile( np.log(np.mean(vol1[:,:,28:]**2,2,keepdims=True)\n                                                            /np.mean(vol1[:,:,:  ]**2,2,keepdims=True))\n                                                    ,0.25, axis=1,keepdims=True))\n    \n    if vol1.shape[1]>100:\n        final_features['v1proj_25_c1'] = np.log( cluster_agg( np.mean(vol1[:,:,25:]**2,2,keepdims=True)\n                                                       / np.mean(vol1[:,:,:  ]**2,2,keepdims=True),\n                                                        lr1_clusters1, np.median)**0.5 )  \n        final_features['v1proj_25_c2'] = np.log( cluster_agg( np.mean(vol1[:,:,25:]**2,2,keepdims=True)\n                                                       / np.mean(vol1[:,:,:  ]**2,2,keepdims=True),\n                                                        lr1_clusters2, np.median)**0.5 )\n        final_features['v1proj_25_c3'] = np.log( cluster_agg( np.mean(vol1[:,:,25:]**2,2,keepdims=True)\n                                                       / np.mean(vol1[:,:,:  ]**2,2,keepdims=True),\n                                                        lr1_clusters3, np.median)**0.5 )\n        final_features['v1proj_25_c4'] = np.log( cluster_agg( np.mean(vol1[:,:,25:]**2,2,keepdims=True)\n                                                       / np.mean(vol1[:,:,:  ]**2,2,keepdims=True),\n                                                        lr1_clusters4, np.median)**0.5 )\n        final_features['v1proj_25_c5'] = np.log( cluster_agg( np.mean(vol1[:,:,25:]**2,2,keepdims=True)\n                                                       / np.mean(vol1[:,:,:  ]**2,2,keepdims=True),\n                                                        lr1_clusters5, np.median)**0.5 )\n        \n  \n\n        final_features['soft_stock_mean_tvpl2_c1'] = np.log( s/l2*cluster_agg(\n            np.nanmean( (vol[:,:, 0:]/liq2[:,:, 0:]*l2/s)**(1/8), 2, keepdims=True),\n                lr1_clusters1, np.mean)**8**.5/v1)\n        final_features['soft_stock_mean_tvpl2_c2'] = np.log( s/l2*cluster_agg(\n            np.nanmean( (vol[:,:, 0:]/liq2[:,:, 0:]*l2/s)**(1/8), 2, keepdims=True),\n                lr1_clusters2, np.mean)**8**.5/v1)\n        final_features['soft_stock_mean_tvpl2_c3'] = np.log( s/l2*cluster_agg(\n            np.nanmean( (vol[:,:, 0:]/liq2[:,:, 0:]*l2/s)**(1/8), 2, keepdims=True),\n                lr1_clusters3, np.mean)**8**.5/v1)\n        \n        final_features['soft_stock_mean_tvpl2_10_c1'] = np.log( s/l2*cluster_agg(\n            np.nanmean( (vol[:,:, 10:]/liq2[:,:,10:]*l2/s)**(1/8), 2, keepdims=True),\n                lr1_clusters1, np.mean)**8**.5/v1)\n        final_features['soft_stock_mean_tvpl2_10_c2'] = np.log( s/l2*cluster_agg(\n            np.nanmean( (vol[:,:, 10:]/liq2[:,:,10:]*l2/s)**(1/8), 2, keepdims=True),\n                lr1_clusters2, np.mean)**8**.5/v1)\n        final_features['soft_stock_mean_tvpl2_10_c3'] = np.log( s/l2*cluster_agg(\n            np.nanmean( (vol[:,:, 10:]/liq2[:,:,10:]*l2/s)**(1/8), 2, keepdims=True),\n                lr1_clusters3, np.mean)**8**.5/v1)\n        \n        final_features['soft_stock_mean_tvpl2_20_c1'] = np.log( s/l2*cluster_agg(\n            np.nanmean( (vol[:,:, 20:]/liq2[:,:,20:]*l2/s)**(1/8), 2, keepdims=True),\n                lr1_clusters1, np.mean)**8**.5/v1)\n        final_features['soft_stock_mean_tvpl2_20_c2'] = np.log( s/l2*cluster_agg(\n            np.nanmean( (vol[:,:, 20:]/liq2[:,:,20:]*l2/s)**(1/8), 2, keepdims=True),\n                lr1_clusters2, np.mean)**8**.5/v1)\n        final_features['soft_stock_mean_tvpl2_20_c3'] = np.log( s/l2*cluster_agg(\n            np.nanmean( (vol[:,:, 20:]/liq2[:,:,20:]*l2/s)**(1/8), 2, keepdims=True),\n                lr1_clusters3, np.mean)**8**.5/v1)\n\n        \n        final_features['v1proj_25_c1_std'] = np.log( cluster_agg( np.log(np.mean(vol1[:,:,25:]**2,2,keepdims=True)\n                                                       / np.mean(vol1[:,:,:  ]**2,2,keepdims=True)),\n                                                        lr1_clusters1, np.nanstd)**0.5  )  \n        final_features['v1proj_25_c2_std'] = np.log( cluster_agg( np.log(np.mean(vol1[:,:,25:]**2,2,keepdims=True)\n                                                       / np.mean(vol1[:,:,:  ]**2,2,keepdims=True)),\n                                                        lr1_clusters2, np.nanstd)**0.5 )\n        final_features['v1proj_25_c3_std'] = np.log( cluster_agg( np.log(np.mean(vol1[:,:,25:]**2,2,keepdims=True)\n                                                       / np.mean(vol1[:,:,:  ]**2,2,keepdims=True)),\n                                                        lr1_clusters3, np.nanstd)**0.5 )\n        final_features['v1proj_25_c4_std'] = np.log( cluster_agg( np.log(np.mean(vol1[:,:,25:]**2,2,keepdims=True)\n                                                       / np.mean(vol1[:,:,:  ]**2,2,keepdims=True)),\n                                                        lr1_clusters4, np.nanstd)**0.5 )\n        final_features['v1proj_25_c5_std'] = np.log( cluster_agg( np.log(np.mean(vol1[:,:,25:]**2,2,keepdims=True)\n                                                       / np.mean(vol1[:,:,:  ]**2,2,keepdims=True)),\n                                                        lr1_clusters5, np.nanstd)**0.5 )\n        \n        \n        final_features['v1proj_25_vc1'] = np.log( cluster_agg( np.mean(vol1[:,:,25:]**2,2,keepdims=True)\n                                                       / np.mean(vol1[:,:,:  ]**2,2,keepdims=True),\n                                                        vol1_clusters1, np.median)**0.5  )  \n        final_features['v1proj_25_vc2'] = np.log( cluster_agg( np.mean(vol1[:,:,25:]**2,2,keepdims=True)\n                                                       / np.mean(vol1[:,:,:  ]**2,2,keepdims=True),\n                                                        vol1_clusters2, np.median)**0.5 )\n        final_features['v1proj_25_vc3'] = np.log( cluster_agg( np.mean(vol1[:,:,25:]**2,2,keepdims=True)\n                                                       / np.mean(vol1[:,:,:  ]**2,2,keepdims=True),\n                                                        vol1_clusters3, np.median)**0.5 )\n    \n        final_features['v1proj_25_vc4'] = np.log( cluster_agg( np.mean(vol1[:,:,25:]**2,2,keepdims=True)\n                                                       / np.mean(vol1[:,:,:  ]**2,2,keepdims=True),\n                                                        vol1_clusters4, np.median)**0.5 )\n\n        \n        final_features['v1proj_25_tc1'] = np.log( cluster_agg( np.mean(vol1[:,:,25:]**2,2,keepdims=True)\n                                                       / np.mean(vol1[:,:,:  ]**2,2,keepdims=True),\n                                                        tvpl3_clusters1, np.median)**0.5  )  \n        final_features['v1proj_25_tc2'] = np.log( cluster_agg( np.mean(vol1[:,:,25:]**2,2,keepdims=True)\n                                                       / np.mean(vol1[:,:,:  ]**2,2,keepdims=True),\n                                                        tvpl3_clusters2, np.median)**0.5 )\n        final_features['v1proj_25_tc3'] = np.log( cluster_agg( np.mean(vol1[:,:,25:]**2,2,keepdims=True)\n                                                       / np.mean(vol1[:,:,:  ]**2,2,keepdims=True),\n                                                        tvpl3_clusters3, np.median)**0.5 )\n        final_features['v1proj_25_tc4'] = np.log( cluster_agg( np.mean(vol1[:,:,25:]**2,2,keepdims=True)\n                                                       / np.mean(vol1[:,:,:  ]**2,2,keepdims=True),\n                                                        tvpl3_clusters4, np.median)**0.5 )\n\n        final_features['v1proj_25_vvc1'] = np.log( cluster_agg( np.mean(vol1[:,:,25:]**2,2,keepdims=True)\n                                                       / np.mean(vol1[:,:,:  ]**2,2,keepdims=True),\n                                                        vol_clusters1, np.median)**0.5  )  \n        final_features['v1proj_25_vvc2'] = np.log( cluster_agg( np.mean(vol1[:,:,25:]**2,2,keepdims=True)\n                                                       / np.mean(vol1[:,:,:  ]**2,2,keepdims=True),\n                                                        vol_clusters2, np.median)**0.5 )\n        final_features['v1proj_25_vvc3'] = np.log( cluster_agg( np.mean(vol1[:,:,25:]**2,2,keepdims=True)\n                                                       / np.mean(vol1[:,:,:  ]**2,2,keepdims=True),\n                                                        vol_clusters3, np.median)**0.5 )\n\n        \n        \n        final_features['v1spprojt15f25_c1'] = np.log( cluster_agg( \n                          - np.mean(lsprd[:,:,:15], (2),keepdims=True)\n                          + np.mean(lsprd[:,:,25:], (2),keepdims=True),\n                           lr1_clusters1, np.median) )\n        final_features['v1spprojt15f25_c2'] = np.log( cluster_agg( \n                          - np.mean(lsprd[:,:,:15], (2),keepdims=True)\n                          + np.mean(lsprd[:,:,25:], (2),keepdims=True),\n                           lr1_clusters2, np.median) )\n        final_features['v1spprojt15f25_c3'] = np.log( cluster_agg( \n                          - np.mean(lsprd[:,:,:15], (2),keepdims=True)\n                          + np.mean(lsprd[:,:,25:], (2),keepdims=True),\n                           lr1_clusters3, np.median) )\n        final_features['v1spprojt15f25_c4'] = np.log( cluster_agg( \n                          - np.mean(lsprd[:,:,:15], (2),keepdims=True)\n                          + np.mean(lsprd[:,:,25:], (2),keepdims=True),\n                           lr1_clusters4, np.median) )\n\n        final_features['v1spprojt15f25_vc1'] = np.log( cluster_agg( \n                          - np.mean(lsprd[:,:,:15], (2),keepdims=True)\n                          + np.mean(lsprd[:,:,25:], (2),keepdims=True),\n                           vol1_clusters1, np.median) )\n        final_features['v1spprojt15f25_vc2'] = np.log( cluster_agg( \n                          - np.mean(lsprd[:,:,:15], (2),keepdims=True)\n                          + np.mean(lsprd[:,:,25:], (2),keepdims=True),\n                           vol1_clusters2, np.median) )\n        final_features['v1spprojt15f25_vc3'] = np.log( cluster_agg( \n                          - np.mean(lsprd[:,:,:15], (2),keepdims=True)\n                          + np.mean(lsprd[:,:,25:], (2),keepdims=True),\n                           vol1_clusters3, np.median) )  \n    \n    \n    final_features['tvpl3_rmed2v1'] = np.log( np.median( \n                                        np.mean( rvol           , 2, keepdims=True)**(1/2)/\n                                        np.mean( r4liq3[:,:, 0:], 2, keepdims=True)**(4/3)/v1,\n                                            1, keepdims=True) )\n    final_features['tvpl3_rmed2v1_29'] = np.log( np.median( \n                                                np.mean( rvol           , 2, keepdims=True)**(1/2)/\n                                                np.mean( r4liq3[:,:, 29:], 2, keepdims=True)**(4/3)/v1,\n                                            1, keepdims=True) )\n\n    \n    final_features['tvpl2_rmed2v1']     = np.log( np.median( ( np.mean( vol              , 2, keepdims=True)**.5\n                                                    / np.mean( liq2[:,:, 0:]**.5, 2, keepdims=True))/v1, 1, keepdims=True))\n    final_features['tvpl2_rmed2v1lf25'] = np.log( np.median(( np.mean( vol, 2, keepdims=True)**.5\n                                                    / np.mean( (liq2[:,:, 25:])**.5, (2), keepdims=True))/v1, 1, keepdims=True))\n    final_features['tvpl2_rmed2v1lf29'] = np.log( np.median(( np.mean( vol, 2, keepdims=True)**.5\n                                                    / np.mean( (liq2[:,:, 29:])**.5, (2), keepdims=True))/v1, 1, keepdims=True))\n    \n    \n    \n    ###\n    final_features['tvpl2']        = np.log( np.mean( vol, 2, keepdims=True)**.5/np.mean( (liq2)**.5, (2), keepdims=True)/v1)\n    final_features['tvpl2_liqf10'] = np.log( np.mean( vol, 2, keepdims=True)**.5/np.mean( (liq2[:,:,10:])**.5, (2), keepdims=True)/v1)\n    final_features['tvpl2_liqf20'] = np.log( np.mean( vol, 2, keepdims=True)**.5/np.mean( (liq2[:,:,20:])**.5, (2), keepdims=True)/v1)\n    final_features['tvpl2_liqf29'] = np.log( np.mean( vol, 2, keepdims=True)**.5/np.mean( (liq2[:,:,29:])**.5, (2), keepdims=True)/v1)\n\n    final_features['tvpl2_smean_vol'       ] = np.log( np.mean( vol, (0,2), keepdims=True)**.5/np.mean( (liq2[:,:, 0:])**.5, (2), keepdims=True)/v1)\n    final_features['tvpl2_smean_vol_liqf10'] = np.log( np.mean( vol, (0,2), keepdims=True)**.5/np.mean( (liq2[:,:,10:])**.5, (2), keepdims=True)/v1)\n    final_features['tvpl2_smean_vol_liqf20'] = np.log( np.mean( vol, (0,2), keepdims=True)**.5/np.mean( (liq2[:,:,20:])**.5, (2), keepdims=True)/v1)\n    final_features['tvpl2_smean_vol_liqf29'] = np.log( np.mean( vol, (0,2), keepdims=True)**.5/np.mean( (liq2[:,:,29:])**.5, (2), keepdims=True)/v1)\n    \n    \n    final_features['tvpl3'       ] = np.log(np.mean( rvol, 2, keepdims=True)/np.mean( (r4liq3[:,:, 0:]), (2), keepdims=True)**(4/3)/v1)\n    final_features['tvpl3_liqf10'] = np.log(np.mean( rvol, 2, keepdims=True)/np.mean( (r4liq3[:,:,10:]), (2), keepdims=True)**(4/3)/v1)\n    final_features['tvpl3_liqf20'] = np.log(np.mean( rvol, 2, keepdims=True)/np.mean( (r4liq3[:,:,20:]), (2), keepdims=True)**(4/3)/v1)\n    final_features['tvpl3_liqf29'] = np.log(np.mean( rvol, 2, keepdims=True)/np.mean( (r4liq3[:,:,29:]), (2), keepdims=True)**(4/3)/v1)\n\n    final_features['tvpl3_smean_vol'       ] = np.log(np.mean( rvol, (0,2), keepdims=True)**.5/np.mean( (r4liq3[:,:, 0:]), (2), keepdims=True)**(4/3)/v1)\n    final_features['tvpl3_smean_vol_liqf10'] = np.log(np.mean( rvol, (0,2), keepdims=True)**.5/np.mean( (r4liq3[:,:,10:]), (2), keepdims=True)**(4/3)/v1)\n    final_features['tvpl3_smean_vol_liqf20'] = np.log(np.mean( rvol, (0,2), keepdims=True)**.5/np.mean( (r4liq3[:,:,20:]), (2), keepdims=True)**(4/3)/v1)\n    final_features['tvpl3_smean_vol_liqf29'] = np.log(np.mean( rvol, (0,2), keepdims=True)**.5/np.mean( (r4liq3[:,:,29:]), (2), keepdims=True)**(4/3)/v1)\n\n\n    \n    \n    \n    final_features['v1liq2projt5'] = np.log( ( np.mean( liq2[:,:,  : 5]**(1/8), 2, keepdims=True)**8\n                                            / np.mean( liq2[:,:,28:  ]       , 2, keepdims=True) )**(1/2) )\n    final_features['v1liq3projt5'] = np.log( ( np.mean( liq3[:,:,  : 5]**(1/8), 2, keepdims=True)**8\n                                            / np.mean( liq3[:,:,28:  ]       , 2, keepdims=True) )**(1/3) )\n    \n    final_features['v1liq2projt10'] = np.log( ( np.mean( liq2[:,:,  :10]**(1/8), 2, keepdims=True)**8\n                                             / np.mean( liq2[:,:,28:  ]       , 2, keepdims=True) )**(1/2) )\n    final_features['v1liq3projt10'] = np.log( ( np.mean( liq3[:,:,  :10]**(1/8), 2, keepdims=True)**8\n                                             / np.mean( liq3[:,:,28:  ]       , 2, keepdims=True) )**(1/3) )\n    \n    final_features['v1liq2projt20'] = np.log( ( np.mean( liq2[:,:,  :20]**(1/8), 2, keepdims=True)**8\n                                             / np.mean( liq2[:,:,28:  ]       , 2, keepdims=True) )**(1/2) )\n    final_features['v1liq3projt20'] = np.log( ( np.mean( liq3[:,:,  :20]**(1/8), 2, keepdims=True)**8\n                                             / np.mean( liq3[:,:,28:  ]       , 2, keepdims=True) )**(1/3) )\n    \n    final_features['liqt10rf29'] = np.log( np.mean( liq2[:,:,:10]**.5, (2), keepdims=True)**2 / liq2[:,:,29:] )\n    final_features['liqt20rf29'] = np.log( np.mean( liq2[:,:,:20]**.5, (2), keepdims=True)**2 / liq2[:,:,29:] )\n    \n    \n    final_features['v1liq2sprojt10f25'] = np.log( np.median(\n                          np.mean(liq2[:,:,  :10]**.125, (2),keepdims=True)**8/\n                          np.mean(liq2[:,:,25:  ]**.125, (2),keepdims=True)**8\n                        , 1, keepdims=True)**(1/2) )\n    final_features['v1liq2sprojt5f25'] = np.log( np.median(\n                          np.mean(liq2[:,:,  : 5]**.125, (2),keepdims=True)**8/\n                          np.mean(liq2[:,:,25:  ]**.125, (2),keepdims=True)**8\n                        , 1, keepdims=True)**(1/2) )\n    final_features['v1liq3sprojt15f25'] = np.log( np.median(\n                          np.mean(liq3[:,:,  :15]**.125, (2),keepdims=True)**8/\n                          np.mean(liq3[:,:,25:  ]**.125, (2),keepdims=True)**8\n                        , 1, keepdims=True)**(1/3) )\n    final_features['v1liq3sprojt5f25'] = np.log( np.median(\n                          np.mean(liq3[:,:,  : 5]**.125, (2),keepdims=True)**8/\n                          np.mean(liq3[:,:,25:  ]**.125, (2),keepdims=True)**8\n                        , 1, keepdims=True)**(1/3) )\n    \n    final_features['v1liq3projt15f25'] = np.log( (\n                          np.mean(liq3[:,:,  :15]**.125, (2),keepdims=True)**8/\n                          np.mean(liq3[:,:,25:  ]**.125, (2),keepdims=True)**8\n                           )**(1/3) )\n    \n    final_features['v1liq3sprojt10f25'] = np.log( np.median(\n                          np.mean(liq3[:,:,  :  ]**.125, (2),keepdims=True)**8/\n                          np.mean(liq3[:,:,25:  ]**.125, (2),keepdims=True)**8\n                        , 1, keepdims=True)**(1/3) )\n    \n    final_features['v1spprojt10f29'] = np.median(\n                          - np.mean(lsprd[:,:,  :  ], (2),keepdims=True)\n                          + np.mean(lsprd[:,:,29:  ], (2),keepdims=True)\n                                    , 1, keepdims=True)\n    final_features['v1spprojt15f25'] = np.median(\n                          - np.mean(lsprd[:,:,  :15], (2),keepdims=True)\n                          + np.mean(lsprd[:,:,25:  ], (2),keepdims=True)\n                           , 1, keepdims=True)\n    \n    final_features['v1spprojt15f29'] = np.median(\n                          - np.mean(lsprd[:,:,  :15], (2),keepdims=True)\n                          + np.mean(lsprd[:,:,29:  ], (2),keepdims=True)\n                                    , 1, keepdims=True)\n\n    final_features['v1spprojt15f29_q1'] = np.quantile(\n                          - np.mean(lsprd[:,:,:15], (2),keepdims=True)\n                          + np.mean(lsprd[:,:,29:], (2),keepdims=True)\n                                    ,0.25, 1, keepdims=True)\n    final_features['v1spprojt15f29_q3'] = np.quantile(\n                          - np.mean(lsprd[:,:,:15], (2),keepdims=True)\n                          + np.mean(lsprd[:,:,29:], (2),keepdims=True)\n                                    ,0.75, 1, keepdims=True)\n    \n    final_features['v1spprojt15f25_q1'] = np.quantile(\n                          - np.mean(lsprd[:,:,:15], (2),keepdims=True)\n                          + np.mean(lsprd[:,:,25:], (2),keepdims=True)\n                                    ,0.25, 1, keepdims=True)\n    final_features['v1spprojt15f25_q3'] = np.quantile(\n                          - np.mean(lsprd[:,:,:15], (2),keepdims=True)\n                          + np.mean(lsprd[:,:,25:], (2),keepdims=True)\n                                    ,0.75, 1, keepdims=True)\n    \n    \n    final_features['v1spprojtf29_q1'] = np.quantile(\n                          - np.mean(lsprd[:,:,:], (2),keepdims=True)\n                          + np.mean(lsprd[:,:,29:], (2),keepdims=True)\n                                    ,0.25, 1, keepdims=True)\n    final_features['v1spprojtf29_q3'] = np.quantile(\n                          - np.mean(lsprd[:,:,:], (2),keepdims=True)\n                          + np.mean(lsprd[:,:,29:], (2),keepdims=True)\n                                    ,0.75, 1, keepdims=True)\n    \n    \n    final_features['v1spprojtf25_q1'] = np.quantile(\n                          - np.mean(lsprd[:,:,:], (2),keepdims=True)\n                          + np.mean(lsprd[:,:,25:], (2),keepdims=True)\n                                    ,0.25, 1, keepdims=True)\n    final_features['v1spprojtf25_q3'] = np.quantile(\n                          - np.mean(lsprd[:,:,:], (2),keepdims=True)\n                          + np.mean(lsprd[:,:,25:], (2),keepdims=True)\n                                    ,0.75, 1, keepdims=True)\n    \n    \ndef read_targets_from_df(df, s, t):\n    t = list(t)\n    s = list(s)\n    \n    \n    Z = np.zeros((len(t), len(s)))\n    \n    dft = np.array(df['time_id'])\n    dfs = np.array(df['stock_id'])\n    dfr = np.array(df['target'])\n    \n    \n    for k in range(df.shape[0]):\n        Z[t.index(dft[k]), s.index(dfs[k])] = dfr[k]\n    return Z\n    \n    \n\ndef get_features(binned_features):\n    final_features = {}\n\n    final_features[ 'time_ids'] = (1*binned_features['time_ids'][:,np.newaxis] + 0*binned_features['stock_ids'][np.newaxis,:])[:,:,np.newaxis]\n    final_features['stock_ids'] = (0*binned_features['time_ids'][:,np.newaxis] + 1*binned_features['stock_ids'][np.newaxis,:])[:,:,np.newaxis]\n\n    final_features['vol1'] = np.mean(binned_features['vol1']**2, 2, keepdims=True)**0.5\n    final_features['cvol1'] = np.log(np.mean(binned_features['cvol1']**2, 2, keepdims=True)**0.5/final_features['vol1'])\n    final_features['evol1'] = np.log(np.mean(binned_features['evol1']**2, 2, keepdims=True)**0.5/final_features['vol1'])\n    final_features['e2vol1'] = np.log(np.mean(binned_features['e2vol1']**2, 2, keepdims=True)**0.5/final_features['vol1'])\n\n    final_features['lvol1'] = np.log(np.mean(binned_features['lvol1']**2, 2, keepdims=True)**0.5)\n    final_features['svol1'] = np.log(np.mean(binned_features['svol1']**2, 2, keepdims=True)**0.5)\n\n    \n    final_features['cvol1_15_15'] = np.log( np.mean(binned_features['cvol1'][:,:,15:  ]**2, 2, keepdims=True)**0.5\n                                          /np.mean(binned_features['cvol1'][:,:,  :15]**2, 2, keepdims=True)**0.5)\n    \n    final_features['lvol1_15_15'] = np.log( np.mean(binned_features['lvol1'][:,:,15:  ]**2, 2, keepdims=True)**0.5\n                                          /np.mean(binned_features['lvol1'][:,:,  :15]**2, 2, keepdims=True)**0.5)\n    final_features['svol1_15_15'] = np.log( np.mean(binned_features['svol1'][:,:,15:  ]**2, 2, keepdims=True)**0.5\n                                          /np.mean(binned_features['svol1'][:,:,  :15]**2, 2, keepdims=True)**0.5)\n    \n    final_features['evol1_15_15'] = np.log( np.mean(binned_features['evol1'][:,:,15:  ]**2, 2, keepdims=True)**0.5\n                                          /np.mean(binned_features['evol1'][:,:,  :15]**2, 2, keepdims=True)**0.5)\n    final_features['e2vol1_15_15'] = np.log( np.mean(binned_features['e2vol1'][:,:,15:  ]**2, 2, keepdims=True)**0.5\n                                          /np.mean(binned_features['e2vol1'][:,:,  :15]**2, 2, keepdims=True)**0.5)\n    final_features['vol2e_15_15'] = np.log( np.mean(binned_features['vol2e'][:,:,15:  ]**2, 2, keepdims=True)**0.5\n                                          /np.mean(binned_features['vol2e'][:,:,  :15]**2, 2, keepdims=True)**0.5)\n    \n    \n    final_features['vol2e_15_15s'] = np.log(np.median( np.mean(binned_features['vol2e'][:,:,15:  ]**2, 2, keepdims=True)**0.5\n                                                    / np.mean(binned_features['vol2e'][:,:,  :15]**2, 2, keepdims=True)**0.5,1,keepdims=True))\n    final_features['cvol1_15_15s'] = np.log(np.median( np.mean(binned_features['cvol1'][:,:,15:  ]**2, 2, keepdims=True)**0.5\n                                                    / np.mean(binned_features['cvol1'][:,:,  :15]**2, 2, keepdims=True)**0.5,1,keepdims=True))\n    final_features['svol1_15_15s'] = np.log(np.median( np.mean(binned_features['svol1'][:,:,15:  ]**2, 2, keepdims=True)**0.5\n                                                    / np.mean(binned_features['svol1'][:,:,  :15]**2, 2, keepdims=True)**0.5,1,keepdims=True))\n    final_features['vol2e'] = np.log(np.mean(binned_features['vol2e']**2, 2, keepdims=True)**0.5/final_features['vol1'])\n    \n    final_features['volq0_lm'] = np.log(np.mean(binned_features['volq0_lm']**2, 2, keepdims=True)**0.5/final_features['vol1'] )\n    final_features['volq0_lp'] = np.log(np.mean(binned_features['volq0_lp']**2, 2, keepdims=True)**0.5/final_features['vol1'] )\n    final_features['volq0_dt'] = final_features['volq0_lp'] - final_features['volq0_lm']\n\n    get_cohesion_features(binned_features, final_features)\n    get_cohesion_features(binned_features, final_features, ffrom=10)\n    get_cohesion_features(binned_features, final_features, ffrom=20)\n    \n    get_misc_features(binned_features, final_features)\n\n    get_simple_features(binned_features, final_features, 'vol1', ffrom=0)\n    get_simple_features(binned_features, final_features, 'vol1', ffrom=10)\n    get_simple_features(binned_features, final_features, 'vol1', ffrom=20)\n    get_simple_features(binned_features, final_features, 'vol1', ffrom=25)\n    \n    get_simple_features(binned_features, final_features, 'qvol1', ffrom=0)\n    get_simple_features(binned_features, final_features, 'qvol1', ffrom=10)\n    get_simple_features(binned_features, final_features, 'qvol1', ffrom=20)\n    get_simple_features(binned_features, final_features, 'qvol1', ffrom=25)\n    \n    \n    final_features['vol1_mean'] = np.log(final_features['vol1']/np.nanmean(final_features['vol1'], 0, keepdims=True))\n    \n    final_features['mean_half_delta'] = np.nanstd( np.log( np.mean( binned_features['vol1'][:,:,15:  ]**2, 2, keepdims=True)\n                                                         / np.mean( binned_features['vol1'][:,:,  :15]**2, 2, keepdims=True) )\n                                               , 0, keepdims=True)  \n    final_features['mean_half_delta_lsprd'] = np.log(np.nanstd( (  binned_features['log_spread'][:,:,-1: ]\n                                                         -  binned_features['log_spread'][:,:,  :1] )\n                                               , 0, keepdims=True) )\n    final_features['vol1'] = np.log(final_features['vol1'])\n    \n    return final_features\n\n\ntrain_feat = get_features(train_binned)\ntrain_feat['target'] = read_targets_from_df(train, train_binned['stock_ids'], train_binned['time_ids'])\n\ntime_features = ['liquidity2', 'vol1', 'lr1']\n\n\n\n\n\ndel train_binned\ngc.collect()\n\n\ntrain_df = merge_features_to_df(train_feat, train, [f for f in list(train_feat.keys()) if ('time_id' not in f)])\n \ntrain_df['lr1_clusters3'] = train_df['stock_id'].apply(lambda x: lr1_clusters3[int(np.argmax(np.array(train_feat['stock_ids'][0,:,0])==x))  ])\ntrain_df['lr1_clusters2'] = train_df['stock_id'].apply(lambda x: lr1_clusters2[int(np.argmax(np.array(train_feat['stock_ids'][0,:,0])==x))  ])\n\ndel train_df['target_x']\ntrain_df['target'] = train_df['target_y']\ndel train_df['target_y']\n\n\n\nif IS_GOOD_TEST:\n    test_feat = get_features(test_binned)\n    \n    del test_binned\n    gc.collect()\n    \n    test_df = merge_features_to_df(test_feat, test, [f for f in list(test_feat.keys()) if ('time_id' not in f)])\n    \n    test_df['lr1_clusters3'] = test_df['stock_id'].apply(lambda x: lr1_clusters3[int(np.argmax(np.array(test_feat['stock_ids'][0,:,0])==x))  ])\n    test_df['lr1_clusters2'] = test_df['stock_id'].apply(lambda x: lr1_clusters2[int(np.argmax(np.array(test_feat['stock_ids'][0,:,0])==x))  ])\n\nelse:\n    test_feat = {}\n    test_df = train_df.copy(deep=True)\n\n    \n    for key in train_feat.keys():\n        if key != 'target':\n            test_feat[key] = train_feat[key].copy()\n\n            \n    dummy_test = train.copy(deep=True)\n    \nfeatures = [f for f in list(train_feat.keys()) if ('time_id' not in f and 'stock_id' not in f and 'target' not in f)]\nfeatures.remove('vol1_from_0')\n\n\n","metadata":{"execution":{"iopub.status.busy":"2021-10-13T20:41:36.58984Z","iopub.execute_input":"2021-10-13T20:41:36.590254Z","iopub.status.idle":"2021-10-13T20:42:41.440152Z","shell.execute_reply.started":"2021-10-13T20:41:36.590207Z","shell.execute_reply":"2021-10-13T20:42:41.439184Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## LGBM Model","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import KFold\n\n\nmodel_name = 'lgbmodel'\npred_name ='lgbpred'\n\ndef feval_RMSPE(preds, train_data):\n    labels = train_data.get_label()\n    return 'RMSPE', round(rmspe(y_true = labels, y_pred = preds),5), False\n\ndef rmspe(y_true, y_pred):\n    return  (np.sqrt(np.mean(np.square((y_true - y_pred) / y_true))))\n\nseed1=11\n\n\nparams_lgbm = {\n        'learning_rate': 0.01,        \n        'lambda_l1': 4,\n        'lambda_l2': 7,\n        'num_leaves': 800,\n        'min_sum_hessian_in_leaf': 20,\n        'feature_fraction': 0.8,\n        'feature_fraction_bynode': 0.8,\n        'bagging_fraction': 0.9,\n        'bagging_freq': 42,\n        'min_data_in_leaf': 256,\n        'max_depth': 8,\n        'objective': 'regression',\n        'metric': 'None',\n    \n        'seed': seed1,\n        'feature_fraction_seed': seed1,\n        'bagging_seed': seed1,\n        'drop_seed': seed1,\n        'data_random_seed': seed1,\n        'boosting': 'gbdt',\n        'verbosity': -1,\n        'n_jobs':-1,\n    }\n\n\ncats=['stock_id']\n\n\nfeatures_to_consider = features + cats   \nfeatures_to_consider = list(np.unique(features_to_consider))\n\ntest_df['target'] = 0.0\n\n    \ngc.collect()\n    \nn_folds = 4\nn_rounds = 5000\nkf = model_selection.KFold(n_splits=n_folds, shuffle=False)\nscores_folds[model_name] = []\ncounter = 1\n\nu = np.unique(train['time_id'])\n\n\nall_models= []\n\ncomb_importances = []\n\ncluster_importances = {}\n\ncluster_key= 'lr1_clusters2'\nfull_score = 0.0\nfor cluster in range(np.max(train_df[cluster_key])+1):\n    models = []\n\n    print('CLUSTER:', cluster, np.mean(train_df[cluster_key]==cluster))\n    \n    train_cluster = train_df.loc[train_df[cluster_key]==cluster].reset_index(drop=True)\n    \n    kf = KFold(4)\n    \n    for ttids, vtids in kf.split(u):\n        \n        train_batch = train_cluster.loc[train_cluster['time_id'].isin(u[ttids])].reset_index(drop=True)\n        val_batch   = train_cluster.loc[train_cluster['time_id'].isin(u[vtids])].reset_index(drop=True)\n\n        \n        v1t = np.exp(train_batch['vol1'])\n        v1v = np.exp(  val_batch['vol1'])\n\n        X_train = train_batch[features_to_consider]\n        y_train = train_batch['target'].values\n        w_train = train_batch['target'].values **-2 * v1t**2\n\n        X_val = val_batch[features_to_consider]\n        y_val = val_batch['target'].values\n        w_val = val_batch['target'].values **-2 * v1v**2\n\n\n        train_data = lgb.Dataset(X_train, label=y_train/v1t, categorical_feature=cats, weight=w_train)\n        val_data   = lgb.Dataset(X_val,   label=  y_val/v1v,   categorical_feature=cats, weight=w_val  )\n\n\n\n        print('model')\n        model = lgb.train(params_lgbm, \n                          train_data, \n                          n_rounds, \n                          valid_sets=val_data, \n                          feval=feval_RMSPE,\n\n                          verbose_eval= 250,\n                          early_stopping_rounds=500)\n\n        models.append(model)\n        all_models.append(model)\n\n        p = model.predict(X_val)*v1v\n        score =  np.mean( ((p-y_val)/y_val)**2 )**0.5\n        \n        full_score += y_val.shape[0]*score**2\n        \n        \n        print('SCORE:', score)\n        print(nancorr(       p/v1v ,        y_val/v1v ))\n        print(nancorr(np.log(p/v1v), np.log(y_val/v1v)))\n\n        print(nancorr(p, y_val))\n        print(nancorr(np.log(p), np.log(y_val)))\n\n        \n        test_pred = (model.predict(test_df[features_to_consider][test_df[cluster_key]==cluster] )\n                                                                 *np.exp(test_df['vol1'][test_df[cluster_key]==cluster]) )\n        \n        test_df['target'][test_df[cluster_key]==cluster] += test_pred/n_folds\n\n\n        counter += 1\n\n\n\n    importances = pd.DataFrame({'Feature': model.feature_name(), \n                                'Importance': sum( [model.feature_importance(importance_type='gain') for model in models] )})\n\n    cluster_importances[cluster] = importances\n    \n\n    importances2 = importances.nlargest(40,'Importance', keep='first').sort_values(by='Importance', ascending=True)\n    importances2[['Importance', 'Feature']].plot(kind = 'barh', x = 'Feature', figsize = (8,6), color = 'blue', fontsize=11);plt.ylabel('Feature', fontsize=12)\n    plt.show()\n\n    \nlgbm_importances = pd.DataFrame({'Feature': model.feature_name(), \n                                'Importance': sum( [model.feature_importance(importance_type='gain') for model in all_models] )})\n    \ndel models, all_models\n    \n    \n\ndef importance(feature):\n    return importances[importances['Feature'] == feature]\n\nfull_score = ( full_score / train_df.shape[0] )**0.5\n\nprint('FULL SCORE:', full_score)\n","metadata":{"execution":{"iopub.status.busy":"2021-10-13T20:42:47.800979Z","iopub.execute_input":"2021-10-13T20:42:47.801356Z","iopub.status.idle":"2021-10-13T21:10:35.618613Z","shell.execute_reply.started":"2021-10-13T20:42:47.801317Z","shell.execute_reply":"2021-10-13T21:10:35.617457Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## NN Model","metadata":{}},{"cell_type":"code","source":"###### https://bignerdranch.com/blog/implementing-swish-activation-function-in-keras/\nimport tensorflow as tf\nfrom keras.backend import sigmoid\n\n\n\ndef swish(x, beta = 1, extrapolation=.667):\n    q = (x**2+1)**.5\n    qq = q**(1-extrapolation)\n\n    return tf.concat( ( x/qq * (x/q + 1)/2, -x/qq * (-x/q + 1)/2,    x, qq/q),  -1)\n\n\nfrom keras import backend as K\n\nfrom keras.utils.generic_utils import get_custom_objects\nfrom keras.layers import Activation\nget_custom_objects().update({'swish': Activation(swish)})\n\n\n\ndef root_mean_squared_per_error(y_true, y_pred):\n    return K.sqrt(K.mean( tf.cast(y_true>0,tf.float32) * K.square( (y_true - y_pred)/ (y_true+1e-10))) )\n    \n\nstock_embedding_size = 12\n\ncat_data = train['stock_id']\n\ndef base_model(n_sids, n_features):\n    \n    \n    stock_embedding_size = 4\n\n    \n    stock_id_input = keras.Input(shape=(n_sids,), name='stock_id')\n\n    num_input = keras.Input(shape=(n_sids, n_features), name='num_data')\n\n\n    x = num_input\n    \n    xm = tf.math.reduce_mean(x, axis=(0), keepdims=True)\n    xs = tf.math.reduce_std( x, axis=(0), keepdims=True)\n                            \n\n                            \n    x = x - xm\n    x = x / (xs + 1e-12)\n    \n    xs = keras.layers.Concatenate()((xm, xs)) \n    xs = keras.layers.Dense(16)(xs)\n    x = keras.layers.Concatenate()((x, xs + 0*x[:,:,:1])) \n    \n    \n    \n    stock_embedded = keras.layers.Embedding(max(cat_data)+1, stock_embedding_size, \n                                           input_length=1, name='stock_embedding')(stock_id_input)\n\n\n    x = keras.layers.GaussianNoise(.05)(x)\n    x = keras.layers.Concatenate()([stock_embedded, x])\n\n        \n\n    hidden_units = ( 256,128,64,32)\n    extrapolations = (.75, .75, .6, .45, .45)\n    dropouts = (0.15, 0.00, 0.0, 0., 0.)\n    \n    \n    for k in range(len(hidden_units)):\n        x = keras.layers.Dense(hidden_units[k], activation='linear')(x)\n        if dropouts[k] > 0:\n            x = keras.layers.Dropout(dropouts[k])(x)\n        x = swish(x, beta=1, extrapolation= extrapolations[k])\n    \n    q = keras.layers.Dense(16)(x)\n    \n    q = tf.transpose(q, [0,2,1])\n    q = keras.layers.SpatialDropout1D(.25)(q)\n    q = tf.transpose(q, [0,2,1])\n \n    q = swish(q, extrapolation=0.85)\n\n    \n    q = keras.layers.Flatten()(q)\n        \n    q = keras.layers.Dense(64)(q)\n    q = keras.layers.Dropout(.25)(q)  \n    q = swish(q, beta=1)\n    \n    \n    q = keras.layers.Dense(32)(q)\n    q = swish(q, beta=1,extrapolation=.6)\n    \n    \n    q = keras.layers.Dense(n_sids)(q)\n    q = keras.layers.Reshape([n_sids, 1])(q)\n    \n\n    out = keras.layers.Dense(1, activation='linear', name='prediction')(x) +  q\n    \n        \n    model = keras.Model(\n    inputs = [stock_id_input,  num_input],\n    outputs = out,\n    )\n    \n    return model","metadata":{"execution":{"iopub.status.busy":"2021-10-13T21:11:57.808479Z","iopub.execute_input":"2021-10-13T21:11:57.808895Z","iopub.status.idle":"2021-10-13T21:11:57.854976Z","shell.execute_reply.started":"2021-10-13T21:11:57.808862Z","shell.execute_reply":"2021-10-13T21:11:57.853695Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def low_reg_model(n_sids, n_features):\n    \n    # Each instance will consist of two inputs: a single user id, and a single movie id\n    \n    stock_embedding_size = 4\n\n    \n    stock_id_input = keras.Input(shape=(n_sids,), name='stock_id')\n\n    num_input = keras.Input(shape=(n_sids, n_features), name='num_data')\n\n    x = num_input\n\n    \n    xm = tf.math.reduce_mean(x, axis=(0), keepdims=True)\n    xs = tf.math.reduce_std( x, axis=(0), keepdims=True)\n                                                        \n    x = x - xm\n    x = x / (xs + 1e-12)\n    \n    xs = keras.layers.Concatenate()((xm, xs)) \n    xs = keras.layers.Dense(16)(xs)\n    x = keras.layers.Concatenate()((x, xs + 0*x[:,:,:1])) \n    \n    \n    stock_embedded = keras.layers.Embedding(max(cat_data)+1, stock_embedding_size, \n                                           input_length=1, name='stock_embedding')(stock_id_input)\n\n\n    x = keras.layers.GaussianNoise(.05)(x)\n\n    \n    x = keras.layers.Concatenate()([stock_embedded, x])\n\n        \n\n    hidden_units = ( 256,128,64,32)\n    extrapolations = (.75, .75, .6, .45, )\n    dropouts = (0.25, 0.0, 0.0, 0.0,)\n    \n    \n    for k in range(len(hidden_units)):\n        x = keras.layers.Dense(hidden_units[k], activation='linear')(x)\n        if dropouts[k] > 0:\n            x = keras.layers.Dropout(dropouts[k])(x)\n        x = swish(x, beta=1, extrapolation= extrapolations[k])\n        \n    \n    \n    q = keras.layers.Dense(16)(x)\n    \n    q = tf.transpose(q, [0,2,1])\n    q = keras.layers.SpatialDropout1D(.25)(q)\n    q = tf.transpose(q, [0,2,1])\n \n    q = swish(q, extrapolation=0.85)\n\n    \n    q = keras.layers.Flatten()(q)\n    \n    \n    q = keras.layers.Dense(64)(q)\n    q = keras.layers.Dropout(.25)(q)  \n    q = swish(q, beta=1)\n    \n    \n    q = keras.layers.Dense(32)(q)\n    q = swish(q, beta=1,extrapolation=.6)\n    \n    \n    q = keras.layers.Dense(n_sids)(q)\n    q = keras.layers.Reshape([n_sids, 1])(q)\n    \n\n    out = keras.layers.Dense(1, activation='linear', name='prediction')(x) +  q\n        \n    model = keras.Model(\n    inputs = [stock_id_input,  num_input],\n    outputs = out,\n    )\n    \n    return model","metadata":{"execution":{"iopub.status.busy":"2021-10-13T21:12:02.762054Z","iopub.execute_input":"2021-10-13T21:12:02.762423Z","iopub.status.idle":"2021-10-13T21:12:02.777244Z","shell.execute_reply.started":"2021-10-13T21:12:02.762391Z","shell.execute_reply":"2021-10-13T21:12:02.775959Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training","metadata":{}},{"cell_type":"code","source":"from sklearn import preprocessing, model_selection\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import QuantileTransformer\nfrom sklearn.metrics import r2_score\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import KFold\n\ndef set_field_from_dict(fdict, df, field_name):\n\n    T = fdict['time_ids']\n    S = fdict['stock_ids']\n    R = fdict[field_name]\n    \n    R = np.reshape( R, [R.shape[0]*R.shape[1]] )\n    S = np.reshape( S, [S.shape[0]*S.shape[1]] )\n    T = np.reshape( T, [T.shape[0]*T.shape[1]] )\n\n    dfz = pd.DataFrame(data=np.vstack([S,T,R]  ).T, columns=['stock_id', 'time_id',field_name])\n\n    dfz['time_id' ] = dfz['time_id' ].astype(int)\n    dfz['stock_id'] = dfz['stock_id'].astype(int)\n\n    dfz = df.merge(dfz, on=['stock_id', 'time_id'], how='left')\n    return dfz\n\ndef rmspe(y_true, y_pred):\n    return  np.sqrt(np.mean((y_true>0)*np.square((y_true - y_pred) / (y_true+1e-10) )))\n\n\nes = tf.keras.callbacks.EarlyStopping(\n    monitor='val_loss', patience=25, verbose=0,\n    mode='min',restore_best_weights=False)\n\nplateau = tf.keras.callbacks.ReduceLROnPlateau(\n    monitor='val_loss', factor=0.15, patience=8, verbose=0,\n    mode='min')\n\nmodel_name = 'full_model'\npred_name = model_name + '_pred'\n\ngc.collect()\n    \nn_folds = 4\n    \nX_test = keys_to_array(test_feat, features)\nX_test[np.isnan(X_test)] = 0\nX_test[X_test<-10000] = -10000\nX_test[X_test> 10000] =  10000\n    \nv_test = np.exp(test_feat['vol1'])\nS_test = test_feat['stock_ids']  \n\ntest_feat[pred_name] = 0*v_test\n                               \n    \n    \nX = keys_to_array(train_feat, features)\n\nfull_score=0\nkf = KFold(n_folds)\nfor ti,vi in kf.split(X):\n    \n        \n    X_train = X[ti,:,:]\n    v_train = np.exp(train_feat['vol1'][ti,:,:])\n    S_train = train_feat['stock_ids'][ti,:,:]   \n    y_train = train_feat['target'][ti,:]\n    \n    X_val = X[vi,:,:]\n    v_val = np.exp(train_feat['vol1'][vi,:,:])\n    S_val = train_feat['stock_ids'][vi,:,:]   \n    y_val = train_feat['target'][vi,:]\n\n    \n    X_train[np.isnan(X_train)] = 0\n    X_val[  np.isnan(X_val)  ] = 0\n    \n    X_train[X_train<-10000] = -10000\n    X_val[  X_val  <-10000] = -10000\n    \n    X_train[X_train>10000] = 10000\n    X_val[  X_val  >10000] = 10000\n\n \n    \n    Xm = np.median(    X_train, axis=(0,1), keepdims=True)\n    Xs = (np.quantile( X_train, .84, axis=(0,1), keepdims=True) - np.quantile( X_train, .16, axis=(0,1), keepdims=True))/2 + 1e-8\n    \n    ym = np.median(    np.log(y_train + 1e-12),      axis=(0,1), keepdims=True)\n    ys = (np.quantile( np.log(y_train + 1e-12), .84, axis=(0,1), keepdims=True) - np.quantile( np.log(y_train), .16, axis=(0,1), keepdims=True))/2 + 1e-8\n    \n    \n    \n    X_train = (X_train - Xm)/Xs\n    X_val   = (X_val   - Xm)/Xs\n    \n    X_train = np.tanh(X_train/5)*5\n    X_val   = np.tanh(X_val  /5)*5\n \n    model1= base_model(X_train.shape[1], X_train.shape[2])\n    \n    model1.compile(\n        keras.optimizers.RMSprop(learning_rate=0.005, rho=.9),\n\n        loss=root_mean_squared_per_error\n    )\n        \n    \n    model1.fit([S_train, X_train], \n              y_train[:,:,np.newaxis]/v_train,               \n              batch_size=64,\n              epochs=300, \n              validation_data=([S_val, X_val], y_val[:,:,np.newaxis]/v_val),\n              callbacks=[es, plateau],\n              validation_batch_size=y_val.shape[0],\n              shuffle=True,\n             verbose = 2)\n \n   \n    preds = (model1.predict([S_val,  X_val], batch_size=X_val.shape[0]))\n    \n    \n    \n    score = rmspe(y_val[:,:,np.newaxis], preds*v_val)\n   \n    full_score += score**2\n\n    print('SCORE:', score)\n    \n\n    test_pred = (model1.predict([ S_test,  np.tanh((X_test - Xm)/Xs/5)*5], batch_size=X_test.shape[0]) )*v_test\n\n    test_feat[pred_name] = test_feat[pred_name] + test_pred/n_folds\n    \n    gc.collect()\n    \nfull_score = (full_score/n_folds)**0.5\n\nprint('FULL SCORE', full_score)\n\n\n\n\nif IS_GOOD_TEST:\n    test       = set_field_from_dict(test_feat,       test, pred_name)\nelse:\n    dummy_test = set_field_from_dict(test_feat, dummy_test, pred_name)\n    \n\n","metadata":{"execution":{"iopub.status.busy":"2021-10-13T21:12:24.235232Z","iopub.execute_input":"2021-10-13T21:12:24.235802Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_name = 'select_model'\n\npred_name = model_name + '_pred'\n\nes = tf.keras.callbacks.EarlyStopping(\n    monitor='val_loss', patience=20, verbose=0,\n    mode='min',restore_best_weights=True)\n\nplateau = tf.keras.callbacks.ReduceLROnPlateau(\n    monitor='val_loss', factor=0.15, patience=7, verbose=0,\n    mode='min')\n\ngc.collect()\n    \nn_folds = 4\nkf = model_selection.KFold(n_splits=n_folds, shuffle=False)\ncounter = 1\n\n\nselect_features = lgbm_importances.nlargest(40,'Importance', keep='first').sort_values(by='Importance', ascending=True)\nselect_features = list(select_features['Feature'])\nselect_features.remove('stock_id')\n    \n\nX_test = keys_to_array(test_feat, select_features)\nX_test[np.isnan(X_test)] = 0\nX_test[X_test<-10000] = -10000\nX_test[X_test> 10000] =  10000\n    \nv_test = np.exp(test_feat['vol1'])\nS_test = test_feat['stock_ids']  \ntest_feat[pred_name] = 0*v_test\n                       \n                               \nX = keys_to_array(train_feat, select_features)\n\nfull_score=0\nkf = KFold(n_folds)\nfor ti,vi in kf.split(X):\n    \n    print(ti.shape[0], vi.shape[0], X.shape[0])\n        \n    X_train = X[ti,:,:]\n    v_train = np.exp(train_feat['vol1'][ti,:,:])\n    S_train = train_feat['stock_ids'][ti,:,:]   \n    y_train = train_feat['target'][ti,:]\n    \n    X_val = X[vi,:,:]\n    v_val = np.exp(train_feat['vol1'][vi,:,:])\n    S_val = train_feat['stock_ids'][vi,:,:]   \n    y_val = train_feat['target'][vi,:]\n\n    \n    X_train[np.isnan(X_train)] = 0\n    X_val[  np.isnan(X_val)  ] = 0\n    \n    X_train[X_train<-10000] = -10000\n    X_val[  X_val  <-10000] = -10000\n    \n    X_train[X_train>10000] = 10000\n    X_val[  X_val  >10000] = 10000\n\n    \n    Xm = np.median(    X_train,      axis=(0,1), keepdims=True)\n    Xs = (np.quantile( X_train, .84, axis=(0,1), keepdims=True) - np.quantile( X_train, .16, axis=(0,1), keepdims=True))/2 + 1e-8\n    \n    ym = np.median(    np.log(y_train + 1e-12),      axis=(0,1), keepdims=True)\n    ys = (np.quantile( np.log(y_train + 1e-12), .84, axis=(0,1), keepdims=True) - np.quantile( np.log(y_train), .16, axis=(0,1), keepdims=True))/2 + 1e-8\n    \n    \n    X_train = (X_train - Xm)/Xs\n    X_val   = (X_val   - Xm)/Xs\n    \n    X_train = np.tanh(X_train/5)*5\n    X_val   = np.tanh(X_val  /5)*5\n\n\n    \n    model1= low_reg_model(X_train.shape[1], X_train.shape[2])\n    \n    model1.compile(\n        keras.optimizers.RMSprop(learning_rate=0.005, rho=.9),\n\n        loss=root_mean_squared_per_error\n    )\n        \n    model1.fit([S_train, X_train], \n              y_train[:,:,np.newaxis]/v_train,               \n              batch_size=64,\n              epochs=200, \n              validation_data=([S_val, X_val], y_val[:,:,np.newaxis]/v_val),\n              callbacks=[es, plateau],\n              validation_batch_size=y_val.shape[0],\n              shuffle=True,\n             verbose = 2)\n \n   \n    preds = (model1.predict([S_val,  X_val], batch_size=X_val.shape[0]))\n        \n    score = rmspe(y_val[:,:,np.newaxis], preds*v_val)\n   \n    full_score += score**2\n\n    print('SCORE:', score)\n    \n\n    test_pred = (model1.predict([ S_test,  np.tanh((X_test - Xm)/Xs/5)*5], batch_size=X_test.shape[0]) )*v_test\n\n\n    \n    test_feat[pred_name] = test_feat[pred_name] + test_pred/n_folds\n\n    \n    gc.collect()\n    \nfull_score = (full_score/n_folds)**0.5\n\nprint('FULL SCORE', full_score)\n\n\n\n\nif IS_GOOD_TEST:\n    test = set_field_from_dict(test_feat, test, pred_name)\nelse:\n    dummy_test = set_field_from_dict(test_feat, dummy_test, pred_name)\n    \n    \n    \n    \nif IS_GOOD_TEST:\n    test['target']       = 0.6*      test['full_model_pred'] + 0.4*      test['select_model_pred']\nelse:\n    dummy_test['target'] = 0.6*dummy_test['full_model_pred'] + 0.4*dummy_test['select_model_pred']\n  \n    \n    print(dummy_test)\n    print(dummy_test.columns)\n    comb_test = dummy_test.merge(train, on=['time_id','stock_id'], how='right')\n    comb_test['target_y'] = 1.0*comb_test['target_y']\n    \n    print(comb_test.columns)\n    \n    print('TEST SCORE:', np.mean( (  (comb_test['target_x']-comb_test['target_y'])/comb_test['target_y']  )**2)**0.5)\n    \n    test['target'] = 0\n\n    \n    \n    \ntest['row_id'] = test['stock_id'].astype(str) + '-' + test['time_id'].astype(str)\n\nprint(test)\n\ntest[['row_id', 'target']].to_csv('submission.csv',index = False)","metadata":{"execution":{"iopub.status.busy":"2021-10-13T20:37:14.429378Z","iopub.status.idle":"2021-10-13T20:37:14.429946Z"},"trusted":true},"execution_count":null,"outputs":[]}]}