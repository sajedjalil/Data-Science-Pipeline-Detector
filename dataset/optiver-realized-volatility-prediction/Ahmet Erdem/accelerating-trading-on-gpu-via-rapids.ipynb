{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Accelerating Trading on GPU via RAPIDS\n## Best scoring CPU kernel is accelerated on GPU. 3.5x Speedup!!!\n\n![](https://i.imgur.com/lkjVW2f.png)","metadata":{"papermill":{"duration":0.014217,"end_time":"2021-07-08T14:58:35.236592","exception":false,"start_time":"2021-07-08T14:58:35.222375","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# https://www.kaggle.com/aerdem4/optiver-pytorch-gpu-no-feature-eng\n\ndef nn_notebook():\n    import numpy as np\n    import pandas as pd\n    import glob\n    from tqdm import tqdm\n    import sys, os\n\n\n    def load_data(mode, path=\"/kaggle/input/optiver-realized-volatility-prediction\"):\n        # mode = \"train\"/\"test\"\n        file_name = f'{path}/{mode}.csv'\n        return pd.read_csv(file_name)\n\n    df = load_data(\"test\")\n    \n    SCALE = 100\n    PATH = \"/kaggle/input/optiver-realized-volatility-prediction\"\n\n    order_book_paths = glob.glob(f'{PATH}/book_test.parquet/*/*')\n    trade_paths = glob.glob(f'{PATH}/trade_test.parquet/*/*')\n\n    \n    order_books = dict()\n    for path in tqdm(order_book_paths):\n        stock_id = int(path.split(\"=\")[1].split(\"/\")[0])\n        book_df = pd.read_parquet(path)\n        books_by_time = dict()\n\n        for time_id in book_df.time_id.unique():\n            books_by_time[time_id] = book_df[book_df[\"time_id\"] == time_id].reset_index(drop=True)\n\n        order_books[stock_id] = books_by_time\n    \n    trades = dict()\n    for path in tqdm(trade_paths):\n        stock_id = int(path.split(\"=\")[1].split(\"/\")[0])\n        trade_df = pd.read_parquet(path)\n        trade_by_time = dict()\n\n        for time_id in trade_df.time_id.unique():\n            trade_by_time[time_id] = trade_df[trade_df[\"time_id\"] == time_id].reset_index(drop=True)\n\n        trades[stock_id] = trade_by_time\n        \n    import torch\n    import torch.nn as nn\n    from torch.utils.data import DataLoader, Dataset\n\n\n    means_order = torch.FloatTensor([  0.9997,   1.0003, 769.9902, 766.7346,   0.9995,   1.0005, 959.3417,\n            928.2203, 300])\n    stds_order = torch.FloatTensor([3.6881e-03, 3.6871e-03, 5.3541e+03, 4.9549e+03, 3.7009e-03, 3.6991e-03,\n            6.6838e+03, 5.7353e+03, 300])\n\n    means_trade = torch.FloatTensor([300, 1.0, 100, 3.0])\n    stds_trade = torch.FloatTensor([300, 0.004, 153, 3.5])\n\n\n\n    class OptiverDataset(Dataset):\n\n        def __init__(self, df, aug=False):\n            super().__init__()\n            self.df = df.reset_index(drop=True)\n            self.aug = aug\n            self.seq_len = 600\n            self.order_features = ['bid_price1', 'ask_price1', 'bid_size1', 'ask_size1','bid_price2', \n                             'ask_price2', 'bid_size2', 'ask_size2', \"seconds_in_bucket\"]\n            self.trade_features = [\"seconds_in_bucket\", \"price\", \"size\", \"order_count\"]\n\n\n        def extract_features(self, data_dict, stock_id, time_id, features, means, stds):\n            X = -torch.ones((self.seq_len, len(features)))\n            try:\n                df = data_dict[stock_id][time_id]\n                feature_array = df[features].values\n                X[-feature_array.shape[0]:] = (torch.FloatTensor(feature_array) - means)/stds\n            except:\n                pass\n            return X\n\n\n        def __getitem__(self, index):\n            row = self.df.iloc[index]\n\n            X1 = self.extract_features(order_books, row.stock_id, row.time_id, self.order_features,\n                                      means_order, stds_order)\n            try:\n                X2 = self.extract_features(trades, row.stock_id, row.time_id, self.trade_features,\n                                          means_trade, stds_trade) \n            except:\n                X2 = -torch.ones((self.seq_len, len(self.trade_features)))\n            target = torch.FloatTensor([0.0])\n            stock = torch.LongTensor([row.stock_id])\n            return X1, X2, stock, target\n\n        def __len__(self):\n            return self.df.shape[0]\n\n    ds = OptiverDataset(df)\n    \n    class ConvBlock(nn.Module):\n        def __init__(self, in_dim, out_dim, kernel_size, stride=1):\n            super().__init__()\n            self.lin = nn.Conv1d(in_dim, out_dim, kernel_size, stride=stride)\n            self.bn = nn.BatchNorm1d(out_dim)\n            self.activation = nn.ReLU()\n\n        def forward(self, x):\n            x = self.lin(x)\n            x = self.bn(x)\n            return self.activation(x)\n\n\n    class SubModel(nn.Module):\n        def __init__(self, in_dim):\n            super().__init__()\n            self.convs1 = nn.Sequential(ConvBlock(in_dim, 16, 3),\n                                       ConvBlock(16, 32, 3))\n            self.stock_conv = ConvBlock(36, 64, 4, stride=4)\n            self.avg_pool = nn.AdaptiveAvgPool1d(8)\n            self.max_pool = nn.AdaptiveMaxPool1d(8)\n            self.convs2 = nn.Sequential(ConvBlock(128, 128, 2, stride=2),\n                                        ConvBlock(128, 32, 2, stride=2),\n                                        ConvBlock(32, 8, 2, stride=2))\n\n        def forward(self, x, s):\n            x = self.convs1(x.transpose(2, 1))\n            x = self.stock_conv(torch.cat([x, s.repeat(1, 1, x.shape[2])], axis=1))\n            x = torch.cat([self.avg_pool(x), self.max_pool(x)], axis=1)\n            x = self.convs2(x).squeeze(-1)\n            return x\n\n\n    class Model(nn.Module):\n        def __init__(self):\n            super().__init__()\n            self.order_model = SubModel(in_dim=9)\n            self.trade_model = SubModel(in_dim=4)\n            self.top = nn.Linear(16, 1)\n            self.stock_emb = nn.Embedding(127, 4)\n\n        def forward(self, inputs):\n            x1, x2, s = inputs\n            s = self.stock_emb(s).transpose(2, 1)\n\n            x1 = self.order_model(x1, s)\n            x2 = self.trade_model(x2, s)\n            x = self.top(torch.cat([x1, x2], axis=1))\n            return x\n        \n    def read_data(data):\n        return tuple(d.cuda() for d in data[:-1]), data[-1].cuda()\n\n    def inference(model, loader, num_folds=5):\n        model.eval()\n\n        tbar = tqdm(loader, file=sys.stdout)\n\n        preds = []\n\n        model_weights = {i: torch.load(f\"/kaggle/input/optiver-nn/optiver_nn_v01_{i}.pth\") for i in range(num_folds)}\n\n        with torch.no_grad():\n            for idx, data in enumerate(tbar):\n                inputs, target = read_data(data)\n\n                model.load_state_dict(model_weights[0])\n                pred = model(inputs)/num_folds\n                for i in range(1, num_folds):\n                    model.load_state_dict(model_weights[i])\n                    pred += model(inputs)/num_folds\n\n\n                preds.append(pred.detach().cpu().numpy().ravel())\n\n        return np.concatenate(preds)\n\n    NW = 4\n    BS = 256\n    loader = DataLoader(ds, batch_size=BS, shuffle=False, num_workers=NW, pin_memory=False, drop_last=False)\n\n\n    model = Model()\n    model = model.cuda()\n\n    y = inference(model, loader)\n    \n    df[\"nn_pred\"] = np.clip(y, 0.0, None)/SCALE\n\n    df.to_csv(\"nn_preds.csv\", index=False, columns=[\"stock_id\", \"time_id\", \"nn_pred\"])\nnn_notebook()","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-08-27T18:26:44.710398Z","iopub.execute_input":"2021-08-27T18:26:44.7108Z","iopub.status.idle":"2021-08-27T18:26:51.379579Z","shell.execute_reply.started":"2021-08-27T18:26:44.710711Z","shell.execute_reply":"2021-08-27T18:26:51.378554Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import cupy as cp\nimport cudf\nimport cuml\nimport glob\nfrom tqdm import tqdm\n\ncudf.__version__","metadata":{"papermill":{"duration":4.333354,"end_time":"2021-07-08T14:58:39.584138","exception":false,"start_time":"2021-07-08T14:58:35.250784","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-08-27T18:26:51.381543Z","iopub.execute_input":"2021-08-27T18:26:51.38193Z","iopub.status.idle":"2021-08-27T18:26:55.661622Z","shell.execute_reply.started":"2021-08-27T18:26:51.381887Z","shell.execute_reply":"2021-08-27T18:26:55.66083Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"PATH = \"/kaggle/input/optiver-realized-volatility-prediction\"\n\n\ndef load_data(mode, path=\"/kaggle/input/optiver-realized-volatility-prediction\"):\n    # mode = \"train\"/\"test\"\n    file_name = f'{path}/{mode}.csv'\n    return cudf.read_csv(file_name)\n\ndev_df = load_data(\"train\", path=PATH)\ndev_df.head()","metadata":{"papermill":{"duration":3.829788,"end_time":"2021-07-08T14:58:43.427608","exception":false,"start_time":"2021-07-08T14:58:39.59782","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-08-27T18:26:55.663286Z","iopub.execute_input":"2021-08-27T18:26:55.663555Z","iopub.status.idle":"2021-08-27T18:26:59.213906Z","shell.execute_reply.started":"2021-08-27T18:26:55.66353Z","shell.execute_reply":"2021-08-27T18:26:59.213095Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"SCALE = 100\ndev_df[\"target\"] *= SCALE\n\nstock_ids = dev_df[\"stock_id\"].unique()\nlen(stock_ids)","metadata":{"execution":{"iopub.status.busy":"2021-08-27T18:26:59.216938Z","iopub.execute_input":"2021-08-27T18:26:59.217189Z","iopub.status.idle":"2021-08-27T18:26:59.735786Z","shell.execute_reply.started":"2021-08-27T18:26:59.217165Z","shell.execute_reply":"2021-08-27T18:26:59.734985Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"order_book_training = glob.glob(f'{PATH}/book_train.parquet/*/*')\norder_book_test = glob.glob(f'{PATH}/book_test.parquet/*/*')\n\nlen(order_book_training), len(order_book_test)","metadata":{"papermill":{"duration":0.207773,"end_time":"2021-07-08T14:58:43.649518","exception":false,"start_time":"2021-07-08T14:58:43.441745","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-08-27T18:26:59.737104Z","iopub.execute_input":"2021-08-27T18:26:59.737442Z","iopub.status.idle":"2021-08-27T18:27:00.004201Z","shell.execute_reply.started":"2021-08-27T18:26:59.737395Z","shell.execute_reply":"2021-08-27T18:27:00.003459Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trades_training = glob.glob(f'{PATH}/trade_train.parquet/*/*')\ntrades_test = glob.glob(f'{PATH}/trade_test.parquet/*/*')\n\nlen(trades_training), len(trades_test)","metadata":{"execution":{"iopub.status.busy":"2021-08-27T18:27:00.006814Z","iopub.execute_input":"2021-08-27T18:27:00.007067Z","iopub.status.idle":"2021-08-27T18:27:00.255195Z","shell.execute_reply.started":"2021-08-27T18:27:00.007043Z","shell.execute_reply":"2021-08-27T18:27:00.254455Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Using rapids-kaggle-utils for missing cuDF aggregation functions","metadata":{"papermill":{"duration":0.014186,"end_time":"2021-07-08T14:58:43.677748","exception":false,"start_time":"2021-07-08T14:58:43.663562","status":"completed"},"tags":[]}},{"cell_type":"code","source":"%cd /kaggle/input/rapids-kaggle-utils/","metadata":{"papermill":{"duration":0.023081,"end_time":"2021-07-08T14:58:43.714941","exception":false,"start_time":"2021-07-08T14:58:43.69186","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-08-27T18:27:00.256383Z","iopub.execute_input":"2021-08-27T18:27:00.256742Z","iopub.status.idle":"2021-08-27T18:27:00.264256Z","shell.execute_reply.started":"2021-08-27T18:27:00.256707Z","shell.execute_reply":"2021-08-27T18:27:00.263267Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import cu_utils.transform as cutran\n\n\n\ndef log_diff(df, in_col, null_val):\n    df[\"logx\"] = df[in_col].log()\n    df[\"logx_shifted\"] = (df[[\"time_id\", \"logx\"]].groupby(\"time_id\")\n                             .apply_grouped(cutran.get_cu_shift_transform(shift_by=1, null_val=null_val),\n                                            incols={\"logx\": 'x'},\n                                            outcols=dict(y_out=cp.float32),\n                                            tpb=32)[\"y_out\"])\n    df[\"keep_row\"] = df[f\"logx_shifted\"] != null_val\n    return df[\"logx\"] - df[\"logx_shifted\"]\n\n\n\ndef extract_raw_book_features(df, null_val=-9999):\n    for n in range(1, 3):\n        p1 = df[f\"bid_price{n}\"]\n        p2 = df[f\"ask_price{n}\"]\n        s1 = df[f\"bid_size{n}\"]\n        s2 = df[f\"ask_size{n}\"]\n        df[f\"wap{n}\"] = (p1*s2 + p2*s1) / (s1 + s2)\n        df[f\"log_return{n}\"] = log_diff(df, in_col=f\"wap{n}\", null_val=null_val)\n        df[f\"realized_vol{n}\"] = df[f\"log_return{n}\"]**2\n        \n    df['wap_balance'] = abs(df['wap1'] - df['wap2'])\n    df['price_spread'] = (df['ask_price1'] - df['bid_price1']) / ((df['ask_price1'] + df['bid_price1']) / 2)\n    df['bid_spread'] = df['bid_price1'] - df['bid_price2']\n    df['ask_spread'] = df['ask_price1'] - df['ask_price2']\n    df['total_volume'] = (df['ask_size1'] + df['ask_size2']) + (df['bid_size1'] + df['bid_size2'])\n    df['volume_imbalance'] = abs((df['ask_size1'] + df['ask_size2']) - (df['bid_size1'] + df['bid_size2']))\n    df[\"c\"] = 1\n    \n    df = df[df[\"keep_row\"]]\n    return df\n\n\ndef extract_raw_trade_features(df, null_val=-9999):\n    df[\"realized_vol_trade\"] = log_diff(df, in_col=f\"price\", null_val=null_val)**2\n    df = df[df[\"keep_row\"]]\n    return df\n\n\ndef agg(df, feature_dict):\n    agg_df = df.groupby(\"time_id\").agg(feature_dict).reset_index()\n    def f(x):\n        if x[1] == \"\":\n            return x[0]\n        return x[0] + \"_\" + x[1]\n    \n    agg_df.columns = [f(x) for x in agg_df.columns]\n    return agg_df    \n\n\ndef extract_book_stats(df):\n    default_stats = [\"sum\", \"mean\", \"std\"]\n    feature_dict = {\n        'wap1': default_stats,\n        'wap2': default_stats,\n        'log_return1': default_stats,\n        'log_return2': default_stats,\n        'wap_balance': default_stats,\n        'price_spread': default_stats,\n        'bid_spread': default_stats,\n        'ask_spread': default_stats,\n        'total_volume': default_stats,\n        'volume_imbalance': default_stats,\n        'c': [\"sum\"],\n        'realized_vol1': [\"sum\"],\n        'realized_vol2': [\"sum\"],\n    }\n    \n    return agg(df, feature_dict)\n    \n\n    \n    \ndef extract_trade_stats(df):\n    feature_dict = {\n        'realized_vol_trade': [\"sum\"],\n        'seconds_in_bucket':[\"count\"],\n        'size': [\"sum\"],\n        'order_count': [\"mean\"],\n    }\n    \n    return agg(df, feature_dict)\n\n\ndef time_constraint_fe(df, stats_df, last_sec, fe_function, cols):\n    sub_df = df[df[\"seconds_in_bucket\"] >= (600 - last_sec)].reset_index(drop=True)\n    if sub_df.shape[0] > 0:\n        sub_stats = fe_function(sub_df)\n    else:\n        sub_stats = cudf.DataFrame(columns=cols)\n    return stats_df.merge(sub_stats, on=\"time_id\", how=\"left\", suffixes=('', f'_{last_sec}'))    \n    \n\ndef feature_engineering(book_path, trade_path):\n    book_df = cudf.read_parquet(book_path)\n    book_df = extract_raw_book_features(book_df)\n    book_stats = extract_book_stats(book_df)\n    book_cols = book_stats.columns\n    \n    trade_df = cudf.read_parquet(trade_path)\n    trade_df = extract_raw_trade_features(trade_df)\n    trade_stats = extract_trade_stats(trade_df)\n    trade_cols = trade_stats.columns\n    \n    for last_sec in [150, 300, 450]:\n        book_stats = time_constraint_fe(book_df, book_stats, last_sec, extract_book_stats, book_cols) \n        trade_stats = time_constraint_fe(trade_df, trade_stats, last_sec, extract_trade_stats, trade_cols) \n\n    return book_stats.merge(trade_stats, on=\"time_id\", how=\"left\")\n\n\ndef process_data(order_book_paths, trade_paths, stock_ids):\n    stock_dfs = []\n    for book_path, trade_path in tqdm(list(zip(order_book_paths, trade_paths))):\n        stock_id = int(book_path.split(\"=\")[1].split(\"/\")[0])\n\n        df = feature_engineering(book_path, trade_path)\n        df[\"stock_id\"] = stock_id\n        stock_dfs.append(df)\n    return cudf.concat(stock_dfs)","metadata":{"papermill":{"duration":0.044884,"end_time":"2021-07-08T14:58:43.774168","exception":false,"start_time":"2021-07-08T14:58:43.729284","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-08-27T18:27:00.267675Z","iopub.execute_input":"2021-08-27T18:27:00.267951Z","iopub.status.idle":"2021-08-27T18:27:00.314352Z","shell.execute_reply.started":"2021-08-27T18:27:00.267925Z","shell.execute_reply":"2021-08-27T18:27:00.313659Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"past_volatility = process_data(order_book_training, trades_training, stock_ids)\npast_test_volatility = process_data(order_book_test, trades_test, stock_ids)\n\npast_volatility.shape, past_test_volatility.shape","metadata":{"papermill":{"duration":128.128858,"end_time":"2021-07-08T15:00:51.917263","exception":false,"start_time":"2021-07-08T14:58:43.788405","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-08-27T18:27:00.315791Z","iopub.execute_input":"2021-08-27T18:27:00.316158Z","iopub.status.idle":"2021-08-27T18:30:29.964177Z","shell.execute_reply.started":"2021-08-27T18:27:00.316125Z","shell.execute_reply":"2021-08-27T18:30:29.963196Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Get NN with no FE features from https://www.kaggle.com/aerdem4/optiver-pytorch-gpu-no-feature-eng","metadata":{}},{"cell_type":"code","source":"past_volatility = past_volatility.merge(cudf.read_csv(\"/kaggle/input/optiver-nn/optiver_nn_v01_oof.csv\"), on=[\"stock_id\", \"time_id\"], how=\"left\")\npast_test_volatility = past_test_volatility.merge(cudf.read_csv(\"/kaggle/working/nn_preds.csv\"), on=[\"stock_id\", \"time_id\"], how=\"left\")\n\n\npast_volatility[\"nn_pred\"] = past_volatility[\"nn_pred\"].clip(0.0, None)*SCALE\npast_test_volatility[\"nn_pred\"] = past_test_volatility[\"nn_pred\"].clip(0.0, None)*SCALE","metadata":{"execution":{"iopub.status.busy":"2021-08-27T18:30:29.96559Z","iopub.execute_input":"2021-08-27T18:30:29.965964Z","iopub.status.idle":"2021-08-27T18:30:30.268178Z","shell.execute_reply.started":"2021-08-27T18:30:29.965928Z","shell.execute_reply":"2021-08-27T18:30:30.26652Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def stock_time_fe(df):\n    cols = ['realized_vol1_sum', 'realized_vol2_sum', 'realized_vol_trade_sum',\n            'realized_vol1_sum_150', 'realized_vol2_sum_150', 'realized_vol_trade_sum_150',\n            'realized_vol1_sum_300', 'realized_vol2_sum_300', 'realized_vol_trade_sum_300',\n            'realized_vol1_sum_450', 'realized_vol2_sum_450', 'realized_vol_trade_sum_450',\n            'nn_pred']\n    \n    for agg_col in [\"stock_id\", \"time_id\"]:\n        for agg_func in [\"mean\", \"max\", \"std\", \"min\"]:\n            agg_df = df.groupby(agg_col)[cols].agg(agg_func)\n            agg_df.columns = [f\"{agg_col}_{agg_func}_{col}\" for col in agg_df.columns]\n            df = df.merge(agg_df.reset_index(), on=agg_col, how=\"left\")\n    \n    return df\n\npast_volatility[\"is_test\"] = False\npast_test_volatility[\"is_test\"] = True\nall_df = past_volatility.append(past_test_volatility).reset_index(drop=True)\n\nall_df = stock_time_fe(all_df)\n\npast_volatility = all_df[~all_df[\"is_test\"]]\npast_test_volatility = all_df[all_df[\"is_test\"]]","metadata":{"execution":{"iopub.status.busy":"2021-08-27T18:30:30.269506Z","iopub.execute_input":"2021-08-27T18:30:30.269882Z","iopub.status.idle":"2021-08-27T18:30:32.141453Z","shell.execute_reply.started":"2021-08-27T18:30:30.269842Z","shell.execute_reply":"2021-08-27T18:30:32.140592Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dev_df = dev_df.merge(past_volatility, on=[\"stock_id\", \"time_id\"], how=\"left\")\n\nfeatures = [col for col in list(dev_df.columns)\n            if col not in {\"stock_id\", \"time_id\", \"target\", \"is_test\"}]\nlen(features)","metadata":{"execution":{"iopub.status.busy":"2021-08-27T18:30:32.142962Z","iopub.execute_input":"2021-08-27T18:30:32.143272Z","iopub.status.idle":"2021-08-27T18:30:32.217385Z","shell.execute_reply.started":"2021-08-27T18:30:32.143237Z","shell.execute_reply":"2021-08-27T18:30:32.216416Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Train XGBoost model on GPU","metadata":{"papermill":{"duration":0.043524,"end_time":"2021-07-08T15:00:52.944345","exception":false,"start_time":"2021-07-08T15:00:52.900821","status":"completed"},"tags":[]}},{"cell_type":"code","source":"import xgboost as xgb\n\ndef rmspe(y_true, y_pred):\n    return (cp.sqrt(cp.mean(cp.square((y_true - y_pred) / y_true))))\n\n\ndef rmspe_xgb(pred, dtrain):\n    y = dtrain.get_label()\n    return 'rmspe', rmspe(cp.array(y), cp.array(pred))\n\n\nNUM_FOLDS = 5\nparam = {'objective': 'reg:squarederror',\n         'learning_rate': 0.1,\n         'max_depth': 3,\n         \"min_child_weight\": 200,\n         \"reg_alpha\": 10.0,\n         \"tree_method\": 'gpu_hist', \"gpu_id\": 0,\n         'disable_default_eval_metric': 1\n    }\n\ntarget = \"target\"\n\noof_preds = cp.zeros(dev_df.shape[0])\ntest_preds = cp.zeros(past_test_volatility.shape[0])\n\nfor fold in range(NUM_FOLDS):\n    print(\"Fold\", fold)\n    train_ind = cp.where(dev_df[\"time_id\"].values % NUM_FOLDS != fold)[0]\n    val_ind = cp.where(dev_df[\"time_id\"].values % NUM_FOLDS == fold)[0]\n        \n    train_df, val_df = dev_df.iloc[train_ind], dev_df.iloc[val_ind]\n\n    d_train = xgb.DMatrix(train_df[features], train_df[target], weight=1/cp.square(train_df[target]))\n    d_val = xgb.DMatrix(val_df[features], val_df[target], weight=1/cp.square(val_df[target]))\n\n    model = xgb.train(param, d_train, evals=[(d_train, \"train\"), (d_val, \"val\")], \n                      num_boost_round=5000, verbose_eval=50, feval=rmspe_xgb,\n                      early_stopping_rounds=200)\n    \n    oof_preds[val_ind] = model.predict(d_val)\n    test_preds += cp.array(model.predict(xgb.DMatrix(past_test_volatility[features].astype(\"float\")))/NUM_FOLDS)","metadata":{"papermill":{"duration":2.208351,"end_time":"2021-07-08T15:00:55.197509","exception":false,"start_time":"2021-07-08T15:00:52.989158","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-08-27T18:30:32.21865Z","iopub.execute_input":"2021-08-27T18:30:32.219001Z","iopub.status.idle":"2021-08-27T18:31:36.661178Z","shell.execute_reply.started":"2021-08-27T18:30:32.218966Z","shell.execute_reply":"2021-08-27T18:31:36.660334Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dev_df[\"pred\"] = oof_preds\nprint(f'The RMSPE score of XGB is {rmspe(dev_df[\"target\"], dev_df[\"pred\"])}')","metadata":{"papermill":{"duration":0.572203,"end_time":"2021-07-08T15:00:55.816575","exception":false,"start_time":"2021-07-08T15:00:55.244372","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-08-27T18:31:36.662594Z","iopub.execute_input":"2021-08-27T18:31:36.663143Z","iopub.status.idle":"2021-08-27T18:31:37.525235Z","shell.execute_reply.started":"2021-08-27T18:31:36.663104Z","shell.execute_reply":"2021-08-27T18:31:37.524282Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"past_test_volatility[\"row_id\"] = past_test_volatility[\"stock_id\"].astype(str) + \"-\" + past_test_volatility[\"time_id\"].astype(str) \npast_test_volatility[\"target\"] = test_preds.clip(0.0, 100.0)/SCALE","metadata":{"papermill":{"duration":0.074496,"end_time":"2021-07-08T15:00:56.774602","exception":false,"start_time":"2021-07-08T15:00:56.700106","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-08-27T18:31:37.526667Z","iopub.execute_input":"2021-08-27T18:31:37.527197Z","iopub.status.idle":"2021-08-27T18:31:38.034083Z","shell.execute_reply.started":"2021-08-27T18:31:37.527156Z","shell.execute_reply":"2021-08-27T18:31:38.033207Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%cd /kaggle/working","metadata":{"papermill":{"duration":0.055862,"end_time":"2021-07-08T15:00:56.879004","exception":false,"start_time":"2021-07-08T15:00:56.823142","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-08-27T18:31:38.03542Z","iopub.execute_input":"2021-08-27T18:31:38.035995Z","iopub.status.idle":"2021-08-27T18:31:38.042624Z","shell.execute_reply.started":"2021-08-27T18:31:38.035953Z","shell.execute_reply":"2021-08-27T18:31:38.041796Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub_df = load_data(\"test\", path=PATH).merge(past_test_volatility[[\"row_id\", \"target\"]], \n                                            on=\"row_id\", how=\"left\").fillna(0.0)\n\nsub_df.to_csv(\"submission.csv\", index=False, columns=[\"row_id\", \"target\"])","metadata":{"papermill":{"duration":0.056971,"end_time":"2021-07-08T15:00:56.98449","exception":false,"start_time":"2021-07-08T15:00:56.927519","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-08-27T18:31:38.044302Z","iopub.execute_input":"2021-08-27T18:31:38.044976Z","iopub.status.idle":"2021-08-27T18:31:38.069849Z","shell.execute_reply.started":"2021-08-27T18:31:38.044938Z","shell.execute_reply":"2021-08-27T18:31:38.069095Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cudf.read_csv(\"submission.csv\")","metadata":{"papermill":{"duration":0.071088,"end_time":"2021-07-08T15:00:57.103829","exception":false,"start_time":"2021-07-08T15:00:57.032741","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-08-27T18:31:38.071184Z","iopub.execute_input":"2021-08-27T18:31:38.071543Z","iopub.status.idle":"2021-08-27T18:31:38.09131Z","shell.execute_reply.started":"2021-08-27T18:31:38.071508Z","shell.execute_reply":"2021-08-27T18:31:38.090338Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}