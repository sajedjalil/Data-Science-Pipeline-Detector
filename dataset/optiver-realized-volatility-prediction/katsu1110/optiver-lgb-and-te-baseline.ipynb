{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"![](https://upload.wikimedia.org/wikipedia/commons/thumb/9/9b/VIX.png/440px-VIX.png)\n\nThis is yet another starter using lightGBM.\n\n**Don't just fork this notebook and publish**, which just confuses people. In case you were to publish a very similar notebook, please cite this notebook.\n\nAs this notebook was created for new participants to get started easily, I tried to keep it as simple as possible. \n\nThe basic strategy employed in this notebook includes:\n\n- Target Encoding (TE): Simply using the past volatility can predicts the future one very well due to the high autocorrelation nature of the volatility signal.\n- Aggregation by multiple keys: not only stock_id x time_id, but also stock_id and time_id, separately (to let our model know the nature of those keys).\n- Multithreads processing for feature engineering: many features are made on a stock_id x time_id basis, so we might want to accelerate the process.\n- LightGBM with the inverse weight of the square of the target: the evaluation metric is the root mean square percentage error, so we use RMSE as the objective function with the inverse weighting of  (square of) the target.\n\nYou can also find its inference only notebook (for faster submission): [[Optiver] LGB and TE baseline (inference)](https://www.kaggle.com/code1110/optiver-lgb-and-te-baseline-inference)\n\nSo let's get the ball rolling!","metadata":{}},{"cell_type":"code","source":"DEBUG = False\nMODE = 'TRAIN'\n# MODE = 'INFERENCE'\nMODEL_DIR = '../input/optiver-lgb-and-te-baseline'","metadata":{"execution":{"iopub.status.busy":"2021-07-30T13:48:59.640735Z","iopub.execute_input":"2021-07-30T13:48:59.641123Z","iopub.status.idle":"2021-07-30T13:48:59.645507Z","shell.execute_reply.started":"2021-07-30T13:48:59.641086Z","shell.execute_reply":"2021-07-30T13:48:59.644108Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Libraries","metadata":{}},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport gc\nimport pathlib\nfrom tqdm.auto import tqdm\nimport json\nfrom multiprocessing import Pool, cpu_count\nimport time\nimport requests as re\nfrom datetime import datetime\nfrom dateutil.relativedelta import relativedelta, FR\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport glob\nimport os\nfrom sklearn import model_selection\nimport joblib\nimport lightgbm as lgb\n\n# visualize\nimport matplotlib.pyplot as plt\nimport matplotlib.style as style\nfrom matplotlib_venn import venn2, venn3\nimport seaborn as sns\nfrom matplotlib import pyplot\nfrom matplotlib.ticker import ScalarFormatter\nsns.set_context(\"talk\")\nstyle.use('seaborn-colorblind')\n\nimport warnings\nwarnings.simplefilter('ignore')\n\npd.get_option(\"display.max_columns\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-07-30T13:48:59.663656Z","iopub.execute_input":"2021-07-30T13:48:59.664025Z","iopub.status.idle":"2021-07-30T13:48:59.67684Z","shell.execute_reply.started":"2021-07-30T13:48:59.663992Z","shell.execute_reply":"2021-07-30T13:48:59.675629Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Config","metadata":{}},{"cell_type":"code","source":"class CFG:\n    INPUT_DIR = '../input/optiver-realized-volatility-prediction'\n    OUTPUT_DIR = './'\n    N_SPLITS = 10\n    SEED = 46","metadata":{"execution":{"iopub.status.busy":"2021-07-30T13:48:59.679895Z","iopub.execute_input":"2021-07-30T13:48:59.680469Z","iopub.status.idle":"2021-07-30T13:48:59.685455Z","shell.execute_reply.started":"2021-07-30T13:48:59.680425Z","shell.execute_reply":"2021-07-30T13:48:59.684364Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Logging is always nice for your experiment:)\ndef init_logger(log_file='train.log'):\n    from logging import getLogger, INFO, FileHandler,  Formatter,  StreamHandler\n    logger = getLogger(__name__)\n    logger.setLevel(INFO)\n    handler1 = StreamHandler()\n    handler1.setFormatter(Formatter(\"%(message)s\"))\n    handler2 = FileHandler(filename=log_file)\n    handler2.setFormatter(Formatter(\"%(message)s\"))\n    logger.addHandler(handler1)\n    logger.addHandler(handler2)\n    return logger\n\nlogger = init_logger(log_file=f'{CFG.OUTPUT_DIR}/baseline.log')\nlogger.info(f'Start Logging...')","metadata":{"execution":{"iopub.status.busy":"2021-07-30T13:48:59.694162Z","iopub.execute_input":"2021-07-30T13:48:59.694512Z","iopub.status.idle":"2021-07-30T13:48:59.707615Z","shell.execute_reply.started":"2021-07-30T13:48:59.694481Z","shell.execute_reply":"2021-07-30T13:48:59.706697Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load data\n\nData description found in :\n\nhttps://www.kaggle.com/c/optiver-realized-volatility-prediction/data","metadata":{"execution":{"iopub.status.busy":"2021-07-01T00:17:44.201575Z","iopub.execute_input":"2021-07-01T00:17:44.202047Z","iopub.status.idle":"2021-07-01T00:17:44.206279Z","shell.execute_reply.started":"2021-07-01T00:17:44.202004Z","shell.execute_reply":"2021-07-01T00:17:44.205301Z"}}},{"cell_type":"markdown","source":"## Train, test, and sample submission\ntrain.csv The ground truth values for the training set.\n\nstock_id - Same as above, but since this is a csv the column will load as an integer instead of categorical.\ntime_id - Same as above.\ntarget - The realized volatility computed over the 10 minute window following the feature data under the same stock/time_id. There is no overlap between feature and target data. You can find more info in our tutorial notebook.\ntest.csv Provides the mapping between the other data files and the submission file. As with other test files, most of the data is only available to your notebook upon submission with just the first few rows available for download.\n\nstock_id - Same as above.\ntime_id - Same as above.\nrow_id - Unique identifier for the submission row. There is one row for each existing time ID/stock ID pair. Each time window is not necessarily containing every individual stock.\nsample_submission.csv - A sample submission file in the correct format.\n\nrow_id - Same as in test.csv.\ntarget - Same definition as in train.csv. The benchmark is using the median target value from train.csv.","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv(os.path.join(CFG.INPUT_DIR, 'train.csv'))\n\nlogger.info('Train data: {}'.format(train.shape))\ntrain.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-30T13:48:59.725512Z","iopub.execute_input":"2021-07-30T13:48:59.725855Z","iopub.status.idle":"2021-07-30T13:48:59.844612Z","shell.execute_reply.started":"2021-07-30T13:48:59.725819Z","shell.execute_reply":"2021-07-30T13:48:59.843406Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['stock_id'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-07-30T13:48:59.845883Z","iopub.execute_input":"2021-07-30T13:48:59.846172Z","iopub.status.idle":"2021-07-30T13:48:59.857501Z","shell.execute_reply.started":"2021-07-30T13:48:59.846144Z","shell.execute_reply":"2021-07-30T13:48:59.856444Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['time_id'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-07-30T13:48:59.859657Z","iopub.execute_input":"2021-07-30T13:48:59.860095Z","iopub.status.idle":"2021-07-30T13:48:59.874402Z","shell.execute_reply.started":"2021-07-30T13:48:59.860051Z","shell.execute_reply":"2021-07-30T13:48:59.873041Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['target'].hist(bins=100)","metadata":{"execution":{"iopub.status.busy":"2021-07-30T13:48:59.876224Z","iopub.execute_input":"2021-07-30T13:48:59.876787Z","iopub.status.idle":"2021-07-30T13:49:00.231117Z","shell.execute_reply.started":"2021-07-30T13:48:59.876752Z","shell.execute_reply":"2021-07-30T13:49:00.230056Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(16, 7, figsize=(20, 60))\nax = ax.flatten()\n\nfor i, stock_id in tqdm(enumerate(train['stock_id'].unique())):\n    ax[i].hist(train.query('stock_id == @stock_id')['target'], bins=100)\n    ax[i].set_title(stock_id)\nplt.tight_layout()","metadata":{"execution":{"iopub.status.busy":"2021-07-30T13:49:00.232255Z","iopub.execute_input":"2021-07-30T13:49:00.232539Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test = pd.read_csv(os.path.join(CFG.INPUT_DIR, 'test.csv'))\n\nlogger.info('Test data: {}'.format(test.shape))\ntest.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"venn2([\n    set(train['time_id'])\n    , set(test['time_id'])\n])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ss = pd.read_csv(os.path.join(CFG.INPUT_DIR, 'sample_submission.csv'))\n\nlogger.info('Sample submission: {}'.format(ss.shape))\nss.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Book","metadata":{}},{"cell_type":"markdown","source":"book_[train/test].parquet A parquet file partitioned by stock_id. Provides order book data on the most competitive buy and sell orders entered into the market. The top two levels of the book are shared. The first level of the book will be more competitive in price terms, it will then receive execution priority over the second level.\n\nstock_id - ID code for the stock. Not all stock IDs exist in every time bucket. Parquet coerces this column to the categorical data type when loaded; you may wish to convert it to int8.\ntime_id - ID code for the time bucket. Time IDs are not necessarily sequential but are consistent across all stocks.\nseconds_in_bucket - Number of seconds from the start of the bucket, always starting from 0.\nbid_price[1/2] - Normalized prices of the most/second most competitive buy level.\nask_price[1/2] - Normalized prices of the most/second most competitive sell level.\nbid_size[1/2] - The number of shares on the most/second most competitive buy level.\nask_size[1/2] - The number of shares on the most/second most competitive sell level.\n\n## Trade\n\ntrade_[train/test].parquet A parquet file partitioned by stock_id. Contains data on trades that actually executed. Usually, in the market, there are more passive buy/sell intention updates (book updates) than actual trades, therefore one may expect this file to be more sparse than the order book.\n\nstock_id - Same as above.\ntime_id - Same as above.\nseconds_in_bucket - Same as above. Note that since trade and book data are taken from the same time window and trade data is more sparse in general, this field is not necessarily starting from 0.\nprice - The average price of executed transactions happening in one second. Prices have been normalized and the average has been weighted by the number of shares traded in each transaction.\nsize - The sum number of shares traded.\norder_count - The number of unique trade orders taking place.","metadata":{}},{"cell_type":"code","source":"train_book_stocks = os.listdir(os.path.join(CFG.INPUT_DIR, 'book_train.parquet'))\n\nif DEBUG:\n    logger.info('Debug mode: using 3 stocks only')\n    train_book_stocks = train_book_stocks[:3]\n\nlogger.info('{:,} train book stocks: {}'.format(len(train_book_stocks), train_book_stocks))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# load stock_id=0\ndef load_book(stock_id=0, data_type='train'):\n    \"\"\"\n    load parquest book data for given stock_id\n    \"\"\"\n    book_df = pd.read_parquet(os.path.join(CFG.INPUT_DIR, f'book_{data_type}.parquet/stock_id={stock_id}'))\n    book_df['stock_id'] = stock_id\n    book_df['stock_id'] = book_df['stock_id'].astype(np.int8)\n    \n    return book_df\n\ndef load_trade(stock_id=0, data_type='train'):\n    \"\"\"\n    load parquest trade data for given stock_id\n    \"\"\"\n    trade_df = pd.read_parquet(os.path.join(CFG.INPUT_DIR, f'trade_{data_type}.parquet/stock_id={stock_id}'))\n    trade_df['stock_id'] = stock_id\n    trade_df['stock_id'] = trade_df['stock_id'].astype(np.int8)\n    \n    return trade_df\n\nbook0 = load_book(0)\nlogger.info('Book data of stock id = 1: {}'.format(book0.shape))\nbook0.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"book0.tail()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trade0 = load_trade(0)\nlogger.info('Book data of stock id = 1: {}'.format(trade0.shape))\ntrade0.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trade0.tail()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"book_df = book0.merge(\n    trade0\n    , how='outer'\n    , on=['time_id', 'stock_id', 'seconds_in_bucket']\n)\n\nprint(book_df.shape)\nbook_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"book_df.tail()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def fix_jsonerr(df):\n    \"\"\"\n    fix json column error for lightgbm\n    \"\"\"\n    df.columns = [\"\".join (c if c.isalnum() else \"_\" for c in str(x)) for x in df.columns]\n    return df\n\ndef log_return(list_stock_prices):\n    return np.log(list_stock_prices).diff() \n\ndef realized_volatility(series_log_return):\n    series_log_return = log_return(series_log_return)\n    return np.sqrt(np.sum(series_log_return ** 2))\n\ndef fe_row(book):\n    \"\"\"\n    Feature engineering (just volatility for now) for each row\n    \"\"\"\n    \n    # volatility\n    for i in [1, 2, ]:  \n        # wap\n        book[f'book_wap{i}'] = (book[f'bid_price{i}'] * book[f'ask_size{i}'] +\n                        book[f'ask_price{i}'] * book[f'bid_size{i}']) / (\n                               book[f'bid_size{i}']+ book[f'ask_size{i}'])\n        \n    # mean wap\n    book['book_wap_mean'] = (book['book_wap1'] + book['book_wap2']) / 2\n    \n    # wap diff\n    book['book_wap_diff'] = book['book_wap1'] - book['book_wap2']\n    \n    # other orderbook features\n    book['book_price_spread'] = (book['ask_price1'] - book['bid_price1']) / (book['ask_price1'] + book['bid_price1'])\n    book['book_bid_spread'] = book['bid_price1'] - book['bid_price2']\n    book['book_ask_spread'] = book['ask_price1'] - book['ask_price2']\n    book['book_total_volume'] = book['ask_size1'] + book['ask_size2'] + book['bid_size1'] + book['bid_size2']\n    book['book_volume_imbalance'] = (book['ask_size1'] + book['ask_size2']) - (book['bid_size1'] + book['bid_size2'])\n    \n    return book    \n\ndef fe_agg(book_df):\n    \"\"\"\n    feature engineering (aggregation by stock_id x time_id)   \n    \"\"\" \n            \n    # features\n    book_feats = book_df.columns[book_df.columns.str.startswith('book_')].values.tolist()\n    trade_feats = ['price', 'size', 'order_count', 'seconds_in_bucket']\n        \n    # agg trade features\n    trade_df = book_df.groupby(['time_id', 'stock_id'])[trade_feats].agg([\n        'sum', 'mean', 'std', 'max', 'min'\n    ]).reset_index()\n    \n    # agg volatility features\n    fe_df = book_df.groupby(['time_id', 'stock_id'])[book_feats].agg([\n        realized_volatility\n    ]).reset_index()\n    fe_df.columns = [\" \".join(col).strip() for col in fe_df.columns.values]\n    \n    # merge\n    fe_df = fe_df.merge(\n        trade_df\n        , how='left'\n        , on=['time_id', 'stock_id']\n    )\n    \n    return fe_df\n    \ndef fe_all(book_df):\n    \"\"\"\n    perform feature engineerings\n    \"\"\"\n      \n    # row-wise feature engineering\n    book_df = fe_row(book_df)\n    \n    # feature engineering agg by stock_id x time_id \n    fe_df = fe_agg(book_df)\n    \n    return fe_df\n    \ndef book_fe_by_stock(stock_id=0):\n    \"\"\"\n    load orderbook and trade data for the given stock_id and merge\n    \n    \"\"\"\n    # load data\n    book_df = load_book(stock_id, 'train')\n    trade_df = load_trade(stock_id, 'train')\n    book_feats = book_df.columns.values.tolist()\n    \n    # merge\n    book_df = book_df.merge(\n        trade_df\n        , how='outer'\n        , on=['time_id', 'seconds_in_bucket', 'stock_id']\n    )\n    \n    # sort by time\n    book_df = book_df.sort_values(by=['time_id', 'seconds_in_bucket'])\n    \n    # fillna for book_df\n    book_df[book_feats] = book_df[book_feats].fillna(method='ffill')\n    \n    # feature engineering\n    fe_df = fe_all(book_df)\n    return fe_df\n\ndef book_fe_by_stock_test(stock_id=0):\n    \"\"\"\n    same function but for the test\n    \n    \"\"\"\n    # load data\n    book_df = load_book(stock_id, 'test')\n    trade_df = load_trade(stock_id, 'test')\n    book_feats = book_df.columns.values.tolist()\n    \n    # merge\n    book_df = book_df.merge(\n        trade_df\n        , how='outer'\n        , on=['time_id', 'seconds_in_bucket', 'stock_id']\n    )\n    \n    # sort by time\n    book_df = book_df.sort_values(by=['time_id', 'seconds_in_bucket'])\n    \n    # fillna for book_df\n    book_df[book_feats] = book_df[book_feats].fillna(method='ffill')\n    \n    # feature engineering\n    fe_df = fe_all(book_df)\n    return fe_df\n    \ndef book_fe_all(stock_ids, data_type='train'): \n    \"\"\"\n    Feature engineering with multithread processing\n    \"\"\"\n    # feature engineering agg by stock_id x time_id\n    with Pool(cpu_count()) as p:\n        if data_type == 'train':\n            feature_dfs = list(tqdm(p.imap(book_fe_by_stock, stock_ids), total=len(stock_ids)))\n        elif data_type == 'test':\n            feature_dfs = list(tqdm(p.imap(book_fe_by_stock_test, stock_ids), total=len(stock_ids)))      \n        \n    fe_df = pd.concat(feature_dfs)\n    \n    # feature engineering agg by stock_id\n    vol_feats = [f for f in fe_df.columns if ('realized' in f) & ('wap' in f)]\n    if data_type == 'train':\n        # agg\n        stock_df = fe_df.groupby('stock_id')[vol_feats].agg(['mean', 'std', 'max', 'min', ]).reset_index()\n        \n        # fix column names\n        stock_df.columns = ['stock_id'] + [f'{f}_stock' for f in stock_df.columns.values.tolist()[1:]]        \n        stock_df = fix_jsonerr(stock_df)\n    \n    # feature engineering agg by time_id\n    time_df = fe_df.groupby('time_id')[vol_feats].agg(['mean', 'std', 'max', 'min', ]).reset_index()\n    time_df.columns = ['time_id'] + [f'{f}_time' for f in time_df.columns.values.tolist()[1:]]\n    \n    # merge\n    fe_df = fe_df.merge(\n        time_df\n        , how='left'\n        , on='time_id'\n    )\n    \n    # make sure to fix json error for lighgbm\n    fe_df = fix_jsonerr(fe_df)\n    \n    # out\n    if data_type == 'train':\n        return fe_df, stock_df\n    elif data_type == 'test':\n        return fe_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\nif MODE == 'TRAIN':\n    # all book data feature engineering\n    stock_ids = [int(i.split('=')[-1]) for i in train_book_stocks]\n    book_df, stock_df = book_fe_all(stock_ids, data_type='train')\n\n    assert book_df['stock_id'].nunique() > 2\n    assert book_df['time_id'].nunique() > 2\n    \n    # save stock_df for the test\n    stock_df.to_pickle('train_stock_df.pkl')\n    logger.info('train stock df saved!')\n    \n    # merge\n    book_df = book_df.merge(\n        stock_df\n        , how='left'\n        , on='stock_id'\n    ).merge(\n        train\n        , how='left'\n        , on=['stock_id', 'time_id']\n    ).replace([np.inf, -np.inf], np.nan).fillna(method='ffill')\n\n    # make row_id\n    book_df['row_id'] = book_df['stock_id'].astype(str) + '-' + book_df['time_id'].astype(str)\n    book_df.to_pickle('book_df.pkl')\n\n    print(book_df.shape)\n    book_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# test\ntest_book_stocks = os.listdir(os.path.join(CFG.INPUT_DIR, 'book_test.parquet'))\n\nlogger.info('{:,} test book stocks: {}'.format(len(test_book_stocks), test_book_stocks))\n\n# all book data feature engineering\ntest_stock_ids = [int(i.split('=')[-1]) for i in test_book_stocks]\ntest_book_df = book_fe_all(test_stock_ids, data_type='test')\n\n# load stock_df, if inference\nif MODE == 'INFERENCE':\n    book_df = pd.read_pickle(f'{MODEL_DIR}/book_df.pkl')\n    stock_df = pd.read_pickle(f'{MODEL_DIR}/train_stock_df.pkl')\n    \n# merge\ntest_book_df = test.merge(\n    stock_df\n    , how='left'\n    , on='stock_id'\n).merge(\n    test_book_df\n    , how='left'\n    , on=['stock_id', 'time_id']\n).replace([np.inf, -np.inf], np.nan).fillna(method='ffill')\n\n# make row_id\ntest_book_df['row_id'] = test_book_df['stock_id'].astype(str) + '-' + test_book_df['time_id'].astype(str)\n\nprint(test_book_df.shape)\ntest_book_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Modeling\n\nCurrently,\n\n- MODEL: LightGBM (with inverse weighting of the square of the target)\n- OBJECTIVE FUNCTION: RMSE\n- METRIC: RMSPE (used for early stopping)\n- Validation Strategy: GroupKFold using time_id","metadata":{}},{"cell_type":"code","source":"target = 'target'\ndrops = [target, 'row_id', 'time_id']\nfeatures = [f for f in test_book_df.columns.values.tolist() if (f not in drops) & (test_book_df[f].isna().sum() == 0) & (book_df[f].isna().sum() == 0)]\ncats = ['stock_id', ]\n\nlogger.info('{:,} features ({:,} categorical): {}'.format(len(features), len(cats), features))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# evaluation metric\ndef RMSPEMetric(XGBoost=False):\n\n    def RMSPE(yhat, dtrain, XGBoost=XGBoost):\n\n        y = dtrain.get_label()\n        elements = ((y - yhat) / y) ** 2\n        if XGBoost:\n            return 'RMSPE', float(np.sqrt(np.sum(elements) / len(y)))\n        else:\n            return 'RMSPE', float(np.sqrt(np.sum(elements) / len(y))), False\n\n    return RMSPE","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# LightGBM parameters\nparams = {\n    'n_estimators': 10000,\n    'objective': 'rmse',\n    'boosting_type': 'gbdt',\n    'max_depth': -1,\n    'learning_rate': 0.01,\n    'subsample': 0.72,\n    'subsample_freq': 4,\n    'feature_fraction': 0.9,\n    'lambda_l1': 1,\n    'lambda_l2': 1,\n    'seed': 46,\n    'early_stopping_rounds': 100,\n    'verbose': -1\n} ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def fit_model(params, X_train, y_train, X_test, features=features, cats=[], era='stock_id', fold_type='kfold', n_fold=5, seed=42):\n    \"\"\"\n    fit model with cross validation\n    \"\"\"\n    \n    models = []\n    oof_df = X_train[['time_id', 'stock_id', target]].copy()\n    oof_df['pred'] = np.nan\n    y_preds = np.zeros((len(X_test),))\n    \n    if fold_type == 'stratifiedshuffle':\n        cv = model_selection.StratifiedShuffleSplit(n_splits=n_fold, random_state=seed)\n        kf = cv.split(X_train, X_train[era])\n    elif fold_type == 'kfold':\n        cv = model_selection.KFold(n_splits=n_fold, shuffle=True, random_state=seed)\n        kf = cv.split(X_train, y_train)      \n    elif fold_type == 'gkf':\n        cv = model_selection.GroupKFold(n_splits=n_fold)\n        kf = cv.split(X_train, y_train, X_train[era])      \n    \n    fi_df = pd.DataFrame()\n    fi_df['features'] = features\n    fi_df['importance'] = 0\n        \n    for fold_id, (train_index, valid_index) in tqdm(enumerate(kf)):\n        # split\n        X_tr = X_train.loc[train_index, features]\n        X_val = X_train.loc[valid_index, features]\n        y_tr = y_train.loc[train_index]\n        y_val = y_train.loc[valid_index]\n        \n        # model (note inverse weighting)\n        train_set = lgb.Dataset(X_tr, y_tr, categorical_feature=cats, weight=1/np.power(y_tr, 2))\n        val_set = lgb.Dataset(X_val, y_val, categorical_feature=cats, weight=1/np.power(y_val, 2))\n        model = lgb.train(\n            params\n            , train_set\n            , valid_sets=[train_set, val_set]\n            , feval=RMSPEMetric()\n            , verbose_eval=250\n        )\n        \n        # feature importance\n        fi_df[f'importance_fold{fold_id}'] = model.feature_importance(importance_type=\"gain\")\n        fi_df['importance'] += fi_df[f'importance_fold{fold_id}'].values\n        \n        # save model\n        joblib.dump(model, f'model_fold{fold_id}.pkl')\n        logger.debug('model saved!')\n\n        # predict\n        oof_df['pred'].iloc[valid_index] = model.predict(X_val)\n        y_pred = model.predict(X_test[features])\n        y_preds += y_pred / n_fold\n        models.append(model)\n        \n    return oof_df, y_preds, models, fi_df\n\nif MODE == 'TRAIN':\n    oof_df, y_preds, models, fi_df = fit_model(params, \n                                          book_df, \n                                          book_df[target], \n                                          test_book_df, \n                                          features=features, \n                                          cats=cats,\n                                          era='time_id',\n                                          fold_type='gkf', \n                                          n_fold=CFG.N_SPLITS, \n                                          seed=CFG.SEED\n                                              )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# CV (Cross-Validation) Score\nHow good is my model?","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import r2_score\ndef rmspe(y_true, y_pred):\n    return  (np.sqrt(np.mean(np.square((y_true - y_pred) / y_true))))\n\nif MODE == 'TRAIN':\n    oof_df.dropna(inplace=True)\n    y_true = oof_df[target].values\n    y_pred = oof_df['pred'].values\n    \n    oof_df[target].hist(bins=100)\n    oof_df['pred'].hist(bins=100)\n    \n    R2 = round(r2_score(y_true, y_pred), 3)\n    RMSPE = round(rmspe(y_true, y_pred), 3)\n    logger.info(f'Performance of the naive prediction: R2 score: {R2}, RMSPE: {RMSPE}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We might want to take a look at by-stock performance: there may be some stocks whose volatility are difficult to predict.","metadata":{}},{"cell_type":"code","source":"# performance by stock_id\nif MODE == 'TRAIN':\n    score_df = pd.DataFrame()\n    score_df['stock_id'] = oof_df['stock_id'].unique()\n    score_df['R2'] = np.nan\n    score_df['RMSPE'] = np.nan\n    for stock_id in oof_df['stock_id'].unique():\n        y_true = oof_df.query('stock_id == @stock_id')[target].values\n        y_pred = oof_df.query('stock_id == @stock_id')['pred'].values\n        \n        R2 = round(r2_score(y_true, y_pred), 3)\n        RMSPE = round(rmspe(y_true, y_pred), 3)\n        logger.info(f'Performance by stock_id={stock_id}: R2 score: {R2}, RMSPE: {RMSPE}')\n        \n        score_df.loc[score_df['stock_id'] == stock_id, 'R2'] = R2\n        score_df.loc[score_df['stock_id'] == stock_id, 'RMSPE'] = RMSPE\n    \n    # save score dataframe for further analysis\n    score_df.to_csv('score_df.csv', index=False)\n    logger.info('Scores saved!')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Feature Importance\nLet's see top features for the prediction.","metadata":{}},{"cell_type":"code","source":"if MODE == 'TRAIN':\n    fi_df = fi_df.sort_values(by='importance', ascending=False)\n    fi_df.to_csv('feature_importance.csv', index=False)\n    \n    fig, ax = plt.subplots(1, 1, figsize=(10, 40))\n    sns.barplot(x='importance', y='features', data=fi_df.iloc[:30], ax=ax)\n    logger.info(fi_df[['features', 'importance']].iloc[:50].to_markdown())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Submit","metadata":{}},{"cell_type":"code","source":"if MODE == 'INFERENCE':\n    \"\"\"\n    used for inference kernel only\n    \"\"\"\n    y_preds = np.zeros(len(test_book_df))\n    files = glob.glob(f'{MODEL_DIR}/*model*.pkl')\n    assert len(files) > 0\n    for i, f in enumerate(files):\n        model = joblib.load(f)\n        y_preds += model.predict(test_book_df[features])\n    y_preds /= (i+1)\n    \ntest_book_df[target] = y_preds","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# test\ntest_book_df[target] = y_preds\n\n# save the submit file\nsub = test_book_df[['row_id', target]]\nsub.to_csv('submission.csv',index = False)\n\nlogger.info('submitted!')\nsub.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}