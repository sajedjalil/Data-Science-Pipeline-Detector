{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport glob\nimport time\nfrom contextlib import contextmanager\nfrom joblib import Parallel, delayed\nfrom sklearn.model_selection import GroupKFold\nimport lightgbm as lgb\nimport matplotlib.pyplot as plt\nimport seaborn as sns","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-09-14T05:18:09.093126Z","iopub.execute_input":"2021-09-14T05:18:09.093491Z","iopub.status.idle":"2021-09-14T05:18:11.665933Z","shell.execute_reply.started":"2021-09-14T05:18:09.093461Z","shell.execute_reply":"2021-09-14T05:18:11.664847Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.options.mode.chained_assignment = None","metadata":{"execution":{"iopub.status.busy":"2021-09-14T05:18:14.14913Z","iopub.execute_input":"2021-09-14T05:18:14.149544Z","iopub.status.idle":"2021-09-14T05:18:14.154472Z","shell.execute_reply.started":"2021-09-14T05:18:14.149499Z","shell.execute_reply":"2021-09-14T05:18:14.153337Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"path = '/kaggle/input/optiver-realized-volatility-prediction/'\nlist_order_book_file_train = glob.glob('/kaggle/input/optiver-comp/ffill/ffill/book_train.parquet/*')\n\ntrain = pd.read_csv(path + 'train.csv')\ntrain['row_id'] = train['stock_id'].astype(str) + '-' + train['time_id'].astype(str)","metadata":{"execution":{"iopub.status.busy":"2021-09-14T05:18:16.451844Z","iopub.execute_input":"2021-09-14T05:18:16.452273Z","iopub.status.idle":"2021-09-14T05:18:17.981094Z","shell.execute_reply.started":"2021-09-14T05:18:16.452229Z","shell.execute_reply":"2021-09-14T05:18:17.980315Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"@contextmanager\ndef timer(name):\n    \"\"\"\n    Time Each Process\n    \"\"\"\n    t0 = time.time()\n    yield\n    print('\\n[{}] done in {} Minutes\\n'.format(name, round((time.time() - t0) / 60, 2)))\n\n\ndef log_return(list_stock_prices):\n    return np.log(list_stock_prices).diff()\n\n\ndef order_updates(order_info):\n    return order_info.diff()\n\n\ndef order_shift(order_info):\n    return order_info.shift(1)\n\n\ndef abs_sum_diff(order_info):\n    return np.sum(np.abs(order_info.diff()))\n\n\ndef realized_volatility(series_log_return):\n    return np.sqrt(np.sum(series_log_return ** 2))\n\n\ndef rmspe(y_true, y_pred):\n    return np.sqrt(np.mean(np.square((y_true - y_pred) / y_true)))\n\n\ndef ffill(data_df):\n    data_df = data_df.set_index(['time_id', 'seconds_in_bucket'])\n    data_df = data_df.reindex(pd.MultiIndex.from_product([data_df.index.levels[0], np.arange(0, 600)],\n                                                         names=['time_id', 'seconds_in_bucket']), method='ffill')\n    return data_df.reset_index()\n\n\ndef rebase(df_book, df_trade):\n    for time_id in df_book['time_id'].unique():\n        sub_df_book = df_book[df_book['time_id'] == time_id]\n        rebase_diff = min(sub_df_book['seconds_in_bucket'])\n        if rebase_diff != 0:\n            df_book.loc[df_book['time_id'] == time_id, 'seconds_in_bucket'] = sub_df_book['seconds_in_bucket'] - rebase_diff\n            sub_df_trade = df_trade[df_trade['time_id'] == time_id]\n            df_trade.loc[df_trade['time_id'] == time_id, 'seconds_in_bucket'] = sub_df_trade['seconds_in_bucket'] - rebase_diff\n    \n    return df_book, df_trade\n\n\ndef calc_tick(df):\n    diff = abs(df.diff())\n    min_diff = np.nanmin(diff.where(lambda x: x > 0))\n    tick_size = np.nanmean(min_diff)\n    return tick_size","metadata":{"execution":{"iopub.status.busy":"2021-09-14T05:18:17.98281Z","iopub.execute_input":"2021-09-14T05:18:17.983542Z","iopub.status.idle":"2021-09-14T05:18:17.999283Z","shell.execute_reply.started":"2021-09-14T05:18:17.983486Z","shell.execute_reply":"2021-09-14T05:18:17.998447Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def feature_engineering(file_path_order_book, file_path_trade, train_test=True):\n    # read two tables\n    df_book_data = pd.read_parquet(file_path_order_book)\n    df_trade_data = pd.read_parquet(file_path_trade)\n    stock_id = file_path_order_book.split('=')[1]\n    \n    if train_test == False:\n        df_book_data, df_trade_data = rebase(df_book_data, df_trade_data)\n        df_book_data = ffill(df_book_data)\n\n    # ------------------------- BOOK-ONLY FEATURES --------------------------\n    # return_square, ba_size_direction\n    # tick size (questionable calculation)\n    # tick_size = df_book_data[['time_id', 'ask_price1', 'ask_price2', 'bid_price1', 'bid_price2']].groupby('time_id').apply(calc_tick)\n\n    # calculate weighted average price\n    df_book_data['wap'] = (df_book_data['bid_price1'] * df_book_data['ask_size1'] + df_book_data['ask_price1'] *\n                           df_book_data['bid_size1']) / (df_book_data['bid_size1'] + df_book_data['ask_size1'])\n    df_book_data['log_return'] = df_book_data.groupby(['time_id'])['wap'].apply(log_return)\n    df_book_data['log_return_sq'] = np.square(df_book_data['log_return'])\n\n    df_book_data['wap2'] = (df_book_data['bid_price2'] * df_book_data['ask_size2'] + df_book_data['ask_price2'] *\n                            df_book_data['bid_size2']) / (df_book_data['bid_size2'] + df_book_data['ask_size2'])\n    df_book_data['log_return2'] = df_book_data.groupby(['time_id'])['wap2'].apply(log_return)\n    df_book_data['log_return2_sq'] = np.square(df_book_data['log_return2'])\n\n    # bid price update\n    df_book_data['bp_update'] = df_book_data.groupby(['time_id'])['bid_price1'].apply(order_updates)\n    # ask price update\n    df_book_data['ap_update'] = df_book_data.groupby(['time_id'])['ask_price1'].apply(order_updates)\n    # bid size update\n    df_book_data['bs_update'] = df_book_data.groupby(['time_id'])['bid_size1'].apply(order_updates)\n    df_book_data['bid_size1_lag'] = df_book_data.groupby(['time_id'])['bid_size1'].apply(order_shift)\n    df_book_data['bs_update_relative'] = df_book_data['bs_update'] / df_book_data['bid_size1_lag']\n    # ask size update\n    df_book_data['as_update'] = df_book_data.groupby(['time_id'])['ask_size1'].apply(order_updates)\n    df_book_data['ask_size1_lag'] = df_book_data.groupby(['time_id'])['ask_size1'].apply(order_shift)\n    df_book_data['as_update_relative'] = df_book_data['as_update'] / df_book_data['ask_size1_lag']\n    # small orders\n    df_book_data['small_orders'] = ((df_book_data['ap_update'] >= 0) & (df_book_data['as_update_relative'] <= -0.9)) | \\\n                                   ((df_book_data['bp_update'] <= 0) & (df_book_data['bs_update_relative'] <= -0.9))\n    \n    # total order size\n    \n    df_book_data['total_order'] = df_book_data['ask_size1'] + df_book_data['bid_size1']\n    df_book_data['total_bid_order'] = df_book_data['bid_size1'] + df_book_data['bid_size2']\n    df_book_data['total_ask_order'] = df_book_data['ask_size1'] + df_book_data['ask_size2']\n\n    # volume_order_imbalance\n    df_book_data['bid_increment'] = np.where(df_book_data['bp_update'] > 0, df_book_data['bid_size1'],\n                                             np.where(df_book_data['bp_update'] < 0, 0, df_book_data['bs_update']))\n    df_book_data['ask_increment'] = np.where(df_book_data['ap_update'] < 0, df_book_data['ask_size1'],\n                                             np.where(df_book_data['ap_update'] > 0, 0, df_book_data['as_update']))\n    df_book_data['VOI'] = df_book_data['bid_increment'] - df_book_data['ask_increment']\n\n    # ba spread\n    df_book_data['ba_spread'] = 2 * (df_book_data['ask_price1'] - df_book_data['bid_price1']) / \\\n                                (df_book_data['ask_price1'] + df_book_data['bid_price1'])\n    # absolute ba spread by tick size\n    df_book_data['abs_ba_spread'] = df_book_data['ask_price1'] - df_book_data['bid_price1']\n    \n    # bid spread\n    df_book_data['bid_spread'] = (df_book_data['bid_price1'] - df_book_data['bid_price2']) / \\\n                                 (df_book_data['ask_price1'] + df_book_data['bid_price1'])\n    df_book_data['bid_spread_vs_ba'] = df_book_data['bid_spread'] / df_book_data['ba_spread']\n    \n    # ask spread\n    df_book_data['ask_spread'] = (df_book_data['ask_price2'] - df_book_data['ask_price1']) / \\\n                                 (df_book_data['ask_price1'] + df_book_data['bid_price1'])\n    df_book_data['ask_spread_vs_ba'] = df_book_data['ask_spread'] / df_book_data['ba_spread']\n\n    # Dispersion\n    df_book_data['dispersion_bid'] = df_book_data['bid_spread'] * df_book_data['bid_size2']\n    df_book_data['dispersion_ask'] = df_book_data['ask_spread'] * df_book_data['ask_size2']\n    \n    # order book imbalance (should be measured in other ways)\n    df_book_data['volume_imbalance'] = abs((df_book_data['bid_size1'] + df_book_data['bid_size2']) \\\n                                           - (df_book_data['ask_size1'] + df_book_data['ask_size2']))\n    \n    df_book_data['total_order_per_ba_spread'] = df_book_data['total_order'] / df_book_data['ba_spread']\n    \n    # move in b/a price versus ba spread\n    df_book_data['bid_price_chg_vs_ba'] = np.abs(df_book_data['bp_update']) / df_book_data['abs_ba_spread']\n    df_book_data['ask_price_chg_vs_ba'] = np.abs(df_book_data['ap_update']) / df_book_data['abs_ba_spread']\n    \n    # number of quote updates\n    df_book_data['best_quote'] = list(zip(df_book_data['ask_price1'], df_book_data['ask_size1'],\n                                          df_book_data['bid_price1'], df_book_data['bid_size1']))\n    df_book_data['best_price'] = list(zip(df_book_data['ask_price1'], df_book_data['bid_price1']))\n\n    # calculate update interval & count order updates\n    shift_ = df_book_data['best_quote'] == (df_book_data.groupby('time_id')['best_quote'].apply(order_shift))\n    df_book_data['order_update'] = (~shift_)\n    shift_ = df_book_data['best_price'] == (df_book_data.groupby('time_id')['best_price'].apply(order_shift))\n    df_book_data['order_price_update'] = (~shift_)\n    \n    # time weights\n    df_book_data['time_weights'] = 0.995 ** (600 - df_book_data['seconds_in_bucket'])\n    df_book_data['time_weighted_return'] = df_book_data['log_return'] * df_book_data['time_weights']\n    df_book_data['time_weighted_sqr_return'] = df_book_data['log_return_sq'] * df_book_data['time_weights']\n    df_book_data['time_weighted_sqr_return2'] = df_book_data['log_return2_sq'] * df_book_data['time_weights']\n    df_book_data['time_weighted_ba_spread'] = df_book_data['ba_spread'] * df_book_data['time_weights']\n\n    # split a time bucket to 4 splits\n    df_book_data['time_split'] = 1\n    df_book_data.loc[(df_book_data['seconds_in_bucket'] >= 150) & (df_book_data['seconds_in_bucket'] < 300), 'time_split'] = 2\n    df_book_data.loc[(df_book_data['seconds_in_bucket'] >= 300) & (df_book_data['seconds_in_bucket'] < 450), 'time_split'] = 3\n    df_book_data.loc[(df_book_data['seconds_in_bucket'] >= 450) & (df_book_data['seconds_in_bucket'] < 600), 'time_split'] = 4\n    time_split_stats = df_book_data.groupby(['time_id', 'time_split']).agg(time_split_rv=('log_return', realized_volatility),\n                                                                           time_split_ba_spread_min=('ba_spread', 'min'),\n                                                                           time_split_ba_spread_max=('ba_spread', 'max'),\n                                                                           time_split_ba_spread_mean=('ba_spread', 'mean'),\n                                                                           ).reset_index()\n\n    feature_cols = ['time_id', 'time_split_rv',\n                    'time_split_ba_spread_min', 'time_split_ba_spread_max', 'time_split_ba_spread_mean']\n\n    first_time_split = time_split_stats.loc[time_split_stats['time_split'] == 1, feature_cols].set_index('time_id')\n    # first_time_split = first_time_split.add_suffix('_first')\n\n    second_time_split = time_split_stats.loc[time_split_stats['time_split'] == 2, feature_cols].set_index('time_id')\n    # second_time_split = second_time_split.add_suffix('_second')\n\n    third_time_split = time_split_stats.loc[time_split_stats['time_split'] == 3, feature_cols].set_index('time_id')\n    # third_time_split = third_time_split.add_suffix('_third')\n\n    last_time_split = time_split_stats.loc[time_split_stats['time_split'] == 4, feature_cols].set_index('time_id')\n    # last_time_split = last_time_split.add_suffix('_last')\n\n    last_time_split['time_split_ba_is_max'] = last_time_split['time_split_ba_spread_max'] / np.max([first_time_split['time_split_ba_spread_max'],\n                                                                                                   second_time_split['time_split_ba_spread_max'],\n                                                                                                   third_time_split['time_split_ba_spread_max']], axis=0)\n    first_time_split['time_split_ba_is_max_first'] = first_time_split['time_split_ba_spread_max'] / np.max([last_time_split['time_split_ba_spread_max'],\n                                                                                                            second_time_split['time_split_ba_spread_max'],\n                                                                                                            third_time_split['time_split_ba_spread_max']], axis=0)\n    uni_cols = ['time_split_rv']\n    last_time_split = last_time_split.add_suffix('_last')\n    time_split_feature_cols = ['time_split_rv_last', 'time_split_ba_spread_min_last', 'time_split_ba_spread_max_last', 'time_split_ba_spread_mean_last',\n                               'time_split_ba_is_max_last']\n\n    \n    # group by time id\n    # df_book_data = df_book_data[~df_book_data['log_return'].isnull()]\n    df_order_features = df_book_data.groupby(['time_id']).agg(ba_spread_mean=('ba_spread', 'mean'), ba_spread_min=('ba_spread', 'min'),\n                                                              ba_spread_std=('ba_spread', 'std'),  ba_spread_max=('ba_spread', 'max'),\n                                                              # abs_ba_spread_std=('abs_ba_spread', 'std'),\n                                                              total_order_per_ba_spread_mean=('total_order_per_ba_spread', 'mean'),\n                                                              total_order_per_ba_spread_min=('total_order_per_ba_spread', 'min'),\n                                                              num_small_orders=('small_orders', 'sum'),\n                                                              ba_spread_abs_change=('ba_spread', abs_sum_diff),\n                                                              # as_update_vol=('as_update_relative', realized_volatility),\n                                                              # bs_update_vol=('bs_update_relative', realized_volatility),\n                                                              # wap_std=('wap', 'std'),\n                                                              log_return_sq_sum=('log_return_sq', 'sum'),\n                                                              # bid_price_chg_vs_ba_mean=('bid_price_chg_vs_ba', 'mean'),\n                                                              # ask_price_chg_vs_ba_mean=('ask_price_chg_vs_ba', 'mean'),\n                                                              log_return_sq_large10_sum=('log_return_sq', lambda x: x.nlargest(10).sum()),\n                                                              weights_sum=('time_weights', 'sum'),\n                                                              weighted_log_return_mean=('time_weighted_return', 'sum'),  # weighted_log_return_max=('time_weighted_return', 'max'),\n                                                              weighted_ba_spread_mean=('time_weighted_ba_spread', 'sum'),  # weighted_ba_spread_max=('time_weighted_ba_spread', 'max'),\n                                                              # weighted_ba_spread_std=('time_weighted_ba_spread', 'std'),\n                                                              # weighted_ba_spread_min=('time_weighted_ba_spread', 'min'),\n                                                              bid_spread_vs_ba_mean=('bid_spread_vs_ba', 'mean'),\n                                                              ask_spread_vs_ba_mean=('ask_spread_vs_ba', 'mean'),\n                                                              time_weighted_rv=('time_weighted_sqr_return', 'sum'),\n                                                              time_weighted_rv2=('time_weighted_sqr_return2', 'sum'),\n                                                              log_return_max=('log_return', 'max'), log_return_min=('log_return', 'min'),\n                                                              # log_return_std=('log_return', 'std'),\n                                                              log_return_sum=('log_return', 'sum'),\n                                                              realized_vol=('log_return', realized_volatility),\n                                                              realized_vol_l2=('log_return2', realized_volatility),\n                                                              num_quote_updates=('order_update', 'sum'),\n                                                              num_price_updates=('order_price_update', 'sum'),\n                                                              bid_order_max=('total_bid_order', 'max'),\n                                                              total_bid_order1=('bid_size1', 'sum'),\n                                                              total_bid_order2=('bid_size2', 'sum'),\n                                                              total_ask_order1=('ask_size1', 'sum'),\n                                                              total_ask_order2=('ask_size2', 'sum'),\n                                                              bid_dispersion_calc=('dispersion_bid', 'sum'),\n                                                              ask_dispersion_calc=('dispersion_ask', 'sum'),\n                                                              # bid_spread_min=('bid_spread', 'min'),  # bid_spread_max=('bid_spread', 'max'),\n                                                              # ask_spread_min=('ask_spread', 'min'),  # ask_spread_max=('ask_spread', 'max'),\n                                                              VOI_min=('VOI', 'min'), VOI_max=('VOI', 'max'),\n                                                              )  # .reset_index()\n    df_order_features['weighted_log_return_mean'] = df_order_features['weighted_log_return_mean'] / df_order_features['weights_sum']\n    df_order_features['weighted_ba_spread_mean'] = df_order_features['weighted_ba_spread_mean'] / df_order_features['weights_sum']\n    df_order_features['rv_spikes'] = df_order_features['log_return_sq_large10_sum'] / df_order_features['log_return_sq_sum']\n    df_order_features['LDispersion'] = (df_order_features['bid_dispersion_calc'] / df_order_features['total_bid_order2'] +\n                                        df_order_features['ask_dispersion_calc'] / df_order_features['total_ask_order2']) / 2\n    \n    for data, suffix in ((first_time_split, '_first'), (second_time_split, '_second'), (third_time_split, '_third')):\n        add_suffix = [col + suffix for col in uni_cols]\n        df_order_features[add_suffix] = data[uni_cols]\n    df_order_features['time_split_ba_is_max_first'] = first_time_split['time_split_ba_is_max_first']\n    df_order_features[time_split_feature_cols] = last_time_split[time_split_feature_cols]\n    df_order_features['first_time_split_rv_impact'] = (df_order_features['time_split_rv_first'] / df_order_features['realized_vol']) ** 2\n    df_order_features['second_time_split_rv_impact'] = (df_order_features['time_split_rv_second'] / df_order_features['realized_vol']) ** 2\n    df_order_features['third_time_split_rv_impact'] = (df_order_features['time_split_rv_third'] / df_order_features['realized_vol']) ** 2\n    df_order_features['last_time_split_rv_impact'] = (df_order_features['time_split_rv_last'] / df_order_features['realized_vol']) ** 2\n    # df_order_features['rv_slope_relative'] = (df_order_features['last_time_split_rv_impact'] - df_order_features['first_time_split_rv_impact']) / df_order_features['first_time_split_rv_impact']\n\n    # add row add stock id + time id\n    df_order_features.reset_index(inplace=True)\n    df_order_features['row_id'] = df_order_features['time_id'].apply(lambda x: f'{stock_id}-{x}')\n\n    # ------------------------- TRADE-ONLY FEATURES --------------------------\n    # trade log return\n    df_trade_data['trade_return'] = df_trade_data.groupby('time_id')['price'].apply(log_return)\n    # df_trade_data['trade_batch_wait_time'] = df_trade_data.groupby('time_id')['seconds_in_bucket'].apply(order_updates)\n    # cancel order\n    df_trade_data = df_book_data.merge(df_trade_data, on=['time_id', 'seconds_in_bucket'], how='left')\n    df_trade_data['trade_perc_bid'] = df_trade_data['size'] / df_trade_data['bid_size1_lag']\n    df_trade_data['trade_perc_ask'] = df_trade_data['size'] / df_trade_data['ask_size1_lag']\n    \n    trade_shift_ = df_trade_data.groupby(['time_id'])[['ask_price1', 'ask_size1', 'ask_price2', 'ask_size2',\n                                                       'bid_price1', 'bid_size1', 'bid_price2', 'bid_size2']].apply(order_shift)\n    condition1 = ((df_trade_data['ask_price2'] == trade_shift_['ask_price2']) & (df_trade_data['ask_size2'] < trade_shift_['ask_size2'])) & (\n                (df_trade_data['ask_price1'] == trade_shift_['ask_price1']) & (df_trade_data['ask_size1'] == trade_shift_['ask_size2']))\n    size1 = trade_shift_['ask_size2'] - df_trade_data['ask_size2']\n    condition2 = (df_trade_data['ask_price2'] > trade_shift_['ask_price2']) & ((df_trade_data['ask_price1'] == trade_shift_['ask_price1']) & (df_trade_data['ask_size1'] == trade_shift_['ask_size2']))\n    size2 = trade_shift_['ask_size2']\n    condition3 = ((df_trade_data['bid_price2'] == trade_shift_['bid_price2']) & (df_trade_data['bid_size2'] < trade_shift_['bid_size2'])) & (\n                (df_trade_data['bid_price1'] == trade_shift_['bid_price1']) & (df_trade_data['bid_size1'] == trade_shift_['bid_size2']))\n    size3 = trade_shift_['bid_size2'] - df_trade_data['bid_size2']\n    condition4 = (df_trade_data['bid_price2'] < trade_shift_['bid_price2']) & ((df_trade_data['bid_price1'] == trade_shift_['bid_price1']) & (df_trade_data['bid_size1'] == trade_shift_['bid_size2']))\n    size4 = trade_shift_['bid_size2']\n    condition5 = (df_trade_data['price'].isna()) & (df_trade_data['ask_price1'] > trade_shift_['ask_price1'])\n    size5 = trade_shift_['ask_size1']\n    condition6 = (df_trade_data['price'].isna()) & ((df_trade_data['ask_price1'] == trade_shift_['ask_price1']) & (df_trade_data['ask_size1'] < trade_shift_['ask_size1']))\n    size6 = trade_shift_['ask_size1'] - df_trade_data['ask_size1']\n    condition7 = (df_trade_data['price'].isna()) & (df_trade_data['bid_price1'] < trade_shift_['bid_price1'])\n    size7 = trade_shift_['bid_size1']\n    condition8 = (df_trade_data['price'].isna()) & ((df_trade_data['bid_price1'] == trade_shift_['bid_price1']) & (df_trade_data['bid_size1'] < trade_shift_['bid_size1']))\n    size8 = trade_shift_['bid_size1'] - df_trade_data['bid_size1']\n    df_trade_data['cancel_order'] = condition1 | condition2 | condition3 | condition4 | condition5 | condition6 | condition7 | condition8\n    df_trade_data['cancel_order_size'] = condition1 * size1 + condition2 * size2 + condition3 * size3 + \\\n                                         condition4 * size4 + condition5 * size5 + condition6 * size6 + \\\n                                         condition7 * size7 + condition8 * size8\n    \n    # sensitivity\n#     avg_size = df_trade_data['size'].mean()\n#     df_trade_data['ask_size_new'] = np.where(df_trade_data['ask_size1'] > avg_size, df_trade_data['ask_size1'] - avg_size,\n#                                              np.where(df_trade_data['ask_size2'] > (avg_size - df_trade_data['ask_size1']),\n#                                                       df_trade_data['ask_size1'] + df_trade_data['ask_size2'] - avg_size,\n#                                                       np.nan))\n#     df_trade_data['bid_size_new'] = np.where(df_trade_data['bid_size1'] > avg_size, df_trade_data['bid_size1'] - avg_size,\n#                                              np.where(df_trade_data['bid_size2'] > (avg_size - df_trade_data['bid_size1']),\n#                                                       df_trade_data['bid_size1'] + df_trade_data['bid_size2'] - avg_size,\n#                                                       np.nan))\n#     df_trade_data['ask_price_new'] = np.where(df_trade_data['ask_size1'] > avg_size, df_trade_data['ask_price1'],\n#                                               np.where(df_trade_data['ask_size2'] > (avg_size - df_trade_data['ask_size1']),\n#                                                        df_trade_data['ask_price2'], np.nan))\n#     df_trade_data['bid_price_new'] = np.where(df_trade_data['bid_size1'] > avg_size, df_trade_data['bid_price1'],\n#                                               np.where(df_trade_data['bid_size2'] > (avg_size - df_trade_data['bid_size1']),\n#                                                        df_trade_data['bid_price2'], np.nan))\n#     df_trade_data['wap_bid'] = (df_trade_data['bid_price_new'] * df_trade_data['ask_size1'] + df_trade_data['ask_price1'] *\n#                                df_trade_data['bid_size_new']) / (df_trade_data['bid_size_new'] + df_trade_data['ask_size1'])\n#     df_trade_data['wap_ask'] = (df_trade_data['bid_price1'] * df_trade_data['ask_size_new'] + df_trade_data['ask_price_new'] *\n#                                df_trade_data['bid_size1']) / (df_trade_data['bid_size1'] + df_trade_data['ask_size_new'])\n#     df_trade_data['wap_bid_change'] = df_trade_data['wap_bid'] - df_trade_data['wap']\n#     df_trade_data['wap_ask_change'] = df_trade_data['wap_ask'] - df_trade_data['wap']\n#     df_trade_data['wap_imbalance'] = df_trade_data['wap_ask'] - df_trade_data['wap_bid']\n    \n    # size, order_count, size_per_order\n    df_trade_features = df_trade_data.groupby(['time_id']).agg(size=('size', 'sum'),\n                                                               order_batches=('order_count', 'count'),\n                                                               order_count=('order_count', 'sum'),\n                                                               trade_rv=('trade_return', realized_volatility),\n                                                               # trade_return_mean=('trade_return', 'mean'),\n                                                               trade_return_std=('trade_return', 'std'),\n                                                               # trade_return_min=('trade_return', 'min'),\n                                                               # trade_outside_ba=('trade_outside_ba', 'sum')\n                                                               cancel_order=('cancel_order', 'sum'),\n                                                               cancel_order_size=('cancel_order_size', 'sum'),\n                                                               # trade_batch_wait_time_mean=('trade_batch_wait_time', 'mean'),\n                                                               # trade_perc_bid_median=('trade_perc_bid', 'median'),\n                                                               # trade_perc_ask_median=('trade_perc_ask', 'median'),\n                                                               trade_perc_ask_20=('trade_perc_ask', lambda x: x.quantile(0.2)),\n                                                               # trade_perc_bid_20=('trade_perc_bid', lambda x: x.quantile(0.2)),\n                                                               ).reset_index()\n\n    df_trade_features['size_per_order'] = df_trade_features['size'] / df_trade_features['order_count']\n    # df_trade_features['avg_trade_size'] = df_trade_features['size'] / df_trade_features['order_batches']\n\n    # add row add stock id + time id\n    df_trade_features['row_id'] = df_trade_features['time_id'].apply(lambda x: f'{stock_id}-{x}')\n    \n    # ------------------------- MERGING --------------------------\n    df = df_order_features.merge(df_trade_features, on=['row_id'], how='left')\n    df['quote_updated_ex_cancel'] = df['num_quote_updates'] - df['cancel_order']\n    return df","metadata":{"execution":{"iopub.status.busy":"2021-09-06T15:06:20.427424Z","iopub.execute_input":"2021-09-06T15:06:20.428003Z","iopub.status.idle":"2021-09-06T15:06:20.499753Z","shell.execute_reply.started":"2021-09-06T15:06:20.427956Z","shell.execute_reply":"2021-09-06T15:06:20.498694Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def joblib_func(book_file, train_test=True):\n    stock_folder = book_file.split('/')[-1]\n    if train_test:\n        trade_file = path + 'trade_train.parquet/' + stock_folder\n    else:\n        trade_file = path + 'trade_test.parquet/' + stock_folder\n    df = feature_engineering(book_file, trade_file, train_test)\n    return df\n\n\n# function: get the aggregated dataframe\ndef paralle_fe(book_list_file, train_test=True):\n    df = Parallel(n_jobs=-1, verbose=0)(delayed(joblib_func)(book_file, train_test) for book_file in book_list_file)\n    df_all_stock = pd.concat(df, ignore_index=True)\n    return df_all_stock","metadata":{"execution":{"iopub.status.busy":"2021-09-06T15:06:20.501324Z","iopub.execute_input":"2021-09-06T15:06:20.501896Z","iopub.status.idle":"2021-09-06T15:06:20.518632Z","shell.execute_reply.started":"2021-09-06T15:06:20.501849Z","shell.execute_reply":"2021-09-06T15:06:20.517398Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def fe_group(df):\n    # log transform of rv\n    df['log_realized_vol'] = np.log(df['realized_vol'])\n    df['log_tw_realized_vol'] = np.log(df['time_weighted_rv'])\n    df['log_tw_realized_vol2'] = np.log(df['time_weighted_rv2'])\n    \n    # order count\n    df['market_order_count_median'] = df.groupby('time_id')['order_count'].transform('median')\n    df['market_order_count_20'] = df.groupby('time_id')['order_count'].transform(lambda x: x.quantile(0.2))\n    df['market_order_count_10'] = df.groupby('time_id')['order_count'].transform(lambda x: x.quantile(0.1))\n    df['market_order_count_80'] = df.groupby('time_id')['order_count'].transform(lambda x: x.quantile(0.8))\n\n    # min ba spread\n    df['market_min_ba_spread_mean'] = df.groupby('time_id')['ba_spread_min'].transform('mean')\n    df['market_min_ba_spread_std'] = df.groupby('time_id')['ba_spread_min'].transform('std')\n    df['market_min_ba_spread_median'] = df.groupby('time_id')['ba_spread_min'].transform('median')\n    df['market_min_ba_spread_20'] = df.groupby('time_id')['ba_spread_min'].transform(lambda x: x.quantile(0.2))\n    df['market_min_ba_spread_80'] = df.groupby('time_id')['ba_spread_min'].transform(lambda x: x.quantile(0.8))\n    # df['standardized_min_ba_spread_market'] = (df['ba_spread_min'] - df['market_min_ba_spread_mean']) / df['market_min_ba_spread_std']\n    # df['market_min_ba_spread_std'] = df.groupby('time_id')['ba_spread_min'].transform('std')\n    # df['market_relative_slope_mean'] = df.groupby('time_id')['rv_slope_relative'].transform('mean')\n    # mean ba spread\n    # df['market_mean_ba_spread_mean'] = df.groupby('time_id')['ba_spread_mean'].transform('mean')\n    # df['market_mean_ba_spread_std'] = df.groupby('time_id')['ba_spread_mean'].transform('std')\n\n    # standardized realized vol on market level\n    df['market_rv_mean'] = df.groupby('time_id')['log_realized_vol'].transform('mean')\n    df['market_rv_std'] = df.groupby('time_id')['log_realized_vol'].transform('std')\n    df['market_rv_median'] = df.groupby('time_id')['log_realized_vol'].transform('median')\n    df['market_rv_20'] = df.groupby('time_id')['log_realized_vol'].transform(lambda x: x.quantile(0.2))\n    df['market_rv_80'] = df.groupby('time_id')['log_realized_vol'].transform(lambda x: x.quantile(0.8))\n    df['standardized_rv_market'] = (df['log_realized_vol'] - df['market_rv_mean']) / df['market_rv_std']\n    # rv slope on market level\n    # df['market_rv_slope_mean'] = df.groupby('time_id')['rv_slope_relative'].transform('mean')\n    # standardized time weighted realized vol on market level\n    # df['market_tw_rv_mean'] = df.groupby('time_id')['log_tw_realized_vol'].transform('mean')\n    # df['market_tw_rv_std'] = df.groupby('time_id')['log_tw_realized_vol'].transform('std')\n    # df['standardized_tw_rv_market'] = (df['log_tw_realized_vol'] - df['market_tw_rv_mean'])/df['market_tw_rv_std']\n    # standardized time weighted realized vol (level 2) on market level\n    df['market_tw_rv2_mean'] = df.groupby('time_id')['log_tw_realized_vol2'].transform('mean')\n    df['market_tw_rv2_std'] = df.groupby('time_id')['log_tw_realized_vol2'].transform('std')\n    df['market_tw_rv2_median'] = df.groupby('time_id')['log_tw_realized_vol2'].transform('median')\n    df['market_tw_rv2_20'] = df.groupby('time_id')['log_tw_realized_vol2'].transform(lambda x: x.quantile(0.2))\n    df['market_tw_rv2_80'] = df.groupby('time_id')['log_tw_realized_vol2'].transform(lambda x: x.quantile(0.8))\n    df['standardized_tw_rv2_market'] = (df['log_tw_realized_vol2'] - df['market_tw_rv2_mean']) / df['market_tw_rv2_std']\n    # diff fe calculation\n    df['log_return_diff'] = df['log_return_max'] - df['log_return_min']\n    df['VOI_diff'] = df['VOI_max'] - df['VOI_min']\n    \n    df['rv_impact_diff_3_4'] = df['last_time_split_rv_impact'] - df['third_time_split_rv_impact']\n    df['rv_impact_diff_1_4'] = df['last_time_split_rv_impact'] - df['first_time_split_rv_impact']\n    df['rv_impact_diff_2_4'] = df['last_time_split_rv_impact'] - df['second_time_split_rv_impact']\n    df['market_relative_slope_mean'] = df.groupby('time_id')['rv_impact_diff_1_4'].transform('mean')\n\n    # corr features\n    df_pivot = train_df.pivot(index='time_id', columns='stock_id', values='log_return_sum')\n    corr = df_pivot.corr()\n    max_stock, max_corr = [], []\n    for i in corr.index:\n        max_stock.append(corr[i].nlargest(2).index[1])\n        max_corr.append(corr[i].nlargest(2).iloc[1])\n    \n    df_corr_manual = pd.DataFrame({'stock_id': corr.index, 'corr_stock_id': max_stock, 'corr_stock_value': max_corr})\n    corr_stock_list = df_corr_manual['corr_stock_id'].unique()\n    low_corr_list = df_corr_manual.loc[df_corr_manual['corr_stock_value'] < 0.5, 'stock_id']\n    corr_columns = ['stock_id', 'time_id', 'ba_spread_min']\n    df_corr_temp = df.loc[df['stock_id'].isin(corr_stock_list), corr_columns].rename(columns={'stock_id': 'corr_stock_id',\n                                                                                              'ba_spread_min': 'ba_spread_min_corr'})\n    df_corr_temp.loc[df_corr_temp['corr_stock_id'].isin(low_corr_list), ['ba_spread_min_corr']] = np.nan\n    df = df.merge(df_corr_manual, on='stock_id', how='left')\n    df = df.merge(df_corr_temp, on=['corr_stock_id', 'time_id'], how='left')\n    \n    return df","metadata":{"execution":{"iopub.status.busy":"2021-09-06T15:06:20.520305Z","iopub.execute_input":"2021-09-06T15:06:20.520945Z","iopub.status.idle":"2021-09-06T15:06:20.539743Z","shell.execute_reply.started":"2021-09-06T15:06:20.520903Z","shell.execute_reply":"2021-09-06T15:06:20.538705Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# running fe in multi-processing\nwith timer(\"Feature Engineering...\"):\n    train_df = paralle_fe(book_list_file=list_order_book_file_train)\n    train_df = train.merge(train_df, on=['row_id'], how='right')\n    train_df = fe_group(train_df)\n","metadata":{"execution":{"iopub.status.busy":"2021-09-06T15:06:20.558963Z","iopub.execute_input":"2021-09-06T15:06:20.559275Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"FEATS = ['realized_vol',\n         'realized_vol_l2',  \n         # 'time_weighted_rv',\n         'time_weighted_rv2',\n         'standardized_rv_market', 'standardized_tw_rv2_market',\n         'market_rv_mean', 'market_rv_std',\n         'market_tw_rv2_mean', 'market_tw_rv2_std',\n         # 'order_batches',\n         # 'size',\n         'order_count',\n         'size_per_order',\n         'trade_rv',  # 'trade_return_std',\n         'ba_spread_mean', 'ba_spread_min',  # 'ba_spread_std',\n         # 'ba_spread_max',\n         'log_return_max',  # 'log_return_std',\n         'log_return_min',  # 'log_return_mean',\n         'weighted_log_return_mean',\n         'num_quote_updates',\n         'LDispersion',\n         'stock_id',\n         'VOI_max',  # 'VOI_mean', 'VOI_std',\n         'VOI_diff',\n         'log_return_diff',  # 'weighted_log_return_diff',\n         'time_split_rv_first',  # 'last_90_rv',\n         'time_split_rv_second',\n         'time_split_rv_third',\n         'time_split_rv_last',\n         'num_price_updates',\n         'time_split_ba_is_max_last',\n         # 'time_split_ba_mean_compare_last',\n         'time_split_ba_spread_min_last',\n         'total_order_per_ba_spread_mean', 'total_order_per_ba_spread_min',\n         'cancel_order_size',\n         'quote_updated_ex_cancel',\n         'market_min_ba_spread_mean',\n         'market_min_ba_spread_std',\n         # 'standardized_min_ba_spread_market',\n         'weighted_ba_spread_mean',  # 'weighted_ba_spread_max',\n         'market_order_count_median',\n         # 'bid_spread_vs_ba_mean',\n         'ask_spread_vs_ba_mean',\n         'ba_spread_abs_change',\n         'ba_spread_min_corr',\n         'rv_spikes',\n         'rv_impact_diff_1_4',\n         'rv_impact_diff_2_4',\n         'market_relative_slope_mean',\n         'time_split_ba_is_max_first',\n         'market_order_count_20', 'market_order_count_80', 'market_order_count_10',\n         ]\n\n\n\ncategorical_features = ['stock_id']\nfor col in categorical_features:\n    train_df[col] = train_df[col].astype('category')\n\nlgb_params = {'objective': 'regression',\n              'metric': 'rmse',\n              'learning_rate': 0.05,\n              'max_depth': 5,\n              'num_leaves': 20,\n              'verbose': -1,\n              # 'feature_fraction': 0.9,\n              # 'bagging_fraction': 0.9\n              }","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function to early stop with root mean squared percentage error\ndef feval_rmspe(y_pred, lgb_train):\n    y_true = lgb_train.get_label()\n    return 'RMSPE', rmspe(y_true, y_pred), False\n\nModel_Dict = {}\ncv_splits = 5\nwith timer('Modeling and Prediction...'):\n    RMSPE_in_sample_array, RMSPE_oos_array = np.ones(cv_splits), np.ones(cv_splits)\n    CV = GroupKFold(n_splits=cv_splits)\n\n    X = train_df[FEATS]\n    y = train_df.target\n    original_y = train_df.target\n\n    for cv_id, (train_idx, val_idx) in enumerate(CV.split(X=train_df, y=y, groups=train_df['time_id'])):\n        print(f'Running Fold # {cv_id + 1}...')\n        train_X, train_y = X.iloc[train_idx, :], y.iloc[train_idx]\n        val_X, val_y = X.iloc[val_idx, :], y.iloc[val_idx]\n        eval_train_y, eval_val_y = original_y.iloc[train_idx], original_y.iloc[val_idx]\n\n        train_weights = 1 / np.square(train_y)\n        val_weights = 1 / np.square(val_y)\n\n        print('Train Data:', len(train_X), 'Validation Data:', len(val_X))\n        lgb_train = lgb.Dataset(train_X, label=train_y, weight=train_weights, feature_name=FEATS)\n        lgb_valid = lgb.Dataset(val_X, label=val_y, weight=val_weights, feature_name=FEATS)\n\n        # =========================================================================\n        start = time.time()\n        model = lgb.train(lgb_params,\n                          train_set=lgb_train,\n                          num_boost_round=1000,\n                          valid_sets=[lgb_train, lgb_valid],  # list of data to be evaluated on during training\n                          verbose_eval=100,\n                          early_stopping_rounds=50,\n                          feval=feval_rmspe)\n        end = time.time()\n        print('LGB training time elapsed: {}'.format(end - start))\n        # =========================================================================\n        train_prediction = model.predict(train_X)\n        val_prediction = model.predict(val_X)\n\n        RMSPE_in_sample_array[cv_id] = rmspe(eval_train_y, train_prediction)\n        RMSPE_oos_array[cv_id] = rmspe(eval_val_y, val_prediction)\n        print('\\n')\n        Model_Dict[f'lgb {cv_id}'] = model\n\nRMSPE = round(RMSPE_oos_array.mean(), 4)\nprint(f'Performance of the naive prediction: RMSPE: {RMSPE}')\n\ntrain_RMSPE = round(RMSPE_in_sample_array.mean(), 4)\nprint(f'Performance of the naive prediction in sample: RMSPE: {train_RMSPE}')\n\n# lgb.plot_importance(Model_Dict['lgb 0'], importance_type='gain', ignore_zero=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# for name, model in Model_Dict.items():\n#     print(f'Feature Importance of Fold {name}')\n#     lgb.plot_importance(model, importance_type='gain', ignore_zero=False)\n#     print('\\n')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Submission","metadata":{}},{"cell_type":"code","source":"def single_stock_fe(book_list_file, train_test=True):\n    df_all_stock = pd.DataFrame()\n    for book_file in book_list_file:\n        stock_folder = book_file.split('/')[-1]\n        if train_test:\n            trade_file = path + 'trade_train.parquet/' + stock_folder\n        else:\n            trade_file = path + 'trade_test.parquet/' + stock_folder\n        df_all_stock = pd.concat([df_all_stock, feature_engineering(book_file, trade_file, train_test)])\n\n    return df_all_stock","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test = pd.read_csv(path + 'test.csv')\nlist_order_book_file_test = glob.glob(path + 'book_test.parquet/*')\n\n# running fe in multi-processing\nwith timer(\"Feature Engineering...\"):\n    test_df = paralle_fe(book_list_file=list_order_book_file_test, train_test=False)\n    test_df = test.merge(test_df, on=['row_id'], how='left')\n    test_df = fe_group(test_df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for col in categorical_features:\n    test_df[col] = test_df[col].astype('category')\ntest_X = test_df[FEATS]\n\npred = []\ntest_df['target'] = 0\nfor name, model in Model_Dict.items():\n    # pred.append(model.predict(test_X))\n    test_df['target'] += model.predict(test_X)\n# avg_pred = np.mean(pred, axis=0)\ntest_df['target'] /= cv_splits\ndf_pred_test = test_df[['row_id', 'target']]\ndf_pred_test.to_csv('submission.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}