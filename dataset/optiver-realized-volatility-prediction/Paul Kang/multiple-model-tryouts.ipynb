{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Welcome to Paul Kang's Data Science practice\n\nLets import the basic (normal stuff that I use when it comes to non-big data ML projects) libraries... and make sure the data that I am dealing with does not have any missing/non-numeric(except for categoricals). This became a habit to me cuz... it bit me a lot of times when doing some advanced analytics works on the job...","metadata":{}},{"cell_type":"code","source":"# import necessary libraries: this will get updated as I go along\nimport pandas as pd\nimport numpy as np\nimport plotly.express as px\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport shap\nfrom sklearn import metrics, model_selection, preprocessing\nimport warnings \nimport xgboost\nfrom statsmodels.graphics import tsaplots as tsa\nwarnings.filterwarnings('ignore')\n%matplotlib inline","metadata":{"execution":{"iopub.status.busy":"2021-08-05T00:50:35.688424Z","iopub.execute_input":"2021-08-05T00:50:35.688692Z","iopub.status.idle":"2021-08-05T00:50:35.696457Z","shell.execute_reply.started":"2021-08-05T00:50:35.688665Z","shell.execute_reply":"2021-08-05T00:50:35.695424Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_csv_route = '../input/optiver-realized-volatility-prediction/train.csv'\ntest_csv_route = '../input/optiver-realized-volatility-prediction/test.csv'\nbook_train = pd.read_parquet('../input/optiver-realized-volatility-prediction/book_train.parquet')\nbook_test = pd.read_parquet('../input/optiver-realized-volatility-prediction/book_test.parquet')\ntrade_train = pd.read_parquet('../input/optiver-realized-volatility-prediction/trade_train.parquet')\ntrade_test = pd.read_parquet('../input/optiver-realized-volatility-prediction/trade_test.parquet')\n\ntrain = pd.read_csv(train_csv_route)\ntest = pd.read_csv(test_csv_route)\ndata = {\n    'book_train':book_train,\n    'book_test':book_test,\n    'trade_train':trade_train,\n    'trade_test':trade_test,\n    'train':train,\n    'test':test\n}","metadata":{"execution":{"iopub.status.busy":"2021-08-05T00:52:30.267261Z","iopub.execute_input":"2021-08-05T00:52:30.267732Z","iopub.status.idle":"2021-08-05T00:52:48.708781Z","shell.execute_reply.started":"2021-08-05T00:52:30.267674Z","shell.execute_reply":"2021-08-05T00:52:48.707883Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Lets study on how the data that I am going to deal with looks like...","metadata":{}},{"cell_type":"code","source":"def set_check(df,df_name):\n    print(\"-\"*40,\"For \",df_name,\" Set\",\"-\"*40)\n    print(f\"column names: {list(df.columns)}\\n\")\n    print(f\"data types: {list(df.dtypes)}\\n\")\n    print(f\"shape of data that we are dealing with in this dataset: {df.shape}\\n\")\n    print(f\"Null data for each columns?: \\n{df.isnull().sum()}\\n\")\n\nfor df_name,df in data.items():\n    set_check(df,df_name)","metadata":{"execution":{"iopub.status.busy":"2021-08-05T00:52:51.354361Z","iopub.execute_input":"2021-08-05T00:52:51.35469Z","iopub.status.idle":"2021-08-05T00:52:55.007163Z","shell.execute_reply.started":"2021-08-05T00:52:51.354662Z","shell.execute_reply":"2021-08-05T00:52:55.006031Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(sorted(list(train['stock_id'].unique())), \"\\n unique values: \", len(list(train['stock_id'].unique())))\nprint(sorted(list(trade_train['stock_id'].unique())), \"\\n unique values: \", len(list(trade_train['stock_id'].unique())))","metadata":{"execution":{"iopub.status.busy":"2021-08-05T00:52:55.008759Z","iopub.execute_input":"2021-08-05T00:52:55.009177Z","iopub.status.idle":"2021-08-05T00:52:55.666514Z","shell.execute_reply.started":"2021-08-05T00:52:55.009136Z","shell.execute_reply":"2021-08-05T00:52:55.665651Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 0. Why am I doing this as a petrochemical process engineer?\n\nIn most cases, when Data Scientists (DS) approach and try to solve a business problem, often there is a need for DS to have at least tiny bit of understanding of what the domain that they are dealing with (domain knowledge) to understand what data that they need, what each of the feature mean for the \"prepared\" data. As a person who came from the Petroleum/Oil trying to observe and study the data science, I can understand that it can be quite difficult for the professional data scientists to actually \"perform their magic\" without understanding underlying meaning of each attributes and phenomenon. \n\nEvery domain knowledge has its expertise; thats why we have so many different field of study: stocks, engineering, education, medicine, pharmacy, finance, fitness, ..... etc; Its endless. For each of the streams, hundreads of years the experts have constructed the knowledge and it just became a castle. Domain experts are the ones who actually studied those mountains, and someone who has an actual experience of applying those domain knowledge to real world is called engineers; and during the process, engineers often go wild data analysis to do data analyst job: find solutions/answers & ask the right questions that will lead to another answers and another chain of questions, whether if it involves interpolating to describe the phenomenon, or extrapolating to predict future using known algorithms such as multivariate analysis, blah blah blah you name it.\n\nData science, to me, it is a field that every engineers should learn if they want to become really good engineers. Unless someone wants to go to research finding/designing novel ML algorithm, it is a skill that every engineers should learn to advance their skills on applying their knowledge to their subject matter.\n\nLet me hone my data skills in kaggle, by using the datasets that does not belong to my subject matter. \n\n# 1. Understanding the Data\n\nNow, I am dealing with 6 datasets, spliting into train/test: book_train, trade_train, train, book_test, trade_test, test, all in csv files, and the book, trade sets are in a parquet. \n\ntest sets are having only 3 rows, which contains the attributes of what booking, trading sets have, and our goal is to predict the realized volatility of the test sets.\n\n* For booking, we have: \n\n>     time_id              time for the stock that are recorded. just think of this as a time, represented by just integer, instead of widely accepted (ISO8601) of dates and time\n>     seconds_in_bucket    yeah. these guys reinvented the wheel on how to record the time. this is just # seconds within the time_id bucket.\n>     bid_price1           obvious. read the introductory notebook and u will understand what this means - it is first best bid price recorded in the order book\n>     ask_price1           it is first best ask price recorded in the order book\n>     bid_price2           second best bid (buy)\n>     ask_price2           second best ask (sell)\n>     bid_size1            at the first best bid, what was the volume (how many is this person trying to buy at the best buying price 1)\n>     ask_size1            at the first best ask, what was the volume (how many is this person trying to sell at the best selling price 1)\n>     bid_size2            at the second best bid, what was the volume (how many is this person trying to buy at the second best buying price 2)\n>     ask_size2            at the second best ask, what was the volume (how many is this person trying to sell at the second selling price 2)\n>     stock_id             this of this as company name like for tesla: its TSLA in NASDAQ or sth. just company name in the stock market\n\n* For trading we have: \n\n>     time_id              same thing as time_id\n>     seconds_in_bucket    same as booking set\n>     price                price of a stock when the trade finally got executed. \n>     size                 overall stock selling/buying at that time, at that price it was executed\n>     order_count          The number of unique trade orders taking place.\n>     stock_id             company name, same as booking set\n\nokay.. we have lots of data and kindly enough, Optiver gave us great hints: bid/ask spread, WAP, Log-return, calculated realized volatility. Before going onto calculate them, lets study the relationships here. \n\nfor the realized volatility, it seems for each stock_id, and each time_id, it is calculated using the data inside those buckets. for example, at company 0, at time 5 (stock_id=0,time_id=5), there are certain amount of \"trade\" that was happening, and the realized volatility is calculated using those \"trade data\" within that bucket. Then this is simple. this data, for stock_id, and time_id, it is a \"long\" data format. \n\nfor each time_id, how can I then represent the \"unique\" order counts, price, size, 1st bidding/ask 2nd bidding/ask volumen and prices? using WAP? each row will have different WAP (execpt for the first row) we need some creativity here.\n\nI have just checked that for each company stock_id, we have same time_id, and just confirmed they are consistent. then this means, the row value can be just only the time_id, because I also want to study the interaction among the companies and how other parameters at different comapnies they affect the realized volatility of comapny '0' \n\nfor each company, we have 9 basic traits at booking data (except for company id and time id), and 112 different companies. then this means, if we pivot this, \n\n9 traits for 112 companies, 1008 basic traits on booking data, 4 traits in 112 companies, 448 basic traits on trading data. What am I going to do with other company stock data if our target is only to predict using stock_id=0? well, I want to see the relationship of how other companies interact and affect the company 0. But... in our test data, we only have the booking and trading information with the company '0', which means if I want to predict with the interaction with other company involved, Optiver needs to provide those trade and booking data for other stock ids as well.\n\nfor feature engineering:\n\n    for trading sets:\n\n        sum the unique order_count, \n        sum the size, \n        summed_size/summed_order_count, \n        weighted average of seconds_in_bucket according to the size, \n        mean the price\n        Log-return of prices\n        highest_price-lowest_price\n\n    for booking sets:\n    \n        mean of:\n            BAS_1\n            BAS_2\n            WAP_1\n            WAP_2\n            Log_return_1\n            Log_return_2\n            Calculated_volatility_1 (sigma_1)\n            Calculated_volatility_2 (sigma_2)\n            bid_price1/2\n            ask_price1/2\n            seconds_in_bucket\n        sum of:\n            bid_size1/2\n            ask_size1/2\n        \n# 2.0 Data wrangling / transforming / feature engineering","metadata":{}},{"cell_type":"code","source":"book_train = book_train[book_train['stock_id']==0]\ntrade_train = trade_train[trade_train['stock_id']==0]\ntrain = train[train['stock_id']==0]\nprint(f\"booking_training set of stock id 0 is: {book_train.shape}\")\nprint(f\"trading_training set of stock id 0 is: {trade_train.shape}\")\nprint(f\"actual recorded volatility training set set of stock id 0 is: {train.shape}\")","metadata":{"execution":{"iopub.status.busy":"2021-08-05T00:53:17.867088Z","iopub.execute_input":"2021-08-05T00:53:17.867453Z","iopub.status.idle":"2021-08-05T00:53:18.529833Z","shell.execute_reply.started":"2021-08-05T00:53:17.867422Z","shell.execute_reply":"2021-08-05T00:53:18.528928Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# configuring class that groups the functions\nclass Optiver_feature_engineered:\n    \n    \"\"\"\n    it is a collection of the features... docstring work in progress. \n    \"\"\"\n    TODO: 'complete docstring for this'\n    \n    def __init__(self,df=None,df_name=None):\n        self.df = df\n        self.df_name = df_name\n        \n    def BAS(self,ask_price,bid_price):\n        return [ask_p/bid_p - 1 for ask_p,bid_p in zip(ask_price,bid_price)]\n\n    def WAP(self,df):\n        wap = (df[df.columns[0]] * df[df.columns[1]] + df[df.columns[2]]*df[df.columns[3]])/(df[df.columns[1]]+df[df.columns[3]])\n        return wap\n\n    def log_return(self,list_stock_prices):\n        return np.log(list_stock_prices).diff() \n\n    def realized_volatility(self,series_log_return):\n        return np.sqrt(np.sum(series_log_return**2))\n","metadata":{"execution":{"iopub.status.busy":"2021-08-05T00:53:19.530975Z","iopub.execute_input":"2021-08-05T00:53:19.531355Z","iopub.status.idle":"2021-08-05T00:53:19.540366Z","shell.execute_reply.started":"2021-08-05T00:53:19.531312Z","shell.execute_reply":"2021-08-05T00:53:19.539368Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# preprocessing for booking dataset:\n# BAS\n# WAP\n# Log return\n# Calculated volatility\n\n\nfe = Optiver_feature_engineered(df,'df')\ndef preprocessings_book(df):\n    \n    df['seconds_in_bucket'] = df['seconds_in_bucket'] + 1 # cuz 0 seconds will mess up the data internally\n    df['seconds_bids'] = df['seconds_in_bucket']*(df['bid_size1']+df['bid_size2'])\n    df['seconds_asks'] = df['seconds_in_bucket']*(df['ask_size1']+df['ask_size2'])\n    df['BAS1'] = fe.BAS(df['ask_price1'],df['bid_price1'])\n    df['BAS2'] = fe.BAS(df['ask_price2'],df['bid_price2'])\n    df['WAP1'] = fe.WAP(df[['bid_price1','ask_size1','ask_price1','bid_size1']])\n    df['WAP2'] = fe.WAP(df[['bid_price2','ask_size2','ask_price2','bid_size2']])\n    df['logr1'] = df.groupby(['time_id'])['WAP1'].apply(fe.log_return)\n    df['logr2'] = df.groupby(['time_id'])['WAP2'].apply(fe.log_return)\n    apply_functions = {\"seconds_in_bucket\":\"mean\",\n                       \"bid_price1\":\"mean\",\n                       \"bid_price2\":\"mean\",\n                       \"ask_price1\":\"mean\",\n                       \"ask_price2\":\"mean\",\n                       \"BAS1\":\"mean\",\n                       \"BAS2\":\"mean\",\n                       \"WAP1\":\"mean\", # null values to be ignored when taking mean\n                       \"WAP2\":\"mean\", # null values to be ignored when taking mean\n                       \"logr1\":\"mean\",\n                       \"logr2\":\"mean\",\n                       \"seconds_bids\":\"sum\",\n                       \"seconds_asks\":\"sum\",\n                       'bid_size1':\"sum\",\n                       'bid_size2':\"sum\",\n                       'ask_size1':\"sum\",\n                       'ask_size2':\"sum\"\n                      }\n    df_feature = df.groupby(['time_id']).agg(apply_functions)\n    df_feature['vol_1'] = df.groupby(['time_id'])['logr1'].apply(fe.realized_volatility)\n    df_feature['vol_2'] = df.groupby(['time_id'])['logr2'].apply(fe.realized_volatility)\n    df_feature['seconds_bids'] = df_feature['seconds_bids']/(df_feature['bid_size1'] + df_feature['bid_size2'])\n    df_feature['seconds_asks'] = df_feature['seconds_asks']/(df_feature['ask_size1'] + df_feature['ask_size2'])\n    df_feature.reset_index(inplace=True)\n    df_feature.drop(columns='seconds_in_bucket',axis=1,inplace=True)\n    return df_feature\n\n# Preprocessing for trading dataset:\n\ndef preprocessings_trade(df):\n\n    df['seconds_in_bucket'] = df['seconds_in_bucket'] + 1\n    df['seconds_size'] = df['seconds_in_bucket']*df['size']\n    df['logr_p'] = df.groupby(['time_id'])['price'].apply(fe.log_return)\n    apply_func = {\n        'order_count':'sum',\n        'seconds_in_bucket':'mean',\n        'size':'sum',\n        'seconds_size':'sum',\n        'price':'mean',\n        'logr_p':'mean'\n    }\n\n    df_feature = df.groupby(['time_id']).agg(apply_func)\n    df_feature['spread'] = df.groupby(['time_id'])['price'].max() - df.groupby(['time_id'])['price'].min()\n    df_feature['vol_p'] = df.groupby(['time_id'])['logr_p'].apply(fe.realized_volatility)\n    df_feature['seconds_size'] = df_feature['seconds_size']/df_feature['size']\n    df_feature.reset_index(inplace=True)\n    df_feature.drop(columns='seconds_in_bucket',axis=1,inplace=True)\n    return df_feature\n\nbook_train_feature = preprocessings_book(book_train)\nbook_test_feature = preprocessings_book(book_test)\ntrade_train_feature = preprocessings_trade(trade_train)\ntrade_test_feature = preprocessings_trade(trade_test)\n\n\ndataset = pd.merge(book_train_feature,trade_train_feature,how='left',on=['time_id'])\ndf = pd.merge(dataset,train,how='right',on=['time_id'])\ndf.drop(columns='stock_id',inplace=True)\ndf.dropna(inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-08-05T00:53:22.228601Z","iopub.execute_input":"2021-08-05T00:53:22.229155Z","iopub.status.idle":"2021-08-05T00:53:32.048586Z","shell.execute_reply.started":"2021-08-05T00:53:22.229104Z","shell.execute_reply":"2021-08-05T00:53:32.047555Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset = pd.merge(book_train_feature,trade_train_feature,how='left',on=['time_id'])\ndf = pd.merge(dataset,train,how='right',on=['time_id'])\ndf.drop(columns='stock_id',inplace=True)\ndf.dropna(inplace=True)\ndataset_val = pd.merge(book_test_feature,trade_test_feature,how='left',on=['time_id'])\ndf_val = pd.merge(dataset_val,test,how='right',on=['time_id'])\ndf_val.drop(columns=['stock_id','row_id'],inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-08-05T00:53:32.05026Z","iopub.execute_input":"2021-08-05T00:53:32.050643Z","iopub.status.idle":"2021-08-05T00:53:32.084806Z","shell.execute_reply.started":"2021-08-05T00:53:32.050603Z","shell.execute_reply":"2021-08-05T00:53:32.084058Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# book_train_feature.describe(percentiles = [i/10 for i in range(1,10)])\n\nfrom IPython.display import display\n\ndisplay(df.head())\ndf_val.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-05T00:53:32.086267Z","iopub.execute_input":"2021-08-05T00:53:32.086817Z","iopub.status.idle":"2021-08-05T00:53:32.146716Z","shell.execute_reply.started":"2021-08-05T00:53:32.086759Z","shell.execute_reply":"2021-08-05T00:53:32.145682Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3.0 Exploratory Data Analysis (EDA) and visualizations\n\nFor me, the easiest way to take a head start is to:\n\n1. study the correlation heatmaps, sort out the predictors that has highest pearson correlation in descending order,\n\n2. look at the scatter matrix to see how the distribution of the datapoints x and y looks like\n\n3. Check each predictor and the response variable with the seasonality or cyclicity (acf,pacf) to check autocorrelations (find out if it is suitable to apply mutivariate RNN/LSTM eventhough it is not sequential dataset, but we can make it like one...\n\n4. draw boxplots for predictors and target\n\n5. draw distribution plot/histogram for each predictors and target to see skewness, kurtosis","metadata":{}},{"cell_type":"markdown","source":"   # 3.1 Correlation heatmaps / Scatter Matrix, listings of correlations","metadata":{}},{"cell_type":"code","source":"import plotly.figure_factory as ff\n\nfig = ff.create_annotated_heatmap(df.corr().apply(lambda x: round(x,2)).to_numpy(),x = list(df.columns),y=list(df.columns))\nfig.update_layout(\n    margin=dict(l=40, r=40, t=40, b=40),\n    autosize=False,\n    width=1500,\n    height=1500,\n    paper_bgcolor=\"LightSteelBlue\",\n)\n\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-04T22:38:45.885195Z","iopub.execute_input":"2021-08-04T22:38:45.885565Z","iopub.status.idle":"2021-08-04T22:38:48.063277Z","shell.execute_reply.started":"2021-08-04T22:38:45.885533Z","shell.execute_reply":"2021-08-04T22:38:48.062124Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = px.scatter_matrix(df)\nfig.update_layout(\n    margin=dict(l=40, r=40, t=40, b=40),\n    autosize=False,\n    width=1500,\n    height=1500,\n    paper_bgcolor=\"LightSteelBlue\",\n)\nfig.update_traces(marker=dict(size=2))\n\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-04T22:38:55.23722Z","iopub.execute_input":"2021-08-04T22:38:55.237677Z","iopub.status.idle":"2021-08-04T22:38:56.391752Z","shell.execute_reply.started":"2021-08-04T22:38:55.237641Z","shell.execute_reply":"2021-08-04T22:38:56.390735Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Sure...what correlates best with the target realized volatility..? shortlist them..","metadata":{}},{"cell_type":"code","source":"def correlation_list(df,var_name):\n    # unwind the matrix to 1d array with the var1,var2 multiindex, descending order\n    corr_list = df.corr().unstack().sort_values(kind='quicksort').iloc[::-1]\n    # reset the indexes\n    corr_list = corr_list.reset_index()\n    # rename the indexes to var_1 and var_2\n    corr_list.rename(columns={'level_0':'var_1','level_1':'var_2',0:'correlation'},inplace=True)\n    # take only the half of the triangle from the correlation matrix, since it is just a duplicate.\n    corr_list.drop_duplicates(subset=['correlation'],inplace=True)\n    corr_list.reset_index(drop=True,inplace=True)\n    # remove the middle diagonal numbers (correlation = 1.0)\n    drop_index = []\n    for i in range(len(corr_list)):\n        if corr_list.iloc[i,0] == corr_list.iloc[i,1]:\n            drop_index.append(i)\n    corr_list.drop(labels=drop_index,axis=0,inplace=True)\n    # check if there is var1 and var 2 overlaps\n    count = (corr_list['var_1'] == corr_list['var_2'])\n    print(f\"duplicated number: {count.sum()}\")\n    if count.sum() != 0:\n        raise Exception('Check your function code. something is not working with ur dataset')\n    else:\n        corr_var1 = corr_list[corr_list['var_1']==var_name]\n        corr_var2 = corr_list[corr_list['var_2']==var_name]\n        corr_var2.rename(columns={'var_1':'var_2','var_2':'var_1'},inplace=True)\n        corr_var = corr_var1.append(corr_var2,ignore_index=True)\n        corr_var.sort_values(by=['correlation'],inplace=True,ascending=False)\n        corr_var.reset_index(drop=True,inplace=True)\n    return corr_var","metadata":{"execution":{"iopub.status.busy":"2021-08-05T00:54:42.650357Z","iopub.execute_input":"2021-08-05T00:54:42.650742Z","iopub.status.idle":"2021-08-05T00:54:42.661407Z","shell.execute_reply.started":"2021-08-05T00:54:42.650711Z","shell.execute_reply":"2021-08-05T00:54:42.660431Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"corr_var = correlation_list(df,'target')\ncorr_var","metadata":{"execution":{"iopub.status.busy":"2021-08-05T00:54:43.810744Z","iopub.execute_input":"2021-08-05T00:54:43.811134Z","iopub.status.idle":"2021-08-05T00:54:43.872515Z","shell.execute_reply.started":"2021-08-05T00:54:43.811103Z","shell.execute_reply":"2021-08-05T00:54:43.871684Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Visualize it:","metadata":{}},{"cell_type":"code","source":"px.bar(corr_var,x='var_2',y='correlation',title='correlation with the target')","metadata":{"execution":{"iopub.status.busy":"2021-08-05T00:54:47.64137Z","iopub.execute_input":"2021-08-05T00:54:47.642509Z","iopub.status.idle":"2021-08-05T00:54:48.893729Z","shell.execute_reply.started":"2021-08-05T00:54:47.642469Z","shell.execute_reply":"2021-08-05T00:54:48.89261Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"okay,... so the target realized volatility is heavily correlated with the calculated volaility from the booking set and trade sets, and the bid/ask spread metric... also, the \"size\" from the trade set also matters... of course. higher the size with frequency may correlate well with the realized volatility... i am just surprized how price is not really correlating with the target realized volatility. but lets go ahead with this for now.","metadata":{}},{"cell_type":"markdown","source":"# 3.2 Autocorrelation factor / Partial autocorrelation (acf/pacf) and cycliclity\n\nin my previous work... I have done some LSTM/RNN study with predicting tesla stock. and for the dataset that has the characteristics of time series / sequences, it is worthwhile to check the autocorrelation. i am going to copy what I did in the past, and raise again with the concept of autocorrelations. (original workbook: https://www.kaggle.com/pkang0831/failed-lstm-rnn-with-new-recommendation )\n\n**Auto Correlation**\n\nIn time series problems, it is important to see what is its autocorrelation and test if there is time-dependent repeated patterns (cyclicity). For this, pandas lag_plot will be used\n\n> A linear shape to the plot suggests that an autoregressive model is probably a better choice.\n\n> An elliptical plot suggests that the data comes from a single-cycle sinusoidal model.\n\nIf your data shows a linear pattern, it suggests autocorrelation is present. A positive linear trend (i.e. going upwards from left to right) is suggestive of positive autocorrelation; a negative linear trend (going downwards from left to right) is suggestive of negative autocorrelation. The tighter the data is clustered around the diagonal, the more autocorrelation is present; perfectly autocorrelated data will cluster in a single diagonal line.\n\n![](https://www.statisticshowto.com/wp-content/uploads/2016/11/lag-plot-linear.png)\n\n\nData can be checked for seasonality by plotting observations for a greater number of periods (lags). Data with seasonality will repeat itself periodically in a sine or cosine-like wave.\n\n![](https://www.statisticshowto.com/wp-content/uploads/2016/11/seasons-lp.png)\n\nlets give a try at 40 offset and see if it selfcorrelate...","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(20,60))\nfor i,var in enumerate(df.columns):\n    plt.subplot(len(df.columns)/3,3,i+1)\n    plt.title(df.columns[i])\n    pd.plotting.lag_plot(df[var],lag=1700)\n","metadata":{"execution":{"iopub.status.busy":"2021-08-04T22:39:28.471455Z","iopub.execute_input":"2021-08-04T22:39:28.471803Z","iopub.status.idle":"2021-08-04T22:39:34.592912Z","shell.execute_reply.started":"2021-08-04T22:39:28.471773Z","shell.execute_reply":"2021-08-04T22:39:34.591662Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So.... nothing at the lag of 30,80 but 1750, we start to have something... but the the cross shape of the lag plot.. dunno what that refers to. if there is any expert who can interpret what the \"+\" shape means in lag plot, please teach me... \n\numm.. lets calculate the acf and pacf and see where about the lag is sufficient for detecting autocorrelation.","metadata":{}},{"cell_type":"code","source":"# autocorrelation factor\nfig,ax = plt.subplots(27,1,figsize=(20,60))\nfor i,var in enumerate(df.columns):\n    tsa.plot_acf(df[var],lags=1750,ax=ax[i],title=df.columns[i])\n    ax[i].set_ylim(0,0.25)","metadata":{"execution":{"iopub.status.busy":"2021-08-04T22:41:08.304684Z","iopub.execute_input":"2021-08-04T22:41:08.305063Z","iopub.status.idle":"2021-08-04T22:41:17.62228Z","shell.execute_reply.started":"2021-08-04T22:41:08.305031Z","shell.execute_reply":"2021-08-04T22:41:17.621373Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# partial autocorrelation factor - it takes 2.5 hrs to run.. if you guys can please optimize the algorithm for me, I WOULD LOVE TO LEARN.\nfig,ax = plt.subplots(27,1,figsize=(20,60))\nfor i,var in enumerate(df.columns):\n    tsa.plot_pacf(df[var],lags=1750,ax=ax[i],title=df.columns[i])\n    ax[i].set_ylim(0,0.15)","metadata":{"execution":{"iopub.status.busy":"2021-08-04T22:41:19.915334Z","iopub.execute_input":"2021-08-04T22:41:19.915866Z","iopub.status.idle":"2021-08-04T23:17:58.372522Z","shell.execute_reply.started":"2021-08-04T22:41:19.915834Z","shell.execute_reply":"2021-08-04T23:17:58.37151Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"so... the way to read the pacf plots is that there is a 95% confidence interval plotted ","metadata":{}},{"cell_type":"code","source":"fig,ax = plt.subplots(figsize=(40,10))\n\nfor i,col in enumerate(df.columns):\n    plt.subplot(3,len(df.columns)/3,i+1)\n    plt.boxplot(x=df[col],showmeans = True, meanline = True)\n    x = np.random.normal(1, 0.04, size=len(df[col]))\n    plt.plot(x,df[col],'r.',alpha=0.05)\n    plt.plot(1,np.mean(df)[i],'bo',alpha=1)\n    plt.xlabel(col)","metadata":{"execution":{"iopub.status.busy":"2021-08-05T00:53:50.890091Z","iopub.execute_input":"2021-08-05T00:53:50.890471Z","iopub.status.idle":"2021-08-05T00:53:54.220777Z","shell.execute_reply.started":"2021-08-05T00:53:50.890441Z","shell.execute_reply":"2021-08-05T00:53:54.220131Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig,ax = plt.subplots(figsize=(40,10))\n\nfor i,col in enumerate(df.columns):\n    plt.subplot(3,len(df.columns)/3,i+1)\n    sns.distplot(x=df[col])\n    plt.xlabel(col)","metadata":{"execution":{"iopub.status.busy":"2021-08-05T00:54:05.339093Z","iopub.execute_input":"2021-08-05T00:54:05.339672Z","iopub.status.idle":"2021-08-05T00:54:13.143336Z","shell.execute_reply.started":"2021-08-05T00:54:05.339637Z","shell.execute_reply":"2021-08-05T00:54:13.14229Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig,ax = plt.subplots(figsize=(40,10))\n\nfor i,col in enumerate(df.columns):\n    plt.subplot(3,len(df.columns)/3,i+1)\n    sns.kdeplot(x=df[col])\n    plt.xlabel(col)","metadata":{"execution":{"iopub.status.busy":"2021-08-05T00:54:18.17206Z","iopub.execute_input":"2021-08-05T00:54:18.17244Z","iopub.status.idle":"2021-08-05T00:54:22.655361Z","shell.execute_reply.started":"2021-08-05T00:54:18.172405Z","shell.execute_reply":"2021-08-05T00:54:22.654566Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So for the ones that are having decent correlation with the target, lets try to plot KDE (kernel density estimation) plot to interpret which one affects the most..\n\nRecall corr_var: ","metadata":{}},{"cell_type":"code","source":"corr_var","metadata":{"execution":{"iopub.status.busy":"2021-08-05T00:55:02.394784Z","iopub.execute_input":"2021-08-05T00:55:02.395305Z","iopub.status.idle":"2021-08-05T00:55:02.410318Z","shell.execute_reply.started":"2021-08-05T00:55:02.395274Z","shell.execute_reply":"2021-08-05T00:55:02.409296Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let us say.. for example, anything that has lesser than 0.3 of pearson R is ignored. lets see how the KDE looks like for this. then from the list above, its going to be until target vs size","metadata":{}},{"cell_type":"code","source":"name_to_plot = list(corr_var['var_2'][0:7])\nfor i in range(0,7):\n    sns.jointplot(x = name_to_plot[i],y = 'target',data = df)","metadata":{"execution":{"iopub.status.busy":"2021-08-05T00:55:04.340686Z","iopub.execute_input":"2021-08-05T00:55:04.341046Z","iopub.status.idle":"2021-08-05T00:55:12.596008Z","shell.execute_reply.started":"2021-08-05T00:55:04.341016Z","shell.execute_reply":"2021-08-05T00:55:12.595116Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"so... most of them positive relationship...\n\nso far what I can say about this data is that for the target, this calculated volatilities from first/second best bid/sell data and the traded calculated volatility is fairly well representative on what the actual volatility is. furthermore, we cannot overlook the importance of the BAS (bid/ask spread). In trading world, i do not know how important this is, but it must be well known variable that tells a story in liquidity of the trading market.","metadata":{}},{"cell_type":"markdown","source":"# 4.0 Modelling\n\nSo.. I see this as a supervised regression modelling problem. Lets use our famous sklearn library, to split the data, scale our data, fit the data to the collection of models and check the performance. \n\nI will use tensorflow(tf) for neural network...just because i love tf and more familiar of ANN mapping using tf... \n\nAnyways!\n\nPredefined Models to study :) (from https://scikit-learn.org/stable/modules/linear_model.html#ordinary-least-squares)\n\n1. Linear methods\n\n    1.1 Ordinary Least Squares - basically... simple multilinear regression that we all know... y = ax+bx+cx..... + e with [a,b,c,...] being weights (X'X)^-1X'y \n    \n    1.2 Ridge regression - from what I know.. this is just applying penalty alpha in OLS method - (X'X+aI)^-1X'y so that we introduce slight bias to reduce the high variance, avoid overfitting... get the alpha from the CV and try out, determine optimal alpha (or lambda)... more about this later in hyperparameter tuning\n    \n    1.3 Lasso regression - this is very similar to the ridge regression, but there is a slight difference in how the model is set up with that alpha term. in Ridge, the penalty does not goes to zero, but in Lasso, it does. this means that ridge regression, even if it identifies that the variable is useless for predicting target, it cannot eliminate the variable. it just reduces the weights. however, in Lasso, it eliminates them (lambda=0). this means that Lasso will perform slightly better with bunch of features which includes variables/predictors terrible predicting the target\n    \n    1.4 Elastic-Net regression - combination of both ridge and lasso. this lambda 1 and lambda 2 values (L1 - Lasso, L2 - Ridge) are determined using CV, and this model is said to be good at dealing correlated predictors\n    \n    ~~1.5 Least Angle Regression (LARS) or LAS Lasso - This is mainly for the high dimension data (feature > observations). in this case, since we are not dealing with it, we will skip it~~\n    \n    1.6 Orthogonal Matching Persuit - Honestly I really don't know about this algorithm. but lets try...\n    \n    1.7 Bayesian Ridge Regression - again, not too sure about the theoreticals behind this, but this, from what I know, is the bayesian probablistic model for the ridge regression.\n    \n    ~~1.8 Logistic Regression - yes, this can be used for regression, but from what I learned is that this one is mainly used for classification. we will ignore this.~~\n    \n    1.9 Generalized Linear Model (GLM) - generalized linear model (GLM) is a flexible generalization of ordinary linear regression that allows for the response variable to have an error distribution other than the normal distribution. (Got this from wikipedia. we are going to use statsmodels library for this. Scikit Learn has this as well, but statsmodels has put the glm as a separate class from their api.)\n    \n    1.10 Stochastic Gradient Descent - SGDRegressor, basically implements a plain SGD learning routine supporting various loss functions and penalties to fit linear regression models.  for learning SGD, well.. u know what gradient descent is right? if not... um just think of it as one of the algorithm finding local minima in whatever function in 2D/3D... SGD algorithm is just putting stochastic algorithm to perform graident descent (like picking one data/batch at a step), thus much more efficient and effective finding the solution. I suggest u also review Markov chain while u learn this.\n    \n    1.11 Perceptron - What I have read from other resources, this is mainly used for classification. However, this will be discussed later when I deal with multilayer perceptron or ANN modelling with tensorflow\n    \n    \n2. Kernel ridge regression - Ridge regression with using kernel trick/method\n\n\n3. Support Vector Machines\n    \n    3.1 Epsilon-Support Vector Regression.\n    \n    3.2 NuSVR\n    \n    3.3 LinearSVR\n    \n\n4. Stochastic Gradient Descent\n\n\n5. Nearest Neighbors\n\n\n6. Gaussian Processes\n\n\n7. PLSRegression (Partial Least Squares...?)\n\n\n8. Decision Trees\n\n\n\n9.  Ensemble methods","metadata":{}},{"cell_type":"code","source":"from sklearn.experimental import enable_hist_gradient_boosting\nfrom sklearn import linear_model, kernel_ridge, svm, neighbors, gaussian_process, cross_decomposition, tree, ensemble, neural_network\nimport xgboost\nRegress_alg = [\n    # Linear Models\n    linear_model.LinearRegression(), # parameter for later gridsearch: fit_intercept=True, normalize=False, copy_X=True, n_jobs=None, positive=False\n    linear_model.RidgeCV(), # alpha=1.0, fit_intercept=True, normalize=False, copy_X=True, max_iter=None, tol=0.001, solver='auto', random_state=None\n    linear_model.LassoCV(), # alpha=1.0, fit_intercept=True, normalize=False, precompute=False, copy_X=True, max_iter=1000, tol=0.0001, warm_start=False, positive=False, random_state=None, selection='cyclic'\n    linear_model.ElasticNet(), # alpha=1.0, l1_ratio=0.5, fit_intercept=True, normalize=False, precompute=False, max_iter=1000, copy_X=True, tol=0.0001, warm_start=False, positive=False, random_state=None, selection='cyclic'\n    linear_model.OrthogonalMatchingPursuit(), # n_nonzero_coefs=None, tol=None, fit_intercept=True, normalize=True, precompute='auto'\n    linear_model.BayesianRidge(), # n_iter=300, tol=0.001, alpha_1=1e-06, alpha_2=1e-06, lambda_1=1e-06, lambda_2=1e-06, alpha_init=None, lambda_init=None, compute_score=False, fit_intercept=True, normalize=False, copy_X=True, verbose=False\n    linear_model.GammaRegressor(), # alpha=1.0, fit_intercept=True, max_iter=100, tol=0.0001, warm_start=False, verbose=0\n    linear_model.PoissonRegressor(), # alpha=1.0, fit_intercept=True, max_iter=100, tol=0.0001, warm_start=False, verbose=0\n    linear_model.TweedieRegressor(), # power=0.0, alpha=1.0, fit_intercept=True, link='auto', max_iter=100, tol=0.0001, warm_start=False, verbose=0\n    # Stochastic Gradient Descent (SGD)\n    linear_model.SGDRegressor(), # loss='squared_loss', *, penalty='l2', alpha=0.0001, l1_ratio=0.15, fit_intercept=True, max_iter=1000, tol=0.001, shuffle=True, verbose=0, epsilon=0.1, random_state=None, learning_rate='invscaling', eta0=0.01, power_t=0.25, early_stopping=False, validation_fraction=0.1, n_iter_no_change=5, warm_start=False, average=False\n    \n    # Kernel Ridge Regression\n#     kernel_ridge.KernelRidge(), # alpha=1, kernel='linear', gamma=None, degree=3, coef0=1, kernel_params=None\n    \n    # Support Vector Machine - Regressions\n#     svm.SVR(), # kernel='rbf', degree=3, gamma='scale', coef0=0.0, tol=0.001, C=1.0, epsilon=0.1, shrinking=True, cache_size=200, verbose=False, max_iter=- 1\n    svm.NuSVR(), # nu=0.5, C=1.0, kernel='rbf', degree=3, gamma='scale', coef0=0.0, shrinking=True, tol=0.001, cache_size=200, verbose=False, max_iter=- 1\n    svm.LinearSVR(), # epsilon=0.0, tol=0.0001, C=1.0, loss='epsilon_insensitive', fit_intercept=True, intercept_scaling=1.0, dual=True, verbose=0, random_state=None, max_iter=1000\n\n    # Nearest Neighbours Regression\n    neighbors.KNeighborsRegressor(), # n_neighbors=5, weights='uniform', algorithm='auto', leaf_size=30, p=2, metric='minkowski', metric_params=None, n_jobs=None\n#     neighbors.RadiusNeighborsRegressor(), # radius=1.0, weights='uniform', algorithm='auto', leaf_size=30, p=2, metric='minkowski', metric_params=None, n_jobs=None\n    \n    # Gaussian Process Regression (GPR)\n    gaussian_process.GaussianProcessRegressor(), # kernel=None, alpha=1e-10, optimizer='fmin_l_bfgs_b', n_restarts_optimizer=0, normalize_y=False, copy_X_train=True, random_state=None\n    \n    # PLS Regression\n#     cross_decomposition.PLSRegression(), # n_components=2, scale=True, max_iter=500, tol=1e-06, copy=True\n    \n    # Decision Tree\n    tree.DecisionTreeRegressor(), # criterion='mse', splitter='best', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=None, random_state=None, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, ccp_alpha=0.0\n    # Ensemble methods\n    ensemble.AdaBoostRegressor(), # base_estimator=None, n_estimators=50, learning_rate=1.0, loss='linear', random_state=None\n    ensemble.BaggingRegressor(), # base_estimator=None, n_estimators=10, *, max_samples=1.0, max_features=1.0, bootstrap=True, bootstrap_features=False, oob_score=False, warm_start=False, n_jobs=None, random_state=None, verbose=0\n    ensemble.ExtraTreesRegressor(), # n_estimators=100, *, criterion='mse', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features='auto', max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, bootstrap=False, oob_score=False, n_jobs=None, random_state=None, verbose=0, warm_start=False, ccp_alpha=0.0, max_samples=None\n    ensemble.RandomForestRegressor(), # n_estimators=100, *, criterion='mse', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features='auto', max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, bootstrap=True, oob_score=False, n_jobs=None, random_state=None, verbose=0, warm_start=False, ccp_alpha=0.0, max_samples=None\n    ensemble.HistGradientBoostingRegressor(), # loss='least_squares', learning_rate=0.1, max_iter=100, max_leaf_nodes=31, max_depth=None, min_samples_leaf=20, l2_regularization=0.0, max_bins=255, categorical_features=None, monotonic_cst=None, warm_start=False, early_stopping='auto', scoring='loss', validation_fraction=0.1, n_iter_no_change=10, tol=1e-07, verbose=0, random_state=None\n    ensemble.GradientBoostingRegressor(), # loss='ls', learning_rate=0.1, n_estimators=100, subsample=1.0, criterion='friedman_mse', min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_depth=3, min_impurity_decrease=0.0, min_impurity_split=None, init=None, random_state=None, max_features=None, alpha=0.9, verbose=0, max_leaf_nodes=None, warm_start=False, validation_fraction=0.1, n_iter_no_change=None, tol=0.0001, ccp_alpha=0.0\n    \n    \n    \n    # Neural Network, multilayer perceptron\n#     neural_network.MLPRegressor(), # hidden_layer_sizes=100, activation='relu', solver='adam', alpha=0.0001, batch_size='auto', learning_rate='constant', learning_rate_init=0.001, power_t=0.5, max_iter=200, shuffle=True, random_state=None, tol=0.0001, verbose=False, warm_start=False, momentum=0.9, nesterovs_momentum=True, early_stopping=False, validation_fraction=0.1, beta_1=0.9, beta_2=0.999, epsilon=1e-08, n_iter_no_change=10, max_fun=15000\n    xgboost.XGBRegressor()\n]","metadata":{"execution":{"iopub.status.busy":"2021-08-05T00:55:22.920594Z","iopub.execute_input":"2021-08-05T00:55:22.920961Z","iopub.status.idle":"2021-08-05T00:55:22.993787Z","shell.execute_reply.started":"2021-08-05T00:55:22.92093Z","shell.execute_reply":"2021-08-05T00:55:22.99289Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Data prep for fitting the models\n\n1. Split the data\n\n2. Normalize the training set and apply the normalized parameters to the test set\n\nbefore I do this, I will just drop 'time_id' column because to me, it just looks like a time index that it just doesn't make sense","metadata":{}},{"cell_type":"code","source":"# bring the right tool for splitting the data and normalizing the data\nfrom sklearn.model_selection import train_test_split, cross_validate\nfrom sklearn import preprocessing\n\n# Split the data\n# X_train, X_test, Y_train, Y_test, = train_test_split(df.drop(columns='target',axis=1),df['target'], test_size = .33)\nX_train, X_test, Y_train, Y_test, = train_test_split(df.drop(columns='target',axis=1),df['target'], test_size = .2)\n\n\n# Normalizing the train,test predictor variables.\nscaler = preprocessing.StandardScaler()\n\n# Normalize the train predictors\nX_train[X_train.columns] = scaler.fit_transform(X_train[X_train.columns]) \n\n# Apply normalization traits to the test predictors\nX_test[X_test.columns] = scaler.transform(X_test[X_test.columns])\n\nprint(f'Train dataset shape: {X_train.shape}')\nprint(f'Test dataset shape: {X_test.shape}')\nprint(f'Train target dataset shape: {Y_train.shape}')\nprint(f'Test target dataset shape: {Y_test.shape}')\n\ndef rmspe(y_true, y_pred):\n    '''\n    Compute Root Mean Square Percentage Error between two arrays.\n    '''\n    loss = -np.sqrt(np.mean(np.square(((y_true - y_pred) / y_true))))\n\n    return loss\n\nrmspe_loss = metrics.make_scorer(rmspe)","metadata":{"execution":{"iopub.status.busy":"2021-08-05T00:55:26.758389Z","iopub.execute_input":"2021-08-05T00:55:26.758745Z","iopub.status.idle":"2021-08-05T00:55:26.823033Z","shell.execute_reply.started":"2021-08-05T00:55:26.758714Z","shell.execute_reply":"2021-08-05T00:55:26.822055Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Pay Homage to Titanic dataset kaggler\nMLA_columns = ['algorithm Name', 'algorithm Parameters','algorithm Train R2', 'algorithm Test R2','algorithm Test RMSPE','algorithm Time']\nMLA_compare = pd.DataFrame(columns = MLA_columns)\n\n#create table to compare MLA predictions\nMLA_predict = Y_train\n\n#index through MLA and save performance to table\nrow_index = 0\n\nscoring = {\n    'r2': 'r2',\n    'rmspe': rmspe_loss\n}\n\nfor alg in Regress_alg:\n\n    #set name and parameters\n    MLA_name = alg.__class__.__name__\n    MLA_compare.loc[row_index, 'algorithm Name'] = MLA_name\n    MLA_compare.loc[row_index, 'algorithm Parameters'] = str(alg.get_params())\n    \n    #score model with cross validation:\n    cv_results = cross_validate(alg, X_train, Y_train, return_train_score=True, scoring=scoring) # 5 fold \n    \n    MLA_compare.loc[row_index, 'algorithm Time'] = cv_results['fit_time'].mean()\n    MLA_compare.loc[row_index, 'algorithm Train R2'] = cv_results['train_r2'].mean()\n    MLA_compare.loc[row_index, 'algorithm Test R2'] = cv_results['test_r2'].mean()\n    MLA_compare.loc[row_index, 'algorithm Test RMSPE'] = cv_results['test_rmspe'].mean()\n    \n    #save MLA predictions - see section 6 for usage\n#     alg.fit(X_train, Y_train)\n#     MLA_predict[MLA_name] = alg.predict(X_train)\n    \n    row_index+=1\n\nMLA_compare.sort_values(by = ['algorithm Test RMSPE'], ascending = False, inplace = True)\nMLA_compare","metadata":{"execution":{"iopub.status.busy":"2021-08-05T00:55:27.944445Z","iopub.execute_input":"2021-08-05T00:55:27.944784Z","iopub.status.idle":"2021-08-05T00:57:14.059622Z","shell.execute_reply.started":"2021-08-05T00:55:27.944755Z","shell.execute_reply":"2021-08-05T00:57:14.058563Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predicted = pd.Series()\n\nfor alg in Regress_alg:\n    model = alg.fit(X_train,Y_train)\n    predicted = pd.concat([predicted,pd.Series(model.predict(X_test),name=alg.__class__.__name__)],axis=1)\npredicted.drop(columns=[0],axis=1,inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-08-05T00:57:35.381416Z","iopub.execute_input":"2021-08-05T00:57:35.381783Z","iopub.status.idle":"2021-08-05T00:58:05.05296Z","shell.execute_reply.started":"2021-08-05T00:57:35.381752Z","shell.execute_reply":"2021-08-05T00:58:05.052021Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from plotly.subplots import make_subplots\nimport plotly.graph_objects as go\n\npredicted_graph = pd.concat([predicted,Y_test.reset_index(drop=True)],axis=1)\nfig = make_subplots(specs=[[{\"secondary_y\": True}]])\nx = [0,0.018]\ny = x\ndf = [x,y]\n\n\n\nfig1 = px.scatter(predicted_graph, x='GradientBoostingRegressor', y='target', opacity=0.65, trendline='ols', trendline_color_override='blue')\nfig2 = px.scatter(df,x=x,y=y,opacity=1.0,trendline='ols',trendline_color_override='red')\n\nfig = make_subplots()\n\nfig.add_trace((go.Scatter(x=fig1.data[0]['x'],y=fig1.data[0]['y'],name='data',mode='markers',opacity=0.65)))\nfig.add_trace((go.Scatter(x=fig1.data[1]['x'],y=fig1.data[1]['y'],name='linear fit to pred vs true',fill='tonexty')))\nfig.add_trace((go.Scatter(x=fig2.data[0]['x'],y=fig2.data[0]['y'],name='reference, 45 deg line',fill='tonexty')))","metadata":{"execution":{"iopub.status.busy":"2021-08-05T00:59:21.859203Z","iopub.execute_input":"2021-08-05T00:59:21.859559Z","iopub.status.idle":"2021-08-05T00:59:22.039463Z","shell.execute_reply.started":"2021-08-05T00:59:21.859528Z","shell.execute_reply":"2021-08-05T00:59:22.038424Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Artificial Neural Network Regression Mapping, Hyperparameter tuning, Visualizing Hyperparameter tuning with Tensorboard\n\nGoing to construct the CNN model architecture","metadata":{}},{"cell_type":"code","source":"%load_ext tensorboard","metadata":{"execution":{"iopub.status.busy":"2021-08-05T00:59:44.744901Z","iopub.execute_input":"2021-08-05T00:59:44.745306Z","iopub.status.idle":"2021-08-05T00:59:44.753472Z","shell.execute_reply.started":"2021-08-05T00:59:44.745276Z","shell.execute_reply":"2021-08-05T00:59:44.752654Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Artificial Neural Network mapping with tensorflow:\n\nimport tensorflow as tf # import tensorflow\n\nfrom tensorflow import feature_column\nfrom tensorboard.plugins.hparams import api as hp\n\n# Delete the previous logs\n!rm -rf ./logs/\n\nTODO:' explain why you are doing this---------------------------------------------------'\n\n# Hidden layer1 # of neurons\nHP_NUM_UNITS1 = hp.HParam('num_units_1', hp.Discrete([8,16,24,32,40])) \n# Hidden layer1 # of neurons\nHP_NUM_UNITS2 = hp.HParam('num_units_2', hp.Discrete([4,8,12,16,20,24]))\n# Optimizer model selection\nHP_OPTIMIZER = hp.HParam('optimizer', hp.Discrete(['adam', 'sgd','RMSprop']))\n\nMETRIC_MAPE = 'mean_absolute_percentage_error'\n\nwith tf.summary.create_file_writer('logs/hparam_tuning1').as_default():\n    hp.hparams_config(\n        hparams=[HP_NUM_UNITS1,HP_NUM_UNITS2 ,HP_OPTIMIZER],\n        metrics=[hp.Metric(METRIC_MAPE, display_name='MAPE')]\n    )\n\n\ndef Neural_Net_Train(hparams):\n    \n    X_initial = tf.keras.layers.Input(shape=[26])\n    H_Initial = tf.keras.layers.Dense(hparams[HP_NUM_UNITS1])(X_initial)\n    H_Initial = tf.keras.layers.BatchNormalization()(H_Initial)\n    H_Initial = tf.keras.layers.Activation('swish')(H_Initial)\n\n    H_Initial = tf.keras.layers.Dense(hparams[HP_NUM_UNITS2])(H_Initial)\n    H_Initial = tf.keras.layers.BatchNormalization()(H_Initial)\n    H_Initial = tf.keras.layers.Activation('swish')(H_Initial)\n\n    Y_Initial = tf.keras.layers.Dense(1)(H_Initial)\n\n    model_ANN = tf.keras.models.Model(X_initial,Y_Initial)\n    model_ANN.compile(optimizer=hparams[HP_OPTIMIZER], loss='mse', metrics=[tf.keras.metrics.MeanAbsolutePercentageError()])\n\n    model_ANN.fit(X_train, Y_train, epochs=100,verbose=0)\n    _, MAPE = model_ANN.evaluate(X_test, Y_test)\n    return MAPE\n\n\ndef run(run_dir, hparams):\n    with tf.summary.create_file_writer(run_dir).as_default():\n        hp.hparams(hparams)  # record the values used in this trial\n        MAPE = Neural_Net_Train(hparams)\n        tf.summary.scalar(METRIC_MAPE, MAPE, step=1)\n\n        \nsession_num = 0\n\nfor num_units1 in HP_NUM_UNITS1.domain.values:\n    for num_units2 in HP_NUM_UNITS2.domain.values:\n        for optimizer in HP_OPTIMIZER.domain.values:\n            hparams = {\n                HP_NUM_UNITS1: num_units1,\n                HP_NUM_UNITS2: num_units2,\n                HP_OPTIMIZER: optimizer\n            }\n            run_name = \"run-%d\" % session_num\n            print('--- Starting trial: %s' % run_name)\n            print({h.name: hparams[h] for h in hparams})\n            run('logs/hparam_tuning1/' + run_name, hparams)\n            session_num += 1","metadata":{"execution":{"iopub.status.busy":"2021-08-05T00:59:48.452176Z","iopub.execute_input":"2021-08-05T00:59:48.452519Z","iopub.status.idle":"2021-08-05T01:17:35.279709Z","shell.execute_reply.started":"2021-08-05T00:59:48.45249Z","shell.execute_reply":"2021-08-05T01:17:35.278817Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# I don't know why it is not working here, but if you try it in ur own jupyter notebbok, it works..\n%tensorboard --logdir logs/hparam_tuning1\n","metadata":{"execution":{"iopub.status.busy":"2021-08-05T01:24:44.296468Z","iopub.execute_input":"2021-08-05T01:24:44.29687Z","iopub.status.idle":"2021-08-05T01:24:44.310427Z","shell.execute_reply.started":"2021-08-05T01:24:44.296832Z","shell.execute_reply":"2021-08-05T01:24:44.309352Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rmspe_loop = []\n# rmspe_val = 1\n\n# Modelling after grid search\nX_initial = tf.keras.layers.Input(shape=[26])\nH_Initial = tf.keras.layers.Dense(27)(X_initial)\nH_Initial = tf.keras.layers.BatchNormalization()(H_Initial)\nH_Initial = tf.keras.layers.Activation('relu')(H_Initial)\n\nH_Initial = tf.keras.layers.Dense(4)(H_Initial)\nH_Initial = tf.keras.layers.BatchNormalization()(H_Initial)\nH_Initial = tf.keras.layers.Activation('relu')(H_Initial)\n\nY_Initial = tf.keras.layers.Dense(1)(H_Initial)\n\nmodel_ANN = tf.keras.models.Model(X_initial,Y_Initial)\n\nmodel_ANN = tf.keras.models.Model(X_initial,Y_Initial)\nmodel_ANN.compile(optimizer='adam', loss='mse')\n\n# while rmspe_val > 0.8:\nfor _ in range(0,200):\n\n    model_ANN.fit(X_train,Y_train,epochs=1,verbose=0)\n    ANN_prediction = model_ANN.predict(X_test)\n    rmspe_val = -rmspe(Y_test.to_numpy(),ANN_prediction)\n    rmspe_loop.append(rmspe_val)\n\nprint(min(rmspe_loop))\npx.line(rmspe_loop)","metadata":{"execution":{"iopub.status.busy":"2021-08-05T01:30:04.98974Z","iopub.execute_input":"2021-08-05T01:30:04.990167Z","iopub.status.idle":"2021-08-05T01:30:54.172883Z","shell.execute_reply.started":"2021-08-05T01:30:04.990133Z","shell.execute_reply":"2021-08-05T01:30:54.171888Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import r2_score\nprint(r2_score(Y_test,ANN_prediction))\nplot_prediction = [item for sublist in ANN_prediction for item in sublist]\nplot_data = pd.DataFrame([plot_prediction,Y_test])\nplot_data = plot_data.transpose()\nplot_data.rename(columns={0:'Predicted',1:'True_val'},inplace=True)\npx.line(plot_data, y=plot_data.columns)","metadata":{"execution":{"iopub.status.busy":"2021-08-05T01:31:14.901226Z","iopub.execute_input":"2021-08-05T01:31:14.90158Z","iopub.status.idle":"2021-08-05T01:31:15.043631Z","shell.execute_reply.started":"2021-08-05T01:31:14.901549Z","shell.execute_reply":"2021-08-05T01:31:15.042886Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So... Artificial Neural Network, even if going through grid search, there is a limitation. I am going to reconvene after the feature selection. Let us go on to grid search with the scikit Learn models","metadata":{}},{"cell_type":"code","source":"for alg in Regress_alg:\n    print(f'Predictors: {alg.__class__.__name__}, \\n\\nParameter list: {alg.get_params()}\\n')\n    ","metadata":{"execution":{"iopub.status.busy":"2021-08-05T01:31:27.835898Z","iopub.execute_input":"2021-08-05T01:31:27.836276Z","iopub.status.idle":"2021-08-05T01:31:27.851578Z","shell.execute_reply.started":"2021-08-05T01:31:27.836245Z","shell.execute_reply":"2021-08-05T01:31:27.850375Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So here, we have 23 ML regression algorithm that we can perform the grid search. However, each algorithm requires their own hyperparameter tuning and they have their own list. so, I am going to make a list of dictionaries which is a compilation of grid search parameters.\n(e.g. GridSearchCV for sklearn.linear_model.LinearRegression())","metadata":{}},{"cell_type":"code","source":"param_grid_LinearRegression = {\n    'fit_intercept': [True,False],\n    'normalize': [True,False], \n    'positive': [True,False]\n}\n\nparam_grid_RidgeCV = {\n    'alphas': [np.array([0.001,0.01]),np.array([0.1,1]),np.array([2,3]),np.array([4,5]),np.array([6,7]),np.array([8,9]),np.array([10,11])],\n    'cv': [3,4,5], \n    'fit_intercept': [True,False], \n    'gcv_mode': ['auto', 'svd', 'eigen'], \n    'normalize': [True,False], \n    'store_cv_values': [False]\n}\n\nparam_grid_LassoCV = {\n    'alphas': [None,np.array([0.001,0.01]),np.array([0.1,1]),np.array([2,3]),np.array([4,5]),np.array([6,7]),np.array([8,9]),np.array([10,11])], \n    'fit_intercept': [True,False], \n    'max_iter': [1000], \n    'n_alphas': [0,1,5,10,20],  \n    'normalize': [True,False], \n    'positive': [True,False], \n    'random_state': [42]\n}\n\nparam_grid_ElasticNet = {\n    'alpha': [1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 0.0, 1.0, 10.0, 100.0],\n    'fit_intercept': [True,False],\n    'l1_ratio': [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9],\n    'normalize': [True,False],\n    'positive': [True,False],\n    'precompute': [True,False],\n    'random_state': [12,42],\n    'tol': [0.0001,0.001], \n    'warm_start': [True,False]\n}\n\nparam_grid_OrthogonalMatchingPursuit = {\n    'fit_intercept': [True,False], \n    'n_nonzero_coefs': [0,1], \n    'tol': [0.0001,0.001,0.01]\n}\n\nparam_grid_BayesianRidge = {\n    'alpha_1': [1e-02,1e-01], \n    'alpha_2': [1e-02,1e-01], \n    'compute_score': [True,False],\n    'fit_intercept':[True,False],\n    'lambda_1': [1e-06,1e-01],\n    'lambda_2': [1e-06,1e-01],\n    'n_iter': [100,200], \n    'normalize': [True,False],\n    'tol': [0.0001,0.001]\n}\n\nparam_grid_GammaRegressor = {\n    'alpha': [0.01,0.1,1.0], \n    'fit_intercept': [True,False], \n    'max_iter': [100,200], \n    'tol': [0.0001,0.001,0.01], \n}\n\nparam_grid_PoissonRegressor = {\n    'alpha': [0.01,0.1,1.0,10,100], \n    'fit_intercept': [True,False], \n    'max_iter': [100,200,300], \n    'tol': [0.0001,0.001,0.01], \n    'warm_start': [True,False]\n}\n\nparam_grid_TweedieRegressor = {\n    'alpha': [0.01,0.1,1.0,10,100], \n    'fit_intercept': [True,False], \n    'max_iter': [100,200,300], \n    'alpha':[0,0.1,1],\n    'tol': [0.0001,0.001,0.01], \n    'warm_start': [True,False]\n}\n\nparam_grid_SGDRegressor = {\n    'alpha': [0.0001], \n    'average': [False],\n    'early_stopping': [False], \n    'epsilon': [0.01], \n    'eta0': [0.01,0.1],\n    'fit_intercept': [True], \n    'l1_ratio': [0.35], \n    'learning_rate': ['invscaling'], \n    'loss': ['huber'],\n    'penalty': ['l1'], \n    'max_iter': [3000],\n    'power_t': [0.35],\n    'shuffle': [True], \n    'validation_fraction': [0.1], \n    'warm_start': [True]\n}\n\n# param_grid_KernelRidge = { # Not working at all...\n#     'alpha': [1,10,100], \n#     'coef0': [0.01,0.001], \n#     'degree': [0.1,1], \n#     'gamma': [None], \n#     'kernel': ['linear', 'rbf','poly']\n# }\n\n# param_grid_SVR = { # Not working at all...\n#     'C': [0.001,1,1.5], \n#     'epsilon': [0.1,0.2,0.3],\n#     'gamma': [1e-4,0.01],\n#     'kernel': ['linear', 'rbf','poly']\n# }\n\nparam_grid_NuSVR = {\n    'C': [0.001,1,1.5], \n    'gamma': [1e-4,0.01],\n    'nu': [0.5,1],\n    'kernel': ['linear', 'rbf','poly']\n}\n\nparam_grid_LinearSVR = {\n    'C': [0.001], \n    'fit_intercept': [True],\n    'max_iter': [1000],\n    'random_state': [42]\n}\n\nparam_grid_KNeighborsRegressor = {\n    'algorithm': ['ball_tree', 'kd_tree', 'brute'], \n    'leaf_size': [1,10],  \n    'n_neighbors': [3,4,5], \n    'p': [1,2]\n}\n\nparam_grid_GaussianProcessRegressor = {\n    'alpha': [1e-10,1e-7,1e-4], \n    'normalize_y': [True,False], \n    'n_restarts_optimizer': [0,2], \n    'random_state': [12,42]\n}\n\nparam_grid_DecisionTreeRegressor = {\n    \"splitter\":[\"best\",\"random\"],\n    \"max_depth\" : [1,3,5,7,9,11,12],\n    \"min_samples_leaf\":[1,2,3,4,5,6,7,8,9,10],\n    \"min_weight_fraction_leaf\":[0.1,0.2,0.3,0.4],\n    \"max_features\":[\"auto\",\"log2\",\"sqrt\",None],\n    \"max_leaf_nodes\":[None,10,20,30,40,50,60,70,80,90]\n}\n\nparam_grid_AdaBoostRegressor = {\n    'base_estimator': [tree.DecisionTreeRegressor()],\n    'n_estimators':[1000,1500],\n    'learning_rate':[.0001,0.001,.01],\n    'random_state':[42]\n}\n\nparam_grid_BaggingRegressor = {\n    'bootstrap': [True], \n    'bootstrap_features': [False], \n    'max_features': [10],\n    'max_samples': [1.0], \n    'n_estimators': [100,200,1000], \n    'oob_score': [True], \n    'warm_start': [False]\n}\n\nparam_grid_ExtraTreesRegressor = {\n    'bootstrap': [True,False], # False\n    'ccp_alpha': [0.0,0.1,0.2], # 0.0\n    'max_depth': [None,1,10,100], # 10\n    'max_features': ['auto','sqrt', 'log2'], # 'auto'\n    'min_samples_leaf': [1,2,4,6,8,10], # 2\n    'min_samples_split': [2,4,6,8,10], # 2\n    'n_estimators': [10,100,1000], # 10\n    'warm_start': [True,False] # False\n}\n\nparam_grid_RandomForestRegressor = {\n    'bootstrap': [True,False], \n    'max_depth': [10, 20],\n    'max_features': ['auto', 'sqrt', 'log2'],\n    'min_samples_leaf': [1, 2],\n    'min_samples_split': [2, 5],\n    'n_estimators': [10,100]\n}\n\nparam_grid_HistGradientBoostingRegressor = {\n    'categorical_features': [None], # we got no categorical feature, no one-hot-encoding involved.\n    'early_stopping': [True,False], # True\n    'l2_regularization':[0.0,0.5,1.0], # 0.5\n    'learning_rate': [0.1,1], # 0.1\n    'loss': ['least_squares','least_absolute_deviation','poisson'], # poisson\n    'max_bins':[100, 255], # 100\n    'max_depth': [10, 100, None], # 100\n    'max_iter': [500], \n    'max_leaf_nodes': [31,None], # 31\n    'min_samples_leaf': [20,10], # 10\n    'validation_fraction': [0.1,0.2] # 0.2\n}\n\nparam_grid_GradientBoostingRegressor = {\n    'alpha': [0.1,0.5,0.9], # 0.9\n    'criterion': ['friedman_mse','mse'], # friedman\n    'learning_rate': [0.05,0.1,0.2], # 0.05\n    'loss': ['ls', 'huber', 'quantile'], # huber\n    'max_depth': [3,5], # 3\n    'max_features': ['sqrt'], # sqrt\n    'min_samples_leaf': [10,20],# 10\n    'min_samples_split': [20,30], # 20\n    'n_estimators': [150,160], # 150\n    'subsample': [1.0], # 1.0\n    'validation_fraction': [0.2] # 0.2\n}\nparam_grid_lists = [param_grid_LinearRegression, param_grid_RidgeCV, param_grid_LassoCV, param_grid_ElasticNet\n                   , param_grid_OrthogonalMatchingPursuit, param_grid_BayesianRidge, param_grid_GammaRegressor\n                   , param_grid_PoissonRegressor, param_grid_TweedieRegressor, param_grid_SGDRegressor\n                   , param_grid_NuSVR, param_grid_LinearSVR, param_grid_KNeighborsRegressor\n                   , param_grid_GaussianProcessRegressor, param_grid_DecisionTreeRegressor, param_grid_AdaBoostRegressor\n                   , param_grid_BaggingRegressor, param_grid_ExtraTreesRegressor, param_grid_RandomForestRegressor\n                   , param_grid_HistGradientBoostingRegressor, param_grid_GradientBoostingRegressor]","metadata":{"execution":{"iopub.status.busy":"2021-08-05T01:37:13.337717Z","iopub.execute_input":"2021-08-05T01:37:13.338221Z","iopub.status.idle":"2021-08-05T01:37:13.383554Z","shell.execute_reply.started":"2021-08-05T01:37:13.338179Z","shell.execute_reply":"2021-08-05T01:37:13.382459Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Import the grid search class from the scikit learn model selection module\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn import metrics\n# Linear Regression estimator has following parameters\n\n# Parameter List: {\n#     'copy_X': True, \n#     'fit_intercept': True, \n#     'n_jobs': None, \n#     'normalize': False, \n#     'positive': False\n# }\n\n# Define parameter lists for grid searching. this will be defined as dictionaries.\n\n# I suggest you run 3 groups at a time to avoid any kernel death, and overloading of the computers\n\ntuned_params = pd.DataFrame(columns = ['name','pre tuned RMSPE','post tuned RMSPE','params'])\nparams = []\nx=1\nfor i , alg in enumerate(Regress_alg[x:x+3]):\n    grid = GridSearchCV(estimator=alg, param_grid=param_grid_lists[i+x], cv=5, n_jobs = -1, scoring = scoring, refit = 'rmspe')\n    grid_result = grid.fit(X_train, Y_train)\n    tuned_params.loc[i,'name'] = alg.__class__.__name__\n    tuned_params.loc[i,'pre tuned RMSPE'] = MLA_compare['algorithm Test RMSPE'].loc[MLA_compare['algorithm Name'] == str(alg.__class__.__name__)].values[0]\n    tuned_params.loc[i,'post tuned RMSPE'] = grid_result.best_score_*-1\n    tuned_params.loc[i,'params'] = str(grid_result.best_params_)\n    params.append(grid_result.best_params_)\n\ntuned_params","metadata":{"execution":{"iopub.status.busy":"2021-08-05T01:37:14.863868Z","iopub.execute_input":"2021-08-05T01:37:14.864317Z","iopub.status.idle":"2021-08-05T01:43:01.406492Z","shell.execute_reply.started":"2021-08-05T01:37:14.86428Z","shell.execute_reply":"2021-08-05T01:43:01.405184Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in range(len(tuned_params)):\n    print(tuned_params.loc[i,'params'],'\\n\\n Vs \\n')\n    print(MLA_compare['algorithm Parameters'].loc[MLA_compare['algorithm Name'] == tuned_params.loc[i,'name']][i+x],'\\n')\n    \n# We will update our model's parameter as per tuned parameters..\n\nRegress_alg_tuned = [\n    # Linear Models\n    linear_model.LinearRegression(fit_intercept= True, normalize= True),\n    linear_model.RidgeCV(alphas = np.array([0.001, 0.01 ]), cv = 5, fit_intercept = True, gcv_mode = 'auto', normalize= False, store_cv_values = False),\n    linear_model.LassoCV(alphas = None, fit_intercept= True, max_iter= 1000, n_alphas= 1, normalize= False, positive= False, random_state= 42), \n    linear_model.ElasticNet(alpha= 1e-05, fit_intercept= True, l1_ratio= 0.7, normalize= False, positive= False, precompute= False, random_state= 12, tol= 0.001, warm_start= True), \n    linear_model.OrthogonalMatchingPursuit(fit_intercept= True, n_nonzero_coefs= 0, tol= 0.0001),\n    linear_model.BayesianRidge(alpha_1= 0.1, alpha_2= 0.01, compute_score= True, fit_intercept= True, lambda_1= 1e-06, lambda_2= 0.1, n_iter= 100, normalize= False, tol= 0.0001),\n    linear_model.GammaRegressor(alpha= 1.0, fit_intercept= True, max_iter= 100, tol= 0.0001),\n    linear_model.PoissonRegressor(alpha= 0.01, fit_intercept= True, max_iter= 100, tol= 0.0001, warm_start= True),\n    linear_model.TweedieRegressor(alpha= 0, fit_intercept= True, max_iter= 100, tol= 0.0001, warm_start= True),\n    # Stochastic Gradient Descent (SGD)\n    linear_model.SGDRegressor(alpha= 0.0001, average= False, early_stopping= False, epsilon= 0.01, eta0= 0.1, fit_intercept= True, l1_ratio= 0.35, learning_rate= 'invscaling', loss= 'huber', max_iter= 3000, penalty= 'l1', power_t= 0.35, shuffle= True, validation_fraction= 0.1, warm_start= True),\n    \n    # Kernel Ridge Regression\n#     kernel_ridge.KernelRidge(), # alpha=1, kernel='linear', gamma=None, degree=3, coef0=1, kernel_params=None\n    \n    # Support Vector Machine - Regressions\n#     svm.SVR(), # kernel='rbf', degree=3, gamma='scale', coef0=0.0, tol=0.001, C=1.0, epsilon=0.1, shrinking=True, cache_size=200, verbose=False, max_iter=- 1\n    svm.NuSVR(C= 1.5, gamma= 0.0001, kernel= 'rbf', nu= 1), \n    svm.LinearSVR(C= 0.001, fit_intercept= True, max_iter= 1000, random_state= 42), \n\n    # Nearest Neighbours Regression\n    neighbors.KNeighborsRegressor(algorithm= 'ball_tree', leaf_size= 1, n_neighbors= 5, p= 1), \n#     neighbors.RadiusNeighborsRegressor(), # radius=1.0, weights='uniform', algorithm='auto', leaf_size=30, p=2, metric='minkowski', metric_params=None, n_jobs=None\n    \n    # Gaussian Process Regression (GPR)\n    gaussian_process.GaussianProcessRegressor(alpha= 1e-10, n_restarts_optimizer= 0, normalize_y= True, random_state= 12),\n    \n    # PLS Regression\n#     cross_decomposition.PLSRegression(), # n_components=2, scale=True, max_iter=500, tol=1e-06, copy=True\n    \n    # Decision Tree\n    tree.DecisionTreeRegressor(max_depth= 7, max_features= 'auto', max_leaf_nodes= 80, min_samples_leaf= 8, min_weight_fraction_leaf= 0.1, splitter= 'best'), \n    # Ensemble methods\n    ensemble.AdaBoostRegressor(base_estimator= tree.DecisionTreeRegressor(), learning_rate= 0.0001, n_estimators= 1500, random_state= 42),\n    ensemble.BaggingRegressor(bootstrap= True, bootstrap_features= False, max_features= 10, max_samples= 1.0, n_estimators= 200, oob_score= True, warm_start= False), \n    ensemble.ExtraTreesRegressor(bootstrap= False, ccp_alpha= 0.0, max_depth= 10, max_features= 'auto', min_samples_leaf= 4, min_samples_split= 6, n_estimators= 100, warm_start= True), \n    ensemble.RandomForestRegressor(bootstrap= True, max_depth= 10, max_features= 'sqrt', min_samples_leaf= 2, min_samples_split= 5, n_estimators= 100),\n    ensemble.HistGradientBoostingRegressor(early_stopping=True,l2_regularization=0.5,learning_rate=0.1,loss='poisson',max_bins=100,max_depth=100,max_iter=500,max_leaf_nodes=31,min_samples_leaf=10,validation_fraction=0.2), \n    ensemble.GradientBoostingRegressor(alpha=0.9,criterion='friedman_mse',learning_rate=0.05,loss='huber',max_depth=3,max_features='sqrt',min_samples_leaf=10,min_samples_split=20,n_estimators=150,subsample=1.0,validation_fraction=0.2),\n\n    # Neural Network, multilayer perceptron\n#     neural_network.MLPRegressor(), # hidden_layer_sizes=100, activation='relu', solver='adam', alpha=0.0001, batch_size='auto', learning_rate='constant', learning_rate_init=0.001, power_t=0.5, max_iter=200, shuffle=True, random_state=None, tol=0.0001, verbose=False, warm_start=False, momentum=0.9, nesterovs_momentum=True, early_stopping=False, validation_fraction=0.1, beta_1=0.9, beta_2=0.999, epsilon=1e-08, n_iter_no_change=10, max_fun=15000\n    xgboost.XGBRegressor()\n]","metadata":{"execution":{"iopub.status.busy":"2021-08-05T01:44:03.060944Z","iopub.execute_input":"2021-08-05T01:44:03.061355Z","iopub.status.idle":"2021-08-05T01:44:03.09463Z","shell.execute_reply.started":"2021-08-05T01:44:03.06132Z","shell.execute_reply":"2021-08-05T01:44:03.093273Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Pay Homage to Titanic dataset kaggler\nMLA_columns_tuned = ['algorithm Name', 'algorithm Parameters_tuned','algorithm Train R2_tuned', 'algorithm Test R2_tuned','algorithm Test RMSPE_tuned','algorithm Time_tuned']\nMLA_compare_tuned = pd.DataFrame(columns = MLA_columns_tuned)\n\n#create table to compare MLA predictions\nMLA_predict_tuned = Y_train\n\n#index through MLA and save performance to table\nrow_index = 0\n\nscoring = {\n    'r2': 'r2',\n    'rmspe': rmspe_loss\n}\n\nfor alg in Regress_alg_tuned:\n\n    #set name and parameters\n    MLA_name_tuned = alg.__class__.__name__\n    MLA_compare_tuned.loc[row_index, 'algorithm Name'] = MLA_name_tuned\n    MLA_compare_tuned.loc[row_index, 'algorithm Parameters_tuned'] = str(alg.get_params())\n    \n    #score model with cross validation:\n    cv_results = cross_validate(alg, X_train, Y_train, return_train_score=True, scoring=scoring) # 5 fold \n    \n    MLA_compare_tuned.loc[row_index, 'algorithm Time_tuned'] = cv_results['fit_time'].mean()\n    MLA_compare_tuned.loc[row_index, 'algorithm Train R2_tuned'] = cv_results['train_r2'].mean()\n    MLA_compare_tuned.loc[row_index, 'algorithm Test R2_tuned'] = cv_results['test_r2'].mean()\n    MLA_compare_tuned.loc[row_index, 'algorithm Test RMSPE_tuned'] = cv_results['test_rmspe'].mean()*-1\n\n    MLA_compare_tuned.loc[row_index, 'RMSPE_pretuned_result'] = -MLA_compare['algorithm Test RMSPE'].loc[MLA_compare['algorithm Name'] == MLA_name_tuned][row_index]\n    \n    #save MLA predictions - see section 6 for usage\n#     alg.fit(X_train, Y_train)\n#     MLA_predict[MLA_name] = alg.predict(X_train)\n    \n    row_index+=1\n\nMLA_compare_tuned.sort_values(by = ['algorithm Test RMSPE_tuned'], ascending = True, inplace = True)\nMLA_compare_tuned","metadata":{"execution":{"iopub.status.busy":"2021-08-05T01:44:10.282944Z","iopub.execute_input":"2021-08-05T01:44:10.283506Z","iopub.status.idle":"2021-08-05T01:54:10.852222Z","shell.execute_reply.started":"2021-08-05T01:44:10.283453Z","shell.execute_reply":"2021-08-05T01:54:10.851085Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"a,b = [],[]\nfor i in Regress_alg_tuned:\n    a.append(\"Tuned\")\n    b.append(\"Not_Tuned\")\n    \na = pd.concat([MLA_compare_tuned[['algorithm Name','algorithm Test RMSPE_tuned']],pd.Series(a,name='Tuned/not_Tuned')],axis=1)\na.rename(columns={'algorithm Test RMSPE_tuned':'RMSPE'},inplace=True)\nb = pd.concat([MLA_compare_tuned[['algorithm Name','RMSPE_pretuned_result']],pd.Series(b,name='Tuned/not_Tuned')],axis=1)\nb.rename(columns={'RMSPE_pretuned_result':'RMSPE'},inplace=True)\ngraph = a.append(b)\npx.bar(graph, x=\"RMSPE\", y=\"algorithm Name\",color='Tuned/not_Tuned', orientation='h').update_layout(barmode='group')","metadata":{"execution":{"iopub.status.busy":"2021-08-05T01:55:02.009178Z","iopub.execute_input":"2021-08-05T01:55:02.009821Z","iopub.status.idle":"2021-08-05T01:55:02.105881Z","shell.execute_reply.started":"2021-08-05T01:55:02.009766Z","shell.execute_reply":"2021-08-05T01:55:02.105067Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Marginal Increase... except for the few algorithms...\n\nHow about I stack them? - Work in Progress\n\nlets pick one and submit","metadata":{}},{"cell_type":"code","source":"df_val.fillna(0,inplace=True)\nans = test\nfor alg in Regress_alg_tuned:\n    model = alg.fit(X_train,Y_train)\n    predicted = model.predict(df_val)\n    predicted = pd.DataFrame(predicted,columns=['predicted '+str(alg.__class__.__name__)])\n    ans = pd.concat([ans,predicted],axis=1)\nans = ans.transpose()\nans","metadata":{"execution":{"iopub.status.busy":"2021-08-05T01:55:19.1989Z","iopub.execute_input":"2021-08-05T01:55:19.199743Z","iopub.status.idle":"2021-08-05T01:57:55.119655Z","shell.execute_reply.started":"2021-08-05T01:55:19.1997Z","shell.execute_reply":"2021-08-05T01:57:55.118455Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import ipywidgets as widgets\n\n# Create the list of all labels for the drop down list\nlist_of_labels = X_train.columns.to_list()\n\n# Create a list of tuples so that the index of the label is what is returned\ntuple_of_labels = list(zip(list_of_labels, range(len(list_of_labels))))\n\n# Create a widget for the labels and then display the widget\ncurrent_label = widgets.Dropdown(options=tuple_of_labels,\n                              value=0,\n                              description='Select Label:'\n                              )\n\n# Display the dropdown list (Note: access index value with 'current_label.value')\ncurrent_label\n\nexplainer = shap.KernelExplainer(model = Regress_alg_tuned[18].predict, data = X_train.head(200), link = \"identity\")\nshap_values = explainer.shap_values(X = X_train.iloc[0:200,:], nsamples = 300)\nshap.initjs()\n\nprint(f'Current Label Shown: {list_of_labels[current_label.value]}\\n')\n\n\nshap.summary_plot(shap_values = shap_values[0:200,:],\n                  features = X_train.iloc[0:200,:]\n                  )","metadata":{"execution":{"iopub.status.busy":"2021-08-05T02:51:57.200978Z","iopub.execute_input":"2021-08-05T02:51:57.201426Z","iopub.status.idle":"2021-08-05T02:55:16.456749Z","shell.execute_reply.started":"2021-08-05T02:51:57.201387Z","shell.execute_reply":"2021-08-05T02:55:16.45581Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Its quite interesting to see that the directionality of each variable impact on the final prediction... hmm.. intereting.\n\nAlso, this result was only using with with the grouped (aggregated) at stock_id = 0, and not considering the data when stock_id != 0. Also, this plot and result is only produced using LinearSVR, but I see that a lot of people are using gradient boosting methods and getting way better result than I do.\n\nI will post another version that uses pyspark","metadata":{"execution":{"iopub.execute_input":"2021-07-25T05:19:42.798693Z","iopub.status.busy":"2021-07-25T05:19:42.798041Z","iopub.status.idle":"2021-07-25T05:19:42.805093Z","shell.execute_reply":"2021-07-25T05:19:42.803867Z","shell.execute_reply.started":"2021-07-25T05:19:42.798654Z"}}},{"cell_type":"code","source":"ans[['row_id','predicted GradientBoostingRegressor']].to_csv('submission.csv',index = False)","metadata":{"execution":{"iopub.status.busy":"2021-08-05T02:57:14.336678Z","iopub.execute_input":"2021-08-05T02:57:14.337112Z","iopub.status.idle":"2021-08-05T02:57:14.351618Z","shell.execute_reply.started":"2021-08-05T02:57:14.337075Z","shell.execute_reply":"2021-08-05T02:57:14.35033Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}