{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# References\n+ https://www.kaggle.com/tommy1028/lightgbm-starter-with-feature-engineering-idea\n+ https://www.kaggle.com/monolith0456/2xlgbm-fnn-ensemble","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport os\nfrom glob import glob\nfrom category_encoders.cat_boost import CatBoostEncoder\nfrom joblib import Parallel, delayed\nimport time","metadata":{"execution":{"iopub.status.busy":"2021-09-25T07:01:47.969498Z","iopub.execute_input":"2021-09-25T07:01:47.969892Z","iopub.status.idle":"2021-09-25T07:01:47.974179Z","shell.execute_reply.started":"2021-09-25T07:01:47.969864Z","shell.execute_reply":"2021-09-25T07:01:47.973658Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"start_time = time.time()\nis_debug = True\nshow_feat_imp = True\ndevice='cpu'\n    \ninput_dir = '../input/optiver-realized-volatility-prediction/'\nagg_feat_list = ['trade_price_mean',\n                 'trade_seconds_in_bucket_count_unique']","metadata":{"execution":{"iopub.status.busy":"2021-09-25T07:01:47.975508Z","iopub.execute_input":"2021-09-25T07:01:47.975881Z","iopub.status.idle":"2021-09-25T07:01:47.992744Z","shell.execute_reply.started":"2021-09-25T07:01:47.975854Z","shell.execute_reply":"2021-09-25T07:01:47.992143Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Preprocess","metadata":{}},{"cell_type":"code","source":"def down_cast(df):\n    f_cols = df.select_dtypes('float').columns\n    i_cols = df.select_dtypes('int').columns\n    df[f_cols] = df[f_cols].apply(pd.to_numeric, downcast='float')\n    df[i_cols] = df[i_cols].apply(pd.to_numeric, downcast='integer')\n    \n    return df","metadata":{"execution":{"iopub.status.busy":"2021-09-25T07:01:47.993963Z","iopub.execute_input":"2021-09-25T07:01:47.994374Z","iopub.status.idle":"2021-09-25T07:01:48.004495Z","shell.execute_reply.started":"2021-09-25T07:01:47.994332Z","shell.execute_reply":"2021-09-25T07:01:48.003384Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Feature Engineering Functions","metadata":{}},{"cell_type":"code","source":"# for book data\ndef calc_wap1(df):\n    wap = (df['bid_price1'] * df['ask_size1'] + df['ask_price1'] * df['bid_size1']) / (df['bid_size1'] + df['ask_size1'])\n    return wap\n\ndef bid_size_sum(df):\n    return df['bid_size1'] + df['bid_size2']\n    \ndef ask_size_sum(df):\n    return df['ask_size1'] + df['ask_size2']\n\ndef diff_bid_ask_price(df):\n    return df['bid_price1'] + df['bid_price2'] - df['ask_price1'] - df['ask_price2']\n\ndef diff_bid_ask_size(df):\n    return df['bid_size1'] + df['bid_size2'] - df['ask_size1'] - df['ask_size2']\n\ndef diff_bid_price(df):\n    return df['bid_price1'] - df['bid_price2']\n\ndef diff_ask_price(df):\n    return df['ask_price1'] - df['ask_price2']\n\ndef diff_bid_size(df):\n    return df['bid_size1'] - df['bid_size2']\n\ndef diff_ask_size(df):\n    return df['ask_size1'] - df['ask_size2']\n\n# for trade data\ndef amount(df):\n    return df['price'] * df['size']\n\ndef count_unique(series):\n    return len(np.unique(series))\n\n# common \ndef log_return(series):\n    return np.log(series).diff()\n\ndef realized_volatility(series):\n    return np.sqrt(np.sum(log_return(series)**2))\n\n\ndef get_stats_window(df, fe_dict, seconds_in_bucket, add_suffix=False):\n    # Group by the window\n    df_feature = df[df['seconds_in_bucket'] >= seconds_in_bucket].groupby(['time_id']).agg(fe_dict).reset_index()\n    # Rename columns joining suffix\n    df_feature.columns = ['_'.join(col) for col in df_feature.columns]\n    df_feature = df_feature.rename(columns={'time_id_': 'time_id'})\n    # Add a suffix to differentiate windows\n    if add_suffix:\n        df_feature = df_feature.add_suffix('_' + str(seconds_in_bucket))\n    return df_feature\n\ndef agg_feat_groupby_stock_id(df, agg_feat_list):\n    df_feature = df.groupby(['stock_id'])[agg_feat_list].agg(['mean', 'std']).reset_index()\n    df_feature.columns = ['_'.join(col) for col in df_feature.columns]\n    df_feature = df_feature.add_suffix('_' + 'stock')\n    return df_feature.rename(columns={'stock_id__stock': 'stock_id'})\n\ndef agg_feat_groupby_time_id(df, agg_feat_list):\n    df_feature = df.groupby(['time_id'])[agg_feat_list].agg(['mean',  'std']).reset_index()\n    df_feature.columns = ['_'.join(col) for col in df_feature.columns]\n    df_feature = df_feature.add_suffix('_' + 'time')\n    return df_feature.rename(columns={'time_id__time': 'time_id'})\n\ndef calc_tau(df):\n    df['size_tau'] = np.sqrt(1 / df['trade_seconds_in_bucket_count_unique'])\n    df['size_tau_400'] = np.sqrt(1 / df['trade_seconds_in_bucket_count_unique_400'])\n    df['size_tau_200'] = np.sqrt(1 / df['trade_seconds_in_bucket_count_unique_200'])\n\n    # delta tau\n    df['size_tau_d'] = df['size_tau_400'] - df['size_tau']\n\n    df['size_tau2'] = np.sqrt(1 / df['trade_order_count_sum'])\n    df['size_tau2_400'] = np.sqrt(0.33 / df['trade_order_count_sum'])\n    df['size_tau2_200'] = np.sqrt(0.66 / df['trade_order_count_sum'])\n\n    # delta tau\n    df['size_tau2_d'] = df['size_tau2_400'] - df['size_tau2']\n    \n    return df","metadata":{"execution":{"iopub.status.busy":"2021-09-25T07:01:48.006812Z","iopub.execute_input":"2021-09-25T07:01:48.007075Z","iopub.status.idle":"2021-09-25T07:01:48.029854Z","shell.execute_reply.started":"2021-09-25T07:01:48.007045Z","shell.execute_reply":"2021-09-25T07:01:48.02901Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Preprocess of Book Data","metadata":{}},{"cell_type":"code","source":"def preprocess_book(file_path):\n    df = pd.read_parquet(file_path)\n    # make new columns\n    df['wap1'] = calc_wap1(df)\n    df['bid_size_sum'] = bid_size_sum(df)\n    df['ask_size_sum'] = ask_size_sum(df)\n    df['diff_bid_ask_price'] = diff_bid_ask_price(df)\n    df['diff_bid_ask_size'] = diff_bid_ask_size(df)\n    df['diff_bid_price'] = diff_bid_price(df)\n    df['diff_ask_price'] = diff_ask_price(df)\n    df['diff_bid_size'] = diff_bid_size(df)\n    df['diff_ask_size'] = diff_ask_size(df)\n    \n    # log returns\n    lr_cols = ['wap1',\n               'bid_price1',\n               'ask_price2',\n               'bid_size1',\n               'ask_size2',\n               'bid_size_sum',\n               'ask_size_sum',\n               'diff_bid_ask_price',\n               'diff_bid_price',\n               'diff_ask_price'\n              ]\n    for col in lr_cols:\n        df[col + '_lr'] = df.groupby(['time_id'])[col].apply(log_return)\n    \n    create_feature_dict = {\n        'wap1': [realized_volatility],\n        'wap1_lr': [np.std, np.max, np.min],\n        'bid_price1': [np.std],\n        'bid_size1': [np.mean],\n        'bid_price2': [np.std],\n        'bid_size2': [np.mean],\n        'ask_price1': [np.std],\n        'ask_price2': [np.std],\n        'diff_bid_ask_price': [realized_volatility, np.mean, np.max, np.min],\n        'diff_bid_price': [np.mean, np.max, np.min],\n        'diff_ask_price': [np.max, np.min],\n        'bid_price1_lr': [np.std],\n        'ask_price2_lr': [np.std],\n        'bid_size_sum_lr': [np.mean],\n        'ask_size_sum_lr': [np.mean],\n        'diff_bid_price_lr': [np.mean, np.std, np.max, np.min],\n    }\n    # Get the stats for different windows\n    df_feature = get_stats_window(df, create_feature_dict,seconds_in_bucket=0, add_suffix=False)\n    df_feature_200 = get_stats_window(df, create_feature_dict, seconds_in_bucket=200, add_suffix=True)\n    df_feature_400 = get_stats_window(df, create_feature_dict, seconds_in_bucket=400, add_suffix=True)\n    \n    df_feature = df_feature.merge(df_feature_200, how='left', left_on='time_id', right_on='time_id_200')\n    df_feature = df_feature.merge(df_feature_400, how='left', left_on='time_id', right_on='time_id_400')\n    # Drop unnecesary time_ids\n    df_feature.drop(['time_id_200'], axis=1, inplace=True)\n    df_feature.drop(['time_id_400'], axis=1, inplace=True)\n    \n    # row_id\n    stock_id = file_path.split('=')[1]\n    df_feature['row_id'] = df_feature['time_id'].apply(lambda x: f'{stock_id}-{x}')\n    df_feature.drop(['time_id'], axis = 1, inplace = True)\n    \n    return df_feature","metadata":{"execution":{"iopub.status.busy":"2021-09-25T07:01:48.031365Z","iopub.execute_input":"2021-09-25T07:01:48.032149Z","iopub.status.idle":"2021-09-25T07:01:48.049276Z","shell.execute_reply.started":"2021-09-25T07:01:48.032105Z","shell.execute_reply":"2021-09-25T07:01:48.048657Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Preprocess of Trade Data","metadata":{}},{"cell_type":"code","source":"def preprocess_trade(file_path):\n    df = pd.read_parquet(file_path)\n    df['amount'] = amount(df)\n    \n    lr_cols = ['price',\n               'amount',\n               'order_count'\n              ]\n    for col in lr_cols:\n        df[col + '_lr'] = df.groupby(['time_id'])[col].apply(log_return)\n    \n    create_feature_dict = {\n        'price': [realized_volatility, np.mean, np.std],\n        'price_lr': [np.max, np.min, np.std],\n        'seconds_in_bucket': [count_unique],\n        'amount_lr': [np.mean, np.std],\n        'order_count': [np.sum]\n    }\n    \n    # Get the stats for different windows\n    df_feature = get_stats_window(df, create_feature_dict,seconds_in_bucket=0, add_suffix=False)\n    df_feature_200 = get_stats_window(df, create_feature_dict, seconds_in_bucket=200, add_suffix=True)\n    df_feature_400 = get_stats_window(df, create_feature_dict, seconds_in_bucket=400, add_suffix=True)\n    \n    df_feature = df_feature.merge(df_feature_200, how='left', left_on='time_id', right_on='time_id_200')\n    df_feature = df_feature.merge(df_feature_400, how='left', left_on='time_id', right_on='time_id_400')\n    # Drop unnecesary time_ids\n    df_feature.drop(['time_id_200'], axis=1, inplace=True)\n    df_feature.drop(['time_id_400'], axis=1, inplace=True)\n    \n    df_feature = df_feature.add_prefix('trade_')\n    stock_id = file_path.split('=')[1]\n    df_feature['row_id'] = df_feature['trade_time_id'].apply(lambda x: f'{stock_id}-{x}')\n    df_feature.drop(['trade_time_id'], axis=1, inplace=True)\n    \n    return df_feature","metadata":{"execution":{"iopub.status.busy":"2021-09-25T07:01:48.050383Z","iopub.execute_input":"2021-09-25T07:01:48.051055Z","iopub.status.idle":"2021-09-25T07:01:48.064023Z","shell.execute_reply.started":"2021-09-25T07:01:48.05101Z","shell.execute_reply":"2021-09-25T07:01:48.063452Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Preprocess of All","metadata":{}},{"cell_type":"code","source":"# Funtion to make preprocessing function in parallel (for each stock id)\ndef preprocessor(list_stock_ids, is_train=True, is_parallel=True):\n    # Parrallel for loop\n    def for_joblib(stock_id):\n        # Train\n        if is_train:\n            file_path_book = input_dir + \"book_train.parquet/stock_id=\" + str(stock_id)\n            file_path_trade = input_dir + \"trade_train.parquet/stock_id=\" + str(stock_id)\n        # Test\n        else:\n            file_path_book = input_dir + \"book_test.parquet/stock_id=\" + str(stock_id)\n            file_path_trade = input_dir + \"trade_test.parquet/stock_id=\" + str(stock_id)\n    \n        # Preprocess book and trade data and merge them\n        df_tmp = pd.merge(preprocess_book(file_path_book), preprocess_trade(file_path_trade), on='row_id', how='left')\n        \n        # Return the merged dataframe\n        return df_tmp\n\n    if is_parallel:\n        # Use parallel api to call paralle for loop\n        df = Parallel(n_jobs=-1, verbose=1)(delayed(for_joblib)(stock_id) for stock_id in list_stock_ids)\n        # Concatenate all the dataframes that return from Parallel\n        df = pd.concat(df, ignore_index = True)\n    else:\n        for stock_id in list_stock_ids:\n            if is_train:\n                file_path_book = input_dir + \"book_train.parquet/stock_id=\" + str(stock_id)\n                file_path_trade = input_dir + \"trade_train.parquet/stock_id=\" + str(stock_id)\n            else:\n                file_path_book = input_dir + \"book_test.parquet/stock_id=\" + str(stock_id)\n                file_path_trade = input_dir + \"trade_test.parquet/stock_id=\" + str(stock_id)\n        df = pd.merge(preprocess_book(file_path_book), preprocess_trade(file_path_trade), on='row_id', how='left')\n    df['stock_id'] = df['row_id'].apply(lambda x: int(str(x).split('-')[0]))\n    df['time_id'] = df['row_id'].apply(lambda x: int(str(x).split('-')[1]))\n\n    return df.drop('row_id', axis=1)","metadata":{"execution":{"iopub.status.busy":"2021-09-25T07:01:48.065074Z","iopub.execute_input":"2021-09-25T07:01:48.066038Z","iopub.status.idle":"2021-09-25T07:01:48.080282Z","shell.execute_reply.started":"2021-09-25T07:01:48.065994Z","shell.execute_reply":"2021-09-25T07:01:48.079484Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def extract_list_stock_ids(book_file_exp, trade_file_exp):\n    book_file_list = glob(book_file_exp)\n    trade_file_list = glob(trade_file_exp)\n    book_stock_ids = {int(file_path.split('stock_id=')[1]) for file_path in book_file_list}\n    trade_stock_ids = {int(file_path.split('stock_id=')[1]) for file_path in trade_file_list}\n\n    return list(book_stock_ids and trade_stock_ids)","metadata":{"execution":{"iopub.status.busy":"2021-09-25T07:01:48.081799Z","iopub.execute_input":"2021-09-25T07:01:48.082007Z","iopub.status.idle":"2021-09-25T07:01:48.095971Z","shell.execute_reply.started":"2021-09-25T07:01:48.081984Z","shell.execute_reply":"2021-09-25T07:01:48.095403Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"book_train_file_exp = input_dir + 'book_train.parquet/stock_id=*'\ntrade_train_file_exp = input_dir + 'trade_train.parquet/stock_id=*'\n\nlist_train_stock_ids = extract_list_stock_ids(book_train_file_exp, trade_train_file_exp)","metadata":{"execution":{"iopub.status.busy":"2021-09-25T07:01:48.097065Z","iopub.execute_input":"2021-09-25T07:01:48.097272Z","iopub.status.idle":"2021-09-25T07:01:48.115354Z","shell.execute_reply.started":"2021-09-25T07:01:48.097249Z","shell.execute_reply":"2021-09-25T07:01:48.114354Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if is_debug:\n    list_train_stock_ids = [0]\ndf_train_org = pd.read_csv(input_dir + 'train.csv') \ndf_train_feature = preprocessor(list_train_stock_ids, is_train=True)\ndf_train = df_train_org.merge(df_train_feature, on=['stock_id', 'time_id'], how='left')\n\n# aggregate features group by stock_id\ndf_stock_id_feat_train = agg_feat_groupby_stock_id(df_train, agg_feat_list)\ndf_train = df_train.merge(df_stock_id_feat_train, on='stock_id', how='left')\n\n# aggregate features group by time_id\ndf_time_id_feat_train = agg_feat_groupby_time_id(df_train, agg_feat_list)\ndf_train = df_train.merge(df_time_id_feat_train, on='time_id', how='left')","metadata":{"execution":{"iopub.status.busy":"2021-09-25T07:01:48.11845Z","iopub.execute_input":"2021-09-25T07:01:48.11876Z","iopub.status.idle":"2021-09-25T07:02:26.86928Z","shell.execute_reply.started":"2021-09-25T07:01:48.118722Z","shell.execute_reply":"2021-09-25T07:02:26.868247Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train = calc_tau(df_train)","metadata":{"execution":{"iopub.status.busy":"2021-09-25T07:02:26.870589Z","iopub.execute_input":"2021-09-25T07:02:26.870841Z","iopub.status.idle":"2021-09-25T07:02:26.905955Z","shell.execute_reply.started":"2021-09-25T07:02:26.870812Z","shell.execute_reply":"2021-09-25T07:02:26.905106Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# find too much corr combinations of feature\nif is_debug:\n    df_corr = df_train.drop(['stock_id', 'time_id', 'target'], axis=1).corr()\n    too_match_corr_list = []\n    threshold = 0.98\n    for col in df_corr:\n        for idx in df_corr.index:\n            if (df_corr.loc[idx, col] > threshold) and (col != idx) and ((idx, col) not in too_match_corr_list):\n                too_match_corr_list.append((col, idx))\n    print(too_match_corr_list)","metadata":{"execution":{"iopub.status.busy":"2021-09-25T07:02:26.907655Z","iopub.execute_input":"2021-09-25T07:02:26.907952Z","iopub.status.idle":"2021-09-25T07:02:30.664945Z","shell.execute_reply.started":"2021-09-25T07:02:26.907912Z","shell.execute_reply":"2021-09-25T07:02:30.66413Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Target Encoding","metadata":{}},{"cell_type":"code","source":"# Train data\ncb_encoder = CatBoostEncoder(cols=['stock_id'])\ncb_encoder.fit(df_train.drop('target', axis=1), df_train['target'])\ndf_train['stock_id_cbenc'] = cb_encoder.transform(df_train.drop('target', axis=1))['stock_id']\ndf_train = down_cast(df_train)","metadata":{"execution":{"iopub.status.busy":"2021-09-25T07:02:30.667278Z","iopub.execute_input":"2021-09-25T07:02:30.667971Z","iopub.status.idle":"2021-09-25T07:02:36.011171Z","shell.execute_reply.started":"2021-09-25T07:02:30.667935Z","shell.execute_reply":"2021-09-25T07:02:36.010474Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import KFold\nimport lightgbm as lgbm\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler","metadata":{"execution":{"iopub.status.busy":"2021-09-25T07:02:36.014318Z","iopub.execute_input":"2021-09-25T07:02:36.015079Z","iopub.status.idle":"2021-09-25T07:02:37.398334Z","shell.execute_reply.started":"2021-09-25T07:02:36.015047Z","shell.execute_reply":"2021-09-25T07:02:37.397765Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if is_debug:\n    d_tmp = (df_train.describe() > 1000000).any().reset_index()\n    d_tmp.columns = ['colname', 'too_large']\n    for i, row in d_tmp.iterrows():\n        if row['too_large']:\n            print(row['colname'])","metadata":{"execution":{"iopub.status.busy":"2021-09-25T07:02:37.399273Z","iopub.execute_input":"2021-09-25T07:02:37.399942Z","iopub.status.idle":"2021-09-25T07:02:38.20612Z","shell.execute_reply.started":"2021-09-25T07:02:37.399906Z","shell.execute_reply":"2021-09-25T07:02:38.205358Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Standard Scaler","metadata":{}},{"cell_type":"code","source":"# train\nscaler_dict = {}\nX = df_train.drop(['stock_id', 'time_id', 'target'], axis=1)\nX.fillna(0)\nfor col in X.columns:\n    if col != 'stock_id':\n        scaler = StandardScaler()\n        x = X[col].values.reshape(-1, 1)\n        scaler.fit(x)\n        X[col] = scaler.transform(x)\n        scaler_dict[col] = scaler\ny = df_train['target']","metadata":{"execution":{"iopub.status.busy":"2021-09-25T07:02:38.207286Z","iopub.execute_input":"2021-09-25T07:02:38.207545Z","iopub.status.idle":"2021-09-25T07:02:39.48098Z","shell.execute_reply.started":"2021-09-25T07:02:38.207517Z","shell.execute_reply":"2021-09-25T07:02:39.480011Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# loss functions\ndef rmspe(y_true, y_pred):\n    return  (np.sqrt(np.mean(np.square((y_true - y_pred) / y_true))))\n\ndef feval_RMSPE(preds, lgbm_train):\n    labels = lgbm_train.get_label()\n    return 'RMSPE', round(rmspe(y_true = labels, y_pred = preds), 5), False","metadata":{"execution":{"iopub.status.busy":"2021-09-25T07:02:39.482295Z","iopub.execute_input":"2021-09-25T07:02:39.482602Z","iopub.status.idle":"2021-09-25T07:02:39.488629Z","shell.execute_reply.started":"2021-09-25T07:02:39.482561Z","shell.execute_reply":"2021-09-25T07:02:39.48782Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# feature importance\ndef calc_model_importance(model, feature_names=None, importance_type='gain'):\n    importance_df = pd.DataFrame(model.feature_importance(importance_type=importance_type),\n                                 index=feature_names,\n                                 columns=['importance']).sort_values('importance')\n    return importance_df\n\n\ndef plot_importance(importance_df, title='',\n                    save_filepath=None, figsize=(16, 32)):\n    fig, ax = plt.subplots(figsize=figsize)\n    importance_df.plot.barh(ax=ax)\n    if title:\n        plt.title(title)\n    plt.tight_layout()\n    if save_filepath is None:\n        plt.show()\n    else:\n        plt.savefig(save_filepath)\n    plt.close()\n    \n    \ndef calc_mean_importance(importance_df_list):\n    mean_importance = np.mean(\n        np.array([df['importance'].values for df in importance_df_list]), axis=0)\n    mean_df = importance_df_list[0].copy()\n    mean_df['importance'] = mean_importance\n    return mean_df","metadata":{"execution":{"iopub.status.busy":"2021-09-25T07:02:39.489966Z","iopub.execute_input":"2021-09-25T07:02:39.490387Z","iopub.status.idle":"2021-09-25T07:02:39.503335Z","shell.execute_reply.started":"2021-09-25T07:02:39.490358Z","shell.execute_reply":"2021-09-25T07:02:39.502606Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# lgbm parms\nparams = {\n      \"objective\": \"rmse\", \n      \"metric\": \"rmse\", \n      \"boosting_type\": \"gbdt\",\n      'early_stopping_rounds': 50,\n      'learning_rate': 0.01,\n      'lambda_l1': 1,\n      'lambda_l2': 1,\n      'feature_fraction': 0.8,\n      'feature_fraction_bynode': 0.8,\n      'device': device\n  }\n# list of models\nmodels = []\n# validation score\nscores = 0.0\ngain_importance_list = []\nsplit_importance_list = []","metadata":{"execution":{"iopub.status.busy":"2021-09-25T07:02:39.504386Z","iopub.execute_input":"2021-09-25T07:02:39.504912Z","iopub.status.idle":"2021-09-25T07:02:39.514761Z","shell.execute_reply.started":"2021-09-25T07:02:39.504878Z","shell.execute_reply":"2021-09-25T07:02:39.51382Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train Model","metadata":{}},{"cell_type":"code","source":"n_splits = 5\nif is_debug:\n    num_boost_round = 100\nelse:\n    num_boost_round = 5000\n    \nkf = KFold(n_splits=n_splits, random_state=20210101, shuffle=True)\nfor fold, (trn_idx, val_idx) in enumerate(kf.split(X, y)):\n    print(f\"Fold : {fold + 1}\")\n    X_train, y_train = X.loc[trn_idx], y[trn_idx]\n    X_valid, y_valid = X.loc[val_idx], y[val_idx]\n    \n    # RMSPE weight\n    weights = 1 / np.square(y_train)\n    lgbm_train = lgbm.Dataset(X_train, y_train, weight=weights)\n\n    weights = 1 / np.square(y_valid)\n    lgbm_valid = lgbm.Dataset(X_valid, y_valid, reference=lgbm_train, weight=weights)\n    \n    # train model\n    model = lgbm.train(params=params,\n                      train_set=lgbm_train,\n                      valid_sets=[lgbm_train, lgbm_valid],\n                      num_boost_round=num_boost_round,         \n                      feval=feval_RMSPE,\n                      verbose_eval=100             \n                     )\n    \n    # validation \n    y_pred = model.predict(X_valid, num_iteration=model.best_iteration)\n\n    RMSPE = round(rmspe(y_true=y_valid, y_pred=y_pred), 3)\n    print(f'RMSPE: {RMSPE}')\n\n    scores += RMSPE / n_splits\n    models.append(model)\n    print(\"*\" * 100)\n    if show_feat_imp:    \n        feature_names = X_train.columns.values.tolist()\n        gain_importance_df = calc_model_importance(\n            model, feature_names=feature_names, importance_type='gain')\n        gain_importance_list.append(gain_importance_df)","metadata":{"execution":{"iopub.status.busy":"2021-09-25T07:02:39.51591Z","iopub.execute_input":"2021-09-25T07:02:39.516137Z","iopub.status.idle":"2021-09-25T07:03:09.596876Z","shell.execute_reply.started":"2021-09-25T07:02:39.516111Z","shell.execute_reply":"2021-09-25T07:03:09.596254Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if show_feat_imp:\n    mean_gain_df = calc_mean_importance(gain_importance_list)\n    plot_importance(mean_gain_df, title='Model feature importance by gain')\n    mean_gain_df = mean_gain_df.reset_index().rename(columns={'index': 'feature_names'})\n    mean_gain_df.to_csv('gain_importance_mean.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2021-09-25T07:03:09.598128Z","iopub.execute_input":"2021-09-25T07:03:09.598579Z","iopub.status.idle":"2021-09-25T07:03:12.77832Z","shell.execute_reply.started":"2021-09-25T07:03:09.598546Z","shell.execute_reply":"2021-09-25T07:03:12.777534Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Test Set","metadata":{}},{"cell_type":"code","source":"book_test_file_exp  = input_dir + 'book_test.parquet/stock_id=*'\ntrade_test_file_exp = input_dir + 'trade_test.parquet/stock_id=*'\nlist_test_stock_ids = extract_list_stock_ids(book_test_file_exp, trade_test_file_exp)","metadata":{"execution":{"iopub.status.busy":"2021-09-25T07:03:12.779285Z","iopub.execute_input":"2021-09-25T07:03:12.779498Z","iopub.status.idle":"2021-09-25T07:03:12.794297Z","shell.execute_reply.started":"2021-09-25T07:03:12.779474Z","shell.execute_reply":"2021-09-25T07:03:12.793689Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test_org = pd.read_csv(input_dir + 'test.csv') \ndf_test_feature = preprocessor(list_test_stock_ids, is_train=False)\n\ndf_test = df_test_org.merge(df_test_feature, on=['stock_id', 'time_id'], how='left')\n\n# aggregate groupby stock_id\ndf_stock_id_feat_test = agg_feat_groupby_stock_id(df_test, agg_feat_list)\ndf_test = df_test.merge(df_stock_id_feat_test, on='stock_id', how='left')\n\n# aggregate groupby time_id\ndf_time_id_feat_test = agg_feat_groupby_time_id(df_test, agg_feat_list)\ndf_test = df_test.merge(df_time_id_feat_test, on='time_id', how='left')\n\ndf_test = calc_tau(df_test)","metadata":{"execution":{"iopub.status.busy":"2021-09-25T07:03:12.795485Z","iopub.execute_input":"2021-09-25T07:03:12.795854Z","iopub.status.idle":"2021-09-25T07:03:13.355809Z","shell.execute_reply.started":"2021-09-25T07:03:12.795827Z","shell.execute_reply":"2021-09-25T07:03:13.355068Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test = df_test.drop('row_id', axis=1)\n# target encoding\ndf_test['stock_id_cbenc'] = cb_encoder.transform(df_test)['stock_id']\n\n# standard scaler\nX_test = df_test.drop(['stock_id', 'time_id'], axis=1)\nfor col in X_test.columns:\n    if col != 'stock_id':\n        x = X_test[col].values.reshape(-1, 1)\n        X_test[col] = scaler_dict[col].transform(x)\n\ndf_test = down_cast(df_test)","metadata":{"execution":{"iopub.status.busy":"2021-09-25T07:03:13.356744Z","iopub.execute_input":"2021-09-25T07:03:13.356935Z","iopub.status.idle":"2021-09-25T07:03:13.473946Z","shell.execute_reply.started":"2021-09-25T07:03:13.356911Z","shell.execute_reply":"2021-09-25T07:03:13.473154Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Prediction","metadata":{}},{"cell_type":"code","source":"# prediction with light gbm models\ntarget = np.zeros(len(X_test))\nfor model in models:\n    pred = model.predict(X_test[X.columns], num_iteration=model.best_iteration)\n    target += pred / len(models)\n    \ndf_test['row_id'] = df_test['stock_id'].astype(str) + '-' + df_test['time_id'].astype(str)\ndf_submission = df_test[['row_id']].assign(target=target)\ndf_submission.to_csv('submission.csv', index=False)\nend_time = time.time()\nprint(f'total time: {round(end_time - start_time)} seconds')","metadata":{"execution":{"iopub.status.busy":"2021-09-25T07:03:13.475109Z","iopub.execute_input":"2021-09-25T07:03:13.475323Z","iopub.status.idle":"2021-09-25T07:03:13.505722Z","shell.execute_reply.started":"2021-09-25T07:03:13.475298Z","shell.execute_reply":"2021-09-25T07:03:13.505049Z"},"trusted":true},"execution_count":null,"outputs":[]}]}