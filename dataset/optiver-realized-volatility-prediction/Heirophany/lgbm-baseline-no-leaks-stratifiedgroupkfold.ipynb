{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Some updates made to prior notebooks:\n\n* Removed validation leakage from agg functions on time_id. This had the effect of making many features redundant (that were previously thought to be important)\n* Stratified GroupKFold introduced. We only want new time id's in the validation sets (because this will be the case when forecasting) and it seems all stocks will be in the forecast so it is best that we learn sufficiently from every stock (hence the stratified part).\n* Introduced some new features (research on arxiv and Google Scholar). You will note many unused functions below for you to play around with. There are also unused functions to try out various other sampling techniques. Enjoy!","metadata":{}},{"cell_type":"code","source":"from IPython.core.display import display, HTML\n\nimport pandas as pd\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport glob\nimport os\nimport gc\n\nfrom joblib import Parallel, delayed\n\nfrom sklearn import preprocessing, model_selection\nfrom sklearn.preprocessing import MinMaxScaler,StandardScaler,LabelEncoder\nfrom sklearn.preprocessing import QuantileTransformer\nfrom sklearn.metrics import r2_score\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.compose import ColumnTransformer\n\nimport lightgbm as lgb\n\nimport matplotlib.pyplot as plt \nimport seaborn as sns\nimport numpy.matlib\nimport random\nfrom collections import Counter, defaultdict\nfrom tqdm import tqdm","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-09-21T05:42:25.612835Z","iopub.execute_input":"2021-09-21T05:42:25.613555Z","iopub.status.idle":"2021-09-21T05:42:51.378387Z","shell.execute_reply.started":"2021-09-21T05:42:25.613464Z","shell.execute_reply":"2021-09-21T05:42:51.377094Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def stratified_group_k_fold(X, y, groups, k, seed=None): # NO SEED? Only for stratified \n    \"\"\" https://www.kaggle.com/jakubwasikowski/stratified-group-k-fold-cross-validation \"\"\"\n    labels_num = np.max(y) + 1 # get max value in y and assign to variable. .max only works on single input array\n    y_counts_per_group = defaultdict(lambda: np.zeros(labels_num)) # make a defaultdict(key values autogenerated per zero packed in with generator) array full of zeroes with a length of the int/tuple passed\n    y_distr = Counter() # assign Counter object to variable for counting hashable objects\n    for label, g in zip(y, groups): #Whats label in y?\n        y_counts_per_group[g][label] += 1 #incrementing counts? What's it adding to the defaultdict?\n        y_distr[label] += 1 #How can you increment an object that counts hashable object????\n\n    y_counts_per_fold = defaultdict(lambda: np.zeros(labels_num)) #Another defaultdict set adding zeros equal to the length of labelsnum dd\n    groups_per_fold = defaultdict(set) #wtf your'e passing a set,declaration???? wtf, as a shape argument for defaultdict?\n\n    def eval_y_counts_per_fold(y_counts, fold):\n        y_counts_per_fold[fold] += y_counts #adds ycounts to the fold col? maybe i shoudl find out what incrementing a default dict does xD\n        std_per_label = []\n        for label in range(labels_num):\n            label_std = np.std([y_counts_per_fold[i][label] / y_distr[label] for i in range(k)])\n            std_per_label.append(label_std)\n        y_counts_per_fold[fold] -= y_counts\n        return np.mean(std_per_label)\n    \n    groups_and_y_counts = list(y_counts_per_group.items())\n    random.Random(seed).shuffle(groups_and_y_counts)\n\n    for g, y_counts in tqdm(sorted(groups_and_y_counts, key=lambda x: -np.std(x[1])), total=len(groups_and_y_counts)):\n        best_fold = None\n        min_eval = None\n        for i in range(k):\n            fold_eval = eval_y_counts_per_fold(y_counts, i)\n            if min_eval is None or fold_eval < min_eval:\n                min_eval = fold_eval\n                best_fold = i\n        y_counts_per_fold[best_fold] += y_counts\n        groups_per_fold[best_fold].add(g)\n\n    all_groups = set(groups)\n    for i in range(k):\n        train_groups = all_groups - groups_per_fold[i]\n        test_groups = groups_per_fold[i]\n\n        train_indices = [i for i, g in enumerate(groups) if g in train_groups]\n        test_indices = [i for i, g in enumerate(groups) if g in test_groups]\n\n        yield train_indices, test_indices","metadata":{"execution":{"iopub.status.busy":"2021-09-21T05:42:51.379791Z","iopub.execute_input":"2021-09-21T05:42:51.380059Z","iopub.status.idle":"2021-09-21T05:42:51.396012Z","shell.execute_reply.started":"2021-09-21T05:42:51.38003Z","shell.execute_reply":"2021-09-21T05:42:51.394636Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def read_train_test():\n    # Function to read our base train and test set\n    \n    train = pd.read_csv('../input/optiver-realized-volatility-prediction/train.csv')\n    test = pd.read_csv('../input/optiver-realized-volatility-prediction/test.csv')\n\n    # Create a key to merge with book and trade data\n    train['row_id'] = train['stock_id'].astype(str) + '-' + train['time_id'].astype(str)\n    test['row_id'] = test['stock_id'].astype(str) + '-' + test['time_id'].astype(str)\n    print(f'Our training set has {train.shape[0]} rows')\n    print(f'Our test set has {test.shape[0]} rows')\n    print(f'Our training set has {train.isna().sum().sum()} missing values')\n    print(f'Our test set has {test.isna().sum().sum()} missing values')\n    \n    return train, test\n\ntrain, test = read_train_test()","metadata":{"execution":{"iopub.status.busy":"2021-09-21T05:42:51.398508Z","iopub.execute_input":"2021-09-21T05:42:51.398878Z","iopub.status.idle":"2021-09-21T05:42:52.874469Z","shell.execute_reply.started":"2021-09-21T05:42:51.398841Z","shell.execute_reply":"2021-09-21T05:42:52.873349Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# data directory\ndata_dir = '../input/optiver-realized-volatility-prediction/'\n\n# Function to calculate first WAP\ndef calc_wap1(df):\n    wap = (df['bid_price1'] * df['ask_size1'] + df['ask_price1'] * df['bid_size1']) / (df['bid_size1'] + df['ask_size1'])\n    return wap\n\n# Function to calculate second WAP\ndef calc_wap2(df):\n    wap = (df['bid_price2'] * df['ask_size2'] + df['ask_price2'] * df['bid_size2']) / (df['bid_size2'] + df['ask_size2'])\n    return wap\n\n# Function to aggregate 1st and 2nd WAP\ndef calc_wap12(df):\n    var1 = df['bid_price1'] * df['ask_size1'] + df['ask_price1'] * df['bid_size1']\n    var2 = df['bid_price2'] * df['ask_size2'] + df['ask_price2'] * df['bid_size2']\n    den = df['bid_size1'] + df['ask_size1'] + df['bid_size2'] + df['ask_size2']\n    return (var1+var2) / den\n\ndef calc_wap3(df):\n    wap = (df['bid_price1'] * df['bid_size1'] + df['ask_price1'] * df['ask_size1']) / (df['bid_size1'] + df['ask_size1'])\n    return wap\n\ndef calc_wap34(df):\n    var1 = df['bid_price1'] * df['bid_size1'] + df['ask_price1'] * df['ask_size1']\n    var2 = df['bid_price2'] * df['bid_size2'] + df['ask_price2'] * df['ask_size2']\n    den = df['bid_size1'] + df['ask_size1'] + df['bid_size2'] + df['ask_size2']\n    return (var1+var2) / den\n\ndef calc_swap12(df):\n    return df['wap12'] - df['wap34']\n\ndef calc_tswap1(df):\n    return -df['swap1'].diff()\n\ndef calc_tswap12(df):\n    return -df['swap12'].diff()\n\ndef calc_wss12(df):\n    ask = (df['ask_price1'] * df['ask_size1'] + df['ask_price2'] * df['ask_size2'])/(df['ask_size1']+df['ask_size2'])\n    bid = (df['bid_price1'] * df['bid_size1'] + df['bid_price2'] * df['bid_size2'])/(df['bid_size1']+df['bid_size2'])\n    return (ask - bid) / df['midprice']\n\n# Calculate order book depth\ndef calc_depth(df):\n    depth = df['bid_price1'] * df['bid_size1'] + df['ask_price1'] * df['ask_size1'] + df['bid_price2'] * df[\n               'bid_size2'] + df['ask_price2'] * df['ask_size2']\n    return depth\n\n# Calculate order book slope\ndef calc_slope(df):\n    v0 = (df['bid_size1']+df['ask_size1'])/2\n    p0 = (df['bid_price1']+df['ask_price1'])/2\n    slope_bid = ((df['bid_size1']/v0)-1)/abs((df['bid_price1']/p0)-1)+(\n                (df['bid_size2']/df['bid_size1'])-1)/abs((df['bid_price2']/df['bid_price1'])-1)\n    slope_ask = ((df['ask_size1']/v0)-1)/abs((df['ask_price1']/p0)-1)+(\n                (df['ask_size2']/df['ask_size1'])-1)/abs((df['ask_price2']/df['ask_price1'])-1)\n    return (slope_bid+slope_ask)/2, abs(slope_bid-slope_ask)\n\n# Calculate order book dispersion\ndef calc_dispersion(df):\n    bspread = df['bid_price1'] - df['bid_price2']\n    aspread = df['ask_price2'] - df['ask_price1']\n    bmid = (df['bid_price1'] + df['ask_price1'])/2  - df['bid_price1']\n    bmid2 = (df['bid_price1'] + df['ask_price1'])/2  - df['bid_price2']\n    amid = df['ask_price1'] - (df['bid_price1'] + df['ask_price1'])/2\n    amid2 = df['ask_price2'] - (df['bid_price1'] + df['ask_price1'])/2\n    bdisp = (df['bid_size1']*bmid + df['bid_size2']*bspread)/(df['bid_size1']+df['bid_size2'])\n    bdisp2 = (df['bid_size1']*bmid + df['bid_size2']*bmid2)/(df['bid_size1']+df['bid_size2'])\n    adisp = (df['ask_size1']*amid + df['ask_size2']*aspread)/(df['ask_size1']+df['ask_size2'])      \n    adisp2 = (df['ask_size1']*amid + df['ask_size2']*amid2)/(df['ask_size1']+df['ask_size2'])\n    return (bdisp + adisp)/2, (bdisp2 + adisp2)/2\n\ndef calc_price_impact(df):\n    ask = (df['ask_price1'] * df['ask_size1'] + df['ask_price2'] * df['ask_size2'])/(df['ask_size1']+df['ask_size2'])\n    bid = (df['bid_price1'] * df['bid_size1'] + df['bid_price2'] * df['bid_size2'])/(df['bid_size1']+df['bid_size2'])\n    return (df['ask_price1'] - ask)/df['ask_price1'], (df['bid_price1'] - bid)/df['bid_price1']\n\n#  order flow imbalance\ndef calc_ofi(df):\n    a = df['bid_size1']*np.where(df['bid_price1'].diff()>=0,1,0)\n    b = df['bid_size1'].shift()*np.where(df['bid_price1'].diff()<=0,1,0)\n    c = df['ask_size1']*np.where(df['ask_price1'].diff()<=0,1,0)\n    d = df['ask_size1'].shift()*np.where(df['ask_price1'].diff()>=0,1,0)\n    return a - b - c + d\n\n# Turnover\ndef calc_tt1(df):\n    p1 = df['ask_price1'] * df['ask_size1'] + df['bid_price1'] * df['bid_size1']\n    p2 = df['ask_price2'] * df['ask_size2'] + df['bid_price2'] * df['bid_size2']      \n    return p2 - p1 \n\n# Function to calculate the log of the return\ndef log_return(series):\n    return np.log(series).diff()\n\ndef log_return_out(series):\n    ret = np.log(series).diff()\n    return remove_outliers(ret)\n\ndef remove_outliers(series):\n    cu = 6\n    ser_mean, ser_std = np.mean(series), np.std(series)\n    series = series.where(series<=(ser_mean + cu*ser_std), ser_mean)\n    series = series.where(series>=(ser_mean - cu*ser_std), ser_mean)\n    return series\n\ndef realized_volatility(series):\n    return np.sqrt(np.sum(series**2))\n    \ndef realized_volatility_downside(series):\n    return np.sqrt(np.sum(series[series<0]**2))\n\ndef realized_volatility_upside(series):\n    return np.sqrt(np.sum(series[series>0]**2))\n\n# realized bipower variation \ndef realized_bipowvar(series):\n    cnt = series.count()\n    if cnt<3:\n        return np.nan\n    else:\n        cons = (np.pi/2)*(cnt/(cnt-2))\n        return cons*np.nansum(np.abs(series)*np.abs(series.shift()))\n    \n# Calculate integrated quarticity\ndef realized_quarticity(series):\n    return (series.count()/3)*np.sum(series**4)\n\n# realized median variation \ndef realized_medianvar(series):\n    cnt = series.count()\n    if cnt<3:\n        return np.nan\n    else:\n        cons = (np.pi/(6-4*np.sqrt(3)+np.pi))*(cnt/(cnt-2))\n        return cons*np.nansum(np.median([np.abs(series),np.abs(series.shift()),np.abs(series.shift(2))], axis=0)**2)\n    \n# Calculate the realized absolute variatian\ndef realized_absvar(series):\n    return np.sqrt(np.pi/(2*series.count()))*np.sum(np.abs(series))\n\n# Calculate weighted volatility\ndef realized_vol_weighted(series):\n    return np.sqrt(np.sum(series**2)/series.count())\n\n# Calculate the realized skew\ndef realized_skew(series):\n    return np.sqrt(series.count())*np.sum(series**3)/(realized_volatility(series)**3)\n\n# Calculate the realized kurtosis\ndef realized_kurtosis(series):\n    return series.count()*np.sum(series**4)/(realized_volatility(series)**4)\n\n# Function to get group stats for different windows (seconds in bucket)\ndef get_stats_bins(df, feat_dict, bins, quantile=False):\n    # Group by the window\n    if bins==0:\n        df_feature = df.groupby('time_id').agg(feat_dict).reset_index()\n        # Rename columns joining suffix\n        df_feature.columns = ['_'.join(col) for col in df_feature.columns]\n        df_feature = df_feature.rename(columns={'time_id_': 'time_id'})\n    else:\n        if quantile:\n            df['sbins'] = pd.qcut(df.seconds_in_bucket, bins, labels=False)\n            q = 'q'\n        else:\n            df['sbins'] = pd.cut(df.seconds_in_bucket, bins, labels=False)\n            q = ''\n        df_feature = None\n        for i in range(bins):\n            df_feat = df.loc[df.sbins==i].groupby('time_id').agg(feat_dict).reset_index()\n            # Rename columns joining suffix\n            df_feat.columns = ['_'.join(col) for col in df_feat.columns]\n            # Add a suffix to differentiate bins\n            df_feat = df_feat.rename(columns={col: col+'_'+q+str(bins)+'bins_'+str(i) for col in df_feat.columns})\n            df_feat = df_feat.rename(columns={'time_id_'+'_'+q+str(bins)+'bins_'+str(i): 'time_id'})\n            if isinstance(df_feature, pd.DataFrame):\n                df_feature = pd.merge(df_feature, df_feat, how='left', on='time_id')\n            else:\n                df_feature = df_feat.copy()\n        df = df.drop('sbins', axis=1)\n    return df_feature\n\n# Function to get group stats for different windows (seconds in bucket)\ndef get_stats_window(df, feat_dict, window):\n    # Group by the window\n    if window==0:\n        df_feature = df.groupby('time_id').agg(feat_dict).reset_index()\n        # Rename columns joining suffix\n        df_feature.columns = ['_'.join(col) for col in df_feature.columns]\n        df_feature = df_feature.rename(columns={'time_id_': 'time_id'})\n    else:\n        df_feature = None\n        for w in range(window, 600, window):\n            df_feat = df.loc[df.seconds_in_bucket>=w].groupby('time_id').agg(feat_dict).reset_index()\n            # Rename columns joining suffix\n            df_feat.columns = ['_'.join(col) for col in df_feat.columns]\n            # Add a suffix to differentiate bins\n            df_feat = df_feat.rename(columns={col: col+'_'+str(window)+'win_'+str(w) for col in df_feat.columns})\n            df_feat = df_feat.rename(columns={'time_id_'+'_'+str(window)+'win_'+str(w): 'time_id'})\n            if isinstance(df_feature, pd.DataFrame):\n                df_feature = pd.merge(df_feature, df_feat, how='left', on='time_id')\n            else:\n                df_feature = df_feat.copy()\n    return df_feature\n\n# Function to sample from the window and average\ndef get_stats_sampled(df, feat_dict, numsamp, fraction, boot):\n    df_feature = None\n    for i in range(numsamp):\n        df_feat = df.groupby('time_id').sample(frac=fraction, random_state=i, replace=boot).reset_index(drop=True)\n        df_feat = df_feat.groupby('time_id').agg(feat_dict).reset_index()\n        df_feat.columns = ['_'.join(col) for col in df_feat.columns]\n        if isinstance(df_feature, pd.DataFrame):\n            df_feature += df_feat.values/numsamp   \n        else:\n            df_feature = df_feat.copy()/numsamp\n    df_feature = df_feature.rename(columns={col: col+'_sample_'+str(fraction)+'_'+str(numsamp) for col in df_feature.columns})\n    df_feature = df_feature.rename(columns={'time_id_'+'_sample_'+str(fraction)+'_'+str(numsamp): 'time_id'})\n    return df_feature","metadata":{"execution":{"iopub.status.busy":"2021-09-21T05:46:29.625129Z","iopub.execute_input":"2021-09-21T05:46:29.625687Z","iopub.status.idle":"2021-09-21T05:46:29.685642Z","shell.execute_reply.started":"2021-09-21T05:46:29.625644Z","shell.execute_reply":"2021-09-21T05:46:29.684946Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function to preprocess book data (for each stock id)\ndef book_preprocessor(file_path):\n    df = pd.read_parquet(file_path)\n    \n    #### Data Cleansing (if any needed)\n    \n    df = df.drop(df.loc[df.ask_price1 <= 0].index)\n    df = df.drop(df.loc[df.bid_price1 <= 0].index)\n    df = df.drop(df.loc[(df['ask_price1'] - df['bid_price1']) < 0].index)\n    df = df.groupby(['time_id','seconds_in_bucket']).mean().reset_index()\n    \n    ####\n    \n    # Calculate prices\n    df['wap1'] = calc_wap1(df)\n    df['wap2'] = calc_wap2(df)\n    \n    df['depth'] = calc_depth(df)\n    df['slope'], _ = calc_slope(df)\n\n    # Calculate log returns\n    df['log_return1'] = df.groupby('time_id')['wap1'].apply(log_return)\n    df['log_return2'] = df.groupby('time_id')['wap2'].apply(log_return)\n    \n    # Calculate spreads\n    df['price_spread'] = (df['ask_price1'] - df['bid_price1']) / ((df['ask_price1'] + df['bid_price1']) / 2)\n    df['dispersion'], _ = calc_dispersion(df)\n\n    # Dict for aggregations\n    create_feature_dict = {\n        'depth': [np.sum],\n        'slope': [np.sum],\n        'log_return1': [realized_volatility, realized_absvar],\n        'log_return2': [realized_volatility, realized_absvar],\n        'price_spread': [np.sum, np.nanmedian],\n    }\n\n    create_feature_dict_bins = {\n        'depth': [np.sum],\n        'slope': [np.sum],\n        'dispersion': [np.sum],\n        'log_return1': [realized_volatility, realized_absvar],\n        'log_return2': [realized_volatility, realized_absvar],\n        'price_spread': [np.sum],\n    }\n\n\n    # Get the stats for different windows\n    df_feature_0 = get_stats_bins(df, create_feature_dict, 0)\n    df_feature_w4 = get_stats_window(df, create_feature_dict_bins, 150)\n\n    # Merge all\n    df_feature_0 = df_feature_0.merge(df_feature_w4, how = 'left', on = 'time_id')   \n\n    df_feature_0 = df_feature_0.add_prefix('book_')\n    stock_id = file_path.split('=')[1]\n    df_feature_0['row_id'] = df_feature_0['book_time_id'].apply(lambda x:f'{stock_id}-{x}')\n    df_feature_0.drop(['book_time_id'], axis = 1, inplace = True)\n    return df_feature_0\n\n# Function to preprocess trade data (for each stock id)\ndef trade_preprocessor(file_path):\n    df = pd.read_parquet(file_path)\n\n    #### Data Cleansing\n    \n    df = df.drop(df.loc[df.price <= 0].index)\n    df = df.groupby(['time_id','seconds_in_bucket']).mean().reset_index()\n    \n    ####\n    \n    df['log_return'] = df.groupby('time_id')['price'].apply(log_return)\n    \n    # Dict for aggregations\n    create_feature_dict = {\n        'log_return': [realized_volatility, realized_absvar],\n        'order_count':[np.sum, np.max],\n    }\n    \n    create_feature_dict_bins = {\n        'log_return': [realized_volatility, realized_absvar],\n        'order_count': [np.sum],\n    }\n    \n    # Get the stats for different windows\n    df_feature_0 = get_stats_bins(df, create_feature_dict, 0)\n    df_feature_w4 = get_stats_window(df, create_feature_dict_bins, 150)\n\n    # Merge all\n    df_feature_0 = df_feature_0.merge(df_feature_w4, how = 'left', on = 'time_id')   \n\n    df_feature_0 = df_feature_0.add_prefix('trade_')\n    stock_id = file_path.split('=')[1]\n    df_feature_0['row_id'] = df_feature_0['trade_time_id'].apply(lambda x:f'{stock_id}-{x}')\n    df_feature_0.drop(['trade_time_id'], axis = 1, inplace = True)\n    return df_feature_0\n    \n# Funtion to make preprocessing function in parallel (for each stock id)\ndef preprocessor(list_stock_ids, is_train = True):\n    \n    # Parrallel for loop\n    def for_joblib(stock_id):\n        # Train\n        if is_train:\n            file_path_book = data_dir + \"book_train.parquet/stock_id=\" + str(stock_id)\n            file_path_trade = data_dir + \"trade_train.parquet/stock_id=\" + str(stock_id)\n        # Test\n        else:\n            file_path_book = data_dir + \"book_test.parquet/stock_id=\" + str(stock_id)\n            file_path_trade = data_dir + \"trade_test.parquet/stock_id=\" + str(stock_id)\n    \n        # Preprocess book and trade data and merge them\n        df_tmp = pd.merge(book_preprocessor(file_path_book), trade_preprocessor(file_path_trade), on = 'row_id', how = 'left')      \n        # Return the merge dataframe\n        return df_tmp\n    \n    # Use parallel api to call paralle for loop\n    df = Parallel(n_jobs = -1, verbose = 1)(delayed(for_joblib)(stock_id) for stock_id in list_stock_ids)\n    # Concatenate all the dataframes that return from Parallel\n    df = pd.concat(df, ignore_index = True)\n    return df\n\n# Get unique stock ids \ntrain_stock_ids = train['stock_id'].unique()\n# Preprocess them using Parallel and our single stock id functions\ntrain_ = preprocessor(train_stock_ids, is_train = True)\ntrain = train.merge(train_, on = ['row_id'], how = 'left')\n\n# Get unique stock ids \ntest_stock_ids = test['stock_id'].unique()\n# Preprocess them using Parallel and our single stock id functions\ntest_ = preprocessor(test_stock_ids, is_train = False)\ntest = test.merge(test_, on = ['row_id'], how = 'left')\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2021-09-21T05:49:18.470027Z","iopub.execute_input":"2021-09-21T05:49:18.470415Z","iopub.status.idle":"2021-09-21T05:49:59.972286Z","shell.execute_reply.started":"2021-09-21T05:49:18.470385Z","shell.execute_reply":"2021-09-21T05:49:59.970424Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['trade_size_tau'] = np.sqrt(1/train['trade_order_count_sum'])\n    \nfor w in range(150, 600, 150):\n    train['trade_size_tau_150win_'+str(w)] = np.sqrt(1/train['trade_order_count_sum_150win_'+str(w)])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test['trade_size_tau'] = np.sqrt(1/test['trade_order_count_sum'])\n    \nfor w in range(150, 600, 150):\n    test['trade_size_tau_150win_'+str(w)] = np.sqrt(1/test['trade_order_count_sum_150win_'+str(w)])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function to calculate the root mean squared percentage error\ndef rmspe(y_true, y_pred):\n    return np.sqrt(np.mean(np.square((y_true - y_pred) / y_true)))\n\n# Function to early stop with root mean squared percentage error\ndef feval_rmspe(y_pred, lgb_train):\n    y_true = lgb_train.get_label()\n    return 'RMSPE', rmspe(y_true, y_pred), False\n\n# Function to get group stats for the stock_id and time_id\ndef get_time_agg(df):\n    gcols=['book_log_return1_realized_volatility']\n    gcols+=['book_log_return1_realized_volatility_150win_150']\n    gcols+=['book_log_return2_realized_volatility_150win_300']\n    gcols+=['book_log_return1_realized_volatility_150win_450']+['trade_log_return_realized_volatility_150win_450']\n    gcols+=['book_price_spread_sum_150win_150']\n    gcols+=['trade_size_tau_150win_150']\n    gcols+=['book_depth_sum_150win_150']\n    gcols+=['book_dispersion_sum_150win_150']\n\n    # Group by the stock id\n    df_time_id = df.groupby('time_id')[gcols].agg(['mean', 'std', 'max', 'min']).reset_index()\n    # Rename columns joining suffix\n    df_time_id.columns = ['_'.join(col) for col in df_time_id.columns]\n    df_time_id = df_time_id.add_suffix('_' + 'time')\n    \n    # Merge with original dataframe\n    df_time_id = df_time_id.rename(columns={'time_id__time':'time_id'})\n    return df_time_id, [col for col in df_time_id if col not in ['time_id']]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cols = ['row_id','time_id','stock_id','target']\ncols+=['book_log_return1_realized_volatility']\ncols+=['book_log_return1_realized_volatility_150win_150']+['book_log_return2_realized_volatility_150win_150']+[\n            'trade_log_return_realized_volatility_150win_150']\ncols+=['book_log_return1_realized_absvar_150win_150']+['book_log_return2_realized_absvar_150win_150']+[\n            'trade_log_return_realized_absvar_150win_150']\ncols+=['book_log_return2_realized_volatility_150win_300']\ncols+=['book_log_return1_realized_volatility_150win_450']+['trade_log_return_realized_volatility_150win_450']\ncols+=['book_price_spread_sum_150win_150']\ncols+=['trade_size_tau_150win_150']\ncols+=['book_depth_sum_150win_150']\ncols+=['book_dispersion_sum_150win_150']\n\ntrain = train[[col for col in train.columns if col in cols]]\ntest = test[[col for col in test.columns if col in cols]]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"seed0=2021\nparams0 = {\n    'objective': 'rmse',\n    'boosting_type': 'gbdt',\n    'max_depth': 4,\n    'num_leaves': 15,\n    'min_data_in_leaf': 250,\n    'learning_rate': 0.1,\n    'subsample': 0.9,\n    'subsample_freq': 1,\n    'feature_fraction': 0.8,\n    'categorical_column': [0],\n    'seed':seed0,\n    'feature_fraction_seed': seed0,\n    'bagging_seed': seed0,\n    'drop_seed': seed0,\n    'data_random_seed': seed0,\n    'n_jobs':-1,\n    'verbose': -1}\n\ndef train_and_evaluate_lgb(train, test, params, try_seed=seed0):\n    # Hyperparammeters (just basic)\n\n    features = [col for col in train.columns if col not in {\"time_id\", \"target\", \"row_id\"}]\n    feats_nostock = [col for col in train.columns if col not in {\"time_id\", \"target\", \"row_id\", \"stock_id\"}] \n    # Create out of folds array\n    y = train['target']\n    oof_predictions = np.zeros(train.shape[0])\n    # Create test array to store predictions\n    test_predictions = np.zeros(test.shape[0])\n    # Create a KFold object\n    kf = 5\n    # Iterate through each fold\n    skf = stratified_group_k_fold(X=train[feats_nostock], y=train['stock_id'].astype('category').cat.codes.values, \n                                  groups=np.array(train['time_id'].astype('category').cat.codes.values), k=kf, seed=try_seed)\n    for fold, (trn_ind, val_ind) in enumerate(skf):\n        print(f'Training fold {fold + 1}')\n        x_train, x_val = train.iloc[trn_ind], train.iloc[val_ind]\n        print(x_train.shape)\n        print(x_val.shape)\n        y_train, y_val = y.iloc[trn_ind], y.iloc[val_ind]\n        tt=test.copy()\n\n        x_train_agg_time, agg_feats_time = get_time_agg(x_train)\n        x_train = x_train.merge(x_train_agg_time, how='left', on='time_id')\n        x_val_agg_time, _ = get_time_agg(x_val)\n        x_val = x_val.merge(x_val_agg_time, how='left', on='time_id')\n        test_agg_time, _ = get_time_agg(tt)\n        tt = tt.merge(test_agg_time, how='left', on='time_id')\n        del x_train_agg_time,  x_val_agg_time, test_agg_time\n        gc.collect()\n        \n        traincols = features+agg_feats_time\n        # Root mean squared percentage error weights\n        train_weights = 1 / np.square(y_train)\n        val_weights = 1 / np.square(y_val)\n        train_dataset = lgb.Dataset(x_train[traincols], y_train, weight = train_weights)\n        val_dataset = lgb.Dataset(x_val[traincols], y_val, weight = val_weights)\n        model = lgb.train(params = params,\n                          num_boost_round=1000,\n                          train_set = train_dataset, \n                          valid_sets = [train_dataset, val_dataset], \n                          verbose_eval = 50,\n                          early_stopping_rounds=50,\n                          feval = feval_rmspe)\n        # Add predictions to the out of folds array\n        oof_predictions[val_ind] = model.predict(x_val[traincols])\n        # Predict the test set       \n        test_predictions += model.predict(tt[traincols]) / kf\n        plt.rcParams[\"figure.figsize\"] = (14, 7) \n        lgb.plot_importance(model, max_num_features=35)\n        plt.rcParams[\"figure.figsize\"] = (14, 7) \n        lgb.plot_importance(model, max_num_features=35, importance_type='gain')\n    rmspe_score = rmspe(y, oof_predictions)\n    print(f'Our out of folds RMSPE is {rmspe_score}')\n\n    # Return test predictions\n    return test_predictions\n\n# Traing and evaluate\npredictions_lgb_1 = train_and_evaluate_lgb(train, test, params0)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"seed0=2021\nparams0 = {\n    'objective': 'rmse',\n    'boosting_type': 'gbdt',\n    'max_depth': 5,\n    'num_leaves': 31,\n    'min_data_in_leaf': 100,\n    'learning_rate': 0.05,\n    'subsample': 0.95,\n    'subsample_freq': 1,\n    'feature_fraction': 0.9,\n    'categorical_column': [0],\n    'seed':seed0,\n    'feature_fraction_seed': seed0,\n    'bagging_seed': seed0,\n    'drop_seed': seed0,\n    'data_random_seed': seed0,\n    'n_jobs':-1,\n    'verbose': -1}\n\ndef train_and_evaluate_lgb(train, test, params, try_seed=seed0):\n    # Hyperparammeters (just basic)\n\n    features = [col for col in train.columns if col not in {\"time_id\", \"target\", \"row_id\"}]\n    feats_nostock = [col for col in train.columns if col not in {\"time_id\", \"target\", \"row_id\", \"stock_id\"}] \n    # Create out of folds array\n    y = train['target']\n    oof_predictions = np.zeros(train.shape[0])\n    # Create test array to store predictions\n    test_predictions = np.zeros(test.shape[0])\n    # Create a KFold object\n    kf = 10\n    # Iterate through each fold\n    skf = stratified_group_k_fold(X=train[feats_nostock], y=train['stock_id'].astype('category').cat.codes.values, \n                                  groups=np.array(train['time_id'].astype('category').cat.codes.values), k=kf, seed=try_seed)\n    for fold, (trn_ind, val_ind) in enumerate(skf):\n        print(f'Training fold {fold + 1}')\n        x_train, x_val = train.iloc[trn_ind], train.iloc[val_ind]\n        print(x_train.shape)\n        print(x_val.shape)\n        y_train, y_val = y.iloc[trn_ind], y.iloc[val_ind]\n        tt=test.copy()\n\n        x_train_agg_time, agg_feats_time = get_time_agg(x_train)\n        x_train = x_train.merge(x_train_agg_time, how='left', on='time_id')\n        x_val_agg_time, _ = get_time_agg(x_val)\n        x_val = x_val.merge(x_val_agg_time, how='left', on='time_id')\n        test_agg_time, _ = get_time_agg(tt)\n        tt = tt.merge(test_agg_time, how='left', on='time_id')\n        del x_train_agg_time,  x_val_agg_time, test_agg_time\n        gc.collect()\n        \n        traincols = features+agg_feats_time\n        # Root mean squared percentage error weights\n        train_weights = 1 / np.square(y_train)\n        val_weights = 1 / np.square(y_val)\n        train_dataset = lgb.Dataset(x_train[traincols], y_train, weight = train_weights)\n        val_dataset = lgb.Dataset(x_val[traincols], y_val, weight = val_weights)\n        model = lgb.train(params = params,\n                          num_boost_round=1000,\n                          train_set = train_dataset, \n                          valid_sets = [train_dataset, val_dataset], \n                          verbose_eval = 50,\n                          early_stopping_rounds=50,\n                          feval = feval_rmspe)\n        # Add predictions to the out of folds array\n        oof_predictions[val_ind] = model.predict(x_val[traincols])\n        # Predict the test set       \n        test_predictions += model.predict(tt[traincols]) / kf\n        plt.rcParams[\"figure.figsize\"] = (14, 7) \n        lgb.plot_importance(model, max_num_features=35)\n        plt.rcParams[\"figure.figsize\"] = (14, 7) \n        lgb.plot_importance(model, max_num_features=35, importance_type='gain')\n    rmspe_score = rmspe(y, oof_predictions)\n    print(f'Our out of folds RMSPE is {rmspe_score}')\n\n    # Return test predictions\n    return test_predictions\n\n# Traing and evaluate\npredictions_lgb_2 = train_and_evaluate_lgb(train, test, params0)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from numpy.random import seed\nimport tensorflow as tf\nfrom keras.utils.generic_utils import get_custom_objects\nfrom keras.layers import Activation\nfrom tensorflow import keras\nimport numpy as np\nfrom keras import backend as K\nfrom keras.backend import sigmoid\n\ndef swish(x, beta = 1):\n    return (x * sigmoid(beta * x))\n\nget_custom_objects().update({'swish': Activation(swish)})\n\ndef root_mean_squared_per_error(y_true, y_pred):\n         return K.sqrt(K.mean(K.square( (y_true - y_pred)/ y_true )))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def base_model(numfeats, cat_data=train['stock_id']):\n    \n    hidden_units = (64, 32, 32, 16, 16, 16, 16, 8, 8)\n    stock_embedding_size = 24\n    \n    # Each instance will consist of two inputs: a single user id, and a single movie id\n    stock_id_input = keras.Input(shape=(1,), name='stock_id')\n    num_input = keras.Input(shape=(numfeats,), name='num_data')\n\n\n    #embedding, flatenning and concatenating\n    stock_embedded = keras.layers.Embedding(max(cat_data)+1, stock_embedding_size, \n                                           input_length=1, name='stock_embedding')(stock_id_input)\n    stock_flattened = keras.layers.Flatten()(stock_embedded)\n    out = keras.layers.Concatenate()([stock_flattened, num_input])\n    \n    # Add one or more hidden layers\n    for n_hidden in hidden_units:\n        out = keras.layers.Dense(n_hidden, activation='swish')(out)\n\n    #out = keras.layers.Concatenate()([out, num_input])\n\n    # A single output: our predicted rating\n    out = keras.layers.Dense(1, activation='linear', name='prediction')(out)\n    \n    model = keras.Model(\n    inputs = [stock_id_input, num_input],\n    outputs = out,\n    )\n    \n    return model\n\ntf.random.set_seed(42)\nseed(42)\n\nes = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=50, verbose=0,\n                                      mode='min', restore_best_weights=True)\n\nplateau = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=15, \n                                               verbose=0, mode='min')\n\nscores_folds = {}\nmodel_name = 'NN'\n\nscores_folds[model_name] = []\ncounter = 1\n\npredictions_nn_1 = np.zeros(test.shape[0])\ny = train['target']\n# Create a KFold object\nkf = 5\n# Iterate through each fold\nfeatures = [col for col in train.columns if col not in {\"time_id\", \"target\", \"row_id\"}]\nfeats_nostock = [col for col in train.columns if col not in {\"time_id\", \"target\", \"row_id\", \"stock_id\"}] \nskf = stratified_group_k_fold(X=train[feats_nostock], y=train['stock_id'].astype('category').cat.codes.values, \n                              groups=np.array(train['time_id'].astype('category').cat.codes.values), k=kf, seed=2021)\nfor fold, (trn_ind, val_ind) in enumerate(skf):\n    print(f'Training fold {fold + 1}')\n    \n    x_train = train.iloc[trn_ind].copy()\n    x_val = train.iloc[val_ind].copy()\n    print(x_train.shape)\n    print(x_val.shape)\n    y_train, y_val = y.iloc[trn_ind], y.iloc[val_ind]\n    tt = test.copy()\n    \n    #############################################################################################\n    # NN\n    #############################################################################################\n    \n    x_train.loc[:, feats_nostock] = x_train.loc[:, feats_nostock].fillna(x_train.groupby('stock_id')[feats_nostock].transform('median')).values\n    for i in x_val.stock_id.unique():\n        x_val.loc[x_val.stock_id==i, feats_nostock] = x_val.loc[x_val.stock_id==i, feats_nostock].fillna(x_train.loc[x_train.stock_id==i, feats_nostock].median()).values\n    for i in tt.stock_id.unique():\n        tt.loc[tt.stock_id==i, feats_nostock] = tt.loc[tt.stock_id==i, feats_nostock].fillna(x_train.loc[x_train.stock_id==i, feats_nostock].median()).values\n    \n    x_train_agg_time, agg_feats_time = get_time_agg(x_train)\n    x_train = x_train.merge(x_train_agg_time, how='left', on='time_id')\n    x_val_agg_time, _ = get_time_agg(x_val)\n    x_val = x_val.merge(x_val_agg_time, how='left', on='time_id')\n    test_agg_time, _ = get_time_agg(tt)\n    tt = tt.merge(test_agg_time, how='left', on='time_id')\n    del x_train_agg_time,  x_val_agg_time, test_agg_time\n    gc.collect()\n\n    traincols = feats_nostock+agg_feats_time\n\n    for i in tt.stock_id.unique():\n        tt.loc[tt.stock_id==i, traincols] = tt.loc[tt.stock_id==i, traincols].fillna(x_train.loc[x_train.stock_id==i, traincols].median()).values\n\n    num_trans = Pipeline([('qt', QuantileTransformer(n_quantiles=2000, output_distribution='normal')),\n                         ('numscaler', MinMaxScaler())])\n    agg_trans = Pipeline([('aggscaler', MinMaxScaler())])\n    preprocessor = ColumnTransformer(transformers=[('num', num_trans, feats_nostock),\n                                                    ('agg', agg_trans, agg_feats_time)])\n    pipe = Pipeline([('pp', preprocessor)])\n    pipe.fit(x_train[traincols])\n    \n    model = base_model(len(traincols))\n    model.compile(\n        keras.optimizers.Adam(learning_rate=0.001),\n        loss=root_mean_squared_per_error\n    )\n    \n    model.fit([x_train['stock_id'], pipe.transform(x_train[traincols])], \n              y_train,               \n              batch_size=2048,\n              epochs=1000,\n              validation_data=([x_val['stock_id'], pipe.transform(x_val[traincols])], y_val),\n              callbacks=[es, plateau],\n              validation_batch_size=len(y_val),\n              shuffle=True,\n              verbose = 1)\n\n    preds = model.predict([x_val['stock_id'], pipe.transform(x_val[traincols])]).reshape(1,-1)[0]\n    \n    score = round(rmspe(y_true = y_val, y_pred = preds), 5)\n    print('Fold {} {}: {}'.format(counter, model_name, score))\n    scores_folds[model_name].append(score)\n    \n    predictions_nn_1 += model.predict([tt['stock_id'], pipe.transform(tt[traincols])]).reshape(1,-1)[0].clip(0,1e10) / kf\n\n    counter += 1\n    \nprint('RMSPE {}: Folds: {}'.format(model_name, scores_folds[model_name]))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test['target'] = predictions_nn_1*0.6 + predictions_lgb_1*0.2 + predictions_lgb_2*0.2\n\ndisplay(test[['row_id', 'target']].head(3))\ntest[['row_id', 'target']].to_csv('submission.csv',index = False)","metadata":{},"execution_count":null,"outputs":[]}]}