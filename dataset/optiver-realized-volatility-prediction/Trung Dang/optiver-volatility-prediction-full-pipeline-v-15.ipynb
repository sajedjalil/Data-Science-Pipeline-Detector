{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Changelog\n* v.6\n    * No features engineering. `book_seconds_lapse=5` & `trade_seconds_lapse=5` (data: `prep-opt-v6`)\n    * Simple NN\n* v.7\n    * Add new features: `stock_id`, multiple `book` and `trade` features. `book_seconds_lapse=10` & `trade_seconds_lapse=15` (data: `prep-opt-v5`)\n    * Add parallel processing\n    * Debugged prediction\n* v.8\n    * v.6 but NN w/ embedding layers (data: `prep-opt-v6`)\n* v.9\n    * v.7 but `book_seconds_lapse=5` (try to fit in the memory) (data: `prep-opt-v7`)\n* v.10\n    * v.7 but MinMaxScaler instead of StandardScaler (data: prep-opt-v5)\n* v.11\n    * v.9 but `trade_seconds_lapse=20` (data: `prep-opt-v8`), since v.9 can't fit in memory when training\n* v.12\n    * v.10 but \n    * Start trying out GB models: XGBoost, LGBM, CatBoost\n* v.13\n    * v.10 but\n    * Emsemble LGBM + NN\n    * Added progress bar for parallel processing\n* v.14\n    * v.13 (MinMaxScaler, `book_seconds=10`, `trade_seconds=15`)\n    * Checked the situation of stock 31\n    * Added a bunch of features: `is_stock_31`, `trading_halted`, multiple book and trade features (data: `prep-opt-v9`)\n* v.15\n    * v.14, removed `is_stock_31` & `trading_halted`, useless features (data: `prep-opt-v10`)\n    * NN `prep-opt-v5` + LGBM `prep-opt-v10`\n    * Predict in batches\n    \n# (Relative) comparision of datasets:\n* Memory: `prep-opt-v7` > `prep-opt-v8` > `prep-opt-v9` > `prep-opt-v10` > `prep-opt-v5` > `prep-opt-v6`\n* Feature creativeness: `prep-opt-v9` = `prep-opt-v10` > `prep-opt-v7` > `prep-opt-v8` >  `prep-opt-v5` > `prep-opt-v6`","metadata":{}},{"cell_type":"code","source":"import os\nimport glob\nimport gc\nimport time\nimport warnings\nimport math\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib as plt\nfrom math import floor\nfrom joblib import Parallel, delayed\nfrom tqdm.auto import tqdm","metadata":{"execution":{"iopub.status.busy":"2021-09-27T07:54:29.241257Z","iopub.execute_input":"2021-09-27T07:54:29.241904Z","iopub.status.idle":"2021-09-27T07:54:29.443434Z","shell.execute_reply.started":"2021-09-27T07:54:29.241807Z","shell.execute_reply":"2021-09-27T07:54:29.442515Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import (\n    Input, layers, activations, initializers, optimizers, models, regularizers, callbacks\n)\nimport lightgbm as lgb","metadata":{"execution":{"iopub.status.busy":"2021-09-27T07:54:29.445308Z","iopub.execute_input":"2021-09-27T07:54:29.445582Z","iopub.status.idle":"2021-09-27T07:54:36.65388Z","shell.execute_reply.started":"2021-09-27T07:54:29.445551Z","shell.execute_reply":"2021-09-27T07:54:36.653149Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Util functions & Global variables","metadata":{}},{"cell_type":"code","source":"def reduce_memory_usage(df):\n    integers = ['int16', 'int32', 'int64']\n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in integers:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                df[col] = df[col].astype(np.int8)\n            elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                df[col] = df[col].astype(np.int16)\n            elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                df[col] = df[col].astype(np.int32)\n            elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                df[col] = df[col].astype(np.int64)\n                \n        elif col_type == 'float64':\n            df[col] = df[col].astype(np.float32)\n            \n    \n    return df","metadata":{"execution":{"iopub.status.busy":"2021-09-27T07:54:36.654988Z","iopub.execute_input":"2021-09-27T07:54:36.655247Z","iopub.status.idle":"2021-09-27T07:54:36.666466Z","shell.execute_reply.started":"2021-09-27T07:54:36.655216Z","shell.execute_reply":"2021-09-27T07:54:36.665767Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def convert_long_to_wide_format(df):\n    \"\"\"Convert book/trade DataFrame from long format (panel data) to wide format\"\"\"\n    df = df.pivot(index='time_id', columns='seconds_in_bucket')\n    multi_col = pd.MultiIndex.from_product([df.columns.levels[0], \n                                            df.columns.levels[1].astype('str')])\n    df.columns = multi_col.map('_'.join)\n    df.reset_index(inplace=True)\n    \n    return df","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ROOT_PATH = '/kaggle/input/optiver-realized-volatility-prediction/'\ntrain_target = pd.read_csv(ROOT_PATH + 'train.csv')\nMIN_TARGET = train_target['target'].min()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Light GBM","metadata":{}},{"cell_type":"code","source":"def add_book_features(df):\n    df['wap1'] = ((df['bid_price1'] * df['ask_size1'] + \n                   df['ask_price1'] * df['bid_size1']) / \n                  (df['bid_size1'] + df['ask_size1']))\n    df['wap2'] = ((df['bid_price2'] * df['ask_size2'] + \n                   df['ask_price2'] * df['bid_size2']) / \n                  (df['bid_size2'] + df['ask_size2']))\n\n    df['log_ret1'] = df.groupby('time_id')['wap1'].apply(\n        lambda x: np.log(x).diff().fillna(0)\n    )\n    df['log_ret2'] = df.groupby('time_id')['wap2'].apply(\n        lambda x: np.log(x).diff().fillna(0)\n    )\n\n    df['bid_ask_spread'] = df['ask_price1'] / df['bid_price1'] - 1\n    df['price_spread'] = (df['ask_price1'] - df['bid_price1']) / ((df['ask_price1'] + df['bid_price1']) / 2)\n    df['bid_spread'] = df['bid_price1'] / df['bid_price2'] - 1\n    df['ask_spread'] = df['ask_price2'] / df['ask_price1'] - 1\n    df['wap_spread'] = df['wap2'] / df['wap1'] - 1\n    df['bid_ask_depth_ratio'] = (df['bid_size1'] + df['bid_size2']) / (df['ask_size1'] + df['ask_size2'])\n    df['bid_ask_depth_ratio1'] = df['bid_size1'] / df['ask_size1']\n    df['bid_ask_depth_ratio2'] = df['bid_size2'] / df['ask_size2']\n    df['total_depth'] = df['bid_size1'] + df['bid_size2'] + df['ask_size1'] + df['ask_size2']\n    \n    return df\n\ndef add_agg_book_features(df):\n    df_agg = pd.DataFrame()\n    df_agg['sigma1'] = df.groupby('time_id')['log_ret1'].apply(lambda x: np.sqrt(np.sum(x**2)))\n    df_agg['sigma2'] = df.groupby('time_id')['log_ret2'].apply(lambda x: np.sqrt(np.sum(x**2)))\n    df_agg['book_update_times'] = df.groupby('time_id')['seconds_in_bucket'].count()\n\n    for col in ['bid_size1', 'ask_size1', 'bid_size2', 'ask_size2',\n               'bid_price1', 'ask_price1', 'bid_price2', 'ask_price2']:\n        df_agg[col + '_update_times'] = df.groupby('time_id')[col].unique().apply(lambda x: x.shape[0])\n    \n    for col in ['bid_size1', 'ask_size1', 'bid_size2', 'ask_size2',\n                'bid_price1', 'ask_price1', 'bid_price2', 'ask_price2',\n                'wap1', 'wap2', 'bid_ask_spread', 'price_spread', \n                'bid_spread', 'ask_spread', 'wap_spread', 'bid_ask_depth_ratio',\n                'bid_ask_depth_ratio1', 'bid_ask_depth_ratio2', 'total_depth']:\n        df_agg['mean_' + col] = df.groupby('time_id')[col].apply(np.mean)\n        df_agg['std_' + col] = df.groupby('time_id')[col].apply(np.std)\n        df_agg['min_' + col] = df.groupby('time_id')[col].apply(np.min)\n        df_agg['max_' + col] = df.groupby('time_id')[col].apply(np.max)\n\n    \n    for col in ['log_ret1', 'log_ret2']:\n        df_agg['mean_' + col] = df.groupby('time_id')[col].apply(np.mean)\n        df_agg['min_' + col] = df.groupby('time_id')[col].apply(np.min)\n        df_agg['max_' + col] = df.groupby('time_id')[col].apply(np.max)\n\n    return df_agg\n\ndef fetch_book_one_stock(stock_id, partition='train', seconds_lapse=10):\n    \"\"\"Fetch book data of one stock into a DataFrame\"\"\"\n    path = f'{ROOT_PATH}book_{partition}.parquet/stock_id={str(stock_id)}'\n    df = pd.read_parquet(path)\n    df = add_book_features(df)\n    df_agg = add_agg_book_features(df)\n    \n    # add missing seconds to book data\n    df = df.set_index(['time_id', 'seconds_in_bucket'])\n    multi_index = pd.MultiIndex.from_product([df.index.levels[0], \n                                              np.arange(0, 600, seconds_lapse)], \n                                             names = ['time_id', 'seconds_in_bucket'])\n    \n    # forward fill book on missing seconds (missing = order book unchanged)\n    df = df.reindex(multi_index, method='ffill').fillna(method='bfill')  # bfill in case of missing second 0\n    df.reset_index(inplace=True)\n    \n    df = convert_long_to_wide_format(df)\n    df = df.merge(df_agg, on='time_id', how='left')\n    df['stock_id'] = stock_id  # add stock_id as primary key for later merging\n    assert not df.isna().any().any()\n\n    return df","metadata":{"execution":{"iopub.status.busy":"2021-09-27T07:54:36.689458Z","iopub.execute_input":"2021-09-27T07:54:36.689658Z","iopub.status.idle":"2021-09-27T07:54:36.712824Z","shell.execute_reply.started":"2021-09-27T07:54:36.689632Z","shell.execute_reply":"2021-09-27T07:54:36.711973Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def add_trade_features(df):\n    df['real_log_ret'] = df.groupby('time_id')['price'].apply(\n        lambda x: np.log(x).diff().fillna(0)\n    )\n    df['volume'] = df['price'] * df['size']\n    df['volume_per_order'] = (df['volume'] / df['order_count']).fillna(0)\n    df['volume_last_seconds'] = df['volume'] * df['seconds_in_bucket'] / 600\n    df['whale_last_seconds'] = df['volume_per_order'] * df['seconds_in_bucket'] / 600\n\n    return df\n\ndef add_agg_trade_features(df, df_agg):\n    df_agg['real_sigma'] = df.groupby('time_id')['real_log_ret'].apply(lambda x: np.sqrt(np.sum(x**2)))\n\n    for col in ['price', 'size', 'order_count', 'volume', \n                'volume_per_order', 'volume_last_seconds', 'whale_last_seconds']:\n        df_agg['mean_' + col] = df.groupby('time_id')[col].apply(np.mean)\n        df_agg['std_' + col] = df.groupby('time_id')[col].apply(np.std)\n        df_agg['min_' + col] = df.groupby('time_id')[col].apply(np.min)\n        df_agg['max_' + col] = df.groupby('time_id')[col].apply(np.max)\n\n    \n    return df_agg\n    \ndef fetch_trade_one_stock(stock_id, partition='train', seconds_lapse=15):\n    \"\"\"Fetch trade data of one stock into a DataFrame\"\"\"\n    path = f'{ROOT_PATH}trade_{partition}.parquet/stock_id={str(stock_id)}'\n    df = pd.read_parquet(path)\n    df_agg = pd.DataFrame()\n    df_agg['trade_times'] = df.groupby('time_id')['seconds_in_bucket'].unique().apply(lambda x: x.shape[0])\n\n    # add ALL missing seconds (0, 1, 2,..., 599, 600) to trade data\n    df = df.set_index(['time_id', 'seconds_in_bucket'])\n    multi_index = pd.MultiIndex.from_product([df.index.levels[0], \n                                              np.arange(0, 600)], \n                                             names = ['time_id', 'seconds_in_bucket'])\n    df = df.reindex(multi_index)\n    df.reset_index(inplace=True)\n    # forward fill price in missing seconds (missing = no trade happens)\n    df['price'] = df['price'].fillna(method='ffill').fillna(method='bfill')  # bfill in case of missing second 0\n    # fill size and order_count with 0\n    df['size'] = df['size'].fillna(value=0).astype(int)\n    df['order_count'] = df['order_count'].fillna(value=0).astype(int)\n    \n    # build another bucket of seconds (1 bucket = seconds_lapse)\n    # e.g. seconds_lapse = 5; then seconds 0, 1, ..., 4 fall into bucket 5; seconds 5 - 9 fall into bucket 10; etc.\n    df['seconds_in_bucket'] = df['seconds_in_bucket'].apply(lambda x: (floor(x / seconds_lapse) + 1) * seconds_lapse)\n    # now compute the average price traded in each bucket of seconds again \n    # & total number shares traded + order_count\n    df['price'] = df['price'] * df['size']\n    df = df.groupby(['time_id', 'seconds_in_bucket']).sum()\n    df['price'] = df['price'] / df['size']\n    # fill missing price due to no trades happened in an entire bucket\n    df['price'] = df['price'].fillna(method='ffill').fillna(method='bfill')\n    df.reset_index(inplace=True)\n    \n    df = add_trade_features(df)\n    df_agg = add_agg_trade_features(df, df_agg)\n    \n    df = convert_long_to_wide_format(df)\n    df = df.merge(df_agg, on='time_id', how='left')\n    df['stock_id'] = stock_id  # add stock_id as primary key for later merging\n    \n    assert not df.isna().any().any()\n\n    return df","metadata":{"execution":{"iopub.status.busy":"2021-09-27T07:54:36.714192Z","iopub.execute_input":"2021-09-27T07:54:36.714463Z","iopub.status.idle":"2021-09-27T07:54:36.733613Z","shell.execute_reply.started":"2021-09-27T07:54:36.714429Z","shell.execute_reply":"2021-09-27T07:54:36.732854Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def fetch_data_one_stock(stock_id, partition='train', book_seconds_lapse=10, trade_seconds_lapse=15):\n    pd.options.mode.chained_assignment = None\n    book_df = fetch_book_one_stock(stock_id, partition, book_seconds_lapse)\n    trade_df = fetch_trade_one_stock(stock_id, partition, trade_seconds_lapse)\n    # merge order books and trade histories into 1 DataFrame\n    features_df = book_df.merge(trade_df, on=['stock_id', 'time_id'], how='left')    \n    del book_df, trade_df\n    gc.collect()\n    features_df.fillna(0, inplace=True)\n\n    if partition == 'train':\n        target_df = train_target[train_target['stock_id'] == stock_id]\n        target_df['target'] = target_df['target'].astype('float32')\n        full_df = target_df.merge(features_df, on=['stock_id', 'time_id'], how='inner')\n        del features_df, target_df\n        gc.collect()\n        return reduce_memory_usage(full_df)\n    \n    elif partition == 'test':\n        cols = list(features_df.columns)\n        cols.insert(0, cols.pop(cols.index('stock_id')))\n        features_df = features_df.loc[:, cols]\n        return reduce_memory_usage(features_df)","metadata":{"execution":{"iopub.status.busy":"2021-09-27T07:54:36.95897Z","iopub.execute_input":"2021-09-27T07:54:36.959503Z","iopub.status.idle":"2021-09-27T07:54:36.968201Z","shell.execute_reply.started":"2021-09-27T07:54:36.959467Z","shell.execute_reply":"2021-09-27T07:54:36.967477Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ProgressParallel(Parallel):\n    def __init__(self, use_tqdm=True, total=None, *args, **kwargs):\n        self._use_tqdm = use_tqdm\n        self._total = total\n        super().__init__(*args, **kwargs)\n\n    def __call__(self, *args, **kwargs):\n        with tqdm(disable=not self._use_tqdm, total=self._total) as self._pbar:\n            return Parallel.__call__(self, *args, **kwargs)\n\n    def print_progress(self):\n        if self._total is None:\n            self._pbar.total = self.n_dispatched_tasks\n        self._pbar.n = self.n_completed_tasks\n        self._pbar.refresh()\n\n        \ndef fetch_data_all_stocks(stock_ids, partition=\"train\", book_seconds_lapse=10, trade_seconds_lapse=15):\n    df = ProgressParallel(n_jobs=-1, verbose=10, total=len(stock_ids))(\n        delayed(fetch_data_one_stock)(\n            stock_id, partition, book_seconds_lapse, trade_seconds_lapse\n        ) for stock_id in stock_ids\n    )\n    df = pd.concat(df, ignore_index=True)\n    gc.collect()\n    \n    assert not df.isna().any().any()\n    return reduce_memory_usage(df)","metadata":{"execution":{"iopub.status.busy":"2021-09-27T07:54:36.972434Z","iopub.execute_input":"2021-09-27T07:54:36.972724Z","iopub.status.idle":"2021-09-27T07:54:36.984158Z","shell.execute_reply.started":"2021-09-27T07:54:36.972691Z","shell.execute_reply":"2021-09-27T07:54:36.983402Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def predict_one_stock(stock_id, model, scaler, model_type):\n    X_num_test_df = fetch_data_one_stock(stock_id, partition='test')\n    submission = pd.DataFrame()\n    submission['row_id'] = X_num_test_df['stock_id'].astype(str) + '-' + X_num_test_df['time_id'].astype(str)\n    \n    X_cat_test = X_num_test_df[CAT_FEATURES].to_numpy()\n    X_num_test_df.drop((CAT_FEATURES + ['time_id']), axis=1, inplace=True)\n    X_num_test = X_num_test_df.to_numpy(dtype='float32')\n    del X_num_test_df\n    gc.collect()\n    scaler.transform(X_num_test)\n    \n    if model_type == 'lgbm':\n        X_test = np.concatenate((X_cat_test, X_num_test), axis=1)\n        del X_num_test, X_cat_test\n        gc.collect()\n        Y_test = np.clip(model.predict(X_test), MIN_TARGET/2, None)\n        del X_test\n        gc.collect()\n        \n    elif model_type == 'nn':\n        Y_test = np.clip(model.predict((X_cat_test, X_num_test)), MIN_TARGET/2, None)\n        del X_cat_test, X_num_test\n        gc.collect()\n    \n    submission['target_' + model_type] = Y_test\n    del Y_test\n    gc.collect()\n\n    return submission\n    \ndef predict_multiple_stocks(stock_ids, model, scaler, model_type, parallel=False):\n    if not parallel:\n        submission = [predict_one_stock(stock_id, model, scaler, model_type) for stock_id in stock_ids]\n    \n    else:\n        submission = ProgressParallel(n_jobs=-1, verbose=10, total=len(stock_ids))(\n            delayed(predict_one_stock)(stock_id, model, scaler, model_type) for stock_id in stock_ids\n        )\n    submission = pd.concat(submission, ignore_index=True)\n    gc.collect()\n    return submission","metadata":{"execution":{"iopub.status.busy":"2021-09-27T08:35:22.357034Z","iopub.execute_input":"2021-09-27T08:35:22.357442Z","iopub.status.idle":"2021-09-27T08:35:22.368094Z","shell.execute_reply.started":"2021-09-27T08:35:22.357408Z","shell.execute_reply":"2021-09-27T08:35:22.367446Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nX_num = pd.read_parquet('/kaggle/input/prep-opt-v10/prep_opt_v10/full_dataset.parquet')  # book=10, trade=15, 1704 features\nX_num.info(memory_usage='deep')","metadata":{"execution":{"iopub.status.busy":"2021-09-27T07:54:37.002419Z","iopub.execute_input":"2021-09-27T07:54:37.002609Z","iopub.status.idle":"2021-09-27T07:55:03.664613Z","shell.execute_reply.started":"2021-09-27T07:54:37.002581Z","shell.execute_reply":"2021-09-27T07:55:03.663142Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"FEATURE_NAMES = [X_num.columns[0]] + list(X_num.columns[3:])\nFEATURE_NAMES[:10]","metadata":{"execution":{"iopub.status.busy":"2021-09-27T07:55:03.665722Z","iopub.execute_input":"2021-09-27T07:55:03.665972Z","iopub.status.idle":"2021-09-27T07:55:03.674686Z","shell.execute_reply.started":"2021-09-27T07:55:03.665937Z","shell.execute_reply":"2021-09-27T07:55:03.673874Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_num.shape","metadata":{"execution":{"iopub.status.busy":"2021-09-27T07:55:03.675862Z","iopub.execute_input":"2021-09-27T07:55:03.676346Z","iopub.status.idle":"2021-09-27T07:55:03.683351Z","shell.execute_reply.started":"2021-09-27T07:55:03.676297Z","shell.execute_reply":"2021-09-27T07:55:03.682478Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"CAT_FEATURES = ['stock_id']\nX_cat = X_num[CAT_FEATURES].to_numpy()\nY = X_num['target'].to_numpy('float32')\nX_num.drop((CAT_FEATURES + ['time_id', 'target']), axis=1, inplace=True)\ngc.collect()\nprint('X_cat shape: ', X_cat.shape)\nprint('X_num shape:', X_num.shape)\nprint('Y shape:', Y.shape)","metadata":{"execution":{"iopub.status.busy":"2021-09-27T07:55:03.684988Z","iopub.execute_input":"2021-09-27T07:55:03.685394Z","iopub.status.idle":"2021-09-27T07:55:05.42775Z","shell.execute_reply.started":"2021-09-27T07:55:03.68522Z","shell.execute_reply":"2021-09-27T07:55:05.427034Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"RANDOM_STATE = 86\n\ndef reset_seed():\n    np.random.seed(RANDOM_STATE)\n    tf.random.set_seed(RANDOM_STATE)\n    \nreset_seed()","metadata":{"execution":{"iopub.status.busy":"2021-09-27T07:55:05.428974Z","iopub.execute_input":"2021-09-27T07:55:05.429251Z","iopub.status.idle":"2021-09-27T07:55:05.436644Z","shell.execute_reply.started":"2021-09-27T07:55:05.429221Z","shell.execute_reply":"2021-09-27T07:55:05.434779Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_cat_train, X_cat_dev, X_num_train_df, X_num_dev_df, Y_train, Y_dev = train_test_split(\n    X_cat, X_num, Y, test_size=10000, random_state=RANDOM_STATE\n)\ndel X_num, Y\ngc.collect()\n\nX_num_train = X_num_train_df.to_numpy(dtype='float32')\ndel X_num_train_df\ngc.collect()\n\nX_num_dev = X_num_dev_df.to_numpy(dtype='float32')\ndel X_num_dev_df\ngc.collect()\n\nprint('X_cat_train shape: ', X_cat_train.shape)\nprint('X_num_train shape:', X_num_train.shape)\nprint('X_cat_dev shape:', X_cat_dev.shape)\nprint('X_num_dev shape:', X_num_dev.shape)","metadata":{"execution":{"iopub.status.busy":"2021-09-27T07:55:05.438067Z","iopub.execute_input":"2021-09-27T07:55:05.43848Z","iopub.status.idle":"2021-09-27T07:55:11.142301Z","shell.execute_reply.started":"2021-09-27T07:55:05.438448Z","shell.execute_reply":"2021-09-27T07:55:11.141567Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scaler = MinMaxScaler(copy=False)\n\nscaler.fit_transform(X_num_train)\nscaler.transform(X_num_dev);","metadata":{"execution":{"iopub.status.busy":"2021-09-27T07:55:11.14335Z","iopub.execute_input":"2021-09-27T07:55:11.144026Z","iopub.status.idle":"2021-09-27T07:55:13.819394Z","shell.execute_reply.started":"2021-09-27T07:55:11.14398Z","shell.execute_reply":"2021-09-27T07:55:13.818637Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train = np.concatenate((X_cat_train, X_num_train), axis=1)\ndel X_num_train, X_cat_train\ngc.collect()\nX_dev = np.concatenate((X_cat_dev, X_num_dev), axis=1)\ndel X_num_dev, X_cat_dev\ngc.collect()\n\nprint('X_train shape: ', X_train.shape)\nprint('X_dev shape:', X_dev.shape)","metadata":{"execution":{"iopub.status.busy":"2021-09-27T07:55:13.820542Z","iopub.execute_input":"2021-09-27T07:55:13.820873Z","iopub.status.idle":"2021-09-27T07:55:15.479662Z","shell.execute_reply.started":"2021-09-27T07:55:13.820832Z","shell.execute_reply":"2021-09-27T07:55:15.478886Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"d_train = lgb.Dataset(X_train, label=Y_train, feature_name=FEATURE_NAMES, \n                      categorical_feature=CAT_FEATURES, free_raw_data=True)\ndel X_train, Y_train\ngc.collect()\nd_dev = lgb.Dataset(X_dev, label=Y_dev, feature_name=FEATURE_NAMES, \n                    categorical_feature=CAT_FEATURES, free_raw_data=True)\ndel X_dev, Y_dev\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2021-09-27T07:55:15.480927Z","iopub.execute_input":"2021-09-27T07:55:15.481371Z","iopub.status.idle":"2021-09-27T07:55:15.836015Z","shell.execute_reply.started":"2021-09-27T07:55:15.481334Z","shell.execute_reply":"2021-09-27T07:55:15.835211Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def rmspe_np(y_true, y_pred):\n    return np.sqrt(np.mean(np.square((y_true - y_pred) / y_true)))\n\n\ndef fobj_rmpse(y_pred, lgb_train):\n    y_true = lgb_train.get_label()\n    grad = -2 * (y_true - y_pred) / (y_true ** 2)\n    hess = 2 / (y_true ** 2)\n    return grad, hess\n \n\ndef feval_rmspe(y_pred, lgb_train):\n    y_true = lgb_train.get_label()\n    return 'RMSPE: ', round(rmspe_np(y_true, y_pred), 4), False","metadata":{"execution":{"iopub.status.busy":"2021-09-27T07:55:15.8388Z","iopub.execute_input":"2021-09-27T07:55:15.839025Z","iopub.status.idle":"2021-09-27T07:55:15.846524Z","shell.execute_reply.started":"2021-09-27T07:55:15.839001Z","shell.execute_reply":"2021-09-27T07:55:15.845767Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"eval_result = {}\nrecord_eval_cb = lgb.record_evaluation(eval_result)\n\nparams = {\n    'boosting_type': 'gbdt',\n    'max_depth': -1,\n    'num_leaves': 40,\n    'max_bin': 255,\n    'min_data_in_leaf': 750,\n    'learning_rate' : 0.05,\n    'subsample': 0.72,\n    'subsample_freq': 3,\n    'feature_fraction': 0.5,\n    'seed': RANDOM_STATE,\n    'n_jobs': -1,\n    'verbose': -1,\n    'device': 'gpu',\n    'num_gpu': 1,\n    'gpu_platform_id': -1,\n    'gpu_device_id': -1,\n    'gpu_use_dp': False,\n}\n\nmodel = lgb.train(params, d_train, num_boost_round=1000, \n                  valid_sets=[d_train, d_dev], valid_names=['Train', 'Dev'], \n                  early_stopping_rounds=50, fobj=fobj_rmpse, feval=feval_rmspe, \n                  verbose_eval = 10, categorical_feature=CAT_FEATURES,\n                  callbacks=[record_eval_cb])","metadata":{"execution":{"iopub.status.busy":"2021-09-27T07:55:15.847729Z","iopub.execute_input":"2021-09-27T07:55:15.848028Z","iopub.status.idle":"2021-09-27T07:59:38.020897Z","shell.execute_reply.started":"2021-09-27T07:55:15.847992Z","shell.execute_reply":"2021-09-27T07:59:38.020296Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lgb.plot_importance(model, max_num_features=20);","metadata":{"execution":{"iopub.status.busy":"2021-09-27T07:59:38.022504Z","iopub.execute_input":"2021-09-27T07:59:38.022976Z","iopub.status.idle":"2021-09-27T07:59:38.420043Z","shell.execute_reply.started":"2021-09-27T07:59:38.022939Z","shell.execute_reply":"2021-09-27T07:59:38.419241Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"val_loss_lgb = np.array(eval_result[\"Dev\"][\"RMSPE: \"])\nmin_val_loss_lgb = np.min(val_loss_lgb)\nbest_epoch_lgb = np.argmin(val_loss_lgb) + 1\nprint(\"LGBM's minimum val_loss: \", min_val_loss_lgb, \", achieved at Round\", best_epoch_lgb)","metadata":{"execution":{"iopub.status.busy":"2021-09-27T07:59:38.421435Z","iopub.execute_input":"2021-09-27T07:59:38.421745Z","iopub.status.idle":"2021-09-27T07:59:38.427935Z","shell.execute_reply.started":"2021-09-27T07:59:38.421705Z","shell.execute_reply":"2021-09-27T07:59:38.42709Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"list_order_book_test = glob.glob(ROOT_PATH + 'book_test.parquet/*')\ntest_ids = [int(path.split('=')[1]) for path in list_order_book_test]\n\nsubmission = predict_multiple_stocks(test_ids, model=model, scaler=scaler, model_type='lgbm', parallel=True)\nsubmission.head()","metadata":{"execution":{"iopub.status.busy":"2021-09-27T08:12:03.032587Z","iopub.execute_input":"2021-09-27T08:12:03.032853Z","iopub.status.idle":"2021-09-27T08:12:05.028953Z","shell.execute_reply.started":"2021-09-27T08:12:03.032826Z","shell.execute_reply":"2021-09-27T08:12:05.028306Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del model, eval_result, record_eval_cb, d_train, d_dev, scaler\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2021-09-27T08:12:20.414446Z","iopub.execute_input":"2021-09-27T08:12:20.414899Z","iopub.status.idle":"2021-09-27T08:12:20.696438Z","shell.execute_reply.started":"2021-09-27T08:12:20.414858Z","shell.execute_reply":"2021-09-27T08:12:20.695608Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# NN","metadata":{}},{"cell_type":"code","source":"def add_book_features(df):\n    df['wap1'] = ((df['bid_price1'] * df['ask_size1'] + \n                   df['ask_price1'] * df['bid_size1']) / \n                  (df['bid_size1'] + df['ask_size1']))\n    df['wap2'] = ((df['bid_price2'] * df['ask_size2'] + \n                   df['ask_price2'] * df['bid_size2']) / \n                  (df['bid_size2'] + df['ask_size2']))\n\n    df['log_ret1'] = df.groupby('time_id')['wap1'].apply(\n        lambda x: np.log(x).diff().fillna(0)\n    )\n    df['log_ret2'] = df.groupby('time_id')['wap2'].apply(\n        lambda x: np.log(x).diff().fillna(0)\n    )\n\n    df['bid_ask_spread'] = df['ask_price1'] / df['bid_price1'] - 1\n    df['bid_spread'] = df['bid_price1'] / df['bid_price2'] - 1\n    df['ask_spread'] = df['ask_price2'] / df['ask_price1'] - 1\n    df['wap_spread'] = df['wap2'] / df['wap1'] - 1\n    df['bid_ask_depth_ratio'] = (df['bid_size1'] + df['bid_size2']) / (df['ask_size1'] + df['ask_size2'])\n    df['bid_ask_depth_ratio1'] = df['bid_size1'] / df['ask_size1']\n    df['bid_ask_depth_ratio2'] = df['bid_size2'] / df['ask_size2']\n    df['total_depth'] = df['bid_size1'] + df['bid_size2'] + df['ask_size1'] + df['ask_size2']\n    \n    return df\n\ndef add_agg_book_features(df):\n    df_agg = pd.DataFrame()\n    df_agg['sigma1'] = df.groupby('time_id')['log_ret1'].apply(lambda x: np.sqrt(np.sum(x**2)))\n    df_agg['sigma2'] = df.groupby('time_id')['log_ret2'].apply(lambda x: np.sqrt(np.sum(x**2)))\n    df_agg['book_update_times'] = df.groupby('time_id')['seconds_in_bucket'].count()\n\n    for col in ['bid_size1', 'ask_size1', 'bid_size2', 'ask_size2',\n               'bid_price1', 'ask_price1', 'bid_price2', 'ask_price2']:\n        df_agg[col + '_update_times'] = df.groupby('time_id')[col].unique().apply(lambda x: x.shape[0])\n    \n    return df_agg\n\ndef fetch_book_one_stock(stock_id, partition='train', seconds_lapse=5):\n    \"\"\"Fetch book data of one stock into a DataFrame\"\"\"\n    path = f'{ROOT_PATH}book_{partition}.parquet/stock_id={str(stock_id)}'\n    df = pd.read_parquet(path)\n    df = add_book_features(df)\n    df_agg = add_agg_book_features(df)\n    \n    # add missing seconds to book data\n    df = df.set_index(['time_id', 'seconds_in_bucket'])\n    multi_index = pd.MultiIndex.from_product([df.index.levels[0], \n                                              np.arange(0, 600, seconds_lapse)], \n                                             names = ['time_id', 'seconds_in_bucket'])\n    \n    # forward fill book on missing seconds (missing = order book unchanged)\n    df = df.reindex(multi_index, method='ffill').fillna(method='bfill')  # bfill in case of missing second 0\n    df.reset_index(inplace=True)\n    \n    df = convert_long_to_wide_format(df)\n    df = df.merge(df_agg, on='time_id', how='left')\n    del df_agg\n    gc.collect()\n    df['stock_id'] = stock_id  # add stock_id as primary key for later merging\n    assert not df.isna().any().any()\n\n    return df","metadata":{"execution":{"iopub.status.busy":"2021-09-27T08:12:43.852899Z","iopub.execute_input":"2021-09-27T08:12:43.853168Z","iopub.status.idle":"2021-09-27T08:12:43.875361Z","shell.execute_reply.started":"2021-09-27T08:12:43.85314Z","shell.execute_reply":"2021-09-27T08:12:43.874506Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def add_trade_features(df):\n    df['real_log_ret'] = df.groupby('time_id')['price'].apply(\n        lambda x: np.log(x).diff().fillna(0)\n    )\n    df['volume'] = df['price'] * df['size']\n    df['volume_per_order'] = (df['volume'] / df['order_count']).fillna(0)\n    df['volume_last_seconds'] = df['volume'] * df['seconds_in_bucket'] / 600\n    df['whale_last_seconds'] = df['volume_per_order'] * df['seconds_in_bucket'] / 600\n\n    return df\n\ndef add_agg_trade_features(df, df_agg):\n    df_agg['real_sigma'] = df.groupby('time_id')['real_log_ret'].apply(lambda x: np.sqrt(np.sum(x**2)))\n\n    for col in ['price', 'size', 'order_count', 'volume', 'volume_per_order', 'volume_last_seconds', 'whale_last_seconds']:\n        df_agg['std_' + col] = df.groupby('time_id')[col].apply(np.std)\n    \n    return df_agg\n    \ndef fetch_trade_one_stock(stock_id, partition='train', seconds_lapse=5):\n    \"\"\"Fetch trade data of one stock into a DataFrame\"\"\"\n    path = f'{ROOT_PATH}trade_{partition}.parquet/stock_id={str(stock_id)}'\n    df = pd.read_parquet(path)\n    df_agg = pd.DataFrame()\n    df_agg['trade_times'] = df.groupby('time_id')['seconds_in_bucket'].unique().apply(lambda x: x.shape[0])\n\n    # add ALL missing seconds (0, 1, 2,..., 599, 600) to trade data\n    df = df.set_index(['time_id', 'seconds_in_bucket'])\n    multi_index = pd.MultiIndex.from_product([df.index.levels[0], \n                                              np.arange(0, 600)], \n                                             names = ['time_id', 'seconds_in_bucket'])\n    df = df.reindex(multi_index)\n    df.reset_index(inplace=True)\n    # forward fill price in missing seconds (missing = no trade happens)\n    df['price'] = df['price'].fillna(method='ffill').fillna(method='bfill')  # bfill in case of missing second 0\n    # fill size and order_count with 0\n    df['size'] = df['size'].fillna(value=0).astype(int)\n    df['order_count'] = df['order_count'].fillna(value=0).astype(int)\n    \n    # build another bucket of seconds (1 bucket = seconds_lapse)\n    # e.g. seconds_lapse = 5; then seconds 0, 1, ..., 4 fall into bucket 5; seconds 5 - 9 fall into bucket 10; etc.\n    df['seconds_in_bucket'] = df['seconds_in_bucket'].apply(lambda x: (floor(x / seconds_lapse) + 1) * seconds_lapse)\n    # now compute the average price traded in each bucket of seconds again \n    # & total number shares traded + order_count\n    df['price'] = df['price'] * df['size']\n    df = df.groupby(['time_id', 'seconds_in_bucket']).sum()\n    df['price'] = df['price'] / df['size']\n    # fill missing price due to no trades happened in an entire bucket\n    df['price'] = df['price'].fillna(method='ffill').fillna(method='bfill')\n    df.reset_index(inplace=True)\n    \n    df = add_trade_features(df)\n    df_agg = add_agg_trade_features(df, df_agg)\n    \n    df = convert_long_to_wide_format(df)\n    df = df.merge(df_agg, on='time_id', how='left')\n    del df_agg\n    gc.collect()\n    df['stock_id'] = stock_id  # add stock_id as primary key for later merging\n    \n    assert not df.isna().any().any()\n\n    return df","metadata":{"execution":{"iopub.status.busy":"2021-09-27T08:12:43.879191Z","iopub.execute_input":"2021-09-27T08:12:43.879424Z","iopub.status.idle":"2021-09-27T08:12:43.897818Z","shell.execute_reply.started":"2021-09-27T08:12:43.8794Z","shell.execute_reply":"2021-09-27T08:12:43.897068Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def fetch_data_one_stock(stock_id, partition='train', book_seconds_lapse=10, trade_seconds_lapse=15):\n    pd.options.mode.chained_assignment = None\n    book_df = fetch_book_one_stock(stock_id, partition, book_seconds_lapse)\n    trade_df = fetch_trade_one_stock(stock_id, partition, trade_seconds_lapse)\n    # merge order books and trade histories into 1 DataFrame\n    features_df = book_df.merge(trade_df, on=['stock_id', 'time_id'], how='left')    \n    del book_df, trade_df\n    gc.collect()\n    features_df.fillna(0, inplace=True)\n\n    if partition == 'train':\n        target_df = train_target[train_target['stock_id'] == stock_id]\n        target_df['target'] = target_df['target'].astype('float32')\n        full_df = target_df.merge(features_df, on=['stock_id', 'time_id'], how='inner')\n        del features_df, target_df\n        gc.collect()\n        return reduce_memory_usage(full_df)\n    \n    elif partition == 'test':\n        cols = list(features_df.columns)\n        cols.insert(0, cols.pop(cols.index('stock_id')))\n        features_df = features_df.loc[:, cols]\n        return reduce_memory_usage(features_df)","metadata":{"execution":{"iopub.status.busy":"2021-09-27T08:12:43.899567Z","iopub.execute_input":"2021-09-27T08:12:43.899788Z","iopub.status.idle":"2021-09-27T08:12:43.911486Z","shell.execute_reply.started":"2021-09-27T08:12:43.899764Z","shell.execute_reply":"2021-09-27T08:12:43.910758Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ProgressParallel(Parallel):\n    def __init__(self, use_tqdm=True, total=None, *args, **kwargs):\n        self._use_tqdm = use_tqdm\n        self._total = total\n        super().__init__(*args, **kwargs)\n\n    def __call__(self, *args, **kwargs):\n        with tqdm(disable=not self._use_tqdm, total=self._total) as self._pbar:\n            return Parallel.__call__(self, *args, **kwargs)\n\n    def print_progress(self):\n        if self._total is None:\n            self._pbar.total = self.n_dispatched_tasks\n        self._pbar.n = self.n_completed_tasks\n        self._pbar.refresh()\n\n        \ndef fetch_data_all_stocks(stock_ids, partition=\"train\", book_seconds_lapse=10, trade_seconds_lapse=15):\n    df = ProgressParallel(n_jobs=-1, verbose=10, total=len(stock_ids))(\n        delayed(fetch_data_one_stock)(\n            stock_id, partition, book_seconds_lapse, trade_seconds_lapse\n        ) for stock_id in stock_ids\n    )\n    df = pd.concat(df, ignore_index=True)\n    gc.collect()\n    \n    assert not df.isna().any().any()\n    return reduce_memory_usage(df)","metadata":{"execution":{"iopub.status.busy":"2021-09-27T08:12:43.91495Z","iopub.execute_input":"2021-09-27T08:12:43.915242Z","iopub.status.idle":"2021-09-27T08:12:43.926795Z","shell.execute_reply.started":"2021-09-27T08:12:43.915216Z","shell.execute_reply":"2021-09-27T08:12:43.92613Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def predict_one_stock(stock_id, model, scaler, model_type):\n    X_num_test_df = fetch_data_one_stock(stock_id, partition='test')\n    submission = pd.DataFrame()\n    submission['row_id'] = X_num_test_df['stock_id'].astype(str) + '-' + X_num_test_df['time_id'].astype(str)\n    \n    X_cat_test = X_num_test_df[CAT_FEATURES].to_numpy()\n    X_num_test_df.drop((CAT_FEATURES + ['time_id']), axis=1, inplace=True)\n    X_num_test = X_num_test_df.to_numpy(dtype='float32')\n    del X_num_test_df\n    gc.collect()\n    scaler.transform(X_num_test)\n    \n    if model_type == 'lgbm':\n        X_test = np.concatenate((X_cat_test, X_num_test), axis=1)\n        del X_num_test, X_cat_test\n        gc.collect()\n        Y_test = np.clip(model.predict(X_test), MIN_TARGET/2, None)\n        del X_test\n        gc.collect()\n        \n    elif model_type == 'nn':\n        Y_test = np.clip(model.predict((X_cat_test, X_num_test)), MIN_TARGET/2, None)\n        del X_cat_test, X_num_test\n        gc.collect()\n    \n    submission['target_' + model_type] = Y_test\n    del Y_test\n    gc.collect()\n\n    return submission\n    \ndef predict_multiple_stocks(stock_ids, model, scaler, model_type, parallel=False):\n    if not parallel:\n        submission = [predict_one_stock(stock_id, model, scaler, model_type) for stock_id in stock_ids]\n    \n    else:\n        submission = ProgressParallel(n_jobs=-1, verbose=10, total=len(stock_ids))(\n            delayed(predict_one_stock)(stock_id, model, scaler, model_type) for stock_id in stock_ids\n        )\n    submission = pd.concat(submission, ignore_index=True)\n    gc.collect()\n    return submission","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nX_num = pd.read_parquet('/kaggle/input/prep-opt-v5/prep_opt/full_dataset.parquet')  # book=10, trade=15, 1541 features\nX_num.info(memory_usage='deep')","metadata":{"execution":{"iopub.status.busy":"2021-09-27T08:12:43.928306Z","iopub.execute_input":"2021-09-27T08:12:43.928549Z","iopub.status.idle":"2021-09-27T08:13:09.355001Z","shell.execute_reply.started":"2021-09-27T08:12:43.928514Z","shell.execute_reply":"2021-09-27T08:13:09.354224Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_cat = X_num[CAT_FEATURES].to_numpy()\nMAX_STOCK_ID = max(X_cat)[0] + 1\nY = X_num['target'].to_numpy('float32')\nX_num.drop((CAT_FEATURES + ['time_id', 'target']), axis=1, inplace=True)\ngc.collect()\nprint('X_cat shape: ', X_cat.shape)\nprint('X_num shape:', X_num.shape)\nprint('Y shape:', Y.shape)","metadata":{"execution":{"iopub.status.busy":"2021-09-27T08:13:09.356898Z","iopub.execute_input":"2021-09-27T08:13:09.357357Z","iopub.status.idle":"2021-09-27T08:13:11.245618Z","shell.execute_reply.started":"2021-09-27T08:13:09.357307Z","shell.execute_reply":"2021-09-27T08:13:11.244838Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"reset_seed()","metadata":{"execution":{"iopub.status.busy":"2021-09-27T08:13:11.246821Z","iopub.execute_input":"2021-09-27T08:13:11.247108Z","iopub.status.idle":"2021-09-27T08:13:11.254228Z","shell.execute_reply.started":"2021-09-27T08:13:11.247069Z","shell.execute_reply":"2021-09-27T08:13:11.253425Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_cat_train, X_cat_dev, X_num_train_df, X_num_dev_df, Y_train, Y_dev = train_test_split(\n    X_cat, X_num, Y, test_size=10000, random_state=RANDOM_STATE\n)\ndel X_num, Y\ngc.collect()\n\nX_num_train = X_num_train_df.to_numpy(dtype='float32')\ndel X_num_train_df\ngc.collect()\n\nX_num_dev = X_num_dev_df.to_numpy(dtype='float32')\ndel X_num_dev_df\ngc.collect()\n\nprint('X_cat_train shape: ', X_cat_train.shape)\nprint('X_num_train shape:', X_num_train.shape)\nprint('X_cat_dev shape:', X_cat_dev.shape)\nprint('X_num_dev shape:', X_num_dev.shape)","metadata":{"execution":{"iopub.status.busy":"2021-09-27T08:13:11.255639Z","iopub.execute_input":"2021-09-27T08:13:11.255905Z","iopub.status.idle":"2021-09-27T08:13:17.265278Z","shell.execute_reply.started":"2021-09-27T08:13:11.255872Z","shell.execute_reply":"2021-09-27T08:13:17.263848Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scaler = MinMaxScaler(copy=False)\n\nscaler.fit_transform(X_num_train)\nscaler.transform(X_num_dev);","metadata":{"execution":{"iopub.status.busy":"2021-09-27T08:13:17.266516Z","iopub.execute_input":"2021-09-27T08:13:17.266784Z","iopub.status.idle":"2021-09-27T08:13:19.697225Z","shell.execute_reply.started":"2021-09-27T08:13:17.266748Z","shell.execute_reply":"2021-09-27T08:13:19.696495Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"keras.backend.clear_session()\nreset_seed()\n\ndef rmspe(y_true, y_pred):\n    return tf.sqrt(tf.reduce_mean(tf.square((y_true - y_pred)/y_true)))\n\ndef optiver_fnn(tpu=True):\n    # params\n    num_units = 64\n    num_layers = 8\n    embedding_size = 96\n    drop_out = 0\n    activation = activations.swish\n    activation_out = activations.swish\n    kernel_initializer = initializers.LecunUniform(seed=RANDOM_STATE)\n    batch_norm = False\n    optimizer = optimizers.Adam\n    learning_rate = 0.0005\n    regularizer = None\n    \n    # build models\n    hidden_units = [num_units] * num_layers\n    quarter_num_layers = int(num_layers / 4)\n    drop_out_rates = ([drop_out] * quarter_num_layers + \n                      [drop_out / 2] * quarter_num_layers + \n                      [drop_out / 4] * quarter_num_layers + \n                      [0] * quarter_num_layers)\n\n    if not len(hidden_units) == len(drop_out_rates):\n        raise Exception(\"Length of hidden units must be equal to length of drop-out rates.\")\n\n    inp_cat = Input(shape=(1,), name='stock_id')\n    inp_num = Input(shape=X_num_train.shape[1], name='num_data')\n    \n    stock_embedded = layers.Embedding(MAX_STOCK_ID, embedding_size, \n                                      input_length=1, name='stock_id_embedding')(inp_cat)\n    stock_flattened = layers.Flatten()(stock_embedded)\n    x = layers.Concatenate()([stock_flattened, inp_num])\n\n    if batch_norm:\n        \n        x = layers.Dense(hidden_units[0], kernel_initializer=kernel_initializer, \n                         kernel_regularizer=regularizer)(x)\n        x = layers.BatchNormalization()(x)\n        x = layers.Activation(activation)(x)\n        x = layers.Dropout(drop_out_rates[0], seed=RANDOM_STATE)(x)\n        \n        for i in range(1, len(hidden_units)):\n            x = layers.Dense(hidden_units[i], kernel_initializer=kernel_initializer, \n                             kernel_regularizer=regularizer)(x)\n            x = layers.BatchNormalization()(x)\n            x = layers.Activation(activation)(x)\n            x = layers.Dropout(drop_out_rates[i], seed=RANDOM_STATE)(x)\n            \n    else:\n        \n        x = layers.Dense(hidden_units[0], kernel_initializer=kernel_initializer, \n                         kernel_regularizer=regularizer)(x)\n        x = layers.Activation(activation)(x)\n        x = layers.Dropout(drop_out_rates[0], seed=RANDOM_STATE)(x)\n\n        for i in range(1, len(hidden_units)):\n            x = layers.Dense(hidden_units[i], kernel_initializer=kernel_initializer, \n                             kernel_regularizer=regularizer)(x)\n            x = layers.Activation(activation)(x)\n            x = layers.Dropout(drop_out_rates[i], seed=RANDOM_STATE)(x)    \n    \n    x = layers.Dense(1)(x)\n    out = layers.Activation(activation_out)(x)\n\n    model = models.Model(inputs=[inp_cat, inp_num], outputs=out)\n    \n    def lr_normalizer(lr, optimizer):\n        if optimizer == optimizers.Adam:\n            return lr\n        elif optimizer == optimizers.Nadam:\n            return lr * 2\n        else:\n            raise Exception(str(optimizer) + 'is neither Adam or Nadam.')\n            \n    model.compile(loss=rmspe, optimizer = optimizer(lr_normalizer(learning_rate, optimizer)))        \n    return model\n\n\nmodel = optiver_fnn()\nearlystop_cb = callbacks.EarlyStopping(patience=20, restore_best_weights=True)\nhistory = model.fit((X_cat_train, X_num_train), Y_train, epochs=400, \n                    validation_data=((X_cat_dev, X_num_dev), Y_dev), batch_size=2048, callbacks=[earlystop_cb]);","metadata":{"execution":{"iopub.status.busy":"2021-09-27T08:13:19.699752Z","iopub.execute_input":"2021-09-27T08:13:19.700201Z","iopub.status.idle":"2021-09-27T08:19:09.112398Z","shell.execute_reply.started":"2021-09-27T08:13:19.700161Z","shell.execute_reply":"2021-09-27T08:19:09.111692Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"val_loss_nn = history.history[\"val_loss\"]\nmin_val_loss_nn = np.min(val_loss_nn)\nbest_epoch_nn = np.argmin(val_loss_nn) + 1\nprint(\"NN's minimum val_loss: \", min_val_loss_nn, \", achieved at Epoch\", best_epoch_nn)","metadata":{"execution":{"iopub.status.busy":"2021-09-27T08:19:09.113762Z","iopub.execute_input":"2021-09-27T08:19:09.114025Z","iopub.status.idle":"2021-09-27T08:19:09.121547Z","shell.execute_reply.started":"2021-09-27T08:19:09.113982Z","shell.execute_reply":"2021-09-27T08:19:09.120646Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del X_cat_train, X_num_train, Y_train, X_cat_dev, X_num_dev, Y_dev, earlystop_cb, history\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2021-09-27T08:19:09.123238Z","iopub.execute_input":"2021-09-27T08:19:09.123503Z","iopub.status.idle":"2021-09-27T08:19:09.339789Z","shell.execute_reply.started":"2021-09-27T08:19:09.123472Z","shell.execute_reply":"2021-09-27T08:19:09.338941Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_nn = predict_multiple_stocks(test_ids, model=model, scaler=scaler, model_type='nn')\nsubmission_nn.head()","metadata":{"execution":{"iopub.status.busy":"2021-09-27T08:36:31.924012Z","iopub.execute_input":"2021-09-27T08:36:31.924274Z","iopub.status.idle":"2021-09-27T08:36:33.96397Z","shell.execute_reply.started":"2021-09-27T08:36:31.924245Z","shell.execute_reply":"2021-09-27T08:36:33.963271Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del model, scaler\ngc.collect()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Ensemble","metadata":{}},{"cell_type":"code","source":"submission = submission.merge(submission_nn, how='left', on='row_id')\ndel submission_nn\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2021-09-27T08:36:50.917536Z","iopub.execute_input":"2021-09-27T08:36:50.918229Z","iopub.status.idle":"2021-09-27T08:36:51.124825Z","shell.execute_reply.started":"2021-09-27T08:36:50.918194Z","shell.execute_reply":"2021-09-27T08:36:51.123859Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"base_rmspe = 0.21\nerror_ratio = (min_val_loss_nn - base_rmspe) / (min_val_loss_lgb - base_rmspe)\nlgb_weight = error_ratio / (error_ratio + 1)\nnn_weight = 1 - lgb_weight\n\nprint('LGBM weight: ', lgb_weight)\nprint('NN weight: ', nn_weight)\nsubmission['target'] = (submission['target_nn'] * nn_weight + \n                        submission['target_lgbm'] * lgb_weight)\nsubmission.drop(['target_nn', 'target_lgbm'], axis=1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-09-27T08:37:10.219218Z","iopub.execute_input":"2021-09-27T08:37:10.219939Z","iopub.status.idle":"2021-09-27T08:37:10.228163Z","shell.execute_reply.started":"2021-09-27T08:37:10.219902Z","shell.execute_reply":"2021-09-27T08:37:10.227495Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission.to_csv('submission.csv', index=False)\nsubmission.head()","metadata":{"execution":{"iopub.status.busy":"2021-09-27T08:37:15.887166Z","iopub.execute_input":"2021-09-27T08:37:15.887735Z","iopub.status.idle":"2021-09-27T08:37:15.902203Z","shell.execute_reply.started":"2021-09-27T08:37:15.887698Z","shell.execute_reply":"2021-09-27T08:37:15.901422Z"},"trusted":true},"execution_count":null,"outputs":[]}]}