{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"This work is build on the basis of https://www.kaggle.com/austinzhao/reproduction-explanation-lgbm-baseline. I used catboost model instead of  LGBM and tuned its hyperparameters with hyperopt.","metadata":{}},{"cell_type":"markdown","source":"# Import\n","metadata":{}},{"cell_type":"code","source":"from catboost import Pool, CatBoostRegressor\n# Import order: data manipulating -> machine/deep learning -> utilities/helpers/improvement -> configuration\nimport pandas as pd\nimport numpy as np\nimport scipy as sc\n\nfrom sklearn.model_selection import KFold\nimport lightgbm as lgb\n\nfrom joblib import Parallel, delayed\n\nimport warnings\nwarnings.filterwarnings('ignore')\npd.set_option('max_columns', 300)\n\nfrom hpsklearn import HyperoptEstimator\nfrom hpsklearn import any_regressor\nfrom hpsklearn import any_preprocessing\nfrom hyperopt import tpe\nfrom  hyperopt import hp","metadata":{"execution":{"iopub.status.busy":"2021-09-03T06:27:13.827775Z","iopub.execute_input":"2021-09-03T06:27:13.828119Z","iopub.status.idle":"2021-09-03T06:27:16.033187Z","shell.execute_reply.started":"2021-09-03T06:27:13.828091Z","shell.execute_reply":"2021-09-03T06:27:16.032313Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Funcs","metadata":{}},{"cell_type":"code","source":"# Define data_directory and data_read func\ndata_dir = '../input/optiver-realized-volatility-prediction/'\ndef read_train_test():\n    train = pd.read_csv('../input/optiver-realized-volatility-prediction/train.csv')\n    test = pd.read_csv('../input/optiver-realized-volatility-prediction/test.csv')\n    # Create a key to merge with book and trade data\n    train['row_id'] = train['stock_id'].astype(str) + '-' + train['time_id'].astype(str)\n    test['row_id'] = test['stock_id'].astype(str) + '-' + test['time_id'].astype(str)\n    print(f'Our training set has {train.shape[0]} rows')\n    return train, test","metadata":{"execution":{"iopub.status.busy":"2021-09-03T06:27:16.034789Z","iopub.execute_input":"2021-09-03T06:27:16.035177Z","iopub.status.idle":"2021-09-03T06:27:16.041792Z","shell.execute_reply.started":"2021-09-03T06:27:16.035118Z","shell.execute_reply":"2021-09-03T06:27:16.040852Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Calculate 1st WAP\ndef calc_wap1(df):\n    wap = (df['ask_price1'] * df['bid_size1'] + df['bid_price1'] * df['ask_size1']) / (df['bid_size1'] + df['ask_size1'])\n    return wap\n# Calculate 2nd WAP\ndef calc_wap2(df):\n    wap = (df['ask_price2'] * df['bid_size2'] + df['bid_price2'] * df['ask_size2']) / (df['bid_size2'] + df['ask_size2'])\n    return wap\n\n# Calculate Log Return\ndef log_return(series):\n    return np.log(series).diff() # log(x / y) = log(x) - log(y), ref[2]\n\n# Realized Volatility\ndef realized_volatility(series):\n    return np.sqrt(np.sum(series**2))\n\n# Count Unique Elements of Series\ndef count_unique(series):\n    return len(np.unique(series))\n\n# Calculate features as specifized feature_dict\ndef calc_features(df, feature_dict):\n    # Calculate STATs (sum, mean, std) for different time-window (seconds in bucket)\n    def calc_certain_window(window, add_suffix=False):\n        # Filter by time-window, Groupy by time_id, then Apply feature_dict\n        df_feature = df[df['seconds_in_bucket'] >= window].groupby(['time_id']).agg(feature_dict).reset_index()\n        # Rename features/columns by joining suffix\n        df_feature.columns = ['_'.join(col) for col in df_feature.columns]\n        # Add a suffix for different time-window\n        if add_suffix:\n            df_feature = df_feature.add_suffix('_' + str(window))\n        return df_feature\n    \n    windows = [0, 150, 300, 450]\n    df_feature = pd.DataFrame()\n    \n    for window in windows:\n        if window == 0:\n            df_feature = calc_certain_window(window=window, add_suffix=False)\n        else:\n            df_feature_tmp = calc_certain_window(window=window, add_suffix=True)\n            df_feature = df_feature.merge(df_feature_tmp, how='left', left_on='time_id_', right_on='time_id__'+str(window))\n        \n    df_feature.drop(['time_id__450', 'time_id__300', 'time_id__150'], axis = 1, inplace = True)\n        \n    return df_feature\n\n\n# Preprocess book-data (applied for each stock_id)\ndef preprocess_book(file_path):\n    df = pd.read_parquet(file_path)\n    \n    # Calculate Wap\n    df['wap1'] = calc_wap1(df)\n    df['wap2'] = calc_wap2(df)\n    # Calculate Log-Return\n    df['log_return1'] = df.groupby(['time_id'])['wap1'].apply(log_return)\n    df['log_return2'] = df.groupby(['time_id'])['wap2'].apply(log_return)\n    # Calculate Wap-Balance\n    df['wap_balance'] = abs(df['wap1'] - df['wap2'])\n    # Calculate Various-Spread\n    df['price_spread'] = (df['ask_price1'] - df['bid_price1']) / ((df['ask_price1'] + df['bid_price1']) / 2)\n    df['bid_spread'] = df['bid_price1'] - df['bid_price2']\n    df['ask_spread'] = df['ask_price1'] - df['ask_price2']\n    df['total_volume'] = (df['ask_size1'] + df['ask_size2']) + (df['bid_size1'] + df['bid_size2'])\n    df['volume_imbalance'] = abs((df['ask_size1'] + df['ask_size2']) - (df['bid_size1'] + df['bid_size2']))\n    \n    # Feature (Generating) Dict for aggregated operations\n    feature_dict = {\n        'wap1': [np.sum, np.mean, np.std],\n        'wap2': [np.sum, np.mean, np.std],\n        'log_return1': [np.sum, realized_volatility, np.mean, np.std],\n        'log_return2': [np.sum, realized_volatility, np.mean, np.std],\n        'wap_balance': [np.sum, np.mean, np.std],\n        'price_spread':[np.sum, np.mean, np.std],\n        'bid_spread':[np.sum, np.mean, np.std],\n        'ask_spread':[np.sum, np.mean, np.std],\n        'total_volume':[np.sum, np.mean, np.std],\n        'volume_imbalance':[np.sum, np.mean, np.std]\n    }\n    \n    df_feature = calc_features(df, feature_dict=feature_dict)\n    \n    # Generate row_id (for later merge)\n    stock_id = file_path.split('=')[1]\n    df_feature['row_id'] = df_feature['time_id_'].apply(lambda x: f'{stock_id}-{x}')\n    # Drop the left time_id_ (after using for generating row_id)\n    df_feature.drop(['time_id_'], axis = 1, inplace = True)\n    \n    return df_feature\n\n# Preprocess trade-Data (applied for each stock_id)\ndef preprocess_trade(file_path):\n    df = pd.read_parquet(file_path)\n    \n    # Calculate Log-Return\n    df['log_return'] = df.groupby('time_id')['price'].apply(log_return)\n    \n    # Feature (Generating) Dict for aggregated operations\n    feature_dict = {\n        'log_return':[realized_volatility],\n        'seconds_in_bucket':[count_unique],\n        'size':[np.sum],\n        'order_count':[np.mean],\n    }\n    \n    df_feature = calc_features(df, feature_dict=feature_dict)\n    \n    df_feature = df_feature.add_prefix('trade_')\n    stock_id = file_path.split('=')[1]\n    df_feature['row_id'] = df_feature['trade_time_id_'].apply(lambda x:f'{stock_id}-{x}')\n    df_feature.drop(['trade_time_id_'], axis = 1, inplace = True)\n    \n    return df_feature\n\n\n# Preprocess/Feature-Engineering in parallel (applied for each stock_id)\ndef preprocess(list_stock_ids, is_train=True):\n    \n    def preprocess_for_stock_id(stock_id):\n        # Generate file_path for train-dataset\n        if is_train:\n            file_path_book = data_dir + \"book_train.parquet/stock_id=\" + str(stock_id)\n            file_path_trade = data_dir + \"trade_train.parquet/stock_id=\" + str(stock_id)\n        # ... for test-dataset\n        else:\n            file_path_book = data_dir + \"book_test.parquet/stock_id=\" + str(stock_id)\n            file_path_trade = data_dir + \"trade_test.parquet/stock_id=\" + str(stock_id)\n    \n        # Preprocess book- and trade- data, then merge both\n        df_tmp = pd.merge(preprocess_book(file_path_book), preprocess_trade(file_path_trade), on='row_id', how='left')\n\n        return df_tmp\n    \n    # Parallelize Preprocessing for Every stock_id\n    df = Parallel(n_jobs=-1, verbose=1)(delayed(preprocess_for_stock_id)(stock_id) for stock_id in list_stock_ids)\n    \n    # Concatenate All Dataframes from Parallelized Preprocessing\n    df = pd.concat(df, ignore_index=True)\n    \n    return df\n\n# Calculate STATs (mean, std, max, min) for realized volatility while groupped by stock_id and time_id\ndef get_time_stock(df_feature):\n    # Enumerate realized volatility features/columns\n    vol_cols = ['log_return1_realized_volatility', 'log_return2_realized_volatility', 'log_return1_realized_volatility_450', 'log_return2_realized_volatility_450', \n                'log_return1_realized_volatility_300', 'log_return2_realized_volatility_300', 'log_return1_realized_volatility_150', 'log_return2_realized_volatility_150', \n                'trade_log_return_realized_volatility', 'trade_log_return_realized_volatility_450', 'trade_log_return_realized_volatility_300', 'trade_log_return_realized_volatility_150']\n\n    # Group by the stock id\n    df_stock_id = df_feature.groupby(['stock_id'])[vol_cols].agg(['mean', 'std', 'max', 'min']).reset_index()\n    # Rename columns joining suffix\n    df_stock_id.columns = ['_'.join(col) for col in df_stock_id.columns]\n    df_stock_id = df_stock_id.add_suffix('_' + 'stock')\n\n    # Group by the stock id\n    df_time_id = df_feature.groupby(['time_id'])[vol_cols].agg(['mean', 'std', 'max', 'min']).reset_index()\n    # Rename columns joining suffix\n    df_time_id.columns = ['_'.join(col) for col in df_time_id.columns]\n    df_time_id = df_time_id.add_suffix('_' + 'time')\n    \n    # Merge with original dataframe\n    df_feature = df_feature.merge(df_stock_id, how='left', left_on=['stock_id'], right_on=['stock_id__stock'])\n    df_feature = df_feature.merge(df_time_id, how='left', left_on=['time_id'], right_on=['time_id__time'])\n    df_feature.drop(['stock_id__stock', 'time_id__time'], axis=1, inplace=True)\n    \n    return df_feature\n\n# Calculate the root mean squared percentage error\ndef rmspe(y_true, y_pred):\n    return np.sqrt(np.mean(np.square((y_true - y_pred) / y_true)))\n\n# Early stop with root mean squared percentage error\ndef feval_rmspe(y_pred, lgb_train):\n    y_true = lgb_train.get_label()\n    return 'RMSPE', rmspe(y_true, y_pred), False","metadata":{"execution":{"iopub.status.busy":"2021-09-03T06:27:17.492845Z","iopub.execute_input":"2021-09-03T06:27:17.493174Z","iopub.status.idle":"2021-09-03T06:27:17.522056Z","shell.execute_reply.started":"2021-09-03T06:27:17.493133Z","shell.execute_reply":"2021-09-03T06:27:17.520873Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Finding best CatBoost model using hyperopt","metadata":{}},{"cell_type":"code","source":"# CatBoost parameters\nctb_reg_params = {\n    'learning_rate':     hp.choice('learning_rate',     np.arange(0.05, 1, 0.05)),\n    'max_depth':         hp.choice('max_depth',         np.arange(5, 16, 1, dtype=int)),\n    'colsample_bylevel': hp.choice('colsample_bylevel', np.arange(0.3, 0.8, 0.1)),\n    'n_estimators':      hp.choice('n_estimators', np.arange(100, 2000, 100)),\n    'eval_metric':       hp.choice('eval_metric', ['RMSE', 'MAPE', 'MAE']),\n    'od_wait': 5 # overfitting detector\n}\nctb_fit_params = {\n    'early_stopping_rounds': 10,\n    \n    'verbose': True\n}\nctb_para = dict()\nctb_para['reg_params'] = ctb_reg_params\nctb_para['fit_params'] = ctb_fit_params\nctb_para['loss_func' ] = lambda y, pred: np.sqrt(mean_squared_error(y, pred))","metadata":{"execution":{"iopub.status.busy":"2021-09-03T06:27:18.166581Z","iopub.execute_input":"2021-09-03T06:27:18.1669Z","iopub.status.idle":"2021-09-03T06:27:18.173942Z","shell.execute_reply.started":"2021-09-03T06:27:18.166871Z","shell.execute_reply":"2021-09-03T06:27:18.172949Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from hyperopt import fmin, tpe, STATUS_OK, STATUS_FAIL, Trials\nimport catboost as ctb\nfrom sklearn.metrics import mean_squared_error\nclass HPOpt(object):\n\n    def __init__(self, x_train, x_test, y_train, y_test):\n        self.x_train = x_train\n        self.x_test  = x_test\n        self.y_train = y_train\n        self.y_test  = y_test\n\n    def process(self, fn_name, space, trials, algo, max_evals):\n        fn = getattr(self, fn_name)\n        try:\n            result = fmin(fn=fn, space=space, algo=algo, max_evals=max_evals, trials=trials)\n        except Exception as e:\n            return {'status': STATUS_FAIL,\n                    'exception': str(e)}\n        return result, trials\n\n\n    def ctb_reg(self, para):\n        reg = ctb.CatBoostRegressor(**para['reg_params'])\n        return self.train_reg(reg, para)\n\n    def train_reg(self, reg, para):\n        reg.fit(self.x_train, self.y_train,\n                eval_set=[(self.x_train, self.y_train), (self.x_test, self.y_test)], \n                **para['fit_params'])\n        pred = reg.predict(self.x_test)\n        loss = para['loss_func'](self.y_test, pred)\n        return {'loss': loss, 'status': STATUS_OK, 'trained_model': reg}","metadata":{"execution":{"iopub.status.busy":"2021-09-03T06:27:19.067271Z","iopub.execute_input":"2021-09-03T06:27:19.067583Z","iopub.status.idle":"2021-09-03T06:27:19.077088Z","shell.execute_reply.started":"2021-09-03T06:27:19.067557Z","shell.execute_reply":"2021-09-03T06:27:19.075981Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_best_model(trials):\n    valid_trial_list = [trial for trial in trials\n                            if STATUS_OK == trial['result']['status']]\n    losses = [ float(trial['result']['loss']) for trial in valid_trial_list]\n    index_having_minumum_loss = np.argmin(losses)\n    best_trial_obj = valid_trial_list[index_having_minumum_loss]\n    best_model = best_trial_obj['result']['trained_model']\n    return best_model","metadata":{"execution":{"iopub.status.busy":"2021-09-03T06:27:24.720793Z","iopub.execute_input":"2021-09-03T06:27:24.721112Z","iopub.status.idle":"2021-09-03T06:27:24.726709Z","shell.execute_reply.started":"2021-09-03T06:27:24.721084Z","shell.execute_reply":"2021-09-03T06:27:24.725626Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_and_evaluate(train, test, best_model):\n\n    # Split features and target\n    x = train.drop(['row_id', 'target', 'time_id'], axis = 1)\n    y = train['target']\n    x_test = test.drop(['row_id', 'time_id'], axis = 1)\n    \n    # Transform stock id to a numeric value\n    x['stock_id'] = x['stock_id'].astype(int)\n    x_test['stock_id'] = x_test['stock_id'].astype(int)\n    \n    # Create out of folds array\n    oof_predictions = np.zeros(x.shape[0])\n    # Create test array to store predictions\n    test_predictions = np.zeros(x_test.shape[0])\n    # Create a KFold object\n    kfold = KFold(n_splits = 5, random_state = 66, shuffle = True)\n    # Iterate through each fold\n    flag = 1\n    model = best_model\n    for fold, (trn_ind, val_ind) in enumerate(kfold.split(x)):\n        print(f'Training fold {fold + 1}')\n        x_train, x_val = x.iloc[trn_ind], x.iloc[val_ind]\n        y_train, y_val = y.iloc[trn_ind], y.iloc[val_ind]\n        # Root mean squared percentage error weights\n        train_weights = 1 / np.square(y_train)\n        val_weights = 1 / np.square(y_val)\n        train_dataset = Pool(x_train,\n                     y_train,\n                     weight=train_weights)\n        val_dataset = Pool(x_val,\n                     y_val,\n                     weight=val_weights)\n\n        if flag == 1:\n            model.fit(train_dataset, eval_set = val_dataset)\n            flag=2\n        else:\n            model.fit(train_dataset,\n                      eval_set = val_dataset,\n                      init_model='model.cbm')\n        # Incremental learning is not needed here, but I leave it for educational purposes\n        model.save_model('model.cbm') \n\n        oof_predictions[val_ind] = model.predict(Pool(x_val))\n        # Predict the test set\n        test_predictions += model.predict(Pool(x_test)) / 5\n        \n    rmspe_score = rmspe(y, oof_predictions)\n    print(f'Our out of folds RMSPE is {rmspe_score}')\n    # Return test predictions\n    return test_predictions","metadata":{"execution":{"iopub.status.busy":"2021-09-03T06:27:25.052567Z","iopub.execute_input":"2021-09-03T06:27:25.052887Z","iopub.status.idle":"2021-09-03T06:27:25.065317Z","shell.execute_reply.started":"2021-09-03T06:27:25.052859Z","shell.execute_reply":"2021-09-03T06:27:25.064281Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Main","metadata":{}},{"cell_type":"code","source":"# Read train and test\ntrain, test = read_train_test()\n\n# Get unique stock_id (as prediction by stock_id)\ntrain_stock_ids = train['stock_id'].unique()\n\n# Generate features\ntrain_feature = preprocess(train_stock_ids, is_train=True)\n# Merge with intiail train data\ntrain = train.merge(train_feature, on=['row_id'], how='left')\n\n# Same for test datas\ntest_stock_ids = test['stock_id'].unique()\ntest_feature = preprocess(test_stock_ids, is_train=False)\ntest = test.merge(test_feature, on=['row_id'], how='left')\n\n# Further generate features with realized-volatility\ntrain = get_time_stock(train)\ntest = get_time_stock(test)","metadata":{"execution":{"iopub.status.busy":"2021-09-03T06:28:33.671643Z","iopub.execute_input":"2021-09-03T06:28:33.67198Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nx = train.drop(['row_id', 'target', 'time_id'], axis = 1)\ny = train['target']\n\nx['stock_id'] = x['stock_id'].astype(int)\n\nx_train, x_val, y_train, y_val =train_test_split( x, y, test_size=0.8, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2021-09-03T06:24:16.830491Z","iopub.execute_input":"2021-09-03T06:24:16.83086Z","iopub.status.idle":"2021-09-03T06:24:17.84459Z","shell.execute_reply.started":"2021-09-03T06:24:16.830831Z","shell.execute_reply":"2021-09-03T06:24:17.843562Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Find best model\nobj = HPOpt(x_train, x_val, y_train, y_val)\nn_epochs_to_evaluate = 50\nctb_obj = obj.process(fn_name='ctb_reg', space=ctb_para, trials=Trials(), \n                      algo=tpe.suggest, max_evals=n_epochs_to_evaluate)\nbest_model = get_best_model(ctb_obj[1])\nbest_model.save_model('best_cv_model.cbm')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Traing and evaluate\ntest_predictions = train_and_evaluate(train, test, best_model)\n\n# Save test predictions\ntest['target'] = test_predictions\ntest[['row_id', 'target']].to_csv('submission.csv', index=False)","metadata":{},"execution_count":null,"outputs":[]}]}