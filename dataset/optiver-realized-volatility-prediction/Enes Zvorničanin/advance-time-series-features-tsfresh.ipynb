{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport copy\nimport glob\nimport numpy as np\nimport pandas as pd\nfrom sklearn import model_selection\nimport xgboost as xgb\nimport matplotlib.pyplot as plt\nfrom tqdm.notebook import tqdm\nimport shap\nfrom tsfresh.feature_extraction import feature_calculators\nimport gc","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-07-10T09:30:52.926765Z","iopub.execute_input":"2021-07-10T09:30:52.927453Z","iopub.status.idle":"2021-07-10T09:31:04.381898Z","shell.execute_reply.started":"2021-07-10T09:30:52.927347Z","shell.execute_reply":"2021-07-10T09:31:04.381119Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The idea of this notebook is to present the way of constructing features using tsfresh package.\n\n**All features will be calculated in respect of the `seconds_in_bucket` column from 0 to 600, where all missed values are first forward filled and after backward filled, as it is the practice with time-series.**\n\nStraight forward using`tsfresh` in order to automatically extract features is hardly feasible here due to the big data, so I've defined some functions manually. Analogously, other features can be extracted using methods from here https://tsfresh.readthedocs.io/en/latest/api/tsfresh.feature_extraction.html#tsfresh.feature_extraction.feature_calculators.abs_energy\n\nHere will be used only a few raw columns for features extraction, just to present the example.","metadata":{}},{"cell_type":"code","source":"def realized_volatility(series_log_return):\n    return np.sqrt(np.sum(series_log_return**2))\n\ndef log_return(list_stock_prices):\n    return np.log(list_stock_prices).diff()\n\ndef abs_sum_of_change(x):\n    x = x.reindex(range(600))\n    x = x.ffill().bfill()\n    return feature_calculators.absolute_sum_of_changes(x)\n\ndef agg_autocorr_mean_120(x):\n    x = x.reindex(range(600))\n    x = x.ffill().bfill()\n    return feature_calculators.agg_autocorrelation(x, [{'f_agg': 'mean', 'maxlag': 100}])[0][1]\n\ndef app_entropy_30(x):\n    x = x.reindex(range(600))\n    x = x.ffill().bfill()\n    return feature_calculators.approximate_entropy([9,200,31,1,5,1,-3,9,2], 30, 0.1)\n\ndef autocorrelation_60(x):\n    x = x.reindex(range(600))\n    x = x.ffill().bfill()\n    return feature_calculators.autocorrelation(x, 60)\n\ndef autocorrelation_120(x):\n    x = x.reindex(range(600))\n    x = x.ffill().bfill()\n    return feature_calculators.autocorrelation(x, 120)\n\ndef autocorrelation_300(x):\n    x = x.reindex(range(600))\n    x = x.ffill().bfill()\n    return feature_calculators.autocorrelation(x, 300)\n\ndef c3_60(x):\n    x = x.reindex(range(600))\n    x = x.ffill().bfill()\n    return feature_calculators.c3(x, 60)\n\ndef cid_ce(x):\n    x = x.reindex(range(600))\n    x = x.ffill().bfill()\n    return feature_calculators.cid_ce(x, True)\n\ndef count_above_mean(x):\n    x = x.reindex(range(600))\n    x = x.ffill().bfill()\n    return feature_calculators.count_above_mean(x)\n\ndef cwt_coeff20_w30_widths_30_60_120_240(x):\n    x = x.reindex(range(600))\n    x = x.ffill().bfill()\n    return list(feature_calculators.cwt_coefficients(x, [{'widths':[30, 60, 120, 240],\n                                                     'coeff':20, 'w':30}]))[0][1]\n\ndef fft_coef1_angle(x):\n    x = x.reindex(range(600))\n    x = x.ffill().bfill()\n    return list(feature_calculators.fft_coefficient(x, [{'coeff': 1, 'attr': \"angle\"}]))[0][1]\n\ndef fft_coef1_abs(x):\n    x = x.reindex(range(600))\n    x = x.ffill().bfill()\n    return list(feature_calculators.fft_coefficient(x, [{'coeff': 1, 'attr': \"abs\"}]))[0][1]\n\ndef first_location_of_maximum(x):\n    x = x.reindex(range(600))\n    x = x.ffill().bfill()\n    return feature_calculators.first_location_of_maximum(x)\n\ndef first_location_of_minimum(x):\n    x = x.reindex(range(600))\n    x = x.ffill().bfill()\n    return feature_calculators.first_location_of_minimum(x)\n\ndef linear_trend_slope(x):\n    x = x.reindex(range(600))\n    x = x.ffill().bfill()\n    return feature_calculators.linear_trend(x, [{'attr': 'slope'}])[0][1]\n\ndef longest_strike_above_mean(x):\n    x = x.reindex(range(600))\n    x = x.ffill().bfill()\n    return feature_calculators.longest_strike_above_mean(x)\n\ndef longest_strike_below_mean(x):\n    x = x.reindex(range(600))\n    x = x.ffill().bfill()\n    return feature_calculators.longest_strike_below_mean(x)\n\ndef mean_abs_change(x):\n    x = x.reindex(range(600))\n    x = x.ffill().bfill()\n    return feature_calculators.mean_abs_change(x)\n\ndef mean_change(x):\n    x = x.reindex(range(600))\n    x = x.ffill().bfill()\n    return feature_calculators.mean_change(x)\n\ndef mean_second_derivative_central(x):\n    x = x.reindex(range(600))\n    x = x.ffill().bfill()\n    return feature_calculators.mean_second_derivative_central(x)\n\ndef number_cwt_peaks_w60(x):\n    x = x.reindex(range(600))\n    x = x.ffill().bfill()\n    return feature_calculators.number_cwt_peaks(x, 60)    \n\ndef partial_autocorrelation_l60(x):\n    x = x.reindex(range(600))\n    x = x.ffill().bfill()\n    return feature_calculators.partial_autocorrelation(x, [{'lag': 60}])[0][1]     \n\ndef ratio_beyond_r_sigma2(x):\n    x = x.reindex(range(600))\n    x = x.ffill().bfill()\n    return feature_calculators.ratio_beyond_r_sigma(x, 2)\n\ndef time_reversal_asymmetry_statistic60(x):\n    x = x.reindex(range(600))\n    x = x.ffill().bfill()\n    return feature_calculators.time_reversal_asymmetry_statistic(x, 60) \n\ndef prepare_book_features(file_path, raw_features, agg_f):\n    \n    df_book_data = pd.read_parquet(file_path)\n    df_book_data['wap'] =(df_book_data['bid_price1'] * df_book_data['ask_size1']+\n                          df_book_data['ask_price1'] * df_book_data['bid_size1'])/(\n        df_book_data['bid_size1']+ df_book_data['ask_size1'])\n        \n    df_book_data['log_return'] = df_book_data.groupby(['time_id'])['wap'].apply(log_return)\n    df_book_data = df_book_data[~df_book_data['log_return'].isnull()]\n    \n    df_stat = df_book_data.set_index('seconds_in_bucket').groupby('time_id').agg(\n        {ckey:agg_f for ckey in raw_features})\n    df_stat.columns = df_stat.columns.map('_'.join)\n    df_stat = df_stat.reset_index()\n    \n    df_realized_vol_per_stock =  pd.DataFrame(df_book_data.groupby(['time_id'])['log_return'].agg(realized_volatility)).reset_index()\n    df_realized_vol_per_stock = df_realized_vol_per_stock.rename(columns = {'log_return':'realized_volatility'})\n    \n    df_realized_vol_per_stock = df_realized_vol_per_stock.merge(df_stat, how='left')\n    stock_id = file_path.split('=')[1]\n    df_realized_vol_per_stock['row_id'] = df_realized_vol_per_stock['time_id'].apply(lambda x:f'{stock_id}-{x}')\n    \n    return df_realized_vol_per_stock\n\ndef prepare_trade_features(file_path, raw_features, agg_f):\n    \n    df_trade_data = pd.read_parquet(file_path)\n    df_trade_data = df_trade_data.set_index('seconds_in_bucket').groupby('time_id').agg(\n        {ckey:agg_f for ckey in raw_features})\n    df_trade_data.columns = df_trade_data.columns.map('_'.join)\n    df_trade_data = df_trade_data.reset_index()\n    \n    stock_id = file_path.split('=')[1]\n    df_trade_data['row_id'] = df_trade_data['time_id'].apply(lambda x:f'{stock_id}-{x}')\n    \n    return df_trade_data\n\ndef process_book_files(files_dir, book_raw_features, agg_f):\n    \n    df_features = pd.DataFrame()\n    list_file = glob.glob(files_dir)\n    \n    for file in tqdm(list_file):\n        df_features = df_features.append(prepare_book_features(file, book_raw_features, agg_f))\n            \n    return df_features\n\ndef process_trade_files(files_dir, trade_raw_features, agg_f):\n    \n    df_features = pd.DataFrame()\n    list_file = glob.glob(files_dir)\n    \n    for file in tqdm(list_file):\n        df_features = df_features.append(prepare_trade_features(file, trade_raw_features, agg_f))\n            \n    return df_features   \n","metadata":{"execution":{"iopub.status.busy":"2021-07-10T09:31:04.383212Z","iopub.execute_input":"2021-07-10T09:31:04.383635Z","iopub.status.idle":"2021-07-10T09:31:04.413983Z","shell.execute_reply.started":"2021-07-10T09:31:04.383603Z","shell.execute_reply":"2021-07-10T09:31:04.412776Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # all tsfresh functions defined above\n# agg_f = [abs_sum_of_change, agg_autocorr_mean_120, app_entropy_30, autocorrelation_60,\n#          autocorrelation_120, autocorrelation_300, c3_60, cid_ce,\n#          count_above_mean, cwt_coeff20_w30_widths_30_60_120_240, fft_coef1_angle, fft_coef1_abs,\n#          first_location_of_maximum, first_location_of_minimum, linear_trend_slope,\n#          longest_strike_above_mean, longest_strike_below_mean, mean_abs_change, mean_change,\n#          mean_second_derivative_central, number_cwt_peaks_w60, partial_autocorrelation_l60,\n#          ratio_beyond_r_sigma2, time_reversal_asymmetry_statistic60]\n\nagg_f = [abs_sum_of_change, first_location_of_minimum, mean_change, ratio_beyond_r_sigma2]\n\n# # all book raw features\n# book_raw_features = ['seconds_in_bucket', 'bid_price1', 'ask_price1', 'bid_price2', 'ask_price2',\n#               'bid_size1', 'ask_size1', 'bid_size2', 'ask_size2', 'wap', 'log_return']\n\nbook_raw_features = ['wap', 'log_return']\n\n# # all trade raw features\n# trade_raw_features = ['seconds_in_bucket_trade', 'price', 'size', 'order_count']\ntrade_raw_features = [ 'price', 'order_count']\n\ndata_dir = '/kaggle/input/optiver-realized-volatility-prediction'\nbook_features = process_book_files(f'{data_dir}/book_train.parquet/*', book_raw_features, agg_f)\ntrade_features = process_trade_files(f'{data_dir}/trade_train.parquet/*', trade_raw_features, agg_f)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"book_features.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trade_features.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"book_features = book_features.drop('time_id', axis=1)\ntrade_features = trade_features.drop('time_id', axis=1)\ndf_features = pd.merge(book_features, trade_features, left_on=['row_id'], right_on=['row_id'], how='left')\ndf_features = df_features.fillna(0)\n\ndf_train = pd.read_csv(f'{data_dir}/train.csv')\ndf_train['row_id'] = df_train[['stock_id', 'time_id']].apply(lambda row: f'{row[0]}-{row[1]}',axis=1)\ndf_train = df_train.drop('stock_id', axis=1)\ndf_train = df_train.drop('time_id', axis=1)\n\ndf_train = pd.merge(df_train, df_features, left_on=['row_id'], right_on=['row_id'], how='left')\ndf_train.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.loc[:, 'kfold'] = -1\ndf_train.sample(frac=1).reset_index(drop=True)\ny = df_train['target'].values\nskf = model_selection.KFold(n_splits=5, shuffle=True)\n\nfor f, (t_, v_) in enumerate(skf.split(X=df_train, y=y)):\n    df_train.loc[v_, 'kfold'] = f\n\ndf_train.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def rmspe(y_true, y_pred):\n    return  (np.sqrt(np.mean(np.square((y_true - y_pred) / y_true))))\n\n#all_features = list(df_train.columns)\n#for f in ['target', 'row_id', 'kfold']:\n#    all_features.remove(f)\nall_features = ['realized_volatility',\n 'wap_abs_sum_of_change', 'order_count_abs_sum_of_change', 'wap_first_location_of_minimum', 'wap_mean_change',\n               'log_return_abs_sum_of_change', 'log_return_first_location_of_minimum','log_return_ratio_beyond_r_sigma2']\n\nfor fold in range(5):\n    \n    df_tr = df_train[df_train.kfold != fold].reset_index(drop=True)\n    df_val = df_train[df_train.kfold == fold].reset_index(drop=True)\n    \n    x_tr = df_tr[all_features].values\n    x_val = df_val[all_features].values\n\n    y_tr = df_tr['target'].values\n    y_val = df_val['target'].values\n    \n    model = xgb.XGBRegressor(n_estimators=50)\n    model.fit(x_tr, y_tr)\n    pred = model.predict(x_val)\n    r = rmspe(y_val, pred)\n    \n    print(f'Fold {fold}, RMSPE:{r}')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"explainer = shap.Explainer(model, df_val[all_features])\nshap_values = explainer.shap_values(df_val[all_features])\nshap.summary_plot(shap_values, df_val[all_features], title='SHAP XGB summary plot', show=False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"book_features_test = process_book_files(f'{data_dir}/book_test.parquet/*', book_raw_features, agg_f)\ntrade_features_test = process_trade_files(f'{data_dir}/trade_test.parquet/*', trade_raw_features, agg_f)\ndf_features_test = pd.merge(book_features_test, trade_features_test, left_on=['row_id'], right_on=['row_id'], how='left')\ndf_features_test = df_features_test.fillna(0)\n\nmodel = xgb.XGBRegressor(n_estimators=50)\nmodel.fit(df_train[all_features].values, df_train['target'].values)\n\npred = model.predict(df_features_test[all_features].values)\n\ndf_features_test['target']=pred\n\ndf_test = pd.read_csv(f'{data_dir}/test.csv')\ndf_test = pd.merge(df_test, df_features_test, left_on=['row_id'], right_on=['row_id'], how='left')\ndf_test = df_test.fillna(0)\n\ndf_test[['row_id', 'target']].to_csv('submission.csv',index = False)","metadata":{},"execution_count":null,"outputs":[]}]}