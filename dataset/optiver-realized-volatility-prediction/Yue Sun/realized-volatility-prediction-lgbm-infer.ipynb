{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport glob\nfrom joblib import Parallel, delayed\nimport xgboost as xgb\nimport copy\nfrom xgboost.sklearn import XGBRegressor\nimport os\nfrom sklearn.linear_model import LinearRegression\nimport warnings\nimport joblib\nfrom sklearn.model_selection import train_test_split, KFold\nimport lightgbm as lgb\nimport datatable as dt\nfrom tqdm import tqdm\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import OneHotEncoder, RobustScaler, StandardScaler\nwarnings.filterwarnings(action='ignore', category=UserWarning)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-06-30T03:57:04.046687Z","iopub.execute_input":"2021-06-30T03:57:04.047222Z","iopub.status.idle":"2021-06-30T03:57:06.401116Z","shell.execute_reply.started":"2021-06-30T03:57:04.047139Z","shell.execute_reply":"2021-06-30T03:57:06.400092Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train: https://www.kaggle.com/yus002/realized-volatility-prediction-lgbm-train","metadata":{}},{"cell_type":"markdown","source":"# References: \n* https://www.kaggle.com/mayunnan/realized-volatility-prediction-code-template by Ma Yunnan\n* https://www.kaggle.com/thanish/randomforest-starter-submission by Thanish Batcha","metadata":{}},{"cell_type":"code","source":"def my_metrics(y_true, y_pred):\n    return np.sqrt(np.mean(np.square((y_true - y_pred) / y_true)))\ndef rmspe(y_true, y_pred):  # f(y_true: array, y_pred: array) -> name: string, eval_result: float, is_higher_better: bool\n    output = my_metrics(y_true, y_pred)\n    return 'rmspe', output, False\ndef log_return(list_stock_prices):\n    return np.log(list_stock_prices).diff() \ndef realized_volatility(series_log_return):\n    return np.sqrt(np.sum(series_log_return**2))\n\ndef get_stock_stat(stock_id : int, dataType = 'train'):   \n    book_train_subset = pd.read_parquet(f'../input/optiver-realized-volatility-prediction/book_{dataType}.parquet/stock_id={stock_id}/')\n    book_train_subset.sort_values(by=['time_id', 'seconds_in_bucket'])\n\n    book_train_subset['bas'] = (book_train_subset[['ask_price1', 'ask_price2']].min(axis = 1)\n                                / book_train_subset[['bid_price1', 'bid_price2']].max(axis = 1)\n                                - 1)                               \n    book_train_subset['wap'] = (book_train_subset['bid_price1'] * book_train_subset['ask_size1'] +\n                            book_train_subset['ask_price1'] * book_train_subset['bid_size1']) / (\n                            book_train_subset['bid_size1']+ book_train_subset['ask_size1'])\n    book_train_subset['log_return'] = (book_train_subset.groupby(by = ['time_id'])['wap'].\n                                       apply(log_return).\n                                       reset_index(drop = True).\n                                       fillna(0)\n                                      )\n    stock_stat = pd.merge(\n        book_train_subset.groupby(by = ['time_id'])['log_return'].agg(realized_volatility).reset_index(),\n        book_train_subset.groupby(by = ['time_id'], as_index = False)['bas'].mean(),\n        on = ['time_id'],\n        how = 'left'\n    )\n    stock_stat['stock_id'] = stock_id\n    return stock_stat\ndef get_dataSet(stock_ids : list, dataType = 'train'):\n    stock_stat = Parallel(n_jobs=-1)(\n        delayed(get_stock_stat)(stock_id, dataType) \n        for stock_id in stock_ids\n    )\n    stock_stat_df = pd.concat(stock_stat, ignore_index = True)\n    return stock_stat_df","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"keep_stock_id = 1\n# keep_stock_id = 0\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train -------------------------\nif keep_stock_id:\n    td = dt.fread('../input/mytrain/X_131_features.csv')\n    X = td.to_pandas()\n    del td\nelse: \n    X = pd.read_csv(\"../input/mytrain/X.csv\")\ny = pd.read_csv(\"../input/mytrain/y.csv\")\n# to_test ----------------------------------------------------\ntest = pd.read_csv('../input/optiver-realized-volatility-prediction/test.csv')\ntest_stock_stat_df = get_dataSet(stock_ids = test['stock_id'].unique(), dataType = 'test')\ntest_dataSet = pd.merge(test, test_stock_stat_df, on = ['stock_id', 'time_id'], how = 'left')\ntest_dataSet = test_dataSet\nfinal_pred1 = test_dataSet[['row_id']]\nto_test = test_dataSet.drop(['row_id'], axis = 1).fillna(0)\nif keep_stock_id:\n    train = pd.read_csv(\"../input/optiver-realized-volatility-prediction/train.csv\")\n    cols = [f'stock_id_{c}' for c in list(set(train.stock_id))]\n    to_test[cols] = pd.DataFrame(np.stack([(to_test.stock_id == c).astype('int') for c in list(set(train.stock_id))]).T, columns = cols)\nelse:\n    to_test = to_test.drop(\"stock_id\", axis = 1)\n    X = X.drop(\"stock_id\", axis = 1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"config = {'input_path': \"../input/optiver-realized-volatility-prediction/trade_\",\n          'train_path': '../input/optiver-realized-volatility-prediction/train.csv',\n          'test_path' : '../input/optiver-realized-volatility-prediction/test.csv'}\ntest_df = pd.read_csv(config['test_path'])\ndef read_data(stock_id, data_type):\n    file = glob.glob(config['input_path']+f'{data_type}.parquet/stock_id={stock_id}/*')[0]\n    df = pd.read_parquet(file)\n    return df\ndef get_final_df(df, data_type):\n    final_df = pd.DataFrame()\n    unique_id = df['stock_id'].unique().tolist()\n    for stock_id in tqdm(unique_id):\n        temp_stock_df = read_data(stock_id=stock_id, data_type=data_type)\n        temp_stock_df['stock_id'] = stock_id\n        final_df = pd.concat([final_df, temp_stock_df])\n    final_df.reset_index(drop=True)\n    return final_df\ndef get_agg_info(df):\n    agg_df = df.groupby(['stock_id', 'time_id']).agg(mean_sec_in_bucket = ('seconds_in_bucket', 'mean'), \n                                                     mean_price = ('price', 'mean'),\n                                                     mean_size = ('size', 'mean'),\n                                                     mean_order = ('order_count', 'mean'),\n                                                     max_sec_in_bucket = ('seconds_in_bucket', 'max'), \n                                                     max_price = ('price', 'max'),\n                                                     max_size = ('size', 'max'),\n                                                     max_order = ('order_count', 'max'),\n                                                     min_sec_in_bucket = ('seconds_in_bucket', 'min'), \n                                                     min_price = ('price', 'min'),\n                                                     min_size = ('size', 'min'),\n                                                     min_order = ('order_count', 'min'),\n                                                     median_sec_in_bucket = ('seconds_in_bucket', 'median'), \n                                                     median_price = ('price', 'median'),\n                                                     median_size = ('size', 'median'),\n                                                     median_order = ('order_count', 'median')\n                                                    ).reset_index()\n    \n    return agg_df\ntest_final_df = get_final_df(df=test_df, data_type='test')\ntest_agg = get_agg_info(df=test_final_df)\ntest_final_df = pd.merge(test_df, test_agg, on=['stock_id', 'time_id'], how='left')\ntest_final_df.fillna(-999, inplace=True)\ntest_final_df = test_final_df.drop(\"row_id\", axis = 1)\nto_test = to_test.merge(test_final_df, on=['stock_id', 'time_id'], how='left')\nto_test.fillna(-999, inplace=True)\nto_test = to_test.drop(\"stock_id\", axis = 1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load in models","metadata":{}},{"cell_type":"code","source":"output = []\nif keep_stock_id:\n    for filepath in glob.iglob('../input/629-rvp-131-features-model/*.pkl'):\n        model = joblib.load(filepath)\n        y_pred = model.predict(to_test, num_iteration = model.best_iteration_)\n        output.append(y_pred)\n        del model\n        del y_pred\nelse:\n    for filepath in glob.iglob('../input/629rvpstock-id-as-continous/*.pkl'):\n        model = joblib.load(filepath)\n        y_pred = model.predict(to_test, num_iteration = model.best_iteration_)\n        output.append(y_pred)\n        del model\n        del y_pred\ny_pred = sum(output) / len(output)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_pred1 = final_pred1.assign(target = y_pred)\nfinal_pred1","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Best Naive baseline 0.29082\n# Reference: https://www.kaggle.com/pratibha9/a-quick-model","metadata":{}},{"cell_type":"code","source":"def realized_volatility_per_time_id(file_path, prediction_column_name):\n    df_book_data = pd.read_parquet(file_path)\n    a = (df_book_data['bid_price1'] * df_book_data['ask_size1'] +\n                                df_book_data['ask_price1'] * df_book_data['bid_size1']) / (\n                                       df_book_data['bid_size1']+ df_book_data['ask_size1'])\n\n    b = (df_book_data['bid_price2'] * df_book_data['ask_size2'] +\n                                df_book_data['ask_price2'] * df_book_data['bid_size2']) / (\n                                       df_book_data['bid_size2']+ df_book_data['ask_size2'])\n    df_book_data['wap'] = (a+b)/2\n    df_book_data['log_return'] = df_book_data.groupby(['time_id'])['wap'].apply(log_return)\n    df_book_data = df_book_data[~df_book_data['log_return'].isnull()]\n    df_realized_vol_per_stock =  pd.DataFrame(df_book_data.groupby(['time_id'])['log_return'].agg(realized_volatility)).reset_index()\n    df_realized_vol_per_stock = df_realized_vol_per_stock.rename(columns = {'log_return':prediction_column_name})\n    stock_id = file_path.split('=')[1]\n    df_realized_vol_per_stock['row_id'] = df_realized_vol_per_stock['time_id'].apply(lambda x:f'{stock_id}-{x}')\n    return df_realized_vol_per_stock[['row_id',prediction_column_name]]\ndef past_realized_volatility_per_stock(list_file,prediction_column_name):\n    df_past_realized = pd.DataFrame()\n    for file in list_file:\n        df_past_realized = pd.concat([df_past_realized,\n                                     realized_volatility_per_time_id(file,prediction_column_name)])\n    return df_past_realized","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"list_order_book_file_test = glob.glob('/kaggle/input/optiver-realized-volatility-prediction/book_test.parquet/*')\nfinal_pred2 = past_realized_volatility_per_stock(list_file=list_order_book_file_test,\n                                                           prediction_column_name='target')\nfinal_pred2","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"output = final_pred1.merge(final_pred2, on = \"row_id\", how = \"outer\").fillna(0)\n# x: my pred, y: naive\nmy_pred_weight = 0.5\noutput[\"target\"] = output.target_x * my_pred_weight + output.target_y * (1 - my_pred_weight)\noutput = output[[\"row_id\", \"target\"]]\noutput.to_csv('submission.csv',index = False)\nprint(output)","metadata":{},"execution_count":null,"outputs":[]}]}