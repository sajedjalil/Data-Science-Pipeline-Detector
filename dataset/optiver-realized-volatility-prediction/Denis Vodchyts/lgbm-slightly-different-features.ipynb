{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"\nThere are two ideas behind this notebook:\n\n- not to use all these min,max,std... features, but instead of it try to find something that can be more interpreteble. I use only 60 features, but i think we can leave half of them without any significant drop. \n- elemenate stock_id from the inputs\n- added feature importance for all calculated features\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-08-23T19:15:11.774902Z","iopub.execute_input":"2021-08-23T19:15:11.775273Z","iopub.status.idle":"2021-08-23T19:15:11.782087Z","shell.execute_reply.started":"2021-08-23T19:15:11.775242Z","shell.execute_reply":"2021-08-23T19:15:11.780708Z"}}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"execution":{"iopub.status.busy":"2021-08-28T08:40:50.679527Z","iopub.execute_input":"2021-08-28T08:40:50.680211Z","iopub.status.idle":"2021-08-28T08:40:51.266868Z","shell.execute_reply.started":"2021-08-28T08:40:50.680087Z","shell.execute_reply":"2021-08-28T08:40:51.265612Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#import modules\nfrom sklearn.metrics import r2_score\nimport pandas as pd\nimport numpy as np\nimport plotly.express as px\n\nimport os\nimport glob\n\nfrom multiprocessing import Pool\n\nfrom pandarallel import pandarallel\npandarallel.initialize()\n\nfrom sklearn.preprocessing import MinMaxScaler\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.layers.experimental import preprocessing\n\nimport time\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\npath_to_files = \"../input/optiver-realized-volatility-prediction\"\n\nbook_train_files =  path_to_files + '/book_train.parquet/stock_id={}'\ntrade_train_files =  path_to_files + '/trade_train.parquet/stock_id={}'\n\nbook_test_files =  path_to_files + '/book_test.parquet/stock_id={}'\ntrade_test_files =  path_to_files + '/trade_test.parquet/stock_id={}'\n\nSMALL_F = 0.00000001\n\ntf.random.set_seed(111)\nnp.random.seed(111)","metadata":{"execution":{"iopub.status.busy":"2021-08-28T08:40:57.08887Z","iopub.execute_input":"2021-08-28T08:40:57.089255Z","iopub.status.idle":"2021-08-28T08:41:06.027264Z","shell.execute_reply.started":"2021-08-28T08:40:57.089219Z","shell.execute_reply":"2021-08-28T08:41:06.026152Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Configuration\ncfg = dict(\n    isCollectDataOnly = True,\n    isStockIdUsed = False,\n    isTFModelUsed = False,\n    trainNotUsedCols = ['row_id', 'target', 'time_id', 'stock_id'],\n    predictNotUsedCols = ['row_id', 'time_id', 'stock_id'],\n    useHyperOpt = False,\n    useLabelTransformation = False,\n    volumeBarThreshold = 500.0\n)\n\n\ncfg","metadata":{"execution":{"iopub.status.busy":"2021-08-28T09:20:04.720188Z","iopub.execute_input":"2021-08-28T09:20:04.72065Z","iopub.status.idle":"2021-08-28T09:20:04.730516Z","shell.execute_reply.started":"2021-08-28T09:20:04.720589Z","shell.execute_reply":"2021-08-28T09:20:04.729196Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Useful functions","metadata":{}},{"cell_type":"code","source":"# Function to count unique elements of a series\ndef count_unique(series):\n    return len(np.unique(series))\n\ndef log_return(prob):\n    prob += SMALL_F\n    return np.log(prob).diff()\n\ndef log_return2(x):\n    return np.log1p(x.pct_change())\n\ndef realized_volatility(series_log_return):\n    return np.sqrt(np.sum(series_log_return**2))\n\ndef rmspe(y_true, y_pred):\n    return  (np.sqrt(np.mean(np.square((y_true - y_pred) / y_true))))\n\n\ndef getOscStoch(x):\n    return (x[-1] - np.min(x))/(np.max(x)-np.min(x)+SMALL_F)\n\ndef getNormVal(x):\n    return (x[-1])/(np.mean(x)+SMALL_F)\n\ndef getBreath(ret, size):\n    f = ret > 0\n    upside = size[f]\n    downside = size[~f]\n    \n    di = np.sum(upside)/(np.sum(downside)+SMALL_F)\n    ado = np.sum(upside) - np.sum(downside)\n    \n    return di, ado\n    \ndef rateLastFirst(x):\n    return np.mean(x)/(np.sum(x)+SMALL_F)\n\ndef rolling_windows_vectorized(array, sub_window_size=2):\n    start = 0\n    max_time = len(array)-sub_window_size\n    sub_windows = (\n        start +\n        # expand_dims are used to convert a 1D array to 2D array.\n        np.expand_dims(np.arange(sub_window_size), 0) +\n        np.expand_dims(np.arange(max_time + 1), 0).T\n    )\n    \n    return array[sub_windows]\n\ndef rolling_windows_vectorized_v2(array, sub_window_size, stride_size):\n    start = 0\n    max_time = len(array)-sub_window_size\n    sub_windows = (\n        start + \n        np.expand_dims(np.arange(sub_window_size), 0) +\n        # Create a rightmost vector as [0, V, 2V, ...].\n        np.expand_dims(np.arange(max_time + 1, step=stride_size), 0).T\n    )\n    \n    return array[sub_windows]    \n\ndef getVolumaBars(i_data, threshold=1000.0):\n    o, h, l, c, v  = 0.0, 0.0, 1000000.0, 0.0, 0.0\n    res_array = []\n    isNewBar = True\n    bar_index = 0.0\n    cum_volume = 0.0\n    data_len = i_data.shape[0]\n    for i in range(data_len):\n        #print(i_data[i])\n        cur_price = i_data[i][0]\n        #print(cur_price)\n        if True == isNewBar:\n            bar_index = i\n            o = cur_price\n            c = 0.0\n            h = 0.0\n            l = 10000000.0\n            v = 0.0\n            \n            isNewBar = False\n\n        if cur_price > h:\n            h = cur_price\n        if cur_price < l:\n            l = cur_price\n\n        v += i_data[i][1]\n        \n        if (v >= threshold) or (i == data_len-1):\n            isNewBar = True\n            #bar_index = i\n            c = cur_price\n            res_array.append([bar_index, o, h, l, c, v])\n\n\n    return pd.DataFrame(res_array, columns=['bar_index', 'open', 'high', 'low', 'close', 'volume'])\n\n\ndef _get_beta(high, low, window):\n\n    beta_r = np.empty(high.shape)\n    beta_r[:] = np.NaN\n\n    ret = np.log(high / low)\n    high_low_ret = ret ** 2\n    beta = rolling_windows_vectorized(high_low_ret, 2).sum(axis=1)\n    beta = rolling_windows_vectorized(beta, window).mean(axis=1)\n    beta_r[len(beta_r)-len(beta):] = beta    \n\n    return beta_r\n\n\ndef _get_gamma(high, low):\n    gamma_r = np.empty(high.shape)\n    gamma_r[:] = np.NaN\n\n    high_max = rolling_windows_vectorized(high, 2).max(axis=1)\n    low_min = rolling_windows_vectorized(low, 2).min(axis=1)\n    gamma = np.log(high_max / low_min) ** 2\n    gamma_r[len(gamma_r)-len(gamma):] = gamma\n\n    return gamma_r    \n\ndef get_bekker_parkinson_vol2(high, low, window: int = 20):\n\n    beta = _get_beta(high, low, window)\n    gamma = _get_gamma(high, low)\n\n    k2 = (8 / np.pi) ** 0.5\n    den = 3 - 2 * 2 ** .5\n    sigma = (2 ** -0.5 - 1) * beta ** 0.5 / (k2 * den)\n    sigma += (gamma / (k2 ** 2 * den)) ** 0.5\n    sigma[sigma < 0] = 0\n\n    return sigma\n\ndef get_garman_class_vol2(open, high, low, close, window):\n    ret_value = np.empty(high.shape)\n    ret_value[:] = np.NaN\n    ret = np.log(high / low)  # High/Low return\n    close_open_ret = np.log(close / open)  # Close/Open return\n    estimator = 0.5 * ret ** 2 - (2 * np.log(2) - 1) * close_open_ret ** 2\n    ret_v = rolling_windows_vectorized(estimator, window).mean(axis=1)\n    ret_value[len(ret_value)-len(ret_v):] = np.sqrt(ret_v)\n    return ret_value\n\ndef getWindows(n_bars, isSpecialWindow=False, min_bar_length=3):\n    if(True==isSpecialWindow):\n        window_size = n_bars//2\n    else:\n        window_size = n_bars-(min_bar_length-1)\n\n    if(window_size<=0):\n        if(n_bars-1 > 0):\n            window_size = n_bars - 1\n        else:\n            window_size=1\n\n    return window_size\n\ndef getMicrostructuralFeatures(input_df, output_df, col_prefix = '', col_postfix = '', min_bar_length = 3, volumeThreshold = cfg['volumeBarThreshold'], isParkinson=True, isGarman=True, isYyang=True, isBekker=True, isMicro=True, isSpecialWindow=False, micro_cols=[]):\n    \n    v_sum = np.sum(input_df.loc[:, 'size']).astype(np.float64)\n    thres = volumeThreshold if v_sum >= volumeThreshold else v_sum\n    volume_bars = getVolumaBars(input_df.loc[:,['price','size']].to_numpy(), thres)\n\n    window_size = getWindows(len(volume_bars), isSpecialWindow=isSpecialWindow, min_bar_length=min_bar_length)\n\n    if(True == isGarman):\n        col_name = col_prefix+'garman_class_vol'+col_postfix\n        pv = get_garman_class_vol2(volume_bars.open.to_numpy(), volume_bars.high.to_numpy(), volume_bars.low.to_numpy(), volume_bars.close.to_numpy(), window=window_size)\n        pv = pv[~np.isnan(pv)]\n        if(len(pv)>0):\n            output_df.loc[:,col_name] = np.median(pv)\n        else:\n            output_df.loc[:,col_name] = 0.0\n\n    if(True == isBekker):\n        col_name = col_prefix+'bekker_parkinson_vol'+col_postfix\n        pv = get_bekker_parkinson_vol2(volume_bars.high.to_numpy(), volume_bars.low.to_numpy(), window=window_size)\n        pv = pv[~np.isnan(pv)]\n        if(len(pv)>0):\n            output_df.loc[:,col_name] = np.median(pv)\n        else:\n            output_df.loc[:,col_name] = 0.0\n\n    return output_df\n","metadata":{"execution":{"iopub.status.busy":"2021-08-28T09:20:05.586201Z","iopub.execute_input":"2021-08-28T09:20:05.586654Z","iopub.status.idle":"2021-08-28T09:20:05.630997Z","shell.execute_reply.started":"2021-08-28T09:20:05.586603Z","shell.execute_reply":"2021-08-28T09:20:05.629565Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Preprocessing book data","metadata":{"execution":{"iopub.status.busy":"2021-08-01T12:12:38.74616Z","iopub.execute_input":"2021-08-01T12:12:38.746458Z","iopub.status.idle":"2021-08-01T12:12:38.75749Z","shell.execute_reply.started":"2021-08-01T12:12:38.746422Z","shell.execute_reply":"2021-08-01T12:12:38.756642Z"}}},{"cell_type":"code","source":"def getDataFromBidAsk_numpy(df, ci):\n    a = 0\n    b = 0\n    spread  = {}\n    for k in [1,2]:\n        #k = i+1\n        bidp = 'bid_price{}'.format(k)\n        askp = 'ask_price{}'.format(k)\n        bids = 'bid_size{}'.format(k)\n        asks = 'ask_size{}'.format(k)\n        #calculate comulative wap\n        a += (df[:,ci[bidp]] * df[:,ci[asks]] + df[:,ci[askp]] * df[:,ci[bids]])\n        b += df[:,ci[bids]] + df[:,ci[asks]]\n\n        #wap 1 and 2\n        spread[f'fb_w_{k}'] = (df[:,ci[bidp]] * df[:,ci[asks]] + df[:,ci[askp]] * df[:,ci[bids]] ) / (df[:,ci[bids]] + df[:,ci[asks]] + SMALL_F)\n        spread[f'fb_mid_point_{k}'] = (df[:,ci[askp]]) + (df[:,ci[bidp]]) / 2\n        spread[f'fb_volume_total_{k}'] = (df[:,ci[asks]]) + (df[:,ci[bids]])\n\n    \n    # mean wap\n    spread['fb_w'] = (a/(b+SMALL_F))\n    # rates\n    spread['fb_w_rate'] = (spread['fb_w_1']) / (spread['fb_w_2']+SMALL_F) \n    spread['fb_mid_point_rate'] = (spread['fb_mid_point_1']) / (spread['fb_mid_point_2']+SMALL_F)\n    #sum volume\n    spread['fb_volume_total'] = spread['fb_volume_total_1'] + spread['fb_volume_total_2']\n\n    \n    ################# test ##################\n    spread['ask_1'] = df[:,ci['ask_price1']]\n    spread['bid_1'] = df[:,ci['bid_price1']]\n    spread['ask_2'] = df[:,ci['ask_price2']]\n    spread['bid_2'] = df[:,ci['bid_price2']]\n    #########################################\n    \n    return spread\n\n\n\ndef Fx(group, stock_id=0, n=10):\n    new_df = pd.DataFrame()\n    name = int(group.time_id.unique()[0])\n    tmp = pd.DataFrame()\n\n    #calculate log return from the following features:\n    cols = [\n        'fb_w', \n        'fb_w_1', \n        'fb_w_2',\n        'fb_mid_point_1',\n        'fb_mid_point_rate',\n        'fb_w_rate',\n    ]\n\n    new_cols = [s + '_lr' for s in cols]\n    group.loc[:,new_cols] = log_return2(group[cols]).to_numpy()\n    group = group[~group['fb_w'].isnull()]\n\n    #calculate realized volatility\n    cols = new_cols\n    new_cols = [s + '_vola' for s in cols]\n    tmp = pd.concat([tmp, pd.DataFrame(realized_volatility(group.loc[:,cols]).to_numpy().reshape(1,-1), columns=new_cols)], axis=1)\n\n    #calculate sum of log return\n    cols = [\n        'fb_w_2_lr',\n    ]\n    new_cols = [s + '_sum' for s in cols]\n    tmp = pd.concat([tmp, pd.DataFrame(np.sum(group.loc[:,cols]).to_numpy().reshape(1,-1), columns=new_cols)], axis=1)\n\n    #calculate sqsum    \n    cols = [\n        'fb_w_lr', \n        'fb_w_1_lr', \n        'fb_w_2_lr',\n        'fb_mid_point_1_lr',\n    ]\n    new_cols = [s + '_sqsum' for s in cols]\n    tmp = pd.concat([tmp, pd.DataFrame(np.sum((group.loc[:,cols])**2).to_numpy().reshape(1,-1), columns=new_cols)], axis=1)\n    \n    #calculate book length\n    tmp.loc[:,'book_length'] = [group.shape[0]]\n\n    #calclulate market microstructural features\n    c = '1'\n    cols_1 = []\n    new_df = pd.DataFrame({'price': group.loc[:,'fb_mid_point_'+c].to_numpy().flatten(), 'size': group.loc[:,'fb_volume_total_'+c].to_numpy().flatten()}).reset_index()\n    tmp = getMicrostructuralFeatures(new_df, tmp, col_prefix = 'fb_', col_postfix = '_'+c, micro_cols=cols_1)\n    c = '2'\n    cols_2 = []  \n    new_df = pd.DataFrame({'price': group.loc[:,'fb_mid_point_'+c].to_numpy().flatten(), 'size': group.loc[:,'fb_volume_total_'+c].to_numpy().flatten()}).reset_index()\n    tmp = getMicrostructuralFeatures(new_df, tmp, col_prefix = 'fb_', col_postfix = '_'+c, micro_cols=cols_2)\n    \n    ############ test idea ################\n    col_name = \"Test_1\"\n    window_size = getWindows(len(group), isSpecialWindow=False, min_bar_length=3)\n    pv = get_bekker_parkinson_vol2(group.ask_1.to_numpy(), group.bid_1.to_numpy(), window=window_size)\n    pv = pv[~np.isnan(pv)]\n    if(len(pv)>0):\n        tmp.loc[:,col_name] = np.median(pv)\n    else:\n        tmp.loc[:,col_name] = 0.0\n\n    col_name = \"Test_2\"\n    window_size = getWindows(len(group), isSpecialWindow=False, min_bar_length=3)\n    pv = get_bekker_parkinson_vol2(group.ask_2.to_numpy(), group.bid_2.to_numpy(), window=window_size)\n    pv = pv[~np.isnan(pv)]\n    if(len(pv)>0):\n        tmp.loc[:,col_name] = np.median(pv)\n    else:\n        tmp.loc[:,col_name] = 0.0\n    #######################################\n    \n    tmp.loc[:,'row_id'] = str(stock_id) + '-' + str(name)\n    tmp.loc[:,'time_id'] = int(name)\n    return tmp\n\ndef getFeaturesFromBookData(df, stock_id, n=10):\n    results = df.groupby(['time_id']).parallel_apply(Fx, stock_id=stock_id, n=n).reset_index(drop=True)\n    return results","metadata":{"execution":{"iopub.status.busy":"2021-08-28T09:20:06.42666Z","iopub.execute_input":"2021-08-28T09:20:06.427069Z","iopub.status.idle":"2021-08-28T09:20:06.460393Z","shell.execute_reply.started":"2021-08-28T09:20:06.427034Z","shell.execute_reply":"2021-08-28T09:20:06.459011Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Preprocessing trade data","metadata":{"execution":{"iopub.status.busy":"2021-08-01T12:57:19.958282Z","iopub.execute_input":"2021-08-01T12:57:19.95897Z","iopub.status.idle":"2021-08-01T12:57:19.966047Z","shell.execute_reply.started":"2021-08-01T12:57:19.958919Z","shell.execute_reply":"2021-08-01T12:57:19.965072Z"}}},{"cell_type":"code","source":"def getDataFromTrade(df):\n    log_ret = log_return(df.price).dropna()\n    rz_vol = realized_volatility(log_ret)\n    \n    tmp = pd.DataFrame()\n\n    \n    tmp.loc[:,'p_vwap_my'] = [np.sum(df['price'].values*df['size'].values)/(np.sum(df['size'].values+SMALL_F))]\n    tmp.loc[:,'p_rz_vol'] = rz_vol\n    tmp.loc[:,'p_sqsum'] = np.sum(log_ret**2)\n    tmp.loc[:,'p_sum'] = np.sum(log_ret)\n    \n    tmp.loc[:,'p_lr_rate'] = rateLastFirst(log_ret)\n    \n    tmp.loc[:,'p_price_count'] = count_unique(df['price'].to_numpy())\n    tmp.loc[:,'p_sec_count'] = count_unique(df['seconds_in_bucket'].to_numpy())\n\n    cols_p = []  \n    tmp = getMicrostructuralFeatures(df.loc[:, ['price', 'size']], tmp, col_prefix = 'p_', col_postfix = '', isParkinson=False, isGarman=False, isYyang=False, micro_cols=cols_p)\n\n    tmp.loc[:,'p_size_mean'] = np.mean(df['size']) \n\n    time_id = df.time_id.unique()[0]\n    tmp.loc[:,'time_id'] = time_id\n    return tmp\n\ndef getFeaturesFromTradeData(df):\n    return df.groupby(['time_id']).parallel_apply(getDataFromTrade).reset_index(drop=True)","metadata":{"execution":{"iopub.status.busy":"2021-08-28T09:20:07.156606Z","iopub.execute_input":"2021-08-28T09:20:07.15704Z","iopub.status.idle":"2021-08-28T09:20:07.169395Z","shell.execute_reply.started":"2021-08-28T09:20:07.157004Z","shell.execute_reply":"2021-08-28T09:20:07.168145Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Constract \\[stock_id,time_id\\] features\n","metadata":{"execution":{"iopub.status.busy":"2021-08-01T12:57:21.851498Z","iopub.execute_input":"2021-08-01T12:57:21.851874Z","iopub.status.idle":"2021-08-01T12:57:21.858463Z","shell.execute_reply.started":"2021-08-01T12:57:21.851841Z","shell.execute_reply":"2021-08-01T12:57:21.857367Z"}}},{"cell_type":"code","source":"def constructPreprocessedDataFrame(file_path, isTrain):\n    stock_id = file_path.split('=')[1]\n    df_book_data = pd.read_parquet(file_path)\n    if True == isTrain:\n        df_trade_data =  pd.read_parquet(trade_train_files.format(stock_id))\n    else:\n        df_trade_data =  pd.read_parquet(trade_test_files.format(stock_id))\n\n    print('Processing stock id:', stock_id)\n    #display(df_book_data.time_id.unique())\n    #preprocess book\n    a = time.time()\n    spread = getDataFromBidAsk_numpy(df_book_data.to_numpy(),{k: v for v, k in enumerate(df_book_data.columns.values)})\n    df_book_data = pd.concat([df_book_data,pd.DataFrame(spread)], axis=1)\n    df_book_datar = getFeaturesFromBookData(df_book_data, stock_id, 10)\n    b = time.time()\n    #print(f'preprocess book: {b-a}')\n    \n    #preprocess trade\n    df_trade_datar = getFeaturesFromTradeData(df_trade_data)\n    df_book_datar = df_book_datar.merge(df_trade_datar, on = ['time_id'], how = 'left')\n    c = time.time()\n    #print(f'preprocess trade: {c-b}')\n\n    df_book_datar.loc[:,'stock_id'] = stock_id\n    df_book_datar = df_book_datar.fillna(0.0)\n    return df_book_datar\n\ndef constructBookDataDataFrame(list_file, isTrain=True):\n    df_book = pd.DataFrame()\n    for file in list_file:\n        df_book = pd.concat([df_book, constructPreprocessedDataFrame(file, isTrain=isTrain)])\n    return df_book","metadata":{"execution":{"iopub.status.busy":"2021-08-28T09:20:07.818387Z","iopub.execute_input":"2021-08-28T09:20:07.818992Z","iopub.status.idle":"2021-08-28T09:20:07.829146Z","shell.execute_reply.started":"2021-08-28T09:20:07.818941Z","shell.execute_reply":"2021-08-28T09:20:07.828076Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## collect and preprocess data","metadata":{"execution":{"iopub.status.busy":"2021-08-01T12:57:22.699289Z","iopub.execute_input":"2021-08-01T12:57:22.699669Z","iopub.status.idle":"2021-08-01T12:57:22.72061Z","shell.execute_reply.started":"2021-08-01T12:57:22.69963Z","shell.execute_reply":"2021-08-01T12:57:22.719424Z"}}},{"cell_type":"code","source":"list_order_book_file_train = glob.glob(path_to_files + '/book_train.parquet/*')\nlist_order_book_file_train[0:1]","metadata":{"execution":{"iopub.status.busy":"2021-08-28T09:20:08.535506Z","iopub.execute_input":"2021-08-28T09:20:08.536127Z","iopub.status.idle":"2021-08-28T09:20:08.548162Z","shell.execute_reply.started":"2021-08-28T09:20:08.53609Z","shell.execute_reply":"2021-08-28T09:20:08.546973Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nret_df = constructBookDataDataFrame(list_order_book_file_train)\ndisplay(ret_df.shape)\nret_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-28T09:20:08.982974Z","iopub.execute_input":"2021-08-28T09:20:08.983369Z","iopub.status.idle":"2021-08-28T09:25:47.436811Z","shell.execute_reply.started":"2021-08-28T09:20:08.983335Z","shell.execute_reply":"2021-08-28T09:25:47.435707Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## construct features across dataset","metadata":{}},{"cell_type":"code","source":"%%time\ndef getDataFromTransformedDataFx(df, prefix=''):\n    cs = ['row_id', 'stock_id', 'time_id']\n    used_cols = list(set(df.columns.to_list()) - set(cs))\n    for c in used_cols:\n        df.loc[:, prefix+c+'_rate'] = df.loc[:,c] / (np.mean(df.loc[:,c]) + SMALL_F)\n        df.loc[:, prefix+c+'_diff'] = df.loc[:,c] - (np.mean(df.loc[:,c]) )\n\n    return df\n\ndef getDataFromTransformedData(df):\n    tmp1 = df.groupby(['stock_id']).parallel_apply(getDataFromTransformedDataFx, prefix='stock_id_').reset_index(drop=True)\n    cols_tmp1 = [x for x in tmp1.columns if 'stock_id_' in x]\n    cols_tmp1.append('stock_id')\n    cols_tmp1.append('time_id')\n    tmp2 = df.groupby(['time_id']).parallel_apply(getDataFromTransformedDataFx, prefix='time_id_').reset_index(drop=True)\n    cols_tmp2 = [x for x in tmp2.columns if 'time_id_' in x]\n    cols_tmp2.append('stock_id')\n    cols_tmp2.append('time_id')\n\n    df = df.merge(tmp1.loc[:,cols_tmp1], on = ['stock_id', 'time_id'], how = 'left')\n    df = df.merge(tmp2.loc[:,cols_tmp2], on = ['stock_id', 'time_id'], how = 'left')\n    \n    print(df.shape)\n\n    return df\n\nret_df = getDataFromTransformedData(ret_df)\nret_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-28T09:25:47.43883Z","iopub.execute_input":"2021-08-28T09:25:47.439147Z","iopub.status.idle":"2021-08-28T09:28:32.830981Z","shell.execute_reply.started":"2021-08-28T09:25:47.439115Z","shell.execute_reply":"2021-08-28T09:28:32.82973Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cs = ['Unnamed: 0', 'row_id', 'stock_id', 'time_id']\nused_cols = list(set(ret_df.columns.to_list()) - set(cs))\ny_col = 'target'\n\nALL_STOCKS = {k: v for v, k in enumerate(ret_df.stock_id.unique())}","metadata":{"execution":{"iopub.status.busy":"2021-08-28T09:28:32.834988Z","iopub.execute_input":"2021-08-28T09:28:32.835325Z","iopub.status.idle":"2021-08-28T09:28:32.841901Z","shell.execute_reply.started":"2021-08-28T09:28:32.835293Z","shell.execute_reply":"2021-08-28T09:28:32.840976Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"_used_cols = ['fb_w_1_lr_sqsum',\n 'fb_w_1_lr_vola',\n 'fb_w_2_lr_sqsum',\n 'p_rz_vol',\n 'fb_w_lr_vola',\n 'fb_bekker_parkinson_vol_2',\n 'fb_w_2_lr_vola',\n 'time_id_p_lr_rate_diff',\n 'fb_bekker_parkinson_vol_1',\n 'time_id_p_sec_count_diff',\n 'book_length',\n 'time_id_fb_bekker_parkinson_vol_1_diff',\n 'p_price_count',\n 'stock_id_fb_w_1_lr_vola_rate',\n 'time_id_fb_bekker_parkinson_vol_2_diff',\n 'stock_id_book_length_diff',\n 'fb_w_rate_lr_vola',\n 'p_sqsum',\n 'p_lr_rate',\n 'p_bekker_parkinson_vol',\n 'p_sec_count',\n #'time_id_fb_avg_tick_size_2_diff',\n 'stock_id_fb_w_1_lr_vola_diff',\n 'stock_id_p_sqsum_diff',\n 'time_id_fb_w_1_lr_vola_rate',\n 'stock_id_fb_w_rate_lr_vola_rate',\n 'time_id_fb_bekker_parkinson_vol_1_rate',\n 'time_id_p_sqsum_diff',\n 'time_id_book_length_rate',\n 'time_id_book_length_diff',\n 'time_id_fb_bekker_parkinson_vol_2_rate',\n 'time_id_p_bekker_parkinson_vol_diff',\n 'fb_garman_class_vol_1',\n 'fb_mid_point_rate_lr_vola',\n'time_id_p_lr_rate_rate',\n 'stock_id_fb_w_lr_vola_rate',\n 'stock_id_fb_mid_point_1_lr_sqsum_diff',\n 'time_id_fb_w_1_lr_vola_diff',\n 'stock_id_fb_bekker_parkinson_vol_2_diff',\n 'stock_id_fb_mid_point_rate_lr_vola_diff',\n 'time_id_fb_w_lr_vola_diff',\n 'stock_id_fb_mid_point_1_lr_vola_diff',\n 'stock_id_fb_bekker_parkinson_vol_1_diff',\n 'stock_id_fb_w_2_lr_sum_diff',\n 'time_id_fb_w_lr_sqsum_rate',\n 'stock_id_fb_mid_point_1_lr_vola_rate',\n 'stock_id_fb_garman_class_vol_1_rate',\n #'time_id_fb_avg_tick_size_1_diff',\n 'stock_id_fb_w_rate_lr_vola_diff',\n 'time_id_fb_garman_class_vol_2_diff',\n 'stock_id_fb_w_lr_vola_diff',\n 'stock_id_fb_mid_point_rate_lr_vola_rate',\n             \n ###########################################\n 'stock_id_Test_1_diff',\n 'stock_id_Test_1_rate',\n 'Test_1',\n 'time_id_Test_1_diff',\n 'time_id_Test_1_rate',\n 'stock_id_Test_2_diff',\n 'stock_id_Test_2_rate',\n 'Test_2',\n 'time_id_Test_2_diff',\n 'time_id_Test_2_rate'\n##########################################             \n ]","metadata":{"execution":{"iopub.status.busy":"2021-08-28T09:28:32.843344Z","iopub.execute_input":"2021-08-28T09:28:32.843886Z","iopub.status.idle":"2021-08-28T09:28:32.856135Z","shell.execute_reply.started":"2021-08-28T09:28:32.843836Z","shell.execute_reply":"2021-08-28T09:28:32.854684Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#list(set(used_cols2) - set(used_cols))","metadata":{"execution":{"iopub.status.busy":"2021-08-28T09:28:32.857671Z","iopub.execute_input":"2021-08-28T09:28:32.858059Z","iopub.status.idle":"2021-08-28T09:28:32.874569Z","shell.execute_reply.started":"2021-08-28T09:28:32.858023Z","shell.execute_reply":"2021-08-28T09:28:32.873422Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ret_df.loc[:, used_cols].head()","metadata":{"execution":{"iopub.status.busy":"2021-08-28T09:28:32.87602Z","iopub.execute_input":"2021-08-28T09:28:32.876332Z","iopub.status.idle":"2021-08-28T09:28:32.934812Z","shell.execute_reply.started":"2021-08-28T09:28:32.8763Z","shell.execute_reply":"2021-08-28T09:28:32.933873Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model Definition","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv(path_to_files + '/train.csv')\ntrain.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-28T09:28:32.936666Z","iopub.execute_input":"2021-08-28T09:28:32.937399Z","iopub.status.idle":"2021-08-28T09:28:33.094238Z","shell.execute_reply.started":"2021-08-28T09:28:32.937347Z","shell.execute_reply":"2021-08-28T09:28:33.093128Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scaler_target = MinMaxScaler()\ndef getTrainData(ret_df, seed = 42):\n    train = pd.read_csv(path_to_files + '/train.csv')\n    #convert stock_id to the same time as in train data\n    ret_df.stock_id = ret_df.stock_id.astype(int)\n    #merge\n    data_df = ret_df.merge(train, on = ['stock_id', 'time_id'], how = 'left')\n    data_df.loc[:,'target_orig'] = data_df.loc[:,'target'] \n\n    if True == cfg['useLabelTransformation']:\n        data_df.loc[:,'target'] = data_df.loc[:,'target'] * 100\n        scaler_target.fit(data_df.loc[:,'target'].to_numpy().reshape(-1,1))\n        data_df.loc[:,'target'] = scaler_target.transform(data_df.loc[:,'target'].to_numpy().reshape(-1,1)).flatten()\n\n    #get train test index \n    all_time_ids = data_df.time_id.unique()\n\n    train_ids, val_ids = train_test_split(all_time_ids, test_size=0.05, random_state=seed)\n    test_ids, val_ids = train_test_split(val_ids, test_size=0.5, random_state=seed)\n\n    f = data_df.time_id.isin(train_ids)\n    train_df = data_df.loc[f].reset_index(drop=True)\n\n    f = data_df.time_id.isin(val_ids)\n    val_df = data_df.loc[f].reset_index(drop=True)\n\n    f = data_df.time_id.isin(test_ids)\n    test_df = data_df.loc[f].reset_index(drop=True)    \n    \n    return train_df, val_df, test_df\n\ndef predictFromModel(model, df, used_cols=used_cols, prediction_column_name='target'):\n    predict = model.predict(df.loc[:, used_cols].values).flatten()\n    df_ret = pd.DataFrame()\n    df_ret[prediction_column_name] = predict\n    df_ret['row_id'] = df['row_id'].values\n    return df_ret[['row_id', prediction_column_name]].reset_index(drop=True)","metadata":{"execution":{"iopub.status.busy":"2021-08-28T09:28:33.098728Z","iopub.execute_input":"2021-08-28T09:28:33.09905Z","iopub.status.idle":"2021-08-28T09:28:33.113389Z","shell.execute_reply.started":"2021-08-28T09:28:33.099017Z","shell.execute_reply":"2021-08-28T09:28:33.112094Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nimport lightgbm as lgbm\n\ndef feval_RMSPE(preds, train_data):\n    labels = train_data.get_label()\n    return 'RMSPE', round(rmspe(y_true = labels, y_pred = preds),5), False\n\ndef printAndReturnModelDescrAndErrors(model, model_id, y_test, predict_y):\n    R2 = round(r2_score(y_true = y_test, y_pred = predict_y),5)\n    RMSPE = round(rmspe(y_true = y_test, y_pred = predict_y),5)\n    print(f'Model {model_id} Performance of the prediction: R2 score: {R2}, RMSPE: {RMSPE}')\n    \n    return  {'model':model, 'R2':R2, 'RMSPE':RMSPE}\n\ndef getTrainDats(df, df_target, test_size):\n    df_target['row_id'] = df_target['stock_id'].astype(str) + '-' + df_target['time_id'].astype(str)\n    df_joined = df.merge(df_target[['row_id','target']], on = ['row_id'], how = 'left')\n    \n    cols_no_used = cfg['trainNotUsedCols']\n    \n    X_train, X_test, y_train, y_test = train_test_split(df_joined.loc[:, ~df_joined.columns.isin(cols_no_used)], \n                                                        df_joined.target,\n                                                        test_size=test_size, random_state=42)\n    return X_train, X_test, y_train, y_test\n\ndef train_model_lgbm(df_train, df_val, df_test, X_col, y_col, cat_feats, model_id):\n    \n    for c in cat_feats:\n        df_train[c] = df_train[c].astype(int)\n        df_val[c] = df_val[c].astype(int)\n        df_test[c] = df_test[c].astype(int)\n    #print(X_train.columns)\n    train = lgbm.Dataset(df_train.loc[:,X_col], label=df_train.loc[:,y_col], categorical_feature=cat_feats, weight=1/np.power(df_train.loc[:,y_col],2))\n    val = lgbm.Dataset(df_val.loc[:,X_col], label=df_val.loc[:,y_col], categorical_feature=cat_feats, weight=1/np.power(df_val.loc[:,y_col],2))\n    test = lgbm.Dataset(df_test.loc[:,X_col], label=df_test.loc[:,y_col], categorical_feature=cat_feats, weight=1/np.power(df_test.loc[:,y_col],2))\n    \n\n    lgbm_params = {\n        'task': 'train',\n        'boosting_type': 'gbdt',\n        'learning_rate': 0.15, #0.01,\n        'objective': 'regression',\n        'metric': 'None',\n        'max_depth': -1,\n        'n_jobs': -1,\n        'feature_fraction': 0.7,\n        'bagging_fraction': 0.75,\n         'lambda_l1': 1, # L1 regularization\n         'lambda_l2': 1, # L2 Regularization\n         'bagging_seed': 100, # random seed, default in light 100\n        \n    }\n\n    model = lgbm.train(lgbm_params, \n                          train, \n                          50000, \n                          valid_sets=test, \n                          feval=feval_RMSPE,\n                          early_stopping_rounds=1500,\n                          verbose_eval=False,\n                          categorical_feature=cat_feats,\n                         )\n\n    predict_y = model.predict(df_test.loc[:,X_col])\n    \n    model_descr = printAndReturnModelDescrAndErrors(model, model_id, df_test.loc[:,y_col], predict_y)\n\n    return model_descr\n","metadata":{"execution":{"iopub.status.busy":"2021-08-28T09:28:33.11524Z","iopub.execute_input":"2021-08-28T09:28:33.115552Z","iopub.status.idle":"2021-08-28T09:28:33.13615Z","shell.execute_reply.started":"2021-08-28T09:28:33.11552Z","shell.execute_reply":"2021-08-28T09:28:33.134972Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Train Models","metadata":{}},{"cell_type":"code","source":"def trainModel(df, n_models=1, seeds=[]):\n    models_stat = {}\n    for i in range(n_models):\n        if len(seeds) == 0:\n            rnd_seed = np.random.randint(1000)\n        else:\n            rnd_seed = seeds[i]\n        \n        train_df, val_df, test_df = getTrainData(ret_df, seed=rnd_seed)\n        used_cols_lgbm = used_cols.copy()\n        #used_cols_lgbm.append('stock_id')\n        models_stat['model'+str(i)] = train_model_lgbm(train_df, val_df, test_df, used_cols_lgbm, 'target_orig', [], \"lgbm_model\"+str(i))\n    return models_stat","metadata":{"execution":{"iopub.status.busy":"2021-08-28T09:28:33.137658Z","iopub.execute_input":"2021-08-28T09:28:33.138023Z","iopub.status.idle":"2021-08-28T09:28:33.153371Z","shell.execute_reply.started":"2021-08-28T09:28:33.13799Z","shell.execute_reply":"2021-08-28T09:28:33.152075Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"models = trainModel(ret_df, n_models=5, seeds=[42,456,555,333,444])","metadata":{"execution":{"iopub.status.busy":"2021-08-28T09:28:33.154789Z","iopub.execute_input":"2021-08-28T09:28:33.155095Z","iopub.status.idle":"2021-08-28T09:29:48.700386Z","shell.execute_reply.started":"2021-08-28T09:28:33.155053Z","shell.execute_reply":"2021-08-28T09:29:48.69914Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def predictFromModels(models_stat, df, used_cols=used_cols, weight=[]):\n    predictions = None\n    if(len(models_stat.keys()) == 1):\n        predictions = predictFromModel(models_stat['model0']['model'], df)\n    else:\n        isFirst=True\n        w = 1/len(models_stat.keys())\n        for k in models_stat.keys():\n            if(isFirst==True):\n                isFirst=False\n                predictions = predictFromModel(models_stat[k]['model'], df)\n                predictions.loc[:,'target'] = predictions.loc[:,'target'] * w\n            else:\n                p = predictFromModel(models_stat[k]['model'], df).loc[:, 'target']\n                predictions.loc[:,'target'] += p * w\n                \n    return predictions","metadata":{"execution":{"iopub.status.busy":"2021-08-28T09:29:48.702363Z","iopub.execute_input":"2021-08-28T09:29:48.702878Z","iopub.status.idle":"2021-08-28T09:29:48.711369Z","shell.execute_reply.started":"2021-08-28T09:29:48.702837Z","shell.execute_reply":"2021-08-28T09:29:48.710344Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Test Prediction","metadata":{}},{"cell_type":"code","source":"train_df, val_df, test_df = getTrainData(ret_df, seed=777)\ntt = predictFromModels(models, test_df)\npredict_y = tt.loc[:,'target']\ntest_y = test_df.loc[:,y_col]\n_ = printAndReturnModelDescrAndErrors(None, \"lgbms\", test_y, predict_y)","metadata":{"execution":{"iopub.status.busy":"2021-08-28T09:29:48.712737Z","iopub.execute_input":"2021-08-28T09:29:48.71319Z","iopub.status.idle":"2021-08-28T09:29:49.068038Z","shell.execute_reply.started":"2021-08-28T09:29:48.713157Z","shell.execute_reply":"2021-08-28T09:29:49.066679Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Prepare test data","metadata":{}},{"cell_type":"code","source":"list_order_book_file_train = glob.glob(path_to_files + '/book_test.parquet/*')\nlist_order_book_file_train","metadata":{"execution":{"iopub.status.busy":"2021-08-28T09:29:49.069729Z","iopub.execute_input":"2021-08-28T09:29:49.070159Z","iopub.status.idle":"2021-08-28T09:29:49.079912Z","shell.execute_reply.started":"2021-08-28T09:29:49.070125Z","shell.execute_reply":"2021-08-28T09:29:49.078543Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nt_df = constructBookDataDataFrame(list_order_book_file_train, isTrain=False)\nt_df = getDataFromTransformedData(t_df)\ndisplay(t_df.shape)\nt_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-28T09:29:49.081731Z","iopub.execute_input":"2021-08-28T09:29:49.082218Z","iopub.status.idle":"2021-08-28T09:29:49.937124Z","shell.execute_reply.started":"2021-08-28T09:29:49.082168Z","shell.execute_reply":"2021-08-28T09:29:49.935627Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Predict test","metadata":{}},{"cell_type":"code","source":"tmp = predictFromModels(models, t_df)\n\ntmp.to_csv('submission.csv',index = False)\ntmp.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-28T09:29:49.939377Z","iopub.execute_input":"2021-08-28T09:29:49.939756Z","iopub.status.idle":"2021-08-28T09:29:49.990094Z","shell.execute_reply.started":"2021-08-28T09:29:49.939714Z","shell.execute_reply":"2021-08-28T09:29:49.988884Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Feature Importance","metadata":{}},{"cell_type":"code","source":"#models\nfor k,v in models.items():\n    print(f\"{k} - R2: {v['R2']} RMSPE: {v['RMSPE']}\")\n    lgbm.plot_importance(v['model'], figsize=(10,25), importance_type='gain')","metadata":{"execution":{"iopub.status.busy":"2021-08-28T09:29:49.991549Z","iopub.execute_input":"2021-08-28T09:29:49.991938Z","iopub.status.idle":"2021-08-28T09:30:14.669421Z","shell.execute_reply.started":"2021-08-28T09:29:49.991903Z","shell.execute_reply":"2021-08-28T09:30:14.668037Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#models\nfor k,v in models.items():\n    print(f\"{k} - R2: {v['R2']} RMSPE: {v['RMSPE']}\")\n    lgbm.plot_importance(v['model'], figsize=(10,25), importance_type='split')","metadata":{"execution":{"iopub.status.busy":"2021-08-28T09:30:14.670769Z","iopub.execute_input":"2021-08-28T09:30:14.671077Z","iopub.status.idle":"2021-08-28T09:30:38.867929Z","shell.execute_reply.started":"2021-08-28T09:30:14.671043Z","shell.execute_reply":"2021-08-28T09:30:38.866525Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## SHAP Feature Importance","metadata":{}},{"cell_type":"code","source":"import shap\n\nseeds=[42,456,555,333,444]\nfor (k,v),s in zip(models.items(), seeds):\n    train_df, val_df, test_df = getTrainData(ret_df, seed=s)\n    X_train = train_df.loc[:,used_cols]\n    explainer = shap.TreeExplainer(v['model'])\n    shap_values = explainer.shap_values(X_train)\n    shap.summary_plot(shap_values, X_train, max_display=40)","metadata":{"execution":{"iopub.status.busy":"2021-08-28T09:34:33.509912Z","iopub.execute_input":"2021-08-28T09:34:33.510365Z","iopub.status.idle":"2021-08-28T09:35:22.101675Z","shell.execute_reply.started":"2021-08-28T09:34:33.510329Z","shell.execute_reply":"2021-08-28T09:35:22.100475Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}