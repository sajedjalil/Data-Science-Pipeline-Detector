{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import gc\nimport glob\nimport os\nimport time\nimport traceback\nfrom contextlib import contextmanager\nfrom enum import Enum\nfrom typing import Dict, List, Optional, Tuple\n\nimport seaborn as sns\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport lightgbm as lgb\nfrom IPython.display import display\n\nfrom joblib import delayed, Parallel\nfrom sklearn.decomposition import LatentDirichletAllocation\nfrom sklearn.manifold import TSNE\nfrom sklearn.model_selection import GroupKFold\nfrom sklearn.neighbors import NearestNeighbors\nfrom sklearn.preprocessing import minmax_scale\nfrom tqdm import tqdm_notebook as tqdm\n\n\n%matplotlib inline\n\nDATA_DIR = '../input'\n\n# data configurations\nUSE_PRECOMPUTE_FEATURES = True  # Load precomputed features for train.csv from private dataset (just for speed up)\n\n# model & ensemble configurations\nPREDICT_CNN = True\nPREDICT_MLP = True\nPREDICT_GBDT = True\nPREDICT_TABNET = False\n\nGBDT_NUM_MODELS = 5 #3\nGBDT_LR = 0.02  # 0.1\n\nNN_VALID_TH = 0.185\nNN_MODEL_TOP_N = 3\nTAB_MODEL_TOP_N = 3\nENSEMBLE_METHOD = 'mean'\nNN_NUM_MODELS = 10\nTABNET_NUM_MODELS = 5\n\n# for saving quota\nIS_1ST_STAGE = False\nSHORTCUT_NN_IN_1ST_STAGE = False  # early-stop training to save GPU quota\nSHORTCUT_GBDT_IN_1ST_STAGE = False\nMEMORY_TEST_MODE = False\n\n# for ablation studies\nCV_SPLIT = 'time'  # 'time': time-series KFold 'group': GroupKFold by stock-id\nUSE_PRICE_NN_FEATURES = True  # Use nearest neighbor features that rely on tick size\nUSE_VOL_NN_FEATURES = True  # Use nearest neighbor features that can be calculated without tick size\nUSE_SIZE_NN_FEATURES = True  # Use nearest neighbor features that can be calculated without tick size\nUSE_RANDOM_NN_FEATURES = False  # Use random index to aggregate neighbors\n\nUSE_TIME_ID_NN = True  # Use time-id based neighbors\nUSE_STOCK_ID_NN = True  # Use stock-id based neighbors\n\nENABLE_RANK_NORMALIZATION = True  # Enable rank-normalization\n\n\n@contextmanager\ndef timer(name: str):\n    s = time.time()\n    yield\n    elapsed = time.time() - s\n    print(f'[{name}] {elapsed: .3f}sec')\n    \ndef print_trace(name: str = ''):\n    print(f'ERROR RAISED IN {name or \"anonymous\"}')\n    print(traceback.format_exc())","metadata":{"execution":{"iopub.execute_input":"2022-01-20T23:03:11.658615Z","iopub.status.busy":"2022-01-20T23:03:11.648909Z","iopub.status.idle":"2022-01-20T23:03:14.820131Z","shell.execute_reply":"2022-01-20T23:03:14.821325Z","shell.execute_reply.started":"2022-01-19T11:20:17.036084Z"},"papermill":{"duration":3.209095,"end_time":"2022-01-20T23:03:14.821667","exception":false,"start_time":"2022-01-20T23:03:11.612572","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip -q install ../input/pytorchtabnet/pytorch_tabnet-2.0.1-py3-none-any.whl","metadata":{"execution":{"iopub.execute_input":"2022-01-20T23:03:14.929785Z","iopub.status.busy":"2022-01-20T23:03:14.928265Z","iopub.status.idle":"2022-01-20T23:03:41.972925Z","shell.execute_reply":"2022-01-20T23:03:41.973843Z","shell.execute_reply.started":"2022-01-19T11:20:20.281019Z"},"papermill":{"duration":27.10141,"end_time":"2022-01-20T23:03:41.97402","exception":false,"start_time":"2022-01-20T23:03:14.87261","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv(os.path.join(DATA_DIR, 'optiver-realized-volatility-prediction', 'train.csv'))\nstock_ids = set(train['stock_id'])","metadata":{"execution":{"iopub.execute_input":"2022-01-20T23:03:42.047337Z","iopub.status.busy":"2022-01-20T23:03:42.046731Z","iopub.status.idle":"2022-01-20T23:03:42.371511Z","shell.execute_reply":"2022-01-20T23:03:42.37196Z","shell.execute_reply.started":"2022-01-19T11:20:47.625381Z"},"papermill":{"duration":0.364096,"end_time":"2022-01-20T23:03:42.372142","exception":false,"start_time":"2022-01-20T23:03:42.008046","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Feature Engineering\n\n### Base Features","metadata":{"papermill":{"duration":0.027862,"end_time":"2022-01-20T23:03:42.428604","exception":false,"start_time":"2022-01-20T23:03:42.400742","status":"completed"},"tags":[]}},{"cell_type":"code","source":"class DataBlock(Enum):\n    TRAIN = 1\n    TEST = 2\n    BOTH = 3\n\n\ndef load_stock_data(stock_id: int, directory: str) -> pd.DataFrame:\n    return pd.read_parquet(os.path.join(DATA_DIR, 'optiver-realized-volatility-prediction', directory, f'stock_id={stock_id}'))\n\n\ndef load_data(stock_id: int, stem: str, block: DataBlock) -> pd.DataFrame:\n    if block == DataBlock.TRAIN:\n        return load_stock_data(stock_id, f'{stem}_train.parquet')\n    elif block == DataBlock.TEST:\n        return load_stock_data(stock_id, f'{stem}_test.parquet')\n    else:\n        return pd.concat([\n            load_data(stock_id, stem, DataBlock.TRAIN),\n            load_data(stock_id, stem, DataBlock.TEST)\n        ]).reset_index(drop=True)\n\ndef load_book(stock_id: int, block: DataBlock=DataBlock.TRAIN) -> pd.DataFrame:\n    return load_data(stock_id, 'book', block)\n\n\ndef load_trade(stock_id: int, block=DataBlock.TRAIN) -> pd.DataFrame:\n    return load_data(stock_id, 'trade', block)\n\n\ndef calc_wap1(df: pd.DataFrame) -> pd.Series:\n    wap = (df['bid_price1'] * df['ask_size1'] + df['ask_price1'] * df['bid_size1']) / (df['bid_size1'] + df['ask_size1'])\n    return wap\n\n\ndef calc_wap2(df: pd.DataFrame) -> pd.Series:\n    wap = (df['bid_price2'] * df['ask_size2'] + df['ask_price2'] * df['bid_size2']) / (df['bid_size2'] + df['ask_size2'])\n    return wap\n\n\ndef realized_volatility(series):\n    return np.sqrt(np.sum(series**2))\n\n\ndef log_return(series: np.ndarray):\n    return np.log(series).diff()\n\n\ndef log_return_df2(series: np.ndarray):\n    return np.log(series).diff(2)\n\n\ndef flatten_name(prefix, src_names):\n    ret = []\n    for c in src_names:\n        if c[0] in ['time_id', 'stock_id']:\n            ret.append(c[0])\n        else:\n            ret.append('.'.join([prefix] + list(c)))\n    return ret\n\n\ndef make_book_feature(stock_id, block = DataBlock.TRAIN):\n    book = load_book(stock_id, block)\n\n    book['wap1'] = calc_wap1(book)\n    book['wap2'] = calc_wap2(book)\n    book['log_return1'] = book.groupby(['time_id'])['wap1'].apply(log_return)\n    book['log_return2'] = book.groupby(['time_id'])['wap2'].apply(log_return)\n    book['log_return_ask1'] = book.groupby(['time_id'])['ask_price1'].apply(log_return)\n    book['log_return_ask2'] = book.groupby(['time_id'])['ask_price2'].apply(log_return)\n    book['log_return_bid1'] = book.groupby(['time_id'])['bid_price1'].apply(log_return)\n    book['log_return_bid2'] = book.groupby(['time_id'])['bid_price2'].apply(log_return)\n\n    book['wap_balance'] = abs(book['wap1'] - book['wap2'])\n    book['price_spread'] = (book['ask_price1'] - book['bid_price1']) / ((book['ask_price1'] + book['bid_price1']) / 2)\n    book['bid_spread'] = book['bid_price1'] - book['bid_price2']\n    book['ask_spread'] = book['ask_price1'] - book['ask_price2']\n    book['total_volume'] = (book['ask_size1'] + book['ask_size2']) + (book['bid_size1'] + book['bid_size2'])\n    book['volume_imbalance'] = abs((book['ask_size1'] + book['ask_size2']) - (book['bid_size1'] + book['bid_size2']))\n    \n    features = {\n        'seconds_in_bucket': ['count'],\n        'wap1': [np.sum, np.mean, np.std],\n        'wap2': [np.sum, np.mean, np.std],\n        'log_return1': [np.sum, realized_volatility, np.mean, np.std],\n        'log_return2': [np.sum, realized_volatility, np.mean, np.std],\n        'log_return_ask1': [np.sum, realized_volatility, np.mean, np.std],\n        'log_return_ask2': [np.sum, realized_volatility, np.mean, np.std],\n        'log_return_bid1': [np.sum, realized_volatility, np.mean, np.std],\n        'log_return_bid2': [np.sum, realized_volatility, np.mean, np.std],\n        'wap_balance': [np.sum, np.mean, np.std],\n        'price_spread':[np.sum, np.mean, np.std],\n        'bid_spread':[np.sum, np.mean, np.std],\n        'ask_spread':[np.sum, np.mean, np.std],\n        'total_volume':[np.sum, np.mean, np.std],\n        'volume_imbalance':[np.sum, np.mean, np.std]\n    }\n    \n    agg = book.groupby('time_id').agg(features).reset_index(drop=False)\n    agg.columns = flatten_name('book', agg.columns)\n    agg['stock_id'] = stock_id\n    \n    for time in [450, 300, 150]:\n        d = book[book['seconds_in_bucket'] >= time].groupby('time_id').agg(features).reset_index(drop=False)\n        d.columns = flatten_name(f'book_{time}', d.columns)\n        agg = pd.merge(agg, d, on='time_id', how='left')\n    return agg\n\n\ndef make_trade_feature(stock_id, block = DataBlock.TRAIN):\n    trade = load_trade(stock_id, block)\n    trade['log_return'] = trade.groupby('time_id')['price'].apply(log_return)\n\n    features = {\n        'log_return':[realized_volatility],\n        'seconds_in_bucket':['count'],\n        'size':[np.sum],\n        'order_count':[np.mean],\n    }\n\n    agg = trade.groupby('time_id').agg(features).reset_index()\n    agg.columns = flatten_name('trade', agg.columns)\n    agg['stock_id'] = stock_id\n        \n    for time in [450, 300, 150]:\n        d = trade[trade['seconds_in_bucket'] >= time].groupby('time_id').agg(features).reset_index(drop=False)\n        d.columns = flatten_name(f'trade_{time}', d.columns)\n        agg = pd.merge(agg, d, on='time_id', how='left')\n    return agg\n\n\ndef make_book_feature_v2(stock_id, block = DataBlock.TRAIN):\n    book = load_book(stock_id, block)\n\n    prices = book.set_index('time_id')[['bid_price1', 'ask_price1', 'bid_price2', 'ask_price2']]\n    time_ids = list(set(prices.index))\n\n    ticks = {}\n    for tid in time_ids:\n        try:\n            price_list = prices.loc[tid].values.flatten()\n            price_diff = sorted(np.diff(sorted(set(price_list))))\n            ticks[tid] = price_diff[0]\n        except Exception:\n            print_trace(f'tid={tid}')\n            ticks[tid] = np.nan\n        \n    dst = pd.DataFrame()\n    dst['time_id'] = np.unique(book['time_id'])\n    dst['stock_id'] = stock_id\n    dst['tick_size'] = dst['time_id'].map(ticks)\n\n    return dst\n\n\ndef make_features(base, block):\n    stock_ids = set(base['stock_id'])\n    with timer('books'):\n        books = Parallel(n_jobs=-1)(delayed(make_book_feature)(i, block) for i in stock_ids)\n        book = pd.concat(books)\n\n    with timer('trades'):\n        trades = Parallel(n_jobs=-1)(delayed(make_trade_feature)(i, block) for i in stock_ids)\n        trade = pd.concat(trades)\n\n    with timer('extra features'):\n        df = pd.merge(base, book, on=['stock_id', 'time_id'], how='left')\n        df = pd.merge(df, trade, on=['stock_id', 'time_id'], how='left')\n        #df = make_extra_features(df)\n\n    return df\n\n\ndef make_features_v2(base, block):\n    stock_ids = set(base['stock_id'])\n    with timer('books(v2)'):\n        books = Parallel(n_jobs=-1)(delayed(make_book_feature_v2)(i, block) for i in stock_ids)\n        book_v2 = pd.concat(books)\n\n    d = pd.merge(base, book_v2, on=['stock_id', 'time_id'], how='left')\n    return d","metadata":{"_kg_hide-input":true,"execution":{"iopub.execute_input":"2022-01-20T23:03:42.511745Z","iopub.status.busy":"2022-01-20T23:03:42.503925Z","iopub.status.idle":"2022-01-20T23:03:42.527188Z","shell.execute_reply":"2022-01-20T23:03:42.526681Z","shell.execute_reply.started":"2022-01-19T11:20:47.920189Z"},"papermill":{"duration":0.070204,"end_time":"2022-01-20T23:03:42.527311","exception":false,"start_time":"2022-01-20T23:03:42.457107","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if USE_PRECOMPUTE_FEATURES:\n    with timer('load feather'):\n        df = pd.read_feather(os.path.join(DATA_DIR, 'optiver-df2', 'features_v2.f'))\nelse:\n    df = make_features(train, DataBlock.TRAIN)\n    # v2\n    df = make_features_v2(df, DataBlock.TRAIN)\n\ndf.to_feather('features_v2.f')  # save cache\n\ntest = pd.read_csv(os.path.join(DATA_DIR, 'optiver-realized-volatility-prediction', 'test.csv'))\nif len(test) == 3:\n    print('is 1st stage')\n    IS_1ST_STAGE = True\n\nif IS_1ST_STAGE and MEMORY_TEST_MODE:\n    print('use copy of training data as test data to immitate 2nd stage RAM usage.')\n    test_df = df.iloc[:170000].copy()\n    test_df['time_id'] += 32767\n    test_df['row_id'] = ''\nelse:\n    test_df = make_features(test, DataBlock.TEST)\n    test_df = make_features_v2(test_df, DataBlock.TEST)\n\nprint(df.shape)\nprint(test_df.shape)\ndf = pd.concat([df, test_df.drop('row_id', axis=1)]).reset_index(drop=True)","metadata":{"execution":{"iopub.execute_input":"2022-01-20T23:03:42.611386Z","iopub.status.busy":"2022-01-20T23:03:42.610622Z","iopub.status.idle":"2022-01-20T23:03:53.474698Z","shell.execute_reply":"2022-01-20T23:03:53.473803Z","shell.execute_reply.started":"2022-01-19T11:20:47.961136Z"},"papermill":{"duration":10.905781,"end_time":"2022-01-20T23:03:53.47487","exception":false,"start_time":"2022-01-20T23:03:42.569089","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Nearest-Neighbor Features","metadata":{"papermill":{"duration":0.029071,"end_time":"2022-01-20T23:03:53.533838","exception":false,"start_time":"2022-01-20T23:03:53.504767","status":"completed"},"tags":[]}},{"cell_type":"code","source":"N_NEIGHBORS_MAX = 80\n\nclass Neighbors:\n    def __init__(self, \n                 name: str, \n                 pivot: pd.DataFrame, \n                 p: float, \n                 metric: str = 'minkowski', \n                 metric_params: Optional[Dict] = None, \n                 exclude_self: bool = False):\n        self.name = name\n        self.exclude_self = exclude_self\n        self.p = p\n        self.metric = metric\n        \n        if metric == 'random':\n            n_queries = len(pivot)\n            self.neighbors = np.random.randint(n_queries, size=(n_queries, N_NEIGHBORS_MAX))\n        else:\n            nn = NearestNeighbors(\n                n_neighbors=N_NEIGHBORS_MAX, \n                p=p, \n                metric=metric, \n                metric_params=metric_params\n            )\n            nn.fit(pivot)\n            _, self.neighbors = nn.kneighbors(pivot, return_distance=True)\n\n        self.columns = self.index = self.feature_values = self.feature_col = None\n\n    def rearrange_feature_values(self, df: pd.DataFrame, feature_col: str) -> None:\n        raise NotImplementedError()\n\n    def make_nn_feature(self, n=5, agg=np.mean) -> pd.DataFrame:\n        assert self.feature_values is not None, \"should call rearrange_feature_values beforehand\"\n\n        start = 1 if self.exclude_self else 0\n\n        pivot_aggs = pd.DataFrame(\n            agg(self.feature_values[start:n,:,:], axis=0), \n            columns=self.columns, \n            index=self.index\n        )\n\n        dst = pivot_aggs.unstack().reset_index()\n        dst.columns = ['stock_id', 'time_id', f'{self.feature_col}_nn{n}_{self.name}_{agg.__name__}']\n        return dst\n\n\nclass TimeIdNeighbors(Neighbors):\n    def rearrange_feature_values(self, df: pd.DataFrame, feature_col: str) -> None:\n        feature_pivot = df.pivot('time_id', 'stock_id', feature_col)\n        feature_pivot = feature_pivot.fillna(feature_pivot.mean())\n        feature_pivot.head()\n\n        feature_values = np.zeros((N_NEIGHBORS_MAX, *feature_pivot.shape))\n\n        for i in range(N_NEIGHBORS_MAX):\n            feature_values[i, :, :] += feature_pivot.values[self.neighbors[:, i], :]\n\n        self.columns = list(feature_pivot.columns)\n        self.index = list(feature_pivot.index)\n        self.feature_values = feature_values\n        self.feature_col = feature_col\n        \n    def __repr__(self) -> str:\n        return f\"time-id NN (name={self.name}, metric={self.metric}, p={self.p})\"\n\n\nclass StockIdNeighbors(Neighbors):\n    def rearrange_feature_values(self, df: pd.DataFrame, feature_col: str) -> None:\n        \"\"\"stock-id based nearest neighbor features\"\"\"\n        feature_pivot = df.pivot('time_id', 'stock_id', feature_col)\n        feature_pivot = feature_pivot.fillna(feature_pivot.mean())\n\n        feature_values = np.zeros((N_NEIGHBORS_MAX, *feature_pivot.shape))\n\n        for i in range(N_NEIGHBORS_MAX):\n            feature_values[i, :, :] += feature_pivot.values[:, self.neighbors[:, i]]\n\n        self.columns = list(feature_pivot.columns)\n        self.index = list(feature_pivot.index)\n        self.feature_values = feature_values\n        self.feature_col = feature_col\n        \n    def __repr__(self) -> str:\n        return f\"stock-id NN (name={self.name}, metric={self.metric}, p={self.p})\"\n","metadata":{"execution":{"iopub.execute_input":"2022-01-20T23:03:53.612496Z","iopub.status.busy":"2022-01-20T23:03:53.610863Z","iopub.status.idle":"2022-01-20T23:03:53.6133Z","shell.execute_reply":"2022-01-20T23:03:53.613762Z","shell.execute_reply.started":"2022-01-19T11:20:58.104849Z"},"papermill":{"duration":0.050397,"end_time":"2022-01-20T23:03:53.613909","exception":false,"start_time":"2022-01-20T23:03:53.563512","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# the tau itself is meaningless for GBDT, but useful as input to aggregate in Nearest Neighbor features\ndf['trade.tau'] = np.sqrt(1 / df['trade.seconds_in_bucket.count'])\ndf['trade_150.tau'] = np.sqrt(1 / df['trade_150.seconds_in_bucket.count'])\ndf['book.tau'] = np.sqrt(1 / df['book.seconds_in_bucket.count'])\ndf['real_price'] = 0.01 / df['tick_size']","metadata":{"execution":{"iopub.execute_input":"2022-01-20T23:03:53.678641Z","iopub.status.busy":"2022-01-20T23:03:53.677539Z","iopub.status.idle":"2022-01-20T23:03:54.041645Z","shell.execute_reply":"2022-01-20T23:03:54.041184Z","shell.execute_reply.started":"2022-01-19T11:20:58.127333Z"},"papermill":{"duration":0.398072,"end_time":"2022-01-20T23:03:54.041812","exception":false,"start_time":"2022-01-20T23:03:53.64374","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Build Nearest Neighbors","metadata":{"execution":{"iopub.execute_input":"2022-01-16T02:18:50.195022Z","iopub.status.busy":"2022-01-16T02:18:50.1946Z","iopub.status.idle":"2022-01-16T02:18:50.201136Z","shell.execute_reply":"2022-01-16T02:18:50.199965Z","shell.execute_reply.started":"2022-01-16T02:18:50.194964Z"},"papermill":{"duration":0.029003,"end_time":"2022-01-20T23:03:54.109447","exception":false,"start_time":"2022-01-20T23:03:54.080444","status":"completed"},"tags":[]}},{"cell_type":"code","source":"time_id_neighbors: List[Neighbors] = []\nstock_id_neighbors: List[Neighbors] = []\n\nwith timer('knn fit'):\n    df_pv = df[['stock_id', 'time_id']].copy()\n    df_pv['price'] = 0.01 / df['tick_size']\n    df_pv['vol'] = df['book.log_return1.realized_volatility']\n    df_pv['trade.tau'] = df['trade.tau']\n    df_pv['trade.size.sum'] = df['book.total_volume.sum']\n\n    if USE_PRICE_NN_FEATURES:\n        pivot = df_pv.pivot('time_id', 'stock_id', 'price')\n        pivot = pivot.fillna(pivot.mean())\n        pivot = pd.DataFrame(minmax_scale(pivot))\n\n        time_id_neighbors.append(\n            TimeIdNeighbors(\n                'time_price_c', \n                pivot, \n                p=2, \n                metric='canberra', \n                exclude_self=True\n            )\n        )\n        time_id_neighbors.append(\n            TimeIdNeighbors(\n                'time_price_m', \n                pivot, \n                p=2, \n                metric='mahalanobis',\n                metric_params={'V':np.cov(pivot.values.T)}\n            )\n        )\n        stock_id_neighbors.append(\n            StockIdNeighbors(\n                'stock_price_l1', \n                minmax_scale(pivot.transpose()), \n                p=1, \n                exclude_self=True)\n        )\n\n    if USE_VOL_NN_FEATURES:\n        pivot = df_pv.pivot('time_id', 'stock_id', 'vol')\n        pivot = pivot.fillna(pivot.mean())\n        pivot = pd.DataFrame(minmax_scale(pivot))\n\n        time_id_neighbors.append(\n            TimeIdNeighbors('time_vol_l1', pivot, p=1)\n        )\n        stock_id_neighbors.append(\n            StockIdNeighbors(\n                'stock_vol_l1', \n                minmax_scale(pivot.transpose()), \n                p=1, \n                exclude_self=True\n            )\n        )\n\n    if USE_SIZE_NN_FEATURES:\n        pivot = df_pv.pivot('time_id', 'stock_id', 'trade.size.sum')\n        pivot = pivot.fillna(pivot.mean())\n        pivot = pd.DataFrame(minmax_scale(pivot))\n\n        time_id_neighbors.append(\n            TimeIdNeighbors(\n                'time_size_m', \n                pivot, \n                p=2, \n                metric='mahalanobis', \n                metric_params={'V':np.cov(pivot.values.T)}\n            )\n        )\n        time_id_neighbors.append(\n            TimeIdNeighbors(\n                'time_size_c', \n                pivot, \n                p=2, \n                metric='canberra'\n            )\n        )\n        \n    if USE_RANDOM_NN_FEATURES:\n        pivot = df_pv.pivot('time_id', 'stock_id', 'vol')\n        pivot = pivot.fillna(pivot.mean())\n        pivot = pd.DataFrame(minmax_scale(pivot))\n\n        time_id_neighbors.append(\n            TimeIdNeighbors(\n                'time_random', \n                pivot, \n                p=2, \n                metric='random'\n            )\n        )\n        stock_id_neighbors.append(\n            StockIdNeighbors(\n                'stock_random', \n                pivot.transpose(), \n                p=2,\n                metric='random')\n        )\n\n\nif not USE_TIME_ID_NN:\n    time_id_neighbors = []\n    \nif not USE_STOCK_ID_NN:\n    stock_id_neighbors = []","metadata":{"execution":{"iopub.execute_input":"2022-01-20T23:03:54.184295Z","iopub.status.busy":"2022-01-20T23:03:54.182983Z","iopub.status.idle":"2022-01-20T23:10:51.580207Z","shell.execute_reply":"2022-01-20T23:10:51.580636Z","shell.execute_reply.started":"2022-01-19T11:20:58.499414Z"},"papermill":{"duration":417.442164,"end_time":"2022-01-20T23:10:51.58081","exception":false,"start_time":"2022-01-20T23:03:54.138646","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Check Neighbor Indices","metadata":{"papermill":{"duration":0.028883,"end_time":"2022-01-20T23:10:51.63897","exception":false,"start_time":"2022-01-20T23:10:51.610087","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def calculate_rank_correraltion(neighbors, top_n=5):\n    if not neighbors:\n        return\n    neighbor_indices = pd.DataFrame()\n    for n in neighbors:\n        neighbor_indices[n.name] = n.neighbors[:,:top_n].flatten()\n\n    sns.heatmap(neighbor_indices.corr('kendall'), annot=True)","metadata":{"execution":{"iopub.execute_input":"2022-01-20T23:10:51.702674Z","iopub.status.busy":"2022-01-20T23:10:51.702136Z","iopub.status.idle":"2022-01-20T23:10:51.706329Z","shell.execute_reply":"2022-01-20T23:10:51.705907Z","shell.execute_reply.started":"2022-01-19T11:27:55.548287Z"},"papermill":{"duration":0.038441,"end_time":"2022-01-20T23:10:51.706442","exception":false,"start_time":"2022-01-20T23:10:51.668001","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"time_ids = np.array(sorted(df['time_id'].unique()))\nfor neighbor in time_id_neighbors:\n    print(neighbor)\n    display(\n        pd.DataFrame(\n            time_ids[neighbor.neighbors[:,:10]], \n            index=pd.Index(time_ids, name='time_id'), \n            columns=[f'top_{i+1}' for i in range(10)]\n        ).iloc[1:6]\n    )","metadata":{"execution":{"iopub.execute_input":"2022-01-20T23:10:51.770734Z","iopub.status.busy":"2022-01-20T23:10:51.769955Z","iopub.status.idle":"2022-01-20T23:10:51.826818Z","shell.execute_reply":"2022-01-20T23:10:51.826355Z","shell.execute_reply.started":"2022-01-19T11:27:55.555683Z"},"papermill":{"duration":0.091343,"end_time":"2022-01-20T23:10:51.826939","exception":false,"start_time":"2022-01-20T23:10:51.735596","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"stock_ids = np.array(sorted(df['stock_id'].unique()))\nfor neighbor in stock_id_neighbors:\n    print(neighbor)\n    display(\n        pd.DataFrame(\n            stock_ids[neighbor.neighbors[:,:10]], \n            index=pd.Index(stock_ids, name='stock_id'), \n            columns=[f'top_{i+1}' for i in range(10)]\n        ).loc[64]\n    )\n    ","metadata":{"execution":{"iopub.execute_input":"2022-01-20T23:10:51.898989Z","iopub.status.busy":"2022-01-20T23:10:51.89801Z","iopub.status.idle":"2022-01-20T23:10:51.912134Z","shell.execute_reply":"2022-01-20T23:10:51.911654Z","shell.execute_reply.started":"2022-01-19T11:39:40.610534Z"},"papermill":{"duration":0.052221,"end_time":"2022-01-20T23:10:51.912254","exception":false,"start_time":"2022-01-20T23:10:51.860033","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"calculate_rank_correraltion(time_id_neighbors)","metadata":{"execution":{"iopub.execute_input":"2022-01-20T23:10:51.985432Z","iopub.status.busy":"2022-01-20T23:10:51.984581Z","iopub.status.idle":"2022-01-20T23:10:52.415817Z","shell.execute_reply":"2022-01-20T23:10:52.416246Z","shell.execute_reply.started":"2022-01-18T14:13:43.542166Z"},"papermill":{"duration":0.469775,"end_time":"2022-01-20T23:10:52.416393","exception":false,"start_time":"2022-01-20T23:10:51.946618","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"calculate_rank_correraltion(stock_id_neighbors)","metadata":{"execution":{"iopub.execute_input":"2022-01-20T23:10:52.502069Z","iopub.status.busy":"2022-01-20T23:10:52.501121Z","iopub.status.idle":"2022-01-20T23:10:52.780726Z","shell.execute_reply":"2022-01-20T23:10:52.780234Z","shell.execute_reply.started":"2022-01-18T14:13:43.949648Z"},"papermill":{"duration":0.325998,"end_time":"2022-01-20T23:10:52.780878","exception":false,"start_time":"2022-01-20T23:10:52.45488","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Aggregate Features With Neighbors","metadata":{"papermill":{"duration":0.04233,"end_time":"2022-01-20T23:10:52.865971","exception":false,"start_time":"2022-01-20T23:10:52.823641","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# features with large changes over time are converted to relative ranks within time-id\nif ENABLE_RANK_NORMALIZATION:\n    df['trade.order_count.mean'] = df.groupby('time_id')['trade.order_count.mean'].rank()\n    df['book.total_volume.sum']  = df.groupby('time_id')['book.total_volume.sum'].rank()\n    df['book.total_volume.mean'] = df.groupby('time_id')['book.total_volume.mean'].rank()\n    df['book.total_volume.std']  = df.groupby('time_id')['book.total_volume.std'].rank()\n\n    df['trade.tau'] = df.groupby('time_id')['trade.tau'].rank()\n\n    for dt in [150, 300, 450]:\n        df[f'book_{dt}.total_volume.sum']  = df.groupby('time_id')[f'book_{dt}.total_volume.sum'].rank()\n        df[f'book_{dt}.total_volume.mean'] = df.groupby('time_id')[f'book_{dt}.total_volume.mean'].rank()\n        df[f'book_{dt}.total_volume.std']  = df.groupby('time_id')[f'book_{dt}.total_volume.std'].rank()\n        df[f'trade_{dt}.order_count.mean'] = df.groupby('time_id')[f'trade_{dt}.order_count.mean'].rank()","metadata":{"execution":{"iopub.execute_input":"2022-01-20T23:10:52.958399Z","iopub.status.busy":"2022-01-20T23:10:52.957368Z","iopub.status.idle":"2022-01-20T23:10:54.788714Z","shell.execute_reply":"2022-01-20T23:10:54.788197Z","shell.execute_reply.started":"2022-01-18T14:13:44.189634Z"},"papermill":{"duration":1.881342,"end_time":"2022-01-20T23:10:54.788852","exception":false,"start_time":"2022-01-20T23:10:52.90751","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def make_nearest_neighbor_feature(df: pd.DataFrame) -> pd.DataFrame:\n    df2 = df.copy()\n    print(df2.shape)\n\n    feature_cols_stock = {\n        'book.log_return1.realized_volatility': [np.mean, np.min, np.max, np.std],\n        'trade.seconds_in_bucket.count': [np.mean],\n        'trade.tau': [np.mean],\n        'trade_150.tau': [np.mean],\n        'book.tau': [np.mean],\n        'trade.size.sum': [np.mean],\n        'book.seconds_in_bucket.count': [np.mean],\n    }\n    \n    feature_cols = {\n        'book.log_return1.realized_volatility': [np.mean, np.min, np.max, np.std],\n        'real_price': [np.max, np.mean, np.min],\n        'trade.seconds_in_bucket.count': [np.mean],\n        'trade.tau': [np.mean],\n        'trade.size.sum': [np.mean],\n        'book.seconds_in_bucket.count': [np.mean],\n        'trade_150.tau_nn20_stock_vol_l1_mean': [np.mean],\n        'trade.size.sum_nn20_stock_vol_l1_mean': [np.mean],\n    }\n\n    time_id_neigbor_sizes = [3, 5, 10, 20, 40]\n    time_id_neigbor_sizes_vol = [2, 3, 5, 10, 20, 40]\n    stock_id_neighbor_sizes = [10, 20, 40]\n\n    ndf: Optional[pd.DataFrame] = None\n\n    def _add_ndf(ndf: Optional[pd.DataFrame], dst: pd.DataFrame) -> pd.DataFrame:\n        if ndf is None:\n            return dst\n        else:\n            ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n            return ndf\n\n    # neighbor stock_id\n    for feature_col in feature_cols_stock.keys():\n        try:\n            if feature_col not in df2.columns:\n                print(f\"column {feature_col} is skipped\")\n                continue\n\n            if not stock_id_neighbors:\n                continue\n\n            for nn in stock_id_neighbors:\n                nn.rearrange_feature_values(df2, feature_col)\n\n            for agg in feature_cols_stock[feature_col]:\n                for n in stock_id_neighbor_sizes:\n                    try:\n                        for nn in stock_id_neighbors:\n                            dst = nn.make_nn_feature(n, agg)\n                            ndf = _add_ndf(ndf, dst)\n                    except Exception:\n                        print_trace('stock-id nn')\n                        pass\n        except Exception:\n            print_trace('stock-id nn')\n            pass\n\n    if ndf is not None:\n        df2 = pd.merge(df2, ndf, on=['time_id', 'stock_id'], how='left')\n    ndf = None\n\n    print(df2.shape)\n\n    # neighbor time_id\n    for feature_col in feature_cols.keys():\n        try:\n            if not USE_PRICE_NN_FEATURES and feature_col == 'real_price':\n                continue\n            if feature_col not in df2.columns:\n                print(f\"column {feature_col} is skipped\")\n                continue\n\n            for nn in time_id_neighbors:\n                nn.rearrange_feature_values(df2, feature_col)\n\n            if 'volatility' in feature_col:\n                time_id_ns = time_id_neigbor_sizes_vol\n            else:\n                time_id_ns = time_id_neigbor_sizes\n\n            for agg in feature_cols[feature_col]:\n                for n in time_id_ns:\n                    try:\n                        for nn in time_id_neighbors:\n                            dst = nn.make_nn_feature(n, agg)\n                            ndf = _add_ndf(ndf, dst)\n                    except Exception:\n                        print_trace('time-id nn')\n                        pass\n        except Exception:\n            print_trace('time-id nn')\n\n    if ndf is not None:\n        df2 = pd.merge(df2, ndf, on=['time_id', 'stock_id'], how='left')\n\n    # features further derived from nearest neighbor features\n    try:\n        if USE_PRICE_NN_FEATURES:\n            for sz in time_id_neigbor_sizes:\n                denominator = f\"real_price_nn{sz}_time_price_c\"\n\n                df2[f'real_price_rankmin_{sz}']  = df2['real_price'] / df2[f\"{denominator}_amin\"]\n                df2[f'real_price_rankmax_{sz}']  = df2['real_price'] / df2[f\"{denominator}_amax\"]\n                df2[f'real_price_rankmean_{sz}'] = df2['real_price'] / df2[f\"{denominator}_mean\"]\n\n            for sz in time_id_neigbor_sizes_vol:\n                denominator = f\"book.log_return1.realized_volatility_nn{sz}_time_price_c\"\n\n                df2[f'vol_rankmin_{sz}'] = \\\n                    df2['book.log_return1.realized_volatility'] / df2[f\"{denominator}_amin\"]\n                df2[f'vol_rankmax_{sz}'] = \\\n                    df2['book.log_return1.realized_volatility'] / df2[f\"{denominator}_amax\"]\n\n        price_cols = [c for c in df2.columns if 'real_price' in c and 'rank' not in c]\n        for c in price_cols:\n            del df2[c]\n\n        if USE_PRICE_NN_FEATURES:\n            for sz in time_id_neigbor_sizes_vol:\n                tgt = f'book.log_return1.realized_volatility_nn{sz}_time_price_m_mean'\n                df2[f'{tgt}_rank'] = df2.groupby('time_id')[tgt].rank()\n    except Exception:\n        print_trace('nn features')\n\n    return df2","metadata":{"execution":{"iopub.execute_input":"2022-01-20T23:10:54.885256Z","iopub.status.busy":"2022-01-20T23:10:54.884242Z","iopub.status.idle":"2022-01-20T23:10:54.886843Z","shell.execute_reply":"2022-01-20T23:10:54.886391Z","shell.execute_reply.started":"2022-01-18T14:13:44.199422Z"},"papermill":{"duration":0.061056,"end_time":"2022-01-20T23:10:54.886973","exception":false,"start_time":"2022-01-20T23:10:54.825917","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gc.collect()\n\nwith timer('make nearest neighbor feature'):\n    df2 = make_nearest_neighbor_feature(df)\n\nprint(df2.shape)\ndf2.reset_index(drop=True).to_feather('optiver_df2.f')\n\ngc.collect()","metadata":{"execution":{"iopub.execute_input":"2022-01-20T23:10:55.117257Z","iopub.status.busy":"2022-01-20T23:10:55.115874Z","iopub.status.idle":"2022-01-20T23:12:12.962741Z","shell.execute_reply":"2022-01-20T23:12:12.963956Z","shell.execute_reply.started":"2022-01-18T14:13:44.224929Z"},"papermill":{"duration":78.040576,"end_time":"2022-01-20T23:12:12.964199","exception":false,"start_time":"2022-01-20T23:10:54.923623","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Misc Features","metadata":{"papermill":{"duration":0.064386,"end_time":"2022-01-20T23:12:13.097635","exception":false,"start_time":"2022-01-20T23:12:13.033249","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# skew correction for NN\ncols_to_log = [\n    'trade.size.sum',\n    'trade_150.size.sum',\n    'trade_300.size.sum',\n    'trade_450.size.sum',\n    'volume_imbalance'\n]\nfor c in df2.columns:\n    for check in cols_to_log:\n        try:\n            if check in c:\n                df2[c] = np.log(df2[c]+1)\n                break\n        except Exception:\n            print_trace('log1p')","metadata":{"execution":{"iopub.execute_input":"2022-01-20T23:12:13.238552Z","iopub.status.busy":"2022-01-20T23:12:13.237551Z","iopub.status.idle":"2022-01-20T23:12:13.681016Z","shell.execute_reply":"2022-01-20T23:12:13.681636Z","shell.execute_reply.started":"2022-01-15T04:54:06.290787Z"},"papermill":{"duration":0.522103,"end_time":"2022-01-20T23:12:13.68438","exception":false,"start_time":"2022-01-20T23:12:13.162277","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Rolling average of RV for similar trading volume\ntry:\n    df2.sort_values(by=['stock_id', 'book.total_volume.sum'], inplace=True)\n    df2.reset_index(drop=True, inplace=True)\n    \n    roll_target = 'book.log_return1.realized_volatility'\n\n    for window_size in [3, 10]:\n        df2[f'realized_volatility_roll{window_size}_by_book.total_volume.mean'] = \\\n            df2.groupby('stock_id')[roll_target].rolling(window_size, center=True, min_periods=1) \\\n                                                .mean() \\\n                                                .reset_index() \\\n                                                .sort_values(by=['level_1'])[roll_target].values\nexcept Exception:\n    print_trace('mean RV')","metadata":{"execution":{"iopub.execute_input":"2022-01-20T23:12:14.914226Z","iopub.status.busy":"2022-01-20T23:12:14.913316Z","iopub.status.idle":"2022-01-20T23:12:16.265343Z","shell.execute_reply":"2022-01-20T23:12:16.264417Z","shell.execute_reply.started":"2022-01-15T04:54:06.724354Z"},"papermill":{"duration":2.468839,"end_time":"2022-01-20T23:12:16.265482","exception":false,"start_time":"2022-01-20T23:12:13.796643","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# stock-id embedding (helps little)\ntry:\n    lda_n = 3\n    lda = LatentDirichletAllocation(n_components=lda_n, random_state=0)\n\n    stock_id_emb = pd.DataFrame(\n        lda.fit_transform(pivot.transpose()), \n        index=df_pv.pivot('time_id', 'stock_id', 'vol').columns\n    )\n\n    for i in range(lda_n):\n        df2[f'stock_id_emb{i}'] = df2['stock_id'].map(stock_id_emb[i])\nexcept Exception:\n    print_trace('LDA')","metadata":{"execution":{"iopub.execute_input":"2022-01-20T23:12:16.375583Z","iopub.status.busy":"2022-01-20T23:12:16.363425Z","iopub.status.idle":"2022-01-20T23:12:19.582643Z","shell.execute_reply":"2022-01-20T23:12:19.58158Z","shell.execute_reply.started":"2022-01-15T04:54:08.318718Z"},"papermill":{"duration":3.27953,"end_time":"2022-01-20T23:12:19.582811","exception":false,"start_time":"2022-01-20T23:12:16.303281","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train = df2[~df2.target.isnull()].copy()\ndf_test = df2[df2.target.isnull()].copy()\ndel df2, df_pv\ngc.collect()","metadata":{"execution":{"iopub.execute_input":"2022-01-20T23:12:20.929699Z","iopub.status.busy":"2022-01-20T23:12:20.928639Z","iopub.status.idle":"2022-01-20T23:12:22.546258Z","shell.execute_reply":"2022-01-20T23:12:22.546658Z","shell.execute_reply.started":"2022-01-15T04:54:13.038956Z"},"papermill":{"duration":2.923895,"end_time":"2022-01-20T23:12:22.546886","exception":false,"start_time":"2022-01-20T23:12:19.622991","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Reverse Engineering time-id Order & Make CV Split","metadata":{"papermill":{"duration":0.036984,"end_time":"2022-01-20T23:12:22.632262","exception":false,"start_time":"2022-01-20T23:12:22.595278","status":"completed"},"tags":[]}},{"cell_type":"code","source":"%matplotlib inline\n\n@contextmanager\ndef timer(name):\n    s = time.time()\n    yield\n    e = time.time() - s\n    print(f\"[{name}] {e:.3f}sec\")\n    \n\ndef calc_price2(df):\n    tick = sorted(np.diff(sorted(np.unique(df.values.flatten()))))[0]\n    return 0.01 / tick\n\n\ndef calc_prices(r):\n    df = pd.read_parquet(r.book_path, columns=['time_id', 'ask_price1', 'ask_price2', 'bid_price1', 'bid_price2'])\n    df = df.set_index('time_id')\n    df = df.groupby(level='time_id').apply(calc_price2).to_frame('price').reset_index()\n    df['stock_id'] = r.stock_id\n    return df\n\n\ndef sort_manifold(df, clf):\n    df_ = df.set_index('time_id')\n    df_ = pd.DataFrame(minmax_scale(df_.fillna(df_.mean())))\n\n    X_compoents = clf.fit_transform(df_)\n\n    dft = df.reindex(np.argsort(X_compoents[:,0])).reset_index(drop=True)\n    return np.argsort(X_compoents[:, 0]), X_compoents\n\n\ndef reconstruct_time_id_order():\n    with timer('load files'):\n        df_files = pd.DataFrame(\n            {'book_path': glob.glob('/kaggle/input/optiver-realized-volatility-prediction/book_train.parquet/**/*.parquet')}) \\\n            .eval('stock_id = book_path.str.extract(\"stock_id=(\\d+)\").astype(\"int\")', engine='python')\n\n    with timer('calc prices'):\n        df_prices = pd.concat(Parallel(n_jobs=4, verbose=51)(delayed(calc_prices)(r) for _, r in df_files.iterrows()))\n        df_prices = df_prices.pivot('time_id', 'stock_id', 'price')\n        df_prices.columns = [f'stock_id={i}' for i in df_prices.columns]\n        df_prices = df_prices.reset_index(drop=False)\n\n    with timer('t-SNE(400) -> 50'):\n        clf = TSNE(n_components=1, perplexity=400, random_state=0, n_iter=2000)\n        order, X_compoents = sort_manifold(df_prices, clf)\n\n        clf = TSNE(n_components=1, perplexity=50, random_state=0, init=X_compoents, n_iter=2000, method='exact')\n        order, X_compoents = sort_manifold(df_prices, clf)\n\n        df_ordered = df_prices.reindex(order).reset_index(drop=True)\n        if df_ordered['stock_id=61'].iloc[0] > df_ordered['stock_id=61'].iloc[-1]:\n            df_ordered = df_ordered.reindex(df_ordered.index[::-1]).reset_index(drop=True)\n\n    # AMZN\n    plt.plot(df_ordered['stock_id=61'])\n    \n    return df_ordered[['time_id']]","metadata":{"execution":{"iopub.execute_input":"2022-01-20T23:12:22.730826Z","iopub.status.busy":"2022-01-20T23:12:22.729921Z","iopub.status.idle":"2022-01-20T23:12:22.732741Z","shell.execute_reply":"2022-01-20T23:12:22.732256Z","shell.execute_reply.started":"2022-01-15T04:54:14.7715Z"},"papermill":{"duration":0.063514,"end_time":"2022-01-20T23:12:22.732879","exception":false,"start_time":"2022-01-20T23:12:22.669365","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if CV_SPLIT == 'time':\n    with timer('calculate order of time-id'):\n        if USE_PRECOMPUTE_FEATURES:\n            timeid_order = pd.read_csv(os.path.join(DATA_DIR, 'optiver-time-id-ordered', 'time_id_order.csv'))\n        else:\n            timeid_order = reconstruct_time_id_order()\n\n    with timer('make folds'):\n        timeid_order['time_id_order'] = np.arange(len(timeid_order))\n        df_train['time_id_order'] = df_train['time_id'].map(timeid_order.set_index('time_id')['time_id_order'])\n        df_train = df_train.sort_values(['time_id_order', 'stock_id']).reset_index(drop=True)\n\n        folds_border = [3830 - 383*4, 3830 - 383*3, 3830 - 383*2, 3830 - 383*1]\n        time_id_orders = df_train['time_id_order']\n\n        folds = []\n        for i, border in enumerate(folds_border):\n            idx_train = np.where(time_id_orders < border)[0]\n            idx_valid = np.where((border <= time_id_orders) & (time_id_orders < border + 383))[0]\n            folds.append((idx_train, idx_valid))\n\n            print(f\"folds{i}: train={len(idx_train)}, valid={len(idx_valid)}\")\n\n    del df_train['time_id_order']\nelif CV_SPLIT == 'group':\n    gkf = GroupKFold(n_splits=4)\n    folds = []\n\n    for i, (idx_train, idx_valid) in enumerate(gkf.split(df_train, None, groups=df_train['time_id'])):\n        folds.append((idx_train, idx_valid))\nelse:\n    raise ValueError()\n\ndf_train.reset_index(drop=True, inplace=True)\ndf_test.reset_index(drop=True, inplace=True)","metadata":{"execution":{"iopub.execute_input":"2022-01-20T23:12:22.820332Z","iopub.status.busy":"2022-01-20T23:12:22.819362Z","iopub.status.idle":"2022-01-20T23:12:22.896733Z","shell.execute_reply":"2022-01-20T23:12:22.896265Z","shell.execute_reply.started":"2022-01-15T04:54:14.801275Z"},"papermill":{"duration":0.125709,"end_time":"2022-01-20T23:12:22.896893","exception":false,"start_time":"2022-01-20T23:12:22.771184","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## LightGBM Training","metadata":{"papermill":{"duration":0.037784,"end_time":"2022-01-20T23:12:22.972672","exception":false,"start_time":"2022-01-20T23:12:22.934888","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def rmspe(y_true, y_pred):\n    return  (np.sqrt(np.mean(np.square((y_true - y_pred) / y_true))))\n\n\ndef feval_RMSPE(preds, train_data):\n    labels = train_data.get_label()\n    return 'RMSPE', round(rmspe(y_true = labels, y_pred = preds),5), False\n\n\n# from: https://blog.amedama.jp/entry/lightgbm-cv-feature-importance\ndef plot_importance(cvbooster, figsize=(10, 10)):\n    raw_importances = cvbooster.feature_importance(importance_type='gain')\n    feature_name = cvbooster.boosters[0].feature_name()\n    importance_df = pd.DataFrame(data=raw_importances,\n                                 columns=feature_name)\n    # order by average importance across folds\n    sorted_indices = importance_df.mean(axis=0).sort_values(ascending=False).index\n    sorted_importance_df = importance_df.loc[:, sorted_indices]\n    # plot top-n\n    PLOT_TOP_N = 50\n    plot_cols = sorted_importance_df.columns[:PLOT_TOP_N]\n    _, ax = plt.subplots(figsize=figsize)\n    ax.grid()\n    ax.set_xscale('log')\n    ax.set_ylabel('Feature')\n    ax.set_xlabel('Importance')\n    sns.boxplot(data=sorted_importance_df[plot_cols],\n                orient='h',\n                ax=ax)\n    plt.show()\n\n\ndef get_X(df_src):\n    cols = [c for c in df_src.columns if c not in ['time_id', 'target', 'tick_size']]\n    return df_src[cols]\n\n\nclass EnsembleModel:\n    def __init__(self, models: List[lgb.Booster], weights: Optional[List[float]] = None):\n        self.models = models\n        self.weights = weights\n\n        features = list(self.models[0].feature_name())\n\n        for m in self.models[1:]:\n            assert features == list(m.feature_name())\n\n    def predict(self, x):\n        predicted = np.zeros((len(x), len(self.models)))\n\n        for i, m in enumerate(self.models):\n            w = self.weights[i] if self.weights is not None else 1\n            predicted[:, i] = w * m.predict(x)\n\n        ttl = np.sum(self.weights) if self.weights is not None else len(self.models)\n        return np.sum(predicted, axis=1) / ttl\n\n    def feature_name(self) -> List[str]:\n        return self.models[0].feature_name()","metadata":{"execution":{"iopub.execute_input":"2022-01-20T23:12:23.062171Z","iopub.status.busy":"2022-01-20T23:12:23.061311Z","iopub.status.idle":"2022-01-20T23:12:23.064162Z","shell.execute_reply":"2022-01-20T23:12:23.063555Z","shell.execute_reply.started":"2022-01-15T04:54:14.902446Z"},"papermill":{"duration":0.054065,"end_time":"2022-01-20T23:12:23.064286","exception":false,"start_time":"2022-01-20T23:12:23.010221","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lr = GBDT_LR\nif SHORTCUT_GBDT_IN_1ST_STAGE and IS_1ST_STAGE:\n    # to save GPU quota\n    lr = 0.3\n\nparams = {\n    'objective': 'regression',\n    'verbose': 0,\n    'metric': '',\n    'reg_alpha': 5,\n    'reg_lambda': 5,\n    'min_data_in_leaf': 1000,\n    'max_depth': -1,\n    'num_leaves': 128,\n    'colsample_bytree': 0.3,\n    'learning_rate': lr\n}\n\nX = get_X(df_train)\ny = df_train['target']\nX.to_feather('X.f')\ndf_train[['target']].to_feather('y.f')\n\ngc.collect()\n\nprint(X.shape)\n\nif PREDICT_GBDT:\n    ds = lgb.Dataset(X, y, weight=1/np.power(y, 2))\n\n    with timer('lgb.cv'):\n        ret = lgb.cv(params, ds, num_boost_round=8000, folds=folds, #cv,\n                     feval=feval_RMSPE, stratified=False, \n                     return_cvbooster=True, verbose_eval=20,\n                     early_stopping_rounds=int(40*0.1/lr))\n\n        print(f\"# overall RMSPE: {ret['RMSPE-mean'][-1]}\")\n\n    best_iteration = len(ret['RMSPE-mean'])\n    for i in range(len(folds)):\n        y_pred = ret['cvbooster'].boosters[i].predict(X.iloc[folds[i][1]], num_iteration=best_iteration)\n        y_true = y.iloc[folds[i][1]]\n        print(f\"# fold{i} RMSPE: {rmspe(y_true, y_pred)}\")\n        \n        if i == len(folds) - 1:\n            np.save('pred_gbdt.npy', y_pred)\n\n    plot_importance(ret['cvbooster'], figsize=(10, 20))\n\n    boosters = []\n    with timer('retraining'):\n        for i in range(GBDT_NUM_MODELS):\n            params['seed'] = i\n            boosters.append(lgb.train(params, ds, num_boost_round=int(1.1*best_iteration)))\n\n    booster = EnsembleModel(boosters)\n    del ret\n    del ds\n\ngc.collect()","metadata":{"execution":{"iopub.execute_input":"2022-01-20T23:12:23.53135Z","iopub.status.busy":"2022-01-20T23:12:23.52487Z","iopub.status.idle":"2022-01-20T23:15:01.921587Z","shell.execute_reply":"2022-01-20T23:15:01.921149Z","shell.execute_reply.started":"2022-01-15T04:54:14.922483Z"},"papermill":{"duration":158.819992,"end_time":"2022-01-20T23:15:01.921725","exception":false,"start_time":"2022-01-20T23:12:23.101733","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## NN Training","metadata":{"papermill":{"duration":0.045012,"end_time":"2022-01-20T23:15:02.013048","exception":false,"start_time":"2022-01-20T23:15:01.968036","status":"completed"},"tags":[]}},{"cell_type":"code","source":"import gc\nimport os\nimport random\nfrom typing import List, Tuple, Optional, Union\n\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nfrom sklearn.preprocessing import StandardScaler\nfrom torch.utils.data import Dataset, DataLoader\nfrom tqdm import tqdm\n\nfrom joblib import Parallel, delayed\nfrom sklearn.decomposition import PCA\nfrom pytorch_tabnet.metrics import Metric\nfrom pytorch_tabnet.tab_model import TabNetRegressor\nfrom torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n\n\nnull_check_cols = [\n    'book.log_return1.realized_volatility',\n    'book_150.log_return1.realized_volatility',\n    'book_300.log_return1.realized_volatility',\n    'book_450.log_return1.realized_volatility',\n    'trade.log_return.realized_volatility',\n    'trade_150.log_return.realized_volatility',\n    'trade_300.log_return.realized_volatility',\n    'trade_450.log_return.realized_volatility'\n]\n\n\ndef seed_everything(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n\n\ndef rmspe_metric(y_true, y_pred):\n    rmspe = np.sqrt(np.mean(np.square((y_true - y_pred) / y_true)))\n    return rmspe\n\n\ndef rmspe_loss(y_true, y_pred):\n    rmspe = torch.sqrt(torch.mean(torch.square((y_true - y_pred) / y_true)))\n    return rmspe\n\n\nclass RMSPE(Metric):\n    def __init__(self):\n        self._name = \"rmspe\"\n        self._maximize = False\n\n    def __call__(self, y_true, y_score):\n        return np.sqrt(np.mean(np.square((y_true - y_score) / y_true)))\n\ndef RMSPELoss_Tabnet(y_pred, y_true):\n    return torch.sqrt(torch.mean( ((y_true - y_pred) / y_true) ** 2 )).clone()\n\n\nclass AverageMeter:\n    \"\"\"Computes and stores the average and current value\"\"\"\n\n    def __init__(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n\n\nclass TabularDataset(Dataset):\n    def __init__(self, x_num: np.ndarray, x_cat: np.ndarray, y: Optional[np.ndarray]):\n        super().__init__()\n        self.x_num = x_num\n        self.x_cat = x_cat\n        self.y = y\n\n    def __len__(self):\n        return len(self.x_num)\n\n    def __getitem__(self, idx):\n        if self.y is None:\n            return self.x_num[idx], torch.LongTensor(self.x_cat[idx])\n        else:\n            return self.x_num[idx], torch.LongTensor(self.x_cat[idx]), self.y[idx]\n\n\nclass MLP(nn.Module):\n    def __init__(self,\n                 src_num_dim: int,\n                 n_categories: List[int],\n                 dropout: float = 0.0,\n                 hidden: int = 50,\n                 emb_dim: int = 10,\n                 dropout_cat: float = 0.2,\n                 bn: bool = False):\n        super().__init__()\n\n        self.embs = nn.ModuleList([\n            nn.Embedding(x, emb_dim) for x in n_categories])\n        self.cat_dim = emb_dim * len(n_categories)\n        self.dropout_cat = nn.Dropout(dropout_cat)\n\n        if bn:\n            self.sequence = nn.Sequential(\n                nn.Linear(src_num_dim + self.cat_dim, hidden),\n                nn.Dropout(dropout),\n                nn.BatchNorm1d(hidden),\n                nn.ReLU(),\n                nn.Linear(hidden, hidden),\n                nn.Dropout(dropout),\n                nn.BatchNorm1d(hidden),\n                nn.ReLU(),\n                nn.Linear(hidden, 1)\n            )\n        else:\n            self.sequence = nn.Sequential(\n                nn.Linear(src_num_dim + self.cat_dim, hidden),\n                nn.Dropout(dropout),\n                nn.ReLU(),\n                nn.Linear(hidden, hidden),\n                nn.Dropout(dropout),\n                nn.ReLU(),\n                nn.Linear(hidden, 1)\n            )\n\n    def forward(self, x_num, x_cat):\n        embs = [embedding(x_cat[:, i]) for i, embedding in enumerate(self.embs)]\n        x_cat_emb = self.dropout_cat(torch.cat(embs, 1))\n        x_all = torch.cat([x_num, x_cat_emb], 1)\n        x = self.sequence(x_all)\n        return torch.squeeze(x)\n\n\nclass CNN(nn.Module):\n    def __init__(self,\n                 num_features: int,\n                 hidden_size: int,\n                 n_categories: List[int],\n                 emb_dim: int = 10,\n                 dropout_cat: float = 0.2,\n                 channel_1: int = 256,\n                 channel_2: int = 512,\n                 channel_3: int = 512,\n                 dropout_top: float = 0.1,\n                 dropout_mid: float = 0.3,\n                 dropout_bottom: float = 0.2,\n                 weight_norm: bool = True,\n                 two_stage: bool = True,\n                 celu: bool = True,\n                 kernel1: int = 5,\n                 leaky_relu: bool = False):\n        super().__init__()\n\n        num_targets = 1\n\n        cha_1_reshape = int(hidden_size / channel_1)\n        cha_po_1 = int(hidden_size / channel_1 / 2)\n        cha_po_2 = int(hidden_size / channel_1 / 2 / 2) * channel_3\n\n        self.cat_dim = emb_dim * len(n_categories)\n        self.cha_1 = channel_1\n        self.cha_2 = channel_2\n        self.cha_3 = channel_3\n        self.cha_1_reshape = cha_1_reshape\n        self.cha_po_1 = cha_po_1\n        self.cha_po_2 = cha_po_2\n        self.two_stage = two_stage\n\n        self.expand = nn.Sequential(\n            nn.BatchNorm1d(num_features + self.cat_dim),\n            nn.Dropout(dropout_top),\n            nn.utils.weight_norm(nn.Linear(num_features + self.cat_dim, hidden_size), dim=None),\n            nn.CELU(0.06) if celu else nn.ReLU()\n        )\n\n        def _norm(layer, dim=None):\n            return nn.utils.weight_norm(layer, dim=dim) if weight_norm else layer\n\n        self.conv1 = nn.Sequential(\n            nn.BatchNorm1d(channel_1),\n            nn.Dropout(dropout_top),\n            _norm(nn.Conv1d(channel_1, channel_2, kernel_size=kernel1, stride=1, padding=kernel1 // 2, bias=False)),\n            nn.ReLU(),\n            nn.AdaptiveAvgPool1d(output_size=cha_po_1),\n            nn.BatchNorm1d(channel_2),\n            nn.Dropout(dropout_top),\n            _norm(nn.Conv1d(channel_2, channel_2, kernel_size=3, stride=1, padding=1, bias=True)),\n            nn.ReLU()\n        )\n\n        if self.two_stage:\n            self.conv2 = nn.Sequential(\n                nn.BatchNorm1d(channel_2),\n                nn.Dropout(dropout_mid),\n                _norm(nn.Conv1d(channel_2, channel_2, kernel_size=3, stride=1, padding=1, bias=True)),\n                nn.ReLU(),\n                nn.BatchNorm1d(channel_2),\n                nn.Dropout(dropout_bottom),\n                _norm(nn.Conv1d(channel_2, channel_3, kernel_size=5, stride=1, padding=2, bias=True)),\n                nn.ReLU()\n            )\n\n        self.max_po_c2 = nn.MaxPool1d(kernel_size=4, stride=2, padding=1)\n\n        self.flt = nn.Flatten()\n\n        if leaky_relu:\n            self.dense = nn.Sequential(\n                nn.BatchNorm1d(cha_po_2),\n                nn.Dropout(dropout_bottom),\n                _norm(nn.Linear(cha_po_2, num_targets), dim=0),\n                nn.LeakyReLU()\n            )\n        else:\n            self.dense = nn.Sequential(\n                nn.BatchNorm1d(cha_po_2),\n                nn.Dropout(dropout_bottom),\n                _norm(nn.Linear(cha_po_2, num_targets), dim=0)\n            )\n\n        self.embs = nn.ModuleList([nn.Embedding(x, emb_dim) for x in n_categories])\n        self.cat_dim = emb_dim * len(n_categories)\n        self.dropout_cat = nn.Dropout(dropout_cat)\n\n    def forward(self, x_num, x_cat):\n        embs = [embedding(x_cat[:, i]) for i, embedding in enumerate(self.embs)]\n        x_cat_emb = self.dropout_cat(torch.cat(embs, 1))\n        x = torch.cat([x_num, x_cat_emb], 1)\n\n        x = self.expand(x)\n\n        x = x.reshape(x.shape[0], self.cha_1, self.cha_1_reshape)\n\n        x = self.conv1(x)\n\n        if self.two_stage:\n            x = self.conv2(x) * x\n\n        x = self.max_po_c2(x)\n        x = self.flt(x)\n        x = self.dense(x)\n\n        return torch.squeeze(x)\n\n\ndef preprocess_nn(\n        X: pd.DataFrame,\n        scaler: Optional[StandardScaler] = None,\n        scaler_type: str = 'standard',\n        n_pca: int = -1,\n        na_cols: bool = True):\n    if na_cols:\n        #for c in X.columns:\n        for c in null_check_cols:\n            if c in X.columns:\n                X[f\"{c}_isnull\"] = X[c].isnull().astype(int)\n\n    cat_cols = [c for c in X.columns if c in ['time_id', 'stock_id']]\n    num_cols = [c for c in X.columns if c not in cat_cols]\n\n    X_num = X[num_cols].values.astype(np.float32)\n    X_cat = np.nan_to_num(X[cat_cols].values.astype(np.int32))\n\n    def _pca(X_num_):\n        if n_pca > 0:\n            pca = PCA(n_components=n_pca, random_state=0)\n            return pca.fit_transform(X_num)\n        return X_num\n\n    if scaler is None:\n        scaler = StandardScaler()\n        X_num = scaler.fit_transform(X_num)\n        X_num = np.nan_to_num(X_num, posinf=0, neginf=0)\n        return _pca(X_num), X_cat, cat_cols, scaler\n    else:\n        X_num = scaler.transform(X_num) #TODO: inf\n        X_num = np.nan_to_num(X_num, posinf=0, neginf=0)\n        return _pca(X_num), X_cat, cat_cols\n\n\ndef train_epoch(data_loader: DataLoader,\n                model: nn.Module,\n                optimizer,\n                scheduler,\n                device,\n                clip_grad: float = 1.5):\n    model.train()\n    losses = AverageMeter()\n    step = 0\n\n    for x_num, x_cat, y in tqdm(data_loader, position=0, leave=True, desc='Training'):\n        batch_size = x_num.size(0)\n        x_num = x_num.to(device, dtype=torch.float)\n        x_cat = x_cat.to(device)\n        y = y.to(device, dtype=torch.float)\n\n        loss = rmspe_loss(y, model(x_num, x_cat))\n        losses.update(loss.detach().cpu().numpy(), batch_size)\n        loss.backward()\n\n        torch.nn.utils.clip_grad_norm_(model.parameters(), clip_grad)\n        optimizer.step()\n        optimizer.zero_grad()\n\n        if scheduler is not None:\n            scheduler.step()\n\n        step += 1\n\n    return losses.avg\n\n\ndef evaluate(data_loader: DataLoader, model, device):\n    model.eval()\n\n    losses = AverageMeter()\n\n    final_targets = []\n    final_outputs = []\n\n    with torch.no_grad():\n        for x_num, x_cat, y in tqdm(data_loader, position=0, leave=True, desc='Evaluating'):\n            batch_size = x_num.size(0)\n            x_num = x_num.to(device, dtype=torch.float)\n            x_cat = x_cat.to(device)\n            y = y.to(device, dtype=torch.float)\n\n            with torch.no_grad():\n                output = model(x_num, x_cat)\n\n            loss = rmspe_loss(y, output)\n            # record loss\n            losses.update(loss.detach().cpu().numpy(), batch_size)\n\n            targets = y.detach().cpu().numpy()\n            output = output.detach().cpu().numpy()\n\n            final_targets.append(targets)\n            final_outputs.append(output)\n\n    final_targets = np.concatenate(final_targets)\n    final_outputs = np.concatenate(final_outputs)\n\n    try:\n        metric = rmspe_metric(final_targets, final_outputs)\n    except:\n        metric = None\n\n    return final_outputs, final_targets, losses.avg, metric\n\n\ndef predict_nn(X: pd.DataFrame,\n               model: Union[List[MLP], MLP],\n               scaler: StandardScaler,\n               device,\n               ensemble_method='mean'):\n    if not isinstance(model, list):\n        model = [model]\n\n    for m in model:\n        m.eval()\n    X_num, X_cat, cat_cols = preprocess_nn(X.copy(), scaler=scaler)\n    valid_dataset = TabularDataset(X_num, X_cat, None)\n    valid_loader = torch.utils.data.DataLoader(valid_dataset,\n                                               batch_size=512,\n                                               shuffle=False,\n                                               num_workers=4)\n\n    final_outputs = []\n\n    with torch.no_grad():\n        for x_num, x_cat in tqdm(valid_loader, position=0, leave=True, desc='Evaluating'):\n            x_num = x_num.to(device, dtype=torch.float)\n            x_cat = x_cat.to(device)\n\n            outputs = []\n            with torch.no_grad():\n                for m in model:\n                    output = m(x_num, x_cat)\n                    outputs.append(output.detach().cpu().numpy())\n\n            if ensemble_method == 'median':\n                pred = np.nanmedian(np.array(outputs), axis=0)\n            else:\n                pred = np.array(outputs).mean(axis=0)\n            final_outputs.append(pred)\n\n    final_outputs = np.concatenate(final_outputs)\n    return final_outputs\n\n\ndef predict_tabnet(X: pd.DataFrame,\n                   model: Union[List[TabNetRegressor], TabNetRegressor],\n                   scaler: StandardScaler,\n                   ensemble_method='mean'):\n    if not isinstance(model, list):\n        model = [model]\n\n    X_num, X_cat, cat_cols = preprocess_nn(X.copy(), scaler=scaler)\n    X_processed = np.concatenate([X_cat, X_num], axis=1)\n\n    predicted = []\n    for m in model:\n        predicted.append(m.predict(X_processed))\n\n    if ensemble_method == 'median':\n        pred = np.nanmedian(np.array(predicted), axis=0)\n    else:\n        pred = np.array(predicted).mean(axis=0)\n\n    return pred\n\n\ndef train_tabnet(X: pd.DataFrame,\n                 y: pd.DataFrame,\n                 folds: List[Tuple],\n                 batch_size: int = 1024,\n                 lr: float = 1e-3,\n                 model_path: str = 'fold_{}.pth',\n                 scaler_type: str = 'standard',\n                 output_dir: str = 'artifacts',\n                 epochs: int = 250,\n                 seed: int = 42,\n                 n_pca: int = -1,\n                 na_cols: bool = True,\n                 patience: int = 10,\n                 factor: float = 0.5,\n                 gamma: float = 2.0,\n                 lambda_sparse: float = 8.0,\n                 n_steps: int = 2,\n                 scheduler_type: str = 'cosine',\n                 n_a: int = 16):\n    seed_everything(seed)\n\n    os.makedirs(output_dir, exist_ok=True)\n\n    y = y.values.astype(np.float32)\n    X_num, X_cat, cat_cols, scaler = preprocess_nn(X.copy(), scaler_type=scaler_type, n_pca=n_pca, na_cols=na_cols)\n\n    best_losses = []\n    best_predictions = []\n\n    for cv_idx, (train_idx, valid_idx) in enumerate(folds):\n        X_tr, X_va = X_num[train_idx], X_num[valid_idx]\n        X_tr_cat, X_va_cat = X_cat[train_idx], X_cat[valid_idx]\n        y_tr, y_va = y[train_idx], y[valid_idx]\n        y_tr = y_tr.reshape(-1,1)\n        y_va = y_va.reshape(-1,1)\n        X_tr = np.concatenate([X_tr_cat, X_tr], axis=1)\n        X_va = np.concatenate([X_va_cat, X_va], axis=1)\n\n        cat_idxs = [0]\n        cat_dims = [128]\n\n        if scheduler_type == 'cosine':\n            scheduler_params = dict(T_0=200, T_mult=1, eta_min=1e-4, last_epoch=-1, verbose=False)\n            scheduler_fn = CosineAnnealingWarmRestarts\n        else:\n            scheduler_params = {'mode': 'min', 'min_lr': 1e-7, 'patience': patience, 'factor': factor, 'verbose': True}\n            scheduler_fn = torch.optim.lr_scheduler.ReduceLROnPlateau\n\n        model = TabNetRegressor(\n            cat_idxs=cat_idxs,\n            cat_dims=cat_dims,\n            cat_emb_dim=1,\n            n_d=n_a,\n            n_a=n_a,\n            n_steps=n_steps,\n            gamma=gamma,\n            n_independent=2,\n            n_shared=2,\n            lambda_sparse=lambda_sparse,\n            optimizer_fn=torch.optim.Adam,\n            optimizer_params={'lr': lr},\n            mask_type=\"entmax\",\n            scheduler_fn=scheduler_fn,\n            scheduler_params=scheduler_params,\n            seed=seed,\n            verbose=10\n            #device_name=device,\n            #clip_value=1.5\n        )\n\n        model.fit(X_tr, y_tr, eval_set=[(X_va, y_va)], max_epochs=epochs, patience=50, batch_size=1024*20,\n                  virtual_batch_size=batch_size, num_workers=4, drop_last=False, eval_metric=[RMSPE], loss_fn=RMSPELoss_Tabnet)\n\n        path = os.path.join(output_dir, model_path.format(cv_idx))\n        model.save_model(path)\n\n        predicted = model.predict(X_va)\n\n        rmspe = rmspe_metric(y_va, predicted)\n        best_losses.append(rmspe)\n        best_predictions.append(predicted)\n\n    return best_losses, best_predictions, scaler, model\n\n\ndef train_nn(X: pd.DataFrame,\n             y: pd.DataFrame,\n             folds: List[Tuple],\n             device,\n             emb_dim: int = 25,\n             batch_size: int = 1024,\n             model_type: str = 'mlp',\n             mlp_dropout: float = 0.0,\n             mlp_hidden: int = 64,\n             mlp_bn: bool = False,\n             cnn_hidden: int = 64,\n             cnn_channel1: int = 32,\n             cnn_channel2: int = 32,\n             cnn_channel3: int = 32,\n             cnn_kernel1: int = 5,\n             cnn_celu: bool = False,\n             cnn_weight_norm: bool = False,\n             dropout_emb: bool = 0.0,\n             lr: float = 1e-3,\n             weight_decay: float = 0.0,\n             model_path: str = 'fold_{}.pth',\n             scaler_type: str = 'standard',\n             output_dir: str = 'artifacts',\n             scheduler_type: str = 'onecycle',\n             optimizer_type: str = 'adam',\n             max_lr: float = 0.01,\n             epochs: int = 30,\n             seed: int = 42,\n             n_pca: int = -1,\n             batch_double_freq: int = 50,\n             cnn_dropout: float = 0.1,\n             na_cols: bool = True,\n             cnn_leaky_relu: bool = False,\n             patience: int = 8,\n             factor: float = 0.5):\n    seed_everything(seed)\n\n    os.makedirs(output_dir, exist_ok=True)\n\n    y = y.values.astype(np.float32)\n    X_num, X_cat, cat_cols, scaler = preprocess_nn(X.copy(), scaler_type=scaler_type, n_pca=n_pca, na_cols=na_cols)\n\n    best_losses = []\n    best_predictions = []\n\n    for cv_idx, (train_idx, valid_idx) in enumerate(folds):\n        X_tr, X_va = X_num[train_idx], X_num[valid_idx]\n        X_tr_cat, X_va_cat = X_cat[train_idx], X_cat[valid_idx]\n        y_tr, y_va = y[train_idx], y[valid_idx]\n\n        cur_batch = batch_size\n        best_loss = 1e10\n        best_prediction = None\n\n        print(f\"fold {cv_idx} train: {X_tr.shape}, valid: {X_va.shape}\")\n\n        train_dataset = TabularDataset(X_tr, X_tr_cat, y_tr)\n        valid_dataset = TabularDataset(X_va, X_va_cat, y_va)\n        train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=cur_batch, shuffle=True,\n                                                   num_workers=4)\n        valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=cur_batch, shuffle=False,\n                                                   num_workers=4)\n\n        if model_type == 'mlp':\n            model = MLP(X_tr.shape[1],\n                        n_categories=[128],\n                        dropout=mlp_dropout, hidden=mlp_hidden, emb_dim=emb_dim,\n                        dropout_cat=dropout_emb, bn=mlp_bn)\n        elif model_type == 'cnn':\n            model = CNN(X_tr.shape[1],\n                        hidden_size=cnn_hidden,\n                        n_categories=[128],\n                        emb_dim=emb_dim,\n                        dropout_cat=dropout_emb,\n                        channel_1=cnn_channel1,\n                        channel_2=cnn_channel2,\n                        channel_3=cnn_channel3,\n                        two_stage=False,\n                        kernel1=cnn_kernel1,\n                        celu=cnn_celu,\n                        dropout_top=cnn_dropout,\n                        dropout_mid=cnn_dropout,\n                        dropout_bottom=cnn_dropout,\n                        weight_norm=cnn_weight_norm,\n                        leaky_relu=cnn_leaky_relu)\n        else:\n            raise NotImplementedError()\n        model = model.to(device)\n\n        if optimizer_type == 'adamw':\n            opt = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n        elif optimizer_type == 'adam':\n            opt = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n        else:\n            raise NotImplementedError()\n\n        scheduler = epoch_scheduler = None\n        if scheduler_type == 'onecycle':\n            scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer=opt, pct_start=0.1, div_factor=1e3,\n                                                            max_lr=max_lr, epochs=epochs,\n                                                            steps_per_epoch=len(train_loader))\n        elif scheduler_type == 'reduce':\n            epoch_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer=opt,\n                                                                         mode='min',\n                                                                         min_lr=1e-7,\n                                                                         patience=patience,\n                                                                         verbose=True,\n                                                                         factor=factor)\n\n        for epoch in range(epochs):\n            if epoch > 0 and epoch % batch_double_freq == 0:\n                cur_batch = cur_batch * 2\n                print(f'batch: {cur_batch}')\n                train_loader = torch.utils.data.DataLoader(train_dataset,\n                                                           batch_size=cur_batch,\n                                                           shuffle=True,\n                                                           num_workers=4)\n            train_loss = train_epoch(train_loader, model, opt, scheduler, device)\n            predictions, valid_targets, valid_loss, rmspe = evaluate(valid_loader, model, device=device)\n            print(f\"epoch {epoch}, train loss: {train_loss:.3f}, valid rmspe: {rmspe:.3f}\")\n\n            if epoch_scheduler is not None:\n                epoch_scheduler.step(rmspe)\n\n            if rmspe < best_loss:\n                print(f'new best:{rmspe}')\n                best_loss = rmspe\n                best_prediction = predictions\n                torch.save(model, os.path.join(output_dir, model_path.format(cv_idx)))\n\n        best_predictions.append(best_prediction)\n        best_losses.append(best_loss)\n        del model, train_dataset, valid_dataset, train_loader, valid_loader, X_tr, X_va, X_tr_cat, X_va_cat, y_tr, y_va, opt\n        if scheduler is not None:\n            del scheduler\n        gc.collect()\n\n    return best_losses, best_predictions, scaler\n","metadata":{"_kg_hide-input":true,"execution":{"iopub.execute_input":"2022-01-20T23:15:02.152719Z","iopub.status.busy":"2022-01-20T23:15:02.131382Z","iopub.status.idle":"2022-01-20T23:15:03.414655Z","shell.execute_reply":"2022-01-20T23:15:03.413725Z","shell.execute_reply.started":"2022-01-15T04:57:16.2193Z"},"papermill":{"duration":1.355324,"end_time":"2022-01-20T23:15:03.414813","exception":false,"start_time":"2022-01-20T23:15:02.059489","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(device)\n\ndel df, df_train\ngc.collect()\n\ndef get_top_n_models(models, scores, top_n):\n    if len(models) <= top_n:\n        print('number of models are less than top_n. all models will be used')\n        return models\n    sorted_ = [(y, x) for y, x in sorted(zip(scores, models), key=lambda pair: pair[0])]\n    print(f'scores(sorted): {[y for y, _ in sorted_]}')\n    return [x for _, x in sorted_][:top_n]\n\n\nif PREDICT_MLP:\n    model_paths = []\n    scores = []\n    \n    if SHORTCUT_NN_IN_1ST_STAGE and IS_1ST_STAGE:\n        print('shortcut to save quota...')\n        epochs = 3\n        valid_th = 100\n    else:\n        epochs = 30\n        valid_th = NN_VALID_TH\n    \n    for i in range(NN_NUM_MODELS):\n        # MLP\n        nn_losses, nn_preds, scaler = train_nn(X, y, \n                                               [folds[-1]], \n                                               device=device, \n                                               batch_size=512,\n                                               mlp_bn=True,\n                                               mlp_hidden=256,\n                                               mlp_dropout=0.0,\n                                               emb_dim=30,\n                                               epochs=epochs,\n                                               lr=0.002,\n                                               max_lr=0.0055,\n                                               weight_decay=1e-7,\n                                               model_path='mlp_fold_{}' + f\"_seed{i}.pth\",\n                                               seed=i)\n        if nn_losses[0] < NN_VALID_TH:\n            print(f'model of seed {i} added.')\n            scores.append(nn_losses[0])\n            model_paths.append(f'artifacts/mlp_fold_0_seed{i}.pth')\n            np.save(f'pred_mlp_seed{i}.npy', nn_preds[0])\n\n    model_paths = get_top_n_models(model_paths, scores, NN_MODEL_TOP_N)\n    mlp_model = [torch.load(path, device) for path in model_paths]\n    print(f'total {len(mlp_model)} models will be used.')\nif PREDICT_CNN:\n    model_paths = []\n    scores = []\n        \n    if SHORTCUT_NN_IN_1ST_STAGE and IS_1ST_STAGE:\n        print('shortcut to save quota...')\n        epochs = 3\n        valid_th = 100\n    else:\n        epochs = 50\n        valid_th = NN_VALID_TH\n\n    for i in range(NN_NUM_MODELS):\n        nn_losses, nn_preds, scaler = train_nn(X, y, \n                                               [folds[-1]], \n                                               device=device, \n                                               cnn_hidden=8*128,\n                                               batch_size=1280,\n                                               model_type='cnn',\n                                               emb_dim=30,\n                                               epochs=epochs, #epochs,\n                                               cnn_channel1=128,\n                                               cnn_channel2=3*128,\n                                               cnn_channel3=3*128,\n                                               lr=0.00038, #0.0011,\n                                               max_lr=0.0013,\n                                               weight_decay=6.5e-6,\n                                               optimizer_type='adam',\n                                               scheduler_type='reduce',\n                                               model_path='cnn_fold_{}' + f\"_seed{i}.pth\",\n                                               seed=i,\n                                               cnn_dropout=0.0,\n                                               cnn_weight_norm=True,\n                                               cnn_leaky_relu=False,\n                                               patience=8,\n                                               factor=0.3)\n        if nn_losses[0] < valid_th:\n            model_paths.append(f'artifacts/cnn_fold_0_seed{i}.pth')\n            scores.append(nn_losses[0])\n            np.save(f'pred_cnn_seed{i}.npy', nn_preds[0])\n            \n    model_paths = get_top_n_models(model_paths, scores, NN_MODEL_TOP_N)\n    cnn_model = [torch.load(path, device) for path in model_paths]\n    print(f'total {len(cnn_model)} models will be used.')\n    \nif PREDICT_TABNET:\n    tab_model = []\n    scores = []\n        \n    if SHORTCUT_NN_IN_1ST_STAGE and IS_1ST_STAGE:\n        print('shortcut to save quota...')\n        epochs = 10\n        valid_th = 1000\n    else:\n        print('train full')\n        epochs = 250\n        valid_th = NN_VALID_TH\n\n    for i in range(TABNET_NUM_MODELS):\n        nn_losses, nn_preds, scaler, model = train_tabnet(X, y,  \n                                                          [folds[-1]], \n                                                          batch_size=1280,\n                                                          epochs=epochs, #epochs,\n                                                          lr=0.04,\n                                                          patience=50,\n                                                          factor=0.5,\n                                                          gamma=1.6,\n                                                          lambda_sparse=3.55e-6,\n                                                          seed=i,\n                                                          n_a=36)\n        if nn_losses[0] < valid_th:\n            tab_model.append(model)\n            scores.append(nn_losses[0])\n            np.save(f'pred_tab_seed{i}.npy', nn_preds[0])\n            model.save_model(f'artifacts/tabnet_fold_0_seed{i}')\n            \n    tab_model = get_top_n_models(tab_model, scores, TAB_MODEL_TOP_N)\n    print(f'total {len(tab_model)} models will be used.')","metadata":{"execution":{"iopub.execute_input":"2022-01-20T23:15:03.711869Z","iopub.status.busy":"2022-01-20T23:15:03.71091Z","iopub.status.idle":"2022-01-20T23:32:39.715022Z","shell.execute_reply":"2022-01-20T23:32:39.702193Z","shell.execute_reply.started":"2022-01-15T04:57:17.584692Z"},"papermill":{"duration":1056.254171,"end_time":"2022-01-20T23:32:39.71516","exception":false,"start_time":"2022-01-20T23:15:03.460989","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del X, y\ngc.collect()","metadata":{"execution":{"iopub.execute_input":"2022-01-20T23:32:43.566422Z","iopub.status.busy":"2022-01-20T23:32:43.565633Z","iopub.status.idle":"2022-01-20T23:32:43.568823Z","shell.execute_reply":"2022-01-20T23:32:43.569546Z","shell.execute_reply.started":"2022-01-15T04:57:17.81586Z"},"papermill":{"duration":2.002375,"end_time":"2022-01-20T23:32:43.569711","exception":false,"start_time":"2022-01-20T23:32:41.567336","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Inference","metadata":{"papermill":{"duration":2.104011,"end_time":"2022-01-20T23:32:47.556364","exception":false,"start_time":"2022-01-20T23:32:45.452353","status":"completed"},"tags":[]}},{"cell_type":"code","source":"X_test = get_X(df_test)\nprint(X_test.shape)","metadata":{"execution":{"iopub.execute_input":"2022-01-20T23:32:51.268297Z","iopub.status.busy":"2022-01-20T23:32:51.267394Z","iopub.status.idle":"2022-01-20T23:32:51.272397Z","shell.execute_reply":"2022-01-20T23:32:51.27284Z","shell.execute_reply.started":"2022-01-15T04:57:18.009945Z"},"papermill":{"duration":1.882865,"end_time":"2022-01-20T23:32:51.272985","exception":false,"start_time":"2022-01-20T23:32:49.39012","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_pred = pd.DataFrame()\ndf_pred['row_id'] = df_test['stock_id'].astype(str) + '-' + df_test['time_id'].astype(str)\n\npredictions = {}\n\nprediction_weights = {}\n\nif PREDICT_GBDT:\n    gbdt_preds = booster.predict(X_test)\n    predictions['gbdt'] = gbdt_preds\n    prediction_weights['gbdt'] = 4\n\n\nif PREDICT_MLP and mlp_model:\n    try:\n        mlp_preds = predict_nn(X_test, mlp_model, scaler, device, ensemble_method=ENSEMBLE_METHOD)\n        print(f'mlp: {mlp_preds.shape}')\n        predictions['mlp'] = mlp_preds\n        prediction_weights['mlp'] = 1\n    except:\n        print(f'failed to predict mlp: {traceback.format_exc()}')\n\n\nif PREDICT_CNN and cnn_model:\n    try:\n        cnn_preds = predict_nn(X_test, cnn_model, scaler, device, ensemble_method=ENSEMBLE_METHOD)\n        print(f'cnn: {cnn_preds.shape}')\n        predictions['cnn'] = cnn_preds\n        prediction_weights['cnn'] = 4\n    except:\n        print(f'failed to predict cnn: {traceback.format_exc()}')\n\n\nif PREDICT_TABNET and tab_model:\n    try:\n        tab_preds = predict_tabnet(X_test, tab_model, scaler, ensemble_method=ENSEMBLE_METHOD).flatten()\n        print(f'tab: {tab_preds.shape}')\n        predictions['tab'] = tab_preds\n        prediction_weights['tab'] = 1\n    except:\n        print(f'failed to predict tab: {traceback.format_exc()}')\n\n        \noverall_preds = None\noverall_weight = np.sum(list(prediction_weights.values()))\n\nprint(f'prediction will be made by: {list(prediction_weights.keys())}')\n\nfor name, preds in predictions.items():\n    w = prediction_weights[name] / overall_weight\n    if overall_preds is None:\n        overall_preds = preds * w\n    else:\n        overall_preds += preds * w\n        \ndf_pred['target'] = np.clip(overall_preds, 0, None)\n","metadata":{"execution":{"iopub.execute_input":"2022-01-20T23:32:55.030658Z","iopub.status.busy":"2022-01-20T23:32:55.029069Z","iopub.status.idle":"2022-01-20T23:32:55.842449Z","shell.execute_reply":"2022-01-20T23:32:55.841984Z","shell.execute_reply.started":"2022-01-15T04:57:18.025123Z"},"papermill":{"duration":2.707635,"end_time":"2022-01-20T23:32:55.842573","exception":false,"start_time":"2022-01-20T23:32:53.134938","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub = pd.read_csv(os.path.join(DATA_DIR, 'optiver-realized-volatility-prediction', 'sample_submission.csv'))\nsubmission = pd.merge(sub[['row_id']], df_pred[['row_id', 'target']], how='left')\nsubmission['target'] = submission['target'].fillna(0)\nsubmission.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.execute_input":"2022-01-20T23:32:59.842259Z","iopub.status.busy":"2022-01-20T23:32:59.84155Z","iopub.status.idle":"2022-01-20T23:32:59.866268Z","shell.execute_reply":"2022-01-20T23:32:59.865602Z","shell.execute_reply.started":"2022-01-15T04:57:18.056985Z"},"papermill":{"duration":1.863386,"end_time":"2022-01-20T23:32:59.866416","exception":false,"start_time":"2022-01-20T23:32:58.00303","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]}]}