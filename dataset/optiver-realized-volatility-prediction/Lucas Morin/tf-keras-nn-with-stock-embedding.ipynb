{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## NN starter\n\nA simple NN starter using stock Embedding. \n\nHeavily inspired from this notebook for the feature engineering part:\nhttps://www.kaggle.com/manels/lgb-starter\n\nEmbedding layer from :\nhttps://www.kaggle.com/colinmorris/embedding-layers\n\nAdded a better feature engineering approach :\nhttps://www.kaggle.com/tommy1028/lightgbm-starter-with-feature-engineering-idea\n\nAlso see:\n* https://www.kaggle.com/jiashenliu/introduction-to-financial-concepts-and-data\n* https://www.kaggle.com/c/optiver-realized-volatility-prediction/discussion/250324\n\n**I hope it will be useful for other beginners.**","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-07-05T17:50:17.731903Z","iopub.execute_input":"2021-07-05T17:50:17.732318Z","iopub.status.idle":"2021-07-05T17:50:19.647572Z","shell.execute_reply.started":"2021-07-05T17:50:17.732235Z","shell.execute_reply":"2021-07-05T17:50:19.646491Z"}}},{"cell_type":"code","source":"from IPython.core.display import display, HTML\n\nimport pandas as pd\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport glob\nimport os\nimport gc\n\nfrom joblib import Parallel, delayed\nfrom sklearn import preprocessing, model_selection\nfrom sklearn.metrics import r2_score\nimport matplotlib.pyplot as plt \nimport seaborn as sns\n\npath_root = '../input/optiver-realized-volatility-prediction'\npath_data = '../input/optiver-realized-volatility-prediction'\npath_submissions = '/'\n\ntarget_name = 'target'\nscores_folds = {}","metadata":{"execution":{"iopub.status.busy":"2021-07-13T19:31:51.792926Z","iopub.execute_input":"2021-07-13T19:31:51.793534Z","iopub.status.idle":"2021-07-13T19:31:52.985293Z","shell.execute_reply.started":"2021-07-13T19:31:51.793413Z","shell.execute_reply":"2021-07-13T19:31:52.984122Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def rmspe(y_true, y_pred):\n    return  (np.sqrt(np.mean(np.square((y_true - y_pred) / y_true))))","metadata":{"execution":{"iopub.status.busy":"2021-07-13T19:31:52.987025Z","iopub.execute_input":"2021-07-13T19:31:52.987475Z","iopub.status.idle":"2021-07-13T19:31:52.992696Z","shell.execute_reply.started":"2021-07-13T19:31:52.987429Z","shell.execute_reply":"2021-07-13T19:31:52.991691Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"def log_return(list_stock_prices):\n    return np.log(list_stock_prices).diff() \n\ndef realized_volatility(series_log_return):\n    return np.sqrt(np.sum(series_log_return**2))\n\n\n\ndef get_stock_stat(stock_id : int, dataType = 'train'):\n    key = ['stock_id', 'time_id', 'seconds_in_bucket']\n    \n    #Book features\n    df_book = pd.read_parquet(os.path.join(path_data, 'book_{}.parquet/stock_id={}/'.format(dataType, stock_id)))\n    df_book['stock_id'] = stock_id\n    cols = key + [col for col in df_book.columns if col not in key]\n    df_book = df_book[cols]\n    \n    df_book['wap1'] = (df_book['bid_price1'] * df_book['ask_size1'] +\n                                    df_book['ask_price1'] * df_book['bid_size1']) / (df_book['bid_size1'] + df_book['ask_size1'])\n    df_book['wap2'] = (df_book['bid_price2'] * df_book['ask_size2'] +\n                                    df_book['ask_price2'] * df_book['bid_size2']) / (df_book['bid_size2'] + df_book['ask_size2'])\n    df_book['log_return1'] = df_book.groupby(by = ['time_id'])['wap1'].apply(log_return).fillna(0)\n    df_book['log_return2'] = df_book.groupby(by = ['time_id'])['wap2'].apply(log_return).fillna(0)\n    \n    features_to_apply_realized_volatility = ['log_return'+str(i+1) for i in range(2)]\n    stock_stat = df_book.groupby(by = ['stock_id', 'time_id'])[features_to_apply_realized_volatility]\\\n                        .agg(realized_volatility).reset_index()\n\n    #Trade features\n    trade_stat =  pd.read_parquet(os.path.join(path_data,'trade_{}.parquet/stock_id={}'.format(dataType, stock_id)))\n    trade_stat = trade_stat.sort_values(by=['time_id', 'seconds_in_bucket']).reset_index(drop=True)\n    trade_stat['stock_id'] = stock_id\n    cols = key + [col for col in trade_stat.columns if col not in key]\n    trade_stat = trade_stat[cols]\n    trade_stat['trade_log_return1'] = trade_stat.groupby(by = ['time_id'])['price'].apply(log_return).fillna(0)\n    trade_stat = trade_stat.groupby(by = ['stock_id', 'time_id'])[['trade_log_return1']]\\\n                           .agg(realized_volatility).reset_index()\n    #Joining book and trade features\n    stock_stat = stock_stat.merge(trade_stat, on=['stock_id', 'time_id'], how='left').fillna(-999)\n    \n    return stock_stat\n\ndef get_dataSet(stock_ids : list, dataType = 'train'):\n\n    stock_stat = Parallel(n_jobs=-1)(\n        delayed(get_stock_stat)(stock_id, dataType) \n        for stock_id in stock_ids\n    )\n    \n    stock_stat_df = pd.concat(stock_stat, ignore_index = True)\n\n    return stock_stat_df\n    \n\"\"\"","metadata":{"execution":{"iopub.status.busy":"2021-07-13T19:31:52.994323Z","iopub.execute_input":"2021-07-13T19:31:52.994613Z","iopub.status.idle":"2021-07-13T19:31:53.016472Z","shell.execute_reply.started":"2021-07-13T19:31:52.994583Z","shell.execute_reply":"2021-07-13T19:31:53.015328Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport glob\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\npd.set_option('max_rows', 300)\npd.set_option('max_columns', 300)\n\n# data directory\ndata_dir = '../input/optiver-realized-volatility-prediction/'","metadata":{"execution":{"iopub.status.busy":"2021-07-13T19:31:53.018353Z","iopub.execute_input":"2021-07-13T19:31:53.018784Z","iopub.status.idle":"2021-07-13T19:31:53.040039Z","shell.execute_reply.started":"2021-07-13T19:31:53.018727Z","shell.execute_reply":"2021-07-13T19:31:53.038986Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def calc_wap(df):\n    wap = (df['bid_price1'] * df['ask_size1'] + df['ask_price1'] * df['bid_size1'])/(df['bid_size1'] + df['ask_size1'])\n    return wap\n\ndef calc_wap2(df):\n    wap = (df['bid_price2'] * df['ask_size2'] + df['ask_price2'] * df['bid_size2'])/(df['bid_size2'] + df['ask_size2'])\n    return wap\n\ndef log_return(list_stock_prices):\n    return np.log(list_stock_prices).diff() \n\ndef realized_volatility(series):\n    return np.sqrt(np.sum(series**2))\n\ndef count_unique(series):\n    return len(np.unique(series))","metadata":{"execution":{"iopub.status.busy":"2021-07-13T19:31:53.041882Z","iopub.execute_input":"2021-07-13T19:31:53.042226Z","iopub.status.idle":"2021-07-13T19:31:53.053917Z","shell.execute_reply.started":"2021-07-13T19:31:53.042172Z","shell.execute_reply":"2021-07-13T19:31:53.05298Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def preprocessor_book(file_path):\n    \n    df = pd.read_parquet(file_path)\n    #calculate return etc\n    df['wap'] = calc_wap(df)\n    df['log_return'] = df.groupby('time_id')['wap'].apply(log_return)\n    \n    df['wap2'] = calc_wap2(df)\n    df['log_return2'] = df.groupby('time_id')['wap2'].apply(log_return)\n    \n    df['wap_balance'] = abs(df['wap'] - df['wap2'])\n    \n    df['price_spread'] = (df['ask_price1'] - df['bid_price1']) / ((df['ask_price1'] + df['bid_price1'])/2)\n    df['bid_spread'] = df['bid_price1'] - df['bid_price2']\n    df['ask_spread'] = df['ask_price1'] - df['ask_price2']\n    df['total_volume'] = (df['ask_size1'] + df['ask_size2']) + (df['bid_size1'] + df['bid_size2'])\n    df['volume_imbalance'] = abs((df['ask_size1'] + df['ask_size2']) - (df['bid_size1'] + df['bid_size2']))\n\n    #dict for aggregate\n    create_feature_dict = {\n        'log_return':[realized_volatility],\n        'log_return2':[realized_volatility],\n        'wap_balance':[np.mean],\n        'price_spread':[np.mean],\n        'bid_spread':[np.mean],\n        'ask_spread':[np.mean],\n        'volume_imbalance':[np.mean],\n        'total_volume':[np.mean],\n        'wap':[np.mean],\n            }\n\n    #####groupby / all seconds\n    df_feature = pd.DataFrame(df.groupby(['time_id']).agg(create_feature_dict)).reset_index()\n    \n    df_feature.columns = ['_'.join(col) for col in df_feature.columns] #time_id is changed to time_id_\n        \n    ######groupby / last XX seconds\n    last_seconds = [300]\n    \n    for second in last_seconds:\n        second = 600 - second \n    \n        df_feature_sec = pd.DataFrame(df.query(f'seconds_in_bucket >= {second}').groupby(['time_id']).agg(create_feature_dict)).reset_index()\n\n        df_feature_sec.columns = ['_'.join(col) for col in df_feature_sec.columns] #time_id is changed to time_id_\n     \n        df_feature_sec = df_feature_sec.add_suffix('_' + str(second))\n\n        df_feature = pd.merge(df_feature,df_feature_sec,how='left',left_on='time_id_',right_on=f'time_id__{second}')\n        df_feature = df_feature.drop([f'time_id__{second}'],axis=1)\n    \n    #create row_id\n    stock_id = file_path.split('=')[1]\n    df_feature['row_id'] = df_feature['time_id_'].apply(lambda x:f'{stock_id}-{x}')\n    df_feature = df_feature.drop(['time_id_'],axis=1)\n    \n    return df_feature\n\n\ndef preprocessor_trade(file_path):\n    df = pd.read_parquet(file_path)\n    df['log_return'] = df.groupby('time_id')['price'].apply(log_return)\n    \n    \n    aggregate_dictionary = {\n        'log_return':[realized_volatility],\n        'seconds_in_bucket':[count_unique],\n        'size':[np.sum],\n        'order_count':[np.mean],\n    }\n    \n    df_feature = df.groupby('time_id').agg(aggregate_dictionary)\n    \n    df_feature = df_feature.reset_index()\n    df_feature.columns = ['_'.join(col) for col in df_feature.columns]\n\n    \n    ######groupby / last XX seconds\n    last_seconds = [300]\n    \n    for second in last_seconds:\n        second = 600 - second\n    \n        df_feature_sec = df.query(f'seconds_in_bucket >= {second}').groupby('time_id').agg(aggregate_dictionary)\n        df_feature_sec = df_feature_sec.reset_index()\n        \n        df_feature_sec.columns = ['_'.join(col) for col in df_feature_sec.columns]\n        df_feature_sec = df_feature_sec.add_suffix('_' + str(second))\n        \n        df_feature = pd.merge(df_feature,df_feature_sec,how='left',left_on='time_id_',right_on=f'time_id__{second}')\n        df_feature = df_feature.drop([f'time_id__{second}'],axis=1)\n    \n    df_feature = df_feature.add_prefix('trade_')\n    stock_id = file_path.split('=')[1]\n    df_feature['row_id'] = df_feature['trade_time_id_'].apply(lambda x:f'{stock_id}-{x}')\n    df_feature = df_feature.drop(['trade_time_id_'],axis=1)\n    \n    return df_feature","metadata":{"execution":{"iopub.status.busy":"2021-07-13T19:31:53.055013Z","iopub.execute_input":"2021-07-13T19:31:53.055316Z","iopub.status.idle":"2021-07-13T19:31:53.076406Z","shell.execute_reply.started":"2021-07-13T19:31:53.055284Z","shell.execute_reply":"2021-07-13T19:31:53.075165Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def preprocessor(list_stock_ids, is_train = True):\n    from joblib import Parallel, delayed # parallel computing to save time\n    df = pd.DataFrame()\n    \n    def for_joblib(stock_id):\n        if is_train:\n            file_path_book = data_dir + \"book_train.parquet/stock_id=\" + str(stock_id)\n            file_path_trade = data_dir + \"trade_train.parquet/stock_id=\" + str(stock_id)\n        else:\n            file_path_book = data_dir + \"book_test.parquet/stock_id=\" + str(stock_id)\n            file_path_trade = data_dir + \"trade_test.parquet/stock_id=\" + str(stock_id)\n            \n        df_tmp = pd.merge(preprocessor_book(file_path_book),preprocessor_trade(file_path_trade),on='row_id',how='left')\n     \n        return pd.concat([df,df_tmp])\n    \n    df = Parallel(n_jobs=-1, verbose=1)(\n        delayed(for_joblib)(stock_id) for stock_id in list_stock_ids\n        )\n\n    df =  pd.concat(df,ignore_index = True)\n    return df","metadata":{"execution":{"iopub.status.busy":"2021-07-13T19:31:53.078608Z","iopub.execute_input":"2021-07-13T19:31:53.079118Z","iopub.status.idle":"2021-07-13T19:31:53.096691Z","shell.execute_reply.started":"2021-07-13T19:31:53.079073Z","shell.execute_reply":"2021-07-13T19:31:53.095864Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Train and test datasets","metadata":{}},{"cell_type":"code","source":"\"\"\"train = pd.read_csv(os.path.join(path_data, 'train.csv'))\n%time train_stock_stat_df = get_dataSet(stock_ids = train['stock_id'].unique(), dataType = 'train')\ntrain = pd.merge(train, train_stock_stat_df, on = ['stock_id', 'time_id'], how = 'left')\nprint('Train shape: {}'.format(train.shape))\ndisplay(train.head(2))\n\ntest = pd.read_csv(os.path.join(path_data, 'test.csv'))\ntest_stock_stat_df = get_dataSet(stock_ids = test['stock_id'].unique(), dataType = 'test')\ntest = pd.merge(test, test_stock_stat_df, on = ['stock_id', 'time_id'], how = 'left').fillna(0)\nprint('Test shape: {}'.format(test.shape))\ndisplay(test.head(2))\"\"\"","metadata":{"execution":{"iopub.status.busy":"2021-07-13T19:31:53.098471Z","iopub.execute_input":"2021-07-13T19:31:53.09879Z","iopub.status.idle":"2021-07-13T19:31:53.116663Z","shell.execute_reply.started":"2021-07-13T19:31:53.098757Z","shell.execute_reply":"2021-07-13T19:31:53.115628Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv(data_dir + 'train.csv')\ntrain_ids = train.stock_id.unique()\ndf_train = preprocessor(list_stock_ids= train_ids, is_train = True)\ntrain['row_id'] = train['stock_id'].astype(str) + '-' + train['time_id'].astype(str)\ntrain = train[['row_id','target']]\ndf_train = train.merge(df_train, on = ['row_id'], how = 'left')\n\ntest = pd.read_csv(data_dir + 'test.csv')\ntest_ids = test.stock_id.unique()\ndf_test = preprocessor(list_stock_ids= test_ids, is_train = False)\ndf_test = test.merge(df_test, on = ['row_id'], how = 'left')\n\nfrom sklearn.model_selection import KFold\n#stock_id target encoding\ndf_train['stock_id'] = df_train['row_id'].apply(lambda x:x.split('-')[0])\ndf_test['stock_id'] = df_test['row_id'].apply(lambda x:x.split('-')[0])\nstock_id_target_mean = df_train.groupby('stock_id')['target'].mean() \ndf_test['stock_id_target_enc'] = df_test['stock_id'].map(stock_id_target_mean) # test_set\n\n#training\ntmp = np.repeat(np.nan, df_train.shape[0])\nkf = KFold(n_splits = 10, shuffle=True,random_state = 777)\nfor idx_1, idx_2 in kf.split(df_train):\n    target_mean = df_train.iloc[idx_1].groupby('stock_id')['target'].mean()\n    tmp[idx_2] = df_train['stock_id'].iloc[idx_2].map(target_mean)\ndf_train['stock_id_target_enc'] = tmp","metadata":{"execution":{"iopub.status.busy":"2021-07-13T19:31:53.117906Z","iopub.execute_input":"2021-07-13T19:31:53.118169Z","iopub.status.idle":"2021-07-13T19:41:54.625057Z","shell.execute_reply.started":"2021-07-13T19:31:53.118142Z","shell.execute_reply":"2021-07-13T19:41:54.624153Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training model and making predictions","metadata":{}},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2021-07-13T20:10:03.06154Z","iopub.execute_input":"2021-07-13T20:10:03.06203Z","iopub.status.idle":"2021-07-13T20:10:03.078858Z","shell.execute_reply.started":"2021-07-13T20:10:03.061997Z","shell.execute_reply":"2021-07-13T20:10:03.077505Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow import keras\nimport numpy as np\nfrom keras import backend as K\n\nhidden_units = [64,32,16,8]\nstock_embedding_size = 16\n\ncat_data = df_train['stock_id'].astype('int')\n\nmean_target = np.median(df_train.target)\n\nfeatures = ['log_return_realized_volatility',\n       'log_return2_realized_volatility', 'wap_balance_mean',\n       'price_spread_mean', 'bid_spread_mean', 'ask_spread_mean',\n       'volume_imbalance_mean', 'total_volume_mean', 'wap_mean',\n       'log_return_realized_volatility_300',\n       'log_return2_realized_volatility_300', 'wap_balance_mean_300',\n       'price_spread_mean_300', 'bid_spread_mean_300', 'ask_spread_mean_300',\n       'volume_imbalance_mean_300', 'total_volume_mean_300', 'wap_mean_300',\n       'trade_log_return_realized_volatility',\n       'trade_seconds_in_bucket_count_unique', 'trade_size_sum',\n       'trade_order_count_mean', 'trade_log_return_realized_volatility_300',\n       'trade_seconds_in_bucket_count_unique_300', 'trade_size_sum_300',\n       'trade_order_count_mean_300','stock_id_target_enc']\nfeatures_to_consider = ['stock_id'] + features\n\nnb_features = len(features)\n\ndef base_model():\n    \n    # Each instance will consist of two inputs: a single user id, and a single movie id\n    stock_id_input = keras.Input(shape=(1,), name='stock_id')\n    num_input = keras.Input(shape=(nb_features,), name='num_data')\n\n\n    #embedding, flatenning and concatenating\n    stock_embedded = keras.layers.Embedding(max(cat_data)+1, stock_embedding_size, \n                                           input_length=1, name='stock_embedding')(stock_id_input)\n    \n    stock_flattened = keras.layers.Flatten()(stock_embedded)\n    \n    out = keras.layers.Concatenate()([stock_flattened, num_input])\n    \n    out = keras.layers.GaussianNoise(0.1)(out)\n    \n    # Add one or more hidden layers\n    for n_hidden in hidden_units:\n\n        out = keras.layers.Dense(n_hidden, activation='swish')(out)\n        \n    #out = keras.layers.Concatenate()([out, num_input])\n\n    # A single output: our predicted rating\n    out = keras.layers.Dense(1, activation='swish', name='prediction',bias_initializer=tf.keras.initializers.Constant(value = mean_target))(out)\n\n    model = keras.Model(\n    inputs = [stock_id_input, num_input],\n    outputs = out,\n    )\n    \n    return model","metadata":{"execution":{"iopub.status.busy":"2021-07-13T21:11:48.009429Z","iopub.execute_input":"2021-07-13T21:11:48.009973Z","iopub.status.idle":"2021-07-13T21:11:48.037999Z","shell.execute_reply.started":"2021-07-13T21:11:48.009934Z","shell.execute_reply":"2021-07-13T21:11:48.036639Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_name = 'NN'\npred_name = 'pred_{}'.format(model_name)\n\nn_folds = 4\nkf = model_selection.KFold(n_splits=n_folds, shuffle=True, random_state=2020)\nscores_folds[model_name] = []\ncounter = 1\n\ndf_train['stock_id'] = df_train['stock_id'].astype('int')\ndf_test['stock_id'] = df_train['stock_id'].astype('int')\n\n\ntrain = df_train.fillna(0)\ntest = df_test.fillna(0)\n\ntrain[pred_name] = 0\ntest['target'] = 0\n\nes = tf.keras.callbacks.EarlyStopping(\n    monitor='val_loss', min_delta=1e-05, patience=7, verbose=1,\n    mode='min')\n\nplateau = tf.keras.callbacks.ReduceLROnPlateau(\n    monitor='val_loss', factor=0.1, patience=3, verbose=1, min_lr=1e-7,\n    mode='min')\n\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\n\n\nfor dev_index, val_index in kf.split(range(len(train))):\n    print('CV {}/{}'.format(counter, n_folds))\n    \n    #Bottleneck ? \n    X_train = train.loc[dev_index, features_to_consider]\n    y_train = train.loc[dev_index, target_name].values\n    X_test = train.loc[val_index, features_to_consider]\n    y_test = train.loc[val_index, target_name].values\n    \n    #############################################################################################\n    # NN\n    #############################################################################################\n    \n    model = base_model()\n    \n    model.compile(\n        keras.optimizers.Adam(learning_rate=0.01),\n        loss='mean_squared_error',\n        metrics=['MSE'],\n    )\n\n    num_data = scaler.fit_transform(X_train[features])\n    cat_data = X_train['stock_id']\n    \n    num_data_test = scaler.transform(X_test[features])\n    cat_data_test = X_test['stock_id']\n\n    model.fit([cat_data, num_data], \n              y_train, \n              sample_weight = 1/np.square(y_train),\n              batch_size=1024,\n              epochs=100,\n              validation_data=([cat_data_test, num_data_test], y_test, 1/np.square(y_test)),\n              callbacks=[es, plateau],\n              shuffle=True,\n             verbose = 1)\n\n    preds = model.predict([cat_data_test, num_data_test]).reshape(1,-1)[0]\n    \n    score = round(rmspe(y_true = y_test, y_pred = preds),5)\n    print('Fold {} {}: {}'.format(counter, model_name, score))\n    scores_folds[model_name].append(score)\n    test[target_name] += model.predict([test['stock_id'], scaler.transform(test[features])]).reshape(1,-1)[0].clip(0,1e10)\n       \n    counter += 1","metadata":{"execution":{"iopub.status.busy":"2021-07-13T21:14:07.591955Z","iopub.execute_input":"2021-07-13T21:14:07.592376Z","iopub.status.idle":"2021-07-13T21:20:40.514527Z","shell.execute_reply.started":"2021-07-13T21:14:07.592342Z","shell.execute_reply":"2021-07-13T21:20:40.512674Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test[target_name] = test[target_name]/n_folds\n\nscore = round(rmspe(y_true = train[target_name].values, y_pred = train[pred_name].values),5)\nprint('RMSPE {}: {} - Folds: {}'.format(model_name, score, scores_folds[model_name]))\n\ndisplay(test[['row_id', target_name]].head(2))\ntest[['row_id', target_name]].to_csv('submission.csv',index = False)","metadata":{"execution":{"iopub.status.busy":"2021-07-13T20:56:43.08071Z","iopub.status.idle":"2021-07-13T20:56:43.081117Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}}]}