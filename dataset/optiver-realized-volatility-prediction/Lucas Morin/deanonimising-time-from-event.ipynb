{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Deanonimising time from event\n\nIt's still unclear what is happening exactly. For a while I tought there was an event between training data and target. But it seems that the event appears before the training data. Optiver even took some extra step to anonymize the time from the event by cutting a random amount of time at the begining of the data set. \n\nI figured this time from event could do a good feature and could be estimated from the volatilities and returns. The whole approach is explained in this notebook: https://www.kaggle.com/lucasmorin/volatility-maximum-likelihood-estimation (which in turn heavily draw from : https://www.kaggle.com/pcarta/jane-street-time-horizons-and-volatilities).\n\nNote that this is an approximation (assuming constant vol from observed data is not ideal for a volatility forecasting competition), but a usefull one. We also assume independant movements, this is also far from ideal.\n\nFor each time id, we use initial returns and estimated realized volatilities of different stocks to minimise:\n\n$$\n\\mathcal{l}(\\sigma, T) = \\sum_{i = 1}^n \\left(\\frac{{\\Delta W_{i}}^2}{2 \\sigma_i^2 (\\Delta T)} + \\frac{1}{2}\\log(\\Delta T) + \\log(\\sigma_i)\\right) + \\text{const}\n$$\n\nIt allows us to get the time from when the price was standardised, which I assume to be the time from event. If we are dealing with decaying volatilities from an event, this features might be a good one, with volatilities generally getting lower as this time from event grows.","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom joblib import Parallel, delayed\nfrom scipy.optimize import minimize\nimport matplotlib.pyplot as plt","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def calc_wap(df):\n    wap = (df['bid_price1'] * df['ask_size1'] + df['ask_price1'] * df['bid_size1'])/(df['bid_size1'] + df['ask_size1'])\n    return wap\n\ndef log_return(list_stock_prices):\n    return np.log(list_stock_prices).diff()\n\ndef realized_volatility(series_log_return):\n    return np.sqrt(np.sum(series_log_return**2))\n\nget_first_ret = lambda x: np.log(x.iloc[0])\nget_first_ret.__name__ = 'get_first_ret'\n\ndef preprocessor_book(file_path_book):\n\n    df_book = pd.read_parquet(file_path_book)\n    stock_id = int(file_path_book.split('=')[1])\n\n    df_book['wap'] = calc_wap(df_book)\n    df_book['log_wap'] = np.log(df_book['wap'])\n    df_book['log_return'] = df_book.groupby('time_id')['log_wap'].diff()\n    \n    create_feature_dict_time = {\n        'wap': [get_first_ret],\n        'log_return': [realized_volatility],\n    }\n\n    # Function to get group stats for different windows (seconds in bucket)\n    def get_stats_window(fe_dict,seconds_in_bucket, add_suffix = False):\n        # Group by the window\n        df_feature = df_book[df_book['seconds_in_bucket'] >= seconds_in_bucket].groupby(['time_id']).agg(fe_dict).reset_index()\n        # Rename columns joining suffix\n        df_feature.columns = ['_'.join(col) for col in df_feature.columns]\n        # Add a suffix to differentiate windows\n        if add_suffix:\n            df_feature = df_feature.add_suffix('_' + str(seconds_in_bucket))\n        return df_feature\n    \n    # all features\n    df_feature = get_stats_window(create_feature_dict_time, seconds_in_bucket = 0, add_suffix = False)\n\n    return df_feature\n\n\ndef preprocessor(list_stock_ids, is_train = True):\n    \n    def for_joblib(stock_id):\n\n        if is_train:\n            file_path_book = data_dir + \"book_train.parquet/stock_id=\" + str(stock_id)\n        else:\n            file_path_book = data_dir + \"book_test.parquet/stock_id=\" + str(stock_id)\n            \n        df_tmp = preprocessor_book(file_path_book)\n        \n        return df_tmp\n    \n    # Use parallel api to call paralle for loop\n    df = Parallel(n_jobs = -1, verbose = 1)(delayed(for_joblib)(stock_id) for stock_id in list_stock_ids)\n    # Concatenate all the dataframes that return from Parallel\n    df = pd.concat(df, ignore_index = True)\n    \n    return df\n\ndef read_train_test():\n    train = pd.read_csv('../input/optiver-realized-volatility-prediction/train.csv')\n    test = pd.read_csv('../input/optiver-realized-volatility-prediction/test.csv')\n    # Create a key to merge with book and trade data\n    train['row_id'] = train['stock_id'].astype(str) + '-' + train['time_id'].astype(str)\n    test['row_id'] = test['stock_id'].astype(str) + '-' + test['time_id'].astype(str)\n    print(f'Our training set has {train.shape[0]} rows')\n    return train, test","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\ndata_dir  ='../input/optiver-realized-volatility-prediction/'\ntrain, test = read_train_test()\ntrain_stock_ids = train['stock_id'].unique()\ntest_stock_ids = test['stock_id'].unique()\ntrain_ = preprocessor(train_stock_ids, is_train = True)\ntest_ = preprocessor(test_stock_ids, is_train = False)","metadata":{"execution":{"iopub.status.busy":"2021-09-12T12:49:45.791504Z","iopub.execute_input":"2021-09-12T12:49:45.791968Z","iopub.status.idle":"2021-09-12T12:52:15.822546Z","shell.execute_reply.started":"2021-09-12T12:49:45.791921Z","shell.execute_reply":"2021-09-12T12:52:15.821317Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\nlist_time_id = []\nlist_time_delta = []\n\n\nfor i in train_.time_id_.unique():\n    dft = train_[train_.time_id_==i]\n\n    dW = dft.wap_get_first_ret\n    Sigma = dft.log_return_realized_volatility\n\n    # initialize time\n    dT_1 = 25\n    log_Sigma = np.log(Sigma)\n    dT_1_log = np.log(dT_1)\n\n    def neg_log_likelihood(dT_1_log):\n         # compute the log-likelihood\n        logL = 1/2 * np.sum((dW**2 @ (1/np.exp(2*log_Sigma) * (1/np.exp(dT_1_log))))) + 1/2*np.sum(dT_1_log) + np.sum(log_Sigma)\n        return logL\n\n    res = np.exp(minimize(neg_log_likelihood, dT_1_log, method='nelder-mead',options={'xatol': 1e-8, 'disp': False}).final_simplex[0][0][0])\n    \n    list_time_id.append(i)\n    list_time_delta.append(res)","metadata":{"execution":{"iopub.status.busy":"2021-09-12T12:53:20.09443Z","iopub.execute_input":"2021-09-12T12:53:20.094869Z","iopub.status.idle":"2021-09-12T12:58:14.942738Z","shell.execute_reply.started":"2021-09-12T12:53:20.094831Z","shell.execute_reply":"2021-09-12T12:58:14.941444Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"map_delta = pd.Series(list_time_delta,index=list_time_id)\ntrain['delta_T'] = train.time_id.map(map_delta)\ntrain['rv'] = train_['log_return_realized_volatility']\ntrain['percentage_error'] = (train['target'] - train['rv'])/train['target']\n\nimport random\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n","metadata":{"execution":{"iopub.status.busy":"2021-09-12T12:52:15.974093Z","iopub.execute_input":"2021-09-12T12:52:15.974428Z","iopub.status.idle":"2021-09-12T12:52:15.979561Z","shell.execute_reply.started":"2021-09-12T12:52:15.974394Z","shell.execute_reply":"2021-09-12T12:52:15.978592Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.regplot(x=train['delta_T'],y=train['rv'],color=(random.random(), random.random(), random.random()), order = 2, line_kws={\"color\": 'black'})","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As expected we can see the volatilities decaying with delta_T.","metadata":{}},{"cell_type":"code","source":"sns.regplot(x=train['delta_T'],y=train['percentage_error'],color=(random.random(), random.random(), random.random()), order = 2, line_kws={\"color\": 'black'})","metadata":{},"execution_count":null,"outputs":[]}]}