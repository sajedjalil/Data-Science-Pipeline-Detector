{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"This notebook shows simple flow to deep dive into the competition.I appreciate community of kaggle.\nI refered to following notebooks.\n\n(Reference)  \n**Introduction to financial concepts and data**  \nhttps://www.kaggle.com/jiashenliu/introduction-to-financial-concepts-and-data  \n\n**LGB Starter**  \nhttps://www.kaggle.com/manels/lgb-starter/notebook","metadata":{}},{"cell_type":"markdown","source":"# Agenda\n\n1. Import modules  \n2. Common settings\n3. Function Definition\n4. Preprocessing  \n  4-1. Book parquet data processing  \n  4-2. Trade parquet data processing  \n  4-3. Merge book and trade data  \n  4-4. Train data preprocessing  \n  4-5. Test data preprocessing  \n5. Training  \n  5-1. Training function1 - Light GBM  \n  5-2. Cross Validation  \n6. Evaluation\n7. Prediction  \n8. Submission\n","metadata":{}},{"cell_type":"markdown","source":"# 1. Import modules","metadata":{}},{"cell_type":"code","source":"import os\nimport sys\nimport time\nimport glob\nfrom pathlib import Path\n\nimport pandas as pd\nimport numpy as np\n\n# Parallel processing\nfrom joblib import Parallel\nfrom joblib import delayed\n\n# Preprocess\nfrom sklearn import preprocessing\nfrom sklearn import model_selection\n\n# Evaluation\nfrom sklearn.metrics import r2_score\n\n# Visullize\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Modeling\n#import lightgbm as lgb\nimport optuna.integration.lightgbm as lgb\n\n\n# Others\nimport warnings\nwarnings.simplefilter(\"ignore\")\n","metadata":{"execution":{"iopub.status.busy":"2021-09-27T23:48:23.272543Z","iopub.execute_input":"2021-09-27T23:48:23.272976Z","iopub.status.idle":"2021-09-27T23:48:23.280006Z","shell.execute_reply.started":"2021-09-27T23:48:23.27292Z","shell.execute_reply":"2021-09-27T23:48:23.278876Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2. Common Settings","metadata":{}},{"cell_type":"code","source":"# Dataset path\ndata_path = Path('../input/optiver-realized-volatility-prediction')\n\n# setting display option\npd.options.display.max_columns = 50","metadata":{"execution":{"iopub.status.busy":"2021-09-27T23:48:23.281997Z","iopub.execute_input":"2021-09-27T23:48:23.282802Z","iopub.status.idle":"2021-09-27T23:48:23.295698Z","shell.execute_reply.started":"2021-09-27T23:48:23.282759Z","shell.execute_reply":"2021-09-27T23:48:23.294827Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Objective variable\ntarget = 'target'\n\n# submission file setting\nsubmit_file = 'submission.csv'\nId_column = 'row_id'","metadata":{"execution":{"iopub.status.busy":"2021-09-27T23:48:23.297408Z","iopub.execute_input":"2021-09-27T23:48:23.297886Z","iopub.status.idle":"2021-09-27T23:48:23.309582Z","shell.execute_reply.started":"2021-09-27T23:48:23.297838Z","shell.execute_reply":"2021-09-27T23:48:23.308604Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3. Functions Definition  ","metadata":{}},{"cell_type":"code","source":"#　Log Return\ndef log_return(list_stock_prices):\n    return np.log(list_stock_prices).diff() \n\n# Realized Volatility\ndef realized_volatility(series_log_return):\n    return np.sqrt(np.sum(series_log_return**2))","metadata":{"execution":{"iopub.status.busy":"2021-09-27T23:48:23.311244Z","iopub.execute_input":"2021-09-27T23:48:23.311765Z","iopub.status.idle":"2021-09-27T23:48:23.322381Z","shell.execute_reply.started":"2021-09-27T23:48:23.311716Z","shell.execute_reply":"2021-09-27T23:48:23.321267Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# WAP calculation\ndef wap_calculation1(df):\n    return (df['bid_price1'] * df['ask_size1'] + df['ask_price1'] * df['bid_size1']) / (df['bid_size1'] + df['ask_size1'])\n\ndef wap_calculation2(df):\n    return (df['bid_price2'] * df['ask_size2'] + df['ask_price2'] * df['bid_size2']) / (df['bid_size2'] + df['ask_size2'])\ndef wap_calculation3(df):\n    wap = (df['bid_price1'] * df['bid_size1'] + df['ask_price1'] * df['ask_size1']) / (df['bid_size1'] + df['ask_size1'])\n    return wap\n\ndef wap_calculation4(df):\n    wap = (df['bid_price2'] * df['bid_size2'] + df['ask_price2'] * df['ask_size2']) / (df['bid_size2'] + df['ask_size2'])\n    return wap","metadata":{"execution":{"iopub.status.busy":"2021-09-27T23:48:23.323762Z","iopub.execute_input":"2021-09-27T23:48:23.32408Z","iopub.status.idle":"2021-09-27T23:48:23.339408Z","shell.execute_reply.started":"2021-09-27T23:48:23.324048Z","shell.execute_reply":"2021-09-27T23:48:23.338244Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# my palams\n# askprice1 - bidprice1\n# askprice2 - bidprice2\n# askprice2 - askprice1\n# bidprice1 - bidprice2\ndef price_ask1_bid1_diff(df):\n    return (df['ask_price1'] - df['bid_price1'])\ndef price_ask2_bid2_diff(df):\n    return (df['ask_price2'] - df['bid_price2'])\ndef price_ask2_bid1_diff(df):\n    return (df['ask_price2'] - df['bid_price1'])\ndef price_ask1_bid2_diff(df):\n    return (df['ask_price1'] - df['bid_price2'])\ndef price_wap1_wap2_diff(df):\n    return (df['wap1'] - df['wap2'])\ndef std_per_mean(df):\n    return np.std(df) / np.mean(df)\n","metadata":{"execution":{"iopub.status.busy":"2021-09-27T23:48:23.341066Z","iopub.execute_input":"2021-09-27T23:48:23.34137Z","iopub.status.idle":"2021-09-27T23:48:23.3559Z","shell.execute_reply.started":"2021-09-27T23:48:23.341338Z","shell.execute_reply":"2021-09-27T23:48:23.354608Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# RMSPE\ndef rmspe(y_true, y_pred):\n    return  (np.sqrt(np.mean(np.square((y_true - y_pred) / y_true))))","metadata":{"execution":{"iopub.status.busy":"2021-09-27T23:48:23.357737Z","iopub.execute_input":"2021-09-27T23:48:23.358104Z","iopub.status.idle":"2021-09-27T23:48:23.368945Z","shell.execute_reply.started":"2021-09-27T23:48:23.358068Z","shell.execute_reply":"2021-09-27T23:48:23.367642Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4. Preprocessing dataset","metadata":{}},{"cell_type":"markdown","source":"## 4-1. Book parquet data processing","metadata":{}},{"cell_type":"code","source":"def book_preprocessing(stock_id : int, data_type = 'train'):\n    # read data\n    df = pd.read_parquet(data_path / f'book_{data_type}.parquet/stock_id={stock_id}/')\n    \n    # set stock_id\n    df['stock_id'] = stock_id\n    \n    # WAP calculation\n    df['wap1'] = wap_calculation1(df)\n    df['wap2'] = wap_calculation2(df)\n    df['wap3'] = wap_calculation3(df)\n    df['wap4'] = wap_calculation4(df)\n\n    # log return calculation\n    df['log_return1'] = df.groupby(['time_id'])['wap1'].apply(log_return).fillna(0)\n    df['log_return2'] = df.groupby(['time_id'])['wap2'].apply(log_return).fillna(0)  \n    df['log_return3'] = df.groupby(['time_id'])['wap3'].apply(log_return).fillna(0)\n    df['log_return4'] = df.groupby(['time_id'])['wap4'].apply(log_return).fillna(0)  \n    \n    # Calculate wap balance\n    df['wap_balance12'] = abs(df['wap1'] - df['wap2'])\n    df['wap_balance34'] = abs(df['wap3'] - df['wap4'])\n    # Calculate spread\n    df['price_spread1'] = (df['ask_price1'] - df['bid_price1']) / ((df['ask_price1'] + df['bid_price1']) / 2)\n    df['price_spread2'] = (df['ask_price2'] - df['bid_price2']) / ((df['ask_price2'] + df['bid_price2']) / 2)\n    df['bid_spread'] = df['bid_price1'] - df['bid_price2']\n    df['ask_spread'] = df['ask_price1'] - df['ask_price2']\n    df[\"bid_ask_spread\"] = abs(df['bid_spread'] - df['ask_spread'])\n    df['total_volume'] = (df['ask_size1'] + df['ask_size2']) + (df['bid_size1'] + df['bid_size2'])\n    df['volume_imbalance'] = abs((df['ask_size1'] + df['ask_size2']) - (df['bid_size1'] + df['bid_size2']))\n                           \n    # Log_return calculation each stock_id and time_id\n    feat_to_calc_rv = ['log_return1','log_return2','log_return3','log_return4']\n    return_values = pd.DataFrame(\n        df.groupby(\n            ['stock_id','time_id']\n        )[feat_to_calc_rv].agg(realized_volatility)\n    ).reset_index()\n    return_values = return_values.rename(\n        columns={\n            'log_return1': 'realized_volatility1',\n            'log_return2': 'realized_volatility2',\n            'log_return3': 'realized_volatility3',\n            'log_return4': 'realized_volatility4'\n        }\n    )\n\n    # 集約関数のため不要な列を削除\n    df = df.drop(['time_id', 'seconds_in_bucket'], axis=1)\n\n#     # skew\n#     return_values = return_values.merge(\n#         df.groupby(['stock_id']).skew(),\n#         on='stock_id',\n#         suffixes=['', '_skew'],\n#         how='left'\n#     )\n#     # sem\n#     return_values = return_values.merge(\n#         df.groupby(['stock_id']).sem(),\n#         on='stock_id',\n#         suffixes=['', '_sem'],\n#         how='left'\n#     )    \n#     # kurt\n#     return_values = return_values.merge(\n#         df.groupby(['stock_id']).apply(pd.Series.kurt).drop('stock_id',axis=1),\n#         on='stock_id',\n#         suffixes=['', '_kurt'],\n#         how='left'\n#     )\n#     # std_per_mean\n#     return_values = return_values.merge(\n#         df.groupby(['stock_id']).agg(std_per_mean),\n#         on='stock_id',\n#         suffixes=['', '_std_per_mean'],\n#         how='left'\n#     )   \n\n    # 後工程で使うのでリスト化\n    features = [\n        'wap1',\n        'wap2',\n        'wap3',\n        'wap4',\n        'ask_price1',\n        'ask_price2',\n        'bid_price1',\n        'bid_price2',\n        'ask_size1',\n        'ask_size2',\n        'bid_size1',\n        'bid_size2',\n        'log_return1',\n        'log_return2',\n        'realized_volatility1',\n        'realized_volatility2',\n        'realized_volatility3',\n        'realized_volatility4',\n        'std_per_mean',\n        'wap_balance12',\n        'wap_balance34',\n        'price_spread1',\n        'price_spread2',\n        'bid_spread',\n        'ask_spread',\n        'bid_ask_spread',\n        'total_volume',\n        'volume_imbalance'\n    ]\n\n    return return_values","metadata":{"execution":{"iopub.status.busy":"2021-09-27T23:48:23.37085Z","iopub.execute_input":"2021-09-27T23:48:23.3713Z","iopub.status.idle":"2021-09-27T23:48:23.387978Z","shell.execute_reply.started":"2021-09-27T23:48:23.371261Z","shell.execute_reply":"2021-09-27T23:48:23.386797Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Check data content of one sample with book_preprocessing function  \ne.g. stock_id = 97","metadata":{}},{"cell_type":"code","source":"df_book = book_preprocessing(97, 'train')\ndf_book","metadata":{"execution":{"iopub.status.busy":"2021-09-27T23:48:23.390002Z","iopub.execute_input":"2021-09-27T23:48:23.390304Z","iopub.status.idle":"2021-09-27T23:48:34.593126Z","shell.execute_reply.started":"2021-09-27T23:48:23.390273Z","shell.execute_reply":"2021-09-27T23:48:34.591672Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4-2. Trade parquet data processing","metadata":{}},{"cell_type":"code","source":"def trade_preprocessing(stock_id : int, data_type = 'train'):\n    # read data\n    df = pd.read_parquet(data_path / f'trade_{data_type}.parquet/stock_id={stock_id}/')\n    \n    df = df.sort_values(by=['time_id', 'seconds_in_bucket']).reset_index(drop=True)\n    \n    # set stock_id\n    df['stock_id'] = stock_id\n    \n    # log return calculation\n    df['trade_log_return1'] = df.groupby(by = ['time_id'])['price'].apply(log_return).fillna(0)\n    \n    # Log_return calculation each stock_id and time_id\n    df = pd.DataFrame(df.groupby(['stock_id','time_id'])[['trade_log_return1']].agg(realized_volatility).reset_index())\n    \n    # その他のデータも使いたい size と order_count\n    \n    return df","metadata":{"execution":{"iopub.status.busy":"2021-09-27T23:48:34.59514Z","iopub.execute_input":"2021-09-27T23:48:34.595589Z","iopub.status.idle":"2021-09-27T23:48:34.603694Z","shell.execute_reply.started":"2021-09-27T23:48:34.59554Z","shell.execute_reply":"2021-09-27T23:48:34.602607Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Check data content of one sample with trade_preprocessing function\ne.g. stock_id = 0","metadata":{}},{"cell_type":"code","source":"df_trade = trade_preprocessing(0,'train')\ndf_trade.head()","metadata":{"execution":{"iopub.status.busy":"2021-09-27T23:48:34.605705Z","iopub.execute_input":"2021-09-27T23:48:34.60633Z","iopub.status.idle":"2021-09-27T23:48:36.792085Z","shell.execute_reply.started":"2021-09-27T23:48:34.606241Z","shell.execute_reply":"2021-09-27T23:48:36.791028Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4-3. Merge book and trade data  \nMerge two data created by preprocessed with book_preprocessing and trade_preprocessing function","metadata":{}},{"cell_type":"code","source":"def get_stock_stat(stock_id : int, data_type = 'train'):\n    \n    # parquet data processing\n    book_stat = book_preprocessing(stock_id, data_type)\n    trade_stat = trade_preprocessing(stock_id, data_type)\n    \n    #Merge book and trade features\n    stock_stat = book_stat.merge(trade_stat, on=['stock_id', 'time_id'], how='left').fillna(-999)\n    \n    return stock_stat","metadata":{"execution":{"iopub.status.busy":"2021-09-27T23:48:36.794141Z","iopub.execute_input":"2021-09-27T23:48:36.794552Z","iopub.status.idle":"2021-09-27T23:48:36.800844Z","shell.execute_reply.started":"2021-09-27T23:48:36.794505Z","shell.execute_reply":"2021-09-27T23:48:36.799813Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_dataSet(stock_ids : list, data_type = 'train'):\n    # Parallel process of get_stock_stat \n    stock_stat = Parallel(n_jobs=-1)(\n        delayed(get_stock_stat)(stock_id, data_type) \n        for stock_id in stock_ids\n    )\n    # concat several stock_stats in vertical direction, axis=0(default)\n    stock_stat_df = pd.concat(stock_stat, ignore_index = True)\n\n    return stock_stat_df","metadata":{"execution":{"iopub.status.busy":"2021-09-27T23:48:36.802327Z","iopub.execute_input":"2021-09-27T23:48:36.802898Z","iopub.status.idle":"2021-09-27T23:48:36.814652Z","shell.execute_reply.started":"2021-09-27T23:48:36.802852Z","shell.execute_reply":"2021-09-27T23:48:36.813379Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4-4. Train data preprocessing","metadata":{}},{"cell_type":"code","source":"train=pd.read_csv(data_path / 'train.csv')\ntrain['row_id'] = train['stock_id'].astype(str) + '-' + train['time_id'].astype(str)\ndisplay(train.head())\nprint('train data shape:', train.shape)","metadata":{"execution":{"iopub.status.busy":"2021-09-27T23:48:36.816356Z","iopub.execute_input":"2021-09-27T23:48:36.816761Z","iopub.status.idle":"2021-09-27T23:48:37.632884Z","shell.execute_reply.started":"2021-09-27T23:48:36.81672Z","shell.execute_reply":"2021-09-27T23:48:37.631874Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def miff_max_min(x):\n#     return max(x) - min(x)\n\n# def add_my_param(data):\n\n#     '''\n#     データを追加する\n#     '''\n#     data_ = data.copy()\n#     # targetはない場合がある\n#     try:\n#         data_ = data_.drop(['target'],axis=1)\n#     except Exception as e:\n#         pass\n\n# #     # 特定のデータの最大値を計算\n# #     # 最大値\n# #     data_ = data_.merge(\n# #         data.groupby('stock_id').max(),\n# #         on='stock_id',\n# #         suffixes=['', '_max'],\n# #         how='left'\n# #     )\n\n# #     # 最小値\n# #     data_ = data_.merge(\n# #         data.groupby('stock_id').min(),\n# #         on='stock_id',\n# #         suffixes=['', '_min'],\n# #         how='left'\n# #     )\n\n#     # 標準偏差\n#     data_ = data_.merge(\n#         data.groupby('stock_id').std(),\n#         on='stock_id',\n#         suffixes=['', '_std'],\n#         how='left'\n#     )\n    \n# #     # 最大 - 最小\n# #     data_ = data_.merge(\n# #         data.groupby('stock_id').min() - data.groupby('stock_id').min(),\n# #         on='stock_id',\n# #         suffixes=['', '_diffmaxmin'],\n# #         how='left'\n# #     )\n\n# #     # 中央値\n# #     data_ = data_.merge(\n# #         data.groupby('stock_id').median(),\n# #         on='stock_id',\n# #         suffixes=['', '_median'],\n# #         how='left'\n# #     )\n    \n# #     # 平均値\n# #     data_ = data_.merge(\n# #         data.groupby('stock_id').mean(),\n# #         on='stock_id',\n# #         suffixes=['', '_mean'],\n# #         how='left'\n# #     )\n    \n#     # skew\n#     data_ = data_.merge(\n#         data.groupby('stock_id').skew(),\n#         on='stock_id',\n#         suffixes=['', '_skew'],\n#         how='left'\n#     )\n    \n#     # sem\n#     data_ = data_.merge(\n#         data.groupby('stock_id').sem(),\n#         on='stock_id',\n#         suffixes=['', '_sem'],\n#         how='left'\n#     )    \n#     return data_","metadata":{"execution":{"iopub.status.busy":"2021-09-27T23:48:37.634334Z","iopub.execute_input":"2021-09-27T23:48:37.634606Z","iopub.status.idle":"2021-09-27T23:48:37.63987Z","shell.execute_reply.started":"2021-09-27T23:48:37.634579Z","shell.execute_reply":"2021-09-27T23:48:37.638861Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # データセット取得\n# train_stock_stat_df = get_dataSet(stock_ids = train['stock_id'].unique(), data_type = 'train')\n\n# # パラメータ追加\n# # train_stock_stat_df = add_my_param(train_stock_stat_df)\n\n# # Merge train with train_stock_stat_df\n# train = pd.merge(train, train_stock_stat_df, on = ['stock_id', 'time_id'], how = 'left')\n\n# train","metadata":{"execution":{"iopub.status.busy":"2021-09-27T23:48:37.642321Z","iopub.execute_input":"2021-09-27T23:48:37.642604Z","iopub.status.idle":"2021-09-27T23:48:37.659351Z","shell.execute_reply.started":"2021-09-27T23:48:37.642576Z","shell.execute_reply":"2021-09-27T23:48:37.65814Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# pickleを保存、読み込んで使う\n# train.to_pickle('train.pkl')\ntrain = pd.read_pickle('../input/from-ver-37/train.pkl')\ntrain","metadata":{"execution":{"iopub.status.busy":"2021-09-27T23:48:37.661344Z","iopub.execute_input":"2021-09-27T23:48:37.661647Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **4-4-1.LOFO importance**","metadata":{}},{"cell_type":"code","source":"# you need to activate internet connection!\n'''!pip install git+https://github.com/aerdem4/lofo-importance\n\nimport pandas as pd\nfrom sklearn.model_selection import KFold\nfrom lofo import LOFOImportance, Dataset, plot_importance\n%matplotlib inline\n\ntarget=\"target\"\n\nsample_df = train.sample(frac=0.01, random_state=0)\n#sample_df.sort_values(\"AvSigVersion\", inplace=True)\n\n# define the binary target and the features\ncv = KFold(n_splits=4, shuffle=False, random_state=0)\n#target = \"HasDetections\"\nfeatures = [col for col in train.columns if col != target]\n#features = [col for col in train.columns]\n\n\n# define the binary target and the features\ndataset = Dataset(df=sample_df, target=\"target\", features=[col for col in sample_df.columns if col != target])\n\n# get the mean and standard deviation of the importances in pandas format\nlofo = LOFOImportance(dataset, cv=cv, scoring=\"neg_mean_absolute_error\")\nimportance_df = lofo.get_importance()\n\n# plot the means and standard deviations of the importances\nplot_importance(importance_df, figsize=(12, 40))'''","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4-5. Test data Preprocessing","metadata":{}},{"cell_type":"code","source":"# テストデータを読み込む\ntest = pd.read_csv(data_path /'test.csv')\n# stock_idとtime_idを組み合わせて、結果で必要になるrow_idを作る\ntest['row_id'] = test['stock_id'].astype(str) + '-' + test['time_id'].astype(str)\n# 確認\ntest","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# book(取引)の情報を全て取得。wapやlogreturnも計算\ntest_stock_stat_df = get_dataSet(stock_ids = test['stock_id'].unique(), data_type = 'test')\n\n# min,max,std等追加の特徴量計算を記載\n# test_stock_stat_df = add_my_param(test_stock_stat_df)\n\n# テストデータと推論に使うbookの情報をマージ\ntest = pd.merge(test, test_stock_stat_df, on = ['stock_id', 'time_id'], how = 'left').fillna(0)\n\n# 確認\ntest","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 5.Training","metadata":{}},{"cell_type":"markdown","source":"## 5-1. Training function1 - Light GBM  ","metadata":{}},{"cell_type":"code","source":"# Parameters of Light GBM\n# first try\n# params_lgbm = {\n#         'task': 'train',\n#         'boosting_type': 'gbdt',\n#         'learning_rate': 0.01,\n#         'objective': 'regression',\n#         'metric': 'None',\n#         'max_depth': -1,\n#         'n_jobs': -1,\n#         'feature_fraction': 0.7,\n#         'bagging_fraction': 0.7,\n#         'lambda_l2': 1,\n#         'verbose': -1\n#         #'bagging_freq': 5\n# }","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define loss function for lightGBM training\ndef feval_RMSPE(preds, train_data):\n    labels = train_data.get_label()\n    return 'RMSPE', round(rmspe(y_true = labels, y_pred = preds),5), False","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Following function is training with Light GBM function.If you would like to try any other function, you could define another function and call it.","metadata":{}},{"cell_type":"code","source":"# training function\ndef light_gbm(X_train, y_train, X_val ,y_val,cats, _pred_name, n_rounds, val_index):\n    \n    print(cats)\n    \n    # Create dataset\n    train_data = lgb.Dataset(X_train, label=y_train, categorical_feature=cats, weight=1/np.power(y_train,2))\n    val_data = lgb.Dataset(X_val, label=y_val, categorical_feature=cats, weight=1/np.power(y_val,2))\n    \n    # training\n    model = lgb.train(params_lgbm, \n                      train_data, \n                      n_rounds, \n                      valid_sets=val_data, \n                      feval=feval_RMSPE,\n                      verbose_eval= 10,\n#                       verbose_eval= 250,\n                     )\n    \n    # Prediction w/ validation data\n    preds_val = model.predict(train.loc[val_index, features_columns])\n    # train.loc[val_index, _pred_name] = preds_val\n    \n    # RMSPE calculation\n    score = round(rmspe(y_true = y_val, y_pred = preds_val),5)\n\n    # Prediction w/ validation data\n    test_preds = model.predict(test[features_columns]).clip(0,1e10)\n    \n    # delete dataset\n    del train_data, val_data\n    \n    return score, test_preds, model, preds_val","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# training function\nimport xgboost as xgb\nfrom sklearn import preprocessing\ndef my_xgboost(X_train, y_train, X_val ,y_val,cats, _pred_name, n_rounds, val_index):\n    \n    print(X_train.columns)\n    print(cats)\n    \n    le = preprocessing.LabelEncoder()\n    \n    lookup = {\n        np.int64: 'int',\n        np.float32: 'float',\n        np.float64: 'float',\n        str: 'c'\n    }\n    feature_types = [lookup[type(train.head(1)[t][0])] for t in train.columns]\n    \n    \n    # Create dataset\n    dtrain = xgb.DMatrix(X_train, label=y_train)\n    dvalid = xgb.DMatrix(X_val, label=y_val)\n    watchlist = [(dtrain, 'train'), (dvalid, 'eval')]#訓練データはdtrain、評価用のテストデータはdvalidと設定\n    \n    # training\n    model = xgb.train(\n        {\n            'objective': 'reg:squarederror',\n            'silent':1, \n            'random_state':1234, \n            # 学習用の指標 (RMSE)\n            'eval_metric': 'rmse',\n            'max_depth' : '8',\n            'eta' : '0.2'\n        },\n        dtrain,#訓練データ\n        n_rounds,#設定した学習回数\n        early_stopping_rounds=500,\n        evals=watchlist,\n    )\n    \n    # Prediction w/ validation data\n    # XGBoostの学習を実行\n    # テストデータで予測、評価\n    dtest = xgb.DMatrix(X_val)\n    preds_val = model.predict(\n        dtest,\n        ntree_limit = model.best_ntree_limit,\n    )\n    #train.loc[val_index, _pred_name] = preds_val\n    \n    # RMSPE calculation\n    score = round(rmspe(y_true = y_val, y_pred = preds_val),5)\n\n    # Prediction w/ validation data\n    dtest = xgb.DMatrix(test[features_columns])\n    test_preds = model.predict(dtest).clip(0,1e10)\n    # テストデータを使ってプレディクトをかける\n    print('pred results:{}'.format(test_preds))\n    \n    # delete dataset\n    del dtrain, dvalid, watchlist, dtest\n    \n    return score, test_preds, model, preds_val","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 5-1-1\n### prepare data to train","metadata":{}},{"cell_type":"code","source":"# Categorical data column list\n# cats = []\ncats = ['stock_id']\n\n# 学習対象特徴量\nfeatures_columns = train.columns.values.tolist()\n\n# drop feat list\n# drop_feat = ['row_id','target','stock_id']\ndrop_feat = ['row_id','target']\nfor i in drop_feat: features_columns.remove(i)\n\nprint(f'Train dataset columns : {len(features_columns)} features')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 5-1-2\n### optuna","metadata":{}},{"cell_type":"code","source":"\n# 2021/09/16 by optuna\n# params_lgbm= {\n#     'objective': 'mean_squared_error',\n#      'metric': 'l1',\n#      'verbosity': -1,\n#      'boosting_type': 'gbdt',\n#      'feature_pre_filter': False,\n#      'lambda_l1': 2.5812494450187865e-05,\n#      'lambda_l2': 0.0005754010268853543,\n#      'num_leaves': 234,\n#      'feature_fraction': 0.42,\n#      'bagging_fraction': 0.9352921232220405,\n#      'bagging_freq': 7,\n#      'min_child_samples': 5,\n#      'num_iterations': 200,\n#      'early_stopping_round': 50\n# }\n\n# # 2021/09/17 by optuna more params add_myparam\n# params_lgbm = {\n#     'objective': 'mean_squared_error',\n#     'metric': 'l1',\n#     'verbosity': -1,\n#     'boosting_type': 'gbdt',\n#     'feature_pre_filter': False,\n#     'lambda_l1': 0.0,\n#     'lambda_l2': 0.0,\n#     'num_leaves': 105,\n#     'feature_fraction': 1.0,\n#     'bagging_fraction': 1.0,\n#     'bagging_freq': 0,\n#     'min_child_samples': 50,\n#     'num_iterations': 200,\n#     'early_stopping_round': 50\n# }\n\n# 2021/09/17 by optuna more params add_myparam\n# params_lgbm = {\n#     'objective': 'mean_squared_error',\n#     'metric': 'l1',\n#     'verbosity': -1,\n#     'boosting_type': 'gbdt',\n#     'feature_pre_filter': False,\n#     'lambda_l1': 0.0,\n#     'lambda_l2': 0.0,\n#     'num_leaves': 177,\n#     'feature_fraction': 0.8,\n#     'bagging_fraction': 1.0,\n#     'bagging_freq': 0,\n#     'min_child_samples': 20,\n#     'num_iterations': 200,\n#     'early_stopping_round': 50\n# }\n\n# 2021/09/17 by optuna more params with std_per_mean\n# params_lgbm = {\n#     'objective': 'mean_squared_error',\n#     'metric': 'l1',\n#     'verbosity': -1,\n#     'boosting_type': 'gbdt',\n#     'feature_pre_filter': False,\n#     'lambda_l1': 1.535303758262475e-07,\n#     'lambda_l2': 0.0066570427899383285,\n#     'num_leaves': 256,\n#     'feature_fraction': 0.41600000000000004,\n#     'bagging_fraction': 1.0,\n#     'bagging_freq': 0,\n#     'min_child_samples': 20,\n#     'num_iterations': 200,\n#     'early_stopping_round': 50\n# }\n\n# # Ver24\n# {'objective': 'mean_squared_error',\n#  'metric': 'l1',\n#  'verbosity': -1,\n#  'boosting_type': 'gbdt',\n#  'feature_pre_filter': False,\n#  'lambda_l1': 0.0,\n#  'lambda_l2': 0.0,\n#  'num_leaves': 179,\n#  'feature_fraction': 1.0,\n#  'bagging_fraction': 1.0,\n#  'bagging_freq': 0,\n#  'min_child_samples': 20,\n#  'num_iterations': 200,\n#  'early_stopping_round': 50}\n\n# ver25\n# params_lgbm = {\n#     'objective': 'mean_squared_error',\n#     'metric': 'l1',\n#     'verbosity': -1,\n#     'boosting_type': 'gbdt',\n#     'feature_pre_filter': False,\n#     'lambda_l1': 0.00013301130015106456,\n#     'lambda_l2': 1.083683899969528,\n#     'num_leaves': 203,\n#     'feature_fraction': 0.8999999999999999,\n#     'bagging_fraction': 0.697954364796923,\n#     'bagging_freq': 4,\n#     'min_child_samples': 20,\n#     'num_iterations': 200,\n#     'early_stopping_round': 50\n# }\n\n# ver37\nparams_lgbm = {\n    'objective': 'mean_squared_error',\n    'metric': 'l1',\n    'verbosity': -1,\n    'boosting_type': 'gbdt',\n    'feature_pre_filter': False,\n    'lambda_l1': 0.0,\n    'lambda_l2': 0.0,\n    'num_leaves': 19,\n    'feature_fraction': 1.0,\n    'bagging_fraction': 0.6511720136604726,\n    'bagging_freq': 1,\n    'min_child_samples': 20,\n    'num_iterations': 200,\n    'early_stopping_round': 50\n}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\noptuna for lightgbm\n'''\n# params = {\n#     'objective': 'mean_squared_error',\n#     'metric': 'mae',\n#     \"verbosity\": -1,\n#     \"boosting_type\": \"gbdt\",\n# }\n\n# best_params, history = {}, []\n\n# lgb_train = lgb.Dataset(train[features_columns], train[target])\n# lgb_eval = lgb.Dataset(test[features_columns], test[target], reference=lgb_train)\n\n# # LightGBM学習\n# gbm = lgb.train(params,\n#                 lgb_train,\n#                 num_boost_round=200,\n#                 valid_sets=[lgb_train, lgb_eval],\n#                 early_stopping_rounds=50\n#                )\n\n# best_params = gbm.params\n# params_lgbm = best_params\n# params_lgbm","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# '''\n# optuna for xgboost\n# '''\n# import optuna\n# import xgboost as xgb\n\n# def objective_wrap(_train, _labels):\n#     def objective(trial):\n\n#         params = {\n#             \"max_depth\": trial.suggest_int(\"max_depth\", 6, 9),\n#             \"min_child_weight\": 1,\n#             \"eta\": trial.suggest_loguniform(\"eta\", 0.01, 1.0),\n#             \"tree_method\": \"exact\",\n#             \"eval_metric\": \"rmse\",\n#             \"predictor\": \"cpu_predictor\"  \n#         }\n\n#     #     a = train[features_columns]\n#     #     b = train[target].values\n#         dtrain = xgb.DMatrix(_train, _labels)\n\n#         cv_results = xgb.cv(\n#             params,\n#             dtrain,\n#             num_boost_round=1000,\n#             seed=0,\n#             nfold=5, # CVの分割数\n#             metrics={\"rmse\"},\n#             early_stopping_rounds=5\n#         )\n\n#         return cv_results[\"test-rmse-mean\"].min()\n#     return objective\n\n\n# study = optuna.create_study()\n# study.optimize(\n#     objective_wrap(\n#         train[features_columns],\n#         train[target].values\n#     ),\n#     n_trials = 40\n# )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"## 5-2. Cross Validation","metadata":{}},{"cell_type":"code","source":"def do_cross_validation_xgb(n_folds, n_rounds, pred_name, reg_alpha=0, reg_lambda=0):\n    \n    scores_folds = []\n    pred_result = []\n\n    # k-flods Ensemble Training\n    kf = model_selection.KFold(n_splits=n_folds, shuffle=True, random_state = 42)\n\n    # Initial value\n    cv_trial = 1\n    \n    # --- Cross Validation ---\n    for train_index, val_index in kf.split(range(len(train))):\n\n        print(f'CV trial : {cv_trial} /{n_folds}')\n\n        # Divide dataset into train and validation data such as Cross Validation\n        X_train = train.loc[train_index, features_columns]\n        y_train = train.loc[train_index, target].values\n        X_val = train.loc[val_index, features_columns]\n        y_val = train.loc[val_index, target].values\n\n        # train with Light GBM\n        rmspe_score, test_preds, model, preds_val = my_xgboost(X_train, y_train, X_val ,y_val,cats, pred_name, n_rounds, val_index)\n        \n        # record score data at each train in CV\n        scores_folds.append(rmspe_score)\n        pred_result.append(test_preds)\n\n        # Each validation Summary \n        print(f'Fold-{cv_trial} train score. Model-{pred_name} RMSPE: {rmspe_score}')\n        \n        # Prediction w/ test data\n        dtest = xgb.DMatrix(test[features_columns])\n        test_preds = model.predict(dtest).clip(0,1e10)\n        # テストデータを使ってプレディクトをかける\n        print('test pred results:{}'.format(test_preds))\n        \n        # cv trial 回数をインクリメント\n        cv_trial += 1\n\n    return pred_result, scores_folds, test_preds","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def do_cross_validation_lightgbm(n_folds, n_rounds, pred_name, reg_alpha=0, reg_lambda=0):\n\n    # k-flods Ensemble Training\n    kf = model_selection.KFold(n_splits=n_folds, shuffle=True, random_state = 42)\n\n    # Initialize scores list\n    scores_folds = []\n    pred_result = []\n\n    # Initial value\n    cv_trial = 1\n\n    params_lgbm['reg_alpha'] = reg_alpha\n    params_lgbm['reg_lambda'] = reg_lambda\n\n    # --- Cross Validation ---\n    for train_index, val_index in kf.split(range(len(train))):\n\n        print(f'CV trial : {cv_trial} /{n_folds}')\n\n        # Divide dataset into train and validation data such as Cross Validation\n        X_train = train.loc[train_index, features_columns]\n        y_train = train.loc[train_index, target].values\n        X_val = train.loc[val_index, features_columns]\n        y_val = train.loc[val_index, target].values\n\n        # train with Light GBM\n        rmspe_score, test_preds, model, preds_val = light_gbm(X_train, y_train, X_val ,y_val,cats, pred_name, n_rounds, val_index)\n        \n         # record score data at each train in CV\n        scores_folds.append(rmspe_score)\n        pred_result.append(test_preds)\n  \n        # Each validation Summary \n        print(f'Fold-{cv_trial} train score. Model-{pred_name} RMSPE: {rmspe_score}')\n\n        # テストデータを使ってプレディクトをかける\n        test_preds = model.predict(test[features_columns]).clip(0,1e10)\n        print('test pred results:{}'.format(test_preds))\n        \n        # インポータンス出力\n        train_index = train.head(0)\n        train_index = train_index.drop(drop_feat, axis=1)\n        importance = model.feature_importance()\n        print(importance)\n        print(train_index.columns)\n        display(pd.DataFrame(importance, index=train_index.columns, columns=['importance']))\n\n        # cv trial 回数をインクリメント\n        cv_trial += 1\n      \n    return pred_result, scores_folds, test_preds","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 6. Evaluation","metadata":{}},{"cell_type":"code","source":"pd.set_option(\"max_rows\", None) # 全て見たい\n\n# results\n# for xgboost\nscores_folds = {}\ntrain_pred_result = {}\ntest_pred_result = {}\nmodel_name_xgb1 = 'xgb1'\npred_name_xgb1 = f'pred_{model_name_xgb1}'\nscores_folds[model_name_xgb1]=[]\ntrain_pred_result[model_name_xgb1]=[]\ntest_pred_result[model_name_xgb1]=[]\n\n\nmodel_name_lgb1 = 'lgb1'\npred_name_lgb1 = f'pred_{model_name_lgb1}'\nscores_folds[model_name_lgb1]=[]\ntrain_pred_result[model_name_lgb1]=[]\ntest_pred_result[model_name_lgb1]=[]\n\n# 過学習抑制パラメータ探索ループ\nreg_alphas =  [0.1]\nreg_lambdas = [0]\ncv_count = 4\nn_rounds = 10000\nfor reg_alpha in reg_alphas:\n    for reg_lambda in reg_lambdas:\n        # 交差検定\n        # xgb\n        pr, sf, tr = do_cross_validation_xgb(cv_count, n_rounds, pred_name_xgb1, reg_alpha, reg_lambda)\n        scores_folds[model_name_xgb1].append(sf)\n        train_pred_result[model_name_xgb1].append(pr)\n        test_pred_result[model_name_xgb1].append(tr)\n\n        # lgb\n        pr, sf, tr = do_cross_validation_lightgbm(4, n_rounds, pred_name_lgb1, reg_alpha, reg_lambda)\n        scores_folds[model_name_lgb1].append(sf)\n        train_pred_result[model_name_lgb1].append(pr)\n        test_pred_result[model_name_lgb1].append(tr)\n\npd.options.display.max_columns = 50 # 表示の抑制を初期設定に戻す","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.idle":"2021-09-28T00:10:21.698405Z","shell.execute_reply.started":"2021-09-27T23:50:29.890085Z","shell.execute_reply":"2021-09-28T00:10:21.69666Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# アンサンブルする\n\n# すべての結果を出力\nprint('='*8+'\\n'+'print all result')\nprint(train_pred_result)\nprint(scores_folds)\n\n# 推定値とRMSPEのスコアの平均値を計算\ntrain_last_result = 0\ntest_last_result = 0\nlast_score = 0\ncount = 0\nfor pr in train_pred_result:\n    train_last_result += pd.DataFrame(train_pred_result[pr][0]).mean()\n    test_last_result += pd.DataFrame(test_pred_result[pr][0])\n    last_score += pd.Series(scores_folds[pr][0]).mean()\n    count += 1\ntrain_last_result /= count\ntest_last_result /= count\nlast_score /= count\n\n# 表示\nprint('='*8+'\\n'+'print mean result')\nprint('train last result:{}'.format(train_last_result))\nprint('test last result:{}'.format(test_last_result))\nprint('last score:{}'.format(last_score))\n\n# 結果を代入する\ntest[target]=test_last_result\n\ndisplay(test[[Id_column, target]].head(2))","metadata":{"execution":{"iopub.status.busy":"2021-09-28T00:10:21.700705Z","iopub.execute_input":"2021-09-28T00:10:21.701157Z","iopub.status.idle":"2021-09-28T00:10:21.733679Z","shell.execute_reply.started":"2021-09-28T00:10:21.701095Z","shell.execute_reply":"2021-09-28T00:10:21.732495Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test","metadata":{"execution":{"iopub.status.busy":"2021-09-28T00:10:21.734901Z","iopub.execute_input":"2021-09-28T00:10:21.735253Z","iopub.status.idle":"2021-09-28T00:10:21.757629Z","shell.execute_reply.started":"2021-09-28T00:10:21.735217Z","shell.execute_reply":"2021-09-28T00:10:21.75645Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 7. Submittion","metadata":{}},{"cell_type":"code","source":"test[[Id_column,target]].to_csv(submit_file, index = False)","metadata":{"execution":{"iopub.status.busy":"2021-09-28T00:10:21.759126Z","iopub.execute_input":"2021-09-28T00:10:21.759465Z","iopub.status.idle":"2021-09-28T00:10:21.76798Z","shell.execute_reply.started":"2021-09-28T00:10:21.759425Z","shell.execute_reply":"2021-09-28T00:10:21.766842Z"},"trusted":true},"execution_count":null,"outputs":[]}]}