{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"I've made this notebook to demonstrate my solution, you can read detailed explanation [here](https://www.kaggle.com/c/optiver-realized-volatility-prediction/discussion/279170). It doesn't include full CV and inference, instead I tried to make it short and include only key elements: preprocessing and NN architecture. I did a lot of refactoring to simplify my code and to fit kaggle RAM limitations, it might affect performance a bit, but hopefully not so much. \n\nOne thing that I changed is data preprocessing using xarray instead of pandas - I really liked how well it fit multidimensional nature of the data. It speed up many group by operations by replacing them with aggregations over array dimensions.\n\nHere I also include 2 modes of training: *single-stock* and *multi-stock*. *multi-stock* training works really fast and you can get decent results after 20 minutes. In this mode train batch includes targets for all stocks in time_id and you have only 3830 training samples. In *single-stock* mode input is still the same and includes data from all stocks, but it also has single stock_id as input and only single stock target for it is predicted. This way batch contains much more diverse (stock_id, time_id) pairs, and I believe this diversity is important to get better score. During the competition I used *single-stock* training, despite much longer training times. Usually my public score was better than my validation score by 0.005-0.006. My best single models without using nearest neigbours scored ~0.199-0.200 on public while validation score was ~0.206. In *multi-stock* mode I can achieve currently only ~0.210-0.211 score, though I haven't tried to wait until training end in *single-stock* mode after refactoring, but it should be better.\n\nAnyway this just a baseline, and there are a lot of things to experiment with: stock attention placement (before/after RNN and internal implementation, feature normalization (you should probably do something with volumes as the way I did it didn't work well on private test), network dimensions, batch size, lr, etc.","metadata":{}},{"cell_type":"code","source":"%%capture\n!cd /kaggle/input/orvp-packages && pip install einops-0.3.0-py2.py3-none-any.whl","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-01-14T13:20:08.25311Z","iopub.execute_input":"2022-01-14T13:20:08.253614Z","iopub.status.idle":"2022-01-14T13:20:17.421958Z","shell.execute_reply.started":"2022-01-14T13:20:08.25353Z","shell.execute_reply":"2022-01-14T13:20:17.421059Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import gc\nimport torch\nimport einops\nimport wandb\nimport pandas as pd\nimport numpy as np\nimport xarray as xr\nimport pytorch_lightning as pl\nimport torch.nn.functional as F\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\nimport ipywidgets as widgets\n\nfrom torch import nn\nfrom tqdm.auto import tqdm\nfrom joblib import Parallel, delayed\nfrom torch.optim import Adam\nfrom torch.optim.lr_scheduler import ExponentialLR\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.model_selection import KFold","metadata":{"execution":{"iopub.status.busy":"2022-01-14T13:20:17.424175Z","iopub.execute_input":"2022-01-14T13:20:17.424469Z","iopub.status.idle":"2022-01-14T13:20:23.639264Z","shell.execute_reply.started":"2022-01-14T13:20:17.424432Z","shell.execute_reply":"2022-01-14T13:20:23.636446Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_dir = '/kaggle/input/optiver-realized-volatility-prediction'\nn_features = 21\nn_stocks = 112\nn_seconds = 600\n# if coarsen > 1, data will be aggregated per this number of seconds,\n# use this to reduce memory usage, though during competition I trained on full data, \n# so I'm not sure how it can affect model performance\ncoarsen = 3\n\ntest_files = !ls {data_dir}/book_test.parquet\nIS_FULL_TEST = len(test_files) > 1","metadata":{"execution":{"iopub.status.busy":"2022-01-14T13:20:23.644235Z","iopub.execute_input":"2022-01-14T13:20:23.646373Z","iopub.status.idle":"2022-01-14T13:20:23.653313Z","shell.execute_reply.started":"2022-01-14T13:20:23.64633Z","shell.execute_reply":"2022-01-14T13:20:23.651927Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def prepare_data(stock_id, stock_ind, set, time_ids, coarsen, norm, out):\n    df_book = pd.read_parquet(f'{data_dir}/book_{set}.parquet/stock_id={stock_id}')\n    df_min_second = df_book.groupby('time_id').agg(min_second=('seconds_in_bucket', 'min'))\n    df_book = df_book.merge(df_min_second, left_on='time_id', right_index=True) \\\n        .eval('seconds_in_bucket = seconds_in_bucket - min_second') \\\n        .drop('min_second', axis=1)\n    df_trade = pd.read_parquet(f'{data_dir}/trade_{set}.parquet/stock_id={stock_id}') \\\n        .merge(df_min_second, left_on='time_id', right_index=True) \\\n        .eval('seconds_in_bucket = seconds_in_bucket - min_second') \\\n        .drop('min_second', axis=1)\n    df = pd.merge(df_book, df_trade, on=['time_id', 'seconds_in_bucket'], how='outer')\n    df['stock_id'] = stock_id\n    df = df.set_index(['stock_id', 'time_id', 'seconds_in_bucket'])\n    df = df.to_xarray().astype('float32')\n    df = df.reindex({'time_id': time_ids, 'seconds_in_bucket': np.arange(n_seconds)})\n    for name in ['bid_price1', 'bid_price2', 'ask_price1', 'ask_price2', \n         'bid_size1', 'bid_size2', 'ask_size1', 'ask_size2']:\n        df[name] = df[name].ffill('seconds_in_bucket')\n    df['wap1'] = (df.bid_price1 * df.ask_size1 + df.ask_price1 * df.bid_size1) / (df.bid_size1 + df.ask_size1)\n    df['wap2'] = (df.bid_price2 * df.ask_size2 + df.ask_price2 * df.bid_size2) / (df.bid_size2 + df.ask_size2)\n    df['log_return1'] = np.log(df.wap1).diff('seconds_in_bucket')\n    df['log_return2'] = np.log(df.wap2).diff('seconds_in_bucket')\n    df['current_vol'] = (df.log_return1 ** 2).sum('seconds_in_bucket') ** 0.5\n    df['current_vol_2nd_half'] = (df.log_return1[..., 300:] ** 2).sum('seconds_in_bucket') ** 0.5\n    if coarsen > 1:\n        mean_features = ['ask_price1', 'ask_price2', 'bid_price1', 'bid_price2',  'ask_size1', 'ask_size2',\n               'bid_size1', 'bid_size2', 'price']\n        sum_features = ['size', 'order_count']\n        \n        df = xr.merge((df[mean_features].coarsen({'seconds_in_bucket': coarsen}, coord_func='min').mean(), \n                       df[sum_features].coarsen({'seconds_in_bucket': coarsen}, coord_func='min').sum(), \n                       df[['current_vol', 'current_vol_2nd_half']]))\n        df['wap1'] = (df.bid_price1 * df.ask_size1 + df.ask_price1 * df.bid_size1) / (df.bid_size1 + df.ask_size1)\n        df['wap2'] = (df.bid_price2 * df.ask_size2 + df.ask_price2 * df.bid_size2) / (df.bid_size2 + df.ask_size2)\n        df['log_return1'] = np.log(df.wap1).diff('seconds_in_bucket')\n        df['log_return2'] = np.log(df.wap2).diff('seconds_in_bucket')\n\n    if set == 'train' and stock_id == 31:\n        df['ask_size1'] /= 8\n        df['ask_size2'] /= 8\n        df['bid_size1'] /= 8\n        df['bid_size2'] /= 8\n        df['size'] /= 8\n\n    df['spread1'] = df.ask_price1 - df.bid_price1\n    df['spread2'] = df.ask_price2 - df.ask_price1\n    df['spread3'] = df.bid_price1 - df.bid_price2\n    df['total_volume'] = df.ask_size1 + df.ask_size2 + df.bid_size1 + df.bid_size2\n    df['volume_imbalance1'] = df.ask_size1 + df.ask_size2 - df.bid_size1 - df.bid_size2\n    df['volume_imbalance2'] = (df.ask_size1 + df.ask_size2 - df.bid_size1 - df.bid_size2) / df.total_volume\n    for name in ['bid_size1', 'bid_size2', 'ask_size1', 'ask_size2', 'size', 'order_count', 'total_volume']:\n        df[name] = np.log1p(df[name])\n        # if name != 'order_count':\n        #     df[name] = df[name].rank('seconds_in_bucket')\n    df['volume_imbalance1'] = np.sign(df['volume_imbalance1']) * np.log1p(abs(df['volume_imbalance1']))\n\n    if stock_id == 26 or stock_id == 58:\n        df['ask_price1'][...] = 1\n        df['ask_price2'][...] = 1\n        df['bid_price1'][...] = 1\n        df['bid_price2'][...] = 1\n        df['price'][...] = 1\n        df['wap1'][...] = 1\n        df['wap2'][...] = 1\n        df['ask_size1'][...] = 0\n        df['ask_size2'][...] = 0\n        df['bid_size1'][...] = 0\n        df['bid_size2'][...] = 0\n        df['size'][...] = 0\n        df['order_count'][...] = 0\n        df['log_return1'][...] = 0\n        df['log_return2'][...] = 0\n        df['spread1'][...] = 0\n        df['spread2'][...] = 0\n        df['spread3'][...] = 0\n        df['total_volume'][...] = 0\n        df['volume_imbalance1'][...] = 0\n        df['volume_imbalance2'][...] = 0\n        df['current_vol'][...] = 0\n        df['current_vol_2nd_half'][...] = 0\n\n\n    volume_columns = ['ask_size1', 'ask_size2', 'bid_size1', 'bid_size2', 'size', 'total_volume', 'volume_imbalance1']\n    mean = df[volume_columns].mean('seconds_in_bucket')\n    std = df[volume_columns].std('seconds_in_bucket')\n    std = std.where(std >= 10 * np.finfo(np.float32).eps, 1)\n    df.update((df - mean) / std)\n\n    df = df.fillna({'ask_price1': 1, 'ask_price2': 1, 'bid_price1': 1, 'bid_price2': 1,  'ask_size1': 0, 'ask_size2': 0,\n               'bid_size1': 0, 'bid_size2': 0, 'price': 1, 'size': 0, 'order_count': 0, 'wap1': 1, 'wap2': 1,\n               'log_return1': 0, 'log_return2': 0, 'spread1': 0, 'spread2': 0, 'spread3': 0, 'total_volume': 0,\n               'volume_imbalance1': 0, 'volume_imbalance2': 0, 'current_vol': 0, 'current_vol_2nd_half': 0})\n    features = ['ask_price1', 'ask_price2', 'bid_price1', 'bid_price2',  'ask_size1', 'ask_size2',\n               'bid_size1', 'bid_size2', 'price', 'size', 'order_count', 'wap1', 'wap2',\n               'log_return1', 'log_return2', 'spread1', 'spread2', 'spread3', 'total_volume',\n               'volume_imbalance1', 'volume_imbalance2']\n    extra = ['current_vol', 'current_vol_2nd_half']\n    \n    if norm is not None:\n        mean = norm['mean'].sel(stock_id=stock_id)\n        std = norm['std'].sel(stock_id=stock_id)\n    else:\n        mean = df.mean(('time_id', 'seconds_in_bucket')).drop(['current_vol', 'current_vol_2nd_half'])\n        std = df.std(('time_id', 'seconds_in_bucket')).drop(['current_vol', 'current_vol_2nd_half'])\n        std = std.where(std >= 10 * np.finfo(np.float32).eps, 1)\n\n    df.update((df - mean) / std)\n    df = df.astype('float32')\n\n    out[:, stock_ind] = einops.rearrange(df[features].to_array().values, 'f () t sec -> t sec f')\n    return df[extra], {'mean': mean, 'std': std}\n\nclass OptiverDataset(Dataset):\n    def __init__(self, features_data, extra_data, mode, time_ids):\n        self.features_data = features_data\n        self.extra_data = extra_data\n        self.time_ids = time_ids\n        self.mode = mode\n\n    def __len__(self):\n        if self.mode == 'single-stock':\n            return len(self.time_ids) * n_stocks\n        elif self.mode == 'multi-stock':\n            return len(self.time_ids)\n\n    def __getitem__(self, i):\n        if self.mode == 'single-stock':\n            time_id = self.time_ids[i // n_stocks]\n            time_ind = self.extra_data.indexes['time_id'].get_loc(time_id)\n            stock_ind = i % n_stocks\n            stock_id = self.extra_data.indexes['stock_id'][stock_ind]\n            return {\n                'data': self.features_data[time_ind], # (112, 600, 21)\n                'target': self.extra_data['target'].values[time_ind, stock_ind] if 'target' in self.extra_data else 0,  # (1,)\n                'current_vol': self.extra_data['current_vol'].values[time_ind, stock_ind],  # (1,) \n                'current_vol_2nd_half': self.extra_data['current_vol_2nd_half'].values[time_ind, stock_ind],  # (1,)                \n                'time_id': time_id,\n                'stock_id': stock_id,\n                'stock_ind': stock_ind\n            }\n        elif self.mode == 'multi-stock':\n            time_id = self.time_ids[i]\n            time_ind = self.extra_data.indexes['time_id'].get_loc(time_id)\n            return {\n                'data': self.features_data[time_ind], # (112, 600, 21)\n                'target': self.extra_data['target'].values[time_ind] if 'target' in self.extra_data else np.zeros(self.extra_data.dims['stock_id'], dtype='float32'),  # (112,)\n                'current_vol': self.extra_data['current_vol'].values[time_ind],  # (112,) \n                'current_vol_2nd_half': self.extra_data['current_vol_2nd_half'].values[time_ind],  # (112,)\n                'time_id': time_id,\n            }\n        \nclass Conv1dNorm(nn.Module):\n    def __init__(self, dim_in, dim_out, kernel, stride, padding=0):\n        super().__init__()\n        self.conv = nn.Conv1d(dim_in, dim_out, kernel, stride, padding)\n        self.norm = nn.LayerNorm([n_stocks, dim_out])\n    \n    def forward(self, x):\n        # x: (b, st, t, f)\n        x = einops.rearrange(x, 'b st t f -> (b st) f t')\n        x = self.conv(x)\n        x = einops.rearrange(x, '(b st) f t -> b t st f', st=n_stocks)\n        x = F.gelu(x)\n        x = self.norm(x)\n        x = einops.rearrange(x, 'b t st f -> b st t f', st=n_stocks)\n        return x\n    \n\nclass TimeAttention(nn.Module):\n    def __init__(self, steps):\n        super().__init__()\n        self.steps = steps\n        self.weights = nn.Parameter(torch.zeros(steps))\n    \n    def forward(self, x):\n        # x: (b, st, t, f)\n        attn = F.softmax(self.weights, 0)\n        x = torch.einsum('b s t f, t -> b s f', x, attn)\n        return x\n\n# You can experiment with other ideas for stock attention: maybe it could be\n# something like MultiHeadAttention module with keys and queries that depends on current input, \n# maybe it could be a linear combination of all stocks (full connected layer), \n# maybe you can try sparse softmax\n# class StockAttention(nn.Module):\n#     def __init__(self, dim):\n#         super().__init__()\n#         self.weight = nn.Parameter(torch.zeros((n_stocks, n_stocks)))\n#         self.bias = nn.Parameter(torch.zeros(n_stocks))\n#         self.fc_combine = nn.Linear(dim * 2, dim)\n#\n#     def forward(self, x):\n#         # x: (b, st, t, f)\n#         attn = F.softmax(self.weight + self.bias[None, :], dim=-1) # (st, st)\n#         y = torch.einsum('b i ..., j i -> b j ...', x, attn)\n#         x = torch.cat((x, y), -1)\n#         x = self.fc_combine(x)\n#         return x\n\nclass StockAttention(nn.Module):\n    def __init__(self, dim):\n        super().__init__()\n        self.weights = nn.Parameter(torch.eye(n_stocks))\n        self.fc = nn.Linear(n_stocks, n_stocks)\n        self.fc.weight.data.zero_()\n        self.fc.bias.data.zero_()\n        self.fc.weight.data[torch.eye(n_stocks).bool()] = 5\n\n\n    def forward(self, x):\n        # x: (b, st, t, f)\n        attn = self.fc(self.weights)\n        attn = F.softmax(attn, dim=-1) # (st, st)\n        x = torch.einsum('b i ..., j i -> b j ...', x, attn)\n        return x\n\n\nclass OptiverModel(pl.LightningModule):\n    def __init__(self, mode='multi-stock', dim=32, conv1_kernel=3, rnn_layers=2, rnn_dropout=0.3, \n                 n_features=21, aux_loss_weight=1.0):\n        super().__init__()\n        self.save_hyperparameters()\n        self.stock_emb = nn.Embedding(n_stocks, dim)\n#         self.stock_emb.weight.data.normal_(0, 0.2)\n        self.conv1 = Conv1dNorm(n_features, dim, conv1_kernel, conv1_kernel)\n        self.conv2 = Conv1dNorm(dim, dim, 1, 1)\n        self.stock_attn = StockAttention(dim)\n        self.rnn = nn.GRU(dim, dim, rnn_layers, batch_first=True, dropout=rnn_dropout)\n        self.timesteps_attn = TimeAttention(600 // conv1_kernel // coarsen)\n        self.timesteps_attn2 = TimeAttention(300 // conv1_kernel // coarsen)\n        self.fc_out1 = nn.Linear(dim, 1)\n        self.fc_out2 = nn.Linear(dim, 1)\n        self.history = pd.DataFrame()\n\n    def forward(self, x, stock_ind=None):\n        # x: (b, st, t, f)\n        x = self.conv1(x)\n        x = self.conv2(x)\n        x = self.stock_attn(x)\n#         x = x + self.stock_emb.weight[None, :, None, :]\n#         x = torch.cat((x, einops.repeat(self.stock_emb.weight, 'st f -> b st t f', b=x.shape[0], t=n_seconds // coarsen)), dim=-1)\n        if self.hparams.mode == 'single-stock':\n            x = x[torch.arange(len(x)), stock_ind][:, None]\n        x = einops.rearrange(x, 'b st t f -> (b st) t f')\n        x = self.rnn(x)[0]\n        x = einops.rearrange(x, '(b st) t f -> b st t f', st=n_stocks if self.hparams.mode == 'multi-stock' else 1)\n        x1 = self.timesteps_attn(x)\n        x2 = self.timesteps_attn2(x[:, :, :self.timesteps_attn2.steps, :])\n        x1 = self.fc_out1(x1)\n        x2 = self.fc_out2(x2)\n        x1 = x1 * 0.63393 - 5.762331\n        x2 = x2 * 0.67473418 - 6.098946\n        x1 = torch.exp(x1)\n        x2 = torch.exp(x2)\n        if self.hparams.mode == 'single-stock':\n            return {\n                'vol': x1[:, 0, 0], # (b,)\n                'vol2': x2[:, 0, 0] # (b,)\n            }\n        else:        \n            return {\n                'vol': x1[..., 0], # (b, st)\n                'vol2': x2[..., 0] # (b, st)\n            }\n\n    def training_step(self, batch, batch_ind):\n        out = self.common_step(batch, 'train')\n        return out\n\n    def validation_step(self, batch, batch_ind):\n        return self.common_step(batch, 'valid')\n    \n    def predict_step(self, batch, batch_ind, *args, **kwargs):\n        ret = self.common_step(batch, 'predict')\n        return {\n            'vol': ret['vol'].cpu(),\n#             'stock_id': ret['stock_id'].cpu(),\n            'time_id': ret['time_id'].cpu()\n        }\n\n    def common_step(self, batch, stage):\n        out = self(batch['data'], batch['stock_ind'] if self.hparams.mode == 'single-stock' else None)\n        mask1 = ~torch.isnan(batch['target'])\n        target1 = torch.where(mask1, batch['target'], torch.tensor(1.0, device=self.device))\n        mask2 = batch['current_vol_2nd_half'] > 0\n        target2 = torch.where(mask2, batch['current_vol_2nd_half'], torch.tensor(1.0, device=self.device))\n        vol_loss = (((out['vol'] - target1) / target1) ** 2)[mask1].mean()# ** 0.5\n        vol2_loss = (((out['vol2'] - target2) / target2) ** 2)[mask2].mean()# ** 0.5\n        loss = vol_loss + self.hparams.aux_loss_weight * vol2_loss\n        if stage != 'predict':\n            self.log(f'{stage}/loss', loss.item(), on_step=False, on_epoch=True)\n            self.log(f'{stage}/vol_loss', vol_loss.item(), on_step=False, on_epoch=True)\n            self.log(f'{stage}/vol2_loss', vol2_loss.item(), on_step=False, on_epoch=True)\n        return {\n            'loss': loss,\n            'target': batch['target'],\n            'vol': out['vol'].detach(),\n            'time_id': batch['time_id']\n        }\n\n    def common_epoch_end(self, outs, stage):\n        target = torch.cat([x['target'] for x in outs])\n        vol = torch.cat([x['vol'] for x in outs])\n        time_ids = torch.cat([x['time_id'] for x in outs])\n        mask = ~torch.isnan(target)\n        target = torch.where(mask, target, torch.tensor(1.0, device=self.device))\n        rmspe = (((vol - target) / target) ** 2)[mask].mean() ** 0.5\n        self.log(f'{stage}/rmspe', rmspe, prog_bar=True, on_step=False, on_epoch=True)\n        self.history.loc[self.trainer.current_epoch, f'{stage}/rmspe'] = rmspe.item()\n\n    def training_epoch_end(self, outs):\n        self.common_epoch_end(outs, 'train')\n        self.history_widget.clear_output(wait=True)\n        with self.history_widget:\n            ylim = [self.history.min().min(), self.history.quantile(0.95).max()]\n            ylim[0] -= (ylim[1] - ylim[0]) * 0.05\n            self.history.plot(color=['C1', 'C0'], style=['--', '-'], ylim=ylim)\n            plt.show()\n\n    def validation_epoch_end(self, outs):\n        self.common_epoch_end(outs, 'valid')\n        \n    def on_fit_start(self):\n        self.history_widget = widgets.Output()\n        display(self.history_widget)\n\n    def configure_optimizers(self):\n        opt = Adam(self.parameters(), lr=0.001)\n#         opt = Adam(self.parameters(), lr=0.0005) # single-stock\n        sched = {\n            'scheduler': ExponentialLR(opt, 0.93), \n#             'scheduler': ExponentialLR(opt, 0.9), #  single-stock\n            'interval': 'epoch'\n        }\n        return [opt], [sched]","metadata":{"execution":{"iopub.status.busy":"2022-01-14T14:00:58.673494Z","iopub.execute_input":"2022-01-14T14:00:58.673794Z","iopub.status.idle":"2022-01-14T14:00:58.7443Z","shell.execute_reply.started":"2022-01-14T14:00:58.673761Z","shell.execute_reply":"2022-01-14T14:00:58.743418Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if IS_FULL_TEST:\n    df_train = pd.read_csv(f'{data_dir}/train.csv')\n    train_data = np.memmap('/kaggle/working/train.npy', 'float16', 'w+',\n                           shape=(df_train.time_id.nunique(), n_stocks, n_seconds // coarsen, n_features))\n\n    res = Parallel(n_jobs=4, verbose=51)(\n        delayed(prepare_data)(stock_id, stock_ind, 'train', np.sort(df_train.time_id.unique()), coarsen, None, train_data)\n        for stock_ind, stock_id in enumerate(np.sort(df_train.stock_id.unique()))\n    )\n\n    train_extra = xr.concat([x[0] for x in res], 'stock_id')\n    train_extra['target'] = df_train.query('stock_id != 26 and stock_id != 58').set_index(['time_id', 'stock_id']).to_xarray()['target'].astype('float32')\n    train_extra = train_extra.transpose('time_id', 'stock_id')\n    train_norm = {\n        'mean': xr.concat([x[1]['mean'] for x in res], 'stock_id'),\n        'std': xr.concat([x[1]['std'] for x in res], 'stock_id')\n    }\n\n    # if you data fits in memory, you can load it entirely from disk, otherwise\n    # training in single-stock mode could be very slow, though it can be OK for multi-stock mode\n    train_data = np.array(train_data)","metadata":{"execution":{"iopub.status.busy":"2022-01-14T13:20:23.773135Z","iopub.execute_input":"2022-01-14T13:20:23.775349Z","iopub.status.idle":"2022-01-14T13:24:15.385404Z","shell.execute_reply.started":"2022-01-14T13:20:23.775312Z","shell.execute_reply":"2022-01-14T13:24:15.384582Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if IS_FULL_TEST:\n    cv = KFold(5, shuffle=True, random_state=1)\n    time_ids = train_extra.indexes['time_id'].values\n    train_time_ids, val_time_ids = next(cv.split(time_ids))\n\n    # multi-stock training (fast)\n    train_ds = OptiverDataset(train_data, train_extra, 'multi-stock', time_ids[train_time_ids])\n    train_dl = DataLoader(train_ds, batch_size=8, shuffle=True, num_workers=1, pin_memory=True, persistent_workers=True)\n    val_ds = OptiverDataset(train_data, train_extra, 'multi-stock', time_ids[val_time_ids])\n    val_dl = DataLoader(val_ds, batch_size=32, shuffle=False, num_workers=1, pin_memory=True, persistent_workers=True)\n\n    # single-stock training (slow)\n    # train_ds = OptiverDataset(train_data, train_extra, 'single-stock', time_ids[train_time_ids])\n    # train_dl = DataLoader(train_ds, batch_size=128, shuffle=True, num_workers=1, pin_memory=True, persistent_workers=True)\n    # val_ds = OptiverDataset(train_data, train_extra, 'single-stock', time_ids[val_time_ids])\n    # val_dl = DataLoader(val_ds, batch_size=128, shuffle=False, num_workers=1, pin_memory=True, persistent_workers=True)","metadata":{"execution":{"iopub.status.busy":"2022-01-14T13:25:45.023401Z","iopub.execute_input":"2022-01-14T13:25:45.02395Z","iopub.status.idle":"2022-01-14T13:25:45.033815Z","shell.execute_reply.started":"2022-01-14T13:25:45.023911Z","shell.execute_reply":"2022-01-14T13:25:45.033077Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if IS_FULL_TEST:\n    model = OptiverModel(mode='multi-stock', dim=32, conv1_kernel=1, aux_loss_weight=1) # multi-stock\n    # model = OptiverModel(mode='single-stock', conv1_kernel=1, aux_loss_weight=0) # single-stock\n    model.summarize(max_depth=1)","metadata":{"execution":{"iopub.status.busy":"2022-01-14T14:01:06.457122Z","iopub.execute_input":"2022-01-14T14:01:06.457951Z","iopub.status.idle":"2022-01-14T14:01:06.474207Z","shell.execute_reply.started":"2022-01-14T14:01:06.457902Z","shell.execute_reply":"2022-01-14T14:01:06.473439Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if IS_FULL_TEST:\n    callbacks = [pl.callbacks.ModelCheckpoint(monitor='valid/rmspe')]\n    trainer = pl.Trainer(gpus=1, precision=16, max_epochs=30, callbacks=callbacks) # multi-stock\n    # trainer = pl.Trainer(gpus=1, precision=16, limit_train_batches=2500, max_epochs=25) # single-stock","metadata":{"execution":{"iopub.status.busy":"2022-01-14T14:01:02.322743Z","iopub.execute_input":"2022-01-14T14:01:02.323357Z","iopub.status.idle":"2022-01-14T14:01:02.332106Z","shell.execute_reply.started":"2022-01-14T14:01:02.323316Z","shell.execute_reply":"2022-01-14T14:01:02.331342Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if IS_FULL_TEST:\n    trainer.fit(model, train_dl, val_dl)","metadata":{"execution":{"iopub.status.busy":"2022-01-14T13:53:45.206159Z","iopub.execute_input":"2022-01-14T13:53:45.2068Z","iopub.status.idle":"2022-01-14T13:55:04.754323Z","shell.execute_reply.started":"2022-01-14T13:53:45.206762Z","shell.execute_reply":"2022-01-14T13:55:04.753624Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if IS_FULL_TEST:\n    print(f'Best epoch {model.history[\"valid/rmspe\"].argmin()}: {model.history[\"valid/rmspe\"].min()}')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if IS_FULL_TEST:\n    pd.Series(F.softmax(model.timesteps_attn.weights, 0).detach()).plot();","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plt.figure(figsize=(7, 7))\n# plt.matshow(F.softmax((model.stock_attn.weight + model.stock_attn.bias[None, :]).detach(), -1), fignum=0, norm=mpl.colors.PowerNorm(0.5));","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if IS_FULL_TEST:\n    plt.figure(figsize=(7, 7))\n    plt.matshow(model.stock_emb.weight.detach(), fignum=0);","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if IS_FULL_TEST:\n    !rm /kaggle/working/train.npy","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if IS_FULL_TEST:\n    del train_data\n    gc.collect()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if IS_FULL_TEST:\n    df_test = pd.read_csv(f'{data_dir}/test.csv')\n    test_data = np.memmap('/kaggle/working/test.npy', 'float16', 'w+',\n                           shape=(df_test.time_id.nunique(), n_stocks, n_seconds // coarsen, n_features))\n\n    res = Parallel(n_jobs=4, verbose=51)(\n        delayed(prepare_data)(stock_id, stock_ind, 'test', np.sort(df_test.time_id.unique()), coarsen, train_norm, test_data)\n        for stock_ind, stock_id in enumerate(np.sort(df_test.stock_id.unique()))\n    )\n\n    test_extra = xr.concat([x[0] for x in res], 'stock_id')\n    test_extra = test_extra.transpose('time_id', 'stock_id')\n\n    # if you data fits in memory, you can load it entirely from disk, otherwise\n    # training in single-stock mode could be very slow, though it can be OK for multi-stock mode\n    test_data = np.array(test_data)","metadata":{"execution":{"iopub.status.busy":"2022-01-14T14:17:10.721938Z","iopub.execute_input":"2022-01-14T14:17:10.722339Z","iopub.status.idle":"2022-01-14T14:17:14.176671Z","shell.execute_reply.started":"2022-01-14T14:17:10.722301Z","shell.execute_reply":"2022-01-14T14:17:14.175906Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if IS_FULL_TEST:\n    test_ds = OptiverDataset(test_data, test_extra, 'multi-stock', test_extra.indexes['time_id'].values)\n    test_dl = DataLoader(test_ds, batch_size=32, shuffle=False, num_workers=1, pin_memory=True, persistent_workers=True)","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if IS_FULL_TEST:\n    out = trainer.predict(model, dataloaders=test_dl)\n    df_pred = pd.concat(pd.DataFrame(x['vol'].numpy(), \n                 index=pd.Series(x['time_id'], name='time_id'),\n                 columns=pd.Series(np.sort(df_train.stock_id.unique()), name='stock_id')) for x in out)\n    df_pred = df_pred.stack().to_frame('target').reset_index()\n    df_pred['row_id'] = df_pred.stock_id.astype('str') + '-' + df_pred.time_id.astype('str')\n    df_test = pd.read_csv('/kaggle/input/optiver-realized-volatility-prediction/test.csv')\n    df_subm = df_test.merge(df_pred[['row_id','target']], on=['row_id'], how='left')\n    # df_subm = df_subm.fillna(0)\n    df_subm = df_subm[['row_id', 'target']]\n    df_subm.to_csv('submission.csv', index=False)\n    df_subm.head()\nelse:\n    df_test = pd.read_csv('/kaggle/input/optiver-realized-volatility-prediction/test.csv')\n    df_test['target'] = 0\n    df_test.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2022-01-14T14:17:22.066263Z","iopub.execute_input":"2022-01-14T14:17:22.066824Z","iopub.status.idle":"2022-01-14T14:17:22.070647Z","shell.execute_reply.started":"2022-01-14T14:17:22.066784Z","shell.execute_reply":"2022-01-14T14:17:22.069822Z"},"trusted":true},"execution_count":null,"outputs":[]}]}