{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<a class=\"anchor\" id=\"0\"></a>\n# [Optiver Realized Volatility Prediction](https://www.kaggle.com/c/optiver-realized-volatility-prediction)","metadata":{"papermill":{"duration":0.030698,"end_time":"2021-08-20T18:37:39.832873","exception":false,"start_time":"2021-08-20T18:37:39.802175","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"### I use the notebook [Stock Embedding - FFNN - My features](https://www.kaggle.com/alexioslyon/stock-embedding-ffnn-my-features) from [alexioslyon](https://www.kaggle.com/alexioslyon) as a basis and tried to tune its various parameters. ","metadata":{}},{"cell_type":"markdown","source":"# Acknowledgements\n\n* [Stock Embedding - FFNN - My features](https://www.kaggle.com/alexioslyon/stock-embedding-ffnn-my-features) from @alexioslyon\n* [Stock Embedding - FFNN - My features](https://www.kaggle.com/tatudoug/stock-embedding-ffnn-my-features) from @tatudoug\n* [Stock Embedding - FFNN - features of the best lgbm](https://www.kaggle.com/tatudoug/stock-embedding-ffnn-features-of-the-best-lgbm) from @tatudoug\n* [NN Starter - Stock Embedding](https://www.kaggle.com/lucasmorin/tf-keras-nn-with-stock-embedding)\n* [Embedding Layers](https://www.kaggle.com/colinmorris/embedding-layers)\n* [Optiver Realized Volatility LGBM Baseline](https://www.kaggle.com/ragnar123/optiver-realized-volatility-lgbm-baseline)\n* tuning and visualization from [Higher LB score by tuning mloss - upgrade & visual](https://www.kaggle.com/vbmokin/higher-lb-score-by-tuning-mloss-upgrade-visual) and [MoA: Pytorch-RankGauss-PCA-NN upgrade & 3D visual](https://www.kaggle.com/vbmokin/moa-pytorch-rankgauss-pca-nn-upgrade-3d-visual)\n* [Data Science for tabular data: Advanced Techniques](https://www.kaggle.com/vbmokin/data-science-for-tabular-data-advanced-techniques)","metadata":{}},{"cell_type":"markdown","source":"## My upgrade:\n\nImproved notebook structure.\n\nTuning and 3D visualization of prediction results is performed for different:\n\n* Feature engineering\n* Learning rate\n* Number of epochs\n\nFE: I tried to use 350 instead of 300 - it worsened the result to LB = 0.20209.","metadata":{}},{"cell_type":"markdown","source":"<a class=\"anchor\" id=\"0.1\"></a>\n## Table of Contents\n\n1. [Import libraries](#1)\n1. [My upgrade](#2)\n    -  [Commit now](#2.1)\n    -  [Previous commits](#2.2)\n    -  [Parameters and LB score visualization](#2.3)\n1. [Download data](#3)\n1. [FE & Data Preprocessing](#4)\n1. [Modeling and prediction](#5)\n1. [Submission](#6)","metadata":{}},{"cell_type":"markdown","source":"## 1. Import libraries<a class=\"anchor\" id=\"1\"></a>\n\n[Back to Table of Contents](#0.1)","metadata":{}},{"cell_type":"code","source":"from IPython.core.display import display, HTML\n\nimport glob\nimport os\nimport gc\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt \nimport seaborn as sns\nimport numpy.matlib\n\nimport plotly.express as px\nimport plotly.graph_objects as go\n\nfrom joblib import Parallel, delayed\n\nfrom sklearn import preprocessing, model_selection\nfrom sklearn.preprocessing import MinMaxScaler, QuantileTransformer\nfrom sklearn.metrics import r2_score\nfrom sklearn.cluster import KMeans\n\nfrom numpy.random import seed\nseed(42)\n\nimport tensorflow as tf\ntf.random.set_seed(42)\nfrom tensorflow import keras\nfrom keras import backend as K\nfrom keras.backend import sigmoid\nfrom keras.utils.generic_utils import get_custom_objects\nfrom keras.layers import Activation\n\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"papermill":{"duration":1.295413,"end_time":"2021-08-20T18:37:41.326217","exception":false,"start_time":"2021-08-20T18:37:40.030804","status":"completed"},"tags":[],"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-09-03T06:08:00.452457Z","iopub.execute_input":"2021-09-03T06:08:00.45312Z","iopub.status.idle":"2021-09-03T06:08:09.451448Z","shell.execute_reply.started":"2021-09-03T06:08:00.452949Z","shell.execute_reply":"2021-09-03T06:08:09.450353Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"path_submissions = '/'\n\ntarget_name = 'target'\nscores_folds = {}","metadata":{"execution":{"iopub.status.busy":"2021-09-03T06:08:09.452993Z","iopub.execute_input":"2021-09-03T06:08:09.453311Z","iopub.status.idle":"2021-09-03T06:08:09.457138Z","shell.execute_reply.started":"2021-09-03T06:08:09.45328Z","shell.execute_reply":"2021-09-03T06:08:09.456199Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2. My upgrade <a class=\"anchor\" id=\"2\"></a>\n\n[Back to Table of Contents](#0.1)","metadata":{}},{"cell_type":"markdown","source":"### 2.1. Commit now <a class=\"anchor\" id=\"2.1\"></a>\n\n[Back to Table of Contents](#0.1)","metadata":{}},{"cell_type":"code","source":"# From the best version (commit) 5\nlearning_rate = 0.006\nnum_epochs = 200","metadata":{"execution":{"iopub.status.busy":"2021-09-03T06:08:09.459103Z","iopub.execute_input":"2021-09-03T06:08:09.4595Z","iopub.status.idle":"2021-09-03T06:08:09.471884Z","shell.execute_reply.started":"2021-09-03T06:08:09.45946Z","shell.execute_reply":"2021-09-03T06:08:09.470797Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2.2 Previous commits <a class=\"anchor\" id=\"2.2\"></a>\n\n[Back to Table of Contents](#0.1)","metadata":{}},{"cell_type":"code","source":"commits_df = pd.DataFrame(columns = ['n_commit', 'learning_rate', 'num_epochs', 'FE', 'target4', 'target32_34', 'LB_score'])","metadata":{"execution":{"iopub.status.busy":"2021-09-03T06:08:09.473634Z","iopub.execute_input":"2021-09-03T06:08:09.474065Z","iopub.status.idle":"2021-09-03T06:08:09.494018Z","shell.execute_reply.started":"2021-09-03T06:08:09.474033Z","shell.execute_reply":"2021-09-03T06:08:09.49284Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Commit 0 (parameters from [Stock Embedding - FFNN - My features](https://www.kaggle.com/alexioslyon/stock-embedding-ffnn-my-features), version 4)","metadata":{}},{"cell_type":"code","source":"n=0\ncommits_df.loc[n, 'n_commit'] = 0                   # Number of version\ncommits_df.loc[n, 'learning_rate'] = 0.005          # Learning rate\ncommits_df.loc[n, 'num_epochs'] = 1000              # Number of epochs\ncommits_df.loc[n, 'FE'] = 0                         # Was there a replacement of 300 for 350?\ncommits_df.loc[n, 'target4'] = 0.000935             # Target 0-4\ncommits_df.loc[n, 'target32_34'] = 0.002423         # Target 0-32 & 0-34\ncommits_df.loc[n, 'LB_score'] = 0.20157             # LB score after submitting","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-09-03T06:08:09.495251Z","iopub.execute_input":"2021-09-03T06:08:09.495716Z","iopub.status.idle":"2021-09-03T06:08:09.517878Z","shell.execute_reply.started":"2021-09-03T06:08:09.495668Z","shell.execute_reply":"2021-09-03T06:08:09.516568Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Commit 3","metadata":{}},{"cell_type":"code","source":"n=1\ncommits_df.loc[n, 'n_commit'] = 3                   # Number of version\ncommits_df.loc[n, 'learning_rate'] = 0.005          # Learning rate\ncommits_df.loc[n, 'num_epochs'] = 1100              # Number of epochs (but all calculations \n                                                    # are completed much earlier)\ncommits_df.loc[n, 'FE'] = 1                         # Was there a replacement of 300 for 350?\ncommits_df.loc[n, 'target4'] = 0.001048             # Target 0-4\ncommits_df.loc[n, 'target32_34'] = 0.002394         # Target 0-32 & 0-34\ncommits_df.loc[n, 'LB_score'] = 0.20209             # LB score after submitting","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-09-03T06:08:09.519443Z","iopub.execute_input":"2021-09-03T06:08:09.519864Z","iopub.status.idle":"2021-09-03T06:08:09.531611Z","shell.execute_reply.started":"2021-09-03T06:08:09.519816Z","shell.execute_reply":"2021-09-03T06:08:09.5304Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Commit 4","metadata":{}},{"cell_type":"code","source":"n=2\ncommits_df.loc[n, 'n_commit'] = 4                   # Number of version\ncommits_df.loc[n, 'learning_rate'] = 0.004          # Learning rate\ncommits_df.loc[n, 'num_epochs'] = 100               # Number of epochs\ncommits_df.loc[n, 'FE'] = 0                         # Was there a replacement of 300 for 350?\ncommits_df.loc[n, 'target4'] = 0.000968             # Target 0-4\ncommits_df.loc[n, 'target32_34'] = 0.002465         # Target 0-32 & 0-34\ncommits_df.loc[n, 'LB_score'] = 0.20219             # LB score after submitting","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-09-03T06:08:09.53343Z","iopub.execute_input":"2021-09-03T06:08:09.533872Z","iopub.status.idle":"2021-09-03T06:08:09.550921Z","shell.execute_reply.started":"2021-09-03T06:08:09.533761Z","shell.execute_reply":"2021-09-03T06:08:09.549728Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Commit 5","metadata":{}},{"cell_type":"code","source":"n=3\ncommits_df.loc[n, 'n_commit'] = 5                   # Number of version\ncommits_df.loc[n, 'learning_rate'] = 0.006          # Learning rate\ncommits_df.loc[n, 'num_epochs'] = 200               # Number of epochs\ncommits_df.loc[n, 'FE'] = 0                         # Was there a replacement of 300 for 350?\ncommits_df.loc[n, 'target4'] = 0.000829             # Target 0-4\ncommits_df.loc[n, 'target32_34'] = 0.002325         # Target 0-32 & 0-34\ncommits_df.loc[n, 'LB_score'] = 0.20012             # LB score after submitting","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-09-03T06:08:09.553659Z","iopub.execute_input":"2021-09-03T06:08:09.554007Z","iopub.status.idle":"2021-09-03T06:08:09.562758Z","shell.execute_reply.started":"2021-09-03T06:08:09.553976Z","shell.execute_reply":"2021-09-03T06:08:09.561727Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Commit 7","metadata":{}},{"cell_type":"code","source":"n=4\ncommits_df.loc[n, 'n_commit'] = 7                   # Number of version\ncommits_df.loc[n, 'learning_rate'] = 0.007          # Learning rate\ncommits_df.loc[n, 'num_epochs'] = 200               # Number of epochs\ncommits_df.loc[n, 'FE'] = 0                         # Was there a replacement of 300 for 350?\ncommits_df.loc[n, 'target4'] = 0.001622             # Target 0-4\ncommits_df.loc[n, 'target32_34'] = 0.002353         # Target 0-32 & 0-34\ncommits_df.loc[n, 'LB_score'] = 0.20101             # LB score after submitting","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-09-03T06:08:09.564943Z","iopub.execute_input":"2021-09-03T06:08:09.565549Z","iopub.status.idle":"2021-09-03T06:08:09.577617Z","shell.execute_reply.started":"2021-09-03T06:08:09.565515Z","shell.execute_reply":"2021-09-03T06:08:09.576567Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Commit 8","metadata":{}},{"cell_type":"code","source":"n=5\ncommits_df.loc[n, 'n_commit'] = 8                   # Number of version\ncommits_df.loc[n, 'learning_rate'] = 0.0055         # Learning rate\ncommits_df.loc[n, 'num_epochs'] = 200               # Number of epochs\ncommits_df.loc[n, 'FE'] = 0                         # Was there a replacement of 300 for 350?\ncommits_df.loc[n, 'target4'] = 0.001225             # Target 0-4\ncommits_df.loc[n, 'target32_34'] = 0.002434         # Target 0-32 & 0-34\ncommits_df.loc[n, 'LB_score'] = 0.20147             # LB score after submitting","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-09-03T06:08:09.579193Z","iopub.execute_input":"2021-09-03T06:08:09.579679Z","iopub.status.idle":"2021-09-03T06:08:09.598142Z","shell.execute_reply.started":"2021-09-03T06:08:09.579645Z","shell.execute_reply":"2021-09-03T06:08:09.597249Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Commit 9","metadata":{}},{"cell_type":"code","source":"n=6\ncommits_df.loc[n, 'n_commit'] = 9                   # Number of version\ncommits_df.loc[n, 'learning_rate'] = 0.0065         # Learning rate\ncommits_df.loc[n, 'num_epochs'] = 200               # Number of epochs\ncommits_df.loc[n, 'FE'] = 0                         # Was there a replacement of 300 for 350?\ncommits_df.loc[n, 'target4'] = 0.001466             # Target 0-4\ncommits_df.loc[n, 'target32_34'] = 0.002362         # Target 0-32 & 0-34\ncommits_df.loc[n, 'LB_score'] = 0.20057             # LB score after submitting","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-09-03T06:08:09.599466Z","iopub.execute_input":"2021-09-03T06:08:09.600087Z","iopub.status.idle":"2021-09-03T06:08:09.615428Z","shell.execute_reply.started":"2021-09-03T06:08:09.600037Z","shell.execute_reply":"2021-09-03T06:08:09.614175Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Commit 11","metadata":{}},{"cell_type":"code","source":"n=7\ncommits_df.loc[n, 'n_commit'] = 11                  # Number of version\ncommits_df.loc[n, 'learning_rate'] = 0.0059         # Learning rate\ncommits_df.loc[n, 'num_epochs'] = 200               # Number of epochs\ncommits_df.loc[n, 'FE'] = 0                         # Was there a replacement of 300 for 350?\ncommits_df.loc[n, 'target4'] = 0.001019             # Target 0-4\ncommits_df.loc[n, 'target32_34'] = 0.002347         # Target 0-32 & 0-34\ncommits_df.loc[n, 'LB_score'] = 0.20161             # LB score after submitting","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-09-03T06:08:09.617262Z","iopub.execute_input":"2021-09-03T06:08:09.617641Z","iopub.status.idle":"2021-09-03T06:08:09.629702Z","shell.execute_reply.started":"2021-09-03T06:08:09.617614Z","shell.execute_reply":"2021-09-03T06:08:09.628474Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Commit 12","metadata":{}},{"cell_type":"code","source":"n=8\ncommits_df.loc[n, 'n_commit'] = 12                  # Number of version\ncommits_df.loc[n, 'learning_rate'] = 0.0061         # Learning rate\ncommits_df.loc[n, 'num_epochs'] = 200               # Number of epochs\ncommits_df.loc[n, 'FE'] = 0                         # Was there a replacement of 300 for 350?\ncommits_df.loc[n, 'target4'] = 0.001295             # Target 0-4\ncommits_df.loc[n, 'target32_34'] = 0.002450         # Target 0-32 & 0-34\ncommits_df.loc[n, 'LB_score'] = 0.20148             # LB score after submitting","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-09-03T06:08:09.631827Z","iopub.execute_input":"2021-09-03T06:08:09.632364Z","iopub.status.idle":"2021-09-03T06:08:09.645552Z","shell.execute_reply.started":"2021-09-03T06:08:09.632319Z","shell.execute_reply":"2021-09-03T06:08:09.644322Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Commit 13","metadata":{}},{"cell_type":"code","source":"n=9\ncommits_df.loc[n, 'n_commit'] = 13                  # Number of version\ncommits_df.loc[n, 'learning_rate'] = 0.00601        # Learning rate\ncommits_df.loc[n, 'num_epochs'] = 200               # Number of epochs\ncommits_df.loc[n, 'FE'] = 0                         # Was there a replacement of 300 for 350?\ncommits_df.loc[n, 'target4'] = 0.000979             # Target 0-4\ncommits_df.loc[n, 'target32_34'] = 0.002414         # Target 0-32 & 0-34\ncommits_df.loc[n, 'LB_score'] = 0.20119             # LB score after submitting","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-09-03T06:08:09.646649Z","iopub.execute_input":"2021-09-03T06:08:09.647103Z","iopub.status.idle":"2021-09-03T06:08:09.66118Z","shell.execute_reply.started":"2021-09-03T06:08:09.647065Z","shell.execute_reply":"2021-09-03T06:08:09.660139Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Commit 14","metadata":{}},{"cell_type":"code","source":"n=10\ncommits_df.loc[n, 'n_commit'] = 14                  # Number of version\ncommits_df.loc[n, 'learning_rate'] = 0.00599        # Learning rate\ncommits_df.loc[n, 'num_epochs'] = 200               # Number of epochs\ncommits_df.loc[n, 'FE'] = 0                         # Was there a replacement of 300 for 350?\ncommits_df.loc[n, 'target4'] = 0.001537             # Target 0-4\ncommits_df.loc[n, 'target32_34'] = 0.002432         # Target 0-32 & 0-34\ncommits_df.loc[n, 'LB_score'] = 0.20216             # LB score after submitting","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-09-03T06:08:09.6625Z","iopub.execute_input":"2021-09-03T06:08:09.662988Z","iopub.status.idle":"2021-09-03T06:08:09.676153Z","shell.execute_reply.started":"2021-09-03T06:08:09.662938Z","shell.execute_reply":"2021-09-03T06:08:09.675196Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Commit 15","metadata":{}},{"cell_type":"code","source":"n=11\ncommits_df.loc[n, 'n_commit'] = 15                  # Number of version\ncommits_df.loc[n, 'learning_rate'] = 0.0049         # Learning rate\ncommits_df.loc[n, 'num_epochs'] = 200               # Number of epochs\ncommits_df.loc[n, 'FE'] = 0                         # Was there a replacement of 300 for 350?\ncommits_df.loc[n, 'target4'] = 0.001378             # Target 0-4\ncommits_df.loc[n, 'target32_34'] = 0.002514         # Target 0-32 & 0-34\ncommits_df.loc[n, 'LB_score'] = 0.20208             # LB score after submitting","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-09-03T06:08:09.677299Z","iopub.execute_input":"2021-09-03T06:08:09.67775Z","iopub.status.idle":"2021-09-03T06:08:09.694292Z","shell.execute_reply.started":"2021-09-03T06:08:09.677708Z","shell.execute_reply":"2021-09-03T06:08:09.693234Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Commit 16","metadata":{}},{"cell_type":"code","source":"n=12\ncommits_df.loc[n, 'n_commit'] = 16                  # Number of version\ncommits_df.loc[n, 'learning_rate'] = 0.0067         # Learning rate\ncommits_df.loc[n, 'num_epochs'] = 200               # Number of epochs\ncommits_df.loc[n, 'FE'] = 0                         # Was there a replacement of 300 for 350?\ncommits_df.loc[n, 'target4'] = 0.001633             # Target 0-4\ncommits_df.loc[n, 'target32_34'] = 0.002473         # Target 0-32 & 0-34\ncommits_df.loc[n, 'LB_score'] = 0.20150             # LB score after submitting","metadata":{"execution":{"iopub.status.busy":"2021-09-03T06:08:09.695603Z","iopub.execute_input":"2021-09-03T06:08:09.696266Z","iopub.status.idle":"2021-09-03T06:08:09.709949Z","shell.execute_reply.started":"2021-09-03T06:08:09.696231Z","shell.execute_reply":"2021-09-03T06:08:09.708525Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Commit 17","metadata":{}},{"cell_type":"code","source":"n=13\ncommits_df.loc[n, 'n_commit'] = 17                  # Number of version\ncommits_df.loc[n, 'learning_rate'] = 0.006          # Learning rate\ncommits_df.loc[n, 'num_epochs'] = 65                # Number of epochs\ncommits_df.loc[n, 'FE'] = 0                         # Was there a replacement of 300 for 350?\ncommits_df.loc[n, 'target4'] = 0.000776             # Target 0-4\ncommits_df.loc[n, 'target32_34'] = 0.002251         # Target 0-32 & 0-34\ncommits_df.loc[n, 'LB_score'] = 0.20050             # LB score after submitting","metadata":{"execution":{"iopub.status.busy":"2021-09-03T06:08:09.711401Z","iopub.execute_input":"2021-09-03T06:08:09.711709Z","iopub.status.idle":"2021-09-03T06:08:09.721498Z","shell.execute_reply.started":"2021-09-03T06:08:09.711679Z","shell.execute_reply":"2021-09-03T06:08:09.719963Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2.3 Parameters and LB score visualization <a class=\"anchor\" id=\"2.3\"></a>\n\n[Back to Table of Contents](#0.1)","metadata":{}},{"cell_type":"code","source":"# Find and mark minimun value of LB score\ncommits_df['LB_score'] = pd.to_numeric(commits_df['LB_score'])\ncommits_df = commits_df.sort_values(by=['LB_score'], ascending = True).reset_index(drop=True)\ncommits_df['min'] = 0\ncommits_df.loc[0, 'min'] = 1\ncommits_df","metadata":{"execution":{"iopub.status.busy":"2021-09-03T06:08:09.723101Z","iopub.execute_input":"2021-09-03T06:08:09.723443Z","iopub.status.idle":"2021-09-03T06:08:09.765674Z","shell.execute_reply.started":"2021-09-03T06:08:09.723413Z","shell.execute_reply":"2021-09-03T06:08:09.764201Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Interactive plot with results of parameters tuning\nfig = px.scatter_3d(commits_df, x='learning_rate', y='num_epochs', z='LB_score', color = 'min', \n                    symbol = 'FE',\n                    title='    Parameters and LB score visualization of \"ORV Prediction\" solutions')\nfig.update(layout=dict(title=dict(x=0.1)))","metadata":{"execution":{"iopub.status.busy":"2021-09-03T06:08:09.767537Z","iopub.execute_input":"2021-09-03T06:08:09.768032Z","iopub.status.idle":"2021-09-03T06:08:11.024396Z","shell.execute_reply.started":"2021-09-03T06:08:09.767986Z","shell.execute_reply":"2021-09-03T06:08:11.023028Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Interactive plot with targets\nfig = px.scatter_3d(commits_df, x='target4', y='target32_34', z='LB_score', color = 'min', \n                    symbol = 'learning_rate',\n                    title='     Targets and LB score visualization of \"ORV Prediction\" solutions')\nfig.update(layout=dict(title=dict(x=0.2)))","metadata":{"execution":{"iopub.status.busy":"2021-09-03T06:08:11.026361Z","iopub.execute_input":"2021-09-03T06:08:11.026869Z","iopub.status.idle":"2021-09-03T06:08:11.174149Z","shell.execute_reply.started":"2021-09-03T06:08:11.026796Z","shell.execute_reply":"2021-09-03T06:08:11.172929Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Interactive plot with learning rate and LB score\ncommits_df = commits_df.sort_values(by=['learning_rate'])\nfig = px.line(commits_df, x='learning_rate', y=\"LB_score\", text='n_commit',\n              title=\"Learning rate and LB score with number of version\",\n              log_y=True,template='gridon',width=800, height=500)\nfig.update_traces(textposition=\"bottom right\")\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2021-09-03T06:08:11.175839Z","iopub.execute_input":"2021-09-03T06:08:11.176198Z","iopub.status.idle":"2021-09-03T06:08:11.267804Z","shell.execute_reply.started":"2021-09-03T06:08:11.176166Z","shell.execute_reply":"2021-09-03T06:08:11.266449Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Interactive plot with target for 0-4 and LB score\ncommits_df = commits_df.sort_values(by=['target4'])\nfig = px.line(commits_df, x='target4', y=\"LB_score\", text='n_commit',\n              title=\"Target4 (for 0-4) and LB score with number of version\",\n              log_y=True,template='gridon',width=800, height=500)\nfig.update_traces(textposition=\"bottom right\")\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2021-09-03T06:08:11.271486Z","iopub.execute_input":"2021-09-03T06:08:11.271933Z","iopub.status.idle":"2021-09-03T06:08:11.327981Z","shell.execute_reply.started":"2021-09-03T06:08:11.271894Z","shell.execute_reply":"2021-09-03T06:08:11.326738Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Interactive plot with target for 0-32 & 0-34 and LB score\ncommits_df = commits_df.sort_values(by=['target32_34'])\nfig = px.line(commits_df, x='target32_34', y=\"LB_score\", text='n_commit',\n              title=\"Target32 (for 0-32 and 0-34) and LB score with number of version\",\n              log_y=True,template='gridon',width=800, height=500)\nfig.update_traces(textposition=\"bottom right\")\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2021-09-03T06:08:11.329709Z","iopub.execute_input":"2021-09-03T06:08:11.330085Z","iopub.status.idle":"2021-09-03T06:08:11.389217Z","shell.execute_reply.started":"2021-09-03T06:08:11.330047Z","shell.execute_reply":"2021-09-03T06:08:11.387906Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3. Download data<a class=\"anchor\" id=\"3\"></a>\n\n[Back to Table of Contents](#0.1)","metadata":{}},{"cell_type":"code","source":"def read_train_test():\n    # Function to read our base train and test set\n    \n    train = pd.read_csv('../input/optiver-realized-volatility-prediction/train.csv')\n    test = pd.read_csv('../input/optiver-realized-volatility-prediction/test.csv')\n\n    # Create a key to merge with book and trade data\n    train['row_id'] = train['stock_id'].astype(str) + '-' + train['time_id'].astype(str)\n    test['row_id'] = test['stock_id'].astype(str) + '-' + test['time_id'].astype(str)\n    print(f'Our training set has {train.shape[0]} rows')\n    \n    return train, test","metadata":{"execution":{"iopub.status.busy":"2021-08-21T16:46:57.312294Z","iopub.execute_input":"2021-08-21T16:46:57.312827Z","iopub.status.idle":"2021-08-21T16:46:57.319886Z","shell.execute_reply.started":"2021-08-21T16:46:57.312787Z","shell.execute_reply":"2021-08-21T16:46:57.319045Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Read train and test\ntrain, test = read_train_test()","metadata":{"execution":{"iopub.status.busy":"2021-08-21T16:46:57.321074Z","iopub.execute_input":"2021-08-21T16:46:57.321348Z","iopub.status.idle":"2021-08-21T16:46:58.849436Z","shell.execute_reply.started":"2021-08-21T16:46:57.321322Z","shell.execute_reply":"2021-08-21T16:46:58.848478Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4. FE & Data Preprocessing <a class=\"anchor\" id=\"4\"></a>\n\n[Back to Table of Contents](#0.1)","metadata":{}},{"cell_type":"code","source":"# data directory\ndata_dir = '../input/optiver-realized-volatility-prediction/'\n\ndef calc_wap1(df):\n    # Function to calculate first WAP\n    wap = (df['bid_price1'] * df['ask_size1'] + df['ask_price1'] * df['bid_size1']) / (df['bid_size1'] + df['ask_size1'])\n    return wap\n\ndef calc_wap2(df):\n    # Function to calculate second WAP\n    wap = (df['bid_price2'] * df['ask_size2'] + df['ask_price2'] * df['bid_size2']) / (df['bid_size2'] + df['ask_size2'])\n    return wap\n\ndef log_return(series):\n    # Function to calculate the log of the return\n    return np.log(series).diff()\n\ndef realized_volatility(series):\n    # Calculate the realized volatility\n    return np.sqrt(np.sum(series**2))\n\ndef count_unique(series):\n    # Function to count unique elements of a series\n    return len(np.unique(series))\n\ndef book_preprocessor(file_path):\n    # Function to preprocess book data (for each stock id)\n    \n    df = pd.read_parquet(file_path)\n    \n    # Calculate Wap\n    df['wap1'] = calc_wap1(df)\n    df['wap2'] = calc_wap2(df)\n    \n    # Calculate log returns\n    df['log_return1'] = df.groupby(['time_id'])['wap1'].apply(log_return)\n    df['log_return2'] = df.groupby(['time_id'])['wap2'].apply(log_return)\n    \n    # Calculate wap balance\n    df['wap_balance'] = abs(df['wap1'] - df['wap2'])\n    \n    # Calculate spread\n    df['price_spread'] = (df['ask_price1'] - df['bid_price1']) / ((df['ask_price1'] + df['bid_price1']) / 2)\n    df['price_spread2'] = (df['ask_price2'] - df['bid_price2']) / ((df['ask_price2'] + df['bid_price2']) / 2)\n    df['bid_spread'] = df['bid_price1'] - df['bid_price2']\n    df['ask_spread'] = df['ask_price1'] - df['ask_price2']\n    df[\"bid_ask_spread\"] = abs(df['bid_spread'] - df['ask_spread'])\n    df['total_volume'] = (df['ask_size1'] + df['ask_size2']) + (df['bid_size1'] + df['bid_size2'])\n    df['volume_imbalance'] = abs((df['ask_size1'] + df['ask_size2']) - (df['bid_size1'] + df['bid_size2']))\n    \n    # Dict for aggregations\n    create_feature_dict = {\n        'wap1': [np.sum, np.mean, np.std],\n        'wap2': [np.sum, np.mean, np.std],\n        'log_return1': [np.sum, realized_volatility, np.mean, np.std],\n        'log_return2': [np.sum, realized_volatility, np.mean, np.std],\n        'wap_balance': [np.sum, np.mean, np.std],\n        'price_spread':[np.sum, np.mean, np.std],\n        'price_spread2':[np.sum, np.mean, np.std],\n        'bid_spread':[np.sum, np.mean, np.std],\n        'ask_spread':[np.sum, np.mean, np.std],\n        'total_volume':[np.sum, np.mean, np.std],\n        'volume_imbalance':[np.sum, np.mean, np.std],\n        \"bid_ask_spread\":[np.sum, np.mean, np.std],\n    }\n    \n    def get_stats_window(seconds_in_bucket, add_suffix = False):\n        # Function to get group stats for different windows (seconds in bucket)\n        \n        # Group by the window\n        df_feature = df[df['seconds_in_bucket'] >= seconds_in_bucket].groupby(['time_id']).agg(create_feature_dict).reset_index()\n        \n        # Rename columns joining suffix\n        df_feature.columns = ['_'.join(col) for col in df_feature.columns]\n        \n        # Add a suffix to differentiate windows\n        if add_suffix:\n            df_feature = df_feature.add_suffix('_' + str(seconds_in_bucket))\n        return df_feature\n    \n    # Get the stats for different windows\n    df_feature = get_stats_window(seconds_in_bucket = 0, add_suffix = False)\n    df_feature_400 = get_stats_window(seconds_in_bucket = 400, add_suffix = True)\n    df_feature_300 = get_stats_window(seconds_in_bucket = 300, add_suffix = True)\n    df_feature_200 = get_stats_window(seconds_in_bucket = 200, add_suffix = True)\n    \n    # Merge all\n    df_feature = df_feature.merge(df_feature_400, how = 'left', left_on = 'time_id_', right_on = 'time_id__400')\n    df_feature = df_feature.merge(df_feature_300, how = 'left', left_on = 'time_id_', right_on = 'time_id__300')\n    df_feature = df_feature.merge(df_feature_200, how = 'left', left_on = 'time_id_', right_on = 'time_id__200')\n\n    # Drop unnecesary time_ids\n    df_feature.drop(['time_id__400', 'time_id__300', 'time_id__200'], axis = 1, inplace = True)\n    \n    \n    # Create row_id so we can merge\n    stock_id = file_path.split('=')[1]\n    df_feature['row_id'] = df_feature['time_id_'].apply(lambda x: f'{stock_id}-{x}')\n    df_feature.drop(['time_id_'], axis = 1, inplace = True)\n    \n    return df_feature\n\n\ndef trade_preprocessor(file_path):\n    # Function to preprocess trade data (for each stock id)\n    \n    df = pd.read_parquet(file_path)\n    df['log_return'] = df.groupby('time_id')['price'].apply(log_return)\n    \n    # Dict for aggregations\n    create_feature_dict = {\n        'log_return':[realized_volatility],\n        'seconds_in_bucket':[count_unique],\n        'size':[np.sum, realized_volatility, np.mean, np.std, np.max, np.min],\n        'order_count':[np.mean,np.sum,np.max],\n    }\n    \n    def get_stats_window(seconds_in_bucket, add_suffix = False):\n        # Function to get group stats for different windows (seconds in bucket)\n        \n        # Group by the window\n        df_feature = df[df['seconds_in_bucket'] >= seconds_in_bucket].groupby(['time_id']).agg(create_feature_dict).reset_index()\n        \n        # Rename columns joining suffix\n        df_feature.columns = ['_'.join(col) for col in df_feature.columns]\n        \n        # Add a suffix to differentiate windows\n        if add_suffix:\n            df_feature = df_feature.add_suffix('_' + str(seconds_in_bucket))\n        return df_feature\n    \n    # Get the stats for different windows\n    df_feature = get_stats_window(seconds_in_bucket = 0, add_suffix = False)\n    df_feature_400 = get_stats_window(seconds_in_bucket = 400, add_suffix = True)\n    df_feature_300 = get_stats_window(seconds_in_bucket = 300, add_suffix = True)\n    df_feature_200 = get_stats_window(seconds_in_bucket = 200, add_suffix = True)\n    \n    def tendency(price, vol):    \n        df_diff = np.diff(price)\n        val = (df_diff/price[1:])*100\n        power = np.sum(val*vol[1:])\n        return(power)\n    \n    lis = []\n    for n_time_id in df['time_id'].unique():\n        df_id = df[df['time_id'] == n_time_id]        \n        tendencyV = tendency(df_id['price'].values, df_id['size'].values)      \n        f_max = np.sum(df_id['price'].values > np.mean(df_id['price'].values))\n        f_min = np.sum(df_id['price'].values < np.mean(df_id['price'].values))\n        df_max =  np.sum(np.diff(df_id['price'].values) > 0)\n        df_min =  np.sum(np.diff(df_id['price'].values) < 0)\n        abs_diff = np.median(np.abs( df_id['price'].values - np.mean(df_id['price'].values)))        \n        energy = np.mean(df_id['price'].values**2)\n        iqr_p = np.percentile(df_id['price'].values,75) - np.percentile(df_id['price'].values,25)\n        abs_diff_v = np.median(np.abs( df_id['size'].values - np.mean(df_id['size'].values)))        \n        energy_v = np.sum(df_id['size'].values**2)\n        iqr_p_v = np.percentile(df_id['size'].values,75) - np.percentile(df_id['size'].values,25)\n        \n        lis.append({'time_id':n_time_id,'tendency':tendencyV,'f_max':f_max,'f_min':f_min,'df_max':df_max,'df_min':df_min,\n                   'abs_diff':abs_diff,'energy':energy,'iqr_p':iqr_p,'abs_diff_v':abs_diff_v,'energy_v':energy_v,'iqr_p_v':iqr_p_v})\n    \n    df_lr = pd.DataFrame(lis)\n        \n   \n    df_feature = df_feature.merge(df_lr, how = 'left', left_on = 'time_id_', right_on = 'time_id')\n    \n    # Merge all\n    df_feature = df_feature.merge(df_feature_400, how = 'left', left_on = 'time_id_', right_on = 'time_id__400')\n    df_feature = df_feature.merge(df_feature_300, how = 'left', left_on = 'time_id_', right_on = 'time_id__300')\n    df_feature = df_feature.merge(df_feature_200, how = 'left', left_on = 'time_id_', right_on = 'time_id__200')\n\n    # Drop unnecesary time_ids\n    df_feature.drop(['time_id__400', 'time_id__300', 'time_id__200','time_id'], axis = 1, inplace = True)\n    df_feature = df_feature.add_prefix('trade_')\n    stock_id = file_path.split('=')[1]\n    df_feature['row_id'] = df_feature['trade_time_id_'].apply(lambda x:f'{stock_id}-{x}')\n    df_feature.drop(['trade_time_id_'], axis = 1, inplace = True)\n    \n    return df_feature\n\n\ndef get_time_stock(df):\n    # Function to get group stats for the stock_id and time_id\n    \n    # Get realized volatility columns\n    vol_cols = ['log_return1_realized_volatility', 'log_return2_realized_volatility', 'log_return1_realized_volatility_400', 'log_return2_realized_volatility_400', \n                'log_return1_realized_volatility_300', 'log_return2_realized_volatility_300', 'log_return1_realized_volatility_200', 'log_return2_realized_volatility_200', \n                'trade_log_return_realized_volatility', 'trade_log_return_realized_volatility_400', 'trade_log_return_realized_volatility_300', 'trade_log_return_realized_volatility_200']\n\n    # Group by the stock id\n    df_stock_id = df.groupby(['stock_id'])[vol_cols].agg(['mean', 'std', 'max', 'min', ]).reset_index()\n    \n    # Rename columns joining suffix\n    df_stock_id.columns = ['_'.join(col) for col in df_stock_id.columns]\n    df_stock_id = df_stock_id.add_suffix('_' + 'stock')\n\n    # Group by the stock id\n    df_time_id = df.groupby(['time_id'])[vol_cols].agg(['mean', 'std', 'max', 'min', ]).reset_index()\n    \n    # Rename columns joining suffix\n    df_time_id.columns = ['_'.join(col) for col in df_time_id.columns]\n    df_time_id = df_time_id.add_suffix('_' + 'time')\n    \n    # Merge with original dataframe\n    df = df.merge(df_stock_id, how = 'left', left_on = ['stock_id'], right_on = ['stock_id__stock'])\n    df = df.merge(df_time_id, how = 'left', left_on = ['time_id'], right_on = ['time_id__time'])\n    df.drop(['stock_id__stock', 'time_id__time'], axis = 1, inplace = True)\n    \n    return df\n    \n    \ndef preprocessor(list_stock_ids, is_train = True):\n    # Funtion to make preprocessing function in parallel (for each stock id)\n    \n    # Parrallel for loop\n    def for_joblib(stock_id):\n        # Train\n        if is_train:\n            file_path_book = data_dir + \"book_train.parquet/stock_id=\" + str(stock_id)\n            file_path_trade = data_dir + \"trade_train.parquet/stock_id=\" + str(stock_id)\n        # Test\n        else:\n            file_path_book = data_dir + \"book_test.parquet/stock_id=\" + str(stock_id)\n            file_path_trade = data_dir + \"trade_test.parquet/stock_id=\" + str(stock_id)\n    \n        # Preprocess book and trade data and merge them\n        df_tmp = pd.merge(book_preprocessor(file_path_book), trade_preprocessor(file_path_trade), on = 'row_id', how = 'left')\n        \n        # Return the merge dataframe\n        return df_tmp\n    \n    # Use parallel api to call paralle for loop\n    df = Parallel(n_jobs = -1, verbose = 1)(delayed(for_joblib)(stock_id) for stock_id in list_stock_ids)\n    \n    # Concatenate all the dataframes that return from Parallel\n    df = pd.concat(df, ignore_index = True)\n    \n    return df\n\n\ndef rmspe(y_true, y_pred):\n    # Function to calculate the root mean squared percentage error\n    return np.sqrt(np.mean(np.square((y_true - y_pred) / y_true)))\n\ndef feval_rmspe(y_pred, lgb_train):\n    # Function to early stop with root mean squared percentage error\n    y_true = lgb_train.get_label()\n    return 'RMSPE', rmspe(y_true, y_pred), False","metadata":{"papermill":{"duration":0.102144,"end_time":"2021-08-20T18:37:41.513544","exception":false,"start_time":"2021-08-20T18:37:41.4114","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-08-21T16:46:58.850994Z","iopub.execute_input":"2021-08-21T16:46:58.851412Z","iopub.status.idle":"2021-08-21T16:46:58.901745Z","shell.execute_reply.started":"2021-08-21T16:46:58.851371Z","shell.execute_reply":"2021-08-21T16:46:58.900961Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get unique stock ids \ntrain_stock_ids = train['stock_id'].unique()\n\n# Preprocess them using Parallel and our single stock id functions\ntrain_ = preprocessor(train_stock_ids, is_train = True)\ntrain = train.merge(train_, on = ['row_id'], how = 'left')\n\n# Get unique stock ids \ntest_stock_ids = test['stock_id'].unique()\n\n# Preprocess them using Parallel and our single stock id functions\ntest_ = preprocessor(test_stock_ids, is_train = False)\ntest = test.merge(test_, on = ['row_id'], how = 'left')\n\n# Get group stats of time_id and stock_id\ntrain = get_time_stock(train)\ntest = get_time_stock(test)","metadata":{"papermill":{"duration":2093.381223,"end_time":"2021-08-20T19:12:34.924531","exception":false,"start_time":"2021-08-20T18:37:41.543308","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-08-21T16:46:58.902814Z","iopub.execute_input":"2021-08-21T16:46:58.903243Z","iopub.status.idle":"2021-08-21T16:48:31.520763Z","shell.execute_reply.started":"2021-08-21T16:46:58.903204Z","shell.execute_reply":"2021-08-21T16:48:31.517552Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# replace by order sum (tau)\ntrain['size_tau'] = np.sqrt(1/train['trade_seconds_in_bucket_count_unique'])\ntest['size_tau'] = np.sqrt(1/test['trade_seconds_in_bucket_count_unique'])\ntrain['size_tau_400'] = np.sqrt(1/train['trade_seconds_in_bucket_count_unique_400'])\ntest['size_tau_400'] = np.sqrt(1/test['trade_seconds_in_bucket_count_unique_400'])\ntrain['size_tau_300'] = np.sqrt(1/train['trade_seconds_in_bucket_count_unique_300'])\ntest['size_tau_300'] = np.sqrt(1/test['trade_seconds_in_bucket_count_unique_300'])\ntrain['size_tau_200'] = np.sqrt(1/train['trade_seconds_in_bucket_count_unique_200'])\ntest['size_tau_200'] = np.sqrt(1/test['trade_seconds_in_bucket_count_unique_200'])","metadata":{"papermill":{"duration":0.075331,"end_time":"2021-08-20T19:12:35.204208","exception":false,"start_time":"2021-08-20T19:12:35.128877","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-08-21T16:48:31.521931Z","iopub.status.idle":"2021-08-21T16:48:31.522358Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# tau2 \ntrain['size_tau2'] = np.sqrt(1/train['trade_order_count_sum'])\ntest['size_tau2'] = np.sqrt(1/test['trade_order_count_sum'])\ntrain['size_tau2_400'] = np.sqrt(0.25/train['trade_order_count_sum'])\ntest['size_tau2_400'] = np.sqrt(0.25/test['trade_order_count_sum'])\ntrain['size_tau2_300'] = np.sqrt(0.5/train['trade_order_count_sum'])\ntest['size_tau2_300'] = np.sqrt(0.5/test['trade_order_count_sum'])\ntrain['size_tau2_200'] = np.sqrt(0.75/train['trade_order_count_sum'])\ntest['size_tau2_200'] = np.sqrt(0.75/test['trade_order_count_sum'])\n\n# delta tau\ntrain['size_tau2_d'] = train['size_tau2_400'] - train['size_tau2']\ntest['size_tau2_d'] = test['size_tau2_400'] - test['size_tau2']","metadata":{"papermill":{"duration":0.076926,"end_time":"2021-08-20T19:12:35.314626","exception":false,"start_time":"2021-08-20T19:12:35.2377","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-08-21T16:48:31.523259Z","iopub.status.idle":"2021-08-21T16:48:31.523631Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 5. Modeling and prediction<a class=\"anchor\" id=\"5\"></a>\n\n[Back to Table of Contents](#0.1)","metadata":{}},{"cell_type":"code","source":"# kfold based on the knn++ algorithm\n\nout_train = pd.read_csv('../input/optiver-realized-volatility-prediction/train.csv')\nout_train = out_train.pivot(index='time_id', columns='stock_id', values='target')\n\n# out_train[out_train.isna().any(axis=1)]\nout_train = out_train.fillna(out_train.mean())\nout_train.head()\n\n# Code to add the just the read data after first execution\n\n# Data separation based on knn ++\nnfolds = 5 # number of folds\nindex = []\ntotDist = []\nvalues = []\n\n# Generates a matriz with the values of \nmat = out_train.values\nscaler = MinMaxScaler(feature_range=(-1, 1))\nmat = scaler.fit_transform(mat)\nnind = int(mat.shape[0]/nfolds) # number of individuals\n\n# Adds index in the last column\nmat = np.c_[mat,np.arange(mat.shape[0])]\nlineNumber = np.random.choice(np.array(mat.shape[0]), size=nfolds, replace=False)\nlineNumber = np.sort(lineNumber)[::-1]\nfor n in range(nfolds):\n    totDist.append(np.zeros(mat.shape[0]-nfolds))\n\n# Saves index\nfor n in range(nfolds):    \n    values.append([lineNumber[n]])\n\ns=[]\nfor n in range(nfolds):\n    s.append(mat[lineNumber[n],:])\n    mat = np.delete(mat, obj=lineNumber[n], axis=0)\n\nfor n in range(nind-1):    \n    luck = np.random.uniform(0,1,nfolds)\n    \n    for cycle in range(nfolds):\n        # Saves the values of index           \n        s[cycle] = np.matlib.repmat(s[cycle], mat.shape[0], 1)\n        sumDist = np.sum( (mat[:,:-1] - s[cycle][:,:-1])**2 , axis=1)   \n        totDist[cycle] += sumDist        \n                \n        # Probabilities\n        f = totDist[cycle]/np.sum(totDist[cycle]) # normalizing the totDist\n        j = 0\n        kn = 0\n        for val in f:\n            j += val        \n            if (j > luck[cycle]): # the column was selected\n                break\n            kn +=1\n        lineNumber[cycle] = kn\n        \n        # Delete line of the value added    \n        for n_iter in range(nfolds):\n            totDist[n_iter] = np.delete(totDist[n_iter],obj=lineNumber[cycle], axis=0)\n            j= 0\n        \n        s[cycle] = mat[lineNumber[cycle],:]\n        values[cycle].append(int(mat[lineNumber[cycle],-1]))\n        mat = np.delete(mat, obj=lineNumber[cycle], axis=0)\n\nfor n_mod in range(nfolds):\n    values[n_mod] = out_train.index[values[n_mod]]","metadata":{"papermill":{"duration":16.508618,"end_time":"2021-08-20T19:12:59.464387","exception":false,"start_time":"2021-08-20T19:12:42.955769","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-08-21T16:48:31.524404Z","iopub.status.idle":"2021-08-21T16:48:31.524763Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def root_mean_squared_per_error(y_true, y_pred):\n         return K.sqrt(K.mean(K.square( (y_true - y_pred)/ y_true )))\n    \nes = tf.keras.callbacks.EarlyStopping(\n    monitor='val_loss', patience=20, verbose=0,\n    mode='min',restore_best_weights=True)\n\nplateau = tf.keras.callbacks.ReduceLROnPlateau(\n    monitor='val_loss', factor=0.2, patience=7, verbose=0,\n    mode='min')","metadata":{"papermill":{"duration":0.043638,"end_time":"2021-08-20T19:12:59.540757","exception":false,"start_time":"2021-08-20T19:12:59.497119","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-08-21T16:48:31.525539Z","iopub.status.idle":"2021-08-21T16:48:31.525899Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"colNames = list(train)\ncolNames.remove('time_id')\ncolNames.remove('target')\ncolNames.remove('row_id')\ncolNames.remove('stock_id')","metadata":{"papermill":{"duration":0.042132,"end_time":"2021-08-20T19:12:59.614646","exception":false,"start_time":"2021-08-20T19:12:59.572514","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-08-21T16:48:31.52696Z","iopub.status.idle":"2021-08-21T16:48:31.527344Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.replace([np.inf, -np.inf], np.nan,inplace=True)\ntest.replace([np.inf, -np.inf], np.nan,inplace=True)\nqt_train = []\n\nfor col in colNames:\n    qt = QuantileTransformer(random_state=21,n_quantiles=2000, output_distribution='normal')\n    train[col] = qt.fit_transform(train[[col]])\n    test[col] = qt.transform(test[[col]])    \n    qt_train.append(qt)","metadata":{"papermill":{"duration":59.937532,"end_time":"2021-08-20T19:13:59.585841","exception":false,"start_time":"2021-08-20T19:12:59.648309","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-08-21T16:48:31.528344Z","iopub.status.idle":"2021-08-21T16:48:31.528733Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Making agg features\n\ntrain_p = pd.read_csv('../input/optiver-realized-volatility-prediction/train.csv')\ntrain_p = train_p.pivot(index='time_id', columns='stock_id', values='target')\ncorr = train_p.corr()\nids = corr.index\nkmeans = KMeans(n_clusters=7, random_state=0).fit(corr.values)\nprint(kmeans.labels_)\nl = []\nfor n in range(7):\n    l.append ( [ (x-1) for x in ( (ids+1)*(kmeans.labels_ == n)) if x > 0] )\n\nmat = []\nmatTest = []\nn = 0\nfor ind in l:\n    print(ind)\n    newDf = train.loc[train['stock_id'].isin(ind) ]\n    newDf = newDf.groupby(['time_id']).agg(np.nanmean)\n    newDf.loc[:,'stock_id'] = str(n)+'c1'\n    mat.append ( newDf )\n    newDf = test.loc[test['stock_id'].isin(ind) ]    \n    newDf = newDf.groupby(['time_id']).agg(np.nanmean)\n    newDf.loc[:,'stock_id'] = str(n)+'c1'\n    matTest.append ( newDf )\n    n+=1\n    \nmat1 = pd.concat(mat).reset_index()\nmat1.drop(columns=['target'],inplace=True)\nmat2 = pd.concat(matTest).reset_index()","metadata":{"papermill":{"duration":3.25352,"end_time":"2021-08-20T19:14:03.328398","exception":false,"start_time":"2021-08-20T19:14:00.074878","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-08-21T16:48:31.529745Z","iopub.status.idle":"2021-08-21T16:48:31.530164Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"matTest = []\nmat = []\nkmeans = []","metadata":{"papermill":{"duration":0.04326,"end_time":"2021-08-20T19:14:03.404456","exception":false,"start_time":"2021-08-20T19:14:03.361196","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-08-21T16:48:31.531162Z","iopub.status.idle":"2021-08-21T16:48:31.53156Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mat2 = pd.concat([mat2,mat1.loc[mat1.time_id==5]])","metadata":{"papermill":{"duration":0.095701,"end_time":"2021-08-20T19:14:03.532907","exception":false,"start_time":"2021-08-20T19:14:03.437206","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-08-21T16:48:31.533349Z","iopub.status.idle":"2021-08-21T16:48:31.533762Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mat1 = mat1.pivot(index='time_id', columns='stock_id')\nmat1.columns = [\"_\".join(x) for x in mat1.columns.ravel()]\nmat1.reset_index(inplace=True)\n\nmat2 = mat2.pivot(index='time_id', columns='stock_id')\nmat2.columns = [\"_\".join(x) for x in mat2.columns.ravel()]\nmat2.reset_index(inplace=True)","metadata":{"papermill":{"duration":0.396973,"end_time":"2021-08-20T19:14:03.963125","exception":false,"start_time":"2021-08-20T19:14:03.566152","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-08-21T16:48:31.5349Z","iopub.status.idle":"2021-08-21T16:48:31.535316Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nnn = ['time_id',\n     'log_return1_realized_volatility_0c1',\n     'log_return1_realized_volatility_1c1',     \n     'log_return1_realized_volatility_3c1',\n     'log_return1_realized_volatility_4c1',     \n     'log_return1_realized_volatility_6c1',\n     'total_volume_mean_0c1',\n     'total_volume_mean_1c1', \n     'total_volume_mean_3c1',\n     'total_volume_mean_4c1', \n     'total_volume_mean_6c1',\n     'trade_size_mean_0c1',\n     'trade_size_mean_1c1', \n     'trade_size_mean_3c1',\n     'trade_size_mean_4c1', \n     'trade_size_mean_6c1',\n     'trade_order_count_mean_0c1',\n     'trade_order_count_mean_1c1',\n     'trade_order_count_mean_3c1',\n     'trade_order_count_mean_4c1',\n     'trade_order_count_mean_6c1',      \n     'price_spread_mean_0c1',\n     'price_spread_mean_1c1',\n     'price_spread_mean_3c1',\n     'price_spread_mean_4c1',\n     'price_spread_mean_6c1',   \n     'bid_spread_mean_0c1',\n     'bid_spread_mean_1c1',\n     'bid_spread_mean_3c1',\n     'bid_spread_mean_4c1',\n     'bid_spread_mean_6c1',       \n     'ask_spread_mean_0c1',\n     'ask_spread_mean_1c1',\n     'ask_spread_mean_3c1',\n     'ask_spread_mean_4c1',\n     'ask_spread_mean_6c1',   \n     'volume_imbalance_mean_0c1',\n     'volume_imbalance_mean_1c1',\n     'volume_imbalance_mean_3c1',\n     'volume_imbalance_mean_4c1',\n     'volume_imbalance_mean_6c1',       \n     'bid_ask_spread_mean_0c1',\n     'bid_ask_spread_mean_1c1',\n     'bid_ask_spread_mean_3c1',\n     'bid_ask_spread_mean_4c1',\n     'bid_ask_spread_mean_6c1',\n     'size_tau2_0c1',\n     'size_tau2_1c1',\n     'size_tau2_3c1',\n     'size_tau2_4c1',\n     'size_tau2_6c1'] ","metadata":{"papermill":{"duration":0.051279,"end_time":"2021-08-20T19:14:04.049124","exception":false,"start_time":"2021-08-20T19:14:03.997845","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-08-21T16:48:31.536453Z","iopub.status.idle":"2021-08-21T16:48:31.53684Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.merge(train,mat1[nnn],how='left',on='time_id')","metadata":{"papermill":{"duration":33.289774,"end_time":"2021-08-20T19:14:37.373845","exception":false,"start_time":"2021-08-20T19:14:04.084071","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-08-21T16:48:31.537986Z","iopub.status.idle":"2021-08-21T16:48:31.538389Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test = pd.merge(test,mat2[nnn],how='left',on='time_id')","metadata":{"papermill":{"duration":0.05874,"end_time":"2021-08-20T19:14:37.467222","exception":false,"start_time":"2021-08-20T19:14:37.408482","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-08-21T16:48:31.539432Z","iopub.status.idle":"2021-08-21T16:48:31.539866Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mat1 = []\nmat2 = []","metadata":{"papermill":{"duration":0.040234,"end_time":"2021-08-20T19:14:37.541722","exception":false,"start_time":"2021-08-20T19:14:37.501488","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-08-21T16:48:31.540847Z","iopub.status.idle":"2021-08-21T16:48:31.541249Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Thanks to https://bignerdranch.com/blog/implementing-swish-activation-function-in-keras/\ndef swish(x, beta = 1):\n    return (x * sigmoid(beta * x))\n\nget_custom_objects().update({'swish': Activation(swish)})","metadata":{"papermill":{"duration":0.058598,"end_time":"2021-08-20T19:14:37.632581","exception":false,"start_time":"2021-08-20T19:14:37.573983","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-08-21T16:48:31.542049Z","iopub.status.idle":"2021-08-21T16:48:31.542451Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"hidden_units = (128,64,32)\nstock_embedding_size = 24\ncat_data = train['stock_id']\n\ndef base_model():\n    \n    # Each instance will consist of two inputs: a single user id, and a single movie id\n    stock_id_input = keras.Input(shape=(1,), name='stock_id')\n    num_input = keras.Input(shape=(362,), name='num_data')\n\n    # Embedding, flatenning and concatenating\n    stock_embedded = keras.layers.Embedding(max(cat_data)+1, stock_embedding_size, \n                                           input_length=1, name='stock_embedding')(stock_id_input)\n    stock_flattened = keras.layers.Flatten()(stock_embedded)\n    out = keras.layers.Concatenate()([stock_flattened, num_input])\n    \n    # Add one or more hidden layers\n    for n_hidden in hidden_units:\n        out = keras.layers.Dense(n_hidden, activation='swish')(out)\n\n    # A single output: our predicted rating\n    out = keras.layers.Dense(1, activation='linear', name='prediction')(out)\n    \n    model = keras.Model(\n    inputs = [stock_id_input, num_input],\n    outputs = out,\n    )\n    \n    return model","metadata":{"papermill":{"duration":0.05059,"end_time":"2021-08-20T19:14:37.715931","exception":false,"start_time":"2021-08-20T19:14:37.665341","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-08-21T16:48:31.543237Z","iopub.status.idle":"2021-08-21T16:48:31.543608Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_name = 'NN'\npred_name = 'pred_{}'.format(model_name)\n\nn_folds = 5\nkf = model_selection.KFold(n_splits=n_folds, shuffle=True, random_state=2020)\nscores_folds[model_name] = []\ncounter = 1\n\nfeatures_to_consider = list(train)\n\nfeatures_to_consider.remove('time_id')\nfeatures_to_consider.remove('target')\nfeatures_to_consider.remove('row_id')\ntry:\n    features_to_consider.remove('pred_NN')\nexcept:\n    pass\n\ntrain[features_to_consider] = train[features_to_consider].fillna(train[features_to_consider].mean())\ntest[features_to_consider] = test[features_to_consider].fillna(train[features_to_consider].mean())\n\ntrain[pred_name] = 0\ntest['target'] = 0\n\nfor n_count in range(n_folds):\n    print('CV {}/{}'.format(counter, n_folds))\n    \n    indexes = np.arange(nfolds).astype(int)    \n    indexes = np.delete(indexes,obj=n_count, axis=0) \n    indexes = np.r_[values[indexes[0]],values[indexes[1]],values[indexes[2]],values[indexes[3]]]\n    \n    X_train = train.loc[train.time_id.isin(indexes), features_to_consider]\n    y_train = train.loc[train.time_id.isin(indexes), target_name]\n    X_test = train.loc[train.time_id.isin(values[n_count]), features_to_consider]\n    y_test = train.loc[train.time_id.isin(values[n_count]), target_name]\n    \n    # NN\n    model = base_model()\n    \n    model.compile(\n        keras.optimizers.Adam(learning_rate=learning_rate),\n        loss=root_mean_squared_per_error\n    )\n    \n    try:\n        features_to_consider.remove('stock_id')\n    except:\n        pass\n    \n    num_data = X_train[features_to_consider]\n    \n    scaler = MinMaxScaler(feature_range=(-1, 1))         \n    num_data = scaler.fit_transform(num_data.values)    \n    \n    cat_data = X_train['stock_id']    \n    target =  y_train\n    \n    num_data_test = X_test[features_to_consider]\n    num_data_test = scaler.transform(num_data_test.values)\n    cat_data_test = X_test['stock_id']\n\n    model.fit([cat_data, num_data], \n              target,               \n              batch_size=2048,\n              epochs=num_epochs,\n              validation_data=([cat_data_test, num_data_test], y_test),\n              callbacks=[es, plateau],\n              validation_batch_size=len(y_test),\n              shuffle=True,\n             verbose = 1)\n\n    preds = model.predict([cat_data_test, num_data_test]).reshape(1,-1)[0]\n    \n    score = round(rmspe(y_true = y_test, y_pred = preds),5)\n    print('Fold {} {}: {}'.format(counter, model_name, score))\n    scores_folds[model_name].append(score)\n    \n    tt =scaler.transform(test[features_to_consider].values)\n    test[target_name] += model.predict([test['stock_id'], tt]).reshape(1,-1)[0].clip(0,1e10)\n       \n    counter += 1\n    features_to_consider.append('stock_id')","metadata":{"papermill":{"duration":1335.415041,"end_time":"2021-08-20T19:36:53.164078","exception":false,"start_time":"2021-08-20T19:14:37.749037","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-08-21T16:48:31.544502Z","iopub.status.idle":"2021-08-21T16:48:31.544906Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 6. Submission <a class=\"anchor\" id=\"6\"></a>\n\n[Back to Table of Contents](#0.1)","metadata":{}},{"cell_type":"code","source":"# Postprocessing\ntest[target_name] = test[target_name]/n_folds\n\nscore = round(rmspe(y_true = train[target_name].values, y_pred = train[pred_name].values),5)\nprint('RMSPE {}: {} - Folds: {}'.format(model_name, score, scores_folds[model_name]))\n\ndisplay(test[['row_id', target_name]].head(2))\n\n# Submission\ntest[['row_id', target_name]].to_csv('submission.csv',index = False)","metadata":{"papermill":{"duration":7.416077,"end_time":"2021-08-20T19:37:07.837487","exception":false,"start_time":"2021-08-20T19:37:00.42141","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-08-21T16:48:31.545895Z","iopub.status.idle":"2021-08-21T16:48:31.546326Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"[Go to Top](#0)","metadata":{}}]}