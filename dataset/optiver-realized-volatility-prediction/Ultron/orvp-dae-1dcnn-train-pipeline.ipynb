{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Imports","metadata":{}},{"cell_type":"code","source":"import os\nimport gc\nimport random\nimport pickle\nimport numpy as np\nimport pandas as pd\nfrom tqdm.notebook import tqdm\nimport matplotlib.pyplot as plt\nfrom joblib import Parallel, delayed\n\nfrom scipy.optimize import minimize\nfrom sklearn.cluster import KMeans\nfrom sklearn.model_selection import KFold\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler, LabelEncoder, QuantileTransformer\n\ntqdm.pandas()\n%matplotlib inline\n\nimport tensorflow as tf\nimport tensorflow.keras.backend as K\nfrom tensorflow.keras import Model, Input\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.utils import get_custom_objects\nfrom tensorflow.keras.layers import Activation, Embedding, Concatenate, Dense, Flatten\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\ndef seed_everything(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    tf.random.set_seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n\n\nSEED = 2021\nseed_everything(SEED)","metadata":{"execution":{"iopub.status.busy":"2021-09-27T20:06:14.558097Z","iopub.execute_input":"2021-09-27T20:06:14.558418Z","iopub.status.idle":"2021-09-27T20:06:19.462432Z","shell.execute_reply.started":"2021-09-27T20:06:14.558346Z","shell.execute_reply":"2021-09-27T20:06:19.461564Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Feature Utils","metadata":{}},{"cell_type":"code","source":"# feature utils\ndef calculate_wap(df, rank=\"1\"):\n    return (df[f\"bid_price{rank}\"] * df[f\"ask_size{rank}\"] + df[f\"bid_size{rank}\"] * df[f\"ask_price{rank}\"]) / (\n                df[f\"bid_size{rank}\"] + df[f\"ask_size{rank}\"])\n\n\ndef calculate_agg_wap(df):\n    wap1 = df[\"bid_price1\"] * df[\"ask_size1\"] + df[\"bid_size1\"] * df[\"ask_price1\"]\n    wap2 = df[\"bid_price2\"] * df[\"ask_size2\"] + df[\"bid_size2\"] * df[\"ask_price2\"]\n    den = df[\"ask_size1\"] + df[\"ask_size2\"] + df[\"bid_size1\"] + df[\"bid_size2\"]\n    \n    return (wap1 + wap2)/den\n    pass\n\n\ndef calculate_inter_wap(df, rank=\"1\"):\n    return (df[f\"bid_price{rank}\"] * df[f\"bid_size{rank}\"] + df[f\"ask_size{rank}\"] * df[f\"ask_price{rank}\"]) / (\n            df[f\"bid_size{rank}\"] + df[f\"ask_size{rank}\"])\n    pass\n\ndef calculate_agg_inter_wap(df):\n    iwap1 = df[\"bid_price1\"] * df[\"bid_size1\"] + df[\"ask_size1\"] * df[\"ask_price1\"]\n    iwap2 = df[\"bid_price2\"] * df[\"bid_size2\"] + df[\"ask_size2\"] * df[\"ask_price2\"]\n    den = df[\"ask_size1\"] + df[\"ask_size2\"] + df[\"bid_size1\"] + df[\"bid_size2\"]\n    \n    return (iwap1 + iwap2)/den\n    pass\n\n\ndef calc_depth(df):\n    depth = df['bid_price1'] * df['bid_size1'] + df['ask_price1'] * df['ask_size1'] + df['bid_price2'] * df[\n               'bid_size2'] + df['ask_price2'] * df['ask_size2']\n    return depth\n\n\ndef calc_slope(df):\n    v0 = (df['bid_size1']+df['ask_size1'])/2\n    p0 = (df['bid_price1']+df['ask_price1'])/2\n    slope_bid = ((df['bid_size1']/v0)-1)/abs((df['bid_price1']/p0)-1)+(\n                (df['bid_size2']/df['bid_size1'])-1)/abs((df['bid_price2']/df['bid_price1'])-1)\n    slope_ask = ((df['ask_size1']/v0)-1)/abs((df['ask_price1']/p0)-1)+(\n                (df['ask_size2']/df['ask_size1'])-1)/abs((df['ask_price2']/df['ask_price1'])-1)\n    return (slope_bid+slope_ask)/2, abs(slope_bid-slope_ask)\n\n\ndef calc_dispersion(df):\n    bspread = df['bid_price1'] - df['bid_price2']\n    aspread = df['ask_price2'] - df['ask_price1']\n    bmid = (df['bid_price1'] + df['ask_price1'])/2  - df['bid_price1']\n    bmid2 = (df['bid_price1'] + df['ask_price1'])/2  - df['bid_price2']\n    amid = df['ask_price1'] - (df['bid_price1'] + df['ask_price1'])/2\n    amid2 = df['ask_price2'] - (df['bid_price1'] + df['ask_price1'])/2\n    bdisp = (df['bid_size1']*bmid + df['bid_size2']*bspread)/(df['bid_size1']+df['bid_size2'])\n    bdisp2 = (df['bid_size1']*bmid + df['bid_size2']*bmid2)/(df['bid_size1']+df['bid_size2'])\n    adisp = (df['ask_size1']*amid + df['ask_size2']*aspread)/(df['ask_size1']+df['ask_size2'])      \n    adisp2 = (df['ask_size1']*amid + df['ask_size2']*amid2)/(df['ask_size1']+df['ask_size2'])\n    return (bdisp + adisp)/2, (bdisp2 + adisp2)/2\n\ndef calc_price_impact(df):\n    ask = (df['ask_price1'] * df['ask_size1'] + df['ask_price2'] * df['ask_size2'])/(df['ask_size1']+df['ask_size2'])\n    bid = (df['bid_price1'] * df['bid_size1'] + df['bid_price2'] * df['bid_size2'])/(df['bid_size1']+df['bid_size2'])\n    return (df['ask_price1'] - ask)/df['ask_price1'], (df['bid_price1'] - bid)/df['bid_price1']\n\n\ndef calc_ofi(df):\n    a = df['bid_size1']*np.where(df['bid_price1'].diff()>=0,1,0)\n    b = df['bid_size1'].shift()*np.where(df['bid_price1'].diff()<=0,1,0)\n    c = df['ask_size1']*np.where(df['ask_price1'].diff()<=0,1,0)\n    d = df['ask_size1'].shift()*np.where(df['ask_price1'].diff()>=0,1,0)\n    return a - b - c + d\n\n\ndef calc_tt1(df):\n    p1 = df['ask_price1'] * df['ask_size1'] + df['bid_price1'] * df['bid_size1']\n    p2 = df['ask_price2'] * df['ask_size2'] + df['bid_price2'] * df['bid_size2']      \n    return p2 - p1 \n\n\ndef calculate_log_return(series):\n    return np.log(series).diff()\n\n\ndef calculate_rv(series):\n    return np.sqrt(np.sum(np.square(series)))\n\n    \n# Calculate integrated quarticity\ndef calculate_rv_quarticity(series):\n    return (series.count()/3)*np.sum(series**4)\n\n# Calculate weighted volatility\ndef calculate_rv_vol_weighted(series):\n    return np.sqrt(np.sum(series**2)/series.count())\n\n\ndef count_unique(series):\n    return len(np.unique(series))\n\n\ndef get_stats_window(df, seconds_in_bucket, features_dict, add_suffix=False):\n    df_feature = df[df[\"seconds_in_bucket\"] >= seconds_in_bucket].groupby([\"time_id\"]).agg(features_dict).reset_index()\n    df_feature.columns = [\"_\".join(col) for col in df_feature.columns]\n\n    if add_suffix:\n        df_feature = df_feature.add_suffix(\"_\" + str(seconds_in_bucket))\n\n    return df_feature\n    pass\n\n\ndef window_stats(df, feature_dict, feature_dict_time, second_windows, additional_dfs=None):\n    df_merged = get_stats_window(df, seconds_in_bucket=0, features_dict=feature_dict)\n\n    if additional_dfs is not None:\n        df_merged = df_merged.merge(additional_dfs, how='left', left_on='time_id_', right_on='time_id')\n\n    temp_dfs = []\n    for window in second_windows:\n        temp_dfs.append(\n            (window,\n             get_stats_window(df, seconds_in_bucket=window, features_dict=feature_dict_time, add_suffix=True)\n             )\n        )\n\n    for window, temp_df in temp_dfs:\n        df_merged = df_merged.merge(temp_df, how=\"left\", left_on=\"time_id_\", right_on=f\"time_id__{window}\")\n        df_merged.drop(columns=[f\"time_id__{window}\"], inplace=True)\n\n    return df_merged\n    pass\n\n\ndef tendency(price, vol):\n    diff = np.diff(price)\n    val = (diff / price[1:]) * 100\n    power = np.sum(val * vol[1:])\n    return power\n    pass\n\n\ndef get_stock_clusters(df, n_clusters=6):\n    pivoted_data = df.pivot(index=\"time_id\", columns=[\"stock_id\"], values=\"target\")\n    corr_pivoted = pivoted_data.corr()\n\n    clusters = KMeans(n_clusters, random_state=cfg.random_state).fit(corr_pivoted.values)\n\n    groups = []\n    for i in range(n_clusters):\n        groups.append([x-1] for x in (corr_pivoted.index+1)*(clusters.labels_ == i) if x > 0)\n    return groups\n    pass\n\n\ndef create_cluster_aggregations(df, groups):\n    feats = []\n\n    for i, idx in enumerate(groups):\n        chunk_df = df.loc[df['stock_id'].isin(idx)]\n        chunk_df = chunk_df.groupby(['time_id']).agg(np.nanmean)\n        chunk_df.loc[:, 'stock_id'] = str(i) + 'c1'\n        feats.append(chunk_df)\n\n    feats = pd.concat(feats).reset_index()\n    if \"target\" in feats.columns:\n        feats.drop(columns=['target'], inplace=True)\n\n    feats = feats.pivot(index='time_id', columns='stock_id')\n    feats.columns = [\"_\".join(x) for x in feats.columns.ravel()]\n    feats.reset_index(inplace=True)\n\n    return pd.merge(df, feats, how=\"left\", on=\"time_id\")\n    pass","metadata":{"execution":{"iopub.status.busy":"2021-09-27T20:06:33.023557Z","iopub.execute_input":"2021-09-27T20:06:33.023903Z","iopub.status.idle":"2021-09-27T20:06:33.201984Z","shell.execute_reply.started":"2021-09-27T20:06:33.023873Z","shell.execute_reply":"2021-09-27T20:06:33.200937Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Config","metadata":{}},{"cell_type":"code","source":"# config\nclass cfg:\n    \n    paths = {\n        # train path\n        \"train_csv\"  : \"../input/optiver-realized-volatility-prediction/train.csv\",\n        \"train_book\" : \"../input/optiver-realized-volatility-prediction/book_train.parquet\",\n        \"train_trade\": \"../input/optiver-realized-volatility-prediction/trade_train.parquet\",\n\n        # test path\n        \"test_csv\"   : \"../input/optiver-realized-volatility-prediction/test.csv\",\n        \"test_book\"  : \"../input/optiver-realized-volatility-prediction/book_test.parquet\",\n        \"test_trade\" : \"../input/optiver-realized-volatility-prediction/trade_test.parquet\",\n        \n        # model paths\n        \"dae\": \"./dae\"\n    }\n\n    feature_dict_book = {\n        \"seconds_in_bucket\": [count_unique],\n        \"wap1\":              [np.sum, np.mean, np.std, np.max],\n        \"wap2\":              [np.sum, np.mean, np.std, np.max],\n        \"wap_agg\":           [np.sum, np.mean, np.std, np.max],\n        \n        \"iwap1\":             [np.sum, np.mean, np.std, np.max],\n        \"iwap2\":             [np.sum, np.mean, np.std, np.max],\n        \"iwap_agg\":          [np.sum, np.mean, np.std, np.max],\n        \n        \"log_return1\":       [np.sum, calculate_rv, calculate_rv_quarticity, calculate_rv_vol_weighted, np.mean, np.std],\n        \"log_return2\":       [np.sum, calculate_rv, calculate_rv_quarticity, calculate_rv_vol_weighted, np.mean, np.std],\n        'log_return_agg':    [np.sum, calculate_rv, calculate_rv_quarticity, calculate_rv_vol_weighted, np.mean, np.std],\n        \n        \"inter_log_return1\": [np.sum, calculate_rv, calculate_rv_quarticity, calculate_rv_vol_weighted, np.mean, np.std],\n        \"inter_log_return2\": [np.sum, calculate_rv, calculate_rv_quarticity, calculate_rv_vol_weighted, np.mean, np.std],\n        'inter_log_return_agg': [np.sum, calculate_rv, calculate_rv_quarticity, calculate_rv_vol_weighted, np.mean, np.std],\n        \n        \"wap_balance\":       [np.sum, np.mean, np.std, np.max],\n        \"volume_imbalance\":  [np.sum, np.mean, np.std, np.max],\n        \"total_volume\":      [np.sum, np.mean, np.std, np.max],\n        \n        \"price_spread1\":     [np.sum, np.mean, np.std, np.max],\n        \"price_spread2\":     [np.sum, np.mean, np.std, np.max],\n        \"bid_spread\":        [np.sum, np.mean, np.std, np.max],\n        \"ask_spread\":        [np.sum, np.mean, np.std, np.max],\n        \n        'depth':             [np.sum, np.mean, np.std, np.max],\n        'slope':             [np.sum, np.mean, np.std, np.max],\n        'dispersion':        [np.sum, np.mean, np.std, np.max],\n        'price_impact':      [np.sum, np.mean, np.std, np.max],\n        'ofi':               [np.sum, np.mean, np.std, np.max],\n        'turn_over':         [np.sum, np.mean, np.std, np.max],\n    }\n\n    feature_dict_book_time = {        \n        \"log_return1\":       [calculate_rv, calculate_rv_quarticity, calculate_rv_vol_weighted],\n        \"log_return2\":       [calculate_rv, calculate_rv_quarticity, calculate_rv_vol_weighted],\n        \"log_return_agg\":    [calculate_rv, calculate_rv_quarticity, calculate_rv_vol_weighted],\n        \n        \"inter_log_return1\": [calculate_rv, calculate_rv_quarticity, calculate_rv_vol_weighted],\n        \"inter_log_return2\": [calculate_rv, calculate_rv_quarticity, calculate_rv_vol_weighted],\n        \"inter_log_return_agg\": [calculate_rv, calculate_rv_quarticity, calculate_rv_vol_weighted],\n    }\n\n    feature_dict_trade = {\n        'seconds_in_bucket': [count_unique],       \n        'log_return':        [np.sum, calculate_rv, calculate_rv_quarticity, calculate_rv_vol_weighted, np.mean, np.std],\n        'size':              [np.sum, np.mean, np.std, np.max],\n        'order_count':       [np.sum, np.mean, np.std, np.max],\n        'amount':            [np.sum, np.mean, np.std, np.max],\n    }\n    \n    feature_dict_trade_time = {\n        'log_return':        [calculate_rv, calculate_rv_quarticity, calculate_rv_vol_weighted],\n        'seconds_in_bucket': [count_unique],\n        'size':              [np.sum, np.std],\n        'order_count':       [np.sum, np.std],\n        'amount':            [np.sum, np.std],\n    }\n    \n    bucket_windows = [100, 200, 300, 400, 500]\n    random_state = SEED\n    pass","metadata":{"execution":{"iopub.status.busy":"2021-09-27T20:06:40.007271Z","iopub.execute_input":"2021-09-27T20:06:40.007606Z","iopub.status.idle":"2021-09-27T20:06:40.025617Z","shell.execute_reply.started":"2021-09-27T20:06:40.007575Z","shell.execute_reply":"2021-09-27T20:06:40.024662Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Create Features","metadata":{}},{"cell_type":"code","source":"# order book features\ndef get_book_features(file_path):\n    book_df = pd.read_parquet(file_path)\n\n    # calculate wap\n    book_df['wap1'] = calculate_wap(book_df, rank=\"1\")\n    book_df['wap2'] = calculate_wap(book_df, rank=\"2\")\n    book_df[\"wap_agg\"] = calculate_agg_wap(book_df)\n    \n    book_df['iwap1'] = calculate_inter_wap(book_df, rank=\"1\")\n    book_df['iwap2'] = calculate_inter_wap(book_df, rank=\"2\")\n    book_df[\"iwap_agg\"] = calculate_agg_inter_wap(book_df)\n\n    # calculate log return\n    book_df[\"log_return1\"] = book_df.groupby([\"time_id\"])[\"wap1\"].apply(calculate_log_return)\n    book_df[\"log_return2\"] = book_df.groupby([\"time_id\"])[\"wap2\"].apply(calculate_log_return)\n    book_df[\"log_return_agg\"] = book_df.groupby([\"time_id\"])[\"wap_agg\"].apply(calculate_log_return)\n    \n    book_df[\"inter_log_return1\"] = book_df.groupby([\"time_id\"])[\"iwap1\"].apply(calculate_log_return)\n    book_df[\"inter_log_return2\"] = book_df.groupby([\"time_id\"])[\"iwap2\"].apply(calculate_log_return)\n    book_df[\"inter_log_return_agg\"] = book_df.groupby([\"time_id\"])[\"iwap_agg\"].apply(calculate_log_return)\n\n    # calculate balance\n    book_df[\"wap_balance\"] = abs(book_df[\"wap1\"] - book_df[\"wap2\"])\n    book_df[\"volume_imbalance\"] = abs(\n        (book_df[\"ask_size1\"] + book_df[\"ask_size2\"]) - (book_df[\"bid_size1\"] + book_df[\"bid_size2\"]))\n    book_df[\"total_volume\"] = book_df[\"ask_size1\"] + book_df[\"ask_size2\"] + book_df[\"bid_size1\"] + book_df[\n        \"bid_size2\"]\n\n    # calculate spread\n    book_df[\"price_spread1\"] = (book_df[\"ask_price1\"] - book_df[\"bid_price1\"]) / (\n            (book_df[\"ask_price1\"] + book_df[\"bid_price1\"]) / 2)\n    book_df[\"price_spread2\"] = (book_df[\"ask_price2\"] - book_df[\"bid_price2\"]) / (\n            (book_df[\"ask_price2\"] + book_df[\"bid_price2\"]) / 2)\n    book_df[\"bid_spread\"] = book_df[\"bid_price1\"] - book_df[\"bid_price2\"]\n    book_df[\"ask_spread\"] = book_df[\"ask_price1\"] - book_df[\"ask_price2\"]\n    \n    book_df[\"depth\"] = calc_depth(book_df)\n    book_df[\"slope\"], _ = calc_slope(book_df)\n    book_df[\"dispersion\"], _ = calc_dispersion(book_df)\n    book_df[\"price_impact\"], _ = calc_price_impact(book_df)\n    book_df[\"ofi\"] = calc_ofi(book_df)\n    book_df[\"turn_over\"] = calc_tt1(book_df)\n    \n    book_df_merged = window_stats(book_df, cfg.feature_dict_book, cfg.feature_dict_book_time, cfg.bucket_windows)\n\n    book_df_merged[\"row_id\"] = book_df_merged[\"time_id_\"].apply(lambda x: f\"{file_path.split('=')[1]}-{x}\")\n    book_df_merged.drop([\"time_id_\"], axis=1, inplace=True)\n\n    return book_df_merged.bfill().ffill()\n                                                                \n# trade features\ndef get_trade_price_features(df):\n    res = []\n    for n_time_id in df['time_id'].unique():\n        df_id = df[df['time_id'] == n_time_id]\n        vol_tendency = tendency(df_id['price'].values, df_id['size'].values)\n        f_max = np.sum(df_id['price'].values > np.mean(df_id['price'].values))\n        f_min = np.sum(df_id['price'].values < np.mean(df_id['price'].values))\n        df_max = np.sum(np.diff(df_id['price'].values) > 0)\n        df_min = np.sum(np.diff(df_id['price'].values) < 0)\n        abs_diff = np.median(np.abs(df_id['price'].values - np.mean(df_id['price'].values)))\n        energy = np.mean(df_id['price'].values ** 2)\n        iqr_p = np.percentile(df_id['price'].values, 75) - np.percentile(df_id['price'].values, 25)\n        abs_diff_v = np.median(np.abs(df_id['size'].values - np.mean(df_id['size'].values)))\n        energy_v = np.sum(df_id['size'].values ** 2)\n        iqr_p_v = np.percentile(df_id['size'].values, 75) - np.percentile(df_id['size'].values, 25)\n\n        res.append({'time_id': n_time_id,\n                    'tendency': vol_tendency,\n                    'f_max': f_max,\n                    'f_min': f_min,\n                    'df_max': df_max,\n                    'df_min': df_min,\n                    'abs_diff': abs_diff,\n                    'energy': energy,\n                    'iqr_p': iqr_p,\n                    'abs_diff_v': abs_diff_v,\n                    'energy_v': energy_v,\n                    'iqr_p_v': iqr_p_v})\n\n    return pd.DataFrame(res)\n    pass\n\n\ndef tau_features(df, sec, weight):\n    tau_feat = 'tau_' + str(sec)\n    bucket_col = 'trade_seconds_in_bucket_count_unique_' + str(sec)\n    df[tau_feat] = np.sqrt(weight/df[bucket_col])\n\n    size_feat = 'size_' + str(sec)\n    order_col = 'trade_order_count_sum_' + str(sec)\n    df[size_feat] = np.sqrt(weight/df[order_col])\n\n    return df\n    pass\n\n\ndef get_trade_features(file_path, buck_windows=cfg.bucket_windows):\n    trade_df = pd.read_parquet(file_path)\n\n    trade_df[\"log_return\"] = trade_df.groupby([\"time_id\"])[\"price\"].apply(calculate_log_return)\n    trade_df[\"amount\"] = trade_df[\"size\"] * trade_df[\"price\"]\n\n    price_features = get_trade_price_features(trade_df)\n    trade_df_merged = window_stats(trade_df, cfg.feature_dict_trade, cfg.feature_dict_trade_time, buck_windows, additional_dfs=price_features)\n\n    trade_df_merged = trade_df_merged.add_prefix(\"trade_\")\n\n    trade_df_merged[\"row_id\"] = trade_df_merged[\"trade_time_id_\"].apply(lambda x: f\"{file_path.split('=')[1]}-{x}\")\n    trade_df_merged.drop([\"trade_time_id_\"], axis=1, inplace=True)\n\n    for sec in buck_windows:\n        trade_df_merged = tau_features(trade_df_merged, sec, weight=sec/600)\n    return trade_df_merged.bfill().ffill() ","metadata":{"execution":{"iopub.status.busy":"2021-09-27T20:06:44.107351Z","iopub.execute_input":"2021-09-27T20:06:44.10768Z","iopub.status.idle":"2021-09-27T20:06:44.134783Z","shell.execute_reply.started":"2021-09-27T20:06:44.107653Z","shell.execute_reply":"2021-09-27T20:06:44.133659Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Create Dataset","metadata":{}},{"cell_type":"code","source":"# create dataset\nclass GetData:\n    def __init__(self, df, book_path, trade_path, is_train=True):\n        self.df = df.copy(deep=True)\n        self.order_book_path = book_path\n        self.trade_path = trade_path\n        self.is_train = is_train\n\n        self._get_rowid()\n\n    def _get_rowid(self):\n        self.df[\"row_id\"] = self.df[\"stock_id\"].astype(str) + \"-\" + self.df[\"time_id\"].astype(str)\n\n    def get_time_stock(self, buck_windows=cfg.bucket_windows):\n        vol_cols = []\n        feat_set = ['log_return1_calculate_rv', 'log_return2_calculate_rv', 'log_return_agg_calculate_rv', 'trade_log_return_calculate_rv']\n        for feat in feat_set:\n            for sec in buck_windows:\n                vol_cols.append(feat + f'_{sec}')\n        vol_cols += feat_set\n\n        df_stock_id = self.df.groupby(['stock_id'])[vol_cols].agg(['mean', 'std', 'max', 'min']).reset_index()\n        df_stock_id.columns = ['_'.join(col) for col in df_stock_id.columns]\n        df_stock_id = df_stock_id.add_suffix('_' + 'stock')\n\n        df_time_id = self.df.groupby(['time_id'])[vol_cols].agg(['mean', 'std', 'max', 'min']).reset_index()\n        df_time_id.columns = ['_'.join(col) for col in df_time_id.columns]\n        df_time_id = df_time_id.add_suffix('_' + 'time')\n\n        # Merge with original dataframe\n        self.df = self.df.merge(df_stock_id, how='left', left_on=['stock_id'], right_on=['stock_id__stock'])\n        self.df = self.df.merge(df_time_id, how='left', left_on=['time_id'], right_on=['time_id__time'])\n        self.df.drop(['stock_id__stock', 'time_id__time'], axis=1, inplace=True)\n        return self.df\n\n    def process_features(self, list_stock_ids):\n        def parallel_helper(stock_id):\n            book_sample_path = os.path.join(self.order_book_path, f\"stock_id={stock_id}\")\n            trade_sample_path = os.path.join(self.trade_path, f\"stock_id={stock_id}\")\n\n            return pd.merge(get_book_features(book_sample_path), get_trade_features(trade_sample_path),\n                            on=\"row_id\",\n                            how=\"left\")\n\n        df = Parallel(n_jobs=-1, verbose=1)(delayed(parallel_helper)(stock_id) for stock_id in list_stock_ids)\n        df = pd.concat(df, ignore_index=True)\n\n        return df\n\n    def _get_features(self):\n        features_df = self.process_features(self.df[\"stock_id\"].unique())\n        self.df = self.df.merge(features_df, on=[\"row_id\"], how=\"left\")\n\n        return self.get_time_stock()\n        pass\n\n    def get_all_features(self, stock_groups):\n        return create_cluster_aggregations(self._get_features(), stock_groups)\n        pass","metadata":{"execution":{"iopub.status.busy":"2021-09-27T20:06:47.073105Z","iopub.execute_input":"2021-09-27T20:06:47.073459Z","iopub.status.idle":"2021-09-27T20:06:47.087776Z","shell.execute_reply.started":"2021-09-27T20:06:47.073428Z","shell.execute_reply":"2021-09-27T20:06:47.086708Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Modeling","metadata":{}},{"cell_type":"code","source":"def swish(x, beta = 1):\n    return (x * K.sigmoid(beta * x))\n\ndef rmspe(y_true, y_pred):\n    return np.sqrt(np.mean(np.square((y_true-y_pred)/y_true)))\n    pass\n\ndef root_mean_squared_per_error(y_true, y_pred):\n         return K.sqrt(K.mean(K.square((y_true-y_pred)/y_true)))\n\n# add as activation\nget_custom_objects().update({'swish': Activation(swish)})\n\ndef create_autoencoder(input_dim, output_dim, noise=0.05):\n    inp = tf.keras.layers.Input(shape=(input_dim, ))\n    encoded = tf.keras.layers.GaussianNoise(noise)(inp)\n    encoded = tf.keras.layers.Dense(32, activation='swish')(encoded)\n    \n    decoded = tf.keras.layers.Dropout(0.2)(encoded)\n    decoded = tf.keras.layers.Dense(input_dim,name='decoded')(decoded)\n    \n    x = tf.keras.layers.Dense(16, activation='swish')(decoded)\n    x = tf.keras.layers.BatchNormalization()(x)\n    x = tf.keras.layers.Dropout(0.2)(x)\n    x = tf.keras.layers.Dense(output_dim, activation='linear', name='label_output')(x)\n    \n    encoder = tf.keras.models.Model(inputs=inp, outputs=encoded)\n    autoencoder = tf.keras.models.Model(inputs=inp, outputs=[decoded, x])\n    \n    autoencoder.compile(optimizer=tf.keras.optimizers.Adam(0.001), \n                        loss={'decoded':'mse',\n                              'label_output':root_mean_squared_per_error})\n    return autoencoder, encoder\n\ndef base_model(input_dim, output_dim, encoder):\n    stock_id_input = tf.keras.Input(shape=(487,), name='stock_id')\n    inputs = tf.keras.Input(shape=(input_dim,), name='num_data')\n    \n    x = encoder(stock_id_input)\n    x = tf.keras.layers.Concatenate()([x, inputs])\n    x = tf.keras.layers.BatchNormalization()(x)\n\n    x = tf.keras.layers.Dense(4096, activation='swish')(x)\n    x = tf.keras.layers.Reshape((256, 16))(x)\n    x = tf.keras.layers.Conv1D(filters=16,\n                               kernel_size=7,\n                               strides=1,\n                               activation='swish')(x)\n    x = tf.keras.layers.MaxPooling1D(pool_size=2)(x)\n    x = tf.keras.layers.Flatten()(x)\n    \n\n    for i in range(2):\n        x = tf.keras.layers.Dense(256//(2 ** i), activation='swish')(x)\n        x = tf.keras.layers.BatchNormalization()(x)\n        x = tf.keras.layers.GaussianNoise(0.01)(x)\n        x = tf.keras.layers.Dropout(0.2)(x)\n    x = tf.keras.layers.Dense(output_dim, activation='linear')(x)\n    \n    model = tf.keras.models.Model(inputs=[stock_id_input, inputs], outputs=x)\n    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n                  loss=root_mean_squared_per_error, \n                  metrics=root_mean_squared_per_error)\n    \n    return model\n\n\ndef encode_stocks(df):\n    stock_encoder = LabelEncoder()\n    df[\"stock_id\"] = stock_encoder.fit_transform(df[\"stock_id\"])\n    pickle.dump(stock_encoder, open(\"./label_stocks_global.pkl\", \"wb\"))\n    return df\n    pass\n\ndef get_quantile_transform(train_df, test_df=None):\n    print(\"[INFO] Applying Quantile Transformation...\")\n    for col in tqdm(train_df.columns):\n        if col==\"stock_id\":\n            continue\n        qt = QuantileTransformer(random_state=21, n_quantiles=2000, output_distribution='normal')\n        train_df[col] = qt.fit_transform(train_df[[col]])\n        if test_df is not None: test_df[col] = qt.transform(test_df[[col]])\n            \n        del qt\n        _ = gc.collect()\n        \n    return train_df, test_df\n    pass\n\n\ndef get_transform(df, val_df, name, file_path=None):\n    print(f\"[INFO] Using {name} scaler...\\n\")\n    if name==\"mm\":\n        for col in tqdm(df.columns):\n            if col==\"stock_id\":\n                continue\n            scaler = MinMaxScaler()\n            df[col] = scaler.fit_transform(df[[col]])\n            if val_df is not None: val_df[col] = scaler.transform(val_df[[col]])\n            \n    elif name==\"mm_11\":\n        for col in tqdm(df.columns):\n            if col==\"stock_id\":\n                continue\n            scaler = MinMaxScaler(feature_range=(-1, 1))\n            df[col] = scaler.fit_transform(df[[col]])\n            if val_df is not None: val_df[col] = scaler.transform(val_df[[col]])\n    else:\n        for col in tqdm(df.columns):\n            if col==\"stock_id\":\n                continue\n            scaler = StandardScaler()\n            df[col] = scaler.fit_transform(df[[col]])\n            if val_df is not None: val_df[col] = scaler.transform(val_df[[col]])\n    \n    return df, val_df\n    pass\n\nclass TrainFer:\n    def __init__(self, n_splits, model_path, random_state):\n        self.n_splits = n_splits\n        self.random_state = random_state\n        self.model_path = model_path\n        if not os.path.isdir(model_path):\n            os.makedirs(model_path)\n            \n    \n    def get_encoder(self, X, y, scaler_name):\n        if os.path.exists(\"../input/dataorvpprocessed/encoder.h5\"):\n            print(\"[INFO] Loading trained Encoder...\")\n            _, encoder = create_autoencoder(487, 1, noise=0.2)\n            encoder.load_weights(\"../input/dataorvpprocessed/encoder.h5\")\n         \n        else:\n            X, _ = get_quantile_transform(X)\n            X, _ = get_transform(X, None, scaler_name, f\"./fulldata_{scaler_name}.pkl\")\n\n            autoencoder, encoder = create_autoencoder(487, 1, noise=0.2)\n            autoencoder.fit(X,\n                            (X, y),\n                            epochs=100,\n                            batch_size=512,\n                            validation_split=0.2,\n                            callbacks=[tf.keras.callbacks.EarlyStopping('val_loss', patience=15, restore_best_weights=True),\n                                      tf.keras.callbacks.ModelCheckpoint(os.path.join(self.model_path, \"dae.h5\"), save_weights_only=True),\n                                      tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=7, verbose=1, mode='min')])\n            encoder.save_weights(os.path.join(self.model_path, \"encoder.h5\"))\n            \n            del X, y, autoencoder\n            _ = gc.collect()   \n            \n        return encoder\n        pass\n    \n    def train(self, X, y, callback_list, scaler_name=\"mm\"):\n        X = encode_stocks(X)\n        encoder = self.get_encoder(X, y, scaler_name)\n        encoder.trainable = False\n        \n        oof_predictions = np.zeros(X.shape[0])\n        kfold = KFold(n_splits=self.n_splits, random_state=0, shuffle=True)\n        oof_scores = []\n\n        for fold, (train_idx, val_idx) in enumerate(kfold.split(X)):\n            print(f\"\\nFold - {fold}\\n\")\n\n            x_train, y_train = X.iloc[train_idx], y.iloc[train_idx]\n            x_val, y_val = X.iloc[val_idx], y.iloc[val_idx]\n            \n            x_train, x_val = get_quantile_transform(x_train, x_val)\n            x_train, x_val = get_transform(x_train, x_val, scaler_name)\n            \n            model = base_model(486, 1, encoder)\n            model.compile(\n                tf.keras.optimizers.Adam(learning_rate=0.005),\n                loss=root_mean_squared_per_error\n            )\n        \n            cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=os.path.join(self.model_path, f\"nnse_{fold}.h5\"),\n                                                             save_weights_only=True,\n                                                             verbose=0)\n            \n            model.fit([x_train, x_train.drop([\"stock_id\"], axis=1)],\n                      y_train,              \n                      batch_size=1024,\n                      epochs=200,\n                      validation_data=([x_val, x_val.drop([\"stock_id\"], axis=1)], y_val),\n                      callbacks=callback_list+[cp_callback],\n                      validation_batch_size=len(y_val),\n                      shuffle=True,\n                      verbose=1)\n            \n            fold_preds = model.predict([x_val, x_val.drop([\"stock_id\"], axis=1)])[:, 0]\n            oof_score = rmspe(y_val, fold_preds)\n            print(f\"\\nRMSPE of fold {fold}: {oof_score}\")\n            \n            oof_scores.append(oof_score)\n            oof_predictions[val_idx] = fold_preds\n            \n            del x_train, x_val, y_train, y_val, model, fold_preds\n            _ = gc.collect()\n        \n        print(f\"\\nOOF Scores: {oof_scores}\\n\")\n        rmspe_score = rmspe(y, oof_predictions)\n        print(f\"OOF RMSPE: {rmspe_score}\")\n        \n        return y, oof_predictions","metadata":{"execution":{"iopub.status.busy":"2021-09-27T20:06:48.620153Z","iopub.execute_input":"2021-09-27T20:06:48.620531Z","iopub.status.idle":"2021-09-27T20:06:48.671863Z","shell.execute_reply.started":"2021-09-27T20:06:48.620498Z","shell.execute_reply":"2021-09-27T20:06:48.671037Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train","metadata":{}},{"cell_type":"code","source":"if __name__ == \"__main__\":\n    _ = gc.collect()\n    \n    train_feats = pickle.load(open(\"../input/dataorvpprocessed/train_df.pkl\", \"rb\"))\n    train_feats.fillna(-1, inplace=True)\n    reg = TrainFer(n_splits=5, model_path=cfg.paths[\"dae\"], random_state=cfg.random_state) \n\n    es = tf.keras.callbacks.EarlyStopping(\n        monitor='val_loss',\n        patience=20,\n        verbose=1,\n        mode='min',\n        restore_best_weights=True)\n\n    plateau = tf.keras.callbacks.ReduceLROnPlateau(\n        monitor='val_loss',\n        factor=0.3,\n        patience=7,\n        verbose=1,\n        mode='min')\n    \n    callback_list = [es, plateau]\n    y_targets, oof_preds = reg.train(train_feats.drop(columns=[\"row_id\", \"target\", \"time_id\"]), train_feats[\"target\"], callback_list, \"mm\")\n    pickle.dump(y_targets, open(\"./y_targets_mm.pkl\", \"wb\"))\n    pickle.dump(oof_preds, open(\"./oof_preds_mm.pkl\", \"wb\"))\n    pass","metadata":{"execution":{"iopub.status.busy":"2021-09-27T20:07:02.041785Z","iopub.execute_input":"2021-09-27T20:07:02.042109Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"EOF","metadata":{}}]}