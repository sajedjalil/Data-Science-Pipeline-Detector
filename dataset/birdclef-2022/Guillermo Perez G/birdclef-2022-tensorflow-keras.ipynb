{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## **INTRODUCTION**\n\n### * Goal: In this notebook, I’ll use tensor flow and keras skills to identify bird species by sound. Specifically, I'll develop a model that can process continuous audio data and then acoustically recognize the species. I'll help advance the science of bioacoustics and support ongoing research to protect endangered birds.\n### * This notebook is part of the BirdCLEF 2022 contest sponsored by Cornell Lab of Ornithology, and will be evaluated on 21 classes there's not much data for these 21 classes - just 1266 entries (based on primary_label). \n\n## **ALTERNATIVE SOLUTION**\n\n### Strategy:\n### 1) Filter and remove audios without information. Eg: XC182414.ogg.\n![XC182414](https://i.pinimg.com/originals/58/13/d2/5813d2f6fd6832aa64fb67a78111b175.jpg)\n### 2) Filter and clean audios with too much noise. Eg: XC663738.ogg.\n![XC663738](https://i.pinimg.com/originals/88/f3/1e/88f31e47b1b57979cc9fa3dabc5d5e44.jpg)\n### 3) Filter and establish the most common song of each bird, since these audios have mixtures of two or three songs, so its impossible for the best model to work properly. Eg: XC644916.ogg.\n![XC644916](https://i.pinimg.com/originals/a9/e1/20/a9e12057a653959a60323c7ad649abf5.jpg)\n### 4) Develop a model with tensor flow and keras in kaggle, starting with two classes. Then we will progressively increase the classes until we reach the 21 requested classes.\n### 5) Verify our model with another AI provider. Eg, I have done tests with Edge Impulse AI provider (https://www.edgeimpulse.com/) and I have achieved an accuracy of 96.9% with bird audio and two classes as shown below:\n![Ede Impulse test](https://i.pinimg.com/originals/de/a3/98/dea398c65ecd5332aef56aa2c518fb56.jpg)","metadata":{}},{"cell_type":"markdown","source":"## **LIBRARIES**\n### Installing dependencies.\n","metadata":{}},{"cell_type":"code","source":"import os\nimport json\nimport tqdm\nimport librosa\nimport librosa.display\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom PIL import Image\nimport plotly.express as px\nimport IPython.display as ipd\nimport matplotlib.pyplot as plt","metadata":{"execution":{"iopub.status.busy":"2022-04-12T20:30:46.440814Z","iopub.execute_input":"2022-04-12T20:30:46.441416Z","iopub.status.idle":"2022-04-12T20:30:50.338933Z","shell.execute_reply.started":"2022-04-12T20:30:46.441323Z","shell.execute_reply":"2022-04-12T20:30:50.338142Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Library downloaded from: https://www.wheelodex.org/projects/noisereduce/","metadata":{}},{"cell_type":"code","source":"!pip install ../input/noisereduce/noisereduce-2.0.0-py3-none-any.whl","metadata":{"execution":{"iopub.status.busy":"2022-04-12T20:30:50.34051Z","iopub.execute_input":"2022-04-12T20:30:50.341436Z","iopub.status.idle":"2022-04-12T20:31:18.617402Z","shell.execute_reply.started":"2022-04-12T20:30:50.341397Z","shell.execute_reply":"2022-04-12T20:31:18.616619Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **LOAD DATA**\n\n### According Cornell Lab, many bird songs have frequency ranges between 1,000 Hz and 8,000 Hz. In addition, the frequency response of a common microphone is between 100 to 10,000 Hz. \nhttps://www.allaboutbirds.org/news/do-bird-songs-have-frequencies-higher-than-humans-can-hear/#:~:text=Many%20bird%20songs%20have%20frequency,reach%208%2C000%20Hz%20and%20beyond.","metadata":{}},{"cell_type":"code","source":"seed = 42\nos.environ['PYTHONHASHSEED'] = str(seed)\nnp.random.seed(seed)\n\nDURATION = 15\nSPEC_SHAPE = (48, 128)\nSAMPLE_RATE = 32000\nTEST_DURATION = 5\nSPEC_SHAPE = (48, 128)\nFMIN = 500\nFMAX = 8500","metadata":{"execution":{"iopub.status.busy":"2022-04-12T20:31:18.619038Z","iopub.execute_input":"2022-04-12T20:31:18.619307Z","iopub.status.idle":"2022-04-12T20:31:18.627262Z","shell.execute_reply.started":"2022-04-12T20:31:18.61927Z","shell.execute_reply":"2022-04-12T20:31:18.626571Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"main_dir = '../input/birdclef-2022'\ntrain_audio_dir = main_dir+'/train_audio'\ntest_audio_dir = main_dir+'/test_soundscapes'\ntrain = pd.read_csv(main_dir+'/train_metadata.csv')\ntrain['time_dt'] = pd.to_datetime(train['time'], errors='coerce')\ntrain['time_dt'] = train['time_dt'].dt.round('30min')\ntrain['time_H_M'] = train['time_dt'].dt.strftime('%H:%M')\ntrain['secondary_label_len'] = train.secondary_labels.apply(lambda x:len(x.split(','))) \ntest = pd.read_csv(main_dir+'/test.csv') \nsubmission = pd.read_csv(main_dir+'/sample_submission.csv')\ntaxonomy = pd.read_csv(main_dir+'/eBird_Taxonomy_v2021.csv')\nscored_birds = json.load(open(main_dir+'/scored_birds.json', 'r'))","metadata":{"execution":{"iopub.status.busy":"2022-04-12T20:31:18.629748Z","iopub.execute_input":"2022-04-12T20:31:18.630402Z","iopub.status.idle":"2022-04-12T20:31:18.992586Z","shell.execute_reply.started":"2022-04-12T20:31:18.630369Z","shell.execute_reply":"2022-04-12T20:31:18.991773Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head(5)","metadata":{"execution":{"iopub.status.busy":"2022-04-12T20:31:18.993873Z","iopub.execute_input":"2022-04-12T20:31:18.994234Z","iopub.status.idle":"2022-04-12T20:31:19.019598Z","shell.execute_reply.started":"2022-04-12T20:31:18.994194Z","shell.execute_reply":"2022-04-12T20:31:19.018963Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.head(5)","metadata":{"execution":{"iopub.status.busy":"2022-04-12T20:31:19.020757Z","iopub.execute_input":"2022-04-12T20:31:19.021177Z","iopub.status.idle":"2022-04-12T20:31:19.030351Z","shell.execute_reply.started":"2022-04-12T20:31:19.021135Z","shell.execute_reply":"2022-04-12T20:31:19.029595Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission.head(5)","metadata":{"execution":{"iopub.status.busy":"2022-04-12T20:31:19.031496Z","iopub.execute_input":"2022-04-12T20:31:19.031998Z","iopub.status.idle":"2022-04-12T20:31:19.044569Z","shell.execute_reply.started":"2022-04-12T20:31:19.031958Z","shell.execute_reply":"2022-04-12T20:31:19.043674Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"taxonomy.head(5)","metadata":{"execution":{"iopub.status.busy":"2022-04-12T20:31:19.046134Z","iopub.execute_input":"2022-04-12T20:31:19.046452Z","iopub.status.idle":"2022-04-12T20:31:19.060168Z","shell.execute_reply.started":"2022-04-12T20:31:19.046414Z","shell.execute_reply":"2022-04-12T20:31:19.059168Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"There are {} no of unique classes but we will be evaluated only on {} no of classes\".format(len(train.primary_label.unique()), len(scored_birds)))","metadata":{"execution":{"iopub.status.busy":"2022-04-12T20:31:19.061781Z","iopub.execute_input":"2022-04-12T20:31:19.062064Z","iopub.status.idle":"2022-04-12T20:31:19.071391Z","shell.execute_reply.started":"2022-04-12T20:31:19.062029Z","shell.execute_reply":"2022-04-12T20:31:19.070358Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(scored_birds)","metadata":{"execution":{"iopub.status.busy":"2022-04-12T20:31:19.075291Z","iopub.execute_input":"2022-04-12T20:31:19.075823Z","iopub.status.idle":"2022-04-12T20:31:19.079781Z","shell.execute_reply.started":"2022-04-12T20:31:19.075792Z","shell.execute_reply":"2022-04-12T20:31:19.078915Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(30, 8))\nsns.countplot(data=train, x='primary_label', ax=ax, order=train['primary_label'].value_counts().index)\nplt.xticks(rotation=90);","metadata":{"execution":{"iopub.status.busy":"2022-04-12T20:31:19.081231Z","iopub.execute_input":"2022-04-12T20:31:19.081642Z","iopub.status.idle":"2022-04-12T20:31:21.052384Z","shell.execute_reply.started":"2022-04-12T20:31:19.081597Z","shell.execute_reply":"2022-04-12T20:31:21.051711Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **SPECTOGRAMS**\n### Here you can try fft values of 256, 512 or 1024","metadata":{}},{"cell_type":"code","source":"import torch\nimport torchaudio\nimport noisereduce as nr\nfrom math import ceil\n\ndef create_spectrogram(\n    fname: str,\n    reduce_noise: bool = False,\n    frame_size: int = 5,\n    frame_step: int = 2,\n    channel: int = 0,\n    device = \"cpu\",\n) -> list:\n    waveform, sample_rate = torchaudio.load(fname)\n    \n    transform = torchaudio.transforms.Spectrogram(n_fft=1024, win_length=512).to(device)\n    if reduce_noise:\n        waveform = torch.tensor(nr.reduce_noise(\n            y=waveform,\n            sr=sample_rate,\n            win_length=transform.win_length,\n            use_tqdm=False,\n            n_jobs=2,\n        ))\n    step = int(frame_step * sample_rate)\n    size = int(frame_size * sample_rate)\n    spectrograms = []\n    for i in range(ceil((waveform.size()[-1] - size) / step)):\n        begin = i * step\n        frame = waveform[channel][begin:begin + size]\n        if len(frame) < size:\n            if i == 0:\n                rep = round(float(size) / len(frame))\n                frame = frame.repeat(int(rep))\n            elif len(frame) < (size * 0.33):\n                continue\n            else:\n                frame = waveform[channel][-size:]\n        sg = transform(frame.to(device))\n        spectrograms.append(np.nan_to_num(torch.log(sg).numpy()))\n        # spectrograms.append(np.nan_to_num(sg.numpy()))\n    return spectrograms\n\n\npath_audio = os.path.join(train_audio_dir, train[\"filename\"][0])\nprint(path_audio)\nsgs = create_spectrogram(path_audio, reduce_noise=True)\n\n\nfig, axarr = plt.subplots(ncols=len(sgs), figsize=(4 * len(sgs), 4))\nfor i, sg in enumerate(sgs):\n    ax = axarr[i].imshow(sg, vmin=-50, vmax=10)\nplt.colorbar(ax)","metadata":{"execution":{"iopub.status.busy":"2022-04-12T20:31:21.053614Z","iopub.execute_input":"2022-04-12T20:31:21.053879Z","iopub.status.idle":"2022-04-12T20:31:24.041109Z","shell.execute_reply.started":"2022-04-12T20:31:21.053843Z","shell.execute_reply":"2022-04-12T20:31:24.040492Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import OneHotEncoder\n\nohe = OneHotEncoder()\nohe.fit(train.primary_label.unique().reshape(-1, 1))","metadata":{"execution":{"iopub.status.busy":"2022-04-12T20:31:24.042576Z","iopub.execute_input":"2022-04-12T20:31:24.043038Z","iopub.status.idle":"2022-04-12T20:31:24.053278Z","shell.execute_reply.started":"2022-04-12T20:31:24.043003Z","shell.execute_reply":"2022-04-12T20:31:24.052569Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **GENERATING THE TRAIN DATA**","metadata":{}},{"cell_type":"code","source":"%%time\nX = []\nY = []\nfor ul in tqdm.tqdm(train.primary_label.unique()):\n    records = train[train.primary_label==ul]\n    for r in records[['filename','primary_label','secondary_labels']].values:\n        file = r[0]\n        pl = r[1]\n        sl = r[2]\n        y = ohe.transform(np.array([pl]).reshape(-1, 1)).todense()\n        arr, sr = librosa.load(os.path.join(train_audio_dir, file), sr=SAMPLE_RATE, duration=DURATION)\n        chunks = []\n        for c_ in range(0, len(arr), (TEST_DURATION*SAMPLE_RATE)):\n            chunk = arr[c_:c_ + TEST_DURATION * SAMPLE_RATE]\n            if len(chunk) < int(TEST_DURATION * SAMPLE_RATE):\n                break\n            chunks.append(chunk)\n        y_arr = []\n        mel_chunks = []\n        for c_ in chunks:\n            hop_length = int(TEST_DURATION * SAMPLE_RATE / (SPEC_SHAPE[1] - 1))\n            #Extract Mel Spec\n            mel_spec = librosa.feature.melspectrogram(y=c_,sr=SAMPLE_RATE,n_fft=1024, hop_length=hop_length, \n                                                  n_mels=SPEC_SHAPE[0], fmin=FMIN, fmax=FMAX)\n    \n            mel_spec = librosa.power_to_db(mel_spec, ref=np.max) \n            # Normalize\n            mel_spec = (mel_spec - mel_spec.min())/(mel_spec.max() - mel_spec.min())\n            mel_chunks.append(np.asarray(Image.fromarray(mel_spec * 255.0).convert(\"L\")))\n            y_arr.append(y)\n        y_arr = np.array(y_arr).reshape(-1, 152)\n        mel_chunks = np.array(mel_chunks)\n        X.extend(mel_chunks)\n        Y.extend(y_arr)\n        \nX = np.array(X)\nY = np.array(Y) ","metadata":{"execution":{"iopub.status.busy":"2022-04-12T20:31:24.05495Z","iopub.execute_input":"2022-04-12T20:31:24.055479Z","iopub.status.idle":"2022-04-12T20:48:01.774843Z","shell.execute_reply.started":"2022-04-12T20:31:24.055439Z","shell.execute_reply":"2022-04-12T20:48:01.774174Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(X.shape,Y.shape)","metadata":{"execution":{"iopub.status.busy":"2022-04-12T20:48:01.77646Z","iopub.execute_input":"2022-04-12T20:48:01.776964Z","iopub.status.idle":"2022-04-12T20:48:01.781886Z","shell.execute_reply.started":"2022-04-12T20:48:01.776925Z","shell.execute_reply":"2022-04-12T20:48:01.781233Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **MODEL TRAINING**\n### Now, let’s build our Sequential neural network model utilizing the ADAM optimizer (try out RMSProp or other optimizers and see if you can squeeze out some more accuracy!). My network architecture consists of 2 convolutional layers with increasing filter density in order to best extract the features of each image with each successive layer (although I have tried with up to four convolutional layers - 2D). The pooling and dropout layers serve to increase computational efficiency and to prevent overfitting, respectively. Also, I have resizing the spectrogram to 48 x 48 in order to reduce the data processing time","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras import layers\nfrom tensorflow.keras import models\nimport tensorflow as tf\nimport tensorflow_addons as tfa\n\ntf.random.set_seed(seed)\nmodel = tf.keras.Sequential([\n    tf.keras.layers.Conv1D(8, 3, 1, activation='relu', \n                           input_shape=(48, 128, 1)),\n    tf.keras.layers.Resizing(48, 48),    \n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.MaxPooling2D(),\n    tf.keras.layers.Dropout(0.25),  \n    \n    tf.keras.layers.Conv1D(16, 3, 1, activation='relu'),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.MaxPooling2D(), \n    tf.keras.layers.Dropout(0.25),  \n        \n    tf.keras.layers.Flatten(),     \n    tf.keras.layers.Dense(len(train.primary_label.unique()), activation='softmax')\n])\n\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy', tfa.metrics.F1Score(num_classes=len(train.primary_label.unique()))])\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2022-04-12T20:48:01.78335Z","iopub.execute_input":"2022-04-12T20:48:01.783908Z","iopub.status.idle":"2022-04-12T20:48:10.771254Z","shell.execute_reply.started":"2022-04-12T20:48:01.78387Z","shell.execute_reply":"2022-04-12T20:48:10.77057Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"callbacks = [tf.keras.callbacks.EarlyStopping(monitor='val_loss', \n                                              verbose=1,\n                                              patience=150)]\n\nhistory = model.fit(np.expand_dims(X, -1), Y, batch_size =128, epochs=150, validation_split = 0.2,\n         callbacks=callbacks)","metadata":{"execution":{"iopub.status.busy":"2022-04-12T20:48:10.77238Z","iopub.execute_input":"2022-04-12T20:48:10.772714Z","iopub.status.idle":"2022-04-12T21:01:34.321279Z","shell.execute_reply.started":"2022-04-12T20:48:10.772676Z","shell.execute_reply":"2022-04-12T21:01:34.320494Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# saving our model for later use\n\nmodel.save('model.h5')","metadata":{"execution":{"iopub.status.busy":"2022-04-12T21:01:34.322889Z","iopub.execute_input":"2022-04-12T21:01:34.323167Z","iopub.status.idle":"2022-04-12T21:01:34.372573Z","shell.execute_reply.started":"2022-04-12T21:01:34.323132Z","shell.execute_reply":"2022-04-12T21:01:34.371947Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **EVALUATION OF THE MODEL**","metadata":{}},{"cell_type":"code","source":"# Plotting the accuracy of the model over the epochs\n\nplt.figure(figsize=(15,5))\nplt.plot(history.history['accuracy'])\nplt.title('Model accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-12T21:01:34.373752Z","iopub.execute_input":"2022-04-12T21:01:34.374004Z","iopub.status.idle":"2022-04-12T21:01:34.576377Z","shell.execute_reply.started":"2022-04-12T21:01:34.373969Z","shell.execute_reply":"2022-04-12T21:01:34.575726Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plotting the loss of the model over the epochs\n\nplt.figure(figsize=(15,5))\nplt.plot(history.history['loss'])\nplt.title('Model loss')\nplt.ylabel('loss')\nplt.xlabel('Epoch')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-12T21:01:34.577662Z","iopub.execute_input":"2022-04-12T21:01:34.577889Z","iopub.status.idle":"2022-04-12T21:01:34.76545Z","shell.execute_reply.started":"2022-04-12T21:01:34.577858Z","shell.execute_reply":"2022-04-12T21:01:34.764738Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plotting the loss of the model over the epochs\n\nplt.figure(figsize=(15,5))\nplt.plot(history.history['val_loss'])\nplt.title('Model val_loss')\nplt.ylabel('val_loss')\nplt.xlabel('Epoch')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-12T21:01:34.766669Z","iopub.execute_input":"2022-04-12T21:01:34.766895Z","iopub.status.idle":"2022-04-12T21:01:34.956449Z","shell.execute_reply.started":"2022-04-12T21:01:34.766863Z","shell.execute_reply":"2022-04-12T21:01:34.955726Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"main_dir = '../input/birdclef-2022'\ntest = pd.read_csv(main_dir+'/test.csv') \nsubmission = pd.read_csv(main_dir+'/sample_submission.csv')","metadata":{"execution":{"iopub.status.busy":"2022-04-12T21:01:34.957631Z","iopub.execute_input":"2022-04-12T21:01:34.957862Z","iopub.status.idle":"2022-04-12T21:01:34.974956Z","shell.execute_reply.started":"2022-04-12T21:01:34.95783Z","shell.execute_reply":"2022-04-12T21:01:34.974287Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-12T21:01:34.976214Z","iopub.execute_input":"2022-04-12T21:01:34.976455Z","iopub.status.idle":"2022-04-12T21:01:34.988305Z","shell.execute_reply.started":"2022-04-12T21:01:34.976422Z","shell.execute_reply":"2022-04-12T21:01:34.9867Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-12T21:01:34.990071Z","iopub.execute_input":"2022-04-12T21:01:34.990388Z","iopub.status.idle":"2022-04-12T21:01:34.999868Z","shell.execute_reply.started":"2022-04-12T21:01:34.990355Z","shell.execute_reply":"2022-04-12T21:01:34.998958Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport random\nfrom tqdm import tqdm\n\ntest_audio_dir = '../input/birdclef-2022/test_soundscapes'\nfor idx in tqdm(range(len(test))):\n    audio_id = test.loc[idx, 'file_id']\n    true_label = test.loc[idx, 'bird']\n    end_time = test.loc[idx, 'end_time']\n    \n    path = os.path.join(test_audio_dir, audio_id, '.ogg')\n\n    if os.path.isfile(path):\n        sig, sr = torchaudio.load(file_pth)\n        \n        rows = sig.shape[1] // (32000 *5)\n        sig = sig.reshape(rows, -1)\n\n        row_id = end_time // 5\n\n        sig = sig[row_id-1].reshape(1,-1)\n\n        audio = MonoToStereo((sig, sr))\n        audio = pad_signal(audio, 10000)\n        audio = time_shift(audio, shift_limit=0.4)\n        spec = mel_spec(audio)\n        aug_spec = spectro_augment(spec)\n        aug_spec = aug_spec.unsqueeze(0)\n        output = model(aug_spec)\n\n        _, pred = torch.max(output, dim=1)\n        if labels[pred] == true_label:\n            submission.loc[idx, 'target'] = True\n        else:\n            submission.loc[idx, 'target'] = False\n        \n    else:\n        pred = True if random.randint(0,1) else False\n        submission.loc[idx, 'target'] = pred\n        continue","metadata":{"execution":{"iopub.status.busy":"2022-04-12T21:01:35.001497Z","iopub.execute_input":"2022-04-12T21:01:35.001775Z","iopub.status.idle":"2022-04-12T21:01:35.024012Z","shell.execute_reply.started":"2022-04-12T21:01:35.001718Z","shell.execute_reply":"2022-04-12T21:01:35.023237Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **MAKE SUBMISSION**","metadata":{}},{"cell_type":"code","source":"submission.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-12T21:01:35.02527Z","iopub.execute_input":"2022-04-12T21:01:35.025507Z","iopub.status.idle":"2022-04-12T21:01:35.033684Z","shell.execute_reply.started":"2022-04-12T21:01:35.025475Z","shell.execute_reply":"2022-04-12T21:01:35.032714Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2022-04-12T21:01:35.035146Z","iopub.execute_input":"2022-04-12T21:01:35.035469Z","iopub.status.idle":"2022-04-12T21:01:35.044269Z","shell.execute_reply.started":"2022-04-12T21:01:35.035423Z","shell.execute_reply":"2022-04-12T21:01:35.043428Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Done!')","metadata":{"execution":{"iopub.status.busy":"2022-04-12T21:01:35.048254Z","iopub.execute_input":"2022-04-12T21:01:35.048462Z","iopub.status.idle":"2022-04-12T21:01:35.052937Z","shell.execute_reply.started":"2022-04-12T21:01:35.048436Z","shell.execute_reply":"2022-04-12T21:01:35.051966Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **CONCLUSION** \n### * There is no perfect model, and I will continue working.\n### * If you like this notebook then, please upvote!\n### * As I said in the introduction of this notebook, the precision will increase until all the audio data will be filtered, either by the sponsor or by the user and spending a lot of time doing it.\n","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}}]}