{"cells":[{"metadata":{},"cell_type":"markdown","source":"# SENTIMENT ANALYSIS ON MOVIE REVIEWS\n## Vu Duong\n### Date: June, 2020","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# CREDITS\n This notebook is inspired by multiple great work:\n - https://www.kaggle.com/chiranjeevbit/movie-review-prediction\n - https://www.kaggle.com/stass30/result-0-66-lstm-vs-machine-learning\n - https://www.kaggle.com/carmensandiego/keras-bert-tfhub-scores-0-695","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# INTRODUCTION\nThe Rotten Tomatoes movie review dataset is a corpus of movie reviews used for sentiment analysis, originally collected by Pang and Lee [1]. In their work on sentiment treebanks, Socher et al. [2] used Amazon's Mechanical Turk to create fine-grained labels for all parsed phrases in the corpus. The dataset is comprised of tab-separated files with phrases from the Rotten Tomatoes dataset.\n\nThe sentiment labels are:\n- 0 - negative\n- 1 - somewhat negative\n- 2 - neutral\n- 3 - somewhat positive\n- 4 - positive\n\nDetailed description of dataset content is described in the following link: https://www.kaggle.com/c/sentiment-analysis-on-movie-reviews/data","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# APPROACH:\n1. Exploring: distribution across labels \n2. Cleaning: remove html, emoji, url, number, non-alphabetic. \n3. Modeling: apply LinearSVC, Embedding methods.\n\n### RESULTS:\n\nAfter applying 4 different techniques on validation data set:\n- CountVectorizer, TFIDF and LinearSVC: 64% accuracy.\n- Traditional Embedding: 66.24% after 4 epochs.\n- GloVe Embedding: 67.99% after 9 epochs.\n- BERT Embedding: 66.27% after 3 epochs.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# LIBRARY","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Data Processing\nimport numpy as np \nfrom numpy import asarray\nimport pandas as pd \nfrom tqdm import tqdm #TQDM is a progress bar library with good support for nested loops and Jupyter/IPython notebooks.\nfrom sklearn.model_selection import train_test_split\nimport re\n\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.preprocessing.text import Tokenizer\n!wget --quiet https://raw.githubusercontent.com/tensorflow/models/master/official/nlp/bert/tokenization.py\nimport tokenization\n\n# Data Modeling\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.feature_extraction.text import TfidfVectorizer # == CountVectorizer + TfidfTransformer\nfrom sklearn.svm import LinearSVC\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense,Dropout,Embedding,LSTM\nfrom keras.callbacks import EarlyStopping\nfrom keras.losses import categorical_crossentropy\nfrom keras.optimizers import Adam\n\nimport tensorflow as tf\nimport tensorflow_hub as hub\n\n\n# Data Evaluation\nfrom sklearn import metrics\n\n\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=UserWarning, module='bs4')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!unzip ../input/sentiment-analysis-on-movie-reviews/train.tsv.zip \n!unzip ../input/sentiment-analysis-on-movie-reviews/test.tsv.zip","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('train.tsv', sep='\\t')\ntest = pd.read_csv('test.tsv', sep='\\t')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# DATA CLEANING","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def remove_URL(text):\n    #url = re.compile(r'https?://\\S+|www\\.\\S+')\n    url = re.compile(r'http\\S+|www.\\S+')  # https / http / www\n    return url.sub(r'',text)\n\ndef remove_html(text):\n    html=re.compile(r'<.*?>') # '<p>string<p>' -> 'string'\n    #html=re.compile(r'<.*>') # '<p>string<p>' -> ''\n    return html.sub(r'',text)\n\ndef remove_emoji(text):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)\n\ndef remove_number(text):\n    url = re.compile(r'[0-9]')  \n    return url.sub(r'',text)\n\ndef remove_non_alphabet(text):\n    url = re.compile(r'[^a-z\\s]')  \n    return url.sub(r' ',text) # space is handled by Tokenizer of Keras, don't worry\n\nimport string\ndef remove_punct(text):\n    table=str.maketrans('','',string.punctuation)\n    return text.translate(table)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['Phrase'] = train['Phrase'].str.lower()\ntrain['Phrase'] = train['Phrase'].transform(remove_URL)\ntrain['Phrase'] = train['Phrase'].transform(remove_html)\ntrain['Phrase'] = train['Phrase'].transform(remove_emoji)\ntrain['Phrase'] = train['Phrase'].transform(remove_number)\ntrain['Phrase'] = train['Phrase'].transform(remove_non_alphabet)\n\ntest['Phrase'] = test['Phrase'].str.lower()\ntest['Phrase'] = test['Phrase'].transform(remove_URL)\ntest['Phrase'] = test['Phrase'].transform(remove_html)\ntest['Phrase'] = test['Phrase'].transform(remove_emoji)\ntest['Phrase'] = test['Phrase'].transform(remove_number)\ntest['Phrase'] = test['Phrase'].transform(remove_non_alphabet)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = train['Phrase']\ny = train['Sentiment']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y,test_size=0.25, random_state=0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# DATA MODELING","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### LinearSVC","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# All steps at once\ntext_clf = Pipeline([('tfidf', TfidfVectorizer()),\n                     ('clf', LinearSVC()),])\n\n# Feed the training data through the pipeline\ntext_clf.fit(X_train, y_train)\n\n# Form a prediction set\ny_pred = text_clf.predict(X_test)\n\n# Print the overall accuracy\nprint('LinearSVC Score: ', metrics.accuracy_score(y_test,y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"### Traditional Embedding method","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenize = Tokenizer()\ntokenize.fit_on_texts(X_train.values)\n\nvocab_size = len(tokenize.word_index) + 1\n\nX_train = tokenize.texts_to_sequences(X_train)\nX_test = tokenize.texts_to_sequences(X_test)\n#X_test = tokenize.texts_to_sequences(test['Phrase'])\n\nmax_lenght = max([len(s.split()) for s in train['Phrase']])\n\nX_train = pad_sequences(X_train, max_lenght, padding='post')\nX_test = pad_sequences(X_test, max_lenght, padding='post')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Sequential()\nmodel.add(Embedding(vocab_size, 300, input_length=max_lenght))\nmodel.add(LSTM(units=128, dropout=0.2, recurrent_dropout=0.2 ))\nmodel.add(Dense(5, activation='softmax'))\nmodel.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\nprint(model.summary())\n\nmodel.fit(X_train, y_train,validation_data=(X_test, y_test), batch_size=128, epochs=4, verbose=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results_Embedding = model.evaluate(X_test, y_test, batch_size=128)\nprint('Embedding Test Accuracy Score: ', results_Embedding[1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"### GloVe Embedding","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# load the whole embedding into memory\nembeddings_index = dict()\nf = open('../input/glove6b100dtxt1/glove.6B.100d.txt', encoding=\"utf8\")\nfor line in f:\n    values = line.split()\n    word = values[0]\n    coefs = asarray(values[1:], dtype='float32')\n    embeddings_index[word] = coefs\nf.close()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# create a weight matrix for words in training docs\nembedding_matrix = np.zeros((vocab_size, 100))\nfor word, i in tokenize.word_index.items():\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        embedding_matrix[i] = embedding_vector","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Sequential()\nmodel.add(Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=max_lenght))\nmodel.add(LSTM(units=128, dropout=0.2, recurrent_dropout=0.2 ))\nmodel.add(Dense(5, activation='softmax'))\nmodel.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\nprint(model.summary())\n\nmodel.fit(X_train, y_train,validation_data=(X_test, y_test), batch_size=128, epochs=7, verbose=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# results_GloVe = model.evaluate(X_test, y_test, batch_size=128)\n# print('GloVe Test Accuracy Score: ', results_GloVe[1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"### BERT","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"module_url = 'https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/2'\nbert_layer = hub.KerasLayer(module_url, trainable=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\ndo_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\ntokenizer = tokenization.FullTokenizer(vocab_file, do_lower_case)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def bert_encode(texts, tokenizer, max_len=512):\n    all_tokens = []\n    all_masks = []\n    all_segments = []\n    \n    for text in texts:\n        text = tokenizer.tokenize(text)\n            \n        text = text[:max_len-2]\n        input_sequence = [\"[CLS]\"] + text + [\"[SEP]\"]\n        pad_len = max_len - len(input_sequence)\n        \n        tokens = tokenizer.convert_tokens_to_ids(input_sequence) + [0] * pad_len\n        pad_masks = [1] * len(input_sequence) + [0] * pad_len\n        segment_ids = [0] * max_len\n        \n        all_tokens.append(tokens)\n        all_masks.append(pad_masks)\n        all_segments.append(segment_ids)\n        \n    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_model(bert_layer, max_len=512):\n    input_word_ids = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n    input_mask = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\n    segment_ids = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=\"segment_ids\")\n\n    pooled_output, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n    clf_output = sequence_output[:, 0, :]\n    net = tf.keras.layers.Dense(64, activation='relu')(clf_output)\n    net = tf.keras.layers.Dropout(0.2)(net)\n    net = tf.keras.layers.Dense(32, activation='relu')(net)\n    net = tf.keras.layers.Dropout(0.2)(net)\n    out = tf.keras.layers.Dense(5, activation='softmax')(net)\n    \n    model = tf.keras.models.Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=out)\n    model.compile(tf.keras.optimizers.Adam(lr=1e-5), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n    \n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"max_len = 52\ntrain_input = bert_encode(train['Phrase'].values, tokenizer, max_len=max_len)\n#train_labels = tf.keras.utils.to_categorical(train['Sentiment'].values, num_classes=5)\ntrain_labels = train['Sentiment'].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = build_model(bert_layer, max_len=max_len)\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"checkpoint = tf.keras.callbacks.ModelCheckpoint('model.h5', monitor='val_accuracy', \n                                                save_best_only=True, verbose=1)\nearlystopping = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=3, verbose=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_history = model.fit(\n    train_input, train_labels, \n    validation_split=0.2,\n    epochs=3,\n    callbacks=[checkpoint, earlystopping],\n    batch_size=32)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# text_clf.fit(X, y)\n\n# # Form a prediction set\n# y_pred = text_clf.predict(test['Phrase'])\n\n# sub = pd.read_csv('../input/sentiment-analysis-on-movie-reviews/sampleSubmission.csv')\n# sub['Sentiment'] = y_pred\n# sub.to_csv('submission.csv', index=False)\n\n\n# y_pred = model.predict(X_test)\n\n# sub = pd.read_csv('../input/sentiment-analysis-on-movie-reviews/sampleSubmission.csv')\n# sub['Sentiment'] = np.argmax(y_pred, axis=-1)\n# sub.to_csv('submission.csv', index=False)\n\n\n# y_pred = model.predict_classes(X_test)\n# sub = pd.read_csv('../input/sentiment-analysis-on-movie-reviews/sampleSubmission.csv')\n# sub['Sentiment'] = y_pred\n# sub.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}