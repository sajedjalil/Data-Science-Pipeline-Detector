{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Notebook resume\n<p style=\"font-size:15px; font-family:verdana; line-height: 1.7em\">   \nThroughout this notebook I'll be using the Tez framework to train neural networks with Pytorch, the idea behind it seems amazing to me. In fact it has allowed me to create a linear model, with very little work and being inexperienced on the subject. Also the resulting code is quite pythonic. The link to the repository is down below. Happy kaggling! </p>\n\n> ## Github: https://github.com/abhishekkrthakur/tez","metadata":{}},{"cell_type":"code","source":"! pip install tez -q","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nimport tez\nimport tez\nfrom tez.datasets import GenericDataset\nfrom tez.callbacks import EarlyStopping\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport torch.utils.data as data\nfrom sklearn import preprocessing, metrics, model_selection","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data loading\n","metadata":{}},{"cell_type":"code","source":"train = pd.read_parquet('../input/d/maxdiazbattan/playgroundkfold/train_kfold_play_dic.parquet')\ntest = pd.read_parquet('../input/d/maxdiazbattan/playgroundkfold/test_play_dic.parquet')\nsubmission = pd.read_parquet('../input/d/maxdiazbattan/playgroundkfold/submission_play_dic.parquet')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feat = [feature for feature in train.columns if feature not in ('id', 'kfold','target')]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"n_features = len(feat)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Dataset\n<p style=\"font-size:15px; font-family:verdana; line-height: 1.7em\">   \nTez has a generic dataset class, you can use that if you want, in this case I don't do it because I want to make some changes to it, for that reason I have used this one.</p>","metadata":{}},{"cell_type":"code","source":"class GenDataset:\n    def __init__(self, data, targets):\n        self.data = data\n        self.targets = targets\n        \n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        \n        if self.targets is not None:\n            data = self.data[idx]\n            targets = self.targets[idx]\n            return {\n                    \"features\": torch.tensor(data, dtype=torch.float),\n                    \"target\": torch.tensor(targets, dtype=torch.long),\n                   }\n        else:\n            data = self.data[idx]\n            return {\n                    \"features\": torch.tensor(data, dtype=torch.float),\n                   }","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Tez Model Class\n<p style=\"font-size:15px; font-family:verdana; line-height: 1.7em\">   \nThis model inherits from the tez.Model module. It's a very simple model, you can change it quite a lot.</p>","metadata":{}},{"cell_type":"code","source":"class TPSModel(tez.Model):\n    def __init__(self, n_features, hidd_layers, num_classes):\n        super().__init__()\n        # You can play with the layers.\n        self.layer_1 = nn.Linear(n_features, hidd_layers)\n        self.relu = nn.ReLU()\n        self.layer_2 = nn.Linear(hidd_layers, num_classes)\n        \n        self.step_scheduler_after = \"batch\"\n        \n    def monitor_metrics(self, outputs, target):\n        # You can monitor several metrics at the same time  if you want.\n        if target is None:\n            return {}\n        outputs = torch.argmax(outputs, dim=1).cpu().detach().numpy() \n        target = target.cpu().detach().numpy()\n        accuracy = metrics.accuracy_score(target, outputs)\n        return {\"accuracy\": accuracy\n               }\n    \n    def loss(self, outputs, target):\n        if target is None:\n            return None\n        return nn.CrossEntropyLoss()(outputs, target)\n    \n    def fetch_optimizer(self):\n        # You can change the optimizer, link about this down below.\n        opt = torch.optim.Adam(self.parameters(), lr=1e-4)\n        return opt\n    \n    def fetch_scheduler(self):\n        # You can also change the scheduler, down below I share a link about this.\n        sch = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(self.optimizer, T_0=10, T_mult=1, eta_min=1e-5, last_epoch=-1)\n        return sch\n\n    def forward(self, features, target=None):\n        outputs = self.layer_1(features)\n        outputs = self.relu(outputs)\n        outputs = self.layer_2(outputs)\n        \n        if target is not None:\n            outputs = outputs\n            target = target\n            \n            loss = self.loss(outputs, target)\n            metrics = self.monitor_metrics(outputs, target)\n            return outputs, loss, metrics\n        return outputs, None, None","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> ## Optimizer & Scheduler link: https://pytorch.org/docs/stable/optim.html","metadata":{}},{"cell_type":"markdown","source":"# Data Split, Preprocessing (with oversampling) & Model Train\n<p style=\"font-size:15px; font-family:verdana; line-height: 1.7em\">   \nIn this section is where everything happens, first, since I'm going to be using CrossEntropyLoss, this loss metric accepts values in the range from 0 to n-1, and for that reason I need to modify the values of the target variable so that it starts at 0. With a simple subtraction it can be done. Second, for time reasons I'm going to train the model just in one fold, for this case fold # 4, which is the one that contains the only value Cover_Type = 5. On the other hand I'm going to do an oversampling assigning a weight to each of the values of the classes, and a simple scaling with standard scaler.</p>","metadata":{}},{"cell_type":"code","source":"train.Cover_Type.value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.Cover_Type = train.Cover_Type - 1 ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.Cover_Type.value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Data split\nfold = 4   \nX_train = train[train.kfold == fold].reset_index(drop=True)\nX_valid = train[train.kfold != fold].reset_index(drop=True)\n\ny_train = X_train.Cover_Type \ny_valid = X_valid.Cover_Type \n\n# Sampling the data\ntrain_labels_unique, class_counts = np.unique(X_train['Cover_Type'], return_counts=True)\nweights = 1. / class_counts\nsamples_weights = np.array([weights[t] for t in y_train])\nsamples_weights = torch.from_numpy(samples_weights)\nsampler = data.WeightedRandomSampler(samples_weights, len(samples_weights))\n\n# Scaling\nfeat = [feature for feature in train.columns if feature not in ('Id', 'kfold','Cover_Type','Weights')]\n\nscl = preprocessing.StandardScaler()\nX_train = scl.fit_transform(X_train[feat].values)\nX_valid = scl.transform(X_valid[feat].values)\nX_test = scl.transform(test[feat].values)\n\n# Creating the datasets\ntrain_dataset = GenDataset(X_train, y_train.values)\nvalid_dataset = GenDataset(X_valid, y_valid.values)\ntest_dataset = GenDataset(X_test, None)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"n_features = len(feat)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = TPSModel(n_features=n_features, hidd_layers=100, num_classes=train['Cover_Type'].nunique())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"e_stop = EarlyStopping(monitor=\"valid_loss\", model_path='Model.bin',patience=5,mode=\"max\")\nmodel.fit(train_dataset,valid_dataset=valid_dataset,\n          train_sampler=sampler,train_shuffle=False,\n          train_bs=32,valid_bs=64,device=\"cuda\",epochs=5, \n          callbacks=[e_stop], n_jobs=2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Predictions\n<p style=\"font-size:15px; font-family:verdana; line-height: 1.7em\">\nTo compensate a bit for running the model in a single fold, I'm going to make it run 5 times when predicting, I don't know if that's the right thing to do, but it gave me a slightly better result. On the other hand, at moment of submitting, I add 1 to the classes to restore the original values.</p>","metadata":{}},{"cell_type":"code","source":"final_preds = None\nfor j in range(5):\n    predictions = model.predict(test_dataset, batch_size=32, n_jobs=-1)\n    temp_preds = None\n    for pred in predictions:\n        if temp_preds is None:\n            temp_preds = pred\n        else:\n            temp_preds = np.vstack((temp_preds, pred))\n    if final_preds is None:\n        final_preds = temp_preds\n    else:\n        final_preds += temp_preds\nfinal_preds /= 5","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_preds = final_preds.argmax(axis=1)\n    \nsubmission.Cover_Type = final_preds\nsubmission.Cover_Type = submission.Cover_Type + 1\nsubmission.to_csv(\"submission.csv\", index=False)","metadata":{},"execution_count":null,"outputs":[]}]}