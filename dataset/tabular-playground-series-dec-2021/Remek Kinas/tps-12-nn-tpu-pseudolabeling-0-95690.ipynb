{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## TPS-12 Tensorflow NN (model traning on GPU - to speed up learning process) and pseudolabeling\n\nSpecial thanks to [AMBROSM](https://www.kaggle.com/ambrosm) for hist great notebook [TPSDEC21-01-Keras Quickstart](https://www.kaggle.com/ambrosm/tpsdec21-01-keras-quickstart) which was inspiration for this one.\n\nThis notebooks contains some noew ideas and improvements:\n- pseudolabeling - made it separately - you can find it and use for own trainings in my dataset [TPS-12 Pseudolabels](https://www.kaggle.com/remekkinas/tps12-pseudolabels) - please vote on my database (it will be updated)\n- TPU training (to speed up process)\n- Tensorflow API isntead of Keras Sequential","metadata":{}},{"cell_type":"code","source":"import os\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport pandas as pd\nimport numpy as np\nimport gc\n\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler, MinMaxScaler\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.compose import make_column_transformer\n\nimport tensorflow as tf\nfrom tensorflow.keras.models import Model, Sequential\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\nfrom tensorflow.keras import layers","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 1. DATA PREPARATION\n\n## 1.1 LOAD DATA AND PREPARE\n","metadata":{}},{"cell_type":"code","source":"train_df = pd.read_csv(\"../input/tabular-playground-series-dec-2021/train.csv\").drop(columns=['Soil_Type7', 'Soil_Type15']) \ntest_df = pd.read_csv(\"../input/tabular-playground-series-dec-2021/test.csv\").drop(columns=['Soil_Type7', 'Soil_Type15']) \npseudolabels_df = pd.read_csv(\"../input/tps12-pseudolabels/tps12-pseudolabels_v2.csv\").drop(columns=['Soil_Type7', 'Soil_Type15']) \n\nsample_submission = pd.read_csv(\"../input/tabular-playground-series-dec-2021/sample_submission.csv\")\n\ntrain_df = train_df[train_df.Cover_Type != 5]\ntrain_df = train_df[train_df.Cover_Type != 4]\ntrain_df = pd.concat([train_df, pseudolabels_df], axis=0)\ntrain_df.reset_index(drop=True)\n\nle = LabelEncoder()\ntarget = le.fit_transform(train_df.Cover_Type)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1.2 DEFINE DATA PIPELINE PREPROCESSOR","metadata":{}},{"cell_type":"code","source":"features = [feat for feat in test_df.columns if feat != 'Id' and feat != 'Cover_Type']\n\ndata_pipe_transformer = make_pipeline(\n    StandardScaler()\n)\n\npreprocessor = make_column_transformer(\n    (data_pipe_transformer, features)\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2. TPU CONFIGURATION","metadata":{}},{"cell_type":"code","source":"# if you want to traing on GPU just set it to False\n\nTPU = True","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"try:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nexcept:\n    strategy = tf.distribute.get_strategy()\n    \nprint('Replicas:', strategy.num_replicas_in_sync)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3. MODEL CONFIGURATION","metadata":{}},{"cell_type":"code","source":"# I use skipped connection which improve score a little bit\n\ndef my_model(X):\n    il = layers.Input(shape=(X.shape[-1]), name=\"input\")\n    x = layers.Dense(128, activation='selu')(il)\n    x1 = layers.BatchNormalization()(x)\n    x = layers.Dense(64, activation='selu')(x1)\n    x = layers.BatchNormalization()(x)\n    x = layers.Dropout(0.2)(layers.Concatenate()([x, x1]))\n    x = layers.Dense(units=64, activation='relu')(x) \n    x = layers.BatchNormalization()(x)\n    x = layers.Dense(64, activation='selu')(x)\n    x = layers.BatchNormalization()(x)\n    output = layers.Dense(len(le.classes_), activation=\"softmax\", name=\"output\")(x)\n\n    model = tf.keras.Model([il], output)\n    return model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4. TRAINING","metadata":{}},{"cell_type":"code","source":"EPOCHS = 90 \nVERBOSE = 2 \nRUNS = 1 \nBATCH_SIZE = 1024 \nFOLDS = 10\n\nnp.random.seed(2021)\ntf.random.set_seed(2021)\n\nscore_list, test_pred_list, history_list = [], [], []\noof_list = [np.full((len(train_df), len(le.classes_)), -1.0, dtype='float32') for run in range(RUNS)]\nfor run in range(RUNS):\n    kf = StratifiedKFold(n_splits=FOLDS, shuffle=True, random_state=1)\n    for fold, (train_idx, val_idx) in enumerate(kf.split(train_df, y=train_df.Cover_Type)):\n        print(f\"Fold {run}.{fold}\")\n\n        X_tr = train_df.iloc[train_idx]\n        X_va = train_df.iloc[val_idx]\n        y_tr = target[train_idx]\n        y_va = target[val_idx]\n        X_tr = X_tr[features]\n        X_va = X_va[features]\n        \n        X_tr = preprocessor.fit_transform(X_tr)\n        X_va = preprocessor.transform(X_va)\n        \n        # TPU model\n        if TPU:\n            with strategy.scope():\n                model = my_model(X_tr)\n                model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=tf.keras.optimizers.Adam(), metrics=[\"accuracy\"])\n        else:\n            \n            # GPU model\n            model = my_model(X_tr)\n            model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=tf.keras.optimizers.Adam(), metrics=[\"accuracy\"])\n  \n\n        lr = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, \n                               patience=5, verbose=VERBOSE)\n\n        es = EarlyStopping(monitor=\"val_acc\", patience=10, \n                           verbose=VERBOSE, mode=\"max\", \n                           restore_best_weights=True)\n\n  \n        history = model.fit(X_tr, y_tr, \n                            validation_data=(X_va, y_va), \n                            epochs=EPOCHS,\n                            verbose=VERBOSE,\n                            batch_size=BATCH_SIZE, \n                            validation_batch_size=len(X_va),\n                            shuffle=True,\n                            callbacks=[lr, es])\n        history_list.append(history.history)\n           \n        y_va_pred = model.predict(X_va, batch_size=len(X_va))\n        oof_list[run][val_idx] = y_va_pred\n        y_va_pred = le.inverse_transform(np.argmax(y_va_pred, axis=1))\n\n        accuracy = accuracy_score(train_df.iloc[val_idx].Cover_Type, y_va_pred)\n\n        print(f\"Fold {run}.{fold} | Epochs: {len(history_list[-1]['loss'])} | Accuracy: {accuracy:.5f}\")\n        \n        test_pred_list.append(model.predict(preprocessor.transform(test_df[features]), batch_size=BATCH_SIZE))\n        \n        del model, y_va_pred\n        gc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 5. SUBMISSION","metadata":{}},{"cell_type":"code","source":"sub = test_df[['Id']].copy()\nsub['Cover_Type'] = le.inverse_transform(np.argmax(sum(test_pred_list), axis=1)) \nsub.to_csv('tps12-pseudeo-submission.csv', index=False)","metadata":{},"execution_count":null,"outputs":[]}]}