{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Keras Quickstart\n\nThis notebook shows\n- how to use Keras for this competition\n- how to correctly cross-validate the model\n- how to set a decreasing learning rate and early stopping\n- how to plot the training curves\n- how to ensemble the five models by soft voting\n- how to save the models and the oof predictions for later use\n\nYou can enable GPU acceleration for this notebook to get the results faster, but you don't need the GPU.\n\nRelease notes:\n- V1: -> lb 0.94821\n- V2: Other network architecture (added one layer), 60 epochs\n- V3: Hidden layers \\[128, 64, 16\\]\n- V4: Hidden layers \\[128, 64, 64, 16\\] -> lb 0.95468\n  - no real difference to V3\n- V5: Fixed the voting classifier which was missing in earlier versions, added L2 regularization, LabelEncoder, drop Cover_Type 5 -> lb 0.95598\n  - Voting makes a big difference\n  - L2 regularization doesn't matter\n- V6: Hidden layers \\[128, 64, 64\\], selu activation, 3 runs -> lb 0.95619\n  - It seems that the architecture is somewhat better than before, but the two additional runs don't improve the lb score.\n- V7: BatchNormalization improves the cv but not the lb, and almost doubles the running time. -> lb 0.95598\n- V8: 10 folds, 90 epochs -> lb 0.95626\n- V9: drop Soil_Type1\n\nqrdl changes (forking from AmbrosM, save any upvotes for his:  https://www.kaggle.com/ambrosm/tpsdec21-01-keras-quickstart )\n- v1: tried dropping all under predict proba 0.44 using covtype, score 0.95625 :(\n- v2: added stratified subset for faster iteration, experiments, sample weight, add prdmean output for rough estimates\n- v3: try all unfitted\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport pickle\nimport itertools\nimport gc\nimport matplotlib.pyplot as plt\nfrom matplotlib.ticker import MaxNLocator, FormatStrFormatter, PercentFormatter\nfrom datetime import datetime\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.model_selection import StratifiedKFold, GroupKFold\nfrom sklearn.preprocessing import StandardScaler, QuantileTransformer, LabelEncoder, minmax_scale\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n\nimport tensorflow as tf\nfrom tensorflow.keras.utils import plot_model\nfrom tensorflow.keras.models import Model, load_model, Sequential\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\nfrom tensorflow.keras.layers import Dense, Dropout, Input, InputLayer, Flatten, LayerNormalization, BatchNormalization\n","metadata":{"execution":{"iopub.status.busy":"2021-12-15T17:55:24.619365Z","iopub.execute_input":"2021-12-15T17:55:24.620147Z","iopub.status.idle":"2021-12-15T17:55:30.098709Z","shell.execute_reply.started":"2021-12-15T17:55:24.620071Z","shell.execute_reply":"2021-12-15T17:55:30.097853Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df = pd.read_csv('../input/tabular-playground-series-dec-2021/train.csv')\ntest_df = pd.read_csv('../input/tabular-playground-series-dec-2021/test.csv')\n\nex, strat, txt = 2, True, \"stratified, remove 1% unfitted / lower < 0.44 proba\"\nex, strat, txt = 1, True, \"stratified, sigmoid20\"\nex, strat, txt = 0, False, \"all, sigmoid\"\nex, strat, txt = 4, False, \"all, remove 3% unfitted / lower < 0.50 proba\"\nif strat: #set to true for stratification, helps speed up iterating\n    strat = train_df.groupby('Cover_Type').apply(lambda x: x.sample(frac=.1, random_state = 1))\n    display(strat['Cover_Type'].value_counts()/len(strat))\n    display(train_df['Cover_Type'].value_counts()/len(train_df))\n    train_df = strat.reset_index(level=0, drop=True)","metadata":{"execution":{"iopub.status.busy":"2021-12-15T17:55:30.102344Z","iopub.execute_input":"2021-12-15T17:55:30.102569Z","iopub.status.idle":"2021-12-15T17:55:50.360803Z","shell.execute_reply.started":"2021-12-15T17:55:30.102536Z","shell.execute_reply":"2021-12-15T17:55:50.35957Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import math\ndef sigmoid(x):\n    \"Numerically-stable sigmoid function.\"\n    if x >= 0:\n        z = math.exp(-x)\n        return 1 / (1 + z)\n    else:\n        z = math.exp(x)\n        return z / (1 + z)\n    \n\n#experiment sigmoid10, idea is to use maxP for weights, sigmoid to emphasize low weights for low probability, and accelerate quickly to high weights for higher thresholds\n#mean prd on stratified - .9607799013352751\nif ex == 0:  \n    import lightgbm\n    covtype = pd.read_csv('../input/forest-cover-type-dataset/covtype.csv')\n    ret = lightgbm.LGBMClassifier().fit(covtype.iloc[:,:-1], covtype.iloc[:,-1:])\n    print(\"Done fit.\")\n    res = ret.predict_proba(train_df.iloc[:,1:-1])\n    print(\"Done Predict.\")\n    retval = []\n    for i in range(len(res)):\n        retval.append(sigmoid(10*(res[i].max() - 0.4)))\n        if i % 1000000 == 0:\n            print(i, end = \" \")\n    train_df['maxP'] = retval\n    \n#experiment sigmoid20, same but faster acceleration\n#haven't tried it yet\nif ex == 1:  #0.9604157942800671\n    import lightgbm\n    covtype = pd.read_csv('../input/forest-cover-type-dataset/covtype.csv')\n    ret = lightgbm.LGBMClassifier().fit(covtype.iloc[:,:-1], covtype.iloc[:,-1:])\n    print(\"Done fit.\")\n    res = ret.predict_proba(train_df.iloc[:,1:-1])\n    print(\"Done Predict.\")\n    retval = []\n    for i in range(len(res)):\n        retval.append(sigmoid(20*(res[i].max() - 0.4)))\n        if i % 1000000 == 0:\n            print(i, end = \" \")\n    train_df['maxP'] = retval\n    \n#experiment drop all 0.44, top 1%\n#mean prd - strat .9605358170892566 / all. .964547551476723\nif ex == 2: \n    import lightgbm\n    covtype = pd.read_csv('../input/forest-cover-type-dataset/covtype.csv')\n    ret = lightgbm.LGBMClassifier().fit(covtype.iloc[:,:-1], covtype.iloc[:,-1:])\n    print(\"Done fit.\")\n    res = ret.predict_proba(train_df.iloc[:,1:-1])\n    print(\"Done Predict.\")\n    retval = []\n    for i in range(len(res)):\n        retval.append(res[i].max())\n        if i % 1000000 == 0:\n            print(i, end = \" \")\n    train_df['maxP'] = retval\n    ltn = train_df.query('maxP < 0.44')\n    display(ltn[['Wilderness_Area1','Wilderness_Area2','Wilderness_Area3','Wilderness_Area4']].sum())\n    #ltn['s'] = ltn.apply(lambda x:np.sum(x[15:55]), axis=1)\n    display(ltn[0:5])\n    l = len(ltn)\n    tl = len(train_df)\n    print(l, tl, l/tl)\n    train_df = train_df.query('maxP > 0.40')\n    train_df['maxP'] = 1\n    print(len(train_df), train_df.keys())    \n\n#experiment drop all unfitted by covtype\n#0.9662882977413416\nif ex == 3: \n    import lightgbm\n    covtype = pd.read_csv('../input/forest-cover-type-dataset/covtype.csv')\n    ret = lightgbm.LGBMClassifier().fit(covtype.iloc[:,:-1], covtype.iloc[:,-1:])\n    print(\"Done fit.\")\n    train_df['pr_CT'] = ret.predict(train_df.iloc[:,1:-1])\n    train_df = train_df[train_df['Cover_Type']==train_df['pr_CT']] \n    del train_df['pr_CT']\n    train_df['maxP'] = 1\n    retval = [0]\n    \n#experiment drop all 0.50 top 3%\nif ex == 4: \n    import lightgbm\n    covtype = pd.read_csv('../input/forest-cover-type-dataset/covtype.csv')\n    ret = lightgbm.LGBMClassifier().fit(covtype.iloc[:,:-1], covtype.iloc[:,-1:])\n    print(\"Done fit.\")\n    res = ret.predict_proba(train_df.iloc[:,1:-1])\n    print(\"Done Predict.\")\n    retval = []\n    for i in range(len(res)):\n        retval.append(res[i].max())\n        if i % 1000000 == 0:\n            print(i, end = \" \")\n    train_df['maxP'] = retval\n    ltn = train_df.query('maxP < 0.40')\n    display(ltn[['Wilderness_Area1','Wilderness_Area2','Wilderness_Area3','Wilderness_Area4']].sum())\n    #ltn['s'] = ltn.apply(lambda x:np.sum(x[15:55]), axis=1)\n    display(ltn[0:5])\n    l = len(ltn)\n    tl = len(train_df)\n    print(l, tl, l/tl)\n    train_df = train_df.query('maxP > 0.40')\n    train_df['maxP'] = 1\n    print(len(train_df), train_df.keys())    \n","metadata":{"execution":{"iopub.status.busy":"2021-12-15T17:55:50.365566Z","iopub.execute_input":"2021-12-15T17:55:50.366042Z","iopub.status.idle":"2021-12-15T17:58:14.769279Z","shell.execute_reply.started":"2021-12-15T17:55:50.365976Z","shell.execute_reply":"2021-12-15T17:58:14.768041Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(len(retval), len(train_df))\nretval[0]","metadata":{"execution":{"iopub.status.busy":"2021-12-15T17:58:14.771451Z","iopub.execute_input":"2021-12-15T17:58:14.77173Z","iopub.status.idle":"2021-12-15T17:58:14.77863Z","shell.execute_reply.started":"2021-12-15T17:58:14.771693Z","shell.execute_reply":"2021-12-15T17:58:14.777807Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Drop some columns\nfor df in [train_df, test_df]:\n    df.drop(columns=['Soil_Type7', 'Soil_Type15'], inplace=True) # These features are always zero\n# Feature is useless according to permutation feature importance\nfeatures = [f for f in test_df.columns if f != 'Id' and f != 'Cover_Type']\n\n# Show the imbalanced class distribution\nprint(\"The imbalanced class distribution:\")\nprint((train_df.groupby('Cover_Type').Id.nunique() / len(train_df)).apply(lambda p: f\"{p:.3%}\"))\n\n# Drop Cover_Type 5 (the class with only one element can be ignored)\ntrain_df = train_df[train_df.Cover_Type != 5]\n\n# Prepare for multiclass classification\nle = LabelEncoder()\ntarget = le.fit_transform(train_df.Cover_Type) # renumbers the 6 classes from 0 to 5\n","metadata":{"execution":{"iopub.status.busy":"2021-12-15T17:58:14.779952Z","iopub.execute_input":"2021-12-15T17:58:14.780445Z","iopub.status.idle":"2021-12-15T17:58:18.185081Z","shell.execute_reply.started":"2021-12-15T17:58:14.780408Z","shell.execute_reply":"2021-12-15T17:58:18.18436Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.keys()","metadata":{"execution":{"iopub.status.busy":"2021-12-15T17:58:18.186337Z","iopub.execute_input":"2021-12-15T17:58:18.186573Z","iopub.status.idle":"2021-12-15T17:58:18.193236Z","shell.execute_reply.started":"2021-12-15T17:58:18.186539Z","shell.execute_reply":"2021-12-15T17:58:18.192417Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot training history\ndef plot_history(history, *, n_epochs=None, plot_lr=False, plot_acc=True, title=None, bottom=None, top=None):\n    \"\"\"Plot (the last unique n_epochs epochs of) the training history\"\"\"\n    plt.figure(figsize=(15, 6))\n    from_epoch = 0 if n_epochs is None else len(history['loss']) - n_epochs\n    \n    # Plot training and validation losses\n    plt.plot(np.arange(from_epoch, len(history['loss'])), history['loss'][from_epoch:], label='Training loss')\n    try:\n        plt.plot(np.arange(from_epoch, len(history['loss'])), history['val_loss'][from_epoch:], label='Validation loss')\n        best_epoch = np.argmin(np.array(history['val_loss']))\n        best_val_loss = history['val_loss'][best_epoch]\n        if best_epoch >= from_epoch:\n            plt.scatter([best_epoch], [best_val_loss], c='r', label=f'Best val_loss = {best_val_loss:.5f}')\n        if best_epoch > 0:\n            almost_epoch = np.argmin(np.array(history['val_loss'])[:best_epoch])\n            almost_val_loss = history['val_loss'][almost_epoch]\n            if almost_epoch >= from_epoch:\n                plt.scatter([almost_epoch], [almost_val_loss], c='orange', label='Second best val_loss')\n    except KeyError:\n        pass\n    if bottom is not None: plt.ylim(bottom=bottom)\n    if top is not None: plt.ylim(top=top)\n    plt.gca().xaxis.set_major_locator(MaxNLocator(integer=True))\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.legend(loc='lower left')\n    if title is not None: plt.title(title)\n        \n    # Plot validation metrics\n    if plot_acc:\n        best_epoch = np.argmax(np.array(history['val_acc']))\n        best_val_acc = history['val_acc'][best_epoch]\n        ax2 = plt.gca().twinx()\n        ax2.plot(np.arange(from_epoch, len(history['loss'])), np.array(history['val_acc'][from_epoch:]), color='r', label='Validation accuracy')\n        if best_epoch >= from_epoch:\n            plt.scatter([best_epoch], [best_val_acc], c='r', label=f'Best val_acc = {best_val_acc:.5f}')\n        ax2.set_ylabel('Accuracy')\n        ax2.legend(loc='center right')\n        \n    # Plot learning rate\n    if plot_lr:\n        ax2 = plt.gca().twinx()\n        ax2.plot(np.arange(from_epoch, len(history['loss'])), np.array(history['lr'][from_epoch:]), color='g', label='Learning rate')\n        ax2.set_ylabel('Learning rate')\n        ax2.legend(loc='upper right')\n        \n    plt.show()\n    ","metadata":{"execution":{"iopub.status.busy":"2021-12-15T17:58:18.194998Z","iopub.execute_input":"2021-12-15T17:58:18.195533Z","iopub.status.idle":"2021-12-15T17:58:18.212951Z","shell.execute_reply.started":"2021-12-15T17:58:18.195493Z","shell.execute_reply":"2021-12-15T17:58:18.212232Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training","metadata":{}},{"cell_type":"code","source":"#%%time\nEPOCHS = 90 # increase the number of epochs if the training curve indicates that a better result is possible\nVERBOSE = 0 # set to 0 for less output, or to 2 for more output\nSINGLE_FOLD = False # set to True for a quick experiment and to False for full cross-validation\nRUNS = 1 # should be 1. increase the number of runs only if you want see how the result depends on the random seed\nBATCH_SIZE = 1024 # if you set this too high, the notebook will crash (out of memory)\nFOLDS = 10\n\ndef my_model(X):\n    \"\"\"Return a compiled Keras model\"\"\"\n    model = Sequential()\n    model.add(InputLayer(input_shape=(X.shape[-1])))\n\n    # Add the hidden layers\n    for size in [128, 64, 64]:\n        model.add(Dense(size, activation='selu'))\n        model.add(BatchNormalization())\n        #model.add(LayerNormalization()) # LayerNormalization gives a similar score increase as BatchNormalization, but is slower\n        #model.add(Dropout(rate=0.1)) # When I tried dropout, accuracy became worse.\n        \n    # Add the final layer with the correct activation function\n    # Adding kernel_regularizer=tf.keras.regularizers.l2(l2=0.03) didn't make a difference\n    model.add(Dense(len(le.classes_), activation='softmax'))\n    \n    # Compile the model\n    model.compile(optimizer=\"adam\", loss='sparse_categorical_crossentropy', metrics=['acc'])\n    return model\n\n# Make the results reproducible\nnp.random.seed(2021)\ntf.random.set_seed(2021)\n\ntotal_start_time = datetime.now()\nscore_list, test_pred_list, history_list = [], [], []\noof_list = [np.full((len(train_df), len(le.classes_)), -1.0, dtype='float32') for run in range(RUNS)]\nfor run in range(RUNS):\n    kf = StratifiedKFold(n_splits=FOLDS, shuffle=True, random_state=1)\n    for fold, (train_idx, val_idx) in enumerate(kf.split(train_df, y=train_df.Cover_Type)):\n        print(f\"Fold {run}.{fold}\")\n        start_time = datetime.now()\n        X_tr = train_df.iloc[train_idx]\n        sw = X_tr['maxP']\n        del X_tr['maxP']\n        X_va = train_df.iloc[val_idx]\n        y_tr = target[train_idx]\n        y_va = target[val_idx]\n        X_tr = X_tr[features]\n        X_va = X_va[features]\n\n        # Train\n        preproc = StandardScaler() # I tried QuantileTransformer, but StandardScaler seems to be better by 0.005\n        X_tr = preproc.fit_transform(X_tr)\n        X_va = preproc.transform(X_va)\n        model = my_model(X_tr)\n\n        # Define two callbacks: ReduceLROnPlateau, EarlyStopping\n        lr = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, \n                               patience=5, verbose=VERBOSE)\n\n        es = EarlyStopping(monitor=\"val_acc\", patience=10, \n                           verbose=VERBOSE, mode=\"max\", \n                           restore_best_weights=True)\n\n        # Train and save the model\n        history = model.fit(X_tr, y_tr, \n                            validation_data=(X_va, y_va), \n                            epochs=EPOCHS,\n                            verbose=VERBOSE,\n                            batch_size=BATCH_SIZE, \n                            validation_batch_size=len(X_va),\n                            shuffle=True,\n                            callbacks=[lr, es],\n                           sample_weight = sw)\n        history_list.append(history.history)\n        model.save(f\"model{run}.{fold}\")\n        \n        # Inference for validation after last epoch of fold\n        y_va_pred = model.predict(X_va, batch_size=len(X_va))\n        oof_list[run][val_idx] = y_va_pred\n        y_va_pred = np.argmax(y_va_pred, axis=1)\n\n        # Inference for test: keep the predicted probabilities\n        prd = model.predict(preproc.transform(test_df[features]),batch_size=BATCH_SIZE)        \n        test_pred_list.append(prd)\n        prd_mn = np.mean([x.max() for x in prd])\n        # Evaluation\n        accuracy = accuracy_score(y_va, y_va_pred)\n        score_list.append((accuracy, datetime.now() - start_time))\n        print(f\"Fold {run}.{fold} | {str(datetime.now() - start_time)[-12:-7]} | Epochs: {len(history_list[-1]['loss'])} | Accuracy: {accuracy:.5f} | PrdMean: {prd_mn:.5f}\")\n        if run == 0: plot_history(history_list[-1], title=f\"Accuracy: {accuracy:.5f}\")\n\n        \n        # Clean up the memory (it seems that Keras doesn't clean up everything at keyboard interrupts)\n        del model, y_va_pred\n        gc.collect()\n        \n        if SINGLE_FOLD: break\n\n# Save all oof and test predictions to later determine ensemble weights\nwith open('oof_list.pickle', 'wb') as handle: pickle.dump(oof_list, handle)\nwith open('test_pred_list.pickle', 'wb') as handle: pickle.dump(test_pred_list, handle)\n    \ntotal_time = datetime.now() - total_start_time\n","metadata":{"execution":{"iopub.status.busy":"2021-12-15T17:58:18.213944Z","iopub.execute_input":"2021-12-15T17:58:18.214185Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Evaluation","metadata":{}},{"cell_type":"code","source":"# Overall evaluation\nif oof_list[0].min() >= 0: # Can only evaluate if all folds have been done (set SINGLE_FOLD to False)\n    \n    # Evaluate the overall cv score\n    print(f\"Single-model Accuracy: {sum([accuracy_score(train_df.Cover_Type, le.inverse_transform(np.argmax(oof, axis=1))) for oof in oof_list]) / len(oof_list):.5f}\")\n\n    # Evaluate the number of epochs and the time taken\n    print(f\"Average epochs: {sum([len(h['loss']) for h in history_list]) / len(history_list):.0f}\")\n    print(f\"Maximum epochs: {max([len(h['loss']) for h in history_list]) / len(history_list):.0f}\")\n    print(f\"Stopped early in {sum([len(h['loss']) < EPOCHS for h in history_list]) / len(history_list):.0%} of runs\")\n    print(f\"Total elapsed time: {str(total_time)[-14:-7]} for {len(history_list)} trainings\") \n    print()\n\n    # Show the confusion matrix\n    def plot_confusion_matrix(cm, classes, cm_type='recall'):\n        if cm_type == 'recall':\n            cm = cm / cm.sum(axis=1).reshape(-1, 1)\n            colors = cm\n            cell_format = '.0%'\n            plt.title('Confusion matrix (sum of every row is 100 %, diagonal shows recall)', fontweight='bold', pad=15)\n        elif cm_type == 'precision':\n            cm = cm / cm.sum(axis=0).reshape(1, -1)\n            colors = cm\n            cell_format = '.0%'\n            plt.title('Confusion matrix (sum of every column is 100 %, diagonal shows precision)', fontweight='bold', pad=15)\n        elif cm_type == 'accuracy':\n            cm = cm / cm.sum()\n            colors = minmax_scale(cm.reshape(-1, 1)).reshape(cm.shape[0], cm.shape[1]) ** 0.3 # make the low-to-medium cells darker\n            cell_format = '.2%'\n            plt.title('Confusion matrix (sum of matrix is 100 %, sum of diagonal shows accuracy)', fontweight='bold', pad=15)\n        elif cm_type == 'count':\n            colors = minmax_scale(cm.reshape(-1, 1)).reshape(cm.shape[0], cm.shape[1]) ** 0.3 # make the low-to-medium cells darker\n            cell_format = 'd'\n            plt.title('Confusion matrix (sample counts)', fontweight='bold', pad=15)\n        else: raise ValueError(f'Illegal value for parameter cm_type: {cm_type}')\n        plt.imshow(colors, interpolation='nearest', cmap=plt.cm.Blues) # or cmap='hot'\n        #plt.colorbar()\n        tick_marks = np.arange(len(classes))\n        plt.xticks(tick_marks, classes, rotation=0)\n        plt.yticks(tick_marks, classes)\n\n        thresh = colors.max() / 2.\n        for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n            value = cm[i, j]\n            plt.text(j, i, format(value, cell_format),\n                     horizontalalignment=\"center\",\n                     color=\"white\" if colors[i, j] > thresh else \"black\")\n\n        plt.ylabel('True label', fontweight='bold')\n        plt.xlabel('Predicted label', fontweight='bold')\n        plt.tight_layout()\n\n    cm = confusion_matrix(train_df.Cover_Type, le.inverse_transform(np.argmax(oof_list[0], axis=1)))\n    plt.figure(figsize=(11, 9))\n    plot_confusion_matrix(cm, le.inverse_transform(np.arange(len(le.classes_))), cm_type='precision')\n    plt.show()\n    plt.figure(figsize=(11, 9))\n    plot_confusion_matrix(cm, le.inverse_transform(np.arange(len(le.classes_))), cm_type='recall')\n    plt.show()\n    plt.figure(figsize=(11, 9))\n    plot_confusion_matrix(cm, le.inverse_transform(np.arange(len(le.classes_))), cm_type='accuracy')\n    plt.show()\n    plt.figure(figsize=(11, 9))\n    plot_confusion_matrix(cm, le.inverse_transform(np.arange(len(le.classes_))), cm_type='count')\n    plt.show()\n    \n    # Print the classification report\n    print(classification_report(train_df.Cover_Type, le.inverse_transform(np.argmax(oof_list[0], axis=1))))\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create the submission file\nsub = test_df[['Id']].copy()\nsub['Cover_Type'] = le.inverse_transform(np.argmax(sum(test_pred_list), axis=1)) # soft voting by adding the probabilities of all models in the ensemble\nsub.to_csv('submission.csv', index=False)\n\n# Plot the distribution of the test predictions\nplt.figure(figsize=(10,3))\nplt.hist(train_df['Cover_Type'], bins=np.linspace(0.5, 7.5, 8), density=True, label='Train labels')\nplt.hist(sub['Cover_Type'], bins=np.linspace(0.5, 7.5, 8), density=True, rwidth=0.7, label='Test predictions')\nplt.xlabel('Cover_Type')\nplt.ylabel('Frequency')\nplt.gca().yaxis.set_major_formatter(PercentFormatter())\nplt.legend()\nplt.show()\n\nsub.head()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"d = pd.read_pickle(\"test_pred_list.pickle\")\nt = 0\nc = 0\nfor s in d:\n    for r in s:\n        t = t + np.max(r)\n        c = c + 1\nprint(txt, t/c)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now it's your turn: Change the model architecture, the number of epochs, the learning rate ... and see what happens!","metadata":{}}]}