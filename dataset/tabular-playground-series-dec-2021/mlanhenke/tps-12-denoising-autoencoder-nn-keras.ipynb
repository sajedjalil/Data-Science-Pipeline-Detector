{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport gc\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras import backend as K\n\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.metrics import confusion_matrix\n\nfrom warnings import filterwarnings\nfilterwarnings('ignore')\n\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-12-06T16:24:16.432059Z","iopub.execute_input":"2021-12-06T16:24:16.432406Z","iopub.status.idle":"2021-12-06T16:24:22.776501Z","shell.execute_reply.started":"2021-12-06T16:24:16.432322Z","shell.execute_reply":"2021-12-06T16:24:22.775474Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<img src=\"https://i.ibb.co/PWvpT9F/header.png\" alt=\"header\" border=\"0\" width=800 height=300>","metadata":{}},{"cell_type":"markdown","source":"# Introduction","metadata":{}},{"cell_type":"markdown","source":"<div style=\"font-size:110%;line-height:155%\">\n<p>Hi,</p>\n<p>while I'm still enjoying my deep-learning adventure, reading and learning a ton of stuff - I decided it's time to implement different kinds of networks with this month competition. I've been trying to implement a denoising autoencoder since TPS-November, but with last month data I wasn't so sure if it would be any success. This time I finally came around to try my luck with a <b>Denoising Autoencoder</b>. My implementation is heavily based on the <a href=\"https://www.kaggle.com/springmanndaniel/1st-place-turn-your-data-into-daeta\">TPS-January winning solution by Danzel</a>. <p>The idea here is to use an autoencoder to learn more meaningful features by discovering latent variables. The architecture here will create three encoding layers, concatenated and then fed into a fully-connected neural network. To avoid learning the identity-function while blowing up the dimensionality, noise will be injected. In this case I implemented the Swap-Row-Noise function using a noise probability of around 15%.</p>\n<p><em>Disclaimer: I am still testing different setups, since training and finetuning the autoencoder does not seem to be so trivial.</em></p>\n    \n<blockquote><img src=\"https://i.ibb.co/j8n07rn/Deepstack-DAE.png\" width=\"50%\" alt=\"Deepstack-DAE\" border=\"0\"></blockquote>\n    \n<p>Feel free to take a look at my other notebooks, covering some different ideas and architectures:\n    <li><a href=\"https://www.kaggle.com/mlanhenke/tps-12-simple-nn-baseline-keras\">Simple NN Baseline</a></li>\n    <li><a href=\"https://www.kaggle.com/mlanhenke/tps-12-deep-wide-nn-keras\">Deep & Wide NN </a></li>\n    <li><a href=\"https://www.kaggle.com/mlanhenke/tps-12-bn-autoencoder-nn-keras\">Bottleneck Autoencoder</a></li>\n    <li><a href=\"https://www.kaggle.com/mlanhenke/tps-12-deep-cross-nn-keras\">Deep & Cross NN</a></li>\n</p>\n    \n<em>Thank you very much for taking some time to read my notebook. Please leave an upvote if you find any of this information useful.</em>\n</div>","metadata":{}},{"cell_type":"markdown","source":"# Import & Prepare Data","metadata":{}},{"cell_type":"code","source":"# import train & test data\ndf_train = pd.read_csv('../input/tabular-playground-series-dec-2021/train.csv')\ndf_test = pd.read_csv('../input/tabular-playground-series-dec-2021/test.csv')\n\nsample_submission = pd.read_csv('../input/tabular-playground-series-dec-2021/sample_submission.csv')\n\n# drop underrepresented class\ndf_train = df_train[df_train['Cover_Type'] != 5]\n\n# split dataframes for later modeling\nX = df_train.drop(columns=['Id','Cover_Type','Soil_Type7','Soil_Type15','Soil_Type1']).copy()\ny = df_train['Cover_Type'].copy()\n\nX_test = df_test.drop(columns=['Id','Soil_Type7','Soil_Type15','Soil_Type1']).copy()\n\n# create label-encoded one-hot-vector for softmax, mutliclass classification\nle = LabelEncoder()\ntarget = keras.utils.to_categorical(le.fit_transform(y))\n\ndel df_train, df_test\ngc.collect()\n\nprint(X.shape, y.shape, target.shape, X_test.shape)","metadata":{"execution":{"iopub.status.busy":"2021-12-06T16:24:22.778147Z","iopub.execute_input":"2021-12-06T16:24:22.778418Z","iopub.status.idle":"2021-12-06T16:24:50.387095Z","shell.execute_reply.started":"2021-12-06T16:24:22.778387Z","shell.execute_reply":"2021-12-06T16:24:50.386221Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Setup","metadata":{}},{"cell_type":"code","source":"# define helper functions\ndef set_seed(seed):\n    np.random.seed(seed)\n    tf.random.set_seed(seed)\n    print(f\"Seed set to: {seed}\")\n\ndef plot_eval_results(scores, n_splits):\n    cols = 5\n    rows = int(np.ceil(n_splits/cols))\n    \n    fig, ax = plt.subplots(rows, cols, tight_layout=True, figsize=(20,2.5))\n    ax = ax.flatten()\n\n    for fold in range(len(scores)):\n        df_eval = pd.DataFrame({'train_loss': scores[fold]['loss'], 'valid_loss': scores[fold]['val_loss']})\n\n        sns.lineplot(\n            x=df_eval.index,\n            y=df_eval['train_loss'],\n            label='train_loss',\n            ax=ax[fold]\n        )\n\n        sns.lineplot(\n            x=df_eval.index,\n            y=df_eval['valid_loss'],\n            label='valid_loss',\n            ax=ax[fold]\n        )\n\n        ax[fold].set_ylabel('')\n\n    sns.despine()\n\ndef plot_cm(cm):\n    metrics = {\n        'accuracy': cm / cm.sum(),\n        'recall' : cm / cm.sum(axis=1),\n        'precision': cm / cm.sum(axis=0)\n    }\n    \n    fig, ax = plt.subplots(1,3, tight_layout=True, figsize=(15,5))\n    ax = ax.flatten()\n\n    mask = (np.eye(cm.shape[0]) == 0) * 1\n\n    for idx, (name, matrix) in enumerate(metrics.items()):\n\n        ax[idx].set_title(name)\n\n        sns.heatmap(\n            data=matrix,\n            cmap=sns.dark_palette(\"#69d\", reverse=True, as_cmap=True),\n            cbar=False,\n            mask=mask,\n            lw=0.25,\n            annot=True,\n            fmt='.2f',\n            ax=ax[idx]\n        )\n    sns.despine()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-12-06T16:24:50.388485Z","iopub.execute_input":"2021-12-06T16:24:50.389197Z","iopub.status.idle":"2021-12-06T16:24:50.410865Z","shell.execute_reply.started":"2021-12-06T16:24:50.389156Z","shell.execute_reply":"2021-12-06T16:24:50.409511Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# define callbacks\nlr = keras.callbacks.ReduceLROnPlateau(\n    monitor=\"val_loss\", \n    factor=0.5, \n    patience=5, \n    verbose=True\n)\n\nes = keras.callbacks.EarlyStopping(\n    monitor=\"val_loss\", \n    patience=10, \n    verbose=True, \n    mode=\"min\", \n    restore_best_weights=True\n)","metadata":{"_kg_hide-input":false,"execution":{"iopub.status.busy":"2021-12-06T16:24:50.419107Z","iopub.execute_input":"2021-12-06T16:24:50.419664Z","iopub.status.idle":"2021-12-06T16:24:50.429013Z","shell.execute_reply.started":"2021-12-06T16:24:50.419615Z","shell.execute_reply":"2021-12-06T16:24:50.428004Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class SwapRowNoise:\n    def __init__(self, proba):\n        self.proba = proba\n    \n    def apply(self, X):\n        random_idx = np.random.randint(low=0, high=X.shape[0], size=1)[0]\n        swap_matrix = K.random_bernoulli(shape=X.shape, p=self.proba) * tf.ones(shape=X.shape)    \n        corrupted = tf.where(swap_matrix==1, X.iloc[random_idx], X)\n        return corrupted.numpy()\n    \n# create autoencoder\nclass EncodingLayer(layers.Layer):\n    def __init__(self, encoding_dim, activation='relu'):\n        super().__init__()\n        self.enc1 = layers.Dense(encoding_dim, activation)\n        self.enc2 = layers.Dense(encoding_dim, activation)\n        self.enc3 = layers.Dense(encoding_dim, activation)\n        self.concat = layers.Concatenate()\n    \n    def call(self, inputs):\n        enc1 = self.enc1(inputs)\n        enc2 = self.enc2(enc1)\n        enc3 = self.enc3(enc2)\n        merge = self.concat([enc1, enc2, enc3])\n        return merge\n\nclass DecodingLayer(layers.Layer):\n    def __init__(self, num_outputs, activation='linear'):\n        super().__init__()\n        self.dec = layers.Dense(num_outputs, activation)\n    \n    def call(self, inputs):\n        return self.dec(inputs)\n    \nclass AutoEncoder(keras.Model):\n    def __init__(self, encoding_dim, num_outputs, activation='relu'):\n        super().__init__()\n        self.encoder = EncodingLayer(encoding_dim, activation,)\n        self.decoder = DecodingLayer(num_outputs)\n    \n    def call(self, inputs):\n        encoder = self.encoder(inputs)\n        decoder = self.decoder(encoder)\n        return decoder\n    \n    def get_encoder(self):\n        return self.encoder","metadata":{"execution":{"iopub.status.busy":"2021-12-06T16:24:50.430139Z","iopub.execute_input":"2021-12-06T16:24:50.432053Z","iopub.status.idle":"2021-12-06T16:24:50.45248Z","shell.execute_reply.started":"2021-12-06T16:24:50.431834Z","shell.execute_reply":"2021-12-06T16:24:50.450713Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# create custom layer\nclass DenseBlock(layers.Layer):\n    def __init__(self, units, activation='relu', dropout_rate=0, l2=0):\n        super().__init__()\n        self.dense = layers.Dense(\n            units, activation,\n            kernel_regularizer=keras.regularizers.l2(l2)\n        )\n        self.batchn = layers.BatchNormalization()\n        self.dropout = layers.Dropout(dropout_rate)\n    \n    def call(self, inputs):\n        x = self.dense(inputs)\n        x = self.batchn(x)\n        x = self.dropout(x)\n        return x\n\n# create fully-connected NN\nclass MLP(keras.Model):\n    def __init__(self, hidden_layers, autoencoder, activation='relu', dropout_rate=0, l2=0):\n        super().__init__()\n        self.encoder = autoencoder.get_encoder()\n        self.hidden_layers = [DenseBlock(units, activation, l2) for units in hidden_layers]\n        self.softmax = layers.Dense(units=target.shape[-1], activation='softmax')\n        self.concat = layers.Concatenate()\n        \n    def call(self, inputs):\n        x = self.encoder(inputs)\n        for layer in self.hidden_layers:\n            x = layer(x)\n        x = self.softmax(x)\n        return x","metadata":{"execution":{"iopub.status.busy":"2021-12-06T16:24:50.454019Z","iopub.execute_input":"2021-12-06T16:24:50.454492Z","iopub.status.idle":"2021-12-06T16:24:50.470438Z","shell.execute_reply.started":"2021-12-06T16:24:50.454458Z","shell.execute_reply":"2021-12-06T16:24:50.46945Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training","metadata":{}},{"cell_type":"code","source":"try:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect()\n    tf_strategy = tf.distribute.experimental.TPUStrategy(tpu)\n    print(\"Running on TPU:\", tpu.master())\nexcept:\n    tf_strategy = tf.distribute.get_strategy()\n    print(f\"Running on {tf_strategy.num_replicas_in_sync} replicas\")\n    print(\"Number of GPUs Available: \", len(tf.config.list_physical_devices('GPU')))","metadata":{"execution":{"iopub.status.busy":"2021-12-06T16:24:50.471659Z","iopub.execute_input":"2021-12-06T16:24:50.472458Z","iopub.status.idle":"2021-12-06T16:24:56.450587Z","shell.execute_reply.started":"2021-12-06T16:24:50.472416Z","shell.execute_reply":"2021-12-06T16:24:56.44959Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"seed = 2021\nset_seed(seed)\n\nnoise_maker = SwapRowNoise(0.10)\nX_noise = noise_maker.apply(X)\n\ncv = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)\n\npredictions = []\noof_preds = {'y_valid': list(), 'y_hat': list()}\nscores_ae = {fold:None for fold in range(cv.n_splits)}\nscores_nn = {fold:None for fold in range(cv.n_splits)}\n\nfor fold, (idx_train, idx_valid) in enumerate(cv.split(X,y)):\n    X_train, y_train = X.iloc[idx_train], target[idx_train]\n    X_valid, y_valid = X.iloc[idx_valid], target[idx_valid]\n    X_noise_train, X_noise_valid = X_noise[idx_train], X_noise[idx_valid]\n\n    # scale data\n    scl = StandardScaler()\n    X_train = scl.fit_transform(X_train)\n    X_noise_train = scl.transform(X_noise_train)\n    X_valid = scl.transform(X_valid)\n    X_noise_valid = scl.transform(X_noise_valid)\n\n    # train autoencoder\n    with tf_strategy.scope():\n        ae = AutoEncoder(\n            encoding_dim=128,\n            num_outputs=X.shape[-1],\n            activation='relu'\n        )\n\n        ae.compile(\n            optimizer=keras.optimizers.Adam(learning_rate=1e-3),\n            loss=keras.losses.MeanSquaredError()\n        )\n\n    print('_'*65)\n    print(f\"Fold {fold+1} || Autoencoder Training\")\n    print('_'*65)\n\n    history_ae = ae.fit(\n        X_noise_train, X_train,\n        validation_data=(X_noise_valid, X_valid),\n        epochs=500,\n        batch_size=4096,\n        shuffle=True,\n        verbose=False,\n        callbacks=[lr,es]\n    )\n\n    scores_ae[fold] = history_ae.history\n\n    print('_'*65)\n    print(f\"Fold {fold+1} || AE Min Val Loss: {np.min(scores_ae[fold]['val_loss'])}\")\n    print('_'*65)\n\n    # train fully-connected nn\n    with tf_strategy.scope():\n        model = MLP(\n            hidden_layers=[32,32,32],\n            autoencoder=ae,\n            activation='relu'\n        )\n\n        model.compile(\n            optimizer=keras.optimizers.Adam(learning_rate=1e-3),\n            loss=keras.losses.CategoricalCrossentropy(),\n            metrics=['acc']\n        )\n\n    print('_'*65)\n    print(f\"Fold {fold+1} || NN Training\")\n    print('_'*65)\n\n    history_nn = model.fit(\n        X_train, y_train,\n        validation_data=(X_valid, y_valid),\n        epochs=500,\n        batch_size=4096,\n        shuffle=True,\n        verbose=False,\n        callbacks=[lr,es]\n    )\n\n    scores_nn[fold] = history_nn.history\n\n    oof_preds['y_valid'].extend(y.iloc[idx_valid])\n    oof_preds['y_hat'].extend(model.predict(X_valid, batch_size=4096))\n\n    prediction = model.predict(scl.transform(X_test), batch_size=4096)\n    predictions.append(prediction)\n\n    del ae, model, prediction\n    gc.collect()\n    K.clear_session()\n\n    print('_'*65)\n    print(f\"Fold {fold+1} || NN Min Val Loss: {np.min(scores_nn[fold]['val_loss'])}\")\n    print('_'*65)\n\noverall_score_ae = [np.min(scores_ae[fold]['val_loss']) for fold in range(cv.n_splits)]\noverall_score_nn = [np.min(scores_nn[fold]['val_loss']) for fold in range(cv.n_splits)]\n\nprint('_'*65)\nprint(f\"Overall AE Mean Validation Loss: {np.mean(overall_score_ae)} || Overall NN Mean Validation Loss: {np.mean(overall_score_nn)}\")","metadata":{"execution":{"iopub.status.busy":"2021-12-06T16:35:32.022726Z","iopub.execute_input":"2021-12-06T16:35:32.024219Z","iopub.status.idle":"2021-12-06T16:40:58.385439Z","shell.execute_reply.started":"2021-12-06T16:35:32.02414Z","shell.execute_reply":"2021-12-06T16:40:58.383814Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Evaluation & Submission","metadata":{}},{"cell_type":"code","source":"plot_eval_results(scores_nn, cv.n_splits)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-12-06T16:24:56.476575Z","iopub.status.idle":"2021-12-06T16:24:56.477715Z","shell.execute_reply.started":"2021-12-06T16:24:56.477385Z","shell.execute_reply":"2021-12-06T16:24:56.477428Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# prepare oof_predictions\noof_y_true = np.array(oof_preds['y_valid'])\noof_y_hat = le.inverse_transform(np.argmax(oof_preds['y_hat'], axis=1))\n\n# create confusion matrix, calculate accuracy, recall & precision\ncm = pd.DataFrame(data=confusion_matrix(oof_y_true, oof_y_hat, labels=le.classes_), index=le.classes_, columns=le.classes_)\nplot_cm(cm)","metadata":{"execution":{"iopub.status.busy":"2021-12-06T16:24:56.478932Z","iopub.status.idle":"2021-12-06T16:24:56.479427Z","shell.execute_reply.started":"2021-12-06T16:24:56.479161Z","shell.execute_reply":"2021-12-06T16:24:56.479185Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# create final prediction, inverse labels to original classes\nfinal_predictions = le.inverse_transform(np.argmax(sum(predictions), axis=1))\n\nsample_submission['Cover_Type'] = final_predictions\nsample_submission.to_csv('./baseline_nn.csv', index=False)\n\nsns.countplot(final_predictions)\nsns.despine()","metadata":{"execution":{"iopub.status.busy":"2021-12-06T16:24:56.481164Z","iopub.status.idle":"2021-12-06T16:24:56.481504Z","shell.execute_reply.started":"2021-12-06T16:24:56.481329Z","shell.execute_reply":"2021-12-06T16:24:56.481345Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_submission.head()","metadata":{"execution":{"iopub.status.busy":"2021-12-06T16:24:56.483033Z","iopub.status.idle":"2021-12-06T16:24:56.483351Z","shell.execute_reply.started":"2021-12-06T16:24:56.483189Z","shell.execute_reply":"2021-12-06T16:24:56.483204Z"},"trusted":true},"execution_count":null,"outputs":[]}]}