{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport gc\nimport numpy as np\nimport pandas as pd\nimport datatable as dt\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras import backend as K\n\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.metrics import confusion_matrix\n\nfrom warnings import filterwarnings\nfilterwarnings('ignore')\n\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-12-05T19:11:45.524585Z","iopub.execute_input":"2021-12-05T19:11:45.525122Z","iopub.status.idle":"2021-12-05T19:11:45.535766Z","shell.execute_reply.started":"2021-12-05T19:11:45.525069Z","shell.execute_reply":"2021-12-05T19:11:45.534955Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<img src=\"https://i.ibb.co/PWvpT9F/header.png\" alt=\"header\" border=\"0\" width=800 height=300>","metadata":{}},{"cell_type":"markdown","source":"# Introduction","metadata":{}},{"cell_type":"markdown","source":"<div style=\"font-size:110%;line-height:155%\">\n<p>Hi,</p>\n<p>while I'm still enjoying my deep-learning adventure, reading and learning a ton of stuff - I decided it's time to implement different kinds of networks with this month competition. With this notebook I'm trying a <b>Deep & Wide</b> architecture. The idea here is to train a fully-connected dense model while creating a skip connection from the input to the final layer, allowing the gradient to flow more freely to avoid the problem of vanishing-gradient when going deeper in terms of layers.\n    \n<blockquote><img src=\"https://i.ibb.co/hZdY4j5/DNN-WIDE-DEEP.png\" width=\"35%\" alt=\"DNN-WIDE-DEEP\" border=\"0\"></blockquote>\n    \n<p>Feel free to take a look at my other notebooks, covering some different ideas and architectures:\n    <li><a href=\"https://www.kaggle.com/mlanhenke/tps-12-simple-nn-baseline-keras\">Simple NN Baseline</a></li>\n    <li><a href=\"https://www.kaggle.com/mlanhenke/tps-12-deep-cross-nn-keras\">Deep & Cross NN </a></li>\n    <li><a href=\"https://www.kaggle.com/mlanhenke/tps-12-bn-autoencoder-nn-keras\">Bottleneck Autoencoder</a></li>\n    <li><a href=\"https://www.kaggle.com/mlanhenke/tps-12-denoising-autoencoder-nn-keras\">Deepstack Denoising Autoencoder</a></li>\n</p>\n    \n<em>Thank you very much for taking some time to read my notebook. Please leave an upvote if you find any of this information useful.</em>\n</div>","metadata":{}},{"cell_type":"markdown","source":"# Import & Prepare Data","metadata":{}},{"cell_type":"code","source":"%%time\n# import train & test data\ndf_train = dt.fread('../input/tabular-playground-series-dec-2021/train.csv').to_pandas()\ndf_test = dt.fread('../input/tabular-playground-series-dec-2021/test.csv').to_pandas()\n\nsample_submission = pd.read_csv('../input/tabular-playground-series-dec-2021/sample_submission.csv')\n\n# drop underrepresented class\ndf_train = df_train[df_train['Cover_Type'] != 5]\n\n# apply feature-engineering\n# thanks to https://www.kaggle.com/c/tabular-playground-series-dec-2021/discussion/293373\ndf_train['Aspect'][df_train['Aspect'] < 0] += 360 \ndf_train['Aspect'][df_train['Aspect'] > 359] -= 360\ndf_test['Aspect'][df_test['Aspect'] < 0] += 360 \ndf_test['Aspect'][df_test['Aspect'] > 359] -= 360\n\ndf_train.loc[df_train[\"Hillshade_9am\"] < 0, \"Hillshade_9am\"] = 0\ndf_test.loc[df_test[\"Hillshade_9am\"] < 0, \"Hillshade_9am\"] = 0\n\ndf_train.loc[df_train[\"Hillshade_Noon\"] < 0, \"Hillshade_Noon\"] = 0\ndf_test.loc[df_test[\"Hillshade_Noon\"] < 0, \"Hillshade_Noon\"] = 0\n\ndf_train.loc[df_train[\"Hillshade_3pm\"] < 0, \"Hillshade_3pm\"] = 0\ndf_test.loc[df_test[\"Hillshade_3pm\"] < 0, \"Hillshade_3pm\"] = 0\n\ndf_train.loc[df_train[\"Hillshade_9am\"] > 255, \"Hillshade_9am\"] = 255\ndf_test.loc[df_test[\"Hillshade_9am\"] > 255, \"Hillshade_9am\"] = 255\n\ndf_train.loc[df_train[\"Hillshade_Noon\"] > 255, \"Hillshade_Noon\"] = 255\ndf_test.loc[df_test[\"Hillshade_Noon\"] > 255, \"Hillshade_Noon\"] = 255\n\ndf_train.loc[df_train[\"Hillshade_3pm\"] > 255, \"Hillshade_3pm\"] = 255\ndf_test.loc[df_test[\"Hillshade_3pm\"] > 255, \"Hillshade_3pm\"] = 255\n\n# split dataframes for later modeling\nX = df_train.drop(columns=['Id','Cover_Type','Soil_Type7','Soil_Type15','Soil_Type1']).copy()\ny = df_train['Cover_Type'].copy()\n\nX_test = df_test.drop(columns=['Id','Soil_Type7','Soil_Type15','Soil_Type1']).copy()\n\n# create label-encoded one-hot-vector for softmax, mutliclass classification\nle = LabelEncoder()\ntarget = keras.utils.to_categorical(le.fit_transform(y))\n\nprint(X.shape, y.shape, target.shape, X_test.shape)","metadata":{"execution":{"iopub.status.busy":"2021-12-05T19:11:45.537419Z","iopub.execute_input":"2021-12-05T19:11:45.537741Z","iopub.status.idle":"2021-12-05T19:12:53.330447Z","shell.execute_reply.started":"2021-12-05T19:11:45.537705Z","shell.execute_reply":"2021-12-05T19:12:53.329649Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Setup","metadata":{}},{"cell_type":"code","source":"# define helper functions\ndef set_seed(seed):\n    np.random.seed(seed)\n    tf.random.set_seed(seed)\n    print(f\"Seed set to: {seed}\")\n\ndef plot_eval_results(scores, n_splits):\n    cols = 5\n    rows = int(np.ceil(n_splits/cols))\n    \n    fig, ax = plt.subplots(rows, cols, tight_layout=True, figsize=(20,2.5))\n    ax = ax.flatten()\n\n    for fold in range(len(scores)):\n        df_eval = pd.DataFrame({'train_loss': scores[fold]['loss'], 'valid_loss': scores[fold]['val_loss']})\n\n        sns.lineplot(\n            x=df_eval.index,\n            y=df_eval['train_loss'],\n            label='train_loss',\n            ax=ax[fold]\n        )\n\n        sns.lineplot(\n            x=df_eval.index,\n            y=df_eval['valid_loss'],\n            label='valid_loss',\n            ax=ax[fold]\n        )\n\n        ax[fold].set_ylabel('')\n\n    sns.despine()\n\ndef plot_cm(cm):\n    metrics = {\n        'accuracy': cm / cm.sum(),\n        'recall' : cm / cm.sum(axis=1),\n        'precision': cm / cm.sum(axis=0)\n    }\n    \n    fig, ax = plt.subplots(1,3, tight_layout=True, figsize=(15,5))\n    ax = ax.flatten()\n\n    mask = (np.eye(cm.shape[0]) == 0) * 1\n\n    for idx, (name, matrix) in enumerate(metrics.items()):\n\n        ax[idx].set_title(name)\n\n        sns.heatmap(\n            data=matrix,\n            cmap=sns.dark_palette(\"#69d\", reverse=True, as_cmap=True),\n            cbar=False,\n            mask=mask,\n            lw=0.25,\n            annot=True,\n            fmt='.2f',\n            ax=ax[idx]\n        )\n    sns.despine()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-12-05T19:12:53.331758Z","iopub.execute_input":"2021-12-05T19:12:53.332043Z","iopub.status.idle":"2021-12-05T19:12:53.345408Z","shell.execute_reply.started":"2021-12-05T19:12:53.332006Z","shell.execute_reply":"2021-12-05T19:12:53.344578Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# define callbacks\nlr = keras.callbacks.ReduceLROnPlateau(\n    monitor=\"val_loss\", \n    factor=0.5, \n    patience=5, \n    verbose=True\n)\n\nes = keras.callbacks.EarlyStopping(\n    monitor=\"val_acc\", \n    patience=10, \n    verbose=True, \n    mode=\"max\", \n    restore_best_weights=True\n)","metadata":{"_kg_hide-input":false,"execution":{"iopub.status.busy":"2021-12-05T19:12:53.346693Z","iopub.execute_input":"2021-12-05T19:12:53.346975Z","iopub.status.idle":"2021-12-05T19:12:53.377693Z","shell.execute_reply.started":"2021-12-05T19:12:53.346924Z","shell.execute_reply":"2021-12-05T19:12:53.376907Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# create custom dense-block\nclass DenseBlock(layers.Layer):\n    def __init__(self, units, activation='relu', dropout_rate=0, l2=0):\n        super().__init__()\n        self.dense = layers.Dense(\n            units, \n            activation,\n            kernel_initializer=\"lecun_normal\", \n            kernel_regularizer=keras.regularizers.l2(l2)\n        )\n        self.batchn = layers.BatchNormalization()\n        self.dropout = layers.Dropout(dropout_rate)\n    \n    def call(self, inputs):\n        x = self.dense(inputs)\n        x = self.batchn(x)\n        x = self.dropout(x)\n        return x\n    \n# create dense & wide model\nclass WideNet(keras.Model):\n    def __init__(self, hidden_layers, activation='relu', dropout_rate=0, l2=0):\n        super().__init__()\n        self.dense_layers = [\n            DenseBlock(units, activation, l2)\n            for units in hidden_layers\n        ]\n        self.concat = layers.Concatenate()\n        self.batchn = layers.BatchNormalization()\n        self.softmax = layers.Dense(units=target.shape[-1], activation='softmax')\n        \n    def call(self, inputs):\n        dense, wide = inputs, inputs\n        for dense_layer in self.dense_layers:\n            dense = dense_layer(dense)\n        wide = self.batchn(wide)\n        merged = self.concat([dense, wide])\n        return self.softmax(merged)","metadata":{"execution":{"iopub.status.busy":"2021-12-05T19:12:53.380495Z","iopub.execute_input":"2021-12-05T19:12:53.380758Z","iopub.status.idle":"2021-12-05T19:12:53.391568Z","shell.execute_reply.started":"2021-12-05T19:12:53.380724Z","shell.execute_reply":"2021-12-05T19:12:53.390907Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training","metadata":{}},{"cell_type":"code","source":"try:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect()\n    tf_strategy = tf.distribute.experimental.TPUStrategy(tpu)\n    print(\"Running on TPU:\", tpu.master())\nexcept:\n    tf_strategy = tf.distribute.get_strategy()\n    print(f\"Running on {tf_strategy.num_replicas_in_sync} replicas\")\n    print(\"Number of GPUs Available: \", len(tf.config.list_physical_devices('GPU')))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"seed = 2021\nset_seed(seed)\n\ncv = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)\n\npredictions = []\noof_preds = {'y_valid': list(), 'y_hat': list()}\nscores_nn = {fold:None for fold in range(cv.n_splits)}\n\nfor fold, (idx_train, idx_valid) in enumerate(cv.split(X,y)):\n    X_train, y_train = X.iloc[idx_train], target[idx_train]\n    X_valid, y_valid = X.iloc[idx_valid], target[idx_valid]\n\n    # scale data\n    scl = StandardScaler()\n    X_train = scl.fit_transform(X_train)\n    X_valid = scl.transform(X_valid)\n    \n    with tf_strategy.scope():\n        model = WideNet(\n            hidden_layers=[128, 64, 64, 32, 16],\n            activation='selu',\n            dropout_rate=0.25\n        )\n\n        model.compile(\n            optimizer=keras.optimizers.Adam(learning_rate=1e-3),\n            loss=keras.losses.CategoricalCrossentropy(),\n            metrics=['acc']\n        )\n        \n    history = model.fit(\n        X_train, y_train,\n        validation_data=(X_valid, y_valid),\n        epochs=500,\n        batch_size=4096,\n        shuffle=True,\n        verbose=False,\n        callbacks=[lr,es]\n    )\n    \n    scores_nn[fold] = history.history\n    \n    oof_preds['y_valid'].extend(y.iloc[idx_valid])\n    oof_preds['y_hat'].extend(model.predict(X_valid, batch_size=4096))\n    \n    prediction = model.predict(scl.transform(X_test), batch_size=4096)\n    predictions.append(prediction)\n    \n    del model, prediction\n    gc.collect()\n    K.clear_session()\n    \n    print('_'*65)\n    print(f\"Fold {fold+1} || Min Val Loss: {np.min(scores_nn[fold]['val_loss'])}\")\n    \noverall_score = [np.min(scores_nn[fold]['val_loss']) for fold in range(cv.n_splits)]\nprint('_'*65)\nprint(f\"Overall Mean Validation Loss: {np.mean(overall_score)}\")","metadata":{"execution":{"iopub.status.busy":"2021-12-05T19:12:53.392687Z","iopub.execute_input":"2021-12-05T19:12:53.393186Z","iopub.status.idle":"2021-12-05T19:13:57.771703Z","shell.execute_reply.started":"2021-12-05T19:12:53.393149Z","shell.execute_reply":"2021-12-05T19:13:57.770798Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Evaluation & Submission","metadata":{}},{"cell_type":"code","source":"plot_eval_results(scores_nn, cv.n_splits)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-12-05T19:13:57.772797Z","iopub.status.idle":"2021-12-05T19:13:57.773685Z","shell.execute_reply.started":"2021-12-05T19:13:57.773422Z","shell.execute_reply":"2021-12-05T19:13:57.773451Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# prepare oof_predictions\noof_y_true = np.array(oof_preds['y_valid'])\noof_y_hat = le.inverse_transform(np.argmax(oof_preds['y_hat'], axis=1))\n\n# create confusion matrix, calculate accuracy, recall & precision\ncm = pd.DataFrame(data=confusion_matrix(oof_y_true, oof_y_hat, labels=le.classes_), index=le.classes_, columns=le.classes_)\nplot_cm(cm)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-12-05T19:13:57.774766Z","iopub.status.idle":"2021-12-05T19:13:57.775719Z","shell.execute_reply.started":"2021-12-05T19:13:57.775477Z","shell.execute_reply":"2021-12-05T19:13:57.775501Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#create final prediction, inverse labels to original classes\nfinal_predictions = le.inverse_transform(np.argmax(sum(predictions), axis=1))\n\nsample_submission['Cover_Type'] = final_predictions\nsample_submission.to_csv('./baseline_nn.csv', index=False)\n\nsns.countplot(final_predictions)\nsns.despine()","metadata":{"execution":{"iopub.status.busy":"2021-12-05T19:13:57.777694Z","iopub.status.idle":"2021-12-05T19:13:57.778255Z","shell.execute_reply.started":"2021-12-05T19:13:57.778015Z","shell.execute_reply":"2021-12-05T19:13:57.77804Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_submission.head()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-12-05T19:13:57.779635Z","iopub.status.idle":"2021-12-05T19:13:57.780224Z","shell.execute_reply.started":"2021-12-05T19:13:57.779989Z","shell.execute_reply":"2021-12-05T19:13:57.780014Z"},"trusted":true},"execution_count":null,"outputs":[]}]}