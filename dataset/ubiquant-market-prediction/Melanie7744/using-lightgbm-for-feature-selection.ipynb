{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport time\nimport gc\n\nimport sklearn\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error\nprint(\"scikit-learn version: {}\". format(sklearn.__version__))\n\nimport lightgbm as lgb\nfrom lightgbm import early_stopping, log_evaluation, record_evaluation\nprint(\"LightGBM version:  {}\".format(lgb.__version__))\n\nfrom scipy.stats import pearsonr\nimport matplotlib.pyplot as plt\nimport pickle","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-output":true,"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-02-18T07:27:18.256881Z","iopub.execute_input":"2022-02-18T07:27:18.257223Z","iopub.status.idle":"2022-02-18T07:27:18.265185Z","shell.execute_reply.started":"2022-02-18T07:27:18.257187Z","shell.execute_reply":"2022-02-18T07:27:18.264254Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Feature Selection with LightGBM\nThe Ubiquant competition's training data is huge. Besides using one of the reduced-size data sets, e.g. [this parquet version](http://www.kaggle.com/robikscube/ubiquant-parquet?select=train_low_mem.parquet) or [this pickle version](http://www.kaggle.com/lonnieqin/ubiquant-market-prediction-half-precision-pickle), feature reduction will be helpful in speeding up training. And faster training means faster iteration & more experiments you can run, right?\n\nIn this notebook I will show how to use LightGBM's build-in feature importance ranking in order to reduce the total number of features and compare the training times.","metadata":{}},{"cell_type":"code","source":"# read the training data\ndf_train = pd.read_pickle('../input/ubiquant-market-prediction-half-precision-pickle/train.pkl')","metadata":{"execution":{"iopub.status.busy":"2022-02-18T07:27:18.267233Z","iopub.execute_input":"2022-02-18T07:27:18.267568Z","iopub.status.idle":"2022-02-18T07:27:28.883634Z","shell.execute_reply.started":"2022-02-18T07:27:18.267524Z","shell.execute_reply":"2022-02-18T07:27:28.882411Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"display(df_train.shape)\ndisplay(df_train.info())\ndf_train.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-18T07:27:28.88515Z","iopub.execute_input":"2022-02-18T07:27:28.885397Z","iopub.status.idle":"2022-02-18T07:27:28.949915Z","shell.execute_reply.started":"2022-02-18T07:27:28.885367Z","shell.execute_reply":"2022-02-18T07:27:28.948897Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"first_time_id_to_use = 500 \nfeatures_to_use= [col for col in df_train.columns if col.startswith(\"f\")] # use only the anonymised features\ntime_id_to_split_train_and_val = 1000\n\n\ndf_train = df_train.loc[df_train.time_id >= first_time_id_to_use]\nprint(\"df_train.shape: \",df_train.shape)\n\nX_train = df_train.loc[df_train.time_id < time_id_to_split_train_and_val]\nX_val = df_train.loc[df_train.time_id >= time_id_to_split_train_and_val]\ny_train = X_train.target\ny_val = X_val.target\nX_train = X_train[features_to_use]\nX_val = X_val[features_to_use]\nprint(\"X_train.shape:  \", X_train.shape)\nprint(\"X_val.shape:    \", X_val.shape)\n#print(\"Features used: \", list(X_train.columns))","metadata":{"execution":{"iopub.status.busy":"2022-02-18T07:27:28.952509Z","iopub.execute_input":"2022-02-18T07:27:28.95303Z","iopub.status.idle":"2022-02-18T07:27:44.884533Z","shell.execute_reply.started":"2022-02-18T07:27:28.952981Z","shell.execute_reply":"2022-02-18T07:27:44.883385Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del df_train #free up memory\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-02-18T07:27:44.88594Z","iopub.execute_input":"2022-02-18T07:27:44.886258Z","iopub.status.idle":"2022-02-18T07:27:45.199082Z","shell.execute_reply.started":"2022-02-18T07:27:44.886228Z","shell.execute_reply":"2022-02-18T07:27:45.198214Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# create lgbm datasets\ndtrain = lgb.Dataset(X_train, label=y_train)\ndval = lgb.Dataset(X_val, label=y_val)","metadata":{"execution":{"iopub.status.busy":"2022-02-18T07:27:45.200404Z","iopub.execute_input":"2022-02-18T07:27:45.201266Z","iopub.status.idle":"2022-02-18T07:27:45.21316Z","shell.execute_reply.started":"2022-02-18T07:27:45.201196Z","shell.execute_reply":"2022-02-18T07:27:45.212264Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# these parameters not tuned yet\nlgb_params = {'objective': 'regression',\n    'metric': 'MSE',\n    'boosting_type': 'gbdt',\n    'lambda_l1': 2.3e-05,\n    'lambda_l2': 0.1,\n    'num_leaves': 4,\n    'feature_fraction': 0.5,\n    'bagging_fraction': 0.9,\n    'bagging_freq': 7,\n    'min_child_samples': 20,\n    'num_iterations': 1000\n             }","metadata":{"execution":{"iopub.status.busy":"2022-02-18T07:27:45.214487Z","iopub.execute_input":"2022-02-18T07:27:45.215072Z","iopub.status.idle":"2022-02-18T07:27:45.226493Z","shell.execute_reply.started":"2022-02-18T07:27:45.215033Z","shell.execute_reply":"2022-02-18T07:27:45.225394Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ts = time.time()\n\nmetric_over_time = {} # dict for logging the evaluation metrics\n\nmodel = lgb.train(        \n        lgb_params, \n        dtrain, \n        valid_sets=[dtrain, dval],\n        valid_names=['train','val'],\n        callbacks=[early_stopping(100), log_evaluation(100), record_evaluation(metric_over_time)]\n    )\n\n\nexecution_time = time.time() - ts\nprint(\"\\nTraining time: \" + str(round(execution_time,3)) + \"s\")","metadata":{"execution":{"iopub.status.busy":"2022-02-18T07:27:45.227567Z","iopub.execute_input":"2022-02-18T07:27:45.228053Z","iopub.status.idle":"2022-02-18T07:30:41.3078Z","shell.execute_reply.started":"2022-02-18T07:27:45.227996Z","shell.execute_reply":"2022-02-18T07:30:41.306985Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_val_hat = model.predict(X_val)\n\n# using MSE as a proxy for pearson corellation (https://www.kaggle.com/c/ubiquant-market-prediction/discussion/302181)\nprint(\"MSE:  \", mean_squared_error(y_val, y_val_hat, squared=True))\nprint(\"RMSE: \", mean_squared_error(y_val, y_val_hat, squared=False))","metadata":{"execution":{"iopub.status.busy":"2022-02-18T07:30:41.312497Z","iopub.execute_input":"2022-02-18T07:30:41.315088Z","iopub.status.idle":"2022-02-18T07:30:48.474601Z","shell.execute_reply.started":"2022-02-18T07:30:41.315023Z","shell.execute_reply":"2022-02-18T07:30:48.473921Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# check the competition metric\ncorr, _ = pearsonr(y_val_hat, y_val)\nprint(\"Pearson Correlation Coeficient Validation Data: \", corr)","metadata":{"execution":{"iopub.status.busy":"2022-02-18T07:30:48.475693Z","iopub.execute_input":"2022-02-18T07:30:48.476022Z","iopub.status.idle":"2022-02-18T07:30:48.511476Z","shell.execute_reply.started":"2022-02-18T07:30:48.475994Z","shell.execute_reply":"2022-02-18T07:30:48.510239Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here we can see how the metric improves as the training goes on. The mean squared error, also called square loss or l2, is decreasing steadily.","metadata":{}},{"cell_type":"code","source":"lgb.plot_metric(metric_over_time, figsize=(10,5))\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-18T07:30:48.513633Z","iopub.execute_input":"2022-02-18T07:30:48.51438Z","iopub.status.idle":"2022-02-18T07:30:48.742937Z","shell.execute_reply.started":"2022-02-18T07:30:48.514328Z","shell.execute_reply":"2022-02-18T07:30:48.742326Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"LightGBM has a nice build in function for plotting the feature importance. Feature importance can be displayed as \"gain\", showing the total gains of splits which use the feature, or \"split\", showing the numbers of times the feature is used in a model. \n\nIn the graph below, we can see that there are feature which are of little importance to LightGBM. So I will retrain without them.","metadata":{}},{"cell_type":"code","source":"# let's look at which features lgbm deems important\nlgb.plot_importance(model, figsize=(10,40), importance_type='gain', max_num_features=300) # importance_type: gain/split: V7 has 'split'\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-18T07:30:48.744374Z","iopub.execute_input":"2022-02-18T07:30:48.744611Z","iopub.status.idle":"2022-02-18T07:30:52.409916Z","shell.execute_reply.started":"2022-02-18T07:30:48.74458Z","shell.execute_reply":"2022-02-18T07:30:52.408612Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# there are features with little importance, get rid of them\nimp = pd.DataFrame({'Value':model.feature_importance(importance_type='gain'),'Feature':X_train.columns}).sort_values(by=\"Value\",ascending=False).reset_index(drop=True)\n\n#imp.Value.value_counts()\nimp = imp[imp.Value>100]  # remove all features with gain lower than 100\nnew_feature_list = list(imp.Feature)\nprint(\"Number of features, new: \", len(new_feature_list))","metadata":{"execution":{"iopub.status.busy":"2022-02-18T07:36:13.805181Z","iopub.execute_input":"2022-02-18T07:36:13.8055Z","iopub.status.idle":"2022-02-18T07:36:13.816467Z","shell.execute_reply.started":"2022-02-18T07:36:13.80547Z","shell.execute_reply":"2022-02-18T07:36:13.815361Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# save model to disk, it will take up approx. 263kB\nfilename = 'finalized_model.sav'\npickle.dump(model, open(filename, 'wb'))","metadata":{"execution":{"iopub.status.busy":"2022-02-18T07:36:34.19467Z","iopub.execute_input":"2022-02-18T07:36:34.194993Z","iopub.status.idle":"2022-02-18T07:36:34.212215Z","shell.execute_reply.started":"2022-02-18T07:36:34.194959Z","shell.execute_reply":"2022-02-18T07:36:34.211409Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del model,y_val_hat,dtrain,dval,imp # free up memory","metadata":{"execution":{"iopub.status.busy":"2022-02-18T07:36:38.248677Z","iopub.execute_input":"2022-02-18T07:36:38.248985Z","iopub.status.idle":"2022-02-18T07:36:38.255281Z","shell.execute_reply.started":"2022-02-18T07:36:38.248956Z","shell.execute_reply":"2022-02-18T07:36:38.254604Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now it's time to retrain with the reduced feature set.","metadata":{}},{"cell_type":"code","source":"dtrain = lgb.Dataset(X_train[new_feature_list], label=y_train)\ndval = lgb.Dataset(X_val[new_feature_list], label=y_val)\n\nts = time.time()\n\nmodel = lgb.train(        \n        lgb_params, \n        dtrain, \n        valid_sets=[dtrain, dval],\n        valid_names=['train','val'],\n        callbacks=[early_stopping(100), log_evaluation(100), record_evaluation(metric_over_time)]\n    )\n\n\nexecution_time = time.time() - ts\nprint(\"\\nTraining time: \" + str(round(execution_time,3)) + \"s\")","metadata":{"execution":{"iopub.status.busy":"2022-02-18T07:36:40.427194Z","iopub.execute_input":"2022-02-18T07:36:40.427708Z","iopub.status.idle":"2022-02-18T07:38:31.000983Z","shell.execute_reply.started":"2022-02-18T07:36:40.427642Z","shell.execute_reply":"2022-02-18T07:38:31.000173Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# before\n# Training time: 176.065s !!!!","metadata":{"execution":{"iopub.status.busy":"2022-02-18T07:17:09.628036Z","iopub.execute_input":"2022-02-18T07:17:09.630643Z","iopub.status.idle":"2022-02-18T07:17:09.634592Z","shell.execute_reply.started":"2022-02-18T07:17:09.63059Z","shell.execute_reply":"2022-02-18T07:17:09.633902Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_val_hat = model.predict(X_val[new_feature_list])\n\n# using MSE as a proxy for pearson corellation (https://www.kaggle.com/c/ubiquant-market-prediction/discussion/302181)\nprint(\"MSE:  \", mean_squared_error(y_val, y_val_hat, squared=True))\nprint(\"RMSE: \", mean_squared_error(y_val, y_val_hat, squared=False))","metadata":{"execution":{"iopub.status.busy":"2022-02-18T07:38:31.002996Z","iopub.execute_input":"2022-02-18T07:38:31.003526Z","iopub.status.idle":"2022-02-18T07:38:37.341579Z","shell.execute_reply.started":"2022-02-18T07:38:31.003485Z","shell.execute_reply":"2022-02-18T07:38:37.340538Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# before\n# MSE:   0.8055005324507704\n# RMSE:  0.8974968147301529","metadata":{"execution":{"iopub.status.busy":"2022-02-18T07:17:14.979444Z","iopub.execute_input":"2022-02-18T07:17:14.979849Z","iopub.status.idle":"2022-02-18T07:17:14.982833Z","shell.execute_reply.started":"2022-02-18T07:17:14.979818Z","shell.execute_reply":"2022-02-18T07:17:14.98207Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"corr, _ = pearsonr(y_val_hat, y_val)\nprint(\"Pearson Correlation Coeficient Validation Data: \", corr)","metadata":{"execution":{"iopub.status.busy":"2022-02-18T07:38:37.342682Z","iopub.execute_input":"2022-02-18T07:38:37.34293Z","iopub.status.idle":"2022-02-18T07:38:37.382778Z","shell.execute_reply.started":"2022-02-18T07:38:37.3429Z","shell.execute_reply":"2022-02-18T07:38:37.381646Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# before\n# Pearson Correlation Coeficient Validation Data:  0.12600954147083407","metadata":{"execution":{"iopub.status.busy":"2022-02-18T07:17:15.028999Z","iopub.execute_input":"2022-02-18T07:17:15.030103Z","iopub.status.idle":"2022-02-18T07:17:15.037144Z","shell.execute_reply.started":"2022-02-18T07:17:15.03005Z","shell.execute_reply":"2022-02-18T07:17:15.036081Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So while MSE, RSME and Pearson Correlation Coeficient are comparable between LightGBM using all 300 features and LightGBM using only the more important features, the training time is greatly reduced! ðŸ¥³","metadata":{}},{"cell_type":"code","source":"# save model to disk, it will take up approx. 205kB\nfilename = 'finalized_model_reduced.sav'\npickle.dump(model, open(filename, 'wb'))","metadata":{"execution":{"iopub.status.busy":"2022-02-18T07:38:37.388016Z","iopub.execute_input":"2022-02-18T07:38:37.392338Z","iopub.status.idle":"2022-02-18T07:38:37.432944Z","shell.execute_reply.started":"2022-02-18T07:38:37.392266Z","shell.execute_reply":"2022-02-18T07:38:37.431827Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Submit\nI make the submissions using the second model.\n\nFor more details on the submission proccess, you can check my other notebook: [Understanding the submission API - for newbies](http://www.kaggle.com/melanie7744/understanding-the-submission-api-for-newbies).","metadata":{}},{"cell_type":"code","source":"def preprocess(df, features):\n    df = df[features]  \n    return df\n    \ndef make_predictions(model, df): # using a function here really only makes sense if you use multiple models for prediction and average their results\n    pred = model.predict(df)\n    return pred","metadata":{"execution":{"iopub.status.busy":"2022-02-18T07:38:53.958911Z","iopub.execute_input":"2022-02-18T07:38:53.959189Z","iopub.status.idle":"2022-02-18T07:38:53.964885Z","shell.execute_reply.started":"2022-02-18T07:38:53.959161Z","shell.execute_reply":"2022-02-18T07:38:53.963921Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import ubiquant\nenv = ubiquant.make_env()   # initialize the environment\niter_test = env.iter_test()    # an iterator which loops over the test set and sample submission\nfor (test_df, sample_prediction_df) in iter_test:\n    test_df = preprocess(test_df, new_feature_list) \n    sample_prediction_df['target'] = make_predictions(model, test_df)  # make your predictions here\n    env.predict(sample_prediction_df)   # register your predictions\n ","metadata":{"execution":{"iopub.status.busy":"2022-02-18T07:38:56.26045Z","iopub.execute_input":"2022-02-18T07:38:56.261448Z","iopub.status.idle":"2022-02-18T07:38:56.295736Z","shell.execute_reply.started":"2022-02-18T07:38:56.261401Z","shell.execute_reply":"2022-02-18T07:38:56.294549Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}