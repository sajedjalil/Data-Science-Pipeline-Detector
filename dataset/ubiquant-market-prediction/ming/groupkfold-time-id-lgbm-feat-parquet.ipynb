{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **âœ” Data Loading & Import**","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport gc\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom tensorflow.keras import layers\nfrom tensorflow import keras\nfrom scipy import stats\nfrom pathlib import Path\nimport seaborn as sns","metadata":{"execution":{"iopub.status.busy":"2022-02-07T02:14:46.954873Z","iopub.execute_input":"2022-02-07T02:14:46.955244Z","iopub.status.idle":"2022-02-07T02:14:54.183701Z","shell.execute_reply.started":"2022-02-07T02:14:46.955148Z","shell.execute_reply":"2022-02-07T02:14:54.182639Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Reading as Parquet Low Memory (Fast & Low Mem Use)**\n- https://www.kaggle.com/robikscube/fast-data-loading-and-low-mem-with-parquet-files","metadata":{}},{"cell_type":"code","source":"%%time\nn_features = 300\nfeatures = [f'f_{i}' for i in range(n_features)]\ntrain = pd.read_parquet('../input/ubiquant-parquet/train_low_mem.parquet')","metadata":{"execution":{"iopub.status.busy":"2022-02-06T14:20:09.100604Z","iopub.execute_input":"2022-02-06T14:20:09.101179Z","iopub.status.idle":"2022-02-06T14:20:46.290442Z","shell.execute_reply.started":"2022-02-06T14:20:09.101144Z","shell.execute_reply":"2022-02-06T14:20:46.28976Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"display(train.info())\ndisplay(train.head())","metadata":{"execution":{"iopub.status.busy":"2022-02-06T14:20:46.292904Z","iopub.execute_input":"2022-02-06T14:20:46.293245Z","iopub.status.idle":"2022-02-06T14:20:46.36018Z","shell.execute_reply.started":"2022-02-06T14:20:46.293201Z","shell.execute_reply":"2022-02-06T14:20:46.359276Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"start_mem = train.memory_usage().sum() / 1024**2\n\nfor col in train.columns:\n    col_type = train[col].dtype\n\n    if col_type != object:\n        c_min = train[col].min()\n        c_max = train[col].max()\n        if str(col_type)[:3] == 'int':\n            if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                train[col] = train[col].astype(np.int8)\n            elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                train[col] = train[col].astype(np.int16)\n            elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                train[col] = train[col].astype(np.int32)\n            elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                train[col] = train[col].astype(np.int64)  \n        else:\n            if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                train[col] = train[col].astype(np.float16)\n            elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                train[col] = train[col].astype(np.float32)\n            else:\n                train[col] = train[col].astype(np.float64)\n    else:\n        train[col] = train[col].astype('category')\n\nend_mem = train.memory_usage().sum() / 1024**2\nprint('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\nprint('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))","metadata":{"execution":{"iopub.status.busy":"2022-02-06T14:20:46.36378Z","iopub.execute_input":"2022-02-06T14:20:46.364086Z","iopub.status.idle":"2022-02-06T14:23:58.215885Z","shell.execute_reply.started":"2022-02-06T14:20:46.364051Z","shell.execute_reply":"2022-02-06T14:23:58.214916Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **ðŸŒ  Simple Insight**","metadata":{}},{"cell_type":"markdown","source":"Thanks for https://www.kaggle.com/lucamassaron/eda-target-analysis","metadata":{}},{"cell_type":"code","source":"obs_by_asset = train.groupby(['investment_id'])['target'].count()\n\nfig, ax = plt.subplots(1, 1, figsize=(12, 6))\nobs_by_asset.plot.hist(bins=60)\nplt.title(\"target by asset distribution\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-06T14:23:58.217179Z","iopub.execute_input":"2022-02-06T14:23:58.217416Z","iopub.status.idle":"2022-02-06T14:23:58.692762Z","shell.execute_reply.started":"2022-02-06T14:23:58.217388Z","shell.execute_reply":"2022-02-06T14:23:58.692155Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mean_target = train.groupby(['investment_id'])['target'].mean()\nmean_mean_target = np.mean(mean_target)\n\nfig, ax = plt.subplots(1, 1, figsize=(12, 6))\nmean_target.plot.hist(bins=60)\nplt.title(\"mean target distribution\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-06T14:23:58.693764Z","iopub.execute_input":"2022-02-06T14:23:58.694483Z","iopub.status.idle":"2022-02-06T14:23:59.119389Z","shell.execute_reply.started":"2022-02-06T14:23:58.694438Z","shell.execute_reply":"2022-02-06T14:23:59.118762Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"std_target = train.groupby(['investment_id'])['target'].std()\nstd_std_target = np.mean(std_target)\n\nfig, ax = plt.subplots(1, 1, figsize=(12,6))\nstd_target.plot.hist(bins=60)\nplt.title(\"std target distribution\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-06T14:23:59.120761Z","iopub.execute_input":"2022-02-06T14:23:59.121049Z","iopub.status.idle":"2022-02-06T14:23:59.544467Z","shell.execute_reply.started":"2022-02-06T14:23:59.121007Z","shell.execute_reply":"2022-02-06T14:23:59.543652Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"time2target_mean = train.groupby(['time_id'])['target'].mean()\ntime2target_std = train.groupby(['time_id'])['target'].std()\n\n_, axes = plt.subplots(1, 1, figsize=(24, 12))\nplt.fill_between(\n        time2target_mean.index,\n        time2target_mean - time2target_std,\n        time2target_mean + time2target_std,\n        alpha=0.1,\n        color=\"b\",\n    )\nplt.plot(\n        time2target_mean.index, time2target_mean, \"o-\", color=\"b\", label=\"Training score\"\n    )\nplt.axhline(y=mean_mean_target, color='r', linestyle='--', label=\"mean\")\naxes.set_ylabel(\"target\")\naxes.set_xlabel(\"time\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-06T14:23:59.546012Z","iopub.execute_input":"2022-02-06T14:23:59.546493Z","iopub.status.idle":"2022-02-06T14:24:00.093242Z","shell.execute_reply.started":"2022-02-06T14:23:59.546435Z","shell.execute_reply":"2022-02-06T14:24:00.092226Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ax = sns.jointplot(x=obs_by_asset, y=mean_target, kind=\"reg\", \n                   height=8, joint_kws={'line_kws':{'color':'red'}})\nax.ax_joint.set_xlabel('observations')\nax.ax_joint.set_ylabel('mean target')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-06T14:24:00.094994Z","iopub.execute_input":"2022-02-06T14:24:00.09554Z","iopub.status.idle":"2022-02-06T14:24:01.47207Z","shell.execute_reply.started":"2022-02-06T14:24:00.095474Z","shell.execute_reply":"2022-02-06T14:24:01.469779Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Target values on a specific time have a lot Volatility.  \nthe less the observations, imply a much more uncertainty in the mean target  \n**Hypothesis : Each time has their own rules or pattern.**\n\n","metadata":{}},{"cell_type":"markdown","source":"\" **Strategy**: now your cv strategy should be clear, you have to do groupkfold on the time_id,  \nkeeping all the assets realtive to a time_id or in train or in validation \"  -LUCA MASSARON(https://www.kaggle.com/lucamassaron) -","metadata":{}},{"cell_type":"markdown","source":"# **ðŸ‘˜ Pipeline & Modeling(LGBM)**","metadata":{}},{"cell_type":"markdown","source":"Thank you for https://www.kaggle.com/lonnieqin/ubiquant-market-prediction-with-dnn","metadata":{}},{"cell_type":"code","source":"train.drop(['row_id'], axis=1, inplace=True)\ntime = train.pop('time_id')\ny = train.pop('target')\ndisplay(train.head())\ndisplay(y.head())","metadata":{"execution":{"iopub.status.busy":"2022-02-06T14:29:40.110981Z","iopub.execute_input":"2022-02-06T14:29:40.111274Z","iopub.status.idle":"2022-02-06T14:29:44.695398Z","shell.execute_reply.started":"2022-02-06T14:29:40.111245Z","shell.execute_reply":"2022-02-06T14:29:44.694531Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **ðŸƒâ€â™‚ï¸ Learning**","metadata":{}},{"cell_type":"code","source":"%%time\nfrom sklearn.model_selection import GroupKFold\nimport lightgbm as lgb\nn_splits = 5\nGKF = GroupKFold(n_splits = 3)\nparams = {'learning_rate': 0.01, \n          'max_depth': 5, \n          'objective': 'regression', \n          'metric': 'mse', \n          'is_training_metric': True, \n          'num_leaves': 144}\n\nmodels = []\nfor index, (train_indices, valid_indices) in enumerate(GKF.split(train, y, groups=time)):\n    X_train, X_val = train.iloc[train_indices], train.iloc[valid_indices]\n    y_train, y_val = y.iloc[train_indices], y.iloc[valid_indices]\n    train_ds = lgb.Dataset(X_train, label = y_train)\n    valid_ds = lgb.Dataset(X_val, label = y_val)\n    checkpoint = keras.callbacks.ModelCheckpoint(f\"model_{index}\", save_best_only=True)\n    early_stop = keras.callbacks.EarlyStopping(patience=10)\n    print('complete 1')\n    model = lgb.train(params, train_ds, 200, valid_ds)\n    models.append(model)\n    print('complete 2')\n    pearson_score = stats.pearsonr(model.predict(X_val).ravel(), y_val.values)[0]\n    print('Pearson:', pearson_score)\n    del X_train\n    del X_val\n    del y_train\n    del y_val\n    del train_ds\n    del valid_ds\n    gc.collect()\n    break","metadata":{"execution":{"iopub.status.busy":"2022-02-06T14:45:58.789447Z","iopub.execute_input":"2022-02-06T14:45:58.790272Z","iopub.status.idle":"2022-02-06T14:48:57.024612Z","shell.execute_reply.started":"2022-02-06T14:45:58.790212Z","shell.execute_reply":"2022-02-06T14:48:57.023942Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **ðŸ‘‹ Submission**","metadata":{}},{"cell_type":"code","source":"def inference(models, ds):\n    y_preds = []\n    for model in models:\n        y_pred = model.predict(ds)\n        y_preds.append(y_pred)\n    return np.mean(y_preds, axis=0)","metadata":{"execution":{"iopub.status.busy":"2022-02-06T09:28:33.653185Z","iopub.status.idle":"2022-02-06T09:28:33.653524Z","shell.execute_reply.started":"2022-02-06T09:28:33.653339Z","shell.execute_reply":"2022-02-06T09:28:33.653362Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import ubiquant\nenv = ubiquant.make_env()\niter_test = env.iter_test() \nfor (test_df, sample_prediction_df) in iter_test:\n    test_df.drop(['row_id'], axis=1, inplace=True)\n    sample_prediction_df['target'] = inference(models, test_df)\n    env.predict(sample_prediction_df) ","metadata":{"execution":{"iopub.status.busy":"2022-02-06T09:28:33.65465Z","iopub.status.idle":"2022-02-06T09:28:33.654977Z","shell.execute_reply.started":"2022-02-06T09:28:33.654807Z","shell.execute_reply":"2022-02-06T09:28:33.654828Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}