{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"lunana  \nlast update 2022 04 26  \nã‚†ã£ãã‚Šã—ã¦ã„ã£ã¦ã­ï¼","metadata":{}},{"cell_type":"markdown","source":"version 6 +Trends","metadata":{}},{"cell_type":"markdown","source":"# Trends  \nUbiquantã‚³ãƒ³ãƒšã¯ã€Optiverã‚³ãƒ³ãƒšã«ä¼¼ã¦ã„ã¾ã™ã€‚  \nhttps://www.kaggle.com/c/optiver-realized-volatility-prediction  \ntraining timelineã¨forcasting timelineã§è¨“ç·´ãƒ‡ãƒ¼ã‚¿ãŒå¤‰ã‚ã‚‹ã‹ã‚‚ã—ã‚Œãªã„ã€‚  \nnotebookãŒå®Ÿè¡Œã•ã‚Œã‚‹ã”ã¨ã«ã€ãƒ¢ãƒ‡ãƒ«è¨“ç·´ãŒå¿…è¦ã‹ã‚‚ã—ã‚Œã¾ã›ã‚“ã€‚  \nãã“ã§ã€åŠ¹æžœæ¤œè¨¼ç”¨ã®notebookã¨æœ€çµ‚æå‡ºç”¨ã®notebookï¼’ã¤ã‚’ä½œã£ã¦ã„ãã¾ã™ã€‚  \n\nThe Ubiquant competition is similar to the Optiver competition.  \nhttps://www.kaggle.com/c/optiver-realized-volatility-prediction  \nTraining data may change between training timeline and forcasting timeline.  \nModel training may be required each time the notebook is run.  \nTherefore, we will create two notebooks, one for effect verification and the other for final submission.  \n\n## EDA  \nhttps://www.kaggle.com/lunapandachan/ubiquant-eda-english\n\n## Add test  \n* ðŸ”¥Ubiquant DNN chiranjeev  \nversion 5ã€€seed=41, 5folds,LB=0.148  \nversion 7ã€€seed=41, 10folds, LB=0.149  \nversion 8ã€€seed=41, 10folds, LB=0.149  \n","metadata":{}},{"cell_type":"markdown","source":"# Score  \nversion 1ã€€val_correlation LB=0.1522  \nversion 2ã€€val_mae LB=0.1496  \nversion 3ã€€val_loss LB=0.1481  \nversion 4ã€€val_mape LB=0.1495  \nversion 5ã€€val_rmse LB=0.1492  \n\næ¤œè¨¼ã—ãŸçµæžœã€version1ã®val_correlationãŒä¸€ç•ªé«˜ã„ã‚¹ã‚³ã‚¢ã§ã—ãŸã€‚","metadata":{}},{"cell_type":"markdown","source":"**éœŠå¤¢:ä»Šæ—¥ã¯Lonnieã•ã‚“ã®ã‚³ãƒ¼ãƒ‰ã‚’è¿½è©¦ã™ã‚‹ã‚ˆã€‚**\n\n**Reimu: Today I'll retest Lonnie's code.**  \n\nhttps://www.kaggle.com/lonnieqin/ubiquant-market-prediction-with-dnn","metadata":{}},{"cell_type":"markdown","source":"ç¾æ™‚ç‚¹ã§ã¯ã€supplemental_train.csvã‚’ä½¿ç”¨ã™ã‚‹ã¨ã€Submission Scoring ErrorãŒå‡ºã¾ã™ã€‚ã“ã‚Œã¯ã€ãƒŽã‚¤ã‚ºãƒ‡ãƒ¼ã‚¿ã®ãŸã‚ã¨è€ƒãˆã¾ã™ã€‚4æœˆ18æ—¥ä»¥é™ã¯æ­£ã—ã„suplemental_train.csvã«ãªã‚‹ã‚ˆã†ãªã®ã§ã€ã“ã‚Œã«æœŸå¾…ã—ã¦ã€ã‚¹ã‚¤ãƒƒãƒã‚’ã¤ã‘ã‚‹ã“ã¨ã«ã—ã¾ã—ãŸã€‚ 4æœˆ17æ—¥ä»¥é™ã«æå‡ºã™ã‚‹ã¨ãã«ã¯ã€æ³¨æ„ã—ã¦ãã ã•ã„ã€‚","metadata":{}},{"cell_type":"code","source":"import datetime\ndt_now = datetime.datetime.now()\nIS_SUP=False\nif dt_now.month>5 or dt_now.day>16:\n    IS_SUP=True","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# CFG","metadata":{}},{"cell_type":"code","source":"class CFG:\n    n_folds=5\n    train_filename='../input/ubiquant-market-prediction-half-precision-pickle/train.pkl'\n    s_filename='../input/ubiquant-market-prediction/supplemental_train.csv'\n    train_dataset_path='../input/ubiquant-tfrecords/'","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## import","metadata":{}},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\nimport gc\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom tensorflow.keras import layers\nfrom tensorflow import keras\nfrom scipy import stats\nimport random\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom scipy.stats import *\nimport warnings\n\nimport time\nimport pickle\nimport lightgbm as lgb\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.python.ops import math_ops\nfrom tensorflow.python.keras import backend as K","metadata":{"papermill":{"duration":7.781864,"end_time":"2022-01-25T15:39:09.265726","exception":false,"start_time":"2022-01-25T15:39:01.483862","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-03-21T14:15:05.231054Z","iopub.execute_input":"2022-03-21T14:15:05.23183Z","iopub.status.idle":"2022-03-21T14:15:10.548349Z","shell.execute_reply.started":"2022-03-21T14:15:05.231609Z","shell.execute_reply":"2022-03-21T14:15:10.547529Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\nn_features = 300\nfeatures = [f'f_{i}' for i in range(n_features)]\ndtype_features={'time_id':int,'investment_id':int,'target':np.float32}\nusecols=['time_id','investment_id','target']\nfor feature in features:\n    dtype_features[feature]= np.float32\n    usecols.append(feature)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if IS_SUP:\n    f=open(CFG.s_filename,'r')\n    strain_file=open('s_train.csv', 'w')\n    flg_first=True\n    index_f=None\n    count =0\n    rows=[]\n    p_count=0\n    for row in f:\n        if flg_first:\n            index_f=row\n            strain_file.write(index_f)\n            flg_first=False\n        else:\n            if count<200000:\n                count+=1\n                strain_file.write(row)\n            else:\n                count=0\n                strain_file.write(row)\n                strain_file.close()\n                ## procedure s_train to df\n                s_train=pd.read_csv(f's_train.csv',usecols=usecols,dtype=dtype_features)\n                s_train.to_parquet(f's_train.parquet{p_count}')\n                p_count+=1                   \n                !rm s_train.csv\n                del s_train\n                gc.collect()\n                ! free -m\n                strain_file=open('s_train.csv', 'w')\n                strain_file.write(index_f)            \n    f.close()\n    strain_file.close()\n    df=pd.read_csv(f's_train.csv',usecols=usecols,dtype=dtype_features)\n    !rm s_train.csv\n    for idx in range(p_count):\n        s_train = pd.read_parquet(f's_train.parquet{idx}')\n        df=pd.concat([df,s_train])\n        del s_train\n        !rm s_train.parquet{idx}\n    gc.collect()\n    !free -m\n    df = df.reset_index(drop=True)\n    df.to_pickle(f's_train.pkl')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\nimport gc\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nimport json\nimport numpy as np\nfrom scipy.special import comb\nfrom itertools import combinations\n\nclass CombinatorialPurgedGroupKFold():\n    def __init__(self, n_splits = 6, n_test_splits = 2, purge = 1, pctEmbargo = 0.01, **kwargs):\n        self.n_splits = n_splits\n        self.n_test_splits = n_test_splits\n        self.purge = purge\n        self.pctEmbargo = pctEmbargo\n        \n    def split(self, X, y = None, groups = None):\n        if groups is None:\n            raise ValueError(\n                \"The 'groups' parameter should not be None\")\n            \n        u, ind = np.unique(groups, return_index = True)\n        unique_groups = u[np.argsort(ind)]\n        n_groups = len(unique_groups)\n        group_dict = {}\n        for idx in range(len(X)):\n            if groups[idx] in group_dict:\n                group_dict[groups[idx]].append(idx)\n            else:\n                group_dict[groups[idx]] = [idx]\n                \n        n_folds = comb(self.n_splits, self.n_test_splits, exact = True)\n        if n_folds > n_groups:\n            raise ValueError(\n                (\"Cannot have number of folds={0} greater than\"\n                 \" the number of groups={1}\").format(n_folds,\n                                                     n_groups))\n            \n        mbrg = int(n_groups * self.pctEmbargo)\n        if mbrg < 0:\n            raise ValueError(\n                \"The number of 'embargoed' groups should not be negative\")\n        \n        split_dict = {}\n        group_test_size = n_groups // self.n_splits\n        for split in range(self.n_splits):\n            if split == self.n_splits - 1:\n                split_dict[split] = unique_groups[int(split * group_test_size):].tolist()\n            else:\n                split_dict[split] = unique_groups[int(split * group_test_size):int((split + 1) * group_test_size)].tolist()\n        \n        for test_splits in combinations(range(self.n_splits), self.n_test_splits):\n            test_groups = []\n            banned_groups = []\n            for split in test_splits:\n                test_groups += split_dict[split]\n                banned_groups += unique_groups[split_dict[split][0] - self.purge:split_dict[split][0]].tolist()\n                banned_groups += unique_groups[split_dict[split][-1] + 1:split_dict[split][-1] + self.purge + mbrg + 1].tolist()\n            train_groups = [i for i in unique_groups if (i not in banned_groups) and (i not in test_groups)]\n\n            train_idx = []\n            test_idx = []\n            for train_group in train_groups:\n                train_idx += group_dict[train_group]\n            for test_group in test_groups:\n                test_idx += group_dict[test_group]\n            yield train_idx, test_idx","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if IS_SUP:\n    investment_id = df.pop(\"investment_id\")\n    time_id = df.pop(\"time_id\")\n    investment_id.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if IS_SUP:\n    y = df.pop(\"target\")\n    y.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_record(i):\n    dic = {}\n    dic[f\"features\"] = tf.train.Feature(float_list=tf.train.FloatList(value=list(df.iloc[i])))\n    dic[\"time_id\"] = tf.train.Feature(int64_list=tf.train.Int64List(value=[time_id.iloc[i]]))\n    dic[\"investment_id\"] = tf.train.Feature(int64_list=tf.train.Int64List(value=[investment_id.iloc[i]]))\n    dic[\"target\"] = tf.train.Feature(float_list=tf.train.FloatList(value=[y.iloc[i]]))\n    record_bytes = tf.train.Example(features=tf.train.Features(feature=dic)).SerializeToString()\n    return record_bytes\n    \ndef decode_function(record_bytes):\n  return tf.io.parse_single_example(\n      # Data\n      record_bytes,\n      # Schema\n      {\n          \"features\": tf.io.FixedLenFeature([300], dtype=tf.float32),\n          \"time_id\": tf.io.FixedLenFeature([], dtype=tf.int64),\n          \"investment_id\": tf.io.FixedLenFeature([], dtype=tf.int64),\n          \"target\": tf.io.FixedLenFeature([], dtype=tf.float32)\n      }\n  )","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nif IS_SUP:\n    import time\n    n_splits = CFG.n_folds\n    n_test_splits = 1\n    kfold = CombinatorialPurgedGroupKFold(n_splits, n_test_splits)\n    for fold, (train_indices, test_indices) in enumerate(kfold.split(df, groups=time_id)):\n        print(\"=\" * 100)\n        print(f\"Fold {fold}\")\n        print(\"=\" * 100)\n        print(\"Train Sample size:\", len(test_indices))\n        train_save_path = f\"s_train_fold{fold}.tfrecords\"\n        begin = time.time()\n        print(f\"Creating {train_save_path}\")\n        with tf.io.TFRecordWriter(train_save_path) as file_writer:\n            for i in test_indices:\n                file_writer.write(create_record(i))\n        print(\"Elapsed time: %.2f\"%(time.time() - begin))\n    del df,time_id,y\n    gc.collect()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!free -m","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model DNN","metadata":{}},{"cell_type":"code","source":"train = pd.read_pickle(CFG.train_filename)\nif IS_SUP:\n    investment_id_2 = train.pop(\"investment_id\")\n    investment_id=pd.concat([investment_id,investment_id_2])\nelse:\n    investment_id = train.pop(\"investment_id\")    \ndel train\ngc.collect()\ninvestment_id.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\ninvestment_id=investment_id.astype(np.int16)\ninvestment_ids = list(investment_id.unique())\ninvestment_id_size = len(investment_ids) + 1\nwith tf.device(\"cpu\"):\n    investment_id_lookup_layer = layers.IntegerLookup(max_tokens=investment_id_size)\n    investment_id_lookup_layer.adapt(pd.DataFrame({\"investment_ids\":investment_ids}))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Make Tensorflow dataset","metadata":{"papermill":{"duration":0.018846,"end_time":"2022-01-25T15:39:30.567495","exception":false,"start_time":"2022-01-25T15:39:30.548649","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def decode_function(record_bytes):\n  return tf.io.parse_single_example(\n      # Data\n      record_bytes,\n      # Schema\n      {\n          \"features\": tf.io.FixedLenFeature([300], dtype=tf.float32),\n          \"time_id\": tf.io.FixedLenFeature([], dtype=tf.int64),\n          \"investment_id\": tf.io.FixedLenFeature([], dtype=tf.int64),\n          \"target\": tf.io.FixedLenFeature([], dtype=tf.float32)\n      }\n  )\ndef preprocess(item):\n    return (item[\"investment_id\"], item[\"features\"]), item[\"target\"]\ndef make_dataset(file_paths, batch_size=4096, mode=\"train\"):\n    ds = tf.data.TFRecordDataset(file_paths)\n    ds = ds.map(decode_function)\n    ds = ds.map(preprocess)\n    #if mode == \"train\":\n     #   ds = ds.shuffle(batch_size * 4)\n    ds = ds.batch(batch_size).cache().prefetch(tf.data.AUTOTUNE)\n    return ds","metadata":{"papermill":{"duration":0.02858,"end_time":"2022-01-25T15:39:30.614302","exception":false,"start_time":"2022-01-25T15:39:30.585722","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-03-21T14:15:12.982927Z","iopub.execute_input":"2022-03-21T14:15:12.98333Z","iopub.status.idle":"2022-03-21T14:15:12.992256Z","shell.execute_reply.started":"2022-03-21T14:15:12.983289Z","shell.execute_reply":"2022-03-21T14:15:12.99158Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Modeling","metadata":{"papermill":{"duration":0.017564,"end_time":"2022-01-25T15:39:30.64918","exception":false,"start_time":"2022-01-25T15:39:30.631616","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def correlation(x, y, axis=-2):\n    \"\"\"Metric returning the Pearson correlation coefficient of two tensors over some axis, default -2.\"\"\"\n    x = tf.convert_to_tensor(x)\n    y = math_ops.cast(y, x.dtype)\n    n = tf.cast(tf.shape(x)[axis], x.dtype)\n    xsum = tf.reduce_sum(x, axis=axis)\n    ysum = tf.reduce_sum(y, axis=axis)\n    xmean = xsum / n\n    ymean = ysum / n\n    xvar = tf.reduce_sum( tf.math.squared_difference(x, xmean), axis=axis)\n    yvar = tf.reduce_sum( tf.math.squared_difference(y, ymean), axis=axis)\n    cov = tf.reduce_sum( (x - xmean) * (y - ymean), axis=axis)\n    corr = cov / tf.sqrt(xvar * yvar)\n    return tf.constant(1.0, dtype=x.dtype) - corr\n\ndef get_model():\n    investment_id_inputs = tf.keras.Input((1, ), dtype=tf.uint16)\n    features_inputs = tf.keras.Input((300, ), dtype=tf.float16)\n    \n    investment_id_x = investment_id_lookup_layer(investment_id_inputs)\n    investment_id_x = layers.Embedding(investment_id_size, 32, input_length=1)(investment_id_x)\n    investment_id_x = layers.Reshape((-1, ))(investment_id_x)\n    investment_id_x = layers.Dense(64, activation='swish')(investment_id_x)\n    investment_id_x = layers.Dropout(0.1)(investment_id_x)\n    investment_id_x = layers.Dense(64, activation='swish')(investment_id_x)\n    investment_id_x = layers.Dropout(0.1)(investment_id_x)\n    investment_id_x = layers.Dense(64, activation='swish')(investment_id_x)\n    investment_id_x = layers.Dropout(0.1)(investment_id_x)\n    \n    feature_x = layers.Dense(256, activation='swish')(features_inputs)\n    feature_x = layers.Dropout(0.1)(feature_x)\n    feature_x = layers.Dense(256, activation='swish')(feature_x)\n    feature_x = layers.Dropout(0.1)(feature_x)\n    feature_x = layers.Dense(256, activation='swish')(feature_x)\n    feature_x = layers.Dropout(0.1)(feature_x)\n    \n    x = layers.Concatenate(axis=1)([investment_id_x, feature_x])\n    x = layers.Dense(512, activation='swish', kernel_regularizer=\"l2\")(x)\n    x = layers.Dropout(0.1)(x)\n    x = layers.Dense(128, activation='swish', kernel_regularizer=\"l2\")(x)\n    x = layers.Dropout(0.1)(x)\n    x = layers.Dense(32, activation='swish', kernel_regularizer=\"l2\")(x)\n    x = layers.Dropout(0.1)(x)\n    output = layers.Dense(1)(x)\n    rmse = keras.metrics.RootMeanSquaredError(name=\"rmse\")\n    model = tf.keras.Model(inputs=[investment_id_inputs, features_inputs], outputs=[output])\n    model.compile(optimizer=tf.optimizers.Adam(0.001), loss='mse', metrics=['mse', \"mae\", \"mape\", rmse, correlation])\n    return model","metadata":{"papermill":{"duration":0.033569,"end_time":"2022-01-25T15:39:30.700649","exception":false,"start_time":"2022-01-25T15:39:30.66708","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-03-21T14:15:12.994568Z","iopub.execute_input":"2022-03-21T14:15:12.994903Z","iopub.status.idle":"2022-03-21T14:15:13.01192Z","shell.execute_reply.started":"2022-03-21T14:15:12.994869Z","shell.execute_reply":"2022-03-21T14:15:13.01117Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's take a look at this Model's architecture.","metadata":{}},{"cell_type":"code","source":"model = get_model()\nmodel.summary()\nkeras.utils.plot_model(model, show_shapes=True)","metadata":{"papermill":{"duration":0.209912,"end_time":"2022-01-25T15:39:30.929112","exception":false,"start_time":"2022-01-25T15:39:30.7192","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-03-21T14:15:13.013083Z","iopub.execute_input":"2022-03-21T14:15:13.013489Z","iopub.status.idle":"2022-03-21T14:15:14.276268Z","shell.execute_reply.started":"2022-03-21T14:15:13.013454Z","shell.execute_reply":"2022-03-21T14:15:14.275452Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model Training","metadata":{}},{"cell_type":"markdown","source":"**é­”ç†æ²™:ã“ã“ã‹ã‚‰ãŒtrainingã ãœã€‚**  \n**Marisa: Training starts here.**","metadata":{}},{"cell_type":"code","source":"import time","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_fold(index):\n    train_path = []\n    valid_path = []\n    for i in range(CFG.n_folds):\n        if i==index:\n            valid_path.append(f\"{CFG.train_dataset_path}train_fold{i}.tfrecords\")\n            if IS_SUP:\n                valid_path.append(f\"s_train_fold{i}.tfrecords\")\n        else:\n            train_path.append(f\"{CFG.train_dataset_path}train_fold{i}.tfrecords\")\n            if IS_SUP:\n                train_path.append(f\"s_train_fold{i}.tfrecords\")\n\n    train_ds = make_dataset(train_path)\n    valid_ds = make_dataset(valid_path, mode=\"valid\")\n    print(f' fold{index}  ')\n    model = get_model()\n    checkpoint = keras.callbacks.ModelCheckpoint(f\"model_{index}.tf\", monitor=\"val_correlation\", mode=\"min\", save_best_only=True, save_weights_only=True)\n    early_stop = keras.callbacks.EarlyStopping(patience=10)\n    history = model.fit(train_ds, epochs=30, validation_data=valid_ds, callbacks=[checkpoint, early_stop])\n    model.load_weights(f\"model_{index}.tf\")\n    model.save(f'model_{index}')\n    #for metric in [\"loss\", \"mae\", \"mape\", \"rmse\", \"correlation\"]:\n    #    pd.DataFrame(history.history, columns=[metric, f\"val_{metric}\"]).plot()\n    #    plt.title(metric.upper())\n    #    plt.show()\n    #y_vals = []\n    #for _, y in valid_ds:\n    #    y_vals += list(y.numpy().reshape(-1))\n    #y_val = np.array(y_vals)\n    #pearson_score = stats.pearsonr(model.predict(valid_ds).reshape(-1), y_val)[0]\n    #print(f\"Pearson Score: {pearson_score}ã€€fold{index} \")\n    del model,train_ds,valid_ds\n    K.clear_session()\n    gc.collect()\n    #time.sleep(10)\n    !free -m","metadata":{"papermill":{"duration":471.71946,"end_time":"2022-01-25T15:47:23.749584","exception":false,"start_time":"2022-01-25T15:39:32.030124","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-03-21T14:15:14.277684Z","iopub.execute_input":"2022-03-21T14:15:14.277962Z","iopub.status.idle":"2022-03-21T14:20:53.079395Z","shell.execute_reply.started":"2022-03-21T14:15:14.277927Z","shell.execute_reply":"2022-03-21T14:20:53.078671Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for index in range(CFG.n_folds):\n    train_fold(index)\n\nprint(\"load all models\")\nmodels = []\n\nfor index in range(CFG.n_folds):\n    model = get_model()\n    model.load_weights(f\"model_{index}\")\n    models.append(model)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Submission","metadata":{"papermill":{"duration":2.11441,"end_time":"2022-01-25T15:47:27.649127","exception":false,"start_time":"2022-01-25T15:47:25.534717","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def preprocess_test(investment_id, feature):\n    return (investment_id, feature), 0\ndef make_test_dataset(feature, investment_id, batch_size=1024):\n    ds = tf.data.Dataset.from_tensor_slices(((investment_id, feature)))\n    ds = ds.map(preprocess_test)\n    ds = ds.batch(batch_size).cache().prefetch(tf.data.AUTOTUNE)\n    return ds\ndef inference(models, ds):\n    y_preds = []\n    for model in models:\n        y_pred = model.predict(ds)\n        y_preds.append(y_pred)\n    return np.mean(y_preds, axis=0)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import ubiquant\nenv = ubiquant.make_env()\niter_test = env.iter_test() \nfeatures = [f\"f_{i}\" for i in range(300)]\nfor (test_df, sample_prediction_df) in iter_test:\n    ds = make_test_dataset(test_df[features], test_df[\"investment_id\"])\n    sample_prediction_df['target'] = inference(models, ds)\n    env.predict(sample_prediction_df) ","metadata":{"papermill":{"duration":2.234161,"end_time":"2022-01-25T15:47:35.24214","exception":false,"start_time":"2022-01-25T15:47:33.007979","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-03-21T14:20:53.099328Z","iopub.execute_input":"2022-03-21T14:20:53.099666Z","iopub.status.idle":"2022-03-21T14:20:53.8352Z","shell.execute_reply.started":"2022-03-21T14:20:53.099633Z","shell.execute_reply":"2022-03-21T14:20:53.834479Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission=pd.read_csv(\"./submission.csv\")\nsubmission","metadata":{},"execution_count":null,"outputs":[]}]}