{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"![](https://drive.google.com/uc?id=1fI0ySFEBL9eUfAJjsta27PEF1neZ6DqZ)\n\n\nFluctuations are common in the financial market irrespective of the investment strategy which has been incoporated . Risks and returns differ based on investment types and other factors affect the stability and volatility of the market . Investment professionals estimate the overall returns taking in to account the market fluctuations . AI based algorithms are predominantly being used in financial market trading and data science has huge potential to help improve quantitative researchers ability to forecast an investment's return\n\n\n# **<span style=\"color:#F7B2B0;\">Goal</span>**\n \nThe goal of this competition is to build a model that forecasts an investment's return rate.\n\n# **<span style=\"color:#F7B2B0;\">Data</span>**\n\nThe WiDS Datathon 2022 focuses on a prediction task involving roughly 100k observations of building energy usage records collected over 7 years and a number of states within the United States. The dataset consists of building characteristics (e.g. floor area, facility type etc), weather data for the location of the building (e.g. annual average temperature, annual total precipitation etc) as well as the energy usage for the building and the given year, measured as Site Energy Usage Intensity (Site EUI). Each row in the data corresponds to the a single building observed in a given year. Your task is to predict the Site EUI for each row, given the characteristics of the building and the weather data for the location of the building.\n\n\n**Files**\n\n`train.csv`\n\n`row_id -` A unique identifier for the row.\n\n`time_id -` The ID code for the time the data was gathered. The time IDs are in order, but the real time between the time IDs is not constant and will likely be shorter for the final private test set than in the training set.\n\n`investment_id`- The ID code for an investment. Not all investment have data in all time IDs.\n\n`target -` The target.\n\n`[f_0:f_299] -` Anonymized features generated from market data.\n\n`example_test.csv` - Random data provided to demonstrate what shape and format of data the API will deliver to your notebook when you submit.\n\n`example_sample_submission.csv` - An example submission file provided so the publicly accessible copy of the API provides the correct data shape and format.\n\n`ubiquant/` - The image delivery API that will serve the test set. You may need Python 3.7 and a Linux environment to run the example test set through the API offline without errors.\n\n`Time-series API Details - The API serves the data in batches, with all of rows for a single time time_id per batch.`\n\n\n# **<span style=\"color:#F7B2B0;\">Evaluation Metric</span>**\n\nThe evaluation metric for this competition is `Pearson correlation coefficient` \n\n","metadata":{}},{"cell_type":"markdown","source":"<img src=\"https://camo.githubusercontent.com/dd842f7b0be57140e68b2ab9cb007992acd131c48284eaf6b1aca758bfea358b/68747470733a2f2f692e696d6775722e636f6d2f52557469567a482e706e67\">\n\n> I will be integrating W&B for visualizations and logging artifacts!\n> \n> [ProbabilisticBNN](https://wandb.ai/usharengaraju/ProbabilisticBNN)\n> \n> - To get the API key, create an account in the [website](https://wandb.ai/site) .\n> - Use secrets to use API Keys more securely ","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nimport math\nimport numpy as np\nimport pandas as pd\nimport warnings\nimport wandb\n\nfrom wandb.keras import WandbCallback\n\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport random\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\nimport tensorflow_probability as tfp\n\nfrom tensorflow.keras.layers import StringLookup\n\n#ignore warnings\nwarnings.filterwarnings(\"ignore\")\n%matplotlib inline","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-01-24T23:47:04.358333Z","iopub.execute_input":"2022-01-24T23:47:04.358641Z","iopub.status.idle":"2022-01-24T23:47:04.369814Z","shell.execute_reply.started":"2022-01-24T23:47:04.358604Z","shell.execute_reply":"2022-01-24T23:47:04.368836Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"try:\n    from kaggle_secrets import UserSecretsClient\n    user_secrets = UserSecretsClient()\n    secret_value_0 = user_secrets.get_secret(\"api_key\")\n    wandb.login(key=secret_value_0)\n    anony=None\nexcept:\n    anony = \"must\"\n    print('If you want to use your W&B account, go to Add-ons -> Secrets and provide your W&B access token. Use the Label name as wandb_api. \\nGet your W&B access token from here: https://wandb.ai/authorize')\n    \nCONFIG = dict(competition = 'ProbabilisticBNN',_wandb_kernel = 'tensorgirl')","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-01-24T21:06:27.805855Z","iopub.execute_input":"2022-01-24T21:06:27.806963Z","iopub.status.idle":"2022-01-24T21:06:28.618194Z","shell.execute_reply.started":"2022-01-24T21:06:27.806927Z","shell.execute_reply":"2022-01-24T21:06:28.617365Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_pickle('../input/ump-train-picklefile/train.pkl')\n#train = train.sample(10000)","metadata":{"execution":{"iopub.status.busy":"2022-01-24T23:47:11.807891Z","iopub.execute_input":"2022-01-24T23:47:11.80905Z","iopub.status.idle":"2022-01-24T23:47:52.619729Z","shell.execute_reply.started":"2022-01-24T23:47:11.808979Z","shell.execute_reply":"2022-01-24T23:47:52.618706Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **<span style=\"color:#F7B2B0;\">Exploratory Data Analysis</span>**","metadata":{}},{"cell_type":"code","source":"time_id_count=train.groupby(\"investment_id\")['time_id'].count()\nfig, ax = plt.subplots(figsize=(25,9))\nsns.distplot(time_id_count, kde= True,hist=True, color = \"#2a9d8f\")\nplt.title('Count Distribution of time_id\\'s per Investment')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-01-24T23:48:57.32816Z","iopub.execute_input":"2022-01-24T23:48:57.32913Z","iopub.status.idle":"2022-01-24T23:48:57.891399Z","shell.execute_reply.started":"2022-01-24T23:48:57.329086Z","shell.execute_reply":"2022-01-24T23:48:57.890692Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **<span style=\"color:#F7B2B0;\">Target Distribution</span>**\n","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(25, 9))\nplt.subplot(121)\nsns.distplot(train.loc[:, 'target'], kde= True,hist = True ,color = \"#2a9d8f\")    \nplt.subplot(122)\nsns.boxplot(y=\"target\", data=train, color=\"#ff355d\")","metadata":{"execution":{"iopub.status.busy":"2022-01-24T23:49:01.727706Z","iopub.execute_input":"2022-01-24T23:49:01.728827Z","iopub.status.idle":"2022-01-24T23:49:32.979567Z","shell.execute_reply.started":"2022-01-24T23:49:01.728793Z","shell.execute_reply":"2022-01-24T23:49:32.97854Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **<span style=\"color:#F7B2B0;\">Feature Distribution</span>**\n\nLets analyze the distribution of first nine features starting from 'f_0' to 'f_8'","metadata":{}},{"cell_type":"code","source":"features_plt =  ['f_0','f_1','f_2','f_3','f_4','f_5','f_6','f_7','f_8']\nfig, ax = plt.subplots(3,3, figsize=(18, 18))\nfor i, feature in enumerate(features_plt):\n    sns.distplot(train[feature], color = \"#2a9d8f\", ax=ax[math.floor(i/3),i%3]).set_title(f'{feature} Distribution')\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2022-01-24T23:49:36.743892Z","iopub.execute_input":"2022-01-24T23:49:36.74416Z","iopub.status.idle":"2022-01-24T23:53:55.98205Z","shell.execute_reply.started":"2022-01-24T23:49:36.744132Z","shell.execute_reply":"2022-01-24T23:53:55.980734Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(25, 9))\nsns.distplot(train['investment_id'], bins=100,color = \"#2a9d8f\", label='investment_id')\n","metadata":{"execution":{"iopub.status.busy":"2022-01-24T23:55:07.283483Z","iopub.execute_input":"2022-01-24T23:55:07.28442Z","iopub.status.idle":"2022-01-24T23:55:32.18134Z","shell.execute_reply.started":"2022-01-24T23:55:07.284385Z","shell.execute_reply":"2022-01-24T23:55:32.179946Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(25, 9))\nsns.distplot(train['time_id'], bins=100,color = \"#2a9d8f\", label='time_id')\n","metadata":{"execution":{"iopub.status.busy":"2022-01-24T23:55:47.394953Z","iopub.execute_input":"2022-01-24T23:55:47.395253Z","iopub.status.idle":"2022-01-24T23:56:12.307693Z","shell.execute_reply.started":"2022-01-24T23:55:47.395225Z","shell.execute_reply":"2022-01-24T23:56:12.306837Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **<span style=\"color:#F7B2B0;\">Correlation Heatmap</span>**\n\nLets analyze the correlation of first nine features starting from 'f_0' to 'f_8'","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(25, 9))\nsns.heatmap(train[[f'{feature}' for feature in features_plt]].corr(),annot=True ,cmap=sns.color_palette(\"BrBG\",2));","metadata":{"execution":{"iopub.status.busy":"2022-01-24T23:56:16.799625Z","iopub.execute_input":"2022-01-24T23:56:16.799885Z","iopub.status.idle":"2022-01-24T23:56:18.569013Z","shell.execute_reply.started":"2022-01-24T23:56:16.799859Z","shell.execute_reply":"2022-01-24T23:56:18.567689Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **<span style=\"color:#F7B2B0;\">Correlation of Target and Features</span>**\n\nLets analyze the correlation of first nine features starting from 'f_0' to 'f_8' with the target","metadata":{}},{"cell_type":"code","source":"corr = []\nfor feature in features_plt:\n    corr.append( train['target'].corr(train[f'{feature}']) )\n    \nplt.figure(figsize=(25,9))\nplt.plot(corr, 'k')\nplt.xlabel('Features')\nplt.ylabel('Target')\nplt.title('Correlation between target and features')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-01-24T23:56:23.037407Z","iopub.execute_input":"2022-01-24T23:56:23.038376Z","iopub.status.idle":"2022-01-24T23:56:23.638356Z","shell.execute_reply.started":"2022-01-24T23:56:23.038306Z","shell.execute_reply":"2022-01-24T23:56:23.637154Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **<span style=\"color:#F7B2B0;\">Preprocessing</span>**","metadata":{}},{"cell_type":"code","source":"all_features = ['investment_id','f_0','f_1','f_2','target']\nTARGET_FEATURE_NAME =\"target\"\nfeatures =  ['investment_id','f_0','f_1','f_2']\ntrain = train[all_features]","metadata":{"execution":{"iopub.status.busy":"2022-01-24T21:16:41.544672Z","iopub.execute_input":"2022-01-24T21:16:41.545019Z","iopub.status.idle":"2022-01-24T21:16:41.886672Z","shell.execute_reply.started":"2022-01-24T21:16:41.544978Z","shell.execute_reply":"2022-01-24T21:16:41.885633Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# random sampling to create train and validation data\nrandom_selection = np.random.rand(len(train.index)) <= 0.85\ntrain_data = train[random_selection]\nvalid_data = train[~random_selection]\n\n#converting training and validation data to csv file\ntrain_data_file = \"train_data.csv\"\nvalid_data_file = \"valid_data.csv\"\ntrain_data.to_csv(train_data_file, index=False, header=False)\nvalid_data.to_csv(valid_data_file, index=False, header=False)","metadata":{"execution":{"iopub.status.busy":"2022-01-24T21:16:44.660542Z","iopub.execute_input":"2022-01-24T21:16:44.660891Z","iopub.status.idle":"2022-01-24T21:17:07.009819Z","shell.execute_reply.started":"2022-01-24T21:16:44.660857Z","shell.execute_reply":"2022-01-24T21:17:07.008953Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **<span style=\"color:#e76f51;\">W & B Artifacts</span>**\n\nAn artifact as a versioned folder of data.Entire datasets can be directly stored as artifacts .\n\nW&B Artifacts are used for dataset versioning, model versioning . They are also used for tracking dependencies and results across machine learning pipelines.Artifact references can be used to point to data in other systems like S3, GCP, or your own system.\n\nYou can learn more about W&B artifacts [here](https://docs.wandb.ai/guides/artifacts)\n\n![](https://drive.google.com/uc?id=1JYSaIMXuEVBheP15xxuaex-32yzxgglV)","metadata":{}},{"cell_type":"code","source":"# Save train data to W&B Artifacts\ntrain.to_csv(\"train_wandb.csv\", index = False)\nrun = wandb.init(project='ProbabilisticBNN', name='training_data', anonymous=anony,config=CONFIG) \nartifact = wandb.Artifact(name='training_data',type='dataset')\nartifact.add_file(\"./train_wandb.csv\")\nwandb.log_artifact(artifact)\nwandb.finish()","metadata":{"execution":{"iopub.status.busy":"2022-01-24T21:17:10.22914Z","iopub.execute_input":"2022-01-24T21:17:10.230098Z","iopub.status.idle":"2022-01-24T21:17:52.919224Z","shell.execute_reply.started":"2022-01-24T21:17:10.230041Z","shell.execute_reply":"2022-01-24T21:17:52.917921Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **<span style=\"color:#e76f51;\">ðŸŽ¯tf.data</span>**\n\n[Source](https://www.tensorflow.org/guide/data)\n\ntf.data API is used for building efficient input pipelines which can handle large amounts of data and perform complex data transformations . tf.data API has provisions for handling different data formats .\n\n<img src=\"https://storage.googleapis.com/jalammar-ml/tf.data/images/tf.data.png\" />\n\n[Image Source](https://www.kaggle.com/jalammar/intro-to-data-input-pipelines-with-tf-data)\n\nData source is essential for building any input pipeline and tf.data.Dataset.from_tensors() or tf.data.Dataset.from_tensor_slices can be used to construct a dataset from data in memory .The recommended format for the iput data stored in file is TFRecord which can be created using TFRecordDataset() .The different data source formats supported are numpy arrays , python generators , csv files ,image , TFRecords , csv and text files. \n\n<img src=\"https://storage.googleapis.com/jalammar-ml/tf.data/images/tf.data-read-data.png\" />\n\n[Image Source](https://www.kaggle.com/jalammar/intro-to-data-input-pipelines-with-tf-data)\n\nConstruction of tf.data input pipeline consists of three phases namely Extract , Transform and Load . The extraction involves the loading of data from different file format and converting it in to tf.data.Dataset object .\n\n## **<span style=\"color:#e76f51;\">ðŸŽ¯tf.data.Dataset</span>**\n\ntf.data.Dataset is an abstraction introduced by tf.data API and consists of sequence of elements where each element has one or more components . For example , in a tabular data pipeline , an element might be a single training example , with a pair of tensor components representing the input features and its label \n\ntf.data.Dataset can be created using two distinct ways\n\nConstructing a dataset using data stored in memory by a data source\n\nConstructing a dataset from one or more tf.data.Dataset objects by a data transformation\n\n<img src=\"https://storage.googleapis.com/jalammar-ml/tf.data/images/tf.data-simple-pipeline.png\" />\n\n[Image Source](https://www.kaggle.com/jalammar/intro-to-data-input-pipelines-with-tf-data)\n\n","metadata":{}},{"cell_type":"code","source":"def get_dataset_from_csv(csv_file_path, shuffle=False, batch_size=128):\n\n    dataset = tf.data.experimental.make_csv_dataset(\n        csv_file_path,\n        batch_size=batch_size,\n        column_names=all_features,\n        label_name=TARGET_FEATURE_NAME,\n        num_epochs=1,\n        header=False,\n        shuffle=shuffle,\n    )\n\n    return dataset","metadata":{"execution":{"iopub.status.busy":"2022-01-24T21:18:14.50856Z","iopub.execute_input":"2022-01-24T21:18:14.509126Z","iopub.status.idle":"2022-01-24T21:18:14.517306Z","shell.execute_reply.started":"2022-01-24T21:18:14.509084Z","shell.execute_reply":"2022-01-24T21:18:14.515948Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_model_inputs():\n    inputs = {}\n    for feature_name in features:\n        inputs[feature_name] = layers.Input(\n            name=feature_name, shape=(1,), dtype=tf.float32\n        )\n    return inputs","metadata":{"execution":{"iopub.status.busy":"2022-01-24T21:18:17.522022Z","iopub.execute_input":"2022-01-24T21:18:17.522817Z","iopub.status.idle":"2022-01-24T21:18:17.530721Z","shell.execute_reply.started":"2022-01-24T21:18:17.522755Z","shell.execute_reply":"2022-01-24T21:18:17.529536Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **<span style=\"color:#e76f51;\">TensorFlow Probability</span>**\n\n[Source](https://www.tensorflow.org/probability/examples/A_Tour_of_TensorFlow_Probability)\n\n`TensorFlow Probability` is a library for probabilistic reasoning and statistical analysis in TensorFlow and it supports modeling, inference, and criticism through composition of low-level modular components.\n\n`Low-level building blocks`\n\nDistributions\n\nBijectors\n\n`High(er)-level constructs`\n\nMarkov chain Monte Carlo\n\nProbabilistic Layers\n\nStructural Time Series\n\nGeneralized Linear Models\n\nOptimizers\n\nIn this tutorial , we will be using distributions and probabilistic layers .\n\n# **<span style=\"color:#e76f51;\">tfp.distributions.Distribution</span>**\n\nA `tfp.distributions.Distribution` is a class with two core methods: sample and log_prob. The distributions which will be used in this tutorial are `tfp.distributions.MultivariateNormalDiag` .The event shape and the batch shape are properties of a Distribution object .\n\n`Event shape` describes the shape of a single draw from the distribution; it may be dependent across dimensions. \n\n`Batch shape` describes independent, not identically distributed draws, aka a \"batch\" of distributions.\n\n`tfp.distributions.MultivariateNormalDiag` is used to  create a multivariate normal with a diagonal covariance . Multivariate distributions has an event shape of 2 . \n\n# **<span style=\"color:#e76f51;\">tfp.layers</span>**\n\nThe layers which will be used in this tutorial are `tfp.layers.VariableLayer` , `tfp.layers.DenseVariational` , `tfp.layers.IndependentNormal`, `tfp.layers.DistributionLambda` and `tfp.layers.MultivariateNormalTriL` .\n\n\n\n`tfp.layers.DenseVariational` : Dense layer with random kernel and bias.\n\n`tfp.layers.DistributionLambda` : Keras layer enabling plumbing TFP distributions through Keras models.\n\n`tfp.layers.IndependentNormal` : An independent normal Keras layer.\n\n`tfp.layers.MultivariateNormalTriL` : A d-variate MVNTriL Keras layer from d + d * (d + 1) // 2 params.\n\n`tfp.layers.VariableLayer`: Simply returns a (trainable) variable, regardless of input.\n\n","metadata":{}},{"cell_type":"code","source":"def run_experiment(model, loss, train_dataset, test_dataset):\n    \n    # Step1: Initialize W&B run\n    wandb.init(project='ProbabilisticBNN')\n\n    # 2. Save model inputs and hyperparameters\n    config = wandb.config\n    config.learning_rate = 0.01\n\n    model.compile(\n        optimizer=keras.optimizers.RMSprop(learning_rate=learning_rate),\n        loss=loss,\n        metrics=[keras.metrics.RootMeanSquaredError()]\n    )\n\n    print(\"Start training the model...\")\n    model.fit(train_dataset, epochs=num_epochs, validation_data=test_dataset,callbacks=[WandbCallback()])\n    print(\"Model training finished.\")\n    _, rmse = model.evaluate(train_dataset, verbose=0)\n    print(f\"Train RMSE: {round(rmse, 3)}\")\n\n    print(\"Evaluating model performance...\")\n    _, rmse = model.evaluate(test_dataset, verbose=0)\n    print(f\"Test RMSE: {round(rmse, 3)}\")","metadata":{"execution":{"iopub.status.busy":"2022-01-24T21:21:24.917956Z","iopub.execute_input":"2022-01-24T21:21:24.919178Z","iopub.status.idle":"2022-01-24T21:21:24.930272Z","shell.execute_reply.started":"2022-01-24T21:21:24.919093Z","shell.execute_reply":"2022-01-24T21:21:24.92949Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_probablistic_bnn_model(train_size):\n    inputs = create_model_inputs()\n    features = keras.layers.concatenate(list(inputs.values()))\n    features = layers.BatchNormalization()(features)\n\n    # Create hidden layers with weight uncertainty using the DenseVariational layer.\n    for units in hidden_units:\n        features = tfp.layers.DenseVariational(\n            units=units,\n            make_prior_fn=prior,\n            make_posterior_fn=posterior,\n            kl_weight=1 / train_size,\n            activation=\"sigmoid\",\n        )(features)\n\n    # Create a probabilisticÃ¥ output (Normal distribution), and use the `Dense` layer\n    # to produce the parameters of the distribution.\n    # We set units=2 to learn both the mean and the variance of the Normal distribution.\n    distribution_params = layers.Dense(units=2)(features)\n    outputs = tfp.layers.IndependentNormal(1)(distribution_params)\n\n    model = keras.Model(inputs=inputs, outputs=outputs)\n    return model","metadata":{"execution":{"iopub.status.busy":"2022-01-24T21:18:31.844097Z","iopub.execute_input":"2022-01-24T21:18:31.844442Z","iopub.status.idle":"2022-01-24T21:18:31.852783Z","shell.execute_reply.started":"2022-01-24T21:18:31.844391Z","shell.execute_reply":"2022-01-24T21:18:31.852121Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"learning_rate = 0.001\ndropout_rate = 0.15\nbatch_size = 256\nnum_epochs = 2\nencoding_size = 16\ntrain_size = 8500\nhidden_units = [8, 8]\n\ntrain_dataset = get_dataset_from_csv(\n    train_data_file, shuffle=True, batch_size=batch_size\n)\nvalid_dataset = get_dataset_from_csv(valid_data_file, batch_size=batch_size)","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define the prior weight distribution as Normal of mean=0 and stddev=1.\n# Note that, in this example, the we prior distribution is not trainable,\n# as we fix its parameters.\ndef prior(kernel_size, bias_size, dtype=None):\n    n = kernel_size + bias_size\n    prior_model = keras.Sequential(\n        [\n            tfp.layers.DistributionLambda(\n                lambda t: tfp.distributions.MultivariateNormalDiag(\n                    loc=tf.zeros(n), scale_diag=tf.ones(n)\n                )\n            )\n        ]\n    )\n    return prior_model\n\n\n# Define variational posterior weight distribution as multivariate Gaussian.\n# Note that the learnable parameters for this distribution are the means,\n# variances, and covariances.\ndef posterior(kernel_size, bias_size, dtype=None):\n    n = kernel_size + bias_size\n    posterior_model = keras.Sequential(\n        [\n            tfp.layers.VariableLayer(\n                tfp.layers.MultivariateNormalTriL.params_size(n), dtype=dtype\n            ),\n            tfp.layers.MultivariateNormalTriL(n),\n        ]\n    )\n    return posterior_model","metadata":{"execution":{"iopub.status.busy":"2022-01-24T21:18:39.359593Z","iopub.execute_input":"2022-01-24T21:18:39.36015Z","iopub.status.idle":"2022-01-24T21:18:39.368335Z","shell.execute_reply.started":"2022-01-24T21:18:39.360113Z","shell.execute_reply":"2022-01-24T21:18:39.36699Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def negative_loglikelihood(targets, estimated_distribution):\n    return -estimated_distribution.log_prob(targets)\n\n\n#num_epochs = 1000\nprob_bnn_model = create_probablistic_bnn_model(train_size)\nrun_experiment(prob_bnn_model, negative_loglikelihood, train_dataset, valid_dataset)","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Acknowledgements : \n\nGoogle supported this work by providing Google Cloud credit","metadata":{}},{"cell_type":"markdown","source":"## References\n\nhttps://www.tensorflow.org/probability/overview\n\nhttps://colab.research.google.com/github/keras-team/keras-io/blob/master/examples/keras_recipes/ipynb/bayesian_neural_networks.ipynb\n\nhttps://www.kaggle.com/edwardcrookenden/eda-and-lgbm-baseline-feature-imp\n\nhttps://www.kaggle.com/datafan07/ubiquant-market-prediction-what-do-we-have-here-\n\nhttps://www.kaggle.com/sytuannguyen/ubiquant-market-prediction-eda\n\nhttps://www.kaggle.com/columbia2131/speed-up-reading-csv-to-pickle\n\n\n","metadata":{}},{"cell_type":"markdown","source":"# Work in progress ðŸš§","metadata":{}}]}