{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Adversarial Validation\n\nThe adversarial validation workflow and feature correction adds 0.001 to the public leaderboard score (0.132 $\\rightarrow$ 0.133) for a simple LightGBM baseline.\n\nWhen we build models were are concerned to ensure that the model generalizes well out-of-sample. A key issue (in real world production deployment and in Kaggle) is distribution shift in the feature space. In other words, if the \"shape\" if the feature changes with time, the model could be overfitting to the training shape. We want to know which features can be distinguished between train and validation across folds -- this is **Adversarial Validation**. To do this, we build a model to predict not the target, but simply **to classify whether or not a sample row is in the train or validation set**. As per the graphic, we\n\n- (1) Start with our traditional cross-validation split\n- (2) Create a new target, which is an indicator of whether the sample is in the train or test set\n- (3) We concatenate the train and test X and y into a single dataframe *and shuffle*\n- (4) We then split that into (X' train, y' train) and (X' valid, y' valid).\n\n![](https://i.imgur.com/qoz5jl0.png)\n\nAnd then we `.fit(...)` the classifier on that!\n\nWe then look at the **feature importance** of this model to see what the offending features are. To avoid overfitting, we need to do something with these offending features: drop them (if they are unimportant in the core model) or transform them. Here is what we will do:\n\n   - (1) Train LGB regressor over CV as usual to predict the target; save the feature importances\n   - (2) Train LGB classifier over CV to predict if a sample is in the train set or validation set\n   - (3) Look for important features from 2; if we find any, it means they are non-stationary and/or leaky\n   - (4) Compare the important features from the classifier to feature importance from the standard regressor\n   - (5) If a feature is important in 1 and 2, we need to transform that feature\n   - (6) If a feature is important in 2 but not in 1, then we drop it...","metadata":{}},{"cell_type":"code","source":"import gc, time\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport lightgbm as lgb\n\nfrom scipy.stats import pearsonr\nfrom tqdm.notebook import tqdm\nfrom sklearn.base import clone\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import log_loss, mean_squared_error\n\nplt.rcParams['figure.figsize'] = (16, 5)\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"execution":{"iopub.status.busy":"2022-03-10T21:21:41.164336Z","iopub.execute_input":"2022-03-10T21:21:41.16469Z","iopub.status.idle":"2022-03-10T21:21:43.894938Z","shell.execute_reply.started":"2022-03-10T21:21:41.164584Z","shell.execute_reply":"2022-03-10T21:21:43.894222Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load and Inspect the Data\n\nThanks [Rob Mulla](https://www.kaggle.com/robikscube) for the reduced memory version of the data.","metadata":{}},{"cell_type":"code","source":"%%time\ntrain = (pd.read_parquet('../input/ubiquant-parquet/train_low_mem.parquet')\n         .sort_values(['time_id', 'investment_id'])\n         .drop(columns=['row_id'])\n         .query('time_id > 599')\n         .reset_index(drop=True));\n\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-03-10T21:21:46.021593Z","iopub.execute_input":"2022-03-10T21:21:46.02214Z","iopub.status.idle":"2022-03-10T21:22:30.705383Z","shell.execute_reply.started":"2022-03-10T21:21:46.022106Z","shell.execute_reply":"2022-03-10T21:22:30.704476Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_columns = train.columns\nfeatures = all_columns[train.columns.str.contains('f_')]","metadata":{"execution":{"iopub.status.busy":"2022-03-10T21:22:30.707354Z","iopub.execute_input":"2022-03-10T21:22:30.707827Z","iopub.status.idle":"2022-03-10T21:22:30.714339Z","shell.execute_reply.started":"2022-03-10T21:22:30.707776Z","shell.execute_reply":"2022-03-10T21:22:30.713653Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.shape","metadata":{"execution":{"iopub.status.busy":"2022-03-10T21:22:30.715936Z","iopub.execute_input":"2022-03-10T21:22:30.716565Z","iopub.status.idle":"2022-03-10T21:22:30.726896Z","shell.execute_reply.started":"2022-03-10T21:22:30.716522Z","shell.execute_reply":"2022-03-10T21:22:30.726253Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head()","metadata":{"execution":{"iopub.status.busy":"2022-03-10T21:22:30.728597Z","iopub.execute_input":"2022-03-10T21:22:30.729223Z","iopub.status.idle":"2022-03-10T21:22:30.762018Z","shell.execute_reply.started":"2022-03-10T21:22:30.729172Z","shell.execute_reply":"2022-03-10T21:22:30.761126Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The CV scheme, `PurgedGroupTimeSeriesSplit` is from my notebook [\"Purged Time Series CV, XGBoost, Optuna\"](https://www.kaggle.com/marketneutral/purged-time-series-cv-xgboost-optuna). You can read details of this CV scheme there.","metadata":{}},{"cell_type":"code","source":"import numpy as np\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection._split import _BaseKFold, indexable, _num_samples\nfrom sklearn.utils.validation import _deprecate_positional_args\n\n# modified code for group gaps; source\n# https://github.com/getgaurav2/scikit-learn/blob/d4a3af5cc9da3a76f0266932644b884c99724c57/sklearn/model_selection/_split.py#L2243\nclass PurgedGroupTimeSeriesSplit(_BaseKFold):\n    \"\"\"Time Series cross-validator variant with non-overlapping groups.\n    Allows for a gap in groups to avoid potentially leaking info from\n    train into test if the model has windowed or lag features.\n    Provides train/test indices to split time series data samples\n    that are observed at fixed time intervals according to a\n    third-party provided group.\n    In each split, test indices must be higher than before, and thus shuffling\n    in cross validator is inappropriate.\n    This cross-validation object is a variation of :class:`KFold`.\n    In the kth split, it returns first k folds as train set and the\n    (k+1)th fold as test set.\n    The same group will not appear in two different folds (the number of\n    distinct groups has to be at least equal to the number of folds).\n    Note that unlike standard cross-validation methods, successive\n    training sets are supersets of those that come before them.\n    Read more in the :ref:`User Guide <cross_validation>`.\n    Parameters\n    ----------\n    n_splits : int, default=5\n        Number of splits. Must be at least 2.\n    max_train_group_size : int, default=Inf\n        Maximum group size for a single training set.\n    group_gap : int, default=None\n        Gap between train and test\n    max_test_group_size : int, default=Inf\n        We discard this number of groups from the end of each train split\n    \"\"\"\n\n    @_deprecate_positional_args\n    def __init__(self,\n                 n_splits=5,\n                 *,\n                 max_train_group_size=np.inf,\n                 max_test_group_size=np.inf,\n                 group_gap=None,\n                 verbose=False\n                 ):\n        super().__init__(n_splits, shuffle=False, random_state=None)\n        self.max_train_group_size = max_train_group_size\n        self.group_gap = group_gap\n        self.max_test_group_size = max_test_group_size\n        self.verbose = verbose\n\n    def split(self, X, y=None, groups=None):\n        \"\"\"Generate indices to split data into training and test set.\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Training data, where n_samples is the number of samples\n            and n_features is the number of features.\n        y : array-like of shape (n_samples,)\n            Always ignored, exists for compatibility.\n        groups : array-like of shape (n_samples,)\n            Group labels for the samples used while splitting the dataset into\n            train/test set.\n        Yields\n        ------\n        train : ndarray\n            The training set indices for that split.\n        test : ndarray\n            The testing set indices for that split.\n        \"\"\"\n        if groups is None:\n            raise ValueError(\n                \"The 'groups' parameter should not be None\")\n        X, y, groups = indexable(X, y, groups)\n        n_samples = _num_samples(X)\n        n_splits = self.n_splits\n        group_gap = self.group_gap\n        max_test_group_size = self.max_test_group_size\n        max_train_group_size = self.max_train_group_size\n        n_folds = n_splits + 1\n        group_dict = {}\n        u, ind = np.unique(groups, return_index=True)\n        unique_groups = u[np.argsort(ind)]\n        n_samples = _num_samples(X)\n        n_groups = _num_samples(unique_groups)\n        for idx in np.arange(n_samples):\n            if (groups[idx] in group_dict):\n                group_dict[groups[idx]].append(idx)\n            else:\n                group_dict[groups[idx]] = [idx]\n        if n_folds > n_groups:\n            raise ValueError(\n                (\"Cannot have number of folds={0} greater than\"\n                 \" the number of groups={1}\").format(n_folds,\n                                                     n_groups))\n\n        group_test_size = min(n_groups // n_folds, max_test_group_size)\n        group_test_starts = range(n_groups - n_splits * group_test_size,\n                                  n_groups, group_test_size)\n        for group_test_start in group_test_starts:\n            train_array = []\n            test_array = []\n\n            group_st = max(0, group_test_start - group_gap - max_train_group_size)\n            for train_group_idx in unique_groups[group_st:(group_test_start - group_gap)]:\n                train_array_tmp = group_dict[train_group_idx]\n                \n                train_array = np.sort(np.unique(\n                                      np.concatenate((train_array,\n                                                      train_array_tmp)),\n                                      axis=None), axis=None)\n\n            train_end = train_array.size\n \n            for test_group_idx in unique_groups[group_test_start:\n                                                group_test_start +\n                                                group_test_size]:\n                test_array_tmp = group_dict[test_group_idx]\n                test_array = np.sort(np.unique(\n                                              np.concatenate((test_array,\n                                                              test_array_tmp)),\n                                     axis=None), axis=None)\n\n            test_array  = test_array[group_gap:]\n            \n            \n            if self.verbose > 0:\n                    pass\n                    \n            yield [int(i) for i in train_array], [int(i) for i in test_array]","metadata":{"execution":{"iopub.status.busy":"2022-03-10T21:22:30.763683Z","iopub.execute_input":"2022-03-10T21:22:30.76442Z","iopub.status.idle":"2022-03-10T21:22:30.782371Z","shell.execute_reply.started":"2022-03-10T21:22:30.764375Z","shell.execute_reply":"2022-03-10T21:22:30.781443Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train LGB Regressor and Store Feature Importance\n\nThis is just a baseline traditional LGB model as usual.","metadata":{}},{"cell_type":"code","source":"# based on source https://www.kaggle.com/artgor/dota-eda-fe-and-models\n\ndef train_model(\n    X,\n    y,\n    params,\n    cv,\n    score_func,\n    plot_feature_importance=False,\n    cat_features=[],\n    importance_type='gain',\n    groups=None,\n    clip=True,\n    clip_bounds=(-15,15)\n):\n\n    oof = []\n    scores = []\n    feature_importance = pd.DataFrame()\n    models = []\n    \n    for fold_n, (train_index, valid_index) in enumerate(cv.split(X, groups=groups)):\n        print('Fold', fold_n+1, 'started at', time.ctime())\n        X_train, X_valid = X.loc[train_index], X.loc[valid_index]\n        y_train, y_valid = y[train_index], y[valid_index]\n        \n        if clip:\n            X_train = X_train.clip(clip_bounds[0], clip_bounds[1])\n        \n        train_data = lgb.Dataset(X_train, label=y_train)\n        valid_data = lgb.Dataset(X_valid, label=y_valid)\n\n        model = lgb.train(\n            params=params,\n            train_set=train_data,\n            num_boost_round=2000,\n            valid_sets=[\n                train_data,\n                valid_data\n            ],\n            callbacks=[\n                lgb.early_stopping(stopping_rounds=200),\n                lgb.log_evaluation(period=200)\n            ],\n            categorical_feature = \\\n                cat_features if len(cat_features) > 0 else 'auto'\n        )\n        \n        models.append(model)\n\n        y_pred_valid = model.predict(X_valid)\n\n        oof.append(pd.DataFrame(index=valid_index, data=y_pred_valid.reshape(-1,), columns=['pred']))\n        scores.append(score_func(y_valid, y_pred_valid))\n\n        fold_importance = pd.DataFrame()\n        fold_importance[\"feature\"] = X.columns\n        fold_importance[\"importance\"] = model.feature_importance(importance_type)\n        fold_importance[\"fold\"] = fold_n + 1\n        feature_importance = \\\n            pd.concat([feature_importance, fold_importance], axis=0)\n\n    print('CV mean score: {0:.4f}, std: {1:.4f}.'.format(np.mean(scores),\n                                                         np.std(scores)))\n    \n    feature_importance[\"importance\"] /= (fold_n + 1)\n\n    if plot_feature_importance:\n        cols = (feature_importance[[\"feature\", \"importance\"]]\n                  .groupby(\"feature\")\n                  .mean()\n                  .sort_values(by=\"importance\", ascending=False)[:50].index)\n\n        best_features = feature_importance.loc[feature_importance.feature.isin(cols)]\n\n        plt.figure(figsize=(16, 10));\n        sns.barplot(\n            x=\"importance\",\n            y=\"feature\",\n            data=best_features.sort_values(by=\"importance\", ascending=False));\n        plt.title(f'LGBM Feature Importances (avgerage over folds): {importance_type}');\n        \n        return oof, scores, feature_importance, models\n    else:\n        return oof, scores, feature_importance, models","metadata":{"execution":{"iopub.status.busy":"2022-03-10T21:28:22.825641Z","iopub.execute_input":"2022-03-10T21:28:22.826439Z","iopub.status.idle":"2022-03-10T21:28:22.842597Z","shell.execute_reply.started":"2022-03-10T21:28:22.826389Z","shell.execute_reply":"2022-03-10T21:28:22.841575Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tscv = PurgedGroupTimeSeriesSplit(\n    n_splits=3,\n    max_train_group_size=120,\n    group_gap=10,\n    max_test_group_size=40\n)","metadata":{"execution":{"iopub.status.busy":"2022-03-10T21:28:23.132243Z","iopub.execute_input":"2022-03-10T21:28:23.132528Z","iopub.status.idle":"2022-03-10T21:28:23.137653Z","shell.execute_reply.started":"2022-03-10T21:28:23.132498Z","shell.execute_reply":"2022-03-10T21:28:23.136721Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"params = {\n    'feature_fraction': 0.85,\n    'bagging_fraction': 0.85,\n    'learning_rate': 0.05,\n    'max_depth': -1,\n    'min_data_in_leaf': 500,\n    'num_threads': -1,\n    'verbosity': -1,\n    'objective': \"regression\"\n}","metadata":{"execution":{"iopub.status.busy":"2022-03-10T21:28:23.471104Z","iopub.execute_input":"2022-03-10T21:28:23.471608Z","iopub.status.idle":"2022-03-10T21:28:23.476003Z","shell.execute_reply.started":"2022-03-10T21:28:23.471563Z","shell.execute_reply":"2022-03-10T21:28:23.47525Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\noof, scores, feature_imp, lgb_models = train_model(\n    train.dropna()[features].reset_index(drop=True),\n    train.dropna()['target'].reset_index(drop=True),\n    params=params,\n    cv=tscv,\n    plot_feature_importance=True,\n    score_func=mean_squared_error,\n    groups=train.dropna().reset_index(drop=True)['time_id']\n)","metadata":{"execution":{"iopub.status.busy":"2022-03-10T21:28:23.817972Z","iopub.execute_input":"2022-03-10T21:28:23.818509Z","iopub.status.idle":"2022-03-10T21:33:23.458727Z","shell.execute_reply.started":"2022-03-10T21:28:23.818468Z","shell.execute_reply":"2022-03-10T21:33:23.457546Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train LGB Classifier and Inspect Feature Importance\n\nHere is the adversarial classifier.","metadata":{}},{"cell_type":"code","source":"# based on source https://www.kaggle.com/artgor/dota-eda-fe-and-models\n\ndef train_adversarial(\n    X,\n    y,\n    params,\n    cv,\n    score_func,\n    plot_feature_importance=False,\n    cat_features=[],\n    importance_type='gain',\n    groups=None,\n    clip=True,\n    clip_bounds=(-15,15)\n):\n\n    oof = []\n    scores = []\n    feature_importance = pd.DataFrame()\n    models = []\n    \n    for fold_n, (train_index, valid_index) in enumerate(cv.split(X, groups=groups)):\n        print('Fold', fold_n+1, 'started at', time.ctime())\n        \n        X_train, X_valid = X.loc[train_index], X.loc[valid_index]\n        X_train['is_valid'] = 0\n        X_valid['is_valid'] = 1\n        \n        X_train = pd.concat([X_train, X_valid])\n        y_train = X_train.pop('is_valid')\n        \n        X_train, X_valid, y_train, y_valid = train_test_split(\n            X_train, y_train, test_size=0.25, random_state=42)\n        \n        if clip:\n            X_train = X_train.clip(clip_bounds[0], clip_bounds[1])\n        \n        train_data = lgb.Dataset(X_train, label=y_train)\n        valid_data = lgb.Dataset(X_valid, label=y_valid)\n\n        model = lgb.train(\n            params=params,\n            train_set=train_data,\n            num_boost_round=100,\n            valid_sets=[\n                train_data,\n                valid_data\n            ],\n            callbacks=[\n                lgb.log_evaluation(period=50)\n            ],\n            categorical_feature = \\\n                cat_features if len(cat_features) > 0 else 'auto'\n        )\n        \n        models.append(model)\n\n        y_pred_valid = model.predict(X_valid)\n\n        #oof.append(pd.DataFrame(index=valid_index, data=y_pred_valid.reshape(-1,), columns=['pred']))\n        scores.append(score_func(y_valid, y_pred_valid))\n\n        fold_importance = pd.DataFrame()\n        fold_importance[\"feature\"] = X.columns\n        fold_importance[\"importance\"] = model.feature_importance(importance_type)\n        fold_importance[\"fold\"] = fold_n + 1\n        feature_importance = \\\n            pd.concat([feature_importance, fold_importance], axis=0)\n\n    print('CV mean score: {0:.4f}, std: {1:.4f}.'.format(np.mean(scores),\n                                                         np.std(scores)))\n    \n    feature_importance[\"importance\"] /= (fold_n + 1)\n\n    if plot_feature_importance:\n        cols = (feature_importance[[\"feature\", \"importance\"]]\n                  .groupby(\"feature\")\n                  .mean()\n                  .sort_values(by=\"importance\", ascending=False)[:50].index)\n\n        best_features = feature_importance.loc[feature_importance.feature.isin(cols)]\n\n        plt.figure(figsize=(16, 10));\n        sns.barplot(\n            x=\"importance\",\n            y=\"feature\",\n            data=best_features.sort_values(by=\"importance\", ascending=False));\n        plt.xlim(0, 900_000);\n        plt.title(f'LGBM Adversarial Feature Importances (avgerage over folds): {importance_type}');\n\n        \n    return scores, feature_importance, models\n\n","metadata":{"execution":{"iopub.status.busy":"2022-03-10T21:34:24.631631Z","iopub.execute_input":"2022-03-10T21:34:24.631973Z","iopub.status.idle":"2022-03-10T21:34:24.646541Z","shell.execute_reply.started":"2022-03-10T21:34:24.631937Z","shell.execute_reply":"2022-03-10T21:34:24.645792Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"params = {\n    'feature_fraction': 0.85,\n    'bagging_fraction': 0.85,\n    'learning_rate': 0.05,\n    'max_depth': -1,\n    'min_data_in_leaf': 500,\n    'num_threads': -1,\n    'verbosity': -1,\n    'objective': 'binary', #Binary target feature\n    'metric': 'binary_logloss' #metric for binary classification\n}","metadata":{"execution":{"iopub.status.busy":"2022-03-10T21:33:47.661473Z","iopub.execute_input":"2022-03-10T21:33:47.662399Z","iopub.status.idle":"2022-03-10T21:33:47.668791Z","shell.execute_reply.started":"2022-03-10T21:33:47.662347Z","shell.execute_reply":"2022-03-10T21:33:47.66822Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tscv = PurgedGroupTimeSeriesSplit(\n    n_splits=3,\n    max_train_group_size=120,\n    group_gap=10,\n    max_test_group_size=40\n)","metadata":{"execution":{"iopub.status.busy":"2022-03-10T21:33:48.913923Z","iopub.execute_input":"2022-03-10T21:33:48.914697Z","iopub.status.idle":"2022-03-10T21:33:48.919138Z","shell.execute_reply.started":"2022-03-10T21:33:48.91465Z","shell.execute_reply":"2022-03-10T21:33:48.918462Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Aversarial Train First Pass","metadata":{}},{"cell_type":"code","source":"%%time\nscores_adv_1, feature_imp_adv_1, lgb_models_adv_1 = train_adversarial(\n    train.dropna()[features].reset_index(drop=True),\n    train.dropna()['target'].reset_index(drop=True),\n    params=params,\n    cv=tscv,\n    plot_feature_importance=True,\n    score_func=log_loss,\n    groups = train.dropna().reset_index(drop=True)['time_id']\n)","metadata":{"execution":{"iopub.status.busy":"2022-03-10T21:34:27.982114Z","iopub.execute_input":"2022-03-10T21:34:27.982806Z","iopub.status.idle":"2022-03-10T21:37:05.17721Z","shell.execute_reply.started":"2022-03-10T21:34:27.982769Z","shell.execute_reply":"2022-03-10T21:37:05.176103Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ðŸš©ðŸš©ðŸš© We have a problem ðŸš©ðŸš©ðŸš©\n\nLog loss of 0.693 is random guessing. That's what we want. We don't want the classifier to be able to distinguish between the train and valid set. We are getting a very low log loss... **the classifier is good, which is bad!**\n\nFortunately, we see that the offending features are small in number:\n- `f_74`\n- `f_142`\n- `f_63`","metadata":{}},{"cell_type":"code","source":"sorted_regression_importance = feature_imp.groupby('feature')['importance'].mean().sort_values(ascending=False)\nsorted_regression_importance.head()","metadata":{"execution":{"iopub.status.busy":"2022-03-10T21:39:00.775537Z","iopub.execute_input":"2022-03-10T21:39:00.775856Z","iopub.status.idle":"2022-03-10T21:39:00.786135Z","shell.execute_reply.started":"2022-03-10T21:39:00.775821Z","shell.execute_reply":"2022-03-10T21:39:00.785271Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's see how important these are in target prediction across the 300 features. Lower number is more important.","metadata":{}},{"cell_type":"code","source":"sorted_regression_importance.index.get_loc('f_74')","metadata":{"execution":{"iopub.status.busy":"2022-03-10T21:39:32.1762Z","iopub.execute_input":"2022-03-10T21:39:32.17697Z","iopub.status.idle":"2022-03-10T21:39:32.183573Z","shell.execute_reply.started":"2022-03-10T21:39:32.176921Z","shell.execute_reply":"2022-03-10T21:39:32.182673Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sorted_regression_importance.index.get_loc('f_142')","metadata":{"execution":{"iopub.status.busy":"2022-03-10T21:39:45.2538Z","iopub.execute_input":"2022-03-10T21:39:45.254395Z","iopub.status.idle":"2022-03-10T21:39:45.261318Z","shell.execute_reply.started":"2022-03-10T21:39:45.254345Z","shell.execute_reply":"2022-03-10T21:39:45.260226Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sorted_regression_importance.index.get_loc('f_63')","metadata":{"execution":{"iopub.status.busy":"2022-03-10T21:39:58.007725Z","iopub.execute_input":"2022-03-10T21:39:58.008344Z","iopub.status.idle":"2022-03-10T21:39:58.01516Z","shell.execute_reply.started":"2022-03-10T21:39:58.008294Z","shell.execute_reply":"2022-03-10T21:39:58.014258Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Transform Offenders","metadata":{}},{"cell_type":"markdown","source":"These offending features are indeed reasonably important in the regressor. We will do a rank transformation by `time_id` to make these features stationary.","metadata":{}},{"cell_type":"code","source":"offenders = ['f_74', 'f_142', 'f_63']","metadata":{"execution":{"iopub.status.busy":"2022-03-10T21:57:39.247625Z","iopub.execute_input":"2022-03-10T21:57:39.247873Z","iopub.status.idle":"2022-03-10T21:57:39.25206Z","shell.execute_reply.started":"2022-03-10T21:57:39.247842Z","shell.execute_reply":"2022-03-10T21:57:39.251316Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['f_74']  = train[['time_id', 'f_74']].groupby('time_id').rank(pct=True)","metadata":{"execution":{"iopub.status.busy":"2022-03-10T21:47:58.833745Z","iopub.execute_input":"2022-03-10T21:47:58.834227Z","iopub.status.idle":"2022-03-10T21:48:00.529407Z","shell.execute_reply.started":"2022-03-10T21:47:58.834171Z","shell.execute_reply":"2022-03-10T21:48:00.52839Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['f_142']  = train[['time_id', 'f_142']].groupby('time_id').rank(pct=True)","metadata":{"execution":{"iopub.status.busy":"2022-03-10T21:48:38.790303Z","iopub.execute_input":"2022-03-10T21:48:38.790614Z","iopub.status.idle":"2022-03-10T21:48:40.348276Z","shell.execute_reply.started":"2022-03-10T21:48:38.790585Z","shell.execute_reply":"2022-03-10T21:48:40.347575Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['f_63']  = train[['time_id', 'f_63']].groupby('time_id').rank(pct=True)","metadata":{"execution":{"iopub.status.busy":"2022-03-10T21:48:40.349859Z","iopub.execute_input":"2022-03-10T21:48:40.350394Z","iopub.status.idle":"2022-03-10T21:48:41.617531Z","shell.execute_reply.started":"2022-03-10T21:48:40.350349Z","shell.execute_reply":"2022-03-10T21:48:41.616535Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# \"Why does the rank transfomation help here?\"\n\nThis is a questions I have gotten a few times, so I am making a section on it. Imagine we have 3 stocks with a single feature that evolves over time like the following.","metadata":{}},{"cell_type":"code","source":"np.random.seed(0)\n\nfeature_unstacked = pd.DataFrame(\n    index = np.arange(100),\n    data = {\n        'A': np.cumsum(0.60 + np.random.normal(size=100)),\n        'B': np.cumsum(0.30 + np.random.normal(size=100)),\n        'C': np.cumsum(0.0 + np.random.normal(size=100))\n})","metadata":{"execution":{"iopub.status.busy":"2022-03-22T17:30:12.970878Z","iopub.execute_input":"2022-03-22T17:30:12.971553Z","iopub.status.idle":"2022-03-22T17:30:12.978394Z","shell.execute_reply.started":"2022-03-22T17:30:12.971503Z","shell.execute_reply":"2022-03-22T17:30:12.977238Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feature = (\n    feature_unstacked\n    .stack()\n    .to_frame()\n    .reset_index()\n    .rename(columns={'level_0': 'time_id', 'level_1': 'investment_id', 0: 'feature'})\n)\n\nfeature.head()","metadata":{"execution":{"iopub.status.busy":"2022-03-22T17:30:13.95594Z","iopub.execute_input":"2022-03-22T17:30:13.956493Z","iopub.status.idle":"2022-03-22T17:30:13.969661Z","shell.execute_reply.started":"2022-03-22T17:30:13.956449Z","shell.execute_reply":"2022-03-22T17:30:13.968617Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"target = (pd.DataFrame(\n    index = np.arange(100),\n    data = {\n        'A': (0.60 + np.random.normal(size=100)),\n        'B': (0.30 + np.random.normal(size=100)),\n        'C': (0.0 + np.random.normal(size=100))\n}).stack()\n  .to_frame()\n  .reset_index()\n  .rename(columns={'level_0': 'time_id', 'level_1': 'investment_id', 0: 'target'}))\n\ntarget.head()","metadata":{"execution":{"iopub.status.busy":"2022-03-22T17:30:14.538694Z","iopub.execute_input":"2022-03-22T17:30:14.539598Z","iopub.status.idle":"2022-03-22T17:30:14.559194Z","shell.execute_reply.started":"2022-03-22T17:30:14.53954Z","shell.execute_reply":"2022-03-22T17:30:14.557869Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.scatter(feature['feature'], target['target'])\nplt.xlabel('feature value')\nplt.ylabel('target');","metadata":{"execution":{"iopub.status.busy":"2022-03-22T17:30:15.352529Z","iopub.execute_input":"2022-03-22T17:30:15.352793Z","iopub.status.idle":"2022-03-22T17:30:15.591873Z","shell.execute_reply.started":"2022-03-22T17:30:15.352766Z","shell.execute_reply":"2022-03-22T17:30:15.590878Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This is a great feature! Pearson corr would be the highest in all the Ubiquant data.","metadata":{}},{"cell_type":"code","source":"pearsonr(feature['feature'], target['target'])[0]","metadata":{"execution":{"iopub.status.busy":"2022-03-22T17:30:50.809856Z","iopub.execute_input":"2022-03-22T17:30:50.810123Z","iopub.status.idle":"2022-03-22T17:30:50.817379Z","shell.execute_reply.started":"2022-03-22T17:30:50.810097Z","shell.execute_reply":"2022-03-22T17:30:50.816428Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So, what's the problem? If we plot the feature by stock, we see that the feature is highly non-stationary. We don't need an adversarial training run to see by eye that we could easily distinguigh the earlier period from the later period. This is the key problem that adversarial validation is meant to uncover.","metadata":{}},{"cell_type":"code","source":"feature_unstacked.plot()","metadata":{"execution":{"iopub.status.busy":"2022-03-22T17:30:55.128874Z","iopub.execute_input":"2022-03-22T17:30:55.129161Z","iopub.status.idle":"2022-03-22T17:30:55.36815Z","shell.execute_reply.started":"2022-03-22T17:30:55.129132Z","shell.execute_reply":"2022-03-22T17:30:55.367277Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"What can we do??? We want to retain this feature because it looks like a good predictior, but it is highly non-stationary so our GBDT model will overfit. We can **rank transform by stock per day** as follows.","metadata":{}},{"cell_type":"code","source":"feature_unstacked.rank(axis=1).plot()","metadata":{"execution":{"iopub.status.busy":"2022-03-22T17:30:55.977184Z","iopub.execute_input":"2022-03-22T17:30:55.977519Z","iopub.status.idle":"2022-03-22T17:30:56.219112Z","shell.execute_reply.started":"2022-03-22T17:30:55.97749Z","shell.execute_reply":"2022-03-22T17:30:56.21815Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We retain the primary relationship in the data: A > B > C. But now it's going to be very hard to tell across folds what time stage we are in. Problem solved!","metadata":{}},{"cell_type":"code","source":"feature_fixed = (\n    feature_unstacked\n    .rank(axis=1)\n    .stack()\n    .to_frame()\n    .reset_index()\n    .rename(columns={'level_0': 'time_id', 'level_1': 'investment_id', 0: 'feature'})\n)\n","metadata":{"execution":{"iopub.status.busy":"2022-03-22T17:30:58.345256Z","iopub.execute_input":"2022-03-22T17:30:58.34555Z","iopub.status.idle":"2022-03-22T17:30:58.355153Z","shell.execute_reply.started":"2022-03-22T17:30:58.345521Z","shell.execute_reply":"2022-03-22T17:30:58.354204Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.scatter(feature_fixed['feature'], target['target'])","metadata":{"execution":{"iopub.status.busy":"2022-03-22T17:30:58.599538Z","iopub.execute_input":"2022-03-22T17:30:58.59985Z","iopub.status.idle":"2022-03-22T17:30:58.820456Z","shell.execute_reply.started":"2022-03-22T17:30:58.599815Z","shell.execute_reply":"2022-03-22T17:30:58.819453Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pearsonr(feature_fixed['feature'], target['target'])[0]","metadata":{"execution":{"iopub.status.busy":"2022-03-22T17:30:59.630494Z","iopub.execute_input":"2022-03-22T17:30:59.63079Z","iopub.status.idle":"2022-03-22T17:30:59.638148Z","shell.execute_reply.started":"2022-03-22T17:30:59.630764Z","shell.execute_reply":"2022-03-22T17:30:59.637085Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"And there it is... we turned a non-stationary feature with a correl of ~0.24 to a stationary feature with a correl of ~0.24. **This is the power of the rank transformataion for financial problems like this.**","metadata":{}},{"cell_type":"markdown","source":"# Adversarial Second Pass\n\nWhen we run the adversarial train again, we see that the classifier gets worse!","metadata":{}},{"cell_type":"code","source":"tscv = PurgedGroupTimeSeriesSplit(\n    n_splits=3,\n    max_train_group_size=120,\n    group_gap=10,\n    max_test_group_size=40\n)","metadata":{"execution":{"iopub.status.busy":"2022-03-10T21:49:07.893216Z","iopub.execute_input":"2022-03-10T21:49:07.893528Z","iopub.status.idle":"2022-03-10T21:49:07.898064Z","shell.execute_reply.started":"2022-03-10T21:49:07.893491Z","shell.execute_reply":"2022-03-10T21:49:07.897217Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nscores_adv_1, feature_imp_adv_1, lgb_models_adv_1 = train_adversarial(\n    train.dropna()[features].reset_index(drop=True),\n    train.dropna()['target'].reset_index(drop=True),\n    params=params,\n    cv=tscv,\n    plot_feature_importance=True,\n    score_func=log_loss,\n    groups = train.dropna().reset_index(drop=True)['time_id']\n)","metadata":{"execution":{"iopub.status.busy":"2022-03-10T21:49:12.751218Z","iopub.execute_input":"2022-03-10T21:49:12.751521Z","iopub.status.idle":"2022-03-10T21:52:05.439794Z","shell.execute_reply.started":"2022-03-10T21:49:12.751487Z","shell.execute_reply":"2022-03-10T21:52:05.437564Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"After transforming the offending features, we see that our classifier is worse (which is good!).\n\n![](https://64.media.tumblr.com/a8ac2ebbec3f7b9829502e1b7ad4ba5f/tumblr_mqrz7oEOYc1rt1ivno7_250.gifv)","metadata":{}},{"cell_type":"markdown","source":"# Train Final Model on Transformed Features","metadata":{}},{"cell_type":"code","source":"tscv = PurgedGroupTimeSeriesSplit(\n    n_splits=3,\n    max_train_group_size=120,\n    group_gap=10,\n    max_test_group_size=40\n)\n\nparams = {\n    'feature_fraction': 0.85,\n    'bagging_fraction': 0.85,\n    'learning_rate': 0.05,\n    'max_depth': -1,\n    'min_data_in_leaf': 500,\n    'num_threads': -1,\n    'verbosity': -1,\n    'objective': \"regression\"\n}","metadata":{"execution":{"iopub.status.busy":"2022-03-10T21:58:05.73615Z","iopub.execute_input":"2022-03-10T21:58:05.736504Z","iopub.status.idle":"2022-03-10T21:58:05.741701Z","shell.execute_reply.started":"2022-03-10T21:58:05.736467Z","shell.execute_reply":"2022-03-10T21:58:05.741078Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\noof_t, scores_t, feature_imp_t, lgb_models_t = train_model(\n    train.dropna()[features].reset_index(drop=True),\n    train.dropna()['target'].reset_index(drop=True),\n    params=params,\n    cv=tscv,\n    plot_feature_importance=False,\n    score_func=mean_squared_error,\n    groups=train.dropna().reset_index(drop=True)['time_id']\n)","metadata":{"execution":{"iopub.status.busy":"2022-03-10T21:58:13.191175Z","iopub.execute_input":"2022-03-10T21:58:13.191712Z","iopub.status.idle":"2022-03-10T22:03:29.217506Z","shell.execute_reply.started":"2022-03-10T21:58:13.191676Z","shell.execute_reply":"2022-03-10T22:03:29.216733Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.mean(scores), np.std(scores)","metadata":{"execution":{"iopub.status.busy":"2022-03-10T22:03:46.605163Z","iopub.execute_input":"2022-03-10T22:03:46.605951Z","iopub.status.idle":"2022-03-10T22:03:46.612804Z","shell.execute_reply.started":"2022-03-10T22:03:46.6059Z","shell.execute_reply":"2022-03-10T22:03:46.612004Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.mean(scores_t), np.std(scores_t)","metadata":{"execution":{"iopub.status.busy":"2022-03-10T22:03:36.798219Z","iopub.execute_input":"2022-03-10T22:03:36.798668Z","iopub.status.idle":"2022-03-10T22:03:36.80569Z","shell.execute_reply.started":"2022-03-10T22:03:36.798636Z","shell.execute_reply":"2022-03-10T22:03:36.804929Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Nice! We get a small improvement in both mean CV score and standard deviation across folds.","metadata":{}},{"cell_type":"markdown","source":"# Prediction Time!","metadata":{}},{"cell_type":"code","source":"%%time\n\nimport ubiquant\n\nenv = ubiquant.make_env()   # initialize the environment\niter_test = env.iter_test()    # an iterator which loops over the test set and sample submission\n\nfor (test_df, sample_prediction_df) in iter_test:\n    \n    test_df['target'] = 0\n    \n    # we need to transform like we did in training!\n    for feat in offenders:\n        test_df[feat] = test_df[feat].rank(pct=True)\n    \n    # we are predicting with the models trained on the transformed offenders\n    for i, mod in enumerate(lgb_models_t): \n        test_df['target'] += mod.predict(test_df[features])\n    test_df['target'] /= len(lgb_models_t)\n\n    env.predict(test_df[['row_id','target']])","metadata":{"execution":{"iopub.status.busy":"2022-03-10T22:35:30.976129Z","iopub.execute_input":"2022-03-10T22:35:30.976784Z","iopub.status.idle":"2022-03-10T22:35:31.138258Z","shell.execute_reply.started":"2022-03-10T22:35:30.976739Z","shell.execute_reply":"2022-03-10T22:35:31.137279Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"[Hamel Husain](https://twitter.com/HamelHusain/status/1504516124632772628?s=20&t=uEiVIr9KO-aOpHPCB1VdRg) kindly shared this slide about AV to monitor production models in a reponse to one of my Tweets.\n\n![](https://i.imgur.com/K1W4NAg.png)\n\nThank you for taking a look at this notebook. Please leave comments and suggestions.","metadata":{}}]}