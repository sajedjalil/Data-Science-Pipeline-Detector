{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Begin","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom tqdm.notebook import tqdm\ntqdm.pandas()\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-01-22T11:59:28.089885Z","iopub.execute_input":"2022-01-22T11:59:28.09058Z","iopub.status.idle":"2022-01-22T11:59:28.176722Z","shell.execute_reply.started":"2022-01-22T11:59:28.090472Z","shell.execute_reply":"2022-01-22T11:59:28.175634Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Setting Environmental Variables","metadata":{}},{"cell_type":"code","source":"import time\nimport torch\nimport random\n\ns = time.time()\n\nseed = 42 #int(np.random.randint(0, 1e9))\n\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Random seed:\", seed)\nprint(\"Device:\", device)\nbasic_cols = ['investment_id', 'target']\nnum_feat = 300 #total of 300 feats from f_0 to f_299\nN_INVID = 3774 #Max investment_id\nfeatures = [f'f_{i}' for i in range(num_feat)]\ncols = basic_cols + features\n\ncol_dtypes = {\n    #'row_id' : np.object,\n    #'time_id' : np.int32,\n    'investment_id' : np.int32,\n    'target' : np.float32,\n}\nfor i in range(300):\n    col_dtypes[f\"f_{i}\"] = np.float32","metadata":{"execution":{"iopub.status.busy":"2022-01-22T11:59:28.178282Z","iopub.execute_input":"2022-01-22T11:59:28.178521Z","iopub.status.idle":"2022-01-22T11:59:29.39348Z","shell.execute_reply.started":"2022-01-22T11:59:28.178488Z","shell.execute_reply":"2022-01-22T11:59:29.392744Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"full_train_df = pd.read_csv(\n        \"../input/ubiquant-market-prediction/train.csv\",\n        dtype=col_dtypes,\n        usecols=cols\n)\nfull_train_df","metadata":{"execution":{"iopub.status.busy":"2022-01-22T11:59:29.394888Z","iopub.execute_input":"2022-01-22T11:59:29.395159Z","iopub.status.idle":"2022-01-22T12:05:52.339572Z","shell.execute_reply.started":"2022-01-22T11:59:29.395122Z","shell.execute_reply":"2022-01-22T12:05:52.33885Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model creation and Training","metadata":{}},{"cell_type":"code","source":"from torch import nn\n\nclass RegressionModel(torch.nn.Module):\n    def __init__(self, in_shape, out_shape, hidden, device='cpu'):\n        super().__init__()\n        self.in_shape = in_shape\n        self.out_shape = out_shape\n        self.hidden = hidden\n        self.device = device\n        self.initialize_weights()\n        \n    def initialize_weights(self):\n        self.w1 = torch.nn.Parameter(torch.randn((self.hidden, self.in_shape), device=self.device, requires_grad=True))\n        self.w2 = torch.nn.Parameter(torch.randn((self.out_shape, self.hidden), device=self.device, requires_grad=True))\n        self.b1 = torch.nn.Parameter(torch.randn(1, device=self.device, requires_grad=True))\n        self.b2 = torch.nn.Parameter(torch.randn(1, device=self.device, requires_grad=True))\n        self.drop = torch.nn.Dropout(p=0.1)\n    \n    def forward(self, x):\n        #basic linear computation\n        y_hat = torch.add(torch.mm(self.w1, x.t()), self.b1)\n        #Apply relu\n        y_hat = self.drop(torch.relu(y_hat))\n        #return regression out\n        return torch.add(torch.mm(self.w2, y_hat), self.b2)\n\nclass TimeSeriesModel(nn.Module):\n    def __init__(self, in_shape, out_shape=1, hidden_shape=64, embed_size=32, device=None):\n        super().__init__()\n        self.in_shape = in_shape\n        self.out_shape = out_shape\n        self.hidden_shape = hidden_shape\n        self.embed_size = embed_size\n        self.layer_dim = 16\n        self.device = device\n        self.init_layers()\n    \n    def init_layers(self):\n        self.embedding = nn.Embedding(N_INVID+1, self.embed_size).to(self.device)\n        self.gru = nn.GRU(self.in_shape, self.hidden_shape, self.layer_dim,\n                          batch_first=True, dropout=0.1).to(self.device)\n        self.out = RegressionModel(self.hidden_shape, self.out_shape, self.layer_dim, self.device)\n    \n    def forward(self, x):\n        #Embed investment_id\n        emb = self.embedding(x[0])\n        emb = torch.mul(x[1], emb)\n        emb = torch.reshape(emb, (x[0].size(0), -1, emb.size(1)))\n        emb = torch.reshape(x[1], (x[1].size(0), -1, x[1].size(1)))\n        \n        # Initializing hidden state for first input with zeros\n        h0 = torch.zeros(self.layer_dim, emb.size(0), self.hidden_shape,\n                         device=self.device, requires_grad=True)\n        \n        # Forward propagation by passing in the input and hidden state into the model\n        y_hat, _ = self.gru(emb, h0.detach())\n        y_hat = torch.reshape(y_hat, (y_hat.size(0)*y_hat.size(1), -1))\n        \n        y_hat = self.out(y_hat)\n        return y_hat[-1, :]","metadata":{"execution":{"iopub.status.busy":"2022-01-22T12:05:52.34163Z","iopub.execute_input":"2022-01-22T12:05:52.342046Z","iopub.status.idle":"2022-01-22T12:05:52.360092Z","shell.execute_reply.started":"2022-01-22T12:05:52.342007Z","shell.execute_reply":"2022-01-22T12:05:52.35918Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"batch_size = 65536\nbatches = []\nfor i in range(0, full_train_df.shape[0], batch_size):\n    batches.append((i, min(full_train_df.shape[0], i+batch_size)))","metadata":{"execution":{"iopub.status.busy":"2022-01-22T12:05:52.361354Z","iopub.execute_input":"2022-01-22T12:05:52.361728Z","iopub.status.idle":"2022-01-22T12:05:52.375117Z","shell.execute_reply.started":"2022-01-22T12:05:52.361692Z","shell.execute_reply":"2022-01-22T12:05:52.37439Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split as tts\nfrom IPython.display import clear_output\nimport torch.optim as optim\n\ndef loss(y_predicted, y_target):\n    #RMSE Loss\n    return torch.sum((y_predicted - y_target)**2)\n\nmodel = TimeSeriesModel(num_feat, hidden_shape=64, embed_size=1, device=device)\nepochs = 1000\nverbose = max(1, epochs // 100)\ntol = epochs//5\n\noptimizer = optim.Adam(model.parameters(), lr=0.001)\nscheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer,\n                                                 len(batches),\n                                                 last_epoch=-1,\n                                                 eta_min=1e-5,\n                                                 verbose=False)\nmin_loss = np.inf\ncnt = 0\n\ntrain_size = 0\nvalid_size = 0\n\nfor t in tqdm(range(1, epochs+1), desc=\"Training\"):\n    #clear_output(wait=True)\n    # Set the gradients to 0.\n    optimizer.zero_grad()\n    total_loss = 0.0\n    valid_loss = 0.0\n    total_train_size = 0\n    total_valid_size = 0\n    \n    for start, end in batches:\n        #Split train and validation data\n        x_dataset, x_valid, y_dataset, y_valid = tts(\n            full_train_df.iloc[start:end][['investment_id']+features],\n            full_train_df.iloc[start:end]['target'].values,\n            test_size=0.1, shuffle=True, random_state=seed)\n        total_train_size += x_dataset.shape[0]\n        total_valid_size += x_valid.shape[0]\n        \n        #Train\n        tx_data = torch.tensor(x_dataset[features].values, dtype=torch.float).to(device)\n        tinv_data = torch.tensor(x_dataset['investment_id'].values, dtype=torch.int).to(device)\n        ty_data = torch.tensor(y_dataset, dtype=torch.float).to(device)\n        \n        #Validation\n        vx_data = torch.tensor(x_valid[features].values, dtype=torch.float).to(device)\n        vinv_data = torch.tensor(x_valid[\"investment_id\"].values, dtype=torch.int).to(device)\n        vy_data = torch.tensor(y_valid, dtype=torch.float).to(device)\n    \n    \n        # Main optimization loop\n        model.train()\n        # Compute the current predicted y's from x_dataset\n        y_predicted = model((tinv_data, tx_data))\n        # See how far off the prediction is\n        current_loss = loss(y_predicted, ty_data)\n        total_loss += current_loss\n        \n        # Compute the gradient of the loss\n        current_loss.backward()\n        # Update model W and b accordingly.\n        optimizer.step()\n        # Update LR of optimizer\n        scheduler.step()\n        \n        #Compute validation loss\n        with torch.no_grad():\n            model.eval() #Change model to evaluation mode\n            vloss = loss(model((vinv_data, vx_data)), vy_data)\n            valid_loss += vloss\n\n    #Check for early stopping\n    if valid_loss >= min_loss:\n        cnt += 1\n        if cnt >= tol:\n            print(\"Early stopping!\")\n            break\n    else:\n        #Save the model weights\n        torch.save(model.state_dict(), \"model_weights.pth\")\n        min_loss = valid_loss\n        cnt = 0\n\n    if t%verbose==0:\n        print(f\"epoch = {t:2}/{epochs}, \" +\n              f\"RMSE loss = {torch.sqrt(total_loss/total_train_size):.6f}, \" +\n              f\"MSE loss = {(total_loss/total_train_size):.6f}, \" +\n              f\"RMSE valid_loss = {torch.sqrt(valid_loss/total_valid_size):.6f}, \" +\n              f\"min_loss = {torch.sqrt(min_loss/total_valid_size):.6f}, \" +\n              f\"cnt={cnt}\"\n             )\n        model.train() #Return to train mode\n    \nprint(f\"Total time spent: {time.time()-s:.4f} seconds\")","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-01-22T12:05:52.376768Z","iopub.execute_input":"2022-01-22T12:05:52.377334Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Prediction","metadata":{}},{"cell_type":"code","source":"import ubiquant\nenv = ubiquant.make_env()\niter_test = env.iter_test()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Switch to evaluation mode\nmodel.load_state_dict(torch.load(\"model_weights.pth\"))\nmodel.eval()\nfor (test_df, sample_prediction_df) in iter_test:\n    inv_x = torch.tensor(test_df['investment_id'].values, dtype=torch.int).to(device)\n    test_x = torch.tensor(test_df[features].values, dtype=torch.float).to(device)\n    pred = model((inv_x, test_x))\n    sample_prediction_df['target'] = pred.detach().cpu().numpy()\n    env.predict(sample_prediction_df)\n    display(sample_prediction_df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}