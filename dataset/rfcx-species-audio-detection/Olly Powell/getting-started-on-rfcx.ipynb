{"cells":[{"metadata":{},"cell_type":"markdown","source":"This is my first Kaggle comp.  I've started with the baseline PyTorch kernel <a href=\"https://www.kaggle.com/fffrrt/all-in-one-rfcx-baseline-for-beginners\">All-in-one-rfcx-baseline-for-beginners</a> and tried to make improvements.  So far nothing I've tried has actually improved the score, so not great for confidence building, and and a lot of the other notebooks are going over my head just because I'm a relative newcomer to OOP.   But I'm learning a lot for next time!\n\nI've noticed that a lot of the same songs are repeated for a given recording.  So by slicing up recordings to just isolate the songs and training on the slices, those patterns get lost.  So in this version I'm going to trying to create a second model, taking the probability vector from the first, grouping by recording ID, pass the combined vectors through a dense MLP or maybe a LSTM network, and train that model using a custom LWLRAP loss metric.  Then for the final predictions I would run the test set through both models."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Housekeeping stuff\n\nimport os\nimport torch\nimport torch.nn as nn\nimport numpy as np\nimport pandas as pd\nimport librosa\nimport csv\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom skimage.transform import resize\nfrom PIL import Image\nimport random\n\nsave_to_disk = False  # A variable used to determine if the image files are left on disk.\n\nprint(\"Initial working directory path: {0}\".format(os.getcwd()))\ncurrent_folder = os.path.basename(os.path.normpath(os.getcwd()))\nprint('Working directory name:',current_folder)\n\n# Change the current working directory if necessary\nif not current_folder == 'working':\n    os.chdir('../working')\ncwd = os.getcwd() + '/'\nprint(\"Current working directory: {0}\".format(cwd))\n\n# check if CUDA is available\ngpu_available = torch.cuda.is_available()\n\nif not gpu_available:\n    print('CUDA is not available :( Training must be done on CPU')\nelse:\n    print('CUDA is available! Training can be done on GPU')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Take a look at the data provided:\ndata_tp=pd.read_csv('../input/rfcx-species-audio-detection/train_tp.csv')\ndata_fp=pd.read_csv('../input/rfcx-species-audio-detection/train_fp.csv')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#True positive Class balance \nplt.figure(figsize=(16,8))\nsns.countplot(data_tp.species_id)\nplt.title('True_positives from the training folder')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#False Positives\nplt.figure(figsize=(16,8))\nsns.countplot(data_fp.species_id)\nplt.title('False positives')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_tp.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_fp.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking if there are true positive recordings with multiple labels.\n\ndata_tp_multi = data_tp['species_id'].groupby(data_tp.recording_id).apply(list).reset_index()\ndata_tp_multi['labels'] = data_tp_multi.species_id.map(len)\ndata_tp_multi = data_tp_multi.sort_values('labels', ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_tp_multi.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# So it would be nice to show this with some graphics, \n# Anyway interesting that the labels are often the same, so there are patterns to be found at the recording level.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Generating Mel spectrograms for training from true positive data"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"fft = 2048\nhop = 512\n# Less rounding errors this way\nsr = 48000\nlength = 10 * sr   #This step here is the length of the slices, that go around each labelled song.\n\nwith open('../input/rfcx-species-audio-detection/train_tp.csv') as f:    \n    reader = csv.reader(f)\n    data = list(reader)\n\n# Check minimum/maximum frequencies for bird calls\n# Not neccesary, but there are usually plenty of noise in low frequencies, and removing it helps\nfmin = 24000\nfmax = 0\n\n# Skip header row (recording_id,species_id,songtype_id,t_min,f_min,t_max,f_max) and start from 1 instead of 0\nfor i in range(1, len(data)):\n    if fmin > float(data[i][4]):\n        fmin = float(data[i][4])\n    if fmax < float(data[i][6]):\n        fmax = float(data[i][6])\n# Get some safety margin\nfmin = int(fmin * 0.9)\nfmax = int(fmax * 1.1)\nprint('Minimum frequency: ' + str(fmin) + ', maximum frequency: ' + str(fmax))\n\nprint('Starting spectrogram generation')\nfor i in range(1, len(data)):\n    # All sound files are 48000 bitrate, no need to slowly resample\n    wav, sr = librosa.load('../input/rfcx-species-audio-detection/train/' + data[i][0] + '.flac', sr=None)\n    \n    t_min = float(data[i][3]) * sr\n    t_max = float(data[i][5]) * sr\n    \n    # Positioning sound slice\n    center = np.round((t_min + t_max) / 2)\n    beginning = center - length / 2\n    if beginning < 0:\n        beginning = 0\n    \n    ending = beginning + length\n    if ending > len(wav):\n        ending = len(wav)\n        beginning = ending - length\n        \n    slice = wav[int(beginning):int(ending)]\n    \n    # Mel spectrogram generation\n    # Default settings were bad, parameters are adjusted to generate somewhat reasonable quality images\n    # The better your images are, the better your neural net would perform\n    # You can also use librosa.stft + librosa.amplitude_to_db instead\n    mel_spec = librosa.feature.melspectrogram(slice, n_fft=fft, hop_length=hop, sr=sr, fmin=fmin, fmax=fmax, power=1.5)\n    mel_spec = resize(mel_spec, (224, 400))\n    \n    # Normalize to 0...1 - this is what goes into neural net\n    mel_spec = mel_spec - np.min(mel_spec)\n    mel_spec = mel_spec / np.max(mel_spec)\n\n    # And this 0...255 is for the saving in bmp format\n    mel_spec = mel_spec * 255\n    mel_spec = np.round(mel_spec)    \n    mel_spec = mel_spec.astype('uint8')\n    mel_spec = np.asarray(mel_spec)\n    \n    bmp = Image.fromarray(mel_spec, 'L')\n    bmp.save(cwd + data[i][0] + '_' + data[i][1] + '_' + str(center) + '.bmp')\n    \n    if i % 200 == 0:\n        print('Processed ' + str(i) + ' train examples from ' + str(len(data)))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Show some spectrograms","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Settings and random seeds initialization for reproducible results"},{"metadata":{"trusted":true},"cell_type":"code","source":"num_birds = 24\n# 6GB GPU-friendly (~4 GB used by model)\n# Increase if neccesary\nbatch_size = 16 # Tried 32 but got no improvement  \n\n# This is enough to exactly reproduce results on local machine (Windows / Turing GPU)\n# Kaggle GPU kernels (Linux / Pascal GPU) are not deterministic even with random seeds set\n# Your score might vary a lot (~up to 0.05) on a different runs due to picking different epochs to submit\nrng_seed = 1234\nrandom.seed(rng_seed)\nnp.random.seed(rng_seed)\nos.environ['PYTHONHASHSEED'] = str(rng_seed)\ntorch.manual_seed(rng_seed)\ntorch.cuda.manual_seed(rng_seed)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Model dataset class"},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch.utils.data as torchdata\n\nclass RainforestDataset(torchdata.Dataset):\n    def __init__(self, filelist):\n        self.specs = []\n        self.labels = []\n        for f in filelist:\n            # Easier to pass species in filename at the start; worth changing later to more capable method\n            label = int(str.split(f, '_')[1])\n            label_array = np.zeros(num_birds, dtype=np.single)\n            label_array[label] = 1.\n            self.labels.append(label_array)\n            \n            # Open and save spectrogram to memory\n            \n            # If you use more spectrograms (add train_fp, for example), then they would not all fit to memory\n            # In this case you should load them on the fly in __getitem__\n            img = Image.open(f)\n            mel_spec = np.array(img)\n            img.close()\n            \n            # Transforming spectrogram from bmp to 0..1 array\n            mel_spec = mel_spec / 255\n            # Stacking for 3-channel image for resnet\n            mel_spec = np.stack((mel_spec, mel_spec, mel_spec))\n            \n            self.specs.append(mel_spec)\n    \n    def __len__(self):\n        return len(self.specs)\n    \n    def __getitem__(self, item):\n        # Augment here if you want\n        return self.specs[item], self.labels[item]\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Split training set on training and validation  \n  \nWhat StratifiedKFold does:  \n![StratifiedKFold](https://scikit-learn.org/stable/_images/sphx_glr_plot_cv_indices_003.png)"},{"metadata":{"trusted":true},"cell_type":"code","source":"file_list = []\nlabel_list = []\n\nfor f in os.listdir(cwd):\n    if '.bmp' in f:\n        file_list.append(f)\n        label = str.split(f, '_')[1]\n        label_list.append(label)\n\nfrom sklearn.model_selection import StratifiedKFold\n\nskf = StratifiedKFold(n_splits=5, shuffle=True, random_state=rng_seed)\n\ntrain_files = []\nval_files = []\n\nfor fold_id, (train_index, val_index) in enumerate(skf.split(file_list, label_list)):\n    # Picking only first fold to train/val on\n    # This means loss of 20% training data\n    # To avoid this, you can train 5 different models on 5 folds and average predictions\n    if fold_id == 0:\n        train_files = np.take(file_list, train_index)\n        val_files = np.take(file_list, val_index)\n\nprint('Training on ' + str(len(train_files)) + ' examples')\nprint('Validating on ' + str(len(val_files)) + ' examples')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Preparing everything for training"},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install resnest > /dev/null","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from resnest.torch import resnest50\n\ntrain_dataset = RainforestDataset(train_files)\nval_dataset = RainforestDataset(val_files)\n\ntrain_loader = torchdata.DataLoader(train_dataset, batch_size=batch_size, sampler=torchdata.RandomSampler(train_dataset))\nval_loader = torchdata.DataLoader(val_dataset, batch_size=batch_size, sampler=torchdata.RandomSampler(val_dataset))\n\n# ResNeSt: Split-Attention Networks\n# https://arxiv.org/abs/2004.08955\n# Significantly outperforms standard Resnet\n\nmodel = resnest50(pretrained=True)\n\nmodel.fc = nn.Sequential(\n    nn.Linear(2048, 1024),\n    nn.ReLU(),\n    nn.Dropout(p=0.2),\n    nn.Linear(1024, 1024),\n    nn.ReLU(),\n    nn.Dropout(p=0.2),\n    nn.Linear(1024, num_birds)\n)\n\n# Picked for this notebook; pick new ones after major changes (such as adding train_fp to train data)\noptimizer = torch.optim.SGD(model.parameters(), lr=0.01, weight_decay=0.0001, momentum=0.9)\nscheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.4)\n\n# This loss function is not exactly suited for competition metric, which only cares about ranking of predictions\npos_weights = torch.ones(num_birds)\npos_weights = pos_weights * num_birds\nloss_function = nn.BCEWithLogitsLoss(pos_weight=pos_weights)\n#loss_function = nn.CrossEntropyLoss()   # Tried this but got an error.\n\nif gpu_available:\n    model = model.cuda()\n    loss_function = loss_function.cuda()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Training model on saved spectrograms"},{"metadata":{"trusted":true,"tags":[]},"cell_type":"code","source":"best_corrects = 0\n\n# Train loop\nprint('Starting training loop')\nfor e in range(0, 2):    # 32 is fine Tried increasing to 64 but no improvement.  Look into this later when doing more to augment data and monitor losses.\n    # Stats\n    train_loss = []\n    train_corr = []\n    \n    # Single epoch - train\n    model.train()\n    for batch, (data, target) in enumerate(train_loader):\n        data = data.float()\n        if gpu_available:\n            data, target = data.cuda(), target.cuda()\n            \n        optimizer.zero_grad()\n        \n        output = model(data)\n        loss = loss_function(output, target)\n\n        loss.backward()\n        optimizer.step()\n        \n        # Stats\n        vals, answers = torch.max(output, 1)\n        vals, targets = torch.max(target, 1)\n        corrects = 0\n        for i in range(0, len(answers)):\n            if answers[i] == targets[i]:\n                corrects = corrects + 1\n        train_corr.append(corrects)\n        \n        train_loss.append(loss.item())\n    \n    # Stats\n    for g in optimizer.param_groups:\n        lr = g['lr']\n    print('Epoch ' + str(e) + ' training end. LR: ' + str(lr) + ', Loss: ' + str(sum(train_loss) / len(train_loss)) +\n          ', Correct answers: ' + str(sum(train_corr)) + '/' + str(train_dataset.__len__()))\n    \n    # Single epoch - validation\n    with torch.no_grad():\n        # Stats\n        val_loss = []\n        val_corr = []\n        \n        model.eval()\n        for batch, (data, target) in enumerate(val_loader):\n            data = data.float()\n            if torch.cuda.is_available():\n                data, target = data.cuda(), target.cuda()\n\n            output = model(data)\n            loss = loss_function(output, target)\n            #loss = LWLRAP(output.cpu(), target.cpu()) # I don't think this achieves much, since we're training on 10 second clips around the labelled song.\n            # Stats\n            vals, answers = torch.max(output, 1)\n            vals, targets = torch.max(target, 1)\n            corrects = 0\n            for i in range(0, len(answers)):\n                if answers[i] == targets[i]:\n                    corrects = corrects + 1\n            val_corr.append(corrects)\n        \n            #val_loss.append(loss.item())  #This one for a tensor\n            val_loss.append(loss)\n    \n    # Stats\n    print('Epoch ' + str(e) + ' validation end. LR: ' + str(lr) + ', Loss: ' + str(sum(val_loss) / len(val_loss)) +\n          ', Correct answers: ' + str(sum(val_corr)) + '/' + str(val_dataset.__len__()))\n    \n    # If this epoch is better than previous on validation, save model\n    # Validation loss is the more common metric, but in this case our loss is misaligned with competition metric, making accuracy a better metric\n    if sum(val_corr) > best_corrects:\n        print('Saving new best model at epoch ' + str(e) + ' (' + str(sum(val_corr)) + '/' + str(val_dataset.__len__()) + ')')\n        torch.save(model, cwd + 'best_model.pt')\n        best_corrects = sum(val_corr)\n        \n    # Call every epoch\n    scheduler.step()\n\n# Free memory\ndel model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"Pseudocode:\n* Take each of the previously prepared annotated slices\n* Run them through the model to provide a probablility vector\n* Group the vectors and their labels by sound recording (up to five per recording, so 24 x n)\n* Order by t_min so they have a time order\n* Flatten into a vector\n* Use for each of the groups > n=1 train a dense MLP with about four or five layers LHS is a 24xn,  256, 256, 256, 24xn vector again\n* Use the LWLRAP metric on both to give a training loss.  (It works on a 24xn tensor)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Custom loss function for LW-LRAP\n# https://www.kaggle.com/c/rfcx-species-audio-detection/discussion/198418   \n# label-level average\n# Assume float preds [BxC], labels [BxC] of 0 or 1\n\ndef LWLRAP(preds, labels):\n    # Ranks of the predictions\n    ranked_classes = torch.argsort(preds, dim=-1, descending=True)\n    # i, j corresponds to rank of prediction in row i\n    class_ranks = torch.zeros_like(ranked_classes)\n    for i in range(ranked_classes.size(0)):\n        for j in range(ranked_classes.size(1)):\n            class_ranks[i, ranked_classes[i][j]] = j + 1\n    # Mask out to only use the ranks of relevant GT labels\n    ground_truth_ranks = class_ranks * labels + (1e6) * (1 - labels)\n    # All the GT ranks are in front now\n    sorted_ground_truth_ranks, _ = torch.sort(ground_truth_ranks, dim=-1, descending=False)\n    # Number of GT labels per instance\n    num_labels = labels.sum(-1)\n    pos_matrix = torch.tensor(np.array([i+1 for i in range(labels.size(-1))])).unsqueeze(0)\n    score_matrix = pos_matrix / sorted_ground_truth_ranks\n    score_mask_matrix, _ = torch.sort(labels, dim=-1, descending=True)\n    scores = score_matrix * score_mask_matrix\n    score = scores.sum() / labels.sum()\n    return score.item()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Prepare the image files for classification by the trained model\n\nprobability_df = pd.DataFrame(columns=['recording_id','s0','s1','s2','s3','s4','s5','s6','s7','s8','s9','s10','s11',\n                               's12','s13','s14','s15','s16','s17','s18','s19','s20','s21','s22','s23'])\n\nfile_list = []\nlabel_list = []\n\nfor f in os.listdir(cwd):\n    if '.bmp' in f:\n        file_list.append(f)\n        label = str.split(f, '_')[1]\n        label_list.append(label)\n\nprint('A total of {} .bmp image files found'.format(len(file_list)))\n\ntrain_dataset = RainforestDataset(file_list)\ntrain_loader = torchdata.DataLoader(train_dataset)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = resnest50(pretrained=True)\nsigmoid = torch.nn.Sigmoid()\n\nmodel.fc = nn.Sequential(\n    nn.Linear(2048, 1024),\n    nn.ReLU(),\n    nn.Dropout(p=0.2),\n    nn.Linear(1024, 1024),\n    nn.ReLU(),\n    nn.Dropout(p=0.2),\n    nn.Linear(1024, num_birds)\n)\n\nmodel = torch.load(cwd + 'best_model.pt')\nmodel.eval()\n\nif gpu_available:\n    model.cuda()\n\n# Prediction loop\nprint('Starting prediction loop on whole training set', batch)\n\nfor batch, (data, target) in enumerate(train_loader):  #default batch size = 1\n    data = data.float()\n    if gpu_available:\n        data = data.cuda()\n\n    output = sigmoid(model(data)).cpu().detach()  # converting output to a probability between 0 and 1\n    \n    # Append a line to the dataframe\n    file_id = str.split(file_list[batch], '.')[0]\n    output_list = [element.item() for element in output.flatten()]\n    probability_df.loc[batch] = [file_id] + output_list\n        \n    if i % 200 == 0 and i > 0:\n        print('Predicted for ' + str(i) + ' of ' + str(len(test_files) + 1) + ' files')\n\nprint('Prediction vectors ready')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"probability_df.head(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Function to split and load one test file"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Already defined above; for reference\n\n# fft = 2048\n# hop = 512\n# sr = 48000\n# length = 10 * sr\n\ndef load_test_file(f):\n    wav, sr = librosa.load('../input/rfcx-species-audio-detection/test/' + f, sr=None)\n\n    # Split for enough segments to not miss anything\n    segments = len(wav) / length\n    segments = int(np.ceil(segments))\n    \n    mel_array = []\n    \n    for i in range(0, segments):\n        # Last segment going from the end\n        if (i + 1) * length > len(wav):\n            slice = wav[len(wav) - length:len(wav)]\n        else:\n            slice = wav[i * length:(i + 1) * length]\n        \n        # Same mel spectrogram as before\n        mel_spec = librosa.feature.melspectrogram(slice, n_fft=fft, hop_length=hop, sr=sr, fmin=fmin, fmax=fmax, power=1.5)\n        mel_spec = resize(mel_spec, (224, 400))\n    \n        mel_spec = mel_spec - np.min(mel_spec)\n        mel_spec = mel_spec / np.max(mel_spec)\n        \n        mel_spec = np.stack((mel_spec, mel_spec, mel_spec))\n\n        mel_array.append(mel_spec)\n    \n    return mel_array","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Submitting predictions with best model"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Loading model back\nmodel = resnest50(pretrained=True)\n\nmodel.fc = nn.Sequential(\n    nn.Linear(2048, 1024),\n    nn.ReLU(),\n    nn.Dropout(p=0.2),\n    nn.Linear(1024, 1024),\n    nn.ReLU(),\n    nn.Dropout(p=0.2),\n    nn.Linear(1024, num_birds)\n)\n\nmodel = torch.load(cwd + 'best_model.pt')\nmodel.eval()\n\n# Scoring does not like many files:(\nif not save_to_disk:\n    for f in os.listdir():\n        os.remove(f)\n\nif gpu_available:\n    model.cuda()\n\n# Prediction loop\nprint('Starting prediction loop')\nwith open('submission.csv', 'w', newline='') as csvfile:\n    submission_writer = csv.writer(csvfile, delimiter=',')\n    submission_writer.writerow(['recording_id','s0','s1','s2','s3','s4','s5','s6','s7','s8','s9','s10','s11',\n                               's12','s13','s14','s15','s16','s17','s18','s19','s20','s21','s22','s23'])\n                               \n    test_files = os.listdir('../input/rfcx-species-audio-detection/test/')\n    print(len(test_files))\n    \n    # Every test file is split on several chunks and prediction is made for each chunk\n    for i in range(0, len(test_files)):\n    #for i in range(0, 400):  #just for code dev purposes\n        data = load_test_file(test_files[i])\n        data = torch.tensor(data)\n        data = data.float()\n        if gpu_available:\n            data = data.cuda()\n\n        output = model(data)\n\n        # Taking max prediction from all slices per bird species\n        # Usually you want Sigmoid layer here to convert output to probabilities\n        # In this competition only relative ranking matters, and not the exact value of prediction, so we can use it directly\n        maxed_output = torch.max(output, dim=0)[0]\n        maxed_output = maxed_output.cpu().detach()\n        \n        file_id = str.split(test_files[i], '.')[0]\n        write_array = [file_id]\n        \n        for out in maxed_output:\n            write_array.append(out.item())\n    \n        submission_writer.writerow(write_array)\n        \n        if i % 200 == 0 and i > 0:\n            print('Predicted for ' + str(i) + ' of ' + str(len(test_files) + 1) + ' files')\n\nprint('Submission generated')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Free memory\ndel model\nif gpu_available:\n    torch.cuda.empty_cache()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}