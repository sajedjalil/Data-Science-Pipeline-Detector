{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"a95149b1-8c3e-b9b0-9ab9-f2b8c4e04dfd"},"source":"# Optimizing maximum scores of multiple submissions\n\nWe can submit the to leaderboard multiple times and our best submission counts. Therefore, it is not necessarily best to go for submissions with the largest expected score. If we create submissions with large variance and try many of them, can we get lucky?\n\nI believe the answer is no! So don't expect too much here ;)\n\nIn this notebook, I try to estimate some stuff to show why I believe it's not a good idea. I create a simplified model of Santa's bag packing problem and optimize for the largest order statistic over a number of submissions. The outline is as follows:\n\n1. Exploratory visualizations to understand the problem\n2. Computing utility distributions of a large number of different kinds of bags\n3. Packing bags and visualizing trade-off between mean and variance of score distributions\n4. Creating random submissions"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e2f8916c-8081-a08b-217d-d626341ece2b"},"outputs":[],"source":"import sys\nimport os\nimport math\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom scipy.stats import norm, gumbel_r\nfrom scipy.optimize import linprog\nimport matplotlib\nimport matplotlib.pyplot as plt\nfrom sklearn.utils.extmath import cartesian\n%matplotlib inline"},{"cell_type":"markdown","metadata":{"_cell_guid":"82b8c0ed-f6a6-ceb6-7f87-96cc61894387"},"source":"# 1. Exploration via sampling\nLet's visualize distributions of individual items and bags.\nWe look at both their weights as well as their utilities (their contribution to the score).\nRules are that you need at least 3 items per bag and their combined weight must not exceed 50."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"31e50876-665f-fa3c-e584-4a1af97c1ea6"},"outputs":[],"source":"def sample_horse(size=1):\n    return np.maximum(0, np.random.normal(5,2,size))\n\ndef sample_ball(size=1):\n    return np.maximum(0, 1 + np.random.normal(1,0.3,size))\n\ndef sample_bike(size=1):\n    return np.maximum(0, np.random.normal(20,10,size))\n\ndef sample_train(size=1):\n    return np.maximum(0, np.random.normal(10,5,size))\n\ndef sample_coal(size=1):\n    return 47 * np.random.beta(0.5,0.5,size)\n\ndef sample_book(size=1):\n    return np.random.chisquare(2,size)\n\ndef sample_doll(size=1):\n    return np.random.gamma(5,1,size)\n\ndef sample_block(size=1):\n    return np.random.triangular(5,10,20,size)\n\ndef sample_gloves(size=1):\n    dist1 = 3.0 + np.random.rand(size)\n    dist2 = np.random.rand(size)\n    toggle = np.random.rand(size) < 0.3\n    dist2[toggle] = dist1[toggle]\n    return dist2\n\nsamplers = {\n    \"horse\": sample_horse,\n    \"ball\": sample_ball,\n    \"bike\": sample_bike,\n    \"train\": sample_train,\n    \"coal\": sample_coal,\n    \"book\": sample_book,\n    \"doll\": sample_doll,\n    \"blocks\": sample_block,\n    \"gloves\": sample_gloves\n}\n\ndef sample(gift, quantity=1, size=1):\n    return np.sum(samplers[gift](quantity * size).reshape(quantity, size), axis=0)\n\nprint(sample(\"horse\", 2, 1))"},{"cell_type":"markdown","metadata":{"_cell_guid":"4acd5ee9-7a61-bb17-9159-425454439acb"},"source":"Ok, so we can sample weights for individual gifts now and can also increase their quantities.\nLet's combine different gifts into bags now."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"fb5e1f91-aa24-4052-74a9-1f32884a0ef7"},"outputs":[],"source":"def bag_name(bag):\n    return str(list(map(lambda gift: \"{}({})\".format(gift, bag[gift]), sorted(bag.keys()))))\n\ndef create_bag_weight_sampler(bag):\n    def bag_weight_sampler(size=1):\n        weight = np.array([0.0]*size)\n        for gift in sorted(bag.keys()):\n            weight += sample(gift, bag[gift], size)\n        return weight\n    return bag_weight_sampler, bag_name(bag)\n\nbag = { \"horse\": 1, \"ball\": 2 }\nbag_weight_sampler, name = create_bag_weight_sampler(bag)\nprint(\"Sampling from bag {}: {}\".format(name, bag_weight_sampler(3)))"},{"cell_type":"markdown","metadata":{"_cell_guid":"cadc00f9-2b36-ebbc-d306-584562fa0a24"},"source":"Got the bag weight samplers, so let's see what the distributions look like:"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"40f0a421-6d4f-385f-eae7-efac093157f7"},"outputs":[],"source":"def plot_bag_weight_distributions(bags, size=10000):\n    plot_distributions(bags, create_bag_weight_sampler, size=size, fit=norm)\n\ndef plot_distributions(bags, sampler_builder, size=10000, fit=None):\n    num_plots = len(bags)\n    num_cols = int(round(math.sqrt(num_plots)))\n    num_rows = (num_plots // num_cols)\n    num_rows = num_rows if num_plots % num_cols == 0 else num_rows + 1\n    \n    f, axes = plt.subplots(num_rows, num_cols)\n    axes = axes.reshape(-1)\n    for i in range(num_plots):\n        current_bag = bags[i]\n        current_bag_sampler, current_bag_name = sampler_builder(current_bag)\n        current_sample = current_bag_sampler(size)\n        print(\"{}: mean={} | std={}\".format(current_bag_name, np.mean(current_sample), np.std(current_sample)))\n        current_axis = axes[i]\n        sns.distplot(current_sample, ax=current_axis, fit=fit, kde=False)\n        current_axis.set_title(current_bag_name)\n        current_axis.set_yticklabels([])\n    plt.tight_layout()\n    plt.show()\n    \nsingle_gift_bags = [\n    {\"horse\": 1},\n    {\"ball\": 1},\n    {\"bike\": 1},\n    {\"train\": 1},\n    {\"coal\": 1},\n    {\"book\": 1},\n    {\"doll\": 1},\n    {\"blocks\": 1},\n    {\"gloves\": 1}\n]\n\nplot_bag_weight_distributions(single_gift_bags)"},{"cell_type":"markdown","metadata":{"_cell_guid":"6efabca6-7bec-30f4-17d1-4ecb83abd754"},"source":"Note: some gift weight distributions are (almost) normal, others not at all.\nLet's look at bigger bags now:"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"870f3f59-dea6-719a-8398-a1f7d10292fa"},"outputs":[],"source":"example_bags = [\n    {\"horse\": 1, \"ball\": 2},\n    {\"train\": 3, \"bike\": 1},\n    {\"coal\": 2, \"book\": 2},\n    {\"gloves\": 12, \"book\": 12},\n]\n\nplot_bag_weight_distributions(example_bags)"},{"cell_type":"markdown","metadata":{"_cell_guid":"8a89caea-a117-55f5-e7e2-f2303d4b7c11"},"source":"Nice :)\nEven though individual gifts are not necessarily normal, big bags will almost certainly be (if we pack sufficiently many gifts into them).\nLet's take a break to thank nature for the central limit theorem.\nThis is really cool because we can approximate the bag weight distribution by just summing up means and variances of the gift weight distributions.\n\nNow: estimate how utilities of bags will be distributed.\nI'll ignore the `enter code here`minimum count of 3 here and only do clipping at weight 50."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"90b1886e-86c6-2bcb-fd30-02ceffec56aa"},"outputs":[],"source":"def plot_bag_utility_distributions(bags, size=10000, fit=norm):\n    plot_distributions(bags, create_bag_utility_sampler, size=size, fit=fit)\n\ndef create_bag_utility_sampler(bag):\n    bag_weight_sampler, bag_name = create_bag_weight_sampler(bag)\n    def bag_utility_sampler(size=1):\n        samples = bag_weight_sampler(size)\n        samples[samples > 50] = 0\n        return samples\n    return bag_utility_sampler, bag_name\n\nbag = { \"horse\": 2, \"ball\": 19 }\nbag_utility_sampler, name = create_bag_utility_sampler(bag)\nprint(\"Sampling utility from bag {}: {}\\n\".format(name, bag_utility_sampler(9)))\nplot_bag_utility_distributions(example_bags)"},{"cell_type":"markdown","metadata":{"_cell_guid":"5bb70d85-4f8d-8aea-1a0b-45e4855e1b13"},"source":"As expected, now that we clip at 50, distributions are again not at all normal because we get a lot of zeors (for bags with expected weighy close to 50, which we need for good scores).\nStill, as we will pack a 1000 bags, the central limit theorem will again come to rescue and things will get back to normal when we add up many bags.\nWe will be able to approximate the final score distribution just by summing up means and variances of the bag utility distributions.\nThus, it makes sense to also characterize bag utilities by mean and standard deviation.\n\nLet's now see what our score will be if we combine a lot of bags. One part is estimating the score distribution we will get from the bags. Furthermore, we will estimate what the best score would be if we could draw multiple times from that score distribution. This makes sense since we can submit to the leaderboard multiple times with randomly permuted gifts inside the bags. Hence, what we will get in the end is the best out of many trials.\n\nImportant to note: what I do below assumed iid draws from the score distributions.\nFor the leaderboard gift weights are fixed and we can only generate permutations of IDs.\nThus, the assumptions don't really hold. This will be a rather crude approximation to keep things simple."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"9da3b55a-9447-43e0-0bee-b80859a65a51"},"outputs":[],"source":"def plot_score_distribution(bags, num_tries=60, size=10000, fit=norm, extremal_fit=gumbel_r):\n    scores = np.zeros(size)\n    for i, bag in enumerate(bags):\n        current_bag_sampler, _ = create_bag_utility_sampler(bag)\n        scores += current_bag_sampler(size)\n    score_mean, score_std = np.mean(scores), np.std(scores)\n    print(\"Scores: mean = {:0.2f} | std = {:0.2f}\".format(score_mean, score_std))\n    sns.distplot(scores, fit=fit, kde=False)\n    \n    plot_extreme_value_distribution(scores, num_tries)\n    plt.title(\"Score distribution / submission distribution with {} tries\".format(num_tries))\n    plt.show()\n\ndef plot_extreme_value_distribution(scores, num_tries, size=10000):\n    samples = np.max(np.random.choice(scores, size=(size, num_tries)), axis=1)\n    sns.distplot(samples, fit=gumbel_r, kde=False)\n    expected_score = np.mean(samples)\n    plt.axvline(expected_score, color='r')\n    print(\"Expected score after {} trials: {:0.2f}\".format(num_tries, expected_score))\n\nplot_score_distribution(example_bags)"},{"cell_type":"markdown","metadata":{"_cell_guid":"d6590d72-baeb-cc79-470a-26373ca43c8d"},"source":"For our small set of 4 example bags, we not only get a crappy score but also the distribution does not look normal. But - interestingly - we can see that our score after 60 submissions would be a lot better than the score distribution's expected value.\n\nWhat would happen if we could create 1000 bag\ns of a certain kind? Also, let's verify that we can compute the score distribution from the means and variances of the bag utility distributions."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"8a7dec14-5176-22d3-14e0-a5fd1cc73a95"},"outputs":[],"source":"def n_bags(bag, n):\n    return [bag for i in range(n)]\n\ndef estimate_mean_std_utility(bag, n=1, size=10000):\n    current_bag_sampler, _ = create_bag_utility_sampler(bag)\n    sample = current_bag_sampler(size)\n    return np.mean(sample) * n, math.sqrt(math.pow(np.std(sample), 2) * n)\n\nbag = {\"gloves\": 12, \"book\": 12}\nn = 1000\n\nest_bag_utility_mean, est_bag_utility_std = estimate_mean_std_utility(bag, n=n)\nprint(\"Computed statistics for {} bags: mean = {:0.2f}, std = {:0.2f}\".format(n, est_bag_utility_mean, est_bag_utility_std))\nplot_score_distribution(n_bags(bag, n))"},{"cell_type":"markdown","metadata":{"_cell_guid":"99613669-8f42-258c-e460-1a40756a9b87"},"source":"Nice.\nComputing and sampling delivers roughly the same (normal) score distribution.\n\n## Summary\n\nMain observations from the experiments above are:\n\n- To get a weight distribution for individual bags, it is sufficient to add means and variances of individual gift weight distributions.\n- Getting the utility distribution for bags requires sampling as there is no immediately available formula (that I would know of).\n- Getting the overall score distribution can be done by adding means and variance of individual bag utility distributions.\n- Getting an estimate of the expected final score after _n_ submissions can be done by sampling from score distribution (or probably also algebraically, but that does not matter too much imho)"},{"cell_type":"markdown","metadata":{"_cell_guid":"7f627e64-e98c-0af5-fc49-597cbc023b10"},"source":"# 2. Computing good bags\nSo let's start by finding bags with good utility distributions in three steps:\n\n1. Get individual gift weight distributions: easy - just sample as there are so few\n2. Get candiate bags: compute many weight distributions for many bags and eliminate most of them heuristically\n3. Getting bag utility distributions: easy but inefficient - we use sampling for the candidate bags"},{"cell_type":"markdown","metadata":{"_cell_guid":"11eb188d-a63e-4143-e233-d6722e02ad08"},"source":"## 2.1 Get individual gift weight distributions\nAs in 1., we just sample mean and variance for gift weights:"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"75cf8c77-b365-e15e-ed5b-62de51832978"},"outputs":[],"source":"np.random.seed(17) # reset seed to make things reproducibe even if you fiddle around in the part above\n\ndef get_gift_weight_distributions(gifts, size=10000):\n    def get_gift_weight_dsitribution(gift):\n        sampler = samplers[gift]\n        sample = sampler(size)\n        return np.mean(sample), np.var(sample)\n    \n    distributions = np.zeros((len(gifts), 2))\n    for i, gift in enumerate(gifts):\n        distributions[i, :] = get_gift_weight_dsitribution(gift)\n    return distributions\n\ngifts = sorted(samplers.keys())\nprint(\"Canonical gift order: {}\\n\".format(gifts))\ngift_weight_distributions = get_gift_weight_distributions(gifts)\nprint(pd.DataFrame(data=gift_weight_distributions, index=gifts, columns=[\"mean\", \"std\"]))"},{"cell_type":"markdown","metadata":{"_cell_guid":"5db874ad-2528-47e6-d473-95eae9abb92f"},"source":"## 2.2 Get bag weight distributions\nNext, we must create a list of bags we want to consider.\nWe do so by creating a numpy array with each row corresponding to a bag, each column to a gift type and the matrix itself to gift quantities per bag.\nAs it does not make much sense to increase expected weights beyond 50, we limit quantities correspondingly."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"787065c3-8a42-547c-af1e-4d563b356ba5"},"outputs":[],"source":"def get_mixed_item_bags_max_quantities(upper_limit=None):\n    max_quantities = np.ceil(50 / gift_weight_distributions[:, 0])\n    if upper_limit is not None:\n        max_quantities[max_quantities > upper_limit] = upper_limit\n    return max_quantities\n\nmixed_item_max_quantities = get_mixed_item_bags_max_quantities()\nprint(\"maximum quantities:\\n{}\".format(np.dstack((np.array(gifts), mixed_item_max_quantities)).squeeze()))\nprint(\"number of different bags: {}\".format(np.prod(mixed_item_max_quantities)))"},{"cell_type":"markdown","metadata":{"_cell_guid":"1a46effa-ecbe-f62a-ac0f-a57928873e8b"},"source":"These are really a lot of combinations.\nLet's limit the gift quantities by 11 to not explode to much here.\nWe can then manually add some bags consisting of lots of low weight gifts (balls, books and gloves) combined with very few gifts of other kinds."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"957b108b-2751-abee-fdbf-a219b2e4cbda"},"outputs":[],"source":"mixed_item_max_quantities = get_mixed_item_bags_max_quantities(11)\nprint(\"maximum quantities:\\n{}\".format(np.dstack((np.array(gifts), mixed_item_max_quantities)).squeeze()))\nprint(\"number of different bags: {}\".format(np.prod(mixed_item_max_quantities)))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"bb80a6c0-1a3b-f98a-df47-1bee50b69044"},"outputs":[],"source":"def create_candidate_bags(max_quantities):\n    gift_counts = []\n    for max_quantity in max_quantities:\n        gift_counts.append(np.arange(max_quantity))\n    return cartesian(gift_counts)\n\nmixed_item_candiadte_bags = create_candidate_bags(mixed_item_max_quantities)\nprint(\"Created candiadate bags: {}\".format(mixed_item_candiadte_bags.shape))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a32133c1-9718-9c04-4bb4-11abcfcaff5f"},"outputs":[],"source":"print(mixed_item_candiadte_bags[10000])"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b237b991-59a6-770d-ebaa-30b7e3e8b751"},"outputs":[],"source":"def get_bag_weight_distributions(candidate_bags):\n    return np.dot(candidate_bags, gift_weight_distributions)\n\ndef filter_by_mean(bags, distributions, min_mean=20, max_pdf_std=0.6745, max_bag_size=50):\n    min_mask = mean_of(distributions) > min_mean\n    distributions = distributions[min_mask]    \n    bags = bags[min_mask]\n    \n    max_mask = mean_of(distributions) < max_bag_size    \n    distributions = distributions[max_mask]\n    bags = bags[max_mask]\n    \n    max_mask = mean_of(distributions) + max_pdf_std * std_of(distributions) < max_bag_size\n    # in more the 50% of the cases the bag should be below max_bag_size\n    distributions = distributions[max_mask]\n    bags = bags[max_mask]\n    \n    return bags, distributions\n\ndef mean_of(distributions):\n    return distributions[:,0]\n\ndef std_of(distributions):\n    return np.sqrt(distributions[:,1])\n\nmixed_item_bag_weight_distributions = get_bag_weight_distributions(mixed_item_candiadte_bags)\nmixed_item_candiadte_bags, mixed_item_bag_weight_distributions = \\\n    filter_by_mean(mixed_item_candiadte_bags, mixed_item_bag_weight_distributions)\nprint(\"Candidate bags left: {}\".format(mixed_item_candiadte_bags.shape))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f0b6ccee-4879-b7b4-0706-3e0bd3c84800"},"outputs":[],"source":"print(mixed_item_candiadte_bags[10000])\nprint(mixed_item_bag_weight_distributions[10000])"},{"cell_type":"markdown","metadata":{"_cell_guid":"ff6f43fd-a72d-bd81-1f3b-a31beff9ed1b"},"source":"To these candidate bags, we now add bags that consist primarily of one of the 3 low weight items (ball / book / gloves), combined with few of the others:"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"8df211c4-ecc4-ee50-7b99-c7bcafa82daa"},"outputs":[],"source":"def get_low_weight_item_candidate_bags():\n    bags = []\n    distributions = []\n    for gift in [\"ball\", \"book\", \"gloves\"]:\n        max_quantities = get_low_weight_item_bags_max_quantities_for(gift)\n        candiadte_bags = create_candidate_bags(max_quantities)\n        bags.append(candiadte_bags)\n        cadidate_bag_weight_distributions = get_bag_weight_distributions(candiadte_bags)\n        distributions.append(cadidate_bag_weight_distributions)\n    return np.vstack(bags), np.vstack(distributions)\n        \n\ndef get_low_weight_item_bags_max_quantities_for(gift):\n    max_quantities = np.ceil(50 / gift_weight_distributions[:, 0])\n    gift_index = gifts.index(gift)\n    for i in range(len(max_quantities)):\n        if not i == gift_index:\n            max_quantities[i] = 5\n    print(\"Gift {}: number of different bags: {}\".format(gift, np.prod(max_quantities)))\n    return max_quantities\n\nlow_weight_item_candidate_bags, low_weight_item_bag_weight_distributions = \\\n    filter_by_mean(*get_low_weight_item_candidate_bags())\nprint(\"Total number of canidate bags: {}\".format(low_weight_item_candidate_bags.shape))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b65b09b9-b859-3211-a05c-4f11a8bb9cda"},"outputs":[],"source":"def drop_duplicate(candidate_bags, distributions):\n    df = pd.DataFrame(data=np.hstack((candidate_bags, distributions)), columns=gifts + [\"mean\", \"var\"])\n    df.drop_duplicates(subset=gifts, inplace=True)\n    return df[gifts].values, df[[\"mean\", \"var\"]].values\n\ncandidate_bags = np.vstack([mixed_item_candiadte_bags, low_weight_item_candidate_bags])\nbag_weight_distributions = np.vstack([mixed_item_bag_weight_distributions, low_weight_item_bag_weight_distributions])\nprint(\"Combined candiadte bags: {}\".format(candidate_bags.shape))\ncandidate_bags, bag_weight_distributions = drop_duplicate(candidate_bags, bag_weight_distributions)\nprint(\"Final candidate bags without duplicates: {}\".format(candidate_bags.shape))"},{"cell_type":"markdown","metadata":{"_cell_guid":"44e96a82-d059-6849-837a-19fca0796784"},"source":"## 2.3 Get bag utility distributions\nNow we use our sampler to estimate the utility distributions.\nThis will take a few minutes..."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"9cf75121-cc64-c2d8-385b-6b071ad3d0e6"},"outputs":[],"source":"def get_bag_utility_distributions(candidate_bags):\n    distributions = []\n    size = len(candidate_bags)\n    for i, candidate_bag in enumerate(candidate_bags):\n        if i % 7000 == 0:\n            sys.stdout.write(\"{:.4f}\\r\".format(float(i) / float(size)))\n        distributions.append(get_bag_utility_distribution(candidate_bag))\n    print(\"\")\n    return np.vstack(distributions)\n\ndef get_bag_utility_distribution(candidate_bag):\n    bag = { gifts[i]: int(candidate_bag[i]) for i in range(len(gifts)) if candidate_bag[i] > 0 }\n    sampler, name = create_bag_utility_sampler(bag)\n    sample = sampler(10000)\n    return np.mean(sample), np.std(sample)\n\nbag_utility_distributions = get_bag_utility_distributions(candidate_bags)\nprint(bag_utility_distributions.shape)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"ec739df3-2b5b-b0c3-e0c7-9fb8451a883e"},"outputs":[],"source":"print(candidate_bags[10000])\nprint(bag_utility_distributions[10000])"},{"cell_type":"markdown","metadata":{"_cell_guid":"13299c76-6b42-c62a-1362-0892a1bb18ca"},"source":"# 3. Combining good bags to bag sets\nNow we have a list of individual bags with their gift quantities and estimated utility distributions.\nWe can use these to create bag sets with their distributions.\nContsrained optimization is needed obey maximum gift number restrictions.\nAs long as we optimize the mean of the score distribution we can use linear programming.\nIf what we care for is the maximum order statistic though and we also like variance, it's not that easy.\nHence, we proceed in two steps:\n- build a bag packer with LP that optimizes expected scores only but allows for a minimum variance constraint\n- grid search over minimum variances and compare resulting score distributions by sampling expected maximum values"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"dd948c95-9d08-195c-5e75-99b834a36aaa"},"outputs":[],"source":"num_gifts_available = {\n    \"horse\": 1000,\n    \"ball\": 1100,\n    \"bike\": 500,\n    \"train\": 1000,\n    \"book\": 1200,\n    \"doll\": 1000,\n    \"blocks\": 1000,\n    \"gloves\": 200,\n    \"coal\": 166\n}"},{"cell_type":"markdown","metadata":{"_cell_guid":"2e4f75ea-cd62-2f5d-5712-e695d3923af7"},"source":"## 3.1 Packing bags with linear programming\nLinear programming is not integer programming, so after solving the problem, we need to fiddle around a bit with the solution to make things work. The main strategy is:\n- floor all bag quantities to get integer values\n- increase maximum bags the algorithm packs until we get >= 1000 bags after flooring\n- if that gives us > 1000 bags, just throw random bags away until we reach 1000\n\nSometimes when I run this, the solver thinks the problem is infeasible. The code also accounts for that and assigns 0 score in these cases. "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"8b5d895f-6e47-a97f-1ac2-0fa5954f6120"},"outputs":[],"source":"def pack_linprog(bags, distributions, min_variance, max_bags=1000):\n    # objective: c.T * x -> min\n    c = - distributions[:,0] # optimize sum of expected bag utilities\n    \n    # constraint: A_ub * x <= b_ub\n    A_ub = bags.T # don't use more gifts than available\n    b_ub = np.array([num_gifts_available[gift] for gift in gifts])\n    \n    A_ub = np.vstack([A_ub, np.ones(A_ub.shape[1])]) # pack at most max_bags gifts\n    b_ub = np.hstack([b_ub, [max_bags]])\n    \n    if min_variance is not None:\n        A_ub = np.vstack([A_ub, -distributions[:,1]]) # require minimum variance\n        b_ub = np.hstack([b_ub, [-min_variance]])\n    \n    result = linprog(c, A_ub=A_ub, b_ub=b_ub)\n    if result[\"success\"] == False:\n        return [], True\n    else:\n        return result[\"x\"].astype('int64'), False\n\n\ndef pack_bags(bags, distributions, min_variance=None):\n    max_bags = 1000\n    bag_quantities, infeasible = pack_linprog(bags, distributions, min_variance=min_variance)\n    while np.sum(bag_quantities) < 1000:\n        max_bags += 1\n        bag_quantities, infeasible = pack_linprog(bags, distributions, min_variance=min_variance, max_bags=max_bags)\n        if max_bags > 1015:\n            print(\"WARNING: not getting 1000 bags\")\n            break\n        if infeasible:\n            continue\n    \n    if infeasible:\n        print(\"infeasible\")\n        return [], [], []\n    \n    chosen_bag_idx = np.where(bag_quantities)[0]\n    chosen_bags = bags[chosen_bag_idx]\n    chosen_distributions = distributions[chosen_bag_idx]\n    chosen_quantities = bag_quantities[chosen_bag_idx]\n    \n    while np.sum(chosen_quantities) > 1000:\n        idx = np.random.randint(len(chosen_quantities))\n        chosen_quantities[idx] = max (chosen_quantities[idx]-1, 0)\n    \n    score_distribution = np.dot(chosen_quantities, chosen_distributions)\n    print(\"{} bags - score distribution: mean = {:.2f} | var = {:.2f}\"\n          .format(np.sum(chosen_quantities), score_distribution[0], score_distribution[1]))\n    \n    return chosen_bags, chosen_distributions, chosen_quantities\n\npacked_bags, packed_distributions, packed_quantities \\\n    = pack_bags(candidate_bags, bag_utility_distributions, min_variance=None)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"3a4597d2-f594-1b95-e43a-ecd712bcfbd4"},"outputs":[],"source":"print(packed_bags)\nprint(packed_distributions)\nprint(packed_quantities)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"cd3bcc5b-5ae8-6217-3087-3d5c307a6a83"},"outputs":[],"source":"def evaluate_variances():\n    results = {}\n    for i, min_variance in enumerate(np.linspace(8000, 14000, num=10)):\n        bags, distributions, quantities = pack_bags(candidate_bags, bag_utility_distributions, min_variance=min_variance)\n        results[min_variance] = np.dot(quantities, distributions)\n    return results\n\nscores_for_min_variance = evaluate_variances()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"932db0f5-3171-f3b6-d8dc-fac12e7844c4"},"outputs":[],"source":"def plot_expected_scores_after(num_submissions, score_distributions):\n    min_variances = sorted(score_distributions.keys())\n    expected_scores = []\n    for min_variance in min_variances:\n        score_distribution = score_distributions[min_variance]\n        if not type(score_distribution) == np.ndarray:\n            expected_scores.append(0)\n            continue\n        samples = np.random.normal(score_distribution[0], score_distribution[1], (10000, num_submissions))\n        expected_scores.append(np.mean(np.max(samples, axis=1)))\n        print(\"{} - {}\".format(min_variance, np.mean(np.max(samples, axis=1))))\n    df = pd.DataFrame(data=np.hstack((np.expand_dims(min_variances, 1), np.expand_dims(expected_scores, 1))), columns=[\"min_variance\", \"expected_score\"])\n    sns.regplot(data=df, x=\"min_variance\", y=\"expected_score\", order=2)\n    plt.title(\"Expected score by minimum variance constraint\")\n\nplot_expected_scores_after(60, scores_for_min_variance)"},{"cell_type":"markdown","metadata":{"_cell_guid":"3180db27-18fd-d8b0-b3c6-61edcc8811e3"},"source":"How sad: we cannot gain anything by increasing variance. It seems as if it's just best to optimize for expected score. Let's create submissions then."},{"cell_type":"markdown","metadata":{"_cell_guid":"4de5a1b8-48c0-a71d-ea9d-c63f9e7d230a"},"source":"# 4. Make submissions\nWe now create a lot of submissions with randomly permuted gifts and hope there is a good one among them."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"6aced5de-745d-a99e-d72e-d17a3fa97ac0"},"outputs":[],"source":"def create_submissions(bags, quantities, num_submissions=60):\n    def create_stock(n):\n        stock = { gift: list(map(lambda id: \"{}_{}\".format(gift, id) ,np.arange(num_gifts_available[gift]))) for gift in gifts }\n        return shuffle(stock, n)\n    \n    def shuffle(stock, seed):\n        np.random.seed(seed)\n        for gift in stock.keys():\n            np.random.shuffle(stock[gift])\n        return stock\n    \n    def generate_submission(n):\n        stock = create_stock(n)\n        with open(\"submission_{}.csv\".format(n), 'w+') as submission_file:\n            submission_file.write('Gifts\\n')\n            for i in range(len(bags)):\n                for quantity in range(quantities[i]):\n                    current_gifts = bags[i]\n                    for gift_idx, gift_quantity in enumerate(current_gifts[:len(gifts)]):\n                        gift_name = gifts[gift_idx]\n                        for j in range(int(gift_quantity)):\n                            submission_file.write(\"{} \".format(stock[gift_name].pop()))\n                    submission_file.write(\"\\n\")\n    \n    for n in range(num_submissions):\n        generate_submission(n)\n        \n\ncreate_submissions(packed_bags, packed_quantities)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"6ffa15eb-ee56-c791-a761-b2b2123e2e82"},"outputs":[],"source":"pd.read_csv('submission_20.csv').head()"}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"}},"nbformat":4,"nbformat_minor":0}