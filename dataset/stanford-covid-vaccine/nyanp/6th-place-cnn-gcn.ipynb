{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import copy\nimport os\nimport time\nimport traceback\nfrom contextlib import contextmanager\nfrom typing import List\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\n\n!pip install nyaggle\nfrom nyaggle.validation import StratifiedGroupKFold\nfrom sklearn.cluster import AgglomerativeClustering\nfrom torch.utils.data import Dataset, DataLoader\nfrom tqdm import tqdm","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"TOKEN2INT = {x: i for i, x in enumerate('().ACGUBEHIMSX')}\nPRED_COLS_SCORED = ['reactivity', 'deg_Mg_pH10', 'deg_Mg_50C']\nPRED_COLS = ['reactivity', 'deg_Mg_pH10', 'deg_Mg_50C', 'deg_pH10', 'deg_50C']\n\nDATA_DIR = \"../input/stanford-covid-vaccine/\"\nREPLACE_DATA_PATH = \"../input/eternafold/eternafold_mfe.csv\"\nPRIMARY_BPPS_DIR = \"../input/eternafold/bpps/\"\nSECONDARY_BPPS_DIR = \"../input/bpps-by-viennat70/\"\nNFOLDS = 7\nBATCH_SIZE = 64\nTRAIN_EPOCHS = 140","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nprint(device)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class SELayer(nn.Module):\n    def __init__(self, channel, reduction=16):\n        super(SELayer, self).__init__()\n        self.avg_pool = nn.AdaptiveAvgPool1d(1)\n        self.fc = nn.Sequential(\n            nn.Linear(channel, channel // reduction, bias=False),\n            nn.ReLU(inplace=True),\n            nn.Linear(channel // reduction, channel, bias=False),\n            nn.Sigmoid()\n        )\n\n    def forward(self, x):\n        b, c, _ = x.size()\n        y = self.avg_pool(x).view(b, c)\n        y = self.fc(y).view(b, c, 1)\n        return x * y.expand_as(x)\n\n\nclass Conv(nn.Module):\n    def __init__(self, d_in, d_out, kernel_size, dropout=0.1):\n        super().__init__()\n        self.conv = nn.Conv1d(d_in, d_out, kernel_size=kernel_size, padding=kernel_size // 2)\n        self.bn = nn.BatchNorm1d(d_out)\n        self.relu = nn.ReLU()\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, src):\n        return self.dropout(self.relu(self.bn(self.conv(src))))\n\n\nclass ResidualGraphAttention(nn.Module):\n    def __init__(self, d_model, kernel_size, dropout):\n        super().__init__()\n        self.conv1 = Conv(d_model, d_model, kernel_size=kernel_size, dropout=dropout)\n        self.conv2 = Conv(d_model, d_model, kernel_size=kernel_size, dropout=dropout)\n        self.relu = nn.ReLU()\n\n    def forward(self, src, attn):\n        h = self.conv2(self.conv1(torch.bmm(src, attn)))\n        return self.relu(src + h)\n    \n\nclass SEResidual(nn.Module):\n    def __init__(self, d_model, kernel_size, dropout):\n        super().__init__()\n        self.conv1 = Conv(d_model, d_model, kernel_size=kernel_size, dropout=dropout)\n        self.conv2 = Conv(d_model, d_model, kernel_size=kernel_size, dropout=dropout)\n        self.relu = nn.ReLU()\n        self.se = SELayer(d_model)\n\n    def forward(self, src):\n        h = self.conv2(self.conv1(src))\n        return self.se(self.relu(src + h))\n\n\nclass FusedEmbedding(nn.Module):\n    def __init__(self, n_emb):\n        super().__init__()\n        self.emb = nn.Embedding(len(TOKEN2INT), n_emb)\n        self.n_emb = n_emb\n\n    def forward(self, src, se):\n        # src: [batch, seq, feature]\n        # se: [batch, seq]\n        embed = self.emb(src)\n        embed = embed.reshape((-1, embed.shape[1], embed.shape[2] * embed.shape[3]))\n        embed = torch.cat((embed, se), 2)\n\n        return embed\n\n    @property\n    def d_out(self):\n        d_emb = 3 * self.n_emb\n        d_feat = 2 * 5  # max, sum, 2nd, 3rd, nb_count\n        return d_emb + d_feat\n    \n\nclass ConvModel(nn.Module):\n    def __init__(self, d_emb=50, d_model=256, dropout=0.6, dropout_res=0.4, dropout_emb=0.0,\n                 kernel_size_conv=7, kernel_size_gc=7):\n        super().__init__()\n\n        self.embedding = FusedEmbedding(d_emb)\n        self.dropout = nn.Dropout(dropout_emb)\n        self.conv = Conv(self.embedding.d_out, d_model, kernel_size=3, dropout=dropout)\n\n        self.block1 = SEResidual(d_model, kernel_size=kernel_size_conv, dropout=dropout_res)\n        self.block2 = SEResidual(d_model, kernel_size=kernel_size_conv, dropout=dropout_res)\n        self.block3 = SEResidual(d_model, kernel_size=kernel_size_conv, dropout=dropout_res)\n        self.block4 = SEResidual(d_model, kernel_size=kernel_size_conv, dropout=dropout_res)\n        self.block5 = SEResidual(d_model, kernel_size=kernel_size_conv, dropout=dropout_res)\n\n        self.attn1 = ResidualGraphAttention(d_model, kernel_size=kernel_size_gc, dropout=dropout_res)\n        self.attn2 = ResidualGraphAttention(d_model, kernel_size=kernel_size_gc, dropout=dropout_res)\n        self.attn3 = ResidualGraphAttention(d_model, kernel_size=kernel_size_gc, dropout=dropout_res)\n        self.attn4 = ResidualGraphAttention(d_model, kernel_size=kernel_size_gc, dropout=dropout_res)\n\n        self.linear = nn.Linear(d_model, len(PRED_COLS))\n\n    def forward(self, \n                src: torch.Tensor, \n                features: torch.Tensor, \n                bpps: torch.Tensor, \n                adj: torch.Tensor):\n        # src: [batch, seq, 3]\n        # features: [batch, seq, 10]\n        # bpps: [batch, seq, seq, 2]\n        # adj: [batch, seq, seq]\n        \n        x = self.dropout(self.embedding(src, features))\n        x = x.permute([0, 2, 1])  # [batch, d-emb, seq]\n        \n        x = self.conv(x)\n        x = self.block1(x)\n        x = self.attn1(x, adj)\n        x = self.block2(x)\n        x = self.attn2(x, adj)\n        x = self.block3(x)\n        x = self.attn3(x, bpps[:, :, :, 0])\n        x = self.attn4(x, bpps[:, :, :, 1])\n        x = self.block4(x)\n        x = self.block5(x)\n\n        x = x.permute([0, 2, 1])  # [batch, seq, features]\n        out = self.linear(x)\n\n        out = torch.clamp(out, -0.5, 1e8)\n\n        return out\n    \n\n\nclass WRMSELoss(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n    def forward(self, yhat, y, sample_weight=None):\n        l = (yhat - y) ** 2\n\n        if sample_weight is not None:\n            l = l * sample_weight.unsqueeze(dim=1)\n\n        return torch.sqrt(torch.mean(l))\n\n\nclass ColWiseLoss(nn.Module):\n    def __init__(self, base_loss):\n        super().__init__()\n        self.base_loss = base_loss\n        self.len_scored = 68\n\n    def forward(self, yhat, y, column_weight=None, sample_weight=None):\n        score = 0\n        for i in range(len(PRED_COLS)):\n            s = self.base_loss(\n                yhat[:, :self.len_scored, i], \n                y[:, :self.len_scored, i], \n                sample_weight\n            ) / len(PRED_COLS)\n            \n            if column_weight is not None:\n                s *= column_weight[i]\n                \n            score += s\n        return score\n\n\nclass MCRMSELoss(ColWiseLoss):\n    def __init__(self):\n        super().__init__(WRMSELoss())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"@contextmanager\ndef timer(name):\n    s = time.time()\n    yield\n    print(f\"{name}: {time.time() - s:.3f}sec\")\n\n\ndef pandas_list_to_array(df: pd.DataFrame) -> np.ndarray:\n    return np.transpose(\n        np.array(\n            df.values\n                .tolist()\n        ),\n        (0, 2, 1)\n    )\n\n\ndef preprocess_inputs(df: pd.DataFrame) -> np.ndarray:\n    return pandas_list_to_array(\n        df[['sequence', 'structure', 'predicted_loop_type']]\n            .applymap(lambda seq: [TOKEN2INT[x] for x in seq])\n    )\n\n\ndef build_adj_matrix(src_df: pd.DataFrame, normalize: bool = True) -> np.ndarray:\n    n = len(src_df['structure'].iloc[0])\n    mat = np.zeros((len(src_df), n, n))\n    start_token_indices = []\n\n    for r, structure in tqdm(enumerate(src_df['structure'])):\n        for i, token in enumerate(structure):\n            if token == \"(\":\n                start_token_indices.append(i)\n            elif token == \")\":\n                j = start_token_indices.pop()\n                mat[r, i, j] = 1\n                mat[r, j, i] = 1\n\n    assert len(start_token_indices) == 0\n\n    if normalize:\n        mat = mat / (mat.sum(axis=2, keepdims=True) + 1e-8)\n\n    return mat\n\n\ndef replace_data(train_df: pd.DataFrame, test_df: pd.DataFrame, replace_data_dir: str):\n    print(f\"using data from {replace_data_dir}\")\n\n    aux = pd.read_csv(replace_data_dir)\n    del train_df['structure']\n    del train_df['predicted_loop_type']\n    del test_df['structure']\n    del test_df['predicted_loop_type']\n    train_df = pd.merge(train_df, aux, on='id', how='left')\n    test_df = pd.merge(test_df, aux, on='id', how='left')\n    assert len(train_df) == 2400\n    assert len(test_df) == 3634\n    assert train_df['structure'].isnull().sum() == 0\n    assert train_df['predicted_loop_type'].isnull().sum() == 0\n    assert test_df['structure'].isnull().sum() == 0\n    assert test_df['predicted_loop_type'].isnull().sum() == 0\n    return train_df, test_df\n\n\ndef load_bpps(df: pd.DataFrame, data_dir: str) -> np.ndarray:\n    return np.array([np.load(f'{data_dir}bpps/{did}.npy') for did in df.id])\n\n\ndef make_bpps_features(bpps_list: List[np.ndarray]) -> np.ndarray:\n    ar = []\n\n    for b in bpps_list:\n        ar.append(b.sum(axis=2))\n\n        # max, 2ndmax, 3rdmax\n        bpps_sorted = np.sort(b, axis=2)[:, :, ::-1]\n        ar.append(bpps_sorted[:, :, 0])\n        ar.append(bpps_sorted[:, :, 1])\n        ar.append(bpps_sorted[:, :, 2])\n\n        # number of nonzero\n        bpps_nb_mean = 0.077522  # mean of bpps_nb across all training data\n        bpps_nb_std = 0.08914  # std of bpps_nb across all training data\n        nb = (b > 0).sum(axis=2)\n        nb = (nb - bpps_nb_mean) / bpps_nb_std\n        ar.append(nb)\n\n    return np.transpose(np.array(ar), (1, 2, 0))\n\n\ndef make_dataset(device, x: np.ndarray, y: np.ndarray,\n                 bpps_primary: np.ndarray,\n                 bpps_secondary: np.ndarray,\n                 adj_matrix: np.ndarray,\n                 prediction_mask: np.ndarray,\n                 signal_to_noise=None):\n    x = copy.deepcopy(x)\n    if y is not None:\n        y = copy.deepcopy(y)\n    bpps_primary = copy.deepcopy(bpps_primary)\n    bpps_secondary = copy.deepcopy(bpps_secondary)\n    bpps = np.concatenate([\n        bpps_primary[:, :, :, np.newaxis],\n        bpps_secondary[:, :, :, np.newaxis]\n    ], axis=-1)\n\n    adj_matrix = copy.deepcopy(adj_matrix)\n    prediction_mask = copy.deepcopy(prediction_mask)\n\n    if y is not None:\n        y = np.clip(y, -0.5, 10)\n        mask = np.abs(y).max(axis=(1, 2)) < 10\n    else:\n        mask = [True] * len(x)\n\n    tensors = [\n        torch.LongTensor(x[mask]),\n        torch.Tensor(make_bpps_features([bpps_primary[mask], bpps_secondary[mask]])),\n        torch.Tensor(bpps[mask]),\n        torch.Tensor(adj_matrix[mask]),\n        torch.Tensor(prediction_mask[mask])\n    ]\n\n    if y is not None:\n        tensors.append(torch.Tensor(y[mask]))\n        \n        sample_weight = np.clip(np.log(signal_to_noise[mask] + 1.1) / 2, 0, 100)\n        tensors.append(torch.Tensor(sample_weight))\n\n    return torch.utils.data.TensorDataset(*[t.to(device) for t in tensors])\n\n\ndef make_dataset_from_df(device, df: pd.DataFrame, bpps_dir: str, secondary_bpps_dir: str):\n    assert df['seq_scored'].nunique() == 1\n\n    inputs = preprocess_inputs(df)\n    bpps = load_bpps(df, bpps_dir)\n    adj = build_adj_matrix(df)\n    secondary_bpps = load_bpps(df, secondary_bpps_dir)\n\n    mask = np.zeros((len(df), len(df['sequence'].iloc[0]), len(PRED_COLS)))\n    mask[:, :df['seq_scored'].iloc[0], :] = 1\n\n    return make_dataset(device, inputs, None, bpps, secondary_bpps, adj, mask)\n\n\ndef dist(s1: str, s2: str) -> int:\n    return sum([c1 != c2 for c1, c2 in zip(s1, s2)])\n\n\ndef get_distance_matrix(s: pd.Series) -> np.ndarray:\n    mat = np.zeros((len(s), len(s)))\n\n    for i in tqdm(range(len(s))):\n        for j in range(i + 1, len(s)):\n            mat[i, j] = mat[j, i] = dist(s[i], s[j])\n    return mat\n\n\ndef batch_predict(model: nn.Module, loader: DataLoader) -> np.ndarray:\n    y_preds = np.zeros((len(loader.dataset), loader.dataset[0][0].shape[0], len(PRED_COLS)))\n\n    for i, (x_batch, x_se, x_bpps, x_adj, y_mask) in enumerate(loader):\n        y_pred = model(x_batch, x_se, x_bpps, x_adj).detach() * y_mask\n        y_preds[i * loader.batch_size:(i + 1) * loader.batch_size, :, :] = y_pred.cpu().numpy()\n        \n    return y_preds\n\n\ndef calc_loss(y_true: np.ndarray, y_pred: np.ndarray):\n    err_w_valid = [1 if s in PRED_COLS_SCORED else 0 for s in PRED_COLS]\n    raw = MCRMSELoss()(torch.Tensor(y_pred), torch.Tensor(y_true), err_w_valid).item()\n    \n    return raw * len(PRED_COLS) / len(PRED_COLS_SCORED)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_model(model, train_loader, valid_loader, y_valid,\n                train_epochs, train_loss, verbose=True,\n                model_path='model'):\n    params = sum([p.numel() for p in model.parameters() if p.requires_grad])\n    print(f'number of params: {params}')\n\n    err_w_train_1 = [1 if s in PRED_COLS_SCORED else 1.0 for s in PRED_COLS]\n    err_w_train_2 = [1 if s in PRED_COLS_SCORED else 0.01 for s in PRED_COLS]\n\n    criterion_train = train_loss\n    optimizer = torch.optim.Adam(model.parameters())\n\n    losses = []\n    val_losses = []\n    y_preds_best = None\n\n    for epoch in range(train_epochs):\n        start_time = time.time()\n\n        model.train()\n        avg_loss = 0.\n\n        for x_batch, x_se, x_bpps, x_adj, y_mask, y_batch, sample_weight in tqdm(train_loader, disable=True):\n            y_pred = model(x_batch, x_se, x_bpps, x_adj) * y_mask\n            \n            # use 5 columns for the first 30 epoch\n            w = err_w_train_1 if epoch < 30 else err_w_train_2\n            \n            loss = criterion_train(y_pred, y_batch, w, sample_weight)\n            optimizer.zero_grad()\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n            optimizer.step()\n            avg_loss += loss.item() / len(train_loader)\n\n        model.eval()\n        y_preds = batch_predict(model, valid_loader)\n        mcloss = calc_loss(y_valid, y_preds)\n        val_losses.append(mcloss)\n        \n        s = f\"{epoch:03d}: trn:{avg_loss:.4f}, clean={mcloss:.4f}, {time.time() - start_time:.2f}s\"\n\n        losses.append(avg_loss)\n\n        if np.min(val_losses) == mcloss:\n            y_preds_best = y_preds\n            torch.save(model.state_dict(), model_path)\n\n        if (isinstance(verbose, bool) and verbose) or (verbose > 0 and (epoch % verbose == 0)):\n            print(s)\n\n    print(f'min val_loss: {np.min(val_losses):.4f} at {np.argmin(val_losses) + 1} epoch')\n\n    # recover best weight\n    model.load_state_dict(torch.load(model_path))\n\n    if not verbose:\n        return np.min(val_losses)\n\n    fig, ax = plt.subplots(1, 3, figsize=(24, 8))\n\n    ax[0].plot(np.arange(1, len(losses) + 1), losses)\n    ax[1].plot(np.arange(1, len(val_losses) + 1), val_losses)\n\n    for i, p in enumerate(PRED_COLS):\n        ax[2].scatter(y_valid[:, :, i].flatten(), y_preds_best[:, :, i].flatten(), alpha=0.5)\n\n    ax[0].legend(['train'])\n    ax[1].legend(['valid(clean)'])\n    ax[2].legend(PRED_COLS)\n    ax[2].set_xlabel('y_true')\n    ax[2].set_ylabel('y_predicted')\n    ax[0].set_xlabel('epoch')\n    ax[0].set_ylabel('loss')\n    ax[1].set_xlabel('epoch')\n    ax[1].set_ylabel('loss')\n    ax[2].set_xlabel('y_true(clean)')\n    ax[2].set_ylabel('y_predicted(clean)')\n    plt.show()\n\n    return np.min(val_losses)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Load data"},{"metadata":{"trusted":true},"cell_type":"code","source":"with timer(\"load data\"):\n    train_df = pd.read_json(DATA_DIR + 'train.json', lines=True)\n    test_df = pd.read_json(DATA_DIR + 'test.json', lines=True)\n    sample_df = pd.read_csv(DATA_DIR + 'sample_submission.csv')\n\n    train_df, test_df = replace_data(train_df, test_df, REPLACE_DATA_PATH)\n\nwith timer(\"clustering\"):\n    # use clustering based on edit distance\n    seq_dist = get_distance_matrix(train_df['sequence'])\n    clf = AgglomerativeClustering(n_clusters=None, \n                                  distance_threshold=20, \n                                  affinity='precomputed',\n                                  linkage='average')\n    group_index = clf.fit_predict(seq_dist)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"with timer(\"preprocess\"):\n    public_df = test_df.query(\"seq_length != 130\")\n    private_df = test_df.query(\"seq_length == 130\")\n\n    x = preprocess_inputs(train_df)\n    y = pandas_list_to_array(train_df[PRED_COLS])\n\n    label_mask = np.ones_like(y)\n    pad = np.zeros((y.shape[0], x.shape[1] - y.shape[1], y.shape[2]))\n    y = np.concatenate((y, pad), axis=1)\n    label_mask = np.concatenate((label_mask, pad), axis=1)\n\n    assert x.shape[1] == y.shape[1]\n\n    train_adj = build_adj_matrix(train_df)\n    primary_bpps = load_bpps(train_df, PRIMARY_BPPS_DIR)    \n    secondary_bpps = load_bpps(train_df, SECONDARY_BPPS_DIR)\n\n    public_data = make_dataset_from_df(device, public_df, PRIMARY_BPPS_DIR, SECONDARY_BPPS_DIR)\n    private_data = make_dataset_from_df(device, private_df, PRIMARY_BPPS_DIR, SECONDARY_BPPS_DIR)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Training"},{"metadata":{"trusted":true},"cell_type":"code","source":"kf = StratifiedGroupKFold(NFOLDS, random_state=42, shuffle=True)\n\npred_oof = np.zeros_like(y)\npred_public = np.zeros((len(public_data), len(public_df['sequence'].iloc[0]), len(PRED_COLS)))\npred_private = np.zeros((len(private_data), len(private_df['sequence'].iloc[0]), len(PRED_COLS)))\n\npublic_loader = DataLoader(public_data, batch_size=128, shuffle=False)\nprivate_loader = DataLoader(private_data, batch_size=128, shuffle=False)\n\nclean_idx = [i for i in range(len(train_df)) if train_df['SN_filter'].iloc[i]]\nsn_mask = train_df['SN_filter'] == 1\n\ncriterion_train = MCRMSELoss()\nmodel_path = \"model_fold{}\"\n\nlosses = []\n\nfor i, (train_index, valid_index) in enumerate(kf.split(x, train_df['SN_filter'], groups=group_index)):\n    print(f'fold {i}')\n    model = ConvModel().to(device)\n    s = time.time()\n\n    train_data = make_dataset(device, x[train_index], y[train_index],\n                              primary_bpps[train_index], secondary_bpps[train_index],\n                              train_adj[train_index],\n                              label_mask[train_index],\n                              signal_to_noise=train_df['signal_to_noise'][train_index].values)\n\n    valid_index_c = [v for v in valid_index if v in clean_idx]\n    valid_data_clean = make_dataset(device, x[valid_index_c], None,\n                                    primary_bpps[valid_index_c],\n                                    secondary_bpps[valid_index_c],\n                                    train_adj[valid_index_c],\n                                    label_mask[valid_index_c])\n    valid_data_noisy = make_dataset(device, x[valid_index], None,\n                                  primary_bpps[valid_index],\n                                  secondary_bpps[valid_index],\n                                  train_adj[valid_index],\n                                  label_mask[valid_index])\n\n    train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n    valid_loader_clean = DataLoader(valid_data_clean, batch_size=128, shuffle=False)\n    valid_loader_noisy = DataLoader(valid_data_noisy, batch_size=128, shuffle=False)\n\n    loss = train_model(model, \n                       train_loader, \n                       valid_loader_clean, \n                       y[valid_index_c], \n                       TRAIN_EPOCHS, \n                       criterion_train,\n                       verbose=5, \n                       model_path=model_path.format(i))\n\n    losses.append(loss)\n\n    # predict\n    pred_oof[valid_index] = batch_predict(model, valid_loader_noisy)\n    pred_public += batch_predict(model, public_loader) / NFOLDS\n    pred_private += batch_predict(model, private_loader) / NFOLDS\n\n    print(f'elapsed: {time.time() - s:.1f}sec')\n\noof_score = calc_loss(y, pred_oof)\nprint(f'oof(all): {oof_score: .4f}')\n\noof_score = calc_loss(y[sn_mask], pred_oof[sn_mask])\nprint(f'oof(clean): {oof_score: .4f}')\n\n# make submission and oof\npreds_ls = []\n\nfor df, preds in [(public_df, pred_public), (private_df, pred_private)]:\n    for i, uid in enumerate(df.id):\n        single_pred = preds[i]\n        single_df = pd.DataFrame(single_pred, columns=PRED_COLS)\n        single_df['id_seqpos'] = [f'{uid}_{x}' for x in range(single_df.shape[0])]\n        preds_ls.append(single_df)\n\npreds_df = pd.concat(preds_ls)\npreds_df.head()\n\nsubmission = sample_df[['id_seqpos']].merge(preds_df, on=['id_seqpos'])\nsubmission.to_csv('submission.csv', index=False)\n\nnp.save('oof', pred_oof)\nnp.save('public', pred_public)\nnp.save('private', pred_private)\n\nprint(losses)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}