{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Fourth place solution to the \"OpenVaccine: COVID-19 mRNA Vaccine Degradation Prediction\"\n\nThis notebook provides the code that can be used to generate our fourth place solution. Our final submission consisted of a blend of multiple variations of this notebook. The variations were only in the 2 RNN layers of the classification part of our network (LSTM+LSTM, LSTM+GRU, GRU+LSTM, GRU+GRU).\n\nThis notebook is mainly based on [this great notebook](https://www.kaggle.com/mrkmakr/covid-ae-pretrain-gnn-attn-cnn) from [@mrmakr](https://www.kaggle.com/mrkmakr).\n\nDifferent datasets are used:\n* **Augmented data:** the output from [@its7171](https://www.kaggle.com/its7171) [his notebook](https://www.kaggle.com/its7171/how-to-generate-augmentation-data). This dataset contains different possible structures for the same RNA sequence.\n* **BPPs generated with ARNIE:** we generated BPPs with different packages using ARNIE. We [provide a notebook te demonstrate this](https://www.kaggle.com/group16/generating-bpps-with-arnie). We also re-use the structures to calculate a certainty feature (fraction of packages that agree on a certain predicted structure).\n* **Predicted loop types with CapR** [CapR](https://github.com/fukunagatsu/CapR) generates probabilities for each of the loop types."},{"metadata":{},"cell_type":"markdown","source":"# 1 Processing the data\n\nWe will create the following inputs for the train set, public test set and private test set:\n* A `NxLxLxE` adjacency matrix, with `N` the number of samples, `L` the sequence length and `E` the number of adjacency (edge) features.\n* A `NxLxV` node feature matrix, with `N` number of samples, `L` sequence length and `V` number of node features (features per base of the RNA sequence)\n* A `NxT` target matrix, with `T`=5 our 5 targets. We also train on the non-scored targets as that improved overall performance."},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport gc\nimport os\nimport pickle\nfrom tqdm.notebook import tqdm\n\n%matplotlib inline\nimport matplotlib.pyplot as plt\n\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 1.1 Loading the competition data"},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_json(\"../input/stanford-covid-vaccine/train.json\",lines=True)\ntest  = pd.read_json(\"../input/stanford-covid-vaccine/test.json\",lines=True)\nsub = pd.read_csv(\"../input/stanford-covid-vaccine/sample_submission.csv\")\n\ntest_pub = test[test[\"seq_length\"] == 107]\ntest_pri = test[test[\"seq_length\"] == 130]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 1.2 Appending the augmentation data"},{"metadata":{"trusted":true},"cell_type":"code","source":"aug_df = pd.read_csv('../input/covid19-mrna-augmentation-data-and-features/aug_data1.csv')\naug_df = aug_df.drop_duplicates(subset=['id', 'structure'])\n\n\ndef aug_data(df):\n    target_df = df.copy()\n    new_df = aug_df[aug_df['id'].isin(target_df['id'])]\n                         \n    del target_df['structure']\n    del target_df['predicted_loop_type']\n    new_df = new_df.merge(target_df, on=['id','sequence'], how='left')\n\n    df['cnt'] = df['id'].map(new_df[['id','cnt']].set_index('id').to_dict()['cnt'])\n    df['log_gamma'] = 100\n    df['score'] = 1.0\n    df = df.append(new_df[df.columns])\n    return df\n\ntrain = aug_data(train).reset_index(drop=True)\ntest_pub = aug_data(test_pub).reset_index(drop=True)\ntest_pri = aug_data(test_pri).reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 1.3 Creating adjacency matrices\n\nWe extract 5 edge features:\n* 3 different BPPs: the provided ones, generated with rnasoft (ARNIE) and generated with contrafold (ARNIE)\n* whether there is a base pairing indicated in the structure ( `(` and `)` )\n* the distances (manhattan) between bases, normalized by sequence length"},{"metadata":{"trusted":true},"cell_type":"code","source":"# The provided BPPs\nAs = []\nfor id in tqdm(train[\"id\"]):\n    a = np.load(f\"../input/stanford-covid-vaccine/bpps/{id}.npy\").astype(np.float16)\n    As.append(a)\nAs = np.array(As)\n\nAs_pub = []\nfor id in tqdm(test_pub[\"id\"]):\n    a = np.load(f\"../input/stanford-covid-vaccine/bpps/{id}.npy\").astype(np.float16)\n    As_pub.append(a)\nAs_pub = np.array(As_pub)\n\nAs_pri = []\nfor id in tqdm(test_pri[\"id\"]):\n    a = np.load(f\"../input/stanford-covid-vaccine/bpps/{id}.npy\").astype(np.float16)\n    As_pri.append(a)\nAs_pri = np.array(As_pri)\n\n# BPPs generated with ARNIE (package='contrafold2')\nAs_cf = []\nfor id in tqdm(train[\"id\"]):\n    a = np.load(f\"../input/covid19-mrna-augmentation-data-and-features/bpps_contrafold_gamma1/bpps_contrafold_gamma1/bpps_contrafold_gamma1/{id}.npy\").astype(np.float16)\n    As_cf.append(a)\nAs_cf = np.array(As_cf)\n\nAs_pub_cf = []\nfor id in tqdm(test_pub[\"id\"]):\n    a = np.load(f\"../input/covid19-mrna-augmentation-data-and-features/bpps_contrafold_gamma1/bpps_contrafold_gamma1/bpps_contrafold_gamma1/{id}.npy\").astype(np.float16)\n    As_pub_cf.append(a)\nAs_pub_cf = np.array(As_pub_cf)\n\nAs_pri_cf = []\nfor id in tqdm(test_pri[\"id\"]):\n    a = np.load(f\"../input/covid19-mrna-augmentation-data-and-features/bpps_contrafold_gamma1/bpps_contrafold_gamma1/bpps_contrafold_gamma1/{id}.npy\").astype(np.float16)\n    As_pri_cf.append(a)\nAs_pri_cf = np.array(As_pri_cf)\n\n# BPPs generated with ARNIE (package='rnasoft')\nAs_rs = []\nfor id in tqdm(train[\"id\"]):\n    a = np.load(f\"../input/covid19-mrna-augmentation-data-and-features/bpps_rnasoft_gamma1/bpps_rnasoft_gamma1/bpps_rnasoft_gamma1/{id}.npy\").astype(np.float16)\n    As_rs.append(a)\nAs_rs = np.array(As_rs)\n\nAs_pub_rs = []\nfor id in tqdm(test_pub[\"id\"]):\n    a = np.load(f\"../input/covid19-mrna-augmentation-data-and-features/bpps_rnasoft_gamma1/bpps_rnasoft_gamma1/bpps_rnasoft_gamma1/{id}.npy\").astype(np.float16)\n    As_pub_rs.append(a)\nAs_pub_rs = np.array(As_pub_rs)\n\nAs_pri_rs = []\nfor id in tqdm(test_pri[\"id\"]):\n    a = np.load(f\"../input/covid19-mrna-augmentation-data-and-features/bpps_rnasoft_gamma1/bpps_rnasoft_gamma1/bpps_rnasoft_gamma1/{id}.npy\").astype(np.float16)\n    As_pri_rs.append(a)\nAs_pri_rs = np.array(As_pri_rs)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"def get_structure_adj(train):\n    Ss = []\n    for i in tqdm(range(len(train))):\n        seq_length = train[\"seq_length\"].iloc[i]\n        structure = train[\"structure\"].iloc[i]\n        sequence = train[\"sequence\"].iloc[i]\n\n        cue = []\n        a_structures = {\n            (\"A\", \"U\") : np.zeros([seq_length, seq_length]),\n            (\"C\", \"G\") : np.zeros([seq_length, seq_length]),\n            (\"U\", \"G\") : np.zeros([seq_length, seq_length]),\n            (\"U\", \"A\") : np.zeros([seq_length, seq_length]),\n            (\"G\", \"C\") : np.zeros([seq_length, seq_length]),\n            (\"G\", \"U\") : np.zeros([seq_length, seq_length]),\n        }\n        a_structure = np.zeros([seq_length, seq_length])\n        for i in range(seq_length):\n            if structure[i] == \"(\":\n                cue.append(i)\n            elif structure[i] == \")\":\n                start = cue.pop()\n                a_structures[(sequence[start], sequence[i])][start, i] = 1\n                a_structures[(sequence[i], sequence[start])][i, start] = 1\n        \n        a_strc = np.stack([a for a in a_structures.values()], axis = 2)\n        a_strc = np.sum(a_strc, axis = 2, keepdims = True)\n        Ss.append(a_strc)\n    \n    Ss = np.array(Ss)\n    return Ss.astype(np.float16)\n\nSs = get_structure_adj(train)\nSs_pub = get_structure_adj(test_pub)\nSs_pri = get_structure_adj(test_pri)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_distance_matrix(As):\n    idx = np.arange(As.shape[1])\n    Ds = []\n    for i in range(len(idx)):\n        d = np.abs(idx[i] - idx)\n        Ds.append(d)\n\n    Ds = np.array(Ds) + 1\n    Ds = Ds / Ds.shape[1]\n    Ds = Ds[None, :,:]\n    Ds = np.repeat(Ds, len(As), axis = 0)\n    \n    Dss = []\n    for i in [1]:\n        Dss.append(Ds ** i)\n    Ds = np.stack(Dss, axis = 3)\n    return Ds.astype(np.float16)\n\nDs = get_distance_matrix(As)\nDs_pub = get_distance_matrix(As_pub)\nDs_pri = get_distance_matrix(As_pri)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"As = np.concatenate([As[:,:,:,None], As_cf[:,:,:,None], As_rs[:,:,:,None], Ss, Ds], axis = 3).astype(np.float16)\ndel Ss, Ds, As_cf, As_rs\n\nAs_pub = np.concatenate([As_pub[:,:,:,None], As_pub_cf[:,:,:,None], As_pub_rs[:,:,:,None], Ss_pub, Ds_pub], axis = 3).astype(np.float16)\ndel Ss_pub, Ds_pub, As_pub_cf, As_pub_rs\n\nAs_pri = np.concatenate([As_pri[:,:,:,None], As_pri_cf[:,:,:,None], As_pri_rs[:,:,:,None], Ss_pri, Ds_pri], axis = 3).astype(np.float16)\ndel Ss_pri, Ds_pri, As_pri_cf, As_pri_rs\n\nprint(As.shape, As_pub.shape, As_pri.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f, ax = plt.subplots(1, 5, figsize=(15, 3))\nfor i in range(As.shape[-1]):\n    ax[i].imshow(As[0, :, :, i].astype(np.float32))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 1.4 Creating node features\n\nWe extract 25 features per base in our sequence (107x25 features for train & public test, 130x25 features for private test):\n* One-hot-encoded base (AGCU) (4 features)\n* One-hot-encoded loop type (provided) (7 features)\n* one-hot-encoded positional feature (index % 3) (3 features)\n* certainty: fraction of packages (3 different ones) that predict `(`, `)` or `.`  (3 features)\n* CapR loop type probabilities (6 features)\n* BPP sum and number of zeroese (2 features)"},{"metadata":{"trusted":true},"cell_type":"code","source":"arnie_train_features = pickle.load(open('../input/covid19-mrna-augmentation-data-and-features/arnie/arnie/train_features.p', 'rb'))\narnie_test_features = pickle.load(open('../input/covid19-mrna-augmentation-data-and-features/arnie/arnie/test_features.p', 'rb'))\ncapr_train_features = pickle.load(open('../input/covid19-mrna-augmentation-data-and-features/CapR_train.pickle', 'rb'))\ncapr_test_features = pickle.load(open('../input/covid19-mrna-augmentation-data-and-features/CapR_test.pickle', 'rb'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## sequence\ndef return_ohe(n, i):\n    tmp = [0] * n\n    tmp[i] = 1\n    return tmp\n\ndef get_input(train):\n    mapping = {}\n    vocab = [\"A\", \"G\", \"C\", \"U\"]\n    for i, s in enumerate(vocab):\n        mapping[s] = return_ohe(len(vocab), i)\n    X_node = np.stack(train[\"sequence\"].apply(lambda x : list(map(lambda y : mapping[y], list(x)))))\n\n    mapping = {}\n    vocab = [\"S\", \"M\", \"I\", \"B\", \"H\", \"E\", \"X\"]\n    for i, s in enumerate(vocab):\n        mapping[s] = return_ohe(len(vocab), i)\n    X_loop = np.stack(train[\"predicted_loop_type\"].apply(lambda x : list(map(lambda y : mapping[y], list(x)))))\n    \n    X_pos1 = np.stack(train[\"sequence\"].apply(lambda x : np.arange(len(x)) % 3 == 0))[:,:,np.newaxis]\n    X_pos2 = np.stack(train[\"sequence\"].apply(lambda x : np.arange(len(x)) % 3 == 1))[:,:,np.newaxis]\n    X_pos3 = np.stack(train[\"sequence\"].apply(lambda x : np.arange(len(x)) % 3 == 2))[:,:,np.newaxis]\n\n    X_certainty = np.zeros((X_pos1.shape[:2] + (3, )))\n    X_loop_capr = np.zeros((X_pos1.shape[:2] + (6, )))\n    X_bpp_sum = np.zeros((X_pos1.shape[:2] + (1, )))\n    X_bpp_nb_zeroes = np.zeros((X_pos1.shape[:2] + (1, )))\n\n    for k, mol_id in enumerate(train['id'].values):\n        if mol_id in arnie_train_features:\n            arr1 = np.array(list(arnie_train_features[mol_id]['mfe']['vienna_2']))\n            arr2 = np.array(list(arnie_train_features[mol_id]['mfe']['contrafold_2']))\n            arr3 = np.array(list(arnie_train_features[mol_id]['mfe']['rnastructure']))\n        else:\n            arr1 = np.array(list(arnie_test_features[mol_id]['mfe']['vienna_2']))\n            arr2 = np.array(list(arnie_test_features[mol_id]['mfe']['contrafold_2']))\n            arr3 = np.array(list(arnie_test_features[mol_id]['mfe']['rnastructure']))\n\n        X_certainty[k, :, 0] = ((arr1 == '(') + (arr2 == '(') + (arr3 == '(')).astype(int) / 3\n        X_certainty[k, :, 1] = ((arr1 == ')') + (arr2 == ')') + (arr3 == ')')).astype(int) / 3\n        X_certainty[k, :, 2] = ((arr1 == '.') + (arr2 == '.') + (arr3 == '.')).astype(int) / 3\n        \n        if mol_id in capr_train_features:\n            X_loop_capr[k] = np.array([list(map(float, i.split()[1:])) for i in capr_train_features[mol_id].split('\\n')[1:-2]]).T\n        else:\n            X_loop_capr[k] = np.array([list(map(float, i.split()[1:])) for i in capr_test_features[mol_id].split('\\n')[1:-2]]).T\n\n        bpp = np.load(f\"../input/stanford-covid-vaccine/bpps/{mol_id}.npy\").astype(np.float16)\n        X_bpp_sum[k] = bpp.sum(axis=1)[:, None]\n        X_bpp_nb_zeroes[k] = ((bpp == 0).sum(axis=1) / X_pos1.shape[1])[:, None]\n\n\n    X_node = np.concatenate([X_node, X_loop, X_pos1, X_pos2, X_pos3, X_certainty, X_loop_capr, X_bpp_sum, X_bpp_nb_zeroes], axis = 2)\n    return X_node\n\nX_node = get_input(train).astype(np.float16)\nX_node_pub = get_input(test_pub).astype(np.float16)\nX_node_pri = get_input(test_pri).astype(np.float16)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(X_node.shape, X_node_pub.shape, X_node_pri.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del arnie_train_features, arnie_test_features, capr_train_features, capr_test_features","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 1.5 Extracting the targets\n\n5 targets, including the 2 non-scored ones (deg_pH10 and deg_50C) as they increase the performance of the 3 others when trained jointly on all 5."},{"metadata":{"trusted":true},"cell_type":"code","source":"targets = ['reactivity', 'deg_Mg_pH10', 'deg_pH10', 'deg_Mg_50C', 'deg_50C']\n\ny_train = []\nseq_len = train[\"seq_length\"].iloc[0]\nseq_len_target = train[\"seq_scored\"].iloc[0]\nignore = -10000\nignore_length = seq_len - seq_len_target\nfor target in targets:\n    y = np.vstack(train[target])\n    dummy = np.zeros([y.shape[0], ignore_length]) + ignore\n    y = np.hstack([y, dummy])\n    y_train.append(y)\ny = np.stack(y_train, axis = 2)\nprint(y.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"N_NODE_FEATURES = X_node.shape[2]\nN_EDGE_FEATURES = As.shape[3]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2. Creating the model & training"},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras import layers as L\nimport tensorflow_addons as tfa\nfrom tensorflow.keras import backend as K\nfrom sklearn.utils import shuffle\nfrom sklearn.model_selection import GroupKFold","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ae_epochs = 25\nae_epochs_each = 5\nae_batch_size = 32\n\nepochs_list = [30, 10, 3, 3, 5, 5]\nbatch_size_list = [8, 16, 32, 64, 128, 256]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def mcrmse(t, p, seq_len_target = seq_len_target):\n    score = np.mean(np.sqrt(tf.keras.losses.mean_squared_error(t, p))[:, :seq_len_target])\n    return score\n\ndef rmse(y_actual, y_pred):\n    mse = tf.keras.losses.mean_squared_error(y_actual, y_pred)\n    return K.sqrt(mse)\n\ndef mcrmse_loss(y_actual, y_pred, num_scored=5):\n    y_actual = y_actual[:, :68, :]\n    y_pred = y_pred[:, :68, :]\n    score = 0\n    for i in range(num_scored):\n        score += rmse(y_actual[:, :, i], y_pred[:, :, i]) / num_scored\n    return score\n\ndef attention(x_inner, x_outer, n_factor, dropout):\n    x_Q =  L.Conv1D(n_factor, 1, activation='linear', \n                  kernel_initializer='glorot_uniform',\n                  bias_initializer='glorot_uniform',\n                 )(x_inner)\n    x_K =  L.Conv1D(n_factor, 1, activation='linear', \n                  kernel_initializer='glorot_uniform',\n                  bias_initializer='glorot_uniform',\n                 )(x_outer)\n    x_V =  L.Conv1D(n_factor, 1, activation='linear', \n                  kernel_initializer='glorot_uniform',\n                  bias_initializer='glorot_uniform',\n                 )(x_outer)\n    x_KT = L.Permute((2, 1))(x_K)\n    res = L.Lambda(lambda c: K.batch_dot(c[0], c[1]) / np.sqrt(n_factor))([x_Q, x_KT])\n    att = L.Lambda(lambda c: K.softmax(c, axis=-1))(res)\n    att = L.Lambda(lambda c: K.batch_dot(c[0], c[1]))([att, x_V])\n    return att\n\ndef multi_head_attention(x, y, n_factor, n_head, dropout):\n    if n_head == 1:\n        att = attention(x, y, n_factor, dropout)\n    else:\n        n_factor_head = n_factor // n_head\n        heads = [attention(x, y, n_factor_head, dropout) for i in range(n_head)]\n        att = L.Concatenate()(heads)\n        att = L.Dense(n_factor, \n                      kernel_initializer='glorot_uniform',\n                      bias_initializer='glorot_uniform',\n                     )(att)\n    x = L.Add()([x, att])\n    x = L.LayerNormalization()(x)\n    if dropout > 0:\n        x = L.Dropout(dropout)(x)\n    return x\n\ndef res(x, unit, kernel = 3, rate = 0.1):\n    h = L.Conv1D(unit, kernel, 1, padding = \"same\", activation = None)(x)\n    h = L.LayerNormalization()(h)\n    h = L.LeakyReLU()(h)\n    h = L.Dropout(rate)(h)\n    return L.Add()([x, h])\n\ndef forward(x, unit, kernel = 3, rate = 0.1):\n#     h = L.Dense(unit, None)(x)\n    h = L.Conv1D(unit, kernel, 1, padding = \"same\", activation = None)(x)\n    h = L.LayerNormalization()(h)\n    h = L.Dropout(rate)(h)\n#         h = tf.keras.activations.swish(h)\n    h = L.LeakyReLU()(h)\n    h = res(h, unit, kernel, rate)\n    return h\n\ndef adj_attn(x, adj, unit, n = 2, rate = 0.1):\n    x_a = x\n    x_as = []\n    for i in range(n):\n        x_a = forward(x_a, unit)\n        x_a = tf.matmul(adj, x_a)\n        x_as.append(x_a)\n    if n == 1:\n        x_a = x_as[0]\n    else:\n        x_a = L.Concatenate()(x_as)\n    x_a = forward(x_a, unit)\n    return x_a\n\n\ndef get_base(config):\n    node = tf.keras.Input(shape = (None, N_NODE_FEATURES), name = \"node\")\n    adj = tf.keras.Input(shape = (None, None, N_EDGE_FEATURES), name = \"adj\")\n    \n    adj_learned = L.Dense(1, \"relu\")(adj)\n    adj_all = L.Concatenate(axis = 3)([adj, adj_learned])\n        \n    xs = []\n    xs.append(node)\n    x1 = forward(node, 128, kernel = 3, rate = 0.0)\n    x2 = forward(x1, 64, kernel = 6, rate = 0.0)\n    x3 = forward(x2, 32, kernel = 15, rate = 0.0)\n    x4 = forward(x3, 16, kernel = 30, rate = 0.0)\n    x = L.Concatenate()([x1, x2, x3, x4])\n    \n    for unit in [128, 64]:\n        x_as = []\n        for i in range(adj_all.shape[3]):\n            x_a = adj_attn(x, adj_all[:, :, :, i], unit, rate = 0.0)\n            x_as.append(x_a)\n        x_c = forward(x, unit, kernel = 30)\n        \n        x = L.Concatenate()(x_as + [x_c])\n        x = forward(x, unit)\n        x = multi_head_attention(x, x, unit, 4, 0.0)\n        xs.append(x)\n        \n    x = L.Concatenate()(xs)\n\n    model = tf.keras.Model(inputs = [node, adj], outputs = [x])\n    return model\n\n\ndef get_ae_model(base, config):\n    node = tf.keras.Input(shape = (None, N_NODE_FEATURES), name = \"node\")\n    adj = tf.keras.Input(shape = (None, None, N_EDGE_FEATURES), name = \"adj\")\n\n    x = base([L.SpatialDropout1D(0.3)(node), adj])\n    x = forward(x, 64, rate = 0.3)\n    p = L.Dense(N_NODE_FEATURES, \"sigmoid\")(x)\n    \n    loss = tf.reduce_mean(tf.math.abs(node - p))\n    model = tf.keras.Model(inputs = [node, adj], outputs = [loss])\n    \n    opt = get_optimizer()\n    model.compile(optimizer = opt, loss = lambda t, y : y)\n    return model\n\ndef gru_layer(hidden_dim, dropout):\n    return L.Bidirectional(L.GRU(hidden_dim, dropout=dropout, return_sequences=True, kernel_initializer = 'orthogonal'))\ndef lstm_layer(hidden_dim, dropout):\n    return L.Bidirectional(L.LSTM(hidden_dim, dropout=dropout, return_sequences=True, kernel_initializer = 'orthogonal'))\n\ndef get_model(base, config):\n    node = tf.keras.Input(shape = (None, N_NODE_FEATURES), name = \"node\")\n    adj = tf.keras.Input(shape = (None, None, N_EDGE_FEATURES), name = \"adj\")\n    \n    x = base([node, adj])\n    x = lstm_layer(256, 0.5)(x)\n    x = lstm_layer(256, 0.5)(x)\n    x = L.Dense(5, None)(x)\n\n    model = tf.keras.Model(inputs = [node, adj], outputs = [x])\n    \n    opt = get_optimizer()\n    model.compile(optimizer = opt, loss = mcrmse_loss)\n    return model\n\ndef get_optimizer():\n    adam = tf.optimizers.Adam()\n    return adam","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"config = {}\n\nif ae_epochs > 0:\n    base = get_base(config)\n    ae_model = get_ae_model(base, config)\n    ## TODO : simultaneous train\n    for i in range(ae_epochs//ae_epochs_each):\n        print(f\"------ {i} ------\")\n        print(\"--- train ---\")\n        X_node_shuff, As_shuff = shuffle(X_node, As)\n        ae_model.fit([X_node_shuff, As_shuff], [np.zeros((len(X_node)))],\n                  epochs = ae_epochs_each,\n                  batch_size = ae_batch_size)\n        \n        print(\"--- public ---\")\n        X_node_pub_shuff, As_pub_shuff = shuffle(X_node_pub, As_pub)\n        ae_model.fit([X_node_pub_shuff, As_pub_shuff], [np.zeros((len(X_node_pub)))],\n                  epochs = ae_epochs_each,\n                  batch_size = ae_batch_size)\n        \n        print(\"--- private ---\")\n        X_node_pri_shuff, As_pri_shuff = shuffle(X_node_pri, As_pri)\n        ae_model.fit([X_node_pri_shuff, As_pri_shuff], [np.zeros((len(X_node_pri)))],\n                  epochs = ae_epochs_each,\n                  batch_size = ae_batch_size)\n        gc.collect()\n    print(\"****** save ae model ******\")\n    base.save_weights(\"base_ae_lstm_lstm\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del ae_model\n\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.save('X_node_pub.npy', X_node_pub)\nnp.save('X_node_pri.npy', X_node_pri)\nnp.save('As_pub.npy', As_pub)\nnp.save('As_pri.npy', As_pri)\n\ndel X_node_pub, X_node_pri, As_pub, As_pri, X_node_shuff, As_shuff, X_node_pub_shuff, As_pub_shuff, X_node_pri_shuff, As_pri_shuff","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"kfold = GroupKFold(5)\n\nconfig = {}\nscores = []\npreds = np.zeros([len(X_node), X_node.shape[1], 5])\nX_node, As, y, weights, groups_shuffled, SN_filter_mask = shuffle(X_node, As, y, train.signal_to_noise.values, train['id'], (train['SN_filter'] == 1).values)\ndel train\nfor i, (tr_idx, va_idx) in enumerate(kfold.split(X_node, As, groups_shuffled)):\n    print(f\"------ fold {i} start -----\")\n    X_node_tr = X_node[tr_idx]\n    X_node_va = X_node[va_idx]\n    As_tr = As[tr_idx]\n    As_va = As[va_idx]  \n    y_tr = y[tr_idx]\n    y_va = y[va_idx]\n    w_trn = np.log(weights[tr_idx]+1.105)/2\n    \n    base = get_base(config)\n    if ae_epochs > 0:\n        print(\"****** load ae model ******\")\n        base.load_weights(\"base_ae_lstm_lstm\")\n    model = get_model(base, config)\n    for epochs, batch_size in zip(epochs_list, batch_size_list):\n        print(f\"epochs : {epochs}, batch_size : {batch_size}\")\n        model.fit([X_node_tr, As_tr], [y_tr],\n                  validation_data=([X_node_va[SN_filter_mask[va_idx]], As_va[SN_filter_mask[va_idx]]], [y_va[SN_filter_mask[va_idx]]]),\n                  epochs = epochs,\n                  sample_weight=w_trn/2,\n                  batch_size = batch_size, validation_freq = 3)\n        \n    model.save_weights(f\"model{i}_lstm_lstm\")\n    p = model.predict([X_node_va, As_va])\n    for col in range(5):\n        print(targets[col], ((p[:, :68, col][SN_filter_mask[va_idx]] - y_va[:,:68,col][SN_filter_mask[va_idx]]) ** 2).mean() ** .5)\n    scores.append(np.mean(mcrmse_loss(y_va, p)))\n    \n    print(f\"fold {i}: mcrmse {scores[-1]}\")\n    preds[va_idx] = p\n    \n    gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds_df = []\nfor p_ix, _id in zip(range(preds.shape[0]), groups_shuffled):\n    for i in range(68):\n        preds_df.append([f'{_id}_{i}', preds[p_ix, i, 0], preds[p_ix, i, 1], \n                         preds[p_ix, i, 3], y[p_ix, i, 0], y[p_ix, i, 1],\n                         y[p_ix, i, 3], SN_filter_mask[p_ix]])\npreds_df = pd.DataFrame(preds_df, columns=['id_seqpos', 'reactivity', 'deg_Mg_pH10', \n                                           'deg_Mg_50C', 'reactivity_gt', 'deg_Mg_pH10_gt', \n                                           'deg_Mg_50C_gt', 'SN_filter'])\npreds_df = preds_df.groupby('id_seqpos').mean()\nrmses = []\nmses = []\nfor col in ['reactivity', 'deg_Mg_pH10', 'deg_Mg_50C']:\n    _rmse = ((preds_df[preds_df['SN_filter'] == 1][col] - preds_df[preds_df['SN_filter'] == 1][col+'_gt']) ** 2).mean() ** .5\n    _mse = ((preds_df[preds_df['SN_filter'] == 1][col] - preds_df[preds_df['SN_filter'] == 1][col+'_gt']) ** 2).mean()\n    rmses.append(_rmse)\n    mses.append(_mse)\n    print(col, _rmse, _mse)\nprint(np.mean(rmses), np.mean(mses))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3. Inference"},{"metadata":{"trusted":true},"cell_type":"code","source":"del X_node, As","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_node_pub = np.load('X_node_pub.npy')\nX_node_pri = np.load('X_node_pri.npy')\nAs_pub = np.load('As_pub.npy')\nAs_pri = np.load('As_pri.npy')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"p_pub = 0\np_pri = 0\nfor i in range(5):\n    config = {}\n    base = get_base(config)\n    if ae_epochs > 0:\n        print(\"****** load ae model ******\")\n        base.load_weights(\"base_ae_lstm_lstm\")\n    model = get_model(base, config)\n    model.load_weights(f\"model{i}_lstm_lstm\")\n    p_pub += model.predict([X_node_pub, As_pub]) / 5\n    p_pri += model.predict([X_node_pri, As_pri]) / 5\n\nfor i, target in enumerate(targets):\n    test_pub[target] = [list(p_pub[k, :, i]) for k in range(p_pub.shape[0])]\n    test_pri[target] = [list(p_pri[k, :, i]) for k in range(p_pri.shape[0])]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds_ls = []\nfor df, preds in [(test_pub, p_pub), (test_pri, p_pri)]:\n    for i, uid in enumerate(df.id):\n        single_pred = preds[i]\n\n        single_df = pd.DataFrame(single_pred, columns=targets)\n        single_df['id_seqpos'] = [f'{uid}_{x}' for x in range(single_df.shape[0])]\n\n        preds_ls.append(single_df)\n\npreds_df = pd.concat(preds_ls)\npreds_df = preds_df.groupby('id_seqpos').mean()\npreds_df.to_csv(\"submission_lstm_lstm.csv\")\npreds_df.head()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}