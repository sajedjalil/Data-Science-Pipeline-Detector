{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Our template single model solution\n\nWe would like to share our template solution\n\nYou can find more details and explanation here: \nhttps://www.kaggle.com/c/stanford-covid-vaccine/discussion/189320"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\npath = '../input/stanford-covid-vaccine/'\nbbps_folder = f'{path}bpps/'\ntrain = pd.read_json(f'{path}train.json', lines=True)\ntest = pd.read_json(f'{path}test.json', lines=True)\nsample_sub = pd.read_csv(f'{path}sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Folds"},{"metadata":{},"cell_type":"markdown","source":"### stratify"},{"metadata":{"trusted":true},"cell_type":"code","source":"targets = np.stack([train[col].to_list() for col in ['reactivity', 'deg_Mg_pH10', 'deg_Mg_50C','deg_pH10', 'deg_50C']], axis=-1)\nfilter_1 = pd.Series((targets > -0.5).all(axis=(1,2))).astype(int)\nfilter_2 = (train['signal_to_noise'] > 1).astype(int)\nreactivity_bins = pd.qcut(train['reactivity'].map(np.mean), 5, labels=False)\ndeg_Mg_pH10_bins = pd.qcut(train['deg_Mg_pH10'].map(np.mean), 5, labels=False)\ndeg_Mg_50C_bins = pd.qcut(train['deg_Mg_50C'].map(np.mean), 5, labels=False)\n\nstratify = pd.concat([filter_1, filter_2, reactivity_bins, deg_Mg_50C_bins, deg_Mg_pH10_bins], axis=1)\nstratify = stratify.astype(str).apply(''.join, axis=1)\nstratify.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 5 folds"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import StratifiedKFold\n\nskf= StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\nfolds = list(skf.split(train, stratify))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"from torch.utils.data import Dataset, DataLoader, TensorDataset\nfrom torch import LongTensor, FloatTensor\nimport torch\nimport numpy as np\nimport pandas as pd\nfrom copy import deepcopy\n\n\ndef get_structure_matrix(structure):\n    n = len(structure)\n    structure_matrix = np.zeros((n,n), int)\n    for i in range(n):\n        if structure[i] == '(':\n            count = 0\n            for j in range(i, n):\n                if structure[j] == '(':\n                    count += 1\n                elif structure[j] == ')':\n                    count -= 1\n                if count==0:\n                    structure_matrix[i,j]=1\n                    structure_matrix[j,i]=1\n                    break\n    return structure_matrix.astype(int)\n\ndef get_stracture_bpps(structure_matrix, bbps):\n    stracture_bbps = np.zeros(structure_matrix.shape[:-1])\n    stracture_bbps[structure_matrix.any(axis=0)] = bbps[structure_matrix.astype(bool)]\n    return stracture_bbps\n\ndef get_distance2pair(structure_matrix):\n    self_idx = np.arange(structure_matrix.shape[0])\n    idx_pair = structure_matrix.argmax(axis=1)\n    idx_pair[~structure_matrix.any(axis=1)] = self_idx[~structure_matrix.any(axis=1)]\n    distance2pair = (idx_pair - self_idx)/(structure_matrix.shape[0]-1)\n    return distance2pair\n\n\ndef proc_df(df):\n    df = deepcopy(df)\n    df['bbps'] = df['id'].apply(lambda x: np.load(f'{bbps_folder}{x}.npy'))\n    df['bbps_sum'] = df['bbps'].apply(lambda x: x.sum(axis=1))\n    df['bbps_max'] = df['bbps'].apply(lambda x: x.max(axis=1))\n    df['structure_matrix'] = df['structure'].apply(get_structure_matrix)\n    df['structure_bpps'] = df.apply(lambda x: get_stracture_bpps(x['structure_matrix'], x['bbps']), axis=1)\n    df['distance2pair'] = df['structure_matrix'].apply(get_distance2pair)\n    return df\n\n\n\nSTRUCTURE_CODE = {'(': 0, '.': 1, ')': 2}\nPREDICTED_LOOP_TYPE_CODE = {'H': 0, 'E': 1, 'B': 2, 'M': 3, 'X': 4, 'S': 5, 'I': 6}\nSEQUANCE_CODE = {'U': 0, 'C': 1, 'A': 2, 'G': 3}\n\nERROR_COLUMNS = ['reactivity_error', 'deg_error_Mg_pH10', 'deg_error_Mg_50C', 'deg_error_pH10', 'deg_error_50C']\nTARGET_COLUMNS = ['reactivity', 'deg_Mg_pH10', 'deg_Mg_50C', 'deg_pH10', 'deg_50C']\n\n\nclass CompDataset(Dataset):\n    def __init__(self, data, bbps_folder, device='cpu', train_mode=True):\n        data = proc_df(data)\n        \n        self.data = {}\n        self.data['sequence'] = LongTensor(np.array([[SEQUANCE_CODE[t] for t in s] for s in data['sequence']]))\n        self.data['structure'] = LongTensor(np.array([[STRUCTURE_CODE[t] for t in s] for s in data['structure']]))\n        self.data['predicted_loop_type'] = LongTensor(np.array([[PREDICTED_LOOP_TYPE_CODE[t] for t in lt] for lt in data['predicted_loop_type']]))\n        self.data['bbps'] = FloatTensor(np.array(data['bbps'].to_list()))\n        self.data['structure_matrix'] = LongTensor(np.array(data['structure_matrix'].to_list()))\n        \n        \n        bbps_sum = FloatTensor(np.array(data['bbps_sum'].to_list()))\n        bbps_max = FloatTensor(np.array(data['bbps_max'].to_list()))\n        structure_bpps = FloatTensor(np.array(data['structure_bpps'].to_list()))\n        distance2pair = FloatTensor(np.array(data['distance2pair'].to_list()))\n        self.data['features'] = torch.stack([bbps_sum, bbps_max, structure_bpps, distance2pair], dim=1)\n        \n        \n        \n        if train_mode:\n            error = np.stack([data[col].tolist() for col in ERROR_COLUMNS], axis=1).swapaxes(1,2)\n            target = np.stack([data[col].tolist() for col in TARGET_COLUMNS], axis=1).swapaxes(1,2)\n            self.data['targets'] = torch.cat([FloatTensor(target), torch.zeros((target.shape[0],39,5))], dim=1)#FloatTensor(target)\n            self.data['error'] = torch.cat([FloatTensor(error), torch.ones((target.shape[0],39,5))*5], dim=1)#FloatTensor(error)\n            self.data['signal_to_noise'] = FloatTensor(data['signal_to_noise'].to_numpy())\n            self.data['sn_filter'] = LongTensor(data['SN_filter'].to_numpy())\n            self.data['filter_1'] = LongTensor((target.min(-1) > -0.5).all(-1))\n            self.data['filter_2'] = LongTensor(target.mean((1,2))/error.mean((1,2)) > 1)\n            \n        for key in self.data.keys():\n            self.data[key] = self.data[key].to(device)\n    \n        \n    def __len__(self):\n        return self.data['sequence'].shape[0]\n    \n    def __getitem__(self, idx: int):\n        return {k: v[idx] for k,v in self.data.items()}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### collate_fn \nused to create crop sequences with same length in one batch"},{"metadata":{"trusted":true},"cell_type":"code","source":"WSMin = 48\ndef random_window_collate_fn(batch):\n    seq_len = batch[0]['sequence'].shape[0]\n    windowed_batch = []\n    window_size = np.random.randint(WSMin, seq_len)\n    for el in batch:\n        window_left = np.random.randint(seq_len-window_size)\n        window_right = window_left+window_size\n        el['features'] = el['features'][:, window_left:window_right]\n        for k in ['sequence', 'structure', 'predicted_loop_type', 'targets', 'error']:\n            if k in el.keys():\n                el[k] = el[k][window_left:window_right]\n        for k in ['bbps', 'structure_matrix']:\n            el[k] = el[k][window_left:window_right, window_left:window_right]\n        windowed_batch.append(el)\n    new_batch = {}\n    for k in batch[0].keys():\n        new_batch[k] = torch.stack([el[k] for el in windowed_batch])\n    return new_batch","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Loss"},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\nfrom torch.nn.modules.loss import _Loss\nimport torch.nn.functional as F\n\n\nMEDIAN_ERROR = 0.105\n\n\nclass CompLoss(_Loss):\n    def forward(self, logits, targets, error, filter_1, filter_2):\n        logits, targets, error = logits[:,:,:3], targets[:,:,:3], error[:,:,:3]\n        weights = MEDIAN_ERROR/(MEDIAN_ERROR + error)\n        \n        loss = (logits - targets)**2\n        loss = loss*weights\n        loss = loss.mean(1)\n        loss = torch.sqrt(loss) \n        loss = loss.mean(0)\n        loss = loss.mean(0)\n        return loss\n    \n        \ndef comp_metric_fn(logits, targets, error, filter_1, filter_2):\n    logits, targets, error = logits[:,:,:3], targets[:,:,:3], error[:,:,:3]\n    logits, targets, error = logits[(filter_1==1)&(filter_2==1)], targets[(filter_1==1)&(filter_2==1)], error[(filter_1==1)&(filter_2==1)]\n    weights = (error < 5).float()\n\n    loss = (logits - targets)**2\n    loss = loss.sum(1)/weights.sum(1)\n    loss = loss.mean(0)\n    loss = torch.sqrt(loss) \n    loss = loss.mean(0)\n    return loss.item()\n\n\nclass PreTrainLoss(_Loss):\n    def forward(self, logits, sequence):\n        logits = logits.reshape(-1,4)\n        sequence = sequence.reshape(-1)\n        return F.cross_entropy(logits, sequence)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### calk metric on test batch end(catalyst Callback)"},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\n\nfrom catalyst.dl import Callback, CallbackOrder\n\n\nclass LoaderMetricCallback(Callback):\n    def __init__(self, input_key, output_key, metric_fn, prefix=\"metric\"):\n        super().__init__(CallbackOrder.Metric)\n        self.input_key = input_key\n        self.output_key = output_key\n        self.metric_fn = metric_fn\n        self.prefix = prefix\n        self.data = {k: [] for k in self.output_key+self.input_key}\n        \n    def on_batch_end(self, state):\n        y_hat = state.output['logits'].detach().cpu().numpy()\n        y = state.input['targets'].detach().cpu().numpy()\n        for k in self.output_key:\n            self.data[k].append(state.output[k].detach().cpu())\n        for k in self.input_key:\n            self.data[k].append(state.input[k].detach().cpu())\n            \n    def on_loader_end(self, state):\n        if state.is_valid_loader:\n            for k in self.output_key+self.input_key:\n                self.data[k] = torch.cat(self.data[k])\n\n            state.loader_metrics[self.prefix] = self.metric_fn(**self.data)\n\n        self.data = {k: [] for k in self.output_key+self.input_key}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\nfrom torch import nn\nimport torch.nn.functional as F\n\n\nclass Mish(nn.Module):\n    def forward(self, x):\n        return x * torch.tanh(F.softplus(x))\n    \n    \nclass Conv1dStack(nn.Module):\n    def __init__(self, in_dim, out_dim, kernel_size=3, padding=1, dilation=1):\n        super(Conv1dStack, self).__init__()\n        self.conv = nn.Sequential(\n            nn.Conv1d(in_dim, out_dim, kernel_size=kernel_size, padding=padding, dilation=dilation, bias=False),\n            nn.BatchNorm1d(out_dim),\n            nn.Dropout(0.1),\n            nn.LeakyReLU(),\n        )\n        self.res = nn.Sequential(\n            nn.Conv1d(out_dim, out_dim, kernel_size=kernel_size, padding=padding, dilation=dilation, bias=False),\n            nn.BatchNorm1d(out_dim),\n            nn.Dropout(0.1),\n            nn.LeakyReLU(),\n        )\n\n    def forward(self, x):\n        x = self.conv(x)\n        h = self.res(x)\n        return x + h\n\n    \nclass SeqEncoder(nn.Module):\n    def __init__(self, in_dim: int):\n        super(SeqEncoder, self).__init__()\n        self.conv0 = Conv1dStack(in_dim, 128, 3, padding=1)\n        self.conv1 = Conv1dStack(128, 64, 6, padding=5, dilation=2)\n        self.conv2 = Conv1dStack(64, 32, 15, padding=7, dilation=1)\n        self.conv3 = Conv1dStack(32, 32, 30, padding=29, dilation=2)\n\n    def forward(self, x):\n        x1 = self.conv0(x)\n        x2 = self.conv1(x1)\n        x3 = self.conv2(x2)\n        x4 = self.conv3(x3)\n        x = torch.cat([x1, x2, x3, x4], dim=1)\n        return x\n\n    \nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.pretrain_mode = False\n        prenet_hidden = 128\n        seq_hidden_size = 1024\n        hidden_cls_dim = 128\n        feature_prenet_hidden = 32\n        n_feature = 4\n        \n        self.sequence_embed = nn.Embedding(4, 8)\n        self.predicted_loop_type_embed = nn.Embedding(7, 16)\n        self.structure_embed = nn.Embedding(4, 8)\n        \n        self.feature_prenet = nn.Sequential(\n                nn.Conv1d(n_feature, feature_prenet_hidden, kernel_size=1),\n                nn.BatchNorm1d(feature_prenet_hidden),\n            )\n\n        self.prenet = nn.Sequential(\n            nn.Conv1d((8+16+8) * 2 + 1 + feature_prenet_hidden*2, prenet_hidden, kernel_size=1),\n            nn.BatchNorm1d(prenet_hidden),\n        )\n        self.seq_encoder = SeqEncoder(prenet_hidden)\n\n        self.reccurent_layers = nn.ModuleList()\n        self.dense_layers = nn.ModuleList()\n        for idx in range(2):\n            self.reccurent_layers.append(nn.GRU(\n                input_size= 256 if idx==0 else seq_hidden_size*2,\n                hidden_size=seq_hidden_size,\n                num_layers=1,\n                batch_first=True,\n                bidirectional=True,\n            ))\n            \n            self.dense_layers.append(nn.Sequential(\n                nn.Conv1d(seq_hidden_size*2, seq_hidden_size, kernel_size=1),\n                nn.BatchNorm1d(seq_hidden_size),\n                Mish(),\n                nn.Dropout(0.3)\n            ))\n        \n        self.classifier = nn.Sequential(\n                nn.Linear(seq_hidden_size*2, hidden_cls_dim), Mish(), nn.Dropout(p=0.3),\n                nn.Linear(hidden_cls_dim, hidden_cls_dim), Mish(), nn.Dropout(p=0.2),\n                nn.Linear(hidden_cls_dim, 5)\n            )\n        self.pretrain = nn.Sequential(\n                nn.Linear(seq_hidden_size*2, hidden_cls_dim), Mish(), nn.Dropout(p=0.3),\n                nn.Linear(hidden_cls_dim, hidden_cls_dim), Mish(), nn.Dropout(p=0.2),\n                nn.Linear(hidden_cls_dim, 4)#, nn.Softmax()\n            )\n\n    def _attention_head(self, x, bbps):\n        return torch.bmm(bbps, x)\n    \n\n    def forward(self, sequence, predicted_loop_type, structure, bbps, features):\n        sequence = self.sequence_embed(sequence)\n        predicted_loop_type = self.predicted_loop_type_embed(predicted_loop_type)\n        structure = self.structure_embed(structure)\n        \n        features = self.feature_prenet(features).permute(0,2,1)\n        \n        x = torch.cat([sequence, predicted_loop_type, structure, features], dim=-1)\n        \n        if self.pretrain_mode:\n            n_elements_to_mask = 3\n            for i in range(x.shape[0]):\n                mask = list(np.random.randint(x.shape[1], size=n_elements_to_mask))\n                x[i, mask]=0\n            \n        x_sec = self._attention_head(x, bbps)\n        attention_sum_f = bbps.sum(-1)[:,:,None]\n        x = torch.cat([x, x_sec, attention_sum_f], dim=-1)\n    \n        x = self.prenet(x.permute(0,2,1)).permute(0,2,1)\n        x = self.seq_encoder(x.permute(0,2,1)).permute(0,2,1)\n\n        for idx, (reccurent_layer, linear_layer) in enumerate(zip(self.reccurent_layers, self.dense_layers)):\n            x, _ = reccurent_layer(x)\n            x = linear_layer(x.permute(0,2,1)).permute(0,2,1)\n            x = torch.cat([x, self._attention_head(x, bbps)], axis=-1)\n        \n        if self.pretrain_mode:\n            out = self.pretrain(x)\n        else:\n            out = self.classifier(x)\n        return out","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# PreTrain"},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nfrom os import listdir\n\n\ndef remove_useless_checkpoints(path, usefull_checkpoint):\n    for el in listdir(path):\n        if el != usefull_checkpoint:\n            print(f'rm {path}/{el}')\n            !rm '{path}/{el}'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\nfrom torch.utils.data import DataLoader\nfrom catalyst.dl import SupervisedRunner, Runner\nfrom catalyst.contrib.nn.optimizers import RAdam, Lookahead, Adam\nfrom catalyst.contrib.nn.criterion import CrossEntropyLoss\nfrom catalyst.dl.callbacks import OptimizerCallback, CriterionCallback\n\n\ndef dataloader_train_init(data, device='cuda'):\n    dataset = CompDataset(data, bbps_folder, device=device, train_mode=False)\n    return DataLoader(dataset, 16, shuffle=True, drop_last=True, collate_fn=random_window_collate_fn)\n\n\ndevice = 'cuda'\n\ndataloader_107 = dataloader_train_init(train.append(test[test['seq_length']==107]), device)\ndataloader_130 = dataloader_train_init(test[test['seq_length']==130], device)\n\nmodel = Model()\nmodel.pretrain_mode = True\n\noptimizer =  Lookahead(RAdam(model.parameters(), lr=1e-3))\n\ncriterion = {\"loss\": PreTrainLoss()}\n\ncallbacks =[CriterionCallback(input_key='sequence', output_key='logits', prefix=\"loss\", criterion_key=\"loss\"),\n            OptimizerCallback(metric_key=\"loss\", accumulation_steps=1),]\n\ninput_keys = ['sequence', 'predicted_loop_type', 'structure', 'bbps', 'features']\ninput_citerion_keys = ['sequence']\nrunner = SupervisedRunner(device=device, input_key=input_keys, input_target_key=input_citerion_keys)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(5):\n    runner.train(\n            model=model,\n            optimizer=optimizer,\n            criterion=criterion,\n            loaders={'train': dataloader_107},\n            logdir='pretrain',\n            callbacks=callbacks,\n            num_epochs=1,\n            verbose= False,\n            main_metric=\"loss\",\n            minimize_metric=True,\n        )      \n    runner.train(\n            model=model,\n            optimizer=optimizer,\n            criterion=criterion,\n            loaders={'train': dataloader_130},\n            logdir='pretrain',\n            callbacks=callbacks,\n            num_epochs=1,\n            verbose=False,\n            main_metric=\"loss\",\n            minimize_metric=True,\n        )\n    \nremove_useless_checkpoints('pretrain/checkpoints', 'last.pth')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Train"},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\nfrom torch.utils.data import DataLoader\nfrom catalyst.dl import SupervisedRunner, Runner\nfrom catalyst.contrib.nn.optimizers import RAdam, Lookahead, Adam\nfrom catalyst.dl.callbacks import OptimizerCallback, EarlyStoppingCallback, SchedulerCallback, CriterionCallback, MetricAggregationCallback, MetricManagerCallback\n\ndevice = 'cuda'\ninput_keys = ['sequence', 'predicted_loop_type', 'structure', 'bbps', 'features']\ninput_citerion_keys = ['targets', 'error', 'filter_1', 'filter_2']\nbatch_size = 16\n\ndef dataloader_train_init(idx):\n    dataset = CompDataset(train.iloc[idx], bbps_folder, device=device)\n    return DataLoader(dataset, batch_size, shuffle=True, drop_last=True, collate_fn=random_window_collate_fn)\n\ndef dataloader_valid_init(idx):\n    dataset = CompDataset(train.iloc[idx], bbps_folder, device=device)\n    return DataLoader(dataset, batch_size, shuffle=False, drop_last=False)\n\ndef model_init():\n    model = Model()\n    model.load_state_dict(torch.load(f'pretrain/checkpoints/last.pth')['model_state_dict'])\n    return model\n\noptimizer_init = lambda model : Lookahead(RAdam(model.parameters(), lr=1e-3))\nscheduler_init = lambda optimizer : torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.25, patience=4, cooldown=5, min_lr=1e-6)\nloss_init = lambda : CompLoss()\ncallbacks_init = lambda : [CriterionCallback(input_key=input_citerion_keys, output_key=['logits'], prefix=\"loss\", criterion_key=\"loss\"),\n                            LoaderMetricCallback(input_key=input_citerion_keys, output_key=['logits'], prefix=\"metric\", metric_fn=comp_metric_fn),\n                            OptimizerCallback(metric_key=\"loss\", accumulation_steps=1),\n                            EarlyStoppingCallback(patience=15, metric='metric', minimize=True),\n                            SchedulerCallback(mode='epoch')]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i, (train_idx, val_idx) in enumerate(folds):\n    model = model_init()\n    optimizer = optimizer_init(model)\n    scheduler = scheduler_init(optimizer)\n    criterion = {\"loss\": loss_init()}\n    loaders = {'train': dataloader_train_init(train_idx), 'valid': dataloader_valid_init(val_idx)}\n    callbacks = callbacks_init()\n    runner = SupervisedRunner(device=device, input_key=input_keys, input_target_key=input_citerion_keys)\n    runner.train(\n            model=model,\n            optimizer=optimizer,\n            scheduler=scheduler,\n            criterion=criterion,\n            loaders=loaders,\n            logdir=f'fold_{i}',\n            callbacks=callbacks,\n            num_epochs=9999,\n            verbose=True,\n            load_best_on_end=True,\n            main_metric=\"metric\",\n            minimize_metric=True,\n        )  \n    remove_useless_checkpoints(f'fold_{i}/checkpoints', 'best.pth')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Validation"},{"metadata":{"trusted":true},"cell_type":"code","source":"from torch import FloatTensor\n\ncv_scores = []\noof_prediction = torch.zeros((train.shape[0], 107, 5))\n\nfor i, (train_idx, val_idx) in enumerate(folds):\n    best_model_path = f'fold_{i}/checkpoints/best.pth'\n    runner = SupervisedRunner(device='cuda', input_key=input_keys, input_target_key=input_citerion_keys)\n    dataloader = dataloader_valid_init(val_idx)\n    \n    prediction = runner.predict_loader(loader=dataloader, model=Model(), resume=best_model_path)\n    prediction = torch.cat([b['logits'] for b in prediction])\n    fold_score = comp_metric_fn(prediction, *[dataloader.dataset.data[el] for el in input_citerion_keys])\n    cv_scores.append(fold_score)\n    \n    oof_prediction[val_idx] = prediction.cpu()#.numpy()\n\ncv_scores = np.array(cv_scores)\ntrain_data = CompDataset(train, bbps_folder).data\noff_score = comp_metric_fn(oof_prediction, *[train_data[el] for el in input_citerion_keys])\n\nnp.save(\"oof_prediction\", oof_prediction[:,:,[0,1,3,2,4]])\n\nfinal_log = f'Fold scores: {list(cv_scores)}\\nOFF score: {off_score}\\nFold mean: {cv_scores.mean()}\\nFold std : {cv_scores.std()}'\nwith open(f\"final_log.txt\", 'w') as file:\n    file.write(final_log)\nprint(final_log)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Inference"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_107_pred = []\ntest_130_pred = []\nfor i, _ in enumerate(folds):\n    best_model_path = f'fold_{i}/checkpoints/best.pth'\n    runner = SupervisedRunner(device='cuda', input_key=input_keys, input_target_key=input_citerion_keys)\n    \n    dataset = CompDataset(test[test['seq_length']==107], bbps_folder, device='cuda', train_mode=False)\n    dataloader = DataLoader(dataset, batch_size,)\n    \n    \n    prediction = runner.predict_loader(loader=dataloader, model=model_init(), resume=best_model_path)\n    prediction = np.concatenate([b['logits'].cpu().numpy() for b in prediction])[:,:,[0,1,3,2,4]]\n    test_107_pred.append(prediction)\n    \n    \n    dataset = CompDataset(test[test['seq_length']==130], bbps_folder, device='cuda', train_mode=False)\n    dataloader = DataLoader(dataset, batch_size,)\n    \n    prediction = runner.predict_loader(loader=dataloader, model=model_init(), resume=best_model_path)\n    prediction = np.concatenate([b['logits'].cpu().numpy() for b in prediction])[:,:,[0,1,3,2,4]]\n    test_130_pred.append(prediction)\n    \ntest_107_pred_oof = np.mean(test_107_pred, axis=0)\ntest_130_pred_oof = np.mean(test_130_pred, axis=0)\n\nsample_sub = pd.read_csv(f'{path}sample_submission.csv')\n\nsample_sub_ids = sample_sub['id_seqpos'].map(lambda x: x[:12])\nssi_107 = sample_sub_ids.isin(test[test['seq_length']==107]['id'])\nssi_130 = sample_sub_ids.isin(test[test['seq_length']==130]['id'])\nsample_sub.loc[ssi_107, 'reactivity':] = test_107_pred_oof.reshape(-1, 5)\nsample_sub.loc[ssi_130, 'reactivity':] = test_130_pred_oof.reshape(-1, 5)\n\nsample_sub.to_csv(f\"submission.csv\", index=False)\nsample_sub","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}