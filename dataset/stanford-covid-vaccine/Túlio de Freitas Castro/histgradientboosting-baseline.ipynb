{"cells":[{"metadata":{},"cell_type":"markdown","source":"# OpenVaccine - Covid-sars-2019\n\nThis notebook has the intend to show a way to expand the list inside th JSONs files and a little\nintrospection with the HistRegressor and a MultiLabel regression task."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\nfrom copy import deepcopy\nimport json\nimport os\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom sklearn.experimental import enable_hist_gradient_boosting\nfrom sklearn.ensemble import HistGradientBoostingRegressor, GradientBoostingRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.multioutput import MultiOutputRegressor\nfrom sklearn.preprocessing import LabelEncoder\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n\n#for dirname, _, filenames in os.walk('/kaggle/input'):\n#    for filename in filenames:\n#        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### ~This function is not used~"},{"metadata":{"trusted":true},"cell_type":"code","source":"def mcrmse_loss(y_true, y_pred, N=3):\n    \"\"\"\n    Calculates competition eval metric\n    \"\"\"\n    assert len(y_true) == len(y_pred)\n    n = len(y_true)\n    return np.sum(np.sqrt(np.sum((y_true - y_pred)**2, axis=0)/n)) / N","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Loading the datasets\n\nThe train and test are not load with Pandas library, only submission file is opened to copy the ID's to final submission."},{"metadata":{"trusted":true},"cell_type":"code","source":"data_train = [json.loads(line) for line in open('../input/stanford-covid-vaccine/train.json','r')]\ndata_test = [json.loads(linha) for linha in open('../input/stanford-covid-vaccine/test.json','r')]\ntest_set = pd.read_csv('../input/stanford-covid-vaccine/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Train treatment"},{"metadata":{"trusted":true},"cell_type":"code","source":"for jason in data_train:\n    jason['step'] = list(range(jason['seq_scored']))\n    jason['sequence'] = list(jason['sequence'])\n    jason['structure'] = list(jason['structure'])\n    jason['predicted_loop_type'] = list(jason['predicted_loop_type'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Test treatment"},{"metadata":{"trusted":true},"cell_type":"code","source":"for jason in data_test:\n    jason['step'] = list(range(jason['seq_scored']))\n    jason['sequence'] = list(jason['sequence'])\n    jason['structure'] = list(jason['structure'])\n    jason['predicted_loop_type'] = list(jason['predicted_loop_type'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.json_normalize(data = data_train, \n                            record_path ='reactivity',  \n                            meta =['id','signal_to_noise',\n                                  'SN_filter','seq_length','seq_scored']) \ntrain.rename(columns={0:'reactivity'}, inplace=True)\ntrain['step'] = pd.json_normalize(data = data_train, \n                            record_path ='step'\n                                        )\ntrain['sequence'] = pd.json_normalize(data = data_train, \n                            record_path ='sequence'\n                                        )\ntrain['structure'] = pd.json_normalize(data = data_train, \n                            record_path ='structure'\n                                        )\ntrain['predicted_loop_type'] = pd.json_normalize(data = data_train, \n                            record_path ='predicted_loop_type'\n                                        )\ntrain['reactivity_error'] = pd.json_normalize(data = data_train, \n                            record_path ='reactivity_error'\n                                        )\ntrain['deg_Mg_pH10'] = pd.json_normalize(data = data_train, \n                            record_path ='deg_Mg_pH10'\n                                        )\ntrain['deg_error_Mg_pH10'] = pd.json_normalize(data = data_train, \n                            record_path ='deg_error_Mg_pH10'\n                                        )\ntrain['deg_pH10'] = pd.json_normalize(data = data_train, \n                            record_path ='deg_pH10',\n                                        )\ntrain['deg_error_pH10'] = pd.json_normalize(data = data_train, \n                            record_path ='deg_error_pH10',\n                                        )\ntrain['deg_Mg_50C'] = pd.json_normalize(data = data_train, \n                            record_path ='deg_Mg_50C',\n                                        )\ntrain['deg_error_Mg_50C'] = pd.json_normalize(data = data_train, \n                            record_path ='deg_error_Mg_50C',\n                                        )\ntrain['deg_50C'] = pd.json_normalize(data = data_train, \n                            record_path ='deg_50C',\n                                        )\ntrain['deg_error_50C'] = pd.json_normalize(data = data_train, \n                            record_path ='deg_error_50C',\n                                        )\n\ntrain.set_index(['id','step'], inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test = pd.json_normalize(data = data_test,\n                         record_path = 'sequence',\n                        meta = ['id','seq_length','seq_scored'])\ntest.rename(columns={0:'sequence'},inplace=True)\ntest['step'] = pd.json_normalize(data = data_test,\n                                record_path = 'step')\ntest['sequence'] = pd.json_normalize(data = data_test,\n                                    record_path = 'sequence')\ntest['structure'] = pd.json_normalize(data = data_test,\n                                     record_path = 'structure')\ntest['predicted_loop_type'] = pd.json_normalize(data = data_test,\n                                               record_path = 'predicted_loop_type')\ntest.set_index(['id','step'], inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Here we get a insight for what happened to our dataset"},{"metadata":{},"cell_type":"markdown","source":"### **Train**"},{"metadata":{"trusted":true},"cell_type":"code","source":"train","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### **Test**"},{"metadata":{"trusted":true},"cell_type":"code","source":"test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.random.seed(2020) #Seed the randomness to be deterministic","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Preprocessing"},{"metadata":{"trusted":true},"cell_type":"code","source":"enc = LabelEncoder()\ncategory_cols_train = [cols for cols in train.columns if train[cols].dtype == 'object']\ncategory_cols_test = [cols for cols in test.columns if test[cols].dtype == 'object']\nprint(category_cols_train)\nprint(category_cols_test)\nX_train_enc = deepcopy(train)\nX_test_enc = deepcopy(test)\nfor cols in category_cols_train:\n    X_train_enc[cols] = enc.fit_transform(X_train_enc[cols])\nfor cols in category_cols_test:\n    X_test_enc[cols] = enc.fit_transform(X_test_enc[cols])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = X_train_enc.drop(['reactivity', 'deg_Mg_pH10', 'deg_pH10', 'deg_Mg_50C', 'deg_50C'],axis=1)\ny = X_train_enc.loc[:,['reactivity', 'deg_Mg_pH10', 'deg_pH10', 'deg_Mg_50C', 'deg_50C']]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model Instance\n\n> Here I only used HistBoosting, due to large dataset (n_samples > 100000), the GradientBoostingRegressor take too long too run."},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"hgbr = MultiOutputRegressor(HistGradientBoostingRegressor(max_iter = 1750, max_depth = 15,early_stopping = True, n_iter_no_change = 10,\n                                                          learning_rate = 0.0025, tol = 1e-6, validation_fraction = 0.2,\n                                                          verbose = 2, max_leaf_nodes = 64),\n                           n_jobs = 4\n)\n\ngbr = MultiOutputRegressor(GradientBoostingRegressor(loss = 'huber', n_estimators = 1000, max_depth = 15,\n                                                     learning_rate = 0.0025, tol = 1e-7, validation_fraction = 0.2,\n                                                     n_iter_no_change = 15, verbose = 2\n    )\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Cross Validation - train_test_split"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"hgbr.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = hgbr.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## RMSE"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(mean_squared_error(y_test,y_pred,squared=False))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_enc[X_test_enc.columns]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"hgbr.fit(X_train_enc[X_test_enc.columns],y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_2 = hgbr.predict(X_test_enc)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_2.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Results\n\nIt's better than the Gaussin Notebook found here, but i think the top 4, will be better with NN than usual machine learning algorithms."},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.DataFrame(np.concatenate([test_set.id_seqpos.values[:,np.newaxis],y_pred_2],axis=1),columns=test_set.columns)\ndisplay(submission.head(10))\nsubmission.to_csv('submission.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This notebook is highly inspired in this notebook:\n\n*[Flatten JSON Data](https://www.kaggle.com/arunprathap/openvaccine-flatten-json-data)\n\nFeel free to comment and upvote üòÅ"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}