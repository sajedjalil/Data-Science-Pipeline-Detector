{"cells":[{"metadata":{},"cell_type":"markdown","source":"<html>\n      <body>\n    <b style=\"font-size:18\"><center><b>Theoretical Background </b>\n        <br><br>\n    <p style=\"font-size:12, line-height: 1.5\"><left> RNA forms secondary and tertiary structures apart from being existant as a single strand. RNA Sequence is formed by the combination of 4 nucleotides or bases { 'A', 'U', 'C', 'G' }. If any base/nucleotide at position i forms a bond with base/nucleotide at position j, they forms a base pair. Bases ('A', 'U') forms the double bond where as ('C', 'G') forms the triple bond. Researchers also noticed ('U', 'G') wobbles.<br><br>Note: Strength of triple bond is more than the double bond. Hence ('C', 'G') base pair is structurally more stable then the ('A', 'U'). <br> <center> The below tables explains the basic descriptions of the bases </p>\n      </body>\n</html>\n         \n<html>\n<head>\n<style>\ntable {\n  width:100%;\n}\ntable, th, td {\n  border: 1px solid black;\n  border-collapse: collapse;\n}\nth, td {\n  padding: 15px;\n  text-align: left;\n}\n#t01 tr:nth-child(even) {\n  background-color: #eee;\n}\n#t01 tr:nth-child(odd) {\n background-color: #fff;\n}\n#t01 th {\n  background-color: black;\n  color: white;\n}\n</style>\n</head>\n<body>\n\n\n<table id=\"t01\">\n  <tr>\n    <th>Base</th>\n    <th>Chemical Formula</th> \n    <th>Molecular Weight</th>\n    <th>Hydrogen Bond Donors</th>\n    <th>Hydrogen Bond Acceptors</th>\n  </tr>\n  <tr>\n    <td><center>Adenine</td>\n    <td><center>C<sub>5</sub>H<sub>5</sub>N<sub>5</sub></td>\n    <td><center>135.13 g/mol</td>\n    <td><center>2</td>\n    <td><center>4</td>\n  </tr>\n  <tr>\n    <td><center>Uracil</td>\n    <td><center>C<sub>4</sub>H<sub>4</sub>N<sub>2</sub>O<sub>2</sub></td>\n    <td><center>112.09 g/mol</td>\n    <td><center>2</td>\n    <td><center>2</td>\n  </tr>\n  <tr>\n    <td><center>Cystine</td>\n    <td><center>C<sub>6</sub>H<sub>12</sub>N<sub>2</sub>O<sub>4</sub>S<sub>2</sub></td>\n    <td><center>240.3 g/mol</td>\n    <td><center>4</td>\n    <td><center>8</td>\n  </tr>\n  <tr>\n    <td><center>Guanine</td>\n    <td><center>C<sub>5</sub>H<sub>5</sub>N<sub>5</sub>O</td>\n    <td><center>151.13 g/mol</td>\n    <td><center>3</td>\n    <td><center>3</td>\n  </tr>\n</table>\n<small><center>source: <a>https://pubchem.ncbi.nlm.nih.gov/ </a></small>\n</body>\n</html>\n"},{"metadata":{},"cell_type":"markdown","source":"\n<body>\n<b style=\"font-size:14\"><left>&nbsp;Rules of Base Pair formation:</b>\n<p style=\"font-size:12\"><left>\n<ol>\n  <li>('A', 'U') and ('C', 'G') forms majority of the base pairs</li>\n  <li>If a base at position i pairs with a base with position j, then (j-i) >= 4. In other words, there should be atleast 4 intermediate bases present between the interacting bases engaging in a bond as observed by researchers. </li>\n  <li>A base at position i can form pair at only one position j where (i != j).</li>\n  <li>No base pair is allowed to overlap with other base pair, i.e say (i,j) denotes the positions of base pair, A and (k, l) denotes the positions of base pair B, then  (i < j < k < l)  or  (i < k < l < j)</li>  \n</ol>  \n<text><right>Source:<a> https://www.coursera.org/lecture/algorithmic-thinking-2/the-rna-secondary-structure-problem-80RrW</a></text>    \n</p>\n</body>\n        \n\n"},{"metadata":{},"cell_type":"markdown","source":"<b style=\"font-size:14\"><left><b>Problem Statement:</b> Covid19 Vaccine Degration Prediction</b>\n<p style=\"font-size:12\">\n    mRNA vaccines have taken the lead as the fastest vaccine candidates for COVID-19, but currently, they face key potential limitations. One of the biggest challenges right now is how to design super stable messenger RNA molecules (mRNA).Researchers have observed that RNA molecules have the tendency to spontaneously degrade.\n    <br><br>\n    The goal is to model/predict the likely degradation rates at each base of an RNA molecule, trained on a subset of an Eterna dataset comprising over 3000 RNA molecules (which span a panoply of sequences and structures) and their degradation rates at each position.\n    \n</p>"},{"metadata":{},"cell_type":"markdown","source":"**Data Description**\n<p style=\"font-size:12\">\n    The detailed description of the data is already given to us by the hosts of the competion which can be found <a href=\"https://www.kaggle.com/c/stanford-covid-vaccine/data\">here</a></b>.\n    <br><br>\n    <b>Feature Analysis:</b><br><br>\n    1) Given a RNA sequence, its secondary structure and loop type can be predicted; thus making them \"derived\" features. Noticed that for the samples considered, the 'structure' feature values in the given data matches with the structure predicted by the <a href=\"http://rna.tbi.univie.ac.at/cgi-bin/RNAWebSuite/RNAfold.cgi\">this</a> website. <br><br>\n\n\n    sample sequence: GGAAAGGGCCCGGCGGGCCCAACGGCCGCCGCGGCCGCAACGGCAACAACAACAACACAGCAAACAAACGCGCGCUUCGGCGCGCGAAAAGAAACAACAACAACAAC \n    structure given: ......((((((((((.((....)))))))).))))................................(((((((....))))))).....................\n    predicted   :    ......((((((((((.((....)))))))).))))................................(((((((....))))))).....................\n   \n   2) Sturcture to Loop type prediction. <br><br>\n   (Note: The below conversions are based on observing few samples. May not accurately describe underlying prediction algorithm)<br><br>\n &emsp;a) one or more dots '.' at the trailing ends are dangling Ends (E) <br>\n \n         .....................\n         EEEEEEEEEEEEEEEEEEEEE\n\n &emsp;b) base pair '(' or ')' forms the stem (S). In other words '(' or ')' ----> 'S' <br>\n \n         (((((((((\n         SSSSSSSSS\n         \n &emsp;c) four or more dots '(....)' between matching brackets forms a hair pin loop (H) <br>\n \n         ((((.....))))\n         SSSSHHHHHSSSS\n &emsp;d) one dot between matching brackets or Stem '(.((' or ').)))' is a bulge (B). <br>\n \n        ((((((....))).)))\n        SSSSSSHHHHSSSBSSS\n &emsp;e) one or more dots (equal from both the sides )  between the stems at the symmetric positions in a matched fragment of RNA  is the Internal loop (I) <br>\n \n         (((..((.((...))))..)))\n         SSSIISSBSSHHHSSSSIISSS\n &emsp;f) two or more dots (unequal from both the sides) between the stems in a matching fragment of RNA is the Multiloop (M) \n \n         ((((((....(((((((....))))))).......((((((....))))))...))))))\n         SSSSSSMMMMSSSSSSSHHHHSSSSSSSMMMMMMMSSSSSSHHHHSSSSSSMMMSSSSSS\n&emsp;g) Dots between right bracket ')' and left bracket '(' in a matched RNA fragment forms Multiloop (M). The above example hold good.<br>\n&emsp;h) Dots between right bracket ')' and left bracket '(' in between two matched RNA fragment forms Multiloop (M) or external Loop.There is some ambiguity here. Probably inconsistancy in the representation i.e used interchangably. Though seems like External loop 'X' might be the appropriate representation. Suggestions are appreciated.<br>\n\n        \n        .....(((....))).....................((((((....))).)))...............(((((((....))))))).....................\n        EEEEESSSHHHHSSSMMMMMMMMMMMMMMMMMMMMMSSSSSSHHHHSSSBSSSMMMMMMMMMMMMMMMSSSSSSSHHHHSSSSSSSEEEEEEEEEEEEEEEEEEEEE\n        \n        ......((((((((((.((....)))))))).))))................................(((((((....))))))).....................\n        EEEEEESSSSSSSSSSBSSHHHHSSSSSSSSBSSSSXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXSSSSSSSHHHHSSSSSSSEEEEEEEEEEEEEEEEEEEEE\n\n</p>"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import packages and modules \n\nimport pandas as pd\nimport numpy as np\nimport os\nfrom os import path\nfrom collections import Counter\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\n# Initialize the paths of the dataset\n\ndataset_dir_path = '/kaggle/input/stanford-covid-vaccine'\n\nbpps_path = path.join(dataset_dir_path, 'bpps')\ntrain_data_path = path.join(dataset_dir_path, 'train.json')\ntest_data_path = path.join(dataset_dir_path, 'test.json')\nsample_submission_path = path.join(dataset_dir_path, 'sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Model:\n    @staticmethod\n    def gru_layer(hidden_dim, dropout):\n        return L.Bidirectional(L.GRU(\n            hidden_dim, dropout=dropout, return_sequences=True, kernel_initializer='orthogonal'))\n\n    @staticmethod\n    def build_model(embed_size, seq_len=107, pred_len=68, dropout=0.5,\n                    sp_dropout=0.2, embed_dim=200, hidden_dim=256, n_layers=3):\n        inputs = L.Input(shape=(seq_len, 2))\n        embed = L.Embedding(input_dim=embed_size, output_dim=embed_dim)(inputs)\n\n        reshaped = tf.reshape(\n            embed, shape=(-1, embed.shape[1], embed.shape[2] * embed.shape[3])\n        )\n        hidden = L.SpatialDropout1D(sp_dropout)(reshaped)\n\n        for x in range(n_layers):\n            hidden = Model.gru_layer(hidden_dim, dropout)(hidden)\n\n        # Since we are only making predictions on the first part of each sequence,\n        # we have to truncate it\n        truncated = hidden[:, :pred_len]\n        out = L.Dense(5, activation='linear')(truncated)\n\n        model = tf.keras.Model(inputs=inputs, outputs=out)\n        model.compile(tf.optimizers.Adam(), loss=HelperFunctions.MCRMSE)\n\n        return model\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nclass Columns:\n    index = 'index'\n    id = 'id'\n    sequence = 'sequence'\n    seq_length = 'seq_length'\n    seq_scored = 'seq_scored'\n    structure = 'structure'\n    signal_to_noise = 'signal_to_noise'\n    SN_filter = 'SN_filter'\n    predicted_loop_type = 'predicted_loop_type'\n    reactivity_error = 'reactivity_error'\n    deg_error_Mg_pH10 = 'deg_error_Mg_pH10'\n    deg_error_pH10 = 'deg_error_pH10'\n    deg_error_Mg_50C = 'deg_error_Mg_50C'\n    deg_error_50C = 'deg_error_50C'\n    reactivity = 'reactivity'\n    deg_Mg_pH10 = 'deg_Mg_pH10'\n    deg_pH10 = 'deg_pH10'\n    deg_Mg_50C = 'deg_Mg_50C'\n    deg_50C = 'deg_50C'\n\n\nclass TestColumns:\n    index = 'index'\n    id = 'id'\n    sequence = 'sequence'\n    seq_length = 'SN_filter'\n    seq_scored = 'seq_scored'\n    structure = 'structure'\n    predicted_loop_type = 'predicted_loop_type'\n\n\nclass SubmissionColumns:\n    id_seqpos = 'id_seqpos'\n    reactivity = 'reactivity'\n    deg_Mg_pH10 = 'deg_Mg_pH10'\n    deg_pH10 = 'deg_pH10'\n    deg_Mg_50C = 'deg_Mg_50C'\n    deg_50C = 'deg_50C'\n\n    @staticmethod\n    def get_target_columns():\n        return [SubmissionColumns.reactivity, SubmissionColumns.deg_Mg_pH10, SubmissionColumns.deg_pH10,\n                SubmissionColumns.deg_Mg_50C, SubmissionColumns.deg_50C]\n\n\nclass Submission(SubmissionColumns):\n    @staticmethod\n    def MCRMSE(y_true, y_pred):\n        colwise_mse = tf.reduce_mean(tf.square(y_true - y_pred), axis=1)\n        return tf.reduce_mean(tf.sqrt(colwise_mse), axis=1)\n\n    @staticmethod\n    def get_submission_columns(optional=False):\n        if optional:\n            return SubmissionColumns.get_target_columns()\n        else:\n            return [SubmissionColumns.reactivity, SubmissionColumns.deg_pH10, SubmissionColumns.deg_Mg_50C]\n\n\nclass Definitions:\n    @staticmethod\n    def structure_tokens():\n        return ['S', 'B', 'M', 'I', 'E', 'H', 'X']\n\n    @staticmethod\n    def nucleotides_tokens():\n        return ['A', 'U', 'C', 'G']\n\n    @staticmethod\n    def tokens():\n        n_tokens = Definitions.nucleotides_tokens()\n        s_tokens = Definitions.structure_tokens()\n        n_tokens.extend(s_tokens)\n        return n_tokens\n\n    @staticmethod\n    def structures():\n        return {\n            'S': 'Paired Stem',\n            'B': 'Bulge',\n            'M': 'Multi Loop',\n            'I': 'Internal Loop',\n            'E': 'Dangling End',\n            'H': 'Hairpin Loop',\n            'X': 'External Loop'\n        }\n\n    @staticmethod\n    def nucleotides():\n        return {\n            'A': 'adenine',\n            'U': 'uracil',\n            'C': 'cytosine',\n            'G': 'guanine',\n        }\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Data class \nclass Data(Columns):\n    def __init__(self, data_path, bpps_path=None):\n        self.path = data_path\n        self.bpps_path = bpps_path\n        self.data = pd.read_json(self.path, lines=True)\n        self.n_rows, self.n_cols = self.data.shape\n\n    def get_sample(self):\n        return self.data.iloc[[np.random.randint(low=0, high=self.n_cols)]]\n\n    def get_bpps_matrix(self, id):\n        bbps_file = id + '.npy'\n        file_path = path.join(self.bpps_path, bbps_file)\n        return np.load(file_path)\n\n    def has_duplicates(self, column):\n        return not self.data[column].is_unique\n\n    def describe_data(self):\n        print(\"Number of Data Samples: {}\".format(self.n_rows))\n        print(\"Number of Columns: {}\".format(self.n_cols))\n        # Added on need to basis\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class SequenceAnalysis:\n    @staticmethod\n    def get_base_pairs_indices(structure):\n        \"\"\"\n        takes the structure of the sequence and finds the indexes of matching brackets\n        :param structure: str\n        :return: list of tuples indicating start and end positions of matching brackets Ex: [(x,y), (x2,y2)]\n        \"\"\"\n        paired_bases = list()\n        stack = list()\n        struct_len = len(structure)\n        # Get the first occurrence of the left bracket\n        i = 0\n        while i < struct_len:\n            if structure[i] == '(':\n                stack.append(i)\n                break\n            i += 1\n\n        # Move to the next char\n        i += 1\n        while i < struct_len:\n            if structure[i] == '(':\n                stack.append(i)\n            elif structure[i] == ')':\n                paired_bases.append((stack.pop(), i))\n            else:\n                # Ignore the '.' while looking for matching base pairs\n                pass\n            i += 1\n\n        # stack is empty at the end if the above loop has no bugs\n        if len(stack) > 0:\n            raise AssertionError(\"Some bug while finding matching base pairs\")\n\n        # sort based on the first index on tuple\n        paired_bases.sort(key=lambda x: x[0])\n        return paired_bases\n\n    @staticmethod\n    def base_pair_counts(sequence, structure, similar=True):\n        \"\"\"\n        accepts rna sequence re\n        :param similar:\n        :param structure:\n        :param sequence:\n        :return: list of tuples of format [(x, y, count)]\n        \"\"\"\n        if not isinstance(sequence, str):\n            raise TypeError('sequence is not of type \"str\"')\n\n        if not isinstance(structure, str):\n            raise TypeError('structure is not of type \"str\"')\n\n        if not isinstance(similar, bool):\n            raise TypeError('similar is not of type \"bool\"')\n\n        bp_indices = SequenceAnalysis.get_base_pairs_indices(structure=structure)\n\n        # convert indices to bases having entries (x, y) and (y,x) as different entries\n        base_pairs_unmerged = [(sequence[i], sequence[j]) for i, j in bp_indices]\n\n        bp_counts = Counter(base_pairs_unmerged)\n        base_pairs_unmerged_counts = [(bp, bp_counts[bp]) for bp in bp_counts]\n\n        if not similar:\n            return base_pairs_unmerged_counts\n\n        else:\n            bases_count = len(Definitions.nucleotides())\n            count_matrix = np.zeros(shape=(bases_count, bases_count), dtype = np.int64)\n            base2int_dict = {base: ind for ind, base in enumerate(Definitions.nucleotides().keys())}\n            int2base_dict = {num: base for base, num in base2int_dict.items()}\n\n\n            # when similar is set to True , (x, y) and (y, x) are treated identically\n            for i, item in enumerate(base_pairs_unmerged_counts):\n                if item[0][0] > item[0][1]:\n                    base_pairs_unmerged_counts[i] = ((item[0][1], item[0][0]), item[1])\n\n            base_pairs_unmerged_counts.sort(key=lambda x: x[0][0])\n\n            for base_pair, bp_count in base_pairs_unmerged_counts:\n                b1, b2 = base_pair[0], base_pair[1]\n                x = base2int_dict[b1]\n                y = base2int_dict[b2]\n                count_matrix[x][y] += bp_count\n                count_matrix[y][x] = count_matrix[x][y]\n\n            base_pairs_merged_counts = list()\n            i = j = 0\n            for i in range(bases_count):\n                j = i+1\n                while j < bases_count:\n                    base_pairs_merged_counts.append(((int2base_dict[i], int2base_dict[j]), count_matrix[i][j]))\n                    j += 1\n        \n            return base_pairs_merged_counts\n    \n    @staticmethod\n    def overall_base_pairs_count(sequences, structures, similar=True, percent=True):\n        if not isinstance(sequences, list):\n            return TypeError('sequences is not of type \"list\"')\n\n        if not isinstance(structures, list):\n            return TypeError('structures is not of type \"list\"')\n\n        total_bp_counts = list()\n\n        for sequence, structure in zip(sequences, structures):\n            total_bp_counts.extend(SequenceAnalysis.base_pair_counts(sequence, structure, similar=similar))\n\n        bp_count_dict = dict()\n\n        for bp, bp_count in total_bp_counts:\n            if bp in bp_count_dict:\n                bp_count_dict[bp] += bp_count\n            else:\n                bp_count_dict[bp] = bp_count\n\n        if not percent:\n            return bp_count_dict\n        else:\n            total_base_pairs = float(sum(bp_count_dict.values()))\n            bp_percents_dict = {bp: round((bp_count / total_base_pairs) * 100, 2) for bp, bp_count in bp_count_dict.items()}\n            return bp_percents_dict\n\n    @staticmethod\n    def base_distribution_by_position(sequences, seq_length, percent=True):\n        \"\"\"\n        Accepts a list of sequences and returns the probabilities of the bases at each position\n        :param sequences: list of strings\n        :return: list keys: position indexes values: dict ('base_key': 'base_count')\n        \"\"\"\n        total_sequences = len(sequences)\n        sequence_matrix = list()\n        distribution_dict = dict()\n        token_dict = {base: 0 for base in Definitions.nucleotides_tokens()}\n\n        for sequence in sequences:\n            sequence_matrix.append(list(sequence[:seq_length]))\n\n        # count the value counts at position i\n        for pos in range(seq_length):\n            distribution_dict[pos] = token_dict.copy()\n            bases = [base[pos] for base in sequence_matrix]\n            value_counts = Counter(bases)\n            for base in value_counts.keys():\n                if not percent:\n                    distribution_dict[pos][base] = round(float(value_counts[base])/total_sequences, 2)\n                else:\n                    distribution_dict[pos][base] = round((float(value_counts[base])/total_sequences) * 100, 2)\n\n        return distribution_dict\n\n    @staticmethod\n    def generate_base_pairing_probability(bpps_matrix):\n        \"\"\"\n        Accepts a numpy 2D bpps matrix and generates the paring probabilites at each position\n        It is as follows:\n            for a position i, sum all the probabilities other than position i\n        :param bpps_matrix:\n        :return: 1d array represing base pairing probabilities at position i\n        \"\"\"\n        if not isinstance(bpps_matrix, np.ndarray) or len(bpps_matrix.shape) != 2:\n            raise TypeError(\"Expected numpy 2d array\")\n\n        # check if the bpps matrix is valid by checking the symetry\n        is_valid = (bpps_matrix == bpps_matrix.transpose()).all()\n        if not is_valid:\n            raise ValueError(\"Expected Symmetric matrix\")\n\n        # base pairing itself probability is zero, assert the same\n        # check if all diagonal elements in the matrix are zeros\n        if (bpps_matrix.diagonal() != np.zeros(shape=bpps_matrix.shape[0])).all():\n            raise ValueError(\"Expected diagonal elements to be all zeros\")\n\n        base_pair_probabilities = np.zeros(shape=bpps_matrix.shape[0])\n        for index, row in enumerate(bpps_matrix):\n            base_pair_probabilities[index] = np.sum(row)\n\n        return base_pair_probabilities\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nclass OneHotEncoder:\n    token2int = dict()\n    int2token = dict()\n\n    def __init__(self, tokens):\n        if not isinstance(tokens, list):\n            raise TypeError('arg:tokens is not of type list')\n        self.tokens = tokens\n\n        for num, token in enumerate(self.tokens):\n            OneHotEncoder.token2int[token] = num + 1\n        # update int2token dictionary\n        OneHotEncoder.int2token = {token: val for token, val in OneHotEncoder.token2int.items()}\n\n    @staticmethod\n    def conv_token2int(word):\n            return [OneHotEncoder.token2int[char] for char in word]\n        \n            \n    @staticmethod\n    def conv_int2token(num_list, concat=True):\n        if concat:\n            return [OneHotEncoder.int2token[num] for num in num_list].join(\"\")\n        else:\n            return [OneHotEncoder.int2token[num] for num in num_list]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class PositionalEncoder:\n    @staticmethod\n    def encode(sequence):\n        \"\"\"\n        Implementation is loosely based on https://www.frontiersin.org/articles/10.3389/fgene.2019.00467/full\n                    #\n                    #  2, (if (Ri=A and Rj= U) or (Ri = U and Rj=A))\n         P(Ri,Rj) = #  3,   (if (Ri= G and Rj= C) or (Ri = C and Rj=G))\n                    #  x {0 < x < 2}  ,   (if (Ri= G and Rj= U) or (Ri= U and Rj=G))\n                    #  0   else\n                    #\n        :param sequence: string\n        :return: numpy 2d array of size (len(sequence) x len(sequence))\n        \"\"\"\n        if not isinstance(sequence, str):\n            raise TypeError(\"'sequence' is not of type 'str'\")\n\n        sequence_length = len(sequence)\n        s = sequence\n        weighted_matrix = np.zeros(shape=())\n        i = j = 0\n        for i in range(sequence_length):\n            j = i+1\n            while j < sequence_length:\n                if (s[i] == 'A' and s[j] == 'U') or (s[i] == 'U' and s[j] == 'A'):\n                    weighted_matrix[i][j] = weighted_matrix[j][i] = 2.\n                elif (s[i] == 'G' and s[j] == 'C') or (s[i] == 'C' and s[j] == 'G'):\n                    weighted_matrix[i][j] = weighted_matrix[j][i] = 3.\n                elif (s[i] == 'G' and s[j] == 'U') or (s[i] == 'U' and s[j] == 'G'):\n                    weighted_matrix[i][j] = weighted_matrix[j][i] = round(np.random.uniform(0, 2), 2)\n                else:\n                    # defaults to 0\n                    pass\n                # increment column index\n                j += 1\n        return weighted_matrix\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class HelperFunctions:\n    @staticmethod\n    def print_feature_property(col_name, d_type, length=None):\n        print('Feature: {} dtype: {} Length: {}'.format(col_name, d_type, length))\n\n    @staticmethod\n    def get_class_variables(class_name):\n        return [attr for attr in dir(class_name) if not callable(getattr(class_name, attr)) and not attr.startswith(\"__\")]\n    \n    \n    @staticmethod\n    def split_train_data(t_x, t_y, split_percent):\n        # split the train and test data\n        mask = np.random.rand(len(t_x)) < 0.8\n        train_x = t_x[mask]\n        train_y = t_y[mask]\n\n        test_x = t_x[~mask]\n        test_y = t_y[~mask]\n\n        return train_x, train_y, test_x, test_y\n\n\n\n    @staticmethod\n    def keys_as_positions_to_labels(positional_values_dict):\n        \"\"\"\n        Accepts a dictionary having positions as keys and values of multiple labels in value dict and converts them\n        to a dict having labels in the keys and positional values in the array\n        :return:\n        \"\"\"\n        if not isinstance(positional_values_dict, dict):\n            raise TypeError('arg: positional_values_dict is not of type dict')\n\n        # convert the dict of dict to arrays\n        values_dict = dict()\n        for token in Definitions.nucleotides_tokens():\n            values_dict[token] = list()\n\n        for pos, value_dict in positional_values_dict.items():\n            for base, base_val in value_dict.items():\n                values_dict[base].append(base_val)\n\n        return values_dict\n    \n    @staticmethod\n    def plot_multi_line_positions_to_values_graph(positions, values_dict, ylabel='Percentage', title=None, fig_size=(25, 8)):\n        # plot all the distributions on the same graph\n        plt.figure(figsize=fig_size)\n        if values_dict.keys()  == ['A', 'U', 'C', 'G']:\n            order = ['A', 'U', 'C', 'G']\n            for key in order:\n                plt.plot(positions, values_dict[key], label=key)\n        else:\n            for key in values_dict.keys():\n                plt.plot(positions, values_dict[key], label=key)\n                \n        plt.xlabel('Position of RNA Seq')\n        plt.ylabel(ylabel=ylabel)\n        plt.xticks(positions)\n        plt.title(title)\n        plt.legend(loc='upper right')\n\n    @staticmethod\n    def plot_stacked_bar_positions_to_values_graph(positions, values_dict, ylabel='Percentage', title=None, fig_size=(25, 10)):\n        # plot all the distributions in stacked bar graph\n        plt.figure(figsize=fig_size)\n        pa = plt.bar(positions, values_dict['A'], label='A')\n        pg = plt.bar(positions, values_dict['G'], label='G', bottom=np.array(values_dict['A']))\n        pc = plt.bar(positions, values_dict['C'], label='C', bottom=(np.array(values_dict['A']) + np.array(values_dict['G'])))\n        pu = plt.bar(positions, values_dict['U'], label='U',\n                     bottom=(np.array(values_dict['A']) + np.array(values_dict['G']) + np.array(values_dict['C'])))\n        plt.xlabel('Position of RNA Seq')\n        plt.ylabel(ylabel=ylabel)\n        plt.xticks(positions)\n        plt.title(title)\n        plt.legend((pa[0], pg[0], pc[0], pu[0]), ('A', 'G', 'C', 'U'), loc='upper right')\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Exploratory Data Analysis  "},{"metadata":{},"cell_type":"markdown","source":"Get the shapes as well as look for possible duplicates in training and test data "},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"# create instance of training data and test data \n\ntraining_instance = Data(data_path=train_data_path, bpps_path= bpps_path)\ntest_instance = Data(data_path=test_data_path)\n\n# Exploratory data analysis for training data\n\nprint(\"In training data:\")\ntraining_instance.describe_data()\n\n# check for duplicate ids in training data\nprint('In training data: Duplicates found: {}'.format(training_instance.has_duplicates(column=Columns.id)))\n\n# Exploratory data analysis for test data\n\nprint(\"In test data\")\ntest_instance.describe_data()\n\n# check for duplicate ids in training data\nprint('In test data: Duplicates found: {}'.format(training_instance.has_duplicates(column=Columns.id)))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Analyse the signal to noise ratio "},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data = training_instance.data\n\n# Analyze the quality of the data for training instance\nprint(\"Training samples with (SN_ratio > 1): {}\".format(train_data[train_data[Columns.signal_to_noise] > 1].shape[0]))\nprint(\"Training samples with (SN_ratio = 1): {}\".format(train_data[train_data[Columns.signal_to_noise] == 1].shape[0]))\nprint(\"Training samples with (SN_ratio < 1): {}\".format(train_data[train_data[Columns.signal_to_noise] < 1].shape[0]))\n\ntraining_instance.noisy_data = train_data[train_data[Columns.signal_to_noise] < 1]\ntraining_instance.data = train_data[train_data[Columns.signal_to_noise] > 1]\ntraining_instance.data = training_instance.data.reset_index(drop=True)\n\n# Analyze data with signal_to_noise > 1 and with SN_filter\nprint(\"Training samples with (SN_ratio > 1) and (SN_filter =1) : {}\".format((training_instance.data[training_instance.data[Columns.SN_filter] ==1 ].shape[0])))\nprint(\"Training samples with (SN_ratio > 1) and (SN_filter =0) : {}\".format((training_instance.data[training_instance.data[Columns.SN_filter] ==0 ].shape[0])))\n                                            \n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Sample a point from the training data and validate if the base pair probability matrix is symmetric or not "},{"metadata":{"trusted":true},"cell_type":"code","source":"# get the sample from training data\nsample_train = training_instance.get_sample()\n\n# get the coresponding bpps matrix\nbpps_matrix = training_instance.get_bpps_matrix(id=sample_train[Columns.id].item())\n\n# shape of bpps matrix\nprint('shape:{}'.format(bpps_matrix.shape))\n\n\n# Check if the matrix is symmetric\nprint('Matrix is symmetric: {}'.format((bpps_matrix  == bpps_matrix.transpose()).all()))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Get all the column names, data types and the array length if the value of each feature is a list"},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_names = HelperFunctions.get_class_variables(Columns)\nfor feature in feature_names:\n    feature_value = sample_train[feature].item()\n    try:\n        length = len(feature_value)\n    except TypeError:\n        length = None\n\n    HelperFunctions.print_feature_property(feature, type(feature_value), length=length)\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Understand the sequence lengths in training and test data "},{"metadata":{"trusted":true},"cell_type":"code","source":"# get unique different sequence lengths in train data\nprint('Training data unique sequence lengths: {}'.format(training_instance.data[Columns.seq_length].value_counts()))\n\n# get unique different sequence lengths in test data\nprint('Test data unique sequence lengths: {}'.format(test_instance.data[Columns.seq_length].value_counts()))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Relation between bpps matrix and reactivity\ntrain_sample = training_instance.get_sample()\nreactivity_sample = train_sample[Columns.reactivity].item()\nbpps_matrix_sample = training_instance.get_bpps_matrix(id=train_sample[Columns.id].item())\npairing_probabilities = SequenceAnalysis.generate_base_pairing_probability(bpps_matrix_sample)[:68]\npositions = np.arange(0, len(pairing_probabilities))\nHelperFunctions.plot_multi_line_positions_to_values_graph(positions=positions, values_dict= {'bpps': pairing_probabilities, 'reactivity': reactivity_sample},\n                                                          ylabel='values', title='Compare Reactivity with Base pair probabilities for Random Sample')\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# convert reactivities to matrices\nreactivity_matrix = np.array(training_instance.data[Columns.reactivity].tolist(), dtype=np.float)\ndeg_50C_matrix = np.array(training_instance.data[Columns.deg_50C].tolist(), dtype=np.float)\ndeg_Mg_50C_matrix = np.array(training_instance.data[Columns.deg_Mg_50C].tolist(), dtype=np.float)\ndeg_pH10_matrix = np.array(training_instance.data[Columns.deg_pH10].tolist(), dtype=np.float)\ndeg_Mg_pH10_matrix = np.array(training_instance.data[Columns.deg_Mg_pH10].tolist(), dtype=np.float)\n\n# convert reactivity errors to matrices\nreactivity_error_matrix = np.array(training_instance.data[Columns.reactivity_error].tolist(), dtype=np.float)\ndeg_50C_error_matrix = np.array(training_instance.data[Columns.deg_error_50C].tolist(), dtype=np.float)\ndeg_Mg_50C_error_matrix = np.array(training_instance.data[Columns.deg_error_Mg_50C].tolist(), dtype=np.float)\ndeg_pH10_error_matrix = np.array(training_instance.data[Columns.deg_error_pH10].tolist(), dtype=np.float)\ndeg_Mg_pH10_error_matrix = np.array(training_instance.data[Columns.deg_error_Mg_pH10].tolist(), dtype=np.float)\n\n# Add errors observed in reactivities to the experimental values\n# Note that error = accepted_value - experimental value\n\nreactivity_matrix = reactivity_matrix + np.negative(reactivity_error_matrix)\ndeg_50C_matrix = deg_50C_matrix + np.negative(deg_50C_error_matrix)\ndeg_Mg_50C_matrix = deg_Mg_50C_matrix + np.negative(deg_Mg_50C_error_matrix)\ndeg_pH10_matrix = deg_pH10_matrix + np.negative(deg_pH10_error_matrix)\ndeg_Mg_pH10_matrix = deg_Mg_pH10_matrix + np.negative(deg_Mg_pH10_error_matrix)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# compare mean reactivity and mean base pairing at every position \n\nmean_reactivity = reactivity_matrix.mean(axis=0)\npairing_probability_matrix = list()\nfor uid in training_instance.data[Columns.id]:\n    bpps_matrix = training_instance.get_bpps_matrix(id=train_sample[Columns.id].item())\n    pairing_probabilities = SequenceAnalysis.generate_base_pairing_probability(bpps_matrix_sample)[:68]\n    pairing_probability_matrix.append(pairing_probabilities)\n\npairing_probability_matrix = np.array(pairing_probability_matrix)\nmean_pairing_probability = pairing_probability_matrix.mean(axis=0)\npositions = np.arange(0, len(mean_pairing_probability))\n\nHelperFunctions.plot_multi_line_positions_to_values_graph(positions=positions, values_dict= {'mean_bpps': mean_pairing_probability, 'reactivity': mean_reactivity},\n                                                          ylabel='values', title='Compare Mean Reactivity with Mean Base pair probabilities for Training Data')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Correlation between mean reactivity and mean pairing probability \n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Find the correlation between mean reactivity and bpps \n\ndf = pd.DataFrame(data={'mean_reactivity': mean_reactivity, 'mean_pair_prob': mean_pairing_probability})\npositions = np.arange(0, len(mean_reactivity))\ndf['mean_reactivity'].corr(df['mean_pair_prob'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Mean Reactivity and Mean Pairing probabilities are slightly negatively correlated. So, pairing decreases the overall reactivity of the RNA, Thus increasing the stability of the RNA. However, the problem statement went ahead and needs stability at each position. "},{"metadata":{},"cell_type":"markdown","source":"# Position wise Correlation b/w reactivity and base pair probabilities"},{"metadata":{"trusted":true},"cell_type":"code","source":"pairing_probability_df = pd.DataFrame(data=pairing_probability_matrix)\nreactivity_df = pd.DataFrame(data=reactivity_matrix)\ncorr = pairing_probability_df.corrwith(reactivity_df, axis=0).to_list()\npositions = np.arange(0, len(corr))\nHelperFunctions.plot_multi_line_positions_to_values_graph(positions=positions, values_dict={'corr': corr}, ylabel='corr', title='Position wise correlation between reactivity and pairing probabilities')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There seems to be less correlation at the position wise as well for the reactivity and base pairing probabilities. "},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_structure = train_sample[Columns.structure].item()\nbase_pairs_indices = SequenceAnalysis.get_base_pairs_indices(structure=sample_structure)\nprint(sample_structure)\nprint(base_pairs_indices)\nprint(len(base_pairs_indices))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"base_pairs_counts = SequenceAnalysis.base_pair_counts(sequence=train_sample[Columns.sequence].item(), structure=sample_structure, similar=False)\nprint('Base pair counts considering symmetry')\nprint(base_pairs_counts)\n\nbase_pairs_counts = SequenceAnalysis.base_pair_counts(train_sample[Columns.sequence].item(), sample_structure)\nprint('Base pair counts with out considering symmetry')\nprint(base_pairs_counts)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Base pair counts for the training data\nsequence_list = training_instance.data[Columns.sequence].tolist()\nstructure_list = training_instance.data[Columns.structure].tolist()\ntraining_data_bp_percents = SequenceAnalysis.overall_base_pairs_count(sequence_list, structure_list)\n\n# get 500 random sequences and corresponding structures\n# generate 500 random indexes from train data\nrandom_indexes = np.random.randint(low=0, high=training_instance.data.shape[0], size=10)\nr_sequences = training_instance.data.loc[random_indexes, Columns.sequence].tolist()\nr_structures = training_instance.data.loc[random_indexes, Columns.structure].tolist()\nrandom_train_bp_percents = SequenceAnalysis.overall_base_pairs_count(r_sequences, r_structures)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Effect of Magnesium on the Reactivities at Different Experimental Conditions "},{"metadata":{},"cell_type":"raw","source":"convert the features having lists of lists to the matrices and add the errors to the experimental observations to get a good theoritical estimate."},{"metadata":{"trusted":true},"cell_type":"code","source":"### Relation between reactivity values before and after incubating with MG\na4_dims = (20, 12)\nfig, axs = plt.subplots(2, 2, figsize=a4_dims)\npos_array = np.arange(68)\n\n# Mean reactivities before and after incubating with Mg at 50C\naxs[0][0].plot(pos_array, deg_50C_matrix.mean(axis=0), color='green', label='before')\naxs[0][0].plot(pos_array, deg_Mg_50C_matrix.mean(axis=0), color='red', label='after')\naxs[0][0].set_xlabel('Position of RNA Seq')\naxs[0][0].set_ylabel('Reactivities')\naxs[0][0].set_title('Mean Reactivity at 50C before and after incubating with Mg')\naxs[0][0].legend(loc='upper right')\n\n# Mean reactivities before and after incubating with Mg at pH10\naxs[0][1].plot(pos_array, deg_pH10_matrix.mean(axis=0), color='green', label='before')\naxs[0][1].plot(pos_array, deg_Mg_pH10_matrix.mean(axis=0), color='red', label='after')\naxs[0][1].set_xlabel('Position of RNA Seq')\naxs[0][1].set_ylabel('Reactivities')\naxs[0][1].set_title('Mean Reactivity at pH10 before and after incubating with Mg')\naxs[0][1].legend(loc='upper right')\n\n# Relation between mean reactivity, reactivity_50C , reactivity at ph10 before incubating with Mg\naxs[1][0].plot(pos_array, reactivity_matrix.mean(axis=0), color='green', label='experimental')\naxs[1][0].plot(pos_array, deg_Mg_50C_matrix.mean(axis=0), color='red', label='50C')\naxs[1][0].plot(pos_array, deg_Mg_pH10_matrix.mean(axis=0), color='blue', label='pH10')\naxs[1][0].set_xlabel('Position of RNA Seq')\naxs[1][0].set_ylabel('Reactivities')\naxs[1][0].set_title('Mean Reactivity Comparison at Different Experimental conditions before incubating with Mg')\naxs[1][0].legend(loc='upper right')\n\n# Relation between mean reactivity, reactivity_50C , reactivity at ph10 after incubating with Mg\naxs[1][1].plot(pos_array, reactivity_matrix.mean(axis=0), color='green', label='experimental')\naxs[1][1].plot(pos_array, deg_50C_matrix.mean(axis=0), color='red', label='Mg_50C')\naxs[1][1].plot(pos_array, deg_pH10_matrix.mean(axis=0), color='blue', label='Mg_pH10')\naxs[1][1].set_xlabel('Position of RNA Seq')\naxs[1][1].set_ylabel('Reactivities')\naxs[1][1].set_title('Mean Reactivity Comparison at Different Experimental conditions after incubating with Mg')\naxs[1][1].legend(loc='upper right')\n\nplt.figure(figsize=(25, 8))\nplt.plot(pos_array, np.exp(deg_Mg_50C_matrix.mean(axis=0) - deg_50C_matrix.mean(axis=0)), color='red', label='50C')\nplt.plot(pos_array, np.exp(deg_Mg_pH10_matrix.mean(axis=0) - deg_pH10_matrix.mean(axis=0)), color = 'blue', label= 'pH10')\nplt.xlabel('Position of RNA Seq')\nplt.ylabel('Relative Degradation scaled with exp')\nplt.title('Effect of Magnesium on LikelyHood Degradation')\nplt.legend(loc='upper right')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Understand the Mean Probability Distributions of Each Base at Every Position"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get sequences  distribution by position\nbase_percentages = SequenceAnalysis.base_distribution_by_position(sequences=sequence_list, seq_length=67, percent=True)\nvalues_dict = HelperFunctions.keys_as_positions_to_labels(base_percentages)\npositions = list(base_percentages.keys())\n\nHelperFunctions.plot_multi_line_positions_to_values_graph(positions, values_dict, title='Train Data: Overall percentage of Base at Position i')\nHelperFunctions.plot_stacked_bar_positions_to_values_graph(positions, values_dict, title='Train Data: Overall Percentage of Each Base at Position i')\n\npublic_test_data = test_instance.data[test_instance.data[Columns.seq_length] == 107]\nprivate_test_data = test_instance.data[test_instance.data[Columns.seq_length] == 130]\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Public Data Mean Probability Distribution"},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot for public test data \npublic_test_data_bp_percents = SequenceAnalysis.base_distribution_by_position(sequences=public_test_data[Columns.sequence], seq_length=67, percent=True)\npub_test_positions = list(public_test_data_bp_percents.keys())\npub_test_values_dict = HelperFunctions.keys_as_positions_to_labels(positional_values_dict=public_test_data_bp_percents)\nHelperFunctions.plot_multi_line_positions_to_values_graph(pub_test_positions, pub_test_values_dict, title='Public Test: Overall percentage of Base at Position i')\nHelperFunctions.plot_stacked_bar_positions_to_values_graph(pub_test_positions, pub_test_values_dict, title='Public Test: Overall Percentage of Each Base at Position i')\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot for private test data\nprivate_test_data_bp_percents = SequenceAnalysis.base_distribution_by_position(sequences=private_test_data[Columns.sequence], seq_length=91, percent=True)\npriv_test_positions = list(private_test_data_bp_percents.keys())\npriv_test_values_dict = HelperFunctions.keys_as_positions_to_labels(positional_values_dict=private_test_data_bp_percents)\nHelperFunctions.plot_multi_line_positions_to_values_graph(priv_test_positions, priv_test_values_dict, title='Private Test:Overall percentage of Base at Position i')\nHelperFunctions.plot_stacked_bar_positions_to_values_graph(priv_test_positions, priv_test_values_dict, title='Private Test: Overall Percentage of Each Base at Position i')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Modelling"},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers as L\n\ntarget_columns = Submission.get_submission_columns()\none_hot_encoder = OneHotEncoder(tokens=Definitions.tokens())\n\n# encoding for train data\ntraining_instance.data['seq_encoded'] = training_instance.data[Columns.sequence].apply(lambda x: OneHotEncoder.conv_token2int(x)).tolist()\ntraining_instance.data['loop_encoded'] = training_instance.data[Columns.predicted_loop_type].apply(lambda x: OneHotEncoder.conv_token2int(x)).tolist()\n\n# encoding for public test\npublic_test_data['seq_encoded'] = public_test_data[Columns.sequence].apply(lambda x: OneHotEncoder.conv_token2int(x)).tolist()\npublic_test_data['loop_encoded'] = public_test_data[Columns.predicted_loop_type].apply(lambda x: OneHotEncoder.conv_token2int(x)).tolist()\n\n# encoding for private test\nprivate_test_data['seq_encoded'] = private_test_data[Columns.sequence].apply(lambda x: OneHotEncoder.conv_token2int(x)).tolist()\nprivate_test_data['loop_encoded'] = private_test_data[Columns.predicted_loop_type].apply(lambda x: OneHotEncoder.conv_token2int(x)).tolist()\n\ntrain_x = pd.DataFrame(list(zip(training_instance.data['seq_encoded'], training_instance.data['loop_encoded'])), columns= ['seq_encoded', 'loop_encoded'])\ntrain_y = training_instance.data[Submission.get_submission_columns(optional=True) ]\n\ntrain_x, train_y, validation_x, validation_y = HelperFunctions.split_train_data(train_x, train_y, split_percent=0.8)\n\n\ndef preprocess_inputs(df, cols=['seq_encoded', 'loop_encoded']):\n    base_fea = np.transpose(\n        np.array(\n            df[cols]\n            .applymap(lambda x: np.asarray(x).astype(np.float))\n            .values\n            .tolist()\n        ),\n        (0, 2, 1)\n    )\n    return base_fea\n\n\ndef preprocess_outputs(df, cols=Submission.get_submission_columns(optional=True)):\n    return np.transpose(\n        np.array(\n            df[cols]\n            .applymap(lambda x: np.asarray(x).astype(np.float))\n            .values\n            .tolist()\n        ),\n        (0, 2, 1)\n    )\n\ntrain_x = preprocess_inputs(train_x)\nvalidation_x = preprocess_inputs(validation_x)\n\ntrain_y = preprocess_outputs(train_y)\nvalidation_y = preprocess_outputs(validation_y)\n\nprint('...............')\nprint(train_x.shape)\nprint(train_y.shape)\nprint(validation_x.shape)\nprint(validation_y.shape)\n#train_x.loc[:, 'seq_encoded'] = train_x.seq_encoded.apply(lambda x: np.asarray(x[:67]).astype(np.float))\n\n#train_x.loc[:, 'loop_encoded'] = train_x.loop_encoded.apply(lambda x: np.transpose(np.asarray(x[:67]).astype(np.float), (0, 2, 1)))\n\n#validation_x.loc[:, 'seq_encoded'] = validation_x.seq_encoded.apply(lambda x: np.asarray(x[:67]).astype(np.float))\n\n#validation_x.loc[:, 'loop_encoded'] = validation_x.loop_encoded.apply(lambda x: np.asarray(x[:67]).astype(np.float))\n\n\ndef MCRMSE(y_true, y_pred):\n    colwise_mse = tf.reduce_mean(tf.square(y_true - y_pred), axis=1)\n    return tf.reduce_mean(tf.sqrt(colwise_mse), axis=1)\n\n\n\ndef gru_layer(hidden_dim, dropout):\n    return L.Bidirectional(L.GRU(\n        hidden_dim, dropout=dropout, return_sequences=True, kernel_initializer='orthogonal'))\n\n\ndef build_model(embed_size, seq_len=107, pred_len=68, dropout=0.5,\n                sp_dropout=0.2, embed_dim=15, hidden_dim=512, n_layers=7):\n    inputs = L.Input(shape=(seq_len, 2))\n    embed = L.Embedding(input_dim=embed_size, output_dim=embed_dim)(inputs)\n\n    reshaped = tf.reshape(\n        embed, shape=(-1, embed.shape[1], embed.shape[2] * embed.shape[3])\n    )\n    hidden = L.SpatialDropout1D(sp_dropout)(reshaped)\n\n    for x in range(n_layers):\n        hidden = gru_layer(hidden_dim, dropout)(hidden)\n\n    # Since we are only making predictions on the first part of each sequence,\n    # we have to truncate it\n    truncated = hidden[:, :pred_len]\n    out = L.Dense(5, activation='linear')(truncated)\n\n    model = tf.keras.Model(inputs=inputs, outputs=out)\n    model.compile(tf.optimizers.Adam(), loss=MCRMSE)\n\n    return model\n\n# detect and init the TPU\ntpu = tf.distribute.cluster_resolver.TPUClusterResolver()\ntf.config.experimental_connect_to_cluster(tpu)\ntf.tpu.experimental.initialize_tpu_system(tpu)\n\n# instantiate a distribution strategy\ntpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)\n\n# instantiating the model in the strategy scope creates the model on the TPU\nwith tpu_strategy.scope():\n    model = build_model(embed_size=len(OneHotEncoder.token2int) + 1)\n\n    model.summary()\n\ncheckpoint_path = 'vaccine.h5'\nnum_epochs = 100\nes_callback = keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=3)\n\ntrain_history = model.fit(train_x, train_y, validation_data=(validation_x, validation_y),  epochs=num_epochs, verbose=2,\n                    callbacks= [ tf.keras.callbacks.ReduceLROnPlateau(patience=5),\n                                tf.keras.callbacks.ModelCheckpoint(checkpoint_path)\n                    ])\n\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_positions = pd.DataFrame(data=np.arange(num_epochs), columns=['epochs'])\nhistory_df = pd.DataFrame(data=train_history.history)\nhistory_df = history_df.join(x_positions)\nsns.lineplot(x=\"epochs\", y='value', hue='variable', data=pd.melt(history_df, [\"epochs\"]))\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_public = build_model(seq_len=107, pred_len=107, embed_size=len(OneHotEncoder.token2int) + 1)\nmodel_private = build_model(seq_len=130, pred_len=130, embed_size=len(OneHotEncoder.token2int) + 1)\n\nmodel_public.load_weights(checkpoint_path)\nmodel_private.load_weights(checkpoint_path)\n\npublic_test_data = preprocess_inputs(public_test_data)\nprivate_test_data = preprocess_inputs(private_test_data)\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"public_preds = model_public.predict(public_test_data)\nprivate_preds = model_private.predict(private_test_data)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\npublic_test_ids = test_instance.data[test_instance.data[Columns.seq_length] == 107].id.tolist()\nprivate_test_ids = test_instance.data[test_instance.data[Columns.seq_length] == 130].id.tolist()\nprint(len(public_test_ids))\nprint(len(private_test_ids))\nprint(len(public_test_ids) + len(private_test_ids) )\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def generate_id_positions(ids, pos_len):\n    positions = np.arange(0, pos_len)\n    return np.array([ id + '_' + str(pos) for id in ids for pos in positions])\n\npublic_preds = public_preds.reshape(-1, 5)\nprivate_preds = private_preds.reshape(-1, 5)\n\npub_test_id_seq_posdf =  pd.DataFrame(data = generate_id_positions(public_test_ids, 107), columns = [SubmissionColumns.id_seqpos])\npriv_test_ids_seq_posdf = pd.DataFrame(data = generate_id_positions(private_test_ids, 130), columns = [SubmissionColumns.id_seqpos])\n\npublic_preds_df = pd.DataFrame(data= public_preds, columns=Submission.get_submission_columns(optional=True))\nprivate_preds_df = pd.DataFrame(data= private_preds, columns=Submission.get_submission_columns(optional=True))\n\nsubmission_pub_data = pd.concat([pub_test_id_seq_posdf, public_preds_df], axis=1)\nsubmission_priv_data = pd.concat([priv_test_ids_seq_posdf, private_preds_df], axis=1)\n\n\nsubmission_df = pd.concat([submission_pub_data, submission_priv_data], axis=0)\nsubmission_df.to_csv('submission.csv', index=False)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}