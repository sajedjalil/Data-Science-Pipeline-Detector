{"cells":[{"metadata":{},"cell_type":"markdown","source":"<a class=\"anchor\" id=\"0\"></a>\n# [OpenVaccine: COVID-19 mRNA Vaccine Degradation Prediction](https://www.kaggle.com/c/stanford-covid-vaccine)"},{"metadata":{},"cell_type":"markdown","source":"## UPDATE:\nIn blending portion, earlier only three values were computed and rest two were filled with dummy values. However currently al 5 values need to be predicted. So change accordingly."},{"metadata":{},"cell_type":"markdown","source":"Current blending:"},{"metadata":{"trusted":true},"cell_type":"code","source":"#blend_preds_df['reactivity'] = ensemble_final['reactivity'] \n#blend_preds_df['deg_Mg_pH10'] = ensemble_final['deg_Mg_pH10']\n#blend_preds_df['deg_pH10'] = ensemble_final['deg_Mg_pH10']\n#blend_preds_df['deg_Mg_50C'] = ensemble_final['deg_Mg_50C']\n#blend_preds_df['deg_50C'] = ensemble_final['deg_Mg_50C']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"To be corrected to:"},{"metadata":{"trusted":true},"cell_type":"code","source":"#blend_preds_df['reactivity'] = ensemble_final['reactivity'] \n#blend_preds_df['deg_Mg_pH10'] = ensemble_final['deg_Mg_pH10']\n#blend_preds_df['deg_pH10'] = ensemble_final['deg_pH10']\n#blend_preds_df['deg_Mg_50C'] = ensemble_final['deg_Mg_50C']\n#blend_preds_df['deg_50C'] = ensemble_final['deg_50C']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This notebook is based on a fork from [notebook](https://www.kaggle.com/vbmokin/gru-lstm-mix-custom-loss-tuning-by-3d-visual). I have replaced the existing models and added encoder portion of a transformer. Still needs tuning but it works. Hope you can make good use of it. Since my GPU quota for this week is over, has implemented for TPU as well. It should work fine with TPU, GPU or CPU."},{"metadata":{},"cell_type":"markdown","source":"## 1. Import libraries"},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"import os \nimport sys\nimport json\nimport math\nimport random\nimport numpy as np\nimport pandas as pd\nimport gc\nfrom tqdm import tqdm\n\nimport matplotlib.pyplot as plt \nimport seaborn as sns\nimport plotly.express as px\nimport plotly.graph_objects as go\n\nfrom sklearn.model_selection import train_test_split, KFold,  StratifiedKFold\n\nimport tensorflow as tf\nimport tensorflow_addons as tfa\nimport tensorflow.keras.backend as K\nimport tensorflow.keras.layers as L\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"seed = 42","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"DEVICE = \"TPU\"\nif DEVICE == \"TPU\":\n    print(\"connecting to TPU...\")\n    try:\n        tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n        print('Running on TPU ', tpu.master())\n    except ValueError:\n        print(\"Could not connect to TPU\")\n        tpu = None\n\n    if tpu:\n        try:\n            print(\"initializing  TPU ...\")\n            tf.config.experimental_connect_to_cluster(tpu)\n            tf.tpu.experimental.initialize_tpu_system(tpu)\n            strategy = tf.distribute.experimental.TPUStrategy(tpu)\n            print(\"TPU initialized\")\n        except _:\n            print(\"failed to initialize TPU\")\n    else:\n        DEVICE = \"GPU\"\n\nif DEVICE != \"TPU\":\n    #if tf.config.list_physical_devices('gpu'):\n    #    strategy = tf.distribute.MirroredStrategy()#if using multiple gpu\n    #else:  # use default strategy\n    strategy = tf.distribute.get_strategy()\n\nif DEVICE == \"GPU\":\n    print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))  \nprint('Number of devices: {}'.format(strategy.num_replicas_in_sync))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dropout_model = 0.36\nhidden_dim_first = 128\nhidden_dim_second = 256\nhidden_dim_third = 128","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2. Read & Process Datasets"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Download datasets\ntrain = pd.read_json('/kaggle/input/stanford-covid-vaccine/train.json', lines=True)\ntest = pd.read_json('/kaggle/input/stanford-covid-vaccine/test.json', lines=True)\nsample_sub = pd.read_csv(\"/kaggle/input/stanford-covid-vaccine/sample_submission.csv\")\ntrain = train[train.signal_to_noise > 1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Target columns \ntarget_cols = ['reactivity', 'deg_Mg_pH10', 'deg_pH10', 'deg_Mg_50C', 'deg_50C']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"token2int = {x:i for i, x in enumerate('().ACGUBEHIMSX')}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def preprocess_inputs(df, cols=['sequence','predicted_loop_type','structure']):\n    base_features = np.transpose(\n        np.array(\n            df[cols]\n            .applymap(lambda seq: [token2int[x] for x in seq])\n            .values\n            .tolist()\n        ),\n        (0, 2, 1)\n    )\n    return base_features\n\ntrain_inputs_all = preprocess_inputs(train)\ntrain_labels_all = np.array(train[target_cols].values.tolist()).transpose((0, 2, 1))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3. Transformer Encoder Model Implementation, Training & Prediction"},{"metadata":{"trusted":true},"cell_type":"code","source":"def scaled_dot_product_attention(q, k, v, mask):\n    \"\"\"Calculate the attention weights.\n    q, k, v must have matching leading dimensions.\n    k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n    The mask has different shapes depending on its type(padding or look ahead) \n    but it must be broadcastable for addition.\n  \n    Args:\n      q: query shape == (..., seq_len_q, depth)\n      k: key shape == (..., seq_len_k, depth)\n      v: value shape == (..., seq_len_v, depth_v)\n      mask: Float tensor with shape broadcastable \n            to (..., seq_len_q, seq_len_k). Defaults to None.\n    \n    Returns:\n     output, attention_weights\n    \"\"\"\n\n    matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n  \n    # scale matmul_qk\n    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n\n    # add the mask to the scaled tensor.\n    if mask is not None:\n        scaled_attention_logits += (mask * -1e9)  \n\n    # softmax is normalized on the last axis (seq_len_k) so that the scores\n    # add up to 1.\n    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n\n    output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n\n    return output, attention_weights\n\nclass MultiHeadAttention(tf.keras.layers.Layer):\n    def __init__(self, d_model, num_heads):\n        super(MultiHeadAttention, self).__init__()\n        self.num_heads = num_heads\n        self.d_model = d_model\n\n        assert d_model % self.num_heads == 0\n\n        self.depth = d_model // self.num_heads\n\n        self.wq = tf.keras.layers.Dense(d_model)\n        self.wk = tf.keras.layers.Dense(d_model)\n        self.wv = tf.keras.layers.Dense(d_model)\n\n        self.dense = tf.keras.layers.Dense(d_model)\n        \n    def split_heads(self, x, batch_size):\n        \"\"\"Split the last dimension into (num_heads, depth).\n        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n        \"\"\"\n        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n        return tf.transpose(x, perm=[0, 2, 1, 3])\n    \n    def call(self, v, k, q, mask):\n        batch_size = tf.shape(q)[0]\n\n        q = self.wq(q)  # (batch_size, seq_len, d_model)\n        k = self.wk(k)  # (batch_size, seq_len, d_model)\n        v = self.wv(v)  # (batch_size, seq_len, d_model)\n\n        q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n        k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n        v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n\n        # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n        # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n        scaled_attention, attention_weights = scaled_dot_product_attention(\n            q, k, v, mask)\n\n        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n\n        concat_attention = tf.reshape(scaled_attention, \n                                      (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n\n        output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n\n        return output, attention_weights\n    \n    def get_config(self):\n\n        config = super().get_config().copy()\n        config.update({\n            'depth': self.depth,\n            'wq': self.wq,\n            'qk': self.wk,\n            'wv': self.wv,\n            'dense': self.dense,\n        })\n        \n        return config\ndef point_wise_feed_forward_network(d_model, dff):\n      return tf.keras.Sequential([\n      tf.keras.layers.Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)\n      tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)\n  ])\n\nclass EncoderLayer(tf.keras.layers.Layer):\n    def __init__(self, d_model, num_heads, dff, rate=0.1):\n        super(EncoderLayer, self).__init__()\n        self.d_model = d_model\n        self.num_heads = num_heads\n        self.dff = dff\n        self.rate = rate\n        \n        self.mha = MultiHeadAttention(d_model, num_heads)\n        self.ffn = point_wise_feed_forward_network(d_model, dff)\n\n        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n\n        self.dropout1 = tf.keras.layers.Dropout(rate)\n        self.dropout2 = tf.keras.layers.Dropout(rate)\n    \n    def call(self, x, training):\n        #mask made None\n        attn_output, _ = self.mha(x, x, x, None)  # (batch_size, input_seq_len, d_model)\n        attn_output = self.dropout1(attn_output, training=training)\n        out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n\n        ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n        ffn_output = self.dropout2(ffn_output, training=training)\n        out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n\n        return out2\n\n    def get_config(self):\n\n        config = super().get_config().copy()\n        config.update({\n            'num_heads': self.num_heads,\n            'rate': self.rate,\n            'd_model': self.d_model,\n            'num_heads': self.num_heads,\n            'dropout1': self.dropout1,\n            'dropout2': self.dropout2,\n            'layernorm1': self.layernorm1,\n            'layernorm2': self.layernorm2,\n            'mha': self.mha,\n            'ffn': self.ffn,\n        })\n        return config","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def MCRMSE(y_true, y_pred):\n    colwise_mse = tf.reduce_mean(tf.square(y_true - y_pred), axis=1)\n    return tf.reduce_mean(tf.sqrt(colwise_mse), axis=1)\n\ndef build_model(model_type=1, seq_len=107, pred_len=68, embed_dim=32, \n                dropout=dropout_model, hidden_dim_first = hidden_dim_first, \n                hidden_dim_second = hidden_dim_second, hidden_dim_third = hidden_dim_third):\n    \n    inputs = tf.keras.layers.Input(shape=(seq_len, 3))\n\n    categorical_feat_dim = 3\n    categorical_fea = inputs[:, :, :categorical_feat_dim]\n    \n    embed = tf.keras.layers.Embedding(input_dim=len(token2int), output_dim=embed_dim)(categorical_fea)\n    reshaped = tf.reshape(\n        embed, shape=(-1, embed.shape[1],  embed.shape[2] * embed.shape[3]))\n    \n    reshaped = tf.keras.layers.SpatialDropout1D(.2)(reshaped)\n    \n    hidden = EncoderLayer(96, 8, 256)(reshaped)\n    hidden = EncoderLayer(96, 8, 256)(hidden)\n        \n    truncated = hidden[:, :pred_len]\n\n    out = tf.keras.layers.Dense(len(target_cols), activation='linear')(truncated)\n\n    model = tf.keras.Model(inputs=inputs, outputs=out)\n\n    adam = tf.optimizers.Adam()\n    model.compile(optimizer=adam, loss=MCRMSE, metrics=[tf.keras.metrics.RootMeanSquaredError()])\n    \n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tf.keras.backend.clear_session()\nfrom tqdm.keras import TqdmCallback\nlr_callback = tf.keras.callbacks.ReduceLROnPlateau()\nes_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss',restore_best_weights=True,min_delta=0.001, patience=10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_and_predict(n_folds=5, model_name=\"model\", model_type=0, epochs=100, debug=True,\n                      dropout_model=dropout_model, hidden_dim_first = hidden_dim_first, \n                      hidden_dim_second = hidden_dim_second, hidden_dim_third = hidden_dim_third,\n                      seed=seed):\n\n    print(\"Model:\", model_name)\n\n    ensemble_preds = pd.DataFrame(index=sample_sub.index, columns=target_cols).fillna(0) # test dataframe with 0 values\n    kf = KFold(n_folds, shuffle=True, random_state=seed)\n    skf = StratifiedKFold(n_folds, shuffle=True, random_state=seed)\n    val_losses = []\n    historys = []\n\n    for i, (train_index, val_index) in enumerate(skf.split(train_inputs_all, train['SN_filter'])):\n        print(\"Fold:\", str(i+1))\n        with strategy.scope():\n            model_train = build_model(model_type=model_type, \n                                      dropout=dropout_model, \n                                      hidden_dim_first = hidden_dim_first, \n                                      hidden_dim_second = hidden_dim_second, \n                                      hidden_dim_third = hidden_dim_third)\n            model_short = build_model(model_type=model_type, seq_len=107, pred_len=107,\n                                      dropout=dropout_model, \n                                      hidden_dim_first = hidden_dim_first, \n                                      hidden_dim_second = hidden_dim_second, \n                                      hidden_dim_third = hidden_dim_third)\n            model_long = build_model(model_type=model_type, seq_len=130, pred_len=130,\n                                     dropout=dropout_model, \n                                     hidden_dim_first = hidden_dim_first, \n                                     hidden_dim_second = hidden_dim_second, \n                                     hidden_dim_third = hidden_dim_third)\n\n        train_inputs, train_labels = train_inputs_all[train_index], train_labels_all[train_index]\n        val_inputs, val_labels = train_inputs_all[val_index], train_labels_all[val_index]\n\n        checkpoint = tf.keras.callbacks.ModelCheckpoint(f'{model_name}_Fold_{str(i+1)}.h5')\n\n        history = model_train.fit(\n            train_inputs , train_labels, \n            validation_data=(val_inputs,val_labels),\n            batch_size=64,\n            epochs=epochs, \n            callbacks=[checkpoint,\n                       lr_callback,\n                       TqdmCallback(),\n                       tf.keras.callbacks.TerminateOnNaN(),\n                       es_callback],\n            verbose= 0\n        )\n\n        print(f\"{model_name} Min training loss={min(history.history['loss'])}, min validation loss={min(history.history['val_loss'])}\")\n\n        val_losses.append(min(history.history['val_loss']))\n        historys.append(history)\n\n        model_short.load_weights(f'{model_name}_Fold_{str(i+1)}.h5')\n        model_long.load_weights(f'{model_name}_Fold_{str(i+1)}.h5')\n\n        public_preds = model_short.predict(public_inputs)\n        private_preds = model_long.predict(private_inputs)\n\n        preds_model = []\n        for df, preds in [(public_df, public_preds), (private_df, private_preds)]:\n            for i, uid in enumerate(df.id):\n                single_pred = preds[i]\n\n                single_df = pd.DataFrame(single_pred, columns=target_cols)\n                single_df['id_seqpos'] = [f'{uid}_{x}' for x in range(single_df.shape[0])]\n\n                preds_model.append(single_df)\n\n        preds_model_df = pd.concat(preds_model)\n        ensemble_preds[target_cols] += preds_model_df[target_cols].values / n_folds\n\n        if debug:\n            print(\"Intermediate ensemble result\")\n            print(ensemble_preds[target_cols].head())\n\n    ensemble_preds[\"id_seqpos\"] = preds_model_df[\"id_seqpos\"].values\n    ensemble_preds = pd.merge(sample_sub[\"id_seqpos\"], ensemble_preds, on=\"id_seqpos\", how=\"left\")\n\n    print(\"Mean Validation loss:\", str(np.mean(val_losses)))\n\n    if debug:\n        fig, ax = plt.subplots(1, 3, figsize = (20, 10))\n        for i, history in enumerate(historys):\n            ax[0].plot(history.history['loss'])\n            ax[0].plot(history.history['val_loss'])\n            ax[0].set_title('model_'+str(i+1))\n            ax[0].set_ylabel('Loss')\n            ax[0].set_xlabel('Epoch')\n            \n            ax[1].plot(history.history['root_mean_squared_error'])\n            ax[1].plot(history.history['val_root_mean_squared_error'])\n            ax[1].set_title('model_'+str(i+1))\n            ax[1].set_ylabel('RMSE')\n            ax[1].set_xlabel('Epoch')\n            \n            ax[2].plot(history.history['lr'])\n            ax[2].set_title('model_'+str(i+1))\n            ax[2].set_ylabel('LR')\n            ax[2].set_xlabel('Epoch')\n        plt.show()\n\n    return ensemble_preds\n\n\npublic_df = test.query(\"seq_length == 107\").copy()\nprivate_df = test.query(\"seq_length == 130\").copy()\npublic_inputs = preprocess_inputs(public_df)\nprivate_inputs = preprocess_inputs(private_df)\n\nensembles = []\n\nfor i in range(1):\n    model_name = \"model_\"+str(i+1)\n\n    ensemble = train_and_predict(n_folds=5, model_name=model_name, model_type=i, epochs=40,\n                                 dropout_model=dropout_model, hidden_dim_first = hidden_dim_first, \n                                 hidden_dim_second = hidden_dim_second, hidden_dim_third = hidden_dim_third,\n                                 seed=seed)\n    ensembles.append(ensemble)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4. Ensembling the solutions and submission\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Ensembling the solutions\nensemble_final = ensembles[0].copy()\nensemble_final[target_cols] = 0\n\nfor ensemble in ensembles:\n    ensemble_final[target_cols] += ensemble[target_cols].values / len(ensembles)\n\nensemble_final.head().T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"blend_preds_df = pd.DataFrame()\nblend_preds_df['id_seqpos'] = ensemble_final['id_seqpos']\nblend_preds_df['reactivity'] = ensemble_final['reactivity'] \nblend_preds_df['deg_Mg_pH10'] = ensemble_final['deg_Mg_pH10']\nblend_preds_df['deg_pH10'] = ensemble_final['deg_pH10']\nblend_preds_df['deg_Mg_50C'] = ensemble_final['deg_Mg_50C']\nblend_preds_df['deg_50C'] = ensemble_final['deg_50C']\nblend_preds_df.head().T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Submission\nblend_preds_df.to_csv('ensemble_final.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#EOF"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}