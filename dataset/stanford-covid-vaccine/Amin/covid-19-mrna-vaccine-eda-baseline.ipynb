{"cells":[{"metadata":{},"cell_type":"markdown","source":"# OpenVaccine: COVID-19 mRNA Vaccine Degradation Prediction\n***\n\nIn this competition, we are provided the COVID-19 mRNA sequence and we are suposed **to develop algorithms and build models to predict the likely degradation rates at each base of an RNA molecule.**\nThe goal is to improve the stability of mRNA vaccines to accelerate mRNA vaccine research and deliver a refrigerator-stable vaccine against SARS-CoV-2.\n\n> Read datasets + Helper functions"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.simplefilter(action='ignore')\n\n# READ DATASETS\ntrain = pd.read_json('../input/stanford-covid-vaccine/train.json',lines=True)\ntest = pd.read_json('../input/stanford-covid-vaccine/test.json', lines=True)\nsub = pd.read_csv('../input/stanford-covid-vaccine/sample_submission.csv')\n\n#### HELPER FUNCTIONS\ndef plotd(f1,f2):\n    plt.style.use('seaborn')\n    sns.set_style('whitegrid')\n    fig = plt.figure(figsize=(15,5))\n    #1 rows 2 cols\n    #first row, first col\n    ax1 = plt.subplot2grid((1,2),(0,0))\n    plt.hist(a[f1], bins=4, color='black',alpha=0.5)\n    plt.title(f'{f1}',weight='bold', fontsize=18)\n    #first row sec col\n    ax1 = plt.subplot2grid((1,2),(0,1))\n    plt.hist(a[f2], bins=4, color='crimson',alpha=0.5)\n    plt.title(f'{f2}',weight='bold', fontsize=18)\n \n    plt.show()\n\ndef plotc(f1,f2):\n    plt.style.use('seaborn')\n    sns.set_style('whitegrid')\n    fig = plt.figure(figsize=(15,5))\n    #1 rows 2 cols\n    #first row, first col\n    ax1 = plt.subplot2grid((1,2),(0,0))\n    plt.hist(a[f1], bins=7, color='black',alpha=0.7)\n    plt.title(f'{f1}',weight='bold', fontsize=18)\n    #first row sec col\n    ax1 = plt.subplot2grid((1,2),(0,1))\n    plt.hist(a[f2], bins=5, color='crimson',alpha=0.7)\n    plt.title(f'{f2}',weight='bold', fontsize=18)\n    plt.xticks(weight='bold')\n    plt.show()\n    \ndef ploth(data, w=15, h=9):\n    plt.figure(figsize=(w,h))\n    sns.heatmap(data.corr(), cmap='hot', annot=True)\n    plt.title('Correlation between the features', fontsize=18, weight='bold')\n    plt.xticks(weight='bold')\n    plt.yticks(weight='bold')\n    return plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**First glimpse:**"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Sequence: COVID-19 mRNA vaccine sequence\n* Structure: (, ), and . characters that describe whether a base is estimated to be paired or unpaired\n* Pridicted loop type: S: paired \"Stem\" M: Multiloop I: Internal loop B: Bulge H: Hairpin loop E: dangling End X: eXternal loop\n* Reactivity: determine the likely secondary structure of the RNA sample.\n* de_pH10: determine the likelihood of degradation at the base/linkage after incubating without magnesium at high pH (pH 10)\n* deg_Mg_pH10: determine the likelihood of degradation at the base/linkage after incubating with magnesium in high pH (pH 10).\n* deg_Mg_50C: determine the likelihood of degradation at the base/linkage after incubating with magnesium at high temperature (50 degrees Celsius).\n* deg_50C: determine the likelihood of degradation at the base/linkage after incubating without magnesium at high temperature (50 degrees Celsius).\n\nMost of the columns have arrays of 68 values that correspond to the first 68 nucleotides (letters in the mRNA sequence of 107 letters). This mRNA sequence (ATGCGGCGT....) should be flattened and the first 68 nucleotides are kept and matched to their values in the features that have arrays of 68 valus.\n\n**To clarify this point: Let's check the length of the sequence string (number of letters)**"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"def length(feature):\n    column= train[[feature]]\n    column['length']= column[feature].apply(len)\n    return column.head()\n\nlength('sequence')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Just the 68 first letters of this string are used in this study. The corresponding features have arrays of 68 elements that match with the first 68 ltters of the sequence string.\n\n**Let's check the number of elements in the feature: reactivity**"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"length('reactivity')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After flattening the string and the arrays, we keep the first 68 letters of the string sequence and discard the rest. To flatten, I used the code from this notebook: [OpenVaccine EDA, feature engineering and modelling](https://www.kaggle.com/artgor/openvaccine-eda-feature-engineering-and-modelling)"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"train_data = []\nfor mol_id in train['id'].unique():\n    sample_data = train.loc[train['id'] == mol_id]\n    for i in range(68):\n        sample_tuple = (sample_data['id'].values[0], sample_data['sequence'].values[0][i],\n                        sample_data['structure'].values[0][i], sample_data['predicted_loop_type'].values[0][i],\n                        sample_data['reactivity'].values[0][i], sample_data['reactivity_error'].values[0][i],\n                        sample_data['deg_Mg_pH10'].values[0][i], sample_data['deg_error_Mg_pH10'].values[0][i],\n                        sample_data['deg_pH10'].values[0][i], sample_data['deg_error_pH10'].values[0][i],\n                        sample_data['deg_Mg_50C'].values[0][i], sample_data['deg_error_Mg_50C'].values[0][i],\n                        sample_data['deg_50C'].values[0][i], sample_data['deg_error_50C'].values[0][i])\n        train_data.append(sample_tuple)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"a = pd.DataFrame(train_data, columns=['id', 'sequence', 'structure', 'predicted_loop_type', 'reactivity', 'reactivity_error', 'deg_Mg_pH10', 'deg_error_Mg_pH10',\n                                  'deg_pH10', 'deg_error_pH10', 'deg_Mg_50C', 'deg_error_Mg_50C', 'deg_50C', 'deg_error_50C'])\na.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now our dataset is ready to be used for deeper analysis!\n\n# 2- The features:\n> ### 2-1 Reactivity"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"plotd('reactivity', 'reactivity_error')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Reactivity values are in the range (-10,10)\n* Reactivity error has outliers *(All the features have error outliers)*.\n\n> ### 2-2 Degradation at 50C:"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"plotd('deg_50C','deg_Mg_50C')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* deg_Mg_50C values are in the range (-12,12).\n* deg_50C values are in the range (-19,19).\n\n**At 50C, the degradation values are higher after incubation without magnesium.**\n\n\n> ### 2-3 Degradation at pH10:"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"plotd('deg_pH10','deg_Mg_pH10')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* deg_pH10 values are in the range (-20,20)\n* deg_Mg_pH10 values are in the range (-7,7)\n\n**At pH=10, the degradation values are higher after incubation without magnesium.**\n\n> ### 2-5 Predicted loop type and structure:"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"plotc('predicted_loop_type', 'structure')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* S (Paired \"Stem\") is the dominant loop type.\n* E (Dangling End) and H (Hairpin Loop) are also highly represented in comparison with the rest\n* **.** structure (unpaired) is dominating, the paired structures **)** and **(** are equally represented (sicne their pair together).\n\n> ### 2-6 Nucleotides:"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"sns.countplot(a['sequence'], palette='terrain', alpha=0.8)\nplt.title('Nucleotides count per sequence', weight='bold', fontsize=12)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* **A** and **G** nucleotides are highly present in the sequences compared to **C** and **U**.\n\n# 3- Correlation between features:\n> ### 3-1 Numerical features"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"b=a[['reactivity', 'deg_Mg_pH10',\n                                  'deg_pH10', 'deg_Mg_50C', 'deg_50C']]\n\nploth(b, 10, 4)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Most correlation values are around 0.5, we can consider our variables **moderately correlated.**\n\n> ### 3-2 Categorical variables:"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"c= a[['id', 'sequence', 'structure', 'predicted_loop_type']]\nc= pd.get_dummies(c, columns=['sequence', 'structure', 'predicted_loop_type'])\nploth(c)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**The correlation between categorical variables is more interesting:**\n* The correlation between structure . and loop type S *(Paired)* is **-1**\n* The correlation between structure ( and ) and loop type S is **0.58**\n\nThis is a given, in the data description: *Paired bases are denoted by opening and closing parentheses*\n***\n# 4- Baseline: NN + Gradient Boosting\n\n* Gradient boosting with MultiLabelOutput fits one regressor per target. It doesn't consider the correlation between the targets, in this case we have moderately correlated targets *(0.5)*\n\n* Neural nets take advantage of correlations between targets.\n\n* In **VERSION 9:** I try a NN with 3 embedding layers for our 3 categorical variables instead of one-hot-encoding.\n\n> ### 4-1 Preprocessing:"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Filter public and private sets\npublic_df = test.query(\"seq_length == 107\").copy()\nprivate_df = test.query(\"seq_length == 130\").copy()\n\n#trunc= trunc.apply(lambda x: x.str.slice(0, 68))\n#trunc2= trunc2.apply(lambda x: x.str.slice(0, 91))\n#trunc=public_df[['sequence']]\n#trunc2=private_df[['sequence']]\n#public_df['sequence']=trunc['sequence']\n#private_df['sequence']=trunc2['sequence']\n\n#PUBLIC SET\npublic_data = []\nfor mol_id in public_df['id'].unique():\n    sample_data = public_df.loc[public_df['id'] == mol_id]\n    for i in range(68):\n        sample_tuple = (sample_data['id'].values[0] + '_' + str(i),\n                        sample_data['sequence'].values[0][i],\n                        sample_data['structure'].values[0][i], \n                        sample_data['predicted_loop_type'].values[0][i],\n                        )\n        public_data.append(sample_tuple)\n\npudf_=pd.DataFrame(public_data, columns=['id', 'sequence', 'structure', 'predicted_loop_type'])\n        \n\n#PRIVATE SET\nprivate_data = []\nfor mol_id in private_df['id'].unique():\n    sample_data = private_df.loc[private_df['id'] == mol_id]\n    for i in range(91):\n        sample_tuple = (sample_data['id'].values[0] + '_' + str(i),\n                        sample_data['sequence'].values[0][i],\n                        sample_data['structure'].values[0][i], sample_data['predicted_loop_type'].values[0][i],\n                        )\n        private_data.append(sample_tuple)\n        \nprdf_=pd.DataFrame(private_data, columns=['id', 'sequence', 'structure', 'predicted_loop_type'])\n\n\n#ENCODE CATEGORICAL FEATURES IN PUBLIC AND PRIVATE SETS\n#X2= pd.get_dummies(pudf, columns=['sequence', 'structure', 'predicted_loop_type'])\n#X3= pd.get_dummies(prdf, columns=['sequence', 'structure', 'predicted_loop_type'])\n\n\n#DROP ID\n#X2= X2.drop('id', axis=1)\n#X3= X3.drop('id', axis=1)\n#X=c.drop('id', axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import OrdinalEncoder\ndf = OrdinalEncoder(dtype=\"int\").fit_transform(a[['id', 'sequence', 'structure', 'predicted_loop_type']])\ndf=pd.DataFrame(df, columns=['id', 'sequence', 'structure', 'predicted_loop_type'] )\n#validation\nvalid_x= df[df['id']==2399]\ndense_cols=['sequence', 'structure', 'predicted_loop_type']\n\n\npudf = OrdinalEncoder(dtype=\"int\").fit_transform(pudf_[['id', 'sequence', 'structure', 'predicted_loop_type']])\nprdf = OrdinalEncoder(dtype=\"int\").fit_transform(prdf_[['id', 'sequence', 'structure', 'predicted_loop_type']])\n\npudf=pd.DataFrame(pudf, columns=['id', 'sequence', 'structure', 'predicted_loop_type'] )\nprdf=pd.DataFrame(prdf, columns=['id', 'sequence', 'structure', 'predicted_loop_type'] )\n\ndef make_X(df):\n    X = {\"dense1\": df[dense_cols].to_numpy()}\n    for i, v in enumerate(dense_cols):\n        X[v] = df[[v]].to_numpy()\n    return X\n\nX2= make_X(pudf)\nX3= make_X(prdf)\n\ndf=make_X(df)\n\nde = np.split(b, b.shape[1], axis=1)\n\nvalid_x=make_X(valid_x)\n\nvalid_y=b[['reactivity']].tail(68)\nvalid_y = np.split(valid_y, valid_y.shape[1], axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> ### 4-2 MODEL:"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"import tensorflow as tf\nimport tensorflow.keras.backend as K\nimport tensorflow.keras.layers as L\nimport tensorflow.keras.models as M\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint\nfrom keras.layers.core import Dense, Dropout, Activation\nfrom keras.models import Sequential\nfrom keras.utils import np_utils\nimport keras.models as models\nimport keras.layers as layers\nfrom keras import regularizers\nimport numpy.random as nr\nimport keras\nfrom keras.layers import Dropout, BatchNormalization\nimport keras.layers as layers\nimport tensorflow as tf\nimport tensorflow.keras as keras\n\nfrom tensorflow.keras import regularizers\nfrom tensorflow.keras.layers import Dense, Input, Embedding, Dropout, concatenate, Flatten\nfrom tensorflow.keras.models import Model\nimport gc\nimport os\nfrom tqdm.notebook import tqdm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_model():\n    tf.random.set_seed(173)\n\n    tf.keras.backend.clear_session()\n    gc.collect()\n\n    # Dense input\n    dense_input = Input(shape=(len(dense_cols), ), name='dense1')\n\n    # Embedding input\n    sequence = Input(shape=(1,), name='sequence')\n    structure = Input(shape=(1,), name='structure')\n    loop = Input(shape=(1,), name='predicted_loop_type')\n    \n    sequence_emb = Flatten()(Embedding(4, 1)(sequence))\n    structure_emb = Flatten()(Embedding(3, 1)(structure))\n    loop_emb = Flatten()(Embedding(7, 1)(loop))\n    \n\n    # Combine dense and embedding parts and add dense layers. Exit on linear scale.\n    x = concatenate([dense_input, sequence_emb, structure_emb, loop_emb])\n    x = Dense(512, activation=\"tanh\")(x)\n    x=  Dropout(0.2)(x)\n    x = Dense(256, activation=\"tanh\")(x)\n    x=  Dropout(0.2)(x)\n    x = Dense(128, activation=\"tanh\")(x)\n    x=  Dropout(0.2)(x)\n    x = Dense(64, activation=\"tanh\")(x)\n    x=  Dropout(0.2)(x)\n    x = Dense(16, activation=\"tanh\")(x)\n    outputs = Dense(5, name='output')(x)\n\n    inputs = {\"dense1\": dense_input, \"seq\": sequence, \"structure\": structure, \"loop\": loop}\n\n    # Connect input and output\n    model = Model(inputs, outputs)\n\n    model.compile(loss=keras.losses.mean_squared_error,\n                  metrics=[\"mse\"],\n                  optimizer=keras.optimizers.Adam())\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = create_model()\nhistory = model.fit(df, \n                    de,\n                    batch_size=64,\n                    epochs=50\n                    #,shuffle=True\n                    ,callbacks=[tf.keras.callbacks.ReduceLROnPlateau(), tf.keras.callbacks.ModelCheckpoint('model.h5')]\n                    #,validation_data=(valid_x, valid_y))\n                   ,validation_split=0.33)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"plt.style.use('seaborn')\nsns.set_style('whitegrid')\nfig = plt.figure(figsize=(15,5))\ntrain_loss = history.history['loss']\ntest_loss = history.history['val_loss']\nx = list(range(1, len(test_loss) + 1))\nplt.plot(x, test_loss, color = 'crimson', label = 'Test loss')\nplt.plot(x, train_loss,color='black', label = 'Training losss')\nplt.legend()\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.title('Loss vs. Epoch',weight='bold', fontsize=18)\n\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#PREDICT PUBLIC AND PRIVATE SETS\npublic_preds = model.predict(X2)\nprivate_preds = model.predict(X3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#MULTIOUTPUTREGRESSOR FOR MULTILABEL TASK\nfrom sklearn.multioutput import MultiOutputRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\n#model= MultiOutputRegressor(GradientBoostingRegressor(random_state=42)).fit(X, b)\n\n#PREDICT PUBLIC SET\n#public_preds=model.predict(X2)\n\n#PREDICT PRIVATE SET\n#private_preds=model.predict(X3)\n\n#DATAFRAMES OF PREDS\npu_predictions= pd.DataFrame(public_preds, columns=b.columns)\npr_predictions= pd.DataFrame(private_preds, columns=b.columns)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> ### 4-3 SUBMISSION"},{"metadata":{"trusted":true},"cell_type":"code","source":"#ADD id_seqpos to merge later with the submission file\npu_predictions['id_seqpos']= pudf_['id']\npr_predictions['id_seqpos']= prdf_['id']\n#CONCAT PUBLIC AND PRIVATE SET\nfinal= pd.concat([pu_predictions, pr_predictions])\n#MERGE WITH SUB FILE\nsub1=sub.merge(final, on='id_seqpos', how='left')\n#DROP AND RENAME SUB COLUMNS\nsub1= sub1.drop(['reactivity_x', 'deg_Mg_pH10_x', 'deg_pH10_x', 'deg_Mg_50C_x', 'deg_50C_x'], axis=1)\nsub1= sub1.rename(columns= {'reactivity_y':'reactivity','deg_Mg_pH10_y':'deg_Mg_pH10',\n                     'deg_pH10_y':'deg_pH10', 'deg_Mg_50C_y':'deg_Mg_50C',\n                     'deg_50C_y':'deg_50C'})\n#FILL NA WITH 0 (NONSCORED SEQUENCES)\nsubmission= sub1.fillna(0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}