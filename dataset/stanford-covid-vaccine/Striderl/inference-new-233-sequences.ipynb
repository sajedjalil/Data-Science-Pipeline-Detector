{"cells":[{"metadata":{},"cell_type":"markdown","source":"## User Preset"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"NFOLDS = 5\nnew_seq_path = r\"/kaggle/input/stanford-covid-vaccine/post_deadline_files/\"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd\nimport gc, os\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.utils import shuffle\n\nimport tensorflow as tf\nfrom tensorflow.keras import layers as L\nimport tensorflow_addons as tfa\nfrom tensorflow.keras import backend as K","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Helper Functions"},{"metadata":{"trusted":true},"cell_type":"code","source":"def pandas_list_to_array(df):\n    \"\"\"\n    Input: dataframe of shape (x, y), containing list of length l\n    Return: np.array of shape (x, l, y)\n    \"\"\"\n    \n    return np.transpose(\n        np.array(df.values.tolist()),\n        (0, 2, 1)\n    )\ndef np_onehot(x, num_class):\n    x = x.reshape(-1).astype(np.int32)\n    onehot = np.zeros((len(x), num_class))\n    onehot[np.arange(len(x)), x] = 1\n    return onehot\n\ndef mk_pair_map(structure, type='pm'):\n    pm = np.full(len(structure), -1, dtype=int)\n    pd = np.full(len(structure), -1, dtype=int)\n    queue = []\n    for i, s in enumerate(structure):\n        if s == \"(\":\n            queue.append(i)\n        elif s == \")\":\n            j = queue.pop()\n            pm[i] = j\n            pm[j] = i\n            pd[i] = i-j\n            pd[j] = i-j\n    if type == 'pm':\n        return pm\n    elif type == 'pd':\n        return pd\n    \ndef pair_type(seq, pair_map):\n    pm_map = {\n        \"0\":\"0\",\n        \"GC\": \"GC\",\n        \"CG\": \"GC\",\n        \"AU\": \"AU\",\n        \"UA\": \"AU\",\n        \"GU\": \"GU\",\n        \"UG\": \"GU\"\n    }\n    \n    result = [\"0\" if pair_map[i] == -1 else seq[i]+seq[pair_map[i]] for i in range(len(pair_map))]\n    result = [pm_map[r] for r in result]\n    return result\n\ndef col_mcrmse_loss(y_true, y_pred):\n    y_true_nan = tf.where(tf.math.is_nan(y_true), y_pred, y_true)\n    rmse = tf.sqrt(tf.reduce_mean(tf.square(y_true_nan - y_pred), axis=1) + EPSILON)\n    return tf.reduce_mean(rmse[:, 0]*0.2 + rmse[:, 1]*0.3 + rmse[:, 2]*0.3 + rmse[:, 3]*0.1 + rmse[:, 4]*0.1)\n    \ndef reverse_2D(_input):\n    return _input[:, ::-1, :]\n\ndef reverse_3D(_input):\n    return _input[:, ::-1, ::-1,:]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Load Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"# This will tell us the columns we are predicting\npred_cols = ['reactivity', 'deg_Mg_pH10', 'deg_Mg_50C', 'deg_pH10', 'deg_50C']\nerror_cols = ['reactivity_error', 'deg_error_Mg_pH10', 'deg_error_Mg_50C', 'deg_error_pH10', 'deg_error_50C']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import json\nimport glob\nfrom tqdm.notebook import tqdm\n\ntest  = pd.read_csv(os.path.join(new_seq_path, \"new_sequences.csv\"))\nsub = pd.read_csv(os.path.join(new_seq_path, \"new_sequences_submission.csv\"))\n\ntest['seq_length'] = test['sequence'].apply(len)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### 1. As: bpp matrices","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### 2. Structure Adjacency\ndef get_structure_adj(train):\n    Ss = []\n    Ssm1 = []\n    Ssp1 = []\n    for i in range(len(train)):\n        seq_length = train[\"seq_length\"].iloc[i]\n        structure = train[\"structure\"].iloc[i]\n        sequence = train[\"sequence\"].iloc[i]\n\n        cue = []\n        a_structures = {\n            (\"A\", \"U\") : np.zeros([seq_length, seq_length]),\n            (\"C\", \"G\") : np.zeros([seq_length, seq_length]),\n            (\"U\", \"G\") : np.zeros([seq_length, seq_length]),\n            (\"U\", \"A\") : np.zeros([seq_length, seq_length]),\n            (\"G\", \"C\") : np.zeros([seq_length, seq_length]),\n            (\"G\", \"U\") : np.zeros([seq_length, seq_length]),\n        }\n        \n        am1_structures = {\n            (\"A\", \"U\") : np.zeros([seq_length, seq_length]),\n            (\"C\", \"G\") : np.zeros([seq_length, seq_length]),\n            (\"U\", \"G\") : np.zeros([seq_length, seq_length]),\n            (\"U\", \"A\") : np.zeros([seq_length, seq_length]),\n            (\"G\", \"C\") : np.zeros([seq_length, seq_length]),\n            (\"G\", \"U\") : np.zeros([seq_length, seq_length]),\n        }\n        \n        ap1_structures = {\n            (\"A\", \"U\") : np.zeros([seq_length, seq_length]),\n            (\"C\", \"G\") : np.zeros([seq_length, seq_length]),\n            (\"U\", \"G\") : np.zeros([seq_length, seq_length]),\n            (\"U\", \"A\") : np.zeros([seq_length, seq_length]),\n            (\"G\", \"C\") : np.zeros([seq_length, seq_length]),\n            (\"G\", \"U\") : np.zeros([seq_length, seq_length]),\n        }\n        \n        for i in range(seq_length):\n            if structure[i] == \"(\":\n                cue.append(i)\n            elif structure[i] == \")\":\n                start = cue.pop()\n                a_structures[(sequence[start], sequence[i])][start, i] = 1\n                a_structures[(sequence[i], sequence[start])][i, start] = 1\n                if i - 1 >=0:\n                    am1_structures[(sequence[start], sequence[i])][start, i-1] = 1\n                if start - 1 >= 0:\n                    ap1_structures[(sequence[i], sequence[start])][i, start-1] = 1\n                if i + 1 < len(sequence):\n                    ap1_structures[(sequence[start], sequence[i])][start, i+1] = 1\n                if start + 1 < len(sequence):\n                    am1_structures[(sequence[i], sequence[start])][i, start+1] = 1\n        \n        a_strc = np.stack([a for a in a_structures.values()], axis = 2)\n        a_strc = np.sum(a_strc, axis = 2, keepdims = True)\n        am1_strc = np.stack([a for a in am1_structures.values()], axis = 2)\n        am1_strc = np.sum(am1_strc, axis = 2, keepdims = True)\n        ap1_strc = np.stack([a for a in ap1_structures.values()], axis = 2)\n        ap1_strc = np.sum(ap1_strc, axis = 2, keepdims = True)\n        Ss.append(a_strc)\n        Ssm1.append(am1_strc)\n        Ssp1.append(ap1_strc)\n        \n    Ss, Ssm1, Ssp1 = np.array(Ss), np.array(Ssm1), np.array(Ssp1)\n    new = np.concatenate([Ss, Ssm1, Ssp1], axis=3)\n    return new","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### 3. distance matrix: \ndef get_distance_matrix(As):\n    idx = np.arange(As.shape[1])\n    Ds = []\n    for i in range(len(idx)):\n        d = np.abs(idx[i] - idx)\n        Ds.append(d)\n\n    Ds = np.array(Ds) + 1\n    Ds = 1/Ds\n    Ds = Ds[None, :,:]\n    Ds = np.repeat(Ds, len(As), axis = 0)\n\n    Dss = []\n    for i in [1, 2, 4]:\n        Dss.append(Ds ** i)\n    Ds = np.stack(Dss, axis = 3)\n    return Ds","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## 4. Node Features\ndef return_ohe(n, i):\n    tmp = [0] * n\n    tmp[i] = 1\n    return tmp\n\ndef get_input(train):\n    ## get node features, which is one hot encoded\n    mapping = {}\n    vocab = [\"A\", \"G\", \"C\", \"U\"]\n    for i, s in enumerate(vocab):\n        mapping[s] = return_ohe(len(vocab), i)\n    X_node = np.stack(train[\"sequence\"].apply(lambda x : list(map(lambda y : mapping[y], list(x)))))\n\n    mapping = {}\n    vocab = [\"S\", \"M\", \"I\", \"B\", \"H\", \"E\", \"X\"]\n    for i, s in enumerate(vocab):\n        mapping[s] = return_ohe(len(vocab), i)\n    X_loop = np.stack(train[\"bpRNA_string\"].apply(lambda x : list(map(lambda y : mapping[y], list(x)))))\n    \n    mapping = {}\n    vocab = [\".\", \"(\", \")\"]\n    for i, s in enumerate(vocab):\n        mapping[s] = return_ohe(len(vocab), i)\n    X_structure = np.stack(train[\"structure\"].apply(lambda x : list(map(lambda y : mapping[y], list(x)))))\n    \n    X_node = np.concatenate([X_node, X_loop], axis = 2)\n    \n    ## interaction\n    a = np.sum(X_node * (2 ** np.arange(X_node.shape[2])[None, None, :]), axis = 2)\n    vocab = [17, 18, 20, 24, 33, 34, 36, 40, 65, 66, 68, 72, 129, 130, 132, 136, 257, 258, 260, 264, 513, 514, 516, 520, 1025, 1026, 1028, 1032]\n    ohes = []\n    for v in vocab:\n        ohes.append(a == v)\n    ohes = np.stack(ohes, axis = 2)\n    X_node = np.concatenate([X_node, ohes], axis = 2).astype(np.float32)\n    \n    return X_node\n\n\npair_dict  = {x:i for i, x in enumerate(['0','AU','GU','GC'])} #4\n\ndef preprocess_inputs_wa(df):\n    df['pair_map'] = df['structure'].apply(mk_pair_map)\n    df[\"pair_type\"] = df.apply(lambda x: pair_type(x[\"sequence\"],x[\"pair_map\"]), axis=1)\n    v_pair_type = df['pair_type'].map(lambda seq: [pair_dict[x] for x in seq]).values\n    \n    bbp_max = []\n    bbp_sum = []\n    bpp_2ndmax_diff = []\n    \n    rna_id = df.id.values\n    for i in rna_id:\n        probability = np.load(os.path.join(new_seq_path, f\"new_sequences_bpps/{i}.npy\"))\n        bbp_max.append(probability.max(-1).tolist())\n        bbp_sum.append((1-probability.sum(-1)).tolist())\n        temp = np.sort(probability)\n        bpp_2ndmax_diff.append((temp[:,-1] - temp[:, -2]).tolist())\n        \n    inputs = []\n    for i in range(len(df)):\n        inputs.append(np.concatenate([\n            np_onehot(np.array(v_pair_type[i]), 4)[:,1:],\n            np.array(bbp_max[i]).reshape(-1, 1),\n            np.array(bbp_sum[i]).reshape(-1, 1),\n            np.array(bpp_2ndmax_diff[i]).reshape(-1, 1),\n        ],1))\n    return np.array(inputs)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# https://www.kaggle.com/nullrecurrent/ov-inference-233-new-seq\n\ndef get_features(df):\n    X_node = get_input(df).astype(np.float32)\n    X_node_hm = preprocess_inputs_wa(df).astype(np.float32)\n    X_node = np.concatenate([X_node, X_node_hm], axis=2)\n    \n    As = []\n    for id in df[\"id\"]:\n        a = np.load(os.path.join(new_seq_path, f\"new_sequences_bpps/{id}.npy\"))\n        As.append(a)\n    As = np.array(As)\n    Ss = get_structure_adj(df).astype(np.float32)\n    Ds = get_distance_matrix(As)\n    As = np.concatenate([As[:,:,:,None], Ss, Ds], axis = 3).astype(np.float32)\n    return X_node, As\n\ndict_X = {}\ndict_A = {}\nfor i in tqdm(test.id):\n    df_temp = test.loc[[i]]\n    dict_X[i], dict_A[i] = get_features(df_temp)\n    \nX_node, As = dict_X[0], dict_A[0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"# GRU/LSTM layer from https://www.kaggle.com/xhlulu/openvaccine-simple-gru-model\n# Wave block from https://www.kaggle.com/ragnar123/wavenet-gru-baseline\n\ndef gru_layer(hidden_dim, dropout):\n    return L.Bidirectional(\n        L.GRU(hidden_dim, dropout=dropout, return_sequences=True, kernel_initializer='orthogonal')\n    )\n\n\ndef lstm_layer(hidden_dim, dropout):\n    return L.Bidirectional(\n              L.LSTM(hidden_dim,dropout=dropout, return_sequences=True,kernel_initializer = 'orthogonal'))\n\ndef wave_block(x, filters, kernel_size, n):\n    dilation_rates = [2 ** i for i in range(n)]\n    x = tf.keras.layers.Conv1D(filters = filters, \n                               kernel_size = 1,\n                               padding = 'same')(x)\n    res_x = x\n    for dilation_rate in dilation_rates:\n        tanh_out = tf.keras.layers.Conv1D(filters = filters,\n                          kernel_size = kernel_size,\n                          padding = 'same', \n                          activation = 'tanh', \n                          dilation_rate = dilation_rate)(x)\n        sigm_out = tf.keras.layers.Conv1D(filters = filters,\n                          kernel_size = kernel_size,\n                          padding = 'same',\n                          activation = 'sigmoid', \n                          dilation_rate = dilation_rate)(x)\n        x = tf.keras.layers.Multiply()([tanh_out, sigm_out])\n        x = tf.keras.layers.Conv1D(filters = filters,\n                   kernel_size = 1,\n                   padding = 'same')(x)\n        res_x = tf.keras.layers.Add()([res_x, x])\n    return res_x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def attention(x_inner, x_outer, n_factor, dropout):\n    x_Q =  L.Conv1D(n_factor, 1, activation='linear', \n                  kernel_initializer='glorot_uniform',\n                  bias_initializer='glorot_uniform',\n                 )(x_inner)\n    x_K =  L.Conv1D(n_factor, 1, activation='linear', \n                  kernel_initializer='glorot_uniform',\n                  bias_initializer='glorot_uniform',\n                 )(x_outer)\n    x_V =  L.Conv1D(n_factor, 1, activation='linear', \n                  kernel_initializer='glorot_uniform',\n                  bias_initializer='glorot_uniform',\n                 )(x_outer)\n    x_KT = L.Permute((2, 1))(x_K)\n    res = L.Lambda(lambda c: K.batch_dot(c[0], c[1]) / np.sqrt(n_factor))([x_Q, x_KT])\n    att = L.Lambda(lambda c: K.softmax(c, axis=-1))(res)\n    att = L.Lambda(lambda c: K.batch_dot(c[0], c[1]))([att, x_V])\n    return att\n\ndef multi_head_attention(x, y, n_factor, n_head, dropout):\n    if n_head == 1:\n        att = attention(x, y, n_factor, dropout)\n    else:\n        n_factor_head = n_factor // n_head\n        heads = [attention(x, y, n_factor_head, dropout) for i in range(n_head)]\n        att = L.Concatenate()(heads)\n        att = L.Dense(n_factor, \n                      kernel_initializer='glorot_uniform',\n                      bias_initializer='glorot_uniform',\n                     )(att)\n    x = L.Add()([x, att])\n    x = L.LayerNormalization()(x)\n    if dropout > 0:\n        x = L.Dropout(dropout)(x)\n    return x\n\ndef res(x, unit, kernel = 3, rate = 0.1):\n    h = L.Conv1D(unit, kernel, 1, padding = \"same\", activation = None)(x)\n    h = L.LayerNormalization()(h)\n    h = L.LeakyReLU()(h)\n    h = L.Dropout(rate)(h)\n    return L.Add()([x, h])\n\ndef forward(x, unit, kernel = 3, rate = 0.1):\n#     h = L.Dense(unit, None)(x)\n    h = L.Conv1D(unit, kernel, 1, padding = \"same\", activation = None)(x)\n    h = L.LayerNormalization()(h)\n    h = L.Dropout(rate)(h)\n#         h = tf.keras.activations.swish(h)\n    h = L.LeakyReLU()(h)\n    h = res(h, unit, kernel, rate)\n    return h\n\ndef adj_attn(x, adj, unit, n = 2, rate = 0.1):\n    x_a = x\n    x_as = []\n    for i in range(n):\n        x_a = forward(x_a, unit)\n        x_a = tf.matmul(adj, x_a)\n        x_as.append(x_a)\n    if n == 1:\n        x_a = x_as[0]\n    else:\n        x_a = L.Concatenate()(x_as)\n    x_a = forward(x_a, unit)\n    return x_a\n\n\ndef get_base(config, dim=None):\n    node = tf.keras.Input(shape = (dim, X_node.shape[2]), name = \"node\")\n    adj = tf.keras.Input(shape = (dim, dim, As.shape[3]), name = \"adj\")\n    \n    adj_learned = L.Dense(1, \"relu\")(adj)\n    adj_all = L.Concatenate(axis = 3)([adj, adj_learned])\n        \n    xs = []\n    xs.append(node)\n    x1 = forward(node, 128, kernel = 3, rate = 0.0)\n    x2 = forward(x1, 64, kernel = 6, rate = 0.0)\n    x3 = forward(x2, 32, kernel = 15, rate = 0.1)\n    x4 = forward(x3, 16, kernel = 30, rate = 0.1)\n    x = L.Concatenate()([x1, x2, x3, x4])\n    \n    for unit in [64, 32]:\n        x_as = []\n        for i in range(adj_all.shape[3]):\n            x_a = adj_attn(x, adj_all[:, :, :, i], unit, rate = 0.0)\n            x_as.append(x_a)\n        x_c = forward(x, unit, kernel = 30)\n        \n        x = L.Concatenate()(x_as + [x_c])\n        x = forward(x, unit)\n        x = multi_head_attention(x, x, unit, 4, 0.0)\n        xs.append(x)\n        \n    x = L.Concatenate()(xs)\n\n    model = tf.keras.Model(inputs = [node, adj], outputs = [x])\n    return model\n\n\ndef get_ae_model(base, config, dim=None):\n    node = tf.keras.Input(shape = (dim, X_node.shape[2]), name = \"node\")\n    adj = tf.keras.Input(shape = (dim, dim, As.shape[3]), name = \"adj\")\n\n    x = base([L.SpatialDropout1D(0.2)(node), adj])\n    x = forward(x, 64, rate = 0.2)\n    p = L.Dense(X_node.shape[2], \"sigmoid\")(x)\n    \n    loss = - tf.reduce_mean(20 * node * tf.math.log(p + 1e-4) + (1 - node) * tf.math.log(1 - p + 1e-4))\n    model = tf.keras.Model(inputs = [node, adj], outputs = [loss])\n    \n    opt = get_optimizer()\n    model.compile(optimizer = opt, loss = lambda t, y : y)\n    return model\n\n\ndef get_model(base, config, dim=None):\n    node = tf.keras.Input(shape = (dim, X_node.shape[2]), name = \"node\")\n    adj = tf.keras.Input(shape = (dim, dim, As.shape[3]), name = \"adj\")\n    \n    x = base([node, adj])\n    x = forward(x, 128, rate = 0.2)\n    \n    if ADDITIONAL_LAYER == 'grux2':\n        x = gru_layer(128, dropout=0.2)(x)\n        x = gru_layer(128, dropout=0.2)(x)\n        \n    elif ADDITIONAL_LAYER == 'lstmx1':\n        x = lstm_layer(128, dropout=0.2)(x)\n        \n    elif ADDITIONAL_LAYER == 'lstmx2':\n        x = lstm_layer(128, dropout=0.2)(x)\n        x = lstm_layer(128, dropout=0.2)(x)\n        \n    elif ADDITIONAL_LAYER == 'wave':\n        dropout = 0.2\n        x = wave_block(x, 16, 3, 12)\n        x = tf.keras.layers.BatchNormalization()(x)\n        x = tf.keras.layers.Dropout(dropout)(x)\n\n        x = wave_block(x, 32, 3, 8)\n        x = tf.keras.layers.BatchNormalization()(x)\n        x = tf.keras.layers.Dropout(dropout)(x)\n\n        x = wave_block(x, 64, 3, 4)\n        x = tf.keras.layers.BatchNormalization()(x)\n        x = tf.keras.layers.Dropout(dropout)(x)\n\n        x = wave_block(x, 128, 3, 1)\n        x = tf.keras.layers.BatchNormalization()(x)\n        x = tf.keras.layers.Dropout(dropout)(x)\n        \n    elif ADDITIONAL_LAYER == 'wave_small':\n        dropout = 0.1\n        x = wave_block(x, 32, 3, 8)\n        x = tf.keras.layers.BatchNormalization()(x)\n        x = tf.keras.layers.Dropout(dropout)(x)\n\n        x = wave_block(x, 64, 3, 4)\n        x = tf.keras.layers.BatchNormalization()(x)\n        x = tf.keras.layers.Dropout(dropout)(x)\n\n        x = wave_block(x, 128, 3, 1)\n        x = tf.keras.layers.BatchNormalization()(x)\n        x = tf.keras.layers.Dropout(dropout)(x)\n    x = L.Dense(5)(x)\n\n    model = tf.keras.Model(inputs = [node, adj], outputs = [x])\n    \n    opt = get_optimizer()\n    model.compile(optimizer = opt, loss = col_mcrmse_loss)\n    return model\n\ndef get_optimizer():\n    adam = tf.optimizers.Adam()\n    return adam","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Inference"},{"metadata":{"trusted":true},"cell_type":"code","source":"config = {}\nmodel_folders = ['gru_3d_tta_denoise', 'lstm_3d_tta', 'lstm_3d_tta_denoise']\nadditional_layers = ['grux2', 'lstmx2', 'lstmx2']\nprediction_list = []\n\nfor model_folder, ADDITIONAL_LAYER in zip(model_folders, additional_layers):\n    print(\"------------------------------\")\n    print(model_folder, ADDITIONAL_LAYER)\n    print(\"------------------------------\")\n    \n    base = get_base(config)\n    base.load_weights(f\"/kaggle/input/openvaccinemodelweights/{model_folder}/base_ae\")\n    model = get_model(base, config)\n        \n    for fold_ in range(NFOLDS):  \n        model.load_weights(f\"/kaggle/input/openvaccinemodelweights/{model_folder}/model_{fold_}.h5\")\n        preds_ls = []\n        for rna_id in tqdm(test.id):\n            X_node, As = dict_X[rna_id], dict_A[rna_id]\n            single_pred = ((model.predict([X_node, As]) + reverse_2D(model.predict([reverse_2D(X_node), reverse_3D(As)])))/2)[0]\n            single_df = pd.DataFrame(single_pred, columns=pred_cols)\n            single_df['id_seqpos'] = [f'{rna_id}_{x}' for x in range(single_df.shape[0])]\n            preds_ls.append(single_df)\n            del single_pred, single_df\n            gc.collect()\n            \n        preds_df = pd.concat(preds_ls).set_index('id_seqpos')\n        preds_df.to_csv(f\"pred_{model_folder}_{fold_}.csv\")\n        prediction_list.append(preds_df)\n        del preds_df, preds_ls\n        gc.collect()\n    del base, model\n    gc.collect()\n    K.clear_session()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### Not capping any downside or upside, but in competition I have a downside cap at -0.5\nN_PREDS = NFOLDS * len(model_folders)\n\nfinal_prediction = prediction_list[0].copy()/N_PREDS\nfor i in range(1, N_PREDS):\n    final_prediction = final_prediction.add(prediction_list[i]/N_PREDS)\nfinal_prediction = final_prediction.reset_index()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = sub[['id_seqpos']].merge(final_prediction, on=['id_seqpos'])\nsubmission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}