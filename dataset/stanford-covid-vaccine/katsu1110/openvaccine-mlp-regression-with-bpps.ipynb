{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Introduction\nMLP starter with a bpp(s) feature and a custom loss. Formatting data for a tabular-like modeling task is time-consuming, so it had been already done in my previous kernel (https://www.kaggle.com/code1110/openvaccine-is-bpp-s-the-most-important). "},{"metadata":{},"cell_type":"markdown","source":"# Libraries"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import gc\nimport os\nimport random\n\nimport lightgbm as lgb\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport itertools\nfrom pathlib import Path\nfrom tqdm import tqdm\n\nfrom matplotlib import pyplot as plt\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import StratifiedKFold, KFold, GroupKFold\nfrom sklearn.cluster import KMeans\nfrom sklearn.decomposition import PCA\n\nfrom typing import List, NoReturn, Union, Tuple, Optional, Text, Generic, Callable, Dict\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder, QuantileTransformer\nfrom sklearn.model_selection import KFold, StratifiedKFold, TimeSeriesSplit\nfrom sklearn.metrics import accuracy_score, roc_auc_score, log_loss, mean_squared_error, mean_absolute_error, f1_score\n\n# model\nimport lightgbm as lgb\n\n# keras\nimport tensorflow as tf\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras import callbacks\nfrom tensorflow.keras import optimizers\nfrom tensorflow.keras import models\nfrom tensorflow.keras import losses\nfrom tensorflow.keras import utils\nfrom tensorflow.keras import layers\nimport tensorflow.keras.backend as kb\n\n# visualize\nimport matplotlib.pyplot as plt\nimport matplotlib.style as style\nimport seaborn as sns\nfrom matplotlib import pyplot\nfrom matplotlib.ticker import ScalarFormatter\nsns.set_context(\"talk\")\nstyle.use('seaborn-colorblind')\n\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# CONFIG"},{"metadata":{"trusted":true},"cell_type":"code","source":"SEED = 42\nNFOLD = 7\nSCALER = 'MinMax' # Standard","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Load premade data"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data = pd.read_feather('../input/openvaccine-is-bpp-s-the-most-important/train_fe.feather')\ntest_data = pd.read_feather('../input/openvaccine-is-bpp-s-the-most-important/test_fe.feather')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train_data.shape)\ntrain_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(test_data.shape)\ntest_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.read_csv('/kaggle/input/stanford-covid-vaccine/sample_submission.csv')\nprint(submission.shape)\nsubmission.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Modeling pipeline"},{"metadata":{"trusted":true},"cell_type":"code","source":"#target columns\ntarget_cols = ['reactivity', 'deg_Mg_pH10', 'deg_pH10', 'deg_Mg_50C', 'deg_50C']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def rmse(y_actual, y_pred):\n    return np.sqrt(mean_squared_error(y_actual, y_pred))\n\ndef mcrmse(y_actual, y_pred, num_scored=len(target_cols)):\n    score = 0\n    for i in range(num_scored):\n        score += rmse(y_actual[:, i], y_pred[:, i]) / num_scored\n        \n    return score\n\n# def MCRMSE(y_true, y_pred):\n#     print(y_true.shape)\n#     print(y_pred.shape)\n#     colwise_mse = tf.reduce_mean(tf.square(y_true - y_pred), axis=1)\n#     return tf.reduce_mean(tf.sqrt(colwise_mse), axis=1)\n\ndef RMSE(y_actual, y_pred, eps=1e-6):\n    mse = tf.keras.losses.mean_squared_error(y_actual, y_pred)\n    return kb.sqrt(mse + eps)\n\ndef MCRMSE(y_actual, y_pred, num_scored=len(target_cols)):\n    score = 0\n    for i in range(num_scored):\n        score += RMSE(y_actual[:, i], y_pred[:, i]) / num_scored\n        \n    return score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"import random\nfrom collections import Counter, defaultdict\nfrom sklearn import model_selection\n\n# ---- GroupKFold ----\nclass GroupKFold(object):\n    \"\"\"\n    GroupKFold with random shuffle with a sklearn-like structure\n    \"\"\"\n\n    def __init__(self, n_splits=4, shuffle=True, random_state=42):\n        self.n_splits = n_splits\n        self.shuffle = shuffle\n        self.random_state = random_state\n\n    def get_n_splits(self, X=None, y=None, group=None):\n        return self.n_splits\n\n    def split(self, X, y, group):\n        kf = model_selection.KFold(n_splits=self.n_splits, shuffle=self.shuffle, random_state=self.random_state)\n        unique_ids = X[group].unique()\n        for fold, (tr_group_idx, va_group_idx) in enumerate(kf.split(unique_ids)):\n            # split group\n            tr_group, va_group = unique_ids[tr_group_idx], unique_ids[va_group_idx]\n            train_idx = np.where(X[group].isin(tr_group))[0]\n            val_idx = np.where(X[group].isin(va_group))[0]\n            yield train_idx, val_idx\n\n# ---- StratifiedGroupKFold ----\nclass StratifiedGroupKFold(object):\n    \"\"\"\n    StratifiedGroupKFold with random shuffle with a sklearn-like structure\n    \"\"\"\n\n    def __init__(self, n_splits=4, shuffle=True, random_state=42):\n        self.n_splits = n_splits\n        self.shuffle = shuffle\n        self.random_state = random_state\n\n    def get_n_splits(self, X=None, y=None, group=None):\n        return self.n_splits\n\n    def split(self, X, y, group):\n        labels_num = np.max(y) + 1\n        y_counts_per_group = defaultdict(lambda: np.zeros(labels_num))\n        y_distr = Counter()\n        groups = X[group].values\n        for label, g in zip(y, groups):\n            y_counts_per_group[g][label] += 1\n            y_distr[label] += 1\n\n        y_counts_per_fold = defaultdict(lambda: np.zeros(labels_num))\n        groups_per_fold = defaultdict(set)\n\n        def eval_y_counts_per_fold(y_counts, fold):\n            y_counts_per_fold[fold] += y_counts\n            std_per_label = []\n            for label in range(labels_num):\n                label_std = np.std([y_counts_per_fold[i][label] / y_distr[label] for i in range(self.n_splits)])\n                std_per_label.append(label_std)\n            y_counts_per_fold[fold] -= y_counts\n            return np.mean(std_per_label)\n        \n        groups_and_y_counts = list(y_counts_per_group.items())\n        random.Random(self.random_state).shuffle(groups_and_y_counts)\n\n        for g, y_counts in sorted(groups_and_y_counts, key=lambda x: -np.std(x[1])):\n            best_fold = None\n            min_eval = None\n            for i in range(self.n_splits):\n                fold_eval = eval_y_counts_per_fold(y_counts, i)\n                if min_eval is None or fold_eval < min_eval:\n                    min_eval = fold_eval\n                    best_fold = i\n            y_counts_per_fold[best_fold] += y_counts\n            groups_per_fold[best_fold].add(g)\n\n        all_groups = set(groups)\n        for i in range(self.n_splits):\n            train_groups = all_groups - groups_per_fold[i]\n            test_groups = groups_per_fold[i]\n\n            train_idx = [i for i, g in enumerate(groups) if g in train_groups]\n            test_idx = [i for i, g in enumerate(groups) if g in test_groups]\n\n            yield train_idx, test_idx","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def get_oof_ypred(model, x_val, x_test, modelname=\"nn\", task=\"regression\"):  \n    \"\"\"\n    get oof and target predictions\n    \"\"\"\n    sklearns = [\"xgb\", \"catb\", \"linear\", \"knn\"]\n    if task == \"multiclass\":\n        sklearns.append(\"lgb\")\n\n    if task == \"binary\": # classification\n        # sklearn API\n        if modelname in sklearns:\n            oof_pred = model.predict_proba(x_val)\n            y_pred = model.predict_proba(x_test)\n            oof_pred = oof_pred[:, 1]\n            y_pred = y_pred[:, 1]\n        else:\n            oof_pred = model.predict(x_val)\n            y_pred = model.predict(x_test)\n\n            # NN specific\n            if modelname == \"nn\":\n                oof_pred = oof_pred.ravel()\n                y_pred = y_pred.ravel()        \n\n    elif task == \"multiclass\":\n        # sklearn API\n        if modelname in sklearns:\n            oof_pred = model.predict_proba(x_val)\n            y_pred = model.predict_proba(x_test)\n        else:\n            oof_pred = model.predict(x_val)\n            y_pred = model.predict(x_test)\n\n        # oof_pred = np.argmax(oof_pred, axis=1)\n        # y_pred = np.argmax(y_pred, axis=1)\n\n    elif task == \"regression\": # regression\n        oof_pred = model.predict(x_val)\n        y_pred = model.predict(x_test)\n\n        # NN specific\n        if modelname == \"nn\":\n            oof_pred = oof_pred.ravel()\n            y_pred = y_pred.ravel()\n            \n    elif task == 'custom':\n        oof_pred = model.predict(x_val)\n        y_pred = model.predict(x_test)\n\n    return oof_pred, y_pred","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"import math\nimport random\nfrom typing import List, NoReturn, Union, Tuple, Optional, Text, Generic, Callable, Dict\n\ndef seed_everything(seed : int) -> NoReturn :    \n    random.seed(seed)\n    np.random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    tf.random.set_seed(seed)\n    \nseed_everything(SEED)\n\ndef nn_model(cls, train_set, val_set):\n    \"\"\"\n    NN hyperparameters and models\n    \"\"\"\n\n    # set seed for tf\n    seed_everything(cls.seed)\n\n    # adapted from https://github.com/ghmagazine/kagglebook/blob/master/ch06/ch06-03-hopt_nn.py\n    if not cls.params:\n        params = {\n            'input_dropout': 0.0,\n            'hidden_layers': 3,\n            'hidden_units': 256,\n            'embedding_out_dim': 4,\n            'hidden_activation': 'relu', \n            'hidden_dropout': 0.04,\n            'gauss_noise': 0.01,\n            'norm_type': 'batch', # layer\n            'optimizer': {'type': 'adam', 'lr': 1e-3},\n            'batch_size': 256,\n            'epochs': 80\n        }\n        cls.params = params\n\n    # NN model architecture\n    inputs = []\n    n_neuron = cls.params['hidden_units']\n\n    # embedding for categorical features \n    if len(cls.categoricals) > 0:\n        embeddings = []\n        embedding_out_dim = cls.params['embedding_out_dim']\n        for i in cls.categoricals:\n            input_ = layers.Input(shape=(1,))\n            embedding = layers.Embedding(int(np.absolute(cls.train_df[i]).max() + 1), embedding_out_dim, input_length=1)(input_)\n            embedding = layers.Reshape(target_shape=(embedding_out_dim,))(embedding)\n            inputs.append(input_)\n            embeddings.append(embedding)\n        input_numeric = layers.Input(shape=(len(cls.features) - len(cls.categoricals),))\n        embedding_numeric = layers.Dense(n_neuron, activation=cls.params['hidden_activation'])(input_numeric)\n        inputs.append(input_numeric)\n        embeddings.append(embedding_numeric)\n        x = layers.Concatenate()(embeddings)\n\n    else: # no categorical features\n        inputs = layers.Input(shape=(len(cls.features), ))\n        x = layers.Dense(n_neuron, activation=cls.params['hidden_activation'])(inputs)\n        x = layers.Dropout(cls.params['hidden_dropout'])(x)\n        x = layers.GaussianNoise(cls.params['gauss_noise'])(x)\n        if cls.params['norm_type'] == 'batch':\n            x = layers.BatchNormalization()(x)\n        elif cls.params['norm_type'] == 'layer':\n            x = layers.LayerNormalization()(x)\n        \n    # more layers\n    for i in np.arange(cls.params['hidden_layers'] - 1):\n        x = layers.Dense(n_neuron // (2 * (i+1)), activation=cls.params['hidden_activation'])(x)\n        x = layers.Dropout(cls.params['hidden_dropout'])(x)\n        x = layers.GaussianNoise(cls.params['gauss_noise'])(x)\n        if cls.params['norm_type'] == 'batch':\n            x = layers.BatchNormalization()(x)\n        elif cls.params['norm_type'] == 'layer':\n            x = layers.LayerNormalization()(x)\n    \n    # output\n    if cls.task == \"regression\":\n        out = layers.Dense(1, activation=\"linear\", name = \"out\")(x)\n        loss = \"mse\"\n    elif cls.task == \"binary\":\n        out = layers.Dense(1, activation='sigmoid', name = 'out')(x)\n        loss = \"binary_crossentropy\"\n    elif cls.task == \"multiclass\":\n        out = layers.Dense(len(np.unique(cls.train_df[cls.target].values)), activation='softmax', name = 'out')(x)\n        loss = \"categorical_crossentropy\"\n    elif cls.task == \"custom\":\n        out = layers.Dense(len(target_cols), activation='linear', name = 'out')(x)\n        loss = MCRMSE\n            \n    model = models.Model(inputs=inputs, outputs=out)\n\n    # compile\n    if cls.params['optimizer']['type'] == 'adam':\n        model.compile(loss=loss, optimizer=optimizers.Adam(lr=cls.params['optimizer']['lr'], beta_1=0.9, beta_2=0.999, decay=cls.params['optimizer']['lr']/100))\n    elif cls.params['optimizer']['type'] == 'sgd':\n        model.compile(loss=loss, optimizer=optimizers.SGD(lr=cls.params['optimizer']['lr'], decay=1e-6, momentum=0.9))\n\n    # callbacks\n    early_stop = callbacks.EarlyStopping(patience=8, min_delta=cls.params['optimizer']['lr'], restore_best_weights=True, monitor='val_loss')\n    lr_schedule = callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=8, verbose=1, epsilon=cls.params['optimizer']['lr'], mode='min')\n    history = model.fit(train_set['X'], train_set['y'], callbacks=[early_stop, lr_schedule],\n                        epochs=cls.params['epochs'], batch_size=cls.params['batch_size'],\n                        validation_data=(val_set['X'], val_set['y']))        \n        \n    fi = np.zeros(len(cls.features)) # no feature importance computed\n\n    return model, fi","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"class RunModel(object):\n    \"\"\"\n    Model Fitting and Prediction Class:\n    :INPUTS:\n    :train_df: train pandas dataframe\n    :test_df: test pandas dataframe\n    :target: target column name (str)\n    :features: list of feature names\n    :categoricals: list of categorical feature names. Note that categoricals need to be in 'features'\n    :model: 'lgb', 'xgb', 'catb', 'linear', or 'nn'\n    :params: dictionary of hyperparameters. If empty dict {} is given, default hyperparams are used\n    :task: 'regression', 'multiclass', or 'binary'\n    :n_splits: K in KFold (default is 4)\n    :cv_method: 'KFold', 'StratifiedKFold', 'TimeSeriesSplit', 'GroupKFold', 'StratifiedGroupKFold'\n    :group: group feature name when GroupKFold or StratifiedGroupKFold are used\n    :target_encoding: True or False\n    :seed: seed (int)\n    :scaler: None, 'MinMax', 'Standard'\n    :verbose: bool\n    :EXAMPLE:\n    # fit LGB regression model\n    model = RunModel(train_df, test_df, target, features, categoricals=categoricals,\n            model=\"lgb\", params={}, task=\"regression\", n_splits=4, cv_method=\"KFold\", \n            group=None, target_encoding=False, seed=1220, scaler=None)\n    \n    # save predictions on train, test data\n    np.save(\"y_pred\", model.y_pred)\n    np.save(\"oof\", model.oof)\n    \"\"\"\n\n    def __init__(self, train_df : pd.DataFrame, test_df : pd.DataFrame, target : str, features : List, categoricals: List=[],\n                model : str=\"lgb\", params : Dict={}, task : str=\"regression\", n_splits : int=4, cv_method : str=\"KFold\", \n                group : str=None, target_encoding=False, seed : int=1220, scaler : str=None, verbose=True):\n\n        # display info\n        print(\"##############################\")\n        print(f\"Starting training model {model} for a {task} task:\")\n        print(f\"- train records: {len(train_df)}, test records: {len(test_df)}\")\n        print(f\"- target column is {target}\")\n        print(f\"- {len(features)} features with {len(categoricals)} categorical features\")\n        if target_encoding:\n            print(f\"- target encoding: Applied\")\n        else:\n            print(f\"- target encoding: NOT Applied\")\n        print(f\"- CV strategy : {cv_method} with {n_splits} splits\")\n        if group is None:\n            print(f\"- no group parameter is used for validation\")\n        else:\n            print(f\"- {group} as group parameter\")\n        if scaler is None:\n            print(\"- No scaler is used\")\n        else:\n            print(f\"- {scaler} scaler is used\")\n        print(\"##############################\")\n\n        # class initializing setups\n        self.train_df = train_df\n        self.test_df = test_df\n        self.target = target\n        self.features = features\n        self.categoricals = categoricals\n        self.model = model\n        self.params = params\n        self.task = task\n        self.n_splits = n_splits\n        self.cv_method = cv_method\n        self.group = group\n        self.target_encoding = target_encoding\n        self.seed = seed\n        self.scaler = scaler\n        self.verbose = verbose\n        self.y_pred, self.score, self.model, self.oof, self.y_val, self.fi_df = self.fit()\n\n    def train_model(self, train_set, val_set):\n        \"\"\"\n        employ a model\n        \"\"\"\n        # compile model\n        if self.model == \"lgb\": # LGB             \n            model, fi = lgb_model(self, train_set, val_set)\n\n        elif self.model == \"xgb\": # xgb\n            model, fi = xgb_model(self, train_set, val_set)\n\n        elif self.model == \"catb\": # catboost\n            model, fi = catb_model(self, train_set, val_set)\n\n        elif self.model == \"linear\": # linear model\n            model, fi = lin_model(self, train_set, val_set)\n\n        elif self.model == \"nn\": # neural network\n            model, fi = nn_model(self, train_set, val_set)\n        \n        return model, fi # fitted model and feature importance\n\n    def convert_dataset(self, x_train, y_train, x_val, y_val):\n        \"\"\"\n        dataset converter\n        \"\"\"\n        if (self.model == \"lgb\") & (self.task != \"multiclass\"):\n            train_set = lgb.Dataset(x_train, y_train, categorical_feature=self.categoricals)\n            val_set = lgb.Dataset(x_val, y_val, categorical_feature=self.categoricals)\n            \n        elif (self.model == \"nn\") & (self.task == \"multiclass\"):\n            ohe = OneHotEncoder(sparse=False, categories='auto')\n            train_set = {'X': x_train, 'y': ohe.fit_transform(y_train.values.reshape(-1, 1))}\n            val_set = {'X': x_val, 'y': ohe.transform(y_val.values.reshape(-1, 1))}\n            \n        else:\n            train_set = {'X': x_train, 'y': y_train}\n            val_set = {'X': x_val, 'y': y_val}\n            \n        return train_set, val_set\n\n    def calc_metric(self, y_true, y_pred): \n        \"\"\"\n        calculate evaluation metric for each task\n        this may need to be changed based on the metric of interest\n        \"\"\"\n        if self.task == \"multiclass\":\n            return f1_score(y_true, y_pred, average=\"macro\")\n        \n        elif self.task == \"binary\":\n            return roc_auc_score(y_true, y_pred) # log_loss\n        \n        elif self.task == \"regression\":\n            return np.sqrt(mean_squared_error(y_true, y_pred))\n        \n        elif self.task == \"custom\":     \n            return mcrmse(y_true, y_pred)\n\n    def get_cv(self):\n        \"\"\"\n        employ CV strategy\n        \"\"\"\n\n        # return cv.split\n        if self.cv_method == \"KFold\":\n            cv = KFold(n_splits=self.n_splits, shuffle=True, random_state=self.seed)\n            return cv.split(self.train_df)\n        \n        elif self.cv_method == \"StratifiedKFold\":\n            cv = StratifiedKFold(n_splits=self.n_splits, shuffle=True, random_state=self.seed)\n            return cv.split(self.train_df, self.train_df[self.target])\n        \n        elif self.cv_method == \"TimeSeriesSplit\":\n            cv = TimeSeriesSplit(max_train_size=None, n_splits=self.n_splits)\n            return cv.split(self.train_df)\n        \n        elif self.cv_method == \"GroupKFold\":\n            cv = GroupKFold(n_splits=self.n_splits, shuffle=True, random_state=self.seed)\n            return cv.split(self.train_df, self.train_df[self.target], self.group)\n        \n        elif self.cv_method == \"StratifiedGroupKFold\":\n            cv = StratifiedGroupKFold(n_splits=self.n_splits, shuffle=True, random_state=self.seed)\n            return cv.split(self.train_df, self.train_df[self.target], self.group)\n\n    def fit(self):\n        \"\"\"\n        perform model fitting        \n        \"\"\"\n\n        # initialize\n        y_vals = np.zeros((self.train_df.shape[0], ))\n        if self.task  == \"multiclass\":\n            n_class = len(self.target)\n            oof_pred = np.zeros((self.train_df.shape[0], n_class))\n            y_pred = np.zeros((self.test_df.shape[0], n_class))\n            \n        elif self.task == 'custom':\n            n_class = len(target_cols)\n            y_vals = np.zeros((self.train_df.shape[0], n_class))\n            oof_pred = np.zeros((self.train_df.shape[0], n_class))\n            y_pred = np.zeros((self.test_df.shape[0], n_class))           \n                \n        else:\n            oof_pred = np.zeros((self.train_df.shape[0], ))\n            y_pred = np.zeros((self.test_df.shape[0], ))\n\n        # group does not kick in when group k fold is used\n        if self.group is not None:\n            if self.group in self.features:\n                self.features.remove(self.group)\n            if self.group in self.categoricals:\n                self.categoricals.remove(self.group)\n        fi = np.zeros((self.n_splits, len(self.features)))\n\n        # target encoding\n        numerical_features = [f for f in self.features if f not in self.categoricals]\n        if self.target_encoding:            \n            # perform target encoding\n            k = 0\n            f = 1\n            overall_mean = self.train_df[self.target].mean()\n            for c in self.categoricals:\n                data_tmp = pd.DataFrame({c: self.train_df[c].values, 'target': self.train_df[self.target].values})\n                tmp = np.nan * np.ones(self.train_df.shape[0])\n                \n                cv = self.get_cv()\n                for fold, (train_idx, val_idx) in enumerate(cv):\n                    # target mean\n                    target_mean = data_tmp.iloc[train_idx].groupby(c)['target'].mean().reset_index() \n                    \n                    # smoothing\n                    target_count = data_tmp.iloc[train_idx].groupby(c)['target'].count().reset_index() \n                    target_count['target'] = target_count['target'].apply(lambda x : 1 / (1 + np.exp((-x-k) / f)))\n                    target_mean['target'] = target_mean['target'] * target_count['target'] + (1 - target_count['target']) * overall_mean\n\n                    # allocate\n                    tmp[val_idx] = self.train_df[c].iloc[val_idx].map(target_mean.to_dict()).values\n                self.train_df[c] = tmp\n                \n                # replace categorical variable in test\n                target_mean = data_tmp.groupby(c)['target'].mean()\n                self.test_df.loc[:, c] = self.test_df[c].map(target_mean).values\n            \n            # no categoricals any more\n            numerical_features = self.features.copy()\n            self.categoricals = []\n        \n        # fill nan\n        if self.model not in ['lgb', 'catb', 'xgb']:\n            # fill NaN (numerical features -> median, categorical features -> mode)\n            self.train_df[numerical_features] = self.train_df[numerical_features].replace([np.inf, -np.inf], np.nan)\n            self.test_df[numerical_features] = self.test_df[numerical_features].replace([np.inf, -np.inf], np.nan)\n            self.train_df[numerical_features] = self.train_df[numerical_features].fillna(self.train_df[numerical_features].median())\n            self.test_df[numerical_features] = self.test_df[numerical_features].fillna(self.test_df[numerical_features].median())\n            self.train_df[self.categoricals] = self.train_df[self.categoricals].fillna(self.train_df[self.categoricals].mode().iloc[0])\n            self.test_df[self.categoricals] = self.test_df[self.categoricals].fillna(self.test_df[self.categoricals].mode().iloc[0])\n      \n        # scaling, if necessary\n        if self.scaler is not None:\n            # to normal\n            pt = QuantileTransformer(n_quantiles=100, random_state=self.seed, output_distribution=\"normal\")\n            self.train_df[numerical_features] = pt.fit_transform(self.train_df[numerical_features])\n            self.test_df[numerical_features] = pt.transform(self.test_df[numerical_features])\n\n            # starndardize\n            if self.scaler == \"MinMax\":\n                scaler = MinMaxScaler()\n            elif self.scaler == \"Standard\":\n                scaler = StandardScaler()\n            self.train_df[numerical_features] = scaler.fit_transform(self.train_df[numerical_features])\n            self.test_df[numerical_features] = scaler.transform(self.test_df[numerical_features])\n\n            x_test = self.test_df.copy()\n            if self.model == \"nn\":\n                x_test = [np.absolute(x_test[i]) for i in self.categoricals] + [x_test[numerical_features]]\n            else:\n                x_test = x_test[self.features]\n        else:\n            x_test = self.test_df[self.features]\n        \n        # fitting with out of fold\n        cv = self.get_cv()\n        for fold, (train_idx, val_idx) in enumerate(cv):\n            # train test split\n            x_train, x_val = self.train_df[self.features].iloc[train_idx], self.train_df[self.features].iloc[val_idx]\n            y_train, y_val = self.train_df[self.target].iloc[train_idx], self.train_df[self.target].iloc[val_idx]\n\n            if self.model == \"nn\":\n                x_train = [np.absolute(x_train[i]) for i in self.categoricals] + [x_train[numerical_features]]\n                x_val = [np.absolute(x_val[i]) for i in self.categoricals] + [x_val[numerical_features]]\n\n            # model fitting\n            train_set, val_set = self.convert_dataset(x_train, y_train, x_val, y_val)\n            model, importance = self.train_model(train_set, val_set)\n            fi[fold, :] = importance\n            y_vals[val_idx] = y_val\n\n            # predictions and check cv score\n            oofs, ypred = get_oof_ypred(model, x_val, x_test, self.model, self.task)\n            y_pred += ypred.reshape(y_pred.shape) / self.n_splits\n            if self.task == \"multiclass\":\n                oof_pred[val_idx, :] = oofs.reshape(oof_pred[val_idx, :].shape)\n                print('Partial score of fold {} is: {}'.format(fold, self.calc_metric(y_vals[val_idx], \n                    np.argmax(oof_pred[val_idx, :], axis=1))))\n            elif self.task == 'custom':\n                oof_pred[val_idx, :] = oofs.reshape(oof_pred[val_idx, :].shape)   \n                print('Partial score of fold {} is: {}'.format(fold, self.calc_metric(y_vals[val_idx, :], \n                    oof_pred[val_idx, :])))\n            else:\n                oof_pred[val_idx] = oofs.reshape(oof_pred[val_idx].shape)\n                print('Partial score of fold {} is: {}'.format(fold, self.calc_metric(y_vals[val_idx], \n                    oof_pred[val_idx])))\n\n        # feature importance data frame\n        fi_df = pd.DataFrame()\n        for n in np.arange(self.n_splits):\n            tmp = pd.DataFrame()\n            tmp[\"features\"] = self.features\n            tmp[\"importance\"] = fi[n, :]\n            tmp[\"fold\"] = n\n            fi_df = pd.concat([fi_df, tmp], ignore_index=True)\n        gfi = fi_df[[\"features\", \"importance\"]].groupby([\"features\"]).mean().reset_index()\n        fi_df = fi_df.merge(gfi, on=\"features\", how=\"left\", suffixes=('', '_mean'))\n\n        # outputs\n        if self.task == \"multiclass\":\n            loss_score = self.calc_metric(y_vals, np.argmax(oof_pred, axis=1))\n        else:\n            loss_score = self.calc_metric(y_vals, oof_pred)\n\n        if self.verbose:\n            print('Our oof loss score is: ', loss_score)\n        return y_pred, loss_score, model, oof_pred, y_vals, fi_df\n\n    def plot_feature_importance(self, rank_range=[1, 50]):\n        \"\"\"\n        function for plotting feature importance (nothing is returned when the model is NN)\n        :EXAMPLE:\n        # fit LGB regression model\n        model = RunModel(train_df, test_df, target, features, categoricals=categoricals,\n                model=\"lgb\", task=\"regression\", n_splits=4, cv_method=\"KFold\", \n                group=None, seed=1220, scaler=None)\n        \n        # plot \n        fi_df = model.plot_feature_importance(rank_range=[1, 100])\n        \n        \"\"\"\n        # plot feature importance\n        _, ax = plt.subplots(1, 1, figsize=(10, 20))\n        sorted_df = self.fi_df.sort_values(by = \"importance_mean\", ascending=False).reset_index().iloc[self.n_splits * (rank_range[0]-1) : self.n_splits * rank_range[1]]\n        sns.barplot(data=sorted_df, x =\"importance\", y =\"features\", orient='h')\n        ax.set_xlabel(\"feature importance\")\n        ax.spines['top'].set_visible(False)\n        ax.spines['right'].set_visible(False)\n        return sorted_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Fitting"},{"metadata":{"trusted":true},"cell_type":"code","source":"not_use_cols = ['id', 'id_seqpos']\nfeatures = [c for c in test_data.columns if c not in not_use_cols]\nprint(features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"categoricals = [f for f in test_data.columns if ('sequence' in f) | ('loop_type' in f)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mymodel = RunModel(train_data, test_data, target_cols, features, categoricals=categoricals,\n            model=\"nn\", params={}, task=\"custom\", n_splits=NFOLD, cv_method=\"GroupKFold\", \n            group='id', target_encoding=False, seed=SEED, scaler=SCALER)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# OOF, Submit files"},{"metadata":{"trusted":true},"cell_type":"code","source":"oof_df = pd.DataFrame(train_data.id_seqpos)\nfor i, target in enumerate(target_cols):\n    oof_df[target] = mymodel.oof[:, i]\n    submission[target] = mymodel.y_pred[:, i]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"oof_df.to_csv('oof_df.csv', index=False)\nsubmission.to_csv('submission.csv', index=False)\nprint('saved!')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The score is also not super bad given that this is a simple LGB model. This approach can be used for ensemble along with other modeling approaches such as RNN (Recurrent Neural Network) and GNN (Graphical Neural Network)."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}