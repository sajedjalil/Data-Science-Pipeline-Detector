{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Introduction\nAccording to this discussion (https://www.kaggle.com/c/stanford-covid-vaccine/discussion/182021#1006800),\n\n>The bpps are numpy arrays we pre-calculated for each sequence. hidehisaarai1213 is right; they're matrices of base pair probabilities, calculated using a recently developed algorithm in our lab.\nWhat you use them for is totally up to you. Biophysically speaking, this matrix gives the probability that each pair of nucleotides in the RNA forms a base pair (given a particular model of RNA folding). You've probably already seen the structural features: imagine that this matrix describes the whole distribution from which one could sample more structures.\nAt the simplest level -- it's a symmetric square matrix with the same length as the sequence, so you can get N more features out of it, if you want them. Each column and each row should sum to one (up to rounding error), but more than one entry in each column/row will be nonzero -- usually somewhere between 1-5 entries.\nFeel free to use them or ignore them based on your own judgement; we pre-generated them because it would take a little specialized knowledge to do so, as well as a little time (a bit under a second for each sequence). At the same time, feel free to generate and share other biophysically inspired features of your own, if you suspect they might be helpful!\n\nIt seems that many public kernel doã€€not use bpps at least explicitly. Here I try to include it as a simple feature and found that it has relatively high feature importance in a tree-based model, at least among other included features. This experiment suggests that using bpps somehow in your model may improve your performance."},{"metadata":{},"cell_type":"markdown","source":"# Libraries"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import gc\nimport os\nimport random\n\nimport lightgbm as lgb\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport itertools\nfrom pathlib import Path\nfrom tqdm import tqdm\n\nfrom matplotlib import pyplot as plt\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import StratifiedKFold, KFold, GroupKFold\nfrom sklearn.cluster import KMeans\nfrom sklearn.decomposition import PCA\n\nfrom typing import List, NoReturn, Union, Tuple, Optional, Text, Generic, Callable, Dict\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder, QuantileTransformer\nfrom sklearn.model_selection import KFold, StratifiedKFold, TimeSeriesSplit\nfrom sklearn.metrics import accuracy_score, roc_auc_score, log_loss, mean_squared_error, mean_absolute_error, f1_score\n\n# model\nimport lightgbm as lgb\n\n# visualize\nimport matplotlib.pyplot as plt\nimport matplotlib.style as style\nimport seaborn as sns\nfrom matplotlib import pyplot\nfrom matplotlib.ticker import ScalarFormatter\nsns.set_context(\"talk\")\nstyle.use('seaborn-colorblind')\n\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# CONFIG"},{"metadata":{"trusted":true},"cell_type":"code","source":"SEED = 42\nNFOLD = 7\nSHIFTS = [1, 2, 3, 4, 5, 6]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Modeling pipeline"},{"metadata":{"trusted":true},"cell_type":"code","source":"#target columns\ntarget_cols = ['reactivity', 'deg_Mg_pH10', 'deg_pH10', 'deg_Mg_50C', 'deg_50C']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"import random\nfrom collections import Counter, defaultdict\nfrom sklearn import model_selection\n\n# ---- GroupKFold ----\nclass GroupKFold(object):\n    \"\"\"\n    GroupKFold with random shuffle with a sklearn-like structure\n    \"\"\"\n\n    def __init__(self, n_splits=4, shuffle=True, random_state=42):\n        self.n_splits = n_splits\n        self.shuffle = shuffle\n        self.random_state = random_state\n\n    def get_n_splits(self, X=None, y=None, group=None):\n        return self.n_splits\n\n    def split(self, X, y, group):\n        kf = model_selection.KFold(n_splits=self.n_splits, shuffle=self.shuffle, random_state=self.random_state)\n        unique_ids = X[group].unique()\n        for fold, (tr_group_idx, va_group_idx) in enumerate(kf.split(unique_ids)):\n            # split group\n            tr_group, va_group = unique_ids[tr_group_idx], unique_ids[va_group_idx]\n            train_idx = np.where(X[group].isin(tr_group))[0]\n            val_idx = np.where(X[group].isin(va_group))[0]\n            yield train_idx, val_idx\n\n# ---- StratifiedGroupKFold ----\nclass StratifiedGroupKFold(object):\n    \"\"\"\n    StratifiedGroupKFold with random shuffle with a sklearn-like structure\n    \"\"\"\n\n    def __init__(self, n_splits=4, shuffle=True, random_state=42):\n        self.n_splits = n_splits\n        self.shuffle = shuffle\n        self.random_state = random_state\n\n    def get_n_splits(self, X=None, y=None, group=None):\n        return self.n_splits\n\n    def split(self, X, y, group):\n        labels_num = np.max(y) + 1\n        y_counts_per_group = defaultdict(lambda: np.zeros(labels_num))\n        y_distr = Counter()\n        groups = X[group].values\n        for label, g in zip(y, groups):\n            y_counts_per_group[g][label] += 1\n            y_distr[label] += 1\n\n        y_counts_per_fold = defaultdict(lambda: np.zeros(labels_num))\n        groups_per_fold = defaultdict(set)\n\n        def eval_y_counts_per_fold(y_counts, fold):\n            y_counts_per_fold[fold] += y_counts\n            std_per_label = []\n            for label in range(labels_num):\n                label_std = np.std([y_counts_per_fold[i][label] / y_distr[label] for i in range(self.n_splits)])\n                std_per_label.append(label_std)\n            y_counts_per_fold[fold] -= y_counts\n            return np.mean(std_per_label)\n        \n        groups_and_y_counts = list(y_counts_per_group.items())\n        random.Random(self.random_state).shuffle(groups_and_y_counts)\n\n        for g, y_counts in sorted(groups_and_y_counts, key=lambda x: -np.std(x[1])):\n            best_fold = None\n            min_eval = None\n            for i in range(self.n_splits):\n                fold_eval = eval_y_counts_per_fold(y_counts, i)\n                if min_eval is None or fold_eval < min_eval:\n                    min_eval = fold_eval\n                    best_fold = i\n            y_counts_per_fold[best_fold] += y_counts\n            groups_per_fold[best_fold].add(g)\n\n        all_groups = set(groups)\n        for i in range(self.n_splits):\n            train_groups = all_groups - groups_per_fold[i]\n            test_groups = groups_per_fold[i]\n\n            train_idx = [i for i, g in enumerate(groups) if g in train_groups]\n            test_idx = [i for i, g in enumerate(groups) if g in test_groups]\n\n            yield train_idx, test_idx","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def get_oof_ypred(model, x_val, x_test, modelname=\"lgb\", task=\"regression\"):  \n    \"\"\"\n    get oof and target predictions\n    \"\"\"\n    sklearns = [\"xgb\", \"catb\", \"linear\", \"knn\"]\n    if task == \"multiclass\":\n        sklearns.append(\"lgb\")\n\n    if task == \"binary\": # classification\n        # sklearn API\n        if modelname in sklearns:\n            oof_pred = model.predict_proba(x_val)\n            y_pred = model.predict_proba(x_test)\n            oof_pred = oof_pred[:, 1]\n            y_pred = y_pred[:, 1]\n        else:\n            oof_pred = model.predict(x_val)\n            y_pred = model.predict(x_test)\n\n            # NN specific\n            if modelname == \"nn\":\n                oof_pred = oof_pred.ravel()\n                y_pred = y_pred.ravel()        \n\n    elif task == \"multiclass\":\n        # sklearn API\n        if modelname in sklearns:\n            oof_pred = model.predict_proba(x_val)\n            y_pred = model.predict_proba(x_test)\n        else:\n            oof_pred = model.predict(x_val)\n            y_pred = model.predict(x_test)\n\n        # oof_pred = np.argmax(oof_pred, axis=1)\n        # y_pred = np.argmax(y_pred, axis=1)\n\n    elif task == \"regression\": # regression\n        oof_pred = model.predict(x_val)\n        y_pred = model.predict(x_test)\n\n        # NN specific\n        if modelname == \"nn\":\n            oof_pred = oof_pred.ravel()\n            y_pred = y_pred.ravel()\n\n    return oof_pred, y_pred","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def lgb_model(cls, train_set, val_set):\n    \"\"\"\n    LightGBM hyperparameters and models\n    \"\"\"\n\n    # verbose\n    verbosity = 100 if cls.verbose else 0\n\n    # list is here: https://lightgbm.readthedocs.io/en/latest/Parameters.html\n    if not cls.params:    \n        params = {\n                    'n_estimators': 24000,\n                    'objective': cls.task,\n                    'boosting_type': 'gbdt',\n                    'max_depth': 7,\n                    'learning_rate': 0.01,\n                    'subsample': 0.72,\n                    'subsample_freq': 4,\n                    'feature_fraction': 0.4,\n                    'lambda_l1': 1,\n                    'lambda_l2': 1,\n                    'seed': cls.seed,\n                    'early_stopping_rounds': 80,\n                    }    \n        if cls.task == \"regression\":\n            params[\"metric\"] = \"rmse\"\n        elif cls.task == \"binary\":\n            params[\"metric\"] = \"auc\" # other candidates: binary_logloss\n            params[\"is_unbalance\"] = True # assume unbalanced data\n        elif cls.task == \"multiclass\":\n            params[\"metric\"] = \"multi_logloss\" # other candidates: cross_entropy, auc_mu\n            params[\"num_class\"] = len(np.unique(cls.train_df[cls.target].values))\n            params[\"class_weight\"] = 'balanced' # assume unbalanced data\n        cls.params = params\n\n    # modeling and feature importance\n    if cls.task == \"multiclass\": # sklearn API for 'class_weight' implementation\n        model = lgb.LGBMClassifier(**cls.params)\n        model.fit(train_set['X'], train_set['y'], eval_set=[(val_set['X'], val_set['y'])],\n            verbose=verbosity, categorical_feature=cls.categoricals)\n        fi = model.booster_.feature_importance(importance_type=\"gain\")\n    else: # python API for efficient memory usage\n        model = lgb.train(cls.params, train_set, valid_sets=[train_set, val_set], verbose_eval=verbosity)\n        fi = model.feature_importance(importance_type=\"gain\")\n\n    return model, fi","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"class RunModel(object):\n    \"\"\"\n    Model Fitting and Prediction Class:\n    :INPUTS:\n    :train_df: train pandas dataframe\n    :test_df: test pandas dataframe\n    :target: target column name (str)\n    :features: list of feature names\n    :categoricals: list of categorical feature names. Note that categoricals need to be in 'features'\n    :model: 'lgb', 'xgb', 'catb', 'linear', or 'nn'\n    :params: dictionary of hyperparameters. If empty dict {} is given, default hyperparams are used\n    :task: 'regression', 'multiclass', or 'binary'\n    :n_splits: K in KFold (default is 4)\n    :cv_method: 'KFold', 'StratifiedKFold', 'TimeSeriesSplit', 'GroupKFold', 'StratifiedGroupKFold'\n    :group: group feature name when GroupKFold or StratifiedGroupKFold are used\n    :target_encoding: True or False\n    :seed: seed (int)\n    :scaler: None, 'MinMax', 'Standard'\n    :verbose: bool\n    :EXAMPLE:\n    # fit LGB regression model\n    model = RunModel(train_df, test_df, target, features, categoricals=categoricals,\n            model=\"lgb\", params={}, task=\"regression\", n_splits=4, cv_method=\"KFold\", \n            group=None, target_encoding=False, seed=1220, scaler=None)\n    \n    # save predictions on train, test data\n    np.save(\"y_pred\", model.y_pred)\n    np.save(\"oof\", model.oof)\n    \"\"\"\n\n    def __init__(self, train_df : pd.DataFrame, test_df : pd.DataFrame, target : str, features : List, categoricals: List=[],\n                model : str=\"lgb\", params : Dict={}, task : str=\"regression\", n_splits : int=4, cv_method : str=\"KFold\", \n                group : str=None, target_encoding=False, seed : int=1220, scaler : str=None, verbose=True):\n\n        # display info\n        print(\"##############################\")\n        print(f\"Starting training model {model} for a {task} task:\")\n        print(f\"- train records: {len(train_df)}, test records: {len(test_df)}\")\n        print(f\"- target column is {target}\")\n        print(f\"- {len(features)} features with {len(categoricals)} categorical features\")\n        if target_encoding:\n            print(f\"- target encoding: Applied\")\n        else:\n            print(f\"- target encoding: NOT Applied\")\n        print(f\"- CV strategy : {cv_method} with {n_splits} splits\")\n        if group is None:\n            print(f\"- no group parameter is used for validation\")\n        else:\n            print(f\"- {group} as group parameter\")\n        if scaler is None:\n            print(\"- No scaler is used\")\n        else:\n            print(f\"- {scaler} scaler is used\")\n        print(\"##############################\")\n\n        # class initializing setups\n        self.train_df = train_df\n        self.test_df = test_df\n        self.target = target\n        self.features = features\n        self.categoricals = categoricals\n        self.model = model\n        self.params = params\n        self.task = task\n        self.n_splits = n_splits\n        self.cv_method = cv_method\n        self.group = group\n        self.target_encoding = target_encoding\n        self.seed = seed\n        self.scaler = scaler\n        self.verbose = verbose\n        self.y_pred, self.score, self.model, self.oof, self.y_val, self.fi_df = self.fit()\n\n    def train_model(self, train_set, val_set):\n        \"\"\"\n        employ a model\n        \"\"\"\n        # compile model\n        if self.model == \"lgb\": # LGB             \n            model, fi = lgb_model(self, train_set, val_set)\n\n        elif self.model == \"xgb\": # xgb\n            model, fi = xgb_model(self, train_set, val_set)\n\n        elif self.model == \"catb\": # catboost\n            model, fi = catb_model(self, train_set, val_set)\n\n        elif self.model == \"linear\": # linear model\n            model, fi = lin_model(self, train_set, val_set)\n\n        elif self.model == \"nn\": # neural network\n            model, fi = nn_model(self, train_set, val_set)\n        \n        return model, fi # fitted model and feature importance\n\n    def convert_dataset(self, x_train, y_train, x_val, y_val):\n        \"\"\"\n        dataset converter\n        \"\"\"\n        if (self.model == \"lgb\") & (self.task != \"multiclass\"):\n            train_set = lgb.Dataset(x_train, y_train, categorical_feature=self.categoricals)\n            val_set = lgb.Dataset(x_val, y_val, categorical_feature=self.categoricals)\n            \n        elif (self.model == \"nn\") & (self.task == \"multiclass\"):\n            ohe = OneHotEncoder(sparse=False, categories='auto')\n            train_set = {'X': x_train, 'y': ohe.fit_transform(y_train.values.reshape(-1, 1))}\n            val_set = {'X': x_val, 'y': ohe.transform(y_val.values.reshape(-1, 1))}\n            \n        else:\n            train_set = {'X': x_train, 'y': y_train}\n            val_set = {'X': x_val, 'y': y_val}\n            \n        return train_set, val_set\n\n    def calc_metric(self, y_true, y_pred): \n        \"\"\"\n        calculate evaluation metric for each task\n        this may need to be changed based on the metric of interest\n        \"\"\"\n        if self.task == \"multiclass\":\n            return f1_score(y_true, y_pred, average=\"macro\")\n        \n        elif self.task == \"binary\":\n            return roc_auc_score(y_true, y_pred) # log_loss\n        \n        elif self.task == \"regression\":\n            return np.sqrt(mean_squared_error(y_true, y_pred))\n\n    def get_cv(self):\n        \"\"\"\n        employ CV strategy\n        \"\"\"\n\n        # return cv.split\n        if self.cv_method == \"KFold\":\n            cv = KFold(n_splits=self.n_splits, shuffle=True, random_state=self.seed)\n            return cv.split(self.train_df)\n        \n        elif self.cv_method == \"StratifiedKFold\":\n            cv = StratifiedKFold(n_splits=self.n_splits, shuffle=True, random_state=self.seed)\n            return cv.split(self.train_df, self.train_df[self.target])\n        \n        elif self.cv_method == \"TimeSeriesSplit\":\n            cv = TimeSeriesSplit(max_train_size=None, n_splits=self.n_splits)\n            return cv.split(self.train_df)\n        \n        elif self.cv_method == \"GroupKFold\":\n            cv = GroupKFold(n_splits=self.n_splits, shuffle=True, random_state=self.seed)\n            return cv.split(self.train_df, self.train_df[self.target], self.group)\n        \n        elif self.cv_method == \"StratifiedGroupKFold\":\n            cv = StratifiedGroupKFold(n_splits=self.n_splits, shuffle=True, random_state=self.seed)\n            return cv.split(self.train_df, self.train_df[self.target], self.group)\n\n    def fit(self):\n        \"\"\"\n        perform model fitting        \n        \"\"\"\n\n        # initialize\n        y_vals = np.zeros((self.train_df.shape[0], ))\n        if self.task == \"multiclass\":\n            n_class = len(np.unique(self.train_df[self.target].values))\n            oof_pred = np.zeros((self.train_df.shape[0], n_class))\n            y_pred = np.zeros((self.test_df.shape[0], n_class))\n        else:\n            oof_pred = np.zeros((self.train_df.shape[0], ))\n            y_pred = np.zeros((self.test_df.shape[0], ))\n\n        # group does not kick in when group k fold is used\n        if self.group is not None:\n            if self.group in self.features:\n                self.features.remove(self.group)\n            if self.group in self.categoricals:\n                self.categoricals.remove(self.group)\n        fi = np.zeros((self.n_splits, len(self.features)))\n\n        # target encoding\n        numerical_features = [f for f in self.features if f not in self.categoricals]\n        if self.target_encoding:            \n            # perform target encoding\n            k = 0\n            f = 1\n            overall_mean = self.train_df[self.target].mean()\n            for c in self.categoricals:\n                data_tmp = pd.DataFrame({c: self.train_df[c].values, 'target': self.train_df[self.target].values})\n                tmp = np.nan * np.ones(self.train_df.shape[0])\n                \n                cv = self.get_cv()\n                for fold, (train_idx, val_idx) in enumerate(cv):\n                    # target mean\n                    target_mean = data_tmp.iloc[train_idx].groupby(c)['target'].mean().reset_index() \n                    \n                    # smoothing\n                    target_count = data_tmp.iloc[train_idx].groupby(c)['target'].count().reset_index() \n                    target_count['target'] = target_count['target'].apply(lambda x : 1 / (1 + np.exp((-x-k) / f)))\n                    target_mean['target'] = target_mean['target'] * target_count['target'] + (1 - target_count['target']) * overall_mean\n\n                    # allocate\n                    tmp[val_idx] = self.train_df[c].iloc[val_idx].map(target_mean.to_dict()).values\n                self.train_df[c] = tmp\n                \n                # replace categorical variable in test\n                target_mean = data_tmp.groupby(c)['target'].mean()\n                self.test_df.loc[:, c] = self.test_df[c].map(target_mean).values\n            \n            # no categoricals any more\n            numerical_features = self.features.copy()\n            self.categoricals = []\n        \n        # fill nan\n        if self.model not in ['lgb', 'catb', 'xgb']:\n            # fill NaN (numerical features -> median, categorical features -> mode)\n            self.train_df[numerical_features] = self.train_df[numerical_features].replace([np.inf, -np.inf], np.nan)\n            self.test_df[numerical_features] = self.test_df[numerical_features].replace([np.inf, -np.inf], np.nan)\n            self.train_df[numerical_features] = self.train_df[numerical_features].fillna(self.train_df[numerical_features].median())\n            self.test_df[numerical_features] = self.test_df[numerical_features].fillna(self.test_df[numerical_features].median())\n            self.train_df[self.categoricals] = self.train_df[self.categoricals].fillna(self.train_df[self.categoricals].mode().iloc[0])\n            self.test_df[self.categoricals] = self.test_df[self.categoricals].fillna(self.test_df[self.categoricals].mode().iloc[0])\n      \n        # scaling, if necessary\n        if self.scaler is not None:\n            # to normal\n            pt = QuantileTransformer(n_quantiles=100, random_state=self.seed, output_distribution=\"normal\")\n            self.train_df[numerical_features] = pt.fit_transform(self.train_df[numerical_features])\n            self.test_df[numerical_features] = pt.transform(self.test_df[numerical_features])\n\n            # starndardize\n            if self.scaler == \"MinMax\":\n                scaler = MinMaxScaler()\n            elif self.scaler == \"Standard\":\n                scaler = StandardScaler()\n            self.train_df[numerical_features] = scaler.fit_transform(self.train_df[numerical_features])\n            self.test_df[numerical_features] = scaler.transform(self.test_df[numerical_features])\n\n            x_test = self.test_df.copy()\n            if self.model == \"nn\":\n                x_test = [np.absolute(x_test[i]) for i in self.categoricals] + [x_test[numerical_features]]\n            else:\n                x_test = x_test[self.features]\n        else:\n            x_test = self.test_df[self.features]\n        \n        # fitting with out of fold\n        cv = self.get_cv()\n        for fold, (train_idx, val_idx) in enumerate(cv):\n            # train test split\n            x_train, x_val = self.train_df[self.features].iloc[train_idx], self.train_df[self.features].iloc[val_idx]\n            y_train, y_val = self.train_df[self.target].iloc[train_idx], self.train_df[self.target].iloc[val_idx]\n\n            if self.model == \"nn\":\n                x_train = [np.absolute(x_train[i]) for i in self.categoricals] + [x_train[numerical_features]]\n                x_val = [np.absolute(x_val[i]) for i in self.categoricals] + [x_val[numerical_features]]\n\n            # model fitting\n            train_set, val_set = self.convert_dataset(x_train, y_train, x_val, y_val)\n            model, importance = self.train_model(train_set, val_set)\n            fi[fold, :] = importance\n            y_vals[val_idx] = y_val\n\n            # predictions and check cv score\n            oofs, ypred = get_oof_ypred(model, x_val, x_test, self.model, self.task)\n            y_pred += ypred.reshape(y_pred.shape) / self.n_splits\n            if self.task == \"multiclass\":\n                oof_pred[val_idx, :] = oofs.reshape(oof_pred[val_idx, :].shape)\n                print('Partial score of fold {} is: {}'.format(fold, self.calc_metric(y_vals[val_idx], \n                    np.argmax(oof_pred[val_idx, :], axis=1))))\n            else:\n                oof_pred[val_idx] = oofs.reshape(oof_pred[val_idx].shape)\n                print('Partial score of fold {} is: {}'.format(fold, self.calc_metric(y_vals[val_idx], \n                    oof_pred[val_idx])))\n\n        # feature importance data frame\n        fi_df = pd.DataFrame()\n        for n in np.arange(self.n_splits):\n            tmp = pd.DataFrame()\n            tmp[\"features\"] = self.features\n            tmp[\"importance\"] = fi[n, :]\n            tmp[\"fold\"] = n\n            fi_df = pd.concat([fi_df, tmp], ignore_index=True)\n        gfi = fi_df[[\"features\", \"importance\"]].groupby([\"features\"]).mean().reset_index()\n        fi_df = fi_df.merge(gfi, on=\"features\", how=\"left\", suffixes=('', '_mean'))\n\n        # outputs\n        if self.task == \"multiclass\":\n            loss_score = self.calc_metric(y_vals, np.argmax(oof_pred, axis=1))\n        else:\n            loss_score = self.calc_metric(y_vals, oof_pred)\n\n        if self.verbose:\n            print('Our oof loss score is: ', loss_score)\n        return y_pred, loss_score, model, oof_pred, y_vals, fi_df\n\n    def plot_feature_importance(self, rank_range=[1, 50]):\n        \"\"\"\n        function for plotting feature importance (nothing is returned when the model is NN)\n        :EXAMPLE:\n        # fit LGB regression model\n        model = RunModel(train_df, test_df, target, features, categoricals=categoricals,\n                model=\"lgb\", task=\"regression\", n_splits=4, cv_method=\"KFold\", \n                group=None, seed=1220, scaler=None)\n        \n        # plot \n        fi_df = model.plot_feature_importance(rank_range=[1, 100])\n        \n        \"\"\"\n        # plot feature importance\n        _, ax = plt.subplots(1, 1, figsize=(10, 20))\n        sorted_df = self.fi_df.sort_values(by = \"importance_mean\", ascending=False).reset_index().iloc[self.n_splits * (rank_range[0]-1) : self.n_splits * rank_range[1]]\n        sns.barplot(data=sorted_df, x =\"importance\", y =\"features\", orient='h')\n        ax.set_xlabel(\"feature importance\")\n        ax.spines['top'].set_visible(False)\n        ax.spines['right'].set_visible(False)\n        return sorted_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# load data"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"DATA_DIR = Path(\"../input/stanford-covid-vaccine/\")\nBPPS_DIR = DATA_DIR / \"bpps\"\n\ntrain = pd.read_json('../input/stanford-covid-vaccine/train.json',lines=True)\ntest = pd.read_json('../input/stanford-covid-vaccine/test.json', lines=True)\nsubmission = pd.read_csv('/kaggle/input/stanford-covid-vaccine/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"seq_feats = ['sequence', 'structure', 'predicted_loop_type', ]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(train['sequence'].iloc[0])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# preprocess"},{"metadata":{},"cell_type":"markdown","source":"I used snipets from https://www.kaggle.com/hidehisaarai1213/openvaccine-checkout-bpps. Thanks Hidehisa Arai for your great work. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# add bppm\ndef get_bppm(id_):\n    return np.load(BPPS_DIR / f\"{id_}.npy\")\n\ndef get_structure(structure: str):\n    pm = np.zeros((len(structure), len(structure)))\n    start_token_indices = []\n    for i, token in enumerate(structure):\n        if token == \"(\":\n            start_token_indices.append(i)\n        elif token == \")\":\n            j = start_token_indices.pop()\n            pm[i, j] = 1.0\n            pm[j, i] = 1.0\n    return pm\n\ndef plot_structures(bppm: np.ndarray, pm: np.ndarray):\n    fig, axes = plt.subplots(1, 2, figsize=(10, 10))\n    axes[0].imshow(bppm)\n    axes[0].set_title(\"BPPM\")\n    axes[1].imshow(pm)\n    axes[1].set_title(\"structure\")\n    plt.show()\n    \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Example\nidx = 0\nsample = train.loc[idx]\n\nbppm = get_bppm(sample.id)\npm = get_structure(sample.structure)\nplot_structures(bppm, pm)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can notice that although the structure of bpps look similar to that of 'structure' feature, there are slight differences. The 'structure' feature is a binary representation, whereas the bpps have continuous values. It may be possible that the bpps contain more rich information than the 'structure'. If this is the case, we might want to include it as a feature for modeling somehow. \n\nOne naive way to use bpps as a feature is to use its 1D representation by taking max along with its row or column (it does not matter which, as the structure is symmetric)."},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_bppm_feats(df, base_len=68):\n    bppm = np.zeros((df['id'].nunique(), base_len))\n    for i, id_ in tqdm(enumerate(df['id'].unique())):\n        img = get_bppm(id_)\n        maxv = np.max(img, axis=0)\n        if len(maxv) >= base_len:\n            bppm[i, :base_len] = maxv[:base_len]\n        else:\n            bppm[i, :len(maxv)] = maxv\n    return bppm\n\nbppm_train = get_bppm_feats(train)\nbppm_test = get_bppm_feats(test, test['seq_length'].max())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's format train and test for an ordinary tabular modeling task. I used snippets from https://www.kaggle.com/t88take/openvaccine-simple-lgb-baseline. Thanks T88 for your great work."},{"metadata":{"trusted":true},"cell_type":"code","source":"def format_train(train, bppm_train, shifts=SHIFTS, shift_cols=seq_feats):\n    train_data = []\n    for mol_id in tqdm(train['id'].unique()):\n        sample_data = train.loc[train['id'] == mol_id]\n        sample_seq_length = sample_data.seq_length.values[0]\n\n        for i in range(68):\n            sample_dict = {'id' : sample_data['id'].values[0],\n                           'id_seqpos' : sample_data['id'].values[0] + '_' + str(i),\n                          }\n            for f in [f for f in train.columns.values.tolist() if f not in ['index', 'id', 'signal_to_noise', 'SN_filter', 'seq_length', 'seq_scored',]]:\n                sample_dict[f] = sample_data[f].values[0][i]\n                \n            for shift,col in itertools.product(shifts, shift_cols):\n                if i - shift >= 0:\n                    sample_dict['b'+str(shift)+'_'+col] = sample_data[col].values[0][i-shift]\n                else:\n                    sample_dict['b'+str(shift)+'_'+col] = -1\n\n                if i + shift <= sample_seq_length - 1:\n                    sample_dict['a'+str(shift)+'_'+col] = sample_data[col].values[0][i+shift]\n                else:\n                    sample_dict['a'+str(shift)+'_'+col] = -1\n            \n            sample_dict['bppm'] = bppm_train[train['id'].unique() == mol_id, i][0]\n\n            train_data.append(sample_dict)\n    train_data = pd.DataFrame(train_data)\n    \n    return train_data\n\ntrain_data = format_train(train, bppm_train)\nprint(train_data.shape)\ntrain_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def format_test(test, bppm_test, shifts=SHIFTS, shift_cols=seq_feats):\n    test_data = []\n    for mol_id in tqdm(test['id'].unique()):\n        sample_data = test.loc[test['id'] == mol_id]\n        sample_seq_length = sample_data.seq_length.values[0]\n        for i in range(sample_seq_length):\n            sample_dict = {'id' : sample_data['id'].values[0],\n                           'id_seqpos' : sample_data['id'].values[0] + '_' + str(i),}\n            for f in seq_feats:\n                sample_dict[f] = sample_data[f].values[0][i]\n\n            for shift,col in itertools.product(shifts, shift_cols):\n                if i - shift >= 0:\n                    sample_dict['b'+str(shift)+'_'+col] = sample_data[col].values[0][i-shift]\n                else:\n                    sample_dict['b'+str(shift)+'_'+col] = -1\n\n                if i + shift <= sample_seq_length - 1:\n                    sample_dict['a'+str(shift)+'_'+col] = sample_data[col].values[0][i+shift]\n                else:\n                    sample_dict['a'+str(shift)+'_'+col] = -1\n            \n            sample_dict['bppm'] = bppm_test[test['id'].unique() == mol_id, i][0]\n\n            test_data.append(sample_dict)\n    test_data = pd.DataFrame(test_data)\n    \n    return test_data\n\ntest_data = format_test(test, bppm_test)\nprint(test_data.shape)\ntest_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# label_encoding\nsequence_encmap = {'A': 0, 'G' : 1, 'C' : 2, 'T': 3, 'U' : 4}\nstructure_encmap = {'.' : 0, '(' : 1, ')' : 2}\nlooptype_encmap = {'S':0, 'E':1, 'H':2, 'I':3, 'X':4, 'M':5, 'B':6}\nenc_maps = [sequence_encmap, structure_encmap, looptype_encmap]\n\nfor t,m in zip(seq_feats, enc_maps):\n    for c in [c for c in train_data.columns if t in c]:\n        try:\n            train_data[c] = train_data[c].replace(m)\n            test_data[c] = test_data[c].replace(m)\n        except:\n            print('ERR: already numeric; ' + c)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train_data.shape)\ntrain_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(test_data.shape)\ntest_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# save them for a rainy day\ntrain_data.to_feather('train_fe.feather')\ntest_data.to_feather('test_fe.feather')\nprint('Saved!')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Fitting"},{"metadata":{"trusted":true},"cell_type":"code","source":"not_use_cols = ['id', 'id_seqpos']\nfeatures = [c for c in test_data.columns if c not in not_use_cols]\nprint(features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"categoricals = []","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mymodels = []\noof_df = pd.DataFrame(train_data.id_seqpos)\nfor target in target_cols:\n    mymodel = RunModel(train_data, test_data, target, features, categoricals=categoricals,\n                model=\"lgb\", params={}, task=\"regression\", n_splits=NFOLD, cv_method=\"GroupKFold\", \n                group='id', target_encoding=False, seed=SEED, scaler=None)\n    oof_df[target] = mymodel.oof\n    submission[target] = mymodel.y_pred\n    mymodels.append(mymodel)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature importance"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(target_cols[0])\nfi_df = mymodels[0].plot_feature_importance(rank_range=[1, 20])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(target_cols[1])\nfi_df = mymodels[1].plot_feature_importance(rank_range=[1, 20])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(target_cols[2])\nfi_df = mymodels[2].plot_feature_importance(rank_range=[1, 20])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(target_cols[3])\nfi_df = mymodels[3].plot_feature_importance(rank_range=[1, 20])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(target_cols[4])\nfi_df = mymodels[4].plot_feature_importance(rank_range=[1, 20])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Surprise, surprise! Bpps seem to contain very rich information about our targets!"},{"metadata":{},"cell_type":"markdown","source":"# OOF, Submit files"},{"metadata":{"trusted":true},"cell_type":"code","source":"oof_df.to_csv('oof_df.csv', index=False)\nsubmission.to_csv('submission.csv', index=False)\nprint('saved!')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The score is also not super bad given that this is a simple LGB model. This approach can be used for ensemble along with other modeling approaches such as RNN (Recurrent Neural Network) and GNN (Graphical Neural Network)."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}