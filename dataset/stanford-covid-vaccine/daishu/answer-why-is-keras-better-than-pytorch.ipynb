{"cells":[{"metadata":{},"cell_type":"markdown","source":"I joined this competition due to [impressive kernel](https://www.kaggle.com/its7171/gru-lstm-with-feature-engineering-and-augmentation) last week. But the pain began since I transferred [Keras into Pytorch](https://www.kaggle.com/daishu/why-keras-is-better-than-pytorch).\n\nIn my experiments, CV is always worse(about 0.02) in Pytorch than Keras. Because the difference couldn't be found, I cannot sleep well, but tonight, it is a sweet dream."},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nimport random\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.nn import Linear, LayerNorm, ReLU, Dropout\nfrom sklearn.model_selection import StratifiedKFold\nfrom tqdm import tqdm\nimport os\nimport copy\nfrom sklearn.cluster import KMeans\nfrom sklearn.model_selection import StratifiedKFold, KFold, GroupKFold\nfrom torch.utils.data import Dataset,TensorDataset, DataLoader,RandomSampler\nimport time,datetime\nimport tensorflow as tf\nimport keras.backend as K\nimport tensorflow.keras.layers as L\n#gpu_options = tf.compat.v1.GPUOptions(allow_growth=True)\n#sess = tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(gpu_options=gpu_options))\n\nos.environ['CUDA_VISIBLE_DEVICES'] = '0'\ndef allocate_gpu_memory(gpu_number=0):\n    physical_devices = tf.config.experimental.list_physical_devices('GPU')\n\n    if physical_devices:\n        try:\n            print(\"Found {} GPU(s)\".format(len(physical_devices)))\n            tf.config.set_visible_devices(physical_devices[gpu_number], 'GPU')\n            tf.config.experimental.set_memory_growth(physical_devices[gpu_number], True)\n            print(\"#{} GPU memory is allocated\".format(gpu_number))\n        except RuntimeError as e:\n            print(e)\n    else:\n        print(\"Not enough GPU hardware devices available\")\nallocate_gpu_memory()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"token2int = {x:i for i, x in enumerate('().ACGUBEHIMSX')}\npred_cols = ['reactivity', 'deg_Mg_pH10', 'deg_pH10', 'deg_Mg_50C', 'deg_50C']\n\ndef rmse(y_actual, y_pred):\n    mse = tf.keras.losses.mean_squared_error(y_actual, y_pred)\n    return K.sqrt(mse)\n\ndef mcrmse(y_actual, y_pred, num_scored=len(pred_cols)):\n    score = 0\n    for i in range(num_scored):\n        score += rmse(y_actual[:, :, i], y_pred[:, :, i]) / num_scored\n    return score\n\ndef preprocess_inputs(df, cols=['sequence', 'structure', 'predicted_loop_type']):\n    base_fea = np.transpose(\n        np.array(\n            df[cols]\n            .applymap(lambda seq: [token2int[x] for x in seq])\n            .values\n            .tolist()\n        ),\n        (0, 2, 1)\n    )\n    bpps_sum_fea = np.array(df['bpps_sum'].to_list())[:,:,np.newaxis]\n    bpps_max_fea = np.array(df['bpps_max'].to_list())[:,:,np.newaxis]\n    bpps_nb_fea = np.array(df['bpps_nb'].to_list())[:,:,np.newaxis]\n    return np.concatenate([base_fea,bpps_sum_fea,bpps_max_fea,bpps_nb_fea], 2)\n\n\ntrain = pd.read_json('../input/stanford-covid-vaccine/train.json', lines=True)\ntest = pd.read_json('../input/stanford-covid-vaccine/test.json', lines=True)\n\ndef read_bpps_sum(df):\n    bpps_arr = []\n    for mol_id in df.id.to_list():\n        bpps_arr.append(np.load(f\"../input/stanford-covid-vaccine/bpps/{mol_id}.npy\").max(axis=1))\n    return bpps_arr\n\ndef read_bpps_max(df):\n    bpps_arr = []\n    for mol_id in df.id.to_list():\n        bpps_arr.append(np.load(f\"../input/stanford-covid-vaccine/bpps/{mol_id}.npy\").sum(axis=1))\n    return bpps_arr\n\ndef read_bpps_nb(df):\n    # normalized non-zero number\n    # from https://www.kaggle.com/symyksr/openvaccine-deepergcn\n    bpps_nb_mean = 0.077522 # mean of bpps_nb across all training data\n    bpps_nb_std = 0.08914   # std of bpps_nb across all training data\n    bpps_arr = []\n    for mol_id in df.id.to_list():\n        bpps = np.load(f\"../input/stanford-covid-vaccine/bpps/{mol_id}.npy\")\n        bpps_nb = (bpps > 0).sum(axis=0) / bpps.shape[0]\n        bpps_nb = (bpps_nb - bpps_nb_mean) / bpps_nb_std\n        bpps_arr.append(bpps_nb)\n    return bpps_arr\n\ntrain['bpps_sum'] = read_bpps_sum(train)\ntest['bpps_sum'] = read_bpps_sum(test)\ntrain['bpps_max'] = read_bpps_max(train)\ntest['bpps_max'] = read_bpps_max(test)\ntrain['bpps_nb'] = read_bpps_nb(train)\ntest['bpps_nb'] = read_bpps_nb(test)\n\nfrom sklearn.cluster import KMeans\n\nkmeans_model = KMeans(n_clusters=200, random_state=110).fit(preprocess_inputs(train)[:,:,0])\ntrain['cluster_id'] = kmeans_model.labels_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Firstly, let's reproduce Keras output by Pytorch. Because the implement of GRU in Pytorch is different to Keras, so I writed a GRU by Pytorch."},{"metadata":{"trusted":true},"cell_type":"code","source":"def gru_layer(hidden_dim, dropout):\n    return L.Bidirectional(L.GRU(hidden_dim, dropout=dropout, return_sequences=True, kernel_initializer = 'orthogonal'))\n\ndef lstm_layer(hidden_dim, dropout):\n    return L.Bidirectional(L.LSTM(hidden_dim, dropout=dropout, return_sequences=True, kernel_initializer = 'orthogonal'))\n\ndef build_model(seq_len=107, pred_len=68, dropout=0.5, embed_dim=100, hidden_dim=256, type=0):\n    inputs = L.Input(shape=(seq_len, 6))\n    \n    # split categorical and numerical features and concatenate them later.\n    categorical_feat_dim = 3\n    categorical_fea = inputs[:, :, :categorical_feat_dim]\n    numerical_fea = inputs[:, :, 3:]\n\n    embed = L.Embedding(input_dim=len(token2int), output_dim=embed_dim)(categorical_fea)\n    reshaped = tf.reshape(embed, shape=(-1, embed.shape[1],  embed.shape[2] * embed.shape[3]))\n    reshaped = L.concatenate([reshaped, numerical_fea], axis=2)\n    \n    if type == 0:\n        hidden = gru_layer(hidden_dim, dropout)(reshaped)\n        hidden = gru_layer(hidden_dim, dropout)(hidden)\n    elif type == 1:\n        hidden = lstm_layer(hidden_dim, dropout)(reshaped)\n        hidden = gru_layer(hidden_dim, dropout)(hidden)\n    elif type == 2:\n        hidden = gru_layer(hidden_dim, dropout)(reshaped)\n        hidden = lstm_layer(hidden_dim, dropout)(hidden)\n    elif type == 3:\n        hidden = lstm_layer(hidden_dim, dropout)(reshaped)\n        hidden = lstm_layer(hidden_dim, dropout)(hidden)\n    \n    truncated = hidden[:, :pred_len]\n    out = L.Dense(5, activation='linear')(truncated)\n    model = tf.keras.Model(inputs=inputs, outputs=out)\n    model.compile(tf.keras.optimizers.Adam(), loss=mcrmse)\n    return model\n\nkeras_model = build_model()\nkeras_model.load_weights('../input/gru-lstm-with-feature-engineering-and-augmentation/modelGRU_LSTM1_cv0.h5')\nkeras_model.layers","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"device = torch.device('cuda:%s'%0 if torch.cuda.is_available() else 'cpu')\ndef Init_params(shape,w=None,b=None):\n    if w is None:\n        w = torch.nn.Parameter(torch.empty(*shape))\n        nn.init.xavier_uniform_(w)\n    else:\n        w = torch.nn.Parameter(w)\n    if b is None:\n        b = torch.nn.Parameter(torch.zeros(shape[1]))\n    else:\n        b = torch.nn.Parameter(b)\n    return w,b\n\nclass GRU(nn.Module):\n    def __init__(self,input_dim,hidden_dim,w_i=None,b_i=None,w_h=None,b_h=None):\n        super(GRU, self).__init__()\n        self.w_i,self.b_i = Init_params([input_dim,3*hidden_dim],w_i,b_i)\n        self.w_h,self.b_h = Init_params([hidden_dim,3*hidden_dim],w_h,b_h)\n        self.hd = hidden_dim\n    def forward(self,x):\n        hidden = torch.zeros((x.shape[0], self.hd)).to(device)\n        output = []\n        for i in range(x.shape[1]):\n            x_z = torch.matmul(x[:,i,:],self.w_i[:,:self.hd]) + self.b_i[:self.hd]\n            x_r = torch.matmul(x[:,i,:],self.w_i[:,self.hd:2*self.hd]) + self.b_i[self.hd:2*self.hd]\n            x_n = torch.matmul(x[:,i,:],self.w_i[:,2*self.hd:]) + self.b_i[2*self.hd:]\n\n            h_z = torch.matmul(hidden,self.w_h[:,:self.hd]) + self.b_h[:self.hd]\n            h_r = torch.matmul(hidden,self.w_h[:,self.hd:2*self.hd]) + self.b_h[self.hd:2*self.hd]\n            h_n = torch.matmul(hidden,self.w_h[:,2*self.hd:]) + self.b_h[2*self.hd:]\n\n            z = torch.sigmoid(x_z+h_z)\n            r = torch.sigmoid(x_r+h_r)\n            n = torch.tanh(x_n+r*h_n)\n            h = (1-z)*n + z*hidden\n            hidden = h\n            output.append(h.unsqueeze(1))\n        return torch.cat(output,1)\n\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        num_target=5\n        w = torch.Tensor(keras_model.layers[2].get_weights()[0])\n        self.cate_emb = nn.Embedding.from_pretrained(w,freeze=False)\n        self.gru = GRU(100*3+3, 256, torch.Tensor(keras_model.layers[-4].get_weights()[0]).contiguous(),\n                       torch.Tensor(keras_model.layers[-4].get_weights()[2][0]).contiguous(),\n                       torch.Tensor(keras_model.layers[-4].get_weights()[1]).contiguous(),\n                       torch.Tensor(keras_model.layers[-4].get_weights()[2][1]).contiguous())\n        self.reverse_gru = GRU(100*3+3, 256, torch.Tensor(keras_model.layers[-4].get_weights()[3]).contiguous(),\n                       torch.Tensor(keras_model.layers[-4].get_weights()[5][0]).contiguous(),\n                       torch.Tensor(keras_model.layers[-4].get_weights()[4]).contiguous(),\n                       torch.Tensor(keras_model.layers[-4].get_weights()[5][1]).contiguous())\n        self.gru1 = GRU(512, 256, torch.Tensor(keras_model.layers[-3].get_weights()[0]).contiguous(),\n                       torch.Tensor(keras_model.layers[-3].get_weights()[2][0]).contiguous(),\n                       torch.Tensor(keras_model.layers[-3].get_weights()[1]).contiguous(),\n                       torch.Tensor(keras_model.layers[-3].get_weights()[2][1]).contiguous())\n        self.reverse_gru1 = GRU(512, 256, torch.Tensor(keras_model.layers[-3].get_weights()[3]).contiguous(),\n                       torch.Tensor(keras_model.layers[-3].get_weights()[5][0]).contiguous(),\n                       torch.Tensor(keras_model.layers[-3].get_weights()[4]).contiguous(),\n                       torch.Tensor(keras_model.layers[-3].get_weights()[5][1]).contiguous())\n        self.predict = nn.Linear(512,num_target)\n        for i,(n,p) in enumerate(self.predict.named_parameters()):\n            if i == 0:\n                p.data = torch.nn.Parameter(torch.Tensor(keras_model.layers[-1].get_weights()[0].T).contiguous())\n            if i == 1:\n                p.data = torch.nn.Parameter(torch.Tensor(keras_model.layers[-1].get_weights()[1]).contiguous())\n\n    def forward(self, cateX,contX):\n        cate_x = self.cate_emb(cateX).view(cateX.shape[0],cateX.shape[1],-1)\n        sequence = torch.cat([cate_x,contX],-1)\n        x = self.gru(sequence)\n        reverse_x = torch.flip(self.reverse_gru(torch.flip(sequence,[1])),[1])\n        sequence = torch.cat([x,reverse_x],-1)\n        x = self.gru1(sequence)\n        reverse_x = torch.flip(self.reverse_gru1(torch.flip(sequence,[1])),[1])\n        x = torch.cat([x,reverse_x],-1)\n        x = F.dropout(x,0.5,training=self.training)\n        predict = self.predict(x)\n        return predict\npytorch_model = Net()\npytorch_model.to(device)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# check 1 samples\nx = preprocess_inputs(train[:1])\ncate_x = torch.LongTensor(x[:,:,:3]).to(device)\ncont_x = torch.Tensor(x[:,:,3:]).to(device)\ny = np.array(train[:1][pred_cols].values.tolist()).transpose((0, 2, 1))","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"keras_y = keras_model.predict(x)\nkeras_y","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"pytorch_model.eval()\npytorch_y = pytorch_model(cate_x,cont_x).detach().cpu().numpy()\npytorch_y","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# output difference between Keras and Pytorch\nnp.mean(np.abs(keras_y-pytorch_y[:,:68,:]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# check all samples\n\ngkf = GroupKFold(n_splits=5)\nkeras_predict = []\npytorch_predict = []\ntargets = []\n\nfor fold, (train_index, valid_index) in enumerate(gkf.split(train,  train['reactivity'], train['cluster_id'])):\n    keras_model.load_weights('../input/gru-lstm-with-feature-engineering-and-augmentation/modelGRU_LSTM1_cv%s.h5'%fold)\n    t_valid = train.iloc[valid_index]\n    t_valid = t_valid[t_valid['SN_filter'] == 1]\n    valid_x = preprocess_inputs(t_valid)\n    valid_count = valid_x.shape[0]\n    valid_cate_x = torch.LongTensor(valid_x[:,:,:3])\n    valid_cont_x = torch.Tensor(valid_x[:,:,3:])\n    valid_y = torch.Tensor(np.array(t_valid[pred_cols].values.tolist()).transpose((0, 2, 1)))\n\n    valid_data = TensorDataset(valid_cate_x,valid_cont_x,valid_y)\n    valid_data_loader = DataLoader(dataset=valid_data,shuffle=False,batch_size=32,num_workers=1)\n    valid_y = valid_y.numpy()\n    targets.append(valid_y)\n    \n    # Keras predict\n    keras_predict.append(keras_model.predict(valid_x))\n    \n    # Pytorch predict and save oof\n    pytorch_model = Net()\n    pytorch_model.to(device)\n    \n    pytorch_model.eval()\n \n    all_pred = []\n\n    for data in valid_data_loader:\n        cate_x,cont_x,y = [x.to(device) for x in data]\n        outputs = pytorch_model(cate_x,cont_x)\n        all_pred.append(outputs.detach().cpu().numpy())\n    all_pred = np.concatenate(all_pred,0)[:,:68,:]\n    pytorch_predict.append(all_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(5):\n    print('fold %s output difference between Keras and Pytorch:'%i,np.mean(np.abs(keras_predict[i]-pytorch_predict[i])))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Evaluations of keras outputs and pytorch outputs"},{"metadata":{"trusted":true},"cell_type":"code","source":"def Metric(target,pred):\n    metric = 0\n    for i in range(target.shape[-1]):\n        metric += (np.sqrt(np.mean((target[:,:,i]-pred[:,:,i])**2))/target.shape[-1])\n    return metric","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(5):\n    print('fold %s'%i,'|','metric of keras outputs:%.6f'%Metric(targets[i],keras_predict[i]),'|','metric of pytorch outputs:%.6f'%Metric(targets[i],pytorch_predict[i]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Evaluations of  keras outputs and pytorch outputs are 0.24+. This is significantly worse than the val_loss  in keras training. Why?"},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's check keras loss fuction\n\ndef rmse(y_actual, y_pred):\n    mse = tf.keras.losses.mean_squared_error(y_actual, y_pred)\n    return K.sqrt(mse)\n\ndef mcrmse(y_actual, y_pred, num_scored=5):\n    score = 0\n    for i in range(num_scored):\n        score += rmse(y_actual[:, :, i], y_pred[:, :, i]) / num_scored\n    return score\n\nfor i in range(5):\n    print('fold %s'%i,mcrmse(targets[i],keras_predict[i]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Keras loss fuction returns a list instead of a number. So how is val_loss calculated in keras training? I guess it's the average of the loss list."},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's check the average of keras loss fuction outputs\nfor i in range(5):\n    print('fold %s'%i,K.mean(mcrmse(targets[i],keras_predict[i])))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### The truth has become known to all. The keras loss fuction is uncorrect. It calculats every sample's mcrmse, then returns the average. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Finaly, let's check the submission of pytorch is same as keras.\n\n# predict test\ndef Pred(df):\n    test_x = preprocess_inputs(df)\n    test_cate_x = torch.LongTensor(test_x[:,:,:3])\n    test_cont_x = torch.Tensor(test_x[:,:,3:])\n    test_data = TensorDataset(test_cate_x,test_cont_x)\n    test_data_loader = DataLoader(dataset=test_data,shuffle=False,batch_size=64,num_workers=1)\n    all_id = []\n    for i,row in df.iterrows():\n        for j in range(row['seq_length']):\n            all_id.append(row['id']+'_%s'%j)\n\n    all_id = np.array(all_id).reshape(-1,1)\n    all_pred = np.zeros(len(all_id)*5).reshape(len(all_id),5)\n    for fold in range(5):\n        keras_model.load_weights('../input/gru-lstm-with-feature-engineering-and-augmentation/modelGRU_LSTM1_cv%s.h5'%fold)\n        model = Net()\n        model.to(device)\n        model.eval()\n        t_all_pred = []\n        for data in test_data_loader:\n            cate_x,cont_x = [x.to(device) for x in data]\n            outputs = model(cate_x,cont_x)\n            t_all_pred.append(outputs.detach().cpu().numpy())\n        t_all_pred = np.concatenate(t_all_pred,0)\n        all_pred += t_all_pred.reshape(-1,5)\n    all_pred /= 5\n    sub = pd.DataFrame(all_pred,columns=['reactivity','deg_Mg_pH10','deg_pH10','deg_Mg_50C','deg_50C'])\n    sub['id_seqpos'] = all_id\n    return sub\npublic_sub = Pred(test.loc[test['seq_length']==107])\nprivate_sub = Pred(test.loc[test['seq_length']==130])\npytorch_sub = pd.concat([public_sub,private_sub]).reset_index(drop=True)\npytorch_sub = pytorch_sub[['id_seqpos']+pred_cols]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pytorch_sub = pytorch_sub.sort_values(by=['id_seqpos']).reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"keras_sub = pd.read_csv('../input/gru-lstm-with-feature-engineering-and-augmentation/submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"keras_sub = keras_sub.sort_values(by=['id_seqpos']).reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"keras_sub.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pytorch_sub.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# submission difference between Keras and Pytorch\nnp.mean(np.abs(keras_sub[pred_cols].values-pytorch_sub[pred_cols].values))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pytorch_sub.to_csv('./submission.csv',index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}