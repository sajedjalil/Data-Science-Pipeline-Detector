{"cells":[{"metadata":{},"cell_type":"markdown","source":"This is a essentially a combination of ['Transformer Encoder Implementation'](https://www.kaggle.com/arunprathap/transformer-encoder-implementation) by [Arun P R](https://www.kaggle.com/arunprathap) and [GRU+LSTM with feature engineering and augmentation](https://www.kaggle.com/its7171/gru-lstm-with-feature-engineering-and-augmentation) by [tito](https://www.kaggle.com/its7171), please check out their work as well.\n\nI have not tuned any hyperparameters. If you do so and find better results, please let me know in the comments.\n\nSo far I have tried the following while working on this:\n\n1. Embedding -> CNN -> RNN -> Transformer (did not do as good as current model)\n2. Added a 'Position' value for each value in sequence - surprisingly helped (ie: 0 for A 1 for B in ABCDE)\n\n\n##### Dan fork: \n* load normal comp data\n* load what looks like the aug data from it's kernel - https://www.kaggle.com/mathurinache/augmented-data-for-stanford-covid-vaccine\n"},{"metadata":{},"cell_type":"markdown","source":"## 1. Import libraries"},{"metadata":{"_kg_hide-output":true,"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"import os \nimport sys\nimport json\nimport math\nimport random\nimport numpy as np\nimport pandas as pd\nimport gc\nfrom tqdm import tqdm\n\nimport matplotlib.pyplot as plt \nimport seaborn as sns\nimport plotly.express as px\nimport plotly.graph_objects as go\n\nfrom sklearn.model_selection import train_test_split, KFold,  StratifiedKFold, GroupKFold\n\nfrom sklearn.cluster import KMeans\n\nimport tensorflow as tf\nimport tensorflow_addons as tfa\nimport tensorflow.keras.backend as K\nimport tensorflow.keras.layers as L\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"seed = 42","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true,"_kg_hide-input":true},"cell_type":"code","source":"DEVICE = \"TPU\"\nif DEVICE == \"TPU\":\n    print(\"connecting to TPU...\")\n    try:\n        tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n        print('Running on TPU ', tpu.master())\n    except ValueError:\n        print(\"Could not connect to TPU\")\n        tpu = None\n\n    if tpu:\n        try:\n            print(\"initializing  TPU ...\")\n            tf.config.experimental_connect_to_cluster(tpu)\n            tf.tpu.experimental.initialize_tpu_system(tpu)\n            strategy = tf.distribute.experimental.TPUStrategy(tpu)\n            print(\"TPU initialized\")\n        except _:\n            print(\"failed to initialize TPU\")\n    else:\n        DEVICE = \"GPU\"\n\nif DEVICE != \"TPU\":\n    #if tf.config.list_physical_devices('gpu'):\n    #    strategy = tf.distribute.MirroredStrategy()#if using multiple gpu\n    #else:  # use default strategy\n    strategy = tf.distribute.get_strategy()\n\nif DEVICE == \"GPU\":\n    print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))  \nprint('Number of devices: {}'.format(strategy.num_replicas_in_sync))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"dropout_model = 0.36\nhidden_dim_first = 128\nhidden_dim_second = 256\nhidden_dim_third = 128","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2. Read & Process Datasets - Including Augmented Data"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# # Download datasets # ORIG - gave error doesn't find file\n# train = pd.read_json('../input/bpps-data-included/out_train (1).json')\n# test = pd.read_json('../input/bpps-data-included/out_test (1).json')\n# sample_sub = pd.read_csv(\"/kaggle/input/stanford-covid-vaccine/sample_submission.csv\")\n\n### use standard file locs -= new - dan\n\n# Download datasets\ntrain = pd.read_json('/kaggle/input/stanford-covid-vaccine/train.json', lines=True)\nprint(\"train shape\",train.shape)\ntest = pd.read_json('/kaggle/input/stanford-covid-vaccine/test.json', lines=True)\nsample_sub = pd.read_csv(\"/kaggle/input/stanford-covid-vaccine/sample_submission.csv\")\n# train = train[train.signal_to_noise > 1] # ORIG","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train[train.signal_to_noise > 1].shape[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# additional features\n\ndef read_bpps_sum(df):\n    bpps_arr = []\n    for mol_id in df.id.to_list():\n        bpps_arr.append(np.load(f\"../input/stanford-covid-vaccine/bpps/{mol_id}.npy\").max(axis=1))\n    return bpps_arr\n\ndef read_bpps_max(df):\n    bpps_arr = []\n    for mol_id in df.id.to_list():\n        bpps_arr.append(np.load(f\"../input/stanford-covid-vaccine/bpps/{mol_id}.npy\").sum(axis=1))\n    return bpps_arr\n\ndef read_bpps_nb(df):\n    # normalized non-zero number\n    # from https://www.kaggle.com/symyksr/openvaccine-deepergcn \n    bpps_nb_mean = 0.077522 # mean of bpps_nb across all training data\n    bpps_nb_std = 0.08914   # std of bpps_nb across all training data\n    bpps_arr = []\n    for mol_id in df.id.to_list():\n        bpps = np.load(f\"../input/stanford-covid-vaccine/bpps/{mol_id}.npy\")\n        bpps_nb = (bpps > 0).sum(axis=0) / bpps.shape[0]\n        bpps_nb = (bpps_nb - bpps_nb_mean) / bpps_nb_std\n        bpps_arr.append(bpps_nb)\n    return bpps_arr \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['bpps_sum'] = read_bpps_sum(train)\ntest['bpps_sum'] = read_bpps_sum(test)\ntrain['bpps_max'] = read_bpps_max(train)\ntest['bpps_max'] = read_bpps_max(test)\ntrain['bpps_nb'] = read_bpps_nb(train)\ntest['bpps_nb'] = read_bpps_nb(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Target columns \ntarget_cols = ['reactivity', 'deg_Mg_pH10', 'deg_pH10', 'deg_Mg_50C', 'deg_50C']\n\ntoken2int = {x:i for i, x in enumerate('().ACGUBEHIMSX')}\n\ndef preprocess_inputs(df, cols=['sequence','predicted_loop_type','structure']):\n    base_fea = np.transpose(\n        np.array(\n            df[cols]\n            .applymap(lambda seq: [token2int[x] for x in seq])\n            .values\n            .tolist()\n        ),\n        (0, 2, 1)\n    )\n    bpps_sum_fea = np.array(df['bpps_sum'].to_list())[:,:,np.newaxis]\n    bpps_max_fea = np.array(df['bpps_max'].to_list())[:,:,np.newaxis]\n    bpps_nb_fea = np.array(df['bpps_nb'].to_list())[:,:,np.newaxis]\n    \n    # This kind of helps...\n    _, position_fea = np.mgrid[0:bpps_nb_fea.shape[0]:1, 0:bpps_nb_fea.shape[1]:1]/(bpps_nb_fea.shape[1]-1)\n    \n    return np.concatenate([base_fea,bpps_sum_fea,bpps_max_fea,bpps_nb_fea, position_fea[:,:,np.newaxis]], 2)\n\n# clustering for  GroupKFold\nkmeans_model = KMeans(n_clusters=200, random_state=110).fit(preprocess_inputs(train)[:,:,0])\ntrain['cluster_id'] = kmeans_model.labels_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# aug_df = pd.read_csv('../input/augmented-data/aug_data1.csv') ## ORIG\n### alt Dan - hopefully correct? from public dataset\naug_df = pd.read_csv('../input/augmented-data-for-stanford-covid-vaccine/48k_augment.csv') # \n\ndisplay(aug_df.head())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Please check out [this notebook](https://www.kaggle.com/its7171/how-to-generate-augmentation-data) as well to see how this was generated"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def aug_data(df):\n    target_df = df.copy()\n    new_df = aug_df[aug_df['id'].isin(target_df['id'])]\n                         \n    del target_df['structure']\n    del target_df['predicted_loop_type']\n    new_df = new_df.merge(target_df, on=['id','sequence'], how='left')\n\n    df['cnt'] = df['id'].map(new_df[['id','cnt']].set_index('id').to_dict()['cnt'])\n    df['log_gamma'] = 100\n    df['score'] = 1.0\n    df = df.append(new_df[df.columns])\n    return df\ntrain = aug_data(train)\ntest = aug_data(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"train_inputs_all = preprocess_inputs(train)\ntrain_labels_all = np.array(train[target_cols].values.tolist()).transpose((0, 2, 1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_inputs_all.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3. Model Implementation, Training & Prediction"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def scaled_dot_product_attention(q, k, v, mask):\n    \"\"\"Calculate the attention weights.\n    q, k, v must have matching leading dimensions.\n    k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n    The mask has different shapes depending on its type(padding or look ahead) \n    but it must be broadcastable for addition.\n  \n    Args:\n      q: query shape == (..., seq_len_q, depth)\n      k: key shape == (..., seq_len_k, depth)\n      v: value shape == (..., seq_len_v, depth_v)\n      mask: Float tensor with shape broadcastable \n            to (..., seq_len_q, seq_len_k). Defaults to None.\n    \n    Returns:\n     output, attention_weights\n    \"\"\"\n\n    matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n  \n    # scale matmul_qk\n    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n\n    # add the mask to the scaled tensor.\n    if mask is not None:\n        scaled_attention_logits += (mask * -1e9)  \n\n    # softmax is normalized on the last axis (seq_len_k) so that the scores\n    # add up to 1.\n    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n\n    output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n\n    return output, attention_weights\n\nclass MultiHeadAttention(tf.keras.layers.Layer):\n    def __init__(self, d_model, num_heads):\n        super(MultiHeadAttention, self).__init__()\n        self.num_heads = num_heads\n        self.d_model = d_model\n\n        assert d_model % self.num_heads == 0\n\n        self.depth = d_model // self.num_heads\n\n        self.wq = tf.keras.layers.Dense(d_model)\n        self.wk = tf.keras.layers.Dense(d_model)\n        self.wv = tf.keras.layers.Dense(d_model)\n\n        self.dense = tf.keras.layers.Dense(d_model)\n        \n    def split_heads(self, x, batch_size):\n        \"\"\"Split the last dimension into (num_heads, depth).\n        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n        \"\"\"\n        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n        return tf.transpose(x, perm=[0, 2, 1, 3])\n    \n    def call(self, v, k, q, mask):\n        batch_size = tf.shape(q)[0]\n\n        q = self.wq(q)  # (batch_size, seq_len, d_model)\n        k = self.wk(k)  # (batch_size, seq_len, d_model)\n        v = self.wv(v)  # (batch_size, seq_len, d_model)\n\n        q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n        k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n        v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n\n        # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n        # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n        scaled_attention, attention_weights = scaled_dot_product_attention(\n            q, k, v, mask)\n\n        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n\n        concat_attention = tf.reshape(scaled_attention, \n                                      (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n\n        output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n\n        return output, attention_weights\n    \n    def get_config(self):\n\n        config = super().get_config().copy()\n        config.update({\n            'depth': self.depth,\n            'wq': self.wq,\n            'qk': self.wk,\n            'wv': self.wv,\n            'dense': self.dense,\n        })\n        \n        return config\ndef point_wise_feed_forward_network(d_model, dff):\n      return tf.keras.Sequential([\n      tf.keras.layers.Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)\n      tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)\n  ])\n\nclass EncoderLayer(tf.keras.layers.Layer):\n    def __init__(self, d_model, num_heads, dff, rate=0.1):\n        super(EncoderLayer, self).__init__()\n        self.d_model = d_model\n        self.num_heads = num_heads\n        self.dff = dff\n        self.rate = rate\n        \n        self.mha = MultiHeadAttention(d_model, num_heads)\n        self.ffn = point_wise_feed_forward_network(d_model, dff)\n\n        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n\n        self.dropout1 = tf.keras.layers.Dropout(rate)\n        self.dropout2 = tf.keras.layers.Dropout(rate)\n    \n    def call(self, x, training):\n        #mask made None\n        attn_output, _ = self.mha(x, x, x, None)  # (batch_size, input_seq_len, d_model)\n        attn_output = self.dropout1(attn_output, training=training)\n        out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n\n        ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n        ffn_output = self.dropout2(ffn_output, training=training)\n        out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n\n        return out2\n\n    def get_config(self):\n\n        config = super().get_config().copy()\n        config.update({\n            'num_heads': self.num_heads,\n            'rate': self.rate,\n            'd_model': self.d_model,\n            'num_heads': self.num_heads,\n            'dropout1': self.dropout1,\n            'dropout2': self.dropout2,\n            'layernorm1': self.layernorm1,\n            'layernorm2': self.layernorm2,\n            'mha': self.mha,\n            'ffn': self.ffn,\n        })\n        return config\n    \ndef gru_layer(hidden_dim, dropout):\n    return L.Bidirectional(L.GRU(hidden_dim, dropout=dropout, return_sequences=True, kernel_initializer = 'orthogonal'))\n\ndef lstm_layer(hidden_dim, dropout):\n    return L.Bidirectional(L.LSTM(hidden_dim, dropout=dropout, return_sequences=True, kernel_initializer = 'orthogonal'))\n\ndef MCRMSE(y_true, y_pred):\n    colwise_mse = tf.reduce_mean(tf.square(y_true - y_pred), axis=1)\n    return tf.reduce_mean(tf.sqrt(colwise_mse), axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_model(model_type=1, seq_len=107, pred_len=68, embed_dim=200, \n                dropout=dropout_model, hidden_dim_first = hidden_dim_first, \n                hidden_dim_second = hidden_dim_second, hidden_dim_third = hidden_dim_third):\n    \n    inputs = tf.keras.layers.Input(shape=(seq_len, 7))\n\n    # Extract features\n    categorical_feat_dim = 3\n    categorical_fea = inputs[:, :, :categorical_feat_dim]\n    numerical_fea = inputs[:, :, 3:6]\n    positional_fea = tf.expand_dims(inputs[:, :, 6], axis=2) \n    \n    # Categorical embedding\n    embed = tf.keras.layers.Embedding(input_dim=len(token2int), output_dim=embed_dim)(categorical_fea)\n    reshaped = tf.reshape(\n        embed, shape=(-1, embed.shape[1],  embed.shape[2] * embed.shape[3]))\n\n    # Concatenate with numerical\n    reshaped = L.concatenate([reshaped, numerical_fea], axis=2)\n    reshaped = tf.keras.layers.SpatialDropout1D(.2)(reshaped)\n    \n    ###ORIG\n#     # Convolve\n#     conv = L.Conv1D(255, 5, padding='same', activation=tf.keras.activations.swish)(reshaped)\n\n##NEW Conv\n    # Convolve\n    conv = L.Conv1D(255, 3, padding='same', activation=tf.keras.activations.swish)(reshaped)\n    conv = L.Conv1D(255, 6, padding='same', activation=tf.keras.activations.swish)(conv)\n\n    \n    # Concatenate with positional\n    reshaped = L.concatenate([conv, positional_fea], axis=2) \n    \n    # Transformer x2 - SWAPPED POSITION WITH RNN\n    hidden = EncoderLayer(256, 128, 512)(reshaped)\n\n    hidden = EncoderLayer(256, 128, 512)(hidden)\n    \n    # RNN\n    if model_type == 0:\n        hidden = gru_layer(256, 0.3)(hidden)\n        hidden = gru_layer(256, 0.3)(hidden)\n    elif model_type == 1:\n        hidden = lstm_layer(256, 0.3)(hidden)\n        hidden = gru_layer(256, 0.3)(hidden)\n    elif model_type == 2:\n        hidden = gru_layer(256, 0.3)(hidden)\n        hidden = lstm_layer(256, 0.3)(hidden)\n    elif model_type == 3:\n        hidden = lstm_layer(256, 0.3)(hidden)\n        hidden = lstm_layer(256, 0.3)(hidden)  \n\n    truncated = hidden[:, :pred_len]\n\n    out = tf.keras.layers.Dense(len(target_cols), activation='linear')(truncated)\n\n    model = tf.keras.Model(inputs=inputs, outputs=out)\n\n    adam = tf.optimizers.Adam()\n    model.compile(optimizer=adam, loss=MCRMSE, metrics=[tf.keras.metrics.RootMeanSquaredError()])\n    \n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"tf.keras.backend.clear_session()\nfrom tqdm.keras import TqdmCallback\nlr_callback = tf.keras.callbacks.ReduceLROnPlateau()\nes_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss',restore_best_weights=True,min_delta=0.001, patience=10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"mse_s = []\nrmse_s = []\n\ndef train_and_predict(n_folds=5, model_name=\"model\", model_type=0, epochs=100, debug=True,\n                      dropout_model=dropout_model, hidden_dim_first = hidden_dim_first, \n                      hidden_dim_second = hidden_dim_second, hidden_dim_third = hidden_dim_third,\n                      seed=seed):\n\n    print(\"Model:\", model_name)\n\n    ensemble_preds = pd.DataFrame(index=sample_sub.index, columns=target_cols).fillna(0) # test dataframe with 0 values\n    kf = KFold(n_folds, shuffle=True, random_state=seed)\n    skf = StratifiedKFold(n_folds, shuffle=True, random_state=seed)\n    gkf = GroupKFold(n_splits=n_folds)\n    val_losses = []\n    historys = []\n    \n    \n    for i, (train_index, val_index) in enumerate(gkf.split(train, train['reactivity'], train['cluster_id'])):\n        print(\"Fold:\", str(i+1))\n        with strategy.scope():\n            model_train = build_model(model_type=model_type, \n                                      dropout=dropout_model, \n                                      hidden_dim_first = hidden_dim_first, \n                                      hidden_dim_second = hidden_dim_second, \n                                      hidden_dim_third = hidden_dim_third)\n            model_short = build_model(model_type=model_type, seq_len=107, pred_len=107,\n                                      dropout=dropout_model, \n                                      hidden_dim_first = hidden_dim_first, \n                                      hidden_dim_second = hidden_dim_second, \n                                      hidden_dim_third = hidden_dim_third)\n            model_long = build_model(model_type=model_type, seq_len=130, pred_len=130,\n                                     dropout=dropout_model, \n                                     hidden_dim_first = hidden_dim_first, \n                                     hidden_dim_second = hidden_dim_second, \n                                     hidden_dim_third = hidden_dim_third)\n\n        train_inputs, train_labels = train_inputs_all[train_index], train_labels_all[train_index]\n        \n        val = train.iloc[val_index]\n        x_val_all = preprocess_inputs(val)\n        \n        val = val[val.SN_filter == 1]\n        \n        val_inputs = preprocess_inputs(val)\n        val_labels = np.array(val[target_cols].values.tolist()).transpose((0, 2, 1))\n        \n        w_trn = np.log(train.iloc[train_index].signal_to_noise+1.1)/2\n\n        checkpoint = tf.keras.callbacks.ModelCheckpoint(f'{model_name}_Fold_{str(i+1)}.h5')\n\n        history = model_train.fit(\n            train_inputs , train_labels, \n            validation_data=(val_inputs,val_labels),\n            batch_size=64,\n            sample_weight=w_trn/2,\n            epochs=epochs, \n            callbacks=[checkpoint,\n                       lr_callback,\n                       TqdmCallback(),\n                       tf.keras.callbacks.TerminateOnNaN(),\n                       es_callback],\n            verbose= 0\n        )\n\n        holdouts = train.iloc[val_index]\n        holdout_preds = model_train.predict(x_val_all)\n        holdout_labels = np.array(holdouts[target_cols].values.tolist()).transpose((0, 2, 1))\n        \n        rmse = ((holdout_labels - holdout_preds) ** 2).mean() ** .5\n        mse = ((holdout_labels - holdout_preds) ** 2).mean()\n\n        print(f\"{model_name} Min training loss={min(history.history['loss'])}, min validation loss={min(history.history['val_loss'])}\")\n        \n        print(f\"{model_name} Holdouts mse ={mse}, Holdouts rmse ={rmse}\")\n        mse_s.append(mse)\n        rmse_s.append(rmse)\n        mse_s_t.append(mse)\n        rmse_s_t.append(rmse)\n        \n        val_losses.append(min(history.history['val_loss']))\n        historys.append(history)\n        \n        model_short.load_weights(f'{model_name}_Fold_{str(i+1)}.h5')\n        model_long.load_weights(f'{model_name}_Fold_{str(i+1)}.h5')\n\n        public_preds = model_short.predict(public_inputs)\n        private_preds = model_long.predict(private_inputs)\n\n        preds_model = []\n        for df, preds in [(public_df, public_preds), (private_df, private_preds)]:\n            for i, uid in enumerate(df.id):\n                single_pred = preds[i]\n\n                single_df = pd.DataFrame(single_pred, columns=target_cols)\n                single_df['id_seqpos'] = [f'{uid}_{x}' for x in range(single_df.shape[0])]\n\n                preds_model.append(single_df)\n            \n        preds_model_df = pd.concat(preds_model).groupby('id_seqpos').mean().reset_index()\n        \n        ensemble_preds[target_cols] += preds_model_df[target_cols].values / n_folds\n\n        if debug:\n            print(\"Intermediate ensemble result\")\n            print(ensemble_preds[target_cols].head())\n\n    ensemble_preds[\"id_seqpos\"] = preds_model_df[\"id_seqpos\"].values\n    ensemble_preds = pd.merge(sample_sub[\"id_seqpos\"], ensemble_preds, on=\"id_seqpos\", how=\"left\")\n\n    print(\"Mean Validation loss:\", str(np.mean(val_losses)))\n\n    if debug:\n        fig, ax = plt.subplots(1, 3, figsize = (20, 10))\n        for i, history in enumerate(historys):\n            ax[0].plot(history.history['loss'])\n            ax[0].plot(history.history['val_loss'])\n            ax[0].set_title('model_'+str(i+1))\n            ax[0].set_ylabel('Loss')\n            ax[0].set_xlabel('Epoch')\n            \n            ax[1].plot(history.history['root_mean_squared_error'])\n            ax[1].plot(history.history['val_root_mean_squared_error'])\n            ax[1].set_title('model_'+str(i+1))\n            ax[1].set_ylabel('RMSE')\n            ax[1].set_xlabel('Epoch')\n            \n            ax[2].plot(history.history['lr'])\n            ax[2].set_title('model_'+str(i+1))\n            ax[2].set_ylabel('LR')\n            ax[2].set_xlabel('Epoch')\n        plt.show()\n\n    return ensemble_preds\n\n\npublic_df = test.query(\"seq_length == 107\").copy()\nprivate_df = test.query(\"seq_length == 130\").copy()\npublic_inputs = preprocess_inputs(public_df)\nprivate_inputs = preprocess_inputs(private_df)\n\nensembles = []\n\nfor i in range(1):\n    model_name = \"model_\"+str(i+1)\n    \n    mse_s_t = []\n    rmse_s_t = []\n\n    ensemble = train_and_predict(n_folds=6, model_name=model_name, model_type=i, epochs=70,\n                                 dropout_model=dropout_model, hidden_dim_first = hidden_dim_first, \n                                 hidden_dim_second = hidden_dim_second, hidden_dim_third = hidden_dim_third,\n                                 seed=seed)\n    ensembles.append(ensemble)\n    print(\"RMSE Avg \", np.array(rmse_s_t).mean())\n    print(\"MSE Avg \", np.array(mse_s_t).mean())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4. Ensembling the solutions and submission\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Score to beat when making changes\nprint(\"RMSE Avg \", np.array(rmse_s).mean())\nprint(\"MSE Avg \", np.array(mse_s).mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Ensembling the solutions\nensemble_final = ensembles[0].copy()\nensemble_final[target_cols] = 0\n\nfor ensemble in ensembles:\n    ensemble_final[target_cols] += ensemble[target_cols].values / len(ensembles)\n\nensemble_final.head().T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"blend_preds_df = pd.DataFrame()\nblend_preds_df['id_seqpos'] = ensemble_final['id_seqpos']\nblend_preds_df['reactivity'] = ensemble_final['reactivity'] \nblend_preds_df['deg_Mg_pH10'] = ensemble_final['deg_Mg_pH10']\nblend_preds_df['deg_pH10'] = ensemble_final['deg_Mg_pH10']\nblend_preds_df['deg_Mg_50C'] = ensemble_final['deg_Mg_50C']\nblend_preds_df['deg_50C'] = ensemble_final['deg_Mg_50C']\nblend_preds_df.head().T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Submission\nblend_preds_df.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}