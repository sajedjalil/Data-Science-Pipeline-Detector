{"cells":[{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"markdown","source":"<center> <h1> Coding principles for rapid machine learning experimentation </h1> </center>\n\n<br />\n\nAccording to François Chollet, the creator of Keras and Someone who at his best, placed 17th on Kaggle's cumulative leaderboard: it is not about having the best idea(s) from the start but rather iterating over ideas often and quickly that allows people to win competitions and publish papers.\n\n<br />\n\n<center>\n\n<figure>\n\n<img src=\"https://i.ibb.co/17ghyM3/Screenshot-6.png\" width='400px'>\n    \n<br />\n    \n<figcaption>Fig. 1 François Chollet's Twitter Feed  <a href=\"https://twitter.com/fchollet/status/1113478477116608512\">Source</a></figcaption>\n\n</figure>\n\n</center>\n\n<br />\n\nIt is therefore important to be able to optimize your workflow and tools to enable you to rapidly experiment and iterate over ideas faster.\n\n<br />\n\n<center>\n\n<figure>\n\n<img src=\"https://i.ibb.co/Jm6DM4s/experimentation-loop.jpg\" width='400px'>\n    \n<br />\n\n<figcaption>Fig. 2 François Chollet's Cycle of Experimentation  <a href=\"https://twitter.com/fchollet/status/1113479155847270400\">Source</a></figcaption>\n\n</figure>\n\n</center>\n\n<br />\n\nThe goal of this series of kaggle kernels is to explore the coding patterns and design principles for speeding up the machine learning workflow and enabling rapid experimentation. \n\nThere are several key components in a regular machine learning workflow, some of them are listed below:\n\n* Exploratory Data Analysis\n* Feature Transformation / Engineering\n* Validation Strategy\n* Model Building\n* Experiment Tracking\n* Model Tuning / Ensembling\n    \nIn this first post, we will look at writing code for feature transformations, creating validation strategies and data versioning.\n\n\n## Feature Engineering\n\nFunctions are the best units of code that work for feature transformations and reusability. These functions when written with the goal of creating reusable, portable, repeatable, and chainable sets of data transformations, work very well for machine learning workflows. \n\n\nLet us understand how to tackle these design goals:\n\n### Naming and Documenting functions\n\nFirstly, it is important to name your functions appropriately, and document them following standards. I generally like to use a custom version of the sklearn docstring template, which can be found at the following [link](https://gist.github.com/jakevdp/3808292)\n\nThe must-have categories in a docstring are:\n\n* A function description\n* Parameters (with expected types)\n* Returns (with the object type)\n\nBased on the complexity of the function, it can either have a single line description or a multi-line description.\n\nI like adding another portion to the documentation, which is __Suggested Imports__. This makes the function easier to be copied across various notebooks / scripts in projects without the worry of having to move scripts around to import just a copule of functions in a notebook and miss any important import statements. It also indirectly lets me know what libraries the function depends on. I would still recommend putting all imports in at the top of a script according to PEP standards.\n\nAlso, it may be beneficial to note down __Example Usage__ patterns of the function, in the rare cases where the function has complex use cases.\n\nLet us now look at how to document a function\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def example_fn(param_one, param_two):\n\n    ''' \n    This is an example function showing how to write\n    docstrings\n\n    Parameters\n    ----------\n    param_one: pd.DataFrame\n               Description of the param_one argument\n    \n    param_two: int\n               Description of the param_two argument\n\n    Returns\n    -------\n    return_data: pd.DataFrame\n                 An example description of the returned      \n                 object\n\n    Suggested Imports\n    ----------------\n    import numpy as np\n    import pandas as pd\n\n    Example Usage\n    -------------\n    transformed_data = example_func(param_one, param_two)\n\n    '''\n    \n    pass","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* This now enables us to access the docstring from anywhere we import, define or call this `example_fn`function from","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"help(example_fn)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Standardizing the UX\n\nOne of the key ingredients to make using a function familiar and easy, is if the inputs and outputs are standardized across the board for a specific use case. Thereby ensuring that the user or developer experience is seamless.\n\nThe scikit-learn API has standardized `.fit()`, `.transform()`, and `.predict()` methods across its library and it is a fantastic example of how a standardized UX can lead to high developer productivity and an extremely low barrier for entry.\n\nFor functions that are meant to perform feature engineering, it is important to always input a dataframe and some auxillary arguments while returning a dataframe, as can be seen in the figure below.\n\n<br />\n\n<center>\n\n<figure>\n\n<img src=\"https://i.ibb.co/QC1J5WQ/feature-transformation-functions.png\" width='400px'/>\n    \n<br />\n    \n<figcaption>Fig.3 Every transformation function should take a data frame as input and return a dataframe</figcaption>\n\n</figure>\n\n</center>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"This design can is independent of pandas and can be ported to other data processing frameworks such as spark. Any logical code written using pandas can then be ported over to run on a spark based parallel processing system using the `Koalas` API, yet following a similar design pattern. \n\n### Pure Functions\n\nPure functions have two important properties:\n* If given the same arguments, the function must return the same value\n\n* When the function is evaluated, there are no side effects (no I/O streams, no mutation of static and non-local variables)\n\n<center>\n\n<figure>\n\n<img src=\"https://i.ibb.co/pPBzYKs/pure-functions.png\" width='400px'>\n    \n<br />\n\n<figcaption>Fig. 4 A visual representation of pure functions  <a href=\"https://livebook.manning.com/book/get-programming-with-scala/chapter-21/v-4/68\">Source</a></figcaption>\n\n</figure>\n\n</center>\n\n<br />\n\nWhen writing functions for data transformations, we cannot always write pure functions, especially considering the limitations of system memory, given that to mutate a non-local variable would require to create an entire new copy of the dataframe.\n\nTherefore, it is important to have a boolean argument `inplace` which can help the developer decide whether or not to mutate the dataframe as per the requirements of the situation.\n\n\n### Type Hinting\n\nType hinting, was introduced in `PEP 484` and `Python 3.5`. Therefore I donot recommend using it completely as of now, unless you are sure that all of the libraries you use in your workflow are compatible for `Python 3.5` and above.\n\nThe basic structure of type hinting in python is as follows:\n\n    def fn_name(arg_name: arg_type) -> return_type:\n        \n        pass\n        \n* Once a function definition is complete, use the `->` symbol to indicate the return type, it could be `int`, `dict` or any other python data type.\n\n* Every argument in the function is followd by a `:` and the data type of the argument\n\n* You can also use more complex ways of representing nested data types, optional data types, etc. using the `typing` module in Python\n\nBelow, you can find an example where I use the `typing` module and use type hinting in python","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from typing import List\n\ndef list_squared(input_list: List[int]) -> List[int]:\n    return [element**2 for element in input_list]\n\nlist_squared([2, 4, 6, 8])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Not all of the principles stated above are necessary, but they are important to consider when designing functions for feature transformation / engineering.\n\nLet us now use these principles to design a function that allows us to engineer date based features. We will be using the `Rossman Store Sales` dataset.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Reading in the dataset","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data = pd.read_csv('../input/rossmann-store-sales/train.csv', low_memory=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Putting the principles in practice\n\nWe will now write a function that allows us to engineer date based features, which can be used in downstream machine learning training tasks especially suited for tree based models","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def generate_date_features(input_data: pd.DataFrame,\n                           date_col:str,\n                           use_col_name: bool=True,\n                           inplace: bool=False) -> pd.DataFrame:\n    \n    ''' \n    generates date features from a date column, such as,\n    year, month, day, etc. which can be used to train\n    Machine Learning models. \n\n    Parameters\n    ----------\n    input_data : pd.DataFrame\n                 The input data frame\n          \n    date_col : str\n               The column name of the date column for which\n               the features have to be generated\n               \n               Ensure that the column is a pandas datetime\n               object. And also has year, month and day in\n               it.\n               \n    use_col_name : bool, default True\n                   If True, the column name will be appended\n                   to the name of the feature that has been\n                   created\n    \n    inplace : bool, default False\n              If False, a new data frame object is returned.\n              Else, the same data frame passed as input is\n              modified\n\n    Returns\n    -------\n    return_data: pd.DataFrame\n                 The returned dataframe which has the appended\n                 date based features\n\n    Suggested Imports\n    ----------------\n    import numpy as np\n    import pandas as pd\n\n    Example Usage\n    -------------\n    data_with_date_features = generate_date_features(data, 'date')\n\n    '''\n    \n    # Inplace or Copy\n    if inplace:\n        data_frame = input_data\n    else:\n        data_frame = input_data.copy()\n        \n    # Use column name\n    if use_col_name:\n        new_col_name = f'{date_col}_'\n    else:\n        new_col_name = ''\n        \n    # ensure that the column is converted to a \n    # pandas datetime object \n    data_frame[date_col] = pd.to_datetime(data_frame[date_col])\n    \n    # Generate date features\n    data_frame[f'{new_col_name}year'] = data_frame[date_col].dt.year\n    data_frame[f'{new_col_name}month'] = data_frame[date_col].dt.month\n    data_frame[f'{new_col_name}day'] = data_frame[date_col].dt.day\n    data_frame[f'{new_col_name}weeknum'] = data_frame[date_col].dt.weekofyear\n    data_frame[f'{new_col_name}dayofweek'] = data_frame[date_col].dt.dayofweek\n    data_frame[f'{new_col_name}quarter'] = data_frame[date_col].dt.quarter\n    \n    return data_frame","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As can be seen from above, the `generate_date_features` function is portable, reusable, flexible and can work acorss various data transformation pipelines.\n\nYou can design `directed acyclic graphs` to execute specific python functions with pre and post dependecies to generate your final transformed dataset.\n\nThis feature engineering pipeline can also be constantly regenerated from new raw data from such a DAG. I would definitely recommend checking out the package `Airflow`, which allows us to write flexible DAGs to manage ETL workloads easily.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"new_features = generate_date_features(train_data, date_col='Date', use_col_name=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"new_features.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Versioning\n\nIt is important to keep track of data that is generated from raw sources, so that, it becomes easier to reproduce results, machine learning models, bugs, or any anomalies found during the machine learning pipeline.\n\nThere are several ways to keep track of data. Two such ways are:\n\n* Saving copies of the modified datasets\n* Creating new columns with a standardized naming scheme to track validation sets, modified and engineered features\n\n\nIn this particular kernel, I will discuss the latter one, which I believe is a strategy that is more suited to a Data Scientist as compared to a machine learning engineer.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Validation Strategy\n\nIf you go to Scikit-Learn's documentation for the `KFold` class, you will see a pattern which most data scientists / ml engineers use when performing validation. This pattern can be found below:\n\n    kf = KFold(n_splits=2)\n    \n    for train_index, test_index in kf.split(X):\n        print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n        X_train, X_test = X[train_index], X[test_index]\n        y_train, y_test = y[train_index], y[test_index]\n        \n* When you start using the target based features such as target encoding in your model, your code must go inside this loop.\n\n* Every time you build your model on a fold, your code must go inside this loop\n\nThis restricts the freedom of a Data Scientist and makes the results / code harder to keep track of.\n\nThis is where a solution that I first came across when reading the x4 Kaggle Grandmaster [Abhishek Thakur's](https://www.kaggle.com/abhishek) book, titled [Approaching any machine learning problem](https://www.amazon.com/Approaching-Almost-Machine-Learning-Problem-ebook/dp/B089P13QHT).\n\nI recommend using a solution that is based on his approach, where we create a new column that tracks the fold number to which the record belongs. This enables me to not only train models on a given fold parallely, but also on machines that do not have any network connection between them. Every team member could build a model for one fold. This approach to validation unshackles the Data Scientist and increases their productivity.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from typing import Optional, List\n\ndef create_time_kfolds_days(input_data: pd.DataFrame,\n                            date_col: str, num_days: int,\n                            group_cols: Optional[List[str]] = None,\n                            num_folds: int = 5, inplace: bool = False\n                           ) -> pd.DataFrame:\n    ''' \n    Creates a kfold column based on time, which allows us to validate our\n    machine learning models. This works for when the forecasts are at\n    the day level.\n\n    Parameters\n    ----------\n    input_data : pd.DataFrame\n                 The input data frame\n           \n    date_col : str\n               The column name of the date column for which\n               the features have to be generated\n               \n               Ensure that the column is a pandas datetime\n               object. And also has year, month and day in\n               it.\n          \n    num_days: int\n              The number of days that have to be included\n              in each validation fold\n               \n    group_cols : List[str] or None, default None\n                 If a list of strings is passed, the folds\n                 are created for the last num_days by grouping\n                 the columns passed.\n                 \n                 If None, the creates the folds by just using\n                 the date_col\n                 \n    num_folds: int default 5\n               The number of folds to create\n    \n    inplace : bool, default False\n              If False, a new data frame object is returned.\n              Else, the same data frame passed as input is\n              modified\n\n    Returns\n    -------\n    return_data: pd.DataFrame\n                 The returned dataframe which has the kfold\n                 column appended to it.\n\n    Suggested Imports\n    ----------------\n    import numpy as np\n    import pandas as pd\n    from typing import Optional, List\n\n    Example Usage\n    -------------\n    1) Accessing training and validation datasets for a particular fold number\n    \n    data_with_kfold = create_time_based_folds(data, 'date', 28, group_cols=['store', 'item'])\n    train_for_fold_1 = data_with_kfold[data_with_kfold['kfold'] != 1]\n    val_for_fold_1 = data_with_kfold[data_with_kfold['kfold'] == 1]\n\n    '''\n    \n    # Inplace or Copy\n    if inplace:\n        data_frame = input_data\n    else:\n        data_frame = input_data.copy()\n        \n    # ensure that the column is converted to a \n    # pandas datetime object \n    data_frame[date_col] = pd.to_datetime(data_frame[date_col])\n    \n    # Sort all observations by date for each of\n    # the groupby columns\n    data_frame = data_frame.groupby(group_cols)\\\n                 .apply(lambda x: x.sort_values(by=date_col))\\\n                 .reset_index(drop=True)\n    \n    max_date_in_data = data_frame[date_col].max()\n\n    date_ranges = []\n    \n    for idx, fold_num in enumerate(range(1, num_folds+1)):\n        date_range_fold = pd.date_range(max_date -\\\n                                        pd.DateOffset(fold_num*(num_days-1)),\\\n                                        max_date -\\\n                                        pd.DateOffset((fold_num-1)*(num_days-1))\n                                       )\n        date_ranges.append(date_range_fold)\n    \n    data_frame['kfold'] = -1\n    \n    date_folds = zip(reversed(date_ranges), range(num_folds))\n    for idx, (date_range, fold_num) in enumerate(date_folds):\n        data_frame.loc[data_frame[date_col].isin(date_range), 'kfold'] = fold_num\n            \n    return data_frame","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"kfold_data = create_time_kfolds_days(train_data, 'Date', 48, ['Store'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For this particular problem, we can just use one time period of 48 days","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_kfold = kfold_data[kfold_data.kfold < 4]\n\nval_kfold = kfold_data[kfold_data.kfold == 4]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_kfold.Date.min()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_kfold.Date.max()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"val_kfold.Date.min()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"val_kfold.Date.max()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"To validate a model on a fold number of `k`, you can extract your train and validation sets using the code below\n\n    train_kfold = kfold_data[kfold_data.kfold < k]\n \n    val_kfold = kfold_data[kfold_data.kfold == k]","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"This now, allows us to build models and validate on different, disconnected systems without worrying about the processor architecture that generates random numbers based on a seed.\n\nWe can fully reproduce our results on each of the validation sets","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}