{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Rossmann Store Sales using Random Forest and XGBoost\n#### Author : Rohini Garg"},{"metadata":{},"cell_type":"markdown","source":"-----------------------------------------------------------------------------------------------------"},{"metadata":{},"cell_type":"markdown","source":"*********************************************************************************************"},{"metadata":{},"cell_type":"markdown","source":"### Objective of  analysis of Rossmann Store Sales?\n#### Forecast sales using store, promotion, and competitor data"},{"metadata":{},"cell_type":"markdown","source":"#### About Data"},{"metadata":{},"cell_type":"markdown","source":"### Files:\n\n* **train.csv**: historical data including Sales\n* **test.csv**: historical data excluding Sales\n* **sample_submission.csv**: a sample submission file in the correct format\n* **store.csv**: supplemental information about the stores\n\n\n************************************************************************\n### Data fields\n* ** Most of the fields are self-explanatory. The following are descriptions for those that aren't.**:\n\n* **Id** - an Id that represents a (Store, Date) duple within the test set\n* **Store** - a unique Id for each store\n* **Sales** - the turnover for any given day (this is what you are predicting)\n* **Customers** - the number of customers on a given day\n* **Open** - an indicator for whether the store was open: 0 = closed, 1 = open\n* **StateHoliday** - indicates a state holiday. Normally all stores, with few exceptions, are closed on state holidays. Note that all schools are closed on public holidays and weekends. a = public holiday, b = Easter holiday, c = Christmas, 0 = None\n* **SchoolHoliday** - indicates if the (Store, Date) was affected by the closure of public schools\n* **StoreType** - differentiates between 4 different store models: a, b, c, d\n* **Assortment** - describes an assortment level: a = basic, b = extra, c = extended\n* **CompetitionDistance** - distance in meters to the nearest competitor store\n* **CompetitionOpenSince[Month/Year]** - gives the approximate year and month of the time the nearest competitor was opened\n* **Promo** - indicates whether a store is running a promo on that day\n* **Promo2** - Promo2 is a continuing and consecutive promotion for some stores: 0 = store is not participating, 1 = store is participating\n* **Promo2Since[Year/Week]** - describes the year and calendar week when the store started participating in Promo2\n* **PromoInterval** - describes the consecutive intervals Promo2 is started, naming the months the promotion is started anew. E.g. \"Feb,May,Aug,Nov\" means each round starts in February, May, August, November of any given year for that store"},{"metadata":{"trusted":true},"cell_type":"code","source":"#1.0 Clear memory\n%reset -f","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Call libraries"},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\n\n# 1.1 Call data manipulation libraries\nimport pandas as pd\nimport numpy as np\nfrom scipy.stats import kurtosis, skew\n\n# 1.3 Dimensionality reduction\nfrom sklearn.decomposition import PCA\n\nfrom sklearn import preprocessing \n# 1.3 Data transformation classes\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import LabelEncoder \nfrom sklearn.preprocessing import StandardScaler \n \n\n\n# 1.4 Data splitting and model parameter search\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom xgboost.sklearn import XGBClassifier\n\n# 1.6 Model pipelining\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.compose import ColumnTransformer\n\n\n# 1.7 Model evaluation metrics\nfrom sklearn.metrics import accuracy_score, f1_score\nfrom sklearn.metrics import plot_roc_curve\nfrom sklearn.metrics import confusion_matrix\n\n# 1.8\nimport matplotlib.pyplot as plt\nfrom xgboost import plot_importance\n\n\n\n# 1.9 RandomForest modeling\nfrom sklearn.ensemble import RandomForestClassifier \n\n# 2.0 Misc\nimport os, gc\n\nfrom scipy.stats import uniform\n\n#Graphing\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.graph_objects as go\nimport plotly.express as px\nfrom matplotlib.colors import LogNorm\n\n# to display all outputs of one cell\nfrom IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = \"all\"\n\n#hide warning\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Read Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"os.chdir(\"/kaggle/input/rossmann-store-sales\")\nos.listdir()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dftrain=pd.read_csv(\"train.csv\")\ndfstore=pd.read_csv('store.csv')\ndftrain.head()\ndfstore.head()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Check train database if any NULL value"},{"metadata":{"trusted":true},"cell_type":"code","source":"dftrain.columns[dftrain.isnull().any()]\n\n#no column has null value so need to fix null values\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndfstore.columns[dfstore.isnull().any()]\ndfstore.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### merge data frames"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train_store = dftrain.merge(dfstore, on = 'Store', copy = False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### consider only open stores and sale > 0 .Because data is already too big"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train_store=df_train_store[df_train_store.Open != 0]\ndf_train_store=df_train_store[df_train_store.Sales > 0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train_store.shape\n#decreased by 2 lakh approx","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### calculate competition open and promo open in months"},{"metadata":{"trusted":true},"cell_type":"code","source":"#get year month day from date column & drop column\nimport calendar\ndf_train_store['Date']=pd.to_datetime(df_train_store['Date'])\ndf_train_store['Year']=df_train_store['Date'].dt.year\ndf_train_store['month']=df_train_store['Date'].dt.month\ndf_train_store['weekofyear']=df_train_store['Date'].dt.weekofyear\ndf_train_store['month_name']=df_train_store['month'].apply(lambda x: calendar.month_abbr[x])\n\n\n#df_train_store.drop(['Date'], axis = 1, inplace= True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"####  Calculate compition open and promo time in months"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train_store['CompetitionOpen'] = 12 * (df_train_store.Year - df_train_store.CompetitionOpenSinceYear) + (df_train_store.month - df_train_store.CompetitionOpenSinceMonth)\ndf_train_store['CompetitionOpen'] = df_train_store.CompetitionOpen.apply(lambda x: x if x > 0 else 0)\ndf_train_store.drop(['CompetitionOpenSinceMonth', 'CompetitionOpenSinceYear'], axis = 1,  inplace = True)\n\ndf_train_store['PromoOpen'] = 12 * (df_train_store.Year - df_train_store.Promo2SinceYear) + (df_train_store.weekofyear - df_train_store.Promo2SinceWeek) / float(4)\ndf_train_store['PromoOpen'] = df_train_store.CompetitionOpen.apply(lambda x: x if x > 0 else 0)\ndf_train_store.drop(['Promo2SinceYear', 'Promo2SinceWeek'], axis = 1,  inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n#drop na values\ndf_train_store.dropna(inplace = True)\n\n#convert PromoInterval and month_name in string\ndf_train_store['PromoInterval']=df_train_store['PromoInterval'].astype(str)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef checkpromomonth(row):\n if (row['month_name'] in row['PromoInterval']):\n    return 1\n else:\n    return 0\ndf_train_store['IsPromoMonth'] =  df_train_store.apply(lambda row: checkpromomonth(row),axis=1)\n\n#Drop Date,month_name,PromoInterval\ndf_train_store.drop(['Date', 'month_name','PromoInterval'], axis = 1,  inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#convert num columns into float 32\ndf_train_store.dtypes.value_counts()\nnum_columns= df_train_store.select_dtypes(exclude=[object]).columns \ncat_columns=df_train_store.select_dtypes(include=[object]).columns \nfor col in num_columns:\n    df_train_store[col]=df_train_store[col].astype('float32')\n\nle = preprocessing.LabelEncoder()\nfrom sklearn import preprocessing\nfor col in cat_columns:\n    df_train_store[col]=le.fit_transform(df_train_store[col].astype('str'))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nimport math\nplt.figure(figsize=(15,18))\nnoofrows= math.ceil(len(num_columns)/3)\n\n\n#set false.Other wise error if  bandwidth =0 \nsns.distributions._has_statsmodels=False\n\nfor i in range(len(num_columns)):\n plt.subplot(noofrows,3,i+1)\n out=sns.distplot(df_train_store[num_columns[i]]) \n    \nplt.tight_layout()\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*  **There are outliers for most of columns so we will use RobustScaler for num_columns** \n* **OneHotEncoder for cat_columns**\n* **define avgsale\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train_store.Sales.mean()\ndf_train_store.loc[(df_train_store.Sales >= df_train_store.Sales.mean()),'aboveAvgSale']=1\ndf_train_store.loc[(df_train_store.Sales < df_train_store.Sales.mean()),'aboveAvgSale']=0\ndf_train_store.aboveAvgSale.value_counts()\n#define y\ny=df_train_store.aboveAvgSale.astype('int')\ny1=np.log1p(df_train_store['Sales'])\ndf_train_store.drop(['Sales','aboveAvgSale'], axis = 1,  inplace = True)\n\nnum_columns=num_columns.drop(labels=['Sales'])\nct=ColumnTransformer([\n    ('abc',RobustScaler(),num_columns),\n    ('abc1',OneHotEncoder(),cat_columns),\n    ],\n    remainder=\"passthrough\"\n    )\nct.fit_transform(df_train_store)\nX=df_train_store","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#store features\ncolnames = X.columns.tolist()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### split data in 7:3 ratio so set test size=30"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=.30)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Define Root Mean Square Percentage Error\n\nquadratic scoring rule that also measures the average magnitude of the error. Itâ€™s the square root of the average of squared differences between prediction and actual observation."},{"metadata":{"trusted":true},"cell_type":"code","source":"#credit : https://www.kaggle.com/tushartilwankar/sklearn-rf\ndef ToWeight(y):\n    w = np.zeros(y.shape, dtype=float)\n    ind = y != 0\n    w[ind] = 1./(y[ind]**2)\n    return w\n\ndef RMSPE(y, yhat):\n    w = ToWeight(y)\n    rmspe = np.sqrt(np.mean( w * (y - yhat)**2 ))\n    return rmspe ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Random forest"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\nrf = RandomForestRegressor(n_estimators = 15)\nrf.fit(X_train, y_train)\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#####  check RMSPE & Feature Importance of Random Forest"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import accuracy_score\n\n\ny_pred = rf.predict(X_test)\nerror = (RMSPE(y_test,y_pred))\nerror\nprint(\"RMSPE of Random Forest %\",error * 100)\n\n#top 10 features of Random Forest\nfeat_importances_rf = pd.Series(rf.feature_importances_, index=colnames)\nfeat_importances_rf.nlargest(10).sort_values(ascending = True).plot(kind='barh')\nplt.xlabel('importance')\nplt.title('Feature Importance in Random Forest')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### XGBoost"},{"metadata":{"trusted":true},"cell_type":"code","source":"ss=preprocessing.StandardScaler\nsteps_xg = [('sts', ss() ),\n            ('pca', PCA()),\n            ('xg',  XGBClassifier(silent = False,\n                                  n_jobs=2)        # Specify other parameters here\n            )\n            ]\n# Instantiate Pipeline object\npipe_xg = Pipeline(steps_xg)\n# What parameters in the pipe are available for tuning\npipe_xg.get_params()\n\nparameters = {'xg__learning_rate':  [0.03, 0.05], # learning rate decides what percentage\n                                                  #  of error is to be fitted by\n                                                  #   by next boosted tree.\n                                                  # See this answer in stackoverflow:\n                                                  # https://stats.stackexchange.com/questions/354484/why-does-xgboost-have-a-learning-rate\n                                                  # Coefficients of boosted trees decide,\n                                                  #  in the overall model or scheme, how much importance\n                                                  #   each boosted tree shall have. Values of these\n                                                  #    Coefficients are calculated by modeling\n                                                  #     algorithm and unlike learning rate are\n                                                  #      not hyperparameters. These Coefficients\n                                                  #       get adjusted by l1 and l2 parameters\n              'xg__n_estimators':   [200,  300],  # Number of boosted trees to fit\n                                                  # l1 and l2 specifications will change\n                                                  # the values of coeff of boosted trees\n                                                  # but not their numbers\n\n              'xg__max_depth':      [4,6],\n              'pca__n_components' : [10,15]\n              }  \n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Tune parameters using  gridsearch"},{"metadata":{"trusted":true},"cell_type":"code","source":"import time\nclfgs = GridSearchCV(pipe_xg,            # pipeline object\n                   parameters,         # possible parameters\n                   n_jobs = 2,         # USe parallel cpu threads\n                   cv =2 ,             # No of folds\n                   verbose =2,         # Higher the value, more the verbosity\n                   scoring = ['accuracy', 'roc_auc'],  # Metrics for performance\n                   refit = 'roc_auc'   # Refitting final model on what parameters?\n                                       # Those which maximise auc\n                   )\n#Start fitting data to pipeline\nstart = time.time()\nclfgs.fit(X_train, y_train)\n   ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### accuracy and predictions of GridSearchCV"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = clfgs.predict(X_test)\ny_pred\n\n# 7.5 Accuracy\naccuracy = accuracy_score(y_test, y_pred)\nf\"Accuracy using GridSearchCV: {accuracy * 100.0}\"             \n\n# 7.6 Confusion matrix\n\n\nfrom sklearn.metrics import confusion_matrix\ncm_gs = pd.DataFrame(confusion_matrix(y_test, y_pred))\nsns.heatmap(cm_gs, annot=True)\nplt.xlabel('Predicted')\nplt.ylabel('Actual')\nplt.title('Predicted vs Actual -GridSearchCV ')\n\n# 7.7 F1 score\nf1_score(y_test,y_pred, pos_label = 1)      \nf1_score(y_test,y_pred, pos_label = 0)      \n\n# 7.8 ROC curve\nplot_roc_curve(clfgs, X_test, y_test)\n\n\n# Get feature importances from GridSearchCV best fitted 'xg' model\n#     See stackoverflow: https://stackoverflow.com/q/48377296\nclfgs.best_estimator_.named_steps[\"xg\"].feature_importances_\nclfgs.best_estimator_.named_steps[\"xg\"].feature_importances_.shape\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###### AUC is Outstanding so discrimination between positive class and negative class"},{"metadata":{},"cell_type":"markdown","source":"#### Tuning parameters using randomized search"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Hyperparameters to tune and their ranges\nparameters = {'xg__learning_rate':  uniform(0, 1),\n              'xg__n_estimators':   range(50,300),\n              'xg__max_depth':      range(3,10),\n              'pca__n_components' : range(10,17)}\n\n\n\n# 8.1 Tune parameters using random search\n#     Create the object first\nrs = RandomizedSearchCV(pipe_xg,\n                        param_distributions=parameters,\n                        scoring= ['roc_auc', 'accuracy'],\n                        n_iter=15,          \n                                            \n                        verbose = 3,\n                        refit = 'roc_auc',\n                        n_jobs = 2,          # Use parallel cpu threads\n                        cv = 2               \n                                             \n                        )\n\n\n# \nrs.fit(X_train, y_train)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### accuracy and predictions of RandomSearchCV"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = rs.predict(X_test)\n\n\naccuracy = accuracy_score(y_test, y_pred)\nf\"Accuracy using randomized search: {accuracy * 100.0}\"        \nf1_score(y_test,y_pred, pos_label = 1) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{"trusted":true},"cell_type":"code","source":"model_gs = XGBClassifier(\n                    learning_rate = clfgs.best_params_['xg__learning_rate'],\n                    max_depth = clfgs.best_params_['xg__max_depth'],\n                    n_estimators=clfgs.best_params_['xg__max_depth']\n                    )\n\n# 9.1 Model with parameters of random search\nmodel_rs = XGBClassifier(\n                    learning_rate = rs.best_params_['xg__learning_rate'],\n                    max_depth = rs.best_params_['xg__max_depth'],\n                    n_estimators=rs.best_params_['xg__max_depth']\n                    )\n\n\n# Modeling with both parameters\n\nmodel_gs.fit(X_train, y_train)\nmodel_rs.fit(X_train, y_train)\n\n#Predictions with both models\ny_pred_gs = model_gs.predict(X_test)\ny_pred_rs = model_rs.predict(X_test)\n\n#Accuracy from both models\naccuracy_gs = accuracy_score(y_test, y_pred_gs)\naccuracy_rs = accuracy_score(y_test, y_pred_rs)\nprint(\"Accuracy with GridSearch XGB model:\",accuracy_gs*100)\nprint(\"Accuracy with Random search XGB model:\",accuracy_rs*100)\n\nrmspe_gs = RMSPE(y_pred_gs,y_test)\nrmspe_rs = RMSPE(y_pred_rs,y_test)\nprint(\"RMSPE of GridSearch XGB modelt %\",rmspe_gs * 100)\nprint(\"RMSPE of Random search XGB modelt %\",rmspe_rs * 100)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### feature importances from Grid Search and Random Search"},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\n#  Plt now\n\n%matplotlib inline\nmodel_gs.feature_importances_\nmodel_rs.feature_importances_\n# Importance type: 'weight'\nplot_importance(\n                model_gs,\n                importance_type = 'weight'   # default\n                )\n#  Importance type: 'gain'\n#        # Normally use this\nplot_importance(\n                model_rs,\n                importance_type = 'gain', \n                title = \"Feature impt by gain\"\n                )\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### *Observations*  : \n* ** Customer is most important feature in Random forest,Grid search,Random search\n* ** Competition is second most important feature in Random forest,Grid search\n* ** RMSPE is least in random forest"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}