{"cells":[{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"","_uuid":"","trusted":true},"cell_type":"code","source":"#Imports\n\nimport pandas as pd\nimport seaborn as sns\nimport numpy as np\nimport scipy as sp\nimport math\nfrom scipy.signal import savgol_filter\nimport matplotlib.pyplot as plt\nimport warnings\nimport random\nwarnings.filterwarnings('ignore')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Reading from ther directory data\n\nX_test = pd.read_csv(\"../input/career-con-2019/X_test.csv\")\nX_train = pd.read_csv(\"../input/career-con-2019/X_train.csv\")\nsample_submission = pd.read_csv(\"../input/career-con-2019/sample_submission.csv\")\ny_train = pd.read_csv(\"../input/career-con-2019/y_train.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Let look at the shape of the data\nprint('X_Train ',X_train.shape)\nprint('X_Test',X_test.shape)\nprint('y_train',y_train.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"target = y_train['surface'].value_counts().reset_index().rename(columns={'index':'target'})\n\n#Plotting the surface i.e labels and their count in y_train\ntarget.plot.bar(x='target',y = 'surface')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#This is the pie plot, where each surface in the train dataset holds how much percentage\nlabels = y_train['surface'].value_counts().index\nvalues = y_train['surface'].value_counts().values\nexplode = [round(random.uniform(0.04,.01),2) for i in range(len(labels))]\nfig = plt.figure(figsize=(10,10))\nplt.axis('equal')\nplt.title('Percentage Distribution for surface category')\nplt.pie(values,labels = labels,radius = 1,autopct='%.2f%%',startangle=45,explode=explode)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Before we start plotting the features using seaborn libraries, let us see the correlation matrix. This matrix tells us relation between every parameter to every other parameter in the dataset. There are negative, positive and no-relation types of correlations."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Calculating the corr in the dataset\n\ncorr = X_train.corr()\nfig, ax = plt.subplots(1,1, figsize = (15,5))\nhm = sns.heatmap(X_train.iloc[:,3:].corr(),\n                ax = ax,\n                cmap = 'twilight_shifted_r',\n                annot = True,\n                fmt = '.4f',\n                linewidths = 0.05)\nfig.subplots_adjust(top=0.90)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Infered : orientation_Y and orientation_Z are highly correlated.\nAlso, orientation_X and orientation_W show positive correlation.\nangular_velocity_Y and angular_velocity_Y are negative coorelated.\nlinear_acceleration_Y and linear_acceleration_Z also shows some relation."},{"metadata":{},"cell_type":"markdown","source":"1. Voilin Plot are the plots that combines both kde distribution and the wisker plot distribution.\n2. We can easily see, what's the median and how well is the distribution formed without using kde and distplot.\n3. They are also good for detecting outliers in the datasets.\n\nLet's visualize out each features distribution"},{"metadata":{"trusted":true},"cell_type":"code","source":"i= 0\ncolor = ['tab:blue','tab:green','tab:red','tab:cyan','tab:purple','tab:orange','tab:olive','tab:gray','tab:brown','tab:pink']\n\nfig = plt.figure(figsize=(25,15))\nfor col in X_train.columns[3:13]:\n    plt.subplot(3,4,i+1)\n    sns.violinplot(x=X_train[col],size=8,color = color[i])\n    i+=1\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Clearly seen from the violin plot, the inferences concluded from correlation matrix is getting reflected.\n* Another set of observation, all features are almost near zero mean and it's a good thing, we can avoid standardizing those as they lie in approx same scale range of zero mean.\n\nNow that we looked at combine kde and distribution plot, let's look at individual plots to more about data."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Kde Plot for each col to understand the underlying data\n\nfig = plt.figure(figsize=(26,16))\ntarget = y_train['surface'].value_counts().index\ndf = X_train.merge(y_train,on='series_id',how='inner')\nfor i,col in enumerate(df.columns[3:13]):\n    ax = plt.subplot(3,4,i+1)\n    ax = plt.title(col)\n    for surface in target:\n        surface_df = df[df['surface'] == surface]\n        sns.kdeplot(surface_df[col],label=surface)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* It can be seen for each surface type,orientation_X and orientation_W exhibit same behavior/orientation_Y and orientation_Z exhibit approx same nature.\n* Now that we are done with high level observation for all columns let's dive in deeper on basis of series id.\nSegregating the dataframe based on series_id"},{"metadata":{"trusted":true},"cell_type":"code","source":"\nseries_dict = {}\nseries_dictTest = {}\n\nfor series in X_train['series_id'].unique():\n    series_dict[series] = X_train[X_train['series_id'] == series]\n\n\nfor series in X_test['series_id'].unique():\n    series_dictTest[series] = X_test[X_test['series_id'] == series]   \n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\ndef noiseDetection(series_id):\n    \n    i = 0\n    plt.figure(figsize=(28, 16))\n    print('Label is ', y_train[y_train['series_id'] == series_id]['surface'].values[0])\n    for col in series_dict[series_id].columns[3:13]:\n        if('orientation_W' == col):\n            continue\n        if ('orientation' in col):\n            color = 'orange'\n        elif('angular' in col):\n            color = 'blue'\n        else:\n            color = 'green'\n        plt.subplot(3,3,i+1)\n        plt.plot(series_dict[series_id][col],linewidth = 3,color = color)\n        plt.title(col)\n        i+=1\n\n        \n        \ndef noiseFreePlot(series_id):\n    \n    i = 0\n    plt.figure(figsize=(25, 13))\n    print('Label is ',y_train[y_train['series_id'] == series_id]['surface'].values[0])\n    for col in series_dict[series_id].columns[3:13]:\n        if('orientation_W' == col):\n            continue\n        if('orientation' in col):\n            color = 'orange'\n        elif('angular' in col):\n            color = 'blue'\n        else:\n            color = 'green'\n        plt.subplot(3,3,i+1)\n        filter_pass = savgol_filter(series_dict[series_id][col],7,2)\n        plt.plot(filter_pass,linewidth = 3,color = color)\n        plt.title(col)\n        i+=1\n    \nnoiseDetection(5)\nnoiseFreePlot(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> As we can see it is now evident the observed noise in the values of features in dataset, which were earlier seen on high level scale.\n> This can ruin the training and may cause reduction in training performance.\n> We need to filter the noise but retaining the statistical importance of the features.\n> I found the article which clearly explains how savgol_filter can help us smooth the data.\n> I have shown one series_id plot to visualize the noise in the data and when filtered the data has become quite smooth."},{"metadata":{},"cell_type":"markdown","source":"> Savitzkyâ€“Golay filter is one of the technique to smooth the data, aka lease-square method. It works on fitting the data using gradient descent and find out best weights that could provide us the estimated smoothed signal.\n> More information can be found at : https://en.wikipedia.org/wiki/Savitzky%E2%80%93Golay_filter\n\n> Referred : https://www.mikulskibartosz.name/smoothing-time-series-in-python-using-savitzky-golay-filter/\n\n> Working :\n\n> * It has two parameters first how many data points will be used to fit a polynomial regression function. The second parameter specifies the degree of the fitted polynomial function. \n> * In every window, a new polynomial is fitted, which gives us the effect of smoothing the input dataset.In every step, the window moves and a different part of the original dataset is used. Then, the local polynomial function is fitted to the data in the window, and a new data point is calculated using the polynomial function. After that, the window moves to the next part of the dataset, and the process repeats.\n"},{"metadata":{},"cell_type":"markdown","source":"> This section is about removing noise from the dataset and set it up in the series dict for train and test datasets.\n> It also calculates the amount of time required to do this transformation.\n> This smoothning is done on both test and train dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"\nimport time\nprint('Filtering for all train and test data')\n\n\ndef filterTrainTestData(series_id,series_dict):\n    for col in series_dict[series_id].columns[3:13]:\n        filter_pass = savgol_filter(series_dict[series_id][col],7,2)\n        series_dict[series_id][col] = filter_pass\n\n        \nstart = time.time()\nfor i in X_train['series_id'].unique().tolist():\n    #print(f'Series_id {i} for Train is running')\n    filterTrainTestData(i,series_dict)\n    \nend = time.time()   \n\nprint(f'Time eclapsed to reduce the noise in Train Data is {(end-start)} secs')\n\nstart = time.time()\n\nfor j in X_test['series_id'].unique().tolist():\n    #print(f'Series_id {j} for Test is running')\n    filterTrainTestData(j,series_dictTest)\n    \nend = time.time()   \n\nprint(f'Time eclapsed to reduce the noise in Test Data is  {(end-start)} secs')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> Concating the dictionary to get the denoised data which were computed earlier\n> Reason being : As we have filtered the noise and restored the dataframes as per dict_ mappings we need to combine those dataframes to get the whole dataset for next process"},{"metadata":{"trusted":true},"cell_type":"code","source":"\ntrain_frames = []\nfor i in X_train['series_id'].unique():\n    train_frames.append(series_dict[i])\n    \ndata_noise_free = pd.concat(train_frames)\n\ntest_frames = []\n\nfor i in X_test['series_id'].unique():\n    test_frames.append(series_dictTest[i])\n\ntest_noise_free = pd.concat(test_frames)\nprint(data_noise_free.shape,test_noise_free.shape)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> Quaternion system is an extension to complex numbers, it can be converted to euler angles to help viewing rotational of objects around x,y,z axis. Quaternion are difficult to understand hence to manipulate in another co-ordinate system is useful.\n> Formulas taken from : https://www.euclideanspace.com/maths/geometry/rotations/conversions/quaternionToEuler/"},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef quaternion_to_euler(qx,qy,qz,qw):\n    \n    #x-axis rotation\n    #atan2 return the angle theta in range pi to -pi and it's the angle made with positive x axis\n    \n    sinr_cosp = 2.0 * (qw * qx + qy + qz)\n    cosr_cosp = 1.0 - 2.0 * (qx * qx + qy * qy)\n    roll = math.atan2(sinr_cosp, cosr_cosp)\n    \n    # y-axis rotation\n    sinp = 2.0 * (qw * qy - qz * qx)\n    if(math.fabs(sinp) >= 1):\n        pitch = copysign(M_PI/2, sinp)\n    else:\n        pitch = math.asin(sinp)\n        \n    #z-axis rotation\n    siny_cosp = 2.0 * (qw * qz + qx * qy)\n    cosy_cosp = 1.0 - 2.0 * (qy * qy + qz * qz)\n    yaw = math.atan2(siny_cosp, cosy_cosp)\n    \n    return roll, pitch, yaw","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> This section converts the quaternion system to euler angles and append this in the train and test dataframe respectively"},{"metadata":{"trusted":true},"cell_type":"code","source":"def eular_angle(data):\n    og_x = list(data['orientation_X'].values) \n    og_y = list(data['orientation_Y'].values)\n    og_z = list(data['orientation_Z'].values)\n    og_w = list(data['orientation_W'].values)\n    \n    c_x, c_y, c_z = [], [], []\n    for i in range(len(og_x)):\n        xx, yy, zz = quaternion_to_euler(og_x[i], og_y[i], og_z[i], og_w[i])\n        c_x.append(xx)\n        c_y.append(yy)\n        c_z.append(zz)\n    \n    data['euler_x'] = c_x\n    data['euler_y'] = c_y\n    data['euler_z'] = c_z\n    \n    return data\n\ndata = eular_angle(data_noise_free)\ntest = eular_angle(test_noise_free)\nprint(data.shape,test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> In this function, we will add new features to the dataset as this is useful for training models to make their decisions more effectively.\n> Reason for adding \"Total\" is because each of this vectors has directions in different axis, finding magnitude of all three components would make resultant component and that will decide the final motion of the rover."},{"metadata":{"trusted":true},"cell_type":"code","source":"def Feature_Engineering1(data):\n    data['T_angular_velocity'] = (data['angular_velocity_X']**2 + data['angular_velocity_Y']**2 + data['angular_velocity_Z']**2) **0.5\n    data['T_orientation'] = (data['orientation_X']**2 + data['orientation_Y']**2 + data['orientation_Z']**2) ** 0.5\n    data['T_linear_acc'] = (data['linear_acceleration_X']**2 + data['linear_acceleration_Y']**2 + data['linear_acceleration_Z']**2) ** 0.5\n    data['T_angle'] = (data['euler_x']**2 + data['euler_y']**2 + data['euler_z']**2) ** 0.5\n    \n    return data\n\n\ndata_modified = Feature_Engineering1(data)\ntest_modified = Feature_Engineering1(test)\nprint(data_modified.shape,test_modified.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> Reason for adding statistical features to the data we want to consice the dataset.Previously,the datasize was equal to 487k train and 488k test, now we will compute as per series_id and make size 3810 tuples.\n> Another reason is to handle the time complexity along with accuracy,so minimizing the training data samples without losing statistical importance is the main agenda of a good machine learning model."},{"metadata":{},"cell_type":"markdown","source":"> Median Absolute Deviation is also one of the imp stats other than mean,mean,std,var. It is less affected by outliers because outliers have a smaller effect on the median than they do on the mean. When data is normal, std and mean defines the spreadness very well but when in case of outliers, MAD is robust to use."},{"metadata":{"trusted":true},"cell_type":"code","source":"def Feature_Engineering2(data):\n    df = pd.DataFrame()\n    \n    for col in data.columns[3:len(data.columns)]:\n        per_col_data = data.groupby(['series_id'])[col]\n        df[col+'_mean'] = per_col_data.mean()\n        df[col+'_median'] = per_col_data.median()\n        df[col+'_std'] = per_col_data.std()\n        df[col + '_mad'] = data.groupby(['series_id'])[col].apply(lambda x: np.median(np.abs(np.diff(x))))\n        df[col+'_var'] = per_col_data.std()**2\n    return df\n\n\n  \ncompiled_data = Feature_Engineering2(data_modified)\ncompiled_test = Feature_Engineering2(test_modified)\nprint(compiled_data.shape,compiled_test.shape) \n\n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Making values numeric for Machine Learning Model to Train on\n#Using Label Encoder we can map categories into integer values, this is helpful as numeric values the machine understands pretty well.\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import KFold,train_test_split\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, ExtraTreesClassifier\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nfrom sklearn import linear_model\nimport gc\nimport xgboost as xgb\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#One Hot encoding technique to convert categorical labels into numeric integers.\nle = LabelEncoder()\ntarget_label = y_train['surface']\ny_train['surface'] = le.fit_transform(y_train['surface'])\n\n\n#Verbose for mapping labels with encoded value\n\nzipping  = zip(target_label,y_train['surface'].values)\nset_zip = set(zipping)\ndict_map = {idx : surf for surf,idx in set_zip}\ndict_map = sorted(dict_map.items(),key=lambda x : x[1],reverse=False)\nprint(dict_map)\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> * Before fitting the model, it's important to know the hyperparamaters to assign that best gives us the optimal accuracy score.\n> * Here comes in Play **RandomSearchCv**,because it randomly apply different combinations of params passed and return best accuracy score achieved when applied this combination of parameter. **GridSearchCv** computing time is more and it might be possible some irrelevant feature would be given more importance resulting in **overfitting**.\n> * RandomSearch is done using **cross-validation** and number of iterations denotes how many times this process should repeat.\n> * **Scoring** is done on bases of **f1_macro** as it calculate metrics for each label, and find their unweighted mean. This does not take label imbalance into account but we figured out from plots that classes in y_train are not skewed."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import RandomizedSearchCV,cross_val_score\nfrom sklearn.metrics import f1_score, make_scorer\nfrom datetime import datetime\nimport time","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Timer Function is used to calculate time taken for randomsearch cv to fit the data\n\ndef timer(startTime=None):\n    time_in_secs = (datetime.now()-startTime).total_seconds()\n    hour,temp_sec = time_in_secs//3600, time_in_secs%3600\n    minute,secs = temp_sec//60, temp_sec % 60\n    print(f'Time taken to complete is {hour} hours, {minute} mins and {round(secs,3)} secs')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef bestEstimate(model):\n    Xx = compiled_data.values\n    Yy = y_train['surface'].values\n    model.fit(Xx,Yy)\n    \n    #Return classifier\n    return model","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> Uncomment them when you are running for the first time, it takes around 1.5 hr to get complete.\n> So I have used pickle to store the model and reload them, which saved our 100% time!!"},{"metadata":{"trusted":true},"cell_type":"code","source":"\nparams_xgb = {'learning_rate': [0.05, 0.10, 0.15, 0.20, 0.25, 0.30 ],\n          'max_depth': [7,8,9,10,12],\n          'n_estimators':[650,600,550,500],\n          'gamma':[0.0,0.1,0.2,0.3,0.4],\n          'min_child_weight' : [10,20,30,50,100],\n          'objective' : ['multi:softprob']\n         }\n\nparams_rf = {'max_depth':[8,9,11,10], \n              'min_samples_leaf':[3,9,10,4,5], \n              'min_samples_split':[11,18,21,25],\n              'n_estimators':[500,650,600,550],\n              'criterion' : ['gini','entropy']}\n\nparams_etc = {'max_depth':[7,8,9,10], \n              'min_samples_leaf':[3,9,10,4,5], \n              'min_samples_split':[11,18,21,25],\n              'n_estimators':[450,475,550,500],\n              'criterion' : ['gini','entropy']}\n\n#Initializing the instances of the classifier\nclf_xgb = xgb.XGBClassifier(n_jobs = -1)\nclf_rf = RandomForestClassifier(n_jobs=-1)\nclf_etc = ExtraTreesClassifier(n_jobs=-1)\n\n#Initial setup for instantiating RandomSearchCv\nmodel_xgb = RandomizedSearchCV(clf_xgb,param_distributions=params_xgb,n_iter=15,scoring='f1_macro',n_jobs=-1,cv=5,verbose=3)\nmodel_rf  = RandomizedSearchCV(clf_rf,param_distributions=params_rf,n_iter=13,scoring='f1_macro',n_jobs=-1,cv=5,verbose=3)\nmodel_etc = RandomizedSearchCV(clf_etc,param_distributions=params_etc,n_iter=15,scoring='f1_macro',n_jobs=-1,cv=7,verbose=3)\n\n#Calculating best params\n\nstart_time = datetime.now()\nmodel_rf = bestEstimate(model_rf)\ntimer(start_time)\n\nstart_time = datetime.now()\nmodel_etc = bestEstimate(model_etc)\ntimer(start_time)\n\nstart_time = datetime.now()\nmodel_xgb = bestEstimate(model_xgb)\ntimer(start_time)\n\nprint('Completed')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Again Uncomment them when running for first time as we write the results in .pickle file"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Pickling helps in save the object in serialized manner and it does this by writing it into word bytes.\n#Advantages : Once we got the best model parameters we can save it in this file to avoid re-training the model again\n#It reduces the time by 99% as my training time is around ~1.5hrs\n\nimport pickle\n\n\n#Writing to a pickle\n\nrandom_pickle_out = open('random_clf.pickle','wb')\npickle.dump(model_rf,random_pickle_out)\nrandom_pickle_out.close()\n\netc_pickle_out = open('etc_clf.pickle','wb')\npickle.dump(model_etc,etc_pickle_out)\netc_pickle_out.close()\n\nxgb_pickle_out = open('xgb_clf.pickle','wb')\npickle.dump(model_xgb,xgb_pickle_out)\nxgb_pickle_out.close()\n\n\n#Reading from a pickle\n\n\nrandom_pickle_in = open('random_clf.pickle','rb')\nmodel_r = pickle.load(random_pickle_in)\nrandom_pickle_in.close()\n\netc_pickle_in = open('etc_clf.pickle','rb')\nmodel_e = pickle.load(etc_pickle_in)\netc_pickle_in.close()\n\n\nxgb_pickle_in = open('xgb_clf.pickle','rb')\nmodel_x = pickle.load(xgb_pickle_in)\nxgb_pickle_in.close()\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Kfold cross validation is a technique to overcome overfitting of the data. It's better than other techniques like train_test split or random uniform shuffle."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Setting up parameters for Kfold Cross Validation\n\nkf = KFold(n_splits = 10,random_state=40,shuffle=True)\n\n\n#Measured is used for Ensembling Training accuracy results\n#Predicted is used for Ensembling Testing samples\n\npredicted = np.zeros((compiled_test.shape[0],9))  #In total there are 9 surfaces\nmeasured= np.zeros((compiled_data.shape[0],3))    #Used for 3 algorithms\nscore = 0\n\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n**RandomForest** - Works on bagging concept where Boostrap data is formed from the original data.\nRandom subset of variables or the columns are selected and split of the node happens on gini indexes and information gain for chosen variables. When trees are created with defined n_estimator, we then test on unseen tuple which were not in the boostrap data.\nIt works on Majority voting technique wherein the test_data_samples are made to run on those trees which were not created with those samples and the tree classify the surfaces and out of it the majority vote given to a surface is chosen.\n\n**Advantage of RF:**\n* Bias is minimized as the full decision trees are created which results in good model fit.\n* Aggregate voting for making decisions.\n\n**XGBoost** - Works similar to AdaBosst and converge using stochastic gradient descent which reduces the error depending on the number of trees to be formed. When incorrectly classified the weight of the feature node is increased and correctly clasified weight is decreased.\nThis help is taking into account misclassified features and try to correct the mistake.This trees are called stumps and they are weak learners, so after n_estimators of trees formed, the model get better and better, weights get updated and we get good classification model.\n\n**Advantage:**\n* Prune the tree reducding the complexity of computing the entropy for the further node\n* Invites regularization which kind of help in achieving the accuracy better as it promote weights of less complexity and try to reduce the features of the model.\n\n**Extra Tree:**\n* It's also called as Extremely Randomized Tree, it works similar to RF.\n* Use unique samples  rather than bootstrap replicas in order to grow the diversify trees. \n* It randomly selects the node and split it randomly too and repeat this step until whole tree is created.\n* Total number of features remains the same i.e sqrt(total_features)\n* It keeps on building trees as specified hyper paramter \"n_estimator\"\n* Works on majority voting technique for classification.\n\n**Advantage:**\n* Faster than all other decision trees and can balance the running time without much losing the accuracy.\n* Works well with noise data, though we tried to filter out the noise\n\n**Hyper Parameters explained:**\n* min_samples_split = min number of samples required to make a split\n* min_samples_leaf = min number of samples required to stop the tree from growing.\n* max_depth = max depth of the tree.\n* gamma = regularization constant used for pruning the trees in Xgboost\n* learning rate/ step_size = how fast you wanna converge used in Xgboost\n* min_child_weight = given some weights to the node and then tweak those to improve from weak learners.\n* n_jobs = -1 use all cores of the cpu to compute.\n"},{"metadata":{},"cell_type":"markdown","source":"> Confusion Matrix : This tells us how our model has performed in terms of classifying the tuples into relevant lables (in our case it's surface)\n> With this, we can determine where our model is going wrong to classify and take some necessary measures to improve it."},{"metadata":{"trusted":true},"cell_type":"code","source":"\n#Training the model and Evaluating the Training accuracy\n#Also predicting the unseen tuples \n#Ensembling the results by majority voting techqnique\n\n\nfrom sklearn import metrics\ndf_pred = pd.DataFrame()\nalgos = ['Random','XGBoost','ExtraTree']\nconf_matrix = []\n\ndef algoRunner(df_pred,algos,model,model_num): \n    score = 0\n    score_calc = []\n    \n    predicted = np.zeros((compiled_test.shape[0],9))\n    print(f'Starting with {algos[model_num]}')\n    i = 0\n    for times,(train_index, test_index) in enumerate(kf.split(compiled_data.values,y_train['surface'].values)):\n        X_train,X_test = compiled_data.iloc[train_index],compiled_data.iloc[test_index]\n        y_train1,y_test = y_train['surface'][train_index],y_train['surface'][test_index]\n        \n        #Fitting the model as we have now the best hypertunning parameter to train the data\n        \n        model.fit(X_train,y_train1)\n        y_pred = model.predict(X_test)\n        conf_matrix.append(metrics.confusion_matrix(y_test, y_pred))\n        \n        #Inserting the prediction column wise in measure numpy array of size (3816,3)\n\n        measured[test_index,model_num] = y_pred\n        score += model.score(X_test,y_test)\n        score_calc.append(model.score(X_test,y_test))\n        print(f'Accuracy for fold {times} : {score_calc[times]}')\n        \n        #Making predictions on unseen data samples, predict_proba will return the list of probabilities  \n        predicted += model.predict_proba(compiled_test)/kf.n_splits\n\n    #Confusion matrix for each fold validation\n    \n    plt.figure(figsize=(20,15))\n    for i in range(len(conf_matrix)):\n        plt.subplot(3,4,i+1)\n        plt.title(f'Fold {i} {algos[model_num]} {round(score_calc[i],4)*100}%')\n        sns.heatmap(conf_matrix[i], annot=True, square = True)\n        plt.ylabel('Actual label')\n        plt.xlabel('Predicted label')\n        \n    score_calc.clear() \n    conf_matrix.clear()\n    #In 10 fold, the prediction done on unseen data is calculated and it has stored in dataframe called df_pred\n    \n    df_pred[algos[model_num]] = predicted.argmax(axis=1)\n    print(f'Model {algos[model_num]} final Training Score is ',score/kf.n_splits)\n    \n    #Garbage collector is used to free up space.\n    gc.collect()\n    \n\n#Without using RandomSearch I previously applied to the best of my knowledge the estimated guess for models\n\n#model_rf = RandomForestClassifier(n_estimators = 700,n_jobs=-1,max_depth = 10,min_sample_split = 20, min_leaf_split = 7)\n#model_xgb = xgb.XGBClassifier(objective = \"multi:softprob\", max_depth = 10, n_estimators = 700, learning_rate = 0.2, gamma = 0.4,min_child_weight = 50, nthread = -1, seed = 0)\n#model_etc = ExtraTreesClassifier(n_estimators = 700, max_depth = 10,random_state = 0,max_features = 10)\n\n#Calculated earlier from RandomSearchCv\nmodel_rf_ = model_r.best_estimator_\nmodel_xgb_ = model_x.best_estimator_\nmodel_etc_ = model_e.best_estimator_\n\n\n#Running the algos with RandomSearch optimization\nalgoRunner(df_pred,algos,model_rf_,0)\nalgoRunner(df_pred,algos,model_xgb_,1)\nalgoRunner(df_pred,algos,model_etc_,2)\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> Using K-Fold Cross Validation we can see how each model has predicted the surface\n> We can ensemble the results and check whether we can do better in training accuracy\n> As XGBoost performed well on y_test, this is just an attempt to combine three model prediction and use them to derive result\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"\nfrom collections import Counter\n\n#print(df_pred.shape)\n#df_pred.drop(columns = ['Ensemble_Result','Actual'],axis=1,inplace=True)\n\ndef ensemblePredictions(df_measured,type_):\n    df_measured.columns = ['Random','XGBoost','ExtraTree']\n\n    lst = []\n    for i in range(df_measured.shape[0]):\n        c = Counter(df_measured.iloc[i,:]).most_common(1)\n        for pred,count in c:\n            lst.append(pred)\n\n    df_measured['Ensemble_Result'] = pd.Series(lst)\n    \n    #We know the label for training i.e y_train after ensembling the models we are computing the accuracy\n    \n    if(type_ == 'training'):\n        df_measured['Actual'] = pd.Series(y_train['surface'].values)\n        wrong_pred = np.array(df_measured['Actual'] - df_measured['Ensemble_Result'])\n        print(((df_measured.shape[0] - np.count_nonzero(wrong_pred))/df_measured.shape[0] ))\n    \n    \nensemblePredictions(pd.DataFrame(measured),'training')\nensemblePredictions(df_pred,'testing')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> Yes we did approx **1% **better by ensembling the models on RF and ETC algos but we curtailed XGboost Prediction :("},{"metadata":{},"cell_type":"markdown","source":"> Let us look at the one of the decision tree generated by Random Forest and ExtraTree to know the importance of the features which are set as decision making parameters in the tree.\n> Graphviz is the library which help us getting the .dot file where all the manual rules of decision are written over\n> Pydot libary will help us interpreting the dot file and graphing out in graph form which is further saved as a png file"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import export_graphviz\nimport pydot\n\ndef plotTree(model,algo):\n    one_tree = model.estimators_[4]  #Getting the 400th tree\n    features_list = compiled_data.columns.tolist()\n    export_graphviz(one_tree, out_file = f'one_tree_{algo}.dot', feature_names = features_list, rounded = True, precision = 1)\n    (graph, ) = pydot.graph_from_dot_file(f'one_tree_{algo}.dot')\n    graph.write_png(f'tree_{algo}.png')\n    \n    \nplotTree(model_rf_,\"RandomForest\")\nplotTree(model_etc_,\"ETC\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> Finally, I converted the encoded labels into orginal surface names and saved it in submission.csv file.\n> I have also plotted which surface has been predicted and their counts."},{"metadata":{"trusted":true},"cell_type":"code","source":"\ny_final = le.inverse_transform(df_pred['Ensemble_Result'])\ncompiled_test['surface'] = y_final\nsns.countplot(y='surface',data=compiled_test)\ncompiled_test['surface'].to_csv('submission.csv',header=True,encoding='utf-8',index=True)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":">  **Resources:**<br>\n>  XGBoost Explaination:<br> \nhttps://www.youtube.com/watch?v=9CPsYsB4OLI<br>\nhttps://www.youtube.com/watch?v=LsK-xG1cLYA&t=210s<br>\nhttps://www.youtube.com/watch?v=jxuNLH5dXCs<br>\n\n>  ExtraTree Classifier:<br>\nPierre G, Damien E, Louis W (2006) Extremely Randomized Trees. In: Springer Science Journal doi:10.1007/s10994-006-6226-1    \nhttps://www.youtube.com/watch?v=Q1qpG7gwix4<br>\n\n>  RandomSearchCv: <br>\nhttps://www.youtube.com/watch?v=Ah4wsTXghwI&list=UU8ofcOdHNINiPrBA9D59Vaw&index=21 <br>                                                 https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html <br>\n                     \n> Quaternion to Euler: <br>\nhttps://www.euclideanspace.com/maths/geometry/rotations/conversions/quaternionToEuler/ <br>\n\n>   Savgol_Filter: <br>\nhttps://www.mikulskibartosz.name/smoothing-time-series-in-python-using-savitzky-golay-filter/ <br>\n     \n>  Kernels referred: <br>\nhttps://www.kaggle.com/hiralmshah/robot-sensor-eda-fe-and-prediction-improvement\n"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}