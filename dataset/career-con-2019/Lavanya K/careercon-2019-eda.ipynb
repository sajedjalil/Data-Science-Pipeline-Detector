{"cells":[{"metadata":{"_kg_hide-output":true,"trusted":true,"_uuid":"3cf73e9866642ec6f91e750ab0018af8a845dc85"},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom matplotlib import pyplot as plt\nfrom bokeh.plotting import figure, ColumnDataSource\nfrom bokeh.models import HoverTool, LinearColorMapper, ColorBar, FuncTickFormatter, FixedTicker, AdaptiveTicker\nfrom itertools import combinations, product, zip_longest\nfrom scipy.stats import skew, kurtosis, gaussian_kde\nfrom collections import Counter\n\nbar_color = \"cornflowerblue\"\ncolors = [\"#ADD8E6\", \"#9AC7E7\", \"#88B6E9\", \"#76A5EB\", \"#6495ED\", \"#647CD8\", \"#6564C3\", \"#654BAE\", \"#663399\"]\n\ndef scatter_with_hover(df, x, y,\n                       fig=None, cols=None, name=None, marker='x',\n                       fig_width=500, fig_height=500, **kwargs):\n    \"\"\"\n    Plots an interactive scatter plot of `x` vs `y` using bokeh, with automatic\n    tooltips showing columns from `df`.\n    Parameters\n    ----------\n    df : pandas.DataFrame\n        DataFrame containing the data to be plotted\n    x : str\n        Name of the column to use for the x-axis values\n    y : str\n        Name of the column to use for the y-axis values\n    fig : bokeh.plotting.Figure, optional\n        Figure on which to plot (if not given then a new figure will be created)\n    cols : list of str\n        Columns to show in the hover tooltip (default is to show all)\n    name : str\n        Bokeh series name to give to the scattered data\n    marker : str\n        Name of marker to use for scatter plot\n    **kwargs\n        Any further arguments to be passed to fig.scatter\n    Returns\n    -------\n    bokeh.plotting.Figure\n        Figure (the same as given, or the newly created figure)\n    Example\n    -------\n    fig = scatter_with_hover(df, 'A', 'B')\n    show(fig)\n    fig = scatter_with_hover(df, 'A', 'B', cols=['C', 'D', 'E'], marker='x', color='red')\n    show(fig)\n    Author\n    ------\n    Robin Wilson <robin@rtwilson.com>\n    with thanks to Max Albert for original code example\n    \"\"\"\n\n    # If we haven't been given a Figure obj then create it with default\n    # size etc.\n    if fig is None:\n        fig = figure(width=fig_width, height=fig_height, tools=['box_zoom', 'reset', 'save'])\n\n    # We're getting data from the given dataframe\n    source = ColumnDataSource(data=df)\n\n    # We need a name so that we can restrict hover tools to just this\n    # particular 'series' on the plot. You can specify it (in case it\n    # needs to be something specific for other reasons), otherwise\n    # we just use 'main'\n    if name is None:\n        name = 'main'\n\n    # Actually do the scatter plot - the easy bit\n    # (other keyword arguments will be passed to this function)\n    fig.scatter(x=x, y=y, source=source, name=name, marker=marker, **kwargs)\n\n    # Now we create the hover tool, and make sure it is only active with\n    # the series we plotted in the previous line\n    hover = HoverTool(names=[name])\n\n    if cols is None:\n        # Display *all* columns in the tooltips\n        hover.tooltips = [(c, '@' + c) for c in df.columns]\n    else:\n        # Display just the given columns in the tooltips\n        hover.tooltips = [(c, '@' + c) for c in cols]\n\n    # Finally add/enable the tool\n    fig.add_tools(hover)\n\n    return fig\n\n\ndef block_heatmap(df, height=600, width=900):\n    \"\"\"\n    Generates a\n    :param df:\n        The Pandas DataFrame to render in block-heatmap style.\n    :return:\n        A Bokeh block heatmap figure modeled after example code.  The figure has additional properties, df for\n        the plot data, and rect for the plot object.\n    \"\"\"\n    # this colormap blatantly copied from the New York Times.\n    colors = [\"#ADD8E6\", \"#9AC7E7\", \"#88B6E9\", \"#76A5EB\", \"#6495ED\", \"#647CD8\", \"#6564C3\", \"#654BAE\", \"#663399\"]\n    mapper = LinearColorMapper(palette=colors, low=0, high=1)\n    cols = {i: c for (i, c) in enumerate(df.columns)}\n    index = {i: r for (i, r) in enumerate(df.index)}\n    cols_by_rows = product(enumerate(df.columns), enumerate(df.index))\n    data = np.array([[x, y, c, r, df.loc[r, c]] for ((x, c), (y, r)) in cols_by_rows])\n    combination_df = pd.DataFrame(data, columns=[\"gene_id\", \"sample_id\", \"gene\", \"sample\", \"value\"])\n    source = ColumnDataSource(combination_df)\n\n    fig = figure(title=\"Clustered Heatmap\", toolbar_location=\"below\", x_range=(0, len(df.columns)),\n                 y_range=(0, len(df.index)), tools=[\"box_zoom\", \"pan\", \"reset\", \"save\"], name=\"heatmap\",\n                 x_axis_location=\"above\", plot_width=width, plot_height=height, active_drag=\"box_zoom\")\n    fig.rect(x=\"gene_id\", y=\"sample_id\", source=source, width=1, height=1,\n             fill_color={'field': 'value', 'transform': mapper}, line_color=None)\n\n    fig.grid.grid_line_color = None\n    fig.axis.axis_line_color = None\n    fig.axis.major_tick_line_color = None\n    fig.axis.major_label_text_font_size = \"7pt\"\n    fig.axis.major_label_standoff = 0\n    fig.xaxis.major_label_orientation = np.pi / 3\n\n    fig.yaxis.formatter = FuncTickFormatter(code=\"\"\"\n        var labels = %s;\n        return labels[tick] || '';\n    \"\"\" % index)\n\n    fig.xaxis.formatter = FuncTickFormatter(code=\"\"\"\n        var labels = %s;\n        return labels[tick] || '';\n    \"\"\" % cols)\n\n    fig.yaxis.ticker = FixedTicker(ticks=list(index.keys()))\n    fig.xaxis.ticker = AdaptiveTicker(mantissas=list(range(10)), min_interval=1, max_interval=5)\n\n    hover = HoverTool(names=[\"heatmap\"])\n    hover.tooltips = [\n        ('gene', '@gene'),\n        ('sample', '@sample'),\n        ('percentile', '@value%')\n    ]\n    fig.add_tools(hover)\n\n    return fig\n\n\ndef plot_histogram(*data, title=None, columns=3):\n    def plot_data(d, a):\n        if d is None:\n            a.axis(\"off\")\n            return\n        a.hist(d, normed=True, color=bar_color, label=None)\n        de = gaussian_kde(d)\n        edge = 1\n        x = pd.Series(np.linspace(edge * d.min(), d.max() / edge, 100))\n        interpolated_y = de(x)\n        cumulative = x.apply(lambda v: de.integrate_box_1d(d.min(), v)) * interpolated_y.max()\n        a.plot(x, interpolated_y, linestyle='--', color=\"rebeccapurple\", label=\"PDF\")\n        a.plot(x, cumulative, linestyle='--', color=\"dimgray\", label=\"CDF\")\n        a.fill_between(x, interpolated_y, interpolate=True, color=\"rebeccapurple\", alpha=0.35, zorder=10)\n        a.fill_between(x, cumulative, interpolate=True, color=\"dimgray\", alpha=0.125, zorder=15)\n        a.set_xlim([x.min(), x.max()])\n\n        a.yaxis.set_ticks_position('none')\n        a.yaxis.set_ticklabels([])\n\n    if columns > len(data):\n        columns = len(data)\n    rows = int(np.ceil(len(data) / columns))\n\n    fig, axes = plt.subplots(rows, columns)\n\n    if columns == 1:\n        plot_data(data[0], axes)\n        if title:\n            axes.set_title(title)\n        axes.set_ylabel(\"Density\")\n        axes.legend()\n    else:\n        flat_axes = axes.flatten()\n        for d, a in zip_longest(data, flat_axes):\n            plot_data(d, a)\n        if title:\n            for t, a in zip(title, flat_axes):\n                a.set_title(t)\n\n    fig.tight_layout()\n    return fig\n\n\ndef counter_histogram(labels):\n    counts = Counter(labels)\n    fig, ax = plt.subplots()\n    int_keys = [int(k) for k in counts.keys()]\n    ax.bar(int_keys, list(counts.values()), color=bar_color)\n    ax.set_xticks(sorted(int_keys))\n\n    k_range = max(counts.keys()) - min(counts.keys())\n    max_v = max(counts.values())\n\n    def offset(k, v):\n        return (k - k_range * 0.0125, v + max_v * 0.01)\n\n    for (k, v) in counts.items():\n        ax.annotate(str(v), offset(k, v))\n\n\ndef add_dummy(dataframe, column_name):\n    dummies = pd.get_dummies(dataframe[column_name], prefix=\"dummy_\" + column_name)\n    return pd.concat([dataframe, dummies], axis=1)\n\n\ndef filtered_combinations(columns, include_dummies=True, combine_dummies=False):\n    def filter_if_dummies(t):\n        a, b = t\n        a_dummy = a.startswith(\"dummy_\")\n        b_dummy = b.startswith(\"dummy_\")\n        if not include_dummies and (a_dummy or b_dummy):\n            return False\n        if a_dummy and b_dummy:\n            if combine_dummies:\n                a_split = a.split(\"_\")\n                b_split = b.split(\"_\")\n                if not a_split[1] == b_split[1]:\n                    return True\n            return False\n        return True\n\n    return filter(filter_if_dummies, combinations(columns))\n\n\ndef generate_moment_statistics(data):\n    data_skew = skew(data)\n    data_kurtosis = kurtosis(data)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"import networkx as nx\n#import hdbscan\nimport warnings\nfrom collections import Counter\nfrom bokeh.io import output_notebook, show\nfrom bokeh.models import Range1d\nfrom scipy.stats import gaussian_kde, anderson, skew, kurtosis, gamma, entropy\nfrom itertools import zip_longest, count, cycle\n#from helpers import block_heatmap, scatter_with_hover, plot_histogram, counter_histogram, bar_color\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler\nfrom sklearn.decomposition import FastICA, PCA\nfrom sklearn.manifold import MDS, TSNE\nfrom statsmodels.distributions.empirical_distribution import ECDF\nfrom ipywidgets import interact\nfrom matplotlib.colors import ListedColormap\n%matplotlib inline\nwarnings.filterwarnings('ignore')\nfrom matplotlib import pyplot as plt\nplt.rcParams['figure.figsize'] = (12, 5)\noutput_notebook()\nimport os\nimport time\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import StratifiedKFold, KFold, RepeatedKFold\nimport lightgbm as lgb\nsns.set()\n\nprint(\"Files in the input folder:\")\nprint(os.listdir(\"../input\"))\ntrain = pd.read_csv('../input/X_train.csv')\ntest = pd.read_csv('../input/X_test.csv')\ny = pd.read_csv('../input/y_train.csv')\nsub = pd.read_csv('../input/sample_submission.csv')\nprint(\"\\nX_train shape: {}, X_test shape: {}\".format(train.shape, test.shape))\nprint(\"y_train shape: {}, submission shape: {}\".format(y.shape, sub.shape))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9538ed6fb3f52d65e1b472bad9cb54850803ddb5"},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nfrom bokeh.plotting import figure, ColumnDataSource\nfrom bokeh.models import HoverTool, LinearColorMapper, ColorBar, FuncTickFormatter, FixedTicker, AdaptiveTicker\nfrom itertools import combinations, product, zip_longest\nfrom scipy.stats import skew, kurtosis, gaussian_kde\nfrom collections import Counter","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c82eaf482a00ea018f8a5cb8975370b3d10fdbd4"},"cell_type":"code","source":"test.head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4632b5978587cf038a3f8eabb48ce8ff489766d0"},"cell_type":"code","source":"y.head(3)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"58e2926b48686f2e281255d8d421d5d9cb468451"},"cell_type":"markdown","source":"**Generating additional summary statistics**\n\nPandas dataframes has a handy method called describe. Describe gives you a variety of summary statistics for each variable. The summary statistics returned by this method are also a dataframe, so you can amend it easily to include other statistics. We'll do exactly that, adding the variable's coefficient of variation.\n\nThe coefficient of variation is just the ratio of the standard deviation to the mean of the distribution. Distributions with a large coefficient of variation are more likely to signal a change in the underlying data generation process, as opposed to random variation."},{"metadata":{"trusted":true,"_uuid":"011543574f9f5b6aef3d726430d9b04cc5c5112b"},"cell_type":"code","source":"description = train.describe()\ncoef_variation = description.loc[\"std\"] / description.loc[\"mean\"]\ndescription.loc[\"cova\"] = coef_variation\ndescription.sort_values(by=\"cova\", axis=1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"69fb9ce60ad7b775e0d8a289529bcaa3a1cd3a21"},"cell_type":"markdown","source":"***Normalization***\n\nOne important initial step before any real analysis is performed is to normalize the data. This is necessary in order to prevent variables with large values from having an undue influence on results. There are a few different methods of normalizing your data:\n\nThe most common method is to divide a dimension’s values by the largest value it contains. This method is easy to interpret, and it is robust to outliers when the coefficient of variation on a dimension is low.\nAnother very common method is to subtract the dimension’s mean and divide by its standard deviation, a process sometimes called standardizing. This method provides more information about the relative similarity of values when the coefficient of variation on a dimension is high. It performs poorly on dimensions with a low coefficient of variation and large outliers.\nAn underutilized option is to use the cumulative distribution function of the dimension’s empirical distribution to get a percentile ranking. This method works well in most cases and is robust to outliers. Its usefulness degrades somewhat when a dimension’s coefficient of variation gets very small.\nThe normalization method you use depends on your data and the aspects of it that you want to highlight. You can use a different normalization method for each dimension of your data, though I don’t advise you do this unless absolutely necessary. In general my preference is for the empirical distribution method."},{"metadata":{"trusted":true,"_uuid":"1bccd2da44b5e5727ad47232641ff3b0003e6b73"},"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler, StandardScaler\nminmax_normalized_df = pd.DataFrame(MinMaxScaler().fit_transform(train),\n                                    columns=train.columns, index=train.index)\n\nstandardized_df = pd.DataFrame(StandardScaler().fit_transform(train), columns=train.columns,\n                               index=train.index)\n\necdf_normalized_df = train.apply(lambda c: pd.Series(ECDF(c)(c), index=c.index))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ffd375688bc31114c13fc3d3be9e930c9eb4000a"},"cell_type":"markdown","source":"***Exploring Data***\n\nMy strategy for exploring data is to look for things that are out of the ordinary. If you had a data set that was just uncorrelated gaussian random variables, you couldn't learn anything from it, it would literally be white noise. The majority of the information content in most data set is are its variations from normality.\n\nThe distribution of the coefficient of variation tells you about the relative variability of that data. High coefficients of variation for a variable indicate a likely candidate for experimental effect."},{"metadata":{"trusted":true,"_uuid":"1a7d2fdf6013a2f51d2a103dfe315b705973449e"},"cell_type":"code","source":"plot_histogram(description.loc[\"cova\"], title=\"Coefficient of variation\");","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f713761c66dd1a0efc85fbeb2e0214d5e92f2035"},"cell_type":"code","source":"high_cova = description.loc[\"cova\"].where(lambda x: x > 0.30).dropna().sort_values(ascending=False)\nhigh_cova","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ea012816540082b346a4b2de6a57b5c07473b51d"},"cell_type":"markdown","source":"Another good step is to perform normality tests on the variables. Variables that deviate highly from normality are more likely to be informative. Either the Anderson-Darling test or the Shapiro-Wilks test works well for this purpose."},{"metadata":{"trusted":true,"_uuid":"900505a5a8804c39de7cbbc537d43814a4a7d46c"},"cell_type":"code","source":"high_anderson_and_cova = set(high_cova.index) & set(high_cova.index)\nprint(high_anderson_and_cova)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"220c9063676f378f3c0439644c9508f2dfc3d91b","scrolled":false},"cell_type":"code","source":"series_data = [train[series_id] for series_id in high_anderson_and_cova]\n\nplot_histogram(*series_data, title=high_anderson_and_cova);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"32d5cc5d0c581cfea3f3449becf37b34a5985938"},"cell_type":"code","source":"train=train.drop(['row_id'], axis=1)\ntrain.head(3)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"aa91c428e6fe7d8c948f5d133366eac77a70b1a9"},"cell_type":"code","source":"skews = train.apply(lambda x: skew(x))\nskews.name = \"skew\"\nkurts = train.apply(lambda x: kurtosis(x))\nkurts.name = \"kurtosis\"\nseries_ids = pd.Series([i.split(\"_\")[0] for i in kurts.index], index=kurts.index, name=\"series_id\")\nsk_df = pd.concat([skews, kurts, series_ids], axis=1)\nfig = scatter_with_hover(sk_df, \"skew\", \"kurtosis\", name=\"skew_kurt\", cols=[\"series_id\"])\nshow(fig, notebook_handle=True);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"935ffa160c05616674a1f7b3a6ce668e51017970"},"cell_type":"code","source":"high_skew_kurt = sk_df.loc[((skews * kurts).abs() >= 5),[\"skew\", \"kurtosis\"]]\nhigh_skew_kurt","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a9fd9631f9e2ba183f226e5c9f01fd8090727fda"},"cell_type":"markdown","source":"Identifying structure:\nIn addition to looking for deviations, it is important to identify the latent structure in the data. There are a number of techniques for accomplishing this. Clustering is a good way to expose structure in the data. Hierarchical clustering algorithms are good because you don't need to specify a cluster count. Hierarchical clustering algorithms also gives your variables a partial order, which you can take advantage of. My preferred form of clustering is a technique called HDBSCAN. It takes the minimum cluster size as the only parameter, and it produces both discrete and hierarchical clustering."},{"metadata":{"trusted":true,"_uuid":"d44113cd8975f3f8f24cd53ff55e37329a3fccad"},"cell_type":"code","source":"pc = PCA()\nreduced_df = pd.DataFrame(pc.fit_transform(ecdf_normalized_df), index=ecdf_normalized_df)\n\nfig, ax = plt.subplots()\ntotal_var = np.sum(pc.explained_variance_)\ncumulative_var = np.cumsum(pc.explained_variance_)\nax.plot(cumulative_var / total_var);\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"67a36496c158c23daf5ed55760bc6b500e22397a"},"cell_type":"code","source":"dist = gamma(8, 1)\nX = np.linspace(0, 30, 100)\nY = [dist.pdf(x) for x in X]\n\nfig, ax = plt.subplots()\nax.plot(X, Y);","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4144a55699172e88eaafd67758ffced2f99e85b8"},"cell_type":"markdown","source":"***I did some eda but some library not working ***"},{"metadata":{"trusted":true,"_uuid":"e43609a477b2846bf089f3828013f36b003f51ca"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}