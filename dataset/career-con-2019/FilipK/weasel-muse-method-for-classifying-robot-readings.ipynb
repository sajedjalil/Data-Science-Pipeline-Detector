{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Table of Contents\n\n* [Introduction](#Introduction)\n* [1. Multivariate Time Series](#1.-Multivariate-Time-Series)\n  * [1.1. Derivatives](#1.1.-Derivatives)\n* [2. Fourier Transformation](#2.-Fourier-Transformations)\n* [3. ANOVA-F test](#3.-ANOVA-F-test)\n* [4. Binning](#4.-Binning)\n* [5. Words](#5.-Words)\n  * [5.1. Bigrams](#5.1.-Bigrams)\n* [6. Bag of Patterns](#6.-Bag-of-Patterns)\n* [7. Summary](#7.-Summary)\n* [Bibliography](#Bibliography)"},{"metadata":{},"cell_type":"markdown","source":"# Introduction\n\nIn this kernel, I'm trying to present Patrick Schäfer's (Humbold University) approach to multivariate time series analysis. The original author proposes to decompose each time series into Fourier's coefficients, choose those that separate classes best and create special words upon those coefficients. The pipeline looks like the one below [1].\n\n![WEASEL MUSE](https://i.imgur.com/47GaxQF.png \"WEASEL MUSE\")\n\nLet's follow each step to learn more details."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nimport os\nprint(os.listdir(\"../input\"))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 1. Multivariate Time Series\n\nIn this competition, we're given data where each column is a time series representing readings from a robot's sensor. We have 4 orientation readings, 3 angular velocity readings, and 3 acceleration sensors. Using this data we're tasked to guess on which surface the robot has been moving. \n\nIn practice, each unique series of readings is represented as a 2d array, where columns represent sensors and rows represents a sample of fa signal from a given sensor.\n\nFirst, we will do a bit of preprocessing, namely, we'll smooth out signals using rolling average and then obtain derivatives of signals."},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"X_train = pd.read_csv('../input/career-con-2019/X_train.csv')\ny_train = pd.read_csv('../input/career-con-2019/y_train.csv')\n\nX_test = pd.read_csv('../input/career-con-2019/X_test.csv')\ny_sample = pd.read_csv('../input/career-con-2019/sample_submission.csv')\n\nX_train.set_index('series_id', inplace=True)\ny_train.set_index('series_id', inplace=True)\n\nX_test.set_index('series_id', inplace=True)\n\nnew_col_names = ['oX', 'oY', 'oZ', 'oW', 'avX', 'avY', 'avZ', 'laX', 'laY', 'laZ']\ncolumns_to_drop = ['row_id', 'measurement_number']\n\nX_train = X_train.drop(columns_to_drop, axis=1)\ny_train = y_train.drop(['group_id'], axis=1)\n\nX_test = X_test.drop(columns_to_drop, axis=1)\n#y_sample = y_sample.drop(['group_id'], axis=1)\n\nX_train.columns = new_col_names\nX_test.columns = new_col_names\n\nX_train = X_train.groupby('series_id').rolling(10).mean()\nX_test = X_test.groupby('series_id').rolling(10).mean()\n\ndef add_derivatives(df):\n    for col in df.columns:\n        series = df[col].values\n        diff = np.diff(np.array(series))\n        df[col+'_der'] = np.append(0, diff)\n    return df\n\nX_train = add_derivatives(X_train).dropna()\nX_train.index = X_train.index.droplevel()\n\nX_test = add_derivatives(X_test).dropna()\nX_test.index = X_test.index.droplevel()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The most common class here is concrete, which constitutes about 20% of all measurements. Therefore our baseline for prediction is 20% when the most common class is chosen for all predictions."},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.DataFrame(y_train['surface'].value_counts()/y_train.shape[0]*100)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 1.1. Derivatives\n\nThe author advises adding derivatives to our dataset, as they might improve our model.\n\nThe column codes are:\n* X, Y, Z, W - stand for direction\n* 'o' - stands for orientation\n* 'av' - stands for angular velocity\n* 'la' - stands for linear acceleration\n* '_der' - stands for derivative\n\nThis is how the pre-processed data looks like. So far we've used rolling mean to smooth out the measurements and calculated derivatives by simply substracting concurrent data points."},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2. Fourier Transformation\n\nIn this kernel, we'll be using sklearn.pipelines for each step in data processing, as it seems to be a consistent way of transforming data.\n\nBefore we'ee go to transforming the data using FFT, we'll z-score standardize our time series first. TF of z-score standardized series follows a Gaussian distribution, which will help us in the next step.\n\nNote we're only using first 100 samples in this example for simplicity."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.base import BaseEstimator, TransformerMixin","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy import stats\n\nclass ZScoreScaler(BaseException, TransformerMixin):\n    \n    def __init__(self):\n        pass\n    \n    def fit(self, X, y=None):\n        return self\n    \n    def transform(self, X):\n        arr = X.groupby('series_id').apply(stats.zscore).values\n        arr = np.concatenate(arr, axis=0)\n        return pd.DataFrame(arr, columns=X.columns, index=X.index)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"zscore = ZScoreScaler().fit_transform(X_train.loc[:100])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"And that's how our z-score transformed series look like."},{"metadata":{"trusted":true},"cell_type":"code","source":"zscore.loc[20, 'oX':'avZ'].reset_index().drop(columns=['series_id']).plot()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"zscore.loc[20, 'oX_der':'avZ_der'].reset_index().drop(columns=['series_id']).plot()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The goal of using FT and choosing the best coefficients is to separate signal from the noise. In the above example, we can see a random sample of readings from sensors (top chart), and their derivatives (bottom chart).\n\nNow it's time to take Fourier transforms of our series **using rolling window**. We transform each series (including derivatives) by applying rolling window of a given length (in our example it's 10 samples) and applying FFT on each window."},{"metadata":{"trusted":true},"cell_type":"code","source":"def rolling_window_2d(arr, size=2):\n    shape = arr.shape\n    strides = arr.strides\n    arr = np.lib.stride_tricks.as_strided(arr,\n                                         shape=(shape[1], shape[0]+1-size, size),\n                                         strides=(strides[1],strides[0],strides[0]))\n    return arr\n\nclass RollingWindowFourier(BaseEstimator, TransformerMixin):\n\n    def __init__(self, window_length):\n        self.window_length = window_length\n        \n    def fit(self, X, y=None):\n        return self\n        \n    def transform(self, X):\n        index = []\n        arr_list = []\n        for i in X.index.unique():\n            new_one = rolling_window_2d(X.loc[i].values, self.window_length)\n            f_transformed = np.fft.fft(new_one, 12)\n            concated = np.concatenate((f_transformed.real, f_transformed.imag), axis=2)\n            two_dims = concated.transpose(1,0,2).reshape(concated.shape[1],-1)\n            arr_list.append(two_dims)\n            index += [i for ind in range(new_one.shape[1])]\n        df = pd.DataFrame(np.concatenate(arr_list, axis=0), index=index)\n        del arr_list\n        return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fourier = RollingWindowFourier(10).fit_transform(zscore)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fourier.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fourier.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Note we got rid of column names. Now we're left with numbers only.\n\nEach column represents now i-th real or imaginary coefficient of j-th sensor reading, i.e.:\n\n>First 24 columns represent Fourier coefficients of 'oX' sensor readings. Now first 12 columns of those 24 for columns represent real, and the rest imaginary coefficients. So for our second reading, 'oY' we'll be looking at columns 24-47, and so on, till column no. 479, because we had 20 readings times 24 coefficients per reading."},{"metadata":{},"cell_type":"markdown","source":"# 3. ANOVA-F test\n\nNow it is time to choose the best Fourier coefficients for each sensor readings. We're using ANOVA-F test to check which coefficients best separate classes.\n\nOther, much simpler (and less reliable) solution, is to choose the first few coefficients as they refer to low frequency, and therefore should separate signal from the noise."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_selection import SelectKBest, f_classif\n\nclass ANOVA_ColumnSelector(BaseEstimator, TransformerMixin):\n\n    def __init__(self, k_best):\n        self.k = k_best\n        \n    def fit(self, X, y):\n        y = X.merge(y, left_on=X.index, right_on=y.index)['surface']\n        col_list = []\n        for sensor in range(20):\n            col_indexes = np.linspace(sensor*24, sensor*24+23, 24).astype('int16')\n            skb = SelectKBest(f_classif, k=self.k).fit(X[col_indexes], y)\n            col_list += list(np.argsort(skb.pvalues_)[:self.k] + sensor*24)\n        self.columns = col_list\n        return self\n    \n    def transform(self, X):\n        df = X[self.columns]\n        del X\n        return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"anova = ANOVA_ColumnSelector(4).fit_transform(fourier, y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The author advises choosing 3-5 best coefficients. We'll go with 4, which is a conservative number - we have neither complex nor oversimplified model."},{"metadata":{"trusted":true},"cell_type":"code","source":"anova.merge(y_train, left_on=anova.index, right_on=y_train.index).boxplot(2, by='surface')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"anova.merge(y_train, left_on=anova.index, right_on=y_train.index).boxplot(178, by='surface')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"anova.merge(y_train, left_on=anova.index, right_on=y_train.index).boxplot(347, by='surface')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Although we see some difference in means between different classes, their variance is quite high. This may cause overlapping of classes, which might result in weak separation.****"},{"metadata":{},"cell_type":"markdown","source":"# 4. Binning\n\nIn order to create words, we need to create bins for each Fourier coefficient [2]. We'll do this by using Entropy Information Gain. Luckily this algorithm is implemented in Decision Trees. The Gini impurity is an entropy-based method, that makes a decision on how to make a split in the decision tree. We'll use those split values to create bins."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import tree\n\nclass Binner(BaseEstimator, TransformerMixin):\n    \n    def __init__(self):\n        pass\n    \n    def fit(self, X, y):\n        y = X.merge(y, left_on=X.index, right_on=y.index)['surface']\n        column_limits = []\n        for col in X.columns:\n            clf = tree.DecisionTreeClassifier(max_depth=2, max_leaf_nodes=4).fit(X[col].values.reshape(-1,1), y)\n            threshold = clf.tree_.threshold[:3]\n            limits = np.sort(np.insert(threshold,0,[np.NINF, np.inf]))\n            column_limits.append(limits)\n        self.column_limits = column_limits\n        return self\n    \n    def transform(self, X):\n        for idx, col in enumerate(X.columns):\n            X.loc[:,col] = pd.cut(X[col], bins=self.column_limits[idx], labels=[1,2,3,4])\n        return X.astype('int16')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"binned = Binner().fit_transform(anova, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"binned.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now each Fourier coefficient has is own label (bin number) based on Decision Tree splits."},{"metadata":{},"cell_type":"markdown","source":"# 5. Words\n\nWe'll combine those bin numbers to 'words'. Those words basically consist of a bin number of certain Fourier coefficient for each sensor reading. We should be left with 20 columns because each word will symbolize Fourier coefficients for certain window in a reading."},{"metadata":{"trusted":true},"cell_type":"code","source":"class CreateWords(BaseEstimator, TransformerMixin):\n    \n    def __init__(self, window_length):\n        self.window_length = window_length\n    \n    def fit(self, X, y=None):\n        return self\n    \n    def transform(self, X):\n        cols = X.columns\n        ind = X.index\n        col_names = ['oX', 'oY', 'oZ', 'oW', 'avX', 'avY', 'avZ', 'laX', 'laY', 'laZ',\n                     'oX_der', 'oY_der', 'oZ_der', 'oW_der', 'avX_der', 'avY_der', 'avZ_der',\n                     'laX_der', 'laY_der', 'laZ_der']\n        df_dict = {}\n        for i in range(20):\n            x = X[cols[i*4:i*4+4]].apply(lambda x: int(''.join(map(str,x))), axis=1)\n            df_dict[str(self.window_length)+col_names[i]] = x\n        df = pd.DataFrame(df_dict, index=ind)\n        del x, df_dict, X\n        return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"words = CreateWords(10).fit_transform(binned)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"words.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We renamed columns by appending used window length to the original sensor name. In this manner, we will be able to differentiate between 'words' of different window lengths and of different sensors later on."},{"metadata":{},"cell_type":"markdown","source":"# 5.1. Bigrams\n\nAnother step proposed by the author is to get bigrams from the above words. This way we'll be able to find which signals tend to occur in pairs."},{"metadata":{"trusted":true},"cell_type":"code","source":"class GetBigrams(BaseEstimator, TransformerMixin):\n    \n    def __init__(self):\n        pass\n    \n    def fit(self, X, y=None):\n        return self\n    \n    def transform(self, X):\n        arr_list = []\n        index = []\n        for i in X.index.unique():\n            unigrams2d = rolling_window_2d(X.loc[i].values, 2)\n            bigrams = np.apply_along_axis(lambda x: int(''.join(map(str,x))), 2, unigrams2d).T\n            stacked = np.vstack((X.loc[i], bigrams))\n            arr_list.append(stacked)\n            index += [i for ind in range(stacked.shape[0])]\n        df = pd.DataFrame(np.concatenate(arr_list, axis=0), columns=X.columns, index=index)\n        del X, arr_list\n        return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bigrams = GetBigrams().fit_transform(words)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bigrams.tail(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 6. Bag of Patterns\n\nNow that we have both words and bigrams, we'll count them for each unique series ID. We will hope to see certain words occuring more frequently for certain classes. I could speculate to see, for example, high frequency terms in medium window and long window lenghts for orientation sensors on concrete, as the concrete tends to be bumpy, which may cause vibration of the robot."},{"metadata":{"trusted":true},"cell_type":"code","source":"class TextTransformer(BaseEstimator, TransformerMixin):\n    \n    def __init__(self):\n        pass\n    \n    def fit(self, X, y=None):\n        return self\n    \n    def transform(self, X):\n        for col in X.columns:\n            X[col] = col+X[col].astype(str)\n        df = X.groupby(X.index).apply(lambda x: ' '.join(x.values.flatten()))\n        del X\n        return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"text = TextTransformer().fit_transform(bigrams)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\n\nclass Dummify(BaseEstimator, TransformerMixin):\n    \n    def __init__(self):\n        pass\n    \n    def fit(self, X, y=None):\n        self.CV = CountVectorizer().fit(X)\n        self.columns = self.CV.get_feature_names()\n        return self\n    \n    def transform(self, X):\n        counts = self.CV.transform(X)\n        index = X.index\n        del X\n        return pd.DataFrame(counts.toarray(), index=index, columns=self.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dummies = Dummify().fit_transform(text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dummies.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 7. Summary\n\nI used this method for prediction using varying window lengths - from 5 to 68 samples. For each window length, I've chosen 350 best features and then combined them together. After choosing multiple models (logreg, SVM, GBT, RFC) and using Grid Search I haven't observed very high accuracy. Most of them score at around 40-45%, which is still better than our baseline 20%.\n\nI suspect that this data is simply not suitable for this model. We can read in the paper [1], that some data sets containing sensor readings did not perform well. It's probably a bad separation between classes of Fourier coefficients (see ANOVA-F test section) that causes such low performance.\n\nNevertheless, it's an interesting approach to signal analysis and certainly an alternative for ANN."},{"metadata":{},"cell_type":"markdown","source":"# Bibliography\n* [1] Multivariate Time Series Classification with WEASEL+MUSE - Patrick Schäfer, Ulf Leser\n* [2] SFA: A Symbolic Fourier Approximation and Index for Similarity Search in High Dimensional Datasets - Patrick Schäfer\n* [3] Fast and Accurate Time Series Classification with WEASEL - Patrick Schäfer, Ulf LeserPatrick Schäfer, Ulf Leser"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.pipeline import make_pipeline, make_union\nfrom sklearn.feature_selection import SelectKBest, chi2\n\nunion = make_union(\n    make_pipeline(\n        ZScoreScaler(),\n        RollingWindowFourier(5),\n        ANOVA_ColumnSelector(4),\n        Binner(),\n        CreateWords(5),\n        GetBigrams(),\n        TextTransformer(),\n        Dummify(),\n        SelectKBest(chi2, k=350)\n    ),\n    make_pipeline(\n        ZScoreScaler(),\n        RollingWindowFourier(10),\n        ANOVA_ColumnSelector(4),\n        Binner(),\n        CreateWords(10),\n        GetBigrams(),\n        TextTransformer(),\n        Dummify(),\n        SelectKBest(chi2, k=350)\n    ),\n    make_pipeline(\n        ZScoreScaler(),\n        RollingWindowFourier(14),\n        ANOVA_ColumnSelector(4),\n        Binner(),\n        CreateWords(14),\n        GetBigrams(),\n        TextTransformer(),\n        Dummify(),\n        SelectKBest(chi2, k=350)\n    ),\n    make_pipeline(\n        ZScoreScaler(),\n        RollingWindowFourier(23),\n        ANOVA_ColumnSelector(4),\n        Binner(),\n        CreateWords(23),\n        GetBigrams(),\n        TextTransformer(),\n        Dummify(),\n        SelectKBest(chi2, k=350)\n    ),\n    make_pipeline(\n        ZScoreScaler(),\n        RollingWindowFourier(32),\n        ANOVA_ColumnSelector(4),\n        Binner(),\n        CreateWords(32),\n        GetBigrams(),\n        TextTransformer(),\n        Dummify(),\n        SelectKBest(chi2, k=350)\n    ),\n    make_pipeline(\n        ZScoreScaler(),\n        RollingWindowFourier(41),\n        ANOVA_ColumnSelector(4),\n        Binner(),\n        CreateWords(41),\n        GetBigrams(),\n        TextTransformer(),\n        Dummify(),\n        SelectKBest(chi2, k=350)\n    ),\n    make_pipeline(\n        ZScoreScaler(),\n        RollingWindowFourier(50),\n        ANOVA_ColumnSelector(4),\n        Binner(),\n        CreateWords(50),\n        GetBigrams(),\n        TextTransformer(),\n        Dummify(),\n        SelectKBest(chi2, k=350)\n    ),\n    make_pipeline(\n        ZScoreScaler(),\n        RollingWindowFourier(59),\n        ANOVA_ColumnSelector(4),\n        Binner(),\n        CreateWords(59),\n        GetBigrams(),\n        TextTransformer(),\n        Dummify(),\n        SelectKBest(chi2, k=350)\n    ),\n    make_pipeline(\n        ZScoreScaler(),\n        RollingWindowFourier(68),\n        ANOVA_ColumnSelector(4),\n        Binner(),\n        CreateWords(68),\n        GetBigrams(),\n        TextTransformer(),\n        Dummify(),\n        SelectKBest(chi2, k=350)\n    )    \n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"union.fit(X_train, y_train)\nX_train_transformed = union.transform(X_train)\nX_test_transformed = union.transform(X_test)\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"X_train_transformed = pd.DataFrame(X_train_transformed)\nX_test_transformed = pd.DataFrame(X_test_transformed)\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"X_train_transformed.to_csv('train_transformed.csv',index=False)\nX_test_transformed.to_csv('test_transformed.csv',index=False)\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"train = pd.read_csv('../input/weaselmuse-robots/train_transformed.csv')\ntest = pd.read_csv('../input/weaselmuse-robots/test_transformed.csv')\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import accuracy_score, make_scorer\n\n\nscorer = make_scorer(accuracy_score)\n\nGBC_params = dict(n_estimators=(100,250,500),\n                  learning_rate=(0.1,1,10),\n                  min_samples_split=(2,3,4))\n\nRFC_params = dict(n_estimators=(10,50,100,200),\n                 oob_score=(False, True),\n                 class_weight=['balanced'])\n\nGBC_grid = GridSearchCV(GradientBoostingClassifier(random_state=42), GBC_params, cv=5, scoring=scorer)\nRFC_grid = GridSearchCV(RandomForestClassifier(random_state=42), RFC_params, cv=5, scoring=scorer)\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#GBC = GradientBoostingClassifier(n_estimators=200, learning_rate=0.1, min_samples_split=3).fit(train, y_train.values.ravel())","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}