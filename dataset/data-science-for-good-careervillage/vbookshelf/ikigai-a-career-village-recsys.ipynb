{"cells":[{"metadata":{"_uuid":"2c3aeb544a8988ecc0371a23249eba8894304153"},"cell_type":"markdown","source":"## Ikigai - A Career Village RecSys\n\nby Marsh [ @vbookshelf ]<br>\n9 April 2019"},{"metadata":{},"cell_type":"markdown","source":"<img src=\"http://bee.test.woza.work/assets/student.jpg\" width=\"500\"></img>"},{"metadata":{"_uuid":"0cb9683fa1ce0d8899abcbe793f63afdd963a8d2"},"cell_type":"markdown","source":"## Contents"},{"metadata":{"_uuid":"0c9cbce17a9dd4be39192ebba5724dd3cd0a08a3"},"cell_type":"markdown","source":"<a href='#Introduction'>1. Introduction</a><br>\n<a href='#Prepare_the_Data'>2. Prepare the Data</a><br>\n<a href='#Ask_a_Question'>3. Ask a Question</a><br>\n<a href='#Model_1'>4. Model 1 -  Tags and Profiles</a><br>\n<a href='#Model_2'>5. Model 2 - Profiles and Answers</a><br>\n<a href='#Model_3'>6. Model 3 - TruncatedSVD</a><br>\n<a href='#Model_4'>7. Model 4 - GloVe Embeddings</a><br>\n<a href='#Select_professionals'>8. Select professionals who are most likely to answer the question</a><br>\n<a href='#Final_Output'>9. Final Output - The chosen ones</a><br>\n<a href='#Testing'>10. Testing and Results</a><br>\n<a href='#Things'>11. Things to keep in mind</a><br>\n<a href='#Ideas'>12. Ideas for sharpening this system</a><br>\n\n<a href='#Citations'>Citations</a><br>\n<a href='#Reference_Kernels'>Reference Kernels</a><br>\n<a href='#Helpful_Resources'>Helpful Resources</a><br>\n<a href='#Conclusion'>Conclusion</a><br>\n"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Set a seed value\nfrom numpy.random import seed\nseed(101)\n\nimport pandas as pd\nimport numpy as np\nimport os\n\nimport pickle\nfrom bs4 import BeautifulSoup\nimport re\nfrom nltk.corpus import stopwords\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import linear_kernel, cosine_similarity\n\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\n\n# Don't Show Warning Messages\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Read the data\n\ndf_questions = \\\npd.read_csv('../input/data-science-for-good-careervillage/questions.csv')\ndf_answers = \\\npd.read_csv('../input/data-science-for-good-careervillage/answers.csv')\ndf_professionals = \\\npd.read_csv('../input/data-science-for-good-careervillage/professionals.csv')\n\ndf_comments = \\\npd.read_csv('../input/data-science-for-good-careervillage/comments.csv')\ndf_tags = \\\npd.read_csv('../input/data-science-for-good-careervillage/tags.csv')\ndf_tag_users = \\\npd.read_csv('../input/data-science-for-good-careervillage/tag_users.csv')\n\n#print(df_questions.shape)\n#print(df_answers.shape)\n#print(df_professionals.shape)\n#print(df_comments.shape)\n#print(df_tags.shape)\n#print(df_tag_users.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2fef69dbf37305f7aef197c0f1ae75b117452c4e"},"cell_type":"markdown","source":"| <a id='Introduction'></a>"},{"metadata":{"_uuid":"c3c21c9ed8afdb1641ac438046fa689e53ffd876"},"cell_type":"markdown","source":"## 1. Introduction"},{"metadata":{"_uuid":"02be8eb9441f4eaf33945eca3e1c982b7b0b5c09"},"cell_type":"markdown","source":"If you're like me then alot of your emails are unopened. It's because you know from reading the title or just from the sender's name that you've got no interest in the content. Who cares if a city that you visited 6 months ago now has \"some great deals on hotel rooms\". Our brains are becoming efficient content filters. Anything that's not relevant will be ignored.\n\nThat's why it's important for CareerVillage to have a good recommender system (RecSys). Professionals need to feel that the questions sent to them are relevant. Creating a personalized experience will make them feel valued. This will lead to more questions being answered and faster answers.\n\nThe objective of this competition is to develop a method to recommend relevant questions to the professionals who are most likely to answer them.\n\nThis solution follows two steps:<br>\n\n*Step 1*: Develop a method to recommend relevant questions to professionals.<br>\n*Step 2*: Identify those professionals who are most likely to answer a relevant question.\n\nIt's not practical to measure the quality of this solution using an evaluation metric. To overcome this we will assess the results qualitatively using a simple run and read approach. \n\nIn step 1 you'll ask a question. Then you'll look at the professionals that each model recommendeds and ask: \"Based on this person's  profile or past answer, will this recommended professional be **capable** of answering this question?\"<br>\nThis is an intuitive way of assessing the relevance of your question to a particular professional.\n\nIn step two, to assess whether or not a professional will respond to a relevant question, this system will look at four indicators:\n\n1. Is this professional a new member?\n2. Has this professional answered a past question from this student?\n3. Did this professional answer a question recently?\n4. Did this professional make a comment recently?\n\nUsing this filter, this system will generate a final list of professionals that are likely to respond to your question.\n\nA central feature of this system is that it compares your question to each professional's background info or to all answers in the dataset. If the question is similar to a particular professional's background or to a past answer that he or she gave - then it's likely that that professional is able to answer your question. In other words, your question is relevant to that professional.\n\n\n**RecSys Architecture**\n\nThis recommender system is made up of the four models and a filter:<br>\n\n( A professional's profile is made up of the industry they work in and their title. )\n\n- Model 1 uses tags followed, professional profiles and tfidf (Term Frequency Inverse Document Frequency)\n- Model 2 uses professional profiles, past answers and tfidf\n- Model 3 uses professional profiles, past answers, tfidf and Truncated SVD (Singular Value Decomposition)\n- Model 4 uses professional profiles, past answers and GloVe pre-trained word embeddings  (Global Vectors for Word Representation)\n\nWhy do we need four models? It's because not all models perform equally well on all questions. This is a small dataset and different careers are not equally represented. For example theres a lot of professionals with a computer science background but few firefighters. Professions are well represented. Trades are not. Also, not all questions are about specific careers, some are about life. Therefore, having a diversity of models that tackle the problem in different ways is the most prudent approach.\n\n**Interactive Notebook**\n\nThis notebook is set up in such a way that you'll be able to select a question from the dataset, run the entire notebook and then see a printout of results for each model, and for the filter. There's also an option to type in your own question as you would on the CareerVillage website.\n\n**Testing and Results**\n\nDoes this recommender system work? Yes it does. \n\nTo demonstrate this fact I've tested it on nine questions that are representative of the data. The number of recommendations produced and the number of false positives in those recommendations is tabulated in section 10.\n\n\nMy goal here is to build a working prototype. Let's start by preparing the data."},{"metadata":{"_uuid":"322f7c1bf7028fdb8440bd8c154f7ebe3bffce54"},"cell_type":"markdown","source":"| <a id='Prepare_the_Data'></a>"},{"metadata":{"_uuid":"ecf6a2989c743f02fb4570856fbbd78dee821e96"},"cell_type":"markdown","source":"## 2. Prepare the Data"},{"metadata":{"_uuid":"897959603025c9aabc17837a55d6ba2d26b34691","trusted":true},"cell_type":"code","source":"# Check what folders are available\n\nos.listdir('../input')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I've done the data preparation in a seperate kernel. This is the link:<br>\nhttps://www.kaggle.com/vbookshelf/data-prep-for-careervillage-recsys\n\n\nHere we'll simply load the prepared data from that kernel's output.\n\ndf_qa_prof.pickle is the pre-processed dataframe that we will use in all three models. This is a merged dataframe that includes questions, answers and professionals. Professionals who didn't answer any questions are not included. \n\nThere's a new column called quest_text where each cell contains both the question title and the question body. There's also a new column called answers_text where each cell contains the combined content of the following columns: professionals_headline, professionals_industry and answers_body."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Load the pickled dataframe\n\npath_1 = '../input/data-prep-for-career-village-recsys/df_qa_prof.pickle'\n\ndf_qa_prof = pickle.load(open(path_1,'rb'))\n\n# check the shape\ndf_qa_prof.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e294fb2f0bd4dff261f84d86a5701bf8fe2922d0","trusted":true},"cell_type":"code","source":"df_qa_prof.head(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Define a function to clean the text\n\ndef process_text(x):\n    \n    # remove the hash sign\n    x = x.replace(\"#\", \"\")\n    \n    # remove the dash sign with a space\n    #x = x.replace(\"-\", \" \")\n    \n    # Remove HTML\n    x = BeautifulSoup(x).get_text()\n    \n    # convert words to lower case\n    x = x.lower()\n    \n    # remove the word question\n    x = x.replace(\"question\", \"\")\n    \n    # remove the word career\n    x = x.replace(\"career\", \"\")\n    \n    # remove the word study\n    x = x.replace(\"study\", \"\")\n    \n    # remove the word student\n    x = x.replace(\"student\", \"\")\n    \n    # remove the word school\n    x = x.replace(\"school\", \"\")\n    \n    # Remove non-letters\n    x = re.sub(\"[^a-zA-Z]\",\" \", x)\n    \n    # Remove stop words\n    # Convert words to lower case and split them\n    words = x.split()\n    stops = stopwords.words(\"english\")\n    x_list = [w for w in words if not w in stops]\n    # convert the list to a string\n    x = ' '.join(x_list)\n    \n    return x","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"eb88ff2b342ff99880410ef097407b98fb1e3ec4"},"cell_type":"markdown","source":"<hr>\n| <a id='Ask_a_Question'></a>"},{"metadata":{"_uuid":"6abbb7b49dcbc41075a4fd841b8c5f20c228026b"},"cell_type":"markdown","source":"## 3. Ask a Question\n\nPlease select any question in the dataset or type your own question. Then run all cells in this kernel.\n"},{"metadata":{"_uuid":"eec04444c4e4337c2d3711760ca3ad459b9ccee4"},"cell_type":"markdown","source":" ### ~ Option 1: Choose a question from the CareerVillage dataset ~\nPlease set QUESTION_INDEX equal to any row index between 0 to 51000.<br>\n<br>\nFor your first try I suggest using QUESTION_INDEX = 777. It's a computer science related question that nicely demonstrates the performance of each of the 4 models and the filter.\n\nIf you'd like to choose Option 2 then please set QUESTION_INDEX = None"},{"metadata":{"_uuid":"78eb71bf98a1d1509aa5c9ad00feb6c81408d95d","trusted":true},"cell_type":"code","source":"###################################\n\nQUESTION_INDEX = 1710\n\n###################################","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c767c4eecf7fc5b5b5475e1269805adf20a314f9"},"cell_type":"markdown","source":"### ~ Option 2: Ask a question as you would on the CareerVillage site ~\nPlease type your text within inverted commas - \" i am a string \""},{"metadata":{"_uuid":"db57aa3c5257e6c2cafb8a57ecb031768d903fed","trusted":true},"cell_type":"code","source":"# =========================================== #\n# Please check that QUESTION_INDEX = None in the above cell before entering\n# your own question.\n\nmy_question_title = \"How do I become a data scientist?\"\n\nmy_question_body = \"I want to be a data scientist. What subjects should I study? #data-science\"\n\n# =========================================== #","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"==> After selecting one of the above options please Run all cells in this kernel. <=="},{"metadata":{"_uuid":"346c38d57da0f53856c3a0f2d188d7c9151e90a7"},"cell_type":"markdown","source":"### ~ This is your Question ~"},{"metadata":{"_kg_hide-input":true,"_uuid":"b495770ae66d570789ee798961dfd69096364f55","trusted":true},"cell_type":"code","source":"# Code to process the question\n\n# if Option 1 is chosen\nif QUESTION_INDEX != None:\n    \n    QUESTION_INDEX = int(QUESTION_INDEX)\n    \n    student_id = df_qa_prof.loc[QUESTION_INDEX, 'questions_author_id']\n    # Get the question info from the dataset.\n    # The text has already been cleaned above.\n    question_id = df_qa_prof.loc[QUESTION_INDEX, 'questions_id']\n    question_title = df_qa_prof.loc[QUESTION_INDEX, 'questions_title']\n    question_body = df_qa_prof.loc[QUESTION_INDEX, 'questions_body']\n    # question_text is clean text that is used in the models\n    question_text = df_qa_prof.loc[QUESTION_INDEX, 'quest_text'] \n\n# if Option 2 is chosen\nelse:\n    student_id = 33333333 # dummy id that's needed for the final selection code\n    # get the input question\n    question_id = 'My Question'\n    question_title = my_question_title\n    question_body = my_question_body\n    # Clean the text using the process_text() function.\n    # question_text is clean text that is used in the models\n    question_text = process_text(question_title) + ' ' + process_text(question_body)\n    \n\n# Print the question\nprint('Question id: ', question_id)\nprint('Question Title: ', question_title)\nprint('\\n')\nprint('Question Body:\\n ', question_body)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c44ff7e96b2727407cc1a8e507f7b0b558eb46be","trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0df314401f4b9c4eac5cbe3922a745dff124fc48","trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d10abdb7750f7926da2be40400ca1a99b2f781e6"},"cell_type":"markdown","source":"<hr>\n| <a id='Model_1'></a>"},{"metadata":{"_uuid":"dc762648f2bcb1e230b91d1817c6aa134b525620"},"cell_type":"markdown","source":"## 4. Model 1 -  Tags, Profiles, Tfidf and Cosine Similarity\n"},{"metadata":{"_uuid":"76c303d74fc63aa6c52393f6283720bfd2b6b15f"},"cell_type":"markdown","source":"This model considers every professional in the dataset irrespective of whether or not they have answered a past question. \n\n**How does this model  work?**\n\nIt compares your question to each professional's background info. Background info is made up of a professional's title, the industry they work in and the tags they follow. The hash symbols are removed from the tags - the model sees the tags as words. The similarity is measured by comparing the vector encoding of the question to the vector encoding of each professional's background info. The encodings are created using Tfidf (Term Frequency Inverse Document Frequency). The vectors are compared using cosine similarity.\n\n> **The idea is that that if a question is similar to a professional's background then there's a good chance that he or she will be able to answer the question.**\n\n**How can we assess how well the model is working?**\n\nWe will \"run and read\". We will run the model and then look at the results to see if they make sense. The model will print out the background info of each professional it has selected. By reading this and comparing it to the question we'll be able to tell whether the model is making reasonable choices.\n\n\n"},{"metadata":{"_uuid":"aab77e650e5628ed3eac24c500b7a8f0c5b4bd68"},"cell_type":"markdown","source":"## 4.1. Prepare the data"},{"metadata":{"_uuid":"ef083f4d6721da3e17319c5eb01813bc26f87e60","trusted":true},"cell_type":"code","source":"# load df_professionals\npath_2 = '../input/data-prep-for-career-village-recsys/df_professionals.pickle'\ndf_professionals = pickle.load(open(path_2,'rb'))\n\n# replace all missing values with nothing\ndf_professionals = df_professionals.fillna('')\n\n# Create a dictionary of tag id's and tag names\nkeys = list(df_tags['tags_tag_id'])\nvalues = list(df_tags['tags_tag_name'])\ntags_dict = dict(zip(keys, values))\n\n# Change the tag id numbers to tag names that we can read\ndf_tag_users['tag_name'] = df_tag_users['tag_users_tag_id'].map(tags_dict)\n\ndf_tag_users.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fdaaa9035fac70cfc401edeec0b13d7c034dfd69"},"cell_type":"markdown","source":"Because anyone is able to follow a tag there could be mixture of students and professionals in df_tag_users. We need to filter out the professionals."},{"metadata":{"_uuid":"fe6e87ad21b681fc270479d3118afeea5edbf8b3","trusted":true},"cell_type":"code","source":"# get a list of professionals\nprof_list = list(df_professionals['professionals_id'])\n# filter out the professionals from df_tag_users\ndf_prof_tag_users = df_tag_users[df_tag_users['tag_users_user_id'].isin(prof_list)]\n\ndf_prof_tag_users.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"03e38c0339d3e71f5758e50db501a7ecf4d15801"},"cell_type":"markdown","source":"Now that we've filtered out all the professionals, let's see which tags each professional follows. The hash sign # has been removed from the tags."},{"metadata":{"_uuid":"345e9fd3cff98fa5eeb6987bfcb1e04a21e5e608","trusted":true},"cell_type":"code","source":"# drop the tag_users_tag_id column\ndf_prof_tag_users = df_prof_tag_users.drop('tag_users_tag_id', axis=1)\n\n# replace missing values with nothing - just be be safe\ndf_prof_tag_users =df_prof_tag_users.fillna('')\n\n# add a space to the end of each tag name\ndef add_space(x):\n    x = x + ' '\n    \n    return x\n\ndf_prof_tag_users['tag_name'] = df_prof_tag_users['tag_name'].apply(add_space)\n\n# groupby tag_users_user_id and sum() the tags\ndf_prof_tag_users = df_prof_tag_users.groupby('tag_users_user_id').sum()\n\n# reset the index\ndf_prof_tag_users = df_prof_tag_users.reset_index()\n\n# check how many professionals follow tags\nnum_followers = len(df_prof_tag_users['tag_users_user_id'])\n\n# Are there professionals who don't follow any tags?\n\nnum_profs = df_professionals['professionals_id'].nunique()\nnum_tag_followers = df_prof_tag_users['tag_users_user_id'].nunique()\n\nnum_not_followers = num_profs - num_tag_followers\n\nprint(num_followers, 'professionals follow tags.')\nprint(num_not_followers, 'professionals do not follow tags.')\n\ndf_prof_tag_users.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0cb135954b4565e8e367069dda87cb130d931d18"},"cell_type":"markdown","source":"2649 professionals don't follow tags."},{"metadata":{"_uuid":"cda9e7ce4e0f2ac47db7276e818cdd458debea49"},"cell_type":"markdown","source":"### Add a new column to the df_professionals dataframe that shows the tags that each professional follows."},{"metadata":{"_uuid":"de7f477b0d9d012e41fbda30557ad6d7a7fe5fe4"},"cell_type":"markdown","source":"Because there are 2649 professionals that don't follow any tags, if we try to merge df_professionals and df_prof_tag_users then those who don't follow any tags will be automatically dropped. We must keep this in mind when merging dataframes. We will do a left join. This will include the rows common to both dataframes as well as all elements from the left dataframe. Please refer to the tutorial video referenced in the 'Helpful Resources' section if you'd like to learn more about merging dataframes."},{"metadata":{"_uuid":"154bfdedb6fb528499549518c082b4a55c8110c5","trusted":true},"cell_type":"code","source":"# https://www.youtube.com/watch?v=h4hOPGo4UVU\n\n# Change column name in df_prof_tag_users. \n# For the merge to work the column called professionals_id needs to be in\n# both dataframes.\nnew_names = ['professionals_id', 'tags_followed']\ndf_prof_tag_users.columns = new_names\n\n# perform the left merge\ndf_profs = pd.merge(df_professionals,df_prof_tag_users, \n                   on='professionals_id', how='left')\n\n# replace missing values with nothing\ndf_profs = df_profs.fillna('')\n\nprint('We now have a combined dataframe containing the tag info and profile info for all professionals.')\n\ndf_profs.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b2acdd0b80b0643667c09920c41762bc2d3b9418"},"cell_type":"markdown","source":"### Create a new column that contains the background info of each professional. Then clean the text in this new column.\n"},{"metadata":{"_uuid":"914ccff0c4a845fdb2ed15c5b02abcc536f6c3ea","trusted":true},"cell_type":"code","source":"# Create the new column by summing the strings from each seperate column.\ndf_profs['prof_info'] = df_profs['professionals_headline'] + ' ' \\\n+ df_profs['professionals_industry'] + ' ' + df_profs['tags_followed']\n\n# clean the text using the process_text() function defined above\ndf_profs['prof_info'] = df_profs['prof_info'].apply(process_text)\n\nprint('The prof_info column contains the combined profile info of each professional.')\ndf_profs.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"81b43e38a90cd455ff5e5bffa4717db15227016d"},"cell_type":"markdown","source":"## 4.2. Process the question\n\nHere we are inserting the question into the top row of the prof_info column. This column contains the background info of each professional. Inserting the question at the top of this column will make the cosine comparison code easier to write."},{"metadata":{"_uuid":"7f75acb2285da8f638e7db4a93f7a8f421e9685b","trusted":true},"cell_type":"code","source":"# copy a row from df_profs\ndf_row1 = df_profs[df_profs.index == 0] \n# set all values to nothing\ndf_row1.loc[:,:] = ''\n# reset the index\ndf_row1 = df_row1.reset_index(drop=True)\n    \n# Assign the prof_info in this row to be the same as the question.\n# We do this because later we will compare this question to all other rows\n# in the prof_info column.\ndf_row1.loc[0, 'prof_info'] = question_text\n\n# Concat df_row to df_profs\n# The question will be the first row\ndf_profs = pd.concat([df_row1, df_profs], axis=0).reset_index(drop=True)\n\nprint('The Question, in processed form, is now located at the top of the prof_info column.')\n\ndf_profs.head(2)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3e75680a7308140e0568f572727a41e4b87ebf5d"},"cell_type":"markdown","source":"## 4.3. Vectorize the data"},{"metadata":{"_uuid":"11dc961f2bea8b776d2bbb93e2558307a2d31e19","trusted":true},"cell_type":"code","source":"\n# Select the data we want to use. \n# This column has our new question at the top.\ndata = df_profs['prof_info']\n\n# instantiate vectorizer\nvect = TfidfVectorizer(stop_words='english', ngram_range=(1, 2), max_df=0.5)\n\n# learn the 'vocabulary' of the data\nvect.fit(data)\n\n# Transform the data into a document term matrix.\n# Keep in mind that the output type is a sparse matrix.\nprof_dtm = vect.transform(data)\n\n#prof_dtm.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d6046bd0adb7a806112a72b95051ad25f5260f1b","trusted":true},"cell_type":"code","source":"# check what features have been created\n#vect.get_feature_names()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cfd7e76f6197299df687971946c4ad307caee360"},"cell_type":"markdown","source":"## 4.4. Calculate the Cosine Similarity\nWe are calculating the similarity of your question to each professional's background info. This profile is made up of a professional's headline, industry and the tags that he or she follows.\n\nBecause we used tfidf, the vectors have already been normalized. Therefore, in order to get the cosine similarity we only need to take the dot product. The dot product is known as the linear kernel."},{"metadata":{"_uuid":"247abe81147021ea7c4362801a6c74d692fa8dbc","trusted":true},"cell_type":"code","source":"# https://stackoverflow.com/questions/12118720/\n# python-tf-idf-cosine-to-find-document-similarity\n\n# prof_dtm[0:1] This selects the first row of prof_info column.\n# We are saying: Tell me how similar every row is to the first row.\ncosine_similarities = linear_kernel(prof_dtm[0:1], prof_dtm)\n\n# The line of code commented out below would give us the cosine similarity score\n# of every row to every other row, just like a correlation matrix.\n# But there's no need for this and the RAM needed for this calculation\n# would cause this kernel to crash.\n\n# cosine_similarities = linear_kernel(prof_dtm, prof_dtm)\n\n# Quick check: The first value should be 1.0 because it's the\n# comparison of the question to itself.\ncosine_similarities","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"aaf49222324d5f112ec89449dbe93865d4c027fd"},"cell_type":"markdown","source":"Let's put everything into a dataframe and sort the similarities from highest to lowest."},{"metadata":{"_uuid":"dc88037c21647522fc1d7a447cadd926c5beaead","trusted":true},"cell_type":"code","source":"# create a dataframe\ndf_cosine_matrix = pd.DataFrame(cosine_similarities)\n\n# get the column names from df_train\ncols = list(df_profs['professionals_id'])\n\n# Change the name of the first column. This is the score for the Question\ncols[0] = 'question_cosine_score'\n\n# rename the columns in the dataframe\ndf_cosine_matrix.columns = cols\n\n# Add the professionals id values as a new column.\n# This is identical to answers_author_id.\n#df_cosine_matrix['answers_id'] = df_train['answers_author_id']\n\n# set the answers_id column as the index\n#df_cosine_matrix.set_index('answers_id', inplace=True)\n\n# transpose the dataframe\ndf = df_cosine_matrix.T\n\n# rename the column\nnew_col = ['cosine_score_for_each_prof_id']\ndf.columns = new_col\n\n# sort the cosine similarity values in descending order\ndf = df.sort_values('cosine_score_for_each_prof_id',axis=0, ascending=False)\n\n# check the top 10 cosine scores\ndf.head(10)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"880c91718737ee0a7f8eb2815c7aa5731a619796"},"cell_type":"markdown","source":"## 4.5. Select the Professionals\nHere we'll select those professionals whose background info has a cosine similarity (to the question) that is greater than or equal to a threshold. I established this threshold by trial and error. I asked several questions and looked for a cosine similarity value below which the answers were not relevant to the question. 0.13 seems to be a reasonable value for this data."},{"metadata":{"_uuid":"80d907ecc7aeffa1c758b03e398927865738f704","trusted":true},"cell_type":"code","source":"# Set the cosine similarity threshold\nMODEL_1_THRESHOLD = 0.13\n\n# filter out all rows that have a cosine_score >= THRESHOLD\ndf = df[df['cosine_score_for_each_prof_id'] >= MODEL_1_THRESHOLD]\n\n# remove the first row because this row is the question we asked\ndf = df[1:]\n\nnum_professionals = len(df)\n\nprint('Number of professionals chosen: ', num_professionals)\n\nprint('This is a sample of the professionals the model has selected.')\n\n# Print the id's of the professionals who have been \n# selected as well as the associated cosine scores\ndf.head(10)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d83db39d19fe0fbe90ccdd29881ce843af1e84ba"},"cell_type":"markdown","source":"Here we create a python list containing the id's of each professional selected."},{"metadata":{"_uuid":"d821938c20256d51f0bf0b3a1f3668d527ce524d","trusted":true},"cell_type":"code","source":"# reset the index\ndf.reset_index(inplace=True)\n\n# rename the columns\nnew_names = ['prof_id', 'cosine_score_for_each_prof_id']\ndf.columns = new_names\n\n# create a list with all answer id values from df\nprof_list = list(df['prof_id'])\n\n# display the list\n#prof_list","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3a8e22a3c184bb9036a85c9bf99e403815150cba"},"cell_type":"markdown","source":"## 4.6. Why did the model select these professionals?"},{"metadata":{"_uuid":"a9b724f22b30fd1ebc8265dae18d61a2936c371a"},"cell_type":"markdown","source":"Next we'll print out the background info of the selected professionals. By looking at this we'll be able to tell if a professional was a reasonable choice or a bad choice. "},{"metadata":{"_uuid":"067bcee0a243120c974b1ae2335836ef7394833b"},"cell_type":"markdown","source":"**First, let's print the question again.**"},{"metadata":{"_kg_hide-input":true,"_uuid":"7fa32de8797493e7638f6c0a0d3f39abcdc7bfd5","trusted":true},"cell_type":"code","source":"print('Question id: ', question_id)\nprint('Question Title: ', question_title)\nprint('\\n')\nprint('Question Body:\\n ', question_body)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e5035293d4ac3876547a0879bcf4262545a1a5bb"},"cell_type":"markdown","source":"**Now let's print the background info. You'll need to scroll through the output. When looking at the printout in a forked kaggle kernel you could mistakenly think that what you see is all there is. Scroll to see more.**<br>\n| <a id='model_1_prof_printout'></a>"},{"metadata":{"_kg_hide-input":true,"_uuid":"d7c1df4066c65465706f6c0300a4326845e89260","trusted":true},"cell_type":"code","source":"# Print the profiles the professionals who can answer this question. \n# Note: If you are running this kernel you may need to scroll the output otherwise\n# you might mistakenly think that the text shown is all there is.\n\nprint('\\n')\nprint('Model 1')\nprint('Number of professionals selected: ', len(prof_list))\nprint('== Printing info on each professional who was selected ==')\n\n# set the index\ndf_professionals = df_professionals.set_index('professionals_id')\n\n# set the index of df_profs to be the question id\ndf_profs = df_profs.set_index('professionals_id')\n\n# Create an empty list to store the professional id's that are\n# associated with the answers that have been selected,\nmodel_1_list = []\n\n\nfor prof_id in prof_list:\n    \n    print('\\n')\n    \n    # print the professional's id (i.e. their name)\n    # get the prof id of the person who wrote the answer\n    \n    print('==> Professional id: ', prof_id)\n    model_1_list.append(prof_id)\n    \n    \n    # print their job title:\n    title = df_professionals.loc[prof_id, 'professionals_headline']\n    print('Title: ', title)\n    \n    # print the industry they work in\n    industry = df_professionals.loc[prof_id, 'professionals_industry']\n    print('Industry: ', industry)\n    \n    # Print the tags that are followed\n    tags = df_profs.loc[prof_id,'tags_followed']\n    print('==Tags being followed:\\n',tags)\n    ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"75616890f733ac7def57d20283628b37e494bbd0"},"cell_type":"markdown","source":"What do you think? Just by looking at their backgrounds would you have chosen these professionals to answer your question?\n\nThis is only a preliminary list. Once we have the recommendations from all four models we'll make a final selection of professionals based on who is most likely respond to an email containing the question. "},{"metadata":{"_uuid":"8926bb25082ab2b99622ad2ede4d2d6493e9da8c"},"cell_type":"markdown","source":"These are the id's of the professionals that Model_1 has selected:"},{"metadata":{"_kg_hide-output":false,"_uuid":"5e04896bfd28d64d3f556cb002d63ccf5e4541f1","trusted":true},"cell_type":"code","source":"model_1_list","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c4cc874c644a89799aa1edef136a148b4b1bb855"},"cell_type":"markdown","source":"Model 1 gives every professional in the dataset a chance to be selected, especially those who've joined recently and haven't yet answered any questions. However, the three models that follow will only consider professionals who have answered past questions. \n\nAlso, Model 1 used the hash tags that professsionals follow. The other three models won't use these hash tags."},{"metadata":{"_uuid":"3449cd6c93fa9cf730787909d72dc51f1a78769c","trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2ec50924dde245470f619c518a8f93e76553de26","trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f016f0016e9575cd42d5361da0e0b0b1237721d1","trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a75c69d057b4d4e12257052feeece3cde79b835e"},"cell_type":"markdown","source":"<hr>\n|<a id='Model_2'></a>"},{"metadata":{"_uuid":"119a44c314b9fbd9f6121a775b910ccac147cf99"},"cell_type":"markdown","source":"## 5. Model 2 - Answers, Tfidf and Cosine Similarity"},{"metadata":{"_uuid":"1ae20e340fe066e8e30a958f1578ed04401ca0f3"},"cell_type":"markdown","source":"This model only considers professionals who've answered a past question.\n\n**How does this model work?**\n\nThis model compares your question to every answer in the dataset.\n\n> **The idea is that if your question is similar to a past answer then there's a good chance that the professional who gave that answer will be able to answer your question.**\n\n\n**How can we assess how well the model is working?**\n\nAt the end the model will print both the past answer that was matched, and the profile of the professional who gave that answer. By reading this information you'll be able to judge whether that professional was a good choice to answer your question. "},{"metadata":{"_uuid":"6cece6e68231bde2fc46a9ce382f72b041a13a56"},"cell_type":"markdown","source":"## 5.1. Load the data"},{"metadata":{"_uuid":"13d4085e68c62e8152a71c433c0f9957cee2809b","trusted":true},"cell_type":"code","source":"# load df_qa_prof\ndf_qa_prof = pickle.load(open(path_1,'rb'))\n# load df_professionals\ndf_professionals = pickle.load(open(path_2,'rb'))\n\n\nprint(df_qa_prof.shape)\nprint(df_professionals.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1cbca9f60428a34ecd342374e39b6769d9e78157"},"cell_type":"markdown","source":"## 5.2. Process the Question"},{"metadata":{"_uuid":"8282f49b2e9cdd9a5b16cd381bc5489ef541330a","trusted":true},"cell_type":"code","source":"# copy a row from df_qa_prof\ndf_row2 = df_qa_prof[df_qa_prof.index == 0] \n# set all values to nothing\ndf_row2.loc[:,:] = ''\n# reset the index\ndf_row2 = df_row2.reset_index(drop=True)\n    \n# Assign the answer_text in this row to be the same as the question.\n# We do this because later we will compare this question to all other rows\n# in the answer_text column.\ndf_row2.loc[0, 'answers_text'] = question_text\n\n# Concat df_row2 to df_qa_prof.\n# The question will be at the top of the first row.\ndf_qa_prof = pd.concat([df_row2, df_qa_prof], axis=0).reset_index(drop=True)\n\ndf_qa_prof.head(2)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"52f82477edfa31df43a4d03a301187ceae16318b"},"cell_type":"markdown","source":"## 5.3. Vectorize the data"},{"metadata":{"_uuid":"6eee5dc3cbc2fbad295ddab688c8b1aa2208829e","trusted":true},"cell_type":"code","source":"# Select the data we want to use. Note we are comparing the question to answers.\n# We need to vectorize the answers_text column.\n# This column has our new question at the top.\ndata = df_qa_prof['answers_text']\n\n# instantiate vectorizer\nvect = TfidfVectorizer(stop_words='english', ngram_range=(1, 2), max_df=0.5)\n\n# learn the vocabulary of the data\nvect.fit(data)\n\n# Transform the data to a document term matrix.\n# The output type is a sparse matrix.\nprof_dtm = vect.transform(data)\n\nprof_dtm.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2feb7fb39293b4a9af1d5fcac4ef0f02b54baaf2"},"cell_type":"markdown","source":"## 5.4. Calculate the Cosine Similarity\nWe are calculating the similarity of your question to all past answers in the dataset."},{"metadata":{"_uuid":"7eab309380d58bc0d139105cee95ec5322377125","trusted":true},"cell_type":"code","source":"# https://stackoverflow.com/questions/12118720/\n# python-tf-idf-cosine-to-find-document-similarity\n\n# prof_dtm[0:1] This selects the first row of prof_info column.\n# We are saying: Tell me how similar every row is to the first row.\ncosine_similarities = linear_kernel(prof_dtm[0:1], prof_dtm)\n\n# The line below would give us the cosine similarity of every row to every other row,\n# just like a correlation matrix.\n# But there's no need for this and the RAM needed for this calculation\n# would cause this kernel to crash.\n# cosine_similarities = linear_kernel(prof_dtm, prof_dtm)\n\n# Quick check: The first value should be 1.0 because it's the\n# comparison of the question to itself.\ncosine_similarities","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a4f8db166d6ce55846a38e6f6173ba2ca52ebef4"},"cell_type":"markdown","source":"Let's put everything into a dataframe and sort the similarities from highest to lowest."},{"metadata":{"_uuid":"7c97ba71b723858eab279bdb254273d74701ab71","trusted":true},"cell_type":"code","source":"# create a dataframe\ndf_cosine_matrix = pd.DataFrame(cosine_similarities)\n\n# get the column names from df_train\ncols = list(df_qa_prof['answers_id'])\n\n# Change the name of the first column. This is the score for the Question\ncols[0] = 'question_cosine_score'\n\n# rename the columns in the dataframe\ndf_cosine_matrix.columns = cols\n\n# Add the professionals id values as a new column.\n# This is identical to answers_author_id.\ndf_cosine_matrix['answers_id'] = df_qa_prof['answers_author_id']\n\n# set the answers_id column as the index\ndf_cosine_matrix.set_index('answers_id', inplace=True)\n\n# transpose the dataframe\ndf = df_cosine_matrix.T\n\n# rename the column\nnew_col = ['cosine_score_for_each_answer_id']\ndf.columns = new_col\n\n# sort the cosine similarity values in descending order\ndf = df.sort_values('cosine_score_for_each_answer_id',axis=0, ascending=False)\n\n# check the top 20 cosine scores\ndf.head(20)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2d0ae627096938fe673c4d629443f38c17b3eebb"},"cell_type":"markdown","source":"## 5.5. Select the Answers\nHere we select the answers that have a cosine similarity that is greater than or equal to a threshold. "},{"metadata":{"_uuid":"f68a010f4632684815287c8287a7241c4c115a39","trusted":true},"cell_type":"code","source":"# Set the cosine similarity threshold\nMODEL_2_THRESHOLD = 0.1\n\n# filter out all rows that have a cosine_score >= THRESHOLD\ndf = df[df['cosine_score_for_each_answer_id'] >= MODEL_2_THRESHOLD]\n\n# remove the first row because this row is the question we asked\ndf = df[1:]\n\nnum_answers = len(df)\n\nprint('Number of answers chosen: ', num_answers)\n\nprint('This is a sample of the answers the model has selected.')\n\n# print the answers that have been selected as well as the associated cosine scores\ndf.head(10)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e9bf11fcca89436f49c7ee09ce6ce39b7b90361d"},"cell_type":"markdown","source":"## 5.6. Identify the professionals that gave each answer"},{"metadata":{"_uuid":"bf8adceb179f5a560a37c434c4e1c96ed0057e02"},"cell_type":"markdown","source":"Here we will identify the professionals that gave each answer. These will be the professionals that this model thinks are best able to answer your question. \n\nWe start by creating a python list containing the id's of each answer selected."},{"metadata":{"_kg_hide-output":true,"_uuid":"223f75849273dd694bb963e13171524bd80adb5a","trusted":true},"cell_type":"code","source":"# reset the index\ndf.reset_index(inplace=True)\n\n# rename the columns\nnew_names = ['answers_id', 'cosine_score_for_each_answer_id']\ndf.columns = new_names\n\n# create a list with all answer id values from df\nanswer_list = list(df['answers_id'])\n\n# display the list\n#answer_list","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c0e2834857ebc44fd77edf713573e00268942bd6"},"cell_type":"markdown","source":"**Let's print the question again.**"},{"metadata":{"_kg_hide-input":true,"_uuid":"bdc211c08c80312204e4c2f7492dda8d17b04921","trusted":true},"cell_type":"code","source":"print('Question id: ', question_id)\nprint('Question Title: ', question_title)\nprint('\\n')\nprint('Question Body:\\n ', question_body)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ab8636d192a498b18ddecbd8c17d9ea6c1f9eaad"},"cell_type":"markdown","source":"First we'll get the id of the professional who gave each answer. Then we'll print the profile of that professional and their answer. There could be duplicate professionals in this list because the same professional could have given several past answers.<br>\n\n| <a id='model_2_prof_printout'></a>"},{"metadata":{"_kg_hide-input":true,"_uuid":"5566042bbfb5fd1a861599444cce69ce61dbfba1","trusted":true},"cell_type":"code","source":"# Print info on the professionals who can answer this question\n\n#print('\\n')\nprint('Model 2')\nprint('Number of professionals selected: ', len(answer_list))\nprint('== Printing info on each professional who was selected ==')\n\n# set the index\ndf_professionals = df_professionals.set_index('professionals_id')\n\n\n\n# Create an empty list to store the professional id's that are\n# associated with the answers that have been selected,\nmodel_2_list = []\n\n# set the index of df_train to be the question id\ndf_qa_prof = df_qa_prof.set_index('answers_id')\n\nfor ans_id in answer_list:\n    \n    # print the professional's id (i.e. their name)\n    # get the prof id of the person who wrote the answer\n    prof_id = df_qa_prof.loc[ans_id, 'answers_author_id']\n    print('\\n')\n    print('==> Professional id: ', prof_id)\n    model_2_list.append(prof_id)\n    \n    \n    # print their job title:\n    title = df_professionals.loc[prof_id, 'professionals_headline']\n    print('Title: ', title)\n    \n    # print the industry they work in\n    industry = df_professionals.loc[prof_id, 'professionals_industry']\n    print('Industry: ', industry)\n    \n    # Print the answer that they wrote which was similar the question being asked\n    answer = df_qa_prof.loc[ans_id,'answers_body']\n    print('==Answer given to similar question:\\n',answer)\n    ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0bc400684aa5e364e22d6c1013b0d181897895b6"},"cell_type":"markdown","source":"We now have a list of professionals that model 2 has chosen to answer your question. Based on their past history printed above, would you say that your question is relevant to them?\n\nOnce again this is just a preliminary list. Later we'll filter out those professionals who have a high possibility of actually submitting an answer."},{"metadata":{"_uuid":"4e39966e6675ec983a83b71809ce3ff19c4c882b"},"cell_type":"markdown","source":"These are the id's of the professionals that Model_2 has selected:"},{"metadata":{"_kg_hide-output":false,"_uuid":"70a6c518407afa898c3063c34c5aa5fba7942439","trusted":true},"cell_type":"code","source":"# uncomment the next line to print the list of professional id's\n\n# model_2_list","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bc3e16fc5b4fb5eea58d558bec451a52a37c52d8","trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"22cb70ac8536f43d6c1c30b9d4966981df424a17","trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c83f4344c29ffe2a3ba08ced9ff75a516b143959","trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4284b41df7f56346715923797c33c7f50bc9d853"},"cell_type":"markdown","source":"<hr>\n| <a id='Model_3'></a>"},{"metadata":{"_uuid":"7827d49a32cbe2484023d255ccc064858d1c178f"},"cell_type":"markdown","source":"## 6. Model 3 - Answers, Tfidf, TruncatedSVD and Cosine Similarity\n"},{"metadata":{"_uuid":"d5be5ac3e65874a765cb26648e5a713b5a1ae2f0"},"cell_type":"markdown","source":"Singular Value Decomposition (SVD) is commonly understood as a dimensionality reduction technique. However, it can be also be seen as a way of creating a new set of features. These are called latent features. It's not clear what each latent feature represents but they're very effective in capturing the essence of the data. The previous model used more than 1.5 million features when calculating cosine similarity. This model will use 200.\n\nThe workflow is almost identical to model 2. The diffference is that we'll take the output from tfidf and transform it using TruncatedSVD.\n"},{"metadata":{"_uuid":"d4d854a967af1bcaa967e214aae23b623fe259ed"},"cell_type":"markdown","source":"## 6.1. Load the data"},{"metadata":{"_uuid":"ec468f57f2d1a69eb3bb3928d08b8440acd4b4aa","trusted":true},"cell_type":"code","source":"# load df_qa_prof\ndf_qa_prof = pickle.load(open(path_1,'rb'))\n# load df_professionals\ndf_professionals = pickle.load(open(path_2,'rb'))\n\n\nprint(df_qa_prof.shape)\nprint(df_professionals.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b4b0926d75ee3e5da1c50665feb86b79ff933ce3"},"cell_type":"markdown","source":"## 6.2. Process the Question"},{"metadata":{"_uuid":"fb18aaae41a25c9eb002a3965c2f18a0abefcb9c","trusted":true},"cell_type":"code","source":"# copy a row from df_qa_prof\ndf_row2 = df_qa_prof[df_qa_prof.index == 0] \n# set all values to nothing\ndf_row2.loc[:,:] = ''\n# reset the index\ndf_row2 = df_row2.reset_index(drop=True)\n    \n# Assign the answer_text in this row to be the same as the question.\n# We do this because later we will compare this question to all other rows\n# in the answer_text column.\ndf_row2.loc[0, 'answers_text'] = question_text\n\n# Concat df_row2 to df_qa_prof.\n# The question will be at the top of the first row.\ndf_qa_prof = pd.concat([df_row2, df_qa_prof], axis=0).reset_index(drop=True)\n\ndf_qa_prof.head(2)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1caba787de7935747daa492c4f4872e428ce070d"},"cell_type":"markdown","source":"## 6.3. Vectorize the data"},{"metadata":{"_uuid":"9535a50efce50c77b05dd6aeac46d016818484de","trusted":true},"cell_type":"code","source":"# Select the data we want to use. Note we are comparing the question to answers.\n# We need to vectorize the answers_body column.\n# This column has our new question at the top.\ndata = df_qa_prof['answers_text']\n\n# instantiate vectorizer\nvect = TfidfVectorizer(stop_words='english', ngram_range=(1, 2), max_df=0.5)\n\n# learn the vocabulary of the data\nvect.fit(data)\n\n# transform the data to a document term matrix\nprof_dtm = vect.transform(data)\n\nprof_dtm.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7221d81d02c14bddf7b6fbb37782b311d5ebc48f"},"cell_type":"markdown","source":"## 6.4. Transform prof_dtm using TruncatedSVD"},{"metadata":{"_uuid":"ae66a5e10fcf52d6cd03439605dcb3609137ba1a","trusted":true},"cell_type":"code","source":"from sklearn.decomposition import TruncatedSVD\n\n# Initialize\ntsvd = TruncatedSVD(n_components=200, random_state=101)\n\n# Fit\ntsvd.fit(prof_dtm)\n\n# Transform\n# This returns a type numpy array and not a sparse matrix type as with tfidf.\nprof_dtm = tsvd.transform(prof_dtm)\n\nprof_dtm.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1f7d51b637ac016b6bbf7b3e15208a18e6e84b25"},"cell_type":"markdown","source":"Let's put the output into a dataframe so we can see what it is. Intially there were 1,522,916 features. Now there are just 200 features. The number of rows is still 51,124."},{"metadata":{"_uuid":"580fd82b48118e0d2f2883fde67812426bf26b9a","trusted":true},"cell_type":"code","source":"# create a dataframe\ndf = pd.DataFrame(prof_dtm)\n\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"47188ab93cb880540db09a4950ee8f2f824f1d02"},"cell_type":"markdown","source":"## 6.5. Calculate the Cosine Similarity\nWe are calculating the similarity of your question to all past answers in the dataset."},{"metadata":{"_uuid":"4ec01bc71ea87474b000f172e89804ae8bc42266","trusted":true},"cell_type":"code","source":"# https://stackoverflow.com/questions/12118720/\n# python-tf-idf-cosine-to-find-document-similarity\n\n# prof_dtm[0:1] This selects the first row of prof_info column. Note this slicing \n# is for a sparse matrix.\n# We are saying: Tell me how similar every row is to the first row.\n# Note that we are using cosine_similarity here and not linear_kernel.\ncosine_similarities = cosine_similarity(prof_dtm[0:1], prof_dtm)\n\n# The line below would give us the cosine similarity of every row to every other row,\n# just like a correlation matrix.\n# But there's no need for this and the RAM needed for this calculation\n# would cause this kernel to crash.\n# cosine_similarities = linear_kernel(prof_dtm, prof_dtm)\n\n# Quick check: The first value should be 1.0 because it's the\n# comparison of the question to itself.\ncosine_similarities","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5b7222774360aaf8d88ec48ad07bcc96cf8f7f25"},"cell_type":"markdown","source":"Let's put everything into a dataframe and sort the similarities from highest to lowest."},{"metadata":{"_uuid":"361db5381d3593a92930c4202c9ec650d7734700","trusted":true},"cell_type":"code","source":"# create a dataframe\ndf_cosine_matrix = pd.DataFrame(cosine_similarities)\n\n# get the column names from df_train\ncols = list(df_qa_prof['answers_id'])\n\n# Change the name of the first column. This is the score for the Question\ncols[0] = 'question_cosine_score'\n\n# rename the columns in the dataframe\ndf_cosine_matrix.columns = cols\n\n# Add the professionals id values as a new column.\n# This is identical to answers_author_id.\ndf_cosine_matrix['answers_id'] = df_qa_prof['answers_author_id']\n\n# set the answers_id column as the index\ndf_cosine_matrix.set_index('answers_id', inplace=True)\n\n# transpose the dataframe\ndf = df_cosine_matrix.T\n\n# rename the column\nnew_col = ['cosine_score_for_each_answer_id']\ndf.columns = new_col\n\n# sort the cosine similarity values in descending order\ndf = df.sort_values('cosine_score_for_each_answer_id',axis=0, ascending=False)\n\n# check the top 20 cosine scores\ndf.head(20)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"735e12da6f178d21d46d65c4d4a08de3e3870953"},"cell_type":"markdown","source":"## 6.6. Select the Answers\nHere we select the answers that have a cosine similarity that is greater than or equal to a threshold. "},{"metadata":{"_uuid":"9c142960c4c6ec57c71cfeb9ffd4877f9d6a9cce","trusted":true},"cell_type":"code","source":"# Set the cosine similarity threshold\nMODEL_3_THRESHOLD = 0.65\n\n# filter out all rows that have a cosine_score >= THRESHOLD\ndf = df[df['cosine_score_for_each_answer_id'] >= MODEL_3_THRESHOLD]\n\n# remove the first row because this row is the question we asked\ndf = df[1:]\n\nnum_answers = len(df)\n\nprint('Number of answers chosen: ', num_answers)\n\nprint('This is a sample of the answers the model has selected.')\n\n# print the answers that have been selected as well as the associated cosine scores\ndf.head(10)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ac71790e6533241a9120dcdc480eba3a7b5a25bd"},"cell_type":"markdown","source":"## 6.7. Identify the professionals that gave each answer"},{"metadata":{"_uuid":"bc5d1975148bba896d5a9133c228d4bee95caef1"},"cell_type":"markdown","source":"Here we will identify the professionals that gave each answer. These will be the professionals that this model thinks are best able to answer your question. \n\nWe start by creating a python list containing the id's of each answer selected."},{"metadata":{"_uuid":"b85abc78b5fb227d1aeebe0f639af8df3bf9c85b","trusted":true},"cell_type":"code","source":"# reset the index\ndf.reset_index(inplace=True)\n\n# rename the columns\nnew_names = ['answers_id', 'cosine_score_for_each_answer_id']\ndf.columns = new_names\n\n# create a list with all answer id values from df\nanswer_list = list(df['answers_id'])\n\n# display the list\n#answer_list","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"da10f11d570e230302c288377f385c85101deef6"},"cell_type":"markdown","source":"**Let's print the question again.**"},{"metadata":{"_uuid":"43bdee6a8dea40ea94de48fd79d260f5d58a456a","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"print('Question id: ', question_id)\nprint('Question Title: ', question_title)\nprint('\\n')\nprint('Question Body:\\n ', question_body)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a0e957e7bbf66fb81d44e94beb16d23147c7a01a"},"cell_type":"markdown","source":"We'll get the id of the professional who gave each answer. Then we'll print the profile of that professional and their answer. There could be duplicate professionals in this list because the same professional could have given several past answers.<br>"},{"metadata":{"_uuid":"c13352f2728e3907cd86caeef2f90a17a296afb7","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Print info on the professionals who can answer this question\n\n#print('\\n')\nprint('Model 3')\nprint('Number of professionals selected: ', len(answer_list))\nprint('== Printing info on each professional who was selected ==')\n\n# set the index\ndf_professionals = df_professionals.set_index('professionals_id')\n\n\n\n# Create an empty list to store the professional id's that are\n# associated with the answers that have been selected,\nmodel_3_list = []\n\n# set the index of df_train to be the question id\ndf_qa_prof = df_qa_prof.set_index('answers_id')\n\nfor ans_id in answer_list:\n    \n    # print the professional's id (i.e. their name)\n    # get the prof id of the person who wrote the answer\n    prof_id = df_qa_prof.loc[ans_id, 'answers_author_id']\n    print('\\n')\n    print('==> Professional id: ', prof_id)\n    model_3_list.append(prof_id)\n    \n    \n    # print their job title:\n    title = df_professionals.loc[prof_id, 'professionals_headline']\n    print('Title: ', title)\n    \n    # print the industry they work in\n    industry = df_professionals.loc[prof_id, 'professionals_industry']\n    print('Industry: ', industry)\n    \n    # Print the answer that they wrote which was similar the question being asked\n    answer = df_qa_prof.loc[ans_id,'answers_body']\n    print('==Answer given to similar question:\\n',answer)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"52b5386a865d335a449bb51b0c6a7681ef68489c"},"cell_type":"markdown","source":"These are the id's of the professionals that Model_3 has selected:"},{"metadata":{"_uuid":"d4de7cf345cf5388f96e7d6258f4d32da4775e4e","trusted":true},"cell_type":"code","source":"# uncomment the next line to print the list of professional id's\n\n# model_3_list","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c7a9b3b085c55fdac9b82a35b3445c4a4dbf1d00"},"cell_type":"markdown","source":"Is your question relevant to these professionals? \n\nIn the next model we're going to add some Ai magic in the form of pre-trained word embeddings."},{"metadata":{"_uuid":"900322e1ba7f30b07b5b3a333edecbfb97193737","trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"71977f0e19037bacf5dcb6ee866acf7b56fa7c03","trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ad89672e9e9bb5a1d18bfeef5f9c487b419ad58a"},"cell_type":"markdown","source":"<hr>\n| <a id='Model_4'></a>"},{"metadata":{"_uuid":"06229a38eea85eab06c9eb176fca2bf5b02f7c56"},"cell_type":"markdown","source":"## 7. Model 4 - Answers, GloVe Embeddings and Cosine Similarity"},{"metadata":{"_uuid":"b3cdf77ac90c9ae72697a4556f1709f0b8716a1f"},"cell_type":"markdown","source":"Pre-trained word embeddings are dense vectors that have been trained on text corpuses containing millions of words. Tfidf encodes word frequency but  embedding vectors encode the \"meaning\" of words and are able to understand analogies such as: man is to woman as king is to queen. In other words, embedding vectors help a model understand that the words man, woman, king and queen are all gender related. Moreover, a model will understand that king and queen are royalty.\n\nIn addition to trying to create relevant matches, here I'm using embedding vectors to increase the diversity of answers (more viewpoints) that student's receive. For example, if a student asks how to become a film star, it would be good if that student also received advice from people involved in theatre. This would be possible because the model would know that the words film and theatre are closely related.\n\nThis model will encode words using pre-trained GloVe word embeddings. We will use 200-dimensional english word vectors. These were pre-trained on the combined Wikipedia 2014 + Gigaword 5th Edition corpora (6B tokens, 400K vocab). Because GloVe vectors are available as a Kaggle dataset I've simply imported them into this kernel.\n\nThe embedding vector length is 200. For each answer we'll consider only the first 500 words (max_length = 500). Shorter answers will be padded with zeros. For long answers, all words beyond the first 500 will be thrown away. To create a vector for a given answer, we will average all the 200-long word vectors that make up that answer.\n\nAgain, this model will compare your question to all answers in the dataset. If your question is similar to a past answer then there's a good chance that the professional who gave that answer will be able to answer your question.\n"},{"metadata":{"_uuid":"4f4cf7e01a35f910f72c55c81cb0c3dbd7505986"},"cell_type":"markdown","source":"## 7.1. Define the document corpus"},{"metadata":{"_uuid":"48bfba753a775fdf2ad48605b62bca94a767638a"},"cell_type":"markdown","source":"\n> The answers_text column in dataframe df_qa_prof will be the corpus of documents that we'll use to create this model. Whenever we refer to a document corpus, this is the column that we'll be referring to.\n"},{"metadata":{"_uuid":"bdf81ba1008f93bcb145c91715e55b562b5efc64"},"cell_type":"markdown","source":"## 7.2. Load the Data"},{"metadata":{"_uuid":"f68b1b6f4e88389a439efffa64e7cad0b4ab72ee","trusted":true},"cell_type":"code","source":"# load df_qa_prof\ndf_qa_prof = pickle.load(open(path_1,'rb'))\n# load df_professionals\ndf_professionals = pickle.load(open(path_2,'rb'))\n\n\nprint(df_qa_prof.shape)\nprint(df_professionals.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"03b09f97236651687d1e57614fed45b8a2102d03","trusted":true},"cell_type":"code","source":"# We will use GloVe vectors that have a standard length of 200\nEMBED_LENGTH = 200","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"827991665a5860e879cca273891b3f920e4046bc"},"cell_type":"markdown","source":"## 7.3. Process the question"},{"metadata":{"_uuid":"6e0ec5621898aa6c38290e0d0c566d4ee1a5cc31","trusted":true},"cell_type":"code","source":"# copy a row from df_qa_prof\ndf_row3 = df_qa_prof[df_qa_prof.index == 0] \n# set all values to nothing\ndf_row3.loc[:,:] = ''\n# reset the index\ndf_row3 = df_row3.reset_index(drop=True)\n    \n# Assign the answer_text in this row to be the same as the question.\n# We do this because later we will compare this question to all other rows\n# in the answer_text column.\ndf_row3.loc[0, 'answers_text'] = question_text\n\n# Concat df_row to df_qa_prof\n# The question will be the first row\ndf_qa_prof = pd.concat([df_row3, df_qa_prof], axis=0).reset_index(drop=True)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9927216f26b9dd73034a782ef5fd7745ed1d8f07"},"cell_type":"markdown","source":"## 7.4. Pre-process the data"},{"metadata":{"_uuid":"5d769299f601b73c1efad49929ba6d486f0dc711","trusted":true},"cell_type":"code","source":"# Create a new column showing the length of each answer\ndf_qa_prof['answer_length'] = df_qa_prof['answers_body'].apply(len)\n\nprint('The answers_text column is the document corpus.')\ndf_qa_prof.head(2)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7ddb37da7e31a24e8cb555902602b243fe722d03"},"cell_type":"markdown","source":"## 7.5. Assemble the GloVe Embedding Matrix for our text corpus\nWe'll use pre-trained GloVe embeddings that are available in Kaggle datasets."},{"metadata":{"_uuid":"b7782cc84d9f7f8d75760348ce2e793e9d04d073","trusted":true},"cell_type":"code","source":"# Create a corpus of documents\ncorpus_text_list = list(df_qa_prof['answers_text'])\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"31ea8c62f6e677c438db5f99370e5f56c324ee2b"},"cell_type":"markdown","source":"### Tokenize the corpus of documents (i.e. extract the vocabulary)"},{"metadata":{"_uuid":"090613e36c8f6f4d3a422488b02f23bc373b68a9","trusted":true},"cell_type":"code","source":"# Instantiate the tokenizer.\n# Note that this is a word tokenizer.\nt = Tokenizer()\n\n# create a dictionary where the word is the key and a number is the value\nt.fit_on_texts(corpus_text_list)\n\n# How many words are there in our corpus vocabulary?\n\nvocab_size = len(t.word_index)\nprint('Vocab size: ', vocab_size)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"_uuid":"5774f7656a4c649e92b80c9f640db0c06164ae91","trusted":true},"cell_type":"code","source":"# These are all the words in the vocabulary of our corpus.\n# Each word is assigned an index starting at 1.\n\nt.word_index","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1ff6c3f89531117ab4340f467f8d1568c9f69ccc","trusted":true},"cell_type":"code","source":"# Add 1 to the number of words in the vocabulary\nvocab_size = len(t.word_index) + 1\nvocab_size","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9ad290870516254c9ae3e3312824a9c02c8fe8f2"},"cell_type":"markdown","source":"### Integer encode the corpus documents\n\nHere we are replacing every word with its cprresponding index. Each row in our text corpus is a seperate list of these index values. Take note that the lists have different lengths.\n\n- encoded_docs is a list of lists. [[2, 101, 605], [33, 77],...]"},{"metadata":{"_uuid":"6abc9207671e6b9bab3a895084b0aa3599f28d0c","trusted":true},"cell_type":"code","source":"# convert the text to sequences of numbers\nencoded_docs = t.texts_to_sequences(corpus_text_list)\n\n# Print the list of lists\n#print(encoded_docs)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"65058f587be4fa2a902babab84751adec30e1b95"},"cell_type":"markdown","source":"### Pad each list so they all have the same length\n\nHere we'll pad each list with zeros so that they all have the same length."},{"metadata":{"_uuid":"946f9e3632bbef8c3402f3f871008709bd27175b","trusted":true},"cell_type":"code","source":"\n# Let's look at the text lengths to decide what max_length to use\nprint('Min length: ', df_qa_prof['answer_length'].min())\nprint('Max length: ',df_qa_prof['answer_length'].max())\nprint('Mean length: ',df_qa_prof['answer_length'].mean())\nprint('Median length: ',df_qa_prof['answer_length'].median())\nprint('Mode lengths: ',df_qa_prof['answer_length'].mode()) # value that appears most often\n\n# Set the max_length \nmax_length = 500\n\n# Pad each list so they all have the same length\npadded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n\n# (num_answers, max_length)\npadded_docs.shape  ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e1a7aa3e37bc9095d52e01008159b07b4d4c5fdc"},"cell_type":"markdown","source":"### Create a GloVe embedding matrix specific to our corpus vocab"},{"metadata":{"_uuid":"5ea90c76a02ef5fdc177cf0517e48a5cd6d2aac5","trusted":true},"cell_type":"code","source":"# source: https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/\n\n# We will use pre-trained GloVe emedding vectors from Kaggle Datasets that\n# have been imported into this kernel.\n# https://www.kaggle.com/rtatman/glove-global-vectors-for-word-representation\n\n# Load the pre-trained GloVe vectors\n# Set the path to glove.6B.200d.txt\npath = '../input/glove-global-vectors-for-word-representation/glove.6B.200d.txt'\n\nembeddings_index = dict()\nf = open(path)\n\nfor line in f:\n    # Note: use split(' ') instead of split() if you get an error.\n    values = line.split(' ')\n    word = values[0]\n    coefs = np.asarray(values[1:], dtype='float32')\n    embeddings_index[word] = coefs\nf.close()\n\nprint('Loaded %s word vectors.' % len(embeddings_index))\n\n# create a weight matrix\nembedding_matrix = np.zeros((vocab_size, EMBED_LENGTH))\nfor word, i in t.word_index.items():\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        embedding_matrix[i] = embedding_vector\n\nprint('The result is a matrix of embeddings.')\nprint('Words are the rows, the features are the columns.')\n\n# The result is a matrix of embeddings only for words in our data.\n# Words are the rows, the features are the columns.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"897add651b2ca1f5ff9158b4d9aff5c39a68e425"},"cell_type":"markdown","source":"Let's put the embedding matrix into a dataframe so we can more clearly see what it is. This matrix includes the embedding vectors for all words in the GloVe vocab. Later we'll extract the vectors that correspond to the words in our document corpus."},{"metadata":{"_uuid":"775d06712b25652f7025ca9ac2cd8b79ce4767e8","trusted":true},"cell_type":"code","source":"\n\n# Note that the words are on the index column\ndf_glove_embeddings = pd.DataFrame(embedding_matrix)\n\n# get all the dictionary keys as a list\nword_dict = t.word_index\n\n# get a list of keys\nkeys = list(word_dict.keys())\n\n# Insert a dummy_word at the first position.\n# The dummy_word exists because our dict key:value pairs\n# start from word:1 and not word:0.\nkeys.insert(0, 'dummy_word')\n\n# transpose the dataframe so that the words become the columns\ndf_glove_embeddings = df_glove_embeddings.T\n\n# set the names of the columns\ndf_glove_embeddings.columns = keys\n\n\n# convert the dataframe back to the original form\ndf_glove_embeddings = df_glove_embeddings.T\n\n# reset the index\ndf_glove_embeddings = df_glove_embeddings.reset_index(drop=False)\n\n# change the name of the first column to 'words'\ncolumn_names = list(df_glove_embeddings.columns)\ncolumn_names[0] = 'words'\ndf_glove_embeddings.columns = column_names\n\nprint('This is the embeddings in a dataframe.')\ndf_glove_embeddings.head(10)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"97c901e7113cda5b45f0ef930821581bb35f52b2"},"cell_type":"markdown","source":"Now let's create an embedding matrix that only includes words in our document corpus. This is simply a look-up function. For every word in a document, this code looks up the embedding vector associated with that word and inserts it in encoding_mat. \n\nTake note that here we are **averaging** all the word vectors that make up a given answer.\n\n"},{"metadata":{"_uuid":"db8199c96ffc1123751009d64a3a15bd77c0034d","trusted":true},"cell_type":"code","source":"# create an empty matrix\nencoding_mat = np.zeros((len(padded_docs), EMBED_LENGTH))\n\nfor i in range(0,len(padded_docs)):\n    # select the document\n    padded_doc = padded_docs[i]\n    # create an empty encoding list\n    encoding = np.zeros(EMBED_LENGTH)\n    # select a document\n    for item in padded_doc:\n        # Here we are adding the vectors together.\n        # This selects a row from embedding_matrix.\n        # The output is a list.\n        encoding = encoding + embedding_matrix[item] # item is an integer value\n        \n    # Insert the encoding to encoding_mat\n    # Here we are averaging the encodings by dividing by the length.\n    encoding_mat[i] = encoding/max_length\n\n# check the shape of the matrix\nencoding_mat.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2b55fc4b5a0fe0d9ee5f2de98d34ee7347269a22","trusted":true},"cell_type":"code","source":"# Display the embedding matrix\n# The words are the rows and the features are the columns.\n\n# Every row represents one answer that has been encoded as a vector\ndf_encoding_mat = pd.DataFrame(encoding_mat)\n\nprint('This is the embedding matrix. Each row represents one answer that has been encoded as a vector.')\nprint('Row 0 is the question.')\ndf_encoding_mat.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b2f8cedd7e99527f532ea238e1ad6095681abada","trusted":true},"cell_type":"code","source":"# check the shape of the embedding matrix\nencoding_mat.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d04cc2860fcaf8f792277fdb042ffbbbe5b63968"},"cell_type":"markdown","source":"## 7.6. Calculate the cosine similarity of the question (first row) to every answer (all other rows)"},{"metadata":{"_uuid":"8cb0cd1bccdbfcde8c49592ff082fe2a0c0a4da7","trusted":true},"cell_type":"code","source":"# reshape the encoding matrix to (num_samples, num_features)\nencoding_mat = encoding_mat #.reshape(max_length,EMBED_LENGTH) \n# reshape the base_document i.e. the one we will compare to all others\nbase_doc = encoding_mat[0].reshape(1,EMBED_LENGTH)\n\n# calculate the cosine similarity\ncosine_similarities = cosine_similarity(base_doc, encoding_mat)\n\n# The following would compute a cosine similiarity matrix comapring every\n# doc to every other doc, like a correlation matrix.\n# This uses a lot of RAM.\n#cosine_similarities = cosine_similarity(encoding_mat, encoding_mat)\n\ncosine_similarities.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3eb7e320649ca460d6afcfdad622b65fc9012b29","trusted":true},"cell_type":"code","source":"# flatten the matrix\ncosine_similarities = cosine_similarities.flatten()\n\n#Check: The first value should be 1.0 because the \n# question is being compared to itself.\ncosine_similarities","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"dd4f3e19e16f24c062268e59391295c378e91091"},"cell_type":"markdown","source":"Let's put everything into a dataframe and sort the similarities from highest to lowest."},{"metadata":{"_uuid":"f27f7ac342166c2e3e08bd88bda70be823002df5","trusted":true},"cell_type":"code","source":"# create a dataframe\ndf_cosine_matrix = pd.DataFrame(cosine_similarities)\n\n# transpose the dataframe\ndf_cosine_matrix = df_cosine_matrix.T\n\n# get the column names from df_train\ncols = list(df_qa_prof['answers_id'])\n\n# Change the name of the first column. This is the score for the Question\ncols[0] = 'question_cosine_score'\n\n# rename the columns in the dataframe\ndf_cosine_matrix.columns = cols\n\n# Add the professionals id values as a new column.\n# This is identical to answers_author_id.\ndf_cosine_matrix['answers_id'] = df_qa_prof['answers_author_id']\n\n# set the answers_id column as the index\ndf_cosine_matrix.set_index('answers_id', inplace=True)\n\n# transpose the dataframe\ndf = df_cosine_matrix.T\n\n# rename the column\nnew_col = ['cosine_score_for_each_answer_id']\ndf.columns = new_col\n\n# sort the cosine similarity values in descending order\ndf = df.sort_values('cosine_score_for_each_answer_id',axis=0, ascending=False)\n\n# check the top 20 cosine scores\ndf.head(20)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" Pause here for a moment. Please take a look at the previous dataframe. You'll notice that Model 4 has identified quite a few professionals. However, for many questions you're going to find that the final printout for this model does not contain any recommended professionals. This is because the threshold is set quite high. It's at 0.94 at the moment. Later I'll explain why this threshold is set high."},{"metadata":{"_uuid":"cb41a189c811112c333dd96c0585dfbd9761cfd6"},"cell_type":"markdown","source":"## 7.7. Select the Answers\nSelect the answers that have a cosine similarity that is greater than or equal to a threshold value."},{"metadata":{"_uuid":"d55a75010b25bddef60113858063ae1bd4e13cce","trusted":true},"cell_type":"code","source":"# Set the cosine similarity threshold\nMODEL_4_THRESHOLD = 0.94\n\n\n# filter out all rows that have a cosine_score >= THRESHOLD\ndf_selected = df[df['cosine_score_for_each_answer_id'] >= MODEL_4_THRESHOLD]\n\n# remove the first row because this row is the question we asked\ndf_selected = df_selected[1:]\n\nnum_answers = len(df_selected)\n\nprint('Number of answers chosen: ', num_answers)\n\nprint('This is a sample of the answers the model has selected.')\n\n# print the answers that have been selected as well as the associated cosine scores\ndf_selected.head(10)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ab6baf8c242e790bac47a95f8f0f837d98b62ec6"},"cell_type":"markdown","source":"This is a list of id's that correspond to the answers that are similar to the question you asked. We'll identify the professional that gave each answer. These will be the professionals that are best able to answer your question."},{"metadata":{"_uuid":"61e3fcd5cfc85ff58543c5eac82d1d2803a2ae55","trusted":true},"cell_type":"code","source":"# reset the index\ndf_selected.reset_index(inplace=True)\n\n# rename the columns\nnew_names = ['answers_id', 'cosine_score_for_each_answer_id']\ndf_selected.columns = new_names\n\n# create a list with all answer id values from df\nanswer_list = list(df_selected['answers_id'])\n\n# display the list\n# answer_list","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ab8dc2e45525fadbd5df9ea5f3095d16e15e877c"},"cell_type":"markdown","source":"**Let's print the question again.**"},{"metadata":{"_uuid":"78dc431a52c1e937d53afee7334cdca3f45bcb99","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"print('Question id: ', question_id)\nprint('Question Title: ', question_title)\nprint('\\n')\nprint('Question Body:\\n ', question_body)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d8916a4cdcb1000bc9a6b4798655e060e70679a7"},"cell_type":"markdown","source":"Next we'll print the profile of each professional as well as the answer they gave to a similar question. Again, there could be duplicate professionals in this list because the same professional could have given several past answers.<br>\n| <a id='model_3_prof_printout'></a>"},{"metadata":{"_uuid":"6cbe2f1cc325afe268a9c1d8ea7d83b75b528d3c","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Print info on the professionals who can answer this question\n\n#print('\\n')\nprint('Model 4')\nprint('Number of professionals selected: ', len(answer_list)) # correct this. there could be duplicates\nprint('== Printing info on each professional who was selected ==')\n\n# set the index\ndf_professionals = df_professionals.set_index('professionals_id')\n\n# Create an empty list to store the professional id's that are\n# associated with the answers that have been selected,\nmodel_4_list = []\n\n# set the index of df_train to be the question id\ndf_qa_prof = df_qa_prof.set_index('answers_id')\n\n\nfor ans_id in answer_list:\n    \n    # print the professional's id (i.e. their name)\n    # get the prof id of the person who wrote the answer\n    prof_id = df_qa_prof.loc[ans_id, 'answers_author_id']\n    print('\\n')\n    print('==> Professional id: ', prof_id)\n    model_4_list.append(prof_id)\n    \n    print('Answer id: ', ans_id)\n    \n    \n    # print their job title:\n    title = df_professionals.loc[prof_id, 'professionals_headline']\n    print('Title: ', title)\n    \n    # print the industry they work in\n    industry = df_professionals.loc[prof_id, 'professionals_industry']\n    print('Industry: ', industry)\n    \n    # Print the answer that they wrote which was similar the question being asked\n    answer = df_qa_prof.loc[ans_id,'answers_body']\n    \n    print('==Answer given to similar question:\\n',answer)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b8c9a86a1b858045e017f00e6016e6e7dd51f03c"},"cell_type":"markdown","source":"These are the id's of the professionals that Model_4 has selected:"},{"metadata":{"_kg_hide-output":false,"_uuid":"6568ad4b6e52bdc3223db2a7737859d156e2d3ae","trusted":true},"cell_type":"code","source":"# uncomment the next line to print the list of professional id's\n\n# model_4_list","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c999e4e1e49bbe0a726135684277d3ecb2a4bd00","trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7fbd2919fc4bbb605235fe934aff41db096a9c8a","trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e5f46c5d6438bee1b5a487c9328a50f39a285389"},"cell_type":"markdown","source":"<hr>\n| <a id='Select_professionals'></a>"},{"metadata":{"_uuid":"b8c77cd2c11a95539ee3c48f01a24213b2149e7f"},"cell_type":"markdown","source":"## 8. Select those professionals who are most likely to answer the question"},{"metadata":{"_uuid":"104269bf724f64fc0c836871eeb91ad10f79c7ea"},"cell_type":"markdown","source":"We have recommendations from four models. Now we need to filter out those professionals who are likely to answer when they are sent this question via email.\n\n**What selection criteria are we going to use?**\n\n> We will ask four questions:\n> 1. Has this professional joined CareerVillage within the last 30 days of the most recent new user who signed up?\n> 2. Has this professional answered a past question from the student who posted this question?\n> 2. Has this professional answered a question within 30 days of the most recent answer posted on CareerVillage?\n> 3. Has this professional made a comment within 30 days of the most recent answer posted on CareerVillage?\n\nIf the answer is yes to any one of these questions then we will conclude that there is a high possibility that this professional will respond to the email.\n\nThis is the logic behind this filter:\n\nNew users are more likely to be highly active. Also, having recently answered a question or made a comment is an indicator that a person is contributing to the community and is therefore likely to respond to a relevant email. Lastly, if a professional answered a past question from the student posting this question, then there is already a kind of \"connection\" between them. There's a good chance of that professional also answering this new question.\n"},{"metadata":{"_uuid":"04588e6e786db6f4853d49714c63300c2a881281"},"cell_type":"markdown","source":"## 8.1. Get a summary of how many professionals each model has selected\nThere could be duplicate professional id's here. We'll remove those duplicates later."},{"metadata":{"_uuid":"bdf606b488233140eac48aef997f5a8ddf31dc09","trusted":true},"cell_type":"code","source":"# Note that there could be duplicate professional id's\n# in these lists.\n\nprint('Model 1 Tags: ', len(model_1_list))\nprint('Model 2 Tfidf: ',len(model_2_list))\nprint('Model 3 TSVD: ',len(model_3_list))\nprint('Model 4 GloVe: ',len(model_4_list))\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"15ebf764b0fe12f387fbc092ad73c406c8d5aa2e"},"cell_type":"markdown","source":"## 8.2. What is the total number of professionals the models have selected?"},{"metadata":{"_uuid":"9229750f360f2c3a6fb8c603a7ce038c948be925","trusted":true},"cell_type":"code","source":"# Join all the lists\ncombined_list = model_1_list + model_2_list + model_3_list + model_4_list\n# Create a dataframe containing all professionals\ndf_selected = pd.DataFrame(combined_list, columns=['professionals_id'])\n\n# Drop any duplicate id's.\n# Because model 2 and model 3 select professionals based on answers, \n# there is a possibility that the same professional could be selected \n# multiple times bcause they gave several answers that matched the Question.\n\n# remove the duplicates\ndf_selected = df_selected.drop_duplicates('professionals_id')\n# get the total number of professionals\ntotal = len(df_selected)\n\nprint(total, 'professionals are able to answer the Question.')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a14bc6cb9b18cfe724443996e504749b3d5895f4"},"cell_type":"markdown","source":"## 8.3. Apply the final selection criteria"},{"metadata":{"_uuid":"882e899e2d44330ff68088d7a292420f3bad1622"},"cell_type":"markdown","source":"### ~ Has this professional joined CareerVillage within the last 30 days of the most recent new user who signed up?"},{"metadata":{"_uuid":"bddfa4d21e9642fd91b00535ce569805330679d4","trusted":true},"cell_type":"code","source":"\ndef new_member(x):\n    # get the value from df_professionals\n    num_days_member = df_professionals.loc[x, 'num_days_member']\n\n    if num_days_member <= 30:\n        return 1\n    else:\n        return 0\n\ndf_selected['new_member'] = df_selected['professionals_id'].apply(new_member)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2dca405154a8c5d008ae25c02f7901b6479d59f5"},"cell_type":"markdown","source":"### ~ Has this professional answered a past question from the student asking this question?"},{"metadata":{"_uuid":"0cf9d91765328b810775e3991ac6d6f86603b605","trusted":true},"cell_type":"code","source":"# Get the id of the student asking the question.\n# student_id variable was captured above.\n\ndef past_interaction(x):\n    # Filter out all the questions this professional has answered in the past\n    df_past = df_qa_prof[df_qa_prof['answers_author_id'] == x]\n\n    # Get a list of stuents who've asked the above questions\n    student_list = list(df_past['questions_author_id'])\n\n    # Check if the student asking this question is in student_list\n    if student_id in student_list:\n        return 1 # there was a past interaction\n    else:\n        return 0 # there has been no past interaction\n\n# create a new column that shows if there was a past interaction\ndf_selected['past_interaction'] = \\\ndf_selected['professionals_id'].apply(past_interaction)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f7cedff8ef30c0f300d3c2b702636ba86b58847b","trusted":true},"cell_type":"code","source":"df_selected.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bea18c715b8be608b684f18d9a11389949abddbe"},"cell_type":"markdown","source":"### ~ Has this professional answered a question within 30 days of the most recent answer posted on CareerVillage?"},{"metadata":{"_uuid":"200370bcc5a1bd961455f7ed089a74f03086b9f0","trusted":true},"cell_type":"code","source":"# Has this professional answered a question within\n# 30 days of the most recent answer posted on CareerVillage?\n# Yes --> send email\n\n# convert the answers_date_added to pandas datetime\ndf_answers['answers_date_added'] = \\\npd.to_datetime(df_answers['answers_date_added'])\n\n# get the date of the most recent answer\nnewest_answer_date = df_answers['answers_date_added'].max()\n\n# Get the number of days a question was answered from the most recent answer posted\n# on CareerVillage.\n\ndef days_from_newest_answer(x):\n    \n    num_days = (newest_answer_date - x).days\n    \n    return num_days\n\n# create a new column\ndf_answers['days_from_newest_answer'] = \\\ndf_answers['answers_date_added'].apply(days_from_newest_answer)\n\n# filter out all rows where days_from_newest_answer <= 30\ndf_filtered = df_answers[df_answers['days_from_newest_answer'] <= 30]\n\n# Drop duplicate professional id's because some professionals\n# may have abswered multiple questions in that time period.\ndf_filtered = df_filtered.drop_duplicates('answers_author_id')\n\n# get a list of professionals that made these recent answers\nprof_list = list(df_filtered['answers_author_id'])\n\n\ndef recent_answer(x):\n    if x in prof_list:\n        return 1\n    else:\n        return 0\n\n# create a new column\ndf_selected['recent_answer'] = \\\ndf_selected['professionals_id'].apply(recent_answer)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2fd61f0b21cf349d66a6216229d16f0fe07c2af5"},"cell_type":"markdown","source":"### ~ Has this professional made a comment within 30 days of the most recent answer posted on CareerVillage?"},{"metadata":{"_uuid":"5aab3a56be874184e4340858884645572672ca9c","trusted":true},"cell_type":"code","source":"# Has this professional made a comment within\n# 30 days of the most recent answer posted on CareerVillage?\n# Yes --> send email\n\n# convert the answers_date_added to pandas datetime\ndf_comments['comments_date_added'] = pd.to_datetime(df_comments['comments_date_added'])\n\n# Get the number of days a question was answered from the most recent answer posted\n# on CareerVillage.\n\ndef days_from_newest_answer(x):\n    \n    num_days = (newest_answer_date - x).days\n    \n    return num_days\n\n# create a new column\ndf_comments['days_from_newest_answer'] = \\\ndf_comments['comments_date_added'].apply(days_from_newest_answer)\n\n# filter out all rows where days_from_newest_answer <= 30\ndf_filtered = df_comments[df_comments['days_from_newest_answer'] <= 30]\n\n# Drop duplicate professional id's because some professionals\n# may have made multiple comments in that time period.\ndf_filtered = df_filtered.drop_duplicates('comments_author_id')\n\n# get a list of professionals that made these recent comments\nprof_list = list(df_filtered['comments_author_id'])\n\n# add a new column to df_selected\ndef recent_comment(x):\n    if x in prof_list:\n        return 1\n    else:\n        return 0\n\ndf_selected['recent_comment'] = df_selected['professionals_id'].apply(recent_comment)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fc216c8f4e69aaa542f2ea7165ed371356832743"},"cell_type":"markdown","source":"<hr>\n| <a id='Final_Output'></a>"},{"metadata":{"_uuid":"98e5539eb2a6e6f857745f4e8871958ed02eb008"},"cell_type":"markdown","source":"## 9. Final Output - The chosen ones\n\n### ~ Filter out those professionals who've met at least one of the selection critera ~"},{"metadata":{"_kg_hide-input":true,"_uuid":"366d1706f5096d2f87bfcac96999666496de05fc","trusted":true},"cell_type":"code","source":"# sum up the row scores for each professional in df_selected\ndef sum_rows(row):\n    \n    total = row['new_member'] + row['recent_answer'] + \\\n    row['recent_comment'] + row['past_interaction']\n    \n    return total\n    \ndf_selected['total_score'] = df_selected.apply(sum_rows, axis=1)\n\n\n# filter out rows where the score > 0\ndf_send_email = df_selected[df_selected['total_score'] > 0]\n\n\n\nfinal_selection_list = list(df_send_email['professionals_id'])\n\nnum_selected = len(final_selection_list)\n\nprint('=== Final Results ===\\n')\n\nprint(num_selected, 'professionals are likely to respond to the email.')\n\n#print('These are their names:\\n', final_selection_list)\n\nprint('These are their scores.')\n\n# Print the list of professionals that have a high likelihood of\n# responding to an email notification\ndf_send_email.head(20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e7621ff9525d42d17ea45a34a05560cbb0a1f8a9"},"cell_type":"markdown","source":"### Show which model selected each professional\nThese professional id's will help you to go back to a specific model's output and find a profile. There could be duplicates here because the same professional could have been selected by more than one model."},{"metadata":{"_uuid":"deb18ce3dad01ca5d6eb08ad9d73edf52ed446c5","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"print('This shows which model selected each chosen professional:\\n')\n\nfor prof_id in final_selection_list:\n    if prof_id in model_1_list:\n\n        print('Model 1 Tags: ', prof_id)\n\nfor prof_id in final_selection_list:\n    if prof_id in model_2_list:\n\n        print('Model 2 Tfidf: ', prof_id)\n        \nfor prof_id in final_selection_list:\n    if prof_id in model_3_list:\n\n        print('Model 3 TSVD: ', prof_id)\n\nfor prof_id in final_selection_list:\n    if prof_id in model_4_list:\n\n        print('Model 4 GloVe: ', prof_id)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a5d3cb24be55fdff94108d691402a3eda1ad7bd3","trusted":true},"cell_type":"code","source":"# End of Recommender System\n#====================================================================#","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"49db77e9434b0c6eaed33a5e9c09a64e97681853","trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a7bc4fc54a6153bf773123f049aff72f72b4ee83","trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"10a0fe9551e55565631b188ff9f1d4bb9dba5751"},"cell_type":"markdown","source":"| <a id='Testing'></a>"},{"metadata":{"_uuid":"672dc4acde2df3b59c01266922009ef1f1dbc30b"},"cell_type":"markdown","source":"## 10. Testing and Results"},{"metadata":{"_uuid":"c31c3f983f0d7cbd80d58c493dd65b2908916fec"},"cell_type":"markdown","source":"I used nine questions for testing and tuning. I tried to choose questions that reflect the strengths, limitations and quirks of this data. These include: \n\n1. Questions relating to careers that are well represented - e.g. computer science\n2. Questions relating to careers that have a low representation - firefighting, plumbing\n3. General questions that are more life-skills related\n \nThe test results for each model and the filter are summarised in a dataframe below.\n\nThese are the questions that I used for testing:\n\n**Index:** 777<br>\n**Question id**:  11ce7c537cd84db0bd7840ad3ca04004<br>\n**Question Title**:  I want to major in computer science. What classes should I take?<br>\n**Question Body**:<br>\n I want to do something like cyber security or write code for companys. #computer-science #programming #computer-engineering #computer-software \n \n \n**Index**: 999<br>\n**Question id**:  0601a843065945ac86e473f421774952<br>\n**Question Title**:  What is required to become a firefighter?<br>\n**Question Body**:<br>\nI want to know because so I can get the things I need to become a firefighter. #fireman \n\n\n**Index**: 2043<br>\n**Question id**:  8b469efa88284afb907179e4c73a99af<br>\n**Question Title**:  What exactly, is the difference between a psychologist and psychiatrist?<br>\n**Question Body**:<br>\ni dont know whether i want to be a psychologist or psychiatrist. I want to work with married people, going through a divorce. Do both of them work with people who are going through a divorce? #psychology #psychiatry\n\n\n**Index**: 2487<br>\n**Question id**:  1aaa4249d4ea41a4b2d196313e4e930e<br>\n**Question Title**:  How do I decide what career I want to choose?<br>\n**Question Body**:<br>\nI am finding it very difficult to decide what I want to spend the rest of my life doing, and I would like to know what process others took to find their paths who were as lost as me. #undecided #unsure #searching\n\n**Index**: 3618<br>\n**Question id**:  e14626d53e5d44ac98e4e1c57404aa9d<br>\n**Question Title**:  What are the best ways to maintain a work and school balance?<br>\n**Question Body**:<br>\nI think many people struggle with this and any type of advice towards this question is valuable.  #business #leadership #organization\n\n**Index**: 1710<br>\n**Question id**:  eb80205482e4424cad8f16bc25aa2d9c<br>\n**Question Title**:  I want to become an army officer. What can I do to become an army officer?<br>\n**Question Body**:<br>\nI am Priyanka from Bangalore . Now am in 10th std . When I go to college I should not get confused on what I want to take to become army officer. So I am asking this question  #military #army\n\n\n**Index**: 1000<br>\n**Question id**:  a5dfa070a89c47c28460557b1f4dabb8<br>\n**Question Title**:  What are the challenges I may face pursuing a career in science, and how can I stand out among the rest?<br>\n**Question Body**:<br>\nFor my first two years in high school I have been studying biotechnology and I have been working diligently in preparing myself for college and the workforce. Though I have not decided on an exact career, I am deeply considering one within forensics, pharmaceuticals, or in the space exploration fields.  What are some of the challenges students may face when applying for college, throughout college, and in the workforce? How can I stand out from the rest? #high-school #student #biotechnology #workforce #science #career #forensics #medicine #pharmaceuticals #space-exploration #future #technology #stem #steam #nasa #astrophysics #planetary-science #women-in-stem\n\n\n**Index**: custom question<br>\n**Question id**:  custom question<br>\n**Question Title**:  How do I become a data scientist?<br>\n**Question Body**:<br>\nI want to be a data scientist. What subjects should I study? #data-science\n\n**Index**: custom question<br>\n**Question id**:  custom question<br>\n**Question Title**:  How do I become a plumber?<br>\n**Question Body**:<br>\nI want to be a plumber. What subjects should I study? #plumber #plumbing\n\n"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"import pandas as pd\n# The next two lines causes all the text to appear. Sentences are not truncated.\n# All columns and all rows are displayed. Nothing is hidden.\n# Note: this must be in the same cell as import pandas as pd\npd.set_option('display.max_colwidth', -1)\npd.set_option('display.max_columns', None) \npd.set_option('display.max_rows', None)\n\n\nresults_dict = {\n'question_index': [777,999,2043,2487,3618,1710,1000,'custom question','custom question'],\n'question_title': ['I want to major in computer science. What classes should I take?',\n                 'What is required to become a firefighter?',\n                 'What exactly, is the difference between a psychologist and psychiatrist?',\n                 'How do I decide what career I want to choose?',\n                 'What are the best ways to maintain a work and school balance?',\n                 'I want to become an army officer. What can I do to become an army officer?',\n                 'What are the challenges I may face pursuing a career in science, and how can I stand out among the rest?',\n                 'How do I become a data scientist?', 'How do I become a plumber?'],\n'Model_1_Tags': ['rec: 481 fp: 0','rec: 2 fp: 0','rec: 1 fp: 1','rec: 0 fp: 0','rec: 1 fp: 1','rec: 21 fp: 2','rec: 0 fp: 0','rec: 105 fp: 1','rec: 4 fp: 0'],\n'Model_2_Tfidf': ['rec: 392 fp: 12','rec: 10 fp: 1','rec: 9 fp: 0','rec: 2 fp: 1','rec: 1 fp: 1','rec: 32 fp: 0','rec: 0 fp: 0','rec: 99 fp: 10','rec: 4 fp: 4'],\n'Model_3_TSVD': ['rec: 140 fp: 1','rec: 0 fp: 0','rec: 0 fp: 0','rec: 6 fp: 1','rec: 0 fp: 0','rec: 46 fp: 0','rec: 0 fp: 0','rec: 108 fp: 8','rec: 7 fp: 7'],\n'Model_4_GloVe': ['rec: 53 fp: 0','rec: 0 fp: 0','rec: 0 fp: 0','rec: 34 fp: 4','rec: 1588 fp: 0','rec: 0 fp: 0','rec: 26 fp: 0','rec: 0 fp: 0','rec: 0 fp: 0'],\n'Final_Filter_Output': ['rec: 29 fp: 1','rec: 2 fp: 1','rec: 1 fp: 1','rec: 2 fp: 0','rec: 54 fp: 0','rec: 9 fp: 0','rec: 3 fp: 0','rec: 14 fp: 1','rec: 2 fp: 1']\n  \n}\n\n    \ndf_results = pd.DataFrame(results_dict)\n\n#df_results.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Results"},{"metadata":{},"cell_type":"markdown","source":"In each cell there are two numbers. One shows the number of recommendations, the other shows the number of false positives (bad matches) in those recommendations. To determine the number of false positives I looked at the output of each model and counted how many recommended professionals I believed were not well matched to the question. This is a subjective way of assessing performance, but it gives us a rough idea of how each model is performing.\n\nFor example, Model_3_TSVD recommended 140 professionals to answer the question: \"I want to major in computer science. What classes should I take?\" There was one false positive in the output. In the dataframe the results are displayed like this - rec: 140 fp: 1\nAfter filtering, the system concluded that 29 professionals are likely to respond to the email. These final recommendations contain one false positive - rec: 29 fp: 1\n\n\nThese are the cosine similarity thresholds for each model:\n\nMODEL_1_THRESHOLD = 0.13<br>\nMODEL_2_THRESHOLD = 0.1<br>\nMODEL_3_THRESHOLD = 0.65<br>\nMODEL_4_THRESHOLD = 0.94"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_results.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<hr>\n\n**Observations**<br>\n\nThis system is performing well on careers for which a lot of data is available. Examples include the computer science, military and data science questions. \n\nFor questions about firefighting, psychology and plumbing the number of final recommendations is low. The models are also generating a higher ratio of false positives. There are professionals in the dataset that are qualified to answer these questions and the system is detecting them. But, the final filter is rejecting these professionals because they are not active. Therefore, I believe that the problem is not with this recommendation system. There's just not enough data representing these careers.\n\nStill, this highlights a weakness of this recommender system: For certain questions, especially those where a career has a low representation in the data, model 4 has a tendency to generate many false positives. To see a demonstration of this try entering index 2018 (question id:  b53c5e9b7436453fa13a416a23b512cc). This is a question about becoming a Politician. \n\nFor the more general/life-skills realted questions...\n\n~ How do I decide what career I want to choose?<br>\n~ What are the challenges I may face pursuing a career in science, and how can I stand out among the rest?<br>\n~ What are the best ways to maintain a work and school balance?\n\n...Model 4 (GloVe) is performing well. It's nicely making up for the weakness that the other three models are showing. \n\n**Is this a good recommender system?**<br>\n\nBased on these tests my preliminary conclusion is that this recommender system works. However, one should be careful about drawing conclusions after only a quick test like this. This system needs to be field tested. Only then will we know how robust it really is.\n\n**Now just a few thoughts on setting thresholds...**\n\nI've tried to reduce the number of irrelevant recommendations (i.e. bad match between question and professional) by setting model 3 and model 4 thresholds to be quite high. Reducing the number of false positives is important to inspire confidence in this system and reduce the amount of irrelevant questions sent to professionals. \n\nAs an example, consider the question: \n\"I want to become an army officer. What can I do to become an army officer?\"\n\nAt the current threshold of 0.94 Model 4 is generating 0 recommendations. However, if this threshold were lowered to 0.85 Model 4 would generate 305 recommendations. This would cause an increase in the number of false positives. In addition to recommending more professionals with a military background, Model 4 would also recommend police officers.\n\nMoreover, if the threshold was set at 0.85 then model 4 would generate 38,220 recommendations when given the very general question: \"What are the best ways to maintain a work and school balance?\" \n\n38,220 (contains duplicates) is a lot of professionals and one might guess that many false positives would be generated... but think again - would you or I consider such a question irrelevant?\n\nThat said, when choosing these thresholds it's important to consider **business priorities**. One of CareerVillage's priorities is 'No question left behind' meaning that no question should be left unanswered. In this case an executive decision could be made to use lower thresholds. The benefit would be that there would be a higher possibility that a question would get an answer because the system would output a longer list of recommended professionals. The risk is that the number of irrelevant questions sent to professionals will increase. One way to manage this risk would be to warn professionals ahead of time that a new system is being tested and request their patience (and possibly their feedback) as the system is being tuned. \n\nAnother point to consider is that these thresholds are dictated by the amount and quality of the data. As the amount of data increases, these thresholds could be adjusted. This system is not static. Its ongoing performance will need to be monitored. With time it could get better as data quantity increases or it could get worse if the data becomes polluted, with spam for example.\n"},{"metadata":{},"cell_type":"markdown","source":"<hr>\n| <a id='Things'></a>"},{"metadata":{},"cell_type":"markdown","source":"## 11. Things to keep in mind"},{"metadata":{},"cell_type":"markdown","source":"**Domain knowledge is like the force, use it...**\n\nIf you are a domain expert i.e you work for CareerVillage or you're an experienced career counselor or child psychologist, then you may be able to use the practical insights you have to improve this solution. \n\nI suggest starting by looking at the filter. Are the conditions too strict? Are there better conditions that can be added?<br>\nYou can then try tuning the threshold values or tuning other model parameters to see if the quality and quantity of the recommendations improve. Also, this system is set up so that the models work independantly. Therefore, it's possible to experiment by including and excluding certain models.\n\n**Be careful...**\n\nHowever, as mentioned above, please be careful when lowering the threshold value for model 4 (GloVe). If a question is very general then this model may cast a wide net and recommend thousands of professionals. This could crash the system.\n\n**What steps can be taken to address the cold start problem?**\n\nSay you've just signed up as a shopper on Amazon. The site won't know the most relevant products to recommend to you because you've never bought anything i.e. you don't have a shopping history. This is called the cold start problem. \n\nHere the new professionals are the \"shoppers\" and the \"products\" are the questions.\n\nIn this recommender system Model 1 gives every professional a chance to be paired with a question. It relies on tags and professional profiles. \n\n(Reminder: A Professional's profile includes a professional's industry and title.)\n\nTherefore, one way of tackling the cold start problem is to encourage new professionals to complete their tag and profile information. There are many professionals whose info is incomplete or sparse. New professionals will have a better chance of being matched if they provide complete and detailed information about themselves. \n\nAnother way of addressing the cold start problem is to simply do nothing. Professionals are currently able to scroll the forums to find questions to answer. With time cold start issues will resolve themselves as these professionals find and answer questions. Once they answer a question Model 2, 3 or 4 will automatically match them to more questions.\n\n**Fairness**\n\nOne of the requirements for fairness is a diverse dataset. This isn't always easy to create for many reasons, one being that the perspective of those creating it may be limited. Let me explain using a professions vs trades example. (Please note that the term bias here means one-sidedness. Not bias in the sense of bias/variance.)\n\nSociety tends to esteem professions above trades. Those of us who've reaped the social and financial rewards of a university education would naturally want to encourage all children to follow this path. But not all children want to be doctors. Many can find secure, fullfilling and often lucrative careers as plumbers and electricians. Not everyone knows that such opportunities exist outside the university system.\n\nIf CareerVillage's marketing team were to target a predominantly white-collar demographic when trying to attract new professionals - then the number of university educated professionals in the dataset will be higher, and growing faster, than the number of tradesmen or tradeswomen. This will lead to a data bias in favour of professions. Any alogorithms constructed using this data will reflect this bias. The consequence - students who submit questions about learning a trade won't get answers. This would be an \"exculsionary user experience\", also known as discrimination. All because the data is not diverse.\n\nHere's an enlightening TED talk on algorithmic bias:<br>\nhttps://www.ted.com/talks/joy_buolamwini_how_i_m_fighting_bias_in_algorithms?language=en#t-232473\n\n"},{"metadata":{},"cell_type":"markdown","source":"<hr>\n| <a id='Ideas'></a>"},{"metadata":{},"cell_type":"markdown","source":"## 12. Ideas for sharpening this system"},{"metadata":{},"cell_type":"markdown","source":"**Include a Stoplist**<br>\n\nA stoplist is a pre-defined list of root words. It can be used to blacklist professionals who've given answers or have profiles that contain words that are in the stoplist. The recommender system shouldn't know that these professionals exist.\n\nWhy? CareerVillage meets a very real need. It's certain to become popular with more students and professionals, in more countries. Unfortunately this increase in popularity will attract both the good and the bad - human and bot. It's important to build a stoplist into the recommender system that will exclude certain professionals from the final output when that person is promoting an agenda that's contrary to the CareerVillage value system.\n\n**Train a Neural Network to automatically rate the quality of answers**<br>\n\nThe CareerVillage vision is to give students answers that are tailored, reliable, encouraging and inspirational. There are many answers in the dataset that meet these requirements. Unfortunately, there are also answers that don't. Being able to automatically rate the quality of answers may help improve them.\n\nHow might this be done?<br>\n\nThe main pre-requisite is a labeled dataset. Once this is in place one could train a deep neural network to read each answer and then rate it on a scale of 1 to 10 according to the CareerVillage [Pro Tips](https://medium.com/@careervillage/introducing-protips-2d4ad51c445a) guidelines. This neural network could have one of several architectures - DNN, CNN, RNN or even BERT, which is a state of the art pre-trained network created by Google. \n\nHowever, factors such as memory requirements, training time, inference time, web page load time, maintenance complexity and hosting costs will need to be considered when deciding if this idea is feasible.\n\n\n**Add \"Recent Login\" as a criteria for selecting professionals**\n\nIf a professional visited CareerVillage recently then he or she was probably scrolling through the forums reading the questions and answers. There's a good chance that this person will respond to an email notification to answer a relevant question. Login information is not part of this dataset. The following condition should be added to the filter:<br>\n\"Has this professional visited the site in the last 14 days?\"\n\n**Encourage all professionals to complete their profile information**\n\nThere are many professionals who have incomplete or sparse profile information. This recommender system relies on profile information. If more professionals provide complete profiles then the performance of this system will improve.\n\n**Add 'About me' and 'Career Stories' to the profile information**\n\nOn some professional's CareerVillage profile page there are sections called \"About me\" and \"Career Stories\". In these sections they share what their career path has been, life experiences, past mistakes, what they do on a typical work day and other useful personal information and experiences. Here's a good example:<br>\nhttps://www.careervillage.org/users/9852/kim/\n\nThis data is not included in this dataset, possibly because only a few professionals choose to share this information. \"About me\" and \"Career Stories\" could be a valuable data source for this recommender system. Including them will also help address the cold start problem."},{"metadata":{"_uuid":"7b38e6827334b7656ee2b993cbee1d904d652ef1"},"cell_type":"markdown","source":"| <a id='Citations'></a>"},{"metadata":{"_uuid":"5f3601f71dc5b71ad92059592732f30f5a5ca88d"},"cell_type":"markdown","source":"## Citations"},{"metadata":{"_uuid":"0ab3e13673ae75a2719de6135c867236a558f897"},"cell_type":"markdown","source":"1. GloVe: [Global Vectors for Word Representation](https://nlp.stanford.edu/pubs/glove.pdf)<br>\nJeffrey Pennington, Richard Socher, and Christopher D. Manning. 2014. \n\n2. Photo by jeshoots.com on Pixabay"},{"metadata":{"_uuid":"1df95d48e345af8d7078eab2647bd0c0817aa99c"},"cell_type":"markdown","source":"| <a id='Reference_Kernels'></a>"},{"metadata":{"_uuid":"800f28393826cd75b4d4f2ee1841cf2ef7cb4446"},"cell_type":"markdown","source":"## Reference Kernels"},{"metadata":{"_uuid":"376e47024bee1c06ea9f4286c27db8cfaeb975de"},"cell_type":"markdown","source":"1. Rounak Banik - Movie Recommender Systems<br>\nhttps://www.kaggle.com/rounakbanik/movie-recommender-systems\n\n2. Chris Crawford - Starter kernel<br>\nhttps://www.kaggle.com/crawford/starter-kernel\n\n3. wjsheng - UPDATE 5: text processing<br>\nhttps://www.kaggle.com/wjshenggggg/update-5-text-processing\n\n4. RodH - Recommender: things to consider<br>\nhttps://www.kaggle.com/rdhnw1/recommender-things-to-consider\n\n5. Marsh - Keras cnn + GloVe + Early Stopping<br>\nhttps://www.kaggle.com/vbookshelf/keras-cnn-glove-early-stopping-0-048-lb\n"},{"metadata":{"_uuid":"0fdb534100e8a8da5ddd9e7e4bb1629f43be23ba"},"cell_type":"markdown","source":"| <a id='Helpful_Resources'></a>"},{"metadata":{"_uuid":"829ac8895a3393eac27440ac5b36f7492a1838ed"},"cell_type":"markdown","source":"## Helpful Resources"},{"metadata":{"_uuid":"084fd5fd87fb871257caeb8aaae7c31904043b26"},"cell_type":"markdown","source":"1. Frank Kane course on Building Recommender Systems<br>\nhttps://www.udemy.com/building-recommender-systems-with-machine-learning-and-ai\n\n2. Andrew Ng Deep Learning Specialization, Sequence Models, Week 2<br>\nhttps://www.coursera.org/learn/nlp-sequence-models\n\n3. What are word embeddings?<br>\nhttps://www.youtube.com/watch?v=Eku_pbZ3-Mw\n\n4. Blog post with a simple example explaining how to use pre trained embeddings:<br>\nhttps://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/\n\n5. Machine learning with text<br>\nhttps://www.youtube.com/watch?v=ZiKMIuYidY0\n\n6. NLTK Tutorial series<br>\nhttps://pythonprogramming.net/tokenizing-words-sentences-nltk-tutorial/\n\n7. Blog post by Kaggle Grandmaster Abhishek Thakur<br>\nhttp://blog.kaggle.com/2016/07/21/approaching-almost-any-machine-learning-problem-abhishek-thakur/\n\n8. Tutorial on merging dataframes<br>\nhttps://www.youtube.com/watch?v=h4hOPGo4UVU\n\n9. Blog post by William Zinsser<br>\nhttps://theamericanscholar.org/writing-english-as-a-second-language/#.XJ8oJhMzYWo\n\n"},{"metadata":{"_uuid":"a5f3c5b24ec432d0c68f737edebdcd6cb8915150"},"cell_type":"markdown","source":"<hr>\n\n| <a id='Conclusion'></a>"},{"metadata":{"_uuid":"be9545192b679b19a18d2c1bfb7ef594fe7e2f35"},"cell_type":"markdown","source":"## Conclusion\n\nIkigai is a formula for happiness and fulfillment. It's a Japanese word that roughly means \"a reason for being\" or \"the reason you wake up in the morning\". It's the area of intersection of four overlapping circles: what you love to do, what you're good at, what you get paid to do and what the world needs.\n\nThank you CareerVillage and Kaggle for hosting this challenging competition."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.0"}},"nbformat":4,"nbformat_minor":1}