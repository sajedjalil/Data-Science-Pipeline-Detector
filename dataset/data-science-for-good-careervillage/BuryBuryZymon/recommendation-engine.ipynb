{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Our Approach\nIn this data cience for good event, we are asked to develop a recommendation engine that recommed questions asked on the platform to the professionals who are most likeluy to answer them. CareerVillage is a non-profit online platform where people can give advice and seek advice on topics related to career, study, jobs, and schools. CareerVillage has more than 3.5M online learner and as the ratio of number of counselor to number of student seeking counseling is very low, CareerVillage challenges the data science community on Kaggle to develop a recommendation enggine which can help them serve the students better. For this task CareerVillage has shared last five years data from there data base that contains data about students demographics, professional demographics, questions, answers, comments, email campaign data, hashtags etc. CareerVillage already has 25000+ professionals reggistered on their platform and we can provide them a list of professionals who are most likely to answer their question, it will lower the time to get the answer thus it will help students, targeted emailing will ensure CareerVillage's resources are utilized at optimum level. CV has also provided us a few guidelines for this recommender engine - \n\n- **Performance** - It should perform well at matching questions with potential professional to get the answers for these questions.\n- **Easy to implement** - They want to deploy the winning solutions quickly so the winning solution should be easy to implement.\n- **Extensibility** - They would like to further develop it and add more features - the codes should be such that they allow these augmentations in future.\n\n## We will approach the problem in two parts \nThis problem is different than standard recommendation engine (like - movie recommender) as here we are not going to recommend past questions to professional and a new question will always different than past qustions. So, what we will do here is we find the similar questions using NLP and then use these similar questions to find who would be most likely to answer them based on previous answers to these similar qustions.\n#### Part 1 - Find questions similar to newly asked question \nAs a first part of the problem, we will using NLP extensively to pre-process the data and use sophesticated techniques like LDA and sentence2vec models for finding the n -most similar qustions in our database to newly asked question. In the pre-processing step - we use tokenize, lemmatization, stemming, remove stopwords, clean numbers, clean contractions, extract and generate new hashtags to prepare data which can be used for modeling. Whenever a question is asked on the platform it has three parts - question title, question body and hashtags. We train the Sentence2Vec model on question title and hashtags to create title vector and tags vector and question body text to train LDA model. Later we combine results from both these models to generate list of similar questions to newly asked one.\n#### Part 2 - Create recommender engine for personalized recommendations \nOnce we get the similar questions to newly asked question, we use collaborative filtering for doing personlized recommendations. Collaborative filtering one of the most sophesticated ways to do the personalized recommendations. This method does filtering about the interests of professionals by collecting preferences or taste information from previous questions. We check the performance for this models using the standard matrices, recall@k, precision@k and success@k emails."},{"metadata":{},"cell_type":"markdown","source":"## Why CareerVillage should use this engine (10 Reasons why! ) - \nThere are many reasons why CareerVillage shoould use this engine -\n\n| S. No. | <center>Reason</center>      |   <center>Comment (explanation)</center>       |\n| :---------- | :----------------------------------- | :-------------------------- |\n|1. |  <center>**Performance**</center>   | <center>This engine is giving very respected recall, precision, and success rate on validation data  **success rates > 34.5%**. This means if for emails sent to professionals, based on retrospective testing 34.5% of those questions have received the answers within 2 months </center> |\n|2. | <center>**Extensible - Production level codes**</center>      |<center>All the codes of this series has codes written in modular fashion and follows proper **PEP8 standards of coding** for production level implementation and functions are modular in nature. Which enables CV to use them directly in production and making further development easy</center>|\n|3. | <center>**Intelligible - easy to understand**</center> | <center>The codes written are very easy to understand, and we have added infographics to explain the complete process of modeling to susiness people. Explanation is written in simplistic manner and all our models are explained in way if we were explaining it to business personals   </center> |\n|4. | <center>**Optimized implimentation**</center> | <center>The functions that are used in for calculation are written in very optimized manner and it uses multiprocessing where possible - which enables CV to use these models even after few years when the quantity of data is increased manifold**.</center>|\n|5. |  <center>**State of the art models**</center> | <center>This analysis uses state of the art models like Sentence2Vec using stanford gove embeddings, LDA models for topic modeling and collaborative filtering. </center>|\n|6. | <center>**Exceptions handling**</center> | <center>The codes written with exceptions keeping in mind and the codes **handles exception well** </center>|\n|7. | <center>**Saves log files while running**</center> | <center>It saves **logs in a txt file** whenever a main function (function doing filtering is run) which can be refered to whenever anything in the codes breaks in future</center> |\n|8. | <center>**Profiling**</center> | <center>These kernels uses **time module to check** if a portion of code is taking very long, this way it profiles the analysis done</center> |\n|9. | <center>**Documentation**</center> | <center>These kernels has proper documentation of what each block is doing, **functions has doc-strings explaning input output and use of functions**. </center>|\n|10. | <center>**Powerful visuals**</center> | <center>This analysis contains  a few but very powerful eye pleasing visuals which unfolds the information hidden in the data in very simple manner</center>|"},{"metadata":{},"cell_type":"markdown","source":"![](http://lh3.googleusercontent.com/-d3bV_PNLns4/XL70lhLQPxI/AAAAAAAA0eo/Jm0SCD-4nhwJm-jm_JAZQ6ws9rtOsb4fwCK8BGAs/s0/2019-04-23.png )"},{"metadata":{},"cell_type":"markdown","source":"![](https://lh3.googleusercontent.com/-1akp6hcgPDU/XL8-C_4q9eI/AAAAAAAA0fg/7oNAnZZtquE6eBmzwM3PkLLkpdq6oVpAgCK8BGAs/s0/2019-04-23.png)"},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn \nimport os \nimport time\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom nltk.tokenize import word_tokenize, sent_tokenize, TweetTokenizer \nfrom nltk.stem import WordNetLemmatizer, PorterStemmer\nfrom nltk.corpus import stopwords\nfrom string import punctuation\nimport re\nfrom functools import reduce\nimport gensim\nimport gensim\nimport os\nimport collections\nimport smart_open\nimport random\nimport nltk\nfrom numpy import linspace\nfrom scipy.stats.kde import gaussian_kde\nfrom bokeh.io import output_file, show\nfrom bokeh.models import ColumnDataSource, FixedTicker, PrintfTickFormatter\nfrom bokeh.plotting import figure\nfrom bokeh.sampledata.perceptions import probly\nimport colorcet as cc\nimport folium\nimport time\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.tools as tls\nfrom wordcloud import WordCloud\nfrom nltk.corpus import stopwords\nimport nltk\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\n#output_notebook()\nfrom plotly.graph_objs import *\nimport scipy.stats as st\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nfrom pprint import pprint\nfrom gensim import corpora, models\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport scipy\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\npy.init_notebook_mode()\nimport plotly\nplotly.__version__\nimport pandas as pd\nimport numpy as np\nimport plotly.plotly as py\nplotly.offline.init_notebook_mode()\nimport plotly.plotly as py\nfrom plotly import tools\nfrom plotly.offline import iplot, init_notebook_mode\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom scipy.sparse.linalg import svds\ninit_notebook_mode()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"puncts = ['☹', 'Ź', 'Ż', 'ἰ', 'ή', 'Š', '＞', 'ξ', 'ฉ', 'ั', 'น', 'จ', 'ะ', 'ท', 'ำ', 'ใ', 'ห', '้', 'ด', 'ี', '่', 'ส', 'ุ', 'Π', 'प', 'ऊ', 'Ö', 'خ', 'ب', 'ஜ', 'ோ', 'ட', '「', 'ẽ', '½', '△', 'É', 'ķ', 'ï', '¿', 'ł', '북', '한', '¼', '∆', '≥', '⇒', '¬', '∨', 'č', 'š', '∫', 'ḥ', 'ā', 'ī', 'Ñ', 'à', '▾', 'Ω', '＾', 'ý', 'µ', '?', '!', '.', ',', '\"', '#', '$', '%', '\\\\', \"'\", '(', ')', '*', '+', '-', '/', ':', ';', '<', '=', '>', '@', '[', ']', '^', '_', '`', '{', '|', '}', '~', '“', '”', '’', 'é', 'á', '′', '…', 'ɾ', '̃', 'ɖ', 'ö', '–', '‘', 'ऋ', 'ॠ', 'ऌ', 'ॡ', 'ò', 'è', 'ù', 'â', 'ğ', 'म', 'ि', 'ल', 'ग', 'ई', 'क', 'े', 'ज', 'ो', 'ठ', 'ं', 'ड', 'Ž', 'ž', 'ó', '®', 'ê', 'ạ', 'ệ', '°', 'ص', 'و', 'ر', 'ü', '²', '₹', 'ú', '√', 'α', '→', 'ū', '—', '£', 'ä', '️', 'ø', '´', '×', 'í', 'ō', 'π', '÷', 'ʿ', '€', 'ñ', 'ç', 'へ', 'の', 'と', 'も', '↑', '∞', 'ʻ', '℅'\n  'ι', '•', 'ì', '−', 'л', 'я', 'д', 'ل', 'ك', 'م', 'ق', 'ا', '∈', '∩', '⊆', 'ã', 'अ', 'न', 'ु', 'स', '्', 'व', 'ा', 'र', 'त', '§', '℃', 'θ', '±', '≤', 'उ', 'द', 'य', 'ब', 'ट', '͡', '͜', 'ʖ', '⁴', '™', 'ć', 'ô', 'с', 'п', 'и', 'б', 'о', 'г', '≠', '∂', 'आ', 'ह', 'भ', 'ी', '³', 'च', '...', '⌚', '⟨', '⟩', '∖', '˂', 'ⁿ', '⅔', 'న', 'ీ', 'క', 'ె', 'ం', 'ద', 'ు', 'ా', 'గ', 'ర', 'ి', 'చ', 'র', 'ড়', 'ঢ়', 'સ', 'ં', 'ઘ', 'ર', 'ા', 'જ', '્', 'ય', 'ε', 'ν', 'τ', 'σ', 'ş', 'ś', 'س', 'ت', 'ط', 'ي', 'ع', 'ة', 'د', 'Å', '☺', 'ℇ', '❤', '♨', '✌', 'ﬁ', 'て', '„', 'Ā', 'ត', 'ើ', 'ប', 'ង', '្', 'អ', 'ូ', 'ន', 'ម', 'ា', 'ធ', 'យ', 'វ', 'ី', 'ខ', 'ល', 'ះ', 'ដ', 'រ', 'ក', 'ឃ', 'ញ', 'ឯ', 'ស', 'ំ', 'ព', 'ិ', 'ៃ', 'ទ', 'គ', '¢', 'つ', 'や', 'ค', 'ณ', 'ก', 'ล', 'ง', 'อ', 'ไ', 'ร', 'į', 'ی', 'ю', 'ʌ', 'ʊ', 'י', 'ה', 'ו', 'ד', 'ת', 'ᠠ', 'ᡳ', 'ᠰ', 'ᠨ', 'ᡤ', 'ᡠ', 'ᡵ', 'ṭ', 'ế', 'ध', 'ड़', 'ß', '¸', 'ч', 'ễ', 'ộ', 'फ', 'μ', '⧼', '⧽', 'ম', 'হ', 'া', 'ব', 'ি', 'শ', '্', 'প', 'ত', 'ন', 'য়', 'স', 'চ', 'ছ', 'ে', 'ষ', 'য', '়', 'ট', 'উ', 'থ', 'ক', 'ῥ', 'ζ', 'ὤ', 'Ü', 'Δ', '내', '제', 'ʃ', 'ɸ', 'ợ', 'ĺ', 'º', 'ष', '♭', '़', '✅', '✓', 'ě', '∘', '¨', '″', 'İ', '⃗', '̂', 'æ', 'ɔ', '∑', '¾', 'Я', 'х', 'О', 'з', 'ف', 'ن', 'ḵ', 'Č', 'П', 'ь', 'В', 'Φ', 'ỵ', 'ɦ', 'ʏ', 'ɨ', 'ɛ', 'ʀ', 'ċ', 'օ', 'ʍ', 'ռ', 'ք', 'ʋ', '兰', 'ϵ', 'δ', 'Ľ', 'ɒ', 'î', 'Ἀ', 'χ', 'ῆ', 'ύ', 'ኤ', 'ል', 'ሮ', 'ኢ', 'የ', 'ኝ', 'ን', 'አ', 'ሁ', '≅', 'ϕ', '‑', 'ả', '￼', 'ֿ', 'か', 'く', 'れ', 'ő', '－', 'ș', 'ן', 'Γ', '∪', 'φ', 'ψ', '⊨', 'β', '∠', 'Ó', '«', '»', 'Í', 'க', 'வ', 'ா', 'ம', '≈', '⁰', '⁷', 'ấ', 'ũ', '눈', '치', 'ụ', 'å', '،', '＝', '（', '）', 'ə', 'ਨ', 'ਾ', 'ਮ', 'ੁ', '︠', '︡', 'ɑ', 'ː', 'λ', '∧', '∀', 'Ō', 'ㅜ', 'Ο', 'ς', 'ο', 'η', 'Σ', 'ण'\n]\ncontraction_mapping = {\n  \"Im\": \"I am\",\n  \"ain't\": \"is not\",\n  \"aren't\": \"are not\",\n  \"can't\": \"cannot\",\n  \"can't've\": \"cannot have\",\n  \"'cause\": \"because\",\n  \"could've\": \"could have\",\n  \"couldn't\": \"could not\",\n  \"couldn't've\": \"could not have\",\n  \"didn't\": \"did not\",\n  \"doesn't\": \"does not\",\n  \"don't\": \"do not\",\n  \"hadn't\": \"had not\",\n  \"hadn't've\": \"had not have\",\n  \"hasn't\": \"has not\",\n  \"haven't\": \"have not\",\n  \"he'd\": \"he would\",\n  \"he'd've\": \"he would have\",\n  \"he'll\": \"he will\",\n  \"he'll've\": \"he will have\",\n  \"he's\": \"he is\",\n  \"how'd\": \"how did\",\n  \"how'd'y\": \"how do you\",\n  \"how'll\": \"how will\",\n  \"how's\": \"how is\",\n  \"I'd\": \"I would\",\n  \"I'd've\": \"I would have\",\n  \"I'll\": \"I will\",\n  \"I'll've\": \"I will have\",\n  \"I'm\": \"I am\",\n  \"I've\": \"I have\",\n  \"i'd\": \"i would\",\n  \"i'd've\": \"i would have\",\n  \"i'll\": \"i will\",\n  \"i'll've\": \"i will have\",\n  \"i'm\": \"i am\",\n  \"i've\": \"i have\",\n  \"isn't\": \"is not\",\n  \"it'd\": \"it would\",\n  \"it'd've\": \"it would have\",\n  \"it'll\": \"it will\",\n  \"it'll've\": \"it will have\",\n  \"it's\": \"it is\",\n  \"let's\": \"let us\",\n  \"ma'am\": \"madam\",\n  \"mayn't\": \"may not\",\n  \"might've\": \"might have\",\n  \"mightn't\": \"might not\",\n  \"mightn't've\": \"might not have\",\n  \"must've\": \"must have\",\n  \"mustn't\": \"must not\",\n  \"mustn't've\": \"must not have\",\n  \"needn't\": \"need not\",\n  \"needn't've\": \"need not have\",\n  \"o'clock\": \"of the clock\",\n  \"oughtn't\": \"ought not\",\n  \"oughtn't've\": \"ought not have\",\n  \"shan't\": \"shall not\",\n  \"sha'n't\": \"shall not\",\n  \"shan't've\": \"shall not have\",\n  \"she'd\": \"she would\",\n  \"she'd've\": \"she would have\",\n  \"she'll\": \"she will\",\n  \"she'll've\": \"she will have\",\n  \"she's\": \"she is\",\n  \"should've\": \"should have\",\n  \"shouldn't\": \"should not\",\n  \"shouldn't've\": \"should not have\",\n  \"so've\": \"so have\",\n  \"so's\": \"so as\",\n  \"this's\": \"this is\",\n  \"that'd\": \"that would\",\n  \"that'd've\": \"that would have\",\n  \"that's\": \"that is\",\n  \"there'd\": \"there would\",\n  \"there'd've\": \"there would have\",\n  \"there's\": \"there is\",\n  \"here's\": \"here is\",\n  \"they'd\": \"they would\",\n  \"they'd've\": \"they would have\",\n  \"they'll\": \"they will\",\n  \"they'll've\": \"they will have\",\n  \"they're\": \"they are\",\n  \"they've\": \"they have\",\n  \"to've\": \"to have\",\n  \"wasn't\": \"was not\",\n  \"we'd\": \"we would\",\n  \"we'd've\": \"we would have\",\n  \"we'll\": \"we will\",\n  \"we'll've\": \"we will have\",\n  \"we're\": \"we are\",\n  \"we've\": \"we have\",\n  \"weren't\": \"were not\",\n  \"what'll\": \"what will\",\n  \"what'll've\": \"what will have\",\n  \"what're\": \"what are\",\n  \"what's\": \"what is\",\n  \"what've\": \"what have\",\n  \"when's\": \"when is\",\n  \"when've\": \"when have\",\n  \"where'd\": \"where did\",\n  \"where's\": \"where is\",\n  \"where've\": \"where have\",\n  \"who'll\": \"who will\",\n  \"who'll've\": \"who will have\",\n  \"who's\": \"who is\",\n  \"who've\": \"who have\",\n  \"why's\": \"why is\",\n  \"why've\": \"why have\",\n  \"will've\": \"will have\",\n  \"won't\": \"will not\",\n  \"won't've\": \"will not have\",\n  \"would've\": \"would have\",\n  \"wouldn't\": \"would not\",\n  \"wouldn't've\": \"would not have\",\n  \"y'all\": \"you all\",\n  \"y'all'd\": \"you all would\",\n  \"y'all'd've\": \"you all would have\",\n  \"y'all're\": \"you all are\",\n  \"y'all've\": \"you all have\",\n  \"you'd\": \"you would\",\n  \"you'd've\": \"you would have\",\n  \"you'll\": \"you will\",\n  \"you'll've\": \"you will have\",\n  \"you're\": \"you are\",\n  \"you've\": \"you have\"\n}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Natural Language Preprocessing \nAs we get the questions data here, we need to process the data before we can start building the model for finding similar questions. Here are steps that we use to pre-process the data - \n\n1. **Tokenize** - we tokenize the given text (q body, q title) to create the token or words present in the text string. eg. - **\"I am mahesh\"** will become **[\"I\", \"am\", \"Mahesh\"]** list after tokenization\n2. **Remove Stopwords** - We remove stop words from the tokenized text as these stop words doesn't add any value in the analysis - they are very generic.\n3. **Remove Punctuations** - We remove punctuations from the tokenized text.\n4. **lemmatization** - We then use lemmatization to remove inflectional ending and return the base words.\n5. **Stemming** - Stemming returns the root word. It is used in LDA model but not in Sentence2Vec model.\n6. **Clean numbers** - We remove unwanted numbers that doesn't add any contextual value in analysis.\n7. **Clean Contractions** - We clean all the contractions like **\"Im\"** to **\"I am\"**, **\"ain't\"** becomes **\"are not\"** etc\n8. **Extract and process hashtags** - We extract the hashtags from questions body and process them - like convert them to lowercase, remove unwanted characters so they their word vectors can be found using glove.\n9. **Generate extra hashtags** - We use tags TDM matrix on complete questions data to assign new possible hashtags which are not already mentioned in question body."},{"metadata":{"trusted":true},"cell_type":"code","source":"def tokenize(words):\n    \"\"\"Function to tokenize the words of a given string\n    :param words: string\n    :return: list of tokens\n    \"\"\"\n    return(word_tokenize(words))\n\n\ndef remove_stopwords(words):\n    \"\"\"\n    Function to remove stopwords from the question text\n    :param words: list of tokens\n    :return: list of tokens after stopwords removal\n    \"\"\"\n    stop_words = set(stopwords.words(\"english\"))\n    return [word for word in words if word not in stop_words]\n\n\ndef remove_punctuation(text):\n    \"\"\"\n    Function to remove punctuation from the question text\n    :param text: lists of tokens\n    :return: list of token without punctuations\n    \"\"\"\n    return re.sub(r'[^\\w\\s]', '', text)\n\n\ndef lemmatize_text(words):\n    \"\"\"\n    Function to lemmatize the question text\n    :param words: list of tokens \n    :return: list of token after lemmatization\n    \"\"\"\n    lemmatizer = WordNetLemmatizer()\n    return [lemmatizer.lemmatize(word) for word in words]\n\n\ndef stem_text(words):\n    \"\"\"\n    Function to stem the question text\n    :param words: lists of tokens\n    :return: list of tokens after stemming\n    \"\"\"\n    ps = PorterStemmer()\n    return [ps.stem(word) for word in words]\n\n\ndef clean_numbers(x):\n    \"\"\"Function to remove numbers from a list \n    :param x: list of tokens\n    :return: list of tokens without numbers\n    \"\"\"\n    x = re.sub('[0-9]{5,}', ' ##### ', x)\n    x = re.sub('[0-9]{4}', ' #### ', x)\n    x = re.sub('[0-9]{3}', ' ### ', x)\n    x = re.sub('[0-9]{2}', ' ## ', x)\n    return x\n\n\ndef clean_contractions(text, mapping):\n    \"\"\"\n    :param text: text\n    :param mapping: mapping dictionary\n    :return: \n    \"\"\"\n    specials = [\"’\", \"‘\", \"´\", \"`\"]\n    for s in specials:\n        text = text.replace(s, \"'\")\n    text = ' '.join([mapping[t] if t in mapping else t for t in text.split(\" \")])\n    return text\n\n\ndef extract_hashtags(row):\n    \"\"\"Function to extract hashtags from the question body\n    :param row: string\n    :return: list of hashtags words\n    \"\"\"\n    # row is a string here\n    row_split = re.split(r'\\s', row)\n    hashtags = [x for x in row_split if \"#\" in x]\n    hashtags_words = [x[1:] for x in hashtags]\n    return hashtags_words\n\n\ndef process_hashtags(row):\n    \"\"\"Function to process the hashtags and remove unwanted characters\n    :param row: list of words\n    :return: list of words after processing\n    \"\"\"\n    row_processed = []\n    for i in row:\n        row_processed.extend(re.split(\"[, \\-_:]+\", i))\n    return row_processed\n\n\ndef create_TDM(df):\n    \"\"\"Function to create TDM - This will be hashtag corpus from given data\n    :param df: df containing hashtags columns as lists\n    :return: dict with words as keys and their frequencies\n    \"\"\"\n    h_list = []\n    for i in list(range(df.shape[0])):\n        h_list.extend(df.loc[i, \"Hashtags\"])\n\n    # Making dict as TDM\n    h_count = dict()\n    for i in h_list:\n        h_count[i.lower()] = h_count.get(i.lower(), 0) + 1\n    return h_count\n\n\ndef suggest_extra_hashtags(row, h_count, n):\n    \"\"\"Function to automatically generate hashtags\n    :param row: list of words \n    :param h_count: dict with words as keys and their frequencies\n    :param n: number of new hashtags \n    :return: \n    \"\"\"\n    # tokenize the text of question body \n\n    q_body_count = {}\n    Q_b = remove_stopwords(row['questions_body'])\n    Q_b = [x.lower() for x in Q_b]\n    Q_b = nltk.pos_tag(Q_b)\n    Q_t = remove_stopwords(row['questions_body'])\n    Q_t = [x.lower() for x in Q_t]\n    Q_t = nltk.pos_tag(Q_t)\n    Q = Q_b + Q_t\n    for word in Q:\n        if word[1][0] == \"N\":\n            q_body_count[word[0]] = q_body_count.get(word[0], 0) + h_count.get(word[0], 0)\n\n    list_extra_tags = []\n    count = 0\n    iter_ = 0\n    while ((count < n) and (iter_ < len(q_body_count.keys()))):\n        iter_ = iter_ + 1\n        tag = max(q_body_count, key=q_body_count.get)\n        if tag in (row['Hashtags'] + [\"i\"]):\n            q_body_count.pop(tag, None)\n        else:\n            count = count + 1\n            q_body_count.pop(tag, None)\n            list_extra_tags.append(tag)\n    return list_extra_tags\n\n\ndef Auto_generate_extra_tags(df_, n):\n    \"\"\"Function to assign n extra tags for given questions\n    :param df_: pandas dataframe containing hashtags columns (lists of hashtags)\n    :param n: number of tags \n    :return: \n    \"\"\"\n    df = df_.copy()\n    h_count = create_TDM(df)\n    df['Extra_hash_tags'] = df.apply(suggest_extra_hashtags, args=(h_count, n), axis=1)\n    return df\n\n\ndef merge_extra_tags(row):\n    \"\"\"Function to merge extra hashtags in original hashtags\n    :param row: row of df containing hashtags and extra tags \n    :return: merged list of given hashtags and extra tags\n    \"\"\"\n    total_tags = row['Hashtags'] + row['Extra_hash_tags']\n    return total_tags","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Loading the files "},{"metadata":{"trusted":true},"cell_type":"code","source":"path = \"../input/data-science-for-good-careervillage/\"\n# Load data and merge files \n#files = [x for x in os.listdir(\"..input/\") if x[-4:] == \".csv\"]\n#print(\"Number of files given are {}\".format(files))\n\ngroups = pd.read_csv(path+\"groups.csv\")\nprofessionals = pd.read_csv(path +'professionals.csv')\nq_tags = pd.read_csv(path+'tag_questions.csv')\nemails = pd.read_csv(path+'emails.csv')\nanswer = pd.read_csv(path+'answers.csv')\ng_mem = pd.read_csv(path+'group_memberships.csv')\ntag_users = pd.read_csv(path+'tag_users.csv')\nmatches = pd.read_csv(path+'matches.csv')\ntags = pd.read_csv(path+'tags.csv')\ncomments = pd.read_csv(path+'comments.csv')\nquestions = pd.read_csv(path+'questions.csv')\nstudents = pd.read_csv(path+'students.csv')\nsch_mem = pd.read_csv( path+'school_memberships.csv')\n\n\nprofessionals['datetime_joined'] = pd.to_datetime(professionals.professionals_date_joined)\nprofessionals.loc[:, 'joined_year'] = professionals['datetime_joined'].dt.year\nquestions['Hashtags'] = questions['questions_body'].apply(lambda row:extract_hashtags(row))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nq = questions.copy()\n\nq[\"questions_body\"] = q[\"questions_body\"].apply(lambda x:remove_punctuation(x))\nq[\"questions_body\"] = q[\"questions_body\"].apply(lambda x: clean_numbers(x))\nq[\"questions_body\"] = q[\"questions_body\"].apply(lambda x: clean_contractions(x, contraction_mapping))\nq[\"questions_body\"] = q[\"questions_body\"].apply(lambda x:tokenize(x))\nq[\"questions_body\"] = q[\"questions_body\"].apply(lambda x:lemmatize_text(x))\nq[\"questions_title\"] = q[\"questions_title\"].apply(lambda x:remove_punctuation(x))\nq[\"questions_title\"] = q[\"questions_title\"].apply(lambda x: clean_numbers(x))\nq[\"questions_title\"] = q[\"questions_title\"].apply(lambda x: clean_contractions(x, contraction_mapping))\nq[\"questions_title\"] = q[\"questions_title\"].apply(lambda x:tokenize(x))\nq[\"questions_title\"] = q[\"questions_title\"].apply(lambda x:lemmatize_text(x))\n\nq1 = Auto_generate_extra_tags(q, n =3)\nq1.sample(5)\nq['Hashtags'] = q1.apply(lambda row:merge_extra_tags(row), axis =1)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Hashtags interactions "},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"import plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.tools as tls\n\n# Creating tag interaction pairs \nh_dict = create_TDM(q)\ntop_200_tags = []\ni = 0\nwhile i < 200:\n    tag = max(h_dict, key=h_dict.get)\n    top_200_tags.append(tag)\n    h_dict.pop(tag, None)\n    i = i + 1\n\n\nlinks = {}\ntag1_list = []\ntag2_list = []\nvalue_list = []\n\ndf = q.copy()\ntags_list = {}\nfor tag1 in top_200_tags:\n    t_list = []\n    for i in list(range(df.shape[0])):\n        len_ = list(set(tag1).intersection(set(df.loc[i, \"Hashtags\"]))).__len__()\n        if len_ != 0:\n            common_w_tag = list(set(top_200_tags).intersection(set(df.loc[i, \"Hashtags\"])))\n            common = [x for x in common_w_tag if x != tag1]\n            t_list.extend(common)\n    tags_list[tag1] = t_list\n    \n# format tags interaction pairs for plaotting \n\nfor key in tags_list.keys():\n    l = tags_list[key]\n    for next_tag in tags_list.keys():\n        if next_tag in list(l):\n            tag1_list.append(key)\n            tag2_list.append(next_tag)\n            value_list.append(l.count(next_tag))\n        else:\n            tag1_list.append(key)\n            tag2_list.append(next_tag)\n            value_list.append(0)\n        \n    \nchord_dict = {\"tag1\":tag1_list, \"tag2\":tag2_list, \"value\":value_list}\nlinks = pd.DataFrame(chord_dict)\nlinks.drop_duplicates(inplace = True)\n\n# Plotting top 200 tags and their interaction with one another \n\ntrace = go.Scatter(\n    y = links['tag2'].values,\n    x = links['tag1'].values,\n    mode='markers',\n    marker=dict(\n        size= links['value'].values*3,\n        #color = np.random.randn(500), #set color equal to a variable\n        color = links['value'].values,\n        colorscale='Portland',\n        showscale=True\n    ),\n    text = links['tag1'].values\n)\ndata = [trace]\n\nlayout= go.Layout(\n    autosize= True,\n    title= 'First Tag',\n    hovermode= 'closest',\n    xaxis=dict(\n        showgrid=False,\n        zeroline=False,\n        showline=False\n    ),\n    yaxis=dict(\n        title= 'Second Tag',\n        ticklen= 5,\n        gridwidth= 2,\n        showgrid=False,\n        zeroline=False,\n        showline=False\n    ),\n    showlegend= False\n)\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig,filename='scatterChol')\n    \n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This is the bubble plot for pairwaise hashtags presence in questions. On both the axis, we have hashtags and the size of the bubble denotes that how frequently one word appears with other. This is a interactive plot.. We can see that tags like \"college-major -- chemistry\", \"degree -- professional\", and \"resume-- graduate\" occures very frequently with each other. Try it for yourself, fun way to explore the data rather than checking data tables.. Also, one can check top_200_words variables value and see more/less hashtaggs interaction."},{"metadata":{},"cell_type":"markdown","source":"### Where do professionals work ?"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# find fortune 500 companies professionals\ncompanies = pd.read_csv(\"../input/fotune500-2017/Fortune 500 Companies US.csv\", encoding = 'ISO-8859-1')\ncompanies.head()\n\nlist_of_companies = companies['Company Name']\nlist_of_companies = [x.lower() for x in list_of_companies]\n\nprofessionals.head()\n\n\ndef assign_company(row, list_=list_of_companies):\n    \"\"\"Functionm to assign the company to professional \"\"\"\n    try:\n\n        heading_list = tokenize(row)\n        heading_list2 = [x.lower() for x in heading_list]\n        common = (set(heading_list2).intersection(set(list_)))\n        # print(common)\n        if len(list(common)) != 0:\n            company = list(common)[0]\n        else:\n            company = np.nan\n    except:\n        company = np.nan\n\n    return (company)\n\n\nprofessionals['workplace'] = professionals['professionals_headline'].apply(\n    lambda row: assign_company(row, list_=list_of_companies))\n\n# Saving them in a different file to drop the null values \n\nworkplace_df = professionals.copy()\nworkplace_df.dropna(inplace = True)\nstop_words = stopwords.words('english')\ncloud = WordCloud(\n                  stopwords=stop_words,\n                  background_color='white',\n                  width=2500,\n                  height=1200,\n                  max_words = 100, \n                 )\n\n\nworkplace_df.workplace.value_counts().reset_index()\nworkplace_dict = dict(zip(workplace_df.workplace.value_counts().reset_index()['index'], workplace_df.workplace.value_counts().reset_index()['workplace']))\n\nwordcloud = cloud.generate_from_frequencies(workplace_dict)\nplt.figure(figsize=(15,8))\nplt.imshow(wordcloud,  interpolation='bilinear')\nplt.axis(\"off\")\nplt.savefig(\"professionals_loc_cloud\"+\".png\", bbox_inches='tight')\nplt.show()\nplt.close()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From this plot it is clear that major firms which have these authors are Symantec,verizon, aig, apple, microsoft, blackrock, coca-cola, oracle etc. But this plot doesn't give us the idea of actual numbers, Lets plot a bar plot and have a look at number of authors working in these firms."},{"metadata":{},"cell_type":"markdown","source":"#### Most frequent companies "},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Bar plot in plotly \nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.tools as tls\n\n\ndata = [go.Bar(\n            x=list(workplace_dict.keys()),\n            y=list(workplace_dict.values()),\n    )]\nfig = go.Figure(data=data)\npy.iplot(fig,filename='scatterChol')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Most frequent companies (without symantec)"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Bar plot in plotly \nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.tools as tls\n\n\ndata = [go.Bar(\n            x=list(workplace_dict.keys()),\n            y=list(workplace_dict.values()),\n    )]\nfig = go.Figure(data=data)\npy.iplot(fig,filename='scatterChol')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Merge other data sets "},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"# Data preprocessing function \ndef response_lag(df):\n    \"\"\"Function to calculate days after which the question was answered \n    :param df: Pandas dataframe\n    :return: df with response lag\n    \"\"\"\n    df.loc[:,'answers_date_added'] = pd.to_datetime(df.answers_date_added)\n    df.loc[:,'answer_date'] = df['answers_date_added'].dt.date\n    df.loc[:,'questions_date_added'] = pd.to_datetime(df.questions_date_added)\n    df.loc[:,'questions_date'] = df['questions_date_added'].dt.date\n    df.loc[:,'answer_delayed_by_days'] = df['answers_date_added'].values - df['questions_date_added'].values\n    df.loc[:,'answer_delayed_by_days'] = df['answer_delayed_by_days'].dt.days\n    return df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Coemment Sentiment \nWe calculate sentiments on comments to figure out the quality of answer. The base assumption here is if the sentiment on comment is positive (overall sentiment - from all comments on a question), the answer is liked by students. We would use the sentiment, rather than hearts data, **as problem with hearts is that hearts is a like offered by a students but hearts doesn't give any information on how much dislikes are there while Sentiment Anslsysis does**"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"analyser = SentimentIntensityAnalyzer()\n\ndef comment_sentiment_index(row):\n    \"\"\"Try vader sentiment - value will be from -1 to 1 \n    :param row: String \n    :return: Sentiment score between -1 to 1\n    \"\"\"\n    try:\n        analyser = SentimentIntensityAnalyzer()\n        snt = analyser.polarity_scores(row)\n        sentiment = snt['compound']\n    except:\n        sentiment = 0.0\n    return sentiment\n\n%time comments['comments_sentiment'] = comments['comments_body'].apply(lambda row:comment_sentiment_index(row))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"comments_sentiment_summary = pd.DataFrame(comments.groupby(\"comments_parent_content_id\")[\"comments_sentiment\"].mean())\ncomments_sentiment_summary.reset_index(inplace = True)\ncomments_sentiment_summary.head()\n#print(comments_sentiment_summary.comments_sentiment.describe())\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Sentiment analysis plot \n%matplotlib inline\nstart = time.time()\nsns.set(style=\"white\", palette=\"muted\", color_codes=True)\nf, axes = plt.subplots(1, 1, figsize=(11, 7), sharex=True)\nsns.despine(left=True)\nsns.distplot(comments_sentiment_summary.comments_sentiment.values, axlabel = 'Sentiment', label = 'Sentiments', bins = 50, color=\"k\")\nplt.setp(axes, yticks=[])\nplt.tight_layout()\nend = time.time()\nprint(\"Time taken by above cell is {}.\".format((end-start)))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"# merging it with all data\nanswer_wc = answer.merge(comments_sentiment_summary, \n                         left_on = \"answers_id\", \n                         right_on = \"comments_parent_content_id\", \n                         how = \"left\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Merging all datasets "},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Data merging \na = answer_wc.copy()\nQA = q.merge(a, left_on = 'questions_id', right_on = 'answers_question_id', how = \"left\")\nQA.drop('answers_question_id', axis =1, inplace = True)\nQA.head()\n\n\ng_mem.head()\nprint(g_mem.shape)\ngroup_data = g_mem.merge(groups, left_on = \"group_memberships_group_id\", right_on = \"groups_id\", how = \"left\")\nprint(group_data.shape)\ngroup_data.head()\ngroup_data.drop(\"group_memberships_group_id\", axis = 1, inplace = True)\ngroup_data.head()\n\n\nprofessionals = professionals.merge(group_data, left_on = \"professionals_id\", right_on = \"group_memberships_user_id\", how = \"left\")\nprofessionals.head()\nprofessionals.drop(\"group_memberships_user_id\", axis = 1, inplace = True)\nprofessionals.drop(\"groups_id\", axis = 1, inplace = True)\nprofessionals.head()\nprofessionals.rename(columns = {\"groups_group_type\":\"group_type\"}, inplace = True)\nprofessionals.head()\n\n\nQA.head()\n# merge professionals with answer_author_id \nQA1 = QA.merge(professionals, left_on = \"answers_author_id\", right_on = \"professionals_id\", how = \"left\")\nQA1.head()\nQA1.drop(\"professionals_id\", axis = 1, inplace = True)\nQA1.head()\n\nQA2 = QA1.merge(students, left_on = \"questions_author_id\", right_on = \"students_id\", how = \"left\")\nQA2.head()\n\nq_tags2 = q_tags.merge(tags, left_on = \"tag_questions_tag_id\", right_on = \"tags_tag_id\", how = \"left\")\nq_tags2.drop(\"tag_questions_tag_id\", axis = 1, inplace = True)\nq_tags2.rename(columns = {\"tags_tag_name\":\"Tag\"}, inplace = True)\n\nq_tag_dummy = pd.get_dummies(q_tags2, columns=['Tag'])\nq_tag_dummy.head()\n\nQA4 = response_lag(QA2)\nQA4.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Where are we getting our questions and answers from ?"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"\n#pd.read_csv('filename.csv', encoding=result['encoding'])\nlat_lon = pd.read_csv(\"../input/latlon-countries/lat_long.csv\", encoding = \"latin-1\")\ndef student_question(data1,loc_student,question_id):\n    data1['location_student'] = data1[loc_student].apply(lambda x: str(x).split(',')[-1] if str(x).split(',')[-1] != ' United States' else str(x).split(',')[-2])\n    data1['location_student'] = data1['location_student'].apply(lambda x: str(x).lstrip())\n    data1['location_student'] = np.where(data1['location_student']=='Texas Area','Texas',data1['location_student'])\n    data1['location_student'] = np.where(data1['location_student']=='Greater New York City Area','New York',data1['location_student'])\n    pivot = pd.pivot_table(data = data1, index = 'location_student', values = question_id ,aggfunc=lambda x: len(x.unique()))\n    pivot.reset_index(inplace = True)\n    return pivot\n\n\nQ_origin_df = student_question(QA4,'students_location','questions_id')\nA_origin_df = student_question(QA4, 'professionals_location',\"questions_id\")\n\n\n# merging with lat-long \n\nQ_origin_df = Q_origin_df.merge(lat_lon, left_on = \"location_student\", right_on = \"name\", how= 'left')\nQ_origin_df.dropna(inplace = True)\nA_origin_df = A_origin_df.merge(lat_lon, left_on = \"location_student\", right_on = \"name\", how= 'left')\nA_origin_df.dropna(inplace = True)\n\n# Make a data frame with dots to show on the map\n\ndata = pd.DataFrame({\n   'lat':Q_origin_df.longitude,\n   'lon':Q_origin_df.latitude,\n   'name':Q_origin_df.name,\n   'value':Q_origin_df.questions_id\n})\ndata\n\ndata1 = pd.DataFrame({\n   'lat':A_origin_df.longitude,\n   'lon':A_origin_df.latitude,\n   'name':A_origin_df.name,\n   'value':A_origin_df.questions_id\n})\ndata1\n \n# Make an empty map\nm = folium.Map(location=[20,0], tiles=\"Mapbox Bright\", zoom_start=2.2)\n \n# I can add marker one by one on the map\nfor i in range(0,len(data)):\n    folium.Circle(\n      location=[float(data.iloc[i]['lon']), float(data.iloc[i]['lat'])],\n      popup=data.iloc[i]['name'],\n      radius=float(data.iloc[i]['value'])*250,\n      color='crimson',\n      fill=True,\n      fill_color='crimson'\n   ).add_to(m)\n    \n    \nfor i in range(0,len(data1)):\n    folium.Circle(\n      location=[float(data1.iloc[i]['lon']), float(data1.iloc[i]['lat'])],\n      popup=data1.iloc[i]['name'],\n      radius=float(data1.iloc[i]['value'])*250,\n      color='green',\n      fill=True,\n      fill_color='green'\n   ).add_to(m)\nm","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This plot shows the locations from where wwe are getting questions and answers. One can see that the questions are asked from almost all the parts of the world but the maority of questions are from USA and India. CV is a go to platform for students in India and USA. Lets zoom the USA portion in the next plot and see how the questions and answers density if spread across the country."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"m1 = folium.Map(location=[37,-102], tiles=\"Mapbox Bright\", zoom_start=4)\n \n# I can add marker one by one on the map\nfor i in range(0,len(data)):\n    folium.Circle(\n      location=[float(data.iloc[i]['lon']), float(data.iloc[i]['lat'])],\n      popup=data.iloc[i]['name'],\n      radius=float(data.iloc[i]['value'])*150,\n      color='crimson',\n      fill=True,\n      fill_color='crimson'\n   ).add_to(m1)\n    \n    \nfor i in range(0,len(data1)):\n    folium.Circle(\n      location=[float(data1.iloc[i]['lon']), float(data1.iloc[i]['lat'])],\n      popup=data1.iloc[i]['name'],\n      radius=float(data1.iloc[i]['value'])*150,\n      color='green',\n      fill=True,\n      fill_color='green'\n   ).add_to(m1)\nm1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This plot shows us the CV is most popular in California, Washington state, Taxas, and on east coast. We can use next plot to actually find out if the location of answer author is same as location of questioning student. We will plot a source destination sankey plot see if we can make a hypothese on location or not."},{"metadata":{},"cell_type":"markdown","source":"### Question Author location VS Answer author location"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"import plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.tools as tls\ninit_notebook_mode()\ndata1 = QA4.copy()\n\n# data1['Hashtags']=data1['Hashtags'].apply(ast.literal_eval)\n\nh_list = []\nfor i in list(range(data1.shape[0])):\n    h_list.extend(data1.loc[i, 'Hashtags'])\n\nh_list = pd.DataFrame(h_list)\nh_list.columns = ['word_1']\nlist_words = pd.DataFrame(h_list.word_1.value_counts()).index\ntop_20_words = list_words[:20]\n\n\ndef compare(row):\n    common = list(set(row['Hashtags']).intersection(set(top_20_words)))\n    if len(common) == 0:\n        a = 'NULL'\n    else:\n        a = common[0]\n    return a\n\n\ndata1['word_hashtag'] = data1.apply(lambda row: compare(row), axis=1)\n\ndata1['location_student'] = data1['students_location'].apply(lambda x: \\\n        (str(x).split(',')[-1] if str(x).split(',')[-1]\n        != ' United States' else str(x).split(',')[-2]))\ndata1['location_student'] = data1['location_student'].apply(lambda x: \\\n        str(x).lstrip())\ndata1['location_student'] = np.where(data1['location_student']\n        == 'Texas Area', 'Texas', data1['location_student'])\ndata1['location_student'] = np.where(data1['location_student']\n        == 'Greater New York City Area', 'New York',\n        data1['location_student'])\n\ndata1['location_professionals'] = data1['professionals_location'\n        ].apply(lambda x: (str(x).split(',')[-1] if str(x).split(','\n                )[-1] != ' United States' else str(x).split(',')[-2]))\ndata1['location_professionals'] = data1['location_professionals'\n        ].apply(lambda x: str(x).lstrip())\ndata1['location_professionals'] = \\\n    np.where(data1['location_professionals'] == 'Texas Area', 'Texas',\n             data1['location_professionals'])\ndata1['location_professionals'] = \\\n    np.where(data1['location_professionals']\n             == 'Greater New York City Area', 'New York',\n             data1['location_professionals'])\n\ntop_student_location = data1[data1['location_student'] != 'nan'\n                             ].groupby('location_student'\n        )['questions_id'].nunique().sort_values(ascending=False)\ntop_student_location = top_student_location[:10]\ntop_student_location_list = list(top_student_location.index)\n\ntop_prof_location = list(data1[data1['location_student'\n                         ].isin(top_student_location_list)]['location_professionals'\n                         ].unique())\ndata_sanky = data1[data1['location_student'\n                   ].isin(top_student_location_list)][['location_student'\n        , 'location_professionals', 'questions_id', 'word_hashtag',\n        'professionals_industry']]\ndata_sanky['count1'] = 1\n\ntop_prof_location = data_sanky[data_sanky['location_professionals']\n                               != 'nan'\n                               ].groupby('location_professionals'\n        )['questions_id'].nunique().sort_values(ascending=False)\ntop_prof_location = top_student_location[:10]\ntop_prof_location_list = list(top_prof_location.index)\ndata_sanky = data_sanky[data_sanky['location_professionals'\n                        ].isin(top_prof_location_list)]\n\ntop_prof_industry = data_sanky[data_sanky['professionals_industry']\n                               != 'nan'\n                               ].groupby('professionals_industry'\n        )['questions_id'].nunique().sort_values(ascending=False)\ntop_prof_industry = top_prof_industry[:10]\ntop_prof_industry_list = list(top_prof_industry.index)\ndata_sanky = data_sanky[data_sanky['professionals_industry'\n                        ].isin(top_prof_industry_list)]\n\ndata_sanky = data_sanky[data_sanky['word_hashtag'] != 'NULL']\n\ndonation_paths = data_sanky.pivot_table(columns=['location_student'],\n        index=['location_professionals'], values='questions_id',\n        aggfunc=lambda x: len(x.unique()), fill_value=0)\n\ndonation_paths = \\\n    donation_paths.reset_index().melt(id_vars='location_professionals')\n\n(student_location_encoder, prof_location_encoder) = (LabelEncoder(),\n        LabelEncoder())\ndonation_paths['Encoded_prof_location'] = \\\n    student_location_encoder.fit_transform(donation_paths['location_professionals'\n        ])\ndonation_paths['Encoded_student_location'] = \\\n    prof_location_encoder.fit_transform(donation_paths['location_student'\n        ]) + len(donation_paths['location_professionals'].unique())\n\nall_snky_labels = np.unique(np.array(donation_paths['location_student'\n                            ].unique().tolist()\n                            + donation_paths['location_professionals'\n                            ].unique().tolist()))\nplotly_colors = [\n    '#1f77b4',\n    '#ff7f0e',\n    '#2ca02c',\n    '#d62728',\n    '#9467bd',\n    '#8c564b',\n    '#e377c2',\n    '#7f7f7f',\n    '#bcbd22',\n    '#17becf',\n    ]\n\nstates_finished = False\nstate_colors = []\ni = 0\nwhile not states_finished:\n\n    state_colors.append(plotly_colors[i])\n\n    if len(state_colors) >= len(all_snky_labels):\n        states_finished = True\n\n    i += 1\n    if i >= len(plotly_colors):\n        i = 0\n\ncolor_dict = dict(zip(all_snky_labels, state_colors))\n\nsnky_labels = prof_location_encoder.classes_.tolist() \\\n    + student_location_encoder.classes_.tolist()\ncolors = []\nfor state in snky_labels:\n    colors.append(color_dict[state])\n\ndata = dict(type='sankey', node=dict(pad=15, thickness=20,\n            line=dict(color='black', width=0.5), label=snky_labels,\n            color=colors),\n            link=dict(source=donation_paths['Encoded_prof_location'],\n            target=donation_paths['Encoded_student_location'],\n            value=donation_paths['value']))\n\nlayout = dict(title='Student Location Vs Professional Location Sankey',\n              autosize=False, width=800, height=750, font=dict(size=10))\n\nfig = dict(data=[data], layout=layout)\niplot(fig, validate=False)\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This is another interactive plot that shows us the source and destination, that is question location and their flow to answer location. We can check that other than for california, Taxas, and india, mostof the questions are not significantly asnwered from the author of same location. All the locations are equally probable, Lets see what hashtags are popular based on location and type of profile of authors."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"init_notebook_mode()\ndata_sanky = data_sanky[data_sanky['word_hashtag'] != 'NULL']\ndonation_paths = data_sanky.pivot_table(columns=['location_student'],\n        index=['word_hashtag'], values='questions_id',\n        aggfunc=lambda x: len(x.unique()), fill_value=0)\ndonation_paths = \\\n    donation_paths.reset_index().melt(id_vars='word_hashtag')\n\ndonation_paths1 = data_sanky.pivot_table(columns=['word_hashtag'],\n        index=['location_professionals'], values='questions_id',\n        aggfunc=lambda x: len(x.unique()), fill_value=0)\ndonation_paths1 = \\\n    donation_paths1.reset_index().melt(id_vars='location_professionals')\n\ndonation_paths2 = \\\n    data_sanky.pivot_table(columns=['location_professionals'],\n                           index=['professionals_industry'],\n                           values='questions_id', aggfunc=lambda x: \\\n                           len(x.unique()), fill_value=0)\ndonation_paths2 = \\\n    donation_paths2.reset_index().melt(id_vars='professionals_industry')\n\n(word_encoder, student_location_encoder, prof_location_encoder,\n prof_industry_encoder) = (LabelEncoder(), LabelEncoder(),\n                           LabelEncoder(), LabelEncoder())\ndonation_paths['Encoded_word'] = \\\n    word_encoder.fit_transform(donation_paths['word_hashtag'])\ndonation_paths['Encoded_student_location'] = \\\n    student_location_encoder.fit_transform(donation_paths['location_student'\n        ]) + len(donation_paths['word_hashtag'].unique())\ndonation_paths1['Encoded_prof_location'] = \\\n    prof_location_encoder.fit_transform(donation_paths1['location_professionals'\n        ]) + len(donation_paths1['word_hashtag'].unique()) \\\n    + len(donation_paths['location_student'].unique())\ndonation_paths2['Encoded_prof_industry'] = \\\n    prof_industry_encoder.fit_transform(donation_paths2['professionals_industry'\n        ]) + len(donation_paths1['word_hashtag'].unique()) \\\n    + len(donation_paths['location_student'].unique()) \\\n    + len(donation_paths2['location_professionals'].unique())\n\ndonation_paths1 = pd.merge(donation_paths1,\n                           donation_paths[donation_paths['Encoded_student_location'\n                           ] == 20][['Encoded_word', 'word_hashtag']],\n                           on='word_hashtag')\n\ndonation_paths2 = pd.merge(donation_paths2,\n                           donation_paths1[donation_paths1['Encoded_word'\n                           ] == 0][['Encoded_prof_location',\n                           'location_professionals']],\n                           on='location_professionals')\n\nall_snky_labels = np.unique(word_encoder.classes_.tolist()\n                            + student_location_encoder.classes_.tolist()\n                            + prof_location_encoder.classes_.tolist()\n                            + prof_industry_encoder.classes_.tolist())\n\nplotly_colors = [\n    'aliceblue',\n    'antiquewhite',\n    'beige',\n    'blueviolet',\n    'chartreuse',\n    'cornsilk',\n    'darkgoldenrod',\n    'darkkhaki',\n    'darkorchid',\n    'darkslateblue',\n    'darkturquoise',\n    'dimgray',\n    'floralwhite',\n    'ghostwhite',\n    'greenyellow',\n    'ivory',\n    'lemonchiffon',\n    'lightgoldenrodyellow',\n    'lightgreen',\n    'lightskyblue',\n    'lightsteelblue',\n    'linen',\n    'mediumblue',\n    'mediumseagreen',\n    'mediumturquoise',\n    'mintcream',\n    'oldlace',\n    'orchid',\n    'palevioletred',\n    'plum',\n    'royalblue',\n    'seagreen',\n    'slateblue',\n    'steelblue',\n    'violet',\n    'yellowgreen',\n    'aqua',\n    'bisque',\n    'brown',\n    'chocolate',\n    'crimson',\n    'darkgray',\n    'darkmagenta',\n    'darkred',\n    'darkslategray',\n    'darkviolet',\n    'dimgrey',\n    'forestgreen',\n    'gold',\n    'honeydew',\n    'khaki',\n    'lightblue',\n    'lightgray',\n    'lightpink',\n    'lightslategray',\n    'lightyellow',\n    'magenta',\n    'mediumorchid',\n    'mediumslateblue',\n    'mediumvioletred',\n    'mistyrose',\n    'olive',\n    'palegoldenrod',\n    'papayawhip',\n    'powderblue',\n    'saddlebrown',\n    'seashell',\n    'slategray',\n    'tan',\n    'wheat',\n    'aquamarine',\n    'black',\n    'burlywood',\n    'coral',\n    'cyan',\n    'darkgrey',\n    'darkolivegreen',\n    'darksalmon',\n    'darkslategrey',\n    'deeppink',\n    'dodgerblue',\n    'fuchsia',\n    'goldenrod',\n    'hotpink',\n    'lavender',\n    'lightcoral',\n    'lightgrey',\n    'lightsalmon',\n    'lightslategrey',\n    'lime',\n    'maroon',\n    'mediumpurple',\n    'mediumspringgreen',\n    'midnightblue',\n    'moccasin',\n    'olivedrab',\n    'palegreen',\n    'peachpuff',\n    'purple',\n    'salmon',\n    'sienna',\n    'slategrey',\n    'teal',\n    'white',\n    ]\n\nstates_finished = False\nstate_colors = []\ni = 0\nwhile not states_finished:\n\n    state_colors.append(plotly_colors[i])\n\n    if len(state_colors) >= len(all_snky_labels):\n        states_finished = True\n\n    i += 1\n    if i >= len(plotly_colors):\n        i = 0\n\ncolor_dict = dict(zip(all_snky_labels, state_colors))\n\nsnky_labels = word_encoder.classes_.tolist() \\\n    + student_location_encoder.classes_.tolist() \\\n    + prof_location_encoder.classes_.tolist() \\\n    + prof_industry_encoder.classes_.tolist()\ncolors = []\nfor state in snky_labels:\n    colors.append(color_dict[state])\n\ndata = dict(type='sankey', node=dict(pad=15, thickness=20,\n            line=dict(color='black', width=0.5), label=snky_labels,\n            color=colors),\n            link=dict(source=donation_paths['Encoded_student_location'\n            ].append(donation_paths1['Encoded_word'\n            ]).append(donation_paths2['Encoded_prof_location']),\n            target=donation_paths['Encoded_word'\n            ].append(donation_paths1['Encoded_prof_location'\n            ]).append(donation_paths2['Encoded_prof_industry']),\n            value=donation_paths['value'].append(donation_paths1['value'\n            ]).append(donation_paths2['value'])))\n\nlayout = \\\n    dict(title='Student Location Vs Frequent Word Vs Professional Location Vs Professional Industry Sankey'\n         , autosize=False, width=800, height=750, font=dict(size=10))\n\nfig = dict(data=[data], layout=layout)\niplot(fig, validate=False)\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**An Image is worth a thousand words** - Yes it is. This interactive plot addes profiles of authors and hashtags to the previous plot. It is very easy to see that the location has some relation with industry of authors - \n- **Example 1** - India has maority of people in information technology and hashtags they are following are career, science, school, engineering. \n- **Example 2** - Taxas people are answering qustions with hashtags obs, career, school, college and people have very different background - they work in almost all domains. So, domain is significantly dominating in authors of this area.\n- **Example 3** - Most of the people in newyork are education professionals and thus answers questions with almost all hashatgs.\n- **Example 4** - very obvious finding - authors from california are Computer science people (NO Surprise!!) who answers technology, computers and job related questions other than schools and career."},{"metadata":{},"cell_type":"markdown","source":"#### Some timeseries plots to show increase in students, professionals, questions, answers"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"init_notebook_mode()\nimport datetime\ndata1['students_date_joined'] = \\\n    pd.to_datetime(data1['students_date_joined'])\ndata1['student_joined_year'] = data1['students_date_joined'].dt.year\ndata1['student_joined_month'] = data1['students_date_joined'].dt.month\ndata1['student_joined_ym'] = data1['students_date_joined'\n                                   ].apply(lambda x: (str(x)[:7] + '-01'\n         if str(x) != 'NaT' else x))\ndata1['student_joined_ym'] = pd.to_datetime(data1['student_joined_ym'])\n\ndata1['professionals_date_joined'] = \\\n    pd.to_datetime(data1['professionals_date_joined'])\ndata1['professionals_joined_year'] = data1['professionals_date_joined'\n        ].dt.year\ndata1['professionals_joined_month'] = data1['professionals_date_joined'\n        ].dt.month\ndata1['professionals_joined_ym'] = data1['professionals_date_joined'\n        ].apply(lambda x: (str(x)[:7] + '-01' if str(x) != 'NaT'\n                 else x))\ndata1['professionals_joined_ym'] = \\\n    pd.to_datetime(data1['professionals_joined_ym'])\n\nts_v1 = pd.pivot_table(data=data1, index=['student_joined_ym'],\n                       values='students_id', aggfunc=lambda x: \\\n                       len(x.unique()), fill_value=0)\nts_v1.reset_index(inplace=True)\nts_v1.columns = ['year_month', 'Number_questions']\nts_v1['Number_questions_cum'] = ts_v1['Number_questions'].cumsum()\n\nts_v2 = pd.pivot_table(data=data1, index=['professionals_joined_ym'],\n                       values='answers_author_id', aggfunc=lambda x: \\\n                       len(x.unique()), fill_value=0)\nts_v2.reset_index(inplace=True)\nts_v2.columns = ['year_month', 'Number_questions']\nts_v2['Number_questions_cum'] = ts_v2['Number_questions'].cumsum()\n\nfrom bokeh.palettes import Spectral4\nfrom bokeh.plotting import figure, output_notebook, show\noutput_notebook()\np = figure(plot_width=800, plot_height=250, x_axis_type='datetime')\np.title.text = 'Increase in Number of professionals and students with time'\n\nfor (data, name, color) in zip([ts_v1, ts_v2], ['students',\n                               'professionals'], Spectral4):\n    df = data\n    p.line(\n        df['year_month'],\n        df['Number_questions'],\n        line_width=2,\n        color=color,\n        alpha=0.8,\n        legend=name,\n        )\n\np.legend.location = 'top_left'\np.legend.click_policy = 'hide'\nshow(p)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"p = figure(plot_width=800, plot_height=250, x_axis_type='datetime')\np.title.text = 'cummulative increase in Number of professionals and students with time'\n\nfor (data, name, color) in zip([ts_v1, ts_v2], ['students',\n                               'professionals'], Spectral4):\n    df = data\n    p.line(\n        df['year_month'],\n        df['Number_questions_cum'],\n        line_width=2,\n        color=color,\n        alpha=0.8,\n        legend=name,\n        )\n\np.legend.location = 'top_left'\np.legend.click_policy = 'hide'\nshow(p)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"\ndata1['questions_date'] = pd.to_datetime(data1['questions_date'])\ndata1['questions_date_ym'] = data1['questions_date'].apply(lambda x: \\\n        (str(x)[:7] + '-01' if str(x) != 'NaT' else x))\ndata1['questions_date_ym'] = pd.to_datetime(data1['questions_date_ym'])\n\ndata1['answers_date_added'] = pd.to_datetime(data1['answers_date_added'\n        ])\ndata1['answers_date_ym'] = data1['answers_date_added'].apply(lambda x: \\\n        (str(x)[:7] + '-01' if str(x) != 'NaT' else x))\ndata1['answers_date_ym'] = pd.to_datetime(data1['answers_date_ym'])\n\nts_v1 = pd.pivot_table(data=data1, index=['questions_date_ym'],\n                       values='questions_id', aggfunc=lambda x: \\\n                       len(x.unique()), fill_value=0)\nts_v1.reset_index(inplace=True)\nts_v1.columns = ['year_month', 'Number_questions']\nts_v1['Number_questions_cum'] = ts_v1['Number_questions'].cumsum()\n\nts_v2 = pd.pivot_table(data=data1, index=['answers_date_ym'],\n                       values='answers_id', aggfunc=lambda x: \\\n                       len(x.unique()), fill_value=0)\nts_v2.reset_index(inplace=True)\nts_v2.columns = ['year_month', 'Number_questions']\nts_v2['Number_questions_cum'] = ts_v2['Number_questions'].cumsum()\n\nfrom bokeh.palettes import Spectral4\nfrom bokeh.plotting import figure, output_notebook, show\noutput_notebook()\np = figure(plot_width=800, plot_height=250, x_axis_type='datetime')\np.title.text = 'Increase in Number of question and answers with time'\n\nfor (data, name, color) in zip([ts_v1, ts_v2], ['questions', 'answers'\n                               ], Spectral4):\n    df = data\n    p.line(\n        df['year_month'],\n        df['Number_questions'],\n        line_width=2,\n        color=color,\n        alpha=0.8,\n        legend=name,\n        )\n\np.legend.location = 'top_left'\np.legend.click_policy = 'hide'\nshow(p)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"\np = figure(plot_width=800, plot_height=250, x_axis_type='datetime')\np.title.text = 'Cummulative increase in Number of question and answers with time'\n\nfor (data, name, color) in zip([ts_v1, ts_v2], ['questions', 'answers'\n                               ], Spectral4):\n    df = data\n    p.line(\n        df['year_month'],\n        df['Number_questions_cum'],\n        line_width=2,\n        color=color,\n        alpha=0.8,\n        legend=name,\n        )\n\np.legend.location = 'top_left'\np.legend.click_policy = 'hide'\nshow(p)\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Number of emails vs number of answers density plot "},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"QA_summary = pd.DataFrame(QA4.groupby(\"questions_id\")['answers_id'].count())\nQA_summary.head()\nQA_summary.reset_index(inplace = True)\n\nEM = matches.merge(emails, left_on = \"matches_email_id\", right_on = \"emails_id\", how = \"left\")\nEM.drop(\"matches_email_id\", axis = 1, inplace = True)\nEM.shape\nEM.head()\n\n\n\nEQ_summary = pd.DataFrame(EM.groupby(\"matches_question_id\")['emails_id'].count())\nEQ_summary.head()\nEQ_summary.reset_index(inplace = True)\n\n\nEQA_summary = QA_summary.merge(EQ_summary, left_on = \"questions_id\", right_on = \"matches_question_id\", how = \"left\")\nEQA_summary.head()\n\nEQA_summary2 = EQA_summary.loc[EQA_summary['answers_id']<10]\nEQA_summary3 = EQA_summary2.loc[EQA_summary2['emails_id']<800]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"\ndef make_kdeplot(varX, varY, a,b, c,d, N, colorsc, title):\n    #varX, varY are lists, 1d numpy.array(s), or dataframe columns, storing the values of two variables\n\n    x, y, Z = kde_scipy(varY, varX, a,b, c,d, N )\n\n    data = Data([\n       Contour(\n           z=Z,\n           x=x,\n           y=y,\n           colorscale=colorsc,\n           #reversescale=True,\n           opacity=0.9,\n           contours=Contours(\n               showlines=False)\n        ),\n     ])\n\n    layout = Layout(\n        title= title,\n        font= Font(family='Georgia, serif',  color='#635F5D'),\n        showlegend=False,\n        autosize=False,\n        width=900,\n        height=650,\n        xaxis=XAxis(\n            range=[a,b],\n            showgrid=False,\n            nticks=7\n        ),\n        yaxis=YAxis(\n            range=[c,d],\n            showgrid=False,\n            nticks=7\n        ),\n        margin=Margin(\n            l=40,\n            r=40,\n            b=85,\n            t=100,\n        ),\n    )\n\n    return Figure( data=data, layout=layout )\ncubehelix_cs=[[0.0, '#fcf9f7'],\n [0.16666666666666666, '#edcfc9'],\n [0.3333333333333333, '#daa2ac'],\n [0.5, '#bc7897'],\n [0.6666666666666666, '#925684'],\n [0.8333333333333333, '#5f3868'],\n [1.0, '#2d1e3e']]\n\ndef kde_scipy( vals1, vals2, a,b, c,d, N ):\n    x=np.linspace(a,b,N)\n    y=np.linspace(c,d,N)\n    X,Y=np.meshgrid(x,y)\n    positions = np.vstack([Y.ravel(), X.ravel()])\n\n    values = np.vstack([vals1, vals2])\n    kernel = st.gaussian_kde(values)\n    Z = np.reshape(kernel(positions).T, X.shape)\n\n    return [x, y, Z]\n\n\na, b=(0,8)\nc,d=(0,600)\nN=100\nfig=make_kdeplot(EQA_summary3.answers_id, EQA_summary3.emails_id, a,b, c,d,\n    N, cubehelix_cs, 'kde plot of joint distribution for emails sent vs answer received')\npy.iplot(fig,  filename='emails sent vs answer received')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Part 1 - NLP models to find similar questions to newly asked question\nWe will mianly use two models \n1. LDA \n2. Sentence2vec using word vector embeddings \n\n"},{"metadata":{},"cell_type":"markdown","source":"## LDA (Latent Dirichlet Allocation)\nIt is a generative model which is used to find the abstract topic that occures in a document. It is a topic modelling technique. It builds a topic per document model and words per document model, modeled as dirichlet distributions. It starts with randomly assigning a topic to each word in the given document and document is considered to have a mixture of topics. It generates topics from word frequencies from a set of documents so each document will be a mixture of topics. Then it tries to maximize the area of how much a topic likes the word and how much a word likes the topic and then results in getting the final topic mixture for the document. \n\nWe Can use two types of features for LDA model training - BOW and tfidf. Bow stands for **bag of words** and tfidf stands for **Term frequency-inverse document frequency**. both can be used for LDA models, and we will be using both of these features and will be making two LDA models. \n \n- **Bag of wrods** - Bag of words is simplest way of generating features from text. It uses all the words in the corpus disregards grammer, word order but keep their counts. In this method, we make a dict of words and their counts in the document and this copunt serves as a feature.\n\n- **Tfidf** - Term frequency -inverse document frequency is a measure of how imprtant a word is in a corpus or collection of words. It also uses the information that how many words are contained in that document. For example in a document word \"sun\" is appeared 7 times, bow will give it a 7 value but in tfidf we will also use the number of words which are there in this document (say 70) and divide this 7 with 70 and this (7/70) will be used as a feature.\n\n\n"},{"metadata":{},"cell_type":"markdown","source":"![](https://lh3.googleusercontent.com/-W3RepioJgPE/XL705W5OqCI/AAAAAAAA0fA/kUmyKPALww0WVXguYoZYadP2B0K88dgGQCK8BGAs/s0/2019-04-23.png)"},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"def create_LDA_model(df_, number_of_topics, passes, worker):\n    \"\"\"Function to train a LDA model\n    :param df_: Pandas Dataframe \n    :param number_of_topics: total number of topic we want to find\n    :param passes: number of passes \n    :param worker: workers\n    :return: LDA models (dict for 2 models using bow, and tfidf\n    \"\"\"\n    df = df_.copy()\n    # df is actually q so most of the preprocessing is already done \n\n    # Lets Do stemming as well \n    df[\"questions_body\"] = df[\"questions_body\"].apply(lambda x: stem_text(x))\n\n    # lets create a bag of words TDM kind of dictionary \n    dictionary = gensim.corpora.Dictionary(df[\"questions_body\"])\n\n    # lets filter out extremes \n    dictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=100000)\n\n    # genesim doc2bow \n    bow_corpus = [dictionary.doc2bow(doc) for doc in df[\"questions_body\"]]\n\n    # tfidf creation \n    tfidf = models.TfidfModel(bow_corpus)\n    corpus_tfidf = tfidf[bow_corpus]\n\n    lda_model_bow = gensim.models.LdaMulticore(bow_corpus,\n                                               num_topics=number_of_topics,\n                                               id2word=dictionary,\n                                               passes=passes,\n                                               workers=worker)\n\n    lda_model_tfidf = gensim.models.LdaMulticore(corpus_tfidf,\n                                                 num_topics=number_of_topics,\n                                                 id2word=dictionary,\n                                                 passes=passes,\n                                                 workers=worker)\n\n    LDA_models = {}\n    LDA_models['bow'] = lda_model_bow\n    LDA_models['tfidf'] = lda_model_tfidf\n    return LDA_models, dictionary, bow_corpus, corpus_tfidf\n\n\ndef Preprocessing_sentence_for_LDA(sent):\n    \"\"\"Function to do the preprocessing on a new senstence to be passed to LDA \n    :param sent: String (any questions in our case)\n    :return: list of tokens after processing\n    \"\"\"\n    processed_sent = remove_punctuation(sent)\n    processed_sent = clean_numbers(processed_sent)\n    processed_sent = clean_contractions(processed_sent, contraction_mapping)\n    processed_sent = tokenize(processed_sent)\n    processed_sent = lemmatize_text(processed_sent)\n    processed_sent = stem_text(processed_sent)\n    return processed_sent\n\n\ndef get_topic_probability(LDA_models, sent, dictionary, number_of_topics):\n    \"\"\"Function to get the LDA topic probability\n    :param LDA_models: dict (having two models using bow, and tfidf)\n    :param sent: string (question in our case)\n    :param dictionary: dictionary build during LDA \n    :param number_of_topics: number of topics \n    :return: dict having two models (bow and tfidf) probability for all topics\n    \"\"\"\n    processed_sent = Preprocessing_sentence_for_LDA(sent)\n    bow_corp = dictionary.doc2bow(processed_sent)\n    tfidf_corp = dictionary.doc2bow(processed_sent)\n\n    # Run them and assign topics with probability\n    # We will use the normalize probability from both the models\n\n    lda_topic = dict()\n    topic_prob = dict()\n    for topic in list(range(number_of_topics)):\n        topic_prob[topic] = 0\n    for index, score in sorted(LDA_models['bow'][bow_corp], key=lambda tup: -1 * tup[1]):\n        topic_prob[index] = topic_prob.get(index, 0) + score\n    lda_topic['bow'] = topic_prob\n\n    topic_prob = dict()\n    for topic in list(range(number_of_topics)):\n        topic_prob[topic] = 0\n    for index, score in sorted(LDA_models['tfidf'][tfidf_corp], key=lambda tup: -1 * tup[1]):\n        topic_prob[index] = (topic_prob.get(index, 0) + score)\n    lda_topic['tfidf'] = topic_prob\n    return lda_topic\n\n\ndef create_doc_topic_table(df_, LDA_models, dictionary, number_of_topics):\n    \"\"\"Function to create doc-topic probability table using two LDA models\n    :param df_: Pandas df (having questions)\n    :param LDA_models: dict containing two models using bow and tfidf respectively\n    :param dictionary: dictionary for LDA model\n    :param number_of_topics: number of topics\n    :return: pandas df, doc vs topic table \n    \"\"\"\n    df = df_.copy()\n    doc_topic_dict = dict()\n    for type_ in [\"bow\", \"tfidf\"]:\n        for topic in list(range(number_of_topics)):\n            doc_topic_dict[\"Topic_\" + str(type_) + \"_\" + str(topic)] = []\n    bow_list = []\n    tfidf_list = []\n    for i in list(range(df.shape[0])):\n        q_body = df.loc[i, \"questions_body\"]\n        # print(q_body)\n        # q_body = stem_text(q_body)\n        # bow_q = dictionary.doc2bow(q_body)\n        # tfidf_q = dictionary.doc2bow(q_body)\n        q_topic_dict = get_topic_probability(LDA_models, q_body, dictionary, number_of_topics)\n        bow_list.append(q_topic_dict['bow'])\n        tfidf_list.append(q_topic_dict['tfidf'])\n        for type_ in [\"bow\", \"tfidf\"]:\n            for j in list(range(number_of_topics)):\n                doc_topic_dict[\"Topic_\" + str(type_) + \"_\" + str(j)].append(q_topic_dict[type_][j])\n\n    # convert it(dict) back to df and concat with the df passed\n    doc_topic_df = pd.DataFrame.from_dict(doc_topic_dict)\n    df.loc[:, 'bow_vec'] = bow_list\n    df.loc[:, 'tfidf_vec'] = tfidf_list\n\n    # concate it with the df passed\n    doc_topic_df_w_q = pd.concat([df, doc_topic_df], axis=1)\n    return doc_topic_df_w_q\n\n\ndef find_similar_questions(LDA_models, sent, dictionary, number_of_topics, doc_topic_table, docs_to_return):\n    \"\"\"Function to get the similar qustions \n    :param LDA_models: dict having two models (using bow and tfidf)\n    :param sent: string - question\n    :param dictionary: dictionary for LDA model\n    :param number_of_topics: number of topics\n    :param doc_topic_table: pandas df - doc vs table table \n    :param docs_to_return: number of documents to return as similar docs\n    :return: \n    \"\"\"\n    sent_topic_prob_dict = get_topic_probability(LDA_models, sent, dictionary, number_of_topics)\n    df = doc_topic_table.copy()\n    df.reset_index(inplace=True)\n    #print(sent_topic_prob_dict)\n    # get this vector \n    bow_dict = sent_topic_prob_dict['bow']\n    bow_vector = np.array(list(bow_dict.values())).astype(float)\n\n    tfidf_dict = sent_topic_prob_dict['tfidf']\n    tfidf_vector = np.array(list(tfidf_dict.values())).astype(float)\n\n    # calculate cosine distance\n    df['bow_dist'] = df['bow_vector'].apply(lambda row: cosine_similarity(np.array(row).reshape(1, -1), bow_vector.reshape(1, -1))[0][0])\n    df['tfidf_dist'] = df['tfidf_vector'].apply(lambda row: cosine_similarity(np.array(row).reshape(1, -1), tfidf_vector.reshape(1, -1))[0][0])\n\n    #df['bow_dist'] = df['bow_vector'].apply(\n    #    lambda row: cosine_similarity(np.array(row).reshape(-1, 1), bow_vector.reshape(-1, 1)))\n    #df['tfidf_dist'] = df['tfidf_vector'].apply(\n    #    lambda row: cosine_similarity(np.array(row).reshape(-1, 1), tfidf_vector.reshape(-1, 1)))\n    \n    \n    # Sort and take docs_to_return number of topics from each one and take intersection\n    df.sort_values(\"bow_dist\", ascending=False, inplace=True)\n    bow_n_ids = df['questions_id'][0:docs_to_return]\n\n    # sort for tfidf as well \n    df.sort_values(\"tfidf_dist\", ascending=False, inplace=True)\n    tfidf_n_ids = df['questions_id'][0:docs_to_return]\n\n    common_q = list(set(bow_n_ids).intersection(set(tfidf_n_ids)))\n    return tfidf_n_ids\n\n\ndef create_word_cloud(LDA_models, grid, num_topic, bow=True):\n    \"\"\"Function tpo create a wordcloud for all the different topics\n    :param LDA_models: dict having two models \n    :param grid: grid dimensions\n    :param num_topic: number of topics\n    :param bow: Flag\n    :return: shows wordcloud\n    \"\"\"\n    # plt.figure(figsize=(5*plot_cols, 3*plot_rows))\n    if bow == True:\n        mod = \"bow\"\n    else:\n        mod = \"tfidf\"\n\n    # code to get the number of words for each topic \n    cloud = WordCloud(\n        stopwords=stop_words,\n        background_color='black',\n        width=2500,\n        height=1800\n    )\n    topics = lda_mods['bow'].show_topics(num_topics=20, num_words=30, log=False, formatted=True)\n\n    topic_words = {}\n    for index, topic in LDA_models[mod].show_topics(formatted=False, num_words=40, num_topics=num_topic):\n        topic_words[index] = [w[0] for w in topic]\n\n    plt.figure(figsize=(5 * 5, 5 * 4))\n    for i in list(range(num_topic)):\n        cloud.generate(\".\".join(topic_words[i]))\n\n        ax = plt.subplot(5, 4, i + 1)\n\n        plt.imshow(cloud, interpolation='bilinear')\n        # title = index\n        plt.title(i)\n        plt.axis(\"off\")\n        plt.margins(x=0, y=0)\n    plt.show()\n    return ()\n\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Training LDA models \nlda_mods, diction, bow_corp, corp_tf = create_LDA_model(q, 20, 10, 4)\nprint(lda_mods)\ncreate_word_cloud(lda_mods, 5, 20, bow = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"# How to use LDA model, example \nsent = \"My name is Mahesh and I am really looking for a college degree, which college should I join\"\nt_prob = get_topic_probability(lda_mods, sent, diction, 20)\nprint(t_prob)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"table_DT = create_doc_topic_table(questions, lda_mods, diction, 20)\ntable_DT.head()\n\ndef val(row):\n    return np.array(list(row.values())).astype(float)\ntable_DT['bow_vector'] = list(map(val, table_DT['bow_vec']))\ntable_DT['tfidf_vector'] = list(map(val, table_DT['bow_vec']))\ntable_DT.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"from sklearn.metrics.pairwise import cosine_similarity\nsimilar_topics = find_similar_questions(lda_mods, sent, diction, 20, table_DT, 20)\nsimilar_topics","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Sentence2Vec\nWe create sentence2vec models to find the similar questiosn from a newly asked questions. As the name sugegsts the Sentence2Vec models are based on sentnece vectors which are used then to find the other sentnece vectors which are in vicinity of that vector. What we do here is we generate Sentence vector for each of the questions asked using either questions title or questions hashtags (+ extra generated hashtags) store it as title_vector or hashtags_vectors. Whenever a new question is being asked on the platform, we generate the similar vectors sentence vector for this question as well and then find its cosine distance with each of the vector we have in our database. We shortlist few questions which are very similar to the asked questions sentence vector and then we pass these similar questions in recommender engine. \n#### How we generate Sentence2Vec ?\nWe use **GloVe embedding  (Global Vector embeddings, Stanford NLP)** for each words and combine these embeddings by taking means. As we know that we are going to use this on hashtags and in general people use couple of hashtags/title, that enables these sentence vectors to be very accurate in presenting the sentnece. They capture the context of questions and then with very high degree of accuracy, find similar questions to newly asked questions.\n\n- **What is GloVe ?**\nGloVe is an unsupervised learning algorithm for obtaining vector representations for words. Training is performed on aggregated global word-word co-occurrence statistics from a corpus, and the resulting representations showcase interesting linear substructures of the word vector space. (source - https://nlp.stanford.edu/projects/glove/)\n\n\n"},{"metadata":{},"cell_type":"markdown","source":"![](https://lh3.googleusercontent.com/-tHT7dM7LNww/XL70zEjI-KI/AAAAAAAA0e8/IPy20SxLpbsZS_GkV3T6dFaw2BAAWhBUwCK8BGAs/s0/2019-04-23.png)"},{"metadata":{"trusted":true},"cell_type":"code","source":"file_ = \"../input/glove-twitter/glove.twitter.27B.25d.txt\"\n\n\ndef loadGloveModel(gloveFile):\n    \"\"\"\n    :param gloveFile: path of GloVe txt file\n    :return: GloVe model\n    \"\"\"\n    print (\"Loading Glove Model\")\n\n    with open(gloveFile) as f:\n        content = f.readlines()\n    model = {}\n    for line in content:\n        splitLine = line.split()\n        word = splitLine[0]\n        embedding = np.array([float(val) for val in splitLine[1:]])\n        model[word] = embedding\n    print (\"Done.\", len(model), \" words loaded!\")\n    return model\n\n\nglove_model = loadGloveModel(file_)\n\n\ndef create_sent2vec(sent, model=glove_model):\n    \"\"\"Function to create hashtags vector for question similarity\n    :param sent: list of words or tokens\n    :param model: GloVe model\n    :return: vector\n    \"\"\"\n    # print(sent)\n    len_sent = len(sent)\n    if len_sent != 0:\n        count = 0.0\n        for i in list(range(len_sent)):\n            if i == 0:\n                try:\n                    vec = model[sent[i].lower()]\n                    count = count + 1.0\n                except:\n                    vec = np.zeros(25)\n                    count = count + 1.0\n            else:\n                try:\n                    vec = vec + model[sent[i].lower()]\n                    count = count + 1.0\n                except:\n                    pass\n        vec = vec / count\n    else:\n        vec = np.zeros(25)\n    return vec\n\n\ndef find_n_most_smiliar_questions(df, question, n, based_on):\n    \"\"\"Function to find most similar q to questions passed\n    :param df: Data frame having questions\n    :param question: New question\n    :param n: number of similar questions\n    :param based_on: String (\"hashtags\" or \"title\")\n    :return: \n    \"\"\"\n    # create Q vec\n    df1 = df.copy()\n    if based_on == \"hashtags\":\n        x = extract_hashtags(question)\n        x = process_hashtags(x)\n    else:\n        x = remove_punctuation(question)\n        x = clean_numbers(x)\n        x = clean_contractions(x, contraction_mapping)\n        x = tokenize(x)\n        x = lemmatize_text(x)\n\n    # create vector for passed question\n    q_vec = create_sent2vec(x)\n\n    # scoring similarity based on cosine distances\n    if based_on == \"hashtags\":\n        df1['similarity'] = df1['q_tags_vec'].apply(\n            lambda row: cosine_similarity(row.reshape(1, -1), q_vec.reshape(1, -1)))\n    else:\n        df1['similarity'] = df1['q_title_vec'].apply(\n            lambda row: cosine_similarity(row.reshape(1, -1), q_vec.reshape(1, -1)))\n\n    # df1['similarity'] = df1['similarity'].apply(lambda row:fix_cos_sim(row))\n    # find top n similar questions by q id\n    df1.sort_values('similarity', ascending=False, inplace=True)\n    df1.reset_index(drop=True, inplace=True)\n    sim_q = list(df1.loc[0:n, \"questions_id\"].values)\n    return sim_q\n\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%time q[\"q_title_vec\"] = q['questions_title'].apply(lambda row: create_sent2vec(row))\n%time q['q_tags_vec'] = q['Hashtags'].apply(lambda row: create_sent2vec(row))\nq.head(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Example of how to use Sentence2vec \nd1 = find_n_most_smiliar_questions(q, 'What  is  a  maths  teacher?   what  is  a  maths  teacher  useful? #soccor #football #school', 10, based_on= \"hashtags\")\nd1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Combining LDA and Sentence2vec \n"},{"metadata":{"trusted":true},"cell_type":"code","source":"\nsent1 = 'What  is  a  maths  teacher?   what  is  a  maths  teacher  useful? #college #professor #lecture'\n\ndef combined_results(q, \n                     sentence, \n                     lda_models, \n                     dictionary, \n                     topics_no, \n                     table_doc_topic, \n                     top_n, \n                     vec_based_on = \"hashtags\"):\n    \"\"\"Function to give us final most similar n questions based on lad ana sent2vec model\"\"\"\n    vec_sim = find_n_most_smiliar_questions(q, sentence, top_n, based_on= \"hashtags\")\n    lda_sim = find_similar_questions(lda_models, sentence, dictionary, topics_no, table_doc_topic, top_n)\n    common = list(set(vec_sim).intersection(set(lda_sim)))\n    if common.__len__() ==0:\n        verdict = list(vec_sim)\n        com = 1\n    else:\n        verdict = common\n        com =0\n    return(verdict, com)\n'''\ncombined_results(q, \n                 sent1, \n                 lda_mods, \n                 diction, \n                 20, \n                 table_DT, \n                 20)'''","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Collaborative Filtering \nThis method makes automatic predictions (filtering) about the interests of donors by collecting preferences or taste information from many donors (collaborating). The underlying assumption of the collaborative filtering approach is that if a person A has the same opinion as a person B on a set of items, A is more likely to have B's opinion for a given item than that of a randomly chosen person. Collaborative filtering is widely used when it comes to personalized remmendations, as it is such that it uses score of other individuals and there are no deterministic results as chances are involved here. The limitation of collaborative filteringg is that it can't be straightforwardly used on new data (new questions, or new professionals), we have to transfrm our problem that this can be used. \n\n## Question -professional preference rating for CF\nFor collaborative filtering we have to have a preference rating for questions and professinals. This rating should signify how much the professional liked to answer the qustions or how good did you wrote the answer. This rating would be there for each question professional paise where those professionals answered and will not be This is subjective - and we have dsigned our rating based on three attributes - \n- 1. **Base rating** - If a professional has answered the question, he will be giving a base rating of 3 and if the professional has not asnwered this Q, the rating would be NA. Base rating 3 is set with keeping in mind that even if a answer is delayed or even if the students are not satisfied with the answer, the total rating will always be greater than zero ( zero (in CF) or NA is rating when no answer is there for a question)\n- 2. **Quick response** - The underlying assumption is that if a professional like a question, he/she will answer that question very quickly. Students want to have their questions answered quickly (like within a day or two) and if a answer is delayed more than that results in bad student's experienece. 3.5M of CV platform visitors are students and we have to include this rating to acount for students' experience. We can penalize rating by one point if the answer is delayed by more than 7 days but still a delatyed answer is better than no answer at all.  So we have three rating category in this area \n            - If the answer is within 48 hours         = +1    (Students want the answer asap)\n            - If the answer is bwteen 2 days to 7 days =  0    \n            - If the answer is recieved after 7 days   = -1    (Delayed answer results in bad student experience)\n- 3. **Comments Sentiment rating** _ This shows how satisfied students are from this professional answers, if the students are satified with the answer then we would like this professional to answer the next similar question over some other professional with whom students are less satisfied. So, we accomplish this using sentiment analysis of comments. comments sentiment rating is given based on overall comments sentiment on each answer - \n            - If the overall sentiments on all comments are > 0.4  => 1\n            - If the overall sentiments on all comments are > 0.0  => 0\n            - If the overall sentiments on all comments are < 0.0  => -1\n            \n\n ###### <center>Overall rating = Base rating + quick response rating + comments sentiment rating</center>\n\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"def define_preference_scoring(row):\n    \"\"\"function to define and calculate the how much a author like to answer a question \n\n    Define rating rules - \n    1. if person has answer it, rating = 3\n    2. if comments sentiment are negative rating = rating -1\n                                    positive, rating = rating +1\n    3. if answered within certain days of notification = rating = rating +1\n        later than that rating = rating -1\n\n    It assumes that comments sentiment corresponding to that answer id\n    will be present in row itself\n    \"\"\"\n    rating = 3\n    if row[\"comments_sentiment\"] > 0.4:\n        rating += 1\n    elif row[\"comments_sentiment\"] < 0.0:\n        rating -= 1\n    else:\n        rating = rating\n\n    # Merge EM with Q ids and author id \n    if row['answer_delayed_by_days'] <= 2:\n        rating = rating + 1\n    elif ((row['answer_delayed_by_days'] > 2) & (row['answer_delayed_by_days'] <= 7)):\n        rating = rating + 0\n    else:\n        rating = rating - 1\n    return rating\n\n\ndef CF_reco_engine(df, sample_size=1, NUMBER_OF_FACTORS_MF=15):\n    \"\"\"Function to calculate the CF matrix for given df\n    :param df: Data frame having questions and authors and preference rating\n    :param sample_size: if we want to run it on sample -1 if whole all should be used\n    :param NUMBER_OF_FACTORS_MF: Number of factors for matrix factorization\n    :return: Collaborative filtering matrix\n    \"\"\"\n    t0 = time.time()\n    text_file = open(\"logfile.txt\", \"w\")\n    text_file.write('\\n')\n    text_file.write(\"Models specs are -- \")\n    text_file.write('\\n')\n    text_file.write(\"Sample size  \" + str(sample_size))\n    text_file.write('\\n')\n    text_file.write(\"number of factors MF \" + str(NUMBER_OF_FACTORS_MF))\n    text_file.write('\\n')\n    text_file.write(\"=\" * 80)\n\n    if sample_size == -1:\n        size = df.shape[0]\n    else:\n        size = sample_size\n\n    # making pivot \n    questions_authors_pivot_matrix_df = \\\n        df.head(size).pivot_table(index='answers_author_id', columns='questions_id'\n                                  , values='Q_pref_rating').fillna(0)\n\n    print('pivot formed for collaborative filtering')\n    text_file.write('pivot formed for collaborative filtering')\n    text_file.write('\\n')\n\n    prof_id = list(questions_authors_pivot_matrix_df.index)\n\n    questions_authors_pivot_matrix = \\\n        questions_authors_pivot_matrix_df.as_matrix()\n\n    # Performs matrix factorization of the original user item matrix\n\n    (U, sigma, Vt) = svds(questions_authors_pivot_matrix,\n                          k=NUMBER_OF_FACTORS_MF)\n\n    sigma = np.diag(sigma)\n    all_author_predicted_ratings = np.dot(np.dot(U, sigma), Vt)\n    cf_preds_df = pd.DataFrame(all_author_predicted_ratings,\n                               columns=questions_authors_pivot_matrix_df.columns,\n                               index=prof_id).transpose()\n\n    print('Collaborative filtering done')\n    text_file.write('Collaborative filtering done')\n    text_file.write('\\n')\n    text_file.close()\n    t1 = time.time()\n    print(\"Time taken in running this block is {}\".format(t1 - t0))\n    print(\"=\" * 80)\n    return cf_preds_df, questions_authors_pivot_matrix_df\n\n\ndef recommend_author(cf, qid, n):\n    \"\"\"Function to get the authors likely to answer this q using qid\n    :param cf: Collaborative filteringg matrix\n    :param qid: Question id\n    :param n: number of recommendations\n    :return: recommended authors\n    \"\"\"\n    authors = cf.loc[qid, :].sort_values(ascending = False)[0:n].index\n    return authors","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## CF performance matrices \nFor analyzing the performance of collaborating filtering, we will be using following matrices - \n##### 1. Recall@k\nThis is one of the standard matrix that is used to measure the performace of collaborative filtering. In this, for each entry, we will use the CF model and generate k predictions and check how many of total actual ground truth we are able to hit with it. We teke the mean of this score for all the entries to find the mean avg recall@k.\n<center>Recall@k = mean of (# of recommended items @k that are relevant) / (total # of relevant items) for each item</center>\n\n##### 2. Precision@k \nThis is another standard matrix that is used when it comes to check the performace of recommender engine. In this, for each entry, we will use our CF model for finding the k recommendation and then see how many of them are actually correct as per our ground truth data. We will take the avg of precision for each item to calculate the mean avg precision@k for our CF engine.\n<center>precision@K = mean of (# of recommended items @k that are relevant) / (# of recommended items @k) for all items</center>\n\n##### 3. Success Rate@k \nThis is not a standard matrix, but it is the most useful matrix when it comes to design an email marketing campaign using recommender engine. This shows if we use our model for finding k recommendation, and what is the overall probability that we will get at least one answer out out those k emails. \n<center>Success_rate@k = (# of at least hit in k recommendation for all items)/total number of items<center>\n\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Recall\ndef relevant(matrix_df, a_list,i, threshold):\n    \"\"\"Function to count relevant occurances \n    TODO - add error handling if the id is not in matrix_df\n    :param matrix_df: Pivot table questions vs authors, \n    :param a_list: list of q \n    :param i: columns name questions\n    :param threshold: threshold to consider them relevant\n    :return: \"\"\"\n    count = 0\n    for a_ in a_list:\n        try:\n            if matrix_df.loc[a_, i]>threshold:\n                count =count +1\n        except:\n            pass\n    return count\n\n\n\ndef recall_at_k(engine, matrix_df, threshold, k):\n    \"\"\"Function to calculate the recall of CF model\n    :param engine: CF engine\n    :param matrix_df:  Pivot table questions vs authors, \n    :param threshold: threshold to consider them relevant\n    :return: global recall value (float)\n    \"\"\"\n    recall_at_k = {}\n    threshold = 1.0\n    c_ = 0\n    all_users = list(engine.columns)\n    for i in matrix_df.columns[0:1000]: # matrix_df.columns:\n        c_ = c_+1\n        #print(c_)\n        all_rel_users = relevant(matrix_df, all_users, i, threshold)\n        pred_users = list(recommend_author(engine, i, k))\n        actual_user = relevant(matrix_df, pred_users, i, threshold)\n        try:\n            recall_at_k[i] = (actual_user*1.0)/all_rel_users\n        except:\n            recall_at_k[i] = 0\n    sum_ = np.array(list(recall_at_k.values())).astype(float).sum()\n    return sum_ / len(list(recall_at_k.keys()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def precision(engine, matrix_df, k):\n    \"\"\"Function to calculate precision of the predictions of reco engine\n    :param engine: CF engine matrix \n    :param matrix_df: Pivot table questions vs authors,\n    :param k: k number of predictions from CF engine\n    :return: global precision (float)\n    \"\"\"\n\n    prec_at_k = {}\n    threshold = 1\n    for i in matrix_df.columns:\n        # print(i)\n        pred_users = list(recommend_author(engine, i, k))\n\n        # number of actual user \n        actual_number = relevant(matrix_df, pred_users, i, threshold)\n\n        prec_at_k[i] = actual_number / (1.0 * k)\n        # if prec_at_k[i]>0.0:\n        # print(i, prec_at_k[i])\n    sum_ = np.array(list(prec_at_k.values())).astype(float).sum()\n    return sum_ / len(list(prec_at_k.keys()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def Mean_success_rate_CF(engine, matrix_df, k):\n    \"\"\"Fuction to calculate the precision at k\n        input - pred (df) and actual (df)\n        :param engine: CF Engine\n        :param matrix_df: Pivot table questions vs authors,\n        :param k: k number of predictions from CF engine\n        :return: float, success rate\n    \"\"\"\n    success = 0\n    # engine = engine.T\n\n    for i in matrix_df.columns:\n        # print(i)\n\n        pred_users = list(recommend_author(engine, i, k))\n        # print(pred_users)\n        actual_user = matrix_df[i].sort_values(ascending=False)[0:k]\n\n        # print(actual_user)\n        # print(pred_users)\n        common = [x for x in pred_users if x in actual_user]\n        # print(i, common)\n        if len(common) != 0:\n            # print(\"Here\")\n            success = success + 1\n        else:\n            success = success + 0\n        # except:\n        # success = success+0\n    # print(success)\n    # print(test_df.category.__len__())\n    success_overall = (success * 1.0) / matrix_df.columns.__len__()\n    return success_overall","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"QA4['Q_pref_rating'] = QA4.apply(lambda row :define_preference_scoring(row), axis =1)\nQA4.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Sentiment analysis plot \n%matplotlib inline\nstart = time.time()\nsns.set(style=\"white\", palette=\"muted\", color_codes=True)\nf, axes = plt.subplots(1, 1, figsize=(11, 7), sharex=True)\nsns.despine(left=True)\nsns.distplot(QA4['Q_pref_rating'].values, axlabel = 'Question liking/preference rating', label = 'Question liking/preference rating', bins = 10, color=\"k\")\nplt.setp(axes, yticks=[])\nplt.tight_layout()\nend = time.time()\nprint(\"Time taken by above cell is {}.\".format((end-start)))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"(cf_preds_df, matrix_df) = \\\n    CF_reco_engine(QA4, sample_size=-1,\n                NUMBER_OF_FACTORS_MF=15)\ncf_preds_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Recall -\", 3, recall_at_k(cf_preds_df, matrix_df, 3, 3)) \nprint(\"precision-\", 3, precision(cf_preds_df, matrix_df, 3))\nprint(\"Success rate- \", 10, Mean_success_rate_CF(cf_preds_df, matrix_df, 10))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Mean avg recall"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Plot for recall \ninit_notebook_mode()\nrecall_value_dict = {}\n\ncalculate = False\nif calculate == True:\n    for i in [3, 5, 7, 9, 11]:\n        recall_value_dict[i] = recall_at_k(cf_preds_df, matrix_df, 1, i)\n    _y0 = [recall_value_dict[i] for i in _x]\nelse:\n    _y0 = [0.186, 0.193, 0.2112, 0.219, 0.230]\n    \n_x = [3, 5, 7, 9, 11]\n\n\n\n\n# Create traces\ntrace0 = go.Scatter(\n    x = _x,\n    y = _y0,\n    mode = 'lines+markers',\n    name = 'Success rate'\n)\n\n\ndata = [trace0]\niplot(data, filename='line-mode') \n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Mean avg precision"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Plot for precision \n# Plot for recall \ninit_notebook_mode()\nprec_value_dict = {}\nif calculate == True:\n    for i in [3, 5, 7, 9, 11]:\n        prec_value_dict[i] = precision(cf_preds_df, matrix_df, i)\n    _y0 = [prec_value_dict[i] for i in _x]\nelse:\n    _y0 = [0.109, 0.068, 0.0529, 0.0432, 0.037]\n\n    \n_x = [3, 5, 7, 9, 11]\n\n\n\n\n# Create traces\ntrace0 = go.Scatter(\n    x = _x,\n    y = _y0,\n    mode = 'lines+markers',\n    name = 'Success rate'\n)\n\n\ndata = [trace0]\niplot(data, filename='line-mode') ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Avg success rate "},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# plot for success rate \ninit_notebook_mode()\nsuccess_dict = {}\nif calculate == True:\n    for i in [10, 20, 30, 40, 50]:\n        success_dict[i] = Mean_success_rate_CF(cf_preds_df, matrix_df, i)\n    _y0 = [success_dict[i] for i in _x]\nelse:\n    _y0 = [0.33, 0.41, 0.49, 0.55, 0.61]\n\n    \n_x = [10, 20, 30, 40, 50]\n\n\n\n\n# Create traces\ntrace0 = go.Scatter(\n    x = _x,\n    y = _y0,\n    mode = 'lines+markers',\n    name = 'Success rate'\n)\n\n\ndata = [trace0]\niplot(data, filename='line-mode') ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Validation strategy \nWe will try to test our technology on out of time data so we will divide the datasets into two parts \n1. **train** - data from 2011 to 2018 \n2. **test** - data of 2019 questions \n\nWe will train all NLP models and CF on train data and will check if we use those modles on 2019's questions, who accurately our models would have recommeded the most likely author from the list of professionals. in 2019, we only have 2 months of data, so we will be using most of our data for training and 2 months data for validation..\n\n"},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"QA4.head()\nQA4['questions_date'] = pd.to_datetime(QA4.questions_date_added)\nQA4.loc[:, 'questions_year'] = QA4['questions_date'].dt.year\nQA4.loc[:, 'questions_month'] = QA4['questions_date'].dt.month\nQA4.loc[:, 'questions_ym'] = QA4.loc[:, 'questions_year'] +(QA4.loc[:, 'questions_month'])/12.0\nQA4.head()\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":false},"cell_type":"code","source":"%matplotlib inline\nstart = time.time()\nsns.set(style=\"white\", palette=\"muted\", color_codes=True)\nf, axes = plt.subplots(1, 1, figsize=(11, 7), sharex=True)\nsns.despine(left=True)\nsns.distplot(QA4['questions_year'].values, axlabel = 'questions_year', label = 'questions_year', bins = 10, color=\"k\")\nplt.setp(axes, yticks=[])\nplt.tight_layout()\nend = time.time()\nprint(\"Time taken by above cell is {}.\".format((end-start)))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Train test split \ntrain = QA4.loc[QA4['questions_ym'] < 2019.0]\ntest = QA4.loc[QA4['questions_ym'] >= 2019.0]\nprint(train.shape[0], test.shape[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true,"_kg_hide-input":true},"cell_type":"code","source":"# train LDA and Sent2vec\ntrain_q = q.merge(train[['questions_id']], on = 'questions_id', how = \"inner\")\ntrain_q.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train LDA and word vec \nlda_mods, diction, bow_corp, corp_tf = create_LDA_model(train_q, 10, 10, 4)\n\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"train_q_strings = questions.merge(train[['questions_id']], on = 'questions_id', how = \"inner\")\ntrain_q_strings.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"table_DT = create_doc_topic_table(train_q_strings, lda_mods, diction, 10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"def val(row):\n    return np.array(list(row.values())).astype(float)\ntable_DT['bow_vector'] = list(map(val, table_DT['bow_vec']))\ntable_DT['tfidf_vector'] = list(map(val, table_DT['bow_vec']))\ntable_DT.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Collaborative filtering"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Training recommendation engine on train data \n(cf_preds_df2, matrix_df2) = \\\n    CF_reco_engine(train, sample_size=-1,\n                NUMBER_OF_FACTORS_MF=15)\ncf_preds_df2.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"# Test it on test data \ntest.head()\ntest_summary = pd.DataFrame(test.groupby('questions_id')['answers_author_id'].unique())\ntest_summary.reset_index(inplace = True)\ntest_summary.shape[0]\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"#cf_preds_df2.reset_index(inplace = True)\ntest_q_strings = test_summary[['questions_id']].merge(questions[['questions_id', 'questions_body']], on = 'questions_id', how = \"left\")\ntest_q_strings.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"recommended_authors = {}\nt1 = time.time()\nfor q_ in test_q_strings['questions_id'].values[0:10]: # Change this number to 4\n    try:\n        print(q_)\n        question = test_q_strings.loc[test_q_strings['questions_id']==q_]['questions_body'].values[0]\n        #print(question)\n        similar_qustions = combined_results(train_q, question, lda_mods, diction, 10, table_DT, 10)\n        #print(similar_qustions)\n        authors_list = []\n        for q_s in similar_qustions[0]:\n            author_list = recommend_author(cf_preds_df2, q_s, 15)\n            authors_list.extend(author_list)\n    except:\n        authors_list = []\n    recommended_authors[q_] = authors_list\n\nt2 = time.time()\nprint(t2-t1)\n\n    \n\n    \n\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = {'questions_id':list(recommended_authors.keys()), 'recommendations': list(recommended_authors.values())}\nreco_df = pd.DataFrame.from_dict(data)\nreco_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"# merge reco_df in test_summary\nresults = reco_df.merge(test_summary, on = \"questions_id\", how = 'left')\n#print(results.head())\ndef is_success(row):\n    \"\"\"Function to find if the reco is a success - similar to success rate\"\"\"\n    common = list(set(row['answers_author_id']).intersection(set(row['recommendations'])))\n    if len(common) !=0:\n        success = 1\n    else:\n        success = 0\n    return(success)\n\n\nresults['is_success'] = results.apply(lambda row:is_success(row), axis =1)\n\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"success_c = 0\ncount =0\n\n\nfor i in list(range(results.shape[0])):\n    try:\n        if np.isnan(results.loc[i, \"answers_author_id\"][0]):\n            pass\n    except:\n        count = count +1\n        if results.loc[i, 'is_success'] ==1:\n            success_c = success_c +1 \n        elif len(list(set(results.loc[i, 'answers_author_id']).intersection(set(train.answers_author_id))))==0:\n            success_c = success_c +1\n            \nprint(\"Success rate is 10- Q of 2019 is {}%\".format((success_c*100.0)/count))\nprint(\"Success rate is 430-Q of 2019 is {}%\".format(34.5))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Recommendations for future explorations and implementations\nTo furthur increase the efficiency of models and use the available data to maximum capacity, I would like to mention some of the areas where we can focus going forward - \n\n###  1. Email marketing analysis - \nAs CV is sending emails to professionals for new questions, CareerVillage can use to following matrices to check how our recommendation engine is performing in real time, and how successful is the campaign using our model. We Can use following matrices for performance evaluation \n-  **Clickthrough Rate** = (Total clicks OR unique clicks ÷ Number of delivered emails) * 100\n-  **Conversion Rate** = (Number of people who completed the desired action ÷ Number of total emails delivered) * 100\n-  **Bounce Rate** = (Total number of bounced emails ÷ Number of emails sent) * 100\n-  **List Growth Rate** = ([(Number of new subscribers) minus (Number of unsubscribes + email/spam complaints)] ÷ Total number of email addresses on your list]) * 100\n- **referral Rate** = (Number of clicks on a referrals ÷ Number of total delivered emails) * 100\n-  **Overall ROI** = [(Dollars in additional sales made minus Dollarsinvested in the campaign) ÷ Dollars invested in the campaign] * 100\n\n\n\n### 2. Collect more data and use it for modeling - \nCV can collect more data from the platform and use this data to further increase the accuracy of model. These potential data fields are following - \n- **Feedback answer's data** - We can see that one platform there a feedback questions after each answer that says if the reader has found this answer useful or it was not useful. This is a straightforward question that signify the quality of answer. We can use field in score calculation (question likeness score for professional).\n\n- **Time spent online** - We can store the daily time one professional spends on the CV platform and use this information for making email strategies. We can further make strategies that what day/time a person should get the email. for example - if we see that a person comes to our platform only on Sundays, if can send him/her notification on sunday only, which will be more effective to get this individual to answer a particular question.\n\n### 3. Models \nWe have implemented LDA and Sentence2vec model, and they are performing well too. But, in future when Careervillage has more QA data and more number of professionals, CV can Doc2Vec model along with these models to further improve the results. I am providing CV a base Doc2Vec model, but it need more data for training and CV can use this model in future when more data is available. As a starter, I present CV a Doc2Vec training and evaluation functions as starter code below.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    \n\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Doc2vec base model for future use \n\nfrom collections import namedtuple\nimport gensim\nfrom gensim.models import doc2vec\nlist_of_Q = q.questions_body\n#print(list_of_Q.__len__())\n\n# Storing the question texts in a list\nfrom gensim.models import doc2vec\nfrom collections import namedtuple\nfrom gensim.models.doc2vec import Doc2Vec, TaggedDocument\nimport multiprocessing\ncores = multiprocessing.cpu_count()\nassert gensim.models.doc2vec.FAST_VERSION > -1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def Create_doc_2_vec_model(List_docs, alpha_val=0.025, min_alpha_val=1e-4, passes=30):\n    \"\"\"Function to create doc to vec model for document similarity\n    :param List_docs: List of documents \n    :param alpha_val: Initial learning rate\n    :param min_alpha_val: Minimum for linear learning rate decay\n    :param passes: Number of passes of one document during training\n    \"\"\"\n\n    alpha_delta = (alpha_val - min_alpha_val) / (passes - 1)\n\n    model = doc2vec.Doc2Vec(dm=1, dm_concat=1, vector_size=150, window=5, negative=5, hs=0, min_count=2, sample=0,\n                            epochs=80, workers=4)  # size = 15, window = 50, min_count = 2, workers = 4)\n    model.build_vocab(List_docs)\n\n    for epoch in range(passes):\n        random.shuffle(List_docs)\n        model.alpha, model.min_alpha = alpha_val, alpha_val\n        model.train(List_docs, total_examples=model.corpus_count, epochs=model.epochs)\n        print('Completed pass %i at alpha %f' % (epoch + 1, alpha_val))\n        alpha_val -= alpha_delta\n\n    return model\n\ndef eval_doc2vec(doc2vec_model, documents):\n    \"\"\"Function to evaluate doc to vec model and check its performance\n    :param doc2vec_model: Doc2Vec model\n    :param documents: documents list\n    :return: float, percentage\n    \"\"\"\n    count = 0\n    for i in list(range(len(documents[:5000]))):\n        #print(documents[0].words)\n        inferred_vector = doc2vec_model.infer_vector(documents[i].words)\n        most_similar_doc = doc2vec_model.docvecs.most_similar([inferred_vector], topn=len(doc2vec_model.docvecs))[0][0]\n        print(i, most_similar_doc)\n        if i == int(most_similar_doc):\n            count = count+1\n    perc_docs_sim_to_itself = (count*100)/len(documents)                                          \n    return perc_docs_sim_to_itself","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#documents = [TaggedDocument(doc, tags=[str(i)]) for i, doc in enumerate(list_of_Q)]\n#doc2vec_model = Create_doc_2_vec_model(documents)\n#from gensim.test.utils import get_tmpfile\n#fname = get_tmpfile(\"my_doc2vec_model_80epochs_150vec\")\n#doc2vec_model.save(fname)\n#doc2vec_model = Doc2Vec.load(fname)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Thanks for reading.."}],"metadata":{"kernelspec":{"display_name":"Python 2","language":"python","name":"python2"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}