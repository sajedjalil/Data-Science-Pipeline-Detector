{"cells":[{"metadata":{},"cell_type":"markdown","source":"# RAKE (RAPID AUTOMATIC KEYWORD EXTRACTION ALGORITHM) Implemenation","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\"\"\"Implementation of Rapid Automatic Keyword Extraction algorithm.\nAs described in the paper `Automatic keyword extraction from individual\ndocuments` by Stuart Rose, Dave Engel, Nick Cramer and Wendy Cowley.\n\"\"\"\nimport re\nimport string\nfrom collections import Counter, defaultdict\nfrom itertools import chain, groupby, product\n\nimport nltk\nfrom enum import Enum\nfrom nltk.tokenize import wordpunct_tokenize\n\n\nclass Metric(Enum):\n    \"\"\"Different metrics that can be used for ranking.\"\"\"\n\n    DEGREE_TO_FREQUENCY_RATIO = 0  # Uses d(w)/f(w) as the metric\n    WORD_DEGREE = 1  # Uses d(w) alone as the metric\n    WORD_FREQUENCY = 2  # Uses f(w) alone as the metric\n\n\nclass Rake(object):\n    \"\"\"Rapid Automatic Keyword Extraction Algorithm.\"\"\"\n\n    def __init__(\n        self,\n        stopwords=None,\n        punctuations=None,\n        language=\"english\",\n        ranking_metric=Metric.DEGREE_TO_FREQUENCY_RATIO,\n        max_length=4,\n        min_length=1,\n    ):\n        \"\"\"Constructor.\n        :param stopwords: List of Words to be ignored for keyword extraction.\n        :param punctuations: Punctuations to be ignored for keyword extraction.\n        :param language: Language to be used for stopwords\n        :param max_length: Maximum limit on the number of words in a phrase\n  \n        :param min_length: Minimum limit on the number of words in a phrase\n\n        \"\"\"\n        # By default use degree to frequency ratio as the metric.\n        if isinstance(ranking_metric, Metric):\n            self.metric = ranking_metric\n        else:\n            self.metric = Metric.DEGREE_TO_FREQUENCY_RATIO\n\n        # If stopwords not provided we use language stopwords by default.\n        self.stopwords = stopwords\n        if self.stopwords is None:\n            self.stopwords = []\n            stop_word_file='/kaggle/input/smartstoplists/SmartStoplist.txt'\n            for line in open(stop_word_file):\n                if line.strip()[0:1] != \"#\":\n                    for word in line.split():  # in case more than one per line\n                        self.stopwords.append(word)\n                #self.stopwords = nltk.corpus.stopwords.words(language)\n\n\n        # If punctuations are not provided we ignore all punctuation symbols.\n        self.punctuations = punctuations\n        if self.punctuations is None:\n            self.punctuations = string.punctuation\n\n        # All things which act as sentence breaks during keyword extraction.\n        self.to_ignore = set(chain(self.stopwords, self.punctuations))\n\n        # Assign min or max length to the attributes\n        self.min_length = min_length\n        self.max_length = max_length\n\n        # Stuff to be extracted from the provided text.\n        self.frequency_dist = None\n        self.degree = None\n        self.rank_list = None\n        self.ranked_phrases = None\n\n    def extract_keywords_from_text(self, text):\n        \"\"\"Method to extract keywords from the text provided.\n        :param text: Text to extract keywords from, provided as a string.\n        \"\"\"\n        sentences = nltk.tokenize.sent_tokenize(text)\n        self.extract_keywords_from_sentences(sentences)\n\n    def extract_keywords_from_sentences(self, sentences):\n        \"\"\"Method to extract keywords from the list of sentences provided.\n        :param sentences: Text to extraxt keywords from, provided as a list\n                          of strings, where each string is a sentence.\n        \"\"\"\n        sentence_delimiters = re.compile(u'[\\\\[\\\\]\\n.!?,;:\\t\\\\-\\\\\"\\\\(\\\\)\\\\\\'\\u2019\\u2013]')\n        sentences = sentence_delimiters.split(text)\n        phrase_list = self._generate_phrases(sentences)\n        self._build_frequency_dist(phrase_list)\n        self._build_word_co_occurance_graph(phrase_list)\n        self._build_ranklist(phrase_list)\n\n    def get_ranked_phrases(self):\n        \"\"\"Method to fetch ranked keyword strings.\n        :return: List of strings where each string represents an extracted\n                 keyword string.\n        \"\"\"\n        return self.ranked_phrases\n\n    def get_ranked_phrases_with_scores(self):\n        \"\"\"Method to fetch ranked keyword strings along with their scores.\n        \"\"\"\n        return self.rank_list\n\n    def get_word_frequency_distribution(self):\n        \"\"\"Method to fetch the word frequency distribution in the given text.\n        \"\"\"\n        return self.frequency_dist\n\n    def get_word_degrees(self):\n        \"\"\"Method to fetch the degree of words in the given text. Degree can be\n        defined as sum of co-occurances of the word with other words in the\n        given text.\n        \"\"\"\n        return self.degree\n\n    def _build_frequency_dist(self, phrase_list):\n        \"\"\"Builds frequency distribution of the words in the given body of text.\n        :param phrase_list: List of List of strings where each sublist is a\n                            collection of words which form a contender phrase.\n        \"\"\"\n        self.frequency_dist = Counter(chain.from_iterable(phrase_list))\n\n    def _build_word_co_occurance_graph(self, phrase_list):\n        \"\"\"Builds the co-occurance graph of words in the given body of text to\n        compute degree of each word.\n        :param phrase_list: List of List of strings where each sublist is a\n                            collection of words which form a contender phrase.\n        \"\"\"\n        co_occurance_graph = defaultdict(lambda: defaultdict(lambda: 0))\n        for phrase in phrase_list:\n            # For each phrase in the phrase list, count co-occurances of the\n            # word with other words in the phrase.\n            #\n            # Note: Keep the co-occurances graph as is, to help facilitate its\n            # use in other creative ways if required later.\n            for (word, coword) in product(phrase, phrase):\n                co_occurance_graph[word][coword] += 1\n        self.degree = defaultdict(lambda: 0)\n        for key in co_occurance_graph:\n            self.degree[key] = sum(co_occurance_graph[key].values())\n\n    def _build_ranklist(self, phrase_list):\n        \"\"\"Method to rank each contender phrase using the formula\n              phrase_score = sum of scores of words in the phrase.\n              word_score = d(w)/f(w) where d is degree and f is frequency.\n        :param phrase_list: List of List of strings where each sublist is a\n                            collection of words which form a contender phrase.\n        \"\"\"\n        self.rank_list = []\n        for phrase in phrase_list:\n            rank = 0.0\n            for word in phrase:\n                if self.metric == Metric.DEGREE_TO_FREQUENCY_RATIO:\n                    rank += 1.0 * self.degree[word] / self.frequency_dist[word]\n                elif self.metric == Metric.WORD_DEGREE:\n                    rank += 1.0 * self.degree[word]\n                else:\n                    rank += 1.0 * self.frequency_dist[word]\n            self.rank_list.append((rank, \" \".join(phrase)))\n        self.rank_list.sort(reverse=True)\n        self.ranked_phrases = [ph[1] for ph in self.rank_list]\n\n    def _generate_phrases(self, sentences):\n        \"\"\"Method to generate contender phrases given the sentences of the text\n        document.\n        :param sentences: List of strings where each string represents a\n                          sentence which forms the text.\n        :return: Set of string tuples where each tuple is a collection\n                 of words forming a contender phrase.\n        \"\"\"\n        phrase_list = set()\n        # Create contender phrases from sentences.\n        for sentence in sentences:\n            word_list = [word.lower() for word in wordpunct_tokenize(sentence)]\n            phrase_list.update(self._get_phrase_list_from_words(word_list))\n        return phrase_list\n\n    def _get_phrase_list_from_words(self, word_list):\n        \"\"\"Method to create contender phrases from the list of words that form\n        a sentence by dropping stopwords and punctuations and grouping the left\n        words into phrases. Only phrases in the given length range (both limits\n        inclusive) would be considered to build co-occurrence matrix.\n        :param word_list: List of words which form a sentence when joined in\n                          the same order.\n        :return: List of contender phrases that are formed after dropping\n                 stopwords and punctuations.\n        \"\"\"\n        groups = groupby(word_list, lambda x: x not in self.to_ignore)\n        phrases = [tuple(group[1]) for group in groups if group[0]]\n        return list(\n            filter(\n                lambda x: self.min_length <= len(x) <= self.max_length, phrases\n            )\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"r = Rake()\ntext='A compiler is a computer program that translates computer code written in one programming language (the source language) into another language (the target language). The name compiler is primarily used for programs that translate source code from a high-level programming language to a lower level language (e.g., assembly language, object code, or machine code) to create an executable program.A compiler is likely to perform many or all of the following operations: preprocessing, lexical analysis, parsing, semantic analysis (syntax-directed translation), conversion of input programs to an intermediate representation, code optimization and code generation. Compilers implement these operations in phases that promote efficient design and correct transformations of source input to target output. Program faults caused by incorrect compiler behavior can be very difficult to track down and work around; therefore, compiler implementers invest significant effort to ensure compiler correctness.'\nr.extract_keywords_from_text(text)\nr.get_ranked_phrases_with_scores()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# TextRank Algorithm Implementation","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from collections import OrderedDict\nimport numpy as np\nimport spacy\nfrom spacy.lang.en.stop_words import STOP_WORDS\n\nnlp = spacy.load('en_core_web_sm')\n\nclass TextRank4Keyword():\n    \"\"\"Extract keywords from text\"\"\"\n    \n    def __init__(self):\n        self.d = 0.85 # damping coefficient, usually is .85\n        self.min_diff = 1e-5 # convergence threshold\n        self.steps = 10 # iteration steps\n        self.node_weight = None # save keywords and its weight\n\n    \n    def set_stopwords(self, stopwords):  \n        \"\"\"Set stop words\"\"\"\n        for word in STOP_WORDS.union(set(stopwords)):\n            lexeme = nlp.vocab[word]\n            lexeme.is_stop = True\n    \n    def sentence_segment(self, doc, candidate_pos, lower):\n        \"\"\"Store those words only in cadidate_pos\"\"\"\n        sentences = []\n        for sent in doc.sents:\n            selected_words = []\n            for token in sent:\n                # Store words only with cadidate POS tag\n                if token.pos_ in candidate_pos and token.is_stop is False:\n                    if lower is True:\n                        selected_words.append(token.text.lower())\n                    else:\n                        selected_words.append(token.text)\n            sentences.append(selected_words)\n        return sentences\n        \n    def get_vocab(self, sentences):\n        \"\"\"Get all tokens\"\"\"\n        vocab = OrderedDict()\n        i = 0\n        for sentence in sentences:\n            for word in sentence:\n                if word not in vocab:\n                    vocab[word] = i\n                    i += 1\n        return vocab\n    \n    def get_token_pairs(self, window_size, sentences):\n        \"\"\"Build token_pairs from windows in sentences\"\"\"\n        token_pairs = list()\n        for sentence in sentences:\n            for i, word in enumerate(sentence):\n                for j in range(i+1, i+window_size):\n                    if j >= len(sentence):\n                        break\n                    pair = (word, sentence[j])\n                    if pair not in token_pairs:\n                        token_pairs.append(pair)\n        return token_pairs\n        \n    def symmetrize(self, a):\n        return a + a.T - np.diag(a.diagonal())\n    \n    def get_matrix(self, vocab, token_pairs):\n        \"\"\"Get normalized matrix\"\"\"\n        # Build matrix\n        vocab_size = len(vocab)\n        g = np.zeros((vocab_size, vocab_size), dtype='float')\n        for word1, word2 in token_pairs:\n            i, j = vocab[word1], vocab[word2]\n            g[i][j] = 1\n            \n        # Get Symmeric matrix\n        g = self.symmetrize(g)\n        \n        # Normalize matrix by column\n        norm = np.sum(g, axis=0)\n        g_norm = np.divide(g, norm, where=norm!=0) # this is ignore the 0 element in norm\n        \n        return g_norm\n\n    \n    def get_keywords(self, number=10):\n        \"\"\"Print top number keywords\"\"\"\n        node_weight = OrderedDict(sorted(self.node_weight.items(), key=lambda t: t[1], reverse=True))\n        for i, (key, value) in enumerate(node_weight.items()):\n            print(key + ' - ' + str(value))\n            if i > number:\n                break\n        \n        \n    def analyze(self, text, \n                candidate_pos=['NOUN', 'PROPN'], \n                window_size=4, lower=False, stopwords=list()):\n        \"\"\"Main function to analyze text\"\"\"\n        \n        # Set stop words\n        self.set_stopwords(stopwords)\n        \n        # Pare text by spaCy\n        doc = nlp(text)\n        \n        # Filter sentences\n        sentences = self.sentence_segment(doc, candidate_pos, lower) # list of list of words\n        \n        # Build vocabulary\n        vocab = self.get_vocab(sentences)\n        \n        # Get token_pairs from windows\n        token_pairs = self.get_token_pairs(window_size, sentences)\n        \n        # Get normalized matrix\n        g = self.get_matrix(vocab, token_pairs)\n        \n        # Initionlization for weight(pagerank value)\n        pr = np.array([1] * len(vocab))\n        \n        # Iteration\n        previous_pr = 0\n        for epoch in range(self.steps):\n            pr = (1-self.d) + self.d * np.dot(g, pr)\n            if abs(previous_pr - sum(pr))  < self.min_diff:\n                break\n            else:\n                previous_pr = sum(pr)\n\n        # Get weight for each node\n        node_weight = dict()\n        for word, index in vocab.items():\n            node_weight[word] = pr[index]\n        \n        self.node_weight = node_weight","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tr4w = TextRank4Keyword()\ntr4w.analyze(text, candidate_pos = ['NOUN', 'PROPN'], window_size=4, lower=False)\ntr4w.get_keywords(10)\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Keyword Extraction from pdfs\n\n    Keywords are extracted from the pdfs in the directory /kaggle/input/fileuploads\n\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install fitz","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install PyMuPDF","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import sys\nimport fitz\nimport os\nimport re \nimport csv\nimport json\n\n     \nwith open('keywords.csv', 'w', newline='') as file:\n    writer = csv.writer(file)\n    writer.writerow([\"filename\", \"filetype\", \"keyword\"])\n\nfor root, dirs, files in os.walk(\"/kaggle/input/fileuploads\"):\n    for filename in files:\n        path='/kaggle/input/fileuploads/'+filename\n        try:\n            doc = fitz.open(path)  \n            i=1\n            keywordList = []\n            print(filename+\" : \")\n            for page in doc:\n                #using RAKE\n                pgno=str(i)\n                print(\"page :\",pgno)\n                text=page.getText()\n                r = Rake()\n                r.extract_keywords_from_sentences(text)\n                keywords=r.get_ranked_phrases_with_scores()\n                if(len(keywords)) > 15:\n                    for j in range(15):\n                        tmp=keywords[j]\n                        key=re.sub(r'[^\\w]', ' ',str(tmp[1])) \n                \n                        keywordList.append(key)\n                        #print(str(key))\n                else:\n                    if len(keywords)!=0:\n                        for j in keywords:\n                            key=re.sub(r'[^\\w]', ' ',str(j[0])) \n                            \n                            keywordList.append(key)\n                            #print(key)\n                \n                print(keywordList)\n                print(len(keywordList))\n                #using textRank\n                tr4w = TextRank4Keyword()\n                tr4w.analyze(text, candidate_pos = ['NOUN', 'PROPN'], window_size=4, lower=False)\n                node_weight = OrderedDict(sorted(tr4w.node_weight.items(), key=lambda t: t[1], reverse=True))\n                kcount=0\n                for k, (key, value) in enumerate(node_weight.items()):\n                    if kcount >= 10:\n                        break\n                    if key not in keywordList:\n\n                        keywordList.append(key)\n                    kcount+=1\n                    len(keywordList)\n                    \n                \n                i+=1\n            doc.close()\n            print(keywordList)\n            print(len(keywordList))\n            stringlist=json.dumps(keywordList)\n\n            with open('keywords.csv', 'a', newline='') as file:  \n                writer = csv.writer(file)\n                writer.writerow([filename, \"pdf\", stringlist])\n            \n        except Exception as e:\n            emsg=str(e)\n            doc.close()\n            print(e)\n                        \n\n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nimport cv2\nimport tempfile\nimport subprocess\nimport json\ndef ocr(path):\n    temp = tempfile.NamedTemporaryFile(delete=False)\n\n    process = subprocess.Popen(['tesseract', path, temp.name], stdout=subprocess.PIPE, stderr=subprocess.STDOUT)\n    process.communicate()\n\n    with open(temp.name + '.txt', 'r') as handle:\n        contents = handle.read()\n\n    os.remove(temp.name + '.txt')\n    os.remove(temp.name)\n\n    return contents\nfor root, dirs, files in os.walk(\"/kaggle/input/abcdefg\"):\n    for filename in files:\n        path='/kaggle/input/abcdefg/test.mp4'\n        filename = 'test.mp4'\n        cam = cv2.VideoCapture(path)\n        currentframe = 0\n        ar=[]\n        \n        while(True): \n          # reading from frame \n                ret,frame = cam.read() \n                if ret: \n                    if currentframe%100==0:\n                        # if video is still left continue creating images \n                        name='frame.jpg'\n                        # writing the extracted images \n                        cv2.imwrite(name, frame)\n                        #img=cv2.imread('./frame.jpg')\n                        #text=pytesseract.image_to_string(img)\n                        text=ocr(name)\n                        #print(text)\n                        i=1\n                        print(\"Extracting frame: \"+str(currentframe))\n                        r = Rake()\n                        r.extract_keywords_from_sentences(text)\n                        keywords=r.get_ranked_phrases_with_scores()\n                        i+=1\n                        if(len(keywords)) > 5:\n                            for j in range(5):\n                                tmp=keywords[j]\n                                print(str(tmp))\n                                key=re.sub(r'[^\\w]', ' ',str(tmp[1]))         \n                                if key not in ar:\n                                    \n                                    ar.append(key)\n                        else:\n                            if len(keywords)!=0:\n                                for j in keywords:\n                                    print(str(j))\n                                    key=re.sub(r'[^\\w]', ' ',str(j[1]))\n                                    if key not in ar:\n                                        \n                                        ar.append(key)\n\n\n\n                    currentframe =currentframe+1\n\n                else:\n                    stringlist=json.dumps(ar)\n\n                    with open('keywords.csv', 'a', newline='') as file:  \n                        writer = csv.writer(file)\n                       \n                        writer.writerow([filename, \"video\", stringlist])\n                    print(ar)\n                    break\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import subprocess\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"# Install bert-as-service\n!pip install bert-serving-server\n!pip install bert-serving-client","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Download and unzip the pre-trained model\n!wget http://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip\n!unzip uncased_L-12_H-768_A-12.zip","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Start the BERT server\nbert_command = 'bert-serving-start -model_dir /kaggle/working/uncased_L-12_H-768_A-12'\nprocess = subprocess.Popen(bert_command.split(), stdout=subprocess.PIPE)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Start the BERT client\nfrom bert_serving.client import BertClient\nbc = BertClient()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nstop_words=[]\nstop_word_file='/kaggle/input/smartstoplists/SmartStoplist.txt'\nfor line in open(stop_word_file):\n    if line.strip()[0:1] != \"#\":\n        for word in line.split():  # in case more than one per line\n            stop_words.append(str(word))                             \nkeywords_extracted=[\"princess\", \"chess\", \"onam\", \"mahabali\",\"football\"]\nqn=\"fifa\"\nquery_vec = np.zeros((768,))\nkeyvector = np.zeros((768,))\n                             \nnumber = 0\nembeddings = bc.encode([qn])\n\n            \nquery_vec = embeddings[0]\nqnvector=np.array(query_vec)\nm=-1\nmkey=keywords_extracted[0]\nembeddings = bc.encode(keywords_extracted)\nfor i in range(len(embeddings)):\n        keyvector = embeddings[i]   \n        cosinesim=np.sum(qnvector*keyvector)/(np.sqrt(np.sum(qnvector**2))*np.sqrt(np.sum(keyvector**2)))\n        if cosinesim > m:\n            m=cosinesim\n            mkey=keywords_extracted[i]\nprint(mkey)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\n\nstop_words=[]\nstop_word_file='/kaggle/input/smartstoplists/SmartStoplist.txt'\nfor line in open(stop_word_file):\n    if line.strip()[0:1] != \"#\":\n        for word in line.split():  # in case more than one per line\n            stop_words.append(str(word))\ncol_names = ['filename','filetype','keyword']\ndf = pd.read_csv(\"keywords.csv\", names=col_names)\nfilenames = (df.filename).tolist()\nfiletypes = (df.filetype).tolist()\n#keywords_extracted = (df.keyword).tolist()\nkeywords_extracted = df['keyword']\n\nprint (keywords_extracted)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pickle\n\nwith open(\"../input/pickled-glove840b300d-for-10sec-loading/glove.840B.300d.pkl\",\"rb\") as f:\n    embeddings_dict_glove = pickle.load(f)\n    \nprint(len(embeddings_dict_glove))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport json\ndocNum=0\nkeyvector1 = np.zeros((300,))\nkeyvectors=['glove_vectors',]\nfor doc in keywords_extracted:\n   \n    #doc now hast the list of keywords of File \n    keyvectorsofdoc=[]\n\n    if(docNum!=0):\n        #convert the set of keyphrases stored as string back to list\n        lst = json.loads(doc)\n       \n        #glove\n        #go through all the keyphrases corresponding to docNUM \n        for phrase in lst:\n            \n            for word in phrase.split():\n                if word not in stop_words:\n                    if embeddings_dict_glove.get(word) is not None:\n                        #print(\"word: \", word)\n                        keyvector1 = embeddings_dict_glove.get(word)\n                        print(keyvector1)\n                        keyvectorsofdoc.append(keyvector1)\n        keyvectors.append(keyvectorsofdoc)\n        print(keyvectorsofdoc)\n            \n\n\n    docNum=docNum+1 \n\n           ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(keyvectors)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['glove_vectors']=keyvectors","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#BERT keyphrase vectors\ndocNum=0\nbertkeyvectors=['bertvectors']\nfor doc in keywords_extracted:\n    if docNum!=0:\n        #convert the set of keyphrases stored as string back to list\n        lst = json.loads(doc)\n        keyembeddings=bc.encode(lst)\n        #print(keyembeddings)\n        bertkeyvectors.append(keyembeddings)\n    docNum+=1\n        \n        \n        \n        \n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(bertkeyvectors)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['bertvectors']=bertkeyvectors","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndf.to_csv('keywords_with_vectors.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#col_names = ['filename','filetype','keyword','glovevectors','bervectors']\n#df=pd.read_csv(\"keywords_with_vectors.csv\", names=col_names)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import json\nqns= [\"what are autotrophs\",\n      \"How many planets in solar system\", \n      \"Which all are the animal species in forest\",\n      \"a game with bat and ball\",\n      \"J K Rowling's novel based film series\",\n      \"Famous monuments\",\n      \"What is Greek Mythology\",\n      \"Who is Princess of Wales\",\n      \"Russian Nuclear disaster\",\n      \"One Kerala festival\",\n      \"Virus infection 2019\",\n      \"A 8 8 indoor board game\",\n      \"Translates computer code written\",\n      \"First black president of United State \",\n      \"Nothern lights\",\n      \"Quit india movement\"]\n\nfilenames = (df.filename).tolist()\nfiletypes = (df.filetype).tolist()\nwith open('qnToPdfMap.csv', 'w', newline='') as file:\n    writer = csv.writer(file)\n    writer.writerow([\"Question\", \"FileName\", \"FileType\"])\n#BERT \nquery_vec = np.zeros((768,))\nkeyvector = np.zeros((768,))\n#get question vectors using bert\nqn_vectors= bc.encode(qns)\n                             \n    \n\nqNum = 0\ndocNum=0\n\n\n#get question vectors using GloVe\nquery_vec1 = np.zeros((300,))\nkeyvector1 = np.zeros((300,))\nqn_vectors1 = []\n\nfor qn in qns:\n    number = 0\n    for word in qn.split():\n        if word not in stop_words:\n            if embeddings_dict_glove.get(word) is not None:\n                query_vec1= query_vec1 + embeddings_dict_glove.get(word)\n                number = number + 1\n    if number!=0:\n        query_vec1 = query_vec1/number\n        #qnvector=np.array(query_vec)\n        qn_vectors1.append(query_vec1)\n\n    \n\n    \n\ngloveiterator=0\n    \n\nqNum = 0\ndocNum=0\n\nglovevectors=df['glove_vectors']\nbertvectors=df['bertvectors']\n\n\nfor qnvector in qn_vectors:\n    qnvector1=qn_vectors1[gloveiterator]\n    \n    print(\"Q: \",qns[qNum])\n    m=-1\n    m1=-1\n    m2=-1\n    docNum=0\n    #store mapped file using BERT avg cosine sim\n    mfilename='' \n    mfiletype=''\n    #store mapped file using BERT without avg cosine sim\n    mfilename1=''\n    mfiletype1=''\n    #store mapped file using GLOVE avg cosine sim\n    mfilename2=''\n    mfiletype2=''\n    docNum=0\n    \n   \n    for i in range(len(df)):\n        glovedoc=glovevectors[i]\n        #doc now hast the list of keywords of File \n        cosineSimSum2=0\n        numWords2=0\n        if(docNum!=0):\n            #convert the set of keyvectors stored as string back to list\n            #lst = json.loads(glovedoc)\n            #glove\n            #go through all the keyphrases corresponding to docNUM \n            for keyvector1 in glovedoc:\n                \n                                       \n                cosinesim = np.sum(qnvector1*keyvector1)/(np.sqrt(np.dot(qnvector1,qnvector1)) * np.sqrt(np.dot(keyvector1,keyvector1)))\n                cosineSimSum2 += cosinesim\n                numWords2 +=1\n                                    \n            if (numWords2!=0) and ((cosineSimSum2/numWords2) > m2):\n                m2 = cosineSimSum2/numWords2\n                mfilename2 = filenames[docNum]\n                mfiletype2 = filetypes[docNum]\n            #bert\n            bertofdoc=bertvectors[i]\n            #lst = json.loads(bertofdoc)\n            numWords=0\n            cosineSimSum=0\n            for keyvector in bertofdoc:               \n                #cosinesim = np.dot(qnvector, keyvector) / (np.sqrt(np.dot(qnvector,qnvector)) * np.sqrt(np.dot(keyvector,keyvector)))\n                cosinesim = np.sum(qnvector*keyvector)/(np.sqrt(np.sum(qnvector**2))*np.sqrt(np.sum(keyvector**2)))\n                if cosinesim > m1:\n                    m1=cosinesim\n                    #print(m)\n                    mfilename1 = filenames[docNum]\n                    #print(docNum)\n                    mfiletype1 = filetypes[docNum]\n\n                cosineSimSum += cosinesim\n                numWords +=1\n            if (numWords!=0) and ((cosineSimSum/numWords) > m):\n                m = cosineSimSum/numWords\n                mfilename = filenames[docNum]\n                mfiletype = filetypes[docNum]\n        docNum+=1\n        \n    mappedict={}\n    filenamewritten=''\n    filetypesdict={mfilename:mfiletype,\n                   mfilename1:mfiletype1,\n                   mfilename2:mfiletype2}\n    #print(filetypesdict)\n    cosinesimlist=[m,m1,m2]\n    fileslist=[mfilename,mfilename1,mfilename2]\n    filetypeslist=[mfiletype,mfiletype1,mfiletype2]\n    for mappedfile in fileslist:\n        if mappedfile in mappedict.keys():\n            mappedict[mappedfile]=mappedict[mappedfile]+1\n        else:\n            mappedict[mappedfile]=1\n    maxcnt=max(mappedict.values())\n    if maxcnt>=2:\n        for k,v in mappedict.items():\n            if v==maxcnt:\n                print(\"File: \",k)\n                filenamewritten=k\n                filetypewritten=filetypesdict[filenamewritten]\n                print(\"FileType: \",filetypewritten)\n                \n                \n                \n                \n                \n    else:\n        maxcosinesim=max(cosinesimlist)\n        for i in range(3):\n            if cosinesimlist[i]==maxcosinesim:\n                print(\"File: \",fileslist[i])\n                \n                filenamewritten=fileslist[i]\n                filetypewritten=filetypesdict[filenamewritten]\n                print(\"FileType: \",filetypewritten)\n                \n                \n    \n    '''   \n    print(m,m1,m2)\n    \n    print(\"File: \",mfilename)\n    print(\"File: \",mfilename1)\n    print(\"File: \",mfilename2)\n    '''\n    \n    print(\" \")\n    \n    \n    with open('qnToPdfMap.csv', 'a', newline='') as file:\n        writer = csv.writer(file)\n        writer.writerow([qns[qNum],filenamewritten, filetypewritten])\n    qNum = qNum + 1 \n    gloveiterator=gloveiterator+1\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\nimport json\n\nqns= [\"How many planets in solar system\", \n      \"Which all are the animal species in forest\",\n      \"a game with bat and ball\",\n      \"J K Rowling's novel based film series\",\n      \"Famous monuments\",\n      \"What is Greek Mythology\",\n      \"Who is Princess of Wales\",\n      \"Russian Nuclear disaster\",\n      \"One Kerala festival\",\n      \"Virus infection 2019\",\n      \"A 8 8 indoor board game\",\n      \"Translates computer code written\",\n      \"First black president of United State \",\n      \"Nothern lights\",\n      \"Quit india movement\"]\n\nwith open('qnToPdfMap.csv', 'w', newline='') as file:\n    writer = csv.writer(file)\n    writer.writerow([\"Question\", \"FileName\", \"FileType\"])\n#BERT \nquery_vec = np.zeros((768,))\nkeyvector = np.zeros((768,))\n#get question vectors using bert\nqn_vectors= bc.encode(qns)\n                             \n    \n\nqNum = 0\ndocNum=0\n\n\n#get question vectors using GloVe\nquery_vec1 = np.zeros((300,))\nkeyvector1 = np.zeros((300,))\nqn_vectors1 = []\n\nfor qn in qns:\n    number = 0\n    for word in qn.split():\n        if word not in stop_words:\n            if embeddings_dict_glove.get(word) is not None:\n                query_vec1= query_vec1 + embeddings_dict_glove.get(word)\n                number = number + 1\n    if number!=0:\n        query_vec1 = query_vec1/number\n        #qnvector=np.array(query_vec)\n        qn_vectors1.append(query_vec1)\n\n    \n\n    \n\ngloveiterator=0\n    \n\nqNum = 0\ndocNum=0\n\n\n\n\nfor qnvector in qn_vectors:\n    qnvector1=qn_vectors1[gloveiterator]\n    \n    print(\"Q: \",qns[qNum])\n    m=-1\n    m1=-1\n    m2=-1\n    docNum=0\n    #store mapped file using BERT avg cosine sim\n    mfilename='' \n    mfiletype=''\n    #store mapped file using BERT without avg cosine sim\n    mfilename1=''\n    mfiletype1=''\n    #store mapped file using GLOVE avg cosine sim\n    mfilename2=''\n    mfiletype2=''\n    docNum=0\n    \n   \n    for doc in keywords_extracted:\n        #doc now hast the list of keywords of File \n        cosineSimSum2=0\n        numWords2=0\n        if(docNum!=0):\n            #convert the set of keyphrases stored as string back to list\n            lst = json.loads(doc)\n            #glove\n            #go through all the keyphrases corresponding to docNUM \n            for phrase in lst:\n                for word in phrase.split():\n                            if word not in stop_words:\n                                if embeddings_dict_glove.get(word) is not None:\n                                    #print(\"word: \", word)\n                                    keyvector1 = embeddings_dict_glove.get(word)     \n                                    cosinesim = np.sum(qnvector1*keyvector1)/(np.sqrt(np.dot(qnvector1,qnvector1)) * np.sqrt(np.dot(keyvector1,keyvector1)))\n                                    cosineSimSum2 += cosinesim\n                                    numWords2 +=1\n                                    \n            if (numWords2!=0) and ((cosineSimSum2/numWords2) > m2):\n                m2 = cosineSimSum2/numWords2\n                mfilename2 = filenames[docNum]\n                mfiletype2 = filetypes[docNum]\n            #bert\n            keyembeddings=bc.encode(lst)\n            numWords=0\n            cosineSimSum=0\n            for keyvector in keyembeddings:               \n                #cosinesim = np.dot(qnvector, keyvector) / (np.sqrt(np.dot(qnvector,qnvector)) * np.sqrt(np.dot(keyvector,keyvector)))\n                cosinesim = np.sum(qnvector*keyvector)/(np.sqrt(np.sum(qnvector**2))*np.sqrt(np.sum(keyvector**2)))\n                if cosinesim > m1:\n                    m1=cosinesim\n                    #print(m)\n                    mfilename1 = filenames[docNum]\n                    #print(docNum)\n                    mfiletype1 = filetypes[docNum]\n\n                cosineSimSum += cosinesim\n                numWords +=1\n            if (numWords!=0) and ((cosineSimSum/numWords) > m):\n                m = cosineSimSum/numWords\n                mfilename = filenames[docNum]\n                mfiletype = filetypes[docNum]\n        docNum+=1\n        \n    mappedict={}\n    cosinesimlist=[m,m1,m2]\n    fileslist=[mfilename,mfilename1,mfilename2]\n    for mappedfile in fileslist:\n        if mappedfile in mappedict.keys():\n            mappedict[mappedfile]=mappedict[mappedfile]+1\n        else:\n            mappedict[mappedfile]=1\n    maxcnt=max(mappedict.values())\n    if maxcnt>=2:\n        for k,v in mappedict.items():\n            if v==maxcnt:\n                print(\"File: \",k)\n    else:\n        maxcosinesim=max(cosinesimlist)\n        for i in range(3):\n            if cosinesimlist[i]==maxcosinesim:\n                print(\"File: \",fileslist[i])\n                \n                \n    \n    \n    print(m,m1,m2)\n    \n    print(\"File: \",mfilename)\n    print(\"File: \",mfilename1)\n    print(\"File: \",mfilename2)\n  \n    \n    print(\" \")\n    \n    \n    with open('qnToPdfMap.csv', 'a', newline='') as file:\n        writer = csv.writer(file)\n        writer.writerow([qns[qNum], mfilename, mfiletype])\n    qNum = qNum + 1 \n    gloveiterator=gloveiterator+1\n'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\nfilename = \"keywords_squad.csv\"\n# opening the file with w+ mode truncates the file\nf = open(filename, \"w+\")\nf.close()\n'''","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Testing","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test=pd.read_csv('/kaggle/input/squad-csv-format/QA Dataset.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import json\nimport re \nimport csv\nsquad_questions=[]\nfor i in range(len(test)):\n    print(i)\n    row=test.iloc[i]\n    text=row[1]\n    squad_questions.append[]\n    keywordList=[]\n    r = Rake()\n    r.extract_keywords_from_sentences(text)\n    keywords=r.get_ranked_phrases_with_scores()\n    if(len(keywords)) > 15:\n        for j in range(15):\n            tmp=keywords[j]\n            key=re.sub(r'[^\\w]', ' ',str(tmp[1])) \n\n            keywordList.append(key)\n            #print(str(key))\n    else:\n        if len(keywords)!=0:\n            for j in keywords:\n                key=re.sub(r'[^\\w]', ' ',str(j[0])) \n                \n                keywordList.append(key)\n                #print(key)\n\n    print(keywordList)\n    print(len(keywordList))\n    #using textRank\n    tr4w = TextRank4Keyword()\n    tr4w.analyze(text, candidate_pos = ['NOUN', 'PROPN'], window_size=4, lower=False)\n    node_weight = OrderedDict(sorted(tr4w.node_weight.items(), key=lambda t: t[1], reverse=True))\n    kcount=0\n    for k, (key, value) in enumerate(node_weight.items()):\n        if kcount >= 10:\n            break\n        if key not in keywordList:\n\n            keywordList.append(key)\n            kcount+=1\n    len(keywordList)\n    stringlist=json.dumps(keywordList)\n    with open('keywords_squaad.csv', 'a', newline='') as file:  \n        writer = csv.writer(file)\n        writer.writerow([str(i), \"text\",stringlist])\n        \n    \n    \n    ","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}