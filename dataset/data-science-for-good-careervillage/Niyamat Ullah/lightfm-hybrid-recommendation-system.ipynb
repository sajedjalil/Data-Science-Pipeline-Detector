{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Introduction \n**Notebook Overview:** This notebook contains a solution for Career village recommendation system competition. In this notebook, I build a hybrid recommendation system for recommending students questions to professionals for CareerVillage.org. The recommender system works by matching professionals with questions by tags they follow, their previous answers' question tags and similar tags. Also, it overcomes some of the most highest rated problem for CareerVillage recommender system like cold-start and others.\n\n\n**Competition problem statements:** CareerVillage.org is a non-profit organization helping underserved youth to provide information to build their career. Students can ask their questions in the CareerVillage.org and professionals(expert people who love to help students) answer their questions. The challenge is that CareerVillage has to recommend correct questions to the professionals so that the questions match with the professional's interests. This will increase the likelihood of a question to get an answer. So in this competition, we have to make a recommendation system that will correctly recommend questions that will match with professionals interest. "},{"metadata":{},"cell_type":"markdown","source":"# Recommendation system\nBefore we deep dive into the solutions, let's first make sure we understand different terminology of recommendation system. By definition, A recommendation system is a system that identifies and provides recommended content or digital items for users by using users interests. Recommender systems have become an important feature in modern websites, e.g. in Amazon, Netflix, or Flickr. Click rates, revenues and other measures of success may be increased by the application of effective recommender systems.\n\nThe difficult task is to identify relevant items even if they are generally unpopular. Recommender systems leverage available context such as user information, time, location, etc. to filter relevant items. Thereby, also items from the tails of the popularity distribution are successfully recommended. [1]\n\n\n**Types of recommendation system:**\n1. **Collaborative Filtering:** Collaborative filtering is a method of making automatic predictions (filtering) about the interests of a user by collecting preferences or taste information from many users (collaborating). The underlying assumption of the collaborative filtering approach is that if a person A has the same opinion as a person B on an issue, A is more likely to have B's opinion on a different issue than that of a randomly chosen person. Below is a picture of how collaborative filtering works [5]. \n\n    ![](https://i.imgur.com/FZli7DC.gif)\n    Though collaborative filtering One major problem of collaborative filtering is \"cold start\". As we’ve seen, collaborative-filtering can be a powerful way of recommending items based on user history, but what if there is no user history? This is called the “cold start” problem, and it can apply both to new items and to new users. Items with lots of history get recommended a lot, while those without never make it into the recommendation engine, resulting in a positive feedback loop. At the same time, new users have no history and thus the system doesn’t have any good recommendations. Potential solution: Onboarding processes can learn basic info to jump-start user preferences, importing social network contacts [4].\n    \n\n\n\n2. **Content-Based Filtering:** These filtering methods are based on the description of an item and a profile of the user’s preferred choices. In a content-based recommendation system, keywords are used to describe the items; besides, a user profile is built to state the type of item this user likes. In other words, the algorithms try to recommend products which are similar to the ones that a user has liked in the past. The idea of content-based filtering is that if you like an item you will also like a ‘similar’ item. For example, when we are recommending the same kind of item like a movie or song recommendation. [2]\n\n    One major problem of this approach is the diversity. Relevance is important, but it’s not all there is. If you watched and liked Star Wars, the odds are pretty good that you’ll also like The Empire Strikes Back, but you probably don’t need a recommendation engine to tell you that. It’s also important for a recommendation engine to come up with results that are novel (that is, stuff the user wasn’t expecting) and diverse (that is, stuff that represents a broad selection of their interests). [3]\n\n \n3. **Hybrid recommender system:** Hybrid recommender system is a special type of recommender system that combines both content and collaborative filtering method. Combining collaborative filtering and content-based filtering could be more effective in some cases. Hybrid approaches can be implemented in several ways: by making content-based and collaborative-based predictions separately and then combining them; by adding content-based capabilities to a collaborative-based approach (and vice versa). Several studies empirically compare the performance of the hybrid with pure collaborative and content-based methods and demonstrate that hybrid methods can provide more accurate recommendations than pure approaches. These methods can also be used to overcome some of the common problems in recommender systems such as cold start and the sparsity problem.\n\n\n\n\n**Types of Data for building recommendation systems:** There are two kinds of data available for building a recommendation system. There are \n1. **Explicit feedback:** Explicit feedback is the data about user explicit feedback(ratings etc) about a product. It tells directly that users like a product or not. \n\n2. **Implicit feedback:** In implicit feedback, we don't have the data about how the user rates a product. Examples for implicit feedback are clicks, watched movies, played songs, purchases or assigned tags.\n\n\nNow we know what a recommended system is and it's different types. Next, we will look into the what method is more suitable for building a recommendation system for CareerVillage.org. "},{"metadata":{},"cell_type":"markdown","source":"# Choosing a Recommendation System for CareerVillage\nWe have to build a recommendation system for CareerVillage. So let's what data career village provides us. Career Village gives us dataset about professionals, professionals answered questions, students, students questions. So let's first decide what data do we have for building a recommender system. Is it implicit or explicit feedback? \n\nBecause we don't have any ratings or equivalent matrix for determining how professionals like a question, we get implicit feedback data. So we are using implicit feedback data for recommendation system. Examples of implicit feedback in CareerVillage is users answered questions' tags, location. \n\n\n**Which method should we choose?**: We can't choose collaborative filtering because this will cause a cold start because a large portion of professionals doesn't answer any questions yet and for new professionals. What about content filtering? This will also cause problems because we will not have diversity and most of them we don't have clear information about professionals for creating user feature matrix. \n\nSo what should we do? Well, we can use a hybrid model. It will combine both collaborative and content-based method and can make a robust recommendation by analysing professionals tags if they have and similar professionals interest. This will help us to solve both cold start and diversity problem. \n\nNow we see that a hybrid model is perfect for this problem. In the next section, we will see how we will build a hybrid recommendation system for CareerVillage. "},{"metadata":{},"cell_type":"markdown","source":"# LightFm hybrid recommender for CareerVillage\nA hybrid recommender is a special kind of recommender that uses both collaborative and content based filtering for making recommendations. Thats make hybrid recommender a very speacial and useful method for building recommendation system. But there are several techniques and methods for buidling hybrid recommender system. But in this project, I like to use **LightFM Hybrid model**. A hybrid matrix factorization model by Lyst. \n\n1. **What is LightFM?**: LightFM is a hybrid matrix factorisation model representing users and items as linear combinations of their content features’ latent factors. The model outperforms both collaborative and content-based models in cold-start or sparse interaction data scenarios (using both user and item metadata), and performs at least as well as a pure collaborative matrix factorisation model where interaction data is abundant.\n\n    In LightFM, like in a collaborative filtering model, users and items are represented as latent vectors (embeddings). However, just as in a CB model, these are entirely defined by functions (in this case, linear combinations) of embeddings of the content features that describe each product or user. \n\n    For example, if the movie ‘Wizard of Oz’ is described by the following features: ‘musical fantasy’, ‘Judy Garland’, and ‘Wizard of Oz’, then its latent representation will be given by the sum of these features’ latent representations. In doing so, LightFM unites the advantages of contentbased and collaborative recommenders. [6]\n\n\n2. **How LightFM works**: The [LightFM paper](https://arxiv.org/pdf/1507.08439.pdf) describes beautifully how lightFM works. To put it simply in words, lightFM model learns embeddings (latent representations in a high-dimensional space) for users and items in a way that encodes user preferences over items. When multiplied together, these representations produce scores for every item for a given user; items scored highly are more likely to be interesting to the user [5].\n\n    The user and item representations are expressed in terms of representations of their features: an embedding is estimated for every feature, and these features are then summed together to arrive at representations for users and items [5].\n    \n    The latent representation of user u is given by the sum of its features’ latent vectors:  \\\\(q_{u}= \\sum_{}^{j\\in f_{u}}\\\\).\n    \n    And same for the items:  \\\\(p_{i}= \\sum_{}^{j\\in f_{i}}\\\\)\n    \n    The model’s prediction for user u and item i is then given by the dot product of user and item representations, adjusted by user and item feature biases:\n    \\\\(\\hat{r_{ui}}= f\\left ( q_{u}\\cdot p_{i}+b_{u}+b_{i}  \\right )\\\\)\n    \n    This is just a general idea of the model. Please read the lightFM model paper more in depth knowledge. \n    \n    \n    \n    \n    \n\n3. **Why LightFM**: \n    * In both cold-start and low density scenarios, LightFM performs at least as well as pure content-based models, substantially outperforming them when either (1) collaborative information is available in the training set or (2) user features are included in the model. This is really useful for our CareerVillage recommendation system beacause we will have a lot of new questions and students that makes a very good environment for cold start problem. \n\n    * When collaborative data is abundant (warm-start, dense user-item matrix), LightFM performs at least as well as the MF model. \n\n    * Embeddings produced by LightFM encode important semantic information about features, and can be used for related recommendation tasks such as tag recommendations. This is also very important for our problem. Because there are useful for finding similar tags so that model can recommend questions that has similiar tags to professionals tags. \n\n"},{"metadata":{},"cell_type":"markdown","source":"# LightFM Python Library \nFortunely, there is a library that makes easy to build a lightFM model. LightFM model is developed by Lyst. They also created a library for building lightfm model. It is very popular on Github having 2400+ stars and 226 closed issues. Because it is well maintained by Lyst( a london based e-commerce compnay) and it's learning community, lightFM python library is a really good source for building lightFM model. \n\n**Benefit of LightFM python library**: We can oviously make our implementation of lightFM model. But that will be reinvented the wheel. Because lightFM library is really well maintained library that are used production by many well reputed brand (Lyst, Sketchfab). \n\nThe biggest benefit of lightfm library is that it implements **WARP (Weighted Approximate-Rank Pairwise) loss** for implicit feedback learning-to-rank. Wait! What is that? \n\nFor optimization of our matrix factorization function we can use different optimization methods e.g ALS, SGD. But there is another special optimization method called WARP (Weighted Approximate-Rank Pairwise). From the documentation, WARP works like these: \n1. For a given (user, positive item pair), sample a negative item at random from all the remaining items. Compute predictions for both items; if the negative item’s prediction exceeds that of the positive item plus a margin, perform a gradient update to rank the positive item higher and the negative item lower. If there is no rank violation, continue sampling negative items until a violation is found.\n\n2. If you found a violating negative example at the first try, make a large gradient update: this indicates that a lot of negative items are ranked higher than positives items given the current state of the model, and the model must be updated by a large amount. If it took a lot of sampling to find a violating example, perform a small update: the model is likely close to the optimum and should be updated at a low rate.\n\nIt performs very well for implicit feedback model. Do you remember that our data is impicit feedback! For that reason, WARP loss is very essential for CareerVillage recommender system. Becasue lightfm library implements this algorithm, that makes lightfm library really stands out. \n\n\n\nAlso, there are other important benefit also:\n1. LightFM is written in Cython and is paralellized via HOGWILD SGD. This will outperform any implementations of lightFM. \n2. Already battle tested by many developers. \n3. Already used in production by many well reputeted brand. \n4. It's API is really developer friendly. This makes building model really easy.\n5. Provide evaluation matrics for evaluating the perfomance of the model. \n6. Finally, it is very very fast. \n\nBuilding lightFM model from scracth by maintaing all of features above is really difficult and time consuming. For that reason, I am uisng LightFM python library for building the model.\n\n\n\n**How LightFM python library works**: This library makes really easy for building lightFM model. There are couple of steps for building model using LightFM library. \n1. Process our Data and Make a lightFM dataset by using it's api \n2. Build interaction matrix, user/item features \n3. Make a model and train the model \n4. Evaluate the model \n5. Make predictions \n\n\n**Want to learn more about LightFM library?**: If you want to deep dive how to use this library please visit it's [official page](https://lyst.github.io/lightfm/docs/index.html). You can find guide for how to use this library.\n\nNow we know which method and library to use for building our recommender model. Let's see why this solutions is effective. "},{"metadata":{},"cell_type":"markdown","source":"# Effectiveness of this solution\nNow I am going to describe the effectiveness of my solution to the CareerVillage recommendation problem. \n1. Solved major recommendation system problem for CareerVillage: \n    * **Cold Start**: LightFM handle cold start problem very excellently because of it's hybrid nature. LightFM uses item and user features for recommendation. For when interection doesn't found for new users, I mean for cold start scenario lightFM fallback to content or collaborative method depending on the available data. That's why lightFM outperforms cold start problem very well. \n    * **Questions rankings**: The objective for emailing questions to professionals is that the students can get answer quickly. So we have to make sure that undesereved question should have higher rank. For example, if a question have 10 answer most probably the students don't need additional answer. But those questions that have less that 3 answer neeed more disperate answer. \n    \n    That's why, I build the model that solved this problem by weighting the model so that the model provides less importance to the questions that have higher number answers. The weight is calculated using (```1/num_of_ans_per_ques```). And this weights are passed with every interection matrix. This makes sure that questions with less answers get boost in recommendation. \n    * **Similar tags**: Professional and questions can have multiple tags and most of the time tags are just same except for some letter difference. My model takes care of that. LightFM creates users and items features embedding that understand the similiarity between tags. This makes this model highly effiencient. Embeddings produced by LightFM encode important semantic information about features (tags). \n    \n    This features makes this model really extra edge. Because there are lot of tags. Most of the time students added tags in questions that is slightly different that professionals tags. But my model can pickup these and find similirities between those. \n    * **Scope for adding new features**: I have provided class and function for building dataset and model. They are designed in way so that new features can be implemented easily. So that CareerVillage can test new features. \n    * **Fast**: Because my model is build using LightFM library and using WARP loss, the speed of training and prediction is blazingly fast. \n\n2. **Ready for deploying in production**:\n    * I have provided step by step guildlines, function and example for deploying this model into production.\n    * I build re-usable function for easy usecase that will useful for production and adding additional features.\n    * All codes are well formated and documented. I provided commend on every single method, function and class so that anybody can understand the code and easy to implement.\n3. **Well documented**: \n    * Documented every thing from what is recommender system to model explaination to production steps.\n    * Written comment for most of the code for easy understandibility.\n    * I provided flowchart for better understanding the model structer and this notebook. \n    \nNow we are ready to build our model. Without further do let's start building. "},{"metadata":{},"cell_type":"markdown","source":"# Kernel Overview\nThis kernel is divided into three major parts. \n1. **Solutions overview**: This section is the above section that you read. This section contains information about recommendation system, different kinds of model, why we choose lightFM, and most importantly the effectiveness of this solutions. \n\n2. **Model Building**: In this section, we will be building our model step by step providing documentation for each. We will provide documenation for why we did certain things. This will help us to understand the model and implementation better. Here is the picture for how we will complete this steps: \n![](https://i.imgur.com/UyaNRZg.png)\n\n3. **Model in production**: This is a very important steps. In this steps, we are also going to make our model and make recommendations. But we will build class for each individuals steps that we build in step 2. And we will be building a pipeline that will makes really easy for putting this model into production. \n\n    You can say why you don't used these function and class in step 2. Well, because in steps 2, we are building model step by step that makes really easy for understanding the model. But in this steps we will put out model in production. The pipeline is same as described in step 2. Here is a simple flowcert for how this model can put into production:\n    ![](https://i.imgur.com/7fG6gvc.png)\n    \n    This is not a definite blueprint for how this model will put into produciton but this will give some idea for how this can implement. So without further talking let's move on to step 2 (Model building). \n    "},{"metadata":{},"cell_type":"markdown","source":"# Gathering Data \nCareerVillage provides us a very rich set of datasets for this competition. Dataset contains information about students, professionals, questions, answers, comment and specially tags. This competition already contains lot of great EDA and data analysis on this data. Feel free to look at those. This will give you very rich idea about the dataset."},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true,"_kg_hide-output":true,"_kg_hide-input":true},"cell_type":"code","source":"################################################\n# Importing necessary library\n################################################\nimport numpy as np\nimport pandas as pd\n\n# all lightfm imports \nfrom lightfm.data import Dataset\nfrom lightfm import LightFM\nfrom lightfm import cross_validation\nfrom lightfm.evaluation import precision_at_k\nfrom lightfm.evaluation import auc_score\n\n# imports re for text cleaning \nimport re\nfrom datetime import datetime, timedelta\n\n# we will ignore pandas warning \nimport warnings\nwarnings.filterwarnings('ignore')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"############################################\n# Read all our datasets and store them in pandas dataframe objects. \n############################################\nbase_path = '../input/'\ndf_answer_scores = pd.read_csv(\n    base_path + 'answer_scores.csv')\n\ndf_answers = pd.read_csv(\n    base_path + 'answers.csv',\n    parse_dates=['answers_date_added'])\n\ndf_comments = pd.read_csv(\n    base_path + 'comments.csv')\n\ndf_emails = pd.read_csv(\n    base_path + 'emails.csv')\n\ndf_group_memberships = pd.read_csv(\n    base_path + 'group_memberships.csv')\n\ndf_groups = pd.read_csv(\n    base_path + 'groups.csv')\n\ndf_matches = pd.read_csv(\n    base_path + 'matches.csv')\n\ndf_professionals = pd.read_csv(\n    base_path + 'professionals.csv',\n    parse_dates=['professionals_date_joined'])\n\ndf_question_scores = pd.read_csv(\n    base_path + 'question_scores.csv')\n\ndf_questions = pd.read_csv(\n    base_path + 'questions.csv',\n    parse_dates=['questions_date_added'])\n\ndf_school_memberships = pd.read_csv(\n    base_path + 'school_memberships.csv')\n\ndf_students = pd.read_csv(\n    base_path + 'students.csv',\n    parse_dates=['students_date_joined'])\n\ndf_tag_questions = pd.read_csv(\n    base_path + 'tag_questions.csv')\n\ndf_tag_users = pd.read_csv(\n    base_path + 'tag_users.csv')\n\ndf_tags = pd.read_csv(\n    base_path + 'tags.csv')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Defining our necessary functions \nBecause for easy and quick production, I have build functions for every major part of data pre-processing and model building. In this steps, I store all those functions without storing them spreading all over the notebook. I provide rich documentation for each function so that they will be easily understandable. "},{"metadata":{"trusted":true},"cell_type":"code","source":"def generate_int_id(dataframe, id_col_name):\n    \"\"\"\n    Generate unique integer id for users, questions and answers\n\n    Parameters\n    ----------\n    dataframe: Dataframe\n        Pandas Dataframe for Users or Q&A. \n    id_col_name : String \n        New integer id's column name.\n        \n    Returns\n    -------\n    Dataframe\n        Updated dataframe containing new id column \n    \"\"\"\n    new_dataframe=dataframe.assign(\n        int_id_col_name=np.arange(len(dataframe))\n        ).reset_index(drop=True)\n    return new_dataframe.rename(columns={'int_id_col_name': id_col_name})\n\n\n\ndef create_features(dataframe, features_name, id_col_name):\n    \"\"\"\n    Generate features that will be ready for feeding into lightfm\n\n    Parameters\n    ----------\n    dataframe: Dataframe\n        Pandas Dataframe which contains features\n    features_name : List\n        List of feature columns name avaiable in dataframe\n    id_col_name: String\n        Column name which contains id of the question or\n        answer that the features will map to.\n        There are two possible values for this variable.\n        1. questions_id_num\n        2. professionals_id_num\n\n    Returns\n    -------\n    Pandas Series\n        A pandas series containing process features\n        that are ready for feed into lightfm.\n        The format of each value\n        will be (user_id, ['feature_1', 'feature_2', 'feature_3'])\n        Ex. -> (1, ['military', 'army', '5'])\n    \"\"\"\n\n    features = dataframe[features_name].apply(\n        lambda x: ','.join(x.map(str)), axis=1)\n    features = features.str.split(',')\n    features = list(zip(dataframe[id_col_name], features))\n    return features\n\n\n\ndef generate_feature_list(dataframe, features_name):\n    \"\"\"\n    Generate features list for mapping \n\n    Parameters\n    ----------\n    dataframe: Dataframe\n        Pandas Dataframe for Users or Q&A. \n    features_name : List\n        List of feature columns name avaiable in dataframe. \n        \n    Returns\n    -------\n    List of all features for mapping \n    \"\"\"\n    features = dataframe[features_name].apply(\n        lambda x: ','.join(x.map(str)), axis=1)\n    features = features.str.split(',')\n    features = features.apply(pd.Series).stack().reset_index(drop=True)\n    return features\n\n\ndef calculate_auc_score(lightfm_model, interactions_matrix, \n                        question_features, professional_features): \n    \"\"\"\n    Measure the ROC AUC metric for a model. \n    A perfect score is 1.0.\n\n    Parameters\n    ----------\n    lightfm_model: LightFM model \n        A fitted lightfm model \n    interactions_matrix : \n        A lightfm interactions matrix \n    question_features, professional_features: \n        Lightfm features \n        \n    Returns\n    -------\n    String containing AUC score \n    \"\"\"\n    score = auc_score( \n        lightfm_model, interactions_matrix, \n        item_features=question_features, \n        user_features=professional_features, \n        num_threads=4).mean()\n    return score","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Preprocessing and feature creation \nData preprocessing is essential for every data science project. We need to clean and modified our data for our own usecases. Also, feature creation is very important. Because it's let's our model good and diverse prediction. "},{"metadata":{},"cell_type":"markdown","source":"**Generate numeric identifier**:\nLightFM python only except numeric id. But the data CareerVillage has provided us is contains uuid for identifying users and professionals and others. In this step, I will make unique identifier for each professionals, students, questions and answers. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# generating unique integer id for users and q&a\ndf_professionals = generate_int_id(df_professionals, 'professionals_id_num')\ndf_students = generate_int_id(df_students, 'students_id_num')\ndf_questions = generate_int_id(df_questions, 'questions_id_num')\ndf_answers = generate_int_id(df_answers, 'answers_id_num')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"#  df_answers.groupby(['answers_author_id'], sort=False).ngroup()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Merging Datasets**: This is one of the most important steps for our solution. Our professionals, students, q&a and tags are stored in seperate datasets. For purpose of model, we have to merge our datasets in very carefull way so that they are useful for our model. \n\n1. All tags (q&a) are stored in a separate dataset. So firstly we merge those tags with questions and answers datasets. \n2. Then, we merge answers with quesitons because one question can have multiple answers. "},{"metadata":{"trusted":true},"cell_type":"code","source":"###########################\n# merging dataset\n###########################\n\n# just dropna from tags \ndf_tags = df_tags.dropna()\ndf_tags['tags_tag_name'] = df_tags['tags_tag_name'].str.replace('#', '')\n\n\n# merge tag_questions with tags name\n# then group all tags for each question into single rows\ndf_tags_question = df_tag_questions.merge(\n    df_tags, how='inner',\n    left_on='tag_questions_tag_id', right_on='tags_tag_id')\ndf_tags_question = df_tags_question.groupby(\n    ['tag_questions_question_id'])['tags_tag_name'].apply(\n        ','.join).reset_index()\ndf_tags_question = df_tags_question.rename(columns={'tags_tag_name': 'questions_tag_name'})\n\n# merge tag_users with tags name \n# then group all tags for each user into single rows \n# after that rename the tag column name \ndf_tags_pro = df_tag_users.merge(\n    df_tags, how='inner',\n    left_on='tag_users_tag_id', right_on='tags_tag_id')\ndf_tags_pro = df_tags_pro.groupby(\n    ['tag_users_user_id'])['tags_tag_name'].apply(\n        ','.join).reset_index()\ndf_tags_pro = df_tags_pro.rename(columns={'tags_tag_name': 'professionals_tag_name'})\n\n\n# merge professionals and questions tags with main merge_dataset \ndf_questions = df_questions.merge(\n    df_tags_question, how='left',\n    left_on='questions_id', right_on='tag_questions_question_id')\ndf_professionals = df_professionals.merge(\n    df_tags_pro, how='left',\n    left_on='professionals_id', right_on='tag_users_user_id')\n\n# merge questions with scores \ndf_questions = df_questions.merge(\n    df_question_scores, how='left',\n    left_on='questions_id', right_on='id')\n# merge questions with students \ndf_questions = df_questions.merge(\n    df_students, how='left',\n    left_on='questions_author_id', right_on='students_id')\n\n\n\n# merge answers with questions \n# then merge professionals and questions score with that \ndf_merge = df_answers.merge(\n    df_questions, how='inner',\n    left_on='answers_question_id', right_on='questions_id')\ndf_merge = df_merge.merge(\n    df_professionals, how='inner',\n    left_on='answers_author_id', right_on='professionals_id')\ndf_merge = df_merge.merge(\n    df_question_scores, how='inner',\n    left_on='questions_id', right_on='id')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Generate some features**: In this steps, we are going to generate some features. We are going to generate ```number of answers by professionals```, ```num of answers in each question```, ```num of tags per professionals``` and ```number of tags per question```. I will not use all of these features in this model. But I will use ```number of answers per question``` for weighting our model so that our model pay less attention to those quesitons that have higher number of answers. "},{"metadata":{"trusted":true},"cell_type":"code","source":"#######################\n# Generate some features for calculates weights\n# that will use with interaction matrix \n#######################\n\ndf_merge['num_of_ans_by_professional'] = df_merge.groupby(['answers_author_id'])['questions_id'].transform('count')\ndf_merge['num_ans_per_ques'] = df_merge.groupby(['questions_id'])['answers_id'].transform('count')\ndf_merge['num_tags_professional'] = df_merge['professionals_tag_name'].str.split(\",\").str.len()\ndf_merge['num_tags_question'] = df_merge['questions_tag_name'].str.split(\",\").str.len()\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Maximum number of answer per question : \" + str(df_merge['num_ans_per_ques'].max()))\nprint(\"Maximum number of tags per professional : \" + str(df_merge['num_tags_professional'].max()))\nprint(\"Maximum number of tags per question : \" + str(df_merge['num_tags_question'].max()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Merge answered questions tags with professional's tags**: Professionals can follow some tags. But not all professional follow tags and most especially we see from EDA that sometime professionals answers questions that is not related to their tags. For that reason, I have merge questions tags that each professional has answered with professional tags. This makes our model more robust and context aware. "},{"metadata":{"trusted":true},"cell_type":"code","source":"########################\n# Merge professionals previous answered \n# questions tags into professionals tags \n########################\n\n# select professionals answered questions tags \n# and stored as a dataframe\nprofessionals_prev_ans_tags = df_merge[['professionals_id', 'questions_tag_name']]\n# drop null values from that \nprofessionals_prev_ans_tags = professionals_prev_ans_tags.dropna()\n# because professsionals answers multiple questions, \n# we group all of tags of each user into single row \nprofessionals_prev_ans_tags = professionals_prev_ans_tags.groupby(\n    ['professionals_id'])['questions_tag_name'].apply(\n        ','.join).reset_index()\n\n# drop duplicates tags from each professionals rows\nprofessionals_prev_ans_tags['questions_tag_name'] = (\n    professionals_prev_ans_tags['questions_tag_name'].str.split(',').apply(set).str.join(','))\n\n# finally merge the dataframe with professionals dataframe \ndf_professionals = df_professionals.merge(professionals_prev_ans_tags, how='left', on='professionals_id')\n\n# join professionals tags and their answered tags \n# we replace nan values with \"\"\ndf_professionals['professional_all_tags'] = (\n    df_professionals[['professionals_tag_name', 'questions_tag_name']].apply(\n        lambda x: ','.join(x.dropna()),\n        axis=1))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Handling null and duplicates values**: Now we want clean our data a little bit. We will handle null and duplicate values. Because if we don't remove that they will cause error and wrong prediction. Also, we will replace null values with generic name or value."},{"metadata":{"trusted":true},"cell_type":"code","source":"# handling null values \ndf_questions['score'] = df_questions['score'].fillna(0)\ndf_questions['score'] = df_questions['score'].astype(int)\ndf_questions['questions_tag_name'] = df_questions['questions_tag_name'].fillna('No Tag')\n# remove duplicates tags from each questions \ndf_questions['questions_tag_name'] = df_questions['questions_tag_name'].str.split(',').apply(set).str.join(',')\n\n\n# fill nan with 'No Tag' if any \ndf_professionals['professional_all_tags'] = df_professionals['professional_all_tags'].fillna('No Tag')\n# replace \"\" with \"No Tag\", because previously we replace nan with \"\"\ndf_professionals['professional_all_tags'] = df_professionals['professional_all_tags'].replace('', 'No Tag')\ndf_professionals['professionals_location'] = df_professionals['professionals_location'].fillna('No Location')\ndf_professionals['professionals_industry'] = df_professionals['professionals_industry'].fillna('No Industry')\n\n# remove duplicates tags from each professionals \ndf_professionals['professional_all_tags'] = df_professionals['professional_all_tags'].str.split(',').apply(set).str.join(',')\n\n\n\n# remove some null values from df_merge\ndf_merge['num_ans_per_ques']  = df_merge['num_ans_per_ques'].fillna(0)\ndf_merge['num_tags_professional'] = df_merge['num_tags_professional'].fillna(0)\ndf_merge['num_tags_question'] = df_merge['num_tags_question'].fillna(0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Building model in LightFM\nIn this steps, we are going to build our lighFM model using lightFM python library. Firstly, we have to create lightFM ```Dataset``` for our model. LightFM Datset class makes it really easy for us for creating ```interection matrix```, ```weights``` and ```user/item features```.\n* ```interection matrix```: It is a matrix that contains user/ item interections or professional/quesiton intereactions. \n* ```weights```: weight of interection matrix. Less weight means less importance to that interection matrix. \n* ```user/item features```: user/item features supplied as like this ```(user_id, ['feature_1', 'feature_2', 'feature_3'])```\n\nIf you want to how lightfm python library's dataset class works and how to use it, please go to this link [Building LightFM Datasets](http://https://lyst.github.io/lightfm/docs/examples/dataset.html). \n\nThen, after that we will be start building our LightFM model using LightFM class. LightFM class makes it really easy for making lightFM model. After that we will train our model by our data. "},{"metadata":{},"cell_type":"markdown","source":"**Creating features list for Dataset class**: LightFM library has a Dataset class that makes it really easy for building necessary information for model. But we have feed set of all professionals/questions unique ids and all questions and professional features list. This will create internel mapping for lightFM to use. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# generating features list for mapping \nquestion_feature_list = generate_feature_list(\n    df_questions,\n    ['questions_tag_name'])\n\nprofessional_feature_list = generate_feature_list(\n    df_professionals,\n    ['professional_all_tags'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# calculate our weight value \ndf_merge['total_weights'] = 1 / (\n    df_merge['num_ans_per_ques'])\n\n\n# creating features for feeding into lightfm \ndf_questions['question_features'] = create_features(\n    df_questions, ['questions_tag_name'], \n    'questions_id_num')\n\ndf_professionals['professional_features'] = create_features(\n    df_professionals,\n    ['professional_all_tags'],\n    'professionals_id_num')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**LightFM Dataset**: In this steps we are going to build lightfm datasets. And then we will be building our interactions matrix, weights and professional/question features. "},{"metadata":{"trusted":true},"cell_type":"code","source":"########################\n# Dataset building for lightfm\n########################\n\n# define our dataset variable\n# then we feed unique professionals and questions ids\n# and item and professional feature list\n# this will create lightfm internel mapping\ndataset = Dataset()\ndataset.fit(\n    set(df_professionals['professionals_id_num']), \n    set(df_questions['questions_id_num']),\n    item_features=question_feature_list, \n    user_features=professional_feature_list)\n\n\n# now we are building interactions matrix between professionals and quesitons\n# we are passing professional and questions id as a tuple\n# e.g -> pd.Series((pro_id, question_id), (pro_id, questin_id))\n# then we use lightfm build in method for building interactions matrix\ndf_merge['author_question_id_tuple'] = list(zip(\n    df_merge.professionals_id_num, df_merge.questions_id_num, df_merge.total_weights))\n\ninteractions, weights = dataset.build_interactions(\n    df_merge['author_question_id_tuple'])\n\n\n\n# now we are building our questions and professionals features\n# in a way that lightfm understand.\n# we are using lightfm build in method for building\n# questions and professionals features \nquestions_features = dataset.build_item_features(\n    df_questions['question_features'])\n\nprofessional_features = dataset.build_user_features(\n    df_professionals['professional_features'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Model building and training**: In ths steps, I am going to build lightfm model and then train the model. If you want to learn how to create lightfm model using this library please read this post [recommender for the Movielens dataset](https://lyst.github.io/lightfm/docs/examples/movielens_implicit.html). "},{"metadata":{"trusted":true},"cell_type":"code","source":"################################\n# Model building part\n################################\n\n# define lightfm model by specifying hyper-parametre\n# then fit the model with ineteractions matrix, item and user features \nmodel = LightFM(\n    no_components=150,\n    learning_rate=0.05,\n    loss='warp',\n    random_state=2019)\n\nmodel.fit(\n    interactions,\n    item_features=questions_features,\n    user_features=professional_features, sample_weight=weights,\n    epochs=5, num_threads=4, verbose=True)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Evaluating the performance of the model \nNow we have to evaluate our model to see it's performance. No matter how good your model is, if you can't evaluate your model correctly you can't imporove and trust your model. For recommendation problem, there is not very good matrics for evaluating. But luckily lightfm provides us a very rich set of evaluating matrics. In this steps, we will be calculating AUC scores for our model.\n\n**What is AUC score in lightfm library?**: It measure the ROC AUC metric for a model: the probability that a randomly chosen positive example has a higher score than a randomly chosen negative example. A perfect score is 1.0. \n\nLet's see what is our model score. "},{"metadata":{"trusted":true},"cell_type":"code","source":"calculate_auc_score(model, interactions, questions_features, professional_features)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Wow! That is really impresive. Over AUC is over 90 percent. That is really excellent. This tells us that the quality of our overall model is very good."},{"metadata":{},"cell_type":"markdown","source":"**Make real recommendations**: Now we already see how our model is by looking at AUC score. But now let's see some real example of recommendation. "},{"metadata":{"trusted":true},"cell_type":"code","source":"from IPython.display import display_html\ndef display_side_by_side(*args):\n    html_str=''\n    for df in args:\n        html_str+=df.to_html()\n    display_html(html_str.replace('table','table style=\"display:inline\"'),raw=True)\n\ndef recommend_questions(professional_ids):\n     \n    for professional in professional_ids:\n        # print their previous answered question title\n        previous_q_id_num = df_merge.loc[df_merge['professionals_id_num'] == professional][:3]['questions_id_num']\n        df_previous_questions = df_questions.loc[df_questions['questions_id_num'].isin(previous_q_id_num)]\n        print('Professional Id (' + str(professional) + \"): Previous Answered Questions\")\n        display_side_by_side(\n            df_previous_questions[['questions_title', 'question_features']],\n            df_professionals.loc[df_professionals.professionals_id_num == professional][['professionals_id_num','professionals_tag_name']])\n        \n        # predict\n        discard_qu_id = df_previous_questions['questions_id_num'].values.tolist()\n        df_use_for_prediction = df_questions.loc[~df_questions['questions_id_num'].isin(discard_qu_id)]\n        questions_id_for_predict = df_use_for_prediction['questions_id_num'].values.tolist()\n        \n        scores = model.predict(\n            professional,\n            questions_id_for_predict,\n            item_features=questions_features,\n            user_features=professional_features)\n        \n        df_use_for_prediction['scores'] = scores\n        df_use_for_prediction = df_use_for_prediction.sort_values(by='scores', ascending=False)[:8]\n        print('Professional Id (' + str(professional) + \"): Recommended Questions: \")\n        display(df_use_for_prediction[['questions_title', 'question_features']])\n    \n\n    \n\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"recommend_questions([1200 ,19897, 3])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Analysis**: Awesome! Finally we can see our recommendation by our model. Let's take some time to ponder over the recommendations. \n* For the first professionl (1200) has not answer any questions yet. But he/she follows some tags. Our model take those tags as features and predict questions that has similar tags. \n* For the second professionals (19897) answered one questions that has tag major. But in his profile he follows tags like creative works like arts, illustrator etc. So our model recommend questions that has creative tags like arts, illustrator because he follows more tags one creative works. \n* For the third professionals (3): answered questions that has tag forensic, criminal, science, justice, detective. From the tags we can get an idea of professionals interests. Our model also learn that. That's why it recommend items that has tags like law, criminal, detective. \n\nThis is just a simple exploration. Hope you get idea of the model recommendations. The model can survive cold-start, high-poularity problem. It also recommend those questions that has less answer because of its weights that I provided during training. Now we build our model and tested it. In the next section, we will look how we can put this model in production. "},{"metadata":{},"cell_type":"markdown","source":"# Model in Production\nWe previously saw how lightfm model works and build for this project. Well, now we are going build a pipeline that will help us for putting this model into production. We are going to build class for each steps discuss in step 2. Also, we are going to build some additional functions and methods that will add additional functionality to the model. \n\nHere is the picture of our pipeline: \n![](https://i.imgur.com/Kh4rVcL.png)\n\n\nWe will now build class for each of these steps. Without further do let's begin. "},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"############################################\n# Read all our datasets agian \n# and store them in pandas dataframe objects. \n############################################\nbase_path = '../input/'\ndf_answer_scores = pd.read_csv(\n    base_path + 'answer_scores.csv')\n\ndf_answers = pd.read_csv(\n    base_path + 'answers.csv',\n    parse_dates=['answers_date_added'])\n\ndf_comments = pd.read_csv(\n    base_path + 'comments.csv')\n\ndf_emails = pd.read_csv(\n    base_path + 'emails.csv')\n\ndf_group_memberships = pd.read_csv(\n    base_path + 'group_memberships.csv')\n\ndf_groups = pd.read_csv(\n    base_path + 'groups.csv')\n\ndf_matches = pd.read_csv(\n    base_path + 'matches.csv')\n\ndf_professionals = pd.read_csv(\n    base_path + 'professionals.csv',\n    parse_dates=['professionals_date_joined'])\n\ndf_question_scores = pd.read_csv(\n    base_path + 'question_scores.csv')\n\ndf_questions = pd.read_csv(\n    base_path + 'questions.csv',\n    parse_dates=['questions_date_added'])\n\ndf_school_memberships = pd.read_csv(\n    base_path + 'school_memberships.csv')\n\ndf_students = pd.read_csv(\n    base_path + 'students.csv',\n    parse_dates=['students_date_joined'])\n\ndf_tag_questions = pd.read_csv(\n    base_path + 'tag_questions.csv')\n\ndf_tag_users = pd.read_csv(\n    base_path + 'tag_users.csv')\n\ndf_tags = pd.read_csv(\n    base_path + 'tags.csv')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Data Processing Class**: Now we are going to build a class that will be used for data cleaning and processing specificly designed for CareerVillage Datasetes. I have provided details document and comment with each part of the code. This will help understanding the code and intention very well. "},{"metadata":{"trusted":true},"cell_type":"code","source":"class CareerVillageDataPreparation:\n    \"\"\"\n    Clean and process data CareerVillage Data. \n    \n    This class process data in a way that will be useful\n    for building lightFM dataset. \n    \"\"\"\n    \n    def __init__(self):\n        pass\n\n    def _assign_unique_id(self, data, id_col_name):\n        \"\"\"\n        Generate unique integer id for users, questions and answers\n\n        Parameters\n        ----------\n        data: Dataframe\n            Pandas Dataframe for Users or Q&A. \n        id_col_name : String \n            New integer id's column name.\n\n        Returns\n        -------\n        Dataframe\n            Updated dataframe containing new id column\n        \"\"\"\n        new_dataframe=data.assign(\n            int_id_col_name=np.arange(len(data))\n            ).reset_index(drop=True)\n        return new_dataframe.rename(columns={'int_id_col_name': id_col_name})\n\n    def _dropna(self, data, column, axis):\n        \"\"\"Drop null values from specific column\"\"\"\n        return data.dropna(column, axis=axis)\n\n    def _merge_data(self, left_data, left_key, right_data, right_key, how):\n        \"\"\"\n        This function is used for merging two dataframe.\n        \n        Parameters\n        -----------\n        left_data: Dataframe\n            Left side dataframe for merge\n        left_key: String\n            Left Dataframe merge key\n        right_data: Dataframe\n            Right side dataframe for merge\n        right_key: String\n            Right Dataframe merge key\n        how: String\n            Method of merge (inner, left, right, outer)\n            \n        \n        Returns\n        --------\n        Dataframe\n            A new dataframe merging left and right dataframe\n        \"\"\"\n        return left_data.merge(\n            right_data,\n            how=how,\n            left_on=left_key,\n            right_on=right_key)\n\n    def _group_tags(self, data, group_by, tag_column):\n        \"\"\"Grouop multiple tags into single rows sepearated by comma\"\"\"\n        return data.groupby(\n            [group_by])[tag_column].apply(\n            ','.join).reset_index()\n\n    def _merge_cv_datasets(\n        self,\n        professionals,students,\n        questions,answers,\n        tags,tag_questions,tag_users, questions_score):\n        \"\"\"\n        This function merges all the necessary \n        CareerVillage dataset in defined way. \n        \n        Parameters\n        ------------\n        professionals,students,\n        questions,answers,\n        tags,tag_questions,\n        tag_users,\n        questions_score: Dataframe\n            Pandas dataframe defined by it's name\n        \n        \n        Returns\n        ---------\n        questions, professionals: Dataframe\n            Updated dataframe after merge\n        merge: Dataframe\n            A new datframe after merging answers with questions\n        \"\"\"\n        \n        \n        # merge tag_questions with tags name\n        # then group all tags for each question into single rows\n        tag_question = self._merge_data(\n            left_data=tag_questions,\n            left_key='tag_questions_tag_id',\n            right_data=tags,\n            right_key='tags_tag_id',\n            how='inner')\n        tag_question = self._group_tags(\n            data=tag_question,\n            group_by='tag_questions_question_id',\n            tag_column='tags_tag_name')\n        \n        tag_question = tag_question.rename(\n            columns={'tags_tag_name': 'questions_tag_name'})\n        \n        # merge tag_users with tags name\n        # then group all tags for each user into single rows \n        # after that rename the tag column name\n        tags_pro = self._merge_data(\n            left_data=tag_users,\n            left_key='tag_users_tag_id',\n            right_data=tags,\n            right_key='tags_tag_id',\n            how='inner')\n        tags_pro = self._group_tags(\n            data=tags_pro,\n            group_by='tag_users_user_id',\n            tag_column='tags_tag_name')\n        tags_pro = tags_pro.rename(\n            columns={'tags_tag_name': 'professionals_tag_name'})\n        \n        # merge professionals and questions tags with main merge_dataset \n        questions = self._merge_data(\n            left_data=questions,\n            left_key='questions_id',\n            right_data=tag_question,\n            right_key='tag_questions_question_id',\n            how='left')\n        professionals = self._merge_data(\n            left_data=professionals,\n            left_key='professionals_id',\n            right_data=tags_pro,\n            right_key='tag_users_user_id',\n            how='left')\n        \n        # merge questions with scores \n        questions = self._merge_data(\n            left_data=questions,\n            left_key='questions_id',\n            right_data=questions_score,\n            right_key='id',\n            how='left')\n        \n        # merge questions with students\n        questions = self._merge_data(\n            left_data=questions,\n            left_key='questions_author_id',\n            right_data=students,\n            right_key='students_id',\n            how='left')\n        \n        # merge answers with questions\n        # then merge professionals and questions score with that\n        merge = self._merge_data(\n            left_data=answers,\n            left_key='answers_question_id',\n            right_data=questions,\n            right_key='questions_id',\n            how='inner')\n        \n        merge = self._merge_data(\n            left_data=merge,\n            left_key='answers_author_id',\n            right_data=professionals,\n            right_key='professionals_id',\n            how='inner')\n        \n        return questions, professionals, merge\n  \n    def _drop_duplicates_tags(self, data, col_name):\n        # drop duplicates tags from each row\n        return (\n            data[col_name].str.split(\n                ',').apply(set).str.join(','))\n\n\n    def _merge_pro_pre_ans_tags(self, professionals, merge):\n        ########################\n        # Merge professionals previous answered\n        # questions tags into professionals tags\n        ########################\n        \n        # select professionals answered questions tags\n        # and stored as a dataframe\n        professionals_prev_ans_tags = (\n            merge[['professionals_id', 'questions_tag_name']])\n        # drop null values from that\n        professionals_prev_ans_tags = professionals_prev_ans_tags.dropna()\n        \n        # because professsionals answers multiple questions,\n        # we group all of tags of each user into single row\n        professionals_prev_ans_tags = self._group_tags(\n            data=professionals_prev_ans_tags,\n            group_by='professionals_id',\n            tag_column='questions_tag_name')\n        \n        # drop duplicates tags from each professionals rows\n        professionals_prev_ans_tags['questions_tag_name'] = \\\n        self._drop_duplicates_tags(\n            professionals_prev_ans_tags, 'questions_tag_name')\n        \n        # finally merge the dataframe with professionals dataframe\n        professionals = self._merge_data(\n            left_data=professionals,\n            left_key='professionals_id',\n            right_data=professionals_prev_ans_tags,\n            right_key='professionals_id',\n            how='left')\n        \n        # join professionals tags and their answered tags \n        # we replace nan values with \"\"\n        professionals['professional_all_tags'] = (\n            professionals[['professionals_tag_name',\n                           'questions_tag_name']].apply(\n                lambda x: ','.join(x.dropna()),\n                axis=1))\n        return professionals\n\n    def prepare(\n        self,\n        professionals,students,\n        questions,answers,\n        tags,tag_questions,tag_users, questions_score):\n        \n        \"\"\"\n        This function clean and process \n        CareerVillage Data sets. \n        \"\"\"\n        \n        # assign unique integer id\n        professionals = self._assign_unique_id(\n            professionals, 'professionals_id_num')\n        students = self._assign_unique_id(\n            students, 'students_id_num')\n        questions = self._assign_unique_id(\n            questions, 'questions_id_num')\n        answers = self._assign_unique_id(\n            answers, 'answers_id_num')\n        \n        # just dropna from tags \n        tags = tags.dropna()\n        tags['tags_tag_name'] = tags['tags_tag_name'].str.replace(\n            '#', '')\n        \n        \n        # merge necessary datasets\n        df_questions, df_professionals, df_merge = self._merge_cv_datasets(\n            professionals,students,\n            questions,answers,\n            tags,tag_questions,tag_users,\n            questions_score)\n        \n        #######################\n        # Generate some features for calculates weights\n        # that will use with interaction matrix\n        #######################\n        df_merge['num_ans_per_ques'] = df_merge.groupby(\n            ['questions_id'])['answers_id'].transform('count')\n        \n        # merge pro previoius answered question tags with pro tags \n        df_professionals = self._merge_pro_pre_ans_tags(\n            df_professionals, df_merge)\n        \n        # some more pre-processing \n        # handling null values \n        df_questions['score'] = df_questions['score'].fillna(0)\n        df_questions['score'] = df_questions['score'].astype(int)\n        df_questions['questions_tag_name'] = \\\n        df_questions['questions_tag_name'].fillna('No Tag')\n        \n        # remove duplicates tags from each questions \n        df_questions['questions_tag_name'] = \\\n        df_questions['questions_tag_name'].str.split(\n            ',').apply(set).str.join(',')\n\n        # fill nan with 'No Tag' if any \n        df_professionals['professional_all_tags'] = \\\n        df_professionals['professional_all_tags'].fillna(\n            'No Tag')\n        # replace \"\" with \"No Tag\", because previously we replace nan with \"\"\n        df_professionals['professional_all_tags'] = \\\n        df_professionals['professional_all_tags'].replace(\n            '', 'No Tag')\n        \n        df_professionals['professionals_location'] = \\\n        df_professionals['professionals_location'].fillna(\n            'No Location')\n        \n        df_professionals['professionals_industry'] = \\\n        df_professionals['professionals_industry'].fillna(\n            'No Industry')\n\n        # remove duplicates tags from each professionals\n        df_professionals['professional_all_tags'] = \\\n        df_professionals['professional_all_tags'].str.split(\n            ',').apply(set).str.join(',')\n\n        # remove some null values from df_merge\n        df_merge['num_ans_per_ques']  = \\\n        df_merge['num_ans_per_ques'].fillna(0)\n        \n        return df_questions, df_professionals, df_merge\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Building Data for LightFM Class**: From step 2 we already know that lightfm library except data in a very specific and elligent way. LightFM data format is already discussed in step 2. Feel free to read that. Now we are building a class that will be put all of dataset building puzzle in a specific class. "},{"metadata":{"trusted":true},"cell_type":"code","source":"class LightFMDataPrep:\n    def __init__(self):\n        pass\n    def create_features(self, dataframe, features_name, id_col_name):\n        \"\"\"\n        Generate features that will be ready for feeding into lightfm\n\n        Parameters\n        ----------\n        dataframe: Dataframe\n            Pandas Dataframe which contains features\n        features_name : List\n            List of feature columns name avaiable in dataframe\n        id_col_name: String\n            Column name which contains id of the question or\n            answer that the features will map to.\n            There are two possible values for this variable.\n            1. questions_id_num\n            2. professionals_id_num\n\n        Returns\n        -------\n        Pandas Series\n            A pandas series containing process features\n            that are ready for feed into lightfm.\n            The format of each value\n            will be (user_id, ['feature_1', 'feature_2', 'feature_3'])\n            Ex. -> (1, ['military', 'army', '5'])\n        \"\"\"\n\n        features = dataframe[features_name].apply(\n            lambda x: ','.join(x.map(str)), axis=1)\n        features = features.str.split(',')\n        features = list(zip(dataframe[id_col_name], features))\n        return features\n\n\n\n    def generate_feature_list(self, dataframe, features_name):\n        \"\"\"\n        Generate features list for mapping \n\n        Parameters\n        ----------\n        dataframe: Dataframe\n            Pandas Dataframe for Users or Q&A. \n        features_name : List\n            List of feature columns name avaiable in dataframe. \n\n        Returns\n        -------\n        List of all features for mapping \n        \"\"\"\n        features = dataframe[features_name].apply(\n            lambda x: ','.join(x.map(str)), axis=1)\n        features = features.str.split(',')\n        features = features.apply(pd.Series).stack().reset_index(drop=True)\n        return features\n    \n    def create_data(self, questions, professionals, merge):\n        question_feature_list = self.generate_feature_list(\n            questions,\n            ['questions_tag_name'])\n\n        professional_feature_list = self.generate_feature_list(\n            professionals,\n            ['professional_all_tags'])\n        \n        merge['total_weights'] = 1 / (\n            merge['num_ans_per_ques'])\n        \n        # creating features for feeding into lightfm \n        questions['question_features'] = self.create_features(\n            questions, ['questions_tag_name'], \n            'questions_id_num')\n\n        professionals['professional_features'] = self.create_features(\n            professionals,\n            ['professional_all_tags'],\n            'professionals_id_num')\n        \n        return question_feature_list,\\\n    professional_feature_list,merge,questions,professionals\n        \n    def fit(self, questions, professionals, merge):\n        ########################\n        # Dataset building for lightfm\n        ########################\n        question_feature_list, \\\n        professional_feature_list,\\\n        merge,questions,professionals = \\\n        self.create_data(questions, professionals, merge)\n        \n        \n        # define our dataset variable\n        # then we feed unique professionals and questions ids\n        # and item and professional feature list\n        # this will create lightfm internel mapping\n        dataset = Dataset()\n        dataset.fit(\n            set(professionals['professionals_id_num']), \n            set(questions['questions_id_num']),\n            item_features=question_feature_list, \n            user_features=professional_feature_list)\n\n\n        # now we are building interactions\n        # matrix between professionals and quesitons\n        # we are passing professional and questions id as a tuple\n        # e.g -> pd.Series((pro_id, question_id), (pro_id, questin_id))\n        # then we use lightfm build in method for building interactions matrix\n        merge['author_question_id_tuple'] = list(zip(\n            merge.professionals_id_num,\n            merge.questions_id_num,\n            merge.total_weights))\n\n        interactions, weights = dataset.build_interactions(\n            merge['author_question_id_tuple'])\n\n\n\n        # now we are building our questions and\n        # professionals features\n        # in a way that lightfm understand.\n        # we are using lightfm build in method for building\n        # questions and professionals features \n        questions_features = dataset.build_item_features(\n            questions['question_features'])\n\n        professional_features = dataset.build_user_features(\n            professionals['professional_features'])\n        \n        return interactions,\\\n    weights,questions_features,professional_features\n        \n        ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Train Model Class**: In step 2, we saw how we build and train our model. Now we are going to put those all together in TrainLightFM class. "},{"metadata":{"trusted":true},"cell_type":"code","source":"class TrainLightFM:\n    def __init__(self):\n        pass\n        \n    def train_test_split(self, interactions, weights):\n        train_interactions, test_interactions = \\\n        cross_validation.random_train_test_split(\n            interactions, \n            random_state=np.random.RandomState(2019))\n        \n        train_weights, test_weights = \\\n        cross_validation.random_train_test_split(\n            weights, \n            random_state=np.random.RandomState(2019))\n        return train_interactions,\\\n    test_interactions, train_weights, test_weights\n    \n    def fit(self, interactions, weights,\n            questions_features, professional_features,\n            cross_validation=False,no_components=150,\n            learning_rate=0.05,\n            loss='warp',\n            random_state=2019,\n            verbose=True,\n            num_threads=4, epochs=5):\n        ################################\n        # Model building part\n        ################################\n\n        # define lightfm model by specifying hyper-parametre\n        # then fit the model with ineteractions matrix,\n        # item and user features\n        \n        model = LightFM(\n            no_components,\n            learning_rate,\n            loss=loss,\n            random_state=random_state)\n        model.fit(\n            interactions,\n            item_features=questions_features,\n            user_features=professional_features, sample_weight=weights,\n            epochs=epochs, num_threads=num_threads, verbose=verbose)\n        \n        return model\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Recommendations classs**: Now we are going to build a class for making recommendations. This will make easy for making recommendations in djono api. This recommendations class build with extra features. You can use this for general prediction by giving professionals ids and questions features. It has another features that let's choose questions from range of two dates and make recommendation from those questions. \n\nThis is useful because those professionals that choose email frequency lavel as \"weekly\" or \"daily\", we can select questions from a week and then recommend those questions. "},{"metadata":{"trusted":true},"cell_type":"code","source":"class LightFMRecommendations:\n    \"\"\"\n    Make prediction given model and professional ids\n    \"\"\"\n    def __init__(self, lightfm_model,\n                 professionals_features,\n                 questions_features,\n                 questions,professionals,merge):\n        self.model = lightfm_model\n        self.professionals_features = professionals_features\n        self.questions_features = questions_features\n        self.questions = questions\n        self.professionals = professionals\n        self.merge = merge\n        \n    def previous_answered_questions(self, professionals_id):\n        previous_q_id_num = (\n            self.merge.loc[\\\n                self.merge['professionals_id_num'] == \\\n                professionals_id]['questions_id_num'])\n        \n        previous_answered_questions = self.questions.loc[\\\n            self.questions['questions_id_num'].isin(\n            previous_q_id_num)]\n        return previous_answered_questions\n        \n    \n    def _filter_question_by_pro(self, professionals_id):\n        \"\"\"Drop questions that professional already answer\"\"\"\n        previous_answered_questions = \\\n        self.previous_answered_questions(professionals_id)\n        \n        discard_qu_id = \\\n        previous_answered_questions['questions_id_num'].values.tolist()\n        \n        questions_for_prediction = \\\n        self.questions.loc[~self.questions['questions_id_num'].isin(discard_qu_id)]\n        \n        return questions_for_prediction\n    \n    def _filter_question_by_date(self, questions, start_date, end_date):\n        mask = \\\n        (questions['questions_date_added'] > start_date) & \\\n        (questions['questions_date_added'] <= end_date)\n        \n        return questions.loc[mask]\n        \n    \n    def recommend_by_pro_id_general(self,\n                                    professional_id,\n                                    num_prediction=8):\n        questions_for_prediction = self._filter_question_by_pro(professional_id)\n        score = self.model.predict(\n            professional_id,\n            questions_for_prediction['questions_id_num'].values.tolist(), \n            item_features=self.questions_features,\n            user_features=self.professionals_features)\n        \n        questions_for_prediction['recommendation_score'] = score\n        questions_for_prediction = questions_for_prediction.sort_values(\n            by='recommendation_score', ascending=False)[:num_prediction]\n        return questions_for_prediction\n    \n    def recommend_by_pro_id_frequency_date_range(self,\n                                                 professional_id,\n                                                 start_date,\n                                                 end_date,\n                                                 num_prediction=8):\n        questions_for_prediction = \\\n        self._filter_question_by_pro(professional_id)\n        \n        start_date = datetime.strptime(start_date, '%Y-%m-%d')\n        end_date = datetime.strptime(end_date, '%Y-%m-%d')\n        \n        questions_for_prediction = self._filter_question_by_date(\n            questions_for_prediction, start_date, end_date)\n        \n        score = self.model.predict(\n            professional_id,\n            questions_for_prediction['questions_id_num'].values.tolist(), \n            item_features=self.questions_features,\n            user_features=self.professionals_features)\n        \n        questions_for_prediction['recommendation_score'] = score\n        questions_for_prediction = questions_for_prediction.sort_values(\n            by='recommendation_score', ascending=False)[:num_prediction]\n        return questions_for_prediction\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Put it all together**: Now we defined all our important class file. Let's use each of these class and build our model. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# instiate all class instance\ncv_data_prep = CareerVillageDataPreparation()\nlight_fm_data_prep = LightFMDataPrep()\ntrain_lightfm = TrainLightFM()\n\n# process raw data\ndf_questions_p, df_professionals_p, df_merge_p = \\\ncv_data_prep.prepare(\n    df_professionals,df_students,\n    df_questions,df_answers,\n    df_tags,df_tag_questions,df_tag_users,\n    df_question_scores)\n\n\n# prepare data for lightfm \ninteractions, weights, \\\nquestions_features, professional_features = \\\nlight_fm_data_prep.fit(\n    df_questions_p, df_professionals_p, df_merge_p)\n\n\n# finally build and trian our model\nmodel = train_lightfm.fit(interactions,\n                          weights,\n                          questions_features,\n                          professional_features)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Awesome! Do you see, how easy it was for building our model. We can surely apply this idea when putting the model into production. Now we are going to see some real recommendations. "},{"metadata":{"trusted":true,"_kg_hide-output":false},"cell_type":"code","source":"# define our recommender class\nlightfm_recommendations = LightFMRecommendations(\n    model,\n    professional_features,questions_features,\n    df_questions_p, df_professionals_p, df_merge_p)\n\n# let's what our model predict for user id 3\nprint(\"Recommendation for professional: \" + str(3))\ndisplay(lightfm_recommendations.recommend_by_pro_id_general(3)[:8])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"# also let's see what our model predicts for professional 3\n# given questions between two dates\nprint(\"Recommendations for professionals (question from 2016-1-1 to 2016-12-31): \" + str(3))\ndisplay(lightfm_recommendations.recommend_by_pro_id_frequency_date_range(3,\n                                                                 '2016-1-1','2016-12-31')[:8])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Awesome! We can see our recommendations. Also, we can see, the new recommendation class has a method for recommending questions by a frequency of date. This is very helpful for recommending questions to professionals that have set their email frequency to daily or weekly. "},{"metadata":{},"cell_type":"markdown","source":"# Conclusion\n\n**Idea that I tried but don't implemented in this notebook**: \n* Adding location features: I tried adding location features but somehow it decreases model AUC score to 91% to 84%. That's why I don't use that features.\n* Adding dates and hearts data: I also tried that but it doesn't improve AUC score. \n* Correcting spelling error: I tried this method and successfully implemented it. But this is really slow. For that reason, I exluded it. \n\n**Idea that I think is important don't implemented in this notebook**: \n* Adding professionals industry and title as a features. This will inhance our model diversity and will increase overall recommendations score.\n* CareerVillage should auto correct the hashtags for students questions asking time. This will help the model to match the tags more efficiently. \n* For those professionals those have choosen email frequency to immediete, we can create another same model just exchange user/item features. I mean train our model by giving questions as users and professionals as items. In this way, we can predict professionals by giving a questions. So that it helps to target daily frequency professionals.\n\nFinally we came to end. I want give you a big thank you for reading this notebook. I have provided a very good recommender system for CareerVillage in the notebook. If you find any mistakes or have any suggestions feel free to comment. And don't forget to upvote. Good luck! "},{"metadata":{},"cell_type":"markdown","source":"References: \n\n[1] [Improving Pairwise Learning for Item Recommendation from Implicit Feedback](http://webia.lip6.fr/~gallinar/gallinari/uploads/Teaching/WSDM2014-rendle.pdf)\n\n[2] [Content-based filtering](https://towardsdatascience.com/what-are-product-recommendation-engines-and-the-various-versions-of-them-9dcab4ee26d5)\n\n[3] [What Is Content-Based Filtering?](https://www.upwork.com/hiring/data/what-is-content-based-filtering/)\n\n[4] [What Is Collaborative Filtering?](https://www.upwork.com/hiring/data/how-collaborative-filtering-works/)\n\n[5] [Collaborative filtering](https://en.wikipedia.org/wiki/Collaborative_filtering)\n\n[6] [LightFM model documentation](https://lyst.github.io/lightfm/docs/lightfm.html)\n\n[7] [Metadata Embeddings for User and Item Cold-start Recommendations](https://arxiv.org/pdf/1507.08439.pdf)"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}