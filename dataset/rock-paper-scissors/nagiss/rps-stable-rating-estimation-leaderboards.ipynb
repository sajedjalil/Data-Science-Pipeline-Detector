{"cells":[{"metadata":{},"cell_type":"markdown","source":"The rating system used in the simulation competition is highly dependent on the most recent performance. When humans play the game, this dependency is necessary because we need to take into account the improvement of their skills. However, agents do not improve, so this mechanism may not be necessary for the simulation competition. So I calculated a rating without that dependency by maximum likelihood estimation. This rating is based on the following assumptions:\n\n- The probability that an agent with a rating of x wins against an agent with a rating of y is $ 1 / (10^{(y-x)/400} + 1) $.\n- The ratings follow a normal distribution and their mean is equal to the mean of the real ratings.\n\nThe first assumption is the same as the one used in the Elo rating, and is probably used in this competition as well.\nThe second is to prevent the ratings of all winning and all losing agents from going to infinity. I had a feeling that the unimodality of the likelihood function would not be guaranteed if we estimated the rating and standard deviation at the same time, but I tried it several times and got the same result, so it is probably okay.\n\n**UPD**: It turned out that it was not okay, so sigma was fixed."},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"!pip install kaggle-environments --upgrade","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"import os\nimport sys\nfrom time import time, sleep\nimport json\nfrom datetime import datetime\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nfrom operator import itemgetter\nfrom itertools import groupby, count\nfrom collections import defaultdict, Counter\n\nimport numpy as np\nimport numba\nfrom scipy import optimize, integrate\nimport pandas as pd\nfrom sklearn.preprocessing import LabelEncoder\nimport matplotlib.dates as mdates\nimport matplotlib.pyplot as plt\nfrom kaggle_environments import (\n    evaluate, make, utils,\n    get_episode_replay, list_episodes, list_episodes_for_submission  # list_episodes_for_team is no longer available\n)\n\nCOMPETITION_ID = 22838\nCOMPETITION = \"rock-paper-scissors\"\npd.options.display.max_rows = 1000","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"df_episode_agents = pd.read_csv(\"../input/meta-kaggle/EpisodeAgents.csv\")\n\ndf_episodes = pd.read_csv(\"../input/meta-kaggle/Episodes.csv\")\ndf_episodes[\"CreateTime\"] = pd.to_datetime(df_episodes[\"CreateTime\"], format=\"%m/%d/%Y %H:%M:%S\")\ndf_episodes[\"EndTime\"] = pd.to_datetime(df_episodes[\"EndTime\"], format=\"%m/%d/%Y %H:%M:%S\")\n\ndf_teams = pd.read_csv(\"../input/meta-kaggle/Teams.csv\")\ndf_teams[\"ScoreFirstSubmittedDate\"] = pd.to_datetime(df_teams[\"ScoreFirstSubmittedDate\"], format=\"%m/%d/%Y\")\ndf_teams[\"LastSubmissionDate\"] = pd.to_datetime(df_teams[\"LastSubmissionDate\"], format=\"%m/%d/%Y\")\ndf_teams[\"MedalAwardDate\"] = pd.to_datetime(df_teams[\"MedalAwardDate\"], format=\"%m/%d/%Y\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"df_episodes = df_episodes[df_episodes[\"CompetitionId\"]==COMPETITION_ID]\ndf_episodes.reset_index(drop=True, inplace=True)\n\nset_episode_ids = set(df_episodes[\"Id\"].tolist())\ndf_episode_agents = df_episode_agents[df_episode_agents[\"EpisodeId\"].isin(set_episode_ids)]\ndf_episode_agents.reset_index(drop=True, inplace=True)\n\ndf_teams = df_teams[df_teams[\"CompetitionId\"]==COMPETITION_ID]\ndf_teams.reset_index(drop=True, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"df_episode_agents = df_episode_agents[df_episode_agents[\"EpisodeId\"].map(df_episode_agents[\"EpisodeId\"].value_counts())==2]\ndf_episode_agents.sort_values([\"EpisodeId\", \"Index\"], inplace=True)\ndf_episode_agents.reset_index(drop=True, inplace=True)\n\ndef calc_win_lose_draw(a, b):\n    if np.isnan(a):\n        a = -9999999\n    if np.isnan(b):\n        b = -9999999\n    if a == b:\n        return 0\n    if a > b:\n        return 1\n    if a < b:\n        return -1\n    assert False\n\nwin_lose_draw = np.empty(len(df_episode_agents), dtype=np.int64)\nwin_lose_draw[::2] = np.vectorize(calc_win_lose_draw)(df_episode_agents.loc[::2, \"Reward\"].values, df_episode_agents.loc[1::2, \"Reward\"].values)\nwin_lose_draw[1::2] = -win_lose_draw[::2]\ndf_episode_agents[\"WinLoseDraw\"] = win_lose_draw\n\nset_validated_submissions = set(df_episode_agents.dropna()[\"SubmissionId\"].tolist())\nprint(f\"num of valid submissions = {len(set_validated_submissions)}\")\n\nlabel_encoder_sub_id = LabelEncoder()\nlabel_encoder_sub_id.fit(df_episode_agents.dropna(subset=[\"InitialScore\"])[\"SubmissionId\"])\n# df_episode_agents[\"LabelEncodedSubmissionId\"] = label_encoder_sub_id.transform(df_episode_agents[\"SubmissionId\"])\n# df_episode_agents[\"OpponentLabelEncodedSubmissionId\"] = df_episode_agents[\"LabelEncodedSubmissionId\"].values[np.arange(len(df_episode_agents)) ^ 1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"%%time\n# get submission data\n\ndict_submissions = {}\n\nepisode_ids_covering_all_submissions = df_episode_agents.dropna(subset=[\"InitialScore\"]).drop_duplicates(\"SubmissionId\").drop_duplicates(\"EpisodeId\")[\"EpisodeId\"].tolist()\n\nfor idx_episode_ids in range(0, len(episode_ids_covering_all_submissions), 1000):\n    ids = episode_ids_covering_all_submissions[idx_episode_ids:idx_episode_ids+1000]\n    print(f\"{idx_episode_ids}\")\n    episodes = list_episodes(ids)\n    sleep(5)\n    for submission in episodes[\"result\"][\"submissions\"]:\n        dict_submissions[submission[\"id\"]] = submission\n\nlen(dict_submissions)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"df = df_episode_agents.dropna(subset=[\"InitialScore\"])\ndf.reset_index(drop=True, inplace=True)\nassert (df.loc[::2, \"EpisodeId\"].values == df.loc[1::2, \"EpisodeId\"].values).all()\ndf[\"LabelEncodedSubmissionId\"] = label_encoder_sub_id.transform(df[\"SubmissionId\"])\nwin_idxs = df.loc[(df[\"WinLoseDraw\"]==1), \"LabelEncodedSubmissionId\"].values\nlose_idxs = df.loc[(df[\"WinLoseDraw\"]==-1), \"LabelEncodedSubmissionId\"].values\ndraw_idxs = df.loc[(df[\"WinLoseDraw\"]==0), \"LabelEncodedSubmissionId\"].values\nassert len(win_idxs) == len(lose_idxs)\nassert len(draw_idxs) % 2 == 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"url = f'https://www.kaggle.com/c/{COMPETITION}/leaderboard.json?includeBeforeUser=true&includeAfterUser=false'\n!wget '{url}' -O leaderboard.json\n\nwith open(\"leaderboard.json\") as f:\n    jsn = json.load(f)\nleaderboard_data = jsn[\"beforeUser\"] + jsn[\"afterUser\"]\ndf_leaderboard = pd.DataFrame(leaderboard_data)\ngold_score = min(float(x[\"score\"]) for x in leaderboard_data if x[\"medal\"]==\"gold\")\nsilver_score = min(float(x[\"score\"]) for x in leaderboard_data if x[\"medal\"]==\"silver\")\nbronze_score = min(float(x[\"score\"]) for x in leaderboard_data if x[\"medal\"]==\"bronze\")\ngold_rank = max(float(x[\"rank\"]) for x in leaderboard_data if x[\"medal\"]==\"gold\")\nsilver_rank = max(float(x[\"rank\"]) for x in leaderboard_data if x[\"medal\"]==\"silver\")\nbronze_rank = max(float(x[\"rank\"]) for x in leaderboard_data if x[\"medal\"]==\"bronze\")\nmedal_thresholds = [gold_score, silver_score, bronze_score]\nmedal_rank_thresholds = [gold_rank, silver_rank, bronze_rank]\nmedal_colors = [\"#B88121\", \"#838280\", \"#8E5B3D\"]\nmedal_colors_2 = ['#B47D1D', '#7F7E7C', '#8A5739']\nbackground_medal_colors = [\"#F9F7F3\", \"#FAFAFA\", \"#F8F7F6\", \"#FCFCFC\"]\nbackground_medal_colors_2 = [\"#F5F3EF\", \"#F6F6F6\", \"#F4F3F2\", \"#F8F8F8\"]\nmedal_thresholds","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"df_submissions = pd.DataFrame(dict_submissions.values())\ndf_submissions = df_submissions[[\"id\", \"dateSubmitted\", \"teamId\"]]\ndf_submissions[\"dateSubmitted\"] = df_submissions[\"dateSubmitted\"].map(lambda x: datetime.fromtimestamp(x[\"seconds\"]))\ndf_submissions[\"teamName\"] = df_submissions[\"teamId\"].map(dict(df_leaderboard[[\"teamId\", \"teamName\"]].values))\ndict_submission_id_to_current_rating = dict(df_episode_agents.drop_duplicates(keep=\"last\", subset=\"SubmissionId\").dropna(subset=[\"UpdatedScore\"])[[\"SubmissionId\", \"UpdatedScore\"]].values)\ndf_submissions[\"currentRating\"] = df_submissions[\"id\"].map(dict_submission_id_to_current_rating)\ndf_submissions[\"numEpisodes\"] = df_submissions[\"id\"].map(df_episode_agents.dropna(subset=[\"InitialScore\"])[\"SubmissionId\"].value_counts())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"# optimize\n\nmu = df_submissions[\"currentRating\"].mean()\nsigma = df_submissions[\"currentRating\"].std()\n\ndef negative_log_likelihood(x, verbose=True):\n    ratings = x[:-1]\n    #sigma = x[-1]\n    \n    # prior distribution\n    a = ((ratings - mu) ** 2.0).sum() / (2.0 * sigma * sigma) \\\n      + len(ratings) * 0.5 * np.log(2.0 * np.pi * sigma * sigma)\n    \n    # Elo rating\n    win_ratings = ratings[win_idxs]\n    lose_ratings = ratings[lose_idxs]\n    draw_ratings = ratings[draw_idxs]\n    b = np.log(10.0 ** ((lose_ratings - win_ratings) / 400.0) + 1.0).sum() \\\n      + 0.5 * np.log(10.0 ** ((draw_ratings[::2] - draw_ratings[1::2]) / 400.0) + 1.0).sum() \\\n      + 0.5 * np.log(10.0 ** ((draw_ratings[1::2] - draw_ratings[::2]) / 400.0) + 1.0).sum()\n    \n    res = a + b\n    if verbose:\n        print(f\"negative_log_likelihood: {res:19.7f}\")\n    return res\n\ndef gradient(x):\n    ratings = x[:-1]\n    #sigma = x[-1]\n    n = len(ratings)\n    \n    res = np.empty_like(x)\n    res[:-1] = (ratings - mu) / (sigma * sigma)\n    res[-1] = 0#- ((ratings - mu) ** 2).sum() / (sigma * sigma * sigma) + n / sigma\n    \n    win_ratings = ratings[win_idxs]\n    lose_ratings = ratings[lose_idxs]\n    draw_ratings = ratings[draw_idxs]\n    l = np.log(10)\n    e = - l / (400.0 * (1.0 + 10.0 ** ((win_ratings - lose_ratings) / 400.0)))\n    res[:-1] += np.bincount(win_idxs, e, n)\n    res[:-1] -= np.bincount(lose_idxs, e, n)\n    e = - 0.5 * l / (400.0 * (1.0 + 10.0 ** ((draw_ratings[::2] - draw_ratings[1::2]) / 400.0))) \\\n      + 0.5 * l / (400.0 * (1.0 + 10.0 ** ((draw_ratings[1::2] - draw_ratings[::2]) / 400.0)))\n    res[:-1] += np.bincount(draw_idxs[::2], e, n)\n    res[:-1] -= np.bincount(draw_idxs[1::2], e, n)\n    return res\n\n# initialize\nx0 = np.empty(len(label_encoder_sub_id.classes_)+1)\nx0[:-1] = np.random.randn(len(label_encoder_sub_id.classes_)) * 300 + 600\nx0[-1] = 300\n#negative_log_likelihood(x0), gradient(x0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"%%time\nbounds = [(None, None) for _ in range(len(x0))]\nbounds[-1] = (1, None)\nx, target_value, optimization_info = optimize.fmin_l_bfgs_b(negative_log_likelihood, x0, fprime=gradient, bounds=bounds, factr=1e2)\nassert optimization_info[\"warnflag\"] == 0, \"not converged\"\nx, target_value, optimization_info","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"dict_estimated_agent_scores = dict(zip(label_encoder_sub_id.classes_, x))\ndf_submissions[\"estimatedRating\"] = df_submissions[\"id\"].map(dict_estimated_agent_scores)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"plt.figure(figsize=(10, 3), dpi=150)\nhist = plt.hist(df_submissions[\"estimatedRating\"], bins=400)\nplt.grid()\nplt.xlabel(\"Estimated rating\")\nplt.title(\"Distribution of estimated ratings\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"plt.figure(figsize=(8, 8), dpi=150)\nylim = df_submissions[\"estimatedRating\"].min()-300, df_submissions[\"estimatedRating\"].max()+300\nplt.vlines(medal_thresholds, *ylim, medal_colors, linewidth=1.5)\nplt.scatter(df_submissions[\"currentRating\"], df_submissions[\"estimatedRating\"], s=0.1, label=\"Agent\")\nplt.text(950, 500, \"Lucky\", rotation=-25, fontsize=16, ha=\"center\", va=\"center\", bbox={\"boxstyle\": \"rarrow\", \"fc\": \"#ffffee\"})\nplt.text(200, 850, \"Unlucky\", rotation=-25, fontsize=16, ha=\"center\", va=\"center\", bbox={\"boxstyle\": \"larrow\", \"fc\": \"#eeeeff\"})\n\nplt.grid()\nplt.axes().set_aspect(\"equal\")\nplt.xlabel(\"Current Rating\")\nplt.ylabel(\"Estimated Rating\")\nplt.ylim(*ylim)\nplt.xticks(np.arange(-1000, 1400, 200))\nplt.title(\"How much does the order of the episodes affect the rating?\")\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Estimated rating vs Submission date\n\nPlease compare the stability with the [one using the real ratings](https://www.kaggle.com/nagiss/rps-leaderboard-analysis#Rating-vs-Submission-date)."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20, 70))\ncmap = plt.get_cmap(\"tab10\")\ndict_team_id_to_team_rank = defaultdict(lambda: np.nan)\ndict_team_id_to_team_rank.update(dict(df_leaderboard[[\"teamId\", \"rank\"]].values))\n\nxlim = datetime(2020, 11, 3), datetime(2021, 2, 3)\nmax_rating = (int(df_submissions[\"estimatedRating\"].max()) // 50 + 1) * 50\nmin_rating = max_rating - 350\n\nfor team_id, df in df_submissions.groupby(\"teamId\"):\n    rank = dict_team_id_to_team_rank[team_id]\n    if not 1 <= rank <= 100:\n        continue\n    plt.subplot(20, 5, rank)\n    #plt.hlines(medal_thresholds, *xlim, medal_colors, linewidth=1.2, linestyles=\"solid\")\n    plt.scatter(df[\"dateSubmitted\"], df[\"estimatedRating\"], s=10, c=cmap((rank-1)%10))\n    plt.xlim(*xlim)\n    plt.ylim(min_rating, max_rating)\n    team_name = df.iloc[0][\"teamName\"]\n    plt.title(f\"{rank}  {team_name}\")\n    plt.gca().xaxis.set_major_locator(mdates.DayLocator(bymonthday=(1, 16)))\n    plt.gca().xaxis.set_major_formatter(mdates.DateFormatter(\"%m-%d\"))\n    plt.grid()\n    \nplt.subplots_adjust(left=None, bottom=None, right=None, top=None, wspace=None, hspace=0.3)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Top agents\n\nThe list of agents with the highest estimated rating."},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"%%time\n\ndef calc_sigma(sub_id):\n    idx_x = label_encoder_sub_id.transform([sub_id])[0]\n    x2 = x.copy()\n    \n    win_idxs_2 = win_idxs[(win_idxs==idx_x) | (lose_idxs==idx_x)]\n    lose_idxs_2 = lose_idxs[(win_idxs==idx_x) | (lose_idxs==idx_x)]\n    draw_idxs_2 = draw_idxs[(draw_idxs==idx_x) | (draw_idxs==idx_x).reshape(-1, 2)[:, ::-1].reshape(-1)]\n    \n    def negative_log_likelihood_2(x, verbose=True):\n        ratings = x[:-1]\n        #sigma = x[-1]\n\n        # prior distribution\n        a = ((ratings[idx_x] - mu) ** 2.0) / (2.0 * sigma * sigma) \\\n          + len(ratings) * 0.5 * np.log(2.0 * np.pi * sigma * sigma)\n\n        # Elo rating\n        win_ratings = ratings[win_idxs_2]\n        lose_ratings = ratings[lose_idxs_2]\n        draw_ratings = ratings[draw_idxs_2]\n        b = np.log(10.0 ** ((lose_ratings - win_ratings) / 400.0) + 1.0).sum() \\\n          + 0.5 * np.log(10.0 ** ((draw_ratings[::2] - draw_ratings[1::2]) / 400.0) + 1.0).sum() \\\n          + 0.5 * np.log(10.0 ** ((draw_ratings[1::2] - draw_ratings[::2]) / 400.0) + 1.0).sum()\n\n        res = a + b\n        if verbose:\n            print(f\"negative_log_likelihood: {res:19.7f}\")\n        return res\n    \n    offset = negative_log_likelihood_2(x, verbose=False)\n    \n    cache = []\n    def f_all(r):\n        r = r + x[idx_x]\n        x2[idx_x] = r\n        p = np.exp(-negative_log_likelihood_2(x2, verbose=False) + offset)\n        #print(r, p)\n        cache.append((r, p))\n        return p\n    \n    def f_mean(r):\n        r = r + x[idx_x]\n        x2[idx_x] = r\n        p = np.exp(-negative_log_likelihood_2(x2, verbose=False) + offset)\n        return p * r\n    \n    def f_sq_mean(r):\n        r = r + x[idx_x]\n        x2[idx_x] = r\n        p = np.exp(-negative_log_likelihood_2(x2, verbose=False) + offset)\n        return p * r * r\n    \n    a, ea = integrate.quad(f_all, -np.inf, np.inf, epsrel=1e-4)\n#     plt.scatter([r for r, _ in cache], [p for _,p in cache])\n#     plt.xlim(1000, 1800)\n#     plt.show()\n    m, em = integrate.quad(f_mean, -np.inf, np.inf, epsrel=1e-4)\n    s, es = integrate.quad(f_sq_mean, -np.inf, np.inf, epsrel=1e-4)\n    #print(a, ea, m, em, s, es)\n    m /= a\n    s /= a\n    res = np.sqrt(s - m * m)\n    return res","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"%%time\nset_top_sub_ids = set(df_submissions[\"id\"][df_submissions[\"estimatedRating\"].nlargest(10000).index].tolist())  # It's time-consuming, so only calculate top agents\ndf_submissions[\"estimatedSigma\"] = [calc_sigma(sub_id) if sub_id in set_top_sub_ids else np.nan for sub_id in df_submissions[\"id\"].tolist()]\ndf_submissions[\"μ-2σ\"] = (df_submissions[\"estimatedRating\"] - df_submissions[\"estimatedSigma\"] * 2).fillna(-9999)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"df_submissions.sort_values(\"estimatedRating\", ascending=False).reset_index(drop=True)[[\"teamName\", \"estimatedRating\", \"currentRating\", \"dateSubmitted\", \"estimatedSigma\", \"numEpisodes\", \"μ-2σ\", \"id\"]].head(100).style.format({\n    \"estimatedRating\": lambda x: f\"{x:7.2f}\",\n    \"currentRating\": lambda x: f\"{x:7.2f}\",\n    \"id\": lambda x: f'<a href=\"https://www.kaggle.com/c/{COMPETITION}/leaderboard?dialog=episodes-submission-{x}\">{x}</a>',\n    \"estimatedSigma\": lambda x: f\"{x:7.2f}\",\n    \"μ-2σ\": lambda x: f\"{x:7.2f}\",\n    \"dateSubmitted\": lambda x: x.strftime(\"%Y-%m-%d\")}\n).background_gradient(subset=[\"estimatedRating\", \"currentRating\", \"μ-2σ\"])\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Leaderboard based on max estimated rating\n\nMake a leaderboard using estimated ratings instead of real ratings."},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"df_rankings = df_submissions.groupby(\"teamId\").agg(\n    teamName=(\"teamName\", \"first\"),\n    numAgents=(\"teamId\", \"count\"),\n    maxCurrentRating=(\"currentRating\", \"max\"),\n    maxEstimatedRating=(\"estimatedRating\", \"max\"),\n    last5EstimatedRating=(\"estimatedRating\", lambda x: x[-5:].mean()),\n    last30EstimatedRating=(\"estimatedRating\", lambda x: x[-30:].mean()),\n    top5EstimatedRating=(\"estimatedRating\", lambda x: np.mean(sorted(x)[-5:])),\n    top30EstimatedRating=(\"estimatedRating\", lambda x: np.mean(sorted(x)[-30:])),\n    maxMuMin2Sigma=(\"μ-2σ\", \"max\"),\n)\n# df_rankings[\"realRank\"] = df_rankings[\"maxCurrentRating\"].rank(method=\"min\", ascending=False).astype(np.int64)\ndf_rankings[\"realRank\"] = df_rankings.index.map(dict(df_leaderboard[[\"teamId\", \"rank\"]].values)).fillna(9999).astype(np.int64)\ndf_rankings[\"top1Rank\"] = df_rankings[\"maxEstimatedRating\"].rank(method=\"min\", ascending=False).astype(np.int64)\ndf_rankings[\"last5Rank\"] = df_rankings[\"last5EstimatedRating\"].rank(method=\"min\", ascending=False).astype(np.int64)\ndf_rankings[\"last30Rank\"] = df_rankings[\"last30EstimatedRating\"].rank(method=\"min\", ascending=False).astype(np.int64)\ndf_rankings[\"top5Rank\"] = df_rankings[\"top5EstimatedRating\"].rank(method=\"min\", ascending=False).astype(np.int64)\ndf_rankings[\"top30Rank\"] = df_rankings[\"top30EstimatedRating\"].rank(method=\"min\", ascending=False).astype(np.int64)\ndf_rankings[\"top1_μ-2σRank\"] = df_rankings[\"maxMuMin2Sigma\"].rank(method=\"min\", ascending=False).astype(np.int64)\ndf_rankings[\"teamMembers\"] = df_rankings.index.map(dict(df_leaderboard[[\"teamId\", \"teamMembers\"]].values))\n#df_rankings.sort_values(\"realRank\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def background_color(df, key):\n    def rank_to_color(rank):\n        for medal in range(3):\n            if rank <= medal_rank_thresholds[medal]:\n                return medal_colors[medal] if rank % 2 else medal_colors_2[medal]\n        return \"#FDFDFD\" if rank % 2 else \"#F9F9F9\"\n    def rank_to_background_color(rank):\n        for medal in range(3):\n            if rank <= medal_rank_thresholds[medal]:\n                return background_medal_colors[medal] if rank % 2 else background_medal_colors_2[medal]\n        return background_medal_colors[3] if rank % 2 else background_medal_colors_2[3]\n    def css(color, text_color_threshold=0.408):\n        def relative_luminance(color):\n            rgba = int(color[1:3], 16) / 255, int(color[3:5], 16) / 255, int(color[5:7], 16) / 255\n            r, g, b = (\n                x / 12.92 if x <= 0.03928 else ((x + 0.055) / 1.055 ** 2.4)\n                for x in rgba[:3]\n            )\n            return 0.2126 * r + 0.7152 * g + 0.0722 * b\n        dark = relative_luminance(color) < text_color_threshold\n        text_color = \"#f1f1f1\" if dark else \"#000000\"\n        return f\"background-color: {color};color: {text_color};\"\n    return pd.DataFrame(\n        [[css(rank_to_color(rank))] + [css(rank_to_background_color(rank))]*(df.shape[1]-1) for rank in df[key].tolist()],\n        index=df.index,\n        columns=df.columns,\n    )\n\ndf = df_submissions.sort_values(\"estimatedRating\", ascending=False).drop_duplicates(subset=\"teamId\", keep=\"first\")\ndf.reset_index(drop=True, inplace=True)\ndf[\"estimatedRatingRank\"] = np.arange(1, len(df)+1)\ndf[\"teamMembers\"] = df[\"teamId\"].map(dict(df_leaderboard[[\"teamId\", \"teamMembers\"]].values))\ndf[\"realRank\"] = df[\"teamId\"].map(df_rankings[\"realRank\"])\ndf[[\"estimatedRatingRank\", \"realRank\", \"teamName\", \"teamMembers\", \"estimatedRating\", \"currentRating\", \"estimatedSigma\", \"id\", \"dateSubmitted\", \"numEpisodes\"]][:300] \\\n.rename(columns={\n    \"estimatedRatingRank\": \"top1Rank\",\n    \"id\": \"submission Id\",\n    \"currentRating\": \"currentRatingOf MaxEstiRatingAgent\",\n    \"estimatedRating\": \"estimated Rating\",\n    \"estimatedSigma\": \"estimated Sigma\",\n    \"dateSubmitted\": \"date Submitted\",\n    \"numEpisodes\": \"num Episodes\"\n}).style.format({\n    \"teamMembers\": lambda x: \"\".join(f'<div style=\"float: right; margin: -4px 2px;\"><a href=\"https://www.kaggle.com{xi[\"profileUrl\"]}\"><img src=\"{xi[\"thumbnailUrl\"]}\" width=\"24\" height=\"24\" alt=\"{xi[\"displayName\"]}\"></a></div>' for xi in x),\n    \"estimatedRating\": lambda x: f\"{x:7.2f}\",\n    \"currentRatingOf MaxEstiRatingAgent\": lambda x: f\"{x:7.2f}\",\n    \"estimated Sigma\": lambda x: f\"{x:7.2f}\",\n    \"submission Id\": lambda x: f'<a href=\"https://www.kaggle.com/c/{COMPETITION}/leaderboard?dialog=episodes-submission-{x}\">{x}</a>',\n    \"date Submitted\": lambda x: x.strftime(\"%Y-%m-%d\")}\n).set_table_styles([{\"selector\": \"th\", \"props\": [(\"max-width\", \"90px\")]}]) \\\n.apply(background_color, axis=None, key=\"top1Rank\").hide_index()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Leaderboard based on last 5 agents\n\nEach team will be ranked according to the average estimated ratings of the last five agents.  \nTeams with high performance variance will be disadvantaged, and teams with a low number of submissions will be advantaged."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"df_rankings.reset_index().sort_values(\"last5Rank\")[[\"last5Rank\", \"realRank\", \"teamName\", \"teamMembers\", \"last5EstimatedRating\", \"numAgents\"]][:300].style.format({\n    \"teamMembers\": lambda x: \"\".join(f'<div style=\"float: right; margin: -4px 2px;\"><a href=\"https://www.kaggle.com{xi[\"profileUrl\"]}\"><img src=\"{xi[\"thumbnailUrl\"]}\" width=\"24\" height=\"24\" alt=\"{xi[\"displayName\"]}\"></a></div>' for xi in x),\n    \"last5EstimatedRating\": lambda x: f\"{x:7.2f}\",\n}).set_table_styles([{\"selector\": \"th\", \"props\": [(\"max-width\", \"150px\")]}]) \\\n.apply(background_color, axis=None, key=\"last5Rank\").hide_index()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Leaderboard based on top 5 agents\n\nRank each team by the average estimated rating of their best five agents."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"df_rankings.reset_index().sort_values(\"top5Rank\")[[\"top5Rank\", \"realRank\", \"teamName\", \"teamMembers\", \"top5EstimatedRating\", \"numAgents\"]][:300].style.format({\n    \"teamMembers\": lambda x: \"\".join(f'<div style=\"float: right; margin: -4px 2px;\"><a href=\"https://www.kaggle.com{xi[\"profileUrl\"]}\"><img src=\"{xi[\"thumbnailUrl\"]}\" width=\"24\" height=\"24\" alt=\"{xi[\"displayName\"]}\"></a></div>' for xi in x),\n    \"top5EstimatedRating\": lambda x: f\"{x:7.2f}\",\n}).set_table_styles([{\"selector\": \"th\", \"props\": [(\"max-width\", \"150px\")]}]) \\\n.apply(background_color, axis=None, key=\"top5Rank\").hide_index()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Comparison"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def background_color_comparison(df):\n    def rank_to_color(rank, idx):\n        for medal in range(3):\n            if rank <= medal_rank_thresholds[medal]:\n                return medal_colors[medal] if idx % 2 else medal_colors_2[medal]\n        return \"#FDFDFD\" if idx % 2 else \"#F9F9F9\"\n    def rank_to_background_color(rank):\n        for medal in range(3):\n            if rank <= medal_rank_thresholds[medal]:\n                return background_medal_colors[medal] if rank % 2 else background_medal_colors_2[medal]\n        return background_medal_colors[3] if rank % 2 else background_medal_colors_2[3]\n    def css(color, text_color_threshold=0.408):\n        def relative_luminance(color):\n            rgba = int(color[1:3], 16) / 255, int(color[3:5], 16) / 255, int(color[5:7], 16) / 255\n            r, g, b = (\n                x / 12.92 if x <= 0.03928 else ((x + 0.055) / 1.055 ** 2.4)\n                for x in rgba[:3]\n            )\n            return 0.2126 * r + 0.7152 * g + 0.0722 * b\n        dark = relative_luminance(color) < text_color_threshold\n        text_color = \"#f1f1f1\" if dark else \"#000000\"\n        return f\"background-color: {color};color: {text_color};\"\n    res = pd.DataFrame(\n        [[css(rank_to_background_color(rank))]*(df.shape[1]) for rank in df[\"real Rank\"].tolist()],\n        index=df.index,\n        columns=df.columns,\n    )\n    for col in df.columns:\n        if col.endswith(\"Rank\"):\n            res[col] = [css(rank_to_color(r, i)) for i, r in enumerate(df[col].values, 1)]\n    return res\n\ndf_rankings.sort_values(\"realRank\").reset_index()[[\"realRank\", \"teamName\", \"teamMembers\", \"last5Rank\", \"last30Rank\", \"top1Rank\", \"top5Rank\", \"top30Rank\", \"top1_μ-2σRank\", \"numAgents\"]][:300] \\\n.rename(columns={\n    \"realRank\": \"real Rank\",\n    \"last5Rank\": \"last5 Rank\",\n    \"last30Rank\": \"last30 Rank\",\n    \"top1Rank\": \"top1 Rank\",\n    \"top5Rank\": \"top5 Rank\",\n    \"top30Rank\": \"top30 Rank\",\n    \"numAgents\": \"num Agents\",\n    \"top1_μ-2σRank\": \"top1 μ-2σ Rank\",\n}).style.format({\n    \"teamMembers\": lambda x: \"\".join(f'<div style=\"float: right; margin: -4px 2px;\"><a href=\"https://www.kaggle.com{xi[\"profileUrl\"]}\"><img src=\"{xi[\"thumbnailUrl\"]}\" width=\"24\" height=\"24\" alt=\"{xi[\"displayName\"]}\"></a></div>' for xi in x),\n}).set_table_styles([{\"selector\": \"th\", \"props\": [(\"max-width\", \"50px\")]}]) \\\n.apply(background_color_comparison, axis=None).hide_index()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_submissions.to_csv(\"submissions.csv\", index=False)\ndf_rankings.to_csv(\"rankings.csv\", index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}