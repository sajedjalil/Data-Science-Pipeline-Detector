{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Dictionary game history keyed agent"},{"metadata":{},"cell_type":"markdown","source":"To create the agent for this competition, we must put its code in \\*.py file.   \nTo do this, we can use the [magic commands](https://ipython.readthedocs.io/en/stable/interactive/magics.html) of Jupyter Notebooks    \nOne of these commands is [writefile](https://ipython.readthedocs.io/en/stable/interactive/magics.html#cellmagic-writefile) which writes the contents of the cell to a file."},{"metadata":{},"cell_type":"markdown","source":"The idea was to try and detect opponent deviations from random and try to penalize them. While at the same time staying as random as possible and not be to deterministic so it can be predicted.  \n  \nAfter seeing that the competition was itself trying to penalize random. I changed mine to fallback to the copied in [Rock, Paper, Scissors with Memory Patterns](https://www.kaggle.com/yegorbiryukov/rock-paper-scissors-with-memory-patterns?select=submission.py) instead of strictly random. This agent seemed to be based on a sort of similar idea. Testing seems to show I fallback about the first 10% of the games building up game history and after that it is pretty much strictly my agent being used the last 90%. How much Memory Patterns fallsback the first 10% of the games to random, if it does, I don't know.  \n  \nParameters that could be varied include the key length. I'm changing from 5 to 6 for this version. Subkeys are also updated so shortening the key probably doesn't make much sense. I require a key to have a history of at least 10 games and one of the RPS choice to have been used at least 35% of the time."},{"metadata":{},"cell_type":"markdown","source":"# Functions and Imports - Memory Patterns"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%writefile submission.py\n# start executing cells from here to rewrite submission.py\n\nimport random\n\ndef evaluate_pattern_efficiency(previous_step_result):\n    \"\"\" \n        evaluate efficiency of the pattern and, if pattern is inefficient,\n        remove it from agent's memory\n    \"\"\"\n    pattern_group_index = previous_action[\"pattern_group_index\"]\n    pattern_index = previous_action[\"pattern_index\"]\n    pattern = groups_of_memory_patterns[pattern_group_index][\"memory_patterns\"][pattern_index]\n    pattern[\"reward\"] += previous_step_result\n    # if pattern is inefficient\n    if pattern[\"reward\"] <= EFFICIENCY_THRESHOLD:\n        # remove pattern from agent's memory\n        del groups_of_memory_patterns[pattern_group_index][\"memory_patterns\"][pattern_index]\n    \ndef find_action(group, group_index):\n    \"\"\" if possible, find my_action in this group of memory patterns \"\"\"\n    if len(current_memory) > group[\"memory_length\"]:\n        this_step_memory = current_memory[-group[\"memory_length\"]:]\n        memory_pattern, pattern_index = find_pattern(group[\"memory_patterns\"], this_step_memory, group[\"memory_length\"])\n        if memory_pattern != None:\n            my_action_amount = 0\n            for action in memory_pattern[\"opp_next_actions\"]:\n                # if this opponent's action occurred more times than currently chosen action\n                # or, if it occured the same amount of times and this one is choosen randomly among them\n                if (action[\"amount\"] > my_action_amount or\n                        (action[\"amount\"] == my_action_amount and random.random() > 0.5)):\n                    my_action_amount = action[\"amount\"]\n                    my_action = action[\"response\"]\n            return my_action, pattern_index\n    return None, None\n\ndef find_pattern(memory_patterns, memory, memory_length):\n    \"\"\" find appropriate pattern and its index in memory \"\"\"\n    for i in range(len(memory_patterns)):\n        actions_matched = 0\n        for j in range(memory_length):\n            if memory_patterns[i][\"actions\"][j] == memory[j]:\n                actions_matched += 1\n            else:\n                break\n        # if memory fits this pattern\n        if actions_matched == memory_length:\n            return memory_patterns[i], i\n    # appropriate pattern not found\n    return None, None\n\ndef get_step_result_for_my_agent(my_agent_action, opp_action):\n    \"\"\" \n        get result of the step for my_agent\n        1, 0 and -1 representing win, tie and lost results of the game respectively\n        reward will be taken from observation in the next release of kaggle environments\n    \"\"\"\n    if my_agent_action == opp_action:\n        return 0\n    elif (my_agent_action == (opp_action + 1)) or (my_agent_action == 0 and opp_action == 2):\n        return 1\n    else:\n        return -1\n    \ndef update_current_memory(obs, my_action):\n    \"\"\" add my_agent's current step to current_memory \"\"\"\n    # if there's too many actions in the current_memory\n    if len(current_memory) > current_memory_max_length:\n        # delete first two elements in current memory\n        # (actions of the oldest step in current memory)\n        del current_memory[:2]\n    # add agent's last action to agent's current memory\n    current_memory.append(my_action)\n    \ndef update_memory_pattern(obs, group):\n    \"\"\" if possible, update or add some memory pattern in this group \"\"\"\n    # if length of current memory is suitable for this group of memory patterns\n    if len(current_memory) > group[\"memory_length\"]:\n        # get memory of the previous step\n        # considering that last step actions of both agents are already present in current_memory\n        previous_step_memory = current_memory[-group[\"memory_length\"] - 2 : -2]\n        previous_pattern, pattern_index = find_pattern(group[\"memory_patterns\"], previous_step_memory, group[\"memory_length\"])\n        if previous_pattern == None:\n            previous_pattern = {\n                # list of actions of both players\n                \"actions\": previous_step_memory.copy(),\n                # total reward earned by using this pattern\n                \"reward\": 0,\n                # list of observed opponent's actions after each occurrence of this pattern\n                \"opp_next_actions\": [\n                    # action that was made by opponent,\n                    # amount of times that action occurred,\n                    # what should be the response of my_agent\n                    {\"action\": 0, \"amount\": 0, \"response\": 1},\n                    {\"action\": 1, \"amount\": 0, \"response\": 2},\n                    {\"action\": 2, \"amount\": 0, \"response\": 0}\n                ]\n            }\n            group[\"memory_patterns\"].append(previous_pattern)\n        # update previous_pattern\n        for action in previous_pattern[\"opp_next_actions\"]:\n            if action[\"action\"] == obs[\"lastOpponentAction\"]:\n                action[\"amount\"] += 1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Global Variables - Memory Patterns"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%writefile -a submission.py\n# \"%%writefile -a submission.py\" will append the code below to submission.py,\n# it WILL NOT rewrite submission.py\n\n# maximum steps in a memory pattern\nSTEPS_MAX = 5\n# minimum steps in a memory pattern\nSTEPS_MIN = 3\n# lowest efficiency threshold of a memory pattern before being removed from agent's memory\nEFFICIENCY_THRESHOLD = -3\n# amount of steps between forced random actions\nFORCED_RANDOM_ACTION_INTERVAL = random.randint(STEPS_MIN, STEPS_MAX)\n\n# current memory of the agent\ncurrent_memory = []\n# previous action of my_agent\nprevious_action = {\n    \"action\": None,\n    # action was taken from pattern\n    \"action_from_pattern\": False,\n    \"pattern_group_index\": None,\n    \"pattern_index\": None\n}\n# amount of steps remained until next forced random action\nsteps_to_random = FORCED_RANDOM_ACTION_INTERVAL\n# maximum length of current_memory\ncurrent_memory_max_length = STEPS_MAX * 2\n# current reward of my_agent\n# will be taken from observation in the next release of kaggle environments\nreward = 0\n# memory length of patterns in first group\n# STEPS_MAX is multiplied by 2 to consider both my_agent's and opponent's actions\ngroup_memory_length = current_memory_max_length\n# list of groups of memory patterns\ngroups_of_memory_patterns = []\nfor i in range(STEPS_MAX, STEPS_MIN - 1, -1):\n    groups_of_memory_patterns.append({\n        # how many steps in a row are in the pattern\n        \"memory_length\": group_memory_length,\n        # list of memory patterns\n        \"memory_patterns\": []\n    })\n    group_memory_length -= 2\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Create Agent - Memory Pattern"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%writefile -a submission.py\n# \"%%writefile -a submission.py\" will append the code below to submission.py,\n# it WILL NOT rewrite submission.py\n\ndef pattern(obs, conf):\n    \"\"\" your ad here \"\"\"\n    # action of my_agent\n    my_action = None\n    \n    # forced random action\n    global steps_to_random\n    steps_to_random -= 1\n    if steps_to_random <= 0:\n        steps_to_random = FORCED_RANDOM_ACTION_INTERVAL\n        # choose action randomly\n        my_action = random.randint(0, 2)\n        # save action's data\n        previous_action[\"action\"] = my_action\n        previous_action[\"action_from_pattern\"] = False\n        previous_action[\"pattern_group_index\"] = None\n        previous_action[\"pattern_index\"] = None\n    \n    # if it's not first step\n    if obs[\"step\"] > 0:\n        # add opponent's last step to current_memory\n        current_memory.append(obs[\"lastOpponentAction\"])\n        # previous step won or lost\n        previous_step_result = get_step_result_for_my_agent(current_memory[-2], current_memory[-1])\n        global reward\n        reward += previous_step_result\n        # if previous action of my_agent was taken from pattern\n        if previous_action[\"action_from_pattern\"]:\n            evaluate_pattern_efficiency(previous_step_result)\n    \n    for i in range(len(groups_of_memory_patterns)):\n        # if possible, update or add some memory pattern in this group\n        update_memory_pattern(obs, groups_of_memory_patterns[i])\n        # if action was not yet found\n        if my_action == None:\n            my_action, pattern_index = find_action(groups_of_memory_patterns[i], i)\n            if my_action != None:\n                # save action's data\n                previous_action[\"action\"] = my_action\n                previous_action[\"action_from_pattern\"] = True\n                previous_action[\"pattern_group_index\"] = i\n                previous_action[\"pattern_index\"] = pattern_index\n    \n    # if no action was found\n    if my_action == None:\n        # choose action randomly\n        my_action = random.randint(0, 2)\n        # save action's data\n        previous_action[\"action\"] = my_action\n        previous_action[\"action_from_pattern\"] = False\n        previous_action[\"pattern_group_index\"] = None\n        previous_action[\"pattern_index\"] = None\n    \n    # add my_agent's current step to current_memory\n    update_current_memory(obs, my_action)\n    return my_action","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* # Functions and Imports"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%writefile -a submission.py\n\nimport collections, itertools\nimport random","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Global Variables"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%writefile -a submission.py\n\nrunning_key = collections.deque(7*(0,0), 7)\n\nd = {}\n\nopp = {\n    0: 0,\n    1: 0,\n    2: 0\n}\n\n# previous action of my_agent\nlast_action = 0","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Create Agent"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%writefile -a submission.py\n\ndef my_agent(observation, configuration):\n    global opp, d\n    global last_action\n    global running_key\n    ROCK = 0\n    PAPER = 1\n    SCISSORS = 2\n    \n    \n    my_action = pattern(observation, configuration)\n    action_from_pattern = True\n    \n    # if it's not first step\n    if observation.step > 0:\n        had_prev = False\n        prev_key = tuple(itertools.islice(running_key,0,5))\n        # update current keys\n        for k in range(len(prev_key),0,-1):\n            subkey_prev = tuple(itertools.islice(prev_key,0,k))\n            if subkey_prev in d:\n                had_prev = True\n                prior = d[subkey_prev]\n                prior[observation.lastOpponentAction] += 1\n            else:\n                prior = opp.copy()\n                prior[observation.lastOpponentAction] = 1\n                d[tuple(subkey_prev)] = prior                \n        # add opponent's last step to current_memory\n        last_game = (last_action,observation.lastOpponentAction)\n        #running_cnt += get_step_result_for_my_agent(last_action,observation.lastOpponentAction)\n        running_key.appendleft(last_game)\n        had_key = False\n        for k in range(len(running_key),0,-1):\n            subkey = tuple(itertools.islice(running_key,0,k))\n            if subkey in d:\n                had_key = True\n                prior = d[subkey]\n                v = prior.values()\n                mv = max(v)\n                total = sum(v)\n                if total < 11 or mv / total < .35: \n                    continue\n                else:\n                    #iv = v.index(mv)\n                    rock_per = prior[ROCK] / total\n                    paper_per = prior[PAPER] / total\n                    scissors_per = prior[SCISSORS] / total                \n                    r = random.uniform(0, 1)\n                    if r <= rock_per:\n                        my_action = PAPER\n                    elif r <= rock_per + paper_per:\n                        my_action = SCISSORS\n                    else: \n                        my_action = ROCK  \n                    action_from_pattern = False\n                break\n            else:\n                prior = opp.copy()\n                prior[observation.lastOpponentAction] = 1\n                d[tuple(subkey)] = prior\n        if not had_key:\n            prior = opp.copy()\n            prior[observation.lastOpponentAction] = 1\n            d[tuple(running_key)] = prior\n     \n    last_action = my_action\n    return my_action","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"3\"></a>\n<h2 style='background:#FBE338; border:0; color:black'><center>Battle Examples<center><h2>"},{"metadata":{},"cell_type":"markdown","source":"This part is devoted to simulating and testing battles with other agents."},{"metadata":{},"cell_type":"markdown","source":"We need to import the library for creating environments and simulating agent battles"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Upgrade kaggle_environments using pip before import\n!pip install -q -U kaggle_environments","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from kaggle_environments import make","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Create a rock-paper-scissors environment (RPS), and set 1000 episodes for each simulation"},{"metadata":{"trusted":true},"cell_type":"code","source":"env = make(\n    \"rps\", \n    configuration={\"episodeSteps\": 100},\n    debug=True\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's take the agent that will copy our previous action from [Rock Paper Scissors - Agents Comparison](https://www.kaggle.com/ihelon/rock-paper-scissors-agents-comparison)"},{"metadata":{"trusted":true},"cell_type":"code","source":"agent_copy_opponent_path = \"../input/rock-paper-scissors-agents-comparison/copy_opponent.py\"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's start simulating the battle nash_equilibrium_agent vs copy_opponent_agent"},{"metadata":{"trusted":true},"cell_type":"code","source":"# nash_equilibrium_agent vs copy_opponent_agent\nprint(\"running\",agent_copy_opponent_path)\nenv.run(\n    [\"submission.py\", agent_copy_opponent_path]\n)\n\nenv.render(mode=\"ipython\", width=500, height=400)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's take the agent that will hit our previous action from [Rock Paper Scissors - Agents Comparison](https://www.kaggle.com/ihelon/rock-paper-scissors-agents-comparison)"},{"metadata":{"trusted":true},"cell_type":"code","source":"agent_reactionary_path = \"../input/rock-paper-scissors-agents-comparison/reactionary.py\"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's start simulating the battle nash_equilibrium_agent vs reactionary"},{"metadata":{"trusted":true},"cell_type":"code","source":"# nash_equilibrium_agent vs reactionary\nenv.run(\n    [\"submission.py\", agent_reactionary_path]\n)\n\nenv.render(mode=\"ipython\", width=500, height=400)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Simulating the battle between two nash_equilibrium_agent agents"},{"metadata":{"trusted":true},"cell_type":"code","source":"# nash_equilibrium_agent vs nash_equilibrium_agent\nenv.run(\n    [\"submission.py\", \"submission.py\"]\n)\n\nenv.render(mode=\"ipython\", width=500, height=400)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's take the agent that will always use Rock from [Rock Paper Scissors - Agents Comparison](https://www.kaggle.com/ihelon/rock-paper-scissors-agents-comparison)"},{"metadata":{"trusted":true},"cell_type":"code","source":"agent_rock_path = \"../input/rock-paper-scissors-agents-comparison/rock.py\"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Simulating the battle between nash_equilibrium_agent and rock"},{"metadata":{"trusted":true},"cell_type":"code","source":"# nash_equilibrium_agent vs rock\nenv.run(\n    [\"submission.py\", agent_rock_path]\n)\n\nenv.render(mode=\"ipython\", width=500, height=400)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's take the agent that will always use Scissors from [Rock Paper Scissors - Agents Comparison](https://www.kaggle.com/ihelon/rock-paper-scissors-agents-comparison)"},{"metadata":{"trusted":true},"cell_type":"code","source":"agent_scissors_path = \"../input/rock-paper-scissors-agents-comparison/scissors.py\"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Simulating the battle between nash_equilibrium_agent and scissors"},{"metadata":{"trusted":true},"cell_type":"code","source":"# nash_equilibrium_agent vs scissors\nenv.run(\n    [\"submission.py\", agent_scissors_path]\n)\n\nenv.render(mode=\"ipython\", width=500, height=400)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"Let's take the agent that will always use Paper from [Rock Paper Scissors - Agents Comparison](https://www.kaggle.com/ihelon/rock-paper-scissors-agents-comparison)"},{"metadata":{"trusted":true},"cell_type":"code","source":"agent_paper_path = \"../input/rock-paper-scissors-agents-comparison/paper.py\"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Simulating the battle between nash_equilibrium_agent and paper"},{"metadata":{"trusted":true},"cell_type":"code","source":"# nash_equilibrium_agent vs scissors\nenv.run(\n    [\"submission.py\", agent_paper_path]\n)\n\nenv.render(mode=\"ipython\", width=500, height=400)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"Let's take the agent that will hit self last actions from [Rock Paper Scissors - Agents Comparison](https://www.kaggle.com/ihelon/rock-paper-scissors-agents-comparison)"},{"metadata":{"trusted":true},"cell_type":"code","source":"agent_hit_the_last_own_action_path = \"../input/rock-paper-scissors-agents-comparison/hit_the_last_own_action.py\"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Simulating the battle between nash_equilibrium_agent and agent_hit_the_last_own_action_path"},{"metadata":{"trusted":true},"cell_type":"code","source":"# nash_equilibrium_agent vs scissors\nenv.run(\n    [\"submission.py\", agent_hit_the_last_own_action_path]\n)\n\nenv.render(mode=\"ipython\", width=500, height=400)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}