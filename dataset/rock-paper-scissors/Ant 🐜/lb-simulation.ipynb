{"cells":[{"metadata":{},"cell_type":"markdown","source":"This notebook analyses how non-random agents would perform under the ELO leaderboard scoring.\n\nIn the last section it shows how scores would evolve. Non-random agents do not seem to consistently come out top. Main reason are possibly:\n\n* in this simulation random agents are more numerous (10:1) to simulate a mass submission of plain random bots; the number of submissions is not equal between players depending on their behavior\n* weak non-random agents are pushed down in score and hence strong non-random agents do not play against them anymore when being paired with similar score agents; they start facing more random agents\n* interestingly it may be the case that the latter point is every stronger, with a larger difference in performance among agents; hence the results becomes more random if the difference in agent performance is larger\n\nJust swamping the leaderboard with any non-dumb agents or just random agents with 5 submissions a day seems to be good strategy?"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nfrom operator import itemgetter\nimport random\nimport matplotlib.pyplot as plt\nfrom collections import Counter, defaultdict\nimport itertools","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Agents\n\nThe Gauss agent has a stochastic performance with a normal distribution. One Gauss agent wins vs another Gauss agent if the current performance drawn from a normal distribution with a defined mean happens to be larger then the opponent's current performance. This matches very well the assumptions of the common ELO system."},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"class Gauss:\n    def __init__(self, loc, scale=1):\n        self.loc = loc\n        self.scale = scale\n    \n    def round_performance(self):\n        return np.random.normal(loc=self.loc, scale=self.scale)\n    \n    def __repr__(self):\n        return f\"Gauss({self.loc}, {self.scale})\"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Setup"},{"metadata":{"trusted":true},"cell_type":"code","source":"num_random=1000           # number of random agents\nnum_loc_non_random=100    # number of non-random agents per strength level\nlocs = [0,1,2]            # strengths of non-random agents (means for Gauss distribution)\n\nagents = []\n\nfor loc in locs:\n    for _ in range(num_loc_non_random):\n        agents.append(Gauss(loc))\n        \nfor _ in range(num_random):\n    agents.append(None)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Rounds"},{"metadata":{"trusted":true},"cell_type":"code","source":"def new_score(score1, score2, winner, k=30):\n    q1 = 10 ** (score1 / 400)\n    q2 = 10 ** (score2 / 400)\n    \n    e1 = q1 / (q1+q2)\n    e2 = q2 / (q1+q2)\n    \n    new_score1 = score1 + k * ((1-winner) - e1)\n    new_score2 = score2 + k * (winner - e2)\n    \n    return new_score1, new_score2\n    \n\ndef make_round_scores(agents, num_rounds_per_agent = 100, diff_rank = 10, init_score=600):\n    # this is a bit slow at the moment; feel free to optimize and let us know!\n    round_scores = []\n    scores = [init_score] * len(agents)\n    \n    for _ in range(num_rounds_per_agent):\n        for i in range(len(agents)):\n            argsort = np.argsort(scores)\n            ranks = np.empty_like(argsort)\n            ranks[argsort] = np.arange(len(ranks))\n\n            my_rank = ranks[i]\n            other_rank = random.choice(list(set(range(max(my_rank - diff_rank, 0), min(my_rank + diff_rank, len(scores)))) - {my_rank}))\n            j=argsort[other_rank]\n\n            if agents[i] is None or agents[j] is None:\n                winner = random.choice([0, 1])\n            else:\n                perf_i = agents[i].round_performance()\n                perf_j = agents[j].round_performance()\n                winner = int(perf_j > perf_i)\n\n            scores[i], scores[j] = new_score(scores[i], scores[j], winner)\n\n        round_scores.append(scores.copy())\n        \n    return round_scores","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Plot\n\nUnder ideal conditions the top scoring Gauss agent should win every time. In fact we'd want almost all top color agents to be ahead of the gray agents.\n\nIn the plots the colors correspond to Gauss agents of a particular performance level. Gray lines are random bots."},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_scores(round_scores):\n    agent_scores = list(zip(*round_scores))\n    for agent, cur_agent_scores in random.sample(list(zip(agents, agent_scores)), k=len(agents)):\n        plt.plot(cur_agent_scores, c={loc: col for loc, col in zip(locs, \"rgb\")}[agent.loc] if agent is not None else \"gray\", zorder=1 if agent is not None else 0)\n    plt.show()\n    \nprint(\"Number of agents\")\nfor loc, cnt in Counter(agent.loc for agent in agents if agent is not None).items():\n    print(f\"{cnt:5} Strength {loc}\")\nprint(f\"{sum(agent is None for agent in agents):5} Random\")\n\nwinner_count = Counter()\n        \nfor _ in range(10):\n    round_scores = make_round_scores(agents)\n    plot_scores(round_scores)\n    winner_idx = np.argmax(round_scores[-1])\n    winner_agent = agents[winner_idx]\n    print(f\"Winner: {winner_agent if winner_agent is not None else 'Random'}\")\n    winner_count[str(winner_agent) if winner_agent is not None else 'Random']+=1\n    print(f\"Current winner count: {', '.join(f'{agent}:{cnt}' for agent, cnt in winner_count.most_common())}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"What we seem to get with the ELO scoring is a nice split of the non-random agents (apart from some outliers), but a background of random bots which spreads over all the score range.\n\n\n# Alternative scoring system with all-vs-all win rate (no ELO)\n\nLet's try to see what the effect of a classical all-vs-all scoring based on win-rates would be.[](http://)"},{"metadata":{"trusted":true},"cell_type":"code","source":"num_rounds_per_agent = 100  # same number of games for a fair comparison\n\n####\nagent_scores=[0] * len(agents)\n\nfor i in range(len(agents)):\n    for j in random.sample(set(range(len(agents)))-{i}, num_rounds_per_agent):\n        agent1 = agents[i]\n        agent2 = agents[j]\n\n        if agent1 is None or agent2 is None:\n            if random.random() < 0.5:\n                agent_scores[i] += 1\n            else:\n                agent_scores[j] += 1\n        else:\n            perf1 = agent1.round_performance()\n            perf2 = agent2.round_performance()\n            if perf1 > perf2:\n                agent_scores[i] += 1\n            else:\n                agent_scores[j] += 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"agent_idxs, sorted_agent_scores = zip(*sorted(random.sample(list(enumerate(agent_scores)), len(agent_scores)), key=itemgetter(1), reverse=True))\n\nloc_color = {loc: color for loc, color in zip(locs, \"rgb\")}\n\nplt.figure(figsize=(30, 5))\nplt.bar(range(len(agent_idxs)), sorted_agent_scores, width=1, color=[loc_color.get(agents[idx].loc) if agents[idx] is not None else \"lightgray\"  for idx in agent_idxs]);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The leaderboard seems to dominated by the best scoring agent much more clearly than for ELO.\n\nAnd this is even though we did not even include non-transitivity issues yet.\n\nThis demonstrates that random bots are no issue with the right scoring system. You can compare it to the previous ELO simulation, where a background of random bots overwhelmed the others.\n\nSome reasons for the inefficiency of ELO to capture the interesting performance differences are outlined in [Elo, Glicko etc rating system not suitable for RPS](https://www.kaggle.com/c/rock-paper-scissors/discussion/196174).\n\nReasons are:\n\n* RPS bots are not transitive. There are some strong bots, where A beats B, B beats C, C beats A convincingly (one of my buggy bots wins almost every time vs a the past winner Greenberg). This breaks the assumption of ELO\n* This means individual comparisons (as done with ELO based on similar scores) are not meaningful. It is much more interesting to see who of A, B, C can beat *most* of D, E, F.\n* The ranking starts to stabilize when you count how *many* bots one can beat, not if you can beat high scoring bots specifically.\n\nCaveats:\n\nHere we play all-vs-all which is a lot of matches. If due to limited matches you only play a random (distinct!) subset of all bots, some bots may be more lucky getting weak bots assigned. This can be investigated with the above simulation.\n\nIf you believe the bot environment changes over time and earlier bots had a harder/easier time, you could introduce an exponentially scaled win rate counter `new_score = score * decay + I(win=1) * (1-decay)`. This would converge to the recent average win rate.\n\nTechnically, even using the given past matches, the whole LB could be rescored with this approach. I believe past RPS competitions used similar scoring with success.\n\n"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}