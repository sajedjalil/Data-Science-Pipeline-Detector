{"cells":[{"metadata":{},"cell_type":"markdown","source":"This notebook isn't my highest one (current 1079.4) but it's base architecture is almost the same as the highest.\nThe difference is that this notebook uses only [RPS GeometryðŸ¦‡](https://www.kaggle.com/superant/rps-geometry-silver-rank-by-minimal-logic) agents and is much cleaner.\n\nThis notebook borrowed many ideas from [Going meta with Kumoko](https://www.kaggle.com/chankhavu/going-meta-with-kumoko) and [RPS GeometryðŸ¦‡](https://www.kaggle.com/superant/rps-geometry-silver-rank-by-minimal-logic) .\nThanks!\n"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"%%writefile geometry.py\n\nimport operator\nimport numpy as np\nimport cmath\nfrom typing import List\nfrom collections import namedtuple\nimport traceback\nimport sys\nimport random\n\n\nBEAT = [1,2,0]\nCEDE = [2,0,1]\n\nwho_win = { \"PP\": 0, \"PR\":1 , \"PS\":-1,\n            \"RP\": -1,\"RR\":0, \"RS\":1,\n            \"SP\": 1, \"SR\":-1, \"SS\":0}\n\nbasis = np.array(\n    [1, cmath.exp(2j * cmath.pi * 1 / 3), cmath.exp(2j * cmath.pi * 2 / 3)]\n)\n\n\nHistMatchResult = namedtuple(\"HistMatchResult\", \"idx length\")\n\n\ndef find_all_longest(seq, max_len=None) -> List[HistMatchResult]:\n    \"\"\"\n    Find all indices where end of `seq` matches some past.\n    \"\"\"\n    result = []\n\n    i_search_start = len(seq) - 2\n\n    while i_search_start > 0:\n        i_sub = -1\n        i_search = i_search_start\n        length = 0\n\n        while i_search >= 0 and seq[i_sub] == seq[i_search]:\n            length += 1\n            i_sub -= 1\n            i_search -= 1\n\n            if max_len is not None and length > max_len:\n                break\n\n        if length > 0:\n            result.append(HistMatchResult(i_search_start + 1, length))\n\n        i_search_start -= 1\n\n    result = sorted(result, key=operator.attrgetter(\"length\"), reverse=True)\n\n    return result\n\n\ndef probs_to_complex(p):\n    return p @ basis\n\n\ndef _fix_probs(probs):\n    \"\"\"\n    Put probs back into triangle. Sometimes this happens due to rounding errors or if you\n    use complex numbers which are outside the triangle.\n    \"\"\"\n    if min(probs) < 0:\n        probs -= min(probs)\n\n    probs /= sum(probs)\n\n    return probs\n\n\ndef complex_to_probs(z):\n    probs = (2 * (z * basis.conjugate()).real + 1) / 3\n    probs = _fix_probs(probs)\n    return probs\n\n\ndef z_from_action(action):\n    return basis[action]\n\n\ndef sample_from_z(z, deterministic=False):\n    probs = complex_to_probs(z)\n    if deterministic:\n        return int(np.argmax(probs))  \n    return np.random.choice(3, p=probs)\n\ndef bound(z):\n    return probs_to_complex(complex_to_probs(z))\n\n\ndef norm(z):\n    return bound(z / abs(z))\n\n\nclass Pred:\n    def __init__(self, *, alpha):\n        self.offset = 0\n        self.alpha = alpha\n        self.last_feat = None\n\n    def train(self, target):\n        if self.last_feat is not None:\n            offset = target * self.last_feat.conjugate()   # fixed\n\n            self.offset = (1 - self.alpha) * self.offset + self.alpha * offset\n\n    def predict(self, feat):\n        \"\"\"\n        feat is an arbitrary feature with a probability on 0,1,2\n        anything which could be useful anchor to start with some kind of sensible direction\n        \"\"\"\n        feat = norm(feat)\n        \n        result = feat * self.offset\n\n        self.last_feat = feat\n\n        return result\n    \n    \nclass BaseAgent:\n    def __init__(self):\n        self.my_hist = []\n        self.opp_hist = []\n        self.my_opp_hist = []\n        self.outcome_hist = []\n        self.step = None\n\n    def __call__(self, obs, conf):\n        try:\n            if obs.step == 0:\n                action = np.random.choice(3)\n                self.my_hist.append(action)\n                return action\n\n            self.step = obs.step\n\n            opp = int(obs.lastOpponentAction)\n            my = self.my_hist[-1]\n\n            self.my_opp_hist.append((my, opp))\n            self.opp_hist.append(opp)\n\n            outcome = {0: 0, 1: 1, 2: -1}[(my - opp) % 3]\n            self.outcome_hist.append(outcome)\n\n            action = self.action()\n\n            self.my_hist.append(action)\n\n            return action\n        except Exception:\n            traceback.print_exc(file=sys.stderr)\n            raise\n    def action(self):\n        pass\n    \n    def set_my_action(self, my_last_action):\n        self.my_hist[-1] = my_last_action\n\nclass Agent(BaseAgent):\n    def __init__(self, alpha=0.01, max_len=20, deterministic=False):\n        super().__init__()\n        self.max_len = max_len\n        self.predictor = Pred(alpha=alpha)\n        self.deterministic = deterministic\n\n    def action(self):\n        self.train()\n\n        pred = self.preds()\n\n        return_action = sample_from_z(pred, self.deterministic)\n\n        return return_action\n\n    def train(self):\n        last_beat_opp = z_from_action((self.opp_hist[-1] + 1) % 3)\n        self.predictor.train(last_beat_opp)\n\n    def preds(self):\n        hist_match = find_all_longest(self.my_opp_hist, max_len=self.max_len)\n\n        if not hist_match:\n             return 0\n\n        feat = z_from_action(self.opp_hist[hist_match[0].idx])\n\n        pred = self.predictor.predict(feat)\n\n        return pred\n    \nclass AttrDict(dict):\n    def __getattr__(self, name):\n        return self[name]\n    \nclass AgentWrapper:\n    def __init__(self, agent):\n        self.agent = agent\n    def __call__(self, observation, configuration, my_last_action):\n        if observation.step > 0:\n            self.agent.set_my_action(my_last_action)\n        return self.agent(observation, configuration)\n\nclass MirrorAgentWrapper:\n    def __init__(self, agent):\n        self.agent = agent\n    def __call__(self, observation, configuration, my_last_action):\n        if observation.step == 0:\n            mirror_observation = observation\n        else:\n            mirror_observation = AttrDict({'step': observation.step, 'lastOpponentAction': my_last_action})\n            self.agent.set_my_action(observation.lastOpponentAction)\n        return BEAT[self.agent(mirror_observation, configuration)]\n\nclass GeobotBeater:\n    def __init__(self, alpha=0.01, max_len=20):\n        self.opp_hist = []\n        self.my_opp_hist = []\n        self.offset = 0\n        self.last_feat = None\n        self.my_last_action = None\n        self.alpha = alpha\n        self.max_len = max_len\n\n    def __call__(self, obs, conf):\n        step = obs.step\n\n        if step == 0:\n            action = np.random.choice(3)\n        else:\n            our_last_action = self.my_last_action\n            opp_last_action = obs[\"lastOpponentAction\"]\n            self.my_opp_hist.append((opp_last_action, our_last_action))\n            self.opp_hist.append(our_last_action)\n            if self.last_feat is not None:\n                this_offset = (basis[(self.opp_hist[-1] + 1) % 3]) * self.last_feat.conjugate()\n                self.offset = (1 - self.alpha) * self.offset + self.alpha * this_offset\n            hist_match = find_all_longest(self.my_opp_hist, self.max_len)\n            if not hist_match:\n                pred = 0\n            else:\n                feat = basis[self.opp_hist[hist_match[0].idx]]\n                self.last_feat = complex_to_probs(feat / abs(feat)) @ basis\n                pred = self.last_feat * self.offset * cmath.exp(2j * cmath.pi * 1/9)\n            probs = complex_to_probs(pred)\n            if probs[np.argmax(probs)] > .334:\n                action = (int(np.argmax(probs))+1)%3\n            else:\n                action = (np.random.choice(3, p=probs)+1)%3\n        self.my_last_action = action\n        return action\n    \n    def set_my_action(self, my_last_action):\n        self.my_last_action = my_last_action\nclass AntiGeoAgentWrapper:\n    def __init__(self, agent):\n        self.agent = agent\n    def __call__(self, observation, configuration, my_last_action):\n        if observation.step > 0:\n            self.agent.set_my_action(my_last_action)\n        return self.agent(observation, configuration)\n\nclass AntiGeoMirrorAgentWrapper:\n    def __init__(self, agent):\n        self.agent = agent\n    def __call__(self, observation, configuration, my_last_action):\n        if observation.step == 0:\n            mirror_observation = observation\n        else:\n            mirror_observation = AttrDict({'step': observation.step, 'lastOpponentAction': my_last_action})\n            self.agent.set_my_action(observation.lastOpponentAction)\n        return BEAT[self.agent(mirror_observation, configuration)]\n\nclass ScoringFunc:\n    def __init__(self, \n                 decay=1.,\n                 win_value=1.,\n                 draw_value=0.,\n                 lose_value=-1.,\n                 drop_prob=0.,\n                 drop_draw=False,\n                 clip_zero=False):\n        super().__init__()\n        self.decay = decay\n        self.win_value = win_value\n        self.draw_value = draw_value\n        self.lose_value = lose_value\n        self.drop_prob = drop_prob\n        self.drop_draw = drop_draw\n        self.clip_zero = clip_zero\n    def __call__(self, score, our_move, his_move):\n        if our_move == his_move:\n            retval = self.decay * score + self.draw_value\n        elif our_move == BEAT[his_move]:\n            retval = self.decay * score + self.win_value\n        elif our_move == CEDE[his_move]:\n            retval = self.decay * score + self.lose_value\n\n        if self.drop_prob > 0. and random.random() < self.drop_prob:\n            if our_move == CEDE[his_move]:\n                score = 0.\n            elif self.drop_draw and our_move == his_move:\n                score = 0.\n\n        if self.clip_zero:\n            retval = max(0., retval)\n        return retval\n\ndef random_agent(observation, configuration, my_last_action):\n    return random.randint(0, 2)\n\nclass MixedAgent:\n    def __init__(self, agents, dllu_scoring_configs, use_cede=False, use_beat=False, generate_random_dllu_scoring_configs=None, anti_geo_agents=[]):\n        self.agents = agents\n        self.scoring_funcs = [ScoringFunc(*cfg) for cfg in dllu_scoring_configs]\n        self.last_action = None\n                    \n        self.score_history = []\n        self.use_cede = use_cede\n        self.use_beat = use_beat\n        self.agent_num = len(self.agents)\n        if self.use_cede:\n            self.agent_num += len(self.agents)\n        if self.use_beat:\n            self.agent_num += len(self.agents)\n        self.last_proposed_actions = [None] * self.agent_num\n        self.selection_log = [0] * self.agent_num\n        self.scores = 3. * np.ones(self.agent_num)\n        self.valied_score_num = self.agent_num\n        self.generate_random_dllu_scoring_configs = generate_random_dllu_scoring_configs\n\n\n        self.anti_geo_agents = anti_geo_agents\n        if len(anti_geo_agents) > 0:    \n            anti_geo_dllu_scoring_configs = [\n                [1.00,  1.00,    0.00,     -1.00,    0.00,      False,     False] \n            ] * len(anti_geo_agents)\n            self.anti_geo_scoring_funcs = [ScoringFunc(*cfg) for cfg in anti_geo_dllu_scoring_configs]\n            self.anti_geo_last_proposed_actions = [None] * (len(anti_geo_agents))\n            self.anti_geo_scores = 1. * np.ones(len(self.anti_geo_last_proposed_actions))\n            self.use_anti_geo = False\n            \n        \n    def __call__(self, observation, configuration):\n\n\n        if observation.step > 0:\n            self.score_history.append(who_win[['R','P','S'][self.last_action]+['R','P','S'][observation.lastOpponentAction]])\n\n#         if not self.generate_random_dllu_scoring_configs == None and  sum(self.score_history[-50:]) < -20:\n#             random_dllu_scoring_configs = self.generate_random_dllu_scoring_configs()\n#             self.scoring_funcs = [ScoringFunc(*cfg) for cfg in random_dllu_scoring_configs]\n                \n        proposed_actions = [None] * self.agent_num\n        for i in range(len(self.agents)):\n            cede_idx = i + len(self.agents)\n            beat_idx = i + len(self.agents)*2\n            if not self.use_cede and self.use_beat:\n                beat_idx = i + len(self.agents)\n            if observation.step > 0:\n                self.scores[i] = self.scoring_funcs[i](self.scores[i], self.last_proposed_actions[i], observation.lastOpponentAction)\n                if self.use_cede:\n                    self.scores[cede_idx] = self.scoring_funcs[cede_idx](self.scores[cede_idx], self.last_proposed_actions[cede_idx], observation.lastOpponentAction)\n                if self.use_beat:\n                    self.scores[beat_idx] = self.scoring_funcs[beat_idx](self.scores[beat_idx], self.last_proposed_actions[beat_idx], observation.lastOpponentAction)\n            proposed_actions[i] = self.agents[i](observation, configuration, self.last_action)\n            if self.use_cede:\n                proposed_actions[cede_idx] = CEDE[proposed_actions[i]]\n            if self.use_beat:\n                proposed_actions[beat_idx] = BEAT[proposed_actions[i]]\n\n                \n                \n        ############\n        if len(self.anti_geo_agents) > 0:\n            anti_geo_proposed_actions = [None] * len(self.anti_geo_last_proposed_actions) \n            for i in range(len(self.anti_geo_agents)):\n                if observation.step > 0:\n                    self.anti_geo_scores[i] = self.anti_geo_scoring_funcs[i](self.anti_geo_scores[i], self.anti_geo_last_proposed_actions[i], observation.lastOpponentAction)\n                anti_geo_proposed_actions[i] = self.anti_geo_agents[i](observation, configuration, self.last_action)\n            self.anti_geo_last_proposed_actions = anti_geo_proposed_actions\n            if self.use_anti_geo:\n        #         best_index = random.choice([i for i, x in enumerate(anti_geo_scores) if x == max(anti_geo_scores)])\n                best_index = random.choices(range(len(self.anti_geo_scores)),weights=([max(0, score) for score in self.anti_geo_scores]))[0]\n                self.last_action = anti_geo_proposed_actions[best_index]\n                return self.last_action\n        ############\n        \n        \n        ##########\n        if len(self.anti_geo_agents) > 0 and not self.use_anti_geo and 300 < observation.step:\n            max_index = np.argmax(self.anti_geo_scores)\n            max_anti_geo_score = self.anti_geo_scores[max_index]\n            if max_anti_geo_score > 70 or (observation.step < 600 and max_anti_geo_score > 40 and max_anti_geo_score - sum(self.score_history) > 30):\n                self.use_anti_geo = True\n        ##########\n    \n    \n#         if observation.step < 25 or observation.step > 975 or (sum(self.score_history[-50:]) > 10 and random.random() < 0.50):\n#             best_index = random.choice([i for i, x in enumerate(self.scores) if x == max(self.scores)])\n#         else:\n#             best_index = random.choices(range(len(self.scores)),weights=([max(0, score) for score in self.scores]))[0]\n\n\n        best_index = random.choices(range(self.valied_score_num),weights=([max(0, score) for score in self.scores[:self.valied_score_num]]))[0]\n\n        my_action = proposed_actions[best_index]\n        self.selection_log[best_index] += 1 \n        \n#         if random.random() < 0.2:\n# #             random.randint(0, 2)\n#             my_action = CEDE[my_action]\n    \n#         if observation.step == 998:\n#             print(self.selection_log)\n\n        self.last_action = my_action\n        self.last_proposed_actions = proposed_actions\n\n        return my_action\n    \n\n    \n# anti_geo_agents = [AntiGeoAgentWrapper(GeobotBeater(0.01, 20)), AntiGeoAgentWrapper(GeobotBeater(0.03, 20)), AntiGeoAgentWrapper(GeobotBeater(0.07, None)), AntiGeoAgentWrapper(GeobotBeater(0.12, None)),  AntiGeoAgentWrapper(GeobotBeater(0.3, None)),  AntiGeoAgentWrapper(GeobotBeater(0.8, None))]\nanti_geo_agents = [AntiGeoAgentWrapper(GeobotBeater(0.01, 20)), AntiGeoAgentWrapper(GeobotBeater(0.03, 20))]\n\n# agents = [AgentWrapper(Agent(0.01, None)), AgentWrapper(Agent(0.07, None)), AgentWrapper(Agent(0.05, 40)),AgentWrapper(Agent(0.03, 20)), AgentWrapper(Agent(0.05, 40)), AgentWrapper(Agent(0.12, None)),AgentWrapper(Agent(0.8, 20)), MirrorAgentWrapper(Agent(0.01, None)), MirrorAgentWrapper(Agent(0.07, None)),MirrorAgentWrapper(Agent(0.05, 40)),MirrorAgentWrapper(Agent(0.03, 20)), MirrorAgentWrapper(Agent(0.05, 40)), MirrorAgentWrapper(Agent(0.12, None)),MirrorAgentWrapper(Agent(0.8, 20))]\nagents = [AgentWrapper(Agent(0.07, None)), AgentWrapper(Agent(0.05, 40)),AgentWrapper(Agent(0.03, 20)),AgentWrapper(Agent(0.12, None)),  MirrorAgentWrapper(Agent(0.07, None)),MirrorAgentWrapper(Agent(0.05, 40)),MirrorAgentWrapper(Agent(0.03, 20)), MirrorAgentWrapper(Agent(0.12, None))]\ndllu_scoring_configs = [\n    [0.60,  3.00,    0.00,     -3.00,    0.00,      False,     False]\n] * len(agents) # * (len(agents)-1) + [[0.30,  3.00,    0.00,     -3.00,    0.05,      False,     True]]\ndllu_scoring_configs *= 3\n# def generate_random_dllu_scoring_configs():  \n#     return [\n#         [random.randrange(5)/10 + 0.3,  3.00,    0.00,     -3.00,    0.00,      False,     False],\n#     ] * len(dllu_scoring_configs)\n\nmain_agent = MixedAgent(agents, dllu_scoring_configs, use_cede=True, use_beat=True, anti_geo_agents=anti_geo_agents)\n\n\n\ndef call_agent(obs, conf):\n    return main_agent(obs, conf)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from kaggle_environments import evaluate, make, utils\n\nenv = make(\"rps\", debug=True)\nenv.run([\"geometry.py\", \"../input/rps-geometry2/geometry.py\"]);\nenv.render(mode=\"ipython\", width=600, height=450)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# import os\n# import pandas as pd\n# import kaggle_environments\n# from datetime import datetime\n# import multiprocessing as pymp\n# from tqdm import tqdm\n# import ray.util.multiprocessing as raymp\n# pd.set_option('display.max_rows', 100)\n\n# # function to return score\n# def get_result(match_settings):\n#     start = datetime.now()\n#     outcomes = kaggle_environments.evaluate(\n#         'rps', [match_settings[0], match_settings[1]], num_episodes=match_settings[2])\n#     won, lost, tie, avg_score = 0, 0, 0, 0.\n#     for outcome in outcomes:\n#         score = outcome[0]\n#         if score > 0: won += 1\n#         elif score < 0: lost += 1\n#         else: tie += 1\n#         avg_score += score\n#     elapsed = datetime.now() - start\n#     return match_settings[1], won, lost, tie, elapsed, float(avg_score) / float(match_settings[2])\n\n\n# def eval_agent_against_baselines(agent, baselines, num_episodes=20, use_ray=False):\n#     df = pd.DataFrame(\n#         columns=['wins', 'loses', 'ties', 'total time', 'avg. score'],\n#         index=range(len(baselines))\n#     )\n    \n#     if use_ray:\n#         pool = raymp.Pool()\n#     else:\n#         pool = pymp.Pool()\n        \n#     matches = [[agent, baseline, num_episodes] for baseline in baselines]\n\n#     results = []\n#     for content in pool.imap_unordered(get_result, matches):\n#         results.append(content)\n        \n#     for i , (baseline_agent, won, lost, tie, elapsed, avg_score) in enumerate(results):\n#         df.loc[i, 'wins'] = won\n#         df.loc[i, 'loses'] = lost\n#         df.loc[i, 'ties'] = tie\n#         df.loc[i, 'total time'] = elapsed\n#         df.loc[i, 'avg. score'] = avg_score\n        \n#     return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# agents = [AgentWrapper(Agent(0.07, None)), AgentWrapper(Agent(0.05, 40)),AgentWrapper(Agent(0.03, 20)), MirrorAgentWrapper(Agent(0.07, None)),MirrorAgentWrapper(Agent(0.05, 40)),MirrorAgentWrapper(Agent(0.03, 20))]\n# dllu_scoring_configs =  [\n#         [0.60,  3.00,    0.00,     -3.00,    0.00,      False,     False]\n#     ] * len(agents)\n# dllu_scoring_configs *= 3\n# use_cede_beat_agent = MixedAgent(agents, dllu_scoring_configs, use_cede=True, use_beat=True)\n\n# agents = [AgentWrapper(Agent(0.07, None)), AgentWrapper(Agent(0.05, 40)),AgentWrapper(Agent(0.03, 20)), MirrorAgentWrapper(Agent(0.07, None)),MirrorAgentWrapper(Agent(0.05, 40)),MirrorAgentWrapper(Agent(0.03, 20))]\n# dllu_scoring_configs =  [\n#         [0.60,  3.00,    0.00,     -3.00,    0.00,      False,     False]\n#     ] * len(agents)\n# dllu_scoring_configs *= 2\n# use_beat_agent = MixedAgent(agents, dllu_scoring_configs, use_cede=False, use_beat=True)\n\n# agents = [AgentWrapper(Agent(0.07, None)), MirrorAgentWrapper(Agent(0.07, None))]\n# dllu_scoring_configs =  [\n#         [0.60,  3.00,    0.00,     -3.00,    0.00,      False,     False]\n#     ] * len(agents)\n# dllu_scoring_configs *= 2\n# mixed_agent_cede_no_random_1 = MixedAgent(agents, dllu_scoring_configs, use_cede=True)\n\n\n# agents = [AgentWrapper(Agent(0.07, None)), AgentWrapper(Agent(0.05, 40)),AgentWrapper(Agent(0.03, 20)), MirrorAgentWrapper(Agent(0.07, None)),MirrorAgentWrapper(Agent(0.05, 40)),MirrorAgentWrapper(Agent(0.03, 20))]\n# dllu_scoring_configs =  [\n#         [0.60,  3.00,    0.00,     -3.00,    0.00,      False,     False]\n#     ] * len(agents)\n# dllu_scoring_configs *= 2\n# mixed_agent_add_cede_no_random = MixedAgent(agents, dllu_scoring_configs, use_cede=True)\n\n# base_lines = [Agent(0.07, None), mixed_agent_cede_no_random_1, use_cede_beat_agent, use_beat_agent]\n\n# eval_agent_against_baselines(mixed_agent_add_cede_no_random, base_lines , 30)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# from kaggle_environments import evaluate, make, utils\n# env = make(\"rps\", debug=True)\n\n# from geometry import AgentWrapper, MirrorAgentWrapper, Agent\n# from anti_geo import AntiGeoAgentWrapper, AntiGeoMirrorAgentWrapper, GeobotBeater\n\n# num_win=0\n# num_loss=0\n# num_matches=0\n# # 0.02, 0.05, 0.07, 0.1 , 0.15\n\n# for _ in range(5):\n#     env.reset()\n# #     result=env.run([GeobotBeater(0.02, None), Agent(0.01, 20)])\n#     result=env.run([\"geometry.py\", Agent(0.01, 20)])\n#     result=env.run([\"geometry.py\", \"../input/rps-geometry2/geometry.py\"])\n#     reward=result[-1][0][\"observation\"][\"reward\"]\n#     if reward>20:\n#         num_win+=1\n#     if reward<-20:\n#         num_loss+=1\n#     num_matches+=1\n    \n#     print(f\"{reward:+4.0f}, {num_matches:2d} matches, {num_win/num_matches:5.1%} win, {num_loss/num_matches:5.1%} loss\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}