{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Install kaggle-environments"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"!pip install 'kaggle-environments>=0.1.6'\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Explanation of The Architecture\n* If reward is more than <code><b>SIGNIFICANT_REWARD</b></code>, action of <code><b>my_agent</b></code> will be random until the end of the game\n* At <code><b>STRATEGY_EVALUATION_STEP</b></code> current strategy is evaluated and may be changed\n* Agent is making random action each <code><b>FORCED_RANDOM_ACTION_INTERVAL</b></code> amount of steps, if action for <code><b>my_agent</b></code> wasn't defined yet\n* If previous action of <code><b>my_agent</b></code> was taken from some pattern, efficiency of that pattern is evaluated\n* In each group of memory patterns, one memory pattern is either added or updated\n* If action for <code><b>my_agent</b></code> wasn't defined yet, it is searched for in every memory pattern of every group of memory patterns\n* If action for <code><b>my_agent</b></code> still wasn't found, it is chosen randomly\n"},{"metadata":{},"cell_type":"markdown","source":"# Release Random Goblin"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%writefile random_goblin.py\n\n# obviously\nimport random\n\ndef random_goblin(obs, conf):\n    \"\"\" bane of the leadreboard \"\"\"\n    return random.randint(0, 2)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Functions and Imports\nI suggest keeping them in alphabetical order"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%writefile submission.py\n# start executing cells from here to rewrite submission.py\n\nimport random\n\ndef evaluate_pattern_efficiency(previous_step_result):\n    \"\"\" \n        evaluate efficiency of the pattern and, if pattern is inefficient,\n        remove it from agent's memory\n    \"\"\"\n    pattern_group_index = previous_action[\"pattern_group_index\"]\n    pattern_index = previous_action[\"pattern_index\"]\n    pattern = groups_of_memory_patterns[pattern_group_index][\"memory_patterns\"][pattern_index]\n    pattern[\"reward\"] += previous_step_result\n    # if pattern is inefficient\n    if pattern[\"reward\"] <= EFFICIENCY_THRESHOLD:\n        # remove pattern from agent's memory\n        del groups_of_memory_patterns[pattern_group_index][\"memory_patterns\"][pattern_index]\n    \ndef find_action(group, group_index):\n    \"\"\" if possible, find my_action in this group of memory patterns \"\"\"\n    if len(current_memory) > group[\"memory_length\"]:\n        this_step_memory = current_memory[-group[\"memory_length\"]:]\n        memory_pattern, pattern_index = find_pattern(group[\"memory_patterns\"], this_step_memory, group[\"memory_length\"])\n        if memory_pattern != None:\n            my_action_amount = 0\n            for action in memory_pattern[\"opp_next_actions\"]:\n                # if this opponent's action occurred more times than currently chosen action\n                # or, if it occured the same amount of times and this one is choosen randomly among them\n                if (action[\"amount\"] > my_action_amount or\n                        (action[\"amount\"] == my_action_amount and random.random() > 0.5)):\n                    my_action_amount = action[\"amount\"]\n                    my_action = action[\"response\"]\n            return my_action, pattern_index\n    return None, None\n\ndef find_pattern(memory_patterns, memory, memory_length):\n    \"\"\" find appropriate pattern and its index in memory \"\"\"\n    for i in range(len(memory_patterns)):\n        actions_matched = 0\n        for j in range(memory_length):\n            if memory_patterns[i][\"actions\"][j] == memory[j]:\n                actions_matched += 1\n            else:\n                break\n        # if memory fits this pattern\n        if actions_matched == memory_length:\n            return memory_patterns[i], i\n    # appropriate pattern not found\n    return None, None\n\ndef get_step_result_for_my_agent(my_agent_action, opp_action):\n    \"\"\" \n        get result of the step for my_agent\n        1, 0 and -1 representing win, tie and lost results of the game respectively\n        reward will be taken from observation in the next release of kaggle environments\n    \"\"\"\n    if my_agent_action == opp_action:\n        return 0\n    elif (my_agent_action == (opp_action + 1)) or (my_agent_action == 0 and opp_action == 2):\n        return 1\n    else:\n        return -1\n    \ndef update_current_memory(obs, my_action):\n    \"\"\" add my_agent's current step to current_memory \"\"\"\n    # if there's too many actions in the current_memory\n    if len(current_memory) > current_memory_max_length:\n        # delete first two elements in current memory\n        # (actions of the oldest step in current memory)\n        del current_memory[:2]\n    # add agent's last action to agent's current memory\n    current_memory.append(my_action)\n    \ndef update_memory_pattern(obs, group):\n    \"\"\" if possible, update or add some memory pattern in this group \"\"\"\n    # if length of current memory is suitable for this group of memory patterns\n    if len(current_memory) > group[\"memory_length\"]:\n        # get memory of the previous step\n        # considering that last step actions of both agents are already present in current_memory\n        previous_step_memory = current_memory[-group[\"memory_length\"] - 2 : -2]\n        previous_pattern, pattern_index = find_pattern(group[\"memory_patterns\"], previous_step_memory, group[\"memory_length\"])\n        if previous_pattern == None:\n            previous_pattern = {\n                # list of actions of both players\n                \"actions\": previous_step_memory.copy(),\n                # total reward earned by using this pattern\n                \"reward\": 0,\n                # list of observed opponent's actions after each occurrence of this pattern\n                \"opp_next_actions\": [\n                    # action that was made by opponent,\n                    # amount of times that action occurred,\n                    # what should be the response of my_agent\n                    {\"action\": 0, \"amount\": 0, \"response\": 1},\n                    {\"action\": 1, \"amount\": 0, \"response\": 2},\n                    {\"action\": 2, \"amount\": 0, \"response\": 0}\n                ]\n            }\n            group[\"memory_patterns\"].append(previous_pattern)\n        # update previous_pattern\n        for action in previous_pattern[\"opp_next_actions\"]:\n            if action[\"action\"] == obs[\"lastOpponentAction\"]:\n                action[\"amount\"] += 1\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Global Variables"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%writefile -a submission.py\n# \"%%writefile -a submission.py\" will append the code below to submission.py,\n# it WILL NOT rewrite submission.py\n\n# maximum steps in a memory pattern\nSTEPS_MAX = 6\n# minimum steps in a memory pattern\nSTEPS_MIN = 3\n# lowest efficiency threshold of a memory pattern before being removed from agent's memory\nEFFICIENCY_THRESHOLD = -3\n# amount of steps between forced random actions\nFORCED_RANDOM_ACTION_INTERVAL = random.randint(STEPS_MIN, STEPS_MAX)\n# at this step current strategy is evaluated and may be changed\nSTRATEGY_EVALUATION_STEP = random.randint(250, 270)\n# reward that is considered singificant\nSIGNIFICANT_REWARD = 80\n# if reward is less than RANDOM_REWARD_THRESHOLD, opponent is stronger or random\nRANDOM_REWARD_THRESHOLD = 40\n\n# strategy against random on/off\nuse_strategy_against_random = False\n# use only random on/off\nuse_only_random = False\n# current memory of the agent\ncurrent_memory = []\n# previous action of my_agent\nprevious_action = {\n    \"action\": None,\n    # action was taken from pattern\n    \"action_from_pattern\": False,\n    \"pattern_group_index\": None,\n    \"pattern_index\": None\n}\n# amount of steps remained until next forced random action\nsteps_to_random = FORCED_RANDOM_ACTION_INTERVAL\n# maximum length of current_memory\ncurrent_memory_max_length = STEPS_MAX * 2\n# current reward of my_agent\n# will be taken from observation in the next release of kaggle environments\nreward = 0\n# counter of occurrence of each opponent's action\nactions_of_opponent = [0, 0, 0]\n# memory length of patterns in first group\n# STEPS_MAX is multiplied by 2 to consider both my_agent's and opponent's actions\ngroup_memory_length = current_memory_max_length\n# list of groups of memory patterns\ngroups_of_memory_patterns = []\nfor i in range(STEPS_MAX, STEPS_MIN - 1, -1):\n    groups_of_memory_patterns.append({\n        # how many steps in a row are in the pattern\n        \"memory_length\": group_memory_length,\n        # list of memory patterns\n        \"memory_patterns\": []\n    })\n    group_memory_length -= 2\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Create Agent"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%writefile -a submission.py\n# \"%%writefile -a submission.py\" will append the code below to submission.py,\n# it WILL NOT rewrite submission.py\n\ndef my_agent(obs, conf):\n    \"\"\" your ad here \"\"\"\n    global reward\n    global steps_to_random\n    global use_only_random\n    global use_strategy_against_random\n    \n    # action of my_agent\n    my_action = None\n            \n    # if reward is already significant, always go random\n    if not use_only_random and reward > SIGNIFICANT_REWARD:\n        use_only_random = True       \n    # if use only random\n    if use_only_random:\n        # choose action randomly\n        my_action = random.randint(0, 2)\n        \n    # evaluate current strategy and change it, if necessary\n    if obs[\"step\"] == STRATEGY_EVALUATION_STEP:\n        if reward < RANDOM_REWARD_THRESHOLD:\n            use_strategy_against_random = True    \n    # use strategy against random\n    if use_strategy_against_random and my_action == None:\n        # find most frequent action\n        most_frequent_action = actions_of_opponent.index(max(actions_of_opponent))\n        # find second most frequent action\n        left_from_m_f_a = most_frequent_action - 1\n        if left_from_m_f_a < 0:\n            left_from_m_f_a = 2\n        right_from_m_f_a = most_frequent_action + 1\n        if right_from_m_f_a > 2:\n            right_from_m_f_a = 0\n        if actions_of_opponent[left_from_m_f_a] >= actions_of_opponent[right_from_m_f_a]:\n            second_most_frequent_action = left_from_m_f_a\n        else:\n            second_most_frequent_action = right_from_m_f_a\n        # \n        if (most_frequent_action == (second_most_frequent_action + 1) or\n                (most_frequent_action == 0 and second_most_frequent_action == 2)):\n            my_action = most_frequent_action\n        else:\n            my_action = second_most_frequent_action\n    \n    # forced random action\n    steps_to_random -= 1\n    if steps_to_random <= 0:\n        steps_to_random = FORCED_RANDOM_ACTION_INTERVAL\n        if my_action == None:\n            # choose action randomly\n            my_action = random.randint(0, 2)\n            # save action's data\n            previous_action[\"action\"] = my_action\n            previous_action[\"action_from_pattern\"] = False\n            previous_action[\"pattern_group_index\"] = None\n            previous_action[\"pattern_index\"] = None\n    \n    # if it's not first step\n    if obs[\"step\"] > 0:\n        # count occurrence of each opponent's action\n        actions_of_opponent[obs[\"lastOpponentAction\"]] += 1\n        # add opponent's last step to current_memory\n        current_memory.append(obs[\"lastOpponentAction\"])\n        # previous step won or lost\n        previous_step_result = get_step_result_for_my_agent(current_memory[-2], current_memory[-1])\n        reward += previous_step_result\n        # if previous action of my_agent was taken from pattern\n        if previous_action[\"action_from_pattern\"]:\n            evaluate_pattern_efficiency(previous_step_result)\n    \n    for i in range(len(groups_of_memory_patterns)):\n        # if possible, update or add some memory pattern in this group\n        update_memory_pattern(obs, groups_of_memory_patterns[i])\n        # if action was not yet found\n        if my_action == None:\n            my_action, pattern_index = find_action(groups_of_memory_patterns[i], i)\n            if my_action != None:\n                # save action's data\n                previous_action[\"action\"] = my_action\n                previous_action[\"action_from_pattern\"] = True\n                previous_action[\"pattern_group_index\"] = i\n                previous_action[\"pattern_index\"] = pattern_index\n    \n    # if no action was found\n    if my_action == None:\n        # choose action randomly\n        my_action = random.randint(0, 2)\n        # save action's data\n        previous_action[\"action\"] = my_action\n        previous_action[\"action_from_pattern\"] = False\n        previous_action[\"pattern_group_index\"] = None\n        previous_action[\"pattern_index\"] = None\n    \n    # add my_agent's current step to current_memory\n    update_current_memory(obs, my_action)\n    return my_action\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Evaluate Agent"},{"metadata":{"trusted":true},"cell_type":"code","source":"from kaggle_environments import evaluate, make, utils\n\nresults = evaluate(\n                    \"rps\",\n#                     [\"submission.py\", \"submission.py\"],\n#                     [\"submission.py\", \"copy_opponent\"],\n#                     [\"submission.py\", \"reactionary\"],\n#                     [\"submission.py\", \"counter_reactionary\"],\n#                     [\"submission.py\", \"statistical\"],\n                    [\"submission.py\", \"random_goblin.py\"],\n                    \n                    num_episodes=10,\n                    configuration={\"agentExec\": \"LOCAL\"}\n                  )\n\nwon = 0\nlost = 0\ntie = 0\nfor result in results:\n    score = result[0]\n    \n    if score > 0:\n        won += 1\n    elif score < 0:\n        lost += 1\n    else:\n        tie += 1\n\nprint(f'\\nwon: {won}, lost: {lost}, tie: {tie}\\n')\nfor result in results:\n    print(result)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Test Agent"},{"metadata":{"trusted":true},"cell_type":"code","source":"env = make(\"rps\", debug=True)\n# env.run([\"submission.py\", \"submission.py\"])\n# env.run([\"submission.py\", \"copy_opponent\"])\n# env.run([\"submission.py\", \"reactionary\"])\n# env.run([\"submission.py\", \"counter_reactionary\"])\nenv.run([\"submission.py\", \"statistical\"])\n# env.run([\"submission.py\", \"random_goblin.py\"])\nenv.render(mode=\"ipython\", width=500, height=450)\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}