{"cells":[{"metadata":{},"cell_type":"markdown","source":"# (Not so) Markov vs Nash Equilibrium: Rock Paper Scissors\n\n\n### 100 seasons of Markov vs Nash on Rock Paper Scissors\n### 1000 episodes per season\n\n### Bonus: Dataset generation"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"1\"></a>\n<h2 style='background:#FBE338; border:0; color:black'><center>Agent: Nash Equilibrium<center><h2>"},{"metadata":{},"cell_type":"markdown","source":"![](https://upload.wikimedia.org/wikipedia/commons/thumb/a/a9/John_Forbes_Nash%2C_Jr._by_Peter_Badge.jpg/220px-John_Forbes_Nash%2C_Jr._by_Peter_Badge.jpg)"},{"metadata":{},"cell_type":"markdown","source":"*...if we all go for the blonde we are blocking each other.*"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"%%writefile nash_equilibrium.py\n\nimport random\n\ndef nash_equilibrium(observation, configuration):\n    return random.randint(0, 2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"1\"></a>\n<h2 style='background:#FBE338; border:0; color:black'><center>Agent: (Not so) Markov<center><h2>"},{"metadata":{},"cell_type":"markdown","source":"![](https://upload.wikimedia.org/wikipedia/commons/thumb/a/a8/Andrei_Markov.jpg/220px-Andrei_Markov.jpg) "},{"metadata":{},"cell_type":"markdown","source":"*...breaking chains excites me!*"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"%%writefile markov_agent.py\n\nimport numpy as np\nimport collections\n\ndef markov_agent(observation, configuration):\n    k = 2\n    global table, action_seq\n    if observation.step % 250 == 0: # refresh table every 250 steps\n        action_seq, table = [], collections.defaultdict(lambda: [1, 1, 1])    \n    if len(action_seq) <= 2 * k + 1:\n        action = int(np.random.randint(3))\n        if observation.step > 0:\n            action_seq.extend([observation.lastOpponentAction, action])\n        else:\n            action_seq.append(action)\n        return action\n    # update table\n    key = ''.join([str(a) for a in action_seq[:-1]])\n    table[key][observation.lastOpponentAction] += 1\n    # update action seq\n    action_seq[:-2] = action_seq[2:]\n    action_seq[-2] = observation.lastOpponentAction\n    # predict opponent next move\n    key = ''.join([str(a) for a in action_seq[:-1]])\n    if observation.step < 500:\n        next_opponent_action_pred = np.argmax(table[key])\n    else:\n        scores = np.array(table[key])\n        next_opponent_action_pred = np.random.choice(3, p=scores/scores.sum()) # add stochasticity for second part of the game\n    # make an action\n    action = (next_opponent_action_pred + 1) % 3\n    # if high probability to lose -> let's surprise our opponent with sudden change of our strategy\n    if observation.step > 900:\n        action = next_opponent_action_pred\n    action_seq[-1] = action\n    return int(action)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"11\"></a>\n<h2 style='background:#FBE338; border:0; color:black'><center>Validate<center><h2>\n\n\n"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"from kaggle_environments import make, evaluate\n\nenv = make(\"rps\", configuration={\"episodeSteps\": 1000})\n\nenv.run([\"markov_agent.py\", \"nash_equilibrium.py\"])\n\nenv.render(mode=\"ipython\", width=800, height=800)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"11\"></a>\n<h2 style='background:#FBE338; border:0; color:black'><center>Action<center><h2>\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"seasons = 100\nepisodes = 1000","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport json\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom kaggle_environments import make\n\nfrom IPython.display import Markdown as md\n\naction_board = pd.DataFrame(columns = [\"season\",\n                                      \"episode\",\n                                      \"Markov Action\",\n                                      \"Nash Action\",\n                                      \"Markov Reward\",\n                                      \"Nash Reward\"])\nleaderboard = pd.DataFrame(columns = [\"season\",\n                                      \"Markov Reward\",\n                                      \"Nash Reward\"])\n\n\nindex = 0\nenv = make(\"rps\", configuration={\"episodeSteps\": episodes})\n\nfor season in range(seasons):\n    env.reset()\n    results = env.run([\"markov_agent.py\", \"nash_equilibrium.py\"])\n    for result in results:\n        if (result[0].observation.step == 0):\n            continue\n        action_board = action_board.append({\"season\": season,\n                              \"episode\": result[0].observation.step,\n                              \"Markov Action\": result[0].action,\n                              \"Nash Action\": result[1].action,\n                              \"Markov Reward\": result[0].reward,\n                              \"Nash Reward\": result[1].reward},\n                                        ignore_index=True)\n        if result[0].status == \"DONE\":\n            leaderboard = leaderboard.append({\"season\": season,\n                              \"Markov Reward\": result[0].reward,\n                              \"Nash Reward\": result[1].reward},\n                                        ignore_index=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h1 style='background:#FBE338; border:0; color:black'><center>Result<center><h1>\n"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"md('# \\(Not so\\) Markov - Nash Equilibrium : {} - {}'.format(len(leaderboard[leaderboard[\"Markov Reward\"] > 0]), len(leaderboard[leaderboard[\"Nash Reward\"] > 0])))","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"md('# Tie : {}'.format(len(leaderboard[leaderboard[\"Markov Reward\"] == 0])))","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"if (len(leaderboard[leaderboard[\"Markov Reward\"] > 0]) == len(leaderboard[leaderboard[\"Nash Reward\"] > 0])):\n    winner = \"Tie!\"\nelif (len(leaderboard[leaderboard[\"Markov Reward\"] > 0]) > len(leaderboard[leaderboard[\"Nash Reward\"] > 0])):\n    winner = \"Winner is Markov!\"\nelse:\n    winner = \"Winner is Nash!\"\nmd('<a id=\"11\"></a><h1 style=\\'background:#FBE338; border:0; color:black\\'><center>{}<center><h2>'.format(winner))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h1 style='background:#FBE338; border:0; color:black'><center>Analysis<center><h1>"},{"metadata":{},"cell_type":"markdown","source":"# Season's results"},{"metadata":{"trusted":true},"cell_type":"code","source":"leaderboard.plot(subplots=True, figsize=(15,10))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Season's reward histogram"},{"metadata":{"trusted":true},"cell_type":"code","source":"leaderboard[['Markov Reward', 'Nash Reward']].plot.hist(bins=10,  alpha=0.5, figsize=(15,10))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Actions histogram"},{"metadata":{"trusted":true},"cell_type":"code","source":"action_board[['Markov Action', 'Nash Action']].plot.hist(bins=3, alpha=0.5, xticks=[0,1,2], figsize=(15,10))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## All episodes reward"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(20,10))\nfor i, g in action_board.groupby('season'):\n    g.plot(x='episode', y='Markov Reward', ax=ax, legend=False )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## First half rewards"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(20,15))\nfor i, g in action_board[(action_board['episode']<episodes/2)].groupby('season'):\n    g.plot(x='episode', y='Markov Reward', ax=ax, legend=False )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Mid-episodes reward"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(20,15))\nfor i, g in action_board[((action_board['episode']>episodes/3) & (action_board['episode']<2*episodes/3))].groupby('season'):\n    g.plot(x='episode', y='Markov Reward', ax=ax, legend=False )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Last half rewards"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(20,15))\nfor i, g in action_board[action_board['episode']>episodes/2].groupby('season'):\n    g.plot(x='episode', y='Markov Reward', ax=ax, legend=False )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h1 style='background:#FBE338; border:0; color:black'><center>Conclusion<center><h1>"},{"metadata":{},"cell_type":"markdown","source":"* Agent `(Not so) Markov` has a clear tendency to shoot `paper`.\n* Agent's `(Not so) Markov` change of strategy at `500` & `900` episode is not always good strategy.\n* When reward is around `-20` looks like a good point to change strategy."},{"metadata":{},"cell_type":"markdown","source":"<h1 style='background:#FBE338; border:0; color:black'><center>Dataset<center><h1>"},{"metadata":{},"cell_type":"markdown","source":"Dataset is exported, collected and publicly shared in [Rock Paper Scissors Agents Battles](https://www.kaggle.com/jumaru/rock-paper-scissors-agents-battles) dataset."},{"metadata":{},"cell_type":"markdown","source":"## Leaderboard"},{"metadata":{},"cell_type":"markdown","source":"### First 5 seasons rewards"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"leaderboard.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Last 5 seasons rewards"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"leaderboard.tail()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Rewards Statistics "},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"leaderboard.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Action board"},{"metadata":{},"cell_type":"markdown","source":"## First 5 actions"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"action_board.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Last 5 actions"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"action_board.tail()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Actions Statistics"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"action_board.drop(columns='season').describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data export"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# Report boards\nleaderboard_csv = 'Not_so_Markov_leaderboard_S' + str(seasons) + 'E' + str(episodes) + '.csv'\naction_board_csv = 'Not_so_Markov_action_board_S'+ str(seasons) + 'E' + str(episodes) + '.csv'\nleaderboard.to_csv(leaderboard_csv)\naction_board.to_csv(action_board_csv)\nprint(leaderboard_csv)\nprint(action_board_csv)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# References"},{"metadata":{},"cell_type":"markdown","source":"* [Rock Paper Scissors - Nash Equilibrium Strategy](https://www.kaggle.com/ihelon/rock-paper-scissors-nash-equilibrium-strategy) & [Rock Paper Scissors - Agents Comparison](https://www.kaggle.com/ihelon/rock-paper-scissors-agents-comparison) by [Yaroslav Isaienkov](https://www.kaggle.com/ihelon)\n* [(Not so) Markov](https://www.kaggle.com/alexandersamarin/not-so-markov) by [Alexander Samarin](https://www.kaggle.com/alexandersamarin)\n* [LB simulation](https://www.kaggle.com/superant/lb-simulation) by [Ant üêú](https://www.kaggle.com/superant)"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}