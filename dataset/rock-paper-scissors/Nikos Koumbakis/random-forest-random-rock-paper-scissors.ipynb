{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Random Forest Random - Rock Paper Scissors\n\n`Random Forest Random` is a Rock Paper Scissors Agent that makes predictions using the `Random Forest Classification` with a bit of `random`. The `random` on this agent has been limited only when losing (and partially on window length) so that the true possibilities of the algorithm are revealed.\n\nScope of this notebook is to extensively test the performance of `Random Forest Random` vs public agents over 10 rounds of 1000 episodes against each agent.\n\nThe notebook is based on [Rock Paper Scissors - Agents Comparison](https://www.kaggle.com/ihelon/rock-paper-scissors-agents-comparison) with adaptations and extras so that can focus on a single agent."},{"metadata":{},"cell_type":"markdown","source":"![](https://storage.googleapis.com/kaggle-competitions/kaggle/22838/logos/header.png?t=2020-11-02-21-55-44)"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"1\"></a>\n<h2 style='background:#FBE338; border:0; color:black'><center>Agent: Random Forest Random<center><h2>"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%writefile random_forest_random.py\nimport random\nimport pandas as pd\nimport numpy as np\nfrom sklearn.ensemble import RandomForestClassifier\n\nactions =  np.empty((0,0), dtype = int)\nobservations =  np.empty((0,0), dtype = int)\ntotal_reward = 0\n\ndef random_forest_random(observation, configuration):\n    global actions, observations, total_reward\n    \n    if observation.step == 0:\n        action = random.randint(0,2)\n        actions = np.append(actions , [action])\n        return action\n    \n    if observation.step == 1:\n        action = random.randint(0,2)\n        actions = np.append(actions , [action])\n        observations = np.append(observations , [observation.lastOpponentAction])\n        # Keep track of score\n        winner = int((3 + actions[-1] - observation.lastOpponentAction) % 3);\n        if winner == 1:\n            total_reward = total_reward + 1\n        elif winner == 2:\n            total_reward = total_reward - 1        \n        return action\n\n    # Get Observation to make the tables (actions & obervations) even.\n    observations = np.append(observations , [observation.lastOpponentAction])\n    \n    # Prepare Data for training\n    # :-1 as we dont have feedback yet.\n    X_train = np.vstack((actions[:-1], observations[:-1])).T\n    \n    # Create Y by rolling observations to bring future a step earlier \n    shifted_observations = np.roll(observations, -1)\n    \n    # trim rolled & last element from rolled observations\n    y_train = shifted_observations[:-1].T\n    \n    # Set the history period. Long chains here will need a lot of time\n    if len(X_train) > 25:\n        random_window_size = 10 + random.randint(0,10)\n        X_train = X_train[-random_window_size:]\n        y_train = y_train[-random_window_size:]\n   \n    # Train a classifier model\n    model = RandomForestClassifier(n_estimators=25)\n    model.fit(X_train, y_train)\n\n    # Predict\n    X_test = np.empty((0,0), dtype = int)\n    X_test = np.append(X_test, [int(actions[-1]), observation.lastOpponentAction])\n    prediction = model.predict(X_test.reshape(1, -1))\n\n    # Keep track of score\n    winner = int((3 + actions[-1] - observation.lastOpponentAction) % 3);\n    if winner == 1:\n        total_reward = total_reward + 1\n    elif winner == 2:\n        total_reward = total_reward - 1\n   \n    # Prepare action\n    action = int((prediction + 1) % 3)\n    \n    # If losing a bit then change strategy and break the patterns by playing a bit random\n    if total_reward < -2:\n        win_tie = random.randint(0,1)\n        action = int((prediction + win_tie) % 3)\n\n    # Update actions\n    actions = np.append(actions , [action])\n\n    # Action \n    return action ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"top\"></a>\n\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='color:black; background:#FBE338; border:0' role=\"tab\" aria-controls=\"home\"><center>Quick Navigation</center></h3>\n\n## Opponents\n* [Agent: Hit The Last Own Action](#1)\n* [Agent: Rock](#2)\n* [Agent: Paper](#3)\n* [Agent: Scissors](#4)\n* [Agent: Copy Opponent](#5)\n* [Agent: Reactionary](#6)\n* [Agent: Counter Reactionary](#7)\n* [Agent: Statistical](#8)\n* [Agent: Nash Equilibrium](#9)\n* [Agent: Markov Agent](#10)\n* [Agent: Memory Patterns](#11)\n* [Agent: Multi Armed Bandit](#12)\n* [Agent: Opponent Transition Matrix](#13)\n* [Agent: Decision Tree Classifier](#14)\n* [Agent: Statistical Prediction](#15)\n\n### Battle    \n* [Setup and validation](#100)\n* [Marathon: Random Forest Random against all agents](#102)\n* [Results](#103)\n* [Review](#104)\n"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"1\"></a>\n<h2 style='background:#FBE338; border:0; color:black'><center>Agent: Hit The Last Own Action<center><h2>"},{"metadata":{},"cell_type":"markdown","source":"The idea of the agent:\n\n- A lot of agents use a simple baseline - copy the last action of the opponent.   \n- That's why we can simply hit our last actions (new action of the opponent)"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"%%writefile hit_the_last_own_action.py\n\nmy_last_action = 0\n\ndef hit_the_last_own_action(observation, configuration):\n    global my_last_action\n    my_last_action = (my_last_action + 1) % 3\n    \n    return my_last_action","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"2\"></a>\n<h2 style='background:#FBE338; border:0; color:black'><center>Agent: Rock<center><h2>"},{"metadata":{},"cell_type":"markdown","source":"Copy from: https://github.com/Kaggle/kaggle-environments/blob/master/kaggle_environments/envs/rps/agents.py    \n\nAlways uses Rock action"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"%%writefile rock.py\n\ndef rock(observation, configuration):\n    return 0","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"3\"></a>\n<h2 style='background:#FBE338; border:0; color:black'><center>Agent: Paper<center><h2>"},{"metadata":{},"cell_type":"markdown","source":"Copy from: https://github.com/Kaggle/kaggle-environments/blob/master/kaggle_environments/envs/rps/agents.py\n\nAlways uses Paper action"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"%%writefile paper.py\n\ndef paper(observation, configuration):\n    return 1\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"4\"></a>\n<h2 style='background:#FBE338; border:0; color:black'><center>Agent: Scissors<center><h2>"},{"metadata":{},"cell_type":"markdown","source":"Copy from: https://github.com/Kaggle/kaggle-environments/blob/master/kaggle_environments/envs/rps/agents.py\n\nAlways uses Scissors action"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"%%writefile scissors.py\n\ndef scissors(observation, configuration):\n    return 2","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"5\"></a>\n<h2 style='background:#FBE338; border:0; color:black'><center>Agent: Copy Opponent<center><h2>"},{"metadata":{},"cell_type":"markdown","source":"Copy from: https://github.com/Kaggle/kaggle-environments/blob/master/kaggle_environments/envs/rps/agents.py\n\nCopy the last action of the opponent"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"%%writefile copy_opponent.py\n\nimport random\nfrom kaggle_environments.envs.rps.utils import get_score\n\ndef copy_opponent(observation, configuration):\n    if observation.step > 0:\n        return observation.lastOpponentAction\n    else:\n        return random.randrange(0, configuration.signs)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"6\"></a>\n<h2 style='background:#FBE338; border:0; color:black'><center>Agent: Reactionary<center><h2>\n"},{"metadata":{},"cell_type":"markdown","source":"Copy from: https://github.com/Kaggle/kaggle-environments/blob/master/kaggle_environments/envs/rps/agents.py\n\nHit the last action of the opponent"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"%%writefile reactionary.py\n\nimport random\nfrom kaggle_environments.envs.rps.utils import get_score\n\nlast_react_action = None\n\n\ndef reactionary(observation, configuration):\n    global last_react_action\n    if observation.step == 0:\n        last_react_action = random.randrange(0, configuration.signs)\n    elif get_score(last_react_action, observation.lastOpponentAction) <= 1:\n        last_react_action = (observation.lastOpponentAction + 1) % configuration.signs\n\n    return last_react_action","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"7\"></a>\n<h2 style='background:#FBE338; border:0; color:black'><center>Agent: Counter Reactionary<center><h2>\n\n"},{"metadata":{},"cell_type":"markdown","source":"Copy from: https://github.com/Kaggle/kaggle-environments/blob/master/kaggle_environments/envs/rps/agents.py"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"%%writefile counter_reactionary.py\n\nimport random\nfrom kaggle_environments.envs.rps.utils import get_score\n\nlast_counter_action = None\n\n\ndef counter_reactionary(observation, configuration):\n    global last_counter_action\n    if observation.step == 0:\n        last_counter_action = random.randrange(0, configuration.signs)\n    elif get_score(last_counter_action, observation.lastOpponentAction) == 1:\n        last_counter_action = (last_counter_action + 2) % configuration.signs\n    else:\n        last_counter_action = (observation.lastOpponentAction + 1) % configuration.signs\n\n    return last_counter_action","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"8\"></a>\n<h2 style='background:#FBE338; border:0; color:black'><center>Agent: Statistical<center><h2>\n\n\n"},{"metadata":{},"cell_type":"markdown","source":"Copy from: https://github.com/Kaggle/kaggle-environments/blob/master/kaggle_environments/envs/rps/agents.py"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"%%writefile statistical.py\n\nimport random\nfrom kaggle_environments.envs.rps.utils import get_score\n\naction_histogram = {}\n\ndef statistical(observation, configuration):\n    global action_histogram\n    if observation.step == 0:\n        action_histogram = {}\n        return\n    action = observation.lastOpponentAction\n    if action not in action_histogram:\n        action_histogram[action] = 0\n    action_histogram[action] += 1\n    mode_action = None\n    mode_action_count = None\n    for k, v in action_histogram.items():\n        if mode_action_count is None or v > mode_action_count:\n            mode_action = k\n            mode_action_count = v\n            continue\n\n    return (mode_action + 1) % configuration.signs","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"9\"></a>\n<h2 style='background:#FBE338; border:0; color:black'><center>Agent: Nash Equilibrium<center><h2>\n\n\n"},{"metadata":{},"cell_type":"markdown","source":"Copy from kernel [Rock Paper Scissors - Nash Equilibrium Strategy](https://www.kaggle.com/ihelon/rock-paper-scissors-nash-equilibrium-strategy)\n\nNash Equilibrium Strategy (always random)"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"%%writefile nash_equilibrium.py\n\nimport random\n\ndef nash_equilibrium(observation, configuration):\n    return random.randint(0, 2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"10\"></a>\n<h2 style='background:#FBE338; border:0; color:black'><center>Agent: Markov Agent<center><h2>\n\n\n"},{"metadata":{},"cell_type":"markdown","source":"Copy from kernel [(Not so) Markov ⛓️](https://www.kaggle.com/alexandersamarin/not-so-markov)"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"%%writefile markov_agent.py\n\nimport numpy as np\nimport collections\n\ndef markov_agent(observation, configuration):\n    k = 2\n    global table, action_seq\n    if observation.step % 250 == 0: # refresh table every 250 steps\n        action_seq, table = [], collections.defaultdict(lambda: [1, 1, 1])    \n    if len(action_seq) <= 2 * k + 1:\n        action = int(np.random.randint(3))\n        if observation.step > 0:\n            action_seq.extend([observation.lastOpponentAction, action])\n        else:\n            action_seq.append(action)\n        return action\n    # update table\n    key = ''.join([str(a) for a in action_seq[:-1]])\n    table[key][observation.lastOpponentAction] += 1\n    # update action seq\n    action_seq[:-2] = action_seq[2:]\n    action_seq[-2] = observation.lastOpponentAction\n    # predict opponent next move\n    key = ''.join([str(a) for a in action_seq[:-1]])\n    if observation.step < 500:\n        next_opponent_action_pred = np.argmax(table[key])\n    else:\n        scores = np.array(table[key])\n        next_opponent_action_pred = np.random.choice(3, p=scores/scores.sum()) # add stochasticity for second part of the game\n    # make an action\n    action = (next_opponent_action_pred + 1) % 3\n    # if high probability to lose -> let's surprise our opponent with sudden change of our strategy\n    if observation.step > 900:\n        action = next_opponent_action_pred\n    action_seq[-1] = action\n    return int(action)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"11\"></a>\n<h2 style='background:#FBE338; border:0; color:black'><center>Agent: Memory Patterns<center><h2>\n\n\n"},{"metadata":{},"cell_type":"markdown","source":"Copy from kernel [Rock, Paper, Scissors with Memory Patterns](https://www.kaggle.com/yegorbiryukov/rock-paper-scissors-with-memory-patterns)"},{"metadata":{"_kg_hide-output":false,"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"%%writefile memory_patterns.py\n\nimport random\n\n# how many steps in a row are in the pattern (multiplied by two)\nmemory_length = 6\n# current memory of the agent\ncurrent_memory = []\n# list of memory patterns\nmemory_patterns = []\n\ndef find_pattern(memory):\n    \"\"\" find appropriate pattern in memory \"\"\"\n    for pattern in memory_patterns:\n        actions_matched = 0\n        for i in range(memory_length):\n            if pattern[\"actions\"][i] == memory[i]:\n                actions_matched += 1\n            else:\n                break\n        # if memory fits this pattern\n        if actions_matched == memory_length:\n            return pattern\n    # appropriate pattern not found\n    return None\n\ndef my_agent(obs, conf):\n    \"\"\" your ad here \"\"\"\n    # if it's not first step, add opponent's last action to agent's current memory\n    if obs[\"step\"] > 0:\n        current_memory.append(obs[\"lastOpponentAction\"])\n    # if length of current memory is bigger than necessary for a new memory pattern\n    if len(current_memory) > memory_length:\n        # get momory of the previous step\n        previous_step_memory = current_memory[:memory_length]\n        previous_pattern = find_pattern(previous_step_memory)\n        if previous_pattern == None:\n            previous_pattern = {\n                \"actions\": previous_step_memory.copy(),\n                \"opp_next_actions\": [\n                    {\"action\": 0, \"amount\": 0, \"response\": 1},\n                    {\"action\": 1, \"amount\": 0, \"response\": 2},\n                    {\"action\": 2, \"amount\": 0, \"response\": 0}\n                ]\n            }\n            memory_patterns.append(previous_pattern)\n        for action in previous_pattern[\"opp_next_actions\"]:\n            if action[\"action\"] == obs[\"lastOpponentAction\"]:\n                action[\"amount\"] += 1\n        # delete first two elements in current memory (actions of the oldest step in current memory)\n        del current_memory[:2]\n    my_action = random.randint(0, 2)\n    pattern = find_pattern(current_memory)\n    if pattern != None:\n        my_action_amount = 0\n        for action in pattern[\"opp_next_actions\"]:\n            # if this opponent's action occurred more times than currently chosen action\n            # or, if it occured the same amount of times, choose action randomly among them\n            if (action[\"amount\"] > my_action_amount or\n                    (action[\"amount\"] == my_action_amount and random.random() > 0.5)):\n                my_action_amount = action[\"amount\"]\n                my_action = action[\"response\"]\n    current_memory.append(my_action)\n    return my_action","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"12\"></a>\n<h2 style='background:#FBE338; border:0; color:black'><center>Agent: Multi Armed Bandit<center><h2>"},{"metadata":{},"cell_type":"markdown","source":"Copy from kernel [Multi-armed bandit vs deterministic agents](https://www.kaggle.com/ilialar/multi-armed-bandit-vs-deterministic-agents)"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"%%writefile multi_armed_bandit.py\n\n\nimport pandas as pd\nimport numpy as np\nimport json\n\n\n# base class for all agents, random agent\nclass agent():\n    def initial_step(self):\n        return np.random.randint(3)\n    \n    def history_step(self, history):\n        return np.random.randint(3)\n    \n    def step(self, history):\n        if len(history) == 0:\n            return int(self.initial_step())\n        else:\n            return int(self.history_step(history))\n    \n# agent that returns (previousCompetitorStep + shift) % 3\nclass mirror_shift(agent):\n    def __init__(self, shift=0):\n        self.shift = shift\n    \n    def history_step(self, history):\n        return (history[-1]['competitorStep'] + self.shift) % 3\n    \n    \n# agent that returns (previousPlayerStep + shift) % 3\nclass self_shift(agent):\n    def __init__(self, shift=0):\n        self.shift = shift\n    \n    def history_step(self, history):\n        return (history[-1]['step'] + self.shift) % 3    \n\n\n# agent that beats the most popular step of competitor\nclass popular_beater(agent):\n    def history_step(self, history):\n        counts = np.bincount([x['competitorStep'] for x in history])\n        return (int(np.argmax(counts)) + 1) % 3\n\n    \n# agent that beats the agent that beats the most popular step of competitor\nclass anti_popular_beater(agent):\n    def history_step(self, history):\n        counts = np.bincount([x['step'] for x in history])\n        return (int(np.argmax(counts)) + 2) % 3\n    \n    \n# simple transition matrix: previous step -> next step\nclass transition_matrix(agent):\n    def __init__(self, deterministic = False, counter_strategy = False, init_value = 0.1, decay = 1):\n        self.deterministic = deterministic\n        self.counter_strategy = counter_strategy\n        if counter_strategy:\n            self.step_type = 'step' \n        else:\n            self.step_type = 'competitorStep'\n        self.init_value = init_value\n        self.decay = decay\n        \n    def history_step(self, history):\n        matrix = np.zeros((3,3)) + self.init_value\n        for i in range(len(history) - 1):\n            matrix = (matrix - self.init_value) / self.decay + self.init_value\n            matrix[int(history[i][self.step_type]), int(history[i+1][self.step_type])] += 1\n\n        if  self.deterministic:\n            step = np.argmax(matrix[int(history[-1][self.step_type])])\n        else:\n            step = np.random.choice([0,1,2], p = matrix[int(history[-1][self.step_type])]/matrix[int(history[-1][self.step_type])].sum())\n        \n        if self.counter_strategy:\n            # we predict our step using transition matrix (as competitor can do) and beat probable competitor step\n            return (step + 2) % 3 \n        else:\n            # we just predict competitors step and beat it\n            return (step + 1) % 3\n    \n\n# similar to the transition matrix but rely on both previous steps\nclass transition_tensor(agent):\n    \n    def __init__(self, deterministic = False, counter_strategy = False, init_value = 0.1, decay = 1):\n        self.deterministic = deterministic\n        self.counter_strategy = counter_strategy\n        if counter_strategy:\n            self.step_type1 = 'step' \n            self.step_type2 = 'competitorStep'\n        else:\n            self.step_type2 = 'step' \n            self.step_type1 = 'competitorStep'\n        self.init_value = init_value\n        self.decay = decay\n        \n    def history_step(self, history):\n        matrix = np.zeros((3,3, 3)) + 0.1\n        for i in range(len(history) - 1):\n            matrix = (matrix - self.init_value) / self.decay + self.init_value\n            matrix[int(history[i][self.step_type1]), int(history[i][self.step_type2]), int(history[i+1][self.step_type1])] += 1\n\n        if  self.deterministic:\n            step = np.argmax(matrix[int(history[-1][self.step_type1]), int(history[-1][self.step_type2])])\n        else:\n            step = np.random.choice([0,1,2], p = matrix[int(history[-1][self.step_type1]), int(history[-1][self.step_type2])]/matrix[int(history[-1][self.step_type1]), int(history[-1][self.step_type2])].sum())\n        \n        if self.counter_strategy:\n            # we predict our step using transition matrix (as competitor can do) and beat probable competitor step\n            return (step + 2) % 3 \n        else:\n            # we just predict competitors step and beat it\n            return (step + 1) % 3\n\n    \nagents = {\n    'mirror_0': mirror_shift(0),\n    'mirror_1': mirror_shift(1),  \n    'mirror_2': mirror_shift(2),\n    'self_0': self_shift(0),\n    'self_1': self_shift(1),  \n    'self_2': self_shift(2),\n    'popular_beater': popular_beater(),\n    'anti_popular_beater': anti_popular_beater(),\n    'random_transitison_matrix': transition_matrix(False, False),\n    'determenistic_transitison_matrix': transition_matrix(True, False),\n    'random_self_trans_matrix': transition_matrix(False, True),\n    'determenistic_self_trans_matrix': transition_matrix(True, True),\n    'random_transitison_tensor': transition_tensor(False, False),\n    'determenistic_transitison_tensor': transition_tensor(True, False),\n    'random_self_trans_tensor': transition_tensor(False, True),\n    'determenistic_self_trans_tensor': transition_tensor(True, True),\n    \n    'random_transitison_matrix_decay': transition_matrix(False, False, decay = 1.05),\n    'random_self_trans_matrix_decay': transition_matrix(False, True, decay = 1.05),\n    'random_transitison_tensor_decay': transition_tensor(False, False, decay = 1.05),\n    'random_self_trans_tensor_decay': transition_tensor(False, True, decay = 1.05),\n    \n    'determenistic_transitison_matrix_decay': transition_matrix(True, False, decay = 1.05),\n    'determenistic_self_trans_matrix_decay': transition_matrix(True, True, decay = 1.05),\n    'determenistic_transitison_tensor_decay': transition_tensor(True, False, decay = 1.05),\n    'determenistic_self_trans_tensor_decay': transition_tensor(True, True, decay = 1.05),\n}\n\n    \ndef multi_armed_bandit_agent (observation, configuration):\n    \n    # bandits' params\n    step_size = 2 # how much we increase a and b \n    decay_rate = 1.05 # how much do we decay old historical data\n    \n    # I don't see how to use any global variables, so will save everything to a CSV file\n    # Using pandas for this is too much, but it can be useful later and it is convinient to analyze\n    def save_history(history, file = 'history.csv'):\n        pd.DataFrame(history).to_csv(file, index = False)\n\n    def load_history(file = 'history.csv'):\n        return pd.read_csv(file).to_dict('records')\n    \n    \n    def log_step(step = None, history = None, agent = None, competitorStep = None):\n        if step is None:\n            step = np.random.randint(3)\n        if history is None:\n            history = []\n        history.append({'step': step, 'competitorStep': competitorStep, 'agent': agent})\n        save_history(history)\n        return step\n    \n    def update_competitor_step(history, competitorStep):\n        history[-1]['competitorStep'] = int(competitorStep)\n        return history\n        \n    \n    # load history\n    if observation.step == 0:\n        history = []\n        bandit_state = {k:[1,1] for k in agents.keys()}\n    else:\n        history = update_competitor_step(load_history(), observation.lastOpponentAction)\n        \n        # load the state of the bandit\n        with open('bandit.json') as json_file:\n            bandit_state = json.load(json_file)\n        \n        # updating bandit_state using the result of the previous step\n        # we can update all states even those that were not used\n        for name, agent in agents.items():\n            agent_step = agent.step(history[:-1])\n            bandit_state[name][1] = (bandit_state[name][1] - 1) / decay_rate + 1\n            bandit_state[name][0] = (bandit_state[name][0] - 1) / decay_rate + 1\n            \n            if (history[-1]['competitorStep'] - agent_step) % 3 == 1:\n                bandit_state[name][1] += step_size\n            elif (history[-1]['competitorStep'] - agent_step) % 3 == 2:\n                bandit_state[name][0] += step_size\n            else:\n                bandit_state[name][0] += step_size/2\n                bandit_state[name][1] += step_size/2\n            \n    with open('bandit.json', 'w') as outfile:\n        json.dump(bandit_state, outfile)\n    \n    \n    # generate random number from Beta distribution for each agent and select the most lucky one\n    best_proba = -1\n    best_agent = None\n    for k in bandit_state.keys():\n        proba = np.random.beta(bandit_state[k][0],bandit_state[k][1])\n        if proba > best_proba:\n            best_proba = proba\n            best_agent = k\n        \n    step = agents[best_agent].step(history)\n    \n    return log_step(step, history, best_agent)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"13\"></a>\n<h2 style='background:#FBE338; border:0; color:black'><center>Agent: Opponent Transition Matrix<center><h2>\n"},{"metadata":{},"cell_type":"markdown","source":"Copy from kernel [RPS: Opponent Transition Matrix](https://www.kaggle.com/group16/rps-opponent-transition-matrix)"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"%%writefile opponent_transition_matrix.py\n\nimport numpy as np\nimport pandas as pd\nimport random\n\nT = np.zeros((3, 3))\nP = np.zeros((3, 3))\n\n# a1 is the action of the opponent 1 step ago\n# a2 is the action of the opponent 2 steps ago\na1, a2 = None, None\n\ndef transition_agent(observation, configuration):\n    global T, P, a1, a2\n    if observation.step > 1:\n        a1 = observation.lastOpponentAction\n        T[a2, a1] += 1\n        P = np.divide(T, np.maximum(1, T.sum(axis=1)).reshape(-1, 1))\n        a2 = a1\n        if np.sum(P[a1, :]) == 1:\n            return int((np.random.choice(\n                [0, 1, 2],\n                p=P[a1, :]\n            ) + 1) % 3)\n        else:\n            return int(np.random.randint(3))\n    else:\n        if observation.step == 1:\n            a2 = observation.lastOpponentAction\n        return int(np.random.randint(3))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"14\"></a>\n<h2 style='background:#FBE338; border:0; color:black'><center>Agent: Decision Tree Classifier<center><h2>\n\n"},{"metadata":{},"cell_type":"markdown","source":"Copy from kernel [Decision Tree Classifier](https://www.kaggle.com/alexandersamarin/decision-tree-classifier?scriptVersionId=46415861)"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"%%writefile decision_tree_classifier.py\n\nimport numpy as np\nimport collections\nfrom sklearn.tree import DecisionTreeClassifier\n\ndef construct_local_features(rollouts):\n    features = np.array([[step % k for step in rollouts['steps']] for k in (2, 3, 5)])\n    features = np.append(features, rollouts['steps'])\n    features = np.append(features, rollouts['actions'])\n    features = np.append(features, rollouts['opp-actions'])\n    return features\n\ndef construct_global_features(rollouts):\n    features = []\n    for key in ['actions', 'opp-actions']:\n        for i in range(3):\n            actions_count = np.mean([r == i for r in rollouts[key]])\n            features.append(actions_count)\n    \n    return np.array(features)\n\ndef construct_features(short_stat_rollouts, long_stat_rollouts):\n    lf = construct_local_features(short_stat_rollouts)\n    gf = construct_global_features(long_stat_rollouts)\n    features = np.concatenate([lf, gf])\n    return features\n\ndef predict_opponent_move(train_data, test_sample):\n    classifier = DecisionTreeClassifier(random_state=42)\n    classifier.fit(train_data['x'], train_data['y'])\n    return classifier.predict(test_sample)\n\ndef update_rollouts_hist(rollouts_hist, last_move, opp_last_action):\n    rollouts_hist['steps'].append(last_move['step'])\n    rollouts_hist['actions'].append(last_move['action'])\n    rollouts_hist['opp-actions'].append(opp_last_action)\n    return rollouts_hist\n\ndef warmup_strategy(observation, configuration):\n    global rollouts_hist, last_move\n    action = int(np.random.randint(3))\n    if observation.step == 0:\n        last_move = {'step': 0, 'action': action}\n        rollouts_hist = {'steps': [], 'actions': [], 'opp-actions': []}\n    else:\n        rollouts_hist = update_rollouts_hist(rollouts_hist, last_move, observation.lastOpponentAction)\n        last_move = {'step': observation.step, 'action': action}\n    return int(action)\n\ndef init_training_data(rollouts_hist, k):\n    for i in range(len(rollouts_hist['steps']) - k + 1):\n        short_stat_rollouts = {key: rollouts_hist[key][i:i+k] for key in rollouts_hist}\n        long_stat_rollouts = {key: rollouts_hist[key][:i+k] for key in rollouts_hist}\n        features = construct_features(short_stat_rollouts, long_stat_rollouts)        \n        data['x'].append(features)\n    test_sample = data['x'][-1].reshape(1, -1)\n    data['x'] = data['x'][:-1]\n    data['y'] = rollouts_hist['opp-actions'][k:]\n    return data, test_sample\n\ndef agent(observation, configuration):\n    # hyperparameters\n    k = 5\n    min_samples = 25\n    global rollouts_hist, last_move, data, test_sample\n    if observation.step == 0:\n        data = {'x': [], 'y': []}\n    # if not enough data -> randomize\n    if observation.step <= min_samples + k:\n        return warmup_strategy(observation, configuration)\n    # update statistics\n    rollouts_hist = update_rollouts_hist(rollouts_hist, last_move, observation.lastOpponentAction)\n    # update training data\n    if len(data['x']) == 0:\n        data, test_sample = init_training_data(rollouts_hist, k)\n    else:        \n        short_stat_rollouts = {key: rollouts_hist[key][-k:] for key in rollouts_hist}\n        features = construct_features(short_stat_rollouts, rollouts_hist)\n        data['x'].append(test_sample[0])\n        data['y'] = rollouts_hist['opp-actions'][k:]\n        test_sample = features.reshape(1, -1)\n        \n    # predict opponents move and choose an action\n    next_opp_action_pred = predict_opponent_move(data, test_sample)\n    action = int((next_opp_action_pred + 1) % 3)\n    last_move = {'step': observation.step, 'action': action}\n    return action","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"15\"></a>\n<h2 style='background:#FBE338; border:0; color:black'><center>Agent: Statistical Prediction<center><h2>\n\n"},{"metadata":{},"cell_type":"markdown","source":"Copy from kernel [Rock Paper Scissors - Statistical Prediction](https://www.kaggle.com/jamesmcguigan/rock-paper-scissors-statistical-prediction)"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"%%writefile statistical_prediction.py\n\nimport random\nimport pydash\nfrom collections import Counter\n\n# Create a small amount of starting history\nhistory = {\n    \"guess\":      [0,1,2],\n    \"prediction\": [0,1,2],\n    \"expected\":   [0,1,2],\n    \"action\":     [0,1,2],\n    \"opponent\":   [0,1],\n}\ndef statistical_prediction_agent(observation, configuration):    \n    global history\n    actions         = list(range(configuration.signs))  # [0,1,2]\n    last_action     = history['action'][-1]\n    opponent_action = observation.lastOpponentAction if observation.step > 0 else 2\n    \n    history['opponent'].append(opponent_action)\n\n    # Make weighted random guess based on the complete move history, weighted towards relative moves based on our last action \n    move_frequency       = Counter(history['opponent'])\n    response_frequency   = Counter(zip(history['action'], history['opponent'])) \n    move_weights         = [ move_frequency.get(n,1) + response_frequency.get((last_action,n),1) for n in range(configuration.signs) ] \n    guess                = random.choices( population=actions, weights=move_weights, k=1 )[0]\n    \n    # Compare our guess to how our opponent actually played\n    guess_frequency      = Counter(zip(history['guess'], history['opponent']))\n    guess_weights        = [ guess_frequency.get((guess,n),1) for n in range(configuration.signs) ]\n    prediction           = random.choices( population=actions, weights=guess_weights, k=1 )[0]\n\n    # Repeat, but based on how many times our prediction was correct\n    prediction_frequency = Counter(zip(history['prediction'], history['opponent']))\n    prediction_weights   = [ prediction_frequency.get((prediction,n),1) for n in range(configuration.signs) ]\n    expected             = random.choices( population=actions, weights=prediction_weights, k=1 )[0]\n\n    # Play the +1 counter move\n    action = (expected + 1) % configuration.signs\n    \n    # Persist state\n    history['guess'].append(guess)\n    history['prediction'].append(prediction)\n    history['expected'].append(expected)\n    history['action'].append(action)\n\n    # Print debug information\n    print('opponent_action                = ', opponent_action)\n    print('move_weights,       guess      = ', move_weights, guess)\n    print('guess_weights,      prediction = ', guess_weights, prediction)\n    print('prediction_weights, expected   = ', prediction_weights, expected)\n    print('action                         = ', action)\n    print()\n    \n    return action","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"<a id=\"100\"></a>\n<h2 style='background:#FBE338; border:0; color:black'><center>Setup and validation<center><h2>"},{"metadata":{},"cell_type":"markdown","source":"We need to import the library for creating environments and simulating agent battles"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Upgrade kaggle_environments using pip before import\n!pip install -q -U kaggle_environments","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom kaggle_environments import make, evaluate","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Create a rock-paper-scissors environment (RPS), and set 1000 episodes for each simulation"},{"metadata":{"trusted":true},"cell_type":"code","source":"env = make(\"rps\", configuration={\"episodeSteps\": 1000}, debug=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Validation"},{"metadata":{"trusted":true},"cell_type":"code","source":"env.run([\"random_forest_random.py\", \"nash_equilibrium.py\"])\n\nenv.render(mode=\"ipython\", width=500, height=400)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# raise SystemExit(\"Stop right there!\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"<a id=\"102\"></a>\n<h2 style='background:#FBE338; border:0; color:black'><center>Marathon: Random Forest Random against all agents<center><h2>\n"},{"metadata":{},"cell_type":"markdown","source":"Setup battlefield"},{"metadata":{"trusted":true},"cell_type":"code","source":"list_names = [\n    \"rock\", \n    \"paper\", \n    \"scissors\",\n    \"hit_the_last_own_action\",  \n    \"copy_opponent\", \n    \"reactionary\", \n    \"counter_reactionary\", \n    \"statistical\", \n    \"nash_equilibrium\",\n    \"markov_agent\", \n    \"memory_patterns\", \n    \"multi_armed_bandit\",\n    \"opponent_transition_matrix\",\n    \"decision_tree_classifier\",\n    \"statistical_prediction\",\n    \"random_forest_random\",\n]\nlist_agents = [agent_name + \".py\" for agent_name in list_names]\n\nscores = np.zeros((len(list_names), 10), dtype=int)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Create environnment without debug"},{"metadata":{"trusted":true},"cell_type":"code","source":"env = make(\"rps\", configuration={\"episodeSteps\": 1000})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Run the simulation"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Simulation of battles. It can take some time...\")\n\nfor ind_agent_1 in range(10):\n    for ind_agent_2 in range(len(list_names)):\n        print(f\"LOG: Random Forest Random vs {list_names[ind_agent_2]}\", end=\"\\r\")\n        \n        current_score = evaluate(\n            \"rps\", \n            [\"random_forest_random.py\", list_agents[ind_agent_2]], \n            configuration={\"episodeSteps\": 1000}\n        )\n        \n        scores[ind_agent_2, ind_agent_1, ] = current_score[0][0]\n    \n    print(\"Round: \", ind_agent_1, \" of 10 \")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"103\"></a>\n<h2 style='background:#FBE338; border:0; color:black'><center>Results<center><h2>"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"df_scores = pd.DataFrame(\n    scores, \n    index=list_names, \n    columns=range(10),\n)\n\n\nplt.figure(figsize=(15, 10))\nsns.heatmap(\n    df_scores, annot=True, cbar=False, cmap='coolwarm', linewidths=1, linecolor='black', fmt=\"d\"\n)\nplt.suptitle('Random Forest Random vs all agents', fontsize=20)\nplt.title('Final Reward Score', fontsize=15)\nplt.xticks(rotation=90, fontsize=15)\nplt.yticks(fontsize=15);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"df_review=pd.DataFrame()\ndf_review['Won'] = df_scores.select_dtypes(include='int').gt(0).sum(axis=1)\ndf_review['Tie'] = df_scores.select_dtypes(include='int').eq(0).sum(axis=1)\ndf_review['Lost'] = df_scores.select_dtypes(include='int').lt(0).sum(axis=1)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(5, 10))\nsns.heatmap(\n    df_review, annot=True, cbar=False, cmap='coolwarm', linewidths=1, linecolor='black', fmt=\"d\"\n)\nplt.suptitle('Random Forest Random vs all agents', fontsize=20)\nplt.title('Total games Won-Tie-Lost', fontsize=15)\nplt.xticks(rotation=90, fontsize=15)\nplt.yticks(fontsize=15);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"104\"></a>\n<h2 style='background:#FBE338; border:0; color:black'><center>Review<center><h2>"},{"metadata":{},"cell_type":"markdown","source":"* `Random Forest Random` can identify the patterns of all simple agents in 5 actions or less.\n* `Statistical` is an easy opponent for `Random Forest Random` and performs better than `Markov Agent` almost every time.\n* Chances to win over `Memory Patterns` or `Multi Armed Bandit` are near zero an only by luck can beat them sometimes.\n* Luck is crucial for the outcome over `Opponent Transition Matrix`, `Decision Tree Classifier` and `Statistical Prediction` as the results can vary a lot over matches.\n* Final conclusion is that `Random Forest Classifiers` can be used to predict opponents actions on `Rock-Paper-Scissors` but advanced `defensive` mechanisms are required when the pattern is identified by the opponent.\n\n**Disclaimer: The above review is done on multiple runs of this notebook and the published results might not represent them exactly.**"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}