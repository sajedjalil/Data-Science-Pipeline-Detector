{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Rock Paper Scissors - LSTM\n\nWork In Progress - Looking for feedback on how to improve this design to actually be competative\n\n---\n\nThis implements an LSTM for Rock Paper Scissors in Pytorch.\n\nInput is encoded multiple ways:\n```\nx = torch.cat([\n    noise.flatten(),    # tensor (3,)  random noise for probablistic moves\n    step.flatten(),     # tensor (7,)  round(log(step)) one-hot encoded - to predict warmup periods    \n    stats.flatten(),    # tensor (2,3) with move frequency percentages   \n    history.flatten(),  # tensor (2,3,window=10) one-hot encoded timeseries history\n])\n```\n\nThen fed through the following network:\n```\nRpsLSTM(\n  (lstm): LSTM(76, 128, num_layers=3, batch_first=True, dropout=0.25)\n  (dense_1): Linear(in_features=204, out_features=128, bias=True)\n  (dense_2): Linear(in_features=128, out_features=128, bias=True)\n  (out_probs): Linear(in_features=128, out_features=3, bias=True)\n  (out_hash): Linear(in_features=128, out_features=128, bias=True)\n  (activation): Softsign()\n  (softmax): Softmax(dim=2)\n)\n```\n\nThere are two loss functions:\n0. loss_probs() - softmax probability vs EV score of opponent move prediction\n1. loss_hash() - Categorical Cross Entropy loss for agent identity prediction using Locality-sensitive hashing\n\n\nThis model can successfully defeat the simplest of agents such as:\n- Rock \n- Paper \n- Scissors \n- Sequential\n\nIt has problems however with more complex agents, where struggles to get beyond a draw\n- anti_rotn\n- multi_stage_decision_tree\n- iocaine_powder \n- greenberg"},{"metadata":{},"cell_type":"markdown","source":"# NNBase Class\n\nThis is a generic baseclass to handle saving/loading the model from file and other utility functions"},{"metadata":{"trusted":true},"cell_type":"code","source":"# %%writefile NNBase.py\n# Source: https://www.kaggle.com/jamesmcguigan/rock-paper-scissors-lstm/\n# Source: https://github.com/JamesMcGuigan/ai-games/blob/master/games/rock-paper-scissors/neural_network/NNBase.py\n\nfrom __future__ import annotations\n\nimport os\nimport re\nfrom abc import ABCMeta\nfrom typing import TypeVar\n\nimport humanize\nimport torch\nimport torch.nn as nn\n\n\n\n# noinspection PyTypeChecker\nT = TypeVar('T', bound='GameOfLifeBase')\nclass NNBase(nn.Module, metaclass=ABCMeta):\n    \"\"\"\n    Base class for GameOfLife based NNs\n    Handles: save/autoload, freeze/unfreeze, casting between data formats, and training loop functions\n    \"\"\"\n    def __init__(self):\n        super().__init__()\n        self.loaded  = False  # can't call sell.load() in constructor, as weights/layers have not been defined yet\n        self.device  = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n\n\n    def __call__(self, *args, **kwargs) -> torch.Tensor:\n        if not self.loaded: self.load()  # autoload on first function call\n        return super().__call__(*args, **kwargs)\n\n\n\n    ### Initialization\n\n    def weights_init(self, layer):\n        ### Default initialization seems to work best, at least for Z shaped ReLU1 - see GameOfLifeHardcodedReLU1_21.py\n        if isinstance(layer, (nn.Conv2d, nn.ConvTranspose2d)):\n            ### kaiming_normal_ corrects for mean and std of the relu function\n            ### xavier_normal_ works better for ReLU6 and Z shaped activations\n            if isinstance(self.activation, (nn.ReLU, nn.LeakyReLU, nn.PReLU)):\n                nn.init.kaiming_normal_(layer.weight)\n                # nn.init.xavier_normal_(layer.weight)\n                if layer.bias is not None:\n                    # small positive bias so that all nodes are initialized\n                    nn.init.constant_(layer.bias, 0.1)\n        else:\n            # Use default initialization\n            pass\n\n    ### Freeze / Unfreeze\n\n    def freeze(self: T) -> T:\n        if not self.loaded: self.load()\n        for name, parameter in self.named_parameters():\n            parameter.requires_grad = False\n        return self\n\n    def unfreeze(self: T) -> T:\n        if not self.loaded: self.load()\n        for name, parameter in self.named_parameters():\n            parameter.requires_grad = True\n        return self\n\n\n\n    ### Load / Save Functionality\n\n    @property\n    def filename(self) -> str:\n        if os.environ.get('KAGGLE_KERNEL_RUN_TYPE'):\n            return f'./{self.__class__.__name__}.pth'\n        else:\n            filename = os.path.join( os.path.dirname(__file__), 'models', f'{self.__class__.__name__}.pth' )\n            os.makedirs(os.path.dirname(filename), exist_ok=True)\n            return filename\n\n    # DOCS: https://pytorch.org/tutorials/beginner/saving_loading_models.html\n    def save(self: T, verbose=True) -> T:\n        os.makedirs(os.path.dirname(self.filename), exist_ok=True)\n        torch.save(self.state_dict(), self.filename)\n        if verbose: print(f'{self.__class__.__name__}.savefile(): {self.filename} = {humanize.naturalsize(os.path.getsize(self.filename))}')\n        return self\n\n\n    def load(self: T, load_weights=True, verbose=True) -> T:\n        if load_weights and os.path.exists(self.filename):\n            try:\n                self.load_state_dict(torch.load(self.filename))\n                if verbose: print(f'{self.__class__.__name__}.load(): {self.filename} = {humanize.naturalsize(os.path.getsize(self.filename))}')\n            except Exception as exception:\n                # Ignore errors caused by model size mismatch\n                if verbose: print(f'{self.__class__.__name__}.load(): model has changed dimensions, reinitializing weights\\n')\n                self.apply(self.weights_init)\n        else:\n            if verbose:\n                if load_weights: print(f'{self.__class__.__name__}.load(): model file not found, reinitializing weights\\n')\n                # else:          print(f'{self.__class__.__name__}.load(): reinitializing weights\\n')\n            self.apply(self.weights_init)\n\n        self.loaded = True    # prevent any infinite if self.loaded loops\n        self.to(self.device)  # ensure all weights, either loaded or untrained are moved to GPU\n        # self.eval()           # default to production mode - disable dropout\n        # self.freeze()         # default to production mode - disable training\n        return self\n\n\n\n    ### Debugging\n\n    def print_params(self):\n        print(self.__class__.__name__)\n        print(self)\n        for name, parameter in sorted(self.named_parameters(), key=lambda pair: pair[0].split('.')[0] ):\n            print(name)\n            print(re.sub(r'\\n( *\\n)+', '\\n', str(parameter.data.cpu().numpy())))  # remove extranious newlines\n            print()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# %run NNBase.py","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# LSTM Agent\n\nThis is the main neural network model class implemented in pytorch.\n\nInput is encoded multiple ways:\n```\nx = torch.cat([\n    noise.flatten(),    # tensor (3,)  random noise for probablistic moves\n    step.flatten(),     # tensor (7,)  round(log(step)) one-hot encoded - to predict warmup periods    \n    stats.flatten(),    # tensor (2,3) with move frequency percentages   \n    history.flatten(),  # tensor (2,3,window=10) one-hot encoded timeseries history\n])\n```\n\nThen fed through the following network:\n```\nRpsLSTM(\n  (lstm): LSTM(76, 128, num_layers=3, batch_first=True, dropout=0.25)\n  (dense_1): Linear(in_features=204, out_features=128, bias=True)\n  (dense_2): Linear(in_features=128, out_features=128, bias=True)\n  (out_probs): Linear(in_features=128, out_features=3, bias=True)\n  (out_hash): Linear(in_features=128, out_features=128, bias=True)\n  (activation): Softsign()\n  (softmax): Softmax(dim=2)\n)\n```\n\nIt defines two custom loss functions: \n- `loss_probs()` is for correctly predicting the next opponent move based on 1/0.5/0 EV scores\n- `loss_hash()` is for predicting who our current opponent is\n\n[Locality-sensitive hashing](https://en.wikipedia.org/wiki/Locality-sensitive_hashing) is used within `loss_hash()`. \nThis is done by creating a xxhash(seed=0) of the agents str label, \nthen using modulo to place it into a fixed-size one-hot-encoded bucket.\nWhist this information is not directly used for making moves,\nthe idea is to train the model to have an internal representation\nof who our opponent is, such that it can better select an appropriate strategy\n\n[Softsign](https://sefiks.com/2017/11/10/softsign-as-a-neural-networks-activation-function/)\n`f(x) = x / (1 + |x|)` is used as the activation function. It has a similar shape to `tanh()`\nbut has two extra \"bumps\" in the curve which a neural network can make use of.\nIts lesser known but has been cited in [recent papers](https://paperswithcode.com/method/softsign-activation).\n\nI have not done a through exploration as to the best activation function to use here,\nhowever it did fix a weird bug caused by [PReLU](https://pytorch.org/docs/stable/generated/torch.nn.PReLU.html)\nthat resulted in the LSTM weights converging to NaN after a thousand epochs.\n\nI also concatinate the original input to the LSTM output before passing it the dense layers.\nI am unsure if I should be doing this, or rely purely on this information being saved into the LSTM embedding."},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip3 install -q xxhash ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# %%writefile RpsLSTM.py\n# Source: https://www.kaggle.com/jamesmcguigan/rock-paper-scissors-lstm/\n# Source: https://github.com/JamesMcGuigan/ai-games/blob/master/games/rock-paper-scissors/neural_network/RpsLSTM.py\n\nimport math\n\nimport torch\nimport torch.nn as nn\nimport xxhash\n\n# from neural_network.NNBase import NNBase\n# from neural_network.NNBase import NNBase\n\n\n# DOCS: https://pytorch.org/tutorials/beginner/nlp/sequence_models_tutorial.html\n# DOCS: https://blog.floydhub.com/long-short-term-memory-from-zero-to-hero-with-pytorch/\n# DOCS: https://github.com/MagaliDrumare/How-to-learn-PyTorch-NN-CNN-RNN-LSTM/blob/master/10-LSTM.ipynb\nclass RpsLSTM(NNBase):\n\n    def __init__(self, hidden_size=128, hash_size=128, num_layers=3, dropout=0.25, window=10):\n        \"\"\"\n        :param hidden_size: size of LSTM embedding\n        :param hash_size:   size of hash for guessing opponent\n        :param num_layers:  number of LSTM layers\n        :param dropout:     dropout parameter for LSTM\n        :param window:      maximum history length passed in as input data\n        \"\"\"\n        super().__init__()\n        self.device      = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n        self.hidden_size = hidden_size\n        self.num_layers  = num_layers\n        self.batch_size  = 1\n        self.dropout     = dropout\n        self.window      = window\n        self.hash_size   = hash_size\n        self.input_size  = self.cast_inputs(0,0).shape[-1]\n\n        self.lstm  = nn.LSTM(\n            input_size  = self.input_size,\n            hidden_size = self.hidden_size,\n            num_layers  = self.num_layers,\n            dropout     = self.dropout,\n            batch_first = True,\n        )\n        self.dense_1   = nn.Linear(self.input_size + hidden_size, hidden_size)\n        self.dense_2   = nn.Linear(hidden_size, hidden_size)\n        self.out_probs = nn.Linear(hidden_size, 3)\n        self.out_hash  = nn.Linear(hidden_size, self.hash_size)\n\n        self.activation = nn.Softsign()  # BUGFIX: PReLU was potentially causing NaNs in model weights\n        self.softmax    = nn.Softmax(dim=2)\n        self.reset()  # call before self.cast_inputs()\n        self.to(self.device)\n\n\n\n    ### Lifecycle\n\n    def reset(self):\n        self.history = [ [], [] ]  # action, opponent\n        self.stats   = torch.zeros((2,3),    dtype=torch.float).to(self.device)\n        self.hidden  = (\n            torch.zeros(self.num_layers, self.batch_size, self.hidden_size),\n            torch.zeros(self.num_layers, self.batch_size, self.hidden_size)\n        )\n\n\n\n    ### Training\n\n    @staticmethod\n    def reward(action: int, opponent: int) -> float:\n        if (action - 1) % 3 == opponent % 3: return  1.0  # win\n        if (action - 0) % 3 == opponent % 3: return  0.5  # draw\n        if (action + 1) % 3 == opponent % 3: return  0.0  # loss\n        return 0.0\n\n\n    def loss_probs(self, probs: torch.Tensor, opponent: int) -> torch.Tensor:\n        \"\"\"\n        Loss based on softmax probability vs EV score of opponent move prediction\n        \"\"\"\n        ev = torch.zeros((3,), dtype=torch.float).to(self.device)\n        ev[(opponent + 0) % 3] = 1.0   # expect rock, play paper + opponent rock     = win\n        ev[(opponent + 1) % 3] = 0.5   # expect rock, play paper + opponent paper    = draw\n        ev[(opponent + 2) % 3] = 0.0   # expect rock, play paper + opponent scissors = loss\n        losses = probs * (1-ev)\n        loss   = torch.sum( losses )\n        # loss   = -torch.sum(torch.log(1-losses))  # cross entropy loss\n        return loss\n\n\n    def loss_hash(self, hash_id: torch.Tensor, agent_name: str) -> torch.Tensor:\n        \"\"\"\n        Categorical Cross Entropy loss for agent identity prediction using Locality-sensitive hashing\n        \"\"\"\n        hash_id    = hash_id.flatten()\n        hash_pred  = torch.argmax(hash_id)\n        agent_hash = xxhash.xxh32(agent_name, seed=0).intdigest() % self.hash_size\n        agent_hot  = self.one_hot_encode(agent_hash, size=self.hash_size).flatten()\n        loss       = -torch.sum( agent_hot * torch.log(hash_id) - (1-agent_hot) * torch.log(1-hash_id) )\n        loss[ torch.isnan(loss) ] = 0.0  # BUGFIX: prevent log(0) = NaN\n        return loss\n\n\n\n    ### Casting\n\n    def one_hot_encode(self, number: int = None, size: int = 3) -> torch.Tensor:\n        \"\"\" One hot encoding of action and opponent action \"\"\"\n        x = torch.zeros((size,), dtype=torch.float).to(self.device)\n        if number is not None:\n            x[ int(number) % size ] = 1.0\n        return x\n\n\n    def encode_actions(self, action: int, opponent: int) -> torch.Tensor:\n        return torch.stack([\n            self.one_hot_encode(action),\n            self.one_hot_encode(opponent),\n        ])\n\n\n    def encode_history(self) -> torch.Tensor:\n        \"\"\"\n        self.history as a one hot encoded tensor\n        history is created via .insert() thus latest move will always be in position 0\n        self.window can be used to restrict the maximum size of history data passed into the model\n        \"\"\"\n        x = torch.zeros((2, self.window, 3), dtype=torch.float).to(self.device)\n        for player in [0,1]:\n            window = min( self.window, len(self.history[player]) )\n            for step in range(window):\n                action = self.history[player][step]\n                if action is None: continue\n                x[ player, step, action % 3 ] = 1.0\n        return x\n\n\n    def encode_stats(self):\n        \"\"\" Normalized percentage frequency from self.stats \"\"\"\n        step  = torch.sum(self.stats[0])\n        stats = ( self.stats / torch.sum(self.stats[0])\n                  if step.item() > 0\n                  else self.stats )\n        return stats\n\n\n    def encode_step(self):\n        \"\"\"\n        Encode the step (current turn number) as one hot encoded version of the logarithm\n        This is mostly for detecting random warmup periods\n        { round(math.log(n)): n for n in range(1,1001)  } = { 0: 1, 1: 4, 2: 12, 3: 33, 4: 90, 5: 244, 6: 665, 7: 1000}\n        \"\"\"\n        step     = torch.sum(self.stats[0]).reshape(1)\n        log_step = torch.log(step+1).round().int().item()  # log(0) == NaN\n        hot_step = self.one_hot_encode(log_step, size=round(math.log(1000)))\n        return hot_step\n\n\n    @staticmethod\n    def cast_action(probs: torch.Tensor) -> int:\n        expected = torch.argmax(probs, dim=2).detach().item()\n        action   = int(expected + 1) % 3\n        return action\n\n\n    def cast_inputs(self, action: int, opponent: int) -> torch.Tensor:\n        \"\"\"\n        Generate the input tensors for the LSTM\n        Assumes that self.update_state(action, opponent) has been called beforehand\n        action + opponent are now encoded as part of the history\n        \"\"\"\n        if not hasattr(self, 'stats'): self.reset()\n\n        noise   = torch.rand((3,)).to(self.device)\n        step    = self.encode_step()\n        stats   = self.encode_stats()\n        history = self.encode_history()\n\n        x = torch.cat([\n            noise.flatten(),    # tensor (3,)            random noise for probablistic moves\n            step.flatten(),     # tensor (7,)            round(log(step)) one-hot encoded - to predict warmup periods\n            stats.flatten(),    # tensor (2,3)           with move frequency percentages\n            history.flatten(),  # tensor (2,3,window=10) one-hot encoded timeseries history\n        ])\n        x = torch.reshape(x, (1,1,-1))    # (seq_len, batch, input_size)\n        return x\n\n\n\n    ### Play\n\n    def update_state(self, action: int, opponent: int):\n        \"\"\"\n        self.stats records total count for each action\n            which will later be normalized as percentage frequency\n        self.history records move history\n            [0] index always being the most recent move\n        \"\"\"\n        if action   is not None: self.stats[0][action]   += 1.0\n        if opponent is not None: self.stats[1][opponent] += 1.0\n        if action   is not None: self.history[0].insert(0, action)\n        if opponent is not None: self.history[1].insert(0, opponent)\n\n\n    def forward(self, action: int, opponent: int):\n        self.update_state(action, opponent)\n        inputs    = self.cast_inputs(action, opponent)\n        x, hidden = self.lstm(inputs)\n        x         = torch.cat([ x, inputs ], dim=2)\n        x         = self.activation( self.dense_1(x)   )\n        x         = self.activation( self.dense_2(x)   )\n        probs     = self.softmax(    self.out_probs(x) )\n        hash_id   = self.softmax(    self.out_hash(x)  )\n        action    = self.cast_action(probs)\n\n        # BUGFIX: occasionally LSTM would return NaNs after 850+ epochs\n        #         this possibly caused by PReLU, now replaced with Softsign\n        if any([ torch.any(torch.isnan(layer)) for layer in [ x, *hidden] ]):\n            raise ValueError(f'{self.__class__.__name__}.forward() - LSTM returned nan')\n\n        self.hidden = hidden\n        return action, probs, hash_id\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Opponents"},{"metadata":{},"cell_type":"markdown","source":"We start with a series of really simple agents: \n- Rock\n- Paper\n- Scissors\n- Sequential\n\nPlus the whitebelt agents from the [RPS Dojo](https://www.kaggle.com/chankhavu/rps-dojo)\n- Reactionary\n- Counter Reactionary\n- Mirror\n- Mirror Shift\n- Statistical\n\nThese act as a baseline showing that the LSTM neural network is at least capable of learning simple logic.\n\nNext up are a variety of more complex agents, implementing a range of different strategies\n- [Anti-Rotn](https://www.kaggle.com/jamesmcguigan/rock-paper-scissors-anti-rotn)\n- [Multi Stage Decision Tree](https://www.kaggle.com/jamesmcguigan/rock-paper-scissors-multi-stage-decision-tree)\n- [Iocaine Powder](https://www.kaggle.com/jamesmcguigan/rps-roshambo-comp-iocaine-powder)\n- [Greenberg](https://www.kaggle.com/jamesmcguigan/rock-paper-scissors-greenberg)\n- [Statistical Prediction](https://www.kaggle.com/jamesmcguigan/rock-paper-scissors-statistical-prediction)\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"!cat ../input/rock-paper-scissors-anti-rotn/anti_rotn.py                  | perl -p -e 's/warmup=\\d+/warmup=1/' | tee anti_rotn.py > /dev/null\n!cat ../input/rock-paper-scissors-multi-stage-decision-tree/submission.py | perl -p -e 's/warmup_period=\\d+/warmup_period=1/; s/random_freq=[\\d.]+/random_freq=0/' | tee decision_tree.py > /dev/null\n!cat ../input/rps-roshambo-comp-iocaine-powder/submission.py              | tee iocaine.py                > /dev/null\n!cat ../input/rock-paper-scissors-greenberg/greenberg.py                  | tee greenberg.py              > /dev/null\n!cat ../input/rock-paper-scissors-statistical-prediction/submission.py    | tee statistical_prediction.py > /dev/null","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import random\nfrom kaggle_environments.envs.rps.utils import get_score\n\ndef rock_agent(observation, configuration):\n    return 0\n\ndef paper_agent(observation, configuration):\n    return 1\n\ndef scissors_agent(observation, configuration):\n    return 2\n\ndef sequential_agent(observation, configuration):\n    return observation.step % configuration.signs\n\n\n# White Belt Agents from https://www.kaggle.com/chankhavu/rps-dojo\nlast_react_action = None\ndef reactionary(observation, configuration):\n    global last_react_action\n    if observation.step == 0:\n        last_react_action = random.randrange(0, configuration.signs)\n    elif get_score(last_react_action, observation.lastOpponentAction) <= 1:\n        last_react_action = (observation.lastOpponentAction + 1) % configuration.signs\n    return last_react_action\n\n\nlast_counter_action = None\ndef counter_reactionary(observation, configuration):\n    global last_counter_action\n    if observation.step == 0:\n        last_counter_action = random.randrange(0, configuration.signs)\n    elif get_score(last_counter_action, observation.lastOpponentAction) == 1:\n        last_counter_action = (last_counter_action + 2) % configuration.signs\n    else:\n        last_counter_action = (observation.lastOpponentAction + 1) % configuration.signs\n    return last_counter_action\n\n\ndef mirror_opponent_agent(observation, configuration):\n    if observation.step > 0:\n        return observation.lastOpponentAction\n    else:\n        return 0\n    \n    \ndef mirror_shift_opponent_agent_1(observation, configuration):\n    if observation.step > 0:\n        return (observation.lastOpponentAction + 1) % 3\n    else:\n        return 0\n\n    \ndef mirror_shift_opponent_agent_2(observation, configuration):\n    if observation.step > 0:\n        return (observation.lastOpponentAction + 2) % 3\n    else:\n        return 0\n\n    \naction_histogram = {}\ndef statistical(observation, configuration):\n    global action_histogram\n    if observation.step == 0:\n        action_histogram = {}\n        return\n    action = observation.lastOpponentAction\n    if action not in action_histogram:\n        action_histogram[action] = 0\n    action_histogram[action] += 1\n    mode_action = None\n    mode_action_count = None\n    for k, v in action_histogram.items():\n        if mode_action_count is None or v > mode_action_count:\n            mode_action = k\n            mode_action_count = v\n            continue\n\n    return (mode_action + 1) % configuration.signs","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# RPS Trainer\n\nThis is a training loop abstracted to play against arbitray kaggle_environment agents. \n\nThe RL interface for kaggle_environments is:\n```\nenv         = make(\"rps\", { \"episodeSteps\": steps }, debug=False)\ntrainer     = env.train(random.sample([None, agent], 2))  # random player order\nobservation = trainer.reset()\ndone        = False\nwhile not done:\n    action = model( observation.lastOpponentAction )\n    observation, reward, done, info = trainer.step(action)\n```\n\nYou can read more about this in the [docs](https://github.com/Kaggle/kaggle-environments)\n\nIn theory this training loop code could be easily repurposed for other kaggle competitions.\n\n---\n\nThis training loop takes a dictionary of opponent agents, and simulates a 100 step = 50 round match against each one.\n`loss.backward()` is only called at the end of each match as its the only way I have figured out\nhow to solve the `RuntimeError: Trying to backward through the graph a second time` exception.\nIf anybody knows a better way of doing this, please let me know.\n\nI did have a bit of code to probablistically select agents based on their accuracy percentage.\nThe idea being to skip the rock/paper/scissors agents once they had reached 100% accuracy,\nand focus most of the training time on the agents that needed the most training.\nHowever when retraining from scratch it was causing weird effects in my output statitsics \n(unsure if this was just a bug in my logging code). Its been disabled for now.\n\n`RMSprop` seems to work better than other optimizers such as `Adam` or `Adadelta`\nfor this reinforcement learning task. I don't have any statistics on this,\nso can't say if my observations where simply based on coincidence. \n\n`CosineAnnealingWarmRestarts` was added in because several of the [DeepMind](https://deepmind.com/)\npapers mention that they use a cosine based scheduling system for their models.\nI am not 100% sure this is the same scheduler, or what the correct settings should be.\nI am unsure if this is helping or hurting my model, and if I should leave it in or not.\n\nFor the sake of making it easier to read the directional trend of the logfile numbers,\nI have implemented a very basic running average by taking the mean of the \ncurrent and next values in the sequence. This might not be the technically correct\nway of doing it (its more like summing an infinite series), but gives  \napproximately correct numbers and is a simple one-liner to smooth the curves \nand make it easier to observe the directional trend of loss and accuracy.\n\nLearning rate is currently set to `1e-4`. One observation from earlier in the development cycle \nwas that if the learning rate was set too high, such as `1e-1`, then the model would fail to train\neven against the simplest Rock/Paper/Scissors agents. I am unsure if I have set it too low,\nor how exactly this interacts with `CosineAnnealingWarmRestarts`. \nGenerally seems safer to be too small than too large. \n\n\nA few technical points to remember:\n- `model.train()` and `model.eval()` can be used to switch between training and production modes. \n    - Dropout is disabled in production, and this may also have an effect on batch normalization layers.\n- `model.load()` and `model.save()` are custom methods of my `NNBase` class\n- `except KeyboardInterrupt: pass` ensures the model gets saved on Ctrl-C exit\n- `if __name__ == '__main__':` prevents the training loop from running if we import `rps_trainer()` from a seperate file\n\n---\n\nLosses and accuracy are displayed based on running averages\n```\n   200 | losses = 0.419607 0.569539 | 100 r  99 p  98 s  99 seq  65 rotn  32 tree  35 iocaine  38 greenberg  47 stats\n   210 | losses = 0.420502 0.617317 | 100 r  99 p  98 s  99 seq  70 rotn  30 tree  46 iocaine  40 greenberg  54 stats\n   220 | losses = 0.411737 0.618371 | 100 r  99 p  98 s  99 seq  54 rotn  32 tree  45 iocaine  40 greenberg  51 stats\n   230 | losses = 0.448281 0.667498 | 100 r  99 p  98 s  99 seq  52 rotn  27 tree  45 iocaine  42 greenberg  51 stats\n```\n\nThe first number is the training epoch, which is a 100 step match against each agent\n\nThere are two loss functions:\n0. loss_probs() - softmax probability vs EV score of opponent move prediction\n1. loss_hash() - Categorical Cross Entropy loss for agent identity prediction using Locality-sensitive hashing\n\nThe last set of numbers are accuracy percentage scores in actual gameplay. \n- 100 = 100% accuracy winning on every round\n- 50  = 50% is a statistical draw, either via draw or win/loss every other round"},{"metadata":{"trusted":true},"cell_type":"code","source":"#!/usr/bin/env python3\n# %%writefile rps_trainer.py\n# Source: https://www.kaggle.com/jamesmcguigan/rock-paper-scissors-lstm/\n# Source: https://github.com/JamesMcGuigan/ai-games/blob/master/games/rock-paper-scissors/neural_network/rps_trainer.py\n\nimport random\nimport re\nimport sys\nimport time\nimport gc\nimport textwrap\nfrom typing import Dict\n\n\nimport torch\nfrom kaggle_environments import make\nfrom torch.autograd import Variable\nfrom torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n\n# from neural_network.RpsLSTM import RpsLSTM\n# from roshambo_competition.anti_rotn import anti_rotn\n# from roshambo_competition.greenberg import greenberg_agent\n# from roshambo_competition.iocaine_powder import iocaine_agent\n# from simple.paper import paper_agent\n# from simple.rock import rock_agent\n# from simple.scissors import scissors_agent\n# from simple.sequential import sequential_agent\n# from statistical.statistical_prediction import statistical_prediction_agent\n# from tree.multi_stage_decision_tree import decision_tree_agent\n\n\n        \ndef rps_trainer(model, agents: Dict, steps=100, epochs=1000, lr=1e-3, log_freq=10, timeout=0):\n    time_start = time.perf_counter()\n    wrapper    = textwrap.TextWrapper(width=78, subsequent_indent=' '*4, initial_indent=' '*4)\n    try:\n        env   = make(\"rps\", { \"episodeSteps\": steps }, debug=False)\n        optimizer = torch.optim.RMSprop(model.parameters(), lr=lr)\n        scheduler = None\n        scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=100)  # TODO: review choice of scheduler\n\n        accuracies     = { agent_name: 0.0 for agent_name in agents.keys()}\n        running_losses = torch.zeros((1,)).to(model.device)\n        for epoch in range(epochs):\n            \n            ### skip high-accuracy agents more often, but train at least 10% of the time\n            ### this seems to cause more problems than it solves\n            # selected_agents = {\n            #     agent_name: agent\n            #     for (agent_name, agent) in agents.items()\n            #     if random.random() + 0.1 >= accuracies[agent_name]\n            # }\n            selected_agents = agents\n            if len(selected_agents) == 0: continue\n\n            scores = Variable(torch.zeros((   len(selected_agents),), requires_grad=True)).to(model.device)\n            losses = Variable(torch.zeros((2, len(selected_agents),), requires_grad=True)).to(model.device)\n\n            for agent_index, (agent_name, agent) in enumerate(selected_agents.items()):\n                trainer     = env.train(random.sample([None, agent], 2))  # random player order\n                observation = trainer.reset()\n\n                model.reset()\n                optimizer.zero_grad()\n                gc.collect()\n\n                action     = None\n                opponent   = None\n                for step in range(1,sys.maxsize):\n                    action, probs, hash_id = model.forward(action=action, opponent=opponent)\n\n                    observation, reward, done, info = trainer.step(action)\n                    opponent = observation.lastOpponentAction\n\n                    losses[0][agent_index] += model.loss_probs(probs, opponent)\n                    losses[1][agent_index] += model.loss_hash(hash_id, agent_name)\n                    scores[agent_index]    += (reward + 1.0) / 2.0\n                    if done: break\n\n                losses[0][agent_index] /= step  # NOTE: steps = 2 * step\n                losses[1][agent_index] /= step\n                scores[agent_index]    /= step\n                accuracies[agent_name]  = ( (accuracies[agent_name] + scores[agent_index].item())\n                                            / (2 if sum(accuracies.values()) else 1) )\n\n            # print(env.render(mode='ansi'))\n            running_losses = ( (running_losses + torch.mean(losses, dim=1))\n                               / (2 if torch.sum(running_losses) else 1) )\n            loss = torch.mean(losses)\n            loss.backward()\n            optimizer.step()\n            if scheduler is not None: scheduler.step(loss)\n\n            if epoch % log_freq == 0:\n                accuracy_log = \" \".join([\n                    ('\\n'+' '*6 if n % 8 == 0 else '') + \n                    f'{round(value * 100):3d} {name},'\n                    for n, (name, value) in enumerate(accuracies.items())\n                ])\n                message = f'{epoch:4d} | losses = {running_losses[0].item():.6f} {running_losses[1].item():.6f} | {accuracy_log}' \n                print(message)\n                if torch.mean(scores).item() >= (1 - 2/steps): break  # allowed first 2 moves wrong\n\n            if timeout and time.perf_counter() > time_start + timeout:  break\n        \n    except KeyboardInterrupt: pass\n    except Exception as exception: \n        print('Exception', exception)\n        pass","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Training"},{"metadata":{"trusted":true},"cell_type":"code","source":"!cp ../input/rock-paper-scissors-lstm/RpsLSTM.pth ./","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n# Source: https://github.com/JamesMcGuigan/ai-games/blob/master/games/rock-paper-scissors/neural_network/rps_trainer.py\n\nif __name__ == '__main__':\n    agents = {\n        'r':          rock_agent,\n        'p':          paper_agent,\n        's':          scissors_agent,\n        'seq':        sequential_agent,\n        'react':      reactionary,\n        'react+1':    counter_reactionary,\n        'mirror':     mirror_opponent_agent,\n        'mirror+1':   mirror_shift_opponent_agent_1,\n        'mirror+2':   mirror_shift_opponent_agent_2,        \n        'stat':       statistical,\n        'stat_pred':  \"statistical_prediction.py\",\n        'rotn':       \"anti_rotn.py\",\n        'tree':       \"decision_tree.py\",\n        'iocaine':    \"iocaine.py\",\n        'greenberg':  \"greenberg.py\",\n    }\n\n    model = RpsLSTM(hidden_size=128, num_layers=3, dropout=0.25).train()\n    print(model)\n    model.load()\n    rps_trainer(model, agents, steps=100, lr=1e-4, epochs=200, timeout=1*60*60)\n    model.save()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Questions\n\nThis model can successfully defeat the simplest of agents such as:\n- Rock \n- Paper \n- Scissors \n- Sequential\n\nIt has problems however with more complex agents, where struggles to get beyond a draw\n- anti_rotn\n- multi_stage_decision_tree\n- iocaine_powder \n- greenberg\n\nI am unsure exactly what I am doing wrong here.\n- Is Rock Paper Scissors a suitable usecase for an LSTM network?\n- Am I training for long enough. The model seemed to plateau at a draw for the advanced agents\n- Are these more advanced agents too complex for a neural network to reverse engineer?\n    - The simple agents are able to train with `hidden_size=16`\n    - Do I need a much larger embedding size?\n- Is my model too small or too large?\n    - Are 3 LSTM layers better than 1\n    - Should I have a pyramid of 3 dense layers rather than a square of 2\n    - The Largest possible model that will fit in 100Mb is hidden_size=1024 with pyramid shaped dense layers, but this is very slow to train\n- I concatenate the original input to the LSTM output before passing it the dense layers\n    - Unsure if I should rely on the LSTM embedding to fully encode everything that needs to be remembered\n- Am I missing any obvious layers such as batch normaliztion?\n- Is there any way to train an LSTM in batch mode with reinforcement learning?\n\n\nThank you for any advice or feedback on how to improve this work"},{"metadata":{},"cell_type":"markdown","source":"# Further Reading\n\nThis notebook is part of a series exploring Rock Paper Scissors:\n\nPredetermined\n- [PI Bot](https://www.kaggle.com/jamesmcguigan/rock-paper-scissors-pi-bot)\n- [Anti-PI Bot](https://www.kaggle.com/jamesmcguigan/rock-paper-scissors-anti-pi-bot)\n- [Anti-Anti-PI Bot](https://www.kaggle.com/jamesmcguigan/rock-paper-scissors-anti-anti-pi-bot)\n- [De Bruijn Sequence](https://www.kaggle.com/jamesmcguigan/rock-paper-scissors-de-bruijn-sequence)\n\nRNG\n- [Random Agent](https://www.kaggle.com/jamesmcguigan/rock-paper-scissors-random-agent)\n- [Random Seed Search](https://www.kaggle.com/jamesmcguigan/rock-paper-scissors-random-seed-search)\n- [RNG Statistics](https://www.kaggle.com/jamesmcguigan/rock-paper-scissors-rng-statistics)\n\nOpponent Response\n- [Anti-Rotn](https://www.kaggle.com/jamesmcguigan/rock-paper-scissors-anti-rotn)\n- [Sequential Strategies](https://www.kaggle.com/jamesmcguigan/rock-paper-scissors-sequential-strategies)\n\nStatistical \n- [Weighted Random Agent](https://www.kaggle.com/jamesmcguigan/rock-paper-scissors-weighted-random-agent)\n- [Anti-Rotn Weighted Random](https://www.kaggle.com/jamesmcguigan/rock-paper-scissors-anti-rotn-weighted-random)\n- [Statistical Prediction](https://www.kaggle.com/jamesmcguigan/rock-paper-scissors-statistical-prediction)\n\nMemory Patterns\n- [Naive Bayes](https://www.kaggle.com/jamesmcguigan/rock-paper-scissors-naive-bayes)\n- [Memory Patterns](https://www.kaggle.com/jamesmcguigan/rock-paper-scissors-memory-patterns)\n\nDecision Tree\n- [XGBoost](https://www.kaggle.com/jamesmcguigan/rock-paper-scissors-xgboost)\n- [Multi Stage Decision Tree](https://www.kaggle.com/jamesmcguigan/rock-paper-scissors-multi-stage-decision-tree)\n- [Decision Tree Ensemble](https://www.kaggle.com/jamesmcguigan/rock-paper-scissors-decision-tree-ensemble)\n\nNeural Networks\n- [LSTM](https://www.kaggle.com/jamesmcguigan/rock-paper-scissors-lstm)\n\nEnsemble\n- [Multi Armed Stats Bandit](https://www.kaggle.com/jamesmcguigan/rock-paper-scissors-multi-armed-stats-bandit)\n\nRoShamBo Competition Winners\n- [Iocaine Powder](https://www.kaggle.com/jamesmcguigan/rps-roshambo-comp-iocaine-powder)\n- [Greenberg](https://www.kaggle.com/jamesmcguigan/rock-paper-scissors-greenberg)"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}