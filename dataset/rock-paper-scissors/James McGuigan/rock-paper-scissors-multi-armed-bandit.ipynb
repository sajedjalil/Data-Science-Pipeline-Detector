{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Rock Paper Scissors - Multi Armed Bandit\n\nThe idea here is to take a selection of agents, have them predict each move, and (hopefully) pick the agent most likely to win.\n\nThere are two implementions here. \n- The first uses the simple logic of computing the running average of the prediction accuracy and uses the agent with the higest score\n- The second correctly implements the Multi-Armed Bandit strategy, using exponential decay and uses `np.random.beta()` to generate a probability for selection\n\nThis notebook is inspired by:\n- https://www.kaggle.com/ilialar/multi-armed-bandit-vs-deterministic-agents"},{"metadata":{},"cell_type":"markdown","source":"# Imports\n\nRather than use a multi-file commit (which might have been easier), I using the method of concatenating all the scripts together. This method however requires care to avoid any global namespace conflicts with variables such as `history`. Thus perl is used to make minon edits to some of the files. "},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"!find ../input/ -name '*.py'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!cat ../input/rock-paper-scissors-anti-anti-pi-bot/pi.py | perl -p -e 's/kaggle_agent/anti_anti_pi_agent/g;' | tee anti_anti_pi.py > /dev/null\n!cat ../input/rock-paper-scissors-anti-pi-bot/pi.py      | perl -p -e 's/kaggle_agent/anti_pi_agent/g;'      | tee anti_pi.py      > /dev/null\n!cat ../input/rock-paper-scissors-pi-bot/pi.py           | perl -p -e 's/kaggle_agent/pi_agent/g;'           | tee pi.py           > /dev/null","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"!cat ../input/rock-paper-scissors/react.py | tee reactionary.py > /dev/null","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!cat ../input/rock-paper-scissors-anti-rotn/anti_rotn.py | perl -p -e 's/history/rotn_history/g;' | tee anti_rotn.py > /dev/null","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!cat ../input/rps-roshambo-comp-iocaine-powder/submission.py | tee -a iocaine.py > /dev/null","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!cat ../input/rock-paper-scissors-greenberg/greenberg.py | perl -p -e 's/kaggle_agent/greenberg_agent/g' | tee greenberg.py > /dev/null","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!cat ../input/rock-paper-scissors-statistical-prediction/submission.py | perl -p -e 's/history/statistical_history/g;' | tee statistical.py > /dev/null","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!cat ../input/rock-paper-scissors-naive-bayes/submission.py | perl -p -e 's/kaggle_agent/naive_bayes/g;' | tee naive_bayes.py > /dev/null","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!cat ../input/rock-paper-scissors-memory-patterns/submission.py | perl -p -e 's/kaggle_agent/memory_patterns/g;' | tee memory_patterns.py > /dev/null","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!cat ../input/rock-paper-scissors-naive-bayes/submission.py | perl -p -e 's/kaggle_agent/naive_bayes/g; s/instance/naive_bayes_instance/g;' | tee naive_bayes.py > /dev/null","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!cat ../input/rps-geometry-silver-rank-by-minimal-logic/geometry.py | perl -p -e 's/call_agent/geometry_agent/g;' | tee geometry.py > /dev/null","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cat ../input/rps-dojo/black_belt/IOU2.py | perl -p -e 's/agent/iou2_agent/g;' | tee IOU2.py > /dev/null","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cat ../input/rps-dojo/black_belt/memory_patterns_v20.py | perl -p -e 's/my_agent/memory_patterns_v20/g;' | tee memory_patterns_v20.py > /dev/null\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cat ../input/rps-dojo/black_belt/centrifugal_bumblepuppy_v4.py | perl -p -e 's/run/centrifugal_bumblepuppy/g; s/gg/gg_bumblepuppy/g; s/code/code_bumblepuppy/;' | tee centrifugal_bumblepuppy_v4.py > /dev/null","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!cat ../input/rps-dojo/black_belt/testing_please_ignore.py | perl -p -e 's/run/testing_please_ignore/g; s/gg/gg_ignore/g; s/code/code_ignore/;' | tee testing_please_ignore.py > /dev/null","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!cat ../input/rps-dojo/black_belt/dllu1.py | perl -p -e 's/run/dllu1_agent/g; s/gg/gg_dllu1/g; s/code/code_dllu1/;' | tee dllu1.py > /dev/null","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!cat ../input/rock-paper-scissors-random-seed-search/main.py | perl -p -e 's/(history|min_seed|best_method|solutions|random_agent)/rss_$1/g; s/seeds_per_turn=200_000/seeds_per_turn=100_000/g' | tee random_seed_search.py > /dev/null","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!cat ../input/rock-paper-scissors-decision-tree/submission.py | perl -p -e 's/history/decision_tree_history_1/g; s/decision_tree_agent/decision_tree_agent_1/g;' | tee decision_tree_1.py > /dev/null\n!cat ../input/rock-paper-scissors-decision-tree/submission.py | perl -p -e 's/history/decision_tree_history_2/g; s/decision_tree_agent/decision_tree_agent_2/g;' | tee decision_tree_2.py > /dev/null \n!cat ../input/rock-paper-scissors-decision-tree/submission.py | perl -p -e 's/history/decision_tree_history_2/g; s/decision_tree_agent/decision_tree_agent_3/g;' | tee decision_tree_3.py > /dev/null","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!cat ../input/rock-paper-scissors-genetic-algorithm/genetics.py ../input/rock-paper-scissors-genetic-algorithm/genetics_choice.py | perl -p -e 's/from genetics/#/' | tee genetics.py > /dev/null","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!cat ../input/rock-paper-scissors-flatten/flatten.py | tee flatten.py > /dev/null","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# !cat ../input/going-meta-with-kumoko/submission.py | perl -p -e 's/def agent/def kumoko_agent/' | tee kumoko.py > /dev/null","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!cat ../input/rps-opponent-transition-matrix/submission.py | tee transition.py > /dev/null","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!find ./ -name '*.py' | xargs -L1 perl -p -i -e 's/\\bprint\\(.*\\)/pass/sg;'","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Simple Winrate Agent\n\nThis was my attempt at rewriting the multi-armed bandit logic from scratch.\n\nIt uses simplified logic, in that it simply computes the (running) mean of predicted winrate for each agent based on historical data, but doesn't formally implement exponential decay or use `np.random.beta()` as a selection method.\n\nThis implemention is no longer being used"},{"metadata":{"trusted":true},"cell_type":"code","source":"# %%writefile multi_armed_stats_bandit.py\n# @unused - this file is not used\n\nfrom collections import defaultdict\nimport numpy as np\nimport time\nfrom operator import itemgetter\n\nmlb_opponent = []\nmlb_expected = defaultdict(list)\nmlb_agents   = {\n#     'random':               (lambda obs, conf: random_agent(obs, conf)),\n#     'pi':                   (lambda obs, conf: pi_agent(obs, conf)),\n    'anti_pi':              (lambda obs, conf: anti_pi_agent(obs, conf)),\n#     'anti_anti_pi':         (lambda obs, conf: anti_anti_pi_agent(obs, conf)),\n#     'reactionary':          (lambda obs, conf: reactionary(obs, conf)),\n#     'anti_rotn':            (lambda obs, conf: anti_rotn(obs, conf, warmup=1)),\n    \n    'iou2':                  (lambda obs, conf: iou2_agent(obs, conf)),\n    'geometry':              (lambda obs, conf: geometry_agent(obs, conf)),\n    'memory_patterns_v20':   (lambda obs, conf: memory_patterns_v20(obs, conf)),\n    'testing_please_ignore': (lambda obs, conf: testing_please_ignore(obs, conf)),\n    'naive_bayes':           (lambda obs, conf: naive_bayes(obs, conf)),\n    'bumblepuppy':           (lambda obs, conf: centrifugal_bumblepuppy(obs, conf)), \n    'dllu1_agent':           (lambda obs, conf: dllu1_agent(obs, conf)), \n\n    'genetics':              (lambda obs, conf: genetics_choice(obs, conf)), \n    'flatten':               (lambda obs, conf: flatten_agent(obs, conf)),\n    'transition':            (lambda obs, conf: transition_agent(obs, conf)),\n    # 'kumoko':                (lambda obs, conf: kumoko_agent(obs, conf)), # broken\n    \n    'memory_patterns':       (lambda obs, conf: memory_patterns(obs, conf)),\n    'naive_bayes':           (lambda obs, conf: naive_bayes(obs, conf)),\n    'iocaine':               (lambda obs, conf: iocaine_agent(obs, conf)),\n    'greenberg':             (lambda obs, conf: greenberg_agent(obs, conf)),\n    'statistical':           (lambda obs, conf: statistical_prediction_agent(obs, conf)),\n    'statistical_expected':  (lambda obs, conf: statistical_history['expected'][-1] + 1),       \n    # 'decision_tree_1':       (lambda obs, conf: decision_tree_agent_1(obs, conf, stages=1, window=4)),\n    'decision_tree_2':       (lambda obs, conf: decision_tree_agent_2(obs, conf, stages=2, window=6)),\n    'decision_tree_3':       (lambda obs, conf: decision_tree_agent_3(obs, conf, stages=3, window=10)),\n    # 'random_seed_search':    (lambda obs, conf: random_seed_search_agent(obs, conf)),\n}\n\n# observation   = {'step': 1, 'lastOpponentAction': 1}\n# configuration = {'episodeSteps': 10, 'agentTimeout': 60, 'actTimeout': 1, 'runTimeout': 1200, 'isProduction': False, 'signs': 3}\ndef multi_armed_bandit_stats_agent(observation, configuration, average='running', window=20):\n    global mlb_expected\n    global mlb_opponent\n    global mlb_agents\n    time_start = time.perf_counter()\n\n    if observation.step != 0: \n        mlb_opponent += [ observation.lastOpponentAction ]\n    # else:\n    #     mlb_opponent += [ random_agent(observation, configuration) ]\n\n    if window: \n        mlb_opponent = mlb_opponent[-window:] \n        for name, agent_fn in list(mlb_agents.items()):\n            mlb_expected[name] = mlb_expected[name][-window:] \n\n    \n    # accuracy is in date order\n    accuracy = {\n        name: np.array([\n            int( mlb_expected[name][-n] == mlb_opponent[-n] )\n            for n in range(1, min(len(values), len(mlb_opponent))+1)\n        ])[::-1]\n        if len(values) else np.array([0.0])\n        for name, values in list(mlb_expected.items())\n    }\n    \n    # Update predictions for next turn\n    for name, agent_fn in list(mlb_agents.items()):\n        try:\n            agent_action        = agent_fn(observation, configuration)\n            agent_expected      = (agent_action - 1) % configuration.signs\n            mlb_expected[name] += [ agent_expected ]\n        except Exception as exception:\n            print('Exception:', name, agent_fn, exception)\n    \n    action     = 1\n    agent_name = 'random'\n\n    scores = {}\n    if observation.step != 0: \n        # Compute average scores\n        for name, values in accuracy.items():\n            if len(values) == 0:\n                scores[name] = 0.0\n            elif average == 'mean':\n                scores[name] = np.mean( values )\n            elif average == 'mean_squared':\n                scores[name] = np.mean( values ) ** 2\n            elif average == 'running':\n                weights = np.sqrt(np.arange(1,len(values)+1))[::-1] \n                scores[name] = np.mean( values / weights * np.sum(weights) )  \n            else: \n                assert average in [ 'mean', 'mean_squared', 'half', 'running' ], f\"average != {[ 'mean', 'mean_squared', 'half', 'running' ]}\" \n            scores[name] = np.round(scores[name], 5)\n\n        scores       = dict(sorted(scores.items(),       key=itemgetter(1),                       reverse=True))\n        mlb_expected = dict(sorted(mlb_expected.items(), key=lambda pair: scores.get(pair[0], 0), reverse=True))\n\n        # Sort by most accurate\n        if len(scores):\n            # agent_name, score = sorted(scores.items(), key=itemgetter(1), reverse=True)[0]\n            agent_name = random.choices( population=list(scores.keys()), weights=list(scores.values()), k=1 )[0]\n            expected   = mlb_expected[agent_name][-1]\n        else:\n            agent_name, score = 'random', 0\n            expected = random_agent(observation, configuration)\n\n        action = (expected + 1) % configuration.signs\n                \n    time_taken = time.perf_counter() - time_start\n    print(f'opponent =', mlb_opponent)\n    print(f'expected =', dict(mlb_expected))\n    print(f'scores   =', dict(scores))\n    print(f'action   = {action} | agent = {agent_name} | step = {observation.step} | {time_taken:.3f}s')\n    print()\n    \n    return int(action)\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Multi-Armed Bandit\n\nThis is a more mathematically correct implemention of the Multi-Armed Bandit logic. \n\nIt uses both exponential decay and with probabilistic selection using `np.random.beta()`. Logic was inspired by:\n- https://www.kaggle.com/ilialar/multi-armed-bandit-vs-deterministic-agents"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%writefile multi_armed_bandit.py\n# Source: https://www.kaggle.com/jamesmcguigan/rock-paper-scissors-multi-armed-bandit/\nimport contextlib\nimport os\nfrom collections import defaultdict\nimport numpy as np\nimport time\nfrom operator import itemgetter\n\n# from memory.memory_patterns import memory_patterns_agent\n# from memory.RPSNaiveBayes import naive_bayes_agent\n# from rng.random_agent import random_agent\n# from roshambo_competition.greenberg import greenberg_agent\n# from roshambo_competition.iocaine_powder import iocaine_agent\n# from statistical.statistical_prediction import statistical_prediction_agent\n# from simple.anti_pi import anti_pi_agent\n# from simple.pi import pi_agent\n\nmlb_history  = {\n    'actions':  [],\n    'opponent': []\n}\nmlb_expected = defaultdict(list)\nmlb_agents   = {\n    #     'random':               (lambda obs, conf: random_agent(obs, conf)),\n    #     'pi':                   (lambda obs, conf: pi_agent(obs, conf)),\n    'anti_pi':               (lambda obs, conf: anti_pi_agent(obs, conf)),\n    #     'anti_anti_pi':         (lambda obs, conf: anti_anti_pi_agent(obs, conf)),\n    #     'reactionary':          (lambda obs, conf: reactionary(obs, conf)),\n    'anti_rotn':            (lambda obs, conf: anti_rotn(obs, conf, warmup=1)),\n\n    'iou2':                  (lambda obs, conf: iou2_agent(obs, conf)),\n    'geometry':              (lambda obs, conf: geometry_agent(obs, conf)),\n    'memory_patterns_v20':   (lambda obs, conf: memory_patterns_v20(obs, conf)),\n    'testing_please_ignore': (lambda obs, conf: testing_please_ignore(obs, conf)),\n    'bumblepuppy':           (lambda obs, conf: centrifugal_bumblepuppy(obs, conf)),\n    'dllu1_agent':           (lambda obs, conf: dllu1_agent(obs, conf)),\n\n    'genetics':              (lambda obs, conf: genetics_choice(obs, conf)),\n    'flatten':               (lambda obs, conf: flatten_agent(obs, conf)),\n    'transition':            (lambda obs, conf: transition_agent(obs, conf)),\n    # 'kumoko':                (lambda obs, conf: kumoko_agent(obs, conf)), # broken    \n\n    'memory_patterns':       (lambda obs, conf: memory_patterns(obs, conf)),\n    'naive_bayes':           (lambda obs, conf: naive_bayes(obs, conf)),\n    'iocaine':               (lambda obs, conf: iocaine_agent(obs, conf)),\n    'greenberg':             (lambda obs, conf: greenberg_agent(obs, conf)),\n    'statistical':           (lambda obs, conf: statistical_prediction_agent(obs, conf)),\n    'statistical_expected':  (lambda obs, conf: statistical_history['expected'][-1] + 1),\n    # 'decision_tree_1':       (lambda obs, conf: decision_tree_agent_1(obs, conf, stages=1, window=20)),\n    'decision_tree_2':       (lambda obs, conf: decision_tree_agent_2(obs, conf, stages=2, window=6)),\n    'decision_tree_3':       (lambda obs, conf: decision_tree_agent_3(obs, conf, stages=3, window=10)),\n    #'random_seed_search':    (lambda obs, conf: random_seed_search_agent(obs, conf)),\n}\n\n# observation   = {'step': 1, 'lastOpponentAction': 1}\n# configuration = {'episodeSteps': 10, 'agentTimeout': 60, 'actTimeout': 1, 'runTimeout': 1200, 'isProduction': False, 'signs': 3}\ndef multi_armed_bandit_agent(observation, configuration, warmup=1, step_reward=3, decay_rate=0.95, verbose=True ):\n    global mlb_expected\n    global mlb_history\n    global mlb_agents\n    time_start = time.perf_counter()\n    if os.environ.get('KAGGLE_KERNEL_RUN_TYPE', 'Localhost') == 'Interactive':\n        warmup = 1\n\n\n    if observation.step != 0:\n        mlb_history['opponent'] += [ observation.lastOpponentAction ]\n    # else:\n    #     mlb_history['opponent'] += [ random_agent(observation, configuration) ]\n\n\n    # Implement Multi Armed Bandit Logic\n    win_loss_scores = defaultdict(lambda: [0.0, 0.0])\n    for name, values in list(mlb_expected.items()):\n        for n in range(min(len(values), len(mlb_history['opponent']))):\n            win_loss_scores[name][1] = (win_loss_scores[name][1] - 1) * decay_rate + 1\n            win_loss_scores[name][0] = (win_loss_scores[name][0] - 1) * decay_rate + 1\n\n            # win | expect rock, play paper -> opponent plays rock\n            if   mlb_expected[name][n] == (mlb_history['opponent'][n] + 0) % configuration.signs:\n                win_loss_scores[name][0] += step_reward\n\n                # draw | expect rock, play paper -> opponent plays paper\n            elif mlb_expected[name][n] == (mlb_history['opponent'][n] + 1) % configuration.signs:\n                win_loss_scores[name][0] += step_reward\n                win_loss_scores[name][1] += step_reward\n\n                # win | expect rock, play paper -> opponent plays scissors\n            elif mlb_expected[name][n] == (mlb_history['opponent'][n] + 2) % configuration.signs:\n                win_loss_scores[name][1] += step_reward\n\n\n    # Update predictions for next turn\n    for name, agent_fn in list(mlb_agents.items()):\n        try:\n            with contextlib.redirect_stdout(None):  # disable stdout for child agents\n                agent_action        = agent_fn(observation, configuration)\n                agent_expected      = (agent_action - 1) % configuration.signs\n                mlb_expected[name] += [ agent_expected ]\n        except Exception as exception:\n            print('Exception:', name, agent_fn, exception)\n\n\n    # Pick the Best Agent\n    beta_scores = {\n        name: np.random.beta(win_loss_scores[name][0], win_loss_scores[name][1])\n        for name in win_loss_scores.keys()\n    }\n\n    if observation.step == 0:\n        # Always play scissors first move\n        # At Auction       - https://www.artsy.net/article/artsy-editorial-christies-sothebys-played-rock-paper-scissors-20-million-consignment\n        # EDA best by test - https://www.kaggle.com/jamesmcguigan/rps-episode-archive-dataset-eda\n        agent_name = 'scissors'\n        expected = 1\n    elif observation.step < warmup:\n        agent_name = 'random'\n        expected   = random_agent(observation, configuration)\n    else:\n        agent_name = sorted(beta_scores.items(), key=itemgetter(1), reverse=True)[0][0]\n        expected   = mlb_expected[agent_name][-1]\n\n    action = (expected + 1) % configuration.signs\n\n\n    if verbose:\n        best_score    = beta_scores.get(agent_name,0)\n        last_opponent = (mlb_history['opponent'] or [0])[-1]\n        win_symbol    = (\n            ' ' if observation.step == 0 else \n            '+' if mlb_history['actions'][-1] == (mlb_history['opponent'][-1] + 1) % 3 else\n            '|' if mlb_history['actions'][-1] == (mlb_history['opponent'][-1] + 0) % 3 else\n            '-'\n        )\n        time_taken    = time.perf_counter() - time_start\n        print(f'{observation.step:4d} | {time_taken:0.2f}s | {last_opponent}{win_symbol} -> action = {expected} -> {action} | {best_score*100:3.0f}% {agent_name}')\n\n    mlb_history['actions'] += [ action ]\n    return int(action)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!rm -vf submission.py\n!ls -tr ./*.py | xargs -L1 -I{} sed -i -z 's!^!\\n##### {} #####\\n\\n!g' {}  # add filename to start of each file\n!ls -tr ./*.py | xargs -L1 -I{} sed -i -z 's/$/\\n\\n/g' {}  # add newlines to end of each file\n!ls -tr ./*.py | xargs cat > submission.py\n%run -i 'submission.py'\n# !head submission.py","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Test"},{"metadata":{"trusted":true},"cell_type":"code","source":"# !cat -n \"submission.py\" | grep 1687 -C 10","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from kaggle_environments import make\n\nenv = make(\"rps\", configuration={\"episodeSteps\": 25}, debug=True)\nenv.run([\"submission.py\", \"pi.py\" ])\nenv.render(mode=\"ipython\", width=600, height=600)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from kaggle_environments import make\n\nenv = make(\"rps\", configuration={\"episodeSteps\": 25}, debug=True)\nenv.run([\"submission.py\", lambda obs, conf: obs.step % 3 ])\nenv.render(mode=\"ipython\", width=600, height=600)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from kaggle_environments import make\n\nenv = make(\"rps\", configuration={\"episodeSteps\": 25}, debug=True)\nenv.run([\"submission.py\", \"anti_rotn.py\"])\nenv.render(mode=\"ipython\", width=600, height=600)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# !cat -n \"submission.py\" | grep -v PI | grep -C 5 3364 ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from kaggle_environments import make\n\nenv = make(\"rps\", configuration={\"episodeSteps\": 25}, debug=True)\nenv.run([\"submission.py\", \"statistical.py\"])\nenv.render(mode=\"ipython\", width=600, height=600)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from kaggle_environments import make\n\nenv = make(\"rps\", configuration={\"episodeSteps\": 100}, debug=False)\nenv.run([\"submission.py\", \"decision_tree_3.py\"])\nenv.render(mode=\"ipython\", width=600, height=600)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Further Reading\n\nThis notebook is part of a series exploring Rock Paper Scissors:\n\nPredetermined\n- [PI Bot](https://www.kaggle.com/jamesmcguigan/rock-paper-scissors-pi-bot)\n- [Anti-PI Bot](https://www.kaggle.com/jamesmcguigan/rock-paper-scissors-anti-pi-bot)\n- [Anti-Anti-PI Bot](https://www.kaggle.com/jamesmcguigan/rock-paper-scissors-anti-anti-pi-bot)\n- [De Bruijn Sequence](https://www.kaggle.com/jamesmcguigan/rock-paper-scissors-de-bruijn-sequence)\n\nRNG\n- [Random Agent](https://www.kaggle.com/jamesmcguigan/rock-paper-scissors-random-agent)\n- [Random Seed Search](https://www.kaggle.com/jamesmcguigan/rock-paper-scissors-random-seed-search)\n- [RNG Statistics](https://www.kaggle.com/jamesmcguigan/rock-paper-scissors-rng-statistics)\n\nOpponent Response\n- [Anti-Rotn](https://www.kaggle.com/jamesmcguigan/rock-paper-scissors-anti-rotn)\n- [Sequential Strategies](https://www.kaggle.com/jamesmcguigan/rock-paper-scissors-sequential-strategies)\n\nStatistical \n- [Weighted Random Agent](https://www.kaggle.com/jamesmcguigan/rock-paper-scissors-weighted-random-agent)\n- [Anti-Rotn Weighted Random](https://www.kaggle.com/jamesmcguigan/rock-paper-scissors-anti-rotn-weighted-random)\n- [Statistical Prediction](https://www.kaggle.com/jamesmcguigan/rock-paper-scissors-statistical-prediction)\n\nMemory Patterns\n- [Naive Bayes](https://www.kaggle.com/jamesmcguigan/rock-paper-scissors-naive-bayes)\n- [Memory Patterns](https://www.kaggle.com/jamesmcguigan/rock-paper-scissors-memory-patterns)\n\nDecision Tree\n- [XGBoost](https://www.kaggle.com/jamesmcguigan/rock-paper-scissors-xgboost)\n- [Multi Stage Decision Tree](https://www.kaggle.com/jamesmcguigan/rock-paper-scissors-multi-stage-decision-tree)\n- [Decision Tree Ensemble](https://www.kaggle.com/jamesmcguigan/rock-paper-scissors-decision-tree-ensemble)\n\nNeural Networks\n- [LSTM](https://www.kaggle.com/jamesmcguigan/rock-paper-scissors-lstm)\n\nEnsemble\n- [Multi Armed Stats Bandit](https://www.kaggle.com/jamesmcguigan/rock-paper-scissors-multi-armed-stats-bandit)\n\nRoShamBo Competition Winners\n- [Iocaine Powder](https://www.kaggle.com/jamesmcguigan/rps-roshambo-comp-iocaine-powder)\n- [Greenberg](https://www.kaggle.com/jamesmcguigan/rock-paper-scissors-greenberg)"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}