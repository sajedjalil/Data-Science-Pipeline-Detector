{"cells":[{"metadata":{},"cell_type":"markdown","source":"This is a slightly updated copy of [Chan Kha Vu](https://www.kaggle.com/chankhavu)'s original [RPS Dojo](https://www.kaggle.com/chankhavu/rps-dojo). Please follow the link first and give the author all the respect he or she deserves! Kernels like this really make a difference and bring more life to Kaggle Competitions!\n\n# Rock-Paper-Scissors Dojo\n\n![Dojo](https://i.imgur.com/jeWU2Ea.png[/img])\n\nIt is very important to keep a diverse pool of agents to test your new agent with. In this notebook, I collected a bunch of agents from public notebooks to form a \"Dojo\" where you can test your agent before submitting.\n\nI am also planning to add my own agents that are weaker than my flagship ones in the future. This notebook can be attached to your notebooks as a dataset. At the end of this notebook, you can also find a simple code for agents comparison.\n\n## <span style='color:blue'>Changes compared to original version</span>\n### Performance\n* Dependency-free local high-performance RPS evaluator\n* All agents are converted from files into classes with same contract and shared library dependencies\n* All agents are ranked by performance and have performance coefficient added to control how many matches will be played against this agent\n* Some slow agents are slightly modified to increase repformance\n* Multi-armed bandits are excluded - those are extremely slow and are in fact a <i>meta</i>-agents that can be composed of any subset of other agents\n\n### Competition\n* New <span style='color:brown'>Brown</span> Belt added\n* Some agents moved to other categories to better match complexity\n* New agents added based on my own research\n\n### New agent format\nAgents are now classes. New instance is created before each match, so `__init__` is where you define global parameters.\n\n`def next_action(self, T, A, S):` instance method is called on every step with following parameters:\n* `T = observation.step`\n* `A = observation.lastOpponentAction if T > 0 else None`\n* `S = configuration.signs`"},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"import random\nimport secrets\nimport math\nimport collections\nimport time\nimport numpy as np\nimport pandas as pd\nfrom tqdm.auto import tqdm\nfrom multiprocessing import Pool\n\nimport pydash\nfrom itertools import combinations_with_replacement\nfrom sklearn.tree import DecisionTreeClassifier\nfrom xgboost import XGBClassifier\nfrom operator import itemgetter\n\ndef get_score(S, A1, A2):\n    return (S + A1 - A2 + 1) % S - 1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h1 style='background:white; border:3px solid; color:black'><center>White Belt</center></h1>\n\n<br>\n\nThese agents are extremely simple and can be used as a minimum testing pool for your agent. A decent agent should beat all of the \"white belt\" baseline ones in 100% of matches. The following agents were added in the \"simple\" category:\n- **`Rock`** &mdash; plays only Rock\n- **`Paper`** &mdash; plays only Paper\n- **`Scissors`** &mdash; plays only Scissors\n- **`Mirror`** &mdash; mirrors the opponent's last moves\n- **`Mirror1`** &mdash; mirrors the opponent's last moves and shift by 1\n- **`Mirror2`** &mdash; mirrors the opponent's last moves and shift by 2\n- **`CounterReact`** &mdash; counter strategy to reactionary `Mirror1` agent\n- **`DeBruijn`** &mdash; from the [Rock Paper Scissors - De Bruijn Sequence](https://www.kaggle.com/jamesmcguigan/rock-paper-scissors-de-bruijn-sequence) notebook\n- **`Statistical`** &mdash; monitors the move statistics and tries to counter the most frequent move\n- **`NotSoMarkov`** &mdash; from [(Not so) Markov](https://www.kaggle.com/alexandersamarin/not-so-markov) (v5)\n- **`RFind`** &mdash; from [Running RPSContest bots](https://www.kaggle.com/purplepuppy/running-rpscontest-bots)"},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":false},"cell_type":"code","source":"class Rock:\n    K = 20000\n    def next_action(self, T, A, S):\n      return 0\n\nclass Paper:\n    K = 20000\n    def next_action(self, T, A, S):\n      return 1\n\nclass Scissors:\n    K = 20000\n    def next_action(self, T, A, S):\n      return 2\n\nclass Mirror:\n    K = 10000\n    def next_action(self, T, A, S):\n        if T == 0:\n            return secrets.randbelow(S)\n        else:\n            return A\n\nclass Mirror1:\n    K = 10000\n    def next_action(self, T, A, S):\n        if T == 0:\n            return secrets.randbelow(S)\n        else:\n            return (A + 1) % S\n\nclass Mirror2:\n    K = 10000\n    def next_action(self, T, A, S):\n        if T == 0:\n            return secrets.randbelow(S)\n        else:\n            return (A + 2) % S\n\nclass CounterReact:\n    K = 6000\n    def __init__(self):\n        self.last_counter_action = 0\n\n    def next_action(self, T, A, S):\n        if T == 0:\n            self.last_counter_action = secrets.randbelow(S)\n        elif get_score(S, self.last_counter_action, A) == 1:\n            self.last_counter_action = (self.last_counter_action + 2) % S\n        else:\n            self.last_counter_action = (A + 1) % S\n\n        return self.last_counter_action\n\nclass DeBruijn:\n    K = 10000\n    actions = pydash.flatten(list(combinations_with_replacement([2,1,0,2,1,0],3)) * 18)\n\n    def next_action(self, T, A, S):\n        return int(DeBruijn.actions[T] % S)\n    \nclass Statistical:\n    K = 5000\n    def __init__(self):\n        self.action_histogram = {}\n\n    def next_action(self, T, A, S):\n        if T == 0:\n            return secrets.randbelow(S)\n        else:\n            if A not in self.action_histogram:\n                self.action_histogram[A] = 0\n            self.action_histogram[A] += 1\n            mode_action = None\n            mode_action_count = None\n            for k, v in self.action_histogram.items():\n                if mode_action_count is None or v > mode_action_count:\n                    mode_action = k\n                    mode_action_count = v\n                    continue\n\n            return (mode_action + 1) % S\n        \nclass NotSoMarkov:\n    K = 150\n    def __init__(self):\n        self.action_seq = []\n        self.table = None\n    \n    @property\n    def key(self):\n        return ''.join([str(a) for a in self.action_seq[:-1]])\n\n    def next_action(self, T, A, S):\n        k = 2\n        if T % 250 == 0: # refresh table every 250 steps\n            self.action_seq = []\n            self.table = collections.defaultdict(lambda: [1, 1, 1])    \n        if len(self.action_seq) <= 2 * k + 1:\n            action = secrets.randbelow(S)\n            if T > 0:\n                self.action_seq.extend([A, action])\n            else:\n                self.action_seq.append(action)\n            return action\n        # update table\n        self.table[self.key][A] += 1\n        # update action seq\n        self.action_seq[:-2] = self.action_seq[2:]\n        self.action_seq[-2] = A\n        # predict opponent next move\n        if T < 500:\n            next_opponent_action_pred = np.argmax(self.table[self.key])\n        else:\n            scores = np.array(self.table[self.key])\n            next_opponent_action_pred = np.random.choice(S, p=scores/scores.sum()) # add stochasticity for second part of the game\n        # make an action\n        action = (next_opponent_action_pred + 1) % S\n        # if high probability to lose -> let's surprise our opponent with sudden change of our strategy\n        if T > 900:\n            action = next_opponent_action_pred\n        self.action_seq[-1] = action\n        return int(action)\n    \nclass RFind:\n    K = 200\n    def __init__(self):\n        self.hist = []  # history of your moves\n        self.dict_last = {}\n        self.max_dict_key = 10\n        self.last_move = 0\n\n    def predict(self, S):\n        for i in reversed(range(min(len(self.hist), self.max_dict_key))):\n            t = tuple(self.hist[-i:])\n            if t in self.dict_last:\n                return self.dict_last[t]\n        return secrets.randbelow(S)\n\n    def update(self, move, A):\n        self.hist.append(move)\n        for i in reversed(range(min(len(self.hist), self.max_dict_key))):\n            t = tuple(self.hist[-i:])\n            self.dict_last[t] = A\n\n    def next_action(self, T, A, S):\n        if T == 0:\n            self.last_move = secrets.randbelow(S)\n        else:\n            self.update(self.last_move, A)\n            self.last_move = (self.predict(S) + 1) % S\n        return self.last_move\n    \nwhite_belt = {\n    \"Rock\": Rock,\n    \"Paper\": Paper,\n    \"Scissors\": Scissors,\n    \"Mirror\": Mirror,\n    \"Mirror1\": Mirror1,\n    \"Mirror2\": Mirror2,\n    \"CounterReact\": CounterReact,\n    \"DeBruijn\": DeBruijn,\n    \"Statistical\": Statistical,\n    \"NotSoMarkov\": NotSoMarkov,\n    \"RFind\": RFind,\n}\nprint(list(white_belt.keys()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h1 style='background:blue; border:3px solid; border-color: blue; color:white'><center>Blue Belt</center></h1>\n\n<br>\n\nYou also need to beat these in 100% of the time if you want any chance to rise to the \"bronze\" range, because the LB is probably already filled with those agents. The following agents were added in the \"Blue Belt\" category:\n\n- **`TransitionMatrix`** &mdash; from [RPS: Opponent Transition Matrix](https://www.kaggle.com/group16/rps-opponent-transition-matrix) notebook (v2)\n- **`StochasticTransitionMatrix`** &mdash; from [RPS - Stochastic Transition Matrix](https://www.kaggle.com/peternagymathe/rps-stochastic-transition-matrix)\n- **`StatisticalPrediction`** &mdash; from [Rock Paper Scissors - Statistical Prediction](https://www.kaggle.com/jamesmcguigan/rock-paper-scissors-statistical-prediction) (v17)\n- **`WeightedRandom`** &mdash; from [Weighted Random Agent](https://www.kaggle.com/jamesmcguigan/rock-paper-scissors-weighted-random-agent?scriptVersionId=46468176) (v4)\n- **`Patterns`** &mdash; stores statistics of opponent moves separately for each combination of preceding actions\n- **`SelfPatterns`** &mdash; stores statistics of own and opponent moves for each combination of preceding actions (2-5 steps to the past)"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"class TransitionMatrix:\n    K = 100\n    def __init__(self):\n        self.T = np.zeros((3, 3))\n        self.P = np.zeros((3, 3))\n\n        # a1 is the action of the opponent 1 step ago\n        # a2 is the action of the opponent 2 steps ago\n        self.a1 = None\n        self.a2 = None\n\n    def next_action(self, T, A, S):\n        if T > 1:\n            self.a1 = A\n            self.T[self.a2, self.a1] += 1\n            self.P = np.divide(self.T, np.maximum(1, self.T.sum(axis=1)).reshape(-1, 1))\n            self.a2 = self.a1\n            if np.sum(self.P[self.a1, :]) == 1:\n                return int((np.random.choice(3, p=self.P[self.a1, :]) + 1) % S)\n            else:\n                return secrets.randbelow(S)\n        else:\n            if T == 1:\n                self.a2 = A\n            return secrets.randbelow(S)\n\nclass StochasticTransitionMatrix:\n    K = 100\n    def __init__(self):\n        self.matrix = np.ones((3,3,3)) * (1/3) #so we can choose object based on what we chose and what the opponent chose transition matrix\n        self.matrix_freq = np.ones((3,3,3)) #frequency matrix\n        self.prev_me = 0\n        self.prev_op = 0\n\n    def next_action(self, T, A, S):\n        if T == 0:\n            self.prev_me = (np.random.choice(S, p=self.matrix[self.prev_op, self.prev_me, :]) + 1) % S\n            return self.prev_me\n        if T > 1:\n            self.matrix_freq[self.prev_op, self.prev_me, A] += 1\n            self.matrix[self.prev_op, self.prev_me, :] = self.matrix_freq[self.prev_op, self.prev_me, :] / np.sum(self.matrix_freq[self.prev_op, self.prev_me, :]) \n        self.prev_op = A #we store the last action of the opponent  \n        self.prev_me = (np.random.choice(S, p=self.matrix[self.prev_op, self.prev_me, :]) + 1) % S\n        return self.prev_me\n\nclass StatisticalPrediction:\n    K = 10\n    def __init__(self):\n        self.history = {\n            \"guess\":      [0,1,2],\n            \"prediction\": [0,1,2],\n            \"expected\":   [0,1,2],\n            \"action\":     [0,1,2],\n            \"opponent\":   [0,1],\n        }\n\n    def next_action(self, T, A, S):\n        actions         = list(range(S))  # [0,1,2]\n        last_action     = self.history['action'][-1]\n        opponent_action = A if T > 0 else 2\n\n        self.history['opponent'].append(opponent_action)\n\n        # Make weighted random guess based on the complete move history, weighted towards relative moves based on our last action \n        move_frequency       = collections.Counter(self.history['opponent'])\n        response_frequency   = collections.Counter(zip(self.history['action'], self.history['opponent'])) \n        move_weights         = [ move_frequency.get(n,1) + response_frequency.get((last_action,n),1) for n in range(S) ] \n        guess                = random.choices( population=actions, weights=move_weights, k=1 )[0]\n\n        # Compare our guess to how our opponent actually played\n        guess_frequency      = collections.Counter(zip(self.history['guess'], self.history['opponent']))\n        guess_weights        = [ guess_frequency.get((guess,n),1) for n in range(S) ]\n        prediction           = random.choices( population=actions, weights=guess_weights, k=1 )[0]\n\n        # Repeat, but based on how many times our prediction was correct\n        prediction_frequency = collections.Counter(zip(self.history['prediction'], self.history['opponent']))\n        prediction_weights   = [ prediction_frequency.get((prediction,n),1) for n in range(S) ]\n        expected             = random.choices( population=actions, weights=prediction_weights, k=1 )[0]\n\n        # Play the +1 counter move\n        action = (expected + 1) % S\n\n        # Persist state\n        self.history['guess'].append(guess)\n        self.history['prediction'].append(prediction)\n        self.history['expected'].append(expected)\n        self.history['action'].append(action)\n\n        return action\n\nclass WeightedRandom:\n    K = 800\n    def next_action(self, T, A, S):\n        if T == 0:\n            self.choices = list(range(S))\n            self.opponent_frequency = [1] * S\n        else:\n            self.opponent_frequency[A] += 1\n\n        expected_action = random.choices(self.choices, weights=self.opponent_frequency, k=1)[0]\n        counter_action  = (expected_action + 1) % S\n        return counter_action\n\nclass Patterns:\n    K = 400\n    def __init__(self):\n        self.rng = random.SystemRandom()\n\n        self.hash1 = 0\n        self.hash2 = 0\n        self.hash3 = 0\n        self.map1 = {}\n        self.map2 = {}\n        self.map3 = {}\n        self.Jmin = 1\n        self.Jmax = 11\n        self.J = self.rng.randrange(self.Jmin, self.Jmax+1)\n        self.D = 2\n        self.G = 3\n        self.R = 0.6\n        self.B = 0\n\n    def add(self, map1, hash1, A):\n        if hash1 not in map1:\n            map1[hash1] = {'S':0}\n        d = map1[hash1]\n        if A not in d:\n            d[A] = 1\n        else:\n            d[A] += 1\n        d['S'] += 1\n\n    def match(self, map1, hash1, S):\n        if hash1 not in map1:\n            return\n        d = map1[hash1]\n        if d['S'] >= self.G:\n            for A in range(S):\n                if A in d and d[A] >= d['S'] * self.R:\n                    self.B = (A+1) % S\n                    self.J = self.rng.randrange(self.Jmin, self.Jmax)\n\n    def next_action(self, T, A, S):\n        if T > self.D:\n            self.add(self.map1, self.hash1, A)\n            self.add(self.map2, self.hash2, A)\n            self.add(self.map3, self.hash3, A)\n        if T > 0:\n            self.hash1 = self.hash1 // S + A * S**(self.D-1)\n            self.hash2 = self.hash2 // S + self.B * S**(self.D-1)\n            self.hash3 = self.hash3 // S**2 + (A + S*self.B) * S**(2*self.D-1)\n        self.B = self.rng.randrange(0, S)\n        if self.J == 0:\n            self.match(self.map1, self.hash1, S)\n            self.match(self.map2, self.hash2, S)\n            self.match(self.map3, self.hash3, S)\n        else:\n            self.J -= 1\n        return self.B\n    \nclass SelfPatterns:\n    K = 100\n    def __init__(self):\n        self.Jmin = 0\n        self.Jmax = 5\n        self.J = self.Jmin + secrets.randbelow(self.Jmax-self.Jmin+1)\n        self.Dmin = 2\n        self.Dmax = 5\n        self.Hash = []\n        self.Map = []\n        self.MyMap = []\n        for D in range(self.Dmin,self.Dmax+1):\n            self.Hash.append([0, 0, 0])\n            self.Map.append([{}, {}, {}])\n            self.MyMap.append([{}, {}, {}])\n        self.G = 2\n        self.R = 0.4\n        self.V = 0.7\n        self.VM = 0.7\n        self.B = 0\n\n    def add(self, map1, hash1, A):\n        if hash1 not in map1:\n            map1[hash1] = {'S':0}\n        d = map1[hash1]\n        if A not in d:\n            d[A] = 1\n        else:\n            d[A] += 1\n        d['S'] += 1\n\n    def match(self, map1, hash1, S):\n        if hash1 not in map1:\n            return\n        d = map1[hash1]\n        if d['S'] >= self.G:\n            for A in range(S):\n                if A in d and (d[A] >= d['S'] * self.R + (1-self.R) * self.G) and secrets.randbelow(101) < 100 * self.V:\n                    if secrets.randbelow(101) < 100 * self.VM:\n                        self.B = (A+1) % S\n                    else:\n                        self.B = A % S\n                    self.J = self.Jmin + secrets.randbelow(self.Jmax-self.Jmin+1)\n\n    def next_action(self, T, A, S):\n        BA = (self.B+1)%S\n        self.B = secrets.randbelow(S)\n        for D in range(self.Dmin,self.Dmax+1):\n            if T > D:\n                self.add(self.Map[D-self.Dmin][0], self.Hash[D-self.Dmin][0], A)\n                self.add(self.Map[D-self.Dmin][1], self.Hash[D-self.Dmin][1], A)\n                self.add(self.Map[D-self.Dmin][2], self.Hash[D-self.Dmin][2], A)\n                self.add(self.MyMap[D-self.Dmin][0], self.Hash[D-self.Dmin][0], BA)\n                self.add(self.MyMap[D-self.Dmin][1], self.Hash[D-self.Dmin][1], BA)\n                self.add(self.MyMap[D-self.Dmin][2], self.Hash[D-self.Dmin][2], BA)\n            if T > 0:\n                self.Hash[D-self.Dmin][0] = self.Hash[D-self.Dmin][0] // S**2 + (A + S*self.B) * S**(2*D-1)\n                self.Hash[D-self.Dmin][1] = self.Hash[D-self.Dmin][1] // S + A * S**(D-1)\n                self.Hash[D-self.Dmin][2] = self.Hash[D-self.Dmin][2] // S + self.B * S**(D-1)\n            if self.J == 0:\n                self.match(self.Map[D-self.Dmin][0], self.Hash[D-self.Dmin][0], S)\n                self.match(self.Map[D-self.Dmin][1], self.Hash[D-self.Dmin][1], S)\n                self.match(self.Map[D-self.Dmin][2], self.Hash[D-self.Dmin][2], S)\n            if self.J == 0:\n                self.match(self.MyMap[D-self.Dmin][0], self.Hash[D-self.Dmin][0], S)\n                self.match(self.MyMap[D-self.Dmin][1], self.Hash[D-self.Dmin][1], S)\n                self.match(self.MyMap[D-self.Dmin][2], self.Hash[D-self.Dmin][2], S)\n        if self.J > 0:\n            self.J -= 1\n        return self.B\n\nblue_belt = {\n    \"TransitionMatrix\": TransitionMatrix,\n    \"StochasticTransitionMatrix\": StochasticTransitionMatrix,\n    \"StatisticalPrediction\": StatisticalPrediction,\n    \"WeightedRandom\": WeightedRandom,\n    \"Patterns\": Patterns,\n    \"SelfPatterns\": SelfPatterns,\n}\nprint(list(blue_belt.keys()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h1 style='background:brown; border:3px solid; border-color: brown; color:white'><center>Brown Belt</center></h1>\n\n<br>\n\nCompletely random agents are expected to be unpredictable, so normally you would end up with nearly all ties against these agents.\n\nHowever, no pseudo random number generator is perfect. If your agent manages to crack specific algorithm used by random generator, it could try to win more often:\n\n- **`Random`** &mdash; random - built-in python random generator\n- **`SystemRandom`** &mdash; SystemRandom - generator provided by underlying OS\n- **`SecretsRandom`** &mdash; secrets - cryptographically strong random generator, arguably most suitable for competitions like this\n- **`NumpyRandom`** &mdash; numpy - numpy random choice random generator"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"class Random:\n    K = 4000\n    def next_action(self, T, A, S):\n        return random.randrange(0, S)\n\nclass SystemRandom:\n    K = 1000\n    def __init__(self):\n        self.rng = random.SystemRandom()\n\n    def next_action(self, T, A, S):\n        return self.rng.randrange(0, S)\n\nclass SecretsRandom:\n    K = 1000\n    def next_action(self, T, A, S):\n        return secrets.randbelow(S)\n    \nclass NumpyRandom:\n    K = 1000\n    def next_action(self, T, A, S):\n        return np.random.choice(S)\n    \nbrown_belt = {\n    \"Random\": Random,\n    \"SystemRandom\": SystemRandom,\n    \"SecretsRandom\": SecretsRandom,\n    \"NumpyRandom\": NumpyRandom,\n}\nprint(list(brown_belt.keys()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h1 style='background:black; border:3px solid; border-color: black; color:white'><center>Black Belt Baselines</center></h1>\n\n<br>\n\nThese are agents that have very nice standing on the leaderboard (near the bronze band and even above). The following agents were added to the \"Black Belt\" category:\n\n- **`DecisionTree`** &mdash; from [Decision Tree Classifier](https://www.kaggle.com/alexandersamarin/decision-tree-classifier?scriptVersionId=46415861) (v4)\n- **`Xgboost`** &mdash; from [XGBoost For Predicting Opponent's Action](https://www.kaggle.com/ollyattwood/xgboost-for-predicting-opponent-s-action) (v1).\n- **`PatternsAggressive`** &mdash; another new agent (see Patterns above), with only last 200 steps included in statistics\n- **`MemoryPatterns`** &mdash; from [Rock, Paper, Scissors with Memory Patterns](https://www.kaggle.com/yegorbiryukov/rock-paper-scissors-with-memory-patterns) (v20)\n- **`Greenberg`** &mdash; from [RPS: RoShamBo Competition - Greenberg](https://www.kaggle.com/group16/rps-roshambo-competition-greenberg)\n- **`Iocaine`** &mdash; from [RPS: RoShamBo Comp - Iocaine Powder](https://www.kaggle.com/group16/rps-roshambo-comp-iocaine-powder)\n- **`TestingPleaseIgnore`** &mdash; from [Running RPSContest bots](https://www.kaggle.com/purplepuppy/running-rpscontest-bots)\n- **`IOU2`** &mdash; from [RPSContest - IO2_fightinguuu](http://www.rpscontest.com/entry/885001)\n- (missing) **`Dllu1`** &mdash; from [RPSContest - dllu1](http://www.rpscontest.com/entry/498002)\n- (missing) **`CentrifugalBumblepuppy4`** &mdash; from [RPSContest - Centrifugal BumblePuppy 4](centrifugal_bumblepuppy_v4)\n- **`NumpyPatterns`** &mdash; like Patterns, but optimized for performance with numpy, which allowed to calculate more statistics and introduce MAB-like scoring"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"class DecisionTree:\n    K = 1\n    def construct_local_features(self, rollouts):\n        return np.concatenate(([step % k for step in rollouts['steps'] for k in (2, 3, 5)], rollouts['steps'], rollouts['actions'], rollouts['opp-actions']), axis=None)\n\n    def construct_global_features(self, rollouts):\n        features = np.zeros((6,), dtype=int)\n        features[:3] = np.mean(np.array(rollouts['actions']).reshape(-1,1) == np.arange(3), axis=0)\n        features[3:] = np.mean(np.array(rollouts['opp-actions']).reshape(-1,1) == np.arange(3), axis=0)\n        return features\n\n    def construct_features(self, short_stat_rollouts, long_stat_rollouts):\n        lf = self.construct_local_features(short_stat_rollouts)\n        gf = self.construct_global_features(long_stat_rollouts)\n        return np.concatenate([lf, gf])\n\n    def predict_opponent_move(self, train_data, test_sample):\n        classifier = DecisionTreeClassifier(random_state=42)\n        classifier.fit(train_data['x'], train_data['y'])\n        return classifier.predict(test_sample)\n\n    def update_rollouts_hist(self, A):\n        self.rollouts_hist['steps'].append(self.last_move['step'])\n        self.rollouts_hist['actions'].append(self.last_move['action'])\n        self.rollouts_hist['opp-actions'].append(A)\n\n    def warmup_strategy(self, S, A, T):\n        action = secrets.randbelow(S)\n        if T == 0:\n            self.rollouts_hist = {'steps': [], 'actions': [], 'opp-actions': []}\n            self.last_move = {'step': 0, 'action': action}\n        else:\n            self.update_rollouts_hist(A)\n            self.last_move = {'step': T, 'action': action}\n        return int(action)\n\n    def init_training_data(self, k):\n        for i in range(len(self.rollouts_hist['steps']) - k + 1):\n            short_stat_rollouts = {key: self.rollouts_hist[key][i:i+k] for key in self.rollouts_hist}\n            long_stat_rollouts = {key: self.rollouts_hist[key][:i+k] for key in self.rollouts_hist}\n            features = self.construct_features(short_stat_rollouts, long_stat_rollouts)        \n            self.data['x'].append(features)\n        self.test_sample = self.data['x'][-1].reshape(1, -1)\n        self.data['x'] = self.data['x'][:-1]\n        self.data['y'] = self.rollouts_hist['opp-actions'][k:]\n\n    def next_action(self, T, A, S):\n        k = 5\n        min_samples = 25\n        if T == 0:\n            self.data = {'x': [], 'y': []}\n        # if not enough data -> randomize\n        if T <= min_samples + k:\n            return self.warmup_strategy(S, A, T)\n        # update statistics\n        self.update_rollouts_hist(A)\n        # update training data\n        if len(self.data['x']) == 0:\n            self.init_training_data(k)\n        else:        \n            short_stat_rollouts = {key: self.rollouts_hist[key][-k:] for key in self.rollouts_hist}\n            features = self.construct_features(short_stat_rollouts, self.rollouts_hist)\n            self.data['x'].append(self.test_sample[0])\n            self.data['y'] = self.rollouts_hist['opp-actions'][k:]\n            self.test_sample = features.reshape(1, -1)\n\n        # predict opponents move and choose an action\n        next_opp_action_pred = self.predict_opponent_move(self.data, self.test_sample)\n        action = int((next_opp_action_pred + 1) % S)\n        self.last_move = {'step': T, 'action': action}\n        return action\n    \nclass Xgboost:\n    K = 0.25\n    def __init__(self):\n        self.numTurnsPredictors = 5 #number of previous turns to use as predictors\n        self.minTrainSetRows = 10 #only start predicting moves after we have enough data\n        self.myLastMove = None\n        self.mySecondLastMove = None\n        self.opponentLastMove = None\n        self.numDummies = 2 #how many dummy vars we need to represent a move\n        self.predictors = pd.DataFrame(columns=[str(x) for x in range(self.numTurnsPredictors * 2 * self.numDummies)]).astype(\"int\")\n        self.opponentsMoves = [0] * 1000\n        self.roundHistory = [None] * 1000\n        self.dummies = [[[0,0,0,0], [0,1,0,0], [1,0,0,0]], [[0,0,0,1], [0,1,0,1], [1,0,0,1]], [[0,0,1,0], [0,1,1,0], [1,0,1,0]]]\n        self.clf = XGBClassifier(n_estimators=10)\n\n    def updateFeatures(self, rounds):\n        self.predictors.loc[len(self.predictors)] = sum(rounds, [])\n\n    def fitAndPredict(self, x, y, newX):\n        self.clf.fit(x.values, y)\n        return int(self.clf.predict(np.array(newX).reshape((1,-1)))[0])\n\n    def next_action(self, T, A, S):\n        if T == 0:\n            self.myLastMove = secrets.randbelow(S)\n            return self.myLastMove\n\n        self.roundHistory[T-1] = self.dummies[self.myLastMove][A]\n        if T == 1:\n            self.myLastMove = secrets.randbelow(S)\n            return self.myLastMove\n        else:\n            self.opponentsMoves[T-2] = A\n\n            if T > self.numTurnsPredictors:\n                self.updateFeatures(self.roundHistory[:T][-self.numTurnsPredictors - 1: -1])\n\n            if len(self.predictors) > self.minTrainSetRows:\n                predictX = sum(self.roundHistory[:T][-self.numTurnsPredictors:], []) #data to predict next move\n                predictedMove = self.fitAndPredict(self.predictors, self.opponentsMoves[:T-1][(self.numTurnsPredictors-1):], predictX)\n                self.myLastMove = (predictedMove + 1) % S\n                return self.myLastMove\n            else:\n                self.myLastMove = secrets.randbelow(S)\n                return self.myLastMove\n\nclass PatternAggressive:\n    K = 10\n    def __init__(self):\n        self.Jmax = 2\n        self.J = self.Jmax - int(math.sqrt(secrets.randbelow((self.Jmax+1)**2)))\n        self.Dmin = 2\n        self.Dmax = 5\n        self.Hash = []\n        self.Map = []\n        self.MyMap = []\n        for D in range(self.Dmin,self.Dmax+1):\n            self.Hash.append([0, 0, 0])\n            self.Map.append([{}, {}, {}])\n            self.MyMap.append([{}, {}, {}])\n        self.G = 2\n        self.R = 0.4\n        self.V = 0.8\n        self.VM = 0.95\n        self.B = 0\n        self.DT = 200\n\n    def add(self, map1, hash1, A, T):\n        if hash1 not in map1:\n            map1[hash1] = {'S': []}\n        d = map1[hash1]\n        if A not in d:\n            d[A] = [T]\n        else:\n            d[A].append(T)\n        d['S'].append(T)\n\n    def rank(self, A, T):\n        return len([a for a in A if a > T - self.DT])\n\n    def match(self, map1, hash1, S, T):\n        if hash1 not in map1:\n            return\n        d = map1[hash1]\n        if self.rank(d['S'], T) >= self.G:\n            for A in range(S):\n                if A in d and (self.rank(d[A], T) >= self.rank(d['S'], T) * self.R + (1-self.R) * self.G) and secrets.randbelow(1001) < 1000 * self.V:\n                    if secrets.randbelow(1001) < 1000 * self.VM:\n                        self.B = (A+1) % S\n                    else:\n                        self.B = A % S\n                    self.J = self.Jmax - int(math.sqrt(secrets.randbelow((self.Jmax+1)**2)))\n    \n    def next_action(self, T, A, S):\n        BA = (self.B+1)%S\n        self.B = secrets.randbelow(S)\n        for D in range(self.Dmin,self.Dmax+1):\n            if T > D:\n                self.add(self.Map[D-self.Dmin][0], self.Hash[D-self.Dmin][0], A, T)\n                self.add(self.Map[D-self.Dmin][1], self.Hash[D-self.Dmin][1], A, T)\n                self.add(self.Map[D-self.Dmin][2], self.Hash[D-self.Dmin][2], A, T)\n                self.add(self.MyMap[D-self.Dmin][0], self.Hash[D-self.Dmin][0], BA, T)\n                self.add(self.MyMap[D-self.Dmin][1], self.Hash[D-self.Dmin][1], BA, T)\n                self.add(self.MyMap[D-self.Dmin][2], self.Hash[D-self.Dmin][2], BA, T)\n            if T > 0:\n                self.Hash[D-self.Dmin][0] = self.Hash[D-self.Dmin][0] // S**2 + (A + S*self.B) * S**(2*D-1)\n                self.Hash[D-self.Dmin][1] = self.Hash[D-self.Dmin][1] // S + A * S**(D-1)\n                self.Hash[D-self.Dmin][2] = self.Hash[D-self.Dmin][2] // S + self.B * S**(D-1)\n            if self.J == 0:\n                self.match(self.Map[D-self.Dmin][0], self.Hash[D-self.Dmin][0], S, T)\n                self.match(self.Map[D-self.Dmin][1], self.Hash[D-self.Dmin][1], S, T)\n                self.match(self.Map[D-self.Dmin][2], self.Hash[D-self.Dmin][2], S, T)\n            if self.J == 0:\n                self.match(self.MyMap[D-self.Dmin][0], self.Hash[D-self.Dmin][0], S, T)\n                self.match(self.MyMap[D-self.Dmin][1], self.Hash[D-self.Dmin][1], S, T)\n                self.match(self.MyMap[D-self.Dmin][2], self.Hash[D-self.Dmin][2], S, T)\n        if self.J > 0:\n            self.J -= 1\n        return self.B\n\nclass MemoryPatterns:\n    K = 3\n    def __init__(self):\n        self.current_memory = []\n        self.previous_action = {\n            \"action\": None,\n            \"action_from_pattern\": False,\n            \"pattern_group_index\": None,\n            \"pattern_index\": None\n        }\n        self.steps_to_random = random.randint(3, 5)\n        self.current_memory_max_length = 10\n        self.reward = 0\n        self.group_memory_length = self.current_memory_max_length\n        self.groups_of_memory_patterns = []\n        for i in range(5, 2, -1):\n            self.groups_of_memory_patterns.append({\n                \"memory_length\": self.group_memory_length,\n                \"memory_patterns\": []\n            })\n            self.group_memory_length -= 2\n\n    def evaluate_pattern_efficiency(self, previous_step_result):\n        pattern_group_index = self.previous_action[\"pattern_group_index\"]\n        pattern_index = self.previous_action[\"pattern_index\"]\n        pattern = self.groups_of_memory_patterns[pattern_group_index][\"memory_patterns\"][pattern_index]\n        pattern[\"reward\"] += previous_step_result\n        if pattern[\"reward\"] <= -3:\n            del self.groups_of_memory_patterns[pattern_group_index][\"memory_patterns\"][pattern_index]\n    \n    def find_action(self, group, group_index):\n        if len(self.current_memory) > group[\"memory_length\"]:\n            this_step_memory = self.current_memory[-group[\"memory_length\"]:]\n            memory_pattern, pattern_index = self.find_pattern(group[\"memory_patterns\"], this_step_memory, group[\"memory_length\"])\n            if memory_pattern != None:\n                my_action_amount = 0\n                for action in memory_pattern[\"opp_next_actions\"]:\n                    if (action[\"amount\"] > my_action_amount or\n                            (action[\"amount\"] == my_action_amount and random.random() > 0.5)):\n                        my_action_amount = action[\"amount\"]\n                        my_action = action[\"response\"]\n                return my_action, pattern_index\n        return None, None\n\n    def find_pattern(self, memory_patterns, memory, memory_length):\n        for i in range(len(memory_patterns)):\n            actions_matched = 0\n            for j in range(memory_length):\n                if memory_patterns[i][\"actions\"][j] == memory[j]:\n                    actions_matched += 1\n                else:\n                    break\n            if actions_matched == memory_length:\n                return memory_patterns[i], i\n        return None, None\n\n    def update_current_memory(self, my_action):\n        if len(self.current_memory) > self.current_memory_max_length:\n            del self.current_memory[:2]\n        self.current_memory.append(my_action)\n    \n    def update_memory_pattern(self, group, A):\n        if len(self.current_memory) > group[\"memory_length\"]:\n            previous_step_memory = self.current_memory[-group[\"memory_length\"] - 2 : -2]\n            previous_pattern, pattern_index = self.find_pattern(group[\"memory_patterns\"], previous_step_memory, group[\"memory_length\"])\n            if previous_pattern == None:\n                previous_pattern = {\n                    \"actions\": previous_step_memory.copy(),\n                    \"reward\": 0,\n                    \"opp_next_actions\": [\n                        {\"action\": 0, \"amount\": 0, \"response\": 1},\n                        {\"action\": 1, \"amount\": 0, \"response\": 2},\n                        {\"action\": 2, \"amount\": 0, \"response\": 0}\n                    ]\n                }\n                group[\"memory_patterns\"].append(previous_pattern)\n            for action in previous_pattern[\"opp_next_actions\"]:\n                if action[\"action\"] == A:\n                    action[\"amount\"] += 1\n    \n    def next_action(self, T, A, S):\n        my_action = None\n    \n        self.steps_to_random -= 1\n        if self.steps_to_random <= 0:\n            self.steps_to_random = random.randint(3, 5)\n            my_action = secrets.randbelow(S)\n            self.previous_action[\"action\"] = my_action\n            self.previous_action[\"action_from_pattern\"] = False\n            self.previous_action[\"pattern_group_index\"] = None\n            self.previous_action[\"pattern_index\"] = None\n\n        if T > 0:\n            self.current_memory.append(A)\n            previous_step_result = get_score(S, self.current_memory[-2], self.current_memory[-1])\n            self.reward += previous_step_result\n            if self.previous_action[\"action_from_pattern\"]:\n                self.evaluate_pattern_efficiency(previous_step_result)\n\n        for i in range(len(self.groups_of_memory_patterns)):\n            self.update_memory_pattern(self.groups_of_memory_patterns[i], A)\n            if my_action == None:\n                my_action, pattern_index = self.find_action(self.groups_of_memory_patterns[i], i)\n                if my_action != None:\n                    self.previous_action[\"action\"] = my_action\n                    self.previous_action[\"action_from_pattern\"] = True\n                    self.previous_action[\"pattern_group_index\"] = i\n                    self.previous_action[\"pattern_index\"] = pattern_index\n\n        if my_action == None:\n            my_action = secrets.randbelow(S)\n            self.previous_action[\"action\"] = my_action\n            self.previous_action[\"action_from_pattern\"] = False\n            self.previous_action[\"pattern_group_index\"] = None\n            self.previous_action[\"pattern_index\"] = None\n\n        self.update_current_memory(my_action)\n        return my_action\n\nclass Iocaine:\n    K = 1\n    class Stats:\n        def __init__(self):\n            self.sum = [[0, 0, 0]]\n        def add(self, move, score):\n            self.sum[-1][move] += score\n        def advance(self):\n            self.sum.append(self.sum[-1])\n        def max(self, age, default, score):\n            if age >= len(self.sum): diff = self.sum[-1]\n            else: diff = [self.sum[-1][i] - self.sum[-1 - age][i] for i in range(3)]\n            m = max(diff)\n            if m > score: return diff.index(m), m\n            return default, score\n\n    class Predictor:\n        def __init__(self):\n            self.stats = Iocaine.Stats()\n            self.lastguess = -1\n\n        def addguess(self, lastmove, guess, S):\n            if lastmove is not None:\n                diff = (lastmove - self.prediction) % S\n                self.stats.add((diff+1) % S, 1)\n                self.stats.add((diff-1) % S, -1)\n                self.stats.advance()\n            self.prediction = guess\n\n        def bestguess(self, age, best, S):\n            bestdiff = self.stats.max(age, (best[0] - self.prediction) % S, best[1])\n            return (bestdiff[0] + self.prediction) % S, bestdiff[1]\n\n    def __init__(self):\n        self.predictors = []\n        self.ages = [1000, 100, 10, 5, 2, 1]\n        self.predict_history = self.predictor((len(self.ages), 2, 3))\n        self.predict_frequency = self.predictor((len(self.ages), 2))\n        self.predict_fixed = self.predictor()\n        self.predict_random = self.predictor()\n        self.predict_meta = [Iocaine.Predictor() for a in range(len(self.ages))]\n        self.stats = [Iocaine.Stats() for i in range(2)]\n        self.histories = [[], [], []]\n\n    def recall(self, age, hist):\n        end, length = 0, 0\n        for past in range(1, min(age + 1, len(hist) - 1)):\n            if length >= len(hist) - past: break\n            for i in range(-1 - length, 0):\n                if hist[i - past] != hist[i]: break\n            else:\n                for length in range(length + 1, len(hist) - past):\n                    if hist[-past - length - 1] != hist[-length - 1]: break\n                else: length += 1\n                end = len(hist) - past\n        return end\n\n    def predictor(self, dims=None):\n        if dims: return [self.predictor(dims[1:]) for i in range(dims[0])]\n        self.predictors.append(Iocaine.Predictor())\n        return self.predictors[-1]\n\n    def next_action(self, T, A, S):\n        if T > 0:\n            self.histories[1].append(A)\n            self.histories[2].append((self.histories[0][-1], A))\n            for watch in range(2):\n                self.stats[watch].add(self.histories[watch][-1], 1)\n\n        rand = random.randrange(3)\n        self.predict_random.addguess(A, rand, S)\n        self.predict_fixed.addguess(A, 0, S)\n\n        for a, age in enumerate(self.ages):\n            best = [self.recall(age, hist) for hist in self.histories]\n            for mimic in range(2):\n                for watch, when in enumerate(best):\n                    if not when: move = rand\n                    else: move = self.histories[mimic][when]\n                    self.predict_history[a][mimic][watch].addguess(A, move, S)\n                mostfreq, score = self.stats[mimic].max(age, rand, -1)\n                self.predict_frequency[a][mimic].addguess(A, mostfreq, S)\n\n        for meta, age in enumerate(self.ages):\n            best = (-1, -1)\n            for predictor in self.predictors:\n                best = predictor.bestguess(age, best, S)\n            self.predict_meta[meta].addguess(A, best[0], S)\n\n        best = (-1, -1)\n        for meta in range(len(self.ages)):\n            best = self.predict_meta[meta].bestguess(len(self.histories[0]) , best, S) \n        self.histories[0].append(best[0])\n        return best[0]\n\nclass Greenberg:\n    K = 0.5\n    def __init__(self):\n        self.opp_moves = []\n        self.my_moves = []\n        self.act = None\n    \n    @staticmethod\n    def min_index(values):\n            return min(enumerate(values), key=itemgetter(1))[0]\n\n    @staticmethod\n    def max_index(values):\n        return max(enumerate(values), key=itemgetter(1))[0]\n\n    def find_best_prediction(self, l, T):  # l = len\n        bs = -1000\n        bp = 0\n        if self.p_random_score > bs:\n            bs = self.p_random_score\n            bp = self.p_random\n        for i in range(3):\n            for j in range(24):\n                for k in range(4):\n                    new_bs = self.p_full_score[T%50][j][k][i] - (self.p_full_score[(50+T-l)%50][j][k][i] if l else 0)\n                    if new_bs > bs:\n                        bs = new_bs\n                        bp = (self.p_full[j][k] + i) % 3\n                for k in range(2):\n                    new_bs = self.r_full_score[T%50][j][k][i] - (self.r_full_score[(50+T-l)%50][j][k][i] if l else 0)\n                    if new_bs > bs:\n                        bs = new_bs\n                        bp = (self.r_full[j][k] + i) % 3\n            for j in range(2):\n                for k in range(2):\n                    new_bs = self.p_freq_score[T%50][j][k][i] - (self.p_freq_score[(50+T-l)%50][j][k][i] if l else 0)\n                    if new_bs > bs:\n                        bs = new_bs\n                        bp = (self.p_freq[j][k] + i) % 3\n                    new_bs = self.r_freq_score[T%50][j][k][i] - (self.r_freq_score[(50+T-l)%50][j][k][i] if l else 0)\n                    if new_bs > bs:\n                        bs = new_bs\n                        bp = (self.r_freq[j][k] + i) % 3\n        return bp\n\n    def next_action(self, TT, A, S):\n        if TT > 0:\n            self.my_moves.append(self.act)\n            self.opp_moves.append(A)\n\n        wins_with = (1,2,0)      #superior\n        best_without = (2,0,1)   #inferior\n\n        lengths = (10, 20, 30, 40, 49, 0)\n        self.p_random = secrets.randbelow(S)\n\n        score_table =((0,-1,1),(1,0,-1),(-1,1,0))\n        T = len(self.opp_moves)  #so T is number of trials completed\n\n        if not self.my_moves:\n            self.opp_history = [0]\n            self.my_history = [0]\n            self.gear = [[0] for _ in range(24)]\n            self.p_random_score = 0\n            self.p_full_score = [[[[0 for i in range(3)] for k in range(4)] for j in range(24)] for l in range(50)]\n            self.r_full_score = [[[[0 for i in range(3)] for k in range(2)] for j in range(24)] for l in range(50)]\n            self.p_freq_score = [[[[0 for i in range(3)] for k in range(2)] for j in range(2)] for l in range(50)]\n            self.r_freq_score = [[[[0 for i in range(3)] for k in range(2)] for j in range(2)] for l in range(50)]\n            self.s_len = [0] * 6\n\n            self.p_full = [[0,0,0,0] for _ in range(24)]\n            self.r_full = [[0,0] for _ in range(24)]\n        else:\n            self.my_history.append(self.my_moves[-1])\n            self.opp_history.append(self.opp_moves[-1])\n            self.p_random_score += score_table[self.p_random][self.opp_history[-1]]\n            self.p_full_score[T%50] = [[[self.p_full_score[(T+49)%50][j][k][i] + score_table[(self.p_full[j][k] + i) % 3][self.opp_history[-1]] for i in range(3)] for k in range(4)] for j in range(24)]\n            self.r_full_score[T%50] = [[[self.r_full_score[(T+49)%50][j][k][i] + score_table[(self.r_full[j][k] + i) % 3][self.opp_history[-1]] for i in range(3)] for k in range(2)] for j in range(24)]\n            self.p_freq_score[T%50] = [[[self.p_freq_score[(T+49)%50][j][k][i] + score_table[(self.p_freq[j][k] + i) % 3][self.opp_history[-1]] for i in range(3)] for k in range(2)] for j in range(2)]\n            self.r_freq_score[T%50] = [[[self.r_freq_score[(T+49)%50][j][k][i] + score_table[(self.r_freq[j][k] + i) % 3][self.opp_history[-1]] for i in range(3)] for k in range(2)] for j in range(2)]\n            self.s_len = [s + score_table[p][self.opp_history[-1]] for s,p in zip(self.s_len,self.p_len)]\n\n        if not self.my_moves:\n            self.my_history_hash = [[0],[0],[0],[0]]\n            self.opp_history_hash = [[0],[0],[0],[0]]\n        else:\n            self.my_history_hash[0].append(self.my_history[-1])\n            self.opp_history_hash[0].append(self.opp_history[-1])\n            for i in range(1,4):\n                self.my_history_hash[i].append(self.my_history_hash[i-1][-1] * 3 + self.my_history[-1])\n                self.opp_history_hash[i].append(self.opp_history_hash[i-1][-1] * 3 + self.opp_history[-1])\n\n        for i in range(24):\n            self.gear[i].append((3 + self.opp_history[-1] - self.p_full[i][2]) % 3)\n            if T > 1:\n                self.gear[i][T] += 3 * self.gear[i][T-1]\n            self.gear[i][T] %= 9\n        if not self.my_moves:\n            self.freq = [[0,0,0],[0,0,0]]\n            value = [[0,0,0],[0,0,0]]\n        else:\n            self.freq[0][self.my_history[-1]] += 1\n            self.freq[1][self.opp_history[-1]] += 1\n            value = [[(1000 * (self.freq[i][2] - self.freq[i][1])) / float(T),\n                      (1000 * (self.freq[i][0] - self.freq[i][2])) / float(T),\n                      (1000 * (self.freq[i][1] - self.freq[i][0])) / float(T)] for i in range(2)]\n        self.p_freq = [[wins_with[Greenberg.max_index(self.freq[i])], wins_with[Greenberg.max_index(value[i])]] for i in range(2)]\n        self.r_freq = [[best_without[Greenberg.min_index(self.freq[i])], best_without[Greenberg.min_index(value[i])]] for i in range(2)]\n\n        f = [[[[0,0,0] for k in range(4)] for j in range(2)] for i in range(3)]\n        t = [[[0,0,0,0] for j in range(2)] for i in range(3)]\n\n        m_len = [[0 for _ in range(T)] for i in range(3)]\n\n        for i in range(T-1,0,-1):\n            m_len[0][i] = 4\n            for j in range(4):\n                if self.my_history_hash[j][i] != self.my_history_hash[j][T]:\n                    m_len[0][i] = j\n                    break\n            for j in range(4):\n                if self.opp_history_hash[j][i] != self.opp_history_hash[j][T]:\n                    m_len[1][i] = j\n                    break\n            for j in range(4):\n                if self.my_history_hash[j][i] != self.my_history_hash[j][T] or self.opp_history_hash[j][i] != self.opp_history_hash[j][T]:\n                    m_len[2][i] = j\n                    break\n\n        for i in range(T-1,0,-1):\n            for j in range(3):\n                for k in range(m_len[j][i]):\n                    f[j][0][k][self.my_history[i+1]] += 1\n                    f[j][1][k][self.opp_history[i+1]] += 1\n                    t[j][0][k] += 1\n                    t[j][1][k] += 1\n\n                    if t[j][0][k] == 1:\n                        self.p_full[j*8 + 0*4 + k][0] = wins_with[self.my_history[i+1]]\n                    if t[j][1][k] == 1:\n                        self.p_full[j*8 + 1*4 + k][0] = wins_with[self.opp_history[i+1]]\n                    if t[j][0][k] == 3:\n                        self.p_full[j*8 + 0*4 + k][1] = wins_with[Greenberg.max_index(f[j][0][k])]\n                        self.r_full[j*8 + 0*4 + k][0] = best_without[Greenberg.min_index(f[j][0][k])]\n                    if t[j][1][k] == 3:\n                        self.p_full[j*8 + 1*4 + k][1] = wins_with[Greenberg.max_index(f[j][1][k])]\n                        self.r_full[j*8 + 1*4 + k][0] = best_without[Greenberg.min_index(f[j][1][k])]\n\n        for j in range(3):\n            for k in range(4):\n                self.p_full[j*8 + 0*4 + k][2] = wins_with[Greenberg.max_index(f[j][0][k])]\n                self.r_full[j*8 + 0*4 + k][1] = best_without[Greenberg.min_index(f[j][0][k])]\n\n                self.p_full[j*8 + 1*4 + k][2] = wins_with[Greenberg.max_index(f[j][1][k])]\n                self.r_full[j*8 + 1*4 + k][1] = best_without[Greenberg.min_index(f[j][1][k])]\n\n        for j in range(24):\n            gear_freq = [0] * 9\n            for i in range(T-1,0,-1):\n                if self.gear[j][i] == self.gear[j][T]:\n                    gear_freq[self.gear[j][i+1]] += 1\n            self.p_full[j][3] = (self.p_full[j][1] + Greenberg.max_index(gear_freq)) % 3\n\n        self.p_len = [self.find_best_prediction(l, T) for l in lengths]\n        self.act = self.p_len[Greenberg.max_index(self.s_len)]\n        return self.act\n\nclass TestingPleaseIgnore:\n    K = 20\n    def counter_prob(self, probs):\n        weighted_list = []\n        for h in self.rps:\n            weighted = 0\n            for p in probs.keys():\n                points = self.score[h + p]\n                prob = probs[p]\n                weighted += points * prob\n            weighted_list.append((h, weighted))\n        return max(weighted_list, key=itemgetter(1))[0]\n\n    def __init__(self):\n        self.score  = {'RR': 0, 'PP': 0, 'SS': 0, \\\n                  'PR': 1, 'RS': 1, 'SP': 1, \\\n                  'RP': -1, 'SR': -1, 'PS': -1,}\n        self.cscore = {'RR': 'r', 'PP': 'r', 'SS': 'r', \\\n                  'PR': 'b', 'RS': 'b', 'SP': 'b', \\\n                  'RP': 'c', 'SR': 'c', 'PS': 'c',}\n        self.beat = {'P': 'S', 'S': 'R', 'R': 'P'}\n        self.cede = {'P': 'R', 'S': 'P', 'R': 'S'}\n        self.rps = ['R', 'P', 'S']\n        self.wlt = {1: 0, -1: 1, 0: 2}\n\n        self.played_probs = collections.defaultdict(lambda: 1)\n        self.dna_probs = [\n            collections.defaultdict(lambda: collections.defaultdict(lambda: 1)) for i in range(18)\n        ]\n        self.wlt_probs = [collections.defaultdict(lambda: 1) for i in range(9)]\n        self.answers = [{'c': 1, 'b': 1, 'r': 1} for i in range(12)]\n        self.patterndict = [collections.defaultdict(str) for i in range(6)]\n        self.consec_strat_usage = [[0] * 6, [0] * 6,\n                                   [0] * 6]  #consecutive strategy usage\n        self.consec_strat_candy = [[], [], []]  #consecutive strategy candidates\n        self.histories = [\"\", \"\", \"\"]\n        self.dna = [\"\" for i in range(12)]\n        self.sc = 0\n        self.strats = [[] for i in range(3)]\n        \n    def next_action(self, T, A, S):\n        if T == 0:\n            self.B = random.choice(self.rps)\n            return {'R': 0, 'P': 1, 'S': 2}[self.B]\n        prev_sc = self.sc\n\n        self.sc = self.score[self.B + 'RPS'[A]]\n        for j in range(3):\n            prev_strats = self.strats[j][:]\n            for i, c in enumerate(self.consec_strat_candy[j]):\n                if c == 'RPS'[A]:\n                    self.consec_strat_usage[j][i] += 1\n                else:\n                    self.consec_strat_usage[j][i] = 0\n            m = max(self.consec_strat_usage[j])\n            self.strats[j] = [\n                i for i, c in enumerate(self.consec_strat_candy[j])\n                if self.consec_strat_usage[j][i] == m\n            ]\n\n            for s1 in prev_strats:\n                for s2 in self.strats[j]:\n                    self.wlt_probs[j * 3 + self.wlt[prev_sc]][chr(s1) + chr(s2)] += 1\n\n            if self.dna[2 * j + 0] and self.dna[2 * j + 1]:\n                self.answers[2 * j + 0][self.cscore['RPS'[A] + self.dna[2 * j + 0]]] += 1\n                self.answers[2 * j + 1][self.cscore['RPS'[A] + self.dna[2 * j + 1]]] += 1\n            if self.dna[2 * j + 6] and self.dna[2 * j + 7]:\n                self.answers[2 * j + 6][self.cscore['RPS'[A] + self.dna[2 * j + 6]]] += 1\n                self.answers[2 * j + 7][self.cscore['RPS'[A] + self.dna[2 * j + 7]]] += 1\n\n            for length in range(min(10, len(self.histories[j])), 0, -2):\n                pattern = self.patterndict[2 * j][self.histories[j][-length:]]\n                if pattern:\n                    for length2 in range(min(10, len(pattern)), 0, -2):\n                        self.patterndict[2 * j + 1][pattern[-length2:]] += self.B + 'RPS'[A]\n                self.patterndict[2 * j][self.histories[j][-length:]] += self.B + 'RPS'[A]\n        self.played_probs['RPS'[A]] += 1\n        self.dna_probs[0][self.dna[0]]['RPS'[A]] += 1\n        self.dna_probs[1][self.dna[1]]['RPS'[A]] += 1\n        self.dna_probs[2][self.dna[1] + self.dna[0]]['RPS'[A]] += 1\n        self.dna_probs[9][self.dna[6]]['RPS'[A]] += 1\n        self.dna_probs[10][self.dna[6]]['RPS'[A]] += 1\n        self.dna_probs[11][self.dna[7] + self.dna[6]]['RPS'[A]] += 1\n\n        self.histories[0] += self.B + 'RPS'[A]\n        self.histories[1] += 'RPS'[A]\n        self.histories[2] += self.B\n\n        self.dna = [\"\" for i in range(12)]\n        for j in range(3):\n            for length in range(min(10, len(self.histories[j])), 0, -2):\n                pattern = self.patterndict[2 * j][self.histories[j][-length:]]\n                if pattern != \"\":\n                    self.dna[2 * j + 1] = pattern[-2]\n                    self.dna[2 * j + 0] = pattern[-1]\n                    for length2 in range(min(10, len(pattern)), 0, -2):\n                        pattern2 = self.patterndict[2 * j + 1][pattern[-length2:]]\n                        if pattern2 != \"\":\n                            self.dna[2 * j + 7] = pattern2[-2]\n                            self.dna[2 * j + 6] = pattern2[-1]\n                            break\n                    break\n\n        probs = {}\n        for hand in self.rps:\n            probs[hand] = self.played_probs[hand]\n\n        for j in range(3):\n            if self.dna[j * 2] and self.dna[j * 2 + 1]:\n                for hand in self.rps:\n                    probs[hand] *= self.dna_probs[j*3+0][self.dna[j*2+0]][hand] * \\\n                                   self.dna_probs[j*3+1][self.dna[j*2+1]][hand] * \\\n                          self.dna_probs[j*3+2][self.dna[j*2+1]+self.dna[j*2+0]][hand]\n                    probs[hand] *= self.answers[j*2+0][self.cscore[hand+self.dna[j*2+0]]] * \\\n                                   self.answers[j*2+1][self.cscore[hand+self.dna[j*2+1]]]\n                self.consec_strat_candy[j] = [self.dna[j*2+0], self.beat[self.dna[j*2+0]], self.cede[self.dna[j*2+0]],\\\n                                         self.dna[j*2+1], self.beat[self.dna[j*2+1]], self.cede[self.dna[j*2+1]]]\n                strats_for_hand = {'R': [], 'P': [], 'S': []}\n                for i, c in enumerate(self.consec_strat_candy[j]):\n                    strats_for_hand[c].append(i)\n                pr = self.wlt_probs[self.wlt[self.sc] + 3 * j]\n                for hand in self.rps:\n                    for s1 in self.strats[j]:\n                        for s2 in strats_for_hand[hand]:\n                            probs[hand] *= pr[chr(s1) + chr(s2)]\n            else:\n                self.consec_strat_candy[j] = []\n        for j in range(3):\n            if self.dna[j * 2 + 6] and self.dna[j * 2 + 7]:\n                for hand in self.rps:\n                    probs[hand] *= self.dna_probs[j*3+9][self.dna[j*2+6]][hand] * \\\n                                   self.dna_probs[j*3+10][self.dna[j*2+7]][hand] * \\\n                          self.dna_probs[j*3+11][self.dna[j*2+7]+self.dna[j*2+6]][hand]\n                    probs[hand] *= self.answers[j*2+6][self.cscore[hand+self.dna[j*2+6]]] * \\\n                                   self.answers[j*2+7][self.cscore[hand+self.dna[j*2+7]]]\n\n        self.B = self.counter_prob(probs)\n        return {'R': 0, 'P': 1, 'S': 2}[self.B]\n    \nclass IOU2:\n    K = 20\n    def __init__(self):\n        self.num_predictor = 27\n        self.len_rfind = [20]\n        self.limit = [10,20,60]\n        self.beat = { \"R\":\"P\" , \"P\":\"S\", \"S\":\"R\"}\n        self.not_lose = { \"R\":\"PPR\" , \"P\":\"SSP\" , \"S\":\"RRS\" } #50-50 chance\n        self.my_his   =\"\"\n        self.your_his =\"\"\n        self.both_his =\"\"\n        self.list_predictor = [\"\"]*self.num_predictor\n        self.length = 0\n        self.temp1 = { \"PP\":\"1\" , \"PR\":\"2\" , \"PS\":\"3\",\n                      \"RP\":\"4\" , \"RR\":\"5\", \"RS\":\"6\",\n                      \"SP\":\"7\" , \"SR\":\"8\", \"SS\":\"9\"}\n        self.temp2 = { \"1\":\"PP\",\"2\":\"PR\",\"3\":\"PS\",\n                        \"4\":\"RP\",\"5\":\"RR\",\"6\":\"RS\",\n                        \"7\":\"SP\",\"8\":\"SR\",\"9\":\"SS\"} \n        self.who_win = { \"PP\": 0, \"PR\":1 , \"PS\":-1,\n                        \"RP\": -1,\"RR\":0, \"RS\":1,\n                        \"SP\": 1, \"SR\":-1, \"SS\":0}\n        self.score_predictor = [0]*self.num_predictor\n        self.output = random.choice(\"RPS\")\n        self.predictors = [self.output]*self.num_predictor\n\n    def next_action(self, T, A, S):\n        to_char = [\"R\", \"P\", \"S\"]\n        from_char = {\"R\": 0, \"P\": 1, \"S\": 2}\n        if T == 0:\n            return from_char[self.output]\n        input = to_char[A]\n\n        if len(self.list_predictor[0])<5:\n            front =0\n        else:\n            front =1\n        for i in range (self.num_predictor):\n            if self.predictors[i]==input:\n                result =\"1\"\n            else:\n                result =\"0\"\n            self.list_predictor[i] = self.list_predictor[i][front:5]+result #only 5 rounds before\n        #history matching 1-6\n        self.my_his += self.output\n        self.your_his += input\n        self.both_his += self.temp1[input+self.output]\n        self.length +=1\n        for i in range(1):\n            len_size = min(self.length,self.len_rfind[i])\n            j=len_size\n            #self.both_his\n            while j>=1 and not self.both_his[self.length-j:self.length] in self.both_his[0:self.length-1]:\n                j-=1\n            if j>=1:\n                k = self.both_his.rfind(self.both_his[self.length-j:self.length],0,self.length-1)\n                self.predictors[0+6*i] = self.your_his[j+k]\n                self.predictors[1+6*i] = self.beat[self.my_his[j+k]]\n            else:\n                self.predictors[0+6*i] = random.choice(\"RPS\")\n                self.predictors[1+6*i] = random.choice(\"RPS\")\n            j=len_size\n            #self.your_his\n            while j>=1 and not self.your_his[self.length-j:self.length] in self.your_his[0:self.length-1]:\n                j-=1\n            if j>=1:\n                k = self.your_his.rfind(self.your_his[self.length-j:self.length],0,self.length-1)\n                self.predictors[2+6*i] = self.your_his[j+k]\n                self.predictors[3+6*i] = self.beat[self.my_his[j+k]]\n            else:\n                self.predictors[2+6*i] = random.choice(\"RPS\")\n                self.predictors[3+6*i] = random.choice(\"RPS\")\n            j=len_size\n            #self.my_his\n            while j>=1 and not self.my_his[self.length-j:self.length] in self.my_his[0:self.length-1]:\n                j-=1\n            if j>=1:\n                k = self.my_his.rfind(self.my_his[self.length-j:self.length],0,self.length-1)\n                self.predictors[4+6*i] = self.your_his[j+k]\n                self.predictors[5+6*i] = self.beat[self.my_his[j+k]]\n            else:\n                self.predictors[4+6*i] = random.choice(\"RPS\")\n                self.predictors[5+6*i] = random.choice(\"RPS\")\n\n        for i in range(3):\n            temp =\"\"\n            search = self.temp1[(self.output+input)] #last round\n            for start in range(2, min(self.limit[i],self.length) ):\n                if search == self.both_his[self.length-start]:\n                    temp+=self.both_his[self.length-start+1]\n            if(temp==\"\"):\n                self.predictors[6+i] = random.choice(\"RPS\")\n            else:\n                collectR = {\"P\":0,\"R\":0,\"S\":0} #take win/lose from opponent into account\n                for sdf in temp:\n                    next_move = self.temp2[sdf]\n                    if(self.who_win[next_move]==-1):\n                        collectR[self.temp2[sdf][1]]+=3\n                    elif(self.who_win[next_move]==0):\n                        collectR[self.temp2[sdf][1]]+=1\n                    elif(self.who_win[next_move]==1):\n                        collectR[self.beat[self.temp2[sdf][0]]]+=1\n                max1 = -1\n                p1 =\"\"\n                for key in collectR:\n                    if(collectR[key]>max1):\n                        max1 = collectR[key]\n                        p1 += key\n                self.predictors[6+i] = random.choice(p1)\n        for i in range(9,27):\n            self.predictors[i] = self.beat[self.beat[self.predictors[i-9]]]\n        len_his = len(self.list_predictor[0])\n        for i in range(self.num_predictor):\n            sum = 0\n            for j in range(len_his):\n                if self.list_predictor[i][j]==\"1\":\n                    sum+=(j+1)*(j+1)\n                else:\n                    sum-=(j+1)*(j+1)\n            self.score_predictor[i] = sum\n        max_score = max(self.score_predictor)\n        if max_score>0:\n            predict = self.predictors[self.score_predictor.index(max_score)]\n        else:\n            predict = random.choice(self.your_his)\n        self.output = random.choice(self.not_lose[predict])\n        return from_char[self.output]\n\nclass NumpyPatterns:\n    K = 20\n    def __init__(self):\n        self.B = 0\n        # Jitter - steps before next non-random move\n        self.Jmax = 2\n        self.J2 = (self.Jmax+1)**2\n        self.J = self.Jmax - int(math.sqrt(secrets.randbelow(self.J2)))\n        # Depth - number of previous steps taken into consideration\n        self.Dmin = 1\n        self.Dmax = 3\n        self.DL = self.Dmax-self.Dmin+1\n        self.HL = 3\n        self.HText = ['Opp',  'Me', 'Score']\n        self.Depth = np.arange(self.DL)\n        self.Hash = np.zeros((self.HL, self.DL), dtype=int)\n        self.G = 2\n        self.R = 0.4\n        self.RG = (1-self.R) * self.G\n        self.Threshold = 0.4\n        \n    def split_idx(self, idx):\n        d = idx % self.DL\n        idx //= self.DL\n        h2 = idx % self.HL\n        idx //= self.HL\n        h1 = idx % self.HL\n        idx //= self.HL\n        return d, h1, h2, idx\n    \n    def next_action(self, T, A, S):\n        B, HL, DL, Dmin, Dmax = self.B, self.HL, self.DL, self.Dmin, self.Dmax\n        SD = S**self.DL\n        if T == 0:\n            self.Map = np.zeros((S, SD**2, HL, HL, DL))\n            self.SList = np.arange(S)[:,None,None,None]\n            self.Predicts = np.full((HL, HL, DL), S, dtype=int)\n            self.Attempts = np.zeros((HL, HL, DL), dtype=int)\n            self.Scores = np.zeros((S, HL, HL, DL))\n            self.OrgID = np.ogrid[:S, :HL, :HL, :DL]\n            self.Hash2 = self.Hash[None,:] + SD*self.Hash[:,None]\n        else:\n            C = get_score(S, A, B) + 1\n            ABC = np.array([A, B, C])[:,None]\n            Depth, Hash, Hash2, Map, SList, OrgID, Predicts, Attempts, Scores = self.Depth, self.Hash, self.Hash2, self.Map, self.SList, self.OrgID, self.Predicts, self.Attempts, self.Scores\n            # Update Moves Map by previous move and previous Hash\n            Map *= 0.995\n            Map[OrgID[0], Hash2, OrgID[1], OrgID[2], OrgID[3]] += (T > Depth + Dmin) * (SList == A)\n            # Update Hash by previous move\n            Hash[:] //= S\n            Hash[:] += ABC[:HL] * S**Depth\n            Hash2[:] = Hash[None,:] + SD*Hash[:,None]\n            \n            # Update prediction scores by previous move\n            PB = Predicts < S\n            Attempts[:] = Attempts + PB\n            Scores[:] += PB * get_score(S, Predicts + SList, A)\n            #print(T, Scores.T[0])\n            # Update prediction scores by previous move\n            PR = Map[OrgID[0], Hash2, OrgID[1], OrgID[2], OrgID[3]]\n            Sum = np.sum(PR, axis=0)\n            Predicts[:] = (np.max((Sum >= self.G) * (PR >= Sum * self.R + self.RG) * (SList + 1), axis=0) - 1) % (S + 1)\n\n        self.B = np.random.choice(S)\n        if self.J > 0:\n            self.J -= 1\n        else:\n            sc = np.where(self.Predicts < S, self.Scores / (self.Attempts + 2), 0).ravel()\n            idx = np.argmax(sc)\n            if sc[idx] > self.Threshold:\n                Raw = self.Predicts.ravel()\n                L = len(Raw)\n                self.B = (Raw[idx % L] + idx // L) % S\n                self.J = self.Jmax - int(math.sqrt(secrets.randbelow(self.J2)))\n                #parts = self.split_idx(idx)\n                #print(T, f'{parts[0]+self.Dmin}: {self.HText[parts[1]]}-{self.HText[parts[2]]}+{parts[3]}', self.Scores[:, parts[1], parts[2], parts[0]], self.B)\n        return self.B\n\nblack_belt = {\n    \"DecisionTree\": DecisionTree,\n    \"Xgboost\": Xgboost,\n    \"PatternAggressive\": PatternAggressive,\n    \"MemoryPatterns\": MemoryPatterns,\n    \"Iocaine\": Iocaine,\n    \"Greenberg\": Greenberg,\n    \"TestingPleaseIgnore\": TestingPleaseIgnore,\n    \"IOU2\": IOU2,\n    \"NumpyPatterns\": NumpyPatterns,\n}\nprint(list(black_belt.keys()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Submission"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"%%writefile submission.py\n\nimport secrets\nimport math\nimport numpy as np\n\ndef get_score(S, A1, A2):\n    return (S + A1 - A2 + 1) % S - 1\n\nclass Submission:\n    K = 10\n    def __init__(self, verbose=False):\n        self.B = 0\n        # Jitter - steps before next non-random move\n        self.Jmax = 2\n        self.J2 = (self.Jmax+1)**2\n        self.J = int(math.sqrt(secrets.randbelow(self.J2)))\n        # Depth - number of previous steps taken into consideration\n        self.Dmin = 2\n        self.Dmax = 6\n        self.DL = self.Dmax-self.Dmin+1\n        self.HL = 2\n        self.HText = ['Opp',  'Me', 'Score']\n        self.Depth = np.arange(self.DL)\n        self.Hash = np.zeros((self.HL, self.DL), dtype=int)\n        self.G = 2\n        self.R = 0.4\n        self.RG = (1-self.R) * self.G\n        self.Threshold = 0.2\n        self.verbose = verbose\n        \n    def split_idx(self, idx):\n        d = idx % self.DL\n        idx //= self.DL\n        h2 = idx % self.HL\n        idx //= self.HL\n        h1 = idx % self.HL\n        idx //= self.HL\n        return d, h1, h2, idx\n    \n    def next_action(self, T, A, S):\n        B, HL, DL, Dmin, Dmax = self.B, self.HL, self.DL, self.Dmin, self.Dmax\n        SD = S**self.DL\n        PR = None\n        if T == 0:\n            self.Map = np.zeros((S, SD**2, HL, HL, DL))\n            self.SList = np.arange(S)[:,None,None,None]\n            self.Predicts = np.full((HL, HL, DL), S, dtype=int)\n            self.Attempts = np.zeros((HL, HL, DL), dtype=int)\n            self.Scores = np.zeros((S, HL, HL, DL))\n            self.OrgID = np.ogrid[:S, :HL, :HL, :DL]\n            self.Hash2 = self.Hash[None,:] + SD*self.Hash[:,None]\n        else:\n            C = get_score(S, A, B) + 1\n            if self.verbose: print(T, f'{B}-{A} {1-C}')\n            ABC = np.array([A, B, C])[:,None]\n            Depth, Hash, Hash2, Map, SList, OrgID, Predicts, Attempts, Scores = self.Depth, self.Hash, self.Hash2, self.Map, self.SList, self.OrgID, self.Predicts, self.Attempts, self.Scores\n            # Update Moves Map by previous move and previous Hash\n            Map *= 0.995\n            Map[OrgID[0], Hash2, OrgID[1], OrgID[2], OrgID[3]] += (T > Depth + Dmin) * (SList == A)\n            # Update Hash by previous move\n            Hash[:] //= S\n            Hash[:] += ABC[:HL] * S**Depth\n            Hash2[:] = Hash[None,:] + SD*Hash[:,None]\n            \n            # Update prediction scores by previous move\n            PB = Predicts < S\n            Attempts[:] = Attempts + PB\n            Scores[:] += PB * get_score(S, Predicts + SList, A)\n            #print(T, Scores.T[0])\n            # Update prediction scores by previous move\n            PR = Map[OrgID[0], Hash2, OrgID[1], OrgID[2], OrgID[3]]\n            Sum = np.sum(PR, axis=0)\n            Predicts[:] = (np.max((Sum >= self.G) * (PR >= Sum * self.R + self.RG) * (SList + 1), axis=0) - 1) % (S + 1)\n\n        self.B = np.random.choice(S)\n        if self.J > 0:\n            self.J -= 1\n        else:\n            sc = np.where(self.Predicts < S, self.Scores / (self.Attempts + 5), 0).ravel()\n            idx = np.argmax(sc)\n            if sc[idx] > self.Threshold:\n                self.Scores.ravel()[idx] -= 1/3\n                Raw = self.Predicts.ravel()\n                L = len(Raw)\n                p = None\n                s = 0\n                if PR is not None:\n                    p = PR.ravel().reshape((3,-1))[:, idx % L]\n                    s = np.sum(p)\n                if s > 0 and np.random.choice(3) > 3:\n                    p /= s\n                    self.B = (np.random.choice(S, p=p) + idx // L) % S\n                    parts = self.split_idx(idx)\n                    if self.verbose: print(T, f'Weighted {parts[0]+self.Dmin}: {self.HText[parts[1]]}-{self.HText[parts[2]]}+{parts[3]}', p, self.B)\n                else:\n                    self.B = (Raw[idx % L] + idx // L) % S\n                    parts = self.split_idx(idx)\n                    if self.verbose: print(T, f'Direct {parts[0]+self.Dmin}: {self.HText[parts[1]]}-{self.HText[parts[2]]}+{parts[3]}', self.Scores.ravel()[idx], self.B)\n                self.J = int(math.sqrt(secrets.randbelow(self.J2)))\n        return self.B\n\nsubmission = Submission(verbose=True)\n\ndef agent(observation, configuration):\n    T = observation.step\n    A = observation.lastOpponentAction if T > 0 else None\n    S = configuration.signs\n    try:\n        return int(submission.next_action(T, A, S))\n    except Exception as e:\n        print(T, f'Failed', e)\n        return int(np.random.choice(S))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%run submission.py","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Evaluation\n\nThis is an example of how you can evaluate your own model using the baseline agents in this Dojo. In this example, we will evaluate `Rock` agent against the other ones.\n\nThe evaluation code uses **multi processing** (python's built-in), as in [RPS - Multiprocessing Agent Comparisons](https://www.kaggle.com/booooooow/rps-multiprocessing-agent-comparisons) notebook.\n\n### Reasoning:\n\nkaggle_environment is too generic and allows for execution of different games with different rules in a similar fascion. It does execution timeout checks and other validations, which makes it really slow when it comes to just testing your agent.\n\nSo I created minimalistic emulator that implements basic RPS rules, with only performance in mind"},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"class Environment:\n    @staticmethod\n    def run(agents):\n        actions = None, None\n        reward = 0\n        for step in range(1000):\n            actions = agents[0].next_action(step, actions[1], 3), agents[1].next_action(step, actions[0], 3)\n            reward += (3 + actions[0] - actions[1] + 1) % 3 - 1\n        return (0 if -reward < 20 else -1) if reward < 20 else 1\n    \n    @staticmethod\n    def evaluate(args):\n        Agent1, Agent2, N = args\n        start = time.time()\n        won, lost, tie = 0, 0, 0\n        N = int(N * min(Agent1.K,Agent2.K))\n        for it in range(N):\n            score = Environment.run([Agent1(), Agent2()])\n            if score > 0: won += 1\n            elif score < 0: lost += 1\n            else: tie += 1\n        elapsed = time.time() - start\n        if N < 1:\n            return Agent1, Agent2, 0, 0, 0, 0, elapsed\n        return Agent1, Agent2, N, 100*won/N, 100*lost/N, 100*tie/N, elapsed\n\n    @staticmethod\n    def evaluate_dojo(agent, all_agents=None, N=1):\n        if all_agents is None:\n            all_agents = {\n                **white_belt,\n                **blue_belt,\n                **brown_belt,\n                **black_belt,\n            }\n        all_agent_names = list(all_agents.keys())\n        L = len(all_agents)\n\n        df = pd.DataFrame(columns=['games', '% wins', '% loses', '% ties', 'duration'], index=all_agent_names)\n\n        settings = [(agent, all_agents[all_agent_names[a]], N) for a in range(L)]\n\n        # For debug purposes you can comment multiprocessing and uncomment simple loop\n        for Agent1, Agent2, N, won, lost, tie, elapsed in tqdm(Pool().imap_unordered(Environment.evaluate, settings), total=len(settings)):\n        # for x in tqdm(settings, total=len(settings)):\n        #    Agent1, Agent2, N, won, lost, tie, elapsed = Environment.evaluate(x)\n            df.loc[Agent2.__name__] = [N, won, lost, tie, elapsed]\n\n        return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Environment.evaluate_dojo(Submission, N=20)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Save Submission"},{"metadata":{"trusted":true},"cell_type":"code","source":"from kaggle_environments import make\nfrom kaggle_environments.envs.rps.agents import agents\nimport numpy as np\nimport pandas as pd\nfrom tqdm.auto import tqdm\n\nall_agents = list(agents.values())\nall_agent_names = list(k[:8] for k in agents.keys())\nenv = make(\"rps\", configuration={\"episodeSteps\": 1000})\nL = len(all_agents)\nresults = np.zeros((L,), dtype=int)\nN = 10\nfor it in tqdm(range(N), total=N):\n    for a in range(L):\n        env.run([\"submission.py\", all_agents[a]])\n        rewards = env.toJSON()['rewards']\n        results[a] += -1 if rewards[0] is None else int(rewards[0] >= 20) - int(rewards[0] <= -20)\n        # env.render(mode=\"ipython\", width=800, height=800)\npd.DataFrame(100*results/N, index=all_agent_names, columns=[\"% wins\"])","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}