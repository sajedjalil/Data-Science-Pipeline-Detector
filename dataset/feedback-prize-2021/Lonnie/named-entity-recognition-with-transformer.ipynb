{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Named Entity Recognition with Transformer\nIn this notebook, I will build a Named Entity Recognition Model using Transformer to evaluate student writing using dataset for Kaggle Competition [Feedback Prize - Evaluating Student Writing](https://www.kaggle.com/c/feedback-prize-2021). \nI also find a way to train the Model using data with different sequence length by converting this dataset to [TextLineDataset](https://www.tensorflow.org/api_docs/python/tf/data/TextLineDataset) and convert with [padded_batch](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#padded_batch) function.\n## Import Packages","metadata":{}},{"cell_type":"code","source":"import json\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\nimport time\nimport tensorflow as tf\nimport seaborn as sns\nfrom tensorflow import keras\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import f1_score, classification_report\nfrom collections import defaultdict\nfrom tensorflow.keras import layers\nfrom collections import defaultdict","metadata":{"execution":{"iopub.status.busy":"2022-01-10T13:24:47.116508Z","iopub.execute_input":"2022-01-10T13:24:47.11677Z","iopub.status.idle":"2022-01-10T13:24:47.122028Z","shell.execute_reply.started":"2022-01-10T13:24:47.116744Z","shell.execute_reply":"2022-01-10T13:24:47.121368Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Utilities","metadata":{}},{"cell_type":"markdown","source":"## Common Parameters","metadata":{}},{"cell_type":"code","source":"vocab_size = 10000 # Vocabulary size\nsequence_length = 1024 # Sequence Length\nbatch_size = 128 # Batch size\nunk_token = \"<UNK>\" # Unknownd token\npadding_token = \"<PAD>\"\nnone_class = \"O\"\nvectorizer_path = \"vectorizer.json\"\n# Use output dataset for inference\noutput_dataset_path = \"../input/name-entity-with-transformer-output/\"\nmodel_path = \"model.h5\"\nembed_size = 128\nhidden_size = 64\nmodes = [\"training\", \"inference\"] # There is training and inference mode\nmode = modes[0]\nepochs = 30","metadata":{"execution":{"iopub.status.busy":"2022-01-10T13:24:50.332796Z","iopub.execute_input":"2022-01-10T13:24:50.333081Z","iopub.status.idle":"2022-01-10T13:24:50.338654Z","shell.execute_reply.started":"2022-01-10T13:24:50.333031Z","shell.execute_reply":"2022-01-10T13:24:50.337959Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Import Datasets","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv(\"../input/feedback-prize-2021/train.csv\")\ntrain.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-10T13:24:53.004813Z","iopub.execute_input":"2022-01-10T13:24:53.005378Z","iopub.status.idle":"2022-01-10T13:24:53.79765Z","shell.execute_reply.started":"2022-01-10T13:24:53.005338Z","shell.execute_reply":"2022-01-10T13:24:53.796988Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission = pd.read_csv(\"../input/feedback-prize-2021/sample_submission.csv\")\nsubmission.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-10T13:24:57.20652Z","iopub.execute_input":"2022-01-10T13:24:57.206771Z","iopub.status.idle":"2022-01-10T13:24:57.220817Z","shell.execute_reply.started":"2022-01-10T13:24:57.206744Z","shell.execute_reply":"2022-01-10T13:24:57.220114Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## EDA & Preprocessing","metadata":{}},{"cell_type":"markdown","source":"### Add File Path to train and submission Files","metadata":{}},{"cell_type":"code","source":"train[\"file_path\"] = train[\"id\"].apply(lambda item: \"../input/feedback-prize-2021/train/\" + item + \".txt\")\ntrain.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-10T13:24:59.8725Z","iopub.execute_input":"2022-01-10T13:24:59.87295Z","iopub.status.idle":"2022-01-10T13:24:59.950081Z","shell.execute_reply.started":"2022-01-10T13:24:59.872912Z","shell.execute_reply":"2022-01-10T13:24:59.949246Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission[\"file_path\"] = submission[\"id\"].apply(lambda item: \"../input/feedback-prize-2021/test/\" + item + \".txt\")\nsubmission.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-10T13:25:04.8174Z","iopub.execute_input":"2022-01-10T13:25:04.817902Z","iopub.status.idle":"2022-01-10T13:25:04.828382Z","shell.execute_reply.started":"2022-01-10T13:25:04.817867Z","shell.execute_reply":"2022-01-10T13:25:04.827738Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Distribution of Labels","metadata":{}},{"cell_type":"code","source":"train[\"discourse_type\"].value_counts().plot(kind=\"bar\")","metadata":{"execution":{"iopub.status.busy":"2022-01-10T13:25:07.446704Z","iopub.execute_input":"2022-01-10T13:25:07.447386Z","iopub.status.idle":"2022-01-10T13:25:07.676624Z","shell.execute_reply.started":"2022-01-10T13:25:07.447347Z","shell.execute_reply":"2022-01-10T13:25:07.675936Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"discourse_types = list(train[\"discourse_type\"].value_counts().index)","metadata":{"execution":{"iopub.status.busy":"2022-01-10T13:25:09.980985Z","iopub.execute_input":"2022-01-10T13:25:09.981546Z","iopub.status.idle":"2022-01-10T13:25:10.003581Z","shell.execute_reply.started":"2022-01-10T13:25:09.981508Z","shell.execute_reply":"2022-01-10T13:25:10.00266Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_tags = [padding_token]\nfor discourse_type in discourse_types:\n    all_tags.append(\"B-\" + discourse_type)\nfor discourse_type in discourse_types:\n    all_tags.append(\"I-\" + discourse_type)\nall_tags.append(none_class)\npad_index = all_tags.index(padding_token)\nnone_index = all_tags.index(none_class)\ntag_index = dict([(tag, index) for (index, tag) in enumerate(all_tags)])\nindex_tag = dict([(tag_index[tag], tag.replace(\"B-\", \"\").replace(\"I-\", \"\")) for tag in tag_index])\nprint(all_tags)\nprint(tag_index)\nprint(index_tag)","metadata":{"execution":{"iopub.status.busy":"2022-01-10T13:25:12.399879Z","iopub.execute_input":"2022-01-10T13:25:12.400183Z","iopub.status.idle":"2022-01-10T13:25:12.408833Z","shell.execute_reply.started":"2022-01-10T13:25:12.400151Z","shell.execute_reply":"2022-01-10T13:25:12.407941Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Number of Unique files","metadata":{}},{"cell_type":"code","source":"len(train[\"id\"].unique())","metadata":{"execution":{"iopub.status.busy":"2022-01-10T13:25:15.156386Z","iopub.execute_input":"2022-01-10T13:25:15.156925Z","iopub.status.idle":"2022-01-10T13:25:15.174948Z","shell.execute_reply.started":"2022-01-10T13:25:15.156888Z","shell.execute_reply":"2022-01-10T13:25:15.174127Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Tokenization","metadata":{}},{"cell_type":"code","source":"def tokenize(content):\n    tokens = content.lower().split()\n    return tokens\n    \ndef calc_word_indices(full_text, discourse_start, discourse_end):\n    start_index = len(full_text[:discourse_start].split())\n    token_len = len(full_text[discourse_start:discourse_end].split())\n    output = list(range(start_index, start_index + token_len))\n    if output[-1] >= len(full_text.split()):\n        output = list(range(start_index, start_index + token_len-1))\n    return output\ndef get_range(item):\n    locations = [int(location) for location in item[\"predictionstring\"].split(\" \")]\n    return (locations[0], locations[-1])\ndef add_annotation(all_data, start_index, end_index, discourse_type):\n    for j in range(start_index, end_index): \n        if j == start_index:\n            all_data[-1][1][j] = tag_index[\"B-\" + discourse_type]\n        else:\n            all_data[-1][1][j] = tag_index[\"I-\" + discourse_type]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nbegin = time.time()\nlast_id = \"\"\nall_data = []\nids = []\nmatch_count = 0\nstart_index = 0\nfor i in range(len(train)):\n    item = train.iloc[i]\n    identifier = item[\"id\"] \n    discourse_type = item[\"discourse_type\"]\n    if identifier != last_id:\n        last_id = identifier\n        with open(item[\"file_path\"]) as f:\n            content = \"\".join(f.readlines()) \n            tokens = tokenize(content)\n            annotations = [none_index] * len(tokens) \n            all_data.append((tokens, annotations))\n            ids.append(last_id)\n            start_index = 0\n    annotation_range = get_range(item)\n    indices = calc_word_indices(content, int(item[\"discourse_start\"]), int(item[\"discourse_end\"]))\n    if annotation_range[0] == indices[0] and annotation_range[1] == indices[-1]:\n        match_count += 1\n        add_annotation(all_data, annotation_range[0], annotation_range[-1] + 1, discourse_type)\nprint(f\"Match count: {match_count}, Correct Rate: {match_count / len(train)}\")\nprint(all_data[0])","metadata":{"execution":{"iopub.status.busy":"2022-01-10T13:26:07.287199Z","iopub.execute_input":"2022-01-10T13:26:07.28755Z","iopub.status.idle":"2022-01-10T13:27:25.187621Z","shell.execute_reply.started":"2022-01-10T13:26:07.287509Z","shell.execute_reply":"2022-01-10T13:27:25.186912Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Distribution of Word Counts","metadata":{}},{"cell_type":"code","source":"word_counter = defaultdict(int)\nfor item in all_data:\n    for token in item[0]: \n        word_counter[token] += 1","metadata":{"execution":{"iopub.status.busy":"2022-01-10T13:28:10.99146Z","iopub.execute_input":"2022-01-10T13:28:10.991733Z","iopub.status.idle":"2022-01-10T13:28:12.582244Z","shell.execute_reply.started":"2022-01-10T13:28:10.991704Z","shell.execute_reply":"2022-01-10T13:28:12.581476Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"word_count = pd.DataFrame({\"key\": word_counter.keys(), \"count\": word_counter.values()})\nword_count.sort_values(by=\"count\", ascending=False, inplace=True)\nword_count.head(30)","metadata":{"execution":{"iopub.status.busy":"2022-01-10T13:28:16.775803Z","iopub.execute_input":"2022-01-10T13:28:16.776354Z","iopub.status.idle":"2022-01-10T13:28:16.866063Z","shell.execute_reply.started":"2022-01-10T13:28:16.776318Z","shell.execute_reply":"2022-01-10T13:28:16.86527Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(15, 10))\nsns.barplot(x=\"key\", y=\"count\", data=word_count[:30])","metadata":{"execution":{"iopub.status.busy":"2022-01-10T13:28:44.209601Z","iopub.execute_input":"2022-01-10T13:28:44.210085Z","iopub.status.idle":"2022-01-10T13:28:44.63947Z","shell.execute_reply.started":"2022-01-10T13:28:44.210047Z","shell.execute_reply":"2022-01-10T13:28:44.638759Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"word_count.describe()","metadata":{"execution":{"iopub.status.busy":"2022-01-10T13:28:50.76689Z","iopub.execute_input":"2022-01-10T13:28:50.767164Z","iopub.status.idle":"2022-01-10T13:28:50.785999Z","shell.execute_reply.started":"2022-01-10T13:28:50.767132Z","shell.execute_reply":"2022-01-10T13:28:50.785293Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Number of words","metadata":{}},{"cell_type":"code","source":"len(word_count)","metadata":{"execution":{"iopub.status.busy":"2022-01-10T13:28:53.772931Z","iopub.execute_input":"2022-01-10T13:28:53.773203Z","iopub.status.idle":"2022-01-10T13:28:53.778674Z","shell.execute_reply.started":"2022-01-10T13:28:53.773173Z","shell.execute_reply":"2022-01-10T13:28:53.777926Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Words appearing only once","metadata":{}},{"cell_type":"code","source":"(word_count[\"count\"] <= 1).sum()","metadata":{"execution":{"iopub.status.busy":"2022-01-10T13:29:02.843036Z","iopub.execute_input":"2022-01-10T13:29:02.843622Z","iopub.status.idle":"2022-01-10T13:29:02.851687Z","shell.execute_reply.started":"2022-01-10T13:29:02.843571Z","shell.execute_reply":"2022-01-10T13:29:02.850942Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Ditrubtion of Sentences Lengths","metadata":{}},{"cell_type":"code","source":"pd.DataFrame({\"Sentence Lengths\": [len(item[0]) for item in all_data]}).hist()","metadata":{"execution":{"iopub.status.busy":"2022-01-10T13:29:18.511644Z","iopub.execute_input":"2022-01-10T13:29:18.511918Z","iopub.status.idle":"2022-01-10T13:29:18.815501Z","shell.execute_reply.started":"2022-01-10T13:29:18.511889Z","shell.execute_reply":"2022-01-10T13:29:18.813202Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Vectorization","metadata":{}},{"cell_type":"code","source":"class Vectorizer:\n    \n    def __init__(self, vocab_size = None, sequence_length = None, unk_token = \"<unk>\"):\n        self.vocab_size = vocab_size\n        self.sequence_length = sequence_length\n        self.unk_token = unk_token\n        \n    def fit_transform(self, sentences):\n        word_counter = dict()\n        for tokens in sentences:\n            for token in tokens: \n                if token in word_counter:\n                    word_counter[token] += 1\n                else:\n                    word_counter[token] = 1\n        word_counter = pd.DataFrame({\"key\": word_counter.keys(), \"count\": word_counter.values()})\n        word_counter.sort_values(by=\"count\", ascending=False, inplace=True)\n        vocab = set(word_counter[\"key\"][0:self.vocab_size-2])\n        word_index = dict()\n        begin_index = 1 \n        word_index[self.unk_token] = begin_index\n        begin_index += 1\n        Xs = []\n        for i in range(len(sentences)):\n            X = []\n            for token in sentences[i]:\n                if token not in word_index and token in vocab:\n                    word_index[token] = begin_index\n                    begin_index += 1\n                if token in word_index:\n                    X.append(word_index[token])\n                else:\n                    X.append(word_index[self.unk_token])\n                if len(X) == self.sequence_length:\n                    break\n            Xs.append(X)\n        self.word_index = word_index\n        self.vocab = vocab\n        return Xs\n    \n    def transform(self, sentences):\n        Xs = []\n        for i in range(len(sentences)):\n            X = []\n            for token in sentences[i]:\n                if token in self.word_index:\n                    X.append(self.word_index[token])\n                else:\n                    X.append(self.word_index[self.unk_token])\n                if len(X) == self.sequence_length:\n                    break\n            Xs.append(X)\n        return Xs\n    \n    def load(self, path):\n        with open(path, 'r') as f:\n            dic = json.load(f)\n            self.vocab_size = dic['vocab_size']\n            self.sequence_length = dic['sequence_length']\n            self.unk_token = dic['unk_token']\n            self.word_index = dic['word_index']\n            \n    def save(self, path):\n        with open(path, 'w') as f:\n            data = json.dumps({\n                \"vocab_size\": self.vocab_size, \n                \"sequence_length\": self.sequence_length, \n                \"unk_token\": self.unk_token,\n                \"word_index\": self.word_index\n            })\n            f.write(data)","metadata":{"execution":{"iopub.status.busy":"2022-01-10T13:29:22.793422Z","iopub.execute_input":"2022-01-10T13:29:22.793679Z","iopub.status.idle":"2022-01-10T13:29:22.809862Z","shell.execute_reply.started":"2022-01-10T13:29:22.79365Z","shell.execute_reply":"2022-01-10T13:29:22.808828Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nvectorizer = Vectorizer(vocab_size = vocab_size, sequence_length = sequence_length, unk_token = unk_token)\ntoken_list = [item[0] for item in all_data]\nif mode == modes[0]:\n    Xs = vectorizer.fit_transform(token_list)\n    vectorizer.save(vectorizer_path)\nelse:\n    vectorizer.load(output_dataset_path + vectorizer_path)\n    Xs = vectorizer.transform(token_list)\nfor i in range(len(all_data)):\n    item = all_data[i]\n    annotation = item[1]\n    if len(annotation) > sequence_length:\n        annotation = annotation[0:sequence_length]\n    all_data[i] = (Xs[i], annotation)\ntrain_data, val_data, train_ids, valid_ids = train_test_split(all_data, ids, test_size = 0.1, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2022-01-10T13:29:32.697662Z","iopub.execute_input":"2022-01-10T13:29:32.697913Z","iopub.status.idle":"2022-01-10T13:29:37.272825Z","shell.execute_reply.started":"2022-01-10T13:29:32.697887Z","shell.execute_reply":"2022-01-10T13:29:37.271334Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Export to files","metadata":{}},{"cell_type":"code","source":"def export_to_file(export_file_path, data):\n    with open(export_file_path, \"w+\") as f:\n        for i in range(len(data)):\n            X = data[i][0]\n            y = data[i][1]\n            f.write(\n                str(len(X))\n                + \"\\t\"\n                + \"\\t\".join([str(item) for item in X])\n                + \"\\t\"\n                + \"\\t\".join([str(item) for item in y])\n                + \"\\n\"\n            )","metadata":{"execution":{"iopub.status.busy":"2022-01-10T13:29:37.502234Z","iopub.execute_input":"2022-01-10T13:29:37.503017Z","iopub.status.idle":"2022-01-10T13:29:37.509586Z","shell.execute_reply.started":"2022-01-10T13:29:37.502968Z","shell.execute_reply":"2022-01-10T13:29:37.50861Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"export_to_file(\"train.txt\", train_data)\nexport_to_file(\"validation.txt\", val_data)","metadata":{"execution":{"iopub.status.busy":"2022-01-10T13:29:40.25095Z","iopub.execute_input":"2022-01-10T13:29:40.251502Z","iopub.status.idle":"2022-01-10T13:29:42.925224Z","shell.execute_reply.started":"2022-01-10T13:29:40.251462Z","shell.execute_reply":"2022-01-10T13:29:42.924484Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Create Tensorflow Dataset","metadata":{}},{"cell_type":"code","source":"def preprocess(record):\n    record = tf.strings.split(record, sep=\"\\t\")\n    length = tf.strings.to_number(record[0], out_type=tf.int32)\n    tokens = record[1 : length + 1]\n    tags = record[length + 1 :]\n    tokens = tf.strings.to_number(tokens, out_type=tf.int64)\n    tags = tf.strings.to_number(tags, out_type=tf.int64)\n    return tokens, tags\ndef make_dataset(file_path, batch_size, mode=\"train\"):\n    ds = tf.data.TextLineDataset(file_path).map(preprocess)\n    if mode == \"train\":\n        ds = ds.shuffle(256)\n    ds = ds.padded_batch(batch_size)\n    \n    ds = ds.cache().prefetch(tf.data.AUTOTUNE)\n    return ds","metadata":{"execution":{"iopub.status.busy":"2022-01-10T13:29:45.251529Z","iopub.execute_input":"2022-01-10T13:29:45.251778Z","iopub.status.idle":"2022-01-10T13:29:45.260556Z","shell.execute_reply.started":"2022-01-10T13:29:45.251751Z","shell.execute_reply":"2022-01-10T13:29:45.257867Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_ds = make_dataset(\"train.txt\", batch_size, mode=\"train\")\nval_ds = make_dataset(\"validation.txt\", batch_size, mode=\"valid\")","metadata":{"execution":{"iopub.status.busy":"2022-01-10T13:29:48.059293Z","iopub.execute_input":"2022-01-10T13:29:48.059543Z","iopub.status.idle":"2022-01-10T13:29:50.371949Z","shell.execute_reply.started":"2022-01-10T13:29:48.059517Z","shell.execute_reply":"2022-01-10T13:29:50.371274Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for X, y in train_ds.take(2):\n    print(X)\n    print(y)","metadata":{"execution":{"iopub.status.busy":"2022-01-10T13:29:53.296967Z","iopub.execute_input":"2022-01-10T13:29:53.297237Z","iopub.status.idle":"2022-01-10T13:29:53.514947Z","shell.execute_reply.started":"2022-01-10T13:29:53.297208Z","shell.execute_reply":"2022-01-10T13:29:53.514251Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Modeling","metadata":{}},{"cell_type":"code","source":"accuracy_metric = keras.metrics.SparseCategoricalAccuracy()\ndef accuracy(y_true, y_pred):\n    acc = accuracy_metric(y_true, y_pred)\n    mask = tf.cast((y_true > 0), dtype=tf.float32)\n    acc = acc * mask\n    return tf.reduce_sum(acc) / tf.reduce_sum(mask)","metadata":{"execution":{"iopub.status.busy":"2022-01-10T13:29:56.889697Z","iopub.execute_input":"2022-01-10T13:29:56.890236Z","iopub.status.idle":"2022-01-10T13:29:56.919498Z","shell.execute_reply.started":"2022-01-10T13:29:56.890194Z","shell.execute_reply":"2022-01-10T13:29:56.918785Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Reference https://keras.io/examples/nlp/ner_transformers/\nclass CustomNonPaddingTokenLoss(keras.losses.Loss):\n    def __init__(self, name=\"custom_ner_loss\"):\n        super().__init__(name=name)\n\n    def call(self, y_true, y_pred):\n        loss_fn = keras.losses.SparseCategoricalCrossentropy(\n            from_logits=True, reduction=keras.losses.Reduction.NONE\n        )\n        loss = loss_fn(y_true, y_pred)\n        mask = tf.cast((y_true > 0), dtype=tf.float32)\n        loss = loss * mask\n        return tf.reduce_sum(loss) / tf.reduce_sum(mask)\n    \nclass TransformerBlock(layers.Layer):\n    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n        super(TransformerBlock, self).__init__()\n        self.att = keras.layers.MultiHeadAttention(\n            num_heads=num_heads, key_dim=embed_dim\n        )\n        self.ffn = keras.Sequential(\n            [\n                keras.layers.Dense(ff_dim, activation=\"relu\"),\n                keras.layers.Dense(embed_dim),\n            ]\n        )\n        self.layernorm1 = keras.layers.LayerNormalization(epsilon=1e-6)\n        self.layernorm2 = keras.layers.LayerNormalization(epsilon=1e-6)\n        self.dropout1 = keras.layers.Dropout(rate)\n        self.dropout2 = keras.layers.Dropout(rate)\n\n    def call(self, inputs, training=False):\n        attn_output = self.att(inputs, inputs)\n        attn_output = self.dropout1(attn_output, training=training)\n        out1 = self.layernorm1(inputs + attn_output)\n        ffn_output = self.ffn(out1)\n        ffn_output = self.dropout2(ffn_output, training=training)\n        return self.layernorm2(out1 + ffn_output)\nclass PositionalEmbedding(layers.Layer):\n    def __init__(self, maxlen, vocab_size, embed_dim):\n        super(PositionalEmbedding, self).__init__()\n        self.token_emb = keras.layers.Embedding(\n            input_dim=vocab_size, output_dim=embed_dim, mask_zero=True\n        )\n        self.pos_emb = keras.layers.Embedding(input_dim=maxlen, output_dim=embed_dim, mask_zero=True)\n\n    def call(self, inputs):\n        maxlen = tf.shape(inputs)[-1]\n        positions = tf.range(start=0, limit=maxlen, delta=1)\n        position_embeddings = self.pos_emb(positions)\n        token_embeddings = self.token_emb(inputs)\n        return token_embeddings + position_embeddings\n    ","metadata":{"execution":{"iopub.status.busy":"2022-01-10T13:30:09.953032Z","iopub.execute_input":"2022-01-10T13:30:09.953462Z","iopub.status.idle":"2022-01-10T13:30:09.97036Z","shell.execute_reply.started":"2022-01-10T13:30:09.953426Z","shell.execute_reply":"2022-01-10T13:30:09.969587Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = keras.Sequential([\n    PositionalEmbedding(sequence_length, vocab_size, embed_size),\n    TransformerBlock(embed_size, 4, 32),\n    TransformerBlock(embed_size, 4, 32),\n    layers.Dropout(0.1),\n    layers.Dense(32, activation=\"relu\"),\n    layers.Dropout(0.1),\n    layers.Dense(len(all_tags), activation=\"softmax\")\n])","metadata":{"execution":{"iopub.status.busy":"2022-01-10T13:30:13.207594Z","iopub.execute_input":"2022-01-10T13:30:13.208153Z","iopub.status.idle":"2022-01-10T13:30:13.242312Z","shell.execute_reply.started":"2022-01-10T13:30:13.208095Z","shell.execute_reply":"2022-01-10T13:30:13.241646Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training","metadata":{}},{"cell_type":"code","source":"if mode == modes[0]:\n    early_stop = keras.callbacks.EarlyStopping(\n        min_delta=1e-4, \n        monitor=\"val_loss\",\n        patience=10\n    )\n    reduce_lr = keras.callbacks.ReduceLROnPlateau(\n        factor=0.3,\n        patience=4, \n        monitor=\"val_loss\",\n        min_lr=1e-7\n    )\n    optimizer = tf.keras.optimizers.Adam(1e-3)\n    loss = CustomNonPaddingTokenLoss()\n    #loss = keras.losses.SparseCategoricalCrossentropy()\n    model.compile(loss=loss, optimizer=optimizer, metrics=[accuracy])\n    callbacks = [early_stop, reduce_lr]\n    \n    model.fit(train_ds, epochs=epochs, validation_data=val_ds, callbacks=callbacks)\n    model.save_weights(model_path)\nelse:\n    model.build(input_shape=(None, None))\n    model.load_weights(output_dataset_path+model_path)","metadata":{"execution":{"iopub.status.busy":"2022-01-10T13:30:17.619372Z","iopub.execute_input":"2022-01-10T13:30:17.620061Z","iopub.status.idle":"2022-01-10T13:30:22.794309Z","shell.execute_reply.started":"2022-01-10T13:30:17.620025Z","shell.execute_reply":"2022-01-10T13:30:22.793157Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.summary()","metadata":{"execution":{"iopub.status.busy":"2022-01-10T13:30:28.124618Z","iopub.execute_input":"2022-01-10T13:30:28.125034Z","iopub.status.idle":"2022-01-10T13:30:28.138553Z","shell.execute_reply.started":"2022-01-10T13:30:28.124999Z","shell.execute_reply":"2022-01-10T13:30:28.13775Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"keras.utils.plot_model(model, show_shapes=True, show_dtype=True)","metadata":{"execution":{"iopub.status.busy":"2022-01-10T13:30:31.10546Z","iopub.execute_input":"2022-01-10T13:30:31.105721Z","iopub.status.idle":"2022-01-10T13:30:31.848687Z","shell.execute_reply.started":"2022-01-10T13:30:31.105692Z","shell.execute_reply":"2022-01-10T13:30:31.84795Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Evaluation","metadata":{}},{"cell_type":"code","source":"def evaluate(model, dataset):\n    all_true_tag_ids, all_predicted_tag_ids = [], []\n    for x, y in dataset:\n        output = model.predict(x)\n        predictions = np.argmax(output, axis=-1)\n        predictions = np.reshape(predictions, [-1])\n\n        true_tag_ids = np.reshape(y, [-1])\n\n        mask = (true_tag_ids != 0) & (true_tag_ids != none_index) & (predictions > 0)\n        true_tag_ids = true_tag_ids[mask]\n        predicted_tag_ids = predictions[mask]\n\n        all_true_tag_ids.append(true_tag_ids)\n        all_predicted_tag_ids.append(predicted_tag_ids)\n\n    all_true_tag_ids = np.concatenate(all_true_tag_ids)\n    all_predicted_tag_ids = np.concatenate(all_predicted_tag_ids)\n    cls_report = classification_report(all_true_tag_ids, all_predicted_tag_ids)\n    print(\"Classifiction report:\")\n    print(cls_report)\n    f1 =  f1_score(all_true_tag_ids, all_predicted_tag_ids, average=\"macro\")\n    print(\"F1 score:\", f1)","metadata":{"execution":{"iopub.status.busy":"2022-01-10T13:30:35.40891Z","iopub.execute_input":"2022-01-10T13:30:35.409657Z","iopub.status.idle":"2022-01-10T13:30:35.417323Z","shell.execute_reply.started":"2022-01-10T13:30:35.409618Z","shell.execute_reply":"2022-01-10T13:30:35.416585Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_prediction_csv_file(model, dataset, origin_ids, file_path):\n    predictions = []\n    classes = []\n    ids = []\n    t = 0\n    for item in dataset:\n        if len(item) == 2:\n            X = item[0]\n        else:\n            X = item\n        y_pred =  np.argmax(model.predict(X), axis=-1)\n        for i in range(y_pred.shape[0]):\n            last_prediction = None\n            indices = []\n            identifier = origin_ids[t]\n            t += 1\n            for j in range(X.shape[1]):\n                if last_prediction != index_tag[y_pred[i, j]]:\n                    if len(indices) > 0:\n                        ids.append(identifier)\n                        predictions.append(indices)\n                        classes.append(last_prediction)\n                        indices = []\n                    last_prediction = index_tag[y_pred[i, j]]\n                if y_pred[i, j] != pad_index and y_pred[i, j] != none_index:\n                    indices.append(j)\n                if j == X.shape[1] - 1:\n                    if len(indices) > 0:\n                        ids.append(identifier)\n                        predictions.append(indices)\n                        classes.append(last_prediction)\n                        indices = []\n                if X[i, j] == pad_index:\n                    break\n    new_ids = []\n    new_classes = []\n    new_preditions = []\n    for i in range(len(ids)):\n        merge = False\n        if ids[i - 1] == ids[i] and i > 0:\n            if len(predictions[i]) <= 3:\n                merge = True\n                j = new_preditions[-1][-1] + 1\n                while j < predictions[i][0]:\n                    new_preditions[-1].append(j)\n                    j += 1\n                new_preditions[-1] = new_preditions[-1] + predictions[i]\n            elif abs(predictions[i][0] - new_preditions[-1][-1]) <= 3 and classes[i] == new_classes[-1]:\n                merge = True\n                j = new_preditions[-1][-1] + 1\n                while j < predictions[i][0]:\n                    new_preditions[-1].append(j)\n                    j += 1\n                new_preditions[-1] = new_preditions[-1] + predictions[i]\n        if not merge:\n            new_ids.append(ids[i])\n            new_classes.append(classes[i])\n            new_preditions.append(predictions[i])\n    df = pd.DataFrame({\"id\": new_ids, \"class\": new_classes, \"predictionstring\": [\" \".join([str(element) for element in item]) for item in new_preditions]})\n    df.to_csv(file_path, index=False)","metadata":{"execution":{"iopub.status.busy":"2022-01-10T13:33:07.263455Z","iopub.execute_input":"2022-01-10T13:33:07.263771Z","iopub.status.idle":"2022-01-10T13:33:07.282048Z","shell.execute_reply.started":"2022-01-10T13:33:07.263739Z","shell.execute_reply":"2022-01-10T13:33:07.281357Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"evaluate(model, val_ds)","metadata":{"execution":{"iopub.status.busy":"2022-01-10T13:32:32.335996Z","iopub.execute_input":"2022-01-10T13:32:32.336552Z","iopub.status.idle":"2022-01-10T13:32:36.849863Z","shell.execute_reply.started":"2022-01-10T13:32:32.336513Z","shell.execute_reply":"2022-01-10T13:32:36.849135Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Submission","metadata":{}},{"cell_type":"code","source":"%%time\ntoken_list = []\nfor i in range(len(submission)):\n    item = submission.iloc[i]\n    identifier = item[\"id\"] \n    with open(item[\"file_path\"]) as f:\n        content = \"\".join(f.readlines())\n        tokens = tokenize(content)\n        token_list.append(tokens)","metadata":{"execution":{"iopub.status.busy":"2021-12-25T17:33:16.598067Z","iopub.execute_input":"2021-12-25T17:33:16.598318Z","iopub.status.idle":"2021-12-25T17:33:16.668092Z","shell.execute_reply.started":"2021-12-25T17:33:16.59829Z","shell.execute_reply":"2021-12-25T17:33:16.667254Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def preprocess_test(record):\n    record = tf.strings.split(record, sep=\"\\t\")\n    length = tf.strings.to_number(record[0], out_type=tf.int32)\n    tokens = record[1 : length + 1]\n    tokens = tf.strings.to_number(tokens, out_type=tf.int64)\n    return tokens\ndef make_test_dataset(Xs, file_path, batch_size):\n    with open(file_path, \"w+\") as f:\n        for i in range(len(Xs)):\n            X = Xs[i]\n            f.write(\n                str(len(X))\n                + \"\\t\"\n                + \"\\t\".join([str(item) for item in X])\n                + \"\\n\"\n            )\n    ds = tf.data.TextLineDataset(file_path).map(preprocess_test)\n    ds = ds.padded_batch(batch_size).cache().prefetch(tf.data.AUTOTUNE)\n    return ds","metadata":{"execution":{"iopub.status.busy":"2021-12-25T17:35:32.483394Z","iopub.execute_input":"2021-12-25T17:35:32.483692Z","iopub.status.idle":"2021-12-25T17:35:32.492041Z","shell.execute_reply.started":"2021-12-25T17:35:32.483662Z","shell.execute_reply":"2021-12-25T17:35:32.491166Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nX_test = vectorizer.transform(token_list)\ntest_ds = make_test_dataset(X_test, \"test.txt\", batch_size)\ncreate_prediction_csv_file(model, test_ds, list(submission[\"id\"]), \"submission.csv\")","metadata":{"execution":{"iopub.status.busy":"2021-12-25T17:35:35.045599Z","iopub.execute_input":"2021-12-25T17:35:35.046273Z","iopub.status.idle":"2021-12-25T17:35:36.486101Z","shell.execute_reply.started":"2021-12-25T17:35:35.046233Z","shell.execute_reply":"2021-12-25T17:35:36.484582Z"},"trusted":true},"execution_count":null,"outputs":[]}]}