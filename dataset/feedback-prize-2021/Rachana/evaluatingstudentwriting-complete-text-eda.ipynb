{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"\n# Introduction: \n\nThe dataset contains argumentative essays written by U.S students in grades 6-12. The essays were annotated by expert raters for elements commonly found in argumentative writing.\n\nTask: To predict the human annotations. You will first need to segment each essay into discrete rhetorical and argumentative elements (i.e., discourse elements) and then classify as one of 7 \"discourse types\". These are:\n\n1. Lead - an introduction that begins with a statistic, a quotation, a description, or some other device to grab the reader’s attention and point toward the thesis\n2. Position - an opinion or conclusion on the main question\n3. Claim - a claim that supports the position\n4. Counterclaim - a claim that refutes another claim or gives an opposing reason to the position\n5. Rebuttal - a claim that refutes a counterclaim\n6. Evidence - ideas or examples that support claims, counterclaims, or rebuttals.\n7. Concluding Statement - a concluding statement that restates the claims\n\nData:\n\ntrain.zip - folder of individual .txt files, with each file containing the full text of an essay response in the training set\n\ntrain.csv - a .csv file containing the annotated version of all essays in the training set\n\ntest.zip - folder of individual .txt files, with each file containing the full text of an essay response in the test set\n\nsample_submission.csv - file in the required format for making predictions - note that if you are making multiple predictions for a document, submit multiple rows","metadata":{}},{"cell_type":"code","source":"# import libraries:\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\n\nfrom glob import glob\n\n#import matplotlib.style as style\n#style.use('fivethirtyeight')\nfrom matplotlib.ticker import FuncFormatter\n\nfrom wordcloud import WordCloud, STOPWORDS\n\n\nimport nltk\nfrom nltk.corpus import stopwords\n\n\nimport warnings\nwarnings.filterwarnings('ignore')\n#import spacy\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport os","metadata":{"execution":{"iopub.status.busy":"2022-02-20T05:58:34.230451Z","iopub.execute_input":"2022-02-20T05:58:34.230974Z","iopub.status.idle":"2022-02-20T05:58:35.739875Z","shell.execute_reply.started":"2022-02-20T05:58:34.230878Z","shell.execute_reply":"2022-02-20T05:58:35.739145Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Part 1:\n# Train.csv file EDA","metadata":{}},{"cell_type":"code","source":"base_path = '/kaggle/input/feedback-prize-2021/'","metadata":{"execution":{"iopub.status.busy":"2022-02-20T05:58:35.741313Z","iopub.execute_input":"2022-02-20T05:58:35.741525Z","iopub.status.idle":"2022-02-20T05:58:35.744844Z","shell.execute_reply.started":"2022-02-20T05:58:35.741496Z","shell.execute_reply":"2022-02-20T05:58:35.744127Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df = pd.read_csv('/kaggle/input/feedback-prize-2021/train.csv')","metadata":{"execution":{"iopub.status.busy":"2022-02-20T05:58:35.747676Z","iopub.execute_input":"2022-02-20T05:58:35.747832Z","iopub.status.idle":"2022-02-20T05:58:37.429886Z","shell.execute_reply.started":"2022-02-20T05:58:35.747811Z","shell.execute_reply":"2022-02-20T05:58:37.429234Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"display(train_df.head(5))\ndisplay(train_df.shape)","metadata":{"execution":{"iopub.status.busy":"2022-02-20T05:58:37.431329Z","iopub.execute_input":"2022-02-20T05:58:37.431548Z","iopub.status.idle":"2022-02-20T05:58:37.454954Z","shell.execute_reply.started":"2022-02-20T05:58:37.43152Z","shell.execute_reply":"2022-02-20T05:58:37.454407Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# finding unique values in each columns\nfor col in train_df.columns:\n    print(col + \":\" + str(len(train_df[col].unique())))","metadata":{"execution":{"iopub.status.busy":"2022-02-20T05:58:37.455952Z","iopub.execute_input":"2022-02-20T05:58:37.457199Z","iopub.status.idle":"2022-02-20T05:58:37.683816Z","shell.execute_reply.started":"2022-02-20T05:58:37.457168Z","shell.execute_reply":"2022-02-20T05:58:37.683146Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# lets take a look at the number of text files :\ntrain_text_files = os.listdir(base_path+'/train')\nlen(train_text_files)","metadata":{"execution":{"iopub.status.busy":"2022-02-20T05:58:37.684899Z","iopub.execute_input":"2022-02-20T05:58:37.685244Z","iopub.status.idle":"2022-02-20T05:58:37.944664Z","shell.execute_reply.started":"2022-02-20T05:58:37.685212Z","shell.execute_reply":"2022-02-20T05:58:37.944018Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.describe()","metadata":{"execution":{"iopub.status.busy":"2022-02-20T05:58:37.945722Z","iopub.execute_input":"2022-02-20T05:58:37.945925Z","iopub.status.idle":"2022-02-20T05:58:37.976838Z","shell.execute_reply.started":"2022-02-20T05:58:37.945898Z","shell.execute_reply":"2022-02-20T05:58:37.976236Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train data set is free from missing values:\ntrain_df.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2022-02-20T05:58:37.977679Z","iopub.execute_input":"2022-02-20T05:58:37.977857Z","iopub.status.idle":"2022-02-20T05:58:38.029868Z","shell.execute_reply.started":"2022-02-20T05:58:37.977832Z","shell.execute_reply":"2022-02-20T05:58:38.029135Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"1. There are 15594 essays.\n2. These essays have been labelled into 144293 discourses.\n3. Each discourses is labelled in one of the 7 discourse types.\n\nThus, Training data consists of 16k(id = 15594) essays and 144k(discourse_id = 144293) lines of annotations (~9 discourses per essay). \n\nThe train.csv contains discourse_text with annotations. Each row corresponds to one discourse element and contains the following:\n\n1. id - ID code for essay response\n\n2. discourse_id - ID code for discourse element\n\n3. discourse_start - character position where discourse element begins in the essay response\n\n4. discourse_end - character position where discourse element ends in the essay response\n\n5. discourse_text - text of discourse element\n\n6. discourse_type - classification of discourse element\n\n7. discourse_type_num - enumerated class label of discourse element\n\n8. predictionstring - the word indices of the training sample, as required for predictions\n\n\n\n","metadata":{}},{"cell_type":"markdown","source":"#Distribution of Discourse type labels in the annonated essays.","metadata":{}},{"cell_type":"code","source":"#distribution of labels in the annotated discourse:\nlabel_dist = train_df['discourse_type'].value_counts()\nlabel_dist *= 100 / label_dist.sum()\nlabel_dist","metadata":{"execution":{"iopub.status.busy":"2022-02-20T05:58:44.079136Z","iopub.execute_input":"2022-02-20T05:58:44.079361Z","iopub.status.idle":"2022-02-20T05:58:44.094249Z","shell.execute_reply.started":"2022-02-20T05:58:44.079338Z","shell.execute_reply":"2022-02-20T05:58:44.093367Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"label_dist.plot.bar(rot=90, title='Distribution of lables')\nplt.ylabel('count')","metadata":{"execution":{"iopub.status.busy":"2022-02-20T05:58:44.8269Z","iopub.execute_input":"2022-02-20T05:58:44.827575Z","iopub.status.idle":"2022-02-20T05:58:45.01255Z","shell.execute_reply.started":"2022-02-20T05:58:44.827522Z","shell.execute_reply":"2022-02-20T05:58:45.011946Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Percentage distribution of discourse_type_number in the annotated discourses:\nav_per_disc = train_df['discourse_type_num'].value_counts(ascending = True)\n#av_per_disc\nav_per_disc *= 100 / av_per_disc.sum()\nav_per_disc.rename_axis('discourse_type_num').reset_index(name='%count')","metadata":{"execution":{"iopub.status.busy":"2022-02-20T05:58:46.165581Z","iopub.execute_input":"2022-02-20T05:58:46.166126Z","iopub.status.idle":"2022-02-20T05:58:46.186215Z","shell.execute_reply.started":"2022-02-20T05:58:46.166075Z","shell.execute_reply":"2022-02-20T05:58:46.185514Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"av_per_disc.plot(kind = \"barh\", figsize = (12, 8))","metadata":{"execution":{"iopub.status.busy":"2022-02-20T05:58:47.646377Z","iopub.execute_input":"2022-02-20T05:58:47.646808Z","iopub.status.idle":"2022-02-20T05:58:48.197491Z","shell.execute_reply.started":"2022-02-20T05:58:47.646773Z","shell.execute_reply":"2022-02-20T05:58:48.19678Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#Distribution of average number of words in discourse_types and discourse_type_number:\n","metadata":{}},{"cell_type":"code","source":"#add columns to 'train_df' which calculates the length of string in dicourse (as dis_len) and prediction string (as pred_len)\ntrain_df['disc_len'] = train_df['discourse_text'].astype(str).apply(len)\ntrain_df['pred_len'] = train_df['predictionstring'].astype(str).apply(len)","metadata":{"execution":{"iopub.status.busy":"2022-02-20T05:58:50.426628Z","iopub.execute_input":"2022-02-20T05:58:50.426902Z","iopub.status.idle":"2022-02-20T05:58:50.551905Z","shell.execute_reply.started":"2022-02-20T05:58:50.426869Z","shell.execute_reply":"2022-02-20T05:58:50.551284Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.head(3)","metadata":{"execution":{"iopub.status.busy":"2022-02-20T05:58:51.45847Z","iopub.execute_input":"2022-02-20T05:58:51.458936Z","iopub.status.idle":"2022-02-20T05:58:51.473355Z","shell.execute_reply.started":"2022-02-20T05:58:51.45891Z","shell.execute_reply":"2022-02-20T05:58:51.47275Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#add columns to 'train_df' which calculates number of words of string in dicourse (as disc_word_count) and prediction string (as pred_word_count)\ntrain_df[\"disc_word_count\"] = train_df[\"discourse_text\"].apply(lambda x: len(x.split()))\ntrain_df[\"pred_word_count\"] = train_df[\"predictionstring\"].apply(lambda x: len(x.split()))\n","metadata":{"execution":{"iopub.status.busy":"2022-02-20T05:58:52.554221Z","iopub.execute_input":"2022-02-20T05:58:52.554493Z","iopub.status.idle":"2022-02-20T05:58:53.161947Z","shell.execute_reply.started":"2022-02-20T05:58:52.55446Z","shell.execute_reply":"2022-02-20T05:58:53.161497Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.head(3)","metadata":{"execution":{"iopub.status.busy":"2022-02-20T05:58:53.634338Z","iopub.execute_input":"2022-02-20T05:58:53.635182Z","iopub.status.idle":"2022-02-20T05:58:53.651745Z","shell.execute_reply.started":"2022-02-20T05:58:53.635125Z","shell.execute_reply":"2022-02-20T05:58:53.651133Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Distribution of text length in discourse text:\ndiscourse_len = train_df['disc_len'] \nfig, ax = plt.subplots(figsize=(12, 8))\n\nsns.distplot(discourse_len, bins = 50 , ax = ax)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-20T05:58:54.719182Z","iopub.execute_input":"2022-02-20T05:58:54.719537Z","iopub.status.idle":"2022-02-20T05:58:55.576702Z","shell.execute_reply.started":"2022-02-20T05:58:54.719503Z","shell.execute_reply":"2022-02-20T05:58:55.576257Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# distribution of number of words in discourse text:\nword_dist = train_df['disc_word_count']\nfig, ax = plt.subplots(figsize=(12, 8))\nsns.distplot(word_dist,bins = 50 , ax = ax )\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-20T05:58:56.054799Z","iopub.execute_input":"2022-02-20T05:58:56.055148Z","iopub.status.idle":"2022-02-20T05:58:56.897465Z","shell.execute_reply.started":"2022-02-20T05:58:56.055116Z","shell.execute_reply":"2022-02-20T05:58:56.896995Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# distribution of number of words in prediction string:\npred_word = train_df[\"pred_word_count\"]\nfig, ax = plt.subplots(figsize=(12, 8))\nsns.distplot(pred_word,bins = 50 , ax = ax )\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-20T05:58:57.206978Z","iopub.execute_input":"2022-02-20T05:58:57.207349Z","iopub.status.idle":"2022-02-20T05:58:58.050491Z","shell.execute_reply.started":"2022-02-20T05:58:57.207316Z","shell.execute_reply":"2022-02-20T05:58:58.049924Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# now lets find out the average number of words per discourse type:\ndis_type_len = train_df.groupby('discourse_type')['disc_word_count'].mean().sort_values()\ndis_type_len","metadata":{"execution":{"iopub.status.busy":"2022-02-20T05:58:58.478836Z","iopub.execute_input":"2022-02-20T05:58:58.479136Z","iopub.status.idle":"2022-02-20T05:58:58.503716Z","shell.execute_reply.started":"2022-02-20T05:58:58.479097Z","shell.execute_reply":"2022-02-20T05:58:58.50314Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#plot the graph for average number of word per discourse type:\ndis_type_len.plot(kind = 'barh', figsize = (10,5))\nplt.xlabel('average number of words')\nplt.title('Average number of word per discourse type')","metadata":{"execution":{"iopub.status.busy":"2022-02-20T05:58:59.658851Z","iopub.execute_input":"2022-02-20T05:58:59.659102Z","iopub.status.idle":"2022-02-20T05:58:59.837835Z","shell.execute_reply.started":"2022-02-20T05:58:59.659059Z","shell.execute_reply":"2022-02-20T05:58:59.836496Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Also find out the average len of prediction string:\npred_str_len = train_df.groupby('discourse_type')['pred_len'].mean().sort_values()\npred_str_len","metadata":{"execution":{"iopub.status.busy":"2022-02-20T05:59:00.910839Z","iopub.execute_input":"2022-02-20T05:59:00.911497Z","iopub.status.idle":"2022-02-20T05:59:00.930698Z","shell.execute_reply.started":"2022-02-20T05:59:00.911463Z","shell.execute_reply":"2022-02-20T05:59:00.93014Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#plot the graph for length of prediction string per type:\npred_str_len.plot(kind = 'barh', figsize = (10,5))\nplt.xlabel('Prediction string length')\nplt.title('Length of pred_string per discourse type')","metadata":{"execution":{"iopub.status.busy":"2022-02-20T05:59:02.135284Z","iopub.execute_input":"2022-02-20T05:59:02.135506Z","iopub.status.idle":"2022-02-20T05:59:02.30572Z","shell.execute_reply.started":"2022-02-20T05:59:02.135481Z","shell.execute_reply":"2022-02-20T05:59:02.30513Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Below you can see a plot with the average positions of the discourse start and end.\ndata = train_df.groupby(\"discourse_type\")[['discourse_end', 'discourse_start']].mean().reset_index().sort_values(by = 'discourse_start', ascending = False)\ndata.plot(x='discourse_type',\n        kind='barh',\n        stacked=False,\n        title='Average start and end position absolute',\n        figsize=(12,4))\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-20T05:59:03.3586Z","iopub.execute_input":"2022-02-20T05:59:03.358813Z","iopub.status.idle":"2022-02-20T05:59:03.570722Z","shell.execute_reply.started":"2022-02-20T05:59:03.35879Z","shell.execute_reply":"2022-02-20T05:59:03.569962Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"1. Discourse texts have lengths from 691 to 18k symbols with most of them around 1-3k symbols.\n\n2. Number of words in each discourse text is around 500-1000 on average, with some outliers.\n\n3. Different type classes are unequally distributed, CLAIM and EVIDENCE are most popular ones.\n\n4. 'Evidence' has most average number of words, followed by 'Concluding Statement' class.\n\n5. Is there a correlation between the length of a discourse and the class (discourse_type)? \nYes, there is. Evidence is the longest discount type on average. When looking at the frequencies of occurence, we see that Counterclaim and Rebuttal are relatively rare.\n\n6. We do have the field discourse_type_num. We see that Evidence1, Position1 and Claim1 are almost always there in an essay. Most students also had at least one Concluding Statement. What's surprising to me is that a Lead is missing in about 40% of the essays (Lead 1 is found in almost 60% of the essays).\n\n7. We also try to find out the number of words in discourse_text and prediction string. They both are of same length as expected.\n\n","metadata":{}},{"cell_type":"markdown","source":"#A look at the discourse text annotation:","metadata":{}},{"cell_type":"code","source":"# Let's look at the first text and its annotation.\ndef print_text(text_id):\n    with open(f'/kaggle/input/feedback-prize-2021/train/{text_id}.txt') as f:\n        lines = f.readlines()\n    print(''.join(lines))\n    \nprint_text('423A1CA112E2')","metadata":{"execution":{"iopub.status.busy":"2022-02-20T05:59:06.975045Z","iopub.execute_input":"2022-02-20T05:59:06.975335Z","iopub.status.idle":"2022-02-20T05:59:06.986226Z","shell.execute_reply.started":"2022-02-20T05:59:06.975306Z","shell.execute_reply":"2022-02-20T05:59:06.985286Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# We can make annotations more clear if we print texts using different colors.\nfrom termcolor import colored\ndef color_text(text_id, train_df, color_scheme = None):\n    if not color_scheme:\n        color_scheme = {\n        'Lead': 'green',\n        'Position': 'red',\n        'Claim': 'blue',\n        'Counterclaim': 'magenta',\n        'Rebuttal': 'yellow',\n        'Evidence': 'cyan',\n        'Concluding Statement': 'grey'\n    } \n    with open(f'/kaggle/input/feedback-prize-2021/train/{text_id}.txt') as f:\n        lines = f.readlines()\n    text = ''.join(lines)\n    \n    annot_df = train_df[train_df.id == text_id]\n    blocks = [(int(row['discourse_start']),int(row['discourse_end']), color_scheme[row['discourse_type']]) for k, row in annot_df.iterrows()]\n    blocks.sort()\n    i = 0\n    last_symbol = -1\n    while i < len(blocks):\n        if blocks[i][0] > last_symbol + 1:\n            blocks.insert(i, (last_symbol+1, blocks[i][0] - 1, None))\n        last_symbol = blocks[i][1]\n        i += 1\n    if last_symbol < len(text):\n        blocks.append((last_symbol+1, len(text) - 1, None))\n\n    colored_text = ''.join([colored(text[x[0]:x[1]+1], x[2]) for x in blocks])\n    return colored_text\n    \nprint(color_text('423A1CA112E2', train_df))","metadata":{"execution":{"iopub.status.busy":"2022-02-20T05:59:08.5426Z","iopub.execute_input":"2022-02-20T05:59:08.542845Z","iopub.status.idle":"2022-02-20T05:59:08.565203Z","shell.execute_reply.started":"2022-02-20T05:59:08.542817Z","shell.execute_reply":"2022-02-20T05:59:08.564441Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# lets try another disourse:\nprint(color_text('6B4F7A0165B9', train_df))","metadata":{"execution":{"iopub.status.busy":"2022-02-20T05:59:09.630627Z","iopub.execute_input":"2022-02-20T05:59:09.631256Z","iopub.status.idle":"2022-02-20T05:59:09.648711Z","shell.execute_reply.started":"2022-02-20T05:59:09.631224Z","shell.execute_reply":"2022-02-20T05:59:09.648017Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#Wordcloud\n\nWord Cloud is a data visualization technique used for representing text data in which the size of each word indicates its frequency or importance. Significant textual data points can be highlighted using a word cloud. Word clouds are widely used for analyzing data from social network websites.\n\nWe will write a simple and intuitive function plot_wordcloud that will help us plot wordclouds with ease.","metadata":{}},{"cell_type":"code","source":"# function to plot world cloud:\ndef plot_wordcloud(column, title):\n    \n    \"\"\"\n    Function to Plot Wordcloud of given dataframe column.\n    \n    params: column(string): The Column of the DataFrame for plotting.\n            title(string) : The Title of the Wordcloud.\n    \"\"\"\n    # Define stopwords\n    stopwords = set(STOPWORDS) \n    \n    # Define the Wordcloud    \n    wordcloud = WordCloud(width = 800, \n                          height = 800,\n                          background_color ='black',\n                          min_font_size = 10,\n                          stopwords = stopwords).generate(' '.join(train_df[column])) \n\n    # Plot the WordCloud image                        \n    plt.figure(figsize = (8, 8), facecolor = None) \n    plt.imshow(wordcloud) \n    plt.axis(\"off\") \n    plt.tight_layout(pad = 0) \n    plt.title('Wordcloud: ' + title, fontsize = 20)\n\n    plt.show() ","metadata":{"execution":{"iopub.status.busy":"2022-02-20T05:59:13.75138Z","iopub.execute_input":"2022-02-20T05:59:13.751823Z","iopub.status.idle":"2022-02-20T05:59:13.757124Z","shell.execute_reply.started":"2022-02-20T05:59:13.751789Z","shell.execute_reply":"2022-02-20T05:59:13.756525Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Most frequent words in 'discourse'\nplot_wordcloud(column = 'discourse_text', title = 'Most frequent words in Discourse Texts')","metadata":{"execution":{"iopub.status.busy":"2022-02-20T05:59:16.171599Z","iopub.execute_input":"2022-02-20T05:59:16.171945Z","iopub.status.idle":"2022-02-20T05:59:31.72363Z","shell.execute_reply.started":"2022-02-20T05:59:16.171918Z","shell.execute_reply":"2022-02-20T05:59:31.723163Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#Most used words in different Discourse Types","metadata":{}},{"cell_type":"code","source":"train_df['discourse_text'] = train_df['discourse_text'].str.lower()\n\n#get stopwords from nltk library\nstop_english = stopwords.words(\"english\")\nother_words_to_take_out = ['school', 'students', 'people', 'would', 'could', 'many']\nstop_english.extend(other_words_to_take_out)\n\n#put dataframe of Top-10 words in dict for all discourse types\ncounts_dict = {}\nfor dt in train_df['discourse_type'].unique():\n    df = train_df.query('discourse_type == @dt')\n    text = df.discourse_text.apply(lambda x: x.split()).tolist()\n    text = [item for elem in text for item in elem]\n    df1 = pd.Series(text).value_counts().to_frame().reset_index()\n    df1.columns = ['Word', 'Frequency']\n    df1 = df1[~df1.Word.isin(stop_english)].head(10)\n    df1 = df1.set_index(\"Word\").sort_values(by = \"Frequency\", ascending = True)\n    counts_dict[dt] = df1\n\nplt.figure(figsize=(15, 12))\nplt.subplots_adjust(hspace=0.5)\n\nkeys = list(counts_dict.keys())\n\nfor n, key in enumerate(keys):\n    ax = plt.subplot(4, 2, n + 1)\n    ax.set_title(f\"Most used words in {key}\")\n    counts_dict[keys[n]].plot(ax=ax, kind = 'barh')\n    plt.ylabel(\"\")\n\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2022-02-20T05:59:36.651061Z","iopub.execute_input":"2022-02-20T05:59:36.651459Z","iopub.status.idle":"2022-02-20T05:59:40.174934Z","shell.execute_reply.started":"2022-02-20T05:59:36.651421Z","shell.execute_reply":"2022-02-20T05:59:40.174217Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#Unigram\n\nNow we need to extract N-Gram features. N-grams are used to describe the number of words used as observation points, e.g., unigram means singly-worded, bigram means 2-worded phrase, and trigram means 3-worded phrase. \n\nIn order to do this, we use scikit-learn’s CountVectorizer function.\n\nFirst, it would be interesting to compare unigrams before and after removing stop words.\n\n\n","metadata":{}},{"cell_type":"code","source":"#The distribution of top unigrams before removing stop words in discourse_text:\ndef get_top_n_words_uni(corpus, n=None):\n    vec = CountVectorizer().fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]\n\ncommon_words = get_top_n_words_uni(train_df['discourse_text'], 20)\nfor word, freq in common_words:\n    print(word, freq)\n","metadata":{"execution":{"iopub.status.busy":"2022-02-20T06:00:06.778401Z","iopub.execute_input":"2022-02-20T06:00:06.778625Z","iopub.status.idle":"2022-02-20T06:00:14.810547Z","shell.execute_reply.started":"2022-02-20T06:00:06.7786Z","shell.execute_reply":"2022-02-20T06:00:14.80976Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df1 = pd.DataFrame(common_words, columns = ['disctext' , 'count'])\ndf1 = df1.groupby('disctext').sum()['count'].sort_values(ascending=False)\ndf1.plot(kind = 'bar', figsize = (10,5))\nplt.xlabel('count')\nplt.title('Top 20 words in discourse texts before removing stop words')","metadata":{"execution":{"iopub.status.busy":"2022-02-20T06:00:14.811915Z","iopub.execute_input":"2022-02-20T06:00:14.812248Z","iopub.status.idle":"2022-02-20T06:00:15.049623Z","shell.execute_reply.started":"2022-02-20T06:00:14.812223Z","shell.execute_reply":"2022-02-20T06:00:15.048994Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#The distribution of top unigrams after removing stop words in discourse texts:\ndef get_top_n_words_uni_real(corpus, n=None):\n    vec = CountVectorizer(stop_words = 'english').fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]\n\ncommon_words_real = get_top_n_words_uni_real(train_df['discourse_text'], 20)\nfor word, freq in common_words_real:\n    print(word, freq)","metadata":{"execution":{"iopub.status.busy":"2022-02-20T06:00:41.199513Z","iopub.execute_input":"2022-02-20T06:00:41.199739Z","iopub.status.idle":"2022-02-20T06:00:51.035199Z","shell.execute_reply.started":"2022-02-20T06:00:41.199713Z","shell.execute_reply":"2022-02-20T06:00:51.034529Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df2 = pd.DataFrame(common_words_real, columns = ['disctext' , 'count'])\ndf2 = df2.groupby('disctext').sum()['count'].sort_values(ascending=False)\ndf2.plot(kind = 'bar', figsize = (10,5))\nplt.xlabel('count')\nplt.title('Top 20 words in discourse texts after removing stop words')","metadata":{"execution":{"iopub.status.busy":"2022-02-20T06:00:51.036477Z","iopub.execute_input":"2022-02-20T06:00:51.036724Z","iopub.status.idle":"2022-02-20T06:00:51.275234Z","shell.execute_reply.started":"2022-02-20T06:00:51.036693Z","shell.execute_reply":"2022-02-20T06:00:51.274583Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#Bigrams - Second, we want to compare bigrams before and after removing stop words.\n","metadata":{}},{"cell_type":"code","source":"#The distribution of top bigrams before removing stop words\ndef get_top_n_bigram(corpus, n=None):\n    vec = CountVectorizer(ngram_range=(2, 2)).fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]\n\ncommon_words_bi = get_top_n_bigram(train_df['discourse_text'], 20)\nfor word, freq in common_words_bi:\n    print(word, freq)\n\n","metadata":{"execution":{"iopub.status.busy":"2022-02-20T06:01:10.375789Z","iopub.execute_input":"2022-02-20T06:01:10.37601Z","iopub.status.idle":"2022-02-20T06:01:27.481032Z","shell.execute_reply.started":"2022-02-20T06:01:10.375984Z","shell.execute_reply":"2022-02-20T06:01:27.480399Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df3 = pd.DataFrame(common_words_bi, columns = ['text' , 'count'])\ndf3 = df3.groupby('text').sum()['count'].sort_values(ascending=False)\ndf3.plot(kind = 'bar', figsize = (10,5))\nplt.xlabel('count')\nplt.title('Top 20 bi grams words in discourse texts before removing stop words')\n","metadata":{"execution":{"iopub.status.busy":"2022-02-20T06:01:27.482339Z","iopub.execute_input":"2022-02-20T06:01:27.482568Z","iopub.status.idle":"2022-02-20T06:01:27.727869Z","shell.execute_reply.started":"2022-02-20T06:01:27.482538Z","shell.execute_reply":"2022-02-20T06:01:27.727118Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#The distribution of top bigrams after removing stop words\n\ndef get_top_n_bigram_real(corpus, n=None):\n    vec = CountVectorizer(ngram_range=(2, 2), stop_words='english').fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]\n\ncommon_words_bi_real = get_top_n_bigram_real(train_df['discourse_text'], 20)\nfor word, freq in common_words_bi_real:\n    print(word, freq)\n","metadata":{"execution":{"iopub.status.busy":"2022-02-20T06:01:43.424171Z","iopub.execute_input":"2022-02-20T06:01:43.424721Z","iopub.status.idle":"2022-02-20T06:01:56.894943Z","shell.execute_reply.started":"2022-02-20T06:01:43.424689Z","shell.execute_reply":"2022-02-20T06:01:56.894134Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df4 = pd.DataFrame(common_words_bi_real, columns = ['text' , 'count'])\ndf4 = df4.groupby('text').sum()['count'].sort_values(ascending=False)\ndf4.plot(kind = 'bar', figsize = (10,5))\nplt.xlabel('count')\nplt.title('Top 20 bigrams words in discourse texts after removing stop words')","metadata":{"execution":{"iopub.status.busy":"2022-02-20T06:01:56.896206Z","iopub.execute_input":"2022-02-20T06:01:56.896377Z","iopub.status.idle":"2022-02-20T06:01:57.203603Z","shell.execute_reply.started":"2022-02-20T06:01:56.896356Z","shell.execute_reply":"2022-02-20T06:01:57.20293Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#TRIGRAMS : Last, we compare trigrams before and after removing stop words.","metadata":{}},{"cell_type":"code","source":"#The distribution of Top trigrams before removing stop words\ndef get_top_n_trigram(corpus, n=None):\n    vec = CountVectorizer(ngram_range=(3, 3)).fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]\n\ncommon_words_tri = get_top_n_trigram(train_df['discourse_text'], 20)\nfor word, freq in common_words_tri:\n    print(word, freq)\n\n","metadata":{"execution":{"iopub.status.busy":"2022-02-20T06:01:57.204833Z","iopub.execute_input":"2022-02-20T06:01:57.205108Z","iopub.status.idle":"2022-02-20T06:02:36.002518Z","shell.execute_reply.started":"2022-02-20T06:01:57.205062Z","shell.execute_reply":"2022-02-20T06:02:36.000381Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df5 = pd.DataFrame(common_words_tri, columns = ['text' , 'count'])\ndf5 = df5.groupby('text').sum()['count'].sort_values(ascending=False)\ndf5.plot(kind = 'bar', figsize = (10,5))\nplt.xlabel('count')\nplt.title('Top 20 trigrams words in discourse texts before removing stop words')","metadata":{"execution":{"iopub.status.busy":"2022-02-20T06:02:36.004111Z","iopub.execute_input":"2022-02-20T06:02:36.004481Z","iopub.status.idle":"2022-02-20T06:02:36.262702Z","shell.execute_reply.started":"2022-02-20T06:02:36.004451Z","shell.execute_reply":"2022-02-20T06:02:36.26209Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#The distribution of Top trigrams after removing stop words\ndef get_top_n_trigram_real(corpus, n=None):\n    vec = CountVectorizer(ngram_range=(3, 3), stop_words='english').fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]\n\ncommon_words_tri_real = get_top_n_trigram_real(train_df['discourse_text'], 20)\nfor word, freq in common_words_tri_real:\n    print(word, freq)\n","metadata":{"execution":{"iopub.status.busy":"2022-02-20T06:02:36.263603Z","iopub.execute_input":"2022-02-20T06:02:36.263845Z","iopub.status.idle":"2022-02-20T06:02:53.762477Z","shell.execute_reply.started":"2022-02-20T06:02:36.26382Z","shell.execute_reply":"2022-02-20T06:02:53.761787Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df6 = pd.DataFrame(common_words_tri_real, columns = ['text' , 'count'])\ndf6 = df6.groupby('text').sum()['count'].sort_values(ascending=False)\ndf6.plot(kind = 'bar', figsize = (10,5))\nplt.xlabel('count')\nplt.title('Top 20 trigrams words in discourse texts after removing stop words')","metadata":{"execution":{"iopub.status.busy":"2022-02-20T06:02:53.763601Z","iopub.execute_input":"2022-02-20T06:02:53.763804Z","iopub.status.idle":"2022-02-20T06:02:54.081539Z","shell.execute_reply.started":"2022-02-20T06:02:53.763777Z","shell.execute_reply":"2022-02-20T06:02:54.080866Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Most popular uni, bi tri grams in discourse_text:\n\n1. 'Student', 'people', 'school' , 'help' are the top words in discourse texts (based on world cloud and unigrams)\n2. 'electoral college', 'driverless cars', 'cell phones' , 'community service'  are the most popuar BIGRAMS.\n3. the most popular TRIGRAMS are : 'facial action coding', 'attend classes home', 'limiting car usage' \n","metadata":{}},{"cell_type":"markdown","source":"# Essay EDA:\nLet's take a look at the 15594 essays in the train_text files.","metadata":{}},{"cell_type":"code","source":"# let's load all essays:\n\ntexts = []\nfor file in train_text_files :\n    with open(f'/kaggle/input/feedback-prize-2021/train/{file}') as f:\n        lines = f.readlines()\n    texts.append({'id': file[:-4], 'text': ''.join(lines)})\ntexts_df = pd.DataFrame(texts)","metadata":{"execution":{"iopub.status.busy":"2022-02-20T06:03:05.776556Z","iopub.execute_input":"2022-02-20T06:03:05.776975Z","iopub.status.idle":"2022-02-20T06:03:49.015932Z","shell.execute_reply.started":"2022-02-20T06:03:05.776934Z","shell.execute_reply":"2022-02-20T06:03:49.014898Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"texts_df.head()\n","metadata":{"execution":{"iopub.status.busy":"2022-02-20T06:03:49.01733Z","iopub.execute_input":"2022-02-20T06:03:49.017533Z","iopub.status.idle":"2022-02-20T06:03:49.025498Z","shell.execute_reply.started":"2022-02-20T06:03:49.017507Z","shell.execute_reply":"2022-02-20T06:03:49.025051Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"texts_df.info()","metadata":{"execution":{"iopub.status.busy":"2022-02-20T06:03:49.026357Z","iopub.execute_input":"2022-02-20T06:03:49.026931Z","iopub.status.idle":"2022-02-20T06:03:49.045121Z","shell.execute_reply.started":"2022-02-20T06:03:49.026904Z","shell.execute_reply":"2022-02-20T06:03:49.044439Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#length of eassays :\ntexts_df['len'] = texts_df['text'].apply(len)\ntexts_df['len'].hist(bins = 50, figsize = (10,6))\nplt.title('Length of Essays')\nprint(texts_df['len'].min(), texts_df['len'].max())","metadata":{"execution":{"iopub.status.busy":"2022-02-20T06:03:49.046787Z","iopub.execute_input":"2022-02-20T06:03:49.047006Z","iopub.status.idle":"2022-02-20T06:03:49.262453Z","shell.execute_reply.started":"2022-02-20T06:03:49.046977Z","shell.execute_reply":"2022-02-20T06:03:49.261639Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# number of words in the essays:\ntexts_df['words_num'] = texts_df['text'].apply(lambda x: len(x.split(' ')))","metadata":{"execution":{"iopub.status.busy":"2022-02-20T06:03:49.263647Z","iopub.execute_input":"2022-02-20T06:03:49.263929Z","iopub.status.idle":"2022-02-20T06:03:49.558053Z","shell.execute_reply.started":"2022-02-20T06:03:49.263891Z","shell.execute_reply":"2022-02-20T06:03:49.557575Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"texts_df['words_num'].hist(bins = 100, figsize = (10,6))\nplt.title('Word Count distribution in the essays')\n\n        ","metadata":{"execution":{"iopub.status.busy":"2022-02-20T06:03:49.558838Z","iopub.execute_input":"2022-02-20T06:03:49.559134Z","iopub.status.idle":"2022-02-20T06:03:49.870199Z","shell.execute_reply.started":"2022-02-20T06:03:49.559106Z","shell.execute_reply":"2022-02-20T06:03:49.869531Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Minimum no. of words {} and max no. of words {} in the Essays'.format(texts_df['words_num'].min(), texts_df['words_num'].max()))\nprint('Minimum length of essays is {} and maximum length of essays is {}'.format(texts_df['len'].min(), texts_df['len'].max()))      ","metadata":{"execution":{"iopub.status.busy":"2022-02-20T06:03:49.871011Z","iopub.execute_input":"2022-02-20T06:03:49.8712Z","iopub.status.idle":"2022-02-20T06:03:49.878661Z","shell.execute_reply.started":"2022-02-20T06:03:49.871174Z","shell.execute_reply":"2022-02-20T06:03:49.87825Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# function to plot world cloud for Essays:\ndef plot_wordcloud_essay(column, title):\n    \n    \"\"\"\n    Function to Plot Wordcloud of given dataframe column.\n    \n    params: column(string): The Column of the DataFrame for plotting.\n            title(string) : The Title of the Wordcloud.\n    \"\"\"\n    # Define stopwords\n    stopwords = set(STOPWORDS) \n    \n    # Define the Wordcloud    \n    wordcloud = WordCloud(width = 800, \n                          height = 800,\n                          background_color ='black',\n                          min_font_size = 10,\n                          stopwords = stopwords).generate(' '.join(texts_df[column])) \n\n    # Plot the WordCloud image                        \n    plt.figure(figsize = (8, 8), facecolor = None) \n    plt.imshow(wordcloud) \n    plt.axis(\"off\") \n    plt.tight_layout(pad = 0) \n    plt.title('Wordcloud: ' + title, fontsize = 20)\n\n    plt.show() ","metadata":{"execution":{"iopub.status.busy":"2022-02-20T06:03:58.597438Z","iopub.execute_input":"2022-02-20T06:03:58.598066Z","iopub.status.idle":"2022-02-20T06:03:58.604444Z","shell.execute_reply.started":"2022-02-20T06:03:58.598031Z","shell.execute_reply":"2022-02-20T06:03:58.603467Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#World Cloud of the Essays:\nplot_wordcloud_essay(column = 'text', title = 'Most frequent words in the Essays')","metadata":{"execution":{"iopub.status.busy":"2022-02-20T06:04:03.952924Z","iopub.execute_input":"2022-02-20T06:04:03.953178Z","iopub.status.idle":"2022-02-20T06:04:21.579641Z","shell.execute_reply.started":"2022-02-20T06:04:03.953146Z","shell.execute_reply":"2022-02-20T06:04:21.578979Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#UNIGRAM of Essays without removing stop words:\ncommon_words_ess = get_top_n_words_uni(texts_df['text'], 20)\nfor word, freq in common_words_ess:\n    print(word, freq)\n","metadata":{"execution":{"iopub.status.busy":"2022-02-20T06:05:32.818381Z","iopub.execute_input":"2022-02-20T06:05:32.81869Z","iopub.status.idle":"2022-02-20T06:05:40.355906Z","shell.execute_reply.started":"2022-02-20T06:05:32.818642Z","shell.execute_reply":"2022-02-20T06:05:40.35528Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_ess_1 = pd.DataFrame(common_words_ess, columns = ['text' , 'count'])\ndf_ess_1 = df_ess_1.groupby('text').sum()['count'].sort_values(ascending=False)\ndf_ess_1.plot(kind = 'bar', figsize = (10,5))\nplt.xlabel('count')\nplt.title('Top 20 words in Essays before removing stop words')","metadata":{"execution":{"iopub.status.busy":"2022-02-20T06:05:50.270166Z","iopub.execute_input":"2022-02-20T06:05:50.270787Z","iopub.status.idle":"2022-02-20T06:05:50.505457Z","shell.execute_reply.started":"2022-02-20T06:05:50.270753Z","shell.execute_reply":"2022-02-20T06:05:50.504858Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#The distribution of top unigrams after removing stop words in Essays:\n\ncommon_words_real_ess = get_top_n_words_uni_real(texts_df['text'], 20)\nfor word, freq in common_words_real_ess:\n    print(word, freq)","metadata":{"execution":{"iopub.status.busy":"2022-02-20T06:07:04.244379Z","iopub.execute_input":"2022-02-20T06:07:04.245629Z","iopub.status.idle":"2022-02-20T06:07:11.574675Z","shell.execute_reply.started":"2022-02-20T06:07:04.245581Z","shell.execute_reply":"2022-02-20T06:07:11.573982Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_ess_2 = pd.DataFrame(common_words_real_ess, columns = ['text' , 'count'])\ndf_ess_2 = df_ess_2.groupby('text').sum()['count'].sort_values(ascending=False)\ndf_ess_2.plot(kind = 'bar', figsize = (10,5))\nplt.xlabel('count')\nplt.title('Top 20 words in Essays after removing stop words')","metadata":{"execution":{"iopub.status.busy":"2022-02-20T06:07:54.162459Z","iopub.execute_input":"2022-02-20T06:07:54.162692Z","iopub.status.idle":"2022-02-20T06:07:54.446152Z","shell.execute_reply.started":"2022-02-20T06:07:54.162668Z","shell.execute_reply":"2022-02-20T06:07:54.445586Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#The distribution of top bigrams in Essays after removing stop words\n\ncommon_words_bi_real_ess = get_top_n_bigram_real(texts_df['text'], 20)\nfor word, freq in common_words_bi_real_ess:\n    print(word, freq)","metadata":{"execution":{"iopub.status.busy":"2022-02-20T06:09:14.186535Z","iopub.execute_input":"2022-02-20T06:09:14.18678Z","iopub.status.idle":"2022-02-20T06:09:42.01939Z","shell.execute_reply.started":"2022-02-20T06:09:14.186754Z","shell.execute_reply":"2022-02-20T06:09:42.018712Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_ess_3 = pd.DataFrame(common_words_bi_real_ess, columns = ['text' , 'count'])\ndf_ess_3 = df_ess_3.groupby('text').sum()['count'].sort_values(ascending=False)\ndf_ess_3.plot(kind = 'bar', figsize = (10,5))\nplt.xlabel('count')\nplt.title('Top 20 Bigrams words in Essays after removing stop words')","metadata":{"execution":{"iopub.status.busy":"2022-02-20T06:10:58.891911Z","iopub.execute_input":"2022-02-20T06:10:58.892724Z","iopub.status.idle":"2022-02-20T06:10:59.205108Z","shell.execute_reply.started":"2022-02-20T06:10:58.892684Z","shell.execute_reply":"2022-02-20T06:10:59.204447Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#The distribution of Top trigrams in Essays after removing stop words\ncommon_words_tri_real_ess = get_top_n_trigram_real(texts_df['text'], 20)\nfor word, freq in common_words_tri_real_ess:\n    print(word, freq)","metadata":{"execution":{"iopub.status.busy":"2022-02-20T06:12:39.193294Z","iopub.execute_input":"2022-02-20T06:12:39.193542Z","iopub.status.idle":"2022-02-20T06:13:01.43907Z","shell.execute_reply.started":"2022-02-20T06:12:39.193517Z","shell.execute_reply":"2022-02-20T06:13:01.43616Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_ess_4 = pd.DataFrame(common_words_tri_real_ess, columns = ['text' , 'count'])\ndf_ess_4 = df_ess_4.groupby('text').sum()['count'].sort_values(ascending=False)\ndf_ess_4.plot(kind = 'bar', figsize = (10,5))\nplt.xlabel('count')\nplt.title('Top 20 Trigrams words in Essays after removing stop words')","metadata":{"execution":{"iopub.status.busy":"2022-02-20T06:13:25.790202Z","iopub.execute_input":"2022-02-20T06:13:25.790473Z","iopub.status.idle":"2022-02-20T06:13:26.047798Z","shell.execute_reply.started":"2022-02-20T06:13:25.790444Z","shell.execute_reply":"2022-02-20T06:13:26.047139Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Most popular uni, bi tri grams in discourse_text:\n\n1. 'Student', 'people', 'school' , 'help' are the top words in discourse texts (based on world cloud and unigrams)\n2. 'electoral college', 'driverless cars', 'cell phones' , 'community service'  are the most popuar BIGRAMS.\n3. the most popular TRIGRAMS are : 'facial action coding', 'attend classes home', 'limiting car usage' \n\nWHILE\n\n# Most popular uni, bi tri grams in Essays:\n\n1. 'Student', 'people', 'school' , 'help' are the top words in Essays (based on world cloud and unigrams)\n2. 'electoral college', 'driverless cars', 'cell phones' , 'community service'  are the most popuar BIGRAMS.\n\n3. the most popular TRIGRAMS are : 'facial action coding', 'attend classes home', 'limiting car usage' \n\nBoth are similar in every aspect. As expected as the discourses are annotated essays!","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}