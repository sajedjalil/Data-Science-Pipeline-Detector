{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Libraries","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport tensorflow as tf\n\nfrom transformers import *","metadata":{"execution":{"iopub.status.busy":"2022-01-08T15:46:46.547075Z","iopub.execute_input":"2022-01-08T15:46:46.547486Z","iopub.status.idle":"2022-01-08T15:46:57.333858Z","shell.execute_reply.started":"2022-01-08T15:46:46.547389Z","shell.execute_reply":"2022-01-08T15:46:57.333024Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df = pd.read_csv('../input/feedback-prize-2021/train.csv')\n\nprint(f'Shape of Train : {train_df.shape}')\ntrain_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-08T15:46:57.335811Z","iopub.execute_input":"2022-01-08T15:46:57.336066Z","iopub.status.idle":"2022-01-08T15:46:59.106146Z","shell.execute_reply.started":"2022-01-08T15:46:57.33603Z","shell.execute_reply":"2022-01-08T15:46:59.105396Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"IDS = train_df['id'].unique()\nLABELS = train_df['discourse_type'].unique()\n\nprint(f'Count of IDs : {len(IDS)}')\nprint(f'Labels : {LABELS}')","metadata":{"execution":{"iopub.status.busy":"2022-01-08T15:46:59.107746Z","iopub.execute_input":"2022-01-08T15:46:59.108285Z","iopub.status.idle":"2022-01-08T15:46:59.141331Z","shell.execute_reply.started":"2022-01-08T15:46:59.108246Z","shell.execute_reply":"2022-01-08T15:46:59.140638Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Tokenize Train","metadata":{}},{"cell_type":"code","source":"MAX_LEN = 1024\nPRE_PATH = '../input/allenailongformerbase4096'\n\ntokenizer = AutoTokenizer.from_pretrained(PRE_PATH)\n\ntrain_tokens = np.zeros((len(IDS), MAX_LEN), dtype='int32')\ntrain_attention = np.zeros((len(IDS), MAX_LEN), dtype='int32')\n\n# the 14 classes for NER\nlead_b = np.zeros((len(IDS),MAX_LEN))\nlead_i = np.zeros((len(IDS),MAX_LEN))\n\nposition_b = np.zeros((len(IDS),MAX_LEN))\nposition_i = np.zeros((len(IDS),MAX_LEN))\n\nevidence_b = np.zeros((len(IDS),MAX_LEN))\nevidence_i = np.zeros((len(IDS),MAX_LEN))\n\nclaim_b = np.zeros((len(IDS),MAX_LEN))\nclaim_i = np.zeros((len(IDS),MAX_LEN))\n\nconclusion_b = np.zeros((len(IDS),MAX_LEN))\nconclusion_i = np.zeros((len(IDS),MAX_LEN))\n\ncounterclaim_b = np.zeros((len(IDS),MAX_LEN))\ncounterclaim_i = np.zeros((len(IDS),MAX_LEN))\n\nrebuttal_b = np.zeros((len(IDS),MAX_LEN))\nrebuttal_i = np.zeros((len(IDS),MAX_LEN))","metadata":{"execution":{"iopub.status.busy":"2022-01-08T15:46:59.143323Z","iopub.execute_input":"2022-01-08T15:46:59.143541Z","iopub.status.idle":"2022-01-08T15:46:59.340305Z","shell.execute_reply.started":"2022-01-08T15:46:59.143511Z","shell.execute_reply":"2022-01-08T15:46:59.339558Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_lens = []\ntargets_b = [lead_b, position_b, evidence_b, claim_b, conclusion_b, counterclaim_b, rebuttal_b]\ntargets_i = [lead_i, position_i, evidence_i, claim_i, conclusion_i, counterclaim_i, rebuttal_i]\ntarget_map = {'Lead':0, 'Position':1, 'Evidence':2, 'Claim':3, 'Concluding Statement':4,\n             'Counterclaim':5, 'Rebuttal':6}\n\n# Create Token\nfor i in range(len(IDS)):\n    \n    doc_id = IDS[i]\n    \n    doc_file = f'../input/feedback-prize-2021/train/{doc_id}.txt'\n    \n    doc_txt = open(doc_file, 'r').read()\n    \n    train_lens.append(len(doc_txt.split()))\n    \n    tokens = tokenizer.encode_plus(doc_txt,\n                                  max_length=MAX_LEN,\n                                  padding='max_length',\n                                  truncation=True,\n                                  return_offsets_mapping=True)\n    \n    train_tokens[i, ] = tokens['input_ids']\n    train_attention[i, ] = tokens['attention_mask']\n    \n    # find targets in text and save in target arrays\n    # offset_mappings are maps from tokens to the original texts\n    offsets = tokens['offset_mapping']\n    offset_index = 0\n    \n    doc_info_df = train_df[train_df['id']==doc_id]\n    \n    for index, row in doc_info_df.iterrows():\n        \n        a = row['discourse_start']\n        b = row['discourse_end']\n        \n        if offset_index > len(offsets)-1 :\n            break\n        \n        c = offsets[offset_index][0]\n        d = offsets[offset_index][1]\n        \n        beginning = True\n        \n        while b > c :\n            if (c >= a) & (b >= d):\n                target_num = target_map[row['discourse_type']]\n                if beginning:\n                    targets_b[target_num][i][offset_index] = 1\n                    beginning = False\n                else:\n                    targets_i[target_num][i][offset_index] = 1\n            \n            offset_index += 1\n            \n            if offset_index > len(offsets)-1:\n                break\n            c = offsets[offset_index][0]\n            d = offsets[offset_index][1]","metadata":{"execution":{"iopub.status.busy":"2022-01-08T15:46:59.343077Z","iopub.execute_input":"2022-01-08T15:46:59.343296Z","iopub.status.idle":"2022-01-08T15:55:19.024941Z","shell.execute_reply.started":"2022-01-08T15:46:59.343271Z","shell.execute_reply":"2022-01-08T15:55:19.024219Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# check tokens and attention\nprint(f'train_tokens \\n{train_tokens}')\nprint('\\n')\nprint(f'train_attention \\n{train_attention}')","metadata":{"execution":{"iopub.status.busy":"2022-01-08T15:55:19.026229Z","iopub.execute_input":"2022-01-08T15:55:19.026465Z","iopub.status.idle":"2022-01-08T15:55:19.035545Z","shell.execute_reply.started":"2022-01-08T15:55:19.026434Z","shell.execute_reply":"2022-01-08T15:55:19.034788Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(1, 1, figsize=(12, 5))\n\nplt.hist(train_lens, bins=100)\nplt.title('Histogram of Train word counts')\nplt. xlabel('Train word count')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-01-08T15:55:19.036577Z","iopub.execute_input":"2022-01-08T15:55:19.036977Z","iopub.status.idle":"2022-01-08T15:55:19.484613Z","shell.execute_reply.started":"2022-01-08T15:55:19.036939Z","shell.execute_reply":"2022-01-08T15:55:19.483929Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#  target array (15594, 1024, 15)\n# 15 means 0 + 14 classes\ntargets = np.zeros((len(IDS), MAX_LEN, 15), dtype='int32')\n\nfor k in range(7):\n    targets[:, :, 2*k] = targets_b[k]\n    targets[:, :, 2*k+1] = targets_i[k]\n\ntargets[:, :, 14] = 1-np.max(targets, axis=-1)","metadata":{"execution":{"iopub.status.busy":"2022-01-08T15:55:19.485854Z","iopub.execute_input":"2022-01-08T15:55:19.486248Z","iopub.status.idle":"2022-01-08T15:55:22.336656Z","shell.execute_reply.started":"2022-01-08T15:55:19.486212Z","shell.execute_reply":"2022-01-08T15:55:22.335946Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Save\nnp.save(f'targets_{MAX_LEN}', targets)\nnp.save(f'tokens_{MAX_LEN}', train_tokens)\nnp.save(f'attention_{MAX_LEN}', train_attention)","metadata":{"execution":{"iopub.status.busy":"2022-01-08T15:55:22.337954Z","iopub.execute_input":"2022-01-08T15:55:22.338209Z","iopub.status.idle":"2022-01-08T15:55:23.085911Z","shell.execute_reply.started":"2022-01-08T15:55:22.338176Z","shell.execute_reply":"2022-01-08T15:55:23.085152Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model\nWe will use LongFormer backbone and add our own NER head using one hidden layer of size 256 and one final layer with softmax.  \nWe use 15 classes because we have a B class and I class for each of 7 labels.  \nAnd we have an additional class (called 0 class) for tokens that do not belong to one of 14 classes.","metadata":{}},{"cell_type":"code","source":"def build_model():\n    \n    tokens = tf.keras.layers.Input(shape=(MAX_LEN, ), name='tokens', dtype=tf.int32)\n    attention = tf.keras.layers.Input(shape=(MAX_LEN, ), name='attention', dtype=tf.int32)\n    \n    config = AutoConfig.from_pretrained(PRE_PATH + '/config.json')\n    backbone = TFAutoModel.from_pretrained(PRE_PATH + '/tf_model.h5', config=config)\n    \n    x = backbone(tokens, attention_mask=attention)     # LongFormer backbone\n    x = tf.keras.layers.Dense(256, activation='relu')(x[0])     # NER head\n    x = tf.keras.layers.Dense(15, activation='softmax', dtype='float32')(x)     # final softmax layer\n    \n    model = tf.keras.Model(inputs=[tokens, attention], outputs=x)\n    \n    model.compile(\n                optimizer = tf.keras.optimizers.Adam(lr=1e-4),\n                loss = [tf.keras.losses.CategoricalCrossentropy()],\n                metrics = [tf.keras.metrics.CategoricalAccuracy()]\n                )\n    \n    return model","metadata":{"execution":{"iopub.status.busy":"2022-01-08T15:55:23.088935Z","iopub.execute_input":"2022-01-08T15:55:23.089275Z","iopub.status.idle":"2022-01-08T15:55:23.098113Z","shell.execute_reply.started":"2022-01-08T15:55:23.089239Z","shell.execute_reply":"2022-01-08T15:55:23.097462Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = build_model()","metadata":{"execution":{"iopub.status.busy":"2022-01-08T15:55:23.099328Z","iopub.execute_input":"2022-01-08T15:55:23.099709Z","iopub.status.idle":"2022-01-08T15:55:57.932657Z","shell.execute_reply.started":"2022-01-08T15:55:23.099674Z","shell.execute_reply":"2022-01-08T15:55:57.931946Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.summary()","metadata":{"execution":{"iopub.status.busy":"2022-01-08T15:55:57.933709Z","iopub.execute_input":"2022-01-08T15:55:57.933963Z","iopub.status.idle":"2022-01-08T15:55:57.961248Z","shell.execute_reply.started":"2022-01-08T15:55:57.93393Z","shell.execute_reply":"2022-01-08T15:55:57.960578Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train Model\nWhen training on Kaggle's 1xP100 GPU, we need to reduce the batch size to 4.  \nAnd we reduce the learning rates to 0.25e-4 and 0.25e-5.  \nEach training epoch on Kaggle takes 1 hour 8 minutes.","metadata":{}},{"cell_type":"code","source":"# learning rate schedule and model checkpoint\nEPOCHS = 1\nBATCH_SIZE = 4\nLRS = [0.25e-4, 0.25e-4, 0.25e-4, 0.25e-4, 0.25e-5]\n\ndef lrfn(epoch):\n    return LRS[epoch]\n\nlr_callback = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose=True)\n\n# train valid split 90 : 10\nnp.random.seed(42)\ntrain_idx = np.random.choice(np.arange(len(IDS)), int(0.9*len(IDS)), replace=False)\nvalid_idx = np.setdiff1d(np.arange(len(IDS)), train_idx)\nnp.random.seed(None)\nprint(f'Train size {len(train_idx)}')\nprint(f'Valid size {len(valid_idx)}')","metadata":{"execution":{"iopub.status.busy":"2022-01-08T15:55:57.962418Z","iopub.execute_input":"2022-01-08T15:55:57.962638Z","iopub.status.idle":"2022-01-08T15:55:57.976115Z","shell.execute_reply.started":"2022-01-08T15:55:57.962607Z","shell.execute_reply":"2022-01-08T15:55:57.975293Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train model (3h:43)\nmodel.fit(x = [train_tokens[train_idx, ], train_attention[train_idx, ]], \n         y = targets[train_idx, ], \n         validation_data = ([train_tokens[valid_idx, ], train_attention[valid_idx, ]],\n                            targets[valid_idx, ]), \n         callbacks = [lr_callback],\n         epochs = EPOCHS,\n         batch_size = BATCH_SIZE,\n         verbose = 2)","metadata":{"execution":{"iopub.status.busy":"2022-01-08T15:55:57.97721Z","iopub.execute_input":"2022-01-08T15:55:57.977403Z","iopub.status.idle":"2022-01-08T17:05:26.715945Z","shell.execute_reply.started":"2022-01-08T15:55:57.977381Z","shell.execute_reply":"2022-01-08T17:05:26.715007Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# save model weights\nmodel.save_weights('longformer_v1.h5')","metadata":{"execution":{"iopub.status.busy":"2022-01-08T17:05:26.71755Z","iopub.execute_input":"2022-01-08T17:05:26.717887Z","iopub.status.idle":"2022-01-08T17:05:27.918129Z","shell.execute_reply.started":"2022-01-08T17:05:26.717849Z","shell.execute_reply":"2022-01-08T17:05:27.917341Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Validation Model - Infer OOF\nWe will now make predictions on the validation texts.  \nOur model makes label predictions for each token, we need to convert this into a list of word indices for each label.  \nNote that the tokens and words are not the same.  \nA single word may be broken into multiple tokens.  \nTherefore we need to first create a map to change token indices to word indices.","metadata":{}},{"cell_type":"code","source":"p = model.predict([train_tokens[valid_idx], train_attention[valid_idx]],\n                 batch_size=16, verbose=2)\n\nprint(f'OOF predictions shape : {p.shape}')\noof_preds = np.argmax(p, axis=-1)","metadata":{"execution":{"iopub.status.busy":"2022-01-08T17:05:27.919644Z","iopub.execute_input":"2022-01-08T17:05:27.91988Z","iopub.status.idle":"2022-01-08T17:09:05.093269Z","shell.execute_reply.started":"2022-01-08T17:05:27.919847Z","shell.execute_reply":"2022-01-08T17:09:05.092426Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"target_map_rev = {0 : 'Lead', 1 : 'Position', 2 : 'Evidence', 3 : 'Claim', \n                 4 : 'Concluding Statement', 5 : 'Conterclaim', 6 : 'Rebuttal', 7 : 'blank'}","metadata":{"execution":{"iopub.status.busy":"2022-01-08T17:09:05.094726Z","iopub.execute_input":"2022-01-08T17:09:05.095212Z","iopub.status.idle":"2022-01-08T17:09:05.100956Z","shell.execute_reply.started":"2022-01-08T17:09:05.09517Z","shell.execute_reply":"2022-01-08T17:09:05.100127Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_preds(dataset='train', verbose=True, text_ids=IDS[valid_idx], preds=oof_preds):\n    \n    all_predictions = []\n    \n    for id_num in range(len(preds)):\n        \n        # get id\n        n = text_ids[id_num]\n        \n        # get token positions in chars\n        name = f'../input/feedback-prize-2021/{dataset}/{n}.txt'\n        txt = open(name, 'r').read()\n        tokens = tokenizer.encode_plus(txt, \n                                       max_length=MAX_LEN, \n                                      padding='max_length', \n                                      truncation=True, \n                                      return_offsets_mapping=True)\n        off = tokens['offset_mapping']\n        \n        # get word positions in chars\n        w = []\n        blank = True\n        for i in range(len(txt)):\n            if (txt[i] != ' ')&(txt[i] != '\\n')&(txt[i] != '\\xa0')&(txt[i] != '\\x85')&(blank==True):\n                w.append(i)\n                blank=False\n            elif (txt[i] == ' ')|(txt[i] == '\\n')|(txt[i] == '\\xa0')|(txt[i] == '\\x85'):\n                blank=True\n        w.append(1e6)\n        \n        # mapping from tokens to words\n        word_map = -1 * np.ones(MAX_LEN, dtype='int32')\n        w_i = 0\n        for i in range(len(off)):\n            if off[i][1]==0:     # attention\n                continue\n            while off[i][0] >= w[w_i+1]:\n                w_i += 1\n            word_map[i] = int(w_i)\n            \n        # convert token predicitons into word labels\n        # 0: LEAD_B, 1: LEAD_I\n        # 2: POSITION_B, 3: POSITION_I\n        # 4: EVIDENCE_B, 5: EVIDENCE_I\n        # 6: CLAIM_B, 7: CLAIM_I\n        # 8: CONCLUSION_B, 9: CONCLUSION_I\n        # 10: COUNTERCLAIM_B, 11: COUNTERCLAIM_I\n        # 12: REBUTTAL_B, 13: REBUTTAL_I\n        # 14: NOTHING\n        # note these values are divided by 2 in next code line\n        pred = preds[id_num, ] / 2.0\n        \n        i = 0\n        while i < MAX_LEN:\n            prediction = []\n            start = pred[i]\n            if start in [0,1,2,3,4,5,6,7]:\n                prediction.append(word_map[i])\n                i += 1\n                if i >= MAX_LEN: \n                    break\n                while pred[i]==start+0.5:\n                    if not word_map[i] in prediction:\n                        prediction.append(word_map[i])\n                    i += 1\n                    if i >= MAX_LEN:\n                        break\n            else:\n                i += 1\n            \n            prediction = [x for x in prediction if x!=-1]\n            if len(prediction)>4:\n                all_predictions.append((n, target_map_rev[int(start)], ' '.join([str(x) for x in prediction])))\n                \n    df = pd.DataFrame(all_predictions)\n    df.columns = ['id', 'class', 'predictionstring']\n    \n    return df\n            ","metadata":{"execution":{"iopub.status.busy":"2022-01-08T17:09:05.102189Z","iopub.execute_input":"2022-01-08T17:09:05.102594Z","iopub.status.idle":"2022-01-08T17:09:05.121676Z","shell.execute_reply.started":"2022-01-08T17:09:05.102557Z","shell.execute_reply":"2022-01-08T17:09:05.12088Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"oof = get_preds( dataset='train', verbose=True, text_ids=IDS[valid_idx] )\noof.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-08T17:09:05.123128Z","iopub.execute_input":"2022-01-08T17:09:05.123633Z","iopub.status.idle":"2022-01-08T17:09:27.410815Z","shell.execute_reply.started":"2022-01-08T17:09:05.123595Z","shell.execute_reply":"2022-01-08T17:09:27.410144Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# CODE FROM : Rob Mulla @robikscube\n# https://www.kaggle.com/robikscube/student-writing-competition-twitch\ndef calc_overlap(row):\n    \"\"\"\n    Calculates the overlap between prediction and\n    ground truth and overlap percentages used for determining\n    true positives.\n    \"\"\"\n    set_pred = set(row.predictionstring_pred.split(' '))\n    set_gt = set(row.predictionstring_gt.split(' '))\n    # Length of each and intersection\n    len_gt = len(set_gt)\n    len_pred = len(set_pred)\n    inter = len(set_gt.intersection(set_pred))\n    overlap_1 = inter / len_gt\n    overlap_2 = inter/ len_pred\n    return [overlap_1, overlap_2]\n\n\ndef score_feedback_comp(pred_df, gt_df):\n    \"\"\"\n    A function that scores for the kaggle\n        Student Writing Competition\n        \n    Uses the steps in the evaluation page here:\n        https://www.kaggle.com/c/feedback-prize-2021/overview/evaluation\n    \"\"\"\n    gt_df = gt_df[['id','discourse_type','predictionstring']] \\\n        .reset_index(drop=True).copy()\n    pred_df = pred_df[['id','class','predictionstring']] \\\n        .reset_index(drop=True).copy()\n    pred_df['pred_id'] = pred_df.index\n    gt_df['gt_id'] = gt_df.index\n    # Step 1. all ground truths and predictions for a given class are compared.\n    joined = pred_df.merge(gt_df,\n                           left_on=['id','class'],\n                           right_on=['id','discourse_type'],\n                           how='outer',\n                           suffixes=('_pred','_gt')\n                          )\n    joined['predictionstring_gt'] = joined['predictionstring_gt'].fillna(' ')\n    joined['predictionstring_pred'] = joined['predictionstring_pred'].fillna(' ')\n\n    joined['overlaps'] = joined.apply(calc_overlap, axis=1)\n\n    # 2. If the overlap between the ground truth and prediction is >= 0.5, \n    # and the overlap between the prediction and the ground truth >= 0.5,\n    # the prediction is a match and considered a true positive.\n    # If multiple matches exist, the match with the highest pair of overlaps is taken.\n    joined['overlap1'] = joined['overlaps'].apply(lambda x: eval(str(x))[0])\n    joined['overlap2'] = joined['overlaps'].apply(lambda x: eval(str(x))[1])\n\n\n    joined['potential_TP'] = (joined['overlap1'] >= 0.5) & (joined['overlap2'] >= 0.5)\n    joined['max_overlap'] = joined[['overlap1','overlap2']].max(axis=1)\n    tp_pred_ids = joined.query('potential_TP') \\\n        .sort_values('max_overlap', ascending=False) \\\n        .groupby(['id','predictionstring_gt']).first()['pred_id'].values\n\n    # 3. Any unmatched ground truths are false negatives\n    # and any unmatched predictions are false positives.\n    fp_pred_ids = [p for p in joined['pred_id'].unique() if p not in tp_pred_ids]\n\n    matched_gt_ids = joined.query('potential_TP')['gt_id'].unique()\n    unmatched_gt_ids = [c for c in joined['gt_id'].unique() if c not in matched_gt_ids]\n\n    # Get numbers of each type\n    TP = len(tp_pred_ids)\n    FP = len(fp_pred_ids)\n    FN = len(unmatched_gt_ids)\n    #calc microf1\n    my_f1_score = TP / (TP + 0.5*(FP+FN))\n    return my_f1_score","metadata":{"execution":{"iopub.status.busy":"2022-01-08T17:09:27.411897Z","iopub.execute_input":"2022-01-08T17:09:27.41215Z","iopub.status.idle":"2022-01-08T17:09:27.4277Z","shell.execute_reply.started":"2022-01-08T17:09:27.412091Z","shell.execute_reply":"2022-01-08T17:09:27.427062Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# VALID DATAFRAME\nvalid = train_df.loc[train_df['id'].isin(IDS[valid_idx])]","metadata":{"execution":{"iopub.status.busy":"2022-01-08T17:09:27.429049Z","iopub.execute_input":"2022-01-08T17:09:27.429495Z","iopub.status.idle":"2022-01-08T17:09:27.465294Z","shell.execute_reply.started":"2022-01-08T17:09:27.429462Z","shell.execute_reply":"2022-01-08T17:09:27.464633Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f1s = []\nCLASSES = oof['class'].unique()\nfor c in CLASSES:\n    pred_df = oof.loc[oof['class']==c].copy()\n    gt_df = valid.loc[valid['discourse_type']==c].copy()\n    f1 = score_feedback_comp(pred_df, gt_df)\n    print(c,f1)\n    f1s.append(f1)\nprint()\nprint('Overall',np.mean(f1s))","metadata":{"execution":{"iopub.status.busy":"2022-01-08T17:09:27.46691Z","iopub.execute_input":"2022-01-08T17:09:27.467366Z","iopub.status.idle":"2022-01-08T17:09:29.328028Z","shell.execute_reply.started":"2022-01-08T17:09:27.467333Z","shell.execute_reply":"2022-01-08T17:09:29.327324Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Inference","metadata":{}},{"cell_type":"code","source":"test_files = os.listdir('../input/feedback-prize-2021/test')\ntest_ids = [f.replace('.txt', '') for f in test_files if 'txt' in f]\n\ntest_tokens = np.zeros((len(test_ids), MAX_LEN), dtype='int32')\ntest_attention = np.zeros((len(test_ids), MAX_LEN), dtype='int32')\n\nfor i in range(len(test_ids)):\n    \n    doc_id = test_ids[i]\n    \n    doc_file = f'../input/feedback-prize-2021/test/{doc_id}.txt'\n    \n    doc_txt = open(doc_file, 'r').read()\n    \n    tokens = tokenizer.encode_plus(doc_txt,\n                                  max_length=MAX_LEN,\n                                  padding='max_length',\n                                  truncation=True,\n                                  return_offsets_mapping=True)\n    \n    test_tokens[i, ] = tokens['input_ids']\n    test_attention[i, ] = tokens['attention_mask']","metadata":{"execution":{"iopub.status.busy":"2022-01-08T17:09:29.329353Z","iopub.execute_input":"2022-01-08T17:09:29.329592Z","iopub.status.idle":"2022-01-08T17:09:29.387487Z","shell.execute_reply.started":"2022-01-08T17:09:29.32956Z","shell.execute_reply":"2022-01-08T17:09:29.386794Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred = model.predict([test_tokens, test_attention], \n                    batch_size=16, verbose=2)\n\ntest_preds = np.argmax(pred, axis=-1)","metadata":{"execution":{"iopub.status.busy":"2022-01-08T17:09:29.38867Z","iopub.execute_input":"2022-01-08T17:09:29.388908Z","iopub.status.idle":"2022-01-08T17:09:30.019867Z","shell.execute_reply.started":"2022-01-08T17:09:29.388876Z","shell.execute_reply":"2022-01-08T17:09:30.019133Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# submit\nsub = get_preds(dataset='test', verbose=False, text_ids = test_ids, preds=test_preds)\nsub.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-08T17:09:30.02106Z","iopub.execute_input":"2022-01-08T17:09:30.021428Z","iopub.status.idle":"2022-01-08T17:09:30.099322Z","shell.execute_reply.started":"2022-01-08T17:09:30.02139Z","shell.execute_reply":"2022-01-08T17:09:30.098571Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2022-01-08T17:09:30.10057Z","iopub.execute_input":"2022-01-08T17:09:30.100881Z","iopub.status.idle":"2022-01-08T17:09:30.10868Z","shell.execute_reply.started":"2022-01-08T17:09:30.100845Z","shell.execute_reply":"2022-01-08T17:09:30.107969Z"},"trusted":true},"execution_count":null,"outputs":[]}]}