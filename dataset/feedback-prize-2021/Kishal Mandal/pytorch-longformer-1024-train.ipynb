{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Essays!! NER-dy Training (KFolds)\n\nThis notebook is basically based on [this](https://www.kaggle.com/cdeotte/pytorch-bigbird-ner-cv-0-615) notebook by @cdeotte where he has used PyTorch and BigBird. \n\nThis code is just shorter and easier to debug and I have used **TorchEZ**, a PyTorch Wrapper to make the train, validation and prediction codes resuable, rather than writing them again and again. [Github Link](https://github.com/kishalxd/torchez)\n\nTorchEZ is still under development. Currently, I have not incorporated the use of schedulers (will update soon). The basic usage can be seen in this notebook.\n\nMoreover, I have tried to train 5 folds of the data. Currently using RoBERTa-L. Also used longformer-L as you can see in the attached datasets. (Each epoch takes 2:45:00 hrs to train with a batch size of 1 - LB : 0.587 for 2 epochs) **Verison 16 : Longformer*\n\n**About the Cross-Validation** : The metric calculation has been taken from [this](https://www.kaggle.com/robikscube/student-writing-competition-twitch-stream) notebook by @robikscube \n\n**Also has the submission, but disabled and made this notebook for training only**\n\n**The Inference and Cross-Validation Notebook can be found here : [Essays!! NERdy indeed | INFER and CV - KFolds ](https://www.kaggle.com/kishalmandal/ner-inference)**\n\n***If you like please UPVOTE :)***","metadata":{}},{"cell_type":"code","source":"import sys\nimport os\nsys.path.append('../input/torchez/')\n\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\n\nfrom transformers import logging\nlogging.set_verbosity(50)\n\nimport pandas as pd\nimport numpy as np\nimport transformers\nfrom transformers import AutoModel, AutoConfig, AutoTokenizer, AdamW, AutoModelForTokenClassification\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nimport torch\nimport torch.nn as nn\nimport ast\nfrom tqdm import tqdm\nimport gc; gc.enable()\nimport torchez as ez","metadata":{"execution":{"iopub.status.busy":"2022-01-03T10:07:23.686484Z","iopub.execute_input":"2022-01-03T10:07:23.686789Z","iopub.status.idle":"2022-01-03T10:07:23.695019Z","shell.execute_reply.started":"2022-01-03T10:07:23.686745Z","shell.execute_reply":"2022-01-03T10:07:23.694282Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('../input/feedback-ner-train/train_folds.csv')#[:10]","metadata":{"execution":{"iopub.status.busy":"2022-01-03T10:07:23.696871Z","iopub.execute_input":"2022-01-03T10:07:23.697252Z","iopub.status.idle":"2022-01-03T10:07:24.849814Z","shell.execute_reply.started":"2022-01-03T10:07:23.697214Z","shell.execute_reply":"2022-01-03T10:07:24.84891Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-03T10:07:24.852929Z","iopub.execute_input":"2022-01-03T10:07:24.853127Z","iopub.status.idle":"2022-01-03T10:07:24.864291Z","shell.execute_reply.started":"2022-01-03T10:07:24.853102Z","shell.execute_reply":"2022-01-03T10:07:24.863671Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"le = LabelEncoder()\nall_tags=[]\nfor i in tqdm(range(len(df))):\n    all_tags.extend(set(ast.literal_eval(df['entities'].values[i])))\n    \nunique_tags = list(set(all_tags))","metadata":{"execution":{"iopub.status.busy":"2022-01-03T10:07:24.865633Z","iopub.execute_input":"2022-01-03T10:07:24.866369Z","iopub.status.idle":"2022-01-03T10:07:35.802681Z","shell.execute_reply.started":"2022-01-03T10:07:24.866329Z","shell.execute_reply":"2022-01-03T10:07:35.801943Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"le.fit_transform(unique_tags)\ndict(zip(le.transform(le.classes_), le.classes_))","metadata":{"execution":{"iopub.status.busy":"2022-01-03T10:07:35.804725Z","iopub.execute_input":"2022-01-03T10:07:35.804984Z","iopub.status.idle":"2022-01-03T10:07:35.813041Z","shell.execute_reply.started":"2022-01-03T10:07:35.804952Z","shell.execute_reply":"2022-01-03T10:07:35.81235Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Config:\n    batch_size = 4\n    lr = 0.125e-6\n    max_len = 1024\n    num_class = 15\n    weight_decay=0.01\n    model_name = 'allenai/longformer-base-4096'\n    fold = 1\n    submission = False","metadata":{"execution":{"iopub.status.busy":"2022-01-03T10:07:35.814208Z","iopub.execute_input":"2022-01-03T10:07:35.814806Z","iopub.status.idle":"2022-01-03T10:07:35.819948Z","shell.execute_reply.started":"2022-01-03T10:07:35.814762Z","shell.execute_reply":"2022-01-03T10:07:35.819252Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"LABEL_ALL_SUBTOKENS = True\n\nclass EntityDataset:\n    def __init__(self, dataframe, tokenizer, max_len, get_wids):\n        self.len = len(dataframe)\n        self.data = dataframe\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n        self.get_wids = get_wids # for validation\n\n    def __getitem__(self, index):\n        # GET TEXT AND WORD LABELS \n        text = self.data['text'].values[index]        \n        word_labels = ast.literal_eval(self.data['entities'].values[index]) if not self.get_wids else None\n\n        # TOKENIZE TEXT\n        encoding = self.tokenizer(text.split(),\n                             is_split_into_words=True,\n                             #return_offsets_mapping=True, \n                             padding='max_length', \n                             truncation=True, \n                             max_length=self.max_len)\n        word_ids = encoding.word_ids()  \n\n        # CREATE TARGETS\n        if not self.get_wids:\n            previous_word_idx = None\n            label_ids = []\n            for word_idx in word_ids:                            \n                if word_idx is None:\n                    label_ids.append(-100)\n                elif word_idx != previous_word_idx:              \n                    label_ids.append( le.transform([word_labels[word_idx]]) )\n                else:\n                    if LABEL_ALL_SUBTOKENS:\n                        label_ids.append( le.transform([word_labels[word_idx]]) )\n                    else:\n                        label_ids.append(-100)\n                previous_word_idx = word_idx\n            encoding['labels'] = label_ids\n\n        # CONVERT TO TORCH TENSORS\n        item = {key: torch.as_tensor(val) for key, val in encoding.items()}\n        if self.get_wids: \n            word_ids2 = [w if w is not None else -1 for w in word_ids]\n            item['wids'] = torch.as_tensor(word_ids2)\n\n        return item\n\n    def __len__(self):\n        return self.len","metadata":{"execution":{"iopub.status.busy":"2022-01-03T10:07:35.821003Z","iopub.execute_input":"2022-01-03T10:07:35.821707Z","iopub.status.idle":"2022-01-03T10:07:35.834443Z","shell.execute_reply.started":"2022-01-03T10:07:35.821665Z","shell.execute_reply":"2022-01-03T10:07:35.833691Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class FeedbackModel(ez.Model):\n    def __init__(self):\n        super(FeedbackModel, self).__init__()\n        self.config = AutoConfig.from_pretrained(args.model_name)\n        self.config.update({'output_hidden_states':False, 'return_dict':False})\n        self.config.num_labels = args.num_class\n        self.roberta = AutoModelForTokenClassification.from_pretrained(args.model_name, config=self.config)\n        \n    def forward(self, ids, mask, labels=None):\n        if labels is not None:\n            loss, logits = self.roberta(ids, attention_mask=mask, labels=labels)\n            return loss, logits\n        else:\n            logits = self.roberta(ids, attention_mask=mask)\n            return logits\n    \n    def get_optimizer(self):\n        optimizer = AdamW(self.parameters(), lr=args.lr, weight_decay = args.weight_decay)\n        return optimizer\n    \n    def training_step(self, input_ids, attention_mask, labels):\n        loss, outputs = self(input_ids, attention_mask, labels)\n        metric = self.metrics(outputs, labels)\n\n        return {**{'loss':loss}, **metric}\n    \n    def validation_step(self, input_ids, attention_mask, labels):\n        loss, outputs = self(input_ids, attention_mask, labels)\n        metric = self.metrics(outputs, labels)\n        \n        return {**{'loss':loss}, **metric}\n    \n    def prediction_step(self, input_ids, attention_mask, wids):\n        outputs = self(input_ids, attention_mask)\n        all_preds = torch.argmax(outputs[0], axis=-1)\n\n        return all_preds\n    \n    def metrics(self, outputs, targets):\n        targets = targets.view(-1)\n        outputs = torch.softmax(outputs, dim=1)\n        active_logits = outputs.view(-1, args.num_class)\n        flattened_predictions = torch.argmax(active_logits, axis=1)\n        \n        active_accuracy = targets.view(-1) != -100\n        targets = torch.masked_select(targets, active_accuracy)\n        \n        predictions = torch.masked_select(flattened_predictions, active_accuracy)\n        \n        diff = accuracy_score(targets.cpu().numpy(), predictions.cpu().numpy())\n        return {'accuracy': diff}","metadata":{"execution":{"iopub.status.busy":"2022-01-03T10:07:35.835576Z","iopub.execute_input":"2022-01-03T10:07:35.836158Z","iopub.status.idle":"2022-01-03T10:07:35.85092Z","shell.execute_reply.started":"2022-01-03T10:07:35.836122Z","shell.execute_reply":"2022-01-03T10:07:35.850144Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"args= Config()\n    \ngc.collect()\nmodel = FeedbackModel()\ntokenizer = AutoTokenizer.from_pretrained(args.model_name, add_prefix_space=True)\n\ntraindf, validdf = df[df['kfold']!=args.fold], df[df['kfold']==args.fold]\n\n# all_tags=[]\n# for i in tqdm(range(len(traindf))):\n#     all_tags.extend(set(ast.literal_eval(traindf['entities'].values[i])))\n    \n    \n# if args.class_weight_compute == 'global':\n#     class_weight = compute_class_weight(classes=unique_tags, y=all_tags, class_weight='balanced')\n#     class_weight = torch.tensor(class_weight, dtype=torch.float).to(args.device)\n\ntrain_dataset = EntityDataset(traindf, tokenizer, args.max_len, get_wids=False)\nvalid_dataset = EntityDataset(traindf, tokenizer, args.max_len, get_wids=False)\n\nprint('='*50)\nprint(f'Fold : {args.fold}')\nprint('='*50)\n\nmodel.fit(\n    train_dataset=train_dataset,\n    train_batch_size=args.batch_size,\n    valid_dataset=valid_dataset,\n    valid_batch_size=args.batch_size*8,\n    device='cuda',\n    epochs=5,\n    save=True,\n    es=True,\n    es_monitor='valid_accuracy',\n    es_epochs=1,\n    es_mode='max',\n    model_path=f'model_f{args.fold}.bin'\n)","metadata":{"execution":{"iopub.status.busy":"2022-01-03T10:08:05.681121Z","iopub.execute_input":"2022-01-03T10:08:05.681396Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Validation","metadata":{}},{"cell_type":"code","source":"validation_df = pd.DataFrame()\nessays=[]\nids=[]\nfor idx in tqdm(validdf.id.unique()):\n    essays.append(open(f'../input/feedback-prize-2021/train/{idx}.txt', 'r').read())\n    ids.append(idx)\nvalidation_df['id'] = ids\nvalidation_df['text'] = essays","metadata":{"execution":{"iopub.status.busy":"2022-01-03T10:07:56.111056Z","iopub.status.idle":"2022-01-03T10:07:56.111485Z","shell.execute_reply.started":"2022-01-03T10:07:56.111248Z","shell.execute_reply":"2022-01-03T10:07:56.11127Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_predictions(df, min_select=1, load_path=None):\n    \n    test_dataset = EntityDataset(df, tokenizer, args.max_len, get_wids=True)\n    model = FeedbackModel()\n    preds = model.predict(test_dataset, batch_size=args.batch_size*4, device='cuda', model_path=f'./model_f{args.fold}.bin')\n    \n    predictions = []\n    for k,text_preds in tqdm(enumerate(preds)):\n        token_preds = le.inverse_transform(text_preds)\n        prediction = []\n        word_ids = test_dataset[k]['wids'].cpu().detach().numpy()\n        previous_word_idx = -1\n        for idx,word_idx in enumerate(word_ids):                            \n            if word_idx == -1:\n                pass\n            elif word_idx != previous_word_idx:              \n                prediction.append(token_preds[idx])\n                previous_word_idx = word_idx\n        predictions.append(prediction)\n    final_preds2 = []\n    \n    for i in range(len(df)):\n\n        idx = df.id.values[i]\n        pred = predictions[i] # Leave \"B\" and \"I\"\n        preds = []\n        j = 0\n        while j < len(pred):\n            cls = pred[j]\n            if cls == 'O': j += 1\n            else: cls = cls.replace('B','I') # spans start with B\n            end = j + 1\n            while end < len(pred) and pred[end] == cls:\n                end += 1\n\n            if cls != 'O' and cls != '' and end - j > min_select:\n                final_preds2.append((idx, cls.replace('I-',''),\n                                     ' '.join(map(str, list(range(j, end))))))\n\n            j = end\n\n    oof = pd.DataFrame(final_preds2)\n    oof.columns = ['id','class','predictionstring']\n    \n    del model, test_dataset\n    gc.collect()\n    return oof","metadata":{"_kg_hide-input":false,"execution":{"iopub.status.busy":"2022-01-03T10:07:56.113Z","iopub.status.idle":"2022-01-03T10:07:56.113418Z","shell.execute_reply.started":"2022-01-03T10:07:56.113189Z","shell.execute_reply":"2022-01-03T10:07:56.113213Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from Rob Mulla @robikscube\n# https://www.kaggle.com/robikscube/student-writing-competition-twitch\ndef calc_overlap(row):\n    \"\"\"\n    Calculates the overlap between prediction and\n    ground truth and overlap percentages used for determining\n    true positives.\n    \"\"\"\n    set_pred = set(row.predictionstring_pred.split(' '))\n    set_gt = set(row.predictionstring_gt.split(' '))\n    # Length of each and intersection\n    len_gt = len(set_gt)\n    len_pred = len(set_pred)\n    inter = len(set_gt.intersection(set_pred))\n    overlap_1 = inter / len_gt\n    overlap_2 = inter/ len_pred\n    return [overlap_1, overlap_2]\n\n\ndef score_feedback_comp(pred_df, gt_df):\n    \"\"\"\n    A function that scores for the kaggle\n        Student Writing Competition\n        \n    Uses the steps in the evaluation page here:\n        https://www.kaggle.com/c/feedback-prize-2021/overview/evaluation\n    \"\"\"\n    gt_df = gt_df[['id','discourse_type','predictionstring']] \\\n        .reset_index(drop=True).copy()\n    pred_df = pred_df[['id','class','predictionstring']] \\\n        .reset_index(drop=True).copy()\n    pred_df['pred_id'] = pred_df.index\n    gt_df['gt_id'] = gt_df.index\n    # Step 1. all ground truths and predictions for a given class are compared.\n    joined = pred_df.merge(gt_df,\n                           left_on=['id','class'],\n                           right_on=['id','discourse_type'],\n                           how='outer',\n                           suffixes=('_pred','_gt')\n                          )\n    joined['predictionstring_gt'] = joined['predictionstring_gt'].fillna(' ')\n    joined['predictionstring_pred'] = joined['predictionstring_pred'].fillna(' ')\n\n    joined['overlaps'] = joined.apply(calc_overlap, axis=1)\n\n    # 2. If the overlap between the ground truth and prediction is >= 0.5, \n    # and the overlap between the prediction and the ground truth >= 0.5,\n    # the prediction is a match and considered a true positive.\n    # If multiple matches exist, the match with the highest pair of overlaps is taken.\n    joined['overlap1'] = joined['overlaps'].apply(lambda x: eval(str(x))[0])\n    joined['overlap2'] = joined['overlaps'].apply(lambda x: eval(str(x))[1])\n\n\n    joined['potential_TP'] = (joined['overlap1'] >= 0.5) & (joined['overlap2'] >= 0.5)\n    joined['max_overlap'] = joined[['overlap1','overlap2']].max(axis=1)\n    tp_pred_ids = joined.query('potential_TP') \\\n        .sort_values('max_overlap', ascending=False) \\\n        .groupby(['id','predictionstring_gt']).first()['pred_id'].values\n\n    # 3. Any unmatched ground truths are false negatives\n    # and any unmatched predictions are false positives.\n    fp_pred_ids = [p for p in joined['pred_id'].unique() if p not in tp_pred_ids]\n\n    matched_gt_ids = joined.query('potential_TP')['gt_id'].unique()\n    unmatched_gt_ids = [c for c in joined['gt_id'].unique() if c not in matched_gt_ids]\n\n    # Get numbers of each type\n    TP = len(tp_pred_ids)\n    FP = len(fp_pred_ids)\n    FN = len(unmatched_gt_ids)\n    #calc microf1\n    my_f1_score = TP / (TP + 0.5*(FP+FN))\n    return my_f1_score","metadata":{"execution":{"iopub.status.busy":"2022-01-03T10:07:56.115668Z","iopub.status.idle":"2022-01-03T10:07:56.116354Z","shell.execute_reply.started":"2022-01-03T10:07:56.116105Z","shell.execute_reply":"2022-01-03T10:07:56.11613Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df = pd.read_csv('../input/feedback-prize-2021/train.csv')","metadata":{"execution":{"iopub.status.busy":"2022-01-03T10:07:56.117436Z","iopub.status.idle":"2022-01-03T10:07:56.117982Z","shell.execute_reply.started":"2022-01-03T10:07:56.117743Z","shell.execute_reply":"2022-01-03T10:07:56.117767Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"COMPUTE_VAL_SCORE = True\n\nprint('='*50)\nprint(f'Fold : {args.fold}')\nprint('='*50)\n\nif COMPUTE_VAL_SCORE: # note this doesn't run during submit\n    # VALID TARGETS\n    valid = train_df.loc[train_df['id'].isin(validdf.id.values)]\n    # OOF PREDICTIONS\n    oof = get_predictions(validation_df, min_select=6)\n\n    # COMPUTE F1 SCORE\n    f1s = []\n    CLASSES = oof['class'].unique()\n    print()\n    for c in CLASSES:\n        pred_df = oof.loc[oof['class']==c].copy()\n        gt_df = valid.loc[valid['discourse_type']==c].copy()\n        f1 = score_feedback_comp(pred_df, gt_df)\n        print(c,f1)\n        f1s.append(f1)\n    print()\n    print('Overall',np.mean(f1s))\n    print()","metadata":{"execution":{"iopub.status.busy":"2022-01-03T10:07:56.119357Z","iopub.status.idle":"2022-01-03T10:07:56.120205Z","shell.execute_reply.started":"2022-01-03T10:07:56.119974Z","shell.execute_reply":"2022-01-03T10:07:56.119998Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Submission","metadata":{}},{"cell_type":"code","source":"# from IPython.display import display\n# if args.submission:\n#     testdf = pd.DataFrame()\n\n#     essays=[]\n#     ids = []\n#     for file in os.listdir('../input/feedback-prize-2021/test'):\n#         essays.append(open(f'../input/feedback-prize-2021/test/{file}', 'r').read())\n#         ids.append(file.split('.')[0])\n\n#     testdf['id'] = ids\n#     testdf['text'] = essays\n\n#     preddf = get_predictions(testdf, min_select=6)\n#     preddf.to_csv('submission.csv', index=False)\n#     display(preddf.head())","metadata":{"execution":{"iopub.status.busy":"2022-01-03T10:07:56.121254Z","iopub.status.idle":"2022-01-03T10:07:56.121967Z","shell.execute_reply.started":"2022-01-03T10:07:56.121717Z","shell.execute_reply":"2022-01-03T10:07:56.121743Z"},"trusted":true},"execution_count":null,"outputs":[]}]}