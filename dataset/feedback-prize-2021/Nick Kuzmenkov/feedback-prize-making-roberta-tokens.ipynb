{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Introduction\n------------\n\nThis notebook is a small preparation stage for [Training RoBERTa in 10 minutes][1]. \nBy splitting the work into two parts I aim to save the TPU quota, separate concerns, and keep each part easy to read and follow.\n\nMany thanks to [Chris Deotte][2] for his [amazing work][3]. \nThis notebook is its Copy-Edit version with just a handful of changes.\n\nAll the outputs of this notebook are also published to [this dataset][4].\n\nImports\n-------\n\n[1]: https://www.kaggle.com/nickuzmenkov/feedback-prize-training-roberta-in-10-minutes/\n[2]: https://www.kaggle.com/cdeotte\n[3]: https://www.kaggle.com/cdeotte/tensorflow-longformer-ner-cv-0-633\n[4]: https://www.kaggle.com/nickuzmenkov/feedback-prize-roberta-tokens-1024","metadata":{}},{"cell_type":"code","source":"from tqdm.notebook import tqdm\nimport transformers\nimport pandas as pd\nimport numpy as np\nimport typing\nimport os","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-01-22T10:30:11.004601Z","iopub.execute_input":"2022-01-22T10:30:11.004894Z","iopub.status.idle":"2022-01-22T10:30:11.00906Z","shell.execute_reply.started":"2022-01-22T10:30:11.004862Z","shell.execute_reply":"2022-01-22T10:30:11.007982Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Configuration\n-------------","metadata":{}},{"cell_type":"code","source":"BASE_MODEL = \"roberta-base\"\nSEQ_LEN = 1024\nTEXT_PATH = \"../input/feedback-prize-2021/train\"\nCSV_PATH = \"../input/feedback-prize-2021/train.csv\"\nLABEL_MAP = {\n    \"Lead\": 0,\n    \"Position\": 1,\n    \"Evidence\": 2,\n    \"Claim\": 3,\n    \"Concluding Statement\": 4,\n    \"Counterclaim\": 5,\n    \"Rebuttal\": 6,\n}","metadata":{"execution":{"iopub.status.busy":"2022-01-22T10:30:11.010842Z","iopub.execute_input":"2022-01-22T10:30:11.012313Z","iopub.status.idle":"2022-01-22T10:30:11.023174Z","shell.execute_reply.started":"2022-01-22T10:30:11.012267Z","shell.execute_reply":"2022-01-22T10:30:11.022328Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Tokenizer code\n--------------\n\nTreat it simply as a black box unless you want to discover it in-depth.","metadata":{}},{"cell_type":"code","source":"class Tokenizer:\n    InputIds = typing.TypeVar(\"InputIds\", bound=typing.List[int])\n    AttentionMask = typing.TypeVar(\"AttentionMask\", bound=typing.List[int])\n    Labels = typing.TypeVar(\"Labels\", bound=typing.List[int])\n    OffsetMapping = typing.TypeVar(\n        \"OffsetMapping\",\n        bound=typing.List[typing.Tuple[int, int]],\n    )\n    TokenizerOutput = typing.NamedTuple(\n        \"TokenizerOutput\",\n        [\n            (\"input_ids\", InputIds),\n            (\"attention_mask\", AttentionMask),\n            (\"offset_mapping\", OffsetMapping),\n        ],\n    )\n    Output = typing.NamedTuple(\n        \"Output\",\n        [\n            (\"input_ids\", InputIds),\n            (\"attention_mask\", AttentionMask),\n            (\"labels\", Labels),\n        ],\n    )\n\n    def __init__(\n        self, df: pd.DataFrame, base_model: transformers.AutoTokenizer\n    ) -> None:\n        \"\"\"\n        Initialize tokenizer instance\n\n        :param df: DataFrame with labels for all texts\n        :param base_model: pre-loaded tokenizer\n        \"\"\"\n        self._df = df\n        self._base_model = base_model\n\n    def _init_output(self, n: int) -> Output:\n        \"\"\"\n        Return output-like arrays of zeros\n\n        :param n: number of unique text ids\n        \"\"\"\n        return self.Output(\n            input_ids=np.zeros((n, SEQ_LEN), dtype=\"int32\"),\n            attention_mask=np.zeros((n, SEQ_LEN), dtype=\"int32\"),\n            labels=np.zeros((n, SEQ_LEN, 2 * len(LABEL_MAP) + 1), dtype=\"int32\"),\n        )\n\n    def _get_tokenizer_output(self, text: str) -> TokenizerOutput:\n        \"\"\"\n        Return input token ids, attention mask and offset mapping of given text\n\n        :param text: essay text to tokenize\n        \"\"\"\n        encoding = self._base_model.encode_plus(\n            text,\n            max_length=SEQ_LEN,\n            padding=\"max_length\",\n            truncation=True,\n            return_offsets_mapping=True,\n        )\n        return self.TokenizerOutput(\n            input_ids=encoding[\"input_ids\"],\n            attention_mask=encoding[\"attention_mask\"],\n            offset_mapping=encoding[\"offset_mapping\"],\n        )\n\n    @staticmethod\n    def _get_labels(df: pd.DataFrame, offset_mapping: OffsetMapping) -> Labels:\n        \"\"\"\n        Return labels translated from initial text words (given) to tokens (machine understandable)\n\n        :param df: slice of DataFrame containing single text id\n        :param offset_mapping: offset mapping returned by tokenizer\n        \"\"\"\n        labels = np.zeros((SEQ_LEN, 2 * len(LABEL_MAP) + 1), dtype=\"int32\")\n        offset_index = 0\n\n        for _, (discourse_start, discourse_end, discourse_type) in df.iterrows():\n            if offset_index > len(offset_mapping) - 1:\n                break\n\n            k = LABEL_MAP[discourse_type]\n\n            token_start = offset_mapping[offset_index][0]\n            token_end = offset_mapping[offset_index][1]\n\n            first = True\n\n            while discourse_end > token_start:\n                if (token_start >= discourse_start) and (token_end <= discourse_end):\n                    if first:\n                        labels[offset_index, 2 * k] = 1\n                        first = False\n                    else:\n                        labels[offset_index, 2 * k + 1] = 1\n\n                offset_index += 1\n\n                if offset_index > len(offset_mapping) - 1:\n                    break\n\n                token_start = offset_mapping[offset_index][0]\n                token_end = offset_mapping[offset_index][1]\n\n        labels[:, -1] = 1 - np.max(labels, axis=-1)\n        return labels\n\n    def tokenize(self, verbose: int = 0) -> Output:\n        n = self._df.index.nunique()\n        ids = enumerate(self._df.index.unique())\n\n        if verbose > 0:\n            ids = tqdm(ids, total=n, desc=\"Tokenizing\")\n\n        output = self._init_output(n=n)\n\n        for i, id_ in ids:\n            with open(os.path.join(TEXT_PATH, id_ + \".txt\")) as file:\n                text = file.read().strip().lower()\n                tokenizer_output = self._get_tokenizer_output(text)\n\n            output.input_ids[i] = tokenizer_output.input_ids\n            output.attention_mask[i] = tokenizer_output.attention_mask\n            output.labels[i] = self._get_labels(\n                self._df[self._df.index == id_],\n                tokenizer_output.offset_mapping,\n            )\n\n        return output","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-01-22T10:30:11.02485Z","iopub.execute_input":"2022-01-22T10:30:11.02559Z","iopub.status.idle":"2022-01-22T10:30:11.053026Z","shell.execute_reply.started":"2022-01-22T10:30:11.025542Z","shell.execute_reply":"2022-01-22T10:30:11.052426Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Load data and RoBerta tokenizer\n-------------------------------","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv(\n    CSV_PATH,\n    usecols=[\"id\", \"discourse_start\", \"discourse_end\", \"discourse_type\"],\n    dtype={\n        \"id\": \"object\",\n        \"discource_start\": \"int32\",\n        \"discourse_end\": \"int32\",\n        \"discourse_type\": \"category\",\n    },\n    index_col=\"id\",\n)\nbase_model = transformers.AutoTokenizer.from_pretrained(BASE_MODEL)","metadata":{"execution":{"iopub.status.busy":"2022-01-22T10:30:11.054435Z","iopub.execute_input":"2022-01-22T10:30:11.055047Z","iopub.status.idle":"2022-01-22T10:30:12.639542Z","shell.execute_reply.started":"2022-01-22T10:30:11.055001Z","shell.execute_reply":"2022-01-22T10:30:12.638959Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The tricky part\n---------------\n\nIn the training data, each **word** is marked by one of 8 classes (lead, position, evidence, claim, counterclaim, rebuttal, concluding statement, or none of those). But what tokenizer produces is **tokens** that are loosely connected to words. Tokens **can be** words as well as word parts, special signs (e.g. text start `0` and text end `2`), or punctuation marks:\n","metadata":{}},{"cell_type":"code","source":"quote = \"It's understood that Hollywood sells Californication...\"\nencoding = base_model.encode_plus(quote, return_offsets_mapping=True)\n\npd.DataFrame(\n    {\"token\": [quote[x[0] : x[1]] for x in encoding[\"offset_mapping\"]]},\n    index=pd.Index(encoding[\"input_ids\"], name=\"token_id\"),\n)","metadata":{"execution":{"iopub.status.busy":"2022-01-22T10:30:12.641978Z","iopub.execute_input":"2022-01-22T10:30:12.64254Z","iopub.status.idle":"2022-01-22T10:30:12.655281Z","shell.execute_reply.started":"2022-01-22T10:30:12.642495Z","shell.execute_reply":"2022-01-22T10:30:12.654376Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"But what we have is word-wise labels, not token-wise labels. So we have to map the former to the latter.\n\nRun tokenizing\n--------------","metadata":{}},{"cell_type":"code","source":"tokenizer = Tokenizer(df=df, base_model=base_model)\noutput = tokenizer.tokenize(verbose=1)","metadata":{"execution":{"iopub.status.busy":"2022-01-22T10:30:12.656654Z","iopub.execute_input":"2022-01-22T10:30:12.657347Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Save results\n------------","metadata":{}},{"cell_type":"code","source":"np.save(\"input_ids.npy\", output.input_ids)\nnp.save(\"attention_mask.npy\", output.attention_mask)\nnp.save(\"labels.npy\", output.labels)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Conclusion\n----------\n\nThanks for reading. If you like this work, please visit the next part: [Training RoBERTa in 10 minutes][1].\nAll the outputs of this notebook are also published to [this dataset][2].\n\nI am in no way good at NLP, so feel free to correct me if you feel so.\n\n[1]: https://www.kaggle.com/nickuzmenkov/feedback-prize-training-roberta-in-10-minutes/\n[2]: https://www.kaggle.com/nickuzmenkov/feedback-prize-roberta-tokens-1024","metadata":{}}]}