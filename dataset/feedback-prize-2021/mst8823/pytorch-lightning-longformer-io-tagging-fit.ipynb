{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Pytorch-Lightning & Longformer Baseline\n\n\n- **Notebook**\n    - pytorch lightning \n    - IO Tagging using custom tokenizer\n    - backborn : longformer (max_length : 1024)\n    - optimizer : AdamW\n    - postprocessing Ã— 2 \n    \n    \n- **Reference**\n    - https://www.kaggle.com/cdeotte/tensorflow-longformer-ner-cv-0-633\n    - https://www.kaggle.com/vuxxxx/tensorflow-longformer-ner-postprocessing\n\n\nInference notebook -> https://www.kaggle.com/mst8823/pytorch-lightning-longformer-io-tagging-infer","metadata":{}},{"cell_type":"code","source":"class Config:\n    competition = \"feedback-prize-2021\"\n    name = \"Longformer-Baseline\"\n    debug = False\n    inference_only = False\n\n    model_name = \"allenai/longformer-base-4096\"\n    max_length = 1024\n    hidden_size = 768\n\n    n_fold = 10\n    trn_fold = [0]\n    seed = 2022\n\n    max_epochs = 8\n    gradient_clip_val = 100\n    accumulate_grad_batches = 1\n    early_stopping = False\n    optimizer = dict(\n        optimizer=\"AdamW\", \n        lr=2e-5, \n        weight_decay=1e-5\n        )\n    scheduler = dict(\n        interval = \"epoch\",\n        scheduler = \"MultiStepLR\",\n        milestones = [2, 3, 4, 5, 6, 7],\n        gamma = 0.5\n    )\n    train_batch_size = 4\n    valid_batch_size = 4\n    num_workers = 4\n    resume_from_checkpoint = None\n\n    colab_dir = \"\"\n    api_path = colab_dir + \"kaggle.json\"\n    drive_path = colab_dir + \"mst8823\"\n    upload_from_colab = False\n\n    kaggle_dataset_path = None\n\n    \"\"\"\n    - step scheduler example\n    scheduler = dict(\n        interval = \"step\",\n        scheduler=\"get_cosine_schedule_with_warmup\",\n        num_warmup_steps=256, \n        num_cycles=0.5)\n\n    \"\"\"\nif Config.debug:\n    Config.train_batch_size = 2\n    Config.valid_batch_size = 2\n    Config.max_epochs = 2\n    Config.max_length = 128","metadata":{"execution":{"iopub.status.busy":"2022-01-07T08:34:56.731985Z","iopub.execute_input":"2022-01-07T08:34:56.733481Z","iopub.status.idle":"2022-01-07T08:34:56.793065Z","shell.execute_reply.started":"2022-01-07T08:34:56.733223Z","shell.execute_reply":"2022-01-07T08:34:56.792091Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport re\nimport sys\nimport logging\nimport shutil\nimport json\nimport datetime\nimport requests\nimport itertools\nimport functools\nimport warnings\n\nimport pandas as pd\nimport numpy as np\nimport spacy\n\nfrom tqdm.auto import tqdm\nfrom sklearn.model_selection import KFold\n\nimport torch\nimport torch.nn as nn\nfrom torch.optim import Adam, AdamW\nfrom torch.optim.lr_scheduler import (\n    CosineAnnealingWarmRestarts,\n    CosineAnnealingLR,\n    MultiStepLR, \n    ReduceLROnPlateau\n    )\nfrom torch.utils.data import Dataset, DataLoader","metadata":{"execution":{"iopub.status.busy":"2022-01-07T08:34:56.795684Z","iopub.execute_input":"2022-01-07T08:34:56.796064Z","iopub.status.idle":"2022-01-07T08:35:08.484306Z","shell.execute_reply.started":"2022-01-07T08:34:56.796018Z","shell.execute_reply":"2022-01-07T08:35:08.482205Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Logger:\n    \"\"\" ref) https://github.com/ghmagazine/kagglebook/blob/master/ch04-model-interface/code/util.py\"\"\"\n    def __init__(self, path):\n        self.general_logger = logging.getLogger(path)\n        stream_handler = logging.StreamHandler()\n        file_general_handler = logging.FileHandler(os.path.join(path, 'Experiment.log'))\n        if len(self.general_logger.handlers) == 0:\n            self.general_logger.addHandler(stream_handler)\n            self.general_logger.addHandler(file_general_handler)\n            self.general_logger.setLevel(logging.INFO)\n\n    def info(self, message):\n        # display time\n        self.general_logger.info('[{}] - {}'.format(self.now_string(), message))\n\n    @staticmethod\n    def now_string():\n        return str(datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S'))\n\n\ndef setup(cfg):\n    \n    cfg.COLAB = \"google.colab\" in sys.modules\n    if cfg.COLAB:\n        print(\"This environment is Google Colab\")\n        \n        # mount\n        from google.colab import drive\n        if not os.path.isdir(\"/content/drive\"):\n            drive.mount('/content/drive') \n        \n        # import library\n        ! pip install --quiet transformers\n        ! pip install --quiet tensorflow-addons\n        ! pip install --quiet pytorch_lightning\n        ! pip install --quiet wandb\n\n        # use kaggle api (need kaggle token)\n        f = open(cfg.api_path, 'r')\n        json_data = json.load(f) \n        os.environ[\"KAGGLE_USERNAME\"] = json_data[\"username\"]\n        os.environ[\"KAGGLE_KEY\"] = json_data[\"key\"]\n        \n        # set dirs\n        cfg.DRIVE = cfg.drive_path\n        cfg.EXP = (cfg.name if cfg.name is not None \n            else requests.get(\"http://172.28.0.2:9000/api/sessions\").json()[0][\"name\"][:-6])\n        cfg.INPUT = os.path.join(cfg.DRIVE, \"Input\")\n        cfg.OUTPUT = os.path.join(cfg.DRIVE, \"Output\")\n        cfg.SUBMISSION = os.path.join(cfg.DRIVE, \"Submission\")\n        cfg.OUTPUT_EXP = os.path.join(cfg.OUTPUT, cfg.EXP) \n        cfg.EXP_MODEL = os.path.join(cfg.OUTPUT_EXP, \"model\")\n        cfg.EXP_FIG = os.path.join(cfg.OUTPUT_EXP, \"fig\")\n        cfg.EXP_PREDS = os.path.join(cfg.OUTPUT_EXP, \"preds\")\n\n        # make dirs\n        for d in [cfg.INPUT, cfg.SUBMISSION, cfg.EXP_MODEL, cfg.EXP_FIG, cfg.EXP_PREDS]:\n            os.makedirs(d, exist_ok=True)\n\n        if not os.path.isfile(os.path.join(cfg.INPUT, \"train.csv\")):\n            # load dataset\n            ! pip install --upgrade --force-reinstall --no-deps kaggle\n            ! kaggle competitions download -c cfg.competition -p $INPUT\n            ! unzip -d $INPUT $INPUT/$cfg.competition.zip \n\n    else:\n        print(\"This environment is Kaggle Kernel\")\n        if not cfg.inference_only:\n            ! pip install --quiet pytorch_lightning==1.5.8 \n        \n        # set dirs\n        cfg.INPUT = f\"../input/{cfg.competition}\"\n        \n        cfg.EXP = cfg.OUTPUT_EXP = cfg.name\n        cfg.SUBMISSION = \"./\"\n        cfg.EXP_MODEL = os.path.join(cfg.EXP, \"model\")\n        cfg.EXP_FIG = os.path.join(cfg.EXP, \"fig\")\n        cfg.EXP_PREDS = os.path.join(cfg.EXP, \"preds\")\n\n        if cfg.kaggle_dataset_path is not None:\n            shutil.copytree(cfg.kaggle_dataset_path, cfg.EXP)\n\n        # make dirs\n        for d in [cfg.EXP_MODEL, cfg.EXP_FIG, cfg.EXP_PREDS]:\n            os.makedirs(d, exist_ok=True)\n\n    # set device    \n    cfg.DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    warnings.filterwarnings(\"ignore\")\n\n    return cfg\n\n\ndef get_target_labels(cfg):\n    cfg.LABELS = [\n        \"O\", \n        \"Lead\", \n        \"Position\", \n        \"Claim\", \n        \"Counterclaim\", \n        \"Rebuttal\", \n        \"Evidence\", \n        \"Concluding Statement\"\n    ]\n\n    cfg.IDS2LABELS = {k : v for k, v in enumerate(cfg.LABELS)}\n    cfg.LABELS2IDS = {v : k for k, v in enumerate(cfg.LABELS)}\n\n    return cfg","metadata":{"execution":{"iopub.status.busy":"2022-01-07T08:35:08.48641Z","iopub.execute_input":"2022-01-07T08:35:08.486769Z","iopub.status.idle":"2022-01-07T08:35:08.623894Z","shell.execute_reply.started":"2022-01-07T08:35:08.486729Z","shell.execute_reply":"2022-01-07T08:35:08.622493Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# =========================\n# SetUp\n# =========================\nConfig = setup(Config)\nConfig = get_target_labels(cfg=Config)\nConfig.logger = Logger(Config.OUTPUT_EXP)\n\n# 2nd import\nfrom transformers import (LongformerConfig, LongformerModel, LongformerTokenizerFast)\nfrom transformers import (get_linear_schedule_with_warmup, get_cosine_schedule_with_warmup)\n\nimport pytorch_lightning as pl\nimport wandb\n\n# wandb setting\nif not Config.COLAB:\n    from kaggle_secrets import UserSecretsClient\n    user_secrets = UserSecretsClient()\n    api_key = user_secrets.get_secret(\"WANDB_API\")\n    wandb.login(key=api_key)\nelse:\n    wandb.login()","metadata":{"execution":{"iopub.status.busy":"2022-01-07T08:35:08.625737Z","iopub.execute_input":"2022-01-07T08:35:08.626072Z","iopub.status.idle":"2022-01-07T08:35:26.621931Z","shell.execute_reply.started":"2022-01-07T08:35:08.626021Z","shell.execute_reply":"2022-01-07T08:35:26.620911Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# =============================\n# Custom Tokenizer\n# =============================\nclass IOTokenizerForNER(LongformerTokenizerFast):\n    \"\"\"\n    ref: bert-book (japanese)\n    https://github.com/stockmarkteam/bert-book/blob/master/Chapter8.ipynb\n    \"\"\"\n    def encode_plus_tagged(self, text, entities, max_length):\n        \"\"\"\n        tokenize & labeling\n        \"\"\"\n        encoding = self.encode_plus(\n            text,\n            max_length = max_length,\n            padding = \"max_length\",\n            truncation = True,\n            return_offsets_mapping=True\n        )\n\n        offset_df = pd.DataFrame(\n            encoding[\"offset_mapping\"],\n            columns=[\"offset_start\", \"offset_end\"]\n            )\n        offset_df[\"labels\"] = 0\n        entities = sorted(entities, key=lambda x:x[\"span\"][0])\n        for entity in entities:\n\n            start = int(entity[\"span\"][0])\n            end = int(entity[\"span\"][1])\n            label = entity[\"type_id\"]\n\n            entity_mask = ((offset_df[\"offset_start\"] >= start) &\n                           (offset_df[\"offset_end\"] <= end)).astype(bool)\n            offset_df.loc[entity_mask, \"labels\"] = label\n        \n        encoding[\"labels\"] = offset_df[\"labels\"].tolist()\n        del encoding[\"offset_mapping\"]\n        return encoding\n\n    def encode_plus_untagged(self, text, max_length=None, return_tensors=\"pt\"):\n\n        encoding = self.encode_plus(\n            text, \n            max_length=max_length, \n            padding=\"max_length\" if max_length else False,\n            truncation=True if max_length else False,\n            return_offsets_mapping=True\n            )\n        \n        sequence_length = len(encoding[\"input_ids\"])\n        spans = encoding[\"offset_mapping\"]\n        spans = spans + [(0, 0)] * (sequence_length - len(spans))\n\n        del encoding[\"offset_mapping\"]\n        \n        if return_tensors == \"pt\":\n            encoding = {k:torch.tensor([v]) for k, v in encoding.items()}\n\n        return encoding, spans\n    \n    def convert_output_to_entities(self, text, labels, spans):\n        \"\"\"\n        get named entity\n        \"\"\"\n        # remove spetial tokens part from labels & spans\n\n        labels = [label for label, span in zip(labels, spans) if span[1] != 0]\n        spans = [span for span in spans if span[1] != 0] \n        entities = []\n        for label, group in itertools.groupby(enumerate(labels), key=lambda x:x[1]):\n\n            group = list(group)\n            start = spans[group[0][0]][0]\n            end = spans[group[-1][0]][1]\n\n            if label != 0:\n                entity = {\n                    \"span\":[start, end],\n                    \"type_id\":label\n                }\n                entities.append(entity)\n        \n        return entities\n\n\n# =============================\n# Funcs\n# =============================\ndef get_train_entities(cfg, train_df, id_code):\n    entities = []\n    _df = train_df[train_df[\"id\"] == id_code].reset_index()\n\n    for _, row in _df.iterrows():\n        start, end = row[\"discourse_start\"], row[\"discourse_end\"]\n        label = row[\"discourse_type\"]\n\n        type_id = cfg.LABELS2IDS[label]\n        entity = {\"span\":(start, end), \"type_id\":type_id}\n        entities.append(entity)\n    \n    return entities\n\n\ndef get_full_text(cfg, filename, data=\"train\"):\n    return open(cfg.INPUT + f\"/{data}/{filename}.txt\", \"r\").read()\n\n\ndef get_text_df(cfg, data=\"train\"):\n    \n    if cfg.COLAB:\n        filepath = os.path.join(cfg.INPUT, f\"{data}_text_df.csv\")\n    else:\n        filepath = f\"{data}_text_df.csv\"\n    if os.path.isfile(filepath):\n        text_df = pd.read_csv(filepath)\n    \n    else:\n        listdir = os.listdir(os.path.join(cfg.INPUT, data))\n        texts, ids = [], []\n        for f in tqdm(listdir):\n            id_code = os.path.splitext(f)[0]\n            text= get_full_text(cfg, filename=id_code, data=data)\n            texts.append(text)\n            ids.append(id_code)\n        \n        text_df = pd.DataFrame({\"id\":ids, \"text\":texts})\n        text_df.to_csv(filepath, index=False)\n\n    return text_df\n# =============================\n# Dataset\n# =============================\nclass FeedbackTrainDataset(Dataset):\n    def __init__(self, cfg, df, tokenizer, directory=\"train\", text_df=None):\n        self.cfg = cfg\n        self.df = df\n        self.filenames = df[\"id\"].unique().tolist()\n        self.tokenizer = tokenizer\n        self.directory = directory\n        self.text_df = text_df\n\n    def __getitem__(self, idx):\n        filename = self.filenames[idx]\n        if self.text_df is None:\n            text = get_full_text(self.cfg, filename, data=self.directory)\n        else:\n            text = self.text_df.loc[self.text_df[\"id\"] == f\"{filename}\", \"text\"].values[0]\n\n        entities = get_train_entities(self.cfg, self.df, id_code=filename)\n        encoding = self.tokenizer.encode_plus_tagged(\n            text, \n            entities=entities, \n            max_length=self.cfg.max_length)\n        \n        inputs = {\"input_ids\":encoding[\"input_ids\"], \n                  \"attention_mask\":encoding[\"attention_mask\"]}\n                  \n        labels = torch.tensor(encoding[\"labels\"])\n        inputs = {k:torch.tensor([v]).flatten() for k, v in inputs.items()}\n        return inputs, labels\n    \n    def __len__(self):\n        return len(self.filenames)\n\n\nclass FeedbackTestDataset(Dataset):\n    def __init__(self, cfg, df, tokenizer, directory, text_df=None):\n        self.cfg = cfg\n        self.df = df\n        self.filenames = df[\"id\"].unique().tolist()\n        self.tokenizer = tokenizer\n        self.directory = directory\n        self.text_df = text_df\n\n    def __getitem__(self, idx):\n        filename = self.filenames[idx]\n        if self.text_df is None:\n            text = get_full_text(self.cfg, filename, data=self.directory)\n        else:\n            text = self.text_df.loc[self.text_df[\"id\"] == f\"{filename}\", \"text\"].values[0]\n\n        inputs, spans = self.tokenizer.encode_plus_untagged(\n            text, \n            max_length=self.cfg.max_length, \n            return_tensors=None)\n        \n        inputs = {k:torch.tensor([v]).flatten() for k, v in inputs.items()}\n        return (inputs, spans, text, filename)\n    \n    def __len__(self):\n        return len(self.filenames)\n\n\nclass FeedbackDataModule(pl.LightningDataModule):\n    def __init__(self, cfg, tokenizer, train_df, valid_df, text_df=None):\n        super(FeedbackDataModule).__init__()\n\n        self.cfg = cfg\n        self.tokenizer = tokenizer\n        self.train_df = train_df\n        self.valid_df = valid_df\n\n        self.text_df = text_df\n        self.train_dataset = None\n        self.va_dataset = None\n\n    def prepare_data(self):\n        self.train_dataset = FeedbackTrainDataset(\n            cfg=self.cfg, tokenizer=self.tokenizer, df=self.train_df, directory=\"train\", text_df=self.text_df)\n        self.val_dataset = FeedbackTrainDataset(\n            cfg=self.cfg, tokenizer=self.tokenizer, df=self.valid_df, directory=\"train\", text_df=self.text_df)\n        \n\n    def train_dataloader(self):\n        train_dataloader = DataLoader(\n            self.train_dataset, \n            batch_size=self.cfg.train_batch_size, \n            shuffle=True, \n            num_workers=self.cfg.num_workers, \n            pin_memory=True, \n            drop_last=True)\n        \n        return train_dataloader\n\n    def val_dataloader(self):\n        val_dataloader = DataLoader(\n            self.val_dataset,\n            batch_size=self.cfg.valid_batch_size,\n            shuffle=False,\n            num_workers=self.cfg.num_workers, \n            pin_memory=True, \n            drop_last=False)\n\n        return val_dataloader\n\n\n# =============================\n# Model\n# =============================\ndef get_optimizer(cfg, parameters):\n    opt = cfg.optimizer\n    if opt[\"optimizer\"] == \"AdamW\":\n        optimizer = AdamW(\n            parameters,\n            lr=opt[\"lr\"],\n            weight_decay=opt[\"weight_decay\"]\n            )\n    \n    elif opt[\"optimizer\"] == \"Adam\":\n        optimizer = Adam(\n            parameters,\n            lr=opt[\"lr\"],\n            weight_decay=opt[\"weight_decay\"]\n            )\n    \n    else:\n        raise NotImplementedError\n    \n    return optimizer\n\n\ndef get_scheduler(cfg, optimizer, num_train_steps):\n    sch = cfg.scheduler\n    if sch[\"scheduler\"] == \"get_linear_schedule_with_warmup\":\n        scheduler = get_linear_schedule_with_warmup(\n            optimizer, \n            num_warmup_steps=sch[\"num_warmup_steps\"],\n            num_training_steps=num_train_steps)\n    \n    elif sch[\"scheduler\"] == \"get_cosine_schedule_with_warmup\":\n        scheduler = get_cosine_schedule_with_warmup(\n            optimizer,\n            num_warmup_steps=sch[\"num_warmup_steps\"],\n            num_training_steps=num_train_steps,\n            num_cycles=sch[\"num_cycles\"]\n            )\n\n    elif sch[\"scheduler\"] == \"MultiStepLR\":\n        scheduler = MultiStepLR(\n            optimizer, \n            milestones=sch[\"milestones\"], \n            gamma=sch[\"gamma\"]\n        )\n\n    else:\n        raise NotImplementedError\n    \n    return scheduler\n\n\nclass FeedbackModel(pl.LightningModule):\n    def __init__(self, cfg, backborn):\n        super(FeedbackModel, self).__init__()\n        self.cfg = cfg\n\n        self.backborn = backborn   \n        self.loss = nn.CrossEntropyLoss()\n        self.linear = nn.Linear(self.cfg.hidden_size, len(self.cfg.LABELS))\n\n        self.total_steps = None\n\n    def forward(self, inputs):\n        x = self.backborn(**inputs)\n        x = x[0]\n        x = self.linear(x)\n        return x\n\n    def training_step(self, batch, batch_idx):\n        inputs, labels = batch\n        outputs = self.forward(inputs)\n        loss = self.loss(outputs.view(-1, len(self.cfg.LABELS)), labels.view(-1))\n        self.log(\"train_loss\", loss, on_step=True, logger=True, prog_bar=True)\n        return loss\n\n    def validation_step(self, batch, batch_idx):\n        inputs, labels = batch\n        outputs = self.forward(inputs)\n        loss = self.loss(outputs.view(-1, len(self.cfg.LABELS)), labels.view(-1))\n        self.log(\"val_loss\", loss, on_step=True, logger=True, prog_bar=True)\n        return loss\n\n    def setup(self, stage=None):\n        if stage != \"fit\":\n            return\n        # calculate total steps\n        train_dataloader = self.trainer._data_connector._train_dataloader_source.dataloader()\n        gpus = 0 if self.trainer.gpus is None else self.trainer.gpus\n        tb_size = self.cfg.train_batch_size * max(1, gpus)\n        ab_size = self.trainer.accumulate_grad_batches * float(self.trainer.max_epochs)\n        self.total_steps = (len(train_dataloader.dataset) // tb_size) // ab_size\n\n    def configure_optimizers(self):\n        optimizer = get_optimizer(self.cfg, parameters=self.parameters())\n\n        if self.cfg.scheduler is None:\n            return [optimizer]\n        else:\n            scheduler = get_scheduler(self.cfg, optimizer, num_train_steps=self.total_steps)\n            return [optimizer], [{\"scheduler\": scheduler, \"interval\": self.cfg.scheduler[\"interval\"]}]\n\n\n# =============================\n# Train & Predict\n# =============================\ndef class2dict(f):\n    return dict((name, getattr(f, name)) for name in dir(f) if not name.startswith('__'))\n\n\ndef get_filname_listdir(dirctory):\n    listdir = os.listdir(dirctory)\n    out_lst = [os.path.splitext(d)[0] for d in listdir]\n    return out_lst\n\n\ndef get_predition_strings(full_text, discourse_start, discourse_end):\n    \"\"\"https://www.kaggle.com/c/feedback-prize-2021/discussion/297591\n    \"\"\"\n    char_start = discourse_start\n    char_end = discourse_end\n    word_start = len(full_text[:char_start].split())\n    word_end = word_start + len(full_text[char_start:char_end].split())\n    word_end = min( word_end, len(full_text.split()) )\n    predictionstring = [x for x in range(word_start,word_end)]\n    return predictionstring\n\n\ndef train_fold(cfg, train_df, valid_df, tokenizer, backborn, filename, text_df):\n\n    wandblogger = pl.loggers.WandbLogger(\n        project=cfg.competition + \"-Public\", \n        config=class2dict(cfg),\n        group=cfg.name, \n        name=\"-\".join(filename.split(\"-\")[-2:]),\n        job_type=\"train\",\n        reinit=True,\n        anonymous=\"must\"  # public\n    )\n\n    lightning_datamodule = FeedbackDataModule(\n        cfg=cfg,\n        train_df=train_df, \n        valid_df=valid_df, \n        tokenizer=tokenizer,\n        text_df=text_df\n        )\n    \n    lightning_model = FeedbackModel(cfg=cfg, backborn=backborn)\n    \n    checkpoint = pl.callbacks.ModelCheckpoint(\n        dirpath=cfg.EXP_MODEL,\n        filename=filename,\n        save_top_k=1,\n        verbose=True,\n        monitor=\"val_loss\",\n        mode=\"min\",\n    )\n    lr_monitor = pl.callbacks.LearningRateMonitor(logging_interval=\"step\")\n    callbacks = [checkpoint, lr_monitor]\n\n    if cfg.early_stopping:\n        early_stopping = pl.callbacks.EarlyStopping(\n            monitor=\"val_loss\", \n            min_delta=0.0, \n            patience=8, \n            mode='min', \n        )\n        callbacks += [early_stopping]\n    \n    trainer = pl.Trainer(\n        max_epochs=cfg.max_epochs,\n        callbacks=callbacks,\n        logger=[wandblogger],\n        gradient_clip_val=cfg.gradient_clip_val,\n        accumulate_grad_batches=cfg.accumulate_grad_batches,\n        resume_from_checkpoint=cfg.resume_from_checkpoint,\n        gpus=-1\n    )\n\n    trainer.fit(lightning_model, datamodule=lightning_datamodule)\n    wandb.finish(quiet=True)\n\ndef train_cv(cfg, df, tokenizer, backborn, text_df=None):\n    \"\"\"cross validation & get oof\"\"\"\n    oof_df = pd.DataFrame()\n    for i_fold in range(cfg.n_fold):\n\n        if i_fold in cfg.trn_fold:\n            filename = f\"{cfg.name}-seed{cfg.seed}-fold{i_fold}\"\n            filelist = get_filname_listdir(cfg.EXP_MODEL)\n\n            val_mask = (df[\"fold\"] == i_fold).astype(bool)\n            train_df = df[~val_mask].reset_index(drop=True)\n            valid_df = df[val_mask].reset_index(drop=True)\n\n            if not filename in filelist:\n                print(f\"# --------- # Start Training Fold={i_fold} # --------- #\")\n                # training\n                train_fold(\n                    cfg=cfg, \n                    train_df=train_df, \n                    valid_df=valid_df, \n                    tokenizer=tokenizer, \n                    backborn=backborn, \n                    filename=filename, \n                    text_df=text_df\n                    )\n\n            # get validation prediction\n            pred_df = predict(\n                cfg=cfg,\n                df=valid_df, \n                tokenizer=tokenizer, \n                backborn=backborn, \n                filename=filename)\n            \n            val_score = get_score(pred_df=pred_df, true_df=valid_df)\n            log = f\"{cfg.name}-seed{cfg.seed}-fold{i_fold}:F1 score={val_score:.4f}\"\n            cfg.logger.info(log)\n\n            oof_df = pd.concat([oof_df, pred_df])\n        \n    return oof_df.reset_index().rename(columns={\"index\":\"sequence_id\"})\n\n\ndef predict(cfg, df, tokenizer, backborn, filename, directory=\"train\", text_df=None):\n    preds = get_raw_prediction(cfg, df, tokenizer, backborn, filename, directory, text_df)\n    pred_labels = [np.argmax(pred, axis=-1) for pred in preds]\n\n    output_df = get_output_prediction(cfg, pred_labels, df, tokenizer, directory, text_df)\n    return output_df\n\n\ndef predict_cv(cfg, df, tokenizer, backborn, directory=\"test\", text_df=None):\n\n    fold_preds = []\n    for i_fold in range(cfg.n_fold):\n        if i_fold in cfg.trn_fold:\n            filename =f\"{cfg.name}-seed{cfg.seed}-fold{i_fold}\"\n            preds = get_raw_prediction(cfg, df, tokenizer, backborn, filename, directory, text_df)\n            fold_preds.append(preds)\n    \n    fold_preds = np.mean(fold_preds, axis=0)\n    pred_labels = [np.argmax(pred, axis=-1) for pred in fold_preds]\n    output_df = get_output_prediction(cfg, pred_labels, df, tokenizer, directory, text_df)\n    return output_df\n\n\ndef get_raw_prediction(cfg, df, tokenizer, backborn, filename, directory=\"train\", text_df=None):\n    test_dataset = FeedbackTestDataset(\n        cfg=cfg, tokenizer=tokenizer, df=df, directory=directory, text_df=text_df)\n    \n    test_dataloader = DataLoader(\n        test_dataset,\n        batch_size=cfg.valid_batch_size,\n        shuffle=False,\n        num_workers=cfg.num_workers, \n        pin_memory=True, \n        drop_last=False)\n    \n    lightning_model = FeedbackModel(cfg=cfg, backborn=backborn)\n    checkpoint_path = os.path.join(cfg.EXP_MODEL, filename + \".ckpt\")\n    lightning_model = lightning_model.load_from_checkpoint(\n        checkpoint_path, cfg=cfg, backborn=backborn)\n\n    preds = []\n    lightning_model.eval()\n    lightning_model.to(cfg.DEVICE)\n\n    for (inputs, _, _, _) in tqdm(test_dataloader,total=len(test_dataloader)):\n        # get predicted labels by batch\n        for k, v in inputs.items():\n            inputs[k] = v.to(cfg.DEVICE)\n\n        with torch.no_grad():\n            pred = lightning_model(inputs)\n\n        preds.append(pred.cpu().numpy()) \n\n    preds = np.concatenate(preds)  # (N * max_length * num_classes)\n    return preds\n\n\ndef get_output_prediction(cfg, pred_labels, df, tokenizer, directory=\"train\", text_df=None):\n    test_dataset = FeedbackTestDataset(\n        cfg=cfg, tokenizer=tokenizer, df=df, directory=directory, text_df=text_df)\n    output_df = pd.DataFrame()\n\n    for (_, span, text, id_code), pred in zip(tqdm(DataLoader(test_dataset, batch_size=1)), pred_labels):\n        # get submission format predictions\n        text = text[0]\n        entities = tokenizer.convert_output_to_entities(\n            text=text,\n            labels=pred,\n            spans=span)\n        \n        predstr_by_id, labels_by_id = [], []\n        for entity in entities:\n            span, type_id = entity[\"span\"], entity[\"type_id\"]\n            prediction_strings = get_predition_strings(\n                full_text=text, \n                discourse_start=span[0].cpu().numpy()[0], \n                discourse_end=span[1].cpu().numpy()[0])\n            \n            predstr_by_id.append(\" \".join(map(str, prediction_strings)))\n            labels_by_id.append(cfg.IDS2LABELS[type_id])\n        \n        out_df_by_id = pd.DataFrame({\n            \"id\":[id_code[0]]*len(labels_by_id),\n            \"class\":labels_by_id, \n            \"predictionstring\":predstr_by_id\n            })\n        output_df = pd.concat([output_df, out_df_by_id])\n    return output_df\n\n# =============================\n# Metrics\n# =============================   \ndef calc_overlap(row):\n    \"\"\"\n    ref: https://www.kaggle.com/robikscube/student-writing-competition-twitch\n    \"\"\"\n    set_pred = set(row[\"predictionstring_pred\"].split(\" \"))\n    set_true = set(row[\"predictionstring_true\"].split(\" \"))\n\n    len_true, len_pred = len(set_true), len(set_pred)\n    intersection = len(set_true.intersection(set_pred))\n\n    overlap_t_and_p = intersection / len_true\n    overlap_p_and_t = intersection / len_pred\n\n    return [overlap_t_and_p, overlap_p_and_t]\n\n\ndef get_micro_f1_score(pred_df, true_df):\n\n    true_df = (true_df[[\"id\", \"discourse_type\", \"predictionstring\"]]\n               .reset_index(drop=True)\n               .copy())\n    pred_df = pred_df[[\"id\", \"class\", \"predictionstring\"]].reset_index(drop=True).copy()\n    pred_df[\"pred_id\"] = pred_df.index\n    true_df[\"true_id\"] = true_df.index\n\n    # 1. all ground truths and predictions for a given class are compared.\n    joined_df = pred_df.merge(\n        true_df,\n        left_on=[\"id\", \"class\"],\n        right_on=[\"id\", \"discourse_type\"],\n        how=\"outer\",\n        suffixes=(\"_pred\", \"_true\"),\n    )\n    joined_df[\"predictionstring_true\"].fillna(\" \", inplace=True)\n    joined_df[\"predictionstring_pred\"].fillna(\" \", inplace=True)\n    joined_df[\"overlaps\"] = joined_df.apply(calc_overlap, axis=1)\n\n    # 2. If the overlap between the ground truth and prediction is >= 0.5,\n    #    and the overlap between the prediction and the ground truth >= 0.5,\n    #    the prediction is a match and considered a true positive.\n    #    If multiple matches exist, the match with the highest pair of overlaps is taken.\n    joined_df[\"overlap1\"] = joined_df[\"overlaps\"].apply(lambda x: eval(str(x))[0])\n    joined_df[\"overlap2\"] = joined_df[\"overlaps\"].apply(lambda x: eval(str(x))[1])\n\n    joined_df[\"potential_TP\"] = (joined_df[\"overlap1\"] >= 0.5) & (joined_df[\"overlap2\"] >= 0.5)\n    joined_df[\"max_overlap\"] = joined_df[[\"overlap1\", \"overlap2\"]].max(axis=1)\n    tp_pred_ids = (\n        joined_df.query(\"potential_TP\")\n        .sort_values(\"max_overlap\", ascending=False)\n        .groupby([\"id\", \"predictionstring_true\"])\n        .first()[\"pred_id\"]\n        .values\n    )\n\n    # 3. Any unmatched ground truths are false negatives\n    #    and any unmatched predictions are false positives.\n    fp_pred_ids = [p for p in joined_df[\"pred_id\"].unique() if p not in tp_pred_ids]\n    matched_gt_ids = joined_df.query(\"potential_TP\")[\"true_id\"].unique()\n    unmatched_gt_ids = [c for c in joined_df[\"true_id\"].unique() if c not in matched_gt_ids]\n\n    # Get numbers of each type\n    TP = len(tp_pred_ids)\n    FP = len(fp_pred_ids)\n    FN = len(unmatched_gt_ids)\n    # calc microF1\n    my_f1_score = TP / (TP + 0.5 * (FP + FN))\n    return my_f1_score\n\n\ndef get_score(pred_df, true_df, return_class_scores=False):\n    class_scores = {}\n    pred_df = pred_df[[\"id\", \"class\", \"predictionstring\"]].reset_index(drop=True).copy()\n    for discourse_type, true_subset in true_df.groupby(\"discourse_type\"):\n        pred_subset = (\n            pred_df.loc[pred_df[\"class\"] == discourse_type]\n            .reset_index(drop=True)\n            .copy()\n        )\n        class_score = get_micro_f1_score(pred_subset, true_subset)\n        class_score = np.round(class_score, decimals=4)\n        class_scores[discourse_type] = class_score\n\n    score = np.mean([v for v in class_scores.values()])\n    score = np.round(score, decimals=4)\n\n    if return_class_scores:\n        return score, class_scores\n\n    return score\n\n# =============================\n# Load Data\n# =============================\ndef get_tokenizer_and_backborn(cfg):\n    \n    pretrained_dir = os.path.join(cfg.EXP_MODEL, \"Pretrain\")\n    if not os.path.isdir(pretrained_dir):\n        model_config = LongformerConfig.from_pretrained(cfg.model_name)\n        backborn = LongformerModel.from_pretrained(cfg.model_name, config=model_config)\n        tokenizer = IOTokenizerForNER.from_pretrained(cfg.model_name)\n\n        tokenizer.save_pretrained(pretrained_dir)\n        backborn.save_pretrained(pretrained_dir)\n    \n    else:\n        model_config = LongformerConfig.from_pretrained(pretrained_dir)\n        backborn = LongformerModel.from_pretrained(pretrained_dir, config=model_config)\n        tokenizer = IOTokenizerForNER.from_pretrained(pretrained_dir)\n\n    return tokenizer, backborn\n\n\ndef get_all_text_df(cfg):\n    train_text_df = get_text_df(cfg, data=\"train\")\n    test_text_df = get_text_df(cfg, data=\"test\")\n    all_text_df = pd.concat([train_text_df, test_text_df])\n    return all_text_df.reset_index(drop=True)\n\n    \ndef get_fold_idx(cfg, train_df):\n    cv_df = pd.DataFrame({\"id\":train_df[\"id\"].unique()})\n    cv_df[\"fold\"] = -1\n    cv_strategy = KFold(n_splits=cfg.n_fold, shuffle=True, random_state=cfg.seed)\n    for i_fold, (tr_idx, va_idx) in enumerate(cv_strategy.split(X=cv_df)):\n        cv_df.loc[va_idx, \"fold\"] = i_fold\n    \n    train_df = pd.merge(train_df, cv_df, on=\"id\", how=\"left\")\n    return train_df\n    \n# =============================\n# PostProcess\n# =============================\ndef postprocessing_for_blank(pred_df, num_blank=1):\n    \"\"\" \n    df must has [\"id\", \"class\", \"predictionstring\"]\n    - label=Lead, predictionstring = [1, 2, 3]\n    - label=Lead, predictionstring = [5, 6, 7]\n    =>\n    - label=Lead, predicitonstring = [1, 2, 3, 5, 6, 7]\n    \"\"\"\n\n    ids = pred_df[\"id\"].unique().tolist()\n    output_df = pd.DataFrame()\n    for id_code in tqdm(ids):\n        output_by_id = []\n        _df = pred_df[pred_df[\"id\"] == id_code].reset_index()\n\n        # get sequence label's predictionstrings\n        for label, group in itertools.groupby(zip(_df[\"predictionstring\"], _df[\"class\"]), key=lambda x:x[1]):\n            group = list(group)\n            if len(group) > 1:\n                pred_str_lst = []\n                for i, g in enumerate(group):\n                    strings_lst = list(map(int, g[0].split()))\n                    if i == 0:\n                        # initial val\n                        pred_str_lst += strings_lst\n\n                    elif pred_str_lst[-1] + num_blank >=  strings_lst[0]:\n                        # avoid blank\n                        pred_str_lst += strings_lst\n\n                    else:\n                        output_by_id_class_no_seq = [id_code, label, strings_lst]\n                        output_by_id.append(output_by_id_class_no_seq)\n            else:\n                pred_str_lst = list(map(int, group[0][0].split()))\n\n            output_by_id_class = [id_code, label, pred_str_lst] \n            output_by_id.append(output_by_id_class)\n\n        output_by_id = sorted(output_by_id, key=lambda x:x[2])  # sort by predictionstrings sequence\n        output_by_id_df = pd.DataFrame(\n            output_by_id, \n            columns=[\"id\", \"class\", \"predictionstring\"])\n        # convert list to str\n        output_by_id_df[\"predictionstring\"] = (\n            output_by_id_df[\"predictionstring\"].apply(lambda x:\" \".join(map(str, x))))\n        # concat by id\n        output_df = pd.concat([output_df, output_by_id_df])\n\n    return output_df.reset_index(drop=True)\n\n\ndef postprocessing_for_min_predstr_length(\n    pred_df, \n    map_clip = {\n        \"Lead\":9, \n        \"Position\":5,\n        \"Evidence\":14,\n        \"Claim\":3,\n        \"Concluding Statement\":11,\n        \"Counterclaim\":6, \n        \"Rebuttal\":4}):\n    \n    \"\"\"ref:https://www.kaggle.com/vuxxxx/tensorflow-longformer-ner-postprocessing\"\"\"\n    \n    output_df = pred_df.copy()\n    output_df[\"len\"] = pd.Series([len(list(map(int, x.split()))) for x in pred_df[\"predictionstring\"]])\n\n    for key, value in map_clip.items():\n        index = output_df[output_df[\"class\"] == key].query(f\"len < {value}\").index\n        output_df.drop(index, inplace = True)\n    \n    return output_df.reset_index(drop=True).drop(\"len\", axis=1)\n\n# =============================\n# EDA\n# =============================\ndef visualize_entity(cfg, id_code, train_df, text_df=None):\n    \"\"\" Not work in COLAB\"\"\"\n    if text_df is None:\n        full_text = get_full_text(cfg, filename=id_code, data=\"train\")\n    else:\n        full_text = text_df[text_df[\"id\"] == id_code][\"text\"].values[0]\n    _df = train_df[train_df[\"id\"] == id_code].reset_index(drop=True) \n\n    entities = []\n    for i, row in _df.iterrows():\n        entity = {\n            'start': int(row['discourse_start']), \n            'end': int(row['discourse_end']), \n            'label': row['discourse_type']\n        }\n        entities.append(entity)\n    doc2 = {\n        \"text\": full_text,\n        \"ents\": entities,\n        \"title\": id_code\n    }\n    colors = {\n        'Lead': '#EE11D0',\n        'Position': '#AB4DE1',\n        'Claim': '#1EDE71',\n        'Evidence': '#33FAFA',\n        'Counterclaim': '#4253C1',\n        'Concluding Statement': 'yellow',\n        'Rebuttal': 'red'\n        }\n    options = {\"ents\": list(colors.keys()), \"colors\":colors}\n    spacy.displacy.render(doc2, style=\"ent\", options=options, manual=True, jupyter=True)","metadata":{"execution":{"iopub.status.busy":"2022-01-07T08:35:26.626089Z","iopub.execute_input":"2022-01-07T08:35:26.626419Z","iopub.status.idle":"2022-01-07T08:35:26.766973Z","shell.execute_reply.started":"2022-01-07T08:35:26.626369Z","shell.execute_reply":"2022-01-07T08:35:26.765955Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def main():\n    \n    # load data\n    train_df = pd.read_csv(Config.INPUT + \"/train.csv\")\n    if Config.debug:\n        train_df = train_df[train_df[\"id\"].isin(train_df[\"id\"].unique()[:20])].reset_index(drop=True)\n    submission_df = pd.read_csv(Config.INPUT + \"/sample_submission.csv\")\n    text_df = get_all_text_df(Config)\n\n    # add fold idx\n    train_df = get_fold_idx(cfg=Config, train_df=train_df)\n\n    # load tokenizer & backborn\n    tokenizer, backborn = get_tokenizer_and_backborn(Config)\n    \n    if not Config.inference_only:\n        # training\n        raw_oof_df = train_cv(cfg=Config, df=train_df, tokenizer=tokenizer, backborn=backborn, text_df=text_df)\n        raw_oof_df[\"predictionstring\"] = raw_oof_df[\"predictionstring\"].replace(\"\", np.nan)\n        raw_oof_df = raw_oof_df.dropna()\n        true_df = train_df[train_df[\"id\"].isin(raw_oof_df[\"id\"].unique())].reset_index(drop=True)\n\n        print(\"# ----------------# RAW OOF # ----------------#\")\n        display(raw_oof_df)\n        Config.logger.info(f\"score={get_score(pred_df=raw_oof_df, true_df=true_df, return_class_scores=True)}\")\n\n        # postprocessing for train\n        print(\"\\n# ----------------# AFTER PP OOF # ----------------#\")\n        oof_df = postprocessing_for_blank(raw_oof_df, num_blank=1)\n        Config.logger.info(f\"pp1-score={get_score(pred_df=oof_df, true_df=true_df, return_class_scores=True)}\")\n\n        oof_df = postprocessing_for_min_predstr_length(oof_df)\n        Config.logger.info(f\"pp2-score={get_score(pred_df=oof_df, true_df=true_df, return_class_scores=True)}\")\n\n        oof_df = postprocessing_for_blank(oof_df, num_blank=1)\n        Config.logger.info(f\"pp3-score={get_score(pred_df=oof_df, true_df=true_df, return_class_scores=True)}\")\n        display(oof_df)\n\n    # inference for test\n    print(\"\\n# ----------------# RAW TEST PREDS # ----------------#\")\n    raw_preds_df = predict_cv(cfg=Config, df=submission_df, tokenizer=tokenizer, backborn=backborn, text_df=text_df)\n    raw_preds_df[\"predictionstring\"] = raw_preds_df[\"predictionstring\"].replace(\"\", np.nan)\n    raw_preds_df = raw_preds_df.dropna()\n    display(raw_preds_df)\n\n    print(\"\\n# ----------------# AFTER PP TEST PREDS # ----------------#\")\n    preds_df = postprocessing_for_blank(raw_preds_df, num_blank=1)\n    preds_df = postprocessing_for_min_predstr_length(preds_df)\n    preds_df = postprocessing_for_blank(preds_df, num_blank=1)\n    display(preds_df)\n\n    # upload output folder to kaggle dataset\n    if Config.upload_from_colab:\n        from kaggle.api.kaggle_api_extended import KaggleApi\n\n        def dataset_create_new(dataset_name, upload_dir):\n            dataset_metadata = {}\n            dataset_metadata['id'] = f'{os.environ[\"KAGGLE_USERNAME\"]}/{dataset_name}'\n            dataset_metadata['licenses'] = [{'name': 'CC0-1.0'}]\n            dataset_metadata['title'] = dataset_name\n            with open(os.path.join(upload_dir, 'dataset-metadata.json'), 'w') as f:\n                json.dump(dataset_metadata, f, indent=4)\n            api = KaggleApi()\n            api.authenticate()\n            api.dataset_create_new(folder=upload_dir, convert_to_csv=False, dir_mode='tar')\n        dataset_create_new(dataset_name=Config.EXP, upload_dir=Config.OUTPUT_EXP)\n\n        \nif __name__ == \"__main__\":\n    main()","metadata":{"execution":{"iopub.status.busy":"2022-01-07T08:35:26.768712Z","iopub.execute_input":"2022-01-07T08:35:26.769034Z","iopub.status.idle":"2022-01-07T08:37:46.785635Z","shell.execute_reply.started":"2022-01-07T08:35:26.768994Z","shell.execute_reply":"2022-01-07T08:37:46.784173Z"},"trusted":true},"execution_count":null,"outputs":[]}]}