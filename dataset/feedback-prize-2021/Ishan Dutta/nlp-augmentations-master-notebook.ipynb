{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<h1><center>NLP Augmentations Master Notebook</center></h1>\n<h3><center>Feedback Prize - Evaluating Student Writing</center></h3>\n\n<center><img src = \"https://www.gsu.edu/wp-content/themes/gsu-flex-2/images/logo.png\" width = \"750\" height = \"500\"/></center>                                                                          ","metadata":{}},{"cell_type":"markdown","source":"After receiving great response (Gold Medal, 180+ Upvotes) for my [Image Augmentations Master Notebook](https://www.kaggle.com/ishandutta/petfinder-data-augmentations-master-notebook) I am creating a similar notebook but for Text based NLP Augmentations.\n\nI will demonstrate with examples how you can apply a variety of transformations to your dataset based on your application!","metadata":{}},{"cell_type":"markdown","source":"<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:orange; border:0; color:white' role=\"tab\" aria-controls=\"home\"><center>Contents</center></h2>","metadata":{}},{"cell_type":"markdown","source":"> | S.No       |                   Heading                |\n> | :------------- | :-------------------:                |         \n> |  01 |  [**Competition Overview**](#competition-overview)  |                   \n> |  02 |  [**Libraries**](#libraries)                        |  \n> |  03 |  [**Global Config**](#global-config)                |\n> |  04 |  [**Weights and Biases**](#weights-and-biases)      |\n> |  05 |  [**Load Datasets**](#load-datasets)                |\n> |  06 |  [**What is Data Augmentation?**](#what-is-data-augmentation)  |\n> |  07 |  [**Basic NLP Augmentations**](#basic-nlp-augmentations)   |\n> |  08 |  [**Model**](#model)","metadata":{}},{"cell_type":"markdown","source":"<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:maroon; border:0; color:white' role=\"tab\" aria-controls=\"home\"><center>If you find this notebook useful, do give me an upvote, it helps to keep up my motivation. This notebook will be updated frequently so keep checking for furthur developments.</center></h3>","metadata":{}},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"<a id=\"competition-overview\"></a>\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:orange; border:0; color:white' role=\"tab\" aria-controls=\"home\"><center>Competition Overview</center></h2>","metadata":{}},{"cell_type":"markdown","source":"## **<span style=\"color:orange;\">Description</span>**\n\n\nIn this competition, you’ll identify elements in student writing. More specifically, you will automatically segment texts and classify argumentative and rhetorical elements in essays written by 6th-12th grade students. You'll have access to the largest dataset of student writing ever released in order to test your skills in natural language processing, a fast-growing area of data science.\n  \n---\n\n## **<span style=\"color:orange;\">Evaluation Metric</span>**\n\nSubmissions are evaluated on the overlap between ground truth and predicted word indices.\n\n1. For each sample, all ground truths and predictions for a given class are compared.\n2. If the overlap between the ground truth and prediction is >= 0.5, and the overlap between the prediction and the ground truth >= 0.5, the prediction is a match and considered a `true positive`. If multiple matches exist, the match with the highest pair of overlaps is taken.\n3. Any unmatched ground truths are `false negatives` and any unmatched predictions are `false positives`.\n\n---","metadata":{}},{"cell_type":"markdown","source":"<a id=\"libraries\"></a>\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:orange; border:0; color:white' role=\"tab\" aria-controls=\"home\"><center>Libraries</center></h2>","metadata":{}},{"cell_type":"code","source":"import os\nimport re\n\nimport json\nimport time\n\nimport numpy as np\nimport pandas as pd\n\nimport random\nfrom tqdm import tqdm\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n\nfrom nltk import sent_tokenize\nfrom albumentations.core.transforms_interface import DualTransform, BasicTransform\n\nfrom termcolor import colored\n\nimport tensorflow as tf\nfrom tensorflow import keras\n\nfrom sklearn.model_selection import train_test_split\nfrom collections import defaultdict\n\nimport wandb\nfrom wandb.keras import WandbCallback\nwandb.login()","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"<a id=\"global-config\"></a>\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:orange; border:0; color:white' role=\"tab\" aria-controls=\"home\"><center>Global Config</center></h2>","metadata":{}},{"cell_type":"code","source":"class config:\n    SEED = 42\n    DIRECTORY_PATH = \"../input/feedback-prize-2021\"\n    TRAIN_CSV_PATH = os.path.join(DIRECTORY_PATH, 'train.csv')","metadata":{"execution":{"iopub.status.busy":"2022-01-03T16:53:14.24314Z","iopub.execute_input":"2022-01-03T16:53:14.243372Z","iopub.status.idle":"2022-01-03T16:53:14.248909Z","shell.execute_reply.started":"2022-01-03T16:53:14.243348Z","shell.execute_reply":"2022-01-03T16:53:14.247629Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def set_seed(seed=config.SEED):\n    random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    np.random.seed(seed)\n    \nset_seed()","metadata":{"execution":{"iopub.status.busy":"2022-01-03T17:58:12.818891Z","iopub.execute_input":"2022-01-03T17:58:12.819933Z","iopub.status.idle":"2022-01-03T17:58:12.830518Z","shell.execute_reply.started":"2022-01-03T17:58:12.819852Z","shell.execute_reply":"2022-01-03T17:58:12.829389Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"<a id=\"weights-and-biases\"></a>\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:orange; border:0; color:white' role=\"tab\" aria-controls=\"home\"><center>Weights and Biases</center></h2>","metadata":{}},{"cell_type":"markdown","source":"<center><img src = \"https://i.imgur.com/1sm6x8P.png\" width = \"750\" height = \"500\"/></center>  ","metadata":{}},{"cell_type":"markdown","source":"**Weights & Biases** is the machine learning platform for developers to build better models faster.\n\nYou can use W&B's lightweight, interoperable tools to\n\n- quickly track experiments,\n- version and iterate on datasets,\n- evaluate model performance,\n- reproduce models,\n- visualize results and spot regressions,\n- and share findings with colleagues.\n  \nSet up W&B in 5 minutes, then quickly iterate on your machine learning pipeline with the confidence that your datasets and models are tracked and versioned in a reliable system of record.\n\nIn this notebook I will use Weights and Biases's amazing features to perform wonderful visualizations and logging seamlessly.","metadata":{}},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"<a id=\"load-datasets\"></a>\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:orange; border:0; color:white' role=\"tab\" aria-controls=\"home\"><center>Load Datasets</center></h2>","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv(config.TRAIN_CSV_PATH)","metadata":{"execution":{"iopub.status.busy":"2022-01-03T16:53:19.078613Z","iopub.execute_input":"2022-01-03T16:53:19.078891Z","iopub.status.idle":"2022-01-03T16:53:21.340093Z","shell.execute_reply.started":"2022-01-03T16:53:19.078863Z","shell.execute_reply":"2022-01-03T16:53:21.338803Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-03T16:53:21.341931Z","iopub.execute_input":"2022-01-03T16:53:21.342538Z","iopub.status.idle":"2022-01-03T16:53:21.373588Z","shell.execute_reply.started":"2022-01-03T16:53:21.342506Z","shell.execute_reply":"2022-01-03T16:53:21.372029Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission = pd.read_csv(\"../input/feedback-prize-2021/sample_submission.csv\")","metadata":{"execution":{"iopub.status.busy":"2022-01-03T17:38:07.295859Z","iopub.execute_input":"2022-01-03T17:38:07.29623Z","iopub.status.idle":"2022-01-03T17:38:07.314973Z","shell.execute_reply.started":"2022-01-03T17:38:07.296207Z","shell.execute_reply":"2022-01-03T17:38:07.313861Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-03T17:38:10.648426Z","iopub.execute_input":"2022-01-03T17:38:10.64948Z","iopub.status.idle":"2022-01-03T17:38:10.660794Z","shell.execute_reply.started":"2022-01-03T17:38:10.64942Z","shell.execute_reply":"2022-01-03T17:38:10.659964Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"what-is-data-augmentation\"></a>\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:orange; border:0; color:white' role=\"tab\" aria-controls=\"home\"><center>What is Data Augmentation</center></h2>","metadata":{}},{"cell_type":"markdown","source":"## **<span style=\"color:orange;\">Definition</span>**\n\nData Augmentation is a technique used to increase the amount of data by adding slightly modified copies of already existing data or newly created synthetic data from existing data.\n\n## **<span style=\"color:orange;\">Why is Data Augmentation Important?</span>**\nData augmentation is useful to improve performance and outcomes of machine learning models by forming new and different examples to train datasets. If dataset in a machine learning model is rich and sufficient, the model performs better and more accurate.\n  \nFor machine learning models, collecting and labeling of data can be exhausting and costly processes. Transformations in datasets by using data augmentation techniques allow companies to reduce these operational costs.\n\n---","metadata":{}},{"cell_type":"markdown","source":"<a id=\"basic-nlp-augmentations\"></a>\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:orange; border:0; color:white' role=\"tab\" aria-controls=\"home\"><center>Basic NLP Augmentations</center></h2>","metadata":{}},{"cell_type":"markdown","source":"In this section we will look at some simple transforms that can be performed on Text Data. To develop this section I am referring to [this](https://www.kaggle.com/shonenkov/nlp-albumentations) notebook by Alex Shonenkov. I will build upon those transforms with more examples and techniques.\n\n**I will demonstrate the 7 basic transforms, namely:**\n1. [Sentence Shuffling](#basic-1)\n2. [Remove Duplicate Sentences](#basic-2)\n3. [Remove Numbers](#basic-3)\n4. [Remove Hashtags](#basic-4)\n5. [Remove Mentions](#basic-5)\n6. [Remove URLs](#basic-6)\n7. [Cut Out Words](#basic-7)","metadata":{}},{"cell_type":"code","source":"class TextAugmentation(BasicTransform):\n    \n    \"\"\" \n    Transform for NLP task.\n    \"\"\"\n\n    @property\n    def targets(self):\n        return {\"data\": self.apply}\n    \n    def update_params(self, params, **kwargs):\n        if hasattr(self, \"interpolation\"):\n            params[\"interpolation\"] = self.interpolation\n        if hasattr(self, \"fill_value\"):\n            params[\"fill_value\"] = self.fill_value\n        return params\n\n    def get_sentences(self, text):\n        return sent_tokenize(text)","metadata":{"execution":{"iopub.status.busy":"2022-01-03T16:53:23.885914Z","iopub.execute_input":"2022-01-03T16:53:23.886179Z","iopub.status.idle":"2022-01-03T16:53:23.894951Z","shell.execute_reply.started":"2022-01-03T16:53:23.88615Z","shell.execute_reply":"2022-01-03T16:53:23.893524Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def demo(augmentation, text):\n    \n    \"\"\"\n    Function to demonstrate the applied augmentation.\n    \n    params:\n        augmentation - The augmentation to be applied on the text\n        text: Text on which transform will be applied\n        \n    \"\"\"\n    \n    output = augmentation(data=(text))['data']\n\n    print(\"Original Text\")\n    print(colored(text, 'red'))\n    \n    print()\n    \n    print(\"Augmented Text\")\n    print(colored(output, 'yellow'))","metadata":{"execution":{"iopub.status.busy":"2022-01-03T17:11:55.71299Z","iopub.execute_input":"2022-01-03T17:11:55.713236Z","iopub.status.idle":"2022-01-03T17:11:55.718696Z","shell.execute_reply.started":"2022-01-03T17:11:55.713211Z","shell.execute_reply":"2022-01-03T17:11:55.71788Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"## **<span style=\"color:orange;\">1. Sentence Shuffling</span>**\n<a id=\"basic-1\"></a>\n\nIn Sentence Shuffling we will randomly shuffle the sentences of the text.","metadata":{}},{"cell_type":"code","source":"class SentenceShuffleAugmentation(TextAugmentation):\n    \"\"\" Shuffle the sentences of the text. \"\"\"\n    \n    def __init__(self, always_apply=False, p=0.5):\n        super(SentenceShuffleAugmentation, self).__init__(always_apply, p)\n\n    def apply(self, data, **params):\n        text = data\n        sentences = self.get_sentences(text)\n        random.shuffle(sentences)\n        return ' '.join(sentences)","metadata":{"execution":{"iopub.status.busy":"2022-01-03T16:53:25.875924Z","iopub.execute_input":"2022-01-03T16:53:25.876213Z","iopub.status.idle":"2022-01-03T16:53:25.885522Z","shell.execute_reply.started":"2022-01-03T16:53:25.876183Z","shell.execute_reply":"2022-01-03T16:53:25.884389Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"demo(\n    augmentation = SentenceShuffleAugmentation(p=1.0),\n    text = train.iloc[0, 4]\n)","metadata":{"execution":{"iopub.status.busy":"2022-01-03T17:11:47.37545Z","iopub.execute_input":"2022-01-03T17:11:47.375846Z","iopub.status.idle":"2022-01-03T17:11:47.382749Z","shell.execute_reply.started":"2022-01-03T17:11:47.375804Z","shell.execute_reply":"2022-01-03T17:11:47.381686Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **<span style=\"color:orange;\">2. Remove Duplicate Sentences</span>**\n<a id=\"basic-2\"></a>\n\nIn Remove Duplicates we will remove duplicate sentences from the text. \n\nTo demonstrate this, we concatenate a sentence with itself and output should only be the original text.","metadata":{}},{"cell_type":"code","source":"class RemoveDuplicateSentencesAugmentation(TextAugmentation):\n    \"\"\" Exclude equal sentences \"\"\"\n    def __init__(self, always_apply=False, p=0.5):\n        super(RemoveDuplicateSentencesAugmentation, self).__init__(always_apply, p)\n\n    def apply(self, data, **params):\n        text = data\n        sentences = []\n        for sentence in self.get_sentences(text):\n            sentence = sentence.strip()\n            if sentence not in sentences:\n                sentences.append(sentence)\n        return ' '.join(sentences)","metadata":{"execution":{"iopub.status.busy":"2022-01-03T16:53:27.947091Z","iopub.execute_input":"2022-01-03T16:53:27.947301Z","iopub.status.idle":"2022-01-03T16:53:27.952622Z","shell.execute_reply.started":"2022-01-03T16:53:27.947277Z","shell.execute_reply":"2022-01-03T16:53:27.951995Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"demo(\n    augmentation = RemoveDuplicateSentencesAugmentation(p=1.0),\n    text = train.iloc[0, 4][0:46] + \" \" + train.iloc[0, 4][0:46]\n)","metadata":{"execution":{"iopub.status.busy":"2022-01-03T17:12:01.694057Z","iopub.execute_input":"2022-01-03T17:12:01.694488Z","iopub.status.idle":"2022-01-03T17:12:01.70112Z","shell.execute_reply.started":"2022-01-03T17:12:01.694459Z","shell.execute_reply":"2022-01-03T17:12:01.700004Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"## **<span style=\"color:orange;\">3. Remove Numbers</span>**\n<a id=\"basic-3\"></a>\n\nIn Remove Numbers, we will remove any number from the text. We can simply achieve this using regular expressions.","metadata":{}},{"cell_type":"code","source":"class RemoveNumbersAugmentation(TextAugmentation):\n    \"\"\" Exclude any numbers \"\"\"\n    def __init__(self, always_apply=False, p=0.5):\n        super(RemoveNumbersAugmentation, self).__init__(always_apply, p)\n\n    def apply(self, data, **params):\n        text = data\n        text = re.sub(r'[0-9]', '', text)\n        text = re.sub(r'\\s+', ' ', text)\n        return text","metadata":{"execution":{"iopub.status.busy":"2022-01-03T16:53:32.698053Z","iopub.execute_input":"2022-01-03T16:53:32.698309Z","iopub.status.idle":"2022-01-03T16:53:32.705089Z","shell.execute_reply.started":"2022-01-03T16:53:32.698273Z","shell.execute_reply":"2022-01-03T16:53:32.7038Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"demo(\n    augmentation = RemoveNumbersAugmentation(p=1.0),\n    text = \"There are 15594 samples of training data.\"\n)","metadata":{"execution":{"iopub.status.busy":"2022-01-03T17:12:06.674901Z","iopub.execute_input":"2022-01-03T17:12:06.67516Z","iopub.status.idle":"2022-01-03T17:12:06.680502Z","shell.execute_reply.started":"2022-01-03T17:12:06.675123Z","shell.execute_reply":"2022-01-03T17:12:06.679841Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"## **<span style=\"color:orange;\">4. Remove Hashtags</span>**\n<a id=\"basic-4\"></a>\n\nIn Remove Hashtags, we will remove any hashtag from the text. ","metadata":{}},{"cell_type":"code","source":"class RemoveHashtagsAugmentation(TextAugmentation):\n    \"\"\" Exclude any hashtags with # \"\"\"\n    def __init__(self, always_apply=False, p=0.5):\n        super(RemoveHashtagsAugmentation, self).__init__(always_apply, p)\n\n    def apply(self, data, **params):\n        text = data\n        text = re.sub(r'#[\\S]+\\b', '', text)\n        text = re.sub(r'\\s+', ' ', text)\n        return text","metadata":{"execution":{"iopub.status.busy":"2022-01-03T16:57:04.919137Z","iopub.execute_input":"2022-01-03T16:57:04.919393Z","iopub.status.idle":"2022-01-03T16:57:04.926455Z","shell.execute_reply.started":"2022-01-03T16:57:04.919365Z","shell.execute_reply":"2022-01-03T16:57:04.925513Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"demo(\n    augmentation = RemoveHashtagsAugmentation(p=1.0),\n    text = \"Kaggle Competitions are fun. #MachineLearning\"\n)","metadata":{"execution":{"iopub.status.busy":"2022-01-03T17:12:11.973958Z","iopub.execute_input":"2022-01-03T17:12:11.974961Z","iopub.status.idle":"2022-01-03T17:12:11.982006Z","shell.execute_reply.started":"2022-01-03T17:12:11.974887Z","shell.execute_reply":"2022-01-03T17:12:11.980879Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"## **<span style=\"color:orange;\">5. Remove Mentions</span>**\n<a id=\"basic-5\"></a>\n\nIn this transform we remove any mentions (word beginning with '@') from the text.","metadata":{}},{"cell_type":"code","source":"class RemoveMentionsAugmentation(TextAugmentation):\n    \"\"\" Exclude @users \"\"\"\n    \n    def __init__(self, always_apply=False, p=0.5):\n        super(RemoveMentionsAugmentation, self).__init__(always_apply, p)\n\n    def apply(self, data, **params):\n        text = data\n        text = re.sub(r'@[\\S]+\\b', '', text)\n        text = re.sub(r'\\s+', ' ', text)\n        return text","metadata":{"execution":{"iopub.status.busy":"2022-01-03T17:00:49.648617Z","iopub.execute_input":"2022-01-03T17:00:49.648878Z","iopub.status.idle":"2022-01-03T17:00:49.656442Z","shell.execute_reply.started":"2022-01-03T17:00:49.648849Z","shell.execute_reply":"2022-01-03T17:00:49.655284Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"demo(\n    augmentation = RemoveMentionsAugmentation(p=1.0),\n    text = \"@AnthonyGoldbloom is the founder of Kaggle.\"\n)","metadata":{"execution":{"iopub.status.busy":"2022-01-03T17:12:17.026814Z","iopub.execute_input":"2022-01-03T17:12:17.027444Z","iopub.status.idle":"2022-01-03T17:12:17.032869Z","shell.execute_reply.started":"2022-01-03T17:12:17.027414Z","shell.execute_reply":"2022-01-03T17:12:17.031632Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"## **<span style=\"color:orange;\">6. Remove URLs</span>**\n<a id=\"basic-6\"></a>\n\nIn this transform we remove any URLs from the text.","metadata":{}},{"cell_type":"code","source":"class RemoveUrlAugmentation(TextAugmentation):\n    \"\"\" Exclude urls \"\"\"\n    \n    def __init__(self, always_apply=False, p=0.5):\n        super(RemoveUrlAugmentation, self).__init__(always_apply, p)\n\n    def apply(self, data, **params):\n        text = data\n        text = re.sub(r'https?\\S+', '', text)\n        text = re.sub(r'\\s+', ' ', text)\n        return text","metadata":{"execution":{"iopub.status.busy":"2022-01-03T17:04:22.203731Z","iopub.execute_input":"2022-01-03T17:04:22.204598Z","iopub.status.idle":"2022-01-03T17:04:22.214163Z","shell.execute_reply.started":"2022-01-03T17:04:22.204533Z","shell.execute_reply":"2022-01-03T17:04:22.213176Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"demo(\n    augmentation = RemoveUrlAugmentation(p=1.0),\n    text = \"https://www.kaggle.com hosts the world's best Machine Learning Hackathons.\"\n)","metadata":{"execution":{"iopub.status.busy":"2022-01-03T17:12:21.808088Z","iopub.execute_input":"2022-01-03T17:12:21.808467Z","iopub.status.idle":"2022-01-03T17:12:21.814349Z","shell.execute_reply.started":"2022-01-03T17:12:21.808428Z","shell.execute_reply":"2022-01-03T17:12:21.813132Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"## **<span style=\"color:orange;\">7. Cut Out Words</span>**\n<a id=\"basic-7\"></a>\n\nIn this transform, we remove some words from the text.","metadata":{}},{"cell_type":"code","source":"class CutOutWordsAugmentation(TextAugmentation):\n    \"\"\" Remove random words \"\"\"\n    \n    def __init__(self, cutout_probability=0.05, always_apply=False, p=0.5):\n        super(CutOutWordsTransform, self).__init__(always_apply, p)\n        self.cutout_probability = cutout_probability\n\n    def apply(self, data, **params):\n        text = data\n        words = text.split()\n        words_count = len(words)\n        if words_count <= 1:\n            return text\n        \n        new_words = []\n        for i in range(words_count):\n            if random.random() < self.cutout_probability:\n                continue\n            new_words.append(words[i])\n\n        if len(new_words) == 0:\n            return words[random.randint(0, words_count-1)]\n\n        return ' '.join(new_words)","metadata":{"execution":{"iopub.status.busy":"2022-01-03T17:23:08.565552Z","iopub.execute_input":"2022-01-03T17:23:08.565803Z","iopub.status.idle":"2022-01-03T17:23:08.573852Z","shell.execute_reply.started":"2022-01-03T17:23:08.565779Z","shell.execute_reply":"2022-01-03T17:23:08.573075Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"demo(\n    augmentation = CutOutWordsTransform(p=1.0, cutout_probability=0.2),\n    text = \"Competition objective is to analyze argumentative writing elements from students grade 6-12.\"\n)","metadata":{"execution":{"iopub.status.busy":"2022-01-03T17:23:12.23086Z","iopub.execute_input":"2022-01-03T17:23:12.231329Z","iopub.status.idle":"2022-01-03T17:23:12.235974Z","shell.execute_reply.started":"2022-01-03T17:23:12.231301Z","shell.execute_reply":"2022-01-03T17:23:12.235231Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"model\"></a>\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:orange; border:0; color:white' role=\"tab\" aria-controls=\"home\"><center>Model</center></h2>","metadata":{}},{"cell_type":"markdown","source":"I am demonstrating a simple NER model where you can try applying above transforms and see the change in results. \n\nNote that some of these transforms might not be suitable entirely for NER, you can try them for other applications as well. For the model I am referring to [this](https://www.kaggle.com/lonnieqin/name-entity-recognition-with-keras) notebook.","metadata":{}},{"cell_type":"markdown","source":"## **<span style=\"color:orange;\">Model Configuration</span>**","metadata":{}},{"cell_type":"code","source":"vocab_size = 10000                    # Vocabulary size\nsequence_length = 1024                # Sequence Length\nbatch_size = 128                      # Batch size\nunk_token = \"<unk>\"                   # Unknownd token\nvectorizer_path = \"vectorizer.json\"\n\n# Use output dataset for inference\noutput_dataset_path = \"../input/name-entity-recognition-with-keras-output/\"\nmodel_path = \"model.h5\"\nembed_size = 64\nhidden_size = 64\n\nmodes = [\"training\", \"inference\"]     # There is training and inference mode\nmode = modes[1]\nepochs = 10\ndropout = 0.2                         # Dropout rate for the Model","metadata":{"execution":{"iopub.status.busy":"2022-01-03T18:04:01.869518Z","iopub.execute_input":"2022-01-03T18:04:01.869816Z","iopub.status.idle":"2022-01-03T18:04:01.874631Z","shell.execute_reply.started":"2022-01-03T18:04:01.869781Z","shell.execute_reply":"2022-01-03T18:04:01.873838Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# wandb config\nWANDB_CONFIG = {\n     'competition': 'Feedback Prize', \n              '_wandb_kernel': 'neuracort'\n    }","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **<span style=\"color:orange;\">Preprocessing</span>**","metadata":{}},{"cell_type":"code","source":"train[\"file_path\"] = train[\"id\"].apply(\n    lambda item: \"../input/feedback-prize-2021/train/\" + item + \".txt\"\n)\n\nsubmission[\"file_path\"] = submission[\"id\"].apply(\n    lambda item: \"../input/feedback-prize-2021/test/\" + item + \".txt\"\n)\n\ndiscourse_types = np.array(\n    [\"<PAD>\", \"<None>\"] + sorted(train[\"discourse_type\"].unique())\n)\n\ndiscourse_types_index = dict(\n    [(discoure_type, index) for (index, discoure_type) in enumerate(discourse_types)]\n)","metadata":{"execution":{"iopub.status.busy":"2022-01-03T18:04:04.666271Z","iopub.execute_input":"2022-01-03T18:04:04.666516Z","iopub.status.idle":"2022-01-03T18:04:04.731217Z","shell.execute_reply.started":"2022-01-03T18:04:04.666491Z","shell.execute_reply":"2022-01-03T18:04:04.730209Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **<span style=\"color:orange;\">Tokenization</span>**","metadata":{}},{"cell_type":"code","source":"def get_range(item):\n    locations = [int(location) for location in item[\"predictionstring\"].split(\" \")]\n    return (locations[0], locations[-1])","metadata":{"execution":{"iopub.status.busy":"2022-01-03T18:04:05.711208Z","iopub.execute_input":"2022-01-03T18:04:05.711463Z","iopub.status.idle":"2022-01-03T18:04:05.716143Z","shell.execute_reply.started":"2022-01-03T18:04:05.711437Z","shell.execute_reply":"2022-01-03T18:04:05.715044Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"character_counter = defaultdict(int)\ncharacter_counter\nallow_set = set(\"'&%-_/$+ÂÃÅËÓâåóþ@|~¢£¢£\")\n\ndef tokenize(text):\n    \n    tokens = []\n    chars = []\n    \n    for i in range(len(text)):\n        c = text[i].lower()\n        character_counter[c] += 1\n        is_valid = c.isalnum() or c in allow_set\n    \n        if i >= 1 and i < len(text) - 1:\n            if text[i-1].isdigit() and text[i+1].isdigit():\n                is_valid = True\n            elif text[i-1].isalpha() and text[i+1].isalpha() and c == \".\":\n                is_valid = True\n        \n        if is_valid:\n            chars.append(c)\n        \n        if (not is_valid or i == len(text) - 1) and len(chars) > 0:\n            tokens.append(\"\".join(chars))\n            chars.clear()\n    \n    return tokens","metadata":{"execution":{"iopub.status.busy":"2022-01-03T18:04:06.351322Z","iopub.execute_input":"2022-01-03T18:04:06.351617Z","iopub.status.idle":"2022-01-03T18:04:06.359435Z","shell.execute_reply.started":"2022-01-03T18:04:06.351582Z","shell.execute_reply":"2022-01-03T18:04:06.358648Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\nbegin = time.time()\nlast_id = \"\"\ncontents = []\nwrong_samples = []\ntoken_list = []\nannotation_list = []\n\nnum_samples = len(train)\nunmaptch_count = 0              # Number of sentences extracted from predictionstring that doesn't discourse_text\nmatch_count = 0                 # Number of sentences extracted from predictionstring that matches discourse_text including shifting\ncompletely_match_count = 0      # Number of sentences extracted from predictionstring that matches discourse_text without shifting\nmismatch_count = 0\n\nfor i in range(len(train)):\n    item = train.iloc[i]\n    identifier = item[\"id\"] \n    discourse_type_id = discourse_types_index[item[\"discourse_type\"]]\n    \n    if identifier != last_id:\n        last_id = identifier\n    \n        with open(item[\"file_path\"]) as f:\n            content = \"\".join(f.readlines())\n            contents.append(content)\n            tokens = tokenize(content)\n            token_list.append(tokens)\n            annotations = [1] * len(tokens)\n            annotation_list.append(annotations)\n    \n    annotation_range = get_range(item)\n    extracted = tokens[annotation_range[0]:annotation_range[1]+1]\n    discourse = tokenize(item[\"discourse_text\"])\n    delta = None\n    num_tokens_to_compare = min(len(discourse), 3)\n    \n    # Compare text extracted from predictionstring with discourse_text, shift discourse_text or right if needed, just compare a few words for performance\n    for j in range(10):\n    \n        if len(extracted) < num_tokens_to_compare or len(discourse) <= j + num_tokens_to_compare:\n            break\n        \n        if extracted[0:num_tokens_to_compare] == discourse[j:num_tokens_to_compare+j]:\n            delta = j\n            break\n    \n    if delta == None:\n        for j in range(10):\n            if len(discourse) < num_tokens_to_compare and len(extracted) <= j + num_tokens_to_compare:\n                break\n            \n            if discourse[0:num_tokens_to_compare] == extracted[j:num_tokens_to_compare+j]:\n                delta = -j\n                break\n    \n    if delta == None:\n        unmaptch_count += 1\n    \n    else:\n        not_match = False\n        for j in range(annotation_range[0] - delta, min(min(annotation_range[1] - delta + 1, len(tokens)), len(discourse) + annotation_range[0] - delta)): \n            if tokens[j] != discourse[j - annotation_range[0] + delta]:\n                mismatch_count += 1\n                not_match = True\n                break\n        \n        if not not_match:\n            for j in range(annotation_range[0] - delta, min(min(annotation_range[1] - delta + 1, len(tokens)), len(discourse) + annotation_range[0] - delta)): \n                annotation_list[-1][j] = discourse_type_id\n            match_count += 1\n        \n        else:\n            unmaptch_count += 1\n        \n        if delta == 0:\n            completely_match_count += 1 \n\nprint(\"Unmatch count:%d Match Count: %d Completedly Match count: %d\"%(unmaptch_count, match_count, completely_match_count))\nprint(\"Mismatch count:\", mismatch_count)\nprint(token_list[0])\nprint(annotation_list[0])","metadata":{"execution":{"iopub.status.busy":"2022-01-03T18:04:07.162666Z","iopub.execute_input":"2022-01-03T18:04:07.163174Z","iopub.status.idle":"2022-01-03T18:08:03.47314Z","shell.execute_reply.started":"2022-01-03T18:04:07.163145Z","shell.execute_reply":"2022-01-03T18:08:03.472206Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **<span style=\"color:orange;\">Vectorization</span>**","metadata":{}},{"cell_type":"code","source":"class Vectorizer:\n    \n    def __init__(self, vocab_size = None, sequence_length = None, unk_token = \"<unk>\"):\n        \n        self.vocab_size = vocab_size\n        self.sequence_length = sequence_length\n        self.unk_token = unk_token\n        \n    def fit_transform(self, sentences):\n        \n        word_counter = dict()\n        \n        for tokens in sentences:\n            for token in tokens: \n                if token in word_counter:\n                    word_counter[token] += 1\n                else:\n                    word_counter[token] = 1\n        \n        word_counter = pd.DataFrame({\"key\": word_counter.keys(), \"count\": word_counter.values()})\n        word_counter.sort_values(by=\"count\", ascending=False, inplace=True)\n        vocab = set(word_counter[\"key\"][0:self.vocab_size-1])\n        \n        word_index = dict()\n        begin_index = 1 \n        word_index[self.unk_token] = begin_index\n        begin_index += 1\n        \n        Xs = []\n        \n        for i in range(len(sentences)):\n            X = []\n            \n            for token in sentences[i]:\n                \n                if token not in word_index and token in vocab:\n                    word_index[token] = begin_index\n                    begin_index += 1\n                \n                if token in word_index:\n                    X.append(word_index[token])\n                \n                else:\n                    X.append(word_index[self.unk_token])\n                \n                if len(X) == self.sequence_length:\n                    break\n            \n            if len(X) < self.sequence_length:\n                X += [0] * (self.sequence_length - len(X))\n            \n            Xs.append(X)\n        \n        self.word_index = word_index\n        self.vocab = vocab\n        \n        return Xs\n    \n    def transform(self, sentences):\n        \n        Xs = []\n        \n        for i in range(len(sentences)):\n            X = []\n            \n            for token in sentences[i]:\n                if token in self.word_index:\n                    X.append(self.word_index[token])\n                else:\n                    X.append(self.word_index[self.unk_token])\n                if len(X) == self.sequence_length:\n                    break\n            \n            if len(X) < self.sequence_length:\n                X += [0] * (self.sequence_length - len(X))\n            Xs.append(X)\n        \n        return Xs\n    \n    def load(self, path):\n        \n        with open(path, 'r') as f:\n            \n            dic = json.load(f)\n            self.vocab_size = dic['vocab_size']\n            self.sequence_length = dic['sequence_length']\n            self.unk_token = dic['unk_token']\n            self.word_index = dic['word_index']\n            \n    def save(self, path):\n        \n        with open(path, 'w') as f:\n            \n            data = json.dumps({\n                \"vocab_size\": self.vocab_size, \n                \"sequence_length\": self.sequence_length, \n                \"unk_token\": self.unk_token,\n                \"word_index\": self.word_index\n            })\n            \n            f.write(data)","metadata":{"execution":{"iopub.status.busy":"2022-01-03T18:08:03.475049Z","iopub.execute_input":"2022-01-03T18:08:03.476154Z","iopub.status.idle":"2022-01-03T18:08:03.507373Z","shell.execute_reply.started":"2022-01-03T18:08:03.476117Z","shell.execute_reply":"2022-01-03T18:08:03.506178Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\nvectorizer = Vectorizer(vocab_size = vocab_size, sequence_length = sequence_length, unk_token = unk_token)\n\nif mode == modes[0]:\n    Xs = vectorizer.fit_transform(token_list)\n    vectorizer.save(vectorizer_path)\n\nelse:\n    vectorizer.load(output_dataset_path + vectorizer_path)\n    Xs = vectorizer.transform(token_list)\n\nys = []\nannotation_count = [0] * len(discourse_types_index)\n\nfor annotation in annotation_list:\n    if len(annotation) <= sequence_length:\n        ys.append(annotation + [0] * (sequence_length - len(annotation)))\n    else:\n        ys.append(annotation[0:sequence_length])\n    for item in ys[-1]:\n        annotation_count[item] += 1\n\nX_train, X_val, y_train, y_val = train_test_split(np.array(Xs), np.array(ys), test_size = 0.2, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2022-01-03T18:08:03.508517Z","iopub.execute_input":"2022-01-03T18:08:03.508765Z","iopub.status.idle":"2022-01-03T18:08:18.937281Z","shell.execute_reply.started":"2022-01-03T18:08:03.508734Z","shell.execute_reply":"2022-01-03T18:08:18.936209Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **<span style=\"color:orange;\">Dataset</span>**","metadata":{}},{"cell_type":"code","source":"def make_dataset(X, y, batch_size, mode=\"train\"):\n    ds = tf.data.Dataset.from_tensor_slices((X, y))\n    if mode == \"train\":\n        ds = ds.shuffle(512)\n    ds = ds.batch(batch_size).cache().prefetch(tf.data.AUTOTUNE)\n    \n    return ds","metadata":{"execution":{"iopub.status.busy":"2022-01-03T18:08:18.939881Z","iopub.execute_input":"2022-01-03T18:08:18.940549Z","iopub.status.idle":"2022-01-03T18:08:18.947858Z","shell.execute_reply.started":"2022-01-03T18:08:18.940468Z","shell.execute_reply":"2022-01-03T18:08:18.947019Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_ds = make_dataset(X_train, y_train, batch_size)\nval_ds = make_dataset(X_val, y_val, batch_size, mode=\"valid\")","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-01-03T18:08:18.949222Z","iopub.execute_input":"2022-01-03T18:08:18.950146Z","iopub.status.idle":"2022-01-03T18:08:19.04875Z","shell.execute_reply.started":"2022-01-03T18:08:18.950103Z","shell.execute_reply":"2022-01-03T18:08:19.047525Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **<span style=\"color:orange;\">Model</span>**","metadata":{}},{"cell_type":"code","source":"model = keras.Sequential([\n    keras.layers.Embedding(vocab_size, embed_size, input_length=sequence_length),\n    keras.layers.SpatialDropout1D(dropout),\n    keras.layers.Bidirectional(keras.layers.LSTM(hidden_size, dropout=dropout, recurrent_dropout=dropout)),\n    keras.layers.RepeatVector(sequence_length),\n    keras.layers.Bidirectional(keras.layers.LSTM(hidden_size, return_sequences=True)),\n    keras.layers.TimeDistributed(keras.layers.Dense(len(discourse_types), activation=\"softmax\"))\n])\n\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2022-01-03T18:08:19.073131Z","iopub.execute_input":"2022-01-03T18:08:19.073682Z","iopub.status.idle":"2022-01-03T18:08:19.850986Z","shell.execute_reply.started":"2022-01-03T18:08:19.073651Z","shell.execute_reply":"2022-01-03T18:08:19.850116Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Initialize W&B\nrun = wandb.init(project='feedback-prize-nlp-augmentations', config=WANDB_CONFIG)\n\nif mode == modes[0]:\n    checkpoint = keras.callbacks.ModelCheckpoint(\n        model_path, \n        save_best_only=True,\n        save_weights_only=True\n    )\n    \n    early_stop = keras.callbacks.EarlyStopping(\n        min_delta=1e-4, \n        patience=10\n    )\n    \n    reduce_lr = keras.callbacks.ReduceLROnPlateau(\n        factor=0.3,\n        patience=2, \n        min_lr=1e-7\n    )\n    \n    loss = tf.keras.losses.SparseCategoricalCrossentropy()\n    callbacks = [early_stop, checkpoint, reduce_lr, WandbCallback()]\n    optimizer = tf.keras.optimizers.Adam(1e-3)\n    model.compile(loss=loss, optimizer=optimizer)\n    model.fit(train_ds, epochs=epochs, validation_data=val_ds, callbacks=callbacks)\n    wandb.log({'loss':loss})\n\nelse:\n    model.load_weights(output_dataset_path + model_path)\n    \nwandb.finish()","metadata":{"execution":{"iopub.status.busy":"2022-01-03T18:08:19.852526Z","iopub.execute_input":"2022-01-03T18:08:19.853755Z","iopub.status.idle":"2022-01-03T18:08:31.271733Z","shell.execute_reply.started":"2022-01-03T18:08:19.853709Z","shell.execute_reply":"2022-01-03T18:08:31.270502Z"},"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"--- \n\n## **<span style=\"color:orange;\">Let's have a Talk!</span>**\n> ### Reach out to me on [LinkedIn](https://www.linkedin.com/in/ishandutta0098)\n\n---","metadata":{}}]}