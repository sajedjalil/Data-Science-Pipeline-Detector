{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Simple PyTorch NER Training on GPU\n\n#### A simple way to train an NER model that will classify each token into a discourse type. This uses the Hugging Face `Trainer` and their default `ModelForTokenClassification`, but you can also stick a custom `nn.Module` into it as well. It has already been seen that Longformer does well, so let's try its close relative: BigBird. It's another model that has a novel attention mechanism that can handle long sequence lengths. There are some benchmarks for long range models that show BigBird doing better than Longformer (see image below), but we'll have to see what works better for this competition. This is using the base version of BigBird, but you'll probably get better performance on the large version -- it will just take longer to train. \n\n#### This is only 1 fold and includes macro-f1 CV score. It would be wise to do k-fold validation with your favorite value of k to get a better sense of how your model is doing.\n\n## Coming soon:\n### âœ… Weights and biases reporting ([jump to section](#Weights-and-Biases-Report))\n### âœ… Custom evaluate function that does competition-specific f1 scoring ([jump to section](#Custom-Trainer))\n### ðŸ”³ Model with custom output layers\n### âœ… Model that has been pretrained further on in-domain data\n    - (See Version 9 for results) Modest improvement of CV from 0.535 -> 0.543\n    - More experiments will be needed to verify value\n    - Pretrained BigBird base model available here: https://www.kaggle.com/nbroad/bigbird-base-idpt-feedback-prize\n### ðŸ”³ Comparison of BIO vs IO labeling scheme\n### ðŸ”³ Comparison of tokenizing after splitting at whitespace vs tokenizing while keeping whitespace\n### ðŸ”³ Any suggestions?\n\n\n### ðŸš¨ ~~It appears that there might be an issue with the bigbird tokenizer ([warning similar to here](https://github.com/huggingface/transformers/pull/11075#issuecomment-833404825)). Will update this when confirming it is/isn't working properly.~~ False alarm ðŸ˜…\n\n\n### ðŸ‘‰ Version 5 (CV F1: 0.535) has the outputs of the first full training without pretraining\n### ðŸ‘‰ Version 9 (CV F1: 0.543) has the outputs of the first full training with pretraining\n### ðŸ‘‰ Version 16 fixes error when calcualting CV score. Also uses corrected training file\n### ðŸ‘‰ Version 17 (CV F1: 0.609) has cleaner train/val splits. Now it doesn't have overlapping file ids. Rookie mistake ðŸ˜¬\n\n<img src=\"https://user-images.githubusercontent.com/1694368/102184635-baab7400-3eea-11eb-8113-b3fb6d4b8bbc.png\" alt=\"benchmark comparison\" width=\"800\"/>\n\n### Code based off of: \n\n- [run_ner.py](https://github.com/huggingface/transformers/blob/master/examples/pytorch/token-classification/run_ner.py)\n- [Rob Mulla's implementation of f1 score (@robikscube)](https://www.kaggle.com/robikscube/student-writing-competition-twitch)\n- [zzy's infer notebook](https://www.kaggle.com/zzy990106/pytorch-ner-infer?scriptVersionId=82677278&cellId=13)","metadata":{}},{"cell_type":"code","source":"# necessary for metrics\n!pip install seqeval -qq","metadata":{"execution":{"iopub.status.busy":"2022-01-09T00:17:22.480856Z","iopub.execute_input":"2022-01-09T00:17:22.481272Z","iopub.status.idle":"2022-01-09T00:17:29.762656Z","shell.execute_reply.started":"2022-01-09T00:17:22.481189Z","shell.execute_reply":"2022-01-09T00:17:29.761629Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nfrom pathlib import Path\nfrom functools import partial\nfrom dataclasses import dataclass, field\nfrom typing import Optional\n\nimport pandas as pd\nimport numpy as np\nimport torch\nimport datasets\nfrom datasets import ClassLabel, load_dataset, load_metric, Dataset\n\nimport transformers\nfrom transformers import (\n    AutoConfig,\n    AutoModelForTokenClassification,\n    AutoTokenizer,\n    DataCollatorForTokenClassification,\n    HfArgumentParser,\n    PreTrainedTokenizerFast,\n    Trainer,\n    TrainingArguments,\n    set_seed,\n)","metadata":{"execution":{"iopub.status.busy":"2022-01-09T00:17:29.764629Z","iopub.execute_input":"2022-01-09T00:17:29.765186Z","iopub.status.idle":"2022-01-09T00:17:32.474449Z","shell.execute_reply.started":"2022-01-09T00:17:29.765144Z","shell.execute_reply":"2022-01-09T00:17:32.473649Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Arguments hidden in next cell","metadata":{}},{"cell_type":"code","source":"@dataclass\nclass ModelArguments:\n    \"\"\"\n    Arguments pertaining to which model/config/tokenizer we are going to fine-tune from.\n    \"\"\"\n\n    model_name_or_path: str = field(\n        metadata={\"help\": \"Path to pretrained model or model identifier from huggingface.co/models\"}\n    )\n    config_name: Optional[str] = field(\n        default=None, metadata={\"help\": \"Pretrained config name or path if not the same as model_name\"}\n    )\n    tokenizer_name: Optional[str] = field(\n        default=None, metadata={\"help\": \"Pretrained tokenizer name or path if not the same as model_name\"}\n    )\n    cache_dir: Optional[str] = field(\n        default=None,\n        metadata={\"help\": \"Where do you want to store the pretrained models downloaded from huggingface.co\"},\n    )\n    model_revision: str = field(\n        default=\"main\",\n        metadata={\"help\": \"The specific model version to use (can be a branch name, tag name or commit id).\"},\n    )\n    use_auth_token: bool = field(\n        default=False,\n        metadata={\n            \"help\": \"Will use the token generated when running `transformers-cli login` (necessary to use this script \"\n            \"with private models).\"\n        },\n    )\n    reinit_layers: int = field(\n        default=None,\n        metadata={\n            \"help\": \"Number of layers from the output to reinitialize in encoder.\"\n        }\n    )\n    use_custom_output: bool = field(\n        default=False,\n        metadata={\"help\": \"Set to true to use the custom model defined below\"}\n    )\n\n\n@dataclass\nclass DataTrainingArguments:\n    \"\"\"\n    Arguments pertaining to what data we are going to input our model for training and eval.\n    \"\"\"\n\n    task_name: Optional[str] = field(default=\"ner\", metadata={\"help\": \"The name of the task (ner, pos...).\"})\n    dataset_name: Optional[str] = field(\n        default=None, metadata={\"help\": \"The name of the dataset to use (via the datasets library).\"}\n    )\n    dataset_config_name: Optional[str] = field(\n        default=None, metadata={\"help\": \"The configuration name of the dataset to use (via the datasets library).\"}\n    )\n    train_file: Optional[str] = field(\n        default=None, metadata={\"help\": \"The input training data file (a csv or JSON file).\"}\n    )\n    validation_file: Optional[str] = field(\n        default=None,\n        metadata={\"help\": \"An optional input evaluation data file to evaluate on (a csv or JSON file).\"},\n    )\n    test_file: Optional[str] = field(\n        default=None,\n        metadata={\"help\": \"An optional input test data file to predict on (a csv or JSON file).\"},\n    )\n    val_split_percentage: Optional[float] = field(\n        default=None,\n        metadata={\"help\": \"How much of the data to use for doing train_test_split. If None, already assumes \"\n                  \"there are separate traiin and validation files\"},\n    )\n    text_column_name: Optional[str] = field(\n        default=None, metadata={\"help\": \"The column name of text to input in the file (a csv or JSON file).\"}\n    )\n    label_column_name: Optional[str] = field(\n        default=None, metadata={\"help\": \"The column name of label to input in the file (a csv or JSON file).\"}\n    )\n    overwrite_cache: bool = field(\n        default=False, metadata={\"help\": \"Overwrite the cached training and evaluation sets\"}\n    )\n    preprocessing_num_workers: Optional[int] = field(\n        default=None,\n        metadata={\"help\": \"The number of processes to use for the preprocessing.\"},\n    )\n    max_seq_length: int = field(\n        default=None,\n        metadata={\n            \"help\": \"The maximum total input sequence length after tokenization. If set, sequences longer \"\n            \"than this will be truncated, sequences shorter will be padded.\"\n        },\n    )\n    pad_to_max_length: bool = field(\n        default=False,\n        metadata={\n            \"help\": \"Whether to pad all samples to model maximum sentence length. \"\n            \"If False, will pad the samples dynamically when batching to the maximum length in the batch. More \"\n            \"efficient on GPU but very bad for TPU.\"\n        },\n    )\n    max_train_samples: Optional[int] = field(\n        default=None,\n        metadata={\n            \"help\": \"For debugging purposes or quicker training, truncate the number of training examples to this \"\n            \"value if set.\"\n        },\n    )\n    max_eval_samples: Optional[int] = field(\n        default=None,\n        metadata={\n            \"help\": \"For debugging purposes or quicker training, truncate the number of evaluation examples to this \"\n            \"value if set.\"\n        },\n    )\n    max_predict_samples: Optional[int] = field(\n        default=None,\n        metadata={\n            \"help\": \"For debugging purposes or quicker training, truncate the number of prediction examples to this \"\n            \"value if set.\"\n        },\n    )\n    label_all_tokens: bool = field(\n        default=False,\n        metadata={\n            \"help\": \"Whether to put the label for one word on all tokens of generated by that word or just on the \"\n            \"one (in which case the other tokens will have a padding index).\"\n        },\n    )\n    return_entity_level_metrics: bool = field(\n        default=False,\n        metadata={\"help\": \"Whether to return all the entity levels during evaluation or just the overall ones.\"},\n    )\n    use_bio: Optional[bool] = field(default=True, metadata={\"help\": \"If True, use BIO tagging. Otherwise use IO.\"})\n    data_file: str = field(default=None, metadata={\"help\": \"Path to single data file that will be used to make train and validation files.\"})\n    ground_truth_file: Optional[str] = field(default=\"../input/feedback-prize-2021/train.csv\", metadata={\"help\": \"Path to single data file that will be used to make train and validation files.\"})\n    def __post_init__(self):\n        if self.dataset_name is None and self.train_file is None and self.validation_file is None:\n            raise ValueError(\"Need either a dataset name or a training/validation file.\")\n        else:\n            if self.train_file is not None:\n                extension = self.train_file.split(\".\")[-1]\n                assert extension in [\"csv\", \"json\"], \"`train_file` should be a csv or a json file.\"\n            if self.validation_file is not None:\n                extension = self.validation_file.split(\".\")[-1]\n                assert extension in [\"csv\", \"json\"], \"`validation_file` should be a csv or a json file.\"\n        self.task_name = self.task_name.lower()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-01-09T00:17:32.476106Z","iopub.execute_input":"2022-01-09T00:17:32.47636Z","iopub.status.idle":"2022-01-09T00:17:32.502524Z","shell.execute_reply.started":"2022-01-09T00:17:32.476325Z","shell.execute_reply":"2022-01-09T00:17:32.501441Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_args = ModelArguments(\n    model_name_or_path=\"../input/bigbird-base-feedback-prize-idpt\",\n    reinit_layers=0,\n    use_custom_output=True,\n)\ndata_args = DataTrainingArguments(\n    train_file=\"train.json\",\n    validation_file=\"validation.json\",\n    val_split_percentage=0.1,\n    text_column_name=\"words\",\n    label_column_name=\"bio\",\n    max_seq_length=1024,\n    pad_to_max_length=True,\n    label_all_tokens=True,\n    return_entity_level_metrics=False, # I wouldn't recommend this-it produces a ton of metrics\n    preprocessing_num_workers=4,\n    use_bio=True,\n    data_file=\"../input/feedbackprize-bio-ner-train-data/corrected/split_at_whitespace.json\",\n    ground_truth_file=\"../input/feedback-prize-corrected-train-csv/corrected_train.csv\",\n    max_train_samples=None, # Useful for quick debugging runs. None means run all\n    max_eval_samples=None, # Useful for quick debugging runs. None means run all\n)\ntraining_args = TrainingArguments(\n    output_dir=\"feedback-prize-bigbird-base\",\n    do_train=True,\n    do_eval=True,\n    do_predict=True,\n    per_device_train_batch_size=4,\n    per_device_eval_batch_size=16,\n    gradient_accumulation_steps=2,\n    learning_rate=2e-5,\n    weight_decay=0.0,\n    num_train_epochs=3,\n    lr_scheduler_type=\"linear\",\n    warmup_ratio=0.1,\n    logging_steps=100,\n    evaluation_strategy=\"steps\",\n    eval_steps=600,\n    save_strategy=\"steps\",\n    save_steps=600,\n    save_total_limit=5,\n    load_best_model_at_end=True,\n    metric_for_best_model=\"eval_Overall-CV-F1\",\n    seed=18,\n    fp16=True,\n    report_to=\"wandb\",\n    run_name=\"bb-base-ner-corrected-weighted-custom-output-idpt-no-dropout-no-wd\",\n)","metadata":{"execution":{"iopub.status.busy":"2022-01-09T00:17:32.505014Z","iopub.execute_input":"2022-01-09T00:17:32.505386Z","iopub.status.idle":"2022-01-09T00:17:32.535809Z","shell.execute_reply.started":"2022-01-09T00:17:32.50535Z","shell.execute_reply":"2022-01-09T00:17:32.535086Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## I use wandb to track progress\n\n#### Feel free to disable it by setting `report_to=\"none\"` in `TrainingArguments`","metadata":{}},{"cell_type":"code","source":"if \"wandb\" in training_args.report_to:\n    !pip install -U wandb -qq\n    import wandb\n\n    from kaggle_secrets import UserSecretsClient\n    user_secrets = UserSecretsClient()\n    wandb_key = user_secrets.get_secret(\"wandb\")\n\n    os.environ[\"WANDB_PROJECT\"] = \"feedback-prize\"\n    wandb.login(key=wandb_key)","metadata":{"_kg_hide-input":false,"execution":{"iopub.status.busy":"2022-01-09T00:17:32.537373Z","iopub.execute_input":"2022-01-09T00:17:32.53764Z","iopub.status.idle":"2022-01-09T00:17:42.677918Z","shell.execute_reply.started":"2022-01-09T00:17:32.537605Z","shell.execute_reply":"2022-01-09T00:17:42.677153Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# train-test data split\n\n(Version 13+) Used to be 80-20 but it looks like everyone else is doing 90-10  \nData is in BIO format.  \nHere is a notebook showing how to make the data:  https://www.kaggle.com/nbroad/feedback-prize-bio-format-for-ner  \nHere is a dataset to use in your own notebooks: https://www.kaggle.com/nbroad/feedbackprize-bio-ner-train-data","metadata":{}},{"cell_type":"code","source":"from transformers.utils.logging import set_verbosity, ERROR\n\nset_verbosity(ERROR)\nset_seed(training_args.seed)\n\nif data_args.data_file is not None:\n    full_dataset = datasets.load_dataset(\"json\", data_files=data_args.data_file, split=\"train\")\n\nif data_args.val_split_percentage is not None:\n    # set_seed should work just fine, but to be safe I'll do it again\n    np.random.seed(training_args.seed)\n\n    file_ids = np.array(full_dataset[\"id\"])\n    np.random.shuffle(file_ids)\n    file_ids = list(set(file_ids))\n\n    num_files = len(file_ids)\n    threshold = int(num_files*data_args.val_split_percentage)\n    train_file_ids = set(file_ids[threshold:])\n\n    train_dataset = full_dataset.filter(lambda x: x[\"id\"] in train_file_ids)\n    val_dataset = full_dataset.filter(lambda x: x[\"id\"] not in train_file_ids)\n    \n    train_dataset.to_json(\"train.json\")\n    val_dataset.to_json(\"validation.json\")\n    del train_dataset, val_dataset\n\nground_truth_dataset = load_dataset(\"csv\", data_files=data_args.ground_truth_file, split=\"train\")\nif \"corrected\" in data_args.ground_truth_file:\n    keep_cols = [\"id\", \"discourse_type\", \"new_predictionstring\"]\n    ground_truth_dataset = ground_truth_dataset.remove_columns([x for x in ground_truth_dataset.column_names if x not in keep_cols])\n    ground_truth_dataset = ground_truth_dataset.rename_column(\"new_predictionstring\", \"predictionstring\")","metadata":{"execution":{"iopub.status.busy":"2022-01-09T00:17:42.679464Z","iopub.execute_input":"2022-01-09T00:17:42.67974Z","iopub.status.idle":"2022-01-09T00:18:33.5323Z","shell.execute_reply.started":"2022-01-09T00:17:42.679702Z","shell.execute_reply":"2022-01-09T00:18:33.531593Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load raw data","metadata":{}},{"cell_type":"code","source":"data_files = {}\nif data_args.train_file is not None:\n    data_files[\"train\"] = data_args.train_file\nif data_args.validation_file is not None:\n    data_files[\"validation\"] = data_args.validation_file\nif data_args.test_file is not None:\n    data_files[\"test\"] = data_args.test_file\nextension = data_args.train_file.split(\".\")[-1]\nraw_datasets = load_dataset(extension, data_files=data_files, cache_dir=model_args.cache_dir)","metadata":{"execution":{"iopub.status.busy":"2022-01-09T00:18:33.533535Z","iopub.execute_input":"2022-01-09T00:18:33.5338Z","iopub.status.idle":"2022-01-09T00:18:36.052981Z","shell.execute_reply.started":"2022-01-09T00:18:33.533763Z","shell.execute_reply":"2022-01-09T00:18:36.052265Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Labels","metadata":{}},{"cell_type":"code","source":"if training_args.do_train:\n    column_names = raw_datasets[\"train\"].column_names\n    features = raw_datasets[\"train\"].features\nelse:\n    column_names = raw_datasets[\"validation\"].column_names\n    features = raw_datasets[\"validation\"].features\n\nif data_args.text_column_name is not None:\n    text_column_name = data_args.text_column_name\nelif \"tokens\" in column_names:\n    text_column_name = \"tokens\"\nelse:\n    text_column_name = column_names[0]\n\nif data_args.label_column_name is not None:\n    label_column_name = data_args.label_column_name\nelif f\"{data_args.task_name}_tags\" in column_names:\n    label_column_name = f\"{data_args.task_name}_tags\"\nelse:\n    label_column_name = column_names[1]\n\n# In the event the labels are not a `Sequence[ClassLabel]`, we will need to go through the dataset to get the\n# unique labels.\ndef get_label_list(labels):\n    unique_labels = set()\n    for label in labels:\n        unique_labels = unique_labels | set(label)\n    label_list = list(unique_labels)\n    label_list.sort()\n    return label_list\n\nif isinstance(features[label_column_name].feature, ClassLabel):\n    label_list = features[label_column_name].feature.names\n    # No need to convert the labels since they are already ints.\n    label2id = {i: i for i in range(len(label_list))}\nelse:\n    label_list = get_label_list(raw_datasets[\"train\"][label_column_name])\n    label2id = {l: i for i, l in enumerate(label_list)}\nnum_labels = len(label_list)\n\nif not data_args.use_bio:\n    # Map that sends B-Xxx label to its I-Xxx counterpart\n    b_to_i_label = []\n    for idx, label in enumerate(label_list):\n        if label.startswith(\"B-\") and label.replace(\"B-\", \"I-\") in label_list:\n            b_to_i_label.append(label_list.index(label.replace(\"B-\", \"I-\")))\n        else:\n            b_to_i_label.append(idx)","metadata":{"execution":{"iopub.status.busy":"2022-01-09T00:18:36.054159Z","iopub.execute_input":"2022-01-09T00:18:36.054962Z","iopub.status.idle":"2022-01-09T00:18:43.205741Z","shell.execute_reply.started":"2022-01-09T00:18:36.054922Z","shell.execute_reply":"2022-01-09T00:18:43.204977Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Custom Bigbird Model","metadata":{}},{"cell_type":"code","source":"from transformers import BigBirdPreTrainedModel, BigBirdModel\nfrom transformers.modeling_outputs import TokenClassifierOutput\n\nclass BigBirdForTokenClassification(BigBirdPreTrainedModel):\n    def __init__(self, config):\n        super().__init__(config)\n        self.num_labels = config.num_labels\n\n        self.transformer = BigBirdModel(config)\n        self.preclassifier = torch.nn.Linear(config.hidden_size, config.hidden_size//2)\n        self.classifier = torch.nn.Linear(config.hidden_size//2, config.num_labels)\n\n        # Initialize weights and apply final processing\n#         self.post_init()\n\n    def forward(\n        self,\n        input_ids=None,\n        attention_mask=None,\n        token_type_ids=None,\n        position_ids=None,\n        head_mask=None,\n        inputs_embeds=None,\n        labels=None,\n        output_attentions=None,\n        output_hidden_states=None,\n        return_dict=None,\n    ):\n        r\"\"\"\n        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n            Labels for computing the token classification loss. Indices should be in `[0, ..., config.num_labels - 1]`.\n        \"\"\"\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        outputs = self.transformer(\n            input_ids,\n            attention_mask=attention_mask,\n            token_type_ids=token_type_ids,\n            position_ids=position_ids,\n            head_mask=head_mask,\n            inputs_embeds=inputs_embeds,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n\n        sequence_output = outputs[0]\n        sequence_output = self.preclassifier(sequence_output)\n        sequence_output = torch.nn.functional.gelu(sequence_output)        \n        logits = self.classifier(sequence_output)\n\n        loss = None\n        if labels is not None:\n            loss_fct = torch.nn.CrossEntropyLoss()\n            # Only keep active parts of the loss\n            if attention_mask is not None:\n                active_loss = attention_mask.view(-1) == 1\n                active_logits = logits.view(-1, self.num_labels)\n                active_labels = torch.where(\n                    active_loss, labels.view(-1), torch.tensor(loss_fct.ignore_index).type_as(labels)\n                )\n                loss = loss_fct(active_logits, active_labels)\n            else:\n                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n\n        if not return_dict:\n            output = (logits,) + outputs[2:]\n            return ((loss,) + output) if loss is not None else output\n\n        return TokenClassifierOutput(\n            loss=loss,\n            logits=logits,\n            hidden_states=outputs.hidden_states,\n            attentions=outputs.attentions,\n        )","metadata":{"execution":{"iopub.status.busy":"2022-01-09T00:18:43.207245Z","iopub.execute_input":"2022-01-09T00:18:43.2075Z","iopub.status.idle":"2022-01-09T00:18:43.237314Z","shell.execute_reply.started":"2022-01-09T00:18:43.207465Z","shell.execute_reply":"2022-01-09T00:18:43.236607Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Config, Tokenizer, Model","metadata":{}},{"cell_type":"code","source":"config = AutoConfig.from_pretrained(\n    model_args.config_name if model_args.config_name else model_args.model_name_or_path,\n    num_labels=num_labels,\n    label2id=label2id,\n    id2label={i: l for l, i in label2id.items()},\n    finetuning_task=data_args.task_name,\n    cache_dir=model_args.cache_dir,\n    revision=model_args.model_revision,\n    use_auth_token=True if model_args.use_auth_token else None,\n)\n\ntokenizer_name_or_path = model_args.tokenizer_name if model_args.tokenizer_name else model_args.model_name_or_path\ntokenizer = AutoTokenizer.from_pretrained(\n    tokenizer_name_or_path,\n    cache_dir=model_args.cache_dir,\n    use_fast=True,\n    add_prefix_space=True,\n    revision=model_args.model_revision,\n    use_auth_token=True if model_args.use_auth_token else None,\n)\n\nif model_args.use_custom_output:\n    model = BigBirdForTokenClassification.from_pretrained(\n        model_args.model_name_or_path,\n        from_tf=bool(\".ckpt\" in model_args.model_name_or_path),\n        config=config,\n        cache_dir=model_args.cache_dir,\n        revision=model_args.model_revision,\n        use_auth_token=True if model_args.use_auth_token else None,\n    )\nelse:\n    model = AutoModelForTokenClassification.from_pretrained(\n        model_args.model_name_or_path,\n        from_tf=bool(\".ckpt\" in model_args.model_name_or_path),\n        config=config,\n        cache_dir=model_args.cache_dir,\n        revision=model_args.model_revision,\n        use_auth_token=True if model_args.use_auth_token else None,\n    )\n\ndef reinit_weights(encoder, reinit_layers, config):\n    print(f\"Reinitializing last {reinit_layers} layers in the encoder.\")\n    for layer in encoder.layer[-reinit_layers:]:\n        for module in layer.modules():\n            if isinstance(module, torch.nn.Linear):\n                module.weight.data.normal_(mean=0.0, std=config.initializer_range)\n                if module.bias is not None:\n                    module.bias.data.zero_()\n            elif isinstance(module, torch.nn.Embedding):\n                module.weight.data.normal_(mean=0.0, std=config.initializer_range)\n                if module.padding_idx is not None:\n                    module.weight.data[module.padding_idx].zero_()\n            elif isinstance(module, torch.nn.LayerNorm):\n                module.bias.data.zero_()\n                module.weight.data.fill_(1.0)\n\nif model_args.reinit_layers:\n    reinit_weights(model.transformer.encoder, model_args.reinit_layers, model.config)","metadata":{"execution":{"iopub.status.busy":"2022-01-09T00:18:43.238747Z","iopub.execute_input":"2022-01-09T00:18:43.239166Z","iopub.status.idle":"2022-01-09T00:18:50.934799Z","shell.execute_reply.started":"2022-01-09T00:18:43.239123Z","shell.execute_reply":"2022-01-09T00:18:50.933876Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Tokenizing","metadata":{}},{"cell_type":"markdown","source":"### Old method that tokenizes after splitting at whitespace","metadata":{}},{"cell_type":"code","source":"# Tokenize all texts and align the labels with them.\ndef tokenize_and_align_labels(examples, train=True):\n    tokenized_inputs = tokenizer(\n        examples[data_args.text_column_name],\n        padding=padding if train else \"longest\",\n        truncation=True,\n        max_length=data_args.max_seq_length if train else tokenizer.model_max_length,\n        # We use this argument because the texts in our dataset are lists of words (with a label for each word).\n        is_split_into_words=True,\n    )\n    labels = []\n    all_word_ids = []\n    for i, label in enumerate(examples[data_args.label_column_name]):\n        word_ids = tokenized_inputs.word_ids(batch_index=i)\n        previous_word_idx = None\n        label_ids = []\n        for word_idx in word_ids:\n            # Special tokens have a word id that is None. We set the label to -100 so they are automatically\n            # ignored in the loss function.\n            if word_idx is None:\n                label_ids.append(-100)\n            # We set the label for the first token of each word.\n            elif word_idx != previous_word_idx:\n                label_ids.append(label2id[label[word_idx]])\n            # For the other tokens in a word, we set the label to either the current label or -100, depending on\n            # the label_all_tokens flag.\n            else:\n                if data_args.label_all_tokens:\n                    # The default script converts all b-labels to i-labels\n                    if data_args.use_bio:\n                        label_ids.append(label2id[label[word_idx]])\n                    else:\n                        label_ids.append(b_to_i_label[label2id[label[word_idx]]])\n                else:\n                    label_ids.append(-100)\n            previous_word_idx = word_idx\n\n        labels.append(label_ids)\n        if not train:\n            all_word_ids.append(word_ids)\n    tokenized_inputs[\"labels\"] = labels\n    \n    if not train:\n        tokenized_inputs[\"word_ids\"] = all_word_ids\n    \n    return tokenized_inputs","metadata":{"execution":{"iopub.status.busy":"2022-01-09T00:18:50.936323Z","iopub.execute_input":"2022-01-09T00:18:50.93662Z","iopub.status.idle":"2022-01-09T00:18:50.946676Z","shell.execute_reply.started":"2022-01-09T00:18:50.936567Z","shell.execute_reply":"2022-01-09T00:18:50.94591Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Preprocessing the dataset\n# Padding strategy\npadding = \"max_length\" if data_args.pad_to_max_length else False\n\n\nif training_args.do_train:\n    if \"train\" not in raw_datasets:\n        raise ValueError(\"--do_train requires a train dataset\")\n    train_dataset = raw_datasets[\"train\"]\n    if data_args.max_train_samples is not None:\n        train_dataset = train_dataset.select(range(data_args.max_train_samples))\n\n    with training_args.main_process_first(desc=\"train dataset map pre-processing\"):\n        train_dataset = train_dataset.map(\n            partial(tokenize_and_align_labels, train=True),\n            batched=True,\n            num_proc=data_args.preprocessing_num_workers,\n            load_from_cache_file=not data_args.overwrite_cache,\n            desc=\"Running tokenizer on train dataset\",\n        )\n\nif training_args.do_eval:\n    if \"validation\" not in raw_datasets:\n        raise ValueError(\"--do_eval requires a validation dataset\")\n    eval_dataset = raw_datasets[\"validation\"]\n    if data_args.max_eval_samples is not None:\n        eval_dataset = eval_dataset.select(range(data_args.max_eval_samples))\n\n    with training_args.main_process_first(desc=\"validation dataset map pre-processing\"):\n        eval_dataset = eval_dataset.map(\n            partial(tokenize_and_align_labels, train=False),\n            batched=True,\n            num_proc=data_args.preprocessing_num_workers,\n            load_from_cache_file=not data_args.overwrite_cache,\n            desc=\"Running tokenizer on validation dataset\",\n        )","metadata":{"execution":{"iopub.status.busy":"2022-01-09T00:18:50.964851Z","iopub.execute_input":"2022-01-09T00:18:50.965073Z","iopub.status.idle":"2022-01-09T00:19:44.601299Z","shell.execute_reply.started":"2022-01-09T00:18:50.965043Z","shell.execute_reply":"2022-01-09T00:19:44.600431Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Collator and metrics","metadata":{}},{"cell_type":"code","source":"# Data collator\npad_to_multiple_of = 8 if training_args.fp16 else None\n\n# These models have minimum padding sizes\nif \"bigbird\" in model_args.model_name_or_path:\n    pad_to_multiple_of = 1024\nif \"longformer\" in model_args.model_name_or_path:\n    pad_to_multiple_of = 512\n    \ndata_collator = DataCollatorForTokenClassification(tokenizer, pad_to_multiple_of=pad_to_multiple_of)\n\n# Metrics\nmetric = load_metric(\"seqeval\")\n\ndef compute_metrics(p):\n    predictions, labels = p\n    predictions = np.argmax(predictions, axis=2)\n\n    # Remove ignored index (special tokens)\n    true_predictions = [\n        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n        for prediction, label in zip(predictions, labels)\n    ]\n    true_labels = [\n        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n        for prediction, label in zip(predictions, labels)\n    ]\n\n    results = metric.compute(predictions=true_predictions, references=true_labels)\n    if data_args.return_entity_level_metrics:\n        # Unpack nested dictionaries\n        final_results = {}\n        for key, value in results.items():\n            if isinstance(value, dict):\n                for n, v in value.items():\n                    final_results[f\"{key}_{n}\"] = v\n            else:\n                final_results[key] = value\n        return final_results\n    else:\n        return {\n            \"precision\": results[\"overall_precision\"],\n            \"recall\": results[\"overall_recall\"],\n            \"f1\": results[\"overall_f1\"],\n            \"accuracy\": results[\"overall_accuracy\"],\n        }","metadata":{"execution":{"iopub.status.busy":"2022-01-09T00:19:44.602871Z","iopub.execute_input":"2022-01-09T00:19:44.603123Z","iopub.status.idle":"2022-01-09T00:19:45.504946Z","shell.execute_reply.started":"2022-01-09T00:19:44.603087Z","shell.execute_reply":"2022-01-09T00:19:45.504306Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# CV Calculation Functions","metadata":{}},{"cell_type":"code","source":"# Rob Mulla @robikscube\n# https://www.kaggle.com/robikscube/student-writing-competition-twitch\ndef calc_overlap(pred, ground_truth):\n    \"\"\"\n    Calculates the overlap between prediction and\n    ground truth and overlap percentages used for determining\n    true positives.\n    \"\"\"\n    set_pred = set(pred.split(' '))\n    set_gt = set(ground_truth.split(' '))\n    # Length of each and intersection\n    len_gt = len(set_gt)\n    len_pred = len(set_pred)\n    inter = len(set_gt.intersection(set_pred))\n    overlap_1 = inter / len_gt\n    overlap_2 = inter/ len_pred\n    return [overlap_1, overlap_2]\n\n\ndef score_feedback_comp(pred_df, gt_df):\n    \"\"\"\n    A function that scores for the kaggle\n        Student Writing Competition\n        \n    Uses the steps in the evaluation page here:\n        https://www.kaggle.com/c/feedback-prize-2021/overview/evaluation\n    \"\"\"\n    gt_df = gt_df[['id','discourse_type','predictionstring']] \\\n        .reset_index(drop=True).copy()\n    pred_df = pred_df[['id','discourse_type','predictionstring']] \\\n        .reset_index(drop=True).copy()\n    pred_df['pred_id'] = pred_df.index\n    gt_df['gt_id'] = gt_df.index\n    # Step 1. all ground truths and predictions for a given discourse_type are compared.\n    joined = pred_df.merge(gt_df,\n                           left_on=['id','discourse_type'],\n                           right_on=['id','discourse_type'],\n                           how='outer',\n                           suffixes=('_pred','_gt')\n                          )\n    joined['predictionstring_gt'] = joined['predictionstring_gt'].fillna(' ')\n    joined['predictionstring_pred'] = joined['predictionstring_pred'].fillna(' ')\n\n    joined['overlaps'] = [calc_overlap(pred, gt) for pred, gt in joined[['predictionstring_pred', 'predictionstring_gt']].values]\n\n    # 2. If the overlap between the ground truth and prediction is >= 0.5, \n    # and the overlap between the prediction and the ground truth >= 0.5,\n    # the prediction is a match and considered a true positive.\n    # If multiple matches exist, the match with the highest pair of overlaps is taken.\n    joined['overlap1'] = joined['overlaps'].apply(lambda x: eval(str(x))[0])\n    joined['overlap2'] = joined['overlaps'].apply(lambda x: eval(str(x))[1])\n\n\n    joined['potential_TP'] = (joined['overlap1'] >= 0.5) & (joined['overlap2'] >= 0.5)\n    joined['max_overlap'] = joined[['overlap1','overlap2']].max(axis=1)\n    tp_pred_ids = joined.query('potential_TP') \\\n        .sort_values('max_overlap', ascending=False) \\\n        .groupby(['id','predictionstring_gt']).first()['pred_id'].values\n\n    # 3. Any unmatched ground truths are false negatives\n    # and any unmatched predictions are false positives.\n    fp_pred_ids = [p for p in joined['pred_id'].unique() if p not in tp_pred_ids]\n\n    matched_gt_ids = joined.query('potential_TP')['gt_id'].unique()\n    unmatched_gt_ids = [c for c in joined['gt_id'].unique() if c not in matched_gt_ids]\n\n    # Get numbers of each type\n    TP = len(tp_pred_ids)\n    FP = len(fp_pred_ids)\n    FN = len(unmatched_gt_ids)\n    #calc microf1\n    denominator = (TP + 0.5*(FP+FN))\n    if denominator == 0:\n        return 0.0\n    my_f1_score = TP / denominator\n    return {\n        \"F1\": round(my_f1_score, 4),\n        \"Precision\": TP/(TP+FP),\n        \"Recall\": TP/(FP+FN),\n    }\n        \n\nid2label={i: l for l, i in label2id.items()}\n# https://www.kaggle.com/zzy990106/pytorch-ner-infer?scriptVersionId=82677278&cellId=13\ndef get_label_predictions(dataset, preds):\n\n    ids = dataset[\"id\"]\n    word_ids = dataset[\"word_ids\"]\n    words = dataset[\"words\"]\n    \n    all_preds = []\n\n    for id_, sample_preds, sample_word_ids, words in zip(ids, preds, word_ids, words):\n        label_preds = [\"\"]*len(words)\n\n        for pred, w_id in zip(sample_preds, sample_word_ids):\n            if w_id is None:\n                continue\n            if label_preds[w_id] == \"\":\n                label_preds[w_id] = id2label[pred]\n\n        j = 0\n        while j < len(label_preds):\n            label = label_preds[j].lstrip(\"BI-\")\n            if label == 'O' or label == '':\n                j += 1\n            else:\n                end = j + 1\n                while end < len(label_preds) and label_preds[end].lstrip(\"BI-\") == label:\n                    end += 1\n\n                if end - j > 8:\n                    all_preds.append((id_, label, ' '.join(map(str, list(range(j, end))))))\n\n                j = end\n                \n    return all_preds","metadata":{"execution":{"iopub.status.busy":"2022-01-09T00:19:45.506322Z","iopub.execute_input":"2022-01-09T00:19:45.506583Z","iopub.status.idle":"2022-01-09T00:19:45.529458Z","shell.execute_reply.started":"2022-01-09T00:19:45.506529Z","shell.execute_reply":"2022-01-09T00:19:45.528843Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Custom Trainer","metadata":{}},{"cell_type":"code","source":"class FeedbackPrizeTrainer(Trainer):\n    \n    def __init__(self, *args, **kwargs):\n        # The Trainer will remove the important columns needed for cv from the eval_dataset,\n        # so we'll just store it like this\n        if \"cv_dataset\" in kwargs:\n            self.cv_dataset = kwargs.pop(\"cv_dataset\")\n        super().__init__(*args, **kwargs)\n        \n        \n    def evaluation_loop(\n        self, \n        dataloader,\n        description,\n        prediction_loss_only = None,\n        ignore_keys = None,\n        metric_key_prefix = \"eval\",\n    ):\n        \n        eval_output =  super().evaluation_loop(\n            dataloader,\n            description,\n            prediction_loss_only,\n            ignore_keys,\n            metric_key_prefix\n        )\n        \n        # This same loop gets called during predict, and we can't do CV when predicting\n        is_in_eval = metric_key_prefix == \"eval\"\n        \n        # Custom CV F1 calculation\n        if is_in_eval:\n            \n            eval_id_preds = eval_output.predictions.argmax(-1)\n            \n            eval_label_preds = get_label_predictions(self.cv_dataset, eval_id_preds)\n            \n            eval_pred_df = pd.DataFrame(eval_label_preds, columns=[\"id\", \"discourse_type\", \"predictionstring\"])\n            ground_truth_df = ground_truth_dataset.to_pandas()\n            \n            eval_gt_df = ground_truth_df[ground_truth_df[\"id\"].isin(self.cv_dataset[\"id\"])].reset_index(drop=True).copy()\n            \n            classes = ['Lead', 'Position', 'Evidence', 'Claim', 'Concluding Statement', 'Counterclaim', 'Rebuttal']\n            f1_scores = []\n            for class_ in classes:\n                gt_df = eval_gt_df.loc[eval_gt_df['discourse_type'] == class_].copy()\n                pred_df = eval_pred_df.loc[eval_pred_df['discourse_type'] == class_].copy()\n                eval_scores = score_feedback_comp(pred_df, gt_df)\n                for score_name, score in eval_scores.items():\n                    eval_output.metrics[f\"{metric_key_prefix}_{class_}-CV-{score_name}\"] = score\n                f1_scores.append(eval_scores[\"F1\"])\n                \n            eval_output.metrics[f\"{metric_key_prefix}_Overall-CV-F1\"] = np.mean(f1_scores)\n        \n        return eval_output\n\n# Initialize our Trainer\ntrainer = FeedbackPrizeTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset if training_args.do_train else None,\n    eval_dataset=eval_dataset if training_args.do_eval else None,\n    cv_dataset=eval_dataset, \n    tokenizer=tokenizer,\n    data_collator=data_collator,\n    compute_metrics=compute_metrics,\n)","metadata":{"execution":{"iopub.status.busy":"2022-01-09T00:19:45.530781Z","iopub.execute_input":"2022-01-09T00:19:45.531236Z","iopub.status.idle":"2022-01-09T00:19:50.260468Z","shell.execute_reply.started":"2022-01-09T00:19:45.531198Z","shell.execute_reply":"2022-01-09T00:19:50.259612Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training\n\n### Scores for each evaluation will be shown in a table. If you go all the way to the right on the table, you'll see the CV F1 score.","metadata":{}},{"cell_type":"code","source":"%env TOKENIZERS_PARALLELISM=true\n\n# Training\nif training_args.do_train:\n    train_result = trainer.train()\n    metrics = train_result.metrics\n    trainer.save_model()  # Saves the tokenizer too for easy upload\n\n    max_train_samples = (\n        data_args.max_train_samples if data_args.max_train_samples is not None else len(train_dataset)\n    )\n    metrics[\"train_samples\"] = min(max_train_samples, len(train_dataset))\n\n    trainer.log_metrics(\"train\", metrics)\n    trainer.save_metrics(\"train\", metrics)\n    trainer.save_state()","metadata":{"execution":{"iopub.status.busy":"2022-01-09T00:19:50.261914Z","iopub.execute_input":"2022-01-09T00:19:50.262163Z","iopub.status.idle":"2022-01-09T06:08:34.099152Z","shell.execute_reply.started":"2022-01-09T00:19:50.262135Z","shell.execute_reply":"2022-01-09T06:08:34.098391Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Final evaluation","metadata":{}},{"cell_type":"code","source":"# Evaluation\nif training_args.do_eval:\n\n    metrics = trainer.evaluate()\n\n    max_eval_samples = data_args.max_eval_samples if data_args.max_eval_samples is not None else len(eval_dataset)\n    metrics[\"eval_samples\"] = min(max_eval_samples, len(eval_dataset))\n\n    trainer.log_metrics(\"eval\", metrics)\n    trainer.save_metrics(\"eval\", metrics)","metadata":{"execution":{"iopub.status.busy":"2022-01-09T06:08:34.10097Z","iopub.execute_input":"2022-01-09T06:08:34.102319Z","iopub.status.idle":"2022-01-09T06:12:23.427063Z","shell.execute_reply.started":"2022-01-09T06:08:34.102275Z","shell.execute_reply":"2022-01-09T06:12:23.42638Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Predicting on test set","metadata":{}},{"cell_type":"code","source":"test_data = {\n    \"id\": [],\n    data_args.text_column_name: []\n}\n\ndef tokenize(examples):\n    tokenized_inputs = tokenizer(\n        examples[data_args.text_column_name],\n        truncation=True,\n        max_length=4096,\n        # We use this argument because the texts in our dataset are lists of words (with a label for each word).\n        is_split_into_words=True,\n    )\n    num_samples = len(tokenized_inputs[\"input_ids\"])\n    tokenized_inputs[\"word_ids\"] = [tokenized_inputs.word_ids(batch_index=i) for i in range(num_samples)]\n    \n    return tokenized_inputs\n\n# If you actually want to submit, it would probably be best in another notebook.\n# Take your model, stick it in the Trainer, and run it through this to get your submission file.\n# A lot of the code above is for training and validation, so only copy what you need.\nif training_args.do_predict:\n    for file in Path(\"../input/feedback-prize-2021/test\").glob(\"*.txt\"):\n        file_id = file.stem\n\n        with open(file) as fp:\n            text = fp.read()\n\n        test_data[data_args.text_column_name].append(text.split())\n        test_data[\"id\"].append(file_id)\n\n    raw_test_dataset = Dataset.from_dict(test_data)\n\n    test_dataset = raw_test_dataset.map(\n            tokenize,\n            batched=True,\n            num_proc=data_args.preprocessing_num_workers,\n            load_from_cache_file=not data_args.overwrite_cache,\n            desc=\"Running tokenizer on test dataset\",\n        )\n\n    test_predictions = trainer.predict(test_dataset)\n    \n    test_id_preds = test_predictions.predictions.argmax(-1)\n    \n    test_label_preds = get_label_predictions(test_dataset, test_id_preds)\n    test_pred_df = pd.DataFrame(test_label_preds, columns=[\"id\", \"class\", \"predictionstring\"])\n    \n    test_pred_df.to_csv(\"submission.csv\", index=False)\n    \n    display(test_pred_df)","metadata":{"execution":{"iopub.status.busy":"2022-01-09T06:12:23.430889Z","iopub.execute_input":"2022-01-09T06:12:23.432757Z","iopub.status.idle":"2022-01-09T06:12:24.785119Z","shell.execute_reply.started":"2022-01-09T06:12:23.432718Z","shell.execute_reply":"2022-01-09T06:12:24.784384Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Weights and Biases Report\n\n<iframe src=\"https://wandb.ai/nbroad/feedback-prize/reports/-Feedback-Prize-Bigbird-base-NER-fine-tuning--VmlldzoxMzc3ODQz\" style=\"border:none;height:1024px;width:100%\">","metadata":{}},{"cell_type":"markdown","source":"### I'm borrowing Darek's beautiful displacy code :)\n\nkaggle.com/thedrcat/feedback-prize-huggingface-baseline-training","metadata":{"_kg_hide-output":true,"_kg_hide-input":true}},{"cell_type":"code","source":"# kaggle.com/thedrcat/feedback-prize-huggingface-baseline-training\nimport spacy\nfrom spacy import displacy\nfrom pylab import cm, matplotlib\n\ncolors = {\n            'Lead': '#8000ff',\n            'Position': '#2b7ff6',\n            'Evidence': '#2adddd',\n            'Claim': '#80ffb4',\n            'Concluding Statement': 'd4dd80',\n            'Counterclaim': '#ff8042',\n            'Rebuttal': '#ff0000',\n            'Other': '#007f00',\n         }\n\ndef visualize(df, text):\n    ents = []\n    example = df['id'].loc[0]\n\n    for i, row in df.iterrows():\n        ents.append({\n                        'start': int(row['discourse_start']), \n                         'end': int(row['discourse_end']), \n                         'label': row['discourse_type']\n                    })\n\n    doc2 = {\n        \"text\": text,\n        \"ents\": ents,\n        \"title\": example\n    }\n\n    options = {\"ents\": train.discourse_type.unique().tolist() + ['Other'], \"colors\": colors}\n    displacy.render(doc2, style=\"ent\", options=options, manual=True, jupyter=True)","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-01-09T06:12:24.786701Z","iopub.execute_input":"2022-01-09T06:12:24.787198Z","iopub.status.idle":"2022-01-09T06:12:29.346842Z","shell.execute_reply.started":"2022-01-09T06:12:24.787156Z","shell.execute_reply":"2022-01-09T06:12:29.345938Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# kaggle.com/thedrcat/feedback-prize-huggingface-baseline-training\ndef get_class(c):\n    return id2label[c].replace(\"B-\", \"\").replace(\"I-\")\n\ndef pred2span(pred, example, viz=False):\n    example_id = example['id']\n    n_tokens = len(example['input_ids'])\n    classes = []\n    all_span = []\n    for i, c in enumerate(pred.tolist()):\n        if i == n_tokens-1:\n            break\n        if i == 0:\n            cur_span = example['offset_mapping'][i]\n            classes.append(get_class(c))\n        elif i > 0 and (c == pred[i-1] or (c-7) == pred[i-1]):\n            cur_span[1] = example['offset_mapping'][i][1]\n        else:\n            all_span.append(cur_span)\n            cur_span = example['offset_mapping'][i]\n            classes.append(get_class(c))\n    all_span.append(cur_span)\n    \n    text = open(f\"../input/feedback-prize-2021/test/{example_id}.txt\").read()\n    \n    # map token ids to word (whitespace) token ids\n    predstrings = []\n    for span in all_span:\n        span_start = span[0]\n        span_end = span[1]\n        before = text[:span_start]\n        token_start = len(before.split())\n        if len(before) == 0: token_start = 0\n        elif before[-1] != ' ': token_start -= 1\n        num_tkns = len(text[span_start:span_end+1].split())\n        tkns = [str(x) for x in range(token_start, token_start+num_tkns)]\n        predstring = ' '.join(tkns)\n        predstrings.append(predstring)\n                    \n    rows = []\n    for c, span, predstring in zip(classes, all_span, predstrings):\n        e = {\n            'id': example_id,\n            'discourse_type': c,\n            'predictionstring': predstring,\n            'discourse_start': span[0],\n            'discourse_end': span[1],\n            'discourse': text[span[0]:span[1]+1]\n        }\n        rows.append(e)\n\n\n    df = pd.DataFrame(rows)\n    df['length'] = df['discourse'].apply(lambda t: len(t.split()))\n    \n    # short spans are likely to be false positives, we can choose a min number of tokens based on validation\n    df = df[df.length > min_tokens].reset_index(drop=True)\n    if viz: visualize(df, text)\n\n    return df","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-01-09T06:12:29.348432Z","iopub.execute_input":"2022-01-09T06:12:29.348769Z","iopub.status.idle":"2022-01-09T06:12:29.364485Z","shell.execute_reply.started":"2022-01-09T06:12:29.348708Z","shell.execute_reply":"2022-01-09T06:12:29.363458Z"},"trusted":true},"execution_count":null,"outputs":[]}]}