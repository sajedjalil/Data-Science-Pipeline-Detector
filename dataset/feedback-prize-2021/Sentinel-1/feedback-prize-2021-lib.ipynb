{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Library of some utility functions for the [Feedback Prize - Evaluating Student Writing](https://www.kaggle.com/c/feedback-prize-2021) competition    \n\n\n- Examples:\n  1. Notebook [feedback-color-print](https://www.kaggle.com/sentinel1/feedback-color-print) demonstrates:\n     - `read_train_csv()` function\n     - `color_print_essay()` function\n  2. Notebook [Feedback Prize 2021 lib - howto 2](https://www.kaggle.com/sentinel1/feedback-prize-2021-lib-howto-2):\n     - `get_train_essay_text()`\n     - `calc_word_indices()`\n     - `stringify_word_indices()`\n  3. Notebook [Feedback Prize 2021 lib - howto 3](https://www.kaggle.com/sentinel1/feedback-prize-2021-lib-howto-3):\n     - `calc_PII_offsets()`\n     - `get_train_df_with_fixed_PII_offsets()`\n  4. Notebook [feedback-prize-2021 K-Fold Split Data](https://www.kaggle.com/sentinel1/feedback-prize-2021-k-fold-split-data):\n     - `train_df_k_fold_split()`\n      \n      \nNOTE: Comments are disabled on this notebook. If you have any comments for this library please instead comment on one of the notebooks from the above list which is the most relevant to your comments.","metadata":{"papermill":{"duration":0.005938,"end_time":"2022-01-06T22:02:17.597058","exception":false,"start_time":"2022-01-06T22:02:17.59112","status":"completed"},"tags":[]}},{"cell_type":"code","source":"from pathlib import Path\n(Path.cwd()/\"lib\").mkdir(exist_ok=True)\n(Path.cwd()/\"lib\"/\"__init__.py\").touch()\n(Path.cwd()/\"__init__.py\").touch()","metadata":{"papermill":{"duration":0.021885,"end_time":"2022-01-06T22:02:17.623802","exception":false,"start_time":"2022-01-06T22:02:17.601917","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-01-14T10:23:35.450765Z","iopub.execute_input":"2022-01-14T10:23:35.451505Z","iopub.status.idle":"2022-01-14T10:23:35.464394Z","shell.execute_reply.started":"2022-01-14T10:23:35.451395Z","shell.execute_reply":"2022-01-14T10:23:35.463453Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%file lib/feedback_util.py\nfrom pathlib import Path\nfrom IPython.display import HTML, display\nimport pandas as pd\nimport numpy as np\nimport string\nimport sys\n\n\n\n__all__ = [\n    \"get_train_essay_text\"\n    , \"color_print_essay\"\n    , \"read_train_csv\"\n    , \"calc_word_indices\"\n    , \"stringify_word_indices\"\n    # , \"calc_PII_offsets\"\n    , \"get_train_df_with_fixed_PII_offsets\"\n    , \"train_df_k_fold_split\"\n]\n\n\n\nTRAIN_FOLDER = (Path.cwd()/'..'/'input'/'feedback-prize-2021'/'train').resolve()\n\ndef get_train_essay_text(essay_id: str) -> str:\n    essay_p = TRAIN_FOLDER/f\"{essay_id}.txt\"\n    assert essay_p.is_file(), f\"No text file found for the essay with id={essay_id}\"\n    \n    with open(essay_p, 'r') as f:\n        essay_text = f.read()\n    return essay_text\n\n\n\ndiscourse_type_colors = {\n    'Claim':                '#e6ab02',\n    'Concluding Statement': '#1b9e77',\n    'Counterclaim':         '#d95f02',\n    'Evidence':             '#7570b3',\n    'Lead':                 '#a6761d',\n    'Position':             '#66a61e',\n    'Rebuttal':             '#e7298a',\n}\n\ncolor_codes = \"\"\nfor k in discourse_type_colors:\n    color_codes += f' <div style=\"color:{discourse_type_colors[k]};margin-right:1em;\">{k}</div> '\ncolor_codes = f'<div style=\"display:flex;font-weight:bold;\">{color_codes}</div><hr>'\n\n\n\ndef color_print_essay(essay_id: str, df: pd.DataFrame, start_end_indicators: bool = False) -> None:\n    \"\"\"\n    Prints single essay with color coded discourse types\n    \n    Parameters:\n     - `essay_id`: id of the essay (as given in the `id` column of the `train.csv`)\n     - `df`: dataframe (can be an original `train.csv` dataframe or any other compatible dataframe)\n             containing the following columns: `id`, `discourse_start`, `discourse_end`, and `discourse_type`\n     - `start_end_indicators`: optional boolean flag, defaults to False. If set to True then startings and\n                               endings of each discourse are marked with ribbon arrows.\n    \"\"\"\n    essay_df = df.loc[df['id'] == essay_id]\n    assert len(essay_df) > 0, \"Bad essay_id\"\n    essay_text = get_train_essay_text(essay_id)\n    \n    if start_end_indicators:\n        end_indicator = '<span style=\"color:gray;\">&#11184;</span>'\n        start_indicator = '<span style=\"color:gray;\">&#11187;</span>'\n    else:\n        end_indicator = start_indicator = \"\"\n    \n    for idx, row in essay_df.sort_values(by='discourse_start', ascending=False).iterrows():\n        start = int(row['discourse_start'])\n        end = int(row['discourse_end'])\n        essay_text = essay_text[:end] + f'</span>{end_indicator}' + essay_text[end:]\n        essay_text = essay_text[:start] + f'{start_indicator}<span style=\"color:{discourse_type_colors[row[\"discourse_type\"]]};\">' + essay_text[start:]\n    display(HTML(color_codes + essay_text.replace('\\n','<br>') + '<hr>'))\n\n\n\ndef read_train_csv() -> pd.DataFrame:\n    \"\"\"\n    Reads original train.csv into pandas dataframe with adjusted data types.\n    \"\"\"\n    return pd.read_csv((Path.cwd()/'..'/'input'/'feedback-prize-2021'/'train.csv').resolve(),\n                       dtype={\n                           'id':                 'string',\n                           'discourse_id':       'Int64',\n                           'discourse_start':    'UInt16',\n                           'discourse_end':      'UInt16',\n                           'discourse_text':     'string',\n                           'discourse_type':     'category',\n                           'discourse_type_num': 'string',\n                           'predictionstring':   'string',\n                       })\n\n\n\ndef calc_word_indices(full_text, discourse_start, discourse_end):\n    \"\"\"\n    This function is a copy of the one given by the competition host.\n    \n    Link of the original source:\n    https://www.kaggle.com/c/feedback-prize-2021/discussion/297688\n    \"\"\"\n    start_index = len(full_text[:discourse_start].split())\n    token_len = len(full_text[discourse_start:discourse_end].split())\n    output = list(range(start_index, start_index + token_len))\n    if output[-1] >= len(full_text.split()):\n        output = list(range(start_index, start_index + token_len-1))\n    return output\n\n\n\ndef stringify_word_indices(word_indices: list) -> str:\n    return \" \".join(str(i) for i in word_indices)\n\n\n\npunctuation_remover = str.maketrans('', '', string.punctuation)\nOFFSET_SEARCH_RANGE = np.iinfo(np.int8).max\n\ndef _calc_PII_offsets_of_essay(group, self_test = False):\n    essay_id = group.name\n    essay_text = get_train_essay_text(essay_id)\n    \n    def get_start_offset(target_discourse):\n        discourse_text = target_discourse['discourse_text']\n        offset = 0\n        for offset_abs in range(OFFSET_SEARCH_RANGE):\n            for offset_sign in [1, -1]:\n                offset = offset_abs * offset_sign\n                discourse_text_in_essay = essay_text[\n                    target_discourse['discourse_start'] + offset:target_discourse['discourse_end'] + offset\n                ]\n                comparison_len = min(len(discourse_text)-1, len(discourse_text_in_essay))  # Ignoring some last characters: interchangeably used different whitespaces and extra punctuation\n\n                if discourse_text[:comparison_len] == discourse_text_in_essay[:comparison_len]:\n                    break\n            else:\n                continue\n            break\n        else:\n            raise Exception(f\"For the essay_id={essay_id} and discourse_id={target_discourse['discourse_id']} start offset was not found in range of {OFFSET_SEARCH_RANGE}!\")\n        return offset\n    \n    first_discourse = group.iloc[0]\n    offset = get_start_offset(first_discourse)  # Here we get `offset` for the text preceeding the first labeled discourse in the essay.\n                                                # So, in most cases this offset apply to the entire essay,\n                                                # unless some additional PII masking are present inside the first discourse or after it.\n    group['discourse_start_PII_offset'] = offset\n    group['discourse_end_PII_offset'] = offset\n    \n    ##\n    # FIXME: We are using shortcut here by taking advantage of the fact\n    #        that in this training dataset most of the included PII\n    #        masks are inside or before the first discourse and as a \n    #        shortcut we are adjusting offsets with one operation\n    #        based on the second discourse instead of doing it for\n    #        all the discourses in the loop.\n    ##\n    if '_NAME' in first_discourse['discourse_text'] and len(group) > 1:  # matches PII masks like: PROPER_NAME, SCHOOL_NAME, LOCATION_NAME, TEACHER_NAME, STUDENT_NAME, etc.\n        second_discourse = group.iloc[1]\n        new_offset = get_start_offset(second_discourse)  # Here we get `offset` for the second and the rest of discourses\n                                                         # for the case when additional PII masks are present in the first discourse\n        group.iloc[1:]['discourse_start_PII_offset'] = new_offset  # Start offset of the first discourse should be preserved (computed before), so skipping the first row\n        group.loc[:,'discourse_end_PII_offset'] = new_offset       # End  offset  of the first discourse needs to be updated due to PII masks being included inside it, NOT skipping the first row\n        # FIXME ignoring possibility of PII mask occuring in the background between the first and the second discourse, it seems there is no such case in this data, self_test below will tell.\n\n    ##\n    # FIXME: essay 'F91D7BB4277C' currently has the LOCATION_NAME\n    # PII mask in the text file but it has actual location name\n    # in the label, which breaks the below given self_test\n    # for no good reason.\n    # Fixing it by PII masking the label in the train dataframe\n    # in order to match the actual text in the text file and\n    # adjusting offset manually for this particular discourse.\n    ##\n    if essay_id == 'F91D7BB4277C':\n        idx = group['discourse_id'] == 1623258656795\n        discourse_text = group.loc[idx,'discourse_text'].item()\n        discourse_text_fixed = discourse_text.replace('florida','LOCATION_NAME')\n        end_offset_delta = len(discourse_text) - len(discourse_text_fixed)\n        group.loc[idx,'discourse_text'] = discourse_text_fixed\n        group.loc[idx,'discourse_end_PII_offset'] += end_offset_delta\n    \n    \n    if self_test:\n        for discourse_id, row in group.set_index('discourse_id').iterrows():\n            discourse_text = row['discourse_text'].translate(punctuation_remover).strip()  # throwing away possible extra punctuations & accounting for newline and space being used interchangeably in the given data\n            discourse_text_in_essay = essay_text[\n                row['discourse_start'] + row['discourse_start_PII_offset']:row['discourse_end'] + row['discourse_end_PII_offset']\n            ].translate(punctuation_remover).strip()\n            comparison_len = min(len(discourse_text), len(discourse_text_in_essay))\n\n            if discourse_text[:comparison_len] != discourse_text_in_essay[:comparison_len]:\n                raise Exception(f\"In the essay_id={essay_id}, with offset={offset} mismatch was found for the discourse_id={discourse_id}!\")\n    \n    # Adjust dtypes to nullable int8 integer:\n    group['discourse_start_PII_offset'] = group['discourse_start_PII_offset'].astype('Int8')\n    group['discourse_end_PII_offset'] = group['discourse_end_PII_offset'].astype('Int8')\n    return group\n\n\n\ndef calc_PII_offsets(train_df: pd.DataFrame, self_test: bool = False) -> pd.DataFrame:\n    return train_df.groupby('id').apply(_calc_PII_offsets_of_essay, self_test = self_test)\n\n\n\nPII_MASK_OFFSETTED_TRAIN_DF_CACHE = (Path.cwd()/\"..\"/\"temp\"/'PII_mask_offsetted_train_df.feather').resolve()\n\ndef get_train_df_with_fixed_PII_offsets(df_with_offsets: pd.DataFrame = None, use_tmp_cache: bool = False) -> pd.DataFrame:\n    \"\"\"\n    Fixes discourse_start and discourse_end by adding values of PII offsets to them.\n    \n    Parameters:\n    - `df_with_offsets`: (OPTIONAL) dataframe having precomputed offset columns in it.\n    Returns:\n    - Dataframe with `discourse_start` and `discourse_end` corrected.\n    \"\"\"\n    if use_tmp_cache and PII_MASK_OFFSETTED_TRAIN_DF_CACHE.is_file():\n        return pd.read_feather(PII_MASK_OFFSETTED_TRAIN_DF_CACHE)\n        \n    if df_with_offsets is None:\n        df_with_offsets = calc_PII_offsets(read_train_csv())\n    offset_columns = ('discourse_start_PII_offset', 'discourse_end_PII_offset',)\n    \n    if all([offset in df_with_offsets.columns for offset in offset_columns]):\n        df = df_with_offsets.copy()\n        df['discourse_start'] += df['discourse_start_PII_offset']\n        df['discourse_end'] += df['discourse_end_PII_offset']\n        df = df[df.columns.difference(offset_columns, sort=False)]\n        \n        if use_tmp_cache:\n            PII_MASK_OFFSETTED_TRAIN_DF_CACHE.parent.mkdir(exist_ok=True)\n            df.to_feather(PII_MASK_OFFSETTED_TRAIN_DF_CACHE)\n        return df\n    \n    print(f\"WARNING: no offset columns ({offset_columns}) found in the df, returning unchanged df.\", file=sys.stderr, flush=True)\n    return df_with_offsets\n\n\n\ntry:\n    from sklearn.model_selection import StratifiedGroupKFold\n    \n    def train_df_k_fold_split(train_df: pd.DataFrame, K: int = 5, display_split_statistics: bool = False) -> pd.DataFrame:\n        \"\"\"\n        Splits data into K folds (i.e. adds the 'CV' column containing the fold numbers to the 'train_df' dataframe)\n        \"\"\"\n        sgkf = StratifiedGroupKFold(n_splits=K, random_state=2022, shuffle=True)\n        train_df[\"CV\"] = 0\n\n        for i,(train_idx, test_idx) in enumerate(sgkf.split(X=np.zeros(len(train_df)), y=train_df[\"discourse_type\"], groups=train_df[\"id\"]), 1):\n            train_df.loc[test_idx,\"CV\"] = i\n\n        if display_split_statistics:\n            fold_stats_df = train_df.groupby('CV')[['id','discourse_type']].apply(lambda x:pd.DataFrame({\n                'Essay': len(x['id'].unique()),\n                'Discourse': len(x),\n                'Claim': len(x[x['discourse_type'] == 'Claim']),\n                'Concluding Statement': len(x[x['discourse_type'] == 'Concluding Statement']),\n                'Counterclaim': len(x[x['discourse_type'] == 'Counterclaim']),\n                'Evidence': len(x[x['discourse_type'] == 'Evidence']),\n                'Lead': len(x[x['discourse_type'] == 'Lead']),\n                'Position': len(x[x['discourse_type'] == 'Position']),\n                'Rebuttal': len(x[x['discourse_type'] == 'Rebuttal']),\n            }, index=[x.name])).reset_index('CV').set_index('CV')\n            assert fold_stats_df['Essay'].sum() == len(train_df['id'].unique()), \"IDs of essays in different folds must be mutually exclusive!\"\n            display(fold_stats_df)\n        return train_df\n    \nexcept ImportError:\n    \n    def train_df_k_fold_split(*args, **kw):\n        try:\n            import sklearn\n            print(f\"Installed version of the `scikit-learn` package is too old: sklearn.__version__={sklearn.__version__}\")\n        except:\n            pass\n        raise Exception(\"The `train_df_k_fold_split()` function requires newer version of `scikit-learn`. Please update the scikit-learn package first.\")\n        ","metadata":{"papermill":{"duration":0.020137,"end_time":"2022-01-06T22:02:17.648705","exception":false,"start_time":"2022-01-06T22:02:17.628568","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-01-14T10:23:35.465938Z","iopub.execute_input":"2022-01-14T10:23:35.466173Z","iopub.status.idle":"2022-01-14T10:23:35.484856Z","shell.execute_reply.started":"2022-01-14T10:23:35.466144Z","shell.execute_reply":"2022-01-14T10:23:35.4837Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}