{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Models and baseline\n\nThanks @abhishek and @chryzal\n\n**two longformers are better than 1**\n\nhttps://www.kaggle.com/abhishek/two-longformers-are-better-than-1\n\n**Feedback Prize 2021 ðŸ‘ Pytorch [better parameters]**\n\nhttps://www.kaggle.com/chryzal/feedback-prize-2021-pytorch-better-parameters","metadata":{}},{"cell_type":"markdown","source":"# 1. Import & Set & Class & Def & Load","metadata":{"_uuid":"66c995cf-7cbb-421b-bdd3-24906f5fb886","_cell_guid":"e0e4bdf6-bcd9-4f82-a53b-9c5f685500a8","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-01-05T18:19:34.765636Z","iopub.execute_input":"2022-01-05T18:19:34.766525Z","iopub.status.idle":"2022-01-05T18:19:43.074135Z","shell.execute_reply.started":"2022-01-05T18:19:34.76641Z","shell.execute_reply":"2022-01-05T18:19:43.072934Z"}}},{"cell_type":"code","source":"import gc\ngc.enable()\n\nimport os\nimport re\nimport random\nimport sys\nsys.path.append(\"../input/tez-lib/\")\n\nimport numpy as np\nimport pandas as pd\npd.set_option('display.max_colwidth', 80)\npd.set_option(\"display.precision\", 12)\n\nimport tez\nimport torch\nimport torch.nn as nn\n\nfrom joblib import Parallel, delayed\nfrom transformers import AutoConfig, AutoModel, AutoTokenizer\n\nfrom spacy import displacy","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-03-12T20:12:13.568089Z","iopub.execute_input":"2022-03-12T20:12:13.568671Z","iopub.status.idle":"2022-03-12T20:12:25.473173Z","shell.execute_reply.started":"2022-03-12T20:12:13.568573Z","shell.execute_reply":"2022-03-12T20:12:25.472399Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"target_id_map = {\n    \"B-Lead\": 0,\n    \"I-Lead\": 1,\n    \"B-Position\": 2,\n    \"I-Position\": 3,\n    \"B-Evidence\": 4,\n    \"I-Evidence\": 5,\n    \"B-Claim\": 6,\n    \"I-Claim\": 7,\n    \"B-Concluding Statement\": 8,\n    \"I-Concluding Statement\": 9,\n    \"B-Counterclaim\": 10,\n    \"I-Counterclaim\": 11,\n    \"B-Rebuttal\": 12,\n    \"I-Rebuttal\": 13,\n    \"O\": 14,\n    \"PAD\": -100,\n}\n\nid_target_map = {v: k for k, v in target_id_map.items()}\n\nCOLORS = {\n    'Lead': '#8000ff',\n    'Position': '#2b7ff6',\n    'Evidence': '#2adddd',\n    'Claim': '#80ffb4',\n    'Concluding Statement': 'd4dd80',\n    'Counterclaim': '#ff8042',\n    'Rebuttal': '#ff0000'\n}\n\nCOMP_DIR = \"../input/feedback-prize-2021/\"","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-03-12T20:12:25.47475Z","iopub.execute_input":"2022-03-12T20:12:25.47545Z","iopub.status.idle":"2022-03-12T20:12:25.48287Z","shell.execute_reply.started":"2022-03-12T20:12:25.475412Z","shell.execute_reply":"2022-03-12T20:12:25.482149Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class args1:\n    input_path = COMP_DIR\n    model = \"../input/longformerlarge4096/longformer-large-4096/\"\n    tez_model= \"../input/fblongformerlarge1536/\"\n    output = \".\"\n    batch_size = 8\n    max_len = 4096\n    \n    \nclass args2:\n    input_path = COMP_DIR\n    model = \"../input/longformerlarge4096/longformer-large-4096/\"\n    tez_model= \"../input/tez-fb-large/\"\n    output = \".\"\n    batch_size = 8\n    max_len = 4096\n\n    \nclass FeedbackDataset:\n    def __init__(self, samples, max_len, tokenizer):\n        self.samples = samples\n        self.max_len = max_len\n        self.tokenizer = tokenizer\n        self.length = len(samples)\n\n    def __len__(self):\n        return self.length\n\n    def __getitem__(self, idx):\n        input_ids = self.samples[idx][\"input_ids\"]\n\n        # add start token id to the input_ids\n        input_ids = [self.tokenizer.cls_token_id] + input_ids\n\n        if len(input_ids) > self.max_len - 1:\n            input_ids = input_ids[: self.max_len - 1]\n\n        # add end token id to the input_ids\n        input_ids = input_ids + [self.tokenizer.sep_token_id]\n        attention_mask = [1] * len(input_ids)\n\n        return {\n            \"ids\": input_ids,\n            \"mask\": attention_mask,\n        }\n\n    \nclass Collate:\n    def __init__(self, tokenizer):\n        self.tokenizer = tokenizer\n\n    def __call__(self, batch):\n        output = dict()\n        output[\"ids\"] = [sample[\"ids\"] for sample in batch]\n        output[\"mask\"] = [sample[\"mask\"] for sample in batch]\n\n        # calculate max token length of this batch\n        batch_max = max([len(ids) for ids in output[\"ids\"]])\n\n        # add padding\n        if self.tokenizer.padding_side == \"right\":\n            output[\"ids\"] = [s + (batch_max - len(s)) * [self.tokenizer.pad_token_id] for s in output[\"ids\"]]\n            output[\"mask\"] = [s + (batch_max - len(s)) * [0] for s in output[\"mask\"]]\n        else:\n            output[\"ids\"] = [(batch_max - len(s)) * [self.tokenizer.pad_token_id] + s for s in output[\"ids\"]]\n            output[\"mask\"] = [(batch_max - len(s)) * [0] + s for s in output[\"mask\"]]\n\n        # convert to tensors\n        output[\"ids\"] = torch.tensor(output[\"ids\"], dtype=torch.long)\n        output[\"mask\"] = torch.tensor(output[\"mask\"], dtype=torch.long)\n\n        return output\n    \n    \nclass FeedbackModel(tez.Model):\n    def __init__(self, model_name, num_labels):\n        super().__init__()\n        self.model_name = model_name\n        self.num_labels = num_labels\n        config = AutoConfig.from_pretrained(model_name)\n\n        hidden_dropout_prob: float = 0.2\n        layer_norm_eps: float = 17589e-7\n        config.update(\n            {\n                \"output_hidden_states\": True,\n                \"hidden_dropout_prob\": hidden_dropout_prob,\n                \"layer_norm_eps\": layer_norm_eps,\n                \"add_pooling_layer\": False,\n            }\n        )\n        self.transformer = AutoModel.from_config(config)\n        self.output = nn.Linear(config.hidden_size, self.num_labels)\n\n    def forward(self, ids, mask):\n        transformer_out = self.transformer(ids, mask)\n        sequence_output = transformer_out.last_hidden_state\n        logits = self.output(sequence_output)\n        logits = torch.softmax(logits, dim=-1)\n        return logits, 0, {}","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-03-12T20:12:25.484257Z","iopub.execute_input":"2022-03-12T20:12:25.484611Z","iopub.status.idle":"2022-03-12T20:12:25.504465Z","shell.execute_reply.started":"2022-03-12T20:12:25.484578Z","shell.execute_reply":"2022-03-12T20:12:25.503773Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def _prepare_test_data_helper(args, tokenizer, ids):\n    test_samples = []\n    for idx in ids:\n        filename = os.path.join(args.input_path, \"test\", idx + \".txt\")\n        with open(filename, \"r\") as f:\n            text = f.read()\n\n        encoded_text = tokenizer.encode_plus(\n            text,\n            add_special_tokens=False,\n            return_offsets_mapping=True,\n        )\n        input_ids = encoded_text[\"input_ids\"]\n        offset_mapping = encoded_text[\"offset_mapping\"]\n\n        sample = {\n            \"id\": idx,\n            \"input_ids\": input_ids,\n            \"text\": text,\n            \"offset_mapping\": offset_mapping,\n        }\n\n        test_samples.append(sample)\n    return test_samples\n\n\ndef prepare_test_data(df, tokenizer, args):\n    test_samples = []\n    ids = df[\"id\"].unique()\n    ids_splits = np.array_split(ids, 4)\n\n    results = Parallel(n_jobs=4, backend=\"multiprocessing\")(\n        delayed(_prepare_test_data_helper)(args, tokenizer, idx) for idx in ids_splits\n    )\n    for result in results:\n        test_samples.extend(result)\n\n    return test_samples\n\n\ndef jn(pst, start, end):\n    return \" \".join([str(x) for x in pst[start:end]])\n\n\ndef link_evidence(oof):\n    thresh = 1\n    thresh2 = 30  # Baseline: 26\n    \n    idu = oof['id'].unique()\n    idc = idu[1]\n    \n    eoof = oof[oof['class'] == \"Evidence\"]\n    neoof = oof[oof['class'] != \"Evidence\"]\n    \n    retval = []\n    \n    for idv in idu:\n        for c in  ['Lead', 'Position', 'Evidence', 'Claim', 'Concluding Statement',\n               'Counterclaim', 'Rebuttal']:\n            q = eoof[(eoof['id'] == idv) & (eoof['class'] == c)]\n            if len(q) == 0:\n                continue\n            pst = []\n            for i,r in q.iterrows():\n                pst = pst +[-1] + [int(x) for x in r['predictionstring'].split()]\n            start = 1\n            end = 1\n            for i in range(2,len(pst)):\n                cur = pst[i]\n                end = i\n\n                if (cur == -1 and c != 'Evidence') or ((cur == -1) and ((pst[i+1] > pst[end-1] + thresh) or (pst[i+1] - pst[start] > thresh2))):\n                    retval.append((idv, c, jn(pst, start, end)))\n                    start = i + 1\n            v = (idv, c, jn(pst, start, end+1))\n\n            retval.append(v)\n            \n    roof = pd.DataFrame(retval, columns = ['id', 'class', 'predictionstring']) \n    roof = roof.merge(neoof, how='outer')\n    \n    return roof\n\n\ndef visualize(example):\n    id_col = \"id\"\n    class_col = \"class\"\n    predict_col = \"predictionstring\"\n\n    example_file = example + \".txt\"\n    example_path = os.path.join(COMP_DIR, 'test', example_file)\n    \n    if not os.path.isfile(example_path):\n        return None\n    \n    with open(example_path, 'r') as file:\n        data = file.read()\n        \n    inds_dict = {i: [ele.start(), ele.end()] for i, ele in enumerate(re.finditer(r'\\S+', data))}\n    \n    example_df = submission[submission[id_col] == example].copy()\n    example_df['indx'] = example_df[predict_col].apply(lambda s: int([x for x in s.split()][0]))\n    example_df = example_df.sort_values(by='indx')\n                                                              \n    ents = []\n    for i, row in example_df.iterrows():\n        discourse = row[predict_col]\n        first_word = int([x for x in discourse.split()][0])\n        last_word = int([x for x in discourse.split()][-1])\n        ents.append({\n            'start': inds_dict.get(first_word)[0],\n            'end': inds_dict.get(last_word)[1],\n            'label': row[class_col]\n        })\n\n    doc2 = {\n        \"text\": data,\n        \"ents\": ents,\n        \"title\": None\n    }\n    \n    options = {\"ents\": list(COLORS.keys()), \"colors\": COLORS}\n    displacy.render(doc2, style=\"ent\", options=options, manual=True, jupyter=True)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-03-12T20:12:25.506664Z","iopub.execute_input":"2022-03-12T20:12:25.50706Z","iopub.status.idle":"2022-03-12T20:12:25.531734Z","shell.execute_reply.started":"2022-03-12T20:12:25.507024Z","shell.execute_reply":"2022-03-12T20:12:25.531064Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv(\"../input/feedback-prize-2021/sample_submission.csv\")\ndf_ids = df[\"id\"].unique()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-03-12T20:12:25.534653Z","iopub.execute_input":"2022-03-12T20:12:25.535423Z","iopub.status.idle":"2022-03-12T20:12:25.556952Z","shell.execute_reply.started":"2022-03-12T20:12:25.535394Z","shell.execute_reply":"2022-03-12T20:12:25.556316Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%whos module","metadata":{"execution":{"iopub.status.busy":"2022-03-12T20:12:25.557837Z","iopub.execute_input":"2022-03-12T20:12:25.560239Z","iopub.status.idle":"2022-03-12T20:12:25.567779Z","shell.execute_reply.started":"2022-03-12T20:12:25.560201Z","shell.execute_reply":"2022-03-12T20:12:25.566798Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%whos function","metadata":{"execution":{"iopub.status.busy":"2022-03-12T20:12:25.570063Z","iopub.execute_input":"2022-03-12T20:12:25.570638Z","iopub.status.idle":"2022-03-12T20:12:25.578864Z","shell.execute_reply.started":"2022-03-12T20:12:25.570603Z","shell.execute_reply":"2022-03-12T20:12:25.578084Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%whos dict or str or DataFrame or ndarray","metadata":{"execution":{"iopub.status.busy":"2022-03-12T20:12:25.579996Z","iopub.execute_input":"2022-03-12T20:12:25.580264Z","iopub.status.idle":"2022-03-12T20:12:25.592133Z","shell.execute_reply.started":"2022-03-12T20:12:25.580235Z","shell.execute_reply":"2022-03-12T20:12:25.591354Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2. Create test_samples (Prediction & Scores)","metadata":{}},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(args1.model)\ntest_samples = prepare_test_data(df, tokenizer, args1)\n\ncollate = Collate(tokenizer=tokenizer)\n\nraw_preds = []\nfor fold_ in range(10):\n    current_idx = 0\n    test_dataset = FeedbackDataset(test_samples, args1.max_len, tokenizer)\n    \n    if fold_ < 5:\n        model = FeedbackModel(model_name=args1.model, num_labels=len(target_id_map) - 1)\n        model.load(os.path.join(args1.tez_model, f\"model_{fold_}.bin\"), weights_only=True)\n        preds_iter = model.predict(test_dataset, batch_size=args1.batch_size, n_jobs=-1, collate_fn=collate)\n    else:\n        model = FeedbackModel(model_name=args2.model, num_labels=len(target_id_map) - 1)\n        model.load(os.path.join(args2.tez_model, f\"model_{fold_-5}.bin\"), weights_only=True)\n        preds_iter = model.predict(test_dataset, batch_size=args2.batch_size, n_jobs=-1, collate_fn=collate)\n        \n    current_idx = 0\n    \n    for preds in preds_iter:\n        preds = preds.astype(np.float16)\n        preds = preds / 10\n        if fold_ == 0:\n            raw_preds.append(preds)\n        else:\n            raw_preds[current_idx] += preds\n            current_idx += 1\n    torch.cuda.empty_cache()\n    gc.collect()","metadata":{"_kg_hide-input":false,"execution":{"iopub.status.busy":"2022-03-12T20:12:25.593511Z","iopub.execute_input":"2022-03-12T20:12:25.593942Z","iopub.status.idle":"2022-03-12T20:16:52.130954Z","shell.execute_reply.started":"2022-03-12T20:12:25.593896Z","shell.execute_reply":"2022-03-12T20:16:52.13018Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_preds = []\nfinal_scores = []\n\nfor rp in raw_preds:\n    pred_class = np.argmax(rp, axis=2)\n    pred_scrs = np.max(rp, axis=2)\n    for pred, pred_scr in zip(pred_class, pred_scrs):\n        pred = pred.tolist()\n        pred_scr = pred_scr.tolist()\n        final_preds.append(pred)\n        final_scores.append(pred_scr)\n\nfor j in range(len(test_samples)):\n    tt = [id_target_map[p] for p in final_preds[j][1:]]\n    tt_score = final_scores[j][1:]\n    test_samples[j][\"preds\"] = tt\n    test_samples[j][\"pred_scores\"] = tt_score","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-03-12T20:16:52.134034Z","iopub.execute_input":"2022-03-12T20:16:52.134618Z","iopub.status.idle":"2022-03-12T20:16:52.14595Z","shell.execute_reply.started":"2022-03-12T20:16:52.134586Z","shell.execute_reply":"2022-03-12T20:16:52.145142Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"n_samples = len(test_samples)\nn_samples","metadata":{"execution":{"iopub.status.busy":"2022-03-12T20:16:52.149544Z","iopub.execute_input":"2022-03-12T20:16:52.14979Z","iopub.status.idle":"2022-03-12T20:16:52.159075Z","shell.execute_reply.started":"2022-03-12T20:16:52.149761Z","shell.execute_reply":"2022-03-12T20:16:52.158096Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"check_sample = test_samples[random.randint(0, n_samples-1)]\nmax_len_values = 180\n\nprint(check_sample.keys(), \"\\n\")\nprint(check_sample.get('id'), \"\\n\")\nprint(check_sample.get('text')[:max_len_values * 5], \"... \\n\")\nprint(check_sample.get('input_ids')[:max_len_values], \"... \\n\")\nprint(check_sample.get('offset_mapping')[:max_len_values], \"... \\n\")\nprint(check_sample.get('preds')[:max_len_values], \"... \\n\")\nprint(check_sample.get('pred_scores')[:max_len_values], \"... \\n\")","metadata":{"execution":{"iopub.status.busy":"2022-03-12T20:16:52.160556Z","iopub.execute_input":"2022-03-12T20:16:52.161154Z","iopub.status.idle":"2022-03-12T20:16:52.173888Z","shell.execute_reply.started":"2022-03-12T20:16:52.161095Z","shell.execute_reply":"2022-03-12T20:16:52.172828Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3. Create & Save Submission","metadata":{}},{"cell_type":"code","source":"# ********************************* Baseline:\nproba_thresh = {\n    \"Lead\": 0.6875,                 # 0.6875\n    \"Position\": 0.5550,             # 0.5375\n    \"Evidence\": 0.5300,             # 0.6375\n    \"Claim\": 0.5300,                # 0.5375\n    \"Concluding Statement\": 0.7100, # 0.6875\n    \"Counterclaim\": 0.5375,         # 0.5375\n    \"Rebuttal\": 0.5375              # 0.5375\n}\n\nmin_thresh = {\n    \"Lead\": 9,                      # 9\n    \"Position\": 5,                  # 5\n    \"Evidence\": 7,                  # 14\n    \"Claim\": 2,                     # 3\n    \"Concluding Statement\": 5,      # 11\n    \"Counterclaim\": 6,              # 6\n    \"Rebuttal\": 4                   # 4\n}","metadata":{"execution":{"iopub.status.busy":"2022-03-12T20:16:52.175768Z","iopub.execute_input":"2022-03-12T20:16:52.176161Z","iopub.status.idle":"2022-03-12T20:16:52.185599Z","shell.execute_reply.started":"2022-03-12T20:16:52.176098Z","shell.execute_reply":"2022-03-12T20:16:52.184526Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission = []\n\nfor sample_idx, sample in enumerate(test_samples):\n    preds = sample[\"preds\"]\n    offset_mapping = sample[\"offset_mapping\"]\n    sample_id = sample[\"id\"]\n    sample_text = sample[\"text\"]\n    sample_input_ids = sample[\"input_ids\"]\n    sample_pred_scores = sample[\"pred_scores\"]\n    sample_preds = []\n\n    if len(preds) < len(offset_mapping):\n        preds = preds + [\"O\"] * (len(offset_mapping) - len(preds))\n        sample_pred_scores = sample_pred_scores + [0] * (len(offset_mapping) - len(sample_pred_scores))\n    \n    idx = 0\n    phrase_preds = []\n    while idx < len(offset_mapping):\n        start, _ = offset_mapping[idx]\n        if preds[idx] != \"O\":\n            label = preds[idx][2:]\n        else:\n            label = \"O\"\n        phrase_scores = []\n        phrase_scores.append(sample_pred_scores[idx])\n        idx += 1\n        while idx < len(offset_mapping):\n            if label == \"O\":\n                matching_label = \"O\"\n            else:\n                matching_label = f\"I-{label}\"\n            if preds[idx] == matching_label:\n                _, end = offset_mapping[idx]\n                phrase_scores.append(sample_pred_scores[idx])\n                idx += 1\n            else:\n                break\n        if \"end\" in locals():\n            phrase = sample_text[start:end]\n            phrase_preds.append((phrase, start, end, label, phrase_scores))\n\n    temp_df = []\n    for phrase_idx, (phrase, start, end, label, phrase_scores) in enumerate(phrase_preds):\n        word_start = len(sample_text[:start].split())\n        word_end = word_start + len(sample_text[start:end].split())\n        word_end = min(word_end, len(sample_text.split()))\n        ps = \" \".join([str(x) for x in range(word_start, word_end)])\n        if label != \"O\":\n            if sum(phrase_scores) / len(phrase_scores) >= proba_thresh[label]:\n                if len(ps.split()) >= min_thresh[label]:\n                    temp_df.append((sample_id, label, ps))\n    \n    submission.extend(temp_df)\n\nsubmission = pd.DataFrame(submission, columns=[\"id\", \"class\", \"predictionstring\"])\nsubmission = link_evidence(submission)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-03-12T20:16:52.187267Z","iopub.execute_input":"2022-03-12T20:16:52.187718Z","iopub.status.idle":"2022-03-12T20:16:52.265006Z","shell.execute_reply.started":"2022-03-12T20:16:52.187613Z","shell.execute_reply":"2022-03-12T20:16:52.264257Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission.head()","metadata":{"execution":{"iopub.status.busy":"2022-03-12T20:16:52.266553Z","iopub.execute_input":"2022-03-12T20:16:52.267078Z","iopub.status.idle":"2022-03-12T20:16:52.287473Z","shell.execute_reply.started":"2022-03-12T20:16:52.267035Z","shell.execute_reply":"2022-03-12T20:16:52.28659Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission.to_csv(\"submission.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2022-03-12T20:16:52.289247Z","iopub.execute_input":"2022-03-12T20:16:52.290009Z","iopub.status.idle":"2022-03-12T20:16:52.298678Z","shell.execute_reply.started":"2022-03-12T20:16:52.289967Z","shell.execute_reply":"2022-03-12T20:16:52.297877Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4. Visualisation the result","metadata":{}},{"cell_type":"code","source":"for g_id, g_data in submission.groupby('id'):\n    sorted_predict = g_data['predictionstring'].str.split(expand=True) \\\n                                [0].astype(int).sort_values().index\n    \n    display(g_data.loc[sorted_predict, :])\n    visualize(g_id)","metadata":{"execution":{"iopub.status.busy":"2022-03-12T20:16:52.299948Z","iopub.execute_input":"2022-03-12T20:16:52.300407Z","iopub.status.idle":"2022-03-12T20:16:52.435751Z","shell.execute_reply.started":"2022-03-12T20:16:52.300363Z","shell.execute_reply":"2022-03-12T20:16:52.435Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}