{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Simple MLP for March Madness Predictions üèÄüèÄüèÄ\n\nIn this notebook, I predict March Madness scores by utilizing a combination of feature engineering and neural networks! This notebook includes a simple neural network which is trained a large amount of somewhat related data, and an advanced neural network which is trained on a small amount of related data. I'll cover the preprocessing, implementations, predictions, and strengths/weaknesses of each model, as well as explain more of my thought process and steps as the notebook progresses, so without further ado, let's begin!\n\nNote: You can follow along with the code even if you don't know basketball very well; however, there are some nuances in the feature engineering which involve some knowledge of basketball (references are provided though). I also use the terms neural network, NN (short for neural network), and <a href=\"https://machinelearningmastery.com/neural-networks-crash-course/\">MLP</a> (short for multilayer perceptron) interchangeably throughout this notebook.\n\n**If you are interested in using your code to generate results which you can use to fill out your March Madness bracket, check out my related notebook <a href=\"https://www.kaggle.com/ironicninja/generate-a-march-madness-bracket\"><strong>here</strong></a>!**"},{"metadata":{},"cell_type":"markdown","source":"# Essential Imports\n\nNote, not every import is used here; this is just a convenient template I like to use (+ it doesn't hurt to have additional imports here)!"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"#-----General------#\nimport numpy as np\nimport pandas as pd\nimport os\nimport sys\nimport math\nimport random\n\n#-----Plotting-----#\nimport matplotlib.pyplot as plt\nimport plotly.graph_objects as go\nimport plotly.express as px\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport seaborn as sns\nfrom pandas_profiling import ProfileReport\n\n#-----Utility-----#\nimport itertools\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport re\nimport gc\n\n#-----DS Packages-----#\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler\nimport tensorflow as tf\nimport tensorflow_addons as tfa\nfrom sklearn.model_selection import GroupKFold, StratifiedKFold, RepeatedStratifiedKFold, KFold, train_test_split\nfrom sklearn.decomposition import PCA\nfrom scipy.stats import pearsonr, spearmanr\nimport xgboost as xgb\nimport keras\nimport keras.layers as layers\n\n#-----Random Keras Stuff for Quick Implementation-----#\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout\nfrom keras.regularizers import l2\nfrom keras.initializers import GlorotNormal\nfrom keras.optimizers import Adam\nfrom keras.callbacks import EarlyStopping\n\nSEED = 42\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntf.random.set_seed(SEED)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Load in the Data\n\nFor this notebook, I use only 4 out of the 20 csv files provided. The main reason is because most of the data is unnecessary for predictions (e.g. cities, teams, etc.), not detailed enough for predictions (compact versions), or too complicated to effectively include in the model (e.g. coaches). I'll give a quick description of each file I use:\n\n* ```STAGE (1 or 2)``` -  Determines which folder to take data from.\n* ```regular_df (MRegularSeasonDetailedResults.csv)``` - Detailed regular season games starting from 2003.\n* ```tourney_df (MNCAATourneyDetailedResults.csv)``` - Detailed MNCAA \"March Madness\" Tournament games starting from 2003.\n* ```nit_df (MSecondaryTourneyCompactResults.csv)``` - National Invitational Tournament (secondary tournament) games starting from 2003.\n* ```rankings_df (MMasseyOrdinals.csv)``` - College basketball rankings organized by Kenneth Massey.\n* ```teams_df (MTeams.csv)``` - Contains the team name associated with each TeamID.\n\nI'll also give a brief rundown of other files I experimented with but did not use in my final predictions:\n\n* ```seeding_df (MNCAATourneySeeds.csv)``` - Seeds for the MCNAA Tournament. Did not include because of the difficulty quantifying differences between conferences (i.e., Seed 1 in Conference A is sometimes only as good as Seed 3 in Conference B).\n* ```conference_tourney_df (MConferenceTourneyGames.csv)``` - Compact results of conference tourney games. There was no score differential for these games and therefore they were not included in the analysis."},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\nSTAGE = 2\n\nregular_df = pd.read_csv(f\"../input/ncaam-march-mania-2021-spread/MDataFiles_Stage{STAGE}_Spread/MRegularSeasonDetailedResults.csv\")\ntourney_df = pd.read_csv(f\"../input/ncaam-march-mania-2021-spread/MDataFiles_Stage{STAGE}_Spread/MNCAATourneyDetailedResults.csv\")\nnit_df = pd.read_csv(f\"../input/ncaam-march-mania-2021-spread/MDataFiles_Stage{STAGE}_Spread/MSecondaryTourneyCompactResults.csv\")\nrankings_df = pd.read_csv(f\"../input/ncaam-march-mania-2021-spread/MDataFiles_Stage{STAGE}_Spread/MMasseyOrdinals.csv\")\nteams_df = pd.read_csv(f\"../input/ncaam-march-mania-2021-spread/MDataFiles_Stage{STAGE}_Spread/MTeams.csv\")\n\n# Not formally used but experimented with before\n\nseeding_df = pd.read_csv(f\"../input/ncaam-march-mania-2021-spread/MDataFiles_Stage{STAGE}_Spread/MNCAATourneySeeds.csv\")\nconference_tourney_df = pd.read_csv(f\"../input/ncaam-march-mania-2021-spread/MDataFiles_Stage{STAGE}_Spread/MConferenceTourneyGames.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Preprocessing\n\nIn this section, I prepare the DataFrames from which I will use for my training data. I'll provide some commentary for each function in the markdown above the code."},{"metadata":{},"cell_type":"markdown","source":"<h3> Function Descriptions </h3>\n\n* ```poss(game, college)``` - Calculates the number of possessions for the game.\n* ```offense_calc(game, college=True)``` - Calculates the offensive rating (ORTG) of a team. For a team, this is points per 100 possessions.\n* ```defense_calc(game, college=True)``` - Calculates the defensive rating (DRTG) of a team. For a team, this is points allowed per 100 possessions.\n\nSince we are in the context of college basketball, I opt to use the college-specific calculation of ORTG and DRTG.\n\n<h3> References </h3>\n\n* <a href=\"https://www.sports-reference.com/cbb/about/glossary.html\"> College Basketball Reference </a>\n* <a href=\"https://www.basketball-reference.com/about/glossary.html\"> NBA Basketball Reference </a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"def poss(game, college):\n    PTS = game['Score']\n    FGM = game['FGM']\n    FGA = game['FGA']\n    FTA = game['FTA']\n    TOV = game['TO']\n    ORB = game['OR']\n    DRB = game['DR']\n    \n    OppPTS = game['OppScore']\n    OppFGM = game['OppFGM']\n    OppFGA = game['OppFGA']\n    OppFTA = game['OppFTA']\n    OppTOV = game['OppTO']\n    OppORB = game['OppOR']\n    OppDRB = game['OppDR']\n    \n    if college:\n        return 0.5 * (FGA + 0.475 * FTA - ORB + TOV) + 0.5 * (OppFGA + 0.475 * OppFTA - OppORB + OppTOV)\n    else:\n        return 0.5 * ((FGA + 0.4 * FTA - 1.07 * (ORB / (ORB + OppDRB)) * (FGA - FGM) + TOV) + \n               (OppFGA + 0.4 * OppFTA - 1.07 * (OppORB / (OppORB + DRB)) * (OppFGA - OppFGM) + OppTOV))\n\ndef offense_calc(game, college=True):\n    PTS = game['Score']\n    POSS = poss(game, college)\n    return PTS/POSS*100\n\ndef defense_calc(game, college=True):\n    PTS = game['OppScore']\n    POSS = poss(game, college)\n    return PTS/POSS*100","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3> Function Description </h3>\n\n```extract_weighted_avg(season, weight_type='linear')``` - Calculates the average stats for each team in a given season. Utilizes a weighted function using the parameter ```weight_type```.\n\n<h3> Options for parameter weight_type</h3>\n\n* ```linear``` - A game played on Day 1 of the season has a weight of 1; a game played on Day 100 of the season has a weight of 100. More recent games have **a lot more** weight than older games.\n* ```log``` - A game played on Day 1 of the season has a weight of ~0.6; a game played on Day 100 of the season has a weight of ~2. More recent games have **slightly more** weight than older games.\n* ```normal``` - A game played on Day 1 of the season has a weight of 1; a game played on Day 100 of the season has a weight of 1. Recent games are indistinguishable from older games (no weight difference)."},{"metadata":{"trusted":true},"cell_type":"code","source":"def extract_weighted_avg(season, weight_type='linear'):\n    season_df = regular_df.loc[regular_df['Season'] == season]\n    min_daynum = min(season_df['DayNum'])\n\n    winner_cols = ['WScore', 'LScore', 'WFGM', 'WFGA', 'WFGM3', 'WFGA3', 'WFTM', 'WFTA', 'WOR', 'WDR',\n       'WAst', 'WTO', 'WStl', 'WBlk', 'WPF', 'LFGM', 'LFGA', 'LFGM3', 'LFGA3',\n       'LFTM', 'LFTA', 'LOR', 'LDR', 'LAst', 'LTO', 'LStl', 'LBlk', 'LPF']\n    loser_cols = ['LScore', 'WScore', 'LFGM', 'LFGA', 'LFGM3', 'LFGA3',\n       'LFTM', 'LFTA', 'LOR', 'LDR', 'LAst', 'LTO', 'LStl', 'LBlk', 'LPF', 'WFGM', 'WFGA', 'WFGM3', 'WFGA3', 'WFTM', 'WFTA', 'WOR', 'WDR',\n       'WAst', 'WTO', 'WStl', 'WBlk', 'WPF']\n    stats_dict = {}\n    weight_dict = {}\n    wl_dict = {}\n    \n    for i in season_df.index:\n        game_ser = season_df.loc[i]\n        \n        if weight_type.lower() == 'linear':\n            day_weight = game_ser['DayNum'] - min_daynum + 1\n        elif weight_type.lower() == 'log':\n            day_weight = np.log(game_ser['DayNum'] - min_daynum + 2)\n        elif weight_type.lower() == 'normal':\n            day_weight = 1\n        else:\n            raise Exception(\"Please enter a correct weight_type (linear, log, normal).\")\n        \n        # Compute Winner\n        winner_arr = day_weight*game_ser[winner_cols].to_numpy()\n        wteamid = game_ser['WTeamID']\n        if wteamid in stats_dict:\n            stats_dict[wteamid] += winner_arr\n            weight_dict[wteamid] += day_weight\n            wl_dict[wteamid][0] += 1\n        else:\n            stats_dict[wteamid] = winner_arr\n            weight_dict[wteamid] = day_weight\n            wl_dict[wteamid] = [1, 0]\n            \n        # Compute Loser\n        loser_arr = day_weight*game_ser[loser_cols].to_numpy()\n        lteamid = game_ser['LTeamID']\n        if lteamid in stats_dict:\n            stats_dict[lteamid] += loser_arr\n            weight_dict[lteamid] += day_weight\n            wl_dict[lteamid][1] += 1\n        else:\n            stats_dict[lteamid] = loser_arr\n            weight_dict[lteamid] = day_weight\n            wl_dict[lteamid] = [0, 1]\n        \n    for team in stats_dict:\n        stats_dict[team] /= weight_dict[team]\n        \n    w_dict, l_dict = {}, {}\n    for team in wl_dict:\n        w_dict[team] = wl_dict[team][0]\n        l_dict[team] = wl_dict[team][1]\n        \n    cols = ['Score', 'OppScore', 'FGM', 'FGA', 'FGM3', 'FGA3',\n       'FTM', 'FTA', 'OR', 'DR', 'Ast', 'TO', 'Stl', 'Blk', 'PF', 'OppFGM', 'OppFGA', 'OppFGM3', 'OppFGA3',\n       'OppFTM', 'OppFTA', 'OppOR', 'OppDR', 'OppAst', 'OppTO', 'OppStl', 'OppBlk', 'OppPF']\n    \n    weighted_df = pd.DataFrame.from_dict(stats_dict).T\n    weighted_df.columns = cols\n    weighted_df.sort_index(inplace=True)\n    weighted_df['Diff'] = weighted_df['Score'] - weighted_df['OppScore']\n    weighted_df['FG%'] = weighted_df['FGM']/weighted_df['FGA']\n    weighted_df['FG3%'] = weighted_df['FGM3']/weighted_df['FGA3']\n    weighted_df['FT%'] = weighted_df['FTM']/weighted_df['FTA']\n    weighted_df['OppFG%'] = weighted_df['OppFGM']/weighted_df['OppFGA']\n    weighted_df['OppFG3%'] = weighted_df['OppFGM3']/weighted_df['OppFGA3']\n    weighted_df['OppFT%'] = weighted_df['OppFTM']/weighted_df['OppFTA']\n    \n    weighted_df['Wins'] = weighted_df.index.map(w_dict)\n    weighted_df['Losses'] = weighted_df.index.map(l_dict)\n    \n    season_rankings_df = rankings_df.loc[rankings_df['Season'] == season]\n    rankings_dict = season_rankings_df.loc[season_rankings_df['RankingDayNum'] == max(season_rankings_df['RankingDayNum'])].set_index(\"TeamID\")['OrdinalRank'].to_dict()\n    weighted_df['Ranking'] = weighted_df.index.map(rankings_dict)\n    \n    ortg_list = []\n    drtg_list = []\n    for i in weighted_df.index:\n        game = weighted_df.loc[i]\n        ortg_list.append(offense_calc(game))\n        drtg_list.append(defense_calc(game))\n    \n    weighted_df['ORTG'] = ortg_list\n    weighted_df['DRTG'] = drtg_list\n    return weighted_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In this snippet of code, I store the data for each season in a dictionary so it's easily accessible."},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\nall_seasons_data = {}\nfor season in regular_df['Season'].unique():\n    print(f\"Extracting {season-1}-{season} Season...\")\n    all_seasons_data[season] = extract_weighted_avg(season, weight_type='log')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3> As a Demonstration... </h3>"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df = extract_weighted_avg(2003)\ntest_log_df = extract_weighted_avg(2003, weight_type='log')\ntest_normal_df = extract_weighted_avg(2003, weight_type='normal')\n\ntest_comb_df = pd.concat((test_df['Score'], test_log_df['Score'], test_normal_df['Score']), axis=1)\ntest_comb_df.columns = ['Linear', 'Log', 'Normal']\ntest_comb_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The differences are subtle but can be very relevant. For the rest of this notebook, I will use ```weight_type``` of log since it weighs recent games a little more than older games, but not by much."},{"metadata":{},"cell_type":"markdown","source":"# Simple Model\n\nIn this first model, I decided to try to predict the score of games based on the statistics of each game. In other words, given my knowledge of Team 1's ```Field Goals Made```, ```Field Goal Percentage```, ```Turnovers```, etc., can I predict the score of the game?\n\nFor my training data, I combine a team's offensive attributes (FGM, FGA, etc.) with the other team's defensive attributes (Steals, Blocks, etc.), which should be sufficient to predict the outcome of a game.\n\nNote: I later opt to not use ```FGM```, ```FGM3```, and ```FTM``` since those attributes had a tendency to overfit the model (since it could just map out the function $2*\\text{FGM} + 3*\\text{FGM3} + \\text{FTM}$)."},{"metadata":{},"cell_type":"markdown","source":"<h3> Extra Model Preprocessing </h3>\n\nThe functions here should be pretty self-explanatory, so I won't provide any commentary for them."},{"metadata":{"trusted":true},"cell_type":"code","source":"def add_data(df, c='W'):\n    df[f'{c}FG%'] = df[f'{c}FGM']/df[f'{c}FGA']\n    df[f'{c}FG3%'] = df[f'{c}FGM3']/df[f'{c}FGA3']\n    df[f'{c}FT%'] = df[f'{c}FTM']/df[f'{c}FTA']\n    return df\n\nextra_df = regular_df.copy()\nextra_df = add_data(extra_df, c='W')\nextra_df = add_data(extra_df, c='L')\nextra_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#offense_cols = ['FGM', 'FG%', 'FGM3', 'FG3%', 'FTM', 'FT%', 'OR', 'Ast', 'TO']\noffense_cols = ['FG%', 'FG3%', 'FT%', 'OR', 'Ast', 'TO'] # For testing without FGM, FGM3, and FTM\n\ndefense_cols = ['DR', 'Stl', 'Blk', 'PF']\ncols = offense_cols + defense_cols + ['Score']\n\nw_cols = ['W' + t for t in offense_cols] + ['L' + t for t in defense_cols]\nl_cols = ['L' + t for t in offense_cols] + ['W' + t for t in defense_cols]\n\nw_tmp = extra_df[w_cols + ['WScore']]\nw_tmp.columns = cols\nl_tmp = extra_df[l_cols + ['LScore']]\nl_tmp.columns = cols\nall_data = pd.concat((w_tmp, l_tmp)).reset_index(drop=True)\nall_data","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We prepare the data by scaling the inputs with ```StandardScaler()``` but not scaling the targets (since it's unnecessary)."},{"metadata":{"trusted":true},"cell_type":"code","source":"all_data = all_data.loc[np.count_nonzero(np.isnan(all_data), axis=1) == 0]\n\nX_scaler_simple = StandardScaler()\nX_simple = X_scaler_simple.fit_transform(all_data.loc[:, ~all_data.columns.str.contains('Score')])\ny_simple = all_data['Score'].to_numpy().reshape(-1, 1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3> Creating and Training a Simple MLP </h3>\n\nThis model is quite simple, with some batch normalization and dropout layers. I opt to use the <a href=\"https://medium.com/@neuralnets/swish-activation-function-by-google-53e1ea86f820\"> swish activation function </a> and <a href=\"https://www.pyimagesearch.com/2019/10/07/is-rectified-adam-actually-better-than-adam/\"> RectifiedAdam optimizer</a>."},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_simple_mlp(num_columns, num_labels, hidden_layers, hidden_units, dropout_rates, learning_rate, regularizer_rate):\n    inp = layers.Input(shape=(num_columns,))\n    x = layers.BatchNormalization()(inp)\n    for i in range(hidden_layers):\n        x = layers.Dense(hidden_units, activation=tf.keras.activations.swish, kernel_regularizer=tf.keras.regularizers.l2(regularizer_rate))(x)\n        x = layers.Dropout(dropout_rates)(x)\n        x = layers.BatchNormalization()(x)\n\n    out = layers.Dense(num_labels)(x)\n\n    model = tf.keras.models.Model(inputs=inp, outputs=out)\n    model.compile(\n        optimizer=tfa.optimizers.RectifiedAdam(learning_rate=learning_rate),\n        loss='mse',\n        metrics=['mae', tf.keras.metrics.RootMeanSquaredError()],\n    )\n\n    return model","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"You can take a look at the model parameters and architecture here."},{"metadata":{"trusted":true},"cell_type":"code","source":"hidden_layers_simple = 2\nhidden_units_simple = 64\ndropout_rates_simple = 0.25\nlearning_rate_simple = 2e-3\nregularizer_rate_simple = 1e-4\nmodel_simple = create_simple_mlp(X_simple.shape[1], y_simple.shape[1], hidden_layers_simple, \n                                 hidden_units_simple, dropout_rates_simple, learning_rate_simple, regularizer_rate_simple)\n\ntf.keras.utils.plot_model(model_simple)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The architecture is quite simple, with two hidden layers consisting of a Dense layer, Dropout layer, and some BatchNormalization. To train the model, I use KFold with 5 splits plus 10 epochs of finetuning at the end."},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\nkf = KFold(n_splits=5, shuffle=True, random_state=SEED)\nall_history = {}\n\nc = 1\nfor train_index, test_index in kf.split(X_simple, y_simple):\n    print(f\"Fold {c} initiating...\")\n    history_simple = model_simple.fit(X_simple[train_index], y_simple[train_index], epochs=20, \n                                      verbose=0, batch_size=256, validation_data=(X_simple[test_index], y_simple[test_index]))\n    \n    if c == 1:\n        all_history = history_simple.history\n    else:\n        for error in history_simple.history:\n            all_history[error] += history_simple.history[error]\n    \n    c += 1\n    \nhistory_simple = model_simple.fit(X_simple, y_simple, epochs=10, \n                    verbose=1, batch_size=256, validation_data=(X_simple[test_index], y_simple[test_index]))\n\nmodel_simple.save_weights(\"MNCAAsimple.hdf5\")\n\nfor error in history_simple.history:\n    all_history[error] += history_simple.history[error]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3> Model Analysis </h3>\n\nError history (MAE, Validation MAE, Validation RMSE) is graphed here."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10, 8))\nplt.plot(all_history['mae'])\nplt.plot(all_history['val_mae'])\nplt.plot(all_history['val_root_mean_squared_error'])\nplt.ylim(0, 2*np.mean(all_history['val_root_mean_squared_error']))\nplt.legend(['mae', 'val_mae', 'val_rmse'])\nplt.xlabel(\"Epochs\", fontsize=14)\nplt.ylabel(\"Error\", fontsize=14)\nplt.title(\"Model Error Over Epochs\", fontsize=16)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The training error doesn't seem to decrease much past epoch 5, and the validation MAE and RMSE greatly fluctuate around 1 for every epoch. It's important to note, however, that an error of 1 is extremely good, since it means the network is predicting the actual score to a margin of 1 point.\n\nHere we prepare the training data. **Note that for these predictions, we include both the MCNAA and NIT Tournaments, but NOT the conference tournaments.** This is so that there is more data when validating our model so it's not as overfit."},{"metadata":{"trusted":true},"cell_type":"code","source":"my_cols = offense_cols + [\"Opp\" + t for t in defense_cols]\n\ndef create_data_simple(season):\n    year_df = all_seasons_data[season]\n    tourney_year_df = tourney_df.loc[tourney_df['Season'] == season]\n    nit_year_df = nit_df.loc[nit_df['Season'] == season]\n    \n    X_1 = []\n    y_1 = []\n    X_2 = []\n    y_2 = []\n    \n    def helper(df):\n        for i in df.index:\n            game = df.loc[i]\n            w_df = year_df.loc[game['WTeamID']]\n            l_df = year_df.loc[game['LTeamID']]\n            \n            X_1.append(w_df[offense_cols].tolist() + l_df[defense_cols].tolist())\n            X_2.append(l_df[offense_cols].tolist() + w_df[defense_cols].tolist())\n                \n            w_score = game['WScore']\n            l_score = game['LScore']\n            y_1.append(w_score)\n            y_2.append(l_score)\n                       \n    helper(tourney_year_df)\n    helper(nit_year_df)\n    \n    return X_1, y_1, X_2, y_2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\nX_val_1_simple, y_true_1_simple = [], []\nX_val_2_simple, y_true_2_simple = [], []\nfor season in regular_df['Season'].unique():\n    print(season)\n    X_1, y_1, X_2, y_2 = create_data_simple(season)\n    X_val_1_simple += X_1\n    y_true_1_simple += y_1\n    X_val_2_simple += X_2\n    y_true_2_simple += y_2","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"And now we can perform some preliminary validation tests."},{"metadata":{"trusted":true},"cell_type":"code","source":"def mae(y_pred, y_true):\n    return np.mean(np.abs(y_pred - y_true))\n\ndef rmse(y_pred, y_true):\n    return np.sqrt(np.mean(np.power(y_pred - y_true, 2)))\n\nX1_simple = X_scaler_simple.transform(np.array(X_val_1_simple, dtype='float64'))\nX2_simple = X_scaler_simple.transform(np.array(X_val_2_simple, dtype='float64'))\ny_true_1_simple = np.array(y_true_1_simple, dtype='float64')\ny_true_2_simple = np.array(y_true_2_simple, dtype='float64')\n\nX_pred_1_simple = model_simple.predict(X1_simple).reshape(-1)\nX_pred_2_simple = model_simple.predict(X2_simple).reshape(-1)\n\nprint(\"Validation MAE 1: %.3f\" % (mae(X_pred_1_simple, y_true_1_simple)))\nprint(\"Validation MAE 2: %.3f\" % (mae(X_pred_2_simple, y_true_2_simple)))\n\npred_diff_simple = X_pred_1_simple - X_pred_2_simple\ntrue_diff_simple = y_true_1_simple - y_true_2_simple\n\nprint(\"True Difference MAE: %.3f\" % mae(pred_diff_simple, true_diff_simple))\nprint(\"True Difference RMSE: %.3f\" % rmse(pred_diff_simple, true_diff_simple))\n\nprint(\"Correct Winner: %.3f%%\" % (100*np.count_nonzero(pred_diff_simple*true_diff_simple > 0)/len(pred_diff_simple)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The RMSE difference is ~13.7, which isn't great but is also not too bad. The model predicts the correct winner only 55.1% of the time though, which is somewhat disappointing."},{"metadata":{},"cell_type":"markdown","source":"<h3> Prediction File </h3>"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\ndef prediction_simple(season):\n    year_df = seeding_df[seeding_df['Season'] == season]\n    all_ids = list(set(year_df['TeamID']))\n    all_ids.sort()\n    \n    season_df = all_seasons_data[season]\n    \n    id1_list = []\n    id2_list = []\n    index_list = []\n    for id1 in all_ids:\n        for id2 in all_ids:\n            if id2 > id1:\n                team1_df = season_df.loc[id1]\n                team2_df = season_df.loc[id2]\n                id1_list.append(team1_df[offense_cols].tolist() + team2_df[defense_cols].tolist())\n                id2_list.append(team2_df[offense_cols].tolist() + team1_df[defense_cols].tolist())\n                index_list.append(f\"{season}_{id1}_{id2}\")\n                \n    pred_1 = model_simple.predict(X_scaler_simple.transform(np.array(id1_list, dtype='float64')))\n    pred_2 = model_simple.predict(X_scaler_simple.transform(np.array(id2_list, dtype='float64')))\n    diff_pred = pred_1 - pred_2\n    predictions_df = pd.DataFrame(data=diff_pred, index=index_list, columns=['Pred'])\n    return predictions_df\n    \npredictions_list = []\nseasons_pred_list = [2015, 2016, 2017, 2018, 2019] if STAGE == 1 else [2021]\nfor season in seasons_pred_list:\n    print(f\"Season {season-1}-{season} Processing...\")\n    predictions_list.append(prediction_simple(season))\n    \nfinal_pred_simple_df = pd.concat(predictions_list).reset_index()\nfinal_pred_simple_df.columns = ['ID', 'Pred']\nfinal_pred_simple_df.to_csv(\"submission_simple.csv\", index=False)\nfinal_pred_simple_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3> Simulating an Actual Tournament </h3>\n\nIn the code below, I simulate a real MNCAA Tournament and use my ```model_simple``` to predict which teams would win.\n\n**The function has two parameters:**\n* ```season (int)``` - Simulation of the MNCAA Tournament for a certain season. Note, I've only tested this code on the 2018, 2019, and 2021 seasons, so there could be issues with prior seasons.\n* ```PRINT (bool)``` - Determines whether the results should be printed to the terminal or not.\n\nNote, this code is outdated and has been improved on in <a href=\"https://www.kaggle.com/ironicninja/generate-a-march-madness-bracket\">this notebook</a>. I've kept it here since it was my first version of my improved code."},{"metadata":{"trusted":true},"cell_type":"code","source":"class predictSeasonSimple():\n    def __init__(self, season, PRINT=True):\n        self.year_df = seeding_df[seeding_df['Season'] == season]\n        self.season_df = all_seasons_data[season]\n        self.season = season\n        self.PRINT = PRINT\n        self.res_list = []\n        \n    def predict_id(self, id_1, id_2):\n        pred_arr = np.array([self.season_df.loc[id_1][offense_cols].tolist() + self.season_df.loc[id_2][defense_cols].tolist()])\n        return model_simple.predict_step(X_scaler_simple.transform(pred_arr)).numpy()[0][0]\n    \n    def team_id(self, id_test):\n        return teams_df.loc[teams_df['TeamID'] == id_test]['TeamName'].iloc[0]\n    \n    def playin_round(self, id_tuple, div_df, playin_df, seed, div):\n        id1, id2 = id_tuple\n\n        res1 = self.predict_id(id1, id2)\n        res2 = self.predict_id(id2, id1)\n\n        team1 = f\"{seed} {self.team_id(id1)}\"\n        team2 = f\"{seed} {self.team_id(id2)}\"\n        \n        final_res = res1-res2\n        final_res_str = \"won\" if final_res >= 0 else \"lost\"\n        self.res_list.append([team1, team2, team1 if final_res >= 0 else team2, div])\n        print(f\"Play-in round, seeds {team1} played {team2} and {final_res_str} by %.2f points.\" % (abs(final_res))) if self.PRINT else 0\n        \n        return div_df.append(playin_df.loc[playin_df['TeamID'] == id1] if final_res >= 0 else playin_df.loc[playin_df['TeamID'] == id2])\n    \n    def predict_div(self, div):\n        div_df = self.year_df.loc[self.year_df['Seed'].str.contains(div)]\n        div_df['Seed'] = div_df['Seed'].str.replace(div, '')\n        div_df['Seed'] = div_df['Seed'].str.lstrip('0')\n\n        # Check for play-in rounds\n        playin_df = div_df.loc[div_df['Seed'].str.len() > 2]\n\n        if len(playin_df):         \n            div_df = div_df.loc[div_df['Seed'].str.len() <= 2]\n            playin_df['Seed'] = playin_df['Seed'].str.slice(0, 2)\n            seed = playin_df.iloc[0]['Seed']\n            id_first = playin_df.iloc[:2]['TeamID'].tolist()\n            div_df = self.playin_round(id_first, div_df, playin_df, seed, div)\n            \n            if len(playin_df) == 4:\n                id_second = playin_df.iloc[-2:]['TeamID'].tolist()\n                div_df = self.playin_round(id_second, div_df, playin_df, 16, div)\n\n        matchup_df = div_df[['Seed', 'TeamID']].set_index('Seed')\n        play_list = [1, 16, 8, 9, 5, 12, 4, 13, 6, 11, 3, 14, 7, 10, 2, 15]\n\n        while True:\n            tmp_list = []\n            for i in range(0, len(play_list), 2):\n                seed1 = str(play_list[i])\n                seed2 = str(play_list[i+1])\n\n                id1 = matchup_df.loc[seed1]['TeamID']\n                id2 = matchup_df.loc[seed2]['TeamID']\n\n                res1 = self.predict_id(id1, id2)\n                res2 = self.predict_id(id2, id1)\n\n                team1 = f\"{seed1} {self.team_id(id1)}\"\n                team2 = f\"{seed2} {self.team_id(id2)}\"\n\n                final_res = res1-res2\n                final_res_str = \"won\" if final_res >= 0 else \"lost\"\n                self.res_list.append([team1, team2, team1 if final_res >= 0 else team2, div])\n                print(f\"Seed {team1} played seed {team2} and {final_res_str} by %.2f points.\" % (abs(final_res))) if self.PRINT else 0\n\n                tmp_list.append(seed1 if final_res >= 0 else seed2)\n\n            play_list = tmp_list.copy()\n            if len(play_list) <= 1:\n                break\n\n        return matchup_df.loc[play_list[0]]['TeamID']\n    \n    def predict_tour(self):\n        div_list = ['W', 'X', 'Y', 'Z']\n        div_winners = {}\n        for div in div_list:\n            print(f\"Division {div}\") if self.PRINT else 0\n            winning_seed = self.predict_div(div)\n            div_winners[div] = winning_seed\n            print(\"\\n\") if self.PRINT else 0\n        \n        # Under the assumption that Division W always plays Division X in the Final Four\n        id1 = div_winners['W']\n        id2 = div_winners['X']\n        id3 = div_winners['Y']\n        id4 = div_winners['Z']\n        \n        team1 = self.team_id(id1)\n        team2 = self.team_id(id2)\n        team3 = self.team_id(id3)\n        team4 = self.team_id(id4)\n        \n        f4_res_1 = self.predict_id(id1, id2)\n        f4_res_2 = self.predict_id(id2, id1)\n        final_res_1 = f4_res_1-f4_res_2\n        final_res_str_1 = \"won\" if final_res_1 >= 0 else \"lost\"\n        f4_winner_1 = [id1, team1] if final_res_1 >= 0 else [id2, team2]\n        self.res_list.append([team1, team2, team1 if final_res_1 >= 0 else team2, \"F4\"])\n        print(f\"{team1} played {team2} and {final_res_str_1} by %.2f points.\" % (abs(final_res_1))) if self.PRINT else 0\n        \n        f4_res_3 = self.predict_id(id3, id4)\n        f4_res_4 = self.predict_id(id4, id3)\n        final_res_2 = f4_res_3-f4_res_4\n        final_res_str_2 = \"won\" if final_res_2 >= 0 else \"lost\"\n        f4_winner_2 = [id3, team3] if final_res_2 >= 0 else [id4, team4]\n        self.res_list.append([team3, team4, team3 if final_res_2 >= 0 else team4, \"F4\"])\n        print(f\"{team3} played {team4} and {final_res_str_2} by %.2f points.\" % (abs(final_res_2))) if self.PRINT else 0\n        \n        champ_res_1 = self.predict_id(f4_winner_1[0], f4_winner_2[0])\n        champ_res_2 = self.predict_id(f4_winner_2[0], f4_winner_1[0])\n        champ_res_final = champ_res_1 - champ_res_2\n        champ_res_str = \"won\" if champ_res_final >= 0 else \"lost\"\n        champion = f4_winner_1[1] if champ_res_final >= 0 else f4_winner_2[1]\n        self.res_list.append([f4_winner_1[1], f4_winner_2[1], champion, \"Finals\"])\n        print(f\"{f4_winner_1[1]} played {f4_winner_2[1]} and {champ_res_str} by %.2f points.\" % (abs(champ_res_final))) if self.PRINT else 0\n        print(f\"THE CHAMPION FOR THE {self.season} SEASON IS {champion}!!! Final score is {int(champ_res_1)} to {int(champ_res_2)}.\")\n        \n        res_df = pd.DataFrame(data=self.res_list, columns=[\"Team 1\", \"Team 2\", \"Winner\", \"Division\"])\n        return res_df\n    \nmy_season = predictSeasonSimple(2021, False)\nres_df = my_season.predict_tour()\npd.set_option('display.max_rows', 68)\nres_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.set_option('display.max_rows', 10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3> Simple Model Strengths & Weaknesses </h3>\n\n**Strengths:**\n* The biggest strength of this model is the sheer amount of data provided as both training and validation data. There are over 180,000 different scores (excluding the tournament scores), which means a neural network can better learn patterns within the data.\n* The model is also quite simple which makes it easier to replicate and less likely to overfit when training.\n\n**Weaknesses:**\n* The biggest weakness of this model is that predicting the outcome of a game when you already have the stats for that game is much different than predicting how many points a team will score based on their season statistics. \n* Another weakness is that this score-predicting model may be significantly overfit; in theory, the neural network would just have to learn the function $2*\\text{FGM} + 3*\\text{FGM3} + \\text{FTM}$ to accurately predict the score. \n    * Indeed, after conducting experiments testing this hypothesis (by removing FGM, FGM3, and FTM from the training data), the neural network can only predict the score accurate to $\\pm$ 3 to 4 points; however, the relative validation error decreases by removing these attributes which have a tendency to overfit."},{"metadata":{},"cell_type":"markdown","source":"# Advanced Model 1 (\"Sparse\")\n\nThe advanced model is well... more advanced. In essence, after aggregating the stats for each team in a given season, each MNCAA/NIT Tournament matchup is split into a team's stats (e.g. PTS, FGM, Ast), the average opponent team's stats (e.g. Points allowed, Field Goals allowed), and then that same process for the other team. To put the methodology a bit more concretely, if Team A plays Team B, then the training data would be Team A's average stats, Team A's average opponent's stats, Team B's average stats, and Team B's average opponent's stats. The target data would be the difference in score between the two teams. I implement this methodology in the next two blocks of code.\n\nNote: I call this model \"sparse\" since it is smaller than Advanced Model 2."},{"metadata":{"trusted":true},"cell_type":"code","source":"my_team_cols = ['Score', 'FGM', 'FG%', 'FGM3', 'FG3%', 'FTM', 'FT%', 'OR', 'DR', 'Ast', 'TO', 'Stl', 'Blk', 'PF']\n#my_team_cols = ['FG%', 'FG3%', 'FT%', 'OR', 'DR', 'Ast', 'TO', 'Stl', 'Blk', 'PF']\n\nopp_team_cols = ['Opp' + t for t in my_team_cols]\n\nother_cols = ['Wins', 'Losses', 'Diff', 'Ranking', 'ORTG', 'DRTG'] # Used for Advanced Model 2\n\ndef create_data_season(season):\n    year_df = all_seasons_data[season]\n    tourney_year_df = tourney_df.loc[tourney_df['Season'] == season]\n    nit_year_df = nit_df.loc[nit_df['Season'] == season]\n    \n    w_my_team_train, w_opp_team_train = [], []\n    l_my_team_train, l_opp_team_train = [], []\n    w_other_train, l_other_train = [], [] # Used for Advanced Model 2\n    diff_train = []\n    \n    def helper(df):\n        for i in df.index:\n            game = df.loc[i]\n            w_game = year_df.loc[game['WTeamID']]\n            l_game = year_df.loc[game['LTeamID']]\n            \n            w_my_team_train.append(w_game[my_team_cols].tolist())\n            w_opp_team_train.append(w_game[opp_team_cols].tolist())\n            l_my_team_train.append(l_game[my_team_cols].tolist())\n            l_opp_team_train.append(l_game[opp_team_cols].tolist())\n            \n            w_other_train.append(w_game[other_cols].tolist()) # Used for Advanced Model 2\n            l_other_train.append(l_game[other_cols].tolist()) # Used for Advanced Model 2\n            \n            diff_train.append(game['WScore'] - game['LScore'])       \n                       \n    helper(tourney_year_df)\n    helper(nit_year_df)\n    \n    return w_my_team_train, w_opp_team_train, l_my_team_train, l_opp_team_train, w_other_train, l_other_train, diff_train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\nmy_team_train1, opp_team_train1 = [], []\nmy_team_train2, opp_team_train2 = [], []\nmy_team_test1, opp_team_test1 = [], []\nmy_team_test2, opp_team_test2 = [], []\nother_train1, other_train2 = [], []\nother_test1, other_test2 = [], []\ny_train, y_test = [], []\n\nfor season in regular_df['Season'].unique():\n    print(season)\n    w_my_team_train, w_opp_team_train, l_my_team_train, l_opp_team_train, w_other_train, l_other_train, diff_train = create_data_season(season)\n    if season < 2015:\n        my_team_train1 += w_my_team_train\n        opp_team_train1 += w_opp_team_train\n        my_team_train2 += l_my_team_train\n        opp_team_train2 += l_opp_team_train\n        other_train1 += w_other_train\n        other_train2 += l_other_train\n        y_train += diff_train\n    else:\n        my_team_test1 += w_my_team_train\n        opp_team_test1 += w_opp_team_train\n        my_team_test2 += l_my_team_train\n        opp_team_test2 += l_opp_team_train\n        other_test1 += w_other_train\n        other_test2 += l_other_train\n        y_test += diff_train","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3> Processing the Data </h3>\n\nThere are two things important to notice here; first, once again, the inputs are scaled using the ```StandardScaler()``` but the targets are not scaled. Second, the training data is concatenated to compensate for the asymmetry of the data, or in other words, there should be no difference with putting Team A then Team B versus Team B then Team A. **While certainly not a perfect solution, it works decently in implementation.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Convert to np.array\n\nmy_team_train1, opp_team_train1 = np.array(my_team_train1, dtype='float64'), np.array(opp_team_train1, dtype='float64')\nmy_team_train2, opp_team_train2 = np.array(my_team_train2, dtype='float64'), np.array(opp_team_train2, dtype='float64')\nmy_team_test1, opp_team_test1 = np.array(my_team_test1, dtype='float64'), np.array(opp_team_test1, dtype='float64')\nmy_team_test2, opp_team_test2 = np.array(my_team_test2, dtype='float64'), np.array(opp_team_test2, dtype='float64')\nother_train1, other_train2 = np.array(other_train1, dtype='float64'), np.array(other_train2, dtype='float64')\nother_test1, other_test2 = np.array(other_test1, dtype='float64'), np.array(other_test2, dtype='float64')\ny_train, y_test = np.array(y_train, dtype='float64'), np.array(y_test, dtype='float64')\n\n# Fit on data\n\nX_scaler = StandardScaler()\nX_scaler.fit(my_team_train1)\nX_scaler.fit(opp_team_train1)\nX_scaler.fit(my_team_train2)\nX_scaler.fit(opp_team_train2)\n\n# Scale all the data based on fit\n\nmy_team_train1_t = X_scaler.transform(my_team_train1)\nopp_team_train1_t = X_scaler.transform(opp_team_train1)\nmy_team_train2_t = X_scaler.transform(my_team_train2)\nopp_team_train2_t = X_scaler.transform(opp_team_train2)\n\nmy_team_test1_t = X_scaler.transform(my_team_test1)\nopp_team_test1_t = X_scaler.transform(opp_team_test1)\nmy_team_test2_t = X_scaler.transform(my_team_test2)\nopp_team_test2_t = X_scaler.transform(opp_team_test2)\n\n# Scale other columns; only relevant for Advanced Model 2\n\nother_scaler = StandardScaler()\nother_scaler.fit(other_train1)\nother_scaler.fit(other_train2)\n\nother_train1_t = other_scaler.transform(other_train1)\nother_train2_t = other_scaler.transform(other_train2)\nother_test1_t = other_scaler.transform(other_test1)\nother_test2_t = other_scaler.transform(other_test2)\n\n# Concatenate the data\n\ny_all = np.concatenate((y_train, -1*y_train))\nX_team_1 = np.concatenate((my_team_train1_t, my_team_train2_t))\nX_team_2 = np.concatenate((my_team_train2_t, my_team_train1_t))\nX_opp_1 = np.concatenate((opp_team_train1_t, opp_team_train2_t))\nX_opp_2 = np.concatenate((opp_team_train2_t, opp_team_train1_t))\nX_other_1 = np.concatenate((other_train1_t, other_train2_t))\nX_other_2 = np.concatenate((other_train2_t, other_train1_t))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_mlp_sparse(num_columns, num_labels, num_layers, hidden_units, dropout_rates, learning_rate, regularizer_rate):\n    inp1 = layers.Input(shape=(num_columns,))\n    x1 = layers.BatchNormalization()(inp1)\n    for i in range(num_layers):\n        x1 = layers.Dense(hidden_units, activation=tf.keras.activations.swish, kernel_regularizer=tf.keras.regularizers.l2(regularizer_rate))(x1)\n        x1 = layers.Dropout(dropout_rates)(x1)\n        x1 = layers.BatchNormalization()(x1)\n    \n    inp2 = layers.Input(shape=(num_columns,))\n    x2 = layers.BatchNormalization()(inp2)\n    for i in range(num_layers):\n        x2 = layers.Dense(hidden_units, activation=tf.keras.activations.swish, kernel_regularizer=tf.keras.regularizers.l2(regularizer_rate))(x2)\n        x2 = layers.Dropout(dropout_rates)(x2)\n        x2 = layers.BatchNormalization()(x2)\n        \n    inp3 = layers.Input(shape=(num_columns,))\n    x3 = layers.BatchNormalization()(inp3)\n    for i in range(num_layers):\n        x3 = layers.Dense(hidden_units, activation=tf.keras.activations.swish, kernel_regularizer=tf.keras.regularizers.l2(regularizer_rate))(x3)\n        x3 = layers.Dropout(dropout_rates)(x3)\n        x3 = layers.BatchNormalization()(x3)\n        \n    inp4 = layers.Input(shape=(num_columns,))\n    x4 = layers.BatchNormalization()(inp4)\n    for i in range(num_layers):\n        x4 = layers.Dense(hidden_units, activation=tf.keras.activations.swish, kernel_regularizer=tf.keras.regularizers.l2(regularizer_rate))(x4)\n        x4 = layers.Dropout(dropout_rates)(x4)\n        x4 = layers.BatchNormalization()(x4)\n        \n    merged1 = layers.Concatenate(axis=1)([x1, x2])\n    merged1 = layers.Dense(hidden_units, activation=tf.keras.activations.swish, kernel_regularizer=tf.keras.regularizers.l2(regularizer_rate))(merged1)\n    merged1 = layers.Dropout(dropout_rates)(merged1)\n    merged1 = layers.BatchNormalization()(merged1)\n    \n    merged2 = layers.Concatenate(axis=1)([x3, x4])\n    merged2 = layers.Dense(hidden_units, activation=tf.keras.activations.swish, kernel_regularizer=tf.keras.regularizers.l2(regularizer_rate))(merged2)\n    merged2 = layers.Dropout(dropout_rates)(merged2)\n    merged2 = layers.BatchNormalization()(merged2)\n    \n    final = layers.Concatenate(axis=1)([merged1, merged2])\n    out = layers.Dense(num_labels)(final)\n\n    model = tf.keras.models.Model(inputs=[inp1, inp2, inp3, inp4], outputs=out)\n    model.compile(\n        optimizer=tfa.optimizers.RectifiedAdam(learning_rate=learning_rate),\n        loss='mse',\n        metrics=['mae', tf.keras.metrics.RootMeanSquaredError()],\n    )\n\n    return model","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I initialize the MLP and visualize its architecture here."},{"metadata":{"trusted":true},"cell_type":"code","source":"num_layers_sparse = 2\nhidden_units_sparse = 64\ndropout_rates_sparse = 0.25\nlearning_rate_sparse = 2e-3\nregularizer_rate_sparse = 1e-4\nmodel_sparse = create_mlp_sparse(my_team_train1_t.shape[1], 1, num_layers_sparse, hidden_units_sparse, \n                                 dropout_rates_sparse, learning_rate_sparse, regularizer_rate_sparse)\n\ntf.keras.utils.plot_model(model_sparse)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Notice that this network consists of a dense top (2 hidden layers for each of the 4 inputs) and bottlenecks into a sparser bottom (1 hidden layer for the concatenated layer). This is a key difference between this model and Advanced Model 2."},{"metadata":{"trusted":true},"cell_type":"code","source":"history_sparse = model_sparse.fit([X_team_1, X_opp_1, X_team_2, X_opp_2], y_all, epochs=200, verbose=0, \n                    batch_size=32, validation_data=([my_team_test1_t, opp_team_test1_t, my_team_test2_t, opp_team_test2_t], y_test))\n\nmodel_sparse.save_weights(\"MCNAAsparse.hdf5\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3> Model Analysis </h3>"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10, 8))\nplt.plot(history_sparse.history['mae'])\nplt.plot(history_sparse.history['val_mae'])\nplt.plot(history_sparse.history['val_root_mean_squared_error'])\nplt.legend(['mae', 'val_mae', 'val_rmse'])\nplt.xlabel(\"Epochs\", fontsize=14)\nplt.ylabel(\"Error\", fontsize=14)\nplt.title(\"Model Error Over Epochs\", fontsize=16)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Unfortunately, it seems like this model overfits very quickly, as although the error continously decreases, the validation error gradually increases. This is definitely not what I would like to see, which is why I opt not to use this model for my actual predictions."},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_norm = model_sparse.predict([my_team_test1_t, opp_team_test1_t, my_team_test2_t, opp_team_test2_t])\ny_pred_alt = model_sparse.predict([my_team_test2_t, opp_team_test2_t, my_team_test1_t, opp_team_test1_t])\ny_comb = ((y_pred_norm-y_pred_alt)/2).reshape(-1)\nprint(\"Normal MAE: %.3f\" % (mae(y_pred_norm, y_test)))\nprint(\"Alternate MAE: %.3f\" % (mae(y_pred_alt, -1*y_test)))\nprint(\"Combined MAE: %.3f\" % (mae(y_comb, y_test)))\nprint(\"Correct Winner: %.3f%%\" % (100*np.count_nonzero(y_comb*y_test > 0)/len(y_test)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3> Sparse Model Strengths & Weaknesses </h3>\n\n**Strengths**:\n* Utilizes very relevant data as training data to provide more accurate predictions.\n\n**Weaknesses**:\n* Low amount of training data (2898 when including both tournaments and concatenation trick).\n* No feature engineering in terms of which features to include, what features not to include, and how to reduce the dimensionality of the data, which may affect accuracy and the model's tendency to overfit."},{"metadata":{},"cell_type":"markdown","source":"# Advanced Model 2 (\"Dense\")\n\nThis \"dense\" model is very similar to Advanced Model 1, except it includes an extra column of inputs and has a slightly different NN architecture."},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_mlp_dense(shape1, shape2, shape3, num_labels, num_layers, hidden_units, dropout_rates, learning_rate, regularizer_rate):\n    \n    # Left Side\n    \n    inp11 = layers.Input(shape=(shape1,))\n    x1 = layers.BatchNormalization()(inp11)\n    x1 = layers.Dense(hidden_units, activation=tf.keras.activations.swish, kernel_regularizer=tf.keras.regularizers.l2(regularizer_rate))(x1)\n    x1 = layers.Dropout(dropout_rates)(x1)\n    x1 = layers.BatchNormalization()(x1)\n    \n    inp12 = layers.Input(shape=(shape2,))\n    x2 = layers.BatchNormalization()(inp12)\n    x2 = layers.Dense(hidden_units, activation=tf.keras.activations.swish, kernel_regularizer=tf.keras.regularizers.l2(regularizer_rate))(x2)\n    x2 = layers.Dropout(dropout_rates)(x2)\n    x2 = layers.BatchNormalization()(x2)\n        \n    inp13 = layers.Input(shape=(shape3,))\n    x3 = layers.BatchNormalization()(inp13)\n    x3 = layers.Dense(hidden_units, activation=tf.keras.activations.swish, kernel_regularizer=tf.keras.regularizers.l2(regularizer_rate))(x3)\n    x3 = layers.Dropout(dropout_rates)(x3)\n    x3 = layers.BatchNormalization()(x3)\n    \n    merged11 = layers.Concatenate(axis=1)([x1, x2])\n    for i in range(num_layers):\n        merged11 = layers.Dense(hidden_units, activation=tf.keras.activations.swish, kernel_regularizer=tf.keras.regularizers.l2(regularizer_rate))(merged11)\n        merged11 = layers.Dropout(dropout_rates)(merged11)\n        merged11 = layers.BatchNormalization()(merged11)\n    \n    merged12 = layers.Concatenate(axis=1)([merged11, x3])\n    merged12 = layers.Dense(hidden_units, activation=tf.keras.activations.swish, kernel_regularizer=tf.keras.regularizers.l2(regularizer_rate))(merged12)\n    merged12 = layers.Dropout(dropout_rates)(merged12)\n    merged12 = layers.BatchNormalization()(merged12)\n        \n    # Right Side\n    \n    inp21 = layers.Input(shape=(shape1,))\n    x4 = layers.BatchNormalization()(inp21)\n    x4 = layers.Dense(hidden_units, activation=tf.keras.activations.swish, kernel_regularizer=tf.keras.regularizers.l2(regularizer_rate))(x4)\n    x4 = layers.Dropout(dropout_rates)(x4)\n    x4 = layers.BatchNormalization()(x4)\n    \n    inp22 = layers.Input(shape=(shape2,))\n    x5 = layers.BatchNormalization()(inp22)\n    x5 = layers.Dense(hidden_units, activation=tf.keras.activations.swish, kernel_regularizer=tf.keras.regularizers.l2(regularizer_rate))(x5)\n    x5 = layers.Dropout(dropout_rates)(x5)\n    x5 = layers.BatchNormalization()(x5)\n    \n    inp23 = layers.Input(shape=(shape3,))\n    x6 = layers.BatchNormalization()(inp23)\n    x6 = layers.Dense(hidden_units, activation=tf.keras.activations.swish, kernel_regularizer=tf.keras.regularizers.l2(regularizer_rate))(x6)\n    x6 = layers.Dropout(dropout_rates)(x6)\n    x6 = layers.BatchNormalization()(x6)\n        \n    merged21 = layers.Concatenate(axis=1)([x4, x5])\n    for i in range(num_layers):\n        merged21 = layers.Dense(hidden_units, activation=tf.keras.activations.swish, kernel_regularizer=tf.keras.regularizers.l2(regularizer_rate))(merged21)\n        merged21 = layers.Dropout(dropout_rates)(merged21)\n        merged21 = layers.BatchNormalization()(merged21)\n    \n    merged22 = layers.Concatenate(axis=1)([merged21, x6])\n    merged22 = layers.Dense(hidden_units, activation=tf.keras.activations.swish, kernel_regularizer=tf.keras.regularizers.l2(regularizer_rate))(merged22)\n    merged22 = layers.Dropout(dropout_rates)(merged22)\n    merged22 = layers.BatchNormalization()(merged22)\n    \n    final = layers.Concatenate(axis=1)([merged12, merged22])\n    out = layers.Dense(num_labels)(final)\n\n    model = tf.keras.models.Model(inputs=[inp11, inp12, inp13, inp21, inp22, inp23], outputs=out)\n    model.compile(\n        optimizer=tfa.optimizers.RectifiedAdam(learning_rate=learning_rate),\n        loss='mse',\n        metrics=['mae', tf.keras.metrics.RootMeanSquaredError()],\n    )\n\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"num_layers = 2\nhidden_units = 16\ndropout_rates = 0.25\nlearning_rate = 2e-3\nregularizer_rate = 1e-4\nmodel_dense = create_mlp_dense(X_team_1.shape[1], X_opp_1.shape[1], X_other_1.shape[1], 1, num_layers, hidden_units, dropout_rates, learning_rate, regularizer_rate)\n\ntf.keras.utils.plot_model(model_dense)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Notice the distinct differences in architecture between this \"Dense\" model and the above \"Sparse\" model. This model has a sparse top but a dense center with an extra two inputs."},{"metadata":{"trusted":true},"cell_type":"code","source":"history_dense = model_dense.fit([X_team_1, X_opp_1, X_other_1, X_team_2, X_opp_2, X_other_2], y_all, epochs=400, verbose=0, \n    batch_size=32, validation_data=([my_team_test1_t, opp_team_test1_t, other_test1_t, my_team_test2_t, opp_team_test2_t, other_test2_t], y_test))\n\nmodel_dense.save_weights(\"MCNAAdense.hdf5\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3> Model Analysis </h3>"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10, 8))\nplt.plot(history_dense.history['mae'])\nplt.plot(history_dense.history['val_mae'])\nplt.plot(history_dense.history['val_root_mean_squared_error'])\nplt.legend(['mae', 'val_mae', 'val_rmse'])\nplt.xlabel(\"Epochs\", fontsize=14)\nplt.ylabel(\"Error\", fontsize=14)\nplt.title(\"Model Error Over Epochs\", fontsize=16)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This graph looks somewhat similar to the error for the sparse model, except the validation error here is not as high nor does it increase as much as the validation error for the sparse model."},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_norm2 = model_dense.predict([my_team_test1_t, opp_team_test1_t, other_test1_t, my_team_test2_t, opp_team_test2_t, other_test2_t])\ny_pred_alt2 = model_dense.predict([my_team_test2_t, opp_team_test2_t, other_test2_t, my_team_test1_t, opp_team_test1_t, other_test1_t])\ny_comb2 = ((y_pred_norm2-y_pred_alt2)/2).reshape(-1)\nprint(\"Normal MAE: %.3f\" % (mae(y_pred_norm2, y_test)))\nprint(\"Alternate MAE: %.3f\" % (mae(y_pred_alt2, -1*y_test)))\nprint(\"Combined MAE: %.3f\" % (mae(y_comb2, y_test)))\nprint(\"Normal RMSE: %.3f\" % (rmse(y_pred_norm2, y_test)))\nprint(\"Alternate RMSE: %.3f\" % (rmse(y_pred_alt2, -1*y_test)))\nprint(\"Combined RMSE: %.3f\" % (rmse(y_comb2, y_test)))\nprint(\"Correct Winner: %.3f%%\" % (100*np.count_nonzero(y_comb2*y_test > 0)/len(y_test)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This model predicts the correct winner an astonishing 65% of the time! Compare that to our sparse model which predicted the correct winner 56% of the time and our simple model which predicted the correct winner 55% of the time and it becomes strikingly clear that this model is much more robust than the others. It's overall error is also less than the other models which is why I opt to use it as my primary predictor going into the 2021 March Madness Tournament."},{"metadata":{},"cell_type":"markdown","source":"<h3> Prediction File </h3>"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\ndef prediction_advanced(season, dense=True):\n    year_df = seeding_df[seeding_df['Season'] == season]\n    all_ids = list(set(year_df['TeamID']))\n    all_ids.sort()\n    \n    season_df = all_seasons_data[season]\n    \n    my_team_1, opp_team_1 = [], []\n    my_team_2, opp_team_2 = [], []\n    other1, other2 = [], []\n    index_list = []\n    \n    for id1 in all_ids:\n        for id2 in all_ids:\n            if id2 > id1:\n                id1_df = season_df.loc[id1]\n                id2_df = season_df.loc[id2]\n                my_team_1.append(id1_df[my_team_cols].tolist())\n                opp_team_1.append(id1_df[opp_team_cols].tolist())\n                my_team_2.append(id2_df[my_team_cols].tolist())\n                opp_team_2.append(id2_df[opp_team_cols].tolist())\n                other1.append(id1_df[other_cols].tolist())\n                other2.append(id2_df[other_cols].tolist())\n                index_list.append(f\"{season}_{id1}_{id2}\")\n                \n    my_team_1, opp_team_1 = X_scaler.transform(np.array(my_team_1, dtype='float64')), X_scaler.transform(np.array(opp_team_1, dtype='float64'))\n    my_team_2, opp_team_2 = X_scaler.transform(np.array(my_team_2, dtype='float64')), X_scaler.transform(np.array(opp_team_2, dtype='float64'))\n    other1, other2 = other_scaler.transform(np.array(other1, dtype='float64')), other_scaler.transform(np.array(other2, dtype='float64'))\n\n    if dense:\n        pred_1 = model_dense.predict([my_team_1, opp_team_1, other1, my_team_2, opp_team_2, other2])\n        pred_2 = model_dense.predict([my_team_2, opp_team_2, other2, my_team_1, opp_team_1, other1])\n    else:\n        pred_1 = model_sparse.predict([my_team_1, opp_team_1, my_team_2, opp_team_2])\n        pred_2 = model_sparse.predict([my_team_2, opp_team_2, my_team_1, opp_team_1])\n        \n    true_pred = (pred_1-pred_2)/2\n    predictions_df = pd.DataFrame(data=true_pred, index=index_list, columns=['Pred'])\n    return predictions_df\n    \npredictions_adv_list = []\nfor season in seasons_pred_list:\n    print(f\"Season {season-1}-{season} Processing...\")\n    predictions_adv_list.append(prediction_advanced(season))\n    \nfinal_pred_adv_df = pd.concat(predictions_adv_list).reset_index()\nfinal_pred_adv_df.columns = ['ID', 'Pred']\nfinal_pred_adv_df.to_csv(\"submission.csv\", index=False)\nfinal_pred_adv_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3> Simulate MNCAA Tournament with Advanced Model </h3>\n\nSimilar to my simulation with the simple model, there is better code provided <a href=\"https://www.kaggle.com/ironicninja/generate-a-march-madness-bracket\"> here</a>."},{"metadata":{"trusted":true},"cell_type":"code","source":"class predictSeasonAdvanced():\n    def __init__(self, season, dense=True, PRINT=True):\n        self.year_df = seeding_df[seeding_df['Season'] == season]\n        self.season_df = all_seasons_data[season]\n        self.season = season\n        self.PRINT = PRINT\n        self.res_list = []\n        self.dense = dense\n        \n    def predict_id(self, id_1, id_2):\n        id1_df = self.season_df.loc[id_1]\n        id2_df = self.season_df.loc[id_2]\n        mt1 = X_scaler.transform(np.array([id1_df[my_team_cols].tolist()]))\n        ot1 = X_scaler.transform(np.array([id1_df[opp_team_cols].tolist()]))\n        o1 = other_scaler.transform(np.array([id1_df[other_cols].tolist()]))\n        mt2 = X_scaler.transform(np.array([id2_df[my_team_cols].tolist()]))\n        ot2 = X_scaler.transform(np.array([id2_df[opp_team_cols].tolist()]))\n        o2 = other_scaler.transform(np.array([id2_df[other_cols].tolist()]))\n        \n        if self.dense:\n            pred_1 = model_dense.predict_step([mt1, ot1, o1, mt2, ot2, o2]).numpy()[0][0]\n            pred_2 = model_dense.predict_step([mt2, ot2, o2, mt1, ot1, o1]).numpy()[0][0]\n        else:\n            pred_1 = model_sparse.predict_step([mt1, ot1, mt2, ot2]).numpy()[0][0]\n            pred_2 = model_sparse.predict_step([mt2, ot2, mt1, ot1]).numpy()[0][0]\n        \n        pred_final = (pred_1-pred_2)/2\n        return pred_final\n    \n    def team_id(self, id_test):\n        return teams_df.loc[teams_df['TeamID'] == id_test]['TeamName'].iloc[0]\n    \n    def playin_round(self, id_tuple, div_df, playin_df, seed, div):\n        id1, id2 = id_tuple\n        final_res = self.predict_id(id1, id2)\n        final_res_str = \"won\" if final_res >= 0 else \"lost\"\n        \n        team1 = f\"{seed} {self.team_id(id1)}\"\n        team2 = f\"{seed} {self.team_id(id2)}\"\n        \n        self.res_list.append([team1, team2, team1 if final_res >= 0 else team2, div])\n        print(f\"Play-in round, seeds {team1} played {team2} and {final_res_str} by %.2f points.\" % (abs(final_res))) if self.PRINT else 0\n        \n        return div_df.append(playin_df.loc[playin_df['TeamID'] == id1] if final_res >= 0 else playin_df.loc[playin_df['TeamID'] == id2])\n    \n    def predict_div(self, div):\n        div_df = self.year_df.loc[self.year_df['Seed'].str.contains(div)]\n        div_df['Seed'] = div_df['Seed'].str.replace(div, '')\n        div_df['Seed'] = div_df['Seed'].str.lstrip('0')\n\n        # Check for play-in rounds\n        playin_df = div_df.loc[div_df['Seed'].str.len() > 2]\n\n        if len(playin_df):         \n            div_df = div_df.loc[div_df['Seed'].str.len() <= 2]\n            playin_df['Seed'] = playin_df['Seed'].str.slice(0, 2)\n            seed = playin_df.iloc[0]['Seed']\n            id_first = playin_df.iloc[:2]['TeamID'].tolist()\n            div_df = self.playin_round(id_first, div_df, playin_df, seed, div)\n            \n            if len(playin_df) == 4:\n                id_second = playin_df.iloc[-2:]['TeamID'].tolist()\n                div_df = self.playin_round(id_second, div_df, playin_df, 16, div)\n\n        matchup_df = div_df[['Seed', 'TeamID']].set_index('Seed')\n        play_list = [1, 16, 8, 9, 5, 12, 4, 13, 6, 11, 3, 14, 7, 10, 2, 15]\n\n        while True:\n            tmp_list = []\n            for i in range(0, len(play_list), 2):\n                seed1 = str(play_list[i])\n                seed2 = str(play_list[i+1])\n\n                id1 = matchup_df.loc[seed1]['TeamID']\n                id2 = matchup_df.loc[seed2]['TeamID']\n\n                final_res = self.predict_id(id1, id2)\n\n                team1 = f\"{seed1} {self.team_id(id1)}\"\n                team2 = f\"{seed2} {self.team_id(id2)}\"\n\n                final_res_str = \"won\" if final_res >= 0 else \"lost\"\n                self.res_list.append([team1, team2, team1 if final_res >= 0 else team2, div])\n                print(f\"Seed {team1} played seed {team2} and {final_res_str} by %.2f points.\" % (abs(final_res))) if self.PRINT else 0\n\n                tmp_list.append(seed1 if final_res >= 0 else seed2)\n\n            play_list = tmp_list.copy()\n            if len(play_list) <= 1:\n                break\n\n        return matchup_df.loc[play_list[0]]['TeamID']\n    \n    def predict_tour(self):\n        div_list = ['W', 'X', 'Y', 'Z']\n        div_winners = {}\n        for div in div_list:\n            print(f\"Division {div}\") if self.PRINT else 0\n            winning_seed = self.predict_div(div)\n            div_winners[div] = winning_seed\n            print(\"\\n\") if self.PRINT else 0\n        \n        # Under the assumption that Division W always plays Division X in the Final Four\n        id1 = div_winners['W']\n        id2 = div_winners['X']\n        id3 = div_winners['Y']\n        id4 = div_winners['Z']\n        \n        team1 = self.team_id(id1)\n        team2 = self.team_id(id2)\n        team3 = self.team_id(id3)\n        team4 = self.team_id(id4)\n        \n        final_res_1 = self.predict_id(id1, id2)\n        final_res_str_1 = \"won\" if final_res_1 >= 0 else \"lost\"\n        f4_winner_1 = [id1, team1] if final_res_1 >= 0 else [id2, team2]\n        self.res_list.append([team1, team2, team1 if final_res_1 >= 0 else team2, \"F4\"])\n        print(f\"{team1} played {team2} and {final_res_str_1} by %.2f points.\" % (abs(final_res_1))) if self.PRINT else 0\n        \n        final_res_2 = self.predict_id(id3, id4)\n        final_res_str_2 = \"won\" if final_res_2 >= 0 else \"lost\"\n        f4_winner_2 = [id3, team3] if final_res_2 >= 0 else [id4, team4]\n        self.res_list.append([team3, team4, team3 if final_res_2 >= 0 else team4, \"F4\"])\n        print(f\"{team3} played {team4} and {final_res_str_2} by %.2f points.\" % (abs(final_res_2))) if self.PRINT else 0\n        \n        champ_res_final = self.predict_id(f4_winner_1[0], f4_winner_2[0])\n        champ_res_str = \"won\" if champ_res_final >= 0 else \"lost\"\n        champion = f4_winner_1[1] if champ_res_final >= 0 else f4_winner_2[1]\n        self.res_list.append([f4_winner_1[1], f4_winner_2[1], champion, \"Finals\"])\n        print(f\"{f4_winner_1[1]} played {f4_winner_2[1]} and {champ_res_str} by %.2f points.\" % (abs(champ_res_final))) if self.PRINT else 0\n        print(f\"THE CHAMPION FOR THE {self.season} SEASON IS {champion}!!! They will win by {int(abs(champ_res_final))} points in the final round.\")\n        \n        res_df = pd.DataFrame(data=self.res_list, columns=[\"Team 1\", \"Team 2\", \"Winner\", \"Division\"])\n        return res_df\n    \nmy_season = predictSeasonAdvanced(2021, dense=True, PRINT=False)\nres_df = my_season.predict_tour()\npd.set_option('display.max_rows', 68)\nres_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.set_option('display.max_rows', 10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I won't provide a separate strengths & weaknesses section here since it would reiterate many of the same points as in Advanced Model 1, except an added strength here is a bit more data to work with and slightly more optimal architecture."},{"metadata":{},"cell_type":"markdown","source":"# Generating a March Madness Bracket\n\nCode is from <a href=\"https://www.kaggle.com/ironicninja/generate-a-march-madness-bracket\"> here</a>, a sample (and working) implementation is below."},{"metadata":{"trusted":true},"cell_type":"code","source":"class predictSeason():\n    def __init__(self, pred_df, season=2021, PRINT=False):\n        assert 'ID' in pred_df.columns, \"Column 'ID' is not found in your input DataFrame. Check your spelling and capitalization.\"\n        assert 'Pred' in pred_df.columns, \"Column 'Pred' is not found in your input DataFrame. Check your spelling and capitalization.\"\n        assert str(season) in pred_df['ID'].str.slice(0, 4).unique(), f\"{season} season not found in your prediction DataFrame.\"\n        \n        try:\n            self.teams_df = pd.read_csv(f\"../input/ncaam-march-mania-2021-spread/MDataFiles_Stage{STAGE}_Spread/MTeams.csv\")\n            self.seeding_df = pd.read_csv(f\"../input/ncaam-march-mania-2021-spread/MDataFiles_Stage2_Spread/MNCAATourneySeeds.csv\")\n        except:\n            raise Exception(\"Some files are not found. Ensure the file paths are correct.\")\n            \n        try:\n            self.year_df = self.seeding_df[self.seeding_df['Season'] == season]\n        except:\n            raise Exception(f\"The {season} season is out of range, please try again.\")\n            \n            \n        self.pred_df = pred_df\n        self.season = season\n        self.PRINT = PRINT\n        self.res_list = []\n        \n    def predict_id(self, id1, id2):\n        \"\"\"\n        Returns a boolean stating whether the team with id1 wins.\n        \"\"\"\n        \n        id_str = f\"{self.season}_{min(id1, id2)}_{max(id1, id2)}\"\n        pred = self.pred_df.loc[self.pred_df['ID'] == id_str]['Pred'].iloc[0]\n        res = True if pred >= 0 else False\n        res = (1-res) if id1 > id2 else res\n        return res\n    \n    def team_id(self, id_test):\n        \"\"\"\n        Returns the name of a team with a certain ID.\n        \"\"\"\n        \n        return self.teams_df.loc[self.teams_df['TeamID'] == id_test]['TeamName'].iloc[0]\n    \n    def playin_round(self, id_tuple, div_df, playin_df, seed, div):\n        \"\"\"\n        Handles logic for playin round (before Round of 64).\n        \"\"\"\n        \n        id1, id2 = id_tuple\n        \n        final_res = self.predict_id(id1, id2)\n\n        team1 = f\"{seed} {self.team_id(id1)}\"\n        team2 = f\"{seed} {self.team_id(id2)}\"\n        \n        final_res_str = \"won\" if final_res >= 0 else \"lost\"\n        self.res_list.append([team1, team2, team1 if final_res else team2, \"play-in\", div])\n        print(f\"Play-in round, seeds {team1} played {team2} and {final_res_str}.\") if self.PRINT else 0\n        \n        return div_df.append(playin_df.loc[playin_df['TeamID'] == id1] if final_res else playin_df.loc[playin_df['TeamID'] == id2])\n    \n    def predict_div(self, div):\n        \"\"\"\n        Simulate and return the division winner.\n        \"\"\"\n        \n        div_df = self.year_df.loc[self.year_df['Seed'].str.contains(div)]\n        div_df['Seed'] = div_df['Seed'].str.replace(div, '')\n        div_df['Seed'] = div_df['Seed'].str.lstrip('0')\n\n        # Check for play-in rounds\n        playin_df = div_df.loc[div_df['Seed'].str.len() > 2]\n\n        if len(playin_df):         \n            div_df = div_df.loc[div_df['Seed'].str.len() <= 2]\n            playin_df['Seed'] = playin_df['Seed'].str.slice(0, 2)\n            seed = playin_df.iloc[0]['Seed']\n            id_first = playin_df.iloc[:2]['TeamID'].tolist()\n            div_df = self.playin_round(id_first, div_df, playin_df, seed, div)\n            \n            if len(playin_df) == 4:\n                id_second = playin_df.iloc[-2:]['TeamID'].tolist()\n                div_df = self.playin_round(id_second, div_df, playin_df, 16, div)\n\n        matchup_df = div_df[['Seed', 'TeamID']].set_index('Seed')\n        play_list = [1, 16, 8, 9, 5, 12, 4, 13, 6, 11, 3, 14, 7, 10, 2, 15] # This initial order handles all of the logic we need for the bracket\n        round_num = 64\n\n        while True:\n            \"\"\"\n            Continue until there's a division winner.\n            \"\"\"\n            \n            tmp_list = []\n            for i in range(0, len(play_list), 2):\n                seed1 = str(play_list[i])\n                seed2 = str(play_list[i+1])\n\n                id1 = matchup_df.loc[seed1]['TeamID']\n                id2 = matchup_df.loc[seed2]['TeamID']\n\n                final_res = self.predict_id(id1, id2)\n\n                team1 = f\"{seed1} {self.team_id(id1)}\"\n                team2 = f\"{seed2} {self.team_id(id2)}\"\n\n                final_res_str = \"won\" if final_res else \"lost\"\n                self.res_list.append([team1, team2, team1 if final_res else team2, f\"R{int(round_num)}\", div])\n                print(f\"Seed {team1} played seed {team2} and {final_res_str}.\") if self.PRINT else 0\n\n                tmp_list.append(seed1 if final_res else seed2)\n\n            play_list = tmp_list.copy()\n            round_num /= 2\n            \n            if len(play_list) <= 1:\n                break\n\n        return matchup_df.loc[play_list[0]]['TeamID']\n    \n    def predict_tour(self):\n        \"\"\"\n        Driver function for creating the bracket. Run this and only this function.\n        \"\"\"\n        \n        div_list = ['W', 'X', 'Y', 'Z']\n        div_winners = {}\n        for div in div_list:\n            print(f\"Division {div}\") if self.PRINT else 0\n            winning_seed = self.predict_div(div)\n            div_winners[div] = winning_seed\n            print(\"\\n\") if self.PRINT else 0\n        \n        # Under the assumption that Division W always plays Division X in the Final Four\n        \n        id1 = div_winners['W']\n        id2 = div_winners['X']\n        id3 = div_winners['Y']\n        id4 = div_winners['Z']\n        \n        team1 = self.team_id(id1)\n        team2 = self.team_id(id2)\n        team3 = self.team_id(id3)\n        team4 = self.team_id(id4)\n        \n        # F4 W & X\n        final_res_1 = self.predict_id(id1, id2)\n        final_res_str_1 = \"won\" if final_res_1 else \"lost\"\n        f4_winner_1 = [id1, team1] if final_res_1 else [id2, team2]\n        self.res_list.append([team1, team2, team1 if final_res_1 else team2, \"R4\", \"F4\"])\n        print(f\"{team1} played {team2} and {final_res_str_1}.\") if self.PRINT else 0\n        \n        # F4 Y & Z\n        final_res_2 = self.predict_id(id3, id4)\n        final_res_str_2 = \"won\" if final_res_2 else \"lost\"\n        f4_winner_2 = [id3, team3] if final_res_2 else [id4, team4]\n        self.res_list.append([team3, team4, team3 if final_res_2 else team4, \"R4\", \"F4\"])\n        print(f\"{team3} played {team4} and {final_res_str_2}.\") if self.PRINT else 0\n        \n        # Championship\n        champ_res_final = self.predict_id(f4_winner_1[0], f4_winner_2[0])\n        champ_res_str = \"won\" if champ_res_final else \"lost\"\n        champion = f4_winner_1[1] if champ_res_final else f4_winner_2[1]\n        self.res_list.append([f4_winner_1[1], f4_winner_2[1], champion, \"R2\", \"Finals\"])\n        print(f\"THE CHAMPION FOR THE {self.season} SEASON IS {champion}!!!\")\n        \n        res_df = pd.DataFrame(data=self.res_list, columns=[\"Team 1\", \"Team 2\", \"Winner\", \"Round\", \"Division\"])\n        return res_df\n    \nmy_season = predictSeason(final_pred_simple_df, season=2021)\nres_df = my_season.predict_tour()\npd.set_option('display.max_rows', 68)\nres_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Concluding Remarks"},{"metadata":{},"cell_type":"markdown","source":"Thank you for reading through this notebook! I put these models together in around 3 days, so they are by no means perfect. I just wanted to share the work I conducted in case someone wants to try and use neural networks to predict March Madness scores or is just interested in MLPs in general. If you found this notebook to be particularly interesting or helpful, I would really appreciate it if you would give the notebook an <span style=\"color: green\"> upvote </span> or a <span style=\"color: blue\"> comment</span>!"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}