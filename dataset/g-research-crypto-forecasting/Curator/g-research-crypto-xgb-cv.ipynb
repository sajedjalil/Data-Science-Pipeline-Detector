{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# G-Research Crypto - Starter XGB Pipeline\n![](https://storage.googleapis.com/kaggle-competitions/kaggle/30894/logos/header.png)\n\n\n### Just a simple pipeline going from zero to a valid submission\n\n\n","metadata":{}},{"cell_type":"markdown","source":"# Import and load dfs\n\nReferences: [Tutorial to the G-Research Crypto Competition](https://www.kaggle.com/cstein06/tutorial-to-the-g-research-crypto-competition)","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom lightgbm import LGBMRegressor\nimport gresearch_crypto\nimport xgboost as xgb\nimport traceback\nimport datetime\nimport matplotlib.pyplot as plt\nfrom scipy.stats import pearsonr\n\n\nTRAIN_CSV = '/kaggle/input/g-research-crypto-forecasting/train.csv'\nASSET_DETAILS_CSV = '/kaggle/input/g-research-crypto-forecasting/asset_details.csv'\nDEVICE = 'GPU'","metadata":{"execution":{"iopub.status.busy":"2021-11-17T17:21:06.773371Z","iopub.execute_input":"2021-11-17T17:21:06.774226Z","iopub.status.idle":"2021-11-17T17:21:08.833668Z","shell.execute_reply.started":"2021-11-17T17:21:06.774129Z","shell.execute_reply":"2021-11-17T17:21:08.832992Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Reduce memory usage","metadata":{}},{"cell_type":"code","source":"# Memory saving function credit to https://www.kaggle.com/gemartin/load-data-reduce-memory-usage\ndef reduce_mem_usage(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.\n    \"\"\"\n    start_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n\n    for col in df.columns:\n        col_type = df[col].dtype.name\n\n        if col_type not in ['object', 'category', 'datetime64[ns, UTC]']:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n\n    end_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n\n    return df","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-11-17T17:21:08.835334Z","iopub.execute_input":"2021-11-17T17:21:08.835567Z","iopub.status.idle":"2021-11-17T17:21:08.850292Z","shell.execute_reply.started":"2021-11-17T17:21:08.835542Z","shell.execute_reply":"2021-11-17T17:21:08.849473Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"DEVICE = 'GPU'\n\ndf = pd.read_csv(TRAIN_CSV)\ndf = df.replace([np.inf, -np.inf], np.nan)\n\n#df_train.fillna(-999, inplace=True)\n# df = df[df['Target'].notna()]\n# df.interpolate(method='linear', inplace=True)\ndf = df.dropna(how=\"any\")\n\n# sorting data into groups of days\ndf['date'] = pd.to_datetime(df['timestamp'], unit = 's')\ndf = df.sort_values('date')\ngroups = pd.factorize(df['date'].dt.day.astype(str) + '_' + df['date'].dt.month.astype(str) + '_' + df['date'].dt.year.astype(str))[0]\n\n# reduce memory usage\ndf.drop(columns = 'date', inplace = True)\ntarget = df['Target'].copy()\ndf.drop(columns = 'Target', inplace = True)\ndf = reduce_mem_usage(df)\ndf['Target'] = target\ndf['groups'] = groups\n\n# getting rid of data overlap\ndf_train = df[df['timestamp'] < 1623542400]\ndf_train.info()","metadata":{"execution":{"iopub.status.busy":"2021-11-17T17:21:08.851447Z","iopub.execute_input":"2021-11-17T17:21:08.851654Z","iopub.status.idle":"2021-11-17T17:24:15.317245Z","shell.execute_reply.started":"2021-11-17T17:21:08.851629Z","shell.execute_reply":"2021-11-17T17:24:15.316444Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_asset_details = pd.read_csv(ASSET_DETAILS_CSV).sort_values(\"Asset_ID\")\ndf_asset_details","metadata":{"execution":{"iopub.status.busy":"2021-11-17T17:24:15.318514Z","iopub.execute_input":"2021-11-17T17:24:15.318797Z","iopub.status.idle":"2021-11-17T17:24:15.341503Z","shell.execute_reply.started":"2021-11-17T17:24:15.318759Z","shell.execute_reply":"2021-11-17T17:24:15.340705Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Utility functions to train a model for one asset","metadata":{}},{"cell_type":"code","source":"\nfrom pandas import DataFrame\nfrom pandas import concat\n \ndef series_to_supervised(data, n_in=1, n_out=1, dropnan=False, interpolate = False):\n    \"\"\"\n    Frame a time series as a supervised learning dataset.\n    Arguments:\n        data: Sequence of observations as a list or NumPy array.\n        n_in: Number of lag observations as input (X).\n        n_out: Number of observations as output (y).\n        dropnan: Boolean whether or not to drop rows with NaN values.\n    Returns:\n        Pandas DataFrame of series framed for supervised learning.\n    \"\"\"\n    n_vars = 1 if type(data) is list else data.shape[1]\n    df = DataFrame(data)\n    cols, names = list(), list()\n    # input sequence (t-n, ... t-1)\n    for i in range(n_in, 0, -1):\n        cols.append(df.shift(i))\n        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n    # forecast sequence (t, t+1, ... t+n)\n    for i in range(0, n_out):\n        cols.append(df.shift(-i))\n        if i == 0:\n            names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n        else:\n            names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n    # put it all together\n    agg = concat(cols, axis=1)\n    agg.columns = names\n    # drop rows with NaN values\n    if dropnan:\n        agg.dropna(inplace=True)\n    elif interpolate:\n        agg.interpolate(method='linear', inplace=True)\n    else:\n        agg.fillna(-999,inplace=True)\n    return agg\n ","metadata":{"execution":{"iopub.status.busy":"2021-11-17T17:24:15.343808Z","iopub.execute_input":"2021-11-17T17:24:15.344035Z","iopub.status.idle":"2021-11-17T17:24:15.3549Z","shell.execute_reply.started":"2021-11-17T17:24:15.344007Z","shell.execute_reply":"2021-11-17T17:24:15.354101Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Functions to get shadows","metadata":{}},{"cell_type":"code","source":"# Two new features from the competition tutorial\ndef upper_shadow(df):\n    return df['High'] - np.maximum(df['Close'], df['Open'])\n\ndef lower_shadow(df):\n    return np.minimum(df['Close'], df['Open']) - df['Low']\n","metadata":{"execution":{"iopub.status.busy":"2021-11-17T17:24:15.356079Z","iopub.execute_input":"2021-11-17T17:24:15.356331Z","iopub.status.idle":"2021-11-17T17:24:15.369374Z","shell.execute_reply.started":"2021-11-17T17:24:15.356268Z","shell.execute_reply":"2021-11-17T17:24:15.368638Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Simple Moving Average (SMA)","metadata":{}},{"cell_type":"code","source":"def SMA(df):\n    df['SMA_5'] = df.groupby('Asset_ID')['Close'].transform(lambda x: x.rolling(window = 5).mean())\n    df['SMA_15'] = df.groupby('Asset_ID')['Close'].transform(lambda x: x.rolling(window = 15).mean())\n    df['SMA_ratio'] = df['SMA_15'] / df['SMA_5']\n    return df","metadata":{"execution":{"iopub.status.busy":"2021-11-17T17:24:15.370338Z","iopub.execute_input":"2021-11-17T17:24:15.370548Z","iopub.status.idle":"2021-11-17T17:24:15.37997Z","shell.execute_reply.started":"2021-11-17T17:24:15.370521Z","shell.execute_reply":"2021-11-17T17:24:15.379053Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Wilderâ€™s Smoothing","metadata":{}},{"cell_type":"code","source":"def Wilder(data,period):\n    return data['Close'].ewm(alpha=1.0 / period,adjust=False,).mean()","metadata":{"execution":{"iopub.status.busy":"2021-11-17T17:24:15.381078Z","iopub.execute_input":"2021-11-17T17:24:15.381888Z","iopub.status.idle":"2021-11-17T17:24:15.390146Z","shell.execute_reply.started":"2021-11-17T17:24:15.381852Z","shell.execute_reply":"2021-11-17T17:24:15.389505Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## One stop get_features function","metadata":{}},{"cell_type":"code","source":"\n# A utility function to build features from the original df\n# It works for rows to, so we can reutilize it.\ndef get_features(df, lag = 1, shuffle = False, less_features = False, diff = True, log= True, sma = True, wilder = True):\n    df_feat = df[['Count', 'Open', 'High', 'Low', 'Close', 'Volume', 'VWAP']].copy()\n    \n    df_feat['minute_log_ret'] = np.log(df_feat.Close/df_feat.Open).fillna(-999)\n    # params\n    if less_features:\n        df_feat = df_feat[['Close', 'Volume','Count','minute_log_ret']]\n    if sma:\n        df_feat[['SMA_5','SMA_15', 'SMA_ratio']] = SMA(df)[['SMA_5','SMA_15', 'SMA_ratio']]\n    if wilder:\n        df_feat_wilder = Wilder(df, 15)\n    if log:\n        for column in df_feat.columns:\n            if column in ['Open', 'High', 'Low', 'Close', 'VWAP']:\n                df_feat[column] = np.log(df_feat[column])\n    if diff:\n        for column in df_feat.columns:\n            if column in ['Open', 'High', 'Low', 'Close', 'VWAP']:\n                df_feat[f'diff_{column}'] = df_feat[column].diff().fillna(-999)\n    # add lagged observations\n    if lag > 0:\n        df_feat = series_to_supervised(df_feat, n_in = lag)\n    df_feat['Upper_Shadow'] = upper_shadow(df)\n    df_feat['Lower_Shadow'] = lower_shadow(df)\n    if shuffle is True:\n        df_feat = df_feat.sample(frac=1)\n    \n    df_feat.replace([np.inf, -np.inf], np.nan)\n    df_feat.fillna(-999,inplace=True)\n    return df_feat","metadata":{"execution":{"iopub.status.busy":"2021-11-17T17:24:15.391741Z","iopub.execute_input":"2021-11-17T17:24:15.392093Z","iopub.status.idle":"2021-11-17T17:24:15.404734Z","shell.execute_reply.started":"2021-11-17T17:24:15.392053Z","shell.execute_reply":"2021-11-17T17:24:15.404175Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def calculate_target(data: pd.DataFrame,  price_column: str, asset_details = df_asset_details):\n    ids = list(asset_details.Asset_ID)\n    asset_names = list(asset_details.Asset_Name)\n    weights = np.array(list(asset_details.Weight))\n\n    times = data['timestamp'].agg(['min', 'max']).to_dict()\n    all_timestamps = np.arange(times['min'], times['max'] + 60, 60)\n    targets = pd.DataFrame(index=all_timestamps)\n\n    for i, id in enumerate(ids):\n        asset = data[data.Asset_ID == id].set_index(keys='timestamp')\n        price = pd.Series(index=all_timestamps, data=asset[price_column])\n        targets[asset_names[i]] = np.log(\n            price.shift(periods=-16) /\n            price.shift(periods=-1)\n        )\n    \n    targets['m'] = np.average(targets.fillna(0), axis=1, weights=weights)\n    \n    m = targets['m']\n\n    num = targets.multiply(m.values, axis=0).rolling(3750).mean().values\n    denom = m.multiply(m.values, axis=0).rolling(3750).mean().values\n    beta = np.nan_to_num(num.T / denom, nan=0., posinf=0., neginf=0.)\n\n    targets = targets - (beta * m.values).T\n    targets.drop('m', axis=1, inplace=True)\n    \n    return targets","metadata":{"execution":{"iopub.status.busy":"2021-11-17T17:24:15.406151Z","iopub.execute_input":"2021-11-17T17:24:15.406484Z","iopub.status.idle":"2021-11-17T17:24:15.4217Z","shell.execute_reply.started":"2021-11-17T17:24:15.406444Z","shell.execute_reply":"2021-11-17T17:24:15.42107Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_15_min_log_return(df: pd.DataFrame,  price_column: str, asset_details = df_asset_details):\n    data = df.copy()\n    ids = list(asset_details.Asset_ID)\n    asset_names = list(asset_details.Asset_Name)\n    weights = np.array(list(asset_details.Weight))\n    \n    times = data['timestamp'].agg(['min', 'max']).to_dict()\n    all_timestamps = np.arange(times['min'], times['max'] + 60, 60)\n#     targets = pd.DataFrame(index = all_timestamps)\n    \n\n    for i, id in enumerate(ids):\n        asset = data[data.Asset_ID == id].set_index(keys='timestamp')\n        price = pd.Series(data=asset[price_column])\n        targets = np.log(\n            price.shift(periods=-16) /\n            price.shift(periods=-1)\n        )\n#         display(targets)\n#         display( targets[asset_names[i]].head(20))\n#         display(data[data.Asset_ID == id].head(10))\n        data.loc[data.Asset_ID == id, '15_log_return'] = targets.fillna(-999).values\n    return data","metadata":{"execution":{"iopub.status.busy":"2021-11-17T17:24:15.423061Z","iopub.execute_input":"2021-11-17T17:24:15.42327Z","iopub.status.idle":"2021-11-17T17:24:15.437188Z","shell.execute_reply.started":"2021-11-17T17:24:15.423244Z","shell.execute_reply":"2021-11-17T17:24:15.436525Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# df = df_train[df_train[\"Asset_ID\"] == 10]\n# df.isna().sum()","metadata":{"execution":{"iopub.status.busy":"2021-11-17T17:24:15.439883Z","iopub.execute_input":"2021-11-17T17:24:15.440594Z","iopub.status.idle":"2021-11-17T17:24:15.451584Z","shell.execute_reply.started":"2021-11-17T17:24:15.440475Z","shell.execute_reply":"2021-11-17T17:24:15.45076Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# df = df_train[df_train[\"Asset_ID\"] == 0]\n# #impute y\n# df['Target'] = df.Target.interpolate(method='slinear')\n# df.isna().sum()","metadata":{"execution":{"iopub.status.busy":"2021-11-17T17:24:15.45269Z","iopub.execute_input":"2021-11-17T17:24:15.453059Z","iopub.status.idle":"2021-11-17T17:24:15.46166Z","shell.execute_reply.started":"2021-11-17T17:24:15.453031Z","shell.execute_reply":"2021-11-17T17:24:15.460946Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Check the model interface\n# x = get_features(df_train.iloc[1])\n# #y_pred = models[0].predict([x])\n# #y_pred[0]\n# y_pred = models[0].predict(pd.DataFrame([x]))\n# y_pred[0]","metadata":{"execution":{"iopub.status.busy":"2021-11-17T17:24:15.464484Z","iopub.execute_input":"2021-11-17T17:24:15.464842Z","iopub.status.idle":"2021-11-17T17:24:15.473322Z","shell.execute_reply.started":"2021-11-17T17:24:15.464812Z","shell.execute_reply":"2021-11-17T17:24:15.472675Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Cross Validation","metadata":{}},{"cell_type":"code","source":"import numpy as np\nfrom sklearn.model_selection import KFold\nfrom sklearn.utils.validation import _deprecate_positional_args\nfrom sklearn.model_selection._split import _BaseKFold, indexable, _num_samples\n\n# modified code for group gaps; source\n# https://github.com/getgaurav2/scikit-learn/blob/d4a3af5cc9da3a76f0266932644b884c99724c57/sklearn/model_selection/_split.py#L2243\nclass PurgedGroupTimeSeriesSplit(_BaseKFold):\n    \"\"\"Time Series cross-validator variant with non-overlapping groups.\n    Allows for a gap in groups to avoid potentially leaking info from\n    train into test if the model has windowed or lag features.\n    Provides train/test indices to split time series data samples\n    that are observed at fixed time intervals according to a\n    third-party provided group.\n    In each split, test indices must be higher than before, and thus shuffling\n    in cross validator is inappropriate.\n    This cross-validation object is a variation of :class:`KFold`.\n    In the kth split, it returns first k folds as train set and the\n    (k+1)th fold as test set.\n    The same group will not appear in two different folds (the number of\n    distinct groups has to be at least equal to the number of folds).\n    Note that unlike standard cross-validation methods, successive\n    training sets are supersets of those that come before them.\n    Read more in the :ref:`User Guide <cross_validation>`.\n    Parameters\n    ----------\n    n_splits : int, default=5\n        Number of splits. Must be at least 2.\n    max_train_group_size : int, default=Inf\n        Maximum group size for a single training set.\n    group_gap : int, default=None\n        Gap between train and test\n    max_test_group_size : int, default=Inf\n        We discard this number of groups from the end of each train split\n    \"\"\"\n\n    @_deprecate_positional_args\n    def __init__(self,\n                 n_splits=5,\n                 *,\n                 max_train_group_size=np.inf,\n                 max_test_group_size=np.inf,\n                 group_gap=None,\n                 verbose=False,\n                 train_gap = 0\n                 ):\n        super().__init__(n_splits, shuffle=False, random_state=None)\n        self.max_train_group_size = max_train_group_size\n        self.group_gap = group_gap\n        self.max_test_group_size = max_test_group_size\n        self.verbose = verbose\n        self.train_gap = train_gap\n\n    def split(self, X, y=None, groups=None):\n        \"\"\"Generate indices to split data into training and test set.\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Training data, where n_samples is the number of samples\n            and n_features is the number of features.\n        y : array-like of shape (n_samples,)\n            Always ignored, exists for compatibility.\n        groups : array-like of shape (n_samples,)\n            Group labels for the samples used while splitting the dataset into\n            train/test set.\n        Yields\n        ------\n        train : ndarray\n            The training set indices for that split.\n        test : ndarray\n            The testing set indices for that split.\n        \"\"\"\n        if groups is None:\n            raise ValueError(\n                \"The 'groups' parameter should not be None\")\n        X, y, groups = indexable(X, y, groups)\n        n_samples = _num_samples(X)\n        train_gap = self.train_gap\n        n_splits = self.n_splits\n        group_gap = self.group_gap\n        max_test_group_size = self.max_test_group_size\n        max_train_group_size = self.max_train_group_size\n        n_folds = n_splits + 1\n        group_dict = {}\n        u, ind = np.unique(groups, return_index=True)\n        unique_groups = u[np.argsort(ind)]\n        n_samples = _num_samples(X)\n        n_groups = _num_samples(unique_groups)\n        for idx in np.arange(n_samples):\n            if (groups[idx] in group_dict):\n                group_dict[groups[idx]].append(idx)\n            else:\n                group_dict[groups[idx]] = [idx]\n        if n_folds > n_groups:\n            raise ValueError(\n                (\"Cannot have number of folds={0} greater than\"\n                 \" the number of groups={1}\").format(n_folds,\n                                                     n_groups))\n\n        group_test_size = min(n_groups // n_folds, max_test_group_size)\n        group_test_starts = range(n_groups - n_splits * group_test_size - train_gap *n_splits,\n                                  n_groups, group_test_size + train_gap)\n        for group_test_start in group_test_starts:\n            train_array = []\n            test_array = []\n\n            group_st = max(0, group_test_start - group_gap - max_train_group_size)\n            for train_group_idx in unique_groups[group_st:(group_test_start - group_gap)]:\n                train_array_tmp = group_dict[train_group_idx]\n                \n                train_array = np.sort(np.unique(\n                                      np.concatenate((train_array,\n                                                      train_array_tmp)),\n                                      axis=None), axis=None)\n\n            train_end = train_array.size\n \n            for test_group_idx in unique_groups[group_test_start:\n                                                group_test_start +\n                                                group_test_size]:\n                test_array_tmp = group_dict[test_group_idx]\n                test_array = np.sort(np.unique(\n                                              np.concatenate((test_array,\n                                                              test_array_tmp)),\n                                     axis=None), axis=None)\n\n            test_array  = test_array[group_gap:]\n            \n            \n            if self.verbose > 0:\n                    pass\n                    \n            yield [int(i) for i in train_array], [int(i) for i in test_array]\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import ListedColormap\n\n# this is code slightly modified from the sklearn docs here:\n# https://scikit-learn.org/stable/auto_examples/model_selection/plot_cv_indices.html#sphx-glr-auto-examples-model-selection-plot-cv-indices-py\ndef plot_cv_indices(cv, X, y, group, ax, n_splits, lw=10):\n    \"\"\"Create a sample plot for indices of a cross-validation object.\"\"\"\n    \n    cmap_cv = plt.cm.coolwarm\n\n    jet = plt.cm.get_cmap('jet', 256)\n    seq = np.linspace(0, 1, 256)\n    _ = np.random.shuffle(seq)   # inplace\n    cmap_data = ListedColormap(jet(seq))\n\n    # Generate the training/testing visualizations for each CV split\n    for ii, (tr, tt) in enumerate(cv.split(X=X, y=y, groups=group)):\n        # Fill in indices with the training/test groups\n        indices = np.array([np.nan] * len(X))\n        indices[tt] = 1\n        indices[tr] = 0\n\n        # Visualize the results\n        ax.scatter(range(len(indices)), [ii + .5] * len(indices),\n                   c=indices, marker='_', lw=lw, cmap=cmap_cv,\n                   vmin=-.2, vmax=1.2)\n\n    # Plot the data classes and groups at the end\n    ax.scatter(range(len(X)), [ii + 1.5] * len(X),\n               c=y, marker='_', lw=lw, cmap=plt.cm.Set3)\n\n    ax.scatter(range(len(X)), [ii + 2.5] * len(X),\n               c=group, marker='_', lw=lw, cmap=cmap_data)\n\n    # Formatting\n    yticklabels = list(range(n_splits)) + ['target', 'day']\n    ax.set(yticks=np.arange(n_splits+2) + .5, yticklabels=yticklabels,\n           xlabel='Sample index', ylabel=\"CV iteration\",\n           ylim=[n_splits+2.2, -.2], xlim=[0, len(y)])\n    ax.set_title('{}'.format(type(cv).__name__), fontsize=15)\n    return ax","metadata":{"_kg_hide-input":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2021-11-17T17:24:15.474732Z","iopub.execute_input":"2021-11-17T17:24:15.475112Z","iopub.status.idle":"2021-11-17T17:24:15.506966Z","shell.execute_reply.started":"2021-11-17T17:24:15.475071Z","shell.execute_reply":"2021-11-17T17:24:15.50624Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# fig, ax = plt.subplots()\n\n# feature_names = [i for i in df_train.columns if i not in ['Target', 'date', 'timestamp', 'VWAP', 'Asset_ID', 'groups']]\n# cv = PurgedGroupTimeSeriesSplit(\n#     n_splits=5,\n#     max_train_group_size=200,\n#     group_gap=10,\n#     max_test_group_size=80,\n#     train_gap = 100\n# )\n\n# plot_cv_indices(\n#     cv,    \n#     df_train.loc[df_train[\"Asset_ID\"] == 1][feature_names].values,\n#     df_train.loc[df_train[\"Asset_ID\"] == 1]['Target'].values > np.nanmean(df_train.loc[df_train[\"Asset_ID\"] == 1]['Target'].values),\n#     df_train.loc[df_train[\"Asset_ID\"] == 1]['groups'].values,\n#     ax,\n#     5,\n#     lw=20\n# );","metadata":{"execution":{"iopub.status.busy":"2021-11-17T17:24:15.508078Z","iopub.execute_input":"2021-11-17T17:24:15.508795Z","iopub.status.idle":"2021-11-17T17:24:15.519235Z","shell.execute_reply.started":"2021-11-17T17:24:15.508758Z","shell.execute_reply":"2021-11-17T17:24:15.51861Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import optuna\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import roc_auc_score, mean_absolute_error\nfrom sklearn import set_config\n\n# get_features_params = {'lag':1,'less_features': True, 'shuffle':False, 'diff':True}\nfeature_names = [i for i in df_train.columns if i not in ['Target', 'date', 'timestamp' 'Asset_ID', 'groups']]\n# Testing with Asset_ID == 0 for now\nasset_df = get_15_min_log_return(df_train[df_train['Asset_ID'] == 1], 'Close')\n# y_labels = asset_df['Target'].values\ny_labels = asset_df['15_log_return'].values\n# X_train = asset_df[feature_names].values\n# X_train = get_features(asset_df[feature_names], **get_features_params).values\ngroups = asset_df['groups'].values\n\ncv = PurgedGroupTimeSeriesSplit(\n    n_splits=5,\n    max_train_group_size=200,\n    group_gap=20,\n    max_test_group_size=80,\n    train_gap = 100\n)\n\ndef objective(trial, cv=cv, cv_fold_func=np.average):\n    \n    # Optuna suggest params for feature engineering\n    feature_params = {\n        'lag':trial.suggest_int('lag', 0, 10),\n        'less_features': trial.suggest_int('less_features',0, 1), \n        'shuffle': trial.suggest_int('shuffle',0, 1), \n        'diff': trial.suggest_int('diff',0, 1),\n        'log': trial.suggest_int('log',0, 1),\n        'sma': trial.suggest_int('sma',0, 1),\n        'wilder': trial.suggest_int('wilder',0, 1),\n        }\n    X_train = get_features(asset_df[feature_names], **feature_params).values\n    # Optuna suggest params for model\n    params = {\n        'n_estimators': 550,\n        'reg_lambda': trial.suggest_int('reg_lambda', 0, 5),\n        \"min_child_weight\" : trial.suggest_int('min_child_weight', 1, 5),\n        'max_depth': trial.suggest_int('max_depth', 6, 15),\n        'learning_rate': trial.suggest_uniform('learning_rate', 0.005, 0.02),\n        'subsample': trial.suggest_uniform('subsample', 0.50, 0.90),\n        'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.40, 0.90),\n#         'gamma': trial.suggest_uniform('gamma', 0, 0.5),\n        'missing': -999,        \n        }\n    \n    if DEVICE == 'GPU': params['tree_method'] = 'gpu_hist'  \n    # setup the pipeline\n    imp_mean = SimpleImputer(missing_values=np.nan, strategy='mean')\n    scaler = StandardScaler()\n    clf = xgb.XGBRegressor(**params)\n\n    pipe = Pipeline(steps=[\n#         ('imputer', imp_mean),\n        ('scaler', scaler),\n        ('xgb', clf)\n    ])\n\n\n    # fit for all folds and return composite MAE score\n    maes = []\n    correlations = []\n    for i, (train_idx, valid_idx) in enumerate(cv.split(\n        X_train,\n        y_labels,\n        groups=groups)):\n        \n        train_data = X_train[train_idx, :], y_labels[train_idx]\n        valid_data = X_train[valid_idx, :], y_labels[valid_idx]\n        \n#         display(X_train[valid_idx, :])\n        pipe.fit(X_train[train_idx, :], y_labels[train_idx])\n        preds = pipe.predict(X_train[valid_idx, :])\n        mae = mean_absolute_error(y_labels[valid_idx], preds)\n        correlation = pearsonr(y_labels[valid_idx], preds)[0]\n#         print(preds)\n        maes.append(mae)\n        correlations.append(correlation)\n    \n    print(f'Trial done: mae values on folds: {maes}, correlation: {correlations}')\n#     return -1.0 * cv_fold_func(maes)\n    return cv_fold_func(correlations)\n","metadata":{"execution":{"iopub.status.busy":"2021-11-17T17:24:15.520967Z","iopub.execute_input":"2021-11-17T17:24:15.521263Z","iopub.status.idle":"2021-11-17T17:24:16.795796Z","shell.execute_reply.started":"2021-11-17T17:24:15.521223Z","shell.execute_reply":"2021-11-17T17:24:16.795103Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\nFIT_XGB = False\n\nn_trials = 50\n\nif FIT_XGB:\n    study = optuna.create_study(direction=\"maximize\")\n    study.optimize(objective, n_trials=n_trials)\n\n    print(\"Number of finished trials: {}\".format(len(study.trials)))\n\n    print(\"Best trial:\")\n    trial = study.best_trial\n\n    print(\"  Value: {}\".format(trial.value))\n\n    print(\"  Params: \")\n    for key, value in trial.params.items():\n        print(\"    {}: {}\".format(key, value))\n    best_params = trial.params        \nelse: best_params = {}","metadata":{"execution":{"iopub.status.busy":"2021-11-17T17:24:16.796893Z","iopub.execute_input":"2021-11-17T17:24:16.79709Z","iopub.status.idle":"2021-11-17T17:24:16.806562Z","shell.execute_reply.started":"2021-11-17T17:24:16.797065Z","shell.execute_reply":"2021-11-17T17:24:16.803257Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Best params recorded:","metadata":{}},{"cell_type":"code","source":"# best params so far for asset ID = 1, trained on 15_log_return, with more feature engineering\n    # Trial 49 finished with value: 0.0317581095907626 and parameters: {'lag': 7, 'less_features': 1, 'shuffle': 1, 'diff': 0, 'log': 0, 'sma': 1, 'wilder': 1, 'reg_lambda': 5, 'min_child_weight': 4, 'max_depth': 15, 'learning_rate': 0.009570687770657272, 'subsample': 0.6285779767805164, 'colsample_bytree': 0.7992539958305589}. Best is trial 22 with value: 0.04930682588394672.\n    # Trial done: mae values on folds: [0.0048964624, 0.003702112, 0.003005475, 0.0027706742, 0.0050811954], correlation: [0.011284554536045564, 0.021701535868510636, 0.019523880588945712, 0.040610525761315026, 0.06567005119899602]\n    # Number of finished trials: 50\n    # Best trial:\n    #   Value: 0.04930682588394672\n    #   Params: \n    #     lag: 9\n    #     less_features: 1\n    #     shuffle: 1\n    #     diff: 0\n    #     log: 0\n    #     sma: 0\n    #     wilder: 1\n    #     reg_lambda: 5\n    #     min_child_weight: 1\n    #     max_depth: 12\n    #     learning_rate: 0.006764463842691914\n    #     subsample: 0.7021426413202513\n    #     colsample_bytree: 0.7429624696167025\n    # CPU times: user 49min 43s, sys: 58.7 s, total: 50min 42s\n    # Wall time: 46min 26s\n# best params so far for asset ID = 1, trained on 15_log_return\n# Trial done: mae values on folds: [0.026105188, 0.026105512, 0.025961224, 0.026012246, 0.026011521], correlation: [0.030052164650627218, 0.029476802869421285, 0.009409405617049048, 0.07442854987788829, 0.057961765589653165]\n# Number of finished trials: 200\n# Best trial:\n#   Value: 0.04418723403026871\n#   Params: \n#     lag: 1\n#     less_features: 0\n#     shuffle: 0\n#     diff: 1\n#     log: 0\n#     n_estimators: 584\n#     reg_lambda: 5\n#     min_child_weight: 3\n#     max_depth: 12\n#     learning_rate: 0.005899619552160016\n#     subsample: 0.6830598708006316\n#     colsample_bytree: 0.8521852685777859\n# CPU times: user 3h 35min 51s, sys: 2min 28s, total: 3h 38min 19s\n# Wall time: 3h 18min 37s\n# best params so far for asset ID = 1\n#     Best trial:\n#       Value: 0.031607513275039106\n#       Params: \n#         lag: 5\n#         less_features: 1\n#         shuffle: 1\n#         diff: 1\n#         log: 0\n#         n_estimators: 503\n#         reg_lambda: 3\n#         min_child_weight: 5\n#         max_depth: 6\n#         learning_rate: 0.010950705188981748\n#         subsample: 0.6407943197975113\n#         colsample_bytree: 0.4936947696220707\n# best params so far for asset ID = 0\n#     Trial done: mae values on folds: [0.00884721001153564, 0.008764936314881898, 0.008990281840705116, 0.009167435202743312], correlation: [0.011463596891310724, 0.03417865196140347, 0.007146227215745203, 0.038132198250439626]\n#     Number of finished trials: 200\n#     Best trial:\n#       Value: 0.02448920005394209\n#       Params: \n#         lag: 2\n#         less_features: 0\n#         shuffle: 0\n#         diff: 1\n#         log: 1\n#         n_estimators: 531\n#         reg_lambda: 0\n#         min_child_weight: 1\n#         max_depth: 6\n#         learning_rate: 0.011160780508297672\n#         subsample: 0.7614506725442451\n#         colsample_bytree: 0.8630802285809316\n#     CPU times: user 2h 49min, sys: 3min 15s, total: 2h 52min 16s\n#     Wall time: 2h 32min 45s","metadata":{"execution":{"iopub.status.busy":"2021-11-17T17:24:16.809179Z","iopub.execute_input":"2021-11-17T17:24:16.809535Z","iopub.status.idle":"2021-11-17T17:24:16.818885Z","shell.execute_reply.started":"2021-11-17T17:24:16.809482Z","shell.execute_reply":"2021-11-17T17:24:16.817874Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# maes = []\n# correlations = []\n# for i, (train_idx, valid_idx) in enumerate(cv.split(\n#     X_train,\n#     y_labels,\n#     groups=groups)):\n\n#     train_data = X_train[train_idx, :], y_labels[train_idx]\n#     valid_data = X_train[valid_idx, :], y_labels[valid_idx]\n\n# #         display(X_train[valid_idx, :])\n#     pipe = XGB_pipeline()\n#     pipe.fit(X_train[train_idx, :], y_labels[train_idx])\n#     preds = pipe.predict(X_train[valid_idx, :])\n#     mae = mean_absolute_error(y_labels[valid_idx], preds)\n#     correlation = pearsonr(y_labels[valid_idx], preds)[0]\n# #         print(preds)\n#     maes.append(mae)\n#     correlations.append(correlation)\n# correlations","metadata":{"execution":{"iopub.status.busy":"2021-11-17T17:24:16.820192Z","iopub.execute_input":"2021-11-17T17:24:16.821025Z","iopub.status.idle":"2021-11-17T17:24:16.832811Z","shell.execute_reply.started":"2021-11-17T17:24:16.820978Z","shell.execute_reply":"2021-11-17T17:24:16.831799Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Building the model","metadata":{}},{"cell_type":"code","source":"# Build pipeline here, imputer might not be necessary as all nans were dropped in the beginning.\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import roc_auc_score, mean_absolute_error\nfrom sklearn import set_config\nset_config(display='diagram') \ndef XGB_pipeline():\n    imp_mean = SimpleImputer(missing_values=np.nan, strategy='mean')\n    scaler = StandardScaler()\n#     scaler = MinMaxScaler()\n\n    #parameters from optuna optimization\n    XGB_model = xgb.XGBRegressor(\n        n_estimators=531,\n#         learning_rate=0.05,\n#         max_depth=12,\n#         subsample=0.9,\n#         colsample_bytree=0.7,\n        #colsample_bylevel=0.75,\n        missing=-999,\n        min_child_weight= 3,\n        reg_lambda= 5,\n        max_depth= 12,\n        learning_rate = 0.006899619552160016,\n        subsample = 0.6830598708006316,\n        colsample_bytree = 0.8521852685777859,\n#         gamma = 9,\n#         reg_lambda = 5,\n        random_state=1111,\n        tree_method='gpu_hist'  \n        )\n\n    pipe = Pipeline(steps=[\n        ('imputer', imp_mean),\n        ('scaler', scaler),\n        ('linear', XGB_model)\n    ])\n    \n    return pipe","metadata":{"execution":{"iopub.status.busy":"2021-11-17T17:24:16.834742Z","iopub.execute_input":"2021-11-17T17:24:16.83549Z","iopub.status.idle":"2021-11-17T17:24:16.845696Z","shell.execute_reply.started":"2021-11-17T17:24:16.835442Z","shell.execute_reply":"2021-11-17T17:24:16.845109Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training","metadata":{}},{"cell_type":"code","source":"def get_Xy_and_model_for_asset(df_train, asset_id, \n                               lag = 1, \n                               shuffle = False, \n                               less_features = False, \n                               diff = True, \n                               log = True, \n                               train_on_log_return = False, \n                               sma = True, \n                               wilder = True\n                                  ):\n    df = df_train[df_train[\"Asset_ID\"] == asset_id]\n    #impute y, not working\n#     df['Target'] = df.Target.interpolate(method='slinear')\n    # TODO: Try different features here!\n    X = get_features(df, lag = lag, \n                     shuffle = shuffle, \n                     less_features = less_features, \n                     diff = diff, \n                     log = log,\n                     sma = sma, \n                     wilder = wilder).values\n    if train_on_log_return:\n        y = get_15_min_log_return(df, 'Close')['15_log_return'].values\n    else:\n        y = df['Target'].values\n    #df_proc = df_proc.dropna(how=\"any\")\n    \n    # TODO: Try different models here!\n    #model = LGBMRegressor(random_state=1111, n_estimators=1200)\n    #model.fit(X, y)\n    #return X, y, model\n    \n    model = XGB_pipeline()\n    \n    model.fit(X, y)\n    return model","metadata":{"execution":{"iopub.status.busy":"2021-11-17T17:24:16.847128Z","iopub.execute_input":"2021-11-17T17:24:16.847653Z","iopub.status.idle":"2021-11-17T17:24:16.864832Z","shell.execute_reply.started":"2021-11-17T17:24:16.847607Z","shell.execute_reply":"2021-11-17T17:24:16.862131Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Loop over all assets","metadata":{}},{"cell_type":"code","source":"# asset_df = df_train[df_train['Asset_ID'] == 1]\n# X_train = asset_df\n# y_labels = asset_df['Target'].values\n# cv = PurgedGroupTimeSeriesSplit(\n#     n_splits=5,\n#     max_train_group_size=200,\n#     group_gap=10,\n#     max_test_group_size=80,\n#     train_gap = 100\n# )\n# for i, (train_idx, valid_idx) in enumerate(cv.split(\n#     asset_df,\n#     y_labels,\n#     groups=groups)):\n\n#     train_data = asset_df.iloc[train_idx, :]\n#     valid_data = asset_df.iloc[valid_idx, :]\n    \n#     break","metadata":{"execution":{"iopub.status.busy":"2021-11-17T17:24:16.866156Z","iopub.execute_input":"2021-11-17T17:24:16.866475Z","iopub.status.idle":"2021-11-17T17:24:16.881643Z","shell.execute_reply.started":"2021-11-17T17:24:16.866432Z","shell.execute_reply":"2021-11-17T17:24:16.880514Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# training models, change params with the 'params' variable.\nparams = {'lag':9,'less_features': 1, 'shuffle':1, 'diff':0, 'log':0, 'sma': 0, 'wilder':1}\n\nmodels = {}\n\nfor asset_id, asset_name in zip(df_asset_details['Asset_ID'], df_asset_details['Asset_Name']):\n    print(f\"Training model for {asset_name:<16} (ID={asset_id:<2})\")\n    try:\n        model = get_Xy_and_model_for_asset(df_train, asset_id, train_on_log_return = 1, **params)  \n        models[asset_id] = model\n    except KeyboardInterrupt:\n        break\n    except: \n        traceback.print_exc()\n        models[asset_id] = None ","metadata":{"execution":{"iopub.status.busy":"2021-11-17T17:24:16.882988Z","iopub.execute_input":"2021-11-17T17:24:16.883697Z","iopub.status.idle":"2021-11-17T17:25:22.833459Z","shell.execute_reply.started":"2021-11-17T17:24:16.883649Z","shell.execute_reply":"2021-11-17T17:25:22.832461Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Testing","metadata":{}},{"cell_type":"code","source":"# testing the model, prints out correlation every 1000 iterations.\nfrom tqdm import tqdm\n\ntrained_on_15_log_return = False\n\nparams['shuffle'] = False\ndf_test = df[df['Asset_ID'] == 1].copy()\n# y_test = df_test[df_test['timestamp'] > 1623542400].Target\nX_test = df_test[df_test['timestamp'] > 1623542400].iloc[:-1]\nif trained_on_15_log_return:\n    # 15_log_return as target\n    y_test = get_15_min_log_return(df_test[df_test['timestamp'] > 1623542400], 'Close')['15_log_return'].values\nelse:\n    y_test = df_test[df_test['timestamp'] > 1623542400].Target\nall_time_stamps = df_test['timestamp'].drop_duplicates()\n\n## to replicate cross validation results\n# df_test = valid_data\n# y_test = df_test.Target\n# X_test = df_test.iloc[:-1]\n\n\ndict_tmp = {}\ncorrelations = []\npred = []\ncounter = 1\nwallet = 1400 # 100 per asset, for profit testing\nlong = {}\nbuy_price = {}\n# m = 0 # for calculating target\n\n\nfor j , row in tqdm(X_test.iterrows()):\n    counter += 1\n    if models[row['Asset_ID']] is not None:\n        try:\n            model = models[row['Asset_ID']]\n            # putting test data in the correct format, not optimized\n            if row['Asset_ID'] not in dict_tmp:\n                dict_tmp[row['Asset_ID']] = pd.DataFrame()\n            dict_tmp[row['Asset_ID']] = dict_tmp[row['Asset_ID']].append(row)\n            if len(dict_tmp[row['Asset_ID']]) > 10:\n                dict_tmp[row['Asset_ID']] = dict_tmp[row['Asset_ID']].tail(10)\n            x_test = get_features(dict_tmp[row['Asset_ID']],**params).tail(1)\n            if len(x_test) < 1 :\n                pred.append(0)\n                continue\n#             y_pred = model.predict(x_test)[0]\n            y_pred = model.predict(x_test.values)[0] # convert to nparray\n            # testing profit\n            if row['Asset_ID'] not in long:\n                long[row['Asset_ID']] = 0\n            if y_pred > 0 and long[row['Asset_ID']] == 0:\n                #buy\n                long[row['Asset_ID']] = 1\n                buy_price[row['Asset_ID']] = dict_tmp[row['Asset_ID']].tail(1).Close.values[0]\n            if y_pred < 0 and long[row['Asset_ID']] ==1:\n                #sell\n                long[row['Asset_ID']] = 0\n                sell_price = dict_tmp[row['Asset_ID']].tail(1).Close.values[0]\n                wallet += 100 * sell_price/buy_price[row['Asset_ID']] - 100\n                \n#             if trained_on_15_log_return:\n#                 targets['m'] = np.average(targets.fillna(0), axis=1, weights=weights)\n#                 m = targets['m']\n#                 num = targets.multiply(m.values, axis=0).rolling(3750).mean().values\n#                 denom = m.multiply(m.values, axis=0).rolling(3750).mean().values\n#                 beta = np.nan_to_num(num.T / denom, nan=0., posinf=0., neginf=0.)\n#                 targets = targets - (beta * m.values).T\n            pred.append(y_pred)\n        except KeyboardInterrupt:\n            break\n        except:\n            pred.append(0)\n            traceback.print_exc()\n    else: \n#         print('no model found')\n        pred.append(0)\n    if counter%1000 == 0:\n        correlations.append(pearsonr(y_test[:len(pred)], pred)[0])\n        print(f'Correlation: {pearsonr(y_test[:len(pred)], pred)[0]}, p-value = {pearsonr(y_test[:len(pred)], pred)[1]}, wallet = {wallet}')\n    if counter%20000 == 0:\n        for asset_id in range(0,13):\n            try:\n                if long[asset_id] ==1:\n                    sell_price = dict_tmp[asset_id].tail(1).Close.values[0]\n                    wallet += 100 * sell_price/buy_price[asset_id] - 100\n            except: \n                continue\n        print(f'Final wallet: {wallet}')\n        break\n","metadata":{"execution":{"iopub.status.busy":"2021-11-17T17:25:22.834998Z","iopub.execute_input":"2021-11-17T17:25:22.8353Z","iopub.status.idle":"2021-11-17T17:25:24.403763Z","shell.execute_reply.started":"2021-11-17T17:25:22.835258Z","shell.execute_reply":"2021-11-17T17:25:24.402976Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#check if things are working properly,the columns should be correct, and preds shouldn't be too small \nprint(x_test)\npred[:10]","metadata":{"execution":{"iopub.status.busy":"2021-11-17T17:25:24.404895Z","iopub.execute_input":"2021-11-17T17:25:24.405126Z","iopub.status.idle":"2021-11-17T17:25:24.585738Z","shell.execute_reply.started":"2021-11-17T17:25:24.40509Z","shell.execute_reply":"2021-11-17T17:25:24.584515Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# This assumes that we have minute-by-minute data for each asset","metadata":{"execution":{"iopub.status.busy":"2021-11-17T17:25:24.586654Z","iopub.status.idle":"2021-11-17T17:25:24.587238Z","shell.execute_reply.started":"2021-11-17T17:25:24.587045Z","shell.execute_reply":"2021-11-17T17:25:24.587066Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## these numbers are all an artifact of that fact that predictions are all very close to 0.\n\n# fixed lag, new XBGoost params:\n    #params = {'lag':5,'less_features': False, 'shuffle':True} test_size = 10000, 0.0051152419695546245 (peak = 0.068)\n    #params = {'lag':0,'less_features': True, 'shuffle':True, 'diff': False} test_size = 10000, 0.0051361107196150665 (peak = 0.06)\n    #params = {'lag':0,'less_features': True, 'shuffle':True } test_size = 10000, 0.004907722808215038 (peak = 0.06)\n    #params = {'lag':0,'less_features': True, 'shuffle':False } test_size = 10000, 0.004716585631229205 (peak = 0.06)\n\n# previous code didn't run correctly, get_features(training) returns all nans except for the two shadow features.\n\n# with scaling:\n    # params = {'lag':10,'less_features': True, 'shuffle':False }, test_size = 10000, 0.03236088315425213\n    \n#with diff, no scaling & model = xgb.XGBRegressor(\n#         n_estimators=500 (i think),\n#         missing=-999,\n#         random_state=1111,\n#         tree_method='gpu_hist'  \n#         ):\n    # params = {'lag':5,'less_features': True, 'shuffle':False }, test_size = 10000, <0\n    # params = {'lag':8,'less_features': True, 'shuffle':False }, test_size = 10000, <0\n    # params = {'lag':10,'less_features': True, 'shuffle':False }, test_size = 10000, 0.059430976177124534\n    # params = {'lag':10,'less_features': True, 'shuffle':True }, test_size = 10000, 0.007976245597496146\n    # params = {'lag':20,'less_features': True, 'shuffle':True }, test_size = 10000, <0\n\n    # with lagged features, lag = 3, test_size = 6000, shuffled, correlation for all assets: -0.025720808709807898\n\n#with n_estimators = 1000:\n\n    # params = {'lag':10,'less_features': True, 'shuffle':False }, test_size = 10000, 0.038023285612531356 (but high variance)\n\n# all shit\n    # with lagged features, lag = 3, test_size = 4000, correlation for all assets: -0.025720808709807898\n    # with lagged features, lag = 3, test_size = 5000, correlation for all assets: 0.010013542572384845\n    # with lagged features, lag = 3, test_size = 6000, correlation for all assets: 0.005208812545830533\n    # with lagged features, lag = 1, test_size = 6000, correlation for asset_ID = 1: 0.012588759721880852\n    # with lagged features,lag = 1, test_size = 6000, correlation for asset_ID = 0: -0.006493023259016858\n    # with lagged features, lag = 3, test_size = 6000, correlation for asset_ID = 0: 0.025999920635118752\n    # without lagged features, test_size = 6000, correlation for asset_ID = 1: <0\n    # without lagged features, test_size = 6000, correlation for asset_ID = 0: <0","metadata":{"execution":{"iopub.status.busy":"2021-11-17T17:25:24.588717Z","iopub.status.idle":"2021-11-17T17:25:24.589153Z","shell.execute_reply.started":"2021-11-17T17:25:24.588915Z","shell.execute_reply":"2021-11-17T17:25:24.588938Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Predict & submit\n\nReferences: [Detailed API Introduction](https://www.kaggle.com/sohier/detailed-api-introduction)\n\nSomething that helped me understand this iterator was adding a pdb checkpoint inside of the for loop:\n\n```python\nimport pdb; pdb.set_trace()\n```\n\nSee [Python Debugging With Pdb](https://realpython.com/python-debugging-pdb/) if you want to use it and you don't know how to.\n","metadata":{"execution":{"iopub.status.busy":"2021-11-02T20:57:49.349459Z","iopub.status.idle":"2021-11-02T20:57:49.349757Z","shell.execute_reply.started":"2021-11-02T20:57:49.349596Z","shell.execute_reply":"2021-11-02T20:57:49.349613Z"}}},{"cell_type":"code","source":"# env = gresearch_crypto.make_env()\n# iter_test = env.iter_test()\n\n# for i, (df_test, df_pred) in enumerate(iter_test):\n#     display(df_test)\n#     for j , row in df_test.iterrows():\n        \n#         if models[row['Asset_ID']] is not None:\n#             try:\n#                 model = models[row['Asset_ID']]\n#                 x_test = get_features(row)\n#                 y_pred = model.predict(pd.DataFrame([x_test]))[0]\n#                 df_pred.loc[df_pred['row_id'] == row['row_id'], 'Target'] = y_pred\n#             except:\n#                 df_pred.loc[df_pred['row_id'] == row['row_id'], 'Target'] = 0\n#                 traceback.print_exc()\n#         else: \n#             df_pred.loc[df_pred['row_id'] == row['row_id'], 'Target'] = 0\n        \n#     env.predict(df_pred)\n\n\n","metadata":{"execution":{"iopub.status.busy":"2021-11-17T17:25:24.59062Z","iopub.status.idle":"2021-11-17T17:25:24.591026Z","shell.execute_reply.started":"2021-11-17T17:25:24.590798Z","shell.execute_reply":"2021-11-17T17:25:24.590819Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}