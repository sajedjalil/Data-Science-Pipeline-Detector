{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport gc\nimport traceback\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport gresearch_crypto\nimport tensorflow as tf\nfrom tensorflow import keras\nimport matplotlib.pyplot as plt\nimport pandas as pd, numpy as np\nfrom tensorflow.keras import layers\nimport tensorflow_probability as tfp\nimport tensorflow.keras.backend as K\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import RobustScaler\nfrom tensorflow.keras.preprocessing.sequence import TimeseriesGenerator\npd.set_option('display.max_columns', None)","metadata":{"papermill":{"duration":8.500901,"end_time":"2021-11-23T06:55:18.133632","exception":false,"start_time":"2021-11-23T06:55:09.632731","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-12-09T14:32:48.153888Z","iopub.execute_input":"2021-12-09T14:32:48.154659Z","iopub.status.idle":"2021-12-09T14:32:57.705427Z","shell.execute_reply.started":"2021-12-09T14:32:48.154537Z","shell.execute_reply":"2021-12-09T14:32:57.704396Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"DEVICE = \"GPU\" #or \"TPU\"\n\nSEED = 42\n\n# LOAD STRICT? YES=1 NO=0 | see: https://www.kaggle.com/julian3833/proposal-for-a-meaningful-lb-strict-lgbm\nLOAD_STRICT = True\n\n# WHICH YEARS TO INCLUDE? YES=1 NO=0\nINC2021 = 0\nINC2020 = 0\nINC2019 = 0\nINC2018 = 0\nINC2017 = 0\nINCCOMP = 1\nINCSUPP = 0\n\nEPOCHS = 4\nDEBUG = True\nN_ASSETS = 14\nWINDOW_SIZE = 15\nBATCH_SIZE = 1024\nPCT_VALIDATION = 10 # last 10% of the data are used as validation set","metadata":{"papermill":{"duration":0.101119,"end_time":"2021-11-23T06:55:18.508185","exception":false,"start_time":"2021-11-23T06:55:18.407066","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-12-09T14:32:57.707681Z","iopub.execute_input":"2021-12-09T14:32:57.708025Z","iopub.status.idle":"2021-12-09T14:32:57.71561Z","shell.execute_reply.started":"2021-12-09T14:32:57.707978Z","shell.execute_reply":"2021-12-09T14:32:57.714561Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if DEVICE == \"TPU\":\n    print(\"connecting to TPU...\")\n    try:\n        tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n        print('Running on TPU ', tpu.master())\n    except ValueError:\n        tpu = None\n    if tpu:\n        try:\n            print(\"initializing  TPU ...\")\n            tf.config.experimental_connect_to_cluster(tpu)\n            tf.tpu.experimental.initialize_tpu_system(tpu)\n            strategy = tf.distribute.experimental.TPUStrategy(tpu)\n            print(\"TPU initialized\")\n        except: print(\"failed to initialize TPU\")\n    else: DEVICE = \"GPU\"\n\nif DEVICE != \"TPU\": strategy = tf.distribute.get_strategy()\nif DEVICE == \"GPU\": print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\nAUTO     = tf.data.experimental.AUTOTUNE\nREPLICAS = strategy.num_replicas_in_sync","metadata":{"_kg_hide-input":true,"papermill":{"duration":0.120468,"end_time":"2021-11-23T06:55:18.720411","exception":false,"start_time":"2021-11-23T06:55:18.599943","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-12-09T14:32:57.717599Z","iopub.execute_input":"2021-12-09T14:32:57.71809Z","iopub.status.idle":"2021-12-09T14:32:57.75796Z","shell.execute_reply.started":"2021-12-09T14:32:57.71804Z","shell.execute_reply":"2021-12-09T14:32:57.756955Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import datatable as dt\nextra_data_files = {0: '../input/cryptocurrency-extra-data-binance-coin', 2: '../input/cryptocurrency-extra-data-bitcoin-cash', 1: '../input/cryptocurrency-extra-data-bitcoin', 3: '../input/cryptocurrency-extra-data-cardano', 4: '../input/cryptocurrency-extra-data-dogecoin', 5: '../input/cryptocurrency-extra-data-eos-io', 6: '../input/cryptocurrency-extra-data-ethereum', 7: '../input/cryptocurrency-extra-data-ethereum-classic', 8: '../input/cryptocurrency-extra-data-iota', 9: '../input/cryptocurrency-extra-data-litecoin', 11: '../input/cryptocurrency-extra-data-monero', 10: '../input/cryptocurrency-extra-data-maker', 12: '../input/cryptocurrency-extra-data-stellar', 13: '../input/cryptocurrency-extra-data-tron'}\n\n# Uncomment to load the original csv [slower]\n# orig_df_train = pd.read_csv(data_path + 'train.csv') \n# supp_df_train = pd.read_csv(data_path + 'supplemental_train.csv')\n# df_asset_details = pd.read_csv(data_path  + 'asset_details.csv').sort_values(\"Asset_ID\")\n\norig_df_train = dt.fread('../input/cryptocurrency-extra-data-binance-coin/orig_train.jay').to_pandas()\ndf_asset_details = dt.fread('../input/cryptocurrency-extra-data-binance-coin/orig_asset_details.jay').to_pandas()\nsupp_df_train = dt.fread('../input/cryptocurrency-extra-data-binance-coin/orig_supplemental_train.jay').to_pandas()\nassets_details = dt.fread('../input/cryptocurrency-extra-data-binance-coin/orig_asset_details.jay').to_pandas()\nasset_weight_dict = {assets_details['Asset_ID'].tolist()[idx]: assets_details['Weight'].tolist()[idx] for idx in range(len(assets_details))}\nasset_name_dict = {assets_details['Asset_ID'].tolist()[idx]: assets_details['Asset_Name'].tolist()[idx] for idx in range(len(assets_details))}\n\ndef load_training_data_for_asset(asset_id, load_jay = True):\n    dfs = []\n    if INCCOMP: dfs.append(orig_df_train[orig_df_train[\"Asset_ID\"] == asset_id].copy())\n    if INCSUPP: dfs.append(supp_df_train[supp_df_train[\"Asset_ID\"] == asset_id].copy())\n    \n    if load_jay:\n        if INC2017 and os.path.exists(extra_data_files[asset_id] + '/full_data__' + str(asset_id) + '__' + str(2017) + '.csv'): dfs.append(dt.fread(extra_data_files[asset_id] + '/full_data__' + str(asset_id) + '__' + str(2017) + '.jay').to_pandas())\n        if INC2018 and os.path.exists(extra_data_files[asset_id] + '/full_data__' + str(asset_id) + '__' + str(2018) + '.csv'): dfs.append(dt.fread(extra_data_files[asset_id] + '/full_data__' + str(asset_id) + '__' + str(2018) + '.jay').to_pandas())\n        if INC2019 and os.path.exists(extra_data_files[asset_id] + '/full_data__' + str(asset_id) + '__' + str(2019) + '.csv'): dfs.append(dt.fread(extra_data_files[asset_id] + '/full_data__' + str(asset_id) + '__' + str(2019) + '.jay').to_pandas())\n        if INC2020 and os.path.exists(extra_data_files[asset_id] + '/full_data__' + str(asset_id) + '__' + str(2020) + '.csv'): dfs.append(dt.fread(extra_data_files[asset_id] + '/full_data__' + str(asset_id) + '__' + str(2020) + '.jay').to_pandas())\n        if INC2021 and os.path.exists(extra_data_files[asset_id] + '/full_data__' + str(asset_id) + '__' + str(2021) + '.csv'): dfs.append(dt.fread(extra_data_files[asset_id] + '/full_data__' + str(asset_id) + '__' + str(2021) + '.jay').to_pandas())\n    else: \n        if INC2017 and os.path.exists(extra_data_files[asset_id] + '/full_data__' + str(asset_id) + '__' + str(2017) + '.csv'): dfs.append(pd.read_csv(extra_data_files[asset_id] + '/full_data__' + str(asset_id) + '__' + str(2017) + '.csv'))\n        if INC2018 and os.path.exists(extra_data_files[asset_id] + '/full_data__' + str(asset_id) + '__' + str(2018) + '.csv'): dfs.append(pd.read_csv(extra_data_files[asset_id] + '/full_data__' + str(asset_id) + '__' + str(2018) + '.csv'))\n        if INC2019 and os.path.exists(extra_data_files[asset_id] + '/full_data__' + str(asset_id) + '__' + str(2019) + '.csv'): dfs.append(pd.read_csv(extra_data_files[asset_id] + '/full_data__' + str(asset_id) + '__' + str(2019) + '.csv'))\n        if INC2020 and os.path.exists(extra_data_files[asset_id] + '/full_data__' + str(asset_id) + '__' + str(2020) + '.csv'): dfs.append(pd.read_csv(extra_data_files[asset_id] + '/full_data__' + str(asset_id) + '__' + str(2020) + '.csv'))\n        if INC2021 and os.path.exists(extra_data_files[asset_id] + '/full_data__' + str(asset_id) + '__' + str(2021) + '.csv'): dfs.append(pd.read_csv(extra_data_files[asset_id] + '/full_data__' + str(asset_id) + '__' + str(2021) + '.csv'))\n    df = pd.concat(dfs, axis = 0) if len(dfs) > 1 else dfs[0]\n    df['date'] = pd.to_datetime(df['timestamp'], unit = 's')\n    if LOAD_STRICT: df = df.loc[df['date'] < \"2021-06-13 00:00:00\"]    \n    df = df.sort_values('date')\n    return df\n\ndef load_data_for_all_assets():\n    dfs = []\n    for asset_id in list(extra_data_files.keys()): dfs.append(load_training_data_for_asset(asset_id))\n    return pd.concat(dfs)","metadata":{"_kg_hide-input":true,"papermill":{"duration":1.560744,"end_time":"2021-11-23T06:55:20.556136","exception":false,"start_time":"2021-11-23T06:55:18.995392","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-12-09T14:32:57.760412Z","iopub.execute_input":"2021-12-09T14:32:57.760699Z","iopub.status.idle":"2021-12-09T14:33:23.755421Z","shell.execute_reply.started":"2021-12-09T14:32:57.760665Z","shell.execute_reply":"2021-12-09T14:33:23.754458Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = load_data_for_all_assets().sort_values('timestamp').set_index(\"timestamp\")\nif DEBUG: train = train[10000000:]\n\ntest = dt.fread('../input/cryptocurrency-extra-data-binance-coin/orig_example_test.jay').to_pandas()\nsample_prediction_df = dt.fread('../input/cryptocurrency-extra-data-binance-coin/orig_example_sample_submission.jay').to_pandas()\nassets = dt.fread('../input/cryptocurrency-extra-data-binance-coin/orig_asset_details.jay').to_pandas()\nassets_order = dt.fread('../input/cryptocurrency-extra-data-binance-coin/orig_supplemental_train.jay').to_pandas().Asset_ID[:N_ASSETS]\nassets_order = dict((t,i) for i,t in enumerate(assets_order))\nprint(\"Loaded all data!\")","metadata":{"papermill":{"duration":12.146866,"end_time":"2021-11-23T06:55:32.801779","exception":false,"start_time":"2021-11-23T06:55:20.654913","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-12-09T14:33:23.758677Z","iopub.execute_input":"2021-12-09T14:33:23.759063Z","iopub.status.idle":"2021-12-09T14:33:40.541034Z","shell.execute_reply.started":"2021-12-09T14:33:23.759014Z","shell.execute_reply":"2021-12-09T14:33:40.540089Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Memory saving function credit to https://www.kaggle.com/gemartin/load-data-reduce-memory-usage\ndef reduce_mem_usage(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.\n    \"\"\"\n    start_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n\n    for col in df.columns:\n        col_type = df[col].dtype.name\n        \n        if col_type not in ['object', 'category', 'datetime64[ns, UTC]', 'datetime64[ns]']:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n\n    end_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n\n    return df\n","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"papermill":{"duration":0.11239,"end_time":"2021-11-23T06:55:33.191508","exception":false,"start_time":"2021-11-23T06:55:33.079118","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-12-09T14:33:40.543046Z","iopub.execute_input":"2021-12-09T14:33:40.543618Z","iopub.status.idle":"2021-12-09T14:33:40.560816Z","shell.execute_reply.started":"2021-12-09T14:33:40.543568Z","shell.execute_reply":"2021-12-09T14:33:40.559905Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from scipy.stats import pearsonr\nfrom sklearn.model_selection import train_test_split\n\ndef upper_shadow(df): return df['High'] - np.maximum(df['Close'], df['Open'])\ndef lower_shadow(df): return np.minimum(df['Close'], df['Open']) - df['Low']\n\ndef get_features(df, row = False):\n    df_feat = df\n    df_feat['spread'] = df_feat['High'] - df_feat['Low']\n    df_feat['mean_trade'] = df_feat['Volume']/df_feat['Count']\n    df_feat['log_price_change'] = np.log(df_feat['Close']/df_feat['Open'])\n    df_feat['upper_Shadow'] = upper_shadow(df_feat)\n    df_feat['lower_Shadow'] = lower_shadow(df_feat)\n    df_feat[\"high_div_low\"] = df_feat[\"High\"] / df_feat[\"Low\"]\n    df_feat['trade'] = df_feat['Close'] - df_feat['Open']\n    df_feat['gtrade'] = df_feat['trade'] / df_feat['Count']\n    df_feat['shadow1'] = df_feat['trade'] / df_feat['Volume']\n    df_feat['shadow3'] = df_feat['upper_Shadow'] / df_feat['Volume']\n    df_feat['shadow5'] = df_feat['lower_Shadow'] / df_feat['Volume']\n    df_feat['diff1'] = df_feat['Volume'] - df_feat['Count']\n    df_feat['mean1'] = (df_feat['shadow5'] + df_feat['shadow3']) / 2\n    df_feat['mean2'] = (df_feat['shadow1'] + df_feat['Volume']) / 2\n    df_feat['mean3'] = (df_feat['trade'] + df_feat['gtrade']) / 2\n    df_feat['mean4'] = (df_feat['diff1'] + df_feat['upper_Shadow']) / 2\n    df_feat['mean5'] = (df_feat['diff1'] + df_feat['lower_Shadow']) / 2\n    df_feat['UPS'] = (df_feat['High'] - np.maximum(df_feat['Close'], df_feat['Open']))\n    df_feat['UPS'] = df_feat['UPS']\n    df_feat['LOS'] = (np.minimum(df_feat['Close'], df_feat['Open']) - df_feat['Low'])\n    df_feat['LOS'] = df_feat['LOS']\n    df_feat['RNG'] = ((df_feat['High'] - df_feat['Low']) / df_feat['VWAP'])\n    df_feat['RNG'] = df_feat['RNG']\n    df_feat['MOV'] = ((df_feat['Close'] - df_feat['Open']) / df_feat['VWAP'])\n    df_feat['MOV'] = df_feat['MOV']\n    df_feat['CLS'] = ((df_feat['Close'] - df_feat['VWAP']) / df_feat['VWAP'])\n    df_feat['CLS'] = df_feat['CLS']\n    df_feat['LOGVOL'] = np.log(1. + df_feat['Volume'])\n    df_feat['LOGVOL'] = df_feat['LOGVOL']\n    df_feat['LOGCNT'] = np.log(1. + df_feat['Count'])\n    df_feat['LOGCNT'] = df_feat['LOGCNT']\n    df_feat[\"Close/Open\"] = df_feat[\"Close\"] / df_feat[\"Open\"]\n    df_feat[\"Close-Open\"] = df_feat[\"Close\"] - df_feat[\"Open\"]\n    df_feat[\"High-Low\"] = df_feat[\"High\"] - df_feat[\"Low\"]\n    df_feat[\"High/Low\"] = df_feat[\"High\"] / df_feat[\"Low\"]\n    if row: df_feat['Mean'] = df_feat[['Open', 'High', 'Low', 'Close']].mean()\n    else: df_feat['Mean'] = df_feat[['Open', 'High', 'Low', 'Close']].mean(axis = 1)\n    df_feat[\"High/Mean\"] = df_feat[\"High\"] / df_feat[\"Mean\"]\n    df_feat[\"Low/Mean\"] = df_feat[\"Low\"] / df_feat[\"Mean\"]\n    df_feat[\"Volume/Count\"] = df_feat[\"Volume\"] / (df_feat[\"Count\"] + 1)\n    mean_price = df_feat[['Open', 'High', 'Low', 'Close']].mean(axis=1)\n    median_price = df_feat[['Open', 'High', 'Low', 'Close']].median(axis=1)\n    df_feat['high2mean'] = df_feat['High'] / mean_price\n    df_feat['low2mean'] = df_feat['Low'] / mean_price\n    df_feat['high2median'] = df_feat['High'] / median_price\n    df_feat['low2median'] = df_feat['Low'] / median_price\n    df_feat['volume2count'] = df_feat['Volume'] / (df_feat['Count'] + 1)\n    return df_feat\n\ntrain[['Count', 'Open', 'High', 'Low', 'Close', 'Volume', 'VWAP', 'Target']] = train[['Count', 'Open', 'High', 'Low', 'Close', 'Volume', 'VWAP', 'Target']].astype(np.float32)\nprint(train.shape)\ntrain['Target'] = train['Target'].fillna(0)\nVWAP_max = np.max(train[np.isfinite(train.VWAP)].VWAP)\nVWAP_min = np.min(train[np.isfinite(train.VWAP)].VWAP)\ntrain['VWAP'] = np.nan_to_num(train.VWAP, posinf=VWAP_max, neginf=VWAP_min)\ndf = train[['Asset_ID', 'Target']].copy()\ntimes = dict((t,i) for i,t in enumerate(df.index.unique()))\ndf['id'] = df.index.map(times)\ndf['id'] = df['id'].astype(str) + '_' + df['Asset_ID'].astype(str)\nids = df.id.copy()\ndel df\ntrain = get_features(train)\ntrain_features = [i for i in train.columns if i not in ['Target', 'date', 'timestamp', 'Asset_ID', 'groups']]","metadata":{"papermill":{"duration":48.227644,"end_time":"2021-11-23T06:56:21.510706","exception":false,"start_time":"2021-11-23T06:55:33.283062","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-12-09T14:33:40.562323Z","iopub.execute_input":"2021-12-09T14:33:40.562609Z","iopub.status.idle":"2021-12-09T14:34:33.234292Z","shell.execute_reply.started":"2021-12-09T14:33:40.562575Z","shell.execute_reply":"2021-12-09T14:34:33.233183Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Load and prepare the data","metadata":{"papermill":{"duration":0.09303,"end_time":"2021-11-23T06:56:21.880648","exception":false,"start_time":"2021-11-23T06:56:21.787618","status":"completed"},"tags":[]}},{"cell_type":"code","source":"train = train.sort_index()\nind = train.index.unique()\ndef reindex(df):\n    df = df.reindex(range(ind[0],ind[-1]+60,60),method='nearest')\n    df = df.fillna(method=\"ffill\").fillna(method=\"bfill\")\n    return df\ntrain = train.groupby('Asset_ID').apply(reindex).reset_index(0, drop=True).sort_index()\ngc.collect()\ntrain.shape","metadata":{"papermill":{"duration":16.485254,"end_time":"2021-11-23T06:56:38.45918","exception":false,"start_time":"2021-11-23T06:56:21.973926","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-12-09T14:34:33.235806Z","iopub.execute_input":"2021-12-09T14:34:33.236091Z","iopub.status.idle":"2021-12-09T14:35:13.415684Z","shell.execute_reply.started":"2021-12-09T14:34:33.236055Z","shell.execute_reply":"2021-12-09T14:35:13.414758Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Matching records and marking generated rows as 'non-real'\ntrain['group_num'] = train.index.map(times)\ntrain = train.dropna(subset=['group_num'])\ntrain['group_num'] = train['group_num'].astype('int')\ntrain['id'] = train['group_num'].astype(str) + '_' + train['Asset_ID'].astype(str)\ntrain['is_real'] = train.id.isin(ids) * 1\ntrain = train.drop('id', axis=1)","metadata":{"papermill":{"duration":74.011495,"end_time":"2021-11-23T06:57:52.606271","exception":false,"start_time":"2021-11-23T06:56:38.594776","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-12-09T14:35:13.419891Z","iopub.execute_input":"2021-12-09T14:35:13.421975Z","iopub.status.idle":"2021-12-09T14:36:15.409864Z","shell.execute_reply.started":"2021-12-09T14:35:13.421917Z","shell.execute_reply":"2021-12-09T14:36:15.408737Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Features values for 'non-real' rows are set to zeros\nfeatures = train.columns.drop(['Asset_ID','group_num','is_real'])\ntrain.loc[train.is_real == 0, features] = 0.","metadata":{"papermill":{"duration":88.844193,"end_time":"2021-11-23T06:59:21.544088","exception":false,"start_time":"2021-11-23T06:57:52.699895","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-12-09T14:36:15.413441Z","iopub.execute_input":"2021-12-09T14:36:15.413817Z","iopub.status.idle":"2021-12-09T14:37:37.913783Z","shell.execute_reply.started":"2021-12-09T14:36:15.413768Z","shell.execute_reply":"2021-12-09T14:37:37.912763Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['asset_order'] = train.Asset_ID.map(assets_order)\ntrain = train.sort_values(by=['group_num', 'asset_order'])\ntrain = reduce_mem_usage(train)\ntrain.head(20)\ngc.collect()","metadata":{"papermill":{"duration":13.15907,"end_time":"2021-11-23T06:59:34.797279","exception":false,"start_time":"2021-11-23T06:59:21.638209","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-12-09T14:37:37.915508Z","iopub.execute_input":"2021-12-09T14:37:37.916053Z","iopub.status.idle":"2021-12-09T14:38:26.964782Z","shell.execute_reply.started":"2021-12-09T14:37:37.916015Z","shell.execute_reply":"2021-12-09T14:38:26.963854Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"targets = train['Target'].to_numpy().reshape(-1, N_ASSETS)\nfeatures = train.columns.drop(['Asset_ID', 'Target', 'group_num', 'is_real', 'date'])\ntrain = train[features]\ntrain = train.values\ntrain = train.reshape(-1, N_ASSETS, train.shape[-1])","metadata":{"papermill":{"duration":3.211094,"end_time":"2021-11-23T06:59:38.103223","exception":false,"start_time":"2021-11-23T06:59:34.892129","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-12-09T14:38:26.966242Z","iopub.execute_input":"2021-12-09T14:38:26.966523Z","iopub.status.idle":"2021-12-09T14:38:36.399085Z","shell.execute_reply.started":"2021-12-09T14:38:26.966488Z","shell.execute_reply":"2021-12-09T14:38:36.398071Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class sample_generator(keras.utils.Sequence):\n    def __init__(self, x_set, y_set, batch_size, length):\n        self.x, self.y = x_set, y_set\n        self.batch_size = batch_size\n        self.length = length\n        self.size = len(x_set)\n    def __len__(self): return int(np.ceil(len(self.x) / float(self.batch_size)))\n    def __getitem__(self, idx):\n        batch_x=[]\n        batch_y=[]\n        for i in range(self.batch_size):\n            start_ind = self.batch_size*idx + i\n            end_ind = start_ind + self.length \n            if end_ind <= self.size:\n                batch_x.append(self.x[start_ind : end_ind])\n                batch_y.append(self.y[end_ind -1])\n        return np.array(batch_x), np.array(batch_y)","metadata":{"papermill":{"duration":0.1059,"end_time":"2021-11-23T06:59:38.491689","exception":false,"start_time":"2021-11-23T06:59:38.385789","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-12-09T14:38:36.400809Z","iopub.execute_input":"2021-12-09T14:38:36.401072Z","iopub.status.idle":"2021-12-09T14:38:36.412975Z","shell.execute_reply.started":"2021-12-09T14:38:36.401038Z","shell.execute_reply":"2021-12-09T14:38:36.411906Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def Data_generator( X,labels,sample_rate=5,num_frames =30, batch_size=1, mode='train'):\n    \n    \n    while True:\n        \n      \n        if mode=='train':\n            \n            sequence_length = seq_len + his\n            result = []\n            result1 = []\n            steps = batch_size+sequence_length+1\n            \n            for start in range(0,len(X) - sequence_length,steps):\n                \n                end = min(len(X) - sequence_length,start+steps)\n                for index in range(start,end):\n                    result.append(X[index: index + sequence_length])\n                    result1.append(labels[index: index + sequence_length])\n                result = np.stack(result, axis=0)\n                result1 = np.stack(result1, axis=0)\n\n                train = result[:]\n                y = result1[:]\n\n                x_train = train[:, :seq_len]\n                x_wd_train = train[:, :seq_len, pre_sens_num-1]\n                y_train = y[:, -1, :]\n                x_data = []\n                x_w = []\n                x_d = []\n                label = []\n                for i in range (len(train)):\n                    if i >= 240:\n                        x_data.append(x_train[i])\n                        x_w.append(x_wd_train[i - 240 + 8])\n                        x_d.append(x_wd_train[i - 60 + 8])\n                        label.append(y_train[i])\n                x_data = np.array(x_data)\n                x_w = np.array(x_w)\n                x_d = np.array(x_d)\n                label = np.array(label)\n            \n          \n\n#               yield x_batch_road, y_batch\n        elif mode=='test':\n          print(\"test\")","metadata":{"execution":{"iopub.status.busy":"2021-12-09T14:38:36.414539Z","iopub.execute_input":"2021-12-09T14:38:36.414821Z","iopub.status.idle":"2021-12-09T14:38:36.42818Z","shell.execute_reply.started":"2021-12-09T14:38:36.414788Z","shell.execute_reply":"2021-12-09T14:38:36.427492Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train, X_test = train[:-len(train)//PCT_VALIDATION], train[-len(train)//PCT_VALIDATION:]\ny_train, y_test = targets[:-len(train)//PCT_VALIDATION], targets[-len(train)//PCT_VALIDATION:]","metadata":{"papermill":{"duration":0.103461,"end_time":"2021-11-23T06:59:38.688776","exception":false,"start_time":"2021-11-23T06:59:38.585315","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-12-09T14:38:36.429664Z","iopub.execute_input":"2021-12-09T14:38:36.430149Z","iopub.status.idle":"2021-12-09T14:38:36.44724Z","shell.execute_reply.started":"2021-12-09T14:38:36.430113Z","shell.execute_reply":"2021-12-09T14:38:36.446482Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.save(\"X_train.npy\",X_train)\nnp.save(\"X_test.npy\",X_test)\nnp.save(\"y_train.npy\",y_train)\nnp.save(\"y_test.npy\",y_test)\n\n","metadata":{"execution":{"iopub.status.busy":"2021-12-09T14:38:36.448423Z","iopub.execute_input":"2021-12-09T14:38:36.4487Z","iopub.status.idle":"2021-12-09T14:39:12.558181Z","shell.execute_reply.started":"2021-12-09T14:38:36.448663Z","shell.execute_reply":"2021-12-09T14:39:12.557407Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train = np.load(\"X_train.npy\")\nX_test  = np.load(\"X_test.npy\")\ny_train = np.load(\"y_train.npy\")\ny_test  = np.load(\"y_test.npy\")","metadata":{"execution":{"iopub.status.busy":"2021-12-09T12:53:26.063368Z","iopub.execute_input":"2021-12-09T12:53:26.064087Z","iopub.status.idle":"2021-12-09T12:54:14.485983Z","shell.execute_reply.started":"2021-12-09T12:53:26.064034Z","shell.execute_reply":"2021-12-09T12:54:14.485043Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_generator = sample_generator(X_train, y_train, length = WINDOW_SIZE, batch_size = BATCH_SIZE)\nval_generator = sample_generator(X_test, y_test, length = WINDOW_SIZE, batch_size = BATCH_SIZE)\nprint(f'Sample shape: {train_generator[0][0].shape}')\nprint(f'Target shape: {train_generator[0][1].shape}')","metadata":{"papermill":{"duration":0.116983,"end_time":"2021-11-23T06:59:38.899643","exception":false,"start_time":"2021-11-23T06:59:38.78266","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del train_generator,val_generator","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import inspect\nfrom typing import List\n\nfrom tensorflow.keras import backend as K, Model, Input, optimizers\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.layers import Activation, SpatialDropout1D, Lambda\nfrom tensorflow.keras.layers import Layer, Conv1D, Dense, BatchNormalization, LayerNormalization","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"epoch = 1\nday = 288  #row\nweek = 2016  #row\nseq_len = 15\n#1=5min, 3=15min, 6=30min, 12=60min\npre_len = 1\n#data 1-7\npre_sens_num = 1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport csv\nimport os\n\n\ndef load_data(data,yy, seq_len=15, his=1, pre_sens_num=1):\n    max = np.max(data)\n    min = np.min(data)\n    med = max - min\n    data_nor = np.array(data, dtype=float)\n#     data_nor = (data - min) / med\n\n    sequence_length = seq_len + his\n    result = []\n    result1 = []\n    \n    for index in range(len(data_nor) - sequence_length):\n        result.append(data_nor[index: index + sequence_length])\n        result1.append(yy[index: index + sequence_length])\n        \n    result = np.stack(result, axis=0)\n    result1 = np.stack(result1, axis=0)\n    \n    train = result[:]\n    y = result1[:]\n    \n    x_train = train[:, :seq_len]\n    x_wd_train = train[:, :seq_len, pre_sens_num-1]\n    y_train = y[:, -1, :]\n    x_data = []\n    x_w = []\n    x_d = []\n    label = []\n    for i in range (len(train)):\n        if i >= 240:\n            x_data.append(x_train[i])\n            x_w.append(x_wd_train[i - 240 + 8])\n            x_d.append(x_wd_train[i - 60 + 8])\n            label.append(y_train[i])\n    x_data = np.array(x_data)\n    x_w = np.array(x_w)\n    x_d = np.array(x_d)\n    label = np.array(label)\n    return x_data,x_w,x_d,label,med,min","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_data1,x_w1,x_d1,label1,med1,min1 = load_data(X_train,y_train, seq_len=15, his=1, pre_sens_num=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_data1.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"label1.shape\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"label1\nimport numpy as np\nfrom tensorflow.keras import backend as K\n\n\ndef rmse_train(y_true, y_pred):\n    return K.sqrt(K.mean(K.square(y_pred - y_true), axis=-1))\n\ndef my_loss(y_true, y_pred):\n    L1 = K.sum(K.abs(y_true - y_pred))\n    L2 = K.sum(K.square(y_true - y_pred))\n    mse = K.mean(K.square(y_true - y_pred), axis = -1)\n    return L1 + L2 + mse\n\ndef predict_point_by_point(model, data):\n    predicted = model.predict(data)\n    predicted = np.reshape(predicted, (predicted.size, ))\n    print (\"predict_size:\", predicted.size)\n    return predicted\n\ndef MAE(pre,true):\n    c = 0\n    b = abs(np.subtract(pre, true))\n    for i in range(len(b)):\n        c = c + b[i]\n    return c / len(b)\n\ndef MAPE(pre, true):\n    a=0\n    for i in range(len(pre)):\n        x = (abs(pre[i] - true[i])/true[i])\n        a = a + x\n    return a / len(pre)\n\ndef RMSE(pre,true):\n    c = 0\n    b = abs(np.subtract(pre, true))\n    b = b*b\n    for i in range(len(b)):\n        c = c + b[i]\n    d = (c / len(b))**0.5\n    return d","metadata":{"papermill":{"duration":3.729679,"end_time":"2021-11-23T06:59:43.219011","exception":false,"start_time":"2021-11-23T06:59:39.489332","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\n\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras.layers import Layer\nfrom tensorflow.keras import initializers, regularizers, constraints\n\n\n\nclass AttentionLayer(Layer):\n    def __init__(self, **kwargs):\n        super(AttentionLayer, self).__init__(** kwargs)\n\n    def build(self, input_shape):                  # Initialization of the AttentionLayer\n        assert len(input_shape)==2\n\n        self.W_0 = self.add_weight(name='att_weight0',\n                                 shape=(input_shape[0][1], input_shape[0][1]),       # Size\n                                 initializer='uniform',     # Random inputs for initial iteration\n                                 trainable=True)\n        self.W_1 = self.add_weight(name='att_weight1',\n                                 shape=(input_shape[1][1], input_shape[1][1]),\n                                 initializer='uniform',\n                                 trainable=True)\n\n        self.W_2 = self.add_weight(name='att_weight2',\n                                   shape=(input_shape[0][1], input_shape[0][1]),\n                                   initializer='uniform',\n                                   trainable=True)\n\n        super(AttentionLayer, self).build(input_shape)\n\n    def call(self, inputs):        # AttentionLayer()   ([lstm_out2, con_out])\n        # inputs.shape = (batch_size, time_steps, seq_len)\n        x1 = K.permute_dimensions(inputs[0], (0, 1))   # LSTM Output (Hts)\n        x2 = K.permute_dimensions(inputs[1][:,-1,:], (0, 1))   # CNN Output (Gts)\n        # x.shape = (batch_size, seq_len, time_steps)\n        a = K.softmax(K.tanh(K.dot(x1, self.W_0)+ K.dot(x2, self.W_1)))            # Equation 11\n        a = K.dot(a, self.W_2)                                                     # Equation 11\n        outputs = K.permute_dimensions(a * x1, (0, 1))                             # Equation 10 \n        outputs = K.l2_normalize(outputs, axis=1) \n        return outputs\n\n    def compute_output_shape(self, input_shape):\n        return input_shape[0][0], input_shape[0][1]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\n#os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\" \n#os.environ['KERAS_BACKEND'] = 'tensorflow'\nimport keras\nimport numpy as np\nfrom tensorflow.keras.layers import Bidirectional\nfrom tensorflow.keras.layers import Dense, Flatten\nfrom tensorflow.keras.layers import LSTM\nfrom tensorflow.keras.models import *\nfrom tensorflow.keras.layers import Conv1D\nfrom tensorflow.keras.layers import TimeDistributed\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom tensorflow.keras.layers import Input, concatenate, Concatenate","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train whole model (cnn-lstm Bi-lstm) with attention\n\nimport os\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\" \nos.environ['KERAS_BACKEND'] = 'tensorflow'\nimport keras\nimport numpy as np\n\nfrom tensorflow.keras.layers import Bidirectional\nfrom tensorflow.keras.layers import Dense, Flatten\nfrom tensorflow.keras.layers import LSTM\nfrom tensorflow.keras.models import *\nfrom tensorflow.keras.layers import Conv1D\n\nfrom tensorflow.keras.layers import TimeDistributed\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom tensorflow.keras.layers import Input, concatenate, Concatenate\n\n\n# conv-lstm\nmain_input = Input((15, 7, 1),name='main_input')\ncon1 = TimeDistributed(Conv1D(filters=10, kernel_size=3, padding='same', activation='relu', strides=1))(main_input)  #This wrapper allows us to apply a layer to every temporal slice of an input.\ncon2 = TimeDistributed(Conv1D(filters=10, kernel_size=3, padding='same', activation='relu', strides=1))(con1)\n#con3 = TimeDistributed(AveragePooling1D(pool_size=2))(con2)\ncon_fl = TimeDistributed(Flatten())(con2)\ncon_out = Dense(15)(con_fl)\n\nlstm_out1 = LSTM(15, return_sequences=True)(con_out)\nlstm_out2 = LSTM(15, return_sequences=False)(lstm_out1)\n# lstm_out3 = AttentionLayer()([lstm_out2, con_out])\nlstm_out3 = Concatenate(axis=-1)([tf.expand_dims(lstm_out2, axis=2), con_out])\nlstm_out3 = _shufflenet_unit(lstm_out3, 25, 3, 2, 3, 2)\n\n\n\n\n# Bilstm\nauxiliary_input_w = Input((15, 1), name='auxiliary_input_w')\nlstm_outw1 = Bidirectional(LSTM(15, return_sequences=True))(auxiliary_input_w)\nlstm_outw2 = Bidirectional(LSTM(15, return_sequences=False))(lstm_outw1)\n\nauxiliary_input_d = Input((15, 1), name='auxiliary_input_d')\nlstm_outd1 = Bidirectional(LSTM(15, return_sequences=True))(auxiliary_input_d)\nlstm_outd2 = Bidirectional(LSTM(15, return_sequences=False))(lstm_outd1)\n\nprint(lstm_out3.shape)\nprint(lstm_outw2.shape)\nprint(lstm_outd2.shape)\n\nx = Concatenate(axis=-1)([lstm_out3, lstm_outw2, lstm_outd2])\nx = Dense(20, activation='relu')(x)\nx = Dense(10, activation='relu')(x)\nmain_output = Dense(1, activation='relu', kernel_regularizer=keras.regularizers.l1_l2(0.1, 0.1), name='main_output')(x)\n\nmodel = Model(inputs = [main_input, auxiliary_input_w, auxiliary_input_d], outputs = main_output)\nmodel.summary()\nmodel.compile(optimizer='adam', loss=my_loss)\n\n#train_save model\nfilepath = \"./model.h5\"\ncheckpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True,\n                             mode='min')\ncallbacks_list = [checkpoint]\n\n\nprint('Train...')\nmodel.fit([train_data1, train_w1, train_d1], label1,\n\t\t  batch_size=128, epochs=epoch, validation_split=0.15, verbose=2,\n\t\t  #class_weight='auto',\n      callbacks=callbacks_list)\n\n\n\n\n#Predict the last model\npredicted4 = predict_point_by_point(model, [test_data1, test_w1, test_d1])\np_real4 = []\nl_real4 = []\nrow=2016\nfor i in range(row):\n    p_real4.append(predicted4[i] * test_med1 + test_min1)            #Exit from the normalization mode that can calculate the criteria  \n    l_real4.append(test_l1[i] * test_med1 + test_min1)\np_real4 = np.array(p_real4)\nl_real4 = np.array(l_real4)\n\nprint (\"MAE:\", MAE(p_real4, l_real4))\nprint (\"MAPE:\", MAPE(p_real4, l_real4))\nprint (\"RMSE:\", RMSE(p_real4, l_real4))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(features)\n\ntf.random.set_seed(0)\nestop = keras.callbacks.EarlyStopping(monitor = 'val_loss', patience = 7, verbose = 0, mode = 'min',restore_best_weights = True)\nscheduler = keras.optimizers.schedules.ExponentialDecay(1e-3, (0.5 * len(X_train) / BATCH_SIZE), 1e-3)\nlr = keras.callbacks.LearningRateScheduler(scheduler, verbose = 1)\nhistory = model.fit(train_generator, validation_data = (val_generator), epochs = EPOCHS, callbacks = [lr, estop])\n\nfig, ax = plt.subplots(1, 2, figsize=(16, 8))\nhistories = pd.DataFrame(history.history)\nepochs = list(range(1,len(histories)+1))\nloss = histories['loss']\nval_loss = histories['val_loss']\nCorrelation = histories['Correlation']\nval_Correlation = histories['val_Correlation']\nax[0].plot(epochs, loss, label = 'Train Loss')\nax[0].plot(epochs, val_loss, label = 'Val Loss')\nax[0].set_title('Losses')\nax[0].set_xlabel('Epoch')\nax[0].legend(loc='upper right')\nax[1].plot(epochs, Correlation, label = 'Train Correlation')\nax[1].plot(epochs, val_Correlation, label = 'Val Correlation')\nax[1].set_title('Correlations')\nax[1].set_xlabel('Epoch')\nax[1].legend(loc='upper right')\nfig.show()\ngc.collect()\n\n# The correlation coefficients by asset for the validation data\npredictions = model.predict(val_generator)\n\nprint('Asset:    Corr. coef.')\nprint('---------------------')\nfor i in range(N_ASSETS):\n    # drop first 14 values in the y_test, since they are absent in val_generator labels\n    y_true = np.squeeze(y_test[WINDOW_SIZE - 1:, i])\n    y_pred = np.squeeze(predictions[:, i])\n    real_target_ind = np.argwhere(y_true!=0)\n    asset_id = list(assets_order.keys())[i]\n    asset_name = assets[assets.Asset_ID == asset_id]['Asset_Name'].item()\n    print(f\"{asset_name}: {np.corrcoef(y_pred[real_target_ind].flatten(), y_true[real_target_ind].flatten())[0,1]:.4f}\")","metadata":{"papermill":{"duration":748.079398,"end_time":"2021-11-23T07:12:13.517292","exception":false,"start_time":"2021-11-23T06:59:45.437894","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sup = dt.fread('../input/cryptocurrency-extra-data-binance-coin/orig_supplemental_train.jay').to_pandas()[:WINDOW_SIZE * (N_ASSETS)]\nplaceholder = get_features(sup)\nplaceholder['asset_order'] = placeholder.Asset_ID.map(assets_order)\ntest_sample = np.array(placeholder[features])\ntest_sample = test_sample.reshape(-1, (N_ASSETS), test_sample.shape[-1])\ntest_sample = np.expand_dims(test_sample, axis=0)","metadata":{"jupyter":{"outputs_hidden":false},"papermill":{"duration":0.547692,"end_time":"2021-11-23T07:12:14.452789","exception":false,"start_time":"2021-11-23T07:12:13.905097","status":"completed"},"pycharm":{"name":"#%%\n"},"tags":[],"collapsed":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"example = dt.fread('../input/cryptocurrency-extra-data-binance-coin/orig_example_test.jay').to_pandas()[:WINDOW_SIZE - 1]\nexample['asset_order'] = example.Asset_ID.map(assets_order) \nexample = example[['Asset_ID','asset_order']]","metadata":{"papermill":{"duration":0.40908,"end_time":"2021-11-23T07:12:15.251669","exception":false,"start_time":"2021-11-23T07:12:14.842589","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <span class=\"title-section w3-xxlarge\" id=\"submit\">Submit To Kaggle ðŸ‡°</span>\n<hr>","metadata":{"papermill":{"duration":0.389045,"end_time":"2021-11-23T07:12:16.035288","exception":false,"start_time":"2021-11-23T07:12:15.646243","status":"completed"},"tags":[]}},{"cell_type":"code","source":"env = gresearch_crypto.make_env()\niter_test = env.iter_test()\n\nfor (test_df, sample_prediction_df) in iter_test:\n    test_df = get_features(test_df)\n    test_data = test_df.merge(example, how='outer', on='Asset_ID').sort_values('asset_order')\n    test = np.array(test_data[features].fillna(0))\n    test = test.reshape(-1, 1, N_ASSETS, test.shape[-1])\n    test_sample = np.hstack([test_sample, test])[:,-1 * WINDOW_SIZE:]\n    y_pred = model.predict(test_sample).squeeze().reshape(-1, 1).squeeze()\n    test_data['Target'] = y_pred\n    for _, row in test_df.iterrows():\n        try: sample_prediction_df.loc[sample_prediction_df['row_id'] == row['row_id'], 'Target'] = test_data.loc[test_data['row_id'] == row['row_id'], 'Target'].item()\n        except: sample_prediction_df.loc[sample_prediction_df['row_id'] == row['row_id'], 'Target'] = 0\n    env.predict(sample_prediction_df)","metadata":{"papermill":{"duration":5.312172,"end_time":"2021-11-23T07:12:21.738075","exception":false,"start_time":"2021-11-23T07:12:16.425903","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <span class=\"title-section w3-xxlarge\">References</span>\n\n<span id=\"f1\">1.</span> [Initial baseline notebook](https://www.kaggle.com/julian3833)<br>\n<span id=\"f2\">2.</span> [Competition tutorial](https://www.kaggle.com/cstein06/tutorial-to-the-g-research-crypto-competition)<br>\n<span id=\"f3\">3.</span> [Competition Overview](https://www.kaggle.com/c/g-research-crypto-forecasting/overview)</span><br>\n<span id=\"f4\">4.</span> [My Initial Ideas for this competition](https://www.kaggle.com/c/g-research-crypto-forecasting/discussion/284903)</span><br>\n<span id=\"f5\">5.</span> [My post notebook about cross validation](https://www.kaggle.com/yamqwe/let-s-talk-validation-grouptimeseriessplit)</span><br>\n<span id=\"f5\">6.</span> [Chris original notebook from SIIM ISIC](https://www.kaggle.com/cdeotte/triple-stratified-kfold-with-tfrecords)</span><br>\n\n<span class=\"title-section w3-large w3-tag\">WORK IN PROGRESS! ðŸš§</span>","metadata":{"papermill":{"duration":0.39276,"end_time":"2021-11-23T07:12:22.523496","exception":false,"start_time":"2021-11-23T07:12:22.130736","status":"completed"},"tags":[]}}]}