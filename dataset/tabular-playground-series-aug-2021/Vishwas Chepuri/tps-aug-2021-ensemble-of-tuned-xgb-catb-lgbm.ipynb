{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Hyperparameter tuning and ensemble of XGB, CatB, LGBM\n\nThis notebook contains \n* Hyperparamter tuning of XGBoost, CatBoost, LightBGM with KFold cross validation.\n* Weighted ensemble of predictions obtained from tuned models with weights as reciprocal of individual scores. (This gave a score of **7.87595** on public LB data)\n\nReferences:\n* https://www.kaggle.com/c/tabular-playground-series-aug-2021/discussion/258009","metadata":{}},{"cell_type":"markdown","source":"## General Imports","metadata":{}},{"cell_type":"code","source":"# General imports\nimport os\nimport cv2\nimport glob \nimport json\nimport random\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nfrom sklearn.svm import *\nfrom sklearn.tree import *\nfrom sklearn.metrics import *\nfrom sklearn.ensemble import *\nfrom sklearn.linear_model import *\nfrom sklearn.decomposition import *\nfrom sklearn.preprocessing import *\nfrom sklearn.model_selection import *\n\nSEED = 42\nnp.random.seed(SEED)\nsns.set_style(\"dark\")\nmpl.rcParams['figure.dpi'] = 200\n%matplotlib inline","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Helper functions","metadata":{}},{"cell_type":"code","source":"def rmse(preds, y):\n    mse = mean_squared_error(preds, y)\n    return np.sqrt(mse)\n\ndef write_subfile(reg, scaler=None, filename=\"submission.csv\"):\n    test_data = pd.read_csv(\"../input/tabular-playground-series-aug-2021/test.csv\")\n    ids = test_data[\"id\"]\n    test_data = test_data.drop(columns=[\"id\"])\n    if scaler:\n        test_data = scaler.transform(test_data)\n    y = reg.predict(test_data)\n    df = pd.DataFrame({\n        \"id\" : ids,\n        \"loss\" : y\n    })\n    df.to_csv(filename, index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Reading and scaling data","metadata":{}},{"cell_type":"code","source":"data = pd.read_csv(\"../input/tabular-playground-series-aug-2021/train.csv\")\ndata.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Y = data[\"loss\"]\nX = data.drop(columns=[\"id\", \"loss\"])\n\nscaler = StandardScaler()\nscaler.fit(X)\nX_scaled = scaler.transform(X) \n\nX_scaled.shape, Y.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import optuna\nfrom optuna.samplers import TPESampler\n\noptuna.logging.set_verbosity(optuna.logging.WARNING)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Tuning XGB Regressor using Optuna","metadata":{}},{"cell_type":"code","source":"from xgboost import XGBRegressor\n\n# Setup XGB hyperparameters for exps\ndef get_xgb_hyperparams(trail):\n    xgb_params = {\n        'learning_rate': 0.01,\n        'tree_method': 'gpu_hist',\n        'booster': 'gbtree',\n        'n_estimators': trail.suggest_int('n_estimators', 500, 2500, 100),\n        'reg_lambda': trail.suggest_int('reg_lambda', 1, 100),\n        'reg_alpha': trail.suggest_int('reg_alpha', 1, 100),\n        'subsample': trail.suggest_float('subsample', 0.2, 1.0, step=0.1),\n        'colsample_bytree': trail.suggest_float('colsample_bytree', 0.2, 1.0, step=0.1),\n        'max_depth': trail.suggest_int('max_depth', 3, 10), \n        'min_child_weight': trail.suggest_int('min_child_weight', 2, 10),\n        'gamma': trail.suggest_float('gamma', 0, 20)        \n    }\n    return xgb_params","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define objective function\ndef objective_xgb(trail, X, Y, n_splits=4):\n       \n    xgb_params = get_xgb_hyperparams(trail)\n    \n    kfolds = KFold(n_splits=n_splits, shuffle=True, random_state=SEED)\n    \n    total_loss = 0\n    \n    for train_index, val_index in kfolds.split(X):\n        x_train, x_val = X[train_index], X[val_index]\n        y_train, y_val = Y[train_index], Y[val_index]\n    \n        xgb_reg = XGBRegressor(**xgb_params)\n    \n        xgb_reg = xgb_reg.fit(x_train, y_train)\n        preds = xgb_reg.predict(x_val)\n    \n        total_loss += rmse(preds, y_val)\n    \n    return total_loss / n_splits","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Callback function to print log messages when the best trail is updated\n\ndef logging_callback(study, frozen_trail):\n    prev_best = study.user_attrs.get('prev_best', None)\n    if prev_best != study.best_value:\n        study.set_user_attr('prev_best', study.best_value)\n        print(f\"Trail {frozen_trail.number} finished with best value {frozen_trail.value}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\nstudy = optuna.create_study(sampler=TPESampler(seed=SEED), \n                            direction='minimize', \n                            study_name='xgb_tuning')\nobjc = lambda trail : objective_xgb(trail, X_scaled, Y)\n\nstudy.optimize(objc, n_trials=500, timeout=60*60, callbacks=[logging_callback])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"Best rmse value: {study.best_value}\")\nprint(f\"Best params: \")\nfor param, value in study.best_params.items():\n    print(f\"\\t{param} : {value}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xgb_reg = XGBRegressor(**study.best_params,\n                      learning_rate= 0.01,\n                      tree_method= 'gpu_hist',\n                      booster= 'gbtree')\n\nxgb_reg.fit(X_scaled, Y, eval_metric='rmse', verbose=True)\n\nprint(f\"RMSE on training data : {rmse(Y, xgb_reg.predict(X_scaled))}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"write_subfile(xgb_reg, scaler, \"xgb_tuned.csv\") # score: 7.87652","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Tuning Cat Boost Regressor using Optuna","metadata":{}},{"cell_type":"code","source":"from catboost import CatBoostRegressor\n\n# Setup CatB hyperparameters for exps\ndef get_catb_hyperparams(trail):\n    catb_params = {\n        'loss_function': 'RMSE',\n        'task_type': 'GPU',\n        'eval_metric': 'RMSE',\n        'bootstrap_type': 'Bernoulli',\n        'iterations': trail.suggest_int('iterations', 1000, 5000),\n        'od_wait': trail.suggest_int('od_wait', 500, 2000),\n        'learning_rate': trail.suggest_uniform('learning_rate', 0.01, 0.5),\n        'reg_lambda': trail.suggest_uniform('reg_lambda', 1e-4, 100),\n        'subsample': trail.suggest_uniform('subsample', 0, 1),\n        'random_strength': trail.suggest_uniform('random_strength', 10, 50),\n        'depth': trail.suggest_int('depth', 1, 15),\n        'min_data_in_leaf': trail.suggest_int('min_data_in_leaf', 1, 30),\n        'leaf_estimation_iterations': trail.suggest_int('leaf_estimation_iterations', 1, 15)\n    }\n    return catb_params","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define objective function\ndef objective_catb(trail, X, Y, n_splits=4):\n    \n    catb_params = get_catb_hyperparams(trail)\n    \n    kfolds = KFold(n_splits=n_splits, shuffle=True, random_state=SEED)\n    \n    total_loss = 0\n    \n    for train_index, val_index in kfolds.split(X):\n        x_train, x_val = X[train_index], X[val_index]\n        y_train, y_val = Y[train_index], Y[val_index]\n    \n        catb_reg = CatBoostRegressor(**catb_params)\n    \n        catb_reg = catb_reg.fit(x_train, y_train)\n    \n        preds = catb_reg.predict(x_val)\n    \n        total_loss += rmse(preds, y_val)\n    \n    return total_loss / n_splits","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\nstudy = optuna.create_study(sampler=TPESampler(seed=SEED), \n                            direction='minimize', \n                            study_name='catb_tuning')\nobjc = lambda trail : objective_catb(trail, X_scaled, Y)\n\nstudy.optimize(objc, timeout=60*5, callbacks=[logging_callback])","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"Best rmse value: {study.best_value}\")\nprint(f\"Best params: \")\nfor param, value in study.best_params.items():\n    print(f\"\\t{param} : {value}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"catb_reg = CatBoostRegressor(**study.best_params,\n                            loss_function='RMSE',\n                            task_type='GPU',\n                            eval_metric='RMSE',\n                            bootstrap_type='Bernoulli')\n\ncatb_reg.fit(X_scaled, Y)\n\nprint(f\"RMSE on training data : {rmse(Y, catb_reg.predict(X_scaled))}\")","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"write_subfile(catb_reg, scaler, \"catb_tuned.csv\") # score: 7.89228","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Tuning Light GBM Regressor using Optuna","metadata":{}},{"cell_type":"code","source":"from lightgbm import LGBMRegressor\n\n# Setup lgbm hyperparameters for exps\ndef get_lgbm_hyperparams(trail):\n    lgbm_params = {\n        \"objective\": \"rmse\",\n        \"metric\": \"rmse\",\n        \"boosting_type\": \"gbdt\",\n        \"learning_rate\": 0.008,\n        'device': 'gpu',\n        'n_estimators': trail.suggest_int(\"n_estimators\", 500, 4000),\n        \"num_leaves\": trail.suggest_int(\"num_leaves\", 8, 256),\n        \"min_child_samples\": trail.suggest_int(\"min_child_samples\", 2, 3000),\n        'feature_fraction': trail.suggest_uniform('feature_fraction', 0.25, 0.7),\n        'bagging_fraction': trail.suggest_uniform('bagging_fraction', 0.7, 1.0),\n        'bagging_freq': trail.suggest_int('bagging_freq', 0, 5),\n        'reg_alpha': trail.suggest_int(\"reg_alpha\", 1, 100),\n        'reg_lambda': trail.suggest_int(\"reg_lambda\", 1, 100),\n    }\n    return lgbm_params","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define objective function\ndef objective_lgbm(trail, X, Y, n_splits=4):\n\n    lgbm_params = get_lgbm_hyperparams(trail)\n    \n    kfolds = KFold(n_splits=n_splits, shuffle=True, random_state=SEED)\n    \n    total_loss = 0\n    \n    for train_index, val_index in kfolds.split(X):\n        x_train, x_val = X[train_index], X[val_index]\n        y_train, y_val = Y[train_index], Y[val_index]\n    \n        lgbm_reg = LGBMRegressor(**lgbm_params)\n    \n        lgbm_reg.fit(x_train, y_train)\n    \n        preds = lgbm_reg.predict(x_val)\n    \n        total_loss += rmse(preds, y_val)\n    \n    return total_loss / n_splits","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\nstudy = optuna.create_study(sampler=TPESampler(seed=SEED), \n                            direction='minimize', \n                            study_name='lgbm_tuning')\nobjc = lambda trail : objective_lgbm(trail, X_scaled, Y)\n\nstudy.optimize(objc, timeout=60*60, callbacks=[logging_callback])","metadata":{"_kg_hide-output":false,"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"Best value (rmse): {study.best_value:.5f}\")\nprint(f\"Best params:\")\nfor key, value in study.best_params.items():\n    print(f\"\\t{key}: {value}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lgbm_reg = LGBMRegressor(**study.best_params,\n                        objective=\"rmse\",\n                        metric=\"rmse\",\n                        boosting_type=\"gbdt\",\n                        learning_rate=0.008,\n                        device='gpu')\n\nlgbm_reg.fit(X_scaled, Y, eval_metric='rmse', verbose=True)\n\nprint(f\"RMSE on training data : {rmse(Y, lgbm_reg.predict(X_scaled))}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"write_subfile(lgbm_reg, scaler, \"lgbm_tuned.csv\") # score: 7.89669","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Ensembling","metadata":{}},{"cell_type":"code","source":"xgb_preds = pd.read_csv('xgb_tuned.csv')['loss']\ncatb_preds = pd.read_csv('catb_tuned.csv')['loss']\nlgbm_preds = pd.read_csv('lgbm_tuned.csv')['loss']\n\nscores = [7.87652, 7.89228, 7.87669]\n\nweights = [1 / score for score in scores]\nweights = np.array([weight / sum(weights) for weight in weights])\n\npreds = np.array([xgb_preds, catb_preds, lgbm_preds])\n\nensemble_preds = np.zeros_like(xgb_preds)\n\nfor pred, weight in zip(preds, weights):\n    ensemble_preds += weight * pred\n    \nensemble_csv = pd.read_csv('xgb_tuned.csv') # Score: 7.87595\nensemble_csv['loss'] = ensemble_preds\nensemble_csv.to_csv('ensemble.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}