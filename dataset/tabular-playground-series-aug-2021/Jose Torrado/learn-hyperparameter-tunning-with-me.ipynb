{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\nimport pandas as pd\nimport numpy as np","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-08-13T20:42:52.773875Z","iopub.execute_input":"2021-08-13T20:42:52.774222Z","iopub.status.idle":"2021-08-13T20:42:52.783886Z","shell.execute_reply.started":"2021-08-13T20:42:52.774195Z","shell.execute_reply":"2021-08-13T20:42:52.783053Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Boosting your score (hopefully?)\nAfter running a standar Random Forrest Regressor *(and getting a submission worthy of a 905th spot)*, it seems like we are going to have to pull out some heavier artillery to crack this one. Now, mind you I did little to no tinkering with the setup of the regressor, so that is something that I really to explore before moving on to the next thing. \n\nAfter doing a bit of reasearch I discovered that what I want to do is called **\"Hyperparameter Tunning\"**. This sounds **hyper**-complicated (haha), so I want to take some time to dive deeper explain what it means.\n\n# 1. What is a Hyperparameter?\nThink about these **hyperparameters** as the *factory settings* of your machine learning model. Out of the box, you throw some data into the model and it will do its best at giving you a prediction using its standard settings. You don't initially assign these parameters any value, but you can at any point if desired. \n\n**Hyperparameter Tunning** is the process of tinkering with and modifying these settings with the goal of getting more accurate predictions out of your model.","metadata":{}},{"cell_type":"markdown","source":"![Mechanic](https://media.giphy.com/media/PqfKAMBfcqwVodnDKs/giphy.gif)\n<cite> <center> Tightening the bolts of your own ML Robot </center><cite>","metadata":{"execution":{"iopub.status.busy":"2021-08-12T01:28:18.172179Z","iopub.execute_input":"2021-08-12T01:28:18.172679Z","iopub.status.idle":"2021-08-12T01:28:18.918008Z","shell.execute_reply.started":"2021-08-12T01:28:18.172567Z","shell.execute_reply":"2021-08-12T01:28:18.916857Z"}}},{"cell_type":"markdown","source":"The thing is, you don't really know what parameters will work best until you start trying different combinations. So to find the best setup you are going to have to get your hands dirty and do some ol'fashioned **trial-and-error** work. However, you must be wary of the most common issues in machine learning: **overfitting**","metadata":{}},{"cell_type":"markdown","source":"# 2. Overfitting and Corss Validation â˜¯\n\nYou will usually be dealing with two sepparate datasets when you are applying your models, a training and test data set. As the names imply, you use your training data set to teach your model how to make predictions (to train it) and your test data set to evaluate how good is your model at making predictions. \n\nA common issue is that your model will be *extremelly* good at making predictions on your training data, but performs really poorly on your test data. This is known as **overfitting**. If we tune and optimize our model for our training data, it will be useless when we try to apply it to new data points. \n\nHere is where we introduce **Cross Validation**! In this notebook we will be using the most common method, <u>K-Fold</u>. \n\n## K-Fold Cross Validation\n\nLike I mentioned earlier, you usually split your data set into train and test whenever you are facing a machine learning problem. \n\n**With K-Fold CV we go a step further, we split our training data into K number of subsets (or Folds). We then itratively fit our model K number of times, training it on K-1 folds and validation on the Kth fold.**\n\nThis can be a bit confusing to a beginner like me, so here is a graphic to better explain:","metadata":{}},{"cell_type":"markdown","source":"![K-FoldCV](https://scikit-learn.org/stable/_images/grid_search_cross_validation.png)\n","metadata":{}},{"cell_type":"markdown","source":"So each time we train the model, we use different parameter settings. At the end:\n* We compare all of the models performances \n* Select the best one (along with its finelly tunned parameters ðŸ˜‰) \n* Train the whole data set using those settings\n\nEven though this sounds like a very complicated and tedious process, the Scikit-Learn library makes it really easy and straightforward!","metadata":{}},{"cell_type":"markdown","source":"# 3. Getting our Hands Dirty\n## What are our Parameters? \nLet's first take a look at what parameter we are dealing with in our RandomForrestRegressor model:","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\n\nregressor = RandomForestRegressor(random_state = 0)\nprint('Parameters used:\\n')\nprint(regressor.get_params())","metadata":{"execution":{"iopub.status.busy":"2021-08-13T20:42:52.785475Z","iopub.execute_input":"2021-08-13T20:42:52.786041Z","iopub.status.idle":"2021-08-13T20:42:52.792552Z","shell.execute_reply.started":"2021-08-13T20:42:52.786001Z","shell.execute_reply":"2021-08-13T20:42:52.791606Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Way too many parameters**. I don't know what half of these mean to be honest. This is where documentation comes in handy! If we look at the random forest documentation, we can find definitions for all of these parameters. For now I will only be looking at:\n\n* n_estimators: The number of trees in the forest.\n* max_features: The number of features to consider when looking for the best split\n* max_depth: The maximum depth of the tree\n* min_samples_split: The minimum number of samples required to split an internal node\n* min_samples_leaf: The minimum number of samples required to be at a leaf node\n* bootstrap: Whether bootstrap samples are used when building trees\n\nIf you would like a more in-depth explanation of these parameters take a look at the [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)","metadata":{}},{"cell_type":"markdown","source":"## Random Sampling\nWe will be using *RandomizedSearchCV* to narrow dowm what values will work the best for our parameters. We will first create a **\"Grid\"** of values to assing to our paramters:","metadata":{}},{"cell_type":"code","source":"# Number of trees in random forest\nn_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n\n# Number of features to consider at every split\nmax_features = ['auto', 'sqrt']\n\n# Maximum number of levels in tree\nmax_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\nmax_depth.append(None)\n\n# Minimum number of samples required to split a node\nmin_samples_split = [2, 5, 10]\n\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [1, 2, 4]\n\n# Method of selecting samples for training each tree\nbootstrap = [True, False]\n\n# Create the random grid\nrandom_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n               'bootstrap': bootstrap}\n\nprint(random_grid)","metadata":{"execution":{"iopub.status.busy":"2021-08-13T20:42:52.794241Z","iopub.execute_input":"2021-08-13T20:42:52.794501Z","iopub.status.idle":"2021-08-13T20:42:52.803262Z","shell.execute_reply.started":"2021-08-13T20:42:52.794476Z","shell.execute_reply":"2021-08-13T20:42:52.802208Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now, we will use our module to pick different combinations of paramters to cross-validate our model with. For this analysis we will using 100 different parameter combinations and 3 folds:","metadata":{}},{"cell_type":"code","source":"# Data\ntrain = pd.read_csv('../input/tabular-playground-series-aug-2021/train.csv')\ntest = pd.read_csv('../input/tabular-playground-series-aug-2021/test.csv')\n\n# Dropping id column\ntrain.drop(['id'], axis=1, inplace= True)\ntest.drop(['id'], axis=1, inplace= True)\n\n# Sepparating our matrix of features and prediction vector\nX = train.iloc[:, 0:-1].values\ny = train.iloc[:, -1].values\n\n# Training and Testing Sets\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size = 0.3, random_state = 0)","metadata":{"execution":{"iopub.status.busy":"2021-08-13T20:42:52.804421Z","iopub.execute_input":"2021-08-13T20:42:52.804704Z","iopub.status.idle":"2021-08-13T20:42:59.544743Z","shell.execute_reply.started":"2021-08-13T20:42:52.804678Z","shell.execute_reply":"2021-08-13T20:42:59.543744Z"}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The following code block would be the next step in finding the best paramter settings:","metadata":{}},{"cell_type":"markdown","source":"<code>from sklearn.model_selection import RandomizedSearchCV\nregressor_random = RandomizedSearchCV(estimator= regressor, param_distributions= random_grid, n_iter=10, cv=2, verbose=2, random_state=0, n_jobs=-1)\nregressor_random.fit(X_train,y_train)</code>","metadata":{"execution":{"iopub.status.busy":"2021-08-13T20:42:59.545714Z","iopub.execute_input":"2021-08-13T20:42:59.545981Z"},"_kg_hide-output":true}},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-info\"> ðŸ“Œ This seems like its going to be way more complicated than I expected it to be. Given the large size of the data, and the large number of parameters, the training time is going to be insanely long. </div>","metadata":{}},{"cell_type":"markdown","source":"# 4. Learned My Lesson\n\nI thought that this approach would be the next logical step at increasing my score. It seems like my hypothesis was wrong. The processing time needed to do a randomized search with this magnitude of data is too long, therefore this approach doesn't seem to be the most optimal. \n\nI have done some more research and it seems like there are many things I could have done better in this approach, [here is a good forum post about it](https://stats.stackexchange.com/questions/270315/how-to-speed-up-hyperparameter-optimization). I will be trying some of these tips in a future analysis as well as doing some research into [Bayesian Optimization](https://machinelearningmastery.com/what-is-bayesian-optimization/)\n\n# Thank you\n\nAs many have said before, data science is a lot of trial and error to find the best solutions. Even though it can be discouraging at times, failing is a great opportunity to learn new things. This is a prime example. Thank you for taking the time to read this notebook, and as always please let me know if you ahve any insight or tips on this topic.\n\nBest,\nJose","metadata":{}}]}