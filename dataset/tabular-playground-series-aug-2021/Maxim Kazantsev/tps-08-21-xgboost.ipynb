{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport random\nfrom xgboost import XGBRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import StratifiedKFold, StratifiedShuffleSplit, KFold\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler, QuantileTransformer\nimport optuna\n\n\n# Pandas setting to display more dataset rows and columns\npd.set_option('display.max_rows', 150)\npd.set_option('display.max_columns', 500)\npd.set_option('display.max_colwidth', None)\npd.set_option('display.float_format', lambda x: '%.5f' % x)\n\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-08-15T17:55:31.584658Z","iopub.execute_input":"2021-08-15T17:55:31.585177Z","iopub.status.idle":"2021-08-15T17:55:32.343782Z","shell.execute_reply.started":"2021-08-15T17:55:31.585094Z","shell.execute_reply":"2021-08-15T17:55:32.342917Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Data import**","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv(\"/kaggle/input/tabular-playground-series-aug-2021/train.csv\", low_memory=False)#, nrows=10000)\n# train[\"date_time\"] = pd.to_datetime(train[\"date_time\"], format=\"%Y-%m-%d %H:%M:%S\")\ntest = pd.read_csv(\"/kaggle/input/tabular-playground-series-aug-2021/test.csv\", low_memory=False)\n# test[\"date_time\"] = pd.to_datetime(test[\"date_time\"], format=\"%Y-%m-%d %H:%M:%S\")\ntrain.info(memory_usage=\"deep\")","metadata":{"execution":{"iopub.status.busy":"2021-08-15T17:55:32.345886Z","iopub.execute_input":"2021-08-15T17:55:32.346296Z","iopub.status.idle":"2021-08-15T17:55:43.659494Z","shell.execute_reply.started":"2021-08-15T17:55:32.346256Z","shell.execute_reply":"2021-08-15T17:55:43.657446Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.info(memory_usage=\"deep\")","metadata":{"execution":{"iopub.status.busy":"2021-08-15T17:55:43.661364Z","iopub.execute_input":"2021-08-15T17:55:43.661877Z","iopub.status.idle":"2021-08-15T17:55:43.675205Z","shell.execute_reply.started":"2021-08-15T17:55:43.66183Z","shell.execute_reply":"2021-08-15T17:55:43.674136Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head(10)","metadata":{"execution":{"iopub.status.busy":"2021-08-15T17:55:43.676806Z","iopub.execute_input":"2021-08-15T17:55:43.677332Z","iopub.status.idle":"2021-08-15T17:55:43.757862Z","shell.execute_reply.started":"2021-08-15T17:55:43.677293Z","shell.execute_reply":"2021-08-15T17:55:43.756969Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **EDA**","metadata":{}},{"cell_type":"code","source":"# Colors to be used for plots\ncolors = [\"lightcoral\", \"sandybrown\", \"darkorange\", \"mediumseagreen\",\n          \"lightseagreen\", \"cornflowerblue\", \"mediumpurple\", \"palevioletred\",\n          \"lightskyblue\", \"sandybrown\", \"yellowgreen\", \"indianred\",\n          \"lightsteelblue\", \"mediumorchid\", \"deepskyblue\"]","metadata":{"execution":{"iopub.status.busy":"2021-08-15T17:55:43.759176Z","iopub.execute_input":"2021-08-15T17:55:43.759509Z","iopub.status.idle":"2021-08-15T17:55:43.764173Z","shell.execute_reply.started":"2021-08-15T17:55:43.759476Z","shell.execute_reply":"2021-08-15T17:55:43.763025Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(5, 5))\npie = ax.pie([len(train), len(test)],\n             labels=[\"Train dataset\", \"Test dataset\"],\n             colors=[\"salmon\", \"teal\"],\n             textprops={\"fontsize\": 15},\n             autopct='%1.1f%%')\nax.axis(\"equal\")\nax.set_title(\"Dataset length comparison\", fontsize=18)\nfig.set_facecolor('white')\nplt.show();","metadata":{"execution":{"iopub.status.busy":"2021-08-15T17:55:43.765771Z","iopub.execute_input":"2021-08-15T17:55:43.766505Z","iopub.status.idle":"2021-08-15T17:55:43.867348Z","shell.execute_reply.started":"2021-08-15T17:55:43.766463Z","shell.execute_reply":"2021-08-15T17:55:43.866398Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.describe().T","metadata":{"execution":{"iopub.status.busy":"2021-08-15T17:55:43.868746Z","iopub.execute_input":"2021-08-15T17:55:43.869096Z","iopub.status.idle":"2021-08-15T17:55:44.891794Z","shell.execute_reply.started":"2021-08-15T17:55:43.869048Z","shell.execute_reply":"2021-08-15T17:55:44.890638Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.isna().sum().sum(), test.isna().sum().sum()","metadata":{"execution":{"iopub.status.busy":"2021-08-15T17:55:44.89505Z","iopub.execute_input":"2021-08-15T17:55:44.895459Z","iopub.status.idle":"2021-08-15T17:55:44.984739Z","shell.execute_reply.started":"2021-08-15T17:55:44.895419Z","shell.execute_reply":"2021-08-15T17:55:44.983762Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are no missing value in the both datasets.\n\nLet's check target distribution.","metadata":{}},{"cell_type":"code","source":"train[\"loss\"].value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-08-15T17:55:44.986856Z","iopub.execute_input":"2021-08-15T17:55:44.987215Z","iopub.status.idle":"2021-08-15T17:55:44.998482Z","shell.execute_reply.started":"2021-08-15T17:55:44.987179Z","shell.execute_reply":"2021-08-15T17:55:44.997119Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(16, 8))\n\nbars = ax.bar(train[\"loss\"].value_counts().sort_index().index,\n              train[\"loss\"].value_counts().sort_index().values,\n              color=colors,\n              edgecolor=\"black\")\nax.set_title(\"Loss (target) distribution\", fontsize=20, pad=15)\nax.set_ylabel(\"Amount of values\", fontsize=14, labelpad=15)\nax.set_xlabel(\"Loss (target) value\", fontsize=14, labelpad=10)\nax.bar_label(bars, [f\"{x:2.2f}%\" for x in train[\"loss\"].value_counts().sort_index().values/(len(train)/100)],\n                 padding=5, fontsize=10, rotation=90)\nax.margins(0.025, 0.12)\nax.grid(axis=\"y\")\n\nplt.show();","metadata":{"execution":{"iopub.status.busy":"2021-08-15T17:55:45.000172Z","iopub.execute_input":"2021-08-15T17:55:45.000534Z","iopub.status.idle":"2021-08-15T17:55:45.52979Z","shell.execute_reply.started":"2021-08-15T17:55:45.000499Z","shell.execute_reply":"2021-08-15T17:55:45.528794Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's check feature values distribution in the both datasets.","metadata":{}},{"cell_type":"code","source":"df = pd.concat([train.drop([\"id\", \"loss\"], axis=1), test.drop(\"id\", axis=1)], axis=0)\ncolumns = df.columns.values\n\ncols = 3\nrows = len(columns) // cols + 1\n\nfig, axs = plt.subplots(ncols=cols, nrows=rows, figsize=(16,100), sharex=False)\n\nplt.subplots_adjust(hspace = 0.3)\ni=0\n\nfor r in np.arange(0, rows, 1):\n    for c in np.arange(0, cols, 1):\n        if i >= len(columns):\n            axs[r, c].set_visible(False)\n        else:\n            hist1 = axs[r, c].hist(train[columns[i]].values,\n                                   range=(df[columns[i]].min(),\n                                          df[columns[i]].max()),\n                                   bins=40,\n                                   color=\"deepskyblue\",\n                                   edgecolor=\"black\",\n                                   alpha=0.7,\n                                   label=\"Train Dataset\")\n            hist2 = axs[r, c].hist(test[columns[i]].values,\n                                   range=(df[columns[i]].min(),\n                                          df[columns[i]].max()),\n                                   bins=40,\n                                   color=\"palevioletred\",\n                                   edgecolor=\"black\",\n                                   alpha=0.7,\n                                   label=\"Test Dataset\")\n            axs[r, c].set_title(columns[i], fontsize=14, pad=5)\n            axs[r, c].tick_params(axis=\"y\", labelsize=13)\n            axs[r, c].tick_params(axis=\"x\", labelsize=13)\n            axs[r, c].grid(axis=\"y\")\n            axs[r, c].legend(fontsize=13)\n                                  \n        i+=1\n#plt.suptitle(\"Feature values distribution in both datasets\", y=0.99)\nplt.show();","metadata":{"execution":{"iopub.status.busy":"2021-08-15T17:55:45.531217Z","iopub.execute_input":"2021-08-15T17:55:45.531809Z","iopub.status.idle":"2021-08-15T17:56:16.440203Z","shell.execute_reply.started":"2021-08-15T17:55:45.531764Z","shell.execute_reply":"2021-08-15T17:56:16.439259Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The datasets are pretty well balanced.","metadata":{}},{"cell_type":"code","source":"train.nunique().sort_values().head()","metadata":{"execution":{"iopub.status.busy":"2021-08-15T17:56:16.441277Z","iopub.execute_input":"2021-08-15T17:56:16.441596Z","iopub.status.idle":"2021-08-15T17:56:17.875135Z","shell.execute_reply.started":"2021-08-15T17:56:16.441558Z","shell.execute_reply":"2021-08-15T17:56:17.874163Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As you can see, f1 feature has the smallest amount of unique values - 289. So I don't think any feature should be treated as categorical.\n\nLet's look at feature correlation.","metadata":{}},{"cell_type":"code","source":"# Plot dataframe\ndf = train.drop(\"id\", axis=1).corr().round(5)\n\n# Mask to hide upper-right part of plot as it is a duplicate\nmask = np.zeros_like(df)\nmask[np.triu_indices_from(mask)] = True\n\n# Making a plot\nplt.figure(figsize=(16,16))\nax = sns.heatmap(df, annot=False, mask=mask, cmap=\"RdBu\", annot_kws={\"weight\": \"bold\", \"fontsize\":13})\nax.set_title(\"Feature correlation heatmap\", fontsize=17)\nplt.setp(ax.get_xticklabels(), rotation=90, ha=\"right\",\n         rotation_mode=\"anchor\", weight=\"normal\")\nplt.setp(ax.get_yticklabels(), weight=\"normal\",\n         rotation_mode=\"anchor\", rotation=0, ha=\"right\")\nplt.show();","metadata":{"execution":{"iopub.status.busy":"2021-08-15T17:56:17.876649Z","iopub.execute_input":"2021-08-15T17:56:17.877Z","iopub.status.idle":"2021-08-15T17:56:25.408797Z","shell.execute_reply.started":"2021-08-15T17:56:17.876963Z","shell.execute_reply":"2021-08-15T17:56:25.407915Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As you can see, the correlation is between ~0.03 and ~0.03 which is pretty small. So the features are weakly correlated. \n\nThere are some features with relatively low correlation with target value even comparing with other features:","metadata":{}},{"cell_type":"code","source":"df[(df[\"loss\"]>-0.001) & (df[\"loss\"]<0.001)][\"loss\"]","metadata":{"execution":{"iopub.status.busy":"2021-08-15T17:56:25.410419Z","iopub.execute_input":"2021-08-15T17:56:25.410794Z","iopub.status.idle":"2021-08-15T17:56:25.432687Z","shell.execute_reply.started":"2021-08-15T17:56:25.410755Z","shell.execute_reply":"2021-08-15T17:56:25.431959Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's visualize each feature vs loss.","metadata":{}},{"cell_type":"code","source":"columns = train.drop([\"id\", \"loss\"], axis=1).columns.values\n\ncols = 4\nrows = len(columns) // cols + 1\n\nfig, axs = plt.subplots(ncols=cols, nrows=rows, figsize=(16,100), sharex=False)\n\nplt.subplots_adjust(hspace = 0.3)\ni=0\n\nfor r in np.arange(0, rows, 1):\n    for c in np.arange(0, cols, 1):\n        if i >= len(columns):\n            axs[r, c].set_visible(False)\n        else:\n            scatter = axs[r, c].scatter(train[columns[i]].values,\n                                        train[\"loss\"],\n                                        color=random.choice(colors))\n            axs[r, c].set_title(columns[i], fontsize=14, pad=5)\n            axs[r, c].tick_params(axis=\"y\", labelsize=11)\n            axs[r, c].tick_params(axis=\"x\", labelsize=11)\n                                  \n        i+=1\n#plt.suptitle(\"Features vs loss\", y=0.99)\nplt.show();","metadata":{"execution":{"iopub.status.busy":"2021-08-15T17:56:25.434008Z","iopub.execute_input":"2021-08-15T17:56:25.434363Z","iopub.status.idle":"2021-08-15T17:57:16.108945Z","shell.execute_reply.started":"2021-08-15T17:56:25.434329Z","shell.execute_reply":"2021-08-15T17:57:16.108077Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Data preprocessing**","metadata":{}},{"cell_type":"code","source":"# Calculating edges of target bins to be used for stratified split\ntarget_bin_edges = np.histogram_bin_edges(train[\"loss\"], bins=10)\ntarget_bin_edges[0] = -np.inf\ntarget_bin_edges[-1] = np.inf\ntarget_bins = pd.cut(train[\"loss\"], target_bin_edges, labels=np.arange(10))\ntarget_bins.value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-08-15T17:57:16.110429Z","iopub.execute_input":"2021-08-15T17:57:16.110998Z","iopub.status.idle":"2021-08-15T17:57:16.135762Z","shell.execute_reply.started":"2021-08-15T17:57:16.110957Z","shell.execute_reply":"2021-08-15T17:57:16.134809Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Scaling data\nx_scaler = StandardScaler()\nX = pd.DataFrame(x_scaler.fit_transform(train.drop([\"id\", \"loss\"], axis=1)), columns=train.drop([\"id\", \"loss\"], axis=1).columns)\nX_test = pd.DataFrame(x_scaler.transform(test.drop(\"id\", axis=1)), columns=test.drop([\"id\"], axis=1).columns)\n\ny = train[\"loss\"].copy()","metadata":{"execution":{"iopub.status.busy":"2021-08-15T17:57:16.137143Z","iopub.execute_input":"2021-08-15T17:57:16.137532Z","iopub.status.idle":"2021-08-15T17:57:17.135981Z","shell.execute_reply.started":"2021-08-15T17:57:16.137495Z","shell.execute_reply":"2021-08-15T17:57:17.135052Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X.describe()","metadata":{"execution":{"iopub.status.busy":"2021-08-15T17:57:17.137392Z","iopub.execute_input":"2021-08-15T17:57:17.137976Z","iopub.status.idle":"2021-08-15T17:57:18.22517Z","shell.execute_reply.started":"2021-08-15T17:57:17.137939Z","shell.execute_reply":"2021-08-15T17:57:18.224283Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_test.describe()","metadata":{"execution":{"iopub.status.busy":"2021-08-15T17:57:18.226471Z","iopub.execute_input":"2021-08-15T17:57:18.226861Z","iopub.status.idle":"2021-08-15T17:57:18.951251Z","shell.execute_reply.started":"2021-08-15T17:57:18.226823Z","shell.execute_reply":"2021-08-15T17:57:18.95012Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y.min(), y.max()","metadata":{"execution":{"iopub.status.busy":"2021-08-15T17:57:18.952977Z","iopub.execute_input":"2021-08-15T17:57:18.953404Z","iopub.status.idle":"2021-08-15T17:57:18.960857Z","shell.execute_reply.started":"2021-08-15T17:57:18.953361Z","shell.execute_reply":"2021-08-15T17:57:18.959816Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Hyperparameters optimization**","metadata":{}},{"cell_type":"code","source":"def train_model_optuna(trial, X_train, X_valid, y_train, y_valid):\n    \"\"\"\n    A function to train a model using different hyperparamerters combinations provided by Optuna. \n    Loss of validation data predictions is returned to estimate hyperparameters effectiveness.\n    \"\"\"\n    preds = 0\n    \n        \n    #A set of hyperparameters to optimize by optuna\n    xgb_params = {\n                 \"n_estimators\": trial.suggest_categorical('n_estimators', [40000]),\n                 \"learning_rate\": trial.suggest_float('learning_rate', 0.01, 1.0, step=0.01),\n                 \"subsample\": trial.suggest_float('subsample', 0.5, 1, step=0.01),\n                 \"colsample_bytree\": trial.suggest_float('colsample_bytree', 0.1, 1, step=0.01),\n                 \"max_depth\": trial.suggest_int(\"max_depth\", 1, 16),\n                 \"booster\": trial.suggest_categorical('booster', [\"gbtree\"]),\n                 \"tree_method\": trial.suggest_categorical('tree_method', [\"gpu_hist\"]),\n                 \"reg_lambda\": trial.suggest_float('reg_lambda', 0.2, 100, step=0.1),\n                 \"reg_alpha\": trial.suggest_float('reg_alpha', 0.1, 50, step=0.1),\n                 \"random_state\": trial.suggest_categorical('random_state', [42]),\n                 \"n_jobs\": trial.suggest_categorical('n_jobs', [4]),\n#                  \"min_child_weight\": trial.suggest_int(\"min_child_weight\", 10, 30),\n                    }\n\n    # Model loading and training\n    model = XGBRegressor(**xgb_params)\n    model.fit(X_train, y_train,\n              eval_set=[(X_train, y_train), (X_valid, y_valid)],\n              eval_metric=\"rmse\",\n              early_stopping_rounds=100,\n              verbose=False)\n    \n    print(f\"Number of boosting rounds: {model.best_iteration}\")\n    oof = model.predict(X_valid)\n    oof[oof<0] = 0\n    \n    return np.sqrt(mean_squared_error(y_valid, oof))","metadata":{"execution":{"iopub.status.busy":"2021-08-15T17:57:18.962525Z","iopub.execute_input":"2021-08-15T17:57:18.962881Z","iopub.status.idle":"2021-08-15T17:57:18.97403Z","shell.execute_reply.started":"2021-08-15T17:57:18.962844Z","shell.execute_reply":"2021-08-15T17:57:18.973124Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The code below is commented in order to save runtime.","metadata":{}},{"cell_type":"code","source":"# %%time\n# # Splitting data into train and valid folds using target bins for stratification\n# split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n# for train_idx, valid_idx in split.split(X, target_bins):\n#     X_train, X_valid = X.loc[train_idx], X.loc[valid_idx]\n#     y_train, y_valid = y.loc[train_idx], y.loc[valid_idx]\n# # Setting optuna verbosity to show only warning messages\n# # If the line is uncommeted each iteration results will be shown\n# # optuna.logging.set_verbosity(optuna.logging.WARNING)\n# time_limit = 3600 * 4\n# study = optuna.create_study(direction='minimize')\n# study.optimize(lambda trial: train_model_optuna(trial, X_train, X_valid,\n#                                                     y_train, y_valid),\n# #                n_trials = 100,\n#                timeout=time_limit\n#               )\n\n# # Showing optimization results\n# print('Number of finished trials:', len(study.trials))\n# print('Best trial parameters:', study.best_trial.params)\n# print('Best score:', study.best_value)","metadata":{"execution":{"iopub.status.busy":"2021-08-15T17:57:18.97518Z","iopub.execute_input":"2021-08-15T17:57:18.975517Z","iopub.status.idle":"2021-08-15T17:57:18.987752Z","shell.execute_reply.started":"2021-08-15T17:57:18.975481Z","shell.execute_reply":"2021-08-15T17:57:18.986739Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Model training**","metadata":{}},{"cell_type":"code","source":"# Hyperparameters optimized by Optuna\n\nxgb_params = {'n_estimators': 40000,\n              'learning_rate': 0.01,\n              'subsample': 0.72,\n              'colsample_bytree': 0.66,\n              'max_depth': 6,\n              'booster': 'gbtree',\n              'tree_method': 'gpu_hist',\n              'reg_lambda': 68.5,\n              'reg_alpha': 21.5,\n              'random_state': 42,\n              'n_jobs': 4}","metadata":{"execution":{"iopub.status.busy":"2021-08-15T17:57:18.989147Z","iopub.execute_input":"2021-08-15T17:57:18.989629Z","iopub.status.idle":"2021-08-15T17:57:18.99925Z","shell.execute_reply.started":"2021-08-15T17:57:18.989568Z","shell.execute_reply":"2021-08-15T17:57:18.998476Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nsplits = 10\nskf = StratifiedKFold(n_splits=splits, shuffle=True, random_state=42)\noof_preds = np.zeros((X.shape[0],))\npreds = 0\nmodel_fi = 0\ntotal_mean_rmse = 0\n\nfor num, (train_idx, valid_idx) in enumerate(skf.split(X, target_bins)):\n    X_train, X_valid = X.loc[train_idx], X.loc[valid_idx]\n    y_train, y_valid = y.loc[train_idx], y.loc[valid_idx]\n    \n    model = XGBRegressor(**xgb_params)\n    model.fit(X_train, y_train,\n              eval_set=[(X_train, y_train), (X_valid, y_valid)],\n              eval_metric=\"rmse\",\n              early_stopping_rounds=100,\n              verbose=False)\n    \n    preds += model.predict(X_test) / splits\n    model_fi += model.feature_importances_\n    oof_preds[valid_idx] = model.predict(X_valid)\n    oof_preds[oof_preds < 0] = 0\n#     fold_rmse = np.sqrt(mean_squared_error(y_scaler.inverse_transform(np.array(y_valid).reshape(-1,1)), y_scaler.inverse_transform(np.array(oof_preds[valid_idx]).reshape(-1,1))))\n    fold_rmse = np.sqrt(mean_squared_error(y_valid, oof_preds[valid_idx]))\n    print(f\"Fold {num} RMSE: {fold_rmse}\")\n#         print(f\"Trees: {model.tree_count_}\")\n    total_mean_rmse += fold_rmse / splits\nprint(f\"\\nOverall RMSE: {total_mean_rmse}\")    ","metadata":{"execution":{"iopub.status.busy":"2021-08-15T17:57:19.000443Z","iopub.execute_input":"2021-08-15T17:57:19.00082Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Feature importances**","metadata":{}},{"cell_type":"code","source":"df = pd.DataFrame(columns=[\"Feature\", \"Importance\"])\ndf[\"Feature\"] = X.columns\ndf[\"Importance\"] = model_fi / model_fi.sum()\ndf.sort_values(\"Importance\", axis=0, ascending=False, inplace=True)\n\nx = np.arange(0, len(df[\"Feature\"]))\nheight = 0.4\n\nfig, ax = plt.subplots(figsize=(16, 30))\nbars1 = ax.barh(x, df[\"Importance\"], height=height,\n                color=\"mediumorchid\", edgecolor=\"black\")\nax.set_title(\"Feature importances\", fontsize=30, pad=15)\nax.set_ylabel(\"Feature names\", fontsize=20, labelpad=15)\nax.set_xlabel(\"Feature importance\", fontsize=20, labelpad=15)\nax.set_yticks(x)\nax.set_yticklabels(df[\"Feature\"], fontsize=15)\nax.tick_params(axis=\"x\", labelsize=15)\nax.grid(axis=\"x\")\nax2 = ax.secondary_xaxis('top')\nax2.set_xlabel(\"Feature importance\", fontsize=20, labelpad=15)\nax2.tick_params(axis=\"x\", labelsize=15)\nplt.margins(0.04, 0.01)\nplt.gca().invert_yaxis()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Submission**","metadata":{}},{"cell_type":"code","source":"predictions = pd.DataFrame()\npredictions[\"id\"] = test[\"id\"]\npredictions[\"loss\"] = preds\n\npredictions.to_csv('submission.csv', index=False, header=predictions.columns)\npredictions.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}