{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# <font color=redred> <b><center> Shopee - Price Match Guarantee </font>\n<font><center>![](https://storage.googleapis.com/kaggle-competitions/kaggle/24286/logos/thumb76_76.png?t=2020-11-20-21-03-50)","metadata":{}},{"cell_type":"markdown","source":"# 0. Preparation:","metadata":{}},{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport cudf,cuml,cupy\n#from cuml.feature_extraction.text import CountVectorizer\n#from cuml.cluster import KMeans\n#from cuml.manifold import TSNE\nimport matplotlib.pyplot as plt \nimport os \nimport tensorflow as tf \nimport cv2 as cv \nfrom PIL import Image\nimport seaborn as sns\nimport plotly.express as px\nimport string\nimport re\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.manifold import TSNE\nfrom sklearn.cluster import KMeans\nfrom sklearn.neighbors import NearestNeighbors\nimport warnings\nfrom tqdm import tqdm\nimport gc\nwarnings.filterwarnings(\"ignore\")\nfrom tensorflow.keras.utils import Sequence\nfrom tensorflow.keras.applications import EfficientNetB0\nfrom tensorflow.keras.preprocessing.image import load_img , img_to_array\nfrom sklearn.metrics.pairwise import cosine_distances\nfrom collections import Counter\nimport nltk","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load data for training sets\ntraining_csv =pd.read_csv(\"../input/shopee-product-matching/train.csv\")\ntraining_img = \"../input/shopee-product-matching/train_images\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load data for test sets\ntest_csv = pd.read_csv(\"../input/shopee-product-matching/test.csv\")\ntest_img = \"../input/shopee-product-matching/test_images\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get look at the datas.\ntraining_csv.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 1. EDA:","metadata":{}},{"cell_type":"code","source":"# explorate number duplicates per label_group\n# And visualize the most 100 duplicated label_group\nser= training_csv[\"label_group\"].value_counts()\nser = pd.DataFrame(ser).reset_index(drop=False).rename(columns={\"label_group\":\"Occurences\",\"index\":\"label_group\"})\nser[\"label_group\"] = ser[\"label_group\"].astype(\"str\")\nfig = px.bar(ser[:100],x=\"label_group\",y=\"Occurences\",color= \"Occurences\",\\\n             title=\"TOP 100 most duplicated label_group \")\nfig.update_layout(title ={\"x\":0.475,\"y\":0.9,\"xanchor\":\"center\",\"yanchor\":\"top\"})\nfig.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create new feature who summarize the list of posting_id associated to each label_group value. \nlab=training_csv.groupby(\"label_group\")[\"posting_id\"].agg(\"unique\")\ntraining_csv[\"target\"]= training_csv.label_group.map(lab)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def display_related_products(feature_value,df=training_csv,feature=\"label_group\",title=\"\"):\n    \"\"\" Display related photos based on the introduced feature criterion and the value of this\n        feature introduced as parametr.\n      @ args :\n      feature_value(int) : \n      df(DataFrame) : the dataframe that we will use in our function (default = training_csv)\n      feature(str) : the name of feature, used to group items in the same collection.\n      title (str) : The title to assigne to the whole displayed images \n        \n    \"\"\"\n    \n    related_photos = df.loc[df[feature] ==feature_value,[\"image\",\"title\"]]\n    l = len(related_photos)\n    range_k = [2,3,4,5,6,7]\n    k = 0\n    for j in range_k :\n        if l%j == 0 :\n            k=j\n            break\n    if k == 0 :\n        k = 5\n    nb_l = l //k\n    nb_l += int((l%k) !=0)\n         \n    fig,ax = plt.subplots(nb_l,k,figsize=(k*10,nb_l*10))\n    i = 0\n    for row in related_photos.iterrows() :\n        \n        chemin = os.path.join(training_img,row[1][\"image\"])\n        image = Image.open(chemin)\n        image = np.array(image)\n        if nb_l == 1 :\n            ax[i%k].imshow(image)\n            ax[i%k].set_title(row[1][\"title\"],fontsize=10,fontweight=\"bold\")\n        else :\n            ax[i//k,i%k].imshow(image)\n            ax[i//k,i%k].set_title(row[1][\"title\"],fontsize=12,fontweight=\"bold\")\n        i += 1\n    plt.suptitle(title,fontsize =36,\\\n                 size=32,color=\"red\",fontweight=\"bold\")\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Display related images for the most duplicated label_group items.\nmost_label_group = int(ser.iloc[0].label_group)\ndisplay_related_products(most_label_group,title=\"RELATED IMAGES FOR THE MOST DUPLICATED LABEL_GROUP ITEMS\")","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Display related images for the smallest duplicated label_group items.\nsmallest_label_group = int (ser.iloc[-1].label_group)\ndisplay_related_products(smallest_label_group,title=\"SMALLEST DUPLICATED LABEL_GROUP ITEMS\")","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can notice from images above :\n- Many related  items have same photos and approximately same title.\n- Many differents images of the same product , but with approximately the same title.\n- Some images are idtentiques or very similar with differents title.\n\n==> This lead us to conclude , that the photos and title should used to determine duplicated products.","metadata":{}},{"cell_type":"markdown","source":"# 2.Modelisation","metadata":{}},{"cell_type":"code","source":"def clean(title):\n    \"\"\"This function, allows to clean title from useless characters and symbols.\n    \n    @ params :\n    title(str) : the title text that the function will clean up.\n    \n    @ returns :\n    title(str) : cleaned title\n\n    \n    \"\"\"\n    title = title.lower()\n    title = re.sub(r\"\\-\",\" \",title)\n    title = re.sub(r\"\\+\",\" \",title)\n    title = re.sub (r\"&\",\"and\",title)\n    title = re.sub(r\"\\|\",\" \",title)\n    title = re.sub(r\"\\\\\",\" \",title)\n    title = re.sub(r\"\\W\",\" \",title)\n    for p in string.punctuation :\n        title = re.sub(r\"f{p}\",\" \",title)\n    \n    title = re.sub(r\"\\s+\",\" \",title)\n    \n    return title","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_cdf = cudf.read_csv(\"../input/shopee-product-matching/train.csv\")\ntest = training_csv\ntest[\"cleaned_title\"] = test[\"title\"].map(clean)\ntest_cdf[\"cleaned_title\"] = test[\"cleaned_title\"]\n#test_cdf = cudf.concat([test_cdf,test_cdf,test_cdf[:],axis=0,ignore_index=False)\n#test = pd.concat([test,test,test[:4000]],axis=0,ignore_index=False)\nsubmission = True \nimages = training_img\nif len(test_csv) > 3 :\n   test_cdf = cudf.read_csv(\"../input/shopee-product-matching/test.csv\")\n   test = pd.read_csv(\"../input/shopee-product-matching/test.csv\")\n   test[\"cleaned_title\"] = test[\"title\"].map(clean)\n   test_cdf[\"cleaned_title\"] = test[\"cleaned_title\"]\n   images = test_img\n   submission = False","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def getMetric(col):\n    def f1score(row):\n        n = len( np.intersect1d(row.target,row[col]) )\n        return 2*n / (len(row.target)+len(row[col]))\n    return f1score","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def sub_matches(row):\n    return \" \".join(row.pred_tf)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2.2 tf_idf :","metadata":{}},{"cell_type":"code","source":"corpus = []\nfor tx in test[\"cleaned_title\"].values:\n    text = tx.lower()\n    corpus.extend(text.split())\nwords = set(corpus)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nuniques_words = len(words)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"counter = Counter(corpus)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"seuil = [0.01,0.025,0.05,0.1,0.2 ,0.4 ,0.6 ]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from cuml.feature_extraction.text import TfidfVectorizer\nfrom cuml.neighbors import NearestNeighbors\nif submission :\n bseuil = 0\n bscore = 0\n bd = 0\n for sl in tqdm(seuil) : \n    seuil = int (sl * nuniques_words)\n    stop_words = list(zip(*counter.most_common(seuil)))[0]\n    sw = set()\n    sw.update(stop_words)\n    sw.update(nltk.corpus.stopwords.words(\"english\"))\n    tf_idf = TfidfVectorizer(stop_words=stop_words,max_features=25000,binary=True)\n    embedding = tf_idf.fit_transform(test_cdf[\"cleaned_title\"]).toarray()\n    tf_distance = NearestNeighbors(n_neighbors=50,metric=\"cosine\")\n    tf_distance.fit(embedding)\n    chunk = 4 * 1024\n    cls = len(test)//chunk\n    cls += int((len(test)% chunk) != 0)\n    d = [0.15,0.2,0.25,0.3,0.35,0.4,0.45,0.5,0.55,0.6]\n    best_d = 0\n    best_score = 0\n    for di in d :\n     prediction = []\n     for i in range(cls) :\n       a = i * chunk \n       b = (i+1) * chunk \n       b = min(b,len(test))\n       distances , indices = tf_distance.kneighbors(embedding[a:b,])\n       for j in range(b-a) :\n         distance = distances[j,:]\n         ind = np.where(distance < di )[0]\n         ind = indices[j,ind]\n         ind = cupy.asnumpy(ind)\n         prediction.append(test.iloc[ind].posting_id.values)\n     test[\"pred_tfidf\"] = prediction \n     test[\"f5\"] = test.apply(getMetric(\"pred_tfidf\"),axis=1)\n     sc = test.f5.mean()\n     if sc > best_score :\n            best_score = sc \n            best_d = di \n    if best_score >  bscore :\n     bseuil =  sl \n     bscore = best_score\n     bd = best_d\n            \n            ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if submission : \n    print('CV score for tf_idf embedding text = ',bscore)\n    print(\"best threshold to use to define our stops words= \",bseuil)\n    print(\"best distance to use to define similarity = \",bd)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"stop_words = list(zip(*counter.most_common(int(0.01 * nuniques_words))))[0]\nsw = set()\nsw.update(stop_words)\nsw.update(nltk.corpus.stopwords.words(\"english\"))\ntf_idf = TfidfVectorizer(stop_words=sw,max_features=25000,binary=True)\nembedding = tf_idf.fit_transform(test_cdf[\"cleaned_title\"]).toarray()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"kn = NearestNeighbors(n_neighbors=50,metric=\"cosine\")\nkn.fit(embedding)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prediction = []\nchunk = 4 *1024 \ncls = len(test) // chunk \ncls += int((len(test) % chunk) !=0)\nfor i in tqdm(range(cls)):\n    a = i * chunk \n    b = (i+1) * chunk \n    b = min (b,len(test))\n    distances , indices = kn.kneighbors(embedding[a:b,])\n    for j in range(b-a):\n        distance = distances[j,:]\n        ind = np.where(distance < 0.45)[0]\n        ind = indices[j,ind]\n        ind = cupy.asnumpy(ind)\n        prediction.append(test.iloc[ind].posting_id.values)\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test[\"pred_tfidf\"] = prediction \nif submission : \n    test[\"f5\"] = test.apply(getMetric(\"pred_tfidf\"),axis=1)\n    \n    print('CV score for tf embedding text =',test.f5.mean())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Resnet0:","metadata":{}},{"cell_type":"code","source":"LIMIT = 1\ngpus = tf.config.experimental.list_physical_devices('GPU')\nif gpus :\n    try :\n       tf.config.experimental.set_virtual_device_configuration(gpus[0],\\\n                                                           [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=1024*LIMIT)])\n       logical_gpus = tf.config.experimental.list_logical_devices(\"GPU\")\n    \n    except RuntimeError as e :\n       print(e)\nprint('We will restrict TensorFlow to max %iGB GPU RAM'%LIMIT)\nprint('then RAPIDS can use %iGB GPU RAM'%(16-LIMIT))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class DataGenerator(Sequence):\n    \n    def __init__(self,df,img_size=224,path = images,batch_size = 32):\n        \n        self.df = df \n        self.img_size = img_size\n        self.path = path \n        self.batch_size = batch_size\n        self.indexes = np.arange(len(self.df))\n    def __len__(self) :\n        \n        cl = len(self.df) // self.batch_size\n        cl += int((len(self.df) % self.batch_size) !=0)\n        return cl\n    def __getitem__(self,index):\n        \n        indices = self.indexes[index * self.batch_size :(index + 1) * self.batch_size]\n        X = self.__data_generation(indices)\n        return X\n    def __data_generation(self,indices) :\n        \n        images = np.zeros((len(indices),self.img_size,self.img_size,3),dtype = \"float32\")\n        ddf = self.df.iloc[indices]\n        for i , (j,row) in enumerate(ddf.iterrows()):\n            img = cv.imread(os.path.join(self.path,row.image))\n            #img = load_img(os.path.join(self.path,row.image),target_size = (self.img_size,self.img_size))\n            #img = img_to_array(img)\n            img = cv.resize(img,(self.img_size,self.img_size))\n            images[i,] = img\n        return images \n            \n        ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"WGT = \"../input/effnetb0/efficientnetb0_notop.h5\"\nmodel = EfficientNetB0(weights=WGT,input_shape=None,include_top = False,pooling=\"avg\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"chunk = 1024 * 4 \ncls = len(test) // chunk \ncls += int (len(test) % chunk != 0)\nimage_embedding = []\nfor i in tqdm(range(cls)) :\n    \n    a = i * chunk \n    b = (i+1) * chunk \n    b = min(b,len(test))\n    data = DataGenerator(test.iloc[a:b])\n    emb = model.predict(data,use_multiprocessing=True,workers = 4)\n    image_embedding.append(emb)\n\ndel(model)\nimage_embedding = np.concatenate(image_embedding,axis=0)\ngc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#from numpy.linalg.linalg import norm\n#Norm = norm(image_embedding,axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Normed_embedding = image_embedding/Norm.reshape(-1,1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = NearestNeighbors(n_neighbors=50,metric=\"cosine\")\nmodel.fit(image_embedding)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"chunk = 4 *1024 \ncl = len(test) // chunk \ncl += int((len(test) % chunk) !=0)\npred_img = []\nfor i in tqdm(range(cl)) :\n    a = i * chunk\n    b = (i+1) * chunk\n    b = min(len(test),b)\n    distances,indices = model.kneighbors(image_embedding[a:b,])\n    for j in range(b-a):\n        distance = distances[j,:]\n        #d = distance[distance !=0]\n        #minim = float(np.min(d)) * 10\n        ind = np.where(distance < 0.2)[0]\n        IND = indices[j,ind]\n        pred_img.append(test.iloc[IND].posting_id.values)\ntest[\"pred_img\"] = pred_img","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if submission :\n    \n    test[\"f2\"] = test.apply(getMetric(\"pred_img\"),axis=1)\n    \n    print('CV score for tf embedding image =',test.f2.mean())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## image_phash related images:","metadata":{}},{"cell_type":"code","source":"image_phash = test.groupby(\"image_phash\").posting_id.unique()\ntest[\"pred_phash\"] = test.image_phash.map(image_phash)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def combine(row):\n    x = np.concatenate([row.pred_img,row.pred_tfidf,row.pred_phash])\n   \n    return np.unique(x)\ndef combine_matches(row):\n    return \" \".join(row.pred)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if submission :\n    \n    test[\"f3\"] = test.apply(getMetric(\"pred_phash\"),axis=1)\n    \n    print('CV score for tf image phash related image =',test.f3.mean())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test[\"pred\"] = test.apply(combine,axis=1)\nif submission :\n    \n    test[\"f\"] = test.apply(getMetric(\"pred\"),axis=1)\n    \n    print('CV score for baseline =',test.f.mean())\ntest[\"matches\"] = test.apply(combine_matches,axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test[[\"posting_id\",\"matches\"]].to_csv(\"submission.csv\",index = False)\nsub = pd.read_csv('submission.csv')\nsub.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <font color=red> We came back soon , please upvote if you like it !","metadata":{}}]}