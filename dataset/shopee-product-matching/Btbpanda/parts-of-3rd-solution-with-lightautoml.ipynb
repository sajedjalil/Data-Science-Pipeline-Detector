{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Parts of 3rd Place solution\n\nThis is simplified version of my solution on 3rd place (train + inference) without trained embeddings, but here I use LightAutoML instead of catboost.  \nAlso I increase individual embeddings cutoffs to make models trainable with Kaggle Kernels memory limit.\n\n\nGeneral approach is described here https://www.kaggle.com/c/shopee-product-matching/discussion/238515\n\n\nEven simplified, it steel scores in Gold range) Good luck!","metadata":{"papermill":{"duration":0.027778,"end_time":"2021-06-23T10:51:37.478925","exception":false,"start_time":"2021-06-23T10:51:37.451147","status":"completed"},"tags":[]}},{"cell_type":"code","source":"%%time\n!pip install ../input/lama-whl/efficientnet_pytorch-0.7.0/dist/efficientnet_pytorch-0.7.0.tar ../input/lama-whl/log_calls-0.3.2/log_calls-0.3.2/ ../input/lama-whl/sphinxcontrib_devhelp-1.0.2-py2.py3-none-any.whl ../input/lama-whl/sphinxcontrib_htmlhelp-1.0.3-py2.py3-none-any.whl ../input/lama-whl/sphinxcontrib_jsmath-1.0.1-py2.py3-none-any.whl ../input/lama-whl/sphinxcontrib_qthelp-1.0.3-py2.py3-none-any.whl ../input/lama-whl/sphinxcontrib_applehelp-1.0.2-py2.py3-none-any.whl ../input/lama-whl/sphinxcontrib_serializinghtml-1.1.4-py2.py3-none-any.whl ../input/lama-whl/importlib_metadata-1.7.0-py2.py3-none-any.whl ../input/lama-whl/poetry_core-1.0.3-py2.py3-none-any.whl ../input/lama-whl/imagesize-1.2.0-py2.py3-none-any.whl ../input/lama-whl/docutils-0.16-py2.py3-none-any.whl ../input/lama-whl/alabaster-0.7.12-py2.py3-none-any.whl ../input/lama-whl/snowballstemmer-2.1.0-py2.py3-none-any.whl ../input/lama-whl/Sphinx-3.5.4-py3-none-any.whl ../input/lama-whl/sphinx_autodoc_typehints-1.11.1-py3-none-any.whl ../input/lama-whl/nbsphinx-0.8.0-py3-none-any.whl ../input/lama-whl/nbsphinx_link-1.3.0-py2.py3-none-any.whl ../input/lama-whl/cssselect-1.1.0-py2.py3-none-any.whl ../input/lama-whl/pyquery-1.4.3-py3-none-any.whl ../input/lama-whl/chuanconggao-html2json-0.2.4.1-0-g99d7fbb/chuanconggao-html2json-99d7fbb/ ../input/lama-whl/json2html-1.3.0/json2html-1.3.0 ../input/lama-whl/lightgbm-2.3.1-py2.py3-none-manylinux1_x86_64.whl ../input/lama-whl/AutoWoE-1.2.1-py3-none-any.whl ../input/lama-whl/LightAutoML-0.2.14-py3-none-any.whl","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.execute_input":"2021-06-23T10:51:37.614624Z","iopub.status.busy":"2021-06-23T10:51:37.613799Z","iopub.status.idle":"2021-06-23T10:52:27.444838Z","shell.execute_reply":"2021-06-23T10:52:27.445374Z"},"papermill":{"duration":49.940117,"end_time":"2021-06-23T10:52:27.445703","exception":false,"start_time":"2021-06-23T10:51:37.505586","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n!pip install /kaggle/input/nvidia-dali/nvidia_dali_cuda100-1.1.0-2239998-py3-none-manylinux2014_x86_64.whl","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"_kg_hide-output":true,"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2021-06-23T10:52:27.541997Z","iopub.status.busy":"2021-06-23T10:52:27.537792Z","iopub.status.idle":"2021-06-23T10:53:03.120273Z","shell.execute_reply":"2021-06-23T10:53:03.1209Z"},"papermill":{"duration":35.631394,"end_time":"2021-06-23T10:53:03.121105","exception":false,"start_time":"2021-06-23T10:52:27.489711","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nimport sys\n!cp -r ../input/clip-pretrained/CLIP/CLIP-main /tmp/\n\n# Kaggle likes to unpack .gz files in datasets... so we have to pack it back\n!gzip -c /tmp/CLIP-main/clip/bpe_simple_vocab_16e6.txt > /tmp/CLIP-main/clip/bpe_simple_vocab_16e6.txt.gz\nsys.path.append('/tmp/CLIP-main')\n!cp -r /tmp/CLIP-main/clip /opt/conda/lib/python3.7/site-packages/\n\n!pip install ../input/clip-pretrained/ftfy-5.9/ftfy-5.9 \\\n             ../input/clip-pretrained/torchvision-0.8.2+cu110-cp37-cp37m-linux_x86_64.whl \\\n             ../input/clip-pretrained/torch-1.7.1+cu110-cp37-cp37m-linux_x86_64.whl ","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.execute_input":"2021-06-23T10:53:03.218741Z","iopub.status.busy":"2021-06-23T10:53:03.218116Z","iopub.status.idle":"2021-06-23T10:54:35.500007Z","shell.execute_reply":"2021-06-23T10:54:35.500797Z"},"papermill":{"duration":92.336481,"end_time":"2021-06-23T10:54:35.501027","exception":false,"start_time":"2021-06-23T10:53:03.164546","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"EFFNET_PATH = '../input/shopee-effnet/effnet_b2.pth'\nID_BERT_PATH = '../input/shopee-id-bert/'\nML_PATH = '../input/shopee-ml/'\n\nDEBUG = False\n\nif DEBUG:\n    test = 'train'\nelse:\n    test = 'test'","metadata":{"execution":{"iopub.execute_input":"2021-06-23T10:54:35.666516Z","iopub.status.busy":"2021-06-23T10:54:35.665626Z","iopub.status.idle":"2021-06-23T10:54:35.668693Z","shell.execute_reply":"2021-06-23T10:54:35.669336Z"},"papermill":{"duration":0.087608,"end_time":"2021-06-23T10:54:35.669538","exception":false,"start_time":"2021-06-23T10:54:35.58193","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import sys\n\nsys.path.append('../input/shopee-effnet/efficientnet_pytorch-0.7.0/')\n\nimport numpy as np\nimport pandas as pd\nimport joblib\n\nimport tqdm\n\nimport torch\nimport os\nimport gc\nimport networkx as nx\nimport treelite\nimport json\nimport cuml\nimport clip\nimport catboost as cb\n\nimport nvidia.dali.ops as ops\nimport nvidia.dali.types as types\n\nfrom pandas import Series, DataFrame\nfrom efficientnet_pytorch import EfficientNet\nfrom transformers import AlbertTokenizer, AlbertModel, BertTokenizer, BertModel, \\\n        AutoModel, AutoTokenizer, BertTokenizerFast\n\nfrom lightautoml.automl.presets.tabular_presets import TabularAutoML\nfrom lightautoml.tasks import Task\n\nfrom copy import deepcopy\n\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n\n\nfrom nvidia.dali.pipeline import Pipeline\nfrom nvidia.dali.plugin.pytorch import DALIGenericIterator\nfrom torch.utils.data import DataLoader, Dataset\nfrom torch.cuda.amp import autocast","metadata":{"execution":{"iopub.execute_input":"2021-06-23T10:54:35.829421Z","iopub.status.busy":"2021-06-23T10:54:35.828661Z","iopub.status.idle":"2021-06-23T10:54:44.413315Z","shell.execute_reply":"2021-06-23T10:54:44.412127Z"},"papermill":{"duration":8.667759,"end_time":"2021-06-23T10:54:44.41347","exception":false,"start_time":"2021-06-23T10:54:35.745711","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Define utils","metadata":{"papermill":{"duration":0.045553,"end_time":"2021-06-23T10:54:44.505585","exception":false,"start_time":"2021-06-23T10:54:44.460032","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def get_validation_folds(df, nfolds=5, random_state=42):\n    \"\"\"\n    Function to create validation folds. Split not only by label group, but also by title, image, phash\n    \"\"\"\n    np.random.seed(random_state)\n    G = nx.Graph()\n\n    for col in ['label_group', 'title', 'image_phash', 'image']:\n\n        agg = df.groupby(col)['posting_id'].agg(list).tolist()\n        for p in agg:\n            nx.add_path(G, p)\n\n    cc = {}\n    for n, c in enumerate(nx.connected_components(G)):\n        val = min(c)\n        for x in c:\n            cc[x] = val\n\n    group_idx = df['posting_id'].map(cc).values\n    groups = np.unique(group_idx)\n    np.random.shuffle(groups)\n\n    split = np.array_split(groups, nfolds)\n\n    folds = np.zeros(df.shape[0], dtype=np.int32)\n\n    for n, s in enumerate(split):\n        folds[np.isin(group_idx, s)] = n\n\n    return folds\n\n\ndef phash_to_bag(x):\n    \"\"\"\n    Transform single phash to OHE representation\n    \"\"\"\n    res = np.zeros(16 * 16, dtype=np.int32)\n\n    for n, i in enumerate(x):\n        res[int(i, 16) + n * 16] += 1\n\n    return res / ((res ** 2).sum() ** .5)\n\n\ndef get_phash_embed(df):\n    \"\"\"\n    Transform df phash to OHE representation\n    \"\"\"\n    embed = np.stack(df['image_phash'].map(phash_to_bag).tolist())\n    return embed.astype(np.float32)\n\n\ndef union_pred(*preds):\n    \"\"\"\n    Union preds from different embeds\n    \"\"\"\n    res = []\n\n    for pp in zip(*preds):\n        row = []\n        for p in pp:\n            row.extend(list(p))\n\n        row = list(set(row))\n        res.append(row)\n\n    return res\n\ndef get_dist_features(D):\n    \"\"\"\n    Get density features for embed point\n    \"\"\"\n    features = []\n    for i in [2, 3, 5, 10, 20, 50]:\n        features.append(D[:, 1: i].mean(axis=1))\n\n    for i in [.5, .6, .7, .8, .9, .95, .97, .99]:\n        features.append((D >= i).sum(axis=1))\n\n    return np.stack(features, axis=1).astype(np.float32)\n\n\ndef get_paired_indexes(pred, folds=None):\n    \"\"\"\n    Create points pairs candidates. First half of pairs is left/right pairs, second part is reflection - right/left\n    \"\"\"\n    left, right, fold = [], [], []\n\n    if folds is None:\n        folds = np.zeros(len(pred))\n\n    added_pairs = set(zip(left, right))\n\n    for n, (pp, f) in enumerate(zip(pred, folds)):\n        for p in pp:\n            if n != p and (p, n) not in added_pairs:\n                left.append(n)\n                right.append(p)\n                fold.append(f)\n                added_pairs.add((n, p))\n\n    # add reversed pairs\n    lc, rc = left.copy(), right.copy()\n    left.extend(rc)\n    right.extend(lc)\n    fold.extend(fold.copy())\n\n    return np.array(left), np.array(right), np.array(fold)\n\n\ndef get_pairwise_dist(left, right, embed, folds=None, batch_size=20000):\n    \"\"\"\n    Compute pairwise distance features\n    \"\"\"\n    res = np.zeros((left.shape[0], 6), dtype=np.float32)\n    embed = torch.from_numpy(embed).cuda()\n\n    q_int = (np.array([0.25, 0.975]) * (embed.shape[1] - 1)).astype(np.int32)\n    f = None\n\n    for i in range(0, left.shape[0], batch_size):\n        left_embed, right_embed = embed[left[i: i + batch_size]], embed[right[i: i + batch_size]]\n        # in case of out-of-fold embeddings\n        if folds is not None and len(embed.shape) == 3:\n            f = torch.from_numpy(folds[i: i + batch_size].astype(np.int64)\n                                 ).view(-1, 1, 1).cuda().repeat(1, embed.shape[1], 1)\n            left_embed = torch.gather(left_embed, dim=2, index=f).squeeze(dim=2)\n            right_embed = torch.gather(right_embed, dim=2, index=f).squeeze(dim=2)\n\n        for n, fn in enumerate([lambda x, y: x * y, lambda x, y: (x - y) ** 2]):\n            coords = fn(left_embed, right_embed)\n            # distance\n            Dist = coords.sum(dim=1)\n            if len(Dist.shape) == 2:\n                Dist = Dist.mean(dim=1)\n\n            res[i: i + batch_size, n * 3] = Dist.cpu().numpy()\n            # quantiles\n            idx = coords.argsort(dim=1)[:, q_int]\n            Qs = torch.gather(coords, dim=1, index=idx)\n            if len(Qs.shape) == 3:\n                Qs = Qs.mean(dim=2)\n            res[i: i + batch_size, (n * 3) + 1: (n * 3) + 3] = Qs.cpu().numpy()\n\n    del embed, left_embed, right_embed, coords, f, Dist, Qs, idx\n    torch.cuda.empty_cache()\n    gc.collect()\n\n    return res\n\n\ndef get_cross_pairwise_dist(left, right, embed_pair, batch_size=20000):\n    \"\"\"\n    Compute text to image/image to text pairwise features (for CLIP embedding)\n    \"\"\"\n    res = np.zeros((left.shape[0], 4), dtype=np.float32)\n    embed_pair = [torch.from_numpy(x).cuda() for x in embed_pair]\n\n    for n, emb0 in enumerate(embed_pair):\n        for k, emb1 in enumerate(embed_pair):\n            for i in range(0, left.shape[0], batch_size):\n                left_embed, right_embed = emb0[left[i: i + batch_size]], emb1[right[i: i + batch_size]]\n                res[i: i + batch_size, n * 2 + k] = (left_embed * right_embed).sum(dim=1).cpu().numpy()\n\n    del embed_pair, emb0, emb1, left_embed, right_embed\n    torch.cuda.empty_cache()\n    gc.collect()\n\n    return res\n\n\ndef get_sparse_pairwise_dist(left, right, embed):\n    \"\"\"\n    Get pairwise features for sparse embedding (tfidf)\n    \"\"\"\n    left = embed[left]\n    right = embed[right]\n\n    return np.array(left.multiply(right).sum(axis=1))\n\n\ndef get_pairwise(left, right, *embeds, folds=None, scores_only=False):\n    \"\"\"\n    Get pairwise features for embeddings list\n    \"\"\"\n    feats = []\n\n    for embed in embeds:\n        # for inference - not to keep embeddings in memory\n        if type(embed) is str:\n            embed = joblib.load(embed)\n\n        if isinstance(embed, np.ndarray):\n            pairw = get_pairwise_dist(left, right, embed, folds=folds, batch_size=10000)\n        elif isinstance(embed, list):\n            pairw = get_cross_pairwise_dist(left, right, embed, batch_size=10000)\n        else:\n            pairw = get_sparse_pairwise_dist(left, right, embed)\n\n        if scores_only:\n            pairw = pairw[:, [0]]\n\n        feats.append(pairw)\n\n    return np.concatenate(feats, axis=1)\n\n\ndef get_features(left, right, texts, points_feats_list, embed_list, count_params, bypass_feats=None, folds=None):\n    \"\"\"\n    Get GBM features from left/right pairs idx\n    \"\"\"\n    feats = []\n\n    for points_feats in points_feats_list:\n        for index in [left, right]:\n            feats.append(points_feats[index])\n\n    feats.append(get_pairwise(left, right, *embed_list, folds=folds))\n\n    for param in count_params:\n        feats.append(get_length_features(left, right, texts, param))\n\n    if bypass_feats is not None:\n        feats.extend(bypass_feats)\n\n    return np.concatenate(feats, axis=1).astype(np.float32)\n\n\ndef get_prediction_index(left, right, prob, k, cutoff=0.3, hard_cutoff=False, exact_add=2):\n    \"\"\"\n    Transform paired prediction to list of product predictions\n    \"\"\"\n    res = [[x] for x in range(k)]\n    probs = [[1] for x in range(k)]\n    sl = prob > cutoff\n\n    for n, (l, r, flg) in enumerate(zip(left, right, sl)):\n        res[l].append(r)\n        probs[l].append(prob[n])\n\n    # calc proxy f1 score and decide - if it worth to add next point\n    # TODO: Check it later !!! small score decrease\n    for i in range(len(res)):\n        arr = res[i]\n\n        orders = np.array(probs[i]).argsort()[::-1][:50]\n        indexes = np.array(arr)[orders]\n        ps = np.array(probs[i])[orders]\n\n        result = []\n        prev_proxy_f1 = 0\n        proxy_total_yt = ps[ps > cutoff].sum()\n        proxy_total_inter = 0\n        len_yp = 0\n        for n, (idx, p) in enumerate(zip(indexes, ps[ps > cutoff])):\n\n            len_yp += 1\n            proxy_total_inter += 2 * p\n            proxy_f1 = proxy_total_inter / (len_yp + proxy_total_yt)\n\n            if (proxy_f1 >= prev_proxy_f1) or hard_cutoff:\n                result.append(idx)\n                prev_proxy_f1 = proxy_f1\n            else:\n                break\n\n        if len(result) < exact_add:\n            result = list(indexes[:exact_add])\n\n        res[i] = result\n        probs[i] = ps[:len(result)]\n\n    return res, probs\n\n\ndef cutoff_prediction(D, I, cutoff, exact_add=2):\n    \"\"\"\n    Cutoff prediction of distances/indices matrices\n    \"\"\"\n    res = []\n\n    ranger = np.arange(D.shape[1])\n\n    for d, i in zip(D, I):\n        res.append(i[(d > cutoff) | (ranger < exact_add)])\n    return res\n\n\ndef get_y_true(df):\n    \"\"\"\n    Get true prediction indices\n    \"\"\"\n    index = Series(np.arange(df.shape[0]))\n    grp = index.groupby(df['label_group'].values).agg(list)\n\n    return df['label_group'].map(grp).tolist()\n\n\ndef f1_score(y_true, y_pred):\n    \"\"\"\n    F1 score\n    \"\"\"\n    metric = 0\n\n    for yt, yp in zip(y_true, y_pred):\n        inter = np.intersect1d(yt, yp)\n        metric += 2 * len(inter) / (len(yt) + len(yp))\n\n    return metric / len(y_true)\n\n\ndef f1_score_co_search(y_true, D, I, cutoffs=None):\n    \"\"\"\n    F1 score with cutoff search for distances/indices matrices\n    \"\"\"\n    if cutoffs is None:\n        cutoffs = np.linspace(0.5, 1, 20)[:-1]\n\n    else:\n        cutoffs = np.sort(cutoffs)\n\n    metric = np.zeros_like(cutoffs)\n\n    for yt, yp, d in zip(y_true, I, D):\n        if not isinstance(yp, np.ndarray):\n            yp = np.array(yp)\n\n        sls = cutoffs[:, np.newaxis] <= d[np.newaxis, :]\n        sls_sum = sls.sum(axis=1)\n\n        prev_s = -np.inf\n        for n, (sl, s) in enumerate(zip(sls, sls_sum)):\n            # if we pass cutoff - switch slice and recalc metric\n            if s != prev_s:\n                yp_ = yp[sl]\n                inter = np.intersect1d(yt, yp_)\n                met_ = 2 * len(inter) / (len(yt) + len(yp_))\n                prev_s = s\n\n            metric[n] += met_\n\n    metric = metric / len(y_true)\n\n    best_val = metric.argmax()\n\n    return metric[best_val], cutoffs[best_val]\n\n\ndef get_di_torch(embed, n_candidates=50, batch_size=1000):\n    \"\"\"\n    Calc distances/indices matrices from embeddings\n    \"\"\"\n    D = np.zeros((embed.shape[0], n_candidates), dtype=np.float32)\n    I = np.zeros((embed.shape[0], n_candidates), dtype=np.int32)\n\n    flg_dense = isinstance(embed, np.ndarray)\n\n    if flg_dense:\n        embed_cuda = torch.from_numpy(embed).cuda()\n    else:\n        embed_cuda = csr_to_torch_sparse(embed).cuda()\n\n    for i in range(0, embed.shape[0], batch_size):\n\n        if flg_dense:\n            embed_batch = embed_cuda[i: i + batch_size]\n            d = torch.matmul(embed_cuda, embed_batch.T).T\n        else:\n            embed_batch = torch.from_numpy(embed[i: i + batch_size].toarray().T).cuda()\n            d = torch.matmul(embed_cuda, embed_batch).T\n\n        idx = torch.argsort(d, dim=1, descending=True)[:, :n_candidates]\n        I[i: i + batch_size, :idx.shape[1]] = idx.cpu().numpy()\n        D[i: i + batch_size, :idx.shape[1]] = torch.gather(d, 1, idx).cpu().numpy()\n\n    del d, idx, embed_cuda, embed_batch\n    torch.cuda.empty_cache()\n\n    return D, I\n\n\ndef get_cross_di_torch(embed_x, embed_y, n_candidates=50, batch_size=1000):\n    \"\"\"\n    Calc cross distances/indices matrices (for CLIP)\n    \"\"\"\n    D = np.zeros((embed_y.shape[0], n_candidates), dtype=np.float32)\n    I = np.zeros((embed_y.shape[0], n_candidates), dtype=np.int32)\n\n    flg_dense = isinstance(embed_x, np.ndarray)\n\n    if flg_dense:\n        embed_x = torch.from_numpy(embed_x).T.cuda()\n        embed_y = torch.from_numpy(embed_y).cuda()\n    else:\n        embed_x = csr_to_torch_sparse(embed_x).cuda()\n\n    for i in range(0, embed_y.shape[0], batch_size):\n\n        if flg_dense:\n            embed_batch = embed_y[i: i + batch_size]\n            # d = torch.matmul(embed_x, embed_batch.T).T\n            d = torch.matmul(embed_batch, embed_x)\n        else:\n            embed_batch = torch.from_numpy(embed_y[i: i + batch_size].toarray().T).cuda()\n            d = torch.matmul(embed_x, embed_batch).T\n\n        idx = torch.argsort(d, dim=1, descending=True)[:, :n_candidates]\n        I[i: i + batch_size, :idx.shape[1]] = idx.cpu().numpy()\n        D[i: i + batch_size, :idx.shape[1]] = torch.gather(d, 1, idx).cpu().numpy()\n\n    del d, idx, embed_x, embed_y, embed_batch\n    torch.cuda.empty_cache()\n\n    return D, I\n\n\n\ndef csr_to_torch_sparse(csr_mat):\n    \"\"\"\n    Transform csr matrix to torch Sparse format\n    \"\"\"\n    coo_mat = csr_mat.astype(np.float32).tocoo()\n\n    row = torch.from_numpy(coo_mat.row).type(torch.int64)\n    col = torch.from_numpy(coo_mat.col).type(torch.int64)\n    edge_index = torch.stack([row, col], dim=0)\n\n    val = torch.from_numpy(coo_mat.data)\n    out = torch.sparse.FloatTensor(edge_index, val, torch.Size(coo_mat.shape))\n\n    return out\n\n\nclass ReadPipeline(Pipeline):\n    \"\"\"\n    DALI Image read pipeline for torch\n    \"\"\"\n    def __init__(self, img_list, batch_size, img_size=300, num_threads=2, device_id=0, num_gpus=1, shuffle=False,\n                 name='Reader', hflip_p=0, scale=1, mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)):\n        super().__init__(batch_size, num_threads, device_id)\n\n        self.input = ops.FileReader(files=img_list, random_shuffle=False,\n                                    shard_id=device_id, shuffle_after_epoch=shuffle, num_shards=num_gpus)\n\n        self.decode = ops.ImageDecoder(device=\"mixed\", output_type=types.RGB)\n        self.resize = ops.Resize(device=\"gpu\", resize_shorter=img_size,\n                                 interp_type=types.INTERP_LINEAR)\n        self.cmn = ops.CropMirrorNormalize(device=\"gpu\",\n                                           dtype=types.FLOAT,\n                                           crop=(img_size, img_size),\n                                           mean=[255 * x for x in mean],\n                                           std=[255 * x for x in std])\n\n        self.uniform = ops.random.Uniform(range=(0.0, 1.0))\n        self.resize_rng = ops.random.Uniform(range=(300, int(img_size * scale) + 1))\n        self.coin = ops.random.CoinFlip(probability=hflip_p)\n\n        self.name = 'Reader'\n\n    def define_graph(self):\n        inputs, labels = self.input(name=self.name)\n        images = self.decode(inputs)\n        images = self.resize(images  # , resize_shorter=self.resize_rng()\n                             )\n        output = self.cmn(images, mirror=self.coin()\n                          # , out_of_bounds_policy='trim_to_shape', crop_pos_x=self.uniform(),\n                          # crop_pos_y=self.uniform()\n                          )\n        return (output, labels)\n\n\nclass DaliTorchLoader(DataLoader):\n    \"\"\"\n    Torch DataLoader with DALI ReadPipeline\n    \"\"\"\n    def __init__(self, image_list, image_size=300, batch_size=128, num_threads=2, n_gpus=1, shuffle=False,\n                 drop_last=False, mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225), name='Reader'):\n\n        self._initialize(image_list, image_size=image_size, batch_size=batch_size, num_threads=num_threads,\n                         n_gpus=n_gpus, shuffle=shuffle,\n                         drop_last=drop_last, mean=mean, std=std, name=name)\n\n    def _initialize(self, image_list, image_size=300, batch_size=128, num_threads=2, n_gpus=1, shuffle=False,\n                    drop_last=False, name='Reader', hflip_p=0, scale=1,\n                    mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)):\n\n        assert n_gpus == 1, 'For now only 1 GPU'\n\n        pipes = [ReadPipeline(image_list, batch_size=batch_size, img_size=image_size, num_threads=num_threads,\n                              device_id=device_id, num_gpus=n_gpus, shuffle=shuffle, name=name, hflip_p=hflip_p,\n                              scale=scale, mean=mean, std=std)\n                 for device_id in range(n_gpus)]\n\n        self.length = len(image_list) // batch_size\n        self.last_batch = len(image_list) % batch_size\n\n        if self.last_batch == 0 or drop_last:\n            self.last_batch = batch_size\n            self.drop_last = True\n        else:\n            self.length += 1\n            self.drop_last = drop_last\n\n        pipes[0].build()\n        self.dali_iter = DALIGenericIterator(pipes, ['data', 'label'], reader_name=name)\n\n    def __len__(self):\n\n        return self.length\n\n    def _process_batch(self, batch):\n        # assume single GPU\n        batch = batch[0]\n        return batch\n\n    def __iter__(self):\n\n        for n, batch in enumerate(self.dali_iter):\n            batch = self._process_batch(batch)\n            # if not drop last - cut last batch\n            if n == (len(self) - 1) and not self.drop_last:\n                for k in batch:\n                    batch[k] = batch[k][:self.last_batch]\n                yield batch\n            else:\n                yield batch\n                # if drop last and last full batch - raise stop iteration\n                if n == (len(self) - 1) and self.drop_last:\n                    return\n                \n                \n\ndef score_with_image_model_dali(df, model, image_path, image_size=300, batch_size=32, device='cuda:0', n_jobs=10,\n                                mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)):\n    \"\"\"\n    Get image embedding\n    \"\"\"\n    dl = DaliTorchLoader(df['image'].map(lambda x: os.path.join(image_path, x)).tolist(), \\\n                         image_size=image_size, batch_size=batch_size, num_threads=n_jobs, mean=mean, std=std)\n\n    model.eval()\n\n    res = []\n\n    with torch.set_grad_enabled(False):\n        for batch in tqdm.tqdm(dl):\n            with autocast():\n                pred = model.extract_features(batch['data'].to(device))\n            pred = pred.view(*pred.shape[:2], -1).mean(dim=-1).detach().cpu().numpy().astype(np.float32)\n            res.append(pred)\n\n    res = np.concatenate(res, axis=0)\n    res = res / ((res ** 2).sum(axis=1, keepdims=True) ** .5)\n    return res\n\n\nclass MatchingTextTest(Dataset):\n    \"\"\"\n    Torch Dataset for text models\n    \"\"\"\n    def __init__(self, data, random_state=42):\n        self.data = data\n\n    def __getitem__(self, idx):\n        row = self.data.iloc[idx]\n        text = row['title']\n\n        return text\n\n    def __len__(self):\n        return self.data.shape[0]\n\n    \ndef score_with_text_model(df, text_model, tokenizer=None, batch_size=32, device='cuda:0',\n                          max_length=128, n_jobs=10, standartize=True, normalize=True):\n    \"\"\"\n    Get text embedding\n    \"\"\"\n    ds = MatchingTextTest(df)\n    dl = DataLoader(ds, batch_size=batch_size, num_workers=n_jobs, drop_last=False, shuffle=False)\n\n    if tokenizer is None:\n        tokenizer = AlbertTokenizer.from_pretrained('albert-base-v2')\n    text_model.eval()\n\n    res = []\n\n    with torch.set_grad_enabled(False):\n        for batch in tqdm.tqdm(dl):\n            texts = tokenizer(batch, return_tensors=\"pt\", padding=True, truncation=True, max_length=max_length)\n            with autocast():\n                texts = {x: texts[x].to(device) for x in texts}\n                texts = text_model(**texts).last_hidden_state[:, 0, :]\n\n            pred = texts.detach().cpu().numpy().astype(np.float32)\n            res.append(pred)\n\n    res = np.concatenate(res, axis=0)\n    if standartize:\n        res = (res - res.mean(axis=0)) / res.std(axis=0)\n\n    if normalize:\n        res = res / ((res ** 2).sum(axis=1, keepdims=True) ** .5)\n    return res\n\n\n\nclass TokenizeWrapper:\n    \"\"\"\n    Tokenizer for CLIP\n    \"\"\"\n    def __init__(self, max_len=77):\n        self.tokenizer = clip.simple_tokenizer.SimpleTokenizer()\n        self.max_len = max_len\n\n    def __call__(self, texts):\n        res = torch.zeros((len(texts), self.max_len), dtype=torch.long)\n        res[:, 0] = 49406\n\n        for n, tx in enumerate(texts):\n            enc = self.tokenizer.encode(tx)\n            enc = enc[:self.max_len - 2] + [49407]\n            res[n, 1:len(enc) + 1] = torch.tensor(enc)\n\n        return res\n\n\nclass DaliClipLoader(DaliTorchLoader):\n    \"\"\"\n    DataLodaer for CLIP\n    \"\"\"\n    def __init__(self, train, image_path='train_images', batch_size=128, num_threads=2, n_gpus=1,\n                 name='Reader', hflip_p=0.5, scale=1.3):\n        self.train = train\n        self.params = {\n\n            'image_size': 244,\n            'batch_size': batch_size,\n            'num_threads': num_threads,\n            'n_gpus': n_gpus,\n            'name': name,\n            'shuffle': False,\n            'drop_last': False,\n            'hflip_p': hflip_p,\n            'scale': scale,\n            'mean': (0.48145466, 0.4578275, 0.40821073),\n            'std': (0.26862954, 0.26130258, 0.27577711)\n\n        }\n        self.texts = TokenizeWrapper()(self.train['title'])\n        self.image_path = image_path\n        self._initialize(train['image'].map(lambda x: os.path.join(image_path, x)).tolist(), **self.params)\n\n    def _process_batch(self, batch):\n        # assume single GPU\n        batch = super()._process_batch(batch)\n        batch['text'] = self.texts[batch['label'].type(torch.long)[:, 0]]\n        return batch\n\n\ndef score_with_clip_model_dali(df, model, image_path, batch_size=32, device='cuda:0', n_jobs=10,\n                               ):\n    \"\"\"\n    Get CLIP embedding\n    \"\"\"\n    dl = DaliClipLoader(df, image_path=image_path, batch_size=batch_size, num_threads=n_jobs)\n\n    model.eval()\n\n    res_img, res_text = [], []\n\n    with torch.set_grad_enabled(False):\n        for batch in tqdm.tqdm(dl):\n            with autocast():\n                res_img.append(model.encode_image(batch['data'].to(device)).detach().cpu().numpy().astype(np.float32))\n                res_text.append(model.encode_text(batch['text'].to(device)).detach().cpu().numpy().astype(np.float32))\n\n    res_img = np.concatenate(res_img, axis=0)\n    res_img = res_img / ((res_img ** 2).sum(axis=1, keepdims=True) ** .5)\n\n    res_text = np.concatenate(res_text, axis=0)\n    res_text = res_text / ((res_text ** 2).sum(axis=1, keepdims=True) ** .5)\n\n    return res_img, res_text\n\n\ndef get_length_features(left, right, texts, vect_params):\n    \"\"\"\n    Get additional features from text\n    \"\"\"\n    res = np.empty((len(left), 3), dtype=np.float32)\n\n    token_counts = CountVectorizer(**vect_params, dtype=np.bool, binary=True).fit_transform(texts)\n    left, right = token_counts[left], token_counts[right]\n\n    inter = left.multiply(right)\n    res[:, 0] = inter.sum(axis=1).ravel()\n\n    diff = left + right - inter\n    res[:, 1] = diff.sum(axis=1).ravel()\n\n    res[:, 2] = np.abs(left.sum(axis=1).ravel() - right.sum(axis=1).ravel())\n\n    return res\n\n\nclass FeaturesGenerator:\n    \"\"\"\n    Features generator (for batch inference)\n    \"\"\"\n    def __init__(self, feature_fn, cache_dir=None, **kwargs):\n\n        self.kwargs = kwargs\n        self.cache_dir = cache_dir\n\n        if cache_dir is not None:\n\n            self.batch = os.path.join(cache_dir, 'batch_{0}.pkl')\n            os.makedirs(cache_dir, exist_ok=True)\n\n            for f in [x for x in os.listdir(cache_dir) if x[:5] == 'batch']:\n\n                path = os.path.join(cache_dir, f)\n\n                if os.path.exists(path):\n                    os.remove(path)\n\n        self.feature_fn = feature_fn\n\n    def features(self, left, right, folds=None, bypass_feats=None):\n\n        if bypass_feats is None:\n            bypass_feats = []\n\n        X = self.feature_fn(left=left, right=right, folds=folds,\n                            bypass_feats=bypass_feats, **self.kwargs)\n        \n        X = DataFrame(X, columns=['feat_{0}'.format(x) for x in range(X.shape[1])])\n\n        return X\n\n    def features_generator(self, left, right, bypass_feats=None, batch_size=500000):\n\n        if bypass_feats is None:\n            bypass_feats = []\n\n        self.cached_files = []\n\n        for n, i in enumerate(range(0, len(left), batch_size)):\n\n            X = self.feature_fn(left=left[i: i + batch_size], right=right[i: i + batch_size], folds=None,\n                                bypass_feats=[x[i: i + batch_size] for x in bypass_feats], **self.kwargs)\n\n            if self.cache_dir is not None:\n                joblib.dump(X, self.batch.format(n))\n                self.cached_files.append(self.batch.format(n))\n            \n            X = DataFrame(X, columns=['feat_{0}'.format(x) for x in range(X.shape[1])])\n\n            yield X\n\n    def cached_generator(self):\n\n        assert len(self.cached_files) > 0, 'No cached files'\n\n        for fname in self.cached_files:\n            yield joblib.load(fname)\n\ndef reflect_prediction(pred):\n    \"\"\"\n    Average left/right and right/left prediction\n    \"\"\"\n    L = pred.shape[0] // 2\n    reflected = pred.copy()\n\n    reflected[:L] += pred[L:]\n    reflected[L:] += pred[:L]\n\n    reflected /= 2\n\n    return reflected\n\n\n\n### Clustering functions\n\n\ndef calc_components_dist(comp0, comp1, orig_dist):\n    res = []\n\n    for c0 in comp0:\n        for c1 in comp1:\n            d = orig_dist[c0].get(c1)\n            if d is None:\n                d = orig_dist[c1].get(c0, 0)\n            res.append(d)\n\n    res = np.array(res)\n\n    return np.mean(res) * 0.75 + res.max() * 0.25\n\n\ndef upd_cutoff(co, cl_s0, cl_s1):\n    cl_size = max(cl_s0, cl_s1)\n\n    if cl_size < 2:\n        co = max(co, 0.55)\n    elif cl_size < 5:\n        co = max(co, 0.45)\n    elif cl_size < 10:\n        co = max(co, 0.35)\n    else:\n        co = max(co, 0.25)\n\n    return co\n\n\ndef _default_cutoff_fn(co, *args, **kwargs):\n    return co\n\n\ndef update_clusters(clusters, cluster_candidates, cluster_distances, cutoff=0.99, max_add=2, max_cl_size=50,\n                    cutoff_fn=None):\n    if cutoff_fn is None:\n        cutoff_fn = _default_cutoff_fn\n\n    G = nx.Graph()\n\n    for cname in clusters:\n\n        cand = cluster_candidates[cname]\n        dist = cluster_distances[cname]\n        valid_candidates = [x for (x, y) in zip(cand, dist)\n                            if y > cutoff_fn(cutoff, len(clusters[x]), len(clusters[cname]))\n                            ][:max_add]\n\n        path = clusters[cname].copy()\n        for cand in valid_candidates:\n\n            valid = clusters[cand]\n            if ((len(valid) + len(path)) <= max_cl_size):\n                path.extend(clusters[cand])\n\n        nx.add_path(G, path)\n\n    clusters = {}\n    backmap = {}\n\n    for comp in nx.connected_components(G):\n\n        comp = list(comp)\n        cname = min(comp)\n\n        clusters[cname] = comp\n\n        for c in comp:\n            backmap[c] = cname\n\n    for c in range(len(backmap)):\n        if c not in backmap:\n            clusters[c] = [c]\n            backmap[c] = c\n\n    return clusters, backmap\n\n\ndef update_distances(clusters, backmap, orig_dist):\n    cluster_candidates = {}\n    cluster_distances = {}\n\n    for cname in clusters:\n        comp = set(clusters[cname])\n        candidates = []\n        for c in comp:\n            candidates.extend([backmap[x] for x in orig_dist[c] if x not in comp])\n        candidates = list(set(candidates))\n\n        distances = np.array([calc_components_dist(clusters[x], comp, orig_dist) for x in candidates])\n        order = distances.argsort()[::-1]\n        cluster_candidates[cname] = list(np.array(candidates)[order])\n        cluster_distances[cname] = distances[order]\n\n    return cluster_candidates, cluster_distances\n\n\ndef init_clusters(oof_pred, oof_probs):\n    orig_dist = []\n\n    for cc, pp in zip(oof_pred, oof_probs):\n        orig_dist.append({x: y for (x, y) in zip(cc[1:], pp[1:])})\n\n    clusters, cluster_candidates, cluster_distances = {}, {}, {}\n\n    for c in range(len(oof_pred)):\n        clusters[c] = [c]\n        cluster_candidates[c] = oof_pred[c]\n        cluster_distances[c] = oof_probs[c]\n\n    return orig_dist, clusters, cluster_candidates, cluster_distances\n\n\ndef get_pred_from_cluster(clusters, orig_dist, cluster_candidates, cluster_distances,\n                          cl_co=0.5, co=0.6, exact_add=1):\n    pred = [None for _ in range(len(orig_dist))]\n\n    for cname in clusters:\n        comp = clusters[cname]\n\n        to_merge = []\n        cand = cluster_candidates[cname]\n        dist = cluster_distances[cname]\n\n        for n, (c, d) in enumerate(zip(cand, dist)):\n            if (d > cl_co) or ((n < exact_add) and len(comp) == 1):\n                to_merge.extend(clusters[c])\n\n        for c in comp:\n\n            pp = [c] + [x for x in comp if x != c] + to_merge\n\n            dist = [1]\n            for p in pp[1:]:\n                dist.append(orig_dist[c].get(p, 0))\n\n            dist = np.array(dist)\n            orders = dist.argsort()\n            pp = np.array(pp)[orders][:50]\n\n            s_pp = set(pp)\n\n            additional = [x for x in orig_dist[c] if x not in s_pp and orig_dist[c][x] > co]\n            pred[c] = list(pp) + additional[:50 - len(pp)]\n\n    return pred","metadata":{"_kg_hide-input":true,"execution":{"iopub.execute_input":"2021-06-23T10:54:44.611359Z","iopub.status.busy":"2021-06-23T10:54:44.610249Z","iopub.status.idle":"2021-06-23T10:54:44.908551Z","shell.execute_reply":"2021-06-23T10:54:44.908097Z"},"papermill":{"duration":0.356925,"end_time":"2021-06-23T10:54:44.908687","exception":false,"start_time":"2021-06-23T10:54:44.551762","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Train part","metadata":{"papermill":{"duration":0.045801,"end_time":"2021-06-23T10:54:45.000892","exception":false,"start_time":"2021-06-23T10:54:44.955091","status":"completed"},"tags":[]}},{"cell_type":"code","source":"data = pd.read_csv('../input/shopee-product-matching/train.csv')\ndevice = 'cuda:0'\ny_true = get_y_true(data)\nfolds = get_validation_folds(data, 5, 42)","metadata":{"execution":{"iopub.execute_input":"2021-06-23T10:54:45.101926Z","iopub.status.busy":"2021-06-23T10:54:45.101411Z","iopub.status.idle":"2021-06-23T10:55:03.604474Z","shell.execute_reply":"2021-06-23T10:55:03.604025Z"},"papermill":{"duration":18.557556,"end_time":"2021-06-23T10:55:03.604609","exception":false,"start_time":"2021-06-23T10:54:45.047053","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def extract_feats_and_preds(embed, co):\n    \"\"\"\n    Calculate points frequencies (features for meta model) and prediction from embedding\n    \"\"\"\n    D, I = get_di_torch(embed)\n    points = get_dist_features(D)\n    pred = cutoff_prediction(D, I, co)\n    \n    return points, pred \n","metadata":{"execution":{"iopub.execute_input":"2021-06-23T10:55:03.701241Z","iopub.status.busy":"2021-06-23T10:55:03.700724Z","iopub.status.idle":"2021-06-23T10:55:03.704421Z","shell.execute_reply":"2021-06-23T10:55:03.704043Z"},"papermill":{"duration":0.053432,"end_time":"2021-06-23T10:55:03.704543","exception":false,"start_time":"2021-06-23T10:55:03.651111","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Image embedding","metadata":{"papermill":{"duration":0.046149,"end_time":"2021-06-23T10:55:03.796544","exception":false,"start_time":"2021-06-23T10:55:03.750395","status":"completed"},"tags":[]}},{"cell_type":"code","source":"%%time\ndef get_effnet_embed(data, co=0.7, images_path='train'):\n    \"\"\"\n    Get embeddings, predctions and frequency stats from Efficient Net\n    \"\"\"\n    model = EfficientNet.from_name('efficientnet-b2')\n    model.load_state_dict(joblib.load(EFFNET_PATH))\n    model = model.to(device)\n    embed = score_with_image_model_dali(data, model, \n                '../input/shopee-product-matching/{0}_images/'.format(images_path), \n                380, batch_size=128, device=device, n_jobs=2)\n    \n    points, pred = extract_feats_and_preds(embed, co)\n\n    del model\n    gc.collect()\n    torch.cuda.empty_cache()\n    \n    return embed, pred, points\n\nimage_embed, image_pred, img_points = get_effnet_embed(data)","metadata":{"execution":{"iopub.execute_input":"2021-06-23T10:55:03.896442Z","iopub.status.busy":"2021-06-23T10:55:03.89571Z","iopub.status.idle":"2021-06-23T10:57:59.506582Z","shell.execute_reply":"2021-06-23T10:57:59.507161Z"},"papermill":{"duration":175.662982,"end_time":"2021-06-23T10:57:59.50736","exception":false,"start_time":"2021-06-23T10:55:03.844378","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Multilang BERT","metadata":{"papermill":{"duration":0.112082,"end_time":"2021-06-23T10:57:59.733696","exception":false,"start_time":"2021-06-23T10:57:59.621614","status":"completed"},"tags":[]}},{"cell_type":"code","source":"%%time\ndef get_multilang_embed(data, co=0.6):\n    \"\"\"\n    Get embeddings, predctions and frequency stats from Multi language model setu4993/LaBSE\n    \"\"\"    \n    model = AutoModel.from_pretrained('{0}/ml.model'.format(ML_PATH), \n                                  return_dict=True).to(device)\n    tokenizer = BertTokenizerFast.from_pretrained('{0}/ml.token'.format(ML_PATH))\n\n    embed = score_with_text_model(data, model, tokenizer=tokenizer, \n                                   batch_size=128, device=device, max_length=160, n_jobs=2)\n    \n    points, pred = extract_feats_and_preds(embed, co)\n\n    del model\n    gc.collect()\n    torch.cuda.empty_cache()\n    \n    return embed, pred, points\n\n\nml_embed, ml_pred, ml_points = get_multilang_embed(data, co=0.6)","metadata":{"execution":{"iopub.execute_input":"2021-06-23T10:57:59.965822Z","iopub.status.busy":"2021-06-23T10:57:59.965296Z","iopub.status.idle":"2021-06-23T10:59:36.840863Z","shell.execute_reply":"2021-06-23T10:59:36.841252Z"},"papermill":{"duration":96.995479,"end_time":"2021-06-23T10:59:36.841409","exception":false,"start_time":"2021-06-23T10:57:59.84593","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Indonesian BERT","metadata":{"papermill":{"duration":0.179868,"end_time":"2021-06-23T10:59:37.202509","exception":false,"start_time":"2021-06-23T10:59:37.022641","status":"completed"},"tags":[]}},{"cell_type":"code","source":"%%time\ndef get_id_embed(data, co=0.6):\n    \n    \"\"\"\n    Get embeddings, predctions and frequency stats from Indonesian model cahya/bert-base-indonesian-522M\n    \"\"\"    \n\n    model = BertModel.from_pretrained('{0}/id.model'.format(ID_BERT_PATH), \n                                return_dict=True).to(device)\n    tokenizer = BertTokenizer.from_pretrained('{0}/id.token'.format(ID_BERT_PATH))\n\n    embed = score_with_text_model(data, model, tokenizer=tokenizer, \n                                batch_size=128, device=device, max_length=160, n_jobs=2)\n    \n    points, pred = extract_feats_and_preds(embed, co)\n\n    del model\n    gc.collect()\n    torch.cuda.empty_cache()\n    \n    return embed, pred, points\n\n\nid_embed, id_pred, id_points = get_id_embed(data, co=0.6)","metadata":{"execution":{"iopub.execute_input":"2021-06-23T10:59:37.577679Z","iopub.status.busy":"2021-06-23T10:59:37.577147Z","iopub.status.idle":"2021-06-23T11:01:12.989248Z","shell.execute_reply":"2021-06-23T11:01:12.988482Z"},"papermill":{"duration":95.598314,"end_time":"2021-06-23T11:01:12.989389","exception":false,"start_time":"2021-06-23T10:59:37.391075","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### CLIP","metadata":{"papermill":{"duration":0.251111,"end_time":"2021-06-23T11:01:13.489496","exception":false,"start_time":"2021-06-23T11:01:13.238385","status":"completed"},"tags":[]}},{"cell_type":"code","source":"%%time\ndef get_clip_embed(data, co_img, co_text, images_path='train'):\n    \"\"\"\n    Get embeddings from CLIP\n    \"\"\"\n    clip_model, _ = clip.load(\"../input/clip-pretrained/ViT-B-32.pt\")\n    clip_model = clip_model.to(device)\n    clip_img_embed, clip_text_embed = score_with_clip_model_dali(data, clip_model,\n            '../input/shopee-product-matching/{0}_images/'.format(images_path))\n\n\n    clip_img_D, clip_img_I = get_cross_di_torch(clip_img_embed, clip_text_embed)\n    clip_img_points = get_dist_features(clip_img_D)\n\n    clip_text_D, clip_text_I = get_cross_di_torch(clip_text_embed, clip_img_embed)\n    clip_text_points = get_dist_features(clip_text_D)\n    \n\n    clip_img_pred = cutoff_prediction(clip_img_D, clip_img_I, co_img)\n    clip_text_pred = cutoff_prediction(clip_text_D, clip_text_I, co_text)\n    \n    del clip_text_D, clip_text_I, clip_img_D, clip_img_I, clip_model\n    torch.cuda.empty_cache()\n    gc.collect()\n    \n    return clip_img_embed, clip_text_embed, clip_img_pred, clip_text_pred, clip_img_points, clip_text_points\n\nclip_img_embed, clip_text_embed, clip_img_pred, \\\n    clip_text_pred, clip_img_points, clip_text_points = get_clip_embed(data, 0.34, 0.34)","metadata":{"execution":{"iopub.execute_input":"2021-06-23T11:01:14.033129Z","iopub.status.busy":"2021-06-23T11:01:14.032553Z","iopub.status.idle":"2021-06-23T11:04:01.38781Z","shell.execute_reply":"2021-06-23T11:04:01.387171Z"},"papermill":{"duration":167.647486,"end_time":"2021-06-23T11:04:01.387972","exception":false,"start_time":"2021-06-23T11:01:13.740486","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### TF IDFs","metadata":{"papermill":{"duration":0.515865,"end_time":"2021-06-23T11:04:02.420293","exception":false,"start_time":"2021-06-23T11:04:01.904428","status":"completed"},"tags":[]}},{"cell_type":"code","source":"%%time\ndef get_tfidf_embed(data, param_list, cutoffs):\n    \"\"\"\n    Get TfIdf embeddings with different tokenize params\n    \"\"\"\n    tfidf_embed, tfidf_D, tfidf_I, tfidf_points = [], [], [], []\n\n    for params in param_list:\n        vect = TfidfVectorizer(**params, dtype=np.float32)\n        tfidf_embed.append(vect.fit_transform(data['title']))\n\n        _d, _i = get_di_torch(tfidf_embed[-1])\n        tfidf_D.append(_d)\n        tfidf_I.append(_i)\n\n        tfidf_points.append(get_dist_features(tfidf_D[-1]))\n        print(params)\n        \n    tfidf_preds = []\n\n    for d, i, co in zip(tfidf_D, tfidf_I, cutoffs):\n\n        tfidf_preds.append(cutoff_prediction(d, i, co))\n        print(sum(map(len, tfidf_preds[-1])) / len(data))\n\n    del tfidf_I, tfidf_D\n    gc.collect()\n        \n    return tfidf_embed, tfidf_preds, tfidf_points\n    \n\ntfidf_embed, tfidf_preds, tfidf_points = get_tfidf_embed(data,                  \n                                        param_list = [\n                                            {'lowercase': True, 'ngram_range': (1, 1)}, \n                                            {'lowercase': True, 'ngram_range': (3, 3),\n                                             'analyzer': 'char'},  \n                                        ], \n                                        cutoffs=[0.45, 0.45])","metadata":{"execution":{"iopub.execute_input":"2021-06-23T11:04:03.519124Z","iopub.status.busy":"2021-06-23T11:04:03.503818Z","iopub.status.idle":"2021-06-23T11:04:22.736013Z","shell.execute_reply":"2021-06-23T11:04:22.736572Z"},"papermill":{"duration":19.798427,"end_time":"2021-06-23T11:04:22.736777","exception":false,"start_time":"2021-06-23T11:04:02.93835","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Union candidates","metadata":{"papermill":{"duration":0.515936,"end_time":"2021-06-23T11:04:23.770216","exception":false,"start_time":"2021-06-23T11:04:23.25428","status":"completed"},"tags":[]}},{"cell_type":"code","source":"%%time\ntotal = union_pred(image_pred, \n                   ml_pred, \n                   id_pred, \n                   clip_img_pred, clip_text_pred,\n                   *tfidf_preds)\nleft, right, fold = get_paired_indexes(total, folds)","metadata":{"execution":{"iopub.execute_input":"2021-06-23T11:04:24.832572Z","iopub.status.busy":"2021-06-23T11:04:24.822425Z","iopub.status.idle":"2021-06-23T11:04:32.670015Z","shell.execute_reply":"2021-06-23T11:04:32.670389Z"},"papermill":{"duration":8.390542,"end_time":"2021-06-23T11:04:32.670571","exception":false,"start_time":"2021-06-23T11:04:24.280029","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"left.shape","metadata":{"execution":{"iopub.execute_input":"2021-06-23T11:04:33.697619Z","iopub.status.busy":"2021-06-23T11:04:33.697082Z","iopub.status.idle":"2021-06-23T11:04:33.702017Z","shell.execute_reply":"2021-06-23T11:04:33.70162Z"},"papermill":{"duration":0.51813,"end_time":"2021-06-23T11:04:33.702123","exception":false,"start_time":"2021-06-23T11:04:33.183993","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# del image_pred, ml_pred, id_pred, tfidf_preds\n# gc.collect()","metadata":{"execution":{"iopub.execute_input":"2021-06-23T11:04:34.757768Z","iopub.status.busy":"2021-06-23T11:04:34.757029Z","iopub.status.idle":"2021-06-23T11:04:34.759991Z","shell.execute_reply":"2021-06-23T11:04:34.759539Z"},"papermill":{"duration":0.547316,"end_time":"2021-06-23T11:04:34.760093","exception":false,"start_time":"2021-06-23T11:04:34.212777","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Get features","metadata":{"papermill":{"duration":0.509726,"end_time":"2021-06-23T11:04:35.780499","exception":false,"start_time":"2021-06-23T11:04:35.270773","status":"completed"},"tags":[]}},{"cell_type":"code","source":"xgen = FeaturesGenerator(get_features, texts=data['title'], \n                         \n                         points_feats_list=[img_points, id_points, ml_points] + tfidf_points, \n                         \n                         embed_list=[image_embed, id_embed, ml_embed, \n                                     [clip_img_embed, clip_text_embed]] + tfidf_embed,\n                         \n                         count_params=[\n                            {'lowercase': True, 'ngram_range': (1, 1)}, \n                            {'lowercase': True, 'ngram_range': (3, 3), \n                             'analyzer': 'char'}, \n                        ])","metadata":{"execution":{"iopub.execute_input":"2021-06-23T11:04:36.811918Z","iopub.status.busy":"2021-06-23T11:04:36.811167Z","iopub.status.idle":"2021-06-23T11:04:36.813901Z","shell.execute_reply":"2021-06-23T11:04:36.813511Z"},"papermill":{"duration":0.51932,"end_time":"2021-06-23T11:04:36.814012","exception":false,"start_time":"2021-06-23T11:04:36.294692","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nX = xgen.features(left, right, fold)\nX['target'] = (data['label_group'].values[left] == data['label_group'].values[right]).astype(np.float32)\nX['fold'] = fold","metadata":{"execution":{"iopub.execute_input":"2021-06-23T11:04:37.859149Z","iopub.status.busy":"2021-06-23T11:04:37.858368Z","iopub.status.idle":"2021-06-23T11:04:56.866665Z","shell.execute_reply":"2021-06-23T11:04:56.866239Z"},"papermill":{"duration":19.543107,"end_time":"2021-06-23T11:04:56.866786","exception":false,"start_time":"2021-06-23T11:04:37.323679","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# del xgen, img_points, id_points, ml_points,  tfidf_points, \\\n#     image_embed, id_embed, ml_embed, tfidf_embed, total\n\n# gc.collect()","metadata":{"execution":{"iopub.execute_input":"2021-06-23T11:04:57.911398Z","iopub.status.busy":"2021-06-23T11:04:57.909972Z","iopub.status.idle":"2021-06-23T11:04:57.912216Z","shell.execute_reply":"2021-06-23T11:04:57.912643Z"},"papermill":{"duration":0.521079,"end_time":"2021-06-23T11:04:57.912781","exception":false,"start_time":"2021-06-23T11:04:57.391702","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### LightAutoML fit predict","metadata":{"papermill":{"duration":0.515184,"end_time":"2021-06-23T11:04:58.938031","exception":false,"start_time":"2021-06-23T11:04:58.422847","status":"completed"},"tags":[]}},{"cell_type":"code","source":"%%time\n\ntask = Task('binary')\nroles = {'target': 'target', 'group': 'fold'}\n\nautoml = TabularAutoML(task = task, \n                       timeout = 3600,\n                       cpu_limit = 2,\n                       general_params = {'nested_cv': False, 'use_algos': [['linear_l2', 'cb']]},\n                       reader_params = {'cv': 5, 'random_state': 42, 'advanced_roles': False}, \n                       cb_params = {'default_params': {'learning_rate': 0.03, \"od_wait\": 300, \"max_bin\": 128, \n                                                       \"min_data_in_leaf\": 10, \"max_depth\": 8}, 'freeze_defaults': True},\n                       selection_params = {'mode': 0},\n                       verbose=2)\n\npreds = automl.fit_predict(X, roles=roles).data[:, 0]","metadata":{"execution":{"iopub.execute_input":"2021-06-23T11:04:59.964111Z","iopub.status.busy":"2021-06-23T11:04:59.963421Z","iopub.status.idle":"2021-06-23T11:15:49.562499Z","shell.execute_reply":"2021-06-23T11:15:49.559844Z"},"papermill":{"duration":650.115226,"end_time":"2021-06-23T11:15:49.562652","exception":false,"start_time":"2021-06-23T11:04:59.447426","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del X\ngc.collect()","metadata":{"execution":{"iopub.execute_input":"2021-06-23T11:15:50.911487Z","iopub.status.busy":"2021-06-23T11:15:50.910707Z","iopub.status.idle":"2021-06-23T11:15:50.914054Z","shell.execute_reply":"2021-06-23T11:15:50.914476Z"},"papermill":{"duration":0.796601,"end_time":"2021-06-23T11:15:50.91462","exception":false,"start_time":"2021-06-23T11:15:50.118019","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Make prediction","metadata":{"papermill":{"duration":0.743071,"end_time":"2021-06-23T11:15:52.29328","exception":false,"start_time":"2021-06-23T11:15:51.550209","status":"completed"},"tags":[]}},{"cell_type":"code","source":"%%time\noof_pred, oof_probs = get_prediction_index(left, right, \n                                           prob=preds,\n                                           k=data.shape[0], cutoff = .4, hard_cutoff=True, \n                                           exact_add=2)","metadata":{"execution":{"iopub.execute_input":"2021-06-23T11:15:53.43236Z","iopub.status.busy":"2021-06-23T11:15:53.431601Z","iopub.status.idle":"2021-06-23T11:15:56.847649Z","shell.execute_reply":"2021-06-23T11:15:56.847156Z"},"papermill":{"duration":3.986774,"end_time":"2021-06-23T11:15:56.847783","exception":false,"start_time":"2021-06-23T11:15:52.861009","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('OOF score with no post processing {0}'.format(f1_score(y_true, oof_pred)))","metadata":{"execution":{"iopub.execute_input":"2021-06-23T11:15:58.010792Z","iopub.status.busy":"2021-06-23T11:15:58.009989Z","iopub.status.idle":"2021-06-23T11:15:59.118261Z","shell.execute_reply":"2021-06-23T11:15:59.117786Z"},"papermill":{"duration":1.667243,"end_time":"2021-06-23T11:15:59.118378","exception":false,"start_time":"2021-06-23T11:15:57.451135","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Make clustering for train data","metadata":{"papermill":{"duration":0.555176,"end_time":"2021-06-23T11:16:00.233024","exception":false,"start_time":"2021-06-23T11:15:59.677848","status":"completed"},"tags":[]}},{"cell_type":"code","source":"oof_pred, oof_probs = get_prediction_index(left, right, \n                                           prob=preds,\n                                           k=data.shape[0], cutoff = .1, hard_cutoff=True, \n                                           exact_add=2)\n\norig_dist, clusters, cluster_candidates, cluster_distances = init_clusters(oof_pred, oof_probs)\n\ncutoff = 1.00\nfor i in range(90 ):\n    \n    cutoff -= 0.01\n        \n    clusters, backmap = update_clusters(clusters, cluster_candidates, \n                                        cluster_distances, cutoff=cutoff, \n                                        max_add=2, max_cl_size=50, \n                                        cutoff_fn=upd_cutoff)\n    \n    cluster_candidates, cluster_distances = update_distances(clusters, backmap, orig_dist)\n    \n    print('Cutoff {0} done. N clusters = {1}'.format(round(cutoff, 3), len(clusters)))\n    \n    new_oof_pred = get_pred_from_cluster(clusters, orig_dist, \n                                         cluster_candidates, cluster_distances, \n                                         cl_co=1, co=1, exact_add=1)\n    sc = f1_score(y_true, new_oof_pred)\n    \n    print(' Score {0}'.format(sc))\n    \n    if (len(data) / len(clusters)) >= 2.82:\n        break","metadata":{"execution":{"iopub.execute_input":"2021-06-23T11:16:01.600644Z","iopub.status.busy":"2021-06-23T11:16:01.599557Z","iopub.status.idle":"2021-06-23T11:22:54.46978Z","shell.execute_reply":"2021-06-23T11:22:54.470343Z"},"papermill":{"duration":413.683832,"end_time":"2021-06-23T11:22:54.47058","exception":false,"start_time":"2021-06-23T11:16:00.786748","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('OOF score with post processing {0}'.format(f1_score(y_true, new_oof_pred)))","metadata":{"execution":{"iopub.execute_input":"2021-06-23T11:22:55.689059Z","iopub.status.busy":"2021-06-23T11:22:55.688223Z","iopub.status.idle":"2021-06-23T11:22:56.833005Z","shell.execute_reply":"2021-06-23T11:22:56.832358Z"},"papermill":{"duration":1.754319,"end_time":"2021-06-23T11:22:56.833194","exception":false,"start_time":"2021-06-23T11:22:55.078875","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del y_true, oof_pred, new_oof_pred, oof_probs, clusters, backmap, cluster_candidates, cluster_distances, orig_dist, data\ngc.collect()","metadata":{"execution":{"iopub.execute_input":"2021-06-23T11:22:58.471828Z","iopub.status.busy":"2021-06-23T11:22:58.471083Z","iopub.status.idle":"2021-06-23T11:22:58.474485Z","shell.execute_reply":"2021-06-23T11:22:58.474889Z"},"papermill":{"duration":1.000635,"end_time":"2021-06-23T11:22:58.475036","exception":false,"start_time":"2021-06-23T11:22:57.474401","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Inference for test","metadata":{"papermill":{"duration":0.615556,"end_time":"2021-06-23T11:22:59.693642","exception":false,"start_time":"2021-06-23T11:22:59.078086","status":"completed"},"tags":[]}},{"cell_type":"code","source":"data = pd.read_csv('../input/shopee-product-matching/{0}.csv'.format(test))","metadata":{"execution":{"iopub.execute_input":"2021-06-23T11:23:01.135603Z","iopub.status.busy":"2021-06-23T11:23:01.135036Z","iopub.status.idle":"2021-06-23T11:23:01.144433Z","shell.execute_reply":"2021-06-23T11:23:01.144038Z"},"papermill":{"duration":0.838116,"end_time":"2021-06-23T11:23:01.144583","exception":false,"start_time":"2021-06-23T11:23:00.306467","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nimage_embed, image_pred, img_points = get_effnet_embed(data, co=0.7, images_path=test)\nml_embed, ml_pred, ml_points = get_multilang_embed(data, co=0.6)\nid_embed, id_pred, id_points = get_id_embed(data, co=0.6)\n\nclip_img_embed, clip_text_embed, clip_img_pred, \\\n    clip_text_pred, clip_img_points, clip_text_points = get_clip_embed(data, 0.34, 0.34, test)\n\ntfidf_embed, tfidf_preds, tfidf_points = get_tfidf_embed(data,                  \n                                        param_list = [\n                                            {'lowercase': True, 'ngram_range': (1, 1)}, \n                                            {'lowercase': True, 'ngram_range': (3, 3), \n                                             'analyzer': 'char'},  \n                                        ], \n                                        cutoffs=[0.45, 0.45])\n\n\ntotal = union_pred(image_pred, \n                   ml_pred, \n                   id_pred, \n                   clip_img_pred, clip_text_pred,\n                   *tfidf_preds)\n\nleft, right, _ = get_paired_indexes(total)\n\nxgen = FeaturesGenerator(get_features, texts=data['title'], \n                         \n                         points_feats_list=[img_points, id_points, ml_points] + tfidf_points, \n                         \n                         embed_list=[image_embed, id_embed, ml_embed, \n                                     [clip_img_embed, clip_text_embed]] + tfidf_embed,\n                         \n                         count_params=[\n                            {'lowercase': True, 'ngram_range': (1, 1)}, \n                            {'lowercase': True, 'ngram_range': (3, 3), \n                             'analyzer': 'char'}, \n                        ])","metadata":{"execution":{"iopub.execute_input":"2021-06-23T11:23:02.351237Z","iopub.status.busy":"2021-06-23T11:23:02.350395Z","iopub.status.idle":"2021-06-23T11:23:45.137997Z","shell.execute_reply":"2021-06-23T11:23:45.137079Z"},"papermill":{"duration":43.394149,"end_time":"2021-06-23T11:23:45.138151","exception":false,"start_time":"2021-06-23T11:23:01.744002","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del image_pred,  ml_pred, id_pred, tfidf_preds\ngc.collect()","metadata":{"execution":{"iopub.execute_input":"2021-06-23T11:23:46.52774Z","iopub.status.busy":"2021-06-23T11:23:46.526928Z","iopub.status.idle":"2021-06-23T11:23:46.530269Z","shell.execute_reply":"2021-06-23T11:23:46.530673Z"},"papermill":{"duration":0.7864,"end_time":"2021-06-23T11:23:46.530813","exception":false,"start_time":"2021-06-23T11:23:45.744413","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nif len(left) > 0:\n    \n    prediction = []\n    \n    for batch in xgen.features_generator(left, right, batch_size=250000):\n        \n        print('Batch shape {0}'.format(batch.shape))\n        \n        # automl predict\n        prediction.append(automl.predict(batch).data[:, 0])\n        \n        del batch\n        gc.collect()\n        torch.cuda.empty_cache()\n    \n    prediction = reflect_prediction(np.concatenate(prediction))\n    # calibrate predictions (assume we have same amount of positive and twice amount of negative)\n    prediction = prediction / (prediction + (1 - prediction) * 2)","metadata":{"execution":{"iopub.execute_input":"2021-06-23T11:23:47.754977Z","iopub.status.busy":"2021-06-23T11:23:47.754036Z","iopub.status.idle":"2021-06-23T11:23:49.707364Z","shell.execute_reply":"2021-06-23T11:23:49.706306Z"},"papermill":{"duration":2.571457,"end_time":"2021-06-23T11:23:49.707599","exception":false,"start_time":"2021-06-23T11:23:47.136142","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if len(left) > 0:\n    pred_index, probs = get_prediction_index(left, right, prob=prediction,\n                                             k=data.shape[0], \n                                             cutoff = .1, hard_cutoff=True, exact_add=2)\n\n    orig_dist, clusters, cluster_candidates, cluster_distances = init_clusters(pred_index, probs)\n\n    cutoff = 1.00\n    for i in range(90):\n        cutoff -= 0.01\n\n        clusters, backmap = update_clusters(clusters, cluster_candidates, \n                                            cluster_distances, cutoff=cutoff, \n                                            max_add=2, max_cl_size=50, \n                                            cutoff_fn=upd_cutoff)\n        \n        cluster_candidates, cluster_distances = update_distances(clusters, backmap, orig_dist)\n\n        print('Cutoff {0} done. N clusters = {1}'.format(round(cutoff, 3), len(clusters)))\n\n        if (len(data) / len(clusters)) >= 2.6:\n            break\n            \n    pred_index = get_pred_from_cluster(clusters, orig_dist, \n                                       cluster_candidates, cluster_distances, \n                                       cl_co=1, co=1, exact_add=1)\n\nelse:\n    pred_index, probs = [[x] for x in range(data.shape[0])], [[1.0]] * data.shape[0]","metadata":{"execution":{"iopub.execute_input":"2021-06-23T11:23:50.951292Z","iopub.status.busy":"2021-06-23T11:23:50.943636Z","iopub.status.idle":"2021-06-23T11:23:50.972422Z","shell.execute_reply":"2021-06-23T11:23:50.973014Z"},"papermill":{"duration":0.656555,"end_time":"2021-06-23T11:23:50.973198","exception":false,"start_time":"2021-06-23T11:23:50.316643","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Create submission","metadata":{"papermill":{"duration":0.60641,"end_time":"2021-06-23T11:23:52.186349","exception":false,"start_time":"2021-06-23T11:23:51.579939","status":"completed"},"tags":[]}},{"cell_type":"code","source":"%%time\nposting_dict = data['posting_id'].reset_index(drop=True).to_dict()\n\npred_posting = []\nfor pp in pred_index:\n    pred_posting.append(' '.join([posting_dict[x] for x in pp]))","metadata":{"execution":{"iopub.execute_input":"2021-06-23T11:23:53.403418Z","iopub.status.busy":"2021-06-23T11:23:53.402849Z","iopub.status.idle":"2021-06-23T11:23:53.40584Z","shell.execute_reply":"2021-06-23T11:23:53.406235Z"},"papermill":{"duration":0.615002,"end_time":"2021-06-23T11:23:53.406365","exception":false,"start_time":"2021-06-23T11:23:52.791363","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prediction = data[['posting_id']].copy()\nprediction['matches'] = pred_posting\n\nprediction","metadata":{"execution":{"iopub.execute_input":"2021-06-23T11:23:54.639407Z","iopub.status.busy":"2021-06-23T11:23:54.63885Z","iopub.status.idle":"2021-06-23T11:23:54.649819Z","shell.execute_reply":"2021-06-23T11:23:54.649391Z"},"papermill":{"duration":0.635148,"end_time":"2021-06-23T11:23:54.649932","exception":false,"start_time":"2021-06-23T11:23:54.014784","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prediction.to_csv('submission.csv',index=False)\nsub = pd.read_csv('submission.csv')\nsub.head()","metadata":{"execution":{"iopub.execute_input":"2021-06-23T11:23:56.120266Z","iopub.status.busy":"2021-06-23T11:23:56.119424Z","iopub.status.idle":"2021-06-23T11:23:56.137729Z","shell.execute_reply":"2021-06-23T11:23:56.136995Z"},"papermill":{"duration":0.879295,"end_time":"2021-06-23T11:23:56.137845","exception":false,"start_time":"2021-06-23T11:23:55.25855","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nif DEBUG:   \n    y_true = get_y_true(data)\n    metric_val = f1_score(y_true, pred_index)\n    \n    print(metric_val)","metadata":{"execution":{"iopub.execute_input":"2021-06-23T11:23:57.416381Z","iopub.status.busy":"2021-06-23T11:23:57.415755Z","iopub.status.idle":"2021-06-23T11:23:57.418418Z","shell.execute_reply":"2021-06-23T11:23:57.41899Z"},"papermill":{"duration":0.668725,"end_time":"2021-06-23T11:23:57.419132","exception":false,"start_time":"2021-06-23T11:23:56.750407","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]}]}