{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import sys\nimport gc\nimport math\nimport configparser\nimport numpy as np\nimport pandas as pd\nimport optuna\nfrom tqdm import tqdm\nimport sklearn\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom typing import Tuple","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"try:\n    # TPU detection. No parameters necessary if TPU_NAME environment variable is\n    # set: this is always the case on Kaggle.\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.TPUStrategy(tpu)\nelse:\n    # Default distribution strategy in Tensorflow. Works on CPU and single GPU.\n    strategy = tf.distribute.get_strategy()\n\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.set_option(\"use_inf_as_na\", True)\npd.set_option(\"display.max_columns\", 9999)\npd.set_option(\"display.max_rows\", 9999)\npd.set_option('max_colwidth', 9999)\nINPUT = '/kaggle/input'\nDATA = f'{INPUT}/shopee-product-matching'\nOUTPUT = '/kaggle/working'\nRESOURCE_DIR = f'{INPUT}/shopee-product-matching-lib/kaggle-shopee-product-matching-1.0'\nsys.path.append(f'{INPUT}/sgcharts-ml/src')\nsys.path.append(f\"{INPUT}/sentence-transformers/sentence-transformers-1.0.4\")\nsys.path.append(f'{RESOURCE_DIR}/src')\nimport mylib\nimport scml\nfrom scml.nlp import strip_punctuation, to_ascii_str\nscml.seed_everything()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"MODEL = 'efficientnetb3'\nCONF = configparser.ConfigParser()\nCONF.read(f\"{RESOURCE_DIR}/app.ini\")\nresolution = int(CONF[MODEL][\"resolution\"])\nINPUT_SHAPE = (resolution, resolution, 3)\nprint(f\"INPUT_SHAPE={INPUT_SHAPE}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv(f\"{DATA}/train.csv\", engine=\"c\", low_memory=False)\ntrain[\"target\"] = mylib.target_label(train)\nle = sklearn.preprocessing.LabelEncoder()\ntrain[\"label_group\"] = le.fit_transform(train['label_group'])\nn_classes=len(le.classes_)\nprint(f\"n_classes={n_classes}\")\ntrain.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def _data_gen(\n    dataframe,\n    directory,\n    target_size,\n    batch_size,\n    color_mode=\"rgb\",\n    class_mode=\"raw\",\n    x_col=\"image\",\n    y_col=\"label_group\"\n):\n    dtype = np.float32\n    rescale = 1./255\n    interpolation = \"bicubic\"\n    data_format = \"channels_last\"\n    shuffle = True\n    idg = keras.preprocessing.image.ImageDataGenerator(\n        shear_range=0.2,\n        zoom_range=0.2,\n        brightness_range=(0.7, 1.3),\n        #rotation_range=90,\n        horizontal_flip=True,\n        rescale=rescale,\n        data_format=data_format,\n        dtype=dtype\n    )\n    g = idg.flow_from_dataframe(\n        dataframe=dataframe,\n        x_col=x_col,\n        y_col=y_col,\n        directory=directory,\n        target_size=target_size,\n        color_mode=color_mode,\n        batch_size=batch_size,\n        shuffle=shuffle,\n        class_mode=class_mode,\n        interpolation=interpolation,\n    )\n    while True:\n        x, y = g.next()\n        yield [x, y], y","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def _model(\n    pretrained,\n    n_classes: int,\n    lr: float,\n    input_shape: Tuple[int, int, int],\n    dtype=np.float32\n):\n    pretrained.trainable = False\n    #kernel_initializer = keras.initializers.he_normal()\n    #kernel_regularizer = keras.regularizers.l2(0.01)\n    image_input = keras.layers.Input(shape=input_shape, name=\"image_input\")\n    label_input = keras.layers.Input(shape=(), name=\"label_input\")\n    x = pretrained(image_input)\n    x = keras.layers.LayerNormalization()(x)\n    x = keras.layers.Dense(pretrained.output_shape[1], activation=\"relu\")(x)\n    x = keras.layers.LayerNormalization(name=\"embedding_output\")(x)\n    x = mylib.ArcMarginProduct(\n        n_classes=n_classes, \n        s=30, \n        m=0.4,  # 1.0 training fails \n        name='head/arc_margin', \n        dtype=dtype\n    )([x, label_input])\n    output = tf.keras.layers.Softmax(dtype=dtype)(x)\n    model = tf.keras.models.Model(inputs = [image_input, label_input], outputs = [output])\n    optimizer = keras.optimizers.Adam(learning_rate=lr)\n    loss = keras.losses.SparseCategoricalCrossentropy()\n    sca = keras.metrics.SparseCategoricalAccuracy()\n    model.compile(loss=loss, optimizer=optimizer, metrics=[sca])\n    return model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pretrained = mylib.efficient_net(\n    variant=MODEL,\n    pooling=\"max\",\n    directory=f\"{RESOURCE_DIR}/pretrained/efficientnet\",\n)\nmodel = _model(\n    pretrained=pretrained,\n    input_shape=INPUT_SHAPE,\n    n_classes=n_classes,\n    lr=1e-3,\n)\nmodel.summary(line_length=150)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def _callbacks(patience: int, directory: str):\n    return [\n        keras.callbacks.EarlyStopping(\n            monitor=\"loss\", patience=patience, verbose=1\n        ),\n        keras.callbacks.ModelCheckpoint(\n            filepath=f\"{directory}/model.h5\",\n            monitor=\"loss\",\n            save_best_only=True,\n            verbose=1\n        )\n    ]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class MyObjective:\n    def __init__(\n        self,\n        df,\n        epochs: int,\n        batch_size: int,\n        patience: int,\n        job_dir: str,\n        lr: Tuple[float, float],\n        n_classes: int,\n        input_shape: Tuple[int, int, int],\n    ):\n        self.df = df\n        self.epochs = epochs\n        self.batch_size = batch_size\n        self.patience = patience\n        self.job_dir = job_dir\n        self.lr = lr\n        self.n_classes = n_classes\n        self.input_shape = input_shape\n        self.history: List[Dict[str, Union[str, int, float]]] = []\n\n    def __call__(self, trial):\n        hist = {\n            \"trial_id\": trial.number,\n            \"learning_rate\": trial.suggest_loguniform(\n                \"learning_rate\", self.lr[0], self.lr[1]\n            ),\n        }\n        train_gen = _data_gen(\n            dataframe=self.df,\n            directory=f\"{DATA}/train_images\",\n            target_size=self.input_shape[:2],\n            batch_size=self.batch_size,\n        )\n        pretrained = mylib.efficient_net(\n            variant=MODEL,\n            pooling=\"max\",\n            directory=f\"{RESOURCE_DIR}/pretrained/efficientnet\",\n        )\n        model = _model(\n            pretrained=pretrained,\n            input_shape=self.input_shape,\n            n_classes=self.n_classes,\n            lr=hist[\"learning_rate\"],\n        )\n        directory = f\"{self.job_dir}/trial_{hist['trial_id']}\"\n        history = model.fit(\n            train_gen,\n            epochs=self.epochs,\n            steps_per_epoch=len(self.df) / self.batch_size + 1,\n            #validation_steps=len(vi) / self.batch_size + 1,\n            #validation_data=val_gen,\n            callbacks=_callbacks(self.patience, directory=directory),\n            verbose=1\n        )\n        #y_pred = model.predict(x_val, batch_size=self.batch_size)\n        #score = metrics.mean_squared_error(y_val, y_pred, squared=False)\n        #print(repr(history.history))\n        score = history.history[\"sparse_categorical_accuracy\"][-1]\n        #log.info(f\"score={score:.4f}, fold={fold}, trial={hist['trial_id']}\")\n        print(f\"score={score:.4f}, trial={hist['trial_id']}\")\n        del model\n        gc.collect()\n        hist[\"score_worst\"] = score\n        self.history.append(hist)\n        return hist[\"score_worst\"]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Result","metadata":{}},{"cell_type":"code","source":"obj = MyObjective(\n    df=train,\n    epochs=40,\n    #batch_size=64 * strategy.num_replicas_in_sync,\n    batch_size=128,  # B3: OOM if batch size > 128 \n    patience=2,\n    job_dir=OUTPUT,\n    lr=(1e-4, 1e-4),\n    n_classes=n_classes,\n    input_shape=INPUT_SHAPE,\n)\nstudy = optuna.create_study(direction=\"maximize\")\nstudy.optimize(obj, n_trials=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = pd.DataFrame.from_records(obj.history)\nhistory.sort_values(\"score_worst\", ascending=False, inplace=True, ignore_index=True)\nhistory.to_csv(f\"{OUTPUT}/cv.csv\", index=False)\nhistory.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}