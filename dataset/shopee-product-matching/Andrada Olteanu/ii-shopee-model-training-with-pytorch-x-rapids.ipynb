{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<img src=\"https://i.imgur.com/TodFykz.png\">\n<center><h1>-Model Training & Submission-</h1></center>\n\n# 1. Introduction\nüü¢ **Goal:** Building a model that can identify which images contain the same product/s.\n\nüü† **To consider**:\n* This competition is a little different, as it doesn't use Supervised ML Techniques, but **Unsupervised** ML Techniques.\n* The goal is to group similar products together: although we have a \"target variable\" (named `label_group`) in the `train` dataset, there can be multiple other types of groups in the `test` dataset (completely unseen during training). Hence, we can't use the `label_group` as our target (`y`) feature.\n\n<div class=\"alert alert-block alert-success\">\n<b>Inspiration:</b> HUGE thanks to Chris Deotte for creating a <a href=\"https://www.kaggle.com/cdeotte/part-2-rapids-tfidfvectorizer-cv-0-700\"> trendsetter notebook with a baseline </a>, so we can all get started and to zzy990106 for his <a href=\"https://www.kaggle.com/zzy990106/b0-bert-cv0-9\"> PyTorch version </a> on Chris's work.\n<p>This notebook has the purpose of going deeper with the explanations regarding the code and process and an attempt of improving the baseline score as we go along. üòä</p>\n</div>\n\n### üìö Libraries + W&B\n\n> You can find my W&B Dashboard on this competition [here](https://wandb.ai/andrada/shopee-kaggle?workspace=user-andrada).","metadata":{}},{"cell_type":"code","source":"# Libraries CPU\nimport wandb     ### comment when Internet OFF\nimport cv2\nimport os\nimport gc\nimport random\nimport tqdm\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom mpl_toolkits import mplot3d\nimport sys\nsys.path = ['../input/efficientnet-pytorch/EfficientNet-PyTorch/EfficientNet-PyTorch-master'\n           ] + sys.path\n\n# Libaries GPU\nimport cudf\nimport cupy\nimport cuml\nfrom cuml.feature_extraction.text import TfidfVectorizer\nfrom cuml.neighbors import NearestNeighbors\n\n# Pytorch & Deep Learning\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, Dataset\nfrom albumentations import Compose, Resize, Normalize, HorizontalFlip, VerticalFlip,\\\n                            Rotate, CenterCrop\n\n\nfrom efficientnet_pytorch import EfficientNet\nfrom transformers import AutoTokenizer\nfrom torchvision.models import resnet34, resnet50\n\n# Environment check\nos.environ[\"WANDB_SILENT\"] = \"true\"      ### comment when Internet OFF\n\n# Secrets ü§´\n### comment when Internet OFF\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nsecret_value_0 = user_secrets.get_secret(\"wandb\")\n\n# Color scheme\nmy_colors = [\"#EDAC54\", \"#F4C5B7\", \"#DD7555\", \"#B95F18\", \"#475A20\"]\n\n# Device\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint('Device available now:', device)\n\n# Base paths\ntrain_base = \"../input/shopee-product-matching/train_images/\"\ntest_base = \"../input/shopee-product-matching/test_images/\"","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def set_seed(seed = 1234):\n    '''Sets the seed of the entire notebook.\n    This is for REPRODUCIBILITY.'''\n    np.random.seed(seed)\n    random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    # When running on the CuDNN backend, two further options must be set\n    torch.backends.cudnn.deterministic = True\n    # Set a fixed value for the hash seed\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    \nset_seed()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"! wandb login $secret_value_0     ### comment when Internet OFF","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2. Load the Data\n\nLet's read the data, by always taking into account the state of the notebook (whether is in **submission** or **commiting** process).\n* For `submission`, we'll read in `test.csv` data\n* For `commiting`, we'll read in `train.csv`, so we can plot a CV score as well","metadata":{}},{"cell_type":"code","source":"# ---- Set COMPUTE_CV value ----\nCOMPUTE_CV = True\n\n# Switch to False if test.csv has more than 3 values\n### check out Chris's notebook for more info on this\ntest = pd.read_csv('../input/shopee-product-matching/test.csv')\n\nif len(test)>3: \n    COMPUTE_CV = False","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if COMPUTE_CV == True:\n    # === CPU data ===\n    # Read in data\n    data = pd.read_csv(\"../input/shopee-product-matching/train.csv\")    \n    # Set a \"filepath\" column\n    data[\"filepath\"] = train_base + data[\"image\"]\n    # Map on for each product all `posting_id` that are labeled as the same\n    ### this way we create a \"target\" column (ONLY FOR TRAIN)\n    group_dicts = data.groupby('label_group')[\"posting_id\"].unique().to_dict()\n    data['target'] = data[\"label_group\"].map(group_dicts)\n    \n    # === GPU data ===\n    data_gpu = cudf.read_csv(\"../input/shopee-product-matching/train.csv\")    \n    data_gpu[\"filepath\"] = train_base + data_gpu[\"image\"]\n\nelse:\n    # === CPU data ===\n    data = pd.read_csv(\"../input/shopee-product-matching/test.csv\")\n    data[\"filepath\"] = test_base + data[\"image\"]\n    # No Target Here\n    \n    # === GPU data ===\n    data_gpu = cudf.read_csv(\"../input/shopee-product-matching/test.csv\")    \n    data_gpu[\"filepath\"] = test_base + data_gpu[\"image\"]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> When this notebook is commited, the `data` variable will have 34,000 rows. However, when we'll commit it, the `data` will access the 70,000 hidden rows in the `test.csv`. This means that **the amount of observations pushed through the pipeline will double**. To avoid any *memory errors*, you would want to also experiment by pushing ~ 70,000 rows as well, to **make sure your code isn't crushing** somewhere along the way.","metadata":{}},{"cell_type":"code","source":"# # === OPTIONAL ===\n# # Increase 2.05 times the amount of data\n# data = pd.concat([data, data, data.loc[:2000]], axis=0)\n# data_gpu = cudf.concat([data_gpu, data_gpu, data_gpu.loc[:2000]], axis=0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's look at it\ndata.head(2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Save data to W&B Artifacts\n### comment when Internet OFF\nrun = wandb.init(project='shopee-kaggle', name='original_data')\nartifact = wandb.Artifact(name='original', \n                          type='dataset')\n\nartifact.add_file(\"../input/shopee-preprocessed-data/train.parquet\")\nartifact.add_file(\"../input/shopee-preprocessed-data/test.parquet\")\n\nwandb.log_artifact(artifact)\nwandb.finish()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3. Competition Metric\n\nLet's now understand the competition metric. I usually like to have this down, as it is a very important part of the prediction process.\n\n*üìå Again, the methodology is highly inspired from [\n[PART 2] - RAPIDS TfidfVectorizer - [CV 0.700]](https://www.kaggle.com/cdeotte/part-2-rapids-tfidfvectorizer-cv-0-700) üìå*\n\n<img src=\"https://i.imgur.com/h3oWxLT.png\" width=800>","metadata":{}},{"cell_type":"code","source":"def F1_score(target_column, pred_column):\n    '''Returns the F1_score for each row in the data.\n    Remember: The final score is the mean F1 score.\n    target_column: the name of the column that contains the target\n    pred_column: the name of the column that contains the prediction\n    '''\n    \n    def get_f1(row):\n        # Find the common values in target and prediction arrays.\n        intersection = len( np.intersect1d(row[target_column], row[pred_column]) )\n        # Computes the score by following the formula\n        f1_score = 2 * intersection / (len(row[target_column]) + len(row[pred_column]))\n        \n        return f1_score\n    \n    return get_f1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So, without doing anything we have a **CV score** of **0.553**.","metadata":{}},{"cell_type":"code","source":"run = wandb.init(project='shopee-kaggle', name='metric_baseline')\n\ndata_baseline = data.copy()\n\n# Create artificial prediction column\n### based on image_phash - all images with the same image_phash are the same\ngroup_baseline = data_baseline.groupby(\"image_phash\")[\"posting_id\"].unique().to_dict()\ndata_baseline['preds'] = data_baseline[\"image_phash\"].map(group_baseline)\n\n# Get F1 score for each row\ndata_baseline['F1'] = data_baseline.apply(F1_score(target_column=\"target\", pred_column=\"preds\"), axis=1)\nprint('CV score for baseline = {:.3f}'.format(data_baseline[\"F1\"].mean()))\nwandb.log({\"Baseline CV Score\" : data_baseline[\"F1\"].mean()})\n\nwandb.finish()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4. PyTorch Dataset\n\nWe'll create a Dataset class called `ShopeeDataset` that will:\n1. Receive the metadata\n2. Read in the `image` and `title`\n3. Perform image augmentation and tokenization\n4. Return the necessary information to feed into the model afterwards\n\n\n### The Bert Tokenizer ([data from Abhishek Thakur](https://www.kaggle.com/abhishek/bert-base-uncased/code?datasetId=431504&sortBy=voteCount)):\n* Pretrained tokenizer that splits sentences into tokens (source from `transformers` library - [click here for more info](https://huggingface.co/transformers/preprocessing.html))\n* The output is as follows:\n    * `input_ids`: indices corresponding to each token in the sentence\n    * `attention_mask`: indicates to the model which tokens should be attended to, and which should not ([documentation on attention_mask here](https://huggingface.co/transformers/glossary.html#attention-mask))\n<img src=\"https://i.imgur.com/3uY3YFi.png\" width=500>","metadata":{}},{"cell_type":"code","source":"class ShopeeDataset(Dataset):\n    \n    def __init__(self, csv, train):\n        self.csv = csv.reset_index()\n        self.train = train\n        \n        # Instantiate one of the tokenizer classes of the library from BERT\n        self.tokenizer = AutoTokenizer.from_pretrained('../input/bert-base-uncased')\n        # Image Augmentation\n        self.transform = Compose([VerticalFlip(p=0.5),\n                                  HorizontalFlip(p=0.5),\n                                  Resize(256, 256),\n                                  Normalize(),\n                                 ])\n        \n    def __len__(self):\n        return len(self.csv)\n    \n    \n    def __getitem__(self, index):\n        '''Read in image & title as PyTorch Dataset.\n        Return the transformed image and text ids and mask.'''\n            \n        # Read in image and text data\n        image = cv2.imread(self.csv[\"filepath\"][index])\n        text = self.csv[\"title\"][index]\n        \n        # Transform image & transpose channels [color, height, width]\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        image_transf = self.transform(image=image)[\"image\"].astype(np.float32)\n        image_transf = torch.tensor(image_transf.transpose(2, 0, 1))\n        \n        # Tokenize the text using BERT\n        text_token = self.tokenizer(text, padding=\"max_length\",\n                                    truncation=True, max_length=16,\n                                    return_tensors=\"pt\")\n        input_ids = text_token[\"input_ids\"][0]\n        attention_mask = text_token[\"attention_mask\"][0]\n        \n        # Return dataset info\n        ### if \"test\", we won't have label_group available\n        if self.train == True:\n            label_group = torch.tensor(self.csv[\"label_group\"][index])\n            return image_transf, input_ids, attention_mask, label_group\n        \n        else:\n            return image_transf, input_ids, attention_mask","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we can create the `dataset` and the `dataloader`. Remember, if:\n* **COMPUTE_CV == True**: `dataset_data` variable will contain `train.csv` data\n* **COMPUTE_CV == False**: `dataset_data` variable will contain `test.csv` data","metadata":{}},{"cell_type":"code","source":"# Compute dataloader for test data\ndataset_data = ShopeeDataset(csv=data, train=False)\ndata_loader = DataLoader(dataset_data, batch_size=16,\n                         num_workers=4)\n\nprint(\"Dataset Len: {:,}\".format(len(dataset_data)), \"\\n\" +\n      \"Image Shape [0]: {}\".format(dataset_data[0][0].shape), \"\\n\" +\n      \"input_ids [0]: {}\".format(dataset_data[0][1]), \"\\n\" +\n      \"attention_mask [0]: {}\".format(dataset_data[0][2]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 5. Grouping using Image Embeddings\n\nNow we can safely extract the embeddings from our images using EffNet. You can find more on PyToch EfficientNet [here](https://github.com/lukemelas/EfficientNet-PyTorch).\n\nThe Embeddings are actually the abstract representation of the images:\n* `input`: an image of [3, 256, 256] (3 channels, of size 256x256)\n* `output`: an array of 1000 items which is the abstract representation of the input structure (see image below)\n<img src=\"https://i.imgur.com/PjLEVaE.png\" width=550>\n\n## I. Retrieving the embeddings\n\n> **üìå Note**: Because we do not have Internet access for this notebook, we need to import the EffNet model from a dataset. Nikita Kozodoi has kindly already created this for us [here](https://www.kaggle.com/kozodoi/efficientnet-pytorch). \n<img src=\"https://miro.medium.com/max/910/1*CjpipU_oChc899f_Esjpyg.png\" width=400>","metadata":{}},{"cell_type":"code","source":"# Extract Efficientnet and put model on GPU\nmodel_effnet = EfficientNet.from_name(\"efficientnet-b2\").cuda()\nmodel_effnet.load_state_dict(torch.load(\"../input/efficientnet-pytorch/efficientnet-b2-27687264.pth\"))\n\n# model_resnet = resnet50(pretrained = False).cuda()\n# model_resnet.load_state_dict(torch.load('../input/pretrained-pytorch-models/resnet50-19c8e357.pth'))","metadata":{"_kg_hide-output":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> **üìå Note**: The cell below takes ~ 6 mins to run. Hence, I have saved the `image_embeddings` numpy array [here](https://www.kaggle.com/andradaolteanu/shopee-preprocessed-data).\n\n> What we are doing is appending to EACH batch of images (`[16, 1000]`) the `ids` extracted from BERT (`[16, 16]`) and the `masks` (`[16, 16]`) => `[16, 1032]`","metadata":{}},{"cell_type":"code","source":"# Extract embeddings of the image (the EffnetB0 representation)\nembeddings = []\n\n# We aren't training, only extracting the representation\nwith torch.no_grad():\n    for image, ids, mask in tqdm.tqdm(data_loader):\n        # Don't forget to append the image to .cuda() as well\n        image = image.cuda()\n        ids = ids.detach().numpy()\n        mask = mask.detach().numpy()\n        \n        img_embeddings = model_effnet(image)\n        img_embeddings = img_embeddings.detach().cpu().numpy()\n        # Add information from ids and mask as well\n        img_embeddings = np.hstack((img_embeddings, ids, mask))\n        embeddings.append(img_embeddings)\n        \n\n# Concatenate all embeddings\nall_image_embeddings = np.concatenate(embeddings)\nprint(\"image_embeddings shape: {:,}/{:,}\".format(all_image_embeddings.shape[0], all_image_embeddings.shape[1]))\n\n# Save it to a binary file in NumPy .npy format.\n# np.save(\"image_embeddings\", all_image_embeddings)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Read in image_embeddings\n# all_image_embeddings = np.load(\"../input/shopee-preprocessed-data/image_embeddings.npy\")\n\n# Save image_embeddings to W&B\n### comment when Internet OFF\nrun = wandb.init(project='shopee-kaggle', name='image_embeddings')\nartifact = wandb.Artifact(name='image_embeddings', \n                          type='dataset')\n\nartifact.add_file(\"../input/shopee-preprocessed-data/image_embeddings.npy\")\n\nwandb.log_artifact(artifact)\nwandb.log({\"Length of Image embeddings\" : all_image_embeddings.shape[1],\n           \"Width of Image embeddings\" : all_image_embeddings.shape[0]})\nwandb.finish()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Clean memory\ndel model_effnet\n_ = gc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## II. Creating the predictions\n\nThe competition says that \"group sizes are capped at 50, so there is no benefit to predict more than 50 matches.\" Hence, we'll create clusters of a maximum size of 50.","metadata":{}},{"cell_type":"code","source":"run = wandb.init(project='shopee-kaggle', name='image_predictions')    ### comment when Internet OFF","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create the model instance\nif len(data) > 3:\n    knn_model = NearestNeighbors(n_neighbors=50)\n    wandb.log({\"n_neighbors\" : 50})     ### comment when Internet OFF\nelse:\n    knn_model = NearestNeighbors(n_neighbors=2)\n    wandb.log({\"n_neighbors\" : 2})      ### comment when Internet OFF\n    \n# Train the model\nknn_model.fit(all_image_embeddings)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Creating the splits, to prevent memory errors\n### more info on this in Chris's notebook\npredictions = []\nCHUNK = 1024 * 4  ### 4096\n\nSPLITS = len(all_image_embeddings) // CHUNK\nif len(all_image_embeddings) % CHUNK != 0: SPLITS += 1\nprint(\"Total Splits:\", SPLITS)\n\n\n# Making the prediction\nprint(\"Finding Similar Images ...\")\n\nfor no in range(SPLITS):\n    \n    a = no * CHUNK\n    b = (no+1) * CHUNK\n    b = min(b, len(all_image_embeddings))\n    print(\"CHUNK:\", a, \"-\", b)\n    \n    distances, indices = knn_model.kneighbors(all_image_embeddings[a:b,])\n    \n    for k in range(b-a):\n        index = np.where(distances[k, ] < 6.0)[0]\n        split = indices[k, index]\n        pred = data.iloc[split][\"posting_id\"].values\n        \n        predictions.append(pred)\n\n        \n# Clean environment\ndel knn_model, distances, indices\n_ = gc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Add predictions to dataframe\ndata['img_pred'] = predictions\ndata.head(3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### comment when Internet OFF\nwandb.finish()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Bonus: 3D Plotting on Image Embeddings Clusters\n\n> We'll use PCA to downsize the data from 1000 features to only 3.","metadata":{}},{"cell_type":"code","source":"# Create dataframe\nimg_embeddings_df = pd.DataFrame(all_image_embeddings)\n\n# Separating out the features\nX = img_embeddings_df.values\n# Standardizing the features\nX = StandardScaler().fit_transform(X)\n\n# Separating out the target\ny = data[\"label_group\"]\n\n\n# PCA\npca = PCA(n_components=3)\nprincipalComponents = pca.fit_transform(X)\n# pca.explained_variance_ratio_.sum()\n\nprincipalDf = pd.DataFrame(data = principalComponents,\n                           columns = ['pc_1', 'pc_2', 'pc_3'])\nfinalDf = pd.concat([principalDf, y], axis = 1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot\nfig = plt.figure(figsize=(20, 15))\nax = plt.axes(projection='3d')\n\nax.scatter3D(finalDf['pc_1'], finalDf['pc_2'], finalDf['pc_3'], c=finalDf['label_group'], cmap='BrBG')\nax.set_title('Image Embeddings: 3D Cluster', size=20);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del img_embeddings_df, X, pca, principalDf, finalDf, all_image_embeddings\n_ = gc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 6. Grouping using Text Embeddings\n\nAs we also have the `title` of the image available, it would be a shame not to use this data for predicting as well. In this part we'll create a TfIdf Vectorizer to extract these embeddings.\n\n## I. Retrieving the embeddings\n\n> A `TfIdf` Process looks like the example below:\n<img src=\"https://i.imgur.com/W2tVXDY.png\" width=700>","metadata":{}},{"cell_type":"code","source":"# Extract the Tf-Idf Matrix\n# TODO: Extract more features & add preprocessing from notebook I\ntf_idf = TfidfVectorizer(stop_words='english', binary=True, max_features=25000)\ntext_embeddings = tf_idf.fit_transform(data_gpu[\"title\"]).toarray()\n\nprint(\"Text Embeddings Matrix format: {:,}/{:,}\".format(text_embeddings.shape[0], text_embeddings.shape[1]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Save image_embeddings to W&B\n### comment when Internet OFF\nrun = wandb.init(project='shopee-kaggle', name='text_embeddings')\nartifact = wandb.Artifact(name='text_embeddings', \n                          type='dataset')\n\nartifact.add_file(\"../input/shopee-preprocessed-data/text_embeddings.npy\")\n\nwandb.log_artifact(artifact)\nwandb.log({\"Length of Text embeddings\" : text_embeddings.shape[1],\n           \"Width of Text embeddings\" : text_embeddings.shape[0]})\nwandb.finish()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## II. Creating the predictions","metadata":{}},{"cell_type":"code","source":"def find_matches_cupy(X, posting_ids, threshold):\n    # TODO: to be developed\n    # https://www.kaggle.com/c/shopee-product-matching/discussion/230486\n    X = cp.array(X)\n    N = X.shape[1]\n    matches = []\n\n    for i in tqdm(range(N)):\n        v = X[:, i].reshape(-1, 1)\n        thresholded_bool = cp.linalg.norm(v - X, axis=0) < threshold\n        thresholded_ix = cp.argwhere(thresholded_bool).squeeze(-1)\n        thresholded_ix = thresholded_ix.get()\n        match = \" \".join(posting_ids[thresholded_ix])\n        matches.append(match)\n\n    return matches","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Creating the splits, to prevent memory errors\n### more info on this in Chris's notebook\npredictions = []\nCHUNK = 1024 * 4  ### 4096\n\nSPLITS = len(text_embeddings) // CHUNK\nif len(text_embeddings) % CHUNK != 0: SPLITS += 1\nprint(\"Total Splits:\", SPLITS)\n\n\n# Making the prediction\nprint(\"Finding Similar Titles ...\")\n\nfor no in range(SPLITS):\n    \n    a = no * CHUNK\n    b = (no+1) * CHUNK\n    b = min(b, len(text_embeddings))\n    print(\"CHUNK:\", a, \"-\", b)\n    \n    # Cosine similarity distance\n    cts = cupy.matmul(text_embeddings, text_embeddings[a:b].T).T\n    \n    for k in range(b-a):\n        index = cupy.where(cts[k,] > 0.7)[0]\n        index = cupy.asnumpy(index)\n        pred = data.iloc[index][\"posting_id\"].values\n        \n        predictions.append(pred)\n\n        \n# Clean environment\ndel tf_idf, text_embeddings\n_ = gc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Add predictions to dataframe\ndata['title_pred'] = predictions\ndata.head(3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Bonus: 3D Plotting on Text Embeddings Clusters\n\n> We'll use PCA to downsize the data from 1000 features to only 3.","metadata":{}},{"cell_type":"code","source":"from cuml.experimental.preprocessing import StandardScaler as StandardScaler_gpu\nfrom cuml.decomposition import PCA as PCA_gpu","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Create dataframe\n# text_embeddings_df = cudf.DataFrame(text_embeddings)\n\n# # Separating out the features\n# X = text_embeddings_df.values\n# # Standardizing the features\n# X = StandardScaler_gpu().fit_transform(X)\n\n# # Separating out the target\n# y = data[\"label_group\"]\n\n\n# # PCA\n# pca = PCA_gpu(n_components=3)\n# principalComponents = pca.fit_transform(X)\n\n# principalDf = cudf.DataFrame(data = principalComponents,\n#                              columns = ['pc_1', 'pc_2', 'pc_3'])\n# finalDf = cudf.concat([principalDf, y], axis = 1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 7. Final predictions\n\nNow that we have predictions linked to both image and title embeddings, we can combine them and create the final predictions that we'll also submit to the leaderboard.","metadata":{}},{"cell_type":"code","source":"# All images that have the same phash are identical, so we'll add these too\nduplicate_dict = data.groupby('image_phash').posting_id.agg('unique').to_dict()\ndata['duplic_pred'] = data[\"image_phash\"].map(duplicate_dict)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def combine_predictions(row, cv=True):\n    '''Combine all predictions together.'''\n    \n    # Concatenate all predictions\n    all_preds = np.concatenate([row[\"img_pred\"],row[\"title_pred\"], row[\"duplic_pred\"]])\n    all_preds = np.unique(all_preds)\n    \n    # Return combined unique preds\n    if cv == True:\n        return all_preds\n    else:\n        return ' '.join(all_preds)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> **CV Score: 0.67** with a submission score in Leaderboard of **0.66**.\n<img src=\"https://i.imgur.com/QLtVqqq.png\" width=600>","metadata":{}},{"cell_type":"code","source":"if COMPUTE_CV == True:\n    \n    data[\"all_preds\"] = data.apply(lambda x: combine_predictions(x, cv=True), axis=1)\n    data[\"f1\"] = data.apply(F1_score(target_column=\"target\", pred_column=\"all_preds\"), axis=1)\n    print(\"CV Score: {:.3}\".format(data[\"f1\"].mean()))\n    \n\ndata[\"matches\"] = data.apply(lambda x: combine_predictions(x, cv=False), axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot F1 Score on product\nplt.figure(figsize = (20, 6))\n\nplot = sns.kdeplot(x = data[\"f1\"])\nplt.title(\"F1 score Distribution\", fontsize=20)\nplt.xlabel(\"F1\", fontsize=15)\nplt.ylabel(\"\");","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# --- Make a custom plot to save into W&B ---\n### comment when Internet OFF\nrun = wandb.init(project='shopee-kaggle', name='f1_final_scores')\n\n# Prepare data\ncustom_data = [[s] for s in data[\"f1\"]]\n\n# Create Table & .log() the plot\ntable = wandb.Table(data=custom_data, columns=[\"f1\"])\nwandb.log({'f1_hist': wandb.plot.histogram(table, \"f1\",\n                                           title=\"F1 score Distribution\")})\n\nwandb.finish()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> This is how the distribution shows in the W&B dashboard:\n<img src=\"https://i.imgur.com/FnL9Br0.png\" width=500>\n\n## üì© Submission\n\n> **üìå Note**: Don't forget to disable the Internet access before submitting.\n\n<div class=\"alert alert-block alert-warning\">\n<b>Note:</b> This notebooks uses internet to connect to the W&B Dashboard. To submit it, you'll have to set the Internet Off and to comment the lines of code that save information into the W&B Project.</p>\n</div>","metadata":{}},{"cell_type":"code","source":"data[['posting_id','matches']].to_csv('submission.csv',index=False)\nprint(\"Submission Ready :)\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<img src=\"https://i.imgur.com/cUQXtS7.png\">\n\n# Specs on how I trained ‚å®Ô∏èüé®¬∂\n### (on my local machine)\n* Z8 G4 Workstation üñ•\n* 2 CPUs & 96GB Memory üíæ\n* NVIDIA Quadro RTX 8000 üéÆ\n* RAPIDS version 0.17 üèÉüèæ‚Äç‚ôÄÔ∏è","metadata":{}}]}