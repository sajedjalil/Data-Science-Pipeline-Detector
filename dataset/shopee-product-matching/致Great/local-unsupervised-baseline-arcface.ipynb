{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## 导入包","metadata":{}},{"cell_type":"code","source":"!pip install ../input/shopeeexternalmodels/Keras_Applications-1.0.8-py3-none-any.whl\n!pip install ../input/shopeeexternalmodels/efficientnet-1.1.0-py3-none-any.whl\nimport numpy as np\nimport pandas as pd\nimport gc\nimport matplotlib.pyplot as plt\nimport cudf\nimport cuml\nimport cupy\nfrom cuml.feature_extraction.text import TfidfVectorizer\nfrom cuml import PCA\nfrom cuml.neighbors import NearestNeighbors\nimport tensorflow as tf\nimport efficientnet.tfkeras as efn\nfrom tqdm.notebook import tqdm\nimport math\nfrom shutil import copyfile\nimport tensorflow_hub as hub\nimport transformers\nfrom collections import Counter\n\ntransformers.__version__","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# For tf.dataset\nAUTO = tf.data.experimental.AUTOTUNE\n\n# Configuration\nBATCH_SIZE = 8\nIMAGE_SIZE = [512, 512]\n# Seed\nSEED = 42\n# Verbosity\nVERBOSE = 1\n# Number of classes\nN_CLASSES = 11011","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# RESTRICT TENSORFLOW TO 2GB OF GPU RAM\n# SO THAT WE HAVE 14GB RAM FOR RAPIDS\nLIMIT = 4.0\ngpus = tf.config.experimental.list_physical_devices('GPU')\nif gpus:\n    try:\n        tf.config.experimental.set_virtual_device_configuration(\n            gpus[0],\n            [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=1024*LIMIT)])\n        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n        #print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n    except RuntimeError as e:\n        print(e)\nprint('We will restrict TensorFlow to max %iGB GPU RAM'%LIMIT)\nprint('then RAPIDS can use %iGB GPU RAM'%(16-LIMIT))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Part1 TensorFlow-Hub Bert & EfficientNetB3","metadata":{}},{"cell_type":"code","source":"# Flag to get cv score\nGET_CV = True\n# Flag to check ram allocations (debug)\nCHECK_SUB = False\n\ndf = pd.read_csv('../input/shopee-product-matching/test.csv')\nprint(len(df))\n# If we are comitting, replace train set for test set and dont get cv\nif len(df) > 3:\n# if len(df) > 1:\n    GET_CV = False\ndel df\n\n# Function to get our f1 score\ndef f1_score(y_true, y_pred):\n    y_true = y_true.apply(lambda x: set(x.split()))\n    y_pred = y_pred.apply(lambda x: set(x.split()))\n    intersection = np.array([len(x[0] & x[1]) for x in zip(y_true, y_pred)])\n    len_y_pred = y_pred.apply(lambda x: len(x)).values\n    len_y_true = y_true.apply(lambda x: len(x)).values\n    f1 = 2 * intersection / (len_y_pred + len_y_true)\n    return f1\n\n\n\n# Function to read out dataset\ndef read_dataset():\n    if GET_CV:\n        print(\"GET_CV\",GET_CV)\n        df = pd.read_csv('../input/shopee-product-matching/train.csv')\n        tmp = df.groupby(['label_group'])['posting_id'].unique().to_dict()\n        df['matches'] = df['label_group'].map(tmp)\n        df['matches'] = df['matches'].apply(lambda x: ' '.join(x))\n        if CHECK_SUB:\n            df = pd.concat([df, df], axis = 0)\n            df.reset_index(drop = True, inplace = True)\n        df_cu = cudf.DataFrame(df)\n        image_paths = '../input/shopee-product-matching/train_images/' + df['image']\n        df['file_path'] = image_paths\n    else:\n        df = pd.read_csv('../input/shopee-product-matching/test.csv')\n        \n        df_cu = cudf.DataFrame(df)\n        image_paths = '../input/shopee-product-matching/test_images/' + df['image']\n        df['file_path'] = image_paths\n        \n    return df, df_cu, image_paths\n\n# Function to decode our images\ndef decode_image(image_data):\n    image = tf.image.decode_jpeg(image_data, channels = 3)\n    image = tf.image.resize(image, IMAGE_SIZE)\n    image = tf.cast(image, tf.float32) / 255.0\n    return image\n\n# Function to read our test image and return image\ndef read_image(image):\n    image = tf.io.read_file(image)\n    image = decode_image(image)\n    return image\n\n# Function to get our dataset that read images\ndef get_dataset(image):\n    dataset = tf.data.Dataset.from_tensor_slices(image)\n    dataset = dataset.map(read_image, num_parallel_calls = AUTO)\n    dataset = dataset.batch(BATCH_SIZE)\n    dataset = dataset.prefetch(AUTO)\n    return dataset\n\n# Arcmarginproduct class keras layer\nclass ArcMarginProduct(tf.keras.layers.Layer):\n    '''\n    Implements large margin arc distance.\n\n    Reference:\n        https://arxiv.org/pdf/1801.07698.pdf\n        https://github.com/lyakaap/Landmark2019-1st-and-3rd-Place-Solution/\n            blob/master/src/modeling/metric_learning.py\n    '''\n    def __init__(self, n_classes, s=30, m=0.50, easy_margin=False,\n                 ls_eps=0.0, **kwargs):\n\n        super(ArcMarginProduct, self).__init__(**kwargs)\n\n        self.n_classes = n_classes\n        self.s = s\n        self.m = m\n        self.ls_eps = ls_eps\n        self.easy_margin = easy_margin\n        self.cos_m = tf.math.cos(m)\n        self.sin_m = tf.math.sin(m)\n        self.th = tf.math.cos(math.pi - m)\n        self.mm = tf.math.sin(math.pi - m) * m\n\n    def get_config(self):\n\n        config = super().get_config().copy()\n        config.update({\n            'n_classes': self.n_classes,\n            's': self.s,\n            'm': self.m,\n            'ls_eps': self.ls_eps,\n            'easy_margin': self.easy_margin,\n        })\n        return config\n\n    def build(self, input_shape):\n        super(ArcMarginProduct, self).build(input_shape[0])\n\n        self.W = self.add_weight(\n            name='W',\n            shape=(int(input_shape[0][-1]), self.n_classes),\n            initializer='glorot_uniform',\n            dtype='float32',\n            trainable=True,\n            regularizer=None)\n\n    def call(self, inputs):\n        X, y = inputs\n        y = tf.cast(y, dtype=tf.int32)\n        cosine = tf.matmul(\n            tf.math.l2_normalize(X, axis=1),\n            tf.math.l2_normalize(self.W, axis=0)\n        )\n        sine = tf.math.sqrt(1.0 - tf.math.pow(cosine, 2))\n        phi = cosine * self.cos_m - sine * self.sin_m\n        if self.easy_margin:\n            phi = tf.where(cosine > 0, phi, cosine)\n        else:\n            phi = tf.where(cosine > self.th, phi, cosine - self.mm)\n        one_hot = tf.cast(\n            tf.one_hot(y, depth=self.n_classes),\n            dtype=cosine.dtype\n        )\n        if self.ls_eps > 0:\n            one_hot = (1 - self.ls_eps) * one_hot + self.ls_eps / self.n_classes\n\n        output = (one_hot * phi) + ((1.0 - one_hot) * cosine)\n        output *= self.s\n        return output\n\n\n\n# Function to get the embeddings of our images with the fine-tuned model\ndef get_image_embeddings(image_paths,model_type='B0'):\n    print(\"get_image_embeddings model type =========>\",model_type)\n    embeds = []\n    \n    margin = ArcMarginProduct(\n            n_classes = 11014, \n            s = 30, \n            m = 0.7, \n            name='head/arc_margin', \n            dtype='float32'\n            )\n\n    inp = tf.keras.layers.Input(shape = (*IMAGE_SIZE, 3), name = 'inp1')\n    label = tf.keras.layers.Input(shape = (), name = 'inp2')\n    if model_type=='B0':\n        model_path='../input/shopee-efficientnetb0-arcmarginproduct/EfficientNetB0_512_42.h5'\n        x = efn.EfficientNetB0(weights = None, include_top = False)(inp)\n    elif model_type=='B1':\n        model_path='../input/shopee-efficientnetb1-arcmarginproduct/EfficientNetB1_512_42.h5'\n        x = efn.EfficientNetB1(weights = None, include_top = False)(inp)\n    elif model_type=='B2':\n        model_path='../input/shopee-efficientnetb2-arcmarginproduct/EfficientNetB2_512_42.h5'\n        x = efn.EfficientNetB2(weights = None, include_top = False)(inp) \n    elif model_type=='B3':\n        model_path='../input/shopee-efficientnetb3-arcmarginproduct/EfficientNetB3_512_42.h5'\n        x = efn.EfficientNetB3(weights = None, include_top = False)(inp) \n    elif model_type=='B4':\n        model_path='../input/shopee-efficientnetb4-arcmarginproduct/EfficientNetB4_512_42.h5'\n        x = efn.EfficientNetB4(weights = None, include_top = False)(inp) \n    else:\n        model_path='../input/shopee-efficientnetb7-arcmarginproduct/EfficientNetB7_512_42.h5'\n        x = efn.EfficientNetB7(weights = None, include_top = False)(inp) \n    x = tf.keras.layers.GlobalAveragePooling2D()(x)\n    x = margin([x, label])\n\n    output = tf.keras.layers.Softmax(dtype='float32')(x)\n    model = tf.keras.models.Model(inputs = [inp, label], outputs = [output])\n    model.load_weights(model_path)\n    \n    model = tf.keras.models.Model(inputs = model.input[0], outputs = model.layers[-4].output)\n    chunk = 5000\n    iterator = np.arange(np.ceil(len(df) / chunk))\n    for j in tqdm(iterator):\n        a = int(j * chunk)\n        b = int((j + 1) * chunk)\n        image_dataset = get_dataset(image_paths[a:b])\n        image_embeddings = model.predict(image_dataset)\n        embeds.append(image_embeddings)\n    del model\n    image_embeddings = np.concatenate(embeds)\n    print(f'Our image embeddings shape is {image_embeddings.shape}')\n    del embeds\n    gc.collect()\n    return image_embeddings\n\n\n# Return tokens, masks and segments from a text array or series\ndef bert_encode(texts, tokenizer, max_len=512):\n    all_tokens = []\n    all_masks = []\n    all_segments = []\n    \n    for text in texts:\n        text = tokenizer.tokenize(text)\n            \n        text = text[:max_len-2]\n        input_sequence = [\"[CLS]\"] + text + [\"[SEP]\"]\n        pad_len = max_len - len(input_sequence)\n        \n        tokens = tokenizer.convert_tokens_to_ids(input_sequence)\n        tokens += [0] * pad_len\n        pad_masks = [1] * len(input_sequence) + [0] * pad_len\n        segment_ids = [0] * max_len\n        \n        all_tokens.append(tokens)\n        all_masks.append(pad_masks)\n        all_segments.append(segment_ids)\n    \n    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)\n\n# Function to get our text title embeddings using a pre-trained bert model\ndef get_text_embeddings(df, max_len = 70):\n    embeds = []\n    module_url = \"/kaggle/input/shopeeexternalmodels/bert_en_uncased_L-24_H-1024_A-16_1\"\n    bert_layer = hub.KerasLayer(module_url, trainable = True)\n    vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n    do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n    tokenizer = tokenization.FullTokenizer(vocab_file, do_lower_case)\n    text = bert_encode(df['title'].values, tokenizer, max_len = max_len)\n    \n    margin = ArcMarginProduct(\n            n_classes = 11014, \n            s = 30, \n            m = 0.5, \n            name='head/arc_margin', \n            dtype='float32'\n            )\n    \n    input_word_ids = tf.keras.layers.Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n    input_mask = tf.keras.layers.Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\n    segment_ids = tf.keras.layers.Input(shape=(max_len,), dtype=tf.int32, name=\"segment_ids\")\n    label = tf.keras.layers.Input(shape = (), name = 'label')\n\n    _, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n    clf_output = sequence_output[:, 0, :]\n    x = margin([clf_output, label])\n    output = tf.keras.layers.Softmax(dtype='float32')(x)\n    model = tf.keras.models.Model(inputs = [input_word_ids, input_mask, segment_ids, label], outputs = [output])\n    \n    model.load_weights('../input/bert-baseline/Bert_123.h5')\n    model = tf.keras.models.Model(inputs = model.input[0:3], outputs = model.layers[-4].output)\n    chunk = 5000\n    iterator = np.arange(np.ceil(len(df) / chunk))\n    for j in tqdm(iterator):\n        a = int(j * chunk)\n        b = int((j + 1) * chunk)\n        text_chunk = ((text[0][a:b], text[1][a:b], text[2][a:b]))\n        text_embeddings = model.predict(text_chunk, batch_size = BATCH_SIZE)\n        embeds.append(text_embeddings)\n    del model\n    text_embeddings = np.concatenate(embeds)\n    print(f'Our text embeddings shape is {text_embeddings.shape}')\n    del embeds\n    gc.collect()\n    return text_embeddings\n    ","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Part2 Huggingface Indonesian-Distilbert","metadata":{}},{"cell_type":"code","source":"\n# https://www.kaggle.com/moeinshariatnia/indonesian-distilbert-finetuning-with-arcmargin\nimport math\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport transformers\nfrom transformers import (BertTokenizer, BertModel,\n                          DistilBertTokenizer, DistilBertModel)\nclass TextDataset(torch.utils.data.Dataset):\n    def __init__(self, dataframe, tokenizer, mode=\"train\", max_length=None):\n        self.dataframe = dataframe\n        if mode != \"test\":\n            self.targets = dataframe['label_code'].values\n        texts = list(dataframe['title'].apply(lambda o: str(o)).values)\n        self.encodings = tokenizer(texts, \n                                   padding=True, \n                                   truncation=True, \n                                   max_length=max_length)\n        self.mode = mode\n        \n        \n    def __getitem__(self, idx):\n        # putting each tensor in front of the corresponding key from the tokenizer\n        # HuggingFace tokenizers give you whatever you need to feed to the corresponding model\n        item = {key: torch.tensor(values[idx]) for key, values in self.encodings.items()}\n        # when testing, there are no targets so we won't do the following\n        if self.mode != \"test\":\n            item['labels'] = torch.tensor(self.targets[idx]).long()\n        return item\n    \n    def __len__(self):\n        return len(self.dataframe)\n    \n    \n    \nclass CFG:\n    DistilBERT = True # if set to False, BERT model will be used\n    bert_hidden_size = 768\n    \n    batch_size = 16\n    epochs = 50\n    num_workers = 4\n    learning_rate = 1e-5 #3e-5\n    scheduler = \"ReduceLROnPlateau\"\n    step = 'epoch'\n    patience = 2\n    factor = 0.8\n    dropout = 0.5\n    model_path = \"outputs/\"\n    max_length = 60\n    model_save_name = \"model.pt\"\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else 'cpu')\n\nclass ArcMarginProduct_torch(nn.Module):\n    r\"\"\"Implement of large margin arc distance: :\n        Args:\n            in_features: size of each input sample\n            out_features: size of each output sample\n            s: norm of input feature\n            m: margin\n            cos(theta + m)\n        \"\"\"\n    def __init__(self, in_features, out_features, s=30.0, m=0.50, easy_margin=False):\n        super(ArcMarginProduct_torch, self).__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.s = s\n        self.m = m\n        self.weight = nn.Parameter(torch.FloatTensor(out_features, in_features))\n        nn.init.xavier_uniform_(self.weight)\n\n        self.easy_margin = easy_margin\n        self.cos_m = math.cos(m)\n        self.sin_m = math.sin(m)\n        self.th = math.cos(math.pi - m)\n        self.mm = math.sin(math.pi - m) * m\n\n    def forward(self, input, label):\n        # --------------------------- cos(theta) & phi(theta) ---------------------------\n        cosine = F.linear(F.normalize(input), F.normalize(self.weight))\n        sine = torch.sqrt((1.0 - torch.pow(cosine, 2)).clamp(0, 1))\n        phi = cosine * self.cos_m - sine * self.sin_m\n        if self.easy_margin:\n            phi = torch.where(cosine > 0, phi, cosine)\n        else:\n            phi = torch.where(cosine > self.th, phi, cosine - self.mm)\n        # --------------------------- convert label to one-hot ---------------------------\n        # one_hot = torch.zeros(cosine.size(), requires_grad=True, device='cuda')\n        one_hot = torch.zeros(cosine.size(), device=CFG.device)\n        one_hot.scatter_(1, label.view(-1, 1).long(), 1)\n        # -------------torch.where(out_i = {x_i if condition_i else y_i) -------------\n        output = (one_hot * phi) + ((1.0 - one_hot) * cosine)  # you can use torch.where if your torch.__version__ is 0.4\n        output *= self.s\n        # print(output)\n\n        return output\n    \nclass Model(nn.Module):\n    def __init__(self, \n                 bert_model, \n                 num_classes=11014, \n                 last_hidden_size=CFG.bert_hidden_size):\n        \n        super().__init__()\n        self.bert_model = bert_model\n        self.arc_margin = ArcMarginProduct_torch(last_hidden_size, \n                                           num_classes, \n                                           s=30.0, \n                                           m=0.50, \n                                           easy_margin=False)\n    \n    def get_bert_features(self, batch):\n        output = self.bert_model(input_ids=batch['input_ids'], attention_mask=batch['attention_mask'])\n        last_hidden_state = output.last_hidden_state # shape: (batch_size, seq_length, bert_hidden_dim)\n        CLS_token_state = last_hidden_state[:, 0, :] # obtaining CLS token state which is the first token.\n        return CLS_token_state\n    \n    def forward(self, batch):\n        CLS_hidden_state = self.get_bert_features(batch)\n        output = self.arc_margin(CLS_hidden_state, batch['labels'])\n        return output\n\nprint(\"image bert\")","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_indoesian_text_embedding(df):\n    \n    # model_name='cahya/distilbert-base-indonesian'\n    model_name='../input/indonesian-distilbert-arcmargin/distilbert-base-indonesian/'\n    tokenizer = DistilBertTokenizer.from_pretrained(model_name,local_files_only=True)\n    bert_model = DistilBertModel.from_pretrained(model_name,local_files_only=True)\n    \n    model = Model(bert_model).to(CFG.device)\n    del bert_model\n#     model.load_state_dict(torch.load('../input/indonesian-arc/model-m0.15.pt'))\n    model.load_state_dict(torch.load('../input/indonesian-arc/model11.pt'))\n    \n    \n    model.eval()\n\n    embeds=[]\n    test_dataset = TextDataset(df, tokenizer, mode='test',max_length=CFG.max_length)\n    test_loader = torch.utils.data.DataLoader(test_dataset,\n                                               batch_size=CFG.batch_size, \n                                               num_workers=CFG.num_workers, \n                                               shuffle=False)\n    \n\n    tqdm_object = tqdm(test_loader, total=len(test_loader))\n    with torch.no_grad():\n        for batch in tqdm_object:\n            batch = {k: v.to(CFG.device) for k, v in batch.items()}\n            preds = model.get_bert_features(batch)\n            embeds.append(preds.cpu())\n        \n    del model\n    text_embeddings = np.concatenate(embeds)\n    print(f'Our text embeddings shape is {text_embeddings.shape}')\n    del embeds\n    gc.collect()\n    return text_embeddings\n\nprint(\"indonesian\")","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Part3 Densenet ArcFace","metadata":{}},{"cell_type":"code","source":"image_size = 512\nbatch_size = 32\nnum_workers = 4\nn_batch = 10 # to avoid oom, split 70000+ images into 10 batches\nsim_thresh = 0.9\n\n\nimport pandas as pd\nimport numpy as np\nimport sys\nsys.path.append('../input/timm-pytorch-image-models/pytorch-image-models-master')\nimport os\nimport sys\nimport time\nimport cv2\nimport PIL.Image\nimport random\nfrom sklearn.metrics import accuracy_score\nfrom tqdm.notebook import tqdm\nimport torch\nfrom torch.utils.data import DataLoader, Dataset\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision\nimport torchvision.transforms as transforms\nimport torch.optim as optim\nfrom torch.optim.lr_scheduler import CosineAnnealingLR\nimport albumentations\nfrom tqdm.notebook import tqdm\nimport matplotlib.pyplot as plt\nimport gc\nfrom sklearn.metrics import roc_auc_score\n%matplotlib inline\nimport seaborn as sns\nfrom pylab import rcParams\nimport timm\nfrom warnings import filterwarnings\nfrom sklearn.preprocessing import LabelEncoder\nimport math\nimport glob\nfilterwarnings(\"ignore\")\n\ndevice = torch.device('cuda') \n\n\ndef seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    print(f'Setting all seeds to be {seed} to reproduce...')\nseed_everything(42)\n\n\ntransforms_valid = albumentations.Compose([\n    albumentations.Resize(image_size, image_size),\n    albumentations.Normalize()\n])\n\n\nclass SHOPEEDataset(Dataset):\n    def __init__(self, df, mode, transform=None):\n        \n        self.df = df.reset_index(drop=True)\n        self.mode = mode\n        self.transform = transform\n        \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, index):\n        row = self.df.loc[index]\n        img = cv2.imread(row.file_path)\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        \n        if self.transform is not None:\n            res = self.transform(image=img)\n            img = res['image']\n                \n        img = img.astype(np.float32)\n        img = img.transpose(2,0,1)\n        \n        if self.mode == 'test':\n            return torch.tensor(img).float()\n        else:\n            return torch.tensor(img).float()\n#             return torch.tensor(img).float(), torch.tensor(row.label_group).float()\n        \n        \n        \n        \nclass DesnetArcModule(nn.Module):\n    def __init__(self, in_features, out_features, s=10, m=0.5):\n        super().__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.s = s\n        self.m = m\n        self.weight = nn.Parameter(torch.FloatTensor(out_features, in_features))\n        nn.init.xavier_normal_(self.weight)\n\n        self.cos_m = math.cos(m)\n        self.sin_m = math.sin(m)\n        self.th = torch.tensor(math.cos(math.pi - m))\n        self.mm = torch.tensor(math.sin(math.pi - m) * m)\n\n    def forward(self, inputs, labels):\n        cos_th = F.linear(inputs, F.normalize(self.weight))\n        cos_th = cos_th.clamp(-1, 1)\n        sin_th = torch.sqrt(1.0 - torch.pow(cos_th, 2))\n        cos_th_m = cos_th * self.cos_m - sin_th * self.sin_m\n        # print(type(cos_th), type(self.th), type(cos_th_m), type(self.mm))\n        cos_th_m = torch.where(cos_th > self.th, cos_th_m, cos_th - self.mm)\n\n        cond_v = cos_th - self.th\n        cond = cond_v <= 0\n        cos_th_m[cond] = (cos_th - self.mm)[cond]\n\n        if labels.dim() == 1:\n            labels = labels.unsqueeze(-1)\n        onehot = torch.zeros(cos_th.size()).cuda()\n        labels = labels.type(torch.LongTensor).cuda()\n        onehot.scatter_(1, labels, 1.0)\n        outputs = onehot * cos_th_m + (1.0 - onehot) * cos_th\n        outputs = outputs * self.s\n        return outputs\nclass SHOPEEDenseNet(nn.Module):\n\n    def __init__(self, channel_size, out_feature, dropout=0.5, backbone='densenet121', pretrained=False):\n        super(SHOPEEDenseNet, self).__init__()\n        self.backbone = timm.create_model(backbone, pretrained=pretrained)\n        self.channel_size = channel_size\n        self.out_feature = out_feature\n        self.in_features = self.backbone.classifier.in_features\n        self.margin = DesnetArcModule(in_features=self.channel_size, out_features = self.out_feature)\n        self.bn1 = nn.BatchNorm2d(self.in_features)\n        self.dropout = nn.Dropout2d(dropout, inplace=True)\n        self.fc1 = nn.Linear(self.in_features * 16 * 16 , self.channel_size)\n        self.bn2 = nn.BatchNorm1d(self.channel_size)\n        \n    def forward(self, x, labels=None):\n        features = self.backbone.features(x)\n        features = self.bn1(features)\n        features = self.dropout(features)\n        features = features.view(features.size(0), -1)\n        features = self.fc1(features)\n        features = self.bn2(features)\n        features = F.normalize(features)\n        if labels is not None:\n            return self.margin(features, labels)\n        return features\n    \n\n    \n    \ndef generate_desent_features(df_loader):\n    model = SHOPEEDenseNet(512, 11014)\n    model.load_state_dict(\n        torch.load('../input/shopeeexternalmodels/baseline_fold0_densenet_512_epoch40.pth', map_location='cuda:0'))\n    model.to(device)\n    model.eval()\n    bar = tqdm(df_loader)\n\n    FEAS = []\n    TARGETS = []\n\n    with torch.no_grad():\n        for batch_idx, (images) in enumerate(bar):\n            #             print(images)\n            images = images.to(device)\n\n            features = model(images)\n\n            FEAS += [features.detach().cpu()]\n    FEAS = torch.cat(FEAS).cpu().numpy()\n    del model\n    gc.collect()\n    return FEAS\n\n\n","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df, df_cu, image_paths = read_dataset()\nprint(df.shape)\ngc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## TFIDF","metadata":{}},{"cell_type":"code","source":"from cuml.feature_extraction.text import TfidfVectorizer\n\nmodel = TfidfVectorizer(stop_words=None, binary=True, max_features=25000)\ntext_embeddings2 = model.fit_transform(df_cu.title).toarray()\nprint('text embeddings shape',text_embeddings2.shape)\n\n\npreds = []\nCHUNK = 1024*4\n\nprint('Finding similar titles...')\nCTS = len(df_cu)//CHUNK\nif len(df_cu)%CHUNK!=0: CTS += 1\nfor j in range( CTS ):\n    \n    a = j*CHUNK\n    b = (j+1)*CHUNK\n    b = min(b,len(df_cu))\n    print('chunk',a,'to',b)\n    \n    # COSINE SIMILARITY DISTANCE\n    # cts = np.dot( text_embeddings, text_embeddings[a:b].T).T\n    cts = cupy.matmul(text_embeddings2, text_embeddings2[a:b].T).T\n    \n    for k in range(b-a):\n        # IDX = np.where(cts[k,]>0.7)[0]\n        IDX = cupy.where(cts[k,]>0.775)[0]\n        o = df_cu.iloc[cupy.asnumpy(IDX)].posting_id.to_pandas().values\n        if len(o) > 1:\n            IDX_again = cupy.where(cts[k,] > 0.80)[0]\n            o_again = df.iloc[cupy.asnumpy(IDX_again)].posting_id.values\n            if len(o_again) > 1 :\n                preds.append(o_again)\n            else:\n                preds.append(o)\n        else:\n            preds.append(o)\n                \ndel model, text_embeddings2\n\ndf_cu['oof_text'] = preds","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## eca-nfnet-l0","metadata":{}},{"cell_type":"code","source":"import sys\nsys.path.append('../input/timm-pytorch-image-models/pytorch-image-models-master')\nimport numpy as np \nimport pandas as pd \n\nimport math\nimport random \nimport os \nimport cv2\nimport timm\n\nfrom tqdm import tqdm \n\nimport albumentations as A \nfrom albumentations.pytorch.transforms import ToTensorV2\n\nimport torch \nfrom torch.utils.data import Dataset \nfrom torch import nn\nimport torch.nn.functional as F \n\nimport gc\nimport cudf\nimport cuml\nimport cupy\nfrom cuml.feature_extraction.text import TfidfVectorizer\nfrom cuml.neighbors import NearestNeighbors\n\n\nclass ECACFG:\n    \n    img_size = 512\n    batch_size = 12\n    seed = 2020\n    \n    device = 'cuda'\n    classes = 11014\n    \n    model_name = 'eca_nfnet_l0'\n#     model_path = '../input/shopee-pytorch-models/arcface_512x512_nfnet_l0 (mish).pt'\n    model_path = '../input/shopee-pytorch-models/arcface_512x512_nfnet_l0 (mish).pt'\n    scale = 30 \n    margin = 0.5\n\n\n\ndef get_test_transforms():\n\n    return A.Compose(\n        [\n            A.Resize(ECACFG.img_size,ECACFG.img_size,always_apply=True),\n            A.Normalize(),\n        ToTensorV2(p=1.0)\n        ]\n    )\n\n\nclass ECAShopeeDataset(Dataset):\n    def __init__(self, image_paths, transforms=None):\n\n        self.image_paths = image_paths\n        self.augmentations = transforms\n\n    def __len__(self):\n        return self.image_paths.shape[0]\n\n    def __getitem__(self, index):\n        image_path = self.image_paths[index]\n        \n        image = cv2.imread(image_path)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        \n        if self.augmentations:\n            augmented = self.augmentations(image=image)\n            image = augmented['image']       \n    \n        return image,torch.tensor(1)\n    \nclass ECAArcMarginProduct(nn.Module):\n    def __init__(self, in_features, out_features, scale=30.0, margin=0.50, easy_margin=False, ls_eps=0.0):\n        super(ECAArcMarginProduct, self).__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.scale = scale\n        self.margin = margin\n        self.ls_eps = ls_eps  # label smoothing\n        self.weight = nn.Parameter(torch.FloatTensor(out_features, in_features))\n        nn.init.xavier_uniform_(self.weight)\n\n        self.easy_margin = easy_margin\n        self.cos_m = math.cos(margin)\n        self.sin_m = math.sin(margin)\n        self.th = math.cos(math.pi - margin)\n        self.mm = math.sin(math.pi - margin) * margin\n\n    def forward(self, input, label):\n        # --------------------------- cos(theta) & phi(theta) ---------------------------\n        cosine = F.linear(F.normalize(input), F.normalize(self.weight))\n        sine = torch.sqrt(1.0 - torch.pow(cosine, 2))\n        phi = cosine * self.cos_m - sine * self.sin_m\n        if self.easy_margin:\n            phi = torch.where(cosine > 0, phi, cosine)\n        else:\n            phi = torch.where(cosine > self.th, phi, cosine - self.mm)\n        # --------------------------- convert label to one-hot ---------------------------\n        # one_hot = torch.zeros(cosine.size(), requires_grad=True, device='cuda')\n        one_hot = torch.zeros(cosine.size(), device='cuda')\n        one_hot.scatter_(1, label.view(-1, 1).long(), 1)\n        if self.ls_eps > 0:\n            one_hot = (1 - self.ls_eps) * one_hot + self.ls_eps / self.out_features\n        # -------------torch.where(out_i = {x_i if condition_i else y_i) -------------\n        output = (one_hot * phi) + ((1.0 - one_hot) * cosine)\n        output *= self.scale\n\n        return output\n\nclass ECAShopeeModel(nn.Module):\n\n    def __init__(\n        self,\n        n_classes = ECACFG.classes,\n        model_name = ECACFG.model_name,\n        fc_dim = 512,\n        margin = ECACFG.margin,\n        scale = ECACFG.scale,\n        use_fc = True,\n        pretrained = False,\n        is_swin=False\n    ):\n\n\n        super(ECAShopeeModel,self).__init__()\n        print('Building Model Backbone for {} model'.format(model_name))\n\n        self.backbone = timm.create_model(model_name, pretrained=pretrained)\n\n        if model_name == 'resnext50_32x4d':\n            final_in_features = self.backbone.fc.in_features\n            self.backbone.fc = nn.Identity()\n            self.backbone.global_pool = nn.Identity()\n\n        elif model_name == 'efficientnet_b3':\n            final_in_features = self.backbone.classifier.in_features\n            self.backbone.classifier = nn.Identity()\n            self.backbone.global_pool = nn.Identity()\n\n        elif model_name == 'tf_efficientnet_b5_ns':\n            final_in_features = self.backbone.classifier.in_features\n            self.backbone.classifier = nn.Identity()\n            self.backbone.global_pool = nn.Identity()\n        \n        elif model_name == 'eca_nfnet_l0':\n            final_in_features = self.backbone.head.fc.in_features\n            self.backbone.head.fc = nn.Identity()\n            self.backbone.head.global_pool = nn.Identity()\n        elif 'swin' in model_name:\n            final_in_features = self.backbone.head.in_features\n            self.backbone.norm = nn.Identity()\n            self.backbone.head = nn.Identity()\n            \n            \n        self.pooling =  nn.AdaptiveAvgPool2d(1)\n\n        self.use_fc = use_fc\n        self.is_swin = is_swin\n\n\n        self.dropout = nn.Dropout(p=0.0)\n        self.fc = nn.Linear(final_in_features, fc_dim)\n        self.bn = nn.BatchNorm1d(fc_dim)\n        self._init_params()\n        final_in_features = fc_dim\n\n        self.final = ECAArcMarginProduct(\n            final_in_features,\n            n_classes,\n            scale = scale,\n            margin = margin,\n            easy_margin = False,\n            ls_eps = 0.0\n        )\n\n    def _init_params(self):\n        nn.init.xavier_normal_(self.fc.weight)\n        nn.init.constant_(self.fc.bias, 0)\n        nn.init.constant_(self.bn.weight, 1)\n        nn.init.constant_(self.bn.bias, 0)\n\n    def forward(self, image, label):\n        if not self.is_swin:\n            feature = self.extract_feat(image)\n        else:\n            feature = self.extract_swin_feat(image)\n            print(feature.shape)\n        #logits = self.final(feature,label)\n        return feature\n\n    def extract_feat(self, x):\n        batch_size = x.shape[0]\n        x = self.backbone(x)\n        x = self.pooling(x).view(batch_size, -1)\n\n        if self.use_fc:\n            x = self.dropout(x)\n            x = self.fc(x)\n            x = self.bn(x)\n        return x\n    \n    def extract_swin_feat(self, x):\n        batch_size = x.shape[0]\n        x = self.backbone(x)\n#         print(x.shape)\n#         print(self.pooling(x).shape)\n#         x = self.pooling(x).view(batch_size, -1)\n\n        if self.use_fc:\n            x = self.dropout(x)\n            x = self.fc(x)\n            x = self.bn(x)\n        \n        return x\n    \nclass Mish_func(torch.autograd.Function):\n    \n    \"\"\"from: https://github.com/tyunist/memory_efficient_mish_swish/blob/master/mish.py\"\"\"\n    \n    @staticmethod\n    def forward(ctx, i):\n        result = i * torch.tanh(F.softplus(i))\n        ctx.save_for_backward(i)\n        return result\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        i = ctx.saved_variables[0]\n  \n        v = 1. + i.exp()\n        h = v.log() \n        grad_gh = 1./h.cosh().pow_(2) \n\n        # Note that grad_hv * grad_vx = sigmoid(x)\n        #grad_hv = 1./v  \n        #grad_vx = i.exp()\n        \n        grad_hx = i.sigmoid()\n\n        grad_gx = grad_gh *  grad_hx #grad_hv * grad_vx \n        \n        grad_f =  torch.tanh(F.softplus(i)) + i * grad_gx \n        \n        return grad_output * grad_f \n\n\nclass Mish(nn.Module):\n    def __init__(self, **kwargs):\n        super().__init__()\n        pass\n    def forward(self, input_tensor):\n        return Mish_func.apply(input_tensor)\n\n\ndef replace_activations(model, existing_layer, new_layer):\n    \n    \"\"\"A function for replacing existing activation layers\"\"\"\n    \n    for name, module in reversed(model._modules.items()):\n        if len(list(module.children())) > 0:\n            model._modules[name] = replace_activations(module, existing_layer, new_layer)\n\n        if type(module) == existing_layer:\n            layer_old = module\n            layer_new = new_layer\n            model._modules[name] = layer_new\n    return model\n\n\ndef get_ecanfnet_embeddings(image_paths, model_name = ECACFG.model_name):\n    embeds = []\n    \n    model = ECAShopeeModel(model_name = model_name)\n    model.eval()\n    \n    if model_name == 'eca_nfnet_l0':\n        model = replace_activations(model, torch.nn.SiLU, Mish())\n\n    model.load_state_dict(torch.load(ECACFG.model_path))\n    model = model.to(ECACFG.device)\n    \n\n    image_dataset = ECAShopeeDataset(image_paths=image_paths,transforms=get_test_transforms())\n    image_loader = torch.utils.data.DataLoader(\n        image_dataset,\n        batch_size=ECACFG.batch_size,\n        pin_memory=True,\n        drop_last=False,\n        num_workers=4\n    )\n    \n    \n    with torch.no_grad():\n        for img,label in tqdm(image_loader): \n            img = img.cuda()\n            label = label.cuda()\n            feat = model(img,label)\n            image_embeddings = feat.detach().cpu().numpy()\n            embeds.append(image_embeddings)\n    \n    del image_dataset,image_loader\n    del model\n    image_embeddings = np.concatenate(embeds)\n    print(f'Our image embeddings shape is {image_embeddings.shape}')\n    del embeds\n    gc.collect()\n    torch.cuda.empty_cache()\n    return image_embeddings\n\n\nclass ECACFG:\n    \n    img_size = 512\n    batch_size = 12\n    seed = 2020\n    \n    device = 'cuda'\n    classes = 11014\n    \n    model_name = 'eca_nfnet_l0'\n#     model_path = '../input/shopee-pytorch-models/arcface_512x512_nfnet_l0 (mish).pt'\n    model_path = '../input/shopee-pytorch-models/arcface_512x512_nfnet_l0 (mish).pt'\n    scale = 30 \n    margin = 0.5\n    \n\n\nif not GET_CV:\n    \n    image_eca_embeddings = get_ecanfnet_embeddings(image_paths.values)\n    print(\"image_eca_embeddings.shape\",image_eca_embeddings.shape)","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## SWIN model","metadata":{}},{"cell_type":"code","source":"import sys\nsys.path.append('../input/timm-pytorch-image-models/pytorch-image-models-master')\nimport numpy as np \nimport pandas as pd \n\nimport math\nimport random \nimport os \nimport cv2\nimport timm\n\nfrom tqdm import tqdm \n\nimport albumentations as A \nfrom albumentations.pytorch.transforms import ToTensorV2\n\nimport torch \nfrom torch.utils.data import Dataset \nfrom torch import nn\nimport torch.nn.functional as F \n\nimport gc\nimport cudf\nimport cuml\nimport cupy\nfrom cuml.feature_extraction.text import TfidfVectorizer\nfrom cuml.neighbors import NearestNeighbors\n\n\n\n\nclass ECACFG:\n    \n    img_size = 224\n    batch_size = 12\n    seed = 2020\n    \n    device = 'cuda'\n    classes = 11014\n    \n    model_name = 'swin_small_patch4_window7_224'\n    model_path = '../input/shopee-pytorch-swin-transformer-image-training/swin_small_patch4_window7_224.pt'\n    scale = 30 \n    margin = 0.5\n\ndef get_test_transforms():\n\n    return A.Compose(\n        [\n            A.Resize(ECACFG.img_size,ECACFG.img_size,always_apply=True),\n            A.Normalize(),\n        ToTensorV2(p=1.0)\n        ]\n    )\n\n\nclass ECAShopeeDataset(Dataset):\n    def __init__(self, image_paths, transforms=None):\n\n        self.image_paths = image_paths\n        self.augmentations = transforms\n\n    def __len__(self):\n        return self.image_paths.shape[0]\n\n    def __getitem__(self, index):\n        image_path = self.image_paths[index]\n        \n        image = cv2.imread(image_path)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        \n        if self.augmentations:\n            augmented = self.augmentations(image=image)\n            image = augmented['image']       \n    \n        return image,torch.tensor(1)\n    \nclass ECAArcMarginProduct(nn.Module):\n    def __init__(self, in_features, out_features, scale=30.0, margin=0.50, easy_margin=False, ls_eps=0.0):\n        super(ECAArcMarginProduct, self).__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.scale = scale\n        self.margin = margin\n        self.ls_eps = ls_eps  # label smoothing\n        self.weight = nn.Parameter(torch.FloatTensor(out_features, in_features))\n        nn.init.xavier_uniform_(self.weight)\n\n        self.easy_margin = easy_margin\n        self.cos_m = math.cos(margin)\n        self.sin_m = math.sin(margin)\n        self.th = math.cos(math.pi - margin)\n        self.mm = math.sin(math.pi - margin) * margin\n\n    def forward(self, input, label):\n        # --------------------------- cos(theta) & phi(theta) ---------------------------\n        cosine = F.linear(F.normalize(input), F.normalize(self.weight))\n        sine = torch.sqrt(1.0 - torch.pow(cosine, 2))\n        phi = cosine * self.cos_m - sine * self.sin_m\n        if self.easy_margin:\n            phi = torch.where(cosine > 0, phi, cosine)\n        else:\n            phi = torch.where(cosine > self.th, phi, cosine - self.mm)\n        # --------------------------- convert label to one-hot ---------------------------\n        # one_hot = torch.zeros(cosine.size(), requires_grad=True, device='cuda')\n        one_hot = torch.zeros(cosine.size(), device='cuda')\n        one_hot.scatter_(1, label.view(-1, 1).long(), 1)\n        if self.ls_eps > 0:\n            one_hot = (1 - self.ls_eps) * one_hot + self.ls_eps / self.out_features\n        # -------------torch.where(out_i = {x_i if condition_i else y_i) -------------\n        output = (one_hot * phi) + ((1.0 - one_hot) * cosine)\n        output *= self.scale\n\n        return output\n\nclass ECAShopeeModel(nn.Module):\n\n    def __init__(\n        self,\n        n_classes = ECACFG.classes,\n        model_name = ECACFG.model_name,\n        fc_dim = 512,\n        margin = ECACFG.margin,\n        scale = ECACFG.scale,\n        use_fc = True,\n        pretrained = False,\n        is_swin=False\n    ):\n\n\n        super(ECAShopeeModel,self).__init__()\n        print('Building Model Backbone for {} model'.format(model_name))\n\n        self.backbone = timm.create_model(model_name, pretrained=pretrained)\n\n        if model_name == 'resnext50_32x4d':\n            final_in_features = self.backbone.fc.in_features\n            self.backbone.fc = nn.Identity()\n            self.backbone.global_pool = nn.Identity()\n\n        elif model_name == 'efficientnet_b3':\n            final_in_features = self.backbone.classifier.in_features\n            self.backbone.classifier = nn.Identity()\n            self.backbone.global_pool = nn.Identity()\n\n        elif model_name == 'tf_efficientnet_b5_ns':\n            final_in_features = self.backbone.classifier.in_features\n            self.backbone.classifier = nn.Identity()\n            self.backbone.global_pool = nn.Identity()\n        \n        elif model_name == 'eca_nfnet_l0':\n            final_in_features = self.backbone.head.fc.in_features\n            self.backbone.head.fc = nn.Identity()\n            self.backbone.head.global_pool = nn.Identity()\n        elif 'swin' in model_name:\n            final_in_features = self.backbone.head.in_features\n            self.backbone.norm = nn.Identity()\n            self.backbone.head = nn.Identity()\n            \n            \n        self.pooling =  nn.AdaptiveAvgPool2d(1)\n\n        self.use_fc = use_fc\n        self.is_swin = is_swin\n\n\n        self.dropout = nn.Dropout(p=0.0)\n        self.fc = nn.Linear(final_in_features, fc_dim)\n        self.bn = nn.BatchNorm1d(fc_dim)\n        self._init_params()\n        final_in_features = fc_dim\n\n        self.final = ECAArcMarginProduct(\n            final_in_features,\n            n_classes,\n            scale = scale,\n            margin = margin,\n            easy_margin = False,\n            ls_eps = 0.0\n        )\n\n    def _init_params(self):\n        nn.init.xavier_normal_(self.fc.weight)\n        nn.init.constant_(self.fc.bias, 0)\n        nn.init.constant_(self.bn.weight, 1)\n        nn.init.constant_(self.bn.bias, 0)\n\n    def forward(self, image, label):\n        if not self.is_swin:\n            feature = self.extract_feat(image)\n        else:\n            feature = self.extract_swin_feat(image)\n        #logits = self.final(feature,label)\n        return feature\n\n    def extract_feat(self, x):\n        batch_size = x.shape[0]\n        x = self.backbone(x)\n        x = self.pooling(x).view(batch_size, -1)\n\n        if self.use_fc:\n            x = self.dropout(x)\n            x = self.fc(x)\n            x = self.bn(x)\n        return x\n    \n    def extract_swin_feat(self, x):\n        batch_size = x.shape[0]\n        x = self.backbone(x)\n#         print(x.shape)\n#         print(self.pooling(x).shape)\n#         x = self.pooling(x).view(batch_size, -1)\n\n        if self.use_fc:\n            x = self.dropout(x)\n            x = self.fc(x)\n            x = self.bn(x)\n        \n        return x\n    \nclass Mish_func(torch.autograd.Function):\n    \n    \"\"\"from: https://github.com/tyunist/memory_efficient_mish_swish/blob/master/mish.py\"\"\"\n    \n    @staticmethod\n    def forward(ctx, i):\n        result = i * torch.tanh(F.softplus(i))\n        ctx.save_for_backward(i)\n        return result\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        i = ctx.saved_variables[0]\n  \n        v = 1. + i.exp()\n        h = v.log() \n        grad_gh = 1./h.cosh().pow_(2) \n\n        # Note that grad_hv * grad_vx = sigmoid(x)\n        #grad_hv = 1./v  \n        #grad_vx = i.exp()\n        \n        grad_hx = i.sigmoid()\n\n        grad_gx = grad_gh *  grad_hx #grad_hv * grad_vx \n        \n        grad_f =  torch.tanh(F.softplus(i)) + i * grad_gx \n        \n        return grad_output * grad_f \n\n\nclass Mish(nn.Module):\n    def __init__(self, **kwargs):\n        super().__init__()\n        pass\n    def forward(self, input_tensor):\n        return Mish_func.apply(input_tensor)\n\n\ndef replace_activations(model, existing_layer, new_layer):\n    \n    \"\"\"A function for replacing existing activation layers\"\"\"\n    \n    for name, module in reversed(model._modules.items()):\n        if len(list(module.children())) > 0:\n            model._modules[name] = replace_activations(module, existing_layer, new_layer)\n\n        if type(module) == existing_layer:\n            layer_old = module\n            layer_new = new_layer\n            model._modules[name] = layer_new\n    return model\n\n\n\ndef get_swin_embeddings(image_paths, model_name = ECACFG.model_name):\n    embeds = []\n    \n    model = ECAShopeeModel(model_name = model_name,is_swin=True)\n    model.eval()\n    \n    if model_name == 'swin':\n        model = replace_activations(model, torch.nn.GELU, Mish())\n\n    model.load_state_dict(torch.load(ECACFG.model_path))\n    model = model.to(ECACFG.device)\n    \n\n    image_dataset = ECAShopeeDataset(image_paths=image_paths,transforms=get_test_transforms())\n    image_loader = torch.utils.data.DataLoader(\n        image_dataset,\n        batch_size=ECACFG.batch_size,\n        pin_memory=True,\n        drop_last=False,\n        num_workers=4\n    )\n    \n    \n    with torch.no_grad():\n        for img,label in tqdm(image_loader): \n            img = img.cuda()\n            label = label.cuda()\n            feat = model(img,label)\n            image_embeddings = feat.detach().cpu().numpy()\n            embeds.append(image_embeddings)\n    \n    del image_dataset,image_loader\n    del model\n    image_embeddings = np.concatenate(embeds)\n    print(f'Our image embeddings shape is {image_embeddings.shape}')\n    del embeds\n    gc.collect()\n    torch.cuda.empty_cache()\n    return image_embeddings\n\n\n\n\nclass ECACFG:\n    \n    img_size = 224\n    batch_size = 12\n    seed = 2020\n    \n    device = 'cuda'\n    classes = 11014\n    \n    model_name = 'swin_small_patch4_window7_224'\n    model_path = '../input/shopee-pytorch-swin-transformer-image-training/swin_small_patch4_window7_224.pt'\n    scale = 30 \n    margin = 0.5\n\n    \nif not GET_CV:\n    \n    image_swin_embeddings = get_swin_embeddings(image_paths.values)\n    print(\"image_swin_embeddings.shape\",image_swin_embeddings.shape)","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if not GET_CV:\n    \n    \n#     np.save(\"image_eca_embeddings.npy\",image_eca_embeddings)    \n    \n#     np.save(\"image_swin_embeddings.npy\",image_swin_embeddings)    \n    image_b3_embeddings = get_image_embeddings(image_paths,model_type=\"B3\")\n    image_b4_embeddings = get_image_embeddings(image_paths,model_type=\"B4\")\n\n    gc.collect()\n    tf.keras.backend.clear_session()\n    \n    \n    text_indonesian_embeddings = get_indoesian_text_embedding(df)\n    torch.cuda.empty_cache()\n    \n    \n    dataset_df = SHOPEEDataset(df, 'train', transform=transforms_valid)\n    df_loader = torch.utils.data.DataLoader(dataset_df, batch_size=batch_size, \n                                            shuffle=False, num_workers=num_workers, \n                                            pin_memory=True)\n    image_desnet512_embeddings = generate_desent_features(df_loader)\n    torch.cuda.empty_cache()\n    \n    \nelse:\n    image_eca_embeddings=np.load('../input/shopeeexternalmodels/image_eca_embeddings.npy')    \n    image_swin_embeddings=np.load('../input/shopeeexternalmodels/image_swin_embeddings.npy')\n    image_b3_embeddings=np.load('../input/allembeddings/image_b3_embeddings.npy')    \n    image_b4_embeddings=np.load('../input/allembeddings/image_b4_embeddings.npy')    \n    text_indonesian_embeddings=np.load('../input/allembeddings/text_indonesian_embeddings.npy')\n    image_desnet512_embeddings=np.load('../input/allembeddings/image_desnet512_embeddings.npy')\n    \n    print(\"image_eca_embeddings.shape\",image_eca_embeddings.shape)    \n    print(\"image_swin_embeddings.shape\",image_swin_embeddings.shape)    \n\n    print(\"image_b3_embeddings.shape\",image_b3_embeddings.shape)    \n    print(\"image_b4_embeddings.shape\",image_b4_embeddings.shape)    \n    print(\"text_indonesian_embeddings.shape\",text_indonesian_embeddings.shape)    \n    print(\"image_desnet512_embeddings.shape\",image_desnet512_embeddings.shape)    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def find_strict_th(cnt=1,ths=[4.8,4.8,4.8,4.8,4.8]):\n    index=0\n    if 1<=cnt<10:\n        index=0\n    elif 10<=cnt<20:\n        index=1\n    elif 20<=cnt<30:\n        index=2\n    elif 30<=cnt<40:\n        index=3\n    else:\n        index=4\n    \n    return ths[index]\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_neighbors(df, \n                  embeddings, \n                  KNN = 50, \n                  image = True,\n                  thresholds=None,\n                  perfect=None,\n                  strict=None,\n                  strict_ths=None,\n                  is_csr=False,\n                  finetuneing=False\n                 ):\n#     KNN=3\n    \n        \n    if is_csr:\n        from sklearn.neighbors import NearestNeighbors\n    else:\n        from cuml.neighbors import NearestNeighbors\n    model = NearestNeighbors(n_neighbors = KNN)\n    model.fit(embeddings)\n    distances, indices = model.kneighbors(embeddings)\n#     print(pd.DataFrame(distances).describe())\n    if GET_CV:\n        scores = []\n        if finetuneing:\n            for threshold in thresholds:\n                predictions = []\n                for k in range(embeddings.shape[0]):\n                    idx = np.where(distances[k,] < threshold)[0]\n                    ids = indices[k,idx]\n                    posting_ids = ' '.join(df['posting_id'].iloc[ids].values)\n                    predictions.append(posting_ids)\n                df['pred_matches'] = predictions\n                df['f1'] = f1_score(df['matches'], df['pred_matches'])\n                score = df['f1'].mean()\n                print(f'Our f1 score for threshold {threshold} is {score}')\n                scores.append(score)\n            thresholds_scores = pd.DataFrame({'thresholds': thresholds, 'scores': scores})\n            max_score = thresholds_scores[thresholds_scores['scores'] == thresholds_scores['scores'].max()]\n            best_threshold = max_score['thresholds'].values[0]\n            best_score = max_score['scores'].values[0]\n            print(f'Our best score is {best_score} and has a threshold {best_threshold}')\n        \n        # Use threshold\n        predictions = []\n        for k in range(embeddings.shape[0]):\n            \n            idx = np.where(distances[k,] < perfect)[0]\n            ids = indices[k,idx]\n            posting_ids = df['posting_id'].iloc[ids].values\n            \n            # 过滤噪音\n#             if len(posting_ids) > 1 and strict_ths:\n            if len(posting_ids) > 1 and strict:\n                \n#                 strict=find_strict_th(cnt=len(posting_ids),ths=strict_ths)\n                idx_image_again = np.where(distances[k,] < strict)[0]\n                ids_image_again = indices[k, idx_image_again]\n                posting_ids_again = df['posting_id'].iloc[ids_image_again].values\n                if len(ids_image_again) > 1:\n                    predictions.append(posting_ids_again)\n                else:\n                    predictions.append(posting_ids)\n            else:\n                predictions.append(posting_ids)\n#             predictions.append(posting_ids)\n    \n    else:\n        predictions = []\n        for k in tqdm(range(embeddings.shape[0])):\n            idx = np.where(distances[k,] < perfect)[0]\n            ids = indices[k,idx]\n            posting_ids = df['posting_id'].iloc[ids].values\n            # 过滤噪音\n#             if len(posting_ids) > 1 and strict_ths:\n            if len(posting_ids) > 1 and strict:\n            \n#                 strict=find_strict_th(cnt=len(posting_ids),ths=strict_ths)\n                idx_image_again = np.where(distances[k,] < strict)[0]\n                ids_image_again = indices[k, idx_image_again]\n                posting_ids_again = df['posting_id'].iloc[ids_image_again].values\n                if len(ids_image_again) > 1:\n                    predictions.append(posting_ids_again)\n                else:\n                    predictions.append(posting_ids)\n            else:\n                predictions.append(posting_ids)\n#             predictions.append(posting_ids)\n        \n    del model, distances, indices\n    gc.collect()\n    return df, predictions\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ====================eca================\neca_thresholds_range = list(np.arange(20, 23, 0.5))\nperfect_eca_threshold=16\n\ndf, image_eca_predictions = get_neighbors(\n    df, image_eca_embeddings, \n    KNN = 100, image = True,\n    thresholds=eca_thresholds_range,\n    perfect=perfect_eca_threshold,\n    strict=15,\n    finetuneing=True\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\n# ====================swin================\nprint(\"# ===============swin=============\")\n\nswin_thresholds_range = list(np.arange(0.4, 1.0, 0.1))\nperfect_swin_threshold=0.4\n\ndf, image_swin_predictions = get_neighbors(\n    df, image_desnet512_embeddings, \n    KNN = 100, image = True,\n    thresholds=swin_thresholds_range,\n    perfect=perfect_swin_threshold,\n    strict=0.3,\n    finetuneing=True\n)\n\n# ===============efficientnet=============\nprint(\"# ===============efficientnet=============\")\n\nb3_thresholds_range = list(np.arange(4.0, 5.0, 0.1))\nperfect_b3_threshold=4.0\n\ndf, image_b3_predictions = get_neighbors(\n    df, image_b3_embeddings, \n    KNN = 100, image = True,\n    thresholds=b3_thresholds_range,\n    perfect=4.0,\n    strict=3.9,\n    finetuneing=True\n    \n#     strict_ths=[3.85,3.85,3.90,3.90,3.90]\n)\ndf, image_b4_predictions = get_neighbors(\n    df, image_b4_embeddings, \n    KNN = 50, image = True,\n    thresholds=b3_thresholds_range,\n    perfect=4.1,\n    strict=4.0,\n    finetuneing=True\n    \n#     strict_ths=[3.95,3.95,4.0,4.0,4.0]\n    \n)\n\n\n\n# ================desnet512================\nprint(\"# ===============desnet512=============\")\n\ndesnet_thresholds_range = list(np.arange(0.2, 0.7, 0.05))\nperfect_desnet_threshold=0.3\n\ndf, image_predictions_desnet = get_neighbors(\n    df, image_desnet512_embeddings, \n    KNN = 100, image = True,\n    thresholds=desnet_thresholds_range,\n    perfect=perfect_desnet_threshold,\n    strict=0.295,\n    finetuneing=True\n    \n#     strict_ths=[0.291,0.291,0.295,0.295,0.295]\n    \n)\n\n\n\n# ===============indonesian bert=============\nprint(\"# ===============indonesian bert=============\")\ntext_thresholds_range = list(np.arange(20, 28, 1))\nperfect_text_threshold=14.0\ndf, text_indonesian_predictions = get_neighbors(\n    df, text_indonesian_embeddings, \n    KNN = 100, image = False,\n    thresholds=text_thresholds_range,\n    perfect=perfect_text_threshold,\n    strict=13.5,\n    finetuneing=True\n    \n#     strict_ths=[13.2,13.2,13.5,13.5,13.5]\n    \n)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def cal_result_metrics(df):\n    print(df.shape)\n    # 计算召回率\n    ## 真实标签 matches,pred_matches\n\n    ## 完全正确\n    # sorted(['train_2278313361','train_129225211 '])\n    # ['train_129225211 ', 'train_2278313361']\n    full_right=0\n    full_recall_right=0\n\n    right_recall_1=0\n    right_recall_2=0\n\n    not_recall=0\n    error_cnt=0\n    for index,row in df.iterrows():\n        if sorted(row['matches'].split())==sorted(row['pred_matches'].split()):\n            full_right+=1\n\n        flag=True\n        for x in row['matches'].split():\n            if x not in row['pred_matches']:\n                not_recall+=1\n                flag=False\n            else:\n                right_recall_1+=1\n\n        if flag:\n            full_recall_right+=1\n\n\n        for x in row['pred_matches'].split():\n            if x not in row['matches']:\n                error_cnt+=1\n            else:\n                right_recall_2+=1\n    df['matches_count']=df['matches'].apply(lambda x:len(x.split()))\n    df['pred_count']=df['pred_matches'].apply(lambda x:len(x.split()))\n\n    print(\"真实样本匹配的个数：\",df['matches_count'].sum())\n    print(\"候选集预测的个数：\",df['pred_count'].sum())\n\n    full_right_ratio=full_right/df.shape[0]\n    print(\"候选集pred_matches完全与matches相等的概率率：\",full_right_ratio)\n    full_right_recall_ratio=full_recall_right/df.shape[0]\n    print(\"每一个matches中的都出现在pred_matches的概率：\",full_right_ratio)\n    print(\"候选集中没有出现在matches中的个数，相当于是预测错误的个数：\",error_cnt)\n    print(\"候选集中没有预测到matches样本的个数，相当于没有召回到的个数：\",not_recall)\n    print(\"候选集中预测正确的个数：\",right_recall_1)\n    print(\"候选集中预测正确的个数：\",right_recall_2)\n\n    print(\"真实召回率\",right_recall_2/df['matches_count'].sum())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function to combine predictions\ndef combine_predictions(row):\n    x = np.concatenate([\n        row['image_eca_predictions'][:48],\n        row['image_swin_predictions'][:48],\n        row['image_b3_predictions'][:48],\n        row['image_b4_predictions'][:48],\n        row['image_predictions_desnet'][:48],\n        row['text_indonesian_predictions'][:48],\n        row['oof_text'][:48],\n    ])\n    return ' '.join( np.unique(x) )\n\ndf['image_eca_predictions'] = image_eca_predictions\ndf['image_swin_predictions'] = image_swin_predictions\ndf['image_b3_predictions'] = image_b3_predictions\ndf['image_b4_predictions'] = image_b4_predictions\ndf['image_predictions_desnet'] = image_predictions_desnet\n\ndf['text_indonesian_predictions'] = text_indonesian_predictions\ndf['oof_text'] = df_cu['oof_text'].to_pandas().values\ndf['pred_matches'] = df.apply(combine_predictions, axis = 1)\n\nif GET_CV:\n    df['f1'] = f1_score(df['matches'], df['pred_matches'])\n    score = df['f1'].mean()\n    print(f'1111 Our final f1 cv score is {score}')\n    cal_result_metrics(df)\n    \n    df['pred_count']=df['pred_matches'].apply(lambda x:len(x.split()))\n    print(\"原始匹配结果数量分布\",df['pred_count'].describe())\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if GET_CV:\n    pari_counts=[\n        (1,10),\n        (10,20),\n        (20,30),\n        (30,40),\n        (40,50),\n        (50,100)\n    ]\n\n    for start,end in pari_counts:\n        print(\"==============>\",start,end,\"====================>\")\n        tmp=df.loc[(start<=df['pred_count'])&(df['pred_count']<end),:]\n        cal_result_metrics(tmp)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 获取tfidf向量\ndef get_tfidf_embeddings(df, max_features = 15500):\n    from sklearn.feature_extraction.text import TfidfVectorizer\n    model = TfidfVectorizer(max_features = max_features)\n    text_embeddings = model.fit_transform(df['title'])\n    print(f'Our title text embedding shape is {text_embeddings.shape}')\n    del model,df\n    gc.collect()\n    return text_embeddings\n\n\ntext_tfidf_embeddings = get_tfidf_embeddings(df, max_features = 21500)\nprint(\"text_tfidf_embeddings.shape\",text_tfidf_embeddings.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df, big_eca_pred = get_neighbors(df, image_eca_embeddings, KNN = 50, image = True,\n                                      thresholds=eca_thresholds_range,\n                                      perfect=21.5)\n\n\ndf, big_swin_pred = get_neighbors(df, image_swin_embeddings, KNN = 50, image = True,\n                                      thresholds=swin_thresholds_range,\n                                      perfect=0.6)\n\n\ndf, big_b3_pred = get_neighbors(df, image_b3_embeddings, KNN = 50, image = True,\n                                      thresholds=b3_thresholds_range,\n                                      perfect=4.8)\n\n\ndf, big_b4_pred = get_neighbors(df, image_b4_embeddings, KNN = 50, image = True,\n                                      thresholds=b3_thresholds_range,\n                                      perfect=4.9)\n\n\n\ndf, big_desnet_pred = get_neighbors(df, image_desnet512_embeddings, KNN = 100, image = True,\n                                             thresholds=desnet_thresholds_range,\n                                             perfect=0.6)\n\ndf, big_indonsian_pred = get_neighbors(df, text_indonesian_embeddings, KNN = 100, image = False,\n                                                thresholds=text_thresholds_range,\n                                                perfect=25)\ndf, big_tfidf_pred = get_neighbors(df, text_tfidf_embeddings, KNN = 100, image = True,\n                                             thresholds=desnet_thresholds_range,\n                                             perfect=0.8,is_csr=True)\n\ndel image_b3_embeddings,image_b4_embeddings,text_indonesian_embeddings\ndel text_tfidf_embeddings\n\n\ndf['big_eca_pred']=big_eca_pred\ndf['big_swin_pred']=big_swin_pred\ndf['big_b3_pred']=big_b3_pred\ndf['big_b4_pred']=big_b4_pred\ndf['big_desnet_pred']=big_desnet_pred\ntmp = df.groupby('image_phash').posting_id.agg('unique').to_dict()\ndf['oof_hash'] = df.image_phash.map(tmp)\n\ndf['big_indonsian_pred']=big_indonsian_pred\ndf['big_tfidf_pred']=big_tfidf_pred\n\ndel big_eca_pred,big_swin_pred\ndel big_b3_pred,big_b4_pred,big_indonsian_pred\ndel big_desnet_pred,tmp,big_tfidf_pred\n\ngc.collect()\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def combine_predictions_v2(row,topn):\n#     #     print(row['text_predictions_indonesian'])\n#     x = np.concatenate([\n#         row['big_b3_pred'][:topn],\n#         row['big_b4_pred'][:topn],\n#         row['big_indonsian_pred'][:topn],\n#         row['big_desnet_pred'][:topn],\n#         row['oof_hash'][:topn],\n#         row['big_tfidf_pred'][:topn],\n#     ])\n#     return ' '.join(np.unique(x))\n\n# df['big_pred_matches'] = df.apply(lambda row:combine_predictions_v2(row,2), axis=1)\n# df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 投票召回频次最大的两个\nfrom collections import Counter\ndef vote_recall_predictions(row):\n    x = np.concatenate([\n        row['big_eca_pred'][:3],\n        row['big_swin_pred'][:3],\n        row['big_b3_pred'][:3],\n        row['big_b4_pred'][:3],\n        row['big_desnet_pred'][:3],\n    ])\n    \n    collection_matches = Counter(x)\n    # 存在两个以上召回两个频次最大的，否则返回自身\n    try:\n        new_x = collection_matches.most_common(2)[0][0] + ' ' + collection_matches.most_common(2)[1][0]\n    except:\n        new_x = collection_matches.most_common(1)[0][0]\n\n    new_x = new_x.split()\n    new_x = np.concatenate([row['oof_hash'][:3], row['big_tfidf_pred'][:3], new_x ])\n    \n    # bert 放在最后做召回\n    if len( np.unique(new_x) ) < 2:\n        new_x = np.concatenate([row['big_indonsian_pred'][:3], row['big_tfidf_pred'][:3]])\n    \n    return ' '.join( np.unique(new_x) )\n\ndf['big_pred_matches'] = df.apply(vote_recall_predictions, axis=1)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Concatenate image predctions with text predictions\nif GET_CV:\n    # 赋值重新召回的值 重新召回结果为1的结果\n    only_one_df=df[df['pred_count']==1]\n    print(only_one_df.shape)\n    df['big_pred_count']=df['big_pred_matches'].apply(lambda x:len(x.split()))\n    stats=df[df['posting_id'].isin(only_one_df['posting_id'])]['big_pred_count'].describe()\n    print(stats)\n    \n    df.loc[only_one_df.index,'pred_matches']=df.loc[only_one_df.index,'big_pred_matches']\n    df['f1'] = f1_score(df['matches'], df['pred_matches'])\n    score = df['f1'].mean()\n    print(f'2222 Our final f1 cv score is {score}')\n    cal_result_metrics(df)\n    df['matches'] = df['pred_matches']\n    df[['posting_id', 'matches']].to_csv('submission.csv', index = False)\n    df['pred_count']=df['pred_matches'].apply(lambda x:len(x.split()))\n    only_one_df=df[df['pred_count']==1]\n    print(\"合并大阈值结果之后的1样本个数\",only_one_df.shape)\nelse:\n    # 赋值重新召回的值\n    df['matches'] = df['pred_matches']\n\n    df['pred_count']=df['matches'].apply(lambda x:len(x.split()))\n    only_one_df=df[df['pred_count']==1]\n    df.loc[only_one_df.index,'matches']=df.loc[only_one_df.index,'big_pred_matches']\n    \n#     only_two_df=df[df['pred_count']==2]\n#     df.loc[only_two_df.index,'matches']=df.loc[only_two_df.index,'big_pred_matches3']\n    \n    \n    df[['posting_id', 'matches']].to_csv('submission.csv', index = False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!rm ./tokenization.py\n!rm ./__pycache__/tokenization.cpython-37.pyc\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[['posting_id', 'matches']].head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}