{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Summary\nThis is the inference notebook for https://www.kaggle.com/underwearfitting/pytorch-densenet-arcface-validation-training. In this notebook, I submitted a single fold trained Arcface Densenet121 with a CV 0.731. I computed the cosine similarities between the feature vectors. To make it faster, I put this process on GPU and compute by batches to avoid OOM issue. Don't hesitate if you have any questions; answering your questions can help me learn as well. ","metadata":{}},{"cell_type":"markdown","source":"## Configuration","metadata":{}},{"cell_type":"code","source":"image_size = 640\nbatch_size = 32\nnum_workers = 4\nn_batch = 10 # to avoid oom, split 70000+ images into 10 batches\nsim_thresh = 0.8\ntext_sim_thresh = 0.9\nCHANGE_P = False\ntext_filter_threshold = 0.1","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Imports","metadata":{}},{"cell_type":"code","source":"!pip install ../input/pytorchtabnet/pytorch_tabnet-3.1.0-py3-none-any.whl","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport sys\nsys.path.append('../input/timm-pytorch-image-models/pytorch-image-models-master')\nimport os\nimport sys\nimport time\nimport cv2\nimport PIL.Image\nimport random\nfrom sklearn.metrics import accuracy_score\nfrom tqdm.notebook import tqdm\nimport torch\nfrom torch.utils.data import DataLoader, Dataset\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision\nimport torchvision.transforms as transforms\nimport torch.optim as optim\nfrom torch.optim.lr_scheduler import CosineAnnealingLR\nimport albumentations\nfrom tqdm.notebook import tqdm\nimport matplotlib.pyplot as plt\nimport gc\nfrom sklearn.metrics import roc_auc_score\n%matplotlib inline\nimport seaborn as sns\nfrom pylab import rcParams\nimport timm\nfrom warnings import filterwarnings\nfrom sklearn.preprocessing import LabelEncoder\nimport math\nimport glob\nfilterwarnings(\"ignore\")\n\ndevice = torch.device('cuda') \nimport pickle\nimport cudf\nimport cuml\nimport cupy\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom cuml.neighbors import NearestNeighbors\nfrom pytorch_tabnet.tab_model import TabNetClassifier","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    print(f'Setting all seeds to be {seed} to reproduce...')\nseed_everything(42)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Transforms","metadata":{}},{"cell_type":"code","source":"transforms_valid = albumentations.Compose([\n    albumentations.Resize(image_size, image_size),\n    albumentations.Normalize()\n])\n\ntransforms_valid_768 = albumentations.Compose([\n    albumentations.Resize(768, 768),\n    albumentations.Normalize()\n])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Dataset","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer\n\nclass SHOPEEDataset(Dataset):\n    def __init__(self, df, mode, tokenizer_path=\"../input/distilbert-base-indonesian\", transform=None, use_image=True):\n        \n        self.df = df.reset_index(drop=True)\n        self.mode = mode\n        self.transform = transform\n        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n        self.use_image = use_image\n#         self.tokenizer = AutoTokenizer.from_pretrained(\"../input/bert-base-uncased\")\n        \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, index):\n        row = self.df.loc[index]\n        if self.use_image:\n            img = cv2.imread(row.file_path)\n            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n            img_512 = img.copy()\n            img_768 = img.copy()\n        \n            if self.transform is not None:\n                res = self.transform(image=img_512)\n                img_512 = res['image']\n                \n            img_512 = img_512.astype(np.float32)\n            img_512 = img_512.transpose(2,0,1)\n            \n            res = transforms_valid_768(image=img_768)\n            img_768 = res['image']\n            img_768 = img_768.astype(np.float32)\n            img_768 = img_768.transpose(2,0,1)\n        else:\n            img_512 = [0, 1, 2] # it's dummy\n            img_768 = [0, 1, 2]\n         \n        text = row.title\n        text = self.tokenizer(text, padding='max_length', truncation=True, max_length=35, return_tensors=\"pt\")\n        input_ids = text['input_ids'][0]\n        attention_mask = text['attention_mask'][0]\n#         token_type_ids = text[\"token_type_ids\"][0]\n        \n        if \"token_type_ids\" in text:\n            token_type_ids = text[\"token_type_ids\"][0]\n#             return torch.tensor(img).float(),text['input_ids'][0], text['attention_mask'][0], text[\"token_type_ids\"][0]\n            return torch.tensor(img_512).float(),torch.tensor(img_768).float() ,text['input_ids'][0], text['attention_mask'][0],token_type_ids\n        else:\n            return torch.tensor(img_512).float(),torch.tensor(img_768).float(),text['input_ids'][0], text['attention_mask'][0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class SHOPEETextDataset(Dataset):\n    def __init__(self, df, data_dir, vectorizer=None):\n        le = LabelEncoder()\n        self.df = df\n        self.df['file_path'] = self.df.image.apply(lambda x: os.path.join(data_dir, x))\n        self.data_dir = data_dir\n\n        titles = self.df[\"title\"].tolist()\n        if vectorizer is None:\n            self.vectorizer = CountVectorizer(analyzer=\"char\", ngram_range=(1, 2))\n            self.array = self.vectorizer.fit_transform(titles).toarray()\n        else:\n            self.vectorizer = vectorizer\n#             self.titles = titles\n            self.array = self.vectorizer.transform(titles)\n        print(self.array.shape)\n\n    def get_vectorizer(self):\n        return self.vectorizer, len(self.array[0])\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, index):\n        row = self.df.iloc[index]\n#         print (self.array[index])\n        arr = self.array[index].toarray()[0]\n#         arr = self.array[index]\n        return torch.tensor(arr).float()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model","metadata":{}},{"cell_type":"code","source":"from torch.nn.parameter import Parameter\n\nclass ArcMarginProduct(nn.Module):\n    def __init__(self, in_features, out_features, s=30, m=0.5):\n        super().__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.s = s\n        self.m = m\n        self.weight = nn.Parameter(torch.FloatTensor(out_features, in_features))\n        nn.init.xavier_normal_(self.weight)\n\n        self.cos_m = math.cos(m)\n        self.sin_m = math.sin(m)\n        self.th = torch.tensor(math.cos(math.pi - m))\n        self.mm = torch.tensor(math.sin(math.pi - m) * m)\n\n    def forward(self, inputs, labels):\n        cos_th = F.linear(F.normalize(inputs.float()), F.normalize(self.weight.float()))\n        cos_th = cos_th.clamp(-1, 1).float()\n        sin_th = torch.sqrt(1.0 - torch.pow(cos_th, 2)).float()\n        cos_th_m = cos_th * self.cos_m - sin_th * self.sin_m\n        # print(type(cos_th), type(self.th), type(cos_th_m), type(self.mm))\n        cos_th_m = torch.where(cos_th > self.th, cos_th_m, cos_th - self.mm)\n\n        cond_v = cos_th - self.th\n        cond = cond_v <= 0\n        cos_th_m[cond] = (cos_th - self.mm)[cond]\n\n        if labels.dim() == 1:\n            labels = labels.unsqueeze(-1)\n        onehot = torch.zeros(cos_th.size()).cuda()\n        labels = labels.type(torch.LongTensor).cuda()\n        onehot.scatter_(1, labels, 1.0)\n        outputs = onehot * cos_th_m + (1.0 - onehot) * cos_th\n        outputs = outputs * self.s\n        return outputs\n\n\ndef gem(x, p=3, eps=1e-6):\n    return F.avg_pool2d(x.clamp(min=eps).pow(p), (x.size(-2), x.size(-1))).pow(1. / p)\n\n\nclass GeM(nn.Module):\n    def __init__(self, p=3, eps=1e-6, p_trainable=True):\n        super(GeM, self).__init__()\n        if p_trainable:\n            self.p = Parameter(torch.ones(1) * p)\n        else:\n            self.p = p\n        self.eps = eps\n\n    def forward(self, x):\n        return gem(x, p=self.p, eps=self.eps)\n\n    def __repr__(self):\n        return self.__class__.__name__ + '(' + 'p=' + '{:.4f}'.format(self.p.data.tolist()[0]) + ', ' + 'eps=' + str(\n            self.eps) + ')'\n\n\nclass Backbone(nn.Module):\n\n    def __init__(self, name='resnet18', pretrained=True):\n        super(Backbone, self).__init__()\n        self.net = timm.create_model(name, pretrained=pretrained)\n\n        if 'regnet' in name:\n            self.out_features = self.net.head.fc.in_features\n        elif 'csp' in name:\n            self.out_features = self.net.head.fc.in_features\n        elif 'res' in name:  # works also for resnest\n            self.out_features = self.net.fc.in_features\n        elif 'efficientnet' in name:\n            self.out_features = self.net.classifier.in_features\n        elif 'densenet' in name:\n            self.out_features = self.net.classifier.in_features\n        elif 'senet' in name:\n            self.out_features = self.net.fc.in_features\n        elif 'inception' in name:\n            self.out_features = self.net.last_linear.in_features\n\n        else:\n            self.out_features = self.net.classifier.in_features\n\n    def forward(self, x):\n        x = self.net.forward_features(x)\n\n        return x\n    \n    \n\nsigmoid = torch.nn.Sigmoid()\nclass Swish(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, i):\n        result = i * sigmoid(i)\n        ctx.save_for_backward(i)\n        return result\n    @staticmethod\n    def backward(ctx, grad_output):\n        i = ctx.saved_variables[0]\n        sigmoid_i = sigmoid(i)\n        return grad_output * (sigmoid_i * (1 + i * (1 - sigmoid_i)))\n\nclass Swish_module(nn.Module):\n    def forward(self, x):\n        return Swish.apply(x)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoModel\nfrom torch.nn.parameter import Parameter\n\nclass TransformerModel(nn.Module):\n    def __init__(self, transformer_type, pooling=\"avg\", p=3., p_trainable=True):\n        super(TransformerModel, self).__init__()\n        self.bert = AutoModel.from_pretrained(transformer_type)\n        self.pooling = pooling\n        self.eps = 1e-6\n        if p_trainable:\n            self.p = Parameter(torch.ones(1) * p)\n        else:\n            self.p = p\n\n    def forward(self, input_ids, attention_mask, token_type_ids):\n        if token_type_ids is None:\n            out = self.bert(\n                input_ids,\n                attention_mask=attention_mask,\n                # token_type_ids = token_type_ids\n            )[0]\n        else:\n            out = self.bert(\n                input_ids,\n                attention_mask=attention_mask,\n                token_type_ids = token_type_ids\n            )[0]\n        att_mask = input_ids > 0\n        # x_bert = super().forward(ids, att_mask, token_type_ids=seg_ids)[0]\n        att_mask = att_mask.unsqueeze(-1)\n        if self.pooling == \"avg\":\n            return (out * att_mask).sum(dim=1) / att_mask.sum(dim=1)\n        else:\n            return((out * att_mask).sum(dim=1).clamp(self.eps).pow(self.p) / att_mask.sum(dim=1)).pow(1. / self.p)\n        # return out\n\nclass Net(nn.Module):\n    def __init__(self, args, pretrained=True):\n        super(Net, self).__init__()\n\n        self.args = args\n        self.image_features(args, pretrained=pretrained)\n        self.embedding_size = args[\"embedding_size\"]\n\n        # https://www.groundai.com/project/arcface-additive-angular-margin-loss-for-deep-face-recognition\n        if args[\"neck\"] == \"option-D\":\n            self.neck = nn.Sequential(\n                nn.Linear(self.backbone.out_features, self.embedding_size, bias=True),\n                nn.BatchNorm1d(self.embedding_size),\n                torch.nn.PReLU()\n            )\n        elif args[\"neck\"] == \"option-F\":\n            self.neck = nn.Sequential(\n                nn.Dropout(0.3),\n                nn.Linear(self.backbone.out_features, self.embedding_size, bias=True),\n                nn.BatchNorm1d(self.embedding_size),\n                torch.nn.PReLU()\n            )\n        else:\n            self.neck = nn.Sequential(\n                nn.Linear(self.backbone.out_features, self.embedding_size, bias=False),\n                nn.BatchNorm1d(self.embedding_size),\n            )\n\n        if args[\"neck\"] == \"option-D\":\n            self.neckv2 = nn.Sequential(\n                nn.Linear(self.backbone.out_features, self.embedding_size, bias=True),\n                nn.BatchNorm1d(self.embedding_size),\n                torch.nn.PReLU()\n            )\n        elif args[\"neck\"] == \"option-F\":\n            self.neckv2 = nn.Sequential(\n                nn.Dropout(0.3),\n                nn.Linear(self.backbone.out_features, self.embedding_size, bias=True),\n                nn.BatchNorm1d(self.embedding_size),\n                torch.nn.PReLU()\n            )\n        else:\n            self.neckv2 = nn.Sequential(\n                nn.Linear(self.backbone.out_features, self.embedding_size, bias=False),\n                nn.BatchNorm1d(self.embedding_size),\n            )\n        # self.neckv2 = nn.Linear(768 + self.backbone.out_features, self.embedding_size)\n        self.swish = Swish_module()\n        self.dropout = nn.Dropout(0.5)\n\n        self.head = ArcMarginProduct(self.embedding_size, args[\"n_classes\"], s=args[\"s\"], m=args[\"m\"])\n        # self.head = ArcMarginProduct_subcenter(self.embedding_size, args[\"n_classes\"])\n\n        if args[\"pretrained_weights\"] is not None:\n            self.load_state_dict(torch.load(args.pretrained_weights, map_location='cpu'), strict=False)\n            print('weights loaded from', args.pretrained_weights)\n\n    def image_features(self, args, pretrained):\n        self.args = args\n        self.backbone = Backbone(args[\"backbone\"], pretrained=pretrained)\n\n        if args[\"pool\"] == \"gem\":\n            self.global_pool = GeM(p_trainable=3)\n        elif args[\"pool\"] == \"identity\":\n            self.global_pool = torch.nn.Identity()\n        else:\n            self.global_pool = nn.AdaptiveAvgPool2d(1)\n            \n    def change_p(self):\n        self.global_pool.p = Parameter(self.global_pool.p.data + torch.tensor(1.0))\n\n    def forward(self, images, labels,input_ids, attention_mask, token_type_ids, get_embeddings=False, get_attentions=False):\n        x = self.backbone(images)\n        x = self.global_pool(x)\n        x = x[:, :, 0, 0]\n        x = self.neckv2(x)\n        return F.normalize(x)\n        \nclass NetNLP(nn.Module):\n    def __init__(self, args, pretrained=True):\n        super(NetNLP, self).__init__()\n\n        self.args = args\n        self.text_features(args, pretrained)\n        self.embedding_size = args[\"embedding_size\"]\n\n        self.neckv2 = nn.Sequential(\n                nn.Dropout(0.1),\n                nn.Linear(768, self.embedding_size, bias=False),\n                nn.BatchNorm1d(self.embedding_size),\n            )\n\n#         self.head = ArcMarginProduct(self.embedding_size, args[\"n_classes\"], s=args[\"s\"], m=args[\"m\"])\n        # self.head = ArcMarginProduct_subcenter(self.embedding_size, args[\"n_classes\"])\n\n        if args[\"pretrained_weights\"] is not None:\n            self.load_state_dict(torch.load(args.pretrained_weights, map_location='cpu'), strict=False)\n            print('weights loaded from', args.pretrained_weights)\n\n        # for param in self.bert.parameters():\n        #     param.requires_grad = False\n\n    def image_features(self, args, pretrained):\n        self.args = args\n        self.backbone = Backbone(args[\"backbone\"], pretrained=pretrained)\n\n        if args[\"pool\"] == \"gem\":\n            self.global_pool = GeM(p_trainable=True)\n        elif args[\"pool\"] == \"identity\":\n            self.global_pool = torch.nn.Identity()\n        else:\n            self.global_pool = nn.AdaptiveAvgPool2d(1)\n\n\n    def text_features(self, args, pretrained):\n        self.bert = TransformerModel(args[\"transformer_type\"], pooling=\"avg\")\n        \n\n    def forward(self, images, labels,input_ids, attention_mask, token_type_ids, get_embeddings=False, get_attentions=False):\n        out = self.bert(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n        # out = self.neckv2(out)\n        # x = torch.cat([x, out], 1)\n        # print (x.size())\n        x = self.neckv2(out)\n        # logits = self.head(x)\n#         logits = self.head(x, labels)\n\n        # print (logits)\n\n        return F.normalize(x)\n        \n        \nclass NetV2(nn.Module):\n    def __init__(self, args, pretrained=True):\n        super(NetV2, self).__init__()\n\n        self.args = args\n        self.image_features(args, pretrained=pretrained)\n        self.text_features(args, pretrained)\n\n        self.embedding_size = args[\"embedding_size\"]\n\n        if args[\"neck\"] == \"option-D\":\n            self.neckv2 = nn.Sequential(\n                nn.Linear(768 + self.backbone.out_features, self.embedding_size, bias=True),\n                nn.BatchNorm1d(self.embedding_size),\n                torch.nn.PReLU()\n            )\n        elif args[\"neck\"] == \"option-F\":\n            self.neckv2 = nn.Sequential(\n                nn.Dropout(0.3),\n                nn.Linear(768 + self.backbone.out_features, self.embedding_size, bias=True),\n                nn.BatchNorm1d(self.embedding_size),\n                torch.nn.PReLU()\n            )\n        else:\n            self.neckv2 = nn.Sequential(\n                nn.Linear(768 + self.backbone.out_features, self.embedding_size, bias=False),\n                nn.BatchNorm1d(self.embedding_size),\n                Swish_module()\n            )\n        # self.neckv2 = nn.Linear(768 + self.backbone.out_features, self.embedding_size)\n        #\n        # self.dropout = nn.Dropout(0.5)\n#         if args[\"metric_type\"] == \"arc_margin\":\n#             self.head = ArcMarginProduct(self.embedding_size, args[\"n_classes\"], s=args[\"s\"], m=args[\"m\"])\n#         else:\n#             self.head = ArcMarginProduct_subcenter(self.embedding_size, args[\"n_classes\"])\n\n\n        if args[\"pretrained_weights\"] is not None:\n            self.load_state_dict(torch.load(args.pretrained_weights, map_location='cpu'), strict=False)\n            print('weights loaded from', args.pretrained_weights)\n\n    def image_features(self, args, pretrained):\n        self.args = args\n        self.backbone = Backbone(args[\"backbone\"], pretrained=pretrained)\n\n        if args[\"pool\"] == \"gem\":\n            self.global_pool = GeM(p_trainable=False)\n        elif args[\"pool\"] == \"identity\":\n            self.global_pool = torch.nn.Identity()\n        else:\n            self.global_pool = nn.AdaptiveAvgPool2d(1)\n\n\n    def text_features(self, args, pretrained):\n        self.bert = TransformerModel(args[\"transformer_type\"], pooling=\"avg\")\n\n    def forward(self, images, labels,input_ids, attention_mask, token_type_ids, get_embeddings=False, get_attentions=False):\n        x = self.backbone(images)\n        x = self.global_pool(x)\n        x = x[:, :, 0, 0]\n        out = self.bert(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n        # out = self.neckv2(out)\n        x = torch.cat([x, out], 1)\n        # print (x.size())\n        x = self.neckv2(x)\n        # print (logits)\n\n        return F.normalize(x)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Read in test data","metadata":{}},{"cell_type":"code","source":"def fix_encoding(x):\n    return normalize(\"NFD\", codecs.escape_decode(x, 'hex')[0].decode(\"utf-8\"))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_dir = '../input/shopee-product-matching/train_images'\ndf_train = pd.read_csv('../input/shopee-product-matching/train.csv')\ndf_train['file_path'] = df_train.image.apply(lambda x: os.path.join(data_dir, x))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# torch.load('../input/shopeae-models/efficientnetb3_baseline.pth', map_location='cuda:0')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model2 = Net(args={\n            \"backbone\": \"resnet200d\",\n            \"pool\": \"gem\",\n            \"s\": 30,\n            \"m\": 0.5,\n            \"neck\": \"\",\n            \"embedding_size\": 512,\n            \"n_classes\": 11014,\n            \"pretrained_weights\": None,\n            \"transformer_type\": \"bert-base-uncased\"\n        }, pretrained=False)\nmodel2.load_state_dict(torch.load('../input/shopee-models/resnet200d_images_baseline_optionH_cutout_bh_gnoise_all_img640.pth', map_location='cuda:0')[\"state_dict\"], strict=True)\nmodel2.to(device);\nif CHANGE_P:\n    print (model2.global_pool.p.data)\n    model2.change_p()\n    print (model2.global_pool.p.data)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = Net(args={\n            \"backbone\": \"tf_efficientnet_b3_ns\",\n            \"pool\": \"gem\",\n            \"s\": 30,\n            \"m\": 0.5,\n            \"neck\": \"\",\n            \"embedding_size\": 512,\n            \"n_classes\": 11014,\n            \"pretrained_weights\": None,\n            \"transformer_type\": \"bert-base-uncased\"\n        }, pretrained=False)\nmodel.load_state_dict(torch.load('../input/shopee-models/efficientnetb3_ns_images_baseline_optionH_cutout_bh_v2_768_all.pth', map_location='cuda:0')[\"state_dict\"], strict=False)\nmodel.to(device);\nif CHANGE_P:\n    print (model.global_pool.p.data)\n    model.change_p()\n    print (model.global_pool.p.data)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model3 = Net(args={\n            \"backbone\": \"tf_efficientnet_b5_ns\",\n            \"pool\": \"gem\",\n            \"s\": 30,\n            \"m\": 0.5,\n            \"neck\": \"\",\n            \"embedding_size\": 512,\n            \"n_classes\": 11014,\n            \"pretrained_weights\": None,\n            \"transformer_type\": \"bert-base-uncased\"\n        }, pretrained=False)\nmodel3.load_state_dict(torch.load('../input/shopee-models/efficientnetb5_ns_images_baseline_optionH_cutout_bh_gnoise_all_size640.pth', map_location='cuda:0')[\"state_dict\"], strict=True)\nmodel3.to(device);\nif CHANGE_P:\n    print (model3.global_pool.p.data)\n    model3.change_p()\n    print (model3.global_pool.p.data)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nlp_model = NetNLP(args={\n            \"backbone\": \"tf_efficientnet_b3\",\n            \"pool\": \"gem\",\n            \"s\": 30,\n            \"m\": 0.5,\n            \"neck\": \"option-D\",\n            \"embedding_size\": 512,\n            \"n_classes\": 11014,\n            \"pretrained_weights\": None,\n            \"transformer_type\": \"../input/distilbert-base-indonesian/\",\n#     \"pretrained_weights\": \n})\nnlp_model.load_state_dict(torch.load('../input/shopee-bert/distilbert_baseline_optionH_count_bs255_all.pth', map_location='cuda:0')[\"state_dict\"], strict=False)\nnlp_model.to(device);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# nlp_bert_model = NetNLP(args={\n#             \"backbone\": \"tf_efficientnet_b3\",\n#             \"pool\": \"gem\",\n#             \"s\": 30,\n#             \"m\": 0.5,\n#             \"neck\": \"option-D\",\n#             \"embedding_size\": 512,\n#             \"n_classes\": 11014,\n#             \"pretrained_weights\": None,\n#             \"transformer_type\": \"../input/bertindo15g/\",\n# #     \"pretrained_weights\": \n# })\n# nlp_bert_model.load_state_dict(torch.load('../input/shopee-bert/bert_indonesian15G_baseline_optionH_count_bs255_all.pth', map_location='cuda:0')[\"state_dict\"], strict=False)\n# nlp_bert_model.to(device);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class NLPMLP(nn.Module):\n    def __init__(self, n_feat, n_class):\n        super(NLPMLP, self).__init__()\n        self.feature_extract = nn.Sequential(\n            nn.Linear(n_feat, 1024),\n            nn.BatchNorm1d(1024),\n            nn.ReLU(),\n            nn.Linear(1024, 512),\n            nn.BatchNorm1d(512),\n        )\n\n#         self.neckv2 = ArcMarginProduct(512, n_class, s=32, m=0.5)\n\n    def forward(self, feat1):\n        x = self.feature_extract(feat1)\n#         logits = self.neckv2(x, label)\n\n        return F.normalize(x)\n\nclass NLPMLPV3(nn.Module):\n    def __init__(self, n_feat, n_class):\n        super(NLPMLPV3, self).__init__()\n        self.feature_extract = nn.Sequential(\n            nn.Linear(n_feat, 1024),\n            nn.BatchNorm1d(1024),\n            nn.PReLU(),\n            nn.Linear(1024, 512),\n            nn.BatchNorm1d(512),\n        )\n\n#         self.neckv2 = ArcMarginProduct(512, n_class, s=32, m=0.5)\n\n    def forward(self, feat1):\n        x = self.feature_extract(feat1)\n#         logits = self.neckv2(x, label)\n\n        return F.normalize(x)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nlp_model2 = NLPMLP(2716, 11014)\nvectorizer = pickle.load(open(\"../input/shopee-mlp/count_vector.pkl\", \"rb\"))\nnlp_model2.load_state_dict(torch.load('../input/shopee-mlp/epoch_0099.pth', map_location='cuda:0')[\"state_dict\"], strict=False)\nnlp_model2.to(device);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nlp_model3 = NLPMLPV3(30210, 11014)\nvectorizer2 = pickle.load(open(\"../input/shopee-mlp/count_vector_1_3_v2.pkl\", \"rb\"))\nnlp_model3.load_state_dict(torch.load('../input/shopee-mlp/mlpv3_images_cutout_bh_ngram13_v2.pth', map_location='cuda:0')[\"state_dict\"], strict=False)\nnlp_model3.to(device);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import codecs\nfrom unicodedata import normalize\n\ndef fix_encoding(x):\n    return normalize(\"NFD\", codecs.escape_decode(x, 'hex')[0].decode(\"utf-8\"))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test = pd.read_csv('../input/shopee-product-matching/test.csv')\n# test = pd.concat([test]*25000)\ntest['file_path'] = test.image.apply(lambda x: os.path.join('../input/shopee-product-matching/test_images',x))\ntest[\"title\"] = test[\"title\"].apply(fix_encoding)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset_test = SHOPEEDataset(test, 'test', transform=transforms_valid)\ntest_loader = torch.utils.data.DataLoader(dataset_test, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"distilbert_dataset_test = SHOPEEDataset(test, 'test', transform=transforms_valid,tokenizer_path=\"../input/distilbert-base-indonesian\",use_image=False)\ndistilbert_test_loader = torch.utils.data.DataLoader(distilbert_dataset_test, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# bert_dataset_test = SHOPEEDataset(test, 'test', transform=transforms_valid,tokenizer_path=\"../input/bertindo15g\",use_image=False)\n# bert_test_loader = torch.utils.data.DataLoader(bert_dataset_test, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test['file_path'] = test.image.apply(lambda x: os.path.join('../input/shopee-product-matching/test_images',x))\ndataset_text_test = SHOPEETextDataset(test, 'test', vectorizer)\ntest_text_loader = torch.utils.data.DataLoader(dataset_text_test, batch_size=128, shuffle=False, num_workers=num_workers, pin_memory=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test['file_path'] = test.image.apply(lambda x: os.path.join('../input/shopee-product-matching/test_images',x))\ndataset_text_test2 = SHOPEETextDataset(test, 'test', vectorizer2)\ntest_text_loader2 = torch.utils.data.DataLoader(dataset_text_test2, batch_size=32, shuffle=False, num_workers=num_workers, pin_memory=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Generate Features","metadata":{}},{"cell_type":"code","source":"def generate_test_features(test_loader):\n    model.eval()\n    model2.eval()\n    model3.eval()\n    bar = tqdm(test_loader)\n    \n    FEAS = []\n    TARGETS = []\n\n    with torch.no_grad():\n        for batch_idx, (images512, images768,input_ids, attention_mask) in enumerate(bar):\n\n            images768 = images768.to(device)\n            images512 = images512.to(device)\n            input_ids = input_ids.to(device)\n            attention_mask = attention_mask.to(device)\n#             token_type_ids = token_type_ids.to(device)\n\n            features = model(images768, None, input_ids, attention_mask, None, get_embeddings=True)\n            features2 = model2(images512, None, input_ids, attention_mask, None, get_embeddings=True)\n            features3 = model3(images512, None, input_ids, attention_mask, None, get_embeddings=True)\n            \n            concat_feat = torch.cat([features, features2, features3], axis=1)\n            \n            FEAS += [concat_feat.detach().cpu()]\n\n    FEAS = torch.cat(FEAS).cpu().numpy()\n    \n    return FEAS","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"FEAS = generate_test_features(test_loader)\nFEAS.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"n, _ = FEAS.shape\nbs = n // 10 ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del model\ngc.collect()\ntorch.cuda.empty_cache()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def generate_test_features2(test_loader):\n    model2.eval()\n    bar = tqdm(test_loader)\n    \n    FEAS_MODEL2 = []\n    TARGETS = []\n\n    with torch.no_grad():\n        for batch_idx, (images, input_ids, attention_mask) in enumerate(bar):\n            images = images.to(device)\n            input_ids = input_ids.to(device)\n            attention_mask = attention_mask.to(device)\n#             token_type_ids = token_type_ids.to(device)\n\n            features = model2(images, None, input_ids, attention_mask, None, get_embeddings=True)\n\n            FEAS_MODEL2 += [features.detach().cpu()]\n\n    FEAS_MODEL2 = torch.cat(FEAS_MODEL2).cpu().numpy()\n    \n    return FEAS_MODEL2","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# FEAS_MODEL2 = generate_test_features2(test_loader)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del model2\ngc.collect()\ntorch.cuda.empty_cache()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def generate_test_features3(test_loader):\n    model3.eval()\n    bar = tqdm(test_loader)\n    \n    FEAS_MODEL3 = []\n    TARGETS = []\n\n    with torch.no_grad():\n        for batch_idx, (images, input_ids, attention_mask, _) in enumerate(bar):\n\n            images = images.to(device)\n            input_ids = input_ids.to(device)\n            attention_mask = attention_mask.to(device)\n#             token_type_ids = token_type_ids.to(device)\n\n            features = model3(images, None, input_ids, attention_mask, None, get_embeddings=True)\n\n            FEAS_MODEL3 += [features.detach().cpu()]\n\n    FEAS_MODEL3 = torch.cat(FEAS_MODEL3).cpu().numpy()\n    \n    return FEAS_MODEL3","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# FEAS_MODEL3  = generate_test_features3(test_loader)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del model3\ngc.collect()\ntorch.cuda.empty_cache()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def generate_nlp_test_features(test_loader):\n    nlp_model2.eval()\n    bar = tqdm(test_loader)\n    \n    FEAS = []\n    TARGETS = []\n\n    with torch.no_grad():\n        for batch_idx, text_feat in enumerate(bar):\n\n            text_feat = text_feat.to(device)\n#             input_ids = input_ids.to(device)\n#             attention_mask = attention_mask.to(device)\n#             token_type_ids = token_type_ids.to(device)\n\n            features = nlp_model2(text_feat)\n\n            FEAS += [features.detach().cpu()]\n\n    FEAS = torch.cat(FEAS).cpu().numpy()\n    \n    return FEAS","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"TEXT_FEAS = generate_nlp_test_features(test_text_loader)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def generate_nlp_test_features3(test_loader):\n    nlp_model3.eval()\n    bar = tqdm(test_loader)\n    \n    FEAS = []\n    TARGETS = []\n\n    with torch.no_grad():\n        for batch_idx, text_feat in enumerate(bar):\n\n            text_feat = text_feat.to(device)\n#             input_ids = input_ids.to(device)\n#             attention_mask = attention_mask.to(device)\n#             token_type_ids = token_type_ids.to(device)\n\n            features = nlp_model3(text_feat)\n\n            FEAS += [features.detach().cpu()]\n\n    FEAS = torch.cat(FEAS).cpu().numpy()\n    \n    return FEAS","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"TEXT_FEAS4 = generate_nlp_test_features3(test_text_loader2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def generate_nlp_test_features(test_loader):\n    nlp_model.eval()\n    bar = tqdm(test_loader)\n    \n    TEXT_FEAS_2 = []\n    TARGETS = []\n\n    with torch.no_grad():\n        for batch_idx, (images, images2,input_ids, attention_mask) in enumerate(bar):\n\n            images = images.to(device)\n            input_ids = input_ids.to(device)\n            attention_mask = attention_mask.to(device)\n#             token_type_ids = token_type_ids.to(device)\n\n            features = nlp_model(images, None, input_ids, attention_mask, None, get_embeddings=True)\n\n            TEXT_FEAS_2 += [features.detach().cpu()]\n\n    TEXT_FEAS_2 = torch.cat(TEXT_FEAS_2).cpu().numpy()\n    \n    return TEXT_FEAS_2","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"TEXT_FEAS_2 = generate_nlp_test_features(distilbert_test_loader)\nTEXT_FEAS_2.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def generate_nlp_test_features_v2(test_loader):\n#     nlp_bert_model.eval()\n#     bar = tqdm(test_loader)\n    \n#     TEXT_FEAS_3 = []\n#     TARGETS = []\n\n#     with torch.no_grad():\n#         for batch_idx, (images, images2, input_ids, attention_mask, token_type_ids) in enumerate(bar):\n\n#             images = images.to(device)\n#             input_ids = input_ids.to(device)\n#             attention_mask = attention_mask.to(device)\n#             token_type_ids = token_type_ids.to(device)\n\n#             features = nlp_bert_model(images, None, input_ids, attention_mask, token_type_ids, get_embeddings=True)\n\n#             TEXT_FEAS_3 += [features.detach().cpu()]\n\n#     TEXT_FEAS_3 = torch.cat(TEXT_FEAS_3).cpu().numpy()\n    \n#     return TEXT_FEAS_3","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# TEXT_FEAS_3 = generate_nlp_test_features_v2(bert_test_loader)\n# TEXT_FEAS_3.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### TFIDF","metadata":{}},{"cell_type":"code","source":"#https://www.kaggle.com/cdeotte/part-2-rapids-tfidfvectorizer-cv-0-700#Use-Text-Embeddings\n\ndef get_text_predictions(df, max_features = 25_000):\n    \n    model = TfidfVectorizer(stop_words = 'english', binary = True, max_features = max_features)\n    text_embeddings = model.fit_transform(df_cu['title']).toarray()\n    preds = []\n    idx_list = []\n    CHUNK = 1024*4\n\n    print('Finding similar titles...')\n    CTS = len(df)//CHUNK\n    if len(df)%CHUNK!=0: CTS += 1\n    for j in range( CTS ):\n\n        a = j*CHUNK\n        b = (j+1)*CHUNK\n        b = min(b,len(df))\n        print('chunk',a,'to',b)\n\n        # COSINE SIMILARITY DISTANCE\n        cts = cupy.matmul( text_embeddings, text_embeddings[a:b].T).T\n\n        for k in range(b-a):\n            IDX = cupy.where(cts[k,]>0.7)[0]\n            o = df.iloc[cupy.asnumpy(IDX)].posting_id.values\n            idx_list.append(IDX)\n            preds.append(o)\n    \n    del model,text_embeddings\n    gc.collect()\n    return preds, idx_list","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# df_cu = cudf.DataFrame(test)\n# text_predictions,text_idx_list = get_text_predictions(test, max_features = 25_000)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def return_dba_feat(feat, thresh, k):\n    feat = torch.tensor(feat).cuda()\n    batches = []\n    for i in range(n_batch):\n        left = bs * i\n        right = bs * (i+1)\n        if i == n_batch - 1:\n            right = n\n        batches.append(feat[left:right,:])\n            \n    matches = []\n    dba_feat = torch.zeros_like(feat)\n    cnt = 0\n    for batch in tqdm(batches):\n        batch = batch.cuda()\n        similarity_matrix = batch@feat.T\n#         print (similarity_matrix.topk(len(test)))\n        selection = ((similarity_matrix > sim_thresh)).cpu().numpy()\n#         selection_indexes = similarity_matrix.topk(len(test))[1].cpu().numpy()[:, k:]\n#         selection = ((batch@feat.T) > sim_thresh).cpu().numpy()\n        \n        for i, row in enumerate(selection):\n#             print (len(row), len(selection_index))\n#             print (feat[row].size())\n            if len(feat[row]) == 1:\n                dba_feat[cnt] = batch[i]\n            else:\n#                 print (feat[row].mean(axis=0))\n                dba_feat[cnt] = feat[row].mean(axis=0)\n            cnt += 1\n#             print (np.mean(feat[row, :], axis=0))\n#             matches.append(' '.join(test.iloc[row].posting_id.tolist()))\n\n    return dba_feat","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Inference by batches","metadata":{}},{"cell_type":"code","source":"def db_aug(V):\n    model = NearestNeighbors(n_neighbors=2, metric=\"cosine\")\n    model.fit(V)\n    distances, indices = model.kneighbors(V)\n    \n    w = np.power(np.clip(2.0 - distances, 0, 2.0), 0.5)\n    \n    V = (w[:, 0, None]*V[indices[:, 0]] + w[:, 1, None]*V[indices[:, 1]])/w.sum(axis=1)[:, None]\n    \n    return V","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_neighbors(embeddings, KNN = 2, image = True):\n    model = NearestNeighbors(n_neighbors = KNN)\n    model.fit(embeddings)\n    distances, indices = model.kneighbors(embeddings)\n    \n    new_embeddings = np.zeros_like(embeddings)\n    for i, idx in enumerate(indices):\n#         print (embeddings[idx, :].shape)\n        new_embeddings[i] = embeddings[idx, :].mean(axis=0)\n    return new_embeddings","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import collections\ndef combine_for_sub(row):\n#     x = np.concatenate([row.image_matches.split(\" \"),row.nlp_matches.split(\" \"), row.image_matches2.split(\" \")])\n#     x = np.concatenate([row.image_matches.split(\" \"),row.nlp_matches.split(\" \"), row.text_predictions])\n    text_unmatches = row.text_unmatches.split(\" \")\n    nlp_matches = [elem for elem in row.nlp_matches.split(\" \") if elem not in text_unmatches]\n#     text_predictions = [elem for elem in row.text_predictions if elem not in text_unmatches]\n#     x = np.concatenate([row.image_matches.split(\" \"), nlp_matches, text_predictions])\n    x = np.concatenate([row.image_matches.split(\" \"), nlp_matches])\n    \n    return ' '.join( np.unique(x) )\n\ndef combine_for_cv(row):\n    x = np.concatenate([row.image_matches,row.nlp_matches])\n    return np.unique(x)\n\ndef combine_graph(df):\n    posting_id_list = df[\"posting_id\"].tolist()\n    match_list = []\n    for posting_id in posting_id_list:\n        match = \" \".join(df[(df[\"matches\"].str.contains(posting_id)) & (df[\"posting_id\"] != posting_id)][\"matches\"].tolist())\n        orig_match = df[df[\"posting_id\"] != posting_id][\"matches\"].iloc[0]\n        \n        c = collections.Counter(match.split(\" \"))\n        expand_items = [key for key, value in c.items() if value > (len(df[df[\"posting_id\"] != posting_id]) / 2)]\n            \n        match_str = ' '.join( np.unique(orig_match.split(\" \") + expand_items))\n        match_list.append(match_str)\n    df[\"matches\"] = match_list\n    return df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"CONCAT_FEAS = F.normalize(torch.tensor(np.hstack([FEAS, TEXT_FEAS_2])).cuda())\nDBA_FEAS = db_aug(CONCAT_FEAS.cpu().numpy())\nDBA_FEAS = F.normalize(torch.tensor(DBA_FEAS).cuda())\n    \nCONCAT_TEXT_FEAS = torch.tensor(np.hstack([TEXT_FEAS4, TEXT_FEAS])).cuda()\nCONCAT_TEXT_FEAS = db_aug(CONCAT_TEXT_FEAS.cpu().numpy())\nCONCAT_TEXT_FEAS = F.normalize(torch.tensor(CONCAT_TEXT_FEAS).cuda())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image_batches = []\nfor i in range(n_batch):\n    left = bs * i\n    right = bs * (i+1)\n    if i == n_batch - 1:\n        right = n\n    image_batches.append(DBA_FEAS[left:right,:])\n\ntext_batches = []\nfor i in range(n_batch):\n    left = bs * i\n    right = bs * (i+1)\n    if i == n_batch - 1:\n        right = n\n    text_batches.append(CONCAT_TEXT_FEAS[left:right,:])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"posting_ids = test[\"posting_id\"].tolist()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"match_list = []\ncnt = 0\n\nfor image_batch, text_batch in zip(image_batches, text_batches):\n    batch = image_batch.cuda()\n    text_batch = text_batch.cuda()\n    \n    similarity_matrix = batch@DBA_FEAS.T\n    selection = (similarity_matrix > sim_thresh).cpu().numpy()\n    \n    text_similarity_matrix = text_batch@CONCAT_TEXT_FEAS.T\n    text_selection = (text_similarity_matrix > sim_thresh).cpu().numpy()\n    \n    for idx, (img_row, text_row) in enumerate(zip(selection, text_selection)):\n        row = img_row | text_row\n        match_ids = test.iloc[row].posting_id.tolist()\n        text_distances = text_similarity_matrix[idx, row]\n        image_distances = similarity_matrix[idx, row]\n        max_text_dist = text_distances.max().item()\n        max_image_dist = image_distances.max().item()\n        \n        posting_id = posting_ids[cnt]\n        for match_id,text_distance, image_distance in zip(match_ids, text_distances, image_distances):\n            match_list.append({\n                \"posting_id\": posting_id, \n                \"matches\": match_id, \n                \"text_dist\": text_distance.item(), \n                \"image_dist\": image_distance.item(),\n                \"max_text_dist\": max_text_dist,\n                \"max_image_dist\": max_image_dist,\n                \"diff_max_text_dist\": max_text_dist - text_distance.item(),\n                \"diff_max_image_dist\": max_image_dist - image_distance.item(),\n            })\n        cnt += 1\n#         matches.append(' '.join(test.iloc[row].posting_id.tolist()))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"res_df = pd.DataFrame(match_list)\nprint(res_df.shape)\nres_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"res_df[\"multiply_dist\"] = res_df[\"text_dist\"] * res_df[\"image_dist\"]\nres_df[\"total_dist\"] = res_df[\"text_dist\"] + res_df[\"image_dist\"]\nres_df[\"dist_rank\"] = res_df.groupby(\"posting_id\")[\"text_dist\"].rank()\nres_df[\"image_dist_rank\"] = res_df.groupby(\"posting_id\")[\"image_dist\"].rank()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds_df = res_df[res_df[\"posting_id\"] < res_df[\"matches\"]]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds_df = preds_df.merge(test[[\"posting_id\", \"title\"]], on=\"posting_id\", how=\"left\")\npreds_df = preds_df.merge(test[[\"posting_id\", \"title\"]].rename(columns={\"posting_id\": \"matches\"}), on=\"matches\", how=\"left\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tfidf = TfidfVectorizer(ngram_range=(1,1), binary=True)\ntfidf.fit(test[\"title\"])\ntfidf2 = TfidfVectorizer(analyzer=\"char\", ngram_range=(5, 5))\ntfidf2.fit(test[\"title\"])\n\npreds_df[\"cos_sim\"] = tfidf.transform(preds_df[\"title_x\"]).multiply(tfidf.transform(preds_df[\"title_y\"])).sum(axis=1)\npreds_df[\"cos_sim2\"] = tfidf2.transform(preds_df[\"title_x\"]).multiply(tfidf2.transform(preds_df[\"title_y\"])).sum(axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import xgboost as xgb\n\nfeatures = [\n    \"image_dist\", \"text_dist\", \"dist_rank\", \"image_dist_rank\",\n    \"multiply_dist\", \"total_dist\", \"cos_sim\", \"cos_sim2\",\n    \"max_text_dist\", \"max_image_dist\", \n    \"diff_max_text_dist\", \"diff_max_image_dist\"\n]\n\nxgb_model = xgb.XGBClassifier()\nxgb_model.load_model(\"../input/shopee-2ndstage/xgb_8076.json\")\n\nxgb_pred = xgb_model.predict_proba(preds_df[features])[:, 1]\nnn_pred = np.zeros(len(preds_df))\nfor f in range(4):\n    with open(f\"../input/shopee-2ndstage/model_{f}.pkl\", 'rb') as model_file:\n        new_clf = pickle.load(model_file)\n    nn_pred += new_clf.predict_proba(np.array(preds_df[features]).astype(np.float32))[:, 1].ravel()\nnn_pred /= 4.\nnormal_pred = (nn_pred + xgb_pred) / 2.0\npreds_df[\"pred\"] = normal_pred\n# preds_df[features] = preds_df[features].rank(pct=True, axis=0)\n\n# xgb_model = xgb.XGBClassifier()\n# xgb_model.load_model(\"../input/shopee-2ndstage/xgb_8111_pct.json\")\n# xgb_pct_pred = xgb_model.predict_proba(preds_df[features])[:, 1]\n\n# nn_pct_pred = np.zeros(len(preds_df))\n# for f in range(4):\n#     with open(f\"../input/shopee-2ndstage/model_{f}.pkl\", 'rb') as model_file:\n#         new_clf = pickle.load(model_file)\n#     nn_pct_pred += new_clf.predict_proba(np.array(preds_df[features]).astype(np.float32))[:, 1].ravel()\n# nn_pct_pred /= 4.\n# pct_pred = (nn_pct_pred + xgb_pct_pred) / 2.0\n\n# preds_df[\"pred\"] = (normal_pred + pct_pred) / 2.0\npreds_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def agglomerative_clustering(preds_df, single_link_threshold=0.30, group_link_threshold=0.50, group_merge_threshold=0.60):\n\n    groups = dict()\n    group_members = dict()\n\n    gix = 0\n    for i, row in tqdm(preds_df.sort_values(\"pred\", ascending=False).iterrows(), total=preds_df.shape[0]):\n        if row.pred > single_link_threshold:\n            g1 = groups.get(row.posting_id)\n            g2 = groups.get(row.matches)\n\n            if g1 is None and g2 is None:\n                groups[row.posting_id] = gix\n                groups[row.matches] = gix\n                group_members[gix] = {row.posting_id, row.matches}\n                gix += 1\n            elif g1 is None:\n                if row.pred > group_link_threshold:\n                    groups[row.posting_id] = g2\n                    group_members[g2].add(row.posting_id)\n            elif g2 is None:\n                if row.pred > group_link_threshold:\n                    groups[row.matches] = g1\n                    group_members[g1].add(row.matches)\n            elif (g1 != g2) and (row.pred > group_merge_threshold):\n                groups[row.matches] = g1\n                group_members[g1].update(group_members[g2])\n\n                del group_members[g2]\n\n                for k, v in groups.items():\n                    if v == g2:\n                        groups[k] = g1\n\n\n    print(len(groups))\n\n    out_df = []\n\n    for k, v in groups.items():\n        for k2 in group_members[v]:\n            if k != k2:\n                out_df.append({\"posting_id\": k, \"matches\": k2})\n\n    return pd.DataFrame(out_df)\n\nout_df = agglomerative_clustering(preds_df, group_link_threshold=0.75, group_merge_threshold=0.80)\nout_df.shape\n\nsame_df =test[[\"posting_id\"]].copy()\nsame_df[\"matches\"] = same_df[\"posting_id\"].values\n\nout_df = out_df.append(same_df)\nout_df.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# THRESHOLD = 0.63\n\n# out_df = preds_df[preds_df[\"pred\"] > THRESHOLD][[\"posting_id\", \"matches\"]]\n# out_df = out_df.append(out_df.rename(columns={\"posting_id\": \"matches\", \"matches\": \"posting_id\"})).drop_duplicates()\n# out_df.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# same_df = test[[\"posting_id\"]].copy()\n# same_df[\"matches\"] = same_df[\"posting_id\"].values\n\n# out_df = out_df.append(same_df)\n# out_df.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"out_df = out_df.groupby(\"posting_id\")[\"matches\"].agg(list).reset_index()\nout_df[\"matches\"] = out_df[\"matches\"].apply(lambda x: \" \".join(x))\nout_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"out_df.to_csv('submission.csv', index=False, columns=['posting_id', 'matches'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# if True:\n# #     FEAS = return_dba_feat(FEAS, 0.95, 3)\n#     FEAS = F.normalize(torch.tensor(np.hstack([FEAS, TEXT_FEAS_2])).cuda())\n# #     FEAS = F.normalize(torch.tensor(np.hstack([FEAS, FEAS_MODEL2])).cuda())\n#     FEAS = db_aug(FEAS.cpu().numpy())\n#     FEAS = F.normalize(torch.tensor(FEAS).cuda())\n# #     FEAS = torch.tensor(FEAS).cuda()\n#     batches = []\n#     for i in range(n_batch):\n#         left = bs * i\n#         right = bs * (i+1)\n#         if i == n_batch - 1:\n#             right = n\n#         batches.append(FEAS[left:right,:])\n    \n#     matches = []\n#     un_matches = []\n#     cnt = 0\n#     for batch in tqdm(batches):\n#         batch = batch.cuda()\n#         similarity_matrix = batch@FEAS.T\n#         selection = (similarity_matrix > sim_thresh).cpu().numpy()\n        \n#         for row in selection:\n#             matches.append(' '.join(test.iloc[row].posting_id.tolist()))\n        \n#         for i in range(len(similarity_matrix)):\n#             un_matches_ids = []\n#             for text_idx in text_idx_list[cnt]:\n#                 if similarity_matrix[i, int(text_idx)] < text_filter_threshold:\n#                     un_matches_ids.append(text_idx)\n#             un_matches.append(' '.join(test.iloc[un_matches_ids].posting_id.tolist()))\n#             cnt += 1\n            \n# #     print (matches)\n\n#     submission = pd.read_csv('../input/shopee-product-matching/sample_submission.csv')\n#     submission['image_matches'] = matches\n#     submission['text_unmatches'] = un_matches\n            \n# #     FEAS_MODEL2 = torch.tensor(FEAS_MODEL2).cuda()\n# #     batches = []\n# #     for i in range(n_batch):\n# #         left = bs * i\n# #         right = bs * (i+1)\n# #         if i == n_batch - 1:\n# #             right = n\n# #         batches.append(FEAS_MODEL2[left:right,:])\n    \n# #     matches = []\n# #     for batch in tqdm(batches):\n# #         batch = batch.cuda()\n# #         selection = ((batch@FEAS_MODEL2.T) > sim_thresh).cpu().numpy()\n        \n# #         for row in selection:\n# #             matches.append(' '.join(test.iloc[row].posting_id.tolist()))\n    \n# #     submission = pd.read_csv('../input/shopee-product-matching/sample_submission.csv')\n# #     submission['image_matches2'] = matches\n\n#     TEXT_FEAS = torch.tensor(np.hstack([TEXT_FEAS, TEXT_FEAS4])).cuda()\n#     TEXT_FEAS = db_aug(TEXT_FEAS.cpu().numpy())\n#     TEXT_FEAS = F.normalize(torch.tensor(TEXT_FEAS).cuda())\n#     batches = []\n#     for i in range(n_batch):\n#         left = bs * i\n#         right = bs * (i+1)\n#         if i == n_batch - 1:\n#             right = n\n#         batches.append(TEXT_FEAS[left:right,:])\n    \n#     matches = []\n#     for batch in tqdm(batches):\n#         batch = batch.cuda()\n#         selection = ((batch@TEXT_FEAS.T) > text_sim_thresh).cpu().numpy()\n        \n#         for row in selection:\n#             matches.append(' '.join(test.iloc[row].posting_id.tolist()))\n\n#     submission['nlp_matches'] = matches\n# #     submission['matches'] = matches\n#     submission['text_predictions'] = text_predictions\n#     submission['matches'] = submission.apply(combine_for_sub, axis = 1)\n\n# #     submission = pd.read_csv('../input/shopee-product-matching/sample_submission.csv')\n# #     TEXT_FEAS = torch.tensor(np.hstack([TEXT_FEAS, FEAS])).cuda()\n# #     batches = []\n# #     for i in range(n_batch):\n# #         left = bs * i\n# #         right = bs * (i+1)\n# #         if i == n_batch - 1:\n# #             right = n\n# #         batches.append(TEXT_FEAS[left:right,:])\n    \n# #     matches = []\n# #     for batch in tqdm(batches):\n# #         batch = batch.cuda()\n# #         selection = ((batch@TEXT_FEAS.T) > text_sim_thresh).cpu().numpy()\n        \n# #         for row in selection:\n# #             matches.append(' '.join(test.iloc[row].posting_id.tolist()))\n# #     submission = pd.read_csv('../input/shopee-product-matching/sample_submission.csv')\n# #     submission['nlp_matches'] = matches\n# #     submission['matches'] = matches\n# #     submission['matches'] = submission.apply(combine_for_sub, axis = 1)\n# #     submission['matches'] = matches\n# #     submission = combine_graph(submission)\n# submission[['posting_id', 'matches']].to_csv('submission.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}