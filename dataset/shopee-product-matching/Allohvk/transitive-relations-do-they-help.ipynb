{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"We will exploit the transitive relationships in this kernel. If 'A' likes 'B' and 'C' then we not only know that 1) B likes A and 2) C likes A but also that 3) B likes C (and viceversa).\n\nLet us do quick POC first followed by an actual implementation. The POC does not need GPU but the actual implementation needs about 8-10 mins of GPU. I have also stored the output of the GPU processing so that one can turn GPU off for post-processing and directly use the output file shared","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.set_option('display.max_colwidth', None)\n\ntrain = pd.read_csv('../input/shopee-product-matching/train.csv')\ntrain['titleUcase'] = train['title'].str.upper()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Before we do anything serious, let us take the output from here : https://www.kaggle.com/cdeotte/part-2-rapids-tfidfvectorizer-cv-0-700 and see if the idea even makes sense","metadata":{}},{"cell_type":"code","source":"out = pd.read_csv('../input/chris-rapids/submission_Chris.csv')\nout.head(4)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%timeit out.loc[out.posting_id == 'train_2288590299']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"out = out.sort_values('posting_id')\nout = out.set_index('posting_id')\nout = out.sort_index()\n\n%timeit out.loc['train_2288590299']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The below cell was taking well over an hour to execute on a CPU. Hence the above index optimizations. CPU time is now < half a minute\n\nOk what we do now is to take each posting_id and then build a superset of all possible transitive matches it can have. So if A is same as BCD and B is same as CDE and C is same as ABD and D is same as A and E is same as D, then ABCDE is a superset and each posting ID in this superset should match with every element in this superset","metadata":{}},{"cell_type":"code","source":"%%time\nctr=0\n\ndef getcombined(l, combined):\n    global ctr\n    if len(combined) == 0: \n        ctr+=1\n        print(ctr) if ctr%6000==0 else None\n    \n    if len(l) < 3 and len(combined)==0 :\n        return l\n    elif len(combined) >= 50:\n        return combined\n    \n    local_combined = set()\n    for item in l:\n        matches = set(out.loc[item]['matches'].split(' '))\n        local_combined.update(matches)\n        \n    remaining = (local_combined - set(l)) - combined\n    combined.update(local_combined)\n\n    if len(remaining) > 0:\n        getcombined(remaining, combined)\n        \n    return list(combined)\n\nout['combined'] = out.apply(lambda x: getcombined(x.matches.split(' '), set()), axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Phew, This was my first recursive function and I asm sure there may be a more elegant way of writing this. Anyway this serves the purpose for now. The reason why we make it recursive is that as we go thru' all the matches for a particular posting ID (one iteration), we may get new matches once the iteration is over and we have to repeat this process for the new iterations. We need to keep doing that until we have iterated thru' all matching postingIDs or the num_postingID>50\n\nNote - I later discovered a bug here in the way I assign the output. I do it just for the row in question. Instead I should do it for all matching IDs that are returned. I correct this in the actual implementation below\n\nOk. Looks good. Let us check our score. Chris's original score was 72.X. Let us use the same function to compute CV","metadata":{}},{"cell_type":"code","source":"def getMetric(col):\n    def f1score(row):\n        n = len( np.intersect1d(row.target,row[col]) )\n        return 2*n / (len(row.target)+len(row[col]))\n    return f1score","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The input to the scoring function will be 'combined' col from the table we just created. Let us merge it back to 'train' and then use that to compute the score. But before that there is a headache to be taken care of. The submission format and the CV format vary slightly..one is a string separated by spaces and the other is a list. We need a list for CV. Our 'combined' is already a list but the original 'matches' isnt. So let us create a new col 'matches_orig' in list format from the submission output","metadata":{}},{"cell_type":"code","source":"out['matches_orig'] = list(out.matches.str.split(' '))\nout.head(1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now, let us use the new cols 'matches_orig' and 'combined' to get the original and the new scores. Dont forget to add the label first.","metadata":{}},{"cell_type":"code","source":"tmp = train.groupby('label_group').posting_id.agg('unique').to_dict()\ntrain['target'] = train.label_group.map(tmp)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.merge(train, out,  how='left', left_on=['posting_id'], right_on = ['posting_id'])\n\ntrain['f1_orig'] = train.apply(getMetric('matches_orig'),axis=1)\nprint('Orig CV Score =', train.f1_orig.mean() )\n\ntrain['f1'] = train.apply(getMetric('combined'),axis=1)\nprint('New CV Score =', train.f1.mean() )\n\n##Drop these columns to avoid confusion later\nif 'f1' in train.columns:\n    train.drop('f1',axis='columns', inplace=True)\n    \nif 'matches' in train.columns:\n    train.drop('matches',axis='columns', inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Ok. The score deteriorated slightly from the original CV. This was kind of expected. The number of false +ves generated would have increased dramatically. In fact I was expecting a far worse score that this. Now here is how we can actually utilize this bit of PP - First, in the regular ML program, we will make the cosine similarity check far more stringent that before. So any matches now are pretty sure to be 'actual' matches. Now we add this additional bit of PP to build transitive relations and expand the 'matches' column. Hopefully this returns a better score. To understand better, take a simple case where A is similar to B(with text comparision) and C(with image comparision). Now this automatically implies that our algorithm will return that B is similar to A and C is similar to A. But what happens if the text for B is not very closely related to the text for C (maybe it just falls below the cut-off criteria or maybe it contains translated words hence there is no matching) and the images of B and C also dont match (maybe the angles and lighting are different). So in that case B need not be equal to C as per our model. But we do know that this cant be the case - if A is equal to B and C (assuming we have high confidence because we have set high cut-offs) then B & C must simply be equal even though our model says No. \n\nCan we exploit this fact? Let us see..\n\nBefore that, let us load Rapids and calculate the cosine simialrity. We will use Chris's code exactly as-is for this purpose with some bare minimal changes while calculating the cosine similarity. I have also added a bit of my documentation so that I can understand this code easily when I look at it later\n\nNote that we will have to turn on GPUs here. I wish Kaggle had an option of turning it on programatically","metadata":{}},{"cell_type":"code","source":"import gc\nimport cv2, matplotlib.pyplot as plt\nimport cudf, cuml, cupy\nfrom cuml.feature_extraction.text import TfidfVectorizer\nfrom cuml.neighbors import NearestNeighbors\nimport tensorflow as tf\nfrom tensorflow.keras.applications import EfficientNetB0\n\nprint('RAPIDS',cuml.__version__)\nprint('TF',tf.__version__)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gpus = tf.config.experimental.list_physical_devices('GPU')\n\n##Give Rapids 15GB of the GPU and let TF take 1 GB (1024 MB)\nif gpus:\n    try:\n        tf.config.experimental.set_virtual_device_configuration(gpus[0],[tf.config.experimental.VirtualDeviceConfiguration(memory_limit=1024)])\n    except RuntimeError as e:\n        print(e)\nelse:\n    print('tough luck. Work with cpus')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let us read the file again and we start from scratch","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv('../input/shopee-product-matching/train.csv')\n\n##create the target based on the label_group\n##first element in target is self and then all the matches\ntmp = train.groupby('label_group').posting_id.agg('unique').to_dict()\ntrain['target'] = train.label_group.map(tmp)\ntrain.head(2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"ok, now let us use the images and text and phash. We will precit using them individually and then we will ensemble. Note that concatenating the embeddings to create one giant embedding is NOT a good strategy for multiple reasons. Instead let us try to use the best of the 3 groups with some intelligent ensembling\n\nWe will also have to convert our datasets to RAPIDS format. This is test_rapids.\n\nKeep in mind that we are calculating CV and we will use the train data instead of the test data.","metadata":{}},{"cell_type":"code","source":"test = pd.read_csv('../input/shopee-product-matching/train.csv')\ntest_rapids = cudf.DataFrame(test)\n    \ntest_rapids.head(4)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Since the dataset is huge, we cant process all the records at the same time. They will not be able to fit in memory at the same time.\n\nWe need to create some sort of a data generator that produces data in batches. We can leverage the keras.utils.Sequence which is the root class for Data Generators. We need to override 4 methods to implement a custom data loader. \n\nOnce we have this, it is fairly easy to generate required batches of data and pass it to the model.fit\n\nWe use open cv to read and resize images which is much faster","metadata":{}},{"cell_type":"code","source":"##Use image embeddings\nclass DataGenerator(tf.keras.utils.Sequence):\n    def __init__(self, df, img_size=256, batch_size=32, path=''): \n        self.df = df\n        self.img_size = img_size\n        self.batch_size = batch_size\n        self.path = path\n        self.indexes = np.arange(len(self.df))\n        \n    def __len__(self):\n        ##This param is supposed to return the length of the batches\n        ct = len(self.df) // self.batch_size\n        ct += int(( (len(self.df)) % self.batch_size)!=0)\n        return ct\n\n    def __getitem__(self, index):\n        ##Hands out one batch of data\n        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n        X = self.__data_generation(indexes)\n        return X\n            \n    def __data_generation(self, indexes):\n        ##This is the actual data generator\n        X = np.zeros((len(indexes),self.img_size,self.img_size,3),dtype='float32')\n        df = self.df.iloc[indexes]\n        for i,(index,row) in enumerate(df.iterrows()):\n            img = cv2.imread(self.path+row.image)\n            X[i,] = cv2.resize(img,(self.img_size,self.img_size))\n        return X","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we can do----  data = DataGenerator(test, path=BASE)\n\nMore particularly since we are going to process in chunks, we will call---- data = DataGenerator(test.iloc[a:b], path=BASE)\n\nFirst we will extract image embeddings using EffNetB0 in chunks. Later we will do a kNN checks. We use tensorflow.keras.applications import EfficientNetB0 which they do not normalize (for other versions we should, unless we do a fine-tuning). The model takes input images of shape (224,224,3) and i/p data from 0 to 255.\n\nBut first a quick intro to Rapids - a collection of neat libraries which executes DIRECTLY on the GPUs. There is no data movement. Everything runs on the GPU.\n\ncuDF is the Python GPU DataFrame library for manipulating data using a DataFrame style API. \n\ncuML is their suite of ML libraries. We use cuML KNN to find images that are similar. Apparently, the RAPIDS cuML implementation of kNN search on GPU is based on Facebookâ€™s SOTA FAISS library and is BLAZINGLY fast. See https://medium.com/rapids-ai/accelerating-k-nearest-neighbors-600x-using-rapids-cuml-82725d56401e by our very own Chris\n\nAll the API (commands) for Pandas work with RAPIDS cuDF and all the API (commands) for Scikit-Learn work with RAPIDS cuML. ","metadata":{}},{"cell_type":"code","source":"%%time\nBASE = '../input/shopee-product-matching/train_images/'\n\nmodel = EfficientNetB0(weights='imagenet', include_top=False, pooling='avg', input_shape=None)\n##If submitting, then internet is off, so download the weights beforehand\n\n##We predict in chunks to avoid OOM issues\nCHUNK = 1024*4\nCTS = len(test)//CHUNK\nif len(test)%CHUNK!=0: \n    ##one last chunk for any left-over records\n    CTS += 1\nembeds = []\n\nfor i,j in enumerate(range( CTS )):\n    a = j*CHUNK\n    b = (j+1)*CHUNK\n    b = min(b,len(test))\n    print('chunk',a,'to',b)\n    \n    test_gen = DataGenerator(test.iloc[a:b], path=BASE)\n    image_embeddings = model.predict(test_gen,verbose=1,use_multiprocessing=True, workers=4)\n    embeds.append(image_embeddings)\n    \ndel model\n_ = gc.collect()\nimage_embeddings = np.concatenate(embeds)\nprint('image embeddings shape',image_embeddings.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"KNN = 50\nmodel = NearestNeighbors(n_neighbors=KNN)\nmodel.fit(image_embeddings)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now for each postingid, let us get the nearest neightbours. Chris uses a value of 6. We will use values of 2 and 8 to get the liberal and stringent versions of nearest neighbours","metadata":{}},{"cell_type":"code","source":"preds, preds_liberal, preds_stringent = [], [], []\n\nCHUNK = 1024*4\nCTS = len(image_embeddings)//CHUNK\nif len(image_embeddings)%CHUNK!=0: \n    CTS += 1\n\nprint('Finding similar images...')\nfor j in range( CTS ):\n    a = j*CHUNK\n    b = (j+1)*CHUNK\n    b = min(b,len(image_embeddings))\n    print('chunk',a,'to',b)\n    distances, indices = model.kneighbors(image_embeddings[a:b,])\n    \n    for k in range(b-a):\n        IDX = np.where(distances[k,]<6.0)[0]\n        IDX_liberal = np.where(distances[k,]<8.0)[0]\n        IDX_stringent = np.where(distances[k,]<2.0)[0]\n        \n        IDS = indices[k,IDX]\n        IDS_liberal = indices[k,IDX_liberal]\n        IDS_stringent = indices[k,IDX_stringent]\n        \n        o = test.iloc[IDS].posting_id.values\n        o_stringent = test.iloc[IDS_stringent].posting_id.values\n        o_liberal = test.iloc[IDS_liberal].posting_id.values\n        \n        preds.append(o)\n        preds_liberal.append(o_liberal)\n        preds_stringent.append(o_stringent)\n\ntest['preds_image'],test['preds_image_lib'],test['preds_image_stri'] = preds, preds_liberal, preds_stringent\ndel model, distances, indices, image_embeddings, embeds\n_ = gc.collect()\n\nprint('Fast and clean as can be. Rapids is good!')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The key items of relevance above is the distance (Euclidean?) choice. Chris seems to have done some sort of analysis here before deciding on 6. We use 2 and 8 and this is an area which can be experimented with\n\nWe could also use cosine distance\n\nLet us move onto TEXT now. We just put it in a function so it can be re-used if time permits for the actual comp. Just note that the test_rapids version is needed to vectorize...else we use the test dataframe for other operations.","metadata":{}},{"cell_type":"code","source":"def get_text_predictions(df_cu, df, max_features = 25_000):\n    \n    model = TfidfVectorizer(stop_words = 'english', binary = True, max_features = max_features)\n    text_embeddings = model.fit_transform(df_cu.title).toarray()\n    \n    preds, preds_liberal, preds_stringent = [],[],[]\n    CHUNK = 1024*4\n\n    print('Finding similar titles...')\n    CTS = len(df)//CHUNK\n    if len(df)%CHUNK!=0: CTS += 1\n        \n    for j in range(CTS):\n        a = j*CHUNK\n        b = (j+1)*CHUNK\n        b = min(b,len(df))\n        print('chunk',a,'to',b)\n\n        cts = cupy.matmul( text_embeddings, text_embeddings[a:b].T).T\n\n        for k in range(b-a):\n            IDX = cupy.where(cts[k,]>0.7)[0]\n            IDX_liberal = cupy.where(cts[k,]>0.4)[0]\n            IDX_stringent = cupy.where(cts[k,]>0.98)[0]\n            \n            o = df.iloc[cupy.asnumpy(IDX)].posting_id.values\n            o_liberal = df.iloc[cupy.asnumpy(IDX)].posting_id.values\n            o_stringent = df.iloc[cupy.asnumpy(IDX)].posting_id.values\n            \n            preds.append(o)\n            preds_liberal.append(o_liberal)\n            preds_stringent.append(o_stringent)\n    \n    del model,text_embeddings\n    gc.collect()\n    return preds,preds_liberal,preds_stringent\n\ntest['preds_text'],test['preds_textlib'],test['preds_textstri'] = get_text_predictions(test_rapids, test, max_features = 25_000)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"##Phash it up\ntmp = test.groupby('image_phash').posting_id.agg('unique').to_dict()\ntest['preds_phash'] = test.image_phash.map(tmp)\ntest.head(2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"##I am sure there is a better way than to repeat\ndef combine_for_sub(row):\n    x = np.concatenate([row.preds_text,row.preds_image, row.preds_phash])\n    return ' '.join(np.unique(x))\n\ndef combine_for_sub_stri(row):\n    x_stringent = np.concatenate([row.preds_textstri,row.preds_image_stri, row.preds_phash])\n    return ' '.join(np.unique(x_stringent))\n\ndef combine_for_sub_lib(row):\n    x_liberal = np.concatenate([row.preds_textlib,row.preds_image_lib, row.preds_phash])\n    return ' '.join(np.unique(x_liberal))\n\ndef combine_for_cv(row):\n    x = np.concatenate([row.preds_text,row.preds_image, row.preds_phash])\n    return np.unique(x)\n\ndef combine_for_cv_stringent(row):\n    x_stringent = np.concatenate([row.preds_textstri,row.preds_image_stri, row.preds_phash])\n    return np.unique(x_stringent)\n\ndef combine_for_cv_liberal(row):\n    x_liberal = np.concatenate([row.preds_textlib,row.preds_image_lib, row.preds_phash])\n    return np.unique(x_liberal)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tmp = test.groupby('label_group').posting_id.agg('unique').to_dict()\ntest['target'] = test.label_group.map(tmp)\ntest['oof']= test.apply(combine_for_cv,axis=1)\ntest['oof_liberal']= test.apply(combine_for_cv_liberal,axis=1)\ntest['oof_stringent']= test.apply(combine_for_cv_stringent,axis=1)\n\ntest['f1'] = test.apply(getMetric('oof'),axis=1)\nprint('CV Score =', test.f1.mean())\ntest['f1'] = test.apply(getMetric('oof_liberal'),axis=1)\nprint('CV Score liberal =', test.f1.mean())    \ntest['f1'] = test.apply(getMetric('oof_stringent'),axis=1)\nprint('CV Score stringent =', test.f1.mean())\n    \ntest['matches'] = test.apply(combine_for_sub,axis=1)\ntest['matches_lib'] = test.apply(combine_for_sub_lib,axis=1)\ntest['matches_stri'] = test.apply(combine_for_sub_stri,axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The CV scores are: \nCV Score = 72.48\n\nCV Score liberal = 69.93\n\nCV Score stringent = 68.19\n\nNow, let us apply getcombined() function for the stringent cases. We will save the output file and process it without GPU","metadata":{}},{"cell_type":"code","source":"test[['posting_id','matches', 'matches_lib', 'matches_stri']].to_csv('CV_difflevels.csv',index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"If we want, we can switch off GPU here and jump directly to this cell after executing first 10 cells of this kernel (upto where we start Rapids and GPU) ","metadata":{}},{"cell_type":"code","source":"out = pd.read_csv('../input/cvwithdifflevels/CV_difflevels (1).csv')\nout.head(4)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"out = out.sort_values('posting_id')\nout = out.set_index('posting_id')\nout = out.sort_index()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There were 2 bugs and I correct them now in the actual function. The first is that once we get the o/p for a particular product ID, we need to update all rows with that superset (not just the row in function). Once we do this, we can also leverage the fact that if 'combined' is found to be updated for a particular row, there is no need to process the function again. The last mistake (which is the silliest one was that I was using the original 'matches' column for the superset updates instead of using the 'matches_stringent'.\n\nLastly we will change the output columns to list instead of strings, so it will be a little easier to process for CV purposes. Let us do that first","metadata":{}},{"cell_type":"code","source":"out['matches_orig'] = list(out.matches.str.split(' '))\nout['matches_stri_orig'] = list(out.matches_stri.str.split(' '))\nout['matches_lib_orig'] = list(out.matches_lib.str.split(' '))\nout.drop('matches_stri',axis='columns', inplace=True)\nout.drop('matches_lib',axis='columns', inplace=True)\nout.drop('matches',axis='columns', inplace=True)\n    \nout.head(1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def getcombined(l, combined, mode):\n    \n    if len(l) < 3 and len(combined)==0 :\n        return l\n    elif len(combined) >= 50:\n        return combined\n    \n    local_combined = set()\n    for item in l:\n        if mode == 'strict':\n            matches = set(out.loc[item]['matches_stri_orig'])\n        elif mode == 'liberal':\n            matches = set(out.loc[item]['matches_lib_orig'])\n        else:\n            matches = set(out.loc[item]['matches_orig'])            \n        local_combined.update(matches)\n        \n    remaining = (local_combined - set(l)) - combined\n    combined.update(local_combined)\n\n    if len(remaining) > 0:\n        getcombined(remaining, combined, mode)\n        \n    return list(combined)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"out['combined'] = None\n\ndef updatetransitive(x_index, x):\n    if out.loc[x_index, 'combined'] != None:\n        return\n        \n    combined = getcombined(x, set(), 'strict')\n\n    for item in combined:\n        out.loc[item, 'combined'] = combined\n\nout.apply(lambda x: updatetransitive(x.name, x.matches_stri_orig), axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"##clean up past merges if any\nfor item in ['combined', 'matches_orig', 'matches_lib_orig', 'matches_stri_orig','combined_x', 'combined_y', 'matches_orig_x', 'matches_orig_y', 'matches_lib_orig_x','matches_lib_orig_y','matches_stri_orig_x','matches_stri_orig_y']:\n    if item in train.columns:\n        train.drop(item,axis='columns', inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.merge(train, out[['combined', 'matches_orig', 'matches_stri_orig', 'matches_lib_orig']],  how='left', left_on=['posting_id'], right_on = ['posting_id'])\ntrain[train.columns[5:]].head(4)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now let us see if it has improved our stringent distance check. If not, this experiment is as good as done and we should wind up","metadata":{}},{"cell_type":"code","source":"train['f1_orig'] = train.apply(getMetric('matches_orig'),axis=1)\nprint('CV Score Orig', train.f1_orig.mean())\n\ntrain['f1_lib'] = train.apply(getMetric('matches_lib_orig'),axis=1)\nprint('CV Score liberal', train.f1_lib.mean())\n\ntrain['f1_stri'] = train.apply(getMetric('matches_stri_orig'),axis=1)\nprint('CV Score stringent', train.f1_stri.mean())\n\ntrain['f1_stri_PP'] = train.apply(getMetric('combined'),axis=1)\nprint('CV Score stringent - WITH TRANSITIVE PP', train.f1_stri_PP.mean())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Ok. I would have expected more improvement, but it looks like there are still many false +ves which contaminate the score when we do this transitive PP. Anyway there is an improvement in CV from 68.19 to 68.84. Let us now try an ensemble. Basically for those loners with no matches, we try to set them up with some dates from matches_orig or matches_liberal in that order.\n\nIf the results go higher than 72.48, then the whole solution makes good\n\nPandas just were not made to store lists. So rather than doing ugly hacks, we will just store the ensemble as a string and then convert them to list. This process takes 3-4 mins on a CPU and definitely needs optimization.","metadata":{}},{"cell_type":"code","source":"def ensemble(row):\n    if len(row.combined) ==1:\n        if len(row.matches_orig) > 1:\n            train.loc[train.posting_id==row.posting_id, 'ensemble'] =  ' '.join(row.matches_orig)\n        elif len(row.matches_lib_orig) > 1:\n            train.loc[train.posting_id==row.posting_id, 'ensemble'] = ' '.join(row.matches_lib_orig)\n        else:\n            train.loc[train.posting_id==row.posting_id, 'ensemble'] = ' '.join(row.combined)\n    else:\n        train.loc[train.posting_id==row.posting_id, 'ensemble'] = ' '.join(row.combined)\n    \ntrain.apply(ensemble, axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['ensemble'] = list(train.ensemble.str.split(' '))\ntrain.head(1)[train.columns[6:]]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['f1_stri_PP_ensemble'] = train.apply(getMetric('ensemble'),axis=1)\nprint('CV Score stringent - WITH TRANSITIVE PP', train.f1_stri_PP_ensemble.mean())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Nope. It does not go beyond 72.4 which was our original CV. A failed experiment then? Depends. We could see that the stringent score CV improves from 68.1 to 68.8 when we consider transitive relationships. The idea is sound but the implementation failed (reminds me of a dialog from the Ghost and the darkness movie from a young Michael Douglas). Anyway, the challenge is that if we reduce the stringency, then we get lots of false positives and the score actually drops due to the PP. Increasing stringency causes the CV to drop initially but even after PP, it does not recover back to its original level. \n\nFor this PP to work, we need stringency but the moment we make cut-off stringent, the CV drops big time and the gains from PP are not enough to go past its original mark. Can we find a sweet spot where CV does not drop too much and the gains from PP carry it past the older score? Would having sharper differentiation between embeddings work? Theoritically it should but unfortunately I couldnt make it work in the limited time I had and it would be nice if someone could fix this. One option could be to go back to those records which have huge number of matches and then try increasing stringency level cut-offs onlhy for those records and then do the PP only for those. Another option is to do this PP only for those records where we are 100% sure of a match.\n\nIn any case, due to work constraints I would be taking a break of a couple of months from Kaggle - at least no new comps or kernels, though I hope I should be able to occassionally keep myself upto date on the discussions happening. All the best to all participants of Shoppe and I will be eagery awaiting the results day discussions.","metadata":{}}]}