{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"We create a domain specific dictionary of 2000+ Indonesean to English words. These are the Indonesan words present in the 'title' field of the training data provided by Shoppee. We explore various versions of Bert, Bart and google translate to make sense of this data","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.set_option('display.max_colwidth', None)\n\ntrain = pd.read_csv('../input/shopee-product-matching/train.csv')\ntrain['titleUcase'] = train['title'].str.upper()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let us try to get the number of unique words across all records. The 'set' is just about the right data structure","metadata":{}},{"cell_type":"code","source":"import re\n\nunique_words = set()\nall_words = train['titleUcase'].str.findall('\\w+')\n\nfor x in all_words:\n    words = []\n    for item in x:\n        if item.isalpha() and len(item)>2:\n            words.append(item)\n    unique_words.update(words)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(unique_words)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from nltk.corpus import words\nsetofwords = set(words.words())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Unfortunately this does not work for plural sometimes...though it seems to work for eveything else","metadata":{}},{"cell_type":"code","source":"print(\"run\",\"run\" in setofwords)\nprint(\"ran\",\"ran\" in setofwords)\nprint(\"running\",\"running\" in setofwords)\nprint(\"ren\",\"ren\" in setofwords)\nprint(\"dog\",\"dog\" in setofwords)\nprint(\"dogs\",\"dogs\" in setofwords)\nprint(\"horse\",\"horse\" in setofwords)\nprint(\"horses\",\"horses\" in setofwords)\nprint(\"sony\",\"horses\" in setofwords)\nprint(\"samsung\",\"horses\" in setofwords)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"You would expect it to be consistent (if not thorough). So this puts a question mark on other words as well. Note that this would also not include joint words (which BERT nicely breaks into constituents). Anyway we want a ballpark, so we can go on","metadata":{}},{"cell_type":"code","source":"unique_english = []\n\nfor item in unique_words:\n    if item.lower() in setofwords:\n        unique_english.append(item)\n    else:\n        if (item[-1] == 'S') and (item[:-1].lower() in setofwords) and (len(item)>3):\n            unique_english.append(item)\n        \nprint(len(unique_english),'\\n')\nprint(unique_english[:100])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I didnt know that Baud, Doff, Wen etc were english words but apparently they are as per the dictionary. The challenge is that these could 'also' be Indo words  with a different meaning. Anyway let us move on","metadata":{}},{"cell_type":"code","source":"unique_nonenglish = unique_words - set(unique_english)\nprint(len(unique_nonenglish), '\\n')\nprint(list(unique_nonenglish)[:100])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"But a GOOD chunk of it will include brand name, location, org names etc.\n\nLet us see if we can get a rough idea of number of non-English words which are not names, brands, orgs etc. We will use Google translate API","metadata":{}},{"cell_type":"code","source":"!pip install google_trans_new","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from google_trans_new import google_translator  \ntranslator = google_translator()  \ntranslate_text = translator.translate('wanita',lang_tgt='en')  \nprint(translate_text)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Google translate has severe constraints on number of concurrent requests. It blocks the IP if used in a loop. So we will have to discard it. \n\nLet us use the dictionary provided below. \n\n\nhttps://raw.githubusercontent.com/sastrawi/sastrawi/master/data/kata-dasar.original.txt\n\nThis is under MIT license\n\nCopyright (c) 2015 Andy Librian\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.","metadata":{}},{"cell_type":"code","source":"with open ('../input/indoneseanwords/kata-dasar.original.txt', \"r\") as myfile:\n    indo_dict=set(myfile.read().splitlines())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_indo_words = []\n\nfor item in unique_nonenglish:\n    if item.lower() in indo_dict:\n        train_indo_words.append(item)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"unique_nonenglish_nonindo = unique_nonenglish - set(train_indo_words)\nprint(len(train_indo_words), '\\n')\nprint(train_indo_words[:100])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(list(unique_nonenglish)[:200])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let us check out the BERT version for Indonesean lang. Huggingface provides a nice autotokenizer and AutomodelwithMLhead options that helps auto-choose models and tokernizers without us having to specify things in great details. Let us try it out..\n\nThey have several translation models from the University of Helsinki in their transformer model zoo. We use opus-mt-id-en. Basically, for any given language code pair you can download a model with the name Helsinki-NLP/optus-mt-{lang}-{target_lang} where lang is the language code for the source language and target_lang is the language code for the target language we want to translate to. ","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelWithLMHead\n\nmodel_helinski = AutoModelWithLMHead.from_pretrained('Helsinki-NLP/opus-mt-id-en')\ntok_helinski = AutoTokenizer.from_pretrained('Helsinki-NLP/opus-mt-id-en')","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We will use a pipeline that absrtacts most of code and provides a nice API for most NLP tasks. In our case, we are interested in translations\n\nIt may seem like there is no need for pipelines or auto-tokenizers etc but believe me when you start exploring the 1000's of models in the zoo these features really come handy","metadata":{}},{"cell_type":"code","source":"from transformers import pipeline\n\ntranslation = pipeline('translation_id_to_en', model=model_helinski, tokenizer=tok_helinski)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Ok. Let us see how good the translator is","metadata":{}},{"cell_type":"code","source":"for item in train_indo_words[:25]:\n    translated_text = translation(item)[0]['translation_text']\n    print(item,translated_text) if item != translated_text else None","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Unfortunately about half the translations are wrong. We cant use this. The SOTA is translation is BART by Facebook and in particular mBart-50 which came in 2020. Let us check it out.\n\nFor Seq-Seq models, we cant use AutoModelWithLMHead for the model. We have to replace with AutoModelForSeq2SeqLM. Rest remains the same","metadata":{}},{"cell_type":"code","source":"from transformers import AutoModelForSeq2SeqLM\n\nmodel_mbart50 = AutoModelForSeq2SeqLM.from_pretrained('facebook/mbart-large-50-many-to-many-mmt')\ntok_mbart50 = AutoTokenizer.from_pretrained('facebook/mbart-large-50-many-to-many-mmt')\ntranslation = pipeline(\"translation_id_to_en\", model=model_mbart50, tokenizer=tok_mbart50)","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In 3 lines we were able to set up a translation pipeline. Notice how the API understands that the tokenizer corresponding to mbart-large-50-many-to-many-mmt is MBart50Tokenizer. \n\nUnfortunately there were multiple issues which I explored to get this working:\n- Initially it gave error saying \"unable to find the MBart50Tokenizer\" path. Since this is a SOTA model I thought, Huggingface has yet to incorporate this in their formal release. On their support site, they request to download and use their source code directly until this is done \n- Linking directly with Hugging face transformer source code didnt work\n- Some debugging of their source code shows that \"sentencepiece\" library is mandatory. Installed that as well but didnt work\n- Transformer version was showing as older one. Though I upgraded transformers (!pip install transformers) it kept showing older version \n- Finally I got to the root of the problem. Kaggle seems to have a work env (on RHS, click on Env-preferences link). We have to override that to get the latest environments. This finally made the above statements work (note it is reccomended to use a stable v ersion rather than a latest version unless there is a compelling reason)","metadata":{}},{"cell_type":"code","source":"!pip freeze | grep transformers","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for item in train_indo_words[:25]:\n    translated_text = translation(item)[0]['translation_text']\n    print(item,translated_text) if item != translated_text else None","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Well looks like the model is wanting to show off about all the languages on the universe it knows. This just does not work for us. Maybe we have to ditch the pipeline API and just try the regular route or creating the tokens and tensors ourselves","metadata":{}},{"cell_type":"code","source":"tok_mbart50.src_lang = \"ja_XX\"\nencoded = tok_mbart50('wanita', return_tensors=\"pt\")\ngenerated_tokens = model_mbart50.generate(**encoded, forced_bos_token_id=tok_mbart50.lang_code_to_id[\"en_XX\"])\ntok_mbart50.batch_decode(generated_tokens, skip_special_tokens=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Phew!!!","metadata":{}},{"cell_type":"code","source":"for item in train_indo_words[:25]:\n    encoded = tok_mbart50(item.lower(), return_tensors=\"pt\")\n    generated_tokens = model_mbart50.generate(**encoded, forced_bos_token_id=tok_mbart50.lang_code_to_id[\"en_XX\"])\n    translated_text = tok_mbart50.batch_decode(generated_tokens, skip_special_tokens=True)\n    print(item,translated_text)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So finally we get the desired outputs. You see how the 'pipeline' API ditched us and we had to abondon it for an alternate way. Well, since this is pretty new tech, maybe these issues are not reported to Huggingface yet. There are couple of other issues I faced and spent lots of time debugging. For e.g., I was originally trying this in TF but this command does not work. There is an issue and though I spent a lot of time debugging and going thru' original codes as well, I couldnt solve it. I had to change the tensors to PyTorch to get it to work.\n\nThe results are much much better than the Helinski translators but still not as great as the Google translator. However it must be noted that there are no wrong translations. We have managed to overcome the bypass API  load limitations at Google and create a neat translator of our own that can translate text from any language in the world to any other using mbart. There is hardly any online help for debugging issues that arose on the way and so there is a small sense of satisfaction :)\n\nBART has both an encoder (like BERT) and a decoder (like GPT), essentially getting the best of both worlds. The encoder uses a denoising objective similar to BERT while the decoder attempts to reproduce the original sequence (autoencoder), token by token, using the previous (uncorrupted) tokens and the output from the encoder.\n\nLastly coming to the mystery of why the 'transformers' version was not getting upgraded even though we were doing pip installs. I found one kernel talk about this solution (though I didnt get time to explore in detail & confirm if it really works) - huggingface uses module \"pkg_resources\" to get the version. But pkg_resources is loaded right after the Kaggle's notebook starting. So it CANNOT get the correct version of transformers after we upgrade (somewhere in the middle of the kernel) and keeps pointing to the old version. The trick is to just releoad pkg_resources. Now hopefully we can change back the environment settings to the old value and not worry about ever-changing versions of libraries :) Note - Havent tested this but intend to test it in future if I get time.\n\nBut human greed sometimes does not have limits and it often challenges us to do a little more. I was thinking of some way to leverage Google tranbslate's awesome service and then realized about the 5000 character document conversions. Basically we just take\nhttps://raw.githubusercontent.com/sastrawi/sastrawi/master/data/kata-dasar.original.txt and break it into 5000 character chunks and feed to Google translate. There are couple of caveats here. Firstly we need to put a fullstop between each word else Google will try to translate it into a sentence and may shuffle the words. For e.g. try translating \"wanita PERANGKAP sapi PERANGKAT APEL wanita\". Secondly let us not translate entire Indo dict but will translate only those 2K odd words that are part of Title field. We may have to do it in 5 iterations","metadata":{}},{"cell_type":"code","source":"for i in range(1,7):\n    with open('translate' + str(i) + '.txt', 'w') as file_t:\n        file_t.write('.\\n'.join(str(item) for item in train_indo_words[(i-1)*500:i*500]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"IMP: There is a manual step here where I run Google translate on these 6 files and append the outputs into the 'translated.txt' file which has been uploaded.","metadata":{}},{"cell_type":"code","source":"with open ('../input/googletranslated/translated.txt', \"r\") as myfile:\n    translated=(myfile.read().splitlines())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"translated = [s.strip('.') for s in translated]\nprint(translated[:25])\nprint('\\n', len(translated))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Perfect, we just have to zip them up to create a dict. Note that I read train_indo_words from the 'translate.txt' file which is nothing but the appending of the 6 output files we created for the translate. Each time the kernel is run, train_indo_words is sorted in a different way because of the set() function. I realized this after translating the 6 files manually and have no intention of going back to sorting the set and re-doing gthe whole thing ","metadata":{}},{"cell_type":"code","source":"with open ('../input/translate/translate.txt', \"r\") as myfile:\n    train_indo_words=(myfile.read().splitlines())\ntrain_indo_words = [s.strip('.') for s in train_indo_words]\n    \nindo_en_dict = dict(zip(train_indo_words, translated))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let us remove the un-translated words. Hopefully there would not be too many of them","metadata":{}},{"cell_type":"code","source":"for k, v in dict(indo_en_dict).items():\n    if k==v:\n        del indo_en_dict[k]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(len(indo_en_dict), '\\n\\n')\nprint(list(indo_en_dict.items())[:100])\n\nwith open('indonesean_english_dict.txt', 'w') as file_t:\n    file_t.write('\\n'.join(str(item) for item in list(indo_en_dict.items())))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"yay! we get a nice dictionary which is domain specific to Shoppe!! Now a few questions beg to be answered at this point. Do we really a need a dict? Ragnar showed good scores with a regular english Bert model. There is also a Indonesean Bert model that is doing the rounds. Will that not suffice? Obviously whosoever is using the English version of BERT will benifit from a translation before-hand. But how about the Indonesean model? Will that benefit? We have seen that there are quite a few English words in the 'Title' field. How will the Indonesean model interpret these words? It is completely Indo right? Let us check the the Indoensian Bert - https://huggingface.co/cahya/bert-base-indonesian-522M","metadata":{}},{"cell_type":"code","source":"from transformers import BertTokenizer, TFBertModel\n\nmodel_name='cahya/bert-base-indonesian-522M'\ntokenizer = BertTokenizer.from_pretrained(model_name)\nbert_layer = TFBertModel.from_pretrained(model_name)","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Firstly no more 'pipeline's for me for a while... Secondly let us choose Tensorflow. There is hardly any documentation for TF on Hugginface models..So maybe we will learn a few new things while trying to figure out the APIs","metadata":{}},{"cell_type":"code","source":"bert_layer.config","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"That was easy..The vocab size is 32K just like the English equivalent. In particular, I wanted the token dictionary to find out if there are English words. There is no documentation on the net for the same but Huggingface APIs are nice and standard. ","metadata":{}},{"cell_type":"code","source":"import random\n\nrndtokens=[]\nfor item in tokenizer.vocab:\n    rndtokens.append(item) if (len(rndtokens) < 320) & (random.randint(0,100)<2) else None\n    \nstr(rndtokens)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see a bunch of characters from diff languages including Chinese, HJindi, possibly Japanese Katakana. We can see a few English words here and there but not lots. Mainly it is Indonesian. Let us check the English words present in the retail domain space","metadata":{}},{"cell_type":"code","source":"[item for (item) in tokenizer.vocab if item in ['organic','best','product','sale','woman', 'shirt','jeans','Original','original', 'gloves', 'hat']]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Interestingly, There are some English words but many are missing.","metadata":{}},{"cell_type":"code","source":"eng_in_indobert = []\n\nfor item in tokenizer.vocab:\n    if len(item)>3 and item.lower() in setofwords:\n        eng_in_indobert.append(item)\n        \nprint(len(eng_in_indobert),'\\n')\nprint(eng_in_indobert[:100])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So there is a quite a bit of English already built-in into Indo Bert. So maybe that is the reason it works well. Secondly keep in mind that token vocab file IS NOT MEANT to be something like a dictionary. BERT works by splitting quite a few words into tokens. So there could be many words that are not present in the token file and this is perfectly fine. But we just wanted to get an idea if Indo BERT has any english tokens at all and hence the peek.\n\nNow that we have established that Indo BERT understands English to a reasonable extent, is there any benefit in this dictionary. I believe so. It may work better by translating the English words to Indonesean using the dictionary above and then using thje Indo BERT.\n\nBut why would someone want to use an Indo BERT or an English BERT when we have MBERT from google and an XLMBERT from Facebook which is more comprehensive. These are trained on 100+ languages and will perform far better. XLMBERT can be loaded using the same 2-3 lines of Hugginface APIs and if you print the length of the token file it comes to a whopping 250000 or about 10 times the token for the English or the Indo Bert. To me, that seems like a good choice to start experimenting with.\n\nBut is this dictonary we created a waste? Absolutely not. We learnt many things and had fun (and some frustrations) while creating this. I didnt see any such good dictionary of 2000+ Indo to English words on the web, so this could be a useful resource for NLP in general outside this comp. We created a neat real-time translator using mBart-50 which has no limits to the translations which can be done. Now coming to this comp - if for some reason the XLM-Roberta is eating memory and one needs a lighter BERT, one can use the above translations. More importantly these can be used to augment the data in the sparse label groups.\n\nOne can also experiment with https://pypi.org/project/trankit/. I discovered this gem a bit late and couldnt cover it, but it uses the XMLRoberta base, so it should be at least as good as mbart50, though I doubt whether it will reach Google translate standard.","metadata":{}},{"cell_type":"code","source":"##Please ignore\nfrom shutil import copyfile\ncopyfile(src = \"../input/tokenization-mbart50-fastpy/tokenization_mbart50_fast.py\", dst = \"../working/tokenization_mbart50_fast.py\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}