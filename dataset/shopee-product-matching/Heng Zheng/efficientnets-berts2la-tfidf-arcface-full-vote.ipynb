{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install ../input/shopee-external-models/Keras_Applications-1.0.8-py3-none-any.whl\n!pip install ../input/shopee-external-models/efficientnet-1.1.0-py3-none-any.whl\n\nimport numpy as np\nimport pandas as pd\nimport gc\nfrom collections import Counter\nimport matplotlib.pyplot as plt\nimport cudf\nimport cuml\nimport cupy\nfrom cuml.feature_extraction.text import TfidfVectorizer\nfrom cuml import PCA\nfrom cuml.neighbors import NearestNeighbors\nimport tensorflow as tf\nimport efficientnet.tfkeras as efn\nfrom tqdm.autonotebook import tqdm\nimport math\nfrom sklearn.preprocessing import LabelEncoder\nimport tensorflow_hub as hub","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Config","metadata":{}},{"cell_type":"code","source":"model_type_threshold = {\n    'b0': 3.3,\n    'b1': 3.4,\n    'b2': 3.5,\n    'b3': 3.7,\n    'b4': 3.8,\n    'b5': 3.9,\n    'b6': 4.0,\n    'b7': 3.9,\n    'bert1': 16.0,\n    'bert2': 14.0,\n    'bert3': 8.0,\n    'bert4': 9.0,\n}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# For tf.dataset\nAUTO = tf.data.experimental.AUTOTUNE\n\n# Configuration\nBATCH_SIZE = 8\nIMAGE_SIZE = [512, 512]\n# Seed\nSEED = 42\n# Verbosity\nVERBOSE = 1\n# Number of classes\nN_CLASSES = 11014","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# RESTRICT TENSORFLOW TO 2GB OF GPU RAM\n# SO THAT WE HAVE 14GB RAM FOR RAPIDS\nLIMIT = 2.0\ngpus = tf.config.experimental.list_physical_devices('GPU')\nif gpus:\n    try:\n        tf.config.experimental.set_virtual_device_configuration(\n            gpus[0],\n            [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=1024*LIMIT)])\n        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n        #print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n    except RuntimeError as e:\n        print(e)\nprint('We will restrict TensorFlow to max %iGB GPU RAM'%LIMIT)\nprint('then RAPIDS can use %iGB GPU RAM'%(16-LIMIT))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Flag to get cv score\nGET_CV = True\n# Flag to check ram allocations (debug)\nCHECK_SUB = False","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\n# Dataset for images\n\ndf = pd.read_csv('../input/shopee-product-matching/test.csv')\n# If we are comitting, replace train set for test set and dont get cv\nif len(df) > 3:\n    GET_CV = False\ndel df\n\n# Function to get our f1 score\ndef f1_score(y_true, y_pred):\n    y_true = y_true.apply(lambda x: set(x.split()))\n    y_pred = y_pred.apply(lambda x: set(x.split()))\n    intersection = np.array([len(x[0] & x[1]) for x in zip(y_true, y_pred)])\n    len_y_pred = y_pred.apply(lambda x: len(x)).values\n    len_y_true = y_true.apply(lambda x: len(x)).values\n    f1 = 2 * intersection / (len_y_pred + len_y_true)\n    return f1\n\n\n# # Function to combine predictions\n# def combine_predictions(row):\n#     x = np.concatenate([\n#         row['image_predictions_b3'], \n#         row['image_predictions_b4'], \n#         row['text_predictions_bert1'], \n#         row['text_predictions_bert2'],\n#         row['text_predictions_tfidf']\n#     ])\n#     return ' '.join( np.unique(x) )\n\n# Function to voting predictions\ndef voter(row):\n    image_ret = list()\n    x = np.concatenate([\n#         row['image_predictions_b0'], \n#         row['image_predictions_b1'],\n#         row['image_predictions_b2'], \n        row['image_predictions_b3'],\n        row['image_predictions_b4'],\n        row['image_predictions_b5'],\n        row['image_predictions_b6'],\n#         row['image_predictions_b7']\n    ])\n    counter = Counter(x)\n    for (item, value) in counter.most_common():\n        if value == 4:\n            image_ret.append(item)\n            \n    text_ret = list()\n    x = np.concatenate([\n#         row['text_predictions_bert1'], \n#         row['text_predictions_bert2'],\n        row['text_predictions_bert3'], \n        row['text_predictions_bert4'],\n    ])\n    counter = Counter(x)\n    for (item, value) in counter.most_common():\n        if value == 2:\n            text_ret.append(item)\n            \n    image_arr = np.array(image_ret)\n    text_arr = np.array(text_ret)\n    \n    res = np.concatenate([image_arr,\n                          text_arr,\n                          row['text_predictions_tfidf']])\n    return ' '.join( np.unique(res) )\n\n# Function to read out dataset\ndef read_dataset():\n    if GET_CV:\n        df = pd.read_csv('../input/shopee-product-matching/train.csv')\n        tmp = df.groupby(['label_group'])['posting_id'].unique().to_dict()\n        df['matches'] = df['label_group'].map(tmp)\n        df['matches'] = df['matches'].apply(lambda x: ' '.join(x))\n        if CHECK_SUB:\n            df = pd.concat([df, df], axis = 0)\n            df.reset_index(drop = True, inplace = True)\n        df_cu = cudf.DataFrame(df)\n        image_paths = '../input/shopee-product-matching/train_images/' + df['image']\n    else:\n        df = pd.read_csv('../input/shopee-product-matching/test.csv')\n        df_cu = cudf.DataFrame(df)\n        image_paths = '../input/shopee-product-matching/test_images/' + df['image']\n        \n    return df, df_cu, image_paths\n\n# Function to decode our images\ndef decode_image(image_data):\n    image = tf.image.decode_jpeg(image_data, channels = 3)\n    image = tf.image.resize(image, IMAGE_SIZE)\n    image = tf.cast(image, tf.float32) / 255.0\n    return image\n\n# Function to read our test image and return image\ndef read_image(image):\n    image = tf.io.read_file(image)\n    image = decode_image(image)\n    return image\n\n# Function to get our dataset that read images\ndef get_dataset(image):\n    dataset = tf.data.Dataset.from_tensor_slices(image)\n    dataset = dataset.map(read_image, num_parallel_calls = AUTO)\n    dataset = dataset.batch(BATCH_SIZE)\n    dataset = dataset.prefetch(AUTO)\n    return dataset\n\n\ndf, df_cu, image_paths = read_dataset()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\n# GET IMAGE EMBEDDINGS\n\n# Arcmarginproduct class keras layer\nclass ArcMarginProduct(tf.keras.layers.Layer):\n    '''\n    Implements large margin arc distance.\n\n    Reference:\n        https://arxiv.org/pdf/1801.07698.pdf\n        https://github.com/lyakaap/Landmark2019-1st-and-3rd-Place-Solution/\n            blob/master/src/modeling/metric_learning.py\n    '''\n    def __init__(self, n_classes, s=30, m=0.50, easy_margin=False,\n                 ls_eps=0.0, **kwargs):\n\n        super(ArcMarginProduct, self).__init__(**kwargs)\n\n        self.n_classes = n_classes\n        self.s = s\n        self.m = m\n        self.ls_eps = ls_eps\n        self.easy_margin = easy_margin\n        self.cos_m = tf.math.cos(m)\n        self.sin_m = tf.math.sin(m)\n        self.th = tf.math.cos(math.pi - m)\n        self.mm = tf.math.sin(math.pi - m) * m\n\n    def get_config(self):\n\n        config = super().get_config().copy()\n        config.update({\n            'n_classes': self.n_classes,\n            's': self.s,\n            'm': self.m,\n            'ls_eps': self.ls_eps,\n            'easy_margin': self.easy_margin,\n        })\n        return config\n\n    def build(self, input_shape):\n        super(ArcMarginProduct, self).build(input_shape[0])\n\n        self.W = self.add_weight(\n            name='W',\n            shape=(int(input_shape[0][-1]), self.n_classes),\n            initializer='glorot_uniform',\n            dtype='float32',\n            trainable=True,\n            regularizer=None)\n\n    def call(self, inputs):\n        X, y = inputs\n        y = tf.cast(y, dtype=tf.int32)\n        cosine = tf.matmul(\n            tf.math.l2_normalize(X, axis=1),\n            tf.math.l2_normalize(self.W, axis=0)\n        )\n        sine = tf.math.sqrt(1.0 - tf.math.pow(cosine, 2))\n        phi = cosine * self.cos_m - sine * self.sin_m\n        if self.easy_margin:\n            phi = tf.where(cosine > 0, phi, cosine)\n        else:\n            phi = tf.where(cosine > self.th, phi, cosine - self.mm)\n        one_hot = tf.cast(\n            tf.one_hot(y, depth=self.n_classes),\n            dtype=cosine.dtype\n        )\n        if self.ls_eps > 0:\n            one_hot = (1 - self.ls_eps) * one_hot + self.ls_eps / self.n_classes\n\n        output = (one_hot * phi) + ((1.0 - one_hot) * cosine)\n        output *= self.s\n        return output\n\n\n# Function to get the embeddings of our images with the fine-tuned model\ndef get_image_embeddings(image_paths, \n                         model_type = 'b3',\n                         model_path = '../input/shopee-efficientnetb3-arcmarginproduct-full/EfficientNetB3_M0.5_512_1024.h5'):\n    embeds = []\n    \n    margin = ArcMarginProduct(\n            n_classes = N_CLASSES, \n            s = 30, \n            m = 0.5, \n            name='head/arc_margin', \n            dtype='float32'\n            )\n\n    inp = tf.keras.layers.Input(shape = (*IMAGE_SIZE, 3), name = 'inp1')\n    label = tf.keras.layers.Input(shape = (), name = 'inp2')\n    if model_type == 'b7':\n        x = efn.EfficientNetB7(weights = None, include_top = False)(inp)\n    elif model_type == 'b6':\n        x = efn.EfficientNetB6(weights = None, include_top = False)(inp)\n    elif model_type == 'b5':\n        x = efn.EfficientNetB5(weights = None, include_top = False)(inp)\n    elif model_type == 'b4':\n        x = efn.EfficientNetB4(weights = None, include_top = False)(inp)\n    elif model_type == 'b3':\n        x = efn.EfficientNetB3(weights = None, include_top = False)(inp)\n    elif model_type == 'b2':\n        x = efn.EfficientNetB2(weights = None, include_top = False)(inp)\n    elif model_type == 'b1':\n        x = efn.EfficientNetB1(weights = None, include_top = False)(inp)\n    elif model_type == 'b0':\n        x = efn.EfficientNetB0(weights = None, include_top = False)(inp)\n    else:\n        x = efn.EfficientNetB0(weights = None, include_top = False)(inp)\n    x = tf.keras.layers.GlobalAveragePooling2D()(x)\n    x = margin([x, label])\n        \n    output = tf.keras.layers.Softmax(dtype='float32')(x)\n\n    model = tf.keras.models.Model(inputs = [inp, label], outputs = [output])\n    model.load_weights(model_path)\n    model = tf.keras.models.Model(inputs = model.input[0], outputs = model.layers[-4].output)\n    chunk = 5000\n    iterator = np.arange(np.ceil(len(df) / chunk))\n    for j in iterator:\n        a = int(j * chunk)\n        b = int((j + 1) * chunk)\n        image_dataset = get_dataset(image_paths[a:b])\n        image_embeddings = model.predict(image_dataset)\n        embeds.append(image_embeddings)\n    del model\n    image_embeddings = np.concatenate(embeds)\n    print(f'Our image embeddings shape is {image_embeddings.shape}')\n    del embeds\n    gc.collect()\n    return image_embeddings\n\n# image_embeddings_b7 = get_image_embeddings(image_paths,\n#                                            model_type = 'b7',\n#                                            model_path = '../input/shopee-efficientnetb7-arcmarginproduct-full/EfficientNetB7_M0.5_512_1029.h5')\nimage_embeddings_b6 = get_image_embeddings(image_paths,\n                                           model_type = 'b6',\n                                           model_path = '../input/shopee-efficientnetb6-arcmarginproduct-full/EfficientNetB6_M0.5_512_520.h5')\nimage_embeddings_b5 = get_image_embeddings(image_paths,\n                                           model_type = 'b5',\n                                           model_path = '../input/shopee-efficientnetb5-arcmarginproduct-full/EfficientNetB5_M0.5_512_1314.h5')\nimage_embeddings_b4 = get_image_embeddings(image_paths,\n                                           model_type = 'b4',\n                                           model_path = '../input/shopee-efficientnetb4-arcmarginproduct-full/EfficientNetB4_M0.5_512_2021.h5')\nimage_embeddings_b3 = get_image_embeddings(image_paths,\n                                           model_type = 'b3',\n                                           model_path = '../input/shopee-efficientnetb3-arcmarginproduct-full/EfficientNetB3_M0.5_512_1024.h5')\n# image_embeddings_b2 = get_image_embeddings(image_paths,\n#                                            model_type = 'b2',\n#                                            model_path = '../input/shopee-efficientnetb2-arcmarginproduct-full/EfficientNetB2_M0.5_512_666.h5')\n# image_embeddings_b1 = get_image_embeddings(image_paths,\n#                                            model_type = 'b1',\n#                                            model_path = '../input/shopee-efficientnetb1-arcmarginproduct-full/EfficientNetB1_M0.5_512_42.h5')\n# image_embeddings_b0 = get_image_embeddings(image_paths,\n#                                            model_type = 'b0',\n#                                            model_path = '../input/shopee-efficientnetb0-arcmarginproduct-full/EfficientNetB0_M0.5_512_777.h5')\n\n\ngc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport transformers\nfrom transformers import (BertTokenizer, BertModel,\n                          DistilBertTokenizer, DistilBertModel)\n\n# For pytorch bert\nclass CFG:\n    N_CLASSES = 11014\n    bert_hidden_size = 768\n    batch_size = 64\n    num_workers = 1\n    max_length = 60\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else 'cpu')\n    \n    \nclass ArcMarginProduct(nn.Module):\n    r\"\"\"Implement of large margin arc distance: :\n        Args:\n            in_features: size of each input sample\n            out_features: size of each output sample\n            s: norm of input feature\n            m: margin\n            cos(theta + m)\n        \"\"\"\n    def __init__(self, in_features, out_features, s=30.0, m=0.50, easy_margin=False):\n        super(ArcMarginProduct, self).__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.s = s\n        self.m = m\n        self.weight = nn.Parameter(torch.FloatTensor(out_features, in_features))\n        nn.init.xavier_uniform_(self.weight)\n\n        self.easy_margin = easy_margin\n        self.cos_m = math.cos(m)\n        self.sin_m = math.sin(m)\n        self.th = math.cos(math.pi - m)\n        self.mm = math.sin(math.pi - m) * m\n\n    def forward(self, input, label):\n        # --------------------------- cos(theta) & phi(theta) ---------------------------\n        cosine = F.linear(F.normalize(input), F.normalize(self.weight))\n        sine = torch.sqrt((1.0 - torch.pow(cosine, 2)).clamp(0, 1))\n        phi = cosine * self.cos_m - sine * self.sin_m\n        if self.easy_margin:\n            phi = torch.where(cosine > 0, phi, cosine)\n        else:\n            phi = torch.where(cosine > self.th, phi, cosine - self.mm)\n        # --------------------------- convert label to one-hot ---------------------------\n        # one_hot = torch.zeros(cosine.size(), requires_grad=True, device='cuda')\n        one_hot = torch.zeros(cosine.size(), device=CFG.device)\n        one_hot.scatter_(1, label.view(-1, 1).long(), 1)\n        # -------------torch.where(out_i = {x_i if condition_i else y_i) -------------\n        output = (one_hot * phi) + ((1.0 - one_hot) * cosine)  # you can use torch.where if your torch.__version__ is 0.4\n        output *= self.s\n        # print(output)\n\n        return output\n    \n    \nclass Model(nn.Module):\n    def __init__(self, \n                 bert_model, \n                 num_classes=CFG.N_CLASSES, \n                 last_hidden_size=CFG.bert_hidden_size):\n        \n        super().__init__()\n        self.bert_model = bert_model\n        self.arc_margin = ArcMarginProduct(last_hidden_size, \n                                           num_classes, \n                                           s=30.0, \n                                           m=0.50, \n                                           easy_margin=False)\n    \n    def get_bert_features(self, batch):\n        output = self.bert_model(input_ids=batch['input_ids'], attention_mask=batch['attention_mask'])\n        last_hidden_state = output.last_hidden_state # shape: (batch_size, seq_length, bert_hidden_dim)\n        CLS_token_state = last_hidden_state[:, 0, :] # obtaining CLS token state which is the first token.\n        return CLS_token_state\n    \n    def get_first_last_avg(self, batch):\n        output = self.bert_model(input_ids=batch['input_ids'], attention_mask=batch['attention_mask'])\n        last_hidden_state = output.last_hidden_state\n        first_hidden_state = output.hidden_states[1]\n        CLS_token_state = (last_hidden_state[:, 0, :] + first_hidden_state[:, 0, :]) / 2\n        return CLS_token_state\n    \n    def forward(self, batch):\n        CLS_hidden_state = self.get_bert_features(batch)\n        output = self.arc_margin(CLS_hidden_state, batch['labels'])\n        return output","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Dataset for text\n\nclass TextDataset(torch.utils.data.Dataset):\n    def __init__(self, dataframe, tokenizer, mode=\"train\", max_length=None):\n        self.dataframe = dataframe\n        if mode != \"test\":\n            lbl_encoder = LabelEncoder()\n            dataframe['label_code'] = lbl_encoder.fit_transform(dataframe['label_group'])\n            self.targets = dataframe['label_code'].values\n        texts = list(dataframe['title'].apply(lambda o: str(o)).values)\n        self.encodings = tokenizer(texts, \n                                   padding=True, \n                                   truncation=True, \n                                   max_length=max_length)\n        self.mode = mode\n        \n        \n    def __getitem__(self, idx):\n        # putting each tensor in front of the corresponding key from the tokenizer\n        # HuggingFace tokenizers give you whatever you need to feed to the corresponding model\n        item = {key: torch.tensor(values[idx]) for key, values in self.encodings.items()}\n        # when testing, there are no targets so we won't do the following\n        if self.mode != \"test\":\n            item['labels'] = torch.tensor(self.targets[idx]).long()\n        return item\n    \n    def __len__(self):\n        return len(self.dataframe)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_text_embeddings(model_type = 'distil',\n                        model_name = '../input/distilbert-base-indonesian', \n                        model_path = '../input/indonesian-distilbert-finetuning-with-arcmargin/final.pt',\n                        emb_type = 'last'):\n    \n    if model_type == 'distil':\n        tokenizer = DistilBertTokenizer.from_pretrained(model_name)\n        bert_model = DistilBertModel.from_pretrained(model_name, output_hidden_states=True)\n    else:\n        tokenizer = BertTokenizer.from_pretrained(model_name)\n        bert_model = BertModel.from_pretrained(model_name, output_hidden_states=True)\n    \n    if GET_CV:\n        dataset = TextDataset(df, tokenizer, mode='train', max_length=CFG.max_length)\n    else:\n        dataset = TextDataset(df, tokenizer, mode='test', max_length=CFG.max_length)\n\n    dataloader = torch.utils.data.DataLoader(dataset, \n                                             batch_size=CFG.batch_size, \n                                             num_workers=CFG.num_workers, \n                                             shuffle=False)\n    batch = next(iter(dataloader))\n    print(batch['input_ids'].shape)\n\n    model = Model(bert_model).to(CFG.device)\n    model.load_state_dict(torch.load(model_path, map_location=CFG.device))\n    model.eval()\n    \n    embeds = []\n    with torch.no_grad():\n        for batch in tqdm(dataloader, total=len(dataloader)):\n            batch = {k: v.to(CFG.device) for k, v in batch.items()}\n            if emb_type == 'last':\n                text_embeddings = model.get_bert_features(batch)\n            elif emb_type == 'first_last_avg':\n                text_embeddings = model.get_first_last_avg(batch)\n            else:\n                text_embeddings = model.get_bert_features(batch)\n            embeds.append(text_embeddings.cpu())\n    del model\n    text_embeddings = np.concatenate(embeds)\n    print(f'Our text embeddings shape is {text_embeddings.shape}')\n    del embeds\n    gc.collect()\n    \n    return text_embeddings","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\n# text_embeddings_bert1 = get_text_embeddings(\n#     model_type = 'bert',\n#     model_name = '../input/bert-base-indonesian-522m',\n#     model_path = '../input/indonesian-bert-finetuning-with-arcmargin-full/final.pt',\n#     emb_type = 'last'\n# )\n\n# text_embeddings_bert2 = get_text_embeddings(\n#     model_type = 'distil',\n#     model_name = '../input/distilbert-base-indonesian',                                  \n#     model_path = '../input/indonesian-distilbert-finetuning-with-arcmargin/final.pt',\n#     emb_type = 'last'\n# )\n\ntext_embeddings_bert3 = get_text_embeddings(\n    model_type = 'bert',\n    model_name = '../input/bert-base-indonesian-522m',\n    model_path = '../input/indonesian-bert-finetuning-with-arcmargin-full/final.pt',\n    emb_type = 'first_last_avg'\n)\n\ntext_embeddings_bert4 = get_text_embeddings(\n    model_type = 'distil',\n    model_name = '../input/distilbert-base-indonesian',                                  \n    model_path = '../input/indonesian-distilbert-finetuning-with-arcmargin/final.pt',\n    emb_type = 'first_last_avg'\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# TF-IDF\n\nfrom cuml.feature_extraction.text import TfidfVectorizer\n\nmodel = TfidfVectorizer(stop_words=None, binary=True, max_features=25000)\ntext_embeddings2 = model.fit_transform(df_cu.title).toarray()\nprint('text embeddings shape',text_embeddings2.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds = []\nCHUNK = 1024*4\n\nprint('Finding similar titles...')\nCTS = len(df_cu)//CHUNK\nif len(df_cu)%CHUNK!=0: CTS += 1\nfor j in range( CTS ):\n    \n    a = j*CHUNK\n    b = (j+1)*CHUNK\n    b = min(b,len(df_cu))\n    print('chunk',a,'to', b)\n    \n    # COSINE SIMILARITY DISTANCE\n    # cts = np.dot( text_embeddings, text_embeddings[a:b].T).T\n    cts = cupy.matmul(text_embeddings2, text_embeddings2[a:b].T).T\n    \n    for k in range(b-a):\n        # IDX = np.where(cts[k,]>0.7)[0]\n        IDX = cupy.where(cts[k,]>0.775)[0]\n        o = df_cu.iloc[cupy.asnumpy(IDX)].posting_id.to_pandas().values\n        preds.append(o)\n        \ndel model, text_embeddings2\n\ndf_cu['oof_text'] = preds","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function to get 50 nearest neighbors of each image and apply a distance threshold to maximize cv\ndef get_neighbors(df, embeddings, KNN = 50, model_type = 'b3'):\n    model = NearestNeighbors(n_neighbors = KNN)\n    model.fit(embeddings)\n    distances, indices = model.kneighbors(embeddings)\n    \n    # Iterate through different thresholds to maximize cv, run this in interactive mode, then replace else clause with a solid threshold\n    if GET_CV:\n        if model_type in ['b0', 'b1', 'b2', 'b3', 'b4', 'b5', 'b6', 'b7']:\n            thresholds = list(np.arange(3.0, 5.0, 0.1))\n        elif model_type in ['bert1', 'bert2']:\n            thresholds = list(np.arange(15, 35, 1))\n        else:\n            thresholds = list(np.arange(6, 18, 1))\n        scores = []\n        for threshold in thresholds:\n            predictions = []\n            for k in range(embeddings.shape[0]):\n                idx = np.where(distances[k,] < threshold)[0]\n                ids = indices[k,idx]\n                posting_ids = ' '.join(df['posting_id'].iloc[ids].values)\n                predictions.append(posting_ids)\n            df['pred_matches'] = predictions\n            df['f1'] = f1_score(df['matches'], df['pred_matches'])\n            score = df['f1'].mean()\n            print(f'Our f1 score for threshold {threshold} is {score}')\n            scores.append(score)\n        thresholds_scores = pd.DataFrame({'thresholds': thresholds, 'scores': scores})\n        max_score = thresholds_scores[thresholds_scores['scores'] == thresholds_scores['scores'].max()]\n        best_threshold = max_score['thresholds'].values[0]\n        best_score = max_score['scores'].values[0]\n        print(f'Our best score is {best_score} and has a threshold {best_threshold}')\n        \n        # Use threshold\n        predictions = []\n        for k in range(embeddings.shape[0]):\n            # Because we are predicting the test set that have 70K images and different label groups, confidence should be smaller\n            if model_type in model_type_threshold:\n                idx = np.where(distances[k,] < model_type_threshold[model_type])[0]\n            else:\n                idx = np.where(distances[k,] < 20.0)[0]\n            ids = indices[k,idx]\n            posting_ids = df['posting_id'].iloc[ids].values\n            predictions.append(posting_ids)\n    \n    # Because we are predicting the test set that have 70K images and different label groups, confidence should be smaller\n    else:\n        predictions = []\n        for k in tqdm(range(embeddings.shape[0])):\n            if model_type in model_type_threshold:\n                idx = np.where(distances[k,] < model_type_threshold[model_type])[0]\n            else:\n                idx = np.where(distances[k,] < 20.0)[0]\n            ids = indices[k,idx]\n            posting_ids = df['posting_id'].iloc[ids].values\n            predictions.append(posting_ids)\n        \n    del model, distances, indices\n    gc.collect()\n    return df, predictions","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get neighbors for image_embeddings\n# df, image_predictions_b7 = get_neighbors(df, image_embeddings_b7, KNN = 100, model_type = 'b7')\ndf, image_predictions_b6 = get_neighbors(df, image_embeddings_b6, KNN = 100, model_type = 'b6')\ndf, image_predictions_b5 = get_neighbors(df, image_embeddings_b5, KNN = 100, model_type = 'b5')\ndf, image_predictions_b4 = get_neighbors(df, image_embeddings_b4, KNN = 100, model_type = 'b4')\ndf, image_predictions_b3 = get_neighbors(df, image_embeddings_b3, KNN = 100, model_type = 'b3')\n# df, image_predictions_b2 = get_neighbors(df, image_embeddings_b2, KNN = 100, model_type = 'b2')\n# df, image_predictions_b1 = get_neighbors(df, image_embeddings_b1, KNN = 100, model_type = 'b1')\n# df, image_predictions_b0 = get_neighbors(df, image_embeddings_b0, KNN = 100, model_type = 'b0')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get neighbors for text_embeddings\n# df, text_predictions_bert1 = get_neighbors(df, text_embeddings_bert1, KNN = 100, model_type = 'bert1')\n# df, text_predictions_bert2 = get_neighbors(df, text_embeddings_bert2, KNN = 100, model_type = 'bert2')\ndf, text_predictions_bert3 = get_neighbors(df, text_embeddings_bert3, KNN = 100, model_type = 'bert3')\ndf, text_predictions_bert4 = get_neighbors(df, text_embeddings_bert4, KNN = 100, model_type = 'bert4')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Concatenate image predctions with text predictions\nif GET_CV:\n#     df['image_predictions_b0'] = image_predictions_b0\n#     df['image_predictions_b1'] = image_predictions_b1\n#     df['image_predictions_b2'] = image_predictions_b2\n    df['image_predictions_b3'] = image_predictions_b3\n    df['image_predictions_b4'] = image_predictions_b4\n    df['image_predictions_b5'] = image_predictions_b5\n    df['image_predictions_b6'] = image_predictions_b6\n#     df['image_predictions_b7'] = image_predictions_b7\n#     df['text_predictions_bert1'] = text_predictions_bert1\n#     df['text_predictions_bert2'] = text_predictions_bert2\n    df['text_predictions_bert3'] = text_predictions_bert3\n    df['text_predictions_bert4'] = text_predictions_bert4\n    df['text_predictions_tfidf'] = df_cu['oof_text'].to_pandas().values\n    df['pred_matches'] = df.apply(voter, axis = 1)\n    df['f1'] = f1_score(df['matches'], df['pred_matches'])\n    score = df['f1'].mean()\n    print(f'Our final f1 cv score is {score}')\n    df['matches'] = df['pred_matches']\n    df[['posting_id', 'matches']].to_csv('submission.csv', index = False)\nelse:\n#     df['image_predictions_b0'] = image_predictions_b0\n#     df['image_predictions_b1'] = image_predictions_b1\n#     df['image_predictions_b2'] = image_predictions_b2\n    df['image_predictions_b3'] = image_predictions_b3\n    df['image_predictions_b4'] = image_predictions_b4\n    df['image_predictions_b5'] = image_predictions_b5\n    df['image_predictions_b6'] = image_predictions_b6\n#     df['image_predictions_b7'] = image_predictions_b7\n#     df['text_predictions_bert1'] = text_predictions_bert1\n#     df['text_predictions_bert2'] = text_predictions_bert2\n    df['text_predictions_bert3'] = text_predictions_bert3\n    df['text_predictions_bert4'] = text_predictions_bert4\n    df['text_predictions_tfidf'] = df_cu['oof_text'].to_pandas().values\n    df['matches'] = df.apply(voter, axis = 1)\n    df[['posting_id', 'matches']].to_csv('submission.csv', index = False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tmp = df[['matches']].copy()\ntmp['item_count'] = tmp['matches'].apply(lambda x: len(x.split()))\ntmp['item_count'].describe()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tmp[tmp['item_count'] == 1].shape","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!head submission.csv","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}