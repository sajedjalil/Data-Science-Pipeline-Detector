{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport collections\nimport gc\nfrom tqdm import tqdm\nimport cv2\nimport cudf, cuml, cupy\nfrom cuml.neighbors import NearestNeighbors\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\n\nimport transformers\nfrom transformers import DistilBertTokenizer, DistilBertModel, DistilBertConfig\n\n\nimport sys\nsys.path.append('../input/timm-shpee/pytorch-image-models-master')\nimport timm\nfrom timm.models.layers import SelectAdaptivePool2d\n\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","papermill":{"duration":11.228015,"end_time":"2021-05-04T18:22:33.326014","exception":false,"start_time":"2021-05-04T18:22:22.097999","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"COMPUTE_CV = False","metadata":{"papermill":{"duration":0.031064,"end_time":"2021-05-04T18:22:33.379558","exception":false,"start_time":"2021-05-04T18:22:33.348494","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cos_threshs = np.array([0.16])\ntest_batch_size = 128\n\n# image model\nGEM_P = 4\nimage_size = 420\n\n# TTA for image(do not use)\nflip_TTAs = [False, False, False, False, False]\ntesting_scales = [[1.0], [1.0], [1.0], [1.0], [1.0]]\n\n# text model\ntext_max_length = 84\n\n# alpha query expansion\nalpha_query_expansion = True\nqe_mid_knn = True\nqe_ms     = [[1, 1], [1, 1], [1, 1], [2, 1], [1, 1]]\nqe_alphas = [[2, 5], [2, 7], [5, 2], [7, 2], [3, 3]]\n\n# adaptive thresholding\nUSE_ADAPTIVE_THRESHOLDING = False\nCONSERVATIVENESS = 1.0\nBETA = np.mean([0.9, 0.8, 0.9, 0.75, 0.3])\n\n# min num preds\nforce_2preds = True\nforce_2preds_relax = 1.2\n\n# kNN\nKNN = 52\nALPHA_QE_KNN = 8\nknn_metric = 'cosine' # cosine or correlation","metadata":{"papermill":{"duration":0.036006,"end_time":"2021-05-04T18:22:33.436116","exception":false,"start_time":"2021-05-04T18:22:33.40011","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_weight_paths = [\n    '../input/joint-f0gem3420768-dbert-highway/Joint_F0GeM3-420-768Emb-1024XBM_DBERT-Aug0.1_XBM4096_fold0_HighwayConc768x3_ep-002_f1-0.88579_thresh-0.43448_bs-30_emb-2304.pth',\n    '../input/joint-f0gem3420768-dbert-highway/Joint_F0GeM3-420-768Emb-1024XBM_DBERT-Aug0.1_XBM4096_fold1_HighwayConc768x3_ep-002_f1-0.89122_thresh-0.42069_bs-32_emb-2304.pth',\n    '../input/joint-f0gem3420768-dbert-highway/Joint_F0GeM3-420-768Emb-1024XBM_DBERT-Aug0.1_XBM4096_fold2_HighwayConc768x3_ep-003_f1-0.88911_thresh-0.44828_bs-30_emb-2304.pth',\n    '../input/joint-f0gem3420768-dbert-highway/Joint_F0GeM3-420-768Emb-1024XBM_DBERT-Aug0.1_XBM4096_fold3_HighwayConc768x3_ep-002_f1-0.88196_thresh-0.44828_bs-32_emb-2304.pth',\n    '../input/joint-f0gem3420768-dbert-highway/Joint_F0GeM3-420-768Emb-1024XBM_DBERT-Aug0.1_XBM4096_fold4_HighwayConc768x3_ep-003_f1-0.88931_thresh-0.44828_bs-32_emb-2304.pth'\n                     ]\ntokenizer = DistilBertTokenizer.from_pretrained('../input/distilberttextaugmadgrad5folds/distil-bert-textaug-madgrad-5folds/tokenizer')\n\n\n# general\nloader_num_workers = 2\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else 'cpu')","metadata":{"papermill":{"duration":0.114995,"end_time":"2021-05-04T18:22:33.573429","exception":false,"start_time":"2021-05-04T18:22:33.458434","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# load data","metadata":{"papermill":{"duration":0.022073,"end_time":"2021-05-04T18:22:33.618159","exception":false,"start_time":"2021-05-04T18:22:33.596086","status":"completed"},"tags":[]}},{"cell_type":"code","source":"if COMPUTE_CV:\n    test = pd.read_csv('../input/shopee-product-matching/train.csv').iloc[0:300]\nelse:\n    test = pd.read_csv('../input/shopee-product-matching/test.csv')\ntest = test.drop(columns='image_phash')\n\nLEN_TEST = len(test)\n\nBASE = '../input/shopee-product-matching/test_images/'\nif COMPUTE_CV:\n    BASE = '../input/shopee-product-matching/train_images/'\n    \nCHUNK = 1024*4\nCTS = LEN_TEST//CHUNK\nif LEN_TEST%CHUNK!=0:\n    CTS += 1\n    \nif LEN_TEST==3:\n    KNN = 3\n    ALPHA_QE_KNN = 3\n    qe_ms     = [[1, 1], [1, 1], [1, 1], [1, 1], [1, 1]]\n    qe_alphas = [[1, 1], [1, 1], [1, 1], [1, 1], [1, 1]]","metadata":{"papermill":{"duration":0.050474,"end_time":"2021-05-04T18:22:33.690928","exception":false,"start_time":"2021-05-04T18:22:33.640454","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# dataset","metadata":{"papermill":{"duration":0.02539,"end_time":"2021-05-04T18:22:33.847485","exception":false,"start_time":"2021-05-04T18:22:33.822095","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# tokenize titles\ntexts = list(test['title'].apply(lambda o: str(o)).values)\ntext_encodings = tokenizer(texts, \n                           padding=True, \n                           truncation=True, \n                           max_length=text_max_length)\n\ntest['input_ids'] = text_encodings['input_ids']\ntest['attention_mask'] = text_encodings['attention_mask']\n\ndel texts, text_encodings, tokenizer\n_=gc.collect()","metadata":{"papermill":{"duration":0.043863,"end_time":"2021-05-04T18:22:33.921606","exception":false,"start_time":"2021-05-04T18:22:33.877743","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Shopee(Dataset):\n    def __init__(self, df, image_dir, augs):\n        self.df = df\n        self.augs = augs \n        self.image_dir = image_dir\n\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        item = {'input_ids': torch.tensor(self.df['input_ids'].iloc[idx]), 'attention_mask': torch.tensor(self.df['attention_mask'].iloc[idx])}\n        \n        # image\n        image = cv2.imread(self.image_dir + self.df.loc[idx, 'image']).astype(np.uint8)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        image = self.augs(image=image)['image']\n\n        return image, item\n    \ndef make_aug(scale=1.0, horizontal_flip=False):\n    im_size = int(round(scale*image_size))\n    if horizontal_flip:\n        valid_aug = A.Compose([A.LongestMaxSize(max_size=im_size, p=1.0),\n                               A.PadIfNeeded(min_height=im_size, min_width=im_size, border_mode=0, p=1.0),\n                               A.HorizontalFlip(p=1.0),\n                               A.Normalize(p=1.0),\n                               ToTensorV2(p=1.0)])\n        \n    else:\n        valid_aug = A.Compose([A.LongestMaxSize(max_size=im_size, p=1.0),\n                               A.PadIfNeeded(min_height=im_size, min_width=im_size, border_mode=0, p=1.0),\n                               A.Normalize(p=1.0),\n                               ToTensorV2(p=1.0)])\n        \n    return valid_aug","metadata":{"papermill":{"duration":0.038573,"end_time":"2021-05-04T18:22:33.983465","exception":false,"start_time":"2021-05-04T18:22:33.944892","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# model","metadata":{"papermill":{"duration":0.022325,"end_time":"2021-05-04T18:22:34.027986","exception":false,"start_time":"2021-05-04T18:22:34.005661","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# J3 without joint embeddings(stacked embeddings)\n# joint embeddings give no score(CV and LB) boost, image/text concat embeddings are sufficient\nclass AdaptiveGeneralizedMeanPool2d(nn.Module):\n    def __init__(self, p=3, eps=1e-6):\n        super(AdaptiveGeneralizedMeanPool2d, self).__init__()\n        self.p = p\n        self.eps = eps\n        self.flatten1 = nn.Flatten()\n\n    def forward(self, x):\n        return self.gem(x, p=self.p, eps=self.eps)\n        \n    def gem(self, x, p=3, eps=1e-6):\n        x = F.adaptive_avg_pool2d(input=x.clamp(min=eps).pow(p), output_size=(1, 1)).pow(1./p)\n        x = self.flatten1(x)\n        return x\n\nclass NFNetF0_GeM_L2(nn.Module):\n    def __init__(self, num_embeddings, pretrained=True):\n        super(NFNetF0_GeM_L2, self).__init__()\n        self.model = timm.create_model('dm_nfnet_f0', pretrained=pretrained)\n        self.model.head.global_pool = AdaptiveGeneralizedMeanPool2d(p=GEM_P)\n        num_features = self.model.head.fc.in_features\n        self.model.head.fc = nn.Linear(num_features, num_embeddings)\n\n    def forward(self, x):\n        x = self.model(x)\n        x = F.normalize(x, p=2, dim=1, eps=1e-12)\n        \n        return x\n\nclass DistilBERT_L2(nn.Module):\n    def __init__(self, bert_model):\n        super(DistilBERT_L2, self).__init__()\n        self.bert_model = bert_model\n    \n    def forward(self, batch):\n        output = self.bert_model(input_ids=batch['input_ids'], attention_mask=batch['attention_mask'])\n        last_hidden_state = output.last_hidden_state\n        CLS_token_state = last_hidden_state[:, 0, :]\n        CLS_token_state = F.normalize(CLS_token_state, p=2, dim=1, eps=1e-12)\n        \n        return CLS_token_state\n\nclass JointModel(nn.Module):\n    def __init__(self, text_model, image_model):\n        super(JointModel, self).__init__()\n        self.text_model = text_model\n        self.image_model = image_model\n        \n        # define fc1(embeddings stacking), this won't be used for forward\n        self.fc1 = nn.Linear(768 + 768, 768)\n\n    \n    def forward(self, image_input, batch):\n        # image embeddings\n        image_emb = self.image_model(image_input)\n\n        # CLS_token as text embeddings\n        text_emb = self.text_model(batch)\n        \n        x = torch.cat((image_emb, text_emb), dim=1)\n\n        return x","metadata":{"papermill":{"duration":0.0458,"end_time":"2021-05-04T18:22:34.344883","exception":false,"start_time":"2021-05-04T18:22:34.299083","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image_model = NFNetF0_GeM_L2(768, False)\nbert_config = DistilBertConfig(activation='gelu',\n                               attention_dropout=0.1,\n                               dim=768,\n                               dropout=0.1,\n                               hidden_dim=3072,\n                               initializer_range=0.02,\n                               max_position_embeddings=512,\n                               model_type='distilbert',\n                               n_heads=12,\n                               n_layers=6,\n                               output_hidden_states=True,\n                               pad_token_id=0,\n                               qa_dropout=0.1,\n                               seq_classif_dropout=0.2,\n                               sinusoidal_pos_embds=True,\n                               tie_weights_=True,\n                               vocab_size=32000)\nbert_model = DistilBertModel(bert_config)\ntext_model = DistilBERT_L2(bert_model)\nmodel = JointModel(text_model, image_model)\ndel bert_model, text_model, image_model\n\n_=model.to(device)\n_=model.eval()","metadata":{"papermill":{"duration":12.853412,"end_time":"2021-05-04T18:22:47.28692","exception":false,"start_time":"2021-05-04T18:22:34.433508","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def joint_embedder(df, model, scale=1.0, flip=False):\n    embeds = []\n    CHUNK = 1024*4\n    for i,j in enumerate(range(CTS)):\n        a = j*CHUNK\n        b = (j+1)*CHUNK\n        b = min(b,len(df))\n        \n        test_data = Shopee(df.iloc[a:b].reset_index(drop=True),\n                           BASE,\n                           augs=make_aug(scale=scale, horizontal_flip=flip))\n        test_loader = DataLoader(test_data,\n                                 shuffle=False,\n                                 num_workers=loader_num_workers,\n                                 pin_memory=False,# False:faster\n                                 batch_size=test_batch_size)\n        with torch.no_grad():\n            for inputs, batch in tqdm(test_loader):\n                batch = {k: v.to(device) for k, v in batch.items()}\n                inputs = inputs.to(device)\n                embedding = model(inputs, batch).detach().cpu().numpy()\n                embeds.append(embedding)\n        \n    return np.concatenate(embeds)\n\n\ndef distance_to_matching_probability(x):\n    # fit distance to matching probability by poly-lines\n    p1 = [0.2, 0.850*CONSERVATIVENESS]\n    p2 = [0.3, 0.600*CONSERVATIVENESS]\n    p3 = [0.4, 0.225*CONSERVATIVENESS]\n    p4 = [0.5, 0.050*CONSERVATIVENESS]\n    if x < 0.0:\n        y = 1.0\n    elif x < p1[0]:\n        y = x*(p1[1] - 1.0)/(p1[0] - 0.0) + 1.0\n    elif x < p2[0]:\n        y = (x - p2[0])*(p2[1] - p1[1])/(p2[0] - p1[0]) + p2[1]\n    elif x < p3[0]:\n        y = (x - p3[0])*(p3[1] - p2[1])/(p3[0] - p2[0]) + p3[1]\n    elif x < p4[0]:\n        y = (x - p4[0])*(p4[1] - p3[1])/(p4[0] - p3[0]) + p4[1]\n    elif x < 0.6:\n        y = (x - 0.6)*(0.0 - p4[1])/(0.6 - p4[0])\n    else:\n        y = 0\n    return y\n\ndef adaptive_thresholding(dists=None, global_thresh=None, beta=None):\n    probs = np.frompyfunc(distance_to_matching_probability, 1, 1)(dists)\n    \n    # expected number of positives\n    ex_num_pos = np.sum(probs > 0.5)\n    \n    # expected F1 change when one more prediction is added\n    # sign of F1 change matters\n    for num_pred in range(0, KNN):\n        #denom = (num_pred + 1 + ex_num_pos)*(num_pred + ex_num_pos)\n        #term1 = 2.0/denom\n        term2 = (num_pred + ex_num_pos)*probs[num_pred] - np.sum(probs[:num_pred])\n        #dF = term1*term2\n        dF = term2\n        \n        if dF < 0:\n            break\n    \n    best_thresh = dists[num_pred]*1.00001\n    best_thresh = 0.5*(best_thresh-0.3)+0.1875\n    adaptive_thresh = beta*best_thresh + (1.0 - beta)*global_thresh\n    \n    #print(f'{global_thresh}-->{adaptive_thresh}')\n    \n    return adaptive_thresh\n\n\ndef knn_matching(knn_model, embeddings, thresh):\n    preds = []\n    CHUNK = 1024*4\n    \n    CTS = len(embeddings)//CHUNK\n    if len(embeddings)%CHUNK!=0: CTS += 1\n        \n    for j in range(CTS):\n        a = j*CHUNK\n        b = (j+1)*CHUNK\n        b = min(b,len(embeddings))\n        distances, indices = knn_model.kneighbors(embeddings[a:b,])\n              \n        for k in range(b-a):\n            dists = distances[k,]\n            if USE_ADAPTIVE_THRESHOLDING:\n                adaptive_thresh = adaptive_thresholding(dists=dists, global_thresh=thresh, beta=BETA)\n            else:\n                adaptive_thresh = thresh\n                \n            IDX = np.where(dists < adaptive_thresh)[0]\n            IDS = indices[k,IDX]\n            \n            # force min_num_preds to be 2\n            if force_2preds:\n                if len(IDS) < 2:\n                    # relax matching threshold\n                    IDX = np.where(dists < thresh*force_2preds_relax)[0]\n                    IDS = indices[k,IDX]\n                        \n            o = test.iloc[IDS].posting_id.values\n            preds.append(o)\n            \n    return preds\n\n\ndef alpha_query_expansion(knn_model, embeddings, qe_alpha, qe_m):\n    expanded_embeddings = []\n    CHUNK = 1024*4\n    \n    CTS = len(embeddings)//CHUNK\n    if len(embeddings)%CHUNK!=0: CTS += 1\n    for j in range( CTS ):\n        \n        a = j*CHUNK\n        b = (j+1)*CHUNK\n        b = min(b,len(embeddings))\n        distances, indices = knn_model.kneighbors(embeddings[a:b,])\n        for i in range(b-a):\n            weights = ((1-distances[i, 0:qe_m+1])**qe_alpha).reshape(qe_m+1, 1)\n            expanded_embedding = np.sum((embeddings[indices[i, 0:qe_m+1]]*weights), axis=0)/np.sum(weights)\n            expanded_embedding = expanded_embedding / np.linalg.norm(expanded_embedding)\n            expanded_embeddings.append(expanded_embedding)\n            \n    return np.array(expanded_embeddings)","metadata":{"papermill":{"duration":0.055497,"end_time":"2021-05-04T18:22:47.366418","exception":false,"start_time":"2021-05-04T18:22:47.310921","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i_model, model_weight_path in enumerate(model_weight_paths):\n    model.load_state_dict(torch.load(model_weight_path))\n    \n    # compute embeddings\n    n_test = 0\n    for testing_scale in testing_scales[i_model]:\n        if n_test ==0:\n            joint_embeddings = joint_embedder(test, model, scale=testing_scale, flip=False)\n        else:\n            joint_embeddings += joint_embedder(test, model, scale=testing_scale, flip=False)\n        n_test += 1\n    \n        if flip_TTAs[i_model]:\n            joint_embeddings += joint_embedder(test, model, scale=testing_scale, flip=True)\n            n_test += 1\n            \n    joint_embeddings = joint_embeddings/n_test\n    \n\n    # alpha query expansion\n    if alpha_query_expansion:\n        knn_model = NearestNeighbors(n_neighbors=ALPHA_QE_KNN, metric=knn_metric)\n        knn_model.fit(joint_embeddings)\n        for qe_alpha, qe_m in zip(qe_alphas[i_model], qe_ms[i_model]):\n            joint_embeddings = alpha_query_expansion(knn_model, joint_embeddings, qe_alpha, qe_m)\n            \n            if qe_mid_knn:\n                knn_model = NearestNeighbors(n_neighbors=ALPHA_QE_KNN, metric=knn_metric)\n                knn_model.fit(joint_embeddings)\n        del knn_model\n        _=gc.collect()\n    \n    # concat fold embeddings\n    if i_model == 0:\n        long_embeddings = joint_embeddings\n    else:\n        long_embeddings = np.hstack([long_embeddings, joint_embeddings])\n        \nprint(long_embeddings.shape)\n\ndel joint_embeddings\n_=gc.collect()\n\n\n# prediction\nknn_model = NearestNeighbors(n_neighbors=KNN, metric=knn_metric)\nknn_model.fit(long_embeddings)\ntest['matches'] = knn_matching(knn_model, long_embeddings, np.mean(cos_threshs))\n\ndel long_embeddings, knn_model, model\n_=gc.collect()","metadata":{"papermill":{"duration":37.906146,"end_time":"2021-05-04T18:23:25.356617","exception":false,"start_time":"2021-05-04T18:22:47.450471","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test['matches'] = test['matches'].map(lambda x : ' '.join(np.array(x)[0:KNN]))","metadata":{"papermill":{"duration":0.042672,"end_time":"2021-05-04T18:23:25.507287","exception":false,"start_time":"2021-05-04T18:23:25.464615","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test[['posting_id','matches']].to_csv('submission.csv',index=False)\nsub = pd.read_csv('submission.csv')\nsub.head()","metadata":{"papermill":{"duration":0.212816,"end_time":"2021-05-04T18:23:25.751465","exception":false,"start_time":"2021-05-04T18:23:25.538649","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"papermill":{"duration":0.032048,"end_time":"2021-05-04T18:23:25.815645","exception":false,"start_time":"2021-05-04T18:23:25.783597","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]}]}