{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"This was the initial approach that I tried.\nThe main idea was comparing every post with every other post with at least one common word and using a simple ML model to do binary classification about them being match or not.\nA dataset was generated for such pairs after extracting features.\nThe features included:\n1. EfficientNetB1 cosine similarity.\n1. Jaccard Distance.\n1. Jaccard Index of the half of text which is more similar.\n1. Fuzzy Ratio\n\nA Decision Tree model with 7 depth was trained on that data.\n\nA lot of text preprocessing was also done which also involved creating dataset of languages used on Shopee.","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"import numpy as np \nimport pandas as pd\nimport cv2\nimport gc\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras.preprocessing import image\nfrom tensorflow.keras.applications.efficientnet import preprocess_input\nfrom tensorflow.keras.models import Model\nfrom sklearn.neighbors import NearestNeighbors\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom numpy import dot\nfrom numpy.linalg import norm\nimport tqdm.notebook as tq\nfrom fuzzywuzzy import fuzz\nimport random\nimport math\nimport re\nimport time\nimport operator\nimport pickle\nimport warnings\nwarnings.filterwarnings('ignore')\nimport os","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def stringReplacer(st):\n    st = st.replace('(','').replace(')','')\n    st = st.replace('[','').replace(']','')\n    st = st.replace('!','').replace(')','')\n    st = st.replace('#',' ')\n    st = st.replace('-',' ')\n    st = st.replace(',',' ')\n    st = st.replace('\\\\',' ')\n    st = st.replace('/',' ')\n    st = st.replace('|',' ')\n    st = st.replace(' IN ',' ')\n    st = st.replace(' THE ',' ')\n    st = st.replace('\"',' ')\n    st = st.replace(' X ',' ')\n    st = st.replace('   ','  ')\n    st = st.replace('  ',' ')\n    return st\n\ndef cleanSlashX(st):\n    st2 = ''\n    i=0\n\n    while i<len(st):\n        if st[i:i+2]=='\\X':\n            i+=4\n        else:\n            st2 += st[i]\n            i+=1\n    return st2\n\ndef cleanup_text(text):\n    text = cleanSlashX(text)\n    text = stringReplacer(text)\n    text = \"\".join([c if ord(c) < 128 else \"\" for c in text]).strip() \n    return text","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test = pd.read_csv('../input/shopee-product-matching/test.csv')\ntest['title'] = test['title'].str.upper()\ntest['title'] = test['title'].apply(cleanup_text)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ewf = pd.read_csv('../input/english-word-frequency/unigram_freq.csv')\newf['word']=ewf['word'].str.upper()\n#engAW = set(ewf['word'].iloc[:30001])\nengAW = set(ewf['word'].iloc[:40001])\nfname = '../input/indonesian-and-malaysian-common-words-list/indonesian.txt'\nwith open(fname) as f:\n    indoAW = set(f.read().upper().splitlines())\nfname = '../input/indonesian-and-malaysian-common-words-list/malay.txt'\nwith open(fname) as f:\n    #malayAW = set(f.read().upper().splitlines()[:20000])\n    malayAW = set(f.read().upper().splitlines()[:30000])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def getLangData(langname,datapath):\n    langEng = pd.read_csv(datapath)\n    langEng[langname]=langEng[langname].str.upper()\n    langEng['English']=langEng['English'].str.upper()\n    langEngDict = {}\n    for i in range(langEng.shape[0]):\n        langEngDict[  langEng[langname].iloc[i]  ] = langEng['English'].iloc[i]\n    langWords = set(langEng[langname])\n    return (langEngDict,langWords)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"indoEngDict,indoWords1K = getLangData('Indonesian','../input/indonesian-english-1kwords/indonesian_eng.csv')\nfilipEngDict,filipWords1K = getLangData('Filipino','../input/indonesian-english-1kwords/filipino_eng.csv')\nmalayEngDict,malayWords1K = getLangData('Malay','../input/indonesian-english-1kwords/malaysian_eng.csv')\nportuEngDict,portuWords1K = getLangData('Portugese','../input/indonesian-english-1kwords/portugese_eng.csv')\nthaiEngDict,thaiWords1K = getLangData('Thai','../input/indonesian-english-1kwords/thai_eng.csv')\nvietEngDict,vietWords1K = getLangData('Vietnamese','../input/indonesian-english-1kwords/vietnamese_eng.csv')\nlangEng = pd.read_csv('../input/indonesian-english-1kwords/indonesian_eng.csv')\nlangEng['English']=langEng['English'].str.upper()\nengWords1K = set(langEng['English'])\nfilipAW = filipWords1K\nportuAW = portuWords1K\ngermEngDict,germWords1K = getLangData('German','../input/indonesian-english-1kwords/german_eng.csv')\ngermAW = germWords1K","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"langFilip = pd.read_csv('../input/indonesian-english-1kwords/filipino_eng.csv')\nlangFilip['Filipino']=langFilip['Filipino'].str.upper()\nfilipSW = set(langFilip['Filipino'].iloc[:51])\n\nlangIndo = pd.read_csv('../input/indonesian-english-1kwords/indonesian_eng.csv')\nlangIndo['Indonesian']=langIndo['Indonesian'].str.upper()\nindoSW = set(langIndo['Indonesian'].iloc[:51])\n\nlangMalay = pd.read_csv('../input/indonesian-english-1kwords/malaysian_eng.csv')\nlangMalay['Malay']=langMalay['Malay'].str.upper()\nmalaySW = set(langMalay['Malay'].iloc[:51])\n\nlangPortu = pd.read_csv('../input/indonesian-english-1kwords/portugese_eng.csv')\nlangPortu['Portugese']=langPortu['Portugese'].str.upper()\nportuSW = set(langPortu['Portugese'].iloc[:51])\n\nlangEng = pd.read_csv('../input/indonesian-english-1kwords/malaysian_eng.csv')\nlangEng['English']=langEng['English'].str.upper()\nengSW = set(langEng['English'].iloc[:51])\nengSW.remove('IN')\nengSW.add('MY')\nengSW.add(\"IT'S\")\n    \nallSW = engSW.union(indoSW,malaySW,portuSW,filipSW)\n#allAW = engAW.union(indoAW,malayAW,portuAW,filipAW)\nallAW = engAW.union(indoAW,malayAW,portuAW,filipAW,germAW)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def cosine_similarity(a, b):\n    a = np.array(a)\n    b = np.array(b)\n    if a.shape != b.shape:\n        raise RuntimeError(\"array {} shape not match {}\".format(a.shape, b.shape))\n    if a.ndim==1:\n        a_norm = np.linalg.norm(a)\n        b_norm = np.linalg.norm(b)\n    elif a.ndim==2:\n        a_norm = np.linalg.norm(a, axis=1, keepdims=True)\n        b_norm = np.linalg.norm(b, axis=1, keepdims=True)\n    else:\n        raise RuntimeError(\"array dimensions {} not right\".format(a.ndim))\n    similiarity = np.dot(a, b.T)/(a_norm * b_norm) \n    return similiarity\n\ndef cosine_similarity2(a, b):\n    a = np.array(a)\n    b = np.array(b)\n    return dot(a, b)/(norm(a)*norm(b))\n\ndef dot_product2(v1, v2):\n    return sum(map(operator.mul, v1, v2))\n\ndef cosine_similarity3(v1, v2):\n    prod = dot_product2(v1, v2)\n    len1 = math.sqrt(dot_product2(v1, v1))\n    len2 = math.sqrt(dot_product2(v2, v2))\n    return prod / (len1 * len2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#from tensorflow.keras.applications import EfficientNetB3\nfrom tensorflow.keras.applications import EfficientNetB1\n\ndef findDifference(f1, f2):\n    return np.linalg.norm(f1-f2)\n \ndef cosine_distance(a, b):\n    if a.shape != b.shape:\n        raise RuntimeError(\"array {} shape not match {}\".format(a.shape, b.shape))\n    if a.ndim==1:\n        a_norm = np.linalg.norm(a)\n        b_norm = np.linalg.norm(b)\n    elif a.ndim==2:\n        a_norm = np.linalg.norm(a, axis=1, keepdims=True)\n        b_norm = np.linalg.norm(b, axis=1, keepdims=True)\n    else:\n        raise RuntimeError(\"array dimensions {} not right\".format(a.ndim))\n    similiarity = np.dot(a, b.T)/(a_norm * b_norm) \n    dist = 1. - similiarity\n    return dist\n\nclass FeatureExtractor_EffNetB1:\n    def __init__(self):\n        \n        #base_model = EfficientNetB3(weights='../input/tfkeras-efficientnet-weights/efficientnetb3_notop.h5',\\\n        #                            include_top=False, pooling='avg', input_shape=(300,300,3))\n        base_model = EfficientNetB1(weights='../input/tfkeras-efficientnet-weights/efficientnetb1_notop.h5',\\\n                                    include_top=False, pooling='avg', input_shape=(240,240,3))\n        self.model = base_model\n    \n    def extract(self, img):\n        #h1,w1 = (img.shape[0],img.shape[1])\n        #img = img[ int(h1*0.2):int(h1*0.8) , int(w1*0.2):int(w1*0.8) , : ]\n        #img = cv2.resize(img, (300,300), interpolation = cv2.INTER_AREA)\n        img = cv2.resize(img, (240,240), interpolation = cv2.INTER_AREA)\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        x = image.img_to_array(img)\n        x = np.expand_dims(x, axis=0)\n        x = preprocess_input(x)\n        feature = self.model.predict(x)[0]\n        return feature / np.linalg.norm(feature)\n    \n    def extract_batch(self,x):\n        features = self.model.predict(x)\n        normi = np.linalg.norm(features,axis=1)\n        features = features / normi[:,None]\n        return features\n\nmodelEffNetB1 = FeatureExtractor_EffNetB1()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def getTermMatches(l1,l2):\n    matches = []\n    l1 = list(set(l1))\n    l2 = list(set(l2))\n    l2C = ''.join(l2)\n    for t1 in l1:\n        inset1 = set(t1[1:-1])\n        if t1 in l2:\n            matches.append(t1)\n        else:\n            if t1 in l2C:\n                matches.append(t1)\n            else:\n                for t2 in l2:\n                    inset2 = set(t2[1:-1])\n                    if len(t1)>3 and len(t2)>3:\n                        if t1[0]==t2[0] and t1[-1]==t2[-1] and len(inset1 ^ inset2)<=2 and len(inset1.intersection(inset2))>3 :\n                            matches.append(t1)\n                            break\n  \n    matches = set(matches)\n    return matches","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def breakPostfixQuants(ttl,psfx):\n    ttlB = []\n    qTpls = []\n    for wrd in ttl:\n        found = False\n        for fx in psfx:\n            if wrd[-len(fx):]==fx:\n                n = len(wrd)-len(fx)\n                if n > 0:\n                    prv = wrd[:n]\n                    if prv.isnumeric():\n                        found = True\n                        ttlB.append(prv)\n                        ttlB.append(fx)\n        if not found:\n            ttlB.append(wrd)\n            \n    psfx = set(psfx)\n    for i,wrd in enumerate(ttlB):\n        if i>0:\n            if wrd in psfx:\n                prv = ttlB[i-1]\n                if prv.isnumeric():\n                    qTpls.append( (prv,wrd) )\n                    \n    for tpl in qTpls:\n        ttlB.remove(tpl[0])\n        ttlB.remove(tpl[1])\n            \n    return (ttlB,qTpls)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def getTermMisMatches(l1,l2):\n    l1 = set(l1)\n    l2 = set(l2)\n    mismatches = list(l1 ^ l2)\n    return set(mismatches)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def preprocessTitle(ttl):\n    #ttl = cleanup_text(ttl)\n    #ttl = stringReplacer(ttl)\n    tl = ttl.split(' ')\n    tl = [wd for wd in tl if wd not in allSW]\n    psfx = ['ML','LTR','KG','GM','GR','GRM','GRAM','PC','PCS','X','W','V','IN','G','CM','M','OZ']\n    tl,qtpl = breakPostfixQuants(tl,psfx)\n    btl = set(zip(tl,tl[1:]))\n    return (tl,qtpl,btl)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def tokenizeTitle(ttl):\n    tl = ttl.split(' ')\n    tl = [wd for wd in tl if wd not in allSW]\n    return set(tl[:30])\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def getUnigramMatchings(tl1,tl2,qtpl1,qtpl2):\n\n    qNums = set([ q[0] for q in qtpl1 ] + [ q[0] for q in qtpl2 ])\n    \n    TF = getTermMatches(tl1,tl2)\n    \n    #TF = [ m for m in TF if any(map(str.isdigit,m))==False ]\n    \n    TF = [wd for wd in TF if len(wd)>0]\n    \n    DF = []\n    NF = []\n    for wd in TF:\n        if wd in allAW:\n            DF.append(wd)\n        else:\n            NF.append(wd)\n    TN = getTermMisMatches(tl1,tl2)\n    \n    TN = [ m for m in TN if m not in qNums ]\n    \n    #TN = [ m for m in TN if any(map(str.isdigit,m))==False ]\n    \n    TN = [wd for wd in TN if len(wd)>0] #0\n    \n    DN = []\n    NN = []\n    for wd in TN:\n        if wd in allAW:\n            DN.append(wd)\n        else:\n            NN.append(wd)\n            \n    mDict = {'TF':TF,'DF':DF,'NF':NF,'TN':TN,'DN':DN,'NN':NN,'T1':tl1,'T2':tl2,'Q1':qtpl1,'Q2':qtpl2 }\n    \n    return mDict","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"testMode = False","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def stringReplacer(st):\n    st = st.replace('(','').replace(')','')\n    st = st.replace('[','').replace(']','')\n    st = st.replace('!','').replace(')','')\n    st = st.replace('#',' ')\n    st = st.replace('-',' ')\n    st = st.replace(',',' ')\n    st = st.replace('\\\\',' ')\n    st = st.replace('/',' ')\n    st = st.replace('|',' ')\n    st = st.replace(' IN ',' ')\n    st = st.replace(' THE ',' ')\n    st = st.replace('\"',' ')\n    st = st.replace(' X ',' ')\n    st = st.replace('   ','  ')\n    st = st.replace('  ',' ')\n    return st\n\ndef cleanSlashX(st):\n    st2 = ''\n    i=0\n\n    while i<len(st):\n        if st[i:i+2]=='\\X':\n            i+=4\n        else:\n            st2 += st[i]\n            i+=1\n    return st2\n\ndef cleanup_text(text):\n    text2 = stringReplacer(text)\n    text3 = \"\".join([c if ord(c) < 128 else \"\" for c in text2]).strip() \n    return cleanSlashX(text2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if testMode:\n    train = pd.read_csv('../input/shopee-product-matching/train.csv')\n    train['title'] = train['title'].str.upper()\n    train['title'] = train['title'].apply(cleanup_text)\n    train = train.sample(frac = 1,random_state=0).reset_index(drop=True) \n    df = train.iloc[:1000]\n    #df = df.append(df)\n    #df = df.reset_index(drop=True)\n    bPath = '../input/shopee-product-matching/train_images'\n    print(df)\n    \nelse:\n    df = test\n    bPath = '../input/shopee-product-matching/test_images'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"startT = time.time()\ndf['tokens'] = df['title'].apply(tokenizeTitle)\nendT = time.time()\nprint('Time Elapsed : ', (endT - startT)  )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def matchesAny(l):\n    global tkns\n    return len(l.intersection(tkns))>1\n\ndef matchesAny2(l):\n    global tkns,ttlC\n    return len(l[0].intersection(tkns))>1 or (l[1]==ttlC)\n\ndef matchesAny3(l):\n    global tkns,ttlC,phsh\n    return len(l[0].intersection(tkns))>1 or l[1]==ttlC or l[2]==phsh ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def jiCheck(l):\n    global tkns\n    return (len(l.intersection(tkns)) / len( l.union(tkns) )) > 0.1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"validVD = {}\nfor i in tq.tqdm(range(  int(df.shape[0])  )):\n    validVD[i] = []\n    tkns = df['tokens'].iloc[i]\n    ttlC = df['title'].iloc[i]\n    phsh = df['image_phash'].iloc[i]\n    mL = set(df[ df['tokens'].apply(matchesAny) ].index)\n    #mL = list(df[ df['tokens'].apply(matchesAny2) ].index)\n    #mL = set(df[ df['tokens'].apply(matchesAny3) ].index)\n    validVD[i] = mL","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.drop(columns=['tokens'],inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pptDict = {}\nembNList = []\n\nfor i in tq.tqdm( range(df.shape[0]) ):\n    \n    ttl1 = df['title'].iloc[i]\n\n    tl1,qtpl1,btl1 = preprocessTitle(ttl1)\n    \n    imgP = df['image'].iloc[i]\n    img = cv2.imread(  os.path.join(bPath,imgP) )\n    embNList.append( modelEffNetB1.extract(img) )\n    \n    pptDict[i] = (tl1,qtpl1,btl1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"imgEmbs_np = np.array(embNList)\ndel embNList\ngc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"startT = time.time()\nnbrs = NearestNeighbors(n_neighbors=min(50,df.shape[0]), algorithm='ball_tree').fit(imgEmbs_np)\ndistances, indices = nbrs.kneighbors(imgEmbs_np)\nendT = time.time()\nprint('Time Elapsed : ',endT - startT)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"n = 0\nfor i in tq.tqdm(range(df.shape[0])):\n    nbrInds = indices[i,1:]\n    nbrDists = distances[i,1:]\n    nbrVInds = []\n    for ni,d in enumerate(nbrDists):\n        if d<0.5:\n            nbrVInds.append( nbrInds[ni] )\n        else:\n            break\n    \n    nbrVInds = set(nbrVInds)\n    vldInds = validVD[i]\n    unn = vldInds.union(nbrVInds)\n    if len(unn) > len(vldInds):\n        validVD[i] = unn\n        n += 1\n        \nprint(n*100/df.shape[0])\n\ndel distances\ndel indices\ngc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def getMaxFuzzyMatch(ttl1,ttl2):\n    ttlS = ttl1\n    ttlB = ttl2\n    if len(ttlB)<len(ttlS):\n        ttlS,ttlB = ttlB,ttlS\n    \n    blockL = ttlB[:len(ttlS)]\n    blockR = ttlB[-len(ttlS):]\n    \n    fzL = fuzz.ratio(ttlS,blockL)\n    fzR = fuzz.ratio(ttlS,blockR)\n    fzMax = max(fzL,fzR)\n    return fzMax","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def getMaxRI(tl1,tl2):\n    ttlS = tl1\n    ttlB = tl2\n    if len(ttlB)<len(ttlS):\n        ttlS,ttlB = ttlB,ttlS\n    \n    blockL = ttlB[:len(ttlS)]\n    blockR = ttlB[-len(ttlS):]\n\n    ri1 =  (len(set(blockL).union(ttlS)) - len( set(blockL).intersection(set(ttlS)) )) / len(set(blockL).union(ttlS))\n    ri2 = (len(set(blockR).union(ttlS)) - len( set(blockR).intersection(set(ttlS)) )) / len(set(blockR).union(ttlS))\n    return max(ri1,ri2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def getMaxJI(tl1,tl2):\n    ttlS = tl1\n    ttlB = tl2\n    if len(ttlB)<len(ttlS):\n        ttlS,ttlB = ttlB,ttlS\n    \n    blockL = ttlB[:len(ttlS)]\n    blockR = ttlB[-len(ttlS):]\n\n    ji1 = len( set(blockL).intersection(set(ttlS)) ) / len(set(blockL).union(ttlS))\n    ji2 = len( set(blockR).intersection(set(ttlS)) ) / len(set(blockR).union(ttlS))\n    return max(ji1,ji2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def quantMismatched(Q1,Q2):\n    #psfx = ['ML','LTR','KG','GM','GR','GRM','GRAM','PC','PCS','X','W','V','IN','G','CM','M','OZ']\n    #Q1 = breakPostfixQuants(TL1,psfx)[1]\n    #Q2 = breakPostfixQuants(TL2,psfx)[1]\n    Q1 = set([q[0] for q in Q1 ])\n    Q2 = set([q[0] for q in Q2 ])\n    nf = False\n    if len(Q1)>0 and len(Q2)>0:\n        QL = Q1\n        QS = Q2\n        if len(QS)>len(QL):\n            QS = Q1\n            QL = Q2\n        nf = False\n        for q in QS:\n            if q not in QL:\n                nf = True\n                break\n    return nf","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"doRec = False","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def getWordCountsS(potDocs):\n    wcountDict = {}\n    for doc in potDocs:\n        tkns = doc.split(' ')\n        for w in tkns:\n            if w in wcountDict:\n                wcountDict[w]+=1\n            else:\n                wcountDict[w]=1\n    return wcountDict\n\ndef getTFIDFSim(querDoc,potDocs):\n    potDocs = [querDoc] + potDocs\n    vectorizer = TfidfVectorizer()\n    X = vectorizer.fit_transform(potDocs)\n    tfidfDVecs = X.todense()\n    tfidSims = cosine_similarity(tfidfDVecs[0],tfidfDVecs)[0][1:]\n    return tfidSims\n\ndef getTFIDFSimPair(tl1,tl2, keyLPos, potWordCounts):\n    \n    vlen = len(keyLPos)\n    \n    vec1 = [0.0]*vlen\n    for w in tl1:\n        pw = potWordCounts[w]\n        if pw>0:\n            vec1[ keyLPos[w] ] = 1.0 / pw\n        \n    vec2 = [0.0]*vlen\n    for w in tl2:\n        pw = potWordCounts[w]\n        if pw>0:\n            vec2[ keyLPos[w] ] = 1.0 / pw \n        \n    try:\n        cos_sim = cosine_similarity3(vec1,vec2)\n    except:\n        cos_sim = 0\n    \n    return cos_sim\n    \ndef getWtdJI(tl1,tl2,wCntDict):\n    aIbt = list(set(tl1).intersection(set(tl2)))\n    aUbt = list(set(tl1).union(set(tl2)))\n    iCnt = 0\n    uCnt = 0\n    for w in aIbt:\n        wc = wCntDict[w]\n        if wc>0:\n            iCnt += (1.0/wc)\n    for w in aUbt:\n        wc = wCntDict[w]\n        if wc>0:\n            uCnt += (1.0/wc)\n    wJI = iCnt / uCnt\n    return wJI","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = pickle.load(open('../input/shopee-decisiontrees/mxdt7_smxjfzri.sav', 'rb'))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"eva = True","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"n = df.shape[0]\nfor i in tq.tqdm(range(n)):\n    qHash = df['image_phash'].iloc[i]\n    hashSet = set(df[ df['image_phash'] == qHash  ].index)\n    qTitle = df['title'].iloc[i]\n    titleSet = set(df[ df['title'] == qTitle  ].index)\n    validVD[i] = validVD[i].union(hashSet)\n    validVD[i] = validVD[i].union(titleSet)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if testMode and False:\n    n = df.shape[0]\n    m = 0\n    for i in tq.tqdm(range(n)):\n        lgp = df['label_group'].iloc[i]\n        gdf = df[ df['label_group']==lgp ]\n        gdfi = set(gdf.index)\n        if len(gdfi)>1:\n            \n            vld = validVD[i]\n            \n            sHash = set(df[ df['image_phash'] == df['image_phash'].iloc[i]  ].index)\n            \n            vld = vld.union(sHash)\n            \n            mt = vld.intersection(gdfi)\n\n            unfounds = []\n            for mi in gdfi:\n                if mi not in vld:\n                    unfounds.append(mi)\n\n\n    print( round(m*100/n,3) )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"resultDict = {}\n\nfeatsDict = {'JI':[],'WJI':[],'SE3':[],'QMM':[],'LR':[],'BF':[],'UF':[],\\\n              'MXFZ':[],'MXJI':[],'WMXJI':[], 'IND':[], 'BJI':[], 'TL1':[], 'TL2':[],\\\n              'FZ':[],'IDFS':[], 'isDup':[] }\n\nres = {}\nres['TP'] = 0\nres['FP'] = 0\nres['TN'] = 0\nres['FN'] = 0\nres['correct'] = 0\nres['total'] = 0\nres['trueP'] = 0\nres['falseP'] = 0\nres['trueT'] = 0\nres['falseT'] = 0\n\ndprobs = []\n\nfor i in tq.tqdm( range(df.shape[0])  ):\n    \n    postID1 = df['posting_id'].iloc[i]\n    ttl1 = df['title'].iloc[i]\n    phash1 = df['image_phash'].iloc[i]\n    \n    resultDict[postID1] = set()\n    \n    tl1,qtpl1,btl1 = pptDict[i]\n    \n    validSet = list(validVD[i])\n    \n    \"\"\"\n    potDocs = [ df['title'][pI] for pI in validSet ]\n    potWordCounts = getWordCountsS(potDocs)\n    keyList = list(potWordCounts.keys())\n    keyLPos = {}\n    for ki,kw in enumerate(keyList):\n        keyLPos[kw]=ki\n    \"\"\"\n    \n    for nj,j in enumerate(validSet):\n        \n        postID2 = df['posting_id'].iloc[j]\n        ttl2 = df['title'].iloc[j]\n        phash2 = df['image_phash'].iloc[j]\n        \n        if testMode and eva:\n            lbl1 = df['label_group'][i]\n            lbl2 = df['label_group'][j]\n            duplicTruth = lbl1==lbl2\n        \n        duplicPred = True\n        \n        if postID1 != postID2:\n\n            if postID2 in resultDict:\n                if postID1 in resultDict[postID2]:\n                    duplicPred = True\n                else:\n                    duplicPred = False\n            else:\n\n                duplicPred = True\n\n                if phash1!=phash2:\n                    \n                    \n                    f1 = imgEmbs_np[i]\n                    f2 = imgEmbs_np[j]\n                    sim = findDifference(f1, f2)\n\n                    tl2,qtpl2,btl2 = pptDict[j]\n                    \n                    \n                    qMM = quantMismatched(qtpl1,qtpl2)\n                    if qMM:\n                        qMM=1\n                    else:\n                        qMM=0\n                    \n                    \n                    #wJI = getWtdJI(tl1,tl2,potWordCounts)\n                    \n                    #wJIMax = getMaxWtdJI(tl1,tl2,potWordCounts)\n                    \n                    #tfidSim = tfidfSims[nj]\n                    \n                    #tfidSim = getTFIDFSimPair(tl1,tl2, keyLPos, potWordCounts)\n                    \n                    aUbU = len( set(tl1).union(set(tl2)) )\n                    aUbB = len( btl1.union(btl2) )\n                    aIbU = len( set(tl1).intersection(set(tl2)) )\n                    aIbB = len( btl1.intersection(btl2) )\n                    \n                    Jid = aIbU / aUbU\n                    Rid = 1 - Jid\n                    \n                    \"\"\"\n                    \n                    if aUbB>0:\n                        BJid = aIbB / aUbB\n                    else:\n                        BJid = 0\n                    \"\"\"\n                    #mDict = getUnigramMatchings(tl1,tl2,qtpl1,qtpl2)\n\n                    #lenRatio = float(max( len(ttl1),len(ttl2) )) / float(min( len(ttl1),len(ttl2) ) )\n                        \n                    fz = fuzz.ratio(ttl1,ttl2)    \n                    \n                    #fzMax = getMaxFuzzyMatch(ttl1,ttl2)\n                    \n                    jiMax = getMaxJI(tl1,tl2)\n                    \n                    duplicPred = False\n                    \n                    #featVec = np.array([[sim,tfidSim,wJI,fzMax]])\n                    featVec = np.array([[sim,jiMax,fz,Rid]])\n                    duplicPred = model.predict( featVec )[0] == 1\n                    \n                    #duplicPred = qMM==0 and (sim<0.63 or tfidSim>0.48)\n                    \n                    duplicPred = duplicPred and qMM==0\n                    \n                    #proba = model.predict_proba( featVec )\n                    #dupProb = proba[0,1]\n                    \"\"\"\n                    ndupProb = proba[0,0]\n                    if duplicPred:\n                        dprobs.append(dupProb)\n                    \"\"\"\n                        \n                    #duplicPred = duplicPred and dupProb>0.6\n                    \n                    #duplicPred = duplicPred or (Jid>0 and sim<0.15)\n                    \n                    #duplicPred = duplicPred or (jiMax>0 and sim<0.25)\n                    #duplicPred = duplicPred or (wJI>0 and sim<0.25)\n                    \n                    #duplicPred = ( (sim < 1.24 and Jid > 0.7) or (sim < 0.8 and Jid > 0.37) or (sim < 0.5 and Jid > 0) or (sim<0.9 and fz>70))\\\n                    #and (len(mDict['TN'])<21) and (lenRatio<13) #** 0.685 \n                    \n                    \n                    ss=0\n                    \n                    if ttl1==ttl2:\n                        duplicPred = True\n                        ss=1\n               \n                    #################\n                    if testMode and eva:\n                        pass\n                        #lbl1 = df['label_group'][i]\n                        #lbl2 = df['label_group'][j]\n                        #duplicTruth = lbl1==lbl2\n                        \n                        \"\"\"\n                        \n                        featsDict['IND'].append( (i,j) )\n                        featsDict['TL1'].append( ttl1 )\n                        featsDict['TL2'].append( ttl2 )\n                        featsDict['JI'].append(Jid)\n                        featsDict['WJI'].append(wJIMax)\n                        featsDict['BJI'].append(BJid)\n                        featsDict['SE3'].append(sim)\n                        featsDict['QMM'].append(qMM)\n                        featsDict['LR'].append(lenRatio)\n                        featsDict['UF'].append(aIbU)\n                        featsDict['BF'].append(aIbB)\n                        featsDict['FZ'].append(fz)\n                        featsDict['MXFZ'].append(fzMax)\n                        featsDict['MXJI'].append(jiMax)\n                        featsDict['WMXJI'].append(wJIMax)\n                        featsDict['IDFS'].append(tfidSim)\n                        \n\n                        if duplicTruth:\n                            featsDict['isDup'].append(1)\n                            \n                        else:\n                            featsDict['isDup'].append(0)\n                            \n                        \"\"\"\n\n                        \n                    \n                    \n                    #################\n                        \n\n                else:\n                    duplicPred = True\n                    \n        if testMode and eva:\n            if duplicTruth==True and duplicPred==True:\n                res['TP'] += 1\n\n            if duplicTruth==True and duplicPred==False:\n                res['FN'] += 1\n\n            if duplicTruth==False and duplicPred==True:\n                res['FP'] += 1\n\n            if duplicTruth==False and duplicPred==False:\n                res['TN'] += 1\n\n            if duplicTruth == duplicPred:\n                res['correct'] += 1\n            if duplicPred==True:\n                res['trueP'] += 1\n            else:\n                res['falseP'] += 1\n            if duplicTruth==True:\n                res['trueT'] += 1\n            else:\n                res['falseT'] += 1\n            res['total'] += 1\n            \n\n        if duplicPred:\n            resultDict[postID1].add(postID2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"posting_id = df['posting_id'].tolist()\nmatches = []\n\nfor i in posting_id:\n    matchesL = list(resultDict[ i ])\n    matchStr = ''\n    for match in matchesL:\n        matchStr += match+' '\n    matchStr = matchStr[:-1]\n    matches.append(matchStr)\n    \nsub_df = pd.DataFrame({'posting_id': posting_id, 'matches': matches})\nsub_df.to_csv('submission.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if testMode:\n    res['F1'] = res['TP'] / ( res['TP'] + 0.5*(res['FP']+res['FN']) )\n    print(res['F1'] )\n    res['Accr'] = res['correct'] * 100 / res['total']\n    print(res)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}