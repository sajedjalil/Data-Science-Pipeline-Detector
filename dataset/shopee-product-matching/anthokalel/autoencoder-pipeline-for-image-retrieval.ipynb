{"cells":[{"metadata":{},"cell_type":"markdown","source":"\n\n![](https://web14.bernama.com/storage/photos/4a4a6e340871b6d98de1a9c458ffb2635ebcf0e42c9ed)\n\n<h1> <center> Shopee - Price Match Guarantee </center> </h1> \n<h2> <center> Convolutional Auto Encoder for Image Retrieval </center> </h2>\n\n"},{"metadata":{},"cell_type":"markdown","source":"# Summary : \n\n* [1. Introduction](#section_one)\n    - [1.1 What is Shopee](#section_one_one)\n    - [1.2 What we need to do ?](#section_one_two)\n    - [1.3 How to evaluate the model ?](#section_one_three)\n* [2. Preparing the data & EDA](#section_two)\n    - [2.1 Librairies and loading the files](#section_two_one)\n    - [2.2 Cleaning the data](#section_two_two)\n    - [2.3 EDA](#section_two_three)\n* [3. Image similarity pipeline](#section_three)\n    - [3.1 Plan of Attack](#section_three_one)\n    - [3.2 Convolutional Autoencoder to generate latent representation of images](#section_three_two)\n    - [3.3 Autoencoder to reduce the dimensionality](#section_three_three)\n    - [3.4 Ensemble clustering](#section_three_four)\n    - [3.5 Visualization of results](#section_three_five)\n* [4. Conclusion](#section_four)\n     \n\n<a id=\"section_one\"></a>\n# 1. Introduction \n\n## 1.1 What is Shopee ?\n\nShopee is the leading e-commerce platform in Southeast Asia and Taiwan.  Customers appreciate its easy, secure, and fast online shopping experience tailored to their region. The company also provides strong payment and logistical support along with a 'Lowest Price Guaranteed' feature on thousands of Shopee's listed products.\n\n## 1.2 What we need to do ? \n\nRetail companies use a variety of methods to assure customers that their products are the cheapest. Among them is product matching, which allows a company to offer products at rates that are competitive to the same product sold by another retailer.\n\nThe purpose of this competition is to use our machine learning skills to perform a model that is able to predict which items are the same product in function of image and text item information. \n\n## 1.3 How to evaluate the model ?\n\nEach item is represented by a row in a dataframe. For each item, the model predict each items corresponding to the same product. Then, the F1 Score is evaluated for each row and then averaged. \n\nThe F1-Score : \n$ F_1 = \\frac{tp}{tp + \\frac{1}{2}(fp + fn)} $\n\n$tp$ : true positive \n\n$fp$ : false positive\n\n$fn$ : false negative\n\n"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"section_two\"> </a>\n# 2. Preparating the data & EDA \n<a id=\"section_two_one\"> </a>\n## 2.1 Librairies and loading the files ðŸ“š "},{"metadata":{"trusted":true},"cell_type":"code","source":"#Basics\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nimport tqdm\nimport cv2\n\n#Deep learning\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom albumentations import  OneOf, Compose, CLAHE, HueSaturationValue, RandomGamma, HorizontalFlip\n\n#Sklearn\nfrom sklearn.model_selection import KFold\nfrom sklearn.cluster import DBSCAN\nfrom sklearn.cluster import OPTICS\nfrom sklearn.neighbors import NearestNeighbors\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"DIR_TRAIN = \"../input/shopee-product-matching/train_images/\"\nDIR_TEST = \"../input/shopee-product-matching/test_images/\"\n\nTRAIN_CSV = \"../input/shopee-product-matching/train.csv\"\nTEST_CSV = \"../input/shopee-product-matching/test.csv\"\nSS_CSV = \"../input/shopee-product-matching/sample_submission.csv\"\n\ntrain = pd.read_csv(TRAIN_CSV)\ntest = pd.read_csv(TEST_CSV)\nsample_submission = pd.read_csv(SS_CSV)\nAUTO = tf.data.experimental.AUTOTUNE","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"section_two_two\"> </a>\n## 2.2 Cleaning the data\n\nHere are the first lines of the dataset : "},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Informations on the columns : \n* posting_id - the ID code for the posting.\n* image - the image id/md5sum.\n* image_phash - a perceptual hash of the image.\n* title - the product description for the posting.\n* label_group - ID code for all postings that map to the same product. "},{"metadata":{"trusted":true},"cell_type":"code","source":"train = train.drop_duplicates(subset=['image']).reset_index() #drop duplicated rows\ntmp = train.groupby('label_group').posting_id.agg('unique').to_dict()\ntrain['target'] = train.label_group.map(tmp) #make a column with posting_id in the same label group\nfilenames = train['image']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"section_two_three\"></a>\n## 2.3 EDA\n\nIn developement..."},{"metadata":{},"cell_type":"markdown","source":"<a id=\"section_three\"></a>\n# 3 Image similarity pipeline\n\n<a id=\"section_three_one\"></a>\n## 3.1 Plan of attack\n       \nIn this notebook, i'm focus on image retrieval without using text informations (for the moment). I made a pipeline summary : \n\n"},{"metadata":{},"cell_type":"markdown","source":"![l](https://www.hebergeur-image.com/upload/81.2.149.83-6058a1cbf196d.PNG)"},{"metadata":{"trusted":true},"cell_type":"code","source":"parameters = {\"IMG_SIZE\" : (256, 256),\n              \"EPOCHS\":1,\n              \"BATCH_SIZE\" : 32,\n             \"LEARNING_RATE\":0.001}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"These functions are used to read the image, resize them and for image augmentation : "},{"metadata":{"trusted":true},"cell_type":"code","source":"def process_path(image_input, image_output):\n  # load the raw data from the file as a string\n    img_input = tf.io.read_file(image_input)\n    img_input = tf.image.decode_jpeg(img_input, channels=3)\n\n    img_output = tf.io.read_file(image_output)\n    img_output = tf.image.decode_jpeg(img_output, channels=3)\n\n    return img_input, img_output\n\ndef aug_fn(image, img_size):\n    data = {\"image\": image}\n    aug_data = transforms(**data)\n    #print(aug_data.shape)\n    aug_img = aug_data[\"image\"]\n    aug_img = tf.cast(aug_img/255.0, tf.float32)\n    aug_img = tf.image.resize(aug_img, size=[img_size, img_size])\n    return aug_img\n\ndef process_aug(img_input, img_output):\n    \n    aug_img_input = tf.numpy_function(func=aug_fn, inp=[img_input, 256], Tout=[tf.float32])\n    aug_img_input = tf.reshape(aug_img_input, [256, 256, 3])\n    aug_img_output = tf.numpy_function(func=aug_fn, inp=[img_output, 256], Tout=[tf.float32])\n    aug_img_output = tf.reshape(aug_img_output, [256, 256, 3])\n    return aug_img_input, aug_img_output\n\ndef set_shapes(img_input, img_output):\n    img_input.set_shape((256, 256, 3))\n    img_output.set_shape((256, 256, 3))\n    return img_input, img_output\n\ndef read_image(image_input, image_output):\n    \n    image_input = tf.io.read_file(image_input)\n    #image_input = transforms(image = image_input)\n    image_input = tf.io.decode_jpeg(image_input, channels = 3)\n    image_input = tf.image.resize(image_input, [256, 256])\n    image_input = tf.cast(image_input, tf.float32)/255.0\n    \n    image_output = tf.io.read_file(image_output)\n    #image_output = transforms(image = image_output)\n    image_output = tf.io.decode_jpeg(image_output, channels = 3)\n    image_output = tf.image.resize(image_output, [256, 256])\n    image_output = tf.cast(image_output, tf.float32)/255.0\n    \n    return image_input, image_output\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#transforms = OneOf([CLAHE(clip_limit=2), IAASharpen(), IAAEmboss(), RandomBrightnessContrast()], p=0.3)\ndef get_train_dataset(batch_size = parameters['BATCH_SIZE'], with_augmentation = False):\n    if with_augmentation :\n        train_dataset = (tf.data.Dataset.from_tensor_slices((DIR_TRAIN + filenames, \n                                                             DIR_TRAIN + filenames))\n                            .map(process_path, num_parallel_calls=AUTO)\n                            .map(process_aug, num_parallel_calls=AUTO)\n                            .map(set_shapes, num_parallel_calls=AUTO)\n                            .batch(batch_size)\n                            .prefetch(AUTO)\n                        )\n        return train_dataset\n    else : \n        train_dataset = (tf.data.Dataset.from_tensor_slices((DIR_TRAIN + filenames, \n                                                         DIR_TRAIN + filenames))\n                     .map(read_image, num_parallel_calls=tf.data.AUTOTUNE)\n                     .batch(parameters['BATCH_SIZE'])\n                     .prefetch(AUTO)\n                    )\n    return train_dataset\n\ntrain_dataset = get_train_dataset(parameters['BATCH_SIZE'], with_augmentation = False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Thanks to [Andrada Olteanu](https://www.kaggle.com/andradaolteanu/shopee-text-prep-fe-image-augmentation) : "},{"metadata":{"trusted":true},"cell_type":"code","source":"def display_augmentations(path):\n    '''Displays different types of augmentations on a chosen image.\n    path: the direct path to the desired image.'''\n    \n    # Read in original image\n    original = cv2.imread(path)\n    original = cv2.cvtColor(original, cv2.COLOR_BGR2RGB)\n\n    # Transformations\n    transform_clahe = Compose([CLAHE(clip_limit=2)])\n    transform_huesv = Compose([HueSaturationValue(hue_shift_limit=100)])\n    transform_hf = Compose([HorizontalFlip()])\n    transform_rg = Compose([RandomGamma(gamma_limit=(200, 400))])\n    \n\n    # Apply transformations\n    transform_clahe = transform_clahe(image=original)[\"image\"]\n    transform_huesv = transform_huesv(image=original)[\"image\"]\n    transform_hf = transform_hf(image=original)[\"image\"]\n    transform_rg = transform_rg(image=original)[\"image\"]\n\n    all_transformations = [original, transform_clahe, transform_huesv, transform_hf, transform_rg]\n    all_names = [\"Original\", \"CLAHE\", \"HueSaturationValue\", \"HorizontalFlip\", \"RandomGamma\"]\n    \n    # Plot\n    fig = plt.figure(figsize=(10, 7))\n    plt.suptitle(f\"Image Augmentations\", fontsize=20)\n    for k, image in enumerate(all_transformations):\n        fig.add_subplot(1, 5, k+1)\n        plt.title(all_names[k])\n        plt.imshow(image)\n        plt.axis(\"off\")\n\n    plt.show();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"display_augmentations(\"../input/shopee-product-matching/test_images/0006c8e5462ae52167402bac1c2e916e.jpg\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"section_three_two\"> </a>\n## 3.2 Convolutional Autoencoder to generate latent representation of images"},{"metadata":{},"cell_type":"markdown","source":"The convolutional autoencoder  allows us to represent the images as a vector with 1024 components. \nThe image having a dimension of 256 * 256 * 3 = 196608 components is thus encoded into a vector with 1024 components."},{"metadata":{"trusted":true},"cell_type":"code","source":"class CAE(Model):\n  def __init__(self):\n    super(CAE, self).__init__()\n    self.encoder = tf.keras.Sequential([\n        tf.keras.layers.Conv2D(filters = 32, kernel_size = (3, 3), padding = \"same\",\n                               activation = \"relu\", input_shape = (256, 256, 3)),\n        tf.keras.layers.AveragePooling2D(pool_size = (2, 2), padding = \"same\"),\n        tf.keras.layers.Conv2D(filters = 16, kernel_size = (3, 3), padding = \"same\", \n                               activation = \"relu\"),\n        tf.keras.layers.AveragePooling2D(pool_size = (2, 2), padding = \"same\"),\n        tf.keras.layers.Conv2D(filters = 16, kernel_size = (3, 3), padding = \"same\", \n                               activation = \"relu\"),\n        tf.keras.layers.AveragePooling2D(pool_size = (2, 2), padding = \"same\"),\n        tf.keras.layers.Conv2D(filters = 8, kernel_size = (3, 3), padding = \"same\", \n                               activation = \"relu\"),\n        tf.keras.layers.AveragePooling2D(pool_size = (2, 2), padding = \"same\"),\n\n        tf.keras.layers.Flatten(),\n        #tf.keras.layers.Dense(64, activation = 'relu')\n    ])\n\n    self.decoder = tf.keras.Sequential([\n        #tf.keras.layers.Dense(8192, activation = 'relu'),\n        tf.keras.layers.Reshape(target_shape = (16, 16, 8)),\n        tf.keras.layers.Conv2D(filters = 8, kernel_size = (3, 3), padding = \"same\", \n                               activation = \"relu\"),\n        tf.keras.layers.UpSampling2D(size = (2, 2)),\n        tf.keras.layers.Conv2D(filters = 16, kernel_size = (3, 3), padding = \"same\", \n                               activation = \"relu\"),\n        tf.keras.layers.UpSampling2D(size = (2, 2)),\n        tf.keras.layers.Conv2D(filters = 16, kernel_size = (3, 3), padding = \"same\", \n                               activation = \"relu\"),\n        tf.keras.layers.UpSampling2D(size = (2, 2)),\n        tf.keras.layers.Conv2D(filters = 32, kernel_size = (3, 3), padding = \"same\",\n                               activation = \"relu\"),\n        tf.keras.layers.UpSampling2D(size = (2, 2)),\n        tf.keras.layers.Conv2D(filters = 3, kernel_size = (3, 3), padding = \"same\", \n                               activation = \"sigmoid\")\n        ])\n    \n  def call(self, x):\n    encoded = self.encoder(x)\n    decoded = self.decoder(encoded)\n    return decoded\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cae_model = CAE()\nopt = tf.keras.optimizers.Adam(learning_rate=parameters['LEARNING_RATE'])\ncae_model.compile(optimizer=opt, loss='binary_crossentropy')\n\ncae_model.fit(train_dataset, \n                epochs=parameters['EPOCHS'],\n                batch_size=parameters['BATCH_SIZE'],\n                shuffle=True,\n              \n              #validation_data=val_dataset\n             )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"encoding = cae_model.encoder.predict(train_dataset) # Generate the latent representation for each images","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_dataset_encoding = (tf.data.Dataset.from_tensor_slices((encoding, encoding))\n                        .batch(parameters['BATCH_SIZE'])\n                        .prefetch(AUTO)\n                    ) #make it a tensorflow dataset","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"section_three_three\"> </a>\n## 3.3 Autoencoder to reduce the dimensionality\n\nNow that we have our 1024 components vector of encoded images, these vectors are again encoded into a auto-encoder neural network to reduce the dimensionality. \n\nWhy do not preserve a vector with 1024 component for our next step of clustering ?\n\nOnly to reduce the costs of computing, storing and acquiring data.\n\nWhy not add layers to the initial convolutional auto-encoder for both get a latent representation of images and reduce dimensionality ?\n\nBecause, if I add convolutional layers to reduce dimensionality, I would have to use also pooling layers that makes the network lose informations in the encoding part. \n\nSo, why not use Dense layers ? When I am decoding the vecteur representation using a CAE with dense layers to reduce dimensionality, the loss is higher. \n\nSo I preferred to use twice autoencoder to get a vecteur represention of image and an other auto encoder to reduce dimensionality (perform better than PCA or t-SNE)."},{"metadata":{"trusted":true},"cell_type":"code","source":"class AutoEncoder(Model):\n  def __init__(self):\n    super(AutoEncoder, self).__init__()\n    self.encoder = tf.keras.Sequential([\n        tf.keras.layers.Dense(2048),\n        tf.keras.layers.LeakyReLU(alpha=0.3),\n        tf.keras.layers.Dense(64),\n        tf.keras.layers.LeakyReLU(alpha=0.3),\n\n        \n    ])\n    self.decoder = tf.keras.Sequential([\n        tf.keras.layers.Dense(512),\n        tf.keras.layers.LeakyReLU(alpha=0.3),\n        tf.keras.layers.Dense(2048),\n        tf.keras.layers.LeakyReLU(alpha=0.3),\n    ])\n\n  def call(self, x):\n    encoded = self.encoder(x)\n    decoded = self.decoder(encoded)\n    return decoded","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def scheduler(epoch, lr):\n  if epoch < 10:\n    return lr\n  else:\n    return lr * tf.math.exp(-0.1)\n\n\nautoencoder = AutoEncoder()\nlearningrate_scheduler = tf.keras.callbacks.LearningRateScheduler(scheduler)\nopt = tf.keras.optimizers.Adam(learning_rate=0.001)\nautoencoder.compile(optimizer=opt, loss=tf.keras.losses.MeanSquaredError())\n\nautoencoder.fit(train_dataset_encoding, \n                epochs=30,\n                batch_size=32,\n                shuffle=True,\n              callbacks = [learningrate_scheduler],\n              #validation_data=val_dataset\n             )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"encoded_images = autoencoder.encoder.predict(train_dataset_encoding)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"section_three_four\"></a>\n## 3.4 Ensemble clustering"},{"metadata":{"trusted":true},"cell_type":"code","source":"def match_self_dbscan(row):\n    if row['posting_id'] not in row['matches_dbscan']:\n        return [row['posting_id']] + row['matches_dbscan']\n    else:\n        return row['matches_dbscan']\ndef match_self_nn(row):\n    if row['posting_id'] not in row['matches_nn']:\n        return [row['posting_id']] + row['matches_nn']\n    else:\n        return row['matches_nn']\ndef match_self_optics(row):\n    if row['posting_id'] not in row['matches_optics']:\n        return [row['posting_id']] + row['matches_optics']\n    else:\n        return row['matches_optics']\ndef getMetric(col):\n    def f1score(row):\n        n = len(np.intersect1d(row.target, row[col]))\n        return 2 * n / (len(row.target) + len(row[col]))\n    return f1score\ndef match_self_ensembling(row):\n    max_len_matches = max([len(row['matches_dbscan']), len(row['matches_nn']), \n                           len(row['matches_optics'])])\n    if max_len_matches == len(row['matches_dbscan']):\n        return row['matches_dbscan']\n    elif max_len_matches == len(row['matches_nn']):\n        return row['matches_nn']\n    elif max_len_matches == len(row['matches_optics']):\n        return row['matches_optics']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Thanks to [Robert Tacbad](https://www.kaggle.com/jollibobert) for some piece of code."},{"metadata":{"trusted":true},"cell_type":"code","source":"eps_range = np.arange(0.001, 0.009, 0.001)\nlast_f1_dbscan = 1\nlast_f1_optics = 1\n\nfor eps in eps_range:\n    #dbscan\n    clustering_DBSCAN = DBSCAN(eps=eps, min_samples=2, metric='cosine').fit(encoded_images)\n    labels = clustering_DBSCAN.labels_\n    \n    train['clusters_dbscan'] = labels\n    clustered = (train['clusters_dbscan'] != -1)\n    tmp = train.loc[clustered].groupby('clusters_dbscan')['posting_id'].agg('unique').to_dict()\n    tmp[-1] = []\n    for key in tmp:\n        if len(tmp[key]) > 50:\n            tmp[key] = tmp[key][:50]  \n    train['matches_dbscan'] = train['clusters_dbscan'].map(tmp)\n    train['matches_dbscan'] = train.apply(match_self_dbscan, axis=1)\n    if train.apply(getMetric('matches_dbscan'), axis=1).mean() < last_f1_dbscan:\n        train['f1_dbscan'] = train.apply(getMetric('matches_dbscan'), axis=1)\n        last_f1_dbscan = train['f1_dbscan'].mean()\n    print(\"DBSCAN : \" + \"eps : \" + str(eps) + \" f1 : \" + str(train['f1_dbscan'].mean()))\n    \n    #optics\n    clustering_OPTICS = OPTICS(min_samples=2, metric='cosine', max_eps = eps).fit(encoded_images)\n    labels = clustering_OPTICS.labels_\n    train['clusters_optics'] = labels\n    clustered = (train['clusters_optics'] != -1)\n    tmp = train.loc[clustered].groupby('clusters_optics')['posting_id'].agg('unique').to_dict()\n    tmp[-1] = []\n    for key in tmp:\n        if len(tmp[key]) > 50:\n            tmp[key] = tmp[key][:50]  \n    train['matches_optics'] = train['clusters_optics'].map(tmp)\n    train['matches_optics'] = train.apply(match_self_optics, axis=1)\n    if train.apply(getMetric('matches_optics'), axis=1).mean() < last_f1_optics:\n        train['f1_optics'] = train.apply(getMetric('matches_optics'), axis=1)\n        last_f1_optics = train['f1_optics'].mean()\n    print(\"OPTICS : \" + \"eps : \" + str(eps) + \" f1 : \" + str(train['f1_optics'].mean()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"eps_range = np.arange(0.001, 0.009, 0.001)\nlast_f1_nn = 1\nfor distance in eps_range:\n    preds = []\n    model = NearestNeighbors(n_neighbors=50, metric = 'cosine')\n    model.fit(encoded_images)\n    distances, indices = model.kneighbors(encoded_images)\n    for k in range(len(encoded_images)):\n        IDX = np.where(distances[k,]<distance)[0]\n        IDS = indices[k,IDX]\n        o = train.iloc[IDS].posting_id.values\n        preds.append(o)\n    train['matches_nn'] = preds\n    train['matches_nn'] = train.apply(match_self_nn, axis=1)\n    if train.apply(getMetric('matches_nn'), axis=1).mean() < last_f1_nn:\n        train['f1_nn'] = train.apply(getMetric('matches_nn'), axis=1)\n        last_f1_nn = train['f1_nn'].mean()\n    print(\"eps : \" + str(distance) + \" f1 : \" + str(train['f1_nn'].mean()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['matches_ensemble'] = train.apply(match_self_ensembling, axis = 1)\ntrain['f1_ensembling'] = train.apply(getMetric('matches_ensemble'), axis=1)\nlast_f1_ensembling = train['f1_ensembling'].mean()\nprint(\"F1 TOTAL: \" + str(train['f1_ensembling'].mean()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"section_three_five\"></a>\n## 3.5 Visualization of results"},{"metadata":{"trusted":true},"cell_type":"code","source":"train['count_images_predicted'] = train.apply(lambda row : len(row['matches_ensemble']), axis = 1)\ntrain[train['count_images_predicted'] > 1].head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def query_image(filename_img):\n    row_image_query = train.query(\"image == @filename_img\")\n    similar_images = row_image_query['matches_ensemble'].iloc[0]\n\n    fig = plt.figure(figsize=(15, 15))\n\n    ax = fig.add_subplot(1, 4, 1)\n    img = mpimg.imread(DIR_TRAIN + filename_img)\n    imgplot = plt.imshow(img)\n    ax.set_title('Query image')\n    \n    ax = fig.add_subplot(1, 4, 2)\n    img = mpimg.imread(DIR_TRAIN + train.query(\"posting_id == @similar_images[0]\")['image'].iloc[0])\n    imgplot = plt.imshow(img)\n    ax.set_title('Similar image 1')\n    \n    if len(similar_images) > 1:\n        ax = fig.add_subplot(1, 4, 3)\n        img = mpimg.imread(DIR_TRAIN + train.query(\"posting_id == @similar_images[1]\")['image'].iloc[0])\n        imgplot = plt.imshow(img)\n        ax.set_title('Similar image 2')\n        \n    if len(similar_images) > 2:\n        ax = fig.add_subplot(1, 4, 4)\n        img = mpimg.imread(DIR_TRAIN + train.query(\"posting_id == @similar_images[2]\")['image'].iloc[0])\n        imgplot = plt.imshow(img)\n        ax.set_title('Similar image 3')\n\nfilename_img = \"002f978c58a44a00aadfca71c3cad2bb.jpg\"\nquery_image(filename_img)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"filename_img = \"004076b57135e761ab8b41d84acc4c94.jpg\"\nquery_image(filename_img)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"filename_img = \"0083bce179f59cdb2234bb2e621bf4b9.jpg\"\nquery_image(filename_img)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"section_four\"></a>\n# 4. Conclusion\n\nConvolutional Auto encoder are a good way to retrieve similar images. But I got a F1 Score of almost 0.56 which I think is not enough to get a robust model. A further improvement would be to use a CNN instead of using a Convolutional auto-encoder.\n\n"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}