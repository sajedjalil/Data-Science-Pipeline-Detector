{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install timm","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"A simple and easy to use fastai pipeline\n\nThe model definiition below can be modified to handle additional architectures that are available in Timm, but in it's current state will be able to handle efficientnets","metadata":{}},{"cell_type":"code","source":"from fastai.vision.all import *\nfrom sklearn.preprocessing import LabelEncoder\nimport timm\nfrom tqdm import tqdm\nfrom torch.nn import Parameter\nfrom sklearn.model_selection import StratifiedKFold","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_path = Path('../input/shopee-product-matching')\ntrain_path = data_path/'train_images'\ndata_path.ls()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df = pd.read_csv(data_path/'train.csv')\ntrain_path = data_path/'train_images'\ntrain_df['image'] = train_df['image'].apply(lambda x: os.path.join(train_path, x))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Since the dataset is imbalanced and we want to ensure that our model is not exposed to a sample in it's valudation that was not in the training set, we'll do the work to get validation indexes for a kfold scheme -- only the first kfold indexes will be used in this example, but the a training loop can be created pretty easily by walking down the list of validation indexes provided from the fxns below","metadata":{}},{"cell_type":"code","source":"def kfold_idxs(df, n_splits):\n    train_idx, val_idx = [], []\n    skf = StratifiedKFold(n_splits=n_splits, shuffle=True)\n    for train_index, valid_index in skf.split(df.image, df.label_group):\n        train_idx.append(train_index)\n        val_idx.append(valid_index)\n    return train_idx, val_idx\n\n\ndef get_val_idxs(df, n_splits):\n    _, val_idxs = kfold_idxs(df, n_splits)\n    return val_idxs","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"val_idxs = get_val_idxs(train_df, 2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"db = DataBlock(blocks = (ImageBlock, CategoryBlock),\n              get_x = ColReader('image'),\n              get_y = ColReader('label_group'),\n              splitter = IndexSplitter(val_idxs[0]),\n              item_tfms = Resize(256))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We have now defined our datablock -- this is the template our dataloader will use when creating batches","metadata":{}},{"cell_type":"code","source":"dls = db.dataloaders(train_df, bs=64)\ndls.show_batch(max_n=3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"Looks like our dataloader is functioning\n\nLets quickly grab an x and y batch in order to illustrate how our forward pass will function (once we've declared our model)","metadata":{}},{"cell_type":"code","source":"xb, yb = next(iter(dls.train))\nxb.shape, yb.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The model and ArcMarginProduct definition(s) below are borrowed from: https://www.kaggle.com/tanulsingh077/pytorch-metric-learning-pipeline-only-images (Pytorch) which was derived from https://www.kaggle.com/ragnar123/unsupervised-baseline-arcface/notebook (TF)\n\nmodified/trimmed down in order to more easily see what's being used in this implementation -- go check those out + head down the rabbit hole to see where they originally derived these from -- they (properly) references the papers/github repos etc","metadata":{}},{"cell_type":"code","source":"class ArcFaceNet(nn.Module):\n    def __init__(self,\n                 n_classes, model_name='efficientnet_b0', s=30.0, \n                 margin=0.50, ls_eps=0.0, theta_zero=0.785, pretrained=True):\n\n        super(ArcFaceNet, self).__init__()\n        print('Building Model Backbone for {} model'.format(model_name))\n\n        self.backbone = timm.create_model(model_name, pretrained=pretrained)\n        final_in_features = self.backbone.classifier.in_features\n        self.backbone.classifier = nn.Identity() #could also redefine model without classifier\n        self.final = ArcMarginProduct(final_in_features, n_classes,\n                                      s=s, m=margin, easy_margin=False, ls_eps=ls_eps)\n\n    def forward(self, x, label):\n        feature = self.backbone(x)\n        return self.final(feature, label)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ArcMarginProduct(nn.Module):\n    r\"\"\"Implement of large margin arc distance: :\n        Args:\n            in_features: size of each input sample\n            out_features: size of each output sample\n            s: norm of input feature\n            m: margin\n            cos(theta + m)\n        \"\"\"\n    def __init__(self, in_features, out_features, s=30.0, m=0.50, easy_margin=False, ls_eps=0.0):\n        super(ArcMarginProduct, self).__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.s = s\n        self.m = m\n        self.ls_eps = ls_eps  # label smoothing\n        self.weight = nn.Parameter(torch.FloatTensor(out_features, in_features)).to('cuda')\n        nn.init.xavier_uniform_(self.weight)\n\n        self.easy_margin = easy_margin\n        self.cos_m = math.cos(m)\n        self.sin_m = math.sin(m)\n        self.th = math.cos(math.pi - m)\n        self.mm = math.sin(math.pi - m) * m\n\n    def forward(self, input, label):\n        # --------------------------- cos(theta) & phi(theta) ---------------------------\n        cosine = F.linear(F.normalize(input), F.normalize(self.weight))\n        sine = torch.sqrt(1.0 - torch.pow(cosine,2)).to(cosine.dtype) #needed for to_fp16()\n        phi = cosine * self.cos_m - sine * self.sin_m\n        if self.easy_margin:\n            phi = torch.where(cosine > 0, phi, cosine)\n        else:\n            phi = torch.where(cosine > self.th, phi, cosine - self.mm)\n        # --------------------------- convert label to one-hot ---------------------------\n        one_hot = torch.zeros(cosine.size(), device='cuda')\n        one_hot.scatter_(1, label.view(-1, 1).long(), 1)\n        if self.ls_eps > 0:\n            one_hot = (1 - self.ls_eps) * one_hot + self.ls_eps / self.out_features\n        # -------------torch.where(out_i = {x_i if condition_i else y_i) -------------\n        output = (one_hot * phi) + ((1.0 - one_hot) * cosine)\n        output *= self.s\n\n        return output","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = ArcFaceNet(11014).to('cuda');","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#let's make sure our fastai dataloader can do a fwd pass on our x and y batches\nmodel(xb, yb).shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#double check that our loss function will work\nCrossEntropyLossFlat()(model(xb,yb), yb)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"You can uncomment the lines below and see the error that is thrown by just instantiating a learner in the typical fashion","metadata":{}},{"cell_type":"code","source":"#learner = Learner(dls, ArcFaceNet(11014, 'efficientnet_b0'),\n#                  loss_func=CrossEntropyLossFlat(), metrics=accuracy)#.to_fp16()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Since our learner object normally expects the model to only have one batch in the x forward pass -- we can either manipulate what is defined as x in our datablock/dataloader (ehhhh) -- or change it with a call back during our training loop --> this latter method is what we'll use\n\nFastai callbacks: https://docs.fast.ai/callback.core.html","metadata":{}},{"cell_type":"code","source":"class AmpCallback(Callback):\n    def before_batch(self):\n        self.learn.xb = (self.x, self.y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Three lines of code!!! :)","metadata":{}},{"cell_type":"code","source":"learner = Learner(dls, ArcFaceNet(11014, 'efficientnet_b0'),\n                  loss_func=CrossEntropyLossFlat(), \n                  cbs=[AmpCallback],\n                 metrics=accuracy).to_fp16()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"learner.fine_tune(3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There's a ton of modifications that can be made from data augmentation, to learning rate schduling to mixup etc","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}