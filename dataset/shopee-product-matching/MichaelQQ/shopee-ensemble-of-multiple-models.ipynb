{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Try\n\nwe can try & experiment here to combine different models here..\n\npredictions = [list(set(a + b + c + d)) for a, b, c, d in zip(predictions1, predictions2,predictions3, predictions4)]\n\n ","metadata":{}},{"cell_type":"code","source":"import sys\nsys.path.append('../input/timm-pytorch-image-models/pytorch-image-models-master')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%capture\n\n# Dirty code to make it work\n\nimport sys\n!cp -r ../input/openai-clip/CLIP/CLIP-main /tmp/\n\n# Kaggle likes to unpack .gz files in datasets... so we have to pack it back\n!gzip -c /tmp/CLIP-main/clip/bpe_simple_vocab_16e6.txt > /tmp/CLIP-main/clip/bpe_simple_vocab_16e6.txt.gz\nsys.path.append('/tmp/CLIP-main')\n\n!pip install ../input/openai-clip/ftfy-5.9/ftfy-5.9 \\\n             ../input/openai-clip/torch-1.7.1+cu110-cp37-cp37m-linux_x86_64.whl \\\n             ../input/openai-clip/torchvision-0.8.2+cu110-cp37-cp37m-linux_x86_64.whl \\\n             ../input/faiss-163/faiss_gpu-1.6.3-cp37-cp37m-manylinux2010_x86_64.whl","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \n\nimport math\nimport random \nimport os \nimport cv2\nimport timm\n\nfrom tqdm import tqdm \n\nimport albumentations as A \nfrom albumentations.pytorch.transforms import ToTensorV2\n\nimport torch \nfrom torch.utils.data import Dataset \nfrom torch import nn\nimport torch.nn.functional as F \n\nimport gc\nimport cudf\nimport cuml\nimport cupy\nfrom cuml.feature_extraction.text import TfidfVectorizer\nfrom cuml.neighbors import NearestNeighbors\nfrom sklearn.preprocessing import normalize","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CFG:\n    \n    img_size = 512\n    fc_dim = 512\n    batch_size = 12\n    seed = 2020\n    \n    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n    classes = 11014\n    \n    model_name = 'tf_efficientnet_b5_ns'\n#     model_name1 =  'tf_efficientnet_b4'\n    model_name2 = 'eca_nfnet_l0'\n    model_name3 = 'efficientnet_b3'\n    \n    model_path = '../input/shopee-pytorch-models/arcface_512x512_eff_b5_.pt'\n#     model_path1 = '../input/utils-shopee/arcface_512x512_tf_efficientnet_b4_LR.pt'\n    model_path2 = '../input/shopee-pytorch-models/arcface_512x512_nfnet_l0 (mish).pt'\n    model_path3 = '../input/arcfacebin/arcface04.bin'\n    \n    scale = 30 \n    margin = 0.5","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def read_dataset():\n    df = pd.read_csv('../input/shopee-product-matching/test.csv')\n    df_cu = cudf.DataFrame(df)\n    image_paths = '../input/shopee-product-matching/test_images/' + df['image']\n    return df, df_cu, image_paths","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def seed_torch(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    \nseed_torch(CFG.seed)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def combine_predictions(row):\n    x = np.concatenate([row['image_predictions'], row['text_predictions'], row['phash'], row['pred_matches02']])\n    return ' '.join( np.unique(x))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_image_predictions(df, embeddings1, embeddings3, embeddings4, threshold = 3.4):\n    \n    if len(df) > 3:\n        KNN = 50\n    else : \n        KNN = 3\n    \n    #--01\n    model = NearestNeighbors(n_neighbors = KNN)\n    model.fit(embeddings1)\n    distances, indices = model.kneighbors(embeddings1)\n    \n    threshold = 1.7\n    predictions1 = []\n    for k in tqdm(range(embeddings1.shape[0])):\n        idx = np.where(distances[k,] < threshold)[0]\n        ids = indices[k,idx]\n        posting_ids = list(df['posting_id'].iloc[ids])\n        predictions1.append(posting_ids)\n        \n    del model, distances, indices, embeddings1\n    gc.collect()\n\n#     #--02\n#     model = NearestNeighbors(n_neighbors = KNN)\n#     model.fit(embeddings2)\n#     distances, indices = model.kneighbors(embeddings2)\n    \n#     threshold = 4.5\n#     predictions2 = []\n#     for k in tqdm(range(embeddings2.shape[0])):\n#         idx = np.where(distances[k,] < threshold)[0]\n#         ids = indices[k,idx]\n#         posting_ids = list(df['posting_id'].iloc[ids])\n#         predictions2.append(posting_ids)\n        \n#     del model, distances, indices, embeddings2\n#     gc.collect()\n    \n    #--03\n    model = NearestNeighbors(n_neighbors = KNN, metric = 'cosine')\n    model.fit(embeddings3)\n    distances, indices = model.kneighbors(embeddings3)\n    \n    threshold=0.36\n    predictions3 = []\n    for k in tqdm(range(embeddings3.shape[0])):\n        idx = np.where(distances[k,] < threshold)[0]\n        ids = indices[k,idx]\n        posting_ids = list(df['posting_id'].iloc[ids])\n        predictions3.append(posting_ids)\n        \n    del model, distances, indices, embeddings3\n    gc.collect()\n    \n    #--04 ################################################ new ###############################\n    model = NearestNeighbors(n_neighbors = KNN)\n    model.fit(embeddings4)\n    distances, indices = model.kneighbors(embeddings4)\n    \n#     threshold=2.9\n    predictions4 = []\n    for k in tqdm(range(embeddings4.shape[0])):\n        idx = np.where(distances[k,] < 2.9)[0]\n        ids = indices[k,idx]\n        posting_ids = list(df['posting_id'].iloc[ids])\n        predictions4.append(posting_ids)\n        \n    del model, distances, indices, embeddings4\n    gc.collect()\n    # combine predictions(i.e. image IDs) of all the models & remove the duplicates.\n    \n    \n    # we can try & experiment here to combine different models here..\n#     predictions = [list(set(a + c)) for a, b, c, d in zip(predictions1, predictions2, predictions3, predictions4)]\n    predictions = [list(a + c + d) for a, c, d in zip(predictions1, predictions3, predictions4)]\n    return predictions","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_test_transforms():\n\n    return A.Compose(\n        [\n            A.Resize(CFG.img_size,CFG.img_size,always_apply=True),\n            A.Normalize(),\n        ToTensorV2(p=1.0)\n        ]\n    )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ShopeeDataset(Dataset):\n    def __init__(self, image_paths, transforms=None):\n\n        self.image_paths = image_paths\n        self.augmentations = transforms\n\n    def __len__(self):\n        return self.image_paths.shape[0]\n\n    def __getitem__(self, index):\n        image_path = self.image_paths[index]\n        \n        image = cv2.imread(image_path)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        \n        if self.augmentations:\n            augmented = self.augmentations(image=image)\n            image = augmented['image']       \n    \n        return image,torch.tensor(1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ArcMarginProduct(nn.Module):\n    def __init__(self, in_features, out_features, scale=30.0, margin=0.50, easy_margin=False, ls_eps=0.0):\n        super(ArcMarginProduct, self).__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.scale = scale\n        self.margin = margin\n        self.ls_eps = ls_eps  # label smoothing\n        self.weight = nn.Parameter(torch.FloatTensor(out_features, in_features))\n        nn.init.xavier_uniform_(self.weight)\n\n        self.easy_margin = easy_margin\n        self.cos_m = math.cos(margin)\n        self.sin_m = math.sin(margin)\n        self.th = math.cos(math.pi - margin)\n        self.mm = math.sin(math.pi - margin) * margin\n        \n    def forward(self, input, label):\n        # --------------------------- cos(theta) & phi(theta) ---------------------------\n        cosine = F.linear(F.normalize(input), F.normalize(self.weight))\n        sine = torch.sqrt(1.0 - torch.pow(cosine, 2))\n        phi = cosine * self.cos_m - sine * self.sin_m\n        if self.easy_margin:\n            phi = torch.where(cosine > 0, phi, cosine)\n        else:\n            phi = torch.where(cosine > self.th, phi, cosine - self.mm)\n        # --------------------------- convert label to one-hot ---------------------------\n        # one_hot = torch.zeros(cosine.size(), requires_grad=True, device='cuda')\n        one_hot = torch.zeros(cosine.size(), device='cuda')\n        one_hot.scatter_(1, label.view(-1, 1).long(), 1)\n        if self.ls_eps > 0:\n            one_hot = (1 - self.ls_eps) * one_hot + self.ls_eps / self.out_features\n        # -------------torch.where(out_i = {x_i if condition_i else y_i) -------------\n        output = (one_hot * phi) + ((1.0 - one_hot) * cosine)\n        output *= self.scale\n\n        return output\n\nclass ShopeeModel(nn.Module):\n\n    def __init__(\n        self,\n        n_classes = CFG.classes,\n        model_name = CFG.model_name,\n        fc_dim = 512,\n        margin = CFG.margin,\n        scale = CFG.scale,\n        use_fc = False,\n        pretrained = False):\n\n\n        super(ShopeeModel,self).__init__()\n        print('Building Model Backbone for {} model'.format(model_name))\n\n        self.backbone = timm.create_model(model_name, pretrained=pretrained)\n\n        if model_name == 'resnext50_32x4d':\n            final_in_features = self.backbone.fc.in_features\n            self.backbone.fc = nn.Identity()\n            self.backbone.global_pool = nn.Identity()\n\n        elif model_name == 'efficientnet_b3':\n            final_in_features = self.backbone.classifier.in_features\n            self.backbone.classifier = nn.Identity()\n            self.backbone.global_pool = nn.Identity()\n\n        elif model_name == 'tf_efficientnet_b5_ns':\n            final_in_features = self.backbone.classifier.in_features\n            self.backbone.classifier = nn.Identity()\n            self.backbone.global_pool = nn.Identity()\n        \n        elif model_name == 'nfnet_f3':\n            final_in_features = self.backbone.head.fc.in_features\n            self.backbone.head.fc = nn.Identity()\n            self.backbone.head.global_pool = nn.Identity()\n\n        self.pooling =  nn.AdaptiveAvgPool2d(1)\n\n        self.use_fc = use_fc\n\n        self.dropout = nn.Dropout(p=0.0)\n        self.fc = nn.Linear(final_in_features, fc_dim)\n        self.bn = nn.BatchNorm1d(fc_dim)\n        self._init_params()\n        final_in_features = fc_dim\n\n        self.final = ArcMarginProduct(\n            final_in_features,\n            n_classes,\n            scale = scale,\n            margin = margin,\n            easy_margin = False,\n            ls_eps = 0.0\n        )\n\n    def _init_params(self):\n        nn.init.xavier_normal_(self.fc.weight)\n        nn.init.constant_(self.fc.bias, 0)\n        nn.init.constant_(self.bn.weight, 1)\n        nn.init.constant_(self.bn.bias, 0)\n\n    def forward(self, image, label):\n        feature = self.extract_feat(image)\n        #logits = self.final(feature,label)\n        return feature\n\n    def extract_feat(self, x):\n        batch_size = x.shape[0]\n        x = self.backbone(x)\n        x = self.pooling(x).view(batch_size, -1)\n\n        if self.use_fc:\n            x = self.dropout(x)\n            x = self.fc(x)\n            x = self.bn(x)\n        return x\n    \n    \n# class ShopeeModel1(nn.Module):\n#     def __init__(\n#         self,\n#         n_classes = CFG.classes,\n#         model_name = CFG.model_name1,\n#         fc_dim = CFG.fc_dim,\n#         margin = CFG.margin,\n#         scale = CFG.scale,\n#         use_fc = True,\n#         pretrained = True):\n\n#         super(ShopeeModel1, self).__init__()\n#         print('Building Model Backbone for {} model'.format(model_name))\n\n#         self.backbone = timm.create_model(model_name, pretrained=pretrained)\n#         in_features = self.backbone.classifier.in_features\n#         self.backbone.classifier = nn.Identity()\n#         self.backbone.global_pool = nn.Identity()\n#         self.pooling =  nn.AdaptiveAvgPool2d(1)\n#         self.use_fc = use_fc\n\n#         if use_fc:\n#             self.dropout = nn.Dropout(p=0.1)\n#             self.classifier = nn.Linear(in_features, fc_dim)\n#             self.bn = nn.BatchNorm1d(fc_dim)\n#             self._init_params()\n#             in_features = fc_dim\n\n#         self.final = ArcMarginProduct(\n#             in_features,\n#             n_classes,\n#             scale = scale,\n#             margin = margin,\n#             easy_margin = False,\n#             ls_eps = 0.0\n#         )\n\n#     def _init_params(self):\n#         nn.init.xavier_normal_(self.classifier.weight)\n#         nn.init.constant_(self.classifier.bias, 0)\n#         nn.init.constant_(self.bn.weight, 1)\n#         nn.init.constant_(self.bn.bias, 0)\n\n#     def forward(self, image, label):\n#         features = self.extract_features(image)\n#         if self.training:\n#             logits = self.final(features, label)\n#             return logits\n#         else:\n#             return features\n\n#     def extract_features(self, x):\n#         batch_size = x.shape[0]\n#         x = self.backbone(x)\n#         x = self.pooling(x).view(batch_size, -1)\n\n#         if self.use_fc and self.training:\n#             x = self.dropout(x)\n#             x = self.classifier(x)\n#             x = self.bn(x)\n#         return x\n    \nclass ShopeeModel2(nn.Module):\n\n    def __init__(\n        self,\n        n_classes = CFG.classes,\n        model_name = CFG.model_name2,\n        fc_dim = 512,\n        margin = CFG.margin,\n        scale = CFG.scale,\n        use_fc = True,\n        pretrained = False):\n\n\n        super(ShopeeModel2,self).__init__()\n        print('Building Model Backbone for {} model'.format(model_name))\n\n        self.backbone = timm.create_model(model_name, pretrained=pretrained)\n\n        if model_name == 'resnext50_32x4d':\n            final_in_features = self.backbone.fc.in_features\n            self.backbone.fc = nn.Identity()\n            self.backbone.global_pool = nn.Identity()\n\n        elif model_name == 'efficientnet_b3':\n            final_in_features = self.backbone.classifier.in_features\n            self.backbone.classifier = nn.Identity()\n            self.backbone.global_pool = nn.Identity()\n\n        elif model_name == 'tf_efficientnet_b5_ns':\n            final_in_features = self.backbone.classifier.in_features\n            self.backbone.classifier = nn.Identity()\n            self.backbone.global_pool = nn.Identity()\n        \n        elif model_name == 'eca_nfnet_l0':\n            final_in_features = self.backbone.head.fc.in_features\n            self.backbone.head.fc = nn.Identity()\n            self.backbone.head.global_pool = nn.Identity()\n\n        self.pooling =  nn.AdaptiveAvgPool2d(1)\n\n        self.use_fc = use_fc\n\n        self.dropout = nn.Dropout(p=0.0)\n        self.fc = nn.Linear(final_in_features, fc_dim)\n        self.bn = nn.BatchNorm1d(fc_dim)\n        self._init_params()\n        final_in_features = fc_dim\n\n        self.final = ArcMarginProduct(\n            final_in_features,\n            n_classes,\n            scale = scale,\n            margin = margin,\n            easy_margin = False,\n            ls_eps = 0.0\n        )\n\n    def _init_params(self):\n        nn.init.xavier_normal_(self.fc.weight)\n        nn.init.constant_(self.fc.bias, 0)\n        nn.init.constant_(self.bn.weight, 1)\n        nn.init.constant_(self.bn.bias, 0)\n\n    def forward(self, image, label):\n        feature = self.extract_feat(image)\n        #logits = self.final(feature,label)\n        return feature\n\n    def extract_feat(self, x):\n        batch_size = x.shape[0]\n        x = self.backbone(x)\n        x = self.pooling(x).view(batch_size, -1)\n\n        if self.use_fc:\n            x = self.dropout(x)\n            x = self.fc(x)\n            x = self.bn(x)\n        return x\n################################ new ##################################\nclass ShopeeModel3(nn.Module):\n\n    def __init__(self,\n                 n_classes = 10873,  #<<<<<<<______________________________corrected classes\n                 model_name=CFG.model_name3,\n                 use_fc=False,\n                 fc_dim=512,\n                 dropout=0.0,\n                 loss_module='softmax',#<<<<<<<______________________________\n                 s=30.0,\n                 margin=0.50,\n                 ls_eps=0.0,\n                 theta_zero=0.785,\n                 pretrained=False):\n        \"\"\"\n        :param n_classes:\n        :param model_name: name of model from pretrainedmodels\n            e.g. resnet50, resnext101_32x4d, pnasnet5large\n        :param pooling: One of ('SPoC', 'MAC', 'RMAC', 'GeM', 'Rpool', 'Flatten', 'CompactBilinearPooling')\n        :param loss_module: One of ('arcface', 'cosface', 'softmax')\n        \"\"\"\n        super(ShopeeModel3, self).__init__()\n        print('Model building for {} backbone'.format(model_name))\n\n        self.backbone = timm.create_model(model_name, pretrained=pretrained)\n        final_in_features = self.backbone.classifier.in_features\n        \n        self.backbone.classifier = nn.Identity()\n        self.backbone.global_pool = nn.Identity()\n        \n        self.pooling =  nn.AdaptiveAvgPool2d(1)\n            \n        self.use_fc = use_fc\n        if use_fc:\n            self.dropout = nn.Dropout(p=dropout)\n            self.fc = nn.Linear(final_in_features, fc_dim)\n            self.bn = nn.BatchNorm1d(fc_dim)\n            self._init_params()\n            final_in_features = fc_dim\n\n        self.loss_module = loss_module\n        if loss_module == 'arcface':\n            self.final = ArcMarginProduct(final_in_features, n_classes,\n                                          scale=s, margin=margin, easy_margin=False, ls_eps=ls_eps)\n        elif loss_module == 'cosface':\n            self.final = AddMarginProduct(final_in_features, n_classes, scale=s, margin=margin)\n        elif loss_module == 'adacos':\n            self.final = AdaCos(final_in_features, n_classes, margin=margin, theta_zero=theta_zero)\n        else:\n            self.final = nn.Linear(final_in_features, n_classes)\n\n    def _init_params(self):\n        nn.init.xavier_normal_(self.fc.weight)\n        nn.init.constant_(self.fc.bias, 0)\n        nn.init.constant_(self.bn.weight, 1)\n        nn.init.constant_(self.bn.bias, 0)\n\n    def forward(self, x, label):\n        feature = self.extract_feat(x)\n        if self.loss_module in ('arcface', 'cosface', 'adacos'):\n            logits = self.final(feature, label)\n        else:\n            logits = self.final(feature)\n        return feature,logits\n\n    def extract_feat(self, x):\n        batch_size = x.shape[0]\n        x = self.backbone(x)\n        x = self.pooling(x).view(batch_size, -1)\n\n        if self.use_fc:\n            x = self.dropout(x)\n            x = self.fc(x)\n            x = self.bn(x)\n\n        return x","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# https://www.kaggle.com/parthdhameliya77/pytorch-eca-nfnet-l0-image-tfidf-inference\nclass Mish_func(torch.autograd.Function):\n    \n    \"\"\"from: https://github.com/tyunist/memory_efficient_mish_swish/blob/master/mish.py\"\"\"\n    \n    @staticmethod\n    def forward(ctx, i):\n        result = i * torch.tanh(F.softplus(i))\n        ctx.save_for_backward(i)\n        return result\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        i = ctx.saved_variables[0]\n  \n        v = 1. + i.exp()\n        h = v.log() \n        grad_gh = 1./h.cosh().pow_(2) \n\n        # Note that grad_hv * grad_vx = sigmoid(x)\n        #grad_hv = 1./v  \n        #grad_vx = i.exp()\n        \n        grad_hx = i.sigmoid()\n\n        grad_gx = grad_gh *  grad_hx #grad_hv * grad_vx \n        \n        grad_f =  torch.tanh(F.softplus(i)) + i * grad_gx \n        \n        return grad_output * grad_f \n\n\nclass Mish(nn.Module):\n    def __init__(self, **kwargs):\n        super().__init__()\n        pass\n    def forward(self, input_tensor):\n        return Mish_func.apply(input_tensor)\n\n\ndef replace_activations(model, existing_layer, new_layer):\n    \n    \"\"\"A function for replacing existing activation layers\"\"\"\n    \n    for name, module in reversed(model._modules.items()):\n        if len(list(module.children())) > 0:\n            model._modules[name] = replace_activations(module, existing_layer, new_layer)\n\n        if type(module) == existing_layer:\n            layer_old = module\n            layer_new = new_layer\n            model._modules[name] = layer_new\n    return model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_image_embeddings(image_paths, model_name = CFG.model_name):\n    \n    #---00\n    embeds = []\n    \n    model = ShopeeModel(model_name = model_name)\n    model.eval()\n    model.load_state_dict(torch.load(CFG.model_path))\n    model = model.to(CFG.device)\n\n    image_dataset = ShopeeDataset(image_paths=image_paths,transforms=get_test_transforms())\n    image_loader = torch.utils.data.DataLoader(\n        image_dataset,\n        batch_size=CFG.batch_size,\n        pin_memory=True,\n        drop_last=False,\n        num_workers=4\n    )\n    \n    with torch.no_grad():\n        for img,label in tqdm(image_loader): \n            img = img.cuda()\n            label = label.cuda()\n            feat = model(img,label)\n            image_embeddings = feat.detach().cpu().numpy()\n            embeds.append(image_embeddings)\n    \n    del model, image_embeddings\n    image_embeddings1 = np.concatenate(embeds)\n#     image_embeddings1 = normalize(image_embeddings1)\n    print(f'image embeddings1 shape is {image_embeddings1.shape}')\n    del embeds\n    gc.collect()\n    \n#     #---01\n#     model = ShopeeModel1(pretrained=False).to(CFG.device)\n#     model.load_state_dict(torch.load(CFG.model_path1))\n#     model.eval()\n\n#     image_dataset = ShopeeDataset(image_paths=image_paths, transforms=get_test_transforms())\n#     image_loader = torch.utils.data.DataLoader(\n#         image_dataset,\n#         batch_size=CFG.batch_size,\n#         num_workers=4\n#     )\n\n#     embeds1 = []\n#     with torch.no_grad():\n#         for img,label in tqdm(image_loader): \n#             img = img.cuda()\n#             label = label.cuda()\n#             features = model(img,label)\n#             image_embeddings = features.detach().cpu().numpy()\n#             embeds1.append(image_embeddings)\n\n#     del model, image_embeddings\n#     image_embeddings2 = np.concatenate(embeds1)\n#     print(f'image embeddings2 shape is {image_embeddings2.shape}')\n#     del embeds1\n#     gc.collect()\n    \n    #---02\n    model = ShopeeModel2()\n    model.eval()\n    model = replace_activations(model, torch.nn.SiLU, Mish())\n\n    model.load_state_dict(torch.load(CFG.model_path2))\n    model = model.to(CFG.device)\n    \n    image_dataset = ShopeeDataset(image_paths=image_paths,transforms=get_test_transforms())\n    image_loader = torch.utils.data.DataLoader(\n        image_dataset,\n        batch_size=CFG.batch_size,\n        pin_memory=True,\n        drop_last=False,\n        num_workers=4\n    )\n    \n    embeds2 = []\n    with torch.no_grad():\n        for img,label in tqdm(image_loader): \n            img = img.cuda()\n            label = label.cuda()\n            feat = model(img,label)\n            image_embeddings = feat.detach().cpu().numpy()\n            embeds2.append(image_embeddings)\n    del model\n    image_embeddings3 = np.concatenate(embeds2)\n#     image_embeddings3 = normalize(image_embeddings3)\n\n    print(f'image embeddings3 shape is {image_embeddings3.shape}')\n    del embeds2\n    gc.collect()\n    \n    #---03 ################################ new ####################################   \n    embeds3 = []\n    model = ShopeeModel3()\n    model.eval()\n    model.load_state_dict(torch.load(CFG.model_path3),strict=False)\n    model = model.to(CFG.device)\n\n    image_dataset = ShopeeDataset(image_paths=image_paths,transforms=get_test_transforms())\n    image_loader = torch.utils.data.DataLoader(\n        image_dataset,\n        batch_size=CFG.batch_size,\n        pin_memory=True,\n        drop_last=False,\n        num_workers=4\n    )\n    \n    with torch.no_grad():\n        for img,label in tqdm(image_loader): \n            img = img.cuda()\n            label = label.cuda()\n            feat, _ = model(img,label)\n            image_embeddings = feat.detach().cpu().numpy()\n            embeds3.append(image_embeddings)\n    \n    del model\n    image_embeddings4 = np.concatenate(embeds3)\n#     image_embeddings4 = normalize(image_embeddings4)\n    print(f'Our image embeddings4 shape is {image_embeddings4.shape}')\n    del embeds3\n    gc.collect()\n    \n    \n    \n    #---\n    \n    # Method 1 of ensembling\n    # image_embeddings = np.concatenate([image_embeddings1, image_embeddings2, image_embeddings3], 1)\n    \n    return image_embeddings1, image_embeddings3, image_embeddings4","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#https://www.kaggle.com/cdeotte/part-2-rapids-tfidfvectorizer-cv-0-700#Use-Text-Embeddings\n\ndef get_text_predictions(df, max_features = 25_000):\n    \n    model = TfidfVectorizer(stop_words = 'english', binary = True, max_features = max_features)\n    text_embeddings = model.fit_transform(df_cu['title']).toarray()\n    preds = []\n    CHUNK = 1024*4\n\n    print('Finding similar titles...')\n    CTS = len(df)//CHUNK\n    if len(df)%CHUNK!=0: CTS += 1\n    for j in range( CTS ):\n\n        a = j*CHUNK\n        b = (j+1)*CHUNK\n        b = min(b,len(df))\n        print('chunk',a,'to',b)\n\n        # COSINE SIMILARITY DISTANCE\n        cts = cupy.matmul( text_embeddings, text_embeddings[a:b].T).T\n\n        for k in range(b-a):\n            IDX = cupy.where(cts[k,]>0.75)[0]\n            o = df.iloc[cupy.asnumpy(IDX)].posting_id.values\n            preds.append(o.tolist())\n    \n    del model,text_embeddings\n    gc.collect()\n    return preds","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df,df_cu,image_paths = read_dataset()\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image_embeddings1, image_embeddings3, image_embeddings4 = get_image_embeddings(image_paths.values)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image_predictions = get_image_predictions(df, image_embeddings1, image_embeddings3, image_embeddings4, threshold = 1.7)\ntext_predictions = get_text_predictions(df, max_features = 25_000)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# image predictinon & text_prediction","metadata":{}},{"cell_type":"code","source":"df['image_predictions'] = image_predictions\ndf['text_predictions'] = text_predictions","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# phash","metadata":{}},{"cell_type":"code","source":"phash = df.groupby('image_phash').posting_id.agg('unique').to_dict()\ndf['phash'] = df.image_phash.map(phash)\ndf['phash'] = df.phash.apply(lambda x: x.tolist())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test = df.copy()\ndf.head(3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = df[['posting_id', 'image_predictions', 'text_predictions', 'phash']]\ndf.head(2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# #################################################################################################","metadata":{}},{"cell_type":"markdown","source":"# clipAI","metadata":{}},{"cell_type":"code","source":"df_test.set_index('posting_id', inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test.head(3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import Dataset, DataLoader\nfrom torch.utils.data.sampler import Sampler\nimport clip\nfrom PIL import Image\nfrom pathlib import Path\nfrom tqdm.auto import tqdm\nimport re\nfrom clip.simple_tokenizer import SimpleTokenizer\nimport faiss\nimport matplotlib.pyplot as plt\n# from triplet_loss import TripletLoss\n\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"_tokenizer = SimpleTokenizer()\n\n# Copied from https://github.com/openai/CLIP/blob/beba48f35392a73c6c47ae67ddffced81ad1916d/clip/clip.py#L164\n# but with relaxed exception\ndef tokenize(texts, context_length: int = 77) -> torch.LongTensor:\n    if isinstance(texts, str):\n        texts = [texts]\n\n    sot_token = _tokenizer.encoder[\"<|startoftext|>\"]\n    eot_token = _tokenizer.encoder[\"<|endoftext|>\"]\n    all_tokens = [[sot_token] + _tokenizer.encode(text) + [eot_token] for text in texts]\n    result = torch.zeros(len(all_tokens), context_length, dtype=torch.long)\n\n    for i, tokens in enumerate(all_tokens):\n        n = min(len(tokens), context_length)\n        result[i, :n] = torch.tensor(tokens)[:n]\n        if len(tokens) > context_length:\n            result[i, -1] = tokens[-1]\n\n    return result","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def l2_normalize(features):\n    return features / features.norm(2, dim=1, keepdim=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Remove EMOJI\nRE_EMOJI = re.compile(r\"\\\\x[A-Za-z0-9./]+\", flags=re.UNICODE)\n\ndef strip_emoji(text):\n    return RE_EMOJI.sub(r'', text)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class RollingMean():\n    def __init__(self):\n        self.n = 0\n        self.mean = 0\n        \n    def update(self, value):\n        self.mean = (self.mean * self.n + value) / (self.n+1)\n        self.n += 1\n        \n    def result(self):\n        return self.mean","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class SameGroupSampler(Sampler):\n    def __init__(self, df ,ds):\n        super().__init__(ds)\n        \n        # Create a dictionary of posting_id -> index in dataset\n        self.index_to_position = dict(zip(df.index, range(len(df))))\n        \n        # Create a Series of label_group -> set(posting_id)\n        self.label_group = df.reset_index().groupby('label_group')['posting_id'].apply(set).map(sorted).map(np.array)\n\n    def __len__(self):\n        return len(self.label_group)\n        \n    def __iter__(self):\n        for _ in range(len(self)):\n            # Sample one label_group\n            label_group_sample = self.label_group.sample(1).iloc[0]\n            \n            # Sample two posting_id's\n            sample1, sample2 = np.random.choice(label_group_sample, 2, replace=False)\n            \n            yield self.index_to_position[sample1]\n            yield self.index_to_position[sample2] ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class MyDataset(Dataset):\n    def __init__(self, df, images_path, preprocess):\n        super().__init__()\n        self.df = df\n        self.images_path = images_path\n        self.preprocess = preprocess\n        self.has_target = ('label_group' in df)\n        \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        \n        image = self.preprocess(Image.open(self.images_path / row['image']))\n        text = tokenize([strip_emoji(row['title'])])[0]\n        \n        if self.has_target:\n            return image, text, row['label_group']\n        else:\n            return image, text, 0","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = \"cuda\" if torch.cuda.is_available() else \"cpu\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CLIPClassifier(nn.Module):\n    def __init__(self, n_classes):\n        super().__init__()\n        self.clip, self.preprocess = clip.load(\"../input/openai-clip/ViT-B-32.pt\", device='cpu', jit=False)\n        \n        self.embed_dim = self.clip.text_projection.shape[1]\n    \n        self.classification_head = nn.Linear(self.embed_dim, n_classes)\n    \n    def forward(self, images, texts, return_classification=False):\n        images_features = self.clip.encode_image(images)\n        texts_features = self.clip.encode_text(texts)\n\n        # Average images and text features, because CLIP was trained to align them\n        features = l2_normalize(l2_normalize(images_features) + l2_normalize(texts_features))\n\n        if return_classification:\n            classification_output = self.classification_head(features)\n            \n            return features, classification_output\n        else:\n            return features","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def find_similarities_and_indexes(df, images_path, top_n=100, features_file=None):\n    # Create pytorch Dataset/DataLoader\n    ds = MyDataset(df, images_path, model.preprocess)\n    dl = DataLoader(ds, batch_size=32, shuffle=False, num_workers=2)\n\n    # Allocate memory for features\n    features = np.empty((len(df), model.embed_dim), dtype=np.float32)\n\n    # Begin predict\n    i = 0\n    for images, texts, _ in tqdm(dl):\n        n = len(images)\n        with torch.no_grad():\n            # Generate image and text features\n            batch_features = model(images.to(device), texts.to(device), return_classification=False)\n\n        # Average images and text features, because CLIP was trained to align them\n        features[i:i+n] = batch_features.cpu()\n\n        i += n\n\n    # Option to save these features (may be usefull to tune cut value)\n    if features_file is not None:\n        np.save(features_file, features)\n\n    # Create index\n    index = faiss.IndexFlatIP(features.shape[1])\n    index.add(features)\n\n    # Search index\n    return index.search(features, top_n)\n\n    # TODO: try range_search\n    # lims, similarities, indexes = index_test.range_search(test_features, GROUP_CUT)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"n_classes = 10875","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = CLIPClassifier(n_classes).to(device)\nmodel.load_state_dict(torch.load('../input/arcfacebin/clip_image_text_4dot66.bin'))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## run test set","metadata":{}},{"cell_type":"code","source":"\nGROUP_CUT = 0.73  # Use option `RUN_ON_TRAIN` to find this number. It will be desabled at submission time\ntest_images_path = Path('../input/shopee-product-matching/test_images')\n# Find similar matches\nsimilarities, indexes = find_similarities_and_indexes(df_test, test_images_path)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Apply cutoff of similiarites\ntest_are_same_groups = (similarities > GROUP_CUT)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Build submission\nresults = []\n\nfor i, (test_is_same_group, index_result) in enumerate(zip(test_are_same_groups, indexes)):\n    row_results = set(df_test.index[index_result[test_is_same_group]])\n    \n    results.append({\n        'posting_id': df_test.index[i],\n#         'pred_matches02': ' '.join(row_results),\n        'pred_matches02': list(row_results)\n    })\n    \ndf_sub = pd.DataFrame(results)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"result = pd.merge(df, df_sub, on='posting_id')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# result['matches'] = result.apply(combine_predictions, axis = 1)\n# result.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"result['tot'] = result.image_predictions+result.text_predictions+result.phash+result.pred_matches02","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import collections\ndef result_on_count(row, top_k=1):\n#     res = []\n#     count_di = collections.Counter(row)\n#     for key, value in count_di.items():\n#         if value >= top_k:\n#             res.append(key)\n#     return sorted(count_di, key=lambda x: count_di[x], reverse=True)[:top_k]\n    count = pd.value_counts(row)\n    return ' '.join(count[count>=top_k].index)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def combine_predictions(row):\n#     x = np.concatenate([row['matches']])\n#     return ' '.join( np.unique(x))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"result['matches'] = result.tot.apply(lambda x: result_on_count(x, 2))\n# result['matches'] = result.apply(combine_predictions, axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"result.head(3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# results","metadata":{}},{"cell_type":"code","source":"result[['posting_id', 'matches']].to_csv('submission.csv', index = False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}