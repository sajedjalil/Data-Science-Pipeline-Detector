{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Shopee Product Matching - Image Similarity Model","metadata":{}},{"cell_type":"markdown","source":"This notebook outlines the training of a CNN to determine image similarities using Tensorflow2 with the following features:\n* TPU Enhanced training. For construction of TFRecords required for TPU acceleration, please reference to [this link](https://www.kaggle.com/sandersli/shopee-product-matching-create-tfrecords)\n* [EfficientNet Backbone](https://keras.io/examples/vision/image_classification_efficientnet_fine_tuning/)\n* [Global Mean Average Pooling](https://arxiv.org/pdf/1711.02512.pdf)\n* [ArcFace Loss](https://arxiv.org/pdf/1801.07698.pdf)\n* Comprehensive image augmentation on dataset construction\n\nThanks to [ragnar's fantastic notebook for providing a jumping off point](https://www.kaggle.com/ragnar123/shopee-efficientnetb3-arcmarginproduct)","metadata":{}},{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport random\nimport tensorflow as tf\nimport tensorflow_addons as tfa\n\nfrom PIL import Image\nfrom sklearn import metrics\nfrom sklearn.model_selection import KFold, train_test_split\nfrom tqdm.notebook import tqdm\nfrom kaggle_datasets import KaggleDatasets","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Enable TPU acceleration","metadata":{}},{"cell_type":"code","source":"AUTO = tf.data.experimental.AUTOTUNE\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver().connect()\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n    print(f\"Running on TPU {tpu.master()} with {strategy.num_replicas_in_sync} replicas\")\nexcept ValueError:\n    print(\"Not connected to a TPU runtime. Using CPU/GPU strategy\")\n    strategy = tf.distribute.MirroredStrategy()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"GCS_PATH = KaggleDatasets().get_gcs_path('shopee-tfrec-224px')\ntrain_filenames = tf.io.gfile.glob([GCS_PATH + '/train/*.tfrec'])\ntest_filenames = tf.io.gfile.glob([GCS_PATH + '/test/*tfrec'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('../input/shopee-product-matching/train.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Use correct image size with pretrained model\nIMAGE_SIZE = (300, 300)\n# Train-test-split size\nTRAIN_SIZE = 0.8\n# Initial learning rate\nLR = 0.001\n# ArcFace must assume a certain number of classes to optimize loss. May get better results on test set with higher N_CLASSES\nN_CLASSES = df['label_group'].nunique()\n\nSEED = 42\n\nEPOCHS = 30\nBATCH_SIZE = 32 * strategy.num_replicas_in_sync\nSTEPS_PER_EPOCH = len(df) * TRAIN_SIZE // BATCH_SIZE","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function to seed everything\ndef seed_everything(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    tf.random.set_seed(seed)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model Construction","metadata":{}},{"cell_type":"code","source":"# ArcFace parameters\n# s: norm of input feature\n# m: margin, Original paper states that m=0.5 gives best results. I found this variable to have the strongest effect when calculating final f1 score\nparams = {\n    'm': 0.3, \n    's': 30\n}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function to augment data\n# As data was serialized to TFRecords, I directly convert TFRecords to datasets and thus cannot use Keras ImageDataGenerator\ndef data_augment(posting_id, image, label_group, matches):   \n    rotate = tf.random.uniform(shape=(), minval=-0.1*np.pi, maxval=0.1*np.pi)\n    image = tfa.image.rotate(image, rotate, interpolation='bilinear', fill_mode='constant')\n    shear_x = tf.random.uniform(shape=(), minval=-0.2, maxval=0.2)\n    shear_y = tf.random.uniform(shape=(), minval=-0.2, maxval=0.2)\n    image = tfa.image.transform(image, [1.0, shear_x, 0.0, shear_y, 1.0, 0.0, 0.0, 0.0], interpolation='bilinear', fill_mode='constant')\n    translate_vec = tf.random.uniform(shape=(2,), minval=-int(0.05*IMAGE_SIZE[0]), maxval=int(0.05*IMAGE_SIZE[0]))\n    image = tfa.image.translate(image, translate_vec, interpolation='bilinear', fill_mode='constant')\n    \n    crop_size = tf.random.uniform(shape=(), minval=int(0.8*IMAGE_SIZE[0]), maxval=int(1.2*IMAGE_SIZE[0]), dtype=tf.int32)\n    image = tf.image.resize_with_crop_or_pad(image, crop_size, crop_size)  \n    image = tf.image.resize(image, IMAGE_SIZE)\n    \n    image = tf.image.random_brightness(image, 0.10)\n    image = tf.image.random_hue(image, 0.01)\n    image = tf.image.random_saturation(image, 0.80, 1.20)\n    image = tf.image.random_contrast(image, 0.80, 1.20)\n    image = tf.image.random_flip_left_right(image)\n    return posting_id, image, label_group, matches\n\n# Function to decode images from serialized image data from TFRecords\ndef decode_image(image_data):\n    image = tf.image.decode_jpeg(image_data, channels = 3)\n    image = tf.image.resize(image, IMAGE_SIZE)\n    image = tf.cast(image, tf.float32) / 255.0\n    return image\n\n# Function to read TFRecords\ndef read_tfrec(example):\n    tfrec_format = {\n        \"posting_id\": tf.io.FixedLenFeature([], tf.string),\n        \"image\": tf.io.FixedLenFeature([], tf.string),\n        \"label_group\": tf.io.FixedLenFeature([], tf.int64),\n        \"matches\": tf.io.FixedLenFeature([], tf.string)\n    }\n\n    example = tf.io.parse_single_example(example, tfrec_format)\n    posting_id = example['posting_id']\n    image = decode_image(example['image'])\n    label_group = tf.cast(example['label_group'], tf.int32)\n    matches = example['matches']\n    return posting_id, image, label_group, matches\n\n# Function to create a dataset by reading TFRecords\ndef load_dataset(filenames):\n    dataset = tf.data.TFRecordDataset(filenames, num_parallel_reads = AUTO)\n    dataset = dataset.map(read_tfrec, num_parallel_calls = AUTO) \n    return dataset\n\n# Function to reformat dataset for model\ndef arcface_format(posting_id, image, label_group, matches):\n    return posting_id, {'image': image, 'label': label_group}, label_group, matches\n\n# Function to construct dataset\ndef get_dataset(filenames, training=False):\n    dataset = load_dataset(filenames)\n    if training:\n        ignore_order = tf.data.Options()\n        dataset = dataset.with_options(ignore_order)\n        dataset = dataset.map(data_augment, num_parallel_calls = AUTO)\n        dataset = dataset.repeat()\n        dataset = dataset.shuffle(2048)\n    dataset = dataset.map(arcface_format, num_parallel_calls = AUTO)\n    dataset = dataset.batch(BATCH_SIZE)\n    dataset = dataset.prefetch(AUTO)\n    return dataset","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Split data into train and validation sets\ntrain, valid = train_test_split(train_filenames, shuffle = True, random_state = SEED)\ntrain_dataset = get_dataset(train, training=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Visualize augmented dataset\nfig = plt.figure(figsize=(24, 24))\nrows, cols = 3, 3\nax = fig.subplots(rows, cols)\nfor example in train_dataset.take(1):\n    for i in range(rows * cols):\n        subplot = i//rows, i%cols\n        ax[subplot].imshow(example[1]['image'][i])\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Construct Model","metadata":{}},{"cell_type":"code","source":"# Establish learning rate function\ncosine_lr_fn = tf.keras.experimental.CosineDecay(LR, 3*STEPS_PER_EPOCH*10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ArcMarginProduct(tf.keras.layers.Layer):\n    '''\n    Implements large margin arc distance.\n\n    Reference:\n        https://arxiv.org/pdf/1801.07698.pdf\n        https://github.com/lyakaap/Landmark2019-1st-and-3rd-Place-Solution/\n            blob/master/src/modeling/metric_learning.py\n    '''\n    def __init__(self, n_classes, s=30, m=0.50, easy_margin=False,\n                 ls_eps=0.0, **kwargs):\n\n        super(ArcMarginProduct, self).__init__(**kwargs)\n\n        self.n_classes = n_classes\n        self.s = s\n        self.m = m\n        self.ls_eps = ls_eps\n        self.easy_margin = easy_margin\n        self.cos_m = tf.math.cos(m)\n        self.sin_m = tf.math.sin(m)\n        self.th = tf.math.cos(np.pi - m)\n        self.mm = tf.math.sin(np.pi - m) * m\n\n    def get_config(self):\n\n        config = super().get_config().copy()\n        config.update({\n            'n_classes': self.n_classes,\n            's': self.s,\n            'm': self.m,\n            'ls_eps': self.ls_eps,\n            'easy_margin': self.easy_margin,\n        })\n        return config\n\n    def build(self, input_shape):\n        super(ArcMarginProduct, self).build(input_shape[0])\n\n        self.W = self.add_weight(\n            name='W',\n            shape=(int(input_shape[0][-1]), self.n_classes),\n            initializer='glorot_uniform',\n            dtype='float32',\n            trainable=True,\n            regularizer=None)\n\n    def call(self, inputs):\n        X, y = inputs\n        y = tf.cast(y, dtype=tf.int32)\n        cosine = tf.matmul(\n            tf.math.l2_normalize(X, axis=1),\n            tf.math.l2_normalize(self.W, axis=0)\n        )\n        sine = tf.math.sqrt(1.0 - tf.math.pow(cosine, 2))\n        phi = cosine * self.cos_m - sine * self.sin_m\n        if self.easy_margin:\n            phi = tf.where(cosine > 0, phi, cosine)\n        else:\n            phi = tf.where(cosine > self.th, phi, cosine - self.mm)\n        one_hot = tf.cast(\n            tf.one_hot(y, depth=self.n_classes),\n            dtype=cosine.dtype\n        )\n        if self.ls_eps > 0:\n            one_hot = (1 - self.ls_eps) * one_hot + self.ls_eps / self.n_classes\n\n        output = (one_hot * phi) + ((1.0 - one_hot) * cosine)\n        output *= self.s\n        return output","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class GeMPoolingLayer(tf.keras.layers.Layer):\n    '''\n    Implements Generalized-Mean Pooling layer\n    Reference:\n        https://arxiv.org/pdf/1711.02512.pdf\n    '''\n    def __init__(self, p=1., eps=1e-6):\n        super().__init__()\n        self.p = p\n        self.eps = eps\n\n    def call(self, inputs: tf.Tensor, **kwargs):\n        inputs = tf.clip_by_value(inputs, clip_value_min=self.eps, clip_value_max=tf.reduce_max(inputs))\n        inputs = tf.pow(inputs, self.p)\n        inputs = tf.reduce_mean(inputs, axis=[1, 2], keepdims=False)\n        inputs = tf.pow(inputs, 1. / self.p)\n        return inputs\n    \n    def get_config(self):\n        return {\n            'p': self.p,\n            'eps': self.eps\n        }","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Construct the model\ndef get_model(params):\n    backbone = tf.keras.applications.EfficientNetB3(weights = 'imagenet', include_top = False)\n    margin = ArcMarginProduct(\n        n_classes = N_CLASSES, \n        s = params['s'],\n        m = params['m'],\n        name='arc_margin_product', \n        dtype='float32'\n        )\n\n    inp = tf.keras.layers.Input(shape = IMAGE_SIZE + (3,), name = 'image')\n    label = tf.keras.layers.Input(shape = (), name = 'label')\n    x = tf.keras.applications.efficientnet.preprocess_input(inp)\n    x = backbone(x)\n    x = GeMPoolingLayer()(x)\n    x = tf.keras.layers.Dense(512, kernel_regularizer=tf.keras.regularizers.l2(), activation=None)(x)\n    x = tf.keras.layers.BatchNormalization()(x)\n    x = margin([x, label])\n\n    output = tf.keras.layers.Softmax(dtype='float32')(x)\n\n    model = tf.keras.models.Model(inputs = [inp, label], outputs = [output])\n\n    model.compile(\n        optimizer = tf.keras.optimizers.Adam(learning_rate = cosine_lr_fn),\n        loss = [tf.keras.losses.SparseCategoricalCrossentropy()],\n        metrics = [tf.keras.metrics.SparseCategoricalAccuracy()]\n        ) \n\n    return model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = get_model(params)\nmodel.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model Training","metadata":{}},{"cell_type":"code","source":"def train_and_evaluate():\n    seed_everything(SEED)\n    train, valid = train_test_split(train_filenames, shuffle = True, random_state = SEED)\n    train_dataset = get_dataset(train, training=True)\n    train_dataset = train_dataset.map(lambda posting_id, image, label_group, matches: (image, label_group))\n    val_dataset = get_dataset(valid)\n    val_dataset = val_dataset.map(lambda posting_id, image, label_group, matches: (image, label_group))\n    tf.keras.backend.clear_session()\n    with strategy.scope():\n        model = get_model(params)\n    # Model checkpoint\n    checkpoint = tf.keras.callbacks.ModelCheckpoint(f\"EfficientNetB3_{IMAGE_SIZE[0]}_{SEED}_m{params['m']}_s{params['s']}.h5\", \n                                                    monitor = 'val_loss',\n                                                    save_best_only = True,\n                                                    save_weights_only = True, \n                                                    mode = 'min')\n\n    history = model.fit(train_dataset,\n                        steps_per_epoch = STEPS_PER_EPOCH,\n                        epochs = EPOCHS,\n                        callbacks = [checkpoint], \n                        validation_data = val_dataset)\n    return history, model\n    \nhist, model = train_and_evaluate()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import plotly.express as px\nfig = px.line(hist.history)\ndisplay(fig)\nfig.write_html(f\"arcface_m{params['m']}.html\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The final acc/loss value isn't a direct measure of how well the model performs. Model loss is evaluated on a classification task (assigning embeddings to labels), while the competition is evaluated on a f1 score based on a clustering task (grouping embeddings together). Having a low loss may result in poor performance on unlabeled test data.","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}