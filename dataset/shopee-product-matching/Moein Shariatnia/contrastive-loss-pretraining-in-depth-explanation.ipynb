{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Pretraining CNNs with Contrastive Loss\n\n![](https://i.ibb.co/BNJrb1C/CL.png)\n\nImages in the graph above are from this competition's dataset :)"},{"metadata":{},"cell_type":"markdown","source":"## Introduction"},{"metadata":{},"cell_type":"markdown","source":"Welcome to this notebook. In this competition, we are provided with a dataset in which for every row (which is a product in a shop) we have its **image** and a **title** explaining what it is (among other things!). The goal is to find which other products are **similar** to a given product.\n\nAs the competition page puts it:\n\n> Finding near-duplicates in large datasets is an important problem for many online businesses. In Shopee's case, everyday users can upload their own images and write their own product descriptions, adding an extra layer of challenge. Your task is to identify which products have been posted repeatedly. The differences between related products may be subtle while photos of identical products may be wildly different!"},{"metadata":{},"cell_type":"markdown","source":"Since we can't compare raw pixels of images to each other(!), we need to first build a representation from each image which is understanable for the computer. Convolutional Neural Nets are good for this purpose; we give them an image and they return a 1D array descriving that image. When we get this array for each image, tons of machine learning models and techniques can be used to find similar images, which is the topic of my next notebook to come!"},{"metadata":{},"cell_type":"markdown","source":"## What we are going to do"},{"metadata":{},"cell_type":"markdown","source":"So, we need good representations (those 1D arrays). We can use a CNN which is pretrained on ImageNet to obtain these representations; but, this approach is sub-optimal because the images in imagenet and those in the dataset can wildly differ. Therefore, in this notebook, we are going to pre-train a CNN on the images of this dataset using **Contrastive Loss**.\n\nIn contrast to its name which sounds way compicated, the idea is really simple! We are going to choose two images randomly from the dataset; then, if their labels (label_group in this dataset) are the same, we make the image's representations similar to that of the other image. And, if the labels are different, we will make the representations different. In this way, we can obtain better representations when the training is done.\n\nStay Tuned :)"},{"metadata":{},"cell_type":"markdown","source":"## Installing and Importing needed libraries"},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"!pip install timm","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os\nimport gc\nimport cv2\nimport sys\nimport copy\nimport math\nimport random\nimport argparse\nimport itertools\nimport pandas as pd\nimport numpy as np\n\nimport torch\nfrom torch import nn, optim\nfrom torch.utils.data import Dataset\nfrom torchvision import models\nimport torch.nn.functional as F\nimport timm\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\n\nimport albumentations as A\nfrom tqdm.autonotebook import tqdm\n\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Configuration Class"},{"metadata":{"trusted":true},"cell_type":"code","source":"class CFG:\n    size = 224\n    batch_size = 16\n    num_workers = 2\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model_name = 'resnet18'\n    pretrained = True \n    dropout = None # put it to a prob. to include a dropout layer in the model\n    linear = None # put to an int to include a nn.Linear layer in the model to lower/higher the length of the repr array\n    margin = 5\n    \n    scheduler = \"ReduceLROnPlateau\"\n    step = \"epoch\" # wheter to step it after epoch or after batch\n    \n    learning_rate = 1e-3\n    factor = 0.5\n    patience = 2\n    epochs = 5\n    model_path = \".\"\n    model_save_name = \"best.pt\"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Loading the dataframe"},{"metadata":{"trusted":true},"cell_type":"code","source":"dataframe = pd.read_csv(\"../input/shopee-product-matching/train.csv\")\nprint(dataframe.shape)\ndataframe.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"See how many unique instances are there in each column"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f\"Number of rows: {dataframe.shape[0]}\")\nfor col in dataframe.columns:\n    print(f\"{col}: \\n\"\n          f\"Number of unique elements {dataframe[col].nunique()}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Building the Dataset"},{"metadata":{},"cell_type":"markdown","source":"I've provided a short description of the code below to understand it easier. I know there may be better ways to do the same function but this came to my mind and I implemented it. The related code is the next code cell."},{"metadata":{},"cell_type":"markdown","source":"![pres](https://i.ibb.co/d5PKJG5/pres2.png)"},{"metadata":{"trusted":true},"cell_type":"code","source":"class ContrastiveDataset(Dataset):\n    def __init__(self, df, transforms, base=\"../input/shopee-product-matching/train_images\"):\n        self.base = base\n        self.transforms = transforms\n        # getting all unique label_groups\n        self.labels = list(df['label_group'].unique())\n        # we put the image names of each label_group in front of it in a big dictionary\n        self.labels_to_imgs = {label: df[df['label_group'] == label].image.values\n                               for label in self.labels}\n    \n    def __getitem__(self, idx):\n        label = self.labels[idx]\n        \n        if random.random() > 0.5:\n            same = True\n            same_label_images = self.labels_to_imgs[label]\n            img1, img2 = np.random.choice(same_label_images, \n                                          size=2, \n                                          replace=False if len(same_label_images) > 1 else True)\n        else:\n            same = False\n            img1 = np.random.choice(self.labels_to_imgs[label], size=1)[0]\n            while True:\n                different_label = np.random.choice(self.labels, size=1)[0]\n                if different_label != label:\n                    break\n            img2 = np.random.choice(self.labels_to_imgs[different_label], size=1)[0]\n        \n        img1_tensor, img2_tensor = self.process_imgs(img1, img2)\n        \n        # returning everything :)\n        return {'images1': img1_tensor,\n                'images2': img2_tensor,\n                'same': torch.tensor(same).float(),\n                'label1': label,\n                'label2': label if same else different_label, \n                'image1_name': img1,\n                'image2_name': img2}\n    \n    def read_transform_one(self, img):\n        img = cv2.imread(f\"{self.base}/{img}\")[..., ::-1]\n        if self.transforms is not None:\n            img = self.transforms(image=img)['image']\n        return torch.tensor(img).float()\n    \n    def process_imgs(self, img1, img2):\n        img1 = self.read_transform_one(img1).permute(2, 0, 1)\n        img2 = self.read_transform_one(img2).permute(2, 0, 1)\n        return img1, img2\n\n    \n    def __len__(self):\n        return len(self.labels)\n\n\ndef remove_normalization(image, mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)):\n    \"\"\"\n    Function to undo the normalization done in the dataset.\n    Useful for visualization purposes\n    \n    :param image: tensor with shape -> (channel, height, width)\n    \"\"\"\n    mean, std = torch.tensor(mean), torch.tensor(std)\n    mean = mean.unsqueeze(1).unsqueeze(2)\n    std = std.unsqueeze(1).unsqueeze(2)\n    return image * std + mean","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"transforms = A.Compose([\n    A.Resize(CFG.size, CFG.size),\n    A.Normalize(max_pixel_value=255.) # Normalizes with ImageNet stats \n])\n\ndataset = ContrastiveDataset(dataframe, transforms)\ndataloader = torch.utils.data.DataLoader(dataset, batch_size=CFG.batch_size, shuffle=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Visualization"},{"metadata":{},"cell_type":"markdown","source":"See some of the images from the dataset. Adjucent to the top if each image, you see the image file name. At the top of each row, you see the label groups for the images and whether they are from the same label_group."},{"metadata":{"trusted":true},"cell_type":"code","source":"batch = next(iter(dataloader))\nprint(batch['images1'].shape, batch['images2'].shape, batch['same'].shape)\n\nRows = 5\nfor r in range(Rows):\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n    img1, img2, same = batch['images1'][r], batch['images2'][r], batch['same'][r]\n    img1, img2 = remove_normalization(img1), remove_normalization(img2)\n    ax1.imshow(img1.permute(1, 2, 0))\n    ax1.axis(\"off\")\n    ax1.set_title(batch['image1_name'][r])\n    ax2.imshow(img2.permute(1, 2, 0))\n    ax2.axis(\"off\")\n    ax2.set_title(batch['image2_name'][r])\n    same = \"Same\" if same == 1 else \"Different\"\n    fig.suptitle(f\"{same} ({batch['label1'][r]}---{batch['label2'][r]})\")\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##  Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"class Model(nn.Module):\n    def __init__(self, \n                 model_name=\"resnet18\",\n                 pretrained=True,\n                 dropout=0.2,\n                 linear=128):\n        \"\"\"\n        :param model_name: many models are available from the awesome timm library (ResNets, EfficientNets, DenseNets, ...)\n        :param pretrained: whether to initialize the model with the pre-trained weights (pre-trained on ImageNet)\n        :param linear: out_dim of nn.Linear. If None or 0, no linear layer will be added\n        :param dropout: dropout ratio. If None or 0, no dropout layer will be added\n\n        \"\"\"\n        super().__init__()\n        model = timm.create_model(model_name, \n                                  pretrained=pretrained, \n                                  num_classes=0)\n        \n        # num of final features after adaptive (global) average pooling\n        self.num_features = model.num_features\n        self.linear = None\n        if linear is not None and linear > 0:\n            self.linear = nn.Linear(self.num_features, linear)\n        \n        # nn.Identity does nothing! just returns the input tensor\n        self.backbone = nn.Sequential(model, \n                                      self.linear if self.linear is not None else nn.Identity(),\n                                      nn.ReLU() if self.linear is not None else nn.Identity(),\n                                      nn.Dropout(0.2) if dropout is not None else nn.Identity())\n    \n    def forward(self, batch):\n        images_1 = self.backbone(batch['images1'].to(CFG.device))\n        images_2 = self.backbone(batch['images2'].to(CFG.device))\n\n        return images_1, images_2","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Contrastive Loss Function"},{"metadata":{},"cell_type":"markdown","source":"Here's the cool part! This is from the [Dimensionality Reduction by Learning an Invariant Mapping](https://ieeexplore.ieee.org/document/1640964) paper (Yann LeCun is in co-authors).\n\nIn the section regrading the ***Dataset***, you saw that when we get images from dataset, with 50% probability we get images from the same label and with 50% from different labels.\n\nWe feed each of those to the model separately (you can see this in the forward function of the model above) and then we get representations out of the model for each of the two images.\n\nNow, we want to guid the model to produce more similar representations for similar images and different representations for different ones. This is done with the loss function below. "},{"metadata":{},"cell_type":"markdown","source":"![](https://miro.medium.com/max/778/1*g2I-W-iIQuCNsczuGPN0ZQ.png)\n\nImage from [HERE](https://towardsdatascience.com/how-to-choose-your-loss-when-designing-a-siamese-neural-net-contrastive-triplet-or-quadruplet-ecba11944ec)."},{"metadata":{},"cell_type":"markdown","source":"Don't panic! It is really simple when converted to code. \n\n1. Just think about what happens when the label is 1; meaning that images are **SIMILAR** -> only the first term remains and the model needs to reduce the **distance** (d) to be able to lower the loss (which it has to!) \n2. Now thing what happens when the label is 0 meaning that images are **DIFFERENT** -> only the second term remains and the model should try to make distance (d) bigger than margin (m) to be able to lower the loss."},{"metadata":{"trusted":true},"cell_type":"code","source":"class ContrastiveLoss(nn.Module):\n    def __init__(self, margin):\n        super().__init__()\n        self.margin = margin\n\n    def forward(self, output1, output2, targets):\n        d = (output1 - output2).pow(2).sum(1).sqrt() # distance\n        loss = torch.mean(0.5 * targets.float() * d.pow(2) + \\\n                          0.5 * (1 - targets.float()) * F.relu(self.margin - d).pow(2))\n        return loss, d # we also return distance; it is needed to evaluate the model","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Train and Evaluation Functions"},{"metadata":{},"cell_type":"markdown","source":"Functions that we will need to train and evaluate our model"},{"metadata":{"trusted":true},"cell_type":"code","source":"class AvgMeter:\n    def __init__(self, name=\"Metric\"):\n        self.name = name\n        self.reset()\n    \n    def reset(self):\n        self.avg, self.sum, self.count = [0]*3\n    \n    def update(self, val, count=1):\n        self.count += count\n        self.sum += val * count\n        self.avg = self.sum / self.count\n    \n    def __repr__(self):\n        text = f\"{self.name}: {self.avg:.4f}\"\n        return text\n    \ndef get_lr(optimizer):\n    for param_group in optimizer.param_groups:\n        return param_group[\"lr\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def one_epoch(model, \n              criterion, \n              loader, \n              optimizer=None, \n              lr_scheduler=None,\n              mode=\"train\", \n              step=\"batch\"):\n    \n    loss_meter = AvgMeter()\n    \n    # these two are needed for the after-epoch evaluation. You're gonna see what they are used for\n    distances = None\n    labels = None\n    \n    tqdm_object = tqdm(loader, total=len(loader))\n    for batch in tqdm_object:\n        images1_f, images2_f = model(batch)\n        loss, d = criterion(images1_f, images2_f, batch['same'].to(CFG.device))\n        if mode == \"train\":\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            if step == \"batch\":\n                lr_scheduler.step()\n                \n        count = batch['same'].size(0)\n        loss_meter.update(loss.item(), count)\n          \n        # collecting all the labels and distances\n        if distances is None:\n            distances = d.detach().cpu()\n            labels = batch['same']\n        else:\n            distances = torch.cat([distances, d.detach().cpu()], dim=0)\n            labels = torch.cat([labels, batch['same']], dim=0)\n\n\n        if mode == \"train\":\n            tqdm_object.set_postfix(train_loss=loss_meter.avg, lr=get_lr(optimizer))\n        else:\n            tqdm_object.set_postfix(valid_loss=loss_meter.avg)\n    \n    return loss_meter, distances, labels","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### How can we evaluate the model during training?\n\nThats a good question!\nActually, the metric you are going to see is probably a bit different from other metrics you've seen because another model gets trained each time we call this metric function!\n\nRemember the margin and distance from the loss function section. The model needed to decrease the distance for similar images and increase it above an arbitrary margin for dissimilar images. So, we can use the distances to find out if the images are similar or not. This was the reason we were collecting all the distances during a training or eval epoch.\n\nThen, after each epoch is done, we will fit a simple Logistic Regression model from scikit-learn library and train it on the distances and labels of the training image pairs. After that, we will see how this simple model predicts if the distances from the validation image pairs describe a similar or dissimilar pair of images."},{"metadata":{},"cell_type":"markdown","source":"If the CNN model is not able to produce good representations whose distances from each other are useless, the logistic regression model will not be able to correctly classify the pairs and should always give an accuracy of 50 percent. But, if this simple model could clasify the pairs with a good accuracy, chances are we are producing useful representations in the model which we can use for the main task of this competition."},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_score(train_d, train_y, valid_d, valid_y):\n    \"\"\"\n    suffix _d means distances\n    suffix _y means labels\n    \"\"\"\n    log_reg = LogisticRegression()\n    log_reg.fit(train_d.numpy().reshape((-1, 1)), train_y.numpy())\n    train_preds = log_reg.predict(train_d.numpy().reshape((-1, 1)))\n    valid_preds = log_reg.predict(valid_d.numpy().reshape((-1, 1)))\n    train_acc = accuracy_score(train_preds, train_y)\n    valid_acc = accuracy_score(valid_preds, valid_y)\n    return train_acc, valid_acc","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_eval(epochs, model, train_loader, valid_loader, \n               criterion, optimizer, lr_scheduler=None):\n    \n    best_loss = float('inf')\n    \n    for epoch in range(epochs):\n        print(\"*\" * 30)\n        print(f\"Epoch {epoch + 1}\")\n        current_lr = get_lr(optimizer)\n        \n        model.train()\n        train_loss, train_d, train_y = one_epoch(model, \n                                                  criterion, \n                                                  train_loader, \n                                                  optimizer=optimizer,\n                                                  lr_scheduler=lr_scheduler,\n                                                  mode=\"train\",\n                                                  step=CFG.step)                     \n        model.eval()\n        with torch.no_grad():\n            valid_loss, valid_d, valid_y = one_epoch(model, \n                                                      criterion, \n                                                      valid_loader, \n                                                      optimizer=None,\n                                                      lr_scheduler=None,\n                                                      mode=\"valid\")\n        \n        train_acc, valid_acc = get_score(train_d, train_y, valid_d, valid_y)\n        print(f\"Train Accuracy: {train_acc:.3f}\")\n        print(f\"Valid Accuracy: {valid_acc:.3f}\")\n        \n        if valid_loss.avg < best_loss:\n            best_loss = valid_loss.avg\n            torch.save(model.state_dict(), f'{CFG.model_path}/{CFG.model_save_name}')\n            print(\"Saved best model!\")\n        \n        # or you could do: if step == \"epoch\":\n        if isinstance(lr_scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):\n            lr_scheduler.step(train_loss.avg)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Train and Valid loaders"},{"metadata":{"trusted":true},"cell_type":"code","source":"labels = dataframe['label_group'].unique()\ntrain_labels, valid_labels = train_test_split(labels, test_size=0.2, shuffle=True, random_state=42)\ntrain_df = dataframe[dataframe['label_group'].isin(train_labels)].reset_index(drop=True)\nvalid_df = dataframe[dataframe['label_group'].isin(valid_labels)].reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_dataset = ContrastiveDataset(train_df, transforms)\ntrain_loader = torch.utils.data.DataLoader(train_dataset, \n                                           batch_size=CFG.batch_size,\n                                           num_workers=CFG.num_workers,\n                                           shuffle=True)\n\nvalid_dataset = ContrastiveDataset(valid_df, transforms)\nvalid_loader = torch.utils.data.DataLoader(valid_dataset, \n                                           batch_size=CFG.batch_size,\n                                           num_workers=CFG.num_workers,\n                                           shuffle=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Model(CFG.model_name, CFG.pretrained, CFG.dropout, CFG.linear)\nmodel.to(CFG.device)\noptimizer = torch.optim.Adam(model.parameters(), lr=CFG.learning_rate)\n\nif CFG.scheduler == \"ReduceLROnPlateau\":\n    lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, \n                                                              mode=\"min\", \n                                                              factor=CFG.factor, \n                                                              patience=CFG.patience)\n\n    # when to step the scheduler: after an epoch or after a batch\n    CFG.step = \"epoch\"\n    \ncriterion = ContrastiveLoss(margin=CFG.margin)\ntrain_eval(CFG.epochs, \n           model, \n           train_loader, \n           valid_loader, \n           criterion, \n           optimizer, \n           lr_scheduler)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"You see that the train and validation accuracies are above 50 percent after training and increase further after each epoch. So it seems that we are making good features for each image.\n\nIn this next notebook to come, I'll use these representations to find similar and dissimilar images and focus on the main task of the competition."},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}