{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Abstract\n\nHi guys, I've added some functions to this notebook (including `threshold_searching`). To test LB or CV score, you're welcome to just simply **Copy and Edit** then **Commit** it. Enjoy exploring!<br>\nAnd, don't forget to use `threshold_searching` to find your best neighbors.\n\nTo save time from generating img Embedding, use pre-saved embedding for faster work. You can use this dataset directly: [Shopee - Price Match Guarantee| Embeddings](https://www.kaggle.com/chienhsianghung/shopee-price-match-guarantee-embeddings).<br>\nI've also done some experiments on KNN and Cosine Similarity in the previous notebook. Click here to see the result: [Shopee| text, img Embedding (Colab enabled)](https://www.kaggle.com/chienhsianghung/shopee-text-img-embedding-colab-enabled).","metadata":{"papermill":{"duration":0.013862,"end_time":"2021-04-12T04:30:24.618934","exception":false,"start_time":"2021-04-12T04:30:24.605072","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"## Notes\n\n* By turning on ` NEIGHBORS_SEARCHING` and using`threshold_searching` function, one may get higher F1 score.\n* Use [this dataset](https://www.kaggle.com/chienhsianghung/shopee-price-match-guarantee-embeddings) to save your time.\n* Change `THRES_METH` to test your searching on LB.\n* Special thanks to @vatsalmavani\n  * This [notebook](https://www.kaggle.com/vatsalmavani/eff-b4-tfidf-0-728) works with any EfficientNet(B0 - B7) Model.\n  * Training Notebook for EfficientNet-B4 can be found [here](https://www.kaggle.com/vatsalmavani/shopee-training-eff-b4)","metadata":{}},{"cell_type":"code","source":"import sys\nsys.path.append('../input/timm-pytorch-image-models/pytorch-image-models-master')","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","papermill":{"duration":0.020911,"end_time":"2021-04-12T04:30:24.67797","exception":false,"start_time":"2021-04-12T04:30:24.657059","status":"completed"},"tags":[],"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport cv2\nimport math\nimport random\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\n\nimport albumentations\nfrom albumentations.pytorch.transforms import ToTensorV2\n\nimport torch\nimport timm\nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\nfrom torch.utils.data import Dataset,DataLoader\n\nimport gc\nimport matplotlib.pyplot as plt\nimport cudf\nimport cuml\nimport cupy\nfrom cuml.feature_extraction.text import TfidfVectorizer\nfrom cuml import PCA\nfrom cuml.neighbors import NearestNeighbors","metadata":{"papermill":{"duration":8.621584,"end_time":"2021-04-12T04:30:33.312428","exception":false,"start_time":"2021-04-12T04:30:24.690844","status":"completed"},"tags":[],"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Config","metadata":{"papermill":{"duration":0.012895,"end_time":"2021-04-12T04:30:33.338671","exception":false,"start_time":"2021-04-12T04:30:33.325776","status":"completed"},"tags":[]}},{"cell_type":"code","source":"COMPUTE_CV = True\nNEIGHBORS_SEARCHING = False\nSAVE_IMGEMBEDDING = False\nBASELINE_CHECKING = False\nTHRES_METH = 'BOOM' # BOOM, BOOM_OPTIMIZED, THRES, THRES_OPTIMIZED\nFIXING = True\nPHASH = True\n\ndf = pd.read_csv('../input/shopee-product-matching/test.csv')\nif len(df)>3: COMPUTE_CV = False\nif COMPUTE_CV: \n    print('this submission notebook will compute CV score but commit notebook will not')\nelse:\n    print('this submission notebook will only be used to submit result')\n    \nif FIXING:\n    NONE25000 = [None, 25000]\nelse:\n    NONE25000 = ['english', 25_000]\n\nclass CFG:\n    seed = 54\n    classes = 11014 \n    scale = 30 \n    margin = 0.5\n    model_name = 'tf_efficientnet_b4'\n    fc_dim = 512\n    img_size = 512\n    batch_size = 20\n    num_workers = 4\n    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n    model_path = '../input/utils-shopee/arcface_512x512_tf_efficientnet_b4_LR.pt'","metadata":{"papermill":{"duration":0.020052,"end_time":"2021-04-12T04:30:33.371579","exception":false,"start_time":"2021-04-12T04:30:33.351527","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Utils","metadata":{"papermill":{"duration":0.013995,"end_time":"2021-04-12T04:30:33.398316","exception":false,"start_time":"2021-04-12T04:30:33.384321","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def read_dataset(COMPUTE_CV):\n    \n    if COMPUTE_CV:\n        df = pd.read_csv('../input/shopee-product-matching/train.csv')\n        df_cu = cudf.DataFrame(df)\n        image_paths = '../input/shopee-product-matching/train_images/' + df['image']\n    \n    else:\n        df = pd.read_csv('../input/shopee-product-matching/test.csv')\n        df_cu = cudf.DataFrame(df)\n        image_paths = '../input/shopee-product-matching/test_images/' + df['image']\n\n    return df, df_cu, image_paths","metadata":{"papermill":{"duration":0.020165,"end_time":"2021-04-12T04:30:33.431442","exception":false,"start_time":"2021-04-12T04:30:33.411277","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def seed_torch(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    \nseed_torch(CFG.seed)","metadata":{"papermill":{"duration":0.024171,"end_time":"2021-04-12T04:30:33.468514","exception":false,"start_time":"2021-04-12T04:30:33.444343","status":"completed"},"tags":[],"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def combine_predictions(row):\n    x = np.concatenate([row['image_predictions'], row['text_predictions']])\n    return ' '.join( np.unique(x) )\n\ndef combine_predictions_phash(row):\n    x = np.concatenate([row['image_predictions'], row['text_predictions'], row['phash_predictions'], row['text_predictions_BERT']])\n    return ' '.join( np.unique(x) )\n\ndef combine_for_cv(row):\n    x = np.concatenate([row['image_predictions'], row['text_predictions']])\n    return np.unique(x)\n\ndef combine_for_cv_phash(row):\n    x = np.concatenate([row['image_predictions'], row['text_predictions'], row['phash_predictions'], row['text_predictions_BERT']])\n    return np.unique(x)","metadata":{"papermill":{"duration":0.019856,"end_time":"2021-04-12T04:30:33.536329","exception":false,"start_time":"2021-04-12T04:30:33.516473","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Image Predictions","metadata":{"papermill":{"duration":0.013363,"end_time":"2021-04-12T04:30:33.563025","exception":false,"start_time":"2021-04-12T04:30:33.549662","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Create Model\n\nclass ArcMarginProduct(nn.Module):\n    def __init__(self, in_features, out_features, scale=30.0, margin=0.50, easy_margin=False, ls_eps=0.0):\n        super(ArcMarginProduct, self).__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.scale = scale\n        self.margin = margin\n        self.ls_eps = ls_eps\n        self.weight = nn.Parameter(torch.FloatTensor(out_features, in_features))\n        nn.init.xavier_uniform_(self.weight)\n\n        self.easy_margin = easy_margin\n        self.cos_m = math.cos(margin)\n        self.sin_m = math.sin(margin)\n        self.th = math.cos(math.pi - margin)\n        self.mm = math.sin(math.pi - margin) * margin\n\n    def forward(self, input, label):\n        cosine = F.linear(F.normalize(input), F.normalize(self.weight))\n        sine = torch.sqrt(1.0 - torch.pow(cosine, 2))\n        phi = cosine * self.cos_m - sine * self.sin_m\n        if self.easy_margin:\n            phi = torch.where(cosine > 0, phi, cosine)\n        else:\n            phi = torch.where(cosine > self.th, phi, cosine - self.mm)\n    \n        one_hot = torch.zeros(cosine.size(), device='cuda')\n        one_hot.scatter_(1, label.view(-1, 1).long(), 1)\n        if self.ls_eps > 0:\n            one_hot = (1 - self.ls_eps) * one_hot + self.ls_eps / self.out_features\n\n        output = (one_hot * phi) + ((1.0 - one_hot) * cosine)\n        output *= self.scale\n\n        return output, nn.CrossEntropyLoss()(output,label)\n\n\nclass ShopeeModel(nn.Module):\n\n    def __init__(\n        self,\n        n_classes = CFG.classes,\n        model_name = CFG.model_name,\n        fc_dim = CFG.fc_dim,\n        margin = CFG.margin,\n        scale = CFG.scale,\n        use_fc = True,\n        pretrained = True):\n\n        super(ShopeeModel,self).__init__()\n        print('Building Model Backbone for {} model'.format(model_name))\n\n        self.backbone = timm.create_model(model_name, pretrained=pretrained)\n        in_features = self.backbone.classifier.in_features\n        self.backbone.classifier = nn.Identity()\n        self.backbone.global_pool = nn.Identity()\n        self.pooling =  nn.AdaptiveAvgPool2d(1)\n        self.use_fc = use_fc\n\n        if use_fc:\n            self.dropout = nn.Dropout(p=0.1)\n            self.classifier = nn.Linear(in_features, fc_dim)\n            self.bn = nn.BatchNorm1d(fc_dim)\n            self._init_params()\n            in_features = fc_dim\n\n        self.final = ArcMarginProduct(\n            in_features,\n            n_classes,\n            scale = scale,\n            margin = margin,\n            easy_margin = False,\n            ls_eps = 0.0\n        )\n\n    def _init_params(self):\n        nn.init.xavier_normal_(self.classifier.weight)\n        nn.init.constant_(self.classifier.bias, 0)\n        nn.init.constant_(self.bn.weight, 1)\n        nn.init.constant_(self.bn.bias, 0)\n\n    def forward(self, image, label):\n        features = self.extract_features(image)\n        if self.training:\n            logits = self.final(features, label)\n            return logits\n        else:\n            return features\n\n    def extract_features(self, x):\n        batch_size = x.shape[0]\n        x = self.backbone(x)\n        x = self.pooling(x).view(batch_size, -1)\n\n        if self.use_fc and self.training:\n            x = self.dropout(x)\n            x = self.classifier(x)\n            x = self.bn(x)\n        return x","metadata":{"papermill":{"duration":0.035463,"end_time":"2021-04-12T04:30:33.612047","exception":false,"start_time":"2021-04-12T04:30:33.576584","status":"completed"},"tags":[],"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_image_neighbors(df, embeddings, KNN=100, threshold=4.5):\n\n    model = NearestNeighbors(n_neighbors = KNN)\n    model.fit(embeddings)\n    distances, indices = model.kneighbors(embeddings)\n    \n    predictions = []\n    for k in tqdm(range(embeddings.shape[0])):\n        idx = np.where(distances[k,] < threshold)[0]\n        ids = indices[k,idx]\n        posting_ids = df['posting_id'].iloc[ids].values\n        predictions.append(posting_ids)\n\n    return df, predictions","metadata":{"papermill":{"duration":0.022049,"end_time":"2021-04-12T04:30:33.64753","exception":false,"start_time":"2021-04-12T04:30:33.625481","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_test_transforms():\n    return albumentations.Compose([\n        albumentations.Resize(CFG.img_size, CFG.img_size, always_apply=True),\n        albumentations.Normalize(),\n        ToTensorV2(p=1.0)\n    ])","metadata":{"papermill":{"duration":0.020559,"end_time":"2021-04-12T04:30:33.681696","exception":false,"start_time":"2021-04-12T04:30:33.661137","status":"completed"},"tags":[],"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ShopeeDataset(Dataset):\n\n    def __init__(self, image_paths, transforms=None):\n        self.image_paths = image_paths\n        self.augmentations = transforms\n\n    def __len__(self):\n        return self.image_paths.shape[0]\n\n    def __getitem__(self, index):\n        image_path = self.image_paths[index]\n        image = cv2.imread(image_path)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        if self.augmentations:\n            augmented = self.augmentations(image=image)\n            image = augmented['image']\n        \n        return image, torch.tensor(1)","metadata":{"papermill":{"duration":0.022188,"end_time":"2021-04-12T04:30:33.717959","exception":false,"start_time":"2021-04-12T04:30:33.695771","status":"completed"},"tags":[],"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_image_embeddings(image_paths):\n\n    model = ShopeeModel(pretrained=False).to(CFG.device)\n    model.load_state_dict(torch.load(CFG.model_path))\n    model.eval()\n\n    image_dataset = ShopeeDataset(image_paths=image_paths, transforms=get_test_transforms())\n    image_loader = torch.utils.data.DataLoader(\n        image_dataset,\n        batch_size=CFG.batch_size,\n        num_workers=CFG.num_workers\n    )\n\n    embeds = []\n    with torch.no_grad():\n        for img,label in tqdm(image_loader): \n            img = img.cuda()\n            label = label.cuda()\n            features = model(img,label)\n            image_embeddings = features.detach().cpu().numpy()\n            embeds.append(image_embeddings)\n\n    image_embeddings = np.concatenate(embeds)\n    print(f'Our image embeddings shape is {image_embeddings.shape}')\n    return image_embeddings","metadata":{"papermill":{"duration":0.02367,"end_time":"2021-04-12T04:30:33.75544","exception":false,"start_time":"2021-04-12T04:30:33.73177","status":"completed"},"tags":[],"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Text Predictions","metadata":{"papermill":{"duration":0.013862,"end_time":"2021-04-12T04:30:33.783134","exception":false,"start_time":"2021-04-12T04:30:33.769272","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def get_text_embeddings(df_cu, max_features=NONE25000[1]):\n    model = TfidfVectorizer(stop_words=NONE25000[0],\n                            binary=True,\n                            max_features=max_features)\n    text_embeddings = model.fit_transform(df_cu['title']).toarray()\n    return text_embeddings\n    \n    \ndef get_text_predictions(df, embeddings, max_features=NONE25000[1], threshold=0.75):\n    print('Finding similar titles...')\n    CHUNK = 1024 * 4\n    CTS = len(df) // CHUNK\n    if (len(df)%CHUNK) != 0:\n        CTS += 1\n\n    preds = []\n    for j in range( CTS ):\n        a = j * CHUNK\n        b = (j+1) * CHUNK\n        b = min(b, len(df))\n        print('chunk', a, 'to', b)\n\n        # COSINE SIMILARITY DISTANCE\n        cts = cupy.matmul(embeddings, embeddings[a:b].T).T\n        for k in range(b-a):\n            IDX = cupy.where(cts[k,]>threshold)[0]\n            o = df.iloc[cupy.asnumpy(IDX)].posting_id.values\n            preds.append(o)\n            \n    return preds","metadata":{"papermill":{"duration":0.024682,"end_time":"2021-04-12T04:30:33.822095","exception":false,"start_time":"2021-04-12T04:30:33.797413","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Load-in and Preparation","metadata":{"papermill":{"duration":0.013858,"end_time":"2021-04-12T04:30:33.850209","exception":false,"start_time":"2021-04-12T04:30:33.836351","status":"completed"},"tags":[]}},{"cell_type":"code","source":"df,df_cu,image_paths = read_dataset(COMPUTE_CV)\ndf.head()","metadata":{"papermill":{"duration":7.410488,"end_time":"2021-04-12T04:30:41.274815","exception":false,"start_time":"2021-04-12T04:30:33.864327","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if not COMPUTE_CV:\n    image_embeddings = get_image_embeddings(image_paths.values)\n    if SAVE_IMGEMBEDDING: np.savetxt('image_embeddings_tf_efficientnet_b4.csv', image_embeddings, delimiter=',')\nelse:\n    image_embeddings = np.loadtxt('../input/shopee-price-match-guarantee-embeddings/image_embeddings_tf_efficientnet_b4.csv', delimiter=',')\n\ntext_embeddings = get_text_embeddings(df_cu)\n\nif BASELINE_CHECKING:\n    text_predictions = get_text_predictions(df, text_embeddings)\n    df, image_predictions = get_image_neighbors(df, image_embeddings, KNN=100 if len(df)>3 else 3)\n    df.head()","metadata":{"papermill":{"duration":19.691996,"end_time":"2021-04-12T04:31:00.981663","exception":false,"start_time":"2021-04-12T04:30:41.289667","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Preparing Submission (Pre-searching)","metadata":{"papermill":{"duration":0.021171,"end_time":"2021-04-12T04:31:01.022378","exception":false,"start_time":"2021-04-12T04:31:01.001207","status":"completed"},"tags":[]}},{"cell_type":"code","source":"if BASELINE_CHECKING:\n    df['image_predictions'] = image_predictions\n    df['text_predictions'] = text_predictions\n    df['matches'] = df.apply(combine_predictions, axis=1)\n    df[['posting_id', 'matches']].to_csv('submission.csv', index=False)","metadata":{"papermill":{"duration":0.176279,"end_time":"2021-04-12T04:31:01.220328","exception":false,"start_time":"2021-04-12T04:31:01.044049","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## CV Score (BASELINE_CHECKING)","metadata":{}},{"cell_type":"code","source":"def getMetric(col):\n    def f1score(row):\n        n = len(np.intersect1d(row.target, row[col]))\n        return 2*n / (len(row.target) + len(row[col]))\n    return f1score","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if COMPUTE_CV and BASELINE_CHECKING:\n    df['matches_CV'] = df.apply(combine_for_cv, axis=1)\n    tmp = df.groupby('label_group').posting_id.agg('unique').to_dict()\n    df['target'] = df.label_group.map(tmp)\n    MyCVScore = df.apply(getMetric('matches_CV'), axis=1)\n    print('CV score =', MyCVScore.mean())\nelif COMPUTE_CV:\n    tmp = df.groupby('label_group').posting_id.agg('unique').to_dict()\n    df['target'] = df.label_group.map(tmp)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Good Neighbors Searching","metadata":{}},{"cell_type":"code","source":"# To find the finest neighbors\n\ndef threshold_searching(df, imgtxt, embeddings,\n                       img_LB=4.0, img_UB=6.0, txt_LB=0.7, txt_UB=0.9,\n                       KNN=100, max_features=25_000):\n    df1 = pd.DataFrame(columns = ['target', 'pred_matches'])\n    df1.target = df.target\n    \n    if imgtxt == 'img':\n        thresholds = list(np.arange(img_LB, img_UB, 0.1))\n        scores = []\n        for threshold in thresholds:\n            _, image_predictions = get_image_neighbors(df, embeddings, KNN, threshold=threshold)\n            df1.pred_matches = image_predictions\n            MyCVScore = df1.apply(getMetric('pred_matches'), axis=1)\n            score = MyCVScore.mean()\n            print(f'CV score for threshold {threshold} = {score}')\n            scores.append(score)\n        thresholds_scores = pd.DataFrame({'thresholds': thresholds, 'scores': scores})\n        max_score = thresholds_scores[thresholds_scores['scores'] == thresholds_scores['scores'].max()]\n        best_threshold = max_score['thresholds'].values[0]\n        best_score = max_score['scores'].values[0]\n        print(f'Our best score is {best_score} and has a threshold {best_threshold}')\n        \n    elif imgtxt == 'txt':\n        thresholds = list(np.arange(txt_LB, txt_UB, 0.01))\n        scores = []\n        for threshold in thresholds:\n            text_predictions = get_text_predictions(df, embeddings, max_features=max_features, threshold=threshold)\n            df1.pred_matches = text_predictions\n            MyCVScore = df1.apply(getMetric('pred_matches'), axis=1)\n            score = MyCVScore.mean()\n            print(f'CV score for threshold {threshold} = {score}')\n            scores.append(score)\n        thresholds_scores = pd.DataFrame({'thresholds': thresholds, 'scores': scores})\n        max_score = thresholds_scores[thresholds_scores['scores'] == thresholds_scores['scores'].max()]\n        best_threshold = max_score['thresholds'].values[0]\n        best_score = max_score['scores'].values[0]\n        print(f'Our best score is {best_score} and has a threshold {best_threshold}')\n    \n    return best_threshold","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if COMPUTE_CV and NEIGHBORS_SEARCHING: \n    best_threshold_img = threshold_searching(df, 'img', image_embeddings, img_LB=5.5, img_UB=6.0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# CV score = 0.7934218709044448 (only tune img)\n# CV score = 0.7949996561875796 (combining txt 0.7)\n# CV score = 0.7687872470155758 (combining txt 0.53)\n\nif COMPUTE_CV: \n    import random\n    text_embeddings[random.randint(0, 34250-1)][random.randint(0, 25000-1)]","metadata":{"_kg_hide-input":false,"_kg_hide-output":false},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if COMPUTE_CV and NEIGHBORS_SEARCHING: \n    best_threshold_txt = threshold_searching(df, 'txt', text_embeddings, txt_LB=0.7, txt_UB=0.76)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Preparing Submission (Post-searching)\n\nAccording to the model in [this notebook](https://www.kaggle.com/anlgrbz/how-optimum-threshold-changes-with-embed-test-size), optimum threshold decreases by 4.9636210^-6 for 1 increase in test set size. Given that hidden test size is 70000. -- slope of the regression line is $-4.9636210^-6$ for embedding size 5000\n\n> (70000−10000)∗4.96362∗10^(−6)=0.2978\n\nHence, decrease your `threshold` by  `0.2978`  to use in your final inference kernel.","metadata":{}},{"cell_type":"code","source":"if COMPUTE_CV and NEIGHBORS_SEARCHING:\n    best_threshold_img = best_threshold_img\n    best_threshold_txt = best_threshold_txt\nelse: \n    if THRES_METH == 'THRES_OPTIMIZED':\n        best_threshold_img = 5.6 - 0.2978\n        best_threshold_txt = 0.53 * (1 + (1 - (5.6-0.2978)/5.6))\n    elif THRES_METH == 'THRES':\n        best_threshold_img = 5.6\n        best_threshold_txt = 0.7\n    elif THRES_METH == 'BOOM':\n        best_threshold_img = 4.5\n        best_threshold_txt = 0.75\n    elif THRES_METH == 'BOOM_OPTIMIZED':\n        best_threshold_img = 4.5 - 0.2978\n        best_threshold_txt = 0.75 * (1 + (1 - (4.5-0.2978)/4.5))\n        \ntext_predictions = get_text_predictions(df, text_embeddings, threshold=best_threshold_txt)\ndf, image_predictions = get_image_neighbors(df, image_embeddings, KNN=100 if len(df)>3 else 3, threshold=best_threshold_img)\n\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import transformers\n\nNUM_WORKERS = 4\nBATCH_SIZE = 16\nSEED = 42\n\ndevice = torch.device('cuda')\n\n################################################# MODEL ####################################################################\n\ntransformer_model = '../input/sentence-transformer-models/paraphrase-xlm-r-multilingual-v1/0_Transformer'\nTOKENIZER = transformers.AutoTokenizer.from_pretrained(transformer_model)\n\n################################################ MODEL PATH ###############################################################\n\nTEXT_MODEL_PATH = '../input/best-multilingual-model/sentence_transfomer_xlm_best_loss_num_epochs_25_arcface.bin'\n\nmodel_params = {\n    'n_classes':11014,\n    'model_name':transformer_model,\n    'use_fc':False,\n    'fc_dim':512,\n    'dropout':0.3,\n}\n\nclass ShopeeDataset(Dataset):\n    def __init__(self, csv):\n        self.csv = csv.reset_index()\n\n    def __len__(self):\n        return self.csv.shape[0]\n\n    def __getitem__(self, index):\n        row = self.csv.iloc[index]\n        \n        text = row.title\n        \n        text = TOKENIZER(text, padding='max_length', truncation=True, max_length=128, return_tensors=\"pt\")\n        input_ids = text['input_ids'][0]\n        attention_mask = text['attention_mask'][0]  \n        \n        return input_ids, attention_mask\n\n\nclass ShopeeNet(nn.Module):\n\n    def __init__(self,\n                 n_classes,\n                 model_name='bert-base-uncased',\n                 use_fc=False,\n                 fc_dim=512,\n                 dropout=0.0):\n        \"\"\"\n        :param n_classes:\n        :param model_name: name of model from pretrainedmodels\n            e.g. resnet50, resnext101_32x4d, pnasnet5large\n        :param pooling: One of ('SPoC', 'MAC', 'RMAC', 'GeM', 'Rpool', 'Flatten', 'CompactBilinearPooling')\n        :param loss_module: One of ('arcface', 'cosface', 'softmax')\n        \"\"\"\n        super(ShopeeNet, self).__init__()\n\n        self.transformer = transformers.AutoModel.from_pretrained(model_name)\n        final_in_features = self.transformer.config.hidden_size\n        \n        self.use_fc = use_fc\n    \n        if use_fc:\n            self.dropout = nn.Dropout(p=dropout)\n            self.fc = nn.Linear(final_in_features, fc_dim)\n            self.bn = nn.BatchNorm1d(fc_dim)\n            self._init_params()\n            final_in_features = fc_dim\n\n\n    def _init_params(self):\n        nn.init.xavier_normal_(self.fc.weight)\n        nn.init.constant_(self.fc.bias, 0)\n        nn.init.constant_(self.bn.weight, 1)\n        nn.init.constant_(self.bn.bias, 0)\n\n    def forward(self, input_ids,attention_mask):\n        feature = self.extract_feat(input_ids,attention_mask)\n        return F.normalize(feature)\n\n    def extract_feat(self, input_ids,attention_mask):\n        x = self.transformer(input_ids=input_ids,attention_mask=attention_mask)\n        \n        features = x[0]\n        features = features[:,0,:]\n\n        if self.use_fc:\n            features = self.dropout(features)\n            features = self.fc(features)\n            features = self.bn(features)\n\n        return features\n    \n    \ndef get_text_embeddings(df):\n    embeds = []\n    \n    model = ShopeeNet(**model_params)\n    model.eval()\n    \n    model.load_state_dict(dict(list(torch.load(TEXT_MODEL_PATH).items())[:-1]))\n    model = model.to(device)\n\n    text_dataset = ShopeeDataset(df)\n    text_loader = torch.utils.data.DataLoader(\n        text_dataset,\n        batch_size=BATCH_SIZE,\n        pin_memory=True,\n        drop_last=False,\n        num_workers=NUM_WORKERS\n    )\n    \n    \n    with torch.no_grad():\n        for input_ids, attention_mask in tqdm(text_loader): \n            input_ids = input_ids.cuda()\n            attention_mask = attention_mask.cuda()\n            feat = model(input_ids, attention_mask)\n            text_embeddings = feat.detach().cpu().numpy()\n            embeds.append(text_embeddings)\n    \n    \n    del model\n    text_embeddings = np.concatenate(embeds)\n    print(f'Our text embeddings shape is {text_embeddings.shape}')\n    del embeds\n    gc.collect()\n    return text_embeddings\n\n\ndef get_neighbours_cos_sim(df,embeddings):\n    '''\n    When using cos_sim use normalized features else use normal features\n    '''\n    embeddings = cupy.array(embeddings)\n    \n    if False:\n        thresholds = list(np.arange(0.5,0.7,0.05))\n\n        scores = []\n        for threshold in thresholds:\n            \n################################################# Code for Getting Preds #########################################\n            preds = []\n            CHUNK = 1024*4\n\n            print('Finding similar titles...for threshold :',threshold)\n            CTS = len(embeddings)//CHUNK\n            if len(embeddings)%CHUNK!=0: CTS += 1\n\n            for j in range( CTS ):\n                a = j*CHUNK\n                b = (j+1)*CHUNK\n                b = min(b,len(embeddings))\n\n                cts = cupy.matmul(embeddings,embeddings[a:b].T).T\n\n                for k in range(b-a):\n                    IDX = cupy.where(cts[k,]>threshold)[0]\n                    o = df.iloc[cupy.asnumpy(IDX)].posting_id.values\n                    o = ' '.join(o)\n                    preds.append(o)\n######################################################################################################################\n            df['pred_matches'] = preds\n            df['f1'] = f1_score(df['matches'], df['pred_matches'])\n            score = df['f1'].mean()\n            print(f'Our f1 score for threshold {threshold} is {score}')\n            scores.append(score)\n            \n        thresholds_scores = pd.DataFrame({'thresholds': thresholds, 'scores': scores})\n        max_score = thresholds_scores[thresholds_scores['scores'] == thresholds_scores['scores'].max()]\n        best_threshold = max_score['thresholds'].values[0]\n        best_score = max_score['scores'].values[0]\n        print(f'Our best score is {best_score} and has a threshold {best_threshold}')\n            \n    else:\n        preds = []\n        CHUNK = 1024*4\n        threshold = 0.8\n\n        print('Finding similar texts...for threshold :',threshold)\n        CTS = len(embeddings)//CHUNK\n        if len(embeddings)%CHUNK!=0: CTS += 1\n\n        for j in range( CTS ):\n            a = j*CHUNK\n            b = (j+1)*CHUNK\n            b = min(b,len(embeddings))\n            print('chunk',a,'to',b)\n\n            cts = cupy.matmul(embeddings,embeddings[a:b].T).T\n\n            for k in range(b-a):\n                IDX = cupy.where(cts[k,]>threshold)[0]\n                o = df.iloc[cupy.asnumpy(IDX)].posting_id.values\n                preds.append(o)\n                    \n    return df, preds","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"text_embeddings_BERT = get_text_embeddings(df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"_, text_predictions_BERT = get_neighbours_cos_sim(df, text_embeddings_BERT)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['image_predictions'] = image_predictions\ndf['text_predictions'] = text_predictions\ndf['text_predictions_BERT'] = text_predictions_BERT\nif PHASH:\n    tmp = df.groupby('image_phash').posting_id.agg('unique').to_dict()\n    df['phash_predictions'] = df.image_phash.map(tmp)\n    df['matches'] = df.apply(combine_predictions_phash, axis=1)\nelse: \n    df['matches'] = df.apply(combine_predictions, axis=1)    \ndf[['posting_id', 'matches']].to_csv('submission.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## CV Score (FINAL)","metadata":{}},{"cell_type":"code","source":"if COMPUTE_CV:\n    if PHASH:\n        df['matches_CV'] = df.apply(combine_for_cv_phash, axis=1)\n    else:\n        df['matches_CV'] = df.apply(combine_for_cv, axis=1)\n    MyCVScore = df.apply(getMetric('matches_CV'), axis=1)\n    print('CV score =', MyCVScore.mean())\n\nprint(f'COMPUTE_CV = {COMPUTE_CV}')\nprint(f'NEIGHBORS_SEARCHING = {NEIGHBORS_SEARCHING}')\nprint(f'BASELINE_CHECKING = {BASELINE_CHECKING}')\nprint(f'SAVE_IMGEMBEDDING = {SAVE_IMGEMBEDDING}')\nprint(f'THRES_METH = {THRES_METH}')\nprint(f'FIXING = {FIXING}')\nprint(f'PHASH = {PHASH}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"|   | CV | LB |\n| - | -- | -- |\n| 5.6 0.7 | 0.795 |   |\n| (fixing) 5.6 0.7 | 0.795 | 0.688 |\n| (fixing) BOOM | 0.774 | 0.728 |\n| (fixing) BOOM PHASH | 0.774 | 0.728 |\n| (fixing) BOOM OPTIMIZED | 0.757 | 0.722 |\n| (fixing) 5.6 0.53 | 0.769 |   | \n| (fixing) 5.6 0.53 OPTIMIZED | 0.782 | 0.675 |\n| BOOM | 0.774 | 0.728 |\n| BOOM OPTIMIZED | 0.757 |   |\n| BERT w/o TFIDF (fixing) BOOM PHASH | 0.975 | 0.716 |\n| BERT w/ TFIDF (fixing) BOOM PHASH | 0.958 | 0.719 |\n| BERT80 w/ TFIDF (fixing) BOOM PHASH |   |   |","metadata":{}},{"cell_type":"markdown","source":"# References\n\n* [Shopee| text, img Embedding (Colab enabled)](https://www.kaggle.com/chienhsianghung/shopee-text-img-embedding-colab-enabled)\n* [Shopee - Price Match Guarantee| Embeddings](https://www.kaggle.com/chienhsianghung/shopee-price-match-guarantee-embeddings)\n* [Eff-B4 + TFIDF >= 0.728](https://www.kaggle.com/vatsalmavani/eff-b4-tfidf-0-728)\n* [Reaching 0.612 with Text Only : Shopee](https://www.kaggle.com/tanulsingh077/reaching-0-612-with-text-only-shopee)","metadata":{}}]}