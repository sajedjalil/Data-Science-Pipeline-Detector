{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Abstract\n\nHi guys, I've added some functions to this notebook (including a naive `threshold_searching`). To test LB or CV score, you're welcome to just simply **Copy and Edit** then **Commit** it. Enjoy exploring!<br>\nAnd, don't forget to use `threshold_searching` to find your best neighbors.\n\nTo save time from generating img Embedding, use pre-saved embedding for faster work. You can use this dataset directly: [Shopee - Price Match Guarantee| Embeddings](https://www.kaggle.com/chienhsianghung/shopee-price-match-guarantee-embeddings).<br>\nI've also done some experiments on KNN and Cosine Similarity in the previous notebook. Click here to see the result: [Shopee| text, img Embedding (Colab enabled)](https://www.kaggle.com/chienhsianghung/shopee-text-img-embedding-colab-enabled).","metadata":{"papermill":{"duration":0.013862,"end_time":"2021-04-12T04:30:24.618934","exception":false,"start_time":"2021-04-12T04:30:24.605072","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"## Notes\n\n* By turning on ` NEIGHBORS_SEARCHING` and using`threshold_searching` function, one may get higher F1 score.\n* Use [this dataset](https://www.kaggle.com/chienhsianghung/shopee-price-match-guarantee-embeddings) to save your time.\n* Change `THRES_METH` to test your searching on LB.\n* Special thanks to [@vatsalmavani](https://www.kaggle.com/vatsalmavani).\n  * This [notebook](https://www.kaggle.com/vatsalmavani/eff-b4-tfidf-0-728) works with any EfficientNet(B0 - B7) Model.\n  * Training Notebook for EfficientNet-B4 can be found [here](https://www.kaggle.com/vatsalmavani/shopee-training-eff-b4)","metadata":{}},{"cell_type":"code","source":"import sys\nsys.path.append('../input/timm-pytorch-image-models/pytorch-image-models-master')","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","papermill":{"duration":0.020911,"end_time":"2021-04-12T04:30:24.67797","exception":false,"start_time":"2021-04-12T04:30:24.657059","status":"completed"},"tags":[],"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport cv2\nimport math\nimport random\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\n\nimport albumentations\nfrom albumentations.pytorch.transforms import ToTensorV2\n\nimport torch\nimport timm\nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\nfrom torch.utils.data import Dataset,DataLoader\n\nimport gc\nimport matplotlib.pyplot as plt\nimport cudf\nimport cuml\nimport cupy\nfrom cuml.feature_extraction.text import TfidfVectorizer\nfrom cuml import PCA\nfrom cuml.neighbors import NearestNeighbors","metadata":{"papermill":{"duration":8.621584,"end_time":"2021-04-12T04:30:33.312428","exception":false,"start_time":"2021-04-12T04:30:24.690844","status":"completed"},"tags":[],"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Config","metadata":{"papermill":{"duration":0.012895,"end_time":"2021-04-12T04:30:33.338671","exception":false,"start_time":"2021-04-12T04:30:33.325776","status":"completed"},"tags":[]}},{"cell_type":"code","source":"COMPUTE_CV = True\nNEIGHBORS_SEARCHING = True\nSAVE_IMGEMBEDDING = False\nBASELINE_CHECKING = False\nTHRES_METH = 'BOOM' # 'BOOM', 'BOOM_OPTIMIZED', 'THRES', 'THRES_OPTIMIZED'\nSTOP_WORDS = None # 'english', None\nIMG_COSINE = True\n\ndf = pd.read_csv('../input/shopee-product-matching/test.csv')\nif len(df)>3: COMPUTE_CV = False\nif COMPUTE_CV: \n    print('this submission notebook will compute CV score but commit notebook will not')\nelse:\n    print('this submission notebook will only be used to submit result')\n\nclass CFG:\n    seed = 54\n    classes = 11014 \n    scale = 30 \n    margin = 0.5\n    model_name =  'tf_efficientnet_b4'\n    fc_dim = 512\n    img_size = 512\n    batch_size = 20\n    num_workers = 4\n    device = device = 'cuda' if torch.cuda.is_available() else 'cpu'\n    model_path = '../input/utils-shopee/arcface_512x512_tf_efficientnet_b4_LR.pt'","metadata":{"papermill":{"duration":0.020052,"end_time":"2021-04-12T04:30:33.371579","exception":false,"start_time":"2021-04-12T04:30:33.351527","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Utils","metadata":{"papermill":{"duration":0.013995,"end_time":"2021-04-12T04:30:33.398316","exception":false,"start_time":"2021-04-12T04:30:33.384321","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def read_dataset(COMPUTE_CV):\n    \n    if COMPUTE_CV:\n        df = pd.read_csv('../input/shopee-product-matching/train.csv')\n        df_cu = cudf.DataFrame(df)\n        image_paths = '../input/shopee-product-matching/train_images/' + df['image']\n    \n    else:\n        df = pd.read_csv('../input/shopee-product-matching/test.csv')\n        df_cu = cudf.DataFrame(df)\n        image_paths = '../input/shopee-product-matching/test_images/' + df['image']\n\n    return df, df_cu, image_paths","metadata":{"papermill":{"duration":0.020165,"end_time":"2021-04-12T04:30:33.431442","exception":false,"start_time":"2021-04-12T04:30:33.411277","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def seed_torch(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    \nseed_torch(CFG.seed)","metadata":{"papermill":{"duration":0.024171,"end_time":"2021-04-12T04:30:33.468514","exception":false,"start_time":"2021-04-12T04:30:33.444343","status":"completed"},"tags":[],"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def combine_predictions(row):\n    x = np.concatenate([row['image_predictions'], row['text_predictions'], row['phash_predictions']])\n    return ' '.join( np.unique(x) )\n\ndef combine_predictions_cosine(row):\n    x = np.concatenate([row['image_predictions'], row['text_predictions'], row['phash_predictions'], row['image_predictions_cosine']])\n    return ' '.join( np.unique(x) )\n\ndef combine_for_cv(row):\n    x = np.concatenate([row['image_predictions'], row['text_predictions'], row['phash_predictions']])\n    return np.unique(x)\n\ndef combine_for_cv_cosine(row):\n    x = np.concatenate([row['image_predictions'], row['text_predictions'], row['phash_predictions'], row['image_predictions_cosine']])\n    return np.unique(x)","metadata":{"papermill":{"duration":0.019856,"end_time":"2021-04-12T04:30:33.536329","exception":false,"start_time":"2021-04-12T04:30:33.516473","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Image Predictions","metadata":{"papermill":{"duration":0.013363,"end_time":"2021-04-12T04:30:33.563025","exception":false,"start_time":"2021-04-12T04:30:33.549662","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Create Model\n\nclass ArcMarginProduct(nn.Module):\n    def __init__(self, in_features, out_features, scale=30.0, margin=0.50, easy_margin=False, ls_eps=0.0):\n        super(ArcMarginProduct, self).__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.scale = scale\n        self.margin = margin\n        self.ls_eps = ls_eps\n        self.weight = nn.Parameter(torch.FloatTensor(out_features, in_features))\n        nn.init.xavier_uniform_(self.weight)\n\n        self.easy_margin = easy_margin\n        self.cos_m = math.cos(margin)\n        self.sin_m = math.sin(margin)\n        self.th = math.cos(math.pi - margin)\n        self.mm = math.sin(math.pi - margin) * margin\n\n    def forward(self, input, label):\n        cosine = F.linear(F.normalize(input), F.normalize(self.weight))\n        sine = torch.sqrt(1.0 - torch.pow(cosine, 2))\n        phi = cosine * self.cos_m - sine * self.sin_m\n        if self.easy_margin:\n            phi = torch.where(cosine > 0, phi, cosine)\n        else:\n            phi = torch.where(cosine > self.th, phi, cosine - self.mm)\n    \n        one_hot = torch.zeros(cosine.size(), device='cuda')\n        one_hot.scatter_(1, label.view(-1, 1).long(), 1)\n        if self.ls_eps > 0:\n            one_hot = (1 - self.ls_eps) * one_hot + self.ls_eps / self.out_features\n\n        output = (one_hot * phi) + ((1.0 - one_hot) * cosine)\n        output *= self.scale\n\n        return output, nn.CrossEntropyLoss()(output,label)\n\n\nclass ShopeeModel(nn.Module):\n\n    def __init__(\n        self,\n        n_classes = CFG.classes,\n        model_name = CFG.model_name,\n        fc_dim = CFG.fc_dim,\n        margin = CFG.margin,\n        scale = CFG.scale,\n        use_fc = True,\n        pretrained = True):\n\n        super(ShopeeModel,self).__init__()\n        print('Building Model Backbone for {} model'.format(model_name))\n\n        self.backbone = timm.create_model(model_name, pretrained=pretrained)\n        in_features = self.backbone.classifier.in_features\n        self.backbone.classifier = nn.Identity()\n        self.backbone.global_pool = nn.Identity()\n        self.pooling =  nn.AdaptiveAvgPool2d(1)\n        self.use_fc = use_fc\n\n        if use_fc:\n            self.dropout = nn.Dropout(p=0.1)\n            self.classifier = nn.Linear(in_features, fc_dim)\n            self.bn = nn.BatchNorm1d(fc_dim)\n            self._init_params()\n            in_features = fc_dim\n\n        self.final = ArcMarginProduct(\n            in_features,\n            n_classes,\n            scale = scale,\n            margin = margin,\n            easy_margin = False,\n            ls_eps = 0.0\n        )\n\n    def _init_params(self):\n        nn.init.xavier_normal_(self.classifier.weight)\n        nn.init.constant_(self.classifier.bias, 0)\n        nn.init.constant_(self.bn.weight, 1)\n        nn.init.constant_(self.bn.bias, 0)\n\n    def forward(self, image, label):\n        features = self.extract_features(image)\n        if self.training:\n            logits = self.final(features, label)\n            return logits\n        else:\n            return features\n\n    def extract_features(self, x):\n        batch_size = x.shape[0]\n        x = self.backbone(x)\n        x = self.pooling(x).view(batch_size, -1)\n\n        if self.use_fc and self.training:\n            x = self.dropout(x)\n            x = self.classifier(x)\n            x = self.bn(x)\n        return x","metadata":{"papermill":{"duration":0.035463,"end_time":"2021-04-12T04:30:33.612047","exception":false,"start_time":"2021-04-12T04:30:33.576584","status":"completed"},"tags":[],"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_image_neighbors(df, embeddings, KNN=100, threshold=4.5, metric='minkowski'):\n    \n    if metric == 'cosine':\n        model = NearestNeighbors(n_neighbors = KNN, metric = metric)\n    else:\n        model = NearestNeighbors(n_neighbors = KNN)\n    model.fit(embeddings)\n    distances, indices = model.kneighbors(embeddings)\n    \n    predictions = []\n    for k in tqdm(range(embeddings.shape[0])):\n        idx = np.where(distances[k,] < threshold)[0]\n        ids = indices[k,idx]\n        posting_ids = df['posting_id'].iloc[ids].values\n        predictions.append(posting_ids)\n\n    return df, predictions","metadata":{"papermill":{"duration":0.022049,"end_time":"2021-04-12T04:30:33.64753","exception":false,"start_time":"2021-04-12T04:30:33.625481","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_test_transforms():\n    return albumentations.Compose([\n        albumentations.Resize(CFG.img_size, CFG.img_size, always_apply=True),\n        albumentations.Normalize(),\n        ToTensorV2(p=1.0)\n    ])","metadata":{"papermill":{"duration":0.020559,"end_time":"2021-04-12T04:30:33.681696","exception":false,"start_time":"2021-04-12T04:30:33.661137","status":"completed"},"tags":[],"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ShopeeDataset(Dataset):\n\n    def __init__(self, image_paths, transforms=None):\n        self.image_paths = image_paths\n        self.augmentations = transforms\n\n    def __len__(self):\n        return self.image_paths.shape[0]\n\n    def __getitem__(self, index):\n        image_path = self.image_paths[index]\n        image = cv2.imread(image_path)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        if self.augmentations:\n            augmented = self.augmentations(image=image)\n            image = augmented['image']\n        \n        return image, torch.tensor(1)","metadata":{"papermill":{"duration":0.022188,"end_time":"2021-04-12T04:30:33.717959","exception":false,"start_time":"2021-04-12T04:30:33.695771","status":"completed"},"tags":[],"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_image_embeddings(image_paths):\n\n    model = ShopeeModel(pretrained=False).to(CFG.device)\n    model.load_state_dict(torch.load(CFG.model_path))\n    model.eval()\n\n    image_dataset = ShopeeDataset(image_paths=image_paths, transforms=get_test_transforms())\n    image_loader = torch.utils.data.DataLoader(\n        image_dataset,\n        batch_size=CFG.batch_size,\n        num_workers=CFG.num_workers\n    )\n\n    embeds = []\n    with torch.no_grad():\n        for img,label in tqdm(image_loader): \n            img = img.cuda()\n            label = label.cuda()\n            features = model(img,label)\n            image_embeddings = features.detach().cpu().numpy()\n            embeds.append(image_embeddings)\n\n    image_embeddings = np.concatenate(embeds)\n    print(f'Our image embeddings shape is {image_embeddings.shape}')\n    return image_embeddings","metadata":{"papermill":{"duration":0.02367,"end_time":"2021-04-12T04:30:33.75544","exception":false,"start_time":"2021-04-12T04:30:33.73177","status":"completed"},"tags":[],"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Text Predictions","metadata":{"papermill":{"duration":0.013862,"end_time":"2021-04-12T04:30:33.783134","exception":false,"start_time":"2021-04-12T04:30:33.769272","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def get_text_embeddings(df_cu, max_features=25_000):\n    model = TfidfVectorizer(stop_words=STOP_WORDS,\n                            binary=True,\n                            max_features=max_features)\n    text_embeddings = model.fit_transform(df_cu['title']).toarray()\n    return text_embeddings\n    \n    \ndef get_text_predictions(df, embeddings, max_features=25_000, threshold=0.75, PRINT_CHUNK=True):\n    print('Finding similar titles...')\n    CHUNK = 1024 * 4\n    CTS = len(df) // CHUNK\n    if (len(df)%CHUNK) != 0:\n        CTS += 1\n\n    preds = []\n    for j in range( CTS ):\n        a = j * CHUNK\n        b = (j+1) * CHUNK\n        b = min(b, len(df))\n        if PRINT_CHUNK:\n            print('chunk', a, 'to', b)\n\n        # COSINE SIMILARITY DISTANCE\n        cts = cupy.matmul(embeddings, embeddings[a:b].T).T\n        for k in range(b-a):\n            IDX = cupy.where(cts[k,]>threshold)[0]\n            o = df.iloc[cupy.asnumpy(IDX)].posting_id.values\n            preds.append(o)\n            \n    return preds","metadata":{"papermill":{"duration":0.024682,"end_time":"2021-04-12T04:30:33.822095","exception":false,"start_time":"2021-04-12T04:30:33.797413","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Load-in and Preparation","metadata":{"papermill":{"duration":0.013858,"end_time":"2021-04-12T04:30:33.850209","exception":false,"start_time":"2021-04-12T04:30:33.836351","status":"completed"},"tags":[]}},{"cell_type":"code","source":"df,df_cu,image_paths = read_dataset(COMPUTE_CV)\ndf.head()","metadata":{"papermill":{"duration":7.410488,"end_time":"2021-04-12T04:30:41.274815","exception":false,"start_time":"2021-04-12T04:30:33.864327","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if not COMPUTE_CV:\n    image_embeddings = get_image_embeddings(image_paths.values)\n    if SAVE_IMGEMBEDDING: np.savetxt('image_embeddings_tf_efficientnet_b4.csv', image_embeddings, delimiter=',')\nelse:\n    image_embeddings = np.loadtxt('../input/shopee-price-match-guarantee-embeddings/image_embeddings_tf_efficientnet_b4.csv', delimiter=',')\n\ntext_embeddings = get_text_embeddings(df_cu)\n\nif BASELINE_CHECKING:\n    text_predictions = get_text_predictions(df, text_embeddings)\n    df, image_predictions = get_image_neighbors(df, image_embeddings, KNN=100 if len(df)>3 else 3)\n    df.head()","metadata":{"papermill":{"duration":19.691996,"end_time":"2021-04-12T04:31:00.981663","exception":false,"start_time":"2021-04-12T04:30:41.289667","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if COMPUTE_CV: \n    import random\n    print(f'text_embeddings.shape : {text_embeddings.shape}')\n    print(f'randomly check a vector inside text_embeddings: {text_embeddings[random.randint(0, 34250-1), random.randint(0, 24939-1)]}')\n    print(f'text_embeddings\\'s average: {cupy.mean(text_embeddings)}')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Preparing Submission (Pre-searching)","metadata":{"papermill":{"duration":0.021171,"end_time":"2021-04-12T04:31:01.022378","exception":false,"start_time":"2021-04-12T04:31:01.001207","status":"completed"},"tags":[]}},{"cell_type":"code","source":"if BASELINE_CHECKING:\n    df['image_predictions'] = image_predictions\n    df['text_predictions'] = text_predictions\n    df['matches'] = df.apply(combine_predictions, axis=1)\n    df[['posting_id', 'matches']].to_csv('submission.csv', index=False)","metadata":{"papermill":{"duration":0.176279,"end_time":"2021-04-12T04:31:01.220328","exception":false,"start_time":"2021-04-12T04:31:01.044049","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## CV Score (BASELINE_CHECKING)","metadata":{}},{"cell_type":"code","source":"def getMetric(col):\n    def f1score(row):\n        n = len(np.intersect1d(row.target, row[col]))\n        return 2*n / (len(row.target) + len(row[col]))\n    return f1score","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if COMPUTE_CV and BASELINE_CHECKING:\n    df['matches_CV'] = df.apply(combine_for_cv, axis=1)\n    tmp = df.groupby('label_group').posting_id.agg('unique').to_dict()\n    df['target'] = df.label_group.map(tmp)\n    MyCVScore = df.apply(getMetric('matches_CV'), axis=1)\n    print('CV score =', MyCVScore.mean())\nelif COMPUTE_CV:\n    tmp = df.groupby('label_group').posting_id.agg('unique').to_dict()\n    df['target'] = df.label_group.map(tmp)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Good Neighbors Searching","metadata":{}},{"cell_type":"code","source":"# To find the finest neighbors\n\ndef threshold_searching(df, imgtxt, embeddings,\n                        LB=4.0, UB=6.0,\n                        PRINT_CHUNK=False, metric='minkowski'):\n    df1 = pd.DataFrame(columns = ['target', 'pred_matches'])\n    df1.target = df.target\n    \n    if imgtxt == 'img':\n        if metric == 'cosine':\n            thresholds = list(np.arange(LB, UB, 0.02))\n        else:\n            thresholds = list(np.arange(LB, UB, 0.2))\n        scores = []\n        for threshold in thresholds:\n            _, image_predictions = get_image_neighbors(df, embeddings, threshold=threshold, metric=metric)\n            df1.pred_matches = image_predictions\n            MyCVScore = df1.apply(getMetric('pred_matches'), axis=1)\n            score = MyCVScore.mean()\n            print(f'CV score for threshold {threshold} = {score}')\n            scores.append(score)\n        thresholds_scores = pd.DataFrame({'thresholds': thresholds, 'scores': scores})\n        max_score = thresholds_scores[thresholds_scores['scores'] == thresholds_scores['scores'].max()]\n        best_threshold = max_score['thresholds'].values[0]\n        best_score = max_score['scores'].values[0]\n        print(f'Our best score is {best_score} and has a threshold {best_threshold}')\n        \n    elif imgtxt == 'txt':\n        thresholds = list(np.arange(LB, UB, 0.02))\n        scores = []\n        for threshold in thresholds:\n            text_predictions = get_text_predictions(df, embeddings, threshold=threshold, PRINT_CHUNK=PRINT_CHUNK)\n            df1.pred_matches = text_predictions\n            MyCVScore = df1.apply(getMetric('pred_matches'), axis=1)\n            score = MyCVScore.mean()\n            print(f'CV score for threshold {threshold} = {score}')\n            scores.append(score)\n        thresholds_scores = pd.DataFrame({'thresholds': thresholds, 'scores': scores})\n        max_score = thresholds_scores[thresholds_scores['scores'] == thresholds_scores['scores'].max()]\n        best_threshold = max_score['thresholds'].values[0]\n        best_score = max_score['scores'].values[0]\n        print(f'Our best score is {best_score} and has a threshold {best_threshold}')\n    \n    return best_threshold","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if COMPUTE_CV and NEIGHBORS_SEARCHING: \n    best_threshold_img = threshold_searching(df, 'img', image_embeddings, LB=4.5, UB=4.6)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if COMPUTE_CV and NEIGHBORS_SEARCHING: \n    best_threshold_txt = threshold_searching(df, 'txt', text_embeddings, LB=0.75, UB=0.76)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if COMPUTE_CV and NEIGHBORS_SEARCHING and IMG_COSINE: \n    best_threshold_img_cosine = threshold_searching(df, 'img', image_embeddings, LB=0.18, UB=0.19, metric='cosine')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Preparing Submission (Post-searching)\n\nAccording to the model in [this notebook](https://www.kaggle.com/anlgrbz/how-optimum-threshold-changes-with-embed-test-size), optimum threshold decreases by 4.9636210^-6 for 1 increase in test set size. Given that hidden test size is 70000. -- slope of the regression line is $-4.9636210^-6$ for embedding size 5000\n\n> (70000−10000)∗4.96362∗10^(−6)=0.2978\n\nHence, decrease your `threshold` by  `0.2978`  to use in your final inference kernel.","metadata":{}},{"cell_type":"code","source":"if COMPUTE_CV and NEIGHBORS_SEARCHING:\n    best_threshold_img = best_threshold_img\n    best_threshold_txt = best_threshold_txt\n    if IMG_COSINE:\n        best_threshold_img_cosine = best_threshold_img_cosine\nelse: \n    # adopt your results here\n    if THRES_METH == 'THRES_OPTIMIZED':\n        best_threshold_img = 5.6 - 0.2978\n        best_threshold_txt = 0.53 * (1 + (1 - (5.6-0.2978)/5.6))\n        best_threshold_img_cosine = 0.44 * ((5.6-0.2978)/5.6)\n\n    elif THRES_METH == 'THRES':\n        best_threshold_img = 5.6\n        best_threshold_txt = 0.7\n        best_threshold_img_cosine = 0.4\n    elif THRES_METH == 'BOOM':\n        best_threshold_img = 4.5\n        best_threshold_txt = 0.75\n        best_threshold_img_cosine = 0.18\n    elif THRES_METH == 'BOOM_OPTIMIZED':\n        best_threshold_img = 4.5 - 0.2978\n        best_threshold_txt = 0.75 * (1 + (1 - (4.5-0.2978)/4.5))\n        \ntext_predictions = get_text_predictions(df, text_embeddings, threshold=best_threshold_txt)\ndf, image_predictions = get_image_neighbors(df, image_embeddings, KNN=100 if len(df)>3 else 3, threshold=best_threshold_img)\nif IMG_COSINE:\n    _, image_predictions_cosine = get_image_neighbors(df, image_embeddings, KNN=100 if len(df)>3 else 3, threshold=best_threshold_img_cosine, metric='cosine')\n\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['image_predictions'] = image_predictions\ndf['text_predictions'] = text_predictions\ntmp = df.groupby('image_phash').posting_id.agg('unique').to_dict()\ndf['phash_predictions'] = df.image_phash.map(tmp)\n\nif IMG_COSINE:\n    df['image_predictions_cosine'] = image_predictions_cosine\n    df['matches'] = df.apply(combine_predictions_cosine, axis=1)\nelse:\n    df['matches'] = df.apply(combine_predictions, axis=1)\n\ndf[['posting_id', 'matches']].to_csv('submission.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# CV Score (FINAL)","metadata":{}},{"cell_type":"code","source":"if COMPUTE_CV:\n    if IMG_COSINE:\n        df['matches_CV'] = df.apply(combine_for_cv_cosine, axis=1)\n    else:\n        df['matches_CV'] = df.apply(combine_for_cv, axis=1)\n    MyCVScore = df.apply(getMetric('matches_CV'), axis=1)\n    print('CV score =', MyCVScore.mean())\n\nprint(f'COMPUTE_CV = {COMPUTE_CV}')\nprint(f'NEIGHBORS_SEARCHING = {NEIGHBORS_SEARCHING}')\nprint(f'BASELINE_CHECKING = {BASELINE_CHECKING}')\nprint(f'SAVE_IMGEMBEDDING = {SAVE_IMGEMBEDDING}')\nprint(f'THRES_METH = {THRES_METH}')\nprint(f'STOP_WORDS = {STOP_WORDS}')\nprint(f'IMG_COSINE = {IMG_COSINE}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"|   | CV | LB |\n| - | -- | -- |\n| 5.6 0.7 | 0.795 |   |\n| (fixing) 5.6 0.7 | 0.795 | 0.688 |\n| (fixing) BOOM | 0.774 | 0.728 |\n| (fixing) BOOM PHASH | 0.774 | 0.728 |\n| (fixing) BOOM 0.18 | 0.780 | 0.728 |\n| (fixing) 4.7 0.75 0.18 | 0.785 |   |\n| (fixing) 4.7 0.75 0.20 | 0.786 |   |\n| (fixing) BOOM IMG_COSINE 0.36 | 0.808 | 0.723 |\n| (fixing) BOOM IMG_COSINE 0.44 | 0.809 | 0.698 |\n| (fixing) THRES IMG_COSINE 0.4 | 0.803 | 0.683 |\n| (fixing) BOOM OPTIMIZED | 0.757 | 0.722 |\n| (fixing) 5.6 0.53 | 0.769 |   | \n| 5.6 0.53 PHASH | 0.767 |   | \n| (fixing) 5.6 0.53 OPTIMIZED | 0.782 | 0.675 |\n| BOOM | 0.774 | 0.728 |\n| BOOM OPTIMIZED | 0.757 |   |\n| BERT w/o TFIDF (fixing) BOOM PHASH | 0.975 | 0.716 |\n| BERT w/ TFIDF (fixing) BOOM PHASH | 0.958 | 0.719 |\n| clean the multiple models (fixing) |   | 0.732 |","metadata":{}},{"cell_type":"markdown","source":"# References\n\n* [Shopee| text, img Embedding (Colab enabled)](https://www.kaggle.com/chienhsianghung/shopee-text-img-embedding-colab-enabled)\n* [Shopee - Price Match Guarantee| Embeddings](https://www.kaggle.com/chienhsianghung/shopee-price-match-guarantee-embeddings)\n* [Eff-B4 + TFIDF >= 0.728](https://www.kaggle.com/vatsalmavani/eff-b4-tfidf-0-728)","metadata":{}}]}