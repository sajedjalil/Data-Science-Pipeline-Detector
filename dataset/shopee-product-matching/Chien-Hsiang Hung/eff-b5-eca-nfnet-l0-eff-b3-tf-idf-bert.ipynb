{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# About\n\nYou can find my all training pipelines here.\n> - [Shopee Training: Eff-B3, B5](https://www.kaggle.com/chienhsianghung/shopee-training-eff-b3-b5)\n> - [Shopee PyTorch eca-nfnet-l0 Image [Training]](https://www.kaggle.com/chienhsianghung/shopee-pytorch-eca-nfnet-l0-image-training)\n\nThis notebook adopted results from these amazing kernels. Please go check them out if you like.\n- [@parthdhameliya77](https://www.kaggle.com/parthdhameliya77)\n - [PyTorch EfficientNet-B5 Image+TFIDF [Inference]](https://www.kaggle.com/parthdhameliya77/pytorch-efficientnet-b5-image-tfidf-inference)\n - [PyTorch eca-nfnet-l0 Image+TFIDF [inference]](https://www.kaggle.com/parthdhameliya77/pytorch-eca-nfnet-l0-image-tfidf-inference)\n - [Shopee PyTorch eca-nfnet-l0 Image [Training]](https://www.kaggle.com/parthdhameliya77/shopee-pytorch-eca-nfnet-l0-image-training)\n- [@vatsalmavani](https://www.kaggle.com/vatsalmavani)\n - [Shopee Training: Eff-B4](https://www.kaggle.com/vatsalmavani/shopee-training-eff-b4)\n- [@prashantkikani](https://www.kaggle.com/prashantkikani)\n - [Shopee-Ensemble of multiple models](https://www.kaggle.com/prashantkikani/shopee-ensemble-of-multiple-models)\n- [@ankursingh12](https://www.kaggle.com/ankursingh12)\n - [ðŸ‘œ Shopee - Ensemble (B3 + nfnet-l0) [inference]](https://www.kaggle.com/ankursingh12/shopee-ensemble-b3-nfnet-l0-inference)\n- [@tanulsingh077](https://www.kaggle.com/tanulsingh077)\n - [Reaching 0.612 with Text Only : Shopee](https://www.kaggle.com/tanulsingh077/reaching-0-612-with-text-only-shopee)\n- [@cdeotte](https://www.kaggle.com/cdeotte)\n - [[PART 2] - RAPIDS TfidfVectorizer - [CV 0.700]](https://www.kaggle.com/cdeotte/part-2-rapids-tfidfvectorizer-cv-0-700)\n - [RAPIDS cuML TfidfVectorizer and KNN](https://www.kaggle.com/cdeotte/rapids-cuml-tfidfvectorizer-and-knn)\n \nHere are some stunning post, definitely needed to be checked out in this competition.\n- [@kirderf](https://www.kaggle.com/kirderf)\n - [Features and post processing that might help](https://www.kaggle.com/c/shopee-product-matching/discussion/233626)\n- [@glushko](https://www.kaggle.com/glushko)\n - [Better way to frame the competition problem and understand how it's connected to prices](https://www.kaggle.com/c/shopee-product-matching/discussion/236496)\n\nAnd, again, to save time from generating img Embedding, use pre-saved embedding for faster work. You can use this dataset directly: [Shopee - Price Match Guarantee| Embeddings](https://www.kaggle.com/chienhsianghung/shopee-price-match-guarantee-embeddings).<br>\nHere are some results from my previous notebook: [Eff-B4 + TFIDF w/ CV for threshold_searching](https://www.kaggle.com/chienhsianghung/eff-b4-tfidf-w-cv-for-threshold-searching)\n\n|   | CV | LB |\n| --- | --- | --- |\n| 5.6 0.7 | 0.795 |   |\n| (fixing) 5.6 0.7 | 0.795 | 0.688 |\n| (fixing) BOOM | 0.774 | 0.728 |\n| (fixing) BOOM PHASH | 0.774 | 0.728 |\n| (fixing) BOOM 0.18 | 0.780 | 0.728 |\n| (fixing) 4.7 0.75 0.18 | 0.785 |   |\n| (fixing) 4.7 0.75 0.20 | 0.786 |   |\n| (fixing) BOOM IMG_COSINE 0.36 | 0.808 | 0.723 |\n| (fixing) BOOM IMG_COSINE 0.44 | 0.809 | 0.698 |\n| (fixing) THRES IMG_COSINE 0.4 | 0.803 | 0.683 |\n| (fixing) BOOM OPTIMIZED | 0.757 | 0.722 |\n| (fixing) 5.6 0.53 | 0.769 |   | \n| 5.6 0.53 PHASH | 0.767 |   | \n| (fixing) 5.6 0.53 OPTIMIZED | 0.782 | 0.675 |\n| BOOM | 0.774 | 0.728 |\n| BOOM OPTIMIZED | 0.757 |   |\n| BERT w/o TFIDF (fixing) BOOM PHASH | 0.975 | 0.716 |\n| BERT w/ TFIDF (fixing) BOOM PHASH | 0.958 | 0.719 |\n| clean the multiple models (fixing) |   | 0.732 |\n\nI've also done some experiments on KNN and Cosine Similarity in the previous notebook. Click here to see: [Shopee| text, img Embedding (Colab enabled)](https://www.kaggle.com/chienhsianghung/shopee-text-img-embedding-colab-enabled).","metadata":{}},{"cell_type":"markdown","source":"# Setting","metadata":{}},{"cell_type":"code","source":"import sys, os\nsys.path.append('../input/timm-pytorch-image-models/pytorch-image-models-master')\n\nimport numpy as np \nimport pandas as pd \n\nimport math, random\nimport cv2\nimport timm\nfrom tqdm import tqdm \n\nimport albumentations as A \nfrom albumentations.pytorch.transforms import ToTensorV2\n\nimport torch \nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\nfrom torch import nn\nimport torch.nn.functional as F \nimport torchvision.models as models\nfrom torch.nn import Parameter\n\nimport gc\nimport cudf, cuml, cupy\nfrom cuml.feature_extraction.text import TfidfVectorizer\nfrom cuml.neighbors import NearestNeighbors\n\nimport copy\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nimport transformers\nfrom transformers import (BertTokenizer, BertModel,\n                          DistilBertTokenizer, DistilBertModel)","metadata":{"papermill":{"duration":10.539319,"end_time":"2021-04-20T02:48:01.752436","exception":false,"start_time":"2021-04-20T02:47:51.213117","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"COMPUTE_CV = True\nSAVE_IMGEMBEDDING = False\nEFF_B5 = 'b5_ns' # 'b5', 'b5_ns'\nECA_NFNET_L0 = False\nMODEL_TESTING_NFNET = False\nMODEL_TESTING_EFF_B3 = False\nMODEL_TESTING_EFF_B5 = False\nSPLIT = False\nBERT = True\nDISTILBERT = False\nSAVE_DISTILBERT = False # You need to enable internet to download pretrained model\nEMBEDDING34_TH = 0.60\nEMBEDDING34_PRED_TH = 0.30\nBERT_PRED_TH = 0.95\n\ndf = pd.read_csv('../input/shopee-product-matching/test.csv')\nif len(df)>3: COMPUTE_CV = False\nif COMPUTE_CV: \n    print('this submission notebook will compute CV score but commit notebook will not')\nelse:\n    print('this submission notebook will only be used to submit result')","metadata":{"papermill":{"duration":0.02681,"end_time":"2021-04-20T02:48:01.794932","exception":false,"start_time":"2021-04-20T02:48:01.768122","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CFG:\n    \n    img_size = 512\n    fc_dim = 512\n    batch_size = 20\n    seed = 2020\n    \n    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n    classes = 11014\n    \n    model_name0 = 'tf_efficientnet_b5'\n    model_name = 'tf_efficientnet_b5_ns'\n    model_name2 = 'eca_nfnet_l0'\n    model_name3 = 'efficientnet_b3'\n    model_name4 = 'tf_efficientnet_b3'\n    \n    model_path0 = '../input/shopee-price-match-guarantee-embeddings/arcface_512x512_lrsq3_9.pt'\n    model_path = '../input/shopee-pytorch-models/arcface_512x512_eff_b5_.pt'\n                # ../input/shopee-pytorch-eca-nfnet-l0-image-training/arcface_512x512_nfnet_l0(mish)_b28l3_v4_17.pt\n    model_path2 = '../input/shopee-price-match-guarantee-embeddings/Shopee PyTorch eca-nfnet-l0 Image Training V3/arcface_512x512_nfnet_l0(mish)_b24l3_v2_6.pt'\n    model_path3 = '../input/shopee-pytorch-models/arcface_512x512_eff_b3.pt'\n    model_path4 = '../input/shopee-training-eff-b3-b5/arcface_512x512_b3_lrsq2_17.pt'\n    \n    image_embeddings0_path = '../input/shopee-price-match-guarantee-embeddings/tf_efficientnet_b5.csv'\n    image_embeddings1_path = '../input/shopee-price-match-guarantee-embeddings/tf_efficientnet_b5_ns.csv'\n    image_embeddings3_path = '../input/shopee-price-match-guarantee-embeddings/eca_nfnet_l0_b24l3_v2_6.csv'\n    image_embeddings4_path = '../input/shopee-price-match-guarantee-embeddings/efficientnet_b3.csv'\n    image_embeddings5_path = '../input/shopee-price-match-guarantee-embeddings/tf_efficientnet_b3_lrsq2_17.csv'\n    \n    text_model_path = '../input/best-multilingual-model/sentence_transfomer_xlm_best_loss_num_epochs_25_arcface.bin'\n    \n    scale = 30 \n    margin = 0.5","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Utility","metadata":{}},{"cell_type":"code","source":"def read_dataset(COMPUTE_CV):\n    \n    if COMPUTE_CV:\n        df = pd.read_csv('../input/shopee-product-matching/train.csv')\n        df_cu = cudf.DataFrame(df)\n        image_paths = '../input/shopee-product-matching/train_images/' + df['image']\n    \n    else:\n        df = pd.read_csv('../input/shopee-product-matching/test.csv')\n        df_cu = cudf.DataFrame(df)\n        image_paths = '../input/shopee-product-matching/test_images/' + df['image']\n\n    return df, df_cu, image_paths","metadata":{"papermill":{"duration":0.025495,"end_time":"2021-04-20T02:48:01.836457","exception":false,"start_time":"2021-04-20T02:48:01.810962","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def seed_torch(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    \nseed_torch(CFG.seed)","metadata":{"papermill":{"duration":0.03056,"end_time":"2021-04-20T02:48:01.88297","exception":false,"start_time":"2021-04-20T02:48:01.85241","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def combine_predictions(row):\n    x = np.concatenate([row['image_predictions'], row['text_predictions'], row['phash_predictions']])\n    return ' '.join( np.unique(x))\ndef combine_for_cv(row):\n    x = np.concatenate([row['image_predictions'], row['text_predictions'], row['phash_predictions']])\n    return np.unique(x)\ndef combine_predictions_BERT(row):\n    x = np.concatenate([row['image_predictions'], row['text_predictions'], row['phash_predictions'], row['text_predictions_BERT']])\n    return ' '.join( np.unique(x))\ndef combine_for_cv_BERT(row):\n    x = np.concatenate([row['image_predictions'], row['text_predictions'], row['phash_predictions'], row['text_predictions_BERT']])\n    return np.unique(x)\n","metadata":{"papermill":{"duration":0.026047,"end_time":"2021-04-20T02:48:01.925418","exception":false,"start_time":"2021-04-20T02:48:01.899371","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def getMetric(col):\n    def f1score(row):\n        n = len(np.intersect1d(row.target, row[col]))\n        return 2*n / (len(row.target) + len(row[col]))\n    return f1score","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_test_transforms():\n\n    return A.Compose(\n        [\n            A.Resize(CFG.img_size,CFG.img_size,always_apply=True),\n            A.Normalize(),\n        ToTensorV2(p=1.0)\n        ]\n    )","metadata":{"papermill":{"duration":0.027622,"end_time":"2021-04-20T02:48:02.023842","exception":false,"start_time":"2021-04-20T02:48:01.99622","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ShopeeDataset(Dataset):\n    def __init__(self, image_paths, transforms=None):\n\n        self.image_paths = image_paths\n        self.augmentations = transforms\n\n    def __len__(self):\n        return self.image_paths.shape[0]\n\n    def __getitem__(self, index):\n        image_path = self.image_paths[index]\n        \n        image = cv2.imread(image_path)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        \n        if self.augmentations:\n            augmented = self.augmentations(image=image)\n            image = augmented['image']       \n    \n        return image,torch.tensor(1)","metadata":{"papermill":{"duration":0.029186,"end_time":"2021-04-20T02:48:02.070133","exception":false,"start_time":"2021-04-20T02:48:02.040947","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Image","metadata":{}},{"cell_type":"markdown","source":"## Model","metadata":{}},{"cell_type":"code","source":"class ArcMarginProduct(nn.Module):\n    def __init__(self, in_features, out_features, scale=30.0, margin=0.50, easy_margin=False, ls_eps=0.0):\n        super(ArcMarginProduct, self).__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.scale = scale\n        self.margin = margin\n        self.ls_eps = ls_eps  # label smoothing\n        self.weight = Parameter(torch.FloatTensor(out_features, in_features))\n        nn.init.xavier_uniform_(self.weight)\n\n        self.easy_margin = easy_margin\n        self.cos_m = math.cos(margin)\n        self.sin_m = math.sin(margin)\n        self.th = math.cos(math.pi - margin)\n        self.mm = math.sin(math.pi - margin) * margin\n\n    def forward(self, input, label):\n        # --------------------------- cos(theta) & phi(theta) ---------------------------\n        cosine = F.linear(F.normalize(input), F.normalize(self.weight))\n        sine = torch.sqrt(1.0 - torch.pow(cosine, 2))\n        phi = cosine * self.cos_m - sine * self.sin_m\n        if self.easy_margin:\n            phi = torch.where(cosine > 0, phi, cosine)\n        else:\n            phi = torch.where(cosine > self.th, phi, cosine - self.mm)\n        # --------------------------- convert label to one-hot ---------------------------\n        # one_hot = torch.zeros(cosine.size(), requires_grad=True, device='cuda')\n        one_hot = torch.zeros(cosine.size(), device='cuda')\n        one_hot.scatter_(1, label.view(-1, 1).long(), 1)\n        if self.ls_eps > 0:\n            one_hot = (1 - self.ls_eps) * one_hot + self.ls_eps / self.out_features\n        # -------------torch.where(out_i = {x_i if condition_i else y_i) -------------\n        output = (one_hot * phi) + ((1.0 - one_hot) * cosine)\n        output *= self.scale\n\n        return output\n\n    \nclass ShopeeModel0(nn.Module):\n\n    def __init__(\n        self,\n        n_classes = CFG.classes,\n        model_name = CFG.model_name0,\n        fc_dim = CFG.fc_dim,\n        margin = CFG.margin,\n        scale = CFG.scale,\n        use_fc = True,\n        pretrained = False):\n\n        super(ShopeeModel0,self).__init__()\n        print('Building Model Backbone for {} model'.format(model_name))\n\n        self.backbone = timm.create_model(model_name, pretrained=pretrained)\n        in_features = self.backbone.classifier.in_features\n        self.backbone.classifier = nn.Identity()\n        self.backbone.global_pool = nn.Identity()\n        self.pooling =  nn.AdaptiveAvgPool2d(1)\n        self.use_fc = use_fc\n\n        if use_fc:\n            self.dropout = nn.Dropout(p=0.1)\n            self.classifier = nn.Linear(in_features, fc_dim)\n            self.bn = nn.BatchNorm1d(fc_dim)\n            self._init_params()\n            in_features = fc_dim\n\n        self.final = ArcMarginProduct(\n            in_features,\n            n_classes,\n            scale = scale,\n            margin = margin,\n            easy_margin = False,\n            ls_eps = 0.0\n        )\n\n    def _init_params(self):\n        nn.init.xavier_normal_(self.classifier.weight)\n        nn.init.constant_(self.classifier.bias, 0)\n        nn.init.constant_(self.bn.weight, 1)\n        nn.init.constant_(self.bn.bias, 0)\n\n    def forward(self, image, label):\n        features = self.extract_features(image)\n        if self.training:\n            logits = self.final(features, label)\n            return logits\n        else:\n            return features\n\n    def extract_features(self, x):\n        batch_size = x.shape[0]\n        x = self.backbone(x)\n        x = self.pooling(x).view(batch_size, -1)\n\n        if self.use_fc and self.training:\n            x = self.dropout(x)\n            x = self.classifier(x)\n            x = self.bn(x)\n        return x\n    \n\nclass ShopeeModel(nn.Module):\n\n    def __init__(\n        self,\n        n_classes = CFG.classes,\n        model_name = CFG.model_name,\n        fc_dim = CFG.fc_dim,\n        margin = CFG.margin,\n        scale = CFG.scale,\n        use_fc = False,\n        pretrained = False):\n\n\n        super(ShopeeModel,self).__init__()\n        print('Building Model Backbone for {} model'.format(model_name))\n\n        self.backbone = timm.create_model(model_name, pretrained=pretrained)\n\n        if model_name == 'resnext50_32x4d':\n            final_in_features = self.backbone.fc.in_features\n            self.backbone.fc = nn.Identity()\n            self.backbone.global_pool = nn.Identity()\n\n        elif model_name == 'efficientnet_b3':\n            final_in_features = self.backbone.classifier.in_features\n            self.backbone.classifier = nn.Identity()\n            self.backbone.global_pool = nn.Identity()\n\n        elif model_name == 'tf_efficientnet_b5_ns':\n            final_in_features = self.backbone.classifier.in_features\n            self.backbone.classifier = nn.Identity()\n            self.backbone.global_pool = nn.Identity()\n        \n        elif model_name == 'nfnet_f3':\n            final_in_features = self.backbone.head.fc.in_features\n            self.backbone.head.fc = nn.Identity()\n            self.backbone.head.global_pool = nn.Identity()\n\n        self.pooling =  nn.AdaptiveAvgPool2d(1)\n\n        self.use_fc = use_fc\n\n        self.dropout = nn.Dropout(p=0.0)\n        self.fc = nn.Linear(final_in_features, fc_dim)\n        self.bn = nn.BatchNorm1d(fc_dim)\n        self._init_params()\n        final_in_features = fc_dim\n\n        self.final = ArcMarginProduct(\n            final_in_features,\n            n_classes,\n            scale = scale,\n            margin = margin,\n            easy_margin = False,\n            ls_eps = 0.0\n        )\n\n    def _init_params(self):\n        nn.init.xavier_normal_(self.fc.weight)\n        nn.init.constant_(self.fc.bias, 0)\n        nn.init.constant_(self.bn.weight, 1)\n        nn.init.constant_(self.bn.bias, 0)\n\n    def forward(self, image, label):\n        feature = self.extract_feat(image)\n        #logits = self.final(feature,label)\n        return feature\n\n    def extract_feat(self, x):\n        batch_size = x.shape[0]\n        x = self.backbone(x)\n        x = self.pooling(x).view(batch_size, -1)\n\n        if self.use_fc:\n            x = self.dropout(x)\n            x = self.fc(x)\n            x = self.bn(x)\n        return x\n    \n    \nclass ShopeeModel2(nn.Module):\n\n    def __init__(\n        self,\n        n_classes = CFG.classes,\n        model_name = CFG.model_name2,\n        fc_dim = CFG.fc_dim,\n        margin = CFG.margin,\n        scale = CFG.scale,\n        use_fc = True,\n        pretrained = False):\n\n\n        super(ShopeeModel2,self).__init__()\n        print('Building Model Backbone for {} model'.format(model_name))\n\n        self.backbone = timm.create_model(model_name, pretrained=pretrained)\n\n        if model_name == 'resnext50_32x4d':\n            final_in_features = self.backbone.fc.in_features\n            self.backbone.fc = nn.Identity()\n            self.backbone.global_pool = nn.Identity()\n\n        elif model_name == 'efficientnet_b3':\n            final_in_features = self.backbone.classifier.in_features\n            self.backbone.classifier = nn.Identity()\n            self.backbone.global_pool = nn.Identity()\n\n        elif model_name == 'tf_efficientnet_b5_ns':\n            final_in_features = self.backbone.classifier.in_features\n            self.backbone.classifier = nn.Identity()\n            self.backbone.global_pool = nn.Identity()\n        \n        elif model_name == 'eca_nfnet_l0':\n            final_in_features = self.backbone.head.fc.in_features\n            self.backbone.head.fc = nn.Identity()\n            self.backbone.head.global_pool = nn.Identity()\n\n        self.pooling =  nn.AdaptiveAvgPool2d(1)\n\n        self.use_fc = use_fc\n\n        self.dropout = nn.Dropout(p=0.0)\n        self.fc = nn.Linear(final_in_features, fc_dim)\n        self.bn = nn.BatchNorm1d(fc_dim)\n        self._init_params()\n        final_in_features = fc_dim\n\n        self.final = ArcMarginProduct(\n            final_in_features,\n            n_classes,\n            scale = scale,\n            margin = margin,\n            easy_margin = False,\n            ls_eps = 0.0\n        )\n\n    def _init_params(self):\n        nn.init.xavier_normal_(self.fc.weight)\n        nn.init.constant_(self.fc.bias, 0)\n        nn.init.constant_(self.bn.weight, 1)\n        nn.init.constant_(self.bn.bias, 0)\n\n    def forward(self, image, label):\n        feature = self.extract_feat(image)\n        #logits = self.final(feature,label)\n        return feature\n\n    def extract_feat(self, x):\n        batch_size = x.shape[0]\n        x = self.backbone(x)\n        x = self.pooling(x).view(batch_size, -1)\n\n        if self.use_fc:\n            x = self.dropout(x)\n            x = self.fc(x)\n            x = self.bn(x)\n        return x\n    \n    \nclass ShopeeModel3(nn.Module):\n\n    def __init__(\n        self, \n        model_name = CFG.model_name3,\n        n_classes = CFG.classes,\n        fc_dim = CFG.fc_dim,\n        margin = CFG.margin,\n        scale = CFG.scale,\n        use_fc = True,\n        pretrained = False):\n\n\n        super(ShopeeModel3,self).__init__()\n        print('Building Model Backbone for {} model'.format(model_name))\n\n        self.backbone = timm.create_model(model_name, pretrained=pretrained)\n\n        if model_name == 'resnext50_32x4d':\n            final_in_features = self.backbone.fc.in_features\n            self.backbone.fc = nn.Identity()\n            self.backbone.global_pool = nn.Identity()\n\n        elif model_name == 'efficientnet_b3':\n            final_in_features = self.backbone.classifier.in_features\n            self.backbone.classifier = nn.Identity()\n            self.backbone.global_pool = nn.Identity()\n\n        elif model_name == 'tf_efficientnet_b5_ns':\n            final_in_features = self.backbone.classifier.in_features\n            self.backbone.classifier = nn.Identity()\n            self.backbone.global_pool = nn.Identity()\n        \n        elif model_name == 'eca_nfnet_l0':\n            final_in_features = self.backbone.head.fc.in_features\n            self.backbone.head.fc = nn.Identity()\n            self.backbone.head.global_pool = nn.Identity()\n\n        self.pooling =  nn.AdaptiveAvgPool2d(1)\n\n        self.use_fc = use_fc\n\n        self.dropout = nn.Dropout(p=0.0)\n        self.fc = nn.Linear(final_in_features, fc_dim)\n        self.bn = nn.BatchNorm1d(fc_dim)\n        self._init_params()\n        final_in_features = fc_dim\n\n        self.final = ArcMarginProduct(\n            final_in_features,\n            n_classes,\n            scale = scale,\n            margin = margin,\n            easy_margin = False,\n            ls_eps = 0.0\n        )\n\n    def _init_params(self):\n        nn.init.xavier_normal_(self.fc.weight)\n        nn.init.constant_(self.fc.bias, 0)\n        nn.init.constant_(self.bn.weight, 1)\n        nn.init.constant_(self.bn.bias, 0)\n\n    def forward(self, image, label):\n        feature = self.extract_feat(image)\n        #logits = self.final(feature,label)\n        return feature\n\n    def extract_feat(self, x):\n        batch_size = x.shape[0]\n        x = self.backbone(x)\n        x = self.pooling(x).view(batch_size, -1)\n\n        if self.use_fc:\n            x = self.dropout(x)\n            x = self.fc(x)\n            x = self.bn(x)\n        return x\n\n\nclass ShopeeModel03(nn.Module):\n\n    def __init__(\n        self,\n        n_classes = CFG.classes,\n        model_name = CFG.model_name4,\n        fc_dim = CFG.fc_dim,\n        margin = CFG.margin,\n        scale = CFG.scale,\n        use_fc = True,\n        pretrained = False):\n\n        super(ShopeeModel03,self).__init__()\n        print('Building Model Backbone for {} model'.format(model_name))\n\n        self.backbone = timm.create_model(model_name, pretrained=pretrained)\n        in_features = self.backbone.classifier.in_features\n        self.backbone.classifier = nn.Identity()\n        self.backbone.global_pool = nn.Identity()\n        self.pooling =  nn.AdaptiveAvgPool2d(1)\n        self.use_fc = use_fc\n\n        if use_fc:\n            self.dropout = nn.Dropout(p=0.1)\n            self.classifier = nn.Linear(in_features, fc_dim)\n            self.bn = nn.BatchNorm1d(fc_dim)\n            self._init_params()\n            in_features = fc_dim\n\n        self.final = ArcMarginProduct(\n            in_features,\n            n_classes,\n            scale = scale,\n            margin = margin,\n            easy_margin = False,\n            ls_eps = 0.0\n        )\n\n    def _init_params(self):\n        nn.init.xavier_normal_(self.classifier.weight)\n        nn.init.constant_(self.classifier.bias, 0)\n        nn.init.constant_(self.bn.weight, 1)\n        nn.init.constant_(self.bn.bias, 0)\n\n    def forward(self, image, label):\n        features = self.extract_features(image)\n        return features\n\n    def extract_features(self, x):\n        batch_size = x.shape[0]\n        x = self.backbone(x)\n        x = self.pooling(x).view(batch_size, -1)\n\n        if self.use_fc:\n            x = self.dropout(x)\n            x = self.classifier(x)\n            x = self.bn(x)\n        return x","metadata":{"papermill":{"duration":0.070425,"end_time":"2021-04-20T02:48:02.15717","exception":false,"start_time":"2021-04-20T02:48:02.086745","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# https://www.kaggle.com/parthdhameliya77/pytorch-eca-nfnet-l0-image-tfidf-inference\nclass Mish_func(torch.autograd.Function):\n    \n    \"\"\"from: https://github.com/tyunist/memory_efficient_mish_swish/blob/master/mish.py\"\"\"\n    \n    @staticmethod\n    def forward(ctx, i):\n        result = i * torch.tanh(F.softplus(i))\n        ctx.save_for_backward(i)\n        return result\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        i = ctx.saved_variables[0]\n  \n        v = 1. + i.exp()\n        h = v.log() \n        grad_gh = 1./h.cosh().pow_(2) \n\n        # Note that grad_hv * grad_vx = sigmoid(x)\n        #grad_hv = 1./v  \n        #grad_vx = i.exp()\n        \n        grad_hx = i.sigmoid()\n\n        grad_gx = grad_gh *  grad_hx #grad_hv * grad_vx \n        \n        grad_f =  torch.tanh(F.softplus(i)) + i * grad_gx \n        \n        return grad_output * grad_f \n\n\nclass Mish(nn.Module):\n    def __init__(self, **kwargs):\n        super().__init__()\n        pass\n    def forward(self, input_tensor):\n        return Mish_func.apply(input_tensor)\n\n\ndef replace_activations(model, existing_layer, new_layer):\n    \n    \"\"\"A function for replacing existing activation layers\"\"\"\n    \n    for name, module in reversed(model._modules.items()):\n        if len(list(module.children())) > 0:\n            model._modules[name] = replace_activations(module, existing_layer, new_layer)\n\n        if type(module) == existing_layer:\n            layer_old = module\n            layer_new = new_layer\n            model._modules[name] = layer_new\n    return model","metadata":{"papermill":{"duration":0.032528,"end_time":"2021-04-20T02:48:02.206824","exception":false,"start_time":"2021-04-20T02:48:02.174296","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Embedding","metadata":{}},{"cell_type":"code","source":"def get_image_embeddings(image_paths, model_name=CFG.model_name, EFF_B5=EFF_B5, \n                         nfnet_only=False, effb5_only=False, effb3_only=False):\n    \n    if EFF_B5 == 'b5_ns' and not nfnet_only and not effb3_only:\n        embeds = []\n\n        model = ShopeeModel(model_name=model_name)\n        model.eval()\n        model.load_state_dict(torch.load(CFG.model_path))\n        model = model.to(CFG.device)\n\n        image_dataset = ShopeeDataset(image_paths=image_paths,transforms=get_test_transforms())\n        image_loader = DataLoader(\n            image_dataset,\n            batch_size=CFG.batch_size,\n            pin_memory=True,\n            drop_last=False,\n            num_workers=4\n        )\n\n        with torch.no_grad():\n            for img,label in tqdm(image_loader): \n                img = img.cuda()\n                label = label.cuda()\n                feat = model(img,label)\n                image_embeddings = feat.detach().cpu().numpy()\n                embeds.append(image_embeddings)\n\n        del model, image_embeddings\n        image_embeddings1 = np.concatenate(embeds)\n        print(f'image embeddings1 shape is {image_embeddings1.shape}')\n        del embeds\n        gc.collect()\n    \n    elif EFF_B5 == 'b5' and not nfnet_only and not effb3_only:\n        embeds = []\n\n        model = ShopeeModel0()\n        model.eval()\n        model.load_state_dict(torch.load(CFG.model_path0))\n        model = model.to(CFG.device)\n\n        image_dataset = ShopeeDataset(image_paths=image_paths,transforms=get_test_transforms())\n        image_loader = DataLoader(\n            image_dataset,\n            batch_size=CFG.batch_size,\n            pin_memory=True,\n            drop_last=False,\n            num_workers=4\n        )\n\n        with torch.no_grad():\n            for img,label in tqdm(image_loader): \n                img = img.cuda()\n                label = label.cuda()\n                feat = model(img,label)\n                image_embeddings = feat.detach().cpu().numpy()\n                embeds.append(image_embeddings)\n\n        del model, image_embeddings\n        image_embeddings1 = np.concatenate(embeds)\n        print(f'image embeddings1 shape is {image_embeddings1.shape}')\n        del embeds\n        gc.collect()\n        \n    else: image_embeddings1 = None\n    \n    #---\n    if not effb5_only and not effb3_only:\n        model = ShopeeModel2()\n        model.eval()\n        model = replace_activations(model, torch.nn.SiLU, Mish())\n\n        model.load_state_dict(torch.load(CFG.model_path2))\n        model = model.to(CFG.device)\n\n        image_dataset = ShopeeDataset(image_paths=image_paths,transforms=get_test_transforms())\n        image_loader = DataLoader(\n            image_dataset,\n            batch_size=CFG.batch_size,\n            pin_memory=True,\n            drop_last=False,\n            num_workers=4\n        )\n\n        embeds2 = []\n        with torch.no_grad():\n            for img,label in tqdm(image_loader): \n                img = img.cuda()\n                label = label.cuda()\n                feat = model(img,label)\n                image_embeddings = feat.detach().cpu().numpy()\n                embeds2.append(image_embeddings)\n\n        del model\n        image_embeddings3 = np.concatenate(embeds2)\n        print(f'image embeddings3 shape is {image_embeddings3.shape}')\n        del embeds2\n        gc.collect()\n        \n    else: image_embeddings3 = None\n    \n    #---\n    if not nfnet_only and not effb5_only and not MODEL_TESTING_EFF_B3:\n        embeds = []\n\n        model = ShopeeModel3()\n        model.eval()\n        model.load_state_dict(torch.load(CFG.model_path3))\n        model = model.to(CFG.device)\n\n\n        image_dataset = ShopeeDataset(image_paths=image_paths,transforms=get_test_transforms())\n        image_loader = DataLoader(\n            image_dataset,\n            batch_size=CFG.batch_size,\n            pin_memory=True,\n            drop_last=False,\n            num_workers=4\n        )\n\n        with torch.no_grad():\n            for img,label in tqdm(image_loader): \n                img = img.cuda()\n                label = label.cuda()\n                feat = model(img,label)\n                image_embeddings = feat.detach().cpu().numpy()\n                embeds.append(image_embeddings)\n\n\n        del model\n        image_embeddings4 = np.concatenate(embeds)\n        print(f'Our image embeddings4 shape is {image_embeddings4.shape}')\n        del embeds\n        gc.collect()\n        \n    elif MODEL_TESTING_EFF_B3:\n        embeds = []\n\n        model = ShopeeModel03()\n        model.eval()\n        model.load_state_dict(torch.load(CFG.model_path4))\n        model = model.to(CFG.device)\n\n        image_dataset = ShopeeDataset(image_paths=image_paths,transforms=get_test_transforms())\n        image_loader = DataLoader(\n            image_dataset,\n            batch_size=CFG.batch_size,\n            pin_memory=True,\n            drop_last=False,\n            num_workers=4\n        )\n\n        with torch.no_grad():\n            for img,label in tqdm(image_loader): \n                img = img.cuda()\n                label = label.cuda()\n                feat = model(img,label)\n                image_embeddings = feat.detach().cpu().numpy()\n                embeds.append(image_embeddings)\n\n        del model, image_embeddings\n        image_embeddings4 = np.concatenate(embeds)\n        print(f'image embeddings4 shape is {image_embeddings4.shape}')\n        del embeds\n        gc.collect()\n\n    else: image_embeddings4 = None\n    \n    \n    return image_embeddings1, image_embeddings3, image_embeddings4","metadata":{"papermill":{"duration":0.039599,"end_time":"2021-04-20T02:48:02.263926","exception":false,"start_time":"2021-04-20T02:48:02.224327","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Prediction","metadata":{}},{"cell_type":"code","source":"# Threshold shifting depending on dataset length\n# https://www.kaggle.com/c/shopee-product-matching/discussion/234927\ndef dataset_th(known_th, known_dataset_len, new_dataset_len):\n    return (-2.051562606852219e-06 * (new_dataset_len-known_dataset_len)) + known_th\n\ndataset_th(1.7, 34250, 70000)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_image_predictions(df, embeddings1, embeddings3, embeddings34, \n                          ECA_NFNET_L0=ECA_NFNET_L0, predictions34_th=0.36):\n    \n    if len(df) > 3:\n        KNN = 50\n    else : \n        KNN = 3\n    \n    #--\n    model = NearestNeighbors(n_neighbors = KNN)\n    model.fit(embeddings1)\n    distances, indices = model.kneighbors(embeddings1)\n\n    threshold = 1.7 - 0.2978\n    predictions1 = []\n    for k in tqdm(range(embeddings1.shape[0])):\n        idx = np.where(distances[k,] < threshold)[0]\n        ids = indices[k,idx]\n        posting_ids = list(df['posting_id'].iloc[ids])\n        # for ii in np.arange(1.7-0.2978, (1.7-0.2978)*1.5, 0.04):\n            # if ii < (1.7-0.2978)*1.5 and len(posting_ids) <= 1:\n                # idx = np.where(distances[k,] < ii)[0]\n                # ids = indices[k,idx]\n                # posting_ids = list(df['posting_id'].iloc[ids].values)  \n        predictions1.append(posting_ids)\n\n    del model, distances, indices, embeddings1\n    gc.collect()\n\n    #--\n    if ECA_NFNET_L0:\n        model = NearestNeighbors(n_neighbors = KNN, metric = 'cosine')\n        model.fit(embeddings3)\n        distances, indices = model.kneighbors(embeddings3)\n\n        threshold=0.36\n        predictions3 = []\n        for k in tqdm(range(embeddings3.shape[0])):\n            idx = np.where(distances[k,] < threshold)[0]\n            ids = indices[k,idx]\n            posting_ids = list(df['posting_id'].iloc[ids])\n            predictions3.append(posting_ids)\n\n        del model, distances, indices, embeddings3\n        gc.collect()    \n    #--\n    \n    model = NearestNeighbors(n_neighbors = KNN, metric = 'cosine')\n    model.fit(embeddings34)\n    distances, indices = model.kneighbors(embeddings34)\n    \n    predictions34 = []\n    for k in tqdm(range(embeddings34.shape[0])):\n        idx = np.where(distances[k,] < predictions34_th)[0]\n        ids = indices[k,idx]\n        posting_ids = list(df['posting_id'].iloc[ids].values)\n        for ii in np.arange(predictions34_th, 0.54, 0.02):\n            if ii < 0.5 and len(posting_ids) <= 1:\n                idx = np.where(distances[k,] < ii)[0]\n                ids = indices[k,idx]\n                posting_ids = list(df['posting_id'].iloc[ids].values)    \n        predictions34.append(posting_ids)\n        \n    del model, distances, indices\n    gc.collect()\n\n    #--\n    \n    if SPLIT:\n        model = NearestNeighbors(n_neighbors = KNN, metric = 'cosine')\n        model.fit(embeddings3)\n        distances, indices = model.kneighbors(embeddings34)\n\n        predictions3 = []\n        for k in tqdm(range(embeddings3.shape[0])):\n            idx = np.where(distances[k,] < predictions34_th)[0]\n            ids = indices[k,idx]\n            posting_ids = list(df['posting_id'].iloc[ids].values)\n            for ii in np.arange(predictions34_th, 0.54, 0.02):\n                if ii < 0.5 and len(posting_ids) <= 1:\n                    idx = np.where(distances[k,] < ii)[0]\n                    ids = indices[k,idx]\n                    posting_ids = list(df['posting_id'].iloc[ids].values)    \n            predictions3.append(posting_ids)\n\n        del model, distances, indices\n        gc.collect()\n    \n    #--\n    \n    if ECA_NFNET_L0 or SPLIT:\n        predictions = [list(set(a + c + d)) for a, c, d in zip(predictions1, predictions3, predictions34)]\n    else:\n        predictions = [list(set(a + d)) for a, d in zip(predictions1, predictions34)]\n    \n    return predictions","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Text","metadata":{}},{"cell_type":"markdown","source":"## Embedding and Prediction","metadata":{}},{"cell_type":"code","source":"#https://www.kaggle.com/cdeotte/part-2-rapids-tfidfvectorizer-cv-0-700#Use-Text-Embeddings\ndef get_text_predictions(df, df_cu, max_features=25_000):\n    model = TfidfVectorizer(stop_words=None, binary=True, max_features=max_features)\n    text_embeddings = model.fit_transform(df_cu['title']).toarray()\n    preds = []\n    CHUNK = 1024*4\n\n    print('Finding similar titles...')\n    CTS = len(df)//CHUNK\n    if len(df)%CHUNK!=0: CTS += 1\n    for j in range( CTS ):\n\n        a = j*CHUNK\n        b = (j+1)*CHUNK\n        b = min(b,len(df))\n        print('chunk',a,'to',b)\n\n        # COSINE SIMILARITY DISTANCE\n        cts = cupy.matmul(text_embeddings, text_embeddings[a:b].T).T\n\n        for k in range(b-a):\n            IDX = cupy.where(cts[k,]>0.80)[0]\n            o = df.iloc[cupy.asnumpy(IDX)].posting_id.values\n            \n            for ii in np.arange(0.80, 0.50, -0.02):\n                if ii > 0.5 and o.shape[0] <= 1:\n                    IDX = cupy.where(cts[k,] > ii)[0]\n                    o = df.iloc[cupy.asnumpy(IDX)].posting_id.values\n            preds.append(o)\n    \n    del model,text_embeddings\n    gc.collect()\n    return preds","metadata":{"papermill":{"duration":0.031277,"end_time":"2021-04-20T02:48:02.312355","exception":false,"start_time":"2021-04-20T02:48:02.281078","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Preparation","metadata":{}},{"cell_type":"markdown","source":"## Loading","metadata":{}},{"cell_type":"code","source":"df, df_cu, image_paths = read_dataset(COMPUTE_CV)\ndf.head()","metadata":{"papermill":{"duration":9.345685,"end_time":"2021-04-20T02:48:11.675542","exception":false,"start_time":"2021-04-20T02:48:02.329857","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Embedding","metadata":{}},{"cell_type":"code","source":"if not COMPUTE_CV or SAVE_IMGEMBEDDING:\n    image_embeddings1, image_embeddings3, image_embeddings4 = get_image_embeddings(image_paths.values, \n                                                                                   nfnet_only=MODEL_TESTING_NFNET if COMPUTE_CV else COMPUTE_CV,\n                                                                                   effb5_only=MODEL_TESTING_EFF_B5 if COMPUTE_CV else COMPUTE_CV,\n                                                                                  effb3_only=MODEL_TESTING_EFF_B3 if COMPUTE_CV else COMPUTE_CV)\n    if SAVE_IMGEMBEDDING and not MODEL_TESTING_NFNET and not MODEL_TESTING_EFF_B5 and not MODEL_TESTING_EFF_B3: \n        np.savetxt('tf_efficientnet_b5_ns.csv', image_embeddings1, delimiter=',')\n        np.savetxt('eca_nfnet_l0.csv', image_embeddings3, delimiter=',')\n        np.savetxt('efficientnet_b3.csv', image_embeddings4, delimiter=',')\n    elif COMPUTE_CV and MODEL_TESTING_NFNET:\n        image_embeddings1 = np.loadtxt(CFG.image_embeddings1_path, delimiter=',')\n        np.savetxt('eca_nfnet_l0_(mish)_b28l3_v4_17.csv', image_embeddings3, delimiter=',')\n        image_embeddings4 = np.loadtxt(CFG.image_embeddings4_path, delimiter=',')\n    elif COMPUTE_CV and MODEL_TESTING_EFF_B5:\n        np.savetxt('tf_efficientnet_b5.csv', image_embeddings1, delimiter=',')\n        image_embeddings3 = np.loadtxt(CFG.image_embeddings3_path, delimiter=',')\n        image_embeddings4 = np.loadtxt(CFG.image_embeddings4_path, delimiter=',')\n    elif COMPUTE_CV and MODEL_TESTING_EFF_B3:\n        image_embeddings1 = np.loadtxt(CFG.image_embeddings1_path, delimiter=',')\n        image_embeddings3 = np.loadtxt(CFG.image_embeddings3_path, delimiter=',')\n        np.savetxt('tf_efficientnet_b3_lrsq2_17.csv', image_embeddings4, delimiter=',')\n        \nelse:\n    if EFF_B5 == 'b5':\n        image_embeddings1 = np.loadtxt(CFG.image_embeddings0_path, delimiter=',')\n    elif EFF_B5 == 'b5_ns':\n        image_embeddings1 = np.loadtxt(CFG.image_embeddings1_path, delimiter=',')\n        \n    image_embeddings3 = np.loadtxt(CFG.image_embeddings3_path, delimiter=',')\n    \n    if MODEL_TESTING_EFF_B3:\n        image_embeddings4 = np.loadtxt(CFG.image_embeddings5_path, delimiter=',')\n    else:\n        image_embeddings4 = np.loadtxt(CFG.image_embeddings4_path, delimiter=',')\n    ","metadata":{"papermill":{"duration":10.485153,"end_time":"2021-04-20T02:48:22.179818","exception":false,"start_time":"2021-04-20T02:48:11.694665","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Blending","metadata":{}},{"cell_type":"code","source":"if not SPLIT:\n    image_embeddings34 = (image_embeddings3 * EMBEDDING34_TH) + (image_embeddings4 * ( 1 - EMBEDDING34_TH ))\n    del image_embeddings4\nelse:\n    image_embeddings34 = image_embeddings4","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Prediction","metadata":{}},{"cell_type":"code","source":"image_predictions = get_image_predictions(df, image_embeddings1, image_embeddings3, image_embeddings34, \n                                          predictions34_th=EMBEDDING34_PRED_TH)\ntext_predictions = get_text_predictions(df, df_cu)","metadata":{"papermill":{"duration":20.315209,"end_time":"2021-04-20T02:48:42.519919","exception":false,"start_time":"2021-04-20T02:48:22.20471","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# DistilBERT\n\n- [LB with only TEXT data](https://www.kaggle.com/c/shopee-product-matching/discussion/230620)\n- [Does using BERT or any transformer Model even makes sense?](https://www.kaggle.com/c/shopee-product-matching/discussion/228741)\n- [Indonesian DistilBERT finetuning with ArcMargin](https://www.kaggle.com/moeinshariatnia/indonesian-distilbert-finetuning-with-arcmargin)","metadata":{}},{"cell_type":"code","source":"if DISTILBERT:\n    class CFG:\n        DistilBERT = True # if set to False, BERT model will be used\n        bert_hidden_size = 768\n\n        batch_size = 64\n        epochs = 30\n        num_workers = 4\n        learning_rate = 1e-5 #3e-5\n        scheduler = \"ReduceLROnPlateau\"\n        step = 'epoch'\n        patience = 2\n        factor = 0.8\n        dropout = 0.5\n        model_path = \"/kaggle/working\"\n        max_length = 30\n        model_save_name = \"model.pt\"\n        device = torch.device(\"cuda\" if torch.cuda.is_available() else 'cpu')\n\n        \n    if SAVE_DISTILBERT:\n        if CFG.DistilBERT:\n            model_name='cahya/distilbert-base-indonesian'\n            tokenizer = DistilBertTokenizer.from_pretrained(model_name)\n            bert_model = DistilBertModel.from_pretrained(model_name)\n        else:\n            model_name='cahya/bert-base-indonesian-522M'\n            tokenizer = BertTokenizer.from_pretrained(model_name)\n            bert_model = BertModel.from_pretrained(model_name)\n        tokenizer.save_pretrained(\"./models/tokenizer/\")\n        bert_model.save_pretrained(\"./models/bert_model/\")\n    else:\n        tokenizer = DistilBertTokenizer.from_pretrained(\"../input/shopee-price-match-guarantee-embeddings/DistilBERT_pretrained/models/tokenizer\")\n        bert_model = DistilBertModel.from_pretrained(\"../input/shopee-price-match-guarantee-embeddings/DistilBERT_pretrained/models/bert_model\")\n\n\n    lbl_encoder = LabelEncoder()\n    df['label_code'] = lbl_encoder.fit_transform(df['label_group'])\n    NUM_CLASSES = df['label_code'].nunique()\n\n\n    class TextDataset(torch.utils.data.Dataset):\n        def __init__(self, dataframe, tokenizer, mode=\"train\", max_length=None):\n            self.dataframe = dataframe\n            if mode != \"test\":\n                self.targets = dataframe['label_code'].values\n            texts = list(dataframe['title'].apply(lambda o: str(o)).values)\n            self.encodings = tokenizer(texts, \n                                       padding=True, \n                                       truncation=True, \n                                       max_length=max_length)\n            self.mode = mode\n\n        def __getitem__(self, idx):\n            # putting each tensor in front of the corresponding key from the tokenizer\n            # HuggingFace tokenizers give you whatever you need to feed to the corresponding model\n            item = {key: torch.tensor(values[idx]) for key, values in self.encodings.items()}\n            # when testing, there are no targets so we won't do the following\n            if self.mode != \"test\":\n                item['labels'] = torch.tensor(self.targets[idx]).long()\n            return item\n\n        def __len__(self):\n            return len(self.dataframe)\n\n\n    dataset = TextDataset(df, tokenizer, mode='test', max_length=CFG.max_length)\n    dataloader = DataLoader(dataset, \n                             batch_size=CFG.batch_size, \n                             num_workers=CFG.num_workers, \n                             shuffle=True)\n    batch = next(iter(dataloader))\n    \n    \n    # code from https://github.com/ronghuaiyang/arcface-pytorch/blob/47ace80b128042cd8d2efd408f55c5a3e156b032/models/metrics.py#L10\n    class ArcMarginProduct(nn.Module):\n        r\"\"\"Implement of large margin arc distance: :\n            Args:\n                in_features: size of each input sample\n                out_features: size of each output sample\n                s: norm of input feature\n                m: margin\n                cos(theta + m)\n            \"\"\"\n        def __init__(self, in_features, out_features, s=30.0, m=0.50, easy_margin=False):\n            super(ArcMarginProduct, self).__init__()\n            self.in_features = in_features\n            self.out_features = out_features\n            self.s = s\n            self.m = m\n            self.weight = Parameter(torch.FloatTensor(out_features, in_features))\n            nn.init.xavier_uniform_(self.weight)\n\n            self.easy_margin = easy_margin\n            self.cos_m = math.cos(m)\n            self.sin_m = math.sin(m)\n            self.th = math.cos(math.pi - m)\n            self.mm = math.sin(math.pi - m) * m\n\n        def forward(self, input, label):\n            # --------------------------- cos(theta) & phi(theta) ---------------------------\n            cosine = F.linear(F.normalize(input), F.normalize(self.weight))\n            sine = torch.sqrt((1.0 - torch.pow(cosine, 2)).clamp(0, 1))\n            phi = cosine * self.cos_m - sine * self.sin_m\n            if self.easy_margin:\n                phi = torch.where(cosine > 0, phi, cosine)\n            else:\n                phi = torch.where(cosine > self.th, phi, cosine - self.mm)\n            # --------------------------- convert label to one-hot ---------------------------\n            # one_hot = torch.zeros(cosine.size(), requires_grad=True, device='cuda')\n            one_hot = torch.zeros(cosine.size(), device=CFG.device)\n            one_hot.scatter_(1, label.view(-1, 1).long(), 1)\n            # -------------torch.where(out_i = {x_i if condition_i else y_i) -------------\n            output = (one_hot * phi) + ((1.0 - one_hot) * cosine)  # you can use torch.where if your torch.__version__ is 0.4\n            output *= self.s\n            # print(output)\n\n            return output\n\n\n    class Model(nn.Module):\n        def __init__(self, \n                     bert_model, \n                     num_classes=NUM_CLASSES, \n                     last_hidden_size=CFG.bert_hidden_size):\n\n            super().__init__()\n            self.bert_model = bert_model\n            self.arc_margin = ArcMarginProduct(last_hidden_size, \n                                               num_classes, \n                                               s=30.0, \n                                               m=0.50, \n                                               easy_margin=False)\n\n        def get_bert_features(self, batch):\n            output = self.bert_model(input_ids=batch['input_ids'], attention_mask=batch['attention_mask'])\n            last_hidden_state = output.last_hidden_state # shape: (batch_size, seq_length, bert_hidden_dim)\n            CLS_token_state = last_hidden_state[:, 0, :] # obtaining CLS token state which is the first token.\n            return CLS_token_state\n\n        def forward(self, batch):\n            CLS_hidden_state = self.get_bert_features(batch)\n            output = self.arc_margin(CLS_hidden_state, batch['labels'])\n            return output\n\n        \n    def get_text_embeddings():\n        model = Model(bert_model) #the class of the model u created\n        model.eval()\n        WEIGHTS_FILE = \"../input/shopee-price-match-guarantee-embeddings/DistilBERT/model.pt\"\n        model.load_state_dict(torch.load(WEIGHTS_FILE))\n        model = model.to(CFG.device)\n                \n        embeds = []\n        with torch.no_grad():\n            for txt, label in tqdm(dataloader):\n                txt = txt.cuda()\n                label = label.cuda()\n                feat = model(txt, label)\n                text_embeddings = feat.detach().cpu().numpy()\n                embeds.append(text_embeddings)\n\n        del model\n        text_embeddings = np.concatenate(embeds)\n        print(f'text_embeddings shape is {text_embeddings.shape}')\n        del embeds\n        gc.collect()\n\n        \n    get_text_embeddings()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# BERT\n\n- [Metric Learning Pipeline : Only Text Sbert](https://www.kaggle.com/tanulsingh077/metric-learning-pipeline-only-text-sbert)","metadata":{}},{"cell_type":"code","source":"if BERT:\n    NUM_WORKERS = 4\n    BATCH_SIZE = 16\n\n    device = torch.device('cuda')\n\n    ################################################# MODEL ####################################################################\n\n    transformer_model = '../input/sentence-transformer-models/paraphrase-xlm-r-multilingual-v1/0_Transformer'\n    TOKENIZER = transformers.AutoTokenizer.from_pretrained(transformer_model)\n\n    model_params = {\n        'n_classes':11014,\n        'model_name':transformer_model,\n        'use_fc':False,\n        'fc_dim':512,\n        'dropout':0.3,\n    }\n\n    class ShopeeDataset(Dataset):\n        def __init__(self, csv):\n            self.csv = csv.reset_index()\n\n        def __len__(self):\n            return self.csv.shape[0]\n\n        def __getitem__(self, index):\n            row = self.csv.iloc[index]\n\n            text = row.title\n\n            text = TOKENIZER(text, padding='max_length', truncation=True, max_length=128, return_tensors=\"pt\")\n            input_ids = text['input_ids'][0]\n            attention_mask = text['attention_mask'][0]  \n\n            return input_ids, attention_mask\n\n\n    class ShopeeNet(nn.Module):\n\n        def __init__(self,\n                     n_classes,\n                     model_name='bert-base-uncased',\n                     use_fc=False,\n                     fc_dim=512,\n                     dropout=0.0):\n            \"\"\"\n            :param n_classes:\n            :param model_name: name of model from pretrainedmodels\n                e.g. resnet50, resnext101_32x4d, pnasnet5large\n            :param pooling: One of ('SPoC', 'MAC', 'RMAC', 'GeM', 'Rpool', 'Flatten', 'CompactBilinearPooling')\n            :param loss_module: One of ('arcface', 'cosface', 'softmax')\n            \"\"\"\n            super(ShopeeNet, self).__init__()\n\n            self.transformer = transformers.AutoModel.from_pretrained(model_name)\n            final_in_features = self.transformer.config.hidden_size\n\n            self.use_fc = use_fc\n\n            if use_fc:\n                self.dropout = nn.Dropout(p=dropout)\n                self.fc = nn.Linear(final_in_features, fc_dim)\n                self.bn = nn.BatchNorm1d(fc_dim)\n                self._init_params()\n                final_in_features = fc_dim\n\n        def _init_params(self):\n            nn.init.xavier_normal_(self.fc.weight)\n            nn.init.constant_(self.fc.bias, 0)\n            nn.init.constant_(self.bn.weight, 1)\n            nn.init.constant_(self.bn.bias, 0)\n\n        def forward(self, input_ids,attention_mask):\n            feature = self.extract_feat(input_ids,attention_mask)\n            return F.normalize(feature)\n\n        def extract_feat(self, input_ids,attention_mask):\n            x = self.transformer(input_ids=input_ids,attention_mask=attention_mask)\n\n            features = x[0]\n            features = features[:,0,:]\n\n            if self.use_fc:\n                features = self.dropout(features)\n                features = self.fc(features)\n                features = self.bn(features)\n\n            return features\n\n\n    def get_BERT_embeddings(df):\n        embeds = []\n\n        model = ShopeeNet(**model_params)\n        model.eval()\n\n        model.load_state_dict(dict(list(torch.load(CFG.text_model_path).items())[:-1]))\n        model = model.to(device)\n\n        text_dataset = ShopeeDataset(df)\n        text_loader = torch.utils.data.DataLoader(\n            text_dataset,\n            batch_size=BATCH_SIZE,\n            pin_memory=True,\n            drop_last=False,\n            num_workers=NUM_WORKERS\n        )\n\n\n        with torch.no_grad():\n            for input_ids, attention_mask in tqdm(text_loader): \n                input_ids = input_ids.cuda()\n                attention_mask = attention_mask.cuda()\n                feat = model(input_ids, attention_mask)\n                text_embeddings = feat.detach().cpu().numpy()\n                embeds.append(text_embeddings)\n\n\n        del model\n        text_embeddings = np.concatenate(embeds)\n        print(f'Our text embeddings shape is {text_embeddings.shape}')\n        del embeds\n        gc.collect()\n        return text_embeddings\n\n\n    def get_neighbours_cos_sim(df, embeddings):\n        '''\n        When using cos_sim use normalized features else use normal features\n        '''\n        embeddings = cupy.array(embeddings)\n\n        if False:\n            thresholds = list(np.arange(0.5,0.7,0.05))\n\n            scores = []\n            for threshold in thresholds:\n\n    ################################################# Code for Getting Preds #########################################\n                preds = []\n                CHUNK = 1024*4\n\n                print('Finding similar titles...for threshold :',threshold)\n                CTS = len(embeddings)//CHUNK\n                if len(embeddings)%CHUNK!=0: CTS += 1\n\n                for j in range( CTS ):\n                    a = j*CHUNK\n                    b = (j+1)*CHUNK\n                    b = min(b,len(embeddings))\n\n                    cts = cupy.matmul(embeddings,embeddings[a:b].T).T\n\n                    for k in range(b-a):\n                        IDX = cupy.where(cts[k,]>threshold)[0]\n                        o = df.iloc[cupy.asnumpy(IDX)].posting_id.values\n                        o = ' '.join(o)\n                        preds.append(o)\n    ######################################################################################################################\n                df['pred_matches'] = preds\n                df['f1'] = f1_score(df['matches'], df['pred_matches'])\n                score = df['f1'].mean()\n                print(f'Our f1 score for threshold {threshold} is {score}')\n                scores.append(score)\n\n            thresholds_scores = pd.DataFrame({'thresholds': thresholds, 'scores': scores})\n            max_score = thresholds_scores[thresholds_scores['scores'] == thresholds_scores['scores'].max()]\n            best_threshold = max_score['thresholds'].values[0]\n            best_score = max_score['scores'].values[0]\n            print(f'Our best score is {best_score} and has a threshold {best_threshold}')\n\n        else:\n            preds = []\n            CHUNK = 1024*4\n            threshold = BERT_PRED_TH\n\n            print('Finding similar texts...for threshold :',threshold)\n            CTS = len(embeddings)//CHUNK\n            if len(embeddings)%CHUNK!=0: CTS += 1\n\n            for j in range( CTS ):\n                a = j*CHUNK\n                b = (j+1)*CHUNK\n                b = min(b,len(embeddings))\n                print('chunk',a,'to',b)\n\n                cts = cupy.matmul(embeddings,embeddings[a:b].T).T\n\n                for k in range(b-a):\n                    IDX = cupy.where(cts[k,]>threshold)[0]\n                    o = df.iloc[cupy.asnumpy(IDX)].posting_id.values\n                    preds.append(o)\n\n        return df, preds\n    \n    \n    text_embeddings_BERT = get_BERT_embeddings(df)\n    _, text_predictions_BERT = get_neighbours_cos_sim(df, text_embeddings_BERT)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Result","metadata":{}},{"cell_type":"code","source":"df['image_predictions'] = image_predictions\ndf['text_predictions'] = text_predictions\ntmp = df.groupby('image_phash').posting_id.agg('unique').to_dict()\ndf['phash_predictions'] = df.image_phash.map(tmp)\nif BERT:\n    df['text_predictions_BERT'] = text_predictions_BERT\n    df['matches'] = df.apply(combine_predictions_BERT, axis = 1)\nelse: \n    df['matches'] = df.apply(combine_predictions, axis = 1)\ndf[['posting_id', 'matches']].to_csv('submission.csv', index = False)\ndf[['posting_id', 'matches']].head()","metadata":{"papermill":{"duration":0.228235,"end_time":"2021-04-20T02:48:42.828189","exception":false,"start_time":"2021-04-20T02:48:42.599954","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if COMPUTE_CV:\n    tmp = df.groupby('label_group').posting_id.agg('unique').to_dict()\n    df['target'] = df.label_group.map(tmp) \n    if BERT:\n        df['matches_CV'] = df.apply(combine_for_cv_BERT, axis = 1)\n    else:\n        df['matches_CV'] = df.apply(combine_for_cv, axis = 1)\n    MyCVScore = df.apply(getMetric('matches_CV'), axis=1)\n    print('CV score =', MyCVScore.mean())\n    \n    \nprint(f'COMPUTE_CV = {COMPUTE_CV}')\nprint(f'SAVE_IMGEMBEDDING = {SAVE_IMGEMBEDDING}')\nprint(f'EFF_B5 = {EFF_B5}')\nprint(f'ECA_NFNET_L0 = {ECA_NFNET_L0}')\nprint(f'MODEL_TESTING_NFNET = {MODEL_TESTING_NFNET}')\nprint(f'MODEL_TESTING_EFF_B5 = {MODEL_TESTING_EFF_B5}')\nprint(f'MODEL_TESTING_EFF_B3 = {MODEL_TESTING_EFF_B3}')\nprint(f'SPLIT = {SPLIT}')\nprint(f'BERT = {BERT}')\nprint(f'DISTILBERT = {DISTILBERT}')\nprint(f'SAVE_DISTILBERT = {SAVE_DISTILBERT}')\nprint(f'EMBEDDING34_TH = {EMBEDDING34_TH}')\nprint(f'EMBEDDING34_PRED_TH = {EMBEDDING34_PRED_TH}')\nprint(f'BERT_PRED_TH = {BERT_PRED_TH}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"|   | CV | LB |\n| --- | --- | --- |\n| eff_b5 + eca_nfnet_l0 + eff_b3 | 0.869 | 0.731 |\n| (fixing) eff_b5 + eca_nfnet_l0 + eff_b3 Phash | 0.869 | 0.731 |\n| (fixing) eff_b5 TO + eca_nfnet_l0 + eff_b3 Phash | 0.869 | 0.732 |\n| eff_b5 TO + eca_nfnet_l0 + eff_b3 (5) | 0.869 |   |\n| (fixing) eff_b5 TO + (eca_nfnet_l0 + eff_b3) Phash | 0.856 | 0.733 |\n| (fixing) (eca_nfnet_l0 + eff_b3)64 Phash | 0.856 | 0.733 |\n| (fixing) eff_b5 TO + (eca_nfnet_l0 + eff_b3)6535 Phash PP | 0.858 | 0.735 |\n| (fixing) eff_b5 TO + (eca_nfnet_l0 + eff_b3)6535 Phash PP2 | 0.843 | 0.715 |\n| (fixing) eff_b5 TO + (eca_nfnet_l0 + eff_b3)6535 Phash PPit50 | 0.873 | 0.735 |\n| (fixing) eff_b5 TO + (eca_nfnet_l0 + eff_b3)6535 Phash PPit54 | 0.873 | 0.735 |\n| (fixing) eff_b5 TO + (eca_nfnet_l0 + eff_b3) Phash PPit54 | 0.871 | 0.736 |\n| (fixing) eff_b5 TO + (eca_nfnet_l0 + eff_b3) Phash PPit50 | 0.871 | 0.736 |\n| (fixing) eff_b5 TO + (eca_nfnet_l0 + eff_b3)73 Phash PPit50 | 0.876 | 0.735 |\n| (fixing) eff_b5 TO + (eca_nfnet_l0 + eff_b3) Phash PPiit54 | 0.872 | 0.734 |\n| (fixing) eff_b5 TO + (eca_nfnet_l0 + eff_b3) Phash PPit54 + 2class |   | timed |\n| (fixing) eff_b5 TO + (eca_nfnet_l0 + eff_b3).35 Phash PPit54 | 0.870 | 0.736 |\n| (fixing) eff_b5 TO + (eca_nfnet_l0 + eff_b3).33 Phash PPit54 | 0.866 | 0.737 |\n| (fixing) eff_b5 TO + (eca_nfnet_l0 + eff_b3).33 t775 Phash PPit54 | 0.868 | 0.739 |\n| (fixing) eff_b5 TO + (eca_nfnet_l0 + eff_b3).33 t80 Phash PPit54 | 0.869 | 0.739 |\n| (fixing) eff_b5 TO + (eca_nfnet_l0 + eff_b3).30 t80 Phash PPit54 | 0.862 | 0.740 |\n| (fixing) eff_b5 TO + (eca_nfnet_l0 + eff_b3) t825 Phash PPit54 | 0.873 | 0.737 |\n| (fixing) eff_b5 TO + (eca_nfnet_l0 + eff_b3) t85 Phash PPit54 | 0.874 | 0.736|\n| (fixing) eff_b5 TO + (eca_nfnet_l0 + eff_b3) t875 Phash PPit54 | 0.875 | 0.736 |\n| (fixing) eff_b5 TO + (eca_nfnet_l0 + eff_b3) t90 Phash PPit54 | 0.875 | 0.736 |\n| (fixing) eff_b5 TO + (eca_nfnet_l0 + eff_b3) t80 Phash PPit54 | 0.874 | 0.737 |\n| (fixing) eff_b5 TO + (eca_nfnet_l0 + eff_b3) t80 Phash PPit54 | 0.874 | 0.737 |\n| (fixing) eff_b5 TO + (eca_nfnet_l0 + eff_b3) t80 Phash PPit54 BERT.8 | 0.937 | 0.737 |\n| (fixing) eff_b5 TO + (eca_nfnet_l0_b24_15 + eff_b3) t80 Phash PPit54 | 0.880 | 0.738 |\n| (fixing) eff_b5 TO + (eca_nfnet_l0 + eff_b3) t80 Phsh PPit54 BERT.9 | 0.899 | 0.739 |\n| eff_b5 TO + (eca_nfnet_l0_b24_15 + eff_b3) t80 PPit54 BERT.9 | 0.903 | 0.739 |\n| eff_b5 TO + (eca_nfnet_l0_b24_15 + eff_b3).30 t80 PPit54 BERT.9 | 0.898 | 0.742 |\n| eff_b5 TO + (eca_nfnet_l0_b24_15 + eff_b3) t80 PPit54 BERT(2).9 | 0.537 |   |\n| **eff_b5 TO + (eca_nfnet_l0_b24l3_v2_6 + eff_b3) t80 PPit54 BERT.9** | 0.922 | 0.739 |\n| **eff_b5 TO + (eca_nfnet_l0_b24l3_v2_6 + eff_b3).30 t80 PPit54 BERT.9** | 0.915 | 0.742 |\n| eff_b5 TO + (eca_nfnet_l0_b24l3_v2_6 625 + eff_b3) t80 PPit54 BERT.9 | 0.925 | 0.738 |\n| eff_b5 TO + (eca_nfnet_l0_b24l3_v2_6 575 + eff_b3) t80 PPit54 BERT.9 | 0.920 | 0.739 |\n| eff_b5 TO + (eca_nfnet_l0_b28l3_v4_17 + eff_b3) t80 PPit54 BERT.9 | 0.906 | 0.738 |\n| eff_b5_lrsq3_9 TO + (eca_nfnet_l0_b24l3_v2_6 + eff_b3) t80 PPit54 BERT.9 | 0.923 | 0.739 |\n| eff_b5_lrsq3_9 TO + eca_nfnet_l0_b24l3_v2_6 + eff_b3_lrsq2_17 t80 PPit54 BERT.9 | 0.897 | 0.731 |\n| eff_b5_lrsq3_9 TO + (eca_nfnet_l0_b24l3_v2_6 + eff_b3_lrsq2_17) t80 PPit54 BERT.9 | 0.928 | 0.738 |\n| eff_b5 TO + (eca_nfnet_l0_b24l3_v2_6 + eff_b3_lrsq2_17) t80 PPit54 BERT.9 | 0.928 |   |\n| eff_b5 TO + (eca_nfnet_l0_b24l3_v2_6 + eff_b3).27 t80 PPit54 BERT.9 | 0.911 | 0.742 |\n| eff_b5 TO + (eca_nfnet_l0_b24l3_v2_6 + eff_b3).30 t80 PPit54 BERT.925 | 0.905 |   |\n| eff_b5 TO + (eca_nfnet_l0_b24l3_v2_6 + eff_b3).30 t80 PPit54 BERT.95 |   |   |","metadata":{}}]}