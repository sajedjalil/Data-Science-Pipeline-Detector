{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"from IPython.display import Image\nimport time\nimport numpy as np\nimport pandas as pd","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train = pd.read_csv(r\"/kaggle/input/shopee-product-matching/train.csv\")\ndf_test = pd.read_csv(r\"/kaggle/input/shopee-product-matching/test.csv\")\n#df_sample_sub = pd.read_csv(r\"/kaggle/input/shopee-product-matching/sample_submission.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"No missing Values. "},{"metadata":{},"cell_type":"markdown","source":"### Hashing:\n- Diffrence between cryptographic Hashes and perceptual hash\n> There are several ways to hash values in computer science. Most common one are **cryptographic hashes** (Md5, Sha1). You create a unique fingerprint of your value (picture). If only one pixel changes, your hash will be completely diffrent (Avalance effect).\n> **Perceptual Hash**. Creates a hash value that you can compare with others. If two pHashes are same, the picture is perceptual the same. You can even meausre how similiar two pictures are by calulating the distance between both phashes (using Hamming distance or MSE)   \n- [Mean Squared Error](https://www.pyimagesearch.com/2014/09/15/python-compare-two-images/)   \n- [pHash Concept](http://www.hackerfactor.com/blog/index.php?/archives/432-Looks-Like-It.html)   \n- [pHash Explanation](https://www.youtube.com/watch?v=pyzMP9qvxl0)\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.image_phash.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Looks like some pictures are already identical (via pHash)"},{"metadata":{},"cell_type":"markdown","source":"Lets plot the pictures with the same pHash and see if they identical "},{"metadata":{"trusted":true},"cell_type":"code","source":"test_pics = df_train[df_train.image_phash == \"fad28daa2ad05595\"] # pHash with most occurences in dataset","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# print all pictures with the same pHash\nfor i in test_pics.image:\n    image_path = r\"/kaggle/input/shopee-product-matching/train_images/{}\".format(i) \n    display(Image(filename=image_path))\n    time.sleep(1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"They are indeed perceptual identical"},{"metadata":{},"cell_type":"markdown","source":"Lets see if we can find more pictures that are identical by calculating the distances between the pHashes.   \n> Hammming Distance 0:\n* 94974f937d4c2433   \n* 94974f937d4c2433   \n(All bits are identical)    \n\n> Hamming Distance 1: \n* 94974f937d4c2433      \n* 94974f937d4c2432    \n(One Bit is diffrent)"},{"metadata":{},"cell_type":"markdown","source":"check if length of phash is always the same (if so, we need to consider this when calculating hemming distance)"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train[\"Length\"] = df_train.image_phash.apply(lambda x: len(x))\ndf_train[\"Length\"].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Its always the same length of 16"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.DataFrame(columns=['Hamming Distance','Bild1','Bild2'])\nfor i in df_train.image_phash[:100]:\n    for j in df_train.image_phash:\n        hamming_distance = sum(c1 != c2 for c1, c2 in zip(i, j))\n        if hamming_distance <= 4:\n            df = df.append({'Hamming Distance': hamming_distance, 'Bild1': i , 'Bild2': j }, ignore_index=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.sort_values(by=\"Hamming Distance\")\ndf[df['Hamming Distance'] == 1]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Here are two Pics with Hemming Distance of 10"},{"metadata":{"trusted":true},"cell_type":"code","source":"#df_train[df_train.image_phash == \"ee9690c347ceb02e\"]\ndisplay(Image(filename=r\"/kaggle/input/shopee-product-matching/train_images/00d4a4865fa5c81896287fc2c3ea0e1d.jpg\"))\n#df_train[df_train.image_phash == \"e899aece46d3b121\"]\ndisplay(Image(filename=r\"/kaggle/input/shopee-product-matching/train_images/fe3511856a8429fbdc1537e4eabcf2d5.jpg\"))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Another example with Hemming 10 "},{"metadata":{"trusted":true},"cell_type":"code","source":"#df_train[df_train.image_phash == \"c1d43c3f1f949685\"]\ndisplay(Image(filename=r\"/kaggle/input/shopee-product-matching/train_images/004ede0e328d05780813f853857463f1.jpg\"))\n#df_train[df_train.image_phash == \"c1843f3f66693d42\"]\ndisplay(Image(filename=r\"/kaggle/input/shopee-product-matching/train_images/5c39f46a76758d565560cc05bf06fb2d.jpg\"))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Hemming 5"},{"metadata":{"trusted":true},"cell_type":"code","source":"abc = df_train[df_train.image_phash == \"e699996699266699\"]\nabc\n#image_path = r\"/kaggle/input/shopee-product-matching/train_images/{}\".format(abc.image.to_string(index=False))\n#display(Image(filename=image_path))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"display(Image(filename=r\"/kaggle/input/shopee-product-matching/train_images/733dee44474dfc4aa8304d58312e7893.jpg\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"abc = df_train[df_train.image_phash == \"e69999668c663399\"]\nimage_path = r\"/kaggle/input/shopee-product-matching/train_images/{}\".format(abc.image.to_string(index=False))\ndisplay(Image(filename=image_path))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Hemming 4"},{"metadata":{"trusted":true},"cell_type":"code","source":"abc = df_train[df_train.image_phash == \"8112eeedf2c4a933\"]\nimage_path = r\"/kaggle/input/shopee-product-matching/train_images/{}\".format(abc.image.to_string(index=False))\ndisplay(Image(filename=image_path))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"abc = df_train[df_train.image_phash == \"8116eee9f2c0ad33\"]\nimage_path = r\"/kaggle/input/shopee-product-matching/train_images/{}\".format(abc.image.to_string(index=False))\ndisplay(Image(filename=image_path))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Bad example of Hemming 4"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train[df_train.image_phash == \"b3cccc26cc339933\"]\ndisplay(Image(filename=r\"/kaggle/input/shopee-product-matching/train_images/733dee44474dfc4aa8304d58312e7893.jpg\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"display(Image(filename=r\"/kaggle/input/shopee-product-matching/train_images/fadb5098b573655c17aabd8decaa1182.jpg\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"abc = df_train[df_train.image_phash == \"b3cccc26cc3333cc\"]\nimage_path = r\"/kaggle/input/shopee-product-matching/train_images/{}\".format(abc.image.to_string(index=False))\ndisplay(Image(filename=image_path))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3rd example with Hemming 4"},{"metadata":{"trusted":true},"cell_type":"code","source":"abc = df_train[df_train.image_phash == \"9acce5335ccd3a14\"]\nimage_path = r\"/kaggle/input/shopee-product-matching/train_images/{}\".format(abc.image.to_string(index=False))\ndisplay(Image(filename=image_path))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"abc = df_train[df_train.image_phash == \"9acce1334ced3a94\"]\nimage_path = r\"/kaggle/input/shopee-product-matching/train_images/{}\".format(abc.image.iloc[1])\ndisplay(Image(filename=image_path))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Hemming 2"},{"metadata":{"trusted":true},"cell_type":"code","source":"#abc = df_train[df_train.image_phash == \"fab111489dc3c74e\"]\n#abc\n#image_path = r\"/kaggle/input/shopee-product-matching/train_images/{}\".format(abc.image.to_string(index=False))\n#display(Image(filename=image_path))\ndisplay(Image(filename=r\"/kaggle/input/shopee-product-matching/train_images/a6fed93df91723e136ab0402a27a0e02.jpg\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"abc = df_train[df_train.image_phash == \"fab111489dc3c76c\"]\nimage_path = r\"/kaggle/input/shopee-product-matching/train_images/{}\".format(abc.image.to_string(index=False))\ndisplay(Image(filename=image_path))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Hemming 1"},{"metadata":{"trusted":true},"cell_type":"code","source":"abc = df_train[df_train.image_phash == \"e699996689663399\"]\nimage_path = r\"/kaggle/input/shopee-product-matching/train_images/{}\".format(abc.image.to_string(index=False))\ndisplay(Image(filename=image_path))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"abc = df_train[df_train.image_phash == \"e69999668c663399\"]\nimage_path = r\"/kaggle/input/shopee-product-matching/train_images/{}\".format(abc.image.to_string(index=False))\ndisplay(Image(filename=image_path))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## TODO:  \n* Schauen welche Bitpositionen relevant für die Bilder sind (zb Position 10 wichtig, Pos 16 eher nicht)\n* pHashes sortieren und beim durchloopen direkt dort anfangen wo es relevant ist (zb. Bild beginnt mit bbb, alle aaa nicht berücksichtigen)\n* Neue Spalten erstellen: 1. OCR Text, 2. Maße aus dem Titel , 3. Nur Text\n* phash Score berechnung: https://cloudinary.com/blog/how_to_automatically_identify_similar_images_using_phash"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}