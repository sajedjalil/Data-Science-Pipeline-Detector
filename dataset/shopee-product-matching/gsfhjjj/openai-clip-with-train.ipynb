{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## OpenAI CLIP with train\n\nThis notebook uses [OpenAI CLIP](https://github.com/openai/CLIP) to generate images and text features.","metadata":{}},{"cell_type":"code","source":"%%capture\n\n# Dirty code to make it work\n\nimport sys\n!cp -r ../input/openai-clip/CLIP/CLIP-main /tmp/\n\n# Kaggle likes to unpack .gz files in datasets... so we have to pack it back\n!gzip -c /tmp/CLIP-main/clip/bpe_simple_vocab_16e6.txt > /tmp/CLIP-main/clip/bpe_simple_vocab_16e6.txt.gz\nsys.path.append('/tmp/CLIP-main')\n\n!pip install ../input/openai-clip/ftfy-5.9/ftfy-5.9 \\\n             ../input/openai-clip/torch-1.7.1+cu110-cp37-cp37m-linux_x86_64.whl \\\n             ../input/openai-clip/torchvision-0.8.2+cu110-cp37-cp37m-linux_x86_64.whl \\\n             ../input/faiss-163/faiss_gpu-1.6.3-cp37-cp37m-manylinux2010_x86_64.whl","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.utils.data.sampler import Sampler\nimport clip\nfrom PIL import Image\nfrom pathlib import Path\nfrom tqdm.auto import tqdm\nimport re\nfrom clip.simple_tokenizer import SimpleTokenizer\nimport faiss\nimport matplotlib.pyplot as plt\nfrom triplet_loss import TripletLoss\n\n%matplotlib inline","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test = pd.read_csv('../input/shopee-product-matching/test.csv', index_col='posting_id')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Run train only for commit\nRUN_ON_TRAIN = len(df_test) == 3","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Utility classes and functions","metadata":{}},{"cell_type":"code","source":"_tokenizer = SimpleTokenizer()\n\n# Copied from https://github.com/openai/CLIP/blob/beba48f35392a73c6c47ae67ddffced81ad1916d/clip/clip.py#L164\n# but with relaxed exception\ndef tokenize(texts, context_length: int = 77) -> torch.LongTensor:\n    if isinstance(texts, str):\n        texts = [texts]\n\n    sot_token = _tokenizer.encoder[\"<|startoftext|>\"]\n    eot_token = _tokenizer.encoder[\"<|endoftext|>\"]\n    all_tokens = [[sot_token] + _tokenizer.encode(text) + [eot_token] for text in texts]\n    result = torch.zeros(len(all_tokens), context_length, dtype=torch.long)\n\n    for i, tokens in enumerate(all_tokens):\n        n = min(len(tokens), context_length)\n        result[i, :n] = torch.tensor(tokens)[:n]\n        if len(tokens) > context_length:\n            result[i, -1] = tokens[-1]\n\n    return result","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Remove EMOJI\nRE_EMOJI = re.compile(r\"\\\\x[A-Za-z0-9./]+\", flags=re.UNICODE)\n\ndef strip_emoji(text):\n    return RE_EMOJI.sub(r'', text)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class RollingMean():\n    def __init__(self):\n        self.n = 0\n        self.mean = 0\n        \n    def update(self, value):\n        self.mean = (self.mean * self.n + value) / (self.n+1)\n        self.n += 1\n        \n    def result(self):\n        return self.mean","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Sampler and dataset\n\nWe implement a sampler that ensures that in every batch, two samples of the same group are always present.\n\nThis is important in order to use Triplet SemiHardLoss (I'm using [this implementation](https://github.com/alfonmedela/triplet-loss-pytorch/blob/master/loss_functions/triplet_loss.py))","metadata":{}},{"cell_type":"code","source":"class SameGroupSampler(Sampler):\n    def __init__(self, df ,ds):\n        super().__init__(ds)\n        \n        # Create a dictionary of posting_id -> index in dataset\n        self.index_to_position = dict(zip(df.index, range(len(df))))\n        \n        # Create a Series of label_group -> set(posting_id)\n        self.label_group = df.reset_index().groupby('label_group')['posting_id'].apply(set).map(sorted).map(np.array)\n\n    def __len__(self):\n        return len(self.label_group)\n        \n    def __iter__(self):\n        for _ in range(len(self)):\n            # Sample one label_group\n            label_group_sample = self.label_group.sample(1).iloc[0]\n            \n            # Sample two posting_id's\n            sample1, sample2 = np.random.choice(label_group_sample, 2, replace=False)\n            \n            yield self.index_to_position[sample1]\n            yield self.index_to_position[sample2]            ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class MyDataset(Dataset):\n    def __init__(self, df, images_path):\n        super().__init__()\n        self.df = df\n        self.images_path = images_path\n        self.has_target = ('label_group' in df)\n        \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        \n        image = preprocess(Image.open(self.images_path / row['image']))\n        text = tokenize([strip_emoji(row['title'])])[0]\n        \n        if self.has_target:\n            return image, text, row['label_group']\n        else:\n            return image, text, 0","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Finetune CLIP on train data\n\nHere we use the triplet loss principe to ajust CLIP:\n\n![Triplet loss](https://user-images.githubusercontent.com/18154355/61485418-1cbb1f00-a96f-11e9-8de8-3c46eef5a7dc.png)\n\n\nWe didn't provide a validation set (yet!), so we are deliberating overfiting.","metadata":{}},{"cell_type":"code","source":"# Load CLIP\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nmodel, preprocess = clip.load(\"../input/openai-clip/ViT-B-32.pt\", device=device, jit=False)\n\n# Get embedding size\nembed_dim = model.text_projection.shape[1]\nembed_dim","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load train data\ntrain_images_path = Path('../input/shopee-product-matching/train_images')\n\ndf_train = pd.read_csv('../input/shopee-product-matching/train.csv', index_col='posting_id')\n\ndstrain = MyDataset(df_train, train_images_path)\ndltrain = DataLoader(dstrain, batch_size=128, num_workers=2, sampler=SameGroupSampler(df_train, dstrain))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"n_epochs = 1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# optim = torch.optim.AdamW(model.parameters(), lr=1e-4, eps=1e-8, weight_decay=1e-2)\noptim = torch.optim.SGD(model.parameters(), lr=1e-2, momentum=0.2)\nscheduler = torch.optim.lr_scheduler.OneCycleLR(optim, 1e-2, total_steps=n_epochs * (2*len(dltrain)-1),\n                                               base_momentum=0.0, max_momentum=0.5, pct_start=0.1, div_factor=1e2, final_div_factor=1e4)\ncriterion = TripletLoss(device)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for epoch in range(n_epochs):\n    with tqdm(total=2*len(dltrain)-1) as bar:\n        loss_mean = RollingMean()\n        for images, texts, targets in dltrain:\n            targets = targets.to(device)\n            \n            # Generate train and text features\n            images_features = model.encode_image(images.to(device))\n            texts_features = model.encode_text(texts.to(device))\n\n            optim.zero_grad()\n\n            # Join train and test features\n            features = torch.hstack([images_features, texts_features])\n            \n            # L2-normalize features\n            features = features / features.norm(2, dim=1, keepdim=True)\n\n            # Apply Triplet SemiHardLoss\n            loss = criterion(features, targets)\n\n            loss.backward()\n            optim.step()\n            scheduler.step()\n\n            # Update metric and progress bar\n            loss_mean.update(loss.item())\n            bar.update()\n            bar.set_description('{:.4f}'.format(loss_mean.result()))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Run on train\n\nIn this section we will generate features using CLIP and perform a similiarity search to find the closest matches.\n\nWe create the final set by taking away those results bellow a threshold similiarity (less 0.7)","metadata":{}},{"cell_type":"code","source":"def find_similarities_and_indexes(df, images_path, top_n=100, features_file=None):\n    # Create pytorch Dataset/DataLoader\n    ds = MyDataset(df, images_path)\n    dl = DataLoader(ds, batch_size=32, shuffle=False, num_workers=2)\n\n    # Allocate memory for features\n    features = np.empty((len(df), 2*embed_dim), dtype=np.float32)\n\n    # Begin predict\n    i = 0\n    for images, texts, _ in tqdm(dl):\n        n = len(images)\n        with torch.no_grad():\n            # Generate image and text features\n            images_features = model.encode_image(images.to(device))\n            texts_features = model.encode_text(texts.to(device))\n\n        # Concat features (first images then texts)\n        features[i:i+n, :embed_dim] = images_features.cpu()\n        features[i:i+n, embed_dim:] = texts_features.cpu()\n\n        i += n\n\n    # Option to save these features (may be usefull to tune cut value)\n    if features_file is not None:\n        np.save(features_file, features)\n\n    # l2-normalize\n    features /= np.linalg.norm(features, 2, axis=1, keepdims=True)\n\n    # Create index\n    index = faiss.IndexFlatIP(2*embed_dim)\n    index.add(features)\n\n    # Search index\n    return index.search(features, top_n)\n\n    # TODO: try range_search\n    # lims, similarities, indexes = index_test.range_search(test_features, GROUP_CUT)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if RUN_ON_TRAIN:\n    # Perform search of similiar items\n    similarities, indexes = find_similarities_and_indexes(df_train, train_images_path, features_file='features-no-norm.npy')\n    \n    # `similarities` will have shape (n, 100) and will have the similarites scores for closest matches\n    # `indexes` will have shape (n, 100) and have the index closest matches.\n    # Both arrays are aligned\n\n    # Convert index to groups, will have shape (n, 100)\n    found_groups = df_train['label_group'].values[indexes]\n\n    # Check if matches are from same group. Will create a boolean vector of (n, 100)\n    is_same_group = (found_groups == df_train['label_group'].values[:, np.newaxis])\n\n    # Plot similarities score from same group and different groups\n    plt.figure(figsize=(10, 5))\n    plt.hist([similarities[is_same_group], similarities[~is_same_group]], density=False, bins=51,\n         label=['Same group', 'Different group'], histtype='stepfilled', alpha=0.75)\n    plt.xlim(0, 1)\n    plt.xlabel('Similarity score')\n    plt.legend();","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Tune CUT\n\nIn this last step we will move the `cut_value` to find optimal F1-score.","metadata":{}},{"cell_type":"code","source":"# SRC: https://www.kaggle.com/c/shopee-product-matching/discussion/224782#1233338\n# With some adaptation\ndef row_wise_f1_score(y_true, y_pred):\n    tp = np.array([len(x[0] & x[1]) for x in zip(y_true, y_pred)])\n    fp = y_pred.apply(lambda x: len(x)).values - tp\n    fn = y_true.apply(lambda x: len(x)).values - tp\n\n    precision = tp / (tp + fp)\n    recall = tp / (tp + fn)\n    f1 = 2 * ((precision * recall) / (precision + recall))\n    return f1\n\n\ndef calc_score(cut_value):\n    # Apply cutoff of similarities\n    groups_are_same = (similarities > cut_value)\n\n    # Build results\n    results = []\n    for i, (group_is_same, index_result) in enumerate(zip(groups_are_same, indexes)):\n        row_results = df_train.index[index_result[group_is_same]]\n\n        # Keep found matches as a `set`\n        results.append(set(row_results))\n\n    df_results = pd.Series(results, index=df_answer.index)\n    \n    # Evaluate results\n    return row_wise_f1_score(df_answer, df_results).mean()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if RUN_ON_TRAIN:\n    # Create answer dataframe. This will have posting_id on index and a set of label_group as values \n    groups = df_train.reset_index().groupby('label_group')['posting_id'].apply(set)\n    df_answer = df_train['label_group'].map(groups)\n\n    # Cut values to evaluate\n    cuts = np.linspace(0.5, 0.95, 51)\n    scores = [calc_score(c) for c in tqdm(cuts)]\n\n    # Plot curve\n    plt.plot(cuts, scores)\n    plt.xlabel('Cutoff value')\n    plt.ylabel('F1 score')\n\n    print('Best cutoff is {:.2f} with expected F1 score of {:.4f}'.format(cuts[np.argmax(scores)], max(scores)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Run on test","metadata":{}},{"cell_type":"code","source":"GROUP_CUT = 0.71  # Use option `RUN_ON_TRAIN` to find this number","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_images_path = Path('../input/shopee-product-matching/test_images')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Find similar matches\nsimilarities, indexes = find_similarities_and_indexes(df_test, test_images_path)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Apply cutoff of similiarites\ntest_are_same_groups = (similarities > GROUP_CUT)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Build submission\nresults = []\n\nfor i, (test_is_same_group, index_result) in enumerate(zip(test_are_same_groups, indexes)):\n    row_results = set(df_test.index[index_result[test_is_same_group]])\n    \n    results.append({\n        'posting_id': df_test.index[i],\n        'matches': ' '.join(row_results)\n    })\n    \ndf_sub = pd.DataFrame(results)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_sub.to_csv('submission.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"☀️☀️☀️ Have a nice day! ☀️☀️☀️","metadata":{"trusted":true}}]}