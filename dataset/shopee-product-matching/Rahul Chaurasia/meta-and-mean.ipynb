{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%%capture\n!pip install ../input/pytorchimagemodels","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import sys\nsys.path.append('../input/facebookdeit')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport gc\nimport sys\n\nimport math\nimport random\n\nfrom tqdm.notebook import tqdm\n\nimport re\n\nimport cv2\nimport numpy as np\nimport pandas as pd\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, Dataset ,RandomSampler\n\nimport timm\n\nimport albumentations\n\nfrom swav_resnet import resnet50w2\n\nimport transformers\nfrom transformers import AutoModel, AutoTokenizer\n\nfrom models import deit_base_distilled_patch16_224\n\n\n\nimport cudf\nimport cuml\nimport cupy","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def set_seed(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n\nset_seed(0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"text_models=[\n    {\n        'transformer_model': '../input/shopee-paraphrase-xlm-r-multilingual/paraphrase-xlm-r-multilingual-v1_pp_7_14',\n        'model_path' : '../input/shopee-paraphrase-xlm-r-multilingual/paraphrase-xlm-r-multilingual-v1_pp_7_14/ckpt.pt',\n        'MAX_LEN' : 128,\n        'params' : {\n            'embed_dim' :1024,\n            'out_dim' : 11014\n        }\n        \n    },\n    {\n            'transformer_model': '../input/shopee-bert-base-indonesian/bert-base-indonesian_pp_7_8',\n            'model_path' : '../input/shopee-bert-base-indonesian/bert-base-indonesian_pp_7_8/ckpt.pt',\n            'MAX_LEN' : 128,\n            'params' : {\n                'embed_dim' :1024,\n                'out_dim' : 11014\n            }\n\n        }\n]\n\nTRANSFORMER_EMBED_DIM = 768","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"joint_models=[\n    {\n        'transformer_model': '../input/shopee-paraphrase-xlm-r-multilingual/paraphrase-xlm-r-multilingual-v1_pp_7_14',\n        'model_path' : '../input/deit-xlm-joint-weights/finetuned_deit_xlm_joint_7_11.pt',\n        'MAX_LEN' : 128,\n        'IMAGE_SIZE' : 224,\n        'params' : {\n                'embed_dim' : 2048,\n                'out_dim' : 11014\n            }\n        \n    }\n]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image_models=[\n    {\n        'vision_model': '../input/shopee-swav-finetuned-resnet50w2/swav_resnet50w2_224_2048_5_epochs_loss_12_beef.pt',\n        'IMAGE_SIZE' : 224,\n        'params' : {\n            'embed_dim' :2048,\n            'out_dim' : 11014\n        }\n    },\n    {\n        'vision_model': '../input/effnet-weights/finetuned_tf_efficientnet_b3_ns_300_2048_7_9.pt',\n        'IMAGE_SIZE' : 300,\n        'params' : {\n            'backbone_name' : 'tf_efficientnet_b3_ns',\n            'embed_dim' :2048,\n            'out_dim' : 11014\n        }\n    },\n    \n    {\n        'vision_model': '../input/deit-base-distilled-patch16-224/finetuned_deit_base_distilled_patch16_224_4_18.pt',\n        'IMAGE_SIZE' : 224,\n        'params' : {\n            'embed_dim' :2048,\n            'out_dim' : 11014\n        }\n    },\n]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"NUM_WORKERS = 4\nBATCH_SIZE = 128","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"CHECK_SUB = False\nGET_CV = True","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test = pd.read_csv('../input/shopee-product-matching/test.csv')\nif len(test)>3: GET_CV = False\nelse: print('this submission notebook will compute CV score, but commit notebook will not')\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def read_dataset():\n    if GET_CV:\n        \n        df = pd.read_csv('../input/shopee-product-matching/train.csv')\n        tmp = df.groupby(['label_group'])['posting_id'].unique().to_dict()\n        df['matches'] = df['label_group'].map(tmp)\n        df['matches'] = df['matches'].apply(lambda x: ' '.join(x))\n        \n        df['filepath'] = df['image'].apply(lambda x: os.path.join('../input/shopee-product-matching/train_images', x))\n        \n        if CHECK_SUB: \n            df = pd.concat([df, df], axis = 0)\n            df.reset_index(drop = True, inplace = True)\n                    \n    else:\n        df = pd.read_csv('../input/shopee-product-matching/test.csv')\n        \n        df['filepath'] = df['image'].apply(lambda x: os.path.join('../input/shopee-product-matching/test_images', x))\n                \n    return df ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def f1_score(y_true, y_pred):\n    y_true = y_true.apply(lambda x: set(x.split()))\n    y_pred = y_pred.apply(lambda x: set(x.split()))\n    intersection = np.array([len(x[0] & x[1]) for x in zip(y_true, y_pred)])\n    len_y_pred = y_pred.apply(lambda x: len(x)).values\n    len_y_true = y_true.apply(lambda x: len(x)).values\n    f1 = 2 * intersection / (len_y_pred + len_y_true)\n    return f1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## ArcFace utils","metadata":{}},{"cell_type":"code","source":"class DenseCrossEntropy(nn.Module):\n    def forward(self, x, target):\n        x = x.float()\n        target = target.float()\n        logprobs = torch.nn.functional.log_softmax(x, dim=-1)\n\n        loss = -logprobs * target\n        loss = loss.sum(-1)\n        return loss.mean()\n\n\nclass ArcMarginProduct_subcenter(nn.Module):\n    def __init__(self, in_features, out_features, k=3):\n        super().__init__()\n        self.weight = nn.Parameter(torch.FloatTensor(out_features*k, in_features))\n        self.reset_parameters()\n        self.k = k\n        self.out_features = out_features\n        \n    def reset_parameters(self):\n        stdv = 1. / math.sqrt(self.weight.size(1))\n        self.weight.data.uniform_(-stdv, stdv)\n        \n    def forward(self, features):\n        cosine_all = F.linear(F.normalize(features), F.normalize(self.weight))\n        cosine_all = cosine_all.view(-1, self.out_features, self.k)\n        cosine, _ = torch.max(cosine_all, dim=2)\n        return cosine   \n\n\nclass ArcFaceLossAdaptiveMargin(nn.modules.Module):\n    def __init__(self, margins, s=30.0):\n        super().__init__()\n        self.crit = DenseCrossEntropy()\n        self.s = s\n        self.margins = margins\n            \n    def forward(self, logits, labels, out_dim):\n        ms = []\n        ms = self.margins[labels.cpu().numpy()]\n        cos_m = torch.from_numpy(np.cos(ms)).float().cuda()\n        sin_m = torch.from_numpy(np.sin(ms)).float().cuda()\n        th = torch.from_numpy(np.cos(math.pi - ms)).float().cuda()\n        mm = torch.from_numpy(np.sin(math.pi - ms) * ms).float().cuda()\n        labels = F.one_hot(labels, out_dim).float()\n        logits = logits.float()\n        cosine = logits\n        sine = torch.sqrt(1.0 - torch.pow(cosine, 2))\n        phi = cosine * cos_m.view(-1,1) - sine * sin_m.view(-1,1)\n        phi = torch.where(cosine > th.view(-1,1), phi, cosine - mm.view(-1,1))\n        output = (labels * phi) + ((1.0 - labels) * cosine)\n        output *= self.s\n        loss = self.crit(output, labels)\n        return loss     \n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Common Projection Util","metadata":{}},{"cell_type":"code","source":"class Projection(nn.Module):\n    def __init__(self, d_in: int, d_out: int, p: float=0.5) -> None:\n        super().__init__()\n        self.linear1 = nn.Linear(d_in, d_out, bias=False)\n        self.linear2 = nn.Linear(d_out, d_out, bias=False)\n        self.layer_norm = nn.LayerNorm(d_out)\n        self.drop = nn.Dropout(p)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        embed1 = self.linear1(x)\n        embed2 = self.drop(self.linear2(F.gelu(embed1)))\n        embeds = self.layer_norm(embed1 + embed2)\n        return embeds","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Vision Model**","metadata":{}},{"cell_type":"code","source":"class VisionEncoderResnet(nn.Module):\n    def __init__(self, embed_dim , out_dim):\n        super().__init__()\n        base = resnet50w2() #torch.hub.load('facebookresearch/swav', 'resnet50w2')\n\n        d_in = 4096\n        self.base = base\n        self.projection = Projection(d_in, embed_dim)\n        self.metric_classify = ArcMarginProduct_subcenter(embed_dim, out_dim)\n\n        for p in self.base.parameters():\n            p.requires_grad = True\n\n    def forward(self, x):\n        projected_vec = self.projection(self.base(x))\n        cosine_feat= self.metric_classify(projected_vec)\n\n        return projected_vec ,cosine_feat","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class VisionEncoderEffnet(nn.Module):\n    def __init__(self,backbone_name, embed_dim , out_dim):\n        super().__init__()\n        base = timm.create_model(backbone_name, pretrained=False)\n        d_in = base.classifier.in_features\n        base.classifier = nn.Identity()\n        \n        self.base = base\n        self.projection = Projection(d_in, embed_dim)\n        self.metric_classify = ArcMarginProduct_subcenter(embed_dim, out_dim)\n\n        for p in self.base.parameters():\n            p.requires_grad = True\n\n    def forward(self, x):\n        projected_vec = self.projection(self.base(x))\n        cosine_feat= self.metric_classify(projected_vec)\n\n        return projected_vec ,cosine_feat","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class DeitWrapperVision(nn.Module):\n\n    def __init__(self):\n        super().__init__()\n        self.backbone = deit_base_distilled_patch16_224(pretrained=False)\n        self.backbone.head= nn.Identity()\n        self.backbone.head_dist = nn.Identity()\n\n    def forward_features(self, x):\n        # taken from https://github.com/facebookresearch/deit/blob/main/models.py\n\n        B = x.shape[0]\n        x = self.backbone.patch_embed(x)\n\n        cls_tokens = self.backbone.cls_token.expand(B, -1, -1)  # stole cls_tokens impl from Phil Wang, thanks\n        dist_token = self.backbone.dist_token.expand(B, -1, -1)\n        x = torch.cat((cls_tokens, dist_token, x), dim=1)\n\n        x = x + self.backbone.pos_embed\n        x = self.backbone.pos_drop(x)\n\n        for blk in self.backbone.blocks:\n            x = blk(x)\n\n        x = self.backbone.norm(x)\n        return x\n    \n    def forward(self, x):\n        hidden_states = self.forward_features(x)\n        _cls, _cls_dist = hidden_states[:, 0], hidden_states[:, 1]\n        return (_cls + _cls_dist) / 2   \n    \n    \nclass VisionEncoderDeit(nn.Module):\n    def __init__(self, embed_dim , out_dim):\n        super().__init__()\n        base = DeitWrapperVision()\n\n        d_in = 768\n        self.base = base\n        self.projection = Projection(d_in, embed_dim)\n        self.metric_classify = ArcMarginProduct_subcenter(embed_dim, out_dim)\n\n        for p in self.base.parameters():\n            p.requires_grad = True\n\n    def forward(self, x):\n        projected_vec = self.projection(self.base(x))\n        cosine_feat= self.metric_classify(projected_vec)\n\n        return projected_vec ,cosine_feat","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ShopeeDatasetVision(Dataset):\n    def __init__(self, csv, mode, transform):\n\n        self.csv = csv.reset_index()\n        self.mode= mode\n        self.transform = transform\n\n\n    def __len__(self):\n        return self.csv.shape[0]\n\n    def __getitem__(self, index):\n        row = self.csv.iloc[index]\n\n        image = cv2.imread(row.filepath)[:,:,::-1]\n\n        if self.transform is not None:\n            res = self.transform(image=image)\n            image = res['image'].astype(np.float32)\n        else:\n            image = image.astype(np.float32)\n\n        image = image.transpose(2, 0, 1)\n\n        if self.mode == 'test':\n            return torch.tensor(image)\n        else:\n            return torch.tensor(image), torch.tensor(row.label)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_transforms(image_size=512):\n\n    transforms_train = albumentations.Compose([\n        albumentations.HorizontalFlip(p=0.5),\n        albumentations.JpegCompression(quality_lower=99, quality_upper=100),\n        albumentations.ShiftScaleRotate(shift_limit=0.2, scale_limit=0.2, rotate_limit=10, border_mode=0, p=0.7),\n        albumentations.Resize(image_size, image_size),\n        albumentations.Cutout(max_h_size=int(image_size * 0.4), max_w_size=int(image_size * 0.4), num_holes=1, p=0.5),\n        albumentations.Normalize()\n    ])\n\n    transforms_val = albumentations.Compose([\n        albumentations.Resize(image_size, image_size),\n        albumentations.Normalize()\n    ])\n\n    return transforms_train, transforms_val","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def generate_image_embeddings(df, config, VisionEncoder):\n    print(config)\n    \n    vision_encoder_path = config['vision_model']\n    vision_encoder_params= config['params']\n    img_size = config['IMAGE_SIZE']\n\n    ckpt= torch.load(vision_encoder_path, map_location='cpu')\n\n    model= VisionEncoder(**vision_encoder_params)\n\n    model.load_state_dict(ckpt)\n    model = model.cuda()\n\n\n    ##########################################################################################################\n\n    _, transforms_val = get_transforms(img_size)\n\n    dataset_test = ShopeeDatasetVision(df, 'test', transform=transforms_val)\n    test_loader = torch.utils.data.DataLoader(dataset_test,  batch_size=BATCH_SIZE, num_workers=NUM_WORKERS)\n\n    ##########################################################################################################\n\n    model.eval()\n\n    image_embeddings = []\n\n    with torch.no_grad():\n        for batch in tqdm(test_loader):\n            images = batch\n\n            images = images.cuda()\n            image_embed , _= model(images)\n\n            image_embeddings.append(image_embed.detach().cpu())\n\n    image_embeddings = torch.cat(image_embeddings, dim=0)\n    #image_embeddings= F.normalize(image_embeddings)\n\n    print(f'image_embeddings shape : {image_embeddings.shape}')\n    \n    del ckpt\n    del model\n    gc.collect()\n    torch.cuda.empty_cache()\n    \n    return image_embeddings","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Text Model**","metadata":{}},{"cell_type":"code","source":"class TextEncoder(nn.Module):\n    def __init__(self, embed_dim , out_dim, transformer_model):\n        super().__init__()\n        \n        self.base = AutoModel.from_pretrained(transformer_model)\n        \n        self.projection = Projection(TRANSFORMER_EMBED_DIM, embed_dim)\n        self.metric_classify = ArcMarginProduct_subcenter(embed_dim, out_dim)\n        \n        for p in self.base.parameters():\n            p.requires_grad = True\n\n    @staticmethod\n    def mean_pooling(model_output, attention_mask):\n        token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n        sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n        sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n        return sum_embeddings / sum_mask\n\n    def forward(self, input_ids,attention_mask):\n        out = self.base(input_ids=input_ids, attention_mask=attention_mask)\n        out = TextEncoder.mean_pooling(out, attention_mask)\n        \n        projected_vec = self.projection(out)\n        cosine_feat= self.metric_classify(projected_vec)\n        \n        return projected_vec ,cosine_feat\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Tokenizer:\n    def __init__(self,max_length, transformer_model):\n        self.tokenizer = AutoTokenizer.from_pretrained(transformer_model)\n        self.max_length = max_length\n\n    def __call__(self, x) :\n        return self.tokenizer(\n            x, max_length=self.max_length, truncation=True, padding='max_length', return_tensors=\"pt\"\n        )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ShopeeDatasetText(Dataset):\n    def __init__(self, csv, mode, tokenizer):\n\n        self.csv = csv.reset_index()\n        self.mode= mode\n        self.tokenizer = tokenizer\n\n\n    def __len__(self):\n        return self.csv.shape[0]\n    \n    @staticmethod\n    def string_escape(s, encoding='utf-8'):\n        return (\n            s.encode('latin1')  # To bytes, required by 'unicode-escape'\n            .decode('unicode-escape')  # Perform the actual octal-escaping decode\n            .encode('latin1')  # 1:1 mapping back to bytes\n            .decode(encoding)\n        )  # Decode original encoding\n\n    @staticmethod\n    def preprocess_text(x):\n        x = ShopeeDatasetText.string_escape(x)\n        x = re.sub(r'[^\\w\\s]',' ', x)\n        return x\n\n    def __getitem__(self, index):\n        row = self.csv.iloc[index]\n        text = row.title\n        \n        text= ShopeeDatasetText.preprocess_text(text)\n\n        encoded_text = self.tokenizer(text)\n        \n        input_ids = encoded_text['input_ids'][0]\n        attention_mask = encoded_text['attention_mask'][0]\n\n        if self.mode == 'test':\n            return input_ids, attention_mask\n        else:\n            return input_ids, attention_mask, torch.tensor(row.label)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def generate_text_embeddings(df, config):\n    print(config)\n    \n    transformer_model= config['transformer_model']\n    max_len= config['MAX_LEN']\n\n\n    ckpt= torch.load(config['model_path'], map_location='cpu')\n\n    model= TextEncoder(**config['params'], transformer_model=transformer_model)\n\n    model.load_state_dict(ckpt)\n    model = model.cuda()\n\n\n    ##########################################################################################################\n\n    dataset_test = ShopeeDatasetText(df, 'test', Tokenizer(max_len, transformer_model))\n    test_loader = torch.utils.data.DataLoader(dataset_test,  batch_size=BATCH_SIZE, num_workers=NUM_WORKERS)\n\n    ##########################################################################################################\n\n    model.eval()\n\n    text_embeddings = []\n\n    with torch.no_grad():\n        for batch in tqdm(test_loader):\n            \n            input_ids, attention_mask = batch\n\n            input_ids, attention_mask = input_ids.cuda(), attention_mask.cuda()\n            \n            text_embed , _= model(input_ids, attention_mask)\n\n            text_embeddings.append(text_embed.detach().cpu())\n\n    text_embeddings = torch.cat(text_embeddings, dim=0)\n    #text_embeddings= F.normalize(text_embeddings)\n\n    print(f'text_embeddings shape : {text_embeddings.shape}')\n    \n    del ckpt\n    del model\n    gc.collect()\n    torch.cuda.empty_cache()\n    \n    return text_embeddings","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Joint Transformer**","metadata":{}},{"cell_type":"code","source":"class ShopeeDatasetJoint(Dataset):\n    def __init__(self, csv, mode, tokenizer, transform):\n\n        self.csv = csv.reset_index()\n        self.mode= mode\n        self.tokenizer = tokenizer\n        self.transform = transform\n\n\n    def __len__(self):\n        return self.csv.shape[0]\n    \n    @staticmethod\n    def string_escape(s, encoding='utf-8'):\n        return (\n            s.encode('latin1')  # To bytes, required by 'unicode-escape'\n            .decode('unicode-escape')  # Perform the actual octal-escaping decode\n            .encode('latin1')  # 1:1 mapping back to bytes\n            .decode(encoding)\n        )  # Decode original encoding\n\n    @staticmethod\n    def preprocess_text(x):\n        x = ShopeeDatasetJoint.string_escape(x)\n        x = re.sub(r'[^\\w\\s]',' ', x)\n        return x\n\n    def __getitem__(self, index):\n        row = self.csv.iloc[index]\n        text = row.title\n        \n        text= ShopeeDatasetJoint.preprocess_text(text)\n\n\n        ##################################################################\n        \n        encoded_text = self.tokenizer(text)\n        input_ids = encoded_text['input_ids'][0]\n        attention_mask = encoded_text['attention_mask'][0]\n        ##################################################################\n        \n        image = cv2.imread(row.filepath)[:,:,::-1]\n\n        if self.transform is not None:\n            res = self.transform(image=image)\n            image = res['image'].astype(np.float32)\n        else:\n            image = image.astype(np.float32)\n\n        image = image.transpose(2, 0, 1)\n        ##################################################################\n        \n\n        if self.mode == 'test':\n            return image, input_ids, attention_mask\n        else:\n            return image, input_ids, attention_mask, torch.tensor(row.label)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class DeitWrapperJoint(nn.Module):\n\n    def __init__(self):\n        super().__init__()\n        self.backbone = deit_base_distilled_patch16_224(pretrained=False)\n        self.backbone.head= nn.Identity()\n        self.backbone.head_dist = nn.Identity()\n\n    def forward_features(self, x):\n        # taken from https://github.com/facebookresearch/deit/blob/main/models.py\n\n        B = x.shape[0]\n        x = self.backbone.patch_embed(x)\n\n        cls_tokens = self.backbone.cls_token.expand(B, -1, -1)  # stole cls_tokens impl from Phil Wang, thanks\n        dist_token = self.backbone.dist_token.expand(B, -1, -1)\n        x = torch.cat((cls_tokens, dist_token, x), dim=1)\n\n        x = x + self.backbone.pos_embed\n        x = self.backbone.pos_drop(x)\n\n        for blk in self.backbone.blocks:\n            x = blk(x)\n\n        x = self.backbone.norm(x)\n        return x\n\n    def forward(self, x):\n        hidden_states = self.forward_features(x)\n        _cls, _cls_dist = hidden_states[:, 0], hidden_states[:, 1]\n        merge_cls= (_cls + _cls_dist) / 2\n        merge_cls = merge_cls.unsqueeze(1)\n        \n        new_hidden_states = torch.cat([merge_cls, hidden_states[:, 1:-1,:]], dim=1)\n        return new_hidden_states","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_text_encoder(transformer_model):\n    text_encoder = AutoModel.from_pretrained(transformer_model)\n    \n    for p in text_encoder.parameters():\n        p.requires_grad = False\n    \n    return text_encoder","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class JointTransformer(nn.Module):\n    \n    def __init__(self, embed_dim, out_dim, transformer_model):\n        super().__init__()\n        \n        \n        self.vision_encoder= DeitWrapperJoint()\n        self.text_encoder= get_text_encoder(transformer_model)\n\n        \n        d_in=768\n        encoder_layer = nn.TransformerEncoderLayer(d_model =768, nhead=8, dim_feedforward=2048, dropout=0.1, activation='gelu')\n        encoder = nn.TransformerEncoder(encoder_layer, num_layers=2)\n        \n        self.transformer_encoder = encoder\n        \n        self.projection = Projection(d_in, embed_dim)\n        self.metric_classify = ArcMarginProduct_subcenter(embed_dim, out_dim)\n    \n    @staticmethod \n    def merge_hidden_states(vision_output, text_output):\n        cls_img= vision_output[:,0,: ]\n        cls_text= text_output[:,0,:]\n        cls_joint= (cls_img+cls_text)/2\n        cls_joint= cls_joint.unsqueeze(1)\n\n        merge_embeddings = torch.cat([cls_joint , vision_output[:,1: ,:] ,  text_output[:,1: ,:]], dim=1)\n        return merge_embeddings\n    \n    @staticmethod \n    def get_pad_mask(image_embed_len , text_attention_mask):\n\n        _batch_size= text_attention_mask.shape[0]\n\n        imge_attention_mask = torch.ones((_batch_size, image_embed_len), dtype=torch.float32).cuda()\n        merge_attention_mask= torch.cat([imge_attention_mask, text_attention_mask], dim=1)\n        merge_attention_mask= (1-merge_attention_mask).bool()\n        return merge_attention_mask\n    \n    def forward_features(self, image, input_ids, attention_mask):\n        \n        vision_output = self.vision_encoder(image)\n        text_output = self.text_encoder(input_ids, attention_mask)[0]\n        \n        text_attention_mask = attention_mask\n\n        image_embed_len = vision_output.shape[1]-1\n\n        joint_embeddings=  JointTransformer.merge_hidden_states(vision_output, text_output)\n        joint_embeddings= joint_embeddings.permute(1,0,2)\n\n        joint_pad_mask = JointTransformer.get_pad_mask(image_embed_len , text_attention_mask)\n        \n        out= self.transformer_encoder(joint_embeddings, src_key_padding_mask = joint_pad_mask)\n        out= out.permute(1,0,2)\n        \n        return out\n    \n    def forward(self, image, input_ids, attention_mask):\n        x= self.forward_features(image, input_ids, attention_mask)\n        x= x[:,0,:] #cls token\n        \n        projected_vec = self.projection(x)\n        cosine_feat= self.metric_classify(projected_vec)\n        return projected_vec ,cosine_feat","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def generate_joint_embeddings(df, config):\n    \n    print(config)\n    \n    transformer_model= config['transformer_model']\n    max_len= config['MAX_LEN']\n    model_path= config['model_path']\n    img_size = config['IMAGE_SIZE']\n\n    ckpt= torch.load(model_path, map_location='cpu')\n\n    model= JointTransformer(**config['params'], transformer_model=transformer_model)\n\n    model.load_state_dict(ckpt)\n    model = model.cuda()\n\n    _, transforms_val = get_transforms(img_size)\n\n    ##########################################################################################################\n\n    dataset_test = ShopeeDatasetJoint(df, 'test', tokenizer=Tokenizer(max_len, transformer_model), transform=transforms_val)\n    test_loader = torch.utils.data.DataLoader(dataset_test,  batch_size=BATCH_SIZE, num_workers=NUM_WORKERS)\n\n    ##########################################################################################################\n\n    model.eval()\n\n    joint_embeddings = []\n\n    with torch.no_grad():\n        for batch in tqdm(test_loader):\n            \n            images, input_ids, attention_mask = batch\n\n            images = images.cuda()\n            input_ids, attention_mask = input_ids.cuda(), attention_mask.cuda()\n            \n            _embed , _= model(images, input_ids, attention_mask)\n\n            joint_embeddings.append(_embed.detach().cpu())\n\n    joint_embeddings = torch.cat(joint_embeddings, dim=0)\n    joint_embeddings= F.normalize(joint_embeddings)\n\n    print(f'joint_embeddings shape : {joint_embeddings.shape}')\n    \n    del ckpt\n    del model\n    gc.collect()\n    torch.cuda.empty_cache()\n    \n    return joint_embeddings","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Smilarity Search**","metadata":{}},{"cell_type":"code","source":"def get_neighbours_cos_sim(df,embeddings, tuned_threshold, mode='image'):\n    '''\n    When using cos_sim use normalized features else use normal features\n    \n    mode: 'image', 'text, 'joint'\n    '''\n    \n    print(f'Setting mode : {mode}')\n    \n    embeddings = cupy.array(embeddings)\n    \n    if GET_CV:\n        if mode=='image':\n            thresholds = list(np.arange(0.3,0.5,0.05))\n            \n        elif mode=='text':\n            thresholds = list(np.arange(0.3,0.5,0.05))\n            \n        elif mode=='joint':\n            thresholds = list(np.arange(0.3,0.6,0.05))\n\n        scores = []\n        for threshold in thresholds:\n            \n################################################# Code for Getting Preds #########################################\n            preds = []\n            preds2= []\n        \n            CHUNK = 1024*4\n\n            print('Finding similar titles...for threshold :',threshold)\n            CTS = len(embeddings)//CHUNK\n            if len(embeddings)%CHUNK!=0: CTS += 1\n\n            for j in range( CTS ):\n                a = j*CHUNK\n                b = (j+1)*CHUNK\n                b = min(b,len(embeddings))\n\n                cts = cupy.matmul(embeddings,embeddings[a:b].T).T\n\n                for k in range(b-a):\n                    IDX = cupy.where(cts[k,]>threshold)[0]\n                    o = df.iloc[cupy.asnumpy(IDX)].posting_id.values\n                    preds.append(o)\n                    \n                    o2 = ' '.join(o)\n                    preds2.append(o2)\n######################################################################################################################\n\n            df['pred_matches'] = preds2\n            df['f1'] = f1_score(df['matches'], df['pred_matches'])\n            score = df['f1'].mean()\n            print(f'Our f1 score for threshold {threshold} is {score}')\n            scores.append(score)\n            \n        thresholds_scores = pd.DataFrame({'thresholds': thresholds, 'scores': scores})\n        max_score = thresholds_scores[thresholds_scores['scores'] == thresholds_scores['scores'].max()]\n        best_threshold = max_score['thresholds'].values[0]\n        best_score = max_score['scores'].values[0]\n        print(f'Our best score is {best_score} and has a threshold {best_threshold}')\n            \n    else:\n        preds = []\n        CHUNK = 1024*4\n        threshold = tuned_threshold\n\n        print('Finding similar texts...for threshold :',threshold)\n        CTS = len(embeddings)//CHUNK\n        if len(embeddings)%CHUNK!=0: CTS += 1\n\n        for j in range( CTS ):\n            a = j*CHUNK\n            b = (j+1)*CHUNK\n            b = min(b,len(embeddings))\n            print('chunk',a,'to',b)\n\n            cts = cupy.matmul(embeddings,embeddings[a:b].T).T\n\n            for k in range(b-a):\n                IDX = cupy.where(cts[k,]>threshold)[0]\n                o = df.iloc[cupy.asnumpy(IDX)].posting_id.values\n                preds.append(o)\n                    \n    return df, preds","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def combine_predictions(row):\n    x = np.concatenate([row['image_predictions'], row['text_predictions'], row['mean_predictions'] , row['joint_predictions']])\n    return ' '.join( np.unique(x) )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class SimilarityFinder:\n    def __init__(self,text_embeddings, image_embeddings, joint_embeddings):\n        self.text_embeddings = cupy.array(text_embeddings)\n        self.image_embeddings = cupy.array(image_embeddings)\n        self.joint_embeddings = cupy.array(joint_embeddings)\n        \n        \n    def get_cosine_similarity(self, idx_a, idx_b):\n        query1= self.text_embeddings[idx_a : idx_b]\n        query2= self.image_embeddings[idx_a : idx_b]\n        query3= self.joint_embeddings[idx_a : idx_b]\n        \n        \n        dot_similarity1 = cupy.matmul(query1, self.text_embeddings.T)\n        dot_similarity2 = cupy.matmul(query2, self.image_embeddings.T)\n        dot_similarity3 = cupy.matmul(query3, self.joint_embeddings.T)\n        \n        \n        dot_similarity= 0.25*dot_similarity1+ 0.25*dot_similarity2 + 0.5*dot_similarity3\n        \n        return dot_similarity\n    \n    def generate_preds(self,df, threshold):\n        preds= []\n\n        CHUNK = 1024*4\n        CTS = len(self.text_embeddings)//CHUNK\n        if len(self.text_embeddings)%CHUNK!=0:\n            CTS += 1\n\n        for j in range( CTS ):\n            a = j*CHUNK\n            b = (j+1)*CHUNK\n            b = min(b,len(self.text_embeddings))\n\n            sims = self.get_cosine_similarity(a, b)\n\n            for idx in range(len(sims)):\n                indices = cupy.where(sims[idx,]>threshold)[0]         \n                _preds= df.iloc[cupy.asnumpy(indices)].posting_id.values\n                preds.append(_preds)\n\n        return preds\n    \n    def threshold_tuner(self,df):\n        thresholds = list(np.arange(0.3,0.5,0.05))\n        scores= []\n        \n        for threshold in thresholds:\n            print('Finding joint sim threshold :',threshold)\n            \n            preds= self.generate_preds(df,threshold)\n            preds2= list(map(lambda x : ' '.join(x) , preds))\n            \n            df['pred_matches'] = preds2\n            df['f1'] = f1_score(df['matches'], df['pred_matches'])\n            score = df['f1'].mean()\n            scores.append(score)\n            \n            print(f'Our f1 score for threshold {threshold} is {score}')\n\n        thresholds_scores = pd.DataFrame({'thresholds': thresholds, 'scores': scores})\n        max_score = thresholds_scores[thresholds_scores['scores'] == thresholds_scores['scores'].max()]\n        best_threshold = max_score['thresholds'].values[0]\n        best_score = max_score['scores'].values[0]\n        print(f'Our best score is {best_score} and has a threshold {best_threshold}')\n        return df,preds\n    \n    def get_preds(self, df, tuned_threshold):\n        if GET_CV:\n            df,preds= self.threshold_tuner(df)\n            #df['matches'] = df['pred_matches']\n        else:\n            preds= self.generate_preds(df,tuned_threshold)\n            #preds= list(map(lambda x : ' '.join(x) , preds))\n            #df['matches'] = preds\n            \n        return df,preds","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Driver**","metadata":{}},{"cell_type":"code","source":"df = read_dataset()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Image Predictions**","metadata":{}},{"cell_type":"code","source":"image_embeddings_swav = generate_image_embeddings(df, image_models[0], VisionEncoderResnet)\nimage_embeddings_effnet = generate_image_embeddings(df, image_models[1], VisionEncoderEffnet)\nimage_embeddings_deit = generate_image_embeddings(df, image_models[2], VisionEncoderDeit)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"meta_image_embedding = 0.33*image_embeddings_deit+ 0.33*image_embeddings_swav+ 0.34*image_embeddings_effnet\n\n# image_embeddings_deit = F.normalize(image_embeddings_deit)\n# image_embeddings_swav = F.normalize(image_embeddings_swav)\n# image_embeddings_effnet = F.normalize(image_embeddings_effnet)\nmeta_image_embedding  = F.normalize(meta_image_embedding)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df,image_predictions = get_neighbours_cos_sim(df,meta_image_embedding, tuned_threshold=0.75, mode='image')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Text Predictions**","metadata":{}},{"cell_type":"code","source":"text_embeddings_xlm = generate_text_embeddings(df, text_models[0])\ntext_embeddings_bert = generate_text_embeddings(df, text_models[1])\n\nmeta_text_embeddings = 0.5*text_embeddings_xlm + 0.5*text_embeddings_bert\nmeta_text_embeddings= F.normalize(meta_text_embeddings)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df,text_predictions = get_neighbours_cos_sim(df,meta_text_embeddings, tuned_threshold=0.7, mode='text')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Joint Predictions**","metadata":{}},{"cell_type":"code","source":"joint_embeddings = generate_joint_embeddings(df, joint_models[0])\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df,joint_predictions = get_neighbours_cos_sim(df,joint_embeddings, tuned_threshold=0.7, mode='joint')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Merge Predictions**","metadata":{}},{"cell_type":"code","source":"sim_finder= SimilarityFinder(meta_text_embeddings, meta_image_embedding, joint_embeddings)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df,mean_preds= sim_finder.get_preds(df,0.5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if GET_CV:\n    df['image_predictions'] = image_predictions\n    df['text_predictions'] = text_predictions\n    df['joint_predictions'] = joint_predictions\n    df['mean_predictions'] = mean_preds\n    \n    df['pred_matches'] = df.apply(combine_predictions, axis = 1)\n    \n    df['f1'] = f1_score(df['matches'], df['pred_matches'])\n    score = df['f1'].mean()\n    print(f'Our final f1 cv score is {score}')\n    df['matches'] = df['pred_matches']\n    df[['posting_id', 'matches']].to_csv('submission.csv', index = False)\nelse:\n    df['image_predictions'] = image_predictions\n    df['text_predictions'] = text_predictions\n    df['joint_predictions'] = joint_predictions\n    df['mean_predictions'] = mean_preds\n    \n    df['matches'] = df.apply(combine_predictions, axis = 1)\n    \n    df[['posting_id', 'matches']].to_csv('submission.csv', index = False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **DEBUG**","metadata":{}},{"cell_type":"raw","source":"import matplotlib.pyplot as plt","metadata":{}},{"cell_type":"raw","source":"_eid = 0\n\n\nselect_embeddings = [meta_image_embedding ,meta_text_embeddings , joint_embeddings,image_embeddings_effnet][_eid]","metadata":{}},{"cell_type":"raw","source":"idx=12517\nquery= select_embeddings[idx].unsqueeze(0)\ndot_similarity = query@select_embeddings.T","metadata":{}},{"cell_type":"raw","source":"sims= dot_similarity.topk(9)\n\nmatches = []\nall_dfs= [] \nfor val , each in zip(sims.values.tolist()[0],sims.indices.tolist()[0]):\n    print(each, val)\n\n    subdf=df.iloc[each:each+1]\n    all_dfs.append(subdf)\n\nmatch_df= pd.concat(all_dfs)\nmatches = list(match_df.filepath)\nmatch_title = list(match_df.title)\n\nplt.figure(figsize=(20, 20))\nfor i in range(9):\n    ax = plt.subplot(3, 3, i + 1)\n    plt.imshow(cv2.imread(matches[i])[...,::-1])\n    plt.title(match_title[i])\n    plt.axis(\"off\")\n\nmatch_df","metadata":{}},{"cell_type":"raw","source":"plt.figure(figsize=(20, 20))\n\ncount=0\n\nfor i, (_, row) in enumerate(list(df.loc[df['label_group'] == match_df.iloc[0].label_group].iterrows())[:9]):\n    ax = plt.subplot(3, 3, i + 1)\n    plt.imshow(cv2.imread(row.filepath)[...,::-1])\n    plt.title(str(row.title)+'\\n')\n    plt.axis(\"off\")\n    count+=1\n    if count==8:\n        break","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}