{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import sys\n!cp ../input/rapids/rapids.0.18.0 /opt/conda/envs/rapids.tar.gz\n!cd /opt/conda/envs/ && tar -xzvf rapids.tar.gz > /dev/null\nsys.path = [\"/opt/conda/envs/rapids/lib/python3.7/site-packages\"] + sys.path\nsys.path = [\"/opt/conda/envs/rapids/lib/python3.7\"] + sys.path\nsys.path = [\"/opt/conda/envs/rapids/lib\"] + sys.path \n!cp /opt/conda/envs/rapids/lib/libxgboost.so /opt/conda/lib/","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Preliminaries\nfrom tqdm import tqdm\n\nimport random\nimport os\n\n\n#torch\nimport torch\nimport torch.nn as nn\nfrom torch.nn import Parameter\nfrom torch.nn import functional as F\nfrom torch.utils.data import Dataset,DataLoader\n\nimport transformers\n\n\nfrom sklearn.preprocessing import Normalizer\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# image\n","metadata":{}},{"cell_type":"code","source":"!pip install ../input/download/Keras_Applications-1.0.8-py3-none-any.whl\n!pip install ../input/download/efficientnet-1.1.1-py3-none-any.whl","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport gc\nimport matplotlib.pyplot as plt\nimport cudf\nimport cuml\nimport cupy\nfrom cuml.feature_extraction.text import TfidfVectorizer\nfrom cuml import PCA\nfrom cuml.neighbors import NearestNeighbors\nimport tensorflow as tf\nimport efficientnet.tfkeras as efn\nfrom tqdm.notebook import tqdm\nimport math\nimport tensorflow_hub as hub","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# For tf.dataset\nAUTO = tf.data.experimental.AUTOTUNE\n\n# Configuration\nBATCH_SIZE = 8\nIMAGE_SIZE = [512, 512]\n# Seed\nSEED = 42\n# Verbosity\nVERBOSE = 1\n# Number of classes\nN_CLASSES = 11014","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"LIMIT = 4.0\ngpus = tf.config.experimental.list_physical_devices('GPU')\nif gpus:\n    try:\n        tf.config.experimental.set_virtual_device_configuration(\n            gpus[0],\n            [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=1024*LIMIT)])\n        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n        #print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n    except RuntimeError as e:\n        print(e)\nprint('We will restrict TensorFlow to max %iGB GPU RAM'%LIMIT)\nprint('then RAPIDS can use %iGB GPU RAM'%(16-LIMIT))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Flag to get cv score\nGET_CV = False\n# Flag to check ram allocations (debug)\nCHECK_SUB = False\n\ndf = pd.read_csv('../input/shopee-product-matching/test.csv')\n# If we are comitting, replace train set for test set and dont get cv\nif len(df) > 3:\n    GET_CV = False\ndel df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function to get our f1 score\ndef f1_score(y_true, y_pred):\n    y_true = y_true.apply(lambda x: set(x.split()))\n    y_pred = y_pred.apply(lambda x: set(x.split()))\n    intersection = np.array([len(x[0] & x[1]) for x in zip(y_true, y_pred)])\n    len_y_pred = y_pred.apply(lambda x: len(x)).values\n    len_y_true = y_true.apply(lambda x: len(x)).values\n    f1 = 2 * intersection / (len_y_pred + len_y_true)\n    return f1\n\n# Function to combine predictions\ndef combine_predictions(row):\n    x = np.concatenate([row['image_predictions']])\n    return ' '.join( np.unique(x) )\n\n\n# Function to read out dataset\ndef read_dataset():\n    if GET_CV:\n        df = pd.read_csv('../input/shopee-product-matching/train.csv')\n        tmp = df.groupby(['label_group'])['posting_id'].unique().to_dict()\n        df['matches'] = df['label_group'].map(tmp)\n        df['matches'] = df['matches'].apply(lambda x: ' '.join(x))\n        if CHECK_SUB:\n            df = pd.concat([df, df], axis = 0)\n            df.reset_index(drop = True, inplace = True)\n        df_cu = cudf.DataFrame(df)\n        image_paths = '../input/shopee-product-matching/train_images/' + df['image']\n    else:\n        df = pd.read_csv('../input/shopee-product-matching/test.csv')\n        df_cu = cudf.DataFrame(df)\n        image_paths = '../input/shopee-product-matching/test_images/' + df['image']\n        \n    return df, df_cu, image_paths\n\n\n# Function to decode our images\ndef decode_image(image_data):\n    image = tf.image.decode_jpeg(image_data, channels = 3)\n    image = tf.image.resize(image, IMAGE_SIZE)\n    image = tf.cast(image, tf.float32) / 255.0\n    return image\n\n# Function to read our test image and return image\ndef read_image(image):\n    image = tf.io.read_file(image)\n    image = decode_image(image)\n    return image\n\n# Function to get our dataset that read images\ndef get_dataset(image):\n    dataset = tf.data.Dataset.from_tensor_slices(image)\n    dataset = dataset.map(read_image, num_parallel_calls = AUTO)\n    dataset = dataset.batch(BATCH_SIZE)\n    dataset = dataset.prefetch(AUTO)\n    return dataset\n\n\n# Arcmarginproduct class keras layer\nclass ArcMarginProduct(tf.keras.layers.Layer):\n    '''\n    Implements large margin arc distance.\n\n    Reference:\n        https://arxiv.org/pdf/1801.07698.pdf\n        https://github.com/lyakaap/Landmark2019-1st-and-3rd-Place-Solution/\n            blob/master/src/modeling/metric_learning.py\n    '''\n    def __init__(self, n_classes, s=30, m=0.50, easy_margin=False,\n                 ls_eps=0.0, **kwargs):\n\n        super(ArcMarginProduct, self).__init__(**kwargs)\n\n        self.n_classes = n_classes\n        self.s = s\n        self.m = m\n        self.ls_eps = ls_eps\n        self.easy_margin = easy_margin\n        self.cos_m = tf.math.cos(m)\n        self.sin_m = tf.math.sin(m)\n        self.th = tf.math.cos(math.pi - m)\n        self.mm = tf.math.sin(math.pi - m) * m\n\n    def get_config(self):\n\n        config = super().get_config().copy()\n        config.update({\n            'n_classes': self.n_classes,\n            's': self.s,\n            'm': self.m,\n            'ls_eps': self.ls_eps,\n            'easy_margin': self.easy_margin,\n        })\n        return config\n\n    def build(self, input_shape):\n        super(ArcMarginProduct, self).build(input_shape[0])\n\n        self.W = self.add_weight(\n            name='W',\n            shape=(int(input_shape[0][-1]), self.n_classes),\n            initializer='glorot_uniform',\n            dtype='float32',\n            trainable=True,\n            regularizer=None)\n\n    def call(self, inputs):\n        X, y = inputs\n        y = tf.cast(y, dtype=tf.int32)\n        cosine = tf.matmul(\n            tf.math.l2_normalize(X, axis=1),\n            tf.math.l2_normalize(self.W, axis=0)\n        )\n        sine = tf.math.sqrt(1.0 - tf.math.pow(cosine, 2))\n        phi = cosine * self.cos_m - sine * self.sin_m\n        if self.easy_margin:\n            phi = tf.where(cosine > 0, phi, cosine)\n        else:\n            phi = tf.where(cosine > self.th, phi, cosine - self.mm)\n        one_hot = tf.cast(\n            tf.one_hot(y, depth=self.n_classes),\n            dtype=cosine.dtype\n        )\n        if self.ls_eps > 0:\n            one_hot = (1 - self.ls_eps) * one_hot + self.ls_eps / self.n_classes\n\n        output = (one_hot * phi) + ((1.0 - one_hot) * cosine)\n        output *= self.s\n        return output\n\n\n\n# Function to get the embeddings of our images with the fine-tuned model\ndef get_image_embeddings(image_paths):\n    embeds = []\n    \n    margin = ArcMarginProduct(\n            n_classes = N_CLASSES, \n            s = 30, \n            m = 0.7, \n            name='head/arc_margin', \n            dtype='float32'\n            )\n\n    inp = tf.keras.layers.Input(shape = (*IMAGE_SIZE, 3), name = 'inp1')\n    label = tf.keras.layers.Input(shape = (), name = 'inp2')\n    x = efn.EfficientNetB3(weights = None, include_top = False)(inp)\n    x = tf.keras.layers.GlobalAveragePooling2D()(x)\n    x = margin([x, label])\n        \n    output = tf.keras.layers.Softmax(dtype='float32')(x)\n\n    model = tf.keras.models.Model(inputs = [inp, label], outputs = [output])\n    model.load_weights('../input/b3-try/EfficientNetB3_512_42.h5')\n    model = tf.keras.models.Model(inputs = model.input[0], outputs = model.layers[-4].output)\n    chunk = 5000\n    iterator = np.arange(np.ceil(len(df) / chunk))\n    for j in iterator:\n        a = int(j * chunk)\n        b = int((j + 1) * chunk)\n        image_dataset = get_dataset(image_paths[a:b])\n        image_embeddings = model.predict(image_dataset)\n        embeds.append(image_embeddings)\n    del model\n    image_embeddings = np.concatenate(embeds)\n    print(f'Our image embeddings shape is {image_embeddings.shape}')\n    del embeds\n    gc.collect()\n    return image_embeddings","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_distances(df, embeddings, KNN = 50):\n    if len(pd.read_csv('../input/shopee-product-matching/test.csv'))<4 and GET_CV==False:\n        KNN=3\n    model = NearestNeighbors(n_neighbors = KNN)\n    model.fit(embeddings)\n    distances, indices = model.kneighbors(embeddings)\n    del model\n    _=gc.collect()\n    return distances, indices","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n    \n\ndf, df_cu, image_paths = read_dataset()\nimage_embeddings = get_image_embeddings(image_paths)\n# text_embeddings = get_text_embeddings(df)\ngc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image_distances, image_indices=get_distances(df, image_embeddings, KNN = 50)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image_embeddings.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# eca-nfnet-l0 ","metadata":{}},{"cell_type":"code","source":"\nsys.path.append('../input/timm-pytorch-image-models/pytorch-image-models-master')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\nimport math\n\nimport cv2\nimport timm\n\nfrom tqdm import tqdm \n\nimport albumentations as A \nfrom albumentations.pytorch.transforms import ToTensorV2\n\nimport torch \nfrom torch.utils.data import Dataset \nfrom torch import nn\nimport torch.nn.functional as F \n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CFG:\n    \n    img_size = 512\n    batch_size = 12\n    seed = 2020\n    \n    device = 'cuda'\n    classes = 11014\n    \n    model_name = 'eca_nfnet_l0'\n    model_path = '../input/shopee-pytorch-models/arcface_512x512_nfnet_l0 (mish).pt'\n    \n    scale = 30 \n    margin = 0.5\n\n    if GET_CV:\n        df = pd.read_csv('../input/shopee-product-matching/train.csv')\n        tmp = df.groupby(['label_group'])['posting_id'].unique().to_dict()\n        df['matches'] = df['label_group'].map(tmp)\n        df['matches'] = df['matches'].apply(lambda x: ' '.join(x))\n        if CHECK_SUB:\n            df = pd.concat([df, df], axis = 0)\n            df.reset_index(drop = True, inplace = True)\n        df_cu = cudf.DataFrame(df)\n        image_paths = '../input/shopee-product-matching/train_images/' + df['image']\n    else:\n        df = pd.read_csv('../input/shopee-product-matching/test.csv')\n        df_cu = cudf.DataFrame(df)\n        image_paths = '../input/shopee-product-matching/test_images/' + df['image']\n        \n\ndef read_dataset():\n    if GET_CV:\n        df = pd.read_csv('../input/shopee-product-matching/train.csv')\n        tmp = df.groupby(['label_group'])['posting_id'].unique().to_dict()\n        df['matches'] = df['label_group'].map(tmp)\n        df['matches'] = df['matches'].apply(lambda x: ' '.join(x))\n        if CHECK_SUB:\n            df = pd.concat([df, df], axis = 0)\n            df.reset_index(drop = True, inplace = True)\n        df_cu = cudf.DataFrame(df)\n        image_paths = '../input/shopee-product-matching/train_images/' + df['image']\n    else:\n        df = pd.read_csv('../input/shopee-product-matching/test.csv')\n        df_cu = cudf.DataFrame(df)\n        image_paths = '../input/shopee-product-matching/test_images/' + df['image']\n        \n    return df, df_cu, image_paths\n\ndef seed_torch(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    \nseed_torch(CFG.seed)\n\n\n# def get_image_predictions(df, embeddings,threshold = 0.0):\n    \n#     if len(df) > 3:\n#         KNN = 50\n#     else : \n#         KNN = 3\n    \n#     model = NearestNeighbors(n_neighbors = KNN, metric = 'cosine')\n#     model.fit(embeddings)\n#     distances, indices = model.kneighbors(embeddings)\n    \n#     predictions = []\n#     for k in tqdm(range(embeddings.shape[0])):\n#         idx = np.where(distances[k,] < threshold)[0]\n#         ids = indices[k,idx]\n#         posting_ids = df['posting_id'].iloc[ids].values\n#         predictions.append(posting_ids)\n        \n#     del model, distances, indices\n#     gc.collect()\n#     return predictions\n\n\ndef get_test_transforms():\n\n    return A.Compose(\n        [\n            A.Resize(CFG.img_size,CFG.img_size,always_apply=True),\n            A.Normalize(),\n        ToTensorV2(p=1.0)\n        ]\n    )\n\n\nclass ShopeeDataset(Dataset):\n    def __init__(self, image_paths, transforms=None):\n\n        self.image_paths = image_paths\n        self.augmentations = transforms\n\n    def __len__(self):\n        return self.image_paths.shape[0]\n\n    def __getitem__(self, index):\n        image_path = self.image_paths[index]\n        \n        image = cv2.imread(image_path)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        \n        if self.augmentations:\n            augmented = self.augmentations(image=image)\n            image = augmented['image']       \n    \n        return image,torch.tensor(1)\n    \n    \n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ArcMarginProduct(nn.Module):\n    def __init__(self, in_features, out_features, scale=30.0, margin=0.50, easy_margin=False, ls_eps=0.0):\n        super(ArcMarginProduct, self).__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.scale = scale\n        self.margin = margin\n        self.ls_eps = ls_eps  # label smoothing\n        self.weight = nn.Parameter(torch.FloatTensor(out_features, in_features))\n        nn.init.xavier_uniform_(self.weight)\n\n        self.easy_margin = easy_margin\n        self.cos_m = math.cos(margin)\n        self.sin_m = math.sin(margin)\n        self.th = math.cos(math.pi - margin)\n        self.mm = math.sin(math.pi - margin) * margin\n\n    def forward(self, input, label):\n        # --------------------------- cos(theta) & phi(theta) ---------------------------\n        cosine = F.linear(F.normalize(input), F.normalize(self.weight))\n        sine = torch.sqrt(1.0 - torch.pow(cosine, 2))\n        phi = cosine * self.cos_m - sine * self.sin_m\n        if self.easy_margin:\n            phi = torch.where(cosine > 0, phi, cosine)\n        else:\n            phi = torch.where(cosine > self.th, phi, cosine - self.mm)\n        # --------------------------- convert label to one-hot ---------------------------\n        # one_hot = torch.zeros(cosine.size(), requires_grad=True, device='cuda')\n        one_hot = torch.zeros(cosine.size(), device='cuda')\n        one_hot.scatter_(1, label.view(-1, 1).long(), 1)\n        if self.ls_eps > 0:\n            one_hot = (1 - self.ls_eps) * one_hot + self.ls_eps / self.out_features\n        # -------------torch.where(out_i = {x_i if condition_i else y_i) -------------\n        output = (one_hot * phi) + ((1.0 - one_hot) * cosine)\n        output *= self.scale\n\n        return output\n\nclass ShopeeModel(nn.Module):\n\n    def __init__(\n        self,\n        n_classes = CFG.classes,\n        model_name = CFG.model_name,\n        fc_dim = 512,\n        margin = CFG.margin,\n        scale = CFG.scale,\n        use_fc = True,\n        pretrained = False):\n\n\n        super(ShopeeModel,self).__init__()\n        print('Building Model Backbone for {} model'.format(model_name))\n\n        self.backbone = timm.create_model(model_name, pretrained=pretrained)\n\n        if model_name == 'resnext50_32x4d':\n            final_in_features = self.backbone.fc.in_features\n            self.backbone.fc = nn.Identity()\n            self.backbone.global_pool = nn.Identity()\n\n        elif model_name == 'efficientnet_b3':\n            final_in_features = self.backbone.classifier.in_features\n            self.backbone.classifier = nn.Identity()\n            self.backbone.global_pool = nn.Identity()\n\n        elif model_name == 'tf_efficientnet_b5_ns':\n            final_in_features = self.backbone.classifier.in_features\n            self.backbone.classifier = nn.Identity()\n            self.backbone.global_pool = nn.Identity()\n        \n        elif model_name == 'eca_nfnet_l0':\n            final_in_features = self.backbone.head.fc.in_features\n            self.backbone.head.fc = nn.Identity()\n            self.backbone.head.global_pool = nn.Identity()\n\n        self.pooling =  nn.AdaptiveAvgPool2d(1)\n\n        self.use_fc = use_fc\n\n        self.dropout = nn.Dropout(p=0.0)\n        self.fc = nn.Linear(final_in_features, fc_dim)\n        self.bn = nn.BatchNorm1d(fc_dim)\n        self._init_params()\n        final_in_features = fc_dim\n\n        self.final = ArcMarginProduct(\n            final_in_features,\n            n_classes,\n            scale = scale,\n            margin = margin,\n            easy_margin = False,\n            ls_eps = 0.0\n        )\n\n    def _init_params(self):\n        nn.init.xavier_normal_(self.fc.weight)\n        nn.init.constant_(self.fc.bias, 0)\n        nn.init.constant_(self.bn.weight, 1)\n        nn.init.constant_(self.bn.bias, 0)\n\n    def forward(self, image, label):\n        feature = self.extract_feat(image)\n        #logits = self.final(feature,label)\n        return feature\n\n    def extract_feat(self, x):\n        batch_size = x.shape[0]\n        x = self.backbone(x)\n        x = self.pooling(x).view(batch_size, -1)\n\n        if self.use_fc:\n            x = self.dropout(x)\n            x = self.fc(x)\n            x = self.bn(x)\n        return x\n    \n    \nclass Mish_func(torch.autograd.Function):\n    \n    \"\"\"from: https://github.com/tyunist/memory_efficient_mish_swish/blob/master/mish.py\"\"\"\n    \n    @staticmethod\n    def forward(ctx, i):\n        result = i * torch.tanh(F.softplus(i))\n        ctx.save_for_backward(i)\n        return result\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        i = ctx.saved_variables[0]\n  \n        v = 1. + i.exp()\n        h = v.log() \n        grad_gh = 1./h.cosh().pow_(2) \n\n        # Note that grad_hv * grad_vx = sigmoid(x)\n        #grad_hv = 1./v  \n        #grad_vx = i.exp()\n        \n        grad_hx = i.sigmoid()\n\n        grad_gx = grad_gh *  grad_hx #grad_hv * grad_vx \n        \n        grad_f =  torch.tanh(F.softplus(i)) + i * grad_gx \n        \n        return grad_output * grad_f \n\n\nclass Mish(nn.Module):\n    def __init__(self, **kwargs):\n        super().__init__()\n        pass\n    def forward(self, input_tensor):\n        return Mish_func.apply(input_tensor)\n\n\ndef replace_activations(model, existing_layer, new_layer):\n    \n    \"\"\"A function for replacing existing activation layers\"\"\"\n    \n    for name, module in reversed(model._modules.items()):\n        if len(list(module.children())) > 0:\n            model._modules[name] = replace_activations(module, existing_layer, new_layer)\n\n        if type(module) == existing_layer:\n            layer_old = module\n            layer_new = new_layer\n            model._modules[name] = layer_new\n    return model\n\n\ndef get_image_embeddings(image_paths, model_name = CFG.model_name):\n    embeds = []\n    \n    model = ShopeeModel(model_name = model_name)\n    model.eval()\n    \n    if model_name == 'eca_nfnet_l0':\n        model = replace_activations(model, torch.nn.SiLU, Mish())\n\n    model.load_state_dict(torch.load(CFG.model_path))\n    model = model.to(CFG.device)\n    \n\n    image_dataset = ShopeeDataset(image_paths=image_paths,transforms=get_test_transforms())\n    image_loader = torch.utils.data.DataLoader(\n        image_dataset,\n        batch_size=CFG.batch_size,\n        pin_memory=True,\n        drop_last=False,\n        num_workers=4\n    )\n    \n    \n    with torch.no_grad():\n        for img,label in tqdm(image_loader): \n            img = img.cuda()\n            label = label.cuda()\n            feat = model(img,label)\n            image_embeddings = feat.detach().cpu().numpy()\n            embeds.append(image_embeddings)\n    \n    \n    del model\n    image_embeddings = np.concatenate(embeds)\n    print(f'Our image embeddings shape is {image_embeddings.shape}')\n    del embeds\n    gc.collect()\n    return image_embeddings","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df,df_cu,image_paths = read_dataset()\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image_embeddings1 = get_image_embeddings(image_paths.values)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_distances_cosine(df, embeddings, KNN = 50):\n    if len(pd.read_csv('../input/shopee-product-matching/test.csv'))<4 and GET_CV==False:\n        KNN=3\n    model = NearestNeighbors(n_neighbors = KNN, metric = 'cosine')\n    model.fit(embeddings)\n    distances, indices = model.kneighbors(embeddings)\n    del model\n    _=gc.collect()\n    return distances, indices","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image_distances1, image_indices1=get_distances_cosine(df, image_embeddings1, KNN = 50)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# image_embeddings_tot=np.concatenate((image_embeddings1,image_embeddings),axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def get_image_predictions(df, embeddings,threshold0 = 0.0):\n    \n#     if len(df) > 3:\n#         KNN = 50\n#     else : \n#         KNN = 3\n\n#     model = NearestNeighbors(n_neighbors = KNN, metric = 'cosine')\n#     model.fit(embeddings)\n#     distances, indices = model.kneighbors(embeddings)\n    \n    \n#     if GET_CV:\n\n#         thresholds = list(np.arange(0.2, 0.4, 0.01))\n\n#         scores = []\n#         for threshold in thresholds:\n#             predictions = []\n#             for k in range(embeddings.shape[0]):\n#                 idx = np.where(distances[k,] < threshold)[0]\n#                 ids = indices[k,idx]\n#                 posting_ids = ' '.join(df['posting_id'].iloc[ids].values)\n#                 predictions.append(posting_ids)\n#             df['pred_matches'] = predictions\n#             df['f1'] = f1_score(df['matches'], df['pred_matches'])\n#             score = df['f1'].mean()\n#             print(f'Our f1 score for threshold {threshold} is {score}')\n#             scores.append(score)\n#         thresholds_scores = pd.DataFrame({'thresholds': thresholds, 'scores': scores})\n#         max_score = thresholds_scores[thresholds_scores['scores'] == thresholds_scores['scores'].max()]\n#         best_threshold = max_score['thresholds'].values[0]\n#         best_score = max_score['scores'].values[0]\n#         print(f'Our best score is {best_score} and has a threshold {best_threshold}')\n        \n        \n#     predictions = []\n#     for k in tqdm(range(embeddings.shape[0])):\n#         idx = np.where(distances[k,] < threshold0)[0]\n#         ids = indices[k,idx]\n#         posting_ids = df['posting_id'].iloc[ids].values\n#         predictions.append(posting_ids)\n        \n#     del model, distances, indices\n#     gc.collect()\n#     return predictions","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# image_predictions1 = get_image_predictions(df, image_embeddings_tot, threshold0 = 0.36)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Test Configuration","metadata":{}},{"cell_type":"code","source":"NUM_WORKERS = 4\nBATCH_SIZE = 16\nSEED = 42\n\ndevice = torch.device('cuda')\n\n################################################  ADJUSTING FOR CV OR SUBMIT ##############################################\n\n\n\ntest = pd.read_csv('../input/shopee-product-matching/test.csv')\nif len(test)>3: GET_CV = False\nelse: print('this submission notebook will compute CV score, but commit notebook will not')\n\n\n################################################# MODEL ####################################################################\n\ntransformer_model = '../input/sentence-transformer-models/paraphrase-xlm-r-multilingual-v1/0_Transformer'\nTOKENIZER = transformers.AutoTokenizer.from_pretrained(transformer_model)\n\n################################################ MODEL PATH ###############################################################\n\nTEXT_MODEL_PATH = '../input/only-text-sbert-with-dict/sentence_transfomer_xlm_best_loss_num_epochs_20_arcface.bin'\n\nmodel_params = {\n    'n_classes':11014,\n    'model_name':transformer_model,\n    'use_fc':False,\n    'fc_dim':512,\n    'dropout':0.3,\n}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"DICT = {}\nfile = open('../input/indonesean-english-dicttxt/indonesean_english_dict.txt','r')\nfor line in file.readlines():\n\n    k = line.split(' ')[0][2:-2]\n    v = line.split(' ')[1][1:-3]\n\n\n    DICT[str(k).lower()] = str(v).lower()\nfile.close()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Reading Data","metadata":{}},{"cell_type":"code","source":"def read_dataset():\n    if GET_CV:\n        df = pd.read_csv('../input/shopee-product-matching/train.csv')\n        df[\"title\"]=df[\"title\"].str.lower()\n\n        for key in list(DICT.keys()):\n            df['title'] = df['title'].str.replace(key,DICT[key])\n        tmp = df.groupby(['label_group'])['posting_id'].unique().to_dict()\n        df['matches'] = df['label_group'].map(tmp)\n        df['matches'] = df['matches'].apply(lambda x: ' '.join(x))\n        if CHECK_SUB:\n            df = pd.concat([df, df], axis = 0)\n            df.reset_index(drop = True, inplace = True)\n        df_cu = cudf.DataFrame(df)\n    else:\n        df = pd.read_csv('../input/shopee-product-matching/test.csv')\n        df[\"title\"]=df[\"title\"].str.lower()\n\n        for key in list(DICT.keys()):\n            df['title'] = df['title'].str.replace(key,DICT[key])\n        df_cu = cudf.DataFrame(df)\n        \n    return df, df_cu","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Utils","metadata":{}},{"cell_type":"code","source":"def seed_torch(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    \nseed_torch(SEED)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def f1_score(y_true, y_pred):\n    y_true = y_true.apply(lambda x: set(x.split()))\n    y_pred = y_pred.apply(lambda x: set(x.split()))\n    intersection = np.array([len(x[0] & x[1]) for x in zip(y_true, y_pred)])\n    len_y_pred = y_pred.apply(lambda x: len(x)).values\n    len_y_true = y_true.apply(lambda x: len(x)).values\n    f1 = 2 * intersection / (len_y_pred + len_y_true)\n    return f1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Generating Embeddings","metadata":{}},{"cell_type":"code","source":"class ShopeeDataset(Dataset):\n    def __init__(self, csv):\n        self.csv = csv.reset_index()\n\n    def __len__(self):\n        return self.csv.shape[0]\n\n    def __getitem__(self, index):\n        row = self.csv.iloc[index]\n        \n        text = row.title\n        \n        text = TOKENIZER(text, padding='max_length', truncation=True, max_length=128, return_tensors=\"pt\")\n        input_ids = text['input_ids'][0]\n        attention_mask = text['attention_mask'][0]  \n        \n        return input_ids, attention_mask","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ShopeeNet(nn.Module):\n\n    def __init__(self,\n                 n_classes,\n                 model_name='bert-base-uncased',\n                 use_fc=False,\n                 fc_dim=512,\n                 dropout=0.0):\n        \"\"\"\n        :param n_classes:\n        :param model_name: name of model from pretrainedmodels\n            e.g. resnet50, resnext101_32x4d, pnasnet5large\n        :param pooling: One of ('SPoC', 'MAC', 'RMAC', 'GeM', 'Rpool', 'Flatten', 'CompactBilinearPooling')\n        :param loss_module: One of ('arcface', 'cosface', 'softmax')\n        \"\"\"\n        super(ShopeeNet, self).__init__()\n\n        self.transformer = transformers.AutoModel.from_pretrained(model_name)\n        final_in_features = self.transformer.config.hidden_size\n        \n        self.use_fc = use_fc\n    \n        if use_fc:\n            self.dropout = nn.Dropout(p=dropout)\n            self.fc = nn.Linear(final_in_features, fc_dim)\n            self.bn = nn.BatchNorm1d(fc_dim)\n            self._init_params()\n            final_in_features = fc_dim\n\n\n    def _init_params(self):\n        nn.init.xavier_normal_(self.fc.weight)\n        nn.init.constant_(self.fc.bias, 0)\n        nn.init.constant_(self.bn.weight, 1)\n        nn.init.constant_(self.bn.bias, 0)\n\n    def forward(self, input_ids,attention_mask):\n        feature = self.extract_feat(input_ids,attention_mask)\n        return F.normalize(feature)\n\n    def extract_feat(self, input_ids,attention_mask):\n        x = self.transformer(input_ids=input_ids,attention_mask=attention_mask)\n        \n        features = x[0]\n        features = features[:,0,:]\n\n        if self.use_fc:\n            features = self.dropout(features)\n            features = self.fc(features)\n            features = self.bn(features)\n\n        return features","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Generating Submission","metadata":{}},{"cell_type":"code","source":"def get_text_embeddings(df):\n    embeds = []\n    \n    model = ShopeeNet(**model_params)\n    model.eval()\n    \n    model.load_state_dict(dict(list(torch.load(TEXT_MODEL_PATH).items())[:-1]))\n    model = model.to(device)\n\n    text_dataset = ShopeeDataset(df)\n    text_loader = torch.utils.data.DataLoader(\n        text_dataset,\n        batch_size=BATCH_SIZE,\n        pin_memory=True,\n        drop_last=False,\n        num_workers=NUM_WORKERS\n    )\n    \n    \n    with torch.no_grad():\n        for input_ids, attention_mask in tqdm(text_loader): \n            input_ids = input_ids.cuda()\n            attention_mask = attention_mask.cuda()\n            feat = model(input_ids, attention_mask)\n            text_embeddings = feat.detach().cpu().numpy()\n            embeds.append(text_embeddings)\n    \n    \n    del model\n    text_embeddings = np.concatenate(embeds)\n    print(f'Our text embeddings shape is {text_embeddings.shape}')\n    del embeds\n    gc.collect()\n    return text_embeddings","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df,df_cu = read_dataset()\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"text_embeddings1 = get_text_embeddings(df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_text_distances(df, embeddings, KNN = 50):\n    if len(pd.read_csv('../input/shopee-product-matching/test.csv'))<4 and GET_CV==False:\n        KNN=3\n    model = NearestNeighbors(n_neighbors = KNN,metric='cosine')\n    model.fit(embeddings)\n    distances, indices = model.kneighbors(embeddings)\n    del model\n    _=gc.collect()\n    return distances, indices","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"text_distances1, text_indices1=get_text_distances(df, text_embeddings1, KNN = 50)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Tfidf","metadata":{}},{"cell_type":"code","source":"def read_dataset():\n    if GET_CV:\n        df = pd.read_csv('../input/shopee-product-matching/train.csv')\n        df[\"title\"]=df[\"title\"].str.lower()\n\n#         for key in list(DICT.keys()):\n#             df['title'] = df['title'].str.replace(key,DICT[key])\n        tmp = df.groupby(['label_group'])['posting_id'].unique().to_dict()\n        df['matches'] = df['label_group'].map(tmp)\n        df['matches'] = df['matches'].apply(lambda x: ' '.join(x))\n        if CHECK_SUB:\n            df = pd.concat([df, df], axis = 0)\n            df.reset_index(drop = True, inplace = True)\n        df_cu = cudf.DataFrame(df)\n    else:\n        df = pd.read_csv('../input/shopee-product-matching/test.csv')\n        df[\"title\"]=df[\"title\"].str.lower()\n\n#         for key in list(DICT.keys()):\n#             df['title'] = df['title'].str.replace(key,DICT[key])\n        df_cu = cudf.DataFrame(df)\n        \n    return df, df_cu\n\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df, df_cu = read_dataset()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = TfidfVectorizer(stop_words='english',\n                        binary=True)\ntext_embeddings = model.fit_transform(df_cu['title'])\ndel model\ngc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"KNN=50\nif len(pd.read_csv('../input/shopee-product-matching/test.csv'))<4 and GET_CV==False:\n    KNN=3\nmodel = NearestNeighbors(n_neighbors = KNN,metric='cosine')\nmodel.fit(text_embeddings)\ndistances, indices = model.kneighbors(text_embeddings)\ndel model\ngc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_text_distances(df, embeddings, KNN = 50):\n    if len(pd.read_csv('../input/shopee-product-matching/test.csv'))<4 and GET_CV==False:\n        KNN=3\n    model = NearestNeighbors(n_neighbors = KNN,metric='cosine')\n    model.fit(embeddings)\n    distances, indices = model.kneighbors(embeddings)\n    del model\n    _=gc.collect()\n    return distances, indices","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"text_distances, text_indices=get_text_distances(df, text_embeddings, KNN = 50)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"text_distances.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_neighbors(df,  image_distances, image_indices, image_distances1, image_indices1, text_distances, text_indices,text_distances1, text_indices1):\n\n    # Iterate through different thresholds to maximize cv, run this in interactive mode, then replace else clause with a solid threshold\n    if GET_CV:\n        predictions0 = []\n        predictions = []\n        for k in range(image_distances.shape[0]):\n                image_idx = np.where(image_distances[k,] <3.0)[0]\n                image_ids = image_indices[k,image_idx]\n                image_set=set(image_ids.tolist())\n\n                image_idx = np.where(image_distances1[k,] <0.27)[0]\n                image_ids = image_indices1[k,image_idx]\n                image1_set=set(image_ids.tolist())\n\n                image_idx = np.where((image_distances[k,] <4.0)&(image_distances[k,] >=3.4))[0]\n                image_ids = image_indices[k,image_idx]\n                image2_set=set(image_ids.tolist())\n\n                image_idx = np.where((image_distances1[k,] <0.33)&(image_distances1[k,] >=0.27))[0]\n                image_ids = image_indices1[k,image_idx]\n                image3_set=set(image_ids.tolist())\n\n                image_idx = np.where((image_distances[k,] <3.4)&(image_distances[k,] >=3.0))[0]\n                image_ids = image_indices[k,image_idx]\n                image4_set=set(image_ids.tolist()) \n                \n                image_idx = np.where(image_distances1[k,] <0.5)[0]\n                image_ids = image_indices1[k,image_idx]\n                image5_set=set(image_ids.tolist())\n                \n\n                text_idx = np.where(text_distances1[k,] <=0.13)[0]\n                text_ids = text_indices1[k,text_idx]\n                text_set0=set(text_ids.tolist())\n\n                text_idx = cupy.where(text_distances[k,] <=0.2)[0]\n                text_ids = cupy.asnumpy(text_indices[k,text_idx])\n                text_set=set(text_ids.tolist())\n\n                text_idx = cupy.where((text_distances[k,]>0.2)&(text_distances[k,]<0.38))[0]\n                text_ids = cupy.asnumpy(text_indices[k,text_idx])\n                text_set1=set(text_ids.tolist())\n\n                text_idx = cupy.where((text_distances[k,]>0.2)&(text_distances[k,]<0.5))[0]\n                text_ids = cupy.asnumpy(text_indices[k,text_idx])\n                text_set2=set(text_ids.tolist())\n\n                text_idx = np.where(text_distances1[k,] <=0.5)[0]\n                text_ids = text_indices1[k,text_idx]\n                text_set3=set(text_ids.tolist())\n\n                image2_set=(image2_set.union(image3_set)).intersection(image5_set)\n                set1=image2_set.intersection(text_set1.intersection(text_set3))\n\n                set2=image4_set.intersection(text_set2.union(text_set3))\n\n                total_set=image_set.union(text_set,image1_set,set1,set2,text_set0)\n\n                tot_idx=np.array(sorted(list(total_set)))\n\n                posting_ids = df['posting_id'].iloc[tot_idx].values\n                predictions.append(posting_ids)\n\n                posting0_ids = ' '.join(df['posting_id'].iloc[tot_idx].values)\n                predictions0.append(posting0_ids)\n                \n                \n        df['pred_matches'] = predictions0\n        df['f1'] = f1_score(df['matches'], df['pred_matches'])\n        score = df['f1'].mean()\n        print(f'Our f1 score for threshold 3.0 is {score}')\n        \n\n\n        \n\n    # Because we are predicting the test set that have 70K images and different label groups, confidence should be smaller\n    else:\n        predictions = []\n        for k in range(image_distances.shape[0]):\n                image_idx = np.where(image_distances[k,] <3.0)[0]\n                image_ids = image_indices[k,image_idx]\n                image_set=set(image_ids.tolist())\n                \n                image_idx = np.where(image_distances1[k,] <0.27)[0]\n                image_ids = image_indices1[k,image_idx]\n                image1_set=set(image_ids.tolist())\n                \n                image_idx = np.where((image_distances[k,] <4.0)&(image_distances[k,] >=3.4))[0]\n                image_ids = image_indices[k,image_idx]\n                image2_set=set(image_ids.tolist())\n                \n                image_idx = np.where((image_distances1[k,] <0.33)&(image_distances1[k,] >=0.27))[0]\n                image_ids = image_indices1[k,image_idx]\n                image3_set=set(image_ids.tolist())\n                                \n                image_idx = np.where((image_distances[k,] <3.4)&(image_distances[k,] >=3.0))[0]\n                image_ids = image_indices[k,image_idx]\n                image4_set=set(image_ids.tolist())                \n                \n                                \n                image_idx = np.where(image_distances1[k,] <0.5)[0]\n                image_ids = image_indices1[k,image_idx]\n                image5_set=set(image_ids.tolist())\n                \n                text_idx = np.where(text_distances1[k,] <=0.13)[0]\n                text_ids = text_indices1[k,text_idx]\n                text_set0=set(text_ids.tolist())\n               \n                text_idx = cupy.where(text_distances[k,] <=0.2)[0]\n                text_ids = cupy.asnumpy(text_indices[k,text_idx])\n                text_set=set(text_ids.tolist())\n                \n                text_idx = cupy.where((text_distances[k,]>0.2)&(text_distances[k,]<0.38))[0]\n                text_ids = cupy.asnumpy(text_indices[k,text_idx])\n                text_set1=set(text_ids.tolist())\n                \n                text_idx = cupy.where((text_distances[k,]>0.2)&(text_distances[k,]<0.5))[0]\n                text_ids = cupy.asnumpy(text_indices[k,text_idx])\n                text_set2=set(text_ids.tolist())\n                \n                text_idx = np.where(text_distances1[k,] <=0.5)[0]\n                text_ids = text_indices1[k,text_idx]\n                text_set3=set(text_ids.tolist())\n                \n                image2_set=(image2_set.union(image3_set)).intersection(image5_set)\n                set1=image2_set.intersection(text_set1.intersection(text_set3))\n                \n                set2=image4_set.intersection(text_set2.union(text_set3))\n                \n                total_set=image_set.union(text_set,image1_set,set1,set2,text_set0)\n\n                tot_idx=np.array(sorted(list(total_set)))\n                \n                posting_ids = df['posting_id'].iloc[tot_idx].values\n                predictions.append(posting_ids)\n        \n    #del distances, indices\n    gc.collect()\n    return df, predictions","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":" df,image_text_predictions=get_neighbors(df,  image_distances, image_indices, image_distances1, image_indices1, text_distances, text_indices,text_distances1, text_indices1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def combine_predictions(row):\n    x = row['image_text_predictions']\n\n    return ' '.join( np.unique(x) )\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if  GET_CV:\n    df[\"image_text_predictions\"]=image_text_predictions\n    \n    \n\n    df['pred_matches'] = df.apply(combine_predictions, axis=1)\n    df['f1'] = f1_score(df['matches'], df['pred_matches'])\n    score = df['f1'].mean()\n    print(f'Our final f1 cv score is {score}')\n    df['matches0'] = df['pred_matches']\n    df[['posting_id','matches0']].to_csv('submission.csv',index=False)\nelse:\n    df[\"image_text_predictions\"]=image_text_predictions\n    \n    \n\n    df['matches'] = df.apply(combine_predictions, axis=1)\n\n    \n    df[['posting_id','matches']].to_csv('submission.csv',index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}