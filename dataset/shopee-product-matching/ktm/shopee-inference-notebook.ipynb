{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import sys\nsys.path.append(\"../input/timm-pytorch-image-models/pytorch-image-models-master\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport cv2\nimport glob\nimport gc\nfrom tqdm.auto import tqdm\nimport os\nimport sys\nimport time\nimport random\nimport math\nimport re\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\nfrom albumentations.core.transforms_interface import ImageOnlyTransform\nimport timm\n\nfrom sklearn.preprocessing import normalize\nfrom sklearn.decomposition import PCA, TruncatedSVD\nimport cupy\nimport cuml\nimport cudf\nfrom cuml.feature_extraction.text import TfidfVectorizer\nfrom cuml.neighbors import NearestNeighbors\n# from cuml.decomposition import PCA, TruncatedSVD\nfrom cuml.manifold import UMAP\n\nfrom transformers import AutoTokenizer, AutoModel, BertTokenizer, BertModel, DistilBertTokenizer, DistilBertModel\nimport transformers\n\n\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\n\nfrom gensim.corpora import Dictionary\nfrom gensim.models import Word2Vec\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CFG:\n    debug = False\n    check_ram = False\n    calc_cv = True\n    \n    # phash\n    n_components = 32\n    \n    # tfidf\n    max_features = 25000\n    tfidf_thresh = 0.75\n    \n    # wrod2vec\n    epochs = 100\n    vector_size = 512  # embedding size\n    window = 5\n    \n    \n    # image models\n    \n    image_model_name1 = \"eca_nfnet_l0\"\n    size1 = 512\n    image_pretrained_path1 = \"../input/shopee-pretrained/nfnet_5/arcface_512x512_nfnet_l0(mish).pt\"\n    image_loss_module1 = \"curricularface\"\n#     pca_components = 128\n    \n    \n    image_model_name2 = \"swin_small_patch4_window7_224\"\n    size2 = 224\n    image_pretrained_path2 = \"../input/shopee-pretrained/swin1/curricularface_224x224_vit_base_patch16_224.pt\"\n    image_loss_module2 = \"curricularface\"\n  \n    \n    image_model_name3 = \"vit_base_patch16_224\"\n    size3 = 224\n    image_pretrained_path3 = \"../input/shopee-pretrained/vit1/curricularface_224x224_vit_base_patch16_224.pt\"\n    image_loss_module3 = \"curricularface\"\n    \n    \n    # bert models\n    transformer_model1 = '../input/sentence-transformer-models/paraphrase-xlm-r-multilingual-v1/0_Transformer'\n    max_length1 = 64\n    bert_pretrained_path1 = \"../input/shopee-pretrained/arcface_sbert3/sentence_transfomer_xlm_best_loss_num_epochs_30_arcface.pth\"\n    bert_loss_module1 = \"arcface\"\n    \n    transformer_model2 = \"../input/distilbert-base-indonesian\"\n    max_length2 = 32\n    bert_pretrained_path2 = \"../input/shopee-pretrained/distilbert2/distilbert_curricularface_30_.pth\"\n    bert_loss_module2 = \"curricularface\"\n    \n    # others\n    n_neighbors = 50\n    \n    thresh = 0.36\n    \n    \n    classes = 11014\n    scale = 30\n    margin = 0.5\n    fc_dim = 512\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"PATH = \"../input/shopee-product-matching/\"\n\ntrain_df = pd.read_csv(PATH + \"train.csv\")\n# train_df = pd.read_csv(\"../input/shopee-cv-splitting-way/train_folds.csv\")\n# train_df = pd.read_csv(\"../input/shopee-cv-folds/train_label_group_5folds.csv\")\ntest_df = pd.read_csv(PATH + \"test.csv\")\nsample_submission = pd.read_csv(PATH + \"sample_submission.csv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df[\"image\"] = train_df[\"image\"].apply(lambda x: PATH + \"train_images/\" + x)\n# if not CFG.check_ram:\ntest_df[\"image\"] = test_df[\"image\"].apply(lambda x: PATH + \"test_images/\" + x)\n\ntmp = train_df.groupby('label_group').posting_id.agg('unique').to_dict()\ntrain_df['target'] = train_df.label_group.map(tmp)\n\nif len(test_df) != 3:\n    CFG.calc_cv = False\n\nif CFG.check_ram and (len(test_df)==3):\n    test_df = pd.concat([train_df, train_df], axis=0).reset_index()\n    \nif CFG.calc_cv:\n    test_df = train_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def seed_everything(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\nseed_everything(42)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def getMetric(col):\n    def f1score(row):\n        n = len( np.intersect1d(row.target, row[col]) )\n        fp = len(np.setdiff1d(row[col], row.target))\n        fn = len(np.setdiff1d(row.target, row[col]))\n#         return fn\n        return 2 * n / (2 * n + fp + fn)\n#         return 2*n / (len(row.target)+len(row[col]))\n    return f1score\n\ndef get_score(df, col):\n    return df.apply(getMetric(col),axis=1).mean()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def predict(features, df, thresh=0.90, chunk=1024):\n    features = cupy.asarray(features)\n    pred = []\n    CHUNK = chunk\n    CTS = len(features) // CHUNK\n    if len(features)%CHUNK!=0: CTS += 1\n    for j in range(CTS):\n\n        a = j*CHUNK\n        b = (j+1)*CHUNK\n        b = min(b,len(features))\n        print('chunk',a,'to',b)\n\n        # COSINE SIMILARITY DISTANCE\n#         cts = np.matmul(phash_vector, phash_vector[a:b].T).T\n        cts = cupy.matmul(features, features[a:b].T).T\n\n        for k in range(b-a):\n            IDX = cupy.where(cts[k,]>thresh)[0]\n            o = df.iloc[cupy.asnumpy(IDX)].posting_id.values\n            pred.append(o)\n        del cts\n    return pred\n\ndef get_neighbor_images(image_embeddings, df, chunk=4096, thresh=0.5, n_neighbors=50, show=False, metric=\"euclidean\"):\n    preds = []\n    CHUNK = chunk\n    model = NearestNeighbors(n_neighbors=n_neighbors, metric=metric)\n    model.fit(image_embeddings)\n\n    print('Finding similar images...')\n    CTS = len(image_embeddings)//CHUNK\n    if len(image_embeddings)%CHUNK!=0: CTS += 1\n    for j in range( CTS ):\n\n        a = j*CHUNK\n        b = (j+1)*CHUNK\n        b = min(b,len(image_embeddings))\n        print('chunk',a,'to',b)\n        distances, indices = model.kneighbors(image_embeddings[a:b,])\n        if (j == 0) and show:\n            show_barplot(distances, indices, df)\n\n        for k in range(b-a):\n            IDX = np.where(distances[k,]<thresh)[0]\n            IDS = indices[k,IDX]\n            o = df.iloc[IDS].posting_id.values\n            preds.append(o)\n            \n    return preds\n\ndef displayDF(train, random=False, COLS=6, ROWS=4, path=\"\"):\n    for k in range(ROWS):\n        plt.figure(figsize=(20,5))\n        for j in range(COLS):\n            if random: row = np.random.randint(0,len(train))\n            else: row = COLS*k + j\n            name = train[\"image\"].values[row]\n            title = train[\"title\"].values[row]\n            title_with_return = \"\"\n            for i,ch in enumerate(title):\n                title_with_return += ch\n                if (i!=0)&(i%20==0): title_with_return += '\\n'\n            img = cv2.imread(path+name)[:, :, ::-1]\n            plt.subplot(1,COLS,j+1)\n            plt.title(title_with_return)\n            plt.axis('off')\n            plt.imshow(img)\n        plt.show()\n        \ndef show_barplot(distances, indices,  df, num_show=8):\n    for k in range(num_show):\n        plt.figure(figsize=(20,3))\n        plt.plot(np.arange(50), cupy.asnumpy(distances[k,]),'o-')\n#         plt.plot(np.arange(50), distances[k,],'o-')\n        plt.title('Text Distance From Train Row %i to Other Train Rows'%k,size=16)\n        plt.ylabel('Distance to Train Row %i'%k,size=14)\n        plt.xlabel('Index Sorted by Distance to Train Row %i'%k,size=14)\n        plt.show()\n        \n        cluster = df.loc[cupy.asnumpy(indices[k,:8])]\n#         cluster = df.loc[indices[k,:8]]\n        displayDF(cluster, random=False, ROWS=2, COLS=4)\n        print( df.loc[cupy.asnumpy(indices[k,:8]), ['title','label_group']] )\n#         print( df.loc[indices[k,:10], ['title','label_group']] )","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def combine_for_sub(row, cols, count=1):\n    x = np.concatenate([row[col] for col in cols])\n    unique, counts = np.unique(x, return_counts=True)\n    return ' '.join(unique[counts>=count])\n\ndef combine_for_cv(row, cols, count=1):\n    x = np.concatenate([row[col] for col in cols])\n    unique, counts = np.unique(x, return_counts=True)\n    return unique[counts>=count]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# text preprocessing","metadata":{}},{"cell_type":"code","source":"def remove_emoji(title):\n    \"\"\"\n    title: str\n    example:\n    >>>title = '\\\\xe2\\\\x9d\\\\xa4 RATU \\\\xe2\\\\x9d\\\\xa4 MAYCREATE MOISTURIZING SPRAY'\n    >>>removed = remove_emoji(title)\n    >>>removed\n    ' RATU  MAYCREATE MOISTURIZING SPRAY'\n    \"\"\"\n    matches_spans = [m.span() for m in re.finditer(r\"\\\\x[0-9a-fA-F][0-9a-fA-F]\", title)]\n    spans = []\n    if len(matches_spans) > 0:\n        for i, (span1, span2) in enumerate(zip(matches_spans[:-1], matches_spans[1:])):\n            if i == 0:\n                spans.append([span1[0], -1])\n            if span1[1] != span2[0]:\n                spans[-1][1] = span1[1]\n                spans.append([span2[0], -1])\n        spans[-1][1] = matches_spans[-1][1]\n        emojis = []\n        for span in spans:\n            emojis.append(title[span[0]:span[1]])\n\n        for emj in emojis:\n            title = title.replace(emj, \"\")\n        return title\n    else:\n        return title\n    \ndef lower_title(title):\n    return title.lower()\n    \ndef unify_units(title):\n    title += \" \" # insert space in order to recognize the last units\n    \n    # gram, kg, mg\n    title = re.sub(r\"(([1-9]\\d*|0)(\\.\\d+)?)(\\s|)(gram|grm|gr\\s|g\\s)\", r\"\\1gram \", title)\n    title = re.sub(r\"(([1-9]\\d*|0)(\\.\\d+)?)(\\s|)kg\", r\"\\1kg \", title)\n    title = re.sub(r\"(([1-9]\\d*|0)(\\.\\d+)?)(\\s|)mg\", r\"\\1mg \", title)\n    \n    # L, ml\n    title = re.sub(r\"(([1-9]\\d*|0)(\\.\\d+)?)(\\s|)(liter|l\\s)\", r\"\\1liter \", title)\n    title = re.sub(r\"(([1-9]\\d*|0)(\\.\\d+)?)(\\s|)ml\", r\"\\1ml \", title)\n    \n    # m, cm, km, mm\n    title = re.sub(r\"(([1-9]\\d*|0)(\\.\\d+)?)(\\s|)(meter|m\\s)\", r\"\\1meter \", title)\n    title = re.sub(r\"(([1-9]\\d*|0)(\\.\\d+)?)(\\s|)cm\", r\"\\1cm \", title)\n    title = re.sub(r\"(([1-9]\\d*|0)(\\.\\d+)?)(\\s|)km\", r\"\\1km \", title)\n    title = re.sub(r\"(([1-9]\\d*|0)(\\.\\d+)?)(\\s|)mm\", r\"\\1mm \", title)\n    \n    # pcs, pieces\n    title = re.sub(r\"(([1-9]\\d*|0)(\\.\\d+)?)(\\s|)(pcs|pieces)\", r\"\\1pcs \", title)\n    \n    # watt\n    title = re.sub(r\"(([1-9]\\d*|0)(\\.\\d+)?)(\\s|)watt\", r\"\\1watt \", title)\n    \n    # A mA mAh\n#     title = re.sub(r\"(([1-9]\\d*|0)(\\.\\d+)?)(\\s|)(a)\", r\"\\1amper\", title)\n#     title = re.sub(r\"(([1-9]\\d*|0)(\\.\\d+)?)(\\s|)(ma)\", r\"\\1ma\", title)\n#     title = re.sub(r\"(([1-9]\\d*|0)(\\.\\d+)?)(\\s|)(mah)\", r\"\\1mha\", title)\n    \n    # GB MB TB\n    title = re.sub(r\"(([1-9]\\d*|0)(\\.\\d+)?)(\\s|)gb\", r\"\\1gb \", title)\n    title = re.sub(r\"(([1-9]\\d*|0)(\\.\\d+)?)(\\s|)mb\", r\"\\1mb \", title)\n    title = re.sub(r\"(([1-9]\\d*|0)(\\.\\d+)?)(\\s|)tb\", r\"\\1tb \", title)\n    \n    # ply\n    title = re.sub(r\"(([1-9]\\d*|0)(\\.\\d+)?)(\\s|)ply\", r\"\\1ply \", title)\n    \n    # inch\n    title = re.sub(r\"(([1-9]\\d*|0)(\\.\\d+)?)(\\s|)inch\", r\"\\1inch \", title)\n    \n    return title\n\ndef remove_letters(title):\n    title = re.sub('[!\"#$\\'\\\\\\\\()*:;<=>?@[\\\\]^_`{|}~「」〔〕“”〈〉『』【】＆＊・（）＄＃＠。、？！｀＋￥％]', \" \", title)\n    \n    # / and -\n    title = re.sub(r\"([a-z0-9])(-|/)([a-z])\", \"\\1 \\3\", title)\n    title = re.sub(r\"([a-z])(-|/)([a-z0-9])\", \"\\1 \\3\", title)\n    \n    title = re.sub(r\"\\s(-|/|\\+)\\s\", \" \", title)\n    \n    \n    # remove sequential space\n    title = title.strip()\n    title = re.sub(r\"\\s{2,}\", \" \", title)\n    \n    \n    # -\n#     title = re.sub(r\"([a-z0-9])/([a-z])\", \"\\1 \\2\", title)\n#     title = re.sub(r\"([a-z])/([a-z0-9])\", \"\\1 \\2\", title)\n    \n#     code_regex = re.compile('[!\"#$%&\\'\\\\\\\\()*+,-/:;<=>?@[\\\\]^_`{|}~「」〔〕“”〈〉『』【】＆＊・（）＄＃＠。、？！｀＋￥％]')\n#     # .  は削除しちゃいかん\n#     title = code_regex.sub(' ', title)\n#     title = title.strip()\n    return title\n\ndef convert_comma(title):\n    title = re.sub(r\"([0-9]),([0-9])\", r\"\\1.\\2\", title)\n    return title\n    \ndef text_preprocessing(text):\n    text = remove_emoji(text)\n    text = text.lower()\n    text = convert_comma(text)\n    text = unify_units(text)\n#     text = remove_letters(text)\n    \n\n    return text","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# TFIDF","metadata":{}},{"cell_type":"code","source":"def predict_tfidf(df, title_col, max_features=25000, thresh=0.5, norm=False):\n    df = df.loc[:, [\"posting_id\", title_col]]\n    gf = cudf.DataFrame(df)\n    model = TfidfVectorizer(stop_words=\"english\", binary=True, max_features=25000, dtype=np.float32)\n    text_embeddings = model.fit_transform(gf[title_col]).toarray()\n    print(text_embeddings.shape)\n#     text_embeddings = cupy.asnumpy(text_embeddings)\n    del model\n    gc.collect()\n    if norm:\n        text_embeddings = cupy.asnumpy(text_embeddings)\n        text_embeddings = text_embeddings.astype(np.float16)\n        text_embeddings = normalize(text_embeddings)\n        text_embeddings = cupy.asarray(text_embeddings)\n    print(\"start predicting\")\n    preds = predict(text_embeddings, df, thresh=thresh)\n    del text_embeddings\n    return preds","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# word2vec","metadata":{}},{"cell_type":"code","source":"def get_word2vec_embeddings(titles, vector_size=50, window=5, min_count=1, workers=4, epochs=1500, negative=10, sg=1):\n    titles_tokens = [title.split() for title in titles]\n    dictionary = Dictionary(titles_tokens)\n    bow_corpus = [dictionary.doc2bow(token) for token in titles_tokens]\n    print(\"start word2vec training\")\n    t0 = time.time()\n    model = Word2Vec(titles_tokens, vector_size=vector_size, window=window, min_count=min_count, workers=workers, epochs=epochs, negative=negative, sg=sg)\n    print(f\"end word2vec training {time.time()-t0:.1f}s\")\n    embeddings = []\n    for sentence in titles:\n        title = sentence.split()\n        vectors = [model.wv[w] for w in title]\n        embeddings.append(np.mean(vectors, axis=0))\n    model.save(\"word2vec_pretrained.bin\")\n    return np.array(embeddings)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# bert ","metadata":{}},{"cell_type":"code","source":"class ShopeeTextDataset(Dataset):\n    def __init__(self, title, label, tokenizer=None, max_length=64):\n        self.title = title\n        self.label = label\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n        \n    def __len__(self):\n        return len(self.title)\n    def __getitem__(self, idx):\n        title = self.title[idx]\n        label = self.label[idx]\n        \n        title = self.tokenizer(title, padding='max_length', truncation=True, max_length=self.max_length, return_tensors=\"pt\")\n#         print(title)\n        input_ids = title['input_ids'][0]\n        attention_mask = title['attention_mask'][0]\n        \n        return {\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"label\": label}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ShopeeNet(nn.Module):\n\n    def __init__(self,\n                 n_classes,\n                 model_name='bert-base-uncased',\n                 pooling='mean_pooling',\n                 use_fc=True,\n                 fc_dim=512,\n                 dropout=0.0,\n                 loss_module='softmax',\n                 s=30.0,\n                 margin=0.50,\n                 ls_eps=0.0,\n                 theta_zero=0.785,\n                 distil=False):\n        \"\"\"\n        :param n_classes:\n        :param model_name: name of model from pretrainedmodels\n            e.g. resnet50, resnext101_32x4d, pnasnet5large\n        :param pooling: One of ('SPoC', 'MAC', 'RMAC', 'GeM', 'Rpool', 'Flatten', 'CompactBilinearPooling')\n        :param loss_module: One of ('arcface', 'cosface', 'softmax')\n        \"\"\"\n        super(ShopeeNet, self).__init__()\n        if distil:\n            self.transformer = transformers.DistilBertModel.from_pretrained(model_name)\n        else:\n            self.transformer = transformers.AutoModel.from_pretrained(model_name)\n        final_in_features = self.transformer.config.hidden_size\n        \n        self.pooling = pooling\n        self.use_fc = use_fc\n    \n        if use_fc:\n            self.dropout = nn.Dropout(p=dropout)\n            self.fc = nn.Linear(final_in_features, fc_dim)\n            self.bn = nn.BatchNorm1d(fc_dim)\n            self.relu = nn.ReLU()\n            self._init_params()\n            final_in_features = fc_dim\n\n        self.loss_module = loss_module\n        if loss_module == 'arcface':\n            self.final = ArcMarginProduct(final_in_features, n_classes,\n                                          scale=s, margin=margin, easy_margin=False, ls_eps=ls_eps)\n        elif loss_module == \"curricularface\":\n            self.final = CurricularFace(final_in_features, n_classes, \n                                           s=s, m=margin)\n        else:\n            self.final = nn.Linear(final_in_features, n_classes)\n\n    def _init_params(self):\n        nn.init.xavier_normal_(self.fc.weight)\n        nn.init.constant_(self.fc.bias, 0)\n        nn.init.constant_(self.bn.weight, 1)\n        nn.init.constant_(self.bn.bias, 0)\n\n    def forward(self, input_ids,attention_mask, label=None):\n        feature = self.extract_feat(input_ids,attention_mask)\n        if label is None:\n            return feature\n        if self.loss_module == 'arcface':\n            logits, loss = self.final(feature, label)\n            return feature, logits, loss\n        else:\n            logits = self.final(feature)\n        return feature, logits\n\n    def extract_feat(self, input_ids,attention_mask):\n        x = self.transformer(input_ids=input_ids,attention_mask=attention_mask)\n        \n        features = x[0]\n        features = features[:,0,:]\n        if self.use_fc:\n            features = self.dropout(features)\n            features = self.fc(features)\n            features = self.bn(features)\n            features = self.relu(features)\n\n        return features","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_bert_embeddings(title, transformer_model, pretrained_path, max_length=64, loss_module=\"arcface\"):\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    if \"distilbert\" in transformer_model:\n        tokenizer =  transformers.DistilBertTokenizer.from_pretrained(transformer_model)\n        distil = True\n    else:\n        tokenizer = AutoTokenizer.from_pretrained(transformer_model)\n        distil = False\n    \n    ds = ShopeeTextDataset(title, np.zeros_like(title), tokenizer, max_length=max_length)\n    data_loader = DataLoader(ds, batch_size=32, shuffle=False, pin_memory=False)\n    \n#     model = ShopeeNLPModel(model_name=\"bert_base_uncased\", out_dims=CFG.bert_dims)\n    \n    model = ShopeeNet(n_classes=CFG.classes, model_name=transformer_model, pooling=\"clf\",\n                     use_fc=True, fc_dim=CFG.fc_dim, loss_module=loss_module, distil=distil)\n    model.load_state_dict(torch.load(\n        pretrained_path, map_location=\"cpu\"))\n    model.to(device)\n    model.eval()\n    feats = []\n    with torch.no_grad():\n        for data in tqdm(data_loader):\n            txt = data[\"input_ids\"].to(device)\n            mask = data[\"attention_mask\"].to(device)\n            feat = model(txt, mask)\n            feats.append(feat.detach().cpu().numpy())\n    del model\n    gc.collect()\n    feats = np.concatenate(feats)\n    return feats","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# phash","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def phash2bin(phash):\n    return format(int(phash, 16), \"64b\")\n\ndef vectorize_bin(bins):\n    \"\"\"\n    bins: np.array \n    vectorize_bin(train_df[\"bin\"].values)\n    \"\"\"\n    def vectorize_row(row):\n        return [int(r) for r in list(row)]\n    list_bin = [vectorize_row(r) for r in bins]\n    return np.array(list_bin)\n\ndef get_phash_embeddings(df, n_components=32):\n    print(\"getting phash embeddings\")\n    df[\"phash_bin\"] = df[\"image_phash\"].apply(phash2bin)\n    features = vectorize_bin(df[\"phash_bin\"].values)\n    print(features.shape)\n    pca = PCA(n_components=n_components)\n    embeddings = pca.fit_transform(features)\n    return embeddings","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Image Dataset","metadata":{}},{"cell_type":"code","source":"class ShopeeImageDataset(Dataset):\n    def __init__(self, x, transforms=None, cc_transforms=None):\n        \"\"\"\n        x: np.array, \n        \"\"\"\n        self.x = x\n        self.transforms = transforms\n        self.cc_transforms = cc_transforms\n    def __len__(self):\n        return len(self.x)\n    def __getitem__(self, idx):\n        image = cv2.imread(self.x[idx])[:, :, ::-1]\n        if self.transforms:\n            image_ = self.transforms(image=image)[\"image\"]\n            if self.cc_transforms:\n                cropped = self.cc_transforms(image=image)[\"image\"]\n            else:\n                cropped = image_\n        return {\"image\": image_, \"cropped\": cropped}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_transforms(size, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]):\n    return A.Compose([\n        A.Resize(size, size),\n#         FaceHiding(p=1.0),\n#         FaceMosaic(p=1.0),\n        A.Normalize(mean, std),\n        ToTensorV2(),\n    ])\ndef get_cc_transforms(size, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], resized_size=600):\n    return A.Compose([\n        A.Resize(resized_size, resized_size),\n        A.CenterCrop(size, size),\n#         FaceHiding(p=1.0),\n#         FaceMosaic(p=1.0),\n        A.Normalize(mean, std),\n        ToTensorV2(),\n    ])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Arcmargin product, CurricularFace","metadata":{}},{"cell_type":"code","source":"class ArcMarginProduct(nn.Module):\n    def __init__(self, in_features, out_features, scale=30.0, margin=0.50, easy_margin=False, ls_eps=0.0):\n        super(ArcMarginProduct, self).__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.scale = scale\n        self.margin = margin\n        self.ls_eps = ls_eps  # label smoothing\n        self.weight = nn.Parameter(torch.FloatTensor(out_features, in_features))\n        nn.init.xavier_uniform_(self.weight)\n\n        self.easy_margin = easy_margin\n        self.cos_m = math.cos(margin)\n        self.sin_m = math.sin(margin)\n        self.th = math.cos(math.pi - margin)\n        self.mm = math.sin(math.pi - margin) * margin\n\n    def forward(self, input, label):\n        cosine = F.linear(F.normalize(input), F.normalize(self.weight))\n        sine = torch.sqrt(1.0 - cosine*cosine)\n        phi = cosine * self.cos_m - sine * self.sin_m\n        if self.easy_margin:\n            phi = torch.where(cosine > 0, phi, cosine)\n        else:\n            phi = torch.where(cosine > self.th, phi, cosine - self.mm)\n    \n        one_hot = torch.zeros(cosine.size(), device='cuda')\n        one_hot.scatter_(1, label.view(-1, 1).long(), 1)\n        if self.ls_eps > 0:\n            one_hot = (1 - self.ls_eps) * one_hot + self.ls_eps / self.out_features\n\n        output = (one_hot * phi) + ((1.0 - one_hot) * cosine)\n        output *= self.scale\n        return output, nn.CrossEntropyLoss()(output,label)\n\ndef l2_norm(input, axis = 1):\n    norm = torch.norm(input, 2, axis, True)\n    output = torch.div(input, norm)\n\n    return output\n\nclass CurricularFace(nn.Module):\n    def __init__(self, in_features, out_features, s = 30, m = 0.50):\n        super(CurricularFace, self).__init__()\n\n        print('Using Curricular Face')\n\n        self.in_features = in_features\n        self.out_features = out_features\n        self.m = m\n        self.s = s\n        self.cos_m = math.cos(m)\n        self.sin_m = math.sin(m)\n        self.threshold = math.cos(math.pi - m)\n        self.mm = math.sin(math.pi - m) * m\n        self.kernel = nn.Parameter(torch.Tensor(in_features, out_features))\n        self.register_buffer('t', torch.zeros(1))\n        nn.init.normal_(self.kernel, std=0.01)\n\n    def forward(self, embbedings, label):\n        embbedings = l2_norm(embbedings, axis = 1)\n        kernel_norm = l2_norm(self.kernel, axis = 0)\n        cos_theta = torch.mm(embbedings, kernel_norm)\n        cos_theta = cos_theta.clamp(-1, 1)  # for numerical stability\n        with torch.no_grad():\n            origin_cos = cos_theta.clone()\n        target_logit = cos_theta[torch.arange(0, embbedings.size(0)), label].view(-1, 1)\n\n        sin_theta = torch.sqrt(1.0 - torch.pow(target_logit, 2))\n        cos_theta_m = target_logit * self.cos_m - sin_theta * self.sin_m #cos(target+margin)\n        mask = cos_theta > cos_theta_m\n        final_target_logit = torch.where(target_logit > self.threshold, cos_theta_m, target_logit - self.mm)\n\n        hard_example = cos_theta[mask]\n        with torch.no_grad():\n            self.t = target_logit.mean() * 0.01 + (1 - 0.01) * self.t\n        cos_theta[mask] = hard_example * (self.t + hard_example)\n        cos_theta.scatter_(1, label.view(-1, 1).long(), final_target_logit)\n        output = cos_theta * self.s\n        return output, nn.CrossEntropyLoss()(output,label)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Image model","metadata":{}},{"cell_type":"code","source":"class ShopeeModel4(nn.Module):\n\n    def __init__(\n        self,\n        n_classes = CFG.classes,\n        model_name = \"eca_nfnet_l0\",\n        fc_dim = CFG.fc_dim,\n        margin = CFG.margin,\n        scale = CFG.scale,\n        use_fc = True,\n        pretrained = True,\n        loss_module=\"arcface\"):\n\n\n        super(ShopeeModel4,self).__init__()\n        print('Building Model Backbone for {} model'.format(model_name))\n\n        self.backbone = timm.create_model(model_name, pretrained=pretrained)\n\n        if model_name == 'resnext50_32x4d':\n            final_in_features = self.backbone.fc.in_features\n            self.backbone.fc = nn.Identity()\n            self.backbone.global_pool = nn.Identity()\n\n        elif 'efficientnet' in model_name:\n            final_in_features = self.backbone.classifier.in_features\n            self.backbone.classifier = nn.Identity()\n            self.backbone.global_pool = nn.Identity()\n        \n        elif 'nfnet' in model_name:\n            final_in_features = self.backbone.head.fc.in_features\n            self.backbone.head.fc = nn.Identity()\n            self.backbone.head.global_pool = nn.Identity()\n        \n        elif (\"swin\" in model_name) or (\"vit\" in model_name):\n            final_in_features = self.backbone.head.in_features\n            self.backbone.head = nn.Identity()\n        \n        if (\"swin\" in model_name) or (\"vit\" in model_name):\n            self.pooling = nn.Identity()\n        else:\n            self.pooling =  nn.AdaptiveAvgPool2d(1)\n#         self.pooling =  nn.AdaptiveAvgPool2d(1)\n\n        self.use_fc = use_fc\n\n        if use_fc:\n            self.dropout = nn.Dropout(p=0.0)\n            self.fc = nn.Linear(final_in_features, fc_dim)\n            self.bn = nn.BatchNorm1d(fc_dim)\n            self._init_params()\n            final_in_features = fc_dim\n            \n        if loss_module == \"curricularface\":\n            self.final = CurricularFace(final_in_features, \n                                               n_classes, \n                                               s=scale, \n                                               m=margin)\n        elif loss_module == \"arcface\":\n            self.final = ArcMarginProduct(final_in_features,\n                                            n_classes,\n                                            scale = scale,\n                                            margin = margin,\n                                            easy_margin = False,\n                                            ls_eps = 0.0)\n\n    def _init_params(self):\n        nn.init.xavier_normal_(self.fc.weight)\n        nn.init.constant_(self.fc.bias, 0)\n        nn.init.constant_(self.bn.weight, 1)\n        nn.init.constant_(self.bn.bias, 0)\n\n    def forward(self, image, label=None):\n        feature = self.extract_feat(image)\n        if self.training:\n            logits = self.final(feature, label)\n            return logits\n        else:\n            return feature\n\n    def extract_feat(self, x):\n        batch_size = x.shape[0]\n        x = self.backbone(x)\n        x = self.pooling(x).view(batch_size, -1)\n\n        if self.use_fc:\n            x = self.dropout(x)\n            x = self.fc(x)\n            x = self.bn(x)\n        return x","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Mish_func(torch.autograd.Function):\n\n    \"\"\"from: https://github.com/tyunist/memory_efficient_mish_swish/blob/master/mish.py\"\"\"\n    \n    @staticmethod\n    def forward(ctx, i):\n        result = i * torch.tanh(F.softplus(i))\n        ctx.save_for_backward(i)\n        return result\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        i = ctx.saved_variables[0]\n        v = 1. + i.exp()\n        h = v.log()\n        grad_gh = 1./h.cosh().pow_(2)\n        grad_hx = i.sigmoid()\n        grad_gx = grad_gh *  grad_hx\n        grad_f =  torch.tanh(F.softplus(i)) + i * grad_gx\n        return grad_output * grad_f \n\n\nclass Mish(nn.Module):\n    def __init__(self, **kwargs):\n        super().__init__()\n        pass\n\n    def forward(self, input_tensor):\n        return Mish_func.apply(input_tensor)\n\n\ndef replace_activations(model, existing_layer, new_layer):\n    \n    \"\"\"A function for replacing existing activation layers\"\"\"\n    \n    for name, module in reversed(model._modules.items()):\n        if len(list(module.children())) > 0:\n            model._modules[name] = replace_activations(module, existing_layer, new_layer)\n        if type(module) == existing_layer:\n            layer_old = module\n            layer_new = new_layer\n            model._modules[name] = layer_new\n    return model","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ShopeeModel(nn.Module):\n\n    def __init__(\n        self,\n        n_classes = CFG.classes,\n        model_name = \"tf_efficientnet_b7_ns\",\n        fc_dim = CFG.fc_dim,\n        margin = CFG.margin,\n        scale = CFG.scale,\n        use_fc = True,\n        pretrained = True):\n\n        super(ShopeeModel,self).__init__()\n        print('Building Model Backbone for {} model'.format(model_name))\n\n        self.backbone = timm.create_model(model_name, pretrained=pretrained)\n        in_features = self.backbone.classifier.in_features\n        self.backbone.classifier = nn.Identity()\n        self.backbone.global_pool = nn.Identity()\n        self.pooling =  nn.AdaptiveAvgPool2d(1)\n        self.use_fc = use_fc\n\n        if use_fc:\n            self.dropout = nn.Dropout(p=0.1)\n            self.classifier = nn.Linear(in_features, fc_dim)\n            self.bn = nn.BatchNorm1d(fc_dim)\n            self._init_params()\n            in_features = fc_dim\n\n        self.final = ArcMarginProduct(\n            in_features,\n            n_classes,\n            scale = scale,\n            margin = margin,\n            easy_margin = False,\n            ls_eps = 0.0\n        )\n\n    def _init_params(self):\n        nn.init.xavier_normal_(self.classifier.weight)\n        nn.init.constant_(self.classifier.bias, 0)\n        nn.init.constant_(self.bn.weight, 1)\n        nn.init.constant_(self.bn.bias, 0)\n\n    def forward(self, image, label=None):\n        features = self.extract_features(image)\n        if self.training:\n            logits = self.final(features, label)\n            return logits\n        else:\n            return features\n\n    def extract_features(self, x):\n        batch_size = x.shape[0]\n        x = self.backbone(x)\n        x = self.pooling(x).view(batch_size, -1)\n\n        if self.use_fc and self.training:\n            x = self.dropout(x)\n            x = self.classifier(x)\n            x = self.bn(x)\n        return x","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Image Enbeddings","metadata":{}},{"cell_type":"code","source":"def get_image_embeddings_from_model(x, model, transforms=None, batch_size=16, tta=False, cc_transforms=None):\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    dataset = ShopeeImageDataset(x, transforms, cc_transforms)\n    dataloader = DataLoader(dataset, shuffle=False, batch_size=batch_size,\n                            pin_memory=False, num_workers=2)\n    \n    model.eval()\n    model.to(device)\n    preds = []\n    with torch.no_grad():\n        for data in tqdm(dataloader):\n            image = data[\"image\"].to(device)\n#             image = image.to(device)\n            if tta:\n                cropped = data[\"cropped\"].to(device)\n                bs, _, size, _ = image.shape\n                image = torch.stack([image, cropped,\n#                                      image.flip(-1), cropped.flip(-1), image.flip(-2), cropped.flip(-2),\n#                                      image.flip(-1).flip(-2), cropped.flip(-1).flip(-2)\n                                    ], 0)\n                image = image.view(-1, 3, size, size)\n            pred = model(image)\n            if tta:\n                pred = pred.view(2, bs, -1).mean(0)\n            preds.append(pred.detach().cpu().numpy())\n            del pred\n            \n    preds = np.concatenate(preds, axis=0)\n    return preds","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_image_embeddings(images, model_name, pretrained_path, size, replace_mish=False, tta=True, loss_module=\"arcface\"):\n\n    if model_name == \"tf_efficientnet_b7_ns\":\n        model = ShopeeModel(pretrained=False, model_name=model_name)\n    else:\n        model = ShopeeModel4(pretrained=False, model_name=model_name, loss_module=loss_module)\n    if replace_mish:\n        existing_layer = torch.nn.SiLU\n        new_layer = Mish()\n        model = replace_activations(model, existing_layer, new_layer)\n        print(\"replaced mish\")\n        \n    model.load_state_dict(\n        torch.load(pretrained_path, map_location=\"cpu\")\n    )\n    model.eval()\n    \n    if \"vit\" in model_name:\n        mean = [0.5, 0.5, 0.5]\n        std = [0.5, 0.5, 0.5]\n    else:\n        mean=[0.485, 0.456, 0.406]\n        std=[0.229, 0.224, 0.225]\n     \n    transforms = get_transforms(size, mean=mean, std=std)\n    if tta:\n        cc_transforms = get_cc_transforms(size, mean=mean, std=mean, resized_size=int(size*1.2))\n    else:\n        cc_transforms = None\n    image_embedding = get_image_embeddings_from_model(images, model, transforms=transforms, batch_size=32, tta=True, cc_transforms=cc_transforms)\n    return image_embedding\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# main","metadata":{}},{"cell_type":"code","source":"if __name__ == \"__main__\":\n    print(\"start\")\n    \n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    \n    if len(test_df) == 3:\n        CFG.n_neighbors = 2\n        CFG.n_components = 1\n        CFG.pca_components = 1\n        \n    # ----- text preprocessing ----------\n    test_df[\"preprocessed_title\"] = test_df[\"title\"].apply(text_preprocessing)\n    titles = test_df[\"preprocessed_title\"].values\n    \n    # ------- phash ----------\n    phash_embeddings = get_phash_embeddings(test_df, n_components=CFG.n_components)\n    \n    \n    # ------ TFIDF -----------\n    tfidf_pred = predict_tfidf(test_df, \"preprocessed_title\", thresh=CFG.tfidf_thresh, max_features=CFG.max_features)\n    test_df[\"tfidf_pred\"] = tfidf_pred\n    \n    if CFG.calc_cv:\n        score = get_score(test_df, \"tfidf_pred\")\n        print(f\"CV for tfidf is {score}\")\n        \n    del tfidf_pred\n    gc.collect()\n    \n    \n    # --------- word2vec -----------\n\n    word2vec_embeddings = get_word2vec_embeddings(titles, vector_size=CFG.vector_size, window=CFG.window, epochs=CFG.epochs)\n    \n        \n    # ------- Image -------------\n    images = test_df[\"image\"].values\n    image_embeddings1 = get_image_embeddings(images, CFG.image_model_name1, CFG.image_pretrained_path1,\n                                 CFG.size1, replace_mish=True, tta=True, loss_module=CFG.image_loss_module1)\n    \n    image_embeddings2 = get_image_embeddings(images, CFG.image_model_name2, CFG.image_pretrained_path2,\n                                 CFG.size2, replace_mish=False, tta=True, loss_module=CFG.image_loss_module2)\n    \n    image_embeddings3 = get_image_embeddings(images, CFG.image_model_name3, CFG.image_pretrained_path3,\n                                 CFG.size3, replace_mish=False, tta=True, loss_module=CFG.image_loss_module3)\n    \n\n#     image_embeddings_pca = PCA(n_components=CFG.pca_components).fit_transform(image_embeddings1)\n    \n    \n    # ------- bert ---------------\n    bert_embeddings1 = get_bert_embeddings(titles, CFG.transformer_model1, CFG.bert_pretrained_path1, max_length=CFG.max_length1, loss_module=CFG.bert_loss_module1)\n    bert_embeddings2 = get_bert_embeddings(titles, CFG.transformer_model2, CFG.bert_pretrained_path2, max_length=CFG.max_length2, loss_module=CFG.bert_loss_module2)\n\n    \n    # ---------- concat --------\n    embeddings = np.concatenate([image_embeddings1,\n                                 bert_embeddings1,\n                                 phash_embeddings,\n                                 word2vec_embeddings,\n#                                  image_embeddings_pca,\n                                 image_embeddings2,\n                                 bert_embeddings2,\n                                 image_embeddings3,\n                                ], axis=1)\n    print(\"embedding shape \",embeddings.shape)\n    pred = get_neighbor_images(embeddings, test_df, thresh=CFG.thresh, n_neighbors=CFG.n_neighbors, metric=\"cosine\")\n    test_df[\"pred\"] = pred\n    \n    del embeddings, image_embeddings1, bert_embeddings1\n    gc.collect()\n    \n    if CFG.calc_cv:\n        score = get_score(test_df, \"pred\")\n        print(\"Score is \", score)\n        \n    # ---------- combine ------------\n    pred_cols = [\"tfidf_pred\", \"pred\"]\n    if CFG.calc_cv:\n        test_df[\"matches\"] = test_df.apply(lambda x: combine_for_cv(x, pred_cols, count=1), axis=1)\n        score = get_score(test_df, \"matches\")\n        print(\"cv score: \", score)\n        test_df.to_csv(\"oof.csv\", index=False)\n    test_df[\"matches\"] = test_df.apply(lambda x: combine_for_sub(x, pred_cols, count=1), axis=1)\n    \n    test_df[['posting_id','matches']].to_csv('submission.csv',index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.read_csv(\"submission.csv\").head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}