{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Outline\n- TFIDF representatation of the text and Effnet output is input into arcface layer. \n- During inference, seperated those 2 embeddings and created seperate predictions for both.\n- BERT based model's weights are taken from [here](https://www.kaggle.com/tanulsingh077/best-multilingual-model) and created predictions from this model too.\n- Took union of those 3 embeddings' predictions as the final predictions.\n- Also final part of the notebook, runs similar experiments with this [notebook](https://www.kaggle.com/anlgrbz/how-optimum-threshold-changes-with-embed-test-size) for different embedding models.","metadata":{}},{"cell_type":"code","source":"from shopee_helper_script import *\nfrom torch.nn import functional as F\nimport transformers\nimport plotly.express as px\n\nIMAGE_SIZE = 512\n\ntfidf_dim=15000\nimg_emb_dim=256\ntfidf_emb_dim=256\n\nimage_threshold = 0.1 - 0.03\nbert_threshold = 0.4 - 0.18\ntfidf_threshold = 0.15 - 0.08\n\n\ndata_folder = \"../input/shopee-product-matching\"\npre_trained_image_model_folder = \"../input/pre-trained-models/\"\nmodel_path = \"../input/pre-trained-models/tfidf_15000_256_256_11014_10.pth\"  #### MODIFY THIS####\nmodel_file_name = \"efficientnet-b1-f1951068.pth\"\nmodel_name = \"efficientnet-b1\"\n\n\ntransformer_model = '../input/sentence-transformer-models/paraphrase-xlm-r-multilingual-v1/0_Transformer'\nTEXT_MODEL_PATH = '../input/best-multilingual-model/sentence_transfomer_xlm_best_loss_num_epochs_25_arcface.bin'\nTOKENIZER = transformers.AutoTokenizer.from_pretrained(transformer_model)\n\n!cp -r ../input/eff-net-whl/EfficientNet-PyTorch .\n!pip install -e EfficientNet-PyTorch/.\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import importlib  \nENet = importlib.import_module(\"EfficientNet-PyTorch.efficientnet_pytorch\")\nEfficientNet = ENet.EfficientNet","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class image_tfidf_embedder(Base_model):\n    def __init__(self, tfidf_dim=15000, img_emb=256, text_emb=256, out_classes=11014):\n        super().__init__()\n\n        try:\n            self.effnet = EfficientNet.from_name(model_name)\n            self.effnet.load_state_dict(torch.load(pre_trained_image_model_folder + model_file_name))\n        except:\n            self.effnet = EfficientNet.from_pretrained(pre_trained_image_model_folder + model_file_name)\n\n        self.linear1 = nn.Linear(1000, img_emb)\n        self.linear2 = nn.Linear(tfidf_dim, text_emb)\n        self.arcface_head = ArcFace(img_emb + text_emb, out_classes)\n\n        self.loss = nn.CrossEntropyLoss()\n        self.metric = metrics.accuracy_score\n        self.threshold = None\n\n\n    def forward(self, data_batch):\n        images = data_batch[\"image\"]\n        text_vec = data_batch[\"text_vec\"]\n        label = data_batch[\"label\"]\n\n        images = self.effnet(images)\n        img_emb = self.linear1(images)\n        text_emb = self.linear2(text_vec)\n\n        full_emb = torch.cat([img_emb,text_emb], dim=1)\n\n\n        if self.training:\n            out = self.arcface_head(full_emb, label)\n            loss = self.loss(out, label)\n            metric = 0\n\n            return out, loss, metric\n        else:\n            return full_emb, 0, 0\n\n\n    def set_optimizer(self, lr):\n        self.optimizer = torch.optim.Adam(self.parameters(), lr=lr)\n\n\n    def validate_all(self, valid_dataset):\n        embeddings = self.predict(valid_dataset, batch_size=self.valid_batch_size)\n\n        # Find best threshold for cosine distance and log the f1\n        best_cosine_threshold = get_best_threshold(cosine_find_matches_cupy, embeddings, valid_dataset.df.posting_id, valid_dataset.df.target, np.arange(0.05,0.30,0.05))\n        matches = cosine_find_matches_cupy(embeddings, valid_dataset.df.posting_id, best_cosine_threshold,create_submission=False)\n        f1_score = matches_to_f1_score(valid_dataset.df.target, pd.Series(matches))\n        wandb.log({\"Valid_cosine_F1\": f1_score}, step=self.current_train_step)\n\n\n\nclass bert_embedder(Base_model):\n    def __init__(self, out_classes=11014, dropout=0.3):\n        super().__init__()\n        self.transformer = transformers.AutoModel.from_pretrained(transformer_model)\n\n\n    def forward(self, data_batch):\n        input_id = data_batch[\"input_id\"]\n        attention_mask = data_batch[\"attention_mask\"]\n        \n        x = self.transformer(input_ids=input_id, attention_mask=attention_mask)\n        \n        feat = x[0]\n        feat = feat[:,0,:]\n        out = F.normalize(feat)\n                \n        return out, None, None\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_transforms(img_size=IMAGE_SIZE):\n    return albumentations.Compose([\n        albumentations.Resize(img_size, img_size),\n        albumentations.Normalize()\n    ])\n\n# Function to get our text title embeddings\ndef get_tfidf(titles, max_features = 15500):\n\n    vectorizer = TfidfVectorizer(stop_words = 'english', binary = True, max_features = max_features)\n    vectorizer = vectorizer.fit(pd.read_csv(data_folder + \"/train.csv\"))\n    text_embeddings = vectorizer.transform(titles)\n    del vectorizer\n    return text_embeddings\n\nclass ShopeeDataset(Dataset):\n    def __init__(self, df, mode, transforms=get_transforms()):\n        self.df = df.reset_index(drop=True)\n        self.transform = transforms\n        self.mode = mode\n        self.text_vec = get_tfidf(df[\"title\"])\n\n    def __len__(self):\n        return self.df.shape[0]\n\n    def __getitem__(self, index):\n        row = self.df.iloc[index,]\n        text_vec = self.text_vec[index,]\n        \n        \n        try:\n            label_group = torch.tensor(row.label_group)\n        except (ValueError, AttributeError):\n            label_group = torch.Tensor()\n\n\n        image = cv2.imread(row.file_path)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        image = self.transform(image=image)\n        image = image[\"image\"].astype(np.float32)\n        image = image.transpose(2, 0, 1) # Turn into pytorch format # Batch, Channels, ...\n        image = torch.tensor(image)\n\n        return {\"image\":image,  \"text_vec\":torch.tensor(np.squeeze(text_vec.toarray().astype(np.float32))) , \"label\":label_group}\n\n        \nclass ShopeeTextDataset(Dataset):\n    def __init__(self, df):\n        self.df = df.reset_index(drop=True)\n\n    def __len__(self):\n        return self.df.shape[0]\n\n    def __getitem__(self, index):\n        row = self.df.iloc[index,]\n        \n        text = row.title\n        \n        text = TOKENIZER(text, padding='max_length', truncation=True, max_length=128, return_tensors=\"pt\")\n        input_ids = text['input_ids'][0]\n        attention_mask = text['attention_mask'][0]\n\n\n        return {\"input_id\":torch.tensor(input_ids),  \"attention_mask\": torch.tensor(attention_mask)}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#train = pd.read_csv(\"../input/shopee-product-matching/train.csv\")\n#text_train_ds = ShopeeTextDataset(train)\n\ntest = pd.read_csv(\"../input/shopee-product-matching/test.csv\")\ntext_test_ds = ShopeeTextDataset(test)\n\ntext_model = bert_embedder(out_classes=11014)\ntext_model.load_state_dict(dict(list(torch.load(TEXT_MODEL_PATH).items())[:-1]))\ntext_model.device=device\ntext_model.to(device)\n\ntext_embed = text_model.predict(text_test_ds, batch_size=1000)\n\ntext_matches = cosine_find_matches_cupy(text_embed, text_test_ds.df.posting_id, bert_threshold, create_submission=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#image_train_ds , _ = create_train_test(mode=\"train\")\ntest_ds , _ = create_train_test(mode=\"inference\")\n\n# Load trained model\nmodel = image_tfidf_embedder(tfidf_dim=tfidf_dim,  img_emb=img_emb_dim, text_emb=tfidf_emb_dim)\nmodel.load_state_dict(torch.load(model_path))\nmodel.device=device\n\n# Generate embeddings and then submission file\nimg_tfidf_embed = model.predict(test_ds) # If doesn't work, self.device attribute might be missing\n\nimg_embed = img_tfidf_embed[:,:256]\ntfidf_embed = img_tfidf_embed[:,256:]\n\nimg_matches = cosine_find_matches_cupy(img_embed, test_ds.df.posting_id, image_threshold, create_submission=False)\ntfidf_matches = cosine_find_matches_cupy(tfidf_embed, test_ds.df.posting_id, tfidf_threshold, create_submission=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Take union of the predictions\n#matches = list(map(lambda x,y,z: \" \".join(set(x.split(\" \") + y.split(\" \") + z.split(\" \"))), img_matches, tfidf_matches, text_matches ))\nmatches = list(map(lambda x,y: \" \".join(set(x.split(\" \") + y.split(\" \") )), img_matches,  text_matches ))\n\npd.DataFrame({\"posting_id\": test_ds.df.posting_id, \"matches\": matches}).to_csv(\"submission.csv\", index=False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Run Optimum Threshold Search for Different Data Size","metadata":{}},{"cell_type":"code","source":"def get_best_threshold2(method, embeddings, posting_ids, correct_matches, candidates):\n\n    scores = dict()\n    for threshold in candidates:\n\n        matches = method(embeddings, posting_ids, threshold, create_submission=False)\n\n        scores[threshold] = matches_to_f1_score(pd.Series(matches), pd.Series(correct_matches))\n\n        print(f\"Method:{method.__name__},   Threshold:{threshold},   F1-Score: {scores[threshold]}\")\n\n    best_threshold = max(scores, key=scores.get)\n    print(f\"Best Threshold:{best_threshold},  Best F1-Score: {scores[best_threshold]}\")\n\n    return best_threshold, scores[best_threshold]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_submission_format(df):\n    tmp = df.groupby(\"label_group\").posting_id.unique().to_dict()\n    matches = df.label_group.map(lambda x: \" \".join(tmp[x]))\n    return matches\n\ntrain = pd.read_csv(data_folder+\"/train.csv\")\ntrain[\"target\"] = create_submission_format(train)\n\ncv_splitter = GroupKFold(n_splits=5)\ntrain[\"fold\"] = -1\n\n# Assign folds for validation\nfor fold, (train_idx, valid_idx) in enumerate(cv_splitter.split(train, None, train.label_group)):\n    train.loc[valid_idx, \"fold\"] = fold","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_no = 1 # \"Bert\"\n\n\ntext_train_ds = ShopeeTextDataset(train)\ntext_model = bert_embedder(out_classes=11014)\ntext_model.load_state_dict(dict(list(torch.load(TEXT_MODEL_PATH).items())[:-1]))\ntext_model.device=device\ntext_model.to(device)\ntext_embed = text_model.predict(text_train_ds, batch_size=500)\n\n    \n    \ntracker = pd.DataFrame(columns=[\"model_no\", \"n_label_group\", \"n_post\", \"optimum_threshold\", \"score\"], data=np.zeros((15,5)))\n\nfor folds_before, _ in enumerate(range(5)):\n    print(\"=\"*50)\n    print(\"All Folds up to Fold:\", folds_before)\n    print(\"=\"*50)\n    valid_emb = text_embed[train.fold <= folds_before,]\n    valid_df = train.loc[train.fold <= folds_before,]\n    n_label_group = valid_df.label_group.nunique()\n    n_post = valid_df.shape[0]\n    print(\"Number of Label Groups: \", n_label_group)\n    print(\"Number of Posts: \", n_post)\n    best_threshold, best_score = get_best_threshold2(cosine_find_matches_cupy, valid_emb, valid_df.posting_id.values, valid_df.target.values, np.arange(0.35, 0.70, 0.05))\n    tracker.iloc[folds_before,] = (model_no, n_label_group, n_post, best_threshold, best_score)\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_no = 2 #\"image\"\ntrain_ds , _ = create_train_test(mode=\"train\")\n\n# Load trained model\nmodel = image_tfidf_embedder(tfidf_dim=tfidf_dim,  img_emb=img_emb_dim, text_emb=tfidf_emb_dim)\nmodel.load_state_dict(torch.load(model_path))\nmodel.device=device\n\n# Generate embeddings and then submission file\nimg_tfidf_embed = model.predict(train_ds) # If doesn't work, self.device attribute might be missing\n\nimg_embed = img_tfidf_embed[:,:256]\ntfidf_embed = img_tfidf_embed[:,256:]\n\n\nfor folds_before, _ in enumerate(range(5)):\n    print(\"=\"*50)\n    print(\"All Folds up to Fold:\", folds_before)\n    print(\"=\"*50)\n    valid_emb = img_embed[train.fold <= folds_before,]\n    valid_df = train.loc[train.fold <= folds_before,]\n    n_label_group = valid_df.label_group.nunique()\n    n_post = valid_df.shape[0]\n    print(\"Number of Label Groups: \", n_label_group)\n    print(\"Number of Posts: \", n_post)\n    best_threshold, best_score = get_best_threshold2(cosine_find_matches_cupy, valid_emb, valid_df.posting_id.values, valid_df.target.values, np.arange(0.05, 0.45, 0.05))\n    tracker.iloc[5+folds_before,] = (model_no, n_label_group, n_post, best_threshold, best_score)\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_no = 3 #\"tfidf\"\n\n\nfor folds_before, _ in enumerate(range(5)):\n    print(\"=\"*50)\n    print(\"All Folds up to Fold:\", folds_before)\n    print(\"=\"*50)\n    valid_emb = tfidf_embed[train.fold <= folds_before,]\n    valid_df = train.loc[train.fold <= folds_before,]\n    n_label_group = valid_df.label_group.nunique()\n    n_post = valid_df.shape[0]\n    print(\"Number of Label Groups: \", n_label_group)\n    print(\"Number of Posts: \", n_post)\n    best_threshold, best_score = get_best_threshold2(cosine_find_matches_cupy, valid_emb, valid_df.posting_id.values, valid_df.target.values, np.arange(0.05, 0.45, 0.05))\n    tracker.iloc[10+folds_before,] = (model_no, n_label_group, n_post, best_threshold, best_score)\n\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tracker = tracker.iloc[:15,]\ntracker.model_no = tracker.model_no.map({1:\"Bert\", 2:\"effnet\" ,3:\"tfidf\"})\nfig = px.scatter(tracker, x=\"n_post\", y=\"optimum_threshold\", trendline=\"ols\", facet_col=\"model_no\")\nfig.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # to have target column \n# tmp_ds , _ = create_train_test(mode=\"train\")\n\n# ############ SEARCH THRESHOLD ON TRAIN ####################\n# text_train_ds = ShopeeTextDataset(pd.read_csv(\"../input/shopee-product-matching/train.csv\"))\n\n# text_model = text_embedder(out_classes=11014)\n# text_model.load_state_dict(dict(list(torch.load(TEXT_MODEL_PATH).items())[:-1]))\n# text_model.device=device\n# text_model.to(device)\n# text_embed = text_model.predict(text_train_ds, batch_size=1000)\n# print(\"For data size:\", len(train_ds))\n# get_best_threshold2(cosine_find_matches_cupy, text_embed, text_train_ds.df.posting_id,  tmp_ds.df.target, np.arange(0.05,0.60,0.05))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ############ SEARCH THRESHOLD ON TRAIN ####################\n# train_ds , _ = create_train_test(mode=\"train\")\n# # Load trained model\n# model = image_embedder(tfidf_dim=tfidf_dim,  img_emb=img_emb, text_emb=text_emb)\n# model.load_state_dict(torch.load(model_path))\n# model.device=device\n\n# # Generate embeddings and then submission file\n# embeddings = model.predict(train_ds) # If doesn't work, self.device attribute might be missing\n\n# img_emb = embeddings[:,:256]\n# text_emb = embeddings[:,256:]\n# best_threshold_img = get_best_threshold(cosine_find_matches_cupy, img_emb, train_ds.df.posting_id,  train_ds.df.target, np.arange(0.05,0.50))\n# best_threshold_text = get_best_threshold(cosine_find_matches_cupy, text_emb, train_ds.df.posting_id,  train_ds.df.target, np.arange(0.05,0.50))\n\n# best_threshold_img = get_best_threshold(cosine_find_matches_cupy, img_emb, train_ds.df.posting_id,  train_ds.df.target, np.arange(0.05,0.50,0.05))\n# best_threshold_text = get_best_threshold(cosine_find_matches_cupy, text_emb, train_ds.df.posting_id,  train_ds.df.target, np.arange(0.05,0.50,0.05))\n\n# print(best_threshold_img)\n# print(best_threshold_text)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ####### MAKE TEST PREDICTIONS FOR IMAGE  ###############\n# # Create test dataset\n# # train_ds , _ = create_train_test(mode=\"train\")\n# test_ds , _ = create_train_test(mode=\"inference\")\n\n# # Load trained model\n# model = image_embedder(img_emb=img_emb)\n# model.load_state_dict(torch.load(model_path))\n# model.device=device\n\n# # Generate embeddings and then submission file\n# embeddings = model.predict(test_ds) # If doesn't work, self.device attribute might be missing\n\n# cosine_find_matches_cupy(embeddings, test_ds.df.posting_id, threshold, create_submission=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Create seperate predictions for text & image then union\n# img_emb = embeddings[:,:256]\n# text_emb = embeddings[:,256:]\n\n# #img_matches = cosine_find_matches_cupy(img_emb, test_ds.df.posting_id, 0.1, create_submission=False)\n# text_matches = cosine_find_matches_cupy(text_emb, test_ds.df.posting_id, 0.20, create_submission=True) # 0.75 on data that it is trained\n# # Take union of the predictions\n# #matches = list(map(lambda x,y: \" \".join(set(x.split(\" \") + y.split(\" \"))), img_matches, text_matches))\n\n# #pd.DataFrame({\"posting_id\": test_ds.df.posting_id, \"matches\": matches}).to_csv(\"submission.csv\", index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}