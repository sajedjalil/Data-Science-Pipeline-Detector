{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# How Optimum Threshold Changes Based on Test Size?\nWe know that test dataset is x2 times the training datasize. Therefore,\nWe know that having more/less crowded space might effect the value of optimum threshold to decide a match. Hence, I will calculate optimum threshold values for differnet datasize to understand the relationship between datasize and optimum threshold. Using that, we can come up with an estimate of optimum threshold for 70k test examples.\n\nAnother factor effecting the relationship between test size and threshold can be the number of dimensions of our embeddings. Therefore, I will be running those experiments for different embedding sizes too and compare the results. \n\nI ran a couple of experiments before to decide ranges to search optimum threshold.\n\n**FOR [RESULTS](#conclusion) PLEASE GO TO THE END OF THE NOTEBOOK**\n\n**FOR THE RESULTS OF SAME EXPERIMENT USING DIFFERENT MODEL EMBEDDINGS (BERT FOR TEXT, EFFNET FOR IMAGE) PLEASE CHECK THIS [NOTEBOOK](https://www.kaggle.com/anlgrbz/embed-type-optimum-threshold-search-inference)**","metadata":{}},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import GroupKFold\nimport cupy as cp\nfrom tqdm import tqdm\nimport plotly.express as px\n\ndata_folder = \"../input/shopee-product-matching/\"\nranges1 = [(0.50, 0.60), (0.45, 0.55), (0.45, 0.55), (0.40, 0.50), (0.40, 0.50)]\nranges2 = [(0.50, 0.60), (0.45, 0.55), (0.40, 0.50), (0.40, 0.50), (0.35, 0.45)]\nranges3 = [(0.45, 0.55), (0.40, 0.50), (0.40, 0.50), (0.30, 0.40), (0.30, 0.40)]\nranges4 = [(0.40, 0.50), (0.30, 0.40), (0.20, 0.30), (0.20, 0.30), (0.20, 0.30)]\nranges5 = [(0.20, 0.30), (0.20, 0.30), (0.15, 0.25), (0.10, 0.20), (0.10, 0.20)]","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_submission_format(df):\n    tmp = df.groupby(\"label_group\").posting_id.unique().to_dict()\n    matches = df.label_group.map(lambda x: \" \".join(tmp[x]))\n    return matches","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv(data_folder+\"train.csv\")\n\ntrain[\"target\"] = create_submission_format(train)\n\ncv_splitter = GroupKFold(n_splits=5)\ntrain[\"fold\"] = -1\n\n# Assign folds for validation\nfor fold, (train_idx, valid_idx) in enumerate(cv_splitter.split(train, None, train.label_group)):\n    train.loc[valid_idx, \"fold\"] = fold\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Taken from Gunes Evitan's post here:  https://www.kaggle.com/c/shopee-product-matching/discussion/224782\ndef matches_to_f1_score(y_true, y_pred, mean=True):\n    y_true = y_true.apply(lambda x: set(x.split()))\n    y_pred = y_pred.apply(lambda x: set(x.split()))\n\n    tp = np.array([len(x[0] & x[1]) for x in zip(y_true, y_pred)])\n    fp = y_pred.apply(lambda x: len(x)).values - tp\n    fn = y_true.apply(lambda x: len(x)).values - tp\n\n    precision = tp / (tp + fp)\n    recall = tp / (tp + fn)\n    f1 = 2 * ((precision * recall) / (precision + recall))\n\n    if mean:\n        f1 = f1.mean()\n\n    return f1\n\ndef get_best_threshold(method, embeddings, posting_ids, correct_matches, candidates):\n\n    scores = dict()\n    for threshold in candidates:\n\n        matches = method(embeddings, posting_ids, threshold, create_submission=False)\n        \n        scores[threshold] = matches_to_f1_score(pd.Series(matches), pd.Series(correct_matches))\n\n        print(f\"Method:{method.__name__},   Threshold:{threshold:.4f},   F1-Score: {scores[threshold]:.4f}\")\n\n    best_threshold = max(scores, key=scores.get)\n    best_score = scores[best_threshold]\n    print(\"*\"*50)\n    print(f\"Best Threshold:{best_threshold:.4f},  Best F1-Score: {best_score:.4f}\")\n    print(\"*\"*50)\n    \n    return best_threshold, best_score\n\n# Modified xhulu's euclidian distance code for cosine distance \ndef cosine_find_matches_cupy(embeddings, posting_ids, threshold, create_submission=True):\n    empty_emb_idx = np.squeeze(np.argwhere(embeddings.sum(axis=1) == 0), axis=1)\n    embeddings = cp.array(embeddings)\n    embeddings =  embeddings / cp.linalg.norm(embeddings, axis=1)[:,None]\n    N = embeddings.shape[0]\n    matches = []\n   \n\n    for i in tqdm(range(N)):\n        v = embeddings[i, :]\n        thresholded_bool = 1 - cp.dot(embeddings,v) < threshold\n        thresholded_ix = cp.argwhere(thresholded_bool).squeeze(-1)\n        thresholded_ix = thresholded_ix.get()\n        match = \" \".join(posting_ids[thresholded_ix])\n        matches.append(match)\n    \n    # Match zero vector embeddins only with themselves\n    for i in empty_emb_idx:\n        matches[i] = posting_ids[i]\n    \n    return matches","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create embeddings\ndimension = 15000\nvectorizer = TfidfVectorizer(stop_words = 'english', binary = True, max_features = dimension)\ntext_emb = vectorizer.fit_transform(pd.read_csv(data_folder + \"train.csv\").title)\n\ntracker = pd.DataFrame(columns=[\"dimension\", \"n_label_group\", \"n_post\", \"optimum_threshold\", \"score\"], data=np.zeros((25,5)))\n\nprint(\"************************ EMBEDDING SIZE: \", dimension, \"**************************************************\")\nfor folds_before, (search_from, search_to) in enumerate(ranges1):\n    print(\"=\"*50)\n    print(\"All Folds up to Fold:\", folds_before)\n    print(\"=\"*50)\n    valid_emb = text_emb[train.fold <= folds_before,].toarray().astype(np.float32)\n    valid_df = train.loc[train.fold <= folds_before,]\n    n_label_group = valid_df.label_group.nunique()\n    n_post = valid_df.shape[0]\n    print(\"Number of Label Groups: \", n_label_group)\n    print(\"Number of Posts: \", n_post)\n    best_threshold, best_score = get_best_threshold(cosine_find_matches_cupy, valid_emb, valid_df.posting_id.values, valid_df.target.values, np.arange(search_from, search_to, 0.02))\n    tracker.iloc[folds_before,] = (dimension, n_label_group, n_post, best_threshold, best_score)\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create embeddings\ndimension = 10000\nvectorizer = TfidfVectorizer(stop_words = 'english', binary = True, max_features = dimension)\ntext_emb = vectorizer.fit_transform(pd.read_csv(data_folder + \"train.csv\").title)\n\n\n\nprint(\"************************ EMBEDDING SIZE: \", dimension, \"**************************************************\")\nfor folds_before, (search_from, search_to) in enumerate(ranges2):\n    print(\"=\"*50)\n    print(\"All Folds up to Fold:\", folds_before)\n    print(\"=\"*50)\n    valid_emb = text_emb[train.fold <= folds_before,].toarray().astype(np.float32)\n    valid_df = train.loc[train.fold <= folds_before,]\n    n_label_group = valid_df.label_group.nunique()\n    n_post = valid_df.shape[0]\n    print(\"Number of Label Groups: \", n_label_group)\n    print(\"Number of Posts: \", n_post)\n    best_threshold, best_score = get_best_threshold(cosine_find_matches_cupy, valid_emb, valid_df.posting_id.values, valid_df.target.values, np.arange(search_from, search_to,0.02))\n    tracker.iloc[5+folds_before,] = (dimension, n_label_group, n_post, best_threshold, best_score)\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create embeddings\ndimension = 5000\nvectorizer = TfidfVectorizer(stop_words = 'english', binary = True, max_features = dimension)\ntext_emb = vectorizer.fit_transform(pd.read_csv(data_folder + \"train.csv\").title)\n\nprint(\"************************ EMBEDDING SIZE: \", dimension, \"**************************************************\")\nfor folds_before, (search_from, search_to) in enumerate(ranges3):\n    print(\"=\"*50)\n    print(\"All Folds up to Fold:\", folds_before)\n    print(\"=\"*50)\n    valid_emb = text_emb[train.fold <= folds_before,].toarray().astype(np.float32)\n    valid_df = train.loc[train.fold <= folds_before,]\n    n_label_group = valid_df.label_group.nunique()\n    n_post = valid_df.shape[0]\n    print(\"Number of Label Groups: \", n_label_group)\n    print(\"Number of Posts: \", n_post)\n    best_threshold, best_score = get_best_threshold(cosine_find_matches_cupy, valid_emb, valid_df.posting_id.values, valid_df.target.values, np.arange(search_from, search_to,0.02))\n    tracker.iloc[10+folds_before,] = (dimension, n_label_group, n_post, best_threshold, best_score)\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create embeddings\ndimension = 1500\nvectorizer = TfidfVectorizer(stop_words = 'english', binary = True, max_features = dimension)\ntext_emb = vectorizer.fit_transform(pd.read_csv(data_folder + \"train.csv\").title)\n\nprint(\"************************ EMBEDDING SIZE: \", dimension, \"**************************************************\")\nfor folds_before, (search_from, search_to) in enumerate(ranges4):\n    print(\"=\"*50)\n    print(\"All Folds up to Fold:\", folds_before)\n    print(\"=\"*50)\n    valid_emb = text_emb[train.fold <= folds_before,].toarray().astype(np.float32)\n    valid_df = train.loc[train.fold <= folds_before,]\n    n_label_group = valid_df.label_group.nunique()\n    n_post = valid_df.shape[0]\n    print(\"Number of Label Groups: \", n_label_group)\n    print(\"Number of Posts: \", n_post)\n    best_threshold, best_score = get_best_threshold(cosine_find_matches_cupy, valid_emb, valid_df.posting_id.values, valid_df.target.values, np.arange(search_from, search_to, 0.02))\n    tracker.iloc[15+folds_before,] = (dimension, n_label_group, n_post, best_threshold, best_score)\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create embeddings\ndimension = 500\nvectorizer = TfidfVectorizer(stop_words = 'english', binary = True, max_features = dimension)\ntext_emb = vectorizer.fit_transform(pd.read_csv(data_folder + \"train.csv\").title)\n\nprint(\"************************ EMBEDDING SIZE: \", dimension, \"**************************************************\")\nfor folds_before, (search_from, search_to) in enumerate(ranges5):\n    print(\"=\"*50)\n    print(\"All Folds up to Fold:\", folds_before)\n    print(\"=\"*50)\n    valid_emb = text_emb[train.fold <= folds_before,].toarray().astype(np.float32)\n    valid_df = train.loc[train.fold <= folds_before,]\n    n_label_group = valid_df.label_group.nunique()\n    n_post = valid_df.shape[0]\n    print(\"Number of Label Groups: \", n_label_group)\n    print(\"Number of Posts: \", n_post)\n    best_threshold, best_score = get_best_threshold(cosine_find_matches_cupy, valid_emb, valid_df.posting_id.values, valid_df.target.values, np.arange(search_from, search_to, 0.02))\n    tracker.iloc[20+folds_before,] = (dimension, n_label_group, n_post, best_threshold, best_score)\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tracker = tracker.iloc[:25,]\nfig = px.scatter(tracker, x=\"n_post\", y=\"optimum_threshold\", trendline=\"ols\", facet_col=\"dimension\")\nfig.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=’conclusion’></a>\n\n# Results\n- We can observe that optimum threshold and datasize has quite strong linear relationship (R^2 ~= 0.95). We can use that linear model to make an estimate for optimum threshold for 70000 test data size.\n\n- Also comparing different embedding size, we see that increasing embedding size results in a higher value of optimum threshold.\n\n## Example showing how to use above results\nFor Example; If your embedding size is 5000  and calculated optimum threshold with vaildation fold size of 10000 posts. According to the above model, optimum threshold decreases by 4.96362*10^-6 for 1 increase in test set size. Given that hidden test size is 70000. -- slope of the regression line is $-4.96362*10^-6$ for embedding size 5000\n\n$$ (70000 - 10000) * 4.96362*10^-6  = 0.2978 $$\n\n**Hence, decrease your threshold by $0.2978$ to use in your final inference kernel.**\n\n### IMPORTANT NOTE \nThis results are only valid for TF-IDF representation of titles but you can use the same procedure/code to find out the relationship between test size - embedding dimensions and optimum threshold for image embeddings or text embeddings created using other methods.\n\n**FOR THE RESULTS OF SAME EXPERIMENT USING DIFFERENT MODEL EMBEDDINGS (BERT FOR TEXT, EFFNET FOR IMAGE) PLEASE CHECK THIS [NOTEBOOK](https://www.kaggle.com/anlgrbz/embed-type-optimum-threshold-search-inference)**","metadata":{}}]}