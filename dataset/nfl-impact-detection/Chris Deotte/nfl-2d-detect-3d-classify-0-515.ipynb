{"cells":[{"metadata":{},"cell_type":"markdown","source":"# NFL Comp - 9th Place Solution\nThis is the gold medal 9th place final submission inference notebook for team `Deep Impact` ( @vaghefi @cdeotte @theoviel @jinssaa ) solution to Kaggle's NFL Comp. This notebook is explained here: https://www.kaggle.com/c/nfl-impact-detection/discussion/209012\n\nFirst we find potential helmet impacts using a 2D detection model. Second, we classify each potential bbox with a 3D classification model. Lastly, we apply post process to remove false positives. This notebook achieves CV 0.5125, Public LB 0.4931, and 9th Place Private LB 0.5153!\n  \nCode to train the 3D classification models is posted here: https://github.com/TheoViel/nfl_impact_detection"},{"metadata":{},"cell_type":"markdown","source":"# Hyperparameters"},{"metadata":{},"cell_type":"markdown","source":"### Best CV [0.5125]"},{"metadata":{"trusted":true},"cell_type":"code","source":"DEBUG = False\n\nDET_THRESHOLD = 0.35 -0.02\nCLS_THRESHOLD = 0.48\n\n# Change threshold after frame\nSWITCH_FRAME = 150\nDET_THRESHOLD2 = 0.40 -0.02\nCLS_THRESHOLD2 = 0.65\n\n# Lower thresholds for sideline\nDELTA_CLS = -0.07\nDELTA_DET = -0.05\n\n# Adjacency post-processing\nNMS_THRESHOLD = 0.41\nMAX_FRAME_DIST = 9\nN_TIMES = 1\n\n# View post-processing\nMIN_DIST = 4\nVIEW_THRESHOLD = 0.86  \n\n# Boxes expansion\nR = 0.22\n\n# Ensemble\nCLS_3D_NEW = 2 #these are i3d, slowonly, slowfast\nCLS_3D_OLD = 1 # these are three resnet18, and one resnet34\nCLS_2D = 0 #these are three 2d classification models\n\n# Model\nconfig_file = '../input/detectron-full/detectors_cascade_x101_fold_0_4_2_.py'\ncheckpoint_file = '../input/detectron-full/epoch_7.pth'","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Detection Model"},{"metadata":{},"cell_type":"markdown","source":"## Initialization"},{"metadata":{},"cell_type":"markdown","source":"### Packages"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true,"_kg_hide-output":true,"_kg_hide-input":true},"cell_type":"code","source":"!pip install ../input/mmdetectionv260/addict-2.4.0-py3-none-any.whl\n!pip install ../input/mmdetectionv260/mmcv_full-latesttorch1.6.0cu102-cp37-cp37m-manylinux1_x86_64.whl\n!pip install ../input/mmdetectionv260/mmpycocotools-12.0.3-cp37-cp37m-linux_x86_64.whl\n!pip install ../input/mmdetectionv260/mmdet-2.6.0-py3-none-any.whl","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Imports"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"import os, gc\nimport cv2\nimport mmcv\nimport copy\nimport mmdet\nimport torch\nimport random\nimport nflimpact\nimport torchvision\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport os.path as osp\nimport matplotlib.pyplot as plt\n\nfrom glob import glob\nfrom tqdm.notebook import tqdm\nfrom torch.utils.data import DataLoader, Dataset\n\nfrom mmcv.ops import get_compiling_cuda_version, get_compiler_version\nfrom mmcv import Config\n\nfrom mmdet.datasets import build_dataset\nfrom mmdet.models import build_detector\nfrom mmdet.datasets.builder import DATASETS\nfrom mmdet.datasets.custom import CustomDataset\nfrom mmdet.apis import train_detector, set_random_seed, init_detector, inference_detector\n\n%matplotlib inline\nplt.rcParams['figure.dpi'] = 100","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Seeding"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"def seed_everything(seed):\n    \"\"\"\n    Seeds basic parameters for reproductibility of results.\n\n    Args:\n        seed (int): Number of the seed.\n    \"\"\"\n    random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True\n\nseed_everything(2020)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Params"},{"metadata":{"trusted":true},"cell_type":"code","source":"DATA_PATH = \"../input/nfl-impact-detection/\"\nDATA_ROOT_PATH = '/tmp/test_images/'\nTEST_VID_PATH = '../input/nfl-impact-detection/test/'","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Visualization"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"def plot_bboxes(img, boxes, transpose=False):\n    thickness = 2 if img.shape[1] > 400 else 1\n    for i, box in enumerate(boxes):\n        if img.max() > 1:\n            color = (255, 255, 0)\n        else:\n            color = (1, 1, 0)\n\n        if not transpose:\n            cv2.rectangle(\n                img, (box[0], box[1]), (box[2], box[3]), color, thickness=thickness\n            )\n        else:\n            cv2.rectangle(\n                img, (box[1], box[0]), (box[3], box[2]), color, thickness=thickness\n            )\n        plt.text(box[0], box[1] - 5, f'#{i}', c='y', size=9)\n\n    plt.imshow(img)\n\n\ndef visualize_preds(df_pred, idx, root=''):\n    video_name = df_pred['video'][idx]\n    frame = df_pred['frame'][idx]\n    img = f\"{video_name[:-4]}_{frame:04d}.png\"\n    img = cv2.imread(root + img)\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n\n    df = df_pred[df_pred[\"video\"] == video_name]\n    df = df[df['frame'] == frame]\n\n    boxes = df[['left', 'width', 'top', 'height']].values\n    boxes[:, 1] += boxes[:, 0]\n    boxes[:, 3] += boxes[:, 2]\n    boxes = boxes[:, [0, 2, 1, 3]]\n\n    plot_bboxes(img, boxes)\n    plt.title(f'Video {video_name} - frame {frame}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Main"},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"model = init_detector(config_file, checkpoint_file, device='cuda:0')\n\ncfg = model.cfg.copy()\n\nmodel.cfg['test_cfg']['rcnn']['score_thr'] = DET_THRESHOLD - 0.1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_lists = glob('../input/nfl-impact-detection/test/*.mp4')\n\nif not os.path.exists(DATA_ROOT_PATH):\n    !mkdir $DATA_ROOT_PATH\n    \nif DEBUG:\n    TEST_VID_PATH = '../input/nfl-impact-detection/train/'\n    test_lists = [TEST_VID_PATH + '57586_000540_Endzone.mp4'] #, TEST_VID_PATH + '57586_000540_Sideline.mp4']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"images = []\nplayID, gameKey, view, video_name, frame_name, scores = [], [], [], [], [], []\nleft, width, top, height = [], [], [], []\n\n\nfor file_name in tqdm(test_lists):\n    vid_name = file_name.split(\"/\")[-1]\n    video = mmcv.VideoReader(file_name)\n    \n    bboxes = np.empty((0, 4))\n    pred = np.empty(0)\n    frame_num = np.empty(0)\n    \n    for i, frame in enumerate(tqdm(video)):        \n        # Saving frame\n        img_name = f\"{vid_name[:-4]}_{i+1:04d}.png\"\n        cv2.imwrite(DATA_ROOT_PATH + img_name, frame)\n        images.append(img_name)\n        \n        if DEBUG:\n             if i > 50:\n                 break\n        \n        # Inference\n        result = inference_detector(model, frame)\n        if not len(result[0]) == 0:\n            for j in range(len(result[0])):\n                bboxes = np.concatenate((\n                    bboxes,\n                    [[\n                        result[0][j][0] + (result[0][j][2] - result[0][j][0]) / 2,\n                        result[0][j][1] + (result[0][j][3] - result[0][j][1]) / 2,\n                        result[0][j][2] - result[0][j][0],\n                        result[0][j][3] - result[0][j][1],\n                    ]],\n                ))\n\n                pred = np.concatenate((pred, [result[0][j][4]]))\n                frame_num = np.concatenate((frame_num, [i + 1]))\n\n    for k in range(len(bboxes)):\n        gameKey.append(int(file_name.split(\"_\")[0].split(\"/\")[-1]))\n        playID.append(int(file_name.split(\"_\")[1]))\n        view.append(file_name.split(\"_\")[2][:-4])\n        video_name.append(file_name.split(\"/\")[-1])\n        frame_name.append(int(frame_num[k]))\n        scores.append(pred[k])\n        left.append(int(bboxes[k][0] - (bboxes[k][2] / 2)))\n        top.append(int(bboxes[k][1] - (bboxes[k][3] / 2)))\n        width.append(int(bboxes[k][2]))\n        height.append(int(bboxes[k][3]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.DataFrame({\n    'gameKey': gameKey,\n    'playID': playID,\n    'view': view,\n    'video': video_name,\n    'frame': frame_name,\n    'left': left,\n    'width': width,\n    'top': top,\n    'height': height,\n    'scores' : scores} # added detection scores to dataframe\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"if DEBUG:\n    frames = submission['frame'].unique()\n    for frame in frames:\n        df_viz = submission[submission['frame'] == frame].reset_index(drop=True)\n        plt.figure(figsize=(16, 8))\n        visualize_preds(df_viz, 0, DATA_ROOT_PATH)\n        plt.axis(False)\n        plt.show()\n\n        break","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f'Detected {len(submission)} impacts.')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.head(20)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Classification Model"},{"metadata":{},"cell_type":"markdown","source":"## Initialization"},{"metadata":{},"cell_type":"markdown","source":"### Imports"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"import sys\nsys.path = [\n    '../input/efficientnet-pytorch/EfficientNet-PyTorch/EfficientNet-PyTorch-master',\n] + sys.path\n\nimport os\nimport cv2\nimport torch\nimport torchvision\nimport numpy as np\nimport torch.nn as nn\nimport albumentations as albu\nimport torch.nn.functional as F\n\n\nfrom efficientnet_pytorch import EfficientNet\nfrom torch.utils.data import DataLoader, Dataset\nfrom albumentations.pytorch.transforms import ToTensorV2","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Params"},{"metadata":{"trusted":true},"cell_type":"code","source":"NUM_WORKERS = 4\n\nSIZE = 512\nIMG_SHAPE = (720, 1280)\n\nMEAN = np.array([0.485, 0.456, 0.406])\nSTD = np.array([0.229, 0.224, 0.225])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Data"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"df_pred = submission.copy()\n\ndf_pred['image_name'] = (df_pred['video'].str.replace('.mp4', '') + '_' + df_pred['frame'].apply(lambda x: f'{x:04d}') + '.png')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def get_transfos_cls(visualize=False, train=True):\n    if visualize:\n        normalizer = albu.Compose(\n            [\n                ToTensorV2(),\n            ],\n            p=1,\n        )\n    else:\n        normalizer = albu.Compose(\n            [\n                albu.Normalize(mean=MEAN, std=STD),\n                ToTensorV2(),\n            ],\n            p=1,\n        )\n\n    if train:\n        return albu.Compose(\n            [\n                albu.HorizontalFlip(p=0.5),\n                albu.ShiftScaleRotate(\n                    scale_limit=0.5, shift_limit=0.5, rotate_limit=90, p=0.75\n                ),\n                color_transforms(p=0.5),\n                channel_transforms(p=0.2),\n                normalizer,\n            ],\n        )\n\n    else:\n        return normalizer","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def extend_box(box, size=64):\n    w = box[1] - box[0]\n    h = box[3] - box[2]\n    \n    dw = (size - w) / 2\n    dh = (size - h) / 2\n    \n    new_box = [box[0] - np.floor(dw), box[1] + np.ceil(dw), box[2] - np.floor(dh), box[3] + np.ceil(dh)]\n    return np.array(new_box).astype(int)\n\ndef adapt_to_shape(box, shape):\n    if box[0] < 0:\n        box[1] -= box[0]\n        box[0] = 0\n    elif box[1] >= shape[1]:\n        diff = box[1] - shape[1]\n        box[1] -= diff\n        box[0] -= diff\n        \n    if box[2] < 0:\n        box[3] -= box[2]\n        box[2] = 0\n\n    elif box[3] >= shape[0]:\n        diff = box[3] - shape[0]\n        box[3] -= diff\n        box[2] -= diff\n    \n    return box","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"class NFLDatasetClsInference(Dataset):\n    def __init__(self, df, transforms=None, root=\"\"):\n        super().__init__()\n        self.transforms = transforms\n        self.root = root\n        \n        self.images = np.unique(df['image_name'].values)\n        self.images = [cv2.imread(self.root + img) for img in self.images]\n        self.images = [cv2.cvtColor(img, cv2.COLOR_BGR2RGB) for img in self.images]\n        \n        self.frame_to_img = list(np.unique(df['frame'].values))\n        self.frames = df['frame'].values\n        \n        self.boxes = df[['left', 'width', 'top', 'height']].values\n        self.boxes[:, 1] += self.boxes[:, 0]\n        self.boxes[:, 3] += self.boxes[:, 2]\n        \n    def __len__(self):\n        return len(self.boxes)\n\n    def __getitem__(self, idx):\n        frame = self.frame_to_img.index(self.frames[idx])\n        image = self.images[frame]\n        \n        box = extend_box(self.boxes[idx], size=64)\n        box = adapt_to_shape(box, image.shape)\n        \n        image = image[box[2] : box[3], box[0] : box[1]]\n\n        if self.transforms:\n            image = self.transforms(image=image)[\"image\"]\n\n        return image","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_pred_vid = df_pred[df_pred[\"video\"] == df_pred['video'].unique()[0]]\n\ndataset = NFLDatasetClsInference(\n    df_pred_vid,\n    transforms=get_transfos_cls(train=False, visualize=True),\n    root=DATA_ROOT_PATH,\n)\n\nplt.figure(figsize=(10, 10))\nfor i in range(9):\n    \n    plt.subplot(3, 3, i + 1)\n    i = np.random.randint(len(dataset))\n    plt.imshow(dataset[i].numpy().transpose(1, 2, 0))\n    plt.axis(False)\n    \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def load_model_weights(model, filename, verbose=1, cp_folder=\"\"):\n    \"\"\"\n    Loads the weights of a PyTorch model. The exception handles cpu/gpu incompatibilities\n\n    Arguments:\n        model {torch module} -- Model to load the weights to\n        filename {str} -- Name of the checkpoint\n\n    Keyword Arguments:\n        verbose {int} -- Whether to display infos (default: {1})\n        cp_folder {str} -- Folder to load from (default: {''})\n\n    Returns:\n        torch module -- Model with loaded weights\n    \"\"\"\n    if verbose:\n        print(f\"\\n -> Loading weights from {os.path.join(cp_folder,filename)}\\n\")\n    try:\n        model.load_state_dict(os.path.join(cp_folder, filename), strict=False)\n    except BaseException:\n        model.load_state_dict(\n            torch.load(os.path.join(cp_folder, filename), map_location=\"cpu\"),\n            strict=True,\n        )\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"RESNETS = [\n    \"resnet18\",\n    \"resnet34\",\n    \"resnet50\",\n    \"resnet101\",\n    \"resnet152\",\n    \"resnext50_32x4d\",\n    \"resnext101_32x8d\",\n    \"wide_resnet50_2\",\n    \"wide_resnet101_2\",\n]\n\n\ndef get_model_cls(\n    name, num_classes=1,\n):\n    \"\"\"\n    Loads a pretrained model.\n    Supports Resnet based models.\n\n    Args:\n        name (str): Model name\n        num_classes (int, optional): Number of classes. Defaults to 1.\n\n    Raises:\n        NotImplementedError: Specified model name is not supported.\n\n    Returns:\n        torch model -- Pretrained model.\n    \"\"\"\n\n    # Load pretrained model\n    if \"resnest\" in name:\n        model = getattr(resnest_torch, name)(pretrained=False)\n    elif name in RESNETS:\n        model = getattr(torchvision.models, name)(pretrained=False)\n    elif \"efficientnet\" in name:\n        model = EfficientNet.from_name(name)\n    else:\n        raise NotImplementedError\n    model.name = name\n\n    if \"efficientnet\" not in name:\n        model.conv1.stride = (1, 1)\n\n        model.nb_ft = model.fc.in_features\n        model.fc = nn.Linear(model.nb_ft, num_classes)\n\n    else:\n        model._conv_stem.stride = (1, 1)\n\n        model.nb_ft = model._fc.in_features\n        model._fc = nn.Linear(model.nb_ft, num_classes)\n\n    model.num_classes = num_classes\n\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"def retrieve_model(config, fold=0, log_folder=\"\"):\n    model = get_model_cls(\n        config['name'],\n        num_classes=config['num_classes'],\n    ).eval()\n    model.zero_grad()\n\n    model = load_model_weights(\n        model,\n        log_folder + f\"{config['name']}_{fold}.pt\"\n    )\n\n    return model","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Inference"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def inference(df, models, batch_size=256, device=\"cuda\", root=\"\"):\n    models = [model.to(device).eval() for model in models]\n\n    dataset = dataset = NFLDatasetClsInference(\n        df.copy(),\n        transforms=get_transfos_cls(train=False),\n        root=root,\n    )\n\n    loader = DataLoader(\n        dataset,\n        batch_size=batch_size,\n        shuffle=False,\n        num_workers=NUM_WORKERS,\n        pin_memory=True,\n    )\n\n    preds = []\n    with torch.no_grad():\n        for img in loader:\n            img = img.to(device)\n            preds_img = []\n            \n            for model in models:\n                y_pred = model(img)\n                preds_img.append(y_pred.sigmoid().detach().cpu().numpy())\n                \n            preds.append(np.mean(preds_img, 0))\n            \n    return np.concatenate(preds)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Main"},{"metadata":{},"cell_type":"markdown","source":"### Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_pred[\"image_name\"] = df_pred['video'].str.replace('.mp4', '') + '_' + df_pred['frame'].apply(lambda x: f'{x:04d}') + '.png'\n\nimages = df_pred[\"image_name\"].unique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Models"},{"metadata":{"trusted":true},"cell_type":"code","source":"CP_FOLDER = \"../input/nfl-dataset-1/\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"configs = {\n    \"efficientnet-b3\": {\n        \"name\": \"efficientnet-b3\",\n        \"num_classes\": 1,\n        \"k\": 5,\n    },\n#     \"efficientnet-b2\": {\n#         \"name\": \"efficientnet-b2\",\n#         \"num_classes\": 1,\n#         \"k\": 5,\n#     },\n    \"efficientnet-b1\": {\n        \"name\": \"efficientnet-b1\",\n        \"num_classes\": 1,\n        \"k\": 5,\n    },\n#     \"efficientnet-b4\": {\n#         \"name\": \"efficientnet-b4\",\n#         \"num_classes\": 1,\n#         \"k\": 5,\n#     },\n    \"resnet18\": {\n        \"name\": \"resnet18\",\n        \"num_classes\": 1,\n        \"k\": 5,\n    },\n#     \"resnet34\": {\n#         \"name\": \"resnet34\",\n#         \"num_classes\": 1,\n#         \"k\": 5,\n#     },\n}\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if CLS_2D>0:\n    models = []\n    for model in configs:\n        n = 1 if DEBUG else configs[model]['k']\n        models += [retrieve_model(configs[model], fold=k, log_folder=CP_FOLDER) for k in range(n)]\n#       models += [retrieve_model(configs[model], fold=0, log_folder=CP_FOLDER)]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Inference"},{"metadata":{"trusted":true},"cell_type":"code","source":"if CLS_2D>0:\n    preds = []\n    for vid in tqdm(df_pred['video'].unique()):\n        df_pred_vid = df_pred[df_pred[\"video\"] == vid]\n\n        pred = inference(df_pred_vid, models, root=DATA_ROOT_PATH)\n        preds.append(pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if CLS_2D>0:\n    df_pred['pred_cls'] = np.concatenate(preds)\nelse:\n    df_pred['pred_cls'] = 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if CLS_2D>0:\n    del pred, preds, vid, df_pred_vid, models\n    _ = gc.collect()\n    torch.cuda.empty_cache()\n    print('Cleared GPU VRAM')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3D Classification Models"},{"metadata":{"trusted":true},"cell_type":"code","source":"import sys\nsys.path.append('../input/mmaction2/mmaction2/mmaction2')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"def extend_box(box, size=64):\n    w = box[1] - box[0]\n    h = box[3] - box[2]\n\n    dw = (size - w) / 2\n    dh = (size - h) / 2\n\n    new_box = [\n        box[0] - np.floor(dw),\n        box[1] + np.ceil(dw),\n        box[2] - np.floor(dh),\n        box[3] + np.ceil(dh),\n    ]\n    return np.array(new_box).astype(int)\n\n\ndef adapt_to_shape(box, shape):\n    if box[0] < 0:\n        box[1] -= box[0]\n        box[0] = 0\n    elif box[1] >= shape[1]:\n        diff = box[1] - shape[1]\n        box[1] -= diff\n        box[0] -= diff\n\n    if box[2] < 0:\n        box[3] -= box[2]\n        box[2] = 0\n\n    elif box[3] >= shape[0]:\n        diff = box[3] - shape[0]\n        box[3] -= diff\n        box[2] -= diff\n\n    return box\n\n\ndef get_adjacent_frames(frame, max_frame=100, n_frames=9, stride=1):\n    frames = np.arange(n_frames) * stride\n    frames = frames - frames[n_frames // 2] + frame\n\n    if frames.min() < 1:\n        frames -= frames.min() - 1\n    elif frames.max() > max_frame:\n        frames += max_frame - frames.max()\n\n    return frames","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"class NFLDatasetClsInference3D(Dataset):\n    def __init__(self, df, n_frames=9, stride=2, visualize=False, root=\"\"):\n        super().__init__()\n        self.n_frames = n_frames\n        self.stride = stride\n        self.visualize = visualize\n        self.root = root\n\n        self.max_frame = df['nb_frame'].values[0]\n\n        self.images = []\n\n        image_name = df['image_name'].values[0].split('.')[0][:-4]\n        self.images = [image_name + f'{f:04d}.png' for f in range(1, self.max_frame + 1)]\n\n        self.images = [cv2.imread(self.root + img) for img in self.images]\n        self.images = [cv2.cvtColor(img, cv2.COLOR_BGR2RGB) for img in self.images]\n\n        self.frame_to_img = list(df[\"frame\"].unique())\n        self.frames = df[\"frame\"].values\n\n        self.boxes = df[[\"left\", \"width\", \"top\", \"height\"]].values\n        self.boxes[:, 1] += self.boxes[:, 0]\n        self.boxes[:, 3] += self.boxes[:, 2]\n\n    def __len__(self):\n        return len(self.boxes)\n\n    def __getitem__(self, idx):\n        frame = self.frames[idx]\n        frames = get_adjacent_frames(\n            frame, max_frame=self.max_frame, n_frames=self.n_frames, stride=self.stride\n        )\n\n        image = [self.images[f - 1] for f in frames]\n        image = np.array(image)\n\n        box = extend_box(self.boxes[idx], size=64)\n        box = adapt_to_shape(box, image.shape[1:])\n\n        image = image[:, box[2]: box[3], box[0]: box[1]]\n\n        if not self.visualize:\n            image = (image / 255 - 0.5) / 0.5\n            image = image.transpose(3, 0, 1, 2)\n            image = torch.from_numpy(image).float()\n\n        return image","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"CP_PATHS = {\n    \"resnet18\": DATA_PATH + \"weights/r3d18_KM_200ep.pth\",\n    \"resnet34\": DATA_PATH + \"weights/r3d34_KM_200ep.pth\",\n    \"resnet50\": DATA_PATH + \"weights/r3d50_KMS_200ep.pth\",\n    \"i3d\": DATA_PATH + \"weights/rgb_imagenet.pt\",\n    \"slowfast\": DATA_PATH + \"weights/slowfast_r50_256p_4x16x1_256e_kinetics400_rgb.pth\",\n    \"slowonly\": DATA_PATH + \"weights/slowonly_r50_omni_4x16x1_kinetics400_rgb.pth\",\n}\n\nCONFIGS = {\n    \"slowfast\": dict(\n        type=\"Recognizer3D\",\n        backbone=dict(\n            type=\"ResNet3dSlowFast\",\n            pretrained=None,\n            resample_rate=8,  # tau\n            speed_ratio=8,  # alpha\n            channel_ratio=8,  # beta_inv\n            slow_pathway=dict(\n                type=\"resnet3d\",\n                depth=50,\n                pretrained=None,\n                lateral=True,\n                conv1_kernel=(1, 7, 7),\n                dilations=(1, 1, 1, 1),\n                conv1_stride_t=1,\n                pool1_stride_t=1,\n                inflate=(0, 0, 1, 1),\n                norm_eval=False,\n            ),\n            fast_pathway=dict(\n                type=\"resnet3d\",\n                depth=50,\n                pretrained=None,\n                lateral=False,\n                base_channels=8,\n                conv1_kernel=(5, 7, 7),\n                conv1_stride_t=1,\n                pool1_stride_t=1,\n                norm_eval=False,\n            ),\n        ),\n        cls_head=dict(\n            type=\"SlowFastHead\",\n            in_channels=2304,  # 2048+256\n            num_classes=400,\n            spatial_type=\"avg\",\n            dropout_ratio=0.5,\n        ),\n    ),\n    \"slowonly\": dict(\n        type=\"Recognizer3D\",\n        backbone=dict(\n            type=\"ResNet3dSlowOnly\",\n            depth=50,\n            pretrained=None,\n            lateral=False,\n            conv1_kernel=(1, 7, 7),\n            conv1_stride_t=1,\n            pool1_stride_t=1,\n            inflate=(0, 0, 1, 1),\n            norm_eval=False,\n        ),\n        cls_head=dict(\n            type=\"I3DHead\",\n            in_channels=2048,\n            num_classes=400,\n            spatial_type=\"avg\",\n            dropout_ratio=0.5,\n        ),\n    ),\n}\n","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n# import numpy as np\n\n\nclass MaxPool3dSamePadding(nn.MaxPool3d):\n    def compute_pad(self, dim, s):\n        if s % self.stride[dim] == 0:\n            return max(self.kernel_size[dim] - self.stride[dim], 0)\n        else:\n            return max(self.kernel_size[dim] - (s % self.stride[dim]), 0)\n\n    def forward(self, x):\n        # compute 'same' padding\n        (batch, channel, t, h, w) = x.size()\n        # print t,h,w\n        # out_t = np.ceil(float(t) / float(self.stride[0]))\n        # out_h = np.ceil(float(h) / float(self.stride[1]))\n        # out_w = np.ceil(float(w) / float(self.stride[2]))\n        # print out_t, out_h, out_w\n        pad_t = self.compute_pad(0, t)\n        pad_h = self.compute_pad(1, h)\n        pad_w = self.compute_pad(2, w)\n        # print pad_t, pad_h, pad_w\n\n        pad_t_f = pad_t // 2\n        pad_t_b = pad_t - pad_t_f\n        pad_h_f = pad_h // 2\n        pad_h_b = pad_h - pad_h_f\n        pad_w_f = pad_w // 2\n        pad_w_b = pad_w - pad_w_f\n\n        pad = (pad_w_f, pad_w_b, pad_h_f, pad_h_b, pad_t_f, pad_t_b)\n        # print x.size()\n        # print pad\n        x = F.pad(x, pad)\n        return super(MaxPool3dSamePadding, self).forward(x)\n\n\nclass Unit3D(nn.Module):\n    def __init__(\n        self,\n        in_channels,\n        output_channels,\n        kernel_shape=(1, 1, 1),\n        stride=(1, 1, 1),\n        padding=0,\n        activation_fn=F.relu,\n        use_batch_norm=True,\n        use_bias=False,\n        name=\"unit_3d\",\n    ):\n\n        \"\"\"Initializes Unit3D module.\"\"\"\n        super(Unit3D, self).__init__()\n\n        self._output_channels = output_channels\n        self._kernel_shape = kernel_shape\n        self._stride = stride\n        self._use_batch_norm = use_batch_norm\n        self._activation_fn = activation_fn\n        self._use_bias = use_bias\n        self.name = name\n        self.padding = padding\n\n        self.conv3d = nn.Conv3d(\n            in_channels=in_channels,\n            out_channels=self._output_channels,\n            kernel_size=self._kernel_shape,\n            stride=self._stride,\n            padding=0,  # we always want padding to be 0 here.\n            bias=self._use_bias,\n        )\n\n        if self._use_batch_norm:\n            self.bn = nn.BatchNorm3d(self._output_channels, eps=0.001, momentum=0.01)\n\n    def compute_pad(self, dim, s):\n        if s % self._stride[dim] == 0:\n            return max(self._kernel_shape[dim] - self._stride[dim], 0)\n        else:\n            return max(self._kernel_shape[dim] - (s % self._stride[dim]), 0)\n\n    def forward(self, x):\n        # compute 'same' padding\n        (batch, channel, t, h, w) = x.size()\n        # print t,h,w\n        # out_t = np.ceil(float(t) / float(self._stride[0]))\n        # out_h = np.ceil(float(h) / float(self._stride[1]))\n        # out_w = np.ceil(float(w) / float(self._stride[2]))\n        # print out_t, out_h, out_w\n        pad_t = self.compute_pad(0, t)\n        pad_h = self.compute_pad(1, h)\n        pad_w = self.compute_pad(2, w)\n        # print pad_t, pad_h, pad_w\n\n        pad_t_f = pad_t // 2\n        pad_t_b = pad_t - pad_t_f\n        pad_h_f = pad_h // 2\n        pad_h_b = pad_h - pad_h_f\n        pad_w_f = pad_w // 2\n        pad_w_b = pad_w - pad_w_f\n\n        pad = (pad_w_f, pad_w_b, pad_h_f, pad_h_b, pad_t_f, pad_t_b)\n        # print x.size()\n        # print pad\n        x = F.pad(x, pad)\n        # print x.size()\n\n        x = self.conv3d(x)\n        if self._use_batch_norm:\n            x = self.bn(x)\n        if self._activation_fn is not None:\n            x = self._activation_fn(x)\n        return x\n\n\nclass InceptionModule(nn.Module):\n    def __init__(self, in_channels, out_channels, name):\n        super(InceptionModule, self).__init__()\n\n        self.b0 = Unit3D(\n            in_channels=in_channels,\n            output_channels=out_channels[0],\n            kernel_shape=[1, 1, 1],\n            padding=0,\n            name=name + \"/Branch_0/Conv3d_0a_1x1\",\n        )\n        self.b1a = Unit3D(\n            in_channels=in_channels,\n            output_channels=out_channels[1],\n            kernel_shape=[1, 1, 1],\n            padding=0,\n            name=name + \"/Branch_1/Conv3d_0a_1x1\",\n        )\n        self.b1b = Unit3D(\n            in_channels=out_channels[1],\n            output_channels=out_channels[2],\n            kernel_shape=[3, 3, 3],\n            name=name + \"/Branch_1/Conv3d_0b_3x3\",\n        )\n        self.b2a = Unit3D(\n            in_channels=in_channels,\n            output_channels=out_channels[3],\n            kernel_shape=[1, 1, 1],\n            padding=0,\n            name=name + \"/Branch_2/Conv3d_0a_1x1\",\n        )\n        self.b2b = Unit3D(\n            in_channels=out_channels[3],\n            output_channels=out_channels[4],\n            kernel_shape=[3, 3, 3],\n            name=name + \"/Branch_2/Conv3d_0b_3x3\",\n        )\n        self.b3a = MaxPool3dSamePadding(\n            kernel_size=[3, 3, 3], stride=(1, 1, 1), padding=0\n        )\n        self.b3b = Unit3D(\n            in_channels=in_channels,\n            output_channels=out_channels[5],\n            kernel_shape=[1, 1, 1],\n            padding=0,\n            name=name + \"/Branch_3/Conv3d_0b_1x1\",\n        )\n        self.name = name\n\n    def forward(self, x):\n        b0 = self.b0(x)\n        b1 = self.b1b(self.b1a(x))\n        b2 = self.b2b(self.b2a(x))\n        b3 = self.b3b(self.b3a(x))\n        return torch.cat([b0, b1, b2, b3], dim=1)\n\n\nclass InceptionI3d(nn.Module):\n    \"\"\"Inception-v1 I3D architecture.\n    The model is introduced in:\n        Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset\n        Joao Carreira, Andrew Zisserman\n        https://arxiv.org/pdf/1705.07750v1.pdf.\n    See also the Inception architecture, introduced in:\n        Going deeper with convolutions\n        Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed,\n        Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, Andrew Rabinovich.\n        http://arxiv.org/pdf/1409.4842v1.pdf.\n    \"\"\"\n\n    # Endpoints of the model in order. During construction, all the endpoints up\n    # to a designated `final_endpoint` are returned in a dictionary as the\n    # second return value.\n    VALID_ENDPOINTS = (\n        \"Conv3d_1a_7x7\",\n        \"MaxPool3d_2a_3x3\",\n        \"Conv3d_2b_1x1\",\n        \"Conv3d_2c_3x3\",\n        \"MaxPool3d_3a_3x3\",\n        \"Mixed_3b\",\n        \"Mixed_3c\",\n        \"MaxPool3d_4a_3x3\",\n        \"Mixed_4b\",\n        \"Mixed_4c\",\n        \"Mixed_4d\",\n        \"Mixed_4e\",\n        \"Mixed_4f\",\n        \"MaxPool3d_5a_2x2\",\n        \"Mixed_5b\",\n        \"Mixed_5c\",\n        \"Logits\",\n        \"Predictions\",\n    )\n\n    def __init__(\n        self,\n        num_classes=400,\n        spatial_squeeze=True,\n        final_endpoint=\"Logits\",\n        name=\"inception_i3d\",\n        in_channels=3,\n        dropout_keep_prob=0.5,\n    ):\n        \"\"\"Initializes I3D model instance.\n        Args:\n          num_classes: The number of outputs in the logit layer (default 400, which\n              matches the Kinetics dataset).\n          spatial_squeeze: Whether to squeeze the spatial dimensions for the logits\n              before returning (default True).\n          final_endpoint: The model contains many possible endpoints.\n              `final_endpoint` specifies the last endpoint for the model to be built\n              up to. In addition to the output at `final_endpoint`, all the outputs\n              at endpoints up to `final_endpoint` will also be returned, in a\n              dictionary. `final_endpoint` must be one of\n              InceptionI3d.VALID_ENDPOINTS (default 'Logits').\n          name: A string (optional). The name of this module.\n        Raises:\n          ValueError: if `final_endpoint` is not recognized.\n        \"\"\"\n\n        if final_endpoint not in self.VALID_ENDPOINTS:\n            raise ValueError(\"Unknown final endpoint %s\" % final_endpoint)\n\n        super(InceptionI3d, self).__init__()\n        self._num_classes = num_classes\n        self._spatial_squeeze = spatial_squeeze\n        self._final_endpoint = final_endpoint\n        self.logits = None\n\n        if self._final_endpoint not in self.VALID_ENDPOINTS:\n            raise ValueError(\"Unknown final endpoint %s\" % self._final_endpoint)\n\n        self.end_points = {}\n        end_point = \"Conv3d_1a_7x7\"\n        self.end_points[end_point] = Unit3D(\n            in_channels=in_channels,\n            output_channels=64,\n            kernel_shape=[7, 7, 7],\n            stride=(2, 2, 2),\n            padding=(3, 3, 3),\n            name=name + end_point,\n        )\n        if self._final_endpoint == end_point:\n            return\n\n        end_point = \"MaxPool3d_2a_3x3\"\n        self.end_points[end_point] = MaxPool3dSamePadding(\n            kernel_size=[1, 3, 3], stride=(1, 2, 2), padding=0\n        )\n        if self._final_endpoint == end_point:\n            return\n\n        end_point = \"Conv3d_2b_1x1\"\n        self.end_points[end_point] = Unit3D(\n            in_channels=64,\n            output_channels=64,\n            kernel_shape=[1, 1, 1],\n            padding=0,\n            name=name + end_point,\n        )\n        if self._final_endpoint == end_point:\n            return\n\n        end_point = \"Conv3d_2c_3x3\"\n        self.end_points[end_point] = Unit3D(\n            in_channels=64,\n            output_channels=192,\n            kernel_shape=[3, 3, 3],\n            padding=1,\n            name=name + end_point,\n        )\n        if self._final_endpoint == end_point:\n            return\n\n        end_point = \"MaxPool3d_3a_3x3\"\n        self.end_points[end_point] = MaxPool3dSamePadding(\n            kernel_size=[1, 3, 3], stride=(1, 2, 2), padding=0\n        )\n        if self._final_endpoint == end_point:\n            return\n\n        end_point = \"Mixed_3b\"\n        self.end_points[end_point] = InceptionModule(\n            192, [64, 96, 128, 16, 32, 32], name + end_point\n        )\n        if self._final_endpoint == end_point:\n            return\n\n        end_point = \"Mixed_3c\"\n        self.end_points[end_point] = InceptionModule(\n            256, [128, 128, 192, 32, 96, 64], name + end_point\n        )\n        if self._final_endpoint == end_point:\n            return\n\n        end_point = \"MaxPool3d_4a_3x3\"\n        self.end_points[end_point] = MaxPool3dSamePadding(\n            kernel_size=[3, 3, 3], stride=(2, 2, 2), padding=0\n        )\n        if self._final_endpoint == end_point:\n            return\n\n        end_point = \"Mixed_4b\"\n        self.end_points[end_point] = InceptionModule(\n            128 + 192 + 96 + 64, [192, 96, 208, 16, 48, 64], name + end_point\n        )\n        if self._final_endpoint == end_point:\n            return\n\n        end_point = \"Mixed_4c\"\n        self.end_points[end_point] = InceptionModule(\n            192 + 208 + 48 + 64, [160, 112, 224, 24, 64, 64], name + end_point\n        )\n        if self._final_endpoint == end_point:\n            return\n\n        end_point = \"Mixed_4d\"\n        self.end_points[end_point] = InceptionModule(\n            160 + 224 + 64 + 64, [128, 128, 256, 24, 64, 64], name + end_point\n        )\n        if self._final_endpoint == end_point:\n            return\n\n        end_point = \"Mixed_4e\"\n        self.end_points[end_point] = InceptionModule(\n            128 + 256 + 64 + 64, [112, 144, 288, 32, 64, 64], name + end_point\n        )\n        if self._final_endpoint == end_point:\n            return\n\n        end_point = \"Mixed_4f\"\n        self.end_points[end_point] = InceptionModule(\n            112 + 288 + 64 + 64, [256, 160, 320, 32, 128, 128], name + end_point\n        )\n        if self._final_endpoint == end_point:\n            return\n\n        end_point = \"MaxPool3d_5a_2x2\"\n        self.end_points[end_point] = MaxPool3dSamePadding(\n            kernel_size=[2, 2, 2], stride=(2, 2, 2), padding=0\n        )\n        if self._final_endpoint == end_point:\n            return\n\n        end_point = \"Mixed_5b\"\n        self.end_points[end_point] = InceptionModule(\n            256 + 320 + 128 + 128, [256, 160, 320, 32, 128, 128], name + end_point\n        )\n        if self._final_endpoint == end_point:\n            return\n\n        end_point = \"Mixed_5c\"\n        self.end_points[end_point] = InceptionModule(\n            256 + 320 + 128 + 128, [384, 192, 384, 48, 128, 128], name + end_point\n        )\n        if self._final_endpoint == end_point:\n            return\n\n        end_point = \"Logits\"\n        self.avg_pool = nn.AdaptiveAvgPool3d((1, 1, 1))\n\n        self.dropout = nn.Dropout(dropout_keep_prob)\n        self.logits = Unit3D(\n            in_channels=384 + 384 + 128 + 128,\n            output_channels=self._num_classes,\n            kernel_shape=[1, 1, 1],\n            padding=0,\n            activation_fn=None,\n            use_batch_norm=False,\n            use_bias=True,\n            name=\"logits\",\n        )\n\n        self.build()\n\n    def replace_logits(self, num_classes):\n        self._num_classes = num_classes\n        self.logits = Unit3D(\n            in_channels=384 + 384 + 128 + 128,\n            output_channels=self._num_classes,\n            kernel_shape=[1, 1, 1],\n            padding=0,\n            activation_fn=None,\n            use_batch_norm=False,\n            use_bias=True,\n            name=\"logits\",\n        )\n\n    def build(self):\n        for k in self.end_points.keys():\n            self.add_module(k, self.end_points[k])\n\n    def forward(self, x):\n        x = self.extract_features(x)\n        x = self.dropout(x.view(x.size(0), -1))\n\n        y = self.logits(x)\n        if self.num_classes_aux > 0:\n            y_aux = self.fc_aux(x)\n            return y, y_aux\n\n        return y, 0\n\n    def extract_features(self, x):\n        for end_point in self.VALID_ENDPOINTS:\n            if end_point in self.end_points:\n                x = self._modules[end_point](x)\n\n        # print(x.size())\n        return self.avg_pool(x)\n","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"from functools import partial\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\ndef get_inplanes():\n    return [64, 128, 256, 512]\n\n\ndef conv3x3x3(in_planes, out_planes, stride=1):\n    return nn.Conv3d(\n        in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False\n    )\n\n\ndef conv1x1x1(in_planes, out_planes, stride=1):\n    return nn.Conv3d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n\n\nclass BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, in_planes, planes, stride=1, downsample=None):\n        super().__init__()\n\n        self.conv1 = conv3x3x3(in_planes, planes, stride)\n        self.bn1 = nn.BatchNorm3d(planes)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = conv3x3x3(planes, planes)\n        self.bn2 = nn.BatchNorm3d(planes)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\nclass Bottleneck(nn.Module):\n    expansion = 4\n\n    def __init__(self, in_planes, planes, stride=1, downsample=None):\n        super().__init__()\n\n        self.conv1 = conv1x1x1(in_planes, planes)\n        self.bn1 = nn.BatchNorm3d(planes)\n        self.conv2 = conv3x3x3(planes, planes, stride)\n        self.bn2 = nn.BatchNorm3d(planes)\n        self.conv3 = conv1x1x1(planes, planes * self.expansion)\n        self.bn3 = nn.BatchNorm3d(planes * self.expansion)\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\nclass ResNet(nn.Module):\n    def __init__(\n        self,\n        block,\n        layers,\n        block_inplanes,\n        n_input_channels=3,\n        conv1_t_size=7,\n        conv1_t_stride=1,\n        no_max_pool=False,\n        shortcut_type=\"B\",\n        widen_factor=1.0,\n        n_classes=400,\n    ):\n        super().__init__()\n\n        block_inplanes = [int(x * widen_factor) for x in block_inplanes]\n\n        self.in_planes = block_inplanes[0]\n        self.no_max_pool = no_max_pool\n\n        self.conv1 = nn.Conv3d(\n            n_input_channels,\n            self.in_planes,\n            kernel_size=(conv1_t_size, 7, 7),\n            stride=(conv1_t_stride, 2, 2),\n            padding=(conv1_t_size // 2, 3, 3),\n            bias=False,\n        )\n        self.bn1 = nn.BatchNorm3d(self.in_planes)\n        self.relu = nn.ReLU(inplace=True)\n        self.maxpool = nn.MaxPool3d(kernel_size=3, stride=2, padding=1)\n        self.layer1 = self._make_layer(\n            block, block_inplanes[0], layers[0], shortcut_type\n        )\n        self.layer2 = self._make_layer(\n            block, block_inplanes[1], layers[1], shortcut_type, stride=2\n        )\n        self.layer3 = self._make_layer(\n            block, block_inplanes[2], layers[2], shortcut_type, stride=2\n        )\n        self.layer4 = self._make_layer(\n            block, block_inplanes[3], layers[3], shortcut_type, stride=2\n        )\n\n        self.avgpool = nn.AdaptiveAvgPool3d((1, 1, 1))\n        self.fc = nn.Linear(block_inplanes[3] * block.expansion, n_classes)\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv3d):\n                nn.init.kaiming_normal_(m.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n            elif isinstance(m, nn.BatchNorm3d):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n\n    def _downsample_basic_block(self, x, planes, stride):\n        out = F.avg_pool3d(x, kernel_size=1, stride=stride)\n        zero_pads = torch.zeros(\n            out.size(0), planes - out.size(1), out.size(2), out.size(3), out.size(4)\n        )\n        if isinstance(out.data, torch.cuda.FloatTensor):\n            zero_pads = zero_pads.cuda()\n\n        out = torch.cat([out.data, zero_pads], dim=1)\n\n        return out\n\n    def _make_layer(self, block, planes, blocks, shortcut_type, stride=1):\n        downsample = None\n        if stride != 1 or self.in_planes != planes * block.expansion:\n            if shortcut_type == \"A\":\n                downsample = partial(\n                    self._downsample_basic_block,\n                    planes=planes * block.expansion,\n                    stride=stride,\n                )\n            else:\n                downsample = nn.Sequential(\n                    conv1x1x1(self.in_planes, planes * block.expansion, stride),\n                    nn.BatchNorm3d(planes * block.expansion),\n                )\n\n        layers = []\n        layers.append(\n            block(\n                in_planes=self.in_planes,\n                planes=planes,\n                stride=stride,\n                downsample=downsample,\n            )\n        )\n        self.in_planes = planes * block.expansion\n        for i in range(1, blocks):\n            layers.append(block(self.in_planes, planes))\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        if not self.no_max_pool:\n            x = self.maxpool(x)\n\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n\n        x = self.avgpool(x)\n\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n\n        return x\n\n\ndef generate_model(model_depth, **kwargs):\n    assert model_depth in [10, 18, 34, 50, 101, 152, 200]\n\n    if model_depth == 10:\n        model = ResNet(BasicBlock, [1, 1, 1, 1], get_inplanes(), **kwargs)\n    elif model_depth == 18:\n        model = ResNet(BasicBlock, [2, 2, 2, 2], get_inplanes(), **kwargs)\n    elif model_depth == 34:\n        model = ResNet(BasicBlock, [3, 4, 6, 3], get_inplanes(), **kwargs)\n    elif model_depth == 50:\n        model = ResNet(Bottleneck, [3, 4, 6, 3], get_inplanes(), **kwargs)\n    elif model_depth == 101:\n        model = ResNet(Bottleneck, [3, 4, 23, 3], get_inplanes(), **kwargs)\n    elif model_depth == 152:\n        model = ResNet(Bottleneck, [3, 8, 36, 3], get_inplanes(), **kwargs)\n    elif model_depth == 200:\n        model = ResNet(Bottleneck, [3, 24, 36, 3], get_inplanes(), **kwargs)\n\n    return model\n\ndef forward_with_aux_resnet_3d(self, x):\n    x = self.conv1(x)\n    x = self.bn1(x)\n    x = self.relu(x)\n    if not self.no_max_pool:\n        x = self.maxpool(x)\n\n    x = self.layer1(x)\n    x = self.layer2(x)\n    x = self.layer3(x)\n    x = self.layer4(x)\n\n    x = self.avgpool(x)\n\n    x = x.view(x.size(0), -1)\n    y = self.fc(x)\n\n    if self.num_classes_aux > 0:\n        y_aux = self.fc_aux(x)\n        return y, y_aux\n\n    return y, 0","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"def forward_slowfast(self, x):\n    x = x[:, :, 1:, :, :]  # size BS x 8 x C x W x H\n    ft1, ft2 = self.extract_feat(x)\n\n    ft1 = self.avg_pool(ft1).view(x.size(0), -1)\n    ft2 = self.avg_pool(ft2).view(x.size(0), -1)\n\n    ft = torch.cat([ft1, ft2], -1)\n    ft = self.dropout(ft)\n\n    y = self.fc(ft)\n\n    if self.num_classes_aux > 0:\n        y_aux = self.fc_aux(ft)\n        return y, y_aux\n\n    return y, 0\n\n\ndef forward_slowonly(self, x):\n    ft = self.extract_feat(x)\n    # print(ft.size())\n\n    ft = self.avg_pool(ft).view(x.size(0), -1)\n    ft = self.dropout(ft)\n\n    y = self.fc(ft)\n\n    if self.num_classes_aux > 0:\n        y_aux = self.fc_aux(ft)\n        return y, y_aux\n\n    return y, 0","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"import mmaction\nfrom mmaction.models import build_model\nfrom mmcv.runner import load_checkpoint\n\n\ndef get_model_cls_3d(name, num_classes=1, num_classes_aux=0, pretrained=False):\n    \"\"\"\n    Loads a pretrained model.\n    Supports Resnet based models.\n\n    Args:\n        name (str): Model name\n        num_classes (int, optional): Number of classes. Defaults to 1.\n\n    Raises:\n        NotImplementedError: Specified model name is not supported.\n\n    Returns:\n        torch model -- Pretrained model.\n    \"\"\"\n\n    # Load pretrained model\n    if \"resnet\" in name:\n        n_classes = 1139 if \"KMS\" in CP_PATHS[name] else 1039\n        depth = int(name[-2:])\n        model = generate_model(depth, n_classes=n_classes)\n\n        if pretrained:\n            load_model_weights_3d(model, CP_PATHS[name])\n    elif name == \"i3d\":  # i3d\n        model = InceptionI3d(num_classes=400, in_channels=3)\n\n        if pretrained:\n            load_model_weights(model, CP_PATHS[name])\n\n    elif name in [\"slowfast\", \"slowonly\"]:\n        model = build_model(CONFIGS[name])\n\n        if pretrained:\n            print(f'\\n -> Loading weighs from \"{CP_PATHS[name]}\"\\n')\n            load_checkpoint(model, CP_PATHS[name])\n    else:\n        raise NotImplementedError\n\n    model.name = name\n    model.num_classes = num_classes\n    model.num_classes_aux = num_classes_aux\n\n    if \"resnet\" in name:\n        # Strides\n        model.conv1.stride = (1, 1, 1)\n        model.layer2[0].conv1.stride = (1, 2, 2)\n        model.layer2[0].downsample[0].stride = (1, 2, 2)\n        model.layer3[0].conv1.stride = (1, 2, 2)\n        model.layer3[0].downsample[0].stride = (1, 2, 2)\n        model.layer4[0].conv1.stride = (1, 2, 2)\n        model.layer4[0].downsample[0].stride = (1, 2, 2)\n        model.maxpool.stride = (1, 2, 2)\n\n        model.nb_ft = model.fc.in_features\n        model.fc = nn.Linear(model.nb_ft, num_classes)\n        model.forward = lambda x: forward_with_aux_resnet_3d(model, x)\n\n        if num_classes_aux:\n            model.fc_aux = nn.Linear(model.nb_ft, num_classes_aux)\n\n    elif name == \"i3d\":\n        model.Conv3d_1a_7x7.conv3d.stride = (1, 1, 1)\n        # model.MaxPool3d_2a_3x3.stride = (1, 1, 1)\n        model.MaxPool3d_4a_3x3.stride = (1, 2, 2)\n        model.MaxPool3d_5a_2x2.stride = (1, 2, 2)\n\n        model.nb_ft = model.logits.conv3d.in_channels\n        model.logits = nn.Linear(model.nb_ft, num_classes)\n\n        if num_classes_aux:\n            model.fc_aux = nn.Linear(model.nb_ft, num_classes_aux)\n\n    elif name == \"slowfast\":\n        model.backbone.slow_path.conv1.stride = (1, 1, 1)\n        model.backbone.fast_path.conv1.stride = (1, 1, 1)\n\n        model.backbone.slow_path.maxpool.stride = (1, 1, 1)\n        model.backbone.fast_path.maxpool.stride = (1, 1, 1)\n\n        model.backbone.slow_path.pool2.stride = (1, 1, 1)\n        model.backbone.fast_path.pool2.stride = (1, 1, 1)\n\n        model.dropout = nn.Dropout(0.5)\n        model.avg_pool = nn.AdaptiveAvgPool3d((1, 1, 1))\n\n        model.nb_ft = model.cls_head.fc_cls.in_features\n\n        model.fc = nn.Linear(model.nb_ft, num_classes)\n        model.forward = lambda x: forward_slowfast(model, x)\n\n        if num_classes_aux:\n            model.fc_aux = nn.Linear(model.nb_ft, num_classes_aux)\n\n    else:\n        model.backbone.conv1.stride = (1, 1, 1)\n        model.backbone.pool2.stride = (1, 1, 1)\n        model.backbone.maxpool.stride = (1, 1, 1)\n\n        model.dropout = nn.Dropout(0.5)\n        model.avg_pool = nn.AdaptiveAvgPool3d((1, 1, 1))\n\n        model.nb_ft = model.cls_head.fc_cls.in_features\n\n        model.fc = nn.Linear(model.nb_ft, num_classes)\n        model.forward = lambda x: forward_slowonly(model, x)\n\n        if num_classes_aux:\n            model.fc_aux = nn.Linear(model.nb_ft, num_classes_aux)\n\n    return model","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"def retrieve_model(config, fold=0, log_folder=\"\"):\n    model = get_model_cls_3d(\n        config[\"name\"],\n        num_classes=config[\"num_classes\"],\n        num_classes_aux=config[\"num_classes_aux\"],\n        pretrained=False\n    ).eval()\n    model.zero_grad()\n\n    model = load_model_weights(model, log_folder + f\"{config['name']}_{fold}.pt\")\n\n    return model","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Inference"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"def inference(df, models, batch_size=256, device=\"cuda\", root=\"\", n_frames=9, stride=2):\n    models = [model.to(device).eval() for model in models]\n\n    dataset = NFLDatasetClsInference3D(\n        df.copy(),\n        root=root,\n        n_frames=n_frames,\n        stride=stride,\n    )\n\n    loader = DataLoader(\n        dataset,\n        batch_size=batch_size,\n        shuffle=False,\n        num_workers=NUM_WORKERS,\n        pin_memory=True,\n    )\n\n    preds = []\n    with torch.no_grad():\n        for img in loader:\n            img = img.to(device)\n            preds_img = []\n\n            for model in models:\n                y_pred = model(img)[0]\n                preds_img.append(y_pred.sigmoid().detach().cpu().numpy())\n\n            preds.append(np.mean(preds_img, 0))\n\n    return np.concatenate(preds)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Main"},{"metadata":{"trusted":true},"cell_type":"code","source":"if \"nb_frame\" not in df_pred.columns:\n    df_frames = pd.DataFrame(os.listdir(DATA_ROOT_PATH), columns=[\"name\"])\n    df_frames['frame'] = df_frames['name'].apply(lambda x: int(x[-8:-4]))\n    df_frames['video'] = df_frames['name'].apply(lambda x: \"_\".join(x.split('_')[:3]) + \".mp4\")\n\n    df_max_frame = df_frames[['video', 'frame']].groupby('video').max().rename(columns={\"frame\": \"nb_frame\"}).reset_index()\n\n    df_pred = df_pred.merge(df_max_frame, on=\"video\")","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"df_pred_vid = df_pred[df_pred[\"video\"] == df_pred['video'].unique()[0]]\n\ndataset = NFLDatasetClsInference3D(\n    df_pred_vid,\n    visualize=True,\n    stride=2,\n    n_frames=9,\n    root=DATA_ROOT_PATH,\n)\n\nfor i in np.random.choice(len(dataset), 1):\n    image = dataset[i]\n\n    plt.figure(figsize=(10, 10))\n    for i, img in enumerate(image):\n        plt.subplot(3, 3, i+1)\n        plt.imshow(img)\n        plt.axis(False)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"CP_FOLDER = \"../input/nfl-dataset-3/\"\n\nconfigs_r18 = {\n    \"30_0\": {\n        \"name\": \"resnet18\",\n        \"num_classes\": 1,\n        \"num_classes_aux\": 0,\n        \"k\": 5,\n        \"stride\": 2,\n        \"num_frames\": 9\n    },\n    \"30_8\": {\n        \"name\": \"resnet18\",\n        \"num_classes\": 1,\n        \"num_classes_aux\": 0,\n        \"k\": 5,\n        \"stride\": 2,\n        \"num_frames\": 9\n    },\n    \"31_0\": {\n        \"name\": \"resnet18\",\n        \"num_classes\": 1,\n        \"num_classes_aux\": 4,\n        \"k\": 5,\n        \"stride\": 2,\n        \"num_frames\": 9\n    },\n    \"31_1\": {\n        \"name\": \"resnet34\",\n        \"num_classes\": 1,\n        \"num_classes_aux\": 0,\n        \"k\": 5,\n        \"stride\": 2,\n        \"num_frames\": 9\n    },\n}\n\nconfigs = {\n    \"03_5\": {\n        \"name\": \"i3d\",\n        \"num_classes\": 1,\n        \"num_classes_aux\": 0,\n        \"k\": 5,\n        \"stride\": 2,\n        \"num_frames\": 9\n    },\n    \"03_1\": {\n        \"name\": \"slowonly\",\n        \"num_classes\": 1,\n        \"num_classes_aux\": 0,\n        \"k\": 5,\n        \"stride\": 2,\n        \"num_frames\": 9\n    },\n    \"03_2\": {\n        \"name\": \"slowfast\",\n        \"num_classes\": 1,\n        \"num_classes_aux\": 0,\n        \"k\": 5,\n        \"stride\": 2,\n        \"num_frames\": 9\n    },\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"models = []\n\nfor name in configs_r18:\n    path = CP_FOLDER + name + \"/\" + name[-1] + \"/\"\n    n = 1 if DEBUG else configs_r18[name]['k']\n    models += [retrieve_model(configs_r18[name], fold=i, log_folder=path) for i in range(n)]\n    \npreds = []\nfor vid in tqdm(df_pred['video'].unique()):\n    df_pred_vid = df_pred[df_pred[\"video\"] == vid]\n\n    pred = inference(df_pred_vid, models, root=DATA_ROOT_PATH, stride=2, n_frames=9, batch_size=16)\n    preds.append(pred)\n    \ndf_pred['pred_cls_3d_r18'] = np.concatenate(preds)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del pred, preds, vid, df_pred_vid, models\n_ = gc.collect()\ntorch.cuda.empty_cache()\nprint('Cleared GPU VRAM')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"models = []\n\nfor name in configs:\n    path = CP_FOLDER + name + \"/\" + name[-1] + \"/\"\n    n = 1 if DEBUG else configs[name]['k']\n    models += [retrieve_model(configs[name], fold=i, log_folder=path) for i in range(n)]\n    \npreds = []\nfor vid in tqdm(df_pred['video'].unique()):\n    df_pred_vid = df_pred[df_pred[\"video\"] == vid]\n\n    pred = inference(df_pred_vid, models, root=DATA_ROOT_PATH, stride=2, n_frames=9, batch_size=16)\n    preds.append(pred)\n    \ndf_pred['pred_cls_3d'] = np.concatenate(preds)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del pred, preds, vid, df_pred_vid, models\n_ = gc.collect()\ntorch.cuda.empty_cache()\nprint('Cleared GPU VRAM')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_pred[\"pred_cls_blend\"] = (CLS_3D_NEW * df_pred['pred_cls_3d'] +  CLS_2D * df_pred['pred_cls'] \n                             + CLS_3D_OLD * df_pred['pred_cls_3d_r18']) / (CLS_3D_NEW + CLS_2D + CLS_3D_OLD)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_pred","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Thresholding"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_pred1 = df_pred.loc[\n    (df_pred.scores > DET_THRESHOLD)\n    & (df_pred.frame <= SWITCH_FRAME)\n    & (df_pred.view == \"Endzone\")\n]\ndf_pred2 = df_pred.loc[\n    (df_pred.scores > DET_THRESHOLD2)\n    & (df_pred.frame > SWITCH_FRAME)\n    & (df_pred.view == \"Endzone\")\n]\ndf_pred3 = df_pred.loc[\n    (df_pred.scores > DET_THRESHOLD - DELTA_DET)\n    & (df_pred.frame <= SWITCH_FRAME)\n    & (df_pred.view == \"Sideline\")\n]\ndf_pred4 = df_pred.loc[\n    (df_pred.scores > DET_THRESHOLD2 - DELTA_DET)\n    & (df_pred.frame > SWITCH_FRAME)\n    & (df_pred.view == \"Sideline\")\n]\ndf_pred = pd.concat([df_pred1, df_pred2, df_pred3, df_pred4], axis=0)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f'Detected {len(df_pred)} impacts after detection thresholding')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_pred_cls1 = df_pred.copy()[\n    (df_pred[\"pred_cls_blend\"] > CLS_THRESHOLD)\n    & (df_pred.frame <= SWITCH_FRAME)\n    & (df_pred.view == \"Endzone\")\n]\ndf_pred_cls2 = df_pred.copy()[\n    (df_pred[\"pred_cls_blend\"] > CLS_THRESHOLD2)\n    & (df_pred.frame > SWITCH_FRAME)\n    & (df_pred.view == \"Endzone\")\n]\ndf_pred_cls3 = df_pred.copy()[\n    (df_pred[\"pred_cls_blend\"] > CLS_THRESHOLD - DELTA_CLS)\n    & (df_pred.frame <= SWITCH_FRAME)\n    & (df_pred.view == \"Sideline\")\n]\ndf_pred_cls4 = df_pred.copy()[\n    (df_pred[\"pred_cls_blend\"] > CLS_THRESHOLD2 - DELTA_CLS)\n    & (df_pred.frame > SWITCH_FRAME)\n    & (df_pred.view == \"Sideline\")\n]\ndf_pred_cls = pd.concat(\n    [df_pred_cls1, df_pred_cls2, df_pred_cls3, df_pred_cls4], axis=0\n).reset_index(drop=True)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f'Detected {len(df_pred_cls)} impacts after classification thresholding')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Post-processing"},{"metadata":{},"cell_type":"markdown","source":"## Metric"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"import numpy as np\nfrom scipy.optimize import linear_sum_assignment\n\n\ndef get_boxes_from_df(df, videos):\n    try:\n        cols = [\"frame\", \"video\", \"x\", \"w\", \"y\", \"h\"]\n        df = df[cols].groupby(\"video\").agg(list)\n    except KeyError:\n        cols = [\"frame\", \"video\", \"left\", \"width\", \"top\", \"height\"]\n        df = df[cols].groupby(\"video\").agg(list)\n\n    boxes = []\n\n    for video in videos:\n        try:\n            frames, x, w, y, h = df.loc[video]\n            boxes_pred = np.concatenate([\n                np.array(frames)[:, None],\n                np.array(x)[:, None],\n                np.array(y)[:, None],\n                np.array(w)[:, None] + np.array(x)[:, None],\n                np.array(h)[:, None] + np.array(y)[:, None],\n            ], -1)\n            boxes.append(boxes_pred)\n        except KeyError:\n            boxes.append([])\n    return boxes\n\n\ndef iou_score(bbox1, bbox2):\n    bbox1 = [float(x) for x in bbox1]\n    bbox2 = [float(x) for x in bbox2]\n\n    (x0_1, y0_1, x1_1, y1_1) = bbox1\n    (x0_2, y0_2, x1_2, y1_2) = bbox2\n\n    # get the overlap rectangle\n    overlap_x0 = max(x0_1, x0_2)\n    overlap_y0 = max(y0_1, y0_2)\n    overlap_x1 = min(x1_1, x1_2)\n    overlap_y1 = min(y1_1, y1_2)\n\n    # check if there is an overlap\n    if overlap_x1 - overlap_x0 <= 0 or overlap_y1 - overlap_y0 <= 0:\n        return 0\n\n    # if yes, calculate the ratio of the overlap to each ROI size and the unified size\n    size_1 = (x1_1 - x0_1) * (y1_1 - y0_1)\n    size_2 = (x1_2 - x0_2) * (y1_2 - y0_2)\n    size_intersection = (overlap_x1 - overlap_x0) * (overlap_y1 - overlap_y0)\n    size_union = size_1 + size_2 - size_intersection\n\n    return size_intersection / size_union\n\n\ndef precision_calc(gt_boxes, pred_boxes, return_assignment=False):\n    cost_matix = np.ones((len(gt_boxes), len(pred_boxes)))\n    for i, box1 in enumerate(gt_boxes):\n        for j, box2 in enumerate(pred_boxes):\n            dist = abs(box1[0] - box2[0])\n            if dist > 4:\n                continue\n            iou = iou_score(box1[1:], box2[1:])\n\n            if iou < 0.35:\n                continue\n\n            else:\n                cost_matix[i, j] = 0\n\n    row_ind, col_ind = linear_sum_assignment(cost_matix)\n\n    if return_assignment:\n        return cost_matix, row_ind, col_ind\n\n    fn = len(gt_boxes) - row_ind.shape[0]\n    fp = len(pred_boxes) - col_ind.shape[0]\n    tp = 0\n    for i, j in zip(row_ind, col_ind):\n        if cost_matix[i, j] == 0:\n            tp += 1\n        else:\n            fp += 1\n            fn += 1\n\n    return tp, fp, fn\n\n\ndef boxes_f1_score(preds, truths):\n    \"\"\"\n    F1 score metric for the competition.\n\n    Predictions and ground truths are lists of the predictions at video level.\n    Each lists contains a list of all the boxes, represented as a 5 element tuple T :\n        - T[0] : Frame\n        - T[1:5]: boxe coordinate\n\n    Args:\n        preds (List): Predictions.\n        truths (List): Truths.\n\n    Returns:\n        float: f1 score\n    \"\"\"\n    ftp, ffp, ffn = [], [], []\n    for pred, truth in zip(preds, truths):\n        tp, fp, fn = precision_calc(truth, pred)\n        ftp.append(tp)\n        ffp.append(fp)\n        ffn.append(fn)\n\n    tp = np.sum(ftp)\n    fp = np.sum(ffp)\n    fn = np.sum(ffn)\n    # print(tp, fp, fn)\n\n    precision = tp / (tp + fp + 1e-6)\n    recall = tp / (tp + fn + 1e-6)\n    # print(precision, recall)\n\n    f1_score = 2 * (precision * recall) / (precision + recall + 1e-6)\n    return f1_score\n\n\ndef num_detected_impacts(gt_boxes, pred_boxes, max_dist=4, threshold=0.35):\n\n    if len(pred_boxes) == 0 or len(gt_boxes) == 0:\n        return 0\n\n    cost_matrix = np.zeros((len(gt_boxes), len(pred_boxes)))\n    for i, box1 in enumerate(gt_boxes):\n        for j, box2 in enumerate(pred_boxes):\n            dist = abs(box1[0] - box2[0])\n            if dist > max_dist:\n                continue\n            iou = iou_score(box1[1:], box2[1:])\n\n            if iou < threshold:\n                continue\n\n            else:\n                cost_matrix[i, j] = 1\n    return cost_matrix.max(1).sum()\n\n\ndef detection_ratio(preds, truths):\n    detected = 0\n    total = 0\n\n    for pred_h, truth_i in zip(preds, truths):\n        total += len(truth_i)\n        detected += num_detected_impacts(\n            truth_i,\n            pred_h,\n            max_dist=4,\n            threshold=0.35\n        )\n\n    return detected / total\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Theo's adjacency PP."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"def iou_score(bbox1, bbox2):\n    bbox1 = [float(x) for x in bbox1]\n    bbox2 = [float(x) for x in bbox2]\n\n    (x0_1, y0_1, x1_1, y1_1) = bbox1\n    (x0_2, y0_2, x1_2, y1_2) = bbox2\n\n    # get the overlap rectangle\n    overlap_x0 = max(x0_1, x0_2)\n    overlap_y0 = max(y0_1, y0_2)\n    overlap_x1 = min(x1_1, x1_2)\n    overlap_y1 = min(y1_1, y1_2)\n\n    # check if there is an overlap\n    if overlap_x1 - overlap_x0 <= 0 or overlap_y1 - overlap_y0 <= 0:\n        return 0\n\n    # if yes, calculate the ratio of the overlap to each ROI size and the unified size\n    size_1 = (x1_1 - x0_1) * (y1_1 - y0_1)\n    size_2 = (x1_2 - x0_2) * (y1_2 - y0_2)\n    size_intersection = (overlap_x1 - overlap_x0) * (overlap_y1 - overlap_y0)\n    size_union = size_1 + size_2 - size_intersection\n\n    return size_intersection / size_union\n\n\ndef compute_ious(df, max_dist=10):\n    ious = np.zeros((len(df), len(df)))\n    for i in range(len(df)):\n        for j in range(len(df)):\n            frames = df[\"frame\"].values[[i, j]]\n            if np.abs(frames[0] - frames[1]) > max_dist:\n                continue\n\n            try:\n                boxes = df[[\"left\", \"width\", \"top\", \"height\"]].values[[i, j]]\n            except KeyError:\n                boxes = df[[\"x\", \"w\", \"y\", \"h\"]].values[[i, j]]\n\n            boxes[:, 1] += boxes[:, 0]\n            boxes[:, 3] += boxes[:, 2]\n            boxes = boxes[:, [0, 2, 1, 3]]\n\n            iou = iou_score(boxes[0], boxes[1])\n            ious[i, j] = iou\n            ious[j, i] = iou\n    return ious\n\n\ndef get_centroids(clusts):\n    centroids = []\n    for clust in clusts:\n        if len(clust) == 1:\n            centroids.append(clust[0])\n        elif (len(clust) % 2) == 1:\n            centroids.append(clust[len(clust) // 2 + 1])\n        else:\n            centroids.append(clust[len(clust) // 2])\n\n    return centroids\n\n\ndef form_clusters(df, threshold=0.5, max_dist=10):\n    ious = compute_ious(df, max_dist=max_dist)\n\n    frames = df[\"frame\"]\n\n    clust_mat = np.zeros((len(df), len(df)))\n\n    for i in range(len(df)):\n        for j in range(len(df)):\n            if frames[i] == frames[j]:\n                continue\n            elif ious[i, j] > threshold:\n                clust_mat[i, j] = 1\n                clust_mat[j, i] = 1\n\n    clusts = [[0]]\n    for i in range(1, len(df)):\n        in_clust = False\n        for clust in clusts[::-1]:\n            if clust_mat[clust[-1], i]:\n                in_clust = True\n                clust.append(i)\n                break\n\n        if not in_clust:\n            clusts.append([i])\n\n    centroids = get_centroids(clusts)\n\n    return clusts, centroids\n\n\ndef post_process_adjacency(df, threshold=0.5, max_dist=10, min_clust_size=0):\n    dfs_pp = []\n    for video in df[\"video\"].unique():\n        df_video = df[df[\"video\"] == video].reset_index(drop=True).copy()\n        clusts, centroids = form_clusters(\n            df_video, threshold=threshold, max_dist=max_dist\n        )\n        centroids = [\n            centroids[i]\n            for i in range(len(centroids))\n            if len(clusts[i]) >= min_clust_size\n        ]\n\n        df_video_pp = (\n            df_video.iloc[centroids].sort_values(\"frame\").reset_index(drop=True)\n        )\n        dfs_pp.append(df_video_pp)\n\n    df_pp = pd.concat(dfs_pp).reset_index(drop=True)\n    return df_pp","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### View PP"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"def post_process_view(df, min_dist=4, threshold=1., cls_col=\"pred_cls\"):\n    to_drop = []\n    for keys in df.groupby([\"gameKey\", \"playID\"]).size().to_dict().keys():\n\n        tmp_df = df.query(\"gameKey == @keys[0] and playID == @keys[1]\")\n        tmp_to_drop = []\n        for index, row in tmp_df.iterrows():\n            if row[\"view\"] == \"Endzone\":\n                other_view = tmp_df.query('view == \"Sideline\"')\n            else:\n                other_view = tmp_df.query('view == \"Endzone\"')\n\n            distances = other_view[\"frame\"].apply(lambda x: np.abs(x - row[\"frame\"]))\n            if (\n                (np.min(distances) > min_dist)\n                & (row['scores'] < threshold)\n                & (row[cls_col] < threshold)\n            ):\n                tmp_to_drop.append(index)\n\n        if len(tmp_to_drop) != len(tmp_df):\n            to_drop += tmp_to_drop\n\n    return df.drop(index=to_drop).reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Expansion"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"def expand_boxes(df, r=0):\n    if r > 0:\n        # Expansion\n        df.left -= df.width * r / 2\n        df.top -= df.height * r / 2\n        df.width *= 1 + r\n        df.height *= 1 + r\n        df.left = np.clip(df.left, 0, None)\n        df.top = np.clip(df.top, 0, None)\n        df.width = np.clip(df.width, 0, 1280 - df.left)\n        df.height = np.clip(df.height, 0, 720 - df.top)\n\n        # Rounding\n\n        right = np.round(df.left + df.width, 0)\n        bot = np.round(df.top + df.height, 0)\n        df.left = np.round(df.left, 0).astype(int)\n        df.top = np.round(df.top, 0).astype(int)\n\n        df.width = (right - df.left).astype(int)\n        df.height = (bot - df.top).astype(int)\n\n    return df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Main"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_pred_pp = df_pred_cls.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_pred_pp = expand_boxes(df_pred_pp, r=R)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for _ in range(N_TIMES):\n    df_pred_pp = post_process_adjacency(\n        df_pred_pp,\n        threshold=NMS_THRESHOLD,\n        max_dist=MAX_FRAME_DIST,\n    )\n\nprint(f'Detected {len(df_pred_pp)} impacts after adjacency post-processing')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if MIN_DIST:\n    df_pred_pp = post_process_view(df_pred_pp, min_dist=MIN_DIST, threshold=VIEW_THRESHOLD, cls_col=\"pred_cls_blend\")\n    print(f'Detected {len(df_pred_pp)} impacts after view post-processing')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"if DEBUG:\n    df_train = pd.read_csv(\"../input/nfl-impact-detection/train_labels.csv\")\n\n    df_train = df_train[df_train['video'].apply(lambda x: \"57586_000540\" in x)]\n    df_train['truth'] = (df_train['impact'] == 1) & (df_train['confidence'] > 1) & (df_train['visibility'] > 0) \n\n    videos = df_train['video'].unique()\n    \n    gt_boxes = get_boxes_from_df(df_train[df_train['truth'] == 1], videos)\n    pred_boxes = get_boxes_from_df(df_pred_pp, videos)\n\n    score = boxes_f1_score(pred_boxes, gt_boxes)\n\n    print(f' -> CV score is {score:.4f}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"if DEBUG:\n    frames = df_pred_pp['frame'].unique()\n    for frame in frames:\n        df_viz = df_pred_pp[df_pred_pp['frame'] == frame].reset_index(drop=True)\n        plt.figure(figsize=(16, 8))\n        visualize_preds(df_viz, 0, DATA_ROOT_PATH)\n        plt.axis(False)\n        plt.show()\n\n    #     break","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_pred_pp.head(20)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Submission"},{"metadata":{"trusted":true},"cell_type":"code","source":"!mv * /tmp/","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cols = [\"gameKey\", \"playID\", \"view\", \"video\", \"frame\", \"left\", \"width\", \"top\", \"height\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"env = nflimpact.make_env()\nenv.predict(df_pred_pp[cols]) ","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}