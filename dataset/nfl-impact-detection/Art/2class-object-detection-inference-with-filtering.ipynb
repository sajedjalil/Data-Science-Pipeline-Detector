{"cells":[{"metadata":{},"cell_type":"markdown","source":"This notebook is based on https://www.kaggle.com/its7171/2class-object-detection-inference <br>\nI added a small filtering step to reduce the number of False Positives\n\n\n\n##### Please upvote if this was helpful to you. Pressing \"fork\" is one click, pressing \"upvote\" is just one extra click which shouldn't take a lot of your time :)"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nd = pd.read_csv('../input/nfl-impact-detection/test_player_tracking.csv')\nIS_PRIVATE = d.shape != (19269, 12)\nprint(IS_PRIVATE)\n\nIS_PRIVATE = True","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"if IS_PRIVATE:\n    !pip install ../input/nfl-lib/timm-0.1.26-py3-none-any.whl\n    !tar xfz ../input/nfl-lib/pkgs.tgz\n    # for pytorch1.6\n    cmd = \"sed -i -e 's/ \\/ / \\/\\/ /' timm-efficientdet-pytorch/effdet/bench.py\"\n    !$cmd","execution_count":null,"outputs":[]},{"metadata":{"incorrectly_encoded_metadata":"_kg_hide-input=true _kg_hide-output=true","trusted":true},"cell_type":"code","source":"import sys\nsys.path.insert(0, \"timm-efficientdet-pytorch\")\nsys.path.insert(0, \"omegaconf\")\n\nimport torch\nimport os\nfrom datetime import datetime\nimport time\nimport random\nimport cv2\nimport pandas as pd\nimport numpy as np\nimport albumentations as A\nimport matplotlib.pyplot as plt\nfrom albumentations.pytorch.transforms import ToTensorV2\nfrom sklearn.model_selection import StratifiedKFold\nfrom torch.utils.data import Dataset,DataLoader\nfrom torch.utils.data.sampler import SequentialSampler, RandomSampler\nfrom glob import glob\nimport pandas as pd\nimport gc\nfrom effdet import get_efficientdet_config, EfficientDet, DetBenchTrain, DetBenchEval\nfrom effdet.efficientdet import HeadNet\nimport warnings\n\nwarnings.filterwarnings(\"ignore\")\n\nDATA_ROOT_PATH = 'test_images'\nSEED = 42\n\ndef seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\nseed_everything(SEED)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#################\n# SET CONSTANTS\n#################\n\nDETECTION_THRESHOLD = 0.4\nDETECTOR_FILTERING_THRESHOLD = 0.3","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def mk_images(video_name, video_labels, video_dir, out_dir, only_with_impact=True):\n    video_path=f\"{video_dir}/{video_name}\"\n    video_name = os.path.basename(video_path)\n    vidcap = cv2.VideoCapture(video_path)\n    if only_with_impact:\n        boxes_all = video_labels.query(\"video == @video_name\")\n        print(video_path, boxes_all[boxes_all.impact == 1.0].shape[0])\n    else:\n        print(video_path)\n    frame = 0\n    while True:\n        it_worked, img = vidcap.read()\n        if not it_worked:\n            break\n        frame += 1\n        if only_with_impact:\n            boxes = video_labels.query(\"video == @video_name and frame == @frame\")\n            boxes_with_impact = boxes[boxes.impact == 1.0]\n            if boxes_with_impact.shape[0] == 0:\n                continue\n        img_name = f\"{video_name}_frame{frame}\"\n        image_path = f'{out_dir}/{video_name}'.replace('.mp4',f'_{frame}.png')\n        _ = cv2.imwrite(image_path, img)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if IS_PRIVATE:\n    out_dir = DATA_ROOT_PATH\n    if not os.path.exists(out_dir):\n        !mkdir -p $out_dir\n        video_dir = '/kaggle/input/nfl-impact-detection/test'\n        uniq_video = [path.split('/')[-1] for path in glob(f'{video_dir}/*.mp4')]\n        for video_name in uniq_video:\n            mk_images(video_name, pd.DataFrame(), video_dir, out_dir, only_with_impact=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_valid_transforms():\n    return A.Compose([\n            A.Resize(height=512, width=512, p=1.0),\n            ToTensorV2(p=1.0),\n        ], p=1.0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class DatasetRetriever(Dataset):\n    def __init__(self, image_ids, transforms=None):\n        super().__init__()\n        self.image_ids = image_ids\n        self.transforms = transforms\n\n    def __getitem__(self, index: int):\n        image_id = self.image_ids[index]\n        image = cv2.imread(f'{DATA_ROOT_PATH}/{image_id}', cv2.IMREAD_COLOR).copy().astype(np.float32)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n        image /= 255.0\n        if self.transforms:\n            sample = {'image': image}\n            sample = self.transforms(**sample)\n            image = sample['image']\n        return image, image_id\n\n    def __len__(self) -> int:\n        return self.image_ids.shape[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def load_net(checkpoint_path):\n    config = get_efficientdet_config('tf_efficientdet_d5')\n    net = EfficientDet(config, pretrained_backbone=False)\n    config.num_classes = 2\n    config.image_size=512\n    net.class_net = HeadNet(config, num_outputs=config.num_classes, norm_kwargs=dict(eps=.001, momentum=.01))\n    checkpoint = torch.load(checkpoint_path)\n    net.load_state_dict(checkpoint['model_state_dict'])\n    net = DetBenchEval(net, config)\n    net.eval();\n    return net.cuda()\nif IS_PRIVATE:\n    net = load_net('../input/nfl-models//best-checkpoint-002epoch.bin')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset = DatasetRetriever(\n    image_ids=np.array([path.split('/')[-1] for path in glob(f'{DATA_ROOT_PATH}/*.png')]),\n    transforms=get_valid_transforms()\n)\n\ndef collate_fn(batch):\n    return tuple(zip(*batch))\n\ndata_loader = DataLoader(\n    dataset,\n    batch_size=16,\n    shuffle=False,\n    num_workers=4,\n    drop_last=False,\n    collate_fn=collate_fn\n)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"def make_predictions(images, score_threshold=0.5):\n    images = torch.stack(images).cuda().float()\n    box_list = []\n    score_list = []\n    with torch.no_grad():\n        det = net(images, torch.tensor([1]*images.shape[0]).float().cuda())\n        for i in range(images.shape[0]):\n            boxes = det[i].detach().cpu().numpy()[:,:4]    \n            scores = det[i].detach().cpu().numpy()[:,4]   \n            label = det[i].detach().cpu().numpy()[:,5]\n            # useing only label = 2\n            indexes = np.where((scores > score_threshold) & (label == 2))[0]\n            boxes[:, 2] = boxes[:, 2] + boxes[:, 0]\n            boxes[:, 3] = boxes[:, 3] + boxes[:, 1]\n            box_list.append(boxes[indexes])\n            score_list.append(scores[indexes])\n    return box_list, score_list\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#check prediction\n\ncnt = 0\nfor images, image_ids in data_loader:\n    box_list, score_list = make_predictions(images, score_threshold=DETECTION_THRESHOLD)\n    for i in range(len(images)):\n        sample = images[i].permute(1,2,0).cpu().numpy()\n        boxes = box_list[i].astype(np.int32).clip(min=0, max=511)\n        scores = score_list[i]\n        if len(scores) >= 1:\n            fig, ax = plt.subplots(1, 1, figsize=(16, 8))\n            sample = cv2.resize(sample , (int(1280), int(720)))\n            for box,score in zip(boxes,scores):\n                box[0] = box[0] * 1280 / 512\n                box[1] = box[1] * 720 / 512\n                box[2] = box[2] * 1280 / 512\n                box[3] = box[3] * 720 / 512\n                cv2.rectangle(sample, (box[0], box[1]), (box[2], box[3]), (1, 0, 0), 3)\n            ax.set_axis_off()\n            ax.imshow(sample);\n            cnt += 1\n    if cnt >= 10:\n        break","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result_image_ids = []\nresults_boxes = []\nresults_scores = []\nfor images, image_ids in data_loader:\n    box_list, score_list = make_predictions(images, score_threshold=DETECTION_THRESHOLD)\n    for i, image in enumerate(images):\n        boxes = box_list[i]\n        scores = score_list[i]\n        image_id = image_ids[i]\n        boxes[:, 0] = (boxes[:, 0] * 1280 / 512)\n        boxes[:, 1] = (boxes[:, 1] * 720 / 512)\n        boxes[:, 2] = (boxes[:, 2] * 1280 / 512)\n        boxes[:, 3] = (boxes[:, 3] * 720 / 512)\n        boxes[:, 2] = boxes[:, 2] - boxes[:, 0]\n        boxes[:, 3] = boxes[:, 3] - boxes[:, 1]\n        boxes = boxes.astype(np.int32)\n        boxes[:, 0] = boxes[:, 0].clip(min=0, max=1280-1)\n        boxes[:, 2] = boxes[:, 2].clip(min=0, max=1280-1)\n        boxes[:, 1] = boxes[:, 1].clip(min=0, max=720-1)\n        boxes[:, 3] = boxes[:, 3].clip(min=0, max=720-1)\n        result_image_ids += [image_id]*len(boxes)\n        results_boxes.append(boxes)\n        results_scores.append(scores)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"box_df = pd.DataFrame(np.concatenate(results_boxes), columns=['left', 'top', 'width', 'height'])\ntest_df = pd.DataFrame({'scores':np.concatenate(results_scores), 'image_name':result_image_ids})\ntest_df = pd.concat([test_df, box_df], axis=1)\n\ntest_df = test_df[test_df.scores > DETECTOR_FILTERING_THRESHOLD]\ntest_df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#gameKey,playID,view,video,frame,left,width,top,height\n#57590,3607,Endzone,57590_003607_Endzone.mp4,1,1,1,1,1\ntest_df['gameKey'] = test_df.image_name.str.split('_').str[0].astype(int)\ntest_df['playID'] = test_df.image_name.str.split('_').str[1].astype(int)\ntest_df['view'] = test_df.image_name.str.split('_').str[2]\ntest_df['frame'] = test_df.image_name.str.split('_').str[3].str.replace('.png','').astype(int)\ntest_df['video'] = test_df.image_name.str.rsplit('_',1).str[0] + '.mp4'\ntest_df = test_df[[\"gameKey\",\"playID\",\"view\",\"video\",\"frame\",\"left\",\"width\",\"top\",\"height\"]]\ntest_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Remove all boxes which are not present in both Sidezone and Endzone views"},{"metadata":{"trusted":true},"cell_type":"code","source":"#################\n# FILTER\n#################\n\n\ndropIDX = []\nfor keys in test_df.groupby(['gameKey', 'playID']).size().to_dict().keys():\n    tmp_df = test_df.query('gameKey == @keys[0] and playID == @keys[1]')\n    \n    for index, row in tmp_df.iterrows():\n        if row['view'] == 'Endzone':\n            check_df = tmp_df.query('view == \"Sideline\"')\n            if check_df['frame'].apply(lambda x: np.abs(x - row['frame']) <= 4).sum() == 0:\n                dropIDX.append(index)\n        \n        if row['view'] == 'Sideline':\n            check_df = tmp_df.query('view == \"Endzone\"')\n            if check_df['frame'].apply(lambda x: np.abs(x - row['frame']) <= 4).sum() == 0:\n                dropIDX.append(index)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df = test_df.drop(index = dropIDX).reset_index(drop = True)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":false,"_kg_hide-output":false,"trusted":true},"cell_type":"code","source":"# clearing working dir\n# be careful when running this code on local environment!\n# !rm -rf *\n!mv * /tmp/","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import nflimpact\nenv = nflimpact.make_env()\n\nif IS_PRIVATE:\n    env.predict(test_df) # df is a pandas dataframe of your entire submission file\nelse:\n    sub = pd.read_csv('../input/nfl-impact-detection/sample_submission.csv')\n    env.predict(sub)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}