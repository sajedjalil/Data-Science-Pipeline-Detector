{"cells":[{"metadata":{},"cell_type":"markdown","source":"Data Overview\n\nThis notebook provides an overview of the data and some examples of how to access and conduct some initial plotting of the data that has been provided. There are three different types of data provided for this problem:\n\n\n\n**Image Data**-Almost 10,000 images and associated helmet labels for the purpose of building a helmet detection computer vision system.\n\n**Video Data** - 120 videos (60 plays) from both a sideline and endzone point of view (one each per play) with associated helmet and helmet impact labels for the purpose of building a helmet impact detection computer vision system.\n\n**Tracking Data**-Tracking data for all players that participate in the provided 60 plays.\n\nThis overview provides an example for how to parse and plot each of these data types. It also briefly summarizes the needed steps to submit a solution for scoring."},{"metadata":{},"cell_type":"markdown","source":"**import needed Libraries**"},{"metadata":{"trusted":true},"cell_type":"code","source":"import imageio\nfrom PIL import Image\nimport cv2\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nimport subprocess\n\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\n%matplotlib inline\nplt.rcParams['figure.dpi'] = 150\n\nimport seaborn as sns\n\nfrom IPython.display import Video, display\n\n#block those warnings from pandas about setting values on a slice\nimport warnings\nwarnings.filterwarnings('ignore') \nimport numpy as np\nimport pandas as pd \n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom matplotlib import patches\nfrom PIL import Image\n\nimport os\nfrom tqdm import tqdm\nfrom IPython.display import clear_output\n\nimport torch\nimport torchvision\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Lets explore the Image Data**\n\n**First let's plot some random images with corresponding helmet bounding-boxes.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"DATA_PATH = '../input/nfl-impact-detection'","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**First let's plot some random images with corresponding helmet bounding-boxes.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"def add_bboxes(ax, img, img_df):\n    img_data = img_df[img_df['image'] == img]\n    for i in range(img_data.shape[0]):\n        data = img_data.iloc[i]\n        bbox = patches.Rectangle((\n            data['left'],\n            data['top']),\n            data['width'],\n            data['height'],\n            linewidth=1,\n            edgecolor='r',\n            facecolor='None',\n            alpha=0.7\n        )\n        ax.add_patch(bbox)\n    return\n\ndef plot_random_images(root_path, plot_bboxes=True, verbose=True):\n   \n    images_path = root_path + '/images/'\n    img_labels_df = pd.read_csv(root_path + '/image_labels.csv')\n    \n    images_list = os.listdir(images_path)\n    n_images = len(images_list)\n    endzone_images = [image for image in images_list if 'Endzone' in image]\n    sideline_images = [image for image in images_list if 'Sideline' in image]\n\n    if verbose:\n        print(f'There are {n_images} images in the `images` folder.')\n        print(f'  {len(endzone_images)} - images from endzone.')\n        print(f'  {len(sideline_images)} - images from sideline.')\n\n    fig, ax = plt.subplots(4, 2, figsize=(14, 12))\n    for i in range(4):\n        for j in range(2):\n            if j == 0:\n                random_idx = np.random.randint(len(endzone_images))\n                random_img_name = endzone_images[random_idx]\n                random_img = Image.open(images_path + random_img_name)\n            else:\n                random_idx = np.random.randint(len(sideline_images))\n                random_img_name = sideline_images[random_idx]\n                random_img = Image.open(images_path + random_img_name)\n            ax[i][j].imshow(random_img)\n            ax[i][j].set_axis_off()\n            if plot_bboxes:\n                add_bboxes(ax[i][j], random_img_name, img_labels_df)\n\n    ax[0][0].set_title('Endzone images')\n    ax[0][1].set_title('Sideline images')\n    fig.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nplot_random_images(DATA_PATH, plot_bboxes=True, verbose=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Now let's check the helmet visibility classes distribution**"},{"metadata":{"trusted":true},"cell_type":"code","source":"img_labels_df = pd.read_csv(DATA_PATH + '/image_labels.csv')\nplt.figure(figsize = (12,6))\nimg_labels_df.label.hist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"#importing the Data\nImage_Labels = pd.read_csv('/kaggle/input/nfl-impact-detection/image_labels.csv')\nImage_Labels.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Image_Labels.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Lets take an Image and add the Labels**"},{"metadata":{"trusted":true},"cell_type":"code","source":"#lets take an Image and add the Labels\nImage_Name = Image_Labels['image'][0]\nImage_Name","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Path to our selected image**"},{"metadata":{"trusted":true},"cell_type":"code","source":"img_path = f\"/kaggle/input/nfl-impact-detection/images/{Image_Name}\"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**read and plot the Image**"},{"metadata":{"trusted":true},"cell_type":"code","source":"img = imageio.imread(img_path)\nplt.imshow(img)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Let's write a function for adding the bounding boxes from the label to the image. Note that the pixel geometry starts with (0,0) in the top left of the image. To draw the bounding box, we need to specify the top left pixel location and the bottom right pixel location of the image.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Function to add labels to an Image\ndef add_img_box(image_name, image_labels):\n    #set Label Color for Bounding Boxes\n    Helmet_Color = (0, 0, 0)\n    \n    boxes = Image_Labels.loc[Image_Labels['image'] == Image_Name]\n    for i, box in boxes.iterrows():\n        color = Helmet_Color\n        \n        # Add a box around the helmet\n        # Note that cv2.rectangle needs us to specify the top left pixel and the bottom right pixel\n        cv2.rectangle(img, (box.left, box.top), (box.left + box.width, box.top + box.height), color, thickness=1)\n        #display the image with Bounding Boxes\n        plt.imshow(img)\n        plt.show()\n        \n        \n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"add_img_box(Image_Name, Image_Labels)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**now we can see Bounding Boxes around the Helmets.**"},{"metadata":{},"cell_type":"markdown","source":"**Video Data**"},{"metadata":{},"cell_type":"markdown","source":"The labeled video dataset provides video for 60 plays observed from both the sideline and endzone perspective (120 videos total). The video_labels.csv file contains labeled bounding boxes for every helmet that is visible in every frame of every video"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Read in the video labels file\nvideo_labels = pd.read_csv('/kaggle/input/nfl-impact-detection/train_labels.csv')\nvideo_labels.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"video_labels.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"video_labels.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n\n**The gameKey, playID, video, and frame fields facilitate matching the bounding box to the appropriate video file and video frame. The label field corresponds to the player field in the tracking data, providing a unique identifier for the helmets of players that are participating in the play. However, there are also helmets (players) that appear in the videos that are not participating in the play. These players are identified with the labels V00 (non-participant on the visiting team) or H00 (non-participant on the home team). In rare cases that a player cannot be uniquely identified that is participating in the play (for example when only the helmet is visible in a pile-up), the appropriate generic V00 or H00 label is applied to that helmet bounding box.**\n\nThe Sideline and Endzone views have been time-synced such that the snap occurs 10 frames into the video. This time alignment should be considered to be accurate to within +/- 3 frames or 0.05 seconds (video data is recorded at approximately 59.94 frames per second).\n\n# **For the purposes of evaluation, definitive helmet impacts are defined as meeting three criteria:**\n# \n• impact = 1\n\n• confidence > 1\n\n• visibility > 0\n\nThose labels with confidence = 1 document cases in which human labelers asserted it was possible that a helmet impact occurred, but it was not clear that the helmet impact altered the trajectory of the helmet. Those labels with visibility = 0 indicate that although there is reason to believe that an impact occurred to that helmet at that time, the impact itself was not visible from the view.\n\n**Let's bring in the very first video and display it.**\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"video_name = video_labels['video'][0]\nvideo_name","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**setting the Path and display the video**"},{"metadata":{"trusted":true},"cell_type":"code","source":"video_path = f\"/kaggle/input/nfl-impact-detection/train/{video_name}\"\ndisplay(Video(data=video_path, embed=True))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Lets created a function that will add bounding boxes to every frame in our video.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create a function to annotate the video at the provided path using labels from the provided dataframe, return the path of the video\ndef annotate_video(video_path: str, video_labels: pd.DataFrame) -> str:\n    VIDEO_CODEC = \"MP4V\"\n    HELMET_COLOR = (0, 0, 0)    # Black\n    IMPACT_COLOR = (0, 0, 255)  # Red\n    video_name = os.path.basename(video_path)\n    \n    vidcap = cv2.VideoCapture(video_path)\n    fps = vidcap.get(cv2.CAP_PROP_FPS)\n    width = int(vidcap.get(cv2.CAP_PROP_FRAME_WIDTH))\n    height = int(vidcap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n    output_path = \"labeled_\" + video_name\n    tmp_output_path = \"tmp_\" + output_path\n    output_video = cv2.VideoWriter(tmp_output_path, cv2.VideoWriter_fourcc(*VIDEO_CODEC), fps, (width, height))\n    frame = 0\n    while True:\n        it_worked, img = vidcap.read()\n        if not it_worked:\n            break\n        \n        # We need to add 1 to the frame count to match the label frame index that starts at 1\n        frame += 1\n        \n        # Let's add a frame index to the video so we can track where we are\n        img_name = f\"{video_name}_frame{frame}\"\n        cv2.putText(img, img_name, (0, 50), cv2.FONT_HERSHEY_SIMPLEX, 1.0, HELMET_COLOR, thickness=2)\n    \n        # Now, add the boxes\n        boxes = video_labels.query(\"video == @video_name and frame == @frame\")\n        for box in boxes.itertuples(index=False):\n            if box.impact == 1 and box.confidence > 1 and box.visibility > 0:    # Filter for definitive head impacts and turn labels red\n                color, thickness = IMPACT_COLOR, 4\n            else:\n                color, thickness = HELMET_COLOR, 1\n            # Add a box around the helmet\n            cv2.rectangle(img, (box.left, box.top), (box.left + box.width, box.top + box.height), color, thickness=thickness)\n            cv2.putText(img, box.label, (box.left, max(0, box.top - 5)), cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, thickness=1)\n        output_video.write(img)\n    output_video.release()\n    \n    # Not all browsers support the codec, we will re-load the file at tmp_output_path and convert to a codec that is more broadly readable using ffmpeg\n    if os.path.exists(output_path):\n        os.remove(output_path)\n    subprocess.run([\"ffmpeg\", \"-i\", tmp_output_path, \"-crf\", \"18\", \"-preset\", \"veryfast\", \"-vcodec\", \"libx264\", output_path])\n    os.remove(tmp_output_path)\n    \n    return output_path","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Label the Video and display it.\nlabeled_video = annotate_video(f\"/kaggle/input/nfl-impact-detection/train/{video_name}\", video_labels)\ndisplay(Video(data=labeled_video, embed=True))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Filter for definitive impacts labeled for this video\nvideo_impacts = video_labels.loc[(video_labels.video == video_name) & (video_labels.impact == 1) & (video_labels.confidence > 1) & (video_labels.visibility > 0)]\nlen(video_impacts) # definitive impacts in this play","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get this list of definitive impacts\nvideo_impacts","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Note that every play consists of two views - a sideline view and an endzone view. So, to find the other view of this play**"},{"metadata":{"trusted":true},"cell_type":"code","source":"sideline_video_name = video_name.replace(\"Endzone\", \"Sideline\")\n# Define the path and then display the video using \nsideline_video_path = f\"/kaggle/input/nfl-impact-detection/train/{sideline_video_name}\"\ndisplay(Video(data=sideline_video_path, embed=True))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# # **Tracking Data**"},{"metadata":{},"cell_type":"markdown","source":"**The player track file in .csv format includes player position, direction, and orientation data for each player during the entire course of the play collected using the Next Gen Stats (NGS) system. This data is indexed by gameKey, playID, and player, with the time variable providing a temporal index within an individual play.**"},{"metadata":{},"cell_type":"markdown","source":"**Lets filter the Track data to the 1st Play we looked at earlier.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"track_data = pd.read_csv('/kaggle/input/nfl-impact-detection/train_player_tracking.csv')\ntrack_data.head()\nlen(track_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"game_key = track_data['gameKey'][0]\nplay_id = track_data['playID'][0]\nplay_track = track_data.loc[(track_data.gameKey == game_key) & (track_data.playID == play_id)]\nlen(play_track)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"play_track['event'].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"play_track['event'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**create a DataFrame for Player Position at the Snap**"},{"metadata":{"trusted":true},"cell_type":"code","source":"at_snap = play_track.loc[play_track['event'] == 'ball_snap']\nat_snap","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import datetime as dt\nimport warnings\n\nimport cv2\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\nfrom IPython.display import Video\nfrom pathlib import Path\n\nfrom matplotlib import animation\nfrom matplotlib import patches\nfrom tqdm.notebook import tqdm\n\nwarnings.simplefilter(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Generation of Football Field**"},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_football_field(linenumbers=True,\n                          endzones=True,\n                          highlight_line=False,\n                          highlight_line_number=50,\n                          highlighted_name='Line of Scrimmage',\n                          fifty_is_los=False,\n                          figsize=(12, 7.33)):\n    \"\"\"\n    Function that plots the football field for viewing plays.\n    Allows for showing or hiding endzones.\n    \"\"\"\n    rect = patches.Rectangle((0, 0), 120, 53.3, linewidth=0.1,\n                             edgecolor='r', facecolor='forestgreen', zorder=0)  # changed the field color to forestgreen\n\n    fig, ax = plt.subplots(1, figsize=figsize)\n    ax.add_patch(rect)\n\n    plt.plot([10, 10, 10, 20, 20, 30, 30, 40, 40, 50, 50, 60, 60, 70, 70, 80,\n              80, 90, 90, 100, 100, 110, 110, 120, 0, 0, 120, 120],\n             [0, 0, 53.3, 53.3, 0, 0, 53.3, 53.3, 0, 0, 53.3, 53.3, 0, 0, 53.3,\n              53.3, 0, 0, 53.3, 53.3, 0, 0, 53.3, 53.3, 53.3, 0, 0, 53.3],\n             color='white')\n    if fifty_is_los:\n        plt.plot([60, 60], [0, 53.3], color='gold')\n        plt.text(62, 50, '<- Player Yardline at Snap', color='gold')\n    # Endzones\n    if endzones:\n        ez1 = patches.Rectangle((0, 0), 10, 53.3,\n                                linewidth=0.1,\n                                edgecolor='r',\n                                facecolor='blue',\n                                alpha=0.2,\n                                zorder=0)\n        ez2 = patches.Rectangle((110, 0), 120, 53.3,\n                                linewidth=0.1,\n                                edgecolor='r',\n                                facecolor='blue',\n                                alpha=0.2,\n                                zorder=0)\n        ax.add_patch(ez1)\n        ax.add_patch(ez2)\n    plt.xlim(0, 120)\n    plt.ylim(-5, 58.3)\n    plt.axis('off')\n    if linenumbers:\n        for x in range(20, 110, 10):\n            numb = x\n            if x > 50:\n                numb = 120 - x\n            plt.text(x, 5, str(numb - 10),\n                     horizontalalignment='center',\n                     fontsize=20,  # fontname='Arial',\n                     color='white')\n            plt.text(x - 0.95, 53.3 - 5, str(numb - 10),\n                     horizontalalignment='center',\n                     fontsize=20,  # fontname='Arial',\n                     color='white', rotation=180)\n    if endzones:\n        hash_range = range(11, 110)\n    else:\n        hash_range = range(1, 120)\n\n    for x in hash_range:\n        ax.plot([x, x], [0.4, 0.7], color='white')\n        ax.plot([x, x], [53.0, 52.5], color='white')\n        ax.plot([x, x], [22.91, 23.57], color='white')\n        ax.plot([x, x], [29.73, 30.39], color='white')\n\n    if highlight_line:\n        hl = highlight_line_number + 10\n        plt.plot([hl, hl], [0, 53.3], color='yellow')\n        plt.text(hl + 2, 50, '<- {}'.format(highlighted_name),\n                 color='yellow')\n    return fig, ax","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**set format of time column**"},{"metadata":{"trusted":true},"cell_type":"code","source":"track_data['time'] = pd.to_datetime(track_data['time'])\ntrack_data['color'] = track_data[\"player\"].map(lambda x:'black' if \"H\" in x else \"white\")\ntrack_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_labels = pd.read_csv(\"../input/nfl-impact-detection/train_labels.csv\")\ntrain_labels.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Let's filter the track data to analyze the same play we displayed above (happens to be the first play in the file).**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Filter the track data to the play of interest\ngame_key = track_data['gameKey'][0]\nplay_id = track_data['playID'][0]\nplay_track = track_data.loc[(track_data.gameKey == game_key) & (track_data.playID == play_id)]\nlen(play_track)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# See what events are stored in the data\nplay_track['event'].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Build a dataframe for the player positions at the snap\n\nat_snap = play_track.loc[play_track.event == 'ball_snap']\nat_snap","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"create_football_field()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**To start, we are going plot the player positions at the snap. Let's use a helper function to set the color for the home and visiting team.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# The visiting team *usually* wears white \ndef set_color(row):\n    if 'H' in row['player']:\n        return \"black\"\n    else:\n        return \"white\"\n\nat_snap['color'] = at_snap.apply(lambda row: set_color(row), axis=1)\nat_snap","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot the positions of players at the snap\n\nfig, ax = create_football_field()\nat_snap.plot(x=\"x\", y=\"y\",  kind='scatter', ax=ax, color = at_snap['color'], s=300)\nat_snap_home = at_snap.loc[at_snap['player'].str.contains('H')]\nat_snap_away = at_snap.loc[at_snap['player'].str.contains('V')]\n\nfor index, row in at_snap_away.iterrows():\n    ax.annotate(row['player'], (row['x'], row['y']), verticalalignment='center', horizontalalignment='center')\nfor index, row in at_snap_home.iterrows():\n    ax.annotate(row['player'], (row['x'], row['y']), verticalalignment='center', horizontalalignment='center', color = 'white')\nx_min = min(at_snap['x']) - 5\nx_max = max(at_snap['x']) + 5\ny_min = min(at_snap['y']) - 5\ny_max = max(at_snap['y']) + 5\nax.set_xlim(x_min, x_max)\nax.set_ylim(y_min, y_max)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot the positions of players through the play\n\nplay_track['color'] = play_track.apply(lambda row: set_color(row), axis=1)\n\n# Filter to only include time after the snap\nsnap_time = at_snap['time'].iloc[0]\nplay_track = play_track.loc[play_track['time'] > snap_time]\n\nfig, ax = create_football_field()\nplay_track.plot(x=\"x\", y=\"y\",  kind='scatter', ax=ax, color = play_track['color'], s= 1)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Lets play more**"},{"metadata":{},"cell_type":"markdown","source":"# Create train_player_tracking.csv with impact annotation"},{"metadata":{},"cell_type":"markdown","source":"modify train_player_tracking.csv to only include tracking data that is in the video and annotate the tracking data point where impacts occured. To do this I first make alignment between tracking data and frames, and determine the time where impact occured by picking the closest time to the imact frame."},{"metadata":{"trusted":true},"cell_type":"code","source":"def make_alignment(train_track: pd.DataFrame, train_label: pd.DataFrame, video_dir: Path, game_key: int, play_id: int):\n    play_track = train_track.query(f\"gameKey == {game_key} & playID == {play_id}\")\n    play_label = train_label.query(f\"gameKey == {game_key} & playID == {play_id}\")\n    \n    play_track[\"impact\"] = 0\n    play_track[\"impactType\"] = \"\"\n    play_track[\"confidence\"] = 0\n    play_track[\"visibility\"] = 0\n    \n    snap_frame = play_track.query(\"event == 'ball_snap'\")\n    snap_time = snap_frame[\"time\"].iloc[0]\n    \n    video_name = f\"{game_key}_{str(play_id).rjust(6, '0')}_Endzone.mp4\"\n    video = cv2.VideoCapture(str(video_dir / video_name))\n    \n    fps = video.get(cv2.CAP_PROP_FPS)\n    nframes = play_label.frame.nunique()\n    \n    snap_time -= dt.timedelta(seconds=1.0 / fps * 10)\n    \n    duration = nframes / fps\n    end_time = snap_time + dt.timedelta(seconds=duration)\n    \n    play = play_track.loc[(play_track[\"time\"] >= snap_time) & (play_track[\"time\"] < end_time)].copy()\n    \n    impact_frames = play_label.query(\"impact == 1 & view == 'Endzone'\")\n    for _, row in impact_frames.iterrows():\n        frame = row.frame\n        label = row.label\n        time_from_start = frame / fps\n        time = snap_time + dt.timedelta(seconds=time_from_start)\n        \n        abs_timedelta = abs(play[\"time\"] - time).dt.total_seconds()\n        min_abs_timedelta = abs_timedelta.min()\n        impact_point_index = play[abs_timedelta == min_abs_timedelta].query(\n            f\"player == '{label}'\").index[0]\n        play.loc[impact_point_index, \"impact\"] = 1\n        play.loc[impact_point_index, \"impactType\"] = row.impactType\n        play.loc[impact_point_index, \"confidence\"] = row.confidence\n        play.loc[impact_point_index, \"visibility\"] = row.visibility\n    play = play.reset_index(drop=False)\n    return play","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pairs = track_data.groupby([\"gameKey\", \"playID\"]).count().index.tolist()\nvideo_dir = Path(\"../input/nfl-impact-detection/train/\")\n\nplay_trackings = []\nfor game_key, play_id in pairs:\n    play_trackings.append(make_alignment(track_data, train_labels, video_dir, game_key, play_id))\n    \nannotated_trackings = pd.concat(play_trackings, axis=0).reset_index(drop=True)\nannotated_trackings.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"annotated_trackings.query(\"impact == 1 & gameKey == 57583 & playID == 82\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_labels.query(\"impact == 1 & gameKey == 57583 & playID == 82 & view == 'Endzone'\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(annotated_trackings.query(\n    \"impact == 1 & gameKey == 57583 & playID == 82\")), len(train_labels.query(\n    \"impact == 1 & gameKey == 57583 & playID == 82 & view == 'Endzone'\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"annotated_trackings.to_csv(\"submission.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"annotated_trackings.impactType.unique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Animation"},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_animation(play_track: pd.DataFrame, video_dir: Path, save_dir: Path):\n    fig, ax = create_football_field(figsize=(24, 12.66))\n\n    snap_frame = play_track.query(\"event == 'ball_snap'\")\n    snap_time = snap_frame[\"time\"].iloc[0]\n    snap_time -= dt.timedelta(seconds=0.1)\n    \n    game_key = play_track[\"gameKey\"].iloc[0]\n    play_id = play_track[\"playID\"].iloc[0]\n    \n    video_name = f\"{game_key}_{str(play_id).rjust(6, '0')}_Endzone.mp4\"\n    video = cv2.VideoCapture(str(video_dir / video_name))\n    \n    fps = video.get(cv2.CAP_PROP_FPS)\n    nframes = 0\n    while True:\n        worked, _ = video.read()\n        if not worked:\n            break\n        nframes += 1\n    \n    duration = nframes / fps\n    end_time = snap_time + dt.timedelta(seconds=duration)\n    \n    play = play_track.loc[(play_track[\"time\"] >= snap_time) & (play_track[\"time\"] < end_time)]\n\n    unique_times = play.time.unique()\n    \n    show_impact_marker = \"impact\" in play_track.columns\n    \n    # initialize the plot\n    points = {}\n    annotations = {}\n    obj_list = []\n    start_time = unique_times[0]\n    tracking_at_that_moment = play[play[\"time\"] == start_time]\n    for _, row in tracking_at_that_moment.iterrows():\n        player_id = row.player\n        x, y = row.x, row.y\n        if show_impact_marker:\n            impact = row.impact\n            if impact == 1:\n                impact_type = row.impactType\n                if impact_type == \"Helmet\":\n                    color = \"red\"\n                elif impact_type == \"Body\":\n                    color = \"yellow\"\n                elif impact_type == \"Shoulder\" or impact_type == \"shoulder\":\n                    color = \"blue\"\n                elif impact_type == \"Hand\":\n                    color = \"orange\"\n                elif impact_type == \"Ground\":\n                    color = \"purple\"\n            else:\n                color = row.color\n        else:\n            color = row.color\n        plot_obj = ax.scatter(x, y, color=color, s=70)\n        anno_obj = ax.annotate(player_id,\n                               (x, y),\n                               verticalalignment=\"center\",\n                               horizontalalignment=\"center\",\n                               color=\"white\" if color == \"black\" else \"black\",\n                               fontsize=10)\n        points[player_id] = plot_obj\n        annotations[player_id] = anno_obj\n        obj_list.append(plot_obj)\n        obj_list.append(anno_obj)\n        \n    def init():\n        return obj_list\n        \n    def update(step: int):\n        time = unique_times[step]\n        tracking_at_that_moment = play[play[\"time\"] == time]\n        for _, row in tracking_at_that_moment.iterrows():\n            player_id = row.player\n            x, y = row.x, row.y\n            points[player_id].set_offsets(np.array([x, y]))\n            if show_impact_marker:\n                impact = row.impact\n                if impact == 1:\n                    impact_type = row.impactType\n                    if impact_type == \"Helmet\":\n                        color = \"red\"\n                    elif impact_type == \"Body\":\n                        color = \"yellow\"\n                    elif impact_type == \"Shoulder\" or impact_type == \"shoulder\":\n                        color = \"blue\"\n                    elif impact_type == \"Hand\":\n                        color = \"orange\"\n                    elif impact_type == \"Ground\":\n                        color = \"purple\"\n                else:\n                    color = row.color\n                points[player_id].set_color(color)\n            annotations[player_id].set_x(x)\n            annotations[player_id].set_y(y)\n        return obj_list\n    \n    ani = animation.FuncAnimation(\n        fig, update, frames=len(unique_times), interval=100, init_func=init)\n    ani.save(save_dir / f\"{game_key}_{str(play_id).rjust(6, '0')}_Tracking.mp4\")\n    plt.close()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"game_key = track_data.loc[0, \"gameKey\"]\nplay_id = track_data.loc[0, \"playID\"]\nplay_track = track_data.query(f\"gameKey == {game_key} & playID == {play_id}\")\n\nvideo_dir = Path(\"../input/nfl-impact-detection/train/\")\nsave_dir = Path(\"./\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"create_animation(play_track, video_dir, save_dir)\n!ls","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Video(data=\"./57583_000082_Tracking.mp4\", embed=True)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Annoatations with impact marker"},{"metadata":{"trusted":true},"cell_type":"code","source":"play_track = annotated_trackings.query(f\"gameKey == {game_key} & playID == {play_id}\")\ncreate_animation(play_track, video_dir, save_dir)\nVideo(data=\"./57583_000082_Tracking.mp4\", embed=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Red markers correspond to impact events.\n\nThis animation corresponds to the play of 57583_000082_Endzone.mp4 and 57583_000082_Sideline.mp4. "},{"metadata":{},"cell_type":"markdown","source":"# Let's check it out."},{"metadata":{"trusted":true},"cell_type":"code","source":"Video(data=\"../input/nfl-impact-detection/train/57583_000082_Endzone.mp4\", embed=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# for sideline view"},{"metadata":{"trusted":true},"cell_type":"code","source":"Video(data=\"../input/nfl-impact-detection/train/57583_000082_Sideline.mp4\", embed=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# finally I am going to create this animation for entire train set."},{"metadata":{"trusted":true},"cell_type":"code","source":"save_dir = Path(\"./train_tracking\")\nsave_dir.mkdir(exist_ok=True, parents=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pairs = track_data.groupby([\"gameKey\", \"playID\"]).count().index.tolist()\npairs[:5]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for game_key, play_id in tqdm(pairs):\n    play_track = track_data.query(f\"gameKey == {game_key} & playID == {play_id}\")\n    create_animation(play_track, video_dir, save_dir)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{"trusted":true},"cell_type":"code","source":"\nsave_dir = Path(\"./train_tracking_with_impact_marker\")\nsave_dir.mkdir(exist_ok=True, parents=True)\nfor game_key, play_id in tqdm(pairs):\n    play_track = annotated_trackings.query(f\"gameKey == {game_key} & playID == {play_id}\")\n    create_animation(play_track, video_dir, save_dir)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Create this animation for all the test set"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_track_data = pd.read_csv(\"../input/nfl-impact-detection/test_player_tracking.csv\")\ntest_track_data[\"time\"] = pd.to_datetime(test_track_data[\"time\"])\ntest_track_data[\"color\"] = test_track_data[\"player\"].map(lambda x: \"black\" if \"H\" in x else \"white\")\nsave_dir = Path(\"./test_tracking\")\nsave_dir.mkdir(exist_ok=True, parents=True)\n\npairs = test_track_data.groupby([\"gameKey\", \"playID\"]).count().index.tolist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"video_dir = Path(\"../input/nfl-impact-detection/test\")\nfor game_key, play_id in tqdm(pairs):\n    play_track = test_track_data.query(f\"gameKey == {game_key} & playID == {play_id}\")\n    create_animation(play_track, video_dir, save_dir)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}