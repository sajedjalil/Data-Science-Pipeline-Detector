{"cells":[{"metadata":{"_uuid":"268c4ba7ad518912ad556de261e58e21665e348f"},"cell_type":"markdown","source":"## Table of Contents\n\n1. [Reading and Cleaning the dataset](#rcd)\n2. [Count Vectorizer with Stopwords](#cvs)\n3. [Sentiment wise term frequency](#swtf)\n4. [Word Cloud of the Negative Sentiment](#wcn)\n5. [Word Cloud of the Positive Sentiment](#wcp)\n6. [Count Vectorizer - Bigram and trigram method](#cvbt)\n7. [Count Vectorizer - Bigram and trigram method without stop words](#cvbtns)\n8. [Comparing Bi-grams - With and without stop words](#cbwwo)\n9. [Tfidf Vectorizer](#tfv)"},{"metadata":{"_uuid":"89154e9b4c53338c1888232a3c73a8d8f25c8f49"},"cell_type":"markdown","source":"<a id='rcd'></a>\n\n## Reading and Cleaning"},{"metadata":{"trusted":false,"_uuid":"d2b4d91406052ea3c2171a433699715a735f030e"},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport warnings\nwarnings.filterwarnings('ignore')\n\ntrain = pd.read_csv(\"../input/train.tsv\", sep = '\\t')\ntest = pd.read_csv(\"../input/test.tsv\", sep = '\\t')\nsub = pd.read_csv(\"../input/sampleSubmission.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"273b426e5e4b1ba720254ec6c9078825f297abcb"},"cell_type":"code","source":"train['Phrase'] = train['Phrase'].str.replace(r'\\'s', '')\ntrain['Phrase'] = train['Phrase'].str.replace(r'.', '')\ntrain['Phrase'] = train['Phrase'].str.replace(r',', '')\ntrain['Phrase'] = train['Phrase'].str.replace(r'does n\\'t', 'does not')\ntrain['Phrase'] = train['Phrase'].str.replace(r'is n\\'t', 'is not')\ntrain['Phrase'] = train['Phrase'].str.replace(r'were n\\'t', 'were not')\ntrain['Phrase'] = train['Phrase'].str.replace(r'are n\\'t', 'are not')\ntrain['Phrase'] = train['Phrase'].str.replace(r'had n\\'t', 'had not')\ntrain['Phrase'] = train['Phrase'].str.replace(r'have n\\'t', 'have not')\ntrain['Phrase'] = train['Phrase'].str.replace(r'would n\\'t', 'would not')\ntrain['Phrase'] = train['Phrase'].str.replace(r'ca n\\'t', 'can not')\ntrain['Phrase'] = train['Phrase'].str.replace(r'could n\\'t', 'could not')\ntrain['Phrase'] = train['Phrase'].str.replace(r'must n\\'t', 'must not')\ntrain['Phrase'] = train['Phrase'].str.replace(r'should n\\'t', 'should not')\ntrain['Phrase'] = train['Phrase'].str.replace(r'wo n\\'t', 'will not')\ntrain['Phrase'] = train['Phrase'].str.replace(r'n\\'t', 'not')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"d991528cfc4b659591a3217aa76bfa7f03b0a6ae"},"cell_type":"code","source":"test['Phrase'] = test['Phrase'].str.replace(r'\\'s', '')\ntest['Phrase'] = test['Phrase'].str.replace(r'.', '')\ntest['Phrase'] = test['Phrase'].str.replace(r',', '')\ntest['Phrase'] = test['Phrase'].str.replace(r'does n\\'t', 'does not')\ntest['Phrase'] = test['Phrase'].str.replace(r'is n\\'t', 'is not')\ntest['Phrase'] = test['Phrase'].str.replace(r'were n\\'t', 'were not')\ntest['Phrase'] = test['Phrase'].str.replace(r'are n\\'t', 'are not')\ntest['Phrase'] = test['Phrase'].str.replace(r'had n\\'t', 'had not')\ntest['Phrase'] = test['Phrase'].str.replace(r'have n\\'t', 'have not')\ntest['Phrase'] = test['Phrase'].str.replace(r'would n\\'t', 'would not')\ntest['Phrase'] = test['Phrase'].str.replace(r'ca n\\'t', 'can not')\ntest['Phrase'] = test['Phrase'].str.replace(r'could n\\'t', 'could not')\ntest['Phrase'] = test['Phrase'].str.replace(r'must n\\'t', 'must not')\ntest['Phrase'] = test['Phrase'].str.replace(r'should n\\'t', 'should not')\ntest['Phrase'] = test['Phrase'].str.replace(r'wo n\\'t', 'will not')\ntest['Phrase'] = test['Phrase'].str.replace(r'n\\'t', 'not')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"37dfba8d52dc9207ddcd2dd804005db88b5c0d05"},"cell_type":"code","source":"y_train = train['Sentiment']","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"7cfcd41c1acfae606b88138ee56fedf17b70816c"},"cell_type":"code","source":"#train = train.drop('Sentiment', axis=1)\n#train","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e130c721df570afb37cd216276cbbe9717070765"},"cell_type":"markdown","source":"<a id='cvs'></a>\n\n## Count Vectorizer with Stopwords"},{"metadata":{"trusted":false,"_uuid":"15af937900a76fb7b7f047c79a461f4acc900995"},"cell_type":"code","source":"# Vectorization\n\nfrom sklearn.feature_extraction.text import CountVectorizer\n\ncv = CountVectorizer(binary=True)\ncv.fit(train.Phrase)\nX = cv.transform(train.Phrase)\nX_test = cv.transform(test.Phrase)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"ea99c87c5448052dab761d047e1b60b5d70e818c"},"cell_type":"code","source":"pos = [3,4]\nneg = [0,1]\nneutral = 2\n\ntrain_pos = train[train.Sentiment.isin(pos)]\ntrain_pos = train_pos['Phrase']\ntrain_neg = train[train.Sentiment.isin(neg)]\ntrain_neg = train_neg['Phrase']","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"2f4252df496cb7e7d25ccfe7460c07e54bfe99f6"},"cell_type":"code","source":"from wordcloud import WordCloud,STOPWORDS\n\ndef wordcloud_draw(data, color = 'black'):\n    words = ' '.join(data)\n    cleaned_word = \" \".join([word for word in words.split()\n                            if not word.startswith(',')\n                            ])\n    wordcloud = WordCloud(stopwords=STOPWORDS,\n                      background_color=color,\n                      width=2500,\n                      height=2000\n                     ).generate(cleaned_word)\n    plt.figure(1,figsize=(13, 13))\n    plt.imshow(wordcloud)\n    plt.axis('off')\n    plt.show()\n    \nprint(\"Positive words\")\nwordcloud_draw(train_pos,'white')\nprint(\"Negative words\")\nwordcloud_draw(train_neg)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"54301a234bc6a4ccf1b06c7ef4d56f803f36f370"},"cell_type":"code","source":"# Building Classifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"eecbcc2bfc619dd6d7fbfd8f4b0c11ca30cc521f"},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\nimport warnings\nwarnings.filterwarnings('ignore')\n\nX_train, X_val, y_train, y_val = train_test_split(\n    X, y_train, train_size = 0.75\n)\n\nfor c in [0.01, 0.05, 0.25, 0.5, 1]:\n    \n    lr = LogisticRegression(C=c)\n    lr.fit(X_train, y_train)\n    print (\"Accuracy for C=%s: %s\" \n           % (c, accuracy_score(y_val, lr.predict(X_val))))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"c83b92b323ea2e723ae5ef37472de6956c209206"},"cell_type":"code","source":"y_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"027e9607d8508456fc4d0b4aaf0d46b6b688366b"},"cell_type":"code","source":"final_model = LogisticRegression(C=1)\n\nfinal_model.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"ee2755a55e06b08a5d49c2bb4f76a842d0033c95"},"cell_type":"code","source":"len(cv.get_feature_names())","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"10c9ef52110dbce0a2a5bf5d0e52868517ee59ec"},"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\ncv = CountVectorizer()\ncv.fit(train.Phrase)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"ba50b77eb8665de88e95809dc736d56d176f39a9"},"cell_type":"code","source":"neg_doc_mat = train.Phrase[train['Sentiment'] == 0]\nneg_document_matrix = cv.transform(train.Phrase[train['Sentiment'] == 0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"77d25ee28a6d1cc7a18b91dd15533ad629636ea8"},"cell_type":"code","source":"pos_doc_mat = train.Phrase[train['Sentiment'] == 4]\npos_document_matrix = cv.transform(pos_doc_mat)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"3daf05d185697983a2462d397fbccbbe6f766176"},"cell_type":"code","source":"neu_doc_mat = train.Phrase[train['Sentiment'] == 2]\nneu_document_matrix = cv.transform(neu_doc_mat)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"e0687bc04cb4d3e275d99bd0cdf9a69d0e20bd51"},"cell_type":"code","source":"sneg_doc_mat = train.Phrase[train['Sentiment'] == 1]\nsneg_document_matrix = cv.transform(sneg_doc_mat)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"146e703f6562048deedb5743d5e18786f775d017"},"cell_type":"code","source":"spos_doc_mat = train.Phrase[train['Sentiment'] == 3]\nspos_document_matrix = cv.transform(spos_doc_mat)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"cd00d59cabc0fc1b8b1340611ddbbbc611604499"},"cell_type":"code","source":"%%time\nneg_batches = np.linspace(0,156061,100).astype(int)\ni=0\nneg_tf = []\nwhile i < len(neg_batches)-1:\n    batch_result = np.sum(neg_document_matrix[neg_batches[i]:neg_batches[i+1]].toarray(),axis=0)\n    neg_tf.append(batch_result)\n    if (i % 10 == 0) | (i == len(neg_batches)-2):\n        print(neg_batches[i+1],\"entries' term frequency calculated\")\n    i += 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"1d5a48c9575ab9414b883393e6b2d82274f8d8e9"},"cell_type":"code","source":"%%time\npos_batches = np.linspace(0,156061,100).astype(int)\ni=0\npos_tf = []\nwhile i < len(pos_batches)-1:\n    batch_result = np.sum(pos_document_matrix[pos_batches[i]:pos_batches[i+1]].toarray(),axis=0)\n    pos_tf.append(batch_result)\n    if (i % 10 == 0) | (i == len(pos_batches)-2):\n        print(pos_batches[i+1],\"entries' term frequency calculated\")\n    i += 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"83142f8c5db83e03e2a69724968d194fcbab4072"},"cell_type":"code","source":"%%time\nneu_batches = np.linspace(0,156061,100).astype(int)\ni=0\nneu_tf = []\nwhile i < len(neu_batches)-1:\n    batch_result = np.sum(neu_document_matrix[neu_batches[i]:neu_batches[i+1]].toarray(),axis=0)\n    neu_tf.append(batch_result)\n    if (i % 10 == 0) | (i == len(neu_batches)-2):\n        print(neu_batches[i+1],\"entries' term frequency calculated\")\n    i += 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"70fa6631076a22628fc6b3d8ddaf681986209d46"},"cell_type":"code","source":"%%time\nsneg_batches = np.linspace(0,156061,100).astype(int)\ni=0\nsneg_tf = []\nwhile i < len(sneg_batches)-1:\n    batch_result = np.sum(sneg_document_matrix[sneg_batches[i]:sneg_batches[i+1]].toarray(),axis=0)\n    sneg_tf.append(batch_result)\n    if (i % 10 == 0) | (i == len(sneg_batches)-2):\n        print(sneg_batches[i+1],\"entries' term frequency calculated\")\n    i += 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"cc65c36d3e5953e9d7f36f40bfdfea4c528cf05b"},"cell_type":"code","source":"%%time\nspos_batches = np.linspace(0,156061,100).astype(int)\ni=0\nspos_tf = []\nwhile i < len(spos_batches)-1:\n    batch_result = np.sum(spos_document_matrix[spos_batches[i]:spos_batches[i+1]].toarray(),axis=0)\n    spos_tf.append(batch_result)\n    if (i % 10 == 0) | (i == len(spos_batches)-2):\n        print(spos_batches[i+1],\"entries' term frequency calculated\")\n    i += 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"daf7f0eb356accbd51071500084bda89cc4b7700"},"cell_type":"code","source":"neg = np.sum(neg_tf,axis=0)\npos = np.sum(pos_tf,axis=0)\nneu = np.sum(neu_tf,axis=0)\nsneg = np.sum(sneg_tf,axis=0)\nspos = np.sum(spos_tf,axis=0)\nterm_freq_df = pd.DataFrame([neg,pos, neu, sneg, spos],columns=cv.get_feature_names()).transpose()\nterm_freq_df.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4d5c9e668304d3e1fc553e9747f5a342f130c2f4"},"cell_type":"markdown","source":"<a id='swtf'></a>\n\n### Sentiment-wise term frequency"},{"metadata":{"trusted":false,"_uuid":"abc67c3809d039fbf52e460d7ae65bbc8443969d"},"cell_type":"code","source":"term_freq_df.columns = ['negative', 'positive', 'neutral', 'somewhat negative', 'somewhat positive']\nterm_freq_df['total'] = term_freq_df['negative'] + term_freq_df['positive'] + term_freq_df['neutral'] + term_freq_df['somewhat negative'] + term_freq_df['somewhat positive']\nterm_freq_df.sort_values(by='total', ascending=False).iloc[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"444fef82359d45735745f35cf5cb27ab9b2e0e7a"},"cell_type":"code","source":"type(term_freq_df)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b6e9b6bc9a0cb771dab406976197e72904258163"},"cell_type":"markdown","source":"<a id='wcn'></a>\n\n####  WORD CLOUD OF THE NEGATIVE SENTIMENT PHRASES"},{"metadata":{"trusted":false,"_uuid":"625b2d8fb96759c1dce5dd090da8ec6d908255ad"},"cell_type":"code","source":"neg_phrases = train[train.Sentiment == 0]\nneg_string = []\nfor t in neg_phrases.Phrase:\n    neg_string.append(t)\nneg_string = pd.Series(neg_string).str.cat(sep=' ')\nfrom wordcloud import WordCloud\n\nwordcloud = WordCloud(width=1600, height=800,max_font_size=200).generate(neg_string)\nplt.figure(figsize=(12,10))\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis(\"off\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ea4b9ee5347ef42c88aee2417578c54d44722dc0"},"cell_type":"markdown","source":"<a id='wcp'></a>\n\n####  WORD CLOUD OF THE POSITIVE SENTIMENT PHRASES"},{"metadata":{"trusted":false,"_uuid":"280e80d7cb2afabed2f93f96f4ffcb7f6f7cb21f"},"cell_type":"code","source":"pos_phrases = train[train.Sentiment == 4]\npos_string = []\nfor t in pos_phrases.Phrase:\n    pos_string.append(t)\npos_string = pd.Series(pos_string).str.cat(sep=' ')\nfrom wordcloud import WordCloud\n\nwordcloud = WordCloud(width=1600, height=800,max_font_size=200).generate(pos_string)\nplt.figure(figsize=(12,10))\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis(\"off\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"93255885ef03d2943bf861cd5953229a2a619f64"},"cell_type":"code","source":"y_pos = np.arange(500)\nplt.figure(figsize=(10,8))\ns = 1\nexpected_zipf = [term_freq_df.sort_values(by='total', ascending=False)['total'][0]/(i+1)**s for i in y_pos]\nplt.bar(y_pos, term_freq_df.sort_values(by='total', ascending=False)['total'][:500], align='center', alpha=0.5)\nplt.plot(y_pos, expected_zipf, color='r', linestyle='--',linewidth=2,alpha=0.5)\nplt.ylabel('Frequency')\nplt.title('Top 500 tokens in the Phrases')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"faa3ff2fe22e7a7f636c8d6a6fa59ce305937988"},"cell_type":"markdown","source":"<a id='vpw'></a>\n\n#### Visualising phrase words without stopwords"},{"metadata":{"trusted":false,"_uuid":"2c345d0c8d20067338489b0824a3b9acb33949be"},"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\ncvec = CountVectorizer(stop_words='english',max_features=10000)\ncvec.fit(train.Phrase)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"e592a93e45a661cf4f989cebb5fefc1e96f47abb"},"cell_type":"code","source":"neg_document_matrix_nostop = cvec.transform(train.Phrase[train['Sentiment'] == 0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"2a0d532263bd3b76cb3ada9cedcd13e487621c42"},"cell_type":"code","source":"%%time\nneg_batches = np.linspace(0,156061,100).astype(int)\ni=0\nneg_tf = []\nwhile i < len(neg_batches)-1:\n    batch_result = np.sum(neg_document_matrix_nostop[neg_batches[i]:neg_batches[i+1]].toarray(),axis=0)\n    neg_tf.append(batch_result)\n    print(neg_batches[i+1],\"entries' term frequency calculated\")\n    i += 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"c078b9ffc939b48fb4dd410d1c2f2fc8bb29641d"},"cell_type":"code","source":"pos_document_matrix_nostop = cvec.transform(train.Phrase[train['Sentiment'] == 4])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"45967a8e0be4066ba966c302f5a5ed74c5f6a8fb"},"cell_type":"code","source":"%%time\npos_batches = np.linspace(0,156061,100).astype(int)\ni=0\npos_tf = []\nwhile i < len(pos_batches)-1:\n    batch_result = np.sum(pos_document_matrix_nostop[pos_batches[i]:pos_batches[i+1]].toarray(),axis=0)\n    pos_tf.append(batch_result)\n    print(pos_batches[i+1],\"entries' term frequency calculated\")\n    i += 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"d148b1f63111d4818801a0f9fb1e6befa6a73f22"},"cell_type":"code","source":"neg = np.sum(neg_tf,axis=0)\npos = np.sum(pos_tf,axis=0)\nterm_freq_df2 = pd.DataFrame([neg,pos],columns=cvec.get_feature_names()).transpose()\nterm_freq_df2.columns = ['negative', 'positive']\nterm_freq_df2['total'] = term_freq_df2['negative'] + term_freq_df2['positive']\nterm_freq_df2.sort_values(by='total', ascending=False).iloc[:10]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bb11c5dc5d83368b8242752fa93df718021979ed"},"cell_type":"markdown","source":"### Top 50 Tokens among the negative sentiment"},{"metadata":{"trusted":false,"_uuid":"03aaa8f81212629457372cc052c958928a82a885"},"cell_type":"code","source":"y_pos = np.arange(50)\nplt.figure(figsize=(12,10))\nplt.bar(y_pos, term_freq_df2.sort_values(by='negative', ascending=False)['negative'][:50], align='center', alpha=0.5)\nplt.xticks(y_pos, term_freq_df2.sort_values(by='negative', ascending=False)['negative'][:50].index,rotation='vertical')\nplt.ylabel('Frequency')\nplt.xlabel('Top 50 negative words')\nplt.title('Top 50 tokens in negative sentiments')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"66b9f7d2a2fc9d043216a37a9cba88495c6f81c9"},"cell_type":"markdown","source":"### Top 50 Tokens among the positive sentiment"},{"metadata":{"trusted":false,"_uuid":"a646812327f2240e5e9fc8cbeb38577eb6932b37"},"cell_type":"code","source":"y_pos = np.arange(50)\nplt.figure(figsize=(12,10))\nplt.bar(y_pos, term_freq_df2.sort_values(by='positive', ascending=False)['positive'][:50], align='center', alpha=0.5)\nplt.xticks(y_pos, term_freq_df2.sort_values(by='positive', ascending=False)['positive'][:50].index,rotation='vertical')\nplt.ylabel('Frequency')\nplt.xlabel('Top 50 positive tokens')\nplt.title('Top 50 tokens in positive sentiments')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e9df1fe92c00e8b9cc167cdf9cd9f84f25155bbd"},"cell_type":"markdown","source":"### In both Negative and positive plot, neutral words such as film, movie, story are quite high up in the rank."},{"metadata":{"trusted":false,"_uuid":"58e1db8040ecf3cf9a9c5f1138759b5573ff15a6"},"cell_type":"code","source":"import seaborn as sns\nplt.figure(figsize=(8,6))\nax = sns.regplot(x=\"negative\", y=\"positive\",fit_reg=False, scatter_kws={'alpha':0.5},data=term_freq_df2)\nplt.ylabel('Positive Frequency')\nplt.xlabel('Negative Frequency')\nplt.title('Negative Frequency vs Positive Frequency')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"798a023b0acf6dd8ba2008bbdcdae37a0f64bcbb"},"cell_type":"code","source":"term_freq_df2['pos_rate'] = term_freq_df2['positive'] * 1./term_freq_df2['total']\nterm_freq_df2.sort_values(by='pos_rate', ascending=False).iloc[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"b98ea42917c40bdaca06e15ef790213baf50528a"},"cell_type":"code","source":"term_freq_df2['pos_freq_pct'] = term_freq_df2['positive'] * 1./term_freq_df2['positive'].sum()\nterm_freq_df2.sort_values(by='pos_freq_pct', ascending=False).iloc[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"5bd82bdd262f56003e687633abba963e482b53aa"},"cell_type":"code","source":"from scipy.stats import hmean\n\nterm_freq_df2['pos_hmean'] = term_freq_df2.apply(lambda x: (hmean([x['pos_rate'], x['pos_freq_pct']])\n                                                                   if x['pos_rate'] > 0 and x['pos_freq_pct'] > 0 \n                                                                   else 0), axis=1)                                                        \nterm_freq_df2.sort_values(by='pos_hmean', ascending=False).iloc[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"bb748e8db6e9b6f7b95d3368308d481eff1cb31e"},"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')\n\nterm_freq_df2['neg_rate'] = term_freq_df2['negative'] * 1./term_freq_df2['total']\nterm_freq_df2['neg_freq_pct'] = term_freq_df2['negative'] * 1./term_freq_df2['negative'].sum()\nterm_freq_df2['neg_hmean'] = term_freq_df2.apply(lambda x: (hmean([x['neg_rate'], x['neg_freq_pct']])\n                                                                   if x['neg_rate'] > 0 and x['neg_freq_pct'] > 0 \n                                                                   else 0), axis=1)                                                        \n#term_freq_df2['neg_rate_normcdf'] = normcdf(term_freq_df2['neg_rate'])\n#term_freq_df2['neg_freq_pct_normcdf'] = normcdf(term_freq_df2['neg_freq_pct'])\n#term_freq_df2['neg_normcdf_hmean'] = hmean([term_freq_df2['neg_rate_normcdf'], term_freq_df2['neg_freq_pct_normcdf']])\n#term_freq_df2.sort_values(by='neg_normcdf_hmean', ascending=False).iloc[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"19b0be34638122ab8de0073240bfb50460e8ff33"},"cell_type":"code","source":"from scipy.stats import norm\ndef normcdf(x):\n    return norm.cdf(x, x.mean(), x.std())\n\nterm_freq_df2['pos_rate_normcdf'] = normcdf(term_freq_df2['pos_rate'])\nterm_freq_df2['pos_freq_pct_normcdf'] = normcdf(term_freq_df2['pos_freq_pct'])\n#term_freq_df2['pos_normcdf_hmean'] = hmean([term_freq_df2['pos_rate_normcdf'], term_freq_df2['pos_freq_pct_normcdf']])\n#term_freq_df2.sort_values(by='pos_normcdf_hmean', ascending=False).iloc[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"2c40795f70995a27034f71d56a9683b01abecf30"},"cell_type":"code","source":"plt.figure(figsize=(8,6))\nax = sns.regplot(x=\"neg_hmean\", y=\"pos_hmean\",fit_reg=False, scatter_kws={'alpha':0.5},data=term_freq_df2)\nplt.ylabel('Positive Rate and Frequency Harmonic Mean')\nplt.xlabel('Negative Rate and Frequency Harmonic Mean')\nplt.title('neg hmean vs pos hmean')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"e7af895413702d0d4f283f5de2f80057d73cea42"},"cell_type":"code","source":"x = train.Phrase\ny = train.Sentiment\nfrom sklearn.cross_validation import train_test_split\n\nSEED = 2000\n\nx_train, x_validation_and_test, y_train, y_validation_and_test = train_test_split(x, y, test_size=.02, random_state=SEED)\nx_validation, x_test, y_validation, y_test = train_test_split(x_validation_and_test, y_validation_and_test, test_size=.5, random_state=SEED)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"2a945b7cf0f03607ff7894a7219350aee66524bd"},"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import Pipeline\nfrom time import time\nfrom sklearn.metrics import confusion_matrix, classification_report, accuracy_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"f8107e50afd4e3c2cfcfca8aa37da7a2337cf975"},"cell_type":"code","source":"def accuracy_summary(pipeline, x_train, y_train, x_test, y_test):\n    if len(x_test[y_test == 0]) / (len(x_test)*1.) > 0.5:\n        null_accuracy = len(x_test[y_test == 0]) / (len(x_test)*1.)\n    else:\n        null_accuracy = 1. - (len(x_test[y_test == 0]) / (len(x_test)*1.))\n    t0 = time()\n    sentiment_fit = pipeline.fit(x_train, y_train)\n    y_pred = sentiment_fit.predict(x_test)\n    train_test_time = time() - t0\n    accuracy = accuracy_score(y_test, y_pred)\n    print(\"null accuracy: {0:.2f}%\".format(null_accuracy*100))\n    print(\"accuracy score: {0:.2f}%\".format(accuracy*100))\n    if accuracy > null_accuracy:\n        print(\"model is {0:.2f}% more accurate than null accuracy\".format((accuracy-null_accuracy)*100))\n    elif accuracy == null_accuracy:\n        print(\"model has the same accuracy with the null accuracy\")\n    else:\n        print(\"model is {0:.2f}% less accurate than null accuracy\".format((null_accuracy-accuracy)*100))\n    print(\"train and test time: {0:.2f}s\".format(train_test_time))\n    print(\"-\"*80)\n    return accuracy, train_test_time\ncvec = CountVectorizer()\nlr = LogisticRegression()\nn_features = np.arange(10000,100001,10000)\n\ndef nfeature_accuracy_checker(vectorizer=cvec, n_features=n_features, stop_words=None, ngram_range=(1, 1), classifier=lr):\n    result = []\n    print (classifier)\n    print(\"\\n\")\n    for n in n_features:\n        vectorizer.set_params(stop_words=stop_words, max_features=n, ngram_range=ngram_range)\n        checker_pipeline = Pipeline([\n            ('vectorizer', vectorizer),\n            ('classifier', classifier)\n        ])\n        print(\"Validation result for {} features\".format(n))\n        nfeature_accuracy,tt_time = accuracy_summary(checker_pipeline, x_train, y_train, x_validation, y_validation)\n        result.append((n,nfeature_accuracy,tt_time))\n    return result","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"989232346c4779485d0251578f9eb102c718f6b1"},"cell_type":"code","source":"# Just to check if these top 10 words in term frequency data frame are actually included in Sklearnâ€™s stop words list","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"a6c125374c541a315d236f09748795749884debc"},"cell_type":"code","source":"from sklearn.feature_extraction import text\n\na = frozenset(list(term_freq_df.sort_values(by='total', ascending=False).iloc[:10].index))\nb = text.ENGLISH_STOP_WORDS\nset(a).issubset(set(b))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"7081e1052480f183a928b23124a7b83a7a804b3d"},"cell_type":"code","source":"my_stop_words = frozenset(list(term_freq_df.sort_values(by='total', ascending=False).iloc[:10].index))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"2b8636bad53e5d0b039180cc57d6709cb985f5ba"},"cell_type":"code","source":"print(\"RESULT FOR UNIGRAM WITHOUT STOP WORDS\\n\")\nfeature_result_wosw = nfeature_accuracy_checker(stop_words='english')\n\nprint(\"RESULT FOR UNIGRAM WITH STOP WORDS\\n\")\nfeature_result_ug = nfeature_accuracy_checker()\n\nprint(\"RESULT FOR UNIGRAM WITHOUT CUSTOM STOP WORDS (Top 10 frequent words)\\n\")\nfeature_result_wocsw = nfeature_accuracy_checker(stop_words=my_stop_words)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"b563ff1497fa0c8fef2cf87f3c66d65412d08f2e"},"cell_type":"code","source":"nfeatures_plot_ug = pd.DataFrame(feature_result_ug,columns=['nfeatures','validation_accuracy','train_test_time'])\nnfeatures_plot_ug_wocsw = pd.DataFrame(feature_result_wocsw,columns=['nfeatures','validation_accuracy','train_test_time'])\nnfeatures_plot_ug_wosw = pd.DataFrame(feature_result_wosw,columns=['nfeatures','validation_accuracy','train_test_time'])\n\nplt.figure(figsize=(8,6))\nplt.plot(nfeatures_plot_ug.nfeatures, nfeatures_plot_ug.validation_accuracy, label='with stop words')\nplt.plot(nfeatures_plot_ug_wocsw.nfeatures, nfeatures_plot_ug_wocsw.validation_accuracy,label='without custom stop words')\nplt.plot(nfeatures_plot_ug_wosw.nfeatures, nfeatures_plot_ug_wosw.validation_accuracy,label='without stop words')\nplt.title(\"Without stop words VS With stop words (Unigram): Accuracy\")\nplt.xlabel(\"Number of features\")\nplt.ylabel(\"Validation set accuracy\")\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8ebaa6c6dd0da04bfd08709c0da59750c4cbfff1"},"cell_type":"markdown","source":"### For Count Vectorizer, validation set accuracy is better without the custom stop words"},{"metadata":{"_uuid":"9cce91ceab12e8e8e05d4c3cc8f8d9a2639f7023"},"cell_type":"markdown","source":"<a id='cvbt'></a>\n\n#### Count Vectorizer - Bigram and trigram method"},{"metadata":{"trusted":false,"_uuid":"8ae29387f32886bee8c3931bf0cec2778375da28"},"cell_type":"code","source":"print(\"RESULT FOR BIGRAM WITH STOP WORDS\\n\")\nfeature_result_bg = nfeature_accuracy_checker(ngram_range=(1, 2))\n\nprint(\"RESULT FOR TRIGRAM WITH STOP WORDS\\n\")\nfeature_result_tg = nfeature_accuracy_checker(ngram_range=(1, 3))","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":false,"_uuid":"da86a72ad34787cfca7dd0deca7caf30804b31cc"},"cell_type":"code","source":"nfeatures_plot_tg = pd.DataFrame(feature_result_tg,columns=['nfeatures','validation_accuracy','train_test_time'])\nnfeatures_plot_bg = pd.DataFrame(feature_result_bg,columns=['nfeatures','validation_accuracy','train_test_time'])\nnfeatures_plot_ug = pd.DataFrame(feature_result_ug,columns=['nfeatures','validation_accuracy','train_test_time'])\n\nplt.figure(figsize=(8,6))\nplt.plot(nfeatures_plot_tg.nfeatures, nfeatures_plot_tg.validation_accuracy,label='trigram')\nplt.plot(nfeatures_plot_bg.nfeatures, nfeatures_plot_bg.validation_accuracy,label='bigram')\nplt.plot(nfeatures_plot_ug.nfeatures, nfeatures_plot_ug.validation_accuracy, label='unigram')\nplt.title(\"N-gram(1~3) test result : Accuracy\")\nplt.xlabel(\"Number of features\")\nplt.ylabel(\"Validation set accuracy\")\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2486fd491e6cb4289c0dd0972ae52a2a3819fdb4"},"cell_type":"markdown","source":"<a id='cvbtns'></a>\n\n#### Count Vectorizer - Bigram and trigram method without stop words"},{"metadata":{"trusted":false,"_uuid":"710ba5e7ec77a629bdc99ecbaef4f5c500a6ef61"},"cell_type":"code","source":"print(\"RESULT FOR BIGRAM WITHOUT STOP WORDS\\n\")\nfeature_result_bg_nostop = nfeature_accuracy_checker(ngram_range=(1, 2), stop_words='english')\n\nprint(\"RESULT FOR TRIGRAM WITHOUT STOP WORDS\\n\")\nfeature_result_tg_nostop = nfeature_accuracy_checker(ngram_range=(1, 3), stop_words='english')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"504940193ea00bda7b38deaf8dce8784de47152c"},"cell_type":"markdown","source":"#### Count vectorizer Bigram and trigram plot - Without Stop words"},{"metadata":{"trusted":false,"_uuid":"52b0e0dfad56fed96d16487af0d47054615c9a13"},"cell_type":"code","source":"nfeatures_plot_bg_nostop = pd.DataFrame(feature_result_bg_nostop,columns=['nfeatures','validation_accuracy','train_test_time'])\nnfeatures_plot_tg_nostop = pd.DataFrame(feature_result_tg_nostop,columns=['nfeatures','validation_accuracy','train_test_time'])\n\nplt.figure(figsize=(8,6))\nplt.plot(nfeatures_plot_tg_nostop.nfeatures, nfeatures_plot_tg_nostop.validation_accuracy,label='trigram')\nplt.plot(nfeatures_plot_bg_nostop.nfeatures, nfeatures_plot_bg_nostop.validation_accuracy,label='bigram')\nplt.title(\"N-gram(2~3) test result : Accuracy\")\nplt.xlabel(\"Number of features\")\nplt.ylabel(\"Validation set accuracy\")\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b5962839e74be409427faa999ffdc23a18b63f29"},"cell_type":"markdown","source":"<a id='cbwwo'></a>\n\n#### Comparing Bi-grams - With and without stop words"},{"metadata":{"trusted":false,"_uuid":"900f62d81d23f0809b20da535586e0d3950c2d11"},"cell_type":"code","source":"plt.figure(figsize=(8,6))\nplt.plot(nfeatures_plot_bg.nfeatures, nfeatures_plot_bg.validation_accuracy, label='Bigram')\nplt.plot(nfeatures_plot_bg_nostop.nfeatures, nfeatures_plot_bg_nostop.validation_accuracy, label='Bigram-Nostop')\nplt.title(\"N-gram(2~3) test result : Accuracy\")\nplt.xlabel(\"Number of features\")\nplt.ylabel(\"Validation set accuracy\")\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ef77dd7bc4418e437bf6c62ebcafe6869ebf3b5b"},"cell_type":"markdown","source":"### Bigram model is better among the 3 "},{"metadata":{"_uuid":"573b96b6982de61d0d0b2f4ae2c73af65f01cc07"},"cell_type":"markdown","source":"### Best performing number of features with each n-gram"},{"metadata":{"trusted":false,"_uuid":"1c2a0900a03db26037cab95fb91e3ff3298721a5"},"cell_type":"code","source":"def train_test_and_evaluate(pipeline, x_train, y_train, x_test, y_test):\n    if len(x_test[y_test == 0]) / (len(x_test)*1.) > 0.5:\n        null_accuracy = len(x_test[y_test == 0]) / (len(x_test)*1.)\n    else:\n        null_accuracy = 1. - (len(x_test[y_test == 0]) / (len(x_test)*1.))\n    sentiment_fit = pipeline.fit(x_train, y_train)\n    \n    y_pred = sentiment_fit.predict(x_test)\n    accuracy = accuracy_score(y_test, y_pred)\n    conmat = np.array(confusion_matrix(y_test, y_pred, labels=[0,1,2,3,4]))\n    confusion = pd.DataFrame(conmat, index=['negative','somewhat negative', 'neutral','somewhat positive', 'positive'],\n                         columns=['predicted negative','predicted somewhat negative', 'predicted neutral','predicted somewhat positive', 'predicted positive'])\n    \n    print(\"null accuracy: {0:.2f}%\".format(null_accuracy*100))\n    print(\"accuracy score: {0:.2f}%\".format(accuracy*100))\n    if accuracy > null_accuracy:\n        print(\"model is {0:.2f}% more accurate than null accuracy\".format((accuracy-null_accuracy)*100))\n    elif accuracy == null_accuracy:\n        print(\"model has the same accuracy with the null accuracy\")\n    else:\n        print(\"model is {0:.2f}% less accurate than null accuracy\".format((null_accuracy-accuracy)*100))\n    print(\"-\"*80)\n    print(\"Confusion Matrix\\n\")\n    print(confusion)\n    print(\"-\"*80)\n    print(\"Classification Report\\n\")\n    print(classification_report(y_test, y_pred, target_names=['negative','somewhat negative', 'neutral','somewhat positive', 'positive']))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"6e38eed0c6a923842c6741235e21918333b48fcb"},"cell_type":"code","source":"%%time\ntg_cvec = CountVectorizer(max_features=80000,ngram_range=(1, 3))\ntg_pipeline = Pipeline([\n        ('vectorizer', tg_cvec),\n        ('classifier', lr)\n    ])\ntrain_test_and_evaluate(tg_pipeline, x_train, y_train, x_validation, y_validation)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"5de88b36d4207f983178731574d4c37d9aca68dd"},"cell_type":"code","source":"%%time\nbg_cvec = CountVectorizer(max_features=80000,ngram_range=(1, 2))\nbg_pipeline = Pipeline([\n        ('vectorizer', bg_cvec),\n        ('classifier', lr)\n    ])\ntrain_test_and_evaluate(bg_pipeline, x_train, y_train, x_validation, y_validation)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"ee16b2bf232dd2ba3a48e2c8a4ea8fe7f4abaadc"},"cell_type":"code","source":"cv_train = bg_cvec.fit_transform(train.Phrase)\nprint(cv_train.shape)\n\ncv_test = bg_cvec.transform(test.Phrase)\nprint(cv_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"bd0a95137148af21be706ebba3f7d401ddecc07c"},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\nfrom sklearn.naive_bayes import MultinomialNB, BernoulliNB\nfrom sklearn.linear_model import RidgeClassifier\nfrom sklearn.linear_model import PassiveAggressiveClassifier\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.neighbors import NearestCentroid\nfrom sklearn.feature_selection import SelectFromModel\n\nlr = LogisticRegression()\nmn = MultinomialNB()\nbnb = BernoulliNB()\nrc = RidgeClassifier()\nabc = AdaBoostClassifier()\nlsvc = LinearSVC()\n\nlr.fit(cv_train,y)\nmn.fit(cv_train,y)\nbnb.fit(cv_train,y)\nrc.fit(cv_train,y)\nabc.fit(cv_train,y)\nlsvc.fit(cv_train,y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5bb5a44b76d68e8e72909402e2c7edcff84ca413"},"cell_type":"code","source":"lr_y_pred = lr.predict(cv_test)\n\nlr_sub = pd.DataFrame({\"PhraseId\": sub['PhraseId'], \"Sentiment\" : lr_y_pred})\n\nlr_sub.to_csv(\"LR_submission.csv\",index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"c465dc6364db05d2d9179364742a67ec4c093ad5"},"cell_type":"code","source":"mn_y_pred = mn.predict(cv_test)\n\nmn_sub = pd.DataFrame({\"PhraseId\": sub['PhraseId'], \"Sentiment\" : mn_y_pred})\n\nmn_sub.to_csv(\"MN_submission.csv\",index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"e5b4b21ee1976267fe30351bfb5ca9622f08ca4f"},"cell_type":"code","source":"rc_y_pred = rc.predict(cv_test)\n\nrc_sub = pd.DataFrame({\"PhraseId\": sub['PhraseId'], \"Sentiment\" : rc_y_pred})\n\nrc_sub.to_csv(\"RC_submission.csv\",index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"b146a888ff9b7109ae652d1b1ea423a397deed43"},"cell_type":"code","source":"abc_y_pred = abc.predict(cv_test)\n\nabc_sub = pd.DataFrame({\"PhraseId\": sub['PhraseId'], \"Sentiment\" : abc_y_pred})\n\nabc_sub.to_csv(\"abc_submission.csv\",index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"1600d788f19ce0f56518a821d199ec199c75dc54"},"cell_type":"code","source":"bnb_y_pred = bnb.predict(cv_test)\n\nbnb_sub = pd.DataFrame({\"PhraseId\": sub['PhraseId'], \"Sentiment\" : bnb_y_pred})\n\nbnb_sub.to_csv(\"bnb_submission.csv\",index=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4359deb898c7192aa70b328739cf7eb87867c992"},"cell_type":"markdown","source":"<a id='tfv'></a>\n\n### Tfidf Vectorizer"},{"metadata":{"trusted":false,"_uuid":"f1908bc8e285db9a7ab517cabde39a675a3e4e79"},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\nfrom wordcloud import WordCloud, STOPWORDS\ntvec = TfidfVectorizer(smooth_idf=False, sublinear_tf=False, norm=None, analyzer='word')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"1b68f0a83707737f01b0c0d0c200e991f25d5a93"},"cell_type":"code","source":"txt_fitted = tvec.fit(train.Phrase)\ntxt_transformed = txt_fitted.transform(train.Phrase)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"211aede81fbb5d4519435702fc45bd0589729a09"},"cell_type":"code","source":"idf = tvec.idf_\nprint(dict(zip(txt_fitted.get_feature_names(), idf)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"7151178bf392218f603e3e7efc629d80dc65911f"},"cell_type":"code","source":"# Instantiate the vectorizer\n\nword_vectorizer = TfidfVectorizer(\n    stop_words='english',\n    sublinear_tf=True,\n    strip_accents='unicode',\n    analyzer='word',\n   # token_pattern=r'\\w{2,}',  #vectorize 2-character words or more\n    ngram_range=(1, 2),\n    max_features=80000)\n\n# fit and transform on it the training features\nword_vectorizer.fit(train.Phrase)\nX_train_word_features = word_vectorizer.transform(train.Phrase)\n\n#transform the test features to sparse matrix\ntest_features = word_vectorizer.transform(test.Phrase)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"d39d582c877e7d8a1d533936b6edcd8a5a590c42"},"cell_type":"code","source":"td_train = word_vectorizer.fit_transform(train.Phrase)\nprint(td_train.shape)\n\ntd_test = word_vectorizer.transform(test.Phrase)\nprint(td_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"3ded1463fb3a1179b9d0c49c928f76843eafe807"},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\nfrom sklearn.naive_bayes import MultinomialNB, BernoulliNB\nfrom sklearn.linear_model import RidgeClassifier\nfrom sklearn.linear_model import PassiveAggressiveClassifier\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.neighbors import NearestCentroid\nfrom sklearn.feature_selection import SelectFromModel\n\nlr = LogisticRegression()\nmn = MultinomialNB()\nbnb = BernoulliNB()\nrc = RidgeClassifier()\nabc = AdaBoostClassifier()\nlsvc = LinearSVC()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"a459f9dc0ce9cad0fc920f0828120f478fbfee17"},"cell_type":"code","source":"lr.fit(td_train,y)\nmn.fit(td_train,y)\nbnb.fit(td_train,y)\nrc.fit(td_train,y)\nabc.fit(td_train,y)\nlsvc.fit(td_train,y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"4735adb81e8414cf2c77641b040eefa6f31026bd"},"cell_type":"code","source":"lr_y_pred_td = lr.predict(td_test)\n\nlr_sub_td = pd.DataFrame({\"PhraseId\": sub['PhraseId'], \"Sentiment\" : lr_y_pred_td})\n\nlr_sub_td.to_csv(\"LR_td_submission.csv\",index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"1dd11b20e22805045053c9c81eb0df6a96866f5d"},"cell_type":"code","source":"mn_y_pred_td = mn.predict(td_test)\n\nmn_sub_td = pd.DataFrame({\"PhraseId\": sub['PhraseId'], \"Sentiment\" : mn_y_pred_td})\n\nmn_sub_td.to_csv(\"MN_td_submission.csv\",index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"22249c6ec4d4c69fe00e49363acfd43d8b5003c9"},"cell_type":"code","source":"bnb_y_pred_td = bnb.predict(td_test)\n\nbnb_sub_td = pd.DataFrame({\"PhraseId\": sub['PhraseId'], \"Sentiment\" : bnb_y_pred_td})\n\nbnb_sub_td.to_csv(\"BN_td_submission.csv\",index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"4d6eb8fc781f1fd08233a7a022f3e18340251d74"},"cell_type":"code","source":"rc_y_pred_td = rc.predict(td_test)\n\nrc_sub_td = pd.DataFrame({\"PhraseId\": sub['PhraseId'], \"Sentiment\" : rc_y_pred_td})\n\nrc_sub_td.to_csv(\"RC_td_submission.csv\",index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"2e0a45942902ef4a39f65ea167bed16a5cb2ae38"},"cell_type":"code","source":"abc_y_pred_td = rc.predict(td_test)\n\nabc_sub_td = pd.DataFrame({\"PhraseId\": sub['PhraseId'], \"Sentiment\" : abc_y_pred_td})\n\nabc_sub_td.to_csv(\"ABC_td_submission.csv\",index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"dd6cdcc808e934b64daa7aaa6b898ebffce5ca45"},"cell_type":"code","source":"lsvc_y_pred_td = lsvc.predict(td_test)\n\nlsvc_sub_td = pd.DataFrame({\"PhraseId\": sub['PhraseId'], \"Sentiment\" : lsvc_y_pred_td})\n\nlsvc_sub_td.to_csv(\"LSVC_td_submission.csv\",index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"85957b5efec8cffd862fefea6cb5ecaca67b2a0e"},"cell_type":"code","source":"%%time\nprint(\"RESULT FOR UNIGRAM WITH STOP WORDS (Tfidf)\\n\")\nfeature_result_ugt = nfeature_accuracy_checker(vectorizer=tvec)\n\nprint(\"RESULT FOR UNIGRAM WITHOUT STOP WORDS (Tfidf)\\n\")\nfeature_result_ugt_nostop = nfeature_accuracy_checker(vectorizer=tvec, stop_words = 'english')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"99e3873787f018fd3589fece98cc1b4009045672"},"cell_type":"code","source":"max(feature_result_ugt)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"14784e17ac94055770759ddde425ee455176bf80"},"cell_type":"code","source":"%%time\nprint(\"RESULT FOR BIGRAM WITH STOP WORDS (Tfidf)\\n\")\nfeature_result_bgt = nfeature_accuracy_checker(vectorizer=tvec,ngram_range=(1, 2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"8d8b98879dbaf529115b12fa07aeff6fc6a7e3a6"},"cell_type":"code","source":"%%time\nprint(\"RESULT FOR TRIGRAM WITH STOP WORDS (Tfidf)\\n\")\nfeature_result_tgt = nfeature_accuracy_checker(vectorizer=tvec,ngram_range=(1, 3))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0fc37a331b5eb32a0698bc3796cebafd8ec5e412"},"cell_type":"markdown","source":"### Comparing count vectorizer & TFIDF Vectorizer"},{"metadata":{"trusted":false,"_uuid":"2a41aeaa6e209b91cd0eb69591ee0b90bcf8c8de"},"cell_type":"code","source":"nfeatures_plot_tgt = pd.DataFrame(feature_result_tgt,columns=['nfeatures','validation_accuracy','train_test_time'])\nnfeatures_plot_bgt = pd.DataFrame(feature_result_bgt,columns=['nfeatures','validation_accuracy','train_test_time'])\nnfeatures_plot_ugt = pd.DataFrame(feature_result_ugt,columns=['nfeatures','validation_accuracy','train_test_time'])\n\nplt.figure(figsize=(8,6))\nplt.plot(nfeatures_plot_tgt.nfeatures, nfeatures_plot_tgt.validation_accuracy, label='trigram tfidf vectorizer',color='royalblue')\nplt.plot(nfeatures_plot_tg.nfeatures, nfeatures_plot_tg.validation_accuracy, label='trigram count vectorizer',linestyle=':', color='royalblue')\nplt.plot(nfeatures_plot_bgt.nfeatures, nfeatures_plot_bgt.validation_accuracy, label='bigram tfidf vectorizer',color='orangered')\nplt.plot(nfeatures_plot_bg.nfeatures, nfeatures_plot_bg.validation_accuracy, label='bigram count vectorizer',linestyle=':',color='orangered')\nplt.plot(nfeatures_plot_ugt.nfeatures, nfeatures_plot_ugt.validation_accuracy, label='unigram tfidf vectorizer',color='gold')\nplt.plot(nfeatures_plot_ug.nfeatures, nfeatures_plot_ug.validation_accuracy, label='unigram count vectorizer',linestyle=':',color='gold')\nplt.title(\"N-gram(1~3) test result : Accuracy\")\nplt.xlabel(\"Number of features\")\nplt.ylabel(\"Validation set accuracy\")\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"32fa69a1e4031112f3ca96d217226bfe17e67936"},"cell_type":"code","source":"%%time\nprint(\"RESULT FOR UNIGRAM WITHOUT STOP WORDS (Tfidf)\\n\")\nfeature_result_ugt_nostop = nfeature_accuracy_checker(vectorizer=tvec, stop_words=STOPWORDS)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"d07274aa2dff149dccaf23aba7c111f0bc3f9dae"},"cell_type":"code","source":"%%time\nprint(\"RESULT FOR BIGRAM WITHOUT STOP WORDS (Tfidf)\\n\")\nfeature_result_bgt_nostop = nfeature_accuracy_checker(vectorizer=tvec,ngram_range=(1, 2),stop_words=STOPWORDS)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"e30b6277a3b5f4122934f8d27bae311997ce26d3"},"cell_type":"code","source":"%%time\nprint(\"RESULT FOR TRIGRAM WITH STOP WORDS (Tfidf)\\n\")\nfeature_result_tgt_nostop = nfeature_accuracy_checker(vectorizer=tvec,ngram_range=(1, 3),stop_words=STOPWORDS)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"abab3f5f63678e27224a9fdf0d0c57814c2ab522"},"cell_type":"markdown","source":"### Without Stop words in tfidf vector"},{"metadata":{"trusted":false,"_uuid":"af090818cb79c909be7eaa5348d478df6546a2f7"},"cell_type":"code","source":"nfeatures_plot_tgt_nostop = pd.DataFrame(feature_result_tgt_nostop,columns=['nfeatures','validation_accuracy','train_test_time'])\nnfeatures_plot_bgt_nostop = pd.DataFrame(feature_result_bgt_nostop,columns=['nfeatures','validation_accuracy','train_test_time'])\nnfeatures_plot_ugt_nostop = pd.DataFrame(feature_result_ugt_nostop,columns=['nfeatures','validation_accuracy','train_test_time'])\n\nplt.figure(figsize=(8,6))\nplt.plot(nfeatures_plot_tgt_nostop.nfeatures, nfeatures_plot_tgt.validation_accuracy,label='trigram tfidf vectorizer',color='royalblue')\nplt.plot(nfeatures_plot_tg.nfeatures, nfeatures_plot_tg.validation_accuracy,label='trigram count vectorizer',linestyle=':', color='royalblue')\nplt.plot(nfeatures_plot_bgt_nostop.nfeatures, nfeatures_plot_bgt.validation_accuracy,label='bigram tfidf vectorizer',color='orangered')\nplt.plot(nfeatures_plot_bg.nfeatures, nfeatures_plot_bg.validation_accuracy,label='bigram count vectorizer',linestyle=':',color='orangered')\nplt.plot(nfeatures_plot_ugt_nostop.nfeatures, nfeatures_plot_ugt.validation_accuracy, label='unigram tfidf vectorizer',color='gold')\nplt.plot(nfeatures_plot_ug.nfeatures, nfeatures_plot_ug.validation_accuracy, label='unigram count vectorizer',linestyle=':',color='gold')\nplt.title(\"N-gram(1~3) test result : Accuracy\")\nplt.xlabel(\"Number of features\")\nplt.ylabel(\"Validation set accuracy\")\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a41c2d4c4a687ff90a757ba2af7b084f28c7e1a2"},"cell_type":"markdown","source":"### Comparison of Bigram Vs Trigram Vs Unigram for Count Vectorizer "},{"metadata":{"trusted":false,"_uuid":"d5d5bf574400d3e827fc95425b28f39beb434008"},"cell_type":"code","source":"print(\"RESULT FOR BIGRAM WITHOUT STOP WORDS\\n\")\nfeature_result_bg_nostop = nfeature_accuracy_checker(vectorizer=cvec, ngram_range=(1, 2), stop_words=STOPWORDS)\n\nprint(\"RESULT FOR TRIGRAM WITHOUT STOP WORDS\\n\")\nfeature_result_tg_nostop = nfeature_accuracy_checker(vectorizer=cvec, ngram_range=(1, 3), stop_words=STOPWORDS)\n\nprint(\"RESULT FOR UNIGRAM WITHOUT STOP WORDS\\n\")\nfeature_result_ug_nostop = nfeature_accuracy_checker(vectorizer=cvec, ngram_range=(1, 1), stop_words=STOPWORDS)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"53ec6a9bb7f4ee6b38e7e6e7a7d0fadcd5ac0c98"},"cell_type":"code","source":"nfeatures_plot_tg_nostop = pd.DataFrame(feature_result_tg_nostop, columns=['nfeatures','validation_accuracy','train_test_time'])\nnfeatures_plot_bg_nostop = pd.DataFrame(feature_result_bg_nostop, columns=['nfeatures','validation_accuracy','train_test_time'])\nnfeatures_plot_ug_nostop = pd.DataFrame(feature_result_ug_nostop, columns=['nfeatures','validation_accuracy','train_test_time'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"a847d3ab7a3a6f2a96c4c16d355c1e6d257244e6"},"cell_type":"code","source":"nfeatures_plot_tgt_nostop = pd.DataFrame(feature_result_tgt_nostop,columns=['nfeatures','validation_accuracy','train_test_time'])\nnfeatures_plot_bgt_nostop = pd.DataFrame(feature_result_bgt_nostop,columns=['nfeatures','validation_accuracy','train_test_time'])\nnfeatures_plot_ugt_nostop = pd.DataFrame(feature_result_ugt_nostop,columns=['nfeatures','validation_accuracy','train_test_time'])\n\nplt.figure(figsize=(8,6))\nplt.plot(nfeatures_plot_tgt_nostop.nfeatures, nfeatures_plot_tgt_nostop.validation_accuracy,label='trigram tfidf vectorizer',color='royalblue')\nplt.plot(nfeatures_plot_tg_nostop.nfeatures, nfeatures_plot_tg_nostop.validation_accuracy,label='trigram count vectorizer',linestyle=':', color='royalblue')\nplt.plot(nfeatures_plot_bgt_nostop.nfeatures, nfeatures_plot_bgt_nostop.validation_accuracy,label='bigram tfidf vectorizer',color='orangered')\nplt.plot(nfeatures_plot_bg_nostop.nfeatures, nfeatures_plot_bg_nostop.validation_accuracy,label='bigram count vectorizer',linestyle=':',color='orangered')\nplt.plot(nfeatures_plot_ugt_nostop.nfeatures, nfeatures_plot_ugt_nostop.validation_accuracy, label='unigram tfidf vectorizer',color='gold')\nplt.plot(nfeatures_plot_ug_nostop.nfeatures, nfeatures_plot_ug_nostop.validation_accuracy, label='unigram count vectorizer',linestyle=':',color='gold')\nplt.title(\"N-gram(1~3) test result : Accuracy\")\nplt.xlabel(\"Number of features\")\nplt.ylabel(\"Validation set accuracy\")\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"dc8c39d38bc6984b108ad5e04b600bfb3a2c6f46"},"cell_type":"markdown","source":"### Other Approaches to follow in the next kernels"},{"metadata":{"trusted":false,"_uuid":"569b93e2dc7f8700b2900c563c8aec5a2aa49496"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}