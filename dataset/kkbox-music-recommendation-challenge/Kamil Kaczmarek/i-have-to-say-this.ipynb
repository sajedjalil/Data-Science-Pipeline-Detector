{"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3","language":"python"},"language_info":{"mimetype":"text/x-python","version":"3.6.3","name":"python","pygments_lexer":"ipython3","file_extension":".py","codemirror_mode":{"version":3,"name":"ipython"},"nbconvert_exporter":"python"}},"nbformat_minor":1,"nbformat":4,"cells":[{"source":"# 1. Introduction\nThis will be the longest EDA you've ever seen...\n\nLet's load some libraries and the data.","metadata":{},"cell_type":"markdown"},{"source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\ntrain = pd.read_csv('../input/train.csv')\ntest = pd.read_csv('../input/test.csv')","metadata":{"_cell_guid":"96eb57fb-a4c9-480d-9c89-94f0a7d54656","_uuid":"f6d03c045dbe1a758efba726e8d515ad4cea871e"},"outputs":[],"execution_count":null,"cell_type":"code"},{"source":"# 2. Adding song year to our datasets","metadata":{},"cell_type":"markdown"},{"source":"songs_extra = pd.read_csv('../input/song_extra_info.csv')\n\ndef isrc_to_year(isrc):\n    if type(isrc) == str:\n        if int(isrc[5:7]) > 17:\n            return 1900 + int(isrc[5:7])\n        else:\n            return 2000 + int(isrc[5:7])\n    else:\n        return np.nan\n        \nsongs_extra['song_year'] = songs_extra['isrc'].apply(isrc_to_year)\nsongs_extra.drop(['isrc', 'name'], axis = 1, inplace = True)\n\ntrain = train.merge(songs_extra, on = 'song_id', how = 'left')\ntest = test.merge(songs_extra, on = 'song_id', how = 'left')","metadata":{"collapsed":true},"outputs":[],"execution_count":null,"cell_type":"code"},{"source":"# 3. Let's count what fraction of songs was released in 2017\nWe will use rolling mean with a window of 50000 for this purpose.","metadata":{},"cell_type":"markdown"},{"source":"train['2017_songs_frac'] = (train['song_year'] == 2017).rolling(window = 50000, center = True).mean()\ntest['2017_songs_frac'] = (test['song_year'] == 2017).rolling(window = 50000, center = True).mean()","metadata":{"scrolled":true},"outputs":[],"execution_count":null,"cell_type":"code"},{"source":"# 4. Let's plot it against train and test index values","metadata":{},"cell_type":"markdown"},{"source":"plt.figure()\nplt.plot(train.index.values, train['2017_songs_frac'], '-',\n        train.shape[0] + test.index.values, test['2017_songs_frac'], '-');","metadata":{"scrolled":true},"outputs":[],"execution_count":null,"cell_type":"code"},{"source":"# 5. Yes! Data is chronologically ordered!\nI think everyone should be aware of this, maybe even organizers should confirm this. It does help to establish a pretty good (as for the time series problem) validation set - you just leave last 2.5 mln (length of the test data) rows of the training data for the validation. It helped me to get 0.69 score without even taking a time series approach to this problem.","metadata":{},"cell_type":"markdown"}]}