{"nbformat_minor":1,"nbformat":4,"metadata":{"kernelspec":{"language":"python","name":"python3","display_name":"Python 3"},"language_info":{"name":"python","nbconvert_exporter":"python","mimetype":"text/x-python","version":"3.6.3","pygments_lexer":"ipython3","codemirror_mode":{"version":3,"name":"ipython"},"file_extension":".py"}},"cells":[{"source":"## Load in data","cell_type":"markdown","metadata":{"_uuid":"3b21683f055ca7cfcc296cc93b0a51450a7c51d2","_cell_guid":"42ab74f9-9074-41d1-8b45-cf8a6f936122"}},{"source":"!ls ..","outputs":[],"execution_count":null,"cell_type":"code","metadata":{"_uuid":"112eadd7c03f76cd3ac349daed4abe9adbaafe8b"}},{"source":"import pandas as pd\nimport numpy as np\nimport gc\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\n\ndf_train = pd.read_csv('../input/train.csv').sample(n=3000000) # transactions\ndf_members = pd.read_csv('../input/members.csv') # members\ndf_songs = pd.read_csv('../input/songs.csv') \nsong_extra = pd.read_csv('../input/song_extra_info.csv')\ndf_songs = df_songs.merge(song_extra,how='left',on='song_id') # merge of songs and song attributes","outputs":[],"execution_count":null,"cell_type":"code","metadata":{"collapsed":true,"_uuid":"0abd6f1e8e3b7372f470cc19ddcc8d1af9d59161","ExecuteTime":{"start_time":"2017-11-20T02:57:31.623265Z","end_time":"2017-11-20T02:57:58.114468Z"},"_cell_guid":"6fe90306-ace9-4c0f-889a-21bb260806f0"}},{"source":"# Merge songs, members to the transactions data\n\ndf_train = pd.merge(pd.merge(df_train, df_members, how='left', on='msno'), df_songs, how='left', on='song_id')\ndel df_members, df_songs, song_extra; gc.collect();","outputs":[],"execution_count":null,"cell_type":"code","metadata":{"collapsed":true,"_uuid":"4040164060b533d2ddaf3214861e73cfb7883075","ExecuteTime":{"start_time":"2017-11-20T02:57:58.116811Z","end_time":"2017-11-20T02:58:01.748687Z"},"_cell_guid":"3921baff-d817-4fbd-b8ce-d18106d3d94f"}},{"source":"# Take a look at null counts and dtypes\ndf_train.info(null_counts=True)","outputs":[],"execution_count":null,"cell_type":"code","metadata":{"_uuid":"bbb50ae2101b47394af0aea7ad0c2fc0a747039b","ExecuteTime":{"start_time":"2017-11-20T02:58:01.752782Z","end_time":"2017-11-20T02:58:02.809522Z"}}},{"source":"# Convert to categorical from numerics:\n\ndf_train['language'] = df_train['language'].apply(str)\ndf_train['city'] = df_train['city'].apply(str)\ndf_train['registered_via'] = df_train['registered_via'].apply(str)\ndf_train['genre_ids'] = df_train['genre_ids'].apply(str)","outputs":[],"execution_count":null,"cell_type":"code","metadata":{"collapsed":true,"_uuid":"be65fd433b80f508a4aa9d60ba23e97b9ef43f6c","ExecuteTime":{"start_time":"2017-11-20T02:58:02.811424Z","end_time":"2017-11-20T02:58:03.551574Z"}}},{"source":"## EDA and Feature Engineering","cell_type":"markdown","metadata":{"_uuid":"82fb7b5ac462028adb9b681a2b4d4d3c2785b638","_cell_guid":"fcc61912-dd6f-4675-85ee-9576e567d4ab"}},{"source":"### Songs","cell_type":"markdown","metadata":{"_uuid":"fcd8b9503bec3d2795fa2283346ef3bf12b96a1c"}},{"source":"First, let's get the likelihood of a song's replay so we can correlate it to our other features.","cell_type":"markdown","metadata":{"_uuid":"d9cfcbf2204182783ff5fa82b0eba81a767c429b"}},{"source":"listens = df_train[['song_id', 'target']].groupby(['song_id']).agg(['mean','count']).reset_index()\nlistens.columns = listens.columns.droplevel()\n# Because target is binary (1,0) we can take the mean of occurences to get probability\nlistens.columns = ['song_id','mean','count']\nlistens['replay_prob'] = listens['mean'] * listens['count']\ndf_train = df_train.merge(listens[['song_id','replay_prob']], how='left', on='song_id')","outputs":[],"execution_count":null,"cell_type":"code","metadata":{"collapsed":true,"_uuid":"8517a937c373bae64e7c203d45609d56618dd7ef","ExecuteTime":{"start_time":"2017-11-20T02:58:03.55325Z","end_time":"2017-11-20T02:58:05.782673Z"}}},{"source":"# Get the year of song release from isrc code\ndef isrc_to_year(isrc):\n    if type(isrc) == str:\n        if int(isrc[5:7]) > 17:\n            return 1900 + int(isrc[5:7])\n        else:\n            return 2000 + int(isrc[5:7])\n    else:\n        return np.nan\n        \ndf_train['song_year'] = pd.to_numeric(df_train.isrc.apply(isrc_to_year))\ndf_train.song_year.hist(bins=70)","outputs":[],"execution_count":null,"cell_type":"code","metadata":{"_uuid":"57c574d7d1df0de424d6cbcb10ce0558023b9364","ExecuteTime":{"start_time":"2017-11-20T02:58:05.784298Z","end_time":"2017-11-20T02:58:07.126267Z"}}},{"source":"How often do certain songs, genres, artists, song languages appear in the data?","cell_type":"markdown","metadata":{"_uuid":"b012b6c75053fa67b8b1f5e3efda0163fa87b3c7"}},{"source":"# Create popularity (counts) for appearances of songs, genres, artists, and languages\n\npopularity_features = ['song_id','genre_ids','artist_name','language']\nfor feat in popularity_features:\n    pop_df = pd.DataFrame(df_train[feat].value_counts()).reset_index().rename(\n        columns={'index': feat, feat: feat+'_popularity'})\n    df_train = df_train.merge(pop_df, how='left', on=feat)","outputs":[],"execution_count":null,"cell_type":"code","metadata":{"collapsed":true,"_uuid":"539939529e63822be391dbf46cd7372d9e717784","ExecuteTime":{"start_time":"2017-11-20T02:58:07.134767Z","end_time":"2017-11-20T02:58:10.795804Z"}}},{"source":"fig, (ax1, ax2) = plt.subplots(nrows=2, figsize=(22,18))\nsns.barplot(x=\"genre_ids\", y=\"genre_ids_popularity\", data=df_train[df_train.genre_ids_popularity > 15], ax=ax1)\nsns.barplot(x=\"language\", y=\"language_popularity\", data=df_train, ax=ax2)\nplt.setp(ax1.get_xticklabels(), rotation=90)","outputs":[],"execution_count":null,"cell_type":"code","metadata":{"_uuid":"0dacd0279969432dcdc6ec16b639c7326cb403cc","ExecuteTime":{"start_time":"2017-11-20T02:58:10.80322Z","end_time":"2017-11-20T02:58:48.716373Z"}}},{"source":"### Members","cell_type":"markdown","metadata":{"_uuid":"767cf474b68cb9c0e2bdc6bf06fab31a31e6c02e"}},{"source":"How often do users show up in the data? Are there some who listen to a higher volume of songs more than others?","cell_type":"markdown","metadata":{"_uuid":"cac941bf42ce6de202c00a28619918be8b208e91"}},{"source":"# Create user song consumption feature ('msno_volume')\nvolume = pd.DataFrame(df_train.msno.value_counts()).reset_index().rename(\n    columns={'index': 'msno', 'msno': 'msno_volume'})\ndf_train = df_train.merge(volume, how='left', on='msno')\ndel volume\ngc.collect()","outputs":[],"execution_count":null,"cell_type":"code","metadata":{"_uuid":"75689498bf590605ef228be19c81728d0057aaa6","ExecuteTime":{"start_time":"2017-11-20T02:58:48.717938Z","end_time":"2017-11-20T02:58:50.140604Z"}}},{"source":"df_train.columns","outputs":[],"execution_count":null,"cell_type":"code","metadata":{"_uuid":"a342876217697384436b690c927a30a33e6ef32b","ExecuteTime":{"start_time":"2017-11-20T02:58:50.142407Z","end_time":"2017-11-20T02:58:50.14815Z"}}},{"source":"df_train.msno_volume.hist(bins=40)\nplt.title(\"Song Listens across KKBOX Users\")","outputs":[],"execution_count":null,"cell_type":"code","metadata":{"_uuid":"b3b357322b6aa65ac8b42119d14858e0cdb4109f","ExecuteTime":{"start_time":"2017-11-20T02:58:50.149886Z","end_time":"2017-11-20T02:58:50.615835Z"}}},{"source":"How long have members been registered with KKBOX?","cell_type":"markdown","metadata":{"_uuid":"05ac55d7c04deba055afa12f53b7283a0d4ae087"}},{"source":"# get membership length\ndf_train.registration_init_time = pd.to_datetime(df_train.registration_init_time,format=\"%Y%m%d\")\ndf_train.expiration_date = pd.to_datetime(df_train.expiration_date,format=\"%Y%m%d\")\ndf_train['membership_days'] = (df_train.expiration_date - df_train.registration_init_time)\ndf_train['membership_days'] = (df_train.membership_days/np.timedelta64(1, 'D')).astype('int')\ndf_train['membership_days'] = df_train.membership_days.clip(lower=0)","outputs":[],"execution_count":null,"cell_type":"code","metadata":{"collapsed":true,"_uuid":"d48e3ea94895fc4e539774f6242ea8d648044409","ExecuteTime":{"start_time":"2017-11-20T02:58:50.617859Z","end_time":"2017-11-20T02:58:51.828658Z"}}},{"source":"df_train.membership_days.hist(bins=40)\nplt.title(\"Membership Days across KKBOX Users\")","outputs":[],"execution_count":null,"cell_type":"code","metadata":{"_uuid":"c8d8e01423ec113564c463a9d7749cfc90a5cc46","ExecuteTime":{"start_time":"2017-11-20T02:58:51.832226Z","end_time":"2017-11-20T02:58:52.276305Z"}}},{"source":"Take a look at the ages. I do not think there are many humans older than 100. If they are around I doubt they are using a streaming music service. Also, I do not think there are many infants using the service.  \nI am going to fill in those values with the mean of the ages (excluding invalid values).","cell_type":"markdown","metadata":{"_uuid":"f5ea0a90d90450766ed95bad7a269d2d0755607c"}},{"source":"df_train.bd.hist(bins=40)","outputs":[],"execution_count":null,"cell_type":"code","metadata":{"_uuid":"cfc7fb21288380f91626a17697ac66707a3ba2c9","ExecuteTime":{"start_time":"2017-11-20T02:58:52.278591Z","end_time":"2017-11-20T02:58:52.777662Z"}}},{"source":"# There are also some obvious instances of missing data:\ndf_train.bd[(df_train.bd > 100) | (df_train.bd < 5)].value_counts()","outputs":[],"execution_count":null,"cell_type":"code","metadata":{"_uuid":"98351cd3d8aa57a9fcc2df56866c1e9d29a36c4c","ExecuteTime":{"start_time":"2017-11-20T02:58:52.789017Z","end_time":"2017-11-20T02:58:52.828763Z"}}},{"source":"# Impute missing ages\nage_mean = df_train[(df_train.bd > 0)&(df_train.bd < 100)].bd.mean()\ndf_train.bd = df_train.bd.apply(lambda x: age_mean if x<=0 or x>100 else x)","outputs":[],"execution_count":null,"cell_type":"code","metadata":{"collapsed":true,"_uuid":"a263e21f81930971c472fa4009965a1273f22675","ExecuteTime":{"start_time":"2017-11-20T02:58:52.839852Z","end_time":"2017-11-20T02:58:53.73984Z"}}},{"source":"# That looks better\ndf_train.bd.hist(bins=40)","outputs":[],"execution_count":null,"cell_type":"code","metadata":{"_uuid":"fb7d91783cea7694127511a2c9a63591d9a5fba5","ExecuteTime":{"start_time":"2017-11-20T02:58:53.748617Z","end_time":"2017-11-20T02:58:54.174484Z"}}},{"source":"exlcude_cols = ['genre_ids', 'msno', 'song_id', 'isrc', 'name', 'composer', 'lyricist', 'artist_name','language']\ncat_cols = [x for x in df_train.select_dtypes(include=['object']).columns if x not in exlcude_cols]\ncat_cols","outputs":[],"execution_count":null,"cell_type":"code","metadata":{"_uuid":"162e897bfef61230851aa5879d4a5f04e591128e","ExecuteTime":{"start_time":"2017-11-20T02:58:54.18335Z","end_time":"2017-11-20T02:58:54.43214Z"}}},{"source":"fig, (ax1, ax2, ax3, ax4, ax5, ax6) = plt.subplots(nrows=6, figsize=(20,55))\naxes=(ax1, ax2, ax3, ax4, ax5, ax6)\nfor i, col in enumerate(cat_cols):\n    vc = df_train[col].value_counts().reset_index().rename(columns={col:'count','index':col})\n\n    sns.barplot(x=\"count\", y=col, data=vc, ax=axes[i], orient='h')","outputs":[],"execution_count":null,"cell_type":"code","metadata":{"_uuid":"093c75381ef2d89cd5d229921f73198b17618a69","ExecuteTime":{"start_time":"2017-11-20T02:58:54.434378Z","end_time":"2017-11-20T02:58:58.238406Z"}}},{"source":"Seems we can have mutliple values separated by '|' for artist_name, genre_ids, lyricist, and composer. Is there some sort of relationship between counts of these values and the likelihood of the target?","cell_type":"markdown","metadata":{"_uuid":"620b3c63d2fb73ae3b637e6b6f93ef992df655d5"}},{"source":"df_train[['genre_ids','composer','lyricist','artist_name']].tail()","outputs":[],"execution_count":null,"cell_type":"code","metadata":{"_uuid":"176614aa329d118c368f38051a4df5315f047ef8","ExecuteTime":{"start_time":"2017-11-20T02:58:58.239966Z","end_time":"2017-11-20T02:58:58.282164Z"}}},{"source":"def count_vals(x):\n    # count number of values separated by '|'\n    if type(x) != str:\n        return 1\n    else:\n        return 1 + x.count('|')\n    \ndf_train['number_of_genres'] = df_train['genre_ids'].apply(count_vals)\ndf_train['number_of_composers'] = df_train['composer'].apply(count_vals)\ndf_train['number_of_lyricists'] = df_train['lyricist'].apply(count_vals)\ndf_train['number_of_artists'] = df_train['artist_name'].apply(count_vals)","outputs":[],"execution_count":null,"cell_type":"code","metadata":{"collapsed":true,"_uuid":"7596afa4f5b46b8d15e697daaba8945c1b1c5e2e","ExecuteTime":{"start_time":"2017-11-20T02:58:58.284427Z","end_time":"2017-11-20T02:59:00.676564Z"}}},{"source":"### Song and Member Interactions","cell_type":"markdown","metadata":{"_uuid":"4387760a86a5c4939f8fddb846048bdcecaf7fa7"}},{"source":"How often do users listen to the same artists, genres, and languages?","cell_type":"markdown","metadata":{"_uuid":"53a926b46c23e97e4d3ec8c93a62c6f926b48bbe"}},{"source":"# merge user tendencies by artist, genre, and language\nsong_features = ['artist_name', 'genre_ids', 'language']\nfor feature in song_features:\n    listens = feature+'_listens'\n    listens_pct = listens+'_pct'\n    listens_by_feat = df_train.groupby(['msno', feature]).\\\n                                count()['song_id'].\\\n                                reset_index().\\\n                                rename(columns={'song_id':listens})\n    df_train = df_train.merge(listens_by_feat,how='left',on=['msno',feature])\n    df_train[listens_pct] = df_train[listens] / df_train['msno_volume']\n    assert df_train[df_train[listens_pct] > 1].empty and df_train[df_train[listens_pct] < 0].empty\n    df_train.drop([listens], axis=1, inplace=True)","outputs":[],"execution_count":null,"cell_type":"code","metadata":{"collapsed":true,"_uuid":"9812c9cd81ba1ec1bdd0bf0d938ac4f4e70e6af2","ExecuteTime":{"start_time":"2017-11-20T02:59:00.679326Z","end_time":"2017-11-20T02:59:09.714593Z"}}},{"source":"### Correlations in the dataset\nLook for correlations to replay probability","cell_type":"markdown","metadata":{"_uuid":"2b76ad1f69aa16e1258f75e374e66dc55807ce97"}},{"source":"mask = ['replay_prob', 'song_year','song_id_popularity',\n       'genre_ids_popularity', 'artist_name_popularity', 'language_popularity',\n       'msno_volume', 'membership_days', 'number_of_genres',\n       'number_of_composers', 'number_of_lyricists', 'number_of_artists',\n       'artist_name_listens_pct', 'genre_ids_listens_pct', 'language_listens_pct']\n\ndf_corr = df_train[mask].fillna(0)","outputs":[],"execution_count":null,"cell_type":"code","metadata":{"collapsed":true,"_uuid":"93e6d19c4571c814c1403fa1d5adf7f46608fa8e","ExecuteTime":{"start_time":"2017-11-20T02:59:09.71636Z","end_time":"2017-11-20T02:59:09.812527Z"}}},{"source":"Does not look like there are any strong correlations except for the **song_id_popularity**, from which the the replay_prob is derived.","cell_type":"markdown","metadata":{"_uuid":"df97ccfa761603850a7cbe20d761c27bd0fca4ed"}},{"source":"df_corr.corr()","outputs":[],"execution_count":null,"cell_type":"code","metadata":{"_uuid":"93cf3ea3cc616ff6462e6959f99ea72b49b224b7","ExecuteTime":{"start_time":"2017-11-20T02:59:09.81838Z","end_time":"2017-11-20T02:59:10.431386Z"}}},{"source":"### Categorical features","cell_type":"markdown","metadata":{"_uuid":"20afbec25d4cbadb7a04263a593b0cf1eedd8451"}},{"source":"# Create dummies for the categorical features and merge back to transactions\nfor c in cat_cols:\n    split = pd.get_dummies(df_train[c])\n    new_names = {i : c+str(i) for i in split.columns}\n    split.rename(columns = new_names, inplace=True)\n    df_train = df_train.merge(split,how='left',left_index=True,right_index=True)","outputs":[],"execution_count":null,"cell_type":"code","metadata":{"collapsed":true,"_uuid":"706a8a4484489c7a9642766ffce8c307768b1af4","ExecuteTime":{"start_time":"2017-11-20T02:59:10.435361Z","end_time":"2017-11-20T02:59:12.224981Z"}}},{"source":"df_train.shape","outputs":[],"execution_count":null,"cell_type":"code","metadata":{"_uuid":"c3e405a0ff48bd555a8e1061fa499b8bda0304e2","ExecuteTime":{"start_time":"2017-11-20T02:59:12.227886Z","end_time":"2017-11-20T02:59:12.237991Z"}}},{"source":"### Interactions with the target\nI am leaking the probability of the target class into the indogenous variables\nThe code below counts user interactions with a given value if the target was true, divided by all of the userâ€™s interactions with that value. I then merge this feature from the training set to the test set (~ 46% songs, 70% users, 53% artists of the train set were in holdout)","cell_type":"markdown","metadata":{"_uuid":"2555070437f0a46006ab5f1915541c6cfa4993db"}},{"source":"from sklearn.model_selection import train_test_split\ndf_train, df_test = train_test_split(df_train, test_size=.15)","outputs":[],"execution_count":null,"cell_type":"code","metadata":{"collapsed":true,"_uuid":"656a79306cfb9f394a6711981e1a02c751fe74a4","ExecuteTime":{"start_time":"2017-11-20T02:59:12.240481Z","end_time":"2017-11-20T02:59:13.576734Z"}}},{"source":"# Calculate the prob(repeat) per user for a given cross feature to the train set\n# Then merge probs to test set\ncrosses = ['artist_name', 'language', 'genre_ids']\n\nfor feat in crosses:\n    cross_col = \"X_\" + 'msno' + '_' + feat\n    df_train[cross_col] = df_train['msno'] + df_train[feat]\n    df_test[cross_col] = df_test['msno'] + df_test[feat]\n    # Because target is binary [0,1], I can take the mean to get a\n    # probability of the target for the given cross feature\n    target_total = df_train.groupby(cross_col).mean()['target'].reset_index()\n    # Check to make sure probabilities are between 0 and 1\n    assert target_total[target_total.target > 1].empty and target_total[target_total.target < 0].empty\n    # Create prob columns in train and merge to test\n    target_total = target_total.rename(columns={'target': 'target_prob_' + feat})\n    df_train = df_train.merge(target_total, how='left', on=cross_col)\n    df_test = df_test.merge(target_total, how='left', on=cross_col)","outputs":[],"execution_count":null,"cell_type":"code","metadata":{"collapsed":true,"_uuid":"c3f9e20ea96befe6851097dc7b6aaf317a1ac0a9","ExecuteTime":{"start_time":"2017-11-20T02:59:13.583899Z","end_time":"2017-11-20T02:59:31.454567Z"},"_cell_guid":"43c8f460-226e-4805-b255-2b4629df30a4"}},{"source":"y_test = df_test.target\nX_test = df_test.loc[:,df_train.columns != 'target']\n\ny_train = df_train.target\nX_train = df_train.loc[:,df_train.columns != 'target']","outputs":[],"execution_count":null,"cell_type":"code","metadata":{"collapsed":true,"_uuid":"a699ac1a82df73cf73fbe3aff761669c93f7d946","ExecuteTime":{"start_time":"2017-11-20T02:59:31.460028Z","end_time":"2017-11-20T02:59:32.702293Z"}}},{"source":"from sklearn.preprocessing import scale\n\ndef preprocess(df_model):\n    # leave numerics only\n    df_model = df_model.select_dtypes(include=['int64', 'int32', 'float64', 'uint8']).fillna(0)\n    df_scaled = pd.DataFrame(scale(df_model), columns=df_model.columns)\n    return df_scaled","outputs":[],"execution_count":null,"cell_type":"code","metadata":{"collapsed":true,"_uuid":"5379958ffb8b5fe389cfc9a34384869a7e7da537","ExecuteTime":{"start_time":"2017-11-20T02:59:32.706365Z","end_time":"2017-11-20T02:59:32.732971Z"},"_cell_guid":"1570f6c2-6af1-44bd-ade4-2115cff384b3"}},{"source":"X_test = preprocess(X_test)\nX_train = preprocess(X_train)\n\ndel df_train, df_test; gc.collect();\n\n# Classes are fairly balanced\nprint(\"\\nCLASS BALANCE: \")\nprint(y_train.value_counts() / len(y_train))\nprint(\"\\nSIZES: \")\nprint(\"X_train: \", X_train.shape)\nprint(\"y_train: \", y_train.shape)\nprint(\"X_test: \", X_test.shape)\nprint(\"y_test: \", y_test.shape)","outputs":[],"execution_count":null,"cell_type":"code","metadata":{"_uuid":"28f4bff847742e15e18b9a86e43b1dda30620763","ExecuteTime":{"start_time":"2017-11-20T02:59:32.742033Z","end_time":"2017-11-20T02:59:34.930244Z"},"_cell_guid":"6026f736-ff75-4d8e-879a-1bbcb9b114e8"}},{"source":"Classes look pretty balanced. We end up with 88 features to put into our model.","cell_type":"markdown","metadata":{"_uuid":"83cfb51ea5cde5ec98f1907e9d7c3a83d09c7d8a"}},{"source":"X = pd.concat([X_train, X_test]).fillna(0)\ny = pd.concat([y_train, y_test]).fillna(0)","outputs":[],"execution_count":null,"cell_type":"code","metadata":{"collapsed":true,"_uuid":"6e940e169b0a13105105b7329251f775e968fc81","ExecuteTime":{"start_time":"2017-11-20T02:59:34.938594Z","end_time":"2017-11-20T02:59:35.48272Z"},"_cell_guid":"d60ab27d-37b5-4911-82f3-b2837f390ff3"}},{"source":"## Modeling","cell_type":"markdown","metadata":{"_uuid":"a57a2be90ab64da311e3ecc6248fdc1ab8518338","_cell_guid":"7f4284ab-6186-47d8-8652-edde21d35cb6"}},{"source":"from sklearn.linear_model import SGDClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom xgboost import XGBClassifier\n\nfrom sklearn.model_selection import RandomizedSearchCV, cross_val_score, KFold\nfrom sklearn.metrics import accuracy_score, roc_auc_score","outputs":[],"execution_count":null,"cell_type":"code","metadata":{"collapsed":true,"_uuid":"e165f55b44df9568e96d23e2b0eb112a3ebdcc31","ExecuteTime":{"start_time":"2017-11-20T02:59:35.488083Z","end_time":"2017-11-20T02:59:35.779712Z"},"_cell_guid":"f15b2636-b9db-4dfc-8792-88d89f64fe80"}},{"source":"models = [KNeighborsClassifier(), \n               LogisticRegression(),\n               SGDClassifier(), \n               RandomForestClassifier(), \n               XGBClassifier()]","outputs":[],"execution_count":null,"cell_type":"code","metadata":{"_uuid":"a691d14b4e31266673991d66c13349e58971673c","ExecuteTime":{"start_time":"2017-11-20T02:59:35.784133Z","end_time":"2017-11-20T02:59:35.791949Z"}}},{"source":"kf = KFold(n_splits=10, shuffle=True, random_state=42)\nstep = 100\nfor est in models:\n    print('\\n',est.get_params)\n    cv_roc = cross_val_score(est, X[::step], y[::step], scoring='roc_auc', cv=kf, n_jobs=-1)\n    cv_acc = cross_val_score(est, X[::step], y[::step], scoring='accuracy', cv=kf, n_jobs=-1)\n    print('CV ROC AUC:  ', cv_roc, np.mean(cv_roc))\n    print('CV Accuracy: ', cv_acc, np.mean(cv_acc))","outputs":[],"execution_count":null,"cell_type":"code","metadata":{"_uuid":"1baa946a68fd331c07861c8cbb912c62d46a325c","ExecuteTime":{"start_time":"2017-11-20T02:59:35.811238Z","end_time":"2017-11-20T03:16:59.552177Z"}}},{"source":"### XGBoost","cell_type":"markdown","metadata":{"_uuid":"b015c43a7b95370c3e94df035213507b739a0091","_cell_guid":"44e33e1d-4b1a-478c-b33b-b496ceb17a0f"}},{"source":"def test_train_scores(model):\n    predictions = model.predict(X_train[::step])\n    auc = roc_auc_score(y_train[::step], predictions)\n    accuracy = accuracy_score(y_train[::step], predictions)\n    print(\"ROC TRAIN AUC   : \", auc)\n    print(\"ACCURACY  : \", accuracy)\n\n    predictions = model.predict(X_test[::step])\n    auc = roc_auc_score(y_test[::step], predictions)\n    accuracy = accuracy_score(y_test[::step], predictions)\n    print(\"ROC TEST  AUC   : \", auc)\n    print(\"ACCURACY  : \", accuracy)\n\ndef plot_learning_curves(model):\n    # retrieve performance metrics\n    results = model.evals_result()\n    epochs = len(results['validation_0']['auc'])\n    x_axis = range(0, epochs)\n\n    # plot roc auc\n    fig, ax = plt.subplots(figsize=(10,6))\n    ax.plot(x_axis, results['validation_0']['auc'], label='Train')\n    ax.plot(x_axis, results['validation_1']['auc'], label='Test')\n    ax.legend()\n    plt.ylabel('ROC AUC')\n    plt.xlabel('Number of Epochs')\n    plt.title('XGBoost ROC AUC')\n    plt.show()\n\n    # plot classification error\n    fig, ax = plt.subplots(figsize=(10,6))\n    ax.plot(x_axis, results['validation_0']['error'], label='Train')\n    ax.plot(x_axis, results['validation_1']['error'], label='Test')\n    ax.legend()\n    plt.ylabel('Error')\n    plt.xlabel('Number of Epochs')\n    plt.title('XGBoost Classification Error')\n    plt.show()","outputs":[],"execution_count":null,"cell_type":"code","metadata":{"collapsed":true,"_uuid":"a12afa91df0f092e0c31f6c241fbc35990c852c2","ExecuteTime":{"start_time":"2017-11-20T03:17:41.386456Z","end_time":"2017-11-20T03:17:41.496555Z"},"_cell_guid":"77073086-0bf4-4735-ab63-aeee304481d0"}},{"source":"from scipy.stats import randint as sp_randint\n\nparams = {'max_depth': [3,4,5,6],\n          'gamma':[15,20,30],\n          'subsample':[.1,.8],\n          'colsample_bytree':[.1,.8],\n          'learning_rate':sp_randint(1e-10,9e-2)\n          }\n\nmodel = RandomizedSearchCV(XGBClassifier(n_estimators=3000), \n                     param_distributions=params, n_jobs=1, scoring='roc_auc')","outputs":[],"execution_count":null,"cell_type":"code","metadata":{"collapsed":true,"_uuid":"c917c7f206e65c818b9904fcefefec3bfbb7bd9d","ExecuteTime":{"start_time":"2017-11-20T03:17:43.680838Z","end_time":"2017-11-20T03:17:43.694489Z"},"_cell_guid":"999e07af-7596-4c99-a52d-b05acde44f41"}},{"source":"model = XGBClassifier(n_estimators=1500, learning_rate=.000000001, max_depth=3,\n                      gamma=5, subsample=.2, colsample_bytree=.125)","outputs":[],"execution_count":null,"cell_type":"code","metadata":{"collapsed":true,"_uuid":"baffc463d9f9bde5906588e0e15d01e02f81f156","ExecuteTime":{"start_time":"2017-11-20T03:30:09.728732Z","end_time":"2017-11-20T03:30:09.745414Z"},"_cell_guid":"b32cf0bb-7c4f-4ab1-9ca6-4b23cf3dc7d0"}},{"source":"%%time\nstep = 5\neval_set = [(X_train[::step], y_train[::step]), (X_test[::step], y_test[::step])]\nmodel.fit(X_train[::step], y_train[::step], \n          eval_metric=['auc','error'],eval_set=eval_set, verbose=False)","outputs":[],"execution_count":null,"cell_type":"code","metadata":{"_uuid":"715033119c38837782fc55685db5417ba94ef2e3","ExecuteTime":{"start_time":"2017-11-20T03:30:10.260894Z","end_time":"2017-11-20T03:31:15.53027Z"},"_cell_guid":"b026ff28-7533-4d13-a991-3374f449dba0"}},{"source":"test_train_scores(model)","outputs":[],"execution_count":null,"cell_type":"code","metadata":{"_uuid":"8195e1741549e9205ff323796898fa3796d24efe","ExecuteTime":{"start_time":"2017-11-20T03:31:15.532615Z","end_time":"2017-11-20T03:31:19.53819Z"},"_cell_guid":"9047891c-b15e-44bb-a907-f4a22db00dd1"}},{"source":"plot_learning_curves(model)","outputs":[],"execution_count":null,"cell_type":"code","metadata":{"_uuid":"8f1dcd01a3480a4e8b2cd9be3284acac3089b5ef","ExecuteTime":{"start_time":"2017-11-20T03:31:19.540466Z","end_time":"2017-11-20T03:31:20.366785Z"},"_cell_guid":"3f53d9de-e384-4469-b199-bc56ac03a375"}},{"source":"feat_imp = sorted(list(zip(X_train.columns, model.feature_importances_)),key=lambda tup: tup[1],reverse=True)\nfeat_imp = pd.DataFrame(feat_imp)[:20]\nsns.set(style=\"whitegrid\")\n# Initialize the matplotlib figure\nf, ax = plt.subplots(figsize=(15, 10))\nsns.set_color_codes(\"pastel\")\nfi = sns.barplot(x=1, y=0, data=feat_imp, color=\"b\")\nax.set(xlabel=\"Feature Importance Score\")\nsns.despine(left=True, bottom=True)","outputs":[],"execution_count":null,"cell_type":"code","metadata":{"_uuid":"ec06ad4e3eb8e7e8a02bdd396838ce0133f54d13","ExecuteTime":{"start_time":"2017-11-20T03:31:57.301194Z","end_time":"2017-11-20T03:31:57.895339Z"},"_cell_guid":"206333d6-8189-423c-b9fa-3e8acc883bf8"}},{"source":"","outputs":[],"execution_count":null,"cell_type":"code","metadata":{"collapsed":true,"_uuid":"720b61f1627a860ea1db56366c206ec0650d1dcb","_cell_guid":"fa251f7d-c24e-426b-bc7c-743ead8410c7"}}]}