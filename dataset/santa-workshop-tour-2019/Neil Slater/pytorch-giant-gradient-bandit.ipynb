{"cells":[{"metadata":{},"cell_type":"markdown","source":"## It is surprising this even works!\n\nGradient Bandits (GB) are related to Reinforcement Learning (RL) and more specifically to Policy Gradient (PG) methods. For more details on the maths behind it you can look at [Reinforcement Learning: An Introduction](http://incompleteideas.net/book/the-book.html) Chapter 2.\n\nTo convert this optimisation problem to a GB one, this script treats each family's day allocation as a separate choice. So we run 5000 simultaneous GB instances, each one treating the other 4999 choices as \"the environment\" which is then clearly a moving target as those families are also changing their choices (called a non-stationary problem in the book). If the families were competing, each would get their own score and we would converge on a Nash equilibrium of action choices, which could even be a stochastic policy. However, we are acting for Santa here, and are looking for something more like Pareto optimality. So each family can therefore use feedback based on the resulting *global* score.\n\nThe surprising thing is that we can get gradients and perform gradient descent optimisation based on this. And it actually converges to valid solutions with moderate/OK scores.\n\nTo get it to work well enough to converge, this script also does these things\n\n * Soft constraints with variable score based on how far away the allocations are from meeting the bounds\n \n * Use a batch of trials to establish the current gradient, otherwise the variance is far too high and gradient too noisy\n \n * Introduce soft constraints and accounting costs gradually. This adds to the non-stationary nature of the GB problem, but we're already set up to cope with that\n \nThe GB solver is not able to optimise aggressively. I have not seen it score better than 75,000. However its output can work nicely for input into other optimisers."},{"metadata":{},"cell_type":"markdown","source":"The code uses syntax for `torch.sum` which requires latest version of PyTorch, so we need to install it."},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install torch --upgrade","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Imports and problem constants"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import torch\nimport time\nimport numpy as np\nimport pandas as pd\nfrom numba import njit\n\n# Reproducibility\ntorch.manual_seed(20191210)\n\n# Constants\nN_DAYS = 100\nN_FAMILIES = 5000\nMAX_OCCUPANCY = 300\nMIN_OCCUPANCY = 125\nINPUT_PATH = '/kaggle/input/santa-workshop-tour-2019/'\nOUTPUT_PATH = ''\n\n# The code will run on `cuda`, however there is a bug that I have not chased down and it will not converge\n# The speed boost from GPU is not that great, either - vectorising the PyTorch score method could make it competitive\nDEFAULT_DEVICE = torch.device('cpu')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Config and hyperparameters"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Agent hyperparams\nMAX_ACTION = 4\nSOFT_PENALTY_PER_PERSON = 1000\nPENALTY_RAMP_TIME = 2000  # Number of batches over which to ramp up the soft penalty and accounting costs\nBATCH_SIZE = 1000\nN_BATCHES = 6000\n\n# Optimiser hyperparams\nLR = 0.025\nGRADIENT_CLIP = 100.0\nMAX_PREFERENCE = 8.5\nUSE_ADAM = True\n\n#  Only used if USE_ADAM=True\nADAM_BETA_M = 0.9\nADAM_BETA_V = 0.99\nADAM_EPSILON = 0.000001\n\n#  Only used if USE_ADAM=False\nMOMENTUM = 0.95","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Numba-based cost function\n\nBased on https://www.kaggle.com/nickel/250x-faster-cost-function-with-numba-jit and similar Numba kernels posted at start of competition.\n\nThe fast scorer is still faster than PyTorch on CPU. Probably this one could be improved too."},{"metadata":{"trusted":true},"cell_type":"code","source":"@njit\ndef faster_cost(allocation, cost_matrix, family_size, days, account_rate, soft_penalty):\n    penalty = 0\n    daily_occupancy = np.zeros(N_DAYS+1)\n    for i in range(N_FAMILIES):\n        n = family_size[i]\n        d = allocation[i]\n        daily_occupancy[d] += n\n        penalty += cost_matrix[i, d]\n\n    relevant_occupancy = daily_occupancy[1:]\n\n    for day in days:\n        today_count = daily_occupancy[day]\n        if today_count > MAX_OCCUPANCY:\n            penalty += soft_penalty * (today_count - MAX_OCCUPANCY)\n            daily_occupancy[day] = MAX_OCCUPANCY\n        elif today_count < MIN_OCCUPANCY:\n            penalty += soft_penalty * (MIN_OCCUPANCY - today_count)\n            daily_occupancy[day] = MIN_OCCUPANCY\n\n    init_occupancy = daily_occupancy[days[0]]\n    accounting_cost = (init_occupancy - 125.0) / 400.0 * init_occupancy**(0.5)\n    accounting_cost = max(0, accounting_cost)\n\n    yesterday_count = init_occupancy\n    for day in days[1:]:\n        today_count = daily_occupancy[day]\n        diff = np.abs(today_count - yesterday_count) * 0.02 * account_rate\n        accounting_cost += max(0, (today_count - 125.0) / 400.0 * today_count**(0.5 + diff))\n        yesterday_count = today_count\n\n    penalty += account_rate * accounting_cost\n\n    return penalty\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Problem definition\n\nProblem class includes the scoring code. There is a mostly-unused PyTorch variant of it here as well."},{"metadata":{"trusted":true},"cell_type":"code","source":"class Problem:\n    def __init__(self, base_path, device):\n        self.base_path = base_path\n        self.data = pd.read_csv(base_path + 'family_data.csv', index_col='family_id')\n        self.device = device\n        self._build()\n        self._set_buffers()\n        self.days = np.arange(N_DAYS, 0, -1)\n        self.np_costs_matrix = self.costs_matrix.cpu().numpy()\n        self.np_family_size = self.family_size.cpu().numpy()\n\n    def _build(self):\n        self.family_size = torch.tensor(self.data.n_people.values, dtype = torch.int, device = self.device)\n        self.penalties_array = torch.tensor([\n            [\n                0,\n                50,\n                50 + 9 * n,\n                100 + 9 * n,\n                200 + 9 * n,\n                200 + 18 * n,\n                300 + 18 * n,\n                300 + 36 * n,\n                400 + 36 * n,\n                500 + 36 * n + 199 * n,\n                500 + 36 * n + 398 * n\n            ]\n            for n in range(self.family_size.max() + 1)\n        ], device = self.device)\n        self.actions_to_days = torch.full((self.data.shape[0], 10), 1, dtype = torch.long, device = self.device)\n        self.costs_matrix = torch.full((self.data.shape[0], N_DAYS + 1), -1, device = self.device)\n\n        for i, choice in enumerate(self.data.loc[:, 'choice_0': 'choice_9'].values):\n            n = self.family_size[i]\n            for day in range(1,101):\n                self.costs_matrix[i, day] = self.penalties_array[n,-1]\n            for d, day in enumerate(choice):\n                self.costs_matrix[i, day] = self.penalties_array[n, d]\n                self.actions_to_days[i, d] = int(day)\n\n    def _set_buffers(self):\n        self.daily_occupancy = torch.zeros(N_DAYS+2, dtype = torch.int, device = self.device)\n        self.penalties = torch.zeros(N_FAMILIES, device = self.device)\n        self.excess = torch.zeros(N_DAYS+2, dtype = torch.int, device = self.device)\n        self.shortage = torch.zeros(N_DAYS+2, dtype = torch.int, device = self.device)\n        self.diff_counts = torch.zeros(N_DAYS, dtype = torch.int, device = self.device)\n        self.today_cost_factor = torch.zeros(N_DAYS, dtype = torch.int, device = self.device)\n        self.today_cost_factor_f = torch.zeros(N_DAYS, dtype = torch.float, device = self.device)\n        self.diff_counts_factor_f = torch.zeros(N_DAYS, dtype = torch.float, device = self.device)\n        self.day_power_f = torch.zeros(N_DAYS, dtype = torch.float, device = self.device)\n        self.day_costs = torch.zeros(N_DAYS, dtype = torch.float, device = self.device)\n\n    def cost_function(self, assignments, account_rate = 1.0, soft_penalty = SOFT_PENALTY_PER_PERSON):\n        # Calculate days\n        self.daily_occupancy.zero_()\n        self.daily_occupancy.index_add_(0, assignments, self.family_size)\n        self.daily_occupancy[0] = MIN_OCCUPANCY\n        self.daily_occupancy[-1] = MIN_OCCUPANCY\n\n        # Individual family costs\n        torch.gather(self.costs_matrix, 1, assignments.view(-1,1), out=self.penalties)\n        penalty = self.penalties.sum().item()\n\n        # Soft penalty\n        torch.clamp(self.daily_occupancy, MAX_OCCUPANCY, 10000000, out=self.excess)\n        self.excess.add_(-MAX_OCCUPANCY)\n        torch.clamp(self.daily_occupancy, 0, MIN_OCCUPANCY, out=self.shortage)\n        self.shortage.add_(-MIN_OCCUPANCY)\n\n        # Clamped to prevent nonlinear penalties dominating soft penalties\n        self.daily_occupancy.clamp_(MIN_OCCUPANCY, MAX_OCCUPANCY)\n\n        penalty += (self.excess.sum() - self.shortage.sum()).item() * soft_penalty\n\n        # Accounting costs\n        today_counts = self.daily_occupancy.narrow(0, 1, N_DAYS)\n        yesterday_counts = self.daily_occupancy.narrow(0, 2, N_DAYS)\n        torch.neg(yesterday_counts, out=self.diff_counts)\n        # All this is so that we use the buffers throughout and avoid spawning new tensors for interim calculations\n        self.diff_counts.add_(today_counts)\n        self.diff_counts.abs_()\n        self.diff_counts[N_DAYS-1] = 0\n        torch.add(today_counts, -MIN_OCCUPANCY, out = self.today_cost_factor)\n        torch.mul(self.today_cost_factor, 0.0025, out = self.today_cost_factor_f)\n        torch.mul(self.diff_counts, 0.02 * account_rate, out = self.diff_counts_factor_f)\n        self.diff_counts_factor_f.add_(0.5)\n\n        # TODO: Check whether today_counts.type(torch.float) is a view or needs a buffer and how?\n        torch.pow(today_counts.type(torch.float), self.diff_counts_factor_f, out=self.day_power_f)\n        self.today_cost_factor_f.mul_(self.day_power_f)\n\n        accounting_cost = self.today_cost_factor_f.sum().item()\n\n        penalty += account_rate * accounting_cost\n\n        return penalty\n\n    def fast_cost_function(self, assignments, account_rate = 1.0, soft_penalty = SOFT_PENALTY_PER_PERSON):\n        return faster_cost(assignments.numpy(), self.np_costs_matrix, self.np_family_size, self.days, account_rate, soft_penalty)\n\n    def is_valid(self, assignments):\n        # Calculate days\n        self.daily_occupancy.zero_()\n        self.daily_occupancy.index_add_(0, assignments, self.family_size)\n        self.daily_occupancy[0] = MIN_OCCUPANCY\n        self.daily_occupancy[-1] = MIN_OCCUPANCY\n\n        torch.clamp(self.daily_occupancy, MAX_OCCUPANCY, 10000000, out=self.excess)\n        self.excess.add_(-MAX_OCCUPANCY)\n        torch.clamp(self.daily_occupancy, 0, MIN_OCCUPANCY, out=self.shortage)\n        self.shortage.add_(-MIN_OCCUPANCY)\n\n        mismatch = (self.excess.sum() - self.shortage.sum()).item()\n\n        return (mismatch == 0)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Submission management\n\nIncludes over-complex rules for deciding which score incrememts are worth saving."},{"metadata":{"trusted":true},"cell_type":"code","source":"class Submission:\n    def __init__(self, sample_path, solutions_path):\n        self.sample_path = sample_path\n        self.solutions_path = solutions_path\n        self.load_sample_solution()\n        self.valid_write_point = 85000.0\n\n    def load_sample_solution(self):\n        # This is just so we have a template to write with\n        self.table = pd.read_csv(self.sample_path + 'sample_submission.csv', index_col='family_id')\n\n    def write(self, torch_data, score):\n        self.table['assigned_day'] = torch_data.cpu().numpy()\n        path = f'{self.solutions_path}submission_{int(score)}.csv'\n        self.table.to_csv(path)\n        print(f'Wrote {path}')\n\n    def get_assigned_days(self):\n        return self.table['assigned_day'].values\n\n    def update_write_point(self, actual_score):\n        if self.valid_write_point > 80000:\n            self.valid_write_point = min(actual_score - 0.1, self.valid_write_point - 1000)\n            return\n\n        if self.valid_write_point > 75000:\n            self.valid_write_point = min(actual_score - 0.1, self.valid_write_point - 500)\n            return\n\n        if self.valid_write_point > 70000:\n            self.valid_write_point = min(actual_score - 0.1, self.valid_write_point - 100)\n            return\n\n        if self.valid_write_point > 69000:\n            self.valid_write_point = min(actual_score - 0.1, self.valid_write_point - 20)\n            return\n\n        self.valid_write_point = actual_score - 0.1\n\n    def write_if_better(self, torch_data, score):\n        if score >= self.valid_write_point:\n            return\n\n        self.write(torch_data, score)\n\n        self.update_write_point(score)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Optimiser\n\nImplemented here as did not want to use PyTorch's built in ones, we're not making use of automatic differentiation."},{"metadata":{"trusted":true},"cell_type":"code","source":"class Optimiser:\n    def __init__(self, device):\n        self.device = device\n        if USE_ADAM:\n            self.adam_beta_m_product = ADAM_BETA_M\n            self.adam_beta_v_product = ADAM_BETA_V\n            self.adam_m = torch.zeros(N_FAMILIES, MAX_ACTION, device = self.device)\n            self.adam_v = torch.zeros(N_FAMILIES, MAX_ACTION, device = self.device)\n        else:\n            self.gradient_vs = torch.zeros(N_FAMILIES, MAX_ACTION, device = self.device)\n\n    def update(self, preferences, gradients):\n        if USE_ADAM:\n            self.adam_m = ADAM_BETA_M * self.adam_m + (1 - ADAM_BETA_M) * gradients\n            self.adam_v = ADAM_BETA_V * self.adam_v + (1 - ADAM_BETA_V) * gradients * gradients\n            m_hat = self.adam_m/(1 - self.adam_beta_m_product)\n            v_hat = self.adam_v/(1 - self.adam_beta_v_product)\n            adam_update = m_hat / (torch.sqrt(v_hat) + ADAM_EPSILON)\n            self.adam_beta_m_product *= ADAM_BETA_M\n            self.adam_beta_v_product *= ADAM_BETA_V\n\n            # Gradient *descent* because we have a cost to minimise, not a reward to maximise\n            preferences -= LR * adam_update\n        else:\n            # Simple momentum-based update\n            self.gradient_vs = (MOMENTUM * self.gradient_vs) + gradients\n            preferences -= LR * gradient_vs\n\n        preferences = (preferences - preferences.max(axis=1).values.view(-1,1)) + MAX_PREFERENCE\n        preferences.clamp_(0.0, MAX_PREFERENCE)\n\n        return preferences\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## The Gradient Bandit Agent\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"class Agent:\n    def __init__(self, problem, optimiser, device):\n        self.device = device\n        self.problem = problem\n        self.optimiser = optimiser\n\n        # Starting preferences\n        self.preferences = torch.zeros(N_FAMILIES, MAX_ACTION, device = self.device)\n\n        # Buffers\n        self.w = torch.zeros(N_FAMILIES, MAX_ACTION, device = self.device)\n        self.z = torch.zeros(N_FAMILIES, 1, device = self.device)\n        self.actions = torch.zeros(N_FAMILIES, BATCH_SIZE, dtype=torch.long, device = self.device)\n        self.days = torch.zeros(N_FAMILIES, BATCH_SIZE, dtype=torch.long, device = self.device)\n\n        self.best_score = 100000000.0\n        self.best_item = None\n        self.ramp_speed = 1.0/PENALTY_RAMP_TIME\n\n    def _get_batch(self):\n        torch.exp(self.preferences, out=self.w)\n        torch.sum(self.w, axis=1, keepdims=True, out=self.z)\n        self.w.div_(self.z)\n        torch.multinomial(self.w, BATCH_SIZE, replacement=True, out=self.actions)\n        torch.gather(self.problem.actions_to_days, 1, self.actions, out=self.days)\n        return (self.w, self.actions.T, self.days.T)\n\n    def _get_greedy(self):\n        actions = self.preferences.argmax(axis=1).view(-1,1)\n        days = self.problem.actions_to_days.gather(1, actions)\n        return days.T[0]\n\n    def _get_batch_scores(self, days_allocated, batch_id):\n        batch_scores = []\n        sp = min(batch_id * self.ramp_speed * SOFT_PENALTY_PER_PERSON, SOFT_PENALTY_PER_PERSON)\n        ar = min(1.0, batch_id * self.ramp_speed)\n\n        for i in range(BATCH_SIZE):\n            assignments = days_allocated[i]\n            score = self.problem.fast_cost_function(assignments, account_rate = ar, soft_penalty = sp)\n            if score < self.best_score:\n                self.best_score = score\n                self.best_item = assignments.clone().detach()\n\n            batch_scores.append( score )\n\n        return batch_scores\n\n    def _get_batch_gradients(self, probs, actions, batch_scores):\n        gradients =  x = torch.zeros(N_FAMILIES, MAX_ACTION, device = self.device)\n\n        for i in range(BATCH_SIZE):\n            score = batch_scores[i]\n            relative_score = (score - self.baseline)\n\n            # First assume none of actions were taken\n            # If we didn't choose an action and the score increased over baseline, then\n            # choosing the unused action instead *might* decrease the score\n            this_gradient = -relative_score * probs\n\n            # Find the actions actually taken\n            selector = actions[i].reshape(-1,1)\n            selected_probs = probs.gather(1, selector)\n\n            # With the taken actions - if we chose an item when the score was higher, then\n            # making this action more probable should increase the score\n            chosen_gradients = relative_score * (1 - selected_probs)\n            this_gradient.scatter_(1, selector, chosen_gradients)\n\n            # Accumulate\n            gradients += this_gradient\n\n        gradients /= BATCH_SIZE\n        gradients.clamp_(-GRADIENT_CLIP, GRADIENT_CLIP)\n\n        return gradients\n\n    def process_batch(self, batch_id):\n        t = time.clock_gettime(0)\n        probs, actions, days_allocated = self._get_batch()\n        batch_scores = self._get_batch_scores(days_allocated, batch_id)\n\n        if batch_id > 0:\n            gradients = self._get_batch_gradients(probs, actions, batch_scores)\n            self.preferences = self.optimiser.update(self.preferences, gradients)\n\n            # Whilst ramping up, best_score needs adjusting to new realities\n            if (batch_id <= PENALTY_RAMP_TIME):\n                sp = min(batch_id * SOFT_PENALTY_PER_PERSON * self.ramp_speed, SOFT_PENALTY_PER_PERSON)\n                ar = min(1.0, batch_id * self.ramp_speed)\n                self.best_score = self.problem.cost_function(self.best_item, account_rate = ar, soft_penalty = sp)\n\n        self.baseline = np.array(batch_scores).mean()\n\n        self.greedy_item = self._get_greedy()\n        self.greedy_score = self.problem.cost_function(self.greedy_item)\n        t = time.clock_gettime(0) - t\n\n        if batch_id % 100 == 0:\n            print(\"Batch {} ({}s). Mean {}. Best {}. Greedy {}\".format(\n                   batch_id, round(t,1), int(np.array(batch_scores).mean()),\n                   int(self.best_score), int(self.greedy_score) ) )\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Main outer loop\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"def main():\n    submission = Submission(INPUT_PATH, OUTPUT_PATH)\n\n    problem = Problem(INPUT_PATH, DEFAULT_DEVICE)\n    optimiser = Optimiser(DEFAULT_DEVICE)\n    agent = Agent(problem, optimiser, DEFAULT_DEVICE)\n\n    for batch_id in range(N_BATCHES):\n        agent.process_batch(batch_id)\n\n        if agent.greedy_score < submission.valid_write_point and problem.is_valid(agent.greedy_item):\n            submission.write_if_better(agent.greedy_item, agent.greedy_score)\n\n        if batch_id > PENALTY_RAMP_TIME and agent.best_score < submission.valid_write_point and problem.is_valid(agent.best_item):\n            submission.write_if_better(agent.best_item, agent.best_score)\n            \n    submission.valid_write_point = 100000\n    \n    # A bit fiddly at the end, as we are never sure that the scores represent anything valid\n    if agent.greedy_score > agent.best_score:\n        if problem.is_valid(agent.best_item):\n            submission.write_if_better(agent.best_item, agent.best_score)\n        if problem.is_valid(agent.greedy_item):\n            submission.write_if_better(agent.greedy_item, agent.greedy_score)\n    else:\n        if problem.is_valid(agent.greedy_item):\n            submission.write_if_better(agent.greedy_item, agent.greedy_score)\n        if problem.is_valid(agent.best_item):\n            submission.write_if_better(agent.best_item, agent.best_score)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Run it\n\nThe output is once every 100 batches. Scores for mean and \"best\" result are shown according to how the GB sees it - with soft constraints and accounting costs ramping up slowly (so they are not relevant to the full problem until after the ramp up batches). The score for the greedy solution is always evaluated using the full penalties and accounting. It starts at around 27 billion!"},{"metadata":{"trusted":true},"cell_type":"code","source":"main()\n\nprint(\"DONE\")","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}