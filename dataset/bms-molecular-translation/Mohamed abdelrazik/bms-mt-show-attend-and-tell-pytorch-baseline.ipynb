{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"%%capture\n!pip install timm\n!pip install python-Levenshtein","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nimport time\n\nimport pandas as pd\nimport numpy as np\n\nimport timm\n\nimport cv2\n\nfrom tqdm import tqdm\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\nfrom torch.nn.utils.rnn import pad_sequence\nfrom torch.nn.utils.rnn import pack_padded_sequence\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.backends.cudnn as cudnn\n\nfrom sklearn.model_selection import train_test_split\n\nfrom Levenshtein import distance as levenshtein_distance","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"BASE_DIR = '../input/bms-molecular-translation/train'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"label_df = pd.read_csv(\"../input/bms-molecular-translation/train_labels.csv\")\nlabel_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"label_df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Currently using only 90k+10k images\nlabel_df = label_df.iloc[:100000]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Train on 45000, validate on 10k\ntrain_df, test_df = train_test_split(label_df, test_size=0.1, shuffle=False)\ntrain_df = train_df.reset_index(drop=True)\ntest_df = test_df.reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.shape, test_df.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Build Vocabulary"},{"metadata":{},"cell_type":"markdown","source":"There are only limited set of characters in the InChI notation. Refer this for more details: https://www.kaggle.com/c/bms-molecular-translation/discussion/223471. So I'll build a character based vocabulary class which will be used for target preprocessing."},{"metadata":{"trusted":true},"cell_type":"code","source":"class Vocabulary:\n    def __init__(self, freq_threshold=2, reverse=False):\n        self.itos = {0: \"<pad>\", 1: \"<sos>\", 2: \"<eos>\", 3: \"<unk>\"}\n        self.stoi = {\"<pad>\": 0, \"<sos>\": 1, \"<eos>\": 2, \"<unk>\": 3}\n        self.freq_threshold = freq_threshold\n        self.reverse = reverse\n        self.tokenizer = self._tokenizer\n\n    def __len__(self):\n        return len(self.itos)\n    \n    def _tokenizer(self, text):\n        return (char for char in text)\n\n    def tokenize(self, text):\n        if self.reverse:\n            return [token for token in self.tokenizer(text)][::-1]\n        else:\n            return [token for token in self.tokenizer(text)]\n\n    def build_vocabulary(self, sentence_list):\n        \"\"\"Basically builds a frequency map for all possible characters.\"\"\"\n        frequencies = {}\n        idx = len(self.itos)\n\n        for sentence in sentence_list:\n            # Preprocess the InChI.\n            for char in self.tokenize(sentence):\n                if char in frequencies:\n                    frequencies[char] += 1\n                else:\n                    frequencies[char] = 1\n\n                if frequencies[char] == self.freq_threshold:\n                    self.stoi[char] = idx\n                    self.itos[idx] = char\n                    idx += 1\n\n    def numericalize(self, text):\n        \"\"\"Convert characters to numbers.\"\"\"\n        tokenized_text = self.tokenize(text)\n\n        return [\n            self.stoi[token] if token in self.stoi else self.stoi[\"<unk>\"]\n            for token in tokenized_text\n        ]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n# Build vocab using training data\nfreq_threshold = 2\nvocab = Vocabulary(freq_threshold=freq_threshold, reverse=False)\n\n# build vocab\nvocab.build_vocabulary(train_df['InChI'].to_list())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"IMG_SIZE = 128","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class BMSDataset(Dataset):\n    def __init__(self, df):\n        super().__init__()\n        self.df = df\n        self.vocab = vocab\n    \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, idx):\n        # Read the image\n        img_id = self.df.iloc[idx]['image_id']\n        label = self.df.iloc[idx]['InChI']\n        label_len = len(label) + 2  # (2 for <sos> and <eos>)\n        img_path = os.path.join(BASE_DIR, img_id[0], img_id[1], img_id[2], f'{img_id}.png')\n        \n        img = self._load_from_file(img_path)\n        \n        # Convert label to numbers\n        label = self._get_numericalized(label, self.vocab)\n        return img, torch.tensor(label), torch.tensor(label_len)\n    \n    def _get_numericalized(self, sentence, vocab):\n        \"\"\"Numericalize given text using prebuilt vocab.\"\"\"\n        numericalized = [vocab.stoi[\"<sos>\"]]\n        numericalized.extend(vocab.numericalize(sentence))\n        numericalized.append(vocab.stoi[\"<eos>\"])\n        return numericalized\n\n    def _load_from_file(self, img_path):\n        image = cv2.imread(img_path, cv2.IMREAD_COLOR)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n        image = cv2.resize(image, (IMG_SIZE, IMG_SIZE)) \n        image /= 255.0  # Normalize\n        return image","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def bms_collate(batch):\n    \n    imgs, labels, label_lens = [], [], []\n    \n    for data_point in batch:\n        imgs.append(torch.from_numpy(data_point[0]).permute(2, 0, 1))\n        labels.append(data_point[1])\n        label_lens.append(data_point[2])\n\n    \n    labels = pad_sequence(labels, batch_first=True, padding_value=vocab.stoi[\"<pad>\"])\n\n    return torch.stack(imgs), labels, torch.stack(label_lens).reshape(-1, 1)\n    \n\ntrain_dataset = BMSDataset(train_df)\n\ntrain_loader = DataLoader(\n    train_dataset,\n    batch_size=64,\n    pin_memory=True,\n    num_workers=4,\n    shuffle=True,\n    collate_fn=bms_collate\n)\n\nval_dataset = BMSDataset(test_df)\nval_loader = DataLoader(\n    val_dataset,\n    batch_size=128,\n    pin_memory=True,\n    num_workers=4,\n    shuffle=False,\n    collate_fn=bms_collate\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Available device: {device}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model Architecture"},{"metadata":{},"cell_type":"markdown","source":"This code is heavily based on this great tutorial: https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Image-Captioning"},{"metadata":{"trusted":true},"cell_type":"code","source":"class Encoder(nn.Module):\n    \"\"\"\n    Encoder.\n    \"\"\"\n\n    def __init__(self, back_bone, encoded_image_size=8):\n        super(Encoder, self).__init__()\n        self.enc_image_size = encoded_image_size\n\n        # Remove linear and pool layers (since we're not doing classification)\n        modules = list(back_bone.children())[:-2]\n        self.back_bone = nn.Sequential(*modules)\n\n        # Resize image to fixed size to allow input images of variable size\n        self.adaptive_pool = nn.AdaptiveAvgPool2d((encoded_image_size, encoded_image_size))\n\n        self.fine_tune(True)\n\n    def forward(self, images):\n        \"\"\"\n        Forward propagation.\n        :param images: images, a tensor of dimensions (batch_size, 3, image_size, image_size)\n        :return: encoded images\n        \"\"\"\n        # Extract features\n        out = self.back_bone(images)  # (batch_size, 2048, image_size/32, image_size/32)\n        out = self.adaptive_pool(out)  # (batch_size, 2048, encoded_image_size, encoded_image_size)\n        out = out.permute(0, 2, 3, 1)  # (batch_size, encoded_image_size, encoded_image_size, 2048)\n        return out\n\n    def fine_tune(self, fine_tune=True):\n        \"\"\"\n        Allow or prevent the computation of gradients for convolutional blocks 2 through 4 of the encoder.\n        :param fine_tune: Allow?\n        \"\"\"\n        for p in self.back_bone.parameters():\n            p.requires_grad = False\n        # If fine-tuning, only fine-tune convolutional blocks 2 through 4\n        for c in list(self.back_bone.children())[5:]:\n            for p in c.parameters():\n                p.requires_grad = fine_tune","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Attention(nn.Module):\n    \"\"\"\n    Attention Network.\n    \"\"\"\n\n    def __init__(self, encoder_dim, decoder_dim, attention_dim):\n        \"\"\"\n        :param encoder_dim: feature size of encoded images\n        :param decoder_dim: size of decoder's RNN\n        :param attention_dim: size of the attention network\n        \"\"\"\n        super(Attention, self).__init__()\n        self.encoder_att = nn.Linear(encoder_dim, attention_dim)  # linear layer to transform encoded image\n        self.decoder_att = nn.Linear(decoder_dim, attention_dim)  # linear layer to transform decoder's output\n        self.full_att = nn.Linear(attention_dim, 1)  # linear layer to calculate values to be softmax-ed\n        self.relu = nn.ReLU()\n        self.softmax = nn.Softmax(dim=1)  # softmax layer to calculate weights\n\n    def forward(self, encoder_out, decoder_hidden):\n        \"\"\"\n        Forward propagation.\n        :param encoder_out: encoded images, a tensor of dimension (batch_size, num_pixels, encoder_dim)\n        :param decoder_hidden: previous decoder output, a tensor of dimension (batch_size, decoder_dim)\n        :return: attention weighted encoding, weights\n        \"\"\"\n        att1 = self.encoder_att(encoder_out)  # (batch_size, num_pixels, attention_dim)\n        att2 = self.decoder_att(decoder_hidden)  # (batch_size, attention_dim)\n        att = self.full_att(self.relu(att1 + att2.unsqueeze(1))).squeeze(2)  # (batch_size, num_pixels)\n        alpha = self.softmax(att)  # (batch_size, num_pixels)\n        attention_weighted_encoding = (encoder_out * alpha.unsqueeze(2)).sum(dim=1)  # (batch_size, encoder_dim)\n\n        return attention_weighted_encoding, alpha","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class DecoderWithAttention(nn.Module):\n    \"\"\"\n    Decoder.\n    \"\"\"\n\n    def __init__(self, attention_dim, embed_dim, decoder_dim, vocab_size, encoder_dim=2048, dropout=0.4):\n        \"\"\"\n        :param attention_dim: size of attention network\n        :param embed_dim: embedding size\n        :param decoder_dim: size of decoder's RNN\n        :param vocab_size: size of vocabulary\n        :param encoder_dim: feature size of encoded images\n        :param dropout: dropout\n        \"\"\"\n        super(DecoderWithAttention, self).__init__()\n\n        self.encoder_dim = encoder_dim\n        self.attention_dim = attention_dim\n        self.embed_dim = embed_dim\n        self.decoder_dim = decoder_dim\n        self.vocab_size = vocab_size\n        self.dropout = dropout\n\n        self.attention = Attention(encoder_dim, decoder_dim, attention_dim)  # attention network\n\n        self.embedding = nn.Embedding(vocab_size, embed_dim)  # embedding layer\n        self.dropout = nn.Dropout(p=self.dropout)\n        self.decode_step = nn.LSTMCell(embed_dim + encoder_dim, decoder_dim, bias=True)  # decoding LSTMCell\n        self.init_h = nn.Linear(encoder_dim, decoder_dim)  # linear layer to find initial hidden state of LSTMCell\n        self.init_c = nn.Linear(encoder_dim, decoder_dim)  # linear layer to find initial cell state of LSTMCell\n        self.f_beta = nn.Linear(decoder_dim, encoder_dim)  # linear layer to create a sigmoid-activated gate\n        self.sigmoid = nn.Sigmoid()\n        self.fc = nn.Linear(decoder_dim, vocab_size)  # linear layer to find scores over vocabulary\n        self.init_weights()  # initialize some layers with the uniform distribution\n\n    def init_weights(self):\n        \"\"\"\n        Initializes some parameters with values from the uniform distribution, for easier convergence.\n        \"\"\"\n        self.embedding.weight.data.uniform_(-0.1, 0.1)\n        self.fc.bias.data.fill_(0)\n        self.fc.weight.data.uniform_(-0.1, 0.1)\n\n    def load_pretrained_embeddings(self, embeddings):\n        \"\"\"\n        Loads embedding layer with pre-trained embeddings.\n        :param embeddings: pre-trained embeddings\n        \"\"\"\n        self.embedding.weight = nn.Parameter(embeddings)\n\n    def fine_tune_embeddings(self, fine_tune=True):\n        \"\"\"\n        Allow fine-tuning of embedding layer? (Only makes sense to not-allow if using pre-trained embeddings).\n        :param fine_tune: Allow?\n        \"\"\"\n        for p in self.embedding.parameters():\n            p.requires_grad = fine_tune\n\n    def init_hidden_state(self, encoder_out):\n        \"\"\"\n        Creates the initial hidden and cell states for the decoder's LSTM based on the encoded images.\n        :param encoder_out: encoded images, a tensor of dimension (batch_size, num_pixels, encoder_dim)\n        :return: hidden state, cell state\n        \"\"\"\n        mean_encoder_out = encoder_out.mean(dim=1)\n        h = self.init_h(mean_encoder_out)  # (batch_size, decoder_dim)\n        c = self.init_c(mean_encoder_out)\n        return h, c\n\n    def forward(self, encoder_out, encoded_captions, caption_lengths):\n        \"\"\"\n        Forward propagation.\n        :param encoder_out: encoded images, a tensor of dimension (batch_size, enc_image_size, enc_image_size, encoder_dim)\n        :param encoded_captions: encoded captions, a tensor of dimension (batch_size, max_caption_length)\n        :param caption_lengths: caption lengths, a tensor of dimension (batch_size, 1)\n        :return: scores for vocabulary, sorted encoded captions, decode lengths, weights, sort indices\n        \"\"\"\n        batch_size = encoder_out.size(0)\n        encoder_dim = encoder_out.size(-1)\n        vocab_size = self.vocab_size\n\n        # Flatten image\n        encoder_out = encoder_out.view(batch_size, -1, encoder_dim)  # (batch_size, num_pixels, encoder_dim)\n        num_pixels = encoder_out.size(1)\n\n        # Sort input data by decreasing lengths; why? apparent below\n        caption_lengths, sort_ind = caption_lengths.squeeze(1).sort(dim=0, descending=True)\n        encoder_out = encoder_out[sort_ind]\n        encoded_captions = encoded_captions[sort_ind]\n\n        # Embedding\n        embeddings = self.embedding(encoded_captions)  # (batch_size, max_caption_length, embed_dim)\n\n        # Initialize LSTM state\n        h, c = self.init_hidden_state(encoder_out)  # (batch_size, decoder_dim)\n\n        # We won't decode at the <end> position, since we've finished generating as soon as we generate <end>\n        # So, decoding lengths are actual lengths - 1\n        decode_lengths = (caption_lengths - 1).tolist()\n\n        # Create tensors to hold word predicion scores and alphas\n        predictions = torch.zeros(batch_size, max(decode_lengths), vocab_size).to(device)\n        alphas = torch.zeros(batch_size, max(decode_lengths), num_pixels).to(device)\n\n        # At each time-step, decode by\n        # attention-weighing the encoder's output based on the decoder's previous hidden state output\n        # then generate a new word in the decoder with the previous word and the attention weighted encoding\n        for t in range(max(decode_lengths)):\n            batch_size_t = sum([l > t for l in decode_lengths])\n            attention_weighted_encoding, alpha = self.attention(encoder_out[:batch_size_t],\n                                                                h[:batch_size_t])\n            gate = self.sigmoid(self.f_beta(h[:batch_size_t]))  # gating scalar, (batch_size_t, encoder_dim)\n            attention_weighted_encoding = gate * attention_weighted_encoding\n            h, c = self.decode_step(\n                torch.cat([embeddings[:batch_size_t, t, :], attention_weighted_encoding], dim=1),\n                (h[:batch_size_t], c[:batch_size_t]))  # (batch_size_t, decoder_dim)\n            preds = self.fc(self.dropout(h))  # (batch_size_t, vocab_size)\n            predictions[:batch_size_t, t, :] = preds\n            alphas[:batch_size_t, t, :] = alpha\n\n        return predictions, encoded_captions, decode_lengths, alphas, sort_ind","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Training"},{"metadata":{"trusted":true},"cell_type":"code","source":"class AverageMeter:\n    \"\"\"\n    Keeps track of most recent, average, sum, and count of a metric.\n    \"\"\"\n\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def accuracy(scores, targets, k):\n    \"\"\"\n    Computes top-k accuracy, from predicted and true labels.\n    :param scores: scores from the model\n    :param targets: true labels\n    :param k: k in top-k accuracy\n    :return: top-k accuracy\n    \"\"\"\n\n    batch_size = targets.size(0)\n    _, ind = scores.topk(k, 1, True, True)\n    correct = ind.eq(targets.view(-1, 1).expand_as(ind))\n    correct_total = correct.view(-1).float().sum()  # 0D tensor\n    return correct_total.item() * (100.0 / batch_size)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def clip_gradient(optimizer, grad_clip):\n    \"\"\"\n    Clips gradients computed during backpropagation to avoid explosion of gradients.\n    :param optimizer: optimizer with the gradients to be clipped\n    :param grad_clip: clip value\n    \"\"\"\n    for group in optimizer.param_groups:\n        for param in group['params']:\n            if param.grad is not None:\n                param.grad.data.clamp_(-grad_clip, grad_clip)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_epoch(train_loader, encoder, decoder, criterion, encoder_optimizer, decoder_optimizer, epoch):\n    \"\"\"\n    Performs one epoch's training.\n    :param train_loader: DataLoader for training data\n    :param encoder: encoder model\n    :param decoder: decoder model\n    :param criterion: loss layer\n    :param encoder_optimizer: optimizer to update encoder's weights (if fine-tuning)\n    :param decoder_optimizer: optimizer to update decoder's weights\n    :param epoch: epoch number\n    \"\"\"\n    \n    batch_time = AverageMeter()  # forward prop. + back prop. time\n    losses = AverageMeter()  # loss (per word decoded)\n    top5accs = AverageMeter()  # top5 accuracy\n    \n    decoder.train()  # train mode (dropout and batchnorm is used)\n    encoder.train()\n    start = time.time()\n\n    for i, (imgs, caps, caplens) in tqdm(enumerate(train_loader), total=len(train_loader), position=0, leave=True):\n        # Move to GPU, if available\n        imgs = imgs.to(device)\n        caps = caps.to(device)\n        caplens = caplens.to(device)\n\n        # Forward prop.\n        imgs = encoder(imgs)\n        scores, caps_sorted, decode_lengths, alphas, sort_ind = decoder(imgs, caps, caplens)\n\n        # Since we decoded starting with <start>, the targets are all words after <start>, up to <end>\n        targets = caps_sorted[:, 1:]\n\n        # Remove timesteps that we didn't decode at, or are pads\n        # pack_padded_sequence is an easy trick to do this\n        scores = pack_padded_sequence(scores, decode_lengths, batch_first=True).data\n        targets = pack_padded_sequence(targets, decode_lengths, batch_first=True).data\n\n        # Calculate loss\n        loss = criterion(scores, targets)\n\n        # Add doubly stochastic attention regularization\n        loss += alpha_c * ((1. - alphas.sum(dim=1)) ** 2).mean()\n\n        # Back prop.\n        decoder_optimizer.zero_grad()\n        if encoder_optimizer is not None:\n            encoder_optimizer.zero_grad()\n        loss.backward()\n\n        # Clip gradients\n        if grad_clip is not None:\n            clip_gradient(decoder_optimizer, grad_clip)\n            if encoder_optimizer is not None:\n                clip_gradient(encoder_optimizer, grad_clip)\n\n        # Update weights\n        decoder_optimizer.step()\n        if encoder_optimizer is not None:\n            encoder_optimizer.step()\n\n        # Keep track of metrics\n        top5 = accuracy(scores, targets, 5)\n        losses.update(loss.item(), sum(decode_lengths))\n        top5accs.update(top5, sum(decode_lengths))\n        batch_time.update(time.time() - start)\n\n        start = time.time()\n        \n    return losses, top5accs, batch_time","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def validate_epoch(val_loader, encoder, decoder, criterion):\n    decoder.eval()  # eval mode (no dropout or batchnorm)\n    if encoder is not None:\n        encoder.eval()\n    \n    batch_time = AverageMeter()\n    losses = AverageMeter()\n    top5accs = AverageMeter()\n    \n    start = time.time()\n\n    references = list()  # references (true captions) for calculating BLEU-4 score\n    hypotheses = list()  # hypotheses (predictions)\n\n    with torch.no_grad():\n        # Batches\n        for i, (imgs, caps, caplens) in tqdm(enumerate(val_loader), total=len(val_loader), position=0, leave=True):\n\n            # Move to device, if available\n            imgs = imgs.to(device)\n            caps = caps.to(device)\n            caplens = caplens.to(device)\n\n            # Forward prop.\n            if encoder is not None:\n                imgs = encoder(imgs)\n            scores, caps_sorted, decode_lengths, alphas, sort_ind = decoder(imgs, caps, caplens)\n\n            # Since we decoded starting with <start>, the targets are all words after <start>, up to <end>\n            targets = caps_sorted[:, 1:]\n\n            # Remove timesteps that we didn't decode at, or are pads\n            # pack_padded_sequence is an easy trick to do this\n            scores_copy = scores.clone()\n            scores = pack_padded_sequence(scores, decode_lengths, batch_first=True).data\n            targets = pack_padded_sequence(targets, decode_lengths, batch_first=True).data\n\n            # Calculate loss\n            loss = criterion(scores, targets)\n\n            # Add doubly stochastic attention regularization\n            loss += alpha_c * ((1. - alphas.sum(dim=1)) ** 2).mean()\n\n            # Keep track of metrics\n            losses.update(loss.item(), sum(decode_lengths))\n            top5 = accuracy(scores, targets, 5)\n            top5accs.update(top5, sum(decode_lengths))\n            batch_time.update(time.time() - start)\n\n            start = time.time()\n        \n        return losses, top5accs, batch_time","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Model parameters\nemb_dim = 512  # dimension of word embeddings\nattention_dim = 512  # dimension of attention linear layers\ndecoder_dim = 512  # dimension of decoder RNN\ndropout = 0.4\ncudnn.benchmark = True  # set to true only if inputs to model are fixed size; otherwise lot of computational overhead\n\n# Training parameters\nstart_epoch = 0\nepochs = 5  # number of epochs to train for (if early stopping is not triggered)\nepochs_since_improvement = 0  # keeps track of number of epochs since there's been an improvement in validation BLEU\nworkers = 4  # for data-loading; right now, only 1 works with h5py\nencoder_lr = 1e-3  # learning rate for encoder if fine-tuning\ndecoder_lr = 4e-3  # learning rate for decoder\ngrad_clip = 5.  # clip gradients at an absolute value of\nalpha_c = 1.  # regularization parameter for 'doubly stochastic attention', as in the paper\nbest_bleu4 = 0.  # BLEU-4 score right now\nfine_tune_encoder = False  # fine-tune encoder?","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"decoder = DecoderWithAttention(attention_dim=attention_dim,\n                               embed_dim=emb_dim,\n                               decoder_dim=decoder_dim,\n                               vocab_size=len(vocab),\n                               dropout=dropout)\ndecoder_optimizer = torch.optim.Adam(params=filter(lambda p: p.requires_grad, decoder.parameters()),\n                                     lr=decoder_lr)\nencoder = Encoder(timm.create_model('resnet101', pretrained=True))\nencoder.fine_tune(fine_tune_encoder)\nencoder_optimizer = torch.optim.Adam(params=filter(lambda p: p.requires_grad, encoder.parameters()),\n                                     lr=encoder_lr) if fine_tune_encoder else None","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"decoder = decoder.to(device)\nencoder = encoder.to(device)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Loss function\ncriterion = nn.CrossEntropyLoss().to(device)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def save_checkpoint(epoch, encoder, decoder, encoder_optimizer, decoder_optimizer):\n    \"\"\"\n    Saves model checkpoint.\n    :param data_name: base name of processed dataset\n    :param epoch: epoch number\n    :param epochs_since_improvement: number of epochs since last improvement in BLEU-4 score\n    :param encoder: encoder model\n    :param decoder: decoder model\n    :param encoder_optimizer: optimizer to update encoder's weights, if fine-tuning\n    :param decoder_optimizer: optimizer to update decoder's weights\n    :param bleu4: validation BLEU-4 score for this epoch\n    :param is_best: is this checkpoint the best so far?\n    \"\"\"\n    state = {'epoch': epoch,\n             'encoder': encoder,\n             'decoder': decoder,\n             'encoder_optimizer': encoder_optimizer,\n             'decoder_optimizer': decoder_optimizer}\n    filename = f'checkpoint_{epoch}.pth.tar'\n    torch.save(state, filename)\n    # If this checkpoint is the best so far, store a copy so it doesn't get overwritten by a worse checkpoint","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# For inference:\nepochs = 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"best_score = 0\nfor epoch in range(start_epoch, epochs):\n\n    # Decay learning rate if there is no improvement for 8 consecutive epochs, and terminate training after 20\n    if epochs_since_improvement == 20:\n        break\n    if epochs_since_improvement > 0 and epochs_since_improvement % 8 == 0:\n        adjust_learning_rate(decoder_optimizer, 0.8)\n        if fine_tune_encoder:\n            adjust_learning_rate(encoder_optimizer, 0.8)\n\n    # One epoch's training\n    train_loss, train_top5acc, batch_time = train_epoch(\n        train_loader=train_loader,\n        encoder=encoder,\n        decoder=decoder,\n        criterion=criterion,\n        encoder_optimizer=encoder_optimizer,\n        decoder_optimizer=decoder_optimizer,\n        epoch=epoch\n    )\n    \n    val_loss, val_top5acc, _ = validate_epoch(\n        val_loader,\n        encoder,\n        decoder,\n        criterion\n    )\n    \n    if best_score < val_top5acc.avg:\n        best_score = val_top5acc.avg\n        print(f\"Saving checkpoint. Best score: {best_score:.4f}\")\n        save_checkpoint(epoch+1, encoder, decoder, encoder_optimizer, decoder_optimizer)\n    \n    \n    print(f'Epoch: {epoch+1:02} | Time: {batch_time.avg} sec')\n    print(f'\\t    Train Loss: {train_loss.avg:.4f} | Val. Loss: {val_loss.avg:.4f}')\n    print(f'\\t    Top5 Acc.: {train_top5acc.avg:.3f} | Val. Top5 Acc.: {val_top5acc.avg:.3f} \\n')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Inference"},{"metadata":{"trusted":true},"cell_type":"code","source":"checkpoint = torch.load('../input/bmsmt/checkpoint_12.pth.tar')\nencoder = checkpoint['encoder']\ndecoder = checkpoint['decoder']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_sub_df = pd.read_csv(\"../input/bms-molecular-translation/sample_submission.csv\")\nsample_sub_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class BMSDatasetTest(Dataset):\n    def __init__(self, df):\n        super().__init__()\n        self.df = df\n    \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, idx):\n        # Read the image\n        img_id = self.df.iloc[idx]['image_id']\n        img_path = os.path.join('../input/bms-molecular-translation/test', img_id[0], img_id[1], img_id[2], f'{img_id}.png')\n        \n        img = self._load_from_file(img_path)\n        return torch.tensor(img).permute(2, 0, 1)\n\n    def _load_from_file(self, img_path):\n        image = cv2.imread(img_path, cv2.IMREAD_COLOR)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n        image = cv2.resize(image, (IMG_SIZE, IMG_SIZE)) \n        image /= 255.0  # Normalize\n        return image","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_dataset = BMSDatasetTest(sample_sub_df)\ntest_batch_size = 128\ntest_dataloader = DataLoader(\n    test_dataset,\n    batch_size=test_batch_size,\n    pin_memory=True,\n    num_workers=4,\n    shuffle=False,\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"max_pred_len = 100","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def inference(encoder, decoder, imgs, vocab):\n    imgs = imgs.to(device)\n    batch_size = len(imgs)\n    \n    encoder_out = encoder(imgs)  # (1, enc_image_size, enc_image_size, encoder_dim)\n    enc_image_size = encoder_out.size(1)\n    encoder_dim = encoder_out.size(3)\n    \n    encoder_out = encoder_out.view(batch_size, -1, encoder_dim)\n    \n     # Start decoding\n    h, c = decoder.init_hidden_state(encoder_out)\n    start = torch.full((batch_size,1), vocab.stoi['<sos>']).to(device)\n    pred = torch.zeros((batch_size, max_pred_len), dtype=torch.long).to(device)\n    pred[:, 0] = start.squeeze()\n    \n    idx = 1\n\n    while True:\n        embeddings = decoder.embedding(start).squeeze(1)\n        \n        awe, _ = decoder.attention(encoder_out, h)\n        \n        gate = decoder.sigmoid(decoder.f_beta(h))  # gating scalar, (s, encoder_dim)\n        awe = gate * awe\n\n        h, c = decoder.decode_step(torch.cat([embeddings, awe], dim=1), (h, c))\n        scores = decoder.fc(h)  # (s, vocab_size)\n        scores = F.log_softmax(scores, dim=1)\n\n        start = scores.argmax(1).reshape(-1, 1).to(device)\n        \n        pred[:, idx] = start.squeeze(1)\n        \n        if idx >= max_pred_len-1:\n            break\n        \n        idx += 1\n        \n    return pred","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def batch_stringify(batch):\n    preds = []\n    for item in batch:\n        pred = np.vectorize(vocab.itos.get)(item)\n        # Truncate everything after <eos>\n        try:\n            pred = pred[1:np.nonzero(pred == '<eos>')[0][0]]\n        except IndexError:\n            pred = pred[1: ]\n            pass\n\n        preds.append(\"\".join(pred))\n    return preds","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Calculating Competition Metric on validation data"},{"metadata":{"trusted":true},"cell_type":"code","source":"preds, gts = [], []\nfor imgs, caps, capslen in tqdm(val_loader):\n    preds.extend(batch_stringify(inference(encoder, decoder, imgs, vocab).cpu().detach().numpy()))\n    gts.extend(batch_stringify(caps))\n\nprint(f'Levenshtein distance: {np.mean(np.vectorize(levenshtein_distance)(preds, gts))}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Prediction on Test Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"preds = []\nfor imgs in tqdm(test_dataloader):\n    preds.extend(batch_stringify(inference(encoder, decoder, imgs, vocab).cpu().detach().numpy()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_sub_df['InChI'] = preds\nsample_sub_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_sub_df.to_csv(\"submission.csv\", index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}