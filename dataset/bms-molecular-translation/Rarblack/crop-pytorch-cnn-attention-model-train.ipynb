{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# About This Notebook\n\nThis notebook is based on https://www.kaggle.com/konradb/model-train-efficientnet & https://www.kaggle.com/konradb/model-infer-efficientnet, with a final score of 8.90 achieved in the BMS competition.","metadata":{}},{"cell_type":"markdown","source":"# Import Libraries","metadata":{}},{"cell_type":"code","source":"!pip install timm","metadata":{"execution":{"iopub.status.busy":"2022-02-28T05:22:04.302164Z","iopub.execute_input":"2022-02-28T05:22:04.30242Z","iopub.status.idle":"2022-02-28T05:22:11.890748Z","shell.execute_reply.started":"2022-02-28T05:22:04.302391Z","shell.execute_reply":"2022-02-28T05:22:11.889802Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport re\nimport gc\nimport cv2\nimport timm\nimport time\nimport math\nimport torch\nimport random\nimport Levenshtein\nimport numpy as np\nimport pandas as pd\nimport torch.nn as nn\nfrom tqdm.auto import tqdm\nimport matplotlib.pyplot as plt\nimport torch.nn.functional as F\nfrom torch.optim import Adam, SGD\nimport torchvision.models as models\nfrom torch.nn.parameter import Parameter\nfrom torch.utils.data import DataLoader, Dataset\nfrom torch.nn.utils.rnn import pad_sequence, pack_padded_sequence\nfrom sklearn.model_selection import StratifiedKFold, GroupKFold, KFold\nfrom logging import getLogger, INFO, FileHandler,  Formatter,  StreamHandler\nfrom torch.optim.lr_scheduler import CosineAnnealingWarmRestarts, CosineAnnealingLR, ReduceLROnPlateau\nfrom albumentations import (\n    Compose, OneOf, Normalize, Resize, RandomResizedCrop, RandomCrop, HorizontalFlip, VerticalFlip, \n    RandomBrightness, RandomContrast, RandomBrightnessContrast, Rotate, ShiftScaleRotate, Cutout, \n    IAAAdditiveGaussianNoise, Transpose, Blur\n    )\nfrom albumentations.pytorch import ToTensorV2\nfrom albumentations import ImageOnlyTransform","metadata":{"execution":{"iopub.status.busy":"2022-02-28T05:22:11.893729Z","iopub.execute_input":"2022-02-28T05:22:11.894045Z","iopub.status.idle":"2022-02-28T05:22:11.905978Z","shell.execute_reply.started":"2022-02-28T05:22:11.893998Z","shell.execute_reply":"2022-02-28T05:22:11.905189Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(device)","metadata":{"execution":{"iopub.status.busy":"2022-02-28T05:22:11.907855Z","iopub.execute_input":"2022-02-28T05:22:11.908632Z","iopub.status.idle":"2022-02-28T05:22:11.9151Z","shell.execute_reply.started":"2022-02-28T05:22:11.90859Z","shell.execute_reply":"2022-02-28T05:22:11.914212Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# specify the number of data points to train on\nN = 50000","metadata":{"execution":{"iopub.status.busy":"2022-02-28T05:22:11.918327Z","iopub.execute_input":"2022-02-28T05:22:11.918636Z","iopub.status.idle":"2022-02-28T05:22:11.922436Z","shell.execute_reply.started":"2022-02-28T05:22:11.918599Z","shell.execute_reply":"2022-02-28T05:22:11.921465Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Read Input Data\n> Import the train dataframe containing image IDs, InChI strings, their actual lengths and parsed sequences.","metadata":{}},{"cell_type":"code","source":"# read the input data contained in the pickle file saved previously\ntrain_df = pd.read_pickle('../input/lstm-model/results-6/results-6/train.pkl')","metadata":{"execution":{"iopub.status.busy":"2022-02-28T05:28:16.698762Z","iopub.execute_input":"2022-02-28T05:28:16.699299Z","iopub.status.idle":"2022-02-28T05:28:19.030946Z","shell.execute_reply.started":"2022-02-28T05:28:16.699259Z","shell.execute_reply":"2022-02-28T05:28:19.030177Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(train_df)","metadata":{"execution":{"iopub.status.busy":"2022-02-28T05:28:19.033036Z","iopub.execute_input":"2022-02-28T05:28:19.033534Z","iopub.status.idle":"2022-02-28T05:28:19.039775Z","shell.execute_reply.started":"2022-02-28T05:28:19.033495Z","shell.execute_reply":"2022-02-28T05:28:19.038976Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Add File Paths\n> Make the process of reading the input data more efficient by storing paths to files in the train dataframe.","metadata":{}},{"cell_type":"code","source":"def get_file_path(image_id: str) -> str:\n    \n    \"\"\"\n    This method returns the paths to train images by indexing into the overall directory\n    and the image_id's components.\n    \n    :param image_id: ID of the image\n    :type  image_id: str\n    :return:         path to image\n    :rtype:          str\n    \"\"\"\n    \n    # index into original train images if '-' is not present\n    return '../input/bms-molecular-translation/train/{}/{}/{}/{}.png'.format(\n        image_id[0], image_id[1], image_id[2], image_id\n    )","metadata":{"execution":{"iopub.status.busy":"2022-02-28T05:28:19.04123Z","iopub.execute_input":"2022-02-28T05:28:19.041491Z","iopub.status.idle":"2022-02-28T05:28:19.048069Z","shell.execute_reply.started":"2022-02-28T05:28:19.041455Z","shell.execute_reply":"2022-02-28T05:28:19.047412Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# get file paths\ntrain_df['file_path'] = train_df['image_id'].apply(get_file_path)\ntrain_df.to_csv('./train_df.csv')","metadata":{"execution":{"iopub.status.busy":"2022-02-28T05:28:19.050118Z","iopub.execute_input":"2022-02-28T05:28:19.050396Z","iopub.status.idle":"2022-02-28T05:28:59.797611Z","shell.execute_reply.started":"2022-02-28T05:28:19.05036Z","shell.execute_reply":"2022-02-28T05:28:59.796625Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import the file back\ntrain_df = pd.read_csv('./train_df.csv')\n\n# display\ntrain_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-28T05:28:59.800163Z","iopub.execute_input":"2022-02-28T05:28:59.800424Z","iopub.status.idle":"2022-02-28T05:29:14.042308Z","shell.execute_reply.started":"2022-02-28T05:28:59.800385Z","shell.execute_reply":"2022-02-28T05:29:14.041585Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# limit to Â±300K data sub-selected by size 200-350 (HxW)\nvalid_ids = pd.read_csv('../input/bmssmalldataset/new_dataset.csv')['image_id']\ntrain_df  = train_df[train_df['image_id'].isin(valid_ids)]\nprint(train_df.shape)","metadata":{"execution":{"iopub.status.busy":"2022-02-28T05:29:14.043619Z","iopub.execute_input":"2022-02-28T05:29:14.043927Z","iopub.status.idle":"2022-02-28T05:29:15.713555Z","shell.execute_reply.started":"2022-02-28T05:29:14.043891Z","shell.execute_reply":"2022-02-28T05:29:15.712591Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.reset_index(inplace=True)\ntrain_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-28T05:29:15.716352Z","iopub.execute_input":"2022-02-28T05:29:15.716634Z","iopub.status.idle":"2022-02-28T05:29:15.731528Z","shell.execute_reply.started":"2022-02-28T05:29:15.716594Z","shell.execute_reply":"2022-02-28T05:29:15.730692Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# shuffle the dataset for randomness\ntrain_df  = train_df.sample(frac=1)\n\n# keep the remaining data as test set\ntest_df   = train_df.iloc[N:, :].reset_index()\n\n# reduced the shuffled dataset to 50K for the purpose of preserving GPU quota\ntrain_df  = train_df.iloc[:N, :].reset_index()\n\n# save as csv files\ntest_df.to_csv('test.csv')\ntrain_df.to_csv('reduced_train.csv')","metadata":{"execution":{"iopub.status.busy":"2022-02-28T05:29:15.733086Z","iopub.execute_input":"2022-02-28T05:29:15.733407Z","iopub.status.idle":"2022-02-28T05:29:21.42772Z","shell.execute_reply.started":"2022-02-28T05:29:15.733318Z","shell.execute_reply":"2022-02-28T05:29:21.426778Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def crop_image(gray_img):\n    \n    \"\"\"Smart crop the grayscale image to trim whitespaces on borders.\n    \n    :param gray_img: read-in grayscale image\n    :type  gray_img: np.array\n    :return:         trimmed RGB image\n    :rtype:          np.array\n    \"\"\"\n    \n    # morphological ops are performed on binary images, hence we perform thresholding below\n    gray       = 255*(gray_img < 128).astype(np.uint8)\n    \n    # specify kernel size for morphological operations\n    O          = np.ones(2, dtype=np.uint8)\n    \n    # apply erosion followed by dilation to remove noise\n    gray_morph = cv2.morphologyEx(gray, cv2.MORPH_OPEN, O)\n    \n    # get coordinates where image is non-zero\n    coords     = cv2.findNonZero(gray_morph)\n    \n    # create a bounding box around those coordinatees\n    x, y, w, h = cv2.boundingRect(coords)\n    \n    # index into the bounding box and convert to RGB\n    rect       = gray_img[y:y+h, x:x+w]\n    rect       = cv2.cvtColor(rect, cv2.COLOR_BGR2RGB)\n    \n    return rect","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Read the Tokenizer\n> Import the string to index mapping for each InChI token, saved previously.","metadata":{}},{"cell_type":"code","source":"class Tokenizer(object):\n    \n    def __init__(self):\n        # string to integer mapping\n        self.stoi = {}\n        # integer to string mapping\n        self.itos = {}\n    \n    def __len__(self) -> None:\n        \n        \"\"\"\n        This method returns the length of token:index map.\n        \n        :return: length of map\n        :rtype: int\n        \"\"\"\n        # return the length of the map\n        return len(self.stoi)\n    \n    def fit_on_texts(self, texts: list) -> None:\n        \n        \"\"\"\n        This method creates a vocabulary of all tokens contained in provided texts,\n        and updates the mapping of token to index, and index to token.\n        \n        :param texts: list of texts\n        :type texts:  list\n        \"\"\"\n        \n        # create a storage for all tokens\n        vocab = set()\n        \n        # add tokens from each text to vocabulary\n        for text in texts:\n            vocab.update(text.split(' '))\n            \n        # sort the vocabulary in alphabetical order\n        vocab = sorted(vocab)\n        \n        # add start, end and pad for sentence\n        vocab.append('<sos>')\n        vocab.append('<eos>')\n        vocab.append('<pad>')\n        \n        # update the string to integer mapping, where integer is the index of the token\n        for i, s in enumerate(vocab):\n            self.stoi[s] = i\n        \n        # reverse the previous vocabulary to create integer to string mapping\n        self.itos = {item[1]: item[0] for item in self.stoi.items()}\n        \n    def text_to_sequence(self, text: str) -> list:\n        \n        \"\"\"\n        This method converts the given text to a list of its individual tokens,\n        including start and end of string symbols.\n        \n        :param text: input textual data\n        :type  text: str\n        :return:     list of tokens\n        :rtype:      list\n        \"\"\"\n        \n        # storage to append symbols to\n        sequence = []\n        \n        # add the start of string symbol to storage\n        sequence.append(self.stoi['<sos>'])\n        \n        # add each token in text to storage\n        for s in text.split(' '):\n            sequence.append(self.stoi[s])\n            \n        # add the end of string symbol to storage\n        sequence.append(self.stoi['<eos>'])\n        \n        return sequence\n    \n    def texts_to_sequences(self, texts: list) -> list:\n        \n        \"\"\"\n        This method converts each text in the provided list into sequences of characters.\n        Each sequence is appended to a list and the said list is returned.\n        \n        :param texts: a list of input texts\n        :type  texts: list\n        :return:      a list of sequences\n        :rtype:       list\n        \"\"\"\n        \n        # storage to append sequences to\n        sequences = []\n        \n        # for each text do\n        for text in texts:\n            # convert the text to a list of characters\n            sequence = self.text_to_sequence(text)\n            # append the lists of characters to an aggregated list storage\n            sequences.append(sequence)\n\n        return sequences\n    \n    def sequence_to_text(self, sequence: list) -> str:\n        \n        \"\"\"\n        This method converts the sequence of characters back into text.\n        \n        :param sequence: list of characters\n        :type  sequence: list\n        :return:         text\n        :rtype:          str \n        \"\"\"\n        # join the characters with no space in between\n        return ''.join(list(map(lambda i: self.itos[i], sequence)))\n    \n    def sequences_to_texts(self, sequences: list) -> list:\n        \n        \"\"\"\n        This method converts each provided sequence into text and returns all texts inside a list.\n        \n        :param sequences: list of character sequences\n        :type  sequences: list\n        :return:          list of texts\n        :rtype:           list\n        \"\"\"\n        \n        # storage for texts\n        texts = []\n        \n        # convert each sequence to text and append to storage\n        for sequence in sequences:\n            text = self.sequence_to_text(sequence)\n            texts.append(text)\n\n        return texts\n    \n    def predict_caption(self, sequence: list) -> str:\n        \n        \"\"\"\n        This method predicts the caption by adding each symbol in sequence to a resulting string.\n        This keeps happening up until the end of sentence or padding is met.\n        \n        :param sequence: list of characters\n        :type  sequence: list\n        :return:         image caption\n        :rtype:          string\n        \"\"\"\n        \n        # storage for the final caption\n        caption = ''\n        \n        # for each index in a sequence of symbols\n        for i in sequence:\n            # if symbol is the end of sentence or padding, break\n            if i == self.stoi['<eos>'] or i == self.stoi['<pad>']:\n                break\n            # otherwise, add the symbol to the final caption\n            caption += self.itos[i]\n            \n        return caption\n    \n    def predict_captions(self, sequences: list) -> list:\n        \n        \"\"\"\n        This method predicts the captions for each sequence in a list of sequences.\n        \n        :param sequences: list of sequences\n        :type  sequences: list\n        :return:          list of final image captions\n        :rtype:           list\n        \"\"\"\n        \n        # storage for captions\n        captions = []\n        \n        # for each sequence, do\n        for sequence in sequences:\n            \n            # predict the caption per sequence\n            caption = self.predict_caption(sequence)\n            \n            # append to the storage of captions\n            captions.append(caption)\n            \n        return captions\n\n# load the saved tokenizer and print its string to index mapping\ntokenizer = torch.load('../input/lstm-model/results-6/results-6/tokenizer.pth')\nprint(tokenizer.stoi)","metadata":{"execution":{"iopub.status.busy":"2022-02-28T05:29:21.433275Z","iopub.execute_input":"2022-02-28T05:29:21.43575Z","iopub.status.idle":"2022-02-28T05:29:21.4669Z","shell.execute_reply.started":"2022-02-28T05:29:21.435681Z","shell.execute_reply":"2022-02-28T05:29:21.466142Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Setup Configurations\n> Set configurations needed for modelling and training in a separate class.","metadata":{}},{"cell_type":"code","source":"class CFG:\n    \n    \"\"\"\n    Set configurations for modelling and training.\n    \"\"\"\n    \n    debug       = False\n    apex        = False\n    max_len     = 275\n    \n    print_freq  = 10000\n    num_workers = 4\n    model_name  = 'resnet34'\n    enc_size    = 512\n    samp_size   = 1000\n    \n    size        = 288\n    \n    scheduler   = 'CosineAnnealingLR'\n    epochs      = 90\n    T_max       = 4\n    \n    encoder_lr  = 1e-4\n    decoder_lr  = 4e-4\n    min_lr      = 1e-6\n    \n    batch_size   = 64\n    weight_decay = 1e-6\n    \n    gradient_accumulation_steps = 1\n    max_grad_norm               = 5\n    \n    attention_dim = 256\n    embed_dim     = 256\n    decoder_dim   = 512\n    dropout       = 0.5\n    seed          = 42\n    n_fold        = 2\n    trn_fold      = [0]\n    train         = True\n    \n    prev_model = '../input/lstm-model/efficientnet_b1_fold1_best.pth'","metadata":{"execution":{"iopub.status.busy":"2022-02-28T05:29:21.468737Z","iopub.execute_input":"2022-02-28T05:29:21.469225Z","iopub.status.idle":"2022-02-28T05:29:21.477096Z","shell.execute_reply.started":"2022-02-28T05:29:21.469182Z","shell.execute_reply":"2022-02-28T05:29:21.47627Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# if in debug mode\nif CFG.debug:\n    \n    # set number of epochs to 1\n    CFG.epochs = 1\n    \n    # reduce the train set to a 1000 examples\n    train_df   = train_df.sample(n=CFG.samp_size, random_state=CFG.seed).reset_index(drop=True)","metadata":{"execution":{"iopub.status.busy":"2022-02-28T05:29:21.478324Z","iopub.execute_input":"2022-02-28T05:29:21.479665Z","iopub.status.idle":"2022-02-28T05:29:21.488655Z","shell.execute_reply.started":"2022-02-28T05:29:21.479613Z","shell.execute_reply":"2022-02-28T05:29:21.487821Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Utilities\n> This is a set of utility functions used throughout the computations.","metadata":{}},{"cell_type":"code","source":"def get_score(y_true: str, y_pred: str) -> float:\n    \n    \"\"\"\n    This function computes the Levenstein distance between a true label and a prediction.\n    This gets computed for all the provided data and an average score is then returned.\n    \n    :param y_true: true InChI label\n    :type  y_true: str\n    :param y_pred: predicted InChI label\n    :type  y_pred: str\n    :return:       average Levenstein score\n    :rtype:        float\n    \"\"\"\n    \n    # storage for all Levenstein scores\n    scores = []\n    \n    # for each (true label, predicted label) pair, do\n    for true, pred in zip(y_true, y_pred):\n        \n        # find Levenstein distance for the pair and append to storage\n        score = Levenshtein.distance(true, pred)\n        scores.append(score)\n    \n    # compute mean Levenstein distance\n    avg_score = np.mean(scores)\n    \n    return avg_score","metadata":{"execution":{"iopub.status.busy":"2022-02-28T05:29:21.489542Z","iopub.execute_input":"2022-02-28T05:29:21.4898Z","iopub.status.idle":"2022-02-28T05:29:21.499623Z","shell.execute_reply.started":"2022-02-28T05:29:21.489763Z","shell.execute_reply":"2022-02-28T05:29:21.498818Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def init_logger(log_file: str ='./train.log'):\n    \n    \"\"\"\n    Initialize the logger file for training.\n    \n    :param log_file: name of the logger file\n    :type  log_file: str\n    :return:         logger\n    :rtype:          object\n    \"\"\"\n    \n    # make a reference to a logger instance\n    logger = getLogger(__name__)\n    \n    # specify lowest-severity log message to be handled by the logger\n    logger.setLevel(INFO)\n    \n    # send logging outputs to stream, i.e. sys.out, and format as message\n    handler1 = StreamHandler()\n    handler1.setFormatter(Formatter(\"%(message)s\"))\n    \n    # send logging outputs to a disk file and format as message\n    handler2 = FileHandler(filename=log_file)\n    handler2.setFormatter(Formatter(\"%(message)s\"))\n    \n    # add both handlers to the logger\n    logger.addHandler(handler1)\n    logger.addHandler(handler2)\n    \n    return logger\n\n# initialize the logger\nLOGGER = init_logger()","metadata":{"execution":{"iopub.status.busy":"2022-02-28T05:29:21.502511Z","iopub.execute_input":"2022-02-28T05:29:21.503078Z","iopub.status.idle":"2022-02-28T05:29:21.510749Z","shell.execute_reply.started":"2022-02-28T05:29:21.503038Z","shell.execute_reply":"2022-02-28T05:29:21.509576Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def seed_torch(seed: int=42) -> None:\n    \n    \"\"\"\n    Seed torch with a specific seed number to ensure code consistency across runs.\n    \n    :param seed: seed number\n    :type  seed: int\n    \"\"\"\n    \n    # set random seed\n    random.seed(seed)\n    \n    # set environment seed\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    \n    # set numpy seed\n    np.random.seed(seed)\n    \n    # set torch seeds\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    \n# seed torch with the seed from configs\nseed_torch(seed=CFG.seed)","metadata":{"execution":{"iopub.status.busy":"2022-02-28T05:29:21.512435Z","iopub.execute_input":"2022-02-28T05:29:21.512978Z","iopub.status.idle":"2022-02-28T05:29:21.522898Z","shell.execute_reply.started":"2022-02-28T05:29:21.512935Z","shell.execute_reply":"2022-02-28T05:29:21.522083Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Cross-Validation Split\n> Here cross validation splits are created for the train dataframe.","metadata":{}},{"cell_type":"code","source":"# create a copy of the train dataframe to modify\nfolds = train_df.copy()\n\n# provide train/validation indices to split data into train/validation sets\nFold  = StratifiedKFold(n_splits=CFG.n_fold, shuffle=True, random_state=CFG.seed)\n\n# for fold_number and (train_index, validation_index) in the splitted fold\nfor n, (train_idx, val_idx) in enumerate(Fold.split(folds, folds['InChI_length'])): # folds = separator, \n                                                                                    # len = maxsplit\n    # assign fold number\n    folds.loc[val_idx, 'fold'] = int(n)\n\n# convert fold number to integer\nfolds['fold'] = folds['fold'].astype(int)\n\n# print the size of each fold\nprint(folds.groupby(['fold']).size())","metadata":{"execution":{"iopub.status.busy":"2022-02-28T05:29:21.606278Z","iopub.execute_input":"2022-02-28T05:29:21.606547Z","iopub.status.idle":"2022-02-28T05:29:21.651003Z","shell.execute_reply.started":"2022-02-28T05:29:21.606509Z","shell.execute_reply":"2022-02-28T05:29:21.649772Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Dataset\n> Return the input data of the following format: (image, label tensor, label length). This is needed to ensure that the LSTM-Attention model is working as intended.","metadata":{}},{"cell_type":"code","source":"class TrainDataset(Dataset):\n    \n    \"\"\"\n    This class stores train dataset attributes and methods.\n    \"\"\"\n    \n    def __init__(self, df, tokenizer, transform=None):\n        \n        \"\"\"\n        Initialize train dataset attributes.\n        \n        :param df:        train dataframe\n        :type  df:        pd.DataFrame\n        :param tokenizer: string tokenizer\n        :type tokenizer:  object\n        :param transform: torch transformation\n        :type transform:  object\n        \"\"\"\n        \n        # inherit from parent class\n        super().__init__()\n        \n        # assign train dataframe\n        self.df = df\n        \n        # assign tokenizer\n        self.tokenizer = tokenizer\n        \n        # assign file paths\n        self.file_paths = df['file_path'].values\n        \n        # assign tokenized labels\n        self.labels     = df['InChI_text'].values\n        \n        # assign transformations\n        self.transform  = transform\n        \n    def __len__(self) -> int:\n        \n        \"\"\"\n        Return size of the train dataframe.\n        \n        :return: length of the dataframe\n        :rtype:  int\n        \"\"\"\n        return len(self.df)\n    \n    def __getitem__(self, idx: int) -> tuple:\n        \n        \"\"\"\n        Get the image, tensor of its label and label length at the inputted index.\n        \n        :param idx: index of dataframe\n        :type  idx: int\n        :return:    image, tensored label and label length\n        :rtype:     tuple \n        \"\"\"\n        \n        # get file path of the indexed item\n        file_path = self.file_paths[idx]\n        \n        # read in the image using the file path\n        image     = cv2.imread(file_path, cv2.IMREAD_GRAYSCALE)\n        image     = crop_image(image).astype(np.float32)\n        \n        # if transform is specified\n        if self.transform:\n            \n            # augment the image with the transform\n            augmented = self.transform(image=image)\n            \n            # update the image reference to point to the transformed image\n            image     = augmented['image']\n        \n        # get the label of the indexed item\n        label = self.labels[idx]\n        \n        # convert label to a sequence of symbols\n        label = self.tokenizer.text_to_sequence(label)\n        \n        # get the length of the label and convert it to Tensor\n        label_length = len(label)\n        label_length = torch.LongTensor([label_length])\n        \n        return image, torch.LongTensor(label), label_length","metadata":{"execution":{"iopub.status.busy":"2022-02-28T05:23:16.928987Z","iopub.status.idle":"2022-02-28T05:23:16.929484Z","shell.execute_reply.started":"2022-02-28T05:23:16.929263Z","shell.execute_reply":"2022-02-28T05:23:16.929288Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class TestDataset(Dataset):\n    \n    \"\"\"\n    This class stores test dataset attributes and methods.\n    \"\"\"\n    \n    def __init__(self, df, transform=None):\n        \n        \"\"\"\n        Initialize test dataset attributes.\n        \n        :param df:        test dataframe\n        :type  df:        pd.DataFrame\n        :param tokenizer: string tokenizer\n        :type tokenizer:  object\n        :param transform: torch transformation\n        :type transform:  object\n        \"\"\"\n        \n        # inherit from parent class\n        super().__init__()\n        \n        # assign train dataframe\n        self.df = df\n        \n        # assign file paths\n        self.file_paths = df['file_path'].values\n        \n        # assign transformations\n        self.transform  = transform\n        \n        # assign fixed transformations\n        self.fix_transform = Compose([Transpose(p=1), VerticalFlip(p=1)])\n        \n    def __len__(self) -> int:\n        \n        \"\"\"\n        Return size of the train dataframe.\n        \n        :return: length of the dataframe\n        :rtype:  int\n        \"\"\"\n        return len(self.df)\n    \n    def __getitem__(self, idx: int) -> np.array:\n        \n        \"\"\"\n        Get the image, tensor of its label and label length at the inputted index.\n        \n        :param idx: index of dataframe\n        :type  idx: int\n        :return:    image\n        :rtype:     array\n        \"\"\"\n        \n        # get file path of the indexed item\n        file_path = self.file_paths[idx]\n        \n        # read in the image using the file path\n        image     = cv2.imread(file_path, cv2.IMREAD_GRAYSCALE)\n        \n        # convert the image to RGB and float type\n        image     = crop_image(image).astype(np.float32)\n        \n        # get image shape\n        h, w, _ = image.shape\n        \n        # if height exceeds width, fix-transform the image\n        if h > w:\n            image = self.fix_transform(image=image)['image']\n        \n        # if transform is specified\n        if self.transform:\n            \n            # augment the image with the transform\n            augmented = self.transform(image=image)\n            \n            # update the image reference to point to the transformed image\n            image     = augmented['image']\n\n        \n        return image","metadata":{"execution":{"iopub.status.busy":"2022-02-28T05:23:16.932769Z","iopub.status.idle":"2022-02-28T05:23:16.933186Z","shell.execute_reply.started":"2022-02-28T05:23:16.932959Z","shell.execute_reply":"2022-02-28T05:23:16.932983Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def bms_collate(batch: tuple) -> tuple:\n    \n    \"\"\"\n    Combine images, labels and label lengths per batch.\n    \n    :param batch: a collection of data points\n    :type  batch: tuple\n    :return:      stacked images, labels and label lengths, i.e. batch\n    :rtype:       tuple\n    \"\"\"\n    \n    # initialize storages for images, labels and label lengths\n    imgs, labels, label_lengths = [], [], []\n    \n    # for each data point, append image, labels and label lengths to respective storages\n    for data_point in batch:\n        imgs.append(data_point[0])\n        labels.append(data_point[1])\n        label_lengths.append(data_point[2])\n    \n    # pad each label sequence with the <pad> index value\n    labels = pad_sequence(labels, batch_first=True, padding_value=tokenizer.stoi[\"<pad>\"])\n \n    return torch.stack(imgs), labels, torch.stack(label_lengths).reshape(-1, 1)","metadata":{"execution":{"iopub.status.busy":"2022-02-28T05:23:16.934507Z","iopub.status.idle":"2022-02-28T05:23:16.934883Z","shell.execute_reply.started":"2022-02-28T05:23:16.934673Z","shell.execute_reply":"2022-02-28T05:23:16.934693Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Transformations\n> Define basic torch transforms for the dataset, including resizing, normalizing and tensoring.","metadata":{}},{"cell_type":"code","source":"def get_transforms(*, data):\n    \n    \"\"\"\n    Compose several transforms together, mainly resizing, normalizing with Imagenet weights and tensoring.\n    \n    :param data: image data\n    :type  data: np.array\n    :return:     transformed image data\n    :rtype:      np.array\n    \"\"\"\n    if data == 'train':\n        return Compose(\n            [\n                Resize(CFG.size, CFG.size),\n                Normalize(\n                    mean=[0.485, 0.456, 0.406],\n                    std=[0.229, 0.224, 0.225],\n                ),\n                ToTensorV2(),\n            ]\n        )\n    \n    elif data == 'valid':\n        return Compose([\n            Resize(CFG.size, CFG.size),\n            Normalize(\n                mean=[0.485, 0.456, 0.406],\n                std=[0.229, 0.224, 0.225],\n            ),\n            ToTensorV2(),\n        ])","metadata":{"execution":{"iopub.status.busy":"2022-02-28T05:23:16.936098Z","iopub.status.idle":"2022-02-28T05:23:16.936509Z","shell.execute_reply.started":"2022-02-28T05:23:16.936278Z","shell.execute_reply":"2022-02-28T05:23:16.936301Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# define train dataset from the train dataframe, using the tokenizer and transformations\ntrain_ds = TrainDataset(train_df, tokenizer, transform=get_transforms(data='train'))","metadata":{"execution":{"iopub.status.busy":"2022-02-28T05:23:16.937589Z","iopub.status.idle":"2022-02-28T05:23:16.938476Z","shell.execute_reply.started":"2022-02-28T05:23:16.938248Z","shell.execute_reply":"2022-02-28T05:23:16.938272Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# display an example from the train dataset\nfor i in range(1):\n    \n    # get image, label and label length from the train dataset\n    image, label, label_length = train_ds[i]\n    # convert label sequence to text\n    text = tokenizer.sequence_to_text(label.numpy())\n    \n    # transpose the image and show it\n    plt.imshow(image.transpose(0,1).transpose(1,2))\n    plt.title(f'label: {label} text: {text} label_length: {label_length}')\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-28T05:23:16.939343Z","iopub.status.idle":"2022-02-28T05:23:16.940997Z","shell.execute_reply.started":"2022-02-28T05:23:16.940766Z","shell.execute_reply":"2022-02-28T05:23:16.940793Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Modeling\n> Create encoder, attention and decoder classes for modeling.","metadata":{}},{"cell_type":"code","source":"class Encoder(nn.Module):\n    \n    \"\"\"\n    Encodes the image with 3 color channels into a smaller learned image.\n    \"\"\"\n    \n    def __init__(self, model_name=CFG.model_name, pretrained=False):\n        \n        \"\"\"\n        Initialize the encoder with CNN equal to the chosen model and set pretrained parameter.\n        \n        :param model_name: name of the model\n        :type  model_name: str\n        :param pretrained: pretrained weights or not\n        :type  pretrained: Boolean\n        \"\"\"\n        \n        # inherit attributes and methods from parent class\n        super().__init__()\n        \n        self.cnn = timm.create_model(model_name, pretrained=pretrained)\n        self.n_features = self.cnn.fc.in_features\n        \n        # replacing the last two layers with no-op\n        self.cnn.global_pool = nn.Identity()\n        self.cnn.fc = nn.Identity()\n        \n    def forward(self, x):\n        \n        \"\"\"\n        Propagate the input forward.\n        \n        :param x: image data\n        :type  x: np.array\n        :return:  image features\n        :rtype:   np.array \n        \"\"\"\n        \n        # get image features from the CNN\n        features = self.cnn(x)\n        \n        # re-arrange the dimensions so that (bs, encoded_image_size, encoded_image_size, n_channels=2048)\n        features = features.permute(0, 2, 3, 1)\n        \n        return features","metadata":{"execution":{"iopub.status.busy":"2022-02-28T05:23:16.942227Z","iopub.status.idle":"2022-02-28T05:23:16.942674Z","shell.execute_reply.started":"2022-02-28T05:23:16.942433Z","shell.execute_reply":"2022-02-28T05:23:16.942455Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Attention(nn.Module):\n    \n    \"\"\"\n    Define Attention network to calculate the attention value.\n    \"\"\"\n    \n    def __init__(self, encoder_dim, decoder_dim, attention_dim):\n        \n        \"\"\"\n        Initialize the Attention network.\n        \n        :param encoder_dim:   input size of the encoder network\n        :type  encoder_dim:   int\n        :param decoder_dim:   input size of the decoder network\n        :type  decoder_dim:   int\n        :param attention_dim: input size of the attention network\n        :type  attention_dim: int\n        \"\"\"\n        \n        # initiliaze and inherit from parent class\n        super(Attention, self).__init__()\n        \n        # 1st linear layer to transform the encoded image\n        self.encoder_att = nn.Linear(encoder_dim, attention_dim)\n        \n        # 1st linear layer to transform the decoder's output\n        self.decoder_att = nn.Linear(decoder_dim, attention_dim)\n        \n        # linear layer to calculate values to be softmax-ed\n        self.full_att    = nn.Linear(attention_dim, 1)\n        \n        # define ReLU function\n        self.relu        = nn.ReLU()\n        \n        # define the softmax layer to calculate weights\n        self.softmax     = nn.Softmax(dim=1)\n        \n    def forward(self, encoder_out, decoder_hidden):\n        \n        \"\"\"\n        Propagate the inputs forward.\n        \n        :param encoder_out: encoded images of dimension (batch_size, num_pixels, encoder_dim)\n        :type  encoder_out: tensor\n        :param decoder_hidden: previous decoder output of dimension (batch_size, decoder_dim)\n        :type  decoder_hidden: tensor\n        :return: attention-weighted encoding, weights\n        :rtype:  tensor of (batch_size, encoder_dim)\n        \"\"\"\n        \n        # apply the linear layer to encoded images to get (bs, num_pixels, attention_dim)\n        att1 = self.encoder_att(encoder_out)\n        \n        # apply the linear layer to transform the decoder's output to get (bs, attention_dim)\n        att2 = self.decoder_att(decoder_hidden)\n        \n        # pass the sum of transformed encoded images and decoder outputs through a ReLU and apply a linear layer\n        # gets (bs, num_pixels)\n        att  = self.full_att(self.relu(att1 + att2.unsqueeze(1))).squeeze(2)\n        \n        # pass the results through a softmax layer to get weights\n        alpha = self.softmax(att)\n        \n        # apply the resulting (bs, num_pixels) weights to the encoder output and sum across dim=1\n        attention_weighted_encoding = (encoder_out * alpha.unsqueeze(2)).sum(dim=1)\n        \n        return attention_weighted_encoding, alpha","metadata":{"execution":{"iopub.status.busy":"2022-02-28T05:23:16.944266Z","iopub.status.idle":"2022-02-28T05:23:16.944687Z","shell.execute_reply.started":"2022-02-28T05:23:16.944468Z","shell.execute_reply":"2022-02-28T05:23:16.944491Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class DecoderWithAttention(nn.Module):\n    \n    \"\"\"\n    Decoder network with attention network used for training.\n    \"\"\"\n    \n    def __init__(self, attention_dim, embed_dim, decoder_dim, vocab_size, device, encoder_dim=CFG.enc_size, dropout=CFG.dropout):\n        \n        \"\"\"\n        Initialize the decoder with attention network.\n        \n        :param attention_dim: input size of the attention network\n        :type  attention_dim: int\n        :param embed_dim:     input size of the embedding network\n        :type  embed_dim:     int\n        :param decoder_dim:   input size of the decoder network\n        :type  decoder_dim:   int\n        :param vocab_size:    total number of characters used in training\n        :type  vocab_size:    int\n        :param encoder_dim:   input size of the encoder network\n        :type  encoder_dim:   int\n        :param dropout:       dropout rate\n        :type  dropout:       float\n        \"\"\"\n        \n        # inherit from parent class\n        super(DecoderWithAttention, self).__init__()\n        \n        # set dimensions of the encoder, attention, embedder and decoder\n        self.encoder_dim   = encoder_dim\n        self.attention_dim = attention_dim\n        self.embed_dim     = embed_dim\n        self.decoder_dim   = decoder_dim\n        \n        # set vocabulary size, dropout rate and the used device\n        self.vocab_size    = vocab_size\n        self.dropout       = dropout\n        self.device        = device\n        \n        # set attention network, embedding network and dropout layer\n        self.attention     = Attention(encoder_dim, decoder_dim, attention_dim)\n        self.embedding     = nn.Embedding(vocab_size, embed_dim)\n        self.dropout       = nn.Dropout(p=self.dropout)\n        \n        # set the LSTM Cell decoder\n        self.decode_step   = nn.LSTMCell(embed_dim+encoder_dim, decoder_dim, bias=True)\n        \n        # linear layer to find the initial hidden state of LSTM Cell\n        self.init_h        = nn.Linear(encoder_dim, decoder_dim)\n        \n        # linear layer to find the initial cell state of LSTM Cell\n        self.init_c        = nn.Linear(encoder_dim, decoder_dim)\n        \n        # linear layer to create a sigmoid-activated gate\n        self.f_beta        = nn.Linear(decoder_dim, encoder_dim)\n        self.sigmoid       = nn.Sigmoid()\n        \n        # linear layer to find scores over vocabulary\n        self.fc            = nn.Linear(decoder_dim, vocab_size)\n        \n        # initialize some layers with uniform distribution\n        self.init_weights()\n        \n    def init_weights(self):\n        \n        \"\"\"\n        Initialize weights with uniform distribution for embedding and FC layers.\n        \"\"\"\n        \n        self.embedding.weight.data.uniform_(-0.1, 0.1)\n        self.fc.bias.data.fill_(0)\n        self.fc.weight.data.uniform_(-0.1, 0.1)\n        \n    def load_pretrained_embeddings(self, embeddings):\n        \n        \"\"\"\n        Load the embedding layer with pre-trained embeddings.\n        \n        :param embeddings: pre-trained embeddings\n        \"\"\"\n        \n        self.embedding.weight = nn.Parameter(embeddings)\n        \n    def fine_tune_embeddings(self, fine_tune=True):\n        \n        \"\"\"\n        Allow fine-tuning of the embedding layer.\n        \n        :param fine_tune: allow fine-tuning\n        :type  fine_tune: Boolean\n        \"\"\"\n        # loop over each embedding parameter to set the fine-tuning option\n        for p in self.embedding.parameters():\n            p.requires_grad = fine_tune\n            \n    def init_hidden_state(self, encoder_out):\n        \n        \"\"\"\n        Create the initial hidden and cell state for decoder's LSTM based on encoded images.\n        \n        :param encoder_out: encoded images, of size (bs, num_pixels, encoder_dim)\n        :type  encoder_out: tensor\n        :return:            initial hidden and cell states\n        :rtype:             tuple\n        \n        \"\"\"\n        \n        # get the mean of the encoded image's dim=1\n        mean_encoder_out = encoder_out.mean(dim=1)\n        \n        # initialize the hidden and cell states\n        h                = self.init_h(mean_encoder_out)\n        c                = self.init_c(mean_encoder_out)\n        \n        return h, c\n    \n    def forward(self, encoder_out, encoded_captions, caption_lengths):\n        \n        \"\"\"\n        Propagate the inputs forward.\n        \n        :param encoder_out: encoded images of dimension (bs, num_pixels, encoder_dim)\n        :type  encoder_out: tensor\n        :param encoded_captions: encoded captions of dimension (bs, max_caption_length)\n        :type  encoded_captions: tensor\n        :param caption_lengths: caption lengths of dimension (bs, 1)\n        :type  caption_lengths: tensor\n        :return: scores for vocabulary, sorted encoded captions, decode lengths, weights, sort indices\n        :rtype:  tuple\n        \"\"\"\n        batch_size  = encoder_out.size(0)\n        encoder_dim = encoder_out.size(-1)\n        vocab_size  = self.vocab_size\n        \n        # flatten the image\n        encoder_out = encoder_out.view(batch_size, -1, encoder_dim)\n        num_pixels  = encoder_out.size(1)\n        \n        # sort input data by decreasing lengths\n        caption_lengths, sort_ind = caption_lengths.squeeze(1).sort(dim=0, descending=True)\n        encoder_out = encoder_out[sort_ind]\n        encoded_captions = encoded_captions[sort_ind]\n        \n        # embed the encoded captions\n        embeddings = self.embedding(encoded_captions)\n        \n        # initialize LSTM state\n        h, c = self.init_hidden_state(encoder_out)\n        \n        # decoding lengths are actual lengths - 1 because of <end>\n        decode_lengths = (caption_lengths - 1).tolist()\n        \n        # initialize tensors to hold word predictions scores and alphas (weights)\n        predictions    = torch.zeros(batch_size, max(decode_lengths), vocab_size).to(self.device)\n        alphas         = torch.zeros(batch_size, max(decode_lengths), num_pixels).to(self.device)\n        \n        # at each time step, decode by \n        for t in range(max(decode_lengths)):\n            batch_size_t = sum([l>t for l in decode_lengths])\n            \n            # attention weighted encoder's output based on decoder's previous state output\n            attention_weighted_encoding, alpha = self.attention(encoder_out[:batch_size_t], h[:batch_size_t])\n            gate = self.sigmoid(self.f_beta(h[:batch_size_t]))\n            attention_weighted_encoding = gate * attention_weighted_encoding\n            \n            # generate a new word with previous word and attention weighted encoding\n            h, c = self.decode_step(\n                torch.cat([embeddings[:batch_size_t, t, :], attention_weighted_encoding], dim=1),\n                (h[:batch_size_t], c[:batch_size_t])\n            )\n            preds = self.fc(self.dropout(h))\n            predictions[:batch_size_t, t, :] = preds\n            alphas[:batch_size_t, t, :] = alpha\n            \n        return predictions, encoded_captions, decode_lengths, alphas, sort_ind\n    \n    def predict(self, encoder_out, decode_lengths, tokenizer):\n        \n        \"\"\"\n        Predict captions.\n        \n        :param encoder_out: encoded images of dimension (bs, num_pixels, encoder_dim)\n        :type  encoder_out: tensor\n        :param decode_lengths: lengths of decoded captions\n        :type  decode_lengths: tensor\n        :param tokenizer: word tokenizer\n        :type  tokenizer: dict\n        :return: predictions\n        :rtype:  str\n        \"\"\"\n        batch_size = encoder_out.size(0)\n        encoder_dim = encoder_out.size(-1)\n        vocab_size = self.vocab_size\n        \n        # flatten the image\n        encoder_out = encoder_out.view(batch_size, -1, encoder_dim)\n        num_pixels = encoder_out.size(1)\n        \n        # define start tokens and embed them\n        start_tokens = torch.ones(batch_size, dtype=torch.long).to(self.device) * tokenizer.stoi[\"<sos>\"]\n        embeddings = self.embedding(start_tokens)\n        \n        # initialize LSTM state\n        h, c = self.init_hidden_state(encoder_out)  # (batch_size, decoder_dim)\n        \n        # initialize tensors to hold predictions and final conditions\n        predictions = torch.zeros(batch_size, decode_lengths, vocab_size).to(self.device)\n        end_condition = torch.zeros(batch_size, dtype=torch.long).to(encoder_out.device)\n        \n        # at each time step decode by\n        for t in range(decode_lengths):\n            \n            # applying attention to encoded image and decoder's previous state output\n            attention_weighted_encoding, alpha = self.attention(encoder_out, h)\n            gate = self.sigmoid(self.f_beta(h))  # gating scalar, (batch_size_t, encoder_dim)\n            attention_weighted_encoding = gate * attention_weighted_encoding\n            \n            # generate a new word with previous word and attention weighted encoding\n            h, c = self.decode_step(\n                torch.cat([embeddings, attention_weighted_encoding], dim=1),\n                (h, c))  # (batch_size_t, decoder_dim)\n            preds = self.fc(self.dropout(h))  # (batch_size_t, vocab_size)\n            predictions[:, t, :] = preds\n            \n            # if end of sentence is reached, stop predicting\n            end_condition |= (torch.argmax(preds, -1) == tokenizer.stoi[\"<eos>\"])\n            if end_condition.sum() == batch_size:\n                break\n                \n            # embed the predictions\n            embeddings = self.embedding(torch.argmax(preds, -1))\n            \n        return predictions","metadata":{"execution":{"iopub.status.busy":"2022-02-28T05:23:16.94615Z","iopub.status.idle":"2022-02-28T05:23:16.946565Z","shell.execute_reply.started":"2022-02-28T05:23:16.946336Z","shell.execute_reply":"2022-02-28T05:23:16.946367Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Helper Functions\n> This is a set of functions used as utilities.","metadata":{}},{"cell_type":"code","source":"class AverageMeter(object):\n    \n    \"\"\"\n    Computes and stores average and current values.\n    \"\"\"\n    \n    def __init__(self):\n        \n        \"\"\"\n        Reset settings.\n        \"\"\"\n        self.reset()\n    \n    def reset(self):\n        \n        \"\"\"\n        Set current, average values, sum and count values to zero.\n        \"\"\"\n        \n        self.val   = 0\n        self.avg   = 0\n        self.sum   = 0\n        self.count = 0\n        \n    def update(self, val, n=1):\n        \n        \"\"\"\n        Update current value, sum, count and average value.\n        \"\"\"\n        \n        self.val    = val\n        self.sum   += val*n\n        self.count += n\n        self.avg = self.sum / self.count","metadata":{"execution":{"iopub.status.busy":"2022-02-28T05:23:16.947922Z","iopub.status.idle":"2022-02-28T05:23:16.948322Z","shell.execute_reply.started":"2022-02-28T05:23:16.948104Z","shell.execute_reply":"2022-02-28T05:23:16.948126Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def asMinutes(s):\n    \n    \"\"\"\n    Convert seconds to minutes.\n    \n    :param s: seconds\n    :type  s: float\n    \"\"\"\n    m  = math.floor(s/60)\n    s -= m * 60\n    return '%d %ds'% (m, s)","metadata":{"execution":{"iopub.status.busy":"2022-02-28T05:23:16.949679Z","iopub.status.idle":"2022-02-28T05:23:16.9501Z","shell.execute_reply.started":"2022-02-28T05:23:16.949884Z","shell.execute_reply":"2022-02-28T05:23:16.949906Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def timeSince(since, percent):\n    \n    \"\"\"\n    Calculate time since.\n    \n    :param since: previous date\n    :type  since: time\n    \"\"\"\n    \n    now = time.time()\n    s   = now - since\n    es  = s / (percent)\n    rs  = es - s\n    \n    return '%s (remain %s)' % (asMinutes(s), asMinutes(rs))","metadata":{"execution":{"iopub.status.busy":"2022-02-28T05:23:16.951306Z","iopub.status.idle":"2022-02-28T05:23:16.952027Z","shell.execute_reply.started":"2022-02-28T05:23:16.95179Z","shell.execute_reply":"2022-02-28T05:23:16.951815Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_fn(\n    train_loader, encoder, decoder, \n    criterion, encoder_optimizer, decoder_optimizer, \n    epoch, encoder_scheduler, decoder_scheduler, device\n):\n    \"\"\"\n    Perform one epoch training.\n    \n    :param train_loader: data loader for training data\n    :type  train_loader: DataLoader\n    :param encoder:      encoder model\n    :type  encoder:      Encoder\n    :param decoder:      decoder model\n    :type  decoder:      Decoder\n    :param criterion:    loss layer\n    :type criterion:     Loss \n    :param encoder_optimizer: optimizer for encoder\n    :type  encoder_optimizer: Optimizer\n    :param decoder_optimizer: optimizer for decoder\n    :type  decoder_optimizer: Optimizer\n    :param epoch:             Epoch number\n    :type  epoch:             int\n    :param encoder_scheduler: Encoder scheduler\n    :type  encoder_scheduler: Encoder\n    :param decoder_scheduler: Decoder scheduler\n    :type  decoder_scheduler: Decoder\n    :param device:            device selection\n    :type  device:            Device\n    :return:                  Average loss\n    :rtype:                   float\n    \"\"\"\n    \n    batch_time = AverageMeter()\n    data_time  = AverageMeter()\n    losses     = AverageMeter()\n    \n    # switch to train mode\n    encoder.train()\n    decoder.train()\n    start = end = time.time()\n    global_step = 0\n    \n    # for index and inputs (imgs, labels and label lengths) in train dataset, do\n    for step, (images, labels, label_lengths) in enumerate(train_loader):\n        \n        # update time step\n        data_time.update(time.time() - end)\n        \n        # send inputs to device\n        images = images.to(device)\n        labels = labels.to(device)\n        label_lengths = label_lengths.to(device)\n        \n        # set batch size\n        batch_size = images.size(0)\n        \n        # encode images to get features\n        features = encoder(images)\n        \n        # return scores for vocabulary, sorted encoded captions, decode lengths, weights, sort indices\n        predictions, caps_sorted, decode_lengths, alphas, sort_ind = decoder(features, labels, label_lengths)\n        \n        # specify true targets (true y's) after <start>\n        targets = caps_sorted[:, 1:]\n        \n        # get predictions in a packed sequence\n        predictions = pack_padded_sequence(predictions, decode_lengths, batch_first=True).data\n        \n        # get targets in a packed sequence\n        targets = pack_padded_sequence(targets, decode_lengths, batch_first=True).data\n        \n        # compute loss between predictions and targets\n        loss = criterion(predictions, targets)\n        \n        # record the loss\n        losses.update(loss.item(), batch_size)\n        \n        # if optimizer steps are more than 1, divide loss by the number of those steps\n        if CFG.gradient_accumulation_steps > 1:\n            loss = loss / CFG.gradient_accumulation_steps\n            \n        if CFG.apex:\n            with amp.scale_loss(loss, decoder_optimizer) as scaled_loss:\n                scaled_loss.backward()\n        else:\n            # backward propagation of loss\n            loss.backward()\n        \n        # perform gradient clipping to avoid exploding gradients\n        encoder_grad_norm = torch.nn.utils.clip_grad_norm_(encoder.parameters(), CFG.max_grad_norm)\n        decoder_grad_norm = torch.nn.utils.clip_grad_norm_(decoder.parameters(), CFG.max_grad_norm)\n        \n        # update weights\n        if (step + 1) % CFG.gradient_accumulation_steps == 0:\n            \n            # take a step based on gradients\n            encoder_optimizer.step()\n            decoder_optimizer.step()\n            \n            # clear all gradients from last step\n            encoder_optimizer.zero_grad()\n            decoder_optimizer.zero_grad()\n            global_step += 1\n            \n        # measure elapsed time\n        batch_time.update(time.time() - end)\n        end = time.time()\n        \n        # print out the results per epoch\n        if step % CFG.print_freq == 0 or step == (len(train_loader)-1):\n            print('Epoch: [{0}][{1}/{2}] '\n                  'Data {data_time.val:.3f} ({data_time.avg:.3f}) '\n                  'Elapsed {remain:s} '\n                  'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n                  'Encoder Grad: {encoder_grad_norm:.4f}  '\n                  'Decoder Grad: {decoder_grad_norm:.4f}  '\n                  .format(\n                   epoch+1, step, len(train_loader), batch_time=batch_time,\n                   data_time=data_time, loss=losses,\n                   remain=timeSince(start, float(step+1)/len(train_loader)),\n                   encoder_grad_norm=encoder_grad_norm,\n                   decoder_grad_norm=decoder_grad_norm,\n                   ))\n            \n    return losses.avg\n\n\ndef valid_fn(valid_loader, encoder, decoder, tokenizer, criterion, device):\n    \n    \"\"\"\n    Predict for validation set.\n    \n    :param valid_loader: Data Loader for validation data\n    :type  valid_loader: DataLoader\n    :param encoder:      Encoder\n    :type  encoder:      Encoder\n    :param decoder:      Decoder\n    :type  decoder:      Decoder\n    :param tokenizer:    Tokenizer\n    :type  tokenizer:    Tokenizer\n    :param criterion:    Loss\n    :type  criterion:    Loss \n    :param device:       device selection\n    :type  device:       Device\n    :return:             predictions\n    :rtype:              list\n    \"\"\"\n    \n    batch_time = AverageMeter()\n    data_time = AverageMeter()\n    \n    # switch to evaluation mode\n    encoder.eval()\n    decoder.eval()\n    \n    # store predictions here\n    text_preds = []\n    \n    start = end = time.time()\n    \n    # for each image in validation set\n    for step, (images) in enumerate(valid_loader):\n        \n        # measure data loading time\n        data_time.update(time.time() - end)\n        \n        # send images to device\n        images = images.to(device)\n        \n        # specify batch size\n        batch_size = images.size(0)\n        \n        # disable gradient calculation to avoid CUDA errors\n        with torch.no_grad():\n            \n            # encode images\n            features = encoder(images)\n            \n            # predict sequence using decoder\n            predictions = decoder.predict(features, CFG.max_len, tokenizer)\n            \n        # choose the best predicted sequence\n        predicted_sequence = torch.argmax(predictions.detach().cpu(), -1).numpy()\n        \n        # predict captions from the predicted sequence and append to storage\n        _text_preds = tokenizer.predict_captions(predicted_sequence)\n        text_preds.append(_text_preds)\n        \n        # measure elapsed time\n        batch_time.update(time.time() - end)\n        end = time.time()\n        \n        # print out the validation results\n        if step % CFG.print_freq == 0 or step == (len(valid_loader)-1):\n            print('EVAL: [{0}/{1}] '\n                  'Data {data_time.val:.3f} ({data_time.avg:.3f}) '\n                  'Elapsed {remain:s} '\n                  .format(\n                   step, len(valid_loader), batch_time=batch_time,\n                   data_time=data_time,\n                   remain=timeSince(start, float(step+1)/len(valid_loader)),\n                   ))\n            \n    # concatenate preds into one string\n    text_preds = np.concatenate(text_preds)\n    \n    return text_preds","metadata":{"execution":{"iopub.status.busy":"2022-02-28T05:23:16.955281Z","iopub.status.idle":"2022-02-28T05:23:16.955699Z","shell.execute_reply.started":"2022-02-28T05:23:16.95548Z","shell.execute_reply":"2022-02-28T05:23:16.955502Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train Loop\n> Performs training.","metadata":{}},{"cell_type":"code","source":"def train_loop(folds, fold):\n    \n    \"\"\"\n    Perform training in a loop.\n    \"\"\"\n\n    LOGGER.info(f\"========== fold: {fold} training ==========\")\n\n    # ====================================================\n    # loader\n    # ====================================================\n    trn_idx = folds[folds['fold'] != fold].index\n    val_idx = folds[folds['fold'] == fold].index\n\n    train_folds = folds.loc[trn_idx].reset_index(drop=True)\n    valid_folds = folds.loc[val_idx].reset_index(drop=True)\n    valid_labels = valid_folds['InChI'].values\n\n    train_dataset = TrainDataset(train_folds, tokenizer, transform=get_transforms(data='train'))\n    valid_dataset = TestDataset(valid_folds, transform=get_transforms(data='valid'))\n\n    train_loader = DataLoader(train_dataset, \n                              batch_size=CFG.batch_size, \n                              shuffle=True, \n                              num_workers=CFG.num_workers, \n                              pin_memory=True,\n                              drop_last=True, \n                              collate_fn=bms_collate)\n    \n    valid_loader = DataLoader(valid_dataset, \n                              batch_size=CFG.batch_size, \n                              shuffle=False, \n                              num_workers=CFG.num_workers,\n                              pin_memory=True, \n                              drop_last=False)\n    \n    # ====================================================\n    # scheduler \n    # ====================================================\n    def get_scheduler(optimizer):\n        if CFG.scheduler=='ReduceLROnPlateau':\n            scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=CFG.factor, patience=CFG.patience, verbose=True, eps=CFG.eps)\n        elif CFG.scheduler=='CosineAnnealingLR':\n            scheduler = CosineAnnealingLR(optimizer, T_max=CFG.T_max, eta_min=CFG.min_lr, last_epoch=-1)\n        elif CFG.scheduler=='CosineAnnealingWarmRestarts':\n            scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=CFG.T_0, T_mult=1, eta_min=CFG.min_lr, last_epoch=-1)\n        return scheduler\n\n    # ====================================================\n    # model & optimizer\n    # ====================================================\n#     states = torch.load(CFG.prev_model, map_location=torch.device('cpu'))\n    encoder = Encoder(CFG.model_name, pretrained=True)\n#     encoder.load_state_dict(states['encoder'])\n    encoder.to(device)\n    encoder_optimizer = Adam(encoder.parameters(), lr=CFG.encoder_lr, weight_decay=CFG.weight_decay, amsgrad=False)\n#     encoder_optimizer.load_state_dict(states['encoder_optimizer'])\n    encoder_scheduler = get_scheduler(encoder_optimizer)\n#     encoder_scheduler.load_state_dict(states['encoder_scheduler'])\n    \n    decoder = DecoderWithAttention(attention_dim=CFG.attention_dim,\n                                   embed_dim=CFG.embed_dim,\n                                   decoder_dim=CFG.decoder_dim,\n                                   vocab_size=len(tokenizer),\n                                   dropout=CFG.dropout,\n                                   device=device)\n    \n#     decoder.load_state_dict(states['decoder'])\n    decoder.to(device)\n    decoder_optimizer = Adam(decoder.parameters(), lr=CFG.decoder_lr, weight_decay=CFG.weight_decay, amsgrad=False)\n#     decoder_optimizer.load_state_dict(states['decoder_optimizer'])\n    \n    decoder_scheduler = get_scheduler(decoder_optimizer)\n#     decoder_scheduler.load_state_dict(states['decoder_scheduler'])\n    \n    # ====================================================\n    # loop\n    # ====================================================\n    criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.stoi[\"<pad>\"])\n\n    best_score = np.inf\n    best_loss = np.inf\n    \n    for epoch in range(CFG.epochs):\n        \n        start_time = time.time()\n        \n        # train\n        avg_loss = train_fn(train_loader, encoder, decoder, criterion, \n                            encoder_optimizer, decoder_optimizer, epoch, \n                            encoder_scheduler, decoder_scheduler, device)\n\n        # eval\n        text_preds = valid_fn(valid_loader, encoder, decoder, tokenizer, criterion, device)\n        text_preds = [f\"InChI=1S/{text}\" for text in text_preds]\n        LOGGER.info(f\"labels: {valid_labels[:5]}\")\n        LOGGER.info(f\"preds: {text_preds[:5]}\")\n        \n        # scoring\n        score = get_score(valid_labels, text_preds)\n        \n        if isinstance(encoder_scheduler, ReduceLROnPlateau):\n            encoder_scheduler.step(score)\n        elif isinstance(encoder_scheduler, CosineAnnealingLR):\n            encoder_scheduler.step()\n        elif isinstance(encoder_scheduler, CosineAnnealingWarmRestarts):\n            encoder_scheduler.step()\n            \n        if isinstance(decoder_scheduler, ReduceLROnPlateau):\n            decoder_scheduler.step(score)\n        elif isinstance(decoder_scheduler, CosineAnnealingLR):\n            decoder_scheduler.step()\n        elif isinstance(decoder_scheduler, CosineAnnealingWarmRestarts):\n            decoder_scheduler.step()\n\n        elapsed = time.time() - start_time\n\n        LOGGER.info(f'Epoch {epoch+1} - avg_train_loss: {avg_loss:.4f}  time: {elapsed:.0f}s')\n        LOGGER.info(f'Epoch {epoch+1} - Score: {score:.4f}')\n        \n        if score < best_score:\n            best_score = score\n            LOGGER.info(f'Epoch {epoch+1} - Save Best Score: {best_score:.4f} Model')\n            torch.save({'encoder': encoder.state_dict(), \n                        'encoder_optimizer': encoder_optimizer.state_dict(), \n                        'encoder_scheduler': encoder_scheduler.state_dict(), \n                        'decoder': decoder.state_dict(), \n                        'decoder_optimizer': decoder_optimizer.state_dict(), \n                        'decoder_scheduler': decoder_scheduler.state_dict(), \n                        'text_preds': text_preds,\n                       },\n                        './'+f'{CFG.model_name}_fold{fold}_best.pth')","metadata":{"execution":{"iopub.status.busy":"2022-02-28T05:23:16.957109Z","iopub.status.idle":"2022-02-28T05:23:16.957521Z","shell.execute_reply.started":"2022-02-28T05:23:16.957292Z","shell.execute_reply":"2022-02-28T05:23:16.957314Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Main Program - Train","metadata":{}},{"cell_type":"code","source":"def main():\n\n    \"\"\"\n    Prepare: 1.train  2.folds\n    \"\"\"\n\n    if CFG.train:\n        # train\n        for fold in range(CFG.n_fold):\n            if fold in CFG.trn_fold:\n                print(fold)\n                train_loop(folds, fold)","metadata":{"execution":{"iopub.status.busy":"2022-02-28T05:23:16.958656Z","iopub.status.idle":"2022-02-28T05:23:16.959732Z","shell.execute_reply.started":"2022-02-28T05:23:16.959462Z","shell.execute_reply":"2022-02-28T05:23:16.959488Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if __name__ == '__main__':\n    main()","metadata":{"execution":{"iopub.status.busy":"2022-02-28T05:23:16.961779Z","iopub.status.idle":"2022-02-28T05:23:16.96219Z","shell.execute_reply.started":"2022-02-28T05:23:16.961958Z","shell.execute_reply":"2022-02-28T05:23:16.961983Z"},"trusted":true},"execution_count":null,"outputs":[]}]}