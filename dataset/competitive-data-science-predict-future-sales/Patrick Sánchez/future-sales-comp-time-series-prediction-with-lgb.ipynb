{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Start with Kaggle comps: Future sales\n\nThe aim of this notebook is to predict monthly sales of a series of products from the C1 company. This includes working with time-series and managing considerably large datasets, and we will need some advanced techniques to deal with them.\n\nMain workflow of the algorithm:\n1. Step 1. Load data\n2. Step 2. Data exploration (EDA)\n3. Step 3. Missings cleaning\n4. Step 4. Feature engineering\n5. Step 5. Mean encoding and generation of lag\n6. Step 6. Data preparation and prediction (LGBoost) \n\nLet's start by importing the libraries:"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport sklearn\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\n\nimport lightgbm as lgb\nfrom xgboost import XGBRegressor\nfrom sklearn.model_selection import cross_val_score\nfrom xgboost import plot_importance\n\nimport time\nimport datetime\nimport re\nfrom itertools import product\nfrom math import isnan\nimport scipy.stats as stats\n\nimport gc\nimport pickle\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# DISCLAIMER: Some procedures and ideas (in particular feature couples to extract lag and duplicated russian shop_names) in this kernel have been influenced by the following two kernels:\n#https://www.kaggle.com/kyakovlev/1st-place-solution-part-1-hands-on-data\n#https://www.kaggle.com/dlarionov/feature-engineering-xgboost","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Step1. Load data\n\nThis step consists on several procedures, not just data loading as usually:\n* Read all data files provided by Kaggle competition\n* Display load data\n* Join train/test data and fill some values as the month of the test data\n* Define a function to downgrade data types (to deal with massive arrays) for future use\n* Fill some missings with 0s \n* Generate support flag features (in_test, is_new_item)"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Load input files\nsales_train = pd.read_csv(\"../input/competitive-data-science-predict-future-sales/sales_train.csv\", parse_dates=['date'], infer_datetime_format=False, dayfirst=True)\ntest = pd.read_csv(\"../input/competitive-data-science-predict-future-sales/test.csv\")\nitem_categories = pd.read_csv(\"../input/competitive-data-science-predict-future-sales/item_categories.csv\")\nitems = pd.read_csv(\"../input/competitive-data-science-predict-future-sales/items.csv\")\nshops = pd.read_csv(\"../input/competitive-data-science-predict-future-sales/shops.csv\")\n\n# Take a brief look on the content\nprint(\"Sales_train\")\ndisplay(sales_train.head(10))\nprint(\"Test\")\ndisplay(test.head(10))\nprint(\"Item_categories\")\ndisplay(item_categories.head(10))\nprint(\"Items\")\ndisplay(items.head(10))\nprint(\"Shops\")\ndisplay(shops.head(1))\n\n# Auxiliar function to reduce data storage\ndef downcast_dtypes(df):\n    # Columns to downcast\n    float_cols = [c for c in df if df[c].dtype == \"float64\"]\n    int_cols =   [c for c in df if df[c].dtype == \"int64\"]\n    # Downcast\n    df[float_cols] = df[float_cols].astype(np.float16)\n    df[int_cols]   = df[int_cols].astype(np.int16)\n    return df\n\n# Prepare the test set to merge it with sales_train\ntest['date_block_num'] = 34\ntest['date'] = datetime.datetime(2015, 11, 1, 0, 0, 0)\n\n# Join train and test sets. Fill date_block_num = 34 for test rows \nall_data = pd.concat([sales_train, test], axis = 0, sort=False)\nall_data['date_block_num'].fillna(34, inplace=True)\n\n# Create flag (in_test) for month=34\nall_data['in_test'] = 0\nall_data.loc[all_data.date_block_num == 34, 'in_test'] = 1\n\n# Create a flag (is_new_item) for elements in test not in sales_train\nnew_items = (set(test['item_id'].unique()) - set(sales_train['item_id'].unique()))\nall_data.loc[all_data['item_id'].isin(new_items), 'is_new_item'] = 1\n\n# Fill missings with 0\nall_data.fillna(0, inplace=True)\nall_data = downcast_dtypes(all_data)\nall_data = all_data.reset_index()\ndisplay(all_data.head(10))\n\nprint(\"Train set size: \", len(sales_train))\nprint(\"Test set size: \", len(test))\nprint(\"Item categories set size: \", len(item_categories))\nprint(\"Items set size: \", len(items))\nprint(\"Shops set size: \", len(shops))\nprint(\"All data size: \", len(all_data))\nprint(\"Duplicates in train dataset: \", len(sales_train[sales_train.duplicated()]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Step 2. Data exploration (EDA)\n\nIn the previous step, we had the opportunity to see how data is structured and which types of data are we dealing with. However, we haven't analysed the existance of outliers, abnormal values (either extremely high or low), duplicate categorical values, etc. That's what we will study in the following code blocks. \n\nA brief summary of our EDA:\n* Analyze extreme values in item_price and item_cnt_day\n* Deal with the outliers (extremely large values and negative counts)\n* Find and deal with duplicates in shop_name\n* Fix negative item_prices\n* Create an enriched dataset for further exploration (optional but recommended). Includes some feature engineering\n* Analyze sells by price categories \n* Analyze monthly sales\n* Create a correlation matrix"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Describe merged data to look for inusual values\ndisplay(all_data.describe())\nprint(\"Item_price outlier: \")\nprint(all_data.loc[all_data['item_price'].idxmax()])\nprint(\"\\nItem_cnt_day maximum: \")\nprint(all_data.loc[all_data['item_cnt_day'].idxmax()])\n\nf1, axes = plt.subplots(1, 2, figsize=(15,5))\nf1.subplots_adjust(hspace=0.4, wspace=0.2)\nsns.boxplot(x=all_data['item_price'], ax=axes[0])\nsns.boxplot(x=all_data['item_cnt_day'], ax=axes[1])\n\nprint(shops['shop_name'].unique())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Conclusions by now:\n1. There are negative prices and counts (errors, returns?)\n2. Item_id = 6066 has an abnormal large price (item_price = 307980), and is only sold one time\n3. 2 items have very large item_cnt_day when compared with the other products\n4. Shop_name contains the shops' city names (Москва, Moscow). An additional feature can be obtained\n5. Якутск city is expressed as Якутск and !Якутск. This could be fixed\n6. Shop_id = 0 & 1 are the same than 57 & 58 but for фран (Google translator => fran, maybe franchise). Shop_id = 10 & 11 are the same\n\nLet's tackle these outliers, duplicates and negative numbers."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Drop outliers and negative counts (see graphs below)\nall_data = all_data.drop(all_data[all_data['item_price']>100000].index)\nall_data = all_data.drop(all_data[all_data['item_cnt_day']>1100].index)\nsales_train = sales_train.drop(sales_train[sales_train['item_price']>100000].index)\nsales_train = sales_train.drop(sales_train[sales_train['item_cnt_day']>1100].index)\n\n# There are shops with same address and almost same name in russian. \n# Unify duplicated shops (see https://www.kaggle.com/dlarionov/feature-engineering-xgboost)\nall_data.loc[all_data['shop_id'] == 11,'shop_id'] = 10\nall_data.loc[all_data['shop_id'] == 57,'shop_id'] = 0\nall_data.loc[all_data['shop_id'] == 58,'shop_id'] = 1\nsales_train.loc[sales_train['shop_id'] == 11,'shop_id'] = 10\nsales_train.loc[sales_train['shop_id'] == 57,'shop_id'] = 0\nsales_train.loc[sales_train['shop_id'] == 58,'shop_id'] = 1\ntest.loc[test['shop_id'] == 11,'shop_id'] = 10\ntest.loc[test['shop_id'] == 57,'shop_id'] = 0\ntest.loc[test['shop_id'] == 58,'shop_id'] = 1\n\n# Instead of deleting negative price items, replace them with the median value for the impacted group:\nall_data.loc[all_data['item_price'] < 0, 'item_price'] = all_data[(all_data['shop_id'] == 32) & \n                                                                  (all_data['item_id'] == 2973) & \n                                                                  (all_data['date_block_num'] == 4) & \n                                                                  (all_data['item_price'] > 0)].item_price.median()\n\nprint(\"Raw data length: \",len(sales_train), \", post-outliers length: \", len(all_data))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, an enriched matrix with additional features will be created just for data exploration purposes. This may proof useful later on to think about how to structure our data and have a general view of our datasets.\n\n**Disclaimer**: This is completely optional and techniques used to enrich data should be considered as feature engineering. However, while developping this kernel I found it useful to figure out which way to deal with time-series data."},{"metadata":{"trusted":true},"cell_type":"code","source":"ts = time.time()\n\n# Enrich data with additional features and aggregates for data exploration purposes\ndef enrich_data(all_data, items, shops, item_categories):\n    \n    # Aggregate at month level. Calculate item_cnt_month and item_price (median)\n    count_data = all_data.groupby(['shop_id', 'item_id', 'date_block_num', 'in_test', 'is_new_item'])['item_cnt_day'].sum().rename('item_cnt_month').reset_index()\n    price_data = all_data.groupby(['shop_id', 'item_id', 'date_block_num', 'in_test', 'is_new_item'])['item_price'].median().rename('item_price_median').reset_index()\n    all_data = pd.merge(count_data, price_data, on=['shop_id', 'item_id', 'in_test', 'date_block_num', 'is_new_item'], how='left')\n\n    # Extract day, month, year\n    #all_data['day'] = all_data['date'].dt.day\n    #all_data['month'] = all_data['date'].dt.month\n    #all_data['year'] = all_data['date'].dt.year\n\n    # Add item, shop and item_category details \n    all_data = all_data.join(items, on='item_id', rsuffix='_item')\n    all_data = all_data.join(shops, on='shop_id', rsuffix='_shop')\n    all_data = all_data.join(item_categories, on='item_category_id', rsuffix='_item_category')\n    all_data = all_data.drop(columns=['item_id_item', 'shop_id_shop', 'item_category_id_item_category', 'item_name'])\n    \n    # Extract main category and subcategory from category name\n    categories_split = all_data['item_category_name'].str.split('-')\n    all_data['main_category'] = categories_split.map(lambda row: row[0].strip())\n    all_data['secondary_category'] = categories_split.map(lambda row: row[1].strip() if (len(row)>1) else 'N/A')\n    \n     # Extract cities information from shop_name. Replace !Якутск by Якутск since it's the same city\n    all_data['city'] = all_data['shop_name'].str.split(' ').map(lambda row: row[0])\n    all_data.loc[all_data.city == '!Якутск', 'city'] = 'Якутск'\n\n    # Encode cities and categories\n    encoder = sklearn.preprocessing.LabelEncoder()\n    all_data['city_label'] = encoder.fit_transform(all_data['city'])\n    all_data['main_category_label'] = encoder.fit_transform(all_data['main_category'])\n    all_data['secondary_category_label'] = encoder.fit_transform(all_data['secondary_category'])\n    all_data = all_data.drop(['city', 'shop_name', 'item_category_name', 'main_category', 'secondary_category'], axis = 1)\n\n    # Create price categories (0-5, 5-10, 10,20, 20,30, 30-50, 50-100, >100)\n    def price_category(row):\n        if row.item_price_median<5.:\n            val = 1\n        elif row.item_price_median<10.:\n            val = 2\n        elif row.item_price_median<100.:\n            val = 3\n        elif row.item_price_median<200.:\n            val = 4\n        elif row.item_price_median<300.:\n            val = 5\n        elif row.item_price_median<500.:\n            val = 6\n        elif row.item_price_median<1000.:\n            val = 7\n        elif row.item_price_median>1000.:\n            val = 8\n        else:\n            val = 0\n        return val\n    \n    all_data['price_cat'] = all_data.apply(price_category, axis=1)\n    \n    # Downgrade numeric data types\n    all_data = downcast_dtypes(all_data)\n    \n    # Performance test dropping month_cnt\n    #all_data.drop('item_cnt_month', axis=1, inplace=True)\n    \n    return all_data\n\nall_data2 = enrich_data(all_data, items, shops, item_categories)\nitems_prices = all_data2[['item_id', 'shop_id', 'date_block_num', 'item_price_median', 'price_cat']]\n\ntime.time() - ts\n\nall_data2.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Alright, now we have an advanced view of the kind of data we are dealing with. This will help us to define how to wotk with time-series in the following steps. But first, let's finish our exploratory analysis by:\n* Study monthly sales by month \n* Study monthly sales by price category\n* Look at the correlation matrix of our enriched data."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Analyze monthly sells for all shops\nall_data2['item_cnt_month'] = all_data2['item_cnt_month'].astype(np.float64)\ncount_monthly_sales = all_data2.groupby('date_block_num').item_cnt_month.sum(axis=0)\n\nf = plt.figure()\nax = f.add_subplot(111)\nplt.plot(count_monthly_sales)\nplt.axvline(x=12,color='grey',linestyle='--')  # Vertical grey line for December month\nplt.axvline(x=24,color='grey',linestyle='--')\nplt.xlabel(\"date_block_num\")\nplt.title('Monthly total sells')\nplt.show()\n\n# Analyze monthly sells for each price category\ncount_price_cat_sales = all_data2.groupby('price_cat').item_cnt_month.sum(axis=0)\n\nf = plt.figure()\nax = f.add_subplot(111)\nplt.plot(count_price_cat_sales)\nplt.xticks([0,1,2,3,4,5,6,7,8],['others', '0<p<5₽','5<p<10₽','10<p<100₽','100<p<200₽','200<p<300₽','300<p<500₽','500<p<1000₽','>1000₽'], rotation='45')\nplt.title('Price category sells')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Looks like C1 company has a decreasing tendency on sales. There are some reasons for this behavior (depreciation of the ruble), but we don't need to tackle this explicitly for our prediction purposes since the algorithm will detect the tendency automatically from data.\nAdditionally, we see there's an increasing sales count on items with higher prices, but this could be due to our bin size. Just take it into account."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Correlation matrix for monthly sales\nall_data2 = all_data2[all_data2['date_block_num']<34]\n#all_data2 = all_data2.drop(columns=['in_test', 'is_new_item'], inplace=True)\n\n# Correlation matrix\nf = plt.figure(figsize=(9, 5))\nplt.matshow(all_data2.corr(), fignum=f.number)\nplt.xticks(range(all_data2.shape[1]),all_data2.columns, fontsize=10, rotation=90)\nplt.yticks(range(all_data2.shape[1]), all_data2.columns, fontsize=10)\ncb = plt.colorbar()\ncb.ax.tick_params(labelsize=14)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Not surprising correlations, but a good look-up result in case we find something interesting later on."},{"metadata":{},"cell_type":"markdown","source":"# Step 3. Missings cleaning\n\nSince we filled missing values with 0s, we expect little or no missings in this section. However, it's always a good practice to check out before feature engineering and detection."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Missings count. There are no missings (remind that we filled all missings on the beginning of this kernel with  0s)\nmissings_count = {col:all_data[col].isnull().sum() for col in all_data.columns}\nmissings = pd.DataFrame.from_dict(missings_count, orient='index')\nprint(missings.nlargest(30, 0))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Step 4. Feature engineering\n\nSteps 4 and 5 are those in which we will need to be more incisive. Since data is strongly dependent on time, it's important to define how to work with it. \n\nHere we have two options:\n* Do we create a row for each item/shop pair and then create a column for each month? \n* Or it could be better to generate one different row for each item/shop/date_block_num sale\n\nYou can try the first option to obtain some decent results (you can see the results here https://www.kaggle.com/saga21/start-with-kaggle-comps-future-sales-v0), but we can make a step further and decide to structure data by item/shop/date_month. With this, we will have a row for each monthly sale, which will help the algorithm to predict future data (and not just predict an additional column for the new month).\n\nWhat we will do:\n* Generate all combinations of existent item/shop/date_block_num (cartesian product) from the training set\n* Revenue. New feature from item_price * item_cnt_day\n* Item_cnt_month. New feature from grouping item/shops by month and summing the item_cnt_day\n* Join test data\n* Join item, shop and item category details (see additional files provided by the competition)\n* Month. Numeric month value from 1 to 12\n* Days. Number of days in each month (no leap years)\n* Main_category. From item categories, extract the principal type\n* Secondary_category. From item categories, extract the secondary type\n* City. Extract the city from shop_name\n* Shop_type. Extract the type from shop_name\n* Encode categorical columns: main_category, secondary_category, city and shop_type"},{"metadata":{"trusted":true},"cell_type":"code","source":"ts = time.time()\n\n# Extend all_data for all item/shop pairs. \ndef add_all_pairs(sales_train, test, items, shops, item_categories, items_prices):\n    \n    tmp = []\n    for month in range(34):\n        sales = sales_train[sales_train.date_block_num==month]\n        tmp.append(np.array(list(product([month], sales.shop_id.unique(), sales.item_id.unique())), dtype='int16'))\n\n    tmp = pd.DataFrame(np.vstack(tmp), columns=['date_block_num','shop_id','item_id'])\n    tmp['date_block_num'] = tmp['date_block_num'].astype(np.int8)\n    tmp['shop_id'] = tmp['shop_id'].astype(np.int8)\n    tmp['item_id'] = tmp['item_id'].astype(np.int16)\n    tmp.sort_values(['date_block_num','shop_id','item_id'],inplace=True)\n    \n    sales_train['revenue'] = sales_train['item_price'] * sales_train['item_cnt_day']\n    group = sales_train.groupby(['date_block_num','shop_id','item_id']).agg({'item_cnt_day': ['sum']})\n    group.columns = ['item_cnt_month']\n    group.reset_index(inplace=True)\n    tmp = pd.merge(tmp, group, on=['date_block_num','shop_id','item_id'], how='left')\n    tmp['item_cnt_month'] = (tmp['item_cnt_month'].fillna(0).clip(0,20).astype(np.float16))\n    tmp = pd.concat([tmp, test], ignore_index=True, sort=False, keys=['date_block_num','shop_id','item_id'])\n    \n    #price_data = tmp.groupby(['shop_id', 'item_id', 'date_block_num', 'in_test', 'is_new_item'])['item_price'].median().rename('item_price_median').reset_index()\n    #tmp = tmp.join(price_data, on=[[]])\n    \n    # Add item, shop and item_category details \n    tmp = tmp.join(items, on='item_id', rsuffix='_item')\n    tmp = tmp.join(shops, on='shop_id', rsuffix='_shop')\n    tmp = tmp.join(item_categories, on='item_category_id', rsuffix='_item_category')\n    tmp = pd.merge(tmp, items_prices, on=['date_block_num','shop_id','item_id'], how='left')\n    tmp = tmp.drop(columns=['item_id_item', 'shop_id_shop', 'item_category_id_item_category', 'item_name'])\n    \n    # Extract month, year & nºdays in each month\n    tmp['month'] = tmp['date_block_num']%12\n    tmp['days'] = tmp['month'].map(pd.Series([31,28,31,30,31,30,31,31,30,31,30,31]))\n    \n    # Extract main category and subcategory from category name\n    categories_split = tmp['item_category_name'].str.split('-')\n    tmp['main_category'] = categories_split.map(lambda row: row[0].strip())\n    tmp['secondary_category'] = categories_split.map(lambda row: row[1].strip() if (len(row)>1) else 'N/A')\n    \n    # Extract cities information from shop_name. Replace !Якутск by Якутск since it's the same city.\n    tmp['city'] = tmp['shop_name'].str.split(' ').map(lambda row: row[0])\n    tmp.loc[tmp.city == '!Якутск', 'city'] = 'Якутск'\n    tmp['shop_type'] = tmp['shop_name'].apply(lambda x: 'мтрц' if 'мтрц' in x \n                                              else 'трц' if 'трц' in x \n                                              else 'трк' if 'трк' in x \n                                              else 'тц' if 'тц' in x \n                                              else 'тк' if 'тк' in x \n                                              else 'NO_DATA')\n\n    # Encode cities and categories\n    encoder = sklearn.preprocessing.LabelEncoder()\n    tmp['city_label'] = encoder.fit_transform(tmp['city'])\n    tmp['shop_type_label'] = encoder.fit_transform(tmp['shop_type'])\n    tmp['main_category_label'] = encoder.fit_transform(tmp['main_category'])\n    tmp['secondary_category_label'] = encoder.fit_transform(tmp['secondary_category'])\n    tmp = tmp.drop(['ID', 'city', 'date', 'shop_name', 'item_category_name', 'main_category', 'secondary_category'], axis = 1)\n    \n    # Downgrade numeric data types\n    tmp = downcast_dtypes(tmp)\n\n    tmp.fillna(0, inplace=True)\n    return tmp\n    \nall_pairs = add_all_pairs(sales_train, test, items, shops, item_categories, items_prices)\n\ntime.time() - ts","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Fine, so we have extracted some nice additional features and now our sales have one row for each item/shop/date_block_num. It looks promising. "},{"metadata":{},"cell_type":"markdown","source":"# Step 5. Mean encoding\n\nLGB algorithm read rows to extract information from them and predict the target value. We need to provide the algorithm with the historical information for each item, and this is obtained through lags. Lags are essentially columns with information from the past. For example, a lag of 1 month from item_cnt_month would inform about the last month sales for this item. \n\nWhat we will add:\n* **Downgrade** (again) data to deal with large arrays\n* **Support functions**. Create some support functions for lag generation; calculate_lag, prepare_lag_columns and prepare_lag_columns_price. This allows to calculate automatic lags for several columns in a readable code-friendly style. As a rule of thumb: if you need to calculate the same non-trivial computation more than once, creater a function instead\n* **Compute lags**. Lags of monthly sales grouped by several column combinations (how many past sales by shop and category, or by secondary category, etc)\n* **Price_trend**. Track item_prices changes to account for price fluctuations (discounts)\n* **Drop columns**. Some features were generated in order to compute another one. Drop those that are not useful any more or may introduce data leaking (for example, item_price is strongly correlated to sales, since items that were never sell have no price informed)."},{"metadata":{"trusted":true},"cell_type":"code","source":"ts = time.time()\n\n# First downgrade some columns (still more) to fasten the mean encoding\nall_pairs['date_block_num'] = all_pairs['date_block_num'].astype(np.int8)\nall_pairs['city_label'] = all_pairs['city_label'].astype(np.int8)\nall_pairs['item_cnt_month'] = all_pairs['item_cnt_month'].astype(np.int8)\nall_pairs['item_category_id'] = all_pairs['item_category_id'].astype(np.int8)\nall_pairs['main_category_label'] = all_pairs['main_category_label'].astype(np.int8)\nall_pairs['secondary_category_label'] = all_pairs['secondary_category_label'].astype(np.int8)\n\n# Function to calculate lag over different columns. Lag gives information about a variable from different past times\ndef calculate_lag(df, lag, column):\n    ancilla = df[['date_block_num','shop_id','item_id',column]]\n    for l in lag:\n        shift_ancilla = ancilla.copy()\n        shift_ancilla.columns = ['date_block_num','shop_id','item_id', column+'_lag_'+str(l)]\n        shift_ancilla['date_block_num'] += l\n        df = pd.merge(df, shift_ancilla, on=['date_block_num','shop_id','item_id'], how='left')\n    return df\n\n# Function to specify lag columns,compute item_cnt aggregate (mean) and call calculate_lag\ndef prepare_lag_columns(df, lag, column_list, name):\n    ancilla = df.groupby(column_list).agg({'item_cnt_month':['mean']})\n    ancilla.columns = [name]\n    ancilla.reset_index(inplace=True)\n    df = pd.merge(df, ancilla, on=column_list, how='left')\n    df[name] = df[name].astype(np.float16)\n    df = calculate_lag(df, lag, name)\n    df.drop([name], axis=1, inplace=True)\n    return df\n\n# Auxiliar function to compute item_price groups (for trends). Lags will be calculated post-preparation\ndef prepare_lag_columns_price(df, column_list, name):\n    ancilla = sales_train.groupby(column_list).agg({'item_price':['mean']})\n    ancilla.columns = [name]\n    ancilla.reset_index(inplace=True)\n    df = pd.merge(df, ancilla, on=column_list, how='left')\n    df[name] = df[name].astype(np.float16)\n    return df\n\n# Let's compute all lags for sells. Arguments of the function are :(df, lag_list, column_list, name of the column)\nall_pairs = calculate_lag(all_pairs, [1,2,3,4,5,6,12], 'item_cnt_month')\nall_pairs = prepare_lag_columns(all_pairs, [1], ['date_block_num', 'item_id'], 'total_avg_month_cnt')\nall_pairs = prepare_lag_columns(all_pairs, [1,2,3,4,5,6,12], ['date_block_num'], 'item_avg_month_cnt')\nall_pairs = prepare_lag_columns(all_pairs, [1,2,3,4,5,6,12], ['date_block_num', 'shop_id'], 'shop_avg_month_cnt')\nall_pairs = prepare_lag_columns(all_pairs, [1], ['date_block_num','city_label'], 'city_avg_month_cnt')\nall_pairs = prepare_lag_columns(all_pairs, [1], ['date_block_num','item_id','city_label'], 'item_city_avg_month_cnt')\nall_pairs = prepare_lag_columns(all_pairs, [1], ['date_block_num', 'item_category_id'], 'category_id_avg_month_cnt')\nall_pairs = prepare_lag_columns(all_pairs, [1], ['date_block_num', 'main_category_label'], 'main_category_avg_month_cnt')\nall_pairs = prepare_lag_columns(all_pairs, [1], ['date_block_num', 'secondary_category_label'], 'secondary_category_avg_month_cnt')\nall_pairs = prepare_lag_columns(all_pairs, [1], ['date_block_num','shop_id','item_category_id'], 'shop_category_id_avg_month_cnt')\nall_pairs = prepare_lag_columns(all_pairs, [1], ['date_block_num','shop_id','main_category_label'], 'shop_main_category_avg_month_cnt')\nall_pairs = prepare_lag_columns(all_pairs, [1], ['date_block_num','shop_id','secondary_category_label'], 'shop_secondary_category_avg_month_cnt')\n\n\n# For item_price the procedure is more tricky. Compute both item price and monthly price in order to compute the trend.\nall_pairs = prepare_lag_columns_price(all_pairs, ['item_id'], 'item_avg_price')\nall_pairs = prepare_lag_columns_price(all_pairs, ['date_block_num','item_id'], 'item_avg_price_month')\nall_pairs = calculate_lag(all_pairs, [1,2,3,4,5,6], 'item_avg_price_month')\n\nfor lag in [1,2,3,4,5,6]:\n    all_pairs['trend_price_lag_'+str(lag)] = (all_pairs['item_avg_price_month_lag_'+str(lag)] - all_pairs['item_avg_price']) / all_pairs['item_avg_price']\n\ndef clean_trend_price_lag(row):\n    for l in [1,2,3,4,5,6]:\n        if row['trend_price_lag_'+str(l)]:\n            return row['trend_price_lag_'+str(l)]\n    return 0\n\n# For some reason my kernel expodes when using df.apply() for all rows, so I had to segment it\ndummy_1, dummy_2, dummy_3, dummy_4 = [], [], [], []      \ndummy_1 = pd.DataFrame(dummy_1)\ndummy_2 = pd.DataFrame(dummy_2)\ndummy_3 = pd.DataFrame(dummy_3)\ndummy_4 = pd.DataFrame(dummy_4)\ndummy_1 = all_pairs[:3000000].apply(clean_trend_price_lag, axis=1)\ndummy_2 = all_pairs[3000000:6000000].apply(clean_trend_price_lag, axis=1)\ndummy_3 = all_pairs[6000000:9000000].apply(clean_trend_price_lag, axis=1)\ndummy_4 = all_pairs[9000000:].apply(clean_trend_price_lag, axis=1)\nall_pairs['trend_price_lag'] = pd.concat([dummy_1, dummy_2, dummy_3, dummy_4])\nall_pairs['trend_price_lag'] = all_pairs['trend_price_lag'].astype(np.float16)\nall_pairs['trend_price_lag'].fillna(0, inplace=True)\n\n#all_pairs.drop(['item_avg_price','item_avg_price_month'], axis=1, inplace=True)\nfor i in [1,2,3,4,5,6]:\n    all_pairs.drop(['item_avg_price_month_lag_'+str(i), 'trend_price_lag_'+str(i)], axis=1, inplace=True)\n\nall_pairs.drop('shop_type', axis=1, inplace=True)\ntime.time() - ts","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Ok, that's been a while. We are working with huge datasets and the computations of additional features are computationally costly, but it will prove to be advantageous. \n\nTo finish up, we will compute some additional values:\n* **Shop_avg_revenue**. All sales for a certain shop, in order to track very profitable shops or poor selling ones. Since we are only interested in the last month, we will drop all additional columns but the lag\n* **Item_shop_first_sale**. Months since the first sell of a certain shop was made\n* **Item_first_sale**. Months since the first sell of a certain item"},{"metadata":{"trusted":true},"cell_type":"code","source":"ts = time.time()\ngroup = sales_train.groupby(['date_block_num','shop_id']).agg({'revenue': ['sum']})\ngroup.columns = ['date_shop_revenue']\ngroup.reset_index(inplace=True)\n\nall_pairs = pd.merge(all_pairs, group, on=['date_block_num','shop_id'], how='left')\nall_pairs['date_shop_revenue'] = all_pairs['date_shop_revenue'].astype(np.float32)\n\ngroup = group.groupby(['shop_id']).agg({'date_shop_revenue': ['mean']})\ngroup.columns = ['shop_avg_revenue']\ngroup.reset_index(inplace=True)\n\nall_pairs = pd.merge(all_pairs, group, on=['shop_id'], how='left')\nall_pairs['shop_avg_revenue'] = all_pairs['shop_avg_revenue'].astype(np.float32)\n\nall_pairs['delta_revenue'] = (all_pairs['date_shop_revenue'] - all_pairs['shop_avg_revenue']) / all_pairs['shop_avg_revenue']\nall_pairs['delta_revenue'] = all_pairs['delta_revenue'].astype(np.float16)\n\nall_pairs = calculate_lag(all_pairs, [1], 'delta_revenue')\n\nall_pairs.drop(['date_shop_revenue','shop_avg_revenue','delta_revenue'], axis=1, inplace=True)\n\n# First sale extraction\nall_pairs['item_shop_first_sale'] = all_pairs['date_block_num'] - all_pairs.groupby(['item_id','shop_id'])['date_block_num'].transform('min')\nall_pairs['item_first_sale'] = all_pairs['date_block_num'] - all_pairs.groupby('item_id')['date_block_num'].transform('min')\n\ntime.time() - ts","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A final correlation matrix and we are done..."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Correlation matrix for monthly sales\nall_pairs2 = all_pairs[all_pairs['date_block_num']<34]\n#all_data2 = all_data2.drop(columns=['in_test', 'is_new_item'], inplace=True)\n\n# Correlation matrix\nf = plt.figure(figsize=(9, 5))\nplt.matshow(all_pairs2.corr(), fignum=f.number)\nplt.xticks(range(all_pairs2.shape[1]),all_pairs2.columns, fontsize=7, rotation=90)\nplt.yticks(range(all_pairs2.shape[1]), all_pairs2.columns, fontsize=7)\ncb = plt.colorbar()\ncb.ax.tick_params(labelsize=14)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Step 6. Data preparation and prediction (LGB)\n\nThis is our last step. We need to carefully prepare data, proceed with our splits and apply the LGB algorithm\n\nIn this section we will proceed with:\n* **Drop first 11 months**. Since some of our lags cover the previous 12 months, the first 11 months have no complete lag information. Hence, to be coherent, we will drop this data (yep, that hurts)\n* **Fill lag missings**. When needed.\n* **Drop columns**. Some of them introduce data leaking (item_price_median), and others provide not enough information and generate noise in the algorithm (this is tested manually through the LGB)\n* **Split data**. The filtering condition is just date_block_num. Train from 11 to 32, validation with 33 and test with 34.\n* **Run LGB**. This might require some fine tuning and parameter optimization. Feel free to perform some grid search through cross-validation. \n* **Submit results**. Finally! Let's grab some coffe."},{"metadata":{"trusted":true},"cell_type":"code","source":"ts = time.time()\nall_pairs = all_pairs[all_pairs.date_block_num > 11]\ntime.time() - ts\n\nts = time.time()\ndef fill_na(df):\n    for col in df.columns:\n        if ('_lag_' in col) & (df[col].isnull().any()):\n            if ('item_cnt' in col):\n                df[col].fillna(0, inplace=True)         \n    return df\n\nall_pairs = fill_na(all_pairs)\nall_pairs.fillna(0, inplace=True)    \ntime.time() - ts","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_pairs.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_pairs.drop(['item_price_median', 'price_cat', 'item_avg_price', 'item_avg_price_month',\n               'main_category_avg_month_cnt_lag_1','secondary_category_avg_month_cnt_lag_1',\n               'shop_main_category_avg_month_cnt_lag_1','shop_secondary_category_avg_month_cnt_lag_1'], inplace=True, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_pairs.to_pickle('data.pkl')\ndata = pd.read_pickle('data.pkl')\n\nX_train = data[data.date_block_num < 33].drop(['item_cnt_month'], axis=1)\nY_train = data[data.date_block_num < 33]['item_cnt_month']\nX_valid = data[data.date_block_num == 33].drop(['item_cnt_month'], axis=1)\nY_valid = data[data.date_block_num == 33]['item_cnt_month']\nX_test = data[data.date_block_num == 34].drop(['item_cnt_month'], axis=1)\ngc.collect();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model=lgb.LGBMRegressor(\n        n_estimators=10000,\n        learning_rate=0.3,\n        min_child_weight=300,\n        #num_leaves=32,\n        colsample_bytree=0.8,\n        subsample=0.8,\n        max_depth=8,\n        #reg_alpha=0.04,\n        #reg_lambda=0.073,\n        #min_split_gain=0.0222415,\n        verbose=1,\n        seed=21)\n\nmodel.fit(X_train, Y_train,eval_metric=\"rmse\", eval_set=[(X_train, Y_train), (X_valid, Y_valid)], verbose=1, early_stopping_rounds = 10)\n\n# Cross validation accuracy for 3 folds\n#scores = cross_val_score(model, X_train, Y_train, cv=3)\n#print(scores)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Y_pred = model.predict(X_valid).clip(0, 20)\nY_test = model.predict(X_test).clip(0, 20)\n\nsubmission = pd.DataFrame({\n    \"ID\": test.index, \n    \"item_cnt_month\": Y_test\n})\nsubmission.to_csv('submission.csv', index=False)\n\n# save predictions for an ensemble\npickle.dump(Y_pred, open('xgb_train.pickle', 'wb'))\npickle.dump(Y_test, open('xgb_test.pickle', 'wb'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\n\nALTERNATIVE OPTION WITH XGB. TIME CONSUMING, BUT ALLOWS TO STUDY FEATURE IMPORTANCE\n\nts = time.time()\n\nmodel = XGBRegressor(\n    max_depth=8,\n    n_estimators=1000,\n    min_child_weight=300, \n    colsample_bytree=0.8, \n    subsample=0.8, \n    eta=0.3,    \n    seed=21)\n\nmodel.fit(\n    X_train, \n    Y_train, \n    eval_metric=\"rmse\", \n    eval_set=[(X_train, Y_train), (X_valid, Y_valid)], \n    verbose=True, \n    early_stopping_rounds = 10)\n\ntime.time() - ts\n\n\ndef plot_features(booster, figsize):    \n    fig, ax = plt.subplots(1,1,figsize=figsize)\n    return plot_importance(booster=booster, ax=ax)\n\nplot_features(model, (10,14))\n\n\"\"\"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}