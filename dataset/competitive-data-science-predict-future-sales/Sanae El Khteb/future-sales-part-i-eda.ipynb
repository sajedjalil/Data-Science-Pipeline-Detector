{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Welcome to this kernel\n\nThe goal of this kernel is very simple. It aims to provide some useful insights about the data and hopefully can guide you into what features to generate and how to tackle the modelling part.\n\n### <span style=\"color:#81c1dc\">The kernel is divided into the following parts</span>\n    \n<a id = \"table_of_contents\"></a>\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\"  role=\"tab\" aria-controls=\"home\">Notebook Content</h3>\n<a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#Imports\" role=\"tab\" aria-controls=\"messages\">Imports<span class=\"badge badge-primary badge-pill\">1</span></a>\n<a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#Quick-look-at-shops-df\" role=\"tab\" aria-controls=\"messages\">Quick look at shops df<span class=\"badge badge-primary badge-pill\">2</span></a>\n<a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#Fix-shops-df-and-generate-some-features\" role=\"tab\" aria-controls=\"messages\">Fix shops df and generate some features<span class=\"badge badge-primary badge-pill\">3</span></a>\n<a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#Quick-look-at-items_category-df\" role=\"tab\" aria-controls=\"messages\">Quick look at items_category df<span class=\"badge badge-primary badge-pill\">4</span></a>\n<a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#Quick-look-at-items-df\" role=\"tab\" aria-controls=\"messages\">Quick look at items df<span class=\"badge badge-primary badge-pill\">5</span></a>\n<a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#Quick-look-at-sales-df\" role=\"tab\" aria-controls=\"messages\">Quick look at sales df<span class=\"badge badge-primary badge-pill\">6</span></a>\n<a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#Joining-df\" role=\"tab\" aria-controls=\"messages\">Joining df<span class=\"badge badge-primary badge-pill\">7</span></a>\n<a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#Exploratory-Data-Analysis-(EDA)\" role=\"tab\" aria-controls=\"messages\">Exploratory Data Analysis (EDA)<span class=\"badge badge-primary badge-pill\">8</span></a>\n<a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#Viz-of-sales-per-week,-month-of-shops-and-item_category-columns\" role=\"tab\" aria-controls=\"messages\">-->Viz of sales per week, month of shops and item_category columns<span class=\"badge badge-primary badge-pill\">9</span></a>\n<a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#Total-sales-and-the-variation-on-secondary-axis\" role=\"tab\" aria-controls=\"messages\">-->Total sales and the variation on secondary axis<span class=\"badge badge-primary badge-pill\">10</span></a>\n<a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#Question-1:-Create-a-plot-with-the-moving-average-of-total-sales-(7-days)-and-the-variation-on-the-second-axis.\" role=\"tab\" aria-controls=\"messages\" style=\"color:#F08691\">-->Question 1: Create a plot with the moving average of total sales (7 days) and the variation on the second axis.<span class=\"badge badge-primary badge-pill\" style=\"color:#F08691\">Q1</span></a>\n<a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#Timeseries-autocorrelation-and-partial-autocorrelation-plots:-daily-sales\" role=\"tab\" aria-controls=\"messages\">-->Timeseries autocorrelation and partial autocorrelation plots: daily sales<span class=\"badge badge-primary badge-pill\">11</span></a>\n<a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#Calendar-heatmap\" role=\"tab\" aria-controls=\"messages\">-->Calendar heatmap<span class=\"badge badge-primary badge-pill\">12</span></a>\n<a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#Timeseries-autocorrelation-and-partial-autocorrelation-plots:-weekly-sales\" role=\"tab\" aria-controls=\"messages\">-->Timeseries autocorrelation and partial autocorrelation plots: weekly sales<span class=\"badge badge-primary badge-pill\">13</span></a>\n<a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#Timeseries-autocorrelation-and-partial-autocorrelation-plots:-monthly-sales\" role=\"tab\" aria-controls=\"messages\">-->Timeseries autocorrelation and partial autocorrelation plots: monthly sales<span class=\"badge badge-primary badge-pill\">14</span></a>\n<a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#Timeseries-decomposition-plots:-daily-sales\" role=\"tab\" aria-controls=\"messages\">-->Timeseries decomposition plots: daily sales<span class=\"badge badge-primary badge-pill\">15</span></a>\n<a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#Timeseries-decomposition-plots:-weekly-sales\" role=\"tab\" aria-controls=\"messages\">-->Timeseries decomposition plots: weekly sales<span class=\"badge badge-primary badge-pill\">16</span></a>\n<a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#Timeseries-decomposition-plots:-monthly-sales\" role=\"tab\" aria-controls=\"messages\">-->Timeseries decomposition plots: monthly sales<span class=\"badge badge-primary badge-pill\">17</span></a>\n<a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#Question-2:-Create-a-decomposition-plot-for-a-city-of-weekly-sales\" role=\"tab\" aria-controls=\"messages\" style=\"color:#F08691\">-->Question 2: Create a decomposition plot for a city of weekly sales<span class=\"badge badge-primary badge-pill\" style=\"color:#F08691\">Q2</span></a> \n<a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#Visualizing-the-most-important-cities\" role=\"tab\" aria-controls=\"messages\">-->Visualizing the most important cities<span class=\"badge badge-primary badge-pill\">18</span></a>\n<a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#Question-3:-Create-a-treemap-plot-for-item_category-and-the-total-combined-sales\" role=\"tab\" aria-controls=\"messages\" style=\"color:#F08691\">-->Question 3: Create a treemap plot for item_category and the total combined sales<span class=\"badge badge-primary badge-pill\" style=\"color:#F08691\">Q3</span></a> \n<a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#Visualizing-nulls-values\" role=\"tab\" aria-controls=\"messages\">-->Visualizing nulls values<span class=\"badge badge-primary badge-pill\">19</span></a>\n<a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#Visualization-of-Outliers\" role=\"tab\" aria-controls=\"messages\">-->Visualization of Outliers<span class=\"badge badge-primary badge-pill\">20</span></a>\n<a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#Conclusion\" role=\"tab\" aria-controls=\"messages\">Conclusion<span class=\"badge badge-primary badge-pill\">21</span></a>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Imports\n[Go back to the Table of Contents](#table_of_contents)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# make calendar maps\n!pip install calmap\nimport calmap","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Main libraries that we will use in this kernel\nimport datetime\nimport numpy as np\nimport pandas as pd\n\n# # garbage collector: free some memory is needed\nimport gc\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# pip install squarify (algorithm for treemap) if missing\nimport squarify\n\n# statistical package and some useful functions to analyze our timeseries\nfrom statsmodels.tsa.stattools import pacf\nfrom statsmodels.graphics.tsaplots import plot_acf, plot_pacf\nfrom statsmodels.tsa.seasonal import seasonal_decompose\nimport statsmodels.tsa.stattools as stattools\n\nimport time\n\nfrom xgboost import XGBRegressor\nfrom string import punctuation\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.linear_model import LinearRegression\n\ndef print_files():\n    import os\n    for dirname, _, filenames in os.walk('/kaggle/input'):\n        for filename in filenames:\n            print(os.path.join(dirname, filename))\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's see how many different files we are dealing with\nprint_files()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Quick look at shops df\n[Go back to the Table of Contents](#table_of_contents)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# import the df\nshops = pd.read_csv(\"/kaggle/input/competitive-data-science-predict-future-sales/shops.csv\")\nshops.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"shops","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We don't have any duplicates in the shop_name field\nshops.shape[0] == len(shops[\"shop_name\"].unique())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# However inspecting the df by name, we can see that shop_id 10 and 11 are very similar. Later we will try and group them once we inspect the sales per shop\nshops[shops[\"shop_id\"].isin([10, 11])]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# The same happens with the shops with shop_id 23 and 24\nshops[shops[\"shop_id\"].isin([23, 24])]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# No missing values in the shops df\nshops.isnull().sum().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Fix shops df and generate some features\n[Go back to the Table of Contents](#table_of_contents)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"shops.sample(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's correct the shops df and also generate a few more features\ndef fix_shops(shops):\n    '''\n    This function modifies the shops df inplace.\n    It correct's 3 shops that we have found to be 'duplicates'\n    and also creates a few more features: extracts the city and encodes it using LabelEncoder\n    '''\n    \n    d = {0:57, 1:58, 10:11, 23:24}\n    \n    # this 'tricks' allows you to map a series to a dictionary, but all values that are not in the dictionary won't be affected\n    # it's handy since if we blindly map the values, the missings values will be replaced with nan\n    shops[\"shop_id\"] = shops[\"shop_id\"].apply(lambda x: d[x] if x in d.keys() else x)\n    \n    # replace all the punctuation in the shop_name columns\n    shops[\"shop_name_cleaned\"] = shops[\"shop_name\"].apply(lambda s: \"\".join([x for x in s if x not in punctuation]))\n    \n    # extract the city name\n    shops[\"city\"] = shops[\"shop_name_cleaned\"].apply(lambda s: s.split()[0])\n    \n    # encode it using a simple LabelEncoder\n    shops[\"city_id\"] = LabelEncoder().fit_transform(shops['city'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# apply our function to the shops_df\nfix_shops(shops)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"shops.sample(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Quick look at items_category df\n[Go back to the Table of Contents](#table_of_contents)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# import df\nitems_category = pd.read_csv(\"/kaggle/input/competitive-data-science-predict-future-sales/item_categories.csv\")\nitems_category.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"items_category.sample(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We don't have any duplicates in the item_category_name field\nitems_category.shape[0] == len(items_category[\"item_category_name\"].unique())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# allow pandas to show all the rows from this df\npd.options.display.max_rows = items_category.shape[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# If we take a closer look, we can see that we have a lot of Play Station categories: like accesories, games and so on. We have the same categories for XBOX and also for PC Games.\n# A lot of categories have to deal with books, presents and computer software and music (CD).\n# We will generate later some features by parsing the names and making groupedby features.\nitems_category","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# If we apply a simple lambda function and extract the everything that contains PS, we will get 16 different categories for PlayStation\nitems_category[\"PS_flag\"] = items_category[\"item_category_name\"].apply(lambda x: True if \"PS\" in x else False)\nitems_category[items_category[\"PS_flag\"] == True]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# No missing values in the items_category df\nitems_category.isnull().sum().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Quick look at items df\n[Go back to the Table of Contents](#table_of_contents)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# import df\nitems = pd.read_csv(\"/kaggle/input/competitive-data-science-predict-future-sales/items.csv\")\nitems.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# allow pandas to show all the rows from this df\npd.options.display.max_rows = items.shape[0]\nitems.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We have a lot of items_id, and as we can see some of them are very familiar.\nitems[items[\"item_id\"].isin([69, 70])][\"item_name\"].iloc[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"items[items[\"item_id\"].isin([69, 70])][\"item_name\"].iloc[1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# No missing values in the items category\nitems.isnull().sum().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's see the top 10 and bottom 10 item categories\nitems_gb = items.groupby(\"item_category_id\").size().to_frame() #to_frame() combierte a Dataframe","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# a sample of our groupby dataframe\nitems_gb.sample(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"items_gb.rename(columns = {0:\"counts\"}, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"items_gb.sort_values(\"counts\", ascending = False, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"top_10 = items_gb[:10]\ntop_10","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bottom_10 = items_gb[-10:]\nbottom_10","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"top_10 = top_10.append(bottom_10)\ntop_10 = top_10.sort_values(\"counts\", ascending = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"top_10.reset_index()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We can notice that in the top 10 most popular items products we have PS3\n# At the same time, in the bottom 10 products, we can find 2 PS2.\n# This means, that we have to be careful while generating features like PS\npd.merge(top_10, items_category, left_on = \"item_category_id\", right_on = \"item_category_id\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Quick look at sales df\n[Go back to the Table of Contents](#table_of_contents)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# import df\nsales = pd.read_csv(\"/kaggle/input/competitive-data-science-predict-future-sales/sales_train.csv\")\nsales.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sales.sample(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sales.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# No null values in the sales df\n\n# Is this True?\n\n#No hay nulos porque solo estan los datos de las ventas realizadas, los dias sin ventas no estan representadas\n\nsales.isnull().sum().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del shops, items_category, items, sales\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Joining df\n[Go back to the Table of Contents](#table_of_contents)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# useful function that manipulates the df and casts all the values to a lower numeric type and saves memory\ndef reduce_mem_usage(df, verbose=True):\n    '''\n    Reduces the space that a DataFrame occupies in memory.\n    '''\n    \n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() / 1024**2    \n    \n    for col in df.columns:\n        \n        col_type = df[col].dtypes\n        \n        if col_type in numerics:\n            \n            c_min = df[col].min()\n            c_max = df[col].max()\n            \n            if str(col_type)[:3] == 'int':\n                \n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                    \n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                    \n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                    \n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64) \n                    \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                    \n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                    \n                else:\n                    df[col] = df[col].astype(np.float64)\n                    \n    end_mem = df.memory_usage().sum() / 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n        \n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# a simple function that creates a global df with all joins and also shops corrections\ndef create_df():\n    '''\n    This is a helper function that creates the train df.\n    '''\n    # import all df\n    shops = pd.read_csv(\"/kaggle/input/competitive-data-science-predict-future-sales/shops.csv\")\n    fix_shops(shops) # fix the shops as we have seen before\n    \n    items_category = pd.read_csv(\"/kaggle/input/competitive-data-science-predict-future-sales/item_categories.csv\")\n    items = pd.read_csv(\"/kaggle/input/competitive-data-science-predict-future-sales/items.csv\")\n    sales = pd.read_csv(\"/kaggle/input/competitive-data-science-predict-future-sales/sales_train.csv\")\n    \n    # fix shop_id in sales so that we can leater merge the df\n    d = {0:57, 1:58, 10:11, 23:24}\n    sales[\"shop_id\"] = sales[\"shop_id\"].apply(lambda x: d[x] if x in d.keys() else x)\n    \n    # create df by merging the previous dataframes\n    df = pd.merge(items, items_category, left_on = \"item_category_id\", right_on = \"item_category_id\")\n    df = pd.merge(sales, df, left_on = \"item_id\", right_on = \"item_id\")\n    df = pd.merge(df, shops, left_on = \"shop_id\", right_on = \"shop_id\")\n    \n    # convert to datetime and sort the values\n#     df[\"date\"] = pd.to_datetime(df[\"date\"], format = \"%d.%m.%Y\")\n    df.sort_values(by = [\"shop_id\", \"date\"], ascending = True, inplace = True)\n    \n    # reduce memory usage\n#     df = reduce_mem_usage(df)\n    \n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = create_df()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Exploratory Data Analysis (EDA)\n[Go back to the Table of Contents](#table_of_contents)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# It seems that there are no null values, however this is not fully true. \n# As we will see in the next section, when we groupby and plot the data, there are a lot of months where there have been no sales so basically it's a null value, and we have to impute zero sales for that month.\ndf.isnull().sum().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Viz of sales per week, month of shops and item_category columns\n[Go back to the Table of Contents](#table_of_contents)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's group by Month and see all the sales\n\n# resample in timeseries is the same as groupby\n# in order it to work, we must set the date column as index, and it must be a datetime format (strings are not valid)\n# when we resample it, we can pass D: daily, W: weekly or M: monthly\n# we can then perform operation on the 'resampled' columns like\n# sum, mean and others.\n\n# calculate the monthly sales\ndf[\"date\"] = pd.to_datetime(df[\"date\"], format = \"%d.%m.%Y\")\nx = df[[\"date\", \"item_cnt_day\"]].set_index(\"date\").resample(\"M\").sum() #resamble (\"M\") significa que quieres agrupar por mes (Y=anuales, W=Semanales...)\nx.head() #Ventas mensuales","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# get the percentile 5 and 95 to plot them on the figure\n\n# plot the data using matplotlib\nplt.figure(figsize = (20, 10))\nplt.plot(x, color = \"blue\", label = \"Monthly sales\")\nplt.title(\"Monthly sales\")\nplt.legend();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From our very first and simple figure, we can already extract very useful information.\n* First of all, we can see big spikes in January, like to be motivated with national holidays in Russia.\n* Second: we see a general trend to decline in our timeseries. If you are planning to use a parametrical model, you must take into account this.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# perform the same operations but on a weekly basis\nx = df[[\"date\", \"item_cnt_day\"]].set_index(\"date\").resample(\"W\").sum()\n\nplt.figure(figsize = (20, 10))\nplt.plot(x.index, x, color = \"blue\", label = \"Weekly sales\")\nplt.title(\"Weekly sales\")\nplt.legend();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Analyzing data on a weekly basis, gives us much more information. We can see more variation between weeks, but the main point stays the same: we have spines in January and sales that go down overtime.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Notice that in the previous examples, we have calculated the percentiles 5 and 95 for ALL years. This might be an issue if we generate such a feature in our model and the lowest or highest sale ocurrs at the end of the year, since we will be using variables from the future and trying to learn. Let's try and adress this issue by generating the percentiles by year. This won't fix the problem, but the visual representation will be different.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"x = df[[\"date\", \"item_cnt_day\"]]\nx[\"YEAR\"] = x[\"date\"].dt.year\nx = x.set_index(\"date\").groupby(\"YEAR\").resample(\"M\")[\"item_cnt_day\"].sum().to_frame().reset_index()\n\nx_95_2013 = [np.percentile(x[x[\"YEAR\"] == 2013][\"item_cnt_day\"], q = 95) for i in range(x[x[\"YEAR\"] == 2013].shape[0])]\n\nx_05_2013 = [np.percentile(x[x[\"YEAR\"] == 2013][\"item_cnt_day\"], q = 5) for i in range(x[x[\"YEAR\"] == 2013].shape[0])]\n\nx_95_2014 = [np.percentile(x[x[\"YEAR\"] == 2014][\"item_cnt_day\"], q = 95) for i in range(x[x[\"YEAR\"] == 2014].shape[0])]\n\nx_05_2014 = [np.percentile(x[x[\"YEAR\"] == 2014][\"item_cnt_day\"], q = 5) for i in range(x[x[\"YEAR\"] == 2014].shape[0])]\n\nx_95_2015 = [np.percentile(x[x[\"YEAR\"] == 2015][\"item_cnt_day\"], q = 95) for i in range(x[x[\"YEAR\"] == 2015].shape[0])]\n\nx_05_2015 = [np.percentile(x[x[\"YEAR\"] == 2015][\"item_cnt_day\"], q = 5) for i in range(x[x[\"YEAR\"] == 2015].shape[0])]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (15, 7.5))\nplt.plot(x[\"date\"], x[\"item_cnt_day\"], color = \"blue\", label = \"Monthly sales\")\n\n# extact the dates for year 2013 and use them as x for the plot\nplt.plot(x[x[\"YEAR\"] == 2013][\"date\"], x_95_2013, color = \"green\")\nplt.plot(x[x[\"YEAR\"] == 2013][\"date\"], x_05_2013, color = \"red\")\n\nplt.plot(x[x[\"YEAR\"] == 2014][\"date\"], x_95_2014, color = \"green\")\nplt.plot(x[x[\"YEAR\"] == 2014][\"date\"], x_05_2014, color = \"red\")\n\nplt.plot(x[x[\"YEAR\"] == 2015][\"date\"], x_95_2015, color = \"green\", label = \"Percentile 95 of Monthly sales in Year 2015\")\nplt.plot(x[x[\"YEAR\"] == 2015][\"date\"], x_05_2015, color = \"red\", label = \"Percentile 5 of Monthly sales in Year 2015\")\nplt.title(\"Monthly sales with percentile 5 and 95 calculated per year\")\nplt.tight_layout()\nplt.legend();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# perform the same operation on a weekly basis\nx = df[[\"date\", \"item_cnt_day\"]]\nx[\"YEAR\"] = x[\"date\"].dt.year\nx = x.set_index(\"date\").groupby(\"YEAR\").resample(\"W\")[\"item_cnt_day\"].sum().to_frame().reset_index()\n\nx_95_2013 = [np.percentile(x[x[\"YEAR\"] == 2013][\"item_cnt_day\"], q = 95) for i in range(x[x[\"YEAR\"] == 2013].shape[0])]\n\nx_05_2013 = [np.percentile(x[x[\"YEAR\"] == 2013][\"item_cnt_day\"], q = 5) for i in range(x[x[\"YEAR\"] == 2013].shape[0])]\n\nx_95_2014 = [np.percentile(x[x[\"YEAR\"] == 2014][\"item_cnt_day\"], q = 95) for i in range(x[x[\"YEAR\"] == 2014].shape[0])]\n\nx_05_2014 = [np.percentile(x[x[\"YEAR\"] == 2014][\"item_cnt_day\"], q = 5) for i in range(x[x[\"YEAR\"] == 2014].shape[0])]\n\nx_95_2015 = [np.percentile(x[x[\"YEAR\"] == 2015][\"item_cnt_day\"], q = 95) for i in range(x[x[\"YEAR\"] == 2015].shape[0])]\n\nx_05_2015 = [np.percentile(x[x[\"YEAR\"] == 2015][\"item_cnt_day\"], q = 5) for i in range(x[x[\"YEAR\"] == 2015].shape[0])]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (15, 7.5))\nplt.plot(x[\"date\"], x[\"item_cnt_day\"], color = \"blue\", label = \"Weekly sales\")\nplt.plot(x[x[\"YEAR\"] == 2013][\"date\"], x_95_2013, color = \"green\")\nplt.plot(x[x[\"YEAR\"] == 2013][\"date\"], x_05_2013, color = \"red\")\n\nplt.plot(x[x[\"YEAR\"] == 2014][\"date\"], x_95_2014, color = \"green\")\nplt.plot(x[x[\"YEAR\"] == 2014][\"date\"], x_05_2014, color = \"red\")\n\nplt.plot(x[x[\"YEAR\"] == 2015][\"date\"], x_95_2015, color = \"green\", label = \"Percentile 95 of Weekly sales in Year 2015\")\nplt.plot(x[x[\"YEAR\"] == 2015][\"date\"], x_05_2015, color = \"red\", label = \"Percentile 5 of Weekly sales in Year 2015\")\nplt.tight_layout()\nplt.legend();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In the next plots we will represent the monthly sales (left plot) and weekly sales (right plot) for each shop. \n\nWe will also plot the percentile 5 and 95 for each shop by year.\n\nIn the light red/pink areas of each plot, we will mark the national holidays in Russia and see if there is any connection with sales spikes.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"russian_holidays_start = [\ndatetime.datetime(2013, 1, 1),\ndatetime.datetime(2013, 2, 23),\ndatetime.datetime(2013, 3, 8),\ndatetime.datetime(2013, 5, 1),\ndatetime.datetime(2013, 5, 9),\ndatetime.datetime(2013, 6, 12),\ndatetime.datetime(2013, 11, 4),\n\ndatetime.datetime(2014, 1, 1),\ndatetime.datetime(2014, 2, 23),\ndatetime.datetime(2014, 3, 8),\ndatetime.datetime(2014, 5, 1),\ndatetime.datetime(2014, 5, 9),\ndatetime.datetime(2014, 6, 12),\ndatetime.datetime(2014, 11, 4),\n\ndatetime.datetime(2015, 1, 1),\ndatetime.datetime(2015, 2, 23),\ndatetime.datetime(2015, 3, 8),\ndatetime.datetime(2015, 5, 1),\ndatetime.datetime(2015, 5, 9),\ndatetime.datetime(2015, 6, 12),\ndatetime.datetime(2015, 11, 4)\n]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"russian_holidays_end = [\ndatetime.datetime(2013, 1, 8),\ndatetime.datetime(2013, 2, 23),\ndatetime.datetime(2013, 3, 8),\ndatetime.datetime(2013, 5, 1),\ndatetime.datetime(2013, 5, 9),\ndatetime.datetime(2013, 6, 12),\ndatetime.datetime(2013, 11, 4),\n\ndatetime.datetime(2014, 1, 8),\ndatetime.datetime(2014, 2, 23),\ndatetime.datetime(2014, 3, 8),\ndatetime.datetime(2014, 5, 1),\ndatetime.datetime(2014, 5, 9),\ndatetime.datetime(2014, 6, 12),\ndatetime.datetime(2014, 11, 4),\n\ndatetime.datetime(2015, 1, 8),\ndatetime.datetime(2015, 2, 23),\ndatetime.datetime(2015, 3, 8),\ndatetime.datetime(2015, 5, 1),\ndatetime.datetime(2015, 5, 9),\ndatetime.datetime(2015, 6, 12),\ndatetime.datetime(2015, 11, 4)\n]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for iterable in sorted(list(df[\"shop_name\"].unique())):\n\n    # create the size of the figure\n    plt.figure(figsize = (20, 10))\n\n    # create the subplot for Monthly sales of the each shop\n    plt.subplot(1, 2, 1)\n    \n    # calculate the Monthly sales of each shop\n    short_df = df[df[\"shop_name\"] == iterable][[\"date\",\"item_cnt_day\"]]\n    short_df[\"date\"] = pd.to_datetime(short_df[\"date\"], format = \"%d.%m.%Y\")\n    short_df[\"YEAR\"] = short_df[\"date\"].dt.year\n    short_df = short_df.set_index(\"date\").groupby(\"YEAR\").resample(\"M\")[\"item_cnt_day\"].sum()\n    short_df = short_df.reset_index()\n    \n    # adding moving average\n    short_df[\"MA3M\"] = short_df[\"item_cnt_day\"].rolling(window = 3).mean() #Para crear una media historica y aplanar la grafica con mÃ¡s datos\n    short_df[\"MA4M\"] = short_df[\"item_cnt_day\"].rolling(window = 4).mean() #Window es la ventanda historica de cuantos datos atras quieres hacer el calculo\n    short_df[\"MA5M\"] = short_df[\"item_cnt_day\"].rolling(window = 5).mean()\n    \n    # assing the data to plot\n    sales = short_df[\"item_cnt_day\"]\n    dates = short_df[\"date\"]\n    sales_95_global = [np.percentile(sales, q = 95) for i in range(len(sales))]\n    sales_5_global = [np.percentile(sales, q = 5) for i in range(len(sales))]\n    \n    average_3_months = short_df[\"MA3M\"]\n    average_4_months = short_df[\"MA4M\"]\n    average_5_months = short_df[\"MA5M\"]\n    \n    # percentile 5 and 95 of year 2013\n    sales_95_2013 = [np.percentile(short_df[short_df[\"YEAR\"] == 2013][\"item_cnt_day\"], q = 95) for i in range(short_df[short_df[\"YEAR\"] == 2013].shape[0])]\n    sales_5_2013 = [np.percentile(short_df[short_df[\"YEAR\"] == 2013][\"item_cnt_day\"], q = 5) for i in range(short_df[short_df[\"YEAR\"] == 2013].shape[0])]\n    dates_2013 = short_df[short_df[\"YEAR\"] == 2013][\"date\"]\n    \n    # percentile 5 and 95 of year 2014\n    sales_95_2014 = [np.percentile(short_df[short_df[\"YEAR\"] == 2014][\"item_cnt_day\"], q = 95) for i in range(short_df[short_df[\"YEAR\"] == 2014].shape[0])]\n    sales_5_2014 = [np.percentile(short_df[short_df[\"YEAR\"] == 2014][\"item_cnt_day\"], q = 5) for i in range(short_df[short_df[\"YEAR\"] == 2014].shape[0])]\n    dates_2014 = short_df[short_df[\"YEAR\"] == 2014][\"date\"]\n    \n    # percentile 5 and 95 of year 2015\n    sales_95_2015 = [np.percentile(short_df[short_df[\"YEAR\"] == 2015][\"item_cnt_day\"], q = 95) for i in range(short_df[short_df[\"YEAR\"] == 2015].shape[0])]\n    sales_5_2015 = [np.percentile(short_df[short_df[\"YEAR\"] == 2015][\"item_cnt_day\"], q = 5) for i in range(short_df[short_df[\"YEAR\"] == 2015].shape[0])]\n    dates_2015 = short_df[short_df[\"YEAR\"] == 2015][\"date\"]\n\n    # plot the data and add label\n    plt.plot(dates, sales, 'o-', label = \"Monthly sales\")\n    \n    plt.plot(dates, average_3_months, '.-', label = \"Average sales of the last 3 months\")\n    \n    plt.plot(dates, sales_95_global, '-', color = \"black\", label = \"P95 of Monthly sales over all years\")\n    plt.plot(dates, sales_5_global, '-', color = \"magenta\", label = \"P5 of Monthly sales over all years\")\n    \n    plt.plot(dates_2013, sales_95_2013, \"--\", color = \"green\")\n    plt.plot(dates_2013, sales_5_2013, \":\", color = \"red\")\n    \n    plt.plot(dates_2014, sales_95_2014, \"--\", color = \"green\")\n    plt.plot(dates_2014, sales_5_2014, \":\", color = \"red\")\n    \n    plt.plot(dates_2015, sales_95_2015, \"--\", color = \"green\", label = \"P95 of Monthly sales by year\")\n    plt.plot(dates_2015, sales_5_2015, \":\", color = \"red\", label = \"P5 of Monthly sales by year\")\n\n    # get current axis and plot the areas\n    ax = plt.gca()\n    alpha = 0.2\n    \n    for start_date, end_date in zip(russian_holidays_start, russian_holidays_end):\n        \n        # add shaded areas for holidays 2013\n        ax.axvspan(start_date, end_date, alpha = alpha, color = 'red')    \n       \n    # add title and show legend    \n    plt.title('Monthly sales of shop {}'.format(iterable))\n    plt.ylabel('Total Monthly sales of shop {}'.format(iterable))\n    plt.xlabel(\"Time grouped by month\")\n    plt.legend()\n    \n    #######################################################################################\n    # Weekly sales\n    #######################################################################################\n    \n    plt.subplot(1, 2, 2)\n    \n      # calculate the Weekly sales of each shop\n    short_df = df[df[\"shop_name\"] == iterable][[\"date\",\"item_cnt_day\"]]\n    short_df[\"date\"] = pd.to_datetime(short_df[\"date\"], format = \"%d.%m.%Y\")\n    short_df[\"YEAR\"] = short_df[\"date\"].dt.year\n    short_df = short_df.set_index(\"date\").groupby(\"YEAR\").resample(\"W\")[\"item_cnt_day\"].sum()\n    short_df = short_df.reset_index()\n    \n    # adding moving average\n    short_df[\"MA3W\"] = short_df[\"item_cnt_day\"].rolling(window=3).mean()\n    short_df[\"MA4W\"] = short_df[\"item_cnt_day\"].rolling(window=4).mean()\n    short_df[\"MA5W\"] = short_df[\"item_cnt_day\"].rolling(window=5).mean()\n    \n    # assing the data to plot\n    \n    # general sales\n    sales = short_df[\"item_cnt_day\"]\n    dates = short_df[\"date\"]\n    sales_95_global = [np.percentile(sales, q = 95) for i in range(len(sales))]\n    sales_5_global = [np.percentile(sales, q = 5) for i in range(len(sales))]\n    \n    average_3_weeks = short_df[\"MA3W\"]\n    average_4_weeks = short_df[\"MA4W\"]\n    average_5_weeks = short_df[\"MA5W\"]\n    \n    # percentile 5 and 95 of year 2013\n    sales_95_2013 = [np.percentile(short_df[short_df[\"YEAR\"] == 2013][\"item_cnt_day\"], q = 95) for i in range(short_df[short_df[\"YEAR\"] == 2013].shape[0])]\n    sales_5_2013 = [np.percentile(short_df[short_df[\"YEAR\"] == 2013][\"item_cnt_day\"], q = 5) for i in range(short_df[short_df[\"YEAR\"] == 2013].shape[0])]\n    dates_2013 = short_df[short_df[\"YEAR\"] == 2013][\"date\"]\n    \n    # percentile 5 and 95 of year 2014\n    sales_95_2014 = [np.percentile(short_df[short_df[\"YEAR\"] == 2014][\"item_cnt_day\"], q = 95) for i in range(short_df[short_df[\"YEAR\"] == 2014].shape[0])]\n    sales_5_2014 = [np.percentile(short_df[short_df[\"YEAR\"] == 2014][\"item_cnt_day\"], q = 5) for i in range(short_df[short_df[\"YEAR\"] == 2014].shape[0])]\n    dates_2014 = short_df[short_df[\"YEAR\"] == 2014][\"date\"]\n    \n    # percentile 5 and 95 of year 2015\n    sales_95_2015 = [np.percentile(short_df[short_df[\"YEAR\"] == 2015][\"item_cnt_day\"], q = 95) for i in range(short_df[short_df[\"YEAR\"] == 2015].shape[0])]\n    sales_5_2015 = [np.percentile(short_df[short_df[\"YEAR\"] == 2015][\"item_cnt_day\"], q = 5) for i in range(short_df[short_df[\"YEAR\"] == 2015].shape[0])]\n    dates_2015 = short_df[short_df[\"YEAR\"] == 2015][\"date\"]\n\n    # plot the data and add label\n    plt.plot(dates, sales, 'o-', label = \"Weekly sales\")\n    plt.plot(dates, average_3_weeks, '.-', label = \"Average sales of the last 3 weeks\")\n    \n    plt.plot(dates, sales_95_global, '-', color = \"black\", label = \"P95 of Weekly sales over all years\")\n    plt.plot(dates, sales_5_global, '-', color = \"magenta\", label = \"P5 of Weekly sales over all years\")\n    \n    plt.plot(dates_2013, sales_95_2013, \"--\", color = \"green\")\n    plt.plot(dates_2013, sales_5_2013, \":\", color = \"red\")\n    \n    plt.plot(dates_2014, sales_95_2014, \"--\", color = \"green\")\n    plt.plot(dates_2014, sales_5_2014, \":\", color = \"red\")\n    \n    plt.plot(dates_2015, sales_95_2015, \"--\", color = \"green\", label = \"P95 of Weekly sales by year\")\n    plt.plot(dates_2015, sales_5_2015, \":\", color = \"red\", label = \"P5 of Weekly sales by year\")\n    \n    # get current axis and plot the areas\n    ax = plt.gca()\n    \n    for start_date, end_date in zip(russian_holidays_start, russian_holidays_end):\n        \n        # add shaded areas for holidays 2013\n        ax.axvspan(start_date, end_date, alpha = alpha, color = 'red')\n    \n    # add title and show legend\n    plt.title('Weekly sales of shop {}'.format(iterable))\n    plt.ylabel('Total Weekly sales of shop {}'.format(iterable))\n    plt.xlabel(\"Time grouped by week\")\n    plt.legend()\n    \n    # general sales\n    plt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In the next plots we will represent the monthly sales (left plot) and weekly sales (right plot) for each item category. \n\nWe will also plot the percentile 5 and 95 for each shop by year.\n\nIn the light red/pink areas of each plot, we will mark the national holidays in Russia and see if there is any connection with sales spikes.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"for iterable in sorted(list(df[\"item_category_name\"].unique())):\n\n    # create the size of the figure\n    plt.figure(figsize = (20, 10))\n\n    # create the subplot for Monthly sales of the each shop\n    plt.subplot(1, 2, 1)\n    \n    # calculate the Monthly sales of each shop\n    short_df = df[df[\"item_category_name\"] == iterable][[\"date\",\"item_cnt_day\"]]\n    short_df[\"date\"] = pd.to_datetime(short_df[\"date\"], format = \"%d.%m.%Y\")\n    short_df[\"YEAR\"] = short_df[\"date\"].dt.year\n    short_df = short_df.set_index(\"date\").groupby(\"YEAR\").resample(\"M\")[\"item_cnt_day\"].sum()\n    short_df = short_df.reset_index()\n    \n    # adding moving average\n    short_df[\"MA3M\"] = short_df[\"item_cnt_day\"].rolling(window=3).mean()\n    short_df[\"MA4M\"] = short_df[\"item_cnt_day\"].rolling(window=4).mean()\n    short_df[\"MA5M\"] = short_df[\"item_cnt_day\"].rolling(window=5).mean()\n    \n    # assing the data to plot\n    sales = short_df[\"item_cnt_day\"]\n    dates = short_df[\"date\"]\n    sales_95_global = [np.percentile(sales, q = 95) for i in range(len(sales))]\n    sales_5_global = [np.percentile(sales, q = 5) for i in range(len(sales))]\n    \n    average_3_months = short_df[\"MA3M\"]\n    average_4_months = short_df[\"MA4M\"]\n    average_5_months = short_df[\"MA5M\"]\n    \n    # percentile 5 and 95 of year 2013\n    sales_95_2013 = [np.percentile(short_df[short_df[\"YEAR\"] == 2013][\"item_cnt_day\"], q = 95) for i in range(short_df[short_df[\"YEAR\"] == 2013].shape[0])]\n    sales_5_2013 = [np.percentile(short_df[short_df[\"YEAR\"] == 2013][\"item_cnt_day\"], q = 5) for i in range(short_df[short_df[\"YEAR\"] == 2013].shape[0])]\n    dates_2013 = short_df[short_df[\"YEAR\"] == 2013][\"date\"]\n    \n    # percentile 5 and 95 of year 2014\n    sales_95_2014 = [np.percentile(short_df[short_df[\"YEAR\"] == 2014][\"item_cnt_day\"], q = 95) for i in range(short_df[short_df[\"YEAR\"] == 2014].shape[0])]\n    sales_5_2014 = [np.percentile(short_df[short_df[\"YEAR\"] == 2014][\"item_cnt_day\"], q = 5) for i in range(short_df[short_df[\"YEAR\"] == 2014].shape[0])]\n    dates_2014 = short_df[short_df[\"YEAR\"] == 2014][\"date\"]\n    \n    # percentile 5 and 95 of year 2015\n    sales_95_2015 = [np.percentile(short_df[short_df[\"YEAR\"] == 2015][\"item_cnt_day\"], q = 95) for i in range(short_df[short_df[\"YEAR\"] == 2015].shape[0])]\n    sales_5_2015 = [np.percentile(short_df[short_df[\"YEAR\"] == 2015][\"item_cnt_day\"], q = 5) for i in range(short_df[short_df[\"YEAR\"] == 2015].shape[0])]\n    dates_2015 = short_df[short_df[\"YEAR\"] == 2015][\"date\"]\n\n    # plot the data and add label\n    plt.plot(dates, sales, 'o-', label = \"Monthly sales\")\n    \n    plt.plot(dates, average_3_months, '.-', label = \"Average sales of the last 3 months\")\n    \n    plt.plot(dates, sales_95_global, '-', color = \"black\", label = \"P95 of Monthly sales over all years\")\n    plt.plot(dates, sales_5_global, '-', color = \"magenta\", label = \"P5 of Monthly sales over all years\")\n    \n    plt.plot(dates_2013, sales_95_2013, \"--\", color = \"green\")\n    plt.plot(dates_2013, sales_5_2013, \":\", color = \"red\")\n    \n    plt.plot(dates_2014, sales_95_2014, \"--\", color = \"green\")\n    plt.plot(dates_2014, sales_5_2014, \":\", color = \"red\")\n    \n    plt.plot(dates_2015, sales_95_2015, \"--\", color = \"green\", label = \"P95 of Monthly sales by year\")\n    plt.plot(dates_2015, sales_5_2015, \":\", color = \"red\", label = \"P5 of Monthly sales by year\")\n\n    # get current axis and plot the areas\n    ax = plt.gca()\n    alpha = 0.2\n    \n    for start_date, end_date in zip(russian_holidays_start, russian_holidays_end):\n        \n        # add shaded areas for holidays 2013\n        ax.axvspan(start_date, end_date, alpha = alpha, color = 'red')   \n    \n    # add title and show legend\n    plt.title('Monthly sales of item category {}'.format(iterable))\n    plt.ylabel('Total Monthly sales of item category {}'.format(iterable))\n    plt.xlabel(\"Time grouped by month\")\n    plt.legend()\n    \n\n    #######################################################################################\n    # Weekly sales\n    #######################################################################################\n    \n    plt.subplot(1, 2, 2)\n    \n      # calculate the Weekly sales of each shop\n    short_df = df[df[\"item_category_name\"] == iterable][[\"date\",\"item_cnt_day\"]]\n    short_df[\"date\"] = pd.to_datetime(short_df[\"date\"], format = \"%d.%m.%Y\")\n    short_df[\"YEAR\"] = short_df[\"date\"].dt.year\n    short_df = short_df.set_index(\"date\").groupby(\"YEAR\").resample(\"W\")[\"item_cnt_day\"].sum()\n    short_df = short_df.reset_index()\n    \n    # adding moving average\n    short_df[\"MA3W\"] = short_df[\"item_cnt_day\"].rolling(window = 3).mean()\n    short_df[\"MA4W\"] = short_df[\"item_cnt_day\"].rolling(window = 4).mean()\n    short_df[\"MA5W\"] = short_df[\"item_cnt_day\"].rolling(window = 5).mean()\n    \n    # assing the data to plot\n    \n    # general sales\n    sales = short_df[\"item_cnt_day\"]\n    dates = short_df[\"date\"]\n    sales_95_global = [np.percentile(sales, q = 95) for i in range(len(sales))]\n    sales_5_global = [np.percentile(sales, q = 5) for i in range(len(sales))]\n    \n    average_3_weeks = short_df[\"MA3W\"]\n    average_4_weeks = short_df[\"MA4W\"]\n    average_5_weeks = short_df[\"MA5W\"]\n    \n    # percentile 5 and 95 of year 2013\n    sales_95_2013 = [np.percentile(short_df[short_df[\"YEAR\"] == 2013][\"item_cnt_day\"], q = 95) for i in range(short_df[short_df[\"YEAR\"] == 2013].shape[0])]\n    sales_5_2013 = [np.percentile(short_df[short_df[\"YEAR\"] == 2013][\"item_cnt_day\"], q = 5) for i in range(short_df[short_df[\"YEAR\"] == 2013].shape[0])]\n    dates_2013 = short_df[short_df[\"YEAR\"] == 2013][\"date\"]\n    \n    # percentile 5 and 95 of year 2014\n    sales_95_2014 = [np.percentile(short_df[short_df[\"YEAR\"] == 2014][\"item_cnt_day\"], q = 95) for i in range(short_df[short_df[\"YEAR\"] == 2014].shape[0])]\n    sales_5_2014 = [np.percentile(short_df[short_df[\"YEAR\"] == 2014][\"item_cnt_day\"], q = 5) for i in range(short_df[short_df[\"YEAR\"] == 2014].shape[0])]\n    dates_2014 = short_df[short_df[\"YEAR\"] == 2014][\"date\"]\n    \n    # percentile 5 and 95 of year 2015\n    sales_95_2015 = [np.percentile(short_df[short_df[\"YEAR\"] == 2015][\"item_cnt_day\"], q = 95) for i in range(short_df[short_df[\"YEAR\"] == 2015].shape[0])]\n    sales_5_2015 = [np.percentile(short_df[short_df[\"YEAR\"] == 2015][\"item_cnt_day\"], q = 5) for i in range(short_df[short_df[\"YEAR\"] == 2015].shape[0])]\n    dates_2015 = short_df[short_df[\"YEAR\"] == 2015][\"date\"]\n\n    # plot the data and add label\n    plt.plot(dates, sales, 'o-', label = \"Weekly sales\")\n    plt.plot(dates, average_3_weeks, '.-', label = \"Average sales of the last 3 weeks\")\n    \n    plt.plot(dates, sales_95_global, '-', color = \"black\", label = \"P95 of Weekly sales over all years\")\n    plt.plot(dates, sales_5_global, '-', color = \"magenta\", label = \"P5 of Weekly sales over all years\")\n    \n    plt.plot(dates_2013, sales_95_2013, \"--\", color = \"green\")\n    plt.plot(dates_2013, sales_5_2013, \":\", color = \"red\")\n    \n    plt.plot(dates_2014, sales_95_2014, \"--\", color = \"green\")\n    plt.plot(dates_2014, sales_5_2014, \":\", color = \"red\")\n    \n    plt.plot(dates_2015, sales_95_2015, \"--\", color = \"green\", label = \"P95 of Weekly sales by year\")\n    plt.plot(dates_2015, sales_5_2015, \":\", color = \"red\", label = \"P5 of Weekly sales by year\")\n    \n    # get current axis and plot the areas\n    ax = plt.gca()\n    \n    for start_date, end_date in zip(russian_holidays_start, russian_holidays_end):\n        \n        # add shaded areas for holidays 2013\n        ax.axvspan(start_date, end_date, alpha = alpha, color = 'red')\n        \n    # add title and show legend\n    plt.title('Weekly sales of item category {}'.format(iterable))\n    plt.ylabel('Total Weekly sales of item category {}'.format(iterable))\n    plt.xlabel(\"Time grouped by week\")\n    plt.legend()\n    # general sales\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Total sales and the variation on secondary axis\n[Go back to the Table of Contents](#table_of_contents)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# we can observe a general trend of decrasing sales.\n# let's add a second axis to see the variation of intradays sales\n\n# select the columns of interest\ndf_var = df[[\"date\", \"item_cnt_day\"]]\n\n# convert to datetime\ndf_var[\"date\"] = pd.to_datetime(df[\"date\"], format = \"%d.%m.%Y\")\n\n# set date as index\ndf_var.set_index(\"date\", inplace = True)\n\n# resample/groupby by date and convert to frame the total daily sales\ndf_var = df_var.resample(\"M\")[\"item_cnt_day\"].sum().to_frame()\n\n# calculate the intra week variation between total sales\ndf_var[\"Variation\"] = df_var[\"item_cnt_day\"].diff()/df_var[\"item_cnt_day\"].shift(1)\n\ndf_var.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# separate x and y\ny_sales = df_var[\"item_cnt_day\"]\ny_variation = df_var[\"Variation\"]\n\n# instanciate the figure\nfig = plt.figure(figsize = (15, 10))\nax = fig.add_subplot(111)\n\n# plot the total sales\nplot1 = ax.plot(y_sales, label = \"Total weekly sales\", color = \"blue\", alpha = 0.5)\n\n# create a secondary axis and plot the variation data\nax_bis = ax.twinx()\nplot2 = ax_bis.plot(y_variation, label = \"Intra - week variation of sales\", color = \"red\", alpha = 0.5)\n\n# create a common legend for both plots\nlns = plot1 + plot2\nlabs = [l.get_label() for l in lns]\nax.legend(lns, labs, loc = \"upper left\")\n\n# add a custom title to the plot\nax.set_title(\"Total weekly sales and variation\");","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Question 1: Create a plot with the moving average of total sales (7 days) and the variation on the second axis.\n[Go back to the Table of Contents](#table_of_contents)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# start with the regular df\ndf_for_question_1 = create_df()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_for_question_1.info()\n#Queda pendiente convertir la fecha en el formato correcto","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_for_question_1['date'].head()\n#Miramos el formato de la fecha","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_for_question_1['date'] = pd.to_datetime(df_for_question_1['date'], format = \"%d.%m.%Y\")\n#Hacemos la conversiÃ³n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_for_question_1.info()\n#Revisamos que sale correcto","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_for_question_1.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_sales_7 = df_for_question_1[[\"date\", \"item_cnt_day\"]]\ndf_sales_7.set_index(\"date\", inplace=True)\ndf_sales_7.head()\n#Creamos un nuevo dataset con las fechas y las ventas que es lo que nos interesa para graficar","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_sales_7 = df_sales_7.resample(\"D\")[\"item_cnt_day\"].sum().to_frame()\ndf_sales_7.head()\n#Agrupamos en ventas por dÃ­a y volvemos a convertir en Dataframe","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#AÃ±adimos una columna con la media mÃ³vil de 7 dÃ­as\ndf_sales_7[\"MA7D\"] = df_sales_7[\"item_cnt_day\"].rolling(window = 7).mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Creamos una columna con la variacion de la media mÃ³vil\ndf_sales_7[\"Variation\"] = df_sales_7[\"MA7D\"].diff()/df_sales_7[\"MA7D\"].shift()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_sales_7.head(20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Definimos las variables con las que vamos a hacer el grafico\nMA7D = df_sales_7[\"MA7D\"]\nvariation = df_sales_7[\"Variation\"]\n\n# Instanciamos la figura\nfig = plt.figure(figsize = (25, 10))\nax = fig.add_subplot(111)\n\n# Generamos el grÃ¡fico de la media mÃ³vil de las ventas\nplot1 = ax.plot(MA7D, label = \"Media mÃ³vil de ventas de 7 dÃ­as\", color = \"green\", alpha = 0.5)\n\n# Generamos un axis secundario y el grafico de la variaciÃ³n de la media mÃ³vil\nax_bis = ax.twinx()\nplot2 = ax_bis.plot(variation, label = \"VariaciÃ³n media mÃ³vil\", color = \"orange\", alpha = 0.5)\n\n# Agrupamos los grÃ¡ficos y creamos una leyenda conjunta\nlns = plot1 + plot2\nlabs = [l.get_label() for l in lns]\nax.legend(lns, labs, loc = \"upper left\")\n\n# AÃ±adimos el tÃ­tulo\nax.set_title(\"Promedio mÃ³vil de 7 dÃ­as de las ventas totales\");","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Calendar heatmap\n[Go back to the Table of Contents](#table_of_contents)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# calendar heatmaps are really useful to see the overall activity for a certain period of time per day and per month.\n# let's build one using python.\n# we will be using the calmap package for this, because it makes it extremenly easy to plot this data\n# select the columns\ndf_calendar = df[[\"date\", \"item_cnt_day\"]]\n\n# set date as index and resample\ndf_calendar.set_index(\"date\", inplace = True)\n# notice that this time, we don't convert it to_frame()\n# df_calendar is a pandas series\n# THIS IS IMPORTANT since calmap expects a series\n# with a datetime index and the values to plot\ndf_calendar = df_calendar.resample(\"D\")[\"item_cnt_day\"].sum()\n\n# ----------------------------------------------------------------------------------------------------\n# plot the data using calmap\ncalmap.calendarplot(df_calendar, # pass the series\n                    fig_kws = {'figsize': (16,10)}, \n                    yearlabel_kws = {'color':'black', 'fontsize':14}, \n                    subplot_kws = {'title':'Total sales per year'}\n                   );","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Timeseries autocorrelation and partial autocorrelation plots: daily sales\n[Go back to the Table of Contents](#table_of_contents)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# This plot are fundamental in timeseries analysis.\n# Basically here we compare the a series again itself but with some lags.\n# These are plots that graphically summarize the strength of a relationship with an observation in a time series with observations at prior time steps.\n\n# More info: \n# https://machinelearningmastery.com/gentle-introduction-autocorrelation-partial-autocorrelation/\n\n# ----------------------------------------------------------------------------------------------------\n# instanciate the figure\nfig, (ax1, ax2) = plt.subplots(1, 2,figsize = (16,6), dpi = 80)\n\n# ----------------------------------------------------------------------------------------------------\n# plot the data using the built in plots from the stats module\n\n# The AutoCorrelation plot: compares a value v with the value v but n times in the past.\nplot_acf(df.set_index(\"date\").resample(\"D\")[\"item_cnt_day\"].sum(), ax = ax1, lags = 7)\n\n# The Parcial AutoCorrelation plot: partial autocorrelation at lag k is the correlation that results after removing the effect of any correlations due to the terms at shorter lags.\nplot_pacf(df.set_index(\"date\").resample(\"D\")[\"item_cnt_day\"].sum(), ax = ax2, lags = 7);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# This code snippets show you have to calculate the Partial Autocorrelation\n# Partial Autocorrelation can be very counter intuitive since in some of our steps we are fitting a linear model\n# to predict the values of t - 2 using t - 1\n# Wait, what? Why we use values from yesterday to predict values before yesterday?\n# Basically because we assume that our timeseries is auto regressive. This means that the data at point t captures\n# all the variance/information from all the previuos data points.\n# This way, t - 1, must have captured all the variance from previous points, thus t - 2, and so t - 1 becomes\n# a good predictor for values from t - 2.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# create a dataframe with total sales per day (all shops and all items)\ndf_total_sales = df.set_index(\"date\").resample(\"D\")[\"item_cnt_day\"].sum().to_frame()\n\n# rename the column item_cnt_day to total_sales\ndf_total_sales.columns = [\"total_sales\"]\n\n# create a few features that we need in order to calculate the parcial autocorrelation\ndf_total_sales[\"T-1\"] = df_total_sales[\"total_sales\"].shift(1)\ndf_total_sales[\"T-2\"] = df_total_sales[\"total_sales\"].shift(2)\n\n# we have a few nan for the first 2 rows so we must drop them\nprint(df_total_sales.shape)\ndf_total_sales.dropna(axis = \"rows\", inplace = True)\nprint(df_total_sales.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# instanciate the Linear model\nmodel = LinearRegression()\n\n# separate X and y\nX = df_total_sales[[\"T-1\"]]\ny = df_total_sales[\"total_sales\"]\n\n# fit and predict with the model\nmodel.fit(X, y)\npredictions = model.predict(X)\n\n# save our predictions to the total_sales df\ndf_total_sales[\"total_sales_from_T-1\"] = predictions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# instanciate the Linear model\nmodel = LinearRegression()\n\n# separate X and y\nX = df_total_sales[[\"T-1\"]]\ny = df_total_sales[\"T-2\"]\n\n# fit and predict with the model\nmodel.fit(X, y)\npredictions = model.predict(X)\n\n# save our predictions to the total_sales df\ndf_total_sales[\"T-2_from_T-1\"] = predictions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# calculate the residual\n# this means: total_sales - total_sales_from_T-1\n# and: T-2 - \"T-2_from_T-1\"\ndf_total_sales[\"Residual_total_sales_T-1\"] = df_total_sales[\"total_sales\"] - df_total_sales[\"total_sales_from_T-1\"]\n\n# this step is very important based on the asumptions we have about many of the timeseries\n# for more information I recommend this read\n# https://towardsdatascience.com/understanding-partial-auto-correlation-fa39271146ac\ndf_total_sales[\"Residual_T-2_T-1\"] = df_total_sales[\"T-2\"] - df_total_sales[\"T-2_from_T-1\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# calculathe the parcial autocorrelation using manual method\nmanual_pacf = df_total_sales.corr(method = \"pearson\")[\"Residual_total_sales_T-1\"][\"Residual_T-2_T-1\"]\nprint(\"Manual parcial autocorrelation method {}\".format(round(manual_pacf, 5)))\n\n# calculate the parcial autocorrelation using statsmodel package\nstats_pacf = pacf(df_total_sales['total_sales'], nlags = 2)[2]\nprint(\"Parcial autocorrelation method using stats package {}\".format(round(stats_pacf, 5)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Timeseries autocorrelation and partial autocorrelation plots: weekly sales\n[Go back to the Table of Contents](#table_of_contents)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# ----------------------------------------------------------------------------------------------------\n# instanciate the figure\nfig, (ax1, ax2) = plt.subplots(1, 2,figsize = (16,6), dpi = 80)\n\n# ----------------------------------------------------------------------------------------------------\n# plot the data using the built in plots from the stats module\nplot_acf(df.set_index(\"date\").resample(\"W\")[\"item_cnt_day\"].sum(), ax = ax1, lags = 8)\nplot_pacf(df.set_index(\"date\").resample(\"W\")[\"item_cnt_day\"].sum(), ax = ax2, lags = 8);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Timeseries autocorrelation and partial autocorrelation plots: monthly sales\n[Go back to the Table of Contents](#table_of_contents)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# ----------------------------------------------------------------------------------------------------\n# instanciate the figure\nfig, (ax1, ax2) = plt.subplots(1, 2,figsize = (16,6), dpi = 80)\n\n# ----------------------------------------------------------------------------------------------------\n# plot the data using the built in plots from the stats module\nplot_acf(df.set_index(\"date\").resample(\"M\")[\"item_cnt_day\"].sum(), ax = ax1, lags = 13)\nplot_pacf(df.set_index(\"date\").resample(\"M\")[\"item_cnt_day\"].sum(), ax = ax2, lags = 13);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Timeseries decomposition plots: daily sales\n[Go back to the Table of Contents](#table_of_contents)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Useful for:\n# The theory behind timeseries, says that a series can be decomposed into 3 parts\n# The trend\n# The seasonal part\n# And the residual\n# This plots shows how to do this\n\n# More info: \n# https://machinelearningmastery.com/decompose-time-series-data-trend-seasonality/\n\ndf_timeindex = df.set_index(\"date\").resample(\"D\")[\"item_cnt_day\"].sum().to_frame()\n\n# decompose the series using stats module\n# results in this case is a special class \n# whose attributes we can acess\nresult = seasonal_decompose(df_timeindex[\"item_cnt_day\"])\n\n# ----------------------------------------------------------------------------------------------------\n# instanciate the figure\n# make the subplots share teh x axis\nfig, axes = plt.subplots(ncols = 1, nrows = 4, sharex = True, figsize = (12,10))\n\n# ----------------------------------------------------------------------------------------------------\n# plot the data\n# using this cool thread:\n# https://stackoverflow.com/questions/45184055/how-to-plot-multiple-seasonal-decompose-plots-in-one-figure\n# This allows us to have more control over the plots\n\n# plot the original data\nresult.observed.plot(ax = axes[0], legend = False)\naxes[0].set_ylabel('Observed')\naxes[0].set_title(\"Decomposition of a series\")\n\n# plot the trend\nresult.trend.plot(ax = axes[1], legend = False)\naxes[1].set_ylabel('Trend')\n\n# plot the seasonal part\nresult.seasonal.plot(ax = axes[2], legend = False)\naxes[2].set_ylabel('Seasonal')\n\n# plot the residual\nresult.resid.plot(ax = axes[3], legend = False)\naxes[3].set_ylabel('Residual')\n\n# ----------------------------------------------------------------------------------------------------\n# prettify the plot\n\n# get the xticks and the xticks labels\nxtick_location = df_timeindex.index.tolist()\n\n# set the xticks to be every 6'th entry\n# every 6 months\nax.set_xticks(xtick_location);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Timeseries decomposition plots: weekly sales\n[Go back to the Table of Contents](#table_of_contents)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_timeindex = df.set_index(\"date\").resample(\"W\")[\"item_cnt_day\"].sum().to_frame()\n\n# decompose the series using stats module\n# results in this case is a special class \n# whose attributes we can acess\nresult = seasonal_decompose(df_timeindex[\"item_cnt_day\"])\n\n# ----------------------------------------------------------------------------------------------------\n# instanciate the figure\n# make the subplots share teh x axis\nfig, axes = plt.subplots(ncols = 1, nrows = 4, sharex = True, figsize = (12,10))\n\n# ----------------------------------------------------------------------------------------------------\n# plot the data\n# using this cool thread:\n# https://stackoverflow.com/questions/45184055/how-to-plot-multiple-seasonal-decompose-plots-in-one-figure\n# This allows us to have more control over the plots\n\n# plot the original data\nresult.observed.plot(ax = axes[0], legend = False)\naxes[0].set_ylabel('Observed')\naxes[0].set_title(\"Decomposition of a series\")\n\n# plot the trend\nresult.trend.plot(ax = axes[1], legend = False)\naxes[1].set_ylabel('Trend')\n\n# plot the seasonal part\nresult.seasonal.plot(ax = axes[2], legend = False)\naxes[2].set_ylabel('Seasonal')\n\n# plot the residual\nresult.resid.plot(ax = axes[3], legend = False)\naxes[3].set_ylabel('Residual')\n\n# ----------------------------------------------------------------------------------------------------\n# prettify the plot\n\n# get the xticks and the xticks labels\nxtick_location = df_timeindex.index.tolist()\n\n# set x_ticks\nax.set_xticks(xtick_location);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Timeseries decomposition plots: monthly sales\n[Go back to the Table of Contents](#table_of_contents)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_timeindex = df.set_index(\"date\").resample(\"M\")[\"item_cnt_day\"].sum().to_frame()\n\n# decompose the series using stats module\n# results in this case is a special class \n# whose attributes we can acess\nresult = seasonal_decompose(df_timeindex[\"item_cnt_day\"])\n\n# ----------------------------------------------------------------------------------------------------\n# instanciate the figure\n# make the subplots share teh x axis\nfig, axes = plt.subplots(ncols = 1, nrows = 4, sharex = True, figsize = (12,10))\n\n# ----------------------------------------------------------------------------------------------------\n# plot the data\n# using this cool thread:\n# https://stackoverflow.com/questions/45184055/how-to-plot-multiple-seasonal-decompose-plots-in-one-figure\n# This allows us to have more control over the plots\n\n# plot the original data\nresult.observed.plot(ax = axes[0], legend = False)\naxes[0].set_ylabel('Observed')\naxes[0].set_title(\"Decomposition of a series\")\n\n# plot the trend\nresult.trend.plot(ax = axes[1], legend = False)\naxes[1].set_ylabel('Trend')\n\n# plot the seasonal part\nresult.seasonal.plot(ax = axes[2], legend = False)\naxes[2].set_ylabel('Seasonal')\n\n# plot the residual\nresult.resid.plot(ax = axes[3], legend = False)\naxes[3].set_ylabel('Residual')\n\n# ----------------------------------------------------------------------------------------------------\n# prettify the plot\n\n# get the xticks and the xticks labels\nxtick_location = df_timeindex.index.tolist()\n\n# set x_ticks\nax.set_xticks(xtick_location);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Question 2: Create a decomposition plot for a city of weekly sales\n\n\n[Go back to the Table of Contents](#table_of_contents)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# start with the regular df\ndf_for_question_2 = create_df()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Seleccionamos los datos de una ciudad\ndf_city_week = df_for_question_2[df_for_question_2[\"city\"] == \"ÐÐ¾ÑÐºÐ²Ð°\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_city_week['date'] = pd.to_datetime(df_city_week[\"date\"], format = \"%d.%m.%Y\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_city_week_timeindex = df_city_week.set_index(\"date\").resample(\"W\")[\"item_cnt_day\"].sum().to_frame()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_city_week_timeindex.head()\n#Vemos que las fechas ya salen por semanas y no por dÃ­as","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# Hacemos la descomposiciÃ³n estacional usando promedios mÃ³viles\nresult = seasonal_decompose(df_city_week_timeindex[\"item_cnt_day\"])\n\n# Instanciamos la figura para el grÃ¡fico\nfig, axes = plt.subplots(ncols = 1, nrows = 4, sharex = True, figsize = (12,10))\n\n# GrÃ¡fico de los datos originales\nresult.observed.plot(ax = axes[0], legend = False)\naxes[0].set_ylabel('Observado')\naxes[0].set_title(\"DecomposiciÃ³n de las series\")\n\n# GrÃ¡fico de la tendencia\nresult.trend.plot(ax = axes[1], legend = False)\naxes[1].set_ylabel('Tendencia')\n\n# GrÃ¡fico de la parte estacional\nresult.seasonal.plot(ax = axes[2], legend = False)\naxes[2].set_ylabel('Estacional')\n\n# Grafico de los datos residuales\nresult.resid.plot(ax = axes[3], legend = False)\naxes[3].set_ylabel('Residual')\n\n#Etiquetamos el eje de la x\nxtick_location = df_timeindex.index.tolist()\n\n# establecemos x_ticks\nax.set_xticks(xtick_location);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Visualizing the most important cities\n[Go back to the Table of Contents](#table_of_contents)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Treemaps are a very useful and visual tools to see different categories and their overall importance in a dataset.\nAlso, they are very cool and easy to make using Python and squarify.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# prepare the data\n\n# extract each year using dt.year\ndf[\"YEAR\"] = df[\"date\"].dt.year\n\n# create a smaller df for year 2013\nshort_df = df[df[\"YEAR\"] == 2013][[\"item_cnt_day\", \"city\"]]\n\n# groupby by city and sum all the sales\nshort_df = short_df.groupby(\"city\")[\"item_cnt_day\"].sum().to_frame()\n\n# sort the values in the smaller df inplace\nshort_df.sort_values(\"item_cnt_day\", ascending = False, inplace = True)\n\n# get the x and y values\nmy_values = short_df[\"item_cnt_day\"]\nmy_pct = short_df[\"item_cnt_day\"]/short_df[\"item_cnt_day\"].sum()\n\n# create custom labels for each city with their total sales and overall contribution\nlabels = ['{} - Sales :{}k \\n {}% of total'.format(city, sales/1000, round(pct, 2)*100) for city, sales, pct in zip(short_df.index, my_values, my_pct)]\n\n# create a color palette, mapped to the previous values\ncmap = matplotlib.cm.Blues\n\n# we want to normalize our values, otherwise a city will have the darkest collor and all the others will pale\nmini = min(my_values)\nmaxi= np.percentile(my_values, q = 99)\nnorm = matplotlib.colors.Normalize(vmin = mini, vmax = maxi)\ncolors = [cmap(norm(value)) for value in my_values]\n\n# instanciate the figure\nplt.figure(figsize = (30, 10))\n# we can pass colors but Moscow is way too big and most of the cities are pale blue\nsquarify.plot(sizes = my_values, label = labels,  alpha = 0.8, color  = colors)\n\n# Remove our axes, set a title and display the plot\nplt.title(\"Sales by city and their % over total sales in 2013\", fontsize = 23, fontweight = \"bold\")\nplt.axis('off')\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# we will do the same plot as before but without custom colors\n# Moscow is a big outlier so it pales the rest of the cities\n\nshort_df = df[df[\"YEAR\"] == 2014][[\"item_cnt_day\", \"city\"]]\nshort_df = short_df.groupby(\"city\")[\"item_cnt_day\"].sum().to_frame()\nshort_df.sort_values(\"item_cnt_day\", ascending = False, inplace = True)\n\nmy_values = short_df[\"item_cnt_day\"]\nmy_pct = short_df[\"item_cnt_day\"]/short_df[\"item_cnt_day\"].sum()\nlabels = ['{} - Sales :{}k \\n {}% of total'.format(city, sales/1000, round(pct, 2)*100) for city, sales, pct in zip(short_df.index, my_values, my_pct)]\n\nplt.figure(figsize = (30, 10))\nsquarify.plot(sizes = my_values, label = labels,  alpha = 0.8)\nplt.title(\"Sales by city and their % over total sales in 2014\",fontsize = 23, fontweight = \"bold\")\n\nplt.axis('off')\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# we will do the same plot as before but without custom colors\n# Moscow is a big outlier so it pales the rest of the cities\n\nshort_df = df[df[\"YEAR\"] == 2015][[\"item_cnt_day\", \"city\"]]\nshort_df = short_df.groupby(\"city\")[\"item_cnt_day\"].sum().to_frame()\nshort_df.sort_values(\"item_cnt_day\", ascending = False, inplace = True)\n\nmy_values = short_df[\"item_cnt_day\"]\nmy_pct = short_df[\"item_cnt_day\"]/short_df[\"item_cnt_day\"].sum()\nlabels = ['{} - Sales :{}k \\n {}% of total'.format(city, sales/1000, round(pct, 2)*100) for city, sales, pct in zip(short_df.index, my_values, my_pct)]\n\nplt.figure(figsize = (30, 10))\nsquarify.plot(sizes = my_values, label = labels,  alpha = 0.8)\nplt.title(\"Sales by city and their % over total sales in 2015\",fontsize = 23, fontweight = \"bold\")\n\nplt.axis('off')\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[[\"city\", \"city_id\"]].drop_duplicates()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# treemaps are very useful to see the difference and the weights of categories\n# but they don't give us that much of information about the distribution of each category\n# let's use boxplot to see the distribution of Moscow city\n\n# we can see huge outliers for Moscow city.\nplt.figure(figsize = (10, 10))\nsns.boxplot(x = \"city\",\n            y = \"item_cnt_day\", \n            data = df[(df[\"YEAR\"] == 2013) & (df[\"city_id\"] == 13)]\n           );","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Question 3: Create a treemap plot for item_category and the total combined sales\n\n<span style=\"color:red\">If the % of a category over total is less 1%, don't put any label!!!</span>\n\n[Go back to the Table of Contents](#table_of_contents)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# start with the regular df\ndf_for_question_3 = create_df()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_for_question_3.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Seleccionamos los datos que nos interesan para hacer el grÃ¡fico\ndf_cat_sales = df_for_question_3[[\"item_cnt_day\", \"item_category_name\"]]\ndf_cat_sales = df_cat_sales.groupby(\"item_category_name\")[\"item_cnt_day\"].sum().to_frame()\ndf_cat_sales.sort_values(\"item_cnt_day\", ascending = False, inplace = True)\n\n# Seleccionamos los datos que vamos a usar para el grÃ¡fico\nmy_values = df_cat_sales[\"item_cnt_day\"]\nmy_pct = df_cat_sales[\"item_cnt_day\"]/df_cat_sales[\"item_cnt_day\"].sum()\nlabels = ['{} - Sales :{}k \\n {}% of total'.format(item_category_name, sales/1000, round(pct, 2)*100) for item_category_name, sales, pct in zip(df_cat_sales.index, my_values, my_pct) if pct >= 0.01]\n\n# Seleccionamos una paleta de color\ncmap = matplotlib.cm.Blues\n\n# Normalizamos los valores\nmini = min(my_values)\nmaxi= np.percentile(my_values, q = 99)\nnorm = matplotlib.colors.Normalize(vmin = mini, vmax = maxi)\ncolors = [cmap(norm(value)) for value in my_values]\n\n# Instanciamos la figura\nplt.figure(figsize = (30, 10))\n\nsquarify.plot(sizes = my_values, label = labels,  alpha = 0.8, color  = colors)\n\n# Eliminamos los axis, aÃ±adimos el titulo y ejecutamos el grÃ¡fico\nplt.title(\"Ventas por categoria y % sobre el total de ventas\", fontsize = 15, fontweight = \"bold\")\nplt.axis('off')\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Visualizing nulls values\n[Go back to the Table of Contents](#table_of_contents)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# This plot will help us visualize the missing values for each datetime and item_id\n# This is the most granular plots possible, since we will be seeing individual sales by day and item_id\n# This plot can be very consufing, but the main point is to show all the \"missing values\" we have\n# We have seen previously in our EDA, that when we groupby and resamples our sales, we might think\n# that we don't have any missing values. But its not true, we only have the reported sales\n# This means that, if we have a shop or item_id that only had 3 sales per year, when we resample\n# our df by day, pandas will generate additional days with null sales.\n# those null sales is what we want to plot here\n\nplt.figure(figsize = (20, 10))\nplot = sns.heatmap(df.pivot_table(index = [\"date\"], columns = ['item_id'], values = \"item_cnt_day\", aggfunc = sum).isnull(), cbar = True, cmap = \"inferno\")\nplot.set_title(\"Null sales by item_id and day\");","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The previous plot is kinda nice, but we can do better.\nLet's organize the plot from the lowest number of missing values to the highest. It will help us a lot to distinguish how many missing values there are.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# create a dataframe with True and False if there are missing values\ngb_df_ = df.pivot_table(index = [\"date\"], columns = ['item_id'], values = \"item_cnt_day\", aggfunc = sum).isnull()\n\n# generate a custom list with the colums name sorte by values\norder_of_columns = list(gb_df_.sum().sort_values().index)\n\n# change the order of the df from the lowest amount of missing values to the highest\ngb_df_.columns = order_of_columns\n\n# plo the data using seaborn heatmap\nplt.figure(figsize = (20, 10))\nplot = sns.heatmap(gb_df_, cbar = True, cmap = \"inferno\")\nplot.set_title(\"Null sales by item_id and day\");","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Wait, nothing changed, why? Can you spot the mistake in the previous cell?","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"gc.collect()\ndel gb_df_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gb_df_ = df.pivot_table(index = [\"date\"], columns = ['item_id'], values = \"item_cnt_day\", aggfunc = sum).isnull()\norder_of_columns = list(gb_df_.sum().sort_values().index)\ngb_df_ = gb_df_[order_of_columns]\nplt.figure(figsize = (20, 10))\nplot = sns.heatmap(gb_df_, cbar = True, cmap = \"inferno\")\nplot.set_title(\"Null sales by item_id and day\");","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gc.collect()\ndel gb_df_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# This is a similar plot to the previous one, but here instead of item_id we will be plotting shop_id and their total sales\n# We expect to have fewer missing values for each shop.\ngb_df_ = df.pivot_table(index = [\"date\"], columns = ['shop_id'], values = \"item_cnt_day\", aggfunc = sum).isnull()\norder_of_columns = list(gb_df_.sum().sort_values().index)\ngb_df_ = gb_df_[order_of_columns]\nplt.figure(figsize = (20, 10))\nplot = sns.heatmap(gb_df_, cbar = True, cmap = \"inferno\")\nplot.set_title(\"Null sales by shop and day\");","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# this will allow us to see a all the columns of the df\npd.options.display.max_columns = 999","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# create a smaller df\nshort_df = df[[\"date\", \"item_cnt_day\", \"shop_name\"]]\n# set the date to be the index (to resample later)\nshort_df.set_index(\"date\", inplace = True)\n# groupby by shop_name\ngb = short_df.groupby(\"shop_name\")\n# resample the df by month sales (resample = groupby by months in timeseries)\ngbr = gb.resample(\"M\")[\"item_cnt_day\"].sum()\n# unstack the gbr to have columns name\ngbr = gbr.unstack(level = -1).T\n# sort the values, from no nulls to more null values\norder_of_columns = list(gbr.isnull().sum().sort_values().index)\n# change the order of the df\ngbr = gbr[order_of_columns]\ngbr.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# change all nulls to 1 and sales to 0\ngbr.head().isnull()*1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's plot the null values for each shop\nplt.figure(figsize=(20, 10))\n# this lines gbr.unstack(level = -1).T.isnull()*1\n# converts any null to 1 and the rest will be 0\nsns.heatmap(gbr.isnull()*1, cmap = \"inferno\", cbar = True).set_title(\"Null values by shop and Month\");","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# create a smaller df\nshort_df = df[[\"date\", \"item_cnt_day\", \"item_category_name\"]]\n\n# set the date to be the index (to resample later)\nshort_df.set_index(\"date\", inplace = True)\n\n# groupby by shop_name\ngb = short_df.groupby(\"item_category_name\")\n\n# resample the df by month sales (resample = groupby by months in timeseries)\ngbr = gb.resample(\"M\")[\"item_cnt_day\"].sum()\n\n# unstack the gbr to have columns name\ngbr = gbr.unstack(level = -1).T\n\n# sort the values, from no nulls to more null values\norder_of_columns = list(gbr.isnull().sum().sort_values().index)\n\n# change the order of the df\ngbr = gbr[order_of_columns]\ngbr.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# change all nulls to 1 and sales to 0\ngbr.head().isnull()*1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's plot the null values for each shop\nplt.figure(figsize=(20, 10))\n\n# this lines gbr.unstack(level = -1).T.isnull()*1\n# converts any null to 1 and the rest will be 0\nsns.heatmap(gbr.isnull()*1, cmap = \"inferno\", cbar = True).set_title(\"Null values by item category and Month\");","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Visualization of Outliers\n[Go back to the Table of Contents](#table_of_contents)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's look at outliers for item sales\n# We will use boxplots because they are very useful to see the distribution of values\nplt.figure(figsize = (10,4))\nsns.boxplot(x = df[\"item_cnt_day\"]);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's look at outliers for item price\nplt.figure(figsize = (10,4))\nplt.xlim(df[\"item_price\"].min(), df[\"item_price\"].max()*1.1)\nsns.boxplot(x = df[\"item_price\"]);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# joint plot is another very convenient way to plot the relationship between 2 variables\n# but because we have huge outliers, we don't see them \n# https://seaborn.pydata.org/generated/seaborn.jointplot.html\nplt.figure(figsize = (10,4))\nsns.jointplot(x = \"item_price\", y = \"item_cnt_day\", data = df);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's filter the outliers and make the same joint plot\ndf = df[(df[\"item_price\"] < np.percentile(df[\"item_price\"], q = 99)) & (df[\"item_cnt_day\"] >= 0) & (df[\"item_cnt_day\"] < np.percentile(df[\"item_cnt_day\"], q = 99))]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# we have removed the outliers and now \nplt.figure(figsize = (10, 10))\nsns.jointplot(x = \"item_price\", y = \"item_cnt_day\", data = df);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Conclusion\n[Go back to the Table of Contents](#table_of_contents)\n\nAfter taking a look at the sales data, here are some conclusion we can extract:\n\n1. We see that the total sales decrease over time. This is very important because, we have to create features for our model that catch this trend.\n\n2. We have seen that the sales present huge spikes in Christmas season. Datetime features can help a lot our model.\n\n3. Data has a lot of missing values and we have not found a specific or category affected by this. More likely it's just the nature of the data.\n\n4. Top 3 cities capture more than 50% of total sales. City based features can be very helpful for the model.\n\n5. The top 3 categories represent more than 40% of total sales: they are Movies, PC Games and Music.\n\n6. Data presents outliers at the sales and price level. Before generating features or training a model, data must be cleaned properly.\n\n7. We have seen thanks to our calendar plots that we a small increase in sales on the weekends. We do see however bigger sales on 14 of February or 9 of May (holidays).","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}