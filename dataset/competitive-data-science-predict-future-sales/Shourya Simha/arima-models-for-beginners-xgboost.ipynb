{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt # data visualization\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"#Reading the Datasets\nsales_train = pd.read_csv('/kaggle/input/competitive-data-science-predict-future-sales/sales_train.csv')\nitems = pd.read_csv(\"/kaggle/input/competitive-data-science-predict-future-sales/items.csv\")\nshops= pd.read_csv(\"/kaggle/input/competitive-data-science-predict-future-sales/shops.csv\")\nitem_categories = pd.read_csv('/kaggle/input/competitive-data-science-predict-future-sales/item_categories.csv')\nprint(sales_train.shape)\nsales_train.head()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"***Since our two columns Item_price and Item_count have negative values i.e. probably the items have been returned back to the shop, the shop is not making any sales on these orders. hence, I will drop the rows that have negative value for item_price and item_count.***","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sales_train.drop(sales_train[(sales_train['item_cnt_day']<=0)|(sales_train['item_price']<=0)].index ,axis=0,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"***convert the date to datetime object.***","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# sales_train['date']=pd.to_datetime(sales_train['date'],dayfirst=True)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**I will not focussing on the exploratory data analysis for this notebook and would dive into time-series modelling and explaining the concepts better.**\n\n*In practice we can assume the series to be stationary if it has constant statistical properties over time and these properties can be:*\n\n*• constant mean*\n\n*• constant variance*\n\n*• an auto co-variance that does not depend on time.*\n\n*These details can be easily retrieved using stat commands in python.*\n\n*The best way to understand you stationarity in a Time Series is by eye-balling the plot:*","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data=sales_train.groupby([\"date\",\"date_block_num\",\"shop_id\",\"item_id\"])[\"item_cnt_day\"].sum().reset_index()\nts=data.groupby(['date'])['item_cnt_day'].sum()\nts.astype('float')\nplt.figure(figsize=(12,10))\nplt.title('Total Sales of the item')\nplt.xlabel('Month-Year')\nplt.ylabel('Quantity of Sales')\nplt.plot(ts)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**We can see a decreasing trend from the above line plot.** \n ***In order to apply a time series model, it is important for the Time series to be stationary; in other words all its statistical properties (mean,variance) remain constant over time***","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Performing Dicker Fuller Test","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from statsmodels.tsa.stattools import adfuller\ndef test_stationarity(timeseries):\n    \n    #Determing rolling statistics\n    rolmean = timeseries.rolling(window=12,center=False).mean()\n    rolstd = timeseries.rolling(window=12,center=False).std()#window=12, because of yearly trend for both mean and variance\n#Plot rolling statistics:\n    plt.figure(figsize=(15,10))\n    plt.plot(timeseries, color='blue',label='Original')\n    plt.plot(rolmean, color='red', label='Rolling Mean')\n    plt.plot(rolstd, color='black', label = 'Rolling Std')\n    plt.legend(loc='best')\n    plt.title('Rolling Mean & Standard Deviation')\n    plt.show()\n    #Perform Dickey-Fuller test:\n    print ('Results of Dickey-Fuller Test:')\n    dftest = adfuller(timeseries, autolag='AIC')\n    dfoutput = pd.Series(dftest[0:4], index=['Test Statistic','p-value','#Lags Used','Number of Observations Used'])\n    for key,value in dftest[4].items():\n        dfoutput['Critical Value (%s)'%key] = value\n    print (dfoutput)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_stationarity(ts)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*** if The data is not stationary because the test statistic is greater than the critical value.***\n***There are two major factors that make a time series non-stationary. They are:***\n\n***• Trend: non-constant mean***\n\n***• Seasonality: Variation at specific time-frames***\n***Trend:***\n***The first step is to reduce the trend using transformation, as we can see here that there is a strong positive trend. These transformation can be log, sq-rt, cube root etc . Basically it penalizes larger values more than the smaller. In this case we will use the logarithmic transformation.***\n\nWe will not be doing this as the test-statistic in our case is lesser than all the critical values.\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# from sklearn.preprocessing import LabelEncoder\n# le = LabelEncoder()\n# def encoding_categorical(dataset):\n#     categorical_columns=['shop_id','item_id']\n#     for column in categorical_columns:\n#         dataset[str(column)]=le.fit_transform(dataset[str(column)])\n#     return dataset\n# complete_data=encoding_categorical()\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Decomposing:**\n***we model both the trend and the seasonality, then the remaining part of the time series is returned.***","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"ts_data = pd.DataFrame(ts)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from statsmodels.tsa.seasonal import seasonal_decompose\ndecomposition = seasonal_decompose(ts_data,period=100)\ntrend = decomposition.trend\nseasonal = decomposition.seasonal\nresidual = decomposition.resid\nplt.figure(figsize=(15,8))\nplt.subplot(411)\nplt.plot(ts,label='Orginial')\nplt.subplot(412)\nplt.plot(trend,label='Trend')\nplt.subplot(413)\nplt.plot(seasonal,label='Seasonal')\nplt.subplot(414)\nplt.plot(residual,label='Residual')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Forecasting a Time Series**:\n\nNow that we have made the Time series stationary, let’s make models on the time series using differencing because it is easy to add the error , trend and seasonality back into predicted values .\nWe will use statistical modelling method called ARIMA to forecast the data where there are dependencies in the values.\nAuto Regressive Integrated Moving Average(ARIMA) — It is like a liner regression equation where the predictors depend on parameters (p,d,q) of the ARIMA model .\n\n\n• p : This is the number of AR (Auto-Regressive) terms . Example — if p is 3 the predictor for y(t) will be y(t-1),y(t-2),y(t-3).\n\n\n• q : This is the number of MA (Moving-Average) terms . Example — if p is 3 the predictor for y(t) will be y(t-1),y(t-2),y(t-3).\n\n\n• d :This is the number of differences or the number of non-seasonal differences .\n\n\nNow let’s check out on how we can figure out what value of p and q to use. We use two popular plotting techniques; they are:\n\n\n• Autocorrelation Function (ACF): It just measures the correlation between two consecutive (lagged version). example at lag 4, ACF will compare series at time instance t1…t2 with series at instance t1–4…t2–4\n\n\n• Partial Autocorrelation Function (PACF): is used to measure the degree of association between y(t) and y(t-p).\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from statsmodels.tsa.arima_model import ARIMA\nfrom statsmodels.tsa.stattools import acf,pacf\n\nlag_acf=acf(ts,fft=False)\nlag_pacf=pacf(ts,method='ols')\nplt.figure(figsize=(11,8))\nplt.subplot(121)\nplt.plot(lag_acf)\nplt.axhline(y=0,linestyle='--',color='gray')\nplt.axhline(y=-1.96/np.sqrt(len(ts)),linestyle='--',color='gray')\nplt.axhline(y=1.96/np.sqrt(len(ts)),linestyle='--',color='gray')\nplt.title('Autocorrelation Function')\n\nplt.subplot(122)\nplt.plot(lag_pacf)\nplt.axhline(y=0,linestyle='--',color='gray')\nplt.axhline(y=-1.96/np.sqrt(len(ts)),linestyle='--',color='gray')\nplt.axhline(y=1.96/np.sqrt(len(ts)),linestyle='--',color='gray')\nplt.title('Partial Autocorrelation Function')\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The two dotted lines on either sides of 0 are the confidence intervals. These can be used to determine the ‘p’ and ‘q’ values as:\n\n\n• p: The first time where the PACF crosses the upper confidence interval, here its close to 2. hence p = 2.\n\n\n• q: The first time where the ACF crosses the upper confidence interval, here its close to 2. hence q = 2.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"model=ARIMA(ts,order=(2,0,2))\nresult = model.fit(disp=-1)\nplt.plot(ts,label=\"Original\")\nplt.plot(result.fittedvalues,color='red',label=\"Predicted\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"forecast_errors = [ts[i]-result.fittedvalues[i] for i in range(len(ts))]\nbias = sum(forecast_errors) * 1.0/len(ts)\nprint('Bias: %f' % bias)\ntest=pd.read_csv(\"../input/competitive-data-science-predict-future-sales/test.csv\")\npredictions = pd.DataFrame(result.fittedvalues).reset_index()\npredictions.columns=[\"date\",\"predictions\"]\npredictions.head()# Monthly sales forecasting","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}