{"cells":[{"metadata":{},"cell_type":"markdown","source":"**Boost Your Score Through Leaderboard Probing**\n\nThe code in this notebook can polish your submission file, so that you get a guaranteed imporvement in your leadboard ranking.  Based on the feedback from graders, we share this leaderboard probing idea.\n\nTo boost your score:\n\n    lbp=LeaderBoardProbing()\n    lbp.mean_scale('YourSubmission.csv')\n\nYou will see an improvement in the new submission file YourSubmission_mean.csv, then take the public score you obtained and do another boost:\n\n    lbp.variance_scale('YourSubmission_mean.csv')\n\nYou will see another improvement with YourSubmission_variance_mean.csv.\n\nWe will explain this in more detail with two examples:\n\n* Take predictions based on a previous-month model, turn its score from 1.167778 into 1.038940.\n* Take predictions based on random noise, turn its score from 3.473406 into 1.199848.\n\nSounds magic, read on ...\n\nFirst, load the implementation code in the next cell."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# coding: utf-8\nimport numpy as np\nimport pandas as pd\nimport os\n\nclass LeaderBoardProbing:\n\n    def __init__(self):\n        if os.path.exists('new_test.csv.gz'):\n            self.test  = pd.read_csv('new_test.csv.gz')\n        else:\n            self.test=pd.read_csv('../input/competitive-data-science-predict-future-sales/test.csv')\n            sales=pd.read_csv('../input/competitive-data-science-predict-future-sales/sales_train.csv')\n            # some routine data cleaning code\n            #sales=sales[(sales.item_price<100000) & (sales.item_cnt_day<1001)]\n            shop_id_map={0:57, 1:58, 10:11}\n            sales['shop_id']=sales['shop_id'].apply(lambda x: shop_id_map.get(x, x))\n            self.test['shop_id']=self.test['shop_id'].apply(lambda x: shop_id_map.get(x, x))\n\n            pairs={ (a, b) for a, b in zip(sales.shop_id, sales.item_id) }\n            items={ a for a in sales.item_id }\n            self.test['date_block_num']=34\n            self.test['test_group']=[ 2 if (a,b) in pairs else (1 if b in items else 0) for a,b in zip(self.test.shop_id, self.test.item_id)]\n            self.test.sort_values('ID', inplace=True)\n            self.test.to_csv('new_test.csv.gz', index=False)\n\n        self.test['item_cnt_month']=0.0\n        self.n=len(self.test)\n        self.n0=sum(self.test.test_group==0)\n        self.n1=sum(self.test.test_group==1)\n        self.n2=sum(self.test.test_group==2)\n\n    def probe_mean(self):\n        \"\"\"Generate 4 LeaderBoardProbing files, set target to 0 for all three test groups,\n        then set target to 1 for only one test group at a time.\n        Manually submit the files to obtain public leaderboard scores.\n        Then feed the scores to estimate_mean() to obtain mean values for all groups\n        and store those means in group_mean()\n        \"\"\"\n        os.makedirs('leak', exist_ok=True)\n        self.save(self.test, 'leak/Probe000.csv')\n\n        tmp=self.test.copy()\n        tmp.loc[tmp.test_group==2, 'item_cnt_month']=1.0\n        self.save(tmp, 'leak/Probe001.csv')\n\n        tmp=self.test.copy()\n        tmp.loc[tmp.test_group==1, 'item_cnt_month']=1.0\n        self.save(tmp, 'leak/Probe010.csv')\n\n        tmp=self.test.copy()\n        tmp.loc[tmp.test_group==0, 'item_cnt_month']=1.0\n        self.save(tmp, 'leak/Probe100.csv')\n\n    def estimate_mean(self, rmse000, rmse100, rmse010, rmse001):\n        \"\"\"Obtain public scores for Probe000, Probe100, Probe010, Probe001\n        Public,Private\n        Probe000,1.250111,1.236582\n        Probe100,1.23528,1.221182\n        Probe010,1.38637,1.373707\n        Probe001,1.29326,1.279869\n        \"\"\"\n\n        def calc(rmse000, n, rmse_i, ni):\n            u=(1-(rmse_i**2-rmse000**2)*n/ni)/2\n            return u\n\n        u0=calc(rmse000, self.n, rmse100, self.n0)\n        u1=calc(rmse000, self.n, rmse010, self.n1)\n        u2=calc(rmse000, self.n, rmse001, self.n2)\n        u=(self.n0*u0+self.n1*u1+self.n2*u2)/self.n\n        return(u0, u1, u2, u)\n\n    def true_means(self):\n        # computed by leader board probing\n        # u0, u1, u2 and overall mean u\n        # Kaggle public scores and Coursera scores slightly differ\n        # Kaggle scores\n        #return [0.7590957299173547, 0.060230457160248385, 0.39458181098366407, 0.2839717256500001]\n        # use Coursera scores here\n        return [0.758939742420249, 0.0601995732152425, 0.3945593622881204, 0.28393632703149974]\n\n    def mean_scale(self, filename):\n        \"\"\"Compare the mean of each test group to their true public leaderboard means\n        shift the prediction so that the means match\n        filename: your submission csv file name\n        \"\"\"\n        df=pd.read_csv(filename)\n        df.sort_values('ID', ascending=True, inplace=True)\n        mask0=self.test.test_group==0\n        mask1=self.test.test_group==1\n        mask2=self.test.test_group==2\n        U=self.true_means()\n        print(\"Group0: predict mean=\", df[ mask0 ].item_cnt_month.mean(), \"true mean=\", U[0])\n        print(\"Group1: predict mean=\", df[ mask1 ].item_cnt_month.mean(), \"true mean=\", U[1])\n        print(\"Group2: predict mean=\", df[ mask2 ].item_cnt_month.mean(), \"true mean=\", U[2])\n        change=999\n        previous=df.item_cnt_month.values.copy()\n        i=1\n        while change>1e-6:\n            df.loc[mask0, 'item_cnt_month']+=U[0]-df[ mask0 ].item_cnt_month.mean()\n            df.loc[mask1, 'item_cnt_month']+=U[1]-df[ mask1 ].item_cnt_month.mean()\n            df.loc[mask2, 'item_cnt_month']+=U[2]-df[ mask2 ].item_cnt_month.mean()\n            df['item_cnt_month']=df['item_cnt_month'].clip(0,20)\n            change=np.sum(np.abs(df.item_cnt_month.values - previous))\n            previous=df.item_cnt_month.values.copy()\n            print(\">loop\", i, \"change:\", change)\n            i+=1\n        self.save(df, filename.replace('.csv', '_mean.csv'))\n\n    def variance_scale(self, filename, rmse, rmse000=1.250111):\n        \"\"\"\n        filename: your submission csv file name\n        rmse: your public leaderboard score\n        \"\"\"\n        df=pd.read_csv(filename)\n        df.sort_values('ID', ascending=True, inplace=True)\n        n=df.shape[0]\n        u=self.true_means()[-1]\n        Yp=df.item_cnt_month.values\n        YpYp=np.sum(Yp*Yp)\n        YYp=n*(rmse000**2-rmse**2)/2+YpYp/2\n        lambda_ = (YYp-u*u*n)/(YpYp-u*u*n)\n        print(\">>>>>multipler lambda=\", lambda_)\n        df['item_cnt_month']=(Yp-u)*lambda_+u\n        filename2=filename.replace('.csv', '_lambda.csv')\n        self.save(df, filename2)\n        self.mean_scale(filename2)\n\n    def save(self, df, filename):\n        \"\"\"Produce LeaderBoardProbing file based on dataframe\"\"\"\n        df = df[['ID','item_cnt_month']].copy()\n        df.sort_values(['ID'], ascending=True, inplace=True)\n        df['item_cnt_month']=df['item_cnt_month'].apply(lambda x: \"%.5f\" % x)\n        if np.isnan(df.item_cnt_month.isnull().sum()):\n            print(\"ERROR>>>>> There should be no nan entry in the LeaderBoardProbing file!\")\n        print(\"Save LeaderBoardProbing to file:\", filename)\n        df.to_csv(filename, index=False)\n\n    def flip_signs(self, filename):\n        \"\"\"\n        Produce LeaderBoardProbing file, flip the sign of prediction for each of the three groups\n        filename: your submission csv file name\n        output:\n            three new submission files with suffix _mpp.csv, _pmp.csv, _ppm.csv\n            notation in the notebook\n            m: minus, p: plus\n            mpp is -++, pmp is +-+, ppm is ++-\n\n        You need to submit these three files to obtain\n            rmse_mpp, rmse_pmp, rmse_ppm\n        Then you call\n            LeaderBoardProbing.variance_scale_v2(filename, rmse_mpp, rmse_pmp, rmse_ppm, rmse)\n            Note: rmse is the original rmse score obtained by your filename\n        \"\"\"\n        df=pd.read_csv(filename)\n        df.sort_values(['ID'], ascending=True, inplace=True)\n        mask0=self.test.test_group==0\n        mask1=self.test.test_group==1\n        mask2=self.test.test_group==2\n        tmp=df.copy()\n        tmp.loc[mask0, 'item_cnt_month']=-tmp[ mask0 ].item_cnt_month\n        self.save(tmp, filename.replace('.csv', '_mpp.csv'))\n        tmp=df.copy()\n        tmp.loc[mask1, 'item_cnt_month']=-tmp[ mask1 ].item_cnt_month\n        self.save(tmp, filename.replace('.csv', '_pmp.csv'))\n        tmp=df.copy()\n        tmp.loc[mask2, 'item_cnt_month']=-tmp[ mask2 ].item_cnt_month\n        self.save(tmp, filename.replace('.csv', '_ppm.csv'))\n\n    def variance_scale_v2(self, filename, rmse_mpp, rmse_pmp, rmse_ppm, rmse):\n        \"\"\"\n        filename: your submission csv file name\n        You must use LeaderBoardProbing.flip_signs(filename)\n            to generate three additional submission files, obtain their public scores\n            and feed those scores as parameters\n        Scores: rmse-++, rmse+-+, rmse++-, rmse+++\n\n        output:\n            New scaled submission file\n        \"\"\"\n        df=pd.read_csv(filename)\n        df.sort_values(['ID'], ascending=True, inplace=True)\n        mask0=self.test.test_group==0\n        mask1=self.test.test_group==1\n        mask2=self.test.test_group==2\n        n=len(df)\n        n0=sum(mask0)\n        n1=sum(mask1)\n        n2=sum(mask2)\n        YYp0=n/4*(rmse_mpp**2-rmse**2)\n        YYp1=n/4*(rmse_pmp**2-rmse**2)\n        YYp2=n/4*(rmse_ppm**2-rmse**2)\n        U=self.true_means()\n        Yp0=df.loc[mask0, 'item_cnt_month'].values\n        Yp1=df.loc[mask1, 'item_cnt_month'].values\n        Yp2=df.loc[mask2, 'item_cnt_month'].values\n        lambda0=(YYp0-U[0]**2*n0)/(np.sum(Yp0*Yp0)-U[0]**2*n0)\n        lambda1=(YYp1-U[1]**2*n1)/(np.sum(Yp1*Yp1)-U[1]**2*n1)\n        lambda2=(YYp2-U[2]**2*n2)/(np.sum(Yp2*Yp2)-U[2]**2*n2)\n        print(\"Labmda: \", lambda0, lambda1, lambda2)\n        df.loc[mask0, 'item_cnt_month']=U[0]+lambda0*(df[ mask0 ].item_cnt_month-U[0])\n        df.loc[mask1, 'item_cnt_month']=U[1]+lambda1*(df[ mask1 ].item_cnt_month-U[1])\n        df.loc[mask2, 'item_cnt_month']=U[2]+lambda2*(df[ mask2 ].item_cnt_month-U[2])\n        df['item_cnt_month']=df['item_cnt_month'].clip(0,20)\n        fn=filename.replace('.csv', '_labmdaV2.csv')\n        self.save(df, fn)\n        self.mean_scale(fn)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"markdown","source":"Our first model is to simply use item_cnt_month data from 2015-Oct for the prediction, entries without training data are set to 0.  This is a popular basedline model seem in the forum."},{"metadata":{"trusted":true},"cell_type":"code","source":"import shutil\nshutil.copy(\"../input/salescompetitionoctmodel/submit_oct.csv\", \"submit_oct.csv\")\n# your submission file is now at submit_oct.csv","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"If you grade this file, you get 3/10 Coursera score with:\n\nYour public and private LB scores are: 1.167778 and 1.172726.\n\nLet us first use mean scaling, this strategy does not require any probing, therefore, you should always apply this step to your submission."},{"metadata":{"trusted":true},"cell_type":"code","source":"lbp = LeaderBoardProbing()\nlbp.mean_scale('submit_oct.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now submit the new file **submit_oct_mean.csv**:\n\nYour public and private LB scores are: 1.118256 and 1.123108\n\nNow we can take the public score 1.118256 and use it to do variance scaling.  We never use private scores, they are shown here just to convince you private scores improve throughout the process as well."},{"metadata":{"trusted":true},"cell_type":"code","source":"lbp.variance_scale('submit_oct_mean.csv', 1.118256)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This new file **submit_oct_mean_lambda_mean.csv**:\n\nYour public and private LB scores are: 1.038965 and 1.040923.\n\nThe Coursera score is now 5/10. You can improve it further, but first we need to create three submission files based on the mean-scaled file and obtain their scores manually (you are going to probe the public leaderboard three times):"},{"metadata":{"trusted":true},"cell_type":"code","source":"lbp.flip_signs('submit_oct_mean.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"You should obtain the following scores, notice, private scores are not needed, we only rely on the public scores.\n\n#Submission,Public,Private\n#submit_oct_mean_mpp.csv, Your public and private LB scores are: 1.189455 and 1.194442.\n#submit_oct_mean_pmp.csv, Your public and private LB scores are: 1.121041 and 1.125736.\n#submit_oct_mean_ppm.csv, Your public and private LB scores are: 2.002616 and 1.958823.\n\nNow use the previous score 1.118256 (**submit_oct_mean.csv**), together with the above three scores to create a final submission:\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"lbp.variance_scale_v2('submit_oct_mean.csv', 1.189455, 1.121041, 2.002616, 1.118256)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The final score on **submit_oct_mean_labmdaV2_mean.csv** is:\n    \nYour public and private LB scores are: 1.038940 and 1.040874.\n\nSo we have improve the score from 1.167778 to 1.038940![](http://)"},{"metadata":{},"cell_type":"markdown","source":"Let's work on another example to show you all the tweaks one more time.  We create a random submission first."},{"metadata":{"trusted":true},"cell_type":"code","source":"df=lbp.test.copy()\nnp.random.seed(42)\ndf['item_cnt_month']=np.clip(np.random.randn(len(df))*4+1, 0, 20)\nlbp.save(df, 'submit_random.csv')\n# Submit and we get\n# Your public and private LB scores are: 3.473406 and 3.465503.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# You should always run mean_scale, as it requires not probing.\n# I actually always send my prediction through mean_scale and submit the processed file\nlbp.mean_scale('submit_random.csv')\n# Your score for submit_random_mean.csv\n# Your public and private LB scores are: 1.545094 and 1.525234.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lbp.variance_scale('submit_random_mean.csv', 1.545094)\n# Your score for submit_random_mean_lambda_mean.csv\n# Your public and private LB scores are: 1.200340 and 1.185505.\n\n# If you are willing to do three more probes\nlbp.flip_signs('submit_random_mean.csv')\n# Your scores are\n# submit_random_mean_mpp.csv\n# Your public and private LB scores are: 1.604641 and 1.583784.\n# submit_random_mean_pmp.csv\n# Your public and private LB scores are: 1.547342 and 1.527319.\n# submit_random_mean_ppm.csv\n# Your public and private LB scores are: 1.645131 and 1.624193\nlbp.variance_scale_v2('submit_random_mean.csv', 1.604641, 1.547342, 1.645131, 1.545094)\n# submit_random_mean_labmdaV2_mean.csv\n# Your public and private LB scores are: 1.199848 and 1.184957.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":""},{"metadata":{"trusted":true},"cell_type":"markdown","source":"You can ignore the theoretical part below, if you do not care about the mechanism behind the probing.  Enjoy an improved ranking.\n\n**How Scaling Works Behind the Scene?**\n\nThere are 214,200 samples in the test set, including 5100 unique item_ids and 42 unique shop_ids.  The test set is made of the full product of 5100x42.\n\nLooking up the test (shop_id, item_id) pairs in the training data, we discovered there are three groups of test samples:\n\n**Group 2:** (shop_id, item_id) has historical sales data (i.e., in the training set). Sales of these samples could be predicted based on their historical time-series data. This group has 111404 samples (52%)\n\n**Group 1:** not in Group 2 but (item_id) has historical sales data in other shop_id.  Sales of these samples could be predicted mostly by sale data for the same item_id in other shops.  This group has 87550 samples (41%).\n\n**Group 0:** item_id has no historical data.  The only information available for prediction is through sales in the same item_category_id and shop_id.  This group has 15246 records (7%).\n\nLet us first probe the mean of each group.\n\nLet $j$ be the test group id, $j = 0, 1, 2$.\nThere are $n_0, n_1, n_2$ samples within each group, respectively.\nThere are $N$ total samples, where $N = n_0 + n_1 + n_2$.\nWe create 4 submission files, Probe000.csv sets all predictions to 0.  Probe100.csv sets all predictions to 0 except 1 for test group 0.  Similarly, Probe010.csv and Probe001.csv has the prediction 1 for test group 1 and test group 2, respectively.\n\nThe RMSE score for the 4 submission are $rmse_{000}, rmse_{101}, rmse_{010}, rmse_{001}$.\n\nThe true target for sample $i$ is $y_i$.\n\nWe have:\n\n$$N \\cdot rmse_{000}^{2} = \\sum_{j=0, i=1}^{n_0}{{y_{ji}}^2}+\\sum_{j=1, i=1}^{n_1}{{y_{ji}}^2}+\\sum_{j=2, i=1}^{n_2}{{y_{ji}}^2}$$\n\n$$N \\cdot rmse_{100}^{2} = \\sum_{j=0, i=1}^{n_0}{{(y_{ji} - 1)}^2}+\\sum_{j=1, i=1}^{n_1}{{y_{ji}}^2}+\\sum_{j=2, i=1}^{n_2}{{y_{ji}}^2}$$\n\n$$N \\cdot rmse_{010}^{2} = \\sum_{j=0, i=1}^{n_0}{{y_{ji}}^2}+\\sum_{j=1, i=1}^{n_1}{{(y_{ji}-1)}^2}+\\sum_{j=2, i=1}^{n_2}{{y_{ji}}^2}$$\n\n$$N \\cdot rmse_{001}^{2} = \\sum_{j=0, i=1}^{n_0}{{y_{ji}}^2}+\\sum_{j=1, i=1}^{n_1}{{y_{ji}}^2}+\\sum_{j=2, i=1}^{n_2}{{(y_{ji}-1)}^2}$$\n\nTherefore,\n\n$$N \\cdot (rmse_{100}^2 - rmse_{000}^2) = n_0 - 2 \\sum_{j=0, i=1}^{n_0}{y_{ji}}$$\n\n$$N \\cdot (rmse_{010}^2 - rmse_{000}^2) = n_1 - 2 \\sum_{j=1, i=1}^{n_1}{y_{ji}}$$\n\n$$N \\cdot (rmse_{001}^2 - rmse_{000}^2) = n_2 - 2 \\sum_{j=2, i=1}^{n_2}{y_{ji}}$$\n\n\ni.e., the means $\\mu_j$ for each test group and the overall mean $\\mu$ are:\n\n$$\\mu_{0} = \\frac{1}{2} (1 - \\frac{N}{n_0} \\cdot (rmse_{100}^2 - rmse_{000}^2))$$\n\n$$\\mu_{1} = \\frac{1}{2} (1 - \\frac{N}{n_1} \\cdot (rmse_{010}^2 - rmse_{000}^2))$$\n\n$$\\mu_{2} = \\frac{1}{2} (1 - \\frac{N}{n_2} \\cdot (rmse_{001}^2 - rmse_{000}^2))$$\n\n$$\\mu = \\frac{1}{N}(n_0 \\mu_0 + n_1 \\mu_1 + n_2 \\mu_2)$$\n\nTherefore, we know the means for each test group:\n\n$\\mu_0=0.758939742420249$, $\\mu_1=0.0601995732152425$, $\\mu_2=0.3945593622881204$, and $\\mu=0.28393632703149974$."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's first generate submission files to obtain rmse000, rmse100, rmse010, rmse001\nlbp.probe_mean()\n# submit and obtain public leaderboard scores, then use the next line to obtain the group means\nlbp.estimate_mean(1.250111, 1.23528, 1.38637, 1.29326)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Mean Scaling**\n\nAfter our model predits the targets for the three test groups, we check the mean prediction for each group and compare them to $\\mu_{0}, \\mu_{1}, \\mu_{2}$, respectively.  We can then add a constant to our predictions so that their means are shifted to better match the leaderboard means.\n\nThe proof for this mean scaling is the following:\n\nIf $y_i$ and $\\hat{y}_i$ are the true value and the predicted value for sample $i$.  We would like to add a constant $c$ to all predictions to reduce the $rmse$ score, i.e.:\n\n$$\\frac{\\partial}{\\partial c} \\sum_{i=1}^{n}{{\\left(y_i - (\\hat{y}_i + c)\\right)}^2} = 0$$\n\nIt is not hard to derive that:\n\n$$c = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)$$\n\nThe get the best results, each of the three test groups are mean scaled independently. Mean scaling is guaranteed to reduce RMSE. We then use clip(0,20) to further improve the score.  Notice clip(0,20) will change the means, therefore, we can repeatively apply the mean scaling routine until predictions converge.  This logic is implemented in **LeaderBoardProbing.mean_scale()**.\n\n*You should always apply this routine to your predictions, as it does not cost you any presubmission and the corrected prediction is always better than your original file!*"},{"metadata":{},"cell_type":"markdown","source":"**Variance Scaling Version 1**\n\nOur prediction has now been mean corrected, so that it matches the true value $\\mu$.  The submission of this **mean.csv** file gives us the public leaderboard $rmse$ score.  This score allows us to multiple the residues of our prediction by a constant $\\lambda$ to further reduce the score, i.e., we aim for a $\\lambda$ by:\n\n$$\\frac{\\partial}{\\partial \\lambda} \\sum_i^N {{\\left(y_i - \\lambda (\\hat{y}_i-\\mu) - \\mu \\right)}^2} = 0 $$,\n\nWe can derive:\n\n$$\\lambda = \\frac{\\sum_i {(y_i -\\mu)(\\hat{y}_i - \\mu)}}{\\sum_i {(\\hat{y}_i - \\mu)}^2} = \\frac{\\sum_i {(y_i \\hat{y}_i -\\mu^2)} }{\\sum_i {(\\hat{y}_i^2 - \\mu^2)}}$$\n\nThe only term unknown is $\\sum_i {y_i \\hat{y}_i}$.  From previous probings, we have $rmse$ score and $rmse_{000}$:\n\n$$ N \\cdot {rmse}_{000}^2 = \\sum_i y_i^2$$\n\n$$N \\cdot {rmse}^2 = \\sum_i {(y_i-\\hat{y}_i)^2} = \\sum_i y_i^2 + \\sum_i \\hat{y}_i^2 - 2 \\sum_i {y_i \\hat{y}_i}$$,\n\ntherefore, \n\n$$\\sum_i {y_i \\hat{y}_i} = \\frac{1}{2} N \\cdot (rmse_{000}^2 - rmse^2) + \\frac{1}{2}\\sum_i \\hat{y}_i^2$$.\n\nThus, by obtaining the $rmse$ score for our **mean.csv** submission, we can calculate $\\lambda$ and obtain a new **mean_lambda_mean.csv** file.  After we apply $\\lambda$ scaling, we can clip(0, 20), then call mean_scale() to polish the answer further.  This whole logic is implemented in **LeaderBoardProbing.variance_scale()**."},{"metadata":{},"cell_type":"markdown","source":"**Variance Scaling Version 2**\n\nWe first need to call **LeaderBoardProbling.flip_signs()** to create three submissions and obtain their RMSE scores.  For example, flip_signs() turns **mean.csv** into **mean_mpp.csv**, **mean_pmp.csv**, and **mean_ppm.csv**.  The suffix \"p\" stands for plus $+$ and \"m\" stands for $-$ in the formula below.\n\nWhat flip_signs() does is to flip the signs of our predictions, once for each test group, i.e., turn $\\hat{y}_i$ into $-\\hat{y}_i$, but only one group at a time.  We already have $rmse$, we just need to obtain $rmse_{-++}, rmse_{+-+}, rmse_{++-}$ through three manual submissions.\n\nSince\n\n$$N \\cdot rmse^2 = \\sum_{j=0,i=1}^{n_0} {(y_i - \\hat{y}_i)^2} + \\sum_{j=1,i=1}^{n_1} {(y_i - \\hat{y}_i)^2} +\\sum_{j=2,i=1}^{n_2} {(y_i - \\hat{y}_i)^2}$$\n\n$$N \\cdot rmse_{-++}^2 = \\sum_{j=0,i=1}^{n_0} {(y_i + \\hat{y}_i)^2} + \\sum_{j=1,i=1}^{n_1} {(y_i - \\hat{y}_i)^2} +\\sum_{j=2,i=1}^{n_2} {(y_i - \\hat{y}_i)^2}$$\n\n$$N \\cdot rmse_{+-+}^2 = \\sum_{j=0,i=1}^{n_0} {(y_i - \\hat{y}_i)^2} + \\sum_{j=1,i=1}^{n_1} {(y_i + \\hat{y}_i)^2} +\\sum_{j=2,i=1}^{n_2} {(y_i - \\hat{y}_i)^2}$$\n\n$$N \\cdot rmse_{++-}^2 = \\sum_{j=0,i=1}^{n_0} {(y_i - \\hat{y}_i)^2} + \\sum_{j=1,i=1}^{n_1} {(y_i - \\hat{y}_i)^2} +\\sum_{j=2,i=1}^{n_2} {(y_i + \\hat{y}_i)^2}$$\n\nFrom the above 4 equations, we can calculate:\n\n$$\\sum_{j=0,i=1}^{n_0} { y_i \\hat{y}_i } = \\frac{N}{4} (rmse_{-++}^2 - rmse_{+++}^2)$$ \n\n$$\\sum_{j=1,i=1}^{n_0} { y_i \\hat{y}_i } = \\frac{N}{4} (rmse_{+-+}^2 - rmse_{+++}^2)$$ \n\n$$\\sum_{j=2,i=1}^{n_0} { y_i \\hat{y}_i } = \\frac{N}{4} (rmse_{++-}^2 - rmse_{+++}^2)$$ \n\nSimilar to version 1, we can now calculate multiplers for each test group independently:\n\n$$\\lambda_0 = \\frac{\\sum_{j=0,i=1}^{n_0} {(y_i \\hat{y}_i -\\mu_0^2)} }{\\sum_{j=0,i=1}^{n_0} {(\\hat{y}_i^2 - \\mu_0^2)}}$$\n\n$$\\lambda_1 = \\frac{\\sum_{j=1,i=1}^{n_1} {(y_i \\hat{y}_i -\\mu_1^2)} }{\\sum_{j=1,i=1}^{n_1} {(\\hat{y}_i^2 - \\mu_1^2)}}$$\n\n$$\\lambda_2 = \\frac{\\sum_{j=2,i=1}^{n_2} {(y_i \\hat{y}_i -\\mu_2^2)} }{\\sum_{j=2,i=1}^{n_2} {(\\hat{y}_i^2 - \\mu_2^2)}}$$\n\nAfter multiplation, we do clip(0,20) followed by mean scaling. The logic is implemented in **LeaderBoardProbing.variance_scale_v2()**.\n\nTheoretically, we can repeat these processes to obtain further improvement.  In practice, the score hardly changes and there is no need to repeat this process.  Also, version 2 typically is only slightly better than version 1.  So you can simply use version 1 and be done with it, only use version 2 for your final submission.\n"},{"metadata":{},"cell_type":"markdown","source":"**Notes**\n\nI enjoy the theoretical aspects of machine learning and occassionally blog on related topics (http://randomsciencystuff.blogspot.com/2017/05/notes-on-machine-learning.html).  Although data leaking and leaderboard probing are useless in real-life machine learning projects, they are a fun topic I learned from this course.  If you get other data leak ideas and would like to share, I will be very interested in learning.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}