{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Predict Future Sales\n\nThese notebook was based on three kernels, mostly on the first one. Thank very much [**dlarionov**](https://www.kaggle.com/dlarionov/), [**dimitreoliveira**](https://www.kaggle.com/dimitreoliveira/), and [**kyakovlev**](https://www.kaggle.com/kyakovlev/)!\n\n- https://www.kaggle.com/dlarionov/feature-engineering-xgboost\n- https://www.kaggle.com/dimitreoliveira/model-stacking-feature-engineering-and-eda\n- https://www.kaggle.com/kyakovlev/1st-place-solution-part-1-hands-on-data\n\n### 1) Exploratory Data Analysis\n\n#### Importing Packages and Datasets"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport time\nimport itertools\nfrom sklearn.preprocessing import LabelEncoder\n\n\nsns.set()\n\ntrain = pd.read_csv('/kaggle/input/competitive-data-science-predict-future-sales/sales_train.csv')\ntrain['date'] = pd.to_datetime(train['date'], format = '%d.%m.%Y', infer_datetime_format = True)\n\ncats = pd.read_csv('/kaggle/input/competitive-data-science-predict-future-sales/item_categories.csv')\nitems = pd.read_csv('/kaggle/input/competitive-data-science-predict-future-sales/items.csv')\nsub = pd.read_csv('/kaggle/input/competitive-data-science-predict-future-sales/sample_submission.csv')\nshops = pd.read_csv('/kaggle/input/competitive-data-science-predict-future-sales/shops.csv')\ntest = pd.read_csv('/kaggle/input/competitive-data-science-predict-future-sales/test.csv').set_index('ID')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Outliers"},{"metadata":{"trusted":true},"cell_type":"code","source":"f, axes = plt.subplots(2, 2, figsize = (17,9))\ng0 = sns.distplot(train.item_price, kde = False, ax = axes[0,0])\ng1 = sns.boxplot(train.item_price, ax = axes[0,1])\ng2 = sns.distplot(train.item_price[train.item_price.between(0,10000)], bins = 100, kde = False, ax = axes[1,0])\ng3 = sns.boxplot(train.item_price[train.item_price.between(0,10000)], ax = axes[1,1], fliersize = 2)\n\nf.suptitle('Item Price - Distribution and Boxplot', fontsize = 16, fontweight='bold')\ng0.set_xlabel('Item Price', size = 14)\ng1.set_xlabel('Item Price', size = 14)\ng2.set_xlabel('Item Price', size = 14)\ng3.set_xlabel('Item Price', size = 14);","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"f, axes = plt.subplots(2, 2, figsize = (17,9))\ng0 = sns.distplot(train.item_cnt_day, kde = False, ax = axes[0,0])\ng1 = sns.boxplot(train.item_cnt_day, ax = axes[0,1])\ng2 = sns.distplot(train.item_cnt_day[train.item_cnt_day.between(0,100)], bins = 100, kde = False, ax = axes[1,0])\ng3 = sns.boxplot(train.item_cnt_day[train.item_cnt_day.between(0,100)], ax = axes[1,1], fliersize = 2)\n\nf.suptitle('Item Count per Day - Distribution and Boxplot', fontsize=16, fontweight='bold')\ng0.set_xlabel('Item Count per Day', size = 14)\ng1.set_xlabel('Item Count per Day', size = 14)\ng2.set_xlabel('Item Count per Day', size = 14)\ng3.set_xlabel('Item Count per Day', size = 14);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Sold Items x Shops"},{"metadata":{"trusted":true},"cell_type":"code","source":"f, axes = plt.subplots(1, 1, figsize = (17,5))\ng = sns.barplot(x = 'shop_id', y = 'item_cnt_day', data = train,\n                estimator = lambda x: np.sum(x) / np.sum(train.item_cnt_day) * 100,\n                order = train.groupby('shop_id').sum()['item_cnt_day'].sort_values(ascending = False).index,\n                color = 'orange', ci = None)\ng.set_xlabel('Shop ID', size = 14)\ng.set_ylabel('Sold Items (%)', size = 14);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Time Series Visualization"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_month_year = train.groupby(['date_block_num']).sum().reset_index().drop(columns = ['item_price', 'shop_id', 'item_id'])\n\nf, axes = plt.subplots(1, 1, figsize = (17,5))\ng = sns.lineplot(x = 'date_block_num', y = 'item_cnt_day', data = train_month_year)\ng.set_xlabel('Month', size = 14)\ng.set_ylabel('Item Count Month', size = 14)\ng.set_xlim(0, 33);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Outdated items"},{"metadata":{"trusted":true},"cell_type":"code","source":"sales_by_item_id = train.pivot_table(index = ['item_id'], values = ['item_cnt_day'], columns = 'date_block_num',\n                                     aggfunc = np.sum, fill_value = 0).reset_index()\nsales_by_item_id.columns = ['item_id'] + list(sales_by_item_id.columns.droplevel()[1:].map(str))\n\noutdated_items_3mo = sales_by_item_id[sales_by_item_id.loc[:,'31':].sum(axis=1)==0]\noutdated_items_6mo = sales_by_item_id[sales_by_item_id.loc[:,'28':].sum(axis=1)==0]\n\nprint('Outdated items (3 months):', 100.0*outdated_items_3mo.shape[0]/sales_by_item_id.shape[0])\nprint('Outdated items (6 months):', 100.0*outdated_items_6mo.shape[0]/sales_by_item_id.shape[0])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Outdated shops"},{"metadata":{"trusted":true},"cell_type":"code","source":"sales_by_shop_id = train.pivot_table(index = ['shop_id'], values = ['item_cnt_day'], columns = 'date_block_num',\n                                     aggfunc = np.sum, fill_value = 0).reset_index()\nsales_by_shop_id.columns = ['shop_id'] + list(sales_by_shop_id.columns.droplevel()[1:].map(str))\n\noutdated_shop_3mo = sales_by_shop_id[sales_by_shop_id.loc[:,'31':].sum(axis=1)==0]\noutdated_shop_6mo = sales_by_shop_id[sales_by_shop_id.loc[:,'28':].sum(axis=1)==0]\n\nprint('Outdated shops (3 months):', 100.0*outdated_shop_3mo.shape[0]/sales_by_shop_id.shape[0])\nprint('Outdated shops (6 months):', 100.0*outdated_shop_6mo.shape[0]/sales_by_shop_id.shape[0])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2) Feature Engineering"},{"metadata":{},"cell_type":"markdown","source":"#### Organizing and Extracting Features - Shops"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_featured = train.copy()\ntest_featured = test.copy()\n\ntrain_featured.loc[train_featured.shop_id == 11, 'shop_id'] = 10\ntest_featured.loc[test_featured.shop_id == 11, 'shop_id'] = 10\n\ntrain_featured.loc[train_featured.shop_id == 1, 'shop_id'] = 58\ntest_featured.loc[test_featured.shop_id == 1, 'shop_id'] = 58\n\ntrain_featured.loc[train_featured.shop_id == 0, 'shop_id'] = 57\ntest_featured.loc[test_featured.shop_id == 0, 'shop_id'] = 57\n\ntrain_featured.loc[train_featured.shop_id == 40, 'shop_id'] = 39\ntest_featured.loc[test_featured.shop_id == 40, 'shop_id'] = 39\n\nshops_featured = shops.copy()\nshops_featured['shop_name'] = shops_featured['shop_name'].apply(lambda x: x.lower()).str.replace('[^\\w\\s]', '').str.replace('\\d+','').str.strip()\nshops_featured['shop_city'] = shops_featured['shop_name'].str.partition(' ')[0]\nshops_featured['shop_type'] = shops_featured['shop_name'].apply(lambda x: 'мтрц' if 'мтрц' in x else 'трц' if 'трц' in x else 'трк' if 'трк' in x else 'тц' if 'тц' in x else 'тк' if 'тк' in x else 'NO_DATA')\n\nshops_featured['shop_city'] = LabelEncoder().fit_transform(shops_featured['shop_city'])\nshops_featured['shop_type'] = LabelEncoder().fit_transform(shops_featured['shop_type'])\n\nshops_featured.drop(columns = 'shop_name', inplace = True)\nshops_featured.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Organizing and Extracting Features - Categories"},{"metadata":{"trusted":true},"cell_type":"code","source":"cats_featured = cats.copy()\ncats_featured['split'] = cats_featured['item_category_name'].str.split('-')\ncats_featured['category_type'] = cats_featured['split'].map(lambda x: x[0].strip())\ncats_featured['category_subtype'] = cats_featured['split'].map(lambda x: x[1].strip() if len(x) > 1 else x[0].strip())\n\ncats_featured['category_type'] = LabelEncoder().fit_transform(cats_featured['category_type'])\ncats_featured['category_subtype'] = LabelEncoder().fit_transform(cats_featured['category_subtype'])\n\n\ncats_featured = cats_featured[['item_category_id','category_type', 'category_subtype']]\ncats_featured.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Organizing and Extracting Features - Items"},{"metadata":{"trusted":true},"cell_type":"code","source":"items_featured = items.copy()\nitems_featured.drop(columns = ['item_name'], inplace = True)\nitems_featured.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Organizing dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"# # Creating base dataframe with all the possible permutations between shops and items present on the test set\n# import itertools\n\n# ts = time.time()\n\n# date_block_nums = np.array(range(35))\n# shops_test = test.shop_id.unique()\n# items_test = test.item_id.unique()\n# base = pd.DataFrame(itertools.product(date_block_nums, shops_test, items_test),\n#                     columns = ['date_block_num','shop_id','item_id'])\n\n# time.time() - ts","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from itertools import product\n\nts = time.time()\n\nbase = []\ncols = ['date_block_num','shop_id','item_id']\nfor i in range(34):\n    sales = train_featured[train_featured.date_block_num==i]\n    base.append(np.array(list(product([i], sales.shop_id.unique(), sales.item_id.unique())), dtype = 'int16'))\n    \nbase = pd.DataFrame(np.vstack(base), columns=cols)\nbase.sort_values(cols,inplace = True)\n\ntime.time() - ts","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Creating Revenue Feature\ntrain_featured['revenue'] = train_featured['item_price'] *  train_featured['item_cnt_day']\n\n# Eliminating \"item_cnt_day\" and \"item_price\" outliers\ntrain_featured = train_featured[(train_featured.item_cnt_day.between(0,1000)) & (train_featured.item_price.between(0,100000))]\n\n# Aggregating by month\ntrain_featured_month = (train_featured[['date_block_num', 'shop_id', 'item_id', 'item_cnt_day']]\n                        .groupby(['date_block_num','shop_id','item_id'])\n                        .sum())\ntrain_featured_month.reset_index(inplace = True)\ntrain_featured_month['item_cnt_month'] = train_featured_month['item_cnt_day'].clip(0,20).fillna(0)\ntrain_featured_month.drop(columns = 'item_cnt_day', inplace = True)\n\n# Preparating test set\ntest_featured['date_block_num'] = 34\ntest_featured = test_featured[['date_block_num', 'shop_id', 'item_id']]\n\n# Concatenating train and test set\nbase = pd.concat([base, test_featured])\n\n# Concatenating shops and categories information\ndf_raw = pd.merge(base, train_featured_month, on = ['date_block_num', 'shop_id', 'item_id'], how = 'left').fillna(0)\ndf_raw = pd.merge(df_raw, shops_featured, on = ['shop_id'], how='left')\ndf_raw = pd.merge(df_raw, items_featured, on = ['item_id'], how='left')\ndf_raw = pd.merge(df_raw, cats_featured, on = ['item_category_id'], how='left')\n\n# Adding month and year feature\ndf_raw['year'] = df_raw['date_block_num'].apply(lambda x: ((x//12) + 2013))\ndf_raw['month'] = df_raw['date_block_num'].apply(lambda x: (x % 12) + 1)\n\ndf_raw.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Lag Features"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_raw_1 = df_raw.copy()\n\ndef lag_feature(df, lags, col):\n    tmp = df[['date_block_num', 'shop_id', 'item_id', col]]\n    for i in lags:\n        shifted = tmp.copy()\n        shifted.columns = ['date_block_num', 'shop_id', 'item_id', col + '_lag_' + str(i)]\n        shifted['date_block_num'] += i\n        df = pd.merge(df, shifted, on = ['date_block_num', 'shop_id', 'item_id'], how = 'left')\n    return df\n\ndf_raw_1 = lag_feature(df_raw_1, [1, 2, 3, 4, 5, 6], 'item_cnt_month')\n\ndataset = df_raw_1.copy()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Trend Features"},{"metadata":{"trusted":true},"cell_type":"code","source":"ts = time.time()\n\n# Add total average price per item feature\ngroup = train_featured.groupby(['item_id']).agg({'item_price': ['mean']})\ngroup.columns = ['item_avg_item_price']\ngroup.reset_index(inplace = True)\ndataset = pd.merge(dataset, group, on = ['item_id'], how = 'left')\n\n# Add monthly average price per item feature\ngroup = train_featured.groupby(['date_block_num', 'item_id']).agg({'item_price': ['mean']})\ngroup.columns = ['date_item_avg_item_price']\ngroup.reset_index(inplace = True)\ndataset = pd.merge(dataset, group, on = ['date_block_num', 'item_id'], how = 'left')\n\nlags = [1, 2, 3, 4, 5, 6]\n\ndef lag_feature_2(df, lags, col):\n    tmp = df[['item_avg_item_price','date_block_num', 'shop_id', 'item_id', col]]\n    for i in lags:\n        shifted = tmp.copy()\n        shifted.columns = ['item_avg_item_price', 'date_block_num', 'shop_id', 'item_id', col + '_lag_' + str(i)]\n        shifted['date_block_num'] += i\n        df = pd.merge(df, shifted, on = ['item_avg_item_price', 'date_block_num', 'shop_id', 'item_id'], how = 'left')\n    return df\n\ndataset = lag_feature_2(dataset, lags, 'date_item_avg_item_price')\n\nfor i in lags:\n    dataset['delta_price_lag_' + str(i)] = \\\n        (dataset['date_item_avg_item_price_lag_' + str(i)] - dataset['item_avg_item_price']) / dataset['item_avg_item_price']\n\ndef select_trend(row):\n    for i in lags:\n        if row['delta_price_lag_' + str(i)]:\n            return row['delta_price_lag_' + str(i)]\n    return 0\n    \ndataset['delta_price_lag'] = dataset.apply(select_trend, axis=1)\ndataset['delta_price_lag'] = dataset['delta_price_lag'].astype(np.float16)\ndataset['delta_price_lag'].fillna(0, inplace = True)\n\nfeatures_to_drop = ['item_avg_item_price', 'date_item_avg_item_price']\nfor i in lags:\n    features_to_drop += ['date_item_avg_item_price_lag_' + str(i)]\n    features_to_drop += ['delta_price_lag_' + str(i)]\n\ndataset.drop(features_to_drop, axis = 1, inplace = True)\n\ntime.time() - ts","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Months since the last sale for each shop/item pair and for item"},{"metadata":{"trusted":true},"cell_type":"code","source":"# ts = time.time()\n# cache = {}\n# dataset['item_shop_last_sale'] = -1\n# dataset['item_shop_last_sale'] = dataset['item_shop_last_sale'].astype(np.int8)\n# for idx, row in dataset.iterrows():    \n#     key = str(row.item_id)+' '+str(row.shop_id)\n#     if key not in cache:\n#         if row.item_cnt_month!=0:\n#             cache[key] = row.date_block_num\n#     else:\n#         last_date_block_num = cache[key]\n#         dataset.at[idx, 'item_shop_last_sale'] = row.date_block_num - last_date_block_num\n#         cache[key] = row.date_block_num         \n\n# ####################################\n\n# cache = {}\n# dataset['item_last_sale'] = -1\n# dataset['item_last_sale'] = dataset['item_last_sale'].astype(np.int8)\n# for idx, row in dataset.iterrows():    \n#     key = row.item_id\n#     if key not in cache:\n#         if row.item_cnt_month!=0:\n#             cache[key] = row.date_block_num\n#     else:\n#         last_date_block_num = cache[key]\n#         if row.date_block_num>last_date_block_num:\n#             dataset.at[idx, 'item_last_sale'] = row.date_block_num - last_date_block_num\n#             cache[key] = row.date_block_num\n\n# ####################################\n\n# dataset['item_shop_first_sale'] = dataset['date_block_num'] - dataset.groupby(['item_id','shop_id'])['date_block_num'].transform('min')\n# dataset['item_first_sale'] = dataset['date_block_num'] - dataset.groupby('item_id')['date_block_num'].transform('min')\n# time.time() - ts","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 3) Train/Test Split\n\n- **`Train set`**: months 0 - 32\n\n- **`Validation set`**: month 33\n\n- **`Test set`**: month 34"},{"metadata":{},"cell_type":"markdown","source":"#### Mean Encoding"},{"metadata":{"trusted":true},"cell_type":"code","source":"# features_to_encode = ['item_id',\n#                       'shop_id', 'shop_city', 'shop_type',\n#                       'item_category_id', 'category_type', 'category_subtype',\n#                       'year', 'month']\n\n# for feature_i in features_to_encode:\n#     gp_feature_mean = dataset[dataset.date_block_num < 34].groupby([feature_i]).agg({'item_cnt_month': ['mean']})\n#     gp_feature_mean.columns = [feature_i + '_mean']\n#     gp_feature_mean.reset_index(inplace = True)\n\n#     dataset = pd.merge(dataset, gp_feature_mean, on = [feature_i], how = 'left')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"list_features_to_encode = [['date_block_num'],\n                           ['date_block_num', 'item_id'],\n                           ['date_block_num', 'shop_id'],\n                           ['date_block_num', 'item_category_id'],\n                           ['date_block_num', 'shop_id', 'item_category_id']]\n\nfor list_feature_i in list_features_to_encode:\n    print(list_feature_i)\n    gp_feature_mean = dataset.groupby(list_feature_i).agg({'item_cnt_month': ['mean']})\n    column_name = '_'.join(list_feature_i) + '_mean'\n    gp_feature_mean.columns = [column_name]\n    gp_feature_mean.reset_index(inplace = True)\n\n    dataset = pd.merge(dataset, gp_feature_mean, on = list_feature_i, how = 'left')\n    dataset = lag_feature(dataset, [1,2], column_name)\n    dataset.drop([column_name], axis = 1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Filling Missing Values"},{"metadata":{"trusted":true},"cell_type":"code","source":"for shop_id in dataset['shop_id'].unique():\n    for column in dataset.columns:\n        shop_median = dataset[(dataset['shop_id'] == shop_id)][column].median()\n        \n        dataset.loc[(dataset[column].isnull()) & (dataset['shop_id'] == shop_id), column] = shop_median","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Saving Dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"del train\ndel cats\ndel items\ndel sub\ndel shops\ndel test\ndel train_featured\ndel cats_featured\ndel items_featured\ndel shops_featured\ndel test_featured\ndel df_raw\ndel df_raw_1\ndel base","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def downcast_dtypes(df):\n    float_cols = [c for c in df if df[c].dtype == \"float64\"]\n    int_cols = [c for c in df if df[c].dtype in [\"int64\", \"int32\"]]\n    df[float_cols] = df[float_cols].astype(np.float32)\n    df[int_cols] = df[int_cols].astype(np.int16)\n    return df\n\ndataset = downcast_dtypes(dataset)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.to_pickle('final_dataset.pkl')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4) Model Training\n\n- **XGBoost**\n\n- **CatBoost**\n\n- **LightGBM**\n\n- **Model Stacking**"},{"metadata":{"trusted":true},"cell_type":"code","source":"import datetime\nimport warnings\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport xgboost as xgb\nimport lightgbm as lgb\nimport catboost\nimport pickle\nfrom catboost import CatBoostRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.linear_model import LinearRegression, Lasso, ElasticNet, Ridge, SGDRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset = pd.read_pickle('/kaggle/working/final_dataset.pkl')\n\ntrain_set = dataset[dataset.date_block_num.between(12,32)]\nvalidation_set = dataset[dataset.date_block_num == 33]\ntest_set = dataset[dataset.date_block_num == 34]\n\nX_train = train_set.drop(columns = ['item_cnt_month'])\nY_train = train_set['item_cnt_month']\n\nX_validation = validation_set.drop(columns = ['item_cnt_month'])\nY_validation = validation_set['item_cnt_month']\n\nX_test = test_set.drop(columns = ['item_cnt_month'])\nY_test = test_set['item_cnt_month']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del dataset\ndel train_set\ndel validation_set\ndel test_set","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4.1) XGBoost"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"dtrain = xgb.DMatrix(X_train, label = Y_train)\ndvalidation = xgb.DMatrix(X_validation, label = Y_validation)\ndtest = xgb.DMatrix(X_test, label = Y_test)\n\nwatchlist = [(dtrain, 'train'), (dvalidation, 'validation')]\n\nparams = {'objective': 'reg:squarederror',\n          'tree_method': 'gpu_hist',\n          'eval_metric': 'rmse',\n          'eta': 0.1,\n          'max_depth': 4,\n          'random_state': 0}\n\nmodel_xgb = xgb.train(params, dtrain, 1000, watchlist, early_stopping_rounds = 10)\npickle.dump(model_xgb, open(\"model_xgb.pickle.dat\", \"wb\"))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4.2) Catboost"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"model_cat = CatBoostRegressor(\n    task_type = 'GPU',\n    iterations = 1000,\n    random_seed = 0,\n    learning_rate = 0.1,\n    od_type = 'Iter',\n    od_wait = 10,\n    eval_metric = 'RMSE'\n)\n\nmodel_cat.fit(\n    X_train, Y_train,\n    eval_set = (X_validation, Y_validation),\n    logging_level = 'Verbose',\n    plot = False\n)\n\npickle.dump(model_cat, open(\"model_cat.pickle.dat\", \"wb\"))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4.3) LightGBM"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"params = {\n    \"objective\" : \"regression\",\n    #\"device_type\": \"gpu\",\n    \"metric\" : \"rmse\",\n    \"learning_rate\": 0.01,\n    \"seed\": 0\n}\n\nlgtrain = lgb.Dataset(X_train, label = Y_train)\nlgval = lgb.Dataset(X_validation, label = Y_validation)\nlgtest = lgb.Dataset(X_test, label = Y_test)\nevals_result = {}\nmodel_light = lgb.train(params, lgtrain, 1000, \n                  valid_sets = [lgtrain, lgval], \n                  early_stopping_rounds = 20, \n                  evals_result = evals_result)\n\npickle.dump(model_light, open(\"model_light.pickle.dat\", \"wb\"))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4.4) Model Stacking and Predictions"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Importing Predictions\nmodel_xgb = pickle.load(open(\"/kaggle/working/model_xgb.pickle.dat\", \"rb\"))\nmodel_cat = pickle.load(open(\"/kaggle/working/model_cat.pickle.dat\", \"rb\"))\nmodel_light = pickle.load(open(\"/kaggle/working/model_light.pickle.dat\", \"rb\"))\n\n# XGBoost Predictions\ny_pred_train_xgb = model_xgb.predict(dtrain).clip(0,20)\ny_pred_validation_xgb = model_xgb.predict(dvalidation).clip(0,20)\ny_pred_test_xgb = model_xgb.predict(dtest).clip(0,20)\n\n# Catboost Predictions\ny_pred_train_cat = model_cat.predict(X_train).clip(0,20)\ny_pred_validation_cat = model_cat.predict(X_validation).clip(0,20)\ny_pred_test_cat = model_cat.predict(X_test).clip(0,20)\n\n# LightGBM Predictions\ny_pred_train_light = model_light.predict(X_train).clip(0,20)\ny_pred_validation_light = model_light.predict(X_validation).clip(0,20)\ny_pred_test_light = model_light.predict(X_test).clip(0,20)\n\n# Mounting train dataframe for stacking\ndf_train = pd.concat([pd.DataFrame(y_pred_train_xgb), pd.DataFrame(y_pred_train_cat), pd.DataFrame(y_pred_train_light), Y_train.reset_index()['item_cnt_month']], axis = 1)\ndf_train.columns = ['XGBoost', 'Catboost', 'LightGBM', 'Y']\n\n# Mounting vaidation dataframe for stacking\ndf_validation = pd.concat([pd.DataFrame(y_pred_validation_xgb), pd.DataFrame(y_pred_validation_cat), pd.DataFrame(y_pred_validation_light), Y_validation.reset_index()['item_cnt_month']], axis = 1)\ndf_validation.columns = ['XGBoost', 'Catboost', 'LightGBM', 'Y']\n\n# Mounting test dataframe for stacking\ndf_test = pd.concat([pd.DataFrame(y_pred_test_xgb), pd.DataFrame(y_pred_test_cat), pd.DataFrame(y_pred_test_light), Y_test.reset_index()['item_cnt_month']], axis = 1)\ndf_test.columns = ['XGBoost', 'Catboost', 'LightGBM', 'Y']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"meta_model = LinearRegression(n_jobs = -1)\n\ndf_train_X = df_train.drop(columns = 'Y')\ndf_train_Y = df_train['Y']\n\ndf_validation_X = df_validation.drop(columns = 'Y')\ndf_validation_Y = df_validation['Y']\n\ndf_test_X = df_test.drop(columns = 'Y')\ndf_test_Y = df_test['Y']\n\nmeta_model.fit(df_train_X, df_train_Y)\nfinal_pred_train = meta_model.predict(df_train_X).clip(0,20)\nfinal_pred_validation = meta_model.predict(df_validation_X).clip(0,20)\nfinal_pred_test = meta_model.predict(df_test_X).clip(0,20)\n\nprint('Stacking | RMSE on Train Set:', np.sqrt(mean_squared_error(final_pred_train, df_train_Y)))\nprint('Stacking | RMSE on Validation Set:', np.sqrt(mean_squared_error(final_pred_validation, df_validation_Y)))\nprint(' XGBoost | RMSE on Validation Set:', ((df_validation.Y - df_validation.XGBoost)**2).mean()**0.5)\nprint('Catboost | RMSE on Validation Set:', ((df_validation.Y - df_validation.Catboost)**2).mean()**0.5)\nprint('LightGBM | RMSE on Validation Set:', ((df_validation.Y - df_validation.LightGBM)**2).mean()**0.5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Saving Predictions\ndef submit_df(df_raw):\n    df_out = df_raw.reset_index()\n    df_out.columns = ['ID', 'item_cnt_month']\n    return df_out\n\nsubmit_df(pd.DataFrame(y_pred_test_xgb)).to_csv('y_pred_test_xgb.csv', index = False)\nsubmit_df(pd.DataFrame(y_pred_test_cat)).to_csv('y_pred_test_cat.csv', index = False)\nsubmit_df(pd.DataFrame(y_pred_test_light)).to_csv('y_pred_test_light.csv', index = False)\nsubmit_df(pd.DataFrame(final_pred_test)).to_csv('y_pred_test_stack.csv', index = False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"}},"nbformat":4,"nbformat_minor":1}