{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Predict Future Sales\n\nNotes from the competition host:\n\n*This challenge serves as final project for the \"How to win a data science competition\" Coursera course.*\n\n*In this competition you will work with a challenging time-series dataset consisting of daily sales data, kindly provided by one of the largest Russian software firms - 1C Company.*\n\n*We are asking you to predict total sales for every product and store in the next month. By solving this competition you will be able to apply and enhance your data science skills.*\n\n*Submissions are evaluated by root mean squared error (RMSE). **True target values are clipped into [0,20] range**.*\n\n*For each id in the test set, you must predict a total number of sales.*"},{"metadata":{},"cell_type":"markdown","source":"# Import Libraries, First Steps of Analysis"},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np  # linear algebra\nimport pandas as pd\n\npd.set_option(\"display.float_format\", lambda x: \"%.2f\" % x)\npd.set_option(\"display.max_columns\", 500)\npd.set_option(\"display.max_rows\", 1500)\n\nimport warnings\n\nimport sys\nimport calendar\nimport datetime\nimport os\n\n# Input data files are available in the read-only '../input/' directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nfrom itertools import product\nimport random\nimport gc\nimport matplotlib.pyplot as plt\nimport matplotlib.ticker as mticker\nimport plotly\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport seaborn as sns\nfrom lightgbm import LGBMRegressor,plot_importance\nfrom plotly.subplots import make_subplots\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom sklearn.preprocessing import StandardScaler,OrdinalEncoder\nfrom sklearn.preprocessing import KBinsDiscretizer\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using 'Save & Run All'\n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"%%javascript\nIPython.OutputArea.auto_scroll_threshold = 9999;","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Loading only train and test sets here.<br> We have more input data than these two; however, due to high memory use of these files, I will import the others later, right when I need them.<br> Additionally, to utilize the use of local variables and reduce memory footprint, I wrapped most of the code in functions."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"random.seed(35)\n  \ntrain = pd.read_csv('../input/competitive-data-science-predict-future-sales/sales_train.csv')\ntest = pd.read_csv('../input/competitive-data-science-predict-future-sales/test.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A small utility function to downcast dataframe throughout the notebook as we're working with large amount of data."},{"metadata":{"trusted":true},"cell_type":"code","source":"def downcast_df(df):\n\n    before = df.memory_usage()\n    # skip any columns with nan to prevent crash\n    cols_to_skip = df.isna().sum()[df.isna().sum() > 0].index\n    numcols = [\n        col for col in df.select_dtypes(\"number\").columns if col not in cols_to_skip\n    ]\n    ints = df.select_dtypes(int).columns\n    floats = df.select_dtypes(float).columns\n\n    for col in ints:\n        df[col] = pd.to_numeric(df[col].values, downcast=\"integer\")\n    for col in floats:\n        df[col] = pd.to_numeric(df[col].values, downcast=\"float\")\n\n    after = df.memory_usage()\n    reduction = (before - after) / before * 100\n\n    comparison = pd.concat([before, after, reduction], axis=1)\n    comparison.columns = [\n        \"before\",\n        \"after\",\n        \"reduction(%)\",\n    ]\n\n    print(f\"Downcasting: \\n  {comparison}\\n\\n\")\n\n    return df\n\n\ndf = downcast_df(train)\ndf = downcast_df(test)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"print(f'Train set contains {train.shape[0]} rows,{train.shape[1]} columns. \\nTest set contains {test.shape[0]} rows, {test.shape[1]} columns.\\n')\nprint(f'These are the features in TRAIN and NOT in TEST ->{set(train.columns) - set(test.columns)}.\\nThese are the features in TEST and NOT in TRAIN -> {set(test.columns) - set(train.columns)}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Checking the data types and missing value counts in train and test sets"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"train.info(verbose=True, null_counts=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"test.info(verbose=True, null_counts=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Looking at a sample of records from train and test sets"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"train.sample(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"test.sample(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Descriptive statistics as follows:"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"train.drop(columns=['item_id','shop_id']).describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"-> There are negative sales quantities (item_cnt_day). I assume they indicate returns. <br>\n-> There is also negative item price. I assume this is an error and needs to be corrected."},{"metadata":{},"cell_type":"markdown","source":"Modifying the test dataframe so that the columns align with train df."},{"metadata":{"trusted":true},"cell_type":"code","source":"test = test.drop('ID', axis=1)\n\ntest['date'] = '01.11.2015'\ntest['date_block_num'] = 34\ntest['item_cnt_day'] = np.nan\ntest['item_price'] = np.nan","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Taking a closer look at the record(s) with the negative item price."},{"metadata":{"trusted":true},"cell_type":"code","source":"train[train['item_price']<0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Only one item shows negative price. Checking other records for the same item/shop to see the other prices used."},{"metadata":{"trusted":true},"cell_type":"code","source":"train[(train['item_id']==2973)&(train['shop_id']==32)]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Imputing the negative item price with the most common occurrance -> 2499"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.loc[train['item_price']<0,'item_price'] = 2499","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Applying grouping to train and test dataframes so that they show monthly sales qty instead of daily.<br> This is because we're asked to provide the forecast at a monthly level.<br> I am also clipping the target variable between 0 and 20 per the instructions."},{"metadata":{"trusted":true},"cell_type":"code","source":"train = train.groupby(by=[\"date_block_num\", \"shop_id\", \"item_id\"], as_index=False).agg(\n    {\"item_cnt_day\": \"sum\", \"item_price\": \"mean\", \"date\": \"first\"})\n\ntrain = train.rename(columns={\"item_cnt_day\": \"item_cnt_month\"})\ntest = test.rename(columns={\"item_cnt_day\": \"item_cnt_month\"})\n\ntrain[\"item_cnt_month\"] = train[\"item_cnt_month\"].clip(0, 20)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Detect Outliers\n\nChecking the item price for outliers below."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"def plot_for_outlier(ft, name):\n    plt.style.use(\"ggplot\")\n    params = {\n        \"axes.labelsize\": 20,\n        \"xtick.labelsize\": 14,\n    }\n    plt.rcParams.update(params)\n\n    fig, ax = plt.subplots(figsize=(16, 7))\n\n    plot1 = sns.boxplot(x=train[ft])\n\n    plot1.set_xlabel(name)\n\n    return fig.show()\n\n\nplot_for_outlier(\"item_price\", \"Item Price\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train[train[\"item_price\"] > 100000]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Removing one extreme outlier from the equation."},{"metadata":{"trusted":true},"cell_type":"code","source":"train = train[train[\"item_price\"] < 100000]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Visualizing the item price again to see the changes."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"plot_for_outlier('item_price','Item Price')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Looks a bit better.<br>"},{"metadata":{},"cell_type":"markdown","source":"# Univariate Vizualization\n\nIn the next two steps, I'll visualize value distributions for item_cnt_month and shop_id."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"def plot_value_dist(df, var, name):\n\n    x = df[var].value_counts().index\n    y = df[var].value_counts().values\n\n    fig = go.Figure(data=[go.Bar(x=x, y=y, text=y, textposition=\"outside\")])\n\n    fig.update_traces(\n        marker_color=\"darkblue\",\n        marker_line_color=\"white\",\n        texttemplate=\"%{text:.2s}\",\n        width=1.2,\n        textfont=dict(size=18),\n        textposition=\"inside\",\n        marker_line_width=2,\n        opacity=0.6,\n    )\n\n    fig.update_xaxes(\n        title_text=name, titlefont=dict(size=18), nticks=20, tickfont=dict(size=16)\n    )\n    fig.update_yaxes(\n        title_text=\"Value Counts\", titlefont=dict(size=18), tickfont=dict(size=16)\n    )\n\n    fig.update_layout(\n        height=500,\n        width=800,\n        template=\"ggplot2\",\n        title=go.layout.Title(\n            text=name + \" - Value Distribution\", font=dict(size=20, color=\"darkslateblue\")\n        ),\n    )\n\n    return fig.show()\n\n\nplot_value_dist(train, \"item_cnt_month\", \"Item Cnt Month\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A large majority of the monthly sales quantity are 1.<br> There are zero values. These came to surface because we clipped the sales quantity in the previous steps. They were most likely negative sales quantities before clipping."},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_value_dist(train, \"shop_id\", \"Shop ID\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There seems to be great variation in terms of the number of transactions per shop.<br> Does this high variance indicate the potential significance of the independent variable for the model ?"},{"metadata":{},"cell_type":"markdown","source":"# Bivariate Visualization"},{"metadata":{},"cell_type":"markdown","source":"Plotting unique item counts, unique shop counts, and unique item/shop combinations per date block. Blue bar represents the test set."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"def plot_counts(ft, name):\n    fig = plotly.graph_objects.Figure()\n\n    vizdata = (\n        train.append(test, ignore_index=True)\n        .groupby(by=[\"date_block_num\"], as_index=False)[ft]\n        .nunique()\n    )\n\n    colors = [\n        \"indianred\",\n    ] * vizdata.shape[0]\n    colors[-1] = \"blue\"\n\n    fig.add_trace(\n        go.Bar(\n            x=vizdata[\"date_block_num\"],\n            y=vizdata[ft],\n            width=0.9,\n            marker_color=colors,\n            name=\"Unique\" + name + \"Count\",\n        )\n    )\n\n    fig.update_xaxes(\n        title_text=\"Date Block\", titlefont=dict(size=20), tickfont=dict(size=16)\n    )\n    fig.update_yaxes(\n        title_text=\"Total Unique \" + name + \" Count\",\n        titlefont=dict(size=20),\n        nticks=8,\n        tickfont=dict(size=16),\n    )\n\n    fig.update_layout(\n        height=500,\n        width=750,\n        template=\"ggplot2\",\n        title=go.layout.Title(\n            text=\"Unique \" + name + \" Counts by Date Block (Incl. Test data)\",\n            font=dict(size=20, color=\"darkslateblue\"),\n        ),\n    )\n\n    return fig.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"plot_counts(ft = 'item_id', name='Item')","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"plot_counts(ft = 'shop_id', name='Shop')","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"def plot_combined():\n    fig = plotly.graph_objects.Figure()\n\n    vizdata = (\n        train.append(test, ignore_index=True)\n        .groupby(by=[\"date_block_num\"], as_index=False)\n        .size()\n    )\n\n    colors = [\n        \"indianred\",\n    ] * vizdata.shape[0]\n    colors[-1] = \"blue\"\n\n    fig.add_trace(\n        go.Bar(\n            x=vizdata[\"date_block_num\"],\n            y=vizdata[\"size\"],\n            width=0.9,\n            marker_color=colors,\n            name=\"Unique Item Count\",\n        )\n    )\n\n    fig.update_xaxes(\n        title_text=\"Date Block\", titlefont=dict(size=20), tickfont=dict(size=16)\n    )\n    fig.update_yaxes(\n        title_text=\"Total Unique Item Count\",\n        titlefont=dict(size=20),\n        nticks=8,\n        tickfont=dict(size=16),\n    )\n\n    fig.update_layout(\n        height=500,\n        width=750,\n        template=\"ggplot2\",\n        title=go.layout.Title(\n            text=\"Item-Shop Combination Counts by Date Block (Incl. Test data)\",\n            font=dict(size=20, color=\"darkslateblue\"),\n        ),\n    )\n\n    return fig.show()\n\n\nplot_combined()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Test set contains all item/shop combinations, unlike the train set where **only** item/shop combinations **with sales** are included.<br><br> Below I'll be extending the train dataframe to include all item/shop combinations just like the test set.<br> Applying cartesian multiplication to each unique item and shop set per date block."},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\ndef extend_train():\n    '''\n    1) Obtain set of date blocks\n    2) Establish date and price lookup tables\n    3) Get artesian product of shop/item pairs \n        and append them to train_extended_list\n    4) Lookup the existing information from train set to the extended train set:\n        Item_cnt_month nan --> fill with zero\n        Item price nan --> fill with avg. price for the date block\n        Sale date --> Lookup based on date block num\n    '''\n    dblocklist = train[\"date_block_num\"].unique()\n    dblock_month = (\n        train[[\"date_block_num\", \"date\"]]\n        .drop_duplicates()\n        .set_index(\"date_block_num\")\n        .to_dict()[\"date\"]\n    )\n    item_price = train.groupby(by=[\"item_id\", \"date_block_num\"])[\"item_price\"].mean()\n\n    \n    train_extended_list = []\n    for dblock in dblocklist:\n        shops_dblock = train.loc[train[\"date_block_num\"] == dblock, \"shop_id\"].unique()\n        items_dblock = train.loc[train[\"date_block_num\"] == dblock, \"item_id\"].unique()\n        cartesian_prod = list(product(*[items_dblock, shops_dblock, [dblock]]))\n        train_extended_list.append(np.array(cartesian_prod))\n\n    train_extended_list = np.vstack(train_extended_list)\n\n    train_extended = pd.DataFrame(\n        train_extended_list, columns=[\"item_id\", \"shop_id\", \"date_block_num\"]\n    )\n    train_extended = train_extended.merge(\n        train, on=[\"item_id\", \"shop_id\", \"date_block_num\"], how=\"left\"\n    )\n    train_extended[\"item_cnt_month\"].fillna(0, inplace=True)\n\n    train_extended[\"date\"] = train_extended[\"date_block_num\"].map(dblock_month)\n    train_extended[\"item_price\"] = train_extended.set_index(\n        [\"item_id\", \"date_block_num\"]\n    ).index.map(item_price)\n\n    train_extended[\"train\"] = 1\n    test[\"train\"] = 0\n\n    return train_extended.append(test).reset_index(drop=True)\n\n\ndf = extend_train()\n\ndel train, test","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In the following step I'll obtain supplamental item category and shop data, and map them back to our dataframe.<br> In addition, I'll be extracting month and year info from the date column."},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\ndef get_cat_shop_info(df):\n    '''\n    1) Load items, shops, and itemcategories files\n    2) Map item category id, item category name, shop name\n    '''\n    \n    itemsdf = pd.read_csv(\n        \"../input/competitive-data-science-predict-future-sales/items.csv\")\n    shopsdf = pd.read_csv(\"../input/filestranslated/shops-translated.csv\")\n    itemcatsdf = pd.read_csv(\"../input/filestranslated/categories_translated.csv\")\n\n    item_cat = dict(zip(itemsdf[\"item_id\"], itemsdf[\"item_category_id\"]))\n    item_cat_name = dict(\n        zip(itemcatsdf[\"item_category_id\"], itemcatsdf[\"item_category_name\"]))\n    shop_name = dict(zip(shopsdf[\"shop_id\"], shopsdf[\"shop_name_translated\"]))\n    \n    df[\"item_cat_id\"] = df[\"item_id\"].map(item_cat)\n    df[\"item_cat_name\"] = df[\"item_cat_id\"].map(item_cat_name)\n    df[\"shop_name\"] = df[\"shop_id\"].map(shop_name)\n\n    return df\n\n\ndef extract_month_year(df):\n    '''\n    Extract month and year from date string\n    '''\n    df[\"month\"] = df[\"date\"].apply(lambda x: x.split(\".\")[1]).astype(int)\n    df[\"year\"] = df[\"date\"].apply(lambda x: x.split(\".\")[2]).astype(int)\n\n    df.drop(columns=\"date\", inplace=True)\n\n    return df\n\n\n\ndf = get_cat_shop_info(df)\ndf = extract_month_year(df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"New features added, downcasting df again."},{"metadata":{"_kg_hide-input":false,"_kg_hide-output":false,"trusted":true},"cell_type":"code","source":"gc.collect()\ndf = downcast_df(df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Combining duplicate/relocated shop ids and names"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.loc[df[\"shop_id\"] == 11, \"shop_id\"] = 10\ndf.loc[df[\"shop_id\"] == 10, \"shop_name\"] = \"Zhukovsky st. Chkalov 39m?\"\ndf.loc[df[\"shop_id\"] == 0, \"shop_id\"] = 57\ndf.loc[df[\"shop_id\"] == 57, \"shop_name\"] = \"Yakutsk Ordzhonikidze, 56\"\ndf.loc[df[\"shop_id\"] == 1, \"shop_id\"] = 58\ndf.loc[df[\"shop_id\"] == 58, \"shop_name\"] = 'Yakutsk TC \"Central\"'\ndf.loc[df[\"shop_id\"] == 40, \"shop_id\"] = 39\ndf.loc[df[\"shop_id\"] == 39, \"shop_name\"] = \"Rostov-on-Don Megacenter Horizon\"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Item Price Imputation\n\nIn the following step, I'll be imputing the missing item prices on test df.<br> Before the imputation, though, I want to see item prices per category - since I'll be using item category to infer item prices (for test items that don't exist in train)."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"def plot_price_per_cat_shop(df, priceby, name):\n    train_prices = df.loc[df[\"item_cnt_month\"] > 0,[\"item_price\", \"shop_name\", \"item_cat_name\"]]\n\n    plt.style.use(\"ggplot\")\n    params = {\n        \"axes.labelsize\": 24,\n        \"xtick.labelsize\": 12,\n        \"ytick.labelsize\": 12,\n    }\n    plt.rcParams.update(params)\n    fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(10, 20))\n\n    ordered = (\n        df.groupby(by=priceby)[\"item_price\"].median().sort_values(ascending=False).index\n    )\n\n    plot = sns.boxplot(\n        y=train_prices[priceby],\n        x=train_prices[\"item_price\"],\n        order=ordered,\n        fliersize=2,\n    )\n\n    plot.set_xlabel(\"Item Price\")\n    plot.set_ylabel(name)\n\n    return fig\n\nplot_price_per_cat_shop(df, priceby=\"item_cat_name\", name=\"Item Category\").show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Imputing iteratively to make the most of the available information."},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_nan_count(df, imputee):\n    nancount = df[\"item_price\"].isna().sum()\n    if nancount > 0:\n        print(\n            f\"Remaining nan counts after imputation using price per {imputee}: {nancount}\"\n        )\n    elif nancount == 0:\n        print(\"No more missing values!\")\n\n\ndef impute_test_prices(df):\n    '''\n    1) Sort the train dataframe to show most recent records at the top\n    2) Obtain the most recent avg. price per item/shop\n    3) Obtain the most recent avg. price per item regardless of shop\n    4) Obtain the most recent avg. price per category/shop\n    5) Obtain the most recent avg. price per category, regardless of shop\n    6) Impute the missing item prices incrementally (same order as above)\n    '''\n    \n    sorted_train = df[(df[\"train\"] == 1) & (df[\"item_cnt_month\"] > 0)].sort_values(\n        by=[\"date_block_num\"], ascending=False\n    )\n    \n    itemNshop_price = sorted_train.groupby(by=[\"item_id\", \"shop_id\"], sort=False)[\n        \"item_price\"\n    ].first()\n\n    \n    item_price = sorted_train.groupby(\n        by=[\"item_id\", \"date_block_num\"], as_index=False, sort=False\n    )[\"item_price\"].mean()\n    item_price = item_price.groupby(by=[\"item_id\"], sort=False)[\"item_price\"].first()\n\n    \n    catNshop_price = sorted_train.groupby(\n        by=[\"item_cat_id\", \"shop_id\", \"date_block_num\"], as_index=False, sort=False\n    )[\"item_price\"].mean()\n    catNshop_price = catNshop_price.groupby(by=[\"item_cat_id\", \"shop_id\"], sort=False)[\n        \"item_price\"\n    ].first()\n    \n    cat_price = sorted_train.groupby(\n        by=[\"item_cat_id\", \"date_block_num\"], as_index=False, sort=False\n    )[\"item_price\"].mean()\n    cat_price = cat_price.groupby(by=[\"item_cat_id\"], sort=False)[\"item_price\"].first()\n    \n    df.loc[df[\"item_price\"].isna(), \"item_price\"] = (\n        df.loc[df[\"item_price\"].isna()]\n        .set_index([\"item_id\", \"shop_id\"])\n        .index.map(itemNshop_price)\n        .values\n    )\n    get_nan_count(df, imputee=\"item and shop\")\n\n    df.loc[df[\"item_price\"].isna(), \"item_price\"] = (\n        df.loc[df[\"item_price\"].isna()]\n        .set_index([\"item_id\"])\n        .index.map(item_price)\n        .values\n    )\n    get_nan_count(df, imputee=\"item\")\n\n    df.loc[df[\"item_price\"].isna(), \"item_price\"] = (\n        df.loc[df[\"item_price\"].isna()]\n        .set_index([\"item_cat_id\", \"shop_id\"])\n        .index.map(catNshop_price)\n        .values\n    )\n    get_nan_count(df, imputee=\"category and shop\")\n\n    df.loc[df[\"item_price\"].isna(), \"item_price\"] = (\n        df.loc[df[\"item_price\"].isna()]\n        .set_index([\"item_cat_id\"])\n        .index.map(cat_price)\n        .values\n    )\n    get_nan_count(df, imputee=\"category\")\n\n    df.loc[df[\"item_price\"].isna(), \"item_price\"] = df[\"item_price\"].mean()\n    get_nan_count(df, imputee=\"item (overall)\")\n\n    return df\n\ndf = impute_test_prices(df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Group the Categoricals\n\nCreating higher level groups of item categories and extracting city information from shops below.<br>"},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"def map_cat_shop(df):\n    '''\n    map each item category to a higher level/more general group\n    map each shop to the city it is located at\n    '''\n\n    category_mapping = {\n        1: \"Console Accessories\",2: \"Console Accessories\",\n        3: \"Console Accessories\",4: \"Console Accessories\",\n        5: \"Console Accessories\",6: \"Console Accessories\",\n        7: \"Console Accessories\",25: \"Game Accessories\",\n        8: \"Tickets\",\n        42: \"Books\",43: \"Books\",44: \"Books\",45: \"Books\",\n        47: \"Books\",49: \"Books\",54: \"Books\",46: \"Books\",\n        48: \"Books\",50: \"Books\",51: \"Books\",52: \"Books\",\n        53: \"Books\",\n        11: \"Game Consoles\",12: \"Game Consoles\",15: \"Game Consoles\",\n        16: \"Game Consoles\",10: \"Game Consoles\",13: \"Game Consoles\",\n        14: \"Game Consoles\",17: \"Game Consoles\",\n        18: \"Games\",24: \"Games\",19: \"Games\",20: \"Games\",\n        21: \"Games\",22: \"Games\",23: \"Games\",28: \"Games\",\n        29: \"Games\",30: \"Games\",31: \"Games\",\n        26: \"Mobile Games\",27: \"Mobile Games\",\n        61: \"Gifts\",71: \"Gifts\",64: \"Gifts\",65: \"Gifts\",\n        67: \"Gifts\",62: \"Gifts\",72: \"Gifts\",69: \"Gifts\",\n        70: \"Gifts\",63: \"Gifts\",66: \"Gifts\",68: \"Gifts\",\n        37: \"Movies\",40: \"Movies\",38: \"Movies\",41: \"Movies\",\n        39: \"Movies\",\n        60: \"Music\",57: \"Music\",58: \"Music\",56: \"Music\",\n        55: \"Music\",59: \"Music\",\n        32: \"Payment Cards\",33: \"Payment Cards\",\n        34: \"Payment Cards\",35: \"Payment Cards\",\n        36: \"Payment Cards\",\n        73: \"Programs\",74: \"Programs\",\n        75: \"Programs\",76: \"Programs\",\n        77: \"Programs\",78: \"Programs\",\n        83: \"Misc\",9: \"Misc\",0: \"Misc\",80: \"Misc\",\n        79: \"Office\",\n        81: \"Blank Media\",82: \"Blank Media\",\n    }\n    shop_mapping = {\n        0: \"Yakutsk\",1: \"Yakutsk\",\n        2: \"Maykop\",\n        3: \"Balashikha\",\n        56: \"Chekhov\",\n        55: \"Online\",12: \"Online\",\n        15: \"Kaluga\",\n        13: \"Kazan\",14: \"Kazan\",\n        54: \"Khimki\",\n        16: \"Kolomna\",\n        18: \"Krasnoyarsk\",17: \"Krasnoyarsk\",\n        19: \"Kursk\",\n        20: \"Moscow\",21: \"Moscow\",\n        30: \"Moscow\",22: \"Moscow\",\n        26: \"Moscow\",24: \"Moscow\",\n        27: \"Moscow\",28: \"Moscow\",\n        31: \"Moscow\",23: \"Moscow\",\n        25: \"Moscow\",29: \"Moscow\",\n        32: \"Moscow\",\n        33: \"Mytishchi\",\n        34: \"Nizhny Novgorod\",35: \"Nizhny Novgorod\",\n        36: \"Novosibirsk\",37: \"Novosibirsk\",\n        9: \"Other\",\n        38: \"Omsk\",\n        40: \"Rostov-on-Don\",39: \"Rostov-on-Don\",\n        41: \"Rostov-on-Don\",\n        42: \"Saint Petersburg\",43: \"Saint Petersburg\",\n        44: \"Samara\",45: \"Samara\",\n        46: \"Sergiyev Posad\",\n        47: \"Surgut\",\n        48: \"Tomsk\",\n        49: \"Tyumen\",50: \"Tyumen\",51: \"Tyumen\",\n        52: \"Ufa\",53: \"Ufa\",\n        5: \"Vologda\",\n        4: \"Volzhsky\",\n        6: \"Voronezh\",7: \"Voronezh\",8: \"Voronezh\",\n        57: \"Yakutsk\",58: \"Yakutsk\",\n        59: \"Yaroslavl\",\n        10: \"Zhukovsky\",11: \"Zhukovsky\",\n    }\n    df[\"item_cat_group\"] = df[\"item_cat_id\"].map(category_mapping)\n    df[\"city\"] = df[\"shop_id\"].astype(int).map(shop_mapping)\n    return df\n\n\ndf = map_cat_shop(df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Sales Quantity Analysis\n\nPlotting the sales numbers against date blocks, then against year and month.<br> "},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"def plot_sales_per_dblock():\n    fig = plotly.graph_objects.Figure()\n    # group the dataframe before plotting. \n    #Â This way works much faster than letting plotly do the grouping\n    vizdata = df[df[\"item_cnt_month\"]>0].groupby(by=[\"date_block_num\"], as_index=False).sum()\n\n    fig.add_trace(\n        go.Scatter(\n            x=vizdata[\"date_block_num\"],\n            y=vizdata[\"item_cnt_month\"],\n            marker_color=\"orangered\",\n            mode=\"lines+markers\",\n            name=\"Total Sales Qty\",\n        )\n    )\n\n    fig.update_xaxes(\n        title_text=\"Date Block\", titlefont=dict(size=20), tickfont=dict(size=16)\n    )\n    fig.update_yaxes(\n        title_text=\"Total Units Sold\",\n        titlefont=dict(size=20),\n        nticks=8,\n        tickfont=dict(size=16),\n    )\n\n    fig.update_layout(\n        height=500,\n        width=800,\n        template=\"ggplot2\",\n        title=go.layout.Title(\n            text=\"Sales by Date Block\", font=dict(size=20, color=\"darkslateblue\")\n        ),\n    )\n\n    return fig\n\nplot_sales_per_dblock().show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There is an overall downward trend, and seasonality that shows peak sales around December.\n\nIn the next step I'll plot monthly/yearly sales to understand the seasonality better.<br>\nWe have 12 months worth of data for 2013 and 2014. For 2015, we are missing November and December data. This will cause a misleading graph if I just plot total sales qty by month. To overcome that, I plot **average** sales qty for monthly and yearly visualization."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"def plot_sales_per_mo_yr():\n    fig = make_subplots(rows=2, cols=1, vertical_spacing=0.2)\n\n    vizdata1 = (\n        df[df[\"train\"] == 1]\n        .groupby(by=[\"month\"])\n        .mean()[\"item_cnt_month\"]\n    )\n\n    vizdata2 = (\n        df[df[\"train\"] == 1].groupby(by=[\"year\"]).mean()[\"item_cnt_month\"]\n    )\n\n    fig.add_trace(\n        go.Scatter(\n            x=vizdata1.index,\n            y=vizdata1.values,\n            marker_color=\"indianred\",\n            mode=\"lines+markers\",\n            name=\"avg sales_by_month\",\n        ),\n        row=1,\n        col=1,\n    )\n\n    fig.add_trace(\n        go.Bar(\n            x=vizdata2.index,\n            y=vizdata2.values,\n            text=vizdata2.values,\n            texttemplate=\"%{text:.2f}\",\n            width=1.2,\n            textfont=dict(size=18),\n            textposition=\"inside\",\n            marker_line_width=2,\n            marker_color=\"royalblue\",\n            name=\"avg sales_by_year\",\n        ),\n        row=2,\n        col=1,\n    )\n\n    ticksize = 14\n    xstitlesize = 16\n\n    fig.update_xaxes(\n        title_text=\"Month\",\n        titlefont=dict(size=xstitlesize),\n        tickfont=dict(size=ticksize),\n        tickangle=45,\n        row=1,\n        col=1,\n    )\n\n    fig.update_xaxes(\n        title_text=\"Year\",\n        titlefont=dict(size=xstitlesize),\n        tickfont=dict(size=ticksize),\n        nticks=6,\n        row=2,\n        col=1,\n        tickformat=\".0f\",\n    )\n\n\n    fig.update_yaxes(\n        title_text=\"Avg. Units Sold\",\n        titlefont=dict(size=xstitlesize),\n        tickfont=dict(size=ticksize),\n        row=1,\n        col=1,\n    )\n\n    fig.update_yaxes(\n        title_text=\"Avg. Units Sold\",\n        titlefont=dict(size=xstitlesize),\n        tickfont=dict(size=ticksize),\n        row=2,\n        col=1,\n    )\n\n    fig.update_layout(\n        height=800,\n        width=800,\n        showlegend=False,\n        template=\"ggplot2\",\n        title=go.layout.Title(\n            text=\"Sales by Month / Year\", font=dict(size=20, color=\"darkslateblue\")\n        ),\n    )\n\n    return fig.show()\n\n\nplot_sales_per_mo_yr()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Visualizing sales quantities per item categories and shops in the next steps"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"def plot_grouped_features(lvl1, lvl1name, lvl2, lvl2name):\n    \n    vizdata = df.loc[df[\"train\"] == 1].groupby(by=[lvl2, lvl1], as_index=False).sum()\n\n    labels = [*vizdata[lvl2].unique()]\n    labels.extend([*vizdata[lvl1].values])\n\n    parents = [\"\"] * len([*vizdata[lvl2].unique()])\n    parents.extend([*vizdata[lvl2].values])\n\n    values = [*vizdata.groupby(by=lvl2)[\"item_cnt_month\"].sum().values]\n    values.extend([*vizdata[\"item_cnt_month\"].values])\n\n    fig = go.Figure(data=go.Sunburst(labels=labels, parents=parents, values=values))\n\n    fig.update_layout(\n        width=800,\n        height=800,\n        template=\"ggplot2\",\n        title=go.layout.Title(\n            text=f\"Sales Qty by {lvl2name} - {lvl1name}\",\n            font=dict(size=20, color=\"darkslateblue\"),\n        ),\n    )\n\n    return fig.show()\n\n\nplot_grouped_features(\n    \"item_cat_name\", \"Item Category\", \"item_cat_group\", \"Item Category Group\"\n)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Games and Movies make up the majority of the sales. This is normal considering the fact that the provider of the dataset is a software/electronics company. "},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"plot_grouped_features('shop_name','Shop','city','City')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Most of the sales takes place around Moscow area. Moscow is the cosmopolitan capital and the most populated city of Russia. So the graph above make sense to me."},{"metadata":{},"cell_type":"markdown","source":"# Bin Item Prices\n\nBinning the item price with the help of KBinsDiscretizer. This is mainly for visualization/analysis purposes."},{"metadata":{"trusted":true},"cell_type":"code","source":"def bin_prices(df):\n    kbd = KBinsDiscretizer(n_bins=10, encode=\"ordinal\", strategy=\"quantile\")\n    df.loc[df[\"train\"] == 1, \"binned_item_price\"] = kbd.fit_transform(\n        df.loc[df[\"train\"] == 1, \"item_price\"].values.reshape(-1, 1)\n    )\n    df.loc[df[\"train\"] == 0, \"binned_item_price\"] = kbd.transform(\n        df.loc[df[\"train\"] == 0, \"item_price\"].values.reshape(-1, 1)\n    )\n    # Getting the lower edge of bins to be used later during plotting\n    item_lvl_bin_edges = dict(zip(range(kbd.n_bins), kbd.bin_edges_[0].astype(int)))\n\n    return df, item_lvl_bin_edges\n\n\ndf, bin_edges = bin_prices(df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's visualize monthly average sales numbers and color code the bar chart by binned item prices. The intent here is to see the effect of seasonality on consumer behavior. Does the consumer behaviour as to how much they want to spend change based on month/season ?"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"def plot_sales_per_bin(bin_edges):\n\n    vizdata = (\n        df.loc[df[\"train\"] == 1]\n        .groupby(by=[\"binned_item_price\", \"month\"], as_index=False)\n        .mean()\n    )\n    vizdata[\"binned_item_price\"] = vizdata[\"binned_item_price\"].map(bin_edges)\n\n    fig = go.Figure(\n        px.bar(\n            data_frame=vizdata,\n            y=\"item_cnt_month\",\n            x=\"month\",\n            labels={\"binned_item_price\": \"Binned Item Price\"},\n            opacity=0.6,\n            color=\"binned_item_price\",\n            color_continuous_scale=px.colors.diverging.Portland,\n            orientation=\"v\",\n        )\n    )\n\n    fig.update_xaxes(\n        title_text=\"Month of Sale\", titlefont=dict(size=20), tickfont=dict(size=16)\n    )\n    fig.update_yaxes(\n        title_text=\"Avg Units Sold\",\n        titlefont=dict(size=20),\n        tickfont=dict(size=16),\n        nticks=10,\n    )\n    fig.update_layout(\n        title=go.layout.Title(\n            text=\"Average Sales per Month and Binned Item Price\",\n            font=dict(size=20, color=\"darkslateblue\"),\n        )\n    )\n\n    annot = []\n\n    for x in set(vizdata[\"month\"]):\n        record = vizdata.loc[vizdata[\"month\"] == x]\n        yd = record[\"item_cnt_month\"].values\n\n        space = 0\n        for n in range(10):\n            annot.append(\n                dict(\n                    xref=\"x\",\n                    yref=\"y\",\n                    y=space + yd[n] / 2,\n                    x=x,\n                    text=\"{0:.0%}\".format(yd[n] / yd.sum()),\n                    font=dict(size=16, color=\"black\", family=\"droid-sans\"),\n                    showarrow=False,\n                )\n            )\n            space += yd[n]\n\n    fig.update_layout(\n        height=800,\n        width=800,\n        template=\"ggplot2\",\n        annotations=annot,\n    )\n\n    return fig\n\n\nplot_sales_per_bin(bin_edges).show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It's quite interesting to see that the demand for the most expensive items increase considerably during winter/holiday season."},{"metadata":{},"cell_type":"markdown","source":"Below I establish a new feature that indicates whether a sales record is the first sales for the item/shop combination. "},{"metadata":{"trusted":true},"cell_type":"code","source":"def determine_first_sale(df):\n    # Determine whether a record indicates first sale for the item/shop\n    item_firstsale = (\n        df[df[\"item_cnt_month\"] > 0].groupby(by=[\"item_id\", \"shop_id\"])[\"date_block_num\"].min())\n    df[\"item_firstsale_month\"] = df.set_index([\"item_id\", \"shop_id\"]).index.map(item_firstsale)\n    \n    df[\"item_first_sale\"] = 0\n    df.loc[df[\"item_firstsale_month\"] == df[\"date_block_num\"], \"item_first_sale\"] = 1\n    df.drop(columns=[\"item_firstsale_month\"], inplace=True)\n    \n    return df\n\n\ndf = determine_first_sale(df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Lag Features\n\n\nCreating lag features to capture previous months sales for the item/shop. <br>The intent here is to reframe/convert the problem from time-series forecasting to supervised learning."},{"metadata":{"trusted":true},"cell_type":"code","source":"def set_lag_features(df):\n    '''\n    1) Sort DF ascending\n    2) Create 1,2,3,4,5,6,12 lag features for item/shop sales (same as item_cnt_month)\n        by shifting/pushing item_cnt_month down\n    3) Create 1,2,3 lag features for cat_shop_sales\n    4) Fill nan values with 0\n    5) Delete newly created features that are non-lag to prevent leak\n    '''\n    \n    \n    df[df[\"train\"] == 1] = df[df[\"train\"] == 1].sort_values(by=[\"date_block_num\"])\n    \n    df[\"item_shop_sales_t_1\"] = df.groupby(by=[\"item_id\", \"shop_id\"], sort=False)[\"item_cnt_month\"].shift(1).values\n    df[\"item_shop_sales_t_2\"] = df.groupby(by=[\"item_id\", \"shop_id\"], sort=False)[\"item_cnt_month\"].shift(2).values\n    df[\"item_shop_sales_t_3\"] = df.groupby(by=[\"item_id\", \"shop_id\"], sort=False)[\"item_cnt_month\"].shift(3).values\n    df[\"item_shop_sales_t_4\"] = df.groupby(by=[\"item_id\", \"shop_id\"], sort=False)[\"item_cnt_month\"].shift(4).values\n    df[\"item_shop_sales_t_5\"] = df.groupby(by=[\"item_id\", \"shop_id\"], sort=False)[\"item_cnt_month\"].shift(5).values\n    df[\"item_shop_sales_t_6\"] = df.groupby(by=[\"item_id\", \"shop_id\"], sort=False)[\"item_cnt_month\"].shift(6).values\n    df[\"item_shop_sales_t_12\"] = df.groupby(by=[\"item_id\", \"shop_id\"], sort=False)[\"item_cnt_month\"].shift(12).values\n    \n    category_shop_sales = df.groupby(by=['item_cat_id','shop_id','date_block_num'])[\"item_cnt_month\"].sum()\n    \n    df['cat_shop_sales'] = df.set_index(['item_cat_id','shop_id','date_block_num']).index.map(category_shop_sales)\n    \n    df[\"cat_shop_sales_t_1\"] = df.groupby(by=[\"item_id\", \"shop_id\"], sort=False)[\"cat_shop_sales\"].shift(1).values\n    df[\"cat_shop_sales_t_2\"] = df.groupby(by=[\"item_id\", \"shop_id\"], sort=False)[\"cat_shop_sales\"].shift(2).values\n    df[\"cat_shop_sales_t_3\"] = df.groupby(by=[\"item_id\", \"shop_id\"], sort=False)[\"cat_shop_sales\"].shift(3).values\n    df[\"cat_shop_sales_t_6\"] = df.groupby(by=[\"item_id\", \"shop_id\"], sort=False)[\"cat_shop_sales\"].shift(6).values\n    df[\"cat_shop_sales_t_12\"] = df.groupby(by=[\"item_id\", \"shop_id\"], sort=False)[\"cat_shop_sales\"].shift(12).values\n    \n    df = df.drop(columns=['cat_shop_sales'])\n    \n    return df.fillna(0)\n\ndf = set_lag_features(df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Identify Similar Items\n\nCalculating cosine similarities of item descriptions.<br> The idea here is that the items with similar vector directions have similar descriptions, therefore they are similar items (and may share similar sales trends). This may be especially helpful for the model to forecast sales for items with no history."},{"metadata":{"trusted":true},"cell_type":"code","source":"def set_similar_items(df):\n    \n    '''\n    1) Obtain a corpus of item descriptions\n    2) Vectorize each item description and calculate cosine similarity \n        between each item/item combination, then store that info in a matrix.\n    3) Pick the top 3 most similar items for each item\n    4) Append the similar item information back to our dataframe\n    \n    '''\n    \n    \n    itemstranslated = pd.read_csv(\"../input/filestranslated/items-translated.csv\")\n\n    itemdf = pd.DataFrame(\n        columns=[\"item_id\", \"item_description\"], data=itemstranslated.values\n    )\n\n    # \n    corpus = list(itemdf[\"item_description\"].values)\n\n\n    vectorizer = TfidfVectorizer()\n    X = vectorizer.fit_transform(corpus)\n    cosine_similarities = pd.DataFrame(\n        cosine_similarity(X), index=itemdf[\"item_id\"], columns=itemdf[\"item_id\"]\n    )\n\n\n    similar_items = []\n    for i in cosine_similarities.columns:\n        similar = np.append(cosine_similarities[i].nlargest(4)[1:].index.values, i)\n        similar_items.append(similar)\n    similar_items = pd.DataFrame(\n        np.vstack(similar_items),\n        columns=[\"similar_item1\", \"similar_item2\", \"similar_item3\", \"item_id\"],\n    )\n\n    sim1 = dict(zip(similar_items[\"item_id\"], similar_items[\"similar_item1\"]))\n    sim2 = dict(zip(similar_items[\"item_id\"], similar_items[\"similar_item2\"]))\n    sim3 = dict(zip(similar_items[\"item_id\"], similar_items[\"similar_item3\"]))\n    df[\"similar_item1\"] = df[\"item_id\"].map(sim1)\n    df[\"similar_item2\"] = df[\"item_id\"].map(sim2)\n    df[\"similar_item3\"] = df[\"item_id\"].map(sim3)\n\n    return df\n\n\ndf = set_similar_items(df)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = downcast_df(df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Ordinal Encode the Categoricals"},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\nOrdinal encoding features such as month, year as well as item_id, item_cat_id\nbecause LGBM prefers contiguous range of integers started from zero\n'''\n\noe = OrdinalEncoder()\ndf[['item_id','similar_item1','item_cat_group',\n    'similar_item2','similar_item3','city',\n    'item_cat_id','shop_id','month','year']] = oe.fit_transform(df[['item_id','similar_item1','item_cat_group',\n                                                                    'similar_item2','similar_item3','city',\n                                                                    'item_cat_id','shop_id','month','year']])\n\n\nX_cols_to_drop = [\n    \"train\",\n    \"item_cnt_month\",\n    \"item_cat_name\",\n    \"shop_name\",\n    'binned_item_price'\n]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"To mimic how the test set is created, I am setting aside the records with the date block num 33 for validation.<br> This is different than the usual way of establishing validation sets e.g. cross_val, train_test_split etc."},{"metadata":{"trusted":true},"cell_type":"code","source":"def df_to_xy(df):\n\n    X_train = df[df[\"date_block_num\"] < 33].drop(columns=X_cols_to_drop)\n    X_val = df[df[\"date_block_num\"] == 33].drop(columns=X_cols_to_drop)\n\n    y_train = df.loc[df[\"date_block_num\"] < 33, \"item_cnt_month\"]\n    y_val = df.loc[df[\"date_block_num\"] == 33, \"item_cnt_month\"]\n\n    X_test = df[df[\"date_block_num\"] == 34].drop(columns=X_cols_to_drop)\n\n    return X_test, X_train, X_val, y_train, y_val\n\n\nX_test, X_train, X_val, y_train, y_val = df_to_xy(df)\n\ndel df\ncapture = gc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Modeling, Feature Importances, and Final Submission"},{"metadata":{},"cell_type":"markdown","source":"I am using XGB regressor for submission. The hyperparameter values are the result of a number of trial-and-errors.<br> I kept the tree relatively shallow with the intent of preventing overfitting.  "},{"metadata":{"trusted":true},"cell_type":"code","source":"model_params = {\n    \"n_estimators\": 200,\n    \"num_leaves\": 40,\n    \"learning_rate\": 0.15,\n    \"min_child_weight\": 0.35,\n    \"colsample_bytree\": 0.27,\n    \"reg_alpha\": 0.83,\n    \"reg_lambda\": 0.56,\n}\n\n\nmodel = LGBMRegressor(**model_params)\nmodel.fit(\n    X_train,\n    y_train,\n    eval_metric=\"rmse\",\n    eval_set=(X_val, y_val),\n    early_stopping_rounds=10,\n    categorical_feature=[\n        \"item_id\",\"shop_id\",'city',\n        \"date_block_num\",\"item_cat_id\",'item_cat_group',\n        \"similar_item1\",\"similar_item2\",\"similar_item3\"\n    ]\n)\n\n# Clipping the target values once more before submission\npred = model.predict(X_test).clip(0, 20)\n\n# Retrieving the submission IDs\nsub_ID = pd.read_csv(\"../input/competitive-data-science-predict-future-sales/test.csv\")[\n    \"ID\"\n].values\n\nsubmissionfile = pd.DataFrame(list(zip(sub_ID, pred)), columns=[\"ID\", \"item_cnt_month\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Visualizing feature importances."},{"metadata":{"trusted":true},"cell_type":"code","source":"params = {\n    \"axes.labelsize\": 20,\n    \"ytick.labelsize\": 16,\n    \"axes.titlesize\": 22,\n    \"axes.edgecolor\": \"#338F4B\",\n}\nplt.rcParams.update(params)\n\nfig, ax = plt.subplots(1, 1, figsize=(10, 12))\n\nplot_importance(model, height=0.8, ax=ax)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Submitting the results..."},{"metadata":{"trusted":true},"cell_type":"code","source":"submissionfile.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}