{"cells":[{"metadata":{},"cell_type":"markdown","source":"Please be aware that this notebook is a work in progress, features and model parameters are not yet perfectly tuned."},{"metadata":{},"cell_type":"markdown","source":"### Import libraries"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"import sys\n!cp ../input/rapids/rapids.0.13.0 /opt/conda/envs/rapids.tar.gz\n!cd /opt/conda/envs/ && tar -xzvf rapids.tar.gz > /dev/null\nsys.path = [\"/opt/conda/envs/rapids/lib/python3.6/site-packages\"] + sys.path\nsys.path = [\"/opt/conda/envs/rapids/lib/python3.6\"] + sys.path\nsys.path = [\"/opt/conda/envs/rapids/lib\"] + sys.path \n!cp /opt/conda/envs/rapids/lib/libxgboost.so /opt/conda/lib/\n\nprint('rapids ai installed')","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\nimport matplotlib.dates as mdates\nimport matplotlib.ticker as tic\nimport matplotlib\nimport datetime\n\nimport re\nfrom itertools import product\nfrom catboost import Pool, CatBoostRegressor,CatBoostClassifier\n\nfrom sklearn.metrics               import mean_squared_error\nfrom sklearn.preprocessing         import StandardScaler\nfrom sklearn.ensemble              import RandomForestRegressor\nfrom sklearn.linear_model          import LinearRegression\n\nimport cudf, cuml, cupy\nfrom cuml.neighbors import KNeighborsRegressor \n\npd.plotting.register_matplotlib_converters()\npd.options.display.max_rows = 100\npd.options.display.max_columns = 100\nmax_width = 12\nimport warnings\nwarnings.filterwarnings('ignore')\n\nprint('all libraries imported')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Overview"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"path = '../input/competitive-data-science-predict-future-sales/'\n\nf_categories      = pd.read_csv(path + 'item_categories.csv')\nf_items           = pd.read_csv(path + 'items.csv')\nf_transactions    = pd.read_csv(path + 'sales_train.csv')\nf_sample          = pd.read_csv(path + 'sample_submission.csv')\nf_shops           = pd.read_csv(path + 'shops.csv')\nf_test            = pd.read_csv(path + 'test.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Available data:"},{"metadata":{},"cell_type":"markdown","source":"We have almost 3 years of transactions from several 1C shops, aggregated daily:"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"f_transactions.date    = pd.to_datetime(f_transactions.date, format = '%d.%m.%Y')\n\nf_sales = f_transactions\\\n.join(f_items.set_index('item_id'), on='item_id')\\\n.join(f_categories.set_index('item_category_id'), on='item_category_id')\\\n.join(f_shops.set_index('shop_id'), on='shop_id')\n\nf_sales.head(2).T","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### What is in test sample:"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"f_test\\\n.join(f_sample.set_index('ID'), on='ID').head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### How to form evaluation sample:"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"pd.pivot_table(f_sales.loc[f_sales.date_block_num==33], index='item_id', columns='shop_id', values='item_cnt_day', aggfunc='sum')\\\n.clip(0,20).fillna(0)\\\n.unstack().to_frame('item_cnt_month').reset_index().head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### First look - item sales per shop per day in relative numbers as a heatmap"},{"metadata":{},"cell_type":"markdown","source":"Some conclusions that we may make from the plot below:\n* shops open and close, may be closed for a day or a month\n* some shops may have wrong id's (id 0,1 vs 57,58, and 10 vs 11)\n* shops differ in type, some are seasonal sales (id 9 and 20)\n* not all shops are in test, but some are stable and may give extra info (for example id 29,30,54)\n* when looking at the demand over time, closed shops may distort the picture"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"#data\npiv = np.log(pd.pivot_table(f_sales,\n                            index='date',\n                            columns=['shop_id','shop_name'],\n                            values='item_cnt_day',\n                            aggfunc='sum').fillna(0).clip(0,)+1).T\n\n#plotting\n\nfig, ax = plt.subplots(figsize=(max_width,10))\n\n_y = piv.index.get_level_values(0)[::-1]\n_x = mdates.date2num(piv.columns)\n    \nax.imshow(piv, aspect='auto', extent = [ _x[0],  _x[-1]+1,  _y[0]+1, _y[-1]], interpolation='none')\n\nax.xaxis_date()\nax.xaxis.set_major_locator(mdates.YearLocator())\nax.xaxis.set_major_formatter(mdates.DateFormatter('%Y - %m'))\nax.xaxis.set_minor_locator(mdates.MonthLocator())\nax.xaxis.set_minor_formatter(mdates.DateFormatter('%m'))\n\nax.yaxis.set_ticks(_y)\nax.set_yticklabels(piv.index.get_level_values(0)[::-1].astype('str') + ' ' + piv.index.get_level_values(1)[::-1],\n                  fontsize=8, va='top')\n\nfig.autofmt_xdate(which='both', rotation=90, ha='left')\n\n\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Preprocessing"},{"metadata":{},"cell_type":"markdown","source":"Basic preprocessing for future EDA needs, with comments in code:"},{"metadata":{"_kg_hide-input":true,"jupyter":{"source_hidden":true},"trusted":true},"cell_type":"code","source":"test = f_test.copy()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"jupyter":{"source_hidden":true},"trusted":true},"cell_type":"code","source":"#prepare transactions\n\nf_transactions.date    = pd.to_datetime(f_transactions.date, format = '%d.%m.%Y')\ntransactions = f_transactions.copy()\n\n#---------------------------------------------------------------------------\n#some handpicked outliers:\ntransactions = transactions.loc[~transactions.index.isin([1573253,1573252,1618930,1571406,2909818,1829149,1088966])]\n\n#---------------------------------------------------------------------------\n# filtering for clipping to speed up process and not to distort items with small ammount of sales\n#a = transactions.groupby('item_id').agg(sales_sum=('item_cnt_day','sum'),\n#                                        sales_mean=('item_cnt_day','mean'),\n#                                        sales_max=('item_cnt_day','max'),)\\\n#.assign(max_to_mean=(lambda x: x.sales_max/x.sales_mean))\\\n#.loc[lambda x: (x.sales_sum>20)&(x.sales_max>10)&(x.max_to_mean>2)]\\\n#.index\n\n#for i in a:\n#    _ = transactions.loc[transactions.item_id==i,'item_cnt_day']\n#    transactions.loc[transactions.item_id==i,'item_cnt_day'] = _.clip(upper=_[_>0].quantile(0.98))\n    \n#---------------------------------------------------------------------------   \n#clip more and  fix shop id's\n\n#transactions.item_cnt_day = transactions.item_cnt_day.clip(upper=100)\n#transactions.shop_id = transactions.shop_id.replace([0,1,11],[57,58,10])\n\n#---------------------------------------------------------------------------   \n\ntransactions['year']            = transactions.date.dt.year\ntransactions['month']           = transactions.date.dt.month\ntransactions['day']             = transactions.date.dt.day\ntransactions['day_of_year']     = transactions.date.dt.dayofyear\ntransactions['week_day']        = transactions.date.dt.weekday\ntransactions['activity_cnt']    = transactions.item_cnt_day.abs()\ntransactions['activity_rev']    = transactions.activity_cnt*transactions.item_price\ntransactions['revenue']         = transactions.item_cnt_day*transactions.item_price","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"jupyter":{"source_hidden":true},"trusted":true},"cell_type":"code","source":"#prepare items\n\nitems = f_items.copy()\n\n#---------------------------------------------------------------------------   \n#separating plastic bag item from accessories to a separate category\nitems.at[20949,'item_category_id'] = 84\n\n#only item from PC category, move to ps accessories\nitems.at[5441,'item_category_id'] = 3\n\n#---------------------------------------------------------------------------   \n\nitems = items\\\n.join(transactions.groupby('item_id').agg(\n    sale_start=('date','min'),\n    sale_end=('date','max'),\n    price_mean=('item_price','mean'),\n    #sales_cnt=('item_cnt_day','sum'),\n    item_total_revenue=('revenue','sum'),\n))","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"jupyter":{"source_hidden":true},"trusted":true},"cell_type":"code","source":"#prepare categories\n\ncategories = f_categories.copy()\n\n#---------------------------------------------------------------------------   \n#separating plastic bag item from accessories to a separate category\n\ncategories = categories.append({'item_category_name': 'Фирменный пакет','item_category_id': 84,}, ignore_index=True)\n\n# global category level\n\ncategories['cat_global'] = np.select(\n    [categories.item_category_id.isin(range(0,8)),\n    categories.item_category_id.isin([8,80]),\n    categories.item_category_id==9,\n    categories.item_category_id.isin(range(10,18)),\n    categories.item_category_id.isin(range(18,32)),\n    categories.item_category_id.isin([32,33,34,35,36,37,79]),\n    categories.item_category_id.isin(range(37,42)),\n    categories.item_category_id.isin(range(42,55)),\n    categories.item_category_id.isin(range(55,61)),\n    categories.item_category_id.isin(range(61,73)),\n    categories.item_category_id.isin(range(73,79)),\n    categories.item_category_id.isin([81,82]),\n    categories.item_category_id==83,\n    categories.item_category_id==84],    \n    ['accessories','tickets','delivery','consoles','games',\n    'payment_cards','movies','books','music','gifts','programs',\n     'discs','batteries','plastic_bags'])\n\n# subcategories by platform\n\ncategories['cat_platform'] = np.select(\n    [categories.item_category_name.str.contains('PS2'),\n    categories.item_category_name.str.contains('PS3'),\n    categories.item_category_name.str.contains('PS4'),\n    categories.item_category_name.str.contains('PSP'),\n    categories.item_category_name.str.contains('PSVita'),\n    categories.item_category_name.str.contains('XBOX 360'),\n    categories.item_category_name.str.contains('XBOX ONE'),\n    categories.item_category_name.str.contains('PC'),\n    categories.item_category_name.str.contains('MAC'),\n    categories.item_category_name.str.contains('Android')],\n    ['PS2','PS3','PS4','PSP','PSVita','XBOX_360','XBOX_ONE','PC','MAC','Android'],\n    default='other')\n\n# digital subcategory\n\ncategories['cat_digital'] = categories.item_category_name\\\n.str.contains('Цифра')*1\n\n# marking categories that are in test set\n\ncategories['cat_in_test'] = categories.item_category_id\\\n.isin(test.set_index('item_id').join(items.item_category_id).item_category_id)*1\n\n# that are too scattered or not standard for training\n\ncategories['cat_to_drop'] = categories.item_category_id\\\n.isin([ 1,8,10,13,17,18,39,46,48,50,51,52,53,66,68,80,81,82])*1","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"jupyter":{"source_hidden":true},"trusted":true},"cell_type":"code","source":"#prepare shops\n\nshops = f_shops.copy()\n\n#---------------------------------------------------------------------------   \n#droppin dublicates, marking shops in test, marking shops that are unsuitable to train\nshops = shops.loc[~shops.shop_id.isin([0,1,11])]\nshops['shop_in_test'] = shops.shop_id.isin(test.shop_id)*1\nshops['shop_to_drop'] = shops.shop_id.isin([8,9,20,23,32,33,40])*1\n\n#---------------------------------------------------------------------------   \n# querying city name from shop name\n\nsplit = shops['shop_name'].\\\nstr.replace('Н.Новгород','ННовгород').\\\nstr.replace('Сергиев Посад','СергиевПосад').\\\nstr.replace('\\W+',' ').\\\nstr.strip().\\\nstr.split(' ', 1).\\\nstr\n    \nshops['shop_city'] = split[0]\nshops['shop_name_part'] = split[1]\n\n#---------------------------------------------------------------------------   \n# calculating relative shop average sales, for further analysis\n\nsold_items_cnt_per_work_day = transactions.groupby(['shop_id']).item_cnt_day.sum()/transactions.groupby(['shop_id']).date.nunique()\nshops = shops.join((sold_items_cnt_per_work_day/sold_items_cnt_per_work_day.mean()).to_frame('shop_size'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After preprocessing is done I combine everything into one dataframe."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"sales = transactions\\\n.join(items.set_index('item_id'), on='item_id')\\\n.join(categories.set_index('item_category_id'), on='item_category_id')\\\n.join(shops.set_index('shop_id'), on='shop_id')\\\n.loc[lambda x: (x.cat_to_drop==0)&(x.shop_to_drop==0)]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Seasonality"},{"metadata":{},"cell_type":"markdown","source":"Since the ammount of opened and closed shops is different all the time, to separate seasonal effects I adjust item sales by the ammount and size of shops that are opened at any given point:"},{"metadata":{"_kg_hide-input":true,"jupyter":{"source_hidden":true},"trusted":true},"cell_type":"code","source":"_ = sales.groupby('date').shop_id.nunique()\n\nsales = sales\\\n.set_index('date')\\\n.join(_.to_frame('shops_opened'))\\\n.reset_index()\\\n.assign(shop_compensation = lambda x: (x.shop_size*x.shops_opened)/(_.mean()))\\\n.assign(item_cnt_day_sw = lambda x: x.item_cnt_day/x.shop_compensation)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It is also oblious that different catgories and shops will have different seasonalities, lets look at it first: "},{"metadata":{"_kg_hide-input":true,"jupyter":{"source_hidden":true},"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(sales.cat_global.nunique(),sharex=True,figsize=(max_width,sales.cat_global.nunique()*2))\nstart = datetime.date(2013,1,1)\nend = datetime.date(2015,10,31)\n\nfor i,j in enumerate(sales.cat_global.unique()):\n    \n    plt.subplots_adjust(hspace=0)\n    \n    view = sales.loc[(sales.cat_global==j)].groupby('date').item_cnt_day.sum().clip(0,)\n\n    ax[i].plot(view, linewidth=1.1)  \n    ax[i].set_title(' '+j, loc='center', fontsize=10, y=0.8, fontweight='bold')\n    ax[i].set_xlim([start,end])\n    \n    ax[i].set_ylim(0,)\n    ax[i].yaxis.set_major_locator(tic.FixedLocator([0,view.max()/2]))\n    \n    ax[i].xaxis.set_major_locator(mdates.YearLocator())\n    ax[i].xaxis.set_major_formatter(mdates.DateFormatter('%Y   -   '))\n    \n    ax[i].xaxis.set_minor_locator(mdates.MonthLocator())\n    ax[i].xaxis.set_minor_formatter(mdates.DateFormatter('%m'))\n\nfig.autofmt_xdate(which='both',rotation=90)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I see clear weekly trends, but monthly seasonality is also possible. From the charts below we can make an important note - for future cross validation:\nsince weekly seasonality is stronger it is better to use 30 day training splits with the same ammount of weekdays, instead of using default monthly split."},{"metadata":{"_kg_hide-input":true,"jupyter":{"source_hidden":true},"trusted":true},"cell_type":"code","source":"m = sales.loc[(~sales.month.isin([12,1,2]))&(sales.day!=31)]\\\n.groupby('day').item_cnt_day_sw.sum()\nm = m/m.mean()\n\n_ = sales.loc[~sales.month.isin([12,1])]\\\n.groupby(['cat_global','week_day'])\\\n.item_cnt_day_sw.sum().unstack().T\n\nweekly_cat = _.div(_.mean())\n\nsales = sales\\\n.set_index(['cat_global','week_day'])\\\n.join(weekly_cat.loc[:,weekly_cat.columns!='delivery'].unstack().to_frame('week_day_effect_cat'), how='left')\\\n.fillna(value={'week_day_effect_cat':1})\\\n.reset_index()\\\n.assign(item_cnt_day_sw=lambda x: x.item_cnt_day_sw/x.week_day_effect_cat)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"jupyter":{"source_hidden":true},"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(1,3, sharey=True, figsize=(max_width-1,3))\n\nax[0].plot(m)\nax[1].plot(_.T.mean().div(_.T.mean().mean()))\nax[2].plot(_.div(_.mean()))\n\nax[0].axhline(1, linestyle='-', color='gray', linewidth=0.5)\nax[1].axhline(1, linestyle='-', color='gray', linewidth=0.5)\nax[2].axhline(1, linestyle='-', color='gray', linewidth=0.5)\n\nax[0].set_title('monthly')\nax[1].set_title('weekly global')\nax[2].set_title('weekly per category')\n\nax[1].tick_params(labelleft=True)\nax[2].tick_params(labelleft=True)\n\nax[0].set_ylim(0,2)\n\nax[0].yaxis.set_major_locator(tic.LinearLocator(numticks=5))\n\nax[0].set_xlim(1,30)\nax[1].set_xlim(0,6)\nax[2].set_xlim(0,6)\n\nax[2].legend(_.columns,\n             loc='center left', \n             bbox_to_anchor=(1.05, 0.5), \n             frameon=False)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"One more thing that we may notice from spikes is that sales count depends on days passed after release date. Separating this effect will make seasonality less noisy.  "},{"metadata":{"_kg_hide-input":true,"jupyter":{"source_hidden":true},"trusted":true},"cell_type":"code","source":"sales['days_from_sale_start'] = (sales.date-sales.sale_start).dt.days\n\n_ = sales.loc[(sales.sale_start>=datetime.datetime(2013, 6, 1))\n             &(~sales.cat_global.isin(['delivery']))\n             &(sales.days_from_sale_start<=60)] \n\n_ = pd.pivot_table(_,\n               index=['cat_global','item_id'],\n               columns='days_from_sale_start',\n               values='item_cnt_day_sw',\n               aggfunc='sum').fillna(0).reset_index()\\\n.drop('item_id',axis=1) \n\n\nnew_item_effect_cat = (_.groupby('cat_global').mean()).div((_.groupby('cat_global').mean()).mean(axis=1),axis=0).T\nnew_item_effect_cat = new_item_effect_cat.div(new_item_effect_cat.tail(20).mean())\n\nnew_item_effect_glob  = (_.mean()/(_.mean().mean())).T\nnew_item_effect_glob = new_item_effect_glob.div(new_item_effect_glob.tail(20).mean())\n\nsales = sales\\\n.set_index(['cat_global','days_from_sale_start'])\\\n.join(new_item_effect_cat.unstack().to_frame('new_item_effect_cat'), how='left')\\\n.fillna(value={'new_item_effect_cat':1})\\\n.reset_index()\\\n.assign(item_cnt_day_sw=lambda x: x.item_cnt_day_sw/x.new_item_effect_cat)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"jupyter":{"source_hidden":true},"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(1,2, sharey=True, sharex=True, figsize=(max_width-1,3))\n\nax[0].plot(new_item_effect_glob)\nax[1].plot(new_item_effect_cat)\n\nax[1].tick_params(labelleft=True)\n\n\nax[0].set_title('global')\nax[1].set_title('per category')\n\n\nax[0].set_ylim(0,20)\nax[0].set_xlim(0,60)\n\nax[1].legend(new_item_effect_cat.columns,\n             loc='center left', \n             bbox_to_anchor=(1.05, 0.5), \n             frameon=False)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Overall sales trend looks to be declining, so I calculate relative year size to adjust sales count. "},{"metadata":{"_kg_hide-input":true,"jupyter":{"source_hidden":true},"trusted":true},"cell_type":"code","source":"year_size = sales.loc[~sales.month.isin([1,2,11,12])].groupby('year').item_cnt_day_sw.sum()\\\n.div(sales.loc[~sales.month.isin([1,2,11,12]),'item_cnt_day_sw'].sum()).multiply(3).to_frame('year_size')\n\nsales = sales.set_index('year')\\\n.join(year_size).reset_index()\\\n.assign(item_cnt_day_sw=lambda x: x.item_cnt_day_sw/x.year_size)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"ax = year_size.plot(kind='bar', color=\"coral\", fontsize=10, legend=False, figsize=(4,3));\nax.set_alpha(0.8)\nax.set_ylim(0,1.5)\nax.set_xlabel(None)\nax.set_title('relative year size by sales')\n\n\nax.axes.get_yaxis().set_visible(False)\n\n\nfor i in ax.patches:\n    ax.text(i.get_x()+.11, i.get_height()+.05, \\\n            str(round((i.get_height())*100, 2))+'%')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After adjusting sales to all the effects above we finally may calculate annual seasonality, global and for each category.\nSince we had only 3 years, weekly fluctuations are slightly visible.\nNational holidays are clearly visible aswell."},{"metadata":{"_kg_hide-input":true,"jupyter":{"source_hidden":true},"trusted":true},"cell_type":"code","source":"_ = sales.loc[(sales.cat_global!='delivery')&(sales.date>=datetime.datetime(2013, 3, 1))]\\\n.groupby(['cat_global','year','day_of_year']).item_cnt_day_sw.sum()\\\n.groupby(['cat_global','day_of_year']).mean()\n\nsales = sales\\\n.set_index(['cat_global','day_of_year'])\\\n.join(_.unstack().T.div(_.unstack().T.mean()).unstack().to_frame('seasonality_annual_cat'), how='left')\\\n.fillna(value={'seasonality_annual_cat':1})\\\n.reset_index()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"jupyter":{"source_hidden":true},"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(max_width,4))\n\n\n_ = sales.loc[sales.cat_global!='delivery']\\\n.groupby(['year','day_of_year']).item_cnt_day_sw.sum()\\\n.groupby(['day_of_year']).mean()\n\n_=_/_.mean()\n\nax.plot(_, linewidth=2)\n\nax.set_xticks([1,32,60,91,121,152,182,213,244,274,305,335])\nax.set_xticklabels(['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec'],rotation=90)\n\nax2 = ax.twiny()\nax2.set_xlim(1,365)\nax2.set_xticks([7,54,67,121,129,163,308,364.5])\nax2.set_xticklabels(['Christmas','Fatherland Defender',\n                     'Womens Day','Labour Day',\n                     'Victory Day','Russia Day',\n                     'Unity Day','New Year'],rotation=90,color='red')\n\nax2.xaxis.grid(True, which='major')\n\nax.set_xlim(1,365)\nax.set_ylim(0,5)\n\nax.axes.get_yaxis().set_visible(False)\nax.set_title('annual seasonality in relative values')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now it is possible to calculate deaseasonalized item sales count:"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"sales['item_cnt_day_deseasoned'] = sales.item_cnt_day\\\n                                .div(sales.week_day_effect_cat)\\\n                                .div(sales.seasonality_annual_cat)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It is also interesting how price affect sales. For example temporary actions are launched for games and usually are a \"plain discount\" or \"buy one get other for free\" type.\nList of all actions may be found here: https://www.1c-interes.ru/special_actions "},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"fig, (ax1,ax2) = plt.subplots(2,figsize=(max_width,5))\n\ncond1 = (sales.date>=datetime.datetime(2013, 11, 1))&(sales.cat_global=='games')\ncond2 = (sales.date>=datetime.datetime(2013, 11, 1))&(sales.cat_global!='games')\n\nsales.loc[cond1].assign(price_to_mean=sales.item_price/sales.price_mean)\\\n.groupby('date').price_to_mean.mean()\\\n.div(sales.loc[cond2].assign(price_to_mean=sales.item_price/sales.price_mean)\\\n.groupby('date').price_to_mean.mean())\\\n.plot(ax=ax1,color='coral')\n\nplt.subplots_adjust(hspace=0)\n\nsales.loc[cond1]\\\n.groupby('date').item_cnt_day_sw.sum()\\\n.plot(ax=ax2)   \n\nax1.set_ylim(0.64,1.16)\nax2.set_ylim(0,5500)\n\nax1.set_ylabel('price to price mean')\nax2.set_ylabel('daily sale count')\n\n\nfig.patches.extend([plt.Rectangle((0.82,0.125),0.029,0.757,\n                                  fill=True, color='gray', alpha=0.2,\n                                  transform=fig.transFigure, figure=fig)])\n\nfig.patches.extend([plt.Rectangle((0.626,0.125),0.029,0.757,\n                                  fill=True, color='gray', alpha=0.2,\n                                  transform=fig.transFigure, figure=fig)])\n\nfig.patches.extend([plt.Rectangle((0.217,0.125),0.029,0.757,\n                                  fill=True, color='gray', alpha=0.2,\n                                  transform=fig.transFigure, figure=fig)])\n\n\nax1.set_title('discount effect in gaming categories')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Visualizing the data that will be used for training:"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"######\nsales = sales.loc[(sales.shop_in_test==1)] #(sales.shop_id!=36)&","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"#data\npiv = np.log(pd.pivot_table(sales,\n                            index='date',\n                            columns=['shop_id','shop_name'],\n                            values='item_cnt_day',\n                            aggfunc='sum').fillna(0).clip(0,)+1).T\n\n#plotting\n\nfig, ax = plt.subplots(figsize=(max_width-1,9))\n\n_y = list(range(piv.shape[0]))[::-1]\n_x = mdates.date2num(piv.columns)\n    \nax.imshow(piv, aspect='auto', extent = [ _x[0],  _x[-1]+1,  _y[0]+1, _y[-1]], interpolation='none')\n\nax.xaxis_date()\nax.xaxis.set_major_locator(mdates.YearLocator())\nax.xaxis.set_major_formatter(mdates.DateFormatter('%Y - %m'))\nax.xaxis.set_minor_locator(mdates.MonthLocator())\nax.xaxis.set_minor_formatter(mdates.DateFormatter('%m'))\n\nax.yaxis.set_ticks(_y)\nax.set_yticklabels(piv.index.get_level_values(0)[::-1].astype('str') + ' ' + piv.index.get_level_values(1)[::-1],\n                  fontsize=8, va='top')\n\nfig.autofmt_xdate(which='both', rotation=90, ha='left')\n\n\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"del f_sales,transactions,f_transactions","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature engineering"},{"metadata":{},"cell_type":"markdown","source":"In the cells below I form time blocks and generate relateg features. The code is more or less readablee: "},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def get_monthly_list(n):  \n    #generate n blocks of days, n0 is test november, n+ are preceding month\n    \n    L=[0]*n\n    for i in range(n):\n        L[i]=pd.date_range(start=pd.Timestamp(2015, 11-i, 1), end=pd.Timestamp(2015, 12-i, 1),closed='left').values\n    return L\n\ndef get_30d_list(n):\n    #generate n blocks of days, n0 is test november, n+ are preceding 30 day intervals from 27th of september, \n    #each shifting 7 days back \n    \n    L=[0]*n\n    L[0]=pd.date_range(pd.Timestamp(2015, 11, 1), freq='D', periods=30).values\n    for i in range(n-1):\n        L[i+1]=pd.date_range(pd.Timestamp(2015, 9, 27), freq='D', periods=30).values-np.timedelta64(14*i, 'D')\n    return L\n\ndef get_grid(sales,L):\n    #generate n blocks of days, n0 is test november, n+ are preceding 30 day intervals from 27th of september, \n    #each shifting 7 days back \n    \n    index_cols = ['shop_id', 'item_id', 'date_block']\n    grid = [] \n    for i in range(len(L)-1):\n        cur_shops  = sales[sales.date.isin(L[i+1])]['shop_id'].unique()\n        cur_items  = sales[sales.date.isin(L[i+1])]['item_id'].unique()\n        grid.append(np.array(list(product(*[cur_shops, cur_items, [i+1]])),dtype='int32'))\n\n\n    grid = pd.DataFrame(np.vstack(grid), columns = index_cols,dtype=np.int32)\\\n            .append(test.loc[:,['shop_id','item_id']].assign(date_block=0))\n    return grid","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def get_features_normal(grid,L):\n    date_length_max = (np.datetime64(L[-1][0],'D')-np.datetime64('2013-01-01','D'))\n\n    X=[0]*len(L)\n    y=[0]*len(L)\n\n    for block in range(len(L)):\n        date_start = L[block][0]\n        \n        col_drop = ['shop_in_test',  \n                    'item_category_name',\n                    'cat_to_drop',\n                    'shop_name',\n                    'shop_name_part',\n                    'cat_in_test',\n                    'shop_to_drop'\n                    ] #,\n\n        data = sales.loc[(sales.date<date_start) & (sales.date>=date_start-date_length_max),\n                         ~sales.columns.isin(col_drop)]\n\n        X_temp = grid.loc[grid.date_block==block]\\\n                .join(items.loc[:,items.columns.isin(['item_id','item_category_id'])].set_index('item_id'), on='item_id')\\\n                .join(categories.set_index('item_category_id'), on='item_category_id')\\\n                .join(shops.set_index('shop_id'), on='shop_id')\\\n                .loc[:,lambda x : ~x.columns.isin(col_drop)]\n        \n\n#--------------------------------------------------\n        \n\n# date features, first/last item sale in shop and global: \n        \n        lt = len(X_temp.columns)\n        X_temp = X_temp\\\n                .join(data.groupby('item_id')\\\n                      .agg(days_first_item_sold_glob=('date','min'),\n                           days_last_item_sold_glob=('date','max')),\n                      on='item_id')\\\n                .join(data.groupby(['item_id','shop_id'])\\\n                      .agg(days_first_item_sold_shop=('date','min'),\n                           days_last_item_sold_shop=('date','max')),\n                      on=['item_id','shop_id'])\\\n        \n        \n        \n        X_temp.iloc[:,lt:] = ((date_start-X_temp.iloc[:,lt:])/np.timedelta64(1, 'D')).fillna(-1)\n        \n        \n#--------------------------------------------------        \n# global features: \n\n        X_temp = X_temp\\\n                .join(data.groupby('item_id')\\\n                       .agg(item_price_mean=('item_price','mean')),\n                      on='item_id')\\\n                .join(data.groupby('item_category_id')\\\n                       .agg(cat_price_mean=('item_price','mean')),\n                      on='item_category_id')\\\n        \n        X_temp = X_temp.assign(new_item = lambda x: x.item_price_mean.isna()*1)\n        \n        X_temp.loc[X_temp.cat_price_mean.isna(),['cat_price_mean']] = data.item_price.mean()        \n        X_temp.loc[X_temp.item_price_mean.isna(),['item_price_mean']] = X_temp['cat_price_mean']  \n\n        \n        lt = len(X_temp.columns)\n        \n        for i in ['item_category_id']:\n       \n            X_temp = X_temp.join(sales.groupby(['shop_id',i]).item_cnt_day.sum().to_frame('s1')\\\n                    .join(sales.groupby('shop_id').item_cnt_day.sum().to_frame('s2'))\\\n                    .join(sales.groupby(i).item_cnt_day.sum().to_frame('g1'))\\\n                    .assign(g2=sales.item_cnt_day.sum())\\\n                    .assign(**{i+'_shop_to_cat_glob': lambda x: (x.s1/x.s2)/(x.g1/x.g2)})\\\n                    .drop(['s1','s2','g1','g2'],axis=1),\n                                on=['shop_id',i])\n        \n            X_temp.iloc[:,-1].fillna(0,inplace=True)\n#--------------------------------------------------        \n# target:     \n        X_temp = X_temp.join(sales.loc[sales.date.isin(L[block])].groupby(['item_id','shop_id'])\\\n                      .agg(target=('item_cnt_day','sum')),\n                      on=['item_id','shop_id'])\n\n    \n#--------------------------------------------------\n# lag features, aggregations in past periods:    \n\n        \n        j0 = 0\n        \n        for j in [30,120]:\n            ll = len(X_temp.columns)\n            dd = data.loc[(data.date>=date_start-np.timedelta64(j, 'D'))&\n                          (data.date<date_start-np.timedelta64(j0, 'D'))]\n            \n                \n            X_temp = X_temp\\\n                .join(dd.groupby('item_id')\\\n                      .agg(item_glob_sales=('item_cnt_day','sum')),on='item_id')\\\n                .join(dd.groupby(['shop_id','item_id'])\\\n                       .agg(item_shop_sales=('item_cnt_day','sum')),on=['shop_id','item_id'])\\\n                .join(dd.groupby('item_category_id')\\\n                      .agg(cat_glob_sales=('item_cnt_day','sum')),on='item_category_id')\\\n                .join(dd.groupby(['shop_id','item_category_id'])\\\n                       .agg(cat_shop_sales=('item_cnt_day','sum')),on=['shop_id','item_category_id'])\\\n                .join(dd.groupby('item_id')\\\n                      .agg(item_glob_days=('date','nunique')),on='item_id')\\\n                .join(dd.groupby(['item_id','shop_id'])\\\n                      .agg(item_shop_days=('date','nunique')),on=['item_id','shop_id'])\\\n\n                \n                \n\n            X_temp.rename(columns=dict(zip(X_temp.iloc[:,ll:].columns,\n                                           X_temp.iloc[:,ll:].columns+'_'+str(j0)+'-'+str(j))),\n                          inplace=True)\n            \n            j0 = j\n            \n        X_temp.iloc[:,lt:] = X_temp.iloc[:,lt:].fillna(0)\n#--------------------------------------------------           \n        X_temp.loc[:,['shop_id','item_category_id']] = X_temp.loc[:,['shop_id','item_category_id']].astype('str')\n\n        X[block] = X_temp.loc[:,~X_temp.columns.isin(['target','cat_digital','shop_city'])] #,'item_id'\n        y[block] = X_temp.target.fillna(0).clip(0,20)\n    \n    \n    X = pd.concat(X)\n    \n    print('features - done')\n    \n    return X,y","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### tune - evaluate - train - test splitting\n\nDue to strong weekly seasonality - I think that it is better to split data not only by monthly chunks, but also try 30 day splits that have same ammount of weekdays."},{"metadata":{},"cell_type":"markdown","source":"monthly split:"},{"metadata":{},"cell_type":"markdown","source":"![abc](https://gb7esq.am.files.1drv.com/y4mnloWp3MGoGY05jQxYinL30uVQRpkv1X7iE9bIFG0L9pn0OSxDsSfXtc5bPszg6-DAwJLiwQuBqsss-hCdmvQK84vIuingocy11JNY3YUOi6XhLyGcfIVmJXD8KVattepX_FA8REqTHImwWuBvLVp8_rF7EgnqP5XK96F2qZPwcERgFjCxQCaGfUaCuKeu_HBQWeGDEJvYydFF_d8y3q_XA?width=709&height=119&cropmode=none)"},{"metadata":{},"cell_type":"markdown","source":"30 day split:"},{"metadata":{},"cell_type":"markdown","source":"![30d](https://tdkgva.am.files.1drv.com/y4m7Qc5M5fdQTawYZmR-9Pe9Blw6LeNhR0NFrusFBEiTEh45zoVPIGaKyHvnlB2bEkesPYyOHHWuIFdkFMbFJQQwYE3hB5mZ1rorBATomBnLqk_eOAQXg7awmc1W5DDbeSVcN65qfyY6vZlnV1jxVcQEZr3ply6eBaCYuNYXwor-gND28L8Y5ZnGcgPvwFIJaADaeQ1OSr3Wyp0IObQZV91Pw?width=709&height=299&cropmode=none)"},{"metadata":{},"cell_type":"markdown","source":"Forming splits for catboost:"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def data_for_cb(X,y,tb,bn,cat_features):\n    \n    tune_set_cb  = Pool(data=X.loc[X.date_block.isin(range(tb+1,bn))],\n                        label=pd.concat(y[tb+1:]),\n                        cat_features=cat_features)   \n\n    eval_set_cb  = Pool(data=X.loc[X.date_block.isin([1])],\n                        label=y[1],\n                        cat_features=cat_features)\n\n    train_set_cb = Pool(data=X.loc[X.date_block.isin(range(1,bn-tb))],\n                        label=pd.concat(y[1:-tb]),\n                        cat_features=cat_features)    \n\n    test_set_cb  = Pool(data=X.loc[X.date_block.isin([0])],\n                        label=y[0],\n                        cat_features=cat_features)\n    \n    print('cb data - done')\n    \n    return tune_set_cb, eval_set_cb, train_set_cb, test_set_cb","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Encoding data for random forest and knn:"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def encode_X(X,cat_features):\n    \n    X = pd.concat([X.loc[:,~X.columns.isin(cat_features)],\n                        pd.get_dummies(X.loc[:,cat_features])], axis=1) \n\n    _ = X.date_block.values\n    X = pd.DataFrame(data=StandardScaler().fit_transform(X), columns=X.columns)\n    X.iloc[:,23:] = X.iloc[:,23:]*0.1\n    X.date_block = _\n    \n    return X","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Forming splits for random forest and knn:"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def data_for_rf_knn(X,y,tb,bn):\n    \n    X_tune_enc  = X.loc[X.date_block.isin(range(tb+1,bn))].astype(np.float32)\n    y_tune_enc  = pd.concat(y[tb+1:]).astype(np.float32)\n\n    X_eval_enc  = X.loc[X.date_block.isin([1])].astype(np.float32)\n    y_eval_enc  = y[1].astype(np.float32)\n\n    X_train_enc = X.loc[X.date_block.isin(range(1,bn-tb))].astype(np.float32)\n    y_train_enc = pd.concat(y[1:-tb]).astype(np.float32)\n\n    X_test_enc  = X.loc[X.date_block.isin([0])].astype(np.float32)\n    y_test_enc  = y[0].astype(np.float32)\n    \n    print('rf and knn data - done')\n    \n    return X_tune_enc,y_tune_enc,X_eval_enc,y_eval_enc,X_train_enc,y_train_enc,X_test_enc,y_test_enc\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model training"},{"metadata":{},"cell_type":"markdown","source":"I use 3 first level models - catboost, random forest from sklearn, knn from rapids ai. Linear model is used on meta features."},{"metadata":{"trusted":true},"cell_type":"code","source":"cb  = CatBoostRegressor(task_type='GPU',\n                        iterations = 1500, \n                        boosting_type = 'Ordered',\n                        depth = 4)\n\nrf  = RandomForestRegressor(min_samples_leaf=15,\n                           n_jobs=-1,\n                           random_state=0,\n                           n_estimators=60)\n\nknn = KNeighborsRegressor(n_neighbors=30)\n\nlr  = LinearRegression(n_jobs=-1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def run_models(tune_set_cb, eval_set_cb, train_set_cb, test_set_cb,X_tune_enc,y_tune_enc,X_eval_enc,y_eval_enc,X_train_enc,y_train_enc,X_test_enc,y_test_enc):\n    \n    eval_df = pd.DataFrame(np.zeros((len(y_eval_enc),5)),columns=['rf','cb','knn','lr','y'])\n    test_df = pd.DataFrame(np.zeros((len(y_test_enc),5)),columns=['rf','cb','knn','lr','y'])\n    \n    print(' ')\n    print('tuning...')\n\n    rf.fit(X_tune_enc,y_tune_enc.values)\n    print('rf - done')\n\n    cb.fit(tune_set_cb, silent=True)\n    print('cb - done')\n\n    knn.fit(cudf.DataFrame.from_pandas(X_tune_enc), y_tune_enc.values)\n    print('knn - done')\n\n    eval_df.cb  = cb.predict(eval_set_cb).clip(0,20)\n    eval_df.rf  = rf.predict(X_eval_enc).clip(0,20)\n    eval_df.knn = knn.predict(cudf.DataFrame.from_pandas(X_eval_enc)).to_pandas().clip(0,20)\n    eval_df.y   = y_eval_enc.values\n\n    print('rf','{:.4}'.format(mean_squared_error(eval_df.y, eval_df.rf,squared=False)))\n    print('cb','{:.4}'.format(mean_squared_error(eval_df.y, eval_df.cb,squared=False)))\n    print('knn','{:.4}'.format(mean_squared_error(eval_df.y, eval_df.knn,squared=False)))\n\n    lr.fit(eval_df.iloc[:,:-2],eval_df.y)\n    print('lr - done')\n\n    eval_df.lr = lr.predict(eval_df.iloc[:,:-2]).clip(0,20)\n\n    print('lr','{:.4}'.format(mean_squared_error(eval_df.y, eval_df.lr,squared=False)))\n    print('model weights',\n          'rf',\n          '{:.1%}'.format(lr.coef_[0]),\n          'cb',\n          '{:.1%}'.format(lr.coef_[1]),\n          'knn',\n          '{:.1%}'.format(lr.coef_[2]))\n\n    print(' ')\n    print('training...')\n\n    rf.fit(X_train_enc,y_train_enc.values)\n    print('rf - done')\n    \n    cb.fit(train_set_cb, silent=True)\n    print('cb - done')\n\n    knn.fit(cudf.DataFrame.from_pandas(X_train_enc), y_train_enc.values)\n    print('knn - done')\n\n    test_df.rf  = rf.predict(X_test_enc).clip(0,20)\n    test_df.cb  = cb.predict(test_set_cb).clip(0,20)\n    test_df.knn = knn.predict(cudf.DataFrame.from_pandas(X_test_enc)).to_pandas().clip(0,20)\n    test_df.lr  = lr.predict(test_df.iloc[:,:-2]).clip(0,20)\n\n    print('all done')\n\n    return test_df.lr","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"print('start monthly models...')\nprint(' ')\n\nbn = 4\ntb = 1\ncat_features = ['cat_global','cat_platform','shop_id','item_category_id']\n\nL = get_monthly_list(bn)\ngrid = get_grid(sales,L)\nX,y = get_features_normal(grid,L)\n\ntune_set_cb, eval_set_cb, train_set_cb, test_set_cb = data_for_cb(X,y,tb,bn,cat_features)\n\nX = encode_X(X,cat_features)\n\nX_tune_enc,y_tune_enc,X_eval_enc,y_eval_enc,X_train_enc,y_train_enc,X_test_enc,y_test_enc = data_for_rf_knn(X,y,tb,bn)\n\ndel X,y\n\npred_monthly = run_models(tune_set_cb, eval_set_cb, train_set_cb, test_set_cb,X_tune_enc,y_tune_enc,X_eval_enc,y_eval_enc,X_train_enc,y_train_enc,X_test_enc,y_test_enc)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"print(' ')\nprint('start 30d models...')\nprint(' ')\n\nbn = 7\ntb = 2\ncat_features = ['cat_global','cat_platform','shop_id','item_category_id']\n\nL = get_30d_list(bn)\ngrid = get_grid(sales,L)\nX,y = get_features_normal(grid,L)\n\ntune_set_cb, eval_set_cb, train_set_cb, test_set_cb = data_for_cb(X,y,tb,bn,cat_features)\n\nX = encode_X(X,cat_features)\n\nX_tune_enc,y_tune_enc,X_eval_enc,y_eval_enc,X_train_enc,y_train_enc,X_test_enc,y_test_enc = data_for_rf_knn(X,y,tb,bn)\n\ndel X,y\n\npred_30d = run_models(tune_set_cb, eval_set_cb, train_set_cb, test_set_cb,X_tune_enc,y_tune_enc,X_eval_enc,y_eval_enc,X_train_enc,y_train_enc,X_test_enc,y_test_enc)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Submission"},{"metadata":{"trusted":true},"cell_type":"code","source":"f_sample.item_cnt_month = (pred_monthly*0.5+pred_30d*0.5).clip(0,20)\nf_sample.to_csv(r'submission.csv',index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"}},"nbformat":4,"nbformat_minor":4}