{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input/eng-translations'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"categories = pd.read_csv(\"../input/eng-translations/categories_eng.csv\")\nitems = pd.read_csv(\"../input/eng-translations/items_eng.csv\")\nsales = pd.read_csv(\"../input/competitive-data-science-predict-future-sales/sales_train.csv\")\ntest = pd.read_csv(\"../input/competitive-data-science-predict-future-sales/test.csv\")\nshops = pd.read_csv(\"../input/eng-translations/shops_eng.csv\")\nsubmission = pd.read_csv(\"../input/competitive-data-science-predict-future-sales/sample_submission.csv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def downcast1(df, verbose=True):\n    \n    \"\"\"\n    Funciton to reduce the memory used of a particular dataframe by downcasting to a less memory-intensive data type.\n    \"\"\"\n    \n    start_mem = df.memory_usage().sum() / 1024**2\n    \n    for col in df.columns:\n        dtype_name = df[col].dtype.name\n        if dtype_name == 'object':\n            pass\n        elif dtype_name == 'bool':\n            df[col] = df[col].astype('int8')\n        elif dtype_name.startswith('int') or (df[col].round() == df[col]).all():\n            df[col] = pd.to_numeric(df[col], downcast='integer')\n        else:\n            df[col] = pd.to_numeric(df[col], downcast='float')\n    \n    end_mem = df.memory_usage().sum() / 1024**2\n    \n    if verbose:\n        print('{:.1f}% compressed'.format(100 * (start_mem - end_mem) / start_mem))\n    \n    return df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_df = [sales, shops, items, categories, test]\nfor df in all_df:\n    df = downcast1(df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Preparation and Cleaning","metadata":{}},{"cell_type":"markdown","source":"## Shops","metadata":{}},{"cell_type":"code","source":"shops.sample(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import re\n\ndef cleans(i):\n    \n    \"\"\"\n    Function to clean strings, removing non-alphanumeric characters.\n    \"\"\"\n    \n    pattern = r'[A-Za-z0-9]+'\n    \n    finds = re.findall(pattern, str(i))\n\n    stringy = \"\"\n    \n    for j in finds:\n        \n        stringy += f\" {j}\"\n        \n    return stringy","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"shops[\"clean\"] = shops[\"shop_name\"].apply(cleans)\nshops.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Deal with obsolete shop_ids\n\nsales.loc[sales[\"shop_id\"]==0, \"shop_id\"] = 57\nsales.loc[sales[\"shop_id\"]==1, \"shop_id\"] = 58\nsales.loc[sales[\"shop_id\"]==10, \"shop_id\"] = 11\nsales.loc[sales[\"shop_id\"]==39, \"shop_id\"] = 40\n\ntest.loc[test['shop_id'] == 0, 'shop_id'] = 57\ntest.loc[test['shop_id'] == 1, 'shop_id'] = 58\ntest.loc[test['shop_id'] == 10, 'shop_id'] = 11\ntest.loc[test['shop_id'] == 39, 'shop_id'] = 40","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Only use shops in train data that are in test data\n\nunique_test_shops = test[\"shop_id\"].unique()\nsales = sales[sales[\"shop_id\"].isin(unique_test_shops)]\n\nprint(f\"Number of Unique Shops in Test Data:{len(unique_test_shops)}\\nNumber of Unique Shops in Sales Data:{len(sales['shop_id'].unique())}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"shops.drop(\"shop_name\", axis=1, inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"shops[\"city\"] = shops[\"clean\"].apply(lambda x: x.split()[0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Use LabelEncoder to convert categorical variables into numerical variables\n\nfrom sklearn.preprocessing import LabelEncoder\n\nle = LabelEncoder()\n\nshops[\"city\"] = le.fit_transform(shops[\"city\"])\nshops.drop(\"clean\", axis=1, inplace=True)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Final shops dataframe\nshops.sample(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Items","metadata":{}},{"cell_type":"code","source":"items[\"item_name\"] = items[\"item_name\"].str.lower()\nitems[\"item_name_clean\"] = items[\"item_name\"].apply(cleans)\nitems.drop(\"item_name\", axis=1, inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Take the first five characters of the item_name string\n\nitems[\"item_name_five\"] = [x[:5] for x in items[\"item_name_clean\"]]\nitems[\"item_name_five\"] = le.fit_transform(items[\"item_name_five\"])\nitems.drop(\"item_name_clean\", axis=1, inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create first_sale_date feature\n\nitems[\"first_sale_date\"] = sales.groupby(\"item_id\").agg({\"date_block_num\":\"min\"})[\"date_block_num\"]\nitems","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# As the NaN values in this table are for items first sold in the test period, replace them with 34 (the date_block_num for the test period)\n\nitems[items[\"first_sale_date\"].isna()]\nitems[\"first_sale_date\"] = items[\"first_sale_date\"].fillna(34)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Categories","metadata":{}},{"cell_type":"code","source":"categories[\"category\"] = categories[\"category_name\"].apply(lambda x: x.split()[0])\ncategories","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"categories[\"category\"].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Cleaning \n\ncategories.loc[categories[\"category\"] == \"Game\"] = \"Games\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def make_misc(x):\n    \n    \"\"\"\n    Function to change the name of low frequency categories to 'Misc'\n    \"\"\"\n    \n    if len(categories[categories['category']==x]) >= 5:\n        return x\n    else:\n        return 'Misc'\n    \ncategories[\"cats\"] = categories[\"category\"].apply(make_misc)\n\ncategories","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"categories.drop([\"category\", \"category_name\"], axis=1, inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Encode the 'cats' feature and delete\n\ncategories[\"cats_le\"] = le.fit_transform(categories[\"cats\"])\n\ncategories.drop(\"cats\", inplace=True, axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Remove Outliers from the Sales Dataframe","metadata":{}},{"cell_type":"code","source":"sales = sales[sales[\"item_price\"] > 0]\nsales = sales[sales[\"item_price\"] < 50000]\nsales = sales[sales[\"item_cnt_day\"] > 0]\nsales = sales[sales[\"item_cnt_day\"] < 1000]\nsales[\"item_price\"] = sales[\"item_price\"].apply(lambda x: round(x,2))\nsales","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data Combinations","metadata":{}},{"cell_type":"code","source":"# Create a dataframe of the Cartesian Product of the unique shops and unique items for each month\n\nfrom itertools import product\n\ntrain = []\n\nfor i in range(0,34):\n    \n    cur_shops = sales.loc[sales[\"date_block_num\"] == i, \"shop_id\"].unique()\n    \n    cur_items = sales.loc[sales[\"date_block_num\"] == i, \"item_id\"].unique()\n    \n    train.append(np.array(list(product(*[[i],cur_shops, cur_items]))))\n    \nindex_feats = [\"date_block_num\", \"shop_id\", \"item_id\"]\n\ntrain = pd.DataFrame(np.vstack(train), columns=index_feats)\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create the column showing how many of each item have been sold in each month. This is the form the target variable will take.\n\ngroup = sales.groupby(index_feats).agg({\"item_cnt_day\": \"sum\"})\ngroup = group.reset_index()\ngroup = group.rename(columns={\"item_cnt_day\": \"item_cnt_month\"})\n\ntrain = pd.merge(train, group, on=index_feats, how=\"left\")\ntrain","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Use garbage collection to minimise memory usage\n\nimport gc\n\ndel group\n\ngc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Add column for count of items sold.\n\ngroup = sales.groupby(index_feats).agg({\"item_cnt_day\":\"count\"})\ngroup = group.reset_index()\ngroup = group.rename(columns={\"item_cnt_day\":\"item_cnt\"})\n\ntrain = pd.merge(train, group, on=index_feats, how=\"left\")\n\ntrain.sample(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del group, sales\ngc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Add Test Data to Overall Dataframe","metadata":{}},{"cell_type":"code","source":"test[\"date_block_num\"] = 34\n\nall_data = pd.concat([train, test.drop(\"ID\", axis=1)], ignore_index=True, keys=index_feats)\n\nall_data = all_data.fillna(0)\n\nall_data.sample(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Merge all dataframes \n\nall_data = pd.merge(all_data, shops, on=\"shop_id\", how=\"left\")\nall_data = pd.merge(all_data, items, on=\"item_id\", how=\"left\")\nall_data = pd.merge(all_data, categories, on=\"category_id\", how=\"left\")\n\nall_data.sample(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_data = downcast1(all_data)\n\ndel shops, items, categories\n\ngc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Create Mean Features","metadata":{}},{"cell_type":"code","source":"def add_mean_feats(df, mean_feats, index_features, agg_col=\"item_cnt_month\", agg_func=\"mean\"):\n    \n    \"\"\"\n    Function to automatically create new features showing the mean item_cnt_month grouped by the specified columns.\n    \"\"\"\n    \n    if len(index_features) == 2:\n        feature_name = index_features[1] + f\"_{agg_col}_{agg_func}\"\n    else: \n        feature_name = index_features[1] + \"_\" + index_features[2] + f\"_{agg_col}_{agg_func}\"\n        \n    group = df.groupby(index_features).agg({agg_col:agg_func}).reset_index().rename(columns={agg_col:feature_name})\n    \n    df = pd.merge(df, group, on=index_features, how=\"left\")\n    \n    df = downcast1(df)\n    \n    mean_feats.append(feature_name)\n    \n    del group\n    gc.collect()\n    \n    return df, mean_feats\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"item_mean_features = []\n\nall_data, item_mean_features = add_mean_feats(all_data, item_mean_features, [\"date_block_num\", \"item_id\"])\n\nall_data","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_data, item_mean_features = add_mean_feats(all_data, item_mean_features, [\"date_block_num\", \"item_id\", \"city\"])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"shop_mean_features = []\n\nall_data, shop_mean_features = add_mean_feats(all_data, shop_mean_features, [\"date_block_num\", \"shop_id\", \"category_id\"])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cat_mean_features = []\n\nall_data, cat_mean_features = add_mean_feats(all_data, cat_mean_features, [\"date_block_num\", \"category_id\"])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_data, cat_mean_features = add_mean_feats(all_data, cat_mean_features, [\"date_block_num\", \"cats_le\"])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_data.sample(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Create Lag Features","metadata":{}},{"cell_type":"code","source":"def add_lags(df, lag_features, index_features, lag_feature, lags=[1,2,3], clip=False):\n    \n    \"\"\"\n    Function to automatically create lag features based on the columns specified.\n    \"\"\"\n    \n    df_temp = df[index_features + [lag_feature]].copy()\n    \n    for i in lags:\n        \n        feat_name = lag_feature + \"_lag\" + str(i)\n        df_temp.columns = index_features + [feat_name]\n        df_temp[\"date_block_num\"] += i\n        df = pd.merge(df, df_temp.drop_duplicates(), on=index_features, how=\"left\")\n        df[feat_name] = df[feat_name].fillna(0)\n        \n        if clip:\n            lag_feats_to_clip.append(feat_name)\n            \n    df = downcast1(df)\n    del df_temp\n    gc.collect()\n    \n    return df, lag_feats_to_clip","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lag_feats_to_clip = []\nindex_features = [\"date_block_num\", \"shop_id\", \"item_id\"]\n\nall_data, lag_feats_to_clip = add_lags(all_data, lag_feats_to_clip, index_features, \"item_cnt_month\", clip=True)\nall_data, lag_feats_to_clip = add_lags(all_data, lag_feats_to_clip, index_features, \"item_cnt\", clip=True)\n\nall_data.sample(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check there is no data that has leaked into test set\n\nX_test_temp = all_data[all_data[\"date_block_num\"]==34]\nX_test_temp[item_mean_features].sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Now use the lists that have saved previously in creating the mean features to create additional lags\n\nfor item in item_mean_features:\n    \n    all_data, lag_feats_to_clip = add_lags(all_data, lag_feats_to_clip, index_features, item, clip=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for shop in shop_mean_features:\n    \n    all_data, lag_feats_to_clip = add_lags(all_data, lag_feats_to_clip, [\"date_block_num\", \"shop_id\", \"category_id\"], \n                                           shop, clip=True)\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for cat in cat_mean_features:\n    \n    all_data, lag_feats_to_clip = add_lags(all_data, lag_feats_to_clip, [\"date_block_num\", \"category_id\"], cat, lags=[1,2,3], clip=True)\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_data = all_data.drop(item_mean_features, axis=1)\nall_data = all_data.drop(shop_mean_features, axis=1)\nall_data = all_data.drop(cat_mean_features, axis=1)\n\nall_data = all_data.drop(all_data[all_data[\"date_block_num\"]<3].index)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_data.sample(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del X_test_temp\ngc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Additional Features","metadata":{}},{"cell_type":"code","source":"# Create feature showing mean of the three lags\n\nall_data[\"item_cnt_month_3lag_mean\"] = all_data[[\"item_cnt_month_lag1\", \"item_cnt_month_lag2\", \"item_cnt_month_lag3\"]].mean(axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_data[lag_feats_to_clip + [\"item_cnt_month_3lag_mean\", \n                                 \"item_cnt_month\"]] =  all_data[lag_feats_to_clip + [\"item_cnt_month_3lag_mean\", \n                                                                                        \"item_cnt_month\"]].clip(0,20)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create lag gradient features\n\nall_data[\"lag_grad_1\"] = all_data[\"item_cnt_month_lag1\"] / all_data[\"item_cnt_month_lag2\"]\nall_data[\"lag_grad_1\"] = all_data[\"lag_grad_1\"].replace([np.inf, -np.inf], np.nan).fillna(0)\n\nall_data[\"lag_grad_2\"] = all_data[\"item_cnt_month_lag2\"] / all_data[\"item_cnt_month_lag3\"]\nall_data[\"lag_grad_2\"] = all_data[\"lag_grad_2\"].replace([np.inf, -np.inf], np.nan).fillna(0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_data[\"new_items\"] = all_data[\"first_sale_date\"] == all_data[\"date_block_num\"]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_data[\"time_since_first_sale\"] = all_data[\"date_block_num\"] - all_data[\"first_sale_date\"]\n\nall_data.drop(\"first_sale_date\", inplace=True, axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_data[\"month\"] = all_data[\"date_block_num\"] % 12","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_data.drop([\"item_cnt\"], axis=1, inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_data = downcast1(all_data)\nall_data.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Change category_id from object datatype to int8\n\nall_data[\"category_id\"] = all_data[\"category_id\"].astype(\"int8\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Creating the Machine Learning Models","metadata":{}},{"cell_type":"code","source":"X_train = all_data[all_data[\"date_block_num\"]<33]\ny_train = X_train[\"item_cnt_month\"]\nX_train = X_train.drop(\"item_cnt_month\", axis=1)\n\nX_val = all_data[all_data[\"date_block_num\"] == 33]\ny_val = X_val[\"item_cnt_month\"]\nX_val = X_val.drop(\"item_cnt_month\", axis=1)\n\nX_test = all_data[all_data[\"date_block_num\"]==34]\nX_test = X_test.drop(\"item_cnt_month\", axis=1)\n\ndel all_data\ngc.collect()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def preds(model, test, name):\n    \n    \"\"\"\n    Function to use the chosen model to make predictions using the chosen test set, format the\n    predictions and save these as a .csv file ready for upload to Kaggle.\n    \"\"\"\n    \n    prediction = model.predict(test)\n    \n    df_sub = pd.read_csv(\"../input/competitive-data-science-predict-future-sales/sample_submission.csv\")\n    \n    df_sub[\"item_cnt_month\"] = prediction.clip(0,20)\n    \n    df_sub.to_csv(f\"{name}.csv\", index=False)\n    \n    print(\"Complete.\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Try Light Gradient Boosting Machine, parameters can be altered for further accuracy.\n\nimport lightgbm as lgb\n\nparams = {'metric': 'rmse',\n          'num_leaves': 255,\n          'learning_rate': 0.005,\n          'feature_fraction': 0.75,\n          'bagging_fraction': 0.75,\n          'bagging_freq': 5,\n          'force_col_wise' : True,\n          'random_state': 10,\n         'num_rounds':1500,\n         'early_stopping':150}\n\nlgb_train = lgb.Dataset(X_train, y_train)\n\nlgb_val = lgb.Dataset(X_val, y_val)\n\nmodel = lgb.train(params=params, train_set=lgb_train, valid_sets=(lgb_train, lgb_val), verbose_eval=50)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds(model, X_test, \"lgb_model\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del lgb_train, lgb_val\ngc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_features(booster, figsize):\n    \n    \"\"\"\n    Function to create a feature importance plot\n    \"\"\"\n    \n    fig, ax = plt.subplots(1, 1, figsize=figsize)\n    return plot_importance(booster=booster, ax=ax)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Try an Extreme Gradient Boosting model\n\nfrom xgboost import XGBRegressor, plot_importance\nimport matplotlib.pyplot as plt\n\n\nxgb_model = XGBRegressor(\n    max_depth=8,\n    n_estimators=100,\n    min_child_weight=300, \n    colsample_bytree=0.8, \n    subsample=0.8, \n    eta=0.1,    \n    seed=42)\n\nxgb_model.fit(X_train, y_train, eval_metric=\"rmse\", eval_set=[(X_train, y_train), (X_val, y_val)])\n\npreds(xgb_model, X_test, f\"xgb_{i}\")\n\nplot_features(xgb_model, (10, 14))","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}