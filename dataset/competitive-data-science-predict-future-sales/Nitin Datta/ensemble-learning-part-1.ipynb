{"cells":[{"metadata":{},"cell_type":"markdown","source":"<font color=\"red\" size=5><center>Ensemble Learning -Part 1</center></font>"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"from IPython.display import Image\nimport os\nImage(\"../input/ensemble-learning-pic/EL.png\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This notebook on Ensemble Learning will be divided into 2 parts.\n\n\nThis is **Part-1**\n### For Part-2 click [here](https://www.kaggle.com/nitindatta/ensemble-learning-part-1) \n"},{"metadata":{},"cell_type":"markdown","source":"## To begin with, below is a short story on why ensemble learning is so widely used in competitions.\n \nYou need a plumber, and you find one that has 4.5 stars rating (out of 5) and charges 100 dollars to do the job. Now I offer you two plumbers, each with 4 stars rating, that charge 75 dollars apiece. My selling point to you is that they would visit one at a time, and the second plumber will fix whatever the first didnâ€™t do right. You laugh at me and take a rock-star plumber. Why would you spend 2x the time and 1.5x the money when the first plumber would probably do the job just fine?\n\nLetâ€™s start with the same premise, but now I offer you 4 plumbers. Each has a 3 star rating, so they charge 23 dollars apiece. They would come to your house together, work as a team, and fix your problem faster and for less money. You think about it for a second, because it would be nice to save 8 dollars. In the end, you decide to go with your rock-star plumber because: a) he must be good if he is charging 100 dollars; b) other 4 plumbers canâ€™t be as good or else they would be charging more. Even though you are probably right on both counts, that still doesnâ€™t guarantee you made the best choice.\n\nIn most societies there is an unwritten rule that a single expert is always better than 3 so-so experts combined. But letâ€™s see if that holds for predictions we have to make.\n\nBelow is a simple example of predicting 10 digits that are evenly split between 1 and 0.\n\n```\n1111100000    Ground truth \n1110100000    Strong learner (90%) Best at predicting 0s\n```\n\nIt seems like we have a very good model â€“ a good expert, if you will. This model is perfect in predicting 0s, and pretty good at predicting 1s.\n\nNow we take three weak models, none of which are better than 70% in predicting digits.\n\n```\n1111100000     Ground truth\n1011110100     Weak learner (70%) Good at predicting 1s\n1101000010     Weak learner (70%) Good at predicting 0s\n0110101001     Weak learner (60%) Not good at predicting anything\n1111100000     Vote average of weak learners (100%)\n```\n\nWe take the average vote of their predictions since none of them are very good. Amazingly, we get a prediction at 100% accuracy. Is this a setup devised by yours truly in number selection, or does it actually hold in real life?\n\nIt is fairly intuitive that blending two good models will again yield a good model, and it also makes sense that the result could be better than either individual model. It is not so obvious that blending a good and a bad model could yield a better result. It is even less obvious that blending 3 bad models could yield a really good model, but that is the case.\n\nThis phenomenon is often referred to as the strength of weak learners. **This doesnâ€™t mean that combining any 3 weak learners will result in a great model**. A complementary expertise is needed. If you get 3 individuals with mediocre expertise that overlaps 95% between them, that would mean that each brings in only 5% unique knowledge compared to their union. On the other hand, 3 WEAK AND DIVERSE experts that overlap 70% in their knowledge and bring 30% of unique expertise each, are likely to blend into a good model. That is exactly the case with 3 weak learners I used in the example above: one of them is equally good/bad at predicting everything, while the other two are good at predicting 1s and 0s, respectively."},{"metadata":{},"cell_type":"markdown","source":"### The above story/information is picked from [here](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/discussion/51058#290767)."},{"metadata":{},"cell_type":"markdown","source":"## Table of Contents\n\n1. [Simple Ensemble Learning](#1)\n\n   a. [Max Voting](#11)\n   \n   b. [Averaging](#12)\n   \n   c. [Weighted Averaging](#13) \n   \n   \n2. [Advanced Ensemble Learning Types](https://www.kaggle.com/nitindatta/ensemble-learning-part-2#3)\n\n    a. [Stacking](https://www.kaggle.com/nitindatta/ensemble-learning-part-2#31)\n    \n    b. [Blending](https://www.kaggle.com/nitindatta/ensemble-learning-part-2#32)\n    \n    c. [Bagging](https://www.kaggle.com/nitindatta/ensemble-learning-part-2#33)\n        \n    d. [Boosting](https://www.kaggle.com/nitindatta/ensemble-learning-part-2#34)\n      \n      * [XGBoost](https://www.kaggle.com/nitindatta/ensemble-learning-part-2#341)\n      \n      * [AdaBoost](https://www.kaggle.com/nitindatta/ensemble-learning-part-2#342)\n      \n      * [Light GBM](https://www.kaggle.com/nitindatta/ensemble-learning-part-2#343)\n      \n      * [Catboost](https://www.kaggle.com/nitindatta/ensemble-learning-part-2#344)"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport time\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import mean_squared_error\nimport datetime\nimport warnings\nimport eli5\nfrom eli5.sklearn import PermutationImportance\n\n%matplotlib inline\nsns.set(style=\"darkgrid\")\nwarnings.filterwarnings(\"ignore\")\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test = pd.read_csv('/kaggle/input/competitive-data-science-predict-future-sales/test.csv')\nitem_categories = pd.read_csv('/kaggle/input/competitive-data-science-predict-future-sales/item_categories.csv')\nitems = pd.read_csv('/kaggle/input/competitive-data-science-predict-future-sales/items.csv')\nshops = pd.read_csv('/kaggle/input/competitive-data-science-predict-future-sales/shops.csv')\nsales = pd.read_csv('/kaggle/input/competitive-data-science-predict-future-sales/sales_train.csv',parse_dates=['date'],dtype={'date': 'str'})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Concatenating item_categories, items, shops and sales dataframes as train\ndf = sales.join(items, on='item_id',rsuffix='_')\ndf = df.join(shops, on='shop_id', rsuffix='_')\ndf = df.join(item_categories, on='item_category_id', rsuffix='_')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As the data consumes high memory we will downcast it.\n\n\nSource of the method: [LINK](https://www.kaggle.com/kyakovlev/1st-place-solution-part-1-hands-on-data)"},{"metadata":{"trusted":true},"cell_type":"code","source":"def downcast_dtypes(df):\n    float_cols = [c for c in df if df[c].dtype == \"float64\"]\n    int_cols = [c for c in df if df[c].dtype in [\"int64\", \"int32\"]]\n    df[float_cols] = df[float_cols].astype(np.float32)\n    df[int_cols] = df[int_cols].astype(np.int16)\n    return df\n\ndf = downcast_dtypes(df)\nprint(df.info())","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"df.head().T","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are some redundant values which we will remove later"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"df.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"print('Dataframe shape :',df.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Data Leakages\n\nThe below code snippet is picked from [here](https://www.kaggle.com/dimitreoliveira/model-stacking-feature-engineering-and-eda)."},{"metadata":{"trusted":true},"cell_type":"code","source":"test_shop_ids = test['shop_id'].unique()\ntest_item_ids = test['item_id'].unique()\n# Only shops that exist in test set.\nleak_df = df[df['shop_id'].isin(test_shop_ids)]\n# Only items that exist in test set.\nleak_df = leak_df[leak_df['item_id'].isin(test_item_ids)]\nprint('Data set size before leaking:', df.shape[0])\nprint('Data set size after leaking:', leak_df.shape[0])\ndf = leak_df","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"print(df.isnull().sum())\nprint('\\nNo null records')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We will drop all the strings (object type) and item_category_id as we will not use them.\ndf.drop(['item_name','shop_name','item_category_name','item_category_id'],axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"print('Is column \\'shop_id\\' equal to \\'shop_id_\\' :',df['shop_id'].equals(df['shop_id_']),'\\n')\nprint('Is column \\'item_id\\' equal to \\'item_id_\\' :',df['item_id'].equals(df['item_id_']),'\\n')\nprint('\\nAll are same so we will drop the duplicates')\ndf.drop(['shop_id_','item_id_'],axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df[df['item_price']>0]\n# Dropped row where item_price is less than 0 ","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"df = df.sort_values('date').groupby(['date_block_num', 'shop_id','item_id'], as_index=False)\ndf = df.agg({'item_price':['sum', 'mean'], 'item_cnt_day':['sum', 'mean','count']})\n# Rename features.\ndf.columns = ['date_block_num', 'shop_id', 'item_id', 'item_price', 'mitem_price', 'item_cnt', 'mitem_cnt', 'transactions']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"df.count()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['year'] = df['date_block_num'].apply(lambda x: ((x//12) + 2013))\ndf['month'] = df['date_block_num'].apply(lambda x: (x % 12))","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(22,8))\nplt.subplot(2, 1, 1)\nsns.boxplot(x=df['item_cnt'])\nplt.subplot(2, 1, 2)\nsns.boxplot(x=df['item_price'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Highly skewed `item_cnt` and `item_price`.\nLet us remove any `item_cnt` above 1500 and `item_prce` above 400000"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df.query('item_cnt >= 0 and item_cnt <= 1500 and item_price < 400000')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"df['cnt_m'] = df.sort_values('date_block_num').groupby(['shop_id','item_id'])['item_cnt'].shift(-1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"df.describe().T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"corr = df.corr()\nmask = np.zeros_like(corr)\nmask[np.triu_indices_from(mask)] = True\nwith sns.axes_style(\"white\"):\n    f, ax = plt.subplots(figsize=(9, 7))\n    ax = sns.heatmap(corr,mask=mask,square=True,annot=True,fmt='0.2f',linewidths=.8,cmap=\"YlGnBu\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"`item_cnt` is correlated with `transactions` and `year` is highly correlated with `date_block_num`"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"fig = sns.jointplot(x='item_price',y='item_id',data=df,\n                   joint_kws={'alpha':0.2,'color':'orange'},\n                   marginal_kws={'color':'red'})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Around `item_id`: 6000 there seems to be an outlier due to high `item_price`"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,6)) \nsns.countplot(df['shop_id'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"`Shop_id 31` has highest number of sales"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"plt.figure(figsize=(20,6)) \nsns.barplot(x=df['shop_id'],y=df['item_cnt'],palette='viridis')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"`Shop_id 9` has highest number of unique items"},{"metadata":{"trusted":true},"cell_type":"code","source":"item_cat_price = df.groupby(['item_id']).sum()['item_price']\nplt.figure(figsize=(18,6))\nitem_cat_price.plot(color ='red')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Somewhere around `item_id`: 6000 we might have an outlier."},{"metadata":{"trusted":true},"cell_type":"code","source":"ts = time.time()\nshop_ids = df['shop_id'].unique()\nitem_ids = df['item_id'].unique()\nempty_df = []\nfor i in range(34):\n    for shop in shop_ids:\n        for item in item_ids:\n            empty_df.append([i, shop, item])\n    \nempty_df = pd.DataFrame(empty_df, columns=['date_block_num','shop_id','item_id'])\nprint(time.time()-ts)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Merge the train set with the complete set (missing records will be filled with 0).\ndf = pd.merge(empty_df, df, on=['date_block_num','shop_id','item_id'], how='left')\ndf.fillna(0, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Splitting the data into `train`, `validation` and `test` set.\n* Train set will be from `date_block_num` : 0-28 \n* Validation set will be from `date_block_num` : 29-32\n* Test set will be from `date_block_num` : 33"},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"train_set = df.query('date_block_num >= 0 and date_block_num < 26').copy()\nvalidation_set = df.query('date_block_num >= 26 and date_block_num < 33').copy()\ntest_set = df.query('date_block_num == 33').copy()\n\nprint('Train set records:', train_set.shape[0])\nprint('Validation set records:', validation_set.shape[0])\nprint('Test set records:', test_set.shape[0])\n\nprint('Percent of train_set:',(train_set.shape[0]/df.shape[0])*100,'%')\nprint('Percent of validation_set:',(validation_set.shape[0]/df.shape[0])*100,'%')\nprint('Percent of test_set:',(test_set.shape[0]/df.shape[0])*100,'%')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_set.dropna(subset=['cnt_m'], inplace=True)\nvalidation_set.dropna(subset=['cnt_m'], inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating training and validation sets\nx_train = train_set.drop(['cnt_m','date_block_num'],axis=1)\ny_train = train_set['cnt_m'].astype(int)\n\nx_val = validation_set.drop(['cnt_m','date_block_num'],axis=1)\ny_val = validation_set['cnt_m'].astype(int)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"latest_records = pd.concat([train_set, validation_set]).drop_duplicates(subset=['shop_id', 'item_id'], keep='last')\nx_test = pd.merge(test, latest_records, on=['shop_id', 'item_id'], how='left', suffixes=['', '_'])\nx_test['year'] = 2015\nx_test['month'] = 9\nx_test.drop('cnt_m', axis=1, inplace=True)\nx_test = x_test[x_train.columns]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ts=time.time()\nsets = [x_train, x_val, x_test]\nfor dataset in sets:\n    for shop_id in dataset['shop_id'].unique():\n        for column in dataset.columns:\n            shop_median = dataset[(dataset['shop_id'] == shop_id)][column].median()\n            dataset.loc[(dataset[column].isnull()) & (dataset['shop_id'] == shop_id), column] = shop_median\n            \n# Fill remaining missing values on test set with mean.\nx_test.fillna(x_test.mean(), inplace=True)\nprint(time.time()-ts)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"x_test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom scipy import stats","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# These will be our base models\nm1 = LinearRegression()\nm2 = DecisionTreeRegressor()\nm3 = RandomForestRegressor(n_estimators=10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"1\"></a> <br>\n# 1-Simple Ensemble Learning \n<a id=\"11\"></a> <br>\n### a.Max Voting\n "},{"metadata":{"trusted":true},"cell_type":"code","source":"ts = time.time()\nfrom sklearn.ensemble import VotingRegressor\nmodel = VotingRegressor([('lr', m1), ('dt', m2),('rf', m3)])\nmodel.fit(x_train, y_train)\ntrain_pred = model.predict(x_train)\nval_pred = model.predict(x_val)\nprint('Total time taken :',time.time()-ts) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"print('Train rmse:', np.sqrt(mean_squared_error(y_train, train_pred)))\nprint('Validation rmse:', np.sqrt(mean_squared_error(y_val, val_pred)))","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"perm = PermutationImportance(model, random_state=1).fit(x_val, y_val)\neli5.show_weights(perm, feature_names = x_val.columns.tolist())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"`item_cnt` is an important feature and plays a vital role in predicting the output."},{"metadata":{},"cell_type":"markdown","source":"<a id=\"12\"></a> <br>\n### b. Average Voting "},{"metadata":{"trusted":true},"cell_type":"code","source":"ts = time.time()\nm1.fit(x_train, y_train)\nm2.fit(x_train, y_train)\nm3.fit(x_train,y_train)\n\navg_train_pred1 = m1.predict(x_train)\navg_train_pred2 = m2.predict(x_train)\navg_train_pred3 = m3.predict(x_train)\n\navg_pred1 = m1.predict(x_val)\navg_pred2 = m2.predict(x_val)\navg_pred3 = m3.predict(x_val)\n\ntrain_pred_avg = (avg_train_pred1+avg_train_pred2+avg_train_pred3)/3\nval_pred_avg = (avg_pred1+avg_pred2+avg_pred3)/3\n\nprint('Total time taken: ',time.time()-ts)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"print('Train rmse:', np.sqrt(mean_squared_error(y_train, train_pred_avg)))\nprint('Validation rmse:', np.sqrt(mean_squared_error(y_val, val_pred_avg)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"13\"></a> <br>\n### c. Weighted Averaging\n\n\nIn this we will first calculate RMSE for each `base model` and then we will give higher weightage to model which has least RMSE."},{"metadata":{"trusted":true},"cell_type":"code","source":"ts = time.time()\nm1.fit(x_train, y_train)\nm2.fit(x_train, y_train)\nm3.fit(x_train,y_train)\n\nwavg_train_pred1 = m1.predict(x_train)\nwavg_train_pred2 = m2.predict(x_train)\nwavg_train_pred3 = m3.predict(x_train)\n\nprint('M1_train:',np.sqrt(mean_squared_error(y_train, wavg_train_pred1)))\nprint('M2_train:',np.sqrt(mean_squared_error(y_train, wavg_train_pred2)))\nprint('M3_train:',np.sqrt(mean_squared_error(y_train, wavg_train_pred3)))\n\nwavg_pred1 = m1.predict(x_val)\nwavg_pred2 = m2.predict(x_val)\nwavg_pred3 = m3.predict(x_val)\n\nprint('\\nM1_validation:',np.sqrt(mean_squared_error(y_val, wavg_pred1)))\nprint('M2_validation:',np.sqrt(mean_squared_error(y_val, wavg_pred2)))\nprint('M3_validation:',np.sqrt(mean_squared_error(y_val, wavg_pred3)))\n\nprint('\\nTotal time taken: ',time.time()-ts)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the above values it is clear that `Decision Tree` overfits the data.\n\nFor our weighted average the weights given will be as follows `Random Forest`:0.5, `Decision Tree`:0.2, `Linear Regression`:0.3"},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"final_val_pred = 0.3 * wavg_pred1 + 0.2 * wavg_pred2 + 0.5 * wavg_pred3\nprint('Weighted Average:',np.sqrt(mean_squared_error(y_val, final_val_pred)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### From the results we can notice that `Weighted Average` performs slightly better when compared to Max Voting and Averaging\n----------------------------------------------------------------------------------------------------------------------------------------------"},{"metadata":{},"cell_type":"markdown","source":" We will use the same dataframe without further processing for 'Advanced Ensemble Learning' so I will save it to csv and use it in ** 'Part-2'**"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_set.to_csv('/kaggle/working/train_set.csv',index=False)\nvalidation_set.to_csv('/kaggle/working/validation_set.csv',index=False)\ntest_set.to_csv('/kaggle/working/test_set.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<font color=\"chocolate\" size=+2.5><b>My Other Kernels</b></font>\n\nClick on the button to view kernel...\n\n\n<a href=\"https://www.kaggle.com/nitindatta/fifa-in-depth-analysis-with-linear-regression\" class=\"btn btn-success\" style=\"color:white;\">FIFA In-Depth Analysis</a><br><br>\n\n<a href=\"https://www.kaggle.com/nitindatta/storytelling-with-gwd-pre-print-data\" class=\"btn btn-success\" style=\"color:white;\">Storytelling with GWD pre_print data</a><br><br>\n\n<a href=\"https://www.kaggle.com/nitindatta/ensemble-learning-part-1\" class=\"btn btn-success\" style=\"color:white;\">Ensemble Learning Part 1</a><br><br>\n\n<a href=\"https://www.kaggle.com/nitindatta/ensemble-learning-part-2\" class=\"btn btn-success\" style=\"color:white;\">Ensemble Learning Part 2</a><br><br>\n\n<a href=\"https://www.kaggle.com/nitindatta/students-performance-in-exams-eda-in-depth\" class=\"btn btn-success\" style=\"color:white;\">Students performance in Exams- EDA in depth ðŸ“ŠðŸ“ˆ</a><br><br>\n\n<a href=\"https://www.kaggle.com/nitindatta/pulmonary-embolism-dicom-preprocessing-eda\" class=\"btn btn-success\" style=\"color:white;\">ðŸ©ºPulmonary Embolism Dicom preprocessing & EDAðŸ©º</a><br><br>\n\n<a href=\"https://www.kaggle.com/nitindatta/first-kaggle-submission\" class=\"btn btn-success\" style=\"color:white;\">Titanic: Machine Learning from Disaster</a><br><br>\n\n<a href=\"https://www.kaggle.com/nitindatta/graduate-admission-chances\" class=\"btn btn-success\" style=\"color:white;\">ðŸ“– Graduate Admission Chances ðŸ“• ðŸ“”</a><br><br>\n\n<a href=\"https://www.kaggle.com/nitindatta/flower-classification-augmentations-eda\" class=\"btn btn-success\" style=\"color:white;\">Flower_Classification+Augmentations+EDA</a><br><br>\n\n<a href=\"https://www.kaggle.com/nitindatta/storytelling-with-gwd-pre-print-data\" class=\"btn btn-success\" style=\"color:white;\">Storytelling with GWD pre_print data</a><br><br>\n\n\n### If these kernels impress you,give them an <font size=\"+2\" color=\"red\"><b>Upvote</b></font>.<br>\n\n<a href=\"#toc\" class=\"btn btn-primary\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\" title=\"go to Colors\">Go to TOP</a>"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}