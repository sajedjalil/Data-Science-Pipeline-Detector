{"cells":[{"metadata":{},"cell_type":"markdown","source":"### Libraries Requirement Check\nThis notebook was created using Kaggle Notebook & the following libraries at the time of creation.\n* numpy 1.18.1\n* pandas 1.0.3\n* matplotlib.pylab 1.18.1\n* sklearn 0.22.2.post1\n* xgboost 1.0.2\n* seaborn 0.10.0\n* matplotlib.pylab 1.18.1\n* re 2.2.1\n\nThe following files should exist at the  directory /kaggle/input/create-model-simple-e-to-e-eda-to-ensemble/ (you should be able to access this by default)\n1. agg_data.csv\n2. rf_pred_30_10.model\n3. rf_pred_30_10.csv\n4. metamodel.model\n5. xgb_pred_100_10.csv\n6. xgb_pred_100_10.model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport tqdm.notebook\nfrom tqdm.notebook import tqdm\nimport re\nimport sklearn\nfrom sklearn.preprocessing import LabelEncoder\nfrom itertools import product\nimport time\nimport os\nimport gc\nimport pickle\nimport xgboost\nfrom xgboost import XGBRegressor\nimport matplotlib.pylab as plt\nfrom matplotlib.pylab import rcParams\nrcParams['figure.figsize'] = 12, 4\npd.set_option('display.max_columns',100)\nimport os\n\nfor p in [np, pd, plt, sklearn, xgboost,sns,plt,re]:\n    print (p.__name__, p.__version__)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 1. tldr; Summary\nThe numbers of holidays for each month are added as I hypothesize they affect sales.\n1. Thanks to [Schrodinger](https://www.kaggle.com/lonewolf45) for the excellent [notebook](https://www.kaggle.com/lonewolf45/coursera-final-project) that performs extraordinary ETL & XGB modelling. I learn a lot from this, parts of the source code there are reused and refactored to fit the author's coding style. \n\n2. The following are the original contributions by the author:\n> * EDA\n> * Augmenting Russia holiday information to the data\n> * Augmenting popular titles/keywords to the item category such as 'Call of Duty', 'Star Wars'\n> * Apply Mean encoding to vacation 'season' relative to the target\n> * Ensemble using Linear Regression","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# > Run the next cell to load preprocessed data, serialized models, and predict\nPlease expect to wait for 10 minutes for ETL, load model, and predict.\nFor recreating model from the scratch, please check this [notebook](https://www.kaggle.com/rrrrrikimaru/create-model-simple-e-to-e-eda-to-ensemble) ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport catboost\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom tqdm.notebook import tqdm\nimport re\nfrom sklearn.preprocessing import LabelEncoder\nfrom itertools import product\nimport time\nimport os\nimport gc\nimport pickle\nfrom xgboost import XGBRegressor\nimport matplotlib.pylab as plt\nfrom matplotlib.pylab import rcParams\nrcParams['figure.figsize'] = 12, 4\npd.set_option('display.max_columns',100)\nimport os\n\ndef reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() / 1024**2    \n    for col in df.columns:#tqdm(df.columns):\n        col_type = df[col].dtypes\n\n        if col_type=='object':\n            df[col] = df[col].astype('category')\n\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() / 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n    return df\n\ndef display_status(message):\n    from IPython.display import display, clear_output\n    import time\n    display(message) # print string\n    clear_output(wait=True)\n    \n\nprint('Available datasets:')\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\nstart = time.time()\ndisplay_status('Load aggregated data (1/3)')\nagg_data = pd.read_csv('/kaggle/input/create-model-simple-e-to-e-eda-to-ensemble/agg_data.csv')\nagg_data = reduce_mem_usage(agg_data)\n\n\nX_test = agg_data[agg_data.date_block_num == 34].drop(['item_cnt_month'], axis=1)\nX_test.fillna(0,inplace=True)\n\ndel agg_data\ngc.collect()\n\nbasepath= '../input/competitive-data-science-predict-future-sales/'\ntest = pd.read_csv( basepath+\"test.csv\" )\n\ndisplay_status('Load serialized models (2/3) --- Time elapsed:{0} secs'.format(time.time()-start))\nrf = pickle.load(open('/kaggle/input/create-model-simple-e-to-e-eda-to-ensemble/rf_pred_30_10.model', 'rb'))\nxgb = pickle.load(open('/kaggle/input/create-model-simple-e-to-e-eda-to-ensemble/xgb_pred_100_10.model', 'rb'))\nmeta_model = pickle.load(open('/kaggle/input/create-model-simple-e-to-e-eda-to-ensemble/metamodel.model', 'rb'))\n\ndisplay_status('Final prediction (3/3) --- Time elapsed:{0} secs'.format(time.time()-start))\nmeta_test = pd.DataFrame({'ID':test.index,'pred_1':rf.predict(X_test),'pred_2':xgb.predict(X_test)})\nX_meta_test = meta_test[['pred_1','pred_2']]\n\nsubmission = pd.DataFrame({\n    \"ID\": test.index, \n    \"item_cnt_month\": meta_model.predict(X_meta_test).clip(0, 20)\n})\nsubmission.to_csv('meta_model_pred.csv', index=False)\nsubmission.to_csv('submission.csv', index=False)\nprint('Final prediction submission.csv file has been generated. Thanks for your patience')\n\ndel rf, xgb, meta_model, meta_test, X_meta_test, submission, X_test\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2. Exploratory Data Analysis","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Load Data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport catboost\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom tqdm.notebook import tqdm\nimport re\nfrom sklearn.preprocessing import LabelEncoder\nfrom itertools import product\nimport time\nimport os\nimport gc\nimport pickle\nfrom xgboost import XGBRegressor\nimport matplotlib.pylab as plt\nfrom matplotlib.pylab import rcParams\nrcParams['figure.figsize'] = 12, 4\npd.set_option('display.max_columns',100)\nimport os\n\n\nbasepath= '../input/competitive-data-science-predict-future-sales/'\n\nitems = pd.read_csv(basepath+\"items.csv\")\nitem_categories = pd.read_csv(basepath+\"item_categories.csv\")\nshops = pd.read_csv(basepath+\"shops.csv\")\ntrain = pd.read_csv( basepath+\"sales_train.csv\" )\n#train = train.sample(20000) #for quick Proof of Concept, sample 20000\ntest = pd.read_csv( basepath+\"test.csv\" )\nprint('All data has been loaded.')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The data mostly consists of PC, Entertainment, Game related. It can be suspected that the sales will be seasonal based on popularity and holidays. Here are my 3 hypotheses:\n1. Holidays affect sales\n2. Popular titles (ex.: games) are bestsellers & deserve their own categories\n3. Data leaks are unlikely for test data for the following reasons: (a) there are some debut items \n, (b) the target variables to guess by LB probing are massive (between -1 to clipped 20)\n, (c) shops may sell 'classical' items (items that has long been not sold, suddenly appears back in certain period of time)\n\n### First suspect: holidays affect sales","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"agg_data = pd.read_csv('/kaggle/input/create-model-simple-e-to-e-eda-to-ensemble/agg_data.csv')\ncols = ['holiday_num','item_cnt_month','next_month_holiday_num','vacation_period']\n\nagg_data[cols].groupby([\"holiday_num\"], as_index=False \n                      ).agg({\"item_cnt_month\": [\"sum\"] }\n                           ).plot(kind='bar',x='holiday_num',y='item_cnt_month',\n                                  title='Current Month Holidays __VS__ Sales', subplots=True, rot=90)\n\nagg_data[cols].groupby([\"next_month_holiday_num\"] , as_index=False \n                      ).agg({\"item_cnt_month\": [\"sum\"] }\n                           ).plot(kind='bar',x='next_month_holiday_num',y='item_cnt_month',\n                                  title='Anticipation of num holidays next month __VS__ Sales', subplots=True, rot=90)\n\n# Remapped to string since the original agg_data was already mean-encoded\ndays = pd.Series(['winter','etc','spring','etc','may',  'summer','summer','summer','etc','winter','winter','winter'])\nagg_data[\"vacation_period\"] = agg_data[\"month\"].map(days).astype('category')\nagg_data[cols].groupby([\"vacation_period\"] , as_index=False \n                      ).agg({\"item_cnt_month\": [\"sum\"] }\n                           ).plot(kind='bar',x='vacation_period',y='item_cnt_month',\n                                  title='Vacation period __VS__ Sales. May is special and deserves its own category', subplots=True, rot=90)\n\ndel agg_data, days\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Popular games are bestsellers","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"alldata = None\nalldata = train.merge(items, how='inner', on='item_id' )\nalldata= alldata.merge(item_categories, how='inner', on='item_category_id' )\nalldata = alldata.merge(shops,how='inner', on='shop_id')\nalldata['year'] = pd.to_datetime(alldata['date']).dt.year\nalldata['month'] = pd.to_datetime(alldata['date']).dt.month\nalldata['year_month'] = alldata['year'].astype(str) + '-' + alldata['month'].astype(str)\npopular_game_oct2015 = alldata.groupby(['year_month','item_category_name','item_name'],as_index=False).sum()\ncols=['date',\t'date_block_num',\t'item_name', \t'item_cnt_day', 'shop_id']\n\ntop_100 = popular_game_oct2015[popular_game_oct2015['year_month']=='2015-10'].sort_values(by=['item_cnt_day'], ascending=False).iloc[:100,:]\ntop_100[['item_name','item_cnt_day']].head(15).plot.bar(x='item_name', rot=-90)\n\n\n#free memory\ndel alldata,popular_game_oct2015,top_100\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"as shown above, given just a month away from Nov 2015(the period to predict), the top-15 sales have repeated popular game products such as Assassin's Creed, Uncharted, GTA V contributing to the sales. Capitalizing this on facts, some keywords from the item_name will be used as category.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Thought on Data Leaks\nLet us profile the numbers of unique data and plot pairs of (item_id, plot_id)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let sample some of the data and plot its kernel distribution between ('shop_id','item_id') pair\nprint('Train unique:\\n{0}    \\n\\nTest unique:\\n{1}'.format( train.nunique(), test.nunique()))\ntrain_g = sns.jointplot(x=\"shop_id\", y=\"item_id\", data=train[['shop_id','item_id']].sample(10000), kind=\"kde\")\nplt.subplots_adjust(top=0.9)\ntrain_g.fig.suptitle('Train Data KDE(sampled randomly)')\ntest_g = sns.jointplot(x=\"shop_id\", y=\"item_id\", data=test[['shop_id','item_id']].sample(10000), kind=\"kde\")\n\nplt.subplots_adjust(top=0.9)\ntest_g.fig.suptitle('Test Data KDE(sampled randomly)')\n\ndel test_g, train_g\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The distributions of item_id and shop_id vary and it is inconclusive that data leakage exists, however since target value is not binary value of 0/1, the possibility target values (from -1 to upper-clipped 20) to guess based on LB probing(submit & check score up/down) are massive. I decided to proceed to the ETL instead. In addition there are some debut items (item_id) in the test set as shown below:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"num_debut = len(set(test['item_id']) - set(train['item_id']))\ntotal_test = len(set(test['item_id']))\npctg = (num_debut/total_test)*100\nprint ('num of debut items: {0} ({1}%) out of {2} items'.format(num_debut,pctg,total_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3. How the Data was Transformed and ETL-ed\n* The data will be group and merged by these 5 columns and some of their combination: \"date_block_num\", \"shop_id\",\"item_id\",'subtype_code','shop_city'. This is to extract their local, group, and global aggregated operation results.\n* The data was also transformed using scikit transformer. This will save memory since the data will be mostly stored in int format (The author followed the way of the original source code mentioned above).","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# 4. Hyperparameter Optimization\nThe optimum number of trees for Random Forest was approximated below","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Hyperparameter\ngc.collect()\nagg_data = pd.read_csv('/kaggle/input/create-model-simple-e-to-e-eda-to-ensemble/agg_data.csv')\n\nX_train = agg_data[agg_data.date_block_num < 33].drop(['item_cnt_month'], axis=1)\ny_train = agg_data[agg_data.date_block_num < 33]['item_cnt_month']\nX_val = agg_data[agg_data.date_block_num == 33].drop(['item_cnt_month'], axis=1)\ny_val = agg_data[agg_data.date_block_num == 33]['item_cnt_month']\nX_test = agg_data[agg_data.date_block_num == 34].drop(['item_cnt_month'], axis=1)\n\nX_train.fillna(0,inplace=True)\nX_val.fillna(0,inplace=True)\nX_test.fillna(0,inplace=True)\n\nrf_filename = 'rf_pred_30_10.model'\nrf = pickle.load(open('/kaggle/input/create-model-simple-e-to-e-eda-to-ensemble/'+rf_filename, 'rb'))\n\ndel agg_data\ngc.collect()\n\n\npredictions = []\nfor tree in rf.estimators_:\n    predictions.append(tree.predict(X_val)[None, :])\n\nfrom sklearn.metrics import mean_squared_error\npredictions = np.vstack(predictions)\nscores = []\n\nacc = []\ntemp = [0] * len(predictions[0])\nfor i in range(len(predictions)):\n    temp += predictions[i] \n    acc.append(temp/(i+1))\n\nscores=[]\nfor pred in acc:\n    scores.append(np.sqrt(mean_squared_error(y_val, pred)))\n\nscores\nplt.figure(figsize=(10, 6))\nplt.plot(scores, linewidth=3)\nplt.xlabel('num_trees')\nplt.ylabel('RMSE')\n\ndel scores,predictions, rf\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In terms of RMSE, we can see above that the num_trees round 10 trees was quite optimum.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# 5. Feature Importances","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_feature_imp(model,model_name):\n    global X_train\n    df = pd.DataFrame({'feature':X_train.columns,\n                       'feature_importances_':model.feature_importances_}).sort_values(by='feature_importances_', ascending=False)\n    df.plot(x='feature',kind='bar',title='Feature importance for '+model_name)\n\n\nxgb = pickle.load(open('/kaggle/input/create-model-simple-e-to-e-eda-to-ensemble/xgb_pred_100_10.model', 'rb'))\nplot_feature_imp(xgb,'XGBoost')\n\nrf = pickle.load(open('/kaggle/input/create-model-simple-e-to-e-eda-to-ensemble/rf_pred_30_10.model', 'rb'))\nplot_feature_imp(rf,'Random Forest')\n\ndel xgb, rf\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Both plots above of shows different importances between XGB and RF. Although both are still Tree based methods, both are ensembled anyway using Linear Regression. (please refer to the[ source code](https://www.kaggle.com/rrrrrikimaru/create-model-simple-e-to-e-eda-to-ensemble) for the details).","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Recreate models and predict from Scratch\n# > Please check this link >> [notebook](https://www.kaggle.com/rrrrrikimaru/create-model-simple-e-to-e-eda-to-ensemble)  <<\n\n### Thank you for reading till the end.\nAny feedback is very welcomed.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}