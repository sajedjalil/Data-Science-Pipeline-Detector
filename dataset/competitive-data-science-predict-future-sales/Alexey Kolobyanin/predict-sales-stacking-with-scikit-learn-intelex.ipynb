{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<big>For classical machine learning algorithms, we often use the most popular Python library, Scikit-learn. With Scikit-learn you can fit models and search for optimal parameters, but it sometimes works for hours.</big><br><br>\n\n<big>I want to show you how to use Scikit-learn library and get the results faster without changing the code. To do this, we will make use of another Python library, <strong> <a href='https://github.com/intel/scikit-learn-intelex'>Intel® Extension for Scikit-learn*</a></strong>.</big><br><br>\n\n<big>I will show you how to <strong>speed up your kernel more than 2 times</strong> without changing your code!</big><big>","metadata":{}},{"cell_type":"code","source":"import pandas as pd \nimport numpy as np\nimport matplotlib.pyplot as plt","metadata":{"execution":{"iopub.status.busy":"2021-08-20T13:45:38.79118Z","iopub.execute_input":"2021-08-20T13:45:38.791765Z","iopub.status.idle":"2021-08-20T13:45:38.802324Z","shell.execute_reply.started":"2021-08-20T13:45:38.791673Z","shell.execute_reply":"2021-08-20T13:45:38.801514Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Importing data","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv('../input/competitive-data-science-predict-future-sales/sales_train.csv')\nitems = pd.read_csv('../input/competitive-data-science-predict-future-sales/items.csv')\ncategories = pd.read_csv('../input/competitive-data-science-predict-future-sales/item_categories.csv')\nshops = pd.read_csv('../input/competitive-data-science-predict-future-sales/shops.csv')\ntest = pd.read_csv('../input/competitive-data-science-predict-future-sales/test.csv')\nsubmission = pd.read_csv('../input/competitive-data-science-predict-future-sales/sample_submission.csv')","metadata":{"execution":{"iopub.status.busy":"2021-08-20T13:45:41.163559Z","iopub.execute_input":"2021-08-20T13:45:41.164096Z","iopub.status.idle":"2021-08-20T13:45:44.172894Z","shell.execute_reply.started":"2021-08-20T13:45:41.164061Z","shell.execute_reply":"2021-08-20T13:45:44.17197Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Preprocessing","metadata":{}},{"cell_type":"markdown","source":"<big>I took preprocessing from  <a href='https://www.kaggle.com/gordotron85/future-sales-xgboost-top-3'>here</a> and <a href='https://www.kaggle.com/sarthakbatra/predicting-sales-tutorial'>here</a> </big><br><br> \n<big>The main steps:</big><br><br>\n<ol>\n<li><big>Cleaning \"shops\": fix store identifiers, adding categories and cities and convert it to a numeric attribute.</big></li><br>\n<li><big>Cleaning \"categories\": select the category and subcategory of the product and convert it to a numeric attribute.</big></li><br>\n<li><big>Cleaning \"items\": clean item names and types.</big></li><br>\n<li><big>Generating prodcuct of Shop-Item pairs for each month in the training data.</big></li><br>\n<li><big>Merge shops, items and categories dataframes with new_train set.</big></li><br>\n<li><big>Generating Lag Features and Mean-Encodings.</big></li><br>\n</ol>","metadata":{}},{"cell_type":"code","source":"# Delete outliers.\n\ntrain = train[train['item_cnt_day'] < 2000]\ntrain = train[train['item_price'] < 300000]\n\n# Delete negative item price values.\n\ntrain = train[train.item_price > 0].reset_index(drop = True)\ntrain.loc[train.item_cnt_day < 1, \"item_cnt_day\"] = 0\n\n# CLEANING SHOPS\n\nshops_train = train['shop_id'].nunique()\nshops_test = test['shop_id'].nunique()\n\n\n\n# Some stores with the same name have different ID. We need to fix this.\ntrain.loc[train['shop_id'] == 0, 'shop_id'] = 57\ntest.loc[test['shop_id'] == 0, 'shop_id'] = 57\n\ntrain.loc[train['shop_id'] == 1, 'shop_id'] = 58\ntest.loc[test['shop_id'] == 1, 'shop_id'] = 58\n\ntrain.loc[train['shop_id'] == 10, 'shop_id'] = 11\ntest.loc[test['shop_id'] == 10, 'shop_id'] = 11\n\n# Add a city and a shop category.\nshops.loc[ shops.shop_name == 'Сергиев Посад ТЦ \"7Я\"',\"shop_name\" ] = 'СергиевПосад ТЦ \"7Я\"'\nshops[\"city\"] = shops.shop_name.str.split(\" \").map( lambda x: x[0] )\nshops[\"category\"] = shops.shop_name.str.split(\" \").map( lambda x: x[1] )\nshops.loc[shops.city == \"!Якутск\", \"city\"] = \"Якутск\"\n\n# If there are less than 5 stores in one category, we will make them the category \"other\".\ncategory = []\nfor cat in shops.category.unique():\n    if len(shops[shops.category == cat]) >= 5:\n        category.append(cat)\nshops.category = shops.category.apply( lambda x: x if (x in category) else \"other\" )\n\n\n# Let's transform the category and city of the store into a numeric attribute.\nfrom sklearn import preprocessing\nle = preprocessing.LabelEncoder()\n\nshops[\"shop_category\"] = le.fit_transform(shops.category)\nshops[\"shop_city\"] = le.fit_transform(shops.city)\nshops = shops[[\"shop_id\", \"shop_category\", \"shop_city\"]]\n\n\n# CLEANING CATEGORIES\nitems_train = train['item_id'].nunique()\nitems_test = test['item_id'].nunique()\n\n\n# Select the category and subcategory of the product and convert it to a numeric attribute.\nmain_categories = categories['item_category_name'].str.split('-')\ncategories['main_category_id'] = main_categories.map(lambda row: row[0].strip())\ncategories['main_category_id'] = le.fit_transform(categories['main_category_id'])\n\n# Some items don't have sub-categories. For those, we will use the main category as a sub-category\ncategories['sub_category_id'] = main_categories.map(lambda row: row[1].strip() if len(row) > 1 else row[0].strip())\ncategories['sub_category_id'] = le.fit_transform(categories['sub_category_id'])\n\n\n# CLEANING ITEMS\nimport re\ndef name_correction(x):\n    x = x.lower() # all letters lower case\n    x = x.partition('[')[0] # partition by square brackets\n    x = x.partition('(')[0] # partition by curly brackets\n    x = re.sub('[^A-Za-z0-9А-Яа-я]+', ' ', x) # remove special characters\n    x = x.replace('  ', ' ') # replace double spaces with single spaces\n    x = x.strip() # remove leading and trailing white space\n    return x\n\n\n# Clean item names\n# split item names by first bracket\nitems[\"name1\"], items[\"name2\"] = items.item_name.str.split(\"[\", 1).str\nitems[\"name1\"], items[\"name3\"] = items.item_name.str.split(\"(\", 1).str\n\n# replace special characters and turn to lower case\nitems[\"name2\"] = items.name2.str.replace('[^A-Za-z0-9А-Яа-я]+', \" \").str.lower()\nitems[\"name3\"] = items.name3.str.replace('[^A-Za-z0-9А-Яа-я]+', \" \").str.lower()\n\n# fill nulls with '0'\nitems = items.fillna('0')\n\nitems[\"item_name\"] = items[\"item_name\"].apply(lambda x: name_correction(x))\n\n# return all characters except the last if name 2 is not \"0\" - the closing bracket\nitems.name2 = items.name2.apply( lambda x: x[:-1] if x !=\"0\" else \"0\")\n\n\n\n# Clean item type\nitems[\"type\"] = items.name2.apply(lambda x: x[0:8] if x.split(\" \")[0] == \"xbox\" else x.split(\" \")[0] )\nitems.loc[(items.type == \"x360\") | (items.type == \"xbox360\") | (items.type == \"xbox 360\") ,\"type\"] = \"xbox 360\"\nitems.loc[ items.type == \"\", \"type\"] = \"mac\"\nitems.type = items.type.apply( lambda x: x.replace(\" \", \"\") )\nitems.loc[ (items.type == 'pc' )| (items.type == 'pс') | (items.type == \"pc\"), \"type\" ] = \"pc\"\nitems.loc[ items.type == 'рs3' , \"type\"] = \"ps3\"\n\ngroup_sum = items.groupby([\"type\"]).agg({\"item_id\": \"count\"})\ngroup_sum = group_sum.reset_index()\ndrop_cols = []\nfor cat in group_sum.type.unique():\n    if group_sum.loc[(group_sum.type == cat), \"item_id\"].values[0] <40:\n        drop_cols.append(cat)\nitems.name2 = items.name2.apply( lambda x: \"other\" if (x in drop_cols) else x )\nitems = items.drop([\"type\"], axis = 1)\n\nitems.name2 = le.fit_transform(items.name2)\nitems.name3 = le.fit_transform(items.name3)\n\nitems.drop([\"item_name\", \"name1\"],axis = 1, inplace= True)\nitems.head()\n\n\n# Convert the date to \"datetime\" format.\ntrain['date'] =  pd.to_datetime(train['date'], format='%d.%m.%Y')\n\n\n\nfrom itertools import product\nfrom tqdm import tqdm_notebook\n\ndef downcast_dtypes(df):\n    '''\n        Changes column types in the dataframe: \n                \n                `float64` type to `float32`\n                `int64`   type to `int32`\n    '''\n    \n    # Select columns to downcast\n    float_cols = [c for c in df if df[c].dtype == \"float64\"]\n    int_cols =   [c for c in df if df[c].dtype == \"int64\"]\n    \n    # Downcast\n    df[float_cols] = df[float_cols].astype(np.float32)\n    df[int_cols]   = df[int_cols].astype(np.int32)\n    \n    return df\n\n\n\n\n# Generating prodcuct of Shop-Item pairs for each month in the training data\nmonths = train['date_block_num'].unique()\n\ncartesian = []\nfor month in months:\n    shops_in_month = train.loc[train['date_block_num']==month, 'shop_id'].unique()\n    items_in_month = train.loc[train['date_block_num']==month, 'item_id'].unique()\n    cartesian.append(np.array(list(product(*[shops_in_month, items_in_month, [month]])), dtype='int32'))\n\ncartesian_df = pd.DataFrame(np.vstack(cartesian), columns = ['shop_id', 'item_id', 'date_block_num'], dtype=np.int32)\n\n\n# Add revenue to the train dataset.\ntrain[\"revenue\"] = train[\"item_cnt_day\"] * train[\"item_price\"]\n\n\n# Aggregating sales to a monthly level and clipping target variable\nx = train.groupby(['shop_id', 'item_id', 'date_block_num'])['item_cnt_day'].sum().rename('item_cnt_month').reset_index()\nx.head()\n\n\n# Now we need to merge our two dataframes.\nnew_train = pd.merge(cartesian_df, x, on=['shop_id', 'item_id', 'date_block_num'], how='left').fillna(0)\n\nnew_train.head()\n\n\n# Now we need to merge our two dataframes. For the intersecting, we will simply put the values that exist in the dataframe x. \n# For the remaining rows, we will sub in zero. Remember, the columns you want to merge on are the intersection of shop_id, item_id, and date_block_num\nnew_train['item_cnt_month'] = np.clip(new_train['item_cnt_month'], 0, 20)\n\n\ndel x\ndel cartesian_df\ndel cartesian\n\n\nnew_train.sort_values(['date_block_num','shop_id','item_id'], inplace = True)\n\n\n# APPENDING TEST SET TO TRAINING SET\n\n# First, let's insert the date_block_num feature for the test set! Using insert method of pandas to place this new column at a specific index. \n# This will allow us to concatenate the test set easily to the training set before we generate mean encodings and lag features.\n\ntest.insert(loc=3, column='date_block_num', value=34)\n\ntest['item_cnt_month'] = 0\n\n\nnew_train = new_train.append(test.drop('ID', axis = 1))\n\n# Merge shops, items and categories dataframes with new_train\n\nnew_train = pd.merge(new_train, shops, on=['shop_id'], how='left')\n\nnew_train = pd.merge(new_train, items, on=['item_id'], how='left')\n\nnew_train = pd.merge(new_train, categories.drop('item_category_name', axis = 1), on=['item_category_id'], how='left')\n\n\n# Generating Lag Features and Mean-Encodings\ndef lag_feature( df,lags, cols ):\n    for col in cols:\n        tmp = df[[\"date_block_num\", \"shop_id\",\"item_id\",col ]]\n        for i in lags:\n            shifted = tmp.copy()\n            shifted.columns = [\"date_block_num\", \"shop_id\", \"item_id\", col + \"_lag_\"+str(i)]\n            shifted.date_block_num = shifted.date_block_num + i\n            df = pd.merge(df, shifted, on=['date_block_num','shop_id','item_id'], how='left')\n    return df\n\n\ndel items\ndel categories\ndel shops\n\n\nnew_train = downcast_dtypes(new_train)\n\nimport gc\ngc.collect()\n\n\n# Add item_cnt_month lag features.\nnew_train = lag_feature( new_train, [1,2,3], [\"item_cnt_month\"] )\n\n\n# Add the previous month's average item_cnt.\ngroup = new_train.groupby( [\"date_block_num\"] ).agg({\"item_cnt_month\" : [\"mean\"]})\ngroup.columns = [\"date_avg_item_cnt\"]\ngroup.reset_index(inplace = True)\n\nnew_train = pd.merge(new_train, group, on = [\"date_block_num\"], how = \"left\")\nnew_train.date_avg_item_cnt = new_train[\"date_avg_item_cnt\"].astype(np.float16)\nnew_train = lag_feature( new_train, [1], [\"date_avg_item_cnt\"] )\nnew_train.drop( [\"date_avg_item_cnt\"], axis = 1, inplace = True )\n\n\n# Add lag values of item_cnt_month for month / item_id.\ngroup = new_train.groupby(['date_block_num', 'item_id']).agg({'item_cnt_month': ['mean']})\ngroup.columns = [ 'date_item_avg_item_cnt' ]\ngroup.reset_index(inplace=True)\n\nnew_train = pd.merge(new_train, group, on=['date_block_num','item_id'], how='left')\nnew_train.date_item_avg_item_cnt = new_train['date_item_avg_item_cnt'].astype(np.float16)\nnew_train = lag_feature(new_train, [1,2,3], ['date_item_avg_item_cnt'])\nnew_train.drop(['date_item_avg_item_cnt'], axis=1, inplace=True)\n\n\n# Add lag values for item_cnt_month for every month / shop combination.\ngroup = new_train.groupby( [\"date_block_num\",\"shop_id\"] ).agg({\"item_cnt_month\" : [\"mean\"]})\ngroup.columns = [\"date_shop_avg_item_cnt\"]\ngroup.reset_index(inplace = True)\n\nnew_train = pd.merge(new_train, group, on = [\"date_block_num\",\"shop_id\"], how = \"left\")\nnew_train.date_avg_item_cnt = new_train[\"date_shop_avg_item_cnt\"].astype(np.float16)\nnew_train = lag_feature( new_train, [1,2,3], [\"date_shop_avg_item_cnt\"] )\nnew_train.drop( [\"date_shop_avg_item_cnt\"], axis = 1, inplace = True )\n\n\n# Add lag values for item_cnt_month for month/shop/item.\ngroup = new_train.groupby( [\"date_block_num\",\"shop_id\",\"item_id\"] ).agg({\"item_cnt_month\" : [\"mean\"]})\ngroup.columns = [\"date_shop_item_avg_item_cnt\"]\ngroup.reset_index(inplace = True)\n\nnew_train = pd.merge(new_train, group, on = [\"date_block_num\",\"shop_id\",\"item_id\"], how = \"left\")\nnew_train.date_avg_item_cnt = new_train[\"date_shop_item_avg_item_cnt\"].astype(np.float16)\nnew_train = lag_feature( new_train, [1,2,3], [\"date_shop_item_avg_item_cnt\"] )\nnew_train.drop( [\"date_shop_item_avg_item_cnt\"], axis = 1, inplace = True )\n\n\n# Add lag values for item_cnt_month for month/shop/item subtype.\ngroup = new_train.groupby(['date_block_num', 'shop_id', 'sub_category_id']).agg({'item_cnt_month': ['mean']})\ngroup.columns = ['date_shop_subtype_avg_item_cnt']\ngroup.reset_index(inplace=True)\n\nnew_train = pd.merge(new_train, group, on=['date_block_num', 'shop_id', 'sub_category_id'], how='left')\nnew_train.date_shop_subtype_avg_item_cnt = new_train['date_shop_subtype_avg_item_cnt'].astype(np.float16)\nnew_train = lag_feature(new_train, [1], ['date_shop_subtype_avg_item_cnt'])\nnew_train.drop(['date_shop_subtype_avg_item_cnt'], axis=1, inplace=True)\n\n\n# Add lag values for item_cnt_month for month/city\ngroup = new_train.groupby(['date_block_num', 'shop_city']).agg({'item_cnt_month': ['mean']})\ngroup.columns = ['date_city_avg_item_cnt']\ngroup.reset_index(inplace=True)\n\nnew_train = pd.merge(new_train, group, on=['date_block_num', \"shop_city\"], how='left')\nnew_train.date_city_avg_item_cnt = new_train['date_city_avg_item_cnt'].astype(np.float16)\nnew_train = lag_feature(new_train, [1], ['date_city_avg_item_cnt'])\nnew_train.drop(['date_city_avg_item_cnt'], axis=1, inplace=True)\n\n# Add lag values for item_cnt_month for month/city/item.\ngroup = new_train.groupby(['date_block_num', 'item_id', 'shop_city']).agg({'item_cnt_month': ['mean']})\ngroup.columns = [ 'date_item_city_avg_item_cnt' ]\ngroup.reset_index(inplace=True)\n\nnew_train = pd.merge(new_train, group, on=['date_block_num', 'item_id', 'shop_city'], how='left')\nnew_train.date_item_city_avg_item_cnt = new_train['date_item_city_avg_item_cnt'].astype(np.float16)\nnew_train = lag_feature(new_train, [1], ['date_item_city_avg_item_cnt'])\nnew_train.drop(['date_item_city_avg_item_cnt'], axis=1, inplace=True)\n\n# Add average item price on to matix df.\n# Add lag values of item price per month.\n# Add delta price values - how current month average pirce relates to global average.\n\ngroup = train.groupby( [\"item_id\"] ).agg({\"item_price\": [\"mean\"]})\ngroup.columns = [\"item_avg_item_price\"]\ngroup.reset_index(inplace = True)\n\nnew_train = new_train.merge( group, on = [\"item_id\"], how = \"left\" )\nnew_train[\"item_avg_item_price\"] = new_train.item_avg_item_price.astype(np.float16)\n\n\ngroup = train.groupby( [\"date_block_num\",\"item_id\"] ).agg( {\"item_price\": [\"mean\"]} )\ngroup.columns = [\"date_item_avg_item_price\"]\ngroup.reset_index(inplace = True)\n\nnew_train = new_train.merge(group, on = [\"date_block_num\",\"item_id\"], how = \"left\")\nnew_train[\"date_item_avg_item_price\"] = new_train.date_item_avg_item_price.astype(np.float16)\nlags = [1, 2, 3]\nnew_train = lag_feature( new_train, lags, [\"date_item_avg_item_price\"] )\nfor i in lags:\n    new_train[\"delta_price_lag_\" + str(i) ] = (new_train[\"date_item_avg_item_price_lag_\" + str(i)]- new_train[\"item_avg_item_price\"] )/ new_train[\"item_avg_item_price\"]\n\ndef select_trends(row) :\n    for i in lags:\n        if row[\"delta_price_lag_\" + str(i)]:\n            return row[\"delta_price_lag_\" + str(i)]\n    return 0\n\nnew_train[\"delta_price_lag\"] = new_train.apply(select_trends, axis = 1)\nnew_train[\"delta_price_lag\"] = new_train.delta_price_lag.astype( np.float16 )\nnew_train[\"delta_price_lag\"].fillna( 0 ,inplace = True)\n\nfeatures_to_drop = [\"item_avg_item_price\", \"date_item_avg_item_price\"]\nfor i in lags:\n    features_to_drop.append(\"date_item_avg_item_price_lag_\" + str(i) )\n    features_to_drop.append(\"delta_price_lag_\" + str(i) )\nnew_train.drop(features_to_drop, axis = 1, inplace = True)\n\n# Add total shop revenue per month to matix df.\n# Add lag values of revenue per month.\n# Add delta revenue values - how current month revenue relates to global average.\n\ngroup = train.groupby( [\"date_block_num\",\"shop_id\"] ).agg({\"revenue\": [\"sum\"] })\ngroup.columns = [\"date_shop_revenue\"]\ngroup.reset_index(inplace = True)\n\nnew_train = new_train.merge( group , on = [\"date_block_num\", \"shop_id\"], how = \"left\" )\nnew_train['date_shop_revenue'] = new_train['date_shop_revenue'].astype(np.float32)\n\ngroup = group.groupby([\"shop_id\"]).agg({ \"date_block_num\":[\"mean\"] })\ngroup.columns = [\"shop_avg_revenue\"]\ngroup.reset_index(inplace = True )\n\nnew_train = new_train.merge( group, on = [\"shop_id\"], how = \"left\" )\nnew_train[\"shop_avg_revenue\"] = new_train.shop_avg_revenue.astype(np.float32)\nnew_train[\"delta_revenue\"] = (new_train['date_shop_revenue'] - new_train['shop_avg_revenue']) / new_train['shop_avg_revenue']\nnew_train[\"delta_revenue\"] = new_train[\"delta_revenue\"]. astype(np.float32)\n\nnew_train = lag_feature(new_train, [1], [\"delta_revenue\"])\nnew_train[\"delta_revenue_lag_1\"] = new_train[\"delta_revenue_lag_1\"].astype(np.float32)\nnew_train.drop( [\"date_shop_revenue\", \"shop_avg_revenue\", \"delta_revenue\"] ,axis = 1, inplace = True)\n\n# Add month and number of days in each month to matrix df.\n\nnew_train[\"month\"] = new_train[\"date_block_num\"] % 12\ndays = pd.Series([31,28,31,30,31,30,31,31,30,31,30,31])\nnew_train[\"days\"] = new_train[\"month\"].map(days).astype(np.int8)\n\n# Add holidays in dataset.\nholiday_dict = {\n    0: 6,\n    1: 3,\n    2: 2,\n    3: 8,\n    4: 3,\n    5: 3,\n    6: 2,\n    7: 8,\n    8: 4,\n    9: 8,\n    10: 5,\n    11: 4,\n}\n\nnew_train['holidays_in_month'] = new_train['month'].map(holiday_dict)\n\n\n# Add the month of each shop and item first sale.\nnew_train[\"item_shop_first_sale\"] = new_train[\"date_block_num\"] - new_train.groupby([\"item_id\",\"shop_id\"])[\"date_block_num\"].transform('min')\nnew_train[\"item_first_sale\"] = new_train[\"date_block_num\"] - new_train.groupby([\"item_id\"])[\"date_block_num\"].transform('min')\n\n# Delete first three months from matrix. They don't have lag values.\n\nnew_train = new_train[new_train[\"date_block_num\"] > 3]\n\nnew_train.head()\n\ndef fill_na(df):\n    for col in df.columns:\n        if ('_lag_' in col) & (df[col].isnull().any()):\n            df[col].fillna(0, inplace=True)         \n    return df\n\nnew_train = downcast_dtypes(new_train)","metadata":{"_kg_hide-output":true,"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-08-19T12:12:14.643161Z","iopub.execute_input":"2021-08-19T12:12:14.643774Z","iopub.status.idle":"2021-08-19T12:21:15.49856Z","shell.execute_reply.started":"2021-08-19T12:12:14.643721Z","shell.execute_reply":"2021-08-19T12:21:15.497469Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_train = pd.read_csv('../input/new-train/new_train.csv', index_col='Unnamed: 0')","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-08-20T13:45:44.174557Z","iopub.execute_input":"2021-08-20T13:45:44.175042Z","iopub.status.idle":"2021-08-20T13:46:30.600456Z","shell.execute_reply.started":"2021-08-20T13:45:44.174994Z","shell.execute_reply":"2021-08-20T13:46:30.599341Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Split data to a train and test sets","metadata":{}},{"cell_type":"code","source":"x_train = new_train[new_train.date_block_num < 34].drop(['item_cnt_month'], axis=1)\ny_train = new_train[new_train.date_block_num < 34]['item_cnt_month']\n\nx_val = new_train[new_train.date_block_num == 33].drop(['item_cnt_month'], axis=1)\ny_val = new_train[new_train.date_block_num == 33]['item_cnt_month']\n\nx_test = new_train[new_train.date_block_num == 34].drop(['item_cnt_month'], axis=1)","metadata":{"_kg_hide-input":false,"execution":{"iopub.status.busy":"2021-08-20T13:46:30.602512Z","iopub.execute_input":"2021-08-20T13:46:30.602878Z","iopub.status.idle":"2021-08-20T13:46:34.251312Z","shell.execute_reply.started":"2021-08-20T13:46:30.60284Z","shell.execute_reply":"2021-08-20T13:46:34.250243Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<big>Normalize data.</big>","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler, MinMaxScaler\nscaler_x = MinMaxScaler()\nscaler_y = StandardScaler()","metadata":{"execution":{"iopub.status.busy":"2021-08-20T13:46:34.253244Z","iopub.execute_input":"2021-08-20T13:46:34.25366Z","iopub.status.idle":"2021-08-20T13:46:35.058985Z","shell.execute_reply.started":"2021-08-20T13:46:34.253624Z","shell.execute_reply":"2021-08-20T13:46:35.057964Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scaler_x.fit(x_train)\nx_train = scaler_x.transform(x_train)\nx_val = scaler_x.transform(x_val)\nx_test = scaler_x.transform(x_test)","metadata":{"execution":{"iopub.status.busy":"2021-08-20T13:46:35.060608Z","iopub.execute_input":"2021-08-20T13:46:35.061101Z","iopub.status.idle":"2021-08-20T13:46:39.817245Z","shell.execute_reply.started":"2021-08-20T13:46:35.061022Z","shell.execute_reply":"2021-08-20T13:46:39.816126Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scaler_y.fit(y_train.to_numpy().reshape(-1, 1))\ny_train = scaler_y.transform(y_train.to_numpy().reshape(-1, 1)).ravel()\ny_val = scaler_y.transform(y_val.to_numpy().reshape(-1, 1)).ravel()","metadata":{"execution":{"iopub.status.busy":"2021-08-20T13:46:39.818451Z","iopub.execute_input":"2021-08-20T13:46:39.818732Z","iopub.status.idle":"2021-08-20T13:46:39.995462Z","shell.execute_reply.started":"2021-08-20T13:46:39.818706Z","shell.execute_reply":"2021-08-20T13:46:39.994359Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Installing Intel(R) Extension for Scikit-learn\n\n<big>Use Intel® Extension for Scikit-learn* for fast compute Scikit-learn estimators.</big>","metadata":{}},{"cell_type":"code","source":"!pip install scikit-learn-intelex -q --progress-bar off","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-08-20T13:46:39.99781Z","iopub.execute_input":"2021-08-20T13:46:39.998158Z","iopub.status.idle":"2021-08-20T13:47:28.810503Z","shell.execute_reply.started":"2021-08-20T13:46:39.998128Z","shell.execute_reply":"2021-08-20T13:47:28.809245Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<big>Patch original scikit-learn.</big>","metadata":{}},{"cell_type":"code","source":"from sklearnex import patch_sklearn\npatch_sklearn()","metadata":{"execution":{"iopub.status.busy":"2021-08-20T13:47:28.813039Z","iopub.execute_input":"2021-08-20T13:47:28.813401Z","iopub.status.idle":"2021-08-20T13:47:29.322762Z","shell.execute_reply.started":"2021-08-20T13:47:28.813364Z","shell.execute_reply":"2021-08-20T13:47:29.321761Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Using optuna to select parameters for Stacking algorithm\n<big>Stacking or generalization is an ensemble of machine learning algorithms.\n\nThis generalization consists of output combination of individual estimators and the final prediction based on it. Stacking allows to use the strength of each individual estimator by using their output as input of a final estimator.</big><br><br>\n<big>We adjust hyperparameters for the best result.</big><br><br>\n\n<big>Parameters that we select:</big><br>\n<big>* <code>alpha</code> - Regularization parameter. Regularization improves the solution and reduces the variance of estimates.<br> </big>\n<big>* <code>l1_ratio</code> - Regularization parameter. For the penalty is a combination of L1 and L2 regularization.<br> </big>","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import Lasso, ElasticNet, Ridge\nfrom sklearn.ensemble import StackingRegressor\nfrom sklearn.metrics import mean_squared_error\nimport optuna","metadata":{"execution":{"iopub.status.busy":"2021-08-20T13:47:29.32405Z","iopub.execute_input":"2021-08-20T13:47:29.324351Z","iopub.status.idle":"2021-08-20T13:47:30.091428Z","shell.execute_reply.started":"2021-08-20T13:47:29.324322Z","shell.execute_reply":"2021-08-20T13:47:30.090205Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_stacking_regressor( alpha4=None,\n                            alpha1=None, alpha2=None, alpha3=None,\n                            l1_ratio=None, l1_ratio2=None\n                            ):\n    elastic = ElasticNet(alpha=alpha1, l1_ratio=l1_ratio, random_state=0)\n    lasso2 = Lasso(alpha=alpha2, random_state=0)\n    ridge = Ridge(alpha=alpha3, random_state=0)\n\n    \n    elastic_f = ElasticNet(alpha=alpha4, l1_ratio=l1_ratio2, random_state=0)\n    stacking_estimators = [\n        ('elastic', elastic),\n        ('lasso2', lasso2),\n        ('ridge', ridge),\n    ]\n    \n    return StackingRegressor(estimators=stacking_estimators, final_estimator=elastic_f)","metadata":{"execution":{"iopub.status.busy":"2021-08-20T13:47:30.09314Z","iopub.execute_input":"2021-08-20T13:47:30.093472Z","iopub.status.idle":"2021-08-20T13:47:30.101043Z","shell.execute_reply.started":"2021-08-20T13:47:30.09344Z","shell.execute_reply":"2021-08-20T13:47:30.100015Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<big>The process of selecting the parameters is too long and computationally intensive, so I selected the parameters in advance.</big>","metadata":{}},{"cell_type":"code","source":"def objective_stack(trial):\n    params ={\n        'alpha4': trial.suggest_float('alpha4', 0.0, 0.02969371481087929),\n        'alpha1': trial.suggest_float('alpha1', 0.0, 0.027694846519887552),\n        'alpha2': trial.suggest_float('alpha2', 0.0, 0.31557621736570013),\n        'alpha3': trial.suggest_float('alpha3', 0.0,  0.029221357138328012),\n        'l1_ratio': trial.suggest_float('l1_ratio', 0.0, 0.31140039770607025),\n        'l1_ratio2': trial.suggest_float('l1_ratio2', 0.0, 0.09864359696600125),\n\n    }\n    model = get_stacking_regressor(**params).fit(x_train, y_train)\n    y_pred = model.predict(x_val)\n    loss = np.sqrt(mean_squared_error(y_val, y_pred))\n    return loss","metadata":{"execution":{"iopub.status.busy":"2021-08-20T13:47:30.102447Z","iopub.execute_input":"2021-08-20T13:47:30.102777Z","iopub.status.idle":"2021-08-20T13:47:30.115299Z","shell.execute_reply.started":"2021-08-20T13:47:30.102747Z","shell.execute_reply":"2021-08-20T13:47:30.114171Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<big><strong>Select parameters</strong></big>","metadata":{}},{"cell_type":"code","source":"study = optuna.create_study(sampler=optuna.samplers.TPESampler(seed=123),\n                            direction=\"minimize\",\n                            pruner=optuna.pruners.HyperbandPruner())","metadata":{"execution":{"iopub.status.busy":"2021-08-20T13:47:30.116667Z","iopub.execute_input":"2021-08-20T13:47:30.11706Z","iopub.status.idle":"2021-08-20T13:47:30.128653Z","shell.execute_reply.started":"2021-08-20T13:47:30.117026Z","shell.execute_reply":"2021-08-20T13:47:30.127434Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<big>Let's see the execution time with Intel(R) Extension for Scikit-learn.</big>","metadata":{}},{"cell_type":"code","source":"%%time\nstudy.optimize(objective_stack, n_trials=10)","metadata":{"execution":{"iopub.status.busy":"2021-08-20T13:47:30.130177Z","iopub.execute_input":"2021-08-20T13:47:30.130473Z","iopub.status.idle":"2021-08-20T14:00:27.120111Z","shell.execute_reply.started":"2021-08-20T13:47:30.130445Z","shell.execute_reply":"2021-08-20T14:00:27.11888Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<big><strong>Training the model with the selected parameters.</strong></big>","metadata":{}},{"cell_type":"code","source":"x_train_full = np.concatenate((x_train, x_val), axis=0)\ny_train_full = np.concatenate((y_train, y_val), axis=0)","metadata":{"execution":{"iopub.status.busy":"2021-08-20T14:00:27.122013Z","iopub.execute_input":"2021-08-20T14:00:27.122334Z","iopub.status.idle":"2021-08-20T14:00:28.134766Z","shell.execute_reply.started":"2021-08-20T14:00:27.122298Z","shell.execute_reply":"2021-08-20T14:00:28.133803Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nfinal_model = get_stacking_regressor(**study.best_params).fit(x_train_full, y_train_full)","metadata":{"execution":{"iopub.status.busy":"2021-08-20T14:00:28.138543Z","iopub.execute_input":"2021-08-20T14:00:28.138846Z","iopub.status.idle":"2021-08-20T14:01:57.084643Z","shell.execute_reply.started":"2021-08-20T14:00:28.138817Z","shell.execute_reply":"2021-08-20T14:01:57.083292Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<big><strong>Prediction.</strong></big>","metadata":{}},{"cell_type":"code","source":"%%time\ny_pred = final_model.predict(x_test)\ny_pred = scaler_y.inverse_transform(y_pred)","metadata":{"execution":{"iopub.status.busy":"2021-08-20T14:01:57.086576Z","iopub.execute_input":"2021-08-20T14:01:57.087007Z","iopub.status.idle":"2021-08-20T14:01:57.169488Z","shell.execute_reply.started":"2021-08-20T14:01:57.086963Z","shell.execute_reply":"2021-08-20T14:01:57.168139Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<big>Save the results in 'submission.csv'.</big>","metadata":{}},{"cell_type":"code","source":"submission['item_cnt_month'] = y_pred\nsubmission.to_csv('submission.csv', index=False)\nsubmission.head(10)","metadata":{"execution":{"iopub.status.busy":"2021-08-20T14:01:57.171083Z","iopub.execute_input":"2021-08-20T14:01:57.171547Z","iopub.status.idle":"2021-08-20T14:01:58.008214Z","shell.execute_reply.started":"2021-08-20T14:01:57.171502Z","shell.execute_reply":"2021-08-20T14:01:58.006997Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Now we use the same algorithms with original scikit-learn","metadata":{}},{"cell_type":"markdown","source":"<big>Let’s run the same code with original scikit-learn and compare its execution time with the execution time of the patched by Intel(R) Extension for Scikit-learn.</big>","metadata":{}},{"cell_type":"code","source":"from sklearnex import unpatch_sklearn\nunpatch_sklearn()","metadata":{"execution":{"iopub.status.busy":"2021-08-20T14:01:58.00953Z","iopub.execute_input":"2021-08-20T14:01:58.009829Z","iopub.status.idle":"2021-08-20T14:01:58.014517Z","shell.execute_reply.started":"2021-08-20T14:01:58.0098Z","shell.execute_reply":"2021-08-20T14:01:58.013506Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.linear_model import Lasso, ElasticNet, Ridge","metadata":{"execution":{"iopub.status.busy":"2021-08-20T14:01:58.019252Z","iopub.execute_input":"2021-08-20T14:01:58.019801Z","iopub.status.idle":"2021-08-20T14:01:58.026088Z","shell.execute_reply.started":"2021-08-20T14:01:58.019754Z","shell.execute_reply":"2021-08-20T14:01:58.025086Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<big><strong>Select parameters</strong></big>","metadata":{}},{"cell_type":"code","source":"study = optuna.create_study(sampler=optuna.samplers.TPESampler(seed=123),\n                            direction=\"minimize\",\n                            pruner=optuna.pruners.HyperbandPruner())","metadata":{"execution":{"iopub.status.busy":"2021-08-20T14:01:58.028057Z","iopub.execute_input":"2021-08-20T14:01:58.028573Z","iopub.status.idle":"2021-08-20T14:01:58.040405Z","shell.execute_reply.started":"2021-08-20T14:01:58.02853Z","shell.execute_reply":"2021-08-20T14:01:58.03931Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<big>Let's see the execution time without patch.</big>","metadata":{}},{"cell_type":"code","source":"%%time\nstudy.optimize(objective_stack, n_trials=10)","metadata":{"execution":{"iopub.status.busy":"2021-08-20T14:01:58.041851Z","iopub.execute_input":"2021-08-20T14:01:58.042214Z","iopub.status.idle":"2021-08-20T14:29:27.298227Z","shell.execute_reply.started":"2021-08-20T14:01:58.042181Z","shell.execute_reply":"2021-08-20T14:29:27.297012Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<big><strong>Training the model with the selected parameters.</strong></big>","metadata":{}},{"cell_type":"code","source":"%%time\nfinal_model = get_stacking_regressor(**study.best_params).fit(x_train_full, y_train_full)","metadata":{"execution":{"iopub.status.busy":"2021-08-20T14:29:27.299955Z","iopub.execute_input":"2021-08-20T14:29:27.300607Z","iopub.status.idle":"2021-08-20T14:32:01.223348Z","shell.execute_reply.started":"2021-08-20T14:29:27.300558Z","shell.execute_reply":"2021-08-20T14:32:01.222279Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2>Conclusions</h2>\n<big>We can see that using only one classical machine learning algorithm may give you a pretty hight accuracy score. We also use well-known libraries Scikit-learn and Optuna, as well as the increasingly popular library Intel® Extension for Scikit-learn. Noted that Intel® Extension for Scikit-learn gives you opportunities to:</big>\n\n* <big>Use your Scikit-learn code for training and inference without modification.</big>\n* <big>Speed up selection of parameters <strong>from 27 minutes to 12 minutes.</strong></big>\n* <big>Get predictions of the similar quality.</big>","metadata":{}}]}