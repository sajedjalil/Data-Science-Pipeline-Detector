{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"\\*\\*\\* Note! This notebook is a repost, I made the original private after some ensembling and postprocessing steps put it into the top 10 on the leaderboard. Kaggle doesn't allow specific versions of a notebook to be made private and I didn't think a top 10 solution should be shared \\*\\*\\*\n","metadata":{}},{"cell_type":"markdown","source":"# Introduction\n\nThis notebook constructs a prediction model for the Predict Future Sales competition that is the final project for the Coursera course \"[How to Win a Data Science Competition](http://www.coursera.org/learn/competitive-data-science/home/welcome)\". The task is to predict monthly sales for various items in different retail outlets of the Russian company 1C.  \n\nI spent several months on this as practice using pandas, so some parts are a bit more complicated than might be expected of a typical short project submission.\n\nThere are some other very good notebooks for this competition which are well worth looking at and taught me a lot:\nhttps://www.kaggle.com/dlarionov/feature-engineering-xgboost  \nhttps://www.kaggle.com/gordotron85/future-sales-xgboost-top-3  \nhttps://www.kaggle.com/deepdivelm/feature-engineering-lightgbm-exploring-performance  \n\nThis is the top-scoring public notebook at the time of writing (0.84325, place 51 on the public leaderboard), which is mainly because of two novel feature types which work well when combined together. First, there is an item name group feature that groups together items with very similar names that are likely to refer to different versions of the same item (e.g. different editions of the same game or music album). Second, the way the test set was generated was exploited to count how many items sold in the month being predicted were in the same group as the item being predicted (e.g. same category, same name group). This combines well with the item name group feature to detect new items which are part of large multi-format releases that are likely to sell well. Detecting high-selling new items is one of the hardest challenges for the model in this competition (and has to be performed manually to get a really high score, I think).\n\nI hope you find the notebook interesting, and I welcome feedback - suggestions for improvements, advice about parts that are unclear, etc","metadata":{}},{"cell_type":"markdown","source":"# Data loading and preprocessing, utility function definition","metadata":{"papermill":{"duration":0.053912,"end_time":"2021-04-28T18:11:27.348187","exception":false,"start_time":"2021-04-28T18:11:27.294275","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"## Imports and data loading","metadata":{"papermill":{"duration":0.049686,"end_time":"2021-04-28T18:11:27.448493","exception":false,"start_time":"2021-04-28T18:11:27.398807","status":"completed"},"tags":[]}},{"cell_type":"code","source":"import gc\nimport itertools\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns","metadata":{"papermill":{"duration":1.032316,"end_time":"2021-04-28T18:11:28.530522","exception":false,"start_time":"2021-04-28T18:11:27.498206","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"A few utility functions used throughout the notebook.","metadata":{}},{"cell_type":"code","source":"def reduce_mem_usage(df, silent=True, allow_categorical=True, float_dtype=\"float32\"):\n    \"\"\" \n    Iterates through all the columns of a dataframe and downcasts the data type\n     to reduce memory usage. Can also factorize categorical columns to integer dtype.\n    \"\"\"\n    def _downcast_numeric(series, allow_categorical=allow_categorical):\n        \"\"\"\n        Downcast a numeric series into either the smallest possible int dtype or a specified float dtype.\n        \"\"\"\n        if pd.api.types.is_sparse(series.dtype) is True:\n            return series\n        elif pd.api.types.is_numeric_dtype(series.dtype) is False:\n            if pd.api.types.is_datetime64_any_dtype(series.dtype):\n                return series\n            else:\n                if allow_categorical:\n                    return series\n                else:\n                    codes, uniques = series.factorize()\n                    series = pd.Series(data=codes, index=series.index)\n                    series = _downcast_numeric(series)\n                    return series\n        else:\n            series = pd.to_numeric(series, downcast=\"integer\")\n        if pd.api.types.is_float_dtype(series.dtype):\n            series = series.astype(float_dtype)\n        return series\n\n    if silent is False:\n        start_mem = np.sum(df.memory_usage()) / 1024 ** 2\n        print(\"Memory usage of dataframe is {:.2f} MB\".format(start_mem))\n    if df.ndim == 1:\n        df = _downcast_numeric(df)\n    else:\n        for col in df.columns:\n            df.loc[:, col] = _downcast_numeric(df.loc[:,col])\n    if silent is False:\n        end_mem = np.sum(df.memory_usage()) / 1024 ** 2\n        print(\"Memory usage after optimization is: {:.2f} MB\".format(end_mem))\n        print(\"Decreased by {:.1f}%\".format(100 * (start_mem - end_mem) / start_mem))\n\n    return df\n\n\ndef shrink_mem_new_cols(matrix, oldcols=None, allow_categorical=False):\n    # Calls reduce_mem_usage on columns which have not yet been optimized\n    if oldcols is not None:\n        newcols = matrix.columns.difference(oldcols)\n    else:\n        newcols = matrix.columns\n    matrix.loc[:,newcols] = reduce_mem_usage(matrix.loc[:,newcols], allow_categorical=allow_categorical)\n    oldcols = matrix.columns  # This is used to track which columns have already been downcast\n    return matrix, oldcols\n\n\ndef list_if_not(s, dtype=str):\n    # Puts a variable in a list if it is not already a list\n    if type(s) not in (dtype, list):\n        raise TypeError\n    if (s != \"\") & (type(s) is not list):\n        s = [s]\n    return s","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Load the provided data.","metadata":{"papermill":{"duration":0.049382,"end_time":"2021-04-28T18:11:28.630332","exception":false,"start_time":"2021-04-28T18:11:28.58095","status":"completed"},"tags":[]}},{"cell_type":"code","source":"items = pd.read_csv(\"../input/competitive-data-science-predict-future-sales/items.csv\")\nshops = pd.read_csv(\"../input/competitive-data-science-predict-future-sales/shops.csv\")\ntrain = pd.read_csv(\"../input/competitive-data-science-predict-future-sales/sales_train.csv\")\ntest = pd.read_csv(\"../input/competitive-data-science-predict-future-sales/test.csv\")","metadata":{"papermill":{"duration":3.762579,"end_time":"2021-04-28T18:11:32.444498","exception":false,"start_time":"2021-04-28T18:11:28.681919","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Convert the date column to the datetime dtype to enable datetime operations.","metadata":{}},{"cell_type":"code","source":"train[\"date\"] = pd.to_datetime(train[\"date\"], format=\"%d.%m.%Y\")","metadata":{"papermill":{"duration":3.762579,"end_time":"2021-04-28T18:11:32.444498","exception":false,"start_time":"2021-04-28T18:11:28.681919","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data cleaning","metadata":{"papermill":{"duration":0.049628,"end_time":"2021-04-28T18:11:32.545881","exception":false,"start_time":"2021-04-28T18:11:32.496253","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"The training dataframe is cleaned with standard steps  \n","metadata":{"papermill":{"duration":0.048834,"end_time":"2021-04-28T18:11:32.643753","exception":false,"start_time":"2021-04-28T18:11:32.594919","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Merge some duplicate shops\ntrain[\"shop_id\"] = train[\"shop_id\"].replace({0: 57, 1: 58, 11: 10, 40: 39})\n# Keep only shops that are in the test set\ntrain = train.loc[train.shop_id.isin(test[\"shop_id\"].unique()), :]\n# Drop training items with extreme or negative prices or sales counts\ntrain = train[(train[\"item_price\"] > 0) & (train[\"item_price\"] < 50000)]\ntrain = train[(train[\"item_cnt_day\"] > 0) & (train[\"item_cnt_day\"] < 1000)]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Preprocessing","metadata":{"papermill":{"duration":0.053208,"end_time":"2021-04-28T18:11:34.280403","exception":false,"start_time":"2021-04-28T18:11:34.227195","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"The test data seems to be every possible combination (the cartesian product) of shops and items that registered a sale in the test month, with the target as the total month's sales made for each of these shop-item combinations. Here a training matrix is made that replicates this structure for every month in the training data period. The test items are concatenated to the end of the training data so that features can be generated for the test period.","metadata":{"papermill":{"duration":0.050656,"end_time":"2021-04-28T18:11:34.384271","exception":false,"start_time":"2021-04-28T18:11:34.333615","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def create_testlike_train(sales_train, test=None):\n    indexlist = []\n    for i in sales_train.date_block_num.unique():\n        x = itertools.product(\n            [i],\n            sales_train.loc[sales_train.date_block_num == i].shop_id.unique(),\n            sales_train.loc[sales_train.date_block_num == i].item_id.unique(),\n        )\n        indexlist.append(np.array(list(x)))\n    df = pd.DataFrame(\n        data=np.concatenate(indexlist, axis=0),\n        columns=[\"date_block_num\", \"shop_id\", \"item_id\"],\n    )\n\n    # Add revenue column to sales_train\n    sales_train[\"item_revenue_day\"] = sales_train[\"item_price\"] * sales_train[\"item_cnt_day\"]\n    # Aggregate item_id / shop_id item_cnts and revenue at the month level\n    sales_train_grouped = sales_train.groupby([\"date_block_num\", \"shop_id\", \"item_id\"]).agg(\n        item_cnt_month=pd.NamedAgg(column=\"item_cnt_day\", aggfunc=\"sum\"),\n        item_revenue_month=pd.NamedAgg(column=\"item_revenue_day\", aggfunc=\"sum\"),\n    )\n\n    # Merge the grouped data with the index\n    df = df.merge(\n        sales_train_grouped, how=\"left\", on=[\"date_block_num\", \"shop_id\", \"item_id\"],\n    )\n\n    if test is not None:\n        test[\"date_block_num\"] = 34\n        test[\"date_block_num\"] = test[\"date_block_num\"].astype(np.int8)\n        test[\"shop_id\"] = test.shop_id.astype(np.int8)\n        test[\"item_id\"] = test.item_id.astype(np.int16)\n        test = test.drop(columns=\"ID\")\n\n        df = pd.concat([df, test[[\"date_block_num\", \"shop_id\", \"item_id\"]]])\n\n    # Fill empty item_cnt entries with 0\n    df.item_cnt_month = df.item_cnt_month.fillna(0)\n    df.item_revenue_month = df.item_revenue_month.fillna(0)\n\n    return df","metadata":{"papermill":{"duration":0.065775,"end_time":"2021-04-28T18:11:34.500373","exception":false,"start_time":"2021-04-28T18:11:34.434598","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"matrix = create_testlike_train(train, test)\ndel(test)","metadata":{"papermill":{"duration":23.143354,"end_time":"2021-04-28T18:11:57.693556","exception":false,"start_time":"2021-04-28T18:11:34.550202","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The function reduce_mem_usage downcasts datatypes to reduce memory usage, which is necessary to prevent memory overflow errors in the Kaggle notebook.","metadata":{}},{"cell_type":"code","source":"matrix = reduce_mem_usage(matrix, silent=False)\noldcols = matrix.columns","metadata":{"papermill":{"duration":23.143354,"end_time":"2021-04-28T18:11:57.693556","exception":false,"start_time":"2021-04-28T18:11:34.550202","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Feature engineering  \nIn this section predictor feature columns are generated and added to the matrix","metadata":{"papermill":{"duration":0.051209,"end_time":"2021-04-28T18:11:57.796356","exception":false,"start_time":"2021-04-28T18:11:57.745147","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"# Item name groups with fuzzywuzzy\n\nItems in the items table are ordered alphabetically according to the item_name field, so that similar items are generally listed next to each other. For example, the first two items in the table below are the same game \"Fuse\" for two different consoles, followed by two different licensing options for the same internet security program. This ordering can be used to help group related items together.  ","metadata":{"papermill":{"duration":0.050933,"end_time":"2021-04-28T18:11:57.90026","exception":false,"start_time":"2021-04-28T18:11:57.849327","status":"completed"},"tags":[]}},{"cell_type":"code","source":"items.query(\"item_id>3564\").head(5)","metadata":{"papermill":{"duration":0.079826,"end_time":"2021-04-28T18:11:58.030271","exception":false,"start_time":"2021-04-28T18:11:57.950445","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The following cell groups similar items together by sequentially looping through items, measuring the similarity of the names of ajacent items using the string matching package fuzzywuzzy (https://github.com/seatgeek/fuzzywuzzy), and assigning items to the same group if their match value is above a threshold.","metadata":{}},{"cell_type":"code","source":"import re\n\nfrom fuzzywuzzy import fuzz\n\n\ndef add_item_name_groups(matrix, train, items, sim_thresh, feature_name=\"item_name_group\"):\n    def partialmatchgroups(items, sim_thresh=sim_thresh):\n        def strip_brackets(string):\n            string = re.sub(r\"\\(.*?\\)\", \"\", string)\n            string = re.sub(r\"\\[.*?\\]\", \"\", string)\n            return string\n\n        items = items.copy()\n        items[\"nc\"] = items.item_name.apply(strip_brackets)\n        items[\"ncnext\"] = np.concatenate((items[\"nc\"].to_numpy()[1:], np.array([\"\"])))\n\n        def partialcompare(s):\n            return fuzz.partial_ratio(s[\"nc\"], s[\"ncnext\"])\n\n        items[\"partialmatch\"] = items.apply(partialcompare, axis=1)\n        # Assign groups\n        grp = 0\n        for i in range(items.shape[0]):\n            items.loc[i, \"partialmatchgroup\"] = grp\n            if items.loc[i, \"partialmatch\"] < sim_thresh:\n                grp += 1\n        items = items.drop(columns=[\"nc\", \"ncnext\", \"partialmatch\"])\n        return items\n\n    items = partialmatchgroups(items)\n    items = items.rename(columns={\"partialmatchgroup\": feature_name})\n    items = items.drop(columns=\"partialmatchgroup\", errors=\"ignore\")\n\n    items[feature_name] = items[feature_name].apply(str)\n    items[feature_name] = items[feature_name].factorize()[0]\n    matrix = matrix.merge(items[[\"item_id\", feature_name]], on=\"item_id\", how=\"left\")\n    train = train.merge(items[[\"item_id\", feature_name]], on=\"item_id\", how=\"left\")\n    return matrix, train\n\n\nmatrix, train = add_item_name_groups(matrix, train, items, 65)","metadata":{"papermill":{"duration":10.183104,"end_time":"2021-04-28T18:12:08.265598","exception":false,"start_time":"2021-04-28T18:11:58.082494","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Music artist / first word of item name  \n\nThis function assigns music items into groups according to the artist name, which is extracted from the item name with regular expressions according to 3 common patterns used to indicate the artist name (all uppercase, separated from the release title by a doublespace, or separated by dot-space (. ).  \nFor non-music categories, the items are grouped according to the first word in the item name instead.","metadata":{"papermill":{"duration":0.050709,"end_time":"2021-04-28T18:12:08.36996","exception":false,"start_time":"2021-04-28T18:12:08.319251","status":"completed"},"tags":[]}},{"cell_type":"code","source":"from nltk.corpus import stopwords\n\n\ndef add_first_word_features(matrix, items=items, feature_name=\"artist_name_or_first_word\"):\n    # This extracts artist names for music categories and adds them as a feature.\n    def extract_artist(st):\n        st = st.strip()\n        if st.startswith(\"V/A\"):\n            artist = \"V/A\"\n        elif st.startswith(\"СБ\"):\n            artist = \"СБ\"\n        else:\n            # Retrieves artist names using the double space or all uppercase pattern\n            mus_artist_dubspace = re.compile(r\".{2,}?(?=\\s{2,})\")\n            match_dubspace = mus_artist_dubspace.match(st)\n            mus_artist_capsonly = re.compile(r\"^([^a-zа-я]+\\s)+\")\n            match_capsonly = mus_artist_capsonly.match(st)\n            candidates = [match_dubspace, match_capsonly]\n            candidates = [m[0] for m in candidates if m is not None]\n            # Sometimes one of the patterns catches some extra words so choose the shortest one\n            if len(candidates):\n                artist = min(candidates, key=len)\n            else:\n                # If neither of the previous patterns found something, use the dot-space pattern\n                mus_artist_dotspace = re.compile(r\".{2,}?(?=\\.\\s)\")\n                match = mus_artist_dotspace.match(st)\n                if match:\n                    artist = match[0]\n                else:\n                    artist = \"\"\n        artist = artist.upper()\n        artist = re.sub(r\"[^A-ZА-Я ]||\\bTHE\\b\", \"\", artist)\n        artist = re.sub(r\"\\s{2,}\", \" \", artist)\n        artist = artist.strip()\n        return artist\n\n    items = items.copy()\n    all_stopwords = stopwords.words(\"russian\")\n    all_stopwords = all_stopwords + stopwords.words(\"english\")\n\n    def first_word(string):\n        # This cleans the string of special characters, excess spaces and stopwords then extracts the first word\n        string = re.sub(r\"[^\\w\\s]\", \"\", string)\n        string = re.sub(r\"\\s{2,}\", \" \", string)\n        tokens = string.lower().split()\n        tokens = [t for t in tokens if t not in all_stopwords]\n        token = tokens[0] if len(tokens) > 0 else \"\"\n        return token\n\n    music_categories = [55, 56, 57, 58, 59, 60]\n    items.loc[items.item_category_id.isin(music_categories), feature_name] = items.loc[\n        items.item_category_id.isin(music_categories), \"item_name\"\n    ].apply(extract_artist)\n    items.loc[items[feature_name] == \"\", feature_name] = \"other music\"\n    items.loc[~items.item_category_id.isin(music_categories), feature_name] = items.loc[\n        ~items.item_category_id.isin(music_categories), \"item_name\"\n    ].apply(first_word)\n    items.loc[items[feature_name] == \"\", feature_name] = \"other non-music\"\n    items[feature_name] = items[feature_name].factorize()[0]\n    matrix = matrix.merge(items[[\"item_id\", feature_name]], on=\"item_id\", how=\"left\",)\n    return matrix\n\n\nmatrix = add_first_word_features(\n    matrix, items=items, feature_name=\"artist_name_or_first_word\"\n)","metadata":{"papermill":{"duration":2.989192,"end_time":"2021-04-28T18:12:11.411023","exception":false,"start_time":"2021-04-28T18:12:08.421831","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Item name length as a feature\nThe name of the item_name field is surprisingly predictive, presumably because similar items often have similar length names. This is recorded both from the raw item name and the name cleaned of special characters and bracketed terms, which often contain information about release formats that obscure similarities between items.","metadata":{"papermill":{"duration":0.051751,"end_time":"2021-04-28T18:12:11.513824","exception":false,"start_time":"2021-04-28T18:12:11.462073","status":"completed"},"tags":[]}},{"cell_type":"code","source":"import re\ndef clean_item_name(string):\n    # Removes bracketed terms, special characters and extra whitespace\n    string = re.sub(r\"\\[.*?\\]\", \"\", string)\n    string = re.sub(r\"\\(.*?\\)\", \"\", string)\n    string = re.sub(r\"[^A-ZА-Яa-zа-я0-9 ]\", \"\", string)\n    string = re.sub(r\"\\s{2,}\", \" \", string)\n    string = string.lower()\n    return string\n\nitems[\"item_name_cleaned_length\"] = items[\"item_name\"].apply(clean_item_name).apply(len)\nitems[\"item_name_length\"] = items[\"item_name\"].apply(len)\nmatrix = matrix.merge(items[['item_id', 'item_name_length', 'item_name_cleaned_length']], how='left', on='item_id')\nitems = items.drop(columns=['item_name_length', 'item_name_cleaned_length'])","metadata":{"papermill":{"duration":1.574616,"end_time":"2021-04-28T18:12:13.140382","exception":false,"start_time":"2021-04-28T18:12:11.565766","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Created name features\")\nmatrix, oldcols = shrink_mem_new_cols(matrix, oldcols)","metadata":{"papermill":{"duration":1.963919,"end_time":"2021-04-28T18:12:15.155549","exception":false,"start_time":"2021-04-28T18:12:13.19163","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Time features\nDay and month-resolution time features are created from the training dataframe, e.g. number of days since the first and last sale of each item.\n\nThe time since the first sale of each items is also used to create a mean sales-per-day feature (\"item_cnt_day_avg\"), which is potentially useful to correct sales counts for items which are less than a month old and therefore were not available to buy for the entire preceding month.  ","metadata":{"papermill":{"duration":0.051242,"end_time":"2021-04-28T18:12:15.258377","exception":false,"start_time":"2021-04-28T18:12:15.207135","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def add_time_features(m, train, correct_item_cnt_day=False):\n    from pandas.tseries.offsets import Day, MonthBegin, MonthEnd\n\n    def item_shop_age_months(m):\n        m[\"item_age\"] = m.groupby(\"item_id\")[\"date_block_num\"].transform(\n            lambda x: x - x.min()\n        )\n        # Sales tend to plateau after 12 months\n        m[\"new_item\"] = m[\"item_age\"] == 0\n        m[\"new_item\"] = m[\"new_item\"].astype(\"int8\")\n        m[\"shop_age\"] = (\n            m.groupby(\"shop_id\")[\"date_block_num\"]\n            .transform(lambda x: x - x.min())\n            .astype(\"int8\")\n        )\n        return m\n\n    # Add dummy values for the test month so that features are created correctly\n    dummies = m.loc[m.date_block_num == 34, [\"date_block_num\", \"shop_id\", \"item_id\"]]\n    dummies = dummies.assign(\n        date=pd.to_datetime(\"2015-11-30\"), item_price=1, item_cnt_day=0, item_revenue_day=0,\n    )\n    train = pd.concat([train, dummies])\n    del dummies\n\n    month_last_day = train.groupby(\"date_block_num\").date.max().rename(\"month_last_day\")\n    month_last_day[~month_last_day.dt.is_month_end] = (\n        month_last_day[~month_last_day.dt.is_month_end] + MonthEnd()\n    )\n    month_first_day = train.groupby(\"date_block_num\").date.min().rename(\"month_first_day\")\n    month_first_day[~month_first_day.dt.is_month_start] = (\n        month_first_day[~month_first_day.dt.is_month_start] - MonthBegin()\n    )\n    month_length = (month_last_day - month_first_day + Day()).rename(\"month_length\")\n    first_shop_date = train.groupby(\"shop_id\").date.min().rename(\"first_shop_date\")\n    first_item_date = train.groupby(\"item_id\").date.min().rename(\"first_item_date\")\n    first_shop_item_date = (\n        train.groupby([\"shop_id\", \"item_id\"]).date.min().rename(\"first_shop_item_date\")\n    )\n    first_item_name_group_date = (\n        train.groupby(\"item_name_group\").date.min().rename(\"first_name_group_date\")\n    )\n\n    m = m.merge(month_first_day, left_on=\"date_block_num\", right_index=True, how=\"left\")\n    m = m.merge(month_last_day, left_on=\"date_block_num\", right_index=True, how=\"left\")\n    m = m.merge(month_length, left_on=\"date_block_num\", right_index=True, how=\"left\")\n    m = m.merge(first_shop_date, left_on=\"shop_id\", right_index=True, how=\"left\")\n    m = m.merge(first_item_date, left_on=\"item_id\", right_index=True, how=\"left\")\n    m = m.merge(\n        first_shop_item_date, left_on=[\"shop_id\", \"item_id\"], right_index=True, how=\"left\"\n    )\n    m = m.merge(\n        first_item_name_group_date, left_on=\"item_name_group\", right_index=True, how=\"left\"\n    )\n\n    # Calculate how long the item was sold for in each month and use this to calculate average sales per day\n    m[\"shop_open_days\"] = m[\"month_last_day\"] - m[\"first_shop_date\"] + Day()\n    m[\"item_first_sale_days\"] = m[\"month_last_day\"] - m[\"first_item_date\"] + Day()\n    m[\"item_in_shop_days\"] = (\n        m[[\"shop_open_days\", \"item_first_sale_days\", \"month_length\"]].min(axis=1).dt.days\n    )\n    m = m.drop(columns=\"item_first_sale_days\")\n    m[\"item_cnt_day_avg\"] = m[\"item_cnt_month\"] / m[\"item_in_shop_days\"]\n    m[\"month_length\"] = m[\"month_length\"].dt.days\n\n    # Calculate the time differences from the beginning of the month so they can be used as features without lagging\n    m[\"shop_open_days\"] = m[\"month_first_day\"] - m[\"first_shop_date\"]\n    m[\"first_item_sale_days\"] = m[\"month_first_day\"] - m[\"first_item_date\"]\n    m[\"first_shop_item_sale_days\"] = m[\"month_first_day\"] - m[\"first_shop_item_date\"]\n    m[\"first_name_group_sale_days\"] = m[\"month_first_day\"] - m[\"first_name_group_date\"]\n    m[\"shop_open_days\"] = m[\"shop_open_days\"].dt.days.fillna(0).clip(lower=0)\n    m[\"first_item_sale_days\"] = (\n        m[\"first_item_sale_days\"].dt.days.fillna(0).clip(lower=0).replace(0, 9999)\n    )\n    m[\"first_shop_item_sale_days\"] = (\n        m[\"first_shop_item_sale_days\"].dt.days.fillna(0).clip(lower=0).replace(0, 9999)\n    )\n    m[\"first_name_group_sale_days\"] = (\n        m[\"first_name_group_sale_days\"].dt.days.fillna(0).clip(lower=0).replace(0, 9999)\n    )\n\n    # Add days since last sale\n    def last_sale_days(matrix):\n        last_shop_item_dates = []\n        for dbn in range(1, 35):\n            lsid_temp = (\n                train.query(f\"date_block_num<{dbn}\")\n                .groupby([\"shop_id\", \"item_id\"])\n                .date.max()\n                .rename(\"last_shop_item_sale_date\")\n                .reset_index()\n            )\n            lsid_temp[\"date_block_num\"] = dbn\n            last_shop_item_dates.append(lsid_temp)\n\n        last_shop_item_dates = pd.concat(last_shop_item_dates)\n        matrix = matrix.merge(\n            last_shop_item_dates, on=[\"date_block_num\", \"shop_id\", \"item_id\"], how=\"left\"\n        )\n\n        def days_since_last_feat(m, feat_name, date_feat_name, missingval):\n            m[feat_name] = (m[\"month_first_day\"] - m[date_feat_name]).dt.days\n            m.loc[m[feat_name] > 2000, feat_name] = missingval\n            m.loc[m[feat_name].isna(), feat_name] = missingval\n            return m\n\n        matrix = days_since_last_feat(\n            matrix, \"last_shop_item_sale_days\", \"last_shop_item_sale_date\", 9999\n        )\n\n        matrix = matrix.drop(columns=[\"last_shop_item_sale_date\"])\n        return matrix\n\n    m = last_sale_days(m)\n    # Month id feature\n    m[\"month\"] = m[\"month_first_day\"].dt.month\n\n    m = m.drop(\n        columns=[\n            \"first_day\",\n            \"month_first_day\",\n            \"month_last_day\",\n            \"first_shop_date\",\n            \"first_item_date\",\n            \"first_name_group_date\",\n            \"item_in_shop_days\",\n            \"first_shop_item_date\",\n            \"month_length\",\n        ],\n        errors=\"ignore\",\n    )\n\n    m = item_shop_age_months(m)\n\n    if correct_item_cnt_day == True:\n        m[\"item_cnt_month_original\"] = m[\"item_cnt_month\"]\n        m[\"item_cnt_month\"] = m[\"item_cnt_day_avg\"] * m[\"month_length\"]\n\n    return m","metadata":{"papermill":{"duration":0.086102,"end_time":"2021-04-28T18:12:15.39602","exception":false,"start_time":"2021-04-28T18:12:15.309918","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"matrix = add_time_features(matrix, train, False)\nprint(\"Time features created\")","metadata":{"papermill":{"duration":44.568578,"end_time":"2021-04-28T18:13:00.017133","exception":false,"start_time":"2021-04-28T18:12:15.448555","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Price features  \n\nThe price of the item in the last month in which it was sold, and its price relative to other items in the same category.","metadata":{"papermill":{"duration":0.052584,"end_time":"2021-04-28T18:13:00.379709","exception":false,"start_time":"2021-04-28T18:13:00.327125","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def add_price_features(matrix, train):\n    # Get mean prices per month from train dataframe\n    price_features = train.groupby([\"date_block_num\", \"item_id\"]).item_price.mean()\n    price_features = pd.DataFrame(price_features)\n    price_features = price_features.reset_index()\n    # Calculate normalized differenced from mean category price per month\n    price_features = price_features.merge(\n        items[[\"item_id\", \"item_category_id\"]], how=\"left\", on=\"item_id\"\n    )\n    price_features[\"norm_diff_cat_price\"] = price_features.groupby(\n        [\"date_block_num\", \"item_category_id\"]\n    )[\"item_price\"].transform(lambda x: (x - x.mean()) / x.mean())\n    # Retain only the necessary features\n    price_features = price_features[\n        [\n            \"date_block_num\",\n            \"item_id\",\n            \"item_price\",\n            \"norm_diff_cat_price\",\n        ]\n    ]\n\n    features = [\n        \"item_price\",\n        \"norm_diff_cat_price\",\n    ]\n    newnames = [\"last_\" + f for f in features]\n    aggs = {f: \"last\" for f in features}\n    renames = {f: \"last_\" + f for f in features}\n    features = []\n    for dbn in range(1, 35):\n        f_temp = (\n            price_features.query(f\"date_block_num<{dbn}\")\n            .groupby(\"item_id\")\n            .agg(aggs)\n            .rename(columns=renames)\n        )\n        f_temp[\"date_block_num\"] = dbn\n        features.append(f_temp)\n    features = pd.concat(features).reset_index()\n    matrix = matrix.merge(features, on=[\"date_block_num\", \"item_id\"], how=\"left\")\n    return matrix","metadata":{"papermill":{"duration":0.067926,"end_time":"2021-04-28T18:13:00.501112","exception":false,"start_time":"2021-04-28T18:13:00.433186","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"matrix = add_price_features(matrix, train)\ndel(train)","metadata":{"papermill":{"duration":4.30382,"end_time":"2021-04-28T18:13:04.85857","exception":false,"start_time":"2021-04-28T18:13:00.55475","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Item category features  \nIn addition to the item categories provided with the data, I also manually defined two additional category groupings - supercategory (e.g. \"games\", \"music\") and platform (e.g. \"PS4\", \"mp3\").","metadata":{"papermill":{"duration":0.052081,"end_time":"2021-04-28T18:13:04.964343","exception":false,"start_time":"2021-04-28T18:13:04.912262","status":"completed"},"tags":[]}},{"cell_type":"code","source":"matrix = matrix.merge(items[['item_id', 'item_category_id']], on='item_id', how='left')\n\nplatform_map = {\n    0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: 7, 8: 8, 9: 8, 10: 1, 11: 2,\n    12: 3, 13: 4, 14: 5, 15: 6, 16: 7, 17: 8, 18: 1, 19: 2, 20: 3, 21: 4, 22: 5,\n    23: 6, 24: 7, 25: 8, 26: 9, 27: 10, 28: 0, 29: 0, 30: 0, 31: 0, 32: 8, 33: 11,\n    34: 11, 35: 3, 36: 0, 37: 12, 38: 12, 39: 12, 40: 13, 41: 13, 42: 14, 43: 15,\n    44: 15, 45: 15, 46: 14, 47: 14, 48: 14, 49: 14, 50: 14, 51: 14, 52: 14, 53: 14,\n    54: 8, 55: 16, 56: 16, 57: 17, 58: 18, 59: 13, 60: 16, 61: 8, 62: 8, 63: 8, 64: 8,\n    65: 8, 66: 8, 67: 8, 68: 8, 69: 8, 70: 8, 71: 8, 72: 8, 73: 0, 74: 10, 75: 0,\n    76: 0, 77: 0, 78: 0, 79: 8, 80: 8, 81: 8, 82: 8, 83: 8,\n}\nmatrix['platform_id'] = matrix['item_category_id'].map(platform_map)\n\nsupercat_map = {\n    0: 0, 1: 1, 2: 1, 3: 1, 4: 1, 5: 1, 6: 1, 7: 1, 8: 2, 9: 2, 10: 1, 11: 1, 12: 1,\n    13: 1, 14: 1, 15: 1, 16: 1, 17: 1, 18: 3, 19: 3, 20: 3, 21: 3, 22: 3, 23: 3,\n    24: 3, 25: 0, 26: 2, 27: 3, 28: 3, 29: 3, 30: 3, 31: 3, 32: 2, 33: 2, 34: 2,\n    35: 2, 36: 2, 37: 4, 38: 4, 39: 4, 40: 4, 41: 4, 42: 5, 43: 5, 44: 5, 45: 5,\n    46: 5, 47: 5, 48: 5, 49: 5, 50: 5, 51: 5, 52: 5, 53: 5, 54: 5, 55: 6, 56: 6,\n    57: 6, 58: 6, 59: 6, 60: 6, 61: 0, 62: 0, 63: 0, 64: 0, 65: 0, 66: 0, 67: 0,\n    68: 0, 69: 0, 70: 0, 71: 0, 72: 0, 73: 7, 74: 7, 75: 7, 76: 7, 77: 7, 78: 7,\n    79: 2, 80: 2, 81: 0, 82: 0, 83: 0\n}\nmatrix['supercategory_id'] = matrix['item_category_id'].map(supercat_map)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Shop city\n(from https://www.kaggle.com/dlarionov/feature-engineering-xgboost)","metadata":{"papermill":{"duration":0.054288,"end_time":"2021-04-28T18:13:08.800954","exception":false,"start_time":"2021-04-28T18:13:08.746666","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def add_city_codes(matrix, shops):\n    shops.loc[\n        shops.shop_name == 'Сергиев Посад ТЦ \"7Я\"', \"shop_name\"\n    ] = 'СергиевПосад ТЦ \"7Я\"'\n    shops[\"city\"] = shops[\"shop_name\"].str.split(\" \").map(lambda x: x[0])\n    shops.loc[shops.city == \"!Якутск\", \"city\"] = \"Якутск\"\n    shops[\"city_code\"] = shops[\"city\"].factorize()[0]\n    shop_labels = shops[[\"shop_id\", \"city_code\"]]\n    matrix = matrix.merge(shop_labels, on='shop_id', how='left')\n    return matrix\n\nmatrix = add_city_codes(matrix, shops)\ndel(shops)","metadata":{"papermill":{"duration":1.896395,"end_time":"2021-04-28T18:13:10.753735","exception":false,"start_time":"2021-04-28T18:13:08.85734","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Shop and item category clustering\n\nShops and item categories are grouped into clusters according to their sales profiles. \nThe following function performs and plots the results of a principle component analysis decomposition and clustering of the shops and item categories.\n\nThe proportion of explained variance between items explained by each of the PCA dimensions is plotted, and the individual items are plotted according to their scores on the PCA dimensions and coloured according to their cluster assignment.\n\nThe silhouette score (a metric of clustering quality) for different values of the cluster number parameter is also plotted. These plots were used to decide the number of clusters.\n\nFor both shops and item categories, over 80% of differences occur on a single dimension, indicating that differences are mainly in magnitude rather than proportion. The item component score plots show that the clustering is mainly into a large cluster containing the large majority of items, and a few clusters containing outlier items.","metadata":{"papermill":{"duration":0.053194,"end_time":"2021-04-28T18:13:10.860623","exception":false,"start_time":"2021-04-28T18:13:10.807429","status":"completed"},"tags":[]}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.cluster import AgglomerativeClustering\nfrom sklearn.decomposition import PCA\nfrom sklearn.metrics import silhouette_score\n\n\ndef cluster_feature(matrix, target_feature, clust_feature, level_feature, n_components=4, n_clusters=5, aggfunc=\"mean\", exclude=None):\n    start_month = 20\n    end_month = 32\n    pt = matrix.query(f\"date_block_num>{start_month} & date_block_num<={end_month}\")\n    if exclude is not None:\n        pt = matrix[~matrix[clust_feature].isin(exclude)]\n    pt = pt.pivot_table(values=target_feature, columns=clust_feature, index=level_feature, fill_value=0, aggfunc=aggfunc)\n    pt = pt.transpose()\n    pca = PCA(n_components=10)\n    components = pca.fit_transform(pt)\n    components = pd.DataFrame(components)\n    # Plot PCA explained variance\n    sns.set_theme()\n    features = list(range(pca.n_components_))\n    fig = plt.figure(figsize=(10,4))\n    ax = fig.add_subplot(121)\n#     ax.bar(features, pca.explained_variance_ratio_, color=\"black\")\n    sns.barplot(x=features, y=pca.explained_variance_ratio_, ax=ax)\n    plt.title(\"Variance by PCA components\")\n    plt.xlabel(\"component\")\n    plt.ylabel(\"explained variance\")\n    plt.xticks(features)\n\n    scorelist = []\n    nrange = range(2, 10)\n    for n in nrange:\n        clusterer = AgglomerativeClustering(n_clusters=n)\n        labels = clusterer.fit_predict(components)\n        silscore = silhouette_score(pt, labels)\n        scorelist.append(silscore)\n    ax = fig.add_subplot(122)\n    sns.lineplot(x=nrange, y=scorelist, ax=ax)\n    plt.title(\"Clustering quality by number of clusters\")\n    plt.xlabel(\"n clusters\")\n    plt.ylabel(\"silhouette score\")\n\n    pca = PCA(n_components=n_components)\n    components = pca.fit_transform(pt)\n    components = pd.DataFrame(components)\n    clusterer = AgglomerativeClustering(n_clusters=n_clusters, linkage=\"average\")\n    labels = clusterer.fit_predict(components)\n    x = components[0]\n    y = components[1]\n    fig = plt.figure(figsize=(10, 4))\n    ax = fig.add_subplot(111)\n    sns.scatterplot(x=x, y=y, hue=labels, palette=sns.color_palette(\"hls\", n_clusters), ax=ax)\n    plt.title(\"Items by cluster\")\n    plt.xlabel(\"component 1 score\")\n    plt.ylabel(\"component 2 score\")\n    for i, txt in enumerate(pt.index.to_list()):\n        ax.annotate(str(txt), (x[i], y[i]))\n    groups = {}\n    for i, s in enumerate(pt.index):\n        groups[s] = labels[i]\n    return groups","metadata":{"papermill":{"duration":0.29023,"end_time":"2021-04-28T18:13:11.205754","exception":false,"start_time":"2021-04-28T18:13:10.915524","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Item categories are clustered according to their mean sales in each month of the year. The principle component plot shows that 3 categories are outliers in this respect.","metadata":{"papermill":{"duration":0.053194,"end_time":"2021-04-28T18:13:10.860623","exception":false,"start_time":"2021-04-28T18:13:10.807429","status":"completed"},"tags":[]}},{"cell_type":"code","source":"category_group_dict = cluster_feature(matrix, 'item_cnt_month', 'item_category_id', 'date_block_num', n_components=2, n_clusters=4, aggfunc=\"mean\", exclude =[])\nmatrix['category_cluster'] = matrix['item_category_id'].map(category_group_dict)","metadata":{"papermill":{"duration":3.980666,"end_time":"2021-04-28T18:13:15.23941","exception":false,"start_time":"2021-04-28T18:13:11.258744","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Shops are clustered by their summed sales of each item category. The principle component plots show that shops mainly differ in the magnitude of their sales, with shop 31 being an outlier due to the volume of its sales. Shops 12 and 55 are outliers on an orthogonal dimension because they sell different (online only) items.","metadata":{}},{"cell_type":"code","source":"shop_group_dict = cluster_feature(matrix, 'item_cnt_month', 'shop_id', 'item_category_id', n_components=4, n_clusters=4, aggfunc=\"mean\", exclude=[36])\nshop_group_dict[36] = shop_group_dict[37]  # Shop36 added separately because it only has one month of data\nmatrix['shop_cluster'] = matrix['shop_id'].map(shop_group_dict)","metadata":{"papermill":{"duration":3.477294,"end_time":"2021-04-28T18:13:18.77379","exception":false,"start_time":"2021-04-28T18:13:15.296496","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gc.collect()\nmatrix, oldcols = shrink_mem_new_cols(matrix, oldcols)  # Use this function periodically to downcast dtypes to save memory","metadata":{"papermill":{"duration":9.033337,"end_time":"2021-04-28T18:13:27.865241","exception":false,"start_time":"2021-04-28T18:13:18.831904","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Number of unique item features\n\nThese features count the number of unique items sharing the same value of a grouping feature or set of features as the current item in the current month, e.g. number of new items in the same category.  \n\nThis could considered to be a kind of data leakage feature, as the set of items in each month (and therefore the test set) is determined by whether each item recorded a sale or not in the month being predicted, which isn't known in advance.","metadata":{"papermill":{"duration":0.058948,"end_time":"2021-04-28T18:13:27.984695","exception":false,"start_time":"2021-04-28T18:13:27.925747","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def uniques(matrix, groupers, name, limitation=None):\n    if limitation is not None:\n        s = (\n            matrix.query(limitation)\n            .groupby(groupers)\n            .item_id.nunique()\n            .rename(name)\n            .reset_index()\n        )\n    else:\n        s = matrix.groupby(groupers).item_id.nunique().rename(name).reset_index()\n    matrix = matrix.merge(s, on=groupers, how=\"left\")\n    matrix[name] = matrix[name].fillna(0)\n    return matrix\n\n\nmatrix = uniques(matrix, [\"date_block_num\"], \"unique_items_month\")\n\nmatrix = uniques(matrix, [\"date_block_num\", \"item_name_group\"], \"name_group_unique_month\")\nmatrix = uniques(\n    matrix,\n    [\"date_block_num\", \"item_category_id\", \"item_name_group\"],\n    \"name_group_cat_unique_month\",\n)\nmatrix = uniques(\n    matrix,\n    [\"date_block_num\", \"item_name_group\"],\n    \"name_group_new_unique_month\",\n    limitation=\"new_item==True\",\n)\nmatrix = uniques(\n    matrix,\n    [\"date_block_num\", \"item_category_id\", \"item_name_group\"],\n    \"name_group_new_cat_unique_month\",\n    limitation=\"new_item==True\",\n)\n\nmatrix = uniques(\n    matrix, [\"date_block_num\", \"artist_name_or_first_word\"], \"first_word_unique_month\"\n)\nmatrix = uniques(\n    matrix,\n    [\"date_block_num\", \"item_category_id\", \"artist_name_or_first_word\"],\n    \"first_word_cat_unique_month\",\n)\nmatrix = uniques(\n    matrix,\n    [\"date_block_num\", \"artist_name_or_first_word\"],\n    \"first_word_new_unique_month\",\n    limitation=\"new_item==True\",\n)\nmatrix = uniques(\n    matrix,\n    [\"date_block_num\", \"item_category_id\", \"artist_name_or_first_word\"],\n    \"first_word_new_cat_unique_month\",\n    limitation=\"new_item==True\",\n)\n\nmatrix = uniques(matrix, [\"date_block_num\", \"item_category_id\"], \"unique_items_cat\")\nmatrix = uniques(\n    matrix,\n    [\"date_block_num\", \"item_category_id\"],\n    \"new_items_cat\",\n    limitation=\"new_item==True\",\n)\nmatrix = uniques(matrix, [\"date_block_num\"], \"new_items_month\", limitation=\"new_item==True\")\n\nmatrix[\"cat_items_proportion\"] = matrix[\"unique_items_cat\"] / matrix[\"unique_items_month\"]\nmatrix[\"name_group_new_proportion_month\"] = (\n    matrix[\"name_group_new_unique_month\"] / matrix[\"name_group_unique_month\"]\n)\n\nmatrix = matrix.drop(columns=[\"unique_items_month\", \"name_group_unique_month\"])","metadata":{"papermill":{"duration":1.661615,"end_time":"2021-04-28T18:14:25.413563","exception":false,"start_time":"2021-04-28T18:14:23.751948","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"matrix, oldcols = shrink_mem_new_cols(matrix, oldcols)","metadata":{"papermill":{"duration":5.877312,"end_time":"2021-04-28T18:14:31.349756","exception":false,"start_time":"2021-04-28T18:14:25.472444","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Percentage change in an aggregate feature  \nThis uses the pandas pct_change method to calculate the proportional change in mean sales count for a specific grouping for a specific time interval, e.g. increase / decrease in mean sales of an item between the last 2 months.","metadata":{"papermill":{"duration":0.058252,"end_time":"2021-04-28T18:14:31.467011","exception":false,"start_time":"2021-04-28T18:14:31.408759","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def add_pct_change(\n    matrix,\n    group_feats,\n    target=\"item_cnt_month\",\n    aggfunc=\"mean\",\n    periods=1,\n    lag=1,\n    clip_value=None,\n):\n    periods = list_if_not(periods, int)\n    group_feats = list_if_not(group_feats)\n    group_feats_full = [\"date_block_num\"] + group_feats\n    dat = matrix.pivot_table(\n        index=group_feats + [\"date_block_num\"],\n        values=target,\n        aggfunc=aggfunc,\n        fill_value=0,\n        dropna=False,\n    ).astype(\"float32\")\n    for g in group_feats:\n        firsts = matrix.groupby(g).date_block_num.min().rename(\"firsts\")\n        dat = dat.merge(firsts, left_on=g, right_index=True, how=\"left\")\n        dat.loc[dat.index.get_level_values(\"date_block_num\") < dat[\"firsts\"], target] = float(\n            \"nan\"\n        )\n        del dat[\"firsts\"]\n    for period in periods:\n        feat_name = \"_\".join(\n            group_feats + [target] + [aggfunc] + [\"delta\"] + [str(period)] + [f\"lag_{lag}\"]\n        )\n        print(f\"Adding feature {feat_name}\")\n        dat = (\n            dat.groupby(group_feats)[target]\n            .transform(lambda x: x.pct_change(periods=period, fill_method=\"pad\"))\n            .rename(feat_name)\n        )\n        if clip_value is not None:\n            dat = dat.clip(lower=-clip_value, upper=clip_value)\n    dat = dat.reset_index()\n    dat[\"date_block_num\"] += lag\n    matrix = matrix.merge(dat, on=[\"date_block_num\"] + group_feats, how=\"left\")\n    matrix[feat_name] = reduce_mem_usage(matrix[feat_name])\n    return matrix","metadata":{"papermill":{"duration":0.074182,"end_time":"2021-04-28T18:14:31.600743","exception":false,"start_time":"2021-04-28T18:14:31.526561","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"matrix = add_pct_change(matrix, [\"item_id\"], \"item_cnt_month\", clip_value=3)\nmatrix = add_pct_change(matrix, [\"item_category_id\"], \"item_cnt_month\", clip_value=3)\nmatrix = add_pct_change(matrix, [\"item_name_group\"], \"item_cnt_month\", clip_value=3)\n# Delta 1 feature lagged by 12 months, intended to capture seasonal trends\nmatrix = add_pct_change(matrix, [\"item_category_id\"], \"item_cnt_month\", lag=12, clip_value=3,)\ngc.collect()","metadata":{"papermill":{"duration":55.239396,"end_time":"2021-04-28T18:15:26.899005","exception":false,"start_time":"2021-04-28T18:14:31.659609","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"matrix, oldcols = shrink_mem_new_cols(matrix, oldcols)","metadata":{"papermill":{"duration":1.450712,"end_time":"2021-04-28T18:15:28.412578","exception":false,"start_time":"2021-04-28T18:15:26.961866","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Windowed aggregates\n\nFeatures aggregated over a specific window to reduce noise. Available windows are expanding (i.e. all preceding timepoints), rolling (i.e. fixed number of equally weighted timepoints) and exponentially weighted mean.  \n\n\nA note about feature names: these are set automatically according to the pattern < grouping features > - < aggregated features > - < monthly aggregation function > - < window type > , where < window type > is either \"rolling - < window aggregation function > - win - < window length in months >\" for square rolling windows, \"expanding - < window aggregation function >\" for expanding windows, and \"ewm_hl - < decay rate in terms of half-life > for exponential weighted means, all connected by underscores.","metadata":{"papermill":{"duration":0.063575,"end_time":"2021-04-28T18:15:28.537486","exception":false,"start_time":"2021-04-28T18:15:28.473911","status":"completed"},"tags":[]}},{"cell_type":"code","source":"shop_id = 16\nitem_id = 482\nim = matrix.query(f\"shop_id=={shop_id} & item_id=={item_id}\")[['date_block_num', 'item_cnt_month']]\nim['moving average'] = im['item_cnt_month'].ewm(halflife=1).mean()\nim['expanding mean'] = im['item_cnt_month'].expanding().mean()\nim['rolling 12 month mean'] = im['item_cnt_month'].rolling(window=12, min_periods=1).mean()\nim = im.set_index('date_block_num')\nax = im.plot(figsize=(12,5), marker='.', title='Time series averaging methods')","metadata":{"papermill":{"duration":0.452093,"end_time":"2021-04-28T18:15:29.05061","exception":false,"start_time":"2021-04-28T18:15:28.598517","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def add_rolling_stats(\n    matrix,\n    features,\n    window=12,\n    kind=\"rolling\",\n    argfeat=\"item_cnt_month\",\n    aggfunc=\"mean\",\n    rolling_aggfunc=\"mean\",\n    dtype=\"float16\",\n    reshape_source=True,\n    lag_offset=0,\n):\n    def rolling_stat(\n        matrix,\n        source,\n        feats,\n        feat_name,\n        window=12,\n        argfeat=\"item_cnt_month\",\n        aggfunc=\"mean\",\n        dtype=dtype,\n        lag_offset=0,\n    ):\n        # Calculate a statistic on a windowed section of a source table,  grouping on specific features\n        store = []\n        for i in range(2 + lag_offset, 35 + lag_offset):\n            if len(feats) > 0:\n                mes = (\n                    source[source.date_block_num.isin(range(max([i - window, 0]), i))]\n                    .groupby(feats)[argfeat]\n                    .agg(aggfunc)\n                    .astype(dtype)\n                    .rename(feat_name)\n                    .reset_index()\n                )\n            else:\n                mes = {}\n                mes[feat_name] = (\n                    source.loc[\n                        source.date_block_num.isin(range(max([i - window, 0]), i)), argfeat\n                    ]\n                    .agg(aggfunc)\n                    .astype(dtype)\n                )\n                mes = pd.DataFrame(data=mes, index=[i])\n            mes[\"date_block_num\"] = i - lag_offset\n            store.append(mes)\n        store = pd.concat(store)\n        matrix = matrix.merge(store, on=feats + [\"date_block_num\"], how=\"left\")\n        return matrix\n\n    \"\"\" An issue when using windowed functions is that missing values from months when items recorded no sales are skipped rather than being correctly\n    treated as zeroes. Creating a pivot_table fills in the zeros.\"\"\"\n    if (reshape_source == True) or (kind == \"ewm\"):\n        source = matrix.pivot_table(\n            index=features + [\"date_block_num\"],\n            values=argfeat,\n            aggfunc=aggfunc,\n            fill_value=0,\n            dropna=False,\n        ).astype(dtype)\n        for g in features:\n            firsts = matrix.groupby(g).date_block_num.min().rename(\"firsts\")\n            source = source.merge(firsts, left_on=g, right_index=True, how=\"left\")\n            # Set values before the items first appearance to nan so they are ignored rather than being treated as zero sales.\n            source.loc[\n                source.index.get_level_values(\"date_block_num\") < source[\"firsts\"], argfeat\n            ] = float(\"nan\")\n            del source[\"firsts\"]\n        source = source.reset_index()\n    else:\n        source = matrix\n\n    if kind == \"rolling\":\n        feat_name = (\n            f\"{'_'.join(features)}_{argfeat}_{aggfunc}_rolling_{rolling_aggfunc}_win_{window}\"\n        )\n        print(f'Creating feature \"{feat_name}\"')\n        return rolling_stat(\n            matrix,\n            source,\n            features,\n            feat_name,\n            window=window,\n            argfeat=argfeat,\n            aggfunc=rolling_aggfunc,\n            dtype=dtype,\n            lag_offset=lag_offset,\n        )\n    elif kind == \"expanding\":\n        feat_name = f\"{'_'.join(features)}_{argfeat}_{aggfunc}_expanding_{rolling_aggfunc}\"\n        print(f'Creating feature \"{feat_name}\"')\n        return rolling_stat(\n            matrix,\n            source,\n            features,\n            feat_name,\n            window=100,\n            argfeat=argfeat,\n            aggfunc=aggfunc,\n            dtype=dtype,\n            lag_offset=lag_offset,\n        )\n    elif kind == \"ewm\":\n        feat_name = f\"{'_'.join(features)}_{argfeat}_{aggfunc}_ewm_hl_{window}\"\n        print(f'Creating feature \"{feat_name}\"')\n        source[feat_name] = (\n            source.groupby(features)[argfeat]\n            .ewm(halflife=window, min_periods=1)\n            .agg(rolling_aggfunc)\n            .to_numpy(dtype=dtype)\n        )\n        del source[argfeat]\n        #         source = source.reset_index()\n        source[\"date_block_num\"] += 1 - lag_offset\n        return matrix.merge(source, on=[\"date_block_num\"] + features, how=\"left\")","metadata":{"papermill":{"duration":0.088969,"end_time":"2021-04-28T18:15:29.204666","exception":false,"start_time":"2021-04-28T18:15:29.115697","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Create rolling mean features. The combinations of grouping features and window types here were selected by generating a large number of features and then pruning them with the scikit-learn RFECV function.","metadata":{"papermill":{"duration":0.063375,"end_time":"2021-04-28T18:15:29.331871","exception":false,"start_time":"2021-04-28T18:15:29.268496","status":"completed"},"tags":[]}},{"cell_type":"code","source":"matrix = add_rolling_stats(\n    matrix,\n    [\"shop_id\", \"artist_name_or_first_word\", \"item_category_id\", \"item_age\"],\n    window=12,\n    reshape_source=False,\n)\nmatrix = add_rolling_stats(\n    matrix,\n    [\"shop_id\", \"artist_name_or_first_word\", \"item_category_id\", \"new_item\"],\n    kind=\"expanding\",\n    reshape_source=False,\n)\nmatrix = add_rolling_stats(\n    matrix,\n    [\"shop_id\", \"artist_name_or_first_word\", \"new_item\"],\n    kind=\"expanding\",\n    reshape_source=False,\n)\nmatrix = add_rolling_stats(matrix, [\"shop_id\", \"category_cluster\"], window=12)\nmatrix = add_rolling_stats(\n    matrix,\n    [\"shop_id\", \"item_category_id\", \"item_age\"],\n    kind=\"expanding\",\n    reshape_source=False,\n)\nmatrix = add_rolling_stats(\n    matrix, [\"shop_id\", \"item_category_id\", \"item_age\"], window=12, reshape_source=False\n)\nmatrix = add_rolling_stats(matrix, [\"shop_id\", \"item_category_id\"], kind=\"ewm\", window=1)\nmatrix = add_rolling_stats(\n    matrix,\n    [\"shop_id\", \"item_category_id\", \"new_item\"],\n    kind=\"expanding\",\n    reshape_source=False,\n)\nmatrix = add_rolling_stats(\n    matrix, [\"shop_id\", \"item_category_id\", \"new_item\"], window=12, reshape_source=False\n)\nmatrix = add_rolling_stats(matrix, [\"shop_id\"], window=12)\nmatrix = add_rolling_stats(matrix, [\"shop_id\", \"item_id\"], kind=\"ewm\", window=1)\nmatrix = add_rolling_stats(matrix, [\"shop_id\", \"item_id\"], window=12)\nmatrix = add_rolling_stats(\n    matrix,\n    [\"shop_id\", \"item_name_group\", \"item_category_id\", \"new_item\"],\n    window=12,\n    reshape_source=False,\n)\nmatrix = add_rolling_stats(\n    matrix, [\"shop_id\", \"item_name_group\", \"new_item\"], kind=\"expanding\", reshape_source=False\n)\nmatrix = add_rolling_stats(\n    matrix, [\"shop_id\", \"supercategory_id\", \"new_item\"], window=12, reshape_source=False\n)\n\nmatrix = add_rolling_stats(matrix, [\"shop_cluster\", \"item_id\"], kind=\"ewm\", window=1)\nmatrix = add_rolling_stats(\n    matrix,\n    [\"shop_cluster\", \"item_category_id\", \"item_age\"],\n    kind=\"expanding\",\n    reshape_source=False,\n)\nmatrix = add_rolling_stats(\n    matrix, [\"shop_cluster\", \"item_name_group\", \"new_item\"], window=12, reshape_source=False\n)\n\nmatrix = add_rolling_stats(\n    matrix, [\"category_cluster\", \"item_age\"], kind=\"expanding\", reshape_source=False\n)\nmatrix = add_rolling_stats(\n    matrix, [\"category_cluster\", \"new_item\"], kind=\"expanding\", reshape_source=False\n)\n\nmatrix = add_rolling_stats(matrix, [\"item_id\"], window=12)\n\nmatrix = add_rolling_stats(matrix, [\"artist_name_or_first_word\"], window=12)\nmatrix = add_rolling_stats(matrix, [\"artist_name_or_first_word\"], kind=\"ewm\", window=1)\nmatrix = add_rolling_stats(\n    matrix, [\"artist_name_or_first_word\", \"item_age\"], window=12, reshape_source=False\n)\nmatrix = add_rolling_stats(\n    matrix,\n    [\"artist_name_or_first_word\", \"item_category_id\", \"item_age\"],\n    window=12,\n    reshape_source=False,\n)\nmatrix = add_rolling_stats(\n    matrix, [\"artist_name_or_first_word\", \"new_item\"], kind=\"expanding\", reshape_source=False\n)\n\nmatrix = add_rolling_stats(\n    matrix, [\"item_category_id\", \"item_age\"], kind=\"expanding\", reshape_source=False\n)\nmatrix = add_rolling_stats(matrix, [\"item_category_id\"], window=12)\nmatrix = add_rolling_stats(matrix, [\"item_category_id\"], kind=\"ewm\", window=1)\nmatrix = add_rolling_stats(\n    matrix, [\"item_category_id\", \"new_item\"], kind=\"expanding\", reshape_source=False\n)\n\nmatrix = add_rolling_stats(\n    matrix, [\"item_name_group\", \"item_age\"], window=12, reshape_source=False\n)\nmatrix = add_rolling_stats(matrix, [\"item_name_group\"], kind=\"ewm\", window=1)\nmatrix = add_rolling_stats(matrix, [\"item_name_group\"], window=12)\n\nmatrix = add_rolling_stats(matrix, [\"platform_id\"], window=12)\nmatrix = add_rolling_stats(matrix, [\"platform_id\"], kind=\"ewm\", window=1)","metadata":{"papermill":{"duration":1905.447107,"end_time":"2021-04-28T18:47:14.842926","exception":false,"start_time":"2021-04-28T18:15:29.395819","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gc.collect()\nmatrix, oldcols = shrink_mem_new_cols(matrix, oldcols)","metadata":{"papermill":{"duration":17.789665,"end_time":"2021-04-28T18:47:32.707445","exception":false,"start_time":"2021-04-28T18:47:14.91778","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The following code block calculates windowed mean sales features with day resolution accuracy","metadata":{"papermill":{"duration":0.448793,"end_time":"2021-04-28T18:47:36.713076","exception":false,"start_time":"2021-04-28T18:47:36.264283","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Summed sales & accurate windowed mean sales per day features\nmatrix = add_rolling_stats(\n    matrix,\n    [\"shop_id\", \"item_id\"],\n    aggfunc=\"sum\",\n    rolling_aggfunc=\"sum\",\n    kind=\"rolling\",\n    window=12,\n    reshape_source=False,\n)\nmatrix = add_rolling_stats(\n    matrix,\n    [\"item_id\"],\n    aggfunc=\"sum\",\n    rolling_aggfunc=\"sum\",\n    kind=\"expanding\",\n    reshape_source=False,\n)\nmatrix[\"1year\"] = 365\nmatrix[\"item_id_day_mean_expanding\"] = matrix[\n    \"item_id_item_cnt_month_sum_expanding_sum\"\n] / matrix[[\"first_item_sale_days\"]].min(axis=1)\nmatrix[\"shop_id_item_id_day_mean_win_12\"] = matrix[\n    \"shop_id_item_id_item_cnt_month_sum_rolling_sum_win_12\"\n] / matrix[[\"first_item_sale_days\", \"shop_open_days\", \"1year\"]].min(axis=1)\nmatrix.loc[matrix.new_item == True, \"item_id_day_mean_expanding\",] = float(\"nan\")\nmatrix = matrix.drop(columns=[\"1year\", \"item_id_item_cnt_month_sum_expanding_sum\"])","metadata":{"papermill":{"duration":92.446203,"end_time":"2021-04-28T18:49:09.252422","exception":false,"start_time":"2021-04-28T18:47:36.806219","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Revenue features","metadata":{"papermill":{"duration":0.217115,"end_time":"2021-04-28T18:49:09.640666","exception":false,"start_time":"2021-04-28T18:49:09.423551","status":"completed"},"tags":[]}},{"cell_type":"code","source":"matrix = add_rolling_stats(\n    matrix,\n    [\"shop_id\", \"item_name_group\"],\n    window=12,\n    argfeat=\"item_revenue_month\",\n    dtype=\"float32\",\n)","metadata":{"papermill":{"duration":91.965703,"end_time":"2021-04-28T18:50:41.781069","exception":false,"start_time":"2021-04-28T18:49:09.815366","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Windowed mean unique item features and ratio of new items in category with mean over the previous year","metadata":{"papermill":{"duration":0.090066,"end_time":"2021-04-28T18:50:41.952101","exception":false,"start_time":"2021-04-28T18:50:41.862035","status":"completed"},"tags":[]}},{"cell_type":"code","source":"matrix = add_rolling_stats(\n    matrix,\n    [\"item_category_id\"],\n    argfeat=\"new_items_cat\",\n    window=12,\n    reshape_source=True,\n    lag_offset=1,\n)\nmatrix = add_rolling_stats(\n    matrix,\n    [\"item_name_group\"],\n    argfeat=\"name_group_new_unique_month\",\n    window=12,\n    reshape_source=True,\n    lag_offset=1,\n)\n\nmatrix[\"new_items_cat_1_12_ratio\"] = (\n    matrix[\"new_items_cat\"]\n    / matrix[\"item_category_id_new_items_cat_mean_rolling_mean_win_12\"]\n)","metadata":{"papermill":{"duration":28.595028,"end_time":"2021-04-28T18:51:10.635694","exception":false,"start_time":"2021-04-28T18:50:42.040666","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gc.collect()\nmatrix, oldcols = shrink_mem_new_cols(matrix, oldcols)","metadata":{"papermill":{"duration":7.661105,"end_time":"2021-04-28T18:51:18.734463","exception":false,"start_time":"2021-04-28T18:51:11.073358","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Lagged features and mean encodings  \nValues for the same shop-item combination from previous months","metadata":{"papermill":{"duration":0.108926,"end_time":"2021-04-28T18:51:19.025566","exception":false,"start_time":"2021-04-28T18:51:18.91664","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def simple_lag_feature(matrix, lag_feature, lags):\n    for lag in lags:\n        newname = lag_feature + f\"_lag_{lag}\"\n        print(f\"Adding feature {newname}\")\n        targetseries = matrix.loc[:, [\"date_block_num\", \"item_id\", \"shop_id\"] + [lag_feature]]\n        targetseries[\"date_block_num\"] += lag\n        targetseries = targetseries.rename(columns={lag_feature: newname})\n        matrix = matrix.merge(\n            targetseries, on=[\"date_block_num\", \"item_id\", \"shop_id\"], how=\"left\"\n        )\n        matrix.loc[\n            (matrix.item_age >= lag) & (matrix.shop_age >= lag) & (matrix[newname].isna()),\n            newname,\n        ] = 0\n    return matrix","metadata":{"papermill":{"duration":0.091696,"end_time":"2021-04-28T18:51:19.215776","exception":false,"start_time":"2021-04-28T18:51:19.12408","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"matrix = simple_lag_feature(matrix, 'item_cnt_month', lags=[1,2,3])\nmatrix = simple_lag_feature(matrix, 'item_cnt_day_avg', lags=[1, 2, 3])\nmatrix = simple_lag_feature(matrix, 'item_revenue_month', lags=[1])\ngc.collect()\nprint(\"Lag features created\")","metadata":{"papermill":{"duration":120.906823,"end_time":"2021-04-28T18:53:20.206272","exception":false,"start_time":"2021-04-28T18:51:19.299449","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Mean encodings\nThe mean or sum value of a target feature for each level of a categorical feature or combination of categorical features, lagged","metadata":{"papermill":{"duration":0.084107,"end_time":"2021-04-28T18:53:20.373538","exception":false,"start_time":"2021-04-28T18:53:20.289431","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def create_apply_ME(\n    matrix, grouping_fields, lags=[1], target=\"item_cnt_day_avg\", aggfunc=\"mean\"\n):\n    grouping_fields = list_if_not(grouping_fields)\n    for lag in lags:\n        newname = \"_\".join(grouping_fields + [target] + [aggfunc] + [f\"lag_{lag}\"])\n        print(f\"Adding feature {newname}\")\n        me_series = (\n            matrix.groupby([\"date_block_num\"] + grouping_fields)[target]\n            .agg(aggfunc)\n            .rename(newname)\n            .reset_index()\n        )\n        me_series[\"date_block_num\"] += lag\n        matrix = matrix.merge(me_series, on=[\"date_block_num\"] + grouping_fields, how=\"left\")\n        del me_series\n        matrix[newname] = matrix[newname].fillna(0)\n        for g in grouping_fields:\n            firsts = matrix.groupby(g).date_block_num.min().rename(\"firsts\")\n            matrix = matrix.merge(firsts, left_on=g, right_index=True, how=\"left\")\n            matrix.loc[\n                matrix[\"date_block_num\"] < (matrix[\"firsts\"] + (lag)), newname\n            ] = float(\"nan\")\n            del matrix[\"firsts\"]\n        matrix[newname] = reduce_mem_usage(matrix[newname])\n    return matrix","metadata":{"papermill":{"duration":0.093518,"end_time":"2021-04-28T18:53:20.548179","exception":false,"start_time":"2021-04-28T18:53:20.454661","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"matrix = create_apply_ME(matrix, [\"item_name_group\"], target=\"item_cnt_month\")\nmatrix = create_apply_ME(matrix, [\"item_name_group\"], target=\"item_cnt_month\", aggfunc=\"sum\")\nmatrix = create_apply_ME(matrix, [\"item_id\"], target=\"item_cnt_month\")\nmatrix = create_apply_ME(matrix, [\"item_id\"])\nmatrix = create_apply_ME(matrix, [\"platform_id\"])\nmatrix = create_apply_ME(matrix, [\"item_name_group\"])\nmatrix = create_apply_ME(matrix, [\"platform_id\"], target=\"item_cnt_month\")\nmatrix = create_apply_ME(matrix, [\"supercategory_id\"])\nmatrix = create_apply_ME(matrix, [\"item_category_id\", \"new_item\"], target=\"item_cnt_month\")\nmatrix = create_apply_ME(matrix, [\"shop_id\", \"item_category_id\"], target=\"item_cnt_month\")\nmatrix = create_apply_ME(matrix, [\"shop_cluster\", \"item_id\"], target=\"item_cnt_month\")\nmatrix = create_apply_ME(matrix, [\"shop_cluster\", \"item_id\"])\nmatrix = create_apply_ME(matrix, [\"city_code\", \"item_id\"])\nmatrix = create_apply_ME(matrix, [\"city_code\", \"item_name_group\"])","metadata":{"papermill":{"duration":116.204762,"end_time":"2021-04-28T18:55:16.832993","exception":false,"start_time":"2021-04-28T18:53:20.628231","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Ratios between lag 1 sales and rolling 12 month means, to capture decreases from previous means","metadata":{"papermill":{"duration":0.085228,"end_time":"2021-04-28T18:55:17.004209","exception":false,"start_time":"2021-04-28T18:55:16.918981","status":"completed"},"tags":[]}},{"cell_type":"code","source":"matrix[\"item_id_item_cnt_1_12_ratio\"] = (\n    matrix[\"item_id_item_cnt_month_mean_lag_1\"]\n    / matrix[\"item_id_item_cnt_month_mean_rolling_mean_win_12\"]\n)\nmatrix[\"shop_id_item_id_item_cnt_1_12_ratio\"] = (\n    matrix[\"item_cnt_day_avg_lag_1\"] / matrix[\"shop_id_item_id_day_mean_win_12\"]\n)","metadata":{"papermill":{"duration":0.12325,"end_time":"2021-04-28T18:55:17.213537","exception":false,"start_time":"2021-04-28T18:55:17.090287","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"matrix, oldcols = shrink_mem_new_cols(matrix, oldcols)\nmatrix.to_pickle(\"matrixcheckpoint.pkl\")\nprint(\"Saved matrixcheckpoint\")\ngc.collect()\nprint(\"Mean encoding features created\")","metadata":{"papermill":{"duration":11.498338,"end_time":"2021-04-28T18:55:28.796406","exception":false,"start_time":"2021-04-28T18:55:17.298068","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Some columns that were used to generate other features can now be discarded.","metadata":{"papermill":{"duration":2.295401,"end_time":"2021-04-28T18:55:46.830211","exception":false,"start_time":"2021-04-28T18:55:44.53481","status":"completed"},"tags":[]}},{"cell_type":"code","source":"surplus_columns = [\n    \"item_revenue_month\",\n    \"item_cnt_day_avg\",\n    \"item_name_group\",\n    \"artist_name_or_first_word\",\n    \"item_age\",\n    \"shop_open_days\",\n    \"shop_age\",\n    \"platform_id\",\n    \"supercategory_id\",\n    \"city_code\",\n    \"category_cluster\",\n    \"shop_cluster\",\n    \"new_items_cat\",\n    \"shop_id_item_id_day_mean_win_12\",\n    \"item_id_item_cnt_month_mean_rolling_mean_win_12\",\n]\nmatrix = matrix.drop(columns=surplus_columns)","metadata":{"papermill":{"duration":5.783446,"end_time":"2021-04-28T18:55:54.944188","exception":false,"start_time":"2021-04-28T18:55:49.160742","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Predictive words in item_name\n\nOne-hot features are made for words in the item_name field that are predictive of item sales.  \n\nTo select *k* word features from the 1000's of words found in item names, words are discarded if they are not in the names of a threshold number of items, or are not in the names of new items in the test or validation months. Remaining words are then selected by the scikit-learn SelectKBest function according to their correlation with the target.","metadata":{"papermill":{"duration":0.084959,"end_time":"2021-04-28T18:55:55.115949","exception":false,"start_time":"2021-04-28T18:55:55.03099","status":"completed"},"tags":[]}},{"cell_type":"code","source":"import re\nimport warnings\n\nwarnings.filterwarnings(\"ignore\", module=\"sklearn\")\n\nfrom nltk.corpus import stopwords\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_selection import SelectKBest, f_regression\n\n\ndef name_token_feats(matrix, items, k=50, item_n_threshold=5, target_month_start=33):\n    def name_correction(st):\n        st = re.sub(r\"[^\\w\\s]\", \"\", st)\n        st = re.sub(r\"\\s{2,}\", \" \", st)\n        st = st.lower().strip()\n        return st\n\n    items[\"item_name_clean\"] = items[\"item_name\"].apply(name_correction)\n\n    def create_item_id_bow_matrix(items):\n        all_stopwords = stopwords.words(\"russian\")\n        all_stopwords = all_stopwords + stopwords.words(\"english\")\n\n        vectorizer = CountVectorizer(stop_words=all_stopwords)\n        X = vectorizer.fit_transform(items.loc[:, \"item_name_clean\"])\n        X = pd.DataFrame.sparse.from_spmatrix(X)\n        print(f\"{len(vectorizer.vocabulary_)} words found in all items\")\n        featuremap = {\n            col: \"word_\" + token\n            for col, token in zip(\n                range(len(vectorizer.vocabulary_)), vectorizer.get_feature_names()\n            )\n        }\n        X = X.rename(columns=featuremap)\n        return X\n\n    items_bow = create_item_id_bow_matrix(items)\n    items_bow = items_bow.clip(0, 1)  # Made the word counts binary\n    common_word_mask = items_bow.sum(axis=0) > item_n_threshold\n    target_items = matrix.query(\n        f\"date_block_num>={target_month_start} & new_item==True\"\n    ).item_id.unique()\n    target_item_mask = items_bow.loc[target_items, :].sum(axis=0) > 1\n    items_bow = items_bow.loc[:, common_word_mask & target_item_mask]\n    print(f\"{items_bow.shape[1]} words of interest\")\n    mxbow = matrix[[\"date_block_num\", \"item_id\", \"item_cnt_month\"]].query(\"date_block_num<34\")\n    mxbow = mxbow.merge(items_bow, left_on=\"item_id\", right_index=True, how=\"left\")\n    X = mxbow.drop(columns=[\"date_block_num\", \"item_id\", \"item_cnt_month\"])\n    y = mxbow[\"item_cnt_month\"].clip(0, 20)\n    selektor = SelectKBest(f_regression, k=k)\n    selektor.fit(X, y)\n    tokencols = X.columns[selektor.get_support()]\n    print(f\"{k} word features selected\")\n    return items_bow[tokencols]","metadata":{"papermill":{"duration":0.117744,"end_time":"2021-04-28T18:55:55.579227","exception":false,"start_time":"2021-04-28T18:55:55.461483","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"items = pd.read_csv(\"../input/competitive-data-science-predict-future-sales/items.csv\")\nword_frame = name_token_feats(matrix, items, k=50, item_n_threshold=5)\nmatrix = matrix.merge(word_frame, left_on='item_id', right_index=True, how='left')\n# LightGBM didn't seem to work with sparse features in this case, so we'll convert them to int\nsparsecols = [c for c in matrix.columns if pd.api.types.is_sparse(matrix[c].dtype)]\nmatrix[sparsecols] = matrix[sparsecols].sparse.to_dense().astype('int8')","metadata":{"papermill":{"duration":1213.623185,"end_time":"2021-04-28T19:16:09.288159","exception":false,"start_time":"2021-04-28T18:55:55.664974","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The final feature frame is saved and the notebook kernel is reset to free up memory for LightGBM.","metadata":{}},{"cell_type":"code","source":"gc.collect()\nmatrix.to_pickle(\"checkpoint_final.pkl\")\nprint(\"All features generated, dataframe saved\")","metadata":{"papermill":{"duration":8.028085,"end_time":"2021-04-28T19:16:22.397834","exception":false,"start_time":"2021-04-28T19:16:14.369749","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%reset -f","metadata":{"papermill":{"duration":0.44176,"end_time":"2021-04-28T19:16:23.862813","exception":false,"start_time":"2021-04-28T19:16:23.421053","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model fitting","metadata":{"papermill":{"duration":0.08846,"end_time":"2021-04-28T19:16:24.040872","exception":false,"start_time":"2021-04-28T19:16:23.952412","status":"completed"},"tags":[]}},{"cell_type":"code","source":"import pandas as pd","metadata":{"papermill":{"duration":0.098249,"end_time":"2021-04-28T19:16:24.227277","exception":false,"start_time":"2021-04-28T19:16:24.129028","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"The feature frame is loaded and the target is clipped to match the test items","metadata":{}},{"cell_type":"code","source":"matrix = pd.read_pickle(\"checkpoint_final.pkl\")\nmatrix['item_cnt_month'] = matrix['item_cnt_month'].clip(0,20)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Define a function to fit and return a lightgbm regressor with or without early stopping","metadata":{}},{"cell_type":"code","source":"import warnings\n\nwarnings.filterwarnings(\"ignore\", module=\"lightgbm\")\n\nimport lightgbm as lgbm\n\n\ndef fit_booster(\n    X_train,\n    y_train,\n    X_test=None,\n    y_test=None,\n    params=None,\n    test_run=False,\n    categoricals=[],\n    early_stopping=True,\n):\n    if params is None:\n        params = {\"learning_rate\": 0.1, \"subsample_for_bin\": 300000, \"n_estimators\": 50}\n\n    early_stopping_rounds = None\n    if early_stopping == True:\n        early_stopping_rounds = 30\n\n    if test_run:\n        eval_set = [(X_train, y_train)]\n    else:\n        eval_set = [(X_train, y_train), (X_test, y_test)]\n\n    booster = lgbm.LGBMRegressor(**params)\n\n    categoricals = [c for c in categoricals if c in X_train.columns]\n\n    booster.fit(\n        X_train,\n        y_train,\n        eval_set=eval_set,\n        eval_metric=[\"rmse\"],\n        verbose=100,\n        categorical_feature=categoricals,\n        early_stopping_rounds=early_stopping_rounds,\n    )\n\n    return booster","metadata":{"papermill":{"duration":0.258176,"end_time":"2021-04-28T19:16:49.761566","exception":false,"start_time":"2021-04-28T19:16:49.50339","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Split train and validation sets from the feature matrix, month 33 used as validation set","metadata":{"papermill":{"duration":20.293087,"end_time":"2021-04-28T19:16:49.414172","exception":false,"start_time":"2021-04-28T19:16:29.121085","status":"completed"},"tags":[]}},{"cell_type":"code","source":"keep_from_month = 2  # The first couple of months are dropped because of distortions to their features (e.g. wrong item age)\ntest_month = 33\ndropcols = [\n    \"shop_id\",\n    \"item_id\",\n    \"new_item\",\n]  # The features are dropped to reduce overfitting\n\nvalid = matrix.drop(columns=dropcols).loc[matrix.date_block_num == test_month, :]\ntrain = matrix.drop(columns=dropcols).loc[matrix.date_block_num < test_month, :]\ntrain = train[train.date_block_num >= keep_from_month]\nX_train = train.drop(columns=\"item_cnt_month\")\ny_train = train.item_cnt_month\nX_valid = valid.drop(columns=\"item_cnt_month\")\ny_valid = valid.item_cnt_month\ndel(matrix, valid, train)","metadata":{"papermill":{"duration":9.884487,"end_time":"2021-04-28T19:16:59.734499","exception":false,"start_time":"2021-04-28T19:16:49.850012","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"These hyperparameters were found by using the hyperparameter optimization framework Optuna to optimize hyperparameters for the validation set.","metadata":{}},{"cell_type":"code","source":"params = {\n    \"num_leaves\": 966,\n    \"cat_smooth\": 45.01680827234465,\n    \"min_child_samples\": 27,\n    \"min_child_weight\": 0.021144950289224463,\n    \"max_bin\": 214,\n    \"learning_rate\": 0.01,\n    \"subsample_for_bin\": 300000,\n    \"min_data_in_bin\": 7,\n    \"colsample_bytree\": 0.8,\n    \"subsample\": 0.6,\n    \"subsample_freq\": 5,\n    \"n_estimators\": 8000,\n}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Fit the booster using early stopping with the validation set","metadata":{}},{"cell_type":"code","source":"categoricals = [\n    \"item_category_id\",\n    \"month\",\n]  # These features will be set as categorical features by LightGBM and handled differently\n\nlgbooster = fit_booster(\n    X_train,\n    y_train,\n    X_valid,\n    y_valid,\n    params=params,\n    test_run=False,\n    categoricals=categoricals,\n)","metadata":{"papermill":{"duration":1221.866881,"end_time":"2021-04-28T19:37:21.690596","exception":false,"start_time":"2021-04-28T19:16:59.823715","status":"completed"},"scrolled":true,"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Plot the feature importances ranked by error reduction on the training set.","metadata":{}},{"cell_type":"code","source":"_ = lgbm.plot_importance(lgbooster, figsize=(10,50), height=0.7, importance_type=\"gain\", max_num_features=50)","metadata":{"papermill":{"duration":3.892104,"end_time":"2021-04-28T19:37:25.679539","exception":false,"start_time":"2021-04-28T19:37:21.787435","status":"completed"},"tags":[],"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Save the trained model","metadata":{}},{"cell_type":"code","source":"import joblib\n_ = joblib.dump(lgbooster, \"trained_lgbooster.pkl\")\nprint(\"Saved single booster\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Simple ensembling with VotingRegressor  \n\nHere we make a simple ensembling predictor with the scikit-learn [VotingRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingRegressor.html) predictor. This wraps a list of predictors and outputs a linear combination of their predictions. In principle any scikit-learn compatible regression model can be used, but here we just use LightGBM models, as no other model types were efficient enough to run in a Kaggle notebook without memory allocation errors.","metadata":{}},{"cell_type":"code","source":"%reset -f\nimport pandas as pd","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Load and split the data for fitting. For the ensemble predictor the LightGBM models are fit on all the training data with a pre-set number of estimators.","metadata":{}},{"cell_type":"code","source":"matrix = pd.read_pickle(\"checkpoint_final.pkl\")\n# Downcast the float columns to reduce RAM usage\nfloatcols = [c for c in matrix.columns if matrix[c].dtype==\"float32\"]\nmatrix[floatcols] = matrix[floatcols].astype(\"float16\")\nmatrix['item_cnt_month'] = matrix['item_cnt_month'].clip(0,20)\nkeep_from_month = 2  # The first couple of months are dropped because of distortions to their features (e.g. wrong item age)\ntest_month = 34\ndropcols = [\n    \"shop_id\",\n    \"item_id\",\n    \"new_item\",\n]  # The features are dropped to reduce overfitting\ncategoricals = [\n    \"item_category_id\",\n    \"month\",\n]\nmatrix[categoricals] = matrix[categoricals].astype(\"category\") \ntest = matrix.drop(columns=dropcols).loc[matrix.date_block_num == test_month, :]\ntrain = matrix.drop(columns=dropcols).loc[matrix.date_block_num < test_month, :]\ntrain = train[train.date_block_num >= keep_from_month]\nX_train = train.drop(columns=\"item_cnt_month\")\ny_train = train.item_cnt_month\nX_test = test.drop(columns=\"item_cnt_month\")\ny_test = test.item_cnt_month\ndel(matrix, test, train, X_test, y_test)","metadata":{"papermill":{"duration":9.884487,"end_time":"2021-04-28T19:16:59.734499","exception":false,"start_time":"2021-04-28T19:16:49.850012","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Different parameters are used for each of 5 LightGBM models used in the ensemble. These parameters were the highest scoring parameters found by Optuna when optimizing on the validation date block 33.","metadata":{}},{"cell_type":"code","source":"best_params = [\n    {\n        \"num_leaves\": 966,\n        \"cat_smooth\": 45.01680827234465,\n        \"min_child_samples\": 27,\n        \"min_child_weight\": 0.021144950289224463,\n        \"max_bin\": 214,\n        \"n_estimators\": 500,\n        \"subsample_for_bin\": 300000,\n        \"learning_rate\": 0.01,\n        \"force_col_wise\": True\n    },\n    {\n        \"num_leaves\": 940,\n        \"cat_smooth\": 43.418286701105615,\n        \"min_child_samples\": 29,\n        \"min_child_weight\": 0.003944267312494195,\n        \"max_bin\": 133,\n        \"n_estimators\": 572,\n        \"subsample_for_bin\": 300000,\n        \"learning_rate\": 0.01,\n        \"force_col_wise\": True\n    },\n    {\n        \"num_leaves\": 971,\n        \"cat_smooth\": 40.103611531065525,\n        \"min_child_samples\": 30,\n        \"min_child_weight\": 0.03951287458923346,\n        \"max_bin\": 212,\n        \"n_estimators\": 828,\n        \"subsample_for_bin\": 300000,\n        \"learning_rate\": 0.01,\n        \"force_col_wise\": True\n    },\n    {\n        \"num_leaves\": 965,\n        \"cat_smooth\": 40.05144976454027,\n        \"min_child_samples\": 27,\n        \"min_child_weight\": 0.029220951478909872,\n        \"max_bin\": 211,\n        \"n_estimators\": 870,\n        \"subsample_for_bin\": 300000,\n        \"learning_rate\": 0.01,\n        \"force_col_wise\": True\n    },\n    {\n        \"num_leaves\": 961,\n        \"cat_smooth\": 40.013529776221134,\n        \"min_child_samples\": 29,\n        \"min_child_weight\": 0.026526521644599493,\n        \"max_bin\": 210,\n        \"n_estimators\": 897,\n        \"subsample_for_bin\": 300000,\n        \"learning_rate\": 0.01,\n        \"force_col_wise\": True\n    },\n]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Create and fit the VotingRegressor ensemble on the training data. This takes a long time.","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import VotingRegressor\nfrom lightgbm import LGBMRegressor\nregressors = []\nfor i, params in enumerate(best_params):\n    booster = LGBMRegressor(**params)\n    regressors.append((f\"lgbr_{i}\", booster))\nvr = VotingRegressor(regressors, verbose=True)\nprint(\"Fitting voting regressor\")\nvr = vr.fit(X_train, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Serialize the trained regressor","metadata":{}},{"cell_type":"code","source":"import joblib\n_ = joblib.dump(vr, \"trained_votingregressor.pkl\")\nprint(\"Voting regressor trained and saved\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Create the test submission  \nSplit the test items from the data matrix and use the trained voting regressor to predict the target.","metadata":{"papermill":{"duration":0.111863,"end_time":"2021-04-28T19:37:25.904072","exception":false,"start_time":"2021-04-28T19:37:25.792209","status":"completed"},"tags":[]}},{"cell_type":"code","source":"%reset -f","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport joblib\nbooster = joblib.load(\"trained_votingregressor.pkl\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"matrix = pd.read_pickle(\"checkpoint_final.pkl\")\nkeep_from_month = 2\ntest_month = 34\ndropcols = [\n    \"shop_id\",\n    \"item_id\",\n    \"new_item\",\n]  # The features are dropped to reduce overfitting\ncategoricals = [\n    \"item_category_id\",\n    \"month\",\n]\nmatrix[categoricals] = matrix[categoricals].astype(\"category\")\ntest = matrix.loc[matrix.date_block_num == test_month, :]\nX_test = test.drop(columns=\"item_cnt_month\")\ny_test = test.item_cnt_month\ndel matrix","metadata":{"papermill":{"duration":50.515225,"end_time":"2021-04-28T19:38:16.529825","exception":false,"start_time":"2021-04-28T19:37:26.0146","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_test[\"item_cnt_month\"] = booster.predict(X_test.drop(columns=dropcols)).clip(0, 20)\n# Merge the predictions with the provided template\ntest_orig = pd.read_csv(\"../input/competitive-data-science-predict-future-sales/test.csv\")\ntest = test_orig.merge(\n    X_test[[\"shop_id\", \"item_id\", \"item_cnt_month\"]],\n    on=[\"shop_id\", \"item_id\"],\n    how=\"inner\",\n    copy=True,\n)\n# Verify that the indices of the submission match the original\nassert test_orig.equals(test[[\"ID\", \"shop_id\", \"item_id\"]])\ntest[[\"ID\", \"item_cnt_month\"]].to_csv(\"submission.csv\", index=False)","metadata":{"papermill":{"duration":9.884487,"end_time":"2021-04-28T19:16:59.734499","exception":false,"start_time":"2021-04-28T19:16:49.850012","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from os import remove\nremove(\"checkpoint_final.pkl\")\nremove(\"matrixcheckpoint.pkl\")\nprint(\"Finished everything!\")","metadata":{"papermill":{"duration":0.122639,"end_time":"2021-04-28T19:38:27.697031","exception":false,"start_time":"2021-04-28T19:38:27.574392","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]}]}