{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Kaggle Competition: Predict Future Sales\n#### Available at: https://www.kaggle.com/c/competitive-data-science-predict-future-sales\n##### The dataset contains historical sales data, and requires a forecast of the total amount of products sold in every shop in the test set for the next month"},{"metadata":{},"cell_type":"markdown","source":"# Import Modules"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# Basic Modules\nimport numpy as np\nimport pandas as pd\n\n# Charting modules\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nfrom bokeh.models import ColumnDataSource, Label, LabelSet, Range1d, NumeralTickFormatter, Legend\nfrom bokeh.plotting import output_notebook, figure, show\nfrom bokeh.palettes import Category20\noutput_notebook()\n\n# Statistics\nfrom statsmodels.tsa.stattools import adfuller, kpss\n\n# Pre-processing\nfrom sklearn.preprocessing import StandardScaler\n\n# Machine Learning Models\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.linear_model import Lasso\nimport xgboost as xgb","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data import, EDA and model preparation"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import sales\nsales = pd.read_csv('../input/competitive-data-science-predict-future-sales/sales_train.csv')\nsales.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Compute monthly values that can be added to the items dataframe\nmonthly_item_sales = sales.groupby(['date_block_num', 'item_id'])[['item_cnt_day']].sum().reset_index()\n\nmonthly_item_min = monthly_item_sales.groupby(['item_id'])[['item_cnt_day']].min().reset_index()\nmonthly_item_min.rename(columns={'item_cnt_day':'Monthly Minimum (Items)'}, inplace=True)\nmonthly_item_mean= monthly_item_sales.groupby(['item_id'])[['item_cnt_day']].mean().reset_index()\nmonthly_item_mean.rename(columns={'item_cnt_day':'Monthly Average (Items)'}, inplace=True)\nmonthly_item_max= monthly_item_sales.groupby(['item_id'])[['item_cnt_day']].max().reset_index()\nmonthly_item_max.rename(columns={'item_cnt_day':'Monthly Maximum (Items)'}, inplace=True)\n\nmonthly_items = pd.merge(monthly_item_min, monthly_item_mean, on='item_id', how='left', sort=False)\nmonthly_items = pd.merge(monthly_items, monthly_item_max, on='item_id', how='left', sort=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import items and add monthly minimum, mean and maximum values which will be useful later\nitems = pd.read_csv('../input/competitive-data-science-predict-future-sales/items.csv')\nitems = pd.merge(items, monthly_items, on='item_id', how='left', sort=False)\nitems.drop(columns=['item_name'], inplace=True)\nitems.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import categories\n\n# Since I have translated and stored the english language categories, we will use that file instead\ncategories = pd.read_csv('../input/futuresaleswithenglishtranslation/categories_english.csv')\ncategories['category']=categories['item_category_english'].str.split(' - ').str[0]\ncategories['subcategory']=categories['item_category_english'].str.split(' - ').str[1]\ncategories.drop(columns=['item_category_name', 'item_category_english'], inplace=True)\n\n# Replace values that should be identical. I don't see specific sub-category items that can be combined\ncategories['category'].replace(to_replace='Movies', value='Movie', inplace=True)\ncategories['category'].replace(to_replace='Programs', value='Program', inplace=True)\ncategories['category'].replace(to_replace='Payment cards', value='Payment card', inplace=True)\ncategories['category'].replace(to_replace='Игры', value='Games', inplace=True)\ncategories.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import shops\n\n# I have also translated the shops info, so we will use the English version instead\nshops = pd.read_csv('../input/futuresaleswithenglishtranslation/shops_english.csv')\nshops['shop_name_clean'] = shops['shop_name_english'].str.replace('! ', '')\nshops['shop_name_clean'].replace(to_replace='Shop Online Emergencies', value='ShopOnline Emergencies', inplace=True)\nshops['shop_name_clean'].replace(to_replace='St. Petersburg TK \"Nevsky Center\"', value='St.Petersburg', inplace=True)\nshops['shop_name_clean'].replace(to_replace='St. Petersburg TK \"Sennaya\"', value='St.Petersburg', inplace=True)\nshops['city']=shops['shop_name_clean'].str.split(' ').str[0]\n#shops.drop(columns=['shop_name', 'shop_name_english', 'shop_name_clean'], inplace=True)\nshops.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# City value counts\ncity_counts = shops['city'].value_counts().reset_index()\ncity_counts.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot city counts\np = figure(plot_width=1200, plot_height=400, x_range=city_counts['index'], title='Number of Stores per City')\np.vbar(x=city_counts['index'], top=city_counts['city'], width=0.9)\np.xaxis.major_label_orientation = 3.1415/2\nshow(p)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Compute monthly values that can be added to the items dataframe\nmonthly_shop_sales = sales.groupby(['date_block_num', 'shop_id'])[['item_cnt_day']].sum().reset_index()\n\nmonthly_shop_min = monthly_shop_sales.groupby(['shop_id'])[['item_cnt_day']].min().reset_index()\nmonthly_shop_min.rename(columns={'item_cnt_day':'Monthly Minimum (Shop)'}, inplace=True)\nmonthly_shop_mean= monthly_shop_sales.groupby(['shop_id'])[['item_cnt_day']].mean().reset_index()\nmonthly_shop_mean.rename(columns={'item_cnt_day':'Monthly Average (Shop)'}, inplace=True)\nmonthly_shop_max= monthly_shop_sales.groupby(['shop_id'])[['item_cnt_day']].max().reset_index()\nmonthly_shop_max.rename(columns={'item_cnt_day':'Monthly Maximum (Shop)'}, inplace=True)\n\nmonthly_shop = pd.merge(monthly_shop_min, monthly_shop_mean, on='shop_id', how='left', sort=False)\nmonthly_shop = pd.merge(monthly_shop, monthly_shop_max, on='shop_id', how='left', sort=False)\n\nshops = pd.merge(shops, monthly_shop, on='shop_id', how='left', sort=False)\nshops.head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import test\ntest  = pd.read_csv('../input/competitive-data-science-predict-future-sales/test.csv')\ntest.head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Print column names\nprint('categories columns:', list(categories.columns))\nprint('items columns:', list(items.columns))\nprint('sales columns:', list(sales.columns))\nprint('shops columns:', list(shops.columns))\nprint('test columns:', list(test.columns))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Merge the dataframes to create a combined dataset\nall_items = pd.merge(items, categories, how='left', on='item_category_id', sort=False)\nall_sales = pd.merge(sales, shops, how='left', on='shop_id', sort=False)\ndata = pd.merge(all_sales, all_items, how='left', on='item_id', sort=False)\ndata['date'] = pd.to_datetime(data['date'])\ndata['YearMonth']=pd.to_datetime(data['date']).dt.to_period('m')\ndata['shop-item']= data['shop_id'].astype(str) + '-' + data['item_id'].astype(str)\ndata = data.sort_values(by=['date'])\n\ndata.head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plotting the number of unique items sold on any given day\np = figure(plot_width=800, plot_height=400, title='# of Transactions',x_axis_label='Date', \n           y_axis_label='# of Transactions',x_axis_type='datetime')\nvc = data['date'].value_counts()\np.vbar(x=vc.index, top=vc.values, width=0.5)\nshow(p)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Based on daily data, it is evident that we are missing a lot of data from 2015. We will end up filling the data with zeros, in effect assuming that there were no sales in any of the stores for the days where data is missing. However, that assumption does not appear to be accurate, especially since the December time-frame has always been a peak time for shopping. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Monthly data\n\n# first we aggregate the item count by month, then pivot on shop and item\nmonthlydata = data.groupby(['YearMonth', 'shop-item'])[['item_cnt_day']].sum()\nmonthlydata.reset_index(inplace=True)\nmonthlypivot = monthlydata.pivot(index='YearMonth', columns='shop-item', values='item_cnt_day').fillna(0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Monthly Data\nmonthly = monthlypivot.sum(axis=1)\n\nx = list(monthly.index.astype(str))\ntop = monthly.values/1000\nlabel = top.astype(int).astype(str)\n\nsource = ColumnDataSource(data=dict(x=x, top=top,label=label))\n\np = figure(plot_width=800, plot_height=450, x_range=x, title='Monthly Sales Quantity (in thousands)',x_axis_label='Time', \n           y_axis_label='Sales (in 1000s)', y_range=(0, 210))\np.vbar(x='x', top='top', width=0.8, source=source)\np.xaxis.major_label_orientation = 365\n\nlabels = LabelSet(x='x', y='top', text='label', level='glyph',x_offset=10, y_offset=2, source=source, render_mode='canvas', \n                  angle=3.14/2)\np.add_layout(labels)\np.xgrid.visible = False; p.ygrid.visible = False\nshow(p)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We now explore categories"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Exploring data by category\ncat_items = data.groupby(['YearMonth', 'category'])[['item_cnt_day']].sum().reset_index()\n\ncat_pivot = cat_items.pivot(index='YearMonth', columns='category', values='item_cnt_day').fillna(0)\ncat_pivot.columns.sort_values()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Total items sold by catgegory for each month\ncat_pivot.head(3)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"# Compute the correlation matrix\ncorr = cat_pivot.corr()\n\n#Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(15, 10))\n\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(corr, center=0, cmap='RdYlGn')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that movie, music, PC games and cinema are highly correlated, new carriers (piece/spire) are correalted, accessories are generally well-correlated with other variables. On the negative correlation side, PC & Android games are negatively correlated, which makes sense. Android games also do not appear to be interested in movies, music or cinema. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Top correlations positive and negative\ncorr_mtx = corr.reset_index()\ncorr_df = corr_mtx.melt(id_vars=['category'], var_name='category_name', value_name='Correlation')\ncorr_df = corr_df[corr_df['Correlation']<1].sort_values(by='Correlation', ascending=False)\ncoff_df_noduplicates = corr_df.drop_duplicates(subset='Correlation', keep='first', inplace=False)\n\n# We can print the positive and negative correlations between the various product category names\ncoff_df_noduplicates.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It seems like movies, music, PC games, cinema are all positively correlated. "},{"metadata":{"trusted":true},"cell_type":"code","source":"coff_df_noduplicates.tail(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here, Android games are most negatively correlated with other variables of interest."},{"metadata":{"trusted":true},"cell_type":"code","source":"p = figure(plot_width=800, plot_height=400, title='Items sold by Category', x_axis_label='Item Category ID', \n           y_axis_label='Items Sold', x_axis_type='datetime')\np.yaxis.formatter=NumeralTickFormatter(format=\"0,0\")\np.add_layout(Legend(), 'right')\np.legend.click_policy=\"hide\"\np.legend.label_text_font_size='8pt'\nfor n,c in zip(range(len(cat_pivot.columns)),Category20[20]):\n    x = cat_pivot.index\n    y = cat_pivot.iloc[:,n]\n    p.line(x=x,y=y, color=c, line_width=3, legend_label=cat_pivot.columns[n])\n\nshow(p)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that most categories have fairly stable quantities; either zero or somewhere less than 4000 per month. However, there are 5 categories that used to be sold in quantities in excess of 10k each month but all those quantities are now under 10k as well. Those 5 categories are Movie, Games PC, Games, Gifts and Music. "},{"metadata":{},"cell_type":"markdown","source":"#### Prepare final dataset for Modeling"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Add a row for the last month that will be used for predictions\nmonthlypivot.loc['2016-01']=0\nmonthlypivot.tail(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Since a vast majority of items have not been sold recently, we identify items that were recently sold\nretention_months = 12\nsale_quantity = 50\n\nsales2015 = monthlypivot.tail(retention_months).sum()\nshop_items2015 = sales2015[sales2015>sale_quantity]\nimportantitems = shop_items2015.index.values\n\n# Only retain shop-item IDs that are important\nimportantsales = monthlypivot[importantitems]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Unpivot dataframe\nmelted = importantsales.reset_index()\nmelted = melted.melt(id_vars='YearMonth', var_name='shop_item', value_name='item_cnt_day')\nmelted['YearMonth']=melted['YearMonth'].astype('str')\n\n# Separate out shop and item so other features can be added\nmelted['shop_id'] = melted['shop_item'].str.split('-').str[0].astype(int)\nmelted['item_id'] = melted['shop_item'].str.split('-').str[1].astype(int)\nmelted['Year']=melted['YearMonth'].str.split('-').str[0].astype(int)\nmelted['Month']=melted['YearMonth'].str.split('-').str[1].astype(int)\n\n# Add other important information\nitems_and_categories = pd.merge(items, categories, how='left', on='item_category_id', sort=False)\nmelted = pd.merge(melted, items_and_categories, how='left', on='item_id', sort=False)\nmelted = pd.merge(melted, shops, how='left', on='shop_id', sort=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Add lags of data\nmonths = melted['YearMonth'].astype('str').unique()\nproducts = melted['shop_item'].unique()\n\nlag_cols = ['lag1', 'lag2', 'lag3', 'lag4', 'lag5', 'lag6', 'lag7', 'lag8', 'lag9', 'lag10', 'lag11', 'lag12']\nmelted[lag_cols]=0\nfor l in range(len(lag_cols)):\n    for p in range(len(products)):\n        for m in range(len(months)-l-1):\n            val = melted.loc[(melted['shop_item']==products[p]) & (melted['YearMonth']==months[m]), 'item_cnt_day'].values\n            melted.loc[(melted['shop_item']==products[p]) & (melted['YearMonth']==months[m+l+1]), lag_cols[l]] = val","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"melted.head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Prepare final dataset\nmodeldata = melted.copy()\nmodeldata['YearMonth'] = modeldata['YearMonth'].astype(str)\nmodeldata.set_index(['YearMonth', 'shop_item'], inplace=True, drop=True)\nmodeldata = modeldata[modeldata['Year']>=2014]\n\nmodeldata.drop(columns=['shop_id', 'item_id', 'Year', 'Month', 'item_category_id'], inplace=True)\nmodeldata['category']=modeldata['category'].astype('category')\nmodeldata['subcategory']=modeldata['subcategory'].astype('category')\nmodeldata['city']=modeldata['city'].astype('category')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"modeldata.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create dummy variables for categorical \nmodeldata_dummies = pd.get_dummies(modeldata)\nmodeldata_dummies.head(3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Function to store output file as CSV"},{"metadata":{"trusted":true},"cell_type":"code","source":"def export_results(results, outputfilename):\n    ind = results.index\n    shopid = ind.str.split('-').str[0].astype('int64')\n    itemid = ind.str.split('-').str[1].astype('int64')\n\n    output = pd.DataFrame({'shop_id':shopid, 'item_id':itemid, 'item_cnt_month':results.values})\n    res = pd.merge(test, output, how='left', on=['shop_id', 'item_id'], sort=False)\n\n    result = res['item_cnt_month'].fillna(0)\n    result.index = res['ID']\n    filename = outputfilename + '.csv'\n    result.to_csv(filename, header=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Test Function for Modeling (calculate RMSE for various tests)"},{"metadata":{"trusted":true},"cell_type":"code","source":"def test_RMSE(y_pred, y_actual):\n    error = y_pred-y_actual\n    squared_error = error*error\n    mean_squared_error = np.mean(squared_error)\n    rmse = np.sqrt(mean_squared_error)\n    print('RMSE value: ', round(rmse,2))\n    return rmse","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model 1: Naive Model\nUse the most recent value of sales to predict future sales"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Use the most recent month's data as the prediction for the upcoming month\ny_pred = monthlypivot.iloc[-3,:]\ny_actual=monthlypivot.iloc[-2,:]\nrm = test_RMSE(y_pred, y_actual)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# The December 2015 prediction is the next prediction\nresults = monthlypivot.iloc[-2,:]\n#export_results(results = results, outputfilename = 'naive')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model 2: Mean Model\nEach prediction is the mean of the product for the entire range of data available"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data = monthlypivot.iloc[0:len(monthlypivot)-2,:]\ny_pred = train_data.mean()\ny_actual=monthlypivot.iloc[-2,:]\nrm = test_RMSE(y_pred, y_actual)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results = monthlypivot.iloc[0:len(monthlypivot)-1,:].mean()\n#export_results(results = results, outputfilename = 'meanmodel')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model 3: January Average (2013, 2014 & 2015)\nThis is because the overall data for 2015 is very spotty. So, using past January averages may be a better indication of future levels. "},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = monthlypivot.iloc[0]/3+monthlypivot.iloc[12]/3+monthlypivot.iloc[24]/3\ny_actual=monthlypivot.iloc[-2,:]\nrm = test_RMSE(y_pred, y_actual)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results = monthlypivot.iloc[0]/3+monthlypivot.iloc[12]/3+monthlypivot.iloc[24]/3\n#export_results(results = results, outputfilename = 'pastjan')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model 4: Lasso"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Undertake prediction on most sold items only; rest of the items will be predicted as zero\ntrain_data = modeldata_dummies.drop(index=['2015-12','2016-01'],level=0)\ntrain_x = train_data.drop(columns=['item_cnt_day'])\ntrain_y = train_data['item_cnt_day']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data = modeldata_dummies.loc[('2015-12'),:]\ntest_x = test_data.drop(columns=['item_cnt_day'])\ntest_y = test_data['item_cnt_day']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Lasso()\nscaler=StandardScaler()\n\n# Scale the x-variables\ntrain_x_scaled = scaler.fit_transform(train_x)\ntest_x_scaled = scaler.transform(test_x)\n\nmodel.fit(train_x_scaled, train_y)\nres = model.predict(test_x_scaled)\n\nprediction = pd.Series(res, index=test_y.index)\n\ny_pred = monthlypivot.iloc[-3,:]\ny_pred.update(prediction)\n\ny_actual = monthlypivot.iloc[-2,:]\nrm = test_RMSE(y_pred, y_actual)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lasso_coefficients = pd.DataFrame({'Variable':train_x.columns, 'Coefficients':model.coef_}).sort_values(by='Coefficients',ascending=False)\nlasso_coefficients.head(20)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Note that none of the average, min, max item sales or shop sales appear to be important factors. Lagged data is the most important in predicting future sales. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Lasso submission\ntrain_data = modeldata_dummies.drop(index=['2016-01'],level=0)\ntrain_x = train_data.drop(columns=['item_cnt_day'])\ntrain_y = train_data['item_cnt_day']\n\ntest_data = modeldata_dummies.loc[('2016-01'),:]\ntest_x = test_data.drop(columns=['item_cnt_day'])\n\nmodel = Lasso()\nscaler=StandardScaler()\n\n# Scale the x-variables\ntrain_x_scaled = scaler.fit_transform(train_x)\ntest_x_scaled = scaler.transform(test_x)\n\nmodel.fit(train_x_scaled, train_y)\nres = model.predict(test_x_scaled)\n\nprediction = pd.Series(res, index=test_y.index)\n\nresults = monthlypivot.iloc[-2,:]\nresults.update(prediction)\n#export_results(results = results, outputfilename = 'Lasso')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model 5: Random Forest"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Undertake prediction on most sold items only; rest of the items will be predicted as zero\ntrain_data = modeldata_dummies.drop(index=['2015-12','2016-01'],level=0)\ntrain_x = train_data.drop(columns=['item_cnt_day'])\ntrain_y = train_data['item_cnt_day']\n\ntest_data = modeldata_dummies.loc[('2015-12'),:]\ntest_x = test_data.drop(columns=['item_cnt_day'])\ntest_y = test_data['item_cnt_day']\n\nmodel = RandomForestRegressor()\nmodel.fit(train_x, train_y)\nres = model.predict(test_x)\n\nprediction = pd.Series(res, index=test_y.index)\n\ny_pred = monthlypivot.iloc[-3,:]\ny_pred.update(prediction)\n\ny_actual = monthlypivot.iloc[-2,:]\nrm = test_RMSE(y_pred, y_actual)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Random Forest submission\ntrain_data = modeldata_dummies.drop(index=['2016-01'],level=0)\ntrain_x = train_data.drop(columns=['item_cnt_day'])\ntrain_y = train_data['item_cnt_day']\n\ntest_data = modeldata_dummies.loc[('2016-01'),:]\ntest_x = test_data.drop(columns=['item_cnt_day'])\n\nmodel = RandomForestRegressor()\nscaler=StandardScaler()\n\n# Scale the x-variables\nmodel.fit(train_x, train_y)\nres = model.predict(test_x)\n\nprediction = pd.Series(res, index=test_y.index)\n\nresults = monthlypivot.iloc[-2,:]\nresults.update(prediction)\n#export_results(results = results, outputfilename = 'RandomForest')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model 6: XGBoost"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Undertake prediction on most sold items only; rest of the items will be predicted as zero\ntrain_data = modeldata_dummies.drop(index=['2015-12','2016-01'],level=0)\ntrain_x = train_data.drop(columns=['item_cnt_day'])\ntrain_y = train_data['item_cnt_day']\n\ntest_data = modeldata_dummies.loc[('2015-12'),:]\ntest_x = test_data.drop(columns=['item_cnt_day'])\ntest_y = test_data['item_cnt_day']\n\nmodel = xgb.XGBRegressor()\nmodel.fit(train_x, train_y)\nres = model.predict(test_x)\n\nprediction = pd.Series(res, index=test_y.index)\n\ny_pred = monthlypivot.iloc[-3,:]\ny_pred.update(prediction)\n\ny_actual = monthlypivot.iloc[-2,:]\nrm = test_RMSE(y_pred, y_actual)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# XGB submission\ntrain_data = modeldata_dummies.drop(index=['2016-01'],level=0)\ntrain_x = train_data.drop(columns=['item_cnt_day'])\ntrain_y = train_data['item_cnt_day']\n\ntest_data = modeldata_dummies.loc[('2016-01'),:]\ntest_x = test_data.drop(columns=['item_cnt_day'])\n\nmodel = RandomForestRegressor()\nscaler=StandardScaler()\n\n# Scale the x-variables\nmodel.fit(train_x, train_y)\nres = model.predict(test_x)\n\nprediction = pd.Series(res, index=test_y.index)\n\nresults = monthlypivot.iloc[-2,:]\nresults.update(prediction)\n#export_results(results = results, outputfilename = 'XGBoost')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# Model 7: Neural Network Sequential Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Prepare data using dataframes\ntrain_data = modeldata_dummies.drop(index=['2015-12','2016-01'],level=0)\ntrain_x = train_data.drop(columns=['item_cnt_day'])\ntrain_y = train_data['item_cnt_day']\n\ntest_data = modeldata_dummies.loc[('2015-12'),:]\ntest_x = test_data.drop(columns=['item_cnt_day'])\ntest_y = test_data['item_cnt_day']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Convert dataframes into numpy arrays that can be formulated as tensors\ntrain_x_np = np.array(train_x)\ntrain_y_np = np.array(train_y)\n\ntest_x_np = np.array(test_x)\ntest_y_np = np.array(test_y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Normalize the tran and test data\nmean = train_x_np.mean(axis=0)\ntrain_x_np -= mean\n\nstd = train_x_np.std(axis=0)\ntrain_x_np /= std\n\ntest_x_np -= mean\ntest_x_np /= std","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras import models, layers, regularizers\n\ndef build_model():\n    model = models.Sequential()\n    model.add(layers.Dense(32, kernel_regularizer=regularizers.l1(0.001), activation='relu', input_shape=(train_x_np.shape[1],)))\n    model.add(layers.Dropout(0.5))\n    model.add(layers.Dense(32, kernel_regularizer=regularizers.l1(0.001), activation='relu'))\n    model.add(layers.Dropout(0.5))\n    model.add(layers.Dense(32, kernel_regularizer=regularizers.l1(0.001), activation='relu'))\n    model.add(layers.Dropout(0.5))\n    model.add(layers.Dense(1))\n    model.compile(optimizer='rmsprop', loss='mse', metrics=['mae'])\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\n\nk=4\nnum_val_samples = len(train_x_np) //k\nnum_epochs = 10\nall_scores = []","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_loss_histories = []\nval_loss_histories = []\n\nfor i in range(k):\n    print('Processing batch#', i,'of', k-1)\n    val_data = train_x_np[i*num_val_samples: (i+1)*num_val_samples]\n    val_targets=train_y_np[i*num_val_samples: (i+1)*num_val_samples]\n    \n    partial_train_data = np.concatenate([train_x_np[:i*num_val_samples],train_x_np[(i+1)*num_val_samples:]],axis=0)\n    partial_train_targets=np.concatenate([train_y_np[:i*num_val_samples], train_y_np[(i+1)*num_val_samples:]], axis=0)\n    \n    model=build_model()\n    history=model.fit(partial_train_data, partial_train_targets, validation_data=(val_data, val_targets), epochs=num_epochs,\n                     batch_size=1, verbose=0)\n    \n    train_loss=history.history['loss']\n    val_loss = history.history['val_loss']\n    train_loss_histories.append(train_loss)\n    val_loss_histories.append(val_loss)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"avg_train_loss_history = [np.mean([x[i] for x in train_loss_histories]) for i in range(num_epochs)]\navg_val_loss_history = [np.mean([x[i] for x in val_loss_histories]) for i in range(num_epochs)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"p = figure(plot_width=950, plot_height=400, x_axis_label='Epochs', y_axis_label='Loss', title='Training & Validation Loss')\nx = range(1,len(avg_train_loss_history)+1)\np.line(x, avg_train_loss_history, line_width=3, legend_label='Training Loss')\np.line(x, avg_val_loss_history, color='green', line_width=3, legend_label='Validation Loss')\nshow(p)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Neural Network Submission\nres = model.predict(test_x_np)\n\nprediction = pd.Series(res[:,0], index=test_y.index)\n\nresults = monthlypivot.iloc[-2,:]\nresults.update(prediction)\nexport_results(results = results, outputfilename = 'NeuralNetwork')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}