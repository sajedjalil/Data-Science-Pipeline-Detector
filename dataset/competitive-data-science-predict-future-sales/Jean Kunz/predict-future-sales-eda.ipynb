{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport tensorflow as tf\nimport functools\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n\nprint(tf.__version__)\n\nsales_df = pd.read_csv(\"/kaggle/input/competitive-data-science-predict-future-sales/sales_train.csv\")\nitems_df = pd.read_csv(\"/kaggle/input/competitive-data-science-predict-future-sales/items.csv\")\nitem_cats_df = pd.read_csv(\"/kaggle/input/competitive-data-science-predict-future-sales/item_categories.csv\")\nshops_df = pd.read_csv(\"/kaggle/input/competitive-data-science-predict-future-sales/shops.csv\")\ntest_df = pd.read_csv(\"/kaggle/input/competitive-data-science-predict-future-sales/test.csv\")\n\ntrain_df = sales_df.merge(items_df[['item_id','item_category_id']], left_on='item_id', right_on='item_id', how='left')\n\nsales_month_df = train_df.sort_values(['shop_id','item_id','date_block_num']).groupby(['shop_id','item_id','date_block_num'])[['item_cnt_day']].sum().reset_index()\n\ntest_shop_ids = set(test_df.shop_id.unique())\ntrain_shop_ids = set(sales_month_df.shop_id.unique())\n# check if some shops, items, are not in train\ntest_item_ids = set(test_df.item_id.unique())\ntrain_item_ids = set(sales_month_df.item_id.unique())\nall_item_ids = set(items_df.item_id.unique())\n#del tmp_sales_months_df\nassert len(train_item_ids) < len(all_item_ids)\n\nmonth_nbs = np.arange(0,sales_month_df.date_block_num.max()+1)\nshops = shops_df.shop_id.unique()\n# we first work only with item in train. Then we will add the one that are in test only (with values inherited from their categories)\nitems = sales_month_df.item_id.unique()\n\n\nshop_item_ids = sales_month_df.drop_duplicates(subset = ['shop_id','item_id'])[['shop_id','item_id']].values.tolist()\n\n\nindex_tuples = [(shop_item[0], shop_item[1], m) for shop_item in shop_item_ids for m in month_nbs ]\n\n\nsales_idx = pd.MultiIndex.from_tuples(index_tuples, names=('shop_id','item_id','date_block_num'))\n\n#sales_idx = pd.MultiIndex.from_product([shops, items, month_nbs], names=('shop_id','item_id','date_block_num'))\n\ntmp_sales_months_df = sales_month_df.set_index(['shop_id',\n                                                'item_id',\n                                                'date_block_num']).reindex(sales_idx, \n                                                                           fill_value=0.).reset_index()\n# clean data memory\ndel sales_idx\ndel sales_month_df\ndel sales_df\ndel train_df\n\n\nfull_sales_month_df = tmp_sales_months_df.merge(items_df[['item_id','item_category_id']],\n                            left_on='item_id',\n                            right_on='item_id', \n                            how='left')\n\n\nlen(test_item_ids), len(train_item_ids), len(all_item_ids), len(full_sales_month_df)\n\n\nfull_sales_month_df.query('shop_id==59 & item_id==30')","metadata":{"_uuid":"32b95bce-ab92-4abc-9e7d-8dff95872b30","_cell_guid":"56d9fd69-fcec-4682-a871-a4e161b90aee","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-09-14T11:38:37.757708Z","iopub.execute_input":"2021-09-14T11:38:37.758124Z","iopub.status.idle":"2021-09-14T11:40:11.263046Z","shell.execute_reply.started":"2021-09-14T11:38:37.758023Z","shell.execute_reply":"2021-09-14T11:40:11.262039Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## compute std and average so we can sample from the distribution for eache item, shop, month\nwe assume a normal distribution","metadata":{}},{"cell_type":"markdown","source":"### analyze test data","metadata":{}},{"cell_type":"code","source":"# only for shop, item tuple in test that are not in train\ntest_shop_item_ids = test_df.drop_duplicates(subset = ['shop_id','item_id'])[['shop_id','item_id']].values.tolist()\n\ntest_shop_item_set = set([(shop_item[0],shop_item[1]) for shop_item in test_shop_item_ids]) \ntrain_shop_item_set = set([(shop_item[0],shop_item[1]) for shop_item in shop_item_ids])\n\ntest_only_shop_items = list(test_shop_item_set.difference(train_shop_item_set))\ntrain_test_shop_items = test_shop_item_set.union(train_shop_item_set)\nlen(test_only_shop_items)+ len(train_test_shop_items), len(test_shop_item_set.intersection(train_shop_item_set)), len(test_df)\n\nprint(\"nb of items with no categories:\", len(items_df[items_df.item_category_id.isna()]))\n\nshop_cat_month_df = full_sales_month_df.groupby(['shop_id',\n                                                 'item_category_id',\n                                                 'date_block_num']).item_cnt_day.agg([np.mean, \n                                                                                      np.std]).reset_index()\n\ntest_index_tuples = [(shop_item[0], shop_item[1], m) for shop_item in test_only_shop_items for m in month_nbs ]\ntest_only_sales_idx = pd.MultiIndex.from_tuples(test_index_tuples, names=('shop_id','item_id','date_block_num'))\n# create a dataframe of sales based on items that are only in test.\n\ntest_only_sales_df = pd.DataFrame(index=test_only_sales_idx)\ntest_only_sales_df.reset_index(inplace=True)\n\n# test only shop, item tuple should not be in train df\ndef by_len(it):\n    return len(full_sales_month_df.query(f\"shop_id=={it[0]} & item_id=={it[1]}\"))>0\nassert len(list(filter( by_len, test_only_shop_items[:100])))==0\n\ntest_only_sales_cat_df = test_only_sales_df.merge(items_df[['item_id','item_category_id']], left_on='item_id', right_on='item_id', how='left')\n\n\ntest_only_sales_cat_df.head(10)\n\ntest_only_sales_cat_avg_df = test_only_sales_cat_df.merge(shop_cat_month_df, \n                                                          left_on=['shop_id','item_category_id','date_block_num'], \n                                                          right_on=['shop_id','item_category_id','date_block_num'])\nassert len(test_only_sales_cat_avg_df== len(test_only_sales_cat_df))\n\ntest_only_sales_cat_avg_df.query('(shop_id==22) &(item_id==13463)').head(10)\n\nnp.random.seed(42)\ntest_only_sales_cat_avg_df['item_cnt_day_sampled']=np.random.normal(test_only_sales_cat_avg_df['mean'].values, \n                                                            test_only_sales_cat_avg_df['std'].values)\n\ntest_only_sales_cat_avg_df.query('(shop_id==22) &(item_id==13463)').head(10)\n\ntest_only_sales_cat_avg_df.loc[:, 'item_cnt_day']=test_only_sales_cat_avg_df.item_cnt_day_sampled.round(0).replace([-0],0)\ntest_only_item_id = test_only_sales_cat_avg_df.item_id.unique()[10]\nassert len(full_sales_month_df.query(f'(shop_id==0)&(item_id=={test_only_item_id})'))==0, \"in full_sales_month_df, there should not be items in test only\"\n\n# columns should be the same\n# Concatenate avg based sales for test only items and regular items\nassert len(set(full_sales_month_df.columns).difference(set(test_only_sales_cat_avg_df.columns)))==0\nall_sales_df = pd.concat([full_sales_month_df, test_only_sales_cat_avg_df[list(full_sales_month_df.columns.values)]])\nassert len(all_sales_df)== len(full_sales_month_df)+len(test_only_sales_cat_avg_df)\nassert len(all_sales_df.item_id.unique())==len(items_df.item_id.unique())\n\nall_sales_df.item_cnt_day.fillna(0., inplace=True)\n\ndel test_only_sales_cat_avg_df\ndel test_only_sales_cat_df\ndel test_only_sales_df\ndel test_only_sales_idx\ndel full_sales_month_df","metadata":{"execution":{"iopub.status.busy":"2021-09-14T11:48:40.228491Z","iopub.execute_input":"2021-09-14T11:48:40.228885Z","iopub.status.idle":"2021-09-14T11:49:01.436364Z","shell.execute_reply.started":"2021-09-14T11:48:40.228846Z","shell.execute_reply":"2021-09-14T11:49:01.435345Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Train-Validation split\nTrain on before last 3 month.\nValidate on last 3 months.","metadata":{}},{"cell_type":"code","source":"train_df = all_sales_df[all_sales_df.date_block_num<33]\nval_df = all_sales_df\n\nall_sales_df.date_block_num.max(), train_df.date_block_num.max()\nprint(train_df[(train_df.shop_id==3)&(train_df.item_id==138)])","metadata":{"execution":{"iopub.status.busy":"2021-09-14T11:49:01.438258Z","iopub.execute_input":"2021-09-14T11:49:01.438566Z","iopub.status.idle":"2021-09-14T11:49:02.711444Z","shell.execute_reply.started":"2021-09-14T11:49:01.438534Z","shell.execute_reply":"2021-09-14T11:49:02.710631Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Prepare features","metadata":{}},{"cell_type":"code","source":"WIN_SIZE=5\ndef prep_win_x(x, win_size=None,):\n    if win_size:\n        l = x.shape[0]-1\n        to = l\n        fr = to-win_size\n        #print(l, fr, to)\n        win_x = x[fr:to]\n        #win_y = x[to:, y_col_idx]\n    else:\n        win_x = x[:-1]\n        #win_y = x[-1:,y_col_idx]\n    return win_x\n\ndef prep_y(x, y_col_idx=None):\n    return x[-1:,y_col_idx][0]\n\ntst_x = [np.array([[59., 30.,  1., 13., 0],\n [59., 30.,  2., 10., 1],\n [59., 30.,  3.,  4., 2],\n [59., 30.,  4.,  0., 3],\n [59., 30.,  5.,  0., 4],\n [59., 30.,  6.,  1., 5],\n [59., 30.,  7.,  1., 6]]),\n np.array([[59., 30.,  1., 13., 0],\n [59., 30.,  2., 10., 1],\n [59., 30.,  3.,  4., 2],\n [59., 30.,  4.,  0., 3],\n [59., 30.,  5.,  0., 4],\n [59., 30.,  6.,  1., 5],\n [59., 30.,  7.,  1., 6]])]\n\nres_win=list(map(functools.partial(prep_win_x, win_size=5), tst_x))\nassert res_win[0].shape[0]==5\nres = list(map(functools.partial(prep_win_x), tst_x))\nassert res[0].shape[0]==6\n\nlist(map(functools.partial(prep_y, y_col_idx=4), tst_x))","metadata":{"execution":{"iopub.status.busy":"2021-09-14T11:49:02.712756Z","iopub.execute_input":"2021-09-14T11:49:02.713562Z","iopub.status.idle":"2021-09-14T11:49:02.73452Z","shell.execute_reply.started":"2021-09-14T11:49:02.713513Z","shell.execute_reply":"2021-09-14T11:49:02.73341Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_win(df, win_size=30):\n    arr = df[['shop_id','item_id','item_category_id','date_block_num', 'item_cnt_day']].sort_values(['shop_id','item_id','date_block_num']).values    \n    groups_idx = np.unique(arr[:,:2], axis=0, return_index=True)\n    #print(groups_idx)\n    group_list = np.split(arr, groups_idx[1][1:])\n    #print(group_list)\n    \n    x = np.array(list(map(functools.partial(prep_win_x, win_size=SEQLEN),group_list[:] )))\n    y = np.array(list(map(functools.partial(prep_y, y_col_idx=4),group_list[:] )))\n    \n    return x, y\n\nSEQLEN=30\ntrain_x, train_y = create_win(train_df[:], win_size=SEQLEN)\nassert train_x.shape[0]== train_y.shape[0]    \n\nval_x, val_y = create_win(val_df, win_size=SEQLEN)\nassert val_x.shape[0]== val_y.shape[0]    \ntrain_x.shape, train_y.shape, val_x.shape, val_y.shape","metadata":{"execution":{"iopub.status.busy":"2021-09-14T11:49:02.737058Z","iopub.execute_input":"2021-09-14T11:49:02.73739Z","iopub.status.idle":"2021-09-14T11:49:36.72863Z","shell.execute_reply.started":"2021-09-14T11:49:02.737342Z","shell.execute_reply":"2021-09-14T11:49:36.727684Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_x[:1], train_y[:1]\nassert np.argwhere(np.isnan(train_x)).shape[0]==0, \"there should be no nan value in train\"\n#train_x[13729]","metadata":{"execution":{"iopub.status.busy":"2021-09-14T11:49:36.730108Z","iopub.execute_input":"2021-09-14T11:49:36.730697Z","iopub.status.idle":"2021-09-14T11:49:37.06413Z","shell.execute_reply.started":"2021-09-14T11:49:36.730652Z","shell.execute_reply":"2021-09-14T11:49:37.063187Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def prep_seq_x(x):\n    \n    cols =['shop_id','item_id', 'item_category_id', 'month_nb','sales']\n    \n    cat_cols = ['shop_id','item_id', 'item_category_id']\n    seq_cat_cols = ['month_nb']\n    seq_cols = ['sales']\n    pred_col = 'sales'\n    X_cat = x[:, :, np.where(np.isin(cols, cat_cols))].squeeze()\n    X_seq = x[:, :, np.where(np.isin(cols, seq_cols))].squeeze(-1)\n    # create embeddings for each month\n    X_seq_cat = x[:, :, np.where(np.isin(cols, seq_cat_cols))].squeeze(-1)\n\n    return X_cat, X_seq, X_seq_cat\n\n\ntrain_X_cat, train_X_seq, train_X_seq_cat = prep_seq_x(train_x[:],)\nval_X_cat, val_X_seq, val_X_seq_cat = prep_seq_x(val_x[:],)\ntrain_X_cat.shape, train_X_seq.shape, train_X_seq_cat.shape, train_y.shape","metadata":{"execution":{"iopub.status.busy":"2021-09-14T11:49:37.065734Z","iopub.execute_input":"2021-09-14T11:49:37.066008Z","iopub.status.idle":"2021-09-14T11:49:37.978678Z","shell.execute_reply.started":"2021-09-14T11:49:37.065978Z","shell.execute_reply":"2021-09-14T11:49:37.977694Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_X_seq.squeeze().shape, train_X_seq.shape","metadata":{"execution":{"iopub.status.busy":"2021-09-14T11:49:37.980457Z","iopub.execute_input":"2021-09-14T11:49:37.980809Z","iopub.status.idle":"2021-09-14T11:49:37.987909Z","shell.execute_reply.started":"2021-09-14T11:49:37.980765Z","shell.execute_reply":"2021-09-14T11:49:37.986994Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# baseline with linear model\nfrom sklearn.linear_model import LinearRegression\n\nlr = LinearRegression()\ntrain_X = train_X_seq.squeeze()\nval_X = val_X_seq.squeeze()\nlr.fit(train_X, train_y)\nlr.score(train_X, train_y), lr.score(val_X, val_y)","metadata":{"execution":{"iopub.status.busy":"2021-09-14T11:49:37.989397Z","iopub.execute_input":"2021-09-14T11:49:37.989996Z","iopub.status.idle":"2021-09-14T11:49:39.718452Z","shell.execute_reply.started":"2021-09-14T11:49:37.989944Z","shell.execute_reply":"2021-09-14T11:49:39.717456Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!mkdir data","metadata":{"execution":{"iopub.status.busy":"2021-09-14T11:49:39.720081Z","iopub.execute_input":"2021-09-14T11:49:39.720683Z","iopub.status.idle":"2021-09-14T11:49:40.536661Z","shell.execute_reply.started":"2021-09-14T11:49:39.720636Z","shell.execute_reply":"2021-09-14T11:49:40.535511Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_X_cat.shape, train_X_seq_cat.shape","metadata":{"execution":{"iopub.status.busy":"2021-09-14T11:49:40.539518Z","iopub.execute_input":"2021-09-14T11:49:40.539801Z","iopub.status.idle":"2021-09-14T11:49:40.546895Z","shell.execute_reply.started":"2021-09-14T11:49:40.539766Z","shell.execute_reply":"2021-09-14T11:49:40.546019Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def to_tf_records(X_cat, X_seq, X_seq_cat,y, file_name):\n    with tf.io.TFRecordWriter(file_name) as writer: \n        for i, item in enumerate(X_seq.tolist()):\n            x_cat_ser = tf.io.serialize_tensor(X_cat[i])  \n            #print(x_cat_ser.numpy())\n            ex_schema = {                \n                'x_cat': tf.train.Feature(bytes_list=tf.train.BytesList(value=[tf.io.serialize_tensor(X_cat[i]).numpy()])),\n                'x_seq': tf.train.Feature(bytes_list=tf.train.BytesList(value=[tf.io.serialize_tensor(X_seq[i]).numpy()])),\n                # another way to encode 1d lists\n                #'x_seq': tf.train.Feature(float_list=tf.train.FloatList(value=X_seq[i])),\n                'x_seq_cat': tf.train.Feature(bytes_list=tf.train.BytesList(value=[tf.io.serialize_tensor(X_seq_cat[i]).numpy()])),\n                #'x_seq_cat': tf.train.Feature(int64_list=tf.train.Int64List(value=X_seq_cat[i])),\n                'y': tf.train.Feature(float_list=tf.train.FloatList(value=[y[i]]))\n            }\n            ex = tf.train.Example(features=tf.train.Features(feature=ex_schema))      \n            writer.write(ex.SerializeToString())\n                                  \n                                  \nnb_ex = 1000000  #train_X_cat.shape[0]\nto_tf_records(train_X_cat[:nb_ex],train_X_seq[:nb_ex], train_X_seq_cat[:nb_ex],train_y[:nb_ex],'data/train_seq.proto')                                  ","metadata":{"execution":{"iopub.status.busy":"2021-09-14T11:49:40.548309Z","iopub.execute_input":"2021-09-14T11:49:40.548873Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def parse_tfrecord_fn(example):\n    feat_desc = {\n        'x_cat': tf.io.FixedLenFeature([], dtype=tf.string),\n        'x_seq': tf.io.FixedLenFeature([], dtype=tf.string),\n        #'x_seq': tf.io.VarLenFeature(tf.float32),\n        'x_seq_cat': tf.io.FixedLenFeature([], dtype=tf.string),        \n        'y': tf.io.FixedLenFeature([], dtype=tf.float32)\n    }    \n    example = tf.io.parse_single_example(example, feat_desc)\n    # other way to decode 1d list, if encoded that way\n    #example[\"x_seq\"] = tf.sparse.to_dense(example[\"x_seq\"])    \n    example['x_seq'] = tf.ensure_shape(tf.io.parse_tensor(example['x_seq'], out_type=tf.float64),(SEQLEN,1))    \n    example['x_seq_cat']= tf.ensure_shape(tf.io.parse_tensor(example['x_seq_cat'], out_type=tf.float64), (SEQLEN, 1))\n    example['x_cat'] = tf.ensure_shape(tf.io.parse_tensor(example['x_cat'], out_type=tf.float64), (SEQLEN, 3))\n    example['y'] = tf.ensure_shape(example['y'], ())\n    return example\n\n\ndebug_ex = False\nraw_dataset = tf.data.TFRecordDataset(['data/train_seq.proto'])\nfor features in raw_dataset.map(parse_tfrecord_fn).batch(batch_size=2):\n    for key in features.keys():       \n        if debug_ex:\n            print(f\"{key}: {features[key]}\")\n    break\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def prepare_sample(features):    \n    ''' filter and format data to given model'''     \n    #x_seq = tf.ensure_shape(features['x_seq'], (1, SEQLEN,1))   \n    x_seq = features['x_seq']\n    y = tf.ensure_shape(features['y'],())\n    print(\"x seq\", x_seq.shape, y.shape)\n    return x_seq, y\n\nAUTOTUNE = tf.data.AUTOTUNE\n\ndef get_dataset(filenames, batch_size):\n    \n    ## to optimize pipelinening instead of tf.data.TFRecord...\n    #(tf.data.Dataset.list_files(filenames)\n    #.interleave(tf.data.TFRecordDataset, num_parallel_calls=tf.data.experimental.AUTOTUNE))\n    \n    dataset = (\n        #tf.data.TFRecordDataset(filenames, num_parallel_reads=AUTOTUNE)\n        tf.data.Dataset.list_files(filenames)\n        .interleave(tf.data.TFRecordDataset, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n        .map(parse_tfrecord_fn, num_parallel_calls=AUTOTUNE)\n        .map(prepare_sample, num_parallel_calls=AUTOTUNE)\n        .shuffle(batch_size * 10)\n        .batch(batch_size)\n        .prefetch(AUTOTUNE)\n    )\n    return dataset\n\n#get_dataset('data/*.proto', batch_size=2)\nds = get_dataset('data/*.proto', batch_size=4)\nX_batch, y_batch = next(iter(ds))\nfor i, row in enumerate(iter(ds)):\n    print(i, row[0].shape, row[1].shape)\n    if i>10:        \n        break","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# detect and init the TPU\ntpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect()\n\n# instantiate a distribution strategy\ntpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)\n","metadata":{"execution":{"iopub.status.busy":"2021-09-14T11:46:21.011563Z","iopub.execute_input":"2021-09-14T11:46:21.011966Z","iopub.status.idle":"2021-09-14T11:46:26.913188Z","shell.execute_reply.started":"2021-09-14T11:46:21.011924Z","shell.execute_reply":"2021-09-14T11:46:26.912263Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class SequenceModel(tf.keras.Model):\n    ...\n    def __init__(self, seq_len, shop_nb, item_nb, month_nb=SEQLEN, size=64, dropout=0.5, output_bias=None):\n        super(SequenceModel, self).__init__()\n        self.size = size\n        self.shop_nb = shop_nb\n        self.item_nb = item_nb\n        self.month_nb = month_nb\n        \n            # https://www.tensorflow.org/tutorials/structured_data/imbalanced_data#checkpoint_the_initial_weights\n        if output_bias is not None:\n            output_bias = tf.keras.initializers.Constant(output_bias)\n        \n        self.i = tf.keras.layers.Input(shape=(SEQLEN,1))\n        self.rnn = tf.keras.layers.GRU(64, activation='relu')\n        self.dnn = tf.keras.layers.Dense(1)        \n    \n    def call(self, X):        \n        print(\"x\",X)\n        i = self.i(X)\n        out = self.rnn(i)\n        y = self.dnn(out)\n        \n        return y\n\n\n        \ndef get_model(seq_len, shop_nb, item_nb, month_nb=SEQLEN, size=64, dropout=0.5, output_bias=None):\n    \n    seq_input = tf.keras.layers.Input(shape=(month_nb,1))\n    x = tf.keras.layers.Flatten()(seq_input)\n    #x = tf.keras.layers.GRU(64, activation='relu')(seq_input)\n    out = tf.keras.layers.Dense(1)(x)\n    model = tf.keras.Model(inputs=seq_input, outputs=out)  \n\n    model = tf.keras.Sequential(\n        [\n            tf.keras.layers.GRU(64, activation=\"relu\"),\n            tf.keras.layers.Dense(1)        \n        ]\n    )\n    \n    \n    model.compile(loss=tf.keras.losses.Huber(),\n              optimizer=tf.keras.optimizers.Adam(1e-4),\n              metrics=[tf.keras.metrics.RootMeanSquaredError()])\n    \n    return model\n\n\ntf.debugging.set_log_device_placement(True)\nshop_nb = len(shops)\nitem_nb = len(items)\nif tpu:\n    with strategy.scope():    \n        model = get_model(SEQLEN, shop_nb, item_nb)\n    BATCH_SIZE = 16 * tpu_strategy.num_replicas_in_sync\nelse:\n    model = get_model(SEQLEN, shop_nb, item_nb)\n    BATCH_SIZE=64\n    \nmodel.build(input_shape=(None,SEQLEN,1))\nmodel.summary()\nmodel.fit(get_dataset('data/*.proto', batch_size=BATCH_SIZE), epochs=5, verbose=1)\n#model.fit(train_X_seq[:1000], train_y[:1000], batch_size=64, epochs=1)","metadata":{"execution":{"iopub.status.busy":"2021-09-13T12:56:31.868038Z","iopub.execute_input":"2021-09-13T12:56:31.868502Z","iopub.status.idle":"2021-09-13T12:56:37.410988Z","shell.execute_reply.started":"2021-09-13T12:56:31.86846Z","shell.execute_reply":"2021-09-13T12:56:37.408527Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_y[:100].shape, train_X_seq[:100].shape","metadata":{"execution":{"iopub.status.busy":"2021-09-13T12:29:01.708773Z","iopub.status.idle":"2021-09-13T12:29:01.709639Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))","metadata":{"execution":{"iopub.status.busy":"2021-09-13T12:29:01.710869Z","iopub.status.idle":"2021-09-13T12:29:01.711448Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n#model.build((None, SEQLEN, 1))\n","metadata":{"execution":{"iopub.status.busy":"2021-09-13T12:29:01.712341Z","iopub.status.idle":"2021-09-13T12:29:01.712912Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model\n\n- don't forget to add a out of vocab categories\n- no need to rescale/std values, because we only have category and the value to regress (sales prediction)","metadata":{}},{"cell_type":"code","source":"# compute regression value mean to init bias\n","metadata":{"execution":{"iopub.status.busy":"2021-09-13T12:29:01.71373Z","iopub.status.idle":"2021-09-13T12:29:01.714282Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.evaluate(val_X_seq, val_y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"win_X","metadata":{"_uuid":"11339b2f-f4eb-491b-90cf-31bfa45732a6","_cell_guid":"39aa147c-e2dc-4096-b70e-436280143fa0","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#https://www.youtube.com/watch?v=ZnukSLKEw34\nds = tf.data.Dataset.list_files('data/*.proto')\nds = ds.interleave(tf.data.TFRecordDataset, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n#ds = tf.data.TFRecordDataset(['data/*.proto'])\nds = ds.map(decode_fn, num_parallel_calls=tf.data.experimental.AUTOTUNE)\nds = ds.batch(batch_size=2)\n# to enable pipelining\nds = ds.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"t1 = [[1, 2]]\nt2 = [[7, 8]]\nnonscalar = tf.concat([t1, t2], 0)\nnonscalar_np = np.array([t1,t2])\nprint(nonscalar)\nprint(\"nonscalarnonscalar\", nonscalar)\n\nserialized_nonscalar = tf.io.serialize_tensor(nonscalar)\n\nprint(\"serialized_nonscalar\", serialized_nonscalar.numpy())\n# to numpy to make it bytes instead of tensor\nfeature_of_bytes = tf.train.Feature(bytes_list=tf.train.BytesList(value=[serialized_nonscalar.numpy()]))\n\n\nprint(\"feat of bytes\",feature_of_bytes)\nfeatures_for_example = {\n  'x1': feature_of_bytes\n}\nexample_proto = tf.train.Example(features=tf.train.Features(feature=features_for_example))\nprint(\"example\",example_proto)\n\n# will be written to tf.record file.\nex_string = example_proto.SerializeToString()\nprint(\"serialized ex in string\", ex_string)\nex_from_str = tf.train.Example.FromString(ex_string)\nprint(\"ex from string\", ex_from_str)\n\nfeat_desc = {\"x1\": tf.io.FixedLenFeature([], dtype=tf.string)}\n\n# it takes a serialized string as input, like a row in a tf.record.\nparsed_ex = tf.io.parse_single_example(ex_string, feat_desc)\nprint(\"parsed ex\", parsed_ex)\nprint(\"parsed x1 feat as tensor\", parsed_ex['x1'])\n\nx1_str = parsed_ex['x1'].numpy()\nprint(\"x1 as str\", x1_str)\n\n(tf.io.parse_tensor(x1_str, out_type=tf.int32) == nonscalar)\ntf.io.parse_tensor(x1_str, out_type=tf.int32)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Windowing","metadata":{"_uuid":"543a5499-d62f-451c-afd8-ad2b03c6d44e","_cell_guid":"9b2c483c-e4bd-4d29-81ea-355604a1d545","trusted":true}},{"cell_type":"code","source":"# no need to pad because we got a 33 fixed size len \ndef rolling_win(x, y=None, win_len=1, verbose=False):    \n    '''\n    y can be none, when we create window in test mode.\n    '''\n    nb_row = x.shape[0]\n    nb_col = x.shape[1]\n    if verbose:\n        print('nb row', nb_row)\n        print('nb col', nb_col)\n        \n    win_nb = nb_row - win_len + 1\n        \n    win_len_dim = np.expand_dims(\n        np.arange(0, win_len), 0)\n    win_nb_dim = np.expand_dims(np.arange(win_nb), 0).T\n    win_idx = win_len_dim + win_nb_dim\n    win_x = x[win_idx]\n    if verbose:\n        print(\"win nb\", win_nb)\n        print(\"win len dim\", win_len_dim)\n        print(\"win nb dim\", win_nb_dim)\n        print(\"win idx\", win_idx)\n        print(\"win x\", win_x)\n        \n    win_y = None\n    if y is not None:\n        y_idx = np.arange(win_len, win_len+win_nb-1)\n        win_y = y[y_idx]\n        if verbose:\n            print(\"y idx\", y_idx)\n            print(\"win y\", win_y)\n    \n        \n    return win_x[:-1], win_y\n    \n    \n\ntst_x = np.array([[59., 30.,  1., 13.],\n [59., 30.,  2., 10.],\n [59., 30.,  3.,  4.],\n [59., 30.,  4.,  0.],\n [59., 30.,  5.,  0.],\n [59., 30.,  6.,  1.],\n [59., 30.,  7.,  1.]])\n\ntst_y = np.expand_dims(np.array([0,1,2,3,4,5,6]),axis=1)\n#tst_y = np.array([[0,1,2,3,4,5,6]])\n\ntst_win_x, tst_win_y = rolling_win(tst_x, tst_y, win_len=3, verbose=False)\n#tst_win_x.shape, tst_win_y.shape, tst_y.shape\n\n\n\n# new approach -> one time serie window per group as input of model\n\ndef prep_win(x):\n    win_x, win_y = rolling_win(x[:-1], x[1:,4], win_len= 29 )\n    return (win_x, win_y)\n\nit = map(prep_win, train_groups[:100])\ntmp = list(it)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#%%timeit\ndef prep_win(df, win_len=10):\n        grp_df = df.sort_values(['shop_id', 'item_id','date_block_num']).groupby(['shop_id','item_id'])\n        all_x = None\n        all_y = None\n        for name, grp in grp_df:\n            x = np.array(grp.values)\n            y = np.expand_dims(np.array(grp.item_cnt_day.values),axis=1)\n            #print(x.shape, y.shape)\n            # we omit last row as we want to predict it (the goal is to predict next month sales).\n            win_x, win_y = rolling_win(x,y, win_len=win_len, verbose=False)                        \n            if all_x is None:\n                all_x = win_x                \n                all_y = win_y\n                #print(all_x.shape, all_y.shape)\n            else:\n                all_x = np.concatenate((all_x, win_x), axis=0)\n                all_y = np.concatenate((all_y, win_y), axis=0)\n                #print(\">\",all_x.shape, all_y.shape, win_y.shape)\n\n        return all_x, all_y\n            \n\nsmall_data = False\nSEQ_LEN = 30\nif small_data:\n    sm_shops = [2,3, 5] #shops[:10]\n    sm_items = [31,12]#items[:10]\n\n    df = train_df.loc[(train_df.shop_id.isin(sm_shops)) & (train_df.item_id.isin(sm_items))]\n    seq, y = prep_win(df,win_len=SEQ_LEN)\nelse:\n    df = train_df\n    seq, y = prep_win(df,win_len=SEQ_LEN)","metadata":{"_uuid":"bd2d00f4-5377-4f67-9868-cb6445b596f8","_cell_guid":"4223c459-58d7-49d5-a9ac-e792857e2746","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def prep_x_y(seq, seq_len):\n    \n    cols =['shop_id','item_id', 'month_nb','sales','item_category_id']\n    cat_cols = ['shop_id','item_id', 'item_category_id']\n    seq_cat_cols = ['month_nb']\n    seq_cols = ['sales']\n    pred_col = 'sales'\n    X_cat = seq[:, :, np.where(np.isin(cols, cat_cols))].squeeze()\n    X_seq = seq[:, :, np.where(np.isin(cols, seq_cols))].squeeze(-1)\n    # create embeddings for each month\n    X_seq_cat = seq[:, :, np.where(np.isin(cols, seq_cat_cols))].squeeze(-1)\n    y = seq[:, :, cols.index(pred_col)].squeeze()[:, seq_len - 1]\n\n    return X_cat, X_seq, X_seq_cat,  y\n\n\nX_cat, X_seq, X_seq_cat, y = prep_x_y(seq, SEQ_LEN)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def rolling_win_padded(x, win_len, pad_nb=None, win_nb=None, verbose=False):\n    '''\n    Args:\n        x: timeserie numpy array of shape (timestep, nb_features )\n        win_len: len of window on timeserie\n        pad_nb: the nb of left padded zeros (or first padded) that will be used to define first windows. If no pad_nb is provided, it will start to pad every timestep except the last one (which is the first item of window)\n        win_nb: forced nb of window to be returned. If None, win_nb is computed.\n        verbose: true if debug info is displayed\n    '''\n    nb_row = x.shape[0]\n    nb_col = x.shape[1]\n\n    # if not defined then compute it.\n    if win_nb is None:\n        if nb_row <= win_len:\n            pad_nb = win_len-nb_row\n        elif pad_nb is None:\n            pad_nb = 0\n        # else we use pad_nb given as parameter.\n        # nb of rows including pad\n        nb_row_total = nb_row + pad_nb\n        win_nb = nb_row_total - win_len + 1\n        start_rolling_idx = 0\n        if verbose:\n            print(\"nb of windows is computed\")\n\n    else:\n        # the len on which win will be rolled over.\n        rolling_len = win_len + (win_nb-1)\n        if nb_row >= rolling_len:\n            pad_nb = 0\n        else:\n            pad_nb = rolling_len - nb_row\n\n        nb_row_total = nb_row + pad_nb\n        start_rolling_idx = nb_row_total - rolling_len\n        if verbose:\n            print(\"nb of windows is forced\")\n\n    if verbose:\n        print('> nb of windows', win_nb)\n        print(\"> win len:\", win_len)\n        print(\"> nb rows:\", nb_row)\n        print(\"> nb rows total (with pad):\", nb_row_total)\n        print(\"> nb col:\", nb_col)\n        print(\"> start index (where we start rolling)\", start_rolling_idx)\n\n    pad = np.zeros((pad_nb, nb_col))\n    padded_x = np.concatenate([pad, x])\n\n    if verbose:\n        print(\"Nb row included pad:\", nb_row_total)\n        print(\"nb windows:\", win_nb)\n        print(\"\\n pad\", pad, \"\\n padded x\", padded_x, '\\n padd nb:', pad_nb)\n    # -----\n    # create a vectorized index based on a rolling index\n    # rolling index defines windows with index pointing to data in x\n    # we create a matrix of shape (win_len_dim, win_nb_dim)\n    win_len_dim = np.expand_dims(\n        np.arange(start_rolling_idx, win_len+start_rolling_idx), 0)\n    win_nb_dim = np.expand_dims(np.arange(win_nb), 0).T\n    if verbose:\n        print(\"win dim\", win_len_dim, \"\\n nb of window dimension\", win_nb_dim)\n    # we add timestep shift to first window of index\n    win_idx = win_len_dim + win_nb_dim\n    if verbose:\n        print(\"rolling idx\", win_idx)\n        print(\"rolling x\", padded_x[win_idx])\n    return padded_x[win_idx]\n\n\nx = np.array([[1, 'a', 1.5],\n              [2, 'b', 3.2],\n              [3, 'c', 3.5],\n              [4, 'd', 3.3],\n              [5, 'e', 5.2],\n              [6, 'f', 8.2]])\n\n\nrolling_x = rolling_win_padded(x[:, :], win_len=3, pad_nb=0, verbose=True)\nassert rolling_x.shape == (4, 3, 3)\n\nrolling_x = rolling_win_padded(x[:, :], win_len=1, pad_nb=0)\nassert rolling_x.shape == (6, 1, 3)\n\nrolling_x = rolling_win_padded(x[:, :], win_len=6, pad_nb=0)\nassert rolling_x.shape == (1, 6, 3)\n\nrolling_x = rolling_win_padded(x[:, :], win_len=8, pad_nb=0)\nassert rolling_x.shape == (1, 8, 3)\n\n\nrolling_x = rolling_win_padded(x[:, :], win_len=8, pad_nb=2)\nassert rolling_x.shape == (1, 8, 3)\n\nrolling_x = rolling_win_padded(x[:, :], win_len=8, win_nb=3, verbose=False)\nassert rolling_x.shape == (3, 8, 3)\n\nrolling_x = rolling_win_padded(x[:, :], win_len=4, win_nb=2, verbose=False)\nassert rolling_x.shape == (2, 4, 3)\nassert rolling_x[-1, -1, 1] == 'f'  # last item is 6,'f',...","metadata":{"_uuid":"2536498e-6904-4610-8d9c-cf8e9bbadea6","_cell_guid":"b9ac1592-9597-4dd3-9595-03b3f86a9102","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# %%time\nfrom numpy import save, load\n\nINDEX_COL_USE_TO_PREDICT = 3\n\n\ndef prep_windowed_data(df, seq_len):\n    grp_df = df.sort_values(['object_id', 'evt_at']).groupby('object_id')\n    all_x = None\n    min_nb_rows = 10\n    for name, group in grp_df:\n        #print(\"name\", name,\"values\", group.values)\n        x = np.array(group.values)\n        # we only select x for which the last timestep will have a use_to_predict==True, it's the first_true_idx\n        first_true_idx = np.where(x[:, INDEX_COL_USE_TO_PREDICT] == True)[0][0]\n        # compute the nb of window as nb of row - first_true_idx\n        win_nb = x.shape[0] - first_true_idx\n        win_x = rolling_win_padded(\n            x[:], win_len=seq_len, win_nb=win_nb, verbose=False)\n        if all_x is None:\n            all_x = win_x\n        else:\n            all_x = np.concatenate((all_x, win_x), axis=0)\n\n    return all_x\n\n\n# len of sequence (window)\nSEQ_LEN = 10\nMIN_NB_ROW_WIN = MIN_NB_EVT_PER_CPNY\n\nload_data = False\nif load_data:\n    train_seq = np_persistence_client.load_np_array(\n        BASE_OUTPUTS_DIR+\"train_seq.npy\")\n    test_seq = np_persistence_client.load_np_array(\n        BASE_OUTPUTS_DIR+\"test_seq.npy\")\nelse:\n    is_dev = False\n    if is_dev:\n        object_id_list = train_prep_df.object_id.unique()\n        # object_id_list=['c:13529']\n        # object_id_list=['c:1']\n        #train_seq = prep_windowed_data(train_prep_df[train_prep_df.object_id.isin(object_id_list[:1000])], seq_len=SEQ_LEN)\n        object_id_list = ['c:40456']\n        object_id_list = ['c:104015']\n        #object_id_list = test_prep_df.object_id.unique()\n        test_seq = prep_windowed_data(test_prep_df[test_prep_df.object_id.isin(\n            object_id_list[:1000])], seq_len=SEQ_LEN)\n\n    else:\n        train_seq = prep_windowed_data(train_prep_df, seq_len=SEQ_LEN)\n        np_persistence_client.save_np_array(\n            train_seq, BASE_OUTPUTS_DIR+\"train_seq.npy\")\n        # ---\n        test_seq = prep_windowed_data(test_prep_df, seq_len=SEQ_LEN)\n        np_persistence_client.save_np_array(\n            test_seq, BASE_OUTPUTS_DIR+\"test_seq.npy\")\n\n        assert np.unique(test_seq[:, -1, INDEX_COL_USE_TO_PREDICT]) == [\n            True], \"The last timestep must always have a use_to_predict==True in test\"\n        assert np.unique(train_seq[:, -1, INDEX_COL_USE_TO_PREDICT]) == [\n            True], \"The last timestep must always have a use_to_predict==True in train\"","metadata":{"_uuid":"38e40673-81bd-4972-bbb1-72af749092e0","_cell_guid":"f37e647b-a5b4-4ec3-848d-5daf2c89bdd6","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def prep_x_y(seq, cols, seq_len):\n    '''\n    return X for sequence, X for company static data and y\n    '''\n    seq_cols = [\n        'cum_acquisition_nb', 'cum_rounds_nb', 'cum_raised_usd',\n        'cum_nb_unique_investors', 'cum_nb_successfull_invest_by_investor',\n        'nb_days_since_last_evt_type', 'nb_days_since_last_any_evt',\n        'nb_days_since_first_evt', 'participants'\n    ]\n    #seq_cols = ['participants', 'cum_raised_usd', 'cum_nb_unique_investors', 'cum_nb_successfull_invest_by_investor', 'nb_days_since_last_any_evt']\n    # try a model without cum counters\n    seq_cat_cols = ['evt_type']\n    cpny_cols = ['country_code', 'category_code']\n    pred_col = 'success_horizon'\n    pred_col = 'success_less_5_year'\n    pred_col = 'success_less_2_year'\n    assert seq.shape[-1] == len(\n        cols), \"There should be as many columns (cols) as the last dimension of seq\"\n    X_cpny = seq[:, :, np.where(np.isin(cols, cpny_cols))].squeeze()\n    X_seq = seq[:, :, np.where(np.isin(cols, seq_cols))].squeeze()\n    X_seq_cat = seq[:, :, np.where(np.isin(cols, seq_cat_cols))].squeeze(-1)\n    y = seq[:, :, cols.index(pred_col)].squeeze()[:, seq_len - 1]\n\n    return X_cpny, X_seq, X_seq_cat, y\n\n\nX_cpny_train, X_seq_train, X_seq_cat_train, y_train = prep_x_y(\n    train_seq, cols, SEQ_LEN)\nX_cpny_test, X_seq_test, X_seq_cat_test, y_test = prep_x_y(\n    test_seq, cols, SEQ_LEN)","metadata":{"_uuid":"380eba43-6e97-4522-a7aa-4ed2b78a94b2","_cell_guid":"ea78581e-412b-4459-8377-1a7717b7031f","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]}]}