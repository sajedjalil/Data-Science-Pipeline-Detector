{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd \nimport numpy as np\nimport lightgbm as lgb\nimport sklearn\nimport sys, os\nfrom sklearn.metrics import r2_score\nfrom catboost import Pool, CatBoostRegressor, cv\nfrom sklearn.model_selection import KFold \nfrom tqdm import tqdm_notebook\nfrom  sklearn.preprocessing import LabelEncoder\nfrom calendar import monthrange\nfrom itertools import product, chain \nimport gc\n\nimport re\nfrom bayes_opt import BayesianOptimization\nfrom bayes_opt.util import Colours\n\nimport matplotlib.pyplot as plt\npd.set_option('display.max_columns',500)\npd.set_option('display.max_rows',100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sales = pd.read_csv('../input/competitive-data-science-predict-future-sales/sales_train.csv')\nshops = pd.read_csv('../input/competitive-data-science-predict-future-sales/shops.csv')\nitems = pd.read_csv('../input/competitive-data-science-predict-future-sales/items.csv')\nitem_cats = pd.read_csv('../input/competitive-data-science-predict-future-sales/item_categories.csv')\ntest = pd.read_csv('../input/competitive-data-science-predict-future-sales/test.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We start with a little bit of EDA."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.subplot(1,2,2)  ### outliers\nplt.plot(sales['item_price'])\nplt.legend('price')\n\nplt.subplot(1,2,1)\nplt.plot(sales['item_cnt_day'])\nplt.legend(('count'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I decided to remove those outliers."},{"metadata":{"trusted":true},"cell_type":"code","source":"sales = sales[sales['item_price'] < 250000]\nsales = sales[sales['item_cnt_day'] < 1000]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since I don't speak russian , I found help in several kernels in Kaggle in order to extract valuables informations from sales,shop and items data by looking at the names. This one especially : https://www.kaggle.com/kyakovlev/1st-place-solution-part-1-hands-on-data"},{"metadata":{},"cell_type":"markdown","source":"# **Feature engineering**"},{"metadata":{},"cell_type":"markdown","source":"SALES FEATURES "},{"metadata":{},"cell_type":"markdown","source":"First, some shops are the same. Let's remove it. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Якутск Орджоникидзе, 56                            ## thanks to Denis Larionov ##\nsales.loc[sales.shop_id == 0, 'shop_id'] = 57\ntest.loc[test.shop_id == 0, 'shop_id'] = 57\n# Якутск ТЦ \"Центральный\"\nsales.loc[sales.shop_id == 1, 'shop_id'] = 58\ntest.loc[test.shop_id == 1, 'shop_id'] = 58\n# Жуковский ул. Чкалова 39м²\nsales.loc[sales.shop_id == 10, 'shop_id'] = 11\ntest.loc[test.shop_id == 10, 'shop_id'] = 11\n\nsales.loc[sales.shop_id == 39, 'shop_id'] = 40\ntest.loc[test.shop_id == 39, 'shop_id'] = 40","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can extract time data."},{"metadata":{"trusted":true},"cell_type":"code","source":"sales['date'] = pd.to_datetime(sales['date'], format='%d.%m.%Y')\nsales['month'] = sales['date'].dt.month\nsales['year'] = sales['date'].dt.year\n\nsales = sales.drop('date', axis = 1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I merge the test data with the sales data."},{"metadata":{"trusted":true},"cell_type":"code","source":"to_append = test[['shop_id', 'item_id']].copy()\n\nto_append['date_block_num'] = sales['date_block_num'].max() + 1\nto_append['year'] = 2015\nto_append['month'] = 11\nto_append['item_cnt_day'] = 0\nto_append['item_price'] = 0\n\nsales = pd.concat([sales, to_append], ignore_index=True, sort=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's add a holiday feature."},{"metadata":{"trusted":true},"cell_type":"code","source":"holiday_dict = {\n    1: 7,\n    2: 4,\n    3: 3,\n    4: 9,\n    5: 4,\n    6: 4,\n    7: 3,\n    8: 9,\n    9: 5,\n    10: 9,\n    11: 6,\n    12: 5,\n}\n\nsales['holidays_in_month'] = sales['month'].map(holiday_dict)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I make a small dataframe containing only time data."},{"metadata":{"trusted":true},"cell_type":"code","source":"period = sales[['date_block_num','year','month','holidays_in_month']].drop_duplicates().reset_index(drop=True)\nperiod['days'] = period.apply(lambda r: monthrange(r.year, r.month)[1], axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"SHOP FEATURES"},{"metadata":{},"cell_type":"markdown","source":"Thanks to the name of shops, we can extract the city and the type of the shop   and then label encode it. "},{"metadata":{"trusted":true},"cell_type":"code","source":"cities = shops['shop_name'].str.split(' ').map(lambda row : row[0])\ncities.unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"shops['shop_city'] = cities\nshops.loc[shops['shop_city'] == '!Якутск', 'shop_city' ] = 'Якутск'\nls = LabelEncoder()\nshops['shop_city'] = ls.fit_transform(shops['shop_city'])\nshops['shop_type'] = shops['shop_name'].apply(lambda x: 'МТРЦ' if 'МТРЦ' in x \n                                              else 'ТРЦ' if 'ТРЦ' in x \n                                              else 'ТРК' if 'ТРК' in x \n                                              else 'ТЦ' if 'ТЦ' in x \n                                              else 'ТК' if 'ТК' in x \n                                              else 'NO_DATA')\nls1  = LabelEncoder()\nshops['shop_type'] = ls1.fit_transform(shops['shop_type'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"ITEM_CAT FEATURES"},{"metadata":{},"cell_type":"markdown","source":"Again, let's extract a main and a sub category from the name of the category."},{"metadata":{"trusted":true},"cell_type":"code","source":"main_cat = item_cats['item_category_name'].str.split('-').map(lambda row : row[0].strip())\nmain_cat.unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ls = LabelEncoder()\nitem_cats['main_cat'] = ls.fit_transform(main_cat)\n\nsub_cat = item_cats['item_category_name'].str.split('-').map(lambda row : row[1].strip()\n                                                             if len(row) > 1 \n                                                             else row[0].strip()) \nitem_cats['sub_cat'] = ls.fit_transform(sub_cat)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"ITEMS FEATURES "},{"metadata":{},"cell_type":"markdown","source":"Different names features can be extracted from the items data."},{"metadata":{"trusted":true},"cell_type":"code","source":"items['name_1'], items['name_2'] = items['item_name'].str.split('[', 1).str\nitems['name_1'], items['name_3'] = items['item_name'].str.split('(', 1).str\n\nitems['name_2'] = items['name_2'].str.replace('[^A-Za-z0-9А-Яа-я]+', ' ').str.lower()\nitems['name_3'] = items['name_3'].str.replace('[^A-Za-z0-9А-Яа-я]+', ' ').str.lower()\nitems.drop('name_1', axis = 1 , inplace=True)\nitems = items.fillna('0')\n\nls = LabelEncoder()\nitems['name_2'] = ls.fit_transform(items['name_2'])\nitems['name_3'] = ls.fit_transform(items['name_3'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can remove the duplicate names by doing the following :"},{"metadata":{"trusted":true},"cell_type":"code","source":"def name_correction(x):\n    x = x.lower()\n    x = x.partition('[')[0]\n    x = x.partition('(')[0]\n    x = re.sub('[^A-Za-z0-9А-Яа-я]+', ' ', x)\n    x = x.replace('  ', ' ')\n    x = x.strip()\n    return x\n\nitems['item_name'] = items['item_name'].apply(lambda x: name_correction(x))\nitems.head()\nprint('Unique item names after correction:', len(items['item_name'].unique()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We first merge item and item_category for simplicity first. "},{"metadata":{"trusted":true},"cell_type":"code","source":"items = items.merge(item_cats.drop('item_category_name', axis=1),\n                    on = ['item_category_id'], how = 'left')\nitems = items[['item_name','item_id','item_category_id','main_cat','sub_cat','name_2','name_3']]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### MERGING ###"},{"metadata":{"trusted":true},"cell_type":"code","source":"## helper functions ##\n\ndef downcast_dtypes(df):\n    '''\n        Changes column types in the dataframe: \n                \n                `float64` type to `float32`\n                `int64`   type to `int32`\n    '''\n    \n    # Select columns to downcast\n    float_cols = [c for c in df if df[c].dtype in [\"float64\"]]\n    int_cols =   [c for c in df if df[c].dtype in [\"int64\"]]\n    \n    # Downcast\n    df[float_cols] = df[float_cols].astype(np.float32)\n    df[int_cols]   = df[int_cols].astype(np.int32)\n    \n    return df\n\ndef position_colum(df,column_name):\n    '''\n        return the position of the column in the dataset \n    '''\n    assert column_name in df, \"Column {} not in the dataset\".format(column_name)\n    colums = df.columns\n    for i in range(len(colums)) :\n        if (column_name == colums[i]):\n            position = i\n        \n    return(position)\n\ndef mean_encoding_reg(df, col_to_enc,target_col, nb_split):\n    '''\n        return a new dataset where the categorical feature has been \n        encoded and replaced with KFOLD_mean_encoding\n        col_to_enc : name of the colums to encode\n    '''\n    col = [col_to_enc,target_col]\n    df_small = df[col]\n\n    target_mean_fold_enc = []\n\n    kf = KFold(n_splits=nb_split, shuffle=False)\n\n    for ind_tr, ind_val in kf.split(df_small):\n        mat_tr, mat_val = df_small.iloc[ind_tr], df_small.iloc[ind_val]\n    \n    \n        target_tr_mean_estimate = mat_tr.groupby(col_to_enc)[target_col].mean()\n        target_val_mean = df_small.loc[ind_val,col_to_enc].map(target_tr_mean_estimate)\n    \n        for mean in target_val_mean :\n            target_mean_fold_enc.append(mean)\n    \n    pos_categorical_feat = position_colum(df,col_to_enc)\n    new_df = df.copy()\n    new_df.insert(pos_categorical_feat,col_to_enc +'_mean_enc',pd.Series(target_mean_fold_enc).fillna(0))\n    \n    return ((downcast_dtypes(new_df)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we can expand the sales dataset. We first create a grid from all shops/items combinations from every month\nFor item_cnt_day we sum them to generate item_cnt_month values, the target.\nFor item_price we average it's values to calculate the item_price_month.\n\nAlso, we clip the target value between 0 & 20 as recommended on Kaggle."},{"metadata":{"trusted":true},"cell_type":"code","source":"## GRID DATA ###\n\nfrom itertools import product, chain \nimport gc\n\nindex_cols = ['shop_id', 'item_id', 'date_block_num']\n\n# For every month we create a grid from all shops/items combinations from that month\ngrid = [] \nfor block_num in sales['date_block_num'].unique():\n    cur_shops = sales.loc[sales['date_block_num'] == block_num, 'shop_id'].unique()\n    cur_items = sales.loc[sales['date_block_num'] == block_num, 'item_id'].unique()\n    grid.append(np.array(list(product(*[cur_shops, cur_items, [block_num]])),dtype='int32'))\n\n# Turn the grid into a dataframe\ngrid = pd.DataFrame(np.vstack(grid), columns = index_cols,dtype=np.int32)\n\n# Groupby data to get shop-item-month aggregates == Target\ngb = sales.groupby(index_cols,as_index=False).agg({'item_cnt_day':{'item_cnt_month':'sum'},\n                                                  'item_price' : {'item_price_month' : 'mean'}})\n# Fix column names\ngb.columns = [col[0] if col[-1]=='' else col[-1] for col in gb.columns.values] \n# Join it to the grid\nall_data = pd.merge(grid, gb, how='left', on=index_cols).fillna(0)\n\n\nall_data['item_cnt_month'] = all_data['item_cnt_month'].clip(0,20)  ## CLIP TARGET VALUE ##","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can finally merge every datasets together and save it all."},{"metadata":{"trusted":true},"cell_type":"code","source":"all_data = all_data.merge(period, on = ['date_block_num'], how = 'left')\nall_data = all_data.merge(shops.drop('shop_name', axis=1), on = ['shop_id'], how = 'left')\nall_data = all_data.merge(items.drop('item_name', axis=1), on = ['item_id'], how ='left')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_data = downcast_dtypes(all_data)\n\nall_data.to_pickle('all_data.pkl')\n\ndel grid, gb , cur_items\ngc.collect();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#all_data = pd.read_pickle('all_data.pkl')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"At this step, the all_data dataset contains all the basic information (train and test data) summarized per month."},{"metadata":{},"cell_type":"markdown","source":"As item_price_month and item_cnt_month were made from data only available from the train data, we cannot use them for the prediction, but we can still count on generated features of past data. this means we can for example use the last 2 or 3 months of an item_id price of any feature combination( item_id and shop_city).\n\nSo in order to do this, let's define some functions to generate the lag of a column created by any kind of combination for the  month we want."},{"metadata":{"trusted":true},"cell_type":"code","source":"def aggregate(data, col_groupby = ['shop_id'],target_col = 'item_cnt_day',\n              new_col_name='', agg_function = 'mean'):\n    '''\n    groupy data to get an aggregate of columns\n    '''\n    \n    gb = data.groupby(col_groupby, as_index=False).agg({target_col:{new_col_name : agg_function}})\n    gb.columns = [col[0] if col[-1]=='' else col[-1] for col in gb.columns.values]\n    return (pd.merge(data, gb, how='left', on=col_groupby).fillna(0))\n\ndef create_lags(data, index_cols, column_to_lag, shift_range = [1]):\n    '''\n    make feature lags for a columns for a dataset groupby index_cols\n    '''\n    for month_shift in tqdm_notebook(shift_range) :\n        train_shift = data[index_cols + [column_to_lag]].copy()\n        train_shift = train_shift.drop_duplicates().reset_index(drop=True)\n        \n        train_shift['date_block_num'] = train_shift['date_block_num'] + month_shift\n        \n        foo = lambda x : '{}_lag_{}'.format(x, month_shift) if x == column_to_lag else x\n        train_shift = train_shift.rename(columns = foo)\n             \n        data = pd.merge(data, train_shift, on=index_cols, how='left').fillna(0)\n        \n      \n        \n    return data\n\ndef agg_and_lag(data, col_groupby, target_col, new_col_name, agg_function, shift_range):\n    ''' \n    wrapper for aggregate and create_lags fonctions\n    '''\n    \n    df_agg = aggregate(data=data, col_groupby = col_groupby,target_col=target_col,\n                      new_col_name=new_col_name, agg_function=agg_function)\n    \n    df_lags = create_lags(data=df_agg, index_cols=col_groupby, \n                          column_to_lag=new_col_name,shift_range=shift_range)\n    \n    df_lags = df_lags.drop(new_col_name, axis = 1) \n    \n    return df_lags","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"PRICE AGGREGATE "},{"metadata":{},"cell_type":"markdown","source":"First, let's make aggregate based on price :"},{"metadata":{"trusted":true},"cell_type":"code","source":"all_data = agg_and_lag(all_data,col_groupby=['date_block_num'],\n                       target_col='item_price_month',\n                       new_col_name='date_price_mean', \n                       agg_function='mean',shift_range=[1])\n\nall_data = agg_and_lag(all_data,col_groupby=['item_id','date_block_num'],\n                       target_col='item_price_month',\n                       new_col_name='date_item_price_mean', \n                       agg_function='mean',shift_range=[1,2])\n\nall_data = agg_and_lag(all_data,col_groupby=['shop_id','date_block_num'],\n                       target_col='item_price_month', \n                       new_col_name='date_shop_price_mean', \n                       agg_function='mean',shift_range=[1,2])\n\nall_data = agg_and_lag(all_data,col_groupby=['sub_cat','shop_id','date_block_num'],\n                       target_col='item_price_month',\n                       new_col_name='date_shop_subcat_price_mean',\n                       agg_function='mean',shift_range=[1])\n\nall_data = agg_and_lag(all_data,col_groupby=['item_category_id','date_block_num'],\n                       target_col='item_price_month', \n                       new_col_name='date_itemcat_price_mean',\n                       agg_function='mean',shift_range=[1])\n\nall_data = agg_and_lag(all_data,col_groupby=['main_cat','date_block_num'],\n                       target_col='item_price_month',\n                       new_col_name='date_maincat_price_mean',\n                       agg_function='mean',shift_range=[1])\n\nall_data = agg_and_lag(all_data,col_groupby=['shop_type','date_block_num'],\n                       target_col='item_price_month',\n                       new_col_name='date_shop_type_price_mean',\n                       agg_function='mean',shift_range=[1])\n\nall_data = agg_and_lag(all_data,col_groupby=['item_id','shop_city','date_block_num'],\n                       target_col='item_price_month',\n                       new_col_name='date_item_shopcity_price_mean',\n                       agg_function='mean',shift_range=[1])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"ITEM_CNT_MONTH AGGREGATE "},{"metadata":{},"cell_type":"markdown","source":"Then, the aggregates based on the item_cnt_month feature :"},{"metadata":{"trusted":true},"cell_type":"code","source":"all_data = agg_and_lag(all_data,col_groupby=['item_id','date_block_num'],\n                       target_col='item_cnt_month',\n                       new_col_name='date_item_month_mean',\n                       agg_function='mean', shift_range=[1,2])\n\n\nall_data = agg_and_lag(all_data,col_groupby=['shop_id','date_block_num'],\n                       target_col='item_cnt_month',\n                       new_col_name='date_shop_month_mean', \n                       agg_function='mean', shift_range=[1,2])\n\nall_data = agg_and_lag(all_data,col_groupby=['item_category_id','date_block_num'],\n                       target_col='item_cnt_month',\n                       new_col_name='date_itemcat_month_mean', \n                       agg_function='mean', shift_range=[1])\n\nall_data = agg_and_lag(all_data,col_groupby=['shop_id','item_id','date_block_num'],\n                       target_col='item_cnt_month',\n                       new_col_name='date_item_shop_month_mean', \n                       agg_function='mean',shift_range=[1])\n\nall_data = agg_and_lag(all_data,col_groupby=['sub_cat','shop_id','date_block_num'],\n                       target_col='item_cnt_month',\n                       new_col_name='date_item_subcat_month_mean', \n                       agg_function='mean', shift_range=[1])\n\nall_data = agg_and_lag(all_data,col_groupby=['main_cat','shop_id','date_block_num'],\n                       target_col='item_cnt_month',\n                       new_col_name='date_item_maincat_month_mean',\n                       agg_function='mean', shift_range=[1])\n\nall_data = agg_and_lag(all_data,col_groupby=['item_id','shop_city','date_block_num'],\n                       target_col='item_cnt_month',\n                       new_col_name='date_item_shopcity_month_mean',\n                       agg_function='mean',shift_range=[1])\n\nall_data = agg_and_lag(all_data,col_groupby=['date_block_num'],\n                       target_col='item_cnt_month',\n                       new_col_name='date_month_mean',\n                       agg_function='mean', shift_range=[1])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Finally, we create lags for the target itself and item_price_month :"},{"metadata":{"trusted":true},"cell_type":"code","source":"index_cols = ['shop_id', 'item_id', 'date_block_num']\n\nall_data = create_lags(all_data, index_cols=index_cols,\n                       column_to_lag='item_cnt_month', shift_range=[1,2,3,12])\nall_data = create_lags(all_data, index_cols=index_cols,\n                       column_to_lag='item_price_month', shift_range=[1,2])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del sales, item_cats, items","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We don't need to use data from the first 3 months since we created lags."},{"metadata":{"trusted":true},"cell_type":"code","source":"all_data = all_data[all_data['date_block_num'] >= 3]  \n\ngc.collect();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_data.to_pickle('all_data_lags.pkl')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#all_data = pd.read_pickle('all_data_lags.pkl')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I tried mean encoding for categorical variables but I ended up not using it because of more overfitting."},{"metadata":{"trusted":true},"cell_type":"code","source":"#all_data_enc = mean_encoding_reg(all_data,'shop_city','item_cnt_month', 5)\n#all_data_enc = mean_encoding_reg(all_data_enc,'main_cat','item_cnt_month', 5)\n#all_data_enc = mean_encoding_reg(all_data_enc,'month','item_cnt_month', 5)\n#all_data_enc = mean_encoding_reg(all_data_enc,'item_category_id','item_cnt_month', 5)\n\n#all_data_enc.to_pickle('all_data_enc.pkl')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#all_data_enc = pd.read_pickle('all_data_enc.pkl')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We store the target and date and then delete from all_data."},{"metadata":{"trusted":true},"cell_type":"code","source":"date = all_data['date_block_num']\ntarget = all_data['item_cnt_month']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_data = all_data.drop('item_cnt_month', axis = 1)\nall_data = all_data.drop('item_price_month', axis = 1 )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"REMOVING FEATURE TOO CORRELATED / USELESS ONE"},{"metadata":{"trusted":true},"cell_type":"code","source":" def correlatedDropper( df_data, thresh = 0.95):\n\n        \"\"\"\n        Remove columns too correlated from a ``pandas.DataFrame\n\n        \"\"\"\n        df = df_data.copy()\n        corr_matrix = df.corr()\n\n        upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n\n        cols_to_drop = [column for column in upper.columns if any(upper[column] > thresh)]\n\n        df = df.drop(cols_to_drop, axis=1)\n        return df, cols_to_drop","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test, col_drop = correlatedDropper(all_data, 0.95)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"col_drop","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After test, I decided to keep it."},{"metadata":{},"cell_type":"markdown","source":" # **Splitting the dataset in train / validation / test set**"},{"metadata":{},"cell_type":"markdown","source":"The last month 34 correspond to the test set. We'll be using the last month - 1 as the validation set since it's time data."},{"metadata":{"trusted":true},"cell_type":"code","source":"last_block_num = 33\n\nX_test = all_data[date == 34]\nX_valid = all_data[date == last_block_num]\nX_train = all_data[date < last_block_num]\n\ny_train = target.loc[date < last_block_num].values\ny_valid = target.loc[date == last_block_num].values\n\n\ncategorical_feat_ind = np.where(X_train.dtypes != np.float32)[0]\nprint(categorical_feat_ind)\n\n#Pool1 = Pool(X_train, y_train, categorical_feat_ind)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del all_data, target, date\n\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Bayesian optimization for hyperparameters**"},{"metadata":{},"cell_type":"markdown","source":"I found this idea thanks to Mazoub parpanchi who implemented it for lgb  in his great kernel : https://www.kaggle.com/masoudmzb/bayesian-optimization-for-lgb\nGive it a look !        "},{"metadata":{},"cell_type":"markdown","source":"Instead of using gridSearchCV , I decided to use Bayesian Optimization. Bayesian Optimization enables to find rapidly good hyperparameters in less than 10 iterations using Gausian processes. For this, we have to define a function , and the algorithm will try to find the best maxima of this function out of several parameters.\nHere, the function is basically the catboost model which output the RMSE and the parameters are the hyperparameters ( like the bagging temperature, l2_reg..).\nThe library is only defined to maximize the output so here the output will be -RMSE since we want to minimize it !.\nBellow is the link of the repo for a more detailled explanation : \nhttps://github.com/fmfn/BayesianOptimization.\nUsing GPU is of course the best choice.\n                                                                                                                        "},{"metadata":{"trusted":true},"cell_type":"code","source":"def catboost_rmse(X_train,y_train,\n                  X_valid, y_valid,\n                  loss_function='RMSE',\n                  iterations=400,\n                  random_seed=0,\n                  learning_rate=0.15,\n                  depth=5,\n                  l2_leaf_reg=1,\n                  one_hot_max_size=200,\n                  min_data_in_leaf = 1,\n                  bagging_temperature = 1,\n                  border_count = 32,\n                  max_ctr_complexity = 2\n               ) :\n    \n    ''' \n    fonction to optimize, here the catboost RMSE \n    '''\n    \n    catmodel = CatBoostRegressor( \n    loss_function= loss_function,\n    iterations=iterations,\n    random_seed=random_seed,\n    learning_rate=learning_rate,\n    depth=depth,\n    l2_leaf_reg=l2_leaf_reg,\n    one_hot_max_size=one_hot_max_size,\n    min_data_in_leaf = min_data_in_leaf,\n    bagging_temperature= bagging_temperature,        \n    border_count=border_count,\n    max_ctr_complexity=max_ctr_complexity,\n    thread_count=-1,\n    od_type='Iter',\n    od_wait= 20,\n    task_type='GPU',\n    devices='0',  \n    verbose = 100\n    )\n\n    catmodel.fit(X_train,\n                 y_train,\n                 eval_set=(X_valid,y_valid),\n                 plot=False\n            )\n\n    predict = catmodel.predict(X_valid).clip(0,20)\n    \n    best_rmse_train = catmodel.best_score_['learn']['RMSE']\n    best_rmse_valid = catmodel.best_score_['validation']['RMSE']\n        \n        \n    print('train_rmse : {:.4f}, test_rmse : {:.4f}'.format(best_rmse_train, best_rmse_valid))\n        \n        \n    r2 = r2_score(y_valid,predict)\n        \n    print('the R2 score is {:.4f}'.format(r2))\n    \n    del predict\n   \n    \n    return - best_rmse_valid","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def optimize_catboost(X_train,y_train,X_valid,y_valid, param_probe=None) :\n    '''\n    return the optimizer  and output the best hyperparameters found\n    param_probe : dict of hyperparameters in order to guide the optimization\n    '''\n    def catboost_wrapper(learning_rate, depth, one_hot_max_size, \n                         min_data_in_leaf, bagging_temperature,\n                         border_count, max_ctr_complexity,\n                         loss_function = 'RMSE', l2_leaf_reg = 1, \n                         random_seed = 0,  iterations = 1000) :\n        '''\n        wrapper for the function to optimize\n        '''\n        \n        return catboost_rmse(X_train=X_train, y_train=y_train ,\n                             X_valid=X_valid ,y_valid=y_valid, \n                             iterations = int(iterations),\n                             random_seed = random_seed, \n                             learning_rate=learning_rate, \n                             depth=int(depth),\n                             l2_leaf_reg=l2_leaf_reg,\n                             one_hot_max_size= int(one_hot_max_size), \n                             min_data_in_leaf= int(min_data_in_leaf),\n                             bagging_temperature = bagging_temperature,\n                             border_count = int(border_count), \n                             max_ctr_complexity = int(max_ctr_complexity)\n              )\n    \n    optimizer = BayesianOptimization(\n        f = catboost_wrapper,\n        pbounds= {\n            \"learning_rate\" : (0.01,0.50),\n            \"depth\" : (2,10),\n            \"l2_leaf_reg\" : (1,50),\n            \"one_hot_max_size\" : (2,230),\n            \"min_data_in_leaf\" : (1,10),\n            \"bagging_temperature\" : (0,100),\n            \"border_count\" : (30, 250),\n            \"max_ctr_complexity\" : (1,4)          \n        },\n        random_state = 63,\n        verbose = 30,\n    )\n    if param_probe != None :\n        optimizer.probe(\n        params= param_probe,\n        lazy=True,\n        )\n    \n    optimizer.maximize(n_iter=3,\n                      init_points=5)\n    \n    print(\"Final result:\", optimizer.max)\n    return optimizer","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since I ran this a lot, I found thoses parameters bellow. Let's probe it to the optimizer in order to guide the optimization and see if we can find better parameters. "},{"metadata":{"trusted":true},"cell_type":"code","source":"params_probe = {'bagging_temperature': 0.28300538794137076, \n                'border_count': 247.51179202909668, \n                'depth': 8.478936311173683, \n                'l2_leaf_reg': 7.73213368476749,\n                'learning_rate': 0.10290800942292017,\n                'max_ctr_complexity': 2.9452851946784433, \n                'min_data_in_leaf': 2.383458113416781, \n                'one_hot_max_size': 209.4502766887087}","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"print(Colours.green(\"\"\" -- Optimizing Catboost Parameters -- \"\"\"))\noptimizer_cat = optimize_catboost(X_train=X_train, y_train=y_train, \n                                  X_valid=X_valid, y_valid=y_valid,\n                                  param_probe=params_probe)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Not for this run, so let's troncate the int parameters and add the constant ones. We stop the training after 8 iterations without test RMSE improving. You can of course increase the number of iterations for a better mapping of the objective function."},{"metadata":{"trusted":true},"cell_type":"code","source":"params_catboost =  {'bagging_temperature': 0.28300538794137076,\n                    'border_count': 247, \n                    'depth': 8, \n                    'l2_leaf_reg': 7.73213368476749,\n                    'learning_rate': 0.10290800942292017,\n                    'max_ctr_complexity': 2, \n                    'min_data_in_leaf': 2.383458113416781, \n                    'one_hot_max_size': 209,\n                    'random_seed' : 0,\n                    'thread_count' : -1,\n                    'task_type' : 'GPU',\n                    'devices' : '0', \n                    'od_type' : 'Iter',\n                    'od_wait' :  20,\n                    'verbose' : 50\n                   }","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"catmodel = CatBoostRegressor(**params_catboost)\n\ncatmodel.fit(X_train,y_train,\n             eval_set=(X_valid,y_valid),\n             plot=False\n            )\n\npredict = catmodel.predict(X_valid).clip(0,20)\nr2 = r2_score(y_valid,predict)\nprint( 'r2_score est {}'.format(r2))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" # **Feature Selection **"},{"metadata":{},"cell_type":"markdown","source":"Since X has a lot of features, we can reduce them in order to reduce overfitting."},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_feature_imp(catmodel, method, X_train, y_train, X_test, y_test, plot = False):\n    '''\n    return the importance of the feature based on a specific method\n    method : - \"PredictionValuesChange\"\n             - \"LossFunctionChange\"\n    '''\n    \n    fi = catmodel.get_feature_importance(Pool(X_test, label=y_test), type=method)\n        \n    feature_score = pd.DataFrame(list(zip(X_test.dtypes.index, fi )),\n                                        columns=['Feature','Score'])\n\n    feature_score = feature_score.sort_values(by='Score', ascending=False, inplace=False,\n                                              kind='quicksort', na_position='last')\n    if plot : \n        plt.rcParams[\"figure.figsize\"] = (12,7)\n        ax = feature_score.plot('Feature', 'Score', kind='bar', color='c')\n        ax.set_title(\"Feature Importance using {}\".format(method), fontsize = 14)\n        ax.set_xlabel(\"features\")\n        plt.show()   \n    return feature_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_pred = get_feature_imp(catmodel=catmodel, method='PredictionValuesChange', \n                               X_train=X_train, y_train=y_train,\n                               X_test=X_valid, y_test=y_valid,\n                               plot=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_useless_pred = feature_pred[feature_pred['Score'] < 0.01]   #0.1\nfeature_useless_pred.Feature","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I decided to remove the feature with virtually no impact on the prediction."},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_pred = X_train.drop(feature_useless_pred.Feature[:-1], axis = 1)\nX_valid_pred = X_valid.drop(feature_useless_pred.Feature[:-1], axis = 1)\nX_test_pred = X_test.drop(feature_useless_pred.Feature[:-1], axis = 1) \n### I kept date_block_num after tests","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Bellow another method in order to perform feature selection, but I didn't use it eventually as I performed worse in the LB with this one."},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_loss = get_feature_imp(catmodel,'LossFunctionChange',\n                               X_train, y_train, \n                               X_valid, y_valid,\n                               plot=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_useless_loss = feature_loss[feature_loss['Score'] < 0.0]\nfeature_useless_loss.Feature","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"PREDICTION VALUE CHANGE"},{"metadata":{},"cell_type":"markdown","source":"Let's run a model based on this shrunken X."},{"metadata":{"trusted":true},"cell_type":"code","source":"catmodel_pred = CatBoostRegressor(**params_catboost)\n\ncatmodel_pred.fit(X_train_pred,y_train,\n             eval_set=(X_valid_pred,y_valid),\n             plot=False\n            )\n\npredict = catmodel_pred.predict(X_valid_pred).clip(0,20)\nr2 = r2_score(y_valid,predict)\nprint( 'r2_score est {}'.format(r2))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The RMSE test decreased and the train RMSE increased. So that's exactly what we wanted since there is less overfitting. My LB improved after this.\nWe can try to improve it by running an another optimization. For me, even if the RMSE decreases, it did not improve in the LB but you can try it by yourself."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(Colours.green(\"\"\" -- Optimizing Catboost Parameters -- \"\"\"))\noptimizer_cat_loss = optimize_catboost(X_train=X_train_pred, y_train=y_train,\n                                       X_valid=X_valid_pred, y_valid=y_valid,\n                                       param_probe=params_probe)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, let's remove the old X and save the new one."},{"metadata":{"trusted":true},"cell_type":"code","source":"del X_train, X_valid","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_pred.to_pickle('X_train_pred.pkl')\nX_valid_pred.to_pickle('X_valid_pred.pkl')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#X_train_pred = pd.read_pickle('X_train_pred.pkl')\n#X_valid_pred = pd.read_pickle('X_valid_pred.pkl')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"![](http://)To check the size of your variables, if you have memory issue."},{"metadata":{},"cell_type":"raw","source":"import sys\n\n# These are the usual ipython objects, including this one you are creating\nipython_vars = ['In', 'Out', 'exit', 'quit', 'get_ipython', 'ipython_vars']\n\n# Get a sorted list of the objects and their sizes\nsorted([(x, sys.getsizeof(globals().get(x))) for x in dir() if not x.startswith('_') and x not in sys.modules and x not in ipython_vars], key=lambda x: x[1], reverse=True)"},{"metadata":{},"cell_type":"markdown","source":"# **Ensembling and final prediction**"},{"metadata":{},"cell_type":"markdown","source":"For emsembling, I use the simple Bagging where models are the same except for the random seed.\nAfter using 10 differents seeds, the overall prediction performed worse so I decided to only choose the best seeds."},{"metadata":{"trusted":true},"cell_type":"code","source":"def find_top_seeds(X_train,y_train,X_valid,y_valid,params, range_min = 0, range_max = 10):\n    \n    \n    metric_dict = pd.DataFrame(columns=['state','rmse'])\n                                  \n    for i in range(range_min,range_max) :\n        print('--- random state : {} ---\\n'.format(i))\n        \n        params['random_seed'] = i \n        catmodel = CatBoostRegressor(**params)\n        catmodel.fit(X_train,y_train,\n             eval_set=(X_valid,y_valid),\n             plot=False\n            )\n        \n        best_rmse_valid = catmodel.best_score_['validation']['RMSE']\n        \n        metric_dict.loc[i-range_min] = [int(i)] + [best_rmse_valid]\n        metric_dict['state'] = metric_dict['state'].astype(np.int32)\n        \n    return metric_dict.sort_values(by='rmse', ascending = 'False')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"metric = find_top_seeds(X_train_pred,y_train,X_valid_pred, y_valid,\n                        params=params_catboost, range_min=0, range_max=10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.rcParams[\"figure.figsize\"] = (7,7)\nax = metric.plot('state', 'rmse', kind='bar', color='c')\nax.set_title(\"rmse en fonction du shuffling\", fontsize = 10)\nax.set_xlabel(\"state\")\nplt.show()  ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After submitting to kaggle several times, I only kept seeds  4,6,8."},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"predictions_test = []   # 0.92949 for 2,4,6,8\n   \nfor i in [4,8,6]:\n    print('random state {}'.format(i))\n    \n    params_catboost['random_seed'] = i\n    \n    catmodel_bagging_pred = CatBoostRegressor(**params_catboost)\n    catmodel_bagging_pred.fit(X_train_pred,y_train,\n             eval_set=(X_valid_pred,y_valid),\n             plot=False\n            )\n\n    predict = catmodel_bagging_pred.predict(X_valid_pred).clip(0,20)\n    r2 = r2_score(y_valid,predict)\n    print( 'r2_score est {}'.format(r2))\n    \n    predictions_test.append(catmodel_bagging_pred.predict(X_test_pred).clip(0,20))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions =  np.mean(np.array(predictions_test), axis = 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dt_test_opt = pd.DataFrame()\ndt_test_opt['item_cnt_month'] = predictions\ndt_test_opt.index.name='ID'\ndt_test_opt.to_csv('submission_featurev11_bagging_[4_6_8].csv')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"}},"nbformat":4,"nbformat_minor":4}