{"cells":[{"metadata":{},"cell_type":"markdown","source":"This notebook is advanced version of the [Feature engineering, xgboost](https://www.kaggle.com/dlarionov/feature-engineering-xgboost). <br>\nWe descrive our contribution below pipline \n\n#### Pipline\n* load data\n* heal data and remove outliers\n* ## (Update) add extra features using shops/items/cats features\n* ## (Update) categorize shops/items/cats objects and features\n* create matrix as product of item/shop pairs within each month in the train set\n* get monthly sales for each item/shop pair in the train set and merge it to the matrix\n* clip item_cnt_month by (0,20)\n* append test to the matrix, fill 34 month nans with zeros\n* merge shops/items/cats to the matrix\n* add target lag features\n* add mean encoded features\n* add price trend features\n* add month & days\n* add months since last sale/months since first sale features\n* cut first year and drop columns which can not be calculated for the test set\n* ## (Update) select best features\n* set validation strategy 34 test, 33 validation, less than 33 train\n* ##  (Update) fine tuning XGB model and training with GPU\n* fit the model, predict and clip targets for the test set"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 100)\n\nfrom itertools import product\nfrom sklearn.preprocessing import LabelEncoder\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom xgboost import XGBRegressor\nfrom xgboost import plot_importance\n\ndef plot_features(booster, figsize):    \n    fig, ax = plt.subplots(1,1,figsize=figsize)\n    return plot_importance(booster=booster, ax=ax)\n\nimport time\nimport sys\nimport gc\nimport pickle\nsys.version_info","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"code","source":"import os\nos.listdir('../input')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"PATH = 'competitive-data-science-predict-future-sales'\nitems = pd.read_csv('../input/' + PATH + '/items.csv')\nshops = pd.read_csv('../input/' + PATH + '/shops.csv')\ncats = pd.read_csv('../input/' + PATH + '/item_categories.csv')\ntrain = pd.read_csv('../input/' + PATH + '/sales_train.csv')\n# set index to ID to avoid droping it later\ntest  = pd.read_csv('../input/' + PATH + '/test.csv').set_index('ID')\n\nprint('------------- train info ------------') ; print(train.info(), '\\n')\nprint('------------- test info ------------') ; print(test.info(), '\\n')\nprint('------------- items info ------------') ; print(items.info(), '\\n')\nprint('------------- shops info ------------') ; print(shops.info(), '\\n')\nprint('------------- categories info ------------') ; print(cats.info(), '\\n')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Drop Outliers\ncheck NaN values and below zero values <br>\nWe drop outliers which price > 10^5 and sales > 1000"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train.isna().sum(), '\\n')\nprint(test.isna().sum())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,4))\nplt.xlim(-100, 3000)\nsns.boxplot(x=train.item_cnt_day)\n\nplt.figure(figsize=(10,4))\nplt.xlim(train.item_price.min(), train.item_price.max()*1.1)\nsns.boxplot(x=train.item_price)\n\nprint(len(train[train.item_cnt_day>999]))\nprint(len(train[train.item_cnt_day>500]))\nprint(len(train[train.item_cnt_day<501]))\n\ntrain = train[train.item_price<100000]\ntrain = train[train.item_cnt_day<1000]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,4))\nplt.xlim(-100, 1000)\nsns.boxplot(x=train.item_cnt_day)\n\nplt.figure(figsize=(10,4))\nplt.xlim(train.item_price.min(), train.item_price.max()*1.1)\nsns.boxplot(x=train.item_price)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There is one item with price below zero. Eliminate outlier <br>\n\nAlso, there are some negative values in item_cnt_day.<br>\n-1.0 : 7252 ,\n-2.0 : 78 ,\n-3.0 : 14 ,\n-4.0 : 3 ,\n-5.0 : 4 ,\n-6.0 : 2 ,\n-9.0 : 1 ,\n-16.0 : 1 ,\n-22.0 : 1 ,<br>\nWe decide to change all of them to 0"},{"metadata":{"trusted":true},"cell_type":"code","source":"train = train[train.item_price > 0].reset_index(drop=True)\ntrain[train.item_cnt_day <= 0].item_cnt_day.unique()\ntrain.loc[train.item_cnt_day < 1, 'item_cnt_day'] = 0","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Several shops are duplicates of each other (according to its name). Fix train and test set.<br>\nWe add 40 to 39."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Якутск Орджоникидзе, 56\ntrain.loc[train.shop_id == 0, 'shop_id'] = 57\ntest.loc[test.shop_id == 0, 'shop_id'] = 57\n# Якутск ТЦ \"Центральный\"\ntrain.loc[train.shop_id == 1, 'shop_id'] = 58\ntest.loc[test.shop_id == 1, 'shop_id'] = 58\n# Жуковский ул. Чкалова 39м²\ntrain.loc[train.shop_id == 11, 'shop_id'] = 10\ntest.loc[test.shop_id == 11, 'shop_id'] = 10\n\ntrain.loc[train.shop_id == 40, 'shop_id'] = 39\ntest.loc[test.shop_id == 40, 'shop_id'] = 39","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Shops dataset preprocessing"},{"metadata":{"trusted":true},"cell_type":"code","source":"shops.shop_name.unique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We categorize subtype of shops in ['Орджоникидзе,' 'ТЦ' 'ТРК' 'ТРЦ', 'ул.' 'Магазин' 'ТК' 'склад' ] <br>\nThen transform other values to 'etc'"},{"metadata":{"trusted":true},"cell_type":"code","source":"shops.loc[shops.shop_name == 'Сергиев Посад ТЦ \"7Я\"', 'shop_name'] = 'СергиевПосад ТЦ \"7Я\"'\n\nshops['city'] = shops['shop_name'].str.split(' ').map(lambda x: x[0])\nshops['category'] = shops['shop_name'].str.split(' ').map(lambda x:x[1]).astype(str)\n\nshops.loc[shops.city == '!Якутск', 'city'] = 'Якутск'\n\ncategory = ['Орджоникидзе,', 'ТЦ', 'ТРК', 'ТРЦ','ул.', 'Магазин', 'ТК', 'склад']\nshops.category = shops.category.apply(lambda x: x if (x in category) else 'etc')\nshops.category.unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"shops.groupby(['category']).sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"However, some categories have small values. So we reduce categories 9 to 5.<br>\n['Орджоникидзе,', 'ТЦ', 'ТРК', 'ТРЦ','ул.', 'Магазин', 'ТК', 'склад', 'etc'] => ['ТЦ', 'ТРК', 'ТРЦ', 'ТК', 'etc']**"},{"metadata":{"trusted":true},"cell_type":"code","source":"category = ['ТЦ', 'ТРК', 'ТРЦ', 'ТК']\nshops.category = shops.category.apply(lambda x: x if (x in category) else 'etc')\nprint('Category Distribution', shops.groupby(['category']).sum())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"shops['shop_city'] = shops.city\nshops['shop_category'] = shops.category\n\nshops['shop_city'] = LabelEncoder().fit_transform(shops['shop_city'])\nshops['shop_category'] = LabelEncoder().fit_transform(shops['shop_category'])\n\nshops = shops[['shop_id','shop_city', 'shop_category']]\nshops.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Category dataset preprocessing"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(len(cats.item_category_name.unique()))\ncats.item_category_name.unique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We think that category 'Игровые консоли' and  'Аксессуары' are same as 'Игры'. <br>\nSo, we transform the two features to 'Игры'<br>\nAlso, PC - Гарнитуры/Наушники and \nchange to Музыка - Гарнитуры/Наушники <br><br>"},{"metadata":{"trusted":true},"cell_type":"code","source":"cats['type_code'] = cats.item_category_name.apply(lambda x: x.split(' ')[0]).astype(str)\ncats.loc[(cats.type_code == 'Игровые') | (cats.type_code == 'Аксессуары'), 'category'] = 'Игры'\ncats.loc[cats.type_code == 'PC', 'category'] = 'Музыка'\n\ncategory = ['Игры', 'Карты', 'Кино', 'Книги','Музыка', 'Подарки', 'Программы', 'Служебные', 'Чистые']\n\ncats['type_code'] = cats.type_code.apply(lambda x: x if (x in category) else 'etc')\n\nprint(cats.groupby(['type_code']).sum())\ncats['type_code'] = LabelEncoder().fit_transform(cats['type_code'])\n\ncats['split'] = cats.item_category_name.apply(lambda x: x.split('-'))\ncats['subtype'] = cats['split'].map(lambda x: x[1].strip() if len(x) > 1 else x[0].strip())\ncats['subtype_code'] = LabelEncoder().fit_transform(cats['subtype'])\ncats = cats[['item_category_id','type_code', 'subtype_code']]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Category dataset preprocessing <br>\nThis code get from [1st place solution - Part 1 - \"Hands on Data\"](https://www.kaggle.com/kyakovlev/1st-place-solution-part-1-hands-on-data). <bt><br>\nWe use features 'name_2' and 'name_3' below code."},{"metadata":{"trusted":true},"cell_type":"code","source":"import re\nfrom collections import Counter\nfrom operator import itemgetter\n\nitems = pd.read_csv('../input/' + PATH + '/items.csv')\n\nitems['name_1'], items['name_2'] = items['item_name'].str.split('[', 1).str\nitems['name_1'], items['name_3'] = items['item_name'].str.split('(', 1).str\n\nitems['name_2'] = items['name_2'].str.replace('[^A-Za-z0-9А-Яа-я]+', ' ').str.lower()\nitems['name_3'] = items['name_3'].str.replace('[^A-Za-z0-9А-Яа-я]+', ' ').str.lower()\nitems = items.fillna('0')\n\nresult_1 = Counter(' '.join(items['name_2'].values.tolist()).split(' ')).items()\nresult_1 = sorted(result_1, key=itemgetter(1))\nresult_1 = pd.DataFrame(result_1, columns=['feature', 'count'])\nresult_1 = result_1[(result_1['feature'].str.len() > 1) & (result_1['count'] > 200)]\n\nresult_2 = Counter(' '.join(items['name_3'].values.tolist()).split(\" \")).items()\nresult_2 = sorted(result_2, key=itemgetter(1))\nresult_2 = pd.DataFrame(result_2, columns=['feature', 'count'])\nresult_2 = result_2[(result_2['feature'].str.len() > 1) & (result_2['count'] > 200)]\n\nresult = pd.concat([result_1, result_2])\nresult = result.drop_duplicates(subset=['feature']).reset_index(drop=True)\n\nprint('Most common aditional features:', result)\n\ndef name_correction(x):\n    x = x.lower()\n    x = x.partition('[')[0]\n    x = x.partition('(')[0]\n    x = re.sub('[^A-Za-z0-9А-Яа-я]+', ' ', x)\n    x = x.replace('  ', ' ')\n    x = x.strip()\n    return x\n\nitems['item_name'] = items['item_name'].apply(lambda x: name_correction(x))\nitems.name_2 = items.name_2.apply(lambda x: x[:-1] if x != '0' else '0')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's create type of item_names. First, cleansing results of name_2. <br>\nConcatenate same meaning of types such as, 'x360' & 'xbox360' -> xbox 360 <br>\nThree words 'pc' in below code look like same. But, after label encoding they transformed different values."},{"metadata":{"trusted":true},"cell_type":"code","source":"items['type'] = items.name_2.apply(lambda x: x[0:8] if x.split(' ')[0] == 'xbox' else x.split(' ')[0])\nitems.loc[(items.type == 'x360') | (items.type == 'xbox360'), 'type'] = 'xbox 360'\nitems.loc[items.type == '', 'type'] = 'mac'\nitems.type = items.type.apply(lambda x: x.replace(' ',''))\nitems.loc[(items.type == 'pc') | (items.type == 'pс') | (items.type == 'рс'), 'type'] = 'pc'\nitems.loc[(items.type == 'рs3'), 'type'] = 'ps3'\n\ngroup_sum = items.groupby('type').sum()\ngroup_sum.loc[group_sum.item_category_id < 200]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"And then, drop types"},{"metadata":{"trusted":true},"cell_type":"code","source":"drop_list = ['5c5', '5c7', '5f4', '6dv', '6jv', '6l6', 'android', 'hm3', 'j72', 'kf6', 'kf7','kg4',\n            'ps2', 's3v', 's4v'\t,'англ', 'русская', 'только', 'цифро']\n\nitems.name_2 = items.type.apply(lambda x: 'etc' if x in drop_list else x)\nitems = items.drop(['type'], axis=1)\nitems.groupby('name_2').sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"items.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"items['name_2'] = LabelEncoder().fit_transform(items['name_2'])\nitems['name_3'] = LabelEncoder().fit_transform(items['name_3'])\nitems.drop(['item_name', 'name_1'], axis=1, inplace=True)\nitems.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Monthly sales\nTest set is a product of some shops and some items within 34 month. There are 5100 items * 42 shops = 214200 pairs. 363 items are new compared to the train. Hence, for the most of the items in the test set target value should be zero. \nIn the other hand train set contains only pairs which were sold or returned in the past. Tha main idea is to calculate monthly sales and <b>extend it with zero sales</b> for each unique pair within the month. This way train data will be similar to test data."},{"metadata":{"trusted":true},"cell_type":"code","source":"ts = time.time()\nmatrix = []\ncols = ['date_block_num','shop_id','item_id']\nfor i in range(34):\n    sales = train[train.date_block_num==i]\n    matrix.append(np.array(list(product([i], sales.shop_id.unique(), sales.item_id.unique())), dtype='int16'))\n    \nmatrix = pd.DataFrame(np.vstack(matrix), columns=cols)\nmatrix['date_block_num'] = matrix['date_block_num'].astype(np.int8)\nmatrix['shop_id'] = matrix['shop_id'].astype(np.int8)\nmatrix['item_id'] = matrix['item_id'].astype(np.int16)\nmatrix.sort_values(cols,inplace=True)\ntime.time() - ts","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(matrix)\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Aggregate train set by shop/item pairs to calculate target aggreagates, then <b>clip(0,20)</b> target value. This way train target will be similar to the test predictions.<br>\nhttps://www.kaggle.com/c/competitive-data-science-predict-future-sales/discussion/50149#latest-287470\n\n<i>I use floats instead of ints for item_cnt_month to avoid downcasting it after concatination with the test set later. If it would be int16, after concatination with NaN values it becomes int64, but foat16 becomes float16 even with NaNs.</i>"},{"metadata":{"trusted":true},"cell_type":"code","source":"train['revenue'] = train['item_price'] *  train['item_cnt_day']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ts = time.time()\ngroup = train.groupby(['date_block_num','shop_id','item_id']).agg({'item_cnt_day': ['sum']})\n\ngroup.columns = ['item_cnt_month']\ngroup.reset_index(inplace=True)\nmatrix = pd.merge(matrix, group, on=cols, how='left')\nmatrix['item_cnt_month'] = (matrix['item_cnt_month']\n                                .fillna(0)#.astype(np.float16))\n                                .clip(0,20) # NB clip target here\n                                .astype(np.float16))\ntime.time() - ts","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Test set\nTo use time tricks append test pairs to the matrix."},{"metadata":{"trusted":true},"cell_type":"code","source":"test['date_block_num'] = 34\ntest['date_block_num'] = test['date_block_num'].astype(np.int8)\ntest['shop_id'] = test['shop_id'].astype(np.int8)\ntest['item_id'] = test['item_id'].astype(np.int16)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ts = time.time()\nmatrix = pd.concat([matrix, test], ignore_index=True, sort=False, keys=cols)\nmatrix.fillna(0, inplace=True) # 34 month\ntime.time() - ts","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Shops/Items/Cats features"},{"metadata":{"trusted":true},"cell_type":"code","source":"ts = time.time()\nmatrix = pd.merge(matrix, shops, on=['shop_id'], how='left')\nmatrix = pd.merge(matrix, items, on=['item_id'], how='left')\nmatrix = pd.merge(matrix, cats, on=['item_category_id'], how='left')\nmatrix['shop_city'] = matrix['shop_city'].astype(np.int8)\nmatrix['shop_category'] = matrix['shop_category'].astype(np.int8)\nmatrix['item_category_id'] = matrix['item_category_id'].astype(np.int8)\nmatrix['type_code'] = matrix['type_code'].astype(np.int8)\nmatrix['subtype_code'] = matrix['subtype_code'].astype(np.int8)\ntime.time() - ts","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"matrix.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Traget lags\nWe use lag_feature : 3"},{"metadata":{"trusted":true},"cell_type":"code","source":"def lag_feature(df, lags, col):\n    tmp = df[['date_block_num','shop_id','item_id',col]]\n    for i in lags:\n        shifted = tmp.copy()\n        shifted.columns = ['date_block_num','shop_id','item_id', col+'_lag_'+str(i)]\n        shifted['date_block_num'] += i\n        df = pd.merge(df, shifted, on=['date_block_num','shop_id','item_id'], how='left')\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ts = time.time()\nmatrix = lag_feature(matrix, [1,2,3], 'item_cnt_month')\ntime.time() - ts","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Mean encoded features<br><br>"},{"metadata":{"trusted":true},"cell_type":"code","source":"ts = time.time()\ngroup = matrix.groupby(['date_block_num']).agg({'item_cnt_month': ['mean']})\ngroup.columns = [ 'date_avg_item_cnt' ]\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['date_block_num'], how='left')\nmatrix['date_avg_item_cnt'] = matrix['date_avg_item_cnt'].astype(np.float16)\nmatrix = lag_feature(matrix, [1], 'date_avg_item_cnt')\nmatrix.drop(['date_avg_item_cnt'], axis=1, inplace=True)\ntime.time() - ts","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ts = time.time()\ngroup = matrix.groupby(['date_block_num', 'item_id']).agg({'item_cnt_month': ['mean']})\ngroup.columns = [ 'date_item_avg_item_cnt' ]\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['date_block_num','item_id'], how='left')\nmatrix['date_item_avg_item_cnt'] = matrix['date_item_avg_item_cnt'].astype(np.float16)\nmatrix = lag_feature(matrix, [1,2,3], 'date_item_avg_item_cnt')\nmatrix.drop(['date_item_avg_item_cnt'], axis=1, inplace=True)\ntime.time() - ts","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ts = time.time()\ngroup = matrix.groupby(['date_block_num', 'shop_id']).agg({'item_cnt_month': ['mean']})\ngroup.columns = [ 'date_shop_avg_item_cnt' ]\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['date_block_num','shop_id'], how='left')\nmatrix['date_shop_avg_item_cnt'] = matrix['date_shop_avg_item_cnt'].astype(np.float16)\nmatrix = lag_feature(matrix, [1,2,3], 'date_shop_avg_item_cnt')\nmatrix.drop(['date_shop_avg_item_cnt'], axis=1, inplace=True)\ntime.time() - ts","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ts = time.time()\ngroup = matrix.groupby(['date_block_num', 'item_category_id']).agg({'item_cnt_month': ['mean']})\ngroup.columns = [ 'date_cat_avg_item_cnt' ]\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['date_block_num','item_category_id'], how='left')\nmatrix['date_cat_avg_item_cnt'] = matrix['date_cat_avg_item_cnt'].astype(np.float16)\nmatrix = lag_feature(matrix, [1], 'date_cat_avg_item_cnt')\nmatrix.drop(['date_cat_avg_item_cnt'], axis=1, inplace=True)\ntime.time() - ts","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ts = time.time()\ngroup = matrix.groupby(['date_block_num', 'shop_id', 'item_category_id']).agg({'item_cnt_month': ['mean']})\ngroup.columns = ['date_shop_cat_avg_item_cnt']\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['date_block_num', 'shop_id', 'item_category_id'], how='left')\nmatrix['date_shop_cat_avg_item_cnt'] = matrix['date_shop_cat_avg_item_cnt'].astype(np.float16)\nmatrix = lag_feature(matrix, [1], 'date_shop_cat_avg_item_cnt')\nmatrix.drop(['date_shop_cat_avg_item_cnt'], axis=1, inplace=True)\ntime.time() - ts","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ts = time.time()\ngroup = matrix.groupby(['date_block_num', 'shop_id', 'itme_id']).agg({'item_cnt_month': ['mean']})\ngroup.columns = ['date_shop_item_avg_item_cnt']\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['date_block_num', 'shop_id', 'itme_id'], how='left')\nmatrix['date_shop_item_avg_item_cnt'] = matrix['date_shop_item_avg_item_cnt'].astype(np.float16)\nmatrix = lag_feature(matrix, [1], 'date_shop_item_avg_item_cnt')\nmatrix.drop(['date_shop_item_avg_item_cnt'], axis=1, inplace=True)\ntime.time() - ts","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ts = time.time()\ngroup = matrix.groupby(['date_block_num', 'shop_id', 'subtype_code']).agg({'item_cnt_month': ['mean']})\ngroup.columns = ['date_shop_subtype_avg_item_cnt']\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['date_block_num', 'shop_id', 'subtype_code'], how='left')\nmatrix['date_shop_subtype_avg_item_cnt'] = matrix['date_shop_subtype_avg_item_cnt'].astype(np.float16)\nmatrix = lag_feature(matrix, [1], 'date_shop_subtype_avg_item_cnt')\nmatrix.drop(['date_shop_subtype_avg_item_cnt'], axis=1, inplace=True)\ntime.time() - ts","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ts = time.time()\ngroup = matrix.groupby(['date_block_num', 'shop_city']).agg({'item_cnt_month': ['mean']})\ngroup.columns = [ 'date_city_avg_item_cnt' ]\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['date_block_num', 'shop_city'], how='left')\nmatrix['date_city_avg_item_cnt'] = matrix['date_city_avg_item_cnt'].astype(np.float16)\nmatrix = lag_feature(matrix, [1], 'date_city_avg_item_cnt')\nmatrix.drop(['date_city_avg_item_cnt'], axis=1, inplace=True)\ntime.time() - ts","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ts = time.time()\ngroup = matrix.groupby(['date_block_num', 'item_id', 'shop_city']).agg({'item_cnt_month': ['mean']})\ngroup.columns = [ 'date_item_city_avg_item_cnt' ]\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['date_block_num', 'item_id', 'shop_city'], how='left')\nmatrix['date_item_city_avg_item_cnt'] = matrix['date_item_city_avg_item_cnt'].astype(np.float16)\nmatrix = lag_feature(matrix, [1], 'date_item_city_avg_item_cnt')\nmatrix.drop(['date_item_city_avg_item_cnt'], axis=1, inplace=True)\ntime.time() - ts","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ts = time.time()\n# group = matrix.groupby(['date_block_num', 'type_code']).agg({'item_cnt_month': ['mean']})\n# group.columns = [ 'date_type_avg_item_cnt' ]\n# group.reset_index(inplace=True)\n\n# matrix = pd.merge(matrix, group, on=['date_block_num', 'type_code'], how='left')\n# matrix['date_type_avg_item_cnt'] = matrix['date_type_avg_item_cnt'].astype(np.float16)\n# matrix = lag_feature(matrix, [1], 'date_type_avg_item_cnt')\n# matrix.drop(['date_type_avg_item_cnt'], axis=1, inplace=True)\n# time.time() - ts","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ts = time.time()\n# group = matrix.groupby(['date_block_num', 'subtype_code']).agg({'item_cnt_month': ['mean']})\n# group.columns = [ 'date_subtype_avg_item_cnt' ]\n# group.reset_index(inplace=True)\n\n# matrix = pd.merge(matrix, group, on=['date_block_num', 'subtype_code'], how='left')\n# matrix['date_subtype_avg_item_cnt'] = matrix['date_subtype_avg_item_cnt'].astype(np.float16)\n# matrix = lag_feature(matrix, [1], 'date_subtype_avg_item_cnt')\n# matrix.drop(['date_subtype_avg_item_cnt'], axis=1, inplace=True)\n# time.time() - ts","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ts = time.time()\ngroup = matrix.groupby(['date_block_num','shop_id' ,'item_id']).agg({'item_cnt_month': ['mean']})\ngroup.columns = [ 'date_shop_item_avg_item_cnt' ]\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['date_block_num','shop_id' ,'item_id'], how='left')\nmatrix['date_shop_item_avg_item_cnt'] = matrix['date_shop_item_avg_item_cnt'].astype(np.float16)\nmatrix = lag_feature(matrix, [1], 'date_shop_item_avg_item_cnt')\nmatrix.drop(['date_shop_item_avg_item_cnt'], axis=1, inplace=True)\ntime.time() - ts","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Trend Features"},{"metadata":{"trusted":true},"cell_type":"code","source":"ts = time.time()\ngroup = train.groupby(['item_id']).agg({'item_price': ['mean']})\ngroup.columns = ['item_avg_item_price']\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['item_id'], how='left')\nmatrix['item_avg_item_price'] = matrix['item_avg_item_price'].astype(np.float16)\n\ngroup = train.groupby(['date_block_num','item_id']).agg({'item_price': ['mean']})\ngroup.columns = ['date_item_avg_item_price']\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['date_block_num','item_id'], how='left')\nmatrix['date_item_avg_item_price'] = matrix['date_item_avg_item_price'].astype(np.float16)\n\nlags = [1,2,3]\nmatrix = lag_feature(matrix, lags, 'date_item_avg_item_price')\n\nfor i in lags:\n    matrix['delta_price_lag_'+str(i)] = \\\n        (matrix['date_item_avg_item_price_lag_'+str(i)] - matrix['item_avg_item_price']) / matrix['item_avg_item_price']\n\ndef select_trend(row):\n    for i in lags:\n        if row['delta_price_lag_'+str(i)]:\n            return row['delta_price_lag_'+str(i)]\n    return 0\n    \nmatrix['delta_price_lag'] = matrix.apply(select_trend, axis=1)\nmatrix['delta_price_lag'] = matrix['delta_price_lag'].astype(np.float16)\nmatrix['delta_price_lag'].fillna(0, inplace=True)\n\nfetures_to_drop = ['item_avg_item_price', 'date_item_avg_item_price']\nfor i in lags:\n    fetures_to_drop += ['date_item_avg_item_price_lag_'+str(i)]\n    fetures_to_drop += ['delta_price_lag_'+str(i)]\n\nmatrix.drop(fetures_to_drop, axis=1, inplace=True)\n\ntime.time() - ts","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ts = time.time()\ngroup = train.groupby(['date_block_num','shop_id']).agg({'revenue': ['sum']})\ngroup.columns = ['date_shop_revenue']\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['date_block_num','shop_id'], how='left')\nmatrix['date_shop_revenue'] = matrix['date_shop_revenue'].astype(np.float32)\n\ngroup = group.groupby(['shop_id']).agg({'date_shop_revenue': ['mean']})\ngroup.columns = ['shop_avg_revenue']\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['shop_id'], how='left')\nmatrix['shop_avg_revenue'] = matrix['shop_avg_revenue'].astype(np.float32)\n\nmatrix['delta_revenue'] = (matrix['date_shop_revenue'] - matrix['shop_avg_revenue']) / matrix['shop_avg_revenue']\nmatrix['delta_revenue'] = matrix['delta_revenue'].astype(np.float16)\n\nmatrix = lag_feature(matrix, [1], 'delta_revenue')\n\nmatrix.drop(['date_shop_revenue','shop_avg_revenue','delta_revenue'], axis=1, inplace=True)\ntime.time() - ts","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Add month, and days in a month."},{"metadata":{"trusted":true},"cell_type":"code","source":"matrix['month'] = matrix['date_block_num'] % 12\n\ndays = pd.Series([31,28,31,30,31,30,31,31,30,31,30,31])\nmatrix['days'] = matrix['month'].map(days).astype(np.int8)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ts = time.time()\n# cache = {}\n# matrix['item_shop_last_sale'] = -1\n# matrix['item_shop_last_sale'] = matrix['item_shop_last_sale'].astype(np.int8)\n# for idx, row in matrix.iterrows():    \n#     key = str(row.item_id)+' '+str(row.shop_id)\n#     if key not in cache:\n#         if row.item_cnt_month!=0:\n#             cache[key] = row.date_block_num\n#     else:\n#         last_date_block_num = cache[key]\n#         matrix.at[idx, 'item_shop_last_sale'] = row.date_block_num - last_date_block_num\n#         cache[key] = row.date_block_num         \n# time.time() - ts","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ts = time.time()\n# cache = {}\n# matrix['item_last_sale'] = -1\n# matrix['item_last_sale'] = matrix['item_last_sale'].astype(np.int8)\n# for idx, row in matrix.iterrows():    \n#     key = row.item_id\n#     if key not in cache:\n#         if row.item_cnt_month!=0:\n#             cache[key] = row.date_block_num\n#     else:\n#         last_date_block_num = cache[key]\n#         if row.date_block_num>last_date_block_num:\n#             matrix.at[idx, 'item_last_sale'] = row.date_block_num - last_date_block_num\n#             cache[key] = row.date_block_num         \n# time.time() - ts","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ts = time.time()\nmatrix['item_shop_first_sale'] = matrix['date_block_num'] - matrix.groupby(['item_id','shop_id'])['date_block_num'].transform('min')\nmatrix['item_first_sale'] = matrix['date_block_num'] - matrix.groupby('item_id')['date_block_num'].transform('min')\ntime.time() - ts","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Final preparations\nBecause of the using 3 as lag value drop first 3 months. Also drop all the columns with this month calculated values (other words which can not be calcucated for the test set)."},{"metadata":{"trusted":true},"cell_type":"code","source":"ts = time.time()\nmatrix = matrix[matrix.date_block_num > 3]\ntime.time() - ts","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Producing lags brings a lot of nulls."},{"metadata":{"trusted":true},"cell_type":"code","source":"ts = time.time()\ndef fill_na(df):\n    for col in df.columns:\n        if ('_lag_' in col) & (df[col].isnull().any()):\n            if ('item_cnt' in col):\n                df[col].fillna(0, inplace=True)         \n    return df\n\nmatrix = fill_na(matrix)\ntime.time() - ts","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"matrix.info()\n\ndel group\ndel items\ndel shops\ndel cats\ndel train\n# leave test for submission\ngc.collect();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"matrix.to_pickle('../working/data.pkl')\n\ndel matrix\ngc.collect();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3> We recommend creating another notebook to build xgb model because of saving ur time :). <br>\n\n## Build XGB Models (* with GPU *) <br>\nIf u want to use 'CPU' mode, eliminate argument tree_method='gpu_hist'. (But we recommend 'GPU' mode)"},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nimport gc\nimport pickle\nimport time\nimport pandas as pd\nimport numpy as np\nfrom xgboost import XGBRegressor\nimport matplotlib.pylab as plt\nfrom matplotlib.pylab import rcParams\nrcParams['figure.figsize'] = 12, 4","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.read_pickle('../working/data.pkl')\ntest  = pd.read_csv('../input/competitive-data-science-predict-future-sales/test.csv').set_index('ID')\n\nprint(len(data.columns))\ndata.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Select best features"},{"metadata":{"trusted":true},"cell_type":"code","source":"data = data[[\n   'date_block_num', 'shop_id', 'item_id', 'item_cnt_month', 'shop_city',\n       'shop_category', 'item_category_id', 'name_2', 'name_3', 'type_code',\n       'subtype_code', 'item_cnt_month_lag_1', 'item_cnt_month_lag_2',\n       'item_cnt_month_lag_3', 'date_avg_item_cnt_lag_1',\n       'date_item_avg_item_cnt_lag_1', 'date_item_avg_item_cnt_lag_2',\n       'date_item_avg_item_cnt_lag_3', 'date_shop_avg_item_cnt_lag_1',\n       'date_shop_avg_item_cnt_lag_2', 'date_shop_avg_item_cnt_lag_3',\n       'date_cat_avg_item_cnt_lag_1', 'date_shop_cat_avg_item_cnt_lag_1',\n       'date_shop_subtype_avg_item_cnt_lag_1', 'date_city_avg_item_cnt_lag_1',\n       'date_item_city_avg_item_cnt_lag_1', \n#     'date_type_avg_item_cnt_lag_1',\n#        'date_subtype_avg_item_cnt_lag_1', \n    'date_shop_item_avg_item_cnt_lag_1',\n    'delta_price_lag',\n       'delta_revenue_lag_1', 'month', 'days', \n#     'item_shop_last_sale',\n#        'item_last_sale', \n    'item_shop_first_sale', \n    'item_first_sale'\n]]\n\nlen(data.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = data[data.date_block_num < 33].drop(['item_cnt_month'], axis=1)\nY_train = data[data.date_block_num < 33]['item_cnt_month']\nX_valid = data[data.date_block_num == 33].drop(['item_cnt_month'], axis=1)\nY_valid = data[data.date_block_num == 33]['item_cnt_month']\nX_test = data[data.date_block_num == 34].drop(['item_cnt_month'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del data\ngc.collect();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ts = time.time()\n\nmodel = XGBRegressor(\n    max_depth=10,\n    n_estimators=1000,\n    min_child_weight=0.5, \n    colsample_bytree=0.8, \n    subsample=0.8, \n    eta=0.1,\n    tree_method='gpu_hist',\n    seed=42)\n\nmodel.fit(\n    X_train, \n    Y_train, \n    eval_metric=\"rmse\", \n    eval_set=[(X_train, Y_train), (X_valid, Y_valid)], \n    verbose=True, \n    early_stopping_rounds = 20)\n\ntime.time() - ts","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Y_pred = model.predict(X_valid).clip(0, 20)\nY_test = model.predict(X_test).clip(0, 20)\n\nsubmission = pd.DataFrame({\n    \"ID\": test.index, \n    \"item_cnt_month\": Y_test\n})\nsubmission.to_csv('xgb_submission.csv', index=False)\n\n# save predictions for an ensemble\npickle.dump(Y_pred, open('xgb_train.pickle', 'wb'))\npickle.dump(Y_test, open('xgb_test.pickle', 'wb'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from xgboost import plot_importance\n\ndef plot_features(booster, figsize):    \n    fig, ax = plt.subplots(1,1,figsize=figsize)\n    return plot_importance(booster=booster, ax=ax)\n\nplot_features(model, (10,14))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}