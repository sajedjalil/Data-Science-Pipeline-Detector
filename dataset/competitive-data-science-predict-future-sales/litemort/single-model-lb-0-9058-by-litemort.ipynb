{"cells":[{"metadata":{},"cell_type":"markdown","source":"LiteMORT is a new open source gradient boosting lib( https://github.com/closest-git/LiteMORT).  \nIn this kernel, it's much faster than XGBoost with a little higher accuracy. (The LB of this notebook is 0.9058)\n"},{"metadata":{},"cell_type":"markdown","source":"Step 1: Install litemort and import.  \n"},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install -i https://test.pypi.org/simple/  litemort==0.1.7\nfrom LiteMORT import *\n\nfrom itertools import product\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import mean_squared_error\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom xgboost import XGBRegressor\nfrom xgboost import plot_importance\nimport time\nimport sys\nimport gc\nimport pickle\nimport random\nimport os\n\n#from bayes_opt import BayesianOptimization\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"If \"*isMORT*\" is true, we will call litemort, otherwise XGBoost"},{"metadata":{"trusted":true},"cell_type":"code","source":"#isMORT = len(sys.argv)>1 and sys.argv[1] == \"mort\"\nisMORT = True\nalg='MORT' if isMORT else 'XGB'\nprint(f\"gradient boosting lib={alg}\")\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Step 2 Load data.  \nSome codes and the \"data.pkl\" are forked from https://www.kaggle.com/dhimananubhav/feature-engineering-xgboost.  \nFor the detail of feature engineering, please visit that notebook."},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 100)\n\n\n#some_rows = 5000\nsome_rows = None\ndata_root = '../input/'\n#data_root = \"~/Datasets/future_sales\"\n\ntest  = pd.read_csv(f'{data_root}/competitive-data-science-predict-future-sales/test.csv').set_index('ID')\ndata = pd.read_pickle(f'{data_root}/predict-future-sales/data.pkl')\nif some_rows is not None:\n    nMost=data.shape[0]\n    random.seed(42)\n    subset = random.sample(range(nMost), some_rows)\n    data = data.iloc[subset, :].reset_index(drop=True)\n    print('====== Some Samples ... data={}'.format(data.shape))\n\ndata = data[[\n    'date_block_num',\n    'shop_id',\n    'item_id',\n    'item_cnt_month',\n    'city_code',\n    'item_category_id',\n    'type_code',\n    'subtype_code',\n    'item_cnt_month_lag_1',\n    'item_cnt_month_lag_2',\n    'item_cnt_month_lag_3',\n    'item_cnt_month_lag_6',\n    'item_cnt_month_lag_12',\n    'date_avg_item_cnt_lag_1',\n    'date_item_avg_item_cnt_lag_1',\n    'date_item_avg_item_cnt_lag_2',\n    'date_item_avg_item_cnt_lag_3',\n    'date_item_avg_item_cnt_lag_6',\n    'date_item_avg_item_cnt_lag_12',\n    'date_shop_avg_item_cnt_lag_1',\n    'date_shop_avg_item_cnt_lag_2',\n    'date_shop_avg_item_cnt_lag_3',\n    'date_shop_avg_item_cnt_lag_6',\n    'date_shop_avg_item_cnt_lag_12',\n    'date_cat_avg_item_cnt_lag_1',\n    'date_shop_cat_avg_item_cnt_lag_1',\n    #'date_shop_type_avg_item_cnt_lag_1',\n    #'date_shop_subtype_avg_item_cnt_lag_1',\n    'date_city_avg_item_cnt_lag_1',\n    'date_item_city_avg_item_cnt_lag_1',\n    #'date_type_avg_item_cnt_lag_1',\n    #'date_subtype_avg_item_cnt_lag_1',\n    'delta_price_lag',\n    'month',\n    'days',\n    'item_shop_last_sale',\n    'item_last_sale',\n    'item_shop_first_sale',\n    'item_first_sale',\n]]\n\nX_train = data[data.date_block_num < 33].drop(['item_cnt_month'], axis=1)\nY_train = data[data.date_block_num < 33]['item_cnt_month']\nX_valid = data[data.date_block_num == 33].drop(['item_cnt_month'], axis=1)\nY_valid = data[data.date_block_num == 33]['item_cnt_month']\nX_test = data[data.date_block_num == 34].drop(['item_cnt_month'], axis=1)\nprint(f\"X_train={X_train.shape} Y_train={Y_train.shape}\")\nprint(f\"X_valid={X_valid.shape} Y_valid={Y_valid.shape}\")\nprint(f\"X_test={X_test.shape} \")\ndel data\ngc.collect();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Step 3 Set the parameter of litemort and testing.  \nLiteMORT use sklearn-like api interface.\nThe meaning of these parameters are similar to that of lightGBM or XGBOOST"},{"metadata":{"_uuid":"3059599f-b7d9-48a3-a21d-69c504157c59","_cell_guid":"1d8775b7-9b98-4c60-a689-be400738593f","trusted":true},"cell_type":"code","source":"params={'num_leaves': 550,   'n_estimators':1000,'early_stopping_rounds':20,\n        'feature_fraction': 1,     'bagging_fraction': 1,\n        'max_bin': 512,\n      # \"adaptive\":'weight1',\n    #\"learning_schedule\":\"adaptive\",\n     'max_depth': 10,\n     'min_child_weight': 300,    #'min_data_in_leaf': 300,\n     'learning_rate': 0.1,\n     'objective': 'regression',\n     'boosting_type': 'gbdt',\n     'verbose': 1,\n     'metric': {'rmse'}\n}\n\nif isMORT:\n    print(f\"Call LiteMORT... \")    \n    t0=time.time()\n    model = LiteMORT(params).fit(X_train,Y_train,eval_set=[(X_valid, Y_valid)])\n    print(f\"LiteMORT......OK time={time.time()-t0:.4g} model={model}\")\nelse:\n    model = XGBRegressor(\n        max_depth=8,\n        n_estimators=1000,\n        min_child_weight=300,\n        colsample_bytree=0.8,\n        subsample=0.8,\n        eta=0.3,\n        seed=42)\n\n    model.fit(\n        X_train,\n        Y_train,\n        eval_metric=\"rmse\",\n        eval_set=[(X_train, Y_train), (X_valid, Y_valid)],\n        verbose=True,\n        early_stopping_rounds = 10)\n    alg = 'xgboost'\n\nY_pred = model.predict(X_valid).clip(0, 20)\nscore = np.sqrt(mean_squared_error(Y_pred, Y_valid))\nY_test = model.predict(X_test).clip(0, 20)\nprint(f\"score={score}\")\n\ndef plot_features(booster, figsize):\n    fig, ax = plt.subplots(1,1,figsize=figsize)\n    return plot_importance(booster=booster, ax=ax)\n\nif not isMORT:\n    plot_features(model, (10, 14))\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Step 4 Get result  \nThe mse error in valid dataset is 0.9025 and the LB is 0.9058"},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.DataFrame({\n        \"ID\": test.index,\n        \"item_cnt_month\": Y_test\n    })\npath = f'submission.csv'\n#print(f\"submission......path={path}......\")\nsubmission.to_csv(path, index=False)\nprint(f\"......Save submit @{path}......\")\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":1}