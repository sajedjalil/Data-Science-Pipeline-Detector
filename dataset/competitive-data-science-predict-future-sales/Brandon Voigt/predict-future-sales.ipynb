{"cells":[{"metadata":{},"cell_type":"markdown","source":"The provided dataset contains historical sales data for a Russian software company. The goal is to predict the total number of products sold in November 2015 for every shop-item combination in the test set. This notebook processes the data, creates some new features, and then makes predictions using the GradientBoostingRegressor from sklearn."},{"metadata":{"_uuid":"2b8c35be-5a93-4026-a894-959221e47ad2","_cell_guid":"ab07111d-967f-42df-b4e4-ea648f08b92e","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nsales_train = pd.read_csv('../input/competitive-data-science-predict-future-sales/sales_train.csv')\nitems = pd.read_csv('../input/competitive-data-science-predict-future-sales/items.csv')\ntest_ids = pd.read_csv('../input/competitive-data-science-predict-future-sales/test.csv')\n\nsales_train = sales_train.drop(['date'], axis=1)\nsales_train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The ID field refers to a unique shop-item pair."},{"metadata":{"trusted":true},"cell_type":"code","source":"ids = list(np.arange(test_ids['ID'].max()+1))*(sales_train['date_block_num'].max()+1)\ndates = list(np.arange(sales_train['date_block_num'].max()+1))*(test_ids['ID'].max()+1)\ndates.sort()\n\ndate_id_dict = {'ID' : ids, 'date_block_num' : dates}\ndate_id_df = pd.DataFrame.from_dict(date_id_dict)\ndate_id_df = date_id_df.merge(test_ids, on='ID')\ndate_id_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The dataset contains daily sales and we want to predict monthly sales, so we aggregate the sales totals by month. We also calculate the average price for each item."},{"metadata":{"trusted":true},"cell_type":"code","source":"grouped = sales_train.groupby(['date_block_num', 'shop_id', 'item_id'], as_index=False)\nitem_cnt = pd.DataFrame(grouped.sum())\nitem_cnt = item_cnt.drop(['item_price'], axis=1)\n\ngrouped = sales_train.groupby(['shop_id', 'item_id'])\navg_price = pd.DataFrame(grouped.mean()['item_price'])\n\nmonthly_sales = item_cnt.merge(avg_price, on=['shop_id', 'item_id'])\nmonthly_sales = monthly_sales.merge(test_ids, on=['shop_id', 'item_id'])\nmonthly_sales.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The dataset only records when a product was actually sold. There may be months where a certain product was not sold in a certain store. So we create an entry for every ID-date_block_num combination, and record a total of 0 if there is no record of a sale for that combination."},{"metadata":{"trusted":true},"cell_type":"code","source":"item_price = monthly_sales[['item_price', 'ID']]\nmonthly_sales = monthly_sales.drop(['item_price'], axis=1)\n\nmonthly_sales = date_id_df.merge(monthly_sales, how='left', on=['ID', 'date_block_num'])\nmonthly_sales = monthly_sales.drop(['shop_id_y', 'item_id_y'], axis=1)\nmonthly_sales['item_cnt_day'].fillna(0, inplace=True)\n\nmonthly_sales = monthly_sales.merge(item_price, how='left', on='ID')\nmonthly_sales['item_price'].fillna(monthly_sales['item_price'].mean(), inplace=True)\n\nmonthly_sales = monthly_sales.drop_duplicates()\ncolumn_dict = {'shop_id_x' : 'shop_id', 'item_id_x' : 'item_id', 'item_cnt_day' : 'item_cnt_month', 'item_price' : 'avg_price'}\nmonthly_sales = monthly_sales.rename(columns=column_dict)\nmonthly_sales.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we incorporate the item_category_id feature, and use date_block_num to create month and year features."},{"metadata":{"trusted":true},"cell_type":"code","source":"monthly_sales = monthly_sales.merge(items, on='item_id')\nmonthly_sales = monthly_sales.drop(['item_name'], axis=1)\n\nmonth = pd.DataFrame([x%12+1 for x in monthly_sales['date_block_num']], columns=['month'])\nyear = pd.DataFrame([np.floor(x/12)+2013 for x in monthly_sales['date_block_num']], columns=['year'])\n\nmonthly_sales = pd.concat([monthly_sales, month, year], axis=1)\nmonthly_sales = monthly_sales[['ID', 'date_block_num', 'shop_id', 'item_category_id', 'avg_price', 'month', 'year', 'item_cnt_month']]\nmonthly_sales.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The total sales in previous months are likely a good predictor of sales in the current month. So we create a lag feature, which records the total sales for each of the previous 12 months."},{"metadata":{"trusted":true},"cell_type":"code","source":"def calculate_item_cnt_lagged(df, lag):\n    tmp = df[['date_block_num', 'ID', 'item_cnt_month']]\n    shifted = tmp.copy()\n    shifted.columns = ['date_block_num', 'ID', 'item_cnt_lag'+str(lag)]\n    shifted.date_block_num = shifted.date_block_num + lag\n    df = pd.merge(df, shifted, on=['date_block_num', 'ID'], how='left')\n    return df\n\nfor lag in range(1, 13):\n    monthly_sales = calculate_item_cnt_lagged(monthly_sales, lag)\n    \nmonthly_sales = monthly_sales[monthly_sales['date_block_num'] > 11]\nmonthly_sales.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we incorporate these same features in the test set."},{"metadata":{"trusted":true},"cell_type":"code","source":"id_price = monthly_sales[['ID', 'avg_price']]\n\nx_test_df = test_ids.merge(items, on='item_id')\nx_test_df = x_test_df.merge(id_price, how='left', on='ID')\nx_test_df.insert(loc=2, column='month', value=11)\nx_test_df.insert(loc=3, column='year', value=2015)\nx_test_df.insert(loc=4, column='date_block_num', value=34)\n\nx_test_df = x_test_df.drop_duplicates()\nx_test_df['avg_price'].fillna(x_test_df['avg_price'].mean(), inplace=True)\n\nmonthly_sales_subset = monthly_sales[['ID', 'date_block_num', 'shop_id', 'item_category_id', 'avg_price', 'month', 'year', 'item_cnt_month']]\nx_all_df = pd.concat((x_test_df, monthly_sales_subset))\n\nfor lag in range(1, 13):\n    x_all_df = calculate_item_cnt_lagged(x_all_df, lag)\n    \nx_test_df = x_all_df[x_all_df['date_block_num'] == 34]\nx_test_df = x_test_df[['ID', 'shop_id', 'item_category_id', 'avg_price', 'month', 'year',\n                    'item_cnt_lag1', 'item_cnt_lag2', 'item_cnt_lag3', 'item_cnt_lag4', \n                    'item_cnt_lag5', 'item_cnt_lag6', 'item_cnt_lag7', 'item_cnt_lag8', \n                    'item_cnt_lag9', 'item_cnt_lag10', 'item_cnt_lag11', 'item_cnt_lag12']]\nx_test_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The next step is to prepare the datasets for the GradientBoostingRegressor. We normalize the avg_price feature, use one-hot encoding for the categorical features, and clip the historical sales totals into the [0, 20] range to match the test data. "},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy.sparse import hstack, vstack\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import OneHotEncoder, normalize","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train_price = np.array(monthly_sales['avg_price']).reshape(-1, 1)\nx_test_price = np.array(x_test_df['avg_price']).reshape(-1, 1)\nx_train_price = normalize(x_train_price)\nx_test_price = normalize(x_test_price)\n\nx_train_lags = np.array(monthly_sales[['item_cnt_lag1', 'item_cnt_lag2', 'item_cnt_lag3', 'item_cnt_lag4', \n                                       'item_cnt_lag5', 'item_cnt_lag6', 'item_cnt_lag7', 'item_cnt_lag8', \n                                       'item_cnt_lag9', 'item_cnt_lag10', 'item_cnt_lag11', 'item_cnt_lag12']])\nx_test_lags = np.array(x_test_df[['item_cnt_lag1', 'item_cnt_lag2', 'item_cnt_lag3', 'item_cnt_lag4', \n                                       'item_cnt_lag5', 'item_cnt_lag6', 'item_cnt_lag7', 'item_cnt_lag8', \n                                       'item_cnt_lag9', 'item_cnt_lag10', 'item_cnt_lag11', 'item_cnt_lag12']])\n\nx_train_categorical = np.array(monthly_sales[['shop_id', 'item_category_id', 'month', 'year']])\nx_test_categorical = np.array(x_test_df[['shop_id', 'item_category_id', 'month', 'year']])\nx_all_categorical = np.concatenate((x_train_categorical, x_test_categorical))\ny_train = np.array(monthly_sales['item_cnt_month'])\n\nencoder = OneHotEncoder()\nencoder.fit(x_all_categorical)\nx_train_categorical = encoder.transform(x_train_categorical)\nx_test_categorical = encoder.transform(x_test_categorical)\n\nx_train = hstack([x_train_categorical, x_train_price, x_train_lags])\nx_test = hstack([x_test_categorical, x_test_price, x_test_lags])\n\ny_train = np.clip(y_train, 0, 20)\n\nprint(x_train.shape)\nprint(x_test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Finally, we fit the GradientBoostingRegressor using n_estimators=500 and use this model to make predictions on the test data."},{"metadata":{"trusted":true},"cell_type":"code","source":"gradient_boost = GradientBoostingRegressor(n_estimators=500)\ngradient_boost.fit(x_train, y_train)\ntrain_pred = gradient_boost.predict(x_train)\nrmse = np.sqrt(mean_squared_error(y_train, train_pred))\nprint(f\"RMSE on training set: {rmse}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_pred = gradient_boost.predict(x_test)\ntest_pred = x_test_df.assign(item_cnt_month=test_pred)\ntest_pred = test_pred[['ID', 'item_cnt_month']]\ntest_pred = test_pred.sort_values(by='ID')\ntest_pred.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}