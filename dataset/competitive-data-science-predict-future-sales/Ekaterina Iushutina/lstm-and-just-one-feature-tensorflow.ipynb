{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing\nimport seaborn as sns\nimport matplotlib.pylab as plt\nimport tensorflow as tf\nfrom sklearn import preprocessing","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"items = pd.read_csv(\"/kaggle/input/competitive-data-science-predict-future-sales/items.csv\")\nitems.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"categories = pd.read_csv(\"/kaggle/input/competitive-data-science-predict-future-sales/item_categories.csv\")\ncategories.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"shops = pd.read_csv(\"/kaggle/input/competitive-data-science-predict-future-sales/shops.csv\")\nshops","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sales = pd.read_csv(\"/kaggle/input/competitive-data-science-predict-future-sales/sales_train.csv\")\nsales.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sales_test = pd.read_csv(\"/kaggle/input/competitive-data-science-predict-future-sales/test.csv\")\nsales_test.tail()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Delete the duplicates","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(sum(items.duplicated(['item_name'])))\nprint(sum(categories.duplicated(['item_category_name'])))\nprint(sum(shops.duplicated(['shop_name'])))\n# We can see that the names of shops 10 and 11 differ only by one letter. It is probably the same shop.  \n# Also 0 and 57, 1 and 58, ?39 and 40?","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's find out if shops 10,11,0,57,1,58 present in the dataframe for forecasting.\nuniq_shops = sales_test['shop_id'].unique()\nfor shop in list([10,11,0,57,1,58]):\n    print(shop, shop in uniq_shops)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"new_shop_id = {11: 10, 0: 57, 1: 58}\nshops['shop_id'] = shops['shop_id'].apply(lambda x: new_shop_id[x] if x in new_shop_id.keys() else x)\nsales['shop_id'] = sales['shop_id'].apply(lambda x: new_shop_id[x] if x in new_shop_id.keys() else x)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's drop the pairs (shop_id, item_id) that are not represented in the dataframe for forecasting. And merge two dataframes.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sales = pd.merge(sales_test, sales, on = ('shop_id', 'item_id'), how = 'left')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(sum(sales.duplicated()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# drop the duplicate rows from sales\nsales = sales.drop_duplicates()\nsales.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(sum(sales.duplicated(['ID','date','date_block_num','item_price'])))\nprint(sum(sales.duplicated(['ID','date','date_block_num','item_cnt_day'])))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We should think carefully which row should be dropped. Price will help us. But now we will just keep first duplicate and drop later.\nsales = sales.drop_duplicates(['date','date_block_num','shop_id','item_id','item_cnt_day'])\nsales.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sales = sales.drop_duplicates(['ID','date','date_block_num'], keep = 'last')\nsales.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(items.isnull().sum().sum())\nprint(categories.isnull().sum().sum())\nprint(shops.isnull().sum().sum())\nprint(sales.isnull().sum().sum())\n# There are missing values in the data. Most of them corresponds to IDs from the forcast set that doesn't represent in training set.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Outliers and negative values","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sales.describe()\n# It is possible that item_price and item_cnt_day has outliers (max >> 0.75-quantile), and item_cnt_day has wrong values (min < 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# change a sign of negative values\nsales.loc[sales.item_cnt_day < 0, 'item_cnt_day'] = -1. * sales.loc[sales.item_cnt_day < 0, 'item_cnt_day']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Convert to month data\nLet's group the sales by ID and calculate month number of sold items and average price.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#sales_month = sales.sort_values('date_block_num').groupby(['ID', 'date_block_num'], as_index = False).agg({'item_cnt_day': ['sum'], 'item_price': ['mean']})\n#sales_month.columns = ['ID', 'date_block_num', 'item_cnt_month', 'item_price']\nsales_month = sales.sort_values('date_block_num').groupby(['ID', 'date_block_num'], as_index = False).agg({'item_cnt_day': ['sum']})\nsales_month.columns = ['ID', 'date_block_num', 'item_cnt_month']\nsales_month.sample(10)\n# after we grouped and aggregate data we delete all rows corresponding to IDs that don't present in train data set (and preset just in forcasting set)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sales_month.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's split the data by ID. We will store ID and corresponding data in a list.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def to_IDs(np_data, col_ID):\n    # np_data - sales converted to numpy array\n    # col_ID - name of ID column\n    sales_by_ID = list()\n    IDs = np.unique(np_data[:,col_ID]).astype(int)\n    for i in IDs:\n        positions = np_data[:,col_ID] == i\n        sales_ID = np_data[positions,1:]\n        sales_by_ID.append(sales_ID)\n    return sales_by_ID, IDs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sales_by_ID, list_IDs = to_IDs(sales_month.values,0)\nprint(len(sales_by_ID))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# to decrease calculation time during a code debugging we remove IDs that don't have observtions for last months\ndef remove_ID_nan_last_year(np_data):\n    N_IDs = len(np_data)\n    col_date = 0\n    clear_data = list()\n    cut_month = 33 - 2\n    for i in range(N_IDs):\n        ID_data = np_data[i]\n        if len(ID_data[ID_data[:,col_date] >= cut_month,1]) != 0:\n            clear_data.append(ID_data)\n    return clear_data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#sales_by_ID = remove_ID_nan_last_year(sales_by_ID)\n#len(sales_by_ID)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have 34 months of observations. Let's split the data into train (33 months) and test (last month) samples.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"markdown","source":"def split_train_test(np_data, col_date = 0, last_month = 33):\n    col_TS = 1 # order of item_cnt_month column\n    N_IDs = len(np_data)\n    train = list()\n    test = list()\n    for i in range(N_IDs):\n        ID_data = np.array(np_data[i])\n        train_rows = ID_data[ID_data[:,col_date] < last_month, :]\n        test_response = ID_data[ID_data[:,col_date] >= last_month, :]\n        #if len(train_rows) == 0:\n            #continue\n        if len(test_response) == 0:\n            test.append(np.array([np.array(range(last_month,34,1)), np.zeros(34-last_month)]).T)\n        else:\n            test.append(test_response)\n        train.append(train_rows)\n    return train, np.array(test)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#val_month = 28\n#train, test_actual = split_train_test(sales_by_ID, last_month = val_month)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#test_actual = np.nan_to_num(test_actual, nan = 0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Fill data for missing months","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's fill the missing date_block_num by NaN for paticular ID\ndef missing_months(np_data, col_date, col_TS, N_months = 34):\n    # col_date - index of date_block_num column\n    # col_TS - index of item_price column and item_cnt_month column\n    # at first fill time series by NaN for all months\n    series = [np.nan for _ in range(N_months)]\n    for i in range(len(np_data)):\n        position = int(np_data[i, col_date] - 1)\n        # fill positions that present in data\n        series[position] = np_data[i, col_TS]\n    return series","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's fill the missing item_cnt_month and item_price for particular ID\ndef to_fill_missing(np_data, N_months = 34):\n    col = ['date_block_num','item_cnt_month']\n    sales_ID = pd.DataFrame(np_data, columns = col)\n    if sales_ID.shape[0] < N_months:\n        date_month = pd.DataFrame(range(N_months),columns = ['date_block_num'])\n        sales_ID = pd.merge(date_month, sales_ID, on = ('date_block_num'), how = 'left')\n        sales_ID = sales_ID.reindex(columns = col)\n        sales_ID['item_cnt_month'] = sales_ID['item_cnt_month'].fillna(0.0)\n    return sales_ID['item_cnt_month'].to_numpy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot time series for particular ID to find out missing months\ndef plot_TS(np_data, n_vars = 1, N_months = 34, flag = 0):\n    # n_vars = 1 or 2 (plot item_cnt OR item_cnt and item_price)\n    plt.figure()\n    if flag == 1:\n        TSs = to_fill_missing(np_data, N_months)\n    for i in range(n_vars):\n        col_plot = i + 1 # index of column to plot\n        if flag == 1:\n            series = TSs#[:,col_plot]\n        else:\n            series = missing_months(np_data, 0, col_plot, N_months)\n        ax = plt.subplot(n_vars, 1, i+1)\n        plt.plot(series, 'o')\n        plt.plot(series)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's take a look on the plots of several IDs","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in np.random.randint(0, len(sales_by_ID), 5):\n    plot_TS(sales_by_ID[i], flag = 1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have totaly different numbers and positions of missing months for different IDs. Let's fill item_cnt_month by 0 and add column of missing flag.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's create 2D-array and each column is counts of particular ID where missing months is filled\ndef full_data(data, N_months = 34):\n    N_IDs = len(data)\n    TS = np.empty((N_months, N_IDs))\n    for i in range(N_IDs):\n        TS[:, i] = to_fill_missing(data[i], N_months)\n    return TS","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TS = full_data(sales_by_ID)\nTS.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"val_month = 29\nvalid_TS = TS[val_month:,:]\nprint(valid_TS.shape)\n\ntrain_TS = TS[:val_month,:]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's take a look on graphs of data with missing values and data with filled missing values","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Scaling","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"scaler = preprocessing.MinMaxScaler()\nscaler.fit(TS)\ntrain_scaled = scaler.transform(train_TS)\nvalid_scaled = scaler.transform(valid_TS)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature engineering","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"In order to execute supervised algorithms we have to modify the data this way:\nseveral lags like an input and next lags like an output.\n\nLet's assume n_lag = 3 than modified time series of item_cnt_month will be like the following:\n\n---------------------input--------------- ||         output \n\ndata_month1 data_month2 data_month3 ||      data_month2 data_month3  data_month4 \n\ndata_month2 data_month3 data_month4 ||      data_month3 data_month4  data_month5 \n\ndata_month3 data_month4 data_month5 ||      data_month4 data_month5  data_month6 \n\n.....","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def to_make_features(TS, n_lag, batch_size):\n    ds = tf.data.Dataset.from_tensor_slices(TS) # each element of dataset is one value of TS \n    ds = ds.window(n_lag+1, shift = 1, drop_remainder = True) # (n_lag+1)-elements of dataset is combined to window\n    ds = ds.flat_map(lambda row: row.batch(n_lag + 1)) # to batch elements in window to tensor (one element) and to flat (now there are no windows)\n  # Let's shuffle befor we combine batches for epoch\n    ds = ds.shuffle(300)\n  # make the tuple: first element is features, second element is labels\n  # features-(1,2,3) and labels-(2,3,4). 2 goes after 1, 3 goes after 2, 4 goes after 3.\n    ds = ds.map(lambda row: (row[:-1,:], row[1:,:]))\n  # combine tuples to banch for gradient descent\n  # instead of a row we will have a matrix in every tuple\n    ds = ds.batch(batch_size).prefetch(1)\n    return ds","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model evaluation","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import random\ntf.random.set_seed(53)\nrandom.seed(53)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"n_lag = 6\nbatch_size = 8\nfeatures = to_make_features(train_scaled, n_lag, batch_size)\nval_features = to_make_features(valid_scaled, n_lag, batch_size)\nConv_filters = 64\nConv_kernel_size = 4\nLSTM_filters = 64\nn_outputs = train_scaled.shape[1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = tf.keras.models.Sequential([\n  tf.keras.layers.Conv1D(filters = Conv_filters, kernel_size = Conv_kernel_size,\n                      strides=1, padding=\"causal\", activation=\"relu\", input_shape=[None, n_outputs]),\n  tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(LSTM_filters, return_sequences=True)),\n  tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(LSTM_filters, return_sequences=True)),\n  tf.keras.layers.Dense(n_outputs)\n])\nlr_schedule = tf.keras.callbacks.LearningRateScheduler(lambda epoch: 1e-5 * 10**(epoch / 20))\n\noptimizer = tf.keras.optimizers.Adam(learning_rate = 1e-5)\n\nmodel.compile(loss=tf.keras.losses.Huber(), optimizer=optimizer, metrics=[\"mae\"])\n\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fitting = model.fit(features, epochs=80, callbacks=[lr_schedule])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.semilogx(fitting.history[\"lr\"], fitting.history[\"loss\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"optimizer = tf.keras.optimizers.Adam(learning_rate = 1e-2)\nmodel.compile(loss=tf.keras.losses.Huber(), optimizer=optimizer, metrics=[\"mae\"])\nfitting = model.fit(features, epochs=300, verbose = 1, validation_data = val_features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mae = fitting.history['mae']\nloss = fitting.history['loss']\nepochs=range(len(loss))\nplt.plot(epochs, mae, 'r')\nplt.plot(epochs, fitting.history['val_mae'], 'r--')\nplt.plot(epochs, loss, 'b')\nplt.plot(epochs, fitting.history['val_loss'], 'b--')\nplt.title('MAE and Loss')\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Accuracy\")\nplt.legend([\"MAE\", \"val_MAE\", \"Loss\", \"val_Loss\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(epochs, fitting.history['val_mae'], 'r--')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def model_forecast(model, TS, n_lag, batch_size):\n    ds = tf.data.Dataset.from_tensor_slices(TS)\n    ds = ds.window(n_lag, shift=1, drop_remainder=True)\n    ds = ds.flat_map(lambda row: row.batch(n_lag))\n    ds = ds.batch(batch_size)\n    forecast = model.predict(ds)\n    return forecast","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"forecast = model_forecast(model, train_scaled, n_lag, batch_size)\nforecast = forecast[:,-1,:]\nforecast = scaler.inverse_transform(forecast)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"iplot = 0\nfor i in np.random.randint(0, n_outputs, 4):\n    iplot += 1\n    plt.subplot(4,1,iplot)\n    plt.plot(range(n_lag, val_month+1), np.append(train_TS[n_lag:,i],valid_TS[0,i]), 'r')\n    plt.plot(range(n_lag, val_month+1), forecast[:,i], 'b')\n    plt.legend([\"actual\", \"predicted\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Tuning of lags' number and batch size","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"lag_set = range(2,34-val_month,2)\nbatch_size_set = np.array([4,8,16])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mae_val = np.zeros((len(lag_set), len(batch_size_set)))\nmae_train = np.zeros((len(lag_set), len(batch_size_set)))\ni, j = 0, 0\nfor batch_size in batch_size_set:\n    for lag in lag_set:\n        features = to_make_features(train_scaled, lag, batch_size)\n        val_features = to_make_features(valid_scaled, lag, batch_size)\n        model = tf.keras.models.Sequential([\n              tf.keras.layers.Conv1D(filters = Conv_filters, kernel_size = Conv_kernel_size,\n                                  strides=1, padding=\"causal\", activation=\"relu\", input_shape=[None, n_outputs]),\n              tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(LSTM_filters, return_sequences=True)),\n              tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(LSTM_filters, return_sequences=True)),\n              tf.keras.layers.Dense(n_outputs)\n        ])\n        optimizer = tf.keras.optimizers.Adam(learning_rate = 1e-2)\n        model.compile(loss=tf.keras.losses.Huber(), optimizer=optimizer, metrics=[\"mae\"])\n        \n        fitting = model.fit(features, epochs = 100, verbose = 1, validation_data = val_features)\n        mae_val_i = np.array(fitting.history['val_mae'])\n        mae_val[i,j] = np.min(mae_val_i[np.nonzero(mae_val_i)])\n        min_position = fitting.history['val_mae'].index(mae_val[i,j])\n        mae_train[i,j] = fitting.history['mae'][min_position]\n        i += 1\n    j += 1\n    i = 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.subplot(2,1,1)\nfor j in range(len(batch_size_set)):\n    plt.plot(lag_set, mae_train[:,j])\nplt.legend([\"batch 1\", \"batch 2\", \"batch 3\"])\nplt.title(\"MAE\")\nplt.subplot(2,1,2)\nfor j in range(len(batch_size_set)):\n    plt.plot(lag_set, mae_val[:,j])\nplt.legend([\"batch 1\", \"batch 2\", \"batch 3\"])\nplt.title(\"val_MAE\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 4 lags and 16 batch size are the best\nn_lag = 4\nbatch_size = 16\nfeatures = to_make_features(train_scaled, n_lag, batch_size)\nval_features = to_make_features(valid_scaled, n_lag, batch_size)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Tuning of number of epochs","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"epoch_set = np.array([50, 100, 300, 500, 1000])\nmae_val_ep = np.zeros(len(epoch_set))\nmae_train_ep = np.zeros(len(epoch_set))\ni = 0\nfor epoch in epoch_set:\n    fitting = model.fit(features, epochs = epoch)\n    forecast_val = model_forecast(model, train_scaled[33-n_lag:,:], n_lag, batch_size)\n    forecast_val = forecast_val[:,-1,:]\n    forecast_val = scaler.inverse_transform(forecast_val)\n    mae_val_ep[i] = np.mean(np.abs(forecast_val - test_actual))\n    mae_train_ep[i] = fitting.history['mae'][-1]\n    i += 1\nplt.subplot(2,1,1)\nplt.plot(epoch_set, mae_train_ep)\nplt.subplot(2,1,2)\nplt.plot(epoch_set, mae_val_ep)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 500 epochs is the best\nn_epoch = 100","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Hyperparameters tuning","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"Conv_filters_set = [32, 64, 256]\nConv_kernel_size_set = [2, 4, 6]\nLSTM_filters_set = [32, 64, 256, 512]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mae_val = np.zeros((len(Conv_filters_set), len(Conv_kernel_size_set)))\nmae_train = np.zeros((len(Conv_filters_set), len(Conv_kernel_size_set)))\ni, j = 0, 0\nfor Conv_filters in Conv_filters_set:\n    for Conv_kernel_size in Conv_kernel_size_set:\n        model = tf.keras.models.Sequential([\n          tf.keras.layers.Conv1D(filters = Conv_filters, kernel_size = Conv_kernel_size,\n                      strides=1, padding=\"causal\", activation=\"relu\", input_shape=[None, n_outputs]),\n          tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(LSTM_filters, return_sequences=True)),\n          tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(LSTM_filters, return_sequences=True)),\n          tf.keras.layers.Dense(n_outputs)\n        ])\n        optimizer = tf.keras.optimizers.Adam(learning_rate = 1e-2)\n        model.compile(loss=tf.keras.losses.Huber(), optimizer=optimizer, metrics=[\"mae\"])\n        fitting = model.fit(features, epochs = n_epoch, verbose = 1, validation_data = val_features)\n\n        mae_val_i = np.array(fitting.history['val_mae'])\n        mae_val[i,j] = np.min(mae_val_i[np.nonzero(mae_val_i)])\n        min_position = fitting.history['val_mae'].index(mae_val[i,j])\n        mae_train[i,j] = fitting.history['mae'][min_position]\n        i += 1\n    j += 1\n    i = 0\nplt.subplot(2,1,1)\nfor j in range(len(Conv_filters_set)):\n    plt.plot(Conv_kernel_size_set, mae_train[:,j])\nplt.legend([\"Filter 1\", \"Filter 2\", \"Filter 3\"])\nplt.subplot(2,1,2)\nfor j in range(len(Conv_filters_set)):\n    plt.plot(Conv_kernel_size_set, mae_val[:,j])\nplt.legend([\"Filter 1\", \"Filter 2\", \"Filter 3\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mae_train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mae_val","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Conv_filters = 32\nConv_kernel_size = 4\n\nmae_val_ep = np.zeros(len(LSTM_filters_set))\nmae_train_ep = np.zeros(len(LSTM_filters_set))\ni = 0\nfor LSTM_filters in LSTM_filters_set:\n    model = tf.keras.models.Sequential([\n          tf.keras.layers.Conv1D(filters = Conv_filters, kernel_size = Conv_kernel_size,\n                      strides=1, padding=\"causal\", activation=\"relu\", input_shape=[None, n_outputs]),\n          tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(LSTM_filters, return_sequences=True)),\n          tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(LSTM_filters, return_sequences=True)),\n          tf.keras.layers.Dense(n_outputs)\n        ])\n    optimizer = tf.keras.optimizers.Adam(learning_rate = 1e-2)\n    model.compile(loss=tf.keras.losses.Huber(), optimizer=optimizer, metrics=[\"mae\"])\n    fitting = model.fit(features, epochs = n_epoch, verbose = 1, validation_data = val_features)\n    \n    mae_val_i = np.array(fitting.history['val_mae'])\n    mae_val_ep[i] = np.min(mae_val_i[np.nonzero(mae_val_i)])\n    min_position = fitting.history['val_mae'].index(mae_val_ep[i])\n    mae_train_ep[i] = fitting.history['mae'][min_position]\n    i += 1\nplt.subplot(2,1,1)\nplt.plot(LSTM_filters_set, mae_train_ep)\nplt.subplot(2,1,2)\nplt.plot(LSTM_filters_set, mae_val_ep)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"LSTM_filters = 32\n\nmodel = tf.keras.models.Sequential([\n          tf.keras.layers.Conv1D(filters = Conv_filters, kernel_size = Conv_kernel_size,\n                      strides=1, padding=\"causal\", activation=\"relu\", input_shape=[None, n_outputs]),\n          tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(LSTM_filters, return_sequences=True)),\n          tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(LSTM_filters, return_sequences=True)),\n          tf.keras.layers.Dense(n_outputs)\n        ])\noptimizer = tf.keras.optimizers.Adam(learning_rate = 1e-2)\nmodel.compile(loss=tf.keras.losses.Huber(), optimizer=optimizer, metrics=[\"mae\"])\nfitting = model.fit(features, epochs = 50, verbose = 1, validation_data = val_features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mae = fitting.history['mae']\nloss = fitting.history['loss']\nepochs=range(len(loss))\nplt.plot(epochs, mae, 'r')\nplt.plot(epochs, fitting.history['val_mae'], 'r--')\nplt.plot(epochs, loss, 'b')\nplt.plot(epochs, fitting.history['val_loss'], 'b--')\nplt.title('MAE and Loss')\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Accuracy\")\nplt.legend([\"MAE\", \"val_MAE\", \"Loss\", \"val_Loss\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"last_month_forecast = model_forecast(model, valid_scaled, n_lag, batch_size)\nlast_month_forecast = forecast[-1,-1,:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"last_month_forecast = scaler.inverse_transform(np.expand_dims(last_month_forecast, axis = 0))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.DataFrame({\n        'ID': list_IDs,\n        'item_cnt_month': np.squeeze(last_month_forecast)\n    })\nsubmission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.loc[submission.item_cnt_month < 0, 'item_cnt_month'] = 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.merge(sales_test.ID, submission, on = ('ID'), how = 'left')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = submission.fillna(0)\nsubmission.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}