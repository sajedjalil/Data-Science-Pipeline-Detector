{"cells":[{"metadata":{"_uuid":"63920a788e6a0b5bb7c681717885ab6bf20b8413"},"cell_type":"markdown","source":"<h1><center>Time-series forecasting with deep learning & LSTM autoencoders</center></h1>\n\n* The purpose of this work is to show one way time-series data can be effiently encoded to lower dimensions, to be used into non time-series models.\n* Here I'll encode a time-series of size 12 (12 months) to a single value and use it on a MLP deep learning model, instead of using the time-series on a LSTM model that could be the regular approach.\n* The first part of the data preparation is from my other kernel [Model stacking, feature engineering and EDA](https://www.kaggle.com/dimitreoliveira/model-stacking-feature-engineering-and-eda).\n* This work was inspired by this Machinelearningmastery post [A Gentle Introduction to LSTM Autoencoders](https://machinelearningmastery.com/lstm-autoencoders/), make sure to check out."},{"metadata":{"_uuid":"a40fdd9863be8fe7c5799345ece1e1174d9e6730"},"cell_type":"markdown","source":"<h2><center>Predict future sales</center></h2>\n\nWe are asking you to predict total sales for every product and store in the next month. By solving this competition you will be able to apply and enhance your data science skills.\n\nYou are provided with daily historical sales data. The task is to forecast the total amount of products sold in every shop for the test set. Note that the list of shops and products slightly changes every month. Creating a robust model that can handle such situations is part of the challenge.\n\n#### Data fields description:\n* ID - an Id that represents a (Shop, Item) tuple within the test set\n* shop_id - unique identifier of a shop\n* item_id - unique identifier of a product\n* item_category_id - unique identifier of item category\n* date_block_num - a consecutive month number, used for convenience. January 2013 is 0, February 2013 is 1,..., October 2015 is 33\n* date - date in format dd/mm/yyyy\n* item_cnt_day - number of products sold. You are predicting a monthly amount of this measure\n* item_price - current price of an item\n* item_name - name of item\n* shop_name - name of shop\n* item_category_name - name of item category\n\n### Dependencies"},{"metadata":{"trusted":true,"_uuid":"60785124984e36422c50287be4d6b1d7944af345","_kg_hide-input":false,"_kg_hide-output":true},"cell_type":"code","source":"import os, warnings, random\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split\nimport tensorflow as tf\nimport tensorflow.keras.layers as L\nfrom tensorflow.keras import optimizers, Sequential, Model\n\n# Set seeds to make the experiment more reproducible.\ndef seed_everything(seed=0):\n    random.seed(seed)\n    np.random.seed(seed)\n    tf.random.set_seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    os.environ['TF_DETERMINISTIC_OPS'] = '1'\n\nseed = 0\nseed_everything(seed)\nwarnings.filterwarnings('ignore')\npd.set_option('display.float_format', lambda x: '%.2f' % x)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0dc8d7422c27df8c88b6b8493c2967da432300a8"},"cell_type":"markdown","source":"### Loading data"},{"metadata":{"trusted":true,"_uuid":"1902600704b6188ec08cd65ae0df9b6541b02a6c"},"cell_type":"code","source":"test = pd.read_csv('../input/test.csv', dtype={'ID': 'int32', 'shop_id': 'int32', \n                                                  'item_id': 'int32'})\nitem_categories = pd.read_csv('../input/item_categories.csv', \n                              dtype={'item_category_name': 'str', 'item_category_id': 'int32'})\nitems = pd.read_csv('../input/items.csv', dtype={'item_name': 'str', 'item_id': 'int32', \n                                                 'item_category_id': 'int32'})\nshops = pd.read_csv('../input/shops.csv', dtype={'shop_name': 'str', 'shop_id': 'int32'})\nsales = pd.read_csv('../input/sales_train.csv', parse_dates=['date'], \n                    dtype={'date': 'str', 'date_block_num': 'int32', 'shop_id': 'int32', \n                      'item_id': 'int32', 'item_price': 'float32', 'item_cnt_day': 'int32'})","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c7670299c9ebef79ffb9daca617fc8773d2d68e7"},"cell_type":"markdown","source":"### Join data sets"},{"metadata":{"trusted":true,"_uuid":"203e9c8159e3f7b9369624eb070b15b255b64003"},"cell_type":"code","source":"train = sales.join(items, on='item_id', rsuffix='_').join(shops, on='shop_id', rsuffix='_').join(item_categories, on='item_category_id', rsuffix='_').drop(['item_id_', 'shop_id_', 'item_category_id_'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8e398989c1b07623db13744c9dc7118a8102996d"},"cell_type":"markdown","source":"### Let's take a look at the raw data"},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"eaa4c16a64104e983e0c6eeded7e6e60a3b843f0"},"cell_type":"code","source":"print(f'Train rows: {train.shape[0]}')\nprint(f'Train columns: {train.shape[1]}')\n\ndisplay(train.head().T)\ndisplay(train.describe())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a652e387ae6ead95eff1d00b7c81592f4f771a3d"},"cell_type":"markdown","source":"### Time period of the dataset"},{"metadata":{"trusted":true,"_uuid":"1fbb85b7b3fceb0d23a22717f0f4214afb46aba3","_kg_hide-input":true},"cell_type":"code","source":"print(f\"Min date from train set: {train['date'].min().date()}\")\nprint(f\"Max date from train set: {train['date'].max().date()}\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5d8d4d0e33aa3debe590f277a6290086d4e96561"},"cell_type":"markdown","source":"I'm leaving only the \"shop_id\" and \"item_id\" that exist in the test set to have more accurate results."},{"metadata":{"trusted":true,"_uuid":"7c147d419e724712336da759c111117f90b3f435"},"cell_type":"code","source":"test_shop_ids = test['shop_id'].unique()\ntest_item_ids = test['item_id'].unique()\n# Only shops that exist in test set.\ntrain = train[train['shop_id'].isin(test_shop_ids)]\n# Only items that exist in test set.\ntrain = train[train['item_id'].isin(test_item_ids)]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7157a7ea84c7d7e024ce8518d4b6c646e6ca2cd4"},"cell_type":"markdown","source":"# Data preprocessing\n* I'm dropping all features but \"item_cnt_day\" because I'll be using only it as a univariate time-series.\n* We are asked to predict total sales for every product and store in the next month, and our data is given by day, so let's aggregate the data by month.\n* Also I'm leaving only monthly \"item_cnt\" >= 0 and <= 20, as this seems to be the distributions of the test set."},{"metadata":{"trusted":true,"_uuid":"0cfa016f89affd460f056a8107cfc8f345773907"},"cell_type":"code","source":"train_monthly = train[['date', 'date_block_num', 'shop_id', 'item_id', 'item_cnt_day']]\ntrain_monthly = train_monthly.sort_values('date').groupby(['date_block_num', 'shop_id', 'item_id'], as_index=False)\ntrain_monthly = train_monthly.agg({'item_cnt_day':['sum']})\ntrain_monthly.columns = ['date_block_num', 'shop_id', 'item_id', 'item_cnt']\ntrain_monthly = train_monthly.query('item_cnt >= 0 and item_cnt <= 20')\n# Label\ntrain_monthly['item_cnt_month'] = train_monthly.sort_values('date_block_num').groupby(['shop_id', 'item_id'])['item_cnt'].shift(-1)\n\ndisplay(train_monthly.head(10).T)\ndisplay(train_monthly.describe().T)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bf8d9e20db300f5b1253b4453320d4b61c27fcba"},"cell_type":"markdown","source":"# Time-series processing\n* As I only need the \"item_cnt\" feature as a series, I can get that easily by just using a pivot operation.\n* This way I'll also get the missing months from each \"shop_id\" and \"item_id\", and then replace them with 0 (otherwise would be \"nan\"). "},{"metadata":{"trusted":true,"_uuid":"9eab7851850f95623ccccdbf000ece06d1bb8ff4"},"cell_type":"code","source":"monthly_series = train_monthly.pivot_table(index=['shop_id', 'item_id'], columns='date_block_num',values='item_cnt', fill_value=0).reset_index()\nmonthly_series.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9cbe681a798f5c2d2183233e4c77db1721c41511"},"cell_type":"markdown","source":"#### Currently I have one series (33 months) for each unique pair of \"shop_id\" and \"item_id\", but probably would be better to have multiple smaller series for each unique pair, so I'm generating multiple series of size 12 (one year) for each unique pair."},{"metadata":{"trusted":true,"_uuid":"dc9089997b7bb9342d5a26583c8722c81c951e4e"},"cell_type":"code","source":"first_month = 20\nlast_month = 33\nserie_size = 12\ndata_series = []\n\nfor index, row in monthly_series.iterrows():\n    for month1 in range((last_month - (first_month + serie_size)) + 1):\n        serie = [row['shop_id'], row['item_id']]\n        for month2 in range(serie_size + 1):\n            serie.append(row[month1 + first_month + month2])\n        data_series.append(serie)\n\ncolumns = ['shop_id', 'item_id']\n[columns.append(i) for i in range(serie_size)]\ncolumns.append('label')\n\ndata_series = pd.DataFrame(data_series, columns=columns)\ndata_series.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0edbab0817b24c8cfe7c4739cc43bd8668b057be"},"cell_type":"markdown","source":"#### Dropping identifier columns as we don't need them anymore."},{"metadata":{"trusted":true,"_uuid":"5e837661c8880b5bf4205dc263d32fb3b9a75eee"},"cell_type":"code","source":"data_series = data_series.drop(['item_id', 'shop_id'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a4f0279dc5228c07a85a70631bf0a52044efdbd3"},"cell_type":"markdown","source":"### Train and validation sets."},{"metadata":{"trusted":true,"_uuid":"9fa3580649586a0386ab21c9a63110f66128f4e2"},"cell_type":"code","source":"labels = data_series['label']\ndata_series.drop('label', axis=1, inplace=True)\ntrain, valid, Y_train, Y_valid = train_test_split(data_series, labels.values, test_size=0.10, random_state=0)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"ac03abac4fb361e40d589f56aa360d2657b190e0"},"cell_type":"code","source":"print(\"Train set\", train.shape)\nprint(\"Validation set\", valid.shape)\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f6580ca6d2314d7f0b73b1d0da74ded3e22b1bb"},"cell_type":"markdown","source":"### Reshape data.\n* Time-series shape **(data points, time-steps, features)**."},{"metadata":{"trusted":true,"_uuid":"2c5e37e8cc7c1566aed88a0b23f894e500da8b1d"},"cell_type":"code","source":"X_train = train.values.reshape((train.shape[0], train.shape[1], 1))\nX_valid = valid.values.reshape((valid.shape[0], valid.shape[1], 1))\n\nprint(\"Train set reshaped\", X_train.shape)\nprint(\"Validation set reshaped\", X_valid.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8e7efc501e46c65c03b5b3ec510dd2e1b0279acb"},"cell_type":"markdown","source":"#### First let's begin with how a regular RNN time-series approach could be.\n\n# Regular LSTM model."},{"metadata":{"trusted":true,"_uuid":"cc0476a8e15f23ccab658e89a521f6328a3a1e7b"},"cell_type":"code","source":"serie_size =  X_train.shape[1] # 12\nn_features =  X_train.shape[2] # 1\n\nepochs = 20\nbatch = 128\nlr = 0.0001\n\nlstm_model = Sequential()\nlstm_model.add(L.LSTM(10, input_shape=(serie_size, n_features), return_sequences=True))\nlstm_model.add(L.LSTM(6, activation='relu', return_sequences=True))\nlstm_model.add(L.LSTM(1, activation='relu'))\nlstm_model.add(L.Dense(10, kernel_initializer='glorot_normal', activation='relu'))\nlstm_model.add(L.Dense(10, kernel_initializer='glorot_normal', activation='relu'))\nlstm_model.add(L.Dense(1))\nlstm_model.summary()\n\nadam = optimizers.Adam(lr)\nlstm_model.compile(loss='mse', optimizer=adam)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bd6c873dd2ce1088505a803db608d1ba943997bd"},"cell_type":"code","source":"lstm_history = lstm_model.fit(X_train, Y_train, \n                              validation_data=(X_valid, Y_valid), \n                              batch_size=batch, \n                              epochs=epochs, \n                              verbose=2)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"51d7dff8a1c9c35a56274ed622aaeec94b54167a"},"cell_type":"markdown","source":"# Autoencoder\n* Now we will build an autoencoder to learn how to reconstruct the input, this way it internally learns the best way to represent the input in lower dimensions.\n* The reconstruct model is composed of an encoder and a decoder, the encoder is responsible for learning how to represent the input into lower dimensions and the decoder learns how to rebuild the smaller representations into the input again.\n* Here is a structural representations of an autoencoder:\n <img src=\"https://raw.githubusercontent.com/dimitreOliveira/MachineLearning/master/Kaggle/Predict%20Future%20Sales/Autoencoder_structure.png\" width=\"400\">\n* After the models is trained we can keep only the encoder part and we'll have a model that is able to do what we want.\n\n### LSTM Autoencoder."},{"metadata":{"trusted":true,"_uuid":"5ff2be02b26b29defb8f72a04d6c8da64ee66b7c","scrolled":false},"cell_type":"code","source":"encoder_decoder = Sequential()\nencoder_decoder.add(L.LSTM(serie_size, activation='relu', input_shape=(serie_size, n_features), return_sequences=True))\nencoder_decoder.add(L.LSTM(6, activation='relu', return_sequences=True))\nencoder_decoder.add(L.LSTM(1, activation='relu'))\nencoder_decoder.add(L.RepeatVector(serie_size))\nencoder_decoder.add(L.LSTM(serie_size, activation='relu', return_sequences=True))\nencoder_decoder.add(L.LSTM(6, activation='relu', return_sequences=True))\nencoder_decoder.add(L.TimeDistributed(L.Dense(1)))\nencoder_decoder.summary()\n\nadam = optimizers.Adam(lr)\nencoder_decoder.compile(loss='mse', optimizer=adam)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"63adaa1c1493776404358a915e53683123587df8"},"cell_type":"code","source":"encoder_decoder_history = encoder_decoder.fit(X_train, X_train, \n                                              batch_size=batch, \n                                              epochs=epochs, \n                                              verbose=2)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b9a93135fa65c99f60d6b475860d595946fa36ce"},"cell_type":"markdown","source":"#### You should be aware that the better the autoencoder is able to reconstruct the input the better it internally encodes the input, in other words if we have a good autoencoder we probably will have an equally good encoder.\n#### Let's take a look at the layers of the encoder_decoder model:"},{"metadata":{"trusted":true,"_uuid":"15dacbee296511b692b9ffacce8b0e4908a043e3"},"cell_type":"code","source":"rpt_vector_layer = Model(inputs=encoder_decoder.inputs, outputs=encoder_decoder.layers[3].output)\ntime_dist_layer = Model(inputs=encoder_decoder.inputs, outputs=encoder_decoder.layers[5].output)\nencoder_decoder.layers","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9f890ab6dc39db44614f56c63ae66e9ed8aa5683"},"cell_type":"markdown","source":"#### About the autoencoder layers\n#### LSTM\n* This is just a regular LSTM layer, a layer that is able to receive sequence data and learn based on it nothing much to talk about."},{"metadata":{"_uuid":"8da47713e9298d43d2533f4592acccc00ddca1e7"},"cell_type":"markdown","source":"#### RepeatVector layer\n* Here is something we don't usually see, this layers basically repeats it's input \"n\" times, the reason to use it is because the last layers from the encoder part (the layer with one neuron) don't return sequences, so it does not outputs a sequenced data, this way we can't just add another LSTM layer after it, we need a way to turn this output into a sequence of the same time-steps of the model input, this is where \"RepeatVector\" layers comes in.\n* Let's see what it outputs."},{"metadata":{"trusted":true,"_uuid":"48a1f24b0f8b6c64ef7e2c77c72584c0b85a1965","_kg_hide-input":true},"cell_type":"code","source":"rpt_vector_layer_output = rpt_vector_layer.predict(X_train[:1])\nprint('Repeat vector output shape', rpt_vector_layer_output.shape)\nprint('Repeat vector output sample')\nprint(rpt_vector_layer_output[0])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9c504b854e6c9c1b8254ee872c18bffb38c69090"},"cell_type":"markdown","source":"As you can see this is just the same value repeated some times to match the same shape of the model input.\n\n#### TimeDistributed layer\n* This layer is more common, sometimes is used when you want to mix RNN layers with other kind of layers.\n* We could output the model with another LSTM layer with one neuron and \"return_sequences=True\" parameter, but using a \"TimeDistributed\" layer wrapping a \"Dense\" layer we will have the same weights for each outputted time-step."},{"metadata":{"trusted":true,"_uuid":"2fdce2f3b27cb4ca1aef856cfe1987c881ff49d0","_kg_hide-input":true},"cell_type":"code","source":"time_dist_layer_output = time_dist_layer.predict(X_train[:1])\nprint('Time distributed output shape', time_dist_layer_output.shape)\nprint('Time distributed output sample')\nprint(time_dist_layer_output[0])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"19df05eca8d0a3e753468df84c759c1170700650"},"cell_type":"markdown","source":"[Another good explanation about the used layers](https://machinelearningmastery.com/encoder-decoder-long-short-term-memory-networks/)"},{"metadata":{"_uuid":"c9d0dd003dbaac41b7a2dab681b5112571697133"},"cell_type":"markdown","source":"#### Defining the encoding model.\n* What I want is to encode the whole series into a single value, so I need the output from the layer with a single neuron (in this case it's the third LSTM layer).\n* I'll take only the encoding part of the model and define it as a new one."},{"metadata":{"trusted":true,"_uuid":"f3d6e03900c87c32819e2689e60b9b56b6d7e8b3"},"cell_type":"code","source":"encoder = Model(inputs=encoder_decoder.inputs, outputs=encoder_decoder.layers[2].output)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ad537909142cdf273ea10de76ba438ee66fc3690"},"cell_type":"markdown","source":"#### Now let's encode the train and validation time-series."},{"metadata":{"trusted":true,"_uuid":"3a545acede584748d12a32a15fd6f9d3dfd27529"},"cell_type":"code","source":"train_encoded = encoder.predict(X_train)\nvalidation_encoded = encoder.predict(X_valid)\nprint('Encoded time-series shape', train_encoded.shape)\nprint('Encoded time-series sample', train_encoded[0])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ddce912149f088891b298392c38d9fb37e59dce4"},"cell_type":"markdown","source":"#### Add new encoded features to the train and validation sets."},{"metadata":{"trusted":true,"_uuid":"9893ee5967a58ed7babf355dc26a7f1791155ca1"},"cell_type":"code","source":"train['encoded'] = train_encoded\ntrain['label'] = Y_train\n\nvalid['encoded'] = validation_encoded\nvalid['label'] = Y_valid\n\ntrain.head(10)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"41752802a4212ff1e105f703a57d10193ed2933e"},"cell_type":"markdown","source":"#### Now we can use the new encoded feature that is a representation of the whole time-series and train a \"less complex\" model that does not receives sequenced data as input.\n\n# MLP with LSTM encoded feature\n* For the MLP model I'm only using the current month \"item_count\" and the encoded time-series feature from our LSTM encoder model, the idea is that we won't need the whole series because we already have a column that represents the whole series into a single value (it's like a dimensionality reduction)."},{"metadata":{"trusted":true,"_uuid":"77b3c31ca5dacbc4774fb3845c348c583a015d9e"},"cell_type":"code","source":"last_month = serie_size - 1\nY_train_encoded = train['label']\ntrain.drop('label', axis=1, inplace=True)\nX_train_encoded = train[[last_month, 'encoded']]\n\nY_valid_encoded = valid['label']\nvalid.drop('label', axis=1, inplace=True)\nX_valid_encoded = valid[[last_month, 'encoded']]\n\nprint(\"Train set\", X_train_encoded.shape)\nprint(\"Validation set\", X_valid_encoded.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e39b4c487ed7fb6fc158d1010f137f7188ff1cb9","_kg_hide-input":true},"cell_type":"code","source":"X_train_encoded.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"73f2010fd8354fc40a52cda0d44c1b545c117567"},"cell_type":"code","source":"mlp_model = Sequential()\nmlp_model.add(L.Dense(10, kernel_initializer='glorot_normal', activation='relu', input_dim=X_train_encoded.shape[1]))\nmlp_model.add(L.Dense(10, kernel_initializer='glorot_normal', activation='relu'))\nmlp_model.add(L.Dense(1))\nmlp_model.summary()\n\nadam = optimizers.Adam(lr)\nmlp_model.compile(loss='mse', optimizer=adam)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"eaa02d75d5021ebefcd878845dcfd2d2efc8aa40","scrolled":false},"cell_type":"code","source":"mlp_history = mlp_model.fit(X_train_encoded.values, Y_train_encoded.values, epochs=epochs, batch_size=batch, validation_data=(X_valid_encoded, Y_valid_encoded), verbose=2)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d882dae62437156086c66c49f81f4bc4a9f413c8"},"cell_type":"markdown","source":"# Comparing models\n* As you can see I tried to build both models with a similar topology (type/number of layers and neurons), so it could make more sense to compare them.\n* The results are pretty close, also they may change a bit depending on the random initialization of the networks weights, so I would say they are very similar in terms of performance.\n\n#### Model training"},{"metadata":{"trusted":true,"_uuid":"0930982fe7cb845c62bcf0b82acda25c9c7fe121","_kg_hide-input":true},"cell_type":"code","source":"fig, (ax1, ax2) = plt.subplots(1, 2, sharey=True,figsize=(22,7))\n\nax1.plot(lstm_history.history['loss'], label='Train loss')\nax1.plot(lstm_history.history['val_loss'], label='Validation loss')\nax1.legend(loc='best')\nax1.set_title('Regular LSTM')\nax1.set_xlabel('Epochs')\nax1.set_ylabel('MSE')\n\nax2.plot(mlp_history.history['loss'], label='Train loss')\nax2.plot(mlp_history.history['val_loss'], label='Validation loss')\nax2.legend(loc='best')\nax2.set_title('MLP with LSTM encoder')\nax2.set_xlabel('Epochs')\nax2.set_ylabel('MSE')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b7542663bca3bc41620e33ebcbe7490c1b7a3604"},"cell_type":"markdown","source":"#### Regular LSTM on train and validation."},{"metadata":{"trusted":true,"_uuid":"844bcb19053a3928125b7afcb7b6e59e65dbf05d","_kg_hide-input":true},"cell_type":"code","source":"lstm_train_pred = lstm_model.predict(X_train)\nlstm_val_pred = lstm_model.predict(X_valid)\nprint('Train rmse:', np.sqrt(mean_squared_error(Y_train, lstm_train_pred)))\nprint('Validation rmse:', np.sqrt(mean_squared_error(Y_valid, lstm_val_pred)))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c06d5f66376de8d2ec863e2a47248387e71e7660"},"cell_type":"markdown","source":"#### MLP with LSTM encoder on train and validation."},{"metadata":{"trusted":true,"_uuid":"b3c19ee0cf3d1d01a19720f260889292bea8a3cf","_kg_hide-input":true},"cell_type":"code","source":"mlp_train_pred2 = mlp_model.predict(X_train_encoded.values)\nmlp_val_pred2 = mlp_model.predict(X_valid_encoded.values)\nprint('Train rmse:', np.sqrt(mean_squared_error(Y_train_encoded, mlp_train_pred2)))\nprint('Validation rmse:', np.sqrt(mean_squared_error(Y_valid_encoded, mlp_val_pred2)))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c78a905f90bad6b2bdeb726e36b10f524ec9d3d3"},"cell_type":"markdown","source":"### Build test set\n#### Since we have two models I'll build test sets to apply on both of them."},{"metadata":{"trusted":true,"_uuid":"ba50f3b1f274bc118d5c1d4a3d8feef70fcc38ec"},"cell_type":"code","source":"latest_records = monthly_series.drop_duplicates(subset=['shop_id', 'item_id'])\nX_test = pd.merge(test, latest_records, on=['shop_id', 'item_id'], how='left', suffixes=['', '_'])\nX_test.fillna(0, inplace=True)\nX_test.drop(['ID', 'item_id', 'shop_id'], axis=1, inplace=True)\nX_test.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"88dac07663f364c3bde17f5fd27d3e313f5af28c"},"cell_type":"markdown","source":"### Regular LSTM model test predictions\n* For the regular LSTM model we just need the last 12 months, because that's our series input size."},{"metadata":{"trusted":true,"_uuid":"c1619071697c4ac3be9c795522792657681fc809","_kg_hide-input":true},"cell_type":"code","source":"X_test = X_test[[(i + (34 - serie_size)) for i in range(serie_size)]]\nX_test.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1b2ab6556c84ccfaaef9a05621d732bdad197989"},"cell_type":"markdown","source":"### Reshape data.\n* Time-series shape **(data points, time-steps, features)**."},{"metadata":{"trusted":true,"_uuid":"59de0ac820298b0e319e57ec22df5ec46641278b","_kg_hide-input":true},"cell_type":"code","source":"X_test_reshaped = X_test.values.reshape((X_test.shape[0], X_test.shape[1], 1))\nprint(X_test_reshaped.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f42e9b8c07aa26e1454f35f7f16e8e7e5354e730"},"cell_type":"markdown","source":"#### Making predictions."},{"metadata":{"trusted":true,"_uuid":"a5795c5aca18cc9072523d0df3d3a5611896b24d"},"cell_type":"code","source":"lstm_test_pred = lstm_model.predict(X_test_reshaped)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"653b086d81c24a063b20be97736b71a0fc9506d7"},"cell_type":"markdown","source":" ### MLP with LSTM encoded feature test predictions\n* For the MLP model with the encoded features I'm only using the current month \"item_count\" and the encoded time-series feature from our LSTM encoder model.\n\n#### Encoding the time-series"},{"metadata":{"trusted":true,"_uuid":"4c32f05942dce7e896bf86048c05f603fb328c60"},"cell_type":"code","source":"test_encoded = encoder.predict(X_test_reshaped)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"78f5d2986f66d91e91ca9848c1cefd9b13a4ba3e"},"cell_type":"markdown","source":"#### Add encoded features to the test set."},{"metadata":{"trusted":true,"_uuid":"88a028777a14411cc9e3de789d28621a30408f0e"},"cell_type":"code","source":"X_test['encoded'] = test_encoded\nX_test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"43f908c12553d5492978e93f59c95b73268fa798"},"cell_type":"code","source":"X_test_encoded = X_test[[33, 'encoded']]\nprint(\"Train set\", X_test_encoded.shape)\nX_test_encoded.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c595f7a5f954a9e15c9d3189e9b809d32ca14abf"},"cell_type":"markdown","source":"#### Making predictions."},{"metadata":{"trusted":true,"_uuid":"8596bb8b6f8905fdda825451d221e9b58e8d21e1"},"cell_type":"code","source":"mlp_test_pred = mlp_model.predict(X_test_encoded)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0e6bbd8adf786e86b3565470373851300e3d4d78"},"cell_type":"markdown","source":"#### Predictions from the regular LSTM model."},{"metadata":{"trusted":true,"_uuid":"3bd83a7c4152f672e3ee4865da6fd61748b5f6b8","_kg_hide-input":true},"cell_type":"code","source":"lstm_prediction = pd.DataFrame(test['ID'], columns=['ID'])\nlstm_prediction['item_cnt_month'] = lstm_test_pred.clip(0., 20.)\nlstm_prediction.to_csv('lstm_submission.csv', index=False)\nlstm_prediction.head(10)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f95a651994bacb87e78359ecd7583554342b6660"},"cell_type":"markdown","source":"#### Predictions from the MLP model with LSTM encodded feature ."},{"metadata":{"trusted":true,"_uuid":"b28761903b6ce8fc59131dd2d57a29bf76ff1add","_kg_hide-input":true},"cell_type":"code","source":"mlp_prediction = pd.DataFrame(test['ID'], columns=['ID'])\nmlp_prediction['item_cnt_month'] = mlp_test_pred.clip(0., 20.)\nmlp_prediction.to_csv('mlp_submission.csv', index=False)\nmlp_prediction.head(10)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c8f980a4d9abb038f83d317ea6c80ebdf127b002"},"cell_type":"markdown","source":"Just a disclaimer, you absolutely can get better results on any of the used models,  I did not spent too much time tuning the models hyper parameters, as this is just for demonstration purpose, so if you want to give the code a try, you should surely tune a little more, if you get better results or any good insight about the models or architecture please let me know.\n\nIf you want to check out some interesting different approaches on time series problems take a look at this kernel [Deep Learning for Time Series Forecasting](https://www.kaggle.com/dimitreoliveira/deep-learning-for-time-series-forecasting)."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}