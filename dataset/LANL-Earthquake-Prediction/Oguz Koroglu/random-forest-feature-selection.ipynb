{"cells":[{"metadata":{},"cell_type":"markdown","source":"<h1>Setup</h1>"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.feature_selection import RFE\nfrom sklearn.feature_selection import RFECV\nimport seaborn as sns # data visualization library  \nimport matplotlib.pyplot as plt\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":1,"outputs":[{"output_type":"stream","text":"['LANL-Earthquake-Prediction', 'even-more-features']\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"This Kernel will use this kernel: https://www.kaggle.com/artgor/even-more-features/ features as input features."},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nprint(os.listdir(\"../input/even-more-features\"))\ntrain_features = pd.read_csv('../input/even-more-features/train_features.csv')\ntest_features = pd.read_csv('../input/even-more-features/test_features.csv')\ny = pd.read_csv('../input/even-more-features/y.csv')","execution_count":2,"outputs":[{"output_type":"stream","text":"['__notebook__.ipynb', 'submission.csv', '__output__.json', '__results___files', 'custom.css', 'y.csv', 'test_features.csv', '__results__.html', 'train_features.csv']\nCPU times: user 2.28 s, sys: 168 ms, total: 2.45 s\nWall time: 2.57 s\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"<h1>Preprocessing Train and Test Data</h1>"},{"metadata":{},"cell_type":"markdown","source":"Scaling data with standart scaler"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nscaler = StandardScaler()\nscaler.fit(train_features)\nX_train_scaled = pd.DataFrame(scaler.transform(train_features), columns=train_features.columns)\nX_test_scaled = pd.DataFrame(scaler.transform(test_features), columns=test_features.columns)","execution_count":3,"outputs":[{"output_type":"stream","text":"/opt/conda/lib/python3.6/site-packages/sklearn/preprocessing/data.py:645: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n  return self.partial_fit(X, y)\n/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:3: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n  This is separate from the ipykernel package so we can avoid doing imports until\n","name":"stderr"},{"output_type":"stream","text":"CPU times: user 160 ms, sys: 176 ms, total: 336 ms\nWall time: 336 ms\n","name":"stdout"},{"output_type":"stream","text":"/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:4: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n  after removing the cwd from sys.path.\n","name":"stderr"}]},{"metadata":{},"cell_type":"markdown","source":"split data train 80 % and test 20"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nx_train, x_valid, y_train, y_valid = train_test_split(X_train_scaled, y, test_size=0.2, random_state=78)","execution_count":4,"outputs":[{"output_type":"stream","text":"CPU times: user 24 ms, sys: 24 ms, total: 48 ms\nWall time: 49.3 ms\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"<h1>Dropping highly correlated features</h1>"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\ncorr_matrix = x_train.corr()\n#f,ax = plt.subplots(figsize=(18, 18))\n#sns.heatmap(corr_matrix, annot=True, linewidths=.5, fmt= '.1f',ax=ax)","execution_count":5,"outputs":[{"output_type":"stream","text":"CPU times: user 11.6 s, sys: 28 ms, total: 11.7 s\nWall time: 11.7 s\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"Select upper triangle of correlation matrix"},{"metadata":{"trusted":true},"cell_type":"code","source":"corr_matrix = corr_matrix.abs()\nupper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\nupper","execution_count":6,"outputs":[{"output_type":"execute_result","execution_count":6,"data":{"text/plain":"                         Hann_window_mean_150    ...        trend\nHann_window_mean_150                      NaN    ...     0.012516\nHann_window_mean_1500                     NaN    ...     0.012531\nHann_window_mean_15000                    NaN    ...     0.012656\nHann_window_mean_50                       NaN    ...     0.012514\nHilbert_mean                              NaN    ...     0.010292\nabs_max                                   NaN    ...     0.005597\nabs_max_roll_mean_10                      NaN    ...     0.001672\nabs_max_roll_mean_100                     NaN    ...     0.008122\nabs_max_roll_mean_1000                    NaN    ...     0.008637\nabs_max_roll_mean_10000                   NaN    ...     0.017413\nabs_max_roll_mean_50                      NaN    ...     0.004854\nabs_max_roll_mean_500                     NaN    ...     0.003250\nabs_max_roll_std_10                       NaN    ...     0.005752\nabs_max_roll_std_100                      NaN    ...     0.001983\nabs_max_roll_std_1000                     NaN    ...     0.003252\nabs_max_roll_std_10000                    NaN    ...     0.005465\nabs_max_roll_std_50                       NaN    ...     0.003012\nabs_max_roll_std_500                      NaN    ...     0.002618\nabs_mean                                  NaN    ...     0.010550\nabs_percentile_1                          NaN    ...          NaN\nabs_percentile_10                         NaN    ...     0.004221\nabs_percentile_20                         NaN    ...     0.004742\nabs_percentile_25                         NaN    ...     0.002078\nabs_percentile_30                         NaN    ...     0.000741\nabs_percentile_40                         NaN    ...     0.019656\nabs_percentile_5                          NaN    ...     0.008751\nabs_percentile_50                         NaN    ...     0.002666\nabs_percentile_60                         NaN    ...     0.013198\nabs_percentile_70                         NaN    ...     0.007619\nabs_percentile_75                         NaN    ...     0.019058\n...                                       ...    ...          ...\nspkt_welch_density_10                     NaN    ...     0.000227\nspkt_welch_density_100                    NaN    ...     0.009503\nspkt_welch_density_5                      NaN    ...     0.007005\nspkt_welch_density_50                     NaN    ...     0.017061\nstd                                       NaN    ...     0.006060\nstd_first_1000                            NaN    ...     0.013414\nstd_first_10000                           NaN    ...     0.012084\nstd_first_50000                           NaN    ...     0.015287\nstd_last_1000                             NaN    ...     0.007865\nstd_last_10000                            NaN    ...     0.004560\nstd_last_50000                            NaN    ...     0.006726\nstd_roll_mean_10                          NaN    ...     0.005566\nstd_roll_mean_100                         NaN    ...     0.006463\nstd_roll_mean_1000                        NaN    ...     0.013276\nstd_roll_mean_10000                       NaN    ...     0.027480\nstd_roll_mean_50                          NaN    ...     0.006651\nstd_roll_mean_500                         NaN    ...     0.008727\nstd_roll_std_10                           NaN    ...     0.006432\nstd_roll_std_100                          NaN    ...     0.005888\nstd_roll_std_1000                         NaN    ...     0.005835\nstd_roll_std_10000                        NaN    ...     0.005301\nstd_roll_std_50                           NaN    ...     0.005876\nstd_roll_std_500                          NaN    ...     0.005825\nsum                                       NaN    ...     0.012514\ntime_rev_asym_stat_1                      NaN    ...     0.012190\ntime_rev_asym_stat_10                     NaN    ...     0.014756\ntime_rev_asym_stat_100                    NaN    ...     0.000204\ntime_rev_asym_stat_5                      NaN    ...     0.019375\ntime_rev_asym_stat_50                     NaN    ...     0.003295\ntrend                                     NaN    ...          NaN\n\n[1419 rows x 1419 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Hann_window_mean_150</th>\n      <th>Hann_window_mean_1500</th>\n      <th>Hann_window_mean_15000</th>\n      <th>Hann_window_mean_50</th>\n      <th>Hilbert_mean</th>\n      <th>abs_max</th>\n      <th>abs_max_roll_mean_10</th>\n      <th>abs_max_roll_mean_100</th>\n      <th>abs_max_roll_mean_1000</th>\n      <th>abs_max_roll_mean_10000</th>\n      <th>abs_max_roll_mean_50</th>\n      <th>abs_max_roll_mean_500</th>\n      <th>abs_max_roll_std_10</th>\n      <th>abs_max_roll_std_100</th>\n      <th>abs_max_roll_std_1000</th>\n      <th>abs_max_roll_std_10000</th>\n      <th>abs_max_roll_std_50</th>\n      <th>abs_max_roll_std_500</th>\n      <th>abs_mean</th>\n      <th>abs_percentile_1</th>\n      <th>abs_percentile_10</th>\n      <th>abs_percentile_20</th>\n      <th>abs_percentile_25</th>\n      <th>abs_percentile_30</th>\n      <th>abs_percentile_40</th>\n      <th>abs_percentile_5</th>\n      <th>abs_percentile_50</th>\n      <th>abs_percentile_60</th>\n      <th>abs_percentile_70</th>\n      <th>abs_percentile_75</th>\n      <th>abs_percentile_80</th>\n      <th>abs_percentile_90</th>\n      <th>abs_percentile_95</th>\n      <th>abs_percentile_99</th>\n      <th>abs_std</th>\n      <th>abs_trend</th>\n      <th>autocorrelation_10</th>\n      <th>autocorrelation_100</th>\n      <th>autocorrelation_1000</th>\n      <th>autocorrelation_10000</th>\n      <th>...</th>\n      <th>range_-3000_-2000</th>\n      <th>range_-4000_-3000</th>\n      <th>range_0_1000</th>\n      <th>range_1000_2000</th>\n      <th>range_2000_3000</th>\n      <th>range_3000_4000</th>\n      <th>range_minf_m4000</th>\n      <th>range_p4000_pinf</th>\n      <th>skew</th>\n      <th>spkt_welch_density_1</th>\n      <th>spkt_welch_density_10</th>\n      <th>spkt_welch_density_100</th>\n      <th>spkt_welch_density_5</th>\n      <th>spkt_welch_density_50</th>\n      <th>std</th>\n      <th>std_first_1000</th>\n      <th>std_first_10000</th>\n      <th>std_first_50000</th>\n      <th>std_last_1000</th>\n      <th>std_last_10000</th>\n      <th>std_last_50000</th>\n      <th>std_roll_mean_10</th>\n      <th>std_roll_mean_100</th>\n      <th>std_roll_mean_1000</th>\n      <th>std_roll_mean_10000</th>\n      <th>std_roll_mean_50</th>\n      <th>std_roll_mean_500</th>\n      <th>std_roll_std_10</th>\n      <th>std_roll_std_100</th>\n      <th>std_roll_std_1000</th>\n      <th>std_roll_std_10000</th>\n      <th>std_roll_std_50</th>\n      <th>std_roll_std_500</th>\n      <th>sum</th>\n      <th>time_rev_asym_stat_1</th>\n      <th>time_rev_asym_stat_10</th>\n      <th>time_rev_asym_stat_100</th>\n      <th>time_rev_asym_stat_5</th>\n      <th>time_rev_asym_stat_50</th>\n      <th>trend</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Hann_window_mean_150</th>\n      <td>NaN</td>\n      <td>0.999999</td>\n      <td>0.999963</td>\n      <td>1.000000</td>\n      <td>0.083330</td>\n      <td>0.018539</td>\n      <td>0.020842</td>\n      <td>0.024626</td>\n      <td>0.099450</td>\n      <td>0.720851</td>\n      <td>0.014454</td>\n      <td>0.060341</td>\n      <td>0.013315</td>\n      <td>0.014517</td>\n      <td>0.016377</td>\n      <td>0.017596</td>\n      <td>0.016204</td>\n      <td>0.014007</td>\n      <td>0.147784</td>\n      <td>NaN</td>\n      <td>0.248716</td>\n      <td>0.461795</td>\n      <td>0.705889</td>\n      <td>0.408417</td>\n      <td>0.462260</td>\n      <td>0.510017</td>\n      <td>0.609534</td>\n      <td>0.562829</td>\n      <td>0.390635</td>\n      <td>0.369216</td>\n      <td>0.255472</td>\n      <td>0.129799</td>\n      <td>0.065246</td>\n      <td>0.023095</td>\n      <td>0.019356</td>\n      <td>0.000934</td>\n      <td>0.020053</td>\n      <td>0.075639</td>\n      <td>0.000880</td>\n      <td>0.009395</td>\n      <td>...</td>\n      <td>0.013062</td>\n      <td>0.003078</td>\n      <td>0.313329</td>\n      <td>0.019515</td>\n      <td>0.014761</td>\n      <td>0.000501</td>\n      <td>0.010633</td>\n      <td>0.027147</td>\n      <td>0.002473</td>\n      <td>0.010381</td>\n      <td>0.013742</td>\n      <td>0.040113</td>\n      <td>0.024114</td>\n      <td>0.039858</td>\n      <td>0.016930</td>\n      <td>0.012595</td>\n      <td>0.006930</td>\n      <td>0.022042</td>\n      <td>0.021484</td>\n      <td>0.003849</td>\n      <td>0.016300</td>\n      <td>0.016816</td>\n      <td>0.019521</td>\n      <td>0.022120</td>\n      <td>0.006971</td>\n      <td>0.016685</td>\n      <td>0.021311</td>\n      <td>0.016276</td>\n      <td>0.016973</td>\n      <td>0.017321</td>\n      <td>0.018025</td>\n      <td>0.016981</td>\n      <td>0.017129</td>\n      <td>1.000000</td>\n      <td>0.020822</td>\n      <td>0.007378</td>\n      <td>0.007634</td>\n      <td>0.012756</td>\n      <td>0.001781</td>\n      <td>0.012516</td>\n    </tr>\n    <tr>\n      <th>Hann_window_mean_1500</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.999969</td>\n      <td>0.999999</td>\n      <td>0.083441</td>\n      <td>0.018661</td>\n      <td>0.020966</td>\n      <td>0.024752</td>\n      <td>0.099580</td>\n      <td>0.720941</td>\n      <td>0.014584</td>\n      <td>0.060477</td>\n      <td>0.013437</td>\n      <td>0.014640</td>\n      <td>0.016499</td>\n      <td>0.017718</td>\n      <td>0.016327</td>\n      <td>0.014128</td>\n      <td>0.147895</td>\n      <td>NaN</td>\n      <td>0.248747</td>\n      <td>0.461832</td>\n      <td>0.705907</td>\n      <td>0.408473</td>\n      <td>0.462313</td>\n      <td>0.510035</td>\n      <td>0.609584</td>\n      <td>0.562845</td>\n      <td>0.390680</td>\n      <td>0.369296</td>\n      <td>0.255532</td>\n      <td>0.129896</td>\n      <td>0.065361</td>\n      <td>0.023204</td>\n      <td>0.019478</td>\n      <td>0.000826</td>\n      <td>0.020050</td>\n      <td>0.075617</td>\n      <td>0.000892</td>\n      <td>0.009392</td>\n      <td>...</td>\n      <td>0.013186</td>\n      <td>0.002935</td>\n      <td>0.313290</td>\n      <td>0.019629</td>\n      <td>0.014883</td>\n      <td>0.000620</td>\n      <td>0.010562</td>\n      <td>0.027311</td>\n      <td>0.002502</td>\n      <td>0.010527</td>\n      <td>0.013877</td>\n      <td>0.040181</td>\n      <td>0.024224</td>\n      <td>0.039990</td>\n      <td>0.017052</td>\n      <td>0.012592</td>\n      <td>0.006941</td>\n      <td>0.022051</td>\n      <td>0.021521</td>\n      <td>0.003932</td>\n      <td>0.016436</td>\n      <td>0.016940</td>\n      <td>0.019659</td>\n      <td>0.022263</td>\n      <td>0.007016</td>\n      <td>0.016821</td>\n      <td>0.021457</td>\n      <td>0.016393</td>\n      <td>0.017094</td>\n      <td>0.017443</td>\n      <td>0.018151</td>\n      <td>0.017101</td>\n      <td>0.017251</td>\n      <td>0.999999</td>\n      <td>0.020767</td>\n      <td>0.007250</td>\n      <td>0.007653</td>\n      <td>0.012637</td>\n      <td>0.001676</td>\n      <td>0.012531</td>\n    </tr>\n    <tr>\n      <th>Hann_window_mean_15000</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.999963</td>\n      <td>0.083840</td>\n      <td>0.019154</td>\n      <td>0.021460</td>\n      <td>0.025285</td>\n      <td>0.100126</td>\n      <td>0.721307</td>\n      <td>0.015183</td>\n      <td>0.061098</td>\n      <td>0.013967</td>\n      <td>0.015158</td>\n      <td>0.017045</td>\n      <td>0.018262</td>\n      <td>0.016833</td>\n      <td>0.014645</td>\n      <td>0.148309</td>\n      <td>NaN</td>\n      <td>0.248987</td>\n      <td>0.461855</td>\n      <td>0.705917</td>\n      <td>0.408588</td>\n      <td>0.462341</td>\n      <td>0.510036</td>\n      <td>0.609555</td>\n      <td>0.562679</td>\n      <td>0.390686</td>\n      <td>0.369374</td>\n      <td>0.255537</td>\n      <td>0.130202</td>\n      <td>0.065729</td>\n      <td>0.023665</td>\n      <td>0.020005</td>\n      <td>0.000012</td>\n      <td>0.020098</td>\n      <td>0.075489</td>\n      <td>0.000811</td>\n      <td>0.009255</td>\n      <td>...</td>\n      <td>0.013861</td>\n      <td>0.002498</td>\n      <td>0.313338</td>\n      <td>0.020183</td>\n      <td>0.015483</td>\n      <td>0.001183</td>\n      <td>0.010531</td>\n      <td>0.027802</td>\n      <td>0.002623</td>\n      <td>0.011115</td>\n      <td>0.014428</td>\n      <td>0.040331</td>\n      <td>0.024682</td>\n      <td>0.040268</td>\n      <td>0.017568</td>\n      <td>0.012362</td>\n      <td>0.006715</td>\n      <td>0.021928</td>\n      <td>0.021536</td>\n      <td>0.004100</td>\n      <td>0.017344</td>\n      <td>0.017461</td>\n      <td>0.020230</td>\n      <td>0.022762</td>\n      <td>0.007143</td>\n      <td>0.017429</td>\n      <td>0.022005</td>\n      <td>0.016903</td>\n      <td>0.017615</td>\n      <td>0.017970</td>\n      <td>0.018710</td>\n      <td>0.017619</td>\n      <td>0.017775</td>\n      <td>0.999963</td>\n      <td>0.020432</td>\n      <td>0.006547</td>\n      <td>0.007932</td>\n      <td>0.012109</td>\n      <td>0.000663</td>\n      <td>0.012656</td>\n    </tr>\n    <tr>\n      <th>Hann_window_mean_50</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.083328</td>\n      <td>0.018533</td>\n      <td>0.020836</td>\n      <td>0.024620</td>\n      <td>0.099443</td>\n      <td>0.720846</td>\n      <td>0.014445</td>\n      <td>0.060333</td>\n      <td>0.013307</td>\n      <td>0.014509</td>\n      <td>0.016369</td>\n      <td>0.017590</td>\n      <td>0.016196</td>\n      <td>0.014000</td>\n      <td>0.147782</td>\n      <td>NaN</td>\n      <td>0.248712</td>\n      <td>0.461796</td>\n      <td>0.705890</td>\n      <td>0.408420</td>\n      <td>0.462258</td>\n      <td>0.510017</td>\n      <td>0.609534</td>\n      <td>0.562834</td>\n      <td>0.390639</td>\n      <td>0.369217</td>\n      <td>0.255476</td>\n      <td>0.129804</td>\n      <td>0.065248</td>\n      <td>0.023092</td>\n      <td>0.019350</td>\n      <td>0.000939</td>\n      <td>0.020052</td>\n      <td>0.075642</td>\n      <td>0.000875</td>\n      <td>0.009394</td>\n      <td>...</td>\n      <td>0.013055</td>\n      <td>0.003094</td>\n      <td>0.313326</td>\n      <td>0.019510</td>\n      <td>0.014752</td>\n      <td>0.000487</td>\n      <td>0.010636</td>\n      <td>0.027132</td>\n      <td>0.002471</td>\n      <td>0.010368</td>\n      <td>0.013734</td>\n      <td>0.040102</td>\n      <td>0.024111</td>\n      <td>0.039845</td>\n      <td>0.016925</td>\n      <td>0.012625</td>\n      <td>0.006941</td>\n      <td>0.022039</td>\n      <td>0.021502</td>\n      <td>0.003837</td>\n      <td>0.016295</td>\n      <td>0.016810</td>\n      <td>0.019514</td>\n      <td>0.022116</td>\n      <td>0.006974</td>\n      <td>0.016678</td>\n      <td>0.021307</td>\n      <td>0.016270</td>\n      <td>0.016967</td>\n      <td>0.017315</td>\n      <td>0.018019</td>\n      <td>0.016975</td>\n      <td>0.017123</td>\n      <td>1.000000</td>\n      <td>0.020827</td>\n      <td>0.007387</td>\n      <td>0.007637</td>\n      <td>0.012767</td>\n      <td>0.001788</td>\n      <td>0.012514</td>\n    </tr>\n    <tr>\n      <th>Hilbert_mean</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.921830</td>\n      <td>0.914453</td>\n      <td>0.913577</td>\n      <td>0.906292</td>\n      <td>0.641933</td>\n      <td>0.900423</td>\n      <td>0.913981</td>\n      <td>0.927295</td>\n      <td>0.924548</td>\n      <td>0.935940</td>\n      <td>0.953927</td>\n      <td>0.926637</td>\n      <td>0.930437</td>\n      <td>0.996892</td>\n      <td>NaN</td>\n      <td>0.069217</td>\n      <td>0.174461</td>\n      <td>0.116197</td>\n      <td>0.340900</td>\n      <td>0.430012</td>\n      <td>0.039092</td>\n      <td>0.413038</td>\n      <td>0.522492</td>\n      <td>0.700277</td>\n      <td>0.785903</td>\n      <td>0.853411</td>\n      <td>0.965587</td>\n      <td>0.987026</td>\n      <td>0.980670</td>\n      <td>0.969054</td>\n      <td>0.135146</td>\n      <td>0.243168</td>\n      <td>0.054447</td>\n      <td>0.124555</td>\n      <td>0.010388</td>\n      <td>...</td>\n      <td>0.855404</td>\n      <td>0.656322</td>\n      <td>0.605957</td>\n      <td>0.904255</td>\n      <td>0.839274</td>\n      <td>0.725561</td>\n      <td>0.494829</td>\n      <td>0.436381</td>\n      <td>0.036778</td>\n      <td>0.790182</td>\n      <td>0.877338</td>\n      <td>0.383361</td>\n      <td>0.889424</td>\n      <td>0.589186</td>\n      <td>0.974516</td>\n      <td>0.115931</td>\n      <td>0.316064</td>\n      <td>0.654495</td>\n      <td>0.209951</td>\n      <td>0.350711</td>\n      <td>0.594741</td>\n      <td>0.970150</td>\n      <td>0.938619</td>\n      <td>0.880219</td>\n      <td>0.297097</td>\n      <td>0.940205</td>\n      <td>0.908090</td>\n      <td>0.974011</td>\n      <td>0.970523</td>\n      <td>0.967566</td>\n      <td>0.954831</td>\n      <td>0.971186</td>\n      <td>0.969030</td>\n      <td>0.083320</td>\n      <td>0.238684</td>\n      <td>0.386343</td>\n      <td>0.147088</td>\n      <td>0.326933</td>\n      <td>0.081098</td>\n      <td>0.010292</td>\n    </tr>\n    <tr>\n      <th>abs_max</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.995275</td>\n      <td>0.967230</td>\n      <td>0.927061</td>\n      <td>0.606543</td>\n      <td>0.957392</td>\n      <td>0.945344</td>\n      <td>0.991110</td>\n      <td>0.992045</td>\n      <td>0.984663</td>\n      <td>0.973417</td>\n      <td>0.995075</td>\n      <td>0.989158</td>\n      <td>0.923147</td>\n      <td>NaN</td>\n      <td>0.073330</td>\n      <td>0.162686</td>\n      <td>0.074312</td>\n      <td>0.286721</td>\n      <td>0.335801</td>\n      <td>0.018561</td>\n      <td>0.295284</td>\n      <td>0.374788</td>\n      <td>0.525919</td>\n      <td>0.624718</td>\n      <td>0.681517</td>\n      <td>0.834035</td>\n      <td>0.885494</td>\n      <td>0.915744</td>\n      <td>0.970382</td>\n      <td>0.228264</td>\n      <td>0.133150</td>\n      <td>0.058669</td>\n      <td>0.110233</td>\n      <td>0.010577</td>\n      <td>...</td>\n      <td>0.841632</td>\n      <td>0.745325</td>\n      <td>0.450612</td>\n      <td>0.869422</td>\n      <td>0.842689</td>\n      <td>0.768089</td>\n      <td>0.577305</td>\n      <td>0.581991</td>\n      <td>0.083787</td>\n      <td>0.814412</td>\n      <td>0.900964</td>\n      <td>0.404000</td>\n      <td>0.889038</td>\n      <td>0.670172</td>\n      <td>0.968720</td>\n      <td>0.044593</td>\n      <td>0.355941</td>\n      <td>0.587770</td>\n      <td>0.219485</td>\n      <td>0.443818</td>\n      <td>0.687016</td>\n      <td>0.970931</td>\n      <td>0.949807</td>\n      <td>0.875184</td>\n      <td>0.284777</td>\n      <td>0.947902</td>\n      <td>0.913363</td>\n      <td>0.966385</td>\n      <td>0.972362</td>\n      <td>0.972990</td>\n      <td>0.971464</td>\n      <td>0.972371</td>\n      <td>0.972710</td>\n      <td>0.018526</td>\n      <td>0.280354</td>\n      <td>0.480662</td>\n      <td>0.129822</td>\n      <td>0.430351</td>\n      <td>0.129086</td>\n      <td>0.005597</td>\n    </tr>\n    <tr>\n      <th>abs_max_roll_mean_10</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.967127</td>\n      <td>0.921973</td>\n      <td>0.604708</td>\n      <td>0.955712</td>\n      <td>0.941927</td>\n      <td>0.980348</td>\n      <td>0.984667</td>\n      <td>0.977934</td>\n      <td>0.965978</td>\n      <td>0.989961</td>\n      <td>0.981603</td>\n      <td>0.915998</td>\n      <td>NaN</td>\n      <td>0.075101</td>\n      <td>0.164459</td>\n      <td>0.073649</td>\n      <td>0.288690</td>\n      <td>0.333737</td>\n      <td>0.018436</td>\n      <td>0.294439</td>\n      <td>0.371534</td>\n      <td>0.519772</td>\n      <td>0.616863</td>\n      <td>0.673395</td>\n      <td>0.827511</td>\n      <td>0.880637</td>\n      <td>0.907536</td>\n      <td>0.963659</td>\n      <td>0.232855</td>\n      <td>0.114814</td>\n      <td>0.066250</td>\n      <td>0.105575</td>\n      <td>0.010961</td>\n      <td>...</td>\n      <td>0.828878</td>\n      <td>0.734133</td>\n      <td>0.443513</td>\n      <td>0.861672</td>\n      <td>0.829060</td>\n      <td>0.750337</td>\n      <td>0.589195</td>\n      <td>0.596644</td>\n      <td>0.081894</td>\n      <td>0.812835</td>\n      <td>0.900733</td>\n      <td>0.412823</td>\n      <td>0.888807</td>\n      <td>0.671950</td>\n      <td>0.961946</td>\n      <td>0.041906</td>\n      <td>0.378269</td>\n      <td>0.582523</td>\n      <td>0.221889</td>\n      <td>0.456111</td>\n      <td>0.693327</td>\n      <td>0.967127</td>\n      <td>0.947900</td>\n      <td>0.869439</td>\n      <td>0.283836</td>\n      <td>0.944699</td>\n      <td>0.909310</td>\n      <td>0.955623</td>\n      <td>0.965217</td>\n      <td>0.965904</td>\n      <td>0.964373</td>\n      <td>0.965167</td>\n      <td>0.965604</td>\n      <td>0.020829</td>\n      <td>0.278799</td>\n      <td>0.484147</td>\n      <td>0.128824</td>\n      <td>0.435176</td>\n      <td>0.141289</td>\n      <td>0.001672</td>\n    </tr>\n    <tr>\n      <th>abs_max_roll_mean_100</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.960696</td>\n      <td>0.643491</td>\n      <td>0.979198</td>\n      <td>0.968511</td>\n      <td>0.963731</td>\n      <td>0.967667</td>\n      <td>0.979871</td>\n      <td>0.976960</td>\n      <td>0.969230</td>\n      <td>0.977391</td>\n      <td>0.917369</td>\n      <td>NaN</td>\n      <td>0.070279</td>\n      <td>0.173607</td>\n      <td>0.075419</td>\n      <td>0.286545</td>\n      <td>0.333473</td>\n      <td>0.022795</td>\n      <td>0.287722</td>\n      <td>0.356903</td>\n      <td>0.505130</td>\n      <td>0.605955</td>\n      <td>0.663228</td>\n      <td>0.823844</td>\n      <td>0.869041</td>\n      <td>0.919575</td>\n      <td>0.970737</td>\n      <td>0.216512</td>\n      <td>0.066991</td>\n      <td>0.046986</td>\n      <td>0.112927</td>\n      <td>0.008479</td>\n      <td>...</td>\n      <td>0.872242</td>\n      <td>0.781024</td>\n      <td>0.410564</td>\n      <td>0.902913</td>\n      <td>0.900804</td>\n      <td>0.786130</td>\n      <td>0.639486</td>\n      <td>0.530086</td>\n      <td>0.044493</td>\n      <td>0.886635</td>\n      <td>0.915146</td>\n      <td>0.389580</td>\n      <td>0.930923</td>\n      <td>0.618643</td>\n      <td>0.968132</td>\n      <td>0.036206</td>\n      <td>0.346658</td>\n      <td>0.566415</td>\n      <td>0.196010</td>\n      <td>0.380082</td>\n      <td>0.642701</td>\n      <td>0.975163</td>\n      <td>0.979315</td>\n      <td>0.913905</td>\n      <td>0.309953</td>\n      <td>0.977933</td>\n      <td>0.946574</td>\n      <td>0.957189</td>\n      <td>0.970167</td>\n      <td>0.971402</td>\n      <td>0.973507</td>\n      <td>0.969441</td>\n      <td>0.970876</td>\n      <td>0.024612</td>\n      <td>0.274371</td>\n      <td>0.447293</td>\n      <td>0.235060</td>\n      <td>0.391080</td>\n      <td>0.109102</td>\n      <td>0.008122</td>\n    </tr>\n    <tr>\n      <th>abs_max_roll_mean_1000</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.720309</td>\n      <td>0.954058</td>\n      <td>0.986543</td>\n      <td>0.936152</td>\n      <td>0.929186</td>\n      <td>0.950981</td>\n      <td>0.961783</td>\n      <td>0.929408</td>\n      <td>0.941261</td>\n      <td>0.917870</td>\n      <td>NaN</td>\n      <td>0.095210</td>\n      <td>0.217518</td>\n      <td>0.126006</td>\n      <td>0.318979</td>\n      <td>0.365987</td>\n      <td>0.062182</td>\n      <td>0.324046</td>\n      <td>0.371375</td>\n      <td>0.514116</td>\n      <td>0.618795</td>\n      <td>0.665697</td>\n      <td>0.821746</td>\n      <td>0.860279</td>\n      <td>0.927042</td>\n      <td>0.956922</td>\n      <td>0.193422</td>\n      <td>0.012816</td>\n      <td>0.089102</td>\n      <td>0.120067</td>\n      <td>0.004640</td>\n      <td>...</td>\n      <td>0.917043</td>\n      <td>0.808856</td>\n      <td>0.336589</td>\n      <td>0.933899</td>\n      <td>0.917147</td>\n      <td>0.826073</td>\n      <td>0.626732</td>\n      <td>0.540763</td>\n      <td>0.022524</td>\n      <td>0.935437</td>\n      <td>0.919966</td>\n      <td>0.394233</td>\n      <td>0.932401</td>\n      <td>0.646822</td>\n      <td>0.953447</td>\n      <td>0.032078</td>\n      <td>0.337452</td>\n      <td>0.572168</td>\n      <td>0.183782</td>\n      <td>0.333666</td>\n      <td>0.622304</td>\n      <td>0.953696</td>\n      <td>0.975332</td>\n      <td>0.959611</td>\n      <td>0.340909</td>\n      <td>0.972752</td>\n      <td>0.975112</td>\n      <td>0.947715</td>\n      <td>0.952971</td>\n      <td>0.954599</td>\n      <td>0.959948</td>\n      <td>0.952101</td>\n      <td>0.953837</td>\n      <td>0.099435</td>\n      <td>0.374864</td>\n      <td>0.545779</td>\n      <td>0.101165</td>\n      <td>0.486383</td>\n      <td>0.206538</td>\n      <td>0.008637</td>\n    </tr>\n    <tr>\n      <th>abs_max_roll_mean_10000</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.631529</td>\n      <td>0.684170</td>\n      <td>0.610063</td>\n      <td>0.605924</td>\n      <td>0.626947</td>\n      <td>0.637572</td>\n      <td>0.606776</td>\n      <td>0.616809</td>\n      <td>0.692560</td>\n      <td>NaN</td>\n      <td>0.227070</td>\n      <td>0.447185</td>\n      <td>0.537874</td>\n      <td>0.475232</td>\n      <td>0.539932</td>\n      <td>0.373458</td>\n      <td>0.603978</td>\n      <td>0.600195</td>\n      <td>0.578373</td>\n      <td>0.637144</td>\n      <td>0.592714</td>\n      <td>0.617939</td>\n      <td>0.600309</td>\n      <td>0.619439</td>\n      <td>0.635138</td>\n      <td>0.133568</td>\n      <td>0.019600</td>\n      <td>0.104401</td>\n      <td>0.078151</td>\n      <td>0.013243</td>\n      <td>...</td>\n      <td>0.614953</td>\n      <td>0.529146</td>\n      <td>0.001622</td>\n      <td>0.630003</td>\n      <td>0.621173</td>\n      <td>0.546485</td>\n      <td>0.410176</td>\n      <td>0.362395</td>\n      <td>0.013463</td>\n      <td>0.626008</td>\n      <td>0.612760</td>\n      <td>0.273778</td>\n      <td>0.631222</td>\n      <td>0.433646</td>\n      <td>0.630986</td>\n      <td>0.025524</td>\n      <td>0.212555</td>\n      <td>0.376971</td>\n      <td>0.103586</td>\n      <td>0.204584</td>\n      <td>0.419092</td>\n      <td>0.631380</td>\n      <td>0.655402</td>\n      <td>0.676523</td>\n      <td>0.382021</td>\n      <td>0.650350</td>\n      <td>0.672755</td>\n      <td>0.625364</td>\n      <td>0.630028</td>\n      <td>0.631512</td>\n      <td>0.636916</td>\n      <td>0.629326</td>\n      <td>0.630785</td>\n      <td>0.720839</td>\n      <td>0.233524</td>\n      <td>0.343550</td>\n      <td>0.086517</td>\n      <td>0.302303</td>\n      <td>0.132448</td>\n      <td>0.017413</td>\n    </tr>\n    <tr>\n      <th>abs_max_roll_mean_50</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.976146</td>\n      <td>0.965340</td>\n      <td>0.962509</td>\n      <td>0.976660</td>\n      <td>0.967296</td>\n      <td>0.961294</td>\n      <td>0.972394</td>\n      <td>0.903651</td>\n      <td>NaN</td>\n      <td>0.075162</td>\n      <td>0.158445</td>\n      <td>0.072584</td>\n      <td>0.276723</td>\n      <td>0.321122</td>\n      <td>0.020902</td>\n      <td>0.274581</td>\n      <td>0.341374</td>\n      <td>0.489243</td>\n      <td>0.587859</td>\n      <td>0.642674</td>\n      <td>0.806543</td>\n      <td>0.857557</td>\n      <td>0.900917</td>\n      <td>0.963639</td>\n      <td>0.281174</td>\n      <td>0.064140</td>\n      <td>0.058868</td>\n      <td>0.102189</td>\n      <td>0.007236</td>\n      <td>...</td>\n      <td>0.884016</td>\n      <td>0.827846</td>\n      <td>0.407118</td>\n      <td>0.884342</td>\n      <td>0.896726</td>\n      <td>0.844106</td>\n      <td>0.638345</td>\n      <td>0.583127</td>\n      <td>0.051147</td>\n      <td>0.896209</td>\n      <td>0.917467</td>\n      <td>0.366566</td>\n      <td>0.887913</td>\n      <td>0.609297</td>\n      <td>0.960920</td>\n      <td>0.037284</td>\n      <td>0.296738</td>\n      <td>0.534519</td>\n      <td>0.217834</td>\n      <td>0.380608</td>\n      <td>0.703569</td>\n      <td>0.965694</td>\n      <td>0.968962</td>\n      <td>0.892641</td>\n      <td>0.295141</td>\n      <td>0.975717</td>\n      <td>0.931237</td>\n      <td>0.953265</td>\n      <td>0.963105</td>\n      <td>0.964265</td>\n      <td>0.966316</td>\n      <td>0.962162</td>\n      <td>0.963787</td>\n      <td>0.014440</td>\n      <td>0.408770</td>\n      <td>0.534708</td>\n      <td>0.202635</td>\n      <td>0.510571</td>\n      <td>0.197104</td>\n      <td>0.004854</td>\n    </tr>\n    <tr>\n      <th>abs_max_roll_mean_500</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.959068</td>\n      <td>0.954163</td>\n      <td>0.972483</td>\n      <td>0.975576</td>\n      <td>0.952649</td>\n      <td>0.963407</td>\n      <td>0.922012</td>\n      <td>NaN</td>\n      <td>0.079930</td>\n      <td>0.189270</td>\n      <td>0.099599</td>\n      <td>0.301990</td>\n      <td>0.347247</td>\n      <td>0.043670</td>\n      <td>0.303024</td>\n      <td>0.355616</td>\n      <td>0.502479</td>\n      <td>0.604849</td>\n      <td>0.656758</td>\n      <td>0.822616</td>\n      <td>0.871902</td>\n      <td>0.928350</td>\n      <td>0.971035</td>\n      <td>0.271534</td>\n      <td>0.033892</td>\n      <td>0.082009</td>\n      <td>0.097838</td>\n      <td>0.007210</td>\n      <td>...</td>\n      <td>0.914121</td>\n      <td>0.813557</td>\n      <td>0.368953</td>\n      <td>0.919620</td>\n      <td>0.907591</td>\n      <td>0.843792</td>\n      <td>0.603499</td>\n      <td>0.577530</td>\n      <td>0.043479</td>\n      <td>0.922060</td>\n      <td>0.924294</td>\n      <td>0.393491</td>\n      <td>0.913906</td>\n      <td>0.652601</td>\n      <td>0.967800</td>\n      <td>0.030079</td>\n      <td>0.299244</td>\n      <td>0.542987</td>\n      <td>0.208757</td>\n      <td>0.371211</td>\n      <td>0.690652</td>\n      <td>0.969098</td>\n      <td>0.980658</td>\n      <td>0.937543</td>\n      <td>0.323161</td>\n      <td>0.981359</td>\n      <td>0.967679</td>\n      <td>0.962430</td>\n      <td>0.968506</td>\n      <td>0.969980</td>\n      <td>0.974416</td>\n      <td>0.967695</td>\n      <td>0.969304</td>\n      <td>0.060326</td>\n      <td>0.398355</td>\n      <td>0.566847</td>\n      <td>0.117384</td>\n      <td>0.519996</td>\n      <td>0.252568</td>\n      <td>0.003250</td>\n    </tr>\n    <tr>\n      <th>abs_max_roll_std_10</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.995239</td>\n      <td>0.988679</td>\n      <td>0.978596</td>\n      <td>0.994337</td>\n      <td>0.991692</td>\n      <td>0.927859</td>\n      <td>NaN</td>\n      <td>0.065797</td>\n      <td>0.152954</td>\n      <td>0.072615</td>\n      <td>0.278868</td>\n      <td>0.333629</td>\n      <td>0.017870</td>\n      <td>0.296455</td>\n      <td>0.375379</td>\n      <td>0.531032</td>\n      <td>0.629687</td>\n      <td>0.686251</td>\n      <td>0.837132</td>\n      <td>0.888300</td>\n      <td>0.921611</td>\n      <td>0.975691</td>\n      <td>0.237854</td>\n      <td>0.158290</td>\n      <td>0.048144</td>\n      <td>0.113921</td>\n      <td>0.010018</td>\n      <td>...</td>\n      <td>0.856389</td>\n      <td>0.762375</td>\n      <td>0.462650</td>\n      <td>0.875027</td>\n      <td>0.851900</td>\n      <td>0.786912</td>\n      <td>0.561849</td>\n      <td>0.568783</td>\n      <td>0.077100</td>\n      <td>0.835680</td>\n      <td>0.895792</td>\n      <td>0.393200</td>\n      <td>0.875393</td>\n      <td>0.656192</td>\n      <td>0.974219</td>\n      <td>0.047232</td>\n      <td>0.321131</td>\n      <td>0.583889</td>\n      <td>0.224058</td>\n      <td>0.425069</td>\n      <td>0.690913</td>\n      <td>0.973425</td>\n      <td>0.952803</td>\n      <td>0.877586</td>\n      <td>0.287690</td>\n      <td>0.953616</td>\n      <td>0.916135</td>\n      <td>0.976216</td>\n      <td>0.978240</td>\n      <td>0.978748</td>\n      <td>0.976869</td>\n      <td>0.978191</td>\n      <td>0.978533</td>\n      <td>0.013301</td>\n      <td>0.327742</td>\n      <td>0.510454</td>\n      <td>0.119267</td>\n      <td>0.464604</td>\n      <td>0.174311</td>\n      <td>0.005752</td>\n    </tr>\n    <tr>\n      <th>abs_max_roll_std_100</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.991111</td>\n      <td>0.979095</td>\n      <td>0.997893</td>\n      <td>0.995444</td>\n      <td>0.925116</td>\n      <td>NaN</td>\n      <td>0.065553</td>\n      <td>0.154636</td>\n      <td>0.071507</td>\n      <td>0.278967</td>\n      <td>0.334163</td>\n      <td>0.017735</td>\n      <td>0.295968</td>\n      <td>0.374300</td>\n      <td>0.529136</td>\n      <td>0.626162</td>\n      <td>0.682891</td>\n      <td>0.832667</td>\n      <td>0.885003</td>\n      <td>0.917226</td>\n      <td>0.975215</td>\n      <td>0.245216</td>\n      <td>0.149290</td>\n      <td>0.055156</td>\n      <td>0.103476</td>\n      <td>0.011044</td>\n      <td>...</td>\n      <td>0.840510</td>\n      <td>0.749352</td>\n      <td>0.461600</td>\n      <td>0.867966</td>\n      <td>0.843148</td>\n      <td>0.773020</td>\n      <td>0.549628</td>\n      <td>0.569354</td>\n      <td>0.077142</td>\n      <td>0.822184</td>\n      <td>0.892167</td>\n      <td>0.399614</td>\n      <td>0.875023</td>\n      <td>0.660427</td>\n      <td>0.973592</td>\n      <td>0.045551</td>\n      <td>0.309628</td>\n      <td>0.574843</td>\n      <td>0.219934</td>\n      <td>0.445245</td>\n      <td>0.688157</td>\n      <td>0.974814</td>\n      <td>0.952030</td>\n      <td>0.869382</td>\n      <td>0.283787</td>\n      <td>0.951496</td>\n      <td>0.910627</td>\n      <td>0.973233</td>\n      <td>0.977845</td>\n      <td>0.978437</td>\n      <td>0.976699</td>\n      <td>0.977831</td>\n      <td>0.978180</td>\n      <td>0.014503</td>\n      <td>0.290897</td>\n      <td>0.477633</td>\n      <td>0.135321</td>\n      <td>0.433700</td>\n      <td>0.150905</td>\n      <td>0.001983</td>\n    </tr>\n    <tr>\n      <th>abs_max_roll_std_1000</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.993486</td>\n      <td>0.990169</td>\n      <td>0.998101</td>\n      <td>0.938045</td>\n      <td>NaN</td>\n      <td>0.071226</td>\n      <td>0.164387</td>\n      <td>0.072298</td>\n      <td>0.287248</td>\n      <td>0.342151</td>\n      <td>0.018474</td>\n      <td>0.294594</td>\n      <td>0.368112</td>\n      <td>0.523505</td>\n      <td>0.625950</td>\n      <td>0.684270</td>\n      <td>0.844265</td>\n      <td>0.895381</td>\n      <td>0.936666</td>\n      <td>0.988347</td>\n      <td>0.246062</td>\n      <td>0.108836</td>\n      <td>0.062880</td>\n      <td>0.105845</td>\n      <td>0.010983</td>\n      <td>...</td>\n      <td>0.880924</td>\n      <td>0.773967</td>\n      <td>0.443309</td>\n      <td>0.900323</td>\n      <td>0.889380</td>\n      <td>0.810022</td>\n      <td>0.568106</td>\n      <td>0.568925</td>\n      <td>0.065541</td>\n      <td>0.858862</td>\n      <td>0.908506</td>\n      <td>0.408227</td>\n      <td>0.908321</td>\n      <td>0.657298</td>\n      <td>0.986293</td>\n      <td>0.042137</td>\n      <td>0.294016</td>\n      <td>0.585569</td>\n      <td>0.215268</td>\n      <td>0.404750</td>\n      <td>0.685425</td>\n      <td>0.987372</td>\n      <td>0.975479</td>\n      <td>0.900419</td>\n      <td>0.297532</td>\n      <td>0.975279</td>\n      <td>0.939573</td>\n      <td>0.984093</td>\n      <td>0.989196</td>\n      <td>0.990136</td>\n      <td>0.990978</td>\n      <td>0.988936</td>\n      <td>0.989715</td>\n      <td>0.016361</td>\n      <td>0.306599</td>\n      <td>0.475713</td>\n      <td>0.176522</td>\n      <td>0.429506</td>\n      <td>0.143081</td>\n      <td>0.003252</td>\n    </tr>\n    <tr>\n      <th>abs_max_roll_std_10000</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.979263</td>\n      <td>0.988053</td>\n      <td>0.956769</td>\n      <td>NaN</td>\n      <td>0.075342</td>\n      <td>0.173647</td>\n      <td>0.072529</td>\n      <td>0.301555</td>\n      <td>0.357917</td>\n      <td>0.017396</td>\n      <td>0.304421</td>\n      <td>0.375255</td>\n      <td>0.538404</td>\n      <td>0.643945</td>\n      <td>0.706373</td>\n      <td>0.867168</td>\n      <td>0.913903</td>\n      <td>0.965542</td>\n      <td>0.996865</td>\n      <td>0.210434</td>\n      <td>0.095356</td>\n      <td>0.067142</td>\n      <td>0.116797</td>\n      <td>0.011749</td>\n      <td>...</td>\n      <td>0.908862</td>\n      <td>0.762649</td>\n      <td>0.441856</td>\n      <td>0.937976</td>\n      <td>0.910652</td>\n      <td>0.807629</td>\n      <td>0.558937</td>\n      <td>0.534761</td>\n      <td>0.053049</td>\n      <td>0.872529</td>\n      <td>0.926853</td>\n      <td>0.412265</td>\n      <td>0.934787</td>\n      <td>0.654328</td>\n      <td>0.995185</td>\n      <td>0.044897</td>\n      <td>0.290970</td>\n      <td>0.612860</td>\n      <td>0.199976</td>\n      <td>0.379710</td>\n      <td>0.654083</td>\n      <td>0.994789</td>\n      <td>0.987074</td>\n      <td>0.924540</td>\n      <td>0.308090</td>\n      <td>0.985723</td>\n      <td>0.958704</td>\n      <td>0.992551</td>\n      <td>0.996257</td>\n      <td>0.997072</td>\n      <td>0.998519</td>\n      <td>0.996042</td>\n      <td>0.996686</td>\n      <td>0.017582</td>\n      <td>0.280439</td>\n      <td>0.463870</td>\n      <td>0.172863</td>\n      <td>0.401870</td>\n      <td>0.132122</td>\n      <td>0.005465</td>\n    </tr>\n    <tr>\n      <th>abs_max_roll_std_50</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.993636</td>\n      <td>0.927392</td>\n      <td>NaN</td>\n      <td>0.069188</td>\n      <td>0.158297</td>\n      <td>0.072366</td>\n      <td>0.283571</td>\n      <td>0.336041</td>\n      <td>0.017289</td>\n      <td>0.298294</td>\n      <td>0.376601</td>\n      <td>0.529859</td>\n      <td>0.628097</td>\n      <td>0.684752</td>\n      <td>0.835561</td>\n      <td>0.887583</td>\n      <td>0.920847</td>\n      <td>0.975756</td>\n      <td>0.232946</td>\n      <td>0.146400</td>\n      <td>0.054693</td>\n      <td>0.106820</td>\n      <td>0.010608</td>\n      <td>...</td>\n      <td>0.845691</td>\n      <td>0.745612</td>\n      <td>0.460396</td>\n      <td>0.872178</td>\n      <td>0.845693</td>\n      <td>0.772552</td>\n      <td>0.556484</td>\n      <td>0.573669</td>\n      <td>0.071983</td>\n      <td>0.823493</td>\n      <td>0.897487</td>\n      <td>0.412298</td>\n      <td>0.886013</td>\n      <td>0.669556</td>\n      <td>0.974193</td>\n      <td>0.046164</td>\n      <td>0.332274</td>\n      <td>0.588986</td>\n      <td>0.219683</td>\n      <td>0.451401</td>\n      <td>0.684611</td>\n      <td>0.976073</td>\n      <td>0.954491</td>\n      <td>0.873914</td>\n      <td>0.286047</td>\n      <td>0.952459</td>\n      <td>0.913912</td>\n      <td>0.972759</td>\n      <td>0.978216</td>\n      <td>0.978802</td>\n      <td>0.976897</td>\n      <td>0.978231</td>\n      <td>0.978548</td>\n      <td>0.016190</td>\n      <td>0.273999</td>\n      <td>0.478092</td>\n      <td>0.140813</td>\n      <td>0.424904</td>\n      <td>0.146762</td>\n      <td>0.003012</td>\n    </tr>\n    <tr>\n      <th>abs_max_roll_std_500</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.931704</td>\n      <td>NaN</td>\n      <td>0.066344</td>\n      <td>0.159950</td>\n      <td>0.070515</td>\n      <td>0.282099</td>\n      <td>0.337708</td>\n      <td>0.017564</td>\n      <td>0.293937</td>\n      <td>0.370615</td>\n      <td>0.524900</td>\n      <td>0.625313</td>\n      <td>0.682714</td>\n      <td>0.837716</td>\n      <td>0.889969</td>\n      <td>0.926893</td>\n      <td>0.982980</td>\n      <td>0.246871</td>\n      <td>0.126463</td>\n      <td>0.057773</td>\n      <td>0.105660</td>\n      <td>0.011584</td>\n      <td>...</td>\n      <td>0.862834</td>\n      <td>0.772764</td>\n      <td>0.452489</td>\n      <td>0.884807</td>\n      <td>0.873428</td>\n      <td>0.796177</td>\n      <td>0.570850</td>\n      <td>0.571159</td>\n      <td>0.069721</td>\n      <td>0.843686</td>\n      <td>0.901958</td>\n      <td>0.401309</td>\n      <td>0.894742</td>\n      <td>0.660355</td>\n      <td>0.981098</td>\n      <td>0.044043</td>\n      <td>0.298809</td>\n      <td>0.576545</td>\n      <td>0.220444</td>\n      <td>0.422915</td>\n      <td>0.683452</td>\n      <td>0.982465</td>\n      <td>0.965841</td>\n      <td>0.886438</td>\n      <td>0.290575</td>\n      <td>0.965580</td>\n      <td>0.927191</td>\n      <td>0.979526</td>\n      <td>0.984697</td>\n      <td>0.985491</td>\n      <td>0.985277</td>\n      <td>0.984532</td>\n      <td>0.985147</td>\n      <td>0.013992</td>\n      <td>0.298814</td>\n      <td>0.471815</td>\n      <td>0.171522</td>\n      <td>0.429310</td>\n      <td>0.127370</td>\n      <td>0.002618</td>\n    </tr>\n    <tr>\n      <th>abs_mean</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.093980</td>\n      <td>0.213299</td>\n      <td>0.162676</td>\n      <td>0.368822</td>\n      <td>0.454353</td>\n      <td>0.073365</td>\n      <td>0.441426</td>\n      <td>0.538989</td>\n      <td>0.702373</td>\n      <td>0.791057</td>\n      <td>0.848764</td>\n      <td>0.959679</td>\n      <td>0.979494</td>\n      <td>0.980549</td>\n      <td>0.970341</td>\n      <td>0.139457</td>\n      <td>0.209865</td>\n      <td>0.063257</td>\n      <td>0.124600</td>\n      <td>0.009504</td>\n      <td>...</td>\n      <td>0.864918</td>\n      <td>0.663741</td>\n      <td>0.549239</td>\n      <td>0.913573</td>\n      <td>0.848531</td>\n      <td>0.734089</td>\n      <td>0.498886</td>\n      <td>0.443352</td>\n      <td>0.036626</td>\n      <td>0.798606</td>\n      <td>0.886155</td>\n      <td>0.387523</td>\n      <td>0.897985</td>\n      <td>0.596575</td>\n      <td>0.974478</td>\n      <td>0.105100</td>\n      <td>0.310244</td>\n      <td>0.650419</td>\n      <td>0.202145</td>\n      <td>0.345383</td>\n      <td>0.594542</td>\n      <td>0.970395</td>\n      <td>0.943458</td>\n      <td>0.887673</td>\n      <td>0.298803</td>\n      <td>0.944433</td>\n      <td>0.915383</td>\n      <td>0.973486</td>\n      <td>0.970897</td>\n      <td>0.968573</td>\n      <td>0.958197</td>\n      <td>0.971430</td>\n      <td>0.969719</td>\n      <td>0.147773</td>\n      <td>0.240299</td>\n      <td>0.391398</td>\n      <td>0.146995</td>\n      <td>0.330893</td>\n      <td>0.083680</td>\n      <td>0.010550</td>\n    </tr>\n    <tr>\n      <th>abs_percentile_1</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>abs_percentile_10</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.474786</td>\n      <td>0.083422</td>\n      <td>0.527015</td>\n      <td>0.112651</td>\n      <td>0.032872</td>\n      <td>0.093215</td>\n      <td>0.139004</td>\n      <td>0.105009</td>\n      <td>0.083585</td>\n      <td>0.070284</td>\n      <td>0.069860</td>\n      <td>0.052349</td>\n      <td>0.074148</td>\n      <td>0.074894</td>\n      <td>0.011167</td>\n      <td>0.131478</td>\n      <td>0.025527</td>\n      <td>0.000305</td>\n      <td>0.006845</td>\n      <td>...</td>\n      <td>0.090924</td>\n      <td>0.032183</td>\n      <td>0.153027</td>\n      <td>0.107008</td>\n      <td>0.086814</td>\n      <td>0.093254</td>\n      <td>0.004622</td>\n      <td>0.059765</td>\n      <td>0.009907</td>\n      <td>0.062444</td>\n      <td>0.099988</td>\n      <td>0.051866</td>\n      <td>0.093437</td>\n      <td>0.037032</td>\n      <td>0.070305</td>\n      <td>0.033003</td>\n      <td>0.022084</td>\n      <td>0.084692</td>\n      <td>0.028366</td>\n      <td>0.025533</td>\n      <td>0.022537</td>\n      <td>0.070913</td>\n      <td>0.082279</td>\n      <td>0.081174</td>\n      <td>0.031629</td>\n      <td>0.082613</td>\n      <td>0.077256</td>\n      <td>0.067946</td>\n      <td>0.071169</td>\n      <td>0.073317</td>\n      <td>0.080341</td>\n      <td>0.070728</td>\n      <td>0.072231</td>\n      <td>0.248698</td>\n      <td>0.016147</td>\n      <td>0.021746</td>\n      <td>0.012727</td>\n      <td>0.004526</td>\n      <td>0.012133</td>\n      <td>0.004221</td>\n    </tr>\n    <tr>\n      <th>abs_percentile_20</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.147986</td>\n      <td>0.653074</td>\n      <td>0.222230</td>\n      <td>0.069235</td>\n      <td>0.195117</td>\n      <td>0.279930</td>\n      <td>0.209690</td>\n      <td>0.234525</td>\n      <td>0.187657</td>\n      <td>0.176125</td>\n      <td>0.152788</td>\n      <td>0.171164</td>\n      <td>0.169705</td>\n      <td>0.003307</td>\n      <td>0.123701</td>\n      <td>0.071442</td>\n      <td>0.010869</td>\n      <td>0.018671</td>\n      <td>...</td>\n      <td>0.172051</td>\n      <td>0.129002</td>\n      <td>0.184011</td>\n      <td>0.198003</td>\n      <td>0.175529</td>\n      <td>0.134575</td>\n      <td>0.102206</td>\n      <td>0.100885</td>\n      <td>0.010836</td>\n      <td>0.163773</td>\n      <td>0.185778</td>\n      <td>0.105864</td>\n      <td>0.197382</td>\n      <td>0.146168</td>\n      <td>0.164572</td>\n      <td>0.003665</td>\n      <td>0.042852</td>\n      <td>0.117232</td>\n      <td>0.006699</td>\n      <td>0.037700</td>\n      <td>0.059545</td>\n      <td>0.165369</td>\n      <td>0.183868</td>\n      <td>0.196125</td>\n      <td>0.072939</td>\n      <td>0.178245</td>\n      <td>0.194663</td>\n      <td>0.160672</td>\n      <td>0.164954</td>\n      <td>0.167220</td>\n      <td>0.175788</td>\n      <td>0.164535</td>\n      <td>0.166101</td>\n      <td>0.461783</td>\n      <td>0.004946</td>\n      <td>0.046893</td>\n      <td>0.029865</td>\n      <td>0.032468</td>\n      <td>0.026348</td>\n      <td>0.004742</td>\n    </tr>\n    <tr>\n      <th>abs_percentile_25</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.137638</td>\n      <td>0.373187</td>\n      <td>0.491815</td>\n      <td>0.613662</td>\n      <td>0.460597</td>\n      <td>0.320610</td>\n      <td>0.290671</td>\n      <td>0.212314</td>\n      <td>0.142379</td>\n      <td>0.102266</td>\n      <td>0.073490</td>\n      <td>0.075174</td>\n      <td>0.004698</td>\n      <td>0.027678</td>\n      <td>0.057066</td>\n      <td>0.020780</td>\n      <td>0.007516</td>\n      <td>...</td>\n      <td>0.060788</td>\n      <td>0.036941</td>\n      <td>0.214975</td>\n      <td>0.068885</td>\n      <td>0.059599</td>\n      <td>0.057600</td>\n      <td>0.018697</td>\n      <td>0.038229</td>\n      <td>0.022630</td>\n      <td>0.051277</td>\n      <td>0.067850</td>\n      <td>0.037577</td>\n      <td>0.066527</td>\n      <td>0.047986</td>\n      <td>0.072886</td>\n      <td>0.006993</td>\n      <td>0.016361</td>\n      <td>0.057555</td>\n      <td>0.021791</td>\n      <td>0.016248</td>\n      <td>0.044204</td>\n      <td>0.072152</td>\n      <td>0.070914</td>\n      <td>0.060299</td>\n      <td>0.008489</td>\n      <td>0.071570</td>\n      <td>0.063311</td>\n      <td>0.073215</td>\n      <td>0.073323</td>\n      <td>0.073781</td>\n      <td>0.074258</td>\n      <td>0.073275</td>\n      <td>0.073535</td>\n      <td>0.705900</td>\n      <td>0.015010</td>\n      <td>0.009312</td>\n      <td>0.009675</td>\n      <td>0.012931</td>\n      <td>0.001161</td>\n      <td>0.002078</td>\n    </tr>\n    <tr>\n      <th>abs_percentile_30</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.382488</td>\n      <td>0.100884</td>\n      <td>0.245936</td>\n      <td>0.297229</td>\n      <td>0.356322</td>\n      <td>0.379173</td>\n      <td>0.368816</td>\n      <td>0.350948</td>\n      <td>0.329986</td>\n      <td>0.323001</td>\n      <td>0.307135</td>\n      <td>0.021821</td>\n      <td>0.008431</td>\n      <td>0.079704</td>\n      <td>0.048290</td>\n      <td>0.004234</td>\n      <td>...</td>\n      <td>0.286423</td>\n      <td>0.186473</td>\n      <td>0.016548</td>\n      <td>0.316180</td>\n      <td>0.268200</td>\n      <td>0.224701</td>\n      <td>0.135451</td>\n      <td>0.133731</td>\n      <td>0.000403</td>\n      <td>0.240239</td>\n      <td>0.307326</td>\n      <td>0.156182</td>\n      <td>0.303501</td>\n      <td>0.206550</td>\n      <td>0.306547</td>\n      <td>0.054638</td>\n      <td>0.094591</td>\n      <td>0.217089</td>\n      <td>0.031186</td>\n      <td>0.097615</td>\n      <td>0.161365</td>\n      <td>0.306342</td>\n      <td>0.305076</td>\n      <td>0.299653</td>\n      <td>0.096390</td>\n      <td>0.303722</td>\n      <td>0.304729</td>\n      <td>0.301985</td>\n      <td>0.303871</td>\n      <td>0.303920</td>\n      <td>0.304681</td>\n      <td>0.303945</td>\n      <td>0.303834</td>\n      <td>0.408403</td>\n      <td>0.024815</td>\n      <td>0.097377</td>\n      <td>0.042409</td>\n      <td>0.067184</td>\n      <td>0.001329</td>\n      <td>0.000741</td>\n    </tr>\n    <tr>\n      <th>abs_percentile_40</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.472393</td>\n      <td>0.506991</td>\n      <td>0.372581</td>\n      <td>0.416412</td>\n      <td>0.526532</td>\n      <td>0.487505</td>\n      <td>0.449933</td>\n      <td>0.422731</td>\n      <td>0.390318</td>\n      <td>0.367468</td>\n      <td>0.034440</td>\n      <td>0.086926</td>\n      <td>0.100711</td>\n      <td>0.068989</td>\n      <td>0.009710</td>\n      <td>...</td>\n      <td>0.314958</td>\n      <td>0.211591</td>\n      <td>0.140391</td>\n      <td>0.353157</td>\n      <td>0.311805</td>\n      <td>0.243408</td>\n      <td>0.137969</td>\n      <td>0.134195</td>\n      <td>0.021697</td>\n      <td>0.275105</td>\n      <td>0.323046</td>\n      <td>0.170647</td>\n      <td>0.336257</td>\n      <td>0.227028</td>\n      <td>0.370393</td>\n      <td>0.091998</td>\n      <td>0.109032</td>\n      <td>0.317948</td>\n      <td>0.067212</td>\n      <td>0.121788</td>\n      <td>0.174788</td>\n      <td>0.366639</td>\n      <td>0.350546</td>\n      <td>0.336813</td>\n      <td>0.109864</td>\n      <td>0.349978</td>\n      <td>0.339928</td>\n      <td>0.369861</td>\n      <td>0.365943</td>\n      <td>0.363989</td>\n      <td>0.357551</td>\n      <td>0.366520</td>\n      <td>0.364880</td>\n      <td>0.462254</td>\n      <td>0.029557</td>\n      <td>0.094783</td>\n      <td>0.048199</td>\n      <td>0.062777</td>\n      <td>0.014571</td>\n      <td>0.019656</td>\n    </tr>\n    <tr>\n      <th>abs_percentile_5</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.438479</td>\n      <td>0.263185</td>\n      <td>0.187473</td>\n      <td>0.153792</td>\n      <td>0.126848</td>\n      <td>0.054718</td>\n      <td>0.029255</td>\n      <td>0.012603</td>\n      <td>0.016651</td>\n      <td>0.002839</td>\n      <td>0.061464</td>\n      <td>0.081451</td>\n      <td>0.008916</td>\n      <td>0.012314</td>\n      <td>...</td>\n      <td>0.015236</td>\n      <td>0.012519</td>\n      <td>0.207210</td>\n      <td>0.016704</td>\n      <td>0.015267</td>\n      <td>0.013434</td>\n      <td>0.009291</td>\n      <td>0.009217</td>\n      <td>0.012558</td>\n      <td>0.014814</td>\n      <td>0.016889</td>\n      <td>0.020973</td>\n      <td>0.017337</td>\n      <td>0.018988</td>\n      <td>0.014049</td>\n      <td>0.001197</td>\n      <td>0.010245</td>\n      <td>0.005279</td>\n      <td>0.006011</td>\n      <td>0.005738</td>\n      <td>0.009249</td>\n      <td>0.014272</td>\n      <td>0.016891</td>\n      <td>0.014417</td>\n      <td>0.020761</td>\n      <td>0.016880</td>\n      <td>0.016595</td>\n      <td>0.013676</td>\n      <td>0.014723</td>\n      <td>0.015523</td>\n      <td>0.017856</td>\n      <td>0.014603</td>\n      <td>0.015093</td>\n      <td>0.510031</td>\n      <td>0.004640</td>\n      <td>0.007371</td>\n      <td>0.002677</td>\n      <td>0.006614</td>\n      <td>0.002244</td>\n      <td>0.008751</td>\n    </tr>\n    <tr>\n      <th>abs_percentile_50</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.602668</td>\n      <td>0.539341</td>\n      <td>0.588822</td>\n      <td>0.531280</td>\n      <td>0.462814</td>\n      <td>0.415017</td>\n      <td>0.343676</td>\n      <td>0.318991</td>\n      <td>0.022669</td>\n      <td>0.202263</td>\n      <td>0.032344</td>\n      <td>0.056039</td>\n      <td>0.005573</td>\n      <td>...</td>\n      <td>0.240923</td>\n      <td>0.156890</td>\n      <td>0.174864</td>\n      <td>0.276744</td>\n      <td>0.230805</td>\n      <td>0.185967</td>\n      <td>0.125963</td>\n      <td>0.089949</td>\n      <td>0.018660</td>\n      <td>0.217625</td>\n      <td>0.260132</td>\n      <td>0.133658</td>\n      <td>0.270945</td>\n      <td>0.188006</td>\n      <td>0.324686</td>\n      <td>0.097784</td>\n      <td>0.143864</td>\n      <td>0.267652</td>\n      <td>0.065399</td>\n      <td>0.120148</td>\n      <td>0.159679</td>\n      <td>0.321140</td>\n      <td>0.291823</td>\n      <td>0.271204</td>\n      <td>0.086597</td>\n      <td>0.292992</td>\n      <td>0.275755</td>\n      <td>0.326342</td>\n      <td>0.320839</td>\n      <td>0.317431</td>\n      <td>0.303577</td>\n      <td>0.321722</td>\n      <td>0.319066</td>\n      <td>0.609528</td>\n      <td>0.032377</td>\n      <td>0.074954</td>\n      <td>0.034547</td>\n      <td>0.052979</td>\n      <td>0.007265</td>\n      <td>0.002666</td>\n    </tr>\n    <tr>\n      <th>abs_percentile_60</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.811593</td>\n      <td>0.601317</td>\n      <td>0.696005</td>\n      <td>0.588889</td>\n      <td>0.534272</td>\n      <td>0.428185</td>\n      <td>0.398298</td>\n      <td>0.031786</td>\n      <td>0.351558</td>\n      <td>0.016797</td>\n      <td>0.070161</td>\n      <td>0.000430</td>\n      <td>...</td>\n      <td>0.277598</td>\n      <td>0.188923</td>\n      <td>0.384422</td>\n      <td>0.318225</td>\n      <td>0.274585</td>\n      <td>0.214149</td>\n      <td>0.162216</td>\n      <td>0.115869</td>\n      <td>0.023806</td>\n      <td>0.252432</td>\n      <td>0.302656</td>\n      <td>0.148612</td>\n      <td>0.318297</td>\n      <td>0.202863</td>\n      <td>0.409158</td>\n      <td>0.149777</td>\n      <td>0.199306</td>\n      <td>0.350205</td>\n      <td>0.099427</td>\n      <td>0.174789</td>\n      <td>0.221588</td>\n      <td>0.404134</td>\n      <td>0.355257</td>\n      <td>0.316409</td>\n      <td>0.102576</td>\n      <td>0.359169</td>\n      <td>0.324771</td>\n      <td>0.412534</td>\n      <td>0.403529</td>\n      <td>0.397377</td>\n      <td>0.372978</td>\n      <td>0.404925</td>\n      <td>0.400429</td>\n      <td>0.562832</td>\n      <td>0.045384</td>\n      <td>0.082912</td>\n      <td>0.054517</td>\n      <td>0.063244</td>\n      <td>0.027145</td>\n      <td>0.013198</td>\n    </tr>\n    <tr>\n      <th>abs_percentile_70</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.698546</td>\n      <td>0.845754</td>\n      <td>0.757719</td>\n      <td>0.718500</td>\n      <td>0.609917</td>\n      <td>0.567524</td>\n      <td>0.046073</td>\n      <td>0.396211</td>\n      <td>0.034925</td>\n      <td>0.085507</td>\n      <td>0.010579</td>\n      <td>...</td>\n      <td>0.419422</td>\n      <td>0.285177</td>\n      <td>0.581352</td>\n      <td>0.480936</td>\n      <td>0.408559</td>\n      <td>0.329950</td>\n      <td>0.243375</td>\n      <td>0.150678</td>\n      <td>0.011449</td>\n      <td>0.383530</td>\n      <td>0.453470</td>\n      <td>0.200842</td>\n      <td>0.470024</td>\n      <td>0.282257</td>\n      <td>0.581057</td>\n      <td>0.187199</td>\n      <td>0.262894</td>\n      <td>0.481355</td>\n      <td>0.130704</td>\n      <td>0.211346</td>\n      <td>0.289363</td>\n      <td>0.573619</td>\n      <td>0.515145</td>\n      <td>0.470223</td>\n      <td>0.153335</td>\n      <td>0.520234</td>\n      <td>0.479760</td>\n      <td>0.584751</td>\n      <td>0.572855</td>\n      <td>0.565449</td>\n      <td>0.536564</td>\n      <td>0.574550</td>\n      <td>0.569095</td>\n      <td>0.390638</td>\n      <td>0.095048</td>\n      <td>0.129102</td>\n      <td>0.069814</td>\n      <td>0.107478</td>\n      <td>0.029810</td>\n      <td>0.007619</td>\n    </tr>\n    <tr>\n      <th>abs_percentile_75</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.802827</td>\n      <td>0.835293</td>\n      <td>0.795442</td>\n      <td>0.710721</td>\n      <td>0.670054</td>\n      <td>0.045487</td>\n      <td>0.349409</td>\n      <td>0.049092</td>\n      <td>0.089267</td>\n      <td>0.000508</td>\n      <td>...</td>\n      <td>0.526532</td>\n      <td>0.359723</td>\n      <td>0.568487</td>\n      <td>0.594396</td>\n      <td>0.513303</td>\n      <td>0.422355</td>\n      <td>0.295398</td>\n      <td>0.189351</td>\n      <td>0.023472</td>\n      <td>0.475971</td>\n      <td>0.558279</td>\n      <td>0.240682</td>\n      <td>0.579303</td>\n      <td>0.347352</td>\n      <td>0.681243</td>\n      <td>0.175569</td>\n      <td>0.279859</td>\n      <td>0.553906</td>\n      <td>0.151041</td>\n      <td>0.212962</td>\n      <td>0.327440</td>\n      <td>0.674062</td>\n      <td>0.621262</td>\n      <td>0.581595</td>\n      <td>0.206794</td>\n      <td>0.625618</td>\n      <td>0.590134</td>\n      <td>0.684516</td>\n      <td>0.674179</td>\n      <td>0.668023</td>\n      <td>0.643433</td>\n      <td>0.675655</td>\n      <td>0.671030</td>\n      <td>0.369217</td>\n      <td>0.114909</td>\n      <td>0.160418</td>\n      <td>0.092359</td>\n      <td>0.130862</td>\n      <td>0.035088</td>\n      <td>0.019058</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>spkt_welch_density_10</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.354444</td>\n      <td>0.925282</td>\n      <td>0.642689</td>\n      <td>0.928966</td>\n      <td>0.007269</td>\n      <td>0.290622</td>\n      <td>0.513415</td>\n      <td>0.163549</td>\n      <td>0.390810</td>\n      <td>0.615847</td>\n      <td>0.933119</td>\n      <td>0.946319</td>\n      <td>0.901924</td>\n      <td>0.289613</td>\n      <td>0.948855</td>\n      <td>0.932843</td>\n      <td>0.915584</td>\n      <td>0.926852</td>\n      <td>0.928567</td>\n      <td>0.934403</td>\n      <td>0.925998</td>\n      <td>0.927719</td>\n      <td>0.013735</td>\n      <td>0.279179</td>\n      <td>0.514821</td>\n      <td>0.191090</td>\n      <td>0.450722</td>\n      <td>0.128930</td>\n      <td>0.000227</td>\n    </tr>\n    <tr>\n      <th>spkt_welch_density_100</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.406997</td>\n      <td>0.521349</td>\n      <td>0.408618</td>\n      <td>0.023590</td>\n      <td>0.068661</td>\n      <td>0.274061</td>\n      <td>0.124626</td>\n      <td>0.340205</td>\n      <td>0.298487</td>\n      <td>0.406317</td>\n      <td>0.420022</td>\n      <td>0.397249</td>\n      <td>0.148116</td>\n      <td>0.394892</td>\n      <td>0.412328</td>\n      <td>0.408371</td>\n      <td>0.408019</td>\n      <td>0.408501</td>\n      <td>0.410960</td>\n      <td>0.408517</td>\n      <td>0.408290</td>\n      <td>0.040085</td>\n      <td>0.015431</td>\n      <td>0.241110</td>\n      <td>0.003824</td>\n      <td>0.151422</td>\n      <td>0.124437</td>\n      <td>0.009503</td>\n    </tr>\n    <tr>\n      <th>spkt_welch_density_5</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.593040</td>\n      <td>0.928726</td>\n      <td>0.017350</td>\n      <td>0.373943</td>\n      <td>0.614080</td>\n      <td>0.130884</td>\n      <td>0.261291</td>\n      <td>0.503611</td>\n      <td>0.932802</td>\n      <td>0.956413</td>\n      <td>0.940513</td>\n      <td>0.325333</td>\n      <td>0.943953</td>\n      <td>0.956431</td>\n      <td>0.914847</td>\n      <td>0.926269</td>\n      <td>0.928097</td>\n      <td>0.934813</td>\n      <td>0.925704</td>\n      <td>0.927200</td>\n      <td>0.024098</td>\n      <td>0.154641</td>\n      <td>0.362485</td>\n      <td>0.299100</td>\n      <td>0.257127</td>\n      <td>0.000167</td>\n      <td>0.007005</td>\n    </tr>\n    <tr>\n      <th>spkt_welch_density_50</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.646262</td>\n      <td>0.015754</td>\n      <td>0.127604</td>\n      <td>0.301725</td>\n      <td>0.219637</td>\n      <td>0.620120</td>\n      <td>0.583236</td>\n      <td>0.640921</td>\n      <td>0.655731</td>\n      <td>0.638451</td>\n      <td>0.207118</td>\n      <td>0.633898</td>\n      <td>0.667192</td>\n      <td>0.649270</td>\n      <td>0.646248</td>\n      <td>0.647350</td>\n      <td>0.652502</td>\n      <td>0.646577</td>\n      <td>0.646816</td>\n      <td>0.039840</td>\n      <td>0.148906</td>\n      <td>0.591300</td>\n      <td>0.183843</td>\n      <td>0.492558</td>\n      <td>0.273224</td>\n      <td>0.017061</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.063401</td>\n      <td>0.296776</td>\n      <td>0.626615</td>\n      <td>0.209639</td>\n      <td>0.387228</td>\n      <td>0.652401</td>\n      <td>0.998631</td>\n      <td>0.984058</td>\n      <td>0.918359</td>\n      <td>0.306635</td>\n      <td>0.984035</td>\n      <td>0.952652</td>\n      <td>0.997611</td>\n      <td>0.999523</td>\n      <td>0.999165</td>\n      <td>0.995918</td>\n      <td>0.999538</td>\n      <td>0.999368</td>\n      <td>0.016917</td>\n      <td>0.281815</td>\n      <td>0.457486</td>\n      <td>0.166300</td>\n      <td>0.399266</td>\n      <td>0.126990</td>\n      <td>0.006060</td>\n    </tr>\n    <tr>\n      <th>std_first_1000</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.158479</td>\n      <td>0.097312</td>\n      <td>0.020167</td>\n      <td>0.045310</td>\n      <td>0.024599</td>\n      <td>0.058938</td>\n      <td>0.039758</td>\n      <td>0.052671</td>\n      <td>0.024647</td>\n      <td>0.038430</td>\n      <td>0.041755</td>\n      <td>0.066716</td>\n      <td>0.058960</td>\n      <td>0.052487</td>\n      <td>0.036950</td>\n      <td>0.059934</td>\n      <td>0.055841</td>\n      <td>0.012640</td>\n      <td>0.001195</td>\n      <td>0.002093</td>\n      <td>0.010893</td>\n      <td>0.004337</td>\n      <td>0.006627</td>\n      <td>0.013414</td>\n    </tr>\n    <tr>\n      <th>std_first_10000</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.521925</td>\n      <td>0.028467</td>\n      <td>0.017953</td>\n      <td>0.033859</td>\n      <td>0.300562</td>\n      <td>0.286255</td>\n      <td>0.330316</td>\n      <td>0.113219</td>\n      <td>0.267135</td>\n      <td>0.311509</td>\n      <td>0.291118</td>\n      <td>0.295158</td>\n      <td>0.293214</td>\n      <td>0.278435</td>\n      <td>0.296144</td>\n      <td>0.294136</td>\n      <td>0.006948</td>\n      <td>0.137156</td>\n      <td>0.225566</td>\n      <td>0.099017</td>\n      <td>0.161156</td>\n      <td>0.082017</td>\n      <td>0.012084</td>\n    </tr>\n    <tr>\n      <th>std_first_50000</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.054732</td>\n      <td>0.054332</td>\n      <td>0.059390</td>\n      <td>0.613108</td>\n      <td>0.579529</td>\n      <td>0.554936</td>\n      <td>0.185339</td>\n      <td>0.566228</td>\n      <td>0.547816</td>\n      <td>0.639759</td>\n      <td>0.624144</td>\n      <td>0.622410</td>\n      <td>0.612164</td>\n      <td>0.625690</td>\n      <td>0.623144</td>\n      <td>0.022030</td>\n      <td>0.116354</td>\n      <td>0.106833</td>\n      <td>0.149089</td>\n      <td>0.077674</td>\n      <td>0.208708</td>\n      <td>0.015287</td>\n    </tr>\n    <tr>\n      <th>std_last_1000</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.376200</td>\n      <td>0.307934</td>\n      <td>0.208150</td>\n      <td>0.192765</td>\n      <td>0.165827</td>\n      <td>0.078736</td>\n      <td>0.192474</td>\n      <td>0.183802</td>\n      <td>0.214026</td>\n      <td>0.210469</td>\n      <td>0.204480</td>\n      <td>0.191247</td>\n      <td>0.210929</td>\n      <td>0.207810</td>\n      <td>0.021539</td>\n      <td>0.154905</td>\n      <td>0.232021</td>\n      <td>0.074254</td>\n      <td>0.233576</td>\n      <td>0.161521</td>\n      <td>0.007865</td>\n    </tr>\n    <tr>\n      <th>std_last_10000</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.577785</td>\n      <td>0.390333</td>\n      <td>0.365674</td>\n      <td>0.302829</td>\n      <td>0.093800</td>\n      <td>0.354263</td>\n      <td>0.338601</td>\n      <td>0.386793</td>\n      <td>0.389811</td>\n      <td>0.387431</td>\n      <td>0.371307</td>\n      <td>0.390629</td>\n      <td>0.388747</td>\n      <td>0.003838</td>\n      <td>0.007432</td>\n      <td>0.454867</td>\n      <td>0.221709</td>\n      <td>0.369657</td>\n      <td>0.326824</td>\n      <td>0.004560</td>\n    </tr>\n    <tr>\n      <th>std_last_50000</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.654052</td>\n      <td>0.648078</td>\n      <td>0.578918</td>\n      <td>0.177081</td>\n      <td>0.659417</td>\n      <td>0.630454</td>\n      <td>0.651456</td>\n      <td>0.654866</td>\n      <td>0.654396</td>\n      <td>0.652324</td>\n      <td>0.654202</td>\n      <td>0.654755</td>\n      <td>0.016292</td>\n      <td>0.444084</td>\n      <td>0.695719</td>\n      <td>0.165833</td>\n      <td>0.663942</td>\n      <td>0.599330</td>\n      <td>0.006726</td>\n    </tr>\n    <tr>\n      <th>std_roll_mean_10</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.986866</td>\n      <td>0.917173</td>\n      <td>0.307688</td>\n      <td>0.986955</td>\n      <td>0.952811</td>\n      <td>0.993150</td>\n      <td>0.998423</td>\n      <td>0.998261</td>\n      <td>0.995483</td>\n      <td>0.998319</td>\n      <td>0.998382</td>\n      <td>0.016803</td>\n      <td>0.268670</td>\n      <td>0.448037</td>\n      <td>0.184307</td>\n      <td>0.389447</td>\n      <td>0.122236</td>\n      <td>0.005566</td>\n    </tr>\n    <tr>\n      <th>std_roll_mean_100</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.952435</td>\n      <td>0.340113</td>\n      <td>0.997050</td>\n      <td>0.980579</td>\n      <td>0.974493</td>\n      <td>0.983053</td>\n      <td>0.984143</td>\n      <td>0.987470</td>\n      <td>0.982365</td>\n      <td>0.983661</td>\n      <td>0.019505</td>\n      <td>0.290604</td>\n      <td>0.482388</td>\n      <td>0.195899</td>\n      <td>0.416221</td>\n      <td>0.152359</td>\n      <td>0.006463</td>\n    </tr>\n    <tr>\n      <th>std_roll_mean_1000</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.498962</td>\n      <td>0.942083</td>\n      <td>0.990097</td>\n      <td>0.910741</td>\n      <td>0.914955</td>\n      <td>0.916366</td>\n      <td>0.923092</td>\n      <td>0.914201</td>\n      <td>0.915656</td>\n      <td>0.022106</td>\n      <td>0.297541</td>\n      <td>0.498957</td>\n      <td>0.111366</td>\n      <td>0.415198</td>\n      <td>0.171175</td>\n      <td>0.013276</td>\n    </tr>\n    <tr>\n      <th>std_roll_mean_10000</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.323744</td>\n      <td>0.426383</td>\n      <td>0.301837</td>\n      <td>0.304552</td>\n      <td>0.304933</td>\n      <td>0.307358</td>\n      <td>0.304275</td>\n      <td>0.304765</td>\n      <td>0.006956</td>\n      <td>0.083215</td>\n      <td>0.156861</td>\n      <td>0.039083</td>\n      <td>0.117143</td>\n      <td>0.071680</td>\n      <td>0.027480</td>\n    </tr>\n    <tr>\n      <th>std_roll_mean_50</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.972459</td>\n      <td>0.974661</td>\n      <td>0.983120</td>\n      <td>0.984077</td>\n      <td>0.986759</td>\n      <td>0.982297</td>\n      <td>0.983669</td>\n      <td>0.016671</td>\n      <td>0.315067</td>\n      <td>0.484448</td>\n      <td>0.189313</td>\n      <td>0.429201</td>\n      <td>0.164914</td>\n      <td>0.006651</td>\n    </tr>\n    <tr>\n      <th>std_roll_mean_500</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.944244</td>\n      <td>0.950149</td>\n      <td>0.951634</td>\n      <td>0.958155</td>\n      <td>0.949416</td>\n      <td>0.950906</td>\n      <td>0.021297</td>\n      <td>0.312030</td>\n      <td>0.519489</td>\n      <td>0.140992</td>\n      <td>0.442341</td>\n      <td>0.175787</td>\n      <td>0.008727</td>\n    </tr>\n    <tr>\n      <th>std_roll_std_10</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.997819</td>\n      <td>0.997309</td>\n      <td>0.993210</td>\n      <td>0.997988</td>\n      <td>0.997580</td>\n      <td>0.016263</td>\n      <td>0.298005</td>\n      <td>0.468689</td>\n      <td>0.140632</td>\n      <td>0.411654</td>\n      <td>0.135370</td>\n      <td>0.006432</td>\n    </tr>\n    <tr>\n      <th>std_roll_std_100</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.999864</td>\n      <td>0.996957</td>\n      <td>0.999988</td>\n      <td>0.999965</td>\n      <td>0.016960</td>\n      <td>0.282103</td>\n      <td>0.458281</td>\n      <td>0.166366</td>\n      <td>0.400482</td>\n      <td>0.128499</td>\n      <td>0.005888</td>\n    </tr>\n    <tr>\n      <th>std_roll_std_1000</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.997905</td>\n      <td>0.999805</td>\n      <td>0.999964</td>\n      <td>0.017308</td>\n      <td>0.282768</td>\n      <td>0.459521</td>\n      <td>0.167176</td>\n      <td>0.401572</td>\n      <td>0.129132</td>\n      <td>0.005835</td>\n    </tr>\n    <tr>\n      <th>std_roll_std_10000</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.996725</td>\n      <td>0.997450</td>\n      <td>0.018012</td>\n      <td>0.285385</td>\n      <td>0.464721</td>\n      <td>0.170405</td>\n      <td>0.405973</td>\n      <td>0.132385</td>\n      <td>0.005301</td>\n    </tr>\n    <tr>\n      <th>std_roll_std_50</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.999928</td>\n      <td>0.016968</td>\n      <td>0.280761</td>\n      <td>0.457369</td>\n      <td>0.165756</td>\n      <td>0.399325</td>\n      <td>0.127259</td>\n      <td>0.005876</td>\n    </tr>\n    <tr>\n      <th>std_roll_std_500</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.017116</td>\n      <td>0.282520</td>\n      <td>0.458991</td>\n      <td>0.166855</td>\n      <td>0.401125</td>\n      <td>0.128938</td>\n      <td>0.005825</td>\n    </tr>\n    <tr>\n      <th>sum</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.020818</td>\n      <td>0.007375</td>\n      <td>0.007641</td>\n      <td>0.012752</td>\n      <td>0.001782</td>\n      <td>0.012514</td>\n    </tr>\n    <tr>\n      <th>time_rev_asym_stat_1</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.643722</td>\n      <td>0.211928</td>\n      <td>0.850817</td>\n      <td>0.399768</td>\n      <td>0.012190</td>\n    </tr>\n    <tr>\n      <th>time_rev_asym_stat_10</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.387812</td>\n      <td>0.920571</td>\n      <td>0.713977</td>\n      <td>0.014756</td>\n    </tr>\n    <tr>\n      <th>time_rev_asym_stat_100</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.379945</td>\n      <td>0.617937</td>\n      <td>0.000204</td>\n    </tr>\n    <tr>\n      <th>time_rev_asym_stat_5</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.624341</td>\n      <td>0.019375</td>\n    </tr>\n    <tr>\n      <th>time_rev_asym_stat_50</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.003295</td>\n    </tr>\n    <tr>\n      <th>trend</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n<p>1419 rows  1419 columns</p>\n</div>"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"Find index of feature columns with correlation greater than 0.95"},{"metadata":{"trusted":true},"cell_type":"code","source":"to_drop = [column for column in upper.columns if any(upper[column] > 0.95)]\nto_drop","execution_count":7,"outputs":[{"output_type":"execute_result","execution_count":7,"data":{"text/plain":"['Hann_window_mean_1500',\n 'Hann_window_mean_15000',\n 'Hann_window_mean_50',\n 'abs_max_roll_mean_10',\n 'abs_max_roll_mean_100',\n 'abs_max_roll_mean_1000',\n 'abs_max_roll_mean_50',\n 'abs_max_roll_mean_500',\n 'abs_max_roll_std_10',\n 'abs_max_roll_std_100',\n 'abs_max_roll_std_1000',\n 'abs_max_roll_std_10000',\n 'abs_max_roll_std_50',\n 'abs_max_roll_std_500',\n 'abs_mean',\n 'abs_percentile_90',\n 'abs_percentile_95',\n 'abs_percentile_99',\n 'abs_std',\n 'av_change_abs_roll_std_50',\n 'av_change_rate_roll_std_10',\n 'av_change_rate_roll_std_100',\n 'av_change_rate_roll_std_1000',\n 'av_change_rate_roll_std_10000',\n 'av_change_rate_roll_std_50',\n 'av_change_rate_roll_std_500',\n 'ave10',\n 'ave_roll_mean_10',\n 'ave_roll_mean_100',\n 'ave_roll_mean_1000',\n 'ave_roll_mean_10000',\n 'ave_roll_mean_50',\n 'ave_roll_mean_500',\n 'ave_roll_std_10',\n 'ave_roll_std_100',\n 'ave_roll_std_1000',\n 'ave_roll_std_10000',\n 'ave_roll_std_50',\n 'ave_roll_std_500',\n 'binned_entropy_50',\n 'binned_entropy_60',\n 'binned_entropy_70',\n 'binned_entropy_75',\n 'binned_entropy_80',\n 'binned_entropy_90',\n 'binned_entropy_95',\n 'binned_entropy_99',\n 'count_big',\n 'count_big_100000_less_threshold_50',\n 'count_big_100000_threshold_10',\n 'count_big_100000_threshold_100',\n 'count_big_100000_threshold_20',\n 'count_big_100000_threshold_5',\n 'count_big_100000_threshold_50',\n 'count_big_150000_less_threshold_10',\n 'count_big_150000_less_threshold_100',\n 'count_big_150000_less_threshold_20',\n 'count_big_150000_less_threshold_5',\n 'count_big_150000_less_threshold_50',\n 'count_big_150000_threshold_10',\n 'count_big_150000_threshold_100',\n 'count_big_150000_threshold_20',\n 'count_big_150000_threshold_5',\n 'count_big_150000_threshold_50',\n 'count_big_50000_less_threshold_50',\n 'count_big_50000_threshold_10',\n 'count_big_50000_threshold_100',\n 'count_big_50000_threshold_20',\n 'count_big_50000_threshold_5',\n 'count_big_50000_threshold_50',\n 'exp_Moving_average_30000_mean',\n 'exp_Moving_average_3000_mean',\n 'exp_Moving_average_300_mean',\n 'exp_Moving_average_300_std',\n 'exp_Moving_average_50000_mean',\n 'exp_Moving_average_50000_std',\n 'exp_Moving_std_30000_mean',\n 'exp_Moving_std_30000_std',\n 'exp_Moving_std_3000_mean',\n 'exp_Moving_std_3000_std',\n 'exp_Moving_std_300_mean',\n 'exp_Moving_std_300_std',\n 'exp_Moving_std_50000_mean',\n 'exp_Moving_std_50000_std',\n 'ffti_Hann_window_mean_50',\n 'ffti_Hilbert_mean',\n 'ffti_abs_max',\n 'ffti_abs_max_roll_mean_500',\n 'ffti_abs_max_roll_std_10',\n 'ffti_abs_max_roll_std_100',\n 'ffti_abs_max_roll_std_1000',\n 'ffti_abs_max_roll_std_10000',\n 'ffti_abs_max_roll_std_50',\n 'ffti_abs_max_roll_std_500',\n 'ffti_abs_mean',\n 'ffti_abs_percentile_20',\n 'ffti_abs_percentile_25',\n 'ffti_abs_percentile_30',\n 'ffti_abs_percentile_40',\n 'ffti_abs_percentile_5',\n 'ffti_abs_percentile_50',\n 'ffti_abs_percentile_60',\n 'ffti_abs_percentile_70',\n 'ffti_abs_percentile_75',\n 'ffti_abs_percentile_80',\n 'ffti_abs_percentile_90',\n 'ffti_abs_percentile_95',\n 'ffti_abs_percentile_99',\n 'ffti_abs_std',\n 'ffti_abs_trend',\n 'ffti_av_change_abs_roll_mean_10',\n 'ffti_av_change_abs_roll_mean_100',\n 'ffti_av_change_abs_roll_mean_1000',\n 'ffti_av_change_abs_roll_mean_50',\n 'ffti_av_change_abs_roll_mean_500',\n 'ffti_av_change_rate_roll_mean_100',\n 'ffti_av_change_rate_roll_mean_1000',\n 'ffti_av_change_rate_roll_mean_10000',\n 'ffti_av_change_rate_roll_mean_50',\n 'ffti_av_change_rate_roll_mean_500',\n 'ffti_av_change_rate_roll_std_10000',\n 'ffti_ave_roll_mean_10',\n 'ffti_ave_roll_mean_100',\n 'ffti_ave_roll_mean_1000',\n 'ffti_ave_roll_mean_10000',\n 'ffti_ave_roll_mean_50',\n 'ffti_ave_roll_mean_500',\n 'ffti_ave_roll_std_10',\n 'ffti_ave_roll_std_100',\n 'ffti_ave_roll_std_1000',\n 'ffti_ave_roll_std_10000',\n 'ffti_ave_roll_std_50',\n 'ffti_ave_roll_std_500',\n 'ffti_binned_entropy_30',\n 'ffti_binned_entropy_40',\n 'ffti_binned_entropy_5',\n 'ffti_binned_entropy_50',\n 'ffti_binned_entropy_60',\n 'ffti_binned_entropy_70',\n 'ffti_binned_entropy_75',\n 'ffti_binned_entropy_80',\n 'ffti_binned_entropy_90',\n 'ffti_binned_entropy_95',\n 'ffti_binned_entropy_99',\n 'ffti_classic_sta_lta2_mean',\n 'ffti_classic_sta_lta4_mean',\n 'ffti_classic_sta_lta8_mean',\n 'ffti_count_big',\n 'ffti_count_big_100000_less_threshold_100',\n 'ffti_count_big_100000_threshold_10',\n 'ffti_count_big_100000_threshold_100',\n 'ffti_count_big_100000_threshold_20',\n 'ffti_count_big_100000_threshold_5',\n 'ffti_count_big_100000_threshold_50',\n 'ffti_count_big_150000_less_threshold_100',\n 'ffti_count_big_150000_less_threshold_20',\n 'ffti_count_big_150000_less_threshold_50',\n 'ffti_count_big_150000_threshold_10',\n 'ffti_count_big_150000_threshold_100',\n 'ffti_count_big_150000_threshold_20',\n 'ffti_count_big_150000_threshold_5',\n 'ffti_count_big_150000_threshold_50',\n 'ffti_count_big_50000_less_threshold_100',\n 'ffti_count_big_50000_less_threshold_50',\n 'ffti_count_big_50000_threshold_10',\n 'ffti_count_big_50000_threshold_100',\n 'ffti_count_big_50000_threshold_20',\n 'ffti_count_big_50000_threshold_5',\n 'ffti_count_big_50000_threshold_50',\n 'ffti_exp_Moving_average_3000_mean',\n 'ffti_exp_Moving_average_300_mean',\n 'ffti_exp_Moving_average_50000_mean',\n 'ffti_exp_Moving_average_50000_std',\n 'ffti_exp_Moving_std_30000_mean',\n 'ffti_exp_Moving_std_30000_std',\n 'ffti_exp_Moving_std_3000_mean',\n 'ffti_exp_Moving_std_3000_std',\n 'ffti_exp_Moving_std_300_mean',\n 'ffti_exp_Moving_std_300_std',\n 'ffti_exp_Moving_std_50000_mean',\n 'ffti_exp_Moving_std_50000_std',\n 'ffti_gmean',\n 'ffti_iqr',\n 'ffti_iqr1',\n 'ffti_kstat_2',\n 'ffti_kstat_4',\n 'ffti_kstatvar_1',\n 'ffti_kstatvar_2',\n 'ffti_mad',\n 'ffti_max',\n 'ffti_max_first_10000',\n 'ffti_max_first_50000',\n 'ffti_max_last_10000',\n 'ffti_max_last_50000',\n 'ffti_max_roll_mean_10',\n 'ffti_max_roll_mean_100',\n 'ffti_max_roll_mean_1000',\n 'ffti_max_roll_mean_10000',\n 'ffti_max_roll_mean_50',\n 'ffti_max_roll_mean_500',\n 'ffti_max_roll_std_10',\n 'ffti_max_roll_std_100',\n 'ffti_max_roll_std_1000',\n 'ffti_max_roll_std_10000',\n 'ffti_max_roll_std_50',\n 'ffti_max_roll_std_500',\n 'ffti_mean_change_rate_first_10000',\n 'ffti_mean_change_rate_last_50000',\n 'ffti_mean_first_1000',\n 'ffti_mean_first_10000',\n 'ffti_mean_last_1000',\n 'ffti_mean_last_10000',\n 'ffti_mean_last_50000',\n 'ffti_min',\n 'ffti_min_first_1000',\n 'ffti_min_first_10000',\n 'ffti_min_first_50000',\n 'ffti_min_last_1000',\n 'ffti_min_last_10000',\n 'ffti_min_last_50000',\n 'ffti_min_roll_mean_10',\n 'ffti_min_roll_mean_100',\n 'ffti_min_roll_mean_1000',\n 'ffti_min_roll_mean_10000',\n 'ffti_min_roll_mean_50',\n 'ffti_min_roll_mean_500',\n 'ffti_moment_2',\n 'ffti_moment_4',\n 'ffti_percentile_1',\n 'ffti_percentile_10',\n 'ffti_percentile_20',\n 'ffti_percentile_25',\n 'ffti_percentile_30',\n 'ffti_percentile_40',\n 'ffti_percentile_5',\n 'ffti_percentile_50',\n 'ffti_percentile_60',\n 'ffti_percentile_70',\n 'ffti_percentile_75',\n 'ffti_percentile_80',\n 'ffti_percentile_90',\n 'ffti_percentile_95',\n 'ffti_percentile_99',\n 'ffti_percentile_roll_mean_10_window_500',\n 'ffti_percentile_roll_mean_1_window_10',\n 'ffti_percentile_roll_mean_1_window_10000',\n 'ffti_percentile_roll_mean_1_window_500',\n 'ffti_percentile_roll_mean_25_window_10',\n 'ffti_percentile_roll_mean_25_window_100',\n 'ffti_percentile_roll_mean_25_window_1000',\n 'ffti_percentile_roll_mean_25_window_10000',\n 'ffti_percentile_roll_mean_25_window_50',\n 'ffti_percentile_roll_mean_25_window_500',\n 'ffti_percentile_roll_mean_30_window_10',\n 'ffti_percentile_roll_mean_30_window_100',\n 'ffti_percentile_roll_mean_30_window_1000',\n 'ffti_percentile_roll_mean_30_window_10000',\n 'ffti_percentile_roll_mean_30_window_50',\n 'ffti_percentile_roll_mean_30_window_500',\n 'ffti_percentile_roll_mean_40_window_10',\n 'ffti_percentile_roll_mean_40_window_100',\n 'ffti_percentile_roll_mean_40_window_50',\n 'ffti_percentile_roll_mean_5_window_10',\n 'ffti_percentile_roll_mean_5_window_100',\n 'ffti_percentile_roll_mean_5_window_10000',\n 'ffti_percentile_roll_mean_5_window_50',\n 'ffti_percentile_roll_mean_5_window_500',\n 'ffti_percentile_roll_mean_60_window_10',\n 'ffti_percentile_roll_mean_60_window_100',\n 'ffti_percentile_roll_mean_60_window_1000',\n 'ffti_percentile_roll_mean_60_window_10000',\n 'ffti_percentile_roll_mean_60_window_50',\n 'ffti_percentile_roll_mean_60_window_500',\n 'ffti_percentile_roll_mean_70_window_10',\n 'ffti_percentile_roll_mean_70_window_100',\n 'ffti_percentile_roll_mean_70_window_1000',\n 'ffti_percentile_roll_mean_70_window_10000',\n 'ffti_percentile_roll_mean_70_window_50',\n 'ffti_percentile_roll_mean_70_window_500',\n 'ffti_percentile_roll_mean_75_window_10',\n 'ffti_percentile_roll_mean_75_window_100',\n 'ffti_percentile_roll_mean_75_window_1000',\n 'ffti_percentile_roll_mean_75_window_10000',\n 'ffti_percentile_roll_mean_75_window_50',\n 'ffti_percentile_roll_mean_75_window_500',\n 'ffti_percentile_roll_mean_80_window_10',\n 'ffti_percentile_roll_mean_80_window_100',\n 'ffti_percentile_roll_mean_80_window_1000',\n 'ffti_percentile_roll_mean_80_window_10000',\n 'ffti_percentile_roll_mean_80_window_50',\n 'ffti_percentile_roll_mean_80_window_500',\n 'ffti_percentile_roll_mean_90_window_10',\n 'ffti_percentile_roll_mean_90_window_100',\n 'ffti_percentile_roll_mean_90_window_1000',\n 'ffti_percentile_roll_mean_90_window_10000',\n 'ffti_percentile_roll_mean_90_window_50',\n 'ffti_percentile_roll_mean_90_window_500',\n 'ffti_percentile_roll_mean_95_window_10',\n 'ffti_percentile_roll_mean_95_window_100',\n 'ffti_percentile_roll_mean_95_window_1000',\n 'ffti_percentile_roll_mean_95_window_10000',\n 'ffti_percentile_roll_mean_95_window_50',\n 'ffti_percentile_roll_mean_95_window_500',\n 'ffti_percentile_roll_mean_99_window_10',\n 'ffti_percentile_roll_mean_99_window_100',\n 'ffti_percentile_roll_mean_99_window_1000',\n 'ffti_percentile_roll_mean_99_window_10000',\n 'ffti_percentile_roll_mean_99_window_50',\n 'ffti_percentile_roll_mean_99_window_500',\n 'ffti_percentile_roll_std_10_window_50',\n 'ffti_percentile_roll_std_1_window_10000',\n 'ffti_percentile_roll_std_20_window_10',\n 'ffti_percentile_roll_std_20_window_50',\n 'ffti_percentile_roll_std_25_window_10',\n 'ffti_percentile_roll_std_25_window_50',\n 'ffti_percentile_roll_std_30_window_10',\n 'ffti_percentile_roll_std_30_window_10000',\n 'ffti_percentile_roll_std_30_window_50',\n 'ffti_percentile_roll_std_30_window_500',\n 'ffti_percentile_roll_std_40_window_10',\n 'ffti_percentile_roll_std_40_window_50',\n 'ffti_percentile_roll_std_40_window_500',\n 'ffti_percentile_roll_std_50_window_10',\n 'ffti_percentile_roll_std_50_window_50',\n 'ffti_percentile_roll_std_50_window_500',\n 'ffti_percentile_roll_std_5_window_10',\n 'ffti_percentile_roll_std_5_window_50',\n 'ffti_percentile_roll_std_5_window_500',\n 'ffti_percentile_roll_std_60_window_10',\n 'ffti_percentile_roll_std_60_window_100',\n 'ffti_percentile_roll_std_60_window_1000',\n 'ffti_percentile_roll_std_60_window_10000',\n 'ffti_percentile_roll_std_60_window_50',\n 'ffti_percentile_roll_std_60_window_500',\n 'ffti_percentile_roll_std_70_window_10',\n 'ffti_percentile_roll_std_70_window_100',\n 'ffti_percentile_roll_std_70_window_1000',\n 'ffti_percentile_roll_std_70_window_10000',\n 'ffti_percentile_roll_std_70_window_50',\n 'ffti_percentile_roll_std_70_window_500',\n 'ffti_percentile_roll_std_75_window_10',\n 'ffti_percentile_roll_std_75_window_100',\n 'ffti_percentile_roll_std_75_window_1000',\n 'ffti_percentile_roll_std_75_window_10000',\n 'ffti_percentile_roll_std_75_window_50',\n 'ffti_percentile_roll_std_75_window_500',\n 'ffti_percentile_roll_std_80_window_10',\n 'ffti_percentile_roll_std_80_window_100',\n 'ffti_percentile_roll_std_80_window_1000',\n 'ffti_percentile_roll_std_80_window_10000',\n 'ffti_percentile_roll_std_80_window_50',\n 'ffti_percentile_roll_std_80_window_500',\n 'ffti_percentile_roll_std_90_window_10',\n 'ffti_percentile_roll_std_90_window_100',\n 'ffti_percentile_roll_std_90_window_1000',\n 'ffti_percentile_roll_std_90_window_10000',\n 'ffti_percentile_roll_std_90_window_50',\n 'ffti_percentile_roll_std_90_window_500',\n 'ffti_percentile_roll_std_95_window_10',\n 'ffti_percentile_roll_std_95_window_100',\n 'ffti_percentile_roll_std_95_window_1000',\n 'ffti_percentile_roll_std_95_window_10000',\n 'ffti_percentile_roll_std_95_window_50',\n 'ffti_percentile_roll_std_95_window_500',\n 'ffti_percentile_roll_std_99_window_10',\n 'ffti_percentile_roll_std_99_window_100',\n 'ffti_percentile_roll_std_99_window_1000',\n 'ffti_percentile_roll_std_99_window_10000',\n 'ffti_percentile_roll_std_99_window_50',\n 'ffti_percentile_roll_std_99_window_500',\n 'ffti_range_-1000_0',\n 'ffti_range_0_1000',\n 'ffti_range_1000_2000',\n 'ffti_range_2000_3000',\n 'ffti_range_3000_4000',\n 'ffti_range_minf_m4000',\n 'ffti_range_p4000_pinf',\n 'ffti_std',\n 'ffti_std_first_1000',\n 'ffti_std_first_10000',\n 'ffti_std_first_50000',\n 'ffti_std_last_1000',\n 'ffti_std_last_10000',\n 'ffti_std_last_50000',\n 'ffti_std_roll_mean_10',\n 'ffti_std_roll_mean_100',\n 'ffti_std_roll_mean_1000',\n 'ffti_std_roll_mean_10000',\n 'ffti_std_roll_mean_50',\n 'ffti_std_roll_mean_500',\n 'ffti_std_roll_std_10',\n 'ffti_std_roll_std_100',\n 'ffti_std_roll_std_1000',\n 'ffti_std_roll_std_10000',\n 'ffti_std_roll_std_50',\n 'ffti_std_roll_std_500',\n 'ffti_sum',\n 'ffti_time_rev_asym_stat_100',\n 'ffti_time_rev_asym_stat_5',\n 'ffti_time_rev_asym_stat_50',\n 'ffti_trend',\n 'fftr_Hann_window_mean_1500',\n 'fftr_Hann_window_mean_15000',\n 'fftr_Hann_window_mean_50',\n 'fftr_Hilbert_mean',\n 'fftr_abs_max',\n 'fftr_abs_max_roll_std_1000',\n 'fftr_abs_max_roll_std_10000',\n 'fftr_abs_max_roll_std_50',\n 'fftr_abs_max_roll_std_500',\n 'fftr_abs_mean',\n 'fftr_abs_percentile_10',\n 'fftr_abs_percentile_20',\n 'fftr_abs_percentile_25',\n 'fftr_abs_percentile_30',\n 'fftr_abs_percentile_40',\n 'fftr_abs_percentile_5',\n 'fftr_abs_percentile_50',\n 'fftr_abs_percentile_60',\n 'fftr_abs_percentile_70',\n 'fftr_abs_percentile_75',\n 'fftr_abs_percentile_80',\n 'fftr_abs_percentile_90',\n 'fftr_abs_percentile_95',\n 'fftr_abs_percentile_99',\n 'fftr_abs_std',\n 'fftr_abs_trend',\n 'fftr_av_change_abs_roll_mean_10',\n 'fftr_av_change_abs_roll_mean_100',\n 'fftr_av_change_abs_roll_mean_1000',\n 'fftr_av_change_abs_roll_mean_50',\n 'fftr_av_change_abs_roll_mean_500',\n 'fftr_av_change_abs_roll_std_10',\n 'fftr_av_change_abs_roll_std_100',\n 'fftr_av_change_abs_roll_std_50',\n 'fftr_av_change_abs_roll_std_500',\n 'fftr_av_change_rate_roll_std_10000',\n 'fftr_ave_roll_mean_10',\n 'fftr_ave_roll_mean_100',\n 'fftr_ave_roll_mean_1000',\n 'fftr_ave_roll_mean_10000',\n 'fftr_ave_roll_mean_50',\n 'fftr_ave_roll_mean_500',\n 'fftr_ave_roll_std_10',\n 'fftr_ave_roll_std_100',\n 'fftr_ave_roll_std_1000',\n 'fftr_ave_roll_std_10000',\n 'fftr_ave_roll_std_50',\n 'fftr_ave_roll_std_500',\n 'fftr_c3_10',\n 'fftr_c3_5',\n 'fftr_c3_50',\n 'fftr_c3_500',\n 'fftr_classic_sta_lta1_mean',\n 'fftr_classic_sta_lta2_mean',\n 'fftr_classic_sta_lta4_mean',\n 'fftr_classic_sta_lta5_mean',\n 'fftr_classic_sta_lta6_mean',\n 'fftr_classic_sta_lta7_mean',\n 'fftr_classic_sta_lta8_mean',\n 'fftr_count_big',\n 'fftr_count_big_100000_less_threshold_100',\n 'fftr_count_big_100000_threshold_10',\n 'fftr_count_big_100000_threshold_100',\n 'fftr_count_big_100000_threshold_20',\n 'fftr_count_big_100000_threshold_5',\n 'fftr_count_big_100000_threshold_50',\n 'fftr_count_big_150000_less_threshold_10',\n 'fftr_count_big_150000_less_threshold_100',\n 'fftr_count_big_150000_less_threshold_20',\n 'fftr_count_big_150000_less_threshold_50',\n 'fftr_count_big_150000_threshold_10',\n 'fftr_count_big_150000_threshold_100',\n 'fftr_count_big_150000_threshold_20',\n 'fftr_count_big_150000_threshold_5',\n 'fftr_count_big_150000_threshold_50',\n 'fftr_count_big_50000_less_threshold_100',\n 'fftr_count_big_50000_less_threshold_50',\n 'fftr_count_big_50000_threshold_10',\n 'fftr_count_big_50000_threshold_100',\n 'fftr_count_big_50000_threshold_20',\n 'fftr_count_big_50000_threshold_5',\n 'fftr_count_big_50000_threshold_50',\n 'fftr_exp_Moving_average_30000_std',\n 'fftr_exp_Moving_average_3000_mean',\n 'fftr_exp_Moving_average_3000_std',\n 'fftr_exp_Moving_average_300_mean',\n 'fftr_exp_Moving_average_300_std',\n 'fftr_exp_Moving_average_50000_mean',\n 'fftr_exp_Moving_average_50000_std',\n 'fftr_exp_Moving_std_30000_mean',\n 'fftr_exp_Moving_std_30000_std',\n 'fftr_exp_Moving_std_3000_mean',\n 'fftr_exp_Moving_std_3000_std',\n 'fftr_exp_Moving_std_300_mean',\n 'fftr_exp_Moving_std_300_std',\n 'fftr_exp_Moving_std_50000_mean',\n 'fftr_exp_Moving_std_50000_std',\n 'fftr_gmean',\n 'fftr_iqr',\n 'fftr_iqr1',\n 'fftr_kstat_1',\n 'fftr_kstat_2',\n 'fftr_kstat_4',\n 'fftr_kstatvar_1',\n 'fftr_kstatvar_2',\n 'fftr_mad',\n 'fftr_max',\n 'fftr_max_first_1000',\n 'fftr_max_first_10000',\n 'fftr_max_first_50000',\n 'fftr_max_last_10000',\n 'fftr_max_last_50000',\n 'fftr_max_roll_mean_10',\n 'fftr_max_roll_mean_100',\n 'fftr_max_roll_mean_50',\n 'fftr_max_roll_std_10',\n 'fftr_max_roll_std_100',\n 'fftr_max_roll_std_1000',\n 'fftr_max_roll_std_10000',\n 'fftr_max_roll_std_50',\n 'fftr_max_roll_std_500',\n 'fftr_max_to_min_diff',\n 'fftr_mean',\n 'fftr_mean_change_abs',\n 'fftr_mean_change_rate_last_50000',\n 'fftr_mean_first_50000',\n 'fftr_mean_last_10000',\n 'fftr_mean_last_50000',\n 'fftr_min',\n 'fftr_min_first_10000',\n 'fftr_min_first_50000',\n 'fftr_min_last_1000',\n 'fftr_min_last_10000',\n 'fftr_min_last_50000',\n 'fftr_min_roll_mean_10',\n 'fftr_min_roll_mean_100',\n 'fftr_min_roll_mean_50',\n 'fftr_min_roll_mean_500',\n 'fftr_moment_2',\n 'fftr_moment_3',\n 'fftr_moment_4',\n 'fftr_num_crossing_0',\n 'fftr_percentile_1',\n 'fftr_percentile_10',\n 'fftr_percentile_20',\n 'fftr_percentile_25',\n 'fftr_percentile_30',\n 'fftr_percentile_5',\n 'fftr_percentile_50',\n 'fftr_percentile_75',\n 'fftr_percentile_80',\n 'fftr_percentile_90',\n 'fftr_percentile_95',\n 'fftr_percentile_99',\n 'fftr_percentile_roll_mean_10_window_10',\n 'fftr_percentile_roll_mean_10_window_500',\n 'fftr_percentile_roll_mean_1_window_10',\n 'fftr_percentile_roll_mean_1_window_100',\n 'fftr_percentile_roll_mean_1_window_1000',\n 'fftr_percentile_roll_mean_1_window_10000',\n 'fftr_percentile_roll_mean_1_window_50',\n 'fftr_percentile_roll_mean_1_window_500',\n 'fftr_percentile_roll_mean_20_window_500',\n 'fftr_percentile_roll_mean_25_window_10',\n 'fftr_percentile_roll_mean_25_window_100',\n 'fftr_percentile_roll_mean_25_window_1000',\n 'fftr_percentile_roll_mean_25_window_10000',\n 'fftr_percentile_roll_mean_25_window_50',\n 'fftr_percentile_roll_mean_25_window_500',\n 'fftr_percentile_roll_mean_30_window_10',\n 'fftr_percentile_roll_mean_30_window_100',\n 'fftr_percentile_roll_mean_30_window_1000',\n 'fftr_percentile_roll_mean_30_window_10000',\n 'fftr_percentile_roll_mean_30_window_50',\n 'fftr_percentile_roll_mean_30_window_500',\n 'fftr_percentile_roll_mean_40_window_100',\n 'fftr_percentile_roll_mean_40_window_1000',\n 'fftr_percentile_roll_mean_40_window_10000',\n 'fftr_percentile_roll_mean_40_window_50',\n 'fftr_percentile_roll_mean_40_window_500',\n 'fftr_percentile_roll_mean_50_window_10',\n 'fftr_percentile_roll_mean_50_window_100',\n 'fftr_percentile_roll_mean_50_window_1000',\n 'fftr_percentile_roll_mean_50_window_10000',\n 'fftr_percentile_roll_mean_50_window_50',\n 'fftr_percentile_roll_mean_50_window_500',\n 'fftr_percentile_roll_mean_5_window_10',\n 'fftr_percentile_roll_mean_5_window_100',\n 'fftr_percentile_roll_mean_5_window_50',\n 'fftr_percentile_roll_mean_5_window_500',\n 'fftr_percentile_roll_mean_60_window_100',\n 'fftr_percentile_roll_mean_60_window_1000',\n 'fftr_percentile_roll_mean_60_window_10000',\n 'fftr_percentile_roll_mean_60_window_50',\n 'fftr_percentile_roll_mean_60_window_500',\n 'fftr_percentile_roll_mean_70_window_100',\n 'fftr_percentile_roll_mean_70_window_1000',\n 'fftr_percentile_roll_mean_70_window_10000',\n 'fftr_percentile_roll_mean_70_window_50',\n 'fftr_percentile_roll_mean_70_window_500',\n 'fftr_percentile_roll_mean_75_window_10',\n 'fftr_percentile_roll_mean_75_window_100',\n 'fftr_percentile_roll_mean_75_window_1000',\n 'fftr_percentile_roll_mean_75_window_10000',\n 'fftr_percentile_roll_mean_75_window_50',\n 'fftr_percentile_roll_mean_75_window_500',\n 'fftr_percentile_roll_mean_80_window_10',\n 'fftr_percentile_roll_mean_80_window_100',\n 'fftr_percentile_roll_mean_80_window_1000',\n 'fftr_percentile_roll_mean_80_window_10000',\n 'fftr_percentile_roll_mean_80_window_50',\n 'fftr_percentile_roll_mean_80_window_500',\n 'fftr_percentile_roll_mean_90_window_10',\n 'fftr_percentile_roll_mean_90_window_500',\n 'fftr_percentile_roll_mean_95_window_10',\n 'fftr_percentile_roll_mean_95_window_100',\n 'fftr_percentile_roll_mean_95_window_50',\n 'fftr_percentile_roll_mean_95_window_500',\n 'fftr_percentile_roll_mean_99_window_10',\n 'fftr_percentile_roll_mean_99_window_100',\n 'fftr_percentile_roll_mean_99_window_10000',\n 'fftr_percentile_roll_mean_99_window_50',\n 'fftr_percentile_roll_mean_99_window_500',\n 'fftr_percentile_roll_std_10_window_50',\n 'fftr_percentile_roll_std_1_window_10000',\n 'fftr_percentile_roll_std_20_window_10',\n 'fftr_percentile_roll_std_20_window_50',\n 'fftr_percentile_roll_std_25_window_10',\n 'fftr_percentile_roll_std_25_window_50',\n 'fftr_percentile_roll_std_30_window_10',\n 'fftr_percentile_roll_std_30_window_50',\n 'fftr_percentile_roll_std_30_window_500',\n 'fftr_percentile_roll_std_40_window_10',\n 'fftr_percentile_roll_std_40_window_50',\n 'fftr_percentile_roll_std_40_window_500',\n 'fftr_percentile_roll_std_50_window_10',\n 'fftr_percentile_roll_std_50_window_10000',\n 'fftr_percentile_roll_std_50_window_50',\n 'fftr_percentile_roll_std_50_window_500',\n 'fftr_percentile_roll_std_5_window_10',\n 'fftr_percentile_roll_std_5_window_50',\n 'fftr_percentile_roll_std_5_window_500',\n 'fftr_percentile_roll_std_60_window_10',\n 'fftr_percentile_roll_std_60_window_100',\n 'fftr_percentile_roll_std_60_window_1000',\n 'fftr_percentile_roll_std_60_window_10000',\n 'fftr_percentile_roll_std_60_window_50',\n 'fftr_percentile_roll_std_60_window_500',\n 'fftr_percentile_roll_std_70_window_10',\n 'fftr_percentile_roll_std_70_window_100',\n 'fftr_percentile_roll_std_70_window_1000',\n 'fftr_percentile_roll_std_70_window_10000',\n 'fftr_percentile_roll_std_70_window_50',\n 'fftr_percentile_roll_std_70_window_500',\n 'fftr_percentile_roll_std_75_window_10',\n 'fftr_percentile_roll_std_75_window_100',\n 'fftr_percentile_roll_std_75_window_1000',\n 'fftr_percentile_roll_std_75_window_10000',\n 'fftr_percentile_roll_std_75_window_50',\n 'fftr_percentile_roll_std_75_window_500',\n 'fftr_percentile_roll_std_80_window_10',\n 'fftr_percentile_roll_std_80_window_100',\n 'fftr_percentile_roll_std_80_window_1000',\n 'fftr_percentile_roll_std_80_window_10000',\n 'fftr_percentile_roll_std_80_window_50',\n 'fftr_percentile_roll_std_80_window_500',\n 'fftr_percentile_roll_std_90_window_10',\n 'fftr_percentile_roll_std_90_window_100',\n 'fftr_percentile_roll_std_90_window_1000',\n 'fftr_percentile_roll_std_90_window_10000',\n 'fftr_percentile_roll_std_90_window_50',\n 'fftr_percentile_roll_std_90_window_500',\n 'fftr_percentile_roll_std_95_window_10',\n 'fftr_percentile_roll_std_95_window_100',\n 'fftr_percentile_roll_std_95_window_1000',\n 'fftr_percentile_roll_std_95_window_10000',\n 'fftr_percentile_roll_std_95_window_50',\n 'fftr_percentile_roll_std_95_window_500',\n 'fftr_percentile_roll_std_99_window_10',\n 'fftr_percentile_roll_std_99_window_100',\n 'fftr_percentile_roll_std_99_window_1000',\n 'fftr_percentile_roll_std_99_window_10000',\n 'fftr_percentile_roll_std_99_window_50',\n 'fftr_percentile_roll_std_99_window_500',\n 'fftr_range_-1000_0',\n 'fftr_range_-4000_-3000',\n 'fftr_range_0_1000',\n 'fftr_range_2000_3000',\n 'fftr_range_3000_4000',\n 'fftr_range_minf_m4000',\n 'fftr_range_p4000_pinf',\n 'fftr_skew',\n 'fftr_spkt_welch_density_1',\n 'fftr_spkt_welch_density_10',\n 'fftr_spkt_welch_density_100',\n 'fftr_spkt_welch_density_5',\n 'fftr_spkt_welch_density_50',\n 'fftr_std',\n 'fftr_std_first_1000',\n 'fftr_std_first_10000',\n 'fftr_std_first_50000',\n 'fftr_std_last_1000',\n 'fftr_std_last_10000',\n 'fftr_std_last_50000',\n 'fftr_std_roll_mean_10',\n 'fftr_std_roll_mean_100',\n 'fftr_std_roll_mean_1000',\n 'fftr_std_roll_mean_50',\n 'fftr_std_roll_mean_500',\n 'fftr_std_roll_std_10',\n 'fftr_std_roll_std_100',\n 'fftr_std_roll_std_1000',\n 'fftr_std_roll_std_10000',\n 'fftr_std_roll_std_50',\n 'fftr_std_roll_std_500',\n 'fftr_sum',\n 'fftr_trend',\n 'hmean',\n 'iqr1',\n 'kstat_1',\n 'kstat_2',\n 'kstat_4',\n 'kstatvar_1',\n 'kstatvar_2',\n 'mad',\n 'max',\n 'max_last_50000',\n 'max_roll_mean_10',\n 'max_roll_mean_100',\n 'max_roll_mean_1000',\n 'max_roll_mean_10000',\n 'max_roll_mean_50',\n 'max_roll_mean_500',\n 'max_roll_std_10',\n 'max_roll_std_100',\n 'max_roll_std_1000',\n 'max_roll_std_10000',\n 'max_roll_std_50',\n 'max_roll_std_500',\n 'mean',\n 'mean_first_50000',\n 'min',\n 'min_first_1000',\n 'min_first_10000',\n 'min_first_50000',\n 'min_last_1000',\n 'min_last_10000',\n 'min_last_50000',\n 'min_roll_mean_10',\n 'min_roll_mean_100',\n 'min_roll_mean_1000',\n 'min_roll_mean_10000',\n 'min_roll_mean_50',\n 'min_roll_mean_500',\n 'moment_2',\n 'moment_3',\n 'moment_4',\n 'percentile_1',\n 'percentile_5',\n 'percentile_50',\n 'percentile_95',\n 'percentile_99',\n 'percentile_roll_mean_10_window_10',\n 'percentile_roll_mean_10_window_100',\n 'percentile_roll_mean_10_window_1000',\n 'percentile_roll_mean_10_window_10000',\n 'percentile_roll_mean_10_window_50',\n 'percentile_roll_mean_10_window_500',\n 'percentile_roll_mean_1_window_10',\n 'percentile_roll_mean_1_window_100',\n 'percentile_roll_mean_1_window_10000',\n 'percentile_roll_mean_1_window_50',\n 'percentile_roll_mean_1_window_500',\n 'percentile_roll_mean_20_window_100',\n 'percentile_roll_mean_20_window_1000',\n 'percentile_roll_mean_20_window_10000',\n 'percentile_roll_mean_20_window_50',\n 'percentile_roll_mean_20_window_500',\n 'percentile_roll_mean_25_window_10',\n 'percentile_roll_mean_25_window_100',\n 'percentile_roll_mean_25_window_1000',\n 'percentile_roll_mean_25_window_10000',\n 'percentile_roll_mean_25_window_50',\n 'percentile_roll_mean_25_window_500',\n 'percentile_roll_mean_30_window_10',\n 'percentile_roll_mean_30_window_100',\n 'percentile_roll_mean_30_window_1000',\n 'percentile_roll_mean_30_window_10000',\n 'percentile_roll_mean_30_window_50',\n 'percentile_roll_mean_30_window_500',\n 'percentile_roll_mean_40_window_10',\n 'percentile_roll_mean_40_window_100',\n 'percentile_roll_mean_40_window_1000',\n 'percentile_roll_mean_40_window_10000',\n 'percentile_roll_mean_40_window_50',\n 'percentile_roll_mean_40_window_500',\n 'percentile_roll_mean_50_window_10',\n 'percentile_roll_mean_50_window_100',\n 'percentile_roll_mean_50_window_1000',\n 'percentile_roll_mean_50_window_10000',\n 'percentile_roll_mean_50_window_50',\n 'percentile_roll_mean_50_window_500',\n 'percentile_roll_mean_5_window_10',\n 'percentile_roll_mean_5_window_100',\n 'percentile_roll_mean_5_window_1000',\n 'percentile_roll_mean_5_window_10000',\n 'percentile_roll_mean_5_window_50',\n 'percentile_roll_mean_5_window_500',\n 'percentile_roll_mean_60_window_10',\n 'percentile_roll_mean_60_window_100',\n 'percentile_roll_mean_60_window_1000',\n 'percentile_roll_mean_60_window_10000',\n 'percentile_roll_mean_60_window_50',\n 'percentile_roll_mean_60_window_500',\n 'percentile_roll_mean_70_window_10',\n 'percentile_roll_mean_70_window_100',\n 'percentile_roll_mean_70_window_1000',\n 'percentile_roll_mean_70_window_10000',\n 'percentile_roll_mean_70_window_50',\n 'percentile_roll_mean_70_window_500',\n 'percentile_roll_mean_75_window_10',\n 'percentile_roll_mean_75_window_100',\n 'percentile_roll_mean_75_window_1000',\n 'percentile_roll_mean_75_window_10000',\n 'percentile_roll_mean_75_window_50',\n 'percentile_roll_mean_75_window_500',\n 'percentile_roll_mean_80_window_10',\n 'percentile_roll_mean_80_window_100',\n 'percentile_roll_mean_80_window_1000',\n 'percentile_roll_mean_80_window_10000',\n 'percentile_roll_mean_80_window_50',\n 'percentile_roll_mean_80_window_500',\n 'percentile_roll_mean_90_window_10',\n 'percentile_roll_mean_90_window_100',\n 'percentile_roll_mean_90_window_1000',\n 'percentile_roll_mean_90_window_10000',\n 'percentile_roll_mean_90_window_50',\n 'percentile_roll_mean_90_window_500',\n 'percentile_roll_mean_95_window_10',\n 'percentile_roll_mean_95_window_100',\n 'percentile_roll_mean_95_window_1000',\n 'percentile_roll_mean_95_window_10000',\n 'percentile_roll_mean_95_window_50',\n 'percentile_roll_mean_95_window_500',\n 'percentile_roll_mean_99_window_10',\n 'percentile_roll_mean_99_window_100',\n 'percentile_roll_mean_99_window_1000',\n 'percentile_roll_mean_99_window_10000',\n 'percentile_roll_mean_99_window_50',\n 'percentile_roll_mean_99_window_500',\n 'percentile_roll_std_10_window_10',\n 'percentile_roll_std_10_window_100',\n 'percentile_roll_std_10_window_1000',\n 'percentile_roll_std_10_window_50',\n 'percentile_roll_std_10_window_500',\n 'percentile_roll_std_1_window_10',\n 'percentile_roll_std_1_window_100',\n 'percentile_roll_std_1_window_10000',\n 'percentile_roll_std_1_window_50',\n 'percentile_roll_std_1_window_500',\n 'percentile_roll_std_20_window_10',\n 'percentile_roll_std_20_window_100',\n 'percentile_roll_std_20_window_1000',\n 'percentile_roll_std_20_window_10000',\n 'percentile_roll_std_20_window_50',\n 'percentile_roll_std_20_window_500',\n 'percentile_roll_std_25_window_10',\n 'percentile_roll_std_25_window_100',\n 'percentile_roll_std_25_window_1000',\n 'percentile_roll_std_25_window_10000',\n 'percentile_roll_std_25_window_50',\n 'percentile_roll_std_25_window_500',\n 'percentile_roll_std_30_window_10',\n 'percentile_roll_std_30_window_100',\n 'percentile_roll_std_30_window_1000',\n 'percentile_roll_std_30_window_10000',\n 'percentile_roll_std_30_window_50',\n 'percentile_roll_std_30_window_500',\n 'percentile_roll_std_40_window_10',\n 'percentile_roll_std_40_window_100',\n 'percentile_roll_std_40_window_1000',\n 'percentile_roll_std_40_window_10000',\n 'percentile_roll_std_40_window_50',\n 'percentile_roll_std_40_window_500',\n 'percentile_roll_std_50_window_10',\n 'percentile_roll_std_50_window_100',\n 'percentile_roll_std_50_window_1000',\n 'percentile_roll_std_50_window_10000',\n 'percentile_roll_std_50_window_50',\n 'percentile_roll_std_50_window_500',\n 'percentile_roll_std_5_window_10',\n 'percentile_roll_std_5_window_100',\n 'percentile_roll_std_5_window_1000',\n 'percentile_roll_std_5_window_10000',\n 'percentile_roll_std_5_window_50',\n 'percentile_roll_std_5_window_500',\n 'percentile_roll_std_60_window_10',\n 'percentile_roll_std_60_window_100',\n 'percentile_roll_std_60_window_1000',\n 'percentile_roll_std_60_window_10000',\n 'percentile_roll_std_60_window_50',\n 'percentile_roll_std_60_window_500',\n 'percentile_roll_std_70_window_10',\n 'percentile_roll_std_70_window_100',\n 'percentile_roll_std_70_window_1000',\n 'percentile_roll_std_70_window_10000',\n 'percentile_roll_std_70_window_50',\n 'percentile_roll_std_70_window_500',\n 'percentile_roll_std_75_window_10',\n 'percentile_roll_std_75_window_100',\n 'percentile_roll_std_75_window_1000',\n 'percentile_roll_std_75_window_10000',\n 'percentile_roll_std_75_window_50',\n 'percentile_roll_std_75_window_500',\n 'percentile_roll_std_80_window_10',\n 'percentile_roll_std_80_window_100',\n 'percentile_roll_std_80_window_1000',\n 'percentile_roll_std_80_window_50',\n 'percentile_roll_std_80_window_500',\n 'percentile_roll_std_90_window_10',\n 'percentile_roll_std_90_window_100',\n 'percentile_roll_std_90_window_1000',\n 'percentile_roll_std_90_window_50',\n 'percentile_roll_std_90_window_500',\n 'percentile_roll_std_95_window_10',\n 'percentile_roll_std_95_window_100',\n 'percentile_roll_std_95_window_1000',\n 'percentile_roll_std_95_window_10000',\n 'percentile_roll_std_95_window_50',\n 'percentile_roll_std_95_window_500',\n 'percentile_roll_std_99_window_10',\n 'percentile_roll_std_99_window_100',\n 'percentile_roll_std_99_window_1000',\n 'percentile_roll_std_99_window_10000',\n 'percentile_roll_std_99_window_50',\n 'percentile_roll_std_99_window_500',\n 'range_-2000_-1000',\n 'range_-3000_-2000',\n 'range_0_1000',\n 'range_1000_2000',\n 'range_2000_3000',\n 'spkt_welch_density_1',\n 'spkt_welch_density_10',\n 'spkt_welch_density_5',\n 'std',\n 'std_first_1000',\n 'std_first_10000',\n 'std_first_50000',\n 'std_last_1000',\n 'std_last_10000',\n 'std_last_50000',\n 'std_roll_mean_10',\n 'std_roll_mean_100',\n 'std_roll_mean_1000',\n 'std_roll_mean_50',\n 'std_roll_mean_500',\n 'std_roll_std_10',\n 'std_roll_std_100',\n 'std_roll_std_1000',\n 'std_roll_std_10000',\n 'std_roll_std_50',\n 'std_roll_std_500',\n 'sum']"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"Drop columns with correlation greater than 0.95"},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train_corr = x_train.drop(to_drop, axis=1)\nx_valid_corr = x_valid.drop(to_drop, axis=1)\nX_test_scaled_corr = X_test_scaled.drop(to_drop, axis=1)","execution_count":8,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Training Shape\", x_train_corr.shape)\nprint(\"Validating Shape\", x_valid_corr.shape)\nprint(\"Testing Shape\", X_test_scaled_corr.shape)","execution_count":9,"outputs":[{"output_type":"stream","text":"Training Shape (3356, 455)\nValidating Shape (839, 455)\nTesting Shape (2624, 455)\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":" <h1>Recursive feature elimination (RFE) with random forest</h1>"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nrf = RandomForestRegressor(n_estimators=2)\nrfe = RFE(estimator=rf, n_features_to_select=100, step=1, verbose=10)\nrfe = rfe.fit(x_train_corr, y_train.values)\nprint('Chosen best 100 feature by rfe:',x_train_corr.columns[rfe.support_])","execution_count":10,"outputs":[{"output_type":"stream","text":"Fitting estimator with 455 features.\n","name":"stdout"},{"output_type":"stream","text":"/opt/conda/lib/python3.6/site-packages/sklearn/utils/validation.py:761: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  y = column_or_1d(y, warn=True)\n","name":"stderr"},{"output_type":"stream","text":"Fitting estimator with 454 features.\nFitting estimator with 453 features.\nFitting estimator with 452 features.\nFitting estimator with 451 features.\nFitting estimator with 450 features.\nFitting estimator with 449 features.\nFitting estimator with 448 features.\nFitting estimator with 447 features.\nFitting estimator with 446 features.\nFitting estimator with 445 features.\nFitting estimator with 444 features.\nFitting estimator with 443 features.\nFitting estimator with 442 features.\nFitting estimator with 441 features.\nFitting estimator with 440 features.\nFitting estimator with 439 features.\nFitting estimator with 438 features.\nFitting estimator with 437 features.\nFitting estimator with 436 features.\nFitting estimator with 435 features.\nFitting estimator with 434 features.\nFitting estimator with 433 features.\nFitting estimator with 432 features.\nFitting estimator with 431 features.\nFitting estimator with 430 features.\nFitting estimator with 429 features.\nFitting estimator with 428 features.\nFitting estimator with 427 features.\nFitting estimator with 426 features.\nFitting estimator with 425 features.\nFitting estimator with 424 features.\nFitting estimator with 423 features.\nFitting estimator with 422 features.\nFitting estimator with 421 features.\nFitting estimator with 420 features.\nFitting estimator with 419 features.\nFitting estimator with 418 features.\nFitting estimator with 417 features.\nFitting estimator with 416 features.\nFitting estimator with 415 features.\nFitting estimator with 414 features.\nFitting estimator with 413 features.\nFitting estimator with 412 features.\nFitting estimator with 411 features.\nFitting estimator with 410 features.\nFitting estimator with 409 features.\nFitting estimator with 408 features.\nFitting estimator with 407 features.\nFitting estimator with 406 features.\nFitting estimator with 405 features.\nFitting estimator with 404 features.\nFitting estimator with 403 features.\nFitting estimator with 402 features.\nFitting estimator with 401 features.\nFitting estimator with 400 features.\nFitting estimator with 399 features.\nFitting estimator with 398 features.\nFitting estimator with 397 features.\nFitting estimator with 396 features.\nFitting estimator with 395 features.\nFitting estimator with 394 features.\nFitting estimator with 393 features.\nFitting estimator with 392 features.\nFitting estimator with 391 features.\nFitting estimator with 390 features.\nFitting estimator with 389 features.\nFitting estimator with 388 features.\nFitting estimator with 387 features.\nFitting estimator with 386 features.\nFitting estimator with 385 features.\nFitting estimator with 384 features.\nFitting estimator with 383 features.\nFitting estimator with 382 features.\nFitting estimator with 381 features.\nFitting estimator with 380 features.\nFitting estimator with 379 features.\nFitting estimator with 378 features.\nFitting estimator with 377 features.\nFitting estimator with 376 features.\nFitting estimator with 375 features.\nFitting estimator with 374 features.\nFitting estimator with 373 features.\nFitting estimator with 372 features.\nFitting estimator with 371 features.\nFitting estimator with 370 features.\nFitting estimator with 369 features.\nFitting estimator with 368 features.\nFitting estimator with 367 features.\nFitting estimator with 366 features.\nFitting estimator with 365 features.\nFitting estimator with 364 features.\nFitting estimator with 363 features.\nFitting estimator with 362 features.\nFitting estimator with 361 features.\nFitting estimator with 360 features.\nFitting estimator with 359 features.\nFitting estimator with 358 features.\nFitting estimator with 357 features.\nFitting estimator with 356 features.\nFitting estimator with 355 features.\nFitting estimator with 354 features.\nFitting estimator with 353 features.\nFitting estimator with 352 features.\nFitting estimator with 351 features.\nFitting estimator with 350 features.\nFitting estimator with 349 features.\nFitting estimator with 348 features.\nFitting estimator with 347 features.\nFitting estimator with 346 features.\nFitting estimator with 345 features.\nFitting estimator with 344 features.\nFitting estimator with 343 features.\nFitting estimator with 342 features.\nFitting estimator with 341 features.\nFitting estimator with 340 features.\nFitting estimator with 339 features.\nFitting estimator with 338 features.\nFitting estimator with 337 features.\nFitting estimator with 336 features.\nFitting estimator with 335 features.\nFitting estimator with 334 features.\nFitting estimator with 333 features.\nFitting estimator with 332 features.\nFitting estimator with 331 features.\nFitting estimator with 330 features.\nFitting estimator with 329 features.\nFitting estimator with 328 features.\nFitting estimator with 327 features.\nFitting estimator with 326 features.\nFitting estimator with 325 features.\nFitting estimator with 324 features.\nFitting estimator with 323 features.\nFitting estimator with 322 features.\nFitting estimator with 321 features.\nFitting estimator with 320 features.\nFitting estimator with 319 features.\nFitting estimator with 318 features.\nFitting estimator with 317 features.\nFitting estimator with 316 features.\nFitting estimator with 315 features.\nFitting estimator with 314 features.\nFitting estimator with 313 features.\nFitting estimator with 312 features.\nFitting estimator with 311 features.\nFitting estimator with 310 features.\nFitting estimator with 309 features.\nFitting estimator with 308 features.\nFitting estimator with 307 features.\nFitting estimator with 306 features.\nFitting estimator with 305 features.\nFitting estimator with 304 features.\nFitting estimator with 303 features.\nFitting estimator with 302 features.\nFitting estimator with 301 features.\nFitting estimator with 300 features.\nFitting estimator with 299 features.\nFitting estimator with 298 features.\nFitting estimator with 297 features.\nFitting estimator with 296 features.\nFitting estimator with 295 features.\nFitting estimator with 294 features.\nFitting estimator with 293 features.\nFitting estimator with 292 features.\nFitting estimator with 291 features.\nFitting estimator with 290 features.\nFitting estimator with 289 features.\nFitting estimator with 288 features.\nFitting estimator with 287 features.\nFitting estimator with 286 features.\nFitting estimator with 285 features.\nFitting estimator with 284 features.\nFitting estimator with 283 features.\nFitting estimator with 282 features.\nFitting estimator with 281 features.\nFitting estimator with 280 features.\nFitting estimator with 279 features.\nFitting estimator with 278 features.\nFitting estimator with 277 features.\nFitting estimator with 276 features.\nFitting estimator with 275 features.\nFitting estimator with 274 features.\nFitting estimator with 273 features.\nFitting estimator with 272 features.\nFitting estimator with 271 features.\nFitting estimator with 270 features.\nFitting estimator with 269 features.\nFitting estimator with 268 features.\nFitting estimator with 267 features.\nFitting estimator with 266 features.\nFitting estimator with 265 features.\nFitting estimator with 264 features.\nFitting estimator with 263 features.\nFitting estimator with 262 features.\nFitting estimator with 261 features.\nFitting estimator with 260 features.\nFitting estimator with 259 features.\nFitting estimator with 258 features.\nFitting estimator with 257 features.\nFitting estimator with 256 features.\nFitting estimator with 255 features.\nFitting estimator with 254 features.\nFitting estimator with 253 features.\nFitting estimator with 252 features.\nFitting estimator with 251 features.\nFitting estimator with 250 features.\nFitting estimator with 249 features.\nFitting estimator with 248 features.\nFitting estimator with 247 features.\nFitting estimator with 246 features.\nFitting estimator with 245 features.\nFitting estimator with 244 features.\nFitting estimator with 243 features.\nFitting estimator with 242 features.\nFitting estimator with 241 features.\nFitting estimator with 240 features.\nFitting estimator with 239 features.\nFitting estimator with 238 features.\nFitting estimator with 237 features.\nFitting estimator with 236 features.\nFitting estimator with 235 features.\nFitting estimator with 234 features.\nFitting estimator with 233 features.\n","name":"stdout"},{"output_type":"stream","text":"Fitting estimator with 232 features.\nFitting estimator with 231 features.\nFitting estimator with 230 features.\nFitting estimator with 229 features.\nFitting estimator with 228 features.\nFitting estimator with 227 features.\nFitting estimator with 226 features.\nFitting estimator with 225 features.\nFitting estimator with 224 features.\nFitting estimator with 223 features.\nFitting estimator with 222 features.\nFitting estimator with 221 features.\nFitting estimator with 220 features.\nFitting estimator with 219 features.\nFitting estimator with 218 features.\nFitting estimator with 217 features.\nFitting estimator with 216 features.\nFitting estimator with 215 features.\nFitting estimator with 214 features.\nFitting estimator with 213 features.\nFitting estimator with 212 features.\nFitting estimator with 211 features.\nFitting estimator with 210 features.\nFitting estimator with 209 features.\nFitting estimator with 208 features.\nFitting estimator with 207 features.\nFitting estimator with 206 features.\nFitting estimator with 205 features.\nFitting estimator with 204 features.\nFitting estimator with 203 features.\nFitting estimator with 202 features.\nFitting estimator with 201 features.\nFitting estimator with 200 features.\nFitting estimator with 199 features.\nFitting estimator with 198 features.\nFitting estimator with 197 features.\nFitting estimator with 196 features.\nFitting estimator with 195 features.\nFitting estimator with 194 features.\nFitting estimator with 193 features.\nFitting estimator with 192 features.\nFitting estimator with 191 features.\nFitting estimator with 190 features.\nFitting estimator with 189 features.\nFitting estimator with 188 features.\nFitting estimator with 187 features.\nFitting estimator with 186 features.\nFitting estimator with 185 features.\nFitting estimator with 184 features.\nFitting estimator with 183 features.\nFitting estimator with 182 features.\nFitting estimator with 181 features.\nFitting estimator with 180 features.\nFitting estimator with 179 features.\nFitting estimator with 178 features.\nFitting estimator with 177 features.\nFitting estimator with 176 features.\nFitting estimator with 175 features.\nFitting estimator with 174 features.\nFitting estimator with 173 features.\nFitting estimator with 172 features.\nFitting estimator with 171 features.\nFitting estimator with 170 features.\nFitting estimator with 169 features.\nFitting estimator with 168 features.\nFitting estimator with 167 features.\nFitting estimator with 166 features.\nFitting estimator with 165 features.\nFitting estimator with 164 features.\nFitting estimator with 163 features.\nFitting estimator with 162 features.\nFitting estimator with 161 features.\nFitting estimator with 160 features.\nFitting estimator with 159 features.\nFitting estimator with 158 features.\nFitting estimator with 157 features.\nFitting estimator with 156 features.\nFitting estimator with 155 features.\nFitting estimator with 154 features.\nFitting estimator with 153 features.\nFitting estimator with 152 features.\nFitting estimator with 151 features.\nFitting estimator with 150 features.\nFitting estimator with 149 features.\nFitting estimator with 148 features.\nFitting estimator with 147 features.\nFitting estimator with 146 features.\nFitting estimator with 145 features.\nFitting estimator with 144 features.\nFitting estimator with 143 features.\nFitting estimator with 142 features.\nFitting estimator with 141 features.\nFitting estimator with 140 features.\nFitting estimator with 139 features.\nFitting estimator with 138 features.\nFitting estimator with 137 features.\nFitting estimator with 136 features.\nFitting estimator with 135 features.\nFitting estimator with 134 features.\nFitting estimator with 133 features.\nFitting estimator with 132 features.\nFitting estimator with 131 features.\nFitting estimator with 130 features.\nFitting estimator with 129 features.\nFitting estimator with 128 features.\nFitting estimator with 127 features.\nFitting estimator with 126 features.\nFitting estimator with 125 features.\nFitting estimator with 124 features.\nFitting estimator with 123 features.\nFitting estimator with 122 features.\nFitting estimator with 121 features.\nFitting estimator with 120 features.\nFitting estimator with 119 features.\nFitting estimator with 118 features.\nFitting estimator with 117 features.\nFitting estimator with 116 features.\nFitting estimator with 115 features.\nFitting estimator with 114 features.\nFitting estimator with 113 features.\nFitting estimator with 112 features.\nFitting estimator with 111 features.\nFitting estimator with 110 features.\nFitting estimator with 109 features.\nFitting estimator with 108 features.\nFitting estimator with 107 features.\nFitting estimator with 106 features.\nFitting estimator with 105 features.\nFitting estimator with 104 features.\nFitting estimator with 103 features.\nFitting estimator with 102 features.\nFitting estimator with 101 features.\nChosen best 100 feature by rfe: Index(['Hilbert_mean', 'abs_max_roll_mean_10000', 'autocorrelation_100',\n       'autocorrelation_1000', 'autocorrelation_10000', 'autocorrelation_5',\n       'autocorrelation_5000', 'av_change_abs_roll_mean_100',\n       'av_change_rate_roll_mean_10', 'av_change_rate_roll_mean_10000',\n       'binned_entropy_10', 'classic_sta_lta2_mean', 'classic_sta_lta3_mean',\n       'ffti_abs_max_roll_mean_1000', 'ffti_autocorrelation_10000',\n       'ffti_av_change_abs_roll_std_1000', 'ffti_av_change_abs_roll_std_10000',\n       'ffti_av_change_abs_roll_std_500', 'ffti_c3_100', 'ffti_c3_10000',\n       'ffti_c3_500', 'ffti_classic_sta_lta7_mean',\n       'ffti_count_big_100000_less_threshold_5',\n       'ffti_count_big_100000_less_threshold_50',\n       'ffti_count_big_150000_less_threshold_10',\n       'ffti_exp_Moving_average_3000_std', 'ffti_max_first_1000',\n       'ffti_max_last_1000', 'ffti_mean', 'ffti_mean_change_rate',\n       'ffti_mean_change_rate_last_10000', 'ffti_min_roll_std_10000',\n       'ffti_min_roll_std_50', 'ffti_num_peaks_10',\n       'ffti_percentile_roll_mean_20_window_10000',\n       'ffti_percentile_roll_mean_40_window_10000',\n       'ffti_percentile_roll_mean_40_window_500',\n       'ffti_percentile_roll_mean_50_window_100',\n       'ffti_percentile_roll_mean_50_window_10000',\n       'ffti_percentile_roll_mean_50_window_50',\n       'ffti_percentile_roll_mean_50_window_500',\n       'ffti_percentile_roll_std_10_window_10',\n       'ffti_percentile_roll_std_10_window_500',\n       'ffti_percentile_roll_std_1_window_10',\n       'ffti_percentile_roll_std_20_window_1000',\n       'ffti_percentile_roll_std_25_window_100',\n       'ffti_percentile_roll_std_40_window_1000',\n       'ffti_percentile_roll_std_50_window_100',\n       'ffti_percentile_roll_std_50_window_10000', 'ffti_skew',\n       'ffti_spkt_welch_density_10', 'ffti_spkt_welch_density_100',\n       'ffti_spkt_welch_density_50', 'fftr_abs_max_roll_mean_500',\n       'fftr_autocorrelation_10', 'fftr_binned_entropy_90', 'fftr_c3_100',\n       'fftr_c3_10000', 'fftr_count_big_150000_less_threshold_5',\n       'fftr_mean_change_rate_first_1000', 'fftr_mean_change_rate_last_1000',\n       'fftr_mean_change_rate_last_10000', 'fftr_med', 'fftr_min_roll_std_10',\n       'fftr_min_roll_std_100', 'fftr_min_roll_std_500',\n       'fftr_percentile_roll_mean_10_window_1000',\n       'fftr_percentile_roll_mean_5_window_1000',\n       'fftr_percentile_roll_mean_5_window_10000',\n       'fftr_percentile_roll_mean_90_window_1000',\n       'fftr_percentile_roll_std_10_window_100',\n       'fftr_percentile_roll_std_10_window_1000',\n       'fftr_percentile_roll_std_10_window_10000',\n       'fftr_percentile_roll_std_1_window_10',\n       'fftr_percentile_roll_std_1_window_50',\n       'fftr_percentile_roll_std_1_window_500',\n       'fftr_percentile_roll_std_25_window_10000',\n       'fftr_percentile_roll_std_30_window_10000',\n       'fftr_percentile_roll_std_40_window_1000',\n       'fftr_percentile_roll_std_50_window_100', 'fftr_range_-3000_-2000',\n       'fftr_time_rev_asym_stat_1', 'fftr_time_rev_asym_stat_100', 'gmean',\n       'max_first_10000', 'max_to_min', 'mean_change_rate',\n       'mean_change_rate_first_10000', 'mean_change_rate_last_10000',\n       'mean_change_rate_last_50000', 'min_roll_std_100', 'min_roll_std_10000',\n       'min_roll_std_500', 'num_peaks_10',\n       'percentile_roll_mean_1_window_1000',\n       'percentile_roll_std_10_window_10000',\n       'percentile_roll_std_1_window_1000', 'range_-1000_0',\n       'spkt_welch_density_100', 'std_roll_mean_10000'],\n      dtype='object')\nCPU times: user 6min 50s, sys: 128 ms, total: 6min 50s\nWall time: 6min 50s\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"<h1>Recursive feature elimination with cross validation and random forest regression<h2>"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nrf = RandomForestRegressor(n_estimators=2)\nrfecv = RFECV(estimator=rf, step=1, cv=3, scoring='neg_mean_absolute_error', verbose=10)   #3-fold cross-validation with mae\nrfecv = rfecv.fit(x_train_corr, y_train.values)\nprint('Optimal number of features :', rfecv.n_features_)\nprint('Best features :', x_train_corr.columns[rfecv.support_])","execution_count":11,"outputs":[{"output_type":"stream","text":"Fitting estimator with 455 features.\n","name":"stdout"},{"output_type":"stream","text":"/opt/conda/lib/python3.6/site-packages/sklearn/utils/validation.py:761: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  y = column_or_1d(y, warn=True)\n","name":"stderr"},{"output_type":"stream","text":"Fitting estimator with 454 features.\nFitting estimator with 453 features.\nFitting estimator with 452 features.\nFitting estimator with 451 features.\nFitting estimator with 450 features.\nFitting estimator with 449 features.\nFitting estimator with 448 features.\nFitting estimator with 447 features.\nFitting estimator with 446 features.\nFitting estimator with 445 features.\nFitting estimator with 444 features.\nFitting estimator with 443 features.\nFitting estimator with 442 features.\nFitting estimator with 441 features.\nFitting estimator with 440 features.\nFitting estimator with 439 features.\nFitting estimator with 438 features.\nFitting estimator with 437 features.\nFitting estimator with 436 features.\nFitting estimator with 435 features.\nFitting estimator with 434 features.\nFitting estimator with 433 features.\nFitting estimator with 432 features.\nFitting estimator with 431 features.\nFitting estimator with 430 features.\nFitting estimator with 429 features.\nFitting estimator with 428 features.\nFitting estimator with 427 features.\nFitting estimator with 426 features.\nFitting estimator with 425 features.\nFitting estimator with 424 features.\nFitting estimator with 423 features.\nFitting estimator with 422 features.\nFitting estimator with 421 features.\nFitting estimator with 420 features.\nFitting estimator with 419 features.\nFitting estimator with 418 features.\nFitting estimator with 417 features.\nFitting estimator with 416 features.\nFitting estimator with 415 features.\nFitting estimator with 414 features.\nFitting estimator with 413 features.\nFitting estimator with 412 features.\nFitting estimator with 411 features.\nFitting estimator with 410 features.\nFitting estimator with 409 features.\nFitting estimator with 408 features.\nFitting estimator with 407 features.\nFitting estimator with 406 features.\nFitting estimator with 405 features.\nFitting estimator with 404 features.\nFitting estimator with 403 features.\nFitting estimator with 402 features.\nFitting estimator with 401 features.\nFitting estimator with 400 features.\nFitting estimator with 399 features.\nFitting estimator with 398 features.\nFitting estimator with 397 features.\nFitting estimator with 396 features.\nFitting estimator with 395 features.\nFitting estimator with 394 features.\nFitting estimator with 393 features.\nFitting estimator with 392 features.\nFitting estimator with 391 features.\nFitting estimator with 390 features.\nFitting estimator with 389 features.\nFitting estimator with 388 features.\nFitting estimator with 387 features.\nFitting estimator with 386 features.\nFitting estimator with 385 features.\nFitting estimator with 384 features.\nFitting estimator with 383 features.\nFitting estimator with 382 features.\nFitting estimator with 381 features.\nFitting estimator with 380 features.\nFitting estimator with 379 features.\nFitting estimator with 378 features.\nFitting estimator with 377 features.\nFitting estimator with 376 features.\nFitting estimator with 375 features.\nFitting estimator with 374 features.\nFitting estimator with 373 features.\nFitting estimator with 372 features.\nFitting estimator with 371 features.\nFitting estimator with 370 features.\nFitting estimator with 369 features.\nFitting estimator with 368 features.\nFitting estimator with 367 features.\nFitting estimator with 366 features.\nFitting estimator with 365 features.\nFitting estimator with 364 features.\nFitting estimator with 363 features.\nFitting estimator with 362 features.\nFitting estimator with 361 features.\nFitting estimator with 360 features.\nFitting estimator with 359 features.\nFitting estimator with 358 features.\nFitting estimator with 357 features.\nFitting estimator with 356 features.\nFitting estimator with 355 features.\nFitting estimator with 354 features.\nFitting estimator with 353 features.\nFitting estimator with 352 features.\nFitting estimator with 351 features.\nFitting estimator with 350 features.\nFitting estimator with 349 features.\nFitting estimator with 348 features.\nFitting estimator with 347 features.\nFitting estimator with 346 features.\nFitting estimator with 345 features.\nFitting estimator with 344 features.\nFitting estimator with 343 features.\nFitting estimator with 342 features.\nFitting estimator with 341 features.\nFitting estimator with 340 features.\nFitting estimator with 339 features.\nFitting estimator with 338 features.\nFitting estimator with 337 features.\nFitting estimator with 336 features.\nFitting estimator with 335 features.\nFitting estimator with 334 features.\nFitting estimator with 333 features.\nFitting estimator with 332 features.\nFitting estimator with 331 features.\nFitting estimator with 330 features.\nFitting estimator with 329 features.\nFitting estimator with 328 features.\nFitting estimator with 327 features.\nFitting estimator with 326 features.\nFitting estimator with 325 features.\nFitting estimator with 324 features.\nFitting estimator with 323 features.\nFitting estimator with 322 features.\nFitting estimator with 321 features.\nFitting estimator with 320 features.\nFitting estimator with 319 features.\nFitting estimator with 318 features.\nFitting estimator with 317 features.\nFitting estimator with 316 features.\nFitting estimator with 315 features.\nFitting estimator with 314 features.\nFitting estimator with 313 features.\nFitting estimator with 312 features.\nFitting estimator with 311 features.\nFitting estimator with 310 features.\nFitting estimator with 309 features.\nFitting estimator with 308 features.\nFitting estimator with 307 features.\nFitting estimator with 306 features.\nFitting estimator with 305 features.\nFitting estimator with 304 features.\nFitting estimator with 303 features.\nFitting estimator with 302 features.\nFitting estimator with 301 features.\nFitting estimator with 300 features.\nFitting estimator with 299 features.\nFitting estimator with 298 features.\nFitting estimator with 297 features.\nFitting estimator with 296 features.\nFitting estimator with 295 features.\nFitting estimator with 294 features.\nFitting estimator with 293 features.\nFitting estimator with 292 features.\nFitting estimator with 291 features.\nFitting estimator with 290 features.\nFitting estimator with 289 features.\nFitting estimator with 288 features.\nFitting estimator with 287 features.\nFitting estimator with 286 features.\nFitting estimator with 285 features.\nFitting estimator with 284 features.\nFitting estimator with 283 features.\nFitting estimator with 282 features.\nFitting estimator with 281 features.\nFitting estimator with 280 features.\nFitting estimator with 279 features.\nFitting estimator with 278 features.\nFitting estimator with 277 features.\nFitting estimator with 276 features.\nFitting estimator with 275 features.\nFitting estimator with 274 features.\nFitting estimator with 273 features.\nFitting estimator with 272 features.\nFitting estimator with 271 features.\nFitting estimator with 270 features.\nFitting estimator with 269 features.\nFitting estimator with 268 features.\nFitting estimator with 267 features.\nFitting estimator with 266 features.\nFitting estimator with 265 features.\nFitting estimator with 264 features.\nFitting estimator with 263 features.\nFitting estimator with 262 features.\nFitting estimator with 261 features.\nFitting estimator with 260 features.\nFitting estimator with 259 features.\nFitting estimator with 258 features.\nFitting estimator with 257 features.\nFitting estimator with 256 features.\nFitting estimator with 255 features.\nFitting estimator with 254 features.\nFitting estimator with 253 features.\nFitting estimator with 252 features.\nFitting estimator with 251 features.\nFitting estimator with 250 features.\nFitting estimator with 249 features.\nFitting estimator with 248 features.\nFitting estimator with 247 features.\nFitting estimator with 246 features.\nFitting estimator with 245 features.\nFitting estimator with 244 features.\nFitting estimator with 243 features.\nFitting estimator with 242 features.\nFitting estimator with 241 features.\nFitting estimator with 240 features.\nFitting estimator with 239 features.\nFitting estimator with 238 features.\nFitting estimator with 237 features.\nFitting estimator with 236 features.\nFitting estimator with 235 features.\nFitting estimator with 234 features.\nFitting estimator with 233 features.\n","name":"stdout"},{"output_type":"stream","text":"Fitting estimator with 232 features.\nFitting estimator with 231 features.\nFitting estimator with 230 features.\nFitting estimator with 229 features.\nFitting estimator with 228 features.\nFitting estimator with 227 features.\nFitting estimator with 226 features.\nFitting estimator with 225 features.\nFitting estimator with 224 features.\nFitting estimator with 223 features.\nFitting estimator with 222 features.\nFitting estimator with 221 features.\nFitting estimator with 220 features.\nFitting estimator with 219 features.\nFitting estimator with 218 features.\nFitting estimator with 217 features.\nFitting estimator with 216 features.\nFitting estimator with 215 features.\nFitting estimator with 214 features.\nFitting estimator with 213 features.\nFitting estimator with 212 features.\nFitting estimator with 211 features.\nFitting estimator with 210 features.\nFitting estimator with 209 features.\nFitting estimator with 208 features.\nFitting estimator with 207 features.\nFitting estimator with 206 features.\nFitting estimator with 205 features.\nFitting estimator with 204 features.\nFitting estimator with 203 features.\nFitting estimator with 202 features.\nFitting estimator with 201 features.\nFitting estimator with 200 features.\nFitting estimator with 199 features.\nFitting estimator with 198 features.\nFitting estimator with 197 features.\nFitting estimator with 196 features.\nFitting estimator with 195 features.\nFitting estimator with 194 features.\nFitting estimator with 193 features.\nFitting estimator with 192 features.\nFitting estimator with 191 features.\nFitting estimator with 190 features.\nFitting estimator with 189 features.\nFitting estimator with 188 features.\nFitting estimator with 187 features.\nFitting estimator with 186 features.\nFitting estimator with 185 features.\nFitting estimator with 184 features.\nFitting estimator with 183 features.\nFitting estimator with 182 features.\nFitting estimator with 181 features.\nFitting estimator with 180 features.\nFitting estimator with 179 features.\nFitting estimator with 178 features.\nFitting estimator with 177 features.\nFitting estimator with 176 features.\nFitting estimator with 175 features.\nFitting estimator with 174 features.\nFitting estimator with 173 features.\nFitting estimator with 172 features.\nFitting estimator with 171 features.\nFitting estimator with 170 features.\nFitting estimator with 169 features.\nFitting estimator with 168 features.\nFitting estimator with 167 features.\nFitting estimator with 166 features.\nFitting estimator with 165 features.\nFitting estimator with 164 features.\nFitting estimator with 163 features.\nFitting estimator with 162 features.\nFitting estimator with 161 features.\nFitting estimator with 160 features.\nFitting estimator with 159 features.\nFitting estimator with 158 features.\nFitting estimator with 157 features.\nFitting estimator with 156 features.\nFitting estimator with 155 features.\nFitting estimator with 154 features.\nFitting estimator with 153 features.\nFitting estimator with 152 features.\nFitting estimator with 151 features.\nFitting estimator with 150 features.\nFitting estimator with 149 features.\nFitting estimator with 148 features.\nFitting estimator with 147 features.\nFitting estimator with 146 features.\nFitting estimator with 145 features.\nFitting estimator with 144 features.\nFitting estimator with 143 features.\nFitting estimator with 142 features.\nFitting estimator with 141 features.\nFitting estimator with 140 features.\nFitting estimator with 139 features.\nFitting estimator with 138 features.\nFitting estimator with 137 features.\nFitting estimator with 136 features.\nFitting estimator with 135 features.\nFitting estimator with 134 features.\nFitting estimator with 133 features.\nFitting estimator with 132 features.\nFitting estimator with 131 features.\nFitting estimator with 130 features.\nFitting estimator with 129 features.\nFitting estimator with 128 features.\nFitting estimator with 127 features.\nFitting estimator with 126 features.\nFitting estimator with 125 features.\nFitting estimator with 124 features.\nFitting estimator with 123 features.\nFitting estimator with 122 features.\nFitting estimator with 121 features.\nFitting estimator with 120 features.\nFitting estimator with 119 features.\nFitting estimator with 118 features.\nFitting estimator with 117 features.\nFitting estimator with 116 features.\nFitting estimator with 115 features.\nFitting estimator with 114 features.\nFitting estimator with 113 features.\nFitting estimator with 112 features.\nFitting estimator with 111 features.\nFitting estimator with 110 features.\nFitting estimator with 109 features.\nFitting estimator with 108 features.\nFitting estimator with 107 features.\nFitting estimator with 106 features.\nFitting estimator with 105 features.\nFitting estimator with 104 features.\nFitting estimator with 103 features.\nFitting estimator with 102 features.\nFitting estimator with 101 features.\nFitting estimator with 100 features.\nFitting estimator with 99 features.\nFitting estimator with 98 features.\nFitting estimator with 97 features.\nFitting estimator with 96 features.\nFitting estimator with 95 features.\nFitting estimator with 94 features.\nFitting estimator with 93 features.\nFitting estimator with 92 features.\nFitting estimator with 91 features.\nFitting estimator with 90 features.\nFitting estimator with 89 features.\nFitting estimator with 88 features.\nFitting estimator with 87 features.\nFitting estimator with 86 features.\nFitting estimator with 85 features.\nFitting estimator with 84 features.\nFitting estimator with 83 features.\nFitting estimator with 82 features.\nFitting estimator with 81 features.\nFitting estimator with 80 features.\nFitting estimator with 79 features.\nFitting estimator with 78 features.\nFitting estimator with 77 features.\nFitting estimator with 76 features.\nFitting estimator with 75 features.\nFitting estimator with 74 features.\nFitting estimator with 73 features.\nFitting estimator with 72 features.\nFitting estimator with 71 features.\nFitting estimator with 70 features.\nFitting estimator with 69 features.\nFitting estimator with 68 features.\nFitting estimator with 67 features.\nFitting estimator with 66 features.\nFitting estimator with 65 features.\nFitting estimator with 64 features.\nFitting estimator with 63 features.\nFitting estimator with 62 features.\nFitting estimator with 61 features.\nFitting estimator with 60 features.\nFitting estimator with 59 features.\nFitting estimator with 58 features.\nFitting estimator with 57 features.\nFitting estimator with 56 features.\nFitting estimator with 55 features.\nFitting estimator with 54 features.\nFitting estimator with 53 features.\nFitting estimator with 52 features.\nFitting estimator with 51 features.\nFitting estimator with 50 features.\nFitting estimator with 49 features.\nFitting estimator with 48 features.\nFitting estimator with 47 features.\nFitting estimator with 46 features.\nFitting estimator with 45 features.\nFitting estimator with 44 features.\nFitting estimator with 43 features.\nFitting estimator with 42 features.\nFitting estimator with 41 features.\nFitting estimator with 40 features.\nFitting estimator with 39 features.\nFitting estimator with 38 features.\nFitting estimator with 37 features.\nFitting estimator with 36 features.\nFitting estimator with 35 features.\nFitting estimator with 34 features.\nFitting estimator with 33 features.\nFitting estimator with 32 features.\nFitting estimator with 31 features.\nFitting estimator with 30 features.\nFitting estimator with 29 features.\nFitting estimator with 28 features.\nFitting estimator with 27 features.\nFitting estimator with 26 features.\nFitting estimator with 25 features.\nFitting estimator with 24 features.\nFitting estimator with 23 features.\nFitting estimator with 22 features.\nFitting estimator with 21 features.\nFitting estimator with 20 features.\nFitting estimator with 19 features.\nFitting estimator with 18 features.\nFitting estimator with 17 features.\nFitting estimator with 16 features.\nFitting estimator with 15 features.\nFitting estimator with 14 features.\nFitting estimator with 13 features.\nFitting estimator with 12 features.\nFitting estimator with 11 features.\nFitting estimator with 10 features.\nFitting estimator with 9 features.\nFitting estimator with 8 features.\nFitting estimator with 7 features.\nFitting estimator with 6 features.\nFitting estimator with 5 features.\nFitting estimator with 4 features.\nFitting estimator with 3 features.\nFitting estimator with 2 features.\n","name":"stdout"},{"output_type":"stream","text":"Fitting estimator with 455 features.\nFitting estimator with 454 features.\nFitting estimator with 453 features.\nFitting estimator with 452 features.\nFitting estimator with 451 features.\nFitting estimator with 450 features.\nFitting estimator with 449 features.\nFitting estimator with 448 features.\nFitting estimator with 447 features.\nFitting estimator with 446 features.\nFitting estimator with 445 features.\nFitting estimator with 444 features.\nFitting estimator with 443 features.\nFitting estimator with 442 features.\nFitting estimator with 441 features.\nFitting estimator with 440 features.\nFitting estimator with 439 features.\nFitting estimator with 438 features.\nFitting estimator with 437 features.\nFitting estimator with 436 features.\nFitting estimator with 435 features.\nFitting estimator with 434 features.\nFitting estimator with 433 features.\nFitting estimator with 432 features.\nFitting estimator with 431 features.\nFitting estimator with 430 features.\nFitting estimator with 429 features.\nFitting estimator with 428 features.\nFitting estimator with 427 features.\nFitting estimator with 426 features.\nFitting estimator with 425 features.\nFitting estimator with 424 features.\nFitting estimator with 423 features.\nFitting estimator with 422 features.\nFitting estimator with 421 features.\nFitting estimator with 420 features.\nFitting estimator with 419 features.\nFitting estimator with 418 features.\nFitting estimator with 417 features.\nFitting estimator with 416 features.\nFitting estimator with 415 features.\nFitting estimator with 414 features.\nFitting estimator with 413 features.\nFitting estimator with 412 features.\nFitting estimator with 411 features.\nFitting estimator with 410 features.\nFitting estimator with 409 features.\nFitting estimator with 408 features.\nFitting estimator with 407 features.\nFitting estimator with 406 features.\nFitting estimator with 405 features.\nFitting estimator with 404 features.\nFitting estimator with 403 features.\nFitting estimator with 402 features.\nFitting estimator with 401 features.\nFitting estimator with 400 features.\nFitting estimator with 399 features.\nFitting estimator with 398 features.\nFitting estimator with 397 features.\nFitting estimator with 396 features.\nFitting estimator with 395 features.\nFitting estimator with 394 features.\nFitting estimator with 393 features.\nFitting estimator with 392 features.\nFitting estimator with 391 features.\nFitting estimator with 390 features.\nFitting estimator with 389 features.\nFitting estimator with 388 features.\nFitting estimator with 387 features.\nFitting estimator with 386 features.\nFitting estimator with 385 features.\nFitting estimator with 384 features.\nFitting estimator with 383 features.\nFitting estimator with 382 features.\nFitting estimator with 381 features.\nFitting estimator with 380 features.\nFitting estimator with 379 features.\nFitting estimator with 378 features.\nFitting estimator with 377 features.\nFitting estimator with 376 features.\nFitting estimator with 375 features.\nFitting estimator with 374 features.\nFitting estimator with 373 features.\nFitting estimator with 372 features.\nFitting estimator with 371 features.\nFitting estimator with 370 features.\nFitting estimator with 369 features.\nFitting estimator with 368 features.\nFitting estimator with 367 features.\nFitting estimator with 366 features.\nFitting estimator with 365 features.\nFitting estimator with 364 features.\nFitting estimator with 363 features.\nFitting estimator with 362 features.\nFitting estimator with 361 features.\nFitting estimator with 360 features.\nFitting estimator with 359 features.\nFitting estimator with 358 features.\nFitting estimator with 357 features.\nFitting estimator with 356 features.\nFitting estimator with 355 features.\nFitting estimator with 354 features.\nFitting estimator with 353 features.\nFitting estimator with 352 features.\nFitting estimator with 351 features.\nFitting estimator with 350 features.\nFitting estimator with 349 features.\nFitting estimator with 348 features.\nFitting estimator with 347 features.\nFitting estimator with 346 features.\nFitting estimator with 345 features.\nFitting estimator with 344 features.\nFitting estimator with 343 features.\nFitting estimator with 342 features.\nFitting estimator with 341 features.\nFitting estimator with 340 features.\nFitting estimator with 339 features.\nFitting estimator with 338 features.\nFitting estimator with 337 features.\nFitting estimator with 336 features.\nFitting estimator with 335 features.\nFitting estimator with 334 features.\nFitting estimator with 333 features.\nFitting estimator with 332 features.\nFitting estimator with 331 features.\nFitting estimator with 330 features.\nFitting estimator with 329 features.\nFitting estimator with 328 features.\nFitting estimator with 327 features.\nFitting estimator with 326 features.\nFitting estimator with 325 features.\nFitting estimator with 324 features.\nFitting estimator with 323 features.\nFitting estimator with 322 features.\nFitting estimator with 321 features.\nFitting estimator with 320 features.\nFitting estimator with 319 features.\nFitting estimator with 318 features.\nFitting estimator with 317 features.\nFitting estimator with 316 features.\nFitting estimator with 315 features.\nFitting estimator with 314 features.\nFitting estimator with 313 features.\nFitting estimator with 312 features.\nFitting estimator with 311 features.\nFitting estimator with 310 features.\nFitting estimator with 309 features.\nFitting estimator with 308 features.\nFitting estimator with 307 features.\nFitting estimator with 306 features.\nFitting estimator with 305 features.\nFitting estimator with 304 features.\nFitting estimator with 303 features.\nFitting estimator with 302 features.\nFitting estimator with 301 features.\nFitting estimator with 300 features.\nFitting estimator with 299 features.\nFitting estimator with 298 features.\nFitting estimator with 297 features.\nFitting estimator with 296 features.\nFitting estimator with 295 features.\nFitting estimator with 294 features.\nFitting estimator with 293 features.\nFitting estimator with 292 features.\nFitting estimator with 291 features.\nFitting estimator with 290 features.\nFitting estimator with 289 features.\nFitting estimator with 288 features.\nFitting estimator with 287 features.\nFitting estimator with 286 features.\nFitting estimator with 285 features.\nFitting estimator with 284 features.\nFitting estimator with 283 features.\nFitting estimator with 282 features.\nFitting estimator with 281 features.\nFitting estimator with 280 features.\nFitting estimator with 279 features.\nFitting estimator with 278 features.\nFitting estimator with 277 features.\nFitting estimator with 276 features.\nFitting estimator with 275 features.\nFitting estimator with 274 features.\nFitting estimator with 273 features.\nFitting estimator with 272 features.\nFitting estimator with 271 features.\nFitting estimator with 270 features.\nFitting estimator with 269 features.\nFitting estimator with 268 features.\nFitting estimator with 267 features.\nFitting estimator with 266 features.\nFitting estimator with 265 features.\nFitting estimator with 264 features.\nFitting estimator with 263 features.\nFitting estimator with 262 features.\nFitting estimator with 261 features.\nFitting estimator with 260 features.\nFitting estimator with 259 features.\nFitting estimator with 258 features.\nFitting estimator with 257 features.\nFitting estimator with 256 features.\nFitting estimator with 255 features.\nFitting estimator with 254 features.\nFitting estimator with 253 features.\nFitting estimator with 252 features.\nFitting estimator with 251 features.\nFitting estimator with 250 features.\nFitting estimator with 249 features.\nFitting estimator with 248 features.\nFitting estimator with 247 features.\nFitting estimator with 246 features.\nFitting estimator with 245 features.\nFitting estimator with 244 features.\nFitting estimator with 243 features.\nFitting estimator with 242 features.\nFitting estimator with 241 features.\nFitting estimator with 240 features.\nFitting estimator with 239 features.\nFitting estimator with 238 features.\nFitting estimator with 237 features.\nFitting estimator with 236 features.\nFitting estimator with 235 features.\nFitting estimator with 234 features.\n","name":"stdout"},{"output_type":"stream","text":"Fitting estimator with 233 features.\nFitting estimator with 232 features.\nFitting estimator with 231 features.\nFitting estimator with 230 features.\nFitting estimator with 229 features.\nFitting estimator with 228 features.\nFitting estimator with 227 features.\nFitting estimator with 226 features.\nFitting estimator with 225 features.\nFitting estimator with 224 features.\nFitting estimator with 223 features.\nFitting estimator with 222 features.\nFitting estimator with 221 features.\nFitting estimator with 220 features.\nFitting estimator with 219 features.\nFitting estimator with 218 features.\nFitting estimator with 217 features.\nFitting estimator with 216 features.\nFitting estimator with 215 features.\nFitting estimator with 214 features.\nFitting estimator with 213 features.\nFitting estimator with 212 features.\nFitting estimator with 211 features.\nFitting estimator with 210 features.\nFitting estimator with 209 features.\nFitting estimator with 208 features.\nFitting estimator with 207 features.\nFitting estimator with 206 features.\nFitting estimator with 205 features.\nFitting estimator with 204 features.\nFitting estimator with 203 features.\nFitting estimator with 202 features.\nFitting estimator with 201 features.\nFitting estimator with 200 features.\nFitting estimator with 199 features.\nFitting estimator with 198 features.\nFitting estimator with 197 features.\nFitting estimator with 196 features.\nFitting estimator with 195 features.\nFitting estimator with 194 features.\nFitting estimator with 193 features.\nFitting estimator with 192 features.\nFitting estimator with 191 features.\nFitting estimator with 190 features.\nFitting estimator with 189 features.\nFitting estimator with 188 features.\nFitting estimator with 187 features.\nFitting estimator with 186 features.\nFitting estimator with 185 features.\nFitting estimator with 184 features.\nFitting estimator with 183 features.\nFitting estimator with 182 features.\nFitting estimator with 181 features.\nFitting estimator with 180 features.\nFitting estimator with 179 features.\nFitting estimator with 178 features.\nFitting estimator with 177 features.\nFitting estimator with 176 features.\nFitting estimator with 175 features.\nFitting estimator with 174 features.\nFitting estimator with 173 features.\nFitting estimator with 172 features.\nFitting estimator with 171 features.\nFitting estimator with 170 features.\nFitting estimator with 169 features.\nFitting estimator with 168 features.\nFitting estimator with 167 features.\nFitting estimator with 166 features.\nFitting estimator with 165 features.\nFitting estimator with 164 features.\nFitting estimator with 163 features.\nFitting estimator with 162 features.\nFitting estimator with 161 features.\nFitting estimator with 160 features.\nFitting estimator with 159 features.\nFitting estimator with 158 features.\nFitting estimator with 157 features.\nFitting estimator with 156 features.\nFitting estimator with 155 features.\nFitting estimator with 154 features.\nFitting estimator with 153 features.\nFitting estimator with 152 features.\nFitting estimator with 151 features.\nFitting estimator with 150 features.\nFitting estimator with 149 features.\nFitting estimator with 148 features.\nFitting estimator with 147 features.\nFitting estimator with 146 features.\nFitting estimator with 145 features.\nFitting estimator with 144 features.\nFitting estimator with 143 features.\nFitting estimator with 142 features.\nFitting estimator with 141 features.\nFitting estimator with 140 features.\nFitting estimator with 139 features.\nFitting estimator with 138 features.\nFitting estimator with 137 features.\nFitting estimator with 136 features.\nFitting estimator with 135 features.\nFitting estimator with 134 features.\nFitting estimator with 133 features.\nFitting estimator with 132 features.\nFitting estimator with 131 features.\nFitting estimator with 130 features.\nFitting estimator with 129 features.\nFitting estimator with 128 features.\nFitting estimator with 127 features.\nFitting estimator with 126 features.\nFitting estimator with 125 features.\nFitting estimator with 124 features.\nFitting estimator with 123 features.\nFitting estimator with 122 features.\nFitting estimator with 121 features.\nFitting estimator with 120 features.\nFitting estimator with 119 features.\nFitting estimator with 118 features.\nFitting estimator with 117 features.\nFitting estimator with 116 features.\nFitting estimator with 115 features.\nFitting estimator with 114 features.\nFitting estimator with 113 features.\nFitting estimator with 112 features.\nFitting estimator with 111 features.\nFitting estimator with 110 features.\nFitting estimator with 109 features.\nFitting estimator with 108 features.\nFitting estimator with 107 features.\nFitting estimator with 106 features.\nFitting estimator with 105 features.\nFitting estimator with 104 features.\nFitting estimator with 103 features.\nFitting estimator with 102 features.\nFitting estimator with 101 features.\nFitting estimator with 100 features.\nFitting estimator with 99 features.\nFitting estimator with 98 features.\nFitting estimator with 97 features.\nFitting estimator with 96 features.\nFitting estimator with 95 features.\nFitting estimator with 94 features.\nFitting estimator with 93 features.\nFitting estimator with 92 features.\nFitting estimator with 91 features.\nFitting estimator with 90 features.\nFitting estimator with 89 features.\nFitting estimator with 88 features.\nFitting estimator with 87 features.\nFitting estimator with 86 features.\nFitting estimator with 85 features.\nFitting estimator with 84 features.\nFitting estimator with 83 features.\nFitting estimator with 82 features.\nFitting estimator with 81 features.\nFitting estimator with 80 features.\nFitting estimator with 79 features.\nFitting estimator with 78 features.\nFitting estimator with 77 features.\nFitting estimator with 76 features.\nFitting estimator with 75 features.\nFitting estimator with 74 features.\nFitting estimator with 73 features.\nFitting estimator with 72 features.\nFitting estimator with 71 features.\nFitting estimator with 70 features.\nFitting estimator with 69 features.\nFitting estimator with 68 features.\nFitting estimator with 67 features.\nFitting estimator with 66 features.\nFitting estimator with 65 features.\nFitting estimator with 64 features.\nFitting estimator with 63 features.\nFitting estimator with 62 features.\nFitting estimator with 61 features.\nFitting estimator with 60 features.\nFitting estimator with 59 features.\nFitting estimator with 58 features.\nFitting estimator with 57 features.\nFitting estimator with 56 features.\nFitting estimator with 55 features.\nFitting estimator with 54 features.\nFitting estimator with 53 features.\nFitting estimator with 52 features.\nFitting estimator with 51 features.\nFitting estimator with 50 features.\nFitting estimator with 49 features.\nFitting estimator with 48 features.\nFitting estimator with 47 features.\nFitting estimator with 46 features.\nFitting estimator with 45 features.\nFitting estimator with 44 features.\nFitting estimator with 43 features.\nFitting estimator with 42 features.\nFitting estimator with 41 features.\nFitting estimator with 40 features.\nFitting estimator with 39 features.\nFitting estimator with 38 features.\nFitting estimator with 37 features.\nFitting estimator with 36 features.\nFitting estimator with 35 features.\nFitting estimator with 34 features.\nFitting estimator with 33 features.\nFitting estimator with 32 features.\nFitting estimator with 31 features.\nFitting estimator with 30 features.\nFitting estimator with 29 features.\nFitting estimator with 28 features.\nFitting estimator with 27 features.\nFitting estimator with 26 features.\nFitting estimator with 25 features.\nFitting estimator with 24 features.\nFitting estimator with 23 features.\nFitting estimator with 22 features.\nFitting estimator with 21 features.\nFitting estimator with 20 features.\nFitting estimator with 19 features.\nFitting estimator with 18 features.\nFitting estimator with 17 features.\nFitting estimator with 16 features.\nFitting estimator with 15 features.\nFitting estimator with 14 features.\nFitting estimator with 13 features.\nFitting estimator with 12 features.\nFitting estimator with 11 features.\nFitting estimator with 10 features.\nFitting estimator with 9 features.\nFitting estimator with 8 features.\nFitting estimator with 7 features.\nFitting estimator with 6 features.\nFitting estimator with 5 features.\n","name":"stdout"},{"output_type":"stream","text":"Fitting estimator with 4 features.\nFitting estimator with 3 features.\nFitting estimator with 2 features.\nFitting estimator with 455 features.\nFitting estimator with 454 features.\nFitting estimator with 453 features.\nFitting estimator with 452 features.\nFitting estimator with 451 features.\nFitting estimator with 450 features.\nFitting estimator with 449 features.\nFitting estimator with 448 features.\nFitting estimator with 447 features.\nFitting estimator with 446 features.\nFitting estimator with 445 features.\nFitting estimator with 444 features.\nFitting estimator with 443 features.\nFitting estimator with 442 features.\nFitting estimator with 441 features.\nFitting estimator with 440 features.\nFitting estimator with 439 features.\nFitting estimator with 438 features.\nFitting estimator with 437 features.\nFitting estimator with 436 features.\nFitting estimator with 435 features.\nFitting estimator with 434 features.\nFitting estimator with 433 features.\nFitting estimator with 432 features.\nFitting estimator with 431 features.\nFitting estimator with 430 features.\nFitting estimator with 429 features.\nFitting estimator with 428 features.\nFitting estimator with 427 features.\nFitting estimator with 426 features.\nFitting estimator with 425 features.\nFitting estimator with 424 features.\nFitting estimator with 423 features.\nFitting estimator with 422 features.\nFitting estimator with 421 features.\nFitting estimator with 420 features.\nFitting estimator with 419 features.\nFitting estimator with 418 features.\nFitting estimator with 417 features.\nFitting estimator with 416 features.\nFitting estimator with 415 features.\nFitting estimator with 414 features.\nFitting estimator with 413 features.\nFitting estimator with 412 features.\nFitting estimator with 411 features.\nFitting estimator with 410 features.\nFitting estimator with 409 features.\nFitting estimator with 408 features.\nFitting estimator with 407 features.\nFitting estimator with 406 features.\nFitting estimator with 405 features.\nFitting estimator with 404 features.\nFitting estimator with 403 features.\nFitting estimator with 402 features.\nFitting estimator with 401 features.\nFitting estimator with 400 features.\nFitting estimator with 399 features.\nFitting estimator with 398 features.\nFitting estimator with 397 features.\nFitting estimator with 396 features.\nFitting estimator with 395 features.\nFitting estimator with 394 features.\nFitting estimator with 393 features.\nFitting estimator with 392 features.\nFitting estimator with 391 features.\nFitting estimator with 390 features.\nFitting estimator with 389 features.\nFitting estimator with 388 features.\nFitting estimator with 387 features.\nFitting estimator with 386 features.\nFitting estimator with 385 features.\nFitting estimator with 384 features.\nFitting estimator with 383 features.\nFitting estimator with 382 features.\nFitting estimator with 381 features.\nFitting estimator with 380 features.\nFitting estimator with 379 features.\nFitting estimator with 378 features.\nFitting estimator with 377 features.\nFitting estimator with 376 features.\nFitting estimator with 375 features.\nFitting estimator with 374 features.\nFitting estimator with 373 features.\nFitting estimator with 372 features.\nFitting estimator with 371 features.\nFitting estimator with 370 features.\nFitting estimator with 369 features.\nFitting estimator with 368 features.\nFitting estimator with 367 features.\nFitting estimator with 366 features.\nFitting estimator with 365 features.\nFitting estimator with 364 features.\nFitting estimator with 363 features.\nFitting estimator with 362 features.\nFitting estimator with 361 features.\nFitting estimator with 360 features.\nFitting estimator with 359 features.\nFitting estimator with 358 features.\nFitting estimator with 357 features.\nFitting estimator with 356 features.\nFitting estimator with 355 features.\nFitting estimator with 354 features.\nFitting estimator with 353 features.\nFitting estimator with 352 features.\nFitting estimator with 351 features.\nFitting estimator with 350 features.\nFitting estimator with 349 features.\nFitting estimator with 348 features.\nFitting estimator with 347 features.\nFitting estimator with 346 features.\nFitting estimator with 345 features.\nFitting estimator with 344 features.\nFitting estimator with 343 features.\nFitting estimator with 342 features.\nFitting estimator with 341 features.\nFitting estimator with 340 features.\nFitting estimator with 339 features.\nFitting estimator with 338 features.\nFitting estimator with 337 features.\nFitting estimator with 336 features.\nFitting estimator with 335 features.\nFitting estimator with 334 features.\nFitting estimator with 333 features.\nFitting estimator with 332 features.\nFitting estimator with 331 features.\nFitting estimator with 330 features.\nFitting estimator with 329 features.\nFitting estimator with 328 features.\nFitting estimator with 327 features.\nFitting estimator with 326 features.\nFitting estimator with 325 features.\nFitting estimator with 324 features.\nFitting estimator with 323 features.\nFitting estimator with 322 features.\nFitting estimator with 321 features.\nFitting estimator with 320 features.\nFitting estimator with 319 features.\nFitting estimator with 318 features.\nFitting estimator with 317 features.\nFitting estimator with 316 features.\nFitting estimator with 315 features.\nFitting estimator with 314 features.\nFitting estimator with 313 features.\nFitting estimator with 312 features.\nFitting estimator with 311 features.\nFitting estimator with 310 features.\nFitting estimator with 309 features.\nFitting estimator with 308 features.\nFitting estimator with 307 features.\nFitting estimator with 306 features.\nFitting estimator with 305 features.\nFitting estimator with 304 features.\nFitting estimator with 303 features.\nFitting estimator with 302 features.\nFitting estimator with 301 features.\nFitting estimator with 300 features.\nFitting estimator with 299 features.\nFitting estimator with 298 features.\nFitting estimator with 297 features.\nFitting estimator with 296 features.\nFitting estimator with 295 features.\nFitting estimator with 294 features.\nFitting estimator with 293 features.\nFitting estimator with 292 features.\nFitting estimator with 291 features.\nFitting estimator with 290 features.\nFitting estimator with 289 features.\nFitting estimator with 288 features.\nFitting estimator with 287 features.\nFitting estimator with 286 features.\nFitting estimator with 285 features.\nFitting estimator with 284 features.\nFitting estimator with 283 features.\nFitting estimator with 282 features.\nFitting estimator with 281 features.\nFitting estimator with 280 features.\nFitting estimator with 279 features.\nFitting estimator with 278 features.\nFitting estimator with 277 features.\nFitting estimator with 276 features.\nFitting estimator with 275 features.\nFitting estimator with 274 features.\nFitting estimator with 273 features.\nFitting estimator with 272 features.\nFitting estimator with 271 features.\nFitting estimator with 270 features.\nFitting estimator with 269 features.\nFitting estimator with 268 features.\nFitting estimator with 267 features.\nFitting estimator with 266 features.\nFitting estimator with 265 features.\nFitting estimator with 264 features.\nFitting estimator with 263 features.\nFitting estimator with 262 features.\nFitting estimator with 261 features.\nFitting estimator with 260 features.\nFitting estimator with 259 features.\nFitting estimator with 258 features.\nFitting estimator with 257 features.\nFitting estimator with 256 features.\nFitting estimator with 255 features.\nFitting estimator with 254 features.\nFitting estimator with 253 features.\nFitting estimator with 252 features.\nFitting estimator with 251 features.\nFitting estimator with 250 features.\nFitting estimator with 249 features.\nFitting estimator with 248 features.\nFitting estimator with 247 features.\nFitting estimator with 246 features.\nFitting estimator with 245 features.\nFitting estimator with 244 features.\nFitting estimator with 243 features.\nFitting estimator with 242 features.\nFitting estimator with 241 features.\nFitting estimator with 240 features.\nFitting estimator with 239 features.\nFitting estimator with 238 features.\nFitting estimator with 237 features.\n","name":"stdout"},{"output_type":"stream","text":"Fitting estimator with 236 features.\nFitting estimator with 235 features.\nFitting estimator with 234 features.\nFitting estimator with 233 features.\nFitting estimator with 232 features.\nFitting estimator with 231 features.\nFitting estimator with 230 features.\nFitting estimator with 229 features.\nFitting estimator with 228 features.\nFitting estimator with 227 features.\nFitting estimator with 226 features.\nFitting estimator with 225 features.\nFitting estimator with 224 features.\nFitting estimator with 223 features.\nFitting estimator with 222 features.\nFitting estimator with 221 features.\nFitting estimator with 220 features.\nFitting estimator with 219 features.\nFitting estimator with 218 features.\nFitting estimator with 217 features.\nFitting estimator with 216 features.\nFitting estimator with 215 features.\nFitting estimator with 214 features.\nFitting estimator with 213 features.\nFitting estimator with 212 features.\nFitting estimator with 211 features.\nFitting estimator with 210 features.\nFitting estimator with 209 features.\nFitting estimator with 208 features.\nFitting estimator with 207 features.\nFitting estimator with 206 features.\nFitting estimator with 205 features.\nFitting estimator with 204 features.\nFitting estimator with 203 features.\nFitting estimator with 202 features.\nFitting estimator with 201 features.\nFitting estimator with 200 features.\nFitting estimator with 199 features.\nFitting estimator with 198 features.\nFitting estimator with 197 features.\nFitting estimator with 196 features.\nFitting estimator with 195 features.\nFitting estimator with 194 features.\nFitting estimator with 193 features.\nFitting estimator with 192 features.\nFitting estimator with 191 features.\nFitting estimator with 190 features.\nFitting estimator with 189 features.\nFitting estimator with 188 features.\nFitting estimator with 187 features.\nFitting estimator with 186 features.\nFitting estimator with 185 features.\nFitting estimator with 184 features.\nFitting estimator with 183 features.\nFitting estimator with 182 features.\nFitting estimator with 181 features.\nFitting estimator with 180 features.\nFitting estimator with 179 features.\nFitting estimator with 178 features.\nFitting estimator with 177 features.\nFitting estimator with 176 features.\nFitting estimator with 175 features.\nFitting estimator with 174 features.\nFitting estimator with 173 features.\nFitting estimator with 172 features.\nFitting estimator with 171 features.\nFitting estimator with 170 features.\nFitting estimator with 169 features.\nFitting estimator with 168 features.\nFitting estimator with 167 features.\nFitting estimator with 166 features.\nFitting estimator with 165 features.\nFitting estimator with 164 features.\nFitting estimator with 163 features.\nFitting estimator with 162 features.\nFitting estimator with 161 features.\nFitting estimator with 160 features.\nFitting estimator with 159 features.\nFitting estimator with 158 features.\nFitting estimator with 157 features.\nFitting estimator with 156 features.\nFitting estimator with 155 features.\nFitting estimator with 154 features.\nFitting estimator with 153 features.\nFitting estimator with 152 features.\nFitting estimator with 151 features.\nFitting estimator with 150 features.\nFitting estimator with 149 features.\nFitting estimator with 148 features.\nFitting estimator with 147 features.\nFitting estimator with 146 features.\nFitting estimator with 145 features.\nFitting estimator with 144 features.\nFitting estimator with 143 features.\nFitting estimator with 142 features.\nFitting estimator with 141 features.\nFitting estimator with 140 features.\nFitting estimator with 139 features.\nFitting estimator with 138 features.\nFitting estimator with 137 features.\nFitting estimator with 136 features.\nFitting estimator with 135 features.\nFitting estimator with 134 features.\nFitting estimator with 133 features.\nFitting estimator with 132 features.\nFitting estimator with 131 features.\nFitting estimator with 130 features.\nFitting estimator with 129 features.\nFitting estimator with 128 features.\nFitting estimator with 127 features.\nFitting estimator with 126 features.\nFitting estimator with 125 features.\nFitting estimator with 124 features.\nFitting estimator with 123 features.\nFitting estimator with 122 features.\nFitting estimator with 121 features.\nFitting estimator with 120 features.\nFitting estimator with 119 features.\nFitting estimator with 118 features.\nFitting estimator with 117 features.\nFitting estimator with 116 features.\nFitting estimator with 115 features.\nFitting estimator with 114 features.\nFitting estimator with 113 features.\nFitting estimator with 112 features.\nFitting estimator with 111 features.\nFitting estimator with 110 features.\nFitting estimator with 109 features.\nFitting estimator with 108 features.\nFitting estimator with 107 features.\nFitting estimator with 106 features.\nFitting estimator with 105 features.\nFitting estimator with 104 features.\nFitting estimator with 103 features.\nFitting estimator with 102 features.\nFitting estimator with 101 features.\nFitting estimator with 100 features.\nFitting estimator with 99 features.\nFitting estimator with 98 features.\nFitting estimator with 97 features.\nFitting estimator with 96 features.\nFitting estimator with 95 features.\nFitting estimator with 94 features.\nFitting estimator with 93 features.\nFitting estimator with 92 features.\nFitting estimator with 91 features.\nFitting estimator with 90 features.\nFitting estimator with 89 features.\nFitting estimator with 88 features.\nFitting estimator with 87 features.\nFitting estimator with 86 features.\nFitting estimator with 85 features.\nFitting estimator with 84 features.\nFitting estimator with 83 features.\nFitting estimator with 82 features.\nFitting estimator with 81 features.\nFitting estimator with 80 features.\nFitting estimator with 79 features.\nFitting estimator with 78 features.\nFitting estimator with 77 features.\nFitting estimator with 76 features.\nFitting estimator with 75 features.\nFitting estimator with 74 features.\nFitting estimator with 73 features.\nFitting estimator with 72 features.\nFitting estimator with 71 features.\nFitting estimator with 70 features.\nFitting estimator with 69 features.\nFitting estimator with 68 features.\nFitting estimator with 67 features.\nFitting estimator with 66 features.\nFitting estimator with 65 features.\nFitting estimator with 64 features.\nFitting estimator with 63 features.\nFitting estimator with 62 features.\nFitting estimator with 61 features.\nFitting estimator with 60 features.\nFitting estimator with 59 features.\nFitting estimator with 58 features.\nFitting estimator with 57 features.\nFitting estimator with 56 features.\nFitting estimator with 55 features.\nFitting estimator with 54 features.\nFitting estimator with 53 features.\nFitting estimator with 52 features.\nFitting estimator with 51 features.\nFitting estimator with 50 features.\nFitting estimator with 49 features.\nFitting estimator with 48 features.\nFitting estimator with 47 features.\nFitting estimator with 46 features.\nFitting estimator with 45 features.\nFitting estimator with 44 features.\nFitting estimator with 43 features.\nFitting estimator with 42 features.\nFitting estimator with 41 features.\nFitting estimator with 40 features.\nFitting estimator with 39 features.\nFitting estimator with 38 features.\nFitting estimator with 37 features.\nFitting estimator with 36 features.\nFitting estimator with 35 features.\nFitting estimator with 34 features.\nFitting estimator with 33 features.\nFitting estimator with 32 features.\nFitting estimator with 31 features.\nFitting estimator with 30 features.\nFitting estimator with 29 features.\nFitting estimator with 28 features.\nFitting estimator with 27 features.\nFitting estimator with 26 features.\nFitting estimator with 25 features.\nFitting estimator with 24 features.\nFitting estimator with 23 features.\nFitting estimator with 22 features.\nFitting estimator with 21 features.\nFitting estimator with 20 features.\nFitting estimator with 19 features.\nFitting estimator with 18 features.\nFitting estimator with 17 features.\nFitting estimator with 16 features.\nFitting estimator with 15 features.\nFitting estimator with 14 features.\nFitting estimator with 13 features.\nFitting estimator with 12 features.\n","name":"stdout"},{"output_type":"stream","text":"Fitting estimator with 11 features.\nFitting estimator with 10 features.\nFitting estimator with 9 features.\nFitting estimator with 8 features.\nFitting estimator with 7 features.\nFitting estimator with 6 features.\nFitting estimator with 5 features.\nFitting estimator with 4 features.\nFitting estimator with 3 features.\nFitting estimator with 2 features.\nFitting estimator with 455 features.\nFitting estimator with 454 features.\nFitting estimator with 453 features.\nFitting estimator with 452 features.\nFitting estimator with 451 features.\nFitting estimator with 450 features.\nFitting estimator with 449 features.\nFitting estimator with 448 features.\nFitting estimator with 447 features.\nFitting estimator with 446 features.\nFitting estimator with 445 features.\nFitting estimator with 444 features.\nFitting estimator with 443 features.\nFitting estimator with 442 features.\nFitting estimator with 441 features.\nFitting estimator with 440 features.\nFitting estimator with 439 features.\nFitting estimator with 438 features.\nFitting estimator with 437 features.\nFitting estimator with 436 features.\nFitting estimator with 435 features.\nFitting estimator with 434 features.\nFitting estimator with 433 features.\nFitting estimator with 432 features.\nFitting estimator with 431 features.\nFitting estimator with 430 features.\nFitting estimator with 429 features.\nFitting estimator with 428 features.\nFitting estimator with 427 features.\nFitting estimator with 426 features.\nFitting estimator with 425 features.\nFitting estimator with 424 features.\nFitting estimator with 423 features.\nFitting estimator with 422 features.\nFitting estimator with 421 features.\nFitting estimator with 420 features.\nFitting estimator with 419 features.\nFitting estimator with 418 features.\nFitting estimator with 417 features.\nFitting estimator with 416 features.\nFitting estimator with 415 features.\nFitting estimator with 414 features.\nFitting estimator with 413 features.\nFitting estimator with 412 features.\nFitting estimator with 411 features.\nFitting estimator with 410 features.\nFitting estimator with 409 features.\nFitting estimator with 408 features.\nFitting estimator with 407 features.\nFitting estimator with 406 features.\nFitting estimator with 405 features.\nFitting estimator with 404 features.\nFitting estimator with 403 features.\nFitting estimator with 402 features.\nFitting estimator with 401 features.\nFitting estimator with 400 features.\nFitting estimator with 399 features.\nFitting estimator with 398 features.\nFitting estimator with 397 features.\nFitting estimator with 396 features.\nFitting estimator with 395 features.\nFitting estimator with 394 features.\nFitting estimator with 393 features.\nFitting estimator with 392 features.\nFitting estimator with 391 features.\nFitting estimator with 390 features.\nFitting estimator with 389 features.\nFitting estimator with 388 features.\nFitting estimator with 387 features.\nFitting estimator with 386 features.\nFitting estimator with 385 features.\nFitting estimator with 384 features.\nFitting estimator with 383 features.\nFitting estimator with 382 features.\nFitting estimator with 381 features.\nFitting estimator with 380 features.\nFitting estimator with 379 features.\nFitting estimator with 378 features.\nFitting estimator with 377 features.\nFitting estimator with 376 features.\nFitting estimator with 375 features.\nFitting estimator with 374 features.\nFitting estimator with 373 features.\nFitting estimator with 372 features.\nFitting estimator with 371 features.\nFitting estimator with 370 features.\nFitting estimator with 369 features.\nFitting estimator with 368 features.\nFitting estimator with 367 features.\nFitting estimator with 366 features.\nFitting estimator with 365 features.\nFitting estimator with 364 features.\nFitting estimator with 363 features.\nFitting estimator with 362 features.\nFitting estimator with 361 features.\nFitting estimator with 360 features.\nFitting estimator with 359 features.\nFitting estimator with 358 features.\nFitting estimator with 357 features.\nFitting estimator with 356 features.\nFitting estimator with 355 features.\nFitting estimator with 354 features.\nFitting estimator with 353 features.\nFitting estimator with 352 features.\nFitting estimator with 351 features.\nFitting estimator with 350 features.\nFitting estimator with 349 features.\nFitting estimator with 348 features.\nFitting estimator with 347 features.\nFitting estimator with 346 features.\nFitting estimator with 345 features.\nFitting estimator with 344 features.\nFitting estimator with 343 features.\nFitting estimator with 342 features.\nFitting estimator with 341 features.\nFitting estimator with 340 features.\nFitting estimator with 339 features.\nFitting estimator with 338 features.\nFitting estimator with 337 features.\nFitting estimator with 336 features.\nFitting estimator with 335 features.\nFitting estimator with 334 features.\nFitting estimator with 333 features.\nFitting estimator with 332 features.\nFitting estimator with 331 features.\nFitting estimator with 330 features.\nFitting estimator with 329 features.\nFitting estimator with 328 features.\nFitting estimator with 327 features.\nFitting estimator with 326 features.\nFitting estimator with 325 features.\nFitting estimator with 324 features.\nFitting estimator with 323 features.\nFitting estimator with 322 features.\nFitting estimator with 321 features.\nFitting estimator with 320 features.\nFitting estimator with 319 features.\nFitting estimator with 318 features.\nFitting estimator with 317 features.\nFitting estimator with 316 features.\nFitting estimator with 315 features.\nFitting estimator with 314 features.\nFitting estimator with 313 features.\nFitting estimator with 312 features.\nFitting estimator with 311 features.\nFitting estimator with 310 features.\nFitting estimator with 309 features.\nFitting estimator with 308 features.\nFitting estimator with 307 features.\nFitting estimator with 306 features.\nFitting estimator with 305 features.\nFitting estimator with 304 features.\nFitting estimator with 303 features.\nFitting estimator with 302 features.\nFitting estimator with 301 features.\nFitting estimator with 300 features.\nFitting estimator with 299 features.\nFitting estimator with 298 features.\nFitting estimator with 297 features.\nFitting estimator with 296 features.\nFitting estimator with 295 features.\nFitting estimator with 294 features.\nFitting estimator with 293 features.\nFitting estimator with 292 features.\nFitting estimator with 291 features.\nFitting estimator with 290 features.\nFitting estimator with 289 features.\nFitting estimator with 288 features.\nFitting estimator with 287 features.\nFitting estimator with 286 features.\nFitting estimator with 285 features.\nFitting estimator with 284 features.\nFitting estimator with 283 features.\nFitting estimator with 282 features.\nFitting estimator with 281 features.\nFitting estimator with 280 features.\nFitting estimator with 279 features.\nFitting estimator with 278 features.\nFitting estimator with 277 features.\nFitting estimator with 276 features.\nFitting estimator with 275 features.\nFitting estimator with 274 features.\nFitting estimator with 273 features.\nFitting estimator with 272 features.\nFitting estimator with 271 features.\nFitting estimator with 270 features.\nFitting estimator with 269 features.\nFitting estimator with 268 features.\nFitting estimator with 267 features.\nFitting estimator with 266 features.\nFitting estimator with 265 features.\nFitting estimator with 264 features.\nFitting estimator with 263 features.\nFitting estimator with 262 features.\nFitting estimator with 261 features.\nFitting estimator with 260 features.\nFitting estimator with 259 features.\nFitting estimator with 258 features.\nFitting estimator with 257 features.\nFitting estimator with 256 features.\nFitting estimator with 255 features.\nFitting estimator with 254 features.\nFitting estimator with 253 features.\nFitting estimator with 252 features.\nFitting estimator with 251 features.\nFitting estimator with 250 features.\nFitting estimator with 249 features.\nFitting estimator with 248 features.\nFitting estimator with 247 features.\nFitting estimator with 246 features.\nFitting estimator with 245 features.\nFitting estimator with 244 features.\n","name":"stdout"},{"output_type":"stream","text":"Fitting estimator with 243 features.\nFitting estimator with 242 features.\nFitting estimator with 241 features.\nFitting estimator with 240 features.\nFitting estimator with 239 features.\nFitting estimator with 238 features.\nFitting estimator with 237 features.\nFitting estimator with 236 features.\nFitting estimator with 235 features.\nFitting estimator with 234 features.\nFitting estimator with 233 features.\nFitting estimator with 232 features.\nFitting estimator with 231 features.\nFitting estimator with 230 features.\nFitting estimator with 229 features.\nFitting estimator with 228 features.\nFitting estimator with 227 features.\nFitting estimator with 226 features.\nFitting estimator with 225 features.\nFitting estimator with 224 features.\nFitting estimator with 223 features.\nFitting estimator with 222 features.\nFitting estimator with 221 features.\nFitting estimator with 220 features.\nFitting estimator with 219 features.\nFitting estimator with 218 features.\nFitting estimator with 217 features.\nFitting estimator with 216 features.\nFitting estimator with 215 features.\nFitting estimator with 214 features.\nFitting estimator with 213 features.\nFitting estimator with 212 features.\nFitting estimator with 211 features.\nFitting estimator with 210 features.\nFitting estimator with 209 features.\nFitting estimator with 208 features.\nFitting estimator with 207 features.\nFitting estimator with 206 features.\nFitting estimator with 205 features.\nFitting estimator with 204 features.\nFitting estimator with 203 features.\nFitting estimator with 202 features.\nFitting estimator with 201 features.\nFitting estimator with 200 features.\nFitting estimator with 199 features.\nFitting estimator with 198 features.\nFitting estimator with 197 features.\nFitting estimator with 196 features.\nFitting estimator with 195 features.\nFitting estimator with 194 features.\nFitting estimator with 193 features.\nFitting estimator with 192 features.\nFitting estimator with 191 features.\nFitting estimator with 190 features.\nFitting estimator with 189 features.\nFitting estimator with 188 features.\nFitting estimator with 187 features.\nFitting estimator with 186 features.\nFitting estimator with 185 features.\nFitting estimator with 184 features.\nFitting estimator with 183 features.\nFitting estimator with 182 features.\nFitting estimator with 181 features.\nFitting estimator with 180 features.\nFitting estimator with 179 features.\nFitting estimator with 178 features.\nFitting estimator with 177 features.\nFitting estimator with 176 features.\nFitting estimator with 175 features.\nFitting estimator with 174 features.\nFitting estimator with 173 features.\nFitting estimator with 172 features.\nFitting estimator with 171 features.\nFitting estimator with 170 features.\nFitting estimator with 169 features.\nFitting estimator with 168 features.\nFitting estimator with 167 features.\nFitting estimator with 166 features.\nFitting estimator with 165 features.\nFitting estimator with 164 features.\nFitting estimator with 163 features.\nFitting estimator with 162 features.\nFitting estimator with 161 features.\nFitting estimator with 160 features.\nFitting estimator with 159 features.\nFitting estimator with 158 features.\nFitting estimator with 157 features.\nFitting estimator with 156 features.\nFitting estimator with 155 features.\nFitting estimator with 154 features.\nFitting estimator with 153 features.\nFitting estimator with 152 features.\nFitting estimator with 151 features.\nFitting estimator with 150 features.\nFitting estimator with 149 features.\nFitting estimator with 148 features.\nFitting estimator with 147 features.\nFitting estimator with 146 features.\nFitting estimator with 145 features.\nFitting estimator with 144 features.\nFitting estimator with 143 features.\nFitting estimator with 142 features.\nFitting estimator with 141 features.\nFitting estimator with 140 features.\nFitting estimator with 139 features.\nFitting estimator with 138 features.\nFitting estimator with 137 features.\nFitting estimator with 136 features.\nFitting estimator with 135 features.\nFitting estimator with 134 features.\nFitting estimator with 133 features.\nFitting estimator with 132 features.\nFitting estimator with 131 features.\nFitting estimator with 130 features.\nFitting estimator with 129 features.\nFitting estimator with 128 features.\nFitting estimator with 127 features.\nFitting estimator with 126 features.\nFitting estimator with 125 features.\nFitting estimator with 124 features.\nFitting estimator with 123 features.\nFitting estimator with 122 features.\nFitting estimator with 121 features.\nFitting estimator with 120 features.\nFitting estimator with 119 features.\nFitting estimator with 118 features.\nFitting estimator with 117 features.\nFitting estimator with 116 features.\nFitting estimator with 115 features.\nFitting estimator with 114 features.\nFitting estimator with 113 features.\nFitting estimator with 112 features.\nFitting estimator with 111 features.\nFitting estimator with 110 features.\nFitting estimator with 109 features.\nFitting estimator with 108 features.\nFitting estimator with 107 features.\nFitting estimator with 106 features.\nFitting estimator with 105 features.\nFitting estimator with 104 features.\nFitting estimator with 103 features.\nFitting estimator with 102 features.\nFitting estimator with 101 features.\nFitting estimator with 100 features.\nFitting estimator with 99 features.\nFitting estimator with 98 features.\nFitting estimator with 97 features.\nFitting estimator with 96 features.\nFitting estimator with 95 features.\nFitting estimator with 94 features.\nFitting estimator with 93 features.\nFitting estimator with 92 features.\nFitting estimator with 91 features.\nFitting estimator with 90 features.\nFitting estimator with 89 features.\nFitting estimator with 88 features.\nFitting estimator with 87 features.\nFitting estimator with 86 features.\nFitting estimator with 85 features.\nFitting estimator with 84 features.\nFitting estimator with 83 features.\nFitting estimator with 82 features.\nFitting estimator with 81 features.\nFitting estimator with 80 features.\nFitting estimator with 79 features.\nFitting estimator with 78 features.\nFitting estimator with 77 features.\nFitting estimator with 76 features.\nFitting estimator with 75 features.\nFitting estimator with 74 features.\nFitting estimator with 73 features.\nFitting estimator with 72 features.\nFitting estimator with 71 features.\nFitting estimator with 70 features.\nFitting estimator with 69 features.\nFitting estimator with 68 features.\nFitting estimator with 67 features.\nFitting estimator with 66 features.\nFitting estimator with 65 features.\nFitting estimator with 64 features.\nFitting estimator with 63 features.\nFitting estimator with 62 features.\nFitting estimator with 61 features.\nFitting estimator with 60 features.\nFitting estimator with 59 features.\nFitting estimator with 58 features.\nFitting estimator with 57 features.\nOptimal number of features : 56\nBest features : Index(['abs_max_roll_mean_10000', 'autocorrelation_100',\n       'autocorrelation_10000', 'autocorrelation_5', 'autocorrelation_500',\n       'classic_sta_lta6_mean', 'count_big_100000_less_threshold_10',\n       'ffti_abs_percentile_10', 'ffti_av_change_abs_roll_std_10000',\n       'ffti_c3_500', 'ffti_exp_Moving_average_30000_std', 'ffti_kstat_1',\n       'ffti_mean_change_rate_last_10000', 'ffti_mean_first_50000',\n       'ffti_min_roll_std_500', 'ffti_percentile_roll_mean_20_window_10000',\n       'ffti_percentile_roll_mean_40_window_500',\n       'ffti_percentile_roll_mean_50_window_100',\n       'ffti_percentile_roll_mean_50_window_10000',\n       'ffti_percentile_roll_std_1_window_100',\n       'ffti_percentile_roll_std_25_window_10000',\n       'ffti_percentile_roll_std_30_window_1000',\n       'ffti_percentile_roll_std_40_window_100',\n       'ffti_percentile_roll_std_40_window_1000',\n       'ffti_percentile_roll_std_50_window_100', 'ffti_range_-4000_-3000',\n       'ffti_skew', 'ffti_spkt_welch_density_100', 'ffti_spkt_welch_density_5',\n       'ffti_spkt_welch_density_50', 'fftr_binned_entropy_50',\n       'fftr_max_last_1000', 'fftr_max_roll_mean_500',\n       'fftr_mean_change_rate_first_1000', 'fftr_mean_change_rate_first_10000',\n       'fftr_min_roll_std_10', 'fftr_percentile_roll_mean_60_window_10',\n       'fftr_percentile_roll_mean_90_window_10000',\n       'fftr_percentile_roll_std_10_window_1000',\n       'fftr_percentile_roll_std_1_window_1000',\n       'fftr_percentile_roll_std_20_window_500',\n       'fftr_percentile_roll_std_30_window_1000',\n       'fftr_percentile_roll_std_40_window_100', 'fftr_time_rev_asym_stat_100',\n       'gmean', 'max_to_min', 'mean_change_rate_first_1000',\n       'mean_change_rate_first_10000', 'mean_change_rate_last_1000',\n       'mean_change_rate_last_10000', 'num_peaks_10', 'num_peaks_20',\n       'percentile_roll_std_10_window_10000',\n       'percentile_roll_std_1_window_1000', 'range_-1000_0',\n       'spkt_welch_density_100'],\n      dtype='object')\nCPU times: user 20min 17s, sys: 636 ms, total: 20min 17s\nWall time: 20min 15s\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"Filter scaled train and test data with optimal features."},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train_corr_rfecv = x_train_corr[x_train_corr.columns[rfecv.support_].values]\nx_valid_corr_rfecv = x_valid_corr[x_valid_corr.columns[rfecv.support_].values]\nX_test_scaled_corr_refcv = X_test_scaled_corr[X_test_scaled_corr.columns[rfecv.support_].values]","execution_count":12,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Training Shape\", x_train_corr_rfecv.shape)\nprint(\"Validating Shape\", x_valid_corr_rfecv.shape)\nprint(\"Testing Shape\", X_test_scaled_corr_refcv.shape)","execution_count":13,"outputs":[{"output_type":"stream","text":"Training Shape (3356, 56)\nValidating Shape (839, 56)\nTesting Shape (2624, 56)\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"<h1>Cross Validator Functions and Hyper Parameter Optimizations</h1>"},{"metadata":{"trusted":true},"cell_type":"code","source":"#import required packages\nimport lightgbm as lgb\nimport xgboost as xgb\nimport catboost as cb\nimport gc\nfrom hyperopt import hp, tpe, Trials, STATUS_OK\nfrom hyperopt.fmin import fmin\nfrom hyperopt.pyll.stochastic import sample\n#optional but advised\nimport warnings\nwarnings.filterwarnings('ignore')\n\n#GLOBAL HYPEROPT PARAMETERS\nNUM_EVALS = 1000 #number of hyperopt evaluation rounds\nN_FOLDS = 5 #number of cross-validation folds on data in each evaluation round\n\n#LIGHTGBM PARAMETERS\nLGBM_MAX_LEAVES = 2**11 #maximum number of leaves per tree for LightGBM\nLGBM_MAX_DEPTH = 25 #maximum tree depth for LightGBM\nEVAL_METRIC_LGBM_REG = 'mae' #LightGBM regression metric. Note that 'rmse' is more commonly used \nEVAL_METRIC_LGBM_CLASS = 'auc'#LightGBM classification metric\n\n#XGBOOST PARAMETERS\nXGB_MAX_LEAVES = 2**12 #maximum number of leaves when using histogram splitting\nXGB_MAX_DEPTH = 25 #maximum tree depth for XGBoost\nEVAL_METRIC_XGB_REG = 'mae' #XGBoost regression metric\nEVAL_METRIC_XGB_CLASS = 'auc' #XGBoost classification metric\n\n#CATBOOST PARAMETERS\nCB_MAX_DEPTH = 8 #maximum tree depth in CatBoost\nOBJECTIVE_CB_REG = 'MAE' #CatBoost regression metric\nOBJECTIVE_CB_CLASS = 'Logloss' #CatBoost classification metric\n\ndef quick_hyperopt(data, labels, package='lgbm', num_evals=NUM_EVALS, diagnostic=False):\n    \n    #==========\n    #LightGBM\n    #==========\n    \n    if package=='lgbm':\n        \n        print('Running {} rounds of LightGBM parameter optimisation:'.format(num_evals))\n        #clear space\n        gc.collect()\n        \n        integer_params = ['max_depth',\n                         'num_leaves',\n                          'max_bin',\n                         'min_data_in_leaf',\n                         'min_data_in_bin']\n        \n        def objective(space_params):\n            \n            #cast integer params from float to int\n            for param in integer_params:\n                space_params[param] = int(space_params[param])\n            \n            #extract nested conditional parameters\n            if space_params['boosting']['boosting'] == 'goss':\n                top_rate = space_params['boosting'].get('top_rate')\n                other_rate = space_params['boosting'].get('other_rate')\n                #0 <= top_rate + other_rate <= 1\n                top_rate = max(top_rate, 0)\n                top_rate = min(top_rate, 0.5)\n                other_rate = max(other_rate, 0)\n                other_rate = min(other_rate, 0.5)\n                space_params['top_rate'] = top_rate\n                space_params['other_rate'] = other_rate\n            \n            subsample = space_params['boosting'].get('subsample', 1.0)\n            space_params['boosting'] = space_params['boosting']['boosting']\n            space_params['subsample'] = subsample\n            \n            #for classification, set stratified=True and metrics=EVAL_METRIC_LGBM_CLASS\n            cv_results = lgb.cv(space_params, train, nfold = N_FOLDS, stratified=False,\n                                early_stopping_rounds=100, metrics=EVAL_METRIC_LGBM_REG, seed=42)\n            \n            best_loss = cv_results['l1-mean'][-1] #'l2-mean' for rmse\n            #for classification, comment out the line above and uncomment the line below:\n            #best_loss = 1 - cv_results['auc-mean'][-1]\n            #if necessary, replace 'auc-mean' with '[your-preferred-metric]-mean'\n            return{'loss':best_loss, 'status': STATUS_OK }\n        \n        train = lgb.Dataset(data, labels)\n                \n        #integer and string parameters, used with hp.choice()\n        boosting_list = [{'boosting': 'gbdt',\n                          'subsample': hp.uniform('subsample', 0.5, 1)},\n                         {'boosting': 'goss',\n                          'subsample': 1.0,\n                         'top_rate': hp.uniform('top_rate', 0, 0.5),\n                         'other_rate': hp.uniform('other_rate', 0, 0.5)}] #if including 'dart', make sure to set 'n_estimators'\n        metric_list = ['MAE', 'RMSE'] \n        #for classification comment out the line above and uncomment the line below\n        #modify as required for other classification metrics classification\n        #metric_list = ['auc']\n        objective_list_reg = ['huber', 'gamma', 'fair', 'tweedie']\n        objective_list_class = ['logloss', 'cross_entropy']\n        #for classification set objective_list = objective_list_class\n        objective_list = objective_list_reg\n\n        space ={'boosting' : hp.choice('boosting', boosting_list),\n                'num_leaves' : hp.quniform('num_leaves', 2, LGBM_MAX_LEAVES, 1),\n                'max_depth': hp.quniform('max_depth', 2, LGBM_MAX_DEPTH, 1),\n                'max_bin': hp.quniform('max_bin', 32, 255, 1),\n                'min_data_in_leaf': hp.quniform('min_data_in_leaf', 1, 256, 1),\n                'min_data_in_bin': hp.quniform('min_data_in_bin', 1, 256, 1),\n                'lambda_l1' : hp.uniform('lambda_l1', 0, 5),\n                'lambda_l2' : hp.uniform('lambda_l2', 0, 5),\n                'learning_rate' : hp.loguniform('learning_rate', np.log(0.005), np.log(0.2)),\n                'metric' : hp.choice('metric', metric_list),\n                'objective' : hp.choice('objective', objective_list),\n                'feature_fraction' : hp.quniform('feature_fraction', 0.5, 1, 0.01),\n                'bagging_fraction' : hp.quniform('bagging_fraction', 0.5, 1, 0.01)\n            }\n        \n        #optional: activate GPU for LightGBM\n        #follow compilation steps here:\n        #https://www.kaggle.com/vinhnguyen/gpu-acceleration-for-lightgbm/\n        #then uncomment lines below:\n        #space['device'] = 'gpu'\n        #space['gpu_platform_id'] = 0,\n        #space['gpu_device_id'] =  0\n\n        trials = Trials()\n        best = fmin(fn=objective,\n                    space=space,\n                    algo=tpe.suggest,\n                    max_evals=num_evals, \n                    trials=trials)\n                \n        #fmin() will return the index of values chosen from the lists/arrays in 'space'\n        #to obtain actual values, index values are used to subset the original lists/arrays\n        best['boosting'] = boosting_list[best['boosting']]['boosting']#nested dict, index twice\n        best['metric'] = metric_list[best['metric']]\n        best['objective'] = objective_list[best['objective']]\n        \n        #cast floats of integer params to int\n        for param in integer_params:\n            best[param] = int(best[param])\n            \n        print('{' + '\\n'.join('{}: {}'.format(k, v) for k, v in best.items()) + '}')\n        if diagnostic:\n            return(best, trials)\n        else:\n            return(best)\n    \n    #==========\n    #XGBoost\n    #==========\n    \n    if package=='xgb':\n        \n        print('Running {} rounds of XGBoost parameter optimisation:'.format(num_evals))\n        #clear space\n        gc.collect()\n        \n        integer_params = ['max_depth']\n        \n        def objective(space_params):\n            \n            for param in integer_params:\n                space_params[param] = int(space_params[param])\n                \n            #extract multiple nested tree_method conditional parameters\n            #libera te tutemet ex inferis\n            if space_params['tree_method']['tree_method'] == 'hist':\n                max_bin = space_params['tree_method'].get('max_bin')\n                space_params['max_bin'] = int(max_bin)\n                if space_params['tree_method']['grow_policy']['grow_policy']['grow_policy'] == 'depthwise':\n                    grow_policy = space_params['tree_method'].get('grow_policy').get('grow_policy').get('grow_policy')\n                    space_params['grow_policy'] = grow_policy\n                    space_params['tree_method'] = 'hist'\n                else:\n                    max_leaves = space_params['tree_method']['grow_policy']['grow_policy'].get('max_leaves')\n                    space_params['grow_policy'] = 'lossguide'\n                    space_params['max_leaves'] = int(max_leaves)\n                    space_params['tree_method'] = 'hist'\n            else:\n                space_params['tree_method'] = space_params['tree_method'].get('tree_method')\n                \n            #for classification replace EVAL_METRIC_XGB_REG with EVAL_METRIC_XGB_CLASS\n            cv_results = xgb.cv(space_params, train, nfold=N_FOLDS, metrics=[EVAL_METRIC_XGB_REG],\n                             early_stopping_rounds=100, stratified=False, seed=42)\n            \n            best_loss = cv_results['test-mae-mean'].iloc[-1] #or 'test-rmse-mean' if using RMSE\n            #for classification, comment out the line above and uncomment the line below:\n            #best_loss = 1 - cv_results['test-auc-mean'].iloc[-1]\n            #if necessary, replace 'test-auc-mean' with 'test-[your-preferred-metric]-mean'\n            return{'loss':best_loss, 'status': STATUS_OK }\n        \n        train = xgb.DMatrix(data, labels)\n        \n        #integer and string parameters, used with hp.choice()\n        boosting_list = ['gbtree', 'gblinear'] #if including 'dart', make sure to set 'n_estimators'\n        metric_list = ['MAE', 'RMSE'] \n        #for classification comment out the line above and uncomment the line below\n        #metric_list = ['auc']\n        #modify as required for other classification metrics classification\n        \n        tree_method = [{'tree_method' : 'exact'},\n               {'tree_method' : 'approx'},\n               {'tree_method' : 'hist',\n                'max_bin': hp.quniform('max_bin', 2**3, 2**7, 1),\n                'grow_policy' : {'grow_policy': {'grow_policy':'depthwise'},\n                                'grow_policy' : {'grow_policy':'lossguide',\n                                                  'max_leaves': hp.quniform('max_leaves', 32, XGB_MAX_LEAVES, 1)}}}]\n        \n        #if using GPU, replace 'exact' with 'gpu_exact' and 'hist' with\n        #'gpu_hist' in the nested dictionary above\n        \n        objective_list_reg = ['reg:linear', 'reg:gamma', 'reg:tweedie']\n        objective_list_class = ['reg:logistic', 'binary:logistic']\n        #for classification change line below to 'objective_list = objective_list_class'\n        objective_list = objective_list_reg\n        \n        space ={'boosting' : hp.choice('boosting', boosting_list),\n                'tree_method' : hp.choice('tree_method', tree_method),\n                'max_depth': hp.quniform('max_depth', 2, XGB_MAX_DEPTH, 1),\n                'reg_alpha' : hp.uniform('reg_alpha', 0, 5),\n                'reg_lambda' : hp.uniform('reg_lambda', 0, 5),\n                'min_child_weight' : hp.uniform('min_child_weight', 0, 5),\n                'gamma' : hp.uniform('gamma', 0, 5),\n                'learning_rate' : hp.loguniform('learning_rate', np.log(0.005), np.log(0.2)),\n                'eval_metric' : hp.choice('eval_metric', metric_list),\n                'objective' : hp.choice('objective', objective_list),\n                'colsample_bytree' : hp.quniform('colsample_bytree', 0.1, 1, 0.01),\n                'colsample_bynode' : hp.quniform('colsample_bynode', 0.1, 1, 0.01),\n                'colsample_bylevel' : hp.quniform('colsample_bylevel', 0.1, 1, 0.01),\n                'subsample' : hp.quniform('subsample', 0.5, 1, 0.05),\n                'nthread' : -1\n            }\n        \n        #optional: activate GPU for XGBoost\n        #uncomment line below\n        #space['tree_method'] = 'gpu_hist'\n        \n        trials = Trials()\n        best = fmin(fn=objective,\n                    space=space,\n                    algo=tpe.suggest,\n                    max_evals=num_evals, \n                    trials=trials)\n        \n        best['tree_method'] = tree_method[best['tree_method']]['tree_method']\n        best['boosting'] = boosting_list[best['boosting']]\n        best['eval_metric'] = metric_list[best['eval_metric']]\n        best['objective'] = objective_list[best['objective']]\n        \n        #cast floats of integer params to int\n        for param in integer_params:\n            best[param] = int(best[param])\n        if 'max_leaves' in best:\n            best['max_leaves'] = int(best['max_leaves'])\n        if 'max_bin' in best:\n            best['max_bin'] = int(best['max_bin'])\n        \n        print('{' + '\\n'.join('{}: {}'.format(k, v) for k, v in best.items()) + '}')\n        \n        if diagnostic:\n            return(best, trials)\n        else:\n            return(best)\n    \n    #==========\n    #CatBoost\n    #==========\n    \n    if package=='cb':\n        \n        print('Running {} rounds of CatBoost parameter optimisation:'.format(num_evals))\n        \n        #clear memory \n        gc.collect()\n            \n        integer_params = ['depth',\n                          #'one_hot_max_size', #for categorical data\n                          'min_data_in_leaf',\n                          'max_bin']\n        \n        def objective(space_params):\n                        \n            #cast integer params from float to int\n            for param in integer_params:\n                space_params[param] = int(space_params[param])\n                \n            #extract nested conditional parameters\n            if space_params['bootstrap_type']['bootstrap_type'] == 'Bayesian':\n                bagging_temp = space_params['bootstrap_type'].get('bagging_temperature')\n                space_params['bagging_temperature'] = bagging_temp\n                \n            #if space_params['grow_policy']['grow_policy'] == 'LossGuide':\n            #    max_leaves = space_params['grow_policy'].get('max_leaves')\n            #    space_params['max_leaves'] = int(max_leaves)\n                \n            space_params['bootstrap_type'] = space_params['bootstrap_type']['bootstrap_type']\n            #space_params['grow_policy'] = space_params['grow_policy']['grow_policy']\n                           \n            #random_strength cannot be < 0\n            space_params['random_strength'] = max(space_params['random_strength'], 0)\n            #fold_len_multiplier cannot be < 1\n            space_params['fold_len_multiplier'] = max(space_params['fold_len_multiplier'], 1)\n                       \n            #for classification set stratified=True\n            cv_results = cb.cv(train, space_params, fold_count=N_FOLDS, \n                             early_stopping_rounds=25, stratified=False, partition_random_seed=42)\n           \n            best_loss = cv_results['test-MAE-mean'].iloc[-1] #'test-RMSE-mean' for RMSE\n            #for classification, comment out the line above and uncomment the line below:\n            #best_loss = cv_results['test-Logloss-mean'].iloc[-1]\n            #if necessary, replace 'test-Logloss-mean' with 'test-[your-preferred-metric]-mean'\n            \n            return{'loss':best_loss, 'status': STATUS_OK}\n        \n        train = cb.Pool(data, labels.astype('float32'))\n        \n        #integer and string parameters, used with hp.choice()\n        bootstrap_type = [\n                          {'bootstrap_type':'Poisson'}, \n                           {'bootstrap_type':'Bayesian',\n                            'bagging_temperature' : hp.loguniform('bagging_temperature', np.log(1), np.log(50))},\n                          {'bootstrap_type':'Bernoulli'}] \n        LEB = ['No', 'AnyImprovement', 'Armijo'] #remove 'Armijo' if not using GPU\n        #score_function = ['Correlation', 'L2', 'NewtonCorrelation', 'NewtonL2']\n        grow_policy = [{'grow_policy':'SymmetricTree'},\n                       #{'grow_policy':'Depthwise'},\n                       #{'grow_policy':'Lossguide',\n                       # 'max_leaves': hp.quniform('max_leaves', 2, 32, 1)}\n                      ]\n        eval_metric_list_reg = ['MAE', 'RMSE', 'Poisson']\n        eval_metric_list_class = ['Logloss', 'AUC', 'F1']\n        #for classification change line below to 'eval_metric_list = eval_metric_list_class'\n        eval_metric_list = eval_metric_list_reg\n                \n        space ={'depth': hp.quniform('depth', 2, CB_MAX_DEPTH, 1),\n                'max_bin' : hp.quniform('max_bin', 1, 32, 1), #if using CPU just set this to 254\n                #'max_bin': 254,\n                'l2_leaf_reg' : hp.uniform('l2_leaf_reg', 0, 5),\n                'min_data_in_leaf' : hp.quniform('min_data_in_leaf', 1, 50, 1),\n                'random_strength' : hp.loguniform('random_strength', np.log(0.005), np.log(5)),\n                #'one_hot_max_size' : hp.quniform('one_hot_max_size', 2, 16, 1), #uncomment if using categorical features\n                'bootstrap_type' : hp.choice('bootstrap_type', bootstrap_type),\n                'learning_rate' : hp.uniform('learning_rate', 0.05, 0.25),\n                'eval_metric' : hp.choice('eval_metric', eval_metric_list),\n                'objective' : OBJECTIVE_CB_REG,\n                #'score_function' : hp.choice('score_function', score_function), #crashes kernel - reason unknown\n                'leaf_estimation_backtracking' : hp.choice('leaf_estimation_backtracking', LEB),\n                #'grow_policy': hp.choice('grow_policy', grow_policy),\n                'colsample_bylevel' : hp.quniform('colsample_bylevel', 0.1, 1, 0.01),# CPU only\n                'fold_len_multiplier' : hp.loguniform('fold_len_multiplier', np.log(1.01), np.log(2.5)),\n                'od_type' : 'Iter',\n                'od_wait' : 25,\n                'task_type' : 'GPU',\n                'verbose' : 0\n            }\n        \n        #optional: run CatBoost without GPU\n        #uncomment line below\n        #space['task_type'] = 'CPU'\n            \n        trials = Trials()\n        best = fmin(fn=objective,\n                    space=space,\n                    algo=tpe.suggest,\n                    max_evals=num_evals, \n                    trials=trials)\n        \n        #unpack nested dicts first\n        best['bootstrap_type'] = bootstrap_type[best['bootstrap_type']]['bootstrap_type']\n        #best['grow_policy'] = grow_policy[best['grow_policy']]['grow_policy']\n        best['eval_metric'] = eval_metric_list[best['eval_metric']]\n        \n        best['score_function'] = score_function[best['score_function']] \n        #best['leaf_estimation_method'] = LEM[best['leaf_estimation_method']] #CPU only\n        best['leaf_estimation_backtracking'] = LEB[best['leaf_estimation_backtracking']]        \n        \n        #cast floats of integer params to int\n        for param in integer_params:\n            best[param] = int(best[param])\n        if 'max_leaves' in best:\n            best['max_leaves'] = int(best['max_leaves'])\n        \n        print('{' + '\\n'.join('{}: {}'.format(k, v) for k, v in best.items()) + '}')\n        \n        if diagnostic:\n            return(best, trials)\n        else:\n            return(best)\n    \n    else:\n        print('Package not recognised. Please use \"lgbm\" for LightGBM, \"xgb\" for XGBoost or \"cb\" for CatBoost.')     ","execution_count":50,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2>Optimize Hyperparameters for XGB</h2>"},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb_params = quick_hyperopt(x_train_corr_rfecv, y_train, 'xgb', 2500)\nnp.save('xgb_params.npy', xgb_params)","execution_count":21,"outputs":[{"output_type":"stream","text":"Running 2500 rounds of XGBoost parameter optimisation:\n100%|| 2500/2500 [47:42<00:00,  1.84s/it, best loss: 2.0891872]          \n{boosting: gbtree\ncolsample_bylevel: 0.74\ncolsample_bynode: 0.31\ncolsample_bytree: 1.0\neval_metric: MAE\ngamma: 1.2861053004892011\nlearning_rate: 0.19987589307498949\nmax_depth: 4\nmin_child_weight: 4.992656946548014\nobjective: reg:linear\nreg_alpha: 1.5106408696730107\nreg_lambda: 0.021714935285173147\nsubsample: 0.9500000000000001\ntree_method: approx}\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"<h2>Optimize Hyperparameters for LGB</h2>"},{"metadata":{"trusted":true},"cell_type":"code","source":"lgbm_params = quick_hyperopt(x_train_corr_rfecv, y_train, 'lgbm', 1500)\nnp.save('lgbm_params.npy', lgbm_params)","execution_count":23,"outputs":[{"output_type":"stream","text":"Running 1500 rounds of LightGBM parameter optimisation:\n100%|| 1500/1500 [22:19<00:00,  1.11s/it, best loss: 2.058066255151868] \n{bagging_fraction: 0.79\nboosting: gbdt\nfeature_fraction: 0.78\nlambda_l1: 3.9138829422260972\nlambda_l2: 4.0651574498215\nlearning_rate: 0.10716823366484945\nmax_bin: 48\nmax_depth: 23\nmetric: RMSE\nmin_data_in_bin: 72\nmin_data_in_leaf: 180\nnum_leaves: 714\nobjective: fair\nsubsample: 0.6862953597478736}\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"<h2>Optimize Hyperparameters for Catboost</h2>"},{"metadata":{"trusted":true},"cell_type":"code","source":"#cb_params = quick_hyperopt(x_train_corr_rfecv, y_train, 'cb', 30)\n#np.save('cb_params.npy', cb_params)","execution_count":52,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h1>Training & Prediction</h1>"},{"metadata":{},"cell_type":"markdown","source":"Read submission file"},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.read_csv('../input/LANL-Earthquake-Prediction/sample_submission.csv')","execution_count":26,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"<h2>XGB</h2>"},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb_params['eval_metric'] = 'mae'\nmodel = xgb.XGBModel(**xgb_params)\nmodel.fit(x_train_corr_rfecv, y_train, \n          eval_set=[(x_train_corr_rfecv, y_train), (x_valid_corr_rfecv, y_valid)], \n          verbose=True)\n\ntest_preds = model.predict(X_test_scaled_corr_refcv)\ntest_segs = X_test_scaled_corr_refcv.index\nsubmission_xgb = pd.DataFrame(columns=['seg_id', 'time_to_failure'])\nsubmission_xgb.seg_id = test_segs\nsubmission_xgb.time_to_failure = test_preds\nsubmission_xgb.to_csv('submission_xgb.csv', index=False, line_terminator='\\n')","execution_count":53,"outputs":[{"output_type":"stream","text":"[0]\tvalidation_0-mae:4.31092\tvalidation_1-mae:4.1415\n[1]\tvalidation_0-mae:3.60703\tvalidation_1-mae:3.47184\n[2]\tvalidation_0-mae:3.07145\tvalidation_1-mae:2.97032\n[3]\tvalidation_0-mae:2.67368\tvalidation_1-mae:2.61635\n[4]\tvalidation_0-mae:2.39812\tvalidation_1-mae:2.37597\n[5]\tvalidation_0-mae:2.21353\tvalidation_1-mae:2.23165\n[6]\tvalidation_0-mae:2.09747\tvalidation_1-mae:2.14772\n[7]\tvalidation_0-mae:2.02604\tvalidation_1-mae:2.09599\n[8]\tvalidation_0-mae:1.98029\tvalidation_1-mae:2.06859\n[9]\tvalidation_0-mae:1.95017\tvalidation_1-mae:2.06072\n[10]\tvalidation_0-mae:1.92694\tvalidation_1-mae:2.06137\n[11]\tvalidation_0-mae:1.90899\tvalidation_1-mae:2.06409\n[12]\tvalidation_0-mae:1.89019\tvalidation_1-mae:2.06667\n[13]\tvalidation_0-mae:1.87888\tvalidation_1-mae:2.06832\n[14]\tvalidation_0-mae:1.86728\tvalidation_1-mae:2.06479\n[15]\tvalidation_0-mae:1.85645\tvalidation_1-mae:2.07068\n[16]\tvalidation_0-mae:1.84272\tvalidation_1-mae:2.07172\n[17]\tvalidation_0-mae:1.82937\tvalidation_1-mae:2.07533\n[18]\tvalidation_0-mae:1.82033\tvalidation_1-mae:2.07842\n[19]\tvalidation_0-mae:1.80668\tvalidation_1-mae:2.08375\n[20]\tvalidation_0-mae:1.79221\tvalidation_1-mae:2.08196\n[21]\tvalidation_0-mae:1.77937\tvalidation_1-mae:2.07942\n[22]\tvalidation_0-mae:1.7703\tvalidation_1-mae:2.08722\n[23]\tvalidation_0-mae:1.75689\tvalidation_1-mae:2.08987\n[24]\tvalidation_0-mae:1.74919\tvalidation_1-mae:2.08981\n[25]\tvalidation_0-mae:1.74246\tvalidation_1-mae:2.08966\n[26]\tvalidation_0-mae:1.73378\tvalidation_1-mae:2.09405\n[27]\tvalidation_0-mae:1.72308\tvalidation_1-mae:2.09321\n[28]\tvalidation_0-mae:1.71061\tvalidation_1-mae:2.0967\n[29]\tvalidation_0-mae:1.6991\tvalidation_1-mae:2.09708\n[30]\tvalidation_0-mae:1.69069\tvalidation_1-mae:2.09847\n[31]\tvalidation_0-mae:1.6811\tvalidation_1-mae:2.0997\n[32]\tvalidation_0-mae:1.67001\tvalidation_1-mae:2.10648\n[33]\tvalidation_0-mae:1.65963\tvalidation_1-mae:2.11046\n[34]\tvalidation_0-mae:1.64824\tvalidation_1-mae:2.11013\n[35]\tvalidation_0-mae:1.63803\tvalidation_1-mae:2.11663\n[36]\tvalidation_0-mae:1.62794\tvalidation_1-mae:2.11378\n[37]\tvalidation_0-mae:1.62163\tvalidation_1-mae:2.11448\n[38]\tvalidation_0-mae:1.61576\tvalidation_1-mae:2.11147\n[39]\tvalidation_0-mae:1.605\tvalidation_1-mae:2.1096\n[40]\tvalidation_0-mae:1.5976\tvalidation_1-mae:2.11524\n[41]\tvalidation_0-mae:1.58873\tvalidation_1-mae:2.11724\n[42]\tvalidation_0-mae:1.57875\tvalidation_1-mae:2.11694\n[43]\tvalidation_0-mae:1.57114\tvalidation_1-mae:2.12068\n[44]\tvalidation_0-mae:1.56372\tvalidation_1-mae:2.11967\n[45]\tvalidation_0-mae:1.55832\tvalidation_1-mae:2.11974\n[46]\tvalidation_0-mae:1.54857\tvalidation_1-mae:2.12447\n[47]\tvalidation_0-mae:1.54273\tvalidation_1-mae:2.12368\n[48]\tvalidation_0-mae:1.5347\tvalidation_1-mae:2.11879\n[49]\tvalidation_0-mae:1.5214\tvalidation_1-mae:2.11996\n[50]\tvalidation_0-mae:1.51143\tvalidation_1-mae:2.1179\n[51]\tvalidation_0-mae:1.50463\tvalidation_1-mae:2.1192\n[52]\tvalidation_0-mae:1.49503\tvalidation_1-mae:2.12127\n[53]\tvalidation_0-mae:1.48801\tvalidation_1-mae:2.12387\n[54]\tvalidation_0-mae:1.48136\tvalidation_1-mae:2.12413\n[55]\tvalidation_0-mae:1.47329\tvalidation_1-mae:2.12824\n[56]\tvalidation_0-mae:1.46729\tvalidation_1-mae:2.13095\n[57]\tvalidation_0-mae:1.46154\tvalidation_1-mae:2.13216\n[58]\tvalidation_0-mae:1.45108\tvalidation_1-mae:2.13213\n[59]\tvalidation_0-mae:1.44616\tvalidation_1-mae:2.12709\n[60]\tvalidation_0-mae:1.44004\tvalidation_1-mae:2.13054\n[61]\tvalidation_0-mae:1.4342\tvalidation_1-mae:2.13517\n[62]\tvalidation_0-mae:1.42777\tvalidation_1-mae:2.13563\n[63]\tvalidation_0-mae:1.41702\tvalidation_1-mae:2.1377\n[64]\tvalidation_0-mae:1.41072\tvalidation_1-mae:2.13697\n[65]\tvalidation_0-mae:1.40281\tvalidation_1-mae:2.1368\n[66]\tvalidation_0-mae:1.39872\tvalidation_1-mae:2.13752\n[67]\tvalidation_0-mae:1.39127\tvalidation_1-mae:2.13698\n[68]\tvalidation_0-mae:1.38395\tvalidation_1-mae:2.1399\n[69]\tvalidation_0-mae:1.37704\tvalidation_1-mae:2.14163\n[70]\tvalidation_0-mae:1.36992\tvalidation_1-mae:2.1413\n[71]\tvalidation_0-mae:1.36049\tvalidation_1-mae:2.14272\n[72]\tvalidation_0-mae:1.35189\tvalidation_1-mae:2.14492\n[73]\tvalidation_0-mae:1.34401\tvalidation_1-mae:2.14684\n[74]\tvalidation_0-mae:1.33991\tvalidation_1-mae:2.14431\n[75]\tvalidation_0-mae:1.33309\tvalidation_1-mae:2.14288\n[76]\tvalidation_0-mae:1.32763\tvalidation_1-mae:2.14409\n[77]\tvalidation_0-mae:1.32053\tvalidation_1-mae:2.14197\n[78]\tvalidation_0-mae:1.31101\tvalidation_1-mae:2.14802\n[79]\tvalidation_0-mae:1.30497\tvalidation_1-mae:2.14928\n[80]\tvalidation_0-mae:1.29794\tvalidation_1-mae:2.15035\n[81]\tvalidation_0-mae:1.28896\tvalidation_1-mae:2.1496\n[82]\tvalidation_0-mae:1.28094\tvalidation_1-mae:2.1526\n[83]\tvalidation_0-mae:1.27531\tvalidation_1-mae:2.15125\n[84]\tvalidation_0-mae:1.26585\tvalidation_1-mae:2.15509\n[85]\tvalidation_0-mae:1.26139\tvalidation_1-mae:2.15547\n[86]\tvalidation_0-mae:1.25391\tvalidation_1-mae:2.15666\n[87]\tvalidation_0-mae:1.24693\tvalidation_1-mae:2.1553\n[88]\tvalidation_0-mae:1.2408\tvalidation_1-mae:2.1587\n[89]\tvalidation_0-mae:1.23472\tvalidation_1-mae:2.15851\n[90]\tvalidation_0-mae:1.22747\tvalidation_1-mae:2.16041\n[91]\tvalidation_0-mae:1.22311\tvalidation_1-mae:2.15955\n[92]\tvalidation_0-mae:1.21886\tvalidation_1-mae:2.1626\n[93]\tvalidation_0-mae:1.21342\tvalidation_1-mae:2.16628\n[94]\tvalidation_0-mae:1.20887\tvalidation_1-mae:2.16513\n[95]\tvalidation_0-mae:1.20459\tvalidation_1-mae:2.16699\n[96]\tvalidation_0-mae:1.19834\tvalidation_1-mae:2.16841\n[97]\tvalidation_0-mae:1.19412\tvalidation_1-mae:2.16859\n[98]\tvalidation_0-mae:1.18821\tvalidation_1-mae:2.17061\n[99]\tvalidation_0-mae:1.18147\tvalidation_1-mae:2.17018\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_xgb","execution_count":58,"outputs":[{"output_type":"execute_result","execution_count":58,"data":{"text/plain":"      seg_id  time_to_failure\n0          0         2.990885\n1          1         4.775519\n2          2         5.642636\n3          3         8.393229\n4          4         7.578168\n5          5         2.962801\n6          6         9.136414\n7          7         4.771952\n8          8         4.429375\n9          9         2.812222\n10        10         3.298413\n11        11         5.889686\n12        12         4.399198\n13        13         2.569221\n14        14         8.946307\n15        15         3.752773\n16        16         5.847452\n17        17         3.667770\n18        18         4.421744\n19        19         5.218143\n20        20         5.803785\n21        21        10.108932\n22        22         3.701456\n23        23         6.447885\n24        24         8.117131\n25        25         4.519305\n26        26         6.800865\n27        27         4.849980\n28        28         2.643125\n29        29         5.373939\n...      ...              ...\n2594    2594         9.925138\n2595    2595         5.955254\n2596    2596         5.777175\n2597    2597         9.589745\n2598    2598         6.181342\n2599    2599         6.348209\n2600    2600         6.253168\n2601    2601         5.648138\n2602    2602         6.317184\n2603    2603         5.328905\n2604    2604         2.910192\n2605    2605         6.130119\n2606    2606         7.215423\n2607    2607         2.452041\n2608    2608         4.998268\n2609    2609         3.928159\n2610    2610         6.131444\n2611    2611         6.766129\n2612    2612         4.561727\n2613    2613         9.599443\n2614    2614         6.816417\n2615    2615        10.926923\n2616    2616         3.467959\n2617    2617         8.343441\n2618    2618         5.715891\n2619    2619         6.923317\n2620    2620         8.267750\n2621    2621         3.769877\n2622    2622         2.141034\n2623    2623        10.000276\n\n[2624 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>seg_id</th>\n      <th>time_to_failure</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>2.990885</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>4.775519</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>5.642636</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>8.393229</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>7.578168</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>5</td>\n      <td>2.962801</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>6</td>\n      <td>9.136414</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>7</td>\n      <td>4.771952</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>8</td>\n      <td>4.429375</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>9</td>\n      <td>2.812222</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>10</td>\n      <td>3.298413</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>11</td>\n      <td>5.889686</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>12</td>\n      <td>4.399198</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>13</td>\n      <td>2.569221</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>14</td>\n      <td>8.946307</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>15</td>\n      <td>3.752773</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>16</td>\n      <td>5.847452</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>17</td>\n      <td>3.667770</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>18</td>\n      <td>4.421744</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>19</td>\n      <td>5.218143</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>20</td>\n      <td>5.803785</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>21</td>\n      <td>10.108932</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>22</td>\n      <td>3.701456</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>23</td>\n      <td>6.447885</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>24</td>\n      <td>8.117131</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>25</td>\n      <td>4.519305</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>26</td>\n      <td>6.800865</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>27</td>\n      <td>4.849980</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>28</td>\n      <td>2.643125</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>29</td>\n      <td>5.373939</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2594</th>\n      <td>2594</td>\n      <td>9.925138</td>\n    </tr>\n    <tr>\n      <th>2595</th>\n      <td>2595</td>\n      <td>5.955254</td>\n    </tr>\n    <tr>\n      <th>2596</th>\n      <td>2596</td>\n      <td>5.777175</td>\n    </tr>\n    <tr>\n      <th>2597</th>\n      <td>2597</td>\n      <td>9.589745</td>\n    </tr>\n    <tr>\n      <th>2598</th>\n      <td>2598</td>\n      <td>6.181342</td>\n    </tr>\n    <tr>\n      <th>2599</th>\n      <td>2599</td>\n      <td>6.348209</td>\n    </tr>\n    <tr>\n      <th>2600</th>\n      <td>2600</td>\n      <td>6.253168</td>\n    </tr>\n    <tr>\n      <th>2601</th>\n      <td>2601</td>\n      <td>5.648138</td>\n    </tr>\n    <tr>\n      <th>2602</th>\n      <td>2602</td>\n      <td>6.317184</td>\n    </tr>\n    <tr>\n      <th>2603</th>\n      <td>2603</td>\n      <td>5.328905</td>\n    </tr>\n    <tr>\n      <th>2604</th>\n      <td>2604</td>\n      <td>2.910192</td>\n    </tr>\n    <tr>\n      <th>2605</th>\n      <td>2605</td>\n      <td>6.130119</td>\n    </tr>\n    <tr>\n      <th>2606</th>\n      <td>2606</td>\n      <td>7.215423</td>\n    </tr>\n    <tr>\n      <th>2607</th>\n      <td>2607</td>\n      <td>2.452041</td>\n    </tr>\n    <tr>\n      <th>2608</th>\n      <td>2608</td>\n      <td>4.998268</td>\n    </tr>\n    <tr>\n      <th>2609</th>\n      <td>2609</td>\n      <td>3.928159</td>\n    </tr>\n    <tr>\n      <th>2610</th>\n      <td>2610</td>\n      <td>6.131444</td>\n    </tr>\n    <tr>\n      <th>2611</th>\n      <td>2611</td>\n      <td>6.766129</td>\n    </tr>\n    <tr>\n      <th>2612</th>\n      <td>2612</td>\n      <td>4.561727</td>\n    </tr>\n    <tr>\n      <th>2613</th>\n      <td>2613</td>\n      <td>9.599443</td>\n    </tr>\n    <tr>\n      <th>2614</th>\n      <td>2614</td>\n      <td>6.816417</td>\n    </tr>\n    <tr>\n      <th>2615</th>\n      <td>2615</td>\n      <td>10.926923</td>\n    </tr>\n    <tr>\n      <th>2616</th>\n      <td>2616</td>\n      <td>3.467959</td>\n    </tr>\n    <tr>\n      <th>2617</th>\n      <td>2617</td>\n      <td>8.343441</td>\n    </tr>\n    <tr>\n      <th>2618</th>\n      <td>2618</td>\n      <td>5.715891</td>\n    </tr>\n    <tr>\n      <th>2619</th>\n      <td>2619</td>\n      <td>6.923317</td>\n    </tr>\n    <tr>\n      <th>2620</th>\n      <td>2620</td>\n      <td>8.267750</td>\n    </tr>\n    <tr>\n      <th>2621</th>\n      <td>2621</td>\n      <td>3.769877</td>\n    </tr>\n    <tr>\n      <th>2622</th>\n      <td>2622</td>\n      <td>2.141034</td>\n    </tr>\n    <tr>\n      <th>2623</th>\n      <td>2623</td>\n      <td>10.000276</td>\n    </tr>\n  </tbody>\n</table>\n<p>2624 rows  2 columns</p>\n</div>"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"<h2>LGB</h2>"},{"metadata":{"trusted":true},"cell_type":"code","source":"model = lgb.train(lgbm_params, \n                  lgb.Dataset(x_train_corr_rfecv,y_train), \n                  num_boost_round=20_000, \n                  valid_sets=lgb.Dataset(x_valid_corr_rfecv,y_valid), \n                  early_stopping_rounds=2000, \n                  verbose_eval=True)\n\ntest_preds = model.predict(X_test_scaled_corr_refcv)\ntest_segs = X_test_scaled_corr_refcv.index\nsubmission_lgbm = pd.DataFrame(columns=['seg_id', 'time_to_failure'])\nsubmission_lgbm.seg_id = test_segs\nsubmission_lgbm.time_to_failure = test_preds\nsubmission_lgbm.to_csv('submission_lgbm.csv', index=False, line_terminator='\\n')","execution_count":55,"outputs":[{"output_type":"stream","text":"[1]\tvalid_0's rmse: 3.38633\nTraining until validation scores don't improve for 2000 rounds.\n[2]\tvalid_0's rmse: 3.20726\n[3]\tvalid_0's rmse: 3.07259\n[4]\tvalid_0's rmse: 2.98055\n[5]\tvalid_0's rmse: 2.91261\n[6]\tvalid_0's rmse: 2.85949\n[7]\tvalid_0's rmse: 2.81667\n[8]\tvalid_0's rmse: 2.78428\n[9]\tvalid_0's rmse: 2.7598\n[10]\tvalid_0's rmse: 2.74245\n[11]\tvalid_0's rmse: 2.72375\n[12]\tvalid_0's rmse: 2.71081\n[13]\tvalid_0's rmse: 2.7032\n[14]\tvalid_0's rmse: 2.69401\n[15]\tvalid_0's rmse: 2.68677\n[16]\tvalid_0's rmse: 2.68286\n[17]\tvalid_0's rmse: 2.67845\n[18]\tvalid_0's rmse: 2.67536\n[19]\tvalid_0's rmse: 2.67267\n[20]\tvalid_0's rmse: 2.67034\n[21]\tvalid_0's rmse: 2.66861\n[22]\tvalid_0's rmse: 2.66687\n[23]\tvalid_0's rmse: 2.66529\n[24]\tvalid_0's rmse: 2.66667\n[25]\tvalid_0's rmse: 2.66427\n[26]\tvalid_0's rmse: 2.66216\n[27]\tvalid_0's rmse: 2.65952\n[28]\tvalid_0's rmse: 2.66084\n[29]\tvalid_0's rmse: 2.66246\n[30]\tvalid_0's rmse: 2.66357\n[31]\tvalid_0's rmse: 2.6622\n[32]\tvalid_0's rmse: 2.66235\n[33]\tvalid_0's rmse: 2.66271\n[34]\tvalid_0's rmse: 2.6628\n[35]\tvalid_0's rmse: 2.66433\n[36]\tvalid_0's rmse: 2.66404\n[37]\tvalid_0's rmse: 2.66545\n[38]\tvalid_0's rmse: 2.66649\n[39]\tvalid_0's rmse: 2.66844\n[40]\tvalid_0's rmse: 2.66819\n[41]\tvalid_0's rmse: 2.66736\n[42]\tvalid_0's rmse: 2.66712\n[43]\tvalid_0's rmse: 2.66732\n[44]\tvalid_0's rmse: 2.66875\n[45]\tvalid_0's rmse: 2.6688\n[46]\tvalid_0's rmse: 2.6684\n[47]\tvalid_0's rmse: 2.66867\n[48]\tvalid_0's rmse: 2.66869\n[49]\tvalid_0's rmse: 2.66746\n[50]\tvalid_0's rmse: 2.66725\n[51]\tvalid_0's rmse: 2.66839\n[52]\tvalid_0's rmse: 2.67148\n[53]\tvalid_0's rmse: 2.67054\n[54]\tvalid_0's rmse: 2.66982\n[55]\tvalid_0's rmse: 2.66904\n[56]\tvalid_0's rmse: 2.67003\n[57]\tvalid_0's rmse: 2.67134\n[58]\tvalid_0's rmse: 2.6737\n[59]\tvalid_0's rmse: 2.67381\n[60]\tvalid_0's rmse: 2.67324\n[61]\tvalid_0's rmse: 2.67387\n[62]\tvalid_0's rmse: 2.67306\n[63]\tvalid_0's rmse: 2.67298\n[64]\tvalid_0's rmse: 2.67333\n[65]\tvalid_0's rmse: 2.67329\n[66]\tvalid_0's rmse: 2.67441\n[67]\tvalid_0's rmse: 2.67449\n[68]\tvalid_0's rmse: 2.67457\n[69]\tvalid_0's rmse: 2.6748\n[70]\tvalid_0's rmse: 2.67733\n[71]\tvalid_0's rmse: 2.67616\n[72]\tvalid_0's rmse: 2.67588\n[73]\tvalid_0's rmse: 2.67455\n[74]\tvalid_0's rmse: 2.67612\n[75]\tvalid_0's rmse: 2.67586\n[76]\tvalid_0's rmse: 2.67581\n[77]\tvalid_0's rmse: 2.6753\n[78]\tvalid_0's rmse: 2.67566\n[79]\tvalid_0's rmse: 2.67539\n[80]\tvalid_0's rmse: 2.6751\n[81]\tvalid_0's rmse: 2.67313\n[82]\tvalid_0's rmse: 2.67421\n[83]\tvalid_0's rmse: 2.67604\n[84]\tvalid_0's rmse: 2.6764\n[85]\tvalid_0's rmse: 2.67602\n[86]\tvalid_0's rmse: 2.67429\n[87]\tvalid_0's rmse: 2.67323\n[88]\tvalid_0's rmse: 2.67439\n[89]\tvalid_0's rmse: 2.67396\n[90]\tvalid_0's rmse: 2.67403\n[91]\tvalid_0's rmse: 2.67355\n[92]\tvalid_0's rmse: 2.67385\n[93]\tvalid_0's rmse: 2.67303\n[94]\tvalid_0's rmse: 2.6729\n[95]\tvalid_0's rmse: 2.67554\n[96]\tvalid_0's rmse: 2.67517\n[97]\tvalid_0's rmse: 2.67446\n[98]\tvalid_0's rmse: 2.67254\n[99]\tvalid_0's rmse: 2.67328\n[100]\tvalid_0's rmse: 2.67374\n[101]\tvalid_0's rmse: 2.674\n[102]\tvalid_0's rmse: 2.67425\n[103]\tvalid_0's rmse: 2.67511\n[104]\tvalid_0's rmse: 2.67577\n[105]\tvalid_0's rmse: 2.67501\n[106]\tvalid_0's rmse: 2.67372\n[107]\tvalid_0's rmse: 2.67478\n[108]\tvalid_0's rmse: 2.67663\n[109]\tvalid_0's rmse: 2.67647\n[110]\tvalid_0's rmse: 2.67621\n[111]\tvalid_0's rmse: 2.67763\n[112]\tvalid_0's rmse: 2.67794\n[113]\tvalid_0's rmse: 2.6786\n[114]\tvalid_0's rmse: 2.67936\n[115]\tvalid_0's rmse: 2.68112\n[116]\tvalid_0's rmse: 2.68074\n[117]\tvalid_0's rmse: 2.68151\n[118]\tvalid_0's rmse: 2.68291\n[119]\tvalid_0's rmse: 2.68352\n[120]\tvalid_0's rmse: 2.68395\n[121]\tvalid_0's rmse: 2.68461\n[122]\tvalid_0's rmse: 2.68537\n[123]\tvalid_0's rmse: 2.685\n[124]\tvalid_0's rmse: 2.68475\n[125]\tvalid_0's rmse: 2.68517\n[126]\tvalid_0's rmse: 2.6856\n[127]\tvalid_0's rmse: 2.68674\n[128]\tvalid_0's rmse: 2.68723\n[129]\tvalid_0's rmse: 2.68694\n[130]\tvalid_0's rmse: 2.68766\n[131]\tvalid_0's rmse: 2.68733\n[132]\tvalid_0's rmse: 2.68709\n[133]\tvalid_0's rmse: 2.68679\n[134]\tvalid_0's rmse: 2.68704\n[135]\tvalid_0's rmse: 2.68709\n[136]\tvalid_0's rmse: 2.68795\n[137]\tvalid_0's rmse: 2.68834\n[138]\tvalid_0's rmse: 2.68832\n[139]\tvalid_0's rmse: 2.68779\n[140]\tvalid_0's rmse: 2.68837\n[141]\tvalid_0's rmse: 2.68871\n[142]\tvalid_0's rmse: 2.68902\n[143]\tvalid_0's rmse: 2.68873\n[144]\tvalid_0's rmse: 2.68963\n[145]\tvalid_0's rmse: 2.69065\n[146]\tvalid_0's rmse: 2.69021\n[147]\tvalid_0's rmse: 2.6904\n[148]\tvalid_0's rmse: 2.69097\n[149]\tvalid_0's rmse: 2.68976\n[150]\tvalid_0's rmse: 2.69045\n[151]\tvalid_0's rmse: 2.6896\n[152]\tvalid_0's rmse: 2.68969\n[153]\tvalid_0's rmse: 2.69022\n[154]\tvalid_0's rmse: 2.69071\n[155]\tvalid_0's rmse: 2.69145\n[156]\tvalid_0's rmse: 2.69232\n[157]\tvalid_0's rmse: 2.69216\n[158]\tvalid_0's rmse: 2.69187\n[159]\tvalid_0's rmse: 2.69208\n[160]\tvalid_0's rmse: 2.69266\n[161]\tvalid_0's rmse: 2.69409\n[162]\tvalid_0's rmse: 2.69536\n[163]\tvalid_0's rmse: 2.69453\n[164]\tvalid_0's rmse: 2.6936\n[165]\tvalid_0's rmse: 2.693\n[166]\tvalid_0's rmse: 2.69321\n[167]\tvalid_0's rmse: 2.69371\n[168]\tvalid_0's rmse: 2.69471\n[169]\tvalid_0's rmse: 2.69484\n[170]\tvalid_0's rmse: 2.69554\n[171]\tvalid_0's rmse: 2.69547\n[172]\tvalid_0's rmse: 2.69635\n[173]\tvalid_0's rmse: 2.69587\n[174]\tvalid_0's rmse: 2.69555\n[175]\tvalid_0's rmse: 2.69515\n[176]\tvalid_0's rmse: 2.6952\n[177]\tvalid_0's rmse: 2.69477\n[178]\tvalid_0's rmse: 2.69464\n[179]\tvalid_0's rmse: 2.69427\n[180]\tvalid_0's rmse: 2.69522\n[181]\tvalid_0's rmse: 2.69577\n[182]\tvalid_0's rmse: 2.69565\n[183]\tvalid_0's rmse: 2.69476\n[184]\tvalid_0's rmse: 2.69624\n[185]\tvalid_0's rmse: 2.6972\n[186]\tvalid_0's rmse: 2.69706\n[187]\tvalid_0's rmse: 2.69663\n[188]\tvalid_0's rmse: 2.69513\n[189]\tvalid_0's rmse: 2.69549\n[190]\tvalid_0's rmse: 2.69569\n[191]\tvalid_0's rmse: 2.69613\n[192]\tvalid_0's rmse: 2.69733\n[193]\tvalid_0's rmse: 2.69746\n[194]\tvalid_0's rmse: 2.69726\n[195]\tvalid_0's rmse: 2.69756\n[196]\tvalid_0's rmse: 2.69692\n[197]\tvalid_0's rmse: 2.69801\n[198]\tvalid_0's rmse: 2.69811\n[199]\tvalid_0's rmse: 2.69808\n[200]\tvalid_0's rmse: 2.69882\n[201]\tvalid_0's rmse: 2.69954\n[202]\tvalid_0's rmse: 2.69821\n[203]\tvalid_0's rmse: 2.69724\n[204]\tvalid_0's rmse: 2.697\n[205]\tvalid_0's rmse: 2.69742\n[206]\tvalid_0's rmse: 2.69739\n[207]\tvalid_0's rmse: 2.69804\n[208]\tvalid_0's rmse: 2.69854\n[209]\tvalid_0's rmse: 2.69863\n[210]\tvalid_0's rmse: 2.70071\n[211]\tvalid_0's rmse: 2.70181\n[212]\tvalid_0's rmse: 2.70291\n[213]\tvalid_0's rmse: 2.70296\n[214]\tvalid_0's rmse: 2.70233\n[215]\tvalid_0's rmse: 2.70166\n[216]\tvalid_0's rmse: 2.70121\n[217]\tvalid_0's rmse: 2.70246\n[218]\tvalid_0's rmse: 2.70251\n[219]\tvalid_0's rmse: 2.7033\n[220]\tvalid_0's rmse: 2.7028\n[221]\tvalid_0's rmse: 2.70348\n[222]\tvalid_0's rmse: 2.70351\n[223]\tvalid_0's rmse: 2.7035\n[224]\tvalid_0's rmse: 2.70271\n[225]\tvalid_0's rmse: 2.70215\n[226]\tvalid_0's rmse: 2.70182\n[227]\tvalid_0's rmse: 2.70211\n[228]\tvalid_0's rmse: 2.70251\n[229]\tvalid_0's rmse: 2.70232\n[230]\tvalid_0's rmse: 2.70278\n[231]\tvalid_0's rmse: 2.70356\n[232]\tvalid_0's rmse: 2.70344\n[233]\tvalid_0's rmse: 2.70344\n[234]\tvalid_0's rmse: 2.70319\n[235]\tvalid_0's rmse: 2.70366\n[236]\tvalid_0's rmse: 2.70392\n[237]\tvalid_0's rmse: 2.70409\n[238]\tvalid_0's rmse: 2.70426\n[239]\tvalid_0's rmse: 2.70368\n[240]\tvalid_0's rmse: 2.70364\n[241]\tvalid_0's rmse: 2.70296\n[242]\tvalid_0's rmse: 2.7029\n[243]\tvalid_0's rmse: 2.70295\n[244]\tvalid_0's rmse: 2.70222\n[245]\tvalid_0's rmse: 2.70266\n[246]\tvalid_0's rmse: 2.703\n[247]\tvalid_0's rmse: 2.70304\n[248]\tvalid_0's rmse: 2.70331\n[249]\tvalid_0's rmse: 2.70335\n[250]\tvalid_0's rmse: 2.70327\n[251]\tvalid_0's rmse: 2.70362\n[252]\tvalid_0's rmse: 2.70308\n[253]\tvalid_0's rmse: 2.70294\n[254]\tvalid_0's rmse: 2.70267\n[255]\tvalid_0's rmse: 2.70273\n[256]\tvalid_0's rmse: 2.70264\n[257]\tvalid_0's rmse: 2.70286\n[258]\tvalid_0's rmse: 2.70274\n[259]\tvalid_0's rmse: 2.70298\n[260]\tvalid_0's rmse: 2.70275\n[261]\tvalid_0's rmse: 2.7033\n[262]\tvalid_0's rmse: 2.7036\n[263]\tvalid_0's rmse: 2.70389\n[264]\tvalid_0's rmse: 2.70363\n[265]\tvalid_0's rmse: 2.70342\n[266]\tvalid_0's rmse: 2.70434\n[267]\tvalid_0's rmse: 2.70388\n[268]\tvalid_0's rmse: 2.70398\n[269]\tvalid_0's rmse: 2.70536\n[270]\tvalid_0's rmse: 2.70593\n[271]\tvalid_0's rmse: 2.70621\n[272]\tvalid_0's rmse: 2.70663\n[273]\tvalid_0's rmse: 2.70642\n[274]\tvalid_0's rmse: 2.70632\n[275]\tvalid_0's rmse: 2.70564\n[276]\tvalid_0's rmse: 2.70638\n[277]\tvalid_0's rmse: 2.7061\n[278]\tvalid_0's rmse: 2.70505\n[279]\tvalid_0's rmse: 2.70553\n[280]\tvalid_0's rmse: 2.70622\n[281]\tvalid_0's rmse: 2.70549\n[282]\tvalid_0's rmse: 2.70631\n[283]\tvalid_0's rmse: 2.70644\n[284]\tvalid_0's rmse: 2.7061\n[285]\tvalid_0's rmse: 2.70634\n[286]\tvalid_0's rmse: 2.70691\n[287]\tvalid_0's rmse: 2.70811\n[288]\tvalid_0's rmse: 2.70892\n[289]\tvalid_0's rmse: 2.70932\n[290]\tvalid_0's rmse: 2.71012\n[291]\tvalid_0's rmse: 2.7116\n[292]\tvalid_0's rmse: 2.71186\n[293]\tvalid_0's rmse: 2.71241\n[294]\tvalid_0's rmse: 2.71215\n[295]\tvalid_0's rmse: 2.71236\n[296]\tvalid_0's rmse: 2.71261\n[297]\tvalid_0's rmse: 2.71226\n[298]\tvalid_0's rmse: 2.71312\n[299]\tvalid_0's rmse: 2.71306\n[300]\tvalid_0's rmse: 2.71356\n[301]\tvalid_0's rmse: 2.71321\n[302]\tvalid_0's rmse: 2.71304\n[303]\tvalid_0's rmse: 2.71373\n[304]\tvalid_0's rmse: 2.71441\n[305]\tvalid_0's rmse: 2.71421\n[306]\tvalid_0's rmse: 2.71524\n[307]\tvalid_0's rmse: 2.71543\n[308]\tvalid_0's rmse: 2.7163\n[309]\tvalid_0's rmse: 2.71649\n[310]\tvalid_0's rmse: 2.71726\n[311]\tvalid_0's rmse: 2.71734\n[312]\tvalid_0's rmse: 2.71714\n[313]\tvalid_0's rmse: 2.71796\n[314]\tvalid_0's rmse: 2.71801\n[315]\tvalid_0's rmse: 2.71839\n[316]\tvalid_0's rmse: 2.71824\n[317]\tvalid_0's rmse: 2.71789\n[318]\tvalid_0's rmse: 2.71769\n[319]\tvalid_0's rmse: 2.719\n[320]\tvalid_0's rmse: 2.7185\n[321]\tvalid_0's rmse: 2.71813\n[322]\tvalid_0's rmse: 2.71771\n[323]\tvalid_0's rmse: 2.71838\n[324]\tvalid_0's rmse: 2.71789\n[325]\tvalid_0's rmse: 2.71836\n","name":"stdout"},{"output_type":"stream","text":"[326]\tvalid_0's rmse: 2.71842\n[327]\tvalid_0's rmse: 2.71792\n[328]\tvalid_0's rmse: 2.71776\n[329]\tvalid_0's rmse: 2.71823\n[330]\tvalid_0's rmse: 2.71751\n[331]\tvalid_0's rmse: 2.71735\n[332]\tvalid_0's rmse: 2.71877\n[333]\tvalid_0's rmse: 2.71916\n[334]\tvalid_0's rmse: 2.71895\n[335]\tvalid_0's rmse: 2.71915\n[336]\tvalid_0's rmse: 2.71847\n[337]\tvalid_0's rmse: 2.7187\n[338]\tvalid_0's rmse: 2.71975\n[339]\tvalid_0's rmse: 2.72038\n[340]\tvalid_0's rmse: 2.7205\n[341]\tvalid_0's rmse: 2.72047\n[342]\tvalid_0's rmse: 2.72122\n[343]\tvalid_0's rmse: 2.72076\n[344]\tvalid_0's rmse: 2.72101\n[345]\tvalid_0's rmse: 2.72162\n[346]\tvalid_0's rmse: 2.72146\n[347]\tvalid_0's rmse: 2.72179\n[348]\tvalid_0's rmse: 2.72129\n[349]\tvalid_0's rmse: 2.72178\n[350]\tvalid_0's rmse: 2.72269\n[351]\tvalid_0's rmse: 2.72313\n[352]\tvalid_0's rmse: 2.72365\n[353]\tvalid_0's rmse: 2.72269\n[354]\tvalid_0's rmse: 2.72301\n[355]\tvalid_0's rmse: 2.72373\n[356]\tvalid_0's rmse: 2.72395\n[357]\tvalid_0's rmse: 2.72371\n[358]\tvalid_0's rmse: 2.72403\n[359]\tvalid_0's rmse: 2.72476\n[360]\tvalid_0's rmse: 2.72436\n[361]\tvalid_0's rmse: 2.72418\n[362]\tvalid_0's rmse: 2.72479\n[363]\tvalid_0's rmse: 2.72484\n[364]\tvalid_0's rmse: 2.72496\n[365]\tvalid_0's rmse: 2.72453\n[366]\tvalid_0's rmse: 2.72387\n[367]\tvalid_0's rmse: 2.72464\n[368]\tvalid_0's rmse: 2.72436\n[369]\tvalid_0's rmse: 2.72421\n[370]\tvalid_0's rmse: 2.72399\n[371]\tvalid_0's rmse: 2.72487\n[372]\tvalid_0's rmse: 2.7248\n[373]\tvalid_0's rmse: 2.72564\n[374]\tvalid_0's rmse: 2.72622\n[375]\tvalid_0's rmse: 2.72622\n[376]\tvalid_0's rmse: 2.72646\n[377]\tvalid_0's rmse: 2.72738\n[378]\tvalid_0's rmse: 2.72843\n[379]\tvalid_0's rmse: 2.72911\n[380]\tvalid_0's rmse: 2.72962\n[381]\tvalid_0's rmse: 2.72994\n[382]\tvalid_0's rmse: 2.72984\n[383]\tvalid_0's rmse: 2.73016\n[384]\tvalid_0's rmse: 2.73055\n[385]\tvalid_0's rmse: 2.73045\n[386]\tvalid_0's rmse: 2.73067\n[387]\tvalid_0's rmse: 2.7309\n[388]\tvalid_0's rmse: 2.73114\n[389]\tvalid_0's rmse: 2.73159\n[390]\tvalid_0's rmse: 2.73255\n[391]\tvalid_0's rmse: 2.7327\n[392]\tvalid_0's rmse: 2.7328\n[393]\tvalid_0's rmse: 2.73301\n[394]\tvalid_0's rmse: 2.73251\n[395]\tvalid_0's rmse: 2.73305\n[396]\tvalid_0's rmse: 2.73464\n[397]\tvalid_0's rmse: 2.73441\n[398]\tvalid_0's rmse: 2.73481\n[399]\tvalid_0's rmse: 2.73513\n[400]\tvalid_0's rmse: 2.73542\n[401]\tvalid_0's rmse: 2.73515\n[402]\tvalid_0's rmse: 2.73468\n[403]\tvalid_0's rmse: 2.73493\n[404]\tvalid_0's rmse: 2.73467\n[405]\tvalid_0's rmse: 2.73554\n[406]\tvalid_0's rmse: 2.73605\n[407]\tvalid_0's rmse: 2.73621\n[408]\tvalid_0's rmse: 2.73648\n[409]\tvalid_0's rmse: 2.73679\n[410]\tvalid_0's rmse: 2.73648\n[411]\tvalid_0's rmse: 2.73603\n[412]\tvalid_0's rmse: 2.73559\n[413]\tvalid_0's rmse: 2.73551\n[414]\tvalid_0's rmse: 2.73578\n[415]\tvalid_0's rmse: 2.73621\n[416]\tvalid_0's rmse: 2.73613\n[417]\tvalid_0's rmse: 2.73595\n[418]\tvalid_0's rmse: 2.73644\n[419]\tvalid_0's rmse: 2.73627\n[420]\tvalid_0's rmse: 2.73667\n[421]\tvalid_0's rmse: 2.73648\n[422]\tvalid_0's rmse: 2.73654\n[423]\tvalid_0's rmse: 2.73661\n[424]\tvalid_0's rmse: 2.73653\n[425]\tvalid_0's rmse: 2.73665\n[426]\tvalid_0's rmse: 2.73649\n[427]\tvalid_0's rmse: 2.73671\n[428]\tvalid_0's rmse: 2.73699\n[429]\tvalid_0's rmse: 2.73754\n[430]\tvalid_0's rmse: 2.7375\n[431]\tvalid_0's rmse: 2.73729\n[432]\tvalid_0's rmse: 2.73766\n[433]\tvalid_0's rmse: 2.737\n[434]\tvalid_0's rmse: 2.73687\n[435]\tvalid_0's rmse: 2.73691\n[436]\tvalid_0's rmse: 2.7377\n[437]\tvalid_0's rmse: 2.73825\n[438]\tvalid_0's rmse: 2.73895\n[439]\tvalid_0's rmse: 2.73927\n[440]\tvalid_0's rmse: 2.73934\n[441]\tvalid_0's rmse: 2.7394\n[442]\tvalid_0's rmse: 2.73971\n[443]\tvalid_0's rmse: 2.73969\n[444]\tvalid_0's rmse: 2.73935\n[445]\tvalid_0's rmse: 2.73976\n[446]\tvalid_0's rmse: 2.73948\n[447]\tvalid_0's rmse: 2.7401\n[448]\tvalid_0's rmse: 2.74078\n[449]\tvalid_0's rmse: 2.74152\n[450]\tvalid_0's rmse: 2.74195\n[451]\tvalid_0's rmse: 2.74144\n[452]\tvalid_0's rmse: 2.74151\n[453]\tvalid_0's rmse: 2.7414\n[454]\tvalid_0's rmse: 2.74103\n[455]\tvalid_0's rmse: 2.74075\n[456]\tvalid_0's rmse: 2.74061\n[457]\tvalid_0's rmse: 2.74028\n[458]\tvalid_0's rmse: 2.74006\n[459]\tvalid_0's rmse: 2.74005\n[460]\tvalid_0's rmse: 2.74082\n[461]\tvalid_0's rmse: 2.74052\n[462]\tvalid_0's rmse: 2.74049\n[463]\tvalid_0's rmse: 2.74169\n[464]\tvalid_0's rmse: 2.74213\n[465]\tvalid_0's rmse: 2.74234\n[466]\tvalid_0's rmse: 2.74304\n[467]\tvalid_0's rmse: 2.74335\n[468]\tvalid_0's rmse: 2.7435\n[469]\tvalid_0's rmse: 2.74372\n[470]\tvalid_0's rmse: 2.74436\n[471]\tvalid_0's rmse: 2.7447\n[472]\tvalid_0's rmse: 2.74519\n[473]\tvalid_0's rmse: 2.74517\n[474]\tvalid_0's rmse: 2.7447\n[475]\tvalid_0's rmse: 2.7443\n[476]\tvalid_0's rmse: 2.74471\n[477]\tvalid_0's rmse: 2.74409\n[478]\tvalid_0's rmse: 2.74486\n[479]\tvalid_0's rmse: 2.74497\n[480]\tvalid_0's rmse: 2.74485\n[481]\tvalid_0's rmse: 2.74515\n[482]\tvalid_0's rmse: 2.74543\n[483]\tvalid_0's rmse: 2.74521\n[484]\tvalid_0's rmse: 2.74547\n[485]\tvalid_0's rmse: 2.74592\n[486]\tvalid_0's rmse: 2.74591\n[487]\tvalid_0's rmse: 2.74558\n[488]\tvalid_0's rmse: 2.74584\n[489]\tvalid_0's rmse: 2.74592\n[490]\tvalid_0's rmse: 2.74583\n[491]\tvalid_0's rmse: 2.74639\n[492]\tvalid_0's rmse: 2.74685\n[493]\tvalid_0's rmse: 2.74713\n[494]\tvalid_0's rmse: 2.74726\n[495]\tvalid_0's rmse: 2.74741\n[496]\tvalid_0's rmse: 2.7473\n[497]\tvalid_0's rmse: 2.74661\n[498]\tvalid_0's rmse: 2.74667\n[499]\tvalid_0's rmse: 2.74644\n[500]\tvalid_0's rmse: 2.74674\n[501]\tvalid_0's rmse: 2.7463\n[502]\tvalid_0's rmse: 2.74654\n[503]\tvalid_0's rmse: 2.74684\n[504]\tvalid_0's rmse: 2.74667\n[505]\tvalid_0's rmse: 2.74695\n[506]\tvalid_0's rmse: 2.74718\n[507]\tvalid_0's rmse: 2.74719\n[508]\tvalid_0's rmse: 2.74724\n[509]\tvalid_0's rmse: 2.74761\n[510]\tvalid_0's rmse: 2.74721\n[511]\tvalid_0's rmse: 2.74683\n[512]\tvalid_0's rmse: 2.74697\n[513]\tvalid_0's rmse: 2.74707\n[514]\tvalid_0's rmse: 2.74751\n[515]\tvalid_0's rmse: 2.74804\n[516]\tvalid_0's rmse: 2.74793\n[517]\tvalid_0's rmse: 2.74773\n[518]\tvalid_0's rmse: 2.74753\n[519]\tvalid_0's rmse: 2.74749\n[520]\tvalid_0's rmse: 2.74715\n[521]\tvalid_0's rmse: 2.74711\n[522]\tvalid_0's rmse: 2.74784\n[523]\tvalid_0's rmse: 2.74781\n[524]\tvalid_0's rmse: 2.74736\n[525]\tvalid_0's rmse: 2.74712\n[526]\tvalid_0's rmse: 2.74693\n[527]\tvalid_0's rmse: 2.74798\n[528]\tvalid_0's rmse: 2.74858\n[529]\tvalid_0's rmse: 2.74927\n[530]\tvalid_0's rmse: 2.7493\n[531]\tvalid_0's rmse: 2.74969\n[532]\tvalid_0's rmse: 2.74993\n[533]\tvalid_0's rmse: 2.75002\n[534]\tvalid_0's rmse: 2.75029\n[535]\tvalid_0's rmse: 2.75095\n[536]\tvalid_0's rmse: 2.75123\n[537]\tvalid_0's rmse: 2.75164\n[538]\tvalid_0's rmse: 2.75176\n[539]\tvalid_0's rmse: 2.75191\n[540]\tvalid_0's rmse: 2.75153\n[541]\tvalid_0's rmse: 2.75119\n[542]\tvalid_0's rmse: 2.75104\n[543]\tvalid_0's rmse: 2.75129\n[544]\tvalid_0's rmse: 2.75157\n[545]\tvalid_0's rmse: 2.75185\n[546]\tvalid_0's rmse: 2.75243\n[547]\tvalid_0's rmse: 2.75229\n[548]\tvalid_0's rmse: 2.75141\n[549]\tvalid_0's rmse: 2.75183\n[550]\tvalid_0's rmse: 2.75205\n[551]\tvalid_0's rmse: 2.75223\n[552]\tvalid_0's rmse: 2.7533\n[553]\tvalid_0's rmse: 2.75307\n[554]\tvalid_0's rmse: 2.75354\n[555]\tvalid_0's rmse: 2.75338\n[556]\tvalid_0's rmse: 2.75346\n[557]\tvalid_0's rmse: 2.75354\n[558]\tvalid_0's rmse: 2.75334\n[559]\tvalid_0's rmse: 2.75343\n[560]\tvalid_0's rmse: 2.75372\n[561]\tvalid_0's rmse: 2.75362\n[562]\tvalid_0's rmse: 2.75386\n[563]\tvalid_0's rmse: 2.75375\n[564]\tvalid_0's rmse: 2.75406\n[565]\tvalid_0's rmse: 2.75363\n[566]\tvalid_0's rmse: 2.75317\n[567]\tvalid_0's rmse: 2.75318\n[568]\tvalid_0's rmse: 2.75407\n[569]\tvalid_0's rmse: 2.75461\n[570]\tvalid_0's rmse: 2.7546\n[571]\tvalid_0's rmse: 2.75449\n[572]\tvalid_0's rmse: 2.75456\n[573]\tvalid_0's rmse: 2.75484\n[574]\tvalid_0's rmse: 2.75433\n[575]\tvalid_0's rmse: 2.75449\n[576]\tvalid_0's rmse: 2.75457\n[577]\tvalid_0's rmse: 2.75435\n[578]\tvalid_0's rmse: 2.75469\n[579]\tvalid_0's rmse: 2.75526\n[580]\tvalid_0's rmse: 2.75516\n[581]\tvalid_0's rmse: 2.75609\n[582]\tvalid_0's rmse: 2.75654\n[583]\tvalid_0's rmse: 2.75625\n[584]\tvalid_0's rmse: 2.75647\n[585]\tvalid_0's rmse: 2.75747\n[586]\tvalid_0's rmse: 2.75714\n[587]\tvalid_0's rmse: 2.75745\n[588]\tvalid_0's rmse: 2.75768\n[589]\tvalid_0's rmse: 2.75758\n[590]\tvalid_0's rmse: 2.7575\n[591]\tvalid_0's rmse: 2.75716\n[592]\tvalid_0's rmse: 2.75761\n[593]\tvalid_0's rmse: 2.75755\n[594]\tvalid_0's rmse: 2.7577\n[595]\tvalid_0's rmse: 2.758\n[596]\tvalid_0's rmse: 2.7582\n[597]\tvalid_0's rmse: 2.75851\n[598]\tvalid_0's rmse: 2.75854\n[599]\tvalid_0's rmse: 2.75849\n[600]\tvalid_0's rmse: 2.75849\n[601]\tvalid_0's rmse: 2.75861\n[602]\tvalid_0's rmse: 2.75844\n[603]\tvalid_0's rmse: 2.75828\n[604]\tvalid_0's rmse: 2.75839\n[605]\tvalid_0's rmse: 2.75843\n[606]\tvalid_0's rmse: 2.75866\n[607]\tvalid_0's rmse: 2.75853\n[608]\tvalid_0's rmse: 2.75833\n[609]\tvalid_0's rmse: 2.75822\n[610]\tvalid_0's rmse: 2.75835\n[611]\tvalid_0's rmse: 2.75785\n[612]\tvalid_0's rmse: 2.75781\n[613]\tvalid_0's rmse: 2.75826\n[614]\tvalid_0's rmse: 2.75826\n[615]\tvalid_0's rmse: 2.75825\n[616]\tvalid_0's rmse: 2.75869\n[617]\tvalid_0's rmse: 2.75923\n[618]\tvalid_0's rmse: 2.75945\n[619]\tvalid_0's rmse: 2.76\n[620]\tvalid_0's rmse: 2.76005\n[621]\tvalid_0's rmse: 2.76053\n[622]\tvalid_0's rmse: 2.76066\n[623]\tvalid_0's rmse: 2.76108\n[624]\tvalid_0's rmse: 2.76133\n[625]\tvalid_0's rmse: 2.76165\n[626]\tvalid_0's rmse: 2.76188\n[627]\tvalid_0's rmse: 2.76211\n[628]\tvalid_0's rmse: 2.76187\n[629]\tvalid_0's rmse: 2.76179\n[630]\tvalid_0's rmse: 2.76238\n[631]\tvalid_0's rmse: 2.7623\n[632]\tvalid_0's rmse: 2.76211\n[633]\tvalid_0's rmse: 2.76186\n[634]\tvalid_0's rmse: 2.76199\n[635]\tvalid_0's rmse: 2.7619\n[636]\tvalid_0's rmse: 2.76213\n[637]\tvalid_0's rmse: 2.76235\n[638]\tvalid_0's rmse: 2.76189\n[639]\tvalid_0's rmse: 2.7621\n[640]\tvalid_0's rmse: 2.76238\n[641]\tvalid_0's rmse: 2.76251\n[642]\tvalid_0's rmse: 2.76237\n[643]\tvalid_0's rmse: 2.76238\n[644]\tvalid_0's rmse: 2.76245\n[645]\tvalid_0's rmse: 2.76258\n[646]\tvalid_0's rmse: 2.76319\n[647]\tvalid_0's rmse: 2.76361\n[648]\tvalid_0's rmse: 2.76411\n[649]\tvalid_0's rmse: 2.7646\n[650]\tvalid_0's rmse: 2.76502\n","name":"stdout"},{"output_type":"stream","text":"[651]\tvalid_0's rmse: 2.76539\n[652]\tvalid_0's rmse: 2.76529\n[653]\tvalid_0's rmse: 2.7651\n[654]\tvalid_0's rmse: 2.76516\n[655]\tvalid_0's rmse: 2.76562\n[656]\tvalid_0's rmse: 2.76556\n[657]\tvalid_0's rmse: 2.76559\n[658]\tvalid_0's rmse: 2.76565\n[659]\tvalid_0's rmse: 2.76624\n[660]\tvalid_0's rmse: 2.76685\n[661]\tvalid_0's rmse: 2.76706\n[662]\tvalid_0's rmse: 2.76675\n[663]\tvalid_0's rmse: 2.76712\n[664]\tvalid_0's rmse: 2.76715\n[665]\tvalid_0's rmse: 2.76664\n[666]\tvalid_0's rmse: 2.76634\n[667]\tvalid_0's rmse: 2.76674\n[668]\tvalid_0's rmse: 2.76701\n[669]\tvalid_0's rmse: 2.76697\n[670]\tvalid_0's rmse: 2.76685\n[671]\tvalid_0's rmse: 2.76723\n[672]\tvalid_0's rmse: 2.76727\n[673]\tvalid_0's rmse: 2.76736\n[674]\tvalid_0's rmse: 2.76761\n[675]\tvalid_0's rmse: 2.76723\n[676]\tvalid_0's rmse: 2.76739\n[677]\tvalid_0's rmse: 2.76726\n[678]\tvalid_0's rmse: 2.76759\n[679]\tvalid_0's rmse: 2.76802\n[680]\tvalid_0's rmse: 2.768\n[681]\tvalid_0's rmse: 2.76793\n[682]\tvalid_0's rmse: 2.76823\n[683]\tvalid_0's rmse: 2.76831\n[684]\tvalid_0's rmse: 2.7681\n[685]\tvalid_0's rmse: 2.76774\n[686]\tvalid_0's rmse: 2.76754\n[687]\tvalid_0's rmse: 2.7678\n[688]\tvalid_0's rmse: 2.76807\n[689]\tvalid_0's rmse: 2.76831\n[690]\tvalid_0's rmse: 2.76863\n[691]\tvalid_0's rmse: 2.76878\n[692]\tvalid_0's rmse: 2.76943\n[693]\tvalid_0's rmse: 2.76928\n[694]\tvalid_0's rmse: 2.76977\n[695]\tvalid_0's rmse: 2.76966\n[696]\tvalid_0's rmse: 2.76942\n[697]\tvalid_0's rmse: 2.769\n[698]\tvalid_0's rmse: 2.76884\n[699]\tvalid_0's rmse: 2.76875\n[700]\tvalid_0's rmse: 2.76964\n[701]\tvalid_0's rmse: 2.76981\n[702]\tvalid_0's rmse: 2.77004\n[703]\tvalid_0's rmse: 2.7702\n[704]\tvalid_0's rmse: 2.77019\n[705]\tvalid_0's rmse: 2.77033\n[706]\tvalid_0's rmse: 2.7704\n[707]\tvalid_0's rmse: 2.77045\n[708]\tvalid_0's rmse: 2.77111\n[709]\tvalid_0's rmse: 2.77122\n[710]\tvalid_0's rmse: 2.77136\n[711]\tvalid_0's rmse: 2.77166\n[712]\tvalid_0's rmse: 2.77181\n[713]\tvalid_0's rmse: 2.77165\n[714]\tvalid_0's rmse: 2.7712\n[715]\tvalid_0's rmse: 2.77079\n[716]\tvalid_0's rmse: 2.77099\n[717]\tvalid_0's rmse: 2.7706\n[718]\tvalid_0's rmse: 2.77056\n[719]\tvalid_0's rmse: 2.77096\n[720]\tvalid_0's rmse: 2.77086\n[721]\tvalid_0's rmse: 2.77092\n[722]\tvalid_0's rmse: 2.77086\n[723]\tvalid_0's rmse: 2.77081\n[724]\tvalid_0's rmse: 2.77078\n[725]\tvalid_0's rmse: 2.77061\n[726]\tvalid_0's rmse: 2.77066\n[727]\tvalid_0's rmse: 2.77055\n[728]\tvalid_0's rmse: 2.77075\n[729]\tvalid_0's rmse: 2.77041\n[730]\tvalid_0's rmse: 2.77048\n[731]\tvalid_0's rmse: 2.77034\n[732]\tvalid_0's rmse: 2.77031\n[733]\tvalid_0's rmse: 2.77011\n[734]\tvalid_0's rmse: 2.77009\n[735]\tvalid_0's rmse: 2.77071\n[736]\tvalid_0's rmse: 2.77063\n[737]\tvalid_0's rmse: 2.7704\n[738]\tvalid_0's rmse: 2.7711\n[739]\tvalid_0's rmse: 2.7711\n[740]\tvalid_0's rmse: 2.77148\n[741]\tvalid_0's rmse: 2.77152\n[742]\tvalid_0's rmse: 2.7712\n[743]\tvalid_0's rmse: 2.77125\n[744]\tvalid_0's rmse: 2.77172\n[745]\tvalid_0's rmse: 2.77214\n[746]\tvalid_0's rmse: 2.77238\n[747]\tvalid_0's rmse: 2.77227\n[748]\tvalid_0's rmse: 2.77278\n[749]\tvalid_0's rmse: 2.77313\n[750]\tvalid_0's rmse: 2.77346\n[751]\tvalid_0's rmse: 2.77353\n[752]\tvalid_0's rmse: 2.77335\n[753]\tvalid_0's rmse: 2.77359\n[754]\tvalid_0's rmse: 2.77335\n[755]\tvalid_0's rmse: 2.77348\n[756]\tvalid_0's rmse: 2.77319\n[757]\tvalid_0's rmse: 2.77309\n[758]\tvalid_0's rmse: 2.77323\n[759]\tvalid_0's rmse: 2.77309\n[760]\tvalid_0's rmse: 2.77351\n[761]\tvalid_0's rmse: 2.77366\n[762]\tvalid_0's rmse: 2.77345\n[763]\tvalid_0's rmse: 2.77324\n[764]\tvalid_0's rmse: 2.77318\n[765]\tvalid_0's rmse: 2.774\n[766]\tvalid_0's rmse: 2.77436\n[767]\tvalid_0's rmse: 2.77476\n[768]\tvalid_0's rmse: 2.77514\n[769]\tvalid_0's rmse: 2.7756\n[770]\tvalid_0's rmse: 2.77593\n[771]\tvalid_0's rmse: 2.77586\n[772]\tvalid_0's rmse: 2.77579\n[773]\tvalid_0's rmse: 2.77549\n[774]\tvalid_0's rmse: 2.77581\n[775]\tvalid_0's rmse: 2.77569\n[776]\tvalid_0's rmse: 2.77608\n[777]\tvalid_0's rmse: 2.77557\n[778]\tvalid_0's rmse: 2.77582\n[779]\tvalid_0's rmse: 2.77597\n[780]\tvalid_0's rmse: 2.77585\n[781]\tvalid_0's rmse: 2.77642\n[782]\tvalid_0's rmse: 2.77648\n[783]\tvalid_0's rmse: 2.77629\n[784]\tvalid_0's rmse: 2.77613\n[785]\tvalid_0's rmse: 2.77625\n[786]\tvalid_0's rmse: 2.77651\n[787]\tvalid_0's rmse: 2.77592\n[788]\tvalid_0's rmse: 2.77579\n[789]\tvalid_0's rmse: 2.77583\n[790]\tvalid_0's rmse: 2.77592\n[791]\tvalid_0's rmse: 2.77589\n[792]\tvalid_0's rmse: 2.77566\n[793]\tvalid_0's rmse: 2.77594\n[794]\tvalid_0's rmse: 2.77562\n[795]\tvalid_0's rmse: 2.77548\n[796]\tvalid_0's rmse: 2.77615\n[797]\tvalid_0's rmse: 2.77589\n[798]\tvalid_0's rmse: 2.7761\n[799]\tvalid_0's rmse: 2.77654\n[800]\tvalid_0's rmse: 2.77671\n[801]\tvalid_0's rmse: 2.77653\n[802]\tvalid_0's rmse: 2.77639\n[803]\tvalid_0's rmse: 2.77628\n[804]\tvalid_0's rmse: 2.77635\n[805]\tvalid_0's rmse: 2.77626\n[806]\tvalid_0's rmse: 2.77631\n[807]\tvalid_0's rmse: 2.77697\n[808]\tvalid_0's rmse: 2.77734\n[809]\tvalid_0's rmse: 2.77743\n[810]\tvalid_0's rmse: 2.77795\n[811]\tvalid_0's rmse: 2.77789\n[812]\tvalid_0's rmse: 2.77765\n[813]\tvalid_0's rmse: 2.7777\n[814]\tvalid_0's rmse: 2.7776\n[815]\tvalid_0's rmse: 2.77764\n[816]\tvalid_0's rmse: 2.77782\n[817]\tvalid_0's rmse: 2.77749\n[818]\tvalid_0's rmse: 2.77739\n[819]\tvalid_0's rmse: 2.77687\n[820]\tvalid_0's rmse: 2.777\n[821]\tvalid_0's rmse: 2.77687\n[822]\tvalid_0's rmse: 2.77679\n[823]\tvalid_0's rmse: 2.77689\n[824]\tvalid_0's rmse: 2.77684\n[825]\tvalid_0's rmse: 2.77674\n[826]\tvalid_0's rmse: 2.77662\n[827]\tvalid_0's rmse: 2.77692\n[828]\tvalid_0's rmse: 2.77714\n[829]\tvalid_0's rmse: 2.77734\n[830]\tvalid_0's rmse: 2.7769\n[831]\tvalid_0's rmse: 2.77778\n[832]\tvalid_0's rmse: 2.77785\n[833]\tvalid_0's rmse: 2.77819\n[834]\tvalid_0's rmse: 2.77842\n[835]\tvalid_0's rmse: 2.77848\n[836]\tvalid_0's rmse: 2.77844\n[837]\tvalid_0's rmse: 2.77822\n[838]\tvalid_0's rmse: 2.77847\n[839]\tvalid_0's rmse: 2.77861\n[840]\tvalid_0's rmse: 2.77851\n[841]\tvalid_0's rmse: 2.7787\n[842]\tvalid_0's rmse: 2.77916\n[843]\tvalid_0's rmse: 2.77888\n[844]\tvalid_0's rmse: 2.77874\n[845]\tvalid_0's rmse: 2.77882\n[846]\tvalid_0's rmse: 2.77854\n[847]\tvalid_0's rmse: 2.77873\n[848]\tvalid_0's rmse: 2.77867\n[849]\tvalid_0's rmse: 2.77867\n[850]\tvalid_0's rmse: 2.77891\n[851]\tvalid_0's rmse: 2.77933\n[852]\tvalid_0's rmse: 2.7795\n[853]\tvalid_0's rmse: 2.77941\n[854]\tvalid_0's rmse: 2.77964\n[855]\tvalid_0's rmse: 2.77981\n[856]\tvalid_0's rmse: 2.7794\n[857]\tvalid_0's rmse: 2.7792\n[858]\tvalid_0's rmse: 2.77924\n[859]\tvalid_0's rmse: 2.77895\n[860]\tvalid_0's rmse: 2.77892\n[861]\tvalid_0's rmse: 2.77878\n[862]\tvalid_0's rmse: 2.77863\n[863]\tvalid_0's rmse: 2.7783\n[864]\tvalid_0's rmse: 2.77799\n[865]\tvalid_0's rmse: 2.77769\n[866]\tvalid_0's rmse: 2.77765\n[867]\tvalid_0's rmse: 2.77744\n[868]\tvalid_0's rmse: 2.77772\n[869]\tvalid_0's rmse: 2.77813\n[870]\tvalid_0's rmse: 2.77805\n[871]\tvalid_0's rmse: 2.778\n[872]\tvalid_0's rmse: 2.77842\n[873]\tvalid_0's rmse: 2.77828\n[874]\tvalid_0's rmse: 2.77825\n[875]\tvalid_0's rmse: 2.77861\n[876]\tvalid_0's rmse: 2.7789\n[877]\tvalid_0's rmse: 2.77915\n[878]\tvalid_0's rmse: 2.77925\n[879]\tvalid_0's rmse: 2.77984\n[880]\tvalid_0's rmse: 2.77996\n[881]\tvalid_0's rmse: 2.7792\n[882]\tvalid_0's rmse: 2.77941\n[883]\tvalid_0's rmse: 2.77948\n[884]\tvalid_0's rmse: 2.77986\n[885]\tvalid_0's rmse: 2.77997\n[886]\tvalid_0's rmse: 2.77978\n[887]\tvalid_0's rmse: 2.77993\n[888]\tvalid_0's rmse: 2.77996\n[889]\tvalid_0's rmse: 2.78013\n[890]\tvalid_0's rmse: 2.78002\n[891]\tvalid_0's rmse: 2.77996\n[892]\tvalid_0's rmse: 2.7804\n[893]\tvalid_0's rmse: 2.78045\n[894]\tvalid_0's rmse: 2.78034\n[895]\tvalid_0's rmse: 2.78038\n[896]\tvalid_0's rmse: 2.78087\n[897]\tvalid_0's rmse: 2.78089\n[898]\tvalid_0's rmse: 2.78126\n[899]\tvalid_0's rmse: 2.78135\n[900]\tvalid_0's rmse: 2.78176\n[901]\tvalid_0's rmse: 2.78171\n[902]\tvalid_0's rmse: 2.78197\n[903]\tvalid_0's rmse: 2.78196\n[904]\tvalid_0's rmse: 2.78214\n[905]\tvalid_0's rmse: 2.7822\n[906]\tvalid_0's rmse: 2.78216\n[907]\tvalid_0's rmse: 2.7821\n[908]\tvalid_0's rmse: 2.78224\n[909]\tvalid_0's rmse: 2.78338\n[910]\tvalid_0's rmse: 2.7834\n[911]\tvalid_0's rmse: 2.78311\n[912]\tvalid_0's rmse: 2.78307\n[913]\tvalid_0's rmse: 2.78321\n[914]\tvalid_0's rmse: 2.78364\n[915]\tvalid_0's rmse: 2.78374\n[916]\tvalid_0's rmse: 2.78396\n[917]\tvalid_0's rmse: 2.78393\n[918]\tvalid_0's rmse: 2.78371\n[919]\tvalid_0's rmse: 2.78392\n[920]\tvalid_0's rmse: 2.78387\n[921]\tvalid_0's rmse: 2.78388\n[922]\tvalid_0's rmse: 2.78384\n[923]\tvalid_0's rmse: 2.78381\n[924]\tvalid_0's rmse: 2.7839\n[925]\tvalid_0's rmse: 2.78412\n[926]\tvalid_0's rmse: 2.78436\n[927]\tvalid_0's rmse: 2.78434\n[928]\tvalid_0's rmse: 2.78436\n[929]\tvalid_0's rmse: 2.78464\n[930]\tvalid_0's rmse: 2.78448\n[931]\tvalid_0's rmse: 2.78438\n[932]\tvalid_0's rmse: 2.78439\n[933]\tvalid_0's rmse: 2.7839\n[934]\tvalid_0's rmse: 2.78386\n[935]\tvalid_0's rmse: 2.78426\n[936]\tvalid_0's rmse: 2.78444\n[937]\tvalid_0's rmse: 2.78425\n[938]\tvalid_0's rmse: 2.78466\n[939]\tvalid_0's rmse: 2.78444\n[940]\tvalid_0's rmse: 2.7843\n[941]\tvalid_0's rmse: 2.78437\n[942]\tvalid_0's rmse: 2.7849\n[943]\tvalid_0's rmse: 2.78482\n[944]\tvalid_0's rmse: 2.78458\n[945]\tvalid_0's rmse: 2.78425\n[946]\tvalid_0's rmse: 2.78409\n[947]\tvalid_0's rmse: 2.78405\n[948]\tvalid_0's rmse: 2.78434\n[949]\tvalid_0's rmse: 2.78425\n[950]\tvalid_0's rmse: 2.78506\n[951]\tvalid_0's rmse: 2.78528\n[952]\tvalid_0's rmse: 2.78573\n[953]\tvalid_0's rmse: 2.78585\n[954]\tvalid_0's rmse: 2.78608\n[955]\tvalid_0's rmse: 2.78633\n[956]\tvalid_0's rmse: 2.78653\n[957]\tvalid_0's rmse: 2.78669\n[958]\tvalid_0's rmse: 2.78699\n[959]\tvalid_0's rmse: 2.78695\n[960]\tvalid_0's rmse: 2.78676\n[961]\tvalid_0's rmse: 2.78738\n[962]\tvalid_0's rmse: 2.78778\n[963]\tvalid_0's rmse: 2.78772\n[964]\tvalid_0's rmse: 2.78812\n[965]\tvalid_0's rmse: 2.7879\n[966]\tvalid_0's rmse: 2.78804\n[967]\tvalid_0's rmse: 2.78841\n[968]\tvalid_0's rmse: 2.78876\n","name":"stdout"},{"output_type":"stream","text":"[969]\tvalid_0's rmse: 2.78868\n[970]\tvalid_0's rmse: 2.78862\n[971]\tvalid_0's rmse: 2.78899\n[972]\tvalid_0's rmse: 2.7893\n[973]\tvalid_0's rmse: 2.78926\n[974]\tvalid_0's rmse: 2.78944\n[975]\tvalid_0's rmse: 2.78925\n[976]\tvalid_0's rmse: 2.78909\n[977]\tvalid_0's rmse: 2.78891\n[978]\tvalid_0's rmse: 2.78906\n[979]\tvalid_0's rmse: 2.78902\n[980]\tvalid_0's rmse: 2.78927\n[981]\tvalid_0's rmse: 2.78942\n[982]\tvalid_0's rmse: 2.78968\n[983]\tvalid_0's rmse: 2.7897\n[984]\tvalid_0's rmse: 2.78993\n[985]\tvalid_0's rmse: 2.78997\n[986]\tvalid_0's rmse: 2.78972\n[987]\tvalid_0's rmse: 2.78984\n[988]\tvalid_0's rmse: 2.79016\n[989]\tvalid_0's rmse: 2.79029\n[990]\tvalid_0's rmse: 2.79026\n[991]\tvalid_0's rmse: 2.79043\n[992]\tvalid_0's rmse: 2.79035\n[993]\tvalid_0's rmse: 2.79045\n[994]\tvalid_0's rmse: 2.79041\n[995]\tvalid_0's rmse: 2.79056\n[996]\tvalid_0's rmse: 2.79075\n[997]\tvalid_0's rmse: 2.79108\n[998]\tvalid_0's rmse: 2.79095\n[999]\tvalid_0's rmse: 2.79099\n[1000]\tvalid_0's rmse: 2.79084\n[1001]\tvalid_0's rmse: 2.79039\n[1002]\tvalid_0's rmse: 2.79054\n[1003]\tvalid_0's rmse: 2.79078\n[1004]\tvalid_0's rmse: 2.79112\n[1005]\tvalid_0's rmse: 2.79119\n[1006]\tvalid_0's rmse: 2.79097\n[1007]\tvalid_0's rmse: 2.79096\n[1008]\tvalid_0's rmse: 2.79098\n[1009]\tvalid_0's rmse: 2.79116\n[1010]\tvalid_0's rmse: 2.79111\n[1011]\tvalid_0's rmse: 2.79129\n[1012]\tvalid_0's rmse: 2.79125\n[1013]\tvalid_0's rmse: 2.7914\n[1014]\tvalid_0's rmse: 2.7917\n[1015]\tvalid_0's rmse: 2.79172\n[1016]\tvalid_0's rmse: 2.79148\n[1017]\tvalid_0's rmse: 2.79123\n[1018]\tvalid_0's rmse: 2.79154\n[1019]\tvalid_0's rmse: 2.79134\n[1020]\tvalid_0's rmse: 2.79122\n[1021]\tvalid_0's rmse: 2.79149\n[1022]\tvalid_0's rmse: 2.79134\n[1023]\tvalid_0's rmse: 2.7916\n[1024]\tvalid_0's rmse: 2.79176\n[1025]\tvalid_0's rmse: 2.79199\n[1026]\tvalid_0's rmse: 2.79229\n[1027]\tvalid_0's rmse: 2.79232\n[1028]\tvalid_0's rmse: 2.79253\n[1029]\tvalid_0's rmse: 2.79241\n[1030]\tvalid_0's rmse: 2.79244\n[1031]\tvalid_0's rmse: 2.79227\n[1032]\tvalid_0's rmse: 2.7921\n[1033]\tvalid_0's rmse: 2.79222\n[1034]\tvalid_0's rmse: 2.79247\n[1035]\tvalid_0's rmse: 2.79286\n[1036]\tvalid_0's rmse: 2.79284\n[1037]\tvalid_0's rmse: 2.79288\n[1038]\tvalid_0's rmse: 2.79294\n[1039]\tvalid_0's rmse: 2.79309\n[1040]\tvalid_0's rmse: 2.79326\n[1041]\tvalid_0's rmse: 2.79329\n[1042]\tvalid_0's rmse: 2.79334\n[1043]\tvalid_0's rmse: 2.79366\n[1044]\tvalid_0's rmse: 2.79377\n[1045]\tvalid_0's rmse: 2.79434\n[1046]\tvalid_0's rmse: 2.79436\n[1047]\tvalid_0's rmse: 2.79439\n[1048]\tvalid_0's rmse: 2.79445\n[1049]\tvalid_0's rmse: 2.79441\n[1050]\tvalid_0's rmse: 2.79435\n[1051]\tvalid_0's rmse: 2.79436\n[1052]\tvalid_0's rmse: 2.79394\n[1053]\tvalid_0's rmse: 2.79381\n[1054]\tvalid_0's rmse: 2.79412\n[1055]\tvalid_0's rmse: 2.79418\n[1056]\tvalid_0's rmse: 2.79429\n[1057]\tvalid_0's rmse: 2.79427\n[1058]\tvalid_0's rmse: 2.79433\n[1059]\tvalid_0's rmse: 2.79444\n[1060]\tvalid_0's rmse: 2.79473\n[1061]\tvalid_0's rmse: 2.79493\n[1062]\tvalid_0's rmse: 2.7948\n[1063]\tvalid_0's rmse: 2.79505\n[1064]\tvalid_0's rmse: 2.79541\n[1065]\tvalid_0's rmse: 2.79557\n[1066]\tvalid_0's rmse: 2.79584\n[1067]\tvalid_0's rmse: 2.79593\n[1068]\tvalid_0's rmse: 2.79612\n[1069]\tvalid_0's rmse: 2.79619\n[1070]\tvalid_0's rmse: 2.79611\n[1071]\tvalid_0's rmse: 2.79612\n[1072]\tvalid_0's rmse: 2.79609\n[1073]\tvalid_0's rmse: 2.79603\n[1074]\tvalid_0's rmse: 2.79621\n[1075]\tvalid_0's rmse: 2.7962\n[1076]\tvalid_0's rmse: 2.7962\n[1077]\tvalid_0's rmse: 2.79623\n[1078]\tvalid_0's rmse: 2.79612\n[1079]\tvalid_0's rmse: 2.79613\n[1080]\tvalid_0's rmse: 2.79652\n[1081]\tvalid_0's rmse: 2.79653\n[1082]\tvalid_0's rmse: 2.79658\n[1083]\tvalid_0's rmse: 2.7966\n[1084]\tvalid_0's rmse: 2.79623\n[1085]\tvalid_0's rmse: 2.79611\n[1086]\tvalid_0's rmse: 2.7962\n[1087]\tvalid_0's rmse: 2.79664\n[1088]\tvalid_0's rmse: 2.79658\n[1089]\tvalid_0's rmse: 2.79651\n[1090]\tvalid_0's rmse: 2.79619\n[1091]\tvalid_0's rmse: 2.79607\n[1092]\tvalid_0's rmse: 2.79598\n[1093]\tvalid_0's rmse: 2.79578\n[1094]\tvalid_0's rmse: 2.79605\n[1095]\tvalid_0's rmse: 2.79638\n[1096]\tvalid_0's rmse: 2.79664\n[1097]\tvalid_0's rmse: 2.79649\n[1098]\tvalid_0's rmse: 2.79642\n[1099]\tvalid_0's rmse: 2.79628\n[1100]\tvalid_0's rmse: 2.79648\n[1101]\tvalid_0's rmse: 2.79662\n[1102]\tvalid_0's rmse: 2.79649\n[1103]\tvalid_0's rmse: 2.7966\n[1104]\tvalid_0's rmse: 2.79687\n[1105]\tvalid_0's rmse: 2.79728\n[1106]\tvalid_0's rmse: 2.79729\n[1107]\tvalid_0's rmse: 2.7974\n[1108]\tvalid_0's rmse: 2.79738\n[1109]\tvalid_0's rmse: 2.79764\n[1110]\tvalid_0's rmse: 2.79762\n[1111]\tvalid_0's rmse: 2.79768\n[1112]\tvalid_0's rmse: 2.79762\n[1113]\tvalid_0's rmse: 2.79749\n[1114]\tvalid_0's rmse: 2.79755\n[1115]\tvalid_0's rmse: 2.7975\n[1116]\tvalid_0's rmse: 2.79759\n[1117]\tvalid_0's rmse: 2.79758\n[1118]\tvalid_0's rmse: 2.79738\n[1119]\tvalid_0's rmse: 2.79761\n[1120]\tvalid_0's rmse: 2.79788\n[1121]\tvalid_0's rmse: 2.7979\n[1122]\tvalid_0's rmse: 2.79795\n[1123]\tvalid_0's rmse: 2.79792\n[1124]\tvalid_0's rmse: 2.79804\n[1125]\tvalid_0's rmse: 2.79739\n[1126]\tvalid_0's rmse: 2.79762\n[1127]\tvalid_0's rmse: 2.79734\n[1128]\tvalid_0's rmse: 2.79748\n[1129]\tvalid_0's rmse: 2.79764\n[1130]\tvalid_0's rmse: 2.7973\n[1131]\tvalid_0's rmse: 2.79761\n[1132]\tvalid_0's rmse: 2.79765\n[1133]\tvalid_0's rmse: 2.79775\n[1134]\tvalid_0's rmse: 2.79783\n[1135]\tvalid_0's rmse: 2.79777\n[1136]\tvalid_0's rmse: 2.79796\n[1137]\tvalid_0's rmse: 2.79782\n[1138]\tvalid_0's rmse: 2.79768\n[1139]\tvalid_0's rmse: 2.79781\n[1140]\tvalid_0's rmse: 2.79797\n[1141]\tvalid_0's rmse: 2.7975\n[1142]\tvalid_0's rmse: 2.79749\n[1143]\tvalid_0's rmse: 2.7976\n[1144]\tvalid_0's rmse: 2.7979\n[1145]\tvalid_0's rmse: 2.79811\n[1146]\tvalid_0's rmse: 2.79846\n[1147]\tvalid_0's rmse: 2.79863\n[1148]\tvalid_0's rmse: 2.79897\n[1149]\tvalid_0's rmse: 2.79914\n[1150]\tvalid_0's rmse: 2.79894\n[1151]\tvalid_0's rmse: 2.79874\n[1152]\tvalid_0's rmse: 2.79876\n[1153]\tvalid_0's rmse: 2.79883\n[1154]\tvalid_0's rmse: 2.79874\n[1155]\tvalid_0's rmse: 2.79856\n[1156]\tvalid_0's rmse: 2.79847\n[1157]\tvalid_0's rmse: 2.79865\n[1158]\tvalid_0's rmse: 2.7988\n[1159]\tvalid_0's rmse: 2.79874\n[1160]\tvalid_0's rmse: 2.79879\n[1161]\tvalid_0's rmse: 2.79886\n[1162]\tvalid_0's rmse: 2.79909\n[1163]\tvalid_0's rmse: 2.79902\n[1164]\tvalid_0's rmse: 2.79904\n[1165]\tvalid_0's rmse: 2.7989\n[1166]\tvalid_0's rmse: 2.79853\n[1167]\tvalid_0's rmse: 2.79812\n[1168]\tvalid_0's rmse: 2.79773\n[1169]\tvalid_0's rmse: 2.79762\n[1170]\tvalid_0's rmse: 2.79771\n[1171]\tvalid_0's rmse: 2.7976\n[1172]\tvalid_0's rmse: 2.79743\n[1173]\tvalid_0's rmse: 2.79779\n[1174]\tvalid_0's rmse: 2.79771\n[1175]\tvalid_0's rmse: 2.7979\n[1176]\tvalid_0's rmse: 2.79807\n[1177]\tvalid_0's rmse: 2.7977\n[1178]\tvalid_0's rmse: 2.79768\n[1179]\tvalid_0's rmse: 2.79775\n[1180]\tvalid_0's rmse: 2.7979\n[1181]\tvalid_0's rmse: 2.79789\n[1182]\tvalid_0's rmse: 2.79784\n[1183]\tvalid_0's rmse: 2.79787\n[1184]\tvalid_0's rmse: 2.79805\n[1185]\tvalid_0's rmse: 2.79802\n[1186]\tvalid_0's rmse: 2.79852\n[1187]\tvalid_0's rmse: 2.79856\n[1188]\tvalid_0's rmse: 2.79856\n[1189]\tvalid_0's rmse: 2.79808\n[1190]\tvalid_0's rmse: 2.79828\n[1191]\tvalid_0's rmse: 2.7983\n[1192]\tvalid_0's rmse: 2.79807\n[1193]\tvalid_0's rmse: 2.79808\n[1194]\tvalid_0's rmse: 2.79784\n[1195]\tvalid_0's rmse: 2.79778\n[1196]\tvalid_0's rmse: 2.79764\n[1197]\tvalid_0's rmse: 2.79763\n[1198]\tvalid_0's rmse: 2.79769\n[1199]\tvalid_0's rmse: 2.79814\n[1200]\tvalid_0's rmse: 2.79792\n[1201]\tvalid_0's rmse: 2.79792\n[1202]\tvalid_0's rmse: 2.79805\n[1203]\tvalid_0's rmse: 2.79829\n[1204]\tvalid_0's rmse: 2.79827\n[1205]\tvalid_0's rmse: 2.79838\n[1206]\tvalid_0's rmse: 2.79823\n[1207]\tvalid_0's rmse: 2.798\n[1208]\tvalid_0's rmse: 2.79871\n[1209]\tvalid_0's rmse: 2.79873\n[1210]\tvalid_0's rmse: 2.79863\n[1211]\tvalid_0's rmse: 2.79863\n[1212]\tvalid_0's rmse: 2.79877\n[1213]\tvalid_0's rmse: 2.79902\n[1214]\tvalid_0's rmse: 2.79911\n[1215]\tvalid_0's rmse: 2.79939\n[1216]\tvalid_0's rmse: 2.79952\n[1217]\tvalid_0's rmse: 2.79952\n[1218]\tvalid_0's rmse: 2.79962\n[1219]\tvalid_0's rmse: 2.79975\n[1220]\tvalid_0's rmse: 2.79978\n[1221]\tvalid_0's rmse: 2.79961\n[1222]\tvalid_0's rmse: 2.7996\n[1223]\tvalid_0's rmse: 2.79967\n[1224]\tvalid_0's rmse: 2.79972\n[1225]\tvalid_0's rmse: 2.79967\n[1226]\tvalid_0's rmse: 2.79951\n[1227]\tvalid_0's rmse: 2.79921\n[1228]\tvalid_0's rmse: 2.79906\n[1229]\tvalid_0's rmse: 2.79895\n[1230]\tvalid_0's rmse: 2.79889\n[1231]\tvalid_0's rmse: 2.79903\n[1232]\tvalid_0's rmse: 2.79889\n[1233]\tvalid_0's rmse: 2.79897\n[1234]\tvalid_0's rmse: 2.79904\n[1235]\tvalid_0's rmse: 2.7991\n[1236]\tvalid_0's rmse: 2.79924\n[1237]\tvalid_0's rmse: 2.79908\n[1238]\tvalid_0's rmse: 2.79911\n[1239]\tvalid_0's rmse: 2.799\n[1240]\tvalid_0's rmse: 2.79907\n[1241]\tvalid_0's rmse: 2.79927\n[1242]\tvalid_0's rmse: 2.79933\n[1243]\tvalid_0's rmse: 2.79941\n[1244]\tvalid_0's rmse: 2.79942\n[1245]\tvalid_0's rmse: 2.79976\n[1246]\tvalid_0's rmse: 2.79967\n[1247]\tvalid_0's rmse: 2.79955\n[1248]\tvalid_0's rmse: 2.79962\n[1249]\tvalid_0's rmse: 2.7997\n[1250]\tvalid_0's rmse: 2.79977\n[1251]\tvalid_0's rmse: 2.79967\n[1252]\tvalid_0's rmse: 2.79956\n[1253]\tvalid_0's rmse: 2.79972\n[1254]\tvalid_0's rmse: 2.80001\n[1255]\tvalid_0's rmse: 2.80004\n[1256]\tvalid_0's rmse: 2.80008\n[1257]\tvalid_0's rmse: 2.80017\n[1258]\tvalid_0's rmse: 2.80024\n[1259]\tvalid_0's rmse: 2.80012\n[1260]\tvalid_0's rmse: 2.80015\n[1261]\tvalid_0's rmse: 2.80013\n[1262]\tvalid_0's rmse: 2.80041\n[1263]\tvalid_0's rmse: 2.80072\n[1264]\tvalid_0's rmse: 2.80075\n[1265]\tvalid_0's rmse: 2.80097\n[1266]\tvalid_0's rmse: 2.80084\n[1267]\tvalid_0's rmse: 2.80074\n[1268]\tvalid_0's rmse: 2.80065\n[1269]\tvalid_0's rmse: 2.80049\n[1270]\tvalid_0's rmse: 2.80037\n[1271]\tvalid_0's rmse: 2.79999\n[1272]\tvalid_0's rmse: 2.80015\n[1273]\tvalid_0's rmse: 2.79995\n[1274]\tvalid_0's rmse: 2.80007\n[1275]\tvalid_0's rmse: 2.80007\n[1276]\tvalid_0's rmse: 2.80013\n[1277]\tvalid_0's rmse: 2.79987\n[1278]\tvalid_0's rmse: 2.79988\n[1279]\tvalid_0's rmse: 2.79977\n[1280]\tvalid_0's rmse: 2.79984\n[1281]\tvalid_0's rmse: 2.79992\n[1282]\tvalid_0's rmse: 2.79987\n[1283]\tvalid_0's rmse: 2.79993\n[1284]\tvalid_0's rmse: 2.79996\n[1285]\tvalid_0's rmse: 2.7999\n[1286]\tvalid_0's rmse: 2.79992\n[1287]\tvalid_0's rmse: 2.80026\n[1288]\tvalid_0's rmse: 2.80005\n[1289]\tvalid_0's rmse: 2.79997\n[1290]\tvalid_0's rmse: 2.79983\n","name":"stdout"},{"output_type":"stream","text":"[1291]\tvalid_0's rmse: 2.79989\n[1292]\tvalid_0's rmse: 2.79991\n[1293]\tvalid_0's rmse: 2.80035\n[1294]\tvalid_0's rmse: 2.80068\n[1295]\tvalid_0's rmse: 2.80096\n[1296]\tvalid_0's rmse: 2.80113\n[1297]\tvalid_0's rmse: 2.80093\n[1298]\tvalid_0's rmse: 2.80093\n[1299]\tvalid_0's rmse: 2.80132\n[1300]\tvalid_0's rmse: 2.8012\n[1301]\tvalid_0's rmse: 2.80109\n[1302]\tvalid_0's rmse: 2.80112\n[1303]\tvalid_0's rmse: 2.80127\n[1304]\tvalid_0's rmse: 2.80129\n[1305]\tvalid_0's rmse: 2.80142\n[1306]\tvalid_0's rmse: 2.80163\n[1307]\tvalid_0's rmse: 2.80181\n[1308]\tvalid_0's rmse: 2.80177\n[1309]\tvalid_0's rmse: 2.80199\n[1310]\tvalid_0's rmse: 2.80165\n[1311]\tvalid_0's rmse: 2.80162\n[1312]\tvalid_0's rmse: 2.80149\n[1313]\tvalid_0's rmse: 2.80169\n[1314]\tvalid_0's rmse: 2.80167\n[1315]\tvalid_0's rmse: 2.80178\n[1316]\tvalid_0's rmse: 2.80197\n[1317]\tvalid_0's rmse: 2.80207\n[1318]\tvalid_0's rmse: 2.80222\n[1319]\tvalid_0's rmse: 2.80219\n[1320]\tvalid_0's rmse: 2.80213\n[1321]\tvalid_0's rmse: 2.80198\n[1322]\tvalid_0's rmse: 2.80194\n[1323]\tvalid_0's rmse: 2.8017\n[1324]\tvalid_0's rmse: 2.80193\n[1325]\tvalid_0's rmse: 2.80209\n[1326]\tvalid_0's rmse: 2.80218\n[1327]\tvalid_0's rmse: 2.80225\n[1328]\tvalid_0's rmse: 2.80218\n[1329]\tvalid_0's rmse: 2.80238\n[1330]\tvalid_0's rmse: 2.80228\n[1331]\tvalid_0's rmse: 2.80203\n[1332]\tvalid_0's rmse: 2.8023\n[1333]\tvalid_0's rmse: 2.80214\n[1334]\tvalid_0's rmse: 2.80231\n[1335]\tvalid_0's rmse: 2.80236\n[1336]\tvalid_0's rmse: 2.80233\n[1337]\tvalid_0's rmse: 2.80251\n[1338]\tvalid_0's rmse: 2.80277\n[1339]\tvalid_0's rmse: 2.80284\n[1340]\tvalid_0's rmse: 2.80282\n[1341]\tvalid_0's rmse: 2.80271\n[1342]\tvalid_0's rmse: 2.80284\n[1343]\tvalid_0's rmse: 2.80287\n[1344]\tvalid_0's rmse: 2.80297\n[1345]\tvalid_0's rmse: 2.8029\n[1346]\tvalid_0's rmse: 2.80284\n[1347]\tvalid_0's rmse: 2.80286\n[1348]\tvalid_0's rmse: 2.80297\n[1349]\tvalid_0's rmse: 2.80294\n[1350]\tvalid_0's rmse: 2.80299\n[1351]\tvalid_0's rmse: 2.80303\n[1352]\tvalid_0's rmse: 2.8031\n[1353]\tvalid_0's rmse: 2.80295\n[1354]\tvalid_0's rmse: 2.80272\n[1355]\tvalid_0's rmse: 2.80264\n[1356]\tvalid_0's rmse: 2.80261\n[1357]\tvalid_0's rmse: 2.80263\n[1358]\tvalid_0's rmse: 2.80271\n[1359]\tvalid_0's rmse: 2.80253\n[1360]\tvalid_0's rmse: 2.80252\n[1361]\tvalid_0's rmse: 2.80227\n[1362]\tvalid_0's rmse: 2.80218\n[1363]\tvalid_0's rmse: 2.80202\n[1364]\tvalid_0's rmse: 2.80193\n[1365]\tvalid_0's rmse: 2.80188\n[1366]\tvalid_0's rmse: 2.80208\n[1367]\tvalid_0's rmse: 2.80213\n[1368]\tvalid_0's rmse: 2.80215\n[1369]\tvalid_0's rmse: 2.80214\n[1370]\tvalid_0's rmse: 2.80247\n[1371]\tvalid_0's rmse: 2.80248\n[1372]\tvalid_0's rmse: 2.80235\n[1373]\tvalid_0's rmse: 2.80236\n[1374]\tvalid_0's rmse: 2.8023\n[1375]\tvalid_0's rmse: 2.80261\n[1376]\tvalid_0's rmse: 2.80244\n[1377]\tvalid_0's rmse: 2.80236\n[1378]\tvalid_0's rmse: 2.80245\n[1379]\tvalid_0's rmse: 2.80283\n[1380]\tvalid_0's rmse: 2.80258\n[1381]\tvalid_0's rmse: 2.80254\n[1382]\tvalid_0's rmse: 2.80256\n[1383]\tvalid_0's rmse: 2.8028\n[1384]\tvalid_0's rmse: 2.80264\n[1385]\tvalid_0's rmse: 2.80264\n[1386]\tvalid_0's rmse: 2.80292\n[1387]\tvalid_0's rmse: 2.80295\n[1388]\tvalid_0's rmse: 2.80284\n[1389]\tvalid_0's rmse: 2.8029\n[1390]\tvalid_0's rmse: 2.80277\n[1391]\tvalid_0's rmse: 2.80304\n[1392]\tvalid_0's rmse: 2.80291\n[1393]\tvalid_0's rmse: 2.80311\n[1394]\tvalid_0's rmse: 2.80315\n[1395]\tvalid_0's rmse: 2.80333\n[1396]\tvalid_0's rmse: 2.80324\n[1397]\tvalid_0's rmse: 2.8032\n[1398]\tvalid_0's rmse: 2.80339\n[1399]\tvalid_0's rmse: 2.80356\n[1400]\tvalid_0's rmse: 2.8037\n[1401]\tvalid_0's rmse: 2.80374\n[1402]\tvalid_0's rmse: 2.80355\n[1403]\tvalid_0's rmse: 2.80364\n[1404]\tvalid_0's rmse: 2.80387\n[1405]\tvalid_0's rmse: 2.80401\n[1406]\tvalid_0's rmse: 2.80402\n[1407]\tvalid_0's rmse: 2.8041\n[1408]\tvalid_0's rmse: 2.80422\n[1409]\tvalid_0's rmse: 2.804\n[1410]\tvalid_0's rmse: 2.80394\n[1411]\tvalid_0's rmse: 2.80392\n[1412]\tvalid_0's rmse: 2.80404\n[1413]\tvalid_0's rmse: 2.80405\n[1414]\tvalid_0's rmse: 2.80405\n[1415]\tvalid_0's rmse: 2.80411\n[1416]\tvalid_0's rmse: 2.80408\n[1417]\tvalid_0's rmse: 2.80411\n[1418]\tvalid_0's rmse: 2.80391\n[1419]\tvalid_0's rmse: 2.8037\n[1420]\tvalid_0's rmse: 2.80386\n[1421]\tvalid_0's rmse: 2.80408\n[1422]\tvalid_0's rmse: 2.80412\n[1423]\tvalid_0's rmse: 2.80452\n[1424]\tvalid_0's rmse: 2.80471\n[1425]\tvalid_0's rmse: 2.8047\n[1426]\tvalid_0's rmse: 2.80468\n[1427]\tvalid_0's rmse: 2.80461\n[1428]\tvalid_0's rmse: 2.80456\n[1429]\tvalid_0's rmse: 2.80456\n[1430]\tvalid_0's rmse: 2.80448\n[1431]\tvalid_0's rmse: 2.80459\n[1432]\tvalid_0's rmse: 2.80451\n[1433]\tvalid_0's rmse: 2.80463\n[1434]\tvalid_0's rmse: 2.80478\n[1435]\tvalid_0's rmse: 2.8048\n[1436]\tvalid_0's rmse: 2.80523\n[1437]\tvalid_0's rmse: 2.80524\n[1438]\tvalid_0's rmse: 2.80517\n[1439]\tvalid_0's rmse: 2.8053\n[1440]\tvalid_0's rmse: 2.80551\n[1441]\tvalid_0's rmse: 2.80549\n[1442]\tvalid_0's rmse: 2.80553\n[1443]\tvalid_0's rmse: 2.80543\n[1444]\tvalid_0's rmse: 2.80547\n[1445]\tvalid_0's rmse: 2.80542\n[1446]\tvalid_0's rmse: 2.80531\n[1447]\tvalid_0's rmse: 2.80542\n[1448]\tvalid_0's rmse: 2.80533\n[1449]\tvalid_0's rmse: 2.80534\n[1450]\tvalid_0's rmse: 2.805\n[1451]\tvalid_0's rmse: 2.80508\n[1452]\tvalid_0's rmse: 2.80505\n[1453]\tvalid_0's rmse: 2.80493\n[1454]\tvalid_0's rmse: 2.80504\n[1455]\tvalid_0's rmse: 2.80525\n[1456]\tvalid_0's rmse: 2.80542\n[1457]\tvalid_0's rmse: 2.80542\n[1458]\tvalid_0's rmse: 2.80531\n[1459]\tvalid_0's rmse: 2.80515\n[1460]\tvalid_0's rmse: 2.80526\n[1461]\tvalid_0's rmse: 2.80516\n[1462]\tvalid_0's rmse: 2.80506\n[1463]\tvalid_0's rmse: 2.80503\n[1464]\tvalid_0's rmse: 2.80505\n[1465]\tvalid_0's rmse: 2.80511\n[1466]\tvalid_0's rmse: 2.80508\n[1467]\tvalid_0's rmse: 2.80512\n[1468]\tvalid_0's rmse: 2.80504\n[1469]\tvalid_0's rmse: 2.80505\n[1470]\tvalid_0's rmse: 2.80476\n[1471]\tvalid_0's rmse: 2.80483\n[1472]\tvalid_0's rmse: 2.80484\n[1473]\tvalid_0's rmse: 2.80503\n[1474]\tvalid_0's rmse: 2.80514\n[1475]\tvalid_0's rmse: 2.80511\n[1476]\tvalid_0's rmse: 2.80538\n[1477]\tvalid_0's rmse: 2.80541\n[1478]\tvalid_0's rmse: 2.80545\n[1479]\tvalid_0's rmse: 2.80535\n[1480]\tvalid_0's rmse: 2.80551\n[1481]\tvalid_0's rmse: 2.80545\n[1482]\tvalid_0's rmse: 2.80549\n[1483]\tvalid_0's rmse: 2.80541\n[1484]\tvalid_0's rmse: 2.80527\n[1485]\tvalid_0's rmse: 2.80559\n[1486]\tvalid_0's rmse: 2.80546\n[1487]\tvalid_0's rmse: 2.80552\n[1488]\tvalid_0's rmse: 2.80583\n[1489]\tvalid_0's rmse: 2.80594\n[1490]\tvalid_0's rmse: 2.80586\n[1491]\tvalid_0's rmse: 2.80591\n[1492]\tvalid_0's rmse: 2.80591\n[1493]\tvalid_0's rmse: 2.80586\n[1494]\tvalid_0's rmse: 2.80601\n[1495]\tvalid_0's rmse: 2.80589\n[1496]\tvalid_0's rmse: 2.80579\n[1497]\tvalid_0's rmse: 2.80552\n[1498]\tvalid_0's rmse: 2.80537\n[1499]\tvalid_0's rmse: 2.80536\n[1500]\tvalid_0's rmse: 2.80551\n[1501]\tvalid_0's rmse: 2.80543\n[1502]\tvalid_0's rmse: 2.80532\n[1503]\tvalid_0's rmse: 2.80526\n[1504]\tvalid_0's rmse: 2.80534\n[1505]\tvalid_0's rmse: 2.80539\n[1506]\tvalid_0's rmse: 2.80526\n[1507]\tvalid_0's rmse: 2.80531\n[1508]\tvalid_0's rmse: 2.80539\n[1509]\tvalid_0's rmse: 2.80554\n[1510]\tvalid_0's rmse: 2.80593\n[1511]\tvalid_0's rmse: 2.80584\n[1512]\tvalid_0's rmse: 2.80584\n[1513]\tvalid_0's rmse: 2.80585\n[1514]\tvalid_0's rmse: 2.80596\n[1515]\tvalid_0's rmse: 2.80612\n[1516]\tvalid_0's rmse: 2.80603\n[1517]\tvalid_0's rmse: 2.80608\n[1518]\tvalid_0's rmse: 2.80606\n[1519]\tvalid_0's rmse: 2.80623\n[1520]\tvalid_0's rmse: 2.80619\n[1521]\tvalid_0's rmse: 2.80621\n[1522]\tvalid_0's rmse: 2.80619\n[1523]\tvalid_0's rmse: 2.80621\n[1524]\tvalid_0's rmse: 2.80639\n[1525]\tvalid_0's rmse: 2.80655\n[1526]\tvalid_0's rmse: 2.80669\n[1527]\tvalid_0's rmse: 2.80689\n[1528]\tvalid_0's rmse: 2.80683\n[1529]\tvalid_0's rmse: 2.80693\n[1530]\tvalid_0's rmse: 2.80688\n[1531]\tvalid_0's rmse: 2.80701\n[1532]\tvalid_0's rmse: 2.80707\n[1533]\tvalid_0's rmse: 2.80725\n[1534]\tvalid_0's rmse: 2.8071\n[1535]\tvalid_0's rmse: 2.807\n[1536]\tvalid_0's rmse: 2.80707\n[1537]\tvalid_0's rmse: 2.80709\n[1538]\tvalid_0's rmse: 2.80718\n[1539]\tvalid_0's rmse: 2.80711\n[1540]\tvalid_0's rmse: 2.80693\n[1541]\tvalid_0's rmse: 2.80713\n[1542]\tvalid_0's rmse: 2.80723\n[1543]\tvalid_0's rmse: 2.80737\n[1544]\tvalid_0's rmse: 2.80761\n[1545]\tvalid_0's rmse: 2.80767\n[1546]\tvalid_0's rmse: 2.80762\n[1547]\tvalid_0's rmse: 2.80759\n[1548]\tvalid_0's rmse: 2.80761\n[1549]\tvalid_0's rmse: 2.80775\n[1550]\tvalid_0's rmse: 2.80785\n[1551]\tvalid_0's rmse: 2.80798\n[1552]\tvalid_0's rmse: 2.80814\n[1553]\tvalid_0's rmse: 2.80818\n[1554]\tvalid_0's rmse: 2.80808\n[1555]\tvalid_0's rmse: 2.80815\n[1556]\tvalid_0's rmse: 2.80821\n[1557]\tvalid_0's rmse: 2.8082\n[1558]\tvalid_0's rmse: 2.80809\n[1559]\tvalid_0's rmse: 2.80808\n[1560]\tvalid_0's rmse: 2.80808\n[1561]\tvalid_0's rmse: 2.80809\n[1562]\tvalid_0's rmse: 2.80803\n[1563]\tvalid_0's rmse: 2.80824\n[1564]\tvalid_0's rmse: 2.8083\n[1565]\tvalid_0's rmse: 2.80843\n[1566]\tvalid_0's rmse: 2.80844\n[1567]\tvalid_0's rmse: 2.80841\n[1568]\tvalid_0's rmse: 2.80845\n[1569]\tvalid_0's rmse: 2.80844\n[1570]\tvalid_0's rmse: 2.80823\n[1571]\tvalid_0's rmse: 2.80823\n[1572]\tvalid_0's rmse: 2.80828\n[1573]\tvalid_0's rmse: 2.8085\n[1574]\tvalid_0's rmse: 2.80847\n[1575]\tvalid_0's rmse: 2.80871\n[1576]\tvalid_0's rmse: 2.80885\n[1577]\tvalid_0's rmse: 2.809\n[1578]\tvalid_0's rmse: 2.80916\n[1579]\tvalid_0's rmse: 2.80932\n[1580]\tvalid_0's rmse: 2.80942\n[1581]\tvalid_0's rmse: 2.80947\n[1582]\tvalid_0's rmse: 2.80954\n[1583]\tvalid_0's rmse: 2.80939\n[1584]\tvalid_0's rmse: 2.80919\n[1585]\tvalid_0's rmse: 2.8091\n[1586]\tvalid_0's rmse: 2.8092\n[1587]\tvalid_0's rmse: 2.80926\n[1588]\tvalid_0's rmse: 2.80926\n[1589]\tvalid_0's rmse: 2.80934\n[1590]\tvalid_0's rmse: 2.80954\n[1591]\tvalid_0's rmse: 2.80964\n[1592]\tvalid_0's rmse: 2.80975\n[1593]\tvalid_0's rmse: 2.80975\n[1594]\tvalid_0's rmse: 2.80995\n[1595]\tvalid_0's rmse: 2.80995\n[1596]\tvalid_0's rmse: 2.81014\n[1597]\tvalid_0's rmse: 2.81022\n[1598]\tvalid_0's rmse: 2.81035\n[1599]\tvalid_0's rmse: 2.81038\n[1600]\tvalid_0's rmse: 2.81046\n[1601]\tvalid_0's rmse: 2.81041\n[1602]\tvalid_0's rmse: 2.81061\n[1603]\tvalid_0's rmse: 2.81053\n[1604]\tvalid_0's rmse: 2.81049\n[1605]\tvalid_0's rmse: 2.81062\n[1606]\tvalid_0's rmse: 2.81051\n[1607]\tvalid_0's rmse: 2.81054\n[1608]\tvalid_0's rmse: 2.8104\n[1609]\tvalid_0's rmse: 2.81063\n[1610]\tvalid_0's rmse: 2.81075\n[1611]\tvalid_0's rmse: 2.81067\n[1612]\tvalid_0's rmse: 2.81057\n[1613]\tvalid_0's rmse: 2.81065\n[1614]\tvalid_0's rmse: 2.81079\n[1615]\tvalid_0's rmse: 2.81058\n[1616]\tvalid_0's rmse: 2.81064\n[1617]\tvalid_0's rmse: 2.8108\n[1618]\tvalid_0's rmse: 2.81088\n[1619]\tvalid_0's rmse: 2.81105\n[1620]\tvalid_0's rmse: 2.81107\n[1621]\tvalid_0's rmse: 2.81112\n[1622]\tvalid_0's rmse: 2.81127\n[1623]\tvalid_0's rmse: 2.8114\n[1624]\tvalid_0's rmse: 2.81158\n[1625]\tvalid_0's rmse: 2.81158\n[1626]\tvalid_0's rmse: 2.81155\n[1627]\tvalid_0's rmse: 2.81156\n[1628]\tvalid_0's rmse: 2.81151\n[1629]\tvalid_0's rmse: 2.8116\n[1630]\tvalid_0's rmse: 2.81171\n","name":"stdout"},{"output_type":"stream","text":"[1631]\tvalid_0's rmse: 2.81185\n[1632]\tvalid_0's rmse: 2.81162\n[1633]\tvalid_0's rmse: 2.81162\n[1634]\tvalid_0's rmse: 2.81169\n[1635]\tvalid_0's rmse: 2.81174\n[1636]\tvalid_0's rmse: 2.81199\n[1637]\tvalid_0's rmse: 2.81207\n[1638]\tvalid_0's rmse: 2.81207\n[1639]\tvalid_0's rmse: 2.81219\n[1640]\tvalid_0's rmse: 2.81211\n[1641]\tvalid_0's rmse: 2.81203\n[1642]\tvalid_0's rmse: 2.81212\n[1643]\tvalid_0's rmse: 2.8123\n[1644]\tvalid_0's rmse: 2.81224\n[1645]\tvalid_0's rmse: 2.81218\n[1646]\tvalid_0's rmse: 2.81222\n[1647]\tvalid_0's rmse: 2.81217\n[1648]\tvalid_0's rmse: 2.81219\n[1649]\tvalid_0's rmse: 2.81225\n[1650]\tvalid_0's rmse: 2.81236\n[1651]\tvalid_0's rmse: 2.81238\n[1652]\tvalid_0's rmse: 2.81245\n[1653]\tvalid_0's rmse: 2.81256\n[1654]\tvalid_0's rmse: 2.81262\n[1655]\tvalid_0's rmse: 2.81273\n[1656]\tvalid_0's rmse: 2.81282\n[1657]\tvalid_0's rmse: 2.81299\n[1658]\tvalid_0's rmse: 2.81306\n[1659]\tvalid_0's rmse: 2.81336\n[1660]\tvalid_0's rmse: 2.8133\n[1661]\tvalid_0's rmse: 2.81335\n[1662]\tvalid_0's rmse: 2.81317\n[1663]\tvalid_0's rmse: 2.81322\n[1664]\tvalid_0's rmse: 2.81323\n[1665]\tvalid_0's rmse: 2.81332\n[1666]\tvalid_0's rmse: 2.81337\n[1667]\tvalid_0's rmse: 2.81335\n[1668]\tvalid_0's rmse: 2.81329\n[1669]\tvalid_0's rmse: 2.81327\n[1670]\tvalid_0's rmse: 2.81304\n[1671]\tvalid_0's rmse: 2.81309\n[1672]\tvalid_0's rmse: 2.81309\n[1673]\tvalid_0's rmse: 2.81297\n[1674]\tvalid_0's rmse: 2.81276\n[1675]\tvalid_0's rmse: 2.81283\n[1676]\tvalid_0's rmse: 2.81276\n[1677]\tvalid_0's rmse: 2.81266\n[1678]\tvalid_0's rmse: 2.81256\n[1679]\tvalid_0's rmse: 2.81275\n[1680]\tvalid_0's rmse: 2.81279\n[1681]\tvalid_0's rmse: 2.81269\n[1682]\tvalid_0's rmse: 2.81276\n[1683]\tvalid_0's rmse: 2.81303\n[1684]\tvalid_0's rmse: 2.81315\n[1685]\tvalid_0's rmse: 2.8131\n[1686]\tvalid_0's rmse: 2.81306\n[1687]\tvalid_0's rmse: 2.81304\n[1688]\tvalid_0's rmse: 2.81297\n[1689]\tvalid_0's rmse: 2.81298\n[1690]\tvalid_0's rmse: 2.81321\n[1691]\tvalid_0's rmse: 2.8132\n[1692]\tvalid_0's rmse: 2.81331\n[1693]\tvalid_0's rmse: 2.81319\n[1694]\tvalid_0's rmse: 2.8132\n[1695]\tvalid_0's rmse: 2.81324\n[1696]\tvalid_0's rmse: 2.81313\n[1697]\tvalid_0's rmse: 2.81322\n[1698]\tvalid_0's rmse: 2.81331\n[1699]\tvalid_0's rmse: 2.81321\n[1700]\tvalid_0's rmse: 2.81337\n[1701]\tvalid_0's rmse: 2.81346\n[1702]\tvalid_0's rmse: 2.81368\n[1703]\tvalid_0's rmse: 2.8136\n[1704]\tvalid_0's rmse: 2.81343\n[1705]\tvalid_0's rmse: 2.81346\n[1706]\tvalid_0's rmse: 2.81352\n[1707]\tvalid_0's rmse: 2.81337\n[1708]\tvalid_0's rmse: 2.81332\n[1709]\tvalid_0's rmse: 2.81327\n[1710]\tvalid_0's rmse: 2.81331\n[1711]\tvalid_0's rmse: 2.81315\n[1712]\tvalid_0's rmse: 2.81312\n[1713]\tvalid_0's rmse: 2.81318\n[1714]\tvalid_0's rmse: 2.81332\n[1715]\tvalid_0's rmse: 2.81338\n[1716]\tvalid_0's rmse: 2.8134\n[1717]\tvalid_0's rmse: 2.8133\n[1718]\tvalid_0's rmse: 2.8133\n[1719]\tvalid_0's rmse: 2.8133\n[1720]\tvalid_0's rmse: 2.81333\n[1721]\tvalid_0's rmse: 2.81312\n[1722]\tvalid_0's rmse: 2.81314\n[1723]\tvalid_0's rmse: 2.81321\n[1724]\tvalid_0's rmse: 2.81327\n[1725]\tvalid_0's rmse: 2.81335\n[1726]\tvalid_0's rmse: 2.8132\n[1727]\tvalid_0's rmse: 2.81331\n[1728]\tvalid_0's rmse: 2.8133\n[1729]\tvalid_0's rmse: 2.8131\n[1730]\tvalid_0's rmse: 2.81307\n[1731]\tvalid_0's rmse: 2.81304\n[1732]\tvalid_0's rmse: 2.81298\n[1733]\tvalid_0's rmse: 2.81292\n[1734]\tvalid_0's rmse: 2.81305\n[1735]\tvalid_0's rmse: 2.81316\n[1736]\tvalid_0's rmse: 2.81309\n[1737]\tvalid_0's rmse: 2.81325\n[1738]\tvalid_0's rmse: 2.81326\n[1739]\tvalid_0's rmse: 2.81348\n[1740]\tvalid_0's rmse: 2.8136\n[1741]\tvalid_0's rmse: 2.81363\n[1742]\tvalid_0's rmse: 2.81356\n[1743]\tvalid_0's rmse: 2.81353\n[1744]\tvalid_0's rmse: 2.81371\n[1745]\tvalid_0's rmse: 2.81354\n[1746]\tvalid_0's rmse: 2.81365\n[1747]\tvalid_0's rmse: 2.8137\n[1748]\tvalid_0's rmse: 2.81362\n[1749]\tvalid_0's rmse: 2.8136\n[1750]\tvalid_0's rmse: 2.81367\n[1751]\tvalid_0's rmse: 2.81354\n[1752]\tvalid_0's rmse: 2.8136\n[1753]\tvalid_0's rmse: 2.81368\n[1754]\tvalid_0's rmse: 2.81362\n[1755]\tvalid_0's rmse: 2.81352\n[1756]\tvalid_0's rmse: 2.81349\n[1757]\tvalid_0's rmse: 2.81352\n[1758]\tvalid_0's rmse: 2.81366\n[1759]\tvalid_0's rmse: 2.8136\n[1760]\tvalid_0's rmse: 2.81382\n[1761]\tvalid_0's rmse: 2.8139\n[1762]\tvalid_0's rmse: 2.81409\n[1763]\tvalid_0's rmse: 2.81421\n[1764]\tvalid_0's rmse: 2.8144\n[1765]\tvalid_0's rmse: 2.81445\n[1766]\tvalid_0's rmse: 2.81445\n[1767]\tvalid_0's rmse: 2.81451\n[1768]\tvalid_0's rmse: 2.81456\n[1769]\tvalid_0's rmse: 2.8146\n[1770]\tvalid_0's rmse: 2.81467\n[1771]\tvalid_0's rmse: 2.81469\n[1772]\tvalid_0's rmse: 2.81451\n[1773]\tvalid_0's rmse: 2.81457\n[1774]\tvalid_0's rmse: 2.8147\n[1775]\tvalid_0's rmse: 2.81464\n[1776]\tvalid_0's rmse: 2.81475\n[1777]\tvalid_0's rmse: 2.81502\n[1778]\tvalid_0's rmse: 2.81509\n[1779]\tvalid_0's rmse: 2.8151\n[1780]\tvalid_0's rmse: 2.81521\n[1781]\tvalid_0's rmse: 2.8152\n[1782]\tvalid_0's rmse: 2.81539\n[1783]\tvalid_0's rmse: 2.81553\n[1784]\tvalid_0's rmse: 2.81553\n[1785]\tvalid_0's rmse: 2.8156\n[1786]\tvalid_0's rmse: 2.81564\n[1787]\tvalid_0's rmse: 2.81567\n[1788]\tvalid_0's rmse: 2.81568\n[1789]\tvalid_0's rmse: 2.81572\n[1790]\tvalid_0's rmse: 2.8158\n[1791]\tvalid_0's rmse: 2.81571\n[1792]\tvalid_0's rmse: 2.81566\n[1793]\tvalid_0's rmse: 2.81565\n[1794]\tvalid_0's rmse: 2.81567\n[1795]\tvalid_0's rmse: 2.8157\n[1796]\tvalid_0's rmse: 2.81551\n[1797]\tvalid_0's rmse: 2.81566\n[1798]\tvalid_0's rmse: 2.81561\n[1799]\tvalid_0's rmse: 2.81554\n[1800]\tvalid_0's rmse: 2.81557\n[1801]\tvalid_0's rmse: 2.81563\n[1802]\tvalid_0's rmse: 2.81559\n[1803]\tvalid_0's rmse: 2.81553\n[1804]\tvalid_0's rmse: 2.81568\n[1805]\tvalid_0's rmse: 2.81564\n[1806]\tvalid_0's rmse: 2.8155\n[1807]\tvalid_0's rmse: 2.81552\n[1808]\tvalid_0's rmse: 2.81549\n[1809]\tvalid_0's rmse: 2.81548\n[1810]\tvalid_0's rmse: 2.81546\n[1811]\tvalid_0's rmse: 2.81562\n[1812]\tvalid_0's rmse: 2.81544\n[1813]\tvalid_0's rmse: 2.81546\n[1814]\tvalid_0's rmse: 2.81551\n[1815]\tvalid_0's rmse: 2.81566\n[1816]\tvalid_0's rmse: 2.81568\n[1817]\tvalid_0's rmse: 2.81578\n[1818]\tvalid_0's rmse: 2.81593\n[1819]\tvalid_0's rmse: 2.81583\n[1820]\tvalid_0's rmse: 2.81582\n[1821]\tvalid_0's rmse: 2.81581\n[1822]\tvalid_0's rmse: 2.81581\n[1823]\tvalid_0's rmse: 2.8159\n[1824]\tvalid_0's rmse: 2.81586\n[1825]\tvalid_0's rmse: 2.81583\n[1826]\tvalid_0's rmse: 2.81593\n[1827]\tvalid_0's rmse: 2.81602\n[1828]\tvalid_0's rmse: 2.81602\n[1829]\tvalid_0's rmse: 2.81601\n[1830]\tvalid_0's rmse: 2.81617\n[1831]\tvalid_0's rmse: 2.81631\n[1832]\tvalid_0's rmse: 2.81639\n[1833]\tvalid_0's rmse: 2.81635\n[1834]\tvalid_0's rmse: 2.81649\n[1835]\tvalid_0's rmse: 2.81646\n[1836]\tvalid_0's rmse: 2.81638\n[1837]\tvalid_0's rmse: 2.81641\n[1838]\tvalid_0's rmse: 2.81635\n[1839]\tvalid_0's rmse: 2.81659\n[1840]\tvalid_0's rmse: 2.81658\n[1841]\tvalid_0's rmse: 2.81664\n[1842]\tvalid_0's rmse: 2.81678\n[1843]\tvalid_0's rmse: 2.81695\n[1844]\tvalid_0's rmse: 2.81712\n[1845]\tvalid_0's rmse: 2.81725\n[1846]\tvalid_0's rmse: 2.8174\n[1847]\tvalid_0's rmse: 2.81749\n[1848]\tvalid_0's rmse: 2.81746\n[1849]\tvalid_0's rmse: 2.81745\n[1850]\tvalid_0's rmse: 2.81715\n[1851]\tvalid_0's rmse: 2.81714\n[1852]\tvalid_0's rmse: 2.81717\n[1853]\tvalid_0's rmse: 2.81707\n[1854]\tvalid_0's rmse: 2.81703\n[1855]\tvalid_0's rmse: 2.81709\n[1856]\tvalid_0's rmse: 2.8172\n[1857]\tvalid_0's rmse: 2.81722\n[1858]\tvalid_0's rmse: 2.81739\n[1859]\tvalid_0's rmse: 2.81748\n[1860]\tvalid_0's rmse: 2.81773\n[1861]\tvalid_0's rmse: 2.81782\n[1862]\tvalid_0's rmse: 2.81783\n[1863]\tvalid_0's rmse: 2.81793\n[1864]\tvalid_0's rmse: 2.81792\n[1865]\tvalid_0's rmse: 2.81804\n[1866]\tvalid_0's rmse: 2.818\n[1867]\tvalid_0's rmse: 2.81785\n[1868]\tvalid_0's rmse: 2.81785\n[1869]\tvalid_0's rmse: 2.81787\n[1870]\tvalid_0's rmse: 2.81792\n[1871]\tvalid_0's rmse: 2.81782\n[1872]\tvalid_0's rmse: 2.81795\n[1873]\tvalid_0's rmse: 2.818\n[1874]\tvalid_0's rmse: 2.81806\n[1875]\tvalid_0's rmse: 2.81792\n[1876]\tvalid_0's rmse: 2.81805\n[1877]\tvalid_0's rmse: 2.8181\n[1878]\tvalid_0's rmse: 2.8183\n[1879]\tvalid_0's rmse: 2.81838\n[1880]\tvalid_0's rmse: 2.81838\n[1881]\tvalid_0's rmse: 2.81846\n[1882]\tvalid_0's rmse: 2.81856\n[1883]\tvalid_0's rmse: 2.81867\n[1884]\tvalid_0's rmse: 2.81886\n[1885]\tvalid_0's rmse: 2.81895\n[1886]\tvalid_0's rmse: 2.81899\n[1887]\tvalid_0's rmse: 2.8191\n[1888]\tvalid_0's rmse: 2.819\n[1889]\tvalid_0's rmse: 2.81895\n[1890]\tvalid_0's rmse: 2.81905\n[1891]\tvalid_0's rmse: 2.81911\n[1892]\tvalid_0's rmse: 2.81905\n[1893]\tvalid_0's rmse: 2.81906\n[1894]\tvalid_0's rmse: 2.81913\n[1895]\tvalid_0's rmse: 2.81914\n[1896]\tvalid_0's rmse: 2.81918\n[1897]\tvalid_0's rmse: 2.8192\n[1898]\tvalid_0's rmse: 2.81923\n[1899]\tvalid_0's rmse: 2.81918\n[1900]\tvalid_0's rmse: 2.81926\n[1901]\tvalid_0's rmse: 2.81931\n[1902]\tvalid_0's rmse: 2.81922\n[1903]\tvalid_0's rmse: 2.81922\n[1904]\tvalid_0's rmse: 2.81933\n[1905]\tvalid_0's rmse: 2.81919\n[1906]\tvalid_0's rmse: 2.81933\n[1907]\tvalid_0's rmse: 2.81933\n[1908]\tvalid_0's rmse: 2.81938\n[1909]\tvalid_0's rmse: 2.81938\n[1910]\tvalid_0's rmse: 2.81952\n[1911]\tvalid_0's rmse: 2.81959\n[1912]\tvalid_0's rmse: 2.81949\n[1913]\tvalid_0's rmse: 2.81961\n[1914]\tvalid_0's rmse: 2.81962\n[1915]\tvalid_0's rmse: 2.81965\n[1916]\tvalid_0's rmse: 2.81969\n[1917]\tvalid_0's rmse: 2.81982\n[1918]\tvalid_0's rmse: 2.81977\n[1919]\tvalid_0's rmse: 2.81994\n[1920]\tvalid_0's rmse: 2.81999\n[1921]\tvalid_0's rmse: 2.81992\n[1922]\tvalid_0's rmse: 2.82008\n[1923]\tvalid_0's rmse: 2.82004\n[1924]\tvalid_0's rmse: 2.82015\n[1925]\tvalid_0's rmse: 2.82012\n[1926]\tvalid_0's rmse: 2.8201\n[1927]\tvalid_0's rmse: 2.82013\n[1928]\tvalid_0's rmse: 2.82012\n[1929]\tvalid_0's rmse: 2.82015\n[1930]\tvalid_0's rmse: 2.82016\n[1931]\tvalid_0's rmse: 2.82008\n[1932]\tvalid_0's rmse: 2.82006\n[1933]\tvalid_0's rmse: 2.82021\n[1934]\tvalid_0's rmse: 2.8203\n[1935]\tvalid_0's rmse: 2.82027\n[1936]\tvalid_0's rmse: 2.82018\n[1937]\tvalid_0's rmse: 2.82016\n[1938]\tvalid_0's rmse: 2.82029\n[1939]\tvalid_0's rmse: 2.82033\n[1940]\tvalid_0's rmse: 2.82041\n[1941]\tvalid_0's rmse: 2.82051\n[1942]\tvalid_0's rmse: 2.82049\n[1943]\tvalid_0's rmse: 2.8205\n[1944]\tvalid_0's rmse: 2.82058\n[1945]\tvalid_0's rmse: 2.82056\n[1946]\tvalid_0's rmse: 2.82053\n[1947]\tvalid_0's rmse: 2.82036\n[1948]\tvalid_0's rmse: 2.82041\n[1949]\tvalid_0's rmse: 2.82032\n[1950]\tvalid_0's rmse: 2.82027\n[1951]\tvalid_0's rmse: 2.82035\n[1952]\tvalid_0's rmse: 2.82044\n[1953]\tvalid_0's rmse: 2.8204\n[1954]\tvalid_0's rmse: 2.82042\n[1955]\tvalid_0's rmse: 2.82053\n[1956]\tvalid_0's rmse: 2.82068\n[1957]\tvalid_0's rmse: 2.82073\n[1958]\tvalid_0's rmse: 2.82066\n[1959]\tvalid_0's rmse: 2.82067\n[1960]\tvalid_0's rmse: 2.82068\n[1961]\tvalid_0's rmse: 2.82059\n[1962]\tvalid_0's rmse: 2.82077\n[1963]\tvalid_0's rmse: 2.82078\n[1964]\tvalid_0's rmse: 2.82101\n[1965]\tvalid_0's rmse: 2.82098\n[1966]\tvalid_0's rmse: 2.82103\n[1967]\tvalid_0's rmse: 2.82097\n[1968]\tvalid_0's rmse: 2.82109\n[1969]\tvalid_0's rmse: 2.82106\n[1970]\tvalid_0's rmse: 2.8211\n[1971]\tvalid_0's rmse: 2.82113\n[1972]\tvalid_0's rmse: 2.82128\n[1973]\tvalid_0's rmse: 2.82122\n[1974]\tvalid_0's rmse: 2.82125\n[1975]\tvalid_0's rmse: 2.82121\n[1976]\tvalid_0's rmse: 2.82113\n[1977]\tvalid_0's rmse: 2.82118\n[1978]\tvalid_0's rmse: 2.82124\n[1979]\tvalid_0's rmse: 2.82137\n[1980]\tvalid_0's rmse: 2.8214\n[1981]\tvalid_0's rmse: 2.82151\n[1982]\tvalid_0's rmse: 2.82141\n[1983]\tvalid_0's rmse: 2.82136\n[1984]\tvalid_0's rmse: 2.82136\n[1985]\tvalid_0's rmse: 2.82136\n[1986]\tvalid_0's rmse: 2.82136\n[1987]\tvalid_0's rmse: 2.82131\n[1988]\tvalid_0's rmse: 2.82133\n[1989]\tvalid_0's rmse: 2.82131\n[1990]\tvalid_0's rmse: 2.82132\n[1991]\tvalid_0's rmse: 2.82131\n[1992]\tvalid_0's rmse: 2.82128\n[1993]\tvalid_0's rmse: 2.82117\n[1994]\tvalid_0's rmse: 2.82122\n[1995]\tvalid_0's rmse: 2.82118\n[1996]\tvalid_0's rmse: 2.82118\n[1997]\tvalid_0's rmse: 2.82122\n[1998]\tvalid_0's rmse: 2.82136\n[1999]\tvalid_0's rmse: 2.82137\n[2000]\tvalid_0's rmse: 2.82145\n[2001]\tvalid_0's rmse: 2.82143\n[2002]\tvalid_0's rmse: 2.82146\n[2003]\tvalid_0's rmse: 2.82148\n[2004]\tvalid_0's rmse: 2.82149\n[2005]\tvalid_0's rmse: 2.82149\n[2006]\tvalid_0's rmse: 2.82149\n[2007]\tvalid_0's rmse: 2.82148\n[2008]\tvalid_0's rmse: 2.82151\n[2009]\tvalid_0's rmse: 2.82155\n[2010]\tvalid_0's rmse: 2.82158\n[2011]\tvalid_0's rmse: 2.82159\n[2012]\tvalid_0's rmse: 2.82158\n[2013]\tvalid_0's rmse: 2.82163\n[2014]\tvalid_0's rmse: 2.82166\n[2015]\tvalid_0's rmse: 2.82162\n[2016]\tvalid_0's rmse: 2.82148\n[2017]\tvalid_0's rmse: 2.82138\n[2018]\tvalid_0's rmse: 2.82135\n[2019]\tvalid_0's rmse: 2.82114\n[2020]\tvalid_0's rmse: 2.82117\n[2021]\tvalid_0's rmse: 2.82122\n[2022]\tvalid_0's rmse: 2.82126\n[2023]\tvalid_0's rmse: 2.8212\n[2024]\tvalid_0's rmse: 2.82118\n[2025]\tvalid_0's rmse: 2.82115\n[2026]\tvalid_0's rmse: 2.8212\n[2027]\tvalid_0's rmse: 2.82119\nEarly stopping, best iteration is:\n[27]\tvalid_0's rmse: 2.65952\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_lgbm","execution_count":57,"outputs":[{"output_type":"execute_result","execution_count":57,"data":{"text/plain":"      seg_id  time_to_failure\n0          0         3.569635\n1          1         5.097076\n2          2         5.550940\n3          3         8.409831\n4          4         6.650740\n5          5         2.660450\n6          6         7.996492\n7          7         3.629713\n8          8         4.652942\n9          9         2.729601\n10        10         2.273888\n11        11         5.781796\n12        12         4.726734\n13        13         2.961602\n14        14         8.975710\n15        15         4.396703\n16        16         5.511370\n17        17         3.791953\n18        18         5.854818\n19        19         4.854001\n20        20         5.318729\n21        21         8.252934\n22        22         3.361629\n23        23         4.751183\n24        24         6.807055\n25        25         4.751733\n26        26         8.298593\n27        27         4.975183\n28        28         2.368711\n29        29         5.563858\n...      ...              ...\n2594    2594         9.043609\n2595    2595         5.266156\n2596    2596         5.836935\n2597    2597         8.869520\n2598    2598         5.719555\n2599    2599         5.125391\n2600    2600         5.405804\n2601    2601         5.829225\n2602    2602         4.384703\n2603    2603         4.591688\n2604    2604         2.636692\n2605    2605         5.641712\n2606    2606         5.684807\n2607    2607         2.318347\n2608    2608         4.803714\n2609    2609         3.690027\n2610    2610         6.306122\n2611    2611         8.376749\n2612    2612         5.043820\n2613    2613         7.454702\n2614    2614         6.934077\n2615    2615         8.050143\n2616    2616         3.727786\n2617    2617         8.811284\n2618    2618         5.872963\n2619    2619         5.796127\n2620    2620         6.480726\n2621    2621         4.451813\n2622    2622         2.225375\n2623    2623         8.688369\n\n[2624 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>seg_id</th>\n      <th>time_to_failure</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>3.569635</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>5.097076</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>5.550940</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>8.409831</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>6.650740</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>5</td>\n      <td>2.660450</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>6</td>\n      <td>7.996492</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>7</td>\n      <td>3.629713</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>8</td>\n      <td>4.652942</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>9</td>\n      <td>2.729601</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>10</td>\n      <td>2.273888</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>11</td>\n      <td>5.781796</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>12</td>\n      <td>4.726734</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>13</td>\n      <td>2.961602</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>14</td>\n      <td>8.975710</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>15</td>\n      <td>4.396703</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>16</td>\n      <td>5.511370</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>17</td>\n      <td>3.791953</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>18</td>\n      <td>5.854818</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>19</td>\n      <td>4.854001</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>20</td>\n      <td>5.318729</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>21</td>\n      <td>8.252934</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>22</td>\n      <td>3.361629</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>23</td>\n      <td>4.751183</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>24</td>\n      <td>6.807055</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>25</td>\n      <td>4.751733</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>26</td>\n      <td>8.298593</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>27</td>\n      <td>4.975183</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>28</td>\n      <td>2.368711</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>29</td>\n      <td>5.563858</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2594</th>\n      <td>2594</td>\n      <td>9.043609</td>\n    </tr>\n    <tr>\n      <th>2595</th>\n      <td>2595</td>\n      <td>5.266156</td>\n    </tr>\n    <tr>\n      <th>2596</th>\n      <td>2596</td>\n      <td>5.836935</td>\n    </tr>\n    <tr>\n      <th>2597</th>\n      <td>2597</td>\n      <td>8.869520</td>\n    </tr>\n    <tr>\n      <th>2598</th>\n      <td>2598</td>\n      <td>5.719555</td>\n    </tr>\n    <tr>\n      <th>2599</th>\n      <td>2599</td>\n      <td>5.125391</td>\n    </tr>\n    <tr>\n      <th>2600</th>\n      <td>2600</td>\n      <td>5.405804</td>\n    </tr>\n    <tr>\n      <th>2601</th>\n      <td>2601</td>\n      <td>5.829225</td>\n    </tr>\n    <tr>\n      <th>2602</th>\n      <td>2602</td>\n      <td>4.384703</td>\n    </tr>\n    <tr>\n      <th>2603</th>\n      <td>2603</td>\n      <td>4.591688</td>\n    </tr>\n    <tr>\n      <th>2604</th>\n      <td>2604</td>\n      <td>2.636692</td>\n    </tr>\n    <tr>\n      <th>2605</th>\n      <td>2605</td>\n      <td>5.641712</td>\n    </tr>\n    <tr>\n      <th>2606</th>\n      <td>2606</td>\n      <td>5.684807</td>\n    </tr>\n    <tr>\n      <th>2607</th>\n      <td>2607</td>\n      <td>2.318347</td>\n    </tr>\n    <tr>\n      <th>2608</th>\n      <td>2608</td>\n      <td>4.803714</td>\n    </tr>\n    <tr>\n      <th>2609</th>\n      <td>2609</td>\n      <td>3.690027</td>\n    </tr>\n    <tr>\n      <th>2610</th>\n      <td>2610</td>\n      <td>6.306122</td>\n    </tr>\n    <tr>\n      <th>2611</th>\n      <td>2611</td>\n      <td>8.376749</td>\n    </tr>\n    <tr>\n      <th>2612</th>\n      <td>2612</td>\n      <td>5.043820</td>\n    </tr>\n    <tr>\n      <th>2613</th>\n      <td>2613</td>\n      <td>7.454702</td>\n    </tr>\n    <tr>\n      <th>2614</th>\n      <td>2614</td>\n      <td>6.934077</td>\n    </tr>\n    <tr>\n      <th>2615</th>\n      <td>2615</td>\n      <td>8.050143</td>\n    </tr>\n    <tr>\n      <th>2616</th>\n      <td>2616</td>\n      <td>3.727786</td>\n    </tr>\n    <tr>\n      <th>2617</th>\n      <td>2617</td>\n      <td>8.811284</td>\n    </tr>\n    <tr>\n      <th>2618</th>\n      <td>2618</td>\n      <td>5.872963</td>\n    </tr>\n    <tr>\n      <th>2619</th>\n      <td>2619</td>\n      <td>5.796127</td>\n    </tr>\n    <tr>\n      <th>2620</th>\n      <td>2620</td>\n      <td>6.480726</td>\n    </tr>\n    <tr>\n      <th>2621</th>\n      <td>2621</td>\n      <td>4.451813</td>\n    </tr>\n    <tr>\n      <th>2622</th>\n      <td>2622</td>\n      <td>2.225375</td>\n    </tr>\n    <tr>\n      <th>2623</th>\n      <td>2623</td>\n      <td>8.688369</td>\n    </tr>\n  </tbody>\n</table>\n<p>2624 rows  2 columns</p>\n</div>"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"<h1>Blend LGB and XGB</h1>"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_segs = X_test_scaled_corr_refcv.index\nsubmission = pd.DataFrame(columns=['seg_id', 'time_to_failure'])\nsubmission.seg_id = test_segs\nsubmission['time_to_failure'] = (submission_xgb['time_to_failure'] + submission_lgbm['time_to_failure'])/2\nsubmission.to_csv('submission.csv', index=False, line_terminator='\\n')","execution_count":59,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission","execution_count":60,"outputs":[{"output_type":"execute_result","execution_count":60,"data":{"text/plain":"      seg_id  time_to_failure\n0          0         3.280260\n1          1         4.936298\n2          2         5.596788\n3          3         8.401530\n4          4         7.114454\n5          5         2.811625\n6          6         8.566453\n7          7         4.200832\n8          8         4.541158\n9          9         2.770912\n10        10         2.786150\n11        11         5.835741\n12        12         4.562966\n13        13         2.765412\n14        14         8.961009\n15        15         4.074738\n16        16         5.679411\n17        17         3.729862\n18        18         5.138281\n19        19         5.036072\n20        20         5.561257\n21        21         9.180933\n22        22         3.531542\n23        23         5.599534\n24        24         7.462093\n25        25         4.635519\n26        26         7.549729\n27        27         4.912582\n28        28         2.505918\n29        29         5.468898\n...      ...              ...\n2594    2594         9.484373\n2595    2595         5.610705\n2596    2596         5.807055\n2597    2597         9.229632\n2598    2598         5.950448\n2599    2599         5.736800\n2600    2600         5.829486\n2601    2601         5.738681\n2602    2602         5.350944\n2603    2603         4.960297\n2604    2604         2.773442\n2605    2605         5.885915\n2606    2606         6.450115\n2607    2607         2.385194\n2608    2608         4.900991\n2609    2609         3.809093\n2610    2610         6.218783\n2611    2611         7.571439\n2612    2612         4.802774\n2613    2613         8.527073\n2614    2614         6.875247\n2615    2615         9.488533\n2616    2616         3.597873\n2617    2617         8.577362\n2618    2618         5.794427\n2619    2619         6.359722\n2620    2620         7.374238\n2621    2621         4.110845\n2622    2622         2.183204\n2623    2623         9.344322\n\n[2624 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>seg_id</th>\n      <th>time_to_failure</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>3.280260</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>4.936298</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>5.596788</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>8.401530</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>7.114454</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>5</td>\n      <td>2.811625</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>6</td>\n      <td>8.566453</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>7</td>\n      <td>4.200832</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>8</td>\n      <td>4.541158</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>9</td>\n      <td>2.770912</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>10</td>\n      <td>2.786150</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>11</td>\n      <td>5.835741</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>12</td>\n      <td>4.562966</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>13</td>\n      <td>2.765412</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>14</td>\n      <td>8.961009</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>15</td>\n      <td>4.074738</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>16</td>\n      <td>5.679411</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>17</td>\n      <td>3.729862</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>18</td>\n      <td>5.138281</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>19</td>\n      <td>5.036072</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>20</td>\n      <td>5.561257</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>21</td>\n      <td>9.180933</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>22</td>\n      <td>3.531542</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>23</td>\n      <td>5.599534</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>24</td>\n      <td>7.462093</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>25</td>\n      <td>4.635519</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>26</td>\n      <td>7.549729</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>27</td>\n      <td>4.912582</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>28</td>\n      <td>2.505918</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>29</td>\n      <td>5.468898</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2594</th>\n      <td>2594</td>\n      <td>9.484373</td>\n    </tr>\n    <tr>\n      <th>2595</th>\n      <td>2595</td>\n      <td>5.610705</td>\n    </tr>\n    <tr>\n      <th>2596</th>\n      <td>2596</td>\n      <td>5.807055</td>\n    </tr>\n    <tr>\n      <th>2597</th>\n      <td>2597</td>\n      <td>9.229632</td>\n    </tr>\n    <tr>\n      <th>2598</th>\n      <td>2598</td>\n      <td>5.950448</td>\n    </tr>\n    <tr>\n      <th>2599</th>\n      <td>2599</td>\n      <td>5.736800</td>\n    </tr>\n    <tr>\n      <th>2600</th>\n      <td>2600</td>\n      <td>5.829486</td>\n    </tr>\n    <tr>\n      <th>2601</th>\n      <td>2601</td>\n      <td>5.738681</td>\n    </tr>\n    <tr>\n      <th>2602</th>\n      <td>2602</td>\n      <td>5.350944</td>\n    </tr>\n    <tr>\n      <th>2603</th>\n      <td>2603</td>\n      <td>4.960297</td>\n    </tr>\n    <tr>\n      <th>2604</th>\n      <td>2604</td>\n      <td>2.773442</td>\n    </tr>\n    <tr>\n      <th>2605</th>\n      <td>2605</td>\n      <td>5.885915</td>\n    </tr>\n    <tr>\n      <th>2606</th>\n      <td>2606</td>\n      <td>6.450115</td>\n    </tr>\n    <tr>\n      <th>2607</th>\n      <td>2607</td>\n      <td>2.385194</td>\n    </tr>\n    <tr>\n      <th>2608</th>\n      <td>2608</td>\n      <td>4.900991</td>\n    </tr>\n    <tr>\n      <th>2609</th>\n      <td>2609</td>\n      <td>3.809093</td>\n    </tr>\n    <tr>\n      <th>2610</th>\n      <td>2610</td>\n      <td>6.218783</td>\n    </tr>\n    <tr>\n      <th>2611</th>\n      <td>2611</td>\n      <td>7.571439</td>\n    </tr>\n    <tr>\n      <th>2612</th>\n      <td>2612</td>\n      <td>4.802774</td>\n    </tr>\n    <tr>\n      <th>2613</th>\n      <td>2613</td>\n      <td>8.527073</td>\n    </tr>\n    <tr>\n      <th>2614</th>\n      <td>2614</td>\n      <td>6.875247</td>\n    </tr>\n    <tr>\n      <th>2615</th>\n      <td>2615</td>\n      <td>9.488533</td>\n    </tr>\n    <tr>\n      <th>2616</th>\n      <td>2616</td>\n      <td>3.597873</td>\n    </tr>\n    <tr>\n      <th>2617</th>\n      <td>2617</td>\n      <td>8.577362</td>\n    </tr>\n    <tr>\n      <th>2618</th>\n      <td>2618</td>\n      <td>5.794427</td>\n    </tr>\n    <tr>\n      <th>2619</th>\n      <td>2619</td>\n      <td>6.359722</td>\n    </tr>\n    <tr>\n      <th>2620</th>\n      <td>2620</td>\n      <td>7.374238</td>\n    </tr>\n    <tr>\n      <th>2621</th>\n      <td>2621</td>\n      <td>4.110845</td>\n    </tr>\n    <tr>\n      <th>2622</th>\n      <td>2622</td>\n      <td>2.183204</td>\n    </tr>\n    <tr>\n      <th>2623</th>\n      <td>2623</td>\n      <td>9.344322</td>\n    </tr>\n  </tbody>\n</table>\n<p>2624 rows  2 columns</p>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}