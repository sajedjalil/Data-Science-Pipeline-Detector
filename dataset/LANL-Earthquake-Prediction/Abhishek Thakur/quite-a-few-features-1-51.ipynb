{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom tqdm import tqdm_notebook as tqdm\nfrom joblib import Parallel, delayed\nimport os\nimport gc\nimport xgboost as xgb\nfrom sklearn.model_selection import KFold\nimport scipy as sp\nfrom sklearn import metrics\nfrom tsfresh.feature_extraction import feature_calculators\nprint(os.listdir(\"../input\"))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"class FeatureGenerator(object):\n    def __init__(self, dtype, n_jobs=1, chunk_size=None):\n        self.chunk_size = chunk_size\n        self.dtype = dtype\n        self.filename = None\n        self.n_jobs = n_jobs\n        self.test_files = []\n        if self.dtype == 'train':\n            self.filename = '../input/train.csv'\n            self.total_data = int(629145481 / self.chunk_size)\n        else:\n            submission = pd.read_csv('../input/sample_submission.csv')\n            for seg_id in submission.seg_id.values:\n                self.test_files.append((seg_id, '../input/test/' + seg_id + '.csv'))\n            self.total_data = int(len(submission))\n\n    def read_chunks(self):\n        if self.dtype == 'train':\n            iter_df = pd.read_csv(self.filename, iterator=True, chunksize=self.chunk_size,\n                                  dtype={'acoustic_data': np.float64, 'time_to_failure': np.float64})\n            for counter, df in enumerate(iter_df):\n                x = df.acoustic_data.values\n                y = df.time_to_failure.values[-1]\n                seg_id = 'train_' + str(counter)\n                del df\n                yield seg_id, x, y\n        else:\n            for seg_id, f in self.test_files:\n                df = pd.read_csv(f, dtype={'acoustic_data': np.float64})\n                x = df.acoustic_data.values[-self.chunk_size:]\n                del df\n                yield seg_id, x, -999\n\n    def features(self, x, y, seg_id):\n        feature_dict = dict()\n        feature_dict['target'] = y\n        feature_dict['seg_id'] = seg_id\n\n        # create features here\n        # numpy\n        feature_dict['mean'] = np.mean(x)\n        feature_dict['max'] = np.max(x)\n        feature_dict['min'] = np.min(x)\n        feature_dict['std'] = np.std(x)\n        feature_dict['var'] = np.var(x)\n        feature_dict['ptp'] = np.ptp(x)\n        feature_dict['percentile_10'] = np.percentile(x, 10)\n        feature_dict['percentile_20'] = np.percentile(x, 20)\n        feature_dict['percentile_30'] = np.percentile(x, 30)\n        feature_dict['percentile_40'] = np.percentile(x, 40)\n        feature_dict['percentile_50'] = np.percentile(x, 50)\n        feature_dict['percentile_60'] = np.percentile(x, 60)\n        feature_dict['percentile_70'] = np.percentile(x, 70)\n        feature_dict['percentile_80'] = np.percentile(x, 80)\n        feature_dict['percentile_90'] = np.percentile(x, 90)\n\n        # scipy\n        feature_dict['skew'] = sp.stats.skew(x)\n        feature_dict['kurtosis'] = sp.stats.kurtosis(x)\n        feature_dict['kstat_1'] = sp.stats.kstat(x, 1)\n        feature_dict['kstat_2'] = sp.stats.kstat(x, 2)\n        feature_dict['kstat_3'] = sp.stats.kstat(x, 3)\n        feature_dict['kstat_4'] = sp.stats.kstat(x, 4)\n        feature_dict['moment_1'] = sp.stats.moment(x, 1)\n        feature_dict['moment_2'] = sp.stats.moment(x, 2)\n        feature_dict['moment_3'] = sp.stats.moment(x, 3)\n        feature_dict['moment_4'] = sp.stats.moment(x, 4)\n        \n        feature_dict['abs_energy'] = feature_calculators.abs_energy(x)\n        feature_dict['abs_sum_of_changes'] = feature_calculators.absolute_sum_of_changes(x)\n        feature_dict['count_above_mean'] = feature_calculators.count_above_mean(x)\n        feature_dict['count_below_mean'] = feature_calculators.count_below_mean(x)\n        feature_dict['mean_abs_change'] = feature_calculators.mean_abs_change(x)\n        feature_dict['mean_change'] = feature_calculators.mean_change(x)\n        feature_dict['var_larger_than_std_dev'] = feature_calculators.variance_larger_than_standard_deviation(x)\n        feature_dict['range_minf_m4000'] = feature_calculators.range_count(x, -np.inf, -4000)\n        feature_dict['range_m4000_m3000'] = feature_calculators.range_count(x, -4000, -3000)\n        feature_dict['range_m3000_m2000'] = feature_calculators.range_count(x, -3000, -2000)\n        feature_dict['range_m2000_m1000'] = feature_calculators.range_count(x, -2000, -1000)\n        feature_dict['range_m1000_0'] = feature_calculators.range_count(x, -1000, 0)\n        feature_dict['range_0_p1000'] = feature_calculators.range_count(x, 0, 1000)\n        feature_dict['range_p1000_p2000'] = feature_calculators.range_count(x, 1000, 2000)\n        feature_dict['range_p2000_p3000'] = feature_calculators.range_count(x, 2000, 3000)\n        feature_dict['range_p3000_p4000'] = feature_calculators.range_count(x, 3000, 4000)\n        feature_dict['range_p4000_pinf'] = feature_calculators.range_count(x, 4000, np.inf)\n\n        feature_dict['ratio_unique_values'] = feature_calculators.ratio_value_number_to_time_series_length(x)\n        feature_dict['first_loc_min'] = feature_calculators.first_location_of_minimum(x)\n        feature_dict['first_loc_max'] = feature_calculators.first_location_of_maximum(x)\n        feature_dict['last_loc_min'] = feature_calculators.last_location_of_minimum(x)\n        feature_dict['last_loc_max'] = feature_calculators.last_location_of_maximum(x)\n        feature_dict['time_rev_asym_stat_10'] = feature_calculators.time_reversal_asymmetry_statistic(x, 10)\n        feature_dict['time_rev_asym_stat_100'] = feature_calculators.time_reversal_asymmetry_statistic(x, 100)\n        feature_dict['time_rev_asym_stat_1000'] = feature_calculators.time_reversal_asymmetry_statistic(x, 1000)\n        feature_dict['autocorrelation_5'] = feature_calculators.autocorrelation(x, 5)\n        feature_dict['autocorrelation_10'] = feature_calculators.autocorrelation(x, 10)\n        feature_dict['autocorrelation_50'] = feature_calculators.autocorrelation(x, 50)\n        feature_dict['autocorrelation_100'] = feature_calculators.autocorrelation(x, 100)\n        feature_dict['autocorrelation_1000'] = feature_calculators.autocorrelation(x, 1000)\n        feature_dict['c3_5'] = feature_calculators.c3(x, 5)\n        feature_dict['c3_10'] = feature_calculators.c3(x, 10)\n        feature_dict['c3_100'] = feature_calculators.c3(x, 100)\n        feature_dict['fft_1_real'] = list(feature_calculators.fft_coefficient(x, [{'coeff': 1, 'attr': 'real'}]))[0][1]\n        feature_dict['fft_1_imag'] = list(feature_calculators.fft_coefficient(x, [{'coeff': 1, 'attr': 'imag'}]))[0][1]\n        feature_dict['fft_1_ang'] = list(feature_calculators.fft_coefficient(x, [{'coeff': 1, 'attr': 'angle'}]))[0][1]\n        feature_dict['fft_2_real'] = list(feature_calculators.fft_coefficient(x, [{'coeff': 2, 'attr': 'real'}]))[0][1]\n        feature_dict['fft_2_imag'] = list(feature_calculators.fft_coefficient(x, [{'coeff': 2, 'attr': 'imag'}]))[0][1]\n        feature_dict['fft_2_ang'] = list(feature_calculators.fft_coefficient(x, [{'coeff': 2, 'attr': 'angle'}]))[0][1]\n        feature_dict['fft_3_real'] = list(feature_calculators.fft_coefficient(x, [{'coeff': 3, 'attr': 'real'}]))[0][1]\n        feature_dict['fft_3_imag'] = list(feature_calculators.fft_coefficient(x, [{'coeff': 3, 'attr': 'imag'}]))[0][1]\n        feature_dict['fft_3_ang'] = list(feature_calculators.fft_coefficient(x, [{'coeff': 3, 'attr': 'angle'}]))[0][1]\n        feature_dict['long_strk_above_mean'] = feature_calculators.longest_strike_above_mean(x)\n        feature_dict['long_strk_below_mean'] = feature_calculators.longest_strike_below_mean(x)\n        feature_dict['cid_ce_0'] = feature_calculators.cid_ce(x, 0)\n        feature_dict['cid_ce_1'] = feature_calculators.cid_ce(x, 1)\n        feature_dict['binned_entropy_5'] = feature_calculators.binned_entropy(x, 5)\n        feature_dict['binned_entropy_10'] = feature_calculators.binned_entropy(x, 10)\n        feature_dict['binned_entropy_20'] = feature_calculators.binned_entropy(x, 20)\n        feature_dict['binned_entropy_50'] = feature_calculators.binned_entropy(x, 50)\n        feature_dict['binned_entropy_80'] = feature_calculators.binned_entropy(x, 80)\n        feature_dict['binned_entropy_100'] = feature_calculators.binned_entropy(x, 100)\n\n        feature_dict['num_crossing_0'] = feature_calculators.number_crossing_m(x, 0)\n        feature_dict['num_peaks_10'] = feature_calculators.number_peaks(x, 10)\n        feature_dict['num_peaks_50'] = feature_calculators.number_peaks(x, 50)\n        feature_dict['num_peaks_100'] = feature_calculators.number_peaks(x, 100)\n        feature_dict['num_peaks_500'] = feature_calculators.number_peaks(x, 500)\n\n        feature_dict['spkt_welch_density_1'] = list(feature_calculators.spkt_welch_density(x, [{'coeff': 1}]))[0][1]\n        feature_dict['spkt_welch_density_10'] = list(feature_calculators.spkt_welch_density(x, [{'coeff': 10}]))[0][1]\n        feature_dict['spkt_welch_density_50'] = list(feature_calculators.spkt_welch_density(x, [{'coeff': 50}]))[0][1]\n        feature_dict['spkt_welch_density_100'] = list(feature_calculators.spkt_welch_density(x, [{'coeff': 100}]))[0][1]\n\n        feature_dict['time_rev_asym_stat_1'] = feature_calculators.time_reversal_asymmetry_statistic(x, 1)\n        feature_dict['time_rev_asym_stat_10'] = feature_calculators.time_reversal_asymmetry_statistic(x, 10)\n        feature_dict['time_rev_asym_stat_100'] = feature_calculators.time_reversal_asymmetry_statistic(x, 100)        \n\n        return feature_dict\n\n    def generate(self):\n        feature_list = []\n        res = Parallel(n_jobs=self.n_jobs,\n                       backend='threading')(delayed(self.features)(x, y, s)\n                                            for s, x, y in tqdm(self.read_chunks(), total=self.total_data))\n        for r in res:\n            feature_list.append(r)\n        return pd.DataFrame(feature_list)\n\n\ntraining_fg = FeatureGenerator(dtype='train', n_jobs=10, chunk_size=150000)\ntraining_data = training_fg.generate()\n\ntest_fg = FeatureGenerator(dtype='test', n_jobs=10, chunk_size=150000)\ntest_data = test_fg.generate()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = training_data.drop(['target', 'seg_id'], axis=1)\nX_test = test_data.drop(['target', 'seg_id'], axis=1)\ntest_segs = test_data.seg_id\ny = training_data.target","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"folds = KFold(n_splits=5, shuffle=True, random_state=42)\noof_preds = np.zeros((len(X), 1))\ntest_preds = np.zeros((len(X_test), 1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"params = {\n    \"learning_rate\": 0.01,\n    \"max_depth\": 4,\n    \"n_estimators\": 10000,\n    \"min_child_weight\": 1,\n    \"colsample_bytree\": 0.9,\n    \"subsample\": 1.0,\n    \"nthread\": 12,\n    \"random_state\": 42,\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for fold_, (trn_, val_) in enumerate(folds.split(X)):\n    print(\"Current Fold: {}\".format(fold_))\n    trn_x, trn_y = X.iloc[trn_], y.iloc[trn_]\n    val_x, val_y = X.iloc[val_], y.iloc[val_]\n\n    clf = xgb.XGBRegressor(**params)\n    clf.fit(\n        trn_x, trn_y,\n        eval_set=[(trn_x, trn_y), (val_x, val_y)],\n        eval_metric='mae',\n        verbose=150,\n        early_stopping_rounds=100\n    )\n    val_pred = clf.predict(val_x, ntree_limit=clf.best_ntree_limit)\n    test_fold_pred = clf.predict(X_test, ntree_limit=clf.best_ntree_limit)\n    print(\"MAE = {}\".format(metrics.mean_absolute_error(val_y, val_pred)))\n    oof_preds[val_, :] = val_pred.reshape((-1, 1))\n    test_preds += test_fold_pred.reshape((-1, 1))\ntest_preds /= 5\n\noof_score = metrics.mean_absolute_error(y, oof_preds)\nprint(\"Mean MAE = {}\".format(oof_score))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.DataFrame(columns=['seg_id', 'time_to_failure'])\nsubmission.seg_id = test_segs\nsubmission.time_to_failure = test_preds\nsubmission.to_csv('submission.csv', index=False)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}