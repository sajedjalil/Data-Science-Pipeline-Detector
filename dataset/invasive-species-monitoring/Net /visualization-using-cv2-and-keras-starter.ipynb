{"nbformat_minor":0,"cells":[{"outputs":[],"metadata":{"_execution_state":"idle","_cell_guid":"1c2f5c85-ac7c-4cd5-91fe-02da863dc340","_uuid":"47400f9371409791c9721ec87b99695d1952324c","collapsed":false},"execution_count":null,"source":"### This notebook is the first try of the invasive classification\nThe first section is the visualization of image and the second part is the model part using Keras.\n\n\n- Make some edit due to run time in Kaggle Kernels ","cell_type":"markdown"},{"outputs":[],"metadata":{"_execution_state":"idle","trusted":false,"_cell_guid":"1afd7af0-16e2-4693-95ff-f8cbce3b53a8","_uuid":"7926d1769245905c3c79b90315af37e8007b170b"},"execution_count":null,"source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom glob import glob\nimport cv2\n\nfrom scipy.misc import imread\nimport os\nimport datetime, time\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nfrom subprocess import check_output","cell_type":"code"},{"outputs":[],"metadata":{"_execution_state":"idle","_cell_guid":"a35cb805-3efe-4dfd-94e6-b29ae01e0c3c","_uuid":"b140736b59c0e9a3132d3918ce32a448bac21775","collapsed":false},"execution_count":null,"source":"## Section 1: Image Visualization\n\n### How many files are we dealing with?\nLet's see the number of both training and testing images for us to make a model and prediction.","cell_type":"markdown"},{"outputs":[],"metadata":{"_execution_state":"idle","trusted":false,"_cell_guid":"ab7ca358-0393-449b-af37-6f4ff520d913","_uuid":"5a4f4c2f93e7dea717594a1d4ccd148301d2aeda","collapsed":false},"execution_count":null,"source":"# Examine the total pictures\ndef get_number_of_file(my_dir):\n    return str(len(os.listdir(my_dir)))\n\nprint(\"# of training files: {}\".format(get_number_of_file(\"../input/train\")))\nprint(\"# of testing files: {}\".format(get_number_of_file(\"../input/test\")))","cell_type":"code"},{"outputs":[],"metadata":{"_execution_state":"idle","_cell_guid":"9618b238-4d55-4f7b-bf80-e2c38b6328d6","_uuid":"97a586824031a241af4a5db9301f05417206eec8","collapsed":false},"execution_count":null,"source":"### The proportion of invasive labels in training datasets","cell_type":"markdown"},{"outputs":[],"metadata":{"_execution_state":"idle","trusted":false,"_cell_guid":"42271d7d-67bd-43bf-bdb4-5e74f24c5295","_uuid":"34b83942831ea120ebb376e553d57998fd49a973","collapsed":false},"execution_count":null,"source":"train_labels = pd.read_csv(\"../input/train_labels.csv\")\ntrain_labels.groupby(['invasive']).size().reset_index(name='counts')","cell_type":"code"},{"outputs":[],"metadata":{"_execution_state":"idle","_cell_guid":"289387b4-0765-4ca1-ad21-ab109e9b31e7","_uuid":"3fcbc5fa3af70bce9d8b2edd35b35e4f7fabd8a4","collapsed":false},"execution_count":null,"source":"### Let's visualize some images\nIn the below section, I will show some images on the training images, as well provided some filters using ***cv2***.","cell_type":"markdown"},{"outputs":[],"metadata":{"_execution_state":"idle","trusted":false,"_cell_guid":"11bbe2a2-9d88-4db4-8fe0-3303545d2eeb","_uuid":"a65a3577b682d6d895117e437b47648e6eb4adf1","collapsed":false},"execution_count":null,"source":"def smpl_visual(path, smpl, dim_y):\n    \n    smpl_pic = glob(smpl)\n    fig = plt.figure(figsize=(20, 14))\n    \n    for i in range(len(smpl_pic)):\n        ax = fig.add_subplot(round(len(smpl_pic)/dim_y), dim_y, i+1)\n        plt.title(\"{}: Height {} Width {} Dim {}\".format(smpl_pic[i].strip(path),\n                                                         plt.imread(smpl_pic[i]).shape[0],\n                                                         plt.imread(smpl_pic[i]).shape[1],\n                                                         plt.imread(smpl_pic[i]).shape[2]\n                                                        )\n                 )\n        plt.imshow(plt.imread(smpl_pic[i]))\n        \n    return smpl_pic\n\nsmpl_pic = smpl_visual('..input/train\\\\', '../input/train/112*.jpg', 4)","cell_type":"code"},{"outputs":[],"metadata":{"_execution_state":"idle","trusted":false,"_cell_guid":"482fa1d1-d82a-46e2-8c37-2230315a2081","_uuid":"6da809b5cc84ba0199fedeea6d856fb1e0b54f51","collapsed":false},"execution_count":null,"source":"def visual_with_transformation (pic):\n\n    for idx in list(range(0, len(pic), 1)):\n        ori_smpl = cv2.imread(pic[idx])\n        smpl_1_rgb = cv2.cvtColor(cv2.imread(pic[idx]), cv2.COLOR_BGR2RGB)\n        smpl_1_lab = cv2.cvtColor(cv2.imread(pic[idx]), cv2.COLOR_BGR2LAB)\n        smpl_1_gray =  cv2.cvtColor(cv2.imread(pic[idx]), cv2.COLOR_BGR2GRAY) \n\n        f, ax = plt.subplots(1, 4,figsize=(30,20))\n        (ax1, ax2, ax3, ax4) = ax.flatten()\n        train_idx = int(pic[idx].strip(\"../input/train\\\\\").strip(\".jpg\"))\n        print(\"The Image name: {} Is Invasive?: {}\".format(pic[idx].strip(\"train\\\\\"), \n                                                           train_labels.loc[train_labels.name.values == train_idx].invasive.values)\n             )\n        ax1.set_title(\"Original - BGR\")\n        ax1.imshow(ori_smpl)\n        ax2.set_title(\"Transformed - RGB\")\n        ax2.imshow(smpl_1_rgb)\n        ax3.set_title(\"Transformed - LAB\")\n        ax3.imshow(smpl_1_lab)\n        ax4.set_title(\"Transformed - GRAY\")\n        ax4.imshow(smpl_1_gray)\n        plt.show()\n\nvisual_with_transformation(smpl_pic)","cell_type":"code"},{"outputs":[],"metadata":{"_execution_state":"idle","_cell_guid":"0f5f9bde-9d0b-41da-bd39-712ef54968f8","_uuid":"bea84027299095bff1b7f0133e86c0befd6163ed","collapsed":false},"execution_count":null,"source":"## Section 2: Let's do some simple model","cell_type":"markdown"},{"outputs":[],"metadata":{"_execution_state":"idle","trusted":false,"_cell_guid":"0185e982-26e7-453b-bf94-9af2000d1f49","_uuid":"0761b81d79eac5131d894c30c95f741991a60002","collapsed":false},"execution_count":null,"source":"import keras\nfrom keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\nfrom keras.models import Sequential\nfrom keras.layers import Convolution2D, MaxPooling2D, ZeroPadding2D\nfrom keras.layers import Flatten, Dense, Dropout\nfrom keras.optimizers import SGD\nfrom skimage import io, transform","cell_type":"code"},{"outputs":[],"metadata":{"_execution_state":"idle","trusted":false,"_cell_guid":"fb63e763-a6e6-4b92-bfcd-1b18230f39f7","_uuid":"b42826dddcdb37e76a7dee6e24eb33b634a3b1cb","collapsed":false},"execution_count":null,"source":"# Initialize values -\n\nx_train = np.empty(shape=(100, 150, 150, 3))\ny_train = np.array(train_labels.invasive.values[0:100])\nx_val = np.empty(shape=(100, 150, 150, 3))\ny_val = np.array(train_labels.invasive.values[100:200])\n\nfor i in range(100):\n    tr_img = cv2.imread(\"../input/train/\" + str(i+1) + '.jpg')\n    x_train[i] = transform.resize(tr_img, output_shape=(150, 150, 3), mode='constant')\n\n    \nfor i in range(100):\n    val_img = cv2.imread(\"../input/train/\" + str(i+1001) + '.jpg')\n    x_val[i] = transform.resize(val_img, output_shape=(150, 150, 3), mode='constant')","cell_type":"code"},{"outputs":[],"metadata":{"_execution_state":"idle","trusted":false,"_cell_guid":"bc8a3646-c897-45a1-b448-f78acc6f39ee","_uuid":"84b8f0c4582b22b5fa8daef0b3202a49ecf4a24b","collapsed":false},"execution_count":null,"source":"# Start some model\nmodel = Sequential()\n\nmodel.add(ZeroPadding2D((1, 1), input_shape=(150, 150, 3)))\n\nmodel.add(Convolution2D(64, (3, 3), activation='relu'))\nmodel.add(ZeroPadding2D((1, 1)))\nmodel.add(Convolution2D(64, (3, 3), activation='relu'))\nmodel.add(MaxPooling2D((2, 2), strides=(2, 2)))\n\nmodel.add(ZeroPadding2D((1, 1)))\nmodel.add(Convolution2D(128, (3, 3), activation='relu'))\nmodel.add(ZeroPadding2D((1, 1)))\nmodel.add(Convolution2D(128, (3, 3), activation='relu'))\nmodel.add(MaxPooling2D((2, 2), strides=(2, 2)))\n\nmodel.add(ZeroPadding2D((1, 1)))\nmodel.add(Convolution2D(256, (3, 3), activation='relu'))\nmodel.add(ZeroPadding2D((1, 1)))\nmodel.add(Convolution2D(256, (3, 3), activation='relu'))\nmodel.add(ZeroPadding2D((1, 1)))\nmodel.add(Convolution2D(256, (3, 3), activation='relu'))\nmodel.add(MaxPooling2D((2, 2), strides=(2, 2)))\n\nmodel.add(Flatten()) # maps back to 1D feature\nmodel.add(Dense(256, activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(1, activation='sigmoid'))\n\nmodel.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])\n\nprint(model.summary())","cell_type":"code"},{"outputs":[],"metadata":{"_execution_state":"idle","trusted":false,"_cell_guid":"f9c4ac0b-1e8f-4dc3-b18b-253c2a55da14","_uuid":"0e7f4852945713588f639302554f506ca00dbf89","collapsed":false},"execution_count":null,"source":"model.fit(x_train, y_train, epochs=3, batch_size=10)","cell_type":"code"},{"outputs":[],"metadata":{"_execution_state":"idle","trusted":false,"_cell_guid":"e0208dfc-42af-4d20-8be1-1c44da10eea2","_uuid":"71a3450f8cd6ec146b28f63ac626a4481feea8e1","collapsed":false},"execution_count":null,"source":"acc = model.evaluate(x_val, y_val)[1]\nprint('Evaluation accuracy:{0}'.format(round(acc, 4)))","cell_type":"code"}],"metadata":{"language_info":{"name":"python","version":"3.6.1","codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","nbconvert_exporter":"python","mimetype":"text/x-python","pygments_lexer":"ipython3"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"}},"nbformat":4}