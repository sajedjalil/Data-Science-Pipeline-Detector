{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","name":"python","file_extension":".py","mimetype":"text/x-python","version":"3.6.1","nbconvert_exporter":"python","codemirror_mode":{"name":"ipython","version":3}}},"cells":[{"metadata":{"_execution_state":"idle","trusted":false,"_uuid":"cfe4a4c158f14f88b5137d8f6d0130f07151d1ea","_cell_guid":"c5256bee-a2a2-44f1-bef0-ef4a8feb6b4c"},"outputs":[],"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom PIL import Image\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":1},{"metadata":{"_execution_state":"idle","trusted":false,"_uuid":"dea67b31c29caafbd6cbc5d16e372f03d26f5d1e","collapsed":false,"_cell_guid":"6189cb19-66d4-419a-8942-e08f7dfe1755"},"outputs":[],"cell_type":"code","source":"im_height = 144\nim_width = 192\n\nim_height = 300\nim_width = 360","execution_count":2},{"metadata":{"_execution_state":"idle","trusted":false,"_uuid":"981ffc82c9ebed7fedd7f8cd5cabc61d41349d33","collapsed":false,"_cell_guid":"521d45fb-7e21-475a-969e-be32c8e0c821"},"outputs":[],"cell_type":"code","source":"train_labels = np.array(pd.read_csv(\"../input/train_labels.csv\"))\n\ntrain_folder = \"../input/train\"\ntrain_list = check_output([\"ls\", train_folder]).decode(\"utf8\").split()\ntrain_data = np.zeros(shape=(len(train_list), im_height,im_width,3), dtype=np.float32) #np.uint8\n\nfor jpg in train_list:\n    im_num = int(jpg.split('.')[0]) - 1\n    im = Image.open(train_folder+\"/\"+jpg).resize((im_width,im_height))#,Image.ANTIALIAS)\n    train_data[im_num] = np.array(im)/255","execution_count":3},{"metadata":{"_execution_state":"idle","trusted":false,"_uuid":"9ea9c572d1453957ef94000a7d9a686e85c7a18f","collapsed":false,"_cell_guid":"d584b9ad-8081-4ec3-a55a-f9ae4a7f78d5"},"execution_count":null,"cell_type":"code","source":"print(train_data[1])","outputs":[]},{"metadata":{"_execution_state":"idle","trusted":false,"_uuid":"92b9ed017bbf9885cd100513bb1483915b82f1c3","collapsed":false,"_cell_guid":"4718a599-7695-4e5c-ab1d-90446329c981"},"outputs":[],"cell_type":"code","source":"#test_folder = \"../input/test\"\n#test_list = check_output([\"ls\", test_folder]).decode(\"utf8\").split()\n#test_data = np.zeros(shape=(len(test_list), im_height,im_width,3), dtype=np.uint8)\n\n#for jpg in test_list:\n#    im_num = int(jpg.split('.')[0]) - 1\n#    im = Image.open(test_folder+\"/\"+jpg).resize((im_width,im_height),Image.ANTIALIAS)\n#    test_data[im_num] = np.array(im)","execution_count":null},{"metadata":{"_execution_state":"idle","trusted":false,"_uuid":"6ff46ac41c948bf6a2cd06a6845fc323a890851a","collapsed":false,"_cell_guid":"4fa9b278-27eb-43c0-a496-31aabd1d0e24"},"outputs":[],"cell_type":"code","source":"np.random.seed(42)\nshuffle = np.random.permutation(np.arange(train_data.shape[0]))\ntrain_data, train_labels = train_data[shuffle], train_labels[shuffle]\n\ntrain_data_sub, train_labels_sub = train_data[:1800], train_labels[:1800]\ndev_data, dev_labels = train_data[1800:], train_labels[1800:]","execution_count":5},{"metadata":{"_execution_state":"idle","trusted":false,"_uuid":"49ec1d1a526a78ae8aec1226e39be22ad73ee16a","collapsed":false,"_cell_guid":"bfd97542-ace9-486a-ab3f-b03ac0f795b8"},"outputs":[],"cell_type":"code","source":"#print(train_labels_sub[:,1])","execution_count":null},{"metadata":{"_execution_state":"idle","trusted":false,"_uuid":"bb10b7ff217b79e718c0ceb9b21f79ffd1a03125","collapsed":false,"_cell_guid":"f3049c35-3e31-4d38-bf65-9f463b0404b3"},"outputs":[],"cell_type":"code","source":"#from keras.models import Sequential, Model, load_model\n#from keras import applications\n#from keras import optimizers\n#from keras.layers import Dropout, Flatten, Dense\n\n#img_rows, img_cols, img_channel = 192, 144, 3\n\n#base_model = applications.VGG16(weights='imagenet', include_top=False, input_shape=(img_rows, img_cols, img_channel))","execution_count":null},{"metadata":{"_execution_state":"idle","trusted":false,"_uuid":"988ced072ae559e4df8824b429c9dae5527d7444","collapsed":false,"_cell_guid":"feac6d6c-9c8b-4105-868c-e5fedbcbd7ad"},"outputs":[],"cell_type":"code","source":"from keras import applications, losses, optimizers\nfrom keras.models import Sequential, Model, load_model\nfrom keras.layers import Dropout, Flatten, Dense, Activation\nfrom keras.utils.np_utils import to_categorical\n\nmodel = Sequential()\nmodel.add(Flatten(input_shape=(im_height,im_width,3)))\n\nmodel.add(Dense(units=2))\nmodel.add(Activation('softmax'))\nmodel.compile(loss=losses.categorical_crossentropy,\n              optimizer=optimizers.SGD(lr=0.01, momentum=0.9, nesterov=True),\n              metrics=['accuracy']\n             )\n\nmodel.fit(train_data_sub, to_categorical(train_labels_sub[:,1]), epochs=2, batch_size=600, verbose=1)\n\n#model.train_on_batch(train_data_sub, train_labels_sub)\n\n#loss_and_metrics = model.evaluate(dev_data, to_categorical(dev_labels[:,1]), batch_size=360)\n\n#classes = model.predict(test_data, batch_size=360)","execution_count":6},{"metadata":{"_execution_state":"idle","_uuid":"0555614863d74b69ec9ee322dd457c0070a364fb","collapsed":false},"execution_count":null,"cell_type":"code","source":"import keras.layers as kl","outputs":[]},{"metadata":{"_execution_state":"idle","_uuid":"921b4af3f1443d3ae931a72225922320b943454c","collapsed":false},"execution_count":null,"cell_type":"code","source":"print(train_data_sub.shape)","outputs":[]},{"metadata":{"_execution_state":"idle","_uuid":"e45e798dca29c6a90633bccc08925d10bc024b8c","collapsed":false},"execution_count":null,"cell_type":"code","source":"model = Sequential()\n#model.add(kl.Flatten(input_shape=(im_height,im_width,3)))\nmodel.add(kl.Conv2D(filters=64, kernel_size=3,strides=1, padding='same',activation='relu',input_shape=(im_height,im_width,3)))\nmodel.add(kl.Conv2D(filters=32, kernel_size=3,strides=1, padding='same',activation='relu'))#,input_shape=(im_height,im_width,3))\nmodel.add(kl.MaxPooling2D(pool_size=(2, 2), padding='valid'))          \n#model.add(kl.Reshape(32 * 150 * 180))\nmodel.add(kl.Dropout(0.25))\nmodel.add(kl.Dense(32))\nmodel.add(kl.Dropout(0.25))\nmodel.add(kl.Dense(2))\nmodel.add(kl.Activation('softmax'))\n\nmodel.compile(loss=losses.categorical_crossentropy,\n              optimizer=optimizers.Adam(),\n              metrics=['accuracy']\n             )\n\nmodel.fit(x=train_data_sub, y=to_categorical(train_labels_sub[:,1]), epochs=25, batch_size=50, verbose=1)","outputs":[]},{"metadata":{"_execution_state":"idle","_uuid":"cfeaf82405856f5737beaef5c83bf1ea712612d9","collapsed":false},"execution_count":null,"cell_type":"code","source":"import tensorflow as tf\n\nfrom tensorflow.python.framework import random_seed\n\ndef reset_graph(seed=42):\n    tf.reset_default_graph()\n    tf.set_random_seed(seed)\n    np.random.seed(seed)","outputs":[]},{"metadata":{"_execution_state":"idle","_uuid":"d060c0b31ccd5f991bcb3083fea383abc15f043f","collapsed":false},"execution_count":null,"cell_type":"code","source":"height = 866\nwidth = 1154\nchannels = 3\nn_inputs = height * width\n\nconv1_fmaps = 64\nconv1_ksize = 3\nconv1_stride = 1\nconv1_pad = \"SAME\"\n\nconv2_fmaps = 32\nconv2_ksize = 3\nconv2_stride = 1\nconv2_pad = \"SAME\"\nconv2_dropout_rate = 0.25\n\n\npool3_fmaps = conv2_fmaps\n\nn_fc1 = 32\nfc1_dropout_rate = 0.25\n\nn_outputs = 2\n\nreset_graph()\n\nwith tf.name_scope(\"inputs\"):\n    X = tf.placeholder(tf.float32, shape=[None, height, width, channels], name=\"X\")\n    X_resized = tf.image.resize_images(X, [300, 360])\n\n    y = tf.placeholder(tf.int32, shape=[None], name=\"y\")\n    training = tf.placeholder_with_default(False, shape=[], name='training')\n\n\nconv1 = tf.layers.conv2d(X_resized, filters=conv1_fmaps, kernel_size=conv1_ksize,\n                         strides=conv1_stride, padding=conv1_pad,\n                         activation=tf.nn.relu, name=\"conv1\")\nconv2 = tf.layers.conv2d(conv1, filters=conv2_fmaps, kernel_size=conv2_ksize,\n                         strides=conv2_stride, padding=conv2_pad,\n                         activation=tf.nn.relu, name=\"conv2\")\n\nwith tf.name_scope(\"pool3\"):\n    pool3 = tf.nn.max_pool(conv2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=\"VALID\")\n    pool3_flat = tf.reshape(pool3, shape=[-1, pool3_fmaps * 150 * 180])\n    pool3_flat_drop = tf.layers.dropout(pool3_flat, conv2_dropout_rate, training=training)\nwith tf.name_scope(\"fc1\"):\n    fc1 = tf.layers.dense(pool3_flat, n_fc1, activation=tf.nn.relu, name=\"fc1\")\n    fc1_drop = tf.layers.dropout(fc1, fc1_dropout_rate, training=training)\n\nwith tf.name_scope(\"output\"):\n    logits = tf.layers.dense(fc1, n_outputs, name=\"output\")\n    Y_proba = tf.nn.softmax(logits, name=\"Y_proba\")\n\nwith tf.name_scope(\"train\"):\n    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=y)\n    loss = tf.reduce_mean(xentropy)\n    optimizer = tf.train.AdamOptimizer()\n    training_op = optimizer.minimize(loss)\n\nwith tf.name_scope(\"eval\"):\n    correct = tf.nn.in_top_k(logits, y, 1)\n    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n\nwith tf.name_scope(\"init_and_save\"):\n    init = tf.global_variables_initializer()\n    saver = tf.train.Saver()","outputs":[]},{"metadata":{"_execution_state":"idle","_uuid":"a2203d6cf8fbfc04d085931c4b5a5983f6e36a60","collapsed":false},"execution_count":null,"cell_type":"code","source":"y_test = train_labels[2200:]\ntest_data = DataSet(np.arange(2200,len(train_labels)), y_test)\ntrain_data =  DataSet(np.arange(0,2200), train_labels[:2200])\nX_val, y_val = test_data.next_batch(70)\n\n\n\n\nn_epochs = 25\nbatch_size = 50\n\nbest_loss_val = np.infty\ncheck_interval = 5\nchecks_since_last_progress = 0\nmax_checks_without_progress = 100\nbest_model_params = None \n\nwith tf.Session() as sess:\n    init.run()\n    for epoch in range(n_epochs):\n        for iteration in range(train_data.num_examples // batch_size):\n            X_batch, y_batch = train_data.next_batch(batch_size)\n            sess.run(training_op, feed_dict={X: X_batch, y: y_batch, training: True})\n            if iteration % check_interval == 0:\n                loss_val = loss.eval(feed_dict={X: X_val,\n                                                y: y_val})\n                if loss_val < best_loss_val:\n                    best_loss_val = loss_val\n                    checks_since_last_progress = 0\n                    best_model_params = get_model_params()\n                else:\n                    checks_since_last_progress += 1\n        acc_train = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n        acc_test = accuracy.eval(feed_dict={X: X_val, y: y_val})\n        loss_val = loss.eval(feed_dict={X: X_val,\n                                                y: y_val})\n        print(\"Epoch {}, train accuracy: {:.4f}%, test accuracy: {:.4f}%, loss: {:.6f}, best loss: {:.6f}\".format(\n                  epoch, acc_train * 100, acc_test * 100, loss_val, best_loss_val))\n        if checks_since_last_progress > max_checks_without_progress:\n            print(\"Early stopping!\")\n            break\n\n    if best_model_params:\n        restore_model_params(best_model_params)\n\n    save_path = saver.save(sess, \"./my_isd_model2\")","outputs":[]},{"metadata":{"_execution_state":"idle","trusted":false,"_uuid":"f7d474f6101b080441562fc64c2a1c0f03340223","collapsed":false,"_cell_guid":"c2d09256-5d7a-438c-aa0b-8b2dd6054872"},"execution_count":null,"cell_type":"code","source":"from keras.layers import convolutional\n\nheight = 866\nwidth = 1154\nchannels = 3\nn_inputs = height * width\n\nconv1_fmaps = 64\nconv1_ksize = 3\nconv1_stride = 1\nconv1_pad = \"SAME\"\n\nconv2_fmaps = 32\nconv2_ksize = 3\nconv2_stride = 1\nconv2_pad = \"SAME\"\nconv2_dropout_rate = 0.25\n\npool3_fmaps = conv2_fmaps\n\nn_fc1 = 32\nfc1_dropout_rate = 0.25\n\nn_outputs = 2\n\nconv1 = convolutional.Conv2d(train_data, filters=conv1_fmaps, kernel_size=conv1_ksize,\n                         strides=conv1_stride, padding=conv1_pad,\n                         activation=Activation(\"relu\"), name=\"conv1\")\nconv2 = convolutional.Conv2d(conv1, filters=conv2_fmaps, kernel_size=conv2_ksize,\n                         strides=conv2_stride, padding=conv2_pad,\n                         activation=Activation(\"relu\"), name=\"conv2\")\n\nconvolutional.Conv2D(filters, kernel_size, strides=(1, 1), padding='valid', \n                     data_format=None, dilation_rate=(1, 1), activation=None, \n                     use_bias=True, kernel_initializer='glorot_uniform', \n                     bias_initializer='zeros', kernel_regularizer=None, \n                     bias_regularizer=None, activity_regularizer=None, \n                     kernel_constraint=None, bias_constraint=None)","outputs":[]}],"nbformat_minor":0,"nbformat":4}