{"nbformat_minor":1,"cells":[{"metadata":{"_uuid":"172930148f95aa6a34f519f9433496e93942e3d8","_cell_guid":"ead9bafa-386f-4edd-89d9-b8e910234e7a"},"cell_type":"markdown","source":"## Introduction\nHere will try to cover some standard technics like tokenization, stemming, lemmatization and topic modeling with LDA.\n\n**Acknowledgments.**  This notebook was inspired by kernels [Spooky NLP and Topic Modelling tutorial](https://www.kaggle.com/arthurtok/spooky-nlp-and-topic-modelling-tutorial) (Anisotropic) and [NIPS papers visualized with NMF and t-SNE](https://www.kaggle.com/dschniertshauer/nips-papers-visualized-with-nmf-and-t-sne) (Lurchi)."},{"execution_count":null,"metadata":{"_uuid":"9cf220e3db0c1be4e176b95898eb12b076c614d6","_cell_guid":"da26699d-f4b2-4397-b2b3-469bdd8c1b08"},"outputs":[],"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n# LDA, tSNE\nfrom sklearn.manifold import TSNE\nfrom gensim.models.ldamodel import LdaModel\nfrom sklearn.metrics.pairwise import pairwise_distances\n# NLTK\nfrom nltk.tokenize import RegexpTokenizer\nfrom nltk.stem.wordnet import WordNetLemmatizer\nfrom nltk import pos_tag\nfrom nltk.corpus import stopwords\nimport re\n# Bokeh\nfrom bokeh.io import output_notebook\nfrom bokeh.plotting import figure, show\nfrom bokeh.models import HoverTool, CustomJS, ColumnDataSource, Slider\nfrom bokeh.layouts import column\nfrom bokeh.palettes import all_palettes\noutput_notebook()"},{"metadata":{"_uuid":"ef6bd53106de3cd33ee6f44bb8a26fa8fa5613f0","_cell_guid":"c6acd75d-9dee-4250-a688-d2ed3fe739ca"},"cell_type":"markdown","source":"## 1. Loading data\nLet's **load the dataset** with papers and glimpse some first rows of a paper."},{"execution_count":null,"metadata":{"_uuid":"9e94cb38f1a78f20311f48349d53a24271720a7c","_cell_guid":"4340b982-e796-4972-a269-72d517cadc07"},"outputs":[],"cell_type":"code","source":"an_author = 1211\ndf = pd.read_csv(\"../input/train.csv\")\ndf_test = pd.read_csv(\"../input/test.csv\")\nprint(\"Author: {}, ID: {}\".format(df.author[an_author], df.id[an_author]))\nprint(df.text[an_author][:500])"},{"metadata":{"_uuid":"bfae5a2fc4421f3dedf9c2b35ea30bfc3b1c8724","_cell_guid":"993858df-b484-4e60-a7f3-fd68eaec0b65"},"cell_type":"markdown","source":"## 2. Processing\nHere we'll process our corpus using some standard technics ...\n### 2.1. Initial cleaning\nJust **removing numbers** (if exist) and **reducing** all words **to the lowercase**. Let also see what we'll get:\n"},{"execution_count":null,"metadata":{"_uuid":"73c29802e092232a0c4d803f6b017bb739ab6855","_cell_guid":"d176d426-163c-4e72-8c90-1ce148d929b6"},"outputs":[],"cell_type":"code","source":"# Removing numerals:\ndf['text_tokens'] = df.text.map(lambda x: re.sub(r'\\d+', '', x))\n# Lower case:\ndf['text_tokens'] = df.text_tokens.map(lambda x: x.lower())\nprint(\"Author: {}, ID: {}\".format(df.author[an_author], df.id[an_author]))\nprint(df['text_tokens'][an_author][:500])"},{"metadata":{"_uuid":"4d2b9a4d06d641eac347b2a87b0e33d722f54485","_cell_guid":"ab1dab0f-e0a5-4fd4-bec1-be53ef077280"},"cell_type":"markdown","source":"### 2.2. Tokenize\n**Spliting** texts **into separete words**, also **removing punctuanions** and other stuff. After that procedure we should obtain texts as lists of words in lowercase:"},{"execution_count":null,"metadata":{"_uuid":"44470ac428f63f4008865c764944017893421a72","_cell_guid":"4f8228aa-6eb9-4d5b-9522-70a9ecf56f91"},"outputs":[],"cell_type":"code","source":"df['text_tokens'] = df.text_tokens.map(lambda x: RegexpTokenizer(r'\\w+').tokenize(x))\nprint(\"Author: {}, ID: {}\".format(df.author[an_author], df.id[an_author]))\nprint(df['text_tokens'][an_author][:25])"},{"metadata":{"_uuid":"8f2a8c4f276f568d4252ee23f39032436a296302","_cell_guid":"ba298d1c-53ba-47d1-9d41-2e83ca655e13"},"cell_type":"markdown","source":"### 2.3 Lemmatization\n*Stemming* is the process of reducing inflected (or sometimes derived) words to their word stem, base or root form. The stem **need not be identical to the morphological root of the word** (see [Wikipedia](https://en.wikipedia.org/wiki/Stemming) for more details).\n\n*Lemmatization* is the process of determining the [lemma](https://en.wikipedia.org/wiki/Lemma_(morphology) of a word based on its intended meaning. Unlike stemming, lemmatisation depends on **correctly identifying the intended part of speech and meaning of a word** in a sentence (see [Wikipedia](https://en.wikipedia.org/wiki/Lemmatisation) for more details.) We'll use `WordNetLemmatizer` from `nltk`. "},{"execution_count":null,"metadata":{"_uuid":"dd4441be11d14924efd54f8de9ce1f07208cc4cd","_cell_guid":"c37e4a7a-e6d2-4595-a83e-1a4d426ad972"},"outputs":[],"cell_type":"code","source":"lemma = WordNetLemmatizer()\ndf['tags'] = df.text_tokens.map(lambda x: list(zip(*pos_tag(x)))[1])\n\ndef recode_tag(tag):\n    if tag[0].lower() in ['n', 'r', 'v', 'j']:\n        if tag[0].lower() == 'j': return 'a'\n        else: return tag[0].lower()\n    else: return None\n\ndf['tags'] = df.tags.map(lambda x: list(map(recode_tag, x)))\ndf['tags'] = df.apply(lambda x: list(zip(x.text_tokens, x.tags)), axis=1)\n\ndef lemmatize_tokens(pairs):\n    return [lemma.lemmatize(tok, pos=tag) if tag != None else tok \n            for (tok, tag) in pairs]\n\ndf['text_tokens'] = df.tags.map(lemmatize_tokens)\nprint(\"Author: {}, ID: {}\".format(df.author[an_author], df.id[an_author]))\nprint(df['text_tokens'][an_author][:25])\nprint(df['tags'][an_author][:25])"},{"metadata":{"_uuid":"5273a5bc7abc91a631bb1920f0bef87ace1e8cc9","_cell_guid":"857b17df-f771-417e-be9e-6b0c2607ef58"},"cell_type":"markdown","source":"\n### 2.4. Stop words\n**Removing common** English **words** like `and`, `the`, `of` and so on."},{"execution_count":null,"metadata":{"_uuid":"cbd9d7c7beafec021409060d94fcbb0d3d9afecb","_cell_guid":"ed172446-1023-4c16-9c48-87403ca85849"},"outputs":[],"cell_type":"code","source":"stop_en = stopwords.words('english')\ndf['text_tokens'] = df.text_tokens.map(lambda x: [t for t in x if t not in stop_en])\nprint(\"Author: {}, ID: {}\".format(df.author[an_author], df.id[an_author]))\nprint(df['text_tokens'][an_author][:25])"},{"metadata":{"_uuid":"31d8f6a726c515d1c5ac4ea15cc4359365f9b4ab","_cell_guid":"03ed1cf7-0922-426f-80c3-4bbee3470630"},"cell_type":"markdown","source":"### 2.5. Bigrams\nLet's construct bigrams (**words' pairs**) for every text:"},{"execution_count":null,"metadata":{"_uuid":"675fd23d7f9c5e87ec2e0f035242bfc6ab0fb201","_cell_guid":"49cd3a93-4802-4ec0-bf6b-f0d9fcd6079f"},"outputs":[],"cell_type":"code","source":"df['text_tokens_bigrams'] = df.text_tokens.map(lambda x: [' '.join(x[i:i+2]) \n                                                          for i in range(len(x)-1)])\nprint(\"Author: {}, ID: {}\".format(df.author[an_author], df.id[an_author]))\nprint(df['text_tokens_bigrams'][an_author][:25])"},{"metadata":{"_uuid":"d3fd161b8bed8598ed9d624a36a06608169864a8","_cell_guid":"bd57d342-571b-4a72-beb8-7a79d98708ba"},"cell_type":"markdown","source":"### 2.5. Final cleaning\n#### 2.5.1. Short words\nHere we'll remove all \"extremely short\" words (that have less than 2 characters, if those are still exist in the texts for some reason)."},{"execution_count":null,"metadata":{"_uuid":"dc312b82d16aadc21f0ebc8074c6c382afd97efc","_cell_guid":"aa169d7e-de52-4c49-8e4e-912582488414"},"outputs":[],"cell_type":"code","source":"df['text_tokens'] = df.text_tokens.map(lambda x: [t for t in x if len(t) > 1])\nprint(\"Author: {}, ID: {}\".format(df.author[an_author], df.id[an_author]))\nprint(df['text_tokens'][an_author][:25])"},{"metadata":{"_uuid":"15934fbd9cb075905e5eda8c08af56ebc29541cb","_cell_guid":"edeec33f-168c-4ae3-9737-4057affdcd1a"},"cell_type":"markdown","source":"#### 2.5.1. Join tokens and bigrams\nAdding our bigrams to to pool of tokens for every text:"},{"execution_count":null,"metadata":{"_uuid":"4a5eeda60768070c8420d3b47c2eabd860ed3f41","_cell_guid":"31e09b75-1594-4d8f-97d9-7ddd7b05e5c5"},"outputs":[],"cell_type":"code","source":"df['text_tokens'] = df.text_tokens + df.text_tokens_bigrams\nprint(\"Author: {}, ID: {}\".format(df.author[an_author], df.id[an_author]))\nprint(df['text_tokens'][an_author][:100])"},{"metadata":{"_uuid":"cdfb77459a42212d271bb25bfbcacffcaff1f00e","_cell_guid":"a8e7127b-79a2-4cc4-9857-67e063f36ddb"},"cell_type":"markdown","source":"\n## 3. LDA\nFinally, let's use **LDA** ([Latent Dirichlet allocation](https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation)) to extract topic structure from the corpus of texts.\n"},{"execution_count":null,"metadata":{"collapsed":true,"_uuid":"0b0844aaf726cd9f8fd1d52e7c1afb30c6147618","_cell_guid":"b280791c-8860-4428-9342-5a50234933fa"},"outputs":[],"cell_type":"code","source":"from gensim import corpora, models\nT = 4 # number of topics\nnp.random.seed(2017)\ntexts = df['text_tokens'].values\ndictionary = corpora.Dictionary(texts)\ncorpus = [dictionary.doc2bow(text) for text in texts]\nldamodel = models.ldamodel.LdaModel(corpus, id2word=dictionary, \n                                    num_topics=T, passes=7, minimum_probability=0)"},{"metadata":{"_uuid":"0e3f5afefa2884f4a54cb2c46fc6d3dc30132c04","_cell_guid":"e66f4e0f-63d5-4c0f-9c78-f5681da05014"},"cell_type":"markdown","source":"Top words for every topic are:"},{"execution_count":null,"metadata":{"_uuid":"66c3e929e9a7dfb1892fc9e3a8a5714432edc6c2","_cell_guid":"03d6a324-1ec3-4f21-9669-96c8cbd54934"},"outputs":[],"cell_type":"code","source":"ldamodel.print_topics(num_topics=3, num_words=5)"},{"metadata":{"_uuid":"1b74df5a7d0ec8a30b2b8206e9fe19c141265239","_cell_guid":"315d8db6-6a27-4b88-8100-65c82e40207b"},"cell_type":"markdown","source":"Refactoring results of LDA into numpy matrix (`number_of_texts` $\\times$ `number_of_topics`). \nAlso let us precompute **pairwise distance** between text with **cosine distance**."},{"execution_count":null,"metadata":{"collapsed":true,"_uuid":"1c976c28a48c6812769ffbe51efaa8587c8582a3","_cell_guid":"6d2c3e98-a64b-4671-a254-c2a34b1280ce"},"outputs":[],"cell_type":"code","source":"# Matrix with topics probabilities for every text:\nhm = np.array([[y for (x,y) in ldamodel[corpus[i]]] for i in range(len(corpus))])\n# Computing pairwise cosine distance between texts:\nprecomp_cosine = pairwise_distances(hm, metric='cosine')"},{"metadata":{"_uuid":"20ab69d415be7130431184fdff9777b916e5fa35","_cell_guid":"b50688ac-5784-4266-8500-b3016fe0e250"},"cell_type":"markdown","source":"And **reduce dimensionality** using **t-SNE**:"},{"execution_count":null,"metadata":{"collapsed":true,"_uuid":"dc99c3c9588d36c8506b2b46c4478ffdb88728dd","_cell_guid":"d7056768-3bb5-49cb-a657-7f698adfb806"},"outputs":[],"cell_type":"code","source":"tsne = TSNE(random_state=2017, perplexity=25, metric='precomputed', early_exaggeration=4)\ntsne_rep = tsne.fit_transform(precomp_cosine)\ntsne_rep = pd.DataFrame(tsne_rep, columns=['x','y'])\ntsne_rep['hue'] = [['EAP', 'HPL', 'MWS'].index(x) for x in df.author.values]"},{"metadata":{"_uuid":"6edd604320f04592fd1db8d59e1031c2edc99bed","_cell_guid":"f2c9e484-c6a8-4383-9581-bf41e60b6592"},"cell_type":"markdown","source":"## 4. Ploting\nUsing `Bokeh` for scatter plot with interactions. Hover mouse over a dot to see the title of the respective text:"},{"execution_count":null,"metadata":{"_uuid":"7df72f4971bb759dde983e9459804e6fc559316a","_kg_hide-input":true,"_cell_guid":"4d5c6842-cbe7-4a64-9192-9511f2092860"},"outputs":[],"cell_type":"code","source":"source = ColumnDataSource(\n        data=dict(\n            x = tsne_rep.x,\n            y = tsne_rep.y,\n            colors = [all_palettes['Inferno'][4][i] for i in tsne_rep.hue],\n            author = df.author,\n            text = df.text,\n            alpha = [0.7] * tsne_rep.shape[0],\n            size = [7] * tsne_rep.shape[0]\n        )\n    )\n\nhover_tsne = HoverTool(names=[\"df\"], tooltips=\"\"\"\n    <div style=\"margin: 10\">\n        <div style=\"margin: 0 auto; width:300px;\">\n            <span style=\"font-size: 12px; font-weight: bold;\">Author:</span>\n            <span style=\"font-size: 12px\">@author</span>\n        </div>\n        <div style=\"margin: 0 auto; width:300px;\">\n            <span style=\"font-size: 12px; font-weight: bold;\">Text:</span>\n            <span style=\"font-size: 12px\">@text</span>\n        </div>\n    </div>\n    \"\"\")\n\ntools_tsne = [hover_tsne, 'pan', 'wheel_zoom', 'reset']\nplot_tsne = figure(plot_width=700, plot_height=700, tools=tools_tsne, title='Spooky')\n\nplot_tsne.circle('x', 'y', size='size', fill_color='colors', \n                 alpha='alpha', line_alpha=0, line_width=0.01, source=source, name=\"df\")\n\nlayout = column(plot_tsne)"},{"execution_count":null,"metadata":{"_uuid":"c756e4de7384ec2a5c8bb9499fa056b97b704129","_kg_hide-input":true,"_cell_guid":"bfefe1a0-2a38-4ef7-88c4-17ad245fcaff"},"outputs":[],"cell_type":"code","source":"show(layout)"},{"metadata":{"_uuid":"28358d8a77865e37e9dc33d1e6e863c1b5497f17","_cell_guid":"f82effe7-7ed0-42c7-8876-ffd8c581b865"},"cell_type":"markdown","source":"Thanks for reading!"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"file_extension":".py","mimetype":"text/x-python","name":"python","version":"3.6.3","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python"}},"nbformat":4}