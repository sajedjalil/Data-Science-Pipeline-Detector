{"cells":[{"source":"## Technical report\n a detailed account of your final model \n \n assumptions that you made during your modeling \n \n \n \n \n ","cell_type":"markdown","metadata":{"_uuid":"9a194413742870233b45243cf59ef4f2df80aee6","_cell_guid":"ddbc96ba-790e-40e1-a0a0-7d1b977e70a2"}},{"source":"How you acquired your data (including any sampling that you did)\n\n\n\nHow the data should be transformed, including justifying your choices\n\nHow you operationalized your outcome variable, including your justification\n\nYour choice of model and any hyperparameters, including what metric or metrics you use to determine a successful model\n\nAny future deployment strategies, additions of data, or modeling techniques you have yet to try","cell_type":"markdown","metadata":{"_uuid":"d5c730dd237abbaa474e475149aea6fba4ef6fe1","_cell_guid":"44bfe7ca-d682-4476-b0a1-30009bfae3a4"}},{"source":"1. Project objective: Identify the author of the sentences in the test set.\n\n2. Dataset: Works of fiction written by spooky authors of the public domain: Edgar Allan Poe, HP Lovecraft and Mary Shelley\n\n3. Initial EDA: Variable distributions, correlations, etc\n\n4.Train/Test Split\n\n5.Preprocessing performed on Training set: data types converted, missing data handled, dummy variables created, data parsed for errors\n\n6.-Initial model created\n\n7.-Initial model evaluated appropriately (R^2, RMSE, MAE, or Precision/Recall/F1)\n\nsome reference: https://www.kaggle.com/anokas/data-analysis-xgboost-starter-0-35460-lb","cell_type":"markdown","metadata":{"_uuid":"65afb590a5cae2d0da62fdadc5363e33fc15af2a","_cell_guid":"420f16dd-4b8a-480a-b0ff-394a6d149711"}},{"source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize \nfrom nltk.stem import SnowballStemmer\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\nimport spacy\nfrom textblob import TextBlob\n\n# Any results you write to the current directory are saved as output.\nfrom keras.preprocessing import sequence\nfrom keras.utils import np_utils\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Activation, Embedding\nfrom keras.layers import LSTM\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.decomposition import LatentDirichletAllocation\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.decomposition import TruncatedSVD","execution_count":null,"outputs":[],"cell_type":"code","metadata":{"_uuid":"35619e3e5c15f15b3d0fcbee4d59cc00bf237438","_cell_guid":"4d627dd6-eac2-40c7-a0f0-111c77ea0c27"}},{"source":"#import the datasets\ntrain=pd.read_csv('../input/train.csv')\ntest=pd.read_csv('../input/test.csv')\nsample=pd.read_csv('../input/sample_submission.csv')","execution_count":null,"outputs":[],"cell_type":"code","metadata":{"_uuid":"8e8d87623b9b9a89150425a43ffe2cbc4ab0b364","_cell_guid":"f2b503ab-fec4-4776-a1b0-dffc833154df","collapsed":true}},{"source":"## ** Preprocessing**","cell_type":"markdown","metadata":{"_uuid":"47928c1e1e11ef06fe30d98489c36772a411f896","_cell_guid":"65b75403-a5b0-42d7-aac6-a9bca2cf086b"}},{"source":"**1.Assign the authors with numbers to a new column 'author_num' to the original dataframe**","cell_type":"markdown","metadata":{"_uuid":"f150ecf0825e14013e3703d7811d4b60a5487652","_cell_guid":"5aba8a13-6229-44b2-9461-13f9ba5a469e"}},{"source":"#Assign the authors with numbers to a new column 'author_num'\n#0 for 'EAP'\n#1 for 'HPL'\n#2 for 'MWS'\ntrain['author_num']=train['author'].apply({'EAP':0,  'HPL':1,'MWS':2}.get)\ntrain.head()","execution_count":null,"outputs":[],"cell_type":"code","metadata":{"_uuid":"680e34cf8436fe7df9b693e47c228b861c46e809","_cell_guid":"dd7d3b28-86e0-4fde-9fe3-3fd276cfd415"}},{"source":"#Assign the features and target\nX_text_train=train['text'].values\nX_text_test=test['text'].values\ny=train['author_num'].values\nnum_labels = len(np.unique(train['author_num']))","execution_count":null,"outputs":[],"cell_type":"code","metadata":{"_uuid":"2e5b50c8aa4042b401e11ee304213b9dfc32c77e","_cell_guid":"34c1d63f-2fd7-4496-aa59-8b7170b514a9","collapsed":true}},{"source":"**2. Removing the stopwords, punctuations and stemming the words**","cell_type":"markdown","metadata":{"_uuid":"1fd03387235f8e9ca5dbda9159ab138c37b82551","_cell_guid":"01e532cc-5cd0-4199-91d4-bc996abd49d1"}},{"source":"#Define the stopwords to remove and the stemming tool\nstop_words = set(stopwords.words('english'))\nstop_words.update(['.', ',', '\"', \"'\", ':', ';', '(', ')', '[', ']', '{', '}'])\nstemmer = SnowballStemmer('english')","execution_count":null,"outputs":[],"cell_type":"code","metadata":{"_uuid":"0108a1847fd5e0837dd33a37d59abd6774fc8bf2","_cell_guid":"d152b67f-ee66-4045-83ab-96fdae5547cd","collapsed":true}},{"source":"## NLTK tokenize package: http://www.nltk.org/api/nltk.tokenize.html\n\n**word_tokenize** ","cell_type":"markdown","metadata":{"_uuid":"81063047cce94a177e4996c402eb561ef2ccaa39","_cell_guid":"697d7832-b0a0-48d7-acbd-0705e64207d4"}},{"source":"#Preprocess the text in training and testing\nprocessed_train = []\nfor doc in X_text_train:\n    tokens = word_tokenize(doc)\n    filtered = [word for word in tokens if word not in stop_words]\n    stemmed = [stemmer.stem(word) for word in filtered]\n    processed_train.append(stemmed)\n    \nprocessed_test = []\nfor doc in X_text_test:\n    tokens = word_tokenize(doc)\n    filtered = [word for word in tokens if word not in stop_words]\n    stemmed = [stemmer.stem(word) for word in filtered]\n    processed_test.append(stemmed)","execution_count":null,"outputs":[],"cell_type":"code","metadata":{"_uuid":"70d0b6c9f650ed423ba9cd31e06ec79ab06783e3","_cell_guid":"e0d83d27-812e-402a-a6ee-2c82a5bfb35e","collapsed":true}},{"source":"","cell_type":"markdown","metadata":{"_uuid":"3471e59535200af3bb661f769178bd3e3f3f983a","_cell_guid":"d19f9556-8c3a-4ddc-9e30-0a82b0eb7737"}},{"source":"X_text_train[1]","execution_count":null,"outputs":[],"cell_type":"code","metadata":{"_uuid":"a117aeffd262580f6d9d9c4a3e9517df7b87a682","_cell_guid":"14396488-e309-4da3-bbc3-f2a13ea0545f"}},{"source":"processed_train[1]","execution_count":null,"outputs":[],"cell_type":"code","metadata":{"_uuid":"979d8d4cd76d26ea636057ee482e304653ba87fe","_cell_guid":"160cbe3a-5076-4011-b9ee-dbb4b6ac037a","scrolled":true}},{"source":"train['processed_train']=processed_train","execution_count":null,"outputs":[],"cell_type":"code","metadata":{"_uuid":"960d0b53ff94ba511c0817dcd0699900cc1c9354","_cell_guid":"fe331b06-1dc3-4f99-93f1-8824a8052934","collapsed":true}},{"source":"train.head()","execution_count":null,"outputs":[],"cell_type":"code","metadata":{"_uuid":"d63ef2096d49cb19a665a40a0fca232eca2ba610","_cell_guid":"129c5a23-e90d-47cc-80d6-2b88c64ed63d"}},{"source":"train.columns","execution_count":null,"outputs":[],"cell_type":"code","metadata":{"_uuid":"545e6dca2762f784134051588050c1b14f18f34f","_cell_guid":"ac9823cc-ea50-4be2-b496-dceab2ca15cb"}},{"source":"row_lst = []\nfor lst in train.loc[:,'processed_train']:\n    text = ''\n    for word in lst:\n        text = text + ' ' + word\n    row_lst.append(text)\n\ntrain['final_processed_text'] = row_lst","execution_count":null,"outputs":[],"cell_type":"code","metadata":{"_uuid":"a2ae9da7f953584692d02ab55c8b2dea7a2d741a","_cell_guid":"754563cd-63c0-41fe-b719-09ef89d51bc4","collapsed":true}},{"source":"test['processed_test']=processed_test\ntest.head()","execution_count":null,"outputs":[],"cell_type":"code","metadata":{"_uuid":"0c392bcca2c9a4acc85f28473f5b9f54998647b4","_cell_guid":"1ecf4f2a-12a8-4efb-895d-6516f934f2e5"}},{"source":"row_lst = []\nfor lst in test.loc[:,'processed_test']:\n    text = ''\n    for word in lst:\n        text = text + ' ' + word\n    row_lst.append(text)\n\ntest['final_processed_test'] = row_lst\n\ntest.head()","execution_count":null,"outputs":[],"cell_type":"code","metadata":{"_uuid":"0fb8e0d6997d7dfaa9d00e1853aafd38145600e6","_cell_guid":"864da92b-0437-4f57-bf7f-c402320895bc"}},{"source":"train.head()","execution_count":null,"outputs":[],"cell_type":"code","metadata":{"_uuid":"2ebb5cfe73ea32f1aa83c6c5b6d7cec247bfe3a8","_cell_guid":"a4f1645e-43b2-467c-9fe2-3839d3c38e2c"}},{"source":"## **Named Entities**","cell_type":"markdown","metadata":{"_uuid":"4ec26611fa00d711ebfce208883a90c8253e6168","_cell_guid":"fc898a26-48e9-4e21-af55-8ea410afee44"}},{"source":"Named entities are business, people, countries, or other things that refer to a specific person, place, or thing (think Apple, computer manufacturer versus apple, delicious crunchy fruit). spaCy can identify named entities for us, which we can either highlight or drop from our analyses.\n","cell_type":"markdown","metadata":{"_uuid":"e992b9232fc44f17d27ba98290712026255f0bd5","_cell_guid":"282a0a67-6ff1-42d4-8300-ba5f7c881f9b"}},{"source":"nlp = spacy.load('en')\ncontent=[]\nfor i in train['processed_train']:\n    content.append(i)\n\n# for named_entity in content.ents:\n#     print(named_entity.text, named_entity.label_)","execution_count":null,"outputs":[],"cell_type":"code","metadata":{"_uuid":"26d9df7fe077c0f4e04e6a5ecc8fe1d348e89077","_cell_guid":"b4490d9f-8a9c-47f8-a577-28d973041462","collapsed":true}},{"source":"\n##  Use LDA to identify topic","cell_type":"markdown","metadata":{"_uuid":"c73f59ffd8e7cfc1fccb5dde98954f9d78370257","_cell_guid":"9a53f4a8-fddf-46ec-b04d-7136f66945ae"}},{"source":"from sklearn.decomposition import LatentDirichletAllocation, TruncatedSVD\ncv = CountVectorizer(stop_words='english')\ncv.fit(train['text'])\nX = cv.transform(train['text'])\nfeature_names = cv.get_feature_names()\n\nlda = LatentDirichletAllocation(n_components=10)\nlda.fit(X)\n\nresults = pd.DataFrame(lda.components_,\n                      columns=feature_names)\n\nfor topic in range(10):\n    print('Topic', topic)\n    word_list = results.T[topic].sort_values(ascending=False).index\n    print(' '.join(word_list[0:25]), '\\n')","execution_count":null,"outputs":[],"cell_type":"code","metadata":{"_uuid":"9e053ec7205255844e3d5f442f8df157970464eb","_cell_guid":"75d5e233-3b82-4a00-aaef-61cfd24a4f30"}},{"source":"## RandomForestClassifier\n\n","cell_type":"markdown","metadata":{"_uuid":"44704d12f21dcf40190c9d52b0321cb7441399a9","_cell_guid":"433c825f-a899-4722-bbc8-9250b5d4ca75"}},{"source":"X_train, X_test, y_train, y_test = train_test_split(train['final_processed_text'],\n                                                   train['author_num'],\n                                                   test_size=0.33,\n                                                   random_state=8675309)","execution_count":null,"outputs":[],"cell_type":"code","metadata":{"_uuid":"0d3d009d95f08b780fec3d1f55d6402c45d7a08d","_cell_guid":"32f35225-ed7c-4c8e-9784-4878076fa7ad","collapsed":true}},{"source":"\ncv = CountVectorizer(stop_words='english')\ncv.fit(X_train)\n\nX_train_cv = cv.transform(X_train)\nX_test_cv = cv.transform(X_test)\n\nrf = RandomForestClassifier()\nrf.fit(X_train_cv, y_train)\nprint(rf.score(X_test_cv, y_test))\npredictions = rf.predict(X_test_cv)\nprint(confusion_matrix(y_test, predictions))\nprint(classification_report(y_test, predictions))","execution_count":null,"outputs":[],"cell_type":"code","metadata":{"_uuid":"8c5b12f0a6c8019fc3a978cb601e1cfe045121fe","_cell_guid":"0dbede8f-09cb-45a8-97f2-05091d71354b","scrolled":true}},{"source":"## TfidfVectorizer() with English stop words","cell_type":"markdown","metadata":{"_uuid":"ad06dd7594854738a29fd78db77353d0a542fdb1","_cell_guid":"8d1aaafb-ad95-4aa3-8bb5-eefccd763fa5"}},{"source":"","execution_count":null,"outputs":[],"cell_type":"code","metadata":{"_uuid":"f1349738c034ef49fd9fca9b131866f853cf5a57","_cell_guid":"ec655b82-a4ac-4359-9b45-59d3f23e1c97","collapsed":true}},{"source":"\ntfidf = TfidfVectorizer(stop_words='english')\ntfidf.fit(X_train)\n\nX_train_tfidf = tfidf.transform(X_train)\nX_test_tfidf = tfidf.transform(X_test)\ntest_tfidf = tfidf.transform(test['final_processed_test'])\n\nrf = RandomForestClassifier()\nrf.fit(X_train_tfidf, y_train)\nprint(rf.score(X_test_tfidf, y_test))\npredictions = rf.predict(X_test_tfidf)\nprint(confusion_matrix(y_test, predictions))\nprint(classification_report(y_test, predictions))","execution_count":null,"outputs":[],"cell_type":"code","metadata":{"_uuid":"12e0b979937c3f760df53f740f885a3e23c2fdae","_cell_guid":"734a694c-9549-4ab8-b1ab-f02205ffc1d3"}},{"source":"","execution_count":null,"outputs":[],"cell_type":"code","metadata":{"_uuid":"b10ec966f8c3ff9a0dd0e1385ff47c6d075a7478","_cell_guid":"13f16290-b9b8-4279-848b-f93b6cc49997","collapsed":true}},{"source":"pred=rf.predict_proba(test_tfidf)\nprob=pd.DataFrame(pred,columns=['EAP','HPL','MWS'])\nsubmit1=pd.concat([test, prob], axis=1)\ndel submit1['text']","execution_count":null,"outputs":[],"cell_type":"code","metadata":{"_uuid":"6cdbe42781abdb3bcfb4c1954293e9595db8f2fa","_cell_guid":"f8fefb76-89c9-4c52-8061-ec74b2100e03","collapsed":true}},{"source":"","execution_count":null,"outputs":[],"cell_type":"code","metadata":{"_uuid":"3840ae27057ea73fb9e037b7cb9f552b8c9b4022","_cell_guid":"561ad4d3-d4d2-4a8d-b89b-67eab539f2f5","collapsed":true}},{"source":"del submit1['processed_test']","execution_count":null,"outputs":[],"cell_type":"code","metadata":{"_uuid":"8bbd8d9c2ad88b70f98bf4fd523d651c625fdd43","_cell_guid":"30323bd3-5ac7-4fbb-a589-2576d4b928b2","collapsed":true}},{"source":"del submit1['final_processed_test']","execution_count":null,"outputs":[],"cell_type":"code","metadata":{"_uuid":"8f36f92713a9ded3566670703753ae4bffc98e19","_cell_guid":"59ffa1e9-2675-4c92-a320-9b674b23743f","collapsed":true}},{"source":"submit1","execution_count":null,"outputs":[],"cell_type":"code","metadata":{"_uuid":"754cae13033aff334c1465c3119bbc4b0eb2faea","_cell_guid":"7423e10f-e379-4edc-94ad-b0c1a070530c"}},{"source":"submit1.to_csv('./TfidfVectorizer.csv', index=False, header=True)","execution_count":null,"outputs":[],"cell_type":"code","metadata":{"_uuid":"546d336bcbc9bcb5b95e9e14b453d58c30e4d3e6","_cell_guid":"8f818bad-7846-469a-93f7-47114e34bb0b","collapsed":true}}],"nbformat":4,"nbformat_minor":1,"metadata":{"kernelspec":{"language":"python","name":"python3","display_name":"Python 3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"name":"python","pygments_lexer":"ipython3","mimetype":"text/x-python","file_extension":".py","version":"3.6.3","nbconvert_exporter":"python"}}}