{"cells":[{"outputs":[],"source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"cell_type":"code","metadata":{"collapsed":true,"_cell_guid":"3beea829-117b-404f-a5ba-b704c0723831","_uuid":"0bc2753ac8c66f5e17d43e2e1f271a6f0a2d4b98"}},{"source":"# Imports\n\n\n","cell_type":"markdown","metadata":{"_cell_guid":"cff64719-3f4a-428c-90aa-73ff8824217c","_uuid":"4a2dbe6454aa21f564bea98b3fc244a2f2bc233c"}},{"outputs":[],"source":"from os import path\nimport string\n\nimport matplotlib.pyplot as plt\n\nfrom plotly import tools\nfrom plotly import __version__\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\nimport plotly.graph_objs as go\nprint(__version__)\ninit_notebook_mode(connected=True)\n\nimport nltk\nfrom nltk import word_tokenize\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.stem.snowball import SnowballStemmer\nfrom nltk.corpus import stopwords\n\n# Spacy\nimport spacy\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.decomposition import NMF, LatentDirichletAllocation, TruncatedSVD\n\nfrom sklearn.metrics.pairwise import cosine_distances\nfrom sklearn.manifold import MDS\nfrom sklearn.manifold import TSNE\n\nfrom PIL import Image\nfrom wordcloud import WordCloud, STOPWORDS\nimport urllib\nfrom io import BytesIO","execution_count":null,"cell_type":"code","metadata":{"collapsed":true,"_cell_guid":"4b4f0f32-66e5-4dfb-8233-d4327a2e553a","_kg_hide-input":false,"_uuid":"fb4e017c6ef80660e8c8a8955a28a6c11c4d96b1"}},{"source":"# Initialization","cell_type":"markdown","metadata":{"_cell_guid":"c5e79d89-b32a-48a2-a8a1-32c9f7093bab","_uuid":"3361d03c8b1ab0477debe5944fca9b9c4898f089"}},{"outputs":[],"source":"# NLTK \nwnl = WordNetLemmatizer()\nsb_stemmer = SnowballStemmer(\"english\")\n\n# Spacy\nnlp = spacy.load('en')\n\n# Plotly\nauthor_images = [\n    \"https://upload.wikimedia.org/wikipedia/commons/8/84/Edgar_Allan_Poe_daguerreotype_crop.png\",\n    \"https://upload.wikimedia.org/wikipedia/commons/6/65/RothwellMaryShelley.jpg\",\n    \"https://upload.wikimedia.org/wikipedia/commons/1/10/H._P._Lovecraft%2C_June_1934.jpg\"\n]\n\npaper_bgcolor=\"rgb(240,240,240)\"\nplot_bgcolor=\"rgb(240,240,240)\"\n\n# color definitions (from colorlover module)\n# import colorlover as cl\n# cl.scales[\"3\"][\"div\"][\"RdBu\"]\nrd_bu = ['rgb(239,138,98)', 'rgb(247,247,247)', 'rgb(103,169,207)']","execution_count":null,"cell_type":"code","metadata":{"collapsed":true,"_cell_guid":"f29820ec-9a79-46c2-8288-5778f53f4e5b","_kg_hide-input":false,"_uuid":"c46802a01c05f1cb176827e52e090bd7412e18f7"}},{"source":"## Load data","cell_type":"markdown","metadata":{"_cell_guid":"4e73fc3f-b0dc-4beb-89b4-d8d5adbcd5d7","_uuid":"92b052bf6b6045d508dfc65dcb5e4a7ea73020ca"}},{"outputs":[],"source":"train = pd.read_csv(\"../input/train.csv\")\ntest = pd.read_csv(\"../input/test.csv\")","execution_count":null,"cell_type":"code","metadata":{"collapsed":true,"_cell_guid":"40d3b364-394e-4534-b0d9-a62fd9f70bd7","_kg_hide-input":false,"_uuid":"ef06ebfe8a1115ae1af8f9c09e0c37496c2be073"}},{"source":"# Preprocessing\n* Reduce the number of instances per class to 1/20 th th to make it easy for visualizing through T-SNE.\n* Before preprocessing, if class A has 220 examples, and class B has 401 examples, after preprocessing, there will be 11 examples for class A and 21 examples for class B, respectively.\n\n","cell_type":"markdown","metadata":{"_cell_guid":"b8801cd9-c982-4444-a43c-506d363bfca9","_uuid":"6670003b649f879da0c13a87e9b96e6633858f3f"}},{"outputs":[],"source":"def shrink_train_data_size_by_factor(factor, text_arr):\n    text_arr_reduced = []\n    for i in range(len(text_arr) // factor):\n        start = i * factor\n        end = (start + factor) - 1\n        text_arr_reduced.append(\"\\n\".join(text_arr[start:end+1]))\n    if len(text_arr) % factor != 0:\n        rem_elments = len(text_arr) % factor\n        start = (len(text_arr) // factor) * factor\n        end = (start + rem_elments) - 1\n        text_arr_reduced.append(\"\\n\".join(text_arr[start:end+1]))\n    return text_arr_reduced","execution_count":null,"cell_type":"code","metadata":{"collapsed":true,"_cell_guid":"f34854f0-a5d7-427c-a2f8-f81888430680","_kg_hide-input":false,"_uuid":"6e6e1eb01af16ca692dc340d74e091f873e59c92"}},{"outputs":[],"source":"counts_by_author = train[\"author\"].value_counts()\nauthor_names = list(counts_by_author.index)\ninstances_per_author = list(counts_by_author)\n\n# make copy of the original training data set\ncounts_by_author_orig = list(counts_by_author)\nauthor_names_orig = list(author_names)\ninstances_per_author_orig = list(instances_per_author)\ntrain_orig = train.copy()\n\n# factor to which the train data should be reduced\nfactor = 20\n\n# temporary variables to hold the reduced data\nshrinked_train = []\nshrinked_labels = []\n\nfor i in range(len(author_names)):\n    instances = train[train[\"author\"] == author_names[i]][\"text\"].as_matrix()\n    instances_red = shrink_train_data_size_by_factor(factor, instances)\n    labels_red = [author_names[i]] * len(instances_red)\n    shrinked_train += instances_red\n    shrinked_labels += labels_red\n\n# training data (shrinked)\ntrain = pd.DataFrame({\"text\": shrinked_train, \"author\": shrinked_labels})","execution_count":null,"cell_type":"code","metadata":{"collapsed":true,"_cell_guid":"e8390e07-c795-46c6-8b1a-bc85e5d757c8","_kg_hide-input":false,"_uuid":"373dc2bf350580e457c6badd0b8905fa153a91fd"}},{"outputs":[],"source":"counts_by_author = train[\"author\"].value_counts()\nauthor_names = list(counts_by_author.index)\ninstances_per_author = list(counts_by_author)\nprint(\"Original training data stats\")\nprint(\"----------------------------\")\nprint(\"Training data size: \", train_orig.shape)\nprint(\"Author names:\", author_names_orig)\nprint(\"Instances per author:\", instances_per_author_orig)\n\nprint(\"\\nAfter shrinking the training data by factor: {}\".format(factor))\nprint(\"------------------------------------------------\")\nprint(\"Training data size: \", train.shape)\nprint(\"Author names:\", author_names)\nprint(\"Instances per author:\", instances_per_author)\ntrain.head()","execution_count":null,"cell_type":"code","metadata":{"collapsed":true,"_cell_guid":"42695101-d7dd-4c70-9539-dd9958da7d6e","_kg_hide-input":false,"_uuid":"68bf171d314758eb29f6b262ef236e35fe918783"}},{"outputs":[],"source":"# concatenate texts belonging to same author\ncombined_texts = []\nfor author in author_names: \n    texts_of_author = train[train[\"author\"] == author][\"text\"]\n    texts_np_array = texts_of_author.as_matrix()\n    text_together = \" \".join(texts_np_array)\n    combined_texts.append(text_together)\ncombined_texts = np.asarray(combined_texts)","execution_count":null,"cell_type":"code","metadata":{"collapsed":true,"_cell_guid":"9513ad07-9755-4b2b-a027-0b32e69d54f5","_kg_hide-input":false,"_uuid":"5ebd061dd8c4ccfc1c07d5a2432edead55f8a2e5"}},{"source":"# EDA\n\n1. General summary: number of instances per class\n1. Corpus level insights","cell_type":"markdown","metadata":{"_cell_guid":"1e9915f9-7ab9-429a-9cf6-e7099d845137","_uuid":"31316714d1ee243fc99001770bb555ffe11c644c"}},{"outputs":[],"source":"layout_images = []\nfor i in range(len(author_images)):\n    layout_image = dict( source = author_images[i], \n                        xref=\"paper\", \n                        yref=\"paper\", \n                        x= (instances_per_author_orig[i] / max(instances_per_author_orig)) - 0.15, \n                        y = (i / len(author_images))+ 0.05, \n                        sizex=0.4,\n                        sizey=0.2,\n                        xanchor=\"left\", \n                        yanchor=\"bottom\")\n    layout_images.append(layout_image)\n        \nbar_data = [go.Bar(\n            x=instances_per_author_orig,\n            y=author_names_orig,\n            orientation = 'h'\n)]\nlayout = go.Layout(title = \"Training data distribution (original data)\", images=layout_images,\n    xaxis=dict(title=\"# of training instances\"),\n    yaxis=dict(title=\"Author\"))\nfig = dict(data=bar_data, layout=layout)\niplot(fig)","execution_count":null,"cell_type":"code","metadata":{"collapsed":true,"_cell_guid":"4fef2cdd-1d7c-4e77-85bb-65a84c055813","scrolled":false,"_kg_hide-input":false,"_uuid":"ff2dda3ccb23e99372f1ca9f6ff0befe4c9bcbb0"}},{"outputs":[],"source":"layout_images = []\nfor i in range(len(author_images)):\n    layout_image = dict( source = author_images[i], \n                        xref=\"paper\", \n                        yref=\"paper\", \n                        x= (instances_per_author[i] / max(instances_per_author)) - 0.15, \n                        y = (i / len(author_images))+ 0.05, \n                        sizex=0.4,\n                        sizey=0.2,\n                        xanchor=\"left\", \n                        yanchor=\"bottom\")\n    layout_images.append(layout_image)\n        \nbar_data = [go.Bar(\n            x=instances_per_author,\n            y=author_names,\n            orientation = 'h'\n)]\nlayout = go.Layout(title = \"Training data distribution (after shrinking)\", images=layout_images,\n    xaxis=dict(title=\"# of training instances\"),\n    yaxis=dict(title=\"Author\"))\nfig = dict(data=bar_data, layout=layout)\niplot(fig)","execution_count":null,"cell_type":"code","metadata":{"collapsed":true,"_cell_guid":"780abe3f-e6a4-44f2-aea0-6318199bfabe","_kg_hide-input":false,"_uuid":"942b8e34e8493a098441cd1f45be45294313ce85"}},{"source":"## Corpus level insights\n1. Heatmap of sentence lengths of training corpus for each author.","cell_type":"markdown","metadata":{"_cell_guid":"2bc880a9-93ce-43a9-8aa0-b569de7800fb","_uuid":"5c2bbd48c92341bfd85c82e4a413cdebd827000c"}},{"outputs":[],"source":"stop_words_en = set(stopwords.words('english'))","execution_count":null,"cell_type":"code","metadata":{"collapsed":true,"_cell_guid":"bd917657-4175-4678-98d7-ccaf1e7fe845","_kg_hide-input":false,"_uuid":"1d4091d92bd6b3141b374d845add40a9d32ab6d5"}},{"outputs":[],"source":"def avg_sentence_length_for_text(text):\n    \"\"\"Given a text containing one more sentences in the form of \n    paragraph, the function returns the average sentence length\n    for the entire paragraph.\n    \"\"\"\n    sentences = nltk.sent_tokenize(text)\n    num_tokens_in_corpus = 0\n    for s in sentences:\n        num_tokens_in_corpus += len(nltk.word_tokenize(s))\n    return float(num_tokens_in_corpus) / len(sentences)","execution_count":null,"cell_type":"code","metadata":{"collapsed":true,"_cell_guid":"2692825b-f0ff-4795-86bc-88c09a1bf582","_kg_hide-input":false,"_uuid":"140c4a8dc9f027f0cad46fd6039b570e3c1964b3"}},{"outputs":[],"source":"def avg_stop_words_per_sentence(text):\n    \"\"\"Given a text containing one more sentences in the form of \n    paragraph, the function returns the average number of function words\n    per sentence in the given text\n    \"\"\"    \n    sentences = nltk.sent_tokenize(text)\n    num_stop_words_in_corpus = 0\n    for s in sentences:\n        s_tokens =  nltk.word_tokenize(s.lower())\n        for w in s_tokens:\n            if w in stop_words_en: \n                num_stop_words_in_corpus += 1\n    return float(num_stop_words_in_corpus) / len(sentences)","execution_count":null,"cell_type":"code","metadata":{"collapsed":true,"_cell_guid":"d9434db8-3ba7-41aa-baf9-e839214df8d1","_kg_hide-input":false,"_uuid":"1aaa7f69a276a3a761c3a152abe7a2ba8160b19d"}},{"outputs":[],"source":"def make_array_size_divisible_by_factor(oned_nparray, fc):\n    \"\"\"Given an 1d array of an arbitrary size, make the shape of the \n    1d array divisible by 100 by appending np.nan values. For ex: If the array \n    dimension is (114, ) then the np.nan 1d array of shape 86 will \n    be appended to the original 1d array to become array size 200. \n    \"\"\"\n    remainder = oned_nparray.shape[0] % fc\n    if remainder > 0:\n        cells_to_fill = fc - remainder\n        nan_array = np.full(cells_to_fill, np.nan)\n        oned_nparray = np.append(oned_nparray, nan_array)\n    num_cols_heatmap = int(oned_nparray.shape[0] / fc)\n    num_rows_heatmap = fc\n    return oned_nparray.reshape((num_rows_heatmap, num_cols_heatmap))","execution_count":null,"cell_type":"code","metadata":{"collapsed":true,"_cell_guid":"cbcb8558-a736-4f85-a8cc-2328cc05302e","_kg_hide-input":false,"_uuid":"8ef8cba5f702062c20c2e68721c92870c33126df"}},{"outputs":[],"source":"def get_reshaped_sen_len_trace_for_author(author_name):\n    sen_lengths_for_author = train_copy[train_copy[\"author\"] == author_name]\n    sen_lengths_nparray = sen_lengths_for_author[\"sen_len\"].as_matrix()\n    sen_lengths_nparray_reshaped = make_array_size_divisible_by_factor(sen_lengths_nparray, 25)    \n    return sen_lengths_nparray_reshaped","execution_count":null,"cell_type":"code","metadata":{"collapsed":true,"_cell_guid":"f6e95eb3-1dd2-4c4d-b6fa-5d340df42bc9","_kg_hide-input":false,"_uuid":"1d43f02292a568ec37e56342c908e90bda49f674"}},{"outputs":[],"source":"def get_reshaped_stop_words_trace_for_author(author_name):\n    stop_words_for_author = train_copy[train_copy[\"author\"] == author_name]\n    stop_words_counts_nparray = stop_words_for_author[\"stop_words\"].as_matrix()\n    stop_words_counts_nparray_reshaped = make_array_size_divisible_by_factor(stop_words_counts_nparray, 25)    \n    return stop_words_counts_nparray_reshaped","execution_count":null,"cell_type":"code","metadata":{"collapsed":true,"_cell_guid":"69e352ca-1a47-4a9a-b82f-728367f7e4d0","_kg_hide-input":false,"_uuid":"b1ab9f0068343c1f2fb1d2ed755bde85be420f46"}},{"outputs":[],"source":"train_copy = train.copy()\ntrain_copy[\"sen_len\"] = train_copy[\"text\"].apply(avg_sentence_length_for_text)\ntrain_copy[\"stop_words\"] = train_copy[\"text\"].apply(avg_stop_words_per_sentence)","execution_count":null,"cell_type":"code","metadata":{"collapsed":true,"_cell_guid":"b67b856e-501a-4c89-9876-c0a0ea5e5e2e","_kg_hide-input":false,"_uuid":"db123544601794970fef802b1037235c5dc62db3"}},{"outputs":[],"source":"avg_sen_len_used_by_authors = []\nfor author in author_names:\n    temp = train_copy[train_copy[\"author\"] == author]\n    num_tokens = temp[\"sen_len\"].sum()\n    avg_sen_len = float(num_tokens) / temp.shape[0]\n    avg_sen_len_used_by_authors.append(avg_sen_len)","execution_count":null,"cell_type":"code","metadata":{"collapsed":true,"_cell_guid":"eca8011c-b36d-470b-9418-b2af08b96c01","_kg_hide-input":false,"_uuid":"b9dee2e679a540e02f4b5929c4082d93dd57d796"}},{"outputs":[],"source":"fig_coords = [(1,1), (1,2), (1,3)]\naxes_names = [(\"x1\", \"y1\"), (\"x2\", \"y2\"), (\"x3\", \"y3\")]\naxes_lo_names = [(\"xaxis1\", \"yaxis1\"), (\"xaxis2\", \"yaxis2\"), (\"xaxis3\", \"yaxis3\")]\nfig = tools.make_subplots(rows=1, cols=3, subplot_titles=(author_names[0], author_names[1], author_names[2]))\nfor author, fig_coord, ax in zip(author_names, fig_coords, axes_names):\n    reshaped_sen_lengths = get_reshaped_sen_len_trace_for_author(author)\n    trace = go.Heatmap(z=reshaped_sen_lengths, colorscale = 'Portland', zmin=0, zmax=80, xaxis=ax[0], yaxis=ax[1])\n    fig.append_trace(trace, fig_coord[0], fig_coord[1])\nfig[\"layout\"].update(title = \"Average sentence lengths (# of words)\")\nfor ax_name in axes_lo_names:\n    fig[\"layout\"][ax_name[0]].update(showgrid=False, showline=False, zeroline=False, ticks='', showticklabels=False)\n    fig[\"layout\"][ax_name[1]].update(showgrid=False, showline=False, zeroline=False, ticks='', showticklabels=False)    \niplot(fig)\n\nfig_coords = [(1,1), (1,2), (1,3)]\naxes_names = [(\"x1\", \"y1\"), (\"x2\", \"y2\"), (\"x3\", \"y3\")]\naxes_lo_names = [(\"xaxis1\", \"yaxis1\"), (\"xaxis2\", \"yaxis2\"), (\"xaxis3\", \"yaxis3\")]\nfig = tools.make_subplots(rows=1, cols=3, subplot_titles=(author_names[0], author_names[1], author_names[2]))\nfor author, fig_coord, ax in zip(author_names, fig_coords, axes_names):\n    reshaped_stop_word_counts = get_reshaped_stop_words_trace_for_author(author)\n    trace = go.Heatmap(z=reshaped_stop_word_counts, colorscale = 'Portland', zmin=0, zmax=35, xaxis=ax[0], yaxis=ax[1])\n    fig.append_trace(trace, fig_coord[0], fig_coord[1])\nfig[\"layout\"].update(title = \"Average number of stop words per sentence\")\nfor ax_name in axes_lo_names:\n    fig[\"layout\"][ax_name[0]].update(showgrid=False, showline=False, zeroline=False, ticks='', showticklabels=False)\n    fig[\"layout\"][ax_name[1]].update(showgrid=False, showline=False, zeroline=False, ticks='', showticklabels=False)\niplot(fig)","execution_count":null,"cell_type":"code","metadata":{"collapsed":true,"_cell_guid":"3470213a-d372-44ca-a7a2-3de5a8c09c17","scrolled":false,"_kg_hide-input":false,"_uuid":"b7189c9e8d917d7796eac95aa6131beb893e9a3c"}},{"source":"# How close authors are to each other?","cell_type":"markdown","metadata":{"_cell_guid":"72e5b16c-3d1b-42eb-9f3a-4f2c8e4eb40f","_uuid":"1b034348528dc7fecb3f11b09fb50287b0954744"}},{"outputs":[],"source":"counts_vectorizer = CountVectorizer(stop_words=\"english\", min_df=3)\ncounts_comb = counts_vectorizer.fit_transform(combined_texts)\n\ntfidf_comb = TfidfTransformer().fit_transform(counts_comb)\n\ncosine_dist = cosine_distances(tfidf_comb)\nmds = MDS(n_components=2, dissimilarity=\"precomputed\", random_state=1)\npos = mds.fit_transform(cosine_dist)\nxs, ys = pos[:,0], pos[:, 1]\ndata = []\nfor i in range(len(author_names)): \n    trace = go.Scatter(x=[xs[i]], y=[ys[i]], mode=\"markers\",\n                       marker= dict(size= 20, line= dict(width=1), color= rd_bu[i]),\n                       name= author_names[i])\n    data.append(trace)\nlayout = go.Layout(title=\"How close authors are to each other?\")\nfig = go.Figure(data=data, layout=layout)\niplot(fig)","execution_count":null,"cell_type":"code","metadata":{"collapsed":true,"_cell_guid":"34d8a9e8-6373-47e3-989f-ea9bd7369597","_kg_hide-input":false,"_uuid":"801383203bd762212cef6c5849b8c664968b1b05"}},{"source":"# Topic modeling","cell_type":"markdown","metadata":{"collapsed":true,"_cell_guid":"4fcf96b9-cbd7-4909-bf9d-d88200945d88","_uuid":"b4f3ab82b88ae6460fc5541a5cf797a7d4551219"}},{"outputs":[],"source":"def is_valid_token(tok):\n    \"\"\"The function returns false, if \n    1. tok length is < 3\n    2. tok contains non-alphabetic characters\n    \"\"\"\n    if not tok.isalpha(): \n        return False \n    if len(tok) < 4: \n        return False\n    return True","execution_count":null,"cell_type":"code","metadata":{"collapsed":true,"_cell_guid":"861d4a4b-2200-422c-9335-b295883340d8","_kg_hide-input":false,"_uuid":"adac8bd300015f70d8143d35cbf3365346c6778a"}},{"outputs":[],"source":"def lemmatize(input_str):\n    tokens = word_tokenize(input_str)\n    tokens = [t for t in tokens if is_valid_token(t) is True]\n    lemmatized = [wnl.lemmatize(t) for t in tokens]\n    return lemmatized","execution_count":null,"cell_type":"code","metadata":{"collapsed":true,"_cell_guid":"56531189-a1fa-4fb3-896a-c5c740ffcc15","_kg_hide-input":false,"_uuid":"be5c7410278349403d2d577b55d9dc7cd01addc1"}},{"outputs":[],"source":"num_topics = 20\n\nvectorizer_ind = CountVectorizer(stop_words=\"english\", min_df=5, tokenizer=lemmatize)\ncounts_ind = vectorizer_ind.fit_transform(train[\"text\"])\nvocab_ind = np.array(vectorizer_ind.get_feature_names())\ntfidf_ind = TfidfTransformer().fit_transform(counts_ind)\n\nclf = NMF(n_components=num_topics)\ndoctopic = clf.fit_transform(tfidf_ind)","execution_count":null,"cell_type":"code","metadata":{"collapsed":true,"_cell_guid":"8ed56076-0ed5-4638-8943-1d0c79db14cb","scrolled":false,"_kg_hide-input":false,"_uuid":"b9ac1ff675beba99da12953adbf35f8ba6b1523c"}},{"outputs":[],"source":"# words associated with topics\ntopic_strings = []\nfor topic in clf.components_:\n    word_idx = np.argsort(topic)[::-1][0:10]\n    topic_words = [vocab_ind[i] for i in word_idx]\n    topic_strings.append(\" \".join(topic_words))\n    #topic_words.append([vocab_ind[i] for i in word_idx])\n\nauthors_of_train_data = train[\"author\"].values\ndoctopic_one_per_author = np.zeros((len(author_names), num_topics))\nfor i in range(len(author_names)): \n    doctopic_one_per_author[i, :] = np.mean(doctopic[authors_of_train_data == author_names[i], :], axis=0)\n\ndoctopic_one_per_author = doctopic_one_per_author / np.sum(doctopic_one_per_author, axis=1, keepdims=True)\n\nfor t in range(len(topic_strings)):\n    print(\"Topic {}: {}\".format(t+1, topic_strings[t]))","execution_count":null,"cell_type":"code","metadata":{"collapsed":true,"_cell_guid":"4d128b57-d04e-451e-8849-3718d355ec22","_kg_hide-input":false,"_uuid":"ca6ff68c4ba44c9b307658bca77f4ef1e52f02e9"}},{"source":"# Main themes of authors in their texts ","cell_type":"markdown","metadata":{"collapsed":true,"_cell_guid":"5256cc2b-ff69-4b92-9ee3-ed9eb3e16a46","_uuid":"8836d72de676c6e4db82948e31d51888d43590de"}},{"source":"## Heatmap of topics","cell_type":"markdown","metadata":{"collapsed":true,"_cell_guid":"53efbcbb-e08c-4ce8-bc78-3d64b7c534f7","_uuid":"21785cb5d76adb31412623c43ecfb8587af698c5"}},{"outputs":[],"source":"topic_headers = [ \"topic-\"+str(i) for i in range(1, num_topics+1)]\ntrace = go.Heatmap(z=doctopic_one_per_author.T, x = author_names, y=topic_strings, colorscale='Portland')\nlayout = go.Layout(height=600, \n                   width=900,\n                   margin=go.Margin(l=400, t=50, r=150, b=200), title=\"Heatmap of topics\")\ndata = [trace]\nfig = go.Figure(data=data, layout=layout)\niplot(fig)","execution_count":null,"cell_type":"code","metadata":{"collapsed":true,"_cell_guid":"9229919e-8bf4-48a7-bf0d-db6d0e8a2a07","scrolled":false,"_kg_hide-input":false,"_uuid":"df90293ee76cf77289e02be1f10d0bc5a698b292"}},{"source":"## Visual clustering with SVD, TSNE","cell_type":"markdown","metadata":{"collapsed":true,"_cell_guid":"03859585-4fac-4bf8-ab62-247ea9c91578","_uuid":"8e248b453e2765051854ce51e830ae596c6e39dd"}},{"outputs":[],"source":"tsne_doctopic_model = TSNE(n_components=2)\ntsne_doctopic_output = tsne_doctopic_model.fit_transform(doctopic)\n\nsvd_doctopic_model = TruncatedSVD(n_components=2)\nsvd_doctopic_output = svd_doctopic_model.fit_transform(doctopic)","execution_count":null,"cell_type":"code","metadata":{"collapsed":true,"_cell_guid":"b4a04097-e7a7-4c48-8bbd-fb63b90525c3","_kg_hide-input":false,"_uuid":"9287f143898d429ece3b928923113792c14192a4"}},{"outputs":[],"source":"fig = tools.make_subplots(rows=1, cols=2, subplot_titles=(\"SVD\", \"TSNE\"))\nfig_coords = [(1,1), (1,2)]\nfor i in range(len(author_names)):\n    x_points = svd_doctopic_output[authors_of_train_data == author_names[i], 0]\n    y_points = svd_doctopic_output[authors_of_train_data == author_names[i], 1]\n    trace = go.Scatter(x=x_points, y=y_points, mode=\"markers\",\n                           marker= dict(size= 10, line= dict(width=0.5), color= rd_bu[i]),\n                           name= author_names[i])\n    fig.append_trace(trace, 1, 1)\nfor i in range(len(author_names)):\n    x_points = tsne_doctopic_output[authors_of_train_data == author_names[i], 0]\n    y_points = tsne_doctopic_output[authors_of_train_data == author_names[i], 1]\n    trace = go.Scatter(x=x_points, y=y_points, mode=\"markers\",\n                           marker= dict(size= 10, line= dict(width=0.5), color= rd_bu[i]),\n                           name= author_names[i], showlegend=False)\n    fig.append_trace(trace, 1, 2)\nfig['layout'].update(height=500, width=800, title=\"Visualizing training instances in reduced dimension\")\niplot(fig)","execution_count":null,"cell_type":"code","metadata":{"collapsed":true,"_cell_guid":"a3d7edca-49c4-408a-b985-e74a58705125","_kg_hide-input":false,"_uuid":"393174c896729d4a2ff79e758c9e1e380bc3c89b"}},{"source":"# References\n\n1. Text Analysis with Topic Models for the Humanities and Social Sciences - https://de.dariah.eu/tatom/\n2. Literature Fingerprinting: A New Method for Visual Literary Analysis - http://ieeexplore.ieee.org/document/4389004/\n3. Overview of Text Visualization Techniques - Springer","cell_type":"markdown","metadata":{"collapsed":true,"_cell_guid":"908a71f5-1abb-49ca-b1a9-2ea239e64d89","_uuid":"4cacc3a13da3aa1bf0bbaf9ebfcb8a30e346dbcc"}}],"nbformat_minor":1,"nbformat":4,"metadata":{"kernelspec":{"name":"python3","language":"python","display_name":"Python 3"},"language_info":{"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","codemirror_mode":{"name":"ipython","version":3},"version":"3.6.3","pygments_lexer":"ipython3"}}}