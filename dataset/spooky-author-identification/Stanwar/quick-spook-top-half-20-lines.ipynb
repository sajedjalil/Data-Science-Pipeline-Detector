{"nbformat_minor":1,"nbformat":4,"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"3c8f8a80-3309-43f5-8960-3fcf4c5c6651","_uuid":"bd6009853541a020f956a36324b9ca6a865f6eb6"},"source":"## Background"},{"cell_type":"markdown","metadata":{"_cell_guid":"23787677-fa60-4cfa-b06d-2553f5fdf7b3","_uuid":"9405a9583c7d8d82a6727008b1d532d34d7adf9a"},"source":"We take **19,759 passages of text** and attempt to [](http://)classify them according to  the **3 authors** that wrote them. The goal is to to minimise **log loss**.\n\nIn the real world accuracy is important but there are other factors in play too, namely:\n* Speed\n* Simplicity\n* Transparency \n* Resources\n* Ethics\n\nOur goal is to quickly produce a model with a few lines of code, running in a few seconds and achieve a top half leaderboard position (at time of writing).\n\n![](http://)![](http://)Fortunately [abhisek](https://www.kaggle.com/abhishek) has created a [wonderful kernel](https://www.kaggle.com/abhishek/approaching-almost-any-nlp-problem-on-kaggle/notebook) that goes through pretty much every model we would want to consider. This really is a fantastic resource and thank you so much for sharing it. Lets take a look at the performance of abhisek's models. "},{"cell_type":"markdown","metadata":{"_cell_guid":"9a6831a1-6d4a-40f4-876a-16001f6c5691","_uuid":"06e61ebb6d6e31e8a68ebd951fbba9f5e292a25d"},"source":"## Results"},{"cell_type":"markdown","metadata":{"_cell_guid":"fe8f1e3a-1d25-4464-b5d6-4fed10bdea2f","_uuid":"2986aaa93868bd928f51ce70d9eb3460736aa0b4"},"source":"![](https://stanwarkaggle.files.wordpress.com/2017/11/model_comparison.jpg)"},{"cell_type":"markdown","metadata":{"_cell_guid":"1940e824-d8e3-433b-9a6b-c14cd505758b","_uuid":"dacf476f444887353ef9b84a0579a18f5c4c7097"},"source":"I'm using an [MSI Phantom Pro 6RE](https://www.msi.com/Laptop/GS43VR-6RE-Phantom-Pro.html) with a [GTX1060](https://www.scan.co.uk/shop/computer-hardware/virtual-reality/nvidia-geforce-gtx-1060-6gb) graphics card. It's worth noting that abhisek intentionally didn't spend long on parameter tuning. There are some obvious caveats that I'll mention in a minute but let's get stuck into the results first.\n\nThe blue bars shows the log loss scores (which are also written in white at the end of the bars). The orange bars show how long the models took to run. The paramaters are as per abhisek's tutorial (no tuning) but I've included a list at the base of this kernel too.\n\nThere are a lot of models here. To make life a bit easier lets focus on the top 5. These have noticeably lower log loss scores than all the other models.\n\nTaking the top 5 in reverse order: \n\n5th - **Bidirectional LSTM with Glove**. this takes over 7,000 seconds to run! (nearly two hours) <br>\n4th - **LSTM with Glove**. This comes in at around 2,800 seconds (just under an hour) <br>\n3rd - **Count Vectorizer with Naive Bayes**. This simple model takes a second to run! <br>\n2nd - **GRU with Glove**. This is the slowest model of the lot, coming in at over 8,300 seconds (well over two hours) <br>\n1st - **Ensemble** This achieved a small step up in accuracy. It used some of the simpler models and therefore took a few seconds to run. <br>\n\nThere have been some interesting discussions on some unpleasant [bias in word embeddings](http://ruder.io/word-embeddings-2017/index.html#bias). The consequences are not much of an issue here but in real world use cases this is something to consider.\n\nI love neural networks as much as the next person, and while they can work a treat on image data, they aren't really doing that much more than Naive Bayes here. Therefore taking accuracy, speed, simplicity bias etc. into consideration I like the look of the third placed model (sending word counts into a Naive Bayes).\n\nLets have a go at running that model. Fortunately abhisek has done that for us so I'll mostly just be grabbing bits of his code."},{"cell_type":"markdown","metadata":{"_cell_guid":"4bcc56cc-cb9d-4dde-bff8-baea8084036e","_uuid":"5c142c2cb51f099a13f7593dbb641ba4b9ab7ea3"},"source":"## Caveats"},{"cell_type":"markdown","metadata":{"_cell_guid":"f02193f3-31c2-47ee-9603-9614285ff4fe","_uuid":"8a5633c2da6ad8026bae20633cfc75fedc11bf58"},"source":"The results here are not necessarily fair or replicable elsewhere. They are very subject to factors such as:\n* Nature of data\n* Parameter choices\n* Features\n\nNonetheless it is still interesting to make a crude comparisson between such a wide range of methods to spot one that meets our criteria well."},{"cell_type":"markdown","metadata":{"_cell_guid":"dd7052b4-90d1-4536-92bc-eb548082393b","_uuid":"4f8fd0739eeebcb2aca105720a92c8156ec91af9"},"source":"### Preperation"},{"cell_type":"markdown","metadata":{"_cell_guid":"acf187f9-fb92-4446-ac90-4b0dd3e25121","_uuid":"5d7b1464d88932e698c66428063be38fb0bf6f78"},"source":"First we import the packages."},{"cell_type":"code","metadata":{"_cell_guid":"71872a3b-0d5f-4cba-87a4-feaaac494c92","collapsed":true,"_uuid":"fdc4e2d9c13e448293ccfc5a7672bc9300b180c5"},"outputs":[],"execution_count":null,"source":"import pandas as pd\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom nltk.corpus import stopwords\nfrom sklearn import preprocessing\nfrom sklearn.naive_bayes import MultinomialNB"},{"cell_type":"markdown","metadata":{"_cell_guid":"e5222db3-2d32-47b5-9473-8c571566d0fc","_uuid":"4c6866471ab64f2de0997fe2801918b0a9398f3c"},"source":"Next we read in the data."},{"cell_type":"code","metadata":{"_cell_guid":"07193da6-75c0-4fe4-afd5-e5e7bb231fcd","_uuid":"07c1f7bb08d7690c166a2c44a99497169f792656"},"outputs":[],"execution_count":null,"source":"train = pd.read_csv('../input/train.csv')\ntest = pd.read_csv('../input/test.csv')\nsample = pd.read_csv('../input/sample_submission.csv')\ntrain.head(3)"},{"cell_type":"markdown","metadata":{"_cell_guid":"c597077a-248d-4f80-818d-f48c8fb9ff54","_uuid":"d20339c22aaf6a44d54e96b403410fe8acb50ac8"},"source":"And we want the labels in the right format. They are placed into an array taking values of 0, 1 or 2 (indicating the 3 authors)."},{"cell_type":"code","metadata":{"_cell_guid":"84fa5c3e-7383-4c3b-ab9a-2d0117d3c775","collapsed":true,"_uuid":"96105ae222371d4aa30a101817aedd0a233671af"},"outputs":[],"execution_count":null,"source":"lbl_enc = preprocessing.LabelEncoder()\ny = lbl_enc.fit_transform(train.author.values)"},{"cell_type":"markdown","metadata":{"_cell_guid":"a0c5ed5d-7354-41c5-9486-5a864c572e13","_uuid":"eab465ef53d274a83b353b710fc97344901e28f8"},"source":" ## Preperation"},{"cell_type":"markdown","metadata":{"_cell_guid":"87c904a2-a6d0-4215-a199-9ca62c9de0a8","_uuid":"ff60225fb864210a7b3c7dac7822fed64024fdb4"},"source":"We create a matrix of counts. Each line represents a passage of text. The columns are populated with every word, bigram and trigram (stopwords are removed)."},{"cell_type":"code","metadata":{"_cell_guid":"e9e77dd8-79e8-4d11-879e-8bf44c3624b3","collapsed":true,"_uuid":"24ec3597f6bd910212e50cc68060b7d057ac611d"},"outputs":[],"execution_count":null,"source":"ctv = CountVectorizer(analyzer='word',token_pattern=r'\\w{1,}',\n            ngram_range=(1, 3), stop_words = 'english')"},{"cell_type":"markdown","metadata":{"_cell_guid":"f86be13e-e31b-4529-9b0a-a46755514d57","_uuid":"136756233950bf6de4c6573e590935183610c414"},"source":"A reminder that this is what a count vector looks like. [Credit goes here](https://medium.com/deep-math-machine-learning-ai/chapter-9-1-nlp-word-vectors-d51bff9628c1)\n![](https://cdn-images-1.medium.com/max/800/1*YXy_Txtmtttw85Vv05JdRQ.jpeg)"},{"cell_type":"markdown","metadata":{"_cell_guid":"fdb573d3-989c-4b4e-8660-27ff431cedb5","_uuid":"ce6c07fb8e408d4134ba02135dd3e387939a31e3"},"source":"## Model"},{"cell_type":"markdown","metadata":{"_cell_guid":"6e72ea68-142c-4e67-b922-b928ee9646d6","_uuid":"35306cf027278466468ec240237489337317ea59"},"source":"We will fit a Naive Bayes model on the counts. We already know this is effective we will fit on all the train data, predict on the validation data and send to  the leaderboard. <br> <br>\nLets not do this blind. We want to know more about this simple but effective method. Lets start by heading to the [Naive Bayes wikipedia page](https://en.wikipedia.org/wiki/Naive_Bayes_classifier)\n\n### Naive Bayes Wikipedia\n\nIn machine learning, naive Bayes classifiers are a family of simple probabilistic classifiers based on applying Bayes' theorem with strong (naive) independence assumptions between the features.\n\nNaive Bayes has been studied extensively since the 1950s. It was introduced under a different name into the text retrieval community in the early 1960s,[1]:488 and remains a popular (baseline) method for text categorization, the problem of judging documents as belonging to one category or the other (such as spam or legitimate, sports or politics, etc.) with word frequencies as the features. With appropriate pre-processing, it is competitive in this domain with more advanced methods including support vector machines.[2] It also finds application in automatic medical diagnosis.[3]\n\nNaive Bayes classifiers are highly scalable, requiring a number of parameters linear in the number of variables (features/predictors) in a learning problem. Maximum-likelihood training can be done by evaluating a closed-form expression,[1]:718 which takes linear time, rather than by expensive iterative approximation as used for many other types of classifiers.\n\nIn the statistics and computer science literature, naive Bayes models are known under a variety of names, including simple Bayes and independence Bayes.[4] All these names reference the use of Bayes' theorem in the classifier's decision rule, but naive Bayes is not (necessarily) a Bayesian method.[1][4]"},{"cell_type":"markdown","metadata":{"_cell_guid":"f2e8d7c9-0f1b-4fe2-bb80-d885e9f7dac6","_uuid":"cdaf7d6bfee20203af0150f0ea9bbe8b2e0131a0"},"source":"Okay so we've got a rough idea but lets try and get our heads around this a bit better. Who do we go to when we want to learn about NLP? It has to be the Lionel Messi of NLP - Dan Jurafsky!\n<img src=\"http://ichef.bbci.co.uk/onesport/cps/480/mcs/media/images/65764000/jpg/_65764217_messi-getty.jpg\" alt=\"Drawing\" style=\"width: 300px;\"/>\n<img src=\"https://web.stanford.edu/~jurafsky/danfall13.jpg\" alt=\"Drawing\" style=\"width: 200px;\"/>"},{"cell_type":"markdown","metadata":{"_cell_guid":"348c8d75-bbf2-40ff-8088-be76d5a49cd6","_uuid":"597533311ffe1a1b45535439733be277c78a8e03"},"source":"And how will we learn most effectively? A worked example... <br>\n<img src=\"https://i.ytimg.com/vi/pc36aYTP44o/hqdefault.jpg\" alt=\"Drawing\" style=\"width: 400px;\"/>\n<br> head over to [youtube](https://www.youtube.com/watch?v=pc36aYTP44o) to watch this one"},{"cell_type":"markdown","metadata":{"_cell_guid":"48ce1867-87aa-4e63-bc0d-a53976fd98f9","_uuid":"fc15a96c83dbe93003f7cd720d401fdf6dcb1cb1"},"source":"We are now ready to run the model. It takes about a second and is just a few lines of code."},{"cell_type":"code","metadata":{"_cell_guid":"00183d5d-b087-4f2a-9185-2f65ff341706","_uuid":"05355254b245a6e49d718eed3799be46945c9c8e"},"outputs":[],"execution_count":null,"source":"xtrain_ctv_all=ctv.fit_transform(train.text.values)\nxtest_ctv_all=ctv.transform(test.text.values)\nclf = MultinomialNB(alpha=1.0)\nclf.fit(xtrain_ctv_all, y)"},{"cell_type":"markdown","metadata":{"_cell_guid":"21e7d1a1-ddfd-4360-8167-731da77e4d18","_uuid":"3313d11d640d9a6c13825382e5759f931b3aafee"},"source":"## Submit"},{"cell_type":"code","metadata":{"_cell_guid":"2e2e14eb-01a2-4196-b79a-6b1137bee45b","_uuid":"3486e5ee5c9a3b5b59e0c47fdfe608774f9ff5b9"},"outputs":[],"execution_count":null,"source":"sub = pd.DataFrame(clf.predict_proba(xtest_ctv_all), columns=[\"EAP\",\"HPL\",\"MWS\"],)\nsub[\"id\"] = test.id\ncols = sub.columns.tolist()\nsub = sub[cols[-1:] + cols[:-1]]\nsub.to_csv(\"simple_spooky_sub.csv\")\nsub.head()"},{"cell_type":"markdown","metadata":{"_cell_guid":"91a0e3ed-403c-40aa-8ed0-2e241075d897","_uuid":"baf056074d38e9b51a558466130aec7fa70a18b2"},"source":"![](https://i.imgur.com/XJyemeI.jpg)"},{"cell_type":"markdown","metadata":{"_cell_guid":"0580e88a-a508-455a-b918-8ebfbeba6fc2","_uuid":"507346675995c6a1941742421d4c37280f171278"},"source":"## Conclusion"},{"cell_type":"markdown","metadata":{"_cell_guid":"fa20323b-05fd-4a9a-85af-fd61a3c8dd9f","_uuid":"2a3018c2aff02c5246693603522d79ab866ac4c9"},"source":"* A simple Naive Bayes model can perform almost as well as complex and time consuming neural network based methods.\n\n* Getting to the top half of the leaderboard with a few lines of code within a few seconds is achievable.\n\n* To boost score for minimal resource effort the inclusion of simple features, parameter tuning and considering simple ensemble methods would be a sensible next step."},{"cell_type":"markdown","metadata":{"_cell_guid":"aa26df10-5edd-47ce-b87d-ea942c46aeba","_uuid":"1b009496a32c144bcc2ba169ff3524f83fd9f425"},"source":"## Challenge!!!"},{"cell_type":"markdown","metadata":{"_cell_guid":"d564fe5b-f82a-4e0c-84f4-5eaed62e8ff0","_uuid":"b4f55cc6ff1fe6f185cc9f48cfd094f7a4bad518"},"source":"Can you produce a higher score with less than 20 lines of codes that runs in under 10 seconds? Please share if you can. It would be great to see."},{"cell_type":"markdown","metadata":{"_cell_guid":"8eeb1faa-f818-4373-a69f-e65f0d59d4d9","_uuid":"95e1be7aa346fd48b727f4317db8883a79a88113"},"source":"## Resources"},{"cell_type":"markdown","metadata":{"_cell_guid":"05816b75-6663-4fe9-a9c2-9da151f5ad2e","_uuid":"ebc0b5018c75d0a6d0fbe35a0408a2c5ecd772b1"},"source":"[Countvectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)\n\n[Naive Bayes](http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html#sklearn.naive_bayes.MultinomialNB)\n\n[Glove embeddings](https://nlp.stanford.edu/projects/glove/)"},{"cell_type":"markdown","metadata":{"_cell_guid":"c09b950c-0f60-412d-a1ca-b3062443fe5f","_uuid":"c5d16170bc2111de6af69dd04f508b67204a94c6"},"source":"## Parameters"},{"cell_type":"markdown","metadata":{"_cell_guid":"684967b7-4454-4264-bd98-292c2e5ec562","_uuid":"f83ae5b1817edcc2682ed522332c989efa260df8"},"source":"\n**TF-IDF Logistic Regression**\tmin doc freq=3, ngram range 1-3, stopwords=nltk English <br> <br>\n**Count Vectorizer Logistic Regression**\tmin doc freq=3, ngram range 1-3, stopwords=nltk English <br> <br>\n**TF-IDF Vectorizer Multinomial Naive Bayes** Default <br> <br>\n**Count Vectorizer Multinomial Naive Bayes** Default <br> <br>\n**SVM on SVD of TFIDF**\tDefault (Probability Estimates enabled) <br> <br>\n**TFIDF Boosted Trees**\t200 boosted trees, max depth=7, subsample=0.8, colsample by tree=0.8, learning rate=0.1 <br> <br>\n**Boosted Trees on Counts**\t200 boosted trees, max depth=7, subsample=0.8, colsample by tree=0.8, learning rate=0.1 <br> <br>\n**Boosted Trees on TFIDF (with SVD)**\t200 boosted trees, max depth=7, subsample=0.8, colsample by tree=0.8, learning rate=0.1 <br> <br>\n**Boosted Trees on Counts (with SVD)**\t200 boosted trees, max depth=7, subsample=0.8, colsample by tree=0.8, learning rate=0.1 <br> <br>\n**Default Boosted Trees on Glove Embeddings** Defaults <br> <br>\n**Boosted Trees on Glove Embeddings**\t200 boosted trees, max depth=7, subsample=0.8, colsample by tree=0.8, learning rate=0.1 <br> <br>\n**3 layer sequential NN on Glove (5 epochs)**\t300 Dense layer - 0.2 dropout - 300 dense layer - 0.3 dropout - batch normailsation - softmax to 3 <br> <br>\n**LSTM with Glove**\t300 embedding - 0.3 spacial dropout - 300 LSTM (with 0.3 dropout and 0.3 recurrent dropout) - 1024 dense - 0.8 dropout - 1024 dense - 0.8 dropout - softmax to 3 <br> <br>\n**Bidirectional LSTM with Glove**\t300 embedding - 0.3 spacial dropout - 300 bidirectional LSTM (with 0.3 dropout and 0.3 recurrent dropout) - 1024 dense - 0.8 dropout - 1024 dense - 0.8 dropout - softmax to 3 <br> <br>\n**GRU with Glove**\t300 embedding - 0.3 spacial dropout - 300 GRU (with 0.3 dropout and 0.3 recurrent dropout) - 300 GRU (with 0.3 dropout and 0.3 recurrent dropout) - 1024 dense - 0.8 dropout - 1024 dense - 0.8 dropout - softmax to 3 <br>\n"},{"cell_type":"code","metadata":{"_cell_guid":"a6380d52-d282-4ef9-adf9-a03292eb3236","collapsed":true,"_uuid":"56e6c64b824a7f47865865336e40645da26c0c64"},"outputs":[],"execution_count":null,"source":""}],"metadata":{"language_info":{"codemirror_mode":{"name":"ipython","version":3},"version":"3.6.3","pygments_lexer":"ipython3","file_extension":".py","mimetype":"text/x-python","nbconvert_exporter":"python","name":"python"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}}}