{"cells":[{"metadata":{},"cell_type":"markdown","source":"*This kernel based on dbaghern`s kernel [\"A Deep Dive Into Sklearn Pipelines\"](https://www.kaggle.com/baghern/a-deep-dive-into-sklearn-pipelines)*<br>\n*I made another DataFrameSelector class because in the original kernel you should select features one by one manually. Also, I changed some text descriptions, where it was necessary.*"},{"metadata":{"_uuid":"125ba73bccf909bf24a1cfee0d6c0bd772d95d06","_cell_guid":"b23858a7-8f9f-4fb7-bfcf-0f1c9a2d5bba"},"cell_type":"markdown","source":"Once you've gotten your feet wet in basic sklearn modeling, you might find yourself doing the same few steps over and over again in the same anaysis. To get to the next level, pipelines are your friend!\n\nPipelines are a way to streamline a lot of the routine processes, encapsulating little pieces of logic into one function call, which makes it easier to actually do modeling instead just writing a bunch of code. Pipelines allow for experiments, and for a dataset like this that only has the text as a feature, you're going to need to do a lot of experiments. Plus, when your modeling gets really complicated, it's sometimes hard to see if you have any data leakage hiding somewhere. Pipelines are set up with the fit/transform/predict functionality, so you can fit a whole pipeline to the training data and transform to the test data, without having to do it individually for each thing you do. Super convenienent, right??\n\nThis notebook is going to break down the pipeline process to make it easier to see how they all fit together. While not exhaustive, it should get you started on building your own pipelines so you can spend more time on the good stuff, thinking.\n\nBut first, we get the data:"},{"metadata":{"_uuid":"282f761b71584453b7c57203edfcc8677d9738a0","_cell_guid":"0289247b-e183-4b83-9a4a-777c1ccd2cbc","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\ndf = pd.read_csv('../input/train.csv')\n\ndf.dropna(axis=0)\ndf.set_index('id', inplace = True)\n\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2686ff0796119ece5ada79c8a9a3c35d323e9420","_cell_guid":"9d72d93c-d319-4d4e-ad04-ee0d153f7fb3"},"cell_type":"markdown","source":"## Preprocessing and Feature Engineering\n\nTo begin, let's do some basic feature engineering. To make it easier to replicate on the submission data, we will encapsulate the logic into a function.\n\nNote, all of this preprocessing is standard stuff, and does not depend on the data it's processing on, so it's ok to do this now. Things like count vectorization and numeric scaling depend on the data it's run on, so that part must be done differently. We will get to that later.\n\nFor now, we will count the number of words in each row, the number of characters, the number of non stop words, and the number of commas, since who knows, maybe using commas helps build suspense??"},{"metadata":{"_uuid":"2596bb126e8ee018a6940ae31c8aa89cc0d84e8d","_cell_guid":"63e391e6-e238-454f-b93c-b2f249d16efc","trusted":true},"cell_type":"code","source":"import re\nfrom nltk.corpus import stopwords\n\nstopWords = set(stopwords.words('english'))\n\n#creating a function to encapsulate preprocessing, to mkae it easy to replicate on  submission data\ndef processing(df):\n    #lowering and removing punctuation\n    df['processed'] = df['text'].apply(lambda x: re.sub(r'[^\\w\\s]','', x.lower()))\n    \n    #numerical feature engineering\n    #total length of sentence\n    df['length'] = df['processed'].apply(lambda x: len(x))\n    #get number of words\n    df['words'] = df['processed'].apply(lambda x: len(x.split(' ')))\n    df['words_not_stopword'] = df['processed'].apply(lambda x: len([t for t in x.split(' ') if t not in stopWords]))\n    #get the average word length\n    df['avg_word_length'] = df['processed'].apply(lambda x: np.mean([len(t) for t in x.split(' ') if t not in stopWords]) if len([len(t) for t in x.split(' ') if t not in stopWords]) > 0 else 0)\n    #get the average word length\n    df['commas'] = df['text'].apply(lambda x: x.count(','))\n\n    return(df)\n\ndf = processing(df)\n\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e477b8cbf9fc8a15c9f4aa13908314350caaa95b","_cell_guid":"9f65b065-59e9-42ea-a76e-9a38a9e69109"},"cell_type":"markdown","source":"### Creating a Pipeline\n\nSklearn's pipeline functionality makes it easier to repeat commonly occuring steps in your modeling process. Similar to the processing function I made above, it provides a way to take code, fit it to the training data, apply it to the test data without having to copy and paste everything.\n\nSuper easy, but I find the documentation a little hard to piece through. So let's build the pipelines up from the bottom. Plus, since pipelines are made from pipelines, it's useful to see how they build on each other.\n\nFirst step, split your data into training and testing."},{"metadata":{"_uuid":"23b0a8d116caf454325f63af4b108e74023ca8de","_cell_guid":"27efeecb-8feb-41c1-96d8-7bc21b452e7b","trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nfeatures= [c for c in df.columns.values if c  not in ['id','text','author']]\nnumeric_features= [c for c in df.columns.values if c  not in ['id','text','author','processed']]\ntarget = 'author'\n\nX_train, X_test, y_train, y_test = train_test_split(df[features], df[target], test_size=0.33, random_state=42)\nprint('X_train size: {}, X_test size: {}'.format(X_train.shape, X_test.shape))\nX_train.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"74efdcc9ffe37aac7901fbb63e1a1feede3602cb","_cell_guid":"b712489b-156e-48bb-a2a2-53eac0060cc9"},"cell_type":"markdown","source":"Now for the tricky parts.\n\nFirst thing I want to do is define how to process my variables. The standard preprocessing apply the same preprocessing to the whole dataset, but in cases where you have heterogeneous data, this doesn't quite work. So first thing I'm going to do is create a selector transformer that simply returns the one column in the dataset by the key values I pass. "},{"metadata":{"_uuid":"2d8983e1d86a9d1323b0bde3083c6fe2e2650378","_cell_guid":"66f0363d-1414-4d23-87fa-a0190c0f6a3a","trusted":true},"cell_type":"code","source":"from sklearn.base import BaseEstimator, TransformerMixin\n\nclass DataFrameSelector(BaseEstimator, TransformerMixin):\n    \"\"\"\n     Transformer to select a part of the data frame to perform additional transformations on\n     \"\"\"\n    def __init__(self, attribute_names):\n        self.attribute_names = attribute_names\n    \n    def fit(self, X, y = None):\n        return self\n    \n    def transform(self, X):\n        return X[self.attribute_names]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"str_feature = 'processed'\nnum_features_list = ['length', 'words', 'words_not_stopword', 'avg_word_length', 'commas']","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f0cbfcf81d77373d88bd24522020c8469abeb9a0","_cell_guid":"b6a864df-d1f5-4199-aba6-658c6cce2c49"},"cell_type":"markdown","source":"To see how this is used, let's actually run it on one column.\n\nI'm going to call it on the text column and transform it with another step. But again, pipelines are all about encapsulating several steps, so I'm going to make a mini pipeline that consists of two steps: first grab just that column from the dataset, then perform tf-idf on just that column and return the results.\n\nTo make a pipeline, just pass an array of tuples of the format (name, object). The first part is the name of the action, and the second is the actual object. So this pipeline consists of \"selecting\" and then \"tfidf-ing\" a column.\n\nTo execute, use it just like any other transformer. You can call text.fit() to fit to training data, text.transform() to apply it to training data, or text.fit_transform() to do both. \n\nSince it's text, it will return a sparse matrix, but we can see that it works:"},{"metadata":{"_uuid":"2caac353c114eb6cc7e493eecfa90639cadb5e84","_cell_guid":"55de63e6-8d2f-478f-9126-6e7b0546207c","trusted":true},"cell_type":"code","source":"from sklearn.pipeline import Pipeline\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\ntext = Pipeline([\n                ('selector', DataFrameSelector(str_feature)),\n                ('tfidf', TfidfVectorizer( stop_words='english'))\n            ])\n\ntext.fit_transform(X_train)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"17e07518d728d90fe52483eeec70b7bdda9b8c12","_cell_guid":"dec38474-3a0a-45d3-a52f-f99dd72f0f2a"},"cell_type":"markdown","source":"Since our data is heterogeneous, we might want to do something else on numeric data, so let's build a mini pipeline for that too.\n\nThis transformer will be a simple scaler. Just like the text one, we combine two steps, first selecting the column, then transforming the column, like so:"},{"metadata":{"_uuid":"eb59242f760520108b9b8f73c108f584af3130f0","_cell_guid":"ea77456b-f7cc-4480-97a5-4b4c819de7a1","trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\n\nnum_feats = Pipeline([\n    ('selector', DataFrameSelector(num_features_list)),\n    ('standard', StandardScaler())\n])\n\nnum_feats.fit_transform(X_train)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2a709500025294ef8a4cfda02f7a7d02a96ff61b","_cell_guid":"150d3ce6-9fed-4ce0-a3e9-8f955ad7874d"},"cell_type":"markdown","source":"To make a pipeline from all of our pipelines, we do the same thing, but now we use a FeatureUnion to join the feature processing pipelines.\n\nThe syntax is the same as a regular pipeline, it's just an array of tuple, with the (name, object) format. \n\nThe feature union itself is not a pipeline, it's just a union, so you need to do *one more step* to make it useable: pass it to a pipeline, with the same structure, an array of tuples, with the simple (name, object) format. . As you can see, we get a pipeline-ception going on the more complex you get! \n\nYou can then apply all those transformations at once with a single fit, transform, or fit_transform call. Nice, right?"},{"metadata":{"_uuid":"65f49fd19c57ba619518b2a0de615e08cb4f3c5f","_cell_guid":"95d16585-367f-4e74-98fd-bd266e19a672","trusted":true},"cell_type":"code","source":"from sklearn.pipeline import FeatureUnion\n\nfeats = FeatureUnion([\n    ('text', text),\n    ('num_feats', num_feats)\n])\n\nfeature_processing = Pipeline([('feats', feats)])\nfeature_processing.fit_transform(X_train)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c1e3d4b661d5a48ee886ddbee613e384f9525d94","_cell_guid":"7e759f03-b2fd-4dcc-893e-ab00bbe0a128"},"cell_type":"markdown","source":"To add a model to the mix and generate predictions as well, you can add a model at the end of the pipeline. The syntax is, you guessed it, an array of tuples, merging the transformations with a model. \n\nWe can see the raw accuracy is at 63%. Not bad for a start.\n"},{"metadata":{"_uuid":"acaeeee80c58268f3b21d9d2c1cd7a9a3b5fcd21","_cell_guid":"84b4ce5d-c4a3-46f7-903f-c4c2d7518ecd","trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n\npipeline = Pipeline([\n    ('features',feats),\n    ('classifier', RandomForestClassifier(random_state = 42)),\n])\n\npipeline.fit(X_train, y_train)\n\npreds = pipeline.predict(X_test)\nnp.mean(preds == y_test)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"eb832df858b4a2e078857de3dc9fc8e302c41bf6","_cell_guid":"637a6944-320b-4d17-8623-2baa449b2903"},"cell_type":"markdown","source":"# Bringing It All Together\n\nPreprocessing is great, but most likely, you actually want to model something too. To do that, we just need to add a classifier at the end of the pipeline. Here I'm going to make a pipeline that does all the processing I made above, then passes it to a Random Forest. As you might guess, this requires passing an array of tuples to the pipeline, with the (name, object) structure. \n\nTo put it to use, we just fit and predict as if it was a regular classifier. First we fit on the training, then predict on the test data, and then see how well it did:"},{"metadata":{"_uuid":"66c104071c5e7bab167da53c4747610f6e568a39","_cell_guid":"10444a55-f32b-41cd-8dbd-c1ea63b4c9cb"},"cell_type":"markdown","source":"# Cross Validation To Find The Best Pipeline\n\nThat alone should give you enough flexibility to create some rather complex pipelines. But we're on a role, let's keep going.\n\nWhat if I wanted to do cross validation on my pipeline? How many trees should I use on my classifier? How deep should I go? Or even more complicated, how many words should I use in my tf-idf transform? Should I include stop words? Pipelines allow you to do that with just a few more lines.\n\nCross validation is all about figuring out what the best hyperparameters of the data set is. To see the list of all the possible things you could fine tune, call get_params().keys() on your pipeline."},{"metadata":{"_uuid":"c0bfe59aaf80d0aacf57eced7fad9252a24a6a4c","_cell_guid":"6ce23a11-a9e8-455e-857e-249615b89066","trusted":true},"cell_type":"code","source":"pipeline.get_params().keys()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"309d37e42e11abfdddc127acebe8d2bcb2b452df","_cell_guid":"dbfb2342-2817-4ced-a609-9938562e01c5"},"cell_type":"markdown","source":"Obviously don't be crazy, cross validation takes a while to run, and the more options you select, the longer it takes. But to give you an idea on how these work together, to test out the different combinations, define a dictionary with the settings you want, with the key being the pipeline's parameter key name, and the value being an array of all the settings you want to apply.\n\nAfter the dictionary is made, call GridSearchCV on your pipeline, passing the dictionary and the number of folds you want to use. \n\nHere's an example with a few settings on 5 fold cross validation."},{"metadata":{"_uuid":"7d2b420ef99bf024a2bd057f97538ab471674fd9","_cell_guid":"602a1465-ff99-44f2-9d4d-4b6847afba2e","trusted":true},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\n\nhyperparameters = { 'features__text__tfidf__max_df': [0.9, 0.95],\n                    'features__text__tfidf__ngram_range': [(1,1), (1,2)],\n                   'classifier__max_depth': [50, 70],\n                    'classifier__min_samples_leaf': [1,2]\n                  }\nclf = GridSearchCV(pipeline, hyperparameters, cv=5)\n \n# Fit and tune model\nclf.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9ad824ce203554cdda13502640f80e8d2c4ddc9d","_cell_guid":"16c7c9d1-4c17-4619-a2f1-138de837e1cf"},"cell_type":"markdown","source":"If you want to see which settings won, you can do so:"},{"metadata":{"_uuid":"bc5e2c7551604acbb3deb49fa662f21710268bf7","_cell_guid":"d1ee7fb2-c946-41b9-adbb-3ac449b04f7e","trusted":true},"cell_type":"code","source":"clf.best_params_","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"db2f77237f9026de72b29384a870a7d1adab5cf3","_cell_guid":"0b2899ab-ca1e-4814-a81c-258aa28224cf"},"cell_type":"markdown","source":"What's really convenient is you can call refit to automatically fit the pipeline on all of the training data with the best_params_setting applied!\n\nThen applying it to the test data is the same as before.\n\nNot much of an improvement, but at least now we can go back and easily change out the individual pieces."},{"metadata":{"_uuid":"d9eba0d0ee5dce9f777c633881bcdd19eda6c9c3","_cell_guid":"23ed5f54-0d58-4a2d-ad7a-ca8d80f5574c","trusted":true},"cell_type":"code","source":"#refitting on entire training data using best settings\nclf.refit\n\npreds = clf.predict(X_test)\nprobs = clf.predict_proba(X_test)\n\nnp.mean(preds == y_test)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f3e4177bc191ee0d529bf84ef60fe89c34b4f6bd","_cell_guid":"2f6094d1-2f14-4188-930a-d67200f49738"},"cell_type":"markdown","source":"# Final Predictions\n\nTo generate submission results, you just need to do the preprocessing on the submission data, then call the pipeline with the predict_proba call, since we want to know all the probabilities, not just the label.\n\nThe only tricky part for the submission is we need the class names as the column values. To access it, you must call clf.best_estimator_.named_steps['classifier'].classes_"},{"metadata":{"_uuid":"1b7038b1d5a30479213799839531867afa91da5e","_cell_guid":"7e8526e3-f855-4ace-b6fb-2e00dbdf8d0b","trusted":true},"cell_type":"code","source":"submission = pd.read_csv('../input/test.csv')\n\n#preprocessing\nsubmission = processing(submission)\npredictions = clf.predict_proba(submission)\n\npreds = pd.DataFrame(data=predictions, columns = clf.best_estimator_.named_steps['classifier'].classes_)\n\n#generating a submission file\nresult = pd.concat([submission[['id']], preds], axis=1)\nresult.set_index('id', inplace = True)\nresult.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6eccc90e40159af2fca19f36fd1607a9eef39b62","_cell_guid":"ead5d354-05bd-48fe-b8d4-f72f1ad86d5e"},"cell_type":"markdown","source":"### Wrapping Up\n\nI hope this helps shed some light on the inner workings of pipelines. Using pipelines effectively can really help elevate you to the next level of data scientist, so once you've mastered the algorithms themselves, I strongly recommend mastering pipelines as well! You can even create a pipeline to test out different classifiers and pick the best one too! It's one step closer to automating away the boring stuff, letting you focus on what matters, the creativity and feature engineering.\n\nMore to come, so stay tuned!"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"version":3,"name":"ipython"},"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.3","mimetype":"text/x-python","file_extension":".py","name":"python"}},"nbformat":4,"nbformat_minor":1}