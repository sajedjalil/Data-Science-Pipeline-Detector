{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\n# Python Stuff\nimport numpy as np\nimport pandas as pd\nimport zipfile\nimport os\nimport gc\nimport sys\nimport string\nfrom collections import defaultdict, Counter\nimport urllib.request\nimport os.path\n\n# Visualization Stuff\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\npd.set_option('display.max_colwidth', 240)\n\n# Statistics Stuff\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.linear_model import LogisticRegression\nimport scipy.stats as stats\n\n# NLP Stuff\nif 'transformers' not in sys.modules:\n  !pip install transformers\nimport transformers\nfrom transformers import DistilBertTokenizer, DistilBertModel\nif 'nltk' not in sys.modules:\n  !pip install nltk\nimport nltk\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\n\n# Neural Networks Stuff\nimport torch\nfrom torch import nn, optim\nfrom torch.utils import data\n\nis_colab = 'google.colab' in sys.modules\nif is_colab:\n    from google.colab import drive\n\nis_kaggle = 'kaggle' in os.getcwd()\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\ndevice\nif torch.cuda.is_available():\n    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n    print('We will use the GPU:', torch.cuda.get_device_name(0))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Spooky Authors Distribution Analysis","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<a id=\"toc\"></a>\n## Table of Contents\n1. [Introduction](#introduction)\n1. [Data Preparation](#preparation)\n1. [Data Exploration and Visualizatin](#exploration)\n1. [Embeddings](#embeddings)\n1. [Classification](#classification)\n1. [Text Generation](#creating_poe)\n1. [Conclusions](#conclusions)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<a id=\"introduction\"></a>\n\n## Introduction\n\nThis notebook contains a statistical analysis of excerpts from horror stories by Edgar Allan Poe, Mary Shelley, and HP Lovecraft.\n\nThe notebook is avaible on [Kaggle](https://www.kaggle.com/odedgolden/spooky-authors-analysis/)\n\n<table><tr><td><img src='https://upload.wikimedia.org/wikipedia/commons/9/97/Edgar_Allan_Poe%2C_circa_1849%2C_restored%2C_squared_off.jpg', width=\"200\"></td>\n    <td><img src='https://upload.wikimedia.org/wikipedia/commons/6/65/RothwellMaryShelley.jpg', width=\"230\"></td>\n    <td><img src='https://upload.wikimedia.org/wikipedia/commons/1/10/H._P._Lovecraft%2C_June_1934.jpg', width=\"230\"></td>\n    </tr></table>\n\nBefore we start with statistics, let's get a better context for our authors.\n\n### Edgar Allan Poe:\nEdgar Allan Poe (January 19, 1809 – October 7, 1849) was an American writer, poet, editor, and literary critic. Poe is best known for his poetry and short stories, particularly his tales of mystery and the macabre. He is widely regarded as a central figure of Romanticism in the United States and of American literature as a whole, and he was one of the country's earliest practitioners of the short story. He is also generally considered the inventor of the detective fiction genre and is further credited with contributing to the emerging genre of science fiction.Poe was the first well-known American writer to earn a living through writing alone, resulting in a financially difficult life and career. (Wikipedia)\n\n### Mary Shelley:\nMary Wollstonecraft Shelley (30 August 1797 – 1 February 1851) was an English novelist who wrote the Gothic novel Frankenstein; or, The Modern Prometheus (1818). She also edited and promoted the works of her husband, the Romantic poet and philosopher Percy Bysshe Shelley. Her father was the political philosopher William Godwin and her mother was the philosopher and feminist Mary Wollstonecraft. (Wikipedia)\n\n###  H.P. Lovecraft:\nHoward Phillips Lovecraft (August 20, 1890 – March 15, 1937) was an American writer of weird fiction and horror fiction, who is known for his creation of what became the Cthulhu Mythos.\nBorn in Providence, Rhode Island, Lovecraft spent most of his life in New England. He was born into affluence, but the family's wealth dissipated soon after the death of his grandfather. In 1913, he wrote a critical letter to a pulp magazine that ultimately led to his involvement in pulp fiction. During the interwar period, he wrote and published stories that focused on his interpretation of humanity's place in the universe. In his view, humanity was an unimportant part of an uncaring cosmos that could be swept away at any moment. These stories also included fantastic elements that represented the perceived fragility of anthropocentrism. (Wikipedia)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Hypothesis:\n\n\nI assume that the three authors do have different ”style” and therefore expect the following:\n1. Statistically significant differences between the authors. \n2. Good prediction for classification task. \n3. Somewhat entertaining yet sensible simulated excerpts from each author.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<a id=\"preparation\"></a>\n## Data Preparation\n\n> “Creative minds are uneven, and the best of fabrics have their dull spots.” - H.P. Lovecraft","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"First, let's take a look at the data - we have three dataframes - train, test and the sample submission.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Download the data if needed:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"if is_colab:\n  drive.mount('/content/drive')\n  DIR_NAME = '/content/drive/My Drive/Colab Notebooks/Spooky/data/'\nelif is_kaggle:\n    DIR_NAME = \"/kaggle/input/spooky-author-identification/\"\n    zips = os.listdir(DIR_NAME)\n    for name in zips:\n        with zipfile.ZipFile(DIR_NAME + name, 'r') as zip_ref:\n            zip_ref.extractall(\".\")\nelse:\n    fnames = [r'./train.csv',r'./test.csv', r'./sample_submission.csv']\n    url = 'https://drive.google.com/drive/folders/1tP8T8_-6Xy5BgQa3q2g7reFHBv4Uer_a?usp=sharing'\n\n    if not os.path.exists(fnames[0]):\n        for fname in fnames:\n            urllib.request.urlretrieve(url, fname)\n    print(fname, 'exists:', os.path.exists(fname))\n    DIR_NAME = './' ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Read the data to dataframes:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"if is_colab:\n  train = pd.read_csv(DIR_NAME+'train.csv')\n  test = pd.read_csv(DIR_NAME+'test.csv')\n  sample = pd.read_csv(DIR_NAME+'sample_submission.csv')\nelse:\n  train = pd.read_csv('./train.csv')\n  test = pd.read_csv('./test.csv')\n  sample = pd.read_csv('./sample_submission.csv')\ntrain.head(3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Initiate BERT Model","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"The BERT model was proposed in [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805) by Jacob Devlin, Ming-Wei Chang, Kenton Lee and Kristina Toutanova. It’s a bidirectional transformer pre-trained using a combination of masked language modeling objective and next sentence prediction on a large corpus comprising the Toronto Book Corpus and Wikipedia.\n\nIt will be very helpfull to use the pretrained BERT model, in order to get the tokenizer and the word embeddings.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"PRE_TRAINED_MODEL_NAME = 'distilbert-base-cased'\ntokenizer = DistilBertTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)\nbert_model = DistilBertModel.from_pretrained(PRE_TRAINED_MODEL_NAME)\n\nbert_embeddings = bert_model.get_input_embeddings()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's look at an example of the tokenizing:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_sentence = \"Creative minds are uneven, and the best of fabrics have their dull spots.\"\ntokens = tokenizer.tokenize(sample_sentence)\ntoken_ids = tokenizer.convert_tokens_to_ids(tokens)\nencoded = tokenizer.encode(sample_sentence)\nprint(tokens)\nprint(token_ids)\nprint(encoded)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I now want to create some metadata, specifically - the word count and stopword count for each excerpt.\n\nI will do it by applying the BERT tokenizer.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train['word_ids'] = train['text'].apply(lambda x: tokenizer.convert_tokens_to_ids(tokenizer.tokenize(x)))\ntrain['word_count'] = train['word_ids'].apply(lambda x: len(x))\n\neng_stopword_toekns = set([tokenizer.convert_tokens_to_ids(str(x)) for x in stopwords.words(\"english\")])\ntrain['stopword_count'] = train['word_ids'].apply(lambda x: len([token for token in x if token in eng_stopword_toekns]))\n\ntrain.head(1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I will first calculate word counts for each of the authors, in order to understand their words distributions.\n\nI suspect that every author may use a slightly different vocabulary.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def count_words(df, author_key):\n    df = df[df['author']==author_key]\n    word_count = df['text'].str.split(expand=True).stack().value_counts()\n    return word_count","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"word_count_EAP = count_words(train, 'EAP')\nword_count_HPL = count_words(train, 'HPL')\nword_count_MWS = count_words(train, 'MWS')\nprint(f'word_count_EAP size: {len(word_count_EAP)}')\nprint(f'word_count_HPL size: {len(word_count_HPL)}')\nprint(f'word_count_MWS size: {len(word_count_MWS)}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After counting the words for each author, I now want to subtract all the stopwords, which are not unique in term of vocabulary.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"english_stopwords = stopwords.words(\"english\") + list(string.punctuation)\n\nword_count_EAP = word_count_EAP.drop(labels=english_stopwords, errors='ignore')\nword_count_HPL = word_count_HPL.drop(labels=english_stopwords, errors='ignore')\nword_count_MWS = word_count_MWS.drop(labels=english_stopwords, errors='ignore')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"However, I am also only interested in the differences, so I will also drop all the common words (words that appear on each and every author in it's top COMMON_WORDS_POOL list).","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"PCA_ON_N_WORDS = 1000\nCOMMON_WORDS_POOL = 400\nPLOT_M_WORDS = 100\n\ncommon_words = pd.Series(list(set(word_count_EAP[:COMMON_WORDS_POOL].index) & set(word_count_HPL[:COMMON_WORDS_POOL].index)  & set(word_count_MWS[:COMMON_WORDS_POOL].index)))\n\nword_count_EAP = word_count_EAP.drop(common_words)\nword_count_HPL = word_count_HPL.drop(common_words)\nword_count_MWS = word_count_MWS.drop(common_words)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Next, let's take the top PCA_ON_N_WORDS words from each author, in order to perform PCA (Principal Component Analysis).\n\nUsing the PCA, I will be able to reduce the embedding dimension from 768 to 2, and therefore plot the most common words.\n\nHopefully, we will be able to derive some insights from that.\n\nWe will start by creating a word count dataframe.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"cols = ['count']\nword_count_EAP_top_n_for_pca = pd.DataFrame(word_count_EAP[:PCA_ON_N_WORDS], columns = cols)\nword_count_EAP_top_n_for_plot = pd.DataFrame(word_count_EAP[:PLOT_M_WORDS], columns = cols)\n\nword_count_EAP_top_n_for_pca['author'] = 'EAP'\nword_count_EAP_top_n_for_plot['author'] = 'EAP'\n\n\nword_count_HPL_top_n_for_pca = pd.DataFrame(word_count_HPL[:PCA_ON_N_WORDS], columns = cols)\nword_count_HPL_top_n_for_plot = pd.DataFrame(word_count_HPL[:PLOT_M_WORDS], columns = cols)\n\nword_count_HPL_top_n_for_pca['author'] = 'HPL'\nword_count_HPL_top_n_for_plot['author'] = 'HPL'\n\nword_count_MWS_top_n_for_pca = pd.DataFrame(word_count_MWS[:PCA_ON_N_WORDS], columns = cols)\nword_count_MWS_top_n_for_plot = pd.DataFrame(word_count_MWS[:PLOT_M_WORDS], columns = cols)\n\nword_count_MWS_top_n_for_pca['author'] = 'MWS'\nword_count_MWS_top_n_for_plot['author'] = 'MWS'\n\ndf_for_pca = pd.concat([word_count_EAP_top_n_for_pca, word_count_HPL_top_n_for_pca, word_count_MWS_top_n_for_pca])\ndf_for_pca = df_for_pca.reset_index()\ndf_for_pca.columns = ['word', 'count', 'author']\n\ndf_for_plot = pd.concat([word_count_EAP_top_n_for_plot, word_count_HPL_top_n_for_plot, word_count_MWS_top_n_for_plot])\ndf_for_plot = df_for_plot.reset_index()\ndf_for_plot.columns = ['word', 'count', 'author']\ndf_for_plot.head(2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"embeddings\"></a>\n\n## Embeddings\n> \"Words have no power to impress the mind without the exquisite horror of their reality.\" - Edgar Allan Poe.\n\nWe will use the BERT pre-trained embeddings which map each word id to its (1,768) vector.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Let's define some helper functions in order to extract the word id for each token, and the embedding vector for each word id:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def word_to_index(word):\n    token_id = tokenizer.convert_tokens_to_ids(word)\n    return token_id\n\nprint(word_to_index('Hello'))\n\ndef indices_to_vec(word_ids):\n    embeded_tokens = bert_embeddings(torch.Tensor(word_ids).to(torch.long))\n    return embeded_tokens.detach().numpy()\n\nprint(indices_to_vec([word_to_index('Hello'), word_to_index('Jacob')]).shape)\n\ndef index_to_vec(word_id):\n    embeded_token = bert_embeddings(torch.Tensor([word_id]).to(torch.long))\n    return embeded_token.detach().numpy()\nvec = index_to_vec(word_to_index('Hello'))\nprint(vec.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now let's apply the functions and fit the PCA for our data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_for_plot['word_id'] = df_for_plot['word'].apply(word_to_index)\ndf_for_pca['word_id'] = df_for_pca['word'].apply(word_to_index)\nvectors = index_to_vec(df_for_pca['word_id'].to_numpy())\npca = PCA(n_components=2)\npca.fit(vectors.squeeze())\ndf_for_pca['word_vec'] = df_for_pca['word_id'].apply(index_to_vec)\n\nvectors.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can now reduce the dimension to 2:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def vec_to_2dim(word_vec):\n    xy = pca.transform(word_vec)\n    return xy[0][0], xy[0][1]\nx, y = vec_to_2dim(vec)\nprint(x, y)\n\ndef tuple_x(xy):\n    return tuple(xy)[0]\nprint(tuple_x((1,2)))\n\ndef tuple_y(xy):\n    return tuple(xy)[1]\nprint(tuple_y((1,2)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_for_pca['word_xy'] = df_for_pca['word_vec'].apply(vec_to_2dim)\ndf_for_pca['word_x'] = df_for_pca['word_xy'].apply(tuple_x)\ndf_for_pca['word_y'] = df_for_pca['word_xy'].apply(tuple_y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"merged_df = df_for_plot.join(df_for_pca, on='word_id',lsuffix='_l', rsuffix='_r') \n# df_cleaned = merged_df[(merged_df['word_x'] < 0.5 ) & (merged_df['count_x'] > np.median(merged_df['count_x']))]\ndf_cleaned = merged_df[merged_df['word_x'] < 0.6 ]\n\n# df_cleaned = merged_df\ndf_cleaned.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_cleaned['count_l'].min()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"exploration\"></a>\n\n## Exploration and Visualization\n\n> \"When falsehood can look so like the truth, who can assure themselves of certain happiness?\" - Mary Shelley.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Before we even start, let's make sure the dataset is kind of even:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(train['author'])\nplt.xlabel('Authors');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I guess we can work with that. That's pretty balanced.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"I also want to check if the word counts are close enough to normal distribution.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"f, axes = plt.subplots(1, 2, figsize=(40,10))\n\nsns.distplot(ax=axes[0], a=train['word_count'])\nsns.distplot(ax=axes[1], a=train['word_count'][train['word_count'] < 150]);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"stats.normaltest(train['word_count'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f, axes = plt.subplots(1, 2, figsize=(40,10))\n\nsns.distplot(ax=axes[0], a=train['stopword_count'])\nsns.distplot(ax=axes[1], a=train['stopword_count'][train['stopword_count'] < 50]);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"stats.normaltest(train['stopword_count'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Seems like it is, and that's great since we are going to need this assumption for later ANOVA tests.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Looking at the lengths of the excerpts, can we find significant differences between the different authors?\n\nLet's first plot it:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"f, axes = plt.subplots(1, 2, figsize=(20,10))\n\nsns.violinplot(x='author', y='word_count', data=train[train['word_count'] < 60], ax=axes[0])\nplt.xlabel('Author Name', fontsize=12)\nplt.ylabel('Number of words in text', fontsize=12)\nplt.title(\"Number of words by author\", fontsize=15);\n\nsns.violinplot(x='author', y='word_count', data=train[train['word_count'] < 90], ax=axes[1])\nplt.xlabel('Author Name', fontsize=12)\nplt.ylabel('Number of words in text', fontsize=12)\nplt.title(\"Number of words by author\", fontsize=15);\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"While it is not very clear from the plot, we can perform one way ANOVA (Analysis of Variance) in order to test for significant differences between the authors","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### ANOVA - Analysis of Variance\n\nSince \"author\" is a categorical variable, and we want to check the difference between the three authors, the appropriate test will be ANOVA.\nIf there is any significant effect, we will perform three T tests.\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def calculate_one_way_anova(column_name):\n    return stats.f_oneway(train[column_name][train['author'] == 'EAP'],\n               train[column_name][train['author'] == 'HPL'],\n               train[column_name][train['author'] == 'MWS'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"one_way = calculate_one_way_anova('word_count')\nprint(f'The F score when comaring all the authors: {one_way.statistic}, which reflect pvalue of: {one_way.pvalue}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We got a very low pvalue, which means that we can conclude that there is indeed significant difference between the authors.\n\nHowever, we don't really know where is this difference coming from.\n\nIn order to do that, we will use T tests for comparing each of the pairs.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"t_EAP_HPL = stats.ttest_ind(train['word_count'][train['author'] == 'EAP'], train['word_count'][train['author'] == 'HPL'])\nt_MWS_HPL = stats.ttest_ind(train['word_count'][train['author'] == 'MWS'], train['word_count'][train['author'] == 'HPL'])\nt_MWS_EAP = stats.ttest_ind(train['word_count'][train['author'] == 'MWS'], train['word_count'][train['author'] == 'EAP'])\n\nprint(f'The T score when comaring EAP and HPL: {t_EAP_HPL.statistic} which reflect pvalue of: {t_EAP_HPL.pvalue}')\nprint(f'The T score when comaring MWS and HPL: {t_MWS_HPL.statistic} which reflect pvalue of: {t_MWS_HPL.pvalue}')\nprint(f'The T score when comaring MWS and EAP: {t_MWS_EAP.statistic} which reflect pvalue of: {t_MWS_EAP.pvalue}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Well, we can see that each of the pairs proved to be statistically different with very high probability.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Now let's perform the same process for stopwords:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"f, axes = plt.subplots(1, 2, figsize=(20,10))\n\nsns.violinplot(x='author', y='stopword_count', data=train[train['stopword_count'] < 30], ax=axes[0])\nplt.xlabel('Author Name', fontsize=12)\nplt.ylabel('Number of stop words in text', fontsize=12)\nplt.title(\"Number of stop words by author\", fontsize=15);\n\nsns.violinplot(x='author', y='stopword_count', data=train[train['stopword_count'] < 60], ax=axes[1])\nplt.xlabel('Author Name', fontsize=12)\nplt.ylabel('Number of stop words in text', fontsize=12)\nplt.title(\"Number of stop words by author\", fontsize=15);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"one_way = calculate_one_way_anova('stopword_count')\nprint(f'The F score when comaring all the authors: {one_way.statistic}, which reflect pvalue of: {one_way.pvalue}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"t_EAP_HPL = stats.ttest_ind(train['stopword_count'][train['author'] == 'EAP'], train['stopword_count'][train['author'] == 'HPL'])\nt_MWS_HPL = stats.ttest_ind(train['stopword_count'][train['author'] == 'MWS'], train['stopword_count'][train['author'] == 'HPL'])\nt_MWS_EAP = stats.ttest_ind(train['stopword_count'][train['author'] == 'MWS'], train['stopword_count'][train['author'] == 'EAP'])\n\nprint(f'The T score when comaring EAP and HPL: {t_EAP_HPL.statistic} which reflect pvalue of: {t_EAP_HPL.pvalue}')\nprint(f'The T score when comaring MWS and HPL: {t_MWS_HPL.statistic} which reflect pvalue of: {t_MWS_HPL.pvalue}')\nprint(f'The T score when comaring MWS and EAP: {t_MWS_EAP.statistic} which reflect pvalue of: {t_MWS_EAP.pvalue}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Again, we got very similar results, with significant differences between all of the authors.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Now, I want to use the computed PCA values of the words from the word counts, in order to see if there is any visible trend.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"p = sns.relplot(x=\"word_x\", y=\"word_y\", hue=\"author_l\", size=\"count_l\",\n             alpha=.5, palette=\"muted\", sizes=(70,450),\n            height=20, aspect=1, data=df_cleaned)\nax = p.axes[0,0]\n\nfor idx, row in df_cleaned.iterrows():\n     ax.text(row['word_x']+ 0.001, row['word_y'], row['word_l'], horizontalalignment='left', size='large', color='black')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Unfortunately, I can't see any obvious trend in this plot, but at least we tried.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"We can also get a sense of the most unique used words by each author: ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set(style=\"white\", context=\"talk\", font_scale = 4)\n\n# Set up the matplotlib figure\nf, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(90, 60), sharex=False)\n\n# Edgar Allan Poe\nx1 = word_count_EAP[:20].index\ny1 = word_count_EAP[:20]\nsns.barplot(x=x1, y=y1, ax=ax1)\nax1.axhline(0, color=\"k\", clip_on=False)\nax1.set_ylabel(\"Edgar Allan Poe\",fontsize=80)\n\n# Mary Shelley\nx2 = word_count_MWS[:20].index\ny2 = word_count_MWS[:20]\nsns.barplot(x=x2, y=y2, ax=ax2)\nax2.axhline(0, color=\"k\", clip_on=False)\nax2.set_ylabel(\"Mary Shelley\",fontsize=80)\n\n# \"H.P. Lovecraft\nx3 = word_count_HPL[:20].index\ny3 = word_count_HPL[:20]\nsns.barplot(x=x3, y=y3, palette=\"deep\", ax=ax3)\nax3.axhline(0, color=\"k\", clip_on=False)\nax3.set_ylabel(\"H.P. Lovecraft\",fontsize=80)\n\n# Finalize the plot\nsns.despine(bottom=True)\nplt.setp(f.axes, yticks=[])\nplt.tight_layout(h_pad=5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"classification\"></a>\n\n## Classification","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"First let's prepare the labels for our train set.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def author_to_label(author):\n    labels = {'EAP': 0,'HPL': 1,'MWS': 2}\n    return labels[author]\n\ndef label_to_author(label):\n    authors = ['EAP','HPL','MWS']\n    return authors[int(label)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['label'] = train['author'].apply(author_to_label)\ndummies = pd.get_dummies(train['author'])\ntrain = pd.concat([train,dummies], axis=1)\ntrain.head(1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Torch Classes","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"We will need two datasets for the train set and the test set:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"class TrainDataSet(data.Dataset):\n    def __init__(self, excerpts, labels, tokenizer, max_len):\n        self.excerpts = excerpts\n        self.labels = labels\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.excerpts)\n    \n    def __getitem__(self, item):\n        excerpt = str(self.excerpts[item])\n        \n        encoding  = self.tokenizer.encode_plus(\n            excerpt,\n            max_length = self.max_len,\n            add_special_tokens=True,\n            pad_to_max_length=True,\n            return_attention_mask=True,\n            return_token_type_ids=False,\n            return_tensors='pt'\n        )\n        \n        return {\n            'excerpt_text': excerpt,\n            'input_ids': encoding['input_ids'].flatten(),\n            'attention_mask': encoding['attention_mask'].flatten(),\n            'targets': torch.tensor(self.labels[item], dtype=torch.long)\n        }","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class TestDataSet(data.Dataset):\n    def __init__(self, ids, excerpts, tokenizer, max_len):\n        self.ids = ids\n        self.excerpts = excerpts\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.excerpts)\n    \n    def __getitem__(self, item):\n        excerpt = str(self.excerpts[item])\n        excerpt_id = str(self.ids[item])\n        \n        encoding  = self.tokenizer.encode_plus(\n            excerpt,\n            max_length = self.max_len,\n            add_special_tokens=True,\n            pad_to_max_length=True,\n            return_attention_mask=True,\n            return_token_type_ids=False,\n            return_tensors='pt'\n        )\n        \n        return {\n            'excerpt_id': excerpt_id,\n            'excerpt_text': excerpt,\n            'input_ids': encoding['input_ids'].flatten(),\n            'attention_mask': encoding['attention_mask'].flatten(),\n        }","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"And dataloaders for them:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_train_data_loader(df, tokenizer, max_len, batch_size):\n    excerpts = df['text'].to_numpy(),\n    print(f'Excerpts size: {len(excerpts)}')\n    labels = df['label'].to_numpy(),\n    dataset = TrainDataSet(excerpts=excerpts[0], labels=labels[0], tokenizer=tokenizer, max_len=max_len)\n    print(f'Dataset size: {len(dataset)}')\n    return data.DataLoader(dataset, batch_size=batch_size, num_workers=4, shuffle=True)\n\ndef create_test_data_loader(df, tokenizer, max_len, batch_size):\n    excerpts = df['text'].to_numpy(),\n    ids = df['id'].to_numpy(),\n    print(f'Excerpts size: {len(excerpts)}')\n    dataset = TestDataSet(ids= ids[0], excerpts=excerpts[0], tokenizer=tokenizer, max_len=max_len)\n    print(f'Dataset size: {len(dataset)}')\n    return data.DataLoader(dataset, batch_size=batch_size, num_workers=4)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"And now split the train data for train and validation sets.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_set, val_set = train_test_split(train, test_size=0.2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will pad excerpt up to MAX_LEN or trim them if needed.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"BATCH_SIZE = 16\nMAX_LEN = 160","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data_loader = create_train_data_loader(train_set, tokenizer, max_len=MAX_LEN, batch_size=BATCH_SIZE)\nval_data_loader = create_train_data_loader(val_set, tokenizer, max_len=MAX_LEN, batch_size=BATCH_SIZE)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample = next(iter(train_data_loader))\nprint(sample['input_ids'].shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The classifier is an nn.Module which adds linear classification layer to the bert model outputs.\n\nI use DistilBERT in order to get a smaller model.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"class DistilBertAuthorClassifier(nn.Module):\n    def __init__(self):\n        super(DistilBertAuthorClassifier, self).__init__()\n        self.num_labels = 3\n\n        self.softmax = nn.Softmax(dim=1)\n        self.distilbert = DistilBertModel.from_pretrained('distilbert-base-cased')\n        # self.pre_classifier = nn.Linear(config.hidden_size, config.hidden_size)\n        self.classifier = nn.Linear(self.distilbert.config.dim, 3)\n        self.dropout = nn.Dropout(0.3)\n\n        nn.init.xavier_normal_(self.classifier.weight)\n\n    def forward(self, input_ids, attention_mask):\n        distilbert_output = self.distilbert(input_ids=input_ids,\n                                            attention_mask=attention_mask)\n        hidden_state = distilbert_output[0]\n        # print(f'hidden_state shape: {hidden_state.shape}')                \n        # print(f'hidden_state shape[2]: {hidden_state.shape[2]}')                \n        pooled_output = hidden_state[:, 0, :]                   \n        # pooled_output = self.pre_classifier(pooled_output)   \n        # pooled_output = nn.ReLU()(pooled_output)             \n        pooled_output = self.dropout(pooled_output)        \n        logits = self.classifier(pooled_output)\n        # logits = self.softmax(logits)\n        return logits","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gc.collect()\nmodel = DistilBertAuthorClassifier()\nmodel = model.to(device)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"input_ids = sample['input_ids'].to(device)\nattention_mask = sample['attention_mask'].to(device)\n\nprint(input_ids.shape)\nprint(attention_mask.shape)\nprob, pred = torch.max(model(input_ids=input_ids, attention_mask=attention_mask),dim=1)\nprint(prob)\nprint(pred)\nprint(sample['targets'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.distilbert.config","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Training","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"I trained the model on Google's Coalb GPU using the following code, using cross entropy loss function and adam optimizer:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"EPOCHES = 3\nif torch.cuda.is_available():\n    optimizer = transformers.AdamW(model.parameters(), lr=2e-5, correct_bias=False)\n    total_steps = len(train_data_loader) * EPOCHES\n\n    scheduler = transformers.get_linear_schedule_with_warmup(\n        optimizer,\n        num_warmup_steps=0,\n        num_training_steps=total_steps\n    )\n\n    loss_fn = nn.CrossEntropyLoss().to(device)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_epoch(model, data_loader, loss_fn, optimizer, scheduler, n_examples):\n    model = model.train()\n    \n    losses = []\n    correct_predictions = 0\n    \n    for d in data_loader:\n        input_ids = d['input_ids'].to(device)\n        attention_mask = d['attention_mask'].to(device)\n        targets = d['targets'].to(device)\n        \n        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n        _, preds = torch.max(outputs, dim=1)\n        # print(targets.shape)\n        loss = loss_fn(outputs, targets)\n        correct_predictions += torch.sum(preds == targets)\n        losses.append(loss.item())\n        loss.backward()\n        \n        # nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n        \n        optimizer.step()\n        scheduler.step()\n        optimizer.zero_grad()\n    return correct_predictions.double() / n_examples, np.mean(losses)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def eval_model(model, data_loader, loss_fn, n_examples):\n    model = model.eval()\n    losses = []\n    \n    correct_predictions = 0\n    with torch.no_grad():\n        for d in data_loader:\n            input_ids = d['input_ids'].to(device)\n            attention_mask = d['attention_mask'].to(device)\n            targets = d['targets'].to(device)\n\n            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n\n            _, preds = torch.max(outputs, dim=1)\n            loss = loss_fn(outputs, targets)\n            correct_predictions += torch.sum(preds == targets)\n            losses.append(loss.item())\n    return correct_predictions.double() / n_examples, np.mean(losses)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def predict_authors(model, data_loader, submission_df):\n    model = model.eval()\n    with torch.no_grad():\n        for d in data_loader:\n            input_ids = d['input_ids'].to(device)\n            attention_mask = d['attention_mask'].to(device)\n            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n            print(outputs)\n            ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if is_colab and torch.cuda.is_available():\n    optimizer = transformers.AdamW(model.parameters(), lr=2e-5, correct_bias=False)\n    total_steps = len(train_data_loader) * EPOCHES\n\n    # scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.4)\n    scheduler = transformers.get_linear_schedule_with_warmup(\n        optimizer,\n        num_warmup_steps=0,\n        num_training_steps=total_steps\n    )\n    loss_fn = nn.CrossEntropyLoss().to(device)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"filename = 'finalized_model.pt'\n\nif is_colab and torch.cuda.is_available():\n    model = model.to(device)\n\n    history = defaultdict(list)\n    best_accuracy = 0\n\n    for epoch in range(EPOCHES):\n        print(f'Epoch {epoch + 1}/{EPOCHES}')\n        print('-'*10)\n\n        train_acc, train_loss = train_epoch(\n            model,\n            train_data_loader,\n            loss_fn,\n            optimizer,\n            scheduler,\n            len(train_set)   \n        )\n        print(f'Train loss: {train_loss}, accuracy: {train_acc}')\n\n        val_acc, val_loss = eval_model(\n            model,\n            val_data_loader,\n            loss_fn,\n            len(val_set)   \n        )\n        print(f'Validation loss: {val_loss}, accuracy: {val_acc}')\n\n    torch.save(model.state_dict(), DIR_NAME+filename)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if not is_colab:\n    model.load_state_dict(torch.load('/kaggle/input/spookydistilbert/finalized_model.pt', map_location=device))\n    model.eval()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data_loader = create_test_data_loader(test, tokenizer, max_len=MAX_LEN, batch_size=BATCH_SIZE)\nsample = next(iter(test_data_loader))\n\ninput_ids = sample['input_ids'].to(device)\nattention_mask = sample['attention_mask'].to(device)\n\nprint(input_ids.shape)\nprint(attention_mask.shape)\nprob, pred = torch.max(model(input_ids=input_ids, attention_mask=attention_mask),dim=1)\nprint(prob)\nprint(pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plt.plot(history['train_acc'], label='train accuracy')\n# plt.plot(history['val_acc'], label='validation accuracy')\n\n# plt.title('Training history')\n# plt.ylabel('Accuracy')\n# plt.xlabel('Epoch')\n# plt.legend()\n# plt.ylim([0, 1]);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Create Submission for Comparison\n\nIn order to be able to compare the results of the model I created a submission for the kaggle contest:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def test_model(model, data_loader, results_df):\n    model = model.eval()\n    submission = []\n\n    with torch.no_grad():\n        for d in data_loader:\n            excerpt_ids = d['excerpt_id'],\n            input_ids = d['input_ids'].to(device)\n            attention_mask = d['attention_mask'].to(device)\n\n            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n            outputs = nn.functional.softmax(outputs,dim=1)\n#             print(excerpt_ids[0])\n            for i, excerpt_id in enumerate(excerpt_ids[0]):\n#                 print(i, excerpt_id)\n#                 print(results_df[results_df['id']==excerpt_id])\n                results_df.loc[results_df['id'] == excerpt_id, ['EAP','HPL','MWS']] =  outputs[i].tolist()\n#                 results_df[results_df['id']==excerpt_id][['EAP','HPL','MWS']] = outputs[i].tolist()\n    return results_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results_df = pd.read_csv('./sample_submission.csv')\nresults_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results_df = test_model(model, test_data_loader, results_df)\nresults_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results_df.to_csv(\"submission.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I ended up with loss of 0.42742 which is pretty mediocre, but still not bad for not a lot of effort, relying mostly on pretrained models.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<a id=\"creating_poe\"></a>\n\n\n## Text Generation:\n\n> “I wish I could write as mysterious as a cat.” - Edgar Allan Poe\n\nUltimately, we would like to \"draw\" a sentence out of each author's words distributions.\nWe therefore need a probabilistic model that capture some aspects of these distributions.\n\n<table><tr><td><img src='https://vignette.wikia.nocookie.net/altered-carbon/images/7/71/Poe.jpg', width=\"300\"> <figcaption>Poe, Owner of the Raven - AI hotel, Altered Carbon</figcaption></td>\n    </tr></table>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### N-gram Language Model","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"A simple approach for sentence generation is N-gram models.\n\nWith N-gram models, we estimate probability of each word given prior context (sequence of words).\n\n$$\nP(red|Roses,are)\n$$\nAn N-gram model uses only N−1 words of prior context.\n* unigram: $$ P(red) $$\n* bigram: $$ P(red|are) $$\n* trigram: $$ P(red|Roses,are) $$\n\nThe approximation of bigram: $$ P(w_1^n)= \\Pi_{k=1}^n P(w_k|w_{k-1}) $$\nThe approximation of N-gram: $$ P(w_1^n)= \\Pi_{k=1}^n P(w_k|w_{k-N+1}^{k-1}) $$\n\nWe can estimate the conditional probabilities from raw text based on the relative frequency of word sequences.\nFor N-gram: $$ P(w_n|w_{n-N+1}^{n-1}) = \\frac{C(w^{n-1}_{n-N+1}w_n)}{C(w_{n-N+1}^{n-1})} $$\n\nHowever, since the word-wise N-gram approach demands too much data for what we have, we will use charachter-wise N-gram model, which is pretty much the same theoretical idea.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"class LM:\n  def __init__(self, n):\n    self.n_gram = n\n    \n  def train(self, text):\n    self.n_counts = defaultdict(Counter)\n    for i in range(0, len(text) - self.n_gram + 1):\n      t = text[i:i+self.n_gram-1]\n      n_char = text[i+self.n_gram-1]\n      self.n_counts[t][n_char] += 1\n  \n  def generate(self, init_text, n):\n    text = init_text\n    while len(text) < n:\n      lookup_text = text[-self.n_gram+1:]\n      if lookup_text not in self.n_counts:\n        break\n      counter = self.n_counts[text[-self.n_gram+1:]]\n      keys = list(counter.keys())\n      values = list(counter.values())\n      probs = [v/sum(values) for v in values]\n      cummulative_probs = np.cumsum(probs)\n      p = np.random.rand()\n      for i in range(len(cummulative_probs)):\n        if p <= cummulative_probs[i]:\n          text += keys[i]\n          break\n    return text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_trained_lm(n, excerpts):\n    lm = LM(n)\n    text = ' '.join(list(excerpts))\n    lm.train(text)\n    return lm","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's train an N-gram language model for each of the authors:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"n=7\nlm_EAP = get_trained_lm(n, train[train['author']=='EAP']['text'])\nlm_HPL = get_trained_lm(n, train[train['author']=='HPL']['text'])\nlm_MWS = get_trained_lm(n, train[train['author']=='MWS']['text'])\nprint(f\"lm_EAP size: {len(lm_EAP.n_counts)}, lm_HPL size: {len(lm_HPL.n_counts)}, lm_MWS size: {len(lm_MWS.n_counts)}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now let's see what are the different sentences created from the same seed text, and different language models:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"seed_string = \"Dark night\"\nl = 120\nmodels = {'Poe':lm_EAP, 'Lovecraft': lm_HPL, 'Shelley': lm_MWS}\nfor model_name in models.keys():\n    generated = models[model_name].generate(seed_string, l)\n    print(f\"\\\"{generated}...\\\", The bot {model_name}, 2020\\n\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As expected, the result is more entertaining than informative, but it sure helps to grasp what a simple model such as the N-gram may achieve.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<a id=\"conclusions\"></a>\n\n\n## Conclusions\n\n> “Never Explain Anything”  - H.P. Lovecraft\n\n\nI started with three assumptions:\n1. Statistically significant differences between the authors. \n2. Good prediction for classification task. \n3. Somewhat entertaining yet sensible simulated excerpts from each author.\n\nMy conclusions are therefore the following:\n1. There is indeed some significant differences between the authors, as we saw in the ANOVA and T tests.\n2. We achieved reasonable prediction for the classification data, where we definitely had an overfitting problem. I assume that if we had ten times the amount of data we would have achieved better results.\n3. It's actually quite surprising that a very simple language model can produce interesting generated excerpts.\n\n","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}