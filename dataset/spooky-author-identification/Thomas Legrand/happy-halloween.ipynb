{"nbformat":4,"nbformat_minor":1,"cells":[{"metadata":{"_uuid":"16072695e888fbaf8eded73e43440f6b77541ac7","_cell_guid":"6cd94d7e-0560-4973-9a18-b253427939b4"},"source":"# HapPy Halloween kernel","cell_type":"markdown"},{"metadata":{"_uuid":"b84e72fe5b113d5f7fee7b1849dc41e6588d5ab7","_cell_guid":"9e8a48eb-bf25-4844-9e5f-5025f811a363"},"source":"![](https://media.giphy.com/media/pLWfbn1WVKzMQ/giphy.gif)","cell_type":"markdown"},{"metadata":{"_uuid":"9e823e8a582bf203ad59fd1d07124529cf627f94","_cell_guid":"42d7ef55-ffe4-4726-8c3d-9f2a55ac53e4"},"source":"This kernel is made by a beginner for beginners.\nThis is my first kernel, so I'm looking for advice !","cell_type":"markdown"},{"metadata":{"_uuid":"ffd772274877dd31953b2cd4655a68f11e347ff3","_cell_guid":"17dea100-c07a-4c7c-9d17-40bf944e1d47"},"source":"## Imports","cell_type":"markdown"},{"outputs":[],"metadata":{"_uuid":"9078bf8d4d2a6ac3d9964ba2e1e9285845a4ed46","collapsed":true,"_cell_guid":"e5de9f60-44cd-4ea0-b7de-5965c80e5503"},"execution_count":null,"source":"import os\nimport pandas as pd\nimport numpy as np\nimport sklearn as sk\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom string import punctuation\nfrom nltk import pos_tag\nfrom nltk.stem import PorterStemmer\nfrom nltk.tokenize import word_tokenize, sent_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk import FreqDist\nfrom wordcloud import WordCloud, STOPWORDS\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.pipeline import Pipeline, FeatureUnion\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer\nfrom xgboost import XGBClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import confusion_matrix","cell_type":"code"},{"outputs":[],"metadata":{"_uuid":"83c7e7809517a04d1ef93685acddf4941b71a1e5","collapsed":true,"_cell_guid":"f0e374c3-2037-4a8c-8c02-71fb4e3f17e0"},"execution_count":null,"source":"train = pd.read_csv(\"../input/train.csv\")\ntest = pd.read_csv(\"../input/test.csv\")","cell_type":"code"},{"metadata":{"_uuid":"53f02120a5b0fd845e5d5434890651072017ad59","_cell_guid":"7324a50d-77bf-4c6b-a04a-5c0bbab030d3"},"source":"## Problem Understanding","cell_type":"markdown"},{"metadata":{"_uuid":"733e71e2954738fed55ca1e37beda8460993db42","_cell_guid":"cc391c97-696f-48f9-be62-ae84e219a3d5"},"source":"It is always important to know what we are going for.\nThe description gives some details:\n> The competition dataset contains text from works of fiction written by spooky authors of the public domain: Edgar Allan Poe, HP Lovecraft and Mary Shelley\n\nThe objective is to accurately **identify the author of the sentences in the test set**.","cell_type":"markdown"},{"metadata":{"_uuid":"9831d801837125cbd0bdc57ce5b5fde3c7993cf8","_cell_guid":"7ed26e51-ef4b-4010-acca-8f2917a1d341"},"source":"## Data Understanding","cell_type":"markdown"},{"outputs":[],"metadata":{"_uuid":"9e3e600de7c1b94f511b55b8ac718fe710083bca","_cell_guid":"5c4cc768-d5b3-4c23-ad23-b19c93ba5471"},"execution_count":null,"source":"train.head()\nprint(\"--- Shape ---\")\nprint(train.shape)\nprint(\"--- Missing values ---\")\ntrain.isnull().sum() * 100 / len(train)","cell_type":"code"},{"metadata":{"_uuid":"de18a9a978c15414292925d0c6d76d11e1666962","_cell_guid":"f9cec234-ecce-45ba-b270-84b29fe8a806"},"source":"* The dataset goes straight to the point with only 3 columns : ids, texts, and the targets which are authors\n* There is no missing value","cell_type":"markdown"},{"outputs":[],"metadata":{"_uuid":"9968f035c1238e0b5f4ef8158f3c511969dece22","_cell_guid":"5435cc4e-fc30-4eb0-9e57-f0ec2fed726e"},"execution_count":null,"source":"sns.countplot(train.author)","cell_type":"code"},{"outputs":[],"metadata":{"_uuid":"0f7a4daf3ee080def0d5896853e2277f9cfe5be6","collapsed":true,"_cell_guid":"609796b9-5b14-4963-9d07-8e9ace4e9ac4"},"execution_count":null,"source":"# Takes a column and concatenate strings\ndef build_corpus(data):\n    data = str(data)\n    corpus = \"\"\n    for sent in data:\n        corpus += str(sent)\n    return corpus","cell_type":"code"},{"outputs":[],"metadata":{"_uuid":"5014ff75e8a1289ad872889bea4432fbe372042f","collapsed":true,"_cell_guid":"3eddf71a-4dc4-4e4d-b0ed-e36a1d3000c2"},"execution_count":null,"source":"# Gather text of authors in different dataframes\neap = train[train.author == \"EAP\"]\nhpl = train[train.author == \"HPL\"]\nmws = train[train.author == \"MWS\"]","cell_type":"code"},{"outputs":[],"metadata":{"_uuid":"bed1d7bd4fad41357aebdea275047c72057f094f","_cell_guid":"972b943d-14a3-473d-aafd-04cabd927094"},"execution_count":null,"source":"plt.figure(figsize=(15,10))\nplt.subplot(331)\neap_wc = WordCloud(background_color=\"white\", max_words=100, stopwords=STOPWORDS)\neap_wc.generate(build_corpus(eap.text))\nplt.title(\"Edgar Allan Poe\", fontsize=20)\nplt.imshow(eap_wc, interpolation='bilinear')\nplt.axis(\"off\")\n\nplt.subplot(332)\nhpl_wc = WordCloud(background_color=\"white\", max_words=100, stopwords=STOPWORDS)\nhpl_wc.generate(build_corpus(hpl.text))\nplt.title(\"HP Lovecraft\", fontsize=20)\nplt.imshow(hpl_wc, interpolation='bilinear')\nplt.axis(\"off\")\n\nplt.subplot(333)\nmws_wc = WordCloud(background_color=\"white\", max_words=100, stopwords=STOPWORDS)\nmws_wc.generate(build_corpus(mws.text))\nplt.title(\"Marry Shelley\", fontsize=20)\nplt.imshow(mws_wc, interpolation='bilinear')\nplt.axis(\"off\")","cell_type":"code"},{"metadata":{"_uuid":"b76f9aad7f53931c6baa45b17e7746c17608f026","_cell_guid":"2b5133ab-b96d-43b6-b955-aa67afa10d90"},"source":"The authors seems to use a different lexical field, that gives us idea about using a Tfidf over texts.","cell_type":"markdown"},{"metadata":{"_uuid":"05f39df026500f22f4a12bd4d4079ecbb4390b0d","_cell_guid":"0809b54b-b580-495c-88cb-7936b17b8e86"},"source":"## Data Preparation","cell_type":"markdown"},{"metadata":{"_uuid":"4a69d64ce658fb03005121ea9bdecfaa0da859f4","_cell_guid":"dfafd151-0680-43ac-b1e2-df647f99cbcf"},"source":"Scikit-Learn doesn't always handle strings, so let's encode authors.","cell_type":"markdown"},{"outputs":[],"metadata":{"_uuid":"3e19559d2ced6b16f4f8665e782b7eef6c6f7f81","collapsed":true,"_cell_guid":"16528ce8-988f-4752-8449-773c4fa2cd28"},"execution_count":null,"source":"le = LabelEncoder()\nauthor_encoded = le.fit_transform(train.author)","cell_type":"code"},{"metadata":{"_uuid":"15b7688a5869c57a934768c34e948fc60fa30e18","_cell_guid":"ac3519d8-c7f5-4642-96e6-aa774a702835"},"source":"## Modelling","cell_type":"markdown"},{"metadata":{"_uuid":"cfad2f3ee02c1f0b209bedf2e2fa405f30f0e55d","_cell_guid":"fd2d3465-6c31-461e-ab5b-21f09e9776ae"},"source":"Now, this is what you all have been waiting for (or not ! :) )\nLet's split a train and a test dataset to evaluate our algorithm.\nI decided to use acurracy as the metric as it is easier to interpret.\nFinally, I define the cross validation method that will be used.","cell_type":"markdown"},{"outputs":[],"metadata":{"_uuid":"a6d72d92ac5d0d5326c0b66771c81f4b79e01abc","collapsed":true,"_cell_guid":"8674caba-ef7f-428d-b45f-2bc4a1ec03bc"},"execution_count":null,"source":"seed = 12\nX_train, X_test, y_train, y_test = train_test_split(train.text, author_encoded, \n    test_size=0.3, random_state=seed)\nmetric = 'accuracy'\nkfold = KFold(n_splits=10, random_state=seed)","cell_type":"code"},{"metadata":{"_uuid":"9734a2a4d78a81db05c19f4f512fda277b31a2a7","_cell_guid":"565de6ce-2454-439c-a99d-e7c505e47488"},"source":"### Useful classes from [PyData Seattle 2017](https://channel9.msdn.com/Events/PyData/Seattle2017/BRK03)","cell_type":"markdown"},{"outputs":[],"metadata":{"_uuid":"b500c39f0a221770e1305ed8ac4252a1dffa4407","collapsed":true,"_cell_guid":"fcde8fa4-b609-484c-9385-dd931929c527"},"execution_count":null,"source":"# Return called columns of a DataFrame\nclass ColumnExtractor(TransformerMixin):\n    def __init__(self, cols):\n        self.cols = cols\n    def transform(self, X):\n        Xcols = X[self.cols]\n        return Xcols\n    def fit(self, X, y=None):\n        return self\n\n# Enables to train an estimator within the pipeline\nclass ModelTransformer(TransformerMixin):\n    def __init__(self, model):\n        self.model = model\n    def fit(self, *args, **kwargs):\n        self.model.fit(*args, **kwargs)\n        return self\n    def transform(self, X, **transform_params):\n        return pd.DataFrame(self.model.predict(X))","cell_type":"code"},{"metadata":{"_uuid":"8147d3a4f6be8146173c4480af7665474988924c","_cell_guid":"4f3831b2-6379-425a-a117-94b8701e24aa"},"source":"### Feature engineering","cell_type":"markdown"},{"outputs":[],"metadata":{"_uuid":"9245246b32e5b10fb00f19dcc8b8071d9cce4caa","collapsed":true,"_cell_guid":"1ea18256-997a-45d5-8975-5a67d39ea568"},"execution_count":null,"source":"# Calculate the length of each text\nclass LengthTransformer(TransformerMixin):\n    def transform(self, X, **transform_params):\n        return pd.DataFrame(X.apply(lambda x: len(str(x)))) \n    def fit(self, X, y=None, **fit_params):\n        return self\n    \n# Count the number of words in each text\nclass WordCountTransformer(TransformerMixin):\n    def transform(self, X, **transform_params):\n        return pd.DataFrame(X.apply(lambda x: len(str(x).split()))) \n    def fit(self, X, y=None, **fit_params):\n        return self\n    \n# Count the number of unique words in each text\nclass UniqueWordCountTransformer(TransformerMixin):\n    def transform(self, X, **transform_params):\n        return pd.DataFrame(X.apply(lambda x: len(set(str(x).split())))) \n    def fit(self, X, y=None, **fit_params):\n        return self\n    \n# Calculate the average length of words in each text\nclass MeanLengthTransformer(TransformerMixin):\n    def transform(self, X, **transform_params):\n        return pd.DataFrame(X.apply(lambda x: np.mean([len(w) for w in str(x).split()]))) \n    def fit(self, X, y=None, **fit_params):\n        return self\n\n# Count the number of punctuation in each sentence\nclass PunctuationCountTransformer(TransformerMixin):\n    def transform(self, X, **transform_params):\n        return pd.DataFrame(X.apply(lambda x: len([p for p in str(x) if p in punctuation]))) \n    def fit(self, X, y=None, **fit_params):\n        return self\n\n# Count the number of unique words in each text\nclass StopWordsCountTransformer(TransformerMixin):\n    def transform(self, X, **transform_params):\n        return pd.DataFrame(X.apply(lambda x: len([sw for sw in str(x).lower().split() if sw in set(stopwords.words(\"english\"))]))) \n    def fit(self, X, y=None, **fit_params):\n        return self","cell_type":"code"},{"metadata":{"_uuid":"eefa413df8e32a7554ab5c37d441cf60674bd92a","_cell_guid":"6c0d9e91-a189-4de0-8d62-0d67cb97a456"},"source":"The idea is to feed the classifier with the count of commas, the length of sentences, counts of words but also Tf-Idf.","cell_type":"markdown"},{"outputs":[],"metadata":{"_uuid":"af3d03c30deaed4d6078823eb9c2afe9582f2293","collapsed":true,"_cell_guid":"5feee8fb-b6c2-4710-92f1-4d7bbf37ebcb"},"execution_count":null,"source":"pipeline = Pipeline([\n    ('features', FeatureUnion([\n        ('text_length', LengthTransformer()),\n        ('word_count', WordCountTransformer()),\n        ('mean_length', MeanLengthTransformer()),\n        ('punctuation_count', PunctuationCountTransformer()),\n        ('stop_words_count', StopWordsCountTransformer()),\n        ('count_vect', CountVectorizer(lowercase=False)),\n        ('tf_idf', TfidfVectorizer())\n    ])),\n  ('classifier', XGBClassifier(objective='multi:softprob', random_state = 12, eval_metric='mlogloss'))\n])","cell_type":"code"},{"metadata":{"_uuid":"9006bd24ed9fb8a9ce3a8bb10a67f6b54934dcef","_cell_guid":"325a455e-2a28-4c37-8be0-a52ff0b2b92b"},"source":"I really like pipelines, it enables to get rid off useless code lines between steps, and it is really clear to interpret.","cell_type":"markdown"},{"metadata":{"_uuid":"1a98d18a70c7e1cd52fdea2022b3498fd5356c28","_cell_guid":"af7480fe-581b-4aeb-a2a2-a5ad66c6f6b3"},"source":"## Evaluation","cell_type":"markdown"},{"outputs":[],"metadata":{"_uuid":"0703482053522b1455ff5e213c7fefe7c4b24587","_cell_guid":"77d4d58e-03aa-4deb-88e6-4a4a063fd7bf"},"execution_count":null,"source":"clf_pipe = pipeline.fit(X_train, y_train)\nscore_pipe = cross_val_score(clf_pipe, X_train, y_train, cv=kfold, scoring=metric)\nprint(\"Mean score = %.3f, Std deviation = %.3f\"%(np.mean(score_pipe),np.std(score_pipe)))\nscore_pipe_test = clf_pipe.score(X_test,y_test)\nprint(\"Mean score = %.3f, Std deviation = %.3f\"%(np.mean(score_pipe_test),np.std(score_pipe_test)))","cell_type":"code"},{"outputs":[],"metadata":{"_uuid":"ea916c98ed75a8fb0af402b5cd1dcf2e9177ec76","_cell_guid":"88c4f14f-4fed-4c3f-b57d-3ebc600118ec"},"execution_count":null,"source":"conf_mat = confusion_matrix(y_test, clf_pipe.predict(X_test))\nsns.heatmap(conf_mat, annot=True)\nplt.xticks(range(3), ('EAP', 'HPL', 'MWS'), horizontalalignment='left')\nplt.yticks(range(3), ('EAP', 'HPL', 'MWS'), rotation=0)","cell_type":"code"},{"metadata":{"_uuid":"fe6d6133bb3c7bfa4d95311905229d99d05b8c1c","_cell_guid":"80061a6c-c94b-4bc7-b8c5-6eca01744c19"},"source":"We can see that it is quite well classified, but we can see that EAP is chosen too often.\n![](https://media.giphy.com/media/azZYZhswISHw4/giphy.gif)","cell_type":"markdown"},{"metadata":{"_uuid":"1d207a6c253f2a33b8a52e5f811712f3e8037c19","_cell_guid":"714822e1-6c2b-48e4-b3a3-8a07893b79e7"},"source":"## Submission","cell_type":"markdown"},{"outputs":[],"metadata":{"_uuid":"a62948761ef2579aa1ff04ac9ede1253c845b62a","scrolled":false,"_cell_guid":"e1e83b35-7c32-482d-8f31-930856402db4"},"execution_count":null,"source":"target_names = ['EAP', 'HPL', 'MWS']\ny_pred = pd.DataFrame(clf_pipe.predict(test.text), columns=target_names)\nsubmission = pd.concat([test[\"id\"],y_pred], 1)\nsubmission.to_csv(\"./submission.csv\", index=False)","cell_type":"code"},{"metadata":{"_uuid":"4c74a259da6d661359b4ee7880248dae01bf88ad","_cell_guid":"9cc8428a-0631-4b08-a225-6263384a68c4"},"source":"## Comments","cell_type":"markdown"},{"metadata":{"_uuid":"922cbcd6ecbd9b0cb4ffedab038ff73778ecf0c9","_cell_guid":"b8d47afc-37bb-4207-ad0e-515c57aeef54"},"source":"* I scored 0.47318 as multiclass loss. Despise the fact it is well classified, probabilities need to be refined.\n* We could try to study in what extent the use of nouns or adverbs could differenciate the authors\n* Open to ideas\n* Open to advice\n* Hope you enjoyed :)","cell_type":"markdown"},{"metadata":{"_uuid":"01a5e23507d0cd5dfe259484888144a9fc44143c","_cell_guid":"7b6482b1-1e1a-4506-85cc-d2dbacc877c9"},"source":"![](https://media.giphy.com/media/3o7btQsLqXMJAPu6Na/giphy.gif)","cell_type":"markdown"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"version":3,"name":"ipython"},"nbconvert_exporter":"python","file_extension":".py","pygments_lexer":"ipython3","version":"3.6.3","mimetype":"text/x-python","name":"python"}}}