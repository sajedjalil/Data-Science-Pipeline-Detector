{"cells":[{"metadata":{"_cell_guid":"803b1871-80d7-4259-ac65-88180fc1644e","_uuid":"3bd3ee9171152e8f90f5c52fdcbff4244f9c7895"},"source":"# Spooky Author Identification\n","cell_type":"markdown"},{"metadata":{"_cell_guid":"51a308c4-7ec6-48e3-91f1-4c156c60a6b2","_uuid":"abe092b0bbe1bf038c3c9253084d90d6e373dcc6"},"source":"Problem Definition:\n  We have sentences from 3 different books in the test dataset which need to be classified against 3 authors. we have been provided with training dataset with sentences which authored by 3 authors (  Edgar Allan Poe, Mary Shelley, and HP Lovecraft). So have multi-class classification problem.\n  \nProblem Solving Approch:\n1) Understand the Training dataset and doing EDA on variables.\n2) Ploting graph on distribution of training data against authors and creating wordcloud.\n3) Converting Text into vectors using NLTK package( Stemming, tokenization, stopwords removal)\n4) Feature extraction from the tokenization text and building a supervised classification model\n5) Fit the model with training dataset and extract the output from model by transforming test dataset.\n6) Build submission file with probability of all 3 authors\n  ","cell_type":"raw"},{"outputs":[],"execution_count":null,"source":"#Import Libraries\nimport numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ncolor = sns.color_palette()\n%matplotlib inline","metadata":{"_cell_guid":"a552f328-0aa3-4f6b-a8d7-cbdb4cf89a4c","_uuid":"310a75be6bc228b26c8709b5e125c5072ce1d3f3","collapsed":true},"cell_type":"code"},{"outputs":[],"execution_count":null,"source":"# Reading Training, Testing dataset\ntrain = pd.read_csv(\"../input/train.csv\")\ntest = pd.read_csv(\"../input/test.csv\")\n","metadata":{"_cell_guid":"5888278a-7a02-40b8-92c9-eed48ea31a27","_uuid":"8a8b9eac1da28a2115107c786526ea53ad3bd6a7","collapsed":true},"cell_type":"code"},{"outputs":[],"execution_count":null,"source":"train.head(10)","metadata":{"_cell_guid":"41de6a69-f8ae-4d35-8b3a-1e24618fb33a","_uuid":"85ce04183eaf6db745589e79e1e7c3ff4834a3fb"},"cell_type":"code"},{"outputs":[],"execution_count":null,"source":"# Number of records in Training dataset is 19579 records\ntrain.shape[0]","metadata":{"_cell_guid":"edca98d6-f0af-4690-9ad7-c5477fd672f2","_uuid":"b2ee12161646cfbfa63720e8bab38aab24e5294c"},"cell_type":"code"},{"outputs":[],"execution_count":null,"source":"# Lets check how the records are distributed against the authors in training dataset\nsns.countplot(x=\"author\", data=train, palette = \"Greens_d\")","metadata":{"_cell_guid":"4d7852bc-36bc-45f1-b93a-6a210a3ea5ab","_uuid":"328b8c8017be401ebf19d343d1fc92dc3098b566"},"cell_type":"code"},{"metadata":{"_cell_guid":"7388d66b-7b9a-43af-8f28-4dae0440e7c7","_uuid":"cc48e9c89762842c03ed93bf28993ccc6d0c5490"},"source":"We do have slightly more records for EAP author but it is negligable. So we have almost equal class distribution in Training dataset.","cell_type":"raw"},{"outputs":[],"execution_count":null,"source":"#Lets create 3 different dataframes according to the Authors\ntrain_eap = train[train.author=='EAP']\ntrain_hpl = train[train.author=='HPL']\ntrain_mws = train[train.author=='MWS']","metadata":{"_cell_guid":"4a090a93-5e82-4d5c-a734-480390bdf607","_uuid":"12bb5ba8fe5e546efc5ae63816234bef0dd36047","collapsed":true},"cell_type":"code"},{"outputs":[],"execution_count":null,"source":"#Wordcloud for EAP\nfrom wordcloud import WordCloud, STOPWORDS\nAlltext= (' '.join(train_eap['text']))\nwc = WordCloud(width = 1000, height = 500,stopwords=STOPWORDS).generate(Alltext)\n\nplt.figure(figsize=(15,5));\nplt.imshow(wc);\nplt.axis('off');\nplt.title('Word Cloud for Edgar Allan Poe');","metadata":{"_cell_guid":"542eb1b0-40f7-47ac-883f-314f53b42779","_uuid":"3707776c19b8827ac4bf41643016554cb0c0ee64"},"cell_type":"code"},{"outputs":[],"execution_count":null,"source":"#Wordcloud for MWS\nAlltext= (' '.join(train_mws['text']))\nwc = WordCloud(width = 1000, height = 500,stopwords=STOPWORDS).generate(Alltext)\n\nplt.figure(figsize=(15,5));\nplt.imshow(wc);\nplt.axis('off');\nplt.title('Word Cloud for Mary Shelley');","metadata":{"_cell_guid":"4d56c1b7-c3d1-43ee-a2d9-e4f902e5e7b8","_uuid":"1daee615d9e72a70312b1409a97c34483641f3a5"},"cell_type":"code"},{"outputs":[],"execution_count":null,"source":"#Wordcloud for HPL\nAlltext= (' '.join(train_hpl['text']))\nwc = WordCloud(width = 1000, height = 500,stopwords=STOPWORDS).generate(Alltext)\n\nplt.figure(figsize=(15,5));\nplt.imshow(wc);\nplt.axis('off');\nplt.title('Word Cloud for HP Lovecraft');","metadata":{"_cell_guid":"07b07298-da5b-497a-a854-10f510b64ee6","_uuid":"19c005c07da08e97ee5cd3087fe1ae9f95c03b01"},"cell_type":"code"},{"outputs":[],"execution_count":null,"source":"# lets create some features from the text columns on Train dataset\ntrain[\"length\"] = train[\"text\"].apply(lambda x: len(str(x).split()))\ntest[\"length\"] = test[\"text\"].apply(lambda x: len(str(x).split()))","metadata":{"_cell_guid":"872c31c9-0907-47fd-a669-f9ec0dadbf06","_uuid":"111cd19ad06a0cde61657b784dcb82d35732d7f0","collapsed":true},"cell_type":"code"},{"outputs":[],"execution_count":null,"source":"print(train.groupby(by=['author'])['length'].mean())\n# the mean length of text for each author is almost same for HPL and MWS.","metadata":{"_cell_guid":"b4525e77-f2ed-4ff6-8262-ad3ce54cf9be","_uuid":"1316519962018b7085a4e9de2d5b4dc4be7da38e"},"cell_type":"code"},{"outputs":[],"execution_count":null,"source":"# Lets start extracting features from NLTK packages\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nimport string\neng_stopwords = set(stopwords.words(\"english\"))\nfrom sklearn.feature_extraction.text import CountVectorizer","metadata":{"_cell_guid":"d7d17ea4-e047-4fca-aaed-5506a79df8a7","_uuid":"f925cc6847fcfbecb58bff364f69eb2096ce8f0a","collapsed":true},"cell_type":"code"},{"metadata":{"_cell_guid":"62f6854a-6a31-4e0e-b50f-ef907909329b","_uuid":"0f24f35d6eb2142c78aaec77e46d884f555c0740"},"source":"#Lets start converting the text to Bag of words using NLTK\nBelow code will be apply functions for each text in the train dataset.\n1) Convert text to lowercase\n2) Spliting text to words\n3) Stemmer on all words\n4) Remove all stopwords\n5) Join back all words \n6) Append the corpus to corpus_text list.\n\n","cell_type":"raw"},{"outputs":[],"execution_count":null,"source":"corpus_text = []\n#train.shape[0]\nfor i in range(0, train.shape[0]):\n    corpus = train[\"text\"][i]\n    corpus = corpus.lower()\n    corpus = corpus.split()\n    ps = PorterStemmer()\n    corpus = [ps.stem(word) for word in corpus if not word in eng_stopwords]\n    corpus = ' '.join(corpus)\n    corpus_text.append(corpus)","metadata":{"_cell_guid":"42accaac-2b96-4c16-a64b-3f79e1341aa9","_uuid":"b7b169c58a5356e69b9cdc75f6f9f1bf950826a2","collapsed":true},"cell_type":"code"},{"outputs":[],"execution_count":null,"source":"from sklearn.feature_extraction.text import CountVectorizer\ncv = CountVectorizer(max_features = 1500)","metadata":{"_cell_guid":"486a8f0b-ed8f-4d94-93e9-3ab22a6f61ab","_uuid":"83557c3c667f259d17f100edca48a5a4d65458e5","collapsed":true},"cell_type":"code"},{"outputs":[],"execution_count":null,"source":"X = cv.fit_transform(corpus_text).toarray()\ny = train.iloc[:, 2].values","metadata":{"_cell_guid":"71a316f0-fa4e-4428-9a70-bdacc0f31516","_uuid":"b2b6a5a97785f9a37e4a6e07a42acf8c1e96875f","collapsed":true},"cell_type":"code"},{"outputs":[],"execution_count":null,"source":"y","metadata":{"_cell_guid":"96b53c35-b2cd-4b5b-a55d-6812c9762823","_uuid":"88969e4710e8d659343554176c5c7ffbf71b36aa"},"cell_type":"code"},{"outputs":[],"execution_count":null,"source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0)","metadata":{"_cell_guid":"2244ef5b-3f00-439e-9ec5-2f39852c3f2b","_uuid":"e6381344d20753f7d57afde593be8d84500346ac","collapsed":true},"cell_type":"code"},{"outputs":[],"execution_count":null,"source":"# Lets use navies bayes classified (MultinomialNB)\nfrom sklearn.naive_bayes import MultinomialNB\nclassifier = MultinomialNB()\nclassifier.fit(X_train, y_train)","metadata":{"_cell_guid":"d13337ad-659b-4eab-92ba-3aeba1c9fbae","_uuid":"c91bf380f627ffb5160cc4e14585a0272b9ce1da"},"cell_type":"code"},{"outputs":[],"execution_count":null,"source":"y_pred = classifier.predict(X_test)","metadata":{"_cell_guid":"482a6b14-aa6e-4969-975e-1caf6ff0ca70","_uuid":"d56ad414934963494c879accfa017038e4637a85","collapsed":true},"cell_type":"code"},{"outputs":[],"execution_count":null,"source":"from sklearn.metrics import confusion_matrix, roc_curve\ncm = confusion_matrix(y_test, y_pred)","metadata":{"_cell_guid":"39410579-9be3-4e53-aca5-3668223ccbce","_uuid":"19d3861683d7f4fd726fbbcdbcb74cec1036b663","collapsed":true},"cell_type":"code"},{"outputs":[],"execution_count":null,"source":"cm","metadata":{"_cell_guid":"55a42609-d095-4414-a2e2-3d1ec065256e","_uuid":"4208bd3dbd144f007c3bc7c6ab9bd81d14f30c8b"},"cell_type":"code"},{"outputs":[],"execution_count":null,"source":"FP = cm.sum(axis=0) - np.diag(cm)  \nFN = cm.sum(axis=1) - np.diag(cm)\nTP = np.diag(cm)\nTN = cm.sum() - (FP + FN + TP)\n\n# Overall accuracy\nACC = (TP+TN)/(TP+FP+FN+TN)","metadata":{"_cell_guid":"e5a7318c-b8ba-4f32-8694-1f1e7dfba352","_uuid":"c0f9d544ec21c4da594fbf4911cc51f651a86f40","collapsed":true},"cell_type":"code"},{"outputs":[],"execution_count":null,"source":"ACC","metadata":{"_cell_guid":"1bdb5b83-aac2-496b-8c78-a05488cf1f7c","_uuid":"e60ca5f8b5275521ce132df593b7a3283d2802af"},"cell_type":"code"},{"metadata":{"_cell_guid":"7f35a973-37df-41ef-8f4c-0f1fde27354c","_uuid":"0ef25e7d8d1aea355343f15b64aca9e5988d5c12"},"source":"Since we have decent accuracy let me try to create the prediction on test output file.","cell_type":"markdown"},{"outputs":[],"execution_count":null,"source":"# Run the NLTK on Test dataset to create the corpus\ntest_corpus_text = []\nfor i in range(0, test.shape[0]):\n    corpus = test[\"text\"][i]\n    corpus = corpus.lower()\n    corpus = corpus.split()\n    ps = PorterStemmer()\n    corpus = [ps.stem(word) for word in corpus if not word in eng_stopwords]\n    corpus = ' '.join(corpus)\n    test_corpus_text.append(corpus)","metadata":{"_cell_guid":"d075759f-2bbe-4f2d-88c0-e5e582b3b298","_uuid":"5454a6888762e207d0cb32e3b7da772e980501b6","collapsed":true},"cell_type":"code"},{"outputs":[],"execution_count":null,"source":"X_test_output = cv.transform(test_corpus_text).toarray()","metadata":{"_cell_guid":"a8449afd-259a-4c41-9939-7b42b1b3bd13","_uuid":"e3d990578e95a3cab795d816b877e1df6d9c44ca","collapsed":true},"cell_type":"code"},{"outputs":[],"execution_count":null,"source":"y_prob_output = classifier.predict_proba(X_test_output)","metadata":{"_cell_guid":"7793003a-685f-467d-8232-78ab85490e2e","_uuid":"b04cd186882214663555a1af0bc36cd541b08cc5","collapsed":true},"cell_type":"code"},{"outputs":[],"execution_count":null,"source":"y_prob_output","metadata":{"_cell_guid":"972c4bc2-dfb2-47cf-8225-9bf955f078dc","_uuid":"f0e2a6cf9ca7a22761a6c841fcf7e15567b6dc16"},"cell_type":"code"},{"outputs":[],"execution_count":null,"source":"submission_df = pd.DataFrame(y_prob_output,index=test['id'],columns=['EAP','HPL','MWS'])","metadata":{"_cell_guid":"71f1ead8-bcbe-47d5-bf53-39b08e3d9ab5","_uuid":"c61997b8dc66ce9588082c037b4753e512ae3864","collapsed":true},"cell_type":"code"},{"outputs":[],"execution_count":null,"source":"submission_df.head()","metadata":{"_cell_guid":"e82d69b8-1144-4ce9-b712-40602c360bef","_uuid":"934c0b532e72f09ab893f81bac92b0e38ae32d7c"},"cell_type":"code"},{"outputs":[],"execution_count":null,"source":"submission_df.to_csv(\"submission.csv\")","metadata":{"_cell_guid":"c84c6c0b-bdad-483b-8680-bc287c835ae8","_uuid":"70c0ab87fef16e3c8a17153d7482c98221d50000","collapsed":true},"cell_type":"code"},{"metadata":{"_cell_guid":"ed10dc3b-8ba4-4d11-8f75-912219bf1750","_uuid":"0ed12f5e81a4878bdfbaed386c7f8a7f9c40b5a5","collapsed":true},"source":"Let me try decision tree using the same data and compare the accuracy","cell_type":"markdown"},{"outputs":[],"execution_count":null,"source":"from sklearn.ensemble import RandomForestClassifier","metadata":{"_cell_guid":"b02aba33-663f-4c59-92bf-9bce8a6c2561","_uuid":"ebd6cfc836e2ba9f1a2da6aef9502765299221e3","collapsed":true},"cell_type":"code"},{"outputs":[],"execution_count":null,"source":"classifier = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 1007)","metadata":{"_cell_guid":"cc08ee9c-7eec-44c8-85df-6d79edd20bae","_uuid":"33f8dda75eeb5e91285d1fc5ace08004d575267c","collapsed":true},"cell_type":"code"},{"outputs":[],"execution_count":null,"source":"classifier.fit(X_train, y_train)\ny_pred = classifier.predict(X_test)","metadata":{"collapsed":true},"cell_type":"code"},{"outputs":[],"execution_count":null,"source":"cm = confusion_matrix(y_test, y_pred)\ncm","metadata":{},"cell_type":"code"},{"outputs":[],"execution_count":null,"source":"FP = cm.sum(axis=0) - np.diag(cm)  \nFN = cm.sum(axis=1) - np.diag(cm)\nTP = np.diag(cm)\nTN = cm.sum() - (FP + FN + TP)\n\n# Overall accuracy\nACC = (TP+TN)/(TP+FP+FN+TN)","metadata":{"collapsed":true},"cell_type":"code"},{"outputs":[],"execution_count":null,"source":"ACC","metadata":{},"cell_type":"code"},{"metadata":{},"source":"It looks like Multilogistic reg algorithm  is doing much better than the random forest, stay tuned, i would be adding more classifier like SVM, Decision tree and xgboost. Please upvote to encourage!!","cell_type":"markdown"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"version":3,"name":"ipython"},"name":"python","mimetype":"text/x-python","pygments_lexer":"ipython3","version":"3.6.3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":1,"nbformat":4}