{"cells":[{"metadata":{"_cell_guid":"28445d0f-ccac-4a52-96de-833927f95700","_uuid":"dd46d2f885afef3adeb862468e3bc7834852c11a"},"cell_type":"markdown","source":"*Hello everyone, this is my first Kernel and I'm very motivated to work on this project! ***\n\n# **Introduction** : sngrams\n\nIn this tutorial, we will try to use the**  [#SNGRAMS ](https://pdfs.semanticscholar.org/4e5a/778e0e45a4bddb81916a33d8e3b380fbb836.pdf) **to find which author wrote which sentences ! Grigori Sidorov, Francisco Velasquez, Efstathios Stamatatos, Alexander Gelbukh and Liliana Chanona did the same research with the same number of authors : THREE ! Strange, isn't it ? \n\n# ** 1. What are sngrams ? **\n##  1.1. Stanford Parser\n\nSyntactic N-grams are using the output of the ** [ #Stanford Parser](http://nlp.stanford.edu:8080/parser/) ** which is a probabilistic parser that use knowledge of language. The parser exists in English and extrats groups of words that have a grammatical relation. The model is based on the structure of the sentences which depends of the language. \n\nThe output of this parser for a sentence like ' It never once occurred to me that the fumbling might be a mere mistake.' (which is the second sentence of our train set) would look like : \n\nnsubj(occurred-4, It-1)<br>\nneg(occurred-4, never-2)<br>\nadvmod(occurred-4, once-3)<br>\nroot(ROOT-0, occurred-4)<br>\ncase(me-6, to-5)<br>\nnmod(occurred-4, me-6)<br>\nmark(mistake-14, that-7)<br>\ndet(fumbling-9, the-8)<br>\nnsubj(mistake-14, fumbling-9)<br>\naux(mistake-14, might-10)<br>\ncop(mistake-14, be-11)<br>\ndet(mistake-14, a-12)<br>\namod(mistake-14, mere-13)<br>\nccomp(occurred-4, mistake-14)<br>\n\nThe following tree is also defined :\n![](https://raw.githubusercontent.com/BoltMaud/Kaggle_images/master/graphstanford.bmp)\n\n*This graphe was designed by the librairy : nltk.draw.tree. * \n*The name 'nsubj', 'neg' .. is the relation between the words in the parenthesis. The number indicates the position of the words in the sentence.   \n\n## 1.2. n-grams using the stanford parser\n\n### 1.2.1 Normal n-grams\nUsing the sentence 'It never once occurred to me that the fumbling might be a mere mistake' the normal 2-grams is : \n\nIt never ;  never once ;  once occurred ; occured to ;  to me ; me that ;  that the ;  the fumbling ; fumbling might ; might be ; be a ; a mere ;  mere mistake\n\nFor a 3-grams : \n\nIt never once ;never once occurred ; once occurred to ; occurred to me ;to me that ; me that the ;that the fumbling ;the fumbling  might ; fumbling  might be ; might be a ; be a mere ; a mere mistake\n \n ### 1.2.2 Sn-grams \n The sn-grams use the result of the stanford parser and gives :\n \n For a s2grams, the couples are all the couple from the roots :\n occured once, occured never, occured it , occured to, occured mistake, mistake mere,mistake a, mistake be, mistake might, mistake fumbling, mistake that, fumbing the, me to \n \n For a s3grams the (occured to) and (to me) become (occured to me) and (mistake fumbling) and (fumbling the) become (mistake fumbling the) \n \n # ** 2. How to use sngrams **\n \n I tried this solution on my computer but to generate all the features, the programme needed 5 hours. I decided to try without the syntaxic ngrams.\n \n # ** 3. Programme with ngrams ** \n \n "},{"metadata":{"_cell_guid":"261e93f5-1dda-4811-8e8b-95ef455ea184","_uuid":"889c9f81b2c443189cf620602ce412bf2b636c5e"},"cell_type":"markdown","source":"First, we import the dataset and the libs : "},{"metadata":{"_cell_guid":"7195c9b3-7782-4fe9-81f7-c382a75ad7f9","collapsed":true,"_uuid":"64e544ad7f624a97c71f2292d4047f3a77e28b53","trusted":false},"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\nimport pandas as pd\n\ndfTrain = pd.read_csv(\"../input/train.csv\") # importing train dataset\ndfTest = pd.read_csv(\"../input/test.csv\") # importing test dataset","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"84de8e9c-82b7-498a-b6a8-b1e43ed3b224","_uuid":"e08ac651131e14a854fe74aa5a25bba2fb290d08"},"cell_type":"markdown","source":"Then we create a matrix with the ngrams using TiDf. We delete the stop-word and accepte ngrams from 1 word to 3. \nThe function fit_tranform will tranform the data into a matrix and fit the model. "},{"metadata":{"_cell_guid":"a83ba9cd-9494-4138-8835-146159468137","collapsed":true,"_uuid":"c92344023b091ee0491b31716b3987b6c3c37a92","trusted":false},"cell_type":"code","source":"vectorizer = CountVectorizer(stop_words=\"english\",analyzer='word', ngram_range=(1,3))\ntrain_counts = vectorizer.fit_transform(dfTrain.text)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"237b16d3-7a13-4244-b8e8-de91fe2c88e1","_uuid":"7717f4a39bc5b82ff9668ff62df17ecb465f00e8"},"cell_type":"markdown","source":"We fit the model with the Multinomiale method because it's the best one for the problems using TiDf and ngrams."},{"metadata":{"_cell_guid":"eb638e63-872d-415a-9208-257f052e383e","collapsed":true,"_uuid":"b6b673f252d80c78efd61c310db9852cbbe4b697","trusted":false},"cell_type":"code","source":"classifier = MultinomialNB()\nclassifier.fit(train_counts, dfTrain.author)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"fdf46071-772a-4db4-add6-0d671f523096","_uuid":"78fcda81b5061764ad9dcbcab843710059d6021b"},"cell_type":"markdown","source":"The test set need to be transform too and the model is ready to predict : "},{"metadata":{"_cell_guid":"90d1c01b-bb55-4633-851b-f325a64a6e49","collapsed":true,"_uuid":"20cff2c59b1cfa37c422bf1a76746eb189c59761","trusted":false},"cell_type":"code","source":"tests_counts = vectorizer.transform(dfTest.text)\npredicted = pd.DataFrame(classifier.predict_proba(tests_counts) )","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"3e9c82db-3b44-4d2f-a278-5c7c3abc52e0","_uuid":"93dd7bc285a9678c555bd3f08d5874af9ad82d1a"},"cell_type":"markdown","source":"Finally, we prepare the submit file :"},{"metadata":{"scrolled":true,"_cell_guid":"8df0b583-8633-4513-8576-250632f30cf7","_kg_hide-output":false,"collapsed":true,"_uuid":"1990aefc7571f7d7044c2ab90293d04c27eb8ae6","trusted":false},"cell_type":"code","source":"submit=pd.DataFrame({})\nsubmit[\"id\"]=dfTest.id\nsubmit[\"EAP\"]=predicted[0]\nsubmit[\"HPL\"]=predicted[1]\nsubmit[\"MWS\"]=predicted[2]\n\nprint(submit)\nsubmit.to_csv(\"submit.csv\", sep=',',index=False)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"e4787ed4-68c1-41e2-9ab7-8478c4613d05","_uuid":"53645af1ac21c88e31a9aeaafc7162e1bd50eca0"},"cell_type":"markdown","source":"# ** 4. Vizualisation ** \n\nIn this part, I'm not sure to keep the rules but I didn't see anything that forbidd to use everything we know. \n\n## 4.1 LDA Vizualisation \n\nFirstly, I used knime to extract the words topics for each authors. I created a vizualisation with the colors of Halloween. The size of a word depend of its weight at the output of LDA algorithm. \n\n![](https://raw.githubusercontent.com/BoltMaud/Kaggle_images/master/viz1.png)\n\nThen I used [#Tropes](http://www.tropes.fr/) to get some interesting information about the grammar.\n\n** THE PRONOUNS **\n![](https://raw.githubusercontent.com/BoltMaud/Kaggle_images/master/pronouns.png)\n\n** THE CONNECTORS **\n![](https://raw.githubusercontent.com/BoltMaud/Kaggle_images/master/connectors_.png)\n\n** THE MODALIZATION ** \n![](https://raw.githubusercontent.com/BoltMaud/Kaggle_images/master/modalities_.png)\n\n\n\n"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}