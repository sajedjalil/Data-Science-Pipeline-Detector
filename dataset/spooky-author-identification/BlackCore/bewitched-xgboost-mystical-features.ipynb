{"cells":[{"source":"# A bewitched XGBoost for all the mad data scientists out there...\n\n![](https://i.pinimg.com/originals/1f/8b/ea/1f8bea26e9a5c7f39c3cdf4b3f8ba7d2.jpg)\n\nIn this notebook, I plan to experiment with some mystical features from EDA & NLP on the Spooky Author dataset, and obtain the magic formula for the most bewitched XGBoost ever seen!\n\n1. **Exploratory Data Analysis (EDA)** - Thorough data analysis and visualizations to better understand the writings of the three different authors: Edgar Allan Poe (EAP), HP Lovecraft (HPL), and Mary Wollstonecraft Shelley (MWS).\n\n2. **Natural Language Processing (NLP) ** - Leverage NLTK (Natural Language Toolkit) for doing some awesome text processing methods such as tokenizations, lemmatization, stemming, bag-of-words and get to the bottom of all the misteries!\n\n3. **Mystical feature engineering**\n\n4. **A bewitched XGBoost**\n---","metadata":{"_uuid":"075ab0f3fc310e293828b3681f1d80642f88c106","_cell_guid":"a3cb0ee3-7bca-4b2b-8a27-be198d18818e"},"cell_type":"markdown"},{"outputs":[],"source":"import base64\nimport numpy as np\nimport pandas as pd\nimport plotly.graph_objs as go\nimport plotly.tools as tls\nfrom collections import Counter\nfrom scipy.misc import imread\nimport xgboost as xgb\nimport seaborn as sns\nimport nltk\nimport string\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.decomposition import NMF, LatentDirichletAllocation, TruncatedSVD\nfrom sklearn import ensemble, metrics, model_selection, naive_bayes\nfrom matplotlib import pyplot as plt\n%matplotlib inline\n\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\n\ncolor = sns.color_palette()\n\npd.options.mode.chained_assignment = None","metadata":{"_uuid":"d1e0f3d0e6455394ae3470b8d861f7258dfe49b1","_cell_guid":"01b01111-7458-454b-825f-b0581431b79a","_kg_hide-input":true},"cell_type":"code","execution_count":null},{"source":"Now...let us take a glimpse right into the soul of this spooky dataset!","metadata":{"_uuid":"0a5186185f32fe24f92b09cefc4eba857386b515","_cell_guid":"88939009-cdfe-4efd-b51e-c276d3f090d4"},"cell_type":"markdown"},{"outputs":[],"source":"# Read training data with Pandas\ndf_train = pd.read_csv(\"../input/train.csv\")\ndf_test = pd.read_csv(\"../input/test.csv\")\nprint(\"Number of rows in train dataset : \",df_train.shape[0])\nprint(\"Number of rows in test dataset : \",df_test.shape[0])","metadata":{"_uuid":"95e5065366bde322d4ad610a9888fb9c187328f0","_cell_guid":"e7f8d60d-5cd0-48af-80f9-c8cb8f4f47b6"},"cell_type":"code","execution_count":null},{"source":"# 1. Exploratory Data Analysis (EDA)\n\nLet us understand some basic facts about the three authors: Edgar Allan Poe (EAP), HP Lovecraft (HPL), and Mary Wollstonecraft Shelley (MWS).","metadata":{"_uuid":"97ee6de5cf262e0dcff70fb9bc74466b1ffe1bd8","_cell_guid":"624cc24b-893e-470d-83b1-2a3f585f6394"},"cell_type":"markdown"},{"outputs":[],"source":"df_train.head()","metadata":{"_uuid":"b0c55dbcfcd55a3bfd7f5521161a2fb21f56eaa6","_cell_guid":"3bbe47be-677c-4925-8f4b-fcae4dfa792c","_kg_hide-input":true,"collapsed":true},"cell_type":"code","execution_count":null},{"outputs":[],"source":"df_train.describe()","metadata":{"_uuid":"3d1b58efed05242f6efb808461703195243afc52","_cell_guid":"363bfb64-d99d-4221-89f4-341aacc85e27","scrolled":true,"collapsed":true},"cell_type":"code","execution_count":null},{"outputs":[],"source":"df_train['text_len']=df_train[\"text\"].apply(lambda x: len(str(x)))\n\nplt.figure(figsize=(14,8))\nsns.violinplot(x=\"text_len\", y=\"author\", data=df_train, scale=\"width\")\nplt.ylabel('Author Name', fontsize=14)\nplt.xlabel('Text Length', fontsize=14)\nplt.show()","metadata":{"_uuid":"7ae93e9a94a36a62c08bd5fdf38aa32051741848","_cell_guid":"292a6eee-4c0e-4fd7-ac93-dd538f20579a","collapsed":true},"cell_type":"code","execution_count":null},{"source":"The above plot doesn't look so good because there are some really long blocks of text by Mary Shelley, that also contain a lot of stop words. We can filter out these cases and see that the majority of texts from the training data are clustered around 100 characters.\nIn this case, on average, Edgar Allan Poe's texts are the shortest, while HP Lovecraft's are the longest.","metadata":{"_uuid":"7b93e651ace66a83cd84348f72bf8c9f18b3da92","_cell_guid":"f3fbeea3-e7be-4b96-8b1c-e76306f413b4"},"cell_type":"markdown"},{"outputs":[],"source":"plt.figure(figsize=(14,8))\nsns.violinplot(x=\"text_len\", y=\"author\", data=df_train[df_train[\"text_len\"] < 400], scale=\"width\")\nplt.ylabel('Author Name', fontsize=14)\nplt.xlabel('Text Length', fontsize=14)\nplt.show()","metadata":{"_uuid":"cf7ece6a99179b1aeba2aa3723b967f0f4a040d7","_cell_guid":"27342859-0afa-4983-a6cc-7fa03b285095","collapsed":true},"cell_type":"code","execution_count":null},{"outputs":[],"source":"auth_cnt = df_train['author'].value_counts()\nauth_cnt.values\n\nplt.figure(figsize=(14,8))\nsns.barplot(auth_cnt.index, auth_cnt.values)\nplt.ylabel('Number of Occurrences', fontsize=14)\nplt.xlabel('Author Name', fontsize=14)\nplt.show()","metadata":{"_uuid":"e4b352a6052d6e4157e217622875a66522b609e9","_cell_guid":"19db8fdc-2a60-45d3-adb9-f5b75ebd040c","collapsed":true},"cell_type":"code","execution_count":null},{"source":"## Stopword Removal\n\nAs alluded to above stopwords are generally words that appear so commonly and at such a high frequency in the corpus that they don't actually contribute much to the learning or predictive process as a learning model would fail to distinguish it from other texts. Stopwordsinclude terms such as \"to\" or \"the\" and therefore, it would be to our benefit to remove them during the pre-processing phase. Conveniently, NLTK comes with a predefined list of 153 english stopwords.\n\nTo filter out stop words from our tokenized list of words, we can simply use a list comprehension as follows:","metadata":{"_uuid":"8e198307e5b932ec7042e5e31aca166f98a8628d","_cell_guid":"8d38240d-17bb-4b48-8779-e6f9e07e11d0"},"cell_type":"markdown"},{"outputs":[],"source":"from nltk.corpus import stopwords\nfrom nltk.tokenize import RegexpTokenizer\n\n# Needed to get rid of punctuation\ntokenizer = RegexpTokenizer(r'\\w+')\n# Searching a set is much faster than searching a list\neng_stopwords = set(stopwords.words(\"english\") + \n                ['one','us','yet','could','would','need','even','might','like',\n                 'must','every','never','go','thus','may','much','however'])\n\ndef cleanup_spooky_text( spooky_text ):\n    # 1. Convert to lower case, and tokenize (split) into individual words\n    spooky_words = tokenizer.tokenize(spooky_text.lower())\n    # 2. Remove stop words\n    meaningful_spooky_words = [w for w in spooky_words if not w in eng_stopwords]   \n    # 3. Join the words back into one string separated by space\n    return( \" \".join(meaningful_spooky_words))\n\nprint(eng_stopwords)","metadata":{"_uuid":"efc62650b3734bbf6fc2e7fb8eb53dcf77c6cfee","_cell_guid":"491a39fa-d379-43ea-8a1e-1c056cde434d","collapsed":true},"cell_type":"code","execution_count":null},{"outputs":[],"source":"df_train[\"text\"].head()","metadata":{"_uuid":"5b0a56d92f9bebc826c7b200d8315d56124043b8","_cell_guid":"058536fc-8144-4897-aa1d-85b499279cad","collapsed":true},"cell_type":"code","execution_count":null},{"outputs":[],"source":"df_train[\"clean_text\"] = df_train[\"text\"].apply(lambda x: cleanup_spooky_text(x))","metadata":{"_uuid":"1af12b5846cd35234582f6f80c41ee4b347ed95c","_cell_guid":"4ed67bc6-f5eb-4449-a273-acf71b0e0446","collapsed":true},"cell_type":"code","execution_count":null},{"outputs":[],"source":"df_train[\"clean_text\"].head()","metadata":{"_uuid":"f7c017ea4aee21d3240583761d3ff4bc2af29c1f","_cell_guid":"43176204-84ed-4965-a4d4-52f535408c84","collapsed":true},"cell_type":"code","execution_count":null},{"source":"We can then display again the above plot to show text length distributions for each author. You notice that now the majority of texts from the training data clusters have shifted more towards 60 characters in length:","metadata":{"_uuid":"2cb8e62e94d25d1d82f2446e3738702a0ed8b003","_cell_guid":"95e9968a-47db-40c6-bc12-6031eb94d9b6"},"cell_type":"markdown"},{"outputs":[],"source":"df_train['clean_text_len']=df_train[\"clean_text\"].apply(lambda x: len(str(x)))\n\nplt.figure(figsize=(14,8))\nsns.violinplot(x=\"clean_text_len\", y=\"author\", data=df_train[df_train[\"clean_text_len\"] < 300], scale=\"width\")\nplt.ylabel('Author Name', fontsize=14)\nplt.xlabel('Text Length', fontsize=14)\nplt.show()","metadata":{"_uuid":"3d27a2ff8981a530c633669bddfe2fc3e6e405c1","_cell_guid":"76c39783-d479-47bf-b356-bf2d8708991d","collapsed":true},"cell_type":"code","execution_count":null},{"source":"## Summary statistics of the training set\n\nHere we can visualize some basic statistics in the data, like the distribution of entries for each author. For this purpose, I will invoke the handy Plot.ly visualisation library and plot some simple bar plots. Unhide the cell below if you want to see the Plot.ly code.","metadata":{"_uuid":"abd739c97ab4ca7904aefa57658867d773974ab5","_cell_guid":"9aa595a9-06b2-4673-9cbc-5c5e7e35cc06"},"cell_type":"markdown"},{"outputs":[],"source":"all_words = df_train['clean_text'].str.split(expand=True).unstack().value_counts()\n\ndata = [go.Bar(\n            x = all_words.index.values[2:40],\n            y = all_words.values[2:40],\n            marker= dict(colorscale='Viridis',color = all_words.values[2:80]),\n            text='Word counts'\n    )]\n\nlayout = go.Layout(title='Top 40 Word frequencies in the cleansed training dataset')\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig)","metadata":{"_uuid":"21eaa1db422ced4253fa2472e29c99a4ab8124ba","_cell_guid":"82236bf0-454b-41ee-ae54-7c6b396e20d7","_kg_hide-input":false,"collapsed":true},"cell_type":"code","execution_count":null},{"source":"Not surprinsingly, most of the words that appear in this word frequency plot don't actually tell us much about the main themes and characters that the authors present to the reader.","metadata":{"_uuid":"58dd795e28474ff99c36b3196ee1767dcf206eed","_cell_guid":"311422ae-29c8-4694-9041-aa5c1601b8a0"},"cell_type":"markdown"},{"source":"## Separate the text of each author\n\nLet's create 3 lists to store the texts of Edgar Allan Poe, HP Lovecraft and Mary Shelley:","metadata":{"_uuid":"ccb59b90aad823eddc3a9969a55a3201b32b1dcf","_cell_guid":"5d44f754-4aff-4ce3-b7c2-b504c0ac9696"},"cell_type":"markdown"},{"outputs":[],"source":"EAP_text = df_train[df_train.author==\"EAP\"][\"clean_text\"].values\nHPL_text = df_train[df_train.author==\"HPL\"][\"clean_text\"].values\nMWS_text = df_train[df_train.author==\"MWS\"][\"clean_text\"].values","metadata":{"_uuid":"0f1c082b4dcaae4d7556a3b0dc126aad7004f0e0","_cell_guid":"a42a461b-3197-4b6d-8067-dfdb6e4faac5","collapsed":true},"cell_type":"code","execution_count":null},{"source":"## WordClouds to visualise the preferred spooky words of each author\n\n","metadata":{"_uuid":"fe3ad42b8d423aafe9190490f180db59c0669e8a","_cell_guid":"bbfe7efe-74a4-45ad-bfba-d926ed9819d4"},"cell_type":"markdown"},{"outputs":[],"source":"from wordcloud import WordCloud","metadata":{"_uuid":"52d17124aa7a9628ec382784c572a738b5bea7a8","_cell_guid":"4be02b75-9ec5-4167-895c-055cb679be55","collapsed":true},"cell_type":"code","execution_count":null},{"outputs":[],"source":"plt.figure(figsize=(20,20))\nplt.subplot(211)\nwc = WordCloud(background_color=\"black\", max_words=100, \n               stopwords=eng_stopwords, max_font_size= 40)\nwc.generate(\" \".join(EAP_text))\nplt.title(\"Edgar Allan Poe\\n\", fontsize=30)\nplt.imshow(wc.recolor(colormap= 'viridis', random_state=17))\nplt.axis('off')","metadata":{"_uuid":"32c76cbcd20330f3c94d9bb3a53a321a8214f65c","_cell_guid":"0ff743de-93ff-4d24-9c93-cc4dc1bb881c","collapsed":true},"cell_type":"code","execution_count":null},{"source":"Among Edgar Allan Poe's preffered words are: little, night, eye, day, thing, thought, matter.","metadata":{"_uuid":"53d42211057b33c4a5fbad7c1cbe2603046c2f2f","_cell_guid":"bdaf20fe-5594-4745-8533-5418d8aa4725"},"cell_type":"markdown"},{"outputs":[],"source":"plt.figure(figsize=(20,20))\nwc = WordCloud(background_color=\"black\", max_words=100, \n               stopwords=eng_stopwords, max_font_size= 40)\nwc.generate(\" \".join(HPL_text))\nplt.title(\"HP Lovecraft\\n\", fontsize=30)\nplt.imshow(wc.recolor(colormap= 'viridis', random_state=17))\nplt.axis('off')","metadata":{"_uuid":"9a7b35b444af6f9846803f535b194e14db84ee5c","_cell_guid":"b63b4921-728c-487a-9e23-5c6fd356df6c","_kg_hide-input":true,"collapsed":true},"cell_type":"code","execution_count":null},{"source":"Similar to Poe,  you can see that HP Lovecraft's favours words such as: night, strange, street, thing, dream, time. These go hand in hand with the idea of the “unnameable”, the “unspeakable”, and the “indescribable, as well as the ancient cults associated with the mystic Cthulhu creature.","metadata":{"_uuid":"c583bd2f618789ac8b42a98a0c14f89c06d46365","_cell_guid":"a4731e63-6337-4e9a-9375-3fee60368dd1"},"cell_type":"markdown"},{"outputs":[],"source":"plt.figure(figsize=(20,20))\nwc = WordCloud(background_color=\"black\", max_words=100, \n               stopwords=eng_stopwords, max_font_size= 40)\nwc.generate(\" \".join(MWS_text))\nplt.title(\"Mary Shelley\\n\", fontsize= 30)\nplt.imshow(wc.recolor(colormap= 'viridis' , random_state=17))\nplt.axis('off')","metadata":{"_uuid":"b7c28510584bc3e0bd8d64239c9b5c1aec171728","_cell_guid":"e44baa42-41cd-4bbf-9591-0c309e8b716d","_kg_hide-input":true,"collapsed":true},"cell_type":"code","execution_count":null},{"source":"On the other hand, we can see that Mary Shelley preffers more optimistic words: life, hope, friend, love, thought, heart. Also, the character Raymond appears quite a bit in the texts.","metadata":{"_uuid":"e620c92670a50391a1d8378e0485aee8089535b6","_cell_guid":"0429939f-eee2-4493-8254-c6bc61ddae12"},"cell_type":"markdown"},{"source":"Let's display a plot to show the distribution of number of words from texts for each author. Given that we're using the texts without stop words, you can notice that the majority clusters around 10 words:","metadata":{"_uuid":"f02fce537a4874e722ca4073fa3b3ba7871a5690","_cell_guid":"d2e6f78e-b9ed-4de2-97be-d48019a2adb7"},"cell_type":"markdown"},{"outputs":[],"source":"## Number of words in the text ##\ndf_train[\"num_words\"] = df_train[\"clean_text\"].apply(lambda x: len(str(x).split()))\n\nplt.figure(figsize=(14,8))\nsns.violinplot(x='num_words', y='author', data=df_train[df_train[\"num_words\"] <= 30])\nplt.xlabel('Number of words in text', fontsize=14)\nplt.ylabel('Author Name', fontsize=14)\nplt.title('Number of words by author', fontsize=14)\nplt.show()","metadata":{"_uuid":"dfdb5bcd1cb2cf9275fa53e233dd87298817a366","_cell_guid":"263dc0b2-0991-4e63-81d3-3c9f77279cfb","collapsed":true},"cell_type":"code","execution_count":null},{"outputs":[],"source":"df_train[\"num_puncts\"] = df_train['text'].apply(lambda x: len([c for c in str(x) \\\n                                                                     if c in string.punctuation]) )\nplt.figure(figsize=(14,8))\nsns.violinplot(x='num_puncts', y='author', data=df_train[df_train['num_puncts'] <= 10])\nplt.xlabel('Author Name', fontsize=14)\nplt.ylabel('Number of punctuations by author', fontsize=14)\nplt.title('Number of puntuations in text', fontsize=14)\nplt.show()","metadata":{"_uuid":"c436463954cf0ab0235edbaedd1c202eec70d728","_cell_guid":"40e71706-02b5-4f8a-be17-477254e0bdec","collapsed":true},"cell_type":"code","execution_count":null},{"source":"# 2. Natural Language Processing\n\n**Natural Language Toolkit (NLTK)** is a comprehensive Python library for natural language processing and text analytics. More information can be found at http://www.nltk.org/, and demos of select NLTK functionality and production-ready APIs are available at http://text-processing.com.\n\nThe main NLP & text pre-processing steps as as follows:\n\n1. **Tokenization** - is the process of splitting a string into a list of pieces or tokens. A token is a piece of a whole, so a word is a token in a sentence, and a sentence is a token in a paragraph.\n\n2. **Stopwords** - are common words that generally do not contribute to the meaning of a sentence or text. Please have a look again at the *cleanup_spooky_text* function and the *eng_stopwords* dataset used in the first chapter.\n\n3. **Stemming**  - is a technique to remove affixes from a word, ending up with the stem. For example, the stem of *thinking* is *think*, and a good stemming algorithm knows that the ing suffix can be removed.\n\n4. **Lemmatization** is very similar to stemming, but is more akin to synonym replacement.\n\n5. **Bag-of-words feature extraction** - Text feature extraction is the process of transforming what is essentially a list of words into a feature set that is usable by a classifier.\n\n\n\n","metadata":{"_uuid":"245126697b14d9e76cb43924c71ec02a1ad76ebf","_cell_guid":"9dc7e717-a262-48c1-8684-b0fca2398d35"},"cell_type":"markdown"},{"source":"## Tokenization\n\n I've already leveraged the *NLTK RegexpTokenizer* in the first section when removing stopwords. Below you have two examples, one with a simple word tokenizer and the other one with the RegexpTokenizer:","metadata":{"_uuid":"3d3ac7816eee311d2016a48bb681f3fb6556bddf","_cell_guid":"6a9618ba-2ec0-4a6f-8169-060b7bed5427"},"cell_type":"markdown"},{"outputs":[],"source":"from nltk.tokenize import word_tokenize, RegexpTokenizer\n\nsample_text = df_train.text[0]\n\nprint('Word Tokenizer output: ' + str(word_tokenize(sample_text)) + \"\\n\")\n\ntokenizer = RegexpTokenizer(r'\\w+') # Keep only words by removing punctuation\nprint('RegexpTokenizer output: ' + str(tokenizer.tokenize(sample_text)))","metadata":{"_uuid":"12c4f0926f03ef50994829b9da509683b62d6c06","_cell_guid":"d4494207-f146-4e40-8a77-7ef2a938df43"},"cell_type":"code","execution_count":null},{"source":"## Stemming and Lemmatization\n\nOne of the most common stemming algorithms is the Porter stemming algorithm by Martin Porter. It is designed to remove and replace well-known suffixes of English words.\nBelow I have created a Porter stemmer instance:","metadata":{"_uuid":"265c9f86933af289e043337db3462bd02845c6c1","_cell_guid":"7495e09a-e8b6-4e7b-bdbd-cd75a3ce27a6"},"cell_type":"markdown"},{"outputs":[],"source":"stemmer = nltk.stem.PorterStemmer()\nprint(\"The stemmed form of thinking is: {}\".format(stemmer.stem(\"thinking\")))","metadata":{"_uuid":"9212fd761ea6c97b2b801bcd3b478d83e6b5f82e","_cell_guid":"1efb1808-e2e2-4c7d-8ccb-06490de56a5d"},"cell_type":"code","execution_count":null},{"source":"A lemma is a root word, as opposed to the root stem. So unlike stemming, you are always left with a valid word that means the same thing. However, the word you end up with can be completely different.","metadata":{"_uuid":"9ea2b6d17255ff5a421213b58722a0c8174facbd","_cell_guid":"a051ac61-9948-44f1-b5fc-112c77a851c8"},"cell_type":"markdown"},{"outputs":[],"source":"from nltk.stem import WordNetLemmatizer\nlemm = WordNetLemmatizer()\nprint(\"The lemmatized form of believes is: {}\".format(lemm.lemmatize(\"believes\")))","metadata":{"_uuid":"fd50d5b556e74060707dcc26dd2599b179db4f31","_cell_guid":"afdc99cc-a30d-40ba-8cba-a4f72637fd6e"},"cell_type":"code","execution_count":null},{"source":"## Bag-of-words feature extraction\n\nThe main goal here is to transform lists of words into feature sets. The bag of words model is the simplest method; it constructs a word presence feature set from all the words of an instance. This method doesn't care about the order of the words, or how many times a word occurs, all that matters is whether the word is present in a list of words.","metadata":{"_uuid":"2ace49576cca7bf5476d2a7e7989e0be76394701","_cell_guid":"fa4f6d1f-eb70-4325-84d1-92212259f98c"},"cell_type":"markdown"},{"outputs":[],"source":"sentence = [\"This process, however, afforded me no means of ascertaining the dimensions of my dungeon; as I might make its circuit, and return to the point whence I set out, without being aware of the fact; so perfectly uniform seemed the wall.\"]\nvectorizer = CountVectorizer(min_df=0)\nsentence_transform = vectorizer.fit_transform(sentence)\n\nprint(\"The features are:\\n {}\".format(vectorizer.get_feature_names()))\nprint(\"\\nThe vectorized array looks like:\\n {}\".format(sentence_transform.toarray()))","metadata":{"_uuid":"e63eb93cf0e6126d01429f4783981d0afabc6994","_cell_guid":"d6883542-1769-451e-98af-a327db924b93","collapsed":true},"cell_type":"code","execution_count":null},{"source":"# 3. Mystical Feature Engineering\n\nFeature engineering is agreed by kagglers to be key to success in applied machine learning. There are two main types of features:\n\n- **Text based features** - in our case, directly extracted from the available text: word frequency, specific sentiments, word2vec, TF-IDF etc.\n- **Meta features** - numerical statistics extracted from the available text: number of characters, number of words, number of stop words, number of punctuations etc\n\n","metadata":{"_uuid":"1dc4cb125b48dec5c46842ef7a2cf8c5abe489d3","_cell_guid":"636fe52f-a72e-413b-abfa-385926baff87"},"cell_type":"markdown"},{"outputs":[],"source":"df_test[\"clean_text\"] = df_test[\"text\"].apply(lambda x: cleanup_spooky_text(x))\n\n## TEXT-BASED FEATURES - TODO\n## Based on the links from this topic: https://www.kaggle.com/c/spooky-author-identification/discussion/42925\n\n## META-FEATURES\n## Number of characters in the text\ndf_train[\"nb_chars\"] = df_train[\"text\"].apply(lambda x: len(str(x)))\ndf_test[\"nb_chars\"] = df_test[\"text\"].apply(lambda x: len(str(x)))\n\n## Number of words in the text\ndf_train[\"nb_words\"] = df_train[\"text\"].apply(lambda x: len(str(x).split()))\ndf_test[\"nb_words\"] = df_test[\"text\"].apply(lambda x: len(str(x).split()))\n\n## Number of relevant words in the text (stop words removed)\ndf_train[\"nb_rel_words\"] = df_train[\"clean_text\"].apply(lambda x: len(str(x).split()))\ndf_test[\"nb_rel_words\"] = df_test[\"clean_text\"].apply(lambda x: len(str(x).split()))\n\n## Number of unique words in the text (stop words removed)\ndf_train[\"nb_uniq_words\"] = df_train[\"clean_text\"].apply(lambda x: len(set(str(x).split())))\ndf_test[\"nb_uniq_words\"] = df_test[\"clean_text\"].apply(lambda x: len(set(str(x).split())))\n\n## Number of stopwords in the text\ndf_train[\"nb_stopwords\"] = df_train[\"text\"].apply(lambda x: len([w for w in str(x).lower().split() if w in eng_stopwords]))\ndf_test[\"nb_stopwords\"] = df_test[\"text\"].apply(lambda x: len([w for w in str(x).lower().split() if w in eng_stopwords]))\n\n## Number of punctuations in the text\ndf_train[\"nb_punct\"] =df_train['text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]) )\ndf_test[\"nb_punct\"] =df_test['text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]) )","metadata":{"_uuid":"a556623367f6837e10674ab6370bf01044f83b21","_cell_guid":"9205fa99-153c-4383-9072-498fc8c8ccf0","collapsed":true},"cell_type":"code","execution_count":null},{"source":"## Data pre-processing","metadata":{"_uuid":"c48aeef87208a8685f261352f18dbbd1bbe330e0","_cell_guid":"ee0d02cc-c91a-4b3b-b34a-14edf0d0b3aa"},"cell_type":"markdown"},{"outputs":[],"source":"## Prepare the data for modeling\n## TODO","metadata":{"_uuid":"f648e42baafdead7dee89eb1e031bb9cccd92653","_cell_guid":"6e7b34be-f0bd-4613-809d-5cd545ce9b49","collapsed":true},"cell_type":"code","execution_count":null},{"source":"# 4. A bewitched XGBoost","metadata":{"_uuid":"b1c62bb0ded21958ca261d84e8dbbb7c250486ae","_cell_guid":"616780a2-91d7-427b-bd1e-097efa882065"},"cell_type":"markdown"},{"outputs":[],"source":"## TODO\ndef bewitched_XGB(train_X, train_y, test_X, test_y):\n    \n    xgb_params = {\n    'seed': 20171106,\n    'colsample_bytree': 0.8,\n    'silent': 1,\n    'subsample': .85,\n    'eta': 0.04,\n    'objective': 'multi:softprob',\n    'num_parallel_tree': 7,\n    'max_depth': 5,\n    'min_child_weight': 10,\n    'nthread': 22,\n    'num_class': 3,\n    'eval_metric': 'mlogloss',\n    }\n    \n    num_rounds = 1000\n    xgtrain = xgb.DMatrix(train_X, label=train_y)","metadata":{"_uuid":"45ec2c255ece478cd596192ccdad7517a49d8b5d","_cell_guid":"f383f889-ed77-4f6a-a3ab-def6a0aa19e8","collapsed":true},"cell_type":"code","execution_count":null},{"source":"**I will update this after experimenting with text based features created based on the information presented in this discussion topic: https://www.kaggle.com/c/spooky-author-identification/discussion/42925**\n\nPlease upvote if you found this useful!\n\nCheers, Andrei","metadata":{"_uuid":"f08ad908820b08d8526f335af9fe148320645b3a","_cell_guid":"69bec4ea-1e66-4608-b6b4-59c1aefa2af2"},"cell_type":"markdown"}],"metadata":{"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","codemirror_mode":{"version":3,"name":"ipython"},"version":"3.6.3","mimetype":"text/x-python","name":"python","file_extension":".py"},"kernelspec":{"name":"python3","language":"python","display_name":"Python 3"}},"nbformat_minor":1,"nbformat":4}