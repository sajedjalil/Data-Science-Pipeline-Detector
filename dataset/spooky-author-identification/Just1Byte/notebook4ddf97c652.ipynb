{"nbformat":4,"nbformat_minor":1,"cells":[{"outputs":[],"cell_type":"code","execution_count":null,"source":"import numpy as np \nimport pandas as pd \n\nimport matplotlib.pyplot as plt","metadata":{"_cell_guid":"ce70a204-4af0-4636-9cf8-c043c83c7508","collapsed":true,"_uuid":"28e0362b8b140ee55b7060fd4d2a5f80a5dd13d3"}},{"outputs":[],"cell_type":"code","execution_count":null,"source":"test_corpus = np.load('../input/preprocessing/test_corp.npy')\ntrain_corpus = np.load('../input/preprocessing/train_corp.npy')\n\nglove_table = pd.read_csv('../input/preprocessing/filled_glove_table.csv', index_col=0)\nglove_table.describe()\n\ntrain_data = pd.read_csv('../input/spooky-author-identification/train.csv')","metadata":{"_cell_guid":"3a82955e-fc19-4103-9cb1-114f1321bf7d","collapsed":true,"_uuid":"e20fa85c56d3b8ce6ae61e13bab21e69ba798b8f","scrolled":true}},{"outputs":[],"cell_type":"code","execution_count":null,"source":"glove_table.loc[['man','woman','man']].as_matrix().shape\n\ntrain_data.iloc[477]","metadata":{"_cell_guid":"0bf40746-3fc0-45de-bc38-5cbf3370aa58","_uuid":"93c332817c8c891b24cc8e1bc95e61b11a52a846"}},{"outputs":[],"cell_type":"code","execution_count":null,"source":"train_sequence = []\ntrain_sequence_lengths   = []\nindex = 0\nfor sentence_chunk in train_corpus:\n    if (len(sentence_chunk) > 0):\n        words = [word for (word, tag) in sentence_chunk]\n        features = glove_table.loc[words].as_matrix()\n        train_sequence.append(features)\n        train_sequence_lengths.append(len(words))\n    else:\n        # If sentence is empty put (1,ndims) zeros as data\n        print(train_data['text'][index])\n        train_sequence.append(np.zeros((1,300)))\n        train_sequence_lengths.append(1)\n    index += 1\n    \ntrain_sequence_lengths = np.array(train_sequence_lengths)","metadata":{"_cell_guid":"20c5b998-590c-4d8a-aef0-2fe9ee569661","_uuid":"2403ce21ec7d5a24d3727ab627c2222b36b3977c"}},{"outputs":[],"cell_type":"code","execution_count":null,"source":"from hmmlearn import hmm\ndef hmm_train_model(train_sequence, train_sequence_lengths):\n    num_hidden_states = 10\n    num_dims = train_sequence[0].shape[1]\n    \n    model = hmm.GaussianHMM(n_components=num_hidden_states, covariance_type=\"full\",\n                            n_iter=20, verbose=True, init_params = \"\")\n\n    start_prob = np.random.rand(num_hidden_states)\n    start_prob = start_prob / np.sum(start_prob)\n    model.startprob_ = start_prob\n\n    transmat = np.random.rand(num_hidden_states,num_hidden_states)\n    transmat = transmat / np.sum(transmat, axis=1)[:, np.newaxis]\n    model.transmat_ = transmat\n\n    model.means_ = np.random.rand(num_hidden_states, num_dims)\n    model.covars_ = np.tile(np.identity(num_dims), (num_hidden_states, 1, 1))\n    print('Training started.')\n    return model.fit(np.concatenate(train_sequence), train_sequence_lengths) ","metadata":{"_cell_guid":"ad8a4c24-43fb-462a-883a-7646f145c1d3","collapsed":true,"_uuid":"ff48ee2a2af44d3a52053da5a16ca562fbca4bdd"}},{"outputs":[],"cell_type":"code","execution_count":null,"source":"import warnings\n\n# List of authors\nauthors = np.unique(train_data['author'])\nauthor_models = {} # Stores 1 Model for each author\n\nfor author in authors:\n    print('Training for {}:'.format(author))\n    \n    # Filter sentences of the same author :D\n    is_author_sentence = np.array(train_data['author'] == author)\n    author_sentences = [train_sequence[i] if is_author_sentence[i] else None for i in range(len(train_sequence))]\n    author_sentences = list(filter(lambda x: x is not None,author_sentences))\n    \n    author_sentence_lengths = train_sequence_lengths[is_author_sentence]\n    \n    # Train for author\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"ignore\")\n        author_models[author] = hmm_train_model(author_sentences, author_sentence_lengths)","metadata":{"_cell_guid":"5baa0af7-a9c9-4263-937a-0efc2dc7d4cb","_uuid":"6fad0f1c039296ab8f6ebc076a5d8973469fe51c"}},{"outputs":[],"cell_type":"code","execution_count":null,"source":"def predict(author_models, sentence):\n    scores = {}\n    predicted_author = list(author_models.keys())[0]\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"ignore\")\n        for author in author_models.keys():\n            scores[author] = author_models[author].score(sentence)\n            if (scores[author] > scores[predicted_author]):\n                predicted_author = author\n    return predicted_author, scores\n\ncorrect_prediction = 0\ntotal_prediction = 0\nfor index in range(1000):\n    prediction, scores = predict(author_models, train_sequence[index]) \n    true_data = train_data.iloc[index]\n    if (true_data['author'] == prediction):\n        correct_prediction += 1\n    total_prediction += 1\n    #print(\"{}\\nPrediction: {}\\tTrue: {}\\n\".format(true_data['text'], prediction, true_data['author']))\n    \nacc = float(correct_prediction) / total_prediction\nprint(acc)","metadata":{"_cell_guid":"40b9aaeb-5da4-4d23-800e-929e18d40ca6","_uuid":"468b10ec259510f0a180b2e91ad8dc140dd3744d"}},{"outputs":[],"cell_type":"code","execution_count":null,"source":"","metadata":{"_cell_guid":"fa507baa-00cc-4692-bd89-cf9396875033","collapsed":true,"_uuid":"2be8a6393410d151126a573e38250135469116a4"}}],"metadata":{"kernelspec":{"language":"python","name":"python3","display_name":"Python 3"},"language_info":{"mimetype":"text/x-python","codemirror_mode":{"version":3,"name":"ipython"},"file_extension":".py","nbconvert_exporter":"python","name":"python","pygments_lexer":"ipython3","version":"3.6.3"}}}