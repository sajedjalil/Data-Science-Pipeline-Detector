{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"version":3,"name":"ipython"},"version":"3.6.3","pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py","mimetype":"text/x-python","name":"python"}},"cells":[{"metadata":{"_cell_guid":"555bc9f8-187d-4b5b-9866-dfac8cf572c5","collapsed":true,"_uuid":"0d6476f1f9825b6d5fd7ba738473445726e8a44b"},"outputs":[],"execution_count":null,"source":"import numpy as np\nimport pandas as pd\n\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import wordpunct_tokenize\n\nimport csv\n\nfrom gensim.models import word2vec\nimport gensim.models\n","cell_type":"code"},{"metadata":{"_cell_guid":"4e6e58c2-f435-4eb6-949f-948e6c4ee44e","collapsed":true,"_uuid":"905c7154d57c457d7c4e7d221038d016a0d38cd2"},"outputs":[],"execution_count":null,"source":"# Read input data\ntrain_data = pd.read_csv(\"../input/spooky-author-identification/train.csv\")\ntest_data  = pd.read_csv(\"../input/spooky-author-identification/test.csv\")\n\ntrain_data.describe()","cell_type":"code"},{"metadata":{"_cell_guid":"2b943e6c-4e55-4e20-a8f3-a33bfd7197ee","collapsed":true,"_uuid":"b4c8ac392b51a4628bd214531aa19494e26b5390"},"outputs":[],"execution_count":null,"source":"\ndef preprocess_text(text, remove_list):   \n    '''\n    tokens = nltk.pos_tag(nltk.word_tokenize(text))\n    print(tokens)\n    good_words = [w for w, wtype in tokens if wtype not in remove_list]\n    print(good_words)\n    '''\n    \n    def clean_word(word):\n        word = word.lower()\n        if (len(word) > 1 and word[0] == '\\''):\n            return word[1:]\n        return word\n            \n    tokens = nltk.pos_tag(nltk.word_tokenize(text))\n    tokens = [(clean_word(word), pos) for (word,pos) in tokens]\n    return [(word, pos) for (word,pos) in tokens if word not in remove_list]\n\ndef preprocess_corpus(corpus):\n    \n    stop_words = set(stopwords.words('english'))\n    stop_words.update(['.', ',', '\"', \"'\", '?', '!', ':', ';', '(', ')', '[', ']', '{', '}']) # remove it if you need punctuation \n    \n    return [preprocess_text(text, stop_words) for text in corpus]\n\npreprocessed_train_corpus = preprocess_corpus(train_data['text'])\npreprocessed_test_corpus  = preprocess_corpus(test_data['text'])\n\nprint(preprocessed_train_corpus[0])","cell_type":"code"},{"metadata":{"_cell_guid":"918fa752-ae8e-4011-aaab-c5be8df8935d","collapsed":true,"_uuid":"7becb21ca2b45acc1524c459179105341e508b40"},"outputs":[],"execution_count":null,"source":"# GLOVE\ndef extract_vocabulary(corpus, vocabulary = set()):\n    for sentence_chunk in corpus:\n        for word,tag in sentence_chunk:\n            vocabulary.add(word)\n    return vocabulary\n            \ndef extract_glove_mapping(vocabulary):   \n    glove_table_path = '../input/glove.42b.300d/glove.42B.300d.txt'\n    glove_table = pd.read_table(glove_table_path, sep=' ', index_col=0, header = None, quoting = csv.QUOTE_NONE)\n    \n    extracted_table = glove_table.loc[list(vocabulary)]\n    \n    del glove_table\n    return extracted_table\n\nvocabulary = extract_vocabulary(preprocessed_train_corpus)\nvocabulary = extract_vocabulary(preprocessed_test_corpus, vocabulary)\nglove_table = extract_glove_mapping(vocabulary)\nglove_table.to_csv('glove_table.csv')","cell_type":"code"},{"metadata":{"_cell_guid":"1be33a31-b117-436f-9184-768ef9da168a","collapsed":true,"_uuid":"059f75f92478fafa7f13f67fb9fb32b35c28b7eb","scrolled":true},"outputs":[],"execution_count":null,"source":"def glove_fill_missing(glove_table):\n    # Find words with missing glove vecs\n    missing_data = glove_table[glove_table.isnull().any(axis=1)]\n    missing_words = list(missing_data.index)\n\n    # Train Word2Vec from whole data\n    sentences = []\n    for sentence_chunk in preprocessed_train_corpus:\n        sentences.append([word for (word, tag) in sentence_chunk])\n\n    for sentence_chunk in preprocessed_test_corpus:\n        sentences.append([word for (word, tag) in sentence_chunk])\n\n    model = word2vec.Word2Vec(sentences,size=100, min_count=1)\n\n    word2vec_model = model.wv\n    del model\n    \n    # \n    def generate_glove_missing(missing_word,k = 5):\n        similar_words = word2vec_model.similar_by_word(missing_word, topn=k)\n        similar_words = [similar_word for (similar_word, similarity) in similar_words]\n        glove_vec = np.nanmean(glove_table.loc[similar_words].as_matrix(),axis=0)\n        return glove_vec\n    \n    for missing_word in missing_words:\n        glove_vec = generate_glove_missing(missing_word)\n        glove_table.loc[missing_word] = glove_vec\n        \n    return glove_table\n    \nglove_table =  glove_fill_missing(glove_table)","cell_type":"code"},{"metadata":{"_cell_guid":"7f6578d8-2a14-474c-be1b-86d07f14b249","collapsed":true,"_uuid":"b05144519374ed51e6fe4066ea786dddafc947f5"},"outputs":[],"execution_count":null,"source":"glove_table.to_csv('filled_glove_table.csv')\n","cell_type":"code"},{"metadata":{"_cell_guid":"99724f58-e534-45e0-9be0-f9bf43a60965","collapsed":true,"_uuid":"9a73b9d2dddbf409c93d8572dbbe910142c1d9e8"},"outputs":[],"execution_count":null,"source":"np.save(\"train_corp.npy\", np.array(preprocessed_train_corpus))\nnp.save(\"test_corp.npy\", np.array(preprocessed_test_corpus))","cell_type":"code"},{"metadata":{"_cell_guid":"89189d4c-395c-4450-bafe-b38e3818ff16","collapsed":true,"_uuid":"3aa7c74b43370d7425a0d14f995c123abe98b48b"},"outputs":[],"execution_count":null,"source":"","cell_type":"code"}],"nbformat":4,"nbformat_minor":1}