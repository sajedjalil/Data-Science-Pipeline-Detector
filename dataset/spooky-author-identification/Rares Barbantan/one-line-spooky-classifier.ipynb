{"nbformat_minor":1,"cells":[{"execution_count":null,"outputs":[],"metadata":{"_kg_hide-output":false,"_kg_hide-input":true,"_uuid":"031635f673b13d03c71be09e0b125a2b243977d9","_cell_guid":"528b1e4a-7eaf-42f9-9df1-c3a2bf60936e","collapsed":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom imblearn.pipeline import make_pipeline\n"},{"execution_count":null,"outputs":[],"metadata":{"_uuid":"b5b15d11e02443d3dab478a4f5fd2b5d44966c72","_cell_guid":"60913ce8-81f6-480f-b3b1-51b792e66799","collapsed":true},"cell_type":"code","source":"pd.concat([pd.read_csv(\"../input/sample_submission.csv\")['id'],pd.DataFrame(make_pipeline(CountVectorizer(), TfidfTransformer(), SGDClassifier(loss='log', penalty='l2', alpha=1e-3, max_iter=10, random_state=42)).fit(*np.split(pd.read_csv(\"../input/train.csv\")[['text','author']].T.values.flatten(), 2)).predict_proba(pd.read_csv(\"../input/test.csv\")['text']), columns=['EAP','HPL','MWS'])], axis=1).to_csv('submission.csv', sep=',',index=False)"},{"cell_type":"markdown","metadata":{"_uuid":"3a733c21d576a4be25616fc64611d8424f421b9f","_cell_guid":"e54039bb-b836-4d6d-9440-705bb01df1f4"},"source":"**\\*record scratch\\* **\n\n**\\*freeze frame\\***\n\n*Yup, that's my code. You're probably wondering how I ended up in this situation.*\n\n\n### So, let's break it down:\n\n*(ok, so I lied a bit in the title: if you also count the imports, it's not really a one-line solution. Sorry about that!*\n\nFirst, we read in the train data (we are only interested in the **text** and **author** columns):"},{"execution_count":null,"outputs":[],"metadata":{"_uuid":"a0d2da098096ea99b65089042ba83559c0c2c309","_cell_guid":"36edc993-3b6b-4f37-b4c2-086bad2a9a55","collapsed":true},"cell_type":"code","source":"train = pd.read_csv(\"../input/train.csv\")\ntrain = train[['text','author']]"},{"cell_type":"markdown","metadata":{"_uuid":"a968bae03fbf5e299695f77e3a17d8593326ca84","_cell_guid":"8ac3629d-6f76-41b6-b221-599f568a1379"},"source":"To classify all the given texts based on their authors, we use the **sklearn** library.\nWe define a *pipeline* that does three things:\n* preprocess, tokenize and filter stopwords, basically transforming texts into feature vectors (**CountVectorizer**)\n* compute the *Term Frequency times Inverse Document Frequency* or tf-idf (**TfidfTransformer**)\n* train a linear classifier with stochastic gradient descent (SGD) learning (**SGDClassifier**)"},{"execution_count":null,"outputs":[],"metadata":{"_uuid":"2dd0b24423c7e69edf53ee48fb192a8091b112cf","_cell_guid":"82e45e99-2070-45db-b5db-09c9f8ccc035","collapsed":true},"cell_type":"code","source":"classifier_pipeline = make_pipeline(\n    CountVectorizer(), \n    TfidfTransformer(), \n    SGDClassifier(loss='log', penalty='l2', alpha=1e-3, max_iter=10, random_state=42)\n)"},{"cell_type":"markdown","metadata":{"_uuid":"f2685fc0b3d01feea55e8fc67a25d6d46d5d3b57","_cell_guid":"de69e7e5-1c22-4170-a97c-bc775c573cc9"},"source":"Our pipeline needs to be trained using a pair of example inputs and outputs. These are our two columns in the *train* data frame. So we do a little trick to pass the two pandas columns as parameters to our *fit* function. \nWe flatten the two columns so that the first half of the resulting array contains the *text* samples and the second the *author* ones.\n"},{"execution_count":null,"outputs":[],"metadata":{"_cell_guid":"cde201b8-a906-44a0-b879-815c54cf4ae5","_uuid":"b82bc16f3297e3ca734853b602a1a7233d70912d","_kg_hide-output":true},"cell_type":"code","source":"flattened = train.T.values.flatten()\nx,y = np.split(flattened, 2) # x is text, y is authors\nclassifier_pipeline.fit(x, y)"},{"cell_type":"markdown","metadata":{"_uuid":"df613d77c00da0eaa0c32186d2850426839c6230","_cell_guid":"183d2674-8119-4974-b7d0-fe0cb318a74e"},"source":"Once our classifier is trained, we read the test data and do our predictions"},{"execution_count":null,"outputs":[],"metadata":{"_uuid":"8b5926b9dd1fd375f7c0d85435cda71822792bf6","_cell_guid":"c8f889d2-8fbf-4238-932a-e7a2610beaf2","collapsed":true},"cell_type":"code","source":"test = pd.read_csv(\"../input/test.csv\")\nprediction = classifier_pipeline.predict_proba(test['text'])"},{"cell_type":"markdown","metadata":{"_uuid":"a47b9d70c26b155f5a0e9c89aef6e43ed81dfa0c","_cell_guid":"33774407-4702-4a5a-a36c-ef6098a8fda5"},"source":"For each text given as input, the prediction contains an array with the probabilities for that text belonging to each of the three authors.\nWe then use the sample submission (we re-use the *id* column) and overwrite the three authors columns with our prediction probabilities"},{"execution_count":null,"outputs":[],"metadata":{"_uuid":"46028d4ddd4a47dabcd6ffe9dd13f3dadc57d2ab","_cell_guid":"0ac90003-74f2-476a-a0b9-52cb3e1f3bf2","collapsed":true},"cell_type":"code","source":"sample_submission = pd.read_csv(\"../input/sample_submission.csv\")\nid_column = sample_submission['id']\n\nauthors = pd.DataFrame(prediction, columns=['EAP','HPL','MWS'])\n\nsubmission = pd.concat([id_column, authors], axis=1)"},{"cell_type":"markdown","metadata":{"_uuid":"f0ee156b67b16db3db94715ac8761d24ee3f28de","_cell_guid":"abd5a030-bfb5-434f-a8dc-3633a122b365"},"source":"Note the *axis=1* above. This means we add columns instead of trying to append rows to our data frame. \nWe are now ready to save our data to a .csv file and submit it."},{"execution_count":null,"outputs":[],"metadata":{"_uuid":"bf52d909cbbfb57a729646e24e82fa94aaf0fce8","_cell_guid":"5097ed51-763d-417b-adbf-5f049136da85","collapsed":true},"cell_type":"code","source":"submission.to_csv('submission_long.csv', sep=',', index=False)"},{"cell_type":"markdown","metadata":{"_uuid":"676a14e7b13f4433c8da407678750dbf80ea73d6","_cell_guid":"580622ed-f36b-491a-af05-0caa5c251a04","collapsed":true},"source":"The two files generated should be identical, and they score a 0.89 on the leaderboard. Not an impressive score, but a good start nonetheless.\nAnd given that you now have a pipeline set up, you can start experimenting with hyper-parameter tuning, cross-validation, and all sorts of other pre-processors and classifiers.\n\nHappy coding!"}],"nbformat":4,"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"nbconvert_exporter":"python","file_extension":".py","version":"3.6.3","name":"python","codemirror_mode":{"name":"ipython","version":3},"mimetype":"text/x-python","pygments_lexer":"ipython3"}}}