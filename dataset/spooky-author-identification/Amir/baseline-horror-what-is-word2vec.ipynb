{"nbformat_minor":1,"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"version":"3.6.3","pygments_lexer":"ipython3","nbconvert_exporter":"python","mimetype":"text/x-python","codemirror_mode":{"version":3,"name":"ipython"},"name":"python","file_extension":".py"}},"cells":[{"source":"<h1>Introduction<h1>\n\nHello, everyone! I am going to try to predict probabilities using the magic of distributional semantics, and I propose a baseline solution based on Logistic Regression and Word2Vec. I hope this notebook will be helpful, and I will highly appreciate any critique or feedback. Feel free to write your thoughts at the comments section!","metadata":{"_uuid":"dfb3d8e9e6f91a03d77fba7d56f67ece94a1b4eb","_cell_guid":"627e8b17-8f99-4c4f-80b9-cfb3c5caea08"},"cell_type":"markdown"},{"outputs":[],"execution_count":null,"source":"import numpy as np\nimport pandas as pd \nfrom subprocess import check_output\nfrom gensim.models import Word2Vec\n\nfrom nltk.tokenize import RegexpTokenizer\nfrom nltk import WordNetLemmatizer\nfrom nltk.corpus import stopwords\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.neighbors import KNeighborsClassifier\n\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.preprocessing import Normalizer\nfrom sklearn.pipeline import Pipeline\n\nimport xgboost as xgb\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, HashingVectorizer\n\nalpha_tokenizer = RegexpTokenizer('[A-Za-z]\\w+')\nlemmatizer = WordNetLemmatizer()\nstop = stopwords.words('english')","metadata":{"_uuid":"89bffacb349704fa6af52acb8dfab87440018b6f","collapsed":true,"_cell_guid":"e1351c4f-200b-49d1-bf9b-5c431121a478"},"cell_type":"code"},{"outputs":[],"execution_count":null,"source":"from sklearn.model_selection import train_test_split\n\ntrain = pd.read_csv('../input/train.csv')\ntest = pd.read_csv('../input/test.csv')\n\nauthor_mapping = {'EAP':0, 'HPL':1, 'MWS':2}\ny_train = train['author'].map(author_mapping).values","metadata":{"_uuid":"431dc7e73b97b244522f6c8c14db78bb91743282","collapsed":true,"_cell_guid":"3db529b1-9713-4045-9863-c7446e129073"},"cell_type":"code"},{"source":"<h1>Pre-process the data<h1>\n\nIn order to use Word2Vec, you need to pre-process the data. It's very simple: you just need to split sentences to words (**tokenization**), bring the words to their basic form (**lemmatization**), and remove some very common words like articles or prepositions (**stop-word removal**). I'm using RegexpTokenizer, WordNetLemmatizer and NLTK stop word list. You could start experimenting already at this step and try to extend the stop word list or to use another lemmatizer! It will be interesting to know what will happen!","metadata":{"_uuid":"5f07d2b06e93a8ecb0d2957073cda2a87e28dd0d","_cell_guid":"6729448a-39f7-4039-af95-fe7b6fa908d8"},"cell_type":"markdown"},{"outputs":[],"execution_count":null,"source":"data = [[lemmatizer.lemmatize(word.lower()) for word in alpha_tokenizer.tokenize(sent) if word.lower() not in stop] for sent in train.text.values]","metadata":{"_uuid":"61b5e0f48944efc104e52b8ad3aa32c2da7a0fd3","collapsed":true,"_cell_guid":"69333bb3-06e8-4661-8798-7b42e2ccb266"},"cell_type":"code"},{"source":"## Some initial experiemnts with simple vectorizers\n\nI will write more about it very soon.","metadata":{"_uuid":"b275cb9118365df1ba3115f74abd21a64c5e695e","_cell_guid":"bb96e408-ce20-4a55-8f46-36c1a8e2300c"},"cell_type":"markdown"},{"outputs":[],"execution_count":null,"source":"vectorizers = [ ('3-gram TF-IDF Vectorizer on words', TfidfVectorizer(ngram_range=(1, 3), analyzer='word', binary=False)),\n                ('3-gram Count Vectorizer on words', CountVectorizer(ngram_range=(1, 3), analyzer='word', binary=False)),\n                ('3-gram Hashing Vectorizer on words', HashingVectorizer(ngram_range=(1, 5), analyzer='word', binary=False)),\n                ('TF-IDF + SVD', Pipeline([('tfidf', TfidfVectorizer(ngram_range=(1, 3), analyzer='word', binary=False)),\n                                 ('svd', TruncatedSVD(n_components=150)),\n                                ])),\n                ('TF-IDF + SVD + Normalizer', Pipeline([('tfidf', TfidfVectorizer(ngram_range=(1, 3), analyzer='word', binary=False)),\n                                 ('svd', TruncatedSVD(n_components=150)),\n                                 ('norm', Normalizer()),\n                                ]))\n              ]","metadata":{"_uuid":"7ae4f0a74f5c5b21d1328aa6984772c76dca4bcb","collapsed":true,"_cell_guid":"90490332-7978-461d-8329-8bb471283401"},"cell_type":"code"},{"outputs":[],"execution_count":null,"source":"estimators = [(KNeighborsClassifier(n_neighbors=3), 'K-Nearest Neighbors', 'yellow'),\n              (SVC(C=1, cache_size=200, class_weight=None, coef0=0.0, decision_function_shape='ovr', degree=3, gamma='auto', kernel='linear', max_iter=-1, probability=False, random_state=None, shrinking=True,tol=0.001, verbose=False), 'Support Vector Machine', 'red'),\n              (LogisticRegression(tol=1e-8, penalty='l2', C=0.1), 'Logistic Regression', 'green'),\n              (MultinomialNB(), 'Naive Bayes', 'magenta'),\n              (RandomForestClassifier(n_estimators=10, criterion='gini'), 'Random Forest', 'gray'),\n              (None, 'XGBoost', 'pink')\n]","metadata":{"_uuid":"eee05c9ffc956053ccd14dd75969c4de8dbf4e8e","collapsed":true,"_cell_guid":"1bc1a25d-0431-43ce-a01d-8472ad339128"},"cell_type":"code"},{"outputs":[],"execution_count":null,"source":"params = {}\nparams['objective'] = 'multi:softprob'\nparams['eta'] = 0.1\nparams['max_depth'] = 3\nparams['silent'] = 1\nparams['num_class'] = 3\nparams['eval_metric'] = 'mlogloss'\nparams['min_child_weight'] = 1\nparams['subsample'] = 0.8\nparams['colsample_bytree'] = 0.3\nparams['seed'] = 0","metadata":{"_uuid":"c4f976fedc538b778c2a8d495de0351b755be40b","collapsed":true,"_cell_guid":"4f432256-c7cf-4259-b837-abeb99385a0c"},"cell_type":"code"},{"outputs":[],"execution_count":null,"source":"y_train, y_test = train_test_split(train, test_size=0.3)\ny_train = y_train['author'].map(author_mapping).values\ny_test = y_test['author'].map(author_mapping).values\n\ndef compare():\n    for vectorizer in vectorizers:\n        print(vectorizer[0] + '\\n')\n        X = vectorizer[1].fit_transform(train.text.values)\n        X_train, X_test = train_test_split(X, test_size=0.3)\n        for estimator in estimators:\n            if estimator[1] == 'XGBoost': \n                xgtrain = xgb.DMatrix(X_train, y_train)\n                xgtest = xgb.DMatrix(X_test)\n                model = xgb.train(params=list(params.items()), dtrain=xgtrain,  num_boost_round=40)\n                predictions = model.predict(xgtest, ntree_limit=model.best_ntree_limit).argmax(axis=1)\n            else:\n                estimator[0].fit(X_train, y_train)\n                predictions = estimator[0].predict(X_test)\n            print(accuracy_score(predictions, y_test), estimator[1])","metadata":{"_uuid":"700e7dcd575ef3ae9e745a37e6ffa04deb852dbd","collapsed":true,"_cell_guid":"e757fb62-19cf-49f2-af37-8026b35771e0"},"cell_type":"code"},{"source":"## Baseline Solution on TF-IDF + SVD + XGBoost","metadata":{"_uuid":"d7d8c0e1452859f760a98b122cb8db730aec19eb","_cell_guid":"b1cb335a-1a44-406e-ada1-165af697ffda"},"cell_type":"markdown"},{"outputs":[],"execution_count":null,"source":"train = pd.read_csv('../input/train.csv')\nX = vectorizers[4][1].fit_transform(np.hstack((train.text.values, test.text.values)))\nX_train, X_test = train_test_split(X, test_size=8392)\nxgtrain = xgb.DMatrix(X_train, y_train)\nxgtest = xgb.DMatrix(X_test)\nmodel = xgb.train(params=list(params.items()), dtrain=xgtrain,  num_boost_round=40)\nprobs = model.predict(xgtest, ntree_limit=model.best_ntree_limit)\nauthor = pd.DataFrame(probs)\nfinal = pd.DataFrame()\nfinal['id'] = test.id\nfinal['EAP'] = author[0]\nfinal['HPL'] = author[1]\nfinal['MWS'] = author[2]\nfinal.to_csv('submission.csv', sep=',',index=False)","metadata":{"_uuid":"89a74100b13c7c4434a50b355b171d81d97c9822","collapsed":true,"_cell_guid":"a272c01d-68b3-4272-bd35-85ff60819940"},"cell_type":"code"},{"source":"<h1>Distributional semantics<h1>\n\n**Distributional semantic models** are frameworks that can represent words of natural language through real-valued vectors of fixed dimensions (the so-called **word embeddings**). The word \"distributional\" here is a reference to a distributional hypothesis that says that word semantics is distributed along all of its contexts. Such models able to capture various functional or topical relations between words through words context for for each word observed in a given corpus. Predicting words given their contexts (like **continuous bag-of-words** (CBOW) works) and  predicting the contexts from the words (like **continuous skip-gram** (SG) works) are two possible options of capturing the context, and this is how the distributional semantic model Word2Vec works. In short, with skip gram, you can create a lot more training instances from limited amount of data. We will set paramater sg to 1. It defines the training algorithm, and if sg=1, skip-gram is employed (and CBOW is employed otherwise).\n\nAbout some other parameters:\n*min_count *= ignore all words with total frequency lower than this.\n*size* is the dimensionality of the feature vectors.\n*window* is the maximum distance between the current and predicted word within a sentence.","metadata":{"_uuid":"f052bc439f5f7d289e530c61186ed2c4d071df10","_cell_guid":"801de0ea-3a67-4bd5-a4dd-63731efa95fc"},"cell_type":"markdown"},{"outputs":[],"execution_count":null,"source":"NUM_FEATURES = 150\n\nmodel = Word2Vec(data, min_count=3, size=NUM_FEATURES, window=5, sg=1, alpha=1e-4, workers=4)","metadata":{"_uuid":"393d7bb7555676f1d02fd0f1e4d3f188a2208a31","collapsed":true,"_cell_guid":"5bd31c80-451f-4f39-abbb-227fb226f8b9"},"cell_type":"code"},{"source":"Now we have 10852 words in our model, and we could try to find most similar words for some examples. Let's try the word \"raven\".","metadata":{"_uuid":"45e5e615b2d38464d4be8ba90a9deb4d2638849e","_cell_guid":"94af3940-0681-4383-9b94-28fb6a0d85de"},"cell_type":"markdown"},{"outputs":[],"execution_count":null,"source":"len(model.wv.vocab)","metadata":{"_uuid":"dec4f889111df3e97ae59cc238d5fde4377e0062","collapsed":true,"_cell_guid":"29efdfc2-bafe-43e5-adc5-b049bdc26c4a"},"cell_type":"code"},{"outputs":[],"execution_count":null,"source":"model.most_similar('raven')","metadata":{"_uuid":"71f7f9217e36bcf1e3465377b9f0828dc887089a","collapsed":true,"_cell_guid":"e027498e-edf9-400a-87f2-f99f239eefea"},"cell_type":"code"},{"source":"<h1>Compositional distributional semantics<h1>\n\nWe are able to represent each word in a form of a vector, but how to represent the whole sentence? Well ,semantics of sentences and phrases can be also captured as a composition of the word embeddings -- for instance, through **compositional distributional semantics** (CDS). CDS is a nominal notion of a method of capturing semantics of composed linguistic units like sentences and phrases by composing the distributional representations of the words that these units contain. The semantics of a whole sentence can be represented as a composition of words embeddings of the words constituting the sentence. An averaged unordered composition (or an arithmetic mean) is a one of the most popular methods of capturing semantics of a sentence since it is an effective solution despite its simplicity. Since one could claim that word embeddings are the building blocks of compositional representation, and while it has been shown that semantic relations can be mapped to translations in the learned vector space, the claim could be made for sentence representations of the embeddings.","metadata":{"_uuid":"db9348f53a82e093c516705136a72de56b288944","_cell_guid":"296955e0-f505-486d-be2a-e606214037ef"},"cell_type":"markdown"},{"outputs":[],"execution_count":null,"source":"def get_feature_vec(tokens, num_features, model):\n    featureVec = np.zeros(shape=(1, num_features), dtype='float32')\n    missed = 0\n    for word in tokens:\n        try:\n            featureVec = np.add(featureVec, model[word])\n        except KeyError:\n            missed += 1\n            pass\n    if len(tokens) - missed == 0:\n        return np.zeros(shape=(num_features), dtype='float32')\n    return np.divide(featureVec, len(tokens) - missed).squeeze()","metadata":{"_uuid":"0be41f886bb95e2c9d8ba135df7e67d749b79ae7","collapsed":true,"_cell_guid":"5d60d027-2a1d-4b1f-9e3d-9e163b570264"},"cell_type":"code"},{"outputs":[],"execution_count":null,"source":"vectors = []\nfor i in train.text.values:\n    vectors.append(get_feature_vec([lemmatizer.lemmatize(word.lower()) for word in alpha_tokenizer.tokenize(i) if word.lower() not in stop], NUM_FEATURES, model))","metadata":{"_uuid":"192bd2267cc84a5e1f483c44e549ba7f7c0f5ecd","collapsed":true,"_cell_guid":"5f18b601-1d78-4f26-ae2f-2dea5991af05"},"cell_type":"code"},{"source":"<h1>Training the classifier<h1>\n\nWe are representing the labels of the authors in a form of numeric class labels, and then we are ready to train the classifier. I picked Logistic Regression, but you could use another one.","metadata":{"_uuid":"8559e72010983b17adb1cee15721b21944e744a5","_cell_guid":"2a3ca556-04ae-4581-92a5-e9506b0498d7"},"cell_type":"markdown"},{"outputs":[],"execution_count":null,"source":"estimator = LogisticRegression(C=1)\nestimator.fit(np.array(vectors), y_train);","metadata":{"_uuid":"79b7802015b252c7e07d16f69cbcbb82c09267a8","collapsed":true,"_cell_guid":"4c25636b-ebc6-4c3e-9692-75f11c599d9b"},"cell_type":"code"},{"source":"<h1>Making predictions<h1>\n\nAnd we are ready to make predictions! We will use the ability of the classifier to predict the probabilities of given classes.","metadata":{"_uuid":"079be6a60ea56015dd4522b1e577e811085dfa82","collapsed":true,"_cell_guid":"282d3b81-d0c2-4d9c-b260-7e8254727ef7"},"cell_type":"markdown"},{"outputs":[],"execution_count":null,"source":"test_vectors = []\nfor i in test.text.values:\n    test_vectors.append(get_feature_vec([lemmatizer.lemmatize(word.lower()) for word in alpha_tokenizer.tokenize(i) if word.lower() not in stop], NUM_FEATURES, model))","metadata":{"_uuid":"c12983bb773b984b886b5e59f348a36f81a61889","collapsed":true,"_cell_guid":"0d883c17-cf41-4c22-9f67-2c99a9c0c35e"},"cell_type":"code"},{"outputs":[],"execution_count":null,"source":"probs = estimator.predict_proba(test_vectors)","metadata":{"_uuid":"40a0087ef5db133ef5b425a6ad79dcaf353d5044","collapsed":true,"_cell_guid":"278299e0-1620-4db3-a7e1-088037653965"},"cell_type":"code"},{"source":"<h1>Submission<h1>\n\nOne final step: make a dataframe to submit our results!","metadata":{"_uuid":"4ceab05de2d42738dd4eda81244ee537b04e6d3a","_cell_guid":"435ee883-801a-4504-bda2-5d300a378ca4"},"cell_type":"markdown"},{"outputs":[],"execution_count":null,"source":"author = pd.DataFrame(probs)\n\nfinal = pd.DataFrame()\nfinal['id'] = test.id\nfinal['EAP'] = author[0]\nfinal['HPL'] = author[1]\nfinal['MWS'] = author[2]\n# final.to_csv('submission.csv', sep=',',index=False)","metadata":{"_uuid":"c9620c4638f54fffd658074c38a9936364d33b67","collapsed":true,"_cell_guid":"950c4c18-b8dd-41ab-9e85-1b40b618e6a1"},"cell_type":"code"},{"source":"That's all for now! Thanks for reading this notebook. I'm glad if it helped you to learn something new. This is a very first version of this small tutorial, and I'm working hard to make it better. I plan to introduce some other methods of word vectors composing and to try to use some syntax features\n\nWitch-ing you a spook-tacular Halloween! Do not let ghouls and spooks to ruin your models, and don't fear the curse of dimensionality!","metadata":{"_uuid":"b2a1e2f573784ff7821cc82c77dda4c849fc6cf7","collapsed":true,"_cell_guid":"3d8a1925-cddd-4438-bdae-fc1a4f363b60"},"cell_type":"markdown"}],"nbformat":4}