{"cells":[{"metadata":{},"cell_type":"markdown","source":"# NLP Challange.\n \n    I have decided a idealistic challange for myself that is learning NLP in 1 week \n    then participating in new \"Jigsaw Multilingual Toxic comment classification challange\"\n    \n    I am going to write all the things I will learn in different notebooks.\n "},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"markdown","source":"# Natural Language Processing \n    \n    Natural language is the language we speak or write in, Natural Language Processing is a study where\n    we make computer understand how our language works and what are the rules on which our language is based\n    \n    Natural Language is hard to understand for computers because Natural Language has very less rules.\n    and sometimes it requires the context inorder to understand the Language.\n    \n    Natural Language Processing has been around for 50 years, but still there is room for improvement.\n    \n    Natural Language Processing is so hard that Turing should have included it in the test of intellegence.\n    \n    \n        \n    "},{"metadata":{},"cell_type":"markdown","source":"## Importing Imp libraries\n\nI have just imported every possibly used library of NLP <br/>\nwe will see what each imported library and class do eventually.<br/>\n\nI am following [this](https://www.kaggle.com/abhishek/approaching-almost-any-nlp-problem-on-kaggle) notebook by abhishek thakur. "},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport xgboost as xgb\nfrom tqdm import tqdm\nfrom sklearn.svm import SVC\nfrom keras.models import Sequential\nfrom keras.layers.recurrent import LSTM,GRU\nfrom keras.layers.core import Dense, Activation, Dropout\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.utils import np_utils\nfrom sklearn import preprocessing, decomposition, model_selection,metrics,pipeline\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import MultinomialNB\nfrom keras.layers import GlobalMaxPooling1D,Conv1D,MaxPooling1D,Flatten,Bidirectional,SpatialDropout1D\nfrom keras.preprocessing import sequence, text\nfrom keras.callbacks import EarlyStopping\nfrom nltk import word_tokenize\nfrom nltk.corpus import stopwords","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" ##  Loading dataset\n \n **About Data and compitition**\n    Here the challange is simply to predict which author wrote <br/>\n    given sentence \n    \n  we are given training data and testing data in separate files\n  training data contains \"id\" of each sentence \"text\" the sentence\n  and author(EAP: Edgar Allan Poe, HPL: HP Lovecraft; \n  MWS: Mary Wollstonecraft Shelley)"},{"metadata":{"trusted":true},"cell_type":"code","source":"PATH = '../input/spooky-author-identification'\ntrain = pd.read_csv(f'{PATH}/train.zip')\ntest = pd.read_csv(f'{PATH}/test.zip')\nsample = pd.read_csv(f'{PATH}/sample_submission.zip')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##  Mertics for calculating loss\n\nHere kaggle is using multiclass log loss for calculating the error.\n\nLog Loss is the mathematical function which is used to measure the error rate in the classification problem, means we need to get minimum value of log loss. Log Loss is different because here we need to calculate the probability of each class rather than predicting the class. \n\nThe twist about the logloss is that it highly penalize the wrong values. that is if in a binary classification if we have probability of 50/ 50 an after that if probability shits 60/40 to the wrong side,\nthat is to wrong class it highly penalizes it.\n\nThe graph of the probability vs logloss is shown below.\n\n![image](https://datawookie.netlify.app/img/2015/12/log-loss-curve.png)\n\n![image2](https://miro.medium.com/max/1192/1*wilGXrItaMAJmZNl6RJq9Q.png)\n\nas this graph shows as the probability of the correct classification increases logloss decreases gradually but if the probability decreases logloss increase exponentially highly penalizing the output.\n\nso it is better to have overall good clasification values, rather than some excellent classification values and many worst classification values.\n\nto learn more about logloss read [this](https://datawookie.netlify.app/blog/2015/12/making-sense-of-logarithmic-loss/) article.\n\n\nHere the difference is that instead of using Binary LogLoss Kaggle is usinig Multi logloss which is for Multiclass classification\n\nand it's formula is\n![image3](https://miro.medium.com/max/1162/0*i2_eUc_t8A1EJObd.png)\n"},{"metadata":{},"cell_type":"markdown","source":" ##  Function for Multiclass-logloss"},{"metadata":{"trusted":true},"cell_type":"code","source":"def multiclass_logloss(actual,predicted,eps=1e-15):\n    \n    #converting the 'actual' values to binary values if it's \n    #not binary values\n    \n    if len(actual.shape) == 1:\n        actual2 = np.zeros((actual.shape[0],predicted.shape[1]))\n        \n        for i, val in enumerate(actual):\n            actual2[i,val] = 1\n        actual = actual2\n    \n    #clip function truncates the number between\n    #a max number and min number\n    clip = np.clip(predicted,eps,1-eps)\n    rows = actual.shape[0]\n    vsota = np.sum(actual * np.log(clip))\n    return -1.0/ rows * vsota ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**we will labelencode the author column using LabelEncoder from scikit-learn**"},{"metadata":{"trusted":true},"cell_type":"code","source":"encoder = preprocessing.LabelEncoder()\ny = encoder.fit_transform(train[\"author\"].values)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**we will make train test split of the data using train_test_split function of scikit which has parameter test_size which decides fraction of the values to use as test data**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# we will use 10% of data for testing\nX_train, X_test, y_train, y_test = train_test_split(train.text.values,y,random_state=42,test_size=0.1,shuffle=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Term Frequency(tf) and Inverse document frequency(idf).\n\n**what is term frequeny(tf)?**: \n* term frequency if simply a number which indicates how many times a number<br/>\n  occur in the document. So it just count number of occurance of the words. It is used to know what words are<br/>\n  important in the documents.\n* Term Frequency is divided by the total number of words for normaizing, it could also be divided by max fequency<br/>\n  or average frquency.\n\n**what is inverse documents frequency?**\n* The problem with the term freuency is that in a language there are many connecters common words like \"the\",\"is\",\"and\"<br/>\n   and this words do not highlight the context of the sentence. one way to solve this problem is to remove stop words<br/>\n   and other is to multiply it with idf score.\n* Idf score of a word is a number determining how unique this word is to given document.\n* It is calculated as log(N/DT) where N is total number of documents and DT is number of doc containing the word.\n\nso tf*idf will highlight the words which gives us context of the data.\n"},{"metadata":{},"cell_type":"markdown","source":"# countvectorizer\n\nBasic function of countvectorizer is to make a sparce matrix of word count for each document.<br/>\nfor example there are two docs like \"say hello to dog\", and \"say hello to everyone\".\n\nCountvectorizer will create a column for each unique word in all the documents<br/>\nand rows for each document. with values of count of each word<br/>\nso countvectorizer could perform as a feature to machine learning model<br/>\n\nso we will have 5 columns(unique words) and 2 rows(no of documents)<br/>\n\nsay | hello | to | dog | everyone<br/>\n   1      1     1     1       0    <br/>\n   1      1     1     0       1    <br/>\n   \nHere instead of column names as \"words\" countvectorizer assigns number to each word<br/>\nlike say=1, hello=2, to =3 ....\n\nNow we will look at various ways we can use countvectorizer    "},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\n#we are going to use this example as our documents.\n\ncat_in_the_hat_docs=[\n       \"One Cent, Two Cents, Old Cent, New Cent: All About Money (Cat in the Hat's Learning Library\",\n       \"Inside Your Outside: All About the Human Body (Cat in the Hat's Learning Library)\",\n       \"Oh, The Things You Can Do That Are Good for You: All About Staying Healthy (Cat in the Hat's Learning Library)\",\n       \"On Beyond Bugs: All About Insects (Cat in the Hat's Learning Library)\",\n       \"There's No Place Like Space: All About Our Solar System (Cat in the Hat's Learning Library)\" \n      ]\n\n#make object of countvectorizer\ncv = CountVectorizer()\ncount_vector = cv.fit_transform(cat_in_the_hat_docs)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Basic steps performed by countvectorizer**\n1. lowecase words (lowercase=False if not want to lower)\n2. uses utf-8 encoding\n3. perform tokenization (converts text to small chunk of text)\n4. word level tokenization (converts each word to token)\n5. ignores single character such as \"a\" and \"I\""},{"metadata":{"trusted":true},"cell_type":"code","source":"#now let's look at the  unique words countvectorizer was able to find\ncv.vocabulary_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In above output number shown are not counts of words they are<br/>\ntheir position (column no.) in the matrix"},{"metadata":{"trusted":true},"cell_type":"code","source":"count_vector.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's remove stopwords from the docs\n\nThere are three ways in which we can remove stop_words in CountVectorizer\n\n1. using custom stop_words list\n2. using sklearn stopword list (not recomended)\n3. using min_df and max_df for removing stopwords (highly recommended)\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"#using cumstom stopword list\ncustom_stop_words = [\"all\",\"in\",\"the\",\"is\",\"and\"]\n\ncv = CountVectorizer(cat_in_the_hat_docs,stop_words=custom_stop_words)\ncount_vector = cv.fit_transform(cat_in_the_hat_docs)\ncount_vector.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The shape changed from (5,43) to (5,40) as the stop words are removed"},{"metadata":{"trusted":true},"cell_type":"code","source":"#have a look at the stop words\ncv.stop_words","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### using MIN_DF (minimum document frequency)\nMin_df will look for the words which have low occurence in the documents<br/>\nlike a name of the person which is occured only once in all the documents coult<br/>\nbe removed.\n\nit can be done by setting min_df argument which can be absolute value like 1,2,3<br/>\nor could be 0.25 means less than 25% of documents\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"cv = CountVectorizer(cat_in_the_hat_docs,min_df=2) #word that has occur in only one document\ncount_vector = cv.fit_transform(cat_in_the_hat_docs)\n\n#now let's look at stop words\ncv.stop_words_\n#see the difference of _ at the end it's because we used min_df \n#instead of custom stop_words","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are too many stop words because our document list is small"},{"metadata":{},"cell_type":"markdown","source":"### Using Max_df (Max document frequency)\n\nAs we have removed words which are less frequent we can remove words<br/>\nwhich are too frequent among documents using max_df parameter."},{"metadata":{"trusted":true},"cell_type":"code","source":"#using max_df\ncv = CountVectorizer(cat_in_the_hat_docs,max_df=0.5) #present in more than 50% of documents\ncount_vector = cv.fit_transform(cat_in_the_hat_docs)\n\ncv.stop_words_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Good thing about CountVectorizer is that it allows you to make your own<br/>\npreprocessor and tokenizer and add it to 'tokenizer' and 'preprocessor' parameter\n\nand other thing is if you just want presence and absence of words instead of counts<br/>\nyou can set binary=True in parameters"},{"metadata":{},"cell_type":"markdown","source":"## Tfidftransformer and Tfidfvectorizer\n\nThis are the classes which calculates tfid scores for the documents.\n\nThere is only little difference between both of them<br/>\nTfidtransformer uses CountVectorizer that we have created manually<br/>\n\nBut Tfidvectorizer performs CountVectorizer function internally and no need to create<br/>\nmanual CountVectorizer object."},{"metadata":{"trusted":true},"cell_type":"code","source":"#using Tfidtrasnformer\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n#we will use this toy example\ndocs=[\"the house had a tiny little mouse\",\n      \"the cat saw the mouse\",\n      \"the mouse ran away from the house\",\n      \"the cat finally ate the mouse\",\n      \"the end of the mouse story\"\n     ]\n\ncv = CountVectorizer(docs,max_df=0.5)\n\ncount_vector = cv.fit_transform(docs)\nprint(count_vector.shape)\n\n#calculate idf values\ntfidf_transformer = TfidfTransformer(smooth_idf=True,use_idf=True)\ntfidf_transformer.fit(count_vector)\n\ndf_idf = pd.DataFrame(tfidf_transformer.idf_,index=cv.get_feature_names(),columns=[\"idf_weights\"])\ndf_idf\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#using tfid_vectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer \n\ntfidf_vectorizer = TfidfVectorizer(smooth_idf=True,use_idf=True)\ntfidf_vectorizer.fit_transform(docs)\n\n#as you can see we don't need CountVectorizer in TfidfVectorizer\n\ndf_idf = pd.DataFrame(tfidf_vectorizer.idf_,index=tfidf_vectorizer.get_feature_names(),columns=[\"idf_weights\"])\ndf_idf","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Back to Competition "},{"metadata":{},"cell_type":"markdown","source":"## LogisticRegression Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"# we can also pass countvectorizer parameters in TfidVectorizer\ntfv = TfidfVectorizer(min_df=3,max_features=None,strip_accents='unicode',analyzer='word',token_pattern=r'\\w{1,}',\n                      ngram_range=(1,3),use_idf=1,smooth_idf=1,stop_words='english')\n\n# max_features confines maximum number of words \n\ntfv.fit(list(X_train) + list(X_test))\nX_train_tfv = tfv.transform(X_train)\nX_test_tfv = tfv.transform(X_test)\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fitting Logistic Regression on TFIDF\nfrom sklearn.linear_model import LogisticRegression\nclf = LogisticRegression(C=1.0)\nclf.fit(X_train_tfv,y_train)\nprediction = clf.predict_proba(X_test_tfv)\n\nprint(\"logloss: %0.3f\" % multiclass_logloss(y_test,prediction))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Logistic regression on count vector"},{"metadata":{"trusted":true},"cell_type":"code","source":"ctv = CountVectorizer(analyzer='word',token_pattern=r'\\w{1,}',\n            ngram_range=(1, 3), stop_words = 'english')\n\nctv.fit(list(X_train)+list(X_test))\nX_train_ctv = ctv.transform(X_train)\nX_test_ctv = ctv.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf = LogisticRegression(C=1.0)\nclf.fit(X_train_ctv,y_train)\nprediction = clf.predict_proba(X_test_ctv)\n\nprint(\"logloss: %0.3f\" % multiclass_logloss(y_test,prediction))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Navie Bayes\n\nFitting on tfidf"},{"metadata":{"trusted":true},"cell_type":"code","source":"clf = MultinomialNB()\nclf.fit(X_train_tfv,y_train)\n\nprediction = clf.predict_proba(X_test_tfv)\n\nprint(\"logloss: %0.3f\" % multiclass_logloss(y_test,prediction))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Fitting on counts"},{"metadata":{"trusted":true},"cell_type":"code","source":"clf = MultinomialNB()\nclf.fit(X_train_ctv,y_train)\n\nprediction = clf.predict_proba(X_test_ctv)\n\nprint(\"logloss: %0.3f\" % multiclass_logloss(y_test,prediction))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## SVM\n\nSVM is very slow algorithm so it takes lot of time to fit so we will<br/> \nuse Singular Value Decomposition before applying SVM<br/>\nand we will also standardize the data.\n"},{"metadata":{},"cell_type":"markdown","source":"## What is Singular Value Decomposition?\n\nSingular value decomposition  or svd is a matrix decompostion technique<br/>\nsvd converts a matrix A  of mxn in dot product of three matrix (U . sigma . V^T) <br/>\n\nHere U is mxm matrix<br/>\nsigma is diagonal matrix of nxm <br/>\nV^T is transpost of nxn matrix <br/>\n\nThen reduction to k columns is done by taking sigma with k columns.<br/>\nor V^T with k rows. and preforming one of below function\n\nT = U dot sigma or T = A dot V^T\n\nT is final reduced matrix.\n\nTo know about SVD in detail read [this](https://machinelearningmastery.com/singular-value-decomposition-for-machine-learning/) article.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"svd = decomposition.TruncatedSVD(n_components=120)\nsvd.fit(X_train_tfv)\nX_train_svd = svd.transform(X_train_tfv)\nX_test_svd = svd.transform(X_test_tfv)\n\nscl = preprocessing.StandardScaler()\nscl.fit(X_train_svd)\n\nX_train_svd_scl = scl.transform(X_train_svd)\nX_test_svd_scl = scl.transform(X_test_svd)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Fitting svm "},{"metadata":{"trusted":true},"cell_type":"code","source":"svm = SVC(C=1.0,probability=True)\n\nsvm.fit(X_train_svd_scl,y_train)\nprediction = svm.predict_proba(X_test_svd_scl)\n\nprint(\"logloss: %0.3f\" % multiclass_logloss(y_test,prediction))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Xgboost\n\nFitting on tfidf"},{"metadata":{"trusted":true},"cell_type":"code","source":"clf = xgb.XGBClassifier(max_depth=7,n_estimators=200,colsample_bytree=0.8,subsample=0.8,nthread=10,learning_rate=0.1)\n\nclf.fit(X_train_tfv.tocsc(),y_train)\nprediction = clf.predict_proba(X_test_tfv.tocsc())\n\nprint(\"logloss: %0.3f\" % multiclass_logloss(y_test,prediction))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Fitting on svd"},{"metadata":{"trusted":true},"cell_type":"code","source":"clf = xgb.XGBClassifier(max_depth=7,n_estimators=200,colsample_bytree=0.8,subsample=0.8,nthread=10,learning_rate=0.1)\n\nclf.fit(X_train_svd,y_train)\nprediction = clf.predict_proba(X_test_svd)\n\nprint(\"logloss: %0.3f\" % multiclass_logloss(y_test,prediction))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Grid Search\n\nWe will do hyperparameter optimization using sklearn GridSearchCV<br/>\nand Data Pipeline is used here to keep code clean"},{"metadata":{"trusted":true},"cell_type":"code","source":"# as Multiclass_logloss is user defined we need to define our own scorer for grid search\n# greater_is_better is True by default but for our smaller the value of logloss better the result\n\nmll_scorer = metrics.make_scorer(multiclass_logloss, greater_is_better=False,needs_proba=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"svd = decomposition.TruncatedSVD()\n\nscl = preprocessing.StandardScaler()\n\nlr_model = LogisticRegression()\n\nclf = pipeline.Pipeline([('svd',svd),\n                         ('scl',scl),\n                         ('lr',lr_model)])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"grid of parameters"},{"metadata":{"trusted":true},"cell_type":"code","source":"params_grid = {'svd__n_components':[120,180],\n               'lr__C':[0.1,1.0,10],\n               'lr__penalty':['l1','l2']}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"we are creating parameter search for logistic regression <br/>\nwith SVD of 120 and 180 lr of 0.1,1,10 and l1 and l2 penalty."},{"metadata":{"trusted":true},"cell_type":"code","source":"model = GridSearchCV(estimator=clf,param_grid=params_grid,scoring=mll_scorer,verbose=10,n_jobs=-1,iid=True,refit=True,cv=2)\n\n#fitting the model\nmodel.fit(X_train_tfv,y_train)\n\nprint('Best score: %0.3f' % model.best_score_)\nprint(\"Best parameters set:\")\nbest_parameters = model.best_estimator_.get_params()\nfor param_name in sorted(params_grid.keys()):\n    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Grid Search for Navi Bayes"},{"metadata":{"trusted":true},"cell_type":"code","source":"nb = MultinomialNB()\n\nclf = pipeline.Pipeline([('nb',nb)])\n\nparams_grid = {'nb__alpha':[0.001,0.01,0.1,1,10,100]}\n\nmodel  = GridSearchCV(estimator=clf,param_grid=params_grid,scoring=mll_scorer,verbose=10,n_jobs=-1,refit=True,cv=2)\n\nmodel.fit(X_train_tfv,y_train)\n\nprint(\"Best score: %0.3f\" % model.best_score_)\nprint(\"Best parameters set:\")\nbest_parameters = model.best_estimator_.get_params()\nfor param_name in sorted(params_grid.keys()):\n    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This is the end of this notebook one, next notebook we will learn about advance stuff<br/>\nlike word vectors , word embeddings and Deep Learning Models"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}