{"nbformat_minor":1,"cells":[{"outputs":[],"source":"# Imports\nimport pandas as pd\nimport numpy as np\nimport re\nimport nltk.data\nimport nltk\nimport os\nfrom collections import OrderedDict\nfrom subprocess import check_call\nfrom shutil import copyfile\nfrom sklearn.metrics import log_loss\nimport matplotlib.pyplot as plt\nimport mpld3\nimport seaborn as sns\nfrom collections import Counter\nfrom sklearn.cross_validation import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn import ensemble, metrics, model_selection, naive_bayes\nfrom sklearn.preprocessing import LabelEncoder\nimport xgboost as xgb\nfrom tqdm import tqdm\nfrom sklearn import preprocessing, decomposition, model_selection, metrics, pipeline\nfrom keras.layers import GlobalAveragePooling1D,Merge,Lambda,Input,GlobalMaxPooling1D, Conv1D, MaxPooling1D, Flatten, Bidirectional, SpatialDropout1D,TimeDistributed\nfrom keras.preprocessing import sequence, text\nfrom keras.callbacks import EarlyStopping\nfrom nltk import word_tokenize\nfrom keras.layers.merge import concatenate\nfrom keras.utils import np_utils\nfrom keras.models import Sequential\nfrom keras.layers.recurrent import LSTM, GRU\nfrom keras.layers.core import Dense, Activation, Dropout\nfrom keras.layers.embeddings import Embedding\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras import initializers\nfrom keras import backend as K\nfrom sklearn.linear_model import SGDClassifier as sgd\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.callbacks import EarlyStopping\nfrom time import time","cell_type":"code","execution_count":null,"metadata":{"_uuid":"81522729384df9780b21e0f76768bd5e402eab31","_cell_guid":"ca1abe0f-ea70-40f6-8bf4-1790a577626b","collapsed":true}},{"source":"Add small test samples here:","cell_type":"markdown","metadata":{"_uuid":"5c69c8e253c3ffb5d287c895e21f4004c9b2fc6c","_cell_guid":"034b43e9-eec3-4962-8b00-21d7dd7d906d"}},{"outputs":[],"source":"start = time()\n# Read data\nprint('Extract...',round(time()-start,0))\ntrain = \"../input/train.csv\" #change this to correct training csv\ntest = \"../input/test.csv\" #change this to correct test csv\nX_train_ = pd.read_csv( train, header=0,delimiter=\",\" )\nX_train=X_train_.sample(frac=0.3, random_state=12345)\nX_test = pd.read_csv( test, header=0,delimiter=\",\" )\n\nauthors = ['EAP','MWS','HPL']\nY_train = LabelEncoder().fit_transform(X_train['author'])","cell_type":"code","execution_count":null,"metadata":{"_uuid":"c2753ac1526af4d716ecb359c186d0ed54d6f61a","_cell_guid":"431ad233-ef87-48bc-a12e-c1481adaeb42","scrolled":true,"collapsed":true}},{"outputs":[],"source":"# Clean data\ndef clean(X_train,X_test):\n    X_train['words'] = [re.sub(\"[^a-zA-Z]\",\" \", data).lower().split() for data in X_train['text']]\n    X_test['words'] = [re.sub(\"[^a-zA-Z]\",\" \", data).lower().split() for data in X_test['text']]\n    return X_train,X_test\nX_train,X_test = clean(X_train,X_test)\nprint('Leaning words...',round(time()-start,0))\n\nauth_wds = {'EAP':0,'HPL':0,'MWS':0}\n\nwd = {}\nfor i, row in X_train.iterrows():\n    for a in row['words']:\n        if len(a) > 1:\n            try: \n                wd[a][row['author']] = wd[a][row['author']] + 1\n                auth_wds[row['author']] = auth_wds[row['author']] + 1\n            except:\n                c_eap = 0\n                c_hpl = 0\n                c_mws = 0\n                try: c_eap = wd[a]['EAP'] \n                except: pass\n                try: c_hpl = wd[a]['HPL'] \n                except: pass\n                try: c_mws = wd[a]['EAP'] \n                except: pass   \n                wd[a] = {'EAP':c_eap,'HPL':c_hpl,'MWS':c_mws}\n                wd[a][row['author']] = wd[a][row['author']] + 1\n                auth_wds[row['author']] = auth_wds[row['author']] + 1\n                \ndef remove_key(dictionary,key):\n    r = dict(dictionary)\n    del r[key]\n    return r        \n\nfor key in list(wd.keys()):\n    pass\n    if wd[key]['EAP'] + wd[key]['HPL'] + wd[key]['MWS'] < 100: \n        wd = remove_key(wd,key)\n        \nc_eap = 0\nc_hpl = 0\nc_mws = 0    \nfor key in list(wd.keys()): \n    pass\n    if not any([(wd[key]['EAP']/auth_wds['EAP'])/((wd[key]['HPL']+1)/auth_wds['HPL'])>2.2,\n          (wd[key]['EAP']/auth_wds['EAP'])/((wd[key]['HPL']+1)/auth_wds['HPL'])>2.2,\n          (wd[key]['HPL']/auth_wds['HPL'])/((wd[key]['EAP']+1)/auth_wds['EAP'])>2.2,\n          (wd[key]['HPL']/auth_wds['HPL'])/((wd[key]['MWS']+1)/auth_wds['MWS'])>2.2,\n          (wd[key]['MWS']/auth_wds['MWS'])/((wd[key]['EAP']+1)/auth_wds['EAP'])>2.2,\n         ( wd[key]['MWS']/auth_wds['MWS'])/((wd[key]['HPL']+1)/auth_wds['HPL'])>2.2]):\n        wd = remove_key(wd,key)\n\ncol_wds = {}\nfor key in list(wd.keys()): \n    col_wds[key]=0\n\nrows = []\nfor words in X_train['words']:\n    line_wds = dict(col_wds)\n    for word in words:\n        try: line_wds[word] = line_wds[word] + 1\n        except: pass\n    row = [line_wds[key] for key in list(line_wds.keys())]\n    rows.append(row)\npd_df = pd.DataFrame(rows)\n    \nfor column in pd_df: \n    pass\n    X_train['wd_'+str(column)] = pd_df[column]\n\nrows = []\nfor words in X_test['words']:\n    line_wds = dict(col_wds)\n    for word in words:\n        try: line_wds[word] = line_wds[word] + 1\n        except: pass\n    row = [line_wds[key] for key in list(line_wds.keys())]\n    rows.append(row)\npd_df = pd.DataFrame(rows)\n    \nfor column in pd_df: \n    pass\n    X_test['wd_'+str(column)] = pd_df[column]\n\nauth_wds = {'EAP':0,'HPL':0,'MWS':0}\n\nwd = {}\nfor i, row in X_train.iterrows():\n    for a in row['words']:\n        if len(a) > 1:\n            try: \n                wd[a][row['author']] = wd[a][row['author']] + 1\n                auth_wds[row['author']] = auth_wds[row['author']] + 1\n            except:\n                c_eap = 0\n                c_hpl = 0\n                c_mws = 0\n                try: c_eap = wd[a]['EAP'] \n                except: pass\n                try: c_hpl = wd[a]['HPL'] \n                except: pass\n                try: c_mws = wd[a]['EAP'] \n                except: pass   \n                wd[a] = {'EAP':c_eap,'HPL':c_hpl,'MWS':c_mws}\n                wd[a][row['author']] = wd[a][row['author']] + 1\n                auth_wds[row['author']] = auth_wds[row['author']] + 1\n                \ndef remove_key(dictionary,key):\n    r = dict(dictionary)\n    del r[key]\n    return r        \n\nfor key in list(wd.keys()):\n    pass\n    if wd[key]['EAP'] + wd[key]['HPL'] + wd[key]['MWS'] < 5: \n        wd = remove_key(wd,key)\n        \n   \ne = auth_wds['EAP']\nh = auth_wds['HPL']\nm = auth_wds['MWS']\neap_wds = []\nhpl_wds = []\nmws_wds = []\nfor key in list(wd.keys()): \n    pass\n    if (wd[key]['EAP']/e)>((wd[key]['HPL']/h)+(wd[key]['MWS'])/m):\n        eap_wds.append(key)\n    elif (wd[key]['HPL']/e)>((wd[key]['EAP']/e)+(wd[key]['MWS'])/m):\n        hpl_wds.append(key)\n    elif (wd[key]['MWS']/e)>((wd[key]['HPL']/h)+(wd[key]['EAP'])/e):\n        mws_wds.append(key)\nc_wd_rows_eap = []\nc_wd_rows_hpl = []\nc_wd_rows_mws = []\ndup_wds = []\nfor row in X_train['words']:\n    if len(row) == len(set(row)): dup_wds.append(0)\n    else: dup_wds.append((len(row)-len(set(row)))/len(row)*10)\n    for word in row:\n        c_eap = 0\n        c_hpl = 0\n        c_mws = 0 \n        if word in eap_wds: c_eap+=1\n        elif word in hpl_wds: c_hpl+=1\n        elif word in mws_wds: c_mws+=1\n    c_wd_rows_eap.append(c_eap)\n    c_wd_rows_hpl.append(c_hpl)\n    c_wd_rows_mws.append(c_mws)\nX_train['c_wd_eap'] = c_wd_rows_eap  \nX_train['c_wd_hpl'] = c_wd_rows_hpl  \nX_train['c_wd_mws'] = c_wd_rows_mws \nX_train['dup_wds'] = dup_wds\nc_wd_rows_eap = []\nc_wd_rows_hpl = []\nc_wd_rows_mws = []\ndup_wds = []\nfor row in X_test['words']:\n    if len(row) == len(set(row)): dup_wds.append(0)\n    else: dup_wds.append((len(row)-len(set(row)))/len(row)*10)\n    for word in row:\n        c_eap = 0\n        c_hpl = 0\n        c_mws = 0 \n        if word in eap_wds: c_eap+=1\n        elif word in hpl_wds: c_hpl+=1\n        elif word in mws_wds: c_mws+=1\n    c_wd_rows_eap.append(c_eap)\n    c_wd_rows_hpl.append(c_hpl)\n    c_wd_rows_mws.append(c_mws)\nX_test['c_wd_eap'] = c_wd_rows_eap  \nX_test['c_wd_hpl'] = c_wd_rows_hpl  \nX_test['c_wd_mws'] = c_wd_rows_mws \nX_test['dup_wds'] = dup_wds\n\nprint('Characters...',round(time()-start,0))\nall_char = set([i for i in str(X_train['text'])])\nfor char in all_char:\n    X_train['punc_'+char] = [(sum([1  for nchar in sentence if nchar == char])/len(sentence)) for sentence in X_train['text']]\n    X_test['punc_'+char] = [(sum([1  for nchar in sentence if nchar == char])/len(sentence)) for sentence in X_test['text']]","cell_type":"code","execution_count":null,"metadata":{"_uuid":"785a93df26ba4790fe278d9413e3c57edeb2e928","_cell_guid":"d93cd623-b12a-4431-9d8e-49b53f29e6b5","collapsed":true}},{"outputs":[],"source":"from gensim.parsing.preprocessing import STOPWORDS\n\n# Feature Engineering\n# Stop Words\nprint('Other columns...',round(time()-start,0))\n_dist_train = [x for x in X_train['words']]\nX_train['stop_word'] = [len([word for word in sentence if word in STOPWORDS])*100.0/len(sentence) for sentence in _dist_train]\n\n_dist_test = [x for x in X_test['words']]\nX_test['stop_word'] = [len([word for word in sentence if word in STOPWORDS])*100.0/len(sentence) for sentence in _dist_test]  \n\n## Number of words in the text ##\nX_train[\"num_words\"] = X_train[\"text\"].apply(lambda x: len(str(x).split()))\nX_test[\"num_words\"] = X_test[\"text\"].apply(lambda x: len(str(x).split()))\n\n## Number of unique words in the text ##\nX_train[\"num_unique_words\"] = X_train[\"text\"].apply(lambda x: len(set(str(x).split())))\nX_test[\"num_unique_words\"] = X_test[\"text\"].apply(lambda x: len(set(str(x).split())))\n\n## Number of characters in the text ##\nX_train[\"num_chars\"] = X_train[\"text\"].apply(lambda x: len(str(x)))\nX_test[\"num_chars\"] = X_test[\"text\"].apply(lambda x: len(str(x)))\n\n## Average length of the words in the text ##\nX_train[\"mean_word_len\"] = X_train[\"text\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\nX_test[\"mean_word_len\"] = X_test[\"text\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\n\nprint('TFIDF...',round(time()-start,0))\n### Fit transform the count vectorizer ###\ntfidf_vec = CountVectorizer(stop_words=STOPWORDS, ngram_range=(1,3))\ntfidf_vec.fit(X_train['text'].values.tolist() + X_test['text'].values.tolist())\ntrain_tfidf = tfidf_vec.transform(X_train['text'].values.tolist())\ntest_tfidf = tfidf_vec.transform(X_test['text'].values.tolist())\n\n# Feature Engineering\n# count - words - nb\ndef countWords(X_train,X_test):\n    count_vec = CountVectorizer(stop_words='english', ngram_range=(1,3))\n    count_vec.fit(X_train['text'].values.tolist() + X_test['text'].values.tolist())\n    train_count = count_vec.transform(X_train['text'].values.tolist())\n    test_count = count_vec.transform(X_test['text'].values.tolist())\n    return train_count,test_count\n    \ndef runMNB(train_X, train_y, test_X, test_y, test_X2):\n    model = naive_bayes.MultinomialNB()\n    model.fit(train_X, train_y)\n    pred_test_y = model.predict_proba(test_X)\n    pred_test_y2 = model.predict_proba(test_X2)\n    return pred_test_y, pred_test_y2, model\n\ndef do_count_MNB(X_train,X_test,Y_train):\n    train_count,test_count=countWords(X_train,X_test)\n    cv_scores = []\n    pred_full_test = 0\n    pred_train = np.zeros([X_train.shape[0], 3])\n    kf = model_selection.KFold(n_splits=5, shuffle=True, random_state=2017)\n    for dev_index, val_index in kf.split(X_train):\n        dev_X, val_X = train_count[dev_index], train_count[val_index]\n        dev_y, val_y = Y_train[dev_index], Y_train[val_index]\n        pred_val_y, pred_test_y, model = runMNB(dev_X, dev_y, val_X, val_y, test_count)\n        pred_full_test = pred_full_test + pred_test_y\n        pred_train[val_index,:] = pred_val_y\n        cv_scores.append(metrics.log_loss(val_y, pred_val_y))\n    print(\"Mean cv score : \", np.mean(cv_scores))\n    pred_full_test = pred_full_test /5.\n    return pred_train,pred_full_test\n\npred_train,pred_test = do_count_MNB(X_train,X_test,Y_train)\nX_train[\"count_words_nb_eap\"] = pred_train[:,0]\nX_train[\"count_words_nb_hpl\"] = pred_train[:,1]\nX_train[\"count_words_nb_mws\"] = pred_train[:,2]\nX_test[\"count_words_nb_eap\"] = pred_test[:,0]\nX_test[\"count_words_nb_hpl\"] = pred_test[:,1]\nX_test[\"count_words_nb_mws\"] = pred_test[:,2]\n\n\n# Feature Engineering\n# tfidf - chars - nb\ndef tfidfWords(X_train,X_test):\n    tfidf_vec = TfidfVectorizer(stop_words='english', ngram_range=(1,5),analyzer='char')\n    full_tfidf = tfidf_vec.fit_transform(X_train['text'].values.tolist() + X_test['text'].values.tolist())\n    train_tfidf = tfidf_vec.transform(X_train['text'].values.tolist())\n    test_tfidf = tfidf_vec.transform(X_test['text'].values.tolist())\n    return train_tfidf,test_tfidf\n    \ndef runMNB(train_X, train_y, test_X, test_y, test_X2):\n    model = naive_bayes.MultinomialNB()\n    model.fit(train_X, train_y)\n    pred_test_y = model.predict_proba(test_X)\n    pred_test_y2 = model.predict_proba(test_X2)\n    return pred_test_y, pred_test_y2, model\n\ndef do(X_train,X_test,Y_train):\n    train_tfidf,test_tfidf = tfidfWords(X_train,X_test)\n    cv_scores = []\n    pred_full_test = 0\n    pred_train = np.zeros([X_train.shape[0], 3])\n    kf = model_selection.KFold(n_splits=5, shuffle=True, random_state=88)\n    for dev_index, val_index in kf.split(X_train):\n        dev_X, val_X = train_tfidf[dev_index], train_tfidf[val_index]\n        dev_y, val_y = Y_train[dev_index], Y_train[val_index]\n        pred_val_y, pred_test_y, model = runMNB(dev_X, dev_y, val_X, val_y, test_tfidf)\n        pred_full_test = pred_full_test + pred_test_y\n        pred_train[val_index,:] = pred_val_y\n        cv_scores.append(metrics.log_loss(val_y, pred_val_y))\n    print(\"Mean cv score : \", np.mean(cv_scores))\n    pred_full_test = pred_full_test /5.\n    return pred_train,pred_full_test\npred_train,pred_test = do(X_train,X_test,Y_train)\n\n\nX_train[\"tfidf_chars_nb_eap\"] = pred_train[:,0]\nX_train[\"tfidf_chars_nb_hpl\"] = pred_train[:,1]\nX_train[\"tfidf_chars_nb_mws\"] = pred_train[:,2]\nX_test[\"tfidf_chars_nb_eap\"] = pred_test[:,0]\nX_test[\"tfidf_chars_nb_hpl\"] = pred_test[:,1]\nX_test[\"tfidf_chars_nb_mws\"] = pred_test[:,2]\nprint('SpaCy...',round(time()-start,0))","cell_type":"code","execution_count":null,"metadata":{"_uuid":"5972266e8f574f158e75ad176c285640dd5fa54b","_cell_guid":"2c5487a2-077f-4393-a158-0603a3ce6dba","collapsed":true}},{"outputs":[],"source":"# incorporate spacy vectors\nimport spacy\nimport en_core_web_lg\nimport string\nnlp = en_core_web_lg.load()\n\n#Clean text before feeding it to spaCy\npunctuations = string.punctuation\n\n# Define function to cleanup text by removing personal pronouns, stopwords, and puncuation\ndef cleanup_text(docs):\n    texts = []\n    for doc in docs:\n        doc = nlp(doc)\n        tokens = [tok.lemma_.lower().strip() for tok in doc if tok.lemma_ != '-PRON-']\n        tokens = [tok for tok in tokens if tok not in STOPWORDS and tok not in punctuations]\n        tokens = ' '.join(tokens)\n        texts.append(tokens)\n    return pd.Series(texts)","cell_type":"code","execution_count":null,"metadata":{"_uuid":"89df3b57be13a7bf7e88e10631562886a15c0934","_cell_guid":"bb76cb4d-6cee-4bdb-bfda-9d4f77d635b7","collapsed":true}},{"outputs":[],"source":"spacy_cleaned_train = cleanup_text(X_train['text'])\nspacy_cleaned_test =  cleanup_text(X_test['text'])","cell_type":"code","execution_count":null,"metadata":{"_uuid":"9af6b2a7d47cd30597dbc070635ef996cce6cd1c","_cell_guid":"c1bed23f-eba8-44c3-9c60-156803a4dc88","collapsed":true}},{"outputs":[],"source":"print('Spacy Vectors...',round(time()-start,0))\ntrain_vec = [doc.vector for doc in nlp.pipe(list(spacy_cleaned_train), batch_size=500, n_threads=4)]\ntrain_vec = np.array(train_vec)\n\nX_train[['spacy_vec_'+str(i) for i in range(300)]] = pd.DataFrame(train_vec.tolist())","cell_type":"code","execution_count":null,"metadata":{"_uuid":"8b50fc72e4ff883d7cf9cbcb0fb681357af9385f","_cell_guid":"d932d701-0834-4fbb-b9c3-efa3d217aba9","collapsed":true}},{"outputs":[],"source":"test_vec = [doc.vector for doc in nlp.pipe(spacy_cleaned_train, batch_size=500, n_threads=4)]\ntest_vec = np.array(train_vec)\n\nX_test[['spacy_vec_'+str(i) for i in range(300)]] = pd.DataFrame(test_vec.tolist())","cell_type":"code","execution_count":null,"metadata":{"_uuid":"c8d99f4aaf8a47a46e38108738c228509389eccf","_cell_guid":"477ca04e-cc5d-49a4-b38a-a2451e318b1a","collapsed":true}},{"outputs":[],"source":"print('Gensim Train...',round(time()-start,0))\nimport gensim\nfrom gensim.models import doc2vec\nLabeledSentence = gensim.models.doc2vec.LabeledSentence\n\n#Gensim doc2vec\ncorpus_train = [z.split() for z in spacy_cleaned_train]\ncorpus_test = [z.split() for z in spacy_cleaned_test]\ncorpus_all = corpus_train + corpus_test\n\n\ndef labelizeReviews(reviews, label_type):\n    labelized = []\n    for i,v in enumerate(reviews):\n        label = '%s_%s'%(label_type, i)\n        labelized.append(LabeledSentence(v, [label]))\n    return labelized\n\n\n\nX_train_lab = labelizeReviews(corpus_train, 'Train')\nX_test_lab = labelizeReviews(corpus_test, 'Test')\nAll_lab = labelizeReviews(corpus_all, 'All')\n\n\nmodel = doc2vec.Doc2Vec(All_lab, min_count=1, window=10, size=300, workers=6)\n\n\ndef getVecs(model, corpus, size, vecs_type):\n    vecs = np.zeros((len(corpus), size))\n    for i in range(0, len(corpus)):\n        vecs[i] = model.docvecs[i]\n    return vecs\n\nprint('Gensim Vectors...',round(time()-start,0))\ngen_train_vecs = getVecs(model, X_train_lab, 300, 'Train')\ngen_test_vecs = getVecs(model, X_test_lab, 300, 'Train')\n\n\nX_train[['d2v_vec_'+str(i) for i in range(300)]] = pd.DataFrame(gen_train_vecs.tolist())\nX_test[['d2v_vec_'+str(i) for i in range(300)]] = pd.DataFrame(gen_test_vecs.tolist())","cell_type":"code","execution_count":null,"metadata":{"_uuid":"7a316c5c56f88b62f7d9d26200127df684c22962","_cell_guid":"931cf529-0424-416f-be1f-c24a146d65bd","collapsed":true}},{"outputs":[],"source":"#SVD\ntfidf_vec = TfidfVectorizer(stop_words='english', ngram_range=(1,5),analyzer='char')\nfull_tfidf = tfidf_vec.fit_transform(X_train['text'].values.tolist() + X_test['text'].values.tolist())\ntrain_tfidf = tfidf_vec.transform(X_train['text'].values.tolist())\ntest_tfidf = tfidf_vec.transform(X_test['text'].values.tolist())\n\nn_comp = 20\nsvd_obj = TruncatedSVD(n_components=n_comp, algorithm='arpack')\nsvd_obj.fit(full_tfidf)\ntrain_svd = pd.DataFrame(svd_obj.transform(train_tfidf))\ntest_svd = pd.DataFrame(svd_obj.transform(test_tfidf))\n\nX_train[['svd_char_'+str(i) for i in range(20)]] = pd.DataFrame(train_svd)\nX_test[['svd_char_'+str(i) for i in range(20)]] = pd.DataFrame(test_svd)","cell_type":"code","execution_count":null,"metadata":{"_uuid":"d3d23bf5d84df7195f540fa5c0ee1c7276aedee0","_cell_guid":"78f6caaf-7258-4508-88be-d2e20ffd8a9d","collapsed":true}},{"outputs":[],"source":"splits = 4","cell_type":"code","execution_count":null,"metadata":{"_uuid":"a0007692ec5522f31426cda0fd19fb26c3e17de9","_cell_guid":"e4c9ab3a-16ec-4a97-b7d6-96dd463c412a","collapsed":true}},{"outputs":[],"source":"# Using Neural Networks and Facebook's Fasttext\nearlyStopping=EarlyStopping(monitor='val_loss', patience=0, verbose=0, mode='auto')\n\n# NN\ndef doAddNN(X_train,X_test,pred_train,pred_test):\n    X_train[\"nn_eap\"] = pred_train[:,0]\n    X_train[\"nn_hpl\"] = pred_train[:,1]\n    X_train[\"nn_mws\"] = pred_train[:,2]\n    X_test[\"nn_eap\"] = pred_test[:,0]\n    X_test[\"nn_hpl\"] = pred_test[:,1]\n    X_test[\"nn_mws\"] = pred_test[:,2]\n    return X_train,X_test\n\ndef initNN(nb_words_cnt,max_len):\n    model = Sequential()\n    model.add(Embedding(nb_words_cnt,32,input_length=max_len))\n    model.add(Dropout(0.3))\n    model.add(Conv1D(64,\n                     5,\n                     padding='valid',\n                     activation='relu'))\n    model.add(Dropout(0.3))\n    model.add(MaxPooling1D())\n    model.add(Flatten())\n    model.add(Dense(800, activation='relu'))\n    model.add(Dropout(0.5))\n    model.add(Dense(3, activation='softmax'))\n\n    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics = ['accuracy'])\n    return model\n\ndef doNN(X_train,X_test,Y_train):\n    max_len = 70\n    nb_words = 10000\n    \n    print('Processing text dataset')\n    texts_1 = []\n    for text in X_train['text']:\n        texts_1.append(text)\n\n    print('Found %s texts.' % len(texts_1))\n    test_texts_1 = []\n    for text in X_test['text']:\n        test_texts_1.append(text)\n    print('Found %s texts.' % len(test_texts_1))\n    \n    tokenizer = Tokenizer(num_words=nb_words)\n    tokenizer.fit_on_texts(texts_1 + test_texts_1)\n    sequences_1 = tokenizer.texts_to_sequences(texts_1)\n    word_index = tokenizer.word_index\n    print('Found %s unique tokens.' % len(word_index))\n\n    test_sequences_1 = tokenizer.texts_to_sequences(test_texts_1)\n\n    xtrain_pad = pad_sequences(sequences_1, maxlen=max_len)\n    xtest_pad = pad_sequences(test_sequences_1, maxlen=max_len)\n    del test_sequences_1\n    del sequences_1\n    nb_words_cnt = min(nb_words, len(word_index)) + 1\n\n    # we need to binarize the labels for the neural net\n    ytrain_enc = np_utils.to_categorical(Y_train)\n    \n    kf = model_selection.KFold(n_splits=splits, shuffle=True, random_state=2017)\n    cv_scores = []\n    pred_full_test = 0\n    pred_train = np.zeros([xtrain_pad.shape[0], 3])\n    for dev_index, val_index in kf.split(xtrain_pad):\n        dev_X, val_X = xtrain_pad[dev_index], xtrain_pad[val_index]\n        dev_y, val_y = ytrain_enc[dev_index], ytrain_enc[val_index]\n        model = initNN(nb_words_cnt,max_len)\n        model.fit(dev_X, y=dev_y, batch_size=32, epochs=4, verbose=1,\n                  validation_data=(val_X, val_y),callbacks=[earlyStopping])\n        pred_val_y = model.predict(val_X)\n        pred_test_y = model.predict(xtest_pad)\n        pred_full_test = pred_full_test + pred_test_y\n        pred_train[val_index,:] = pred_val_y\n    return doAddNN(X_train,X_test,pred_train,pred_full_test/splits)\n\n# Fast Text\n\ndef doAddFastText(X_train,X_test,pred_train,pred_test):\n    X_train[\"ff_eap\"] = pred_train[:,0]\n    X_train[\"ff_hpl\"] = pred_train[:,1]\n    X_train[\"ff_mws\"] = pred_train[:,2]\n    X_test[\"ff_eap\"] = pred_test[:,0]\n    X_test[\"ff_hpl\"] = pred_test[:,1]\n    X_test[\"ff_mws\"] = pred_test[:,2]\n    return X_train,X_test\n\n\ndef initFastText(embedding_dims,input_dim):\n    model = Sequential()\n    model.add(Embedding(input_dim=input_dim, output_dim=embedding_dims))\n    model.add(GlobalAveragePooling1D())\n    model.add(Dense(3, activation='softmax'))\n\n    model.compile(loss='categorical_crossentropy',\n                  optimizer='adam',\n                  metrics=['accuracy'])\n    return model\n\ndef preprocessFastText(text_docs):\n    text_docs = text_docs.replace(\"' \", \" ' \")\n    signs = set(',.:;\"?!')\n    prods = set(text_docs) & signs\n    if not prods:\n        return text_docs\n\n    for sign in prods:\n        text_docs = text_docs.replace(sign, ' {} '.format(sign) )\n    return text_docs\n\ndef create_docs(df, n_gram_max=2):\n    def add_ngram(q, n_gram_max):\n            ngrams = []\n            for n in range(2, n_gram_max+1):\n                for w_index in range(len(q)-n+1):\n                    ngrams.append('--'.join(q[w_index:w_index+n]))\n            return q + ngrams\n    docs = []\n    for doc in df.text:\n        doc = preprocessFastText(doc).split()\n        docs.append(' '.join(add_ngram(doc, n_gram_max)))\n    return docs\n\ndef doFastText(X_train,X_test,Y_train):\n    min_count = 2\n\n    docs = create_docs(X_train)\n    tokenizer = Tokenizer(lower=False, filters='')\n    tokenizer.fit_on_texts(docs)\n    num_words = sum([1 for _, v in tokenizer.word_counts.items() if v >= min_count])\n\n    tokenizer = Tokenizer(num_words=num_words, lower=False, filters='')\n    tokenizer.fit_on_texts(docs)\n    docs = tokenizer.texts_to_sequences(docs)\n\n    maxlen = 300\n\n    docs = pad_sequences(sequences=docs, maxlen=maxlen)\n    input_dim = np.max(docs) + 1\n    embedding_dims = 20\n\n    # we need to binarize the labels for the neural net\n    ytrain_enc = np_utils.to_categorical(Y_train)\n\n    docs_test = create_docs(X_test)\n    docs_test = tokenizer.texts_to_sequences(docs_test)\n    docs_test = pad_sequences(sequences=docs_test, maxlen=maxlen)\n    xtrain_pad = docs \n    kf = model_selection.KFold(n_splits=3, shuffle=True, random_state=2017)\n    pred_full_test = 0\n    pred_train = np.zeros([xtrain_pad.shape[0], 3])\n    for dev_index, val_index in kf.split(xtrain_pad):\n        dev_X, val_X = xtrain_pad[dev_index], xtrain_pad[val_index]\n        dev_y, val_y = ytrain_enc[dev_index], ytrain_enc[val_index]\n        model = initFastText(embedding_dims,input_dim)\n        model.fit(dev_X, y=dev_y, batch_size=32, epochs=28, verbose=1,\n                  validation_data=(val_X, val_y),callbacks=[earlyStopping])\n        pred_val_y = model.predict(val_X)\n        pred_test_y = model.predict(docs_test)\n        pred_full_test = pred_full_test + pred_test_y\n        pred_train[val_index,:] = pred_val_y\n    return doAddFastText(X_train,X_test,pred_train,pred_full_test/3)\nprint('Other cool methods..',round(time()-start,0))\nX_train,X_test = doFastText(X_train,X_test,Y_train)\nX_train,X_test = doNN(X_train,X_test,Y_train)","cell_type":"code","execution_count":null,"metadata":{"_uuid":"790b2f4906be95944867ff5167f450415c6a8f13","_cell_guid":"3abe3cfb-97fe-4d04-a1f3-04f5d2b7423b","scrolled":true,"collapsed":true}},{"outputs":[],"source":"# Final Model\n# XGBoost\ndef runXGB(train_X, train_y, test_X, test_y=None, test_X2=None, seed_val=0, child=1, colsample=0.3):\n    param = {}\n    param['objective'] = 'multi:softprob'\n    param['eta'] = 0.1\n    param['max_depth'] = 3\n    param['silent'] = 1\n    param['num_class'] = 3\n    param['eval_metric'] = \"mlogloss\"\n    param['min_child_weight'] = child\n    param['subsample'] = 0.8\n    param['colsample_bytree'] = colsample\n    param['seed'] = seed_val\n    num_rounds = 2000\n\n    plst = list(param.items())\n    xgtrain = xgb.DMatrix(train_X, label=train_y)\n\n    if test_y is not None:\n        xgtest = xgb.DMatrix(test_X, label=test_y)\n        watchlist = [ (xgtrain,'train'), (xgtest, 'test') ]\n        model = xgb.train(plst, xgtrain, num_rounds, watchlist, early_stopping_rounds=30, verbose_eval=20)\n    else:\n        xgtest = xgb.DMatrix(test_X)\n        model = xgb.train(plst, xgtrain, num_rounds)\n\n    pred_test_y = model.predict(xgtest, ntree_limit = model.best_ntree_limit)\n    if test_X2 is not None:\n        xgtest2 = xgb.DMatrix(test_X2)\n        pred_test_y2 = model.predict(xgtest2, ntree_limit = model.best_ntree_limit)\n    return pred_test_y, pred_test_y2, model\n\ndef do(X_train,X_test,Y_train):\n    drop_columns=[\"id\",\"text\",\"words\"]\n    x_train = X_train.drop(drop_columns+['author'],axis=1)\n    x_test = X_test.drop(drop_columns,axis=1)\n    y_train = Y_train\n    \n    kf = model_selection.KFold(n_splits=4, shuffle=True, random_state=2017)\n    cv_scores = []\n    pred_full_test = 0\n    pred_train = np.zeros([x_train.shape[0], 3])\n    for dev_index, val_index in kf.split(x_train):\n        dev_X, val_X = x_train.loc[dev_index], x_train.loc[val_index]\n        dev_y, val_y = y_train[dev_index], y_train[val_index]\n        pred_val_y, pred_test_y, model = runXGB(dev_X, dev_y, val_X, val_y, x_test, seed_val=0, colsample=0.7)\n        pred_full_test = pred_full_test + pred_test_y\n        pred_train[val_index,:] = pred_val_y\n        cv_scores.append(metrics.log_loss(val_y, pred_val_y))\n    print(\"cv scores : \", cv_scores)\n    return pred_full_test/4\nresult = do(X_train,X_test,Y_train)\n\nresult = pd.DataFrame(list(result),columns=['EAP','HPL','MWS'])\nresult['id'] = X_test['id']\nresult.to_csv('output_easier_process_version.csv',index=False)\nprint('Time to completion: ',round(time()-start,0))","cell_type":"code","execution_count":null,"metadata":{"_uuid":"561397401f6978bb3ba70076a3fbe7e0a77e62a7","_cell_guid":"94e93d29-b79d-408b-be06-b8493494af8e","collapsed":true}},{"outputs":[],"source":"len(result)","cell_type":"code","execution_count":null,"metadata":{"_uuid":"e9d49f6465438a7a35284550594776713322872a","_cell_guid":"1540b851-fcbe-4a17-98f0-96a8ed8cec3c","collapsed":true}},{"outputs":[],"source":"","cell_type":"code","execution_count":null,"metadata":{"_uuid":"d238a896afb7745609b1d03e1cd2cfad494c3097","_cell_guid":"a91fa843-2770-4baf-932f-7729ca77977e","collapsed":true}},{"outputs":[],"source":"result.to_csv('test.csv')","cell_type":"code","execution_count":null,"metadata":{"_uuid":"e50756eb56f1ed07b794b63a278a1502a76bebe2","_cell_guid":"d7fce055-a72e-409f-b0c7-38111508b0be","collapsed":true}},{"outputs":[],"source":"","cell_type":"code","execution_count":null,"metadata":{"_uuid":"e80771db4f85e3a602c00ccabcd57cccdfed19d3","_cell_guid":"e0e35bca-6d13-4e87-9ef0-10de6ca48ed0","collapsed":true}}],"nbformat":4,"metadata":{"language_info":{"version":"3.6.3","pygments_lexer":"ipython3","mimetype":"text/x-python","codemirror_mode":{"version":3,"name":"ipython"},"file_extension":".py","name":"python","nbconvert_exporter":"python"},"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"}}}