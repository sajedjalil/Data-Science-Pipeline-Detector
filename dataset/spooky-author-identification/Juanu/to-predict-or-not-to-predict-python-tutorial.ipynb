{"nbformat_minor":1,"nbformat":4,"cells":[{"source":"#### Last Update: 2017-10-27\n***\n\n# Tidy Notebook\n***\nThe idea for this notebook, is to also try to keep it as tidy as possible, in order to make it easy to understand for the reader. This includes trying to write every analysis we do on the markdown cells, and explain the code as much as possible on the code comments. ** COMMENTS ARE EVERY PROGRAMMERS FRIENDS **\n\n**# About me\n***\nI've been a developer for more than 12 years and recently, just a few months ago, I got captivated by the magic of machine learning, and how easy it got for us programmers to get into it. My main motivation is learning and grow in the Machine Learning area, but in the process, I'd love to help anyone who's not been on the programming field, to make their career path easier. <br>\n> Finally we get to make AI!\n\nI'm from Buenos Aires, Argentina, 33 years old.<br>\nIf you want; you can reach me on:\n* Linked In: https://www.linkedin.com/in/juanuhaedo/\n* Twitter: https://www.twitter.com/juanumusic/\n* GitHub: https://github.com/HarkDev/\n\n**Don't forget to upvote if you liked this notebook!**","metadata":{"_uuid":"b96302ec7fca94ce4398a893b02af93ef7f69755","_cell_guid":"a446fd0e-646c-41ac-b0c9-790a2fe572a2"},"cell_type":"markdown"},{"source":"# 1. First Steps\n***\n## 1.1 Import Required Libraries\n***\nThe first steps on every python script/notebook is to import the required libraries.<br>\n**What are libraries?**\n> Libraries are like pieces of code that do a lot of stuff, and that we use, so that we do not need to write them all over. \n\nWe'll import the following libraries:\n* **numpy**: for linear algebra operations.\n* **pandas**: for data processing, reading csv files, etc.[](http://)\n* **matplotlib**: most used library for plotting on python.\n* **seaborn**: A library that used matplotlib and wraps functionality, so that drawing plots is even easier.\n* **nltk**: Natural Language Toolkit. We will use some of the functions from the library, but not all of them.\n* **string**: Python's string library","metadata":{"_uuid":"84a3026c96e324b9526038fff54f1922e63692cb","_cell_guid":"e3590106-7bdf-4098-999d-6d76ab797bf4"},"cell_type":"markdown"},{"outputs":[],"source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt # Library for drawing plots.\nimport seaborn as sns # An extension of matplotlib that helps make plots easier in less code\nimport nltk\nimport string","execution_count":null,"metadata":{"_uuid":"ce3c536cd70422503f02c1a9af51efee161dcb30","collapsed":true,"_cell_guid":"8419724f-5918-4721-889b-74b512c9baf4"},"cell_type":"code"},{"source":"## 1.2 Read CSV files.\n***\nUsing pandas we will read the datasets provided, which are in CSV format.<br>\n**What is CSV format?**\n> CSV stands for: *\"comma separated values* and is one of the most common ways to store information on plain text file.\n\nLet's read the csv into two variables:\n* **df_train** will contain the train information\n* **df_test** will contain the test information**","metadata":{"_uuid":"7a0487eb1fbf22cb49666513db53ef9bb9413e65","_cell_guid":"23078d6e-512a-4e0d-9f8a-4d0015f08dee"},"cell_type":"markdown"},{"outputs":[],"source":"df_train = pd.read_csv('../input/train.csv') # Train dataset\ndf_test = pd.read_csv('../input/test.csv') # Test dataset","execution_count":null,"metadata":{"_uuid":"bfaa0878a79d238f49c21165c34d0c7715ebecf8","collapsed":true,"_cell_guid":"5baffc3c-366a-4475-b2ea-eec368da0ecd"},"cell_type":"code"},{"source":"## 1.3 Take a peek at the data\n***\nIt's always important to actually look at the data loaded, in order to know that we are dealing with what we had in mind. In this case, we can see that the data contained, are fragments of books, identified by their author. <br>\n* The `id` column is a unique value that identified the row. We will discard this for our predictions.\n* The `text` column contains some text written by a specific author.\n* The `author` column, contains a string, that's identifies the author that wrote the text. This is the value that we will use to train our model and let it know to whom it corresponds, so that when me make the predictions, our model will know what values to predict.\n\nThe `.sample(10)` function of pandas, displays a random sample from the entire dataframe, with the size passed. In this case, we are going to take a peek at 10 samples.","metadata":{"_uuid":"1b8d18142b09b62181dcdd8c8e7579fe882a5b5a","_cell_guid":"1dba47c7-d23d-49bf-a8fb-fd9526a6587d"},"cell_type":"markdown"},{"outputs":[],"source":"df_train.sample(10) # Look at 10 random samples of the dataframe","execution_count":null,"metadata":{"_uuid":"515d49809ddfebc09321a11b7f74143c5666925c","_cell_guid":"9f53dcaf-4343-4259-9053-9c29aa507901"},"cell_type":"code"},{"source":"# 2 Data Munging\n***\nWe will now process the data, in order to try to make the best features for when we need to make our predictions.\n\nThere are great libraries for working with language, and text. The actual state of the art is NLTK(Natural Language Toolkit). This tutorial will cover the basics of treating text, without the help of NLTK, in order to understand what happens behind this library, although not everything that it does.\n\n## 1.2 Feature Engineering\n***\n### 1.2.1 Remove Punctuation\n***\nRemoving punctiation is important when you work with text, since you don't want your predictions to take special characters like '.', ',' or ':' into account.\n\nOn this cell, I'll show you how to make this removal by hand, even though there are many libaries to do this (including NLTK).\n\nThe punctuation attribute in `string.punctuation` contains a list of punctuations so we don't have to type them by hand.","metadata":{"_uuid":"2999ac7042a0923b1143793614e044bb13ad3199","_cell_guid":"1fc0669e-c3e7-48d8-90d4-4f0929402f66"},"cell_type":"markdown"},{"outputs":[],"source":"\n# We create a function to do the punctuation removal\ndef remove_punctuation(text):\n\n    # For each punctuation in our list\n    for punct in string.punctuation:\n        # Replace the actual punctuation with a space.\n        text = text.replace(punct,'')\n\n    # Return the new text\n    return text\n\n# Now, we will apply the remove punctiation to all our text\ndf_train.text = df_train.text.apply(remove_punctuation)\ndf_test.text = df_test.text.apply(remove_punctuation)","execution_count":null,"metadata":{"_uuid":"53b3df38d32c189302842e9c32ddd4d2f2a269c0","collapsed":true,"_cell_guid":"7b09798c-4ccc-43a9-b42c-f4de403f24cd"},"cell_type":"code"},{"source":"### 1.2.2 Stemming\n***\nStemming is the process of converting words that mean the same. For example, verb conjugation. Words like run, running, ran, all are the same word on different conjugations.\n\nWe will use NLTK for this technique.\n\n***\nA side note here. You will see that the way I apply stemming is in one line of code with a lot of things going on. I will try to explain the best I can what's going on...\n\n1.  In every one-liner code, you need to try to understand whats happening first in all of that. In our case, the first thing thats happening is that we are splitting our text by spaces :`text.split(\" \")`. This returns a list of words.\n2. We create a for loop (list comprehension), looping on each word and appying the stem using `stemmer.stem(word)`.\n3. We rejoin the words, with a space, to get the sentence stemmed using `\" \".join(...)`\n\n**NOTE:** If the explanation above is not clear, please, leave a comment with the stuff you don't understand and I will try to correct the best possible way.\n\n# STEMMING WARNING\n***\nFor this challenge, the idea is to classify to whom the text belongs","metadata":{"_uuid":"697d05e2d1b015827aacb7e6ef1103337543f68c","_cell_guid":"c2562649-ad32-49e4-bab4-895cdf6a1c78"},"cell_type":"markdown"},{"outputs":[],"source":"APPLY_STEMMING = False\n\nif APPLY_STEMMING:\n    import nltk.stem as stm # Import stem class from nltk\n    import re\n    stemmer = stm.PorterStemmer()\n\n    # Crazy one-liner code here...\n    # Explanation above...\n    df_train.text = df_train.text.apply(lambda text: \" \".join([stemmer.stem(word) for word in text.split(\" \")]))\n    df_test.text = df_test.text.apply(lambda text: \" \".join([stemmer.stem(word) for word in text.split(\" \")]))","execution_count":null,"metadata":{"_uuid":"26c15d3122fb472f06ae1bd4eb0517d3f437c107","collapsed":true,"_cell_guid":"f62c6889-705e-411e-8391-83b51ea4053c"},"cell_type":"code"},{"source":"### 1.2.3 Vectorization\nOne of the most common feature engineering when dealing with text, it's vectorization.<br>\nOn this notebook we will make use of two different implementations: **Count Vectorizer** and **TfidfVectorizerÂ¶**\n\nBoth work by counting each word on a corpus of text, so here I will show code for each one, but both have the same following parameters which are important to understand:\n\n#### NGram Range\n***\nNgram range is a parameter that will do the vectorization for each word, and for each combination of n words. For example, if `ngram_range =(1,2)` the vectorization will be:\n\n|the|cat|is|on|table|the_cat|cat_is|is_on|on_the|the_table|\n|---|---|--|--|-----|-|-|-|-|-|\n|2|1|1|1|1|1|1|1|1|1|\n\n### Stop Words\n***\nFinally, the stop words parameter will let us remove the stop words such as *the, a, and, or, etc...*. We can pass the name of the language (scikit learn only comes with english by default) or a list of words, on our example, we will use **english**.\n\n###  1.2.3.1 Count Vectorizer\n***\nCountVectorizer takes each text, and creates a column for each word that exists on the corpus, and sets the number of times that that word repeats, on the column, for a given text.<br>\n\nFor example, the text *\"the cat is on the table\"* will be vectorized as:\n\n|the|cat|is|on|table|\n|---|---|--|--|-----|\n|2|1|1|1|1|","metadata":{"_uuid":"382b288624956ec247c501043f0586fdfe860c77","_cell_guid":"32170b5d-eef3-4778-9e17-271647b1bd82"},"cell_type":"markdown"},{"outputs":[],"source":"from sklearn.feature_extraction.text import CountVectorizer # Import the library to vectorize the text\n\n# Instantiate the count vectorizer with an NGram Range from 1 to 3 and english for stop words.\ncount_vect = CountVectorizer(ngram_range=(1,3),stop_words='english')\n\n# Fit the text and transform it into a vector. This will return a sparse matrix.\ncount_vectorized = count_vect.fit_transform(df_train.text)","execution_count":null,"metadata":{"_uuid":"42b21c480f951458e5f9fa2a57192168a9f0e401","collapsed":true,"_cell_guid":"759c1a01-6398-4187-9334-75f73083f5fc"},"cell_type":"code"},{"source":"### 1.2.3.2 TfidfVectorizer\n***\nTfidfVectorizer is another way of treating each word. The name *TfIdf* stands for *\"Term Frequency Inverse Document Frequency*. Since [SciKit's learn documentation explains it so well](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html), I'm just going to cite what this does:\n> Scales down the impact of tokens that occur very frequently in a given corpus and that are hence empirically less informative than features that occur in a small fraction of the training corpus.\n\nAs you can see, her we set a value of (1,1) for ngram_range. This is because, after trying a few values, I found that the best accuracy on predictions, using TfIdf, is to use an ngram_range of 1. <br>\nThe process of trying different values is called **Hyperparameter Tuning**.","metadata":{"_uuid":"74014f425043be23ba30d5a420cbbc4dae775e7a","_cell_guid":"9b5475de-bfee-46cc-a77b-49e0424e5532"},"cell_type":"markdown"},{"outputs":[],"source":"from sklearn.feature_extraction.text import TfidfVectorizer # Import the library to vectorize the text\n\n# Instantiate the count vectorizer with an NGram Range from 1 to 3 and english for stop words.\ntfidf_vect = TfidfVectorizer(ngram_range=(1,3), stop_words='english')\n\n# Fit the text and transform it into a vector. This will return a sparse matrix.\ntfidf_vectorized = tfidf_vect.fit_transform(df_train.text)","execution_count":null,"metadata":{"_uuid":"2267650140d8aea6e5a5bd3bba5b840c32ad994b","collapsed":true,"_cell_guid":"dd8e5d0a-ac1a-40c5-877f-227139e6e1ee"},"cell_type":"code"},{"source":"# 3 Model\n***\nIt's now time to prepare our model. We will prepare the data, validate itm and finally make the predictions.\n\n## 3.1 Split Train and Test for validation\n***\nIt's important to validate our model, so that we know what we are working with. In order to validate it, we need to train it with some values, and then, predict other values that we know the real answer.\n\nTo do this, we will split the train data, since it contains the target values (author). We will use a piece of the data to train the model, and the other piece, to make the predictions and then validate the predictions.\n\nRemember that we played with both **CountVectorizer** and **TfidfVectorizer** so from now on we will create two models and see which one performs better.","metadata":{"_uuid":"5c9a42f1ba2fd3cfef945ed73a452669a6847e7f","_cell_guid":"b4ef90f4-124c-4221-9516-becdfff23836"},"cell_type":"markdown"},{"outputs":[],"source":"from sklearn.model_selection import train_test_split # Import the function that makes splitting easier.\n\n# Split the vectorized data. Here we pass the vectorized values and the author column.\n# Also, we specify that we want to use a 75% of the data for train, and the rest for test.\n\n###########################\n# COUNT VECTORIZED TOKENS #\n###########################\nX_train_count, X_test_count, y_train_count, y_test_count = train_test_split(count_vectorized, df_train.author, train_size=0.75)\n\n###########################\n# TFIDF VECTORIZED TOKENS #\n###########################\nX_train_tfidf, X_test_tfidf, y_train_tfidf, y_test_tfidf = train_test_split(tfidf_vectorized, df_train.author, train_size=0.75)","execution_count":null,"metadata":{"_uuid":"77048a9de5e722cb71a302928552bd8df3468981","_cell_guid":"16e21ba2-7c40-4553-ba20-59bbf97d3e95"},"cell_type":"code"},{"source":"## 3.2 Multinomial Naive Bayes\n***\nMultinomial Naive Bayes is one of the most common algorithm used in text clasification, so we will work with it.","metadata":{"_uuid":"1f7cc8c4717a30c5308ff656bd4ae304d3a4d900","_cell_guid":"b6ec5bbe-87ca-4b68-955b-87862ffb2d88"},"cell_type":"markdown"},{"outputs":[],"source":"# First, import the Multinomial Naive bayes library from sklearn \nfrom sklearn.naive_bayes import MultinomialNB\n\n# Instantiate the model.\n# One for Count Vectorized words\nmodel_count_NB = MultinomialNB()\n# One for TfIdf vectorized words\nmodel_tfidf_NB = MultinomialNB()\n\n# Train the model, passing the x values, and the target (y)\nmodel_count_NB.fit(X_train_count, y_train_count)\nmodel_tfidf_NB.fit(X_train_tfidf, y_train_tfidf)","execution_count":null,"metadata":{"_uuid":"ff9cf8f97a50b85dff59cb2f8129887bf642085b","_cell_guid":"f6472353-1ba1-442a-aef8-737d1f1c1273"},"cell_type":"code"},{"source":"## 3.3. Predict test values\n***\nOnce we have our model trained, we can predict the test values, and then compare them to the real values. ","metadata":{"_uuid":"0a9ec8d34598b7d507a848fa57d5f8646ba158cc","_cell_guid":"9773d306-b734-431f-ae51-262b0950fb84"},"cell_type":"markdown"},{"outputs":[],"source":"# Predict the values, using the test features for both vectorized data.\npredictions_count = model_count_NB.predict(X_test_count)\npredictions_tfidf = model_tfidf_NB.predict(X_test_tfidf)","execution_count":null,"metadata":{"_uuid":"266708d88d8508dea23077346c657d9095e791da","collapsed":true,"_cell_guid":"72873db7-0f77-4703-8573-c333b10f41aa"},"cell_type":"code"},{"source":"## 3.4 Validate the model\n***\nNext step is validate the model. We will import the `accuracy_score` function of sklearn, to see how is our accuracy.","metadata":{"_uuid":"b9d38174c1bb74d3c2d740612686d4f669c8bb01","_cell_guid":"1a465f8f-d77a-41c8-897e-b6e2eb3ebacf"},"cell_type":"markdown"},{"outputs":[],"source":"# Primero calculamos el accuracy general del modelo\nfrom sklearn.metrics import accuracy_score\naccuracy_count = accuracy_score(y_test_count, predictions_count)\naccuracy_tfidf = accuracy_score(y_test_tfidf, predictions_tfidf)\nprint('Count Vectorized Words Accuracy:', accuracy_count)\nprint('TfIdf Vectorized Words Accuracy:', accuracy_tfidf)","execution_count":null,"metadata":{"_uuid":"f51ea188b84a3992e6b2d3851bd09d356f0c5c65","_cell_guid":"f93f84ea-5a34-49a0-a20d-cda3b94cefba"},"cell_type":"code"},{"source":"The accuracy seems pretty good, let's look at the confusion matrix.\n** What is confusion matrix?**\n> A confusion matrix is a matrix where we can see where the predicted values are, and where they should be.","metadata":{"_uuid":"0df9026538bcab32eb6d09a7bd111cb78e553cf7","_cell_guid":"a4e2d3b8-8013-4c00-ba4b-b55e38891948"},"cell_type":"markdown"},{"outputs":[],"source":"# Import the confusion matrix method from sklearn\nfrom sklearn.metrics import confusion_matrix\n\n# Calculate the confusion matrix passing the real values and the predicted ones\n# Count\nconf_mat_count = confusion_matrix(y_test_count, predictions_count)\n# tfIdf\nconf_mat_tfidf = confusion_matrix(y_test_tfidf, predictions_tfidf)\n\n# Set plot size\nplt.figure(figsize=(12,10))\n# Use 2 subplots.\nplt.subplot(1,2,1)\n\n# Finally, plot the confusion matrix using seaborn's heatmap.\nsns.heatmap(conf_mat_count.T, square=True, annot=True, fmt='d', cbar=True,\n            xticklabels=y_test_count.unique(), yticklabels=y_test_count.unique())\nplt.xlabel('True values')\nplt.ylabel('Predicted Values');\nplt.title('Count Vectorizer', fontsize=16)\n\nplt.subplot(1,2,2)\n# Finally, plot the confusion matrix using seaborn's heatmap.\nsns.heatmap(conf_mat_tfidf.T, square=True, annot=True, fmt='d', cbar=True,\n            xticklabels=y_test_tfidf.unique(), yticklabels=y_test_tfidf.unique())\nplt.xlabel('True values')\nplt.ylabel('Predicted Values');\nplt.title('TfIdf Vectorizer', fontsize=16)","execution_count":null,"metadata":{"_uuid":"3906fee133fa56bfc8b351b0c164fcbf184d444f","_cell_guid":"82e09421-7d63-44da-95e0-aff34a54bb71"},"cell_type":"code"},{"source":"#### Count Vectorizer\nWe can see that:\n* For EAP, we correctly predicted 1309 cases, and missed 155 + 78\n* For MWS, we correctly predicted 1117 cases, and missed 145 + 106\n* For HPL, we correctly predicted 1613 cases, and missed 138 + 234\n\n#### TfIdf Vectorizer\nWe can see that:\n* For EAP, we correctly predicted 1742 cases, and missed 83 + 167\n* For MWS, we correctly predicted 999 cases, and missed 301 + 85\n* For HPL, we correctly predicted 1241 cases, and missed 238 + 39\n\n> **Our best model seems to be the one with CountVectorizer, so we will make our submission with that model.**","metadata":{"_uuid":"b3734ac224703d6e603fdc89d0d7618c3b19df9c","_cell_guid":"59f50f6f-bd2e-46e3-a392-36a92af3241c"},"cell_type":"markdown"},{"source":"# 4 Final prediction and submission\n***\nWe have our model trained, with a somehow good accuracy. We will now train the model with the entire dataset, in order to show most of the data we have, and then we'll make the predictions wih the test dataset.\n\n## 4.1 Train with full dataset\n***","metadata":{"_uuid":"d5f8eec624d80c2a895582008d817726c273ab7b","_cell_guid":"54d9af3b-6d93-4876-a501-0a0cd4e455db"},"cell_type":"markdown"},{"outputs":[],"source":"# Instantiate the model.\nmodel_NB = MultinomialNB()\n\n# Train the model, passing the x values, and the target (y)\n# the vectorized variable contains all the test data.\nmodel_NB.fit(count_vectorized, df_train.author)","execution_count":null,"metadata":{"_uuid":"5ced3d3e8ff2da207ecd33f87e8af56ccdcba38a","_cell_guid":"779660d4-ae71-41bf-a048-ca87fa9bc0e0"},"cell_type":"code"},{"source":"## 4.2 Remove punctiation and vectorize test with the same vectorizer\n***\nOur predictive model needs to receive as inputs, the same features with which it was trained<br>\nTo do this, we will be using the CountVectorizer we fitted at the begining of this notebook.","metadata":{"_uuid":"059beb348260a8abd2d5f3ae411ee5806f6a397d","_cell_guid":"cdfe4519-3c42-4b9f-ad26-5e39563bcb9b"},"cell_type":"markdown"},{"outputs":[],"source":"# Transform the text to a vector, with the same shape of the trained data.\nX_test = count_vect.transform(df_test.text)","execution_count":null,"metadata":{"_uuid":"caf93687cb55c5525dccda4c8c0044dd701f79af","collapsed":true,"_cell_guid":"459b6cd7-f6fc-4d56-b7a1-cb70b0a0c740"},"cell_type":"code"},{"source":"## 4.3 Predict the values\n***\nNow it's time to make the predictions. We wil lnow use predict_proba instead of predict. This will return an array with the probabilities of being a certain author, instead of the class itself.","metadata":{"_uuid":"f302e7c7c15abeaa55f14d641a9443c4281095fc","_cell_guid":"d338393a-475c-46ed-909d-70806c161635"},"cell_type":"markdown"},{"outputs":[],"source":"# Run the prediction\npredicted_values = model_NB.predict_proba(X_test)","execution_count":null,"metadata":{"_uuid":"02734668345691aab283b1447b6472439cca91b2","collapsed":true,"_cell_guid":"ce664399-11cc-40da-90c4-703e3fa191f3"},"cell_type":"code"},{"source":"## 4.4 Generate Submission File\n***\nWith our values predicted, we can generate our submission file.  The submission file sholud contain the following columns:\n* id\n* EAP\n* HPL\n* MWS\n\nIf we look at the `classes_` attribute of the model, we can see that these are in the same order needed.","metadata":{"_uuid":"79bbd23c2768a9bddd5791d2c7f8918e30bb3a35","_cell_guid":"a6fd4986-26c6-45fc-ad60-c0884f2fbf66"},"cell_type":"markdown"},{"outputs":[],"source":"model_NB.classes_","execution_count":null,"metadata":{"_uuid":"26698bd9114a1fe53973e62b220d03d2b9343550","_cell_guid":"d63eeb65-36a9-49ac-9060-fd92681d2a20"},"cell_type":"code"},{"source":"Finally, we can now create our sumissions file. What I like to do, is to automatically set the date and time in which this file was generated and the accuracy. All on the filename.","metadata":{"_uuid":"70a549c2e1a3de9778577f36f4f2c3175356b524","_cell_guid":"b1ceb0eb-e4b6-466e-bd75-4c023fe26ab7"},"cell_type":"markdown"},{"outputs":[],"source":"# Import the time library\nimport time\n\n# Create the submission dataframe\ndf_submission = pd.DataFrame({\n    'id': df_test.id.values,\n    'EAP': predicted_values[:,0],\n    'HPL': predicted_values[:,1],\n    'MWS': predicted_values[:,2]\n})\n\n\n# Create the date and time string. (year month day _ hours minutes seconds)\ndatetime = time.strftime(\"%Y%m%d_%H%M%S\")\n\n# generate the file name with the date, the time and the accuracy of the count vectorized test.\nfilename = 'submission_' + datetime + '_acc_' + str(accuracy_count) + '.csv'\n\n# Finally, convert it to csv. Index=True tells pandas not to include the index as a column\ndf_submission.to_csv(filename, index=False)\n\nprint('File',filename,'created.')","execution_count":null,"metadata":{"_uuid":"fc8402a66ed58f9878cf4e62de07d6a6117cfe74","_cell_guid":"ac9d2d96-1cfe-4d1d-b5b3-d7be94f0f943"},"cell_type":"code"},{"source":"# Foreword\n***\nHope this tutorial has elped anyone starting with machine learning with text corpuses. If you use this notebook, or fork it, please, cite me anywhere, or at least, leave a comment!<br>\nIf you find any corrections, mistakes, or things to add, please, feel free to let me know on the comments too!","metadata":{"_uuid":"9461596c4cf9f3b75553677262b755e6116d9381","collapsed":true,"_cell_guid":"d4b162b6-c2e3-40c0-bab5-6ae90446c52f"},"cell_type":"markdown"},{"source":"* **Don't forget to upvote and/or comment if you liked this notebook!**","metadata":{"_uuid":"4732ab4b058522afd7aa24169181a4d8bd393a4b","collapsed":true,"_cell_guid":"d13c1d0d-54d8-4785-9942-1cd97d5a0829"},"cell_type":"markdown"},{"outputs":[],"source":"","execution_count":null,"metadata":{"_uuid":"bcec99e5b2e1de7a5e153e14809b2e034f94ea11","collapsed":true,"_cell_guid":"c2771eaa-1826-4acd-a7f2-2d790a5008f0"},"cell_type":"code"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"nbconvert_exporter":"python","file_extension":".py","name":"python","mimetype":"text/x-python","codemirror_mode":{"version":3,"name":"ipython"},"pygments_lexer":"ipython3","version":"3.6.3"}}}