{"metadata":{"language_info":{"file_extension":".py","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"version":"3.6.3","name":"python","pygments_lexer":"ipython3","nbconvert_exporter":"python"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat_minor":1,"nbformat":4,"cells":[{"metadata":{"_uuid":"9ac53d679466b9a912834f35996cb272cb9dae0a","_cell_guid":"65a056b8-5212-48a6-a8ce-487ff54e404b"},"cell_type":"markdown","source":"## Notes\n<p>\nIn this notebook you will learn how to train a simple LSTM RNN (Long-Short Term Model, Recurrent Neural Network) to generate text in the style of Edgar Allen Poe.  \n<br>\nI myself am actually quite new to NLP and deep learning, and this notebook was created mostly on information found in this tutorial:  \nhttps://chunml.github.io/ChunML.github.io/project/Creating-Text-Generator-Using-Recurrent-Neural-Network/  \n<br>\nThis was also a helpful resource for me when getting introduced to the subject:  \nhttp://colah.github.io/posts/2015-08-Understanding-LSTMs/  \n<br>\nAnd before that, it would be best to know the basics of neural networks, such as forward/back prop, layers, activation, etc.\n1. I would recommend Andrew Ng's machine learning course on coursera for a good starting point (Rated 4.9 out of 5 of 52,993 ratings):  \nhttps://www.coursera.org/learn/machine-learning  \nHe's actually recently released a 5-course specialization solely focused on neural networks; the 5th course titled 'Sequential Model' will cover RNNs and is expected to be available within the month.  \n<br>\nI won't be explaining the theory as to why LSTMs work, but hopefully you will gain a better understanding of what format the input data needs to be in, and how to assemble a simple RNN.\n</p>\n"},{"metadata":{"_uuid":"8f0e9b77d46ff9cbe609b207918efd961b7b389f","_cell_guid":"46de4a33-1cfd-44d6-a070-0a15a33e81f9"},"cell_type":"code","execution_count":null,"outputs":[],"source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport re # regex package\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Activation, TimeDistributed, LSTM, Dropout\nfrom keras import optimizers\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\ndef load_data(file, nrows = 100):\n    df = pd.read_csv('../input/' + file + '.csv', nrows = nrows)\n    return(df)\n\ndef shape_data(df):\n    df = df.groupby('author', as_index=True).agg([lambda x: ' '.join(x)])\n    return(df)\n\ndef to_char_list(string):\n    char_list = []\n    for char in string:\n        char_list.append(char)\n    return(char_list)\n\n## replace multiple whitespaces with single ' ' character\n## also trim the start and end of the string, because it will come from a list\n## so has leading and trailing '\"[ ' and ']\"' characters respectively\ndef clean_text(s):\n    replaced = re.sub('\\\\s+', ' ', s).strip() # replace multiple whitespaces with single ' ' character\n    replaced = replaced[3:-2] # remove leading and trailing '\"[ ' and ']\"' characters respectively\n    return(replaced)\n\n## Model\nHIDDEN_DIM = 256\nLAYER_NUM = 2\ndef get_model(learning_rate = 0.009):\n    model = Sequential()\n    model.add(LSTM(HIDDEN_DIM, input_shape=(None, VOCAB_SIZE), return_sequences=True))\n    model.add(Dropout(0.3))\n    for i in range(LAYER_NUM - 1):\n        model.add(LSTM(HIDDEN_DIM, return_sequences=True))\n        model.add(Dropout(0.3))\n    model.add(TimeDistributed(Dense(VOCAB_SIZE)))\n    model.add(Activation('softmax'))\n    optimizer = optimizers.Adam(lr=learning_rate, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n    model.compile(loss='categorical_crossentropy', optimizer=optimizer)\n    return(model)\n\n## Train model\nBATCH_SIZE = 64\nGENERATE_LENGTH = 150\ndef train_model(model, X, y, epochs, every=2):\n    nb_epoch = 0\n    while True:\n        #print('\\n\\n')\n        if not nb_epoch % every == 0:\n            model.fit(X, y, batch_size=BATCH_SIZE, verbose=0, epochs=1)\n            nb_epoch += 1\n        if nb_epoch % every == 0:\n            model.fit(X, y, batch_size=BATCH_SIZE, verbose=1, epochs=1)\n            print('nb_epoch = %i'%nb_epoch)\n            print(generate_text(model, GENERATE_LENGTH))\n            model.save_weights('checkpoint_{}_epoch_{}.hdf5'.format(HIDDEN_DIM, nb_epoch))\n            if nb_epoch % epochs == 0 and not nb_epoch == 0:\n                break\n            nb_epoch += 1\n            \n## Text Generation function\ndef generate_text(model, length):\n    ix = [np.random.randint(VOCAB_SIZE)]\n    #ix = [char_to_ix['.']]\n    y_char = [ix_to_char[ix[-1]]]\n    X = np.zeros((1, length, VOCAB_SIZE))\n    for i in range(length):\n        X[0, i, :][ix[-1]] = 1\n        #print(ix_to_char[ix[-1]], end=\"\")\n        ix = np.argmax(model.predict(X[:, :i+1, :])[0], 1)\n        y_char.append(ix_to_char[ix[-1]])\n    return ('').join(y_char)"},{"metadata":{"_uuid":"10d542861ba6c0b80aaa3f93dacf3aefed27bb3a","_cell_guid":"af83d14e-7215-4215-9e7b-3b226a4a3dd3"},"cell_type":"code","execution_count":null,"outputs":[],"source":"## Main ## \ntrain_sample = load_data('train', nrows=5000) # read first 5000 rows of the dataset\nprint(train_sample.shape)\n#test = load_data('test')\ntrain = shape_data(train_sample) # aggregate text column, groupped by author\nprint(train.shape)"},{"metadata":{"_uuid":"f7f7c1a4c18489a09e505581e7e4a3578bd401f0","_cell_guid":"bc431ee8-3642-413d-92ff-295c48384e82"},"cell_type":"markdown","source":"## store the text of each author in 1 combined string"},{"metadata":{"_uuid":"a8918f89c0b43b747cfa12ccbd262740645b9c59","_cell_guid":"70a96cda-4778-4ceb-beea-2bb73ed3daaa","collapsed":true},"cell_type":"code","execution_count":null,"outputs":[],"source":"eap_text = clean_text(str(train['text'].loc['EAP',:].values))\nmws_text = clean_text(str(train['text'].loc['MWS',:].values))\nhpl_text = clean_text(str(train['text'].loc['HPL',:].values))"},{"metadata":{"_uuid":"1f76f99c60bfdb8f413916b28c78091633eae7c8","_cell_guid":"d6237379-9c34-481a-9997-1429eaf1148b"},"cell_type":"markdown","source":"## convert that string into a list of characters"},{"metadata":{"_uuid":"e89484bb35d7418dd70465199e6c5afbd276031b","_cell_guid":"547aa1df-028a-40cf-8e17-11bda8ff285b"},"cell_type":"code","execution_count":null,"outputs":[],"source":"eap_char_list = to_char_list(eap_text)\nprint('num characters in EAP text: %i'%len(eap_char_list))\nmws_char_list = to_char_list(mws_text)\nprint('num characters in MWS text: %i'%len(mws_char_list))\nhpl_char_list = to_char_list(hpl_text)\nprint('num characters in HPL text: %i'%len(hpl_char_list))"},{"metadata":{"_uuid":"4af3b7f49d2791b40df1b01c0723d8a916cddf33","_cell_guid":"56c41763-8d16-4bc4-8ead-e4f1a0195b93"},"cell_type":"markdown","source":"## Create mappings from characters to numeric"},{"metadata":{"_uuid":"7811af5c725be635ac8f66d4f3a087be5e168b3e","_cell_guid":"223b3275-d139-4c1a-bab7-5a81dece85d4"},"cell_type":"code","execution_count":null,"outputs":[],"source":"eap_char_to_ix = {x:y for x,y in zip(set(eap_char_list),range(len(set(eap_char_list))))}\nmws_char_to_ix = {x:y for x,y in zip(set(mws_char_list),range(len(set(mws_char_list))))}\nhpl_char_to_ix = {x:y for x,y in zip(set(hpl_char_list),range(len(set(hpl_char_list))))}\nprint('num unique chars eap: {}\\nnum unique chars mws: {}\\nnum unique chars hpl: {}'.format(len(eap_char_to_ix),len(mws_char_to_ix),len(hpl_char_to_ix)))"},{"metadata":{"_uuid":"5e995ce32c08f62239efb9d9eafcf896c5516861","_kg_hide-input":true,"_cell_guid":"cb14b64d-1d35-4ac6-b8e8-03b1213a002b","_kg_hide-output":true,"collapsed":true},"cell_type":"code","execution_count":null,"outputs":[],"source":"## Quick Question: which characters does Edgar Allen Poe use that Mary Shelley does not?\n## (same question for H.P. Lovecraft and all 6 combinations).\n## List Comprehensions\n## Letters Edgar Allen Poe Uses\neap_not_mws = [x for x in list(eap_char_to_ix.keys()) if x not in list(mws_char_to_ix.keys())]\neap_not_hpl = [x for x in list(eap_char_to_ix.keys()) if x not in list(hpl_char_to_ix.keys())]\n## Letters Mary Shelley Uses\nmws_not_eap = [x for x in list(mws_char_to_ix.keys()) if x not in list(eap_char_to_ix.keys())]\nmws_not_hpl = [x for x in list(mws_char_to_ix.keys()) if x not in list(hpl_char_to_ix.keys())]\n## Letters Lovecraft Uses\nhpl_not_eap = [x for x in list(hpl_char_to_ix.keys()) if x not in list(eap_char_to_ix.keys())]\nhpl_not_mws = [x for x in list(hpl_char_to_ix.keys()) if x not in list(mws_char_to_ix.keys())]\n\n# Even when loading the full training set, this didn't really lead to any obvious character\n# abuses, except for some EAP and HPL text having non-english characters."},{"metadata":{"_uuid":"31a548426f6feead94c7205827c6f046d71969ca","_cell_guid":"9268ed07-2eb0-4405-bc9c-817bd1ea6c7a"},"cell_type":"markdown","source":"## More Data Prepping"},{"metadata":{"_uuid":"edc4bb94728cd2ae9fefa5482f60c332159b25c5","_cell_guid":"1bef63f4-77b7-4e83-b96a-4d5716aa218e"},"cell_type":"code","execution_count":null,"outputs":[],"source":"AUTHOR = 'EAP'\nif AUTHOR == 'EAP':\n    data = eap_char_list # our text data in 'by character' format\n    char_to_ix = eap_char_to_ix # our dictionary\n\nix_to_char = dict (zip(char_to_ix.values(),char_to_ix.keys())) # reverse of our dictionary\nVOCAB_SIZE = len(char_to_ix) # length of our dictionary\nSEQ_LENGTH = 150\n#SEQ_LENGTH = (len(eap_char_list) + len(mws_char_list) + len(hpl_char_list)) // train_sample.shape[0] # how long to make a sentence\nNUM_SEQUENCES = len(data)//SEQ_LENGTH # number of sequences we're training on\n\n## Making a dataset to train on from what we have so far\nX = np.zeros((NUM_SEQUENCES, SEQ_LENGTH, VOCAB_SIZE))\ny = np.zeros((NUM_SEQUENCES, SEQ_LENGTH, VOCAB_SIZE))\nfor i in range(0, NUM_SEQUENCES):\n    X_sequence = data[i*SEQ_LENGTH:(i+1)*SEQ_LENGTH]\n    X_sequence_ix = [char_to_ix[value] for value in X_sequence]\n    input_sequence = np.zeros((SEQ_LENGTH, VOCAB_SIZE))\n    for j in range(SEQ_LENGTH):\n        input_sequence[j][X_sequence_ix[j]] = 1.\n    X[i] = input_sequence\n\n    y_sequence = data[i*SEQ_LENGTH+1:(i+1)*SEQ_LENGTH+1]\n    y_sequence_ix = [char_to_ix[value] for value in y_sequence]\n    target_sequence = np.zeros((SEQ_LENGTH, VOCAB_SIZE))\n    for j in range(SEQ_LENGTH):\n        target_sequence[j][y_sequence_ix[j]] = 1.\n    y[i] = target_sequence\n    \nprint('AUTHOR: {}'.format(AUTHOR))\nprint('SEQUENCE LENGTH: {}'.format(SEQ_LENGTH))\nprint('NUM SEQUENCES: {}'.format(NUM_SEQUENCES))\nprint('VOCAB SIZE: {}'.format(VOCAB_SIZE))"},{"metadata":{"_uuid":"d28bfa5c92f0c1539e636d3e88b44d45ad5f59ca","_cell_guid":"12b664ed-ec7b-4c96-8428-143f42b0cc64"},"cell_type":"markdown","source":"[](http://)## What does our dataset look like now that it's ready for training?"},{"metadata":{"_uuid":"5f6f8b07ee90c2f11549ae4c971c2839fbb37eb0","_cell_guid":"d14415f8-6627-4b78-a254-7cd35fcf28c3"},"cell_type":"code","execution_count":null,"outputs":[],"source":"print('X: type-{}, shape-{}'.format(type(X), X.shape))\nprint('y: type-{}, shape-{}'.format(type(y), y.shape))"},{"metadata":{"_uuid":"3d6fb8e6b7762ccc09df54e00747ddc70c281219","_cell_guid":"ed46eec9-ad6a-413d-b121-47da83a7da71"},"cell_type":"markdown","source":"## What does our dataset look like now that it's ready for training? (continued)\n<p> There are 3 dimensions: (num_sequences, seq_length, num_unique_char)  \nIn the first 5000 rows of this dataset, the text with author Edgar Allen Poe has 55,474 characters.  \nWe chose to look at sequences of length 150, so our num_sequences = 55,474//150 = 369  \nnum_unique_char is the number of unique characters in our dicitonary  \n**X and y are are are collection of 1908 arrays of shape (150, 70), where each array represents a 150 character sequence.  \nEach row is a character, and the columns are binary flags representing the index of that character in our dicitonary.**\n</p>"},{"metadata":{"_uuid":"8db4043a188c5d51ea779c7a5c22a8d14756126c","_cell_guid":"78894da3-cfc5-41a5-94e4-4c6d2a32df6c"},"cell_type":"markdown","source":"## Here's a better view of one of what X and y are"},{"metadata":{"_uuid":"148655a2b0e821fed09304d9a57a05b162d46611","_cell_guid":"ccefaec4-5688-485a-b67d-cad95684fdb8"},"cell_type":"code","execution_count":null,"outputs":[],"source":"print(\"X shape - \", X[0,:,:].shape, 'getting col sums...')\nprint(X[0,:,:].sum(0), '\\ngetting row sums...\\n', X[0,:,:].sum(1))\n\nprint(\"y shape - \", y[0,:,:].shape, 'getting col sums...')\nprint(y[0,:,:].sum(0), '\\ngetting row sums...\\n', y[0,:,:].sum(1))"},{"metadata":{"_uuid":"8436c516380aa3c97492b24cdbeeeef274b39071","_cell_guid":"3b35ec4e-e0f6-40d1-832c-a93fc2ce4966"},"cell_type":"markdown","source":"## Here's a better view of one of what X and y are (continued)\n<p>\nIf you look through the two arrays carefully, you will see one index in X decreases by 1, and one index in y increases by 1.\nThis is because the target sequence (the y sequence) is just the X sequence shifted forward by 1 character.  \nBy setting up our data in this way, we train the model to predict the next character in the sequence.  \nThis is visualized below:\n</p>"},{"metadata":{"_uuid":"8f2d3e3936cdfe25e144dbf7efb57ec984cfe571","_kg_hide-input":true,"_cell_guid":"49d694aa-08c2-4af7-b93c-ab0a1ec2574d"},"cell_type":"code","execution_count":null,"outputs":[],"source":"X_seq = []\nfor row in X[0,:,:]: # iterate over the 150 rows in this sequence\n    ix = np.argmax(row) # gets the index of the max value in the row (will be 1 and the rest 0)\n    char = ix_to_char[ix] # looks in our dictionary to switch from index to character\n    X_seq.append(char) # appends the character to our list\nX_seq = ''.join(X_seq) # turns our list into a string\n\ny_seq = []\nfor row in y[0,:,:]: # iterate over the 150 rows in this sequence\n    ix = np.argmax(row) # gets the index of the max value in the row (will be 1 and the rest 0)\n    char = ix_to_char[ix] # looks in our dictionary to switch from index to character\n    y_seq.append(char) # appends the character to our list\ny_seq = ''.join(y_seq) # turns our list into a string\nprint('X_seq: %s'%X_seq)\nprint('\\ny_seq: %s'%y_seq)\nprint('\\nCapital T appears 1 time in X and 0 times in y')\nprint('Capital I appears 1 time in X and 2 times in y')"},{"metadata":{"_uuid":"6281fee6e2ce9f616212b3db516c443de06b11d7","_cell_guid":"faeb4e2c-92c4-4e8f-bcc3-a91f6649258d"},"cell_type":"markdown","source":"## Model Summary"},{"metadata":{"_uuid":"53b9484685c6fd103a2f5994ff546e02c382bede","_cell_guid":"531ff93e-ace5-4fa4-859f-a0f815ff3acb"},"cell_type":"code","execution_count":null,"outputs":[],"source":"model = get_model(learning_rate = .003)\nmodel.summary()"},{"metadata":{"_uuid":"93d9a5ed12d534ece3497d2c23965e9af1a8f420","_cell_guid":"1d27728b-e8b1-43bb-8355-de51683674ff"},"cell_type":"markdown","source":"## Training\n<p> see the code for train_model at the top of this notebook if you'd like to edit it.  It will print results and save model weights every 2 epochs.  If you stop the kernel while it's training, you can run it again and it will run additional epochs from where the model left off.\n</p>"},{"metadata":{"_uuid":"350e57667c2c9a68e906818ba824666fc2e7c574","_cell_guid":"6c0ec8cf-c402-45e1-999a-b0444ab0bd34"},"cell_type":"code","execution_count":null,"outputs":[],"source":"print(\"Building Edgar Allen Poe model, taking into account previous {} characters...\".format(SEQ_LENGTH))\ntrain_model(model, X, y, 30, 5) # train 30 epochs, will print/save every 5 epochs"},{"metadata":{"_uuid":"8ce275d5806f1efe6ad9b5fe4b103195390f1545","_cell_guid":"a0bc1fe2-e0b9-46ce-824b-214698af9961","collapsed":true},"cell_type":"code","execution_count":null,"outputs":[],"source":"generate_text(model, 150) # Generate random text of length 150"},{"metadata":{"_uuid":"71d87fb7cbd05b8d3318dc96f9a1182f2fe771e1","_cell_guid":"6d2b4ff1-5454-47a4-8b5c-32ec3523a0a5","collapsed":true},"cell_type":"code","execution_count":null,"outputs":[],"source":"generate_text(model, 150) # Generate random text of length 150"},{"metadata":{"_uuid":"1061d5b4211a91dd705ceb74d949c936126000d7","_cell_guid":"0fd091dd-3ad6-4038-80a2-67c8446ea63d"},"cell_type":"markdown","source":"To get 'good' results, you should probably train on the full dataset, lower the learning_rate, and run a higher number of epochs & hidden nodes...  this may take several hours/days even on a GPU."}]}