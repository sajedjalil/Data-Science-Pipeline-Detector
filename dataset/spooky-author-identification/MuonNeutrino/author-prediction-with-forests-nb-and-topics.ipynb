{"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Introduction\n\nIn this competition, we are asked to analyze a series of texts and identify them as being written by Mary Shelley, HP Lovecraft, or Edgar Allen Poe. The text snippets are fairly short, so we have to make predictions based on a small number of features per example.\n\nThe data set is also quite small, so overfitting is likely to be a problem for many methods.\n\nFirst, we should import some basic libraries that will be useful. While scikit-learn has some NLP classes that I will use later, I will first start out with NLTK and gensim, which are dedicated NLP packages.","metadata":{"_cell_guid":"23feb6bb-25cb-4257-b042-4d870f1cddd1","_uuid":"583e1db285c3e2e9f75554ba50dbae635b2df036"}},{"cell_type":"code","source":"import nltk\nimport gensim\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport re\n\n%matplotlib inline","outputs":[],"metadata":{"_cell_guid":"cced5882-610f-4df0-914e-6fe8753172bd","_uuid":"0c5898556293ced284fd1dbcf043160900aeb165","collapsed":true},"execution_count":5},{"cell_type":"markdown","source":"Now, we can read in the files. They are just CSV files, so Pandas is fine. I am also intentionally not setting the ID column so that I can can just use the array index with brackets on a Series. This way I can treat Series objects the same way that I would treat lists, which might simplify some things.","metadata":{"_cell_guid":"c84805ae-293e-4c3a-a89c-8684c47e8730","_uuid":"fbb2e48d14a1b6e068336753614ed94da8796660"}},{"cell_type":"code","source":"train = pd.read_csv('../input/train.csv')\ntest = pd.read_csv('../input/test.csv')","outputs":[],"metadata":{"_cell_guid":"d5a496cd-c233-4572-bdea-2d45b5cb73f0","_uuid":"066280d3898e2b08fca76784877a6e2c97b5438a","collapsed":true},"execution_count":6},{"cell_type":"code","source":"train.head()","outputs":[],"metadata":{"_cell_guid":"0da81c62-5ec3-4dc7-bbd9-b5241bfb4842","_uuid":"11b93ee0bb67a085c542edc8607ed22776dc2615"},"execution_count":7},{"cell_type":"code","source":"print('Train length: '+str(len(train['text'])))\nprint('Test length: '+str(len(test['text'])))\nprint('Target Distribution: \\n'+str(train['author'].value_counts()))","outputs":[],"metadata":{"_cell_guid":"002640f8-b8ef-4af0-be04-f3417cac60b1","_uuid":"456752cefc7413ce3eaf00b5bd8efc50c6951ea6"},"execution_count":8},{"cell_type":"markdown","source":"We can see that about 40% of all entries are from Poe. The data is somewhat imbalanced, so for things like decision trees, we might want to set sample weights to even things out.\n\n# Data Munging\n\nNow, we want to somehow map these texts onto the same space so that we can try to make predictions. First, we'll transform the targets into a numerical encoding.","metadata":{"_cell_guid":"dcf0a9da-21b5-4f2e-86e8-59b50fb483b5","_uuid":"2dc2dd80a3463a783a5831bb63ea91bc83371a37"}},{"cell_type":"code","source":"train_text = train['text']\ntest_text = test['text']\nauth_map = {0:'EAP',1:'HPL',2:'MWS'}\nauth_map_inv = {'EAP':0,'HPL':1,'MWS':2}\ntrain_tgt = train['author'].map(auth_map_inv)","outputs":[],"metadata":{"_cell_guid":"d9c9e809-fb4e-4bf3-af20-3f20a378a246","_uuid":"d5ae42551be7a19a7e6773e8e7ca51906c859d73","collapsed":true},"execution_count":9},{"cell_type":"markdown","source":"### Basic Text Cleaning\n\nThe text is from English-language authors, so we can easily just set everything to lowercase and remove punctuation. We will replace all non-alphabetic characters to spaces here.","metadata":{"_cell_guid":"dac92c61-e187-481a-a4fe-316ddd81be64","_uuid":"7af247eefbfdd4ff8312909e7f2f4c529e8a6b38"}},{"cell_type":"code","source":"train_text = [re.sub(r'[^a-z]',' ',text.lower()) for text in train_text]\ntest_text = [re.sub(r'[^a-z]',' ',text.lower()) for text in test_text]\n","outputs":[],"metadata":{"_cell_guid":"7928df9c-ca9c-460c-a8d1-d8646c5665c2","_uuid":"b46921608fb9b1c893a6dfb08d4eb2d7a85f0a56","collapsed":true},"execution_count":10},{"cell_type":"markdown","source":"### Splitting Into Words\n\nNow we should split the text into individual words for further analysis.","metadata":{"_cell_guid":"51385be5-ef45-4e5b-bf03-b63933fd7a96","_uuid":"79e8bfeda0891dd07fe8c793b2f963bf053e3bd1"}},{"cell_type":"code","source":"all_text = train_text + test_text\nall_text = [text.split() for text in all_text]","outputs":[],"metadata":{"_cell_guid":"f1b2ce55-237a-4e4d-8505-cfb9d44842aa","_uuid":"ed0fdfa01f26720cb48b10de782025f5f400c1f7","collapsed":true},"execution_count":11},{"cell_type":"markdown","source":"### Removing Stop Words\n\nStop words are very common words that hold little information. Words like \"a,\" \"the,\" \"I,\", etc. typically don't tell us much about the text, so we can typically remove them to help improve our models. This is perhaps a bit risky here. For example, if one of the authors prefers to use first-person narratives often, removing words like \"I,\" \"me,\" and \"my\" might hide this. I won't worry about this for now.\n\nFor stop words, I will use the standard list from NLTK.","metadata":{"_cell_guid":"9e35c7ca-684c-4840-b4dd-809147d2c610","_uuid":"806700e69d4f4a864155823284ce1177918ff632"}},{"cell_type":"code","source":"stops = nltk.corpus.stopwords.words('english')\nall_text = [ [ word for word in text if word not in stops] for text in all_text ]","outputs":[],"metadata":{"_cell_guid":"1aa92618-a225-4e5d-8614-576c20c7907f","_uuid":"7f2c42b31bbfbe29d6240922c7f1ffe193aa631a","collapsed":true},"execution_count":12},{"cell_type":"markdown","source":"### Stemming\n\nMany words will appear in a number of forms, such as singular and plural nouns, adjectives and adverbs, etc. We can use stemming to try to remove the word endings so that only the \"stem\" - the part that holds the meaning of the word as opposed to its part of speech. Again, this could actually hurt us if one of the author often uses particular forms of certain words.\n\nThe Porter stemmer is one example of a fairly common stemmer, so I'll use the NLTK implementation.","metadata":{"_cell_guid":"92aeeceb-6179-4476-b21b-e03b679d2174","_uuid":"3335bd158871f457dfcf65c8a02a21a11206f2b2"}},{"cell_type":"code","source":"porter = nltk.stem.porter.PorterStemmer()\nall_text = [ [ porter.stem(word) for word in text] for text in all_text]","outputs":[],"metadata":{"_cell_guid":"86901536-e134-4944-8061-b9a3866845fc","_uuid":"ae63b053e00e6605d7f172498bd580e8e3dd5465","collapsed":true},"execution_count":13},{"cell_type":"markdown","source":"### Removing Very Uncommon Words\n\nAfter this, I am close to my final set of words to use. The last thing I'll do is remove very uncommon words. I'm using methods based on a bag-of-words model, so words that only appear a couple times are probably not going to be that useful. It could be useful to retain these words for methods that are sensitive to word context.\n\nI'll just set a minimum frequency of 3. It would actually probably be better to set a minimum document frequency here, since a single rare word could appear multiple times in a single entry.","metadata":{"_cell_guid":"3b9f41c4-ccd3-4fee-9bef-4a95acfa7a27","_uuid":"96fc22d88599c99d7f65ae0a0f376ddf1d3e61f5"}},{"cell_type":"code","source":"min_num = 3\n\nfrom collections import Counter\n\ncounts = Counter()\nfor text in all_text:\n    for word in text:\n        counts[word] += 1\nto_remove = []\nfor word in reversed(counts.most_common()):\n    if counts[word[0]]<= min_num:\n        to_remove.append(word[0])\n    else:\n        break\nprint('Total number of words: '+str(len(counts)))\nprint('Number of words to remove: '+str(len(to_remove)))\n#print('Words to remove: \\n' + str(to_remove))\nall_text = [ [word for word in text if word not in to_remove] for text in all_text]","outputs":[],"metadata":{"_cell_guid":"0c11b877-8447-46e3-b7c9-fb7df3cecab4","_uuid":"760d07e16695fe9b99b713553f4ee38b22d141f5"},"execution_count":14},{"cell_type":"markdown","source":"We see that in this small data set, about half of all words appear only very rarely. We will be left with a set of around 8000 words to use. We could expand this by using larger n-grams.\n\n## How Many Words Are in Each Entry?\n\nNow that we've cleaned some data, let's see how many words are left in each entry.","metadata":{"_cell_guid":"9da9f73e-395e-4d05-ac0f-1e43244c55b2","_uuid":"7672ef0904047eed06f5e9f5361132d55f4a1ecc"}},{"cell_type":"code","source":"lens = [len(text) for text in all_text]\nplt.hist(lens,bins=100,range=(0,100))\nplt.xlabel('Number of Words After Cleaning')\nplt.ylabel('Number of Documents')\nplt.show()","outputs":[],"metadata":{"_cell_guid":"a35757c5-565c-4b00-a31f-233ed7717371","_uuid":"706769c1df92f2a396fdf932c8f6923912486edb"},"execution_count":15},{"cell_type":"markdown","source":"Most of the entries are in the 10-20 word range but there is a long tail. Some documents even have more than 60 words. At any rate, the typical entry has only a small number of features for us to use.\n\nThe last thing that I will do is remove very small entries. I will bring these back when I start training classifiers, but for now, I will remove entries with only one or two remaining words.","metadata":{"_cell_guid":"894a35ad-5bd2-4ee1-b3bf-fd522ba6e010","_uuid":"112723b94b83bbc061a1132df1bfe37df3bef21c"}},{"cell_type":"code","source":"minlen = 3\nn_to_remove = 0\nfor text in all_text:\n    if len(text)<minlen:\n        n_to_remove+=1\n        \nprint('Number of texts smaller than minimum: '+str(n_to_remove) + ' of '+str(len(all_text)))","outputs":[],"metadata":{"_cell_guid":"02e66c96-d5ba-4540-9c0a-5817c2240b51","_uuid":"cdb150a179a57fb5800d28b04c9ae90d15451b80"},"execution_count":16},{"cell_type":"code","source":"all_text = [text for text in all_text if len(text)>=minlen]","outputs":[],"metadata":{"_cell_guid":"b3b017b2-f29c-4b2e-adfc-dab97d40c519","_uuid":"a4794c9eba6a0db39b946bd2da03c64c5c6b1348","collapsed":true},"execution_count":17},{"cell_type":"markdown","source":"Only a very small fraction of entries was removed there.\n\n# Representing Entries as Vectors\n\nNow that we have a cleaned data set, we can try to map the text onto a vector space with a fixed number of dimensions. This can be done in a number of packages, but I'll use gensim here.\n\n### Bag of Words Model\n\nFirst, we can create a dictionary from our text. This allows us to make something like a one-hot encoding of the 8000 or so unique words in our data. This is will give us a bag-of-words model with each document being represented by a sparse vector of term frequencies.","metadata":{"_cell_guid":"2d3e8b0d-a968-42fa-b7b9-45131ddb1c9b","_uuid":"f158174f75d6c4b21a8bd450ab518d1553692a50"}},{"cell_type":"code","source":"dictionary = gensim.corpora.Dictionary(all_text)\nword_vec = [ dictionary.doc2bow(text) for text in all_text ]","outputs":[],"metadata":{"_cell_guid":"a6d04146-fb46-4334-b330-3bf3b4386f30","_uuid":"ea5b85f73bd519ed0e2669801d9e9e4863891b22","collapsed":true},"execution_count":18},{"cell_type":"markdown","source":"### TF-IDF Weighting Scheme\n\nSome of these words are present in many of our documents. These typically have less meaning (or less ability to distinguish entries), so we can generate tf-idf weights. There are various ways to define tf-idf, but they all involve the term frequency (tf) for each document and the inverse of the document frequency (i.e. how many documents the word appears in).","metadata":{"_cell_guid":"f749e4f1-bc67-441d-9adb-b6759bd08ba6","_uuid":"3dbe5a0d429a3f050e759d4991882d452ac12e5a"}},{"cell_type":"code","source":"tfidf = gensim.models.TfidfModel(word_vec)\ntfidf_vec = [tfidf[vec] for vec in word_vec]\n","outputs":[],"metadata":{"_cell_guid":"693735b2-6ed3-4f30-9f96-cc705f10eaa5","_uuid":"4a84e55b8d7fe74037a7ef010508a14ec4406e57","collapsed":true},"execution_count":19},{"cell_type":"markdown","source":"### Vector Encoding/Dimensionality Reduction\n\nOne issue with a bag-of-words model, especially when applied to short pieces of text, is that any two entries will have little to no overlap. We can instead try to use the fact that certain words will often appear with similar other words to create smaller-dimensional vectors representing common \"topics.\"\n\nTwo common ways to do this are Latent Semantic Analysis (LSA/LSI) and Latent Dirichlet Allocation (LDA). LSA is just doing a singular value decomposition and then usually taking only a subset of entries in the diagonal part. This is more or less the equivalent of PCA for a non-square matrix. I don't know too much about LDA, but it uses a probabilistic model to learn features.","metadata":{"_cell_guid":"e130d676-dde7-4729-9264-cf3cdf810dd8","_uuid":"26d4fbcbc48b047a24e137faedc94f382ecae5c8"}},{"cell_type":"code","source":"lsi = gensim.models.lsimodel.LsiModel(corpus=tfidf_vec,\n                                      id2word=dictionary, \n                                      num_topics=50)\n\nlda = gensim.models.ldamodel.LdaModel(corpus=tfidf_vec,\n                                      id2word=dictionary,\n                                      num_topics=50,passes=10)\n#                                      update_every=1,\n#                                      chunksize=5000,\n#                                      passes=10)","outputs":[],"metadata":{"_cell_guid":"ddbbd77a-82a7-4360-8253-d97027496399","_uuid":"360c0efd466e26dc1a0e5572737988743d2159c9","collapsed":true},"execution_count":20},{"cell_type":"code","source":"print('LSA Results')\nlsi.print_topics(50)\n","outputs":[],"metadata":{"_cell_guid":"e2d78219-72c9-46f8-b738-023742eb3e65","_uuid":"6cb6fb9c28ce06cf08121ca169b92bf52fa7da72"},"execution_count":21},{"cell_type":"code","source":"print('LDA Results')\nlda.print_topics(50)","outputs":[],"metadata":{"_cell_guid":"36aee99a-04f8-419d-9e6e-d889c66fe1d8","_uuid":"238e49f9a05871f2b0a5e4a1d5e31bc9a11d06b0"},"execution_count":22},{"cell_type":"markdown","source":"The summary of LSA topics looks quite confusing. For the LDA topics, we seem to see that many have a few similar words plus an assortment of seemingly random terms.\n\n# Preparing the Training and Test Sets\n\nThe work we did previously is all data munging and unsupervised training on the full (training and test) data set. Now, we should repeat our work separately for the training and test sets. We could just split the combined array back into two, but it's nicer to package our work into a single short function.","metadata":{"_cell_guid":"447d4787-ba20-41f3-add6-0f9e3ef48c32","_uuid":"5497d241883ef174002b68dcc14d334ea2faa8e6"}},{"cell_type":"code","source":"# mod gives the mapping from tfidf to the 50-feature space\ndef prepare_text(data,mod):\n    data = [re.sub(r'[^a-z]',' ',text.lower()) for text in data]\n    data = [text.split() for text in data]\n    data = [ [ word for word in text if word not in stops] for text in data ]\n    data = [ [ porter.stem(word) for word in text] for text in data] \n    data = [dictionary.doc2bow(text) for text in data]\n    data = [tfidf[text] for text in data ]\n    data = [mod[text] for text in data]\n\n    data_arr = []\n    for text in data:\n        data_arr.append([0.]*50)\n        for word in text:\n            data_arr[-1][word[0]] = word[1]\n\n    return np.array(data_arr)","outputs":[],"metadata":{"_cell_guid":"9278fbde-aa62-4e14-a9d0-70e71848f1bf","_uuid":"a43771e31764a93cc9f18b7fbbef69c3504d0bcd","collapsed":true},"execution_count":23},{"cell_type":"markdown","source":"Now, we can run on the training and test sets using the LSA model.","metadata":{"_cell_guid":"9d33331d-2a1d-4211-a883-4a6d4fd420d3","_uuid":"0a11e6d455ed90b633a3764c7ce927c75c52c5e9"}},{"cell_type":"code","source":"train_prep = prepare_text(train_text,lsi)\ntest_prep = prepare_text(test_text,lsi)","outputs":[],"metadata":{"_cell_guid":"329cbae9-7add-460d-95a4-d09770afbcff","_uuid":"b97c4903b3881d425b73a2a1cc32fef50666537b","collapsed":true},"execution_count":24},{"cell_type":"markdown","source":"# Running a Random Forest Classifier\n\nA Random Forest is a fairly straightforward type of classifier. We use the 50 features of our encoding as inputs and try to predict which of the three authors wrote which text.","metadata":{"_cell_guid":"fa4cede4-2b3b-4d9d-8946-a41cd1b6b882","_uuid":"4f8767ed5219c811e2e5336c1b7f99c78a679216"}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import cross_validate\nimport sklearn.metrics as metrics\n\n\n# Get class weights: The data set is imbalanced\n\nweights = np.max(train_tgt.value_counts())/train_tgt.value_counts()\nprint(weights)\n\nc_weight = {}\nfor i in range(len(weights)):\n    c_weight[weights.index[i]] = weights[i]\n    \n# This is just the same as scoring='neg_log_loss'\nlogloss_score = metrics.make_scorer(metrics.log_loss,needs_proba=True,greater_is_better=False)\n\n# Random forest with some random params\nrfc = RandomForestClassifier(max_depth=10,min_samples_leaf=10,n_estimators=50,\n                             class_weight=c_weight)\n\n# Cross validate\nscores = cross_validate(rfc,train_prep,np.array(train_tgt),cv=5,\n                        scoring=logloss_score)\nscores","outputs":[],"metadata":{"_cell_guid":"2ff9e643-e7a2-49ba-b6fc-205111702c11","_uuid":"178b8e15602e1ee84c6ab415479bf30fb034b2b6"},"execution_count":25},{"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\n\npars = {'max_depth':[5,10,15],'min_samples_leaf':[5,10,15],'n_estimators':[50]}\ngcv = GridSearchCV(rfc,param_grid=pars,scoring=logloss_score,cv=5)\ngcv.fit(train_prep,np.array(train_tgt))\n","outputs":[],"metadata":{"_cell_guid":"3fc94b12-b211-4c61-8b35-ba649b7ffe7a","_uuid":"b9bf611daeeb2820f4de9e2c377da0806367b57b"},"execution_count":26},{"cell_type":"code","source":"print(gcv.best_params_)\nprint(gcv.cv_results_['mean_test_score'])\nprint(gcv.cv_results_['mean_train_score'])","outputs":[],"metadata":{"_cell_guid":"e6840b3e-30d6-4ee8-8c66-b316a57be207","_uuid":"f4801f76bc7107b7b632ca5bc86ed99517b4254a"},"execution_count":27},{"cell_type":"markdown","source":"We see that, at least without heavily tuning the different parameters of the model, we can do better than just guessing Poe for everything, but we still only get around 60% correct if we look at the equivalent accuracy scores. Furthermore, this model is quite slow. \n\nWhy might this approach not work too well? First, as I already said, this analysis is not optimized very well. Perhaps there is some combination of parameters that works much better. It might take a long time to find those parameters if they do exist. We could also try other methods such as neural nets or boosted decision trees, but we'll have to be very careful about overtraining. More importantly, we're trying to cluster short pieces of text into individual topics. However, these authors all wrote works with similar themes in English in the 1800s and (for Lovecraft) early 1900s. It may simply be that many of the topics that we've found appear in the works of all three. It also may be that the texts are simply too short to get a good encoding of the topic. So, are there any better ways to analyze our data?\n\n# Naive Bayes Classification\n\nA naive Bayes classifier might do much better here. It will certainly do a better job at catching words that are particularly favored by one of the authors. It can also try to find the relative weights of the different authors even on the unlabeled test set.\n\n### Document Term Frequency Matrix\n\nFor this set, we will want to run a multinomial naive Bayes classifier on the term frequency matrix. The CountVectorizer in Scikit-learn is an easy way to generate a Scikit-learn-friendly term frequency matrix. We can also choose the range of n-grams to use and the minimum document frequency. I'll just set some numbers here. I'm still using the text after removing stop words and running the stemming algorithm, so it's likely that things like 3-grams are usually unique to a single document. It's quite likely that the data munging can be better tuned to this model.","metadata":{"_cell_guid":"b4f0e34c-52bb-47e1-ab14-141cfc3361f4","_uuid":"6d1f41b57ca648b54493e03b279a1a65fa21b3a5"}},{"cell_type":"code","source":"from sklearn.naive_bayes import MultinomialNB\nfrom sklearn.feature_extraction.text import CountVectorizer\nall_text_joined = [ ' '.join(text) for text in all_text]\n\ncvec = CountVectorizer(ngram_range=(1,3),min_df=2)\ncvec.fit(all_text_joined)","outputs":[],"metadata":{"_cell_guid":"dea3918f-798d-4208-9174-ca0e71edd1f6","_uuid":"4ca33c015b12b342935b447226b4124dbafe5d38"},"execution_count":28},{"cell_type":"markdown","source":"### Preparing the Training and Test Sets\n\nAs before, I'll package all the work into a short script to process the separate test and training sets.","metadata":{"_cell_guid":"795e85b4-f3e8-4e34-b6e4-d58c47456fba","_uuid":"e67e4db84b07b6e7e27fabb5f675528e4feba63e"}},{"cell_type":"code","source":"\n\ndef prepare_text_counts(data):\n    data = [re.sub(r'[^a-z]',' ',text.lower()) for text in data]\n    data = [text.split() for text in data]\n    data = [ [ word for word in text if word not in stops] for text in data ]\n    data = [ [ porter.stem(word) for word in text] for text in data] \n    data = [ ' '.join(text) for text in data]\n    data = cvec.transform(data)\n    return data\n    \n    ","outputs":[],"metadata":{"_cell_guid":"fd503aa4-81ac-4f82-8b07-33582593aa04","_uuid":"0e5aff7ac29b84b9f064a143cb84377eec67733b","collapsed":true},"execution_count":29},{"cell_type":"code","source":"train_text = train['text']\ntest_text = test['text']\ntrain_prep = prepare_text_counts(train_text)\ntest_prep = prepare_text_counts(test_text)","outputs":[],"metadata":{"_cell_guid":"55600f3f-931c-44f6-81e3-35deb92e85cc","_uuid":"7bbd36b1502db3d623ff9a7e24ddfca46b50849d","collapsed":true},"execution_count":30},{"cell_type":"markdown","source":"### Running the Multinomial NB Model\n\nThere aren't too many options for the Scikit-learn naive Bayes model. I will optimize the smoothing parameter alpha, and use the default options for the others. Thus, this model will try to fit for the prior probabilities for the different authors (i.e. the relative normalizations).","metadata":{"_cell_guid":"c8cbd70f-8f45-4261-9aa2-45058dfe1206","_uuid":"5283211f53b849772ff2dfd409d96b7d48ecfd37"}},{"cell_type":"code","source":"from sklearn.model_selection import validation_curve\n\nmnb = MultinomialNB(alpha=1)\nvals = np.logspace(-1,2,50)\ntrain_scores,valid_scores = validation_curve(mnb,train_prep,\n                                             np.array(train_tgt),'alpha',\n                                             vals,cv=10,scoring=logloss_score)\n\ntrain_mean = -np.mean(train_scores,axis=1)\nvalid_mean = -np.mean(valid_scores,axis=1)\ntrain_std = np.std(train_scores,axis=1)\nvalid_std = np.std(valid_scores,axis=1)\n\nfig = plt.figure(1,(8,8))\nplt.plot(vals,train_mean,color='b',label='Training Scores')\nplt.fill_between(vals,train_mean-train_std,train_mean+train_std,facecolor='b',alpha=0.3)\n\nplt.plot(vals,valid_mean,color='r',label='Validation Scores')\nplt.fill_between(vals,valid_mean-valid_std,valid_mean+valid_std,facecolor='r',alpha=0.3)\n\nplt.legend()\nplt.ylabel('Log-Loss Score')\nplt.xlabel('Alpha')\nplt.xscale('log')\nplt.title('Validation Curves for Multinomial Naive Bayes')\nplt.show()","outputs":[],"metadata":{"_cell_guid":"670c2996-7201-48c9-bde9-899dfb1be4f6","_uuid":"e5c6283bd7b244647a366ad2423a947be8058a51"},"execution_count":31},{"cell_type":"markdown","source":"We immediately see that the NB model is doing much better than the Random Forest even without doing any tuning. It's possible that a Random Forest using the TF matrix or TF-IDF matrix might outperform the NB model. I've tried doing some tests with a normalized TF matrix using a Random Forest but did not get any promising results for the different parameters that I tried. The NB also runs much faster. We get an optimum alpha of around 2 here, with a log-loss score of around 0.45. This is roughly an accuracy of 80%, as opposed to around 60% for our initial Random Forest model.","metadata":{"_cell_guid":"3ee6f23c-9eca-4b95-803f-169073cc3d88","_uuid":"b8f12eb0bef8fecc0c2399356393494a8ed35274"}}],"nbformat_minor":1,"metadata":{"kernelspec":{"name":"python3","language":"python","display_name":"Python 3"},"language_info":{"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","version":"3.6.3"}}}