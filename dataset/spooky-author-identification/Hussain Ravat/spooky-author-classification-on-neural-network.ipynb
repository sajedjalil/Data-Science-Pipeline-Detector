{"cells":[{"source":"#####while this model scores low on accuracy it is still a working version\n###############of nueral net using tensorfl\n##import necessary packages\n#####\nimport tensorflow as tf \nimport numpy as np # linear algebra\nimport scipy as sp \nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib\nimport seaborn as sns \nimport nltk\nfrom nltk.corpus import stopwords\nimport wordcloud\nfrom wordcloud import WordCloud, STOPWORDS\nfrom matplotlib import pyplot as plt\n##read in the data file an display \ndf_spooky_author=pd.read_csv('../input/train.csv')\ndf_spooky_author\n\nsentence_list=df_spooky_author['text'].values.tolist()\nauthor_list=df_spooky_author['author'].values.tolist()\nid_list=df_spooky_author['id'].values.tolist()\nsentence_list\nauthor_list\n\n\ncombined_list=[list(author_list) for author_list in zip(author_list, sentence_list)]\n\ntokenized_list=[]\n\nfor authors,sentence in combined_list:\n    tokenized_list.append([authors,nltk.word_tokenize(sentence)])\n\ntokenized_list\n\n##make individual authorwise list of words after removing stop words.\n##This is to prepare the data for wordcloud\nstop = set(stopwords.words('english'))    \n\ntokenized_stop_words_list=[]\n\n\nfor author,sentence in tokenized_list:\n    tokenized_stop_words_list_temp=[]   \n    for word in sentence:\n        if not word in stop:\n            tokenized_stop_words_list_temp.append(word)\n    tokenized_stop_words_list.append([author,tokenized_stop_words_list_temp])        \n            \ntokenized_stop_words_list\n\ntokenized_words_EAP=[]\ntokenized_words_MWS=[]\ntokenized_words_HPL=[]\n\nfor author,sentence in tokenized_stop_words_list:\n    if author=='EAP':\n        for words in sentence:\n            tokenized_words_EAP.append(words)\n    if author=='MWS':\n        for words in sentence:\n            tokenized_words_MWS.append(words)\n    if author=='HPL':\n        for words in sentence:\n            tokenized_words_HPL.append(words)\n            \n##word cloud for HPLovenCraft            \nplt.figure(figsize=(30,30))\nwc = WordCloud(background_color=\"black\", max_words=10000, \n               stopwords=stop, max_font_size= 40)\nwc.generate(\" \".join(tokenized_words_HPL))\n##plt.title(\"HP Lovecraft (Cthulhu-Squidy)\", fontsize=16)\n##Uncomment below line and run to see wordcloud for HP Lovencraft\n##plt.imshow(wc.recolor( colormap= 'Pastel1_r' , random_state=17), alpha=0.98)\nplt.axis('off')\n\n\n\n##word cloud for Edgar Allen Poe         \nplt.figure(figsize=(30,30))\nwc = WordCloud(background_color=\"black\", max_words=10000, \n               stopwords=stop, max_font_size= 40)\nwc.generate(\" \".join(tokenized_words_EAP))\n##plt.title(\"HP Lovecraft (Cthulhu-Squidy)\", fontsize=16)\n##Uncomment below line and run to see wordcloud for Edgar Allen Poe\n##plt.imshow(wc.recolor( colormap= 'Pastel1_r' , random_state=17), alpha=0.98)\nplt.axis('off')\n\n\n\n##word cloud for Mary Shelley            \nplt.figure(figsize=(30,30))\nwc = WordCloud(background_color=\"black\", max_words=10000, \n               stopwords=stop, max_font_size= 40)\nwc.generate(\" \".join(tokenized_words_HPL))\n##plt.title(\"HP Lovecraft (Cthulhu-Squidy)\", fontsize=16)\n##Uncomment below line and run to see wordcloud for Mary Shelley \n##plt.imshow(wc.recolor( colormap= 'Pastel1_r' , random_state=17), alpha=0.98)\nplt.axis('off')\n\ndf_test=pd.DataFrame(tokenized_words_HPL)\ndf_test[0].value_counts()\n\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import preprocessing  \nfrom sklearn import linear_model\nfrom sklearn import metrics\nfrom sklearn.metrics import accuracy_score\n\n## TF-IDF implementation\ncount_vect = CountVectorizer()\n\nsentence_train_counts = count_vect.fit_transform(sentence_list)\nsentence_train_counts.shape\n\n\ntfidf_transformer = TfidfTransformer()\nsentence_train_tfidf = tfidf_transformer.fit_transform(sentence_train_counts)\nsentence_train_tfidf\n\nsentence_train_tfidf.shape\nlen(sentence_list)\n\n##Label Encoder to encode values\nle = preprocessing.LabelEncoder()\nle.fit(author_list)\n\nlist(le.classes_)\n\nauthor_list_encoded=le.transform(author_list)\nprint(author_list_encoded)\n\n\n####One Hot encoding\n#onehot_encoder = preprocessing.OneHotEncoder()\n#author_list_oh_encoded = onehot_encoder.fit_transform(author_list_encoded)\n\n#author_list\n#author_list_encoded\n#print(author_list_oh_encoded.size)\n\n########Setting up tensor flow\nlearning_rate = 0.05\nepochs = 3\nbatch_size = 2000\n\n# declare the training data placeholders\n# There are a total of 19579 examples \n# There are a total of 25068 inputs\nx = tf.placeholder(tf.float32, [None, 25068])\n\n# 3 digits for the 3 authors\ny = tf.placeholder(tf.float32, [None, 3])\n\n###declare number of nodes and layers\ninputs=25068\nhidden_layer_1_nodes=100\nhidden_layer_2_nodes=100\noutput_layer=3\n\n####declare the weights and biases for the nueral network\n\n# Weights and biases between input layer and hidden layer 1 \nW1 = tf.Variable(tf.random_normal([inputs,hidden_layer_1_nodes], stddev=0.03), name='W1')\nb1 = tf.Variable(tf.random_normal([hidden_layer_1_nodes]), name='b1')\n\n# Weights and biases between hidden layer 1 and hidden layer2\nW2 = tf.Variable(tf.random_normal([hidden_layer_1_nodes,hidden_layer_2_nodes], stddev=0.03), name='W2')\nb2 = tf.Variable(tf.random_normal([hidden_layer_2_nodes]), name='b2')\n\n# Weights and biases between hidden layer 2 and ourput layer\nW3 = tf.Variable(tf.random_normal([hidden_layer_2_nodes,output_layer], stddev=0.03), name='W3')\nb3 = tf.Variable(tf.random_normal([output_layer]), name='b3')\n\n\n####Calculate output of hidden layer 1\nhidden_out_1 = tf.add(tf.matmul(x, W1), b1)\nhidden_out_1 = tf.nn.sigmoid(hidden_out_1)\n\n####Calculate output of hidden layer 2\nhidden_out_2 = tf.add(tf.matmul(hidden_out_1, W2), b2)\nhidden_out_2 = tf.nn.sigmoid(hidden_out_2)\n\n####Calculate output of output layer\nfinal_output = tf.nn.softmax(tf.add(tf.matmul(hidden_out_2, W3), b3))\nfinal_output=tf.nn.sigmoid(final_output)\n\n\n#limit the output to 1e-10, 0.9999999 to prevent o from being retutned\nfinal_output_clipped = tf.clip_by_value(final_output, 1e-10, 0.9999999)\n\ncross_entropy = -tf.reduce_mean(tf.reduce_sum(y * tf.log(final_output_clipped)\n                         + (1 - y) * tf.log(1 - final_output_clipped), axis=1))\n\n\noptimiser = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cross_entropy)\n\n# varaibles initialzer\ninit_op = tf.global_variables_initializer()\n\n# define an accuracy assessment operation\ncorrect_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(final_output, 1))\naccuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n\nX_scale_train,X_scale_test,Y_scale_train,Y_scale_test = train_test_split(sentence_train_tfidf,author_list_encoded,test_size=0.3,random_state=78)\n\ny_onehot_labels_train = tf.one_hot(Y_scale_train, 3)\ny_onehot_labels_test = tf.one_hot(Y_scale_test, 3)\n\nX_scale_train_toarray=X_scale_train.toarray()\nX_scale_test_toarray=X_scale_test.toarray()\n\n\n#######prepare the preidction data\ndf_spooky_author_test=pd.read_csv('../input/test.csv')\ndf_spooky_author_test\n\nsentence_list_test=df_spooky_author_test['text'].values.tolist()\nid_list_test=df_spooky_author_test['id'].values.tolist()\n\n\n#####Very important use transorm here instread of fit transofrm\n#####Training data has been fitted above hence here we need to use only transform\n##### If fit transform is used we get dimensional mismatch while making predictions from the trained model\nsentence_test_count=count_vect.transform(sentence_list_test)\nsentence_test_count.shape\n\nsentence_test_tfidf = tfidf_transformer.transform(sentence_test_count)\nsentence_test_tfidf_to_array=sentence_test_tfidf.toarray()\n\nprediction=tf.argmax(y,1)\n\n\n\n# start the session\nwith tf.Session() as sess:\n    # initialise the variables\n    sess.run(init_op)\n    total_batch = 1 ##int(len(Y_scale_train) / batch_size)\n    \n    \n    for epoch in range(epochs):\n        print(\"Epoch:\", (epoch + 1))\n        avg_cost = 0\n        for i in range(total_batch):\n            print('Batch:-',i)\n            _, c = sess.run([optimiser, cross_entropy], \n                         feed_dict={x: X_scale_train_toarray, y: y_onehot_labels_train.eval()}) \n        avg_cost += c / total_batch\n        print(\"Epoch:\", (epoch + 1), \"cost =\", \"{:.3f}\".format(avg_cost))\n    print(sess.run(accuracy, feed_dict={x: X_scale_test_toarray , y: y_onehot_labels_test.eval()}))        \n    ##print(sess.run(y,prediction.eval(feed_dict={x: sentence_test_tfidf_to_array,y:})))\n    final_pred=sess.run(final_output, feed_dict={x: sentence_test_tfidf_to_array})   \n    \n    \n####Prepareoutput forsubmission\ndf_X_prob_predict=pd.DataFrame(final_pred)\ndf_X_prob_predict.columns=['EAP', 'HPL', 'MWS']\n\ndf_X_prob_predict\ndf_X_ID=pd.DataFrame(id_list_test)\ndf_X_ID.columns=['id']\ndf_predict_final=df_X_ID.join(df_X_prob_predict)\ndf_predict_final\n\n###Export Data to csv for final submission\ndf_predict_final.to_csv('hravat_spooky_autor_predict.csv',sep=',')\n\n\n","outputs":[],"cell_type":"code","metadata":{"_cell_guid":"22b5ecdf-73f3-45dd-b400-780b58b35d1b","_kg_hide-output":true,"_kg_hide-input":false,"scrolled":false,"_uuid":"056ff353273e0845a64b92eb2d2322134aa1852a"},"execution_count":3}],"nbformat":4,"metadata":{"language_info":{"nbconvert_exporter":"python","mimetype":"text/x-python","version":"3.6.3","name":"python","file_extension":".py","pygments_lexer":"ipython3","codemirror_mode":{"version":3,"name":"ipython"}},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat_minor":1}