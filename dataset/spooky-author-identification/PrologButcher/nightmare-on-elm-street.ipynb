{"nbformat_minor":1,"cells":[{"source":"import numpy as np\nimport pandas as pd\nfrom collections import Counter\nfrom random import shuffle, sample\nimport re\nimport itertools\nimport funcy\nimport keras\nimport keras.backend as k\nfrom keras.layers import Dense, GlobalAveragePooling1D, Embedding\nfrom keras.callbacks import EarlyStopping\nfrom keras.models import Sequential\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.utils import to_categorical\nfrom keras.wrappers.scikit_learn import KerasClassifier\nimport nltk\nimport nltk.data\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nfrom nltk.stem import WordNetLemmatizer\nfrom scipy.stats import mstats\nfrom sklearn.preprocessing import LabelBinarizer, MinMaxScaler, StandardScaler, PolynomialFeatures\nfrom sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\nfrom sklearn.decomposition import PCA\nfrom sklearn.pipeline import Pipeline\nfrom sklearn import metrics\nfrom sklearn.ensemble import RandomForestClassifier\nfrom gensim.models.doc2vec import Doc2Vec, TaggedDocument\nimport matplotlib\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\nimport seaborn as sns\n% matplotlib inline","metadata":{"_kg_hide-output":true,"_cell_guid":"693dc49d-434f-4b78-9bb5-8f1e22549db6","_uuid":"eaff806685d44401a789a796af0d53bb97e9b2c5","_kg_hide-input":true},"cell_type":"code","outputs":[],"execution_count":null},{"source":"#### In this notebook I am going to try out some feature extraction, model stacking, and word embedding solutions to the Spooky Author Identification competition.","cell_type":"markdown","metadata":{"_uuid":"4b5435d171602a88f77ad05daffb476d25351d20","_cell_guid":"a494e268-22b0-4d12-b0b5-b36e1d8fd04c"}},{"source":"### Load the data","cell_type":"markdown","metadata":{"_uuid":"3e7d301d422fcdae5acef9e45db87fc9df80edfe","_cell_guid":"11c39cf7-5527-46c3-9b97-7a28a3565d75"}},{"source":"train = pd.read_csv(\"../input/train.csv\")\ntest = pd.read_csv(\"../input/test.csv\")","metadata":{"collapsed":true,"_uuid":"d3354010db9ea67524538ef02ad4d073ad8c0140","_cell_guid":"6616e575-751d-4afd-a151-ac3bc3eacfbc"},"cell_type":"code","outputs":[],"execution_count":null},{"source":"print(train.shape, test.shape)","metadata":{"_uuid":"c3ffaea485ff3601153dfca903f18d034547ba77","_cell_guid":"3a50f71e-b11b-4917-b512-583ccbeb71bd"},"cell_type":"code","outputs":[],"execution_count":null},{"source":"### Now let's extract some features using some tools from NLTK","cell_type":"markdown","metadata":{"_uuid":"18f4a5681bcbf84b63a628357f0cb3b3f9c24a78","_cell_guid":"006d94e6-8d78-43b6-85e4-6dc24bcae27d"}},{"source":"def content_words(text, target_tags):\n    return [x[0] for x in nltk.pos_tag(text)\n            if x[-1] in target_tags]\n\ndef extract_features(df):\n    punct = re.compile('[\\\\.,\\\\?]')\n    df[\"char_count\"] = df[\"text\"].str.len()\n    df[\"word_count\"] = df[\"text\"].apply(lambda x: len(x.split()))\n    df[\"unique_chars\"] = df[\"text\"].apply(lambda x: len(set([ch for ch in x])))\n    df[\"av_word_len\"] = df[\"char_count\"] / df[\"word_count\"]\n    df[\"punct\"] = df[\"text\"].apply(lambda x: len(re.findall(punct, x)))\n    df[\"punct_per_w\"] = df[\"punct\"] / df[\"word_count\"]\n    df[\"content_words\"] = df[\"text\"].apply(lambda x: content_words(x.split(), ['NN', 'NNS', 'JJ', 'RB', \"VBG\", \"VBD\", \"VB\"]))\n    df[\"n_content_words\"] = df[\"content_words\"].apply(lambda x: len(x))\n    df[\"cw_per_w\"] = df[\"n_content_words\"] / df[\"word_count\"]\n    df[\"adjectives\"] = df[\"content_words\"].apply(lambda x: content_words(x, ['JJ', \"RB\"]))\n    df[\"n_adj\"] = df[\"adjectives\"].apply(lambda x: len(x))\n    df[\"adj_per_w\"] = df[\"n_adj\"] / df[\"word_count\"]\n    return df","metadata":{"collapsed":true,"_uuid":"9ad206b4472b59a9ebf445ff28c659c1e6203ed7","_cell_guid":"be084299-f8c4-4667-b2d0-0d5efea139e2"},"cell_type":"code","outputs":[],"execution_count":null},{"source":"df = train.copy()\ndf = extract_features(df)","metadata":{"collapsed":true,"_uuid":"c93c8c7a0d7ad091bc3adecf57b25eb066e01e93","_cell_guid":"4270da96-05ef-4f42-9f11-be2b9db77a86"},"cell_type":"code","outputs":[],"execution_count":null},{"source":"grouped = df.groupby(\"author\")\nfeatures = [\"word_count\", \"punct_per_w\", \"av_word_len\", \"char_count\",\n            \"n_content_words\", \"cw_per_w\", \"n_adj\", \"adj_per_w\", \"unique_chars\"]\n\nfor feature in features:\n    print(feature)\n    print(grouped[feature].agg(np.mean), \"\\n\")","metadata":{"_uuid":"3bbc8de838335c71eca63b6705189cd9ac40a04d","_cell_guid":"52fd5b98-8d9b-4e1e-8fa6-15e50484ddcc"},"cell_type":"code","outputs":[],"execution_count":null},{"source":"### Now we can plot some of the data","cell_type":"markdown","metadata":{"_uuid":"0d38a68d30f84c89d5dcbd50ea773fba2c3d969b","_cell_guid":"6af4f681-f207-43ec-b3d5-0518d32e8408"}},{"source":"dfn = df[features].apply(lambda x: mstats.winsorize(x, limits=[0.05, 0.05]))\ndfn.head()","metadata":{"_uuid":"f2a61c52eba5f5acdf08d9ae41600c7dfc856cb3","_cell_guid":"fcbc21c4-7269-469b-ab80-6235f9fc4c93"},"cell_type":"code","outputs":[],"execution_count":null},{"source":"scaler = StandardScaler()\npca = PCA(n_components=3)\nX_reduced = pca.fit_transform(scaler.fit_transform(dfn.values))\nd = {'EAP': 0, 'HPL': 1, 'MWS': 2}\ndf[\"author\"] = df[\"author\"].apply(lambda x: d[x])\ny = to_categorical(df[\"author\"].values)\nxp = fig = plt.figure(1, figsize=(8, 6))\nax = Axes3D(fig, elev=-150, azim=110)\nax.scatter(X_reduced[:, 0], X_reduced[:, 1], X_reduced[:, 2], c=y,\n           cmap=plt.cm.Blues, edgecolor='k', s=40)\nax.set_title(\"First three PCA directions\")\nax.set_xlabel(\"1st eigenvector\")\nax.w_xaxis.set_ticklabels([])\nax.set_ylabel(\"2nd eigenvector\")\nax.w_yaxis.set_ticklabels([])\nax.set_zlabel(\"3rd eigenvector\")\nax.w_zaxis.set_ticklabels([])\nplt.show()","metadata":{"_uuid":"0da9f98ff8e02c134a066564332107ed2ef5cd96","_cell_guid":"1bb77e5b-2dc7-4743-8473-d6b6231ac814"},"cell_type":"code","outputs":[],"execution_count":null},{"source":"#### Not very promising - try adding some polynomial features","cell_type":"markdown","metadata":{"_uuid":"c6705885ae4f1b7f31ffd990370f45e8077617c9","_cell_guid":"73aabe9c-59fe-41e8-b4b1-cf3ca426718a"}},{"source":"pipe = Pipeline(\n    steps=[\n        (\"scaler\", StandardScaler()),\n        (\"poly_fs\", PolynomialFeatures(degree=3)),\n        (\"pca\", PCA(n_components=3))\n    ])\n\nX_poly_reduced = pipe.fit_transform(dfn[features].values)\nxp = fig = plt.figure(1, figsize=(8, 6))\nax = Axes3D(fig, elev=-150, azim=110)\nax.scatter(X_poly_reduced[:, 0], X_reduced[:, 1], X_reduced[:, 2], c=y,\n           cmap=plt.cm.Blues, edgecolor='k', s=40)\nax.set_title(\"First three PCA directions\")\nax.set_xlabel(\"1st eigenvector\")\nax.w_xaxis.set_ticklabels([])\nax.set_ylabel(\"2nd eigenvector\")\nax.w_yaxis.set_ticklabels([])\nax.set_zlabel(\"3rd eigenvector\")\nax.w_zaxis.set_ticklabels([])\nplt.show()","metadata":{"_uuid":"59d68cf4b8ac1a10bcb5621105b3af0d9932b07d","_cell_guid":"37bb30d1-b54b-401b-b533-2eb573b89145"},"cell_type":"code","outputs":[],"execution_count":null},{"source":"### Let's try a simple model on this data. No need to scale data for Random Forest classifier","cell_type":"markdown","metadata":{"_uuid":"9e201b098f900541a13dd30ee0c945990fc400ad","_cell_guid":"7489cb94-d422-43c1-815c-69a657866f14"}},{"source":"df = df.drop([\"id\", \"author\", \"text\", \"content_words\", \"adjectives\"], axis=1)\nskf = StratifiedKFold(n_splits=3)","metadata":{"collapsed":true,"_uuid":"7266557840c80cfc41c17cab613afd601a881885","_cell_guid":"1937b096-3667-47b6-a004-da5ea7b6c644"},"cell_type":"code","outputs":[],"execution_count":null},{"source":"clf = RandomForestClassifier(n_estimators=1000, n_jobs=-1)\nX = dfn.values\nfor tr, te in skf.split(X, np.array([np.argmax(row) for row in y])):\n    preds1 = clf.fit(X[tr], y[tr]).predict(X[te])\n    print(metrics.log_loss(y[te], preds1))","metadata":{"_uuid":"18306cf8f45d59b69e5be9c09fa811cda521273a","_cell_guid":"189650a9-b138-41cf-9062-ea27c6886798"},"cell_type":"code","outputs":[],"execution_count":null},{"source":"### Not good, let's try an embedding model instead","cell_type":"markdown","metadata":{"_uuid":"6677e01766b54ca09a182a4e9721f8fc0466c3da","_cell_guid":"1cff122f-b8d9-42cc-84b4-8fd42a26d56c"}},{"source":"df = train.copy()\ndfte = test.copy()","metadata":{"collapsed":true,"_uuid":"fe78a822bb892931eceab19d0f78605afa6dedcf","_cell_guid":"bc419c9d-1d69-4ce7-8fc0-59b254d810c6"},"cell_type":"code","outputs":[],"execution_count":null},{"source":"### Start by defining some pre-processing functions to apply to the text","cell_type":"markdown","metadata":{"_uuid":"3abe6381c96cb162759e304b56568d17725169c3","_cell_guid":"7696a98c-98c8-49dc-a366-9b2b53fa03ee"}},{"source":"def extract_symbols(df):\n    \"\"\" separate symbols from words so they are vectorised independently\"\"\"\n    print(\"extracting symbols\")\n    df2 = df.copy()\n    df2[\"text\"] = df[\"text\"].apply(lambda x: \"\".join([\" {} \".format(s)\n                                                  if s in [\",\", \".\", \"#\", \"!\", \"'\", \"?\",\n                                                           \"Â£\", \"$\", \"^\", \"&\", \"*\", \"(\",\n                                                          \")\", \"-\", \"+\", \"`\", \":\", \";\"]\n                                                  else \"{}\".format(s)\n                                                  for s in list(x)])).apply(lambda x: re.sub(\" +\", \" \", x))\n    return df2\n\ndef remove_stopwords(df):\n    print(\"removing stopwords\")\n    sws = set(stopwords.words('english'))\n    df2 = df.copy()\n    df2[\"text\"] = df[\"text\"].apply(lambda x: \" \".join([w for w in x.split()\n                                                       if w.lower() not in sws]))\n    return df2\n\ndef _ngrams(text, n):\n    words = text.split()\n    return (words[i:i + n] for i in range(len(words) - n + 1))\n\ndef make_ngrams(df, n=3):\n    print(\"making ngrams\")\n    df2 = df.copy()\n    df2[\"ngrams\"] = df[\"text\"].apply(lambda x: \" \".join(_ngrams(x, n)))\n    return df2\n\ndef stemmer(df):\n    print(\"stemming\")\n    pst = PorterStemmer()\n    df2 = df.copy()\n    df2[\"text\"] = df[\"text\"].apply(lambda x: \" \".join([pst.stem(w) for w in x.split()]))\n    return df2\n\ndef remove_rare(df, n=3):\n    print(\"removing rare words...\")\n    # use the Counter class from python collections to calculate word frequencies\n    word_counter = Counter(list(itertools.chain(*[w for w in df[\"text\"].apply(lambda x: \n                                                                              [t.lower()\n                                                                               for t in x.split()])])))\n    rare_words = set([w for w in word_counter.keys() if word_counter[w] < n])\n    print(\"removing {} words: {}...\".format(len(rare_words), \" \".join(list(rare_words)[:10])))\n    df2 = df.copy()\n    df2[\"text\"] = df[\"text\"].apply(lambda x: \" \".join([w for w in x.split()\n                                                  if w not in rare_words]))\n    return df2\n\ndef trim_expand(df, minlen=15, maxlen=256):\n    print(\"trimming / expanding\")\n    df2 = pd.DataFrame()\n    df2[\"text\"] = df[\"text\"].apply(lambda x: x[maxlen:])\n    df2[\"author\"] = df[\"author\"]\n    dfx = pd.concat([df, df2])\n    dfx = dfx[dfx[\"text\"].map(len) > minlen]\n    return dfx\n\ndef lower(df):\n    df2 = df.copy()\n    df2[\"text\"] = df[\"text\"].apply(lambda x: \" \".join([w.lower() for w in x.split()]))\n    return df2\n\ndef count_vectoriser(text, v=None):\n    print(\"vectorising\")\n    if not v:\n        v = CountVectorizer(stop_words=None, ngram_range=(1, 4))\n        v.fit(text)\n    else:\n        print(\"using {}\".format(v))\n    return v.transform(text), v","metadata":{"collapsed":true,"_uuid":"79b96f3a17f532c49fdf8c61a76b8bb0b8d27a3b","_cell_guid":"33e4d922-5b80-476e-836b-cf50a9223adf"},"cell_type":"code","outputs":[],"execution_count":null},{"source":"### We can test various combinations of preprocessors by composing the functions","cell_type":"markdown","metadata":{"_uuid":"0899368fee197b9918eed09c6512b6c208238cd0","_cell_guid":"e44433d2-6f87-4c5a-a642-caa0c725f1d1"}},{"source":"# remember that the functions are applied in reverse order\ntext_prep_train = funcy.compose(remove_rare,\n                                #stemmer,\n                                #remove_stopwords,\n                                #lower,\n                                extract_symbols)\n\ntext_prep_test = funcy.compose(#stemmer,\n                               #remove_stopwords,\n                               #lower,\n                               extract_symbols)","metadata":{"collapsed":true,"_uuid":"d55f22724188135c3f800de77c76694c7e3ef1a6","_cell_guid":"7b8c911e-2010-45db-b360-7f4f17ee64a6"},"cell_type":"code","outputs":[],"execution_count":null},{"source":"dftrain = text_prep_train(df)\ndftest = text_prep_test(dfte)","metadata":{"_uuid":"3e9fa39504c241668ce97326856ae2cc3020707f","_cell_guid":"64880384-f042-4918-9dad-b84c747e66ff"},"cell_type":"code","outputs":[],"execution_count":null},{"source":"print(dftrain.shape, dftest.shape)","metadata":{"_uuid":"0bd01b463b6e7c55a5af00e43028a7b7d5426e2a","_cell_guid":"a9cdcb0a-3378-45dc-bf4e-288bb6162cb6"},"cell_type":"code","outputs":[],"execution_count":null},{"source":"### Now we will extract the target variable (author), and take a validation set from the training set","cell_type":"markdown","metadata":{"_uuid":"9bb3e4fb61403ec4d89c64e103245a32a5c943a3","_cell_guid":"c13121d7-ee19-4a63-8e82-f083208f5549"}},{"source":"d = {'EAP': 0, 'HPL': 1, 'MWS': 2}\ndftrain[\"author\"] = dftrain[\"author\"].apply(lambda x: d[x])\ny = to_categorical(dftrain[\"author\"].values)","metadata":{"collapsed":true,"_uuid":"f63a715451f5c432a54b6a366a22ab3017a6b412","_cell_guid":"6d00617e-decf-47c1-87b0-0f8a2a2a05e3"},"cell_type":"code","outputs":[],"execution_count":null},{"source":"# y is now a one-hot encoding of authors\ny[:4]","metadata":{"_uuid":"377c155b29c50e9babbdab68db82154d8c746865","_cell_guid":"973820bb-e31f-4ee7-85b6-90c5e81cdc48"},"cell_type":"code","outputs":[],"execution_count":null},{"source":"Xtrain, Xval, ytrain, yval = train_test_split(dftrain[\"text\"].values, y)\n[a.shape for a in (Xtrain, Xval, ytrain, yval)]","metadata":{"_uuid":"d7018370a7bdd41208afed979af505322855f513","_cell_guid":"6d1e443d-c7ed-48e2-8716-81b6f73b4fa1"},"cell_type":"code","outputs":[],"execution_count":null},{"source":"### Now tokenise the text","cell_type":"markdown","metadata":{"_uuid":"7bfb3d00db5453143598af35b56600ab2c0662f7","_cell_guid":"07171c95-6859-4d0d-8afb-c0bb1b6e1a21"}},{"source":"tokeniser = Tokenizer()\ntokeniser.fit_on_texts(Xtrain)","metadata":{"collapsed":true,"_uuid":"0e65e414d04c164ec2bb3fbce4a474eb78feb551","_cell_guid":"1950a71f-c2ba-4fea-883d-f71aeb6cfd67"},"cell_type":"code","outputs":[],"execution_count":null},{"source":"print(np.mean([len(t) for t in Xtrain]))\nprint(np.median([len(t) for t in Xtrain]))\nprint(np.max([len(t) for t in Xtrain]))\nprint(len([t for t in Xtrain if len(t) > 300]) / len(Xtrain))\nprint(np.argmax([len(t) for t in Xtrain]))\n# so let's use 300 as the max length","metadata":{"_uuid":"1cbf53408b40f0dba4a01505f455d1d4f06cdf97","_cell_guid":"2c03929f-aed6-42af-9624-13717a308954"},"cell_type":"code","outputs":[],"execution_count":null},{"source":"def tokenise(x, tokeniser, maxlen=256):\n    return pad_sequences(\n        sequences=tokeniser.texts_to_sequences(x),\n        maxlen=maxlen)","metadata":{"collapsed":true,"_uuid":"cf26345409f947da9758febea46c04461cba6fac","_cell_guid":"20307913-b32b-4864-8b89-4bc36d277361"},"cell_type":"code","outputs":[],"execution_count":null},{"source":"X_train_tokens, X_val_tokens, X_test_tokens = (tokenise(x, tokeniser)\n                                               for x in (Xtrain, Xval, dftest[\"text\"].values))","metadata":{"collapsed":true,"_uuid":"073eeded3d5e5df9f07e2dccd125a14f253a9400","_cell_guid":"d91686b6-ac71-47bf-86c9-2be7224263cb"},"cell_type":"code","outputs":[],"execution_count":null},{"source":"# this is what the longest phrase looks like after being tokenised;\n# shorted passages are padded with leading zeros\nlongest = np.argmax([len(t) for t in Xtrain])\nX_train_tokens[longest]","metadata":{"_uuid":"0ddf6f21d3f33cd6fcd453eb57b86bd5746b4861","_cell_guid":"e29d0851-75a9-4627-9b0b-4f2e2f911280"},"cell_type":"code","outputs":[],"execution_count":null},{"source":"### Let's try an implementation of FastText (see https://www.kaggle.com/nzw0301/simple-keras-fasttext-val-loss-0-31)","cell_type":"markdown","metadata":{"_uuid":"8495ad37c6b5c52d31e164bf5e5a1d44f341ecb6","_cell_guid":"5b8f52d0-83b0-48de-8ceb-1d42c6eab4a6"}},{"source":"input_dim = np.max(X_train_tokens) + 1\nembedding_dims = 15\ninput_dim","metadata":{"_uuid":"861824c33718316affd022aa262736ec0db4f979","_cell_guid":"3db44546-fd04-4a0c-9c6d-f9700995b535"},"cell_type":"code","outputs":[],"execution_count":null},{"source":"def build_model(input_dims, embedding_dims=20, optimiser=\"adam\"):\n    model = Sequential()\n    model.add(Embedding(input_dim=input_dims, output_dim=embedding_dims))\n    model.add(GlobalAveragePooling1D())\n    model.add(Dense(3, activation=\"softmax\"))\n    model.compile(loss=\"categorical_crossentropy\",\n                 optimizer=optimiser,\n                 metrics=[\"accuracy\"])\n    return model","metadata":{"collapsed":true,"_uuid":"2711975b96fb56cfc5b2416c41928f019e61a051","_cell_guid":"aebfa32c-158f-4116-b2e8-b1ff3276fb36"},"cell_type":"code","outputs":[],"execution_count":null},{"source":"epochs = 50\nmodel = build_model(input_dim, embedding_dims)","metadata":{"_uuid":"8b774fff40bbc93948d9456245f4ccf084cd207c","_cell_guid":"6d3101c8-379d-46fb-8f11-3fce49360b48"},"cell_type":"code","outputs":[],"execution_count":null},{"source":"data = model.fit(X_train_tokens, ytrain, batch_size=16, validation_data=(X_val_tokens, yval),\n                epochs=epochs, callbacks=[EarlyStopping(patience=2, monitor=\"val_loss\")])","metadata":{"_uuid":"d360100ae064de39db4dc49eb0b316df235583f3","_cell_guid":"6b5f6d61-22ff-46dc-8d59-87dff7984f0d"},"cell_type":"code","outputs":[],"execution_count":null},{"source":"preds = model.predict_proba(X_val_tokens)","metadata":{"_uuid":"8af0f1404634baa017617bd6cadafe394d074c25","_cell_guid":"a41350ed-f78d-4271-9c2c-ce6c94029bda"},"cell_type":"code","outputs":[],"execution_count":null},{"source":"print(metrics.log_loss(yval, preds))\nprint(metrics.roc_auc_score(yval, preds))","metadata":{"_uuid":"69aac92dd8226e887268cd1c40be6fb47a00c895","_cell_guid":"f97b734d-1b29-4789-bdcc-b3fef1569a66"},"cell_type":"code","outputs":[],"execution_count":null},{"source":"### How does this compare to Doc2Vec?","cell_type":"markdown","metadata":{"_uuid":"b2fce3c4b56f846d28722abe89327d880b5029fa","_cell_guid":"9fe8c3c7-e802-4561-abfd-a6ffee8feac2"}},{"source":"# Doc2Vec works with sentences, so first define some functions to split text into sentences\ndef _tagger(sentence_n):\n    sentence, i = sentence_n[1], sentence_n[0]\n    return TaggedDocument(sentence.split(), [i])\n\ndef tag_sentences(text, tokenizer):\n    tokens = tokenizer.tokenize(text)\n    return list(map(_tagger, enumerate(tokens)))","metadata":{"collapsed":true,"_uuid":"87350e3e63778ffbc8335dc85e78f46babe3bcc3","_cell_guid":"762e23d3-684e-4fd5-a8a8-59cf55c07f1e"},"cell_type":"code","outputs":[],"execution_count":null},{"source":"df = train.copy()\ndfte = test.copy()","metadata":{"collapsed":true,"_uuid":"f8846c1134b6e2d7a42c78994c633e1d70cecedf","_cell_guid":"14e342e5-eb82-431a-81ac-c70d6238b27b"},"cell_type":"code","outputs":[],"execution_count":null},{"source":"Xtrain, Xval, ytrain, yval = train_test_split(dftrain[\"text\"].values, y)\n[a.shape for a in (Xtrain, Xval, ytrain, yval)]","metadata":{"_uuid":"3b3e7bf7894551377f49e8746b6a89ae3ad92ecd","_cell_guid":"c665ad77-d9ce-4b79-9b15-e5e83fe65f7d"},"cell_type":"code","outputs":[],"execution_count":null},{"source":"### first we need to build the model's vocabulary; we will use all of the training data to do that","cell_type":"markdown","metadata":{"_uuid":"33dc32bc6ac0e5044f671a38cb36d7cd0f9dfbc3","_cell_guid":"7e362766-2799-49db-8ecc-a1fd72ce980d"}},{"source":"# use the tagger to tag each phrase\ntokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\ndocs = tag_sentences(\" \".join(itertools.chain(Xtrain)), tokenizer)\nprint(docs[:5])\nlen(docs)","metadata":{"_uuid":"5a145e2f958c73c36589ce78bd5cefc14809558f","_cell_guid":"f76dd2f6-f7bd-4d29-a4ee-f2456eef1d10"},"cell_type":"code","outputs":[],"execution_count":null},{"source":"# now we can build the model and add its vocabulary\nmodel = Doc2Vec(size=100, min_count=3, iter=1, window=8, workers=8)\nmodel.build_vocab(docs)","metadata":{"_uuid":"91dfbf16655391cdb8ddbc6ca4c1aeb0df3c24a5","_cell_guid":"6d9f5bd3-c347-427f-b5b9-111d524ee305"},"cell_type":"code","outputs":[],"execution_count":null},{"source":"model.corpus_count","metadata":{"_uuid":"006b7bc2fb33e298ec3fadfda146df31a928be88","_cell_guid":"34ea8e2b-2436-4650-bdee-a234a4ac385f"},"cell_type":"code","outputs":[],"execution_count":null},{"source":"# now we can train the model on the phrases in the training set\nn = len(Xtrain)\ni = 1\nfor sentence in Xtrain:\n    if i % 500 == 0:\n        print(\"trained on {}/{} phrases\".format(i, n))\n    doc = tag_sentences(sentence, tokenizer)\n    # the loop below allows us to shuffle the words in the sentence after each epoch of training\n    for _ in range(25):\n        model.train(doc, total_examples=model.corpus_count, epochs=model.iter)\n        shuffle(doc)\n    i += 1\nprint(\"done: {}\".format(model))\n# save the trained model\nwith open(\"d2v.raw\", \"wb\") as f:\n    model.save(f)","metadata":{"_uuid":"439629e1d11d0601c046c14cb970efe0c86a6248","_cell_guid":"1d7f049f-d99c-4c99-aab4-3fb73f6e3a29"},"cell_type":"code","outputs":[],"execution_count":null},{"source":"model = Doc2Vec.load(\"d2v.raw\")\nmodel.delete_temporary_training_data(keep_doctags_vectors=True, keep_inference=True)\nmodel.wv.similar_by_vector(\"big\")","metadata":{"_uuid":"25984b3295a40f70cdb3c2301726be98b412cb00","_cell_guid":"3360eeea-5783-42ee-96f2-c11301963b13"},"cell_type":"code","outputs":[],"execution_count":null},{"source":"# the model is now trained; we can use it to encode the phrases into vectors\nvecs_tr = np.array([model.infer_vector(phrase) for phrase in Xtrain])\nvecs_te = np.array([model.infer_vector(phrase) for phrase in Xval])\nprint(vecs_tr.shape, vecs_te.shape)","metadata":{"_uuid":"b9e81ea0b6c3518c49605c78332efb9439083794","_cell_guid":"f69e4b3d-ce26-4476-acd3-7b90c8508cbf"},"cell_type":"code","outputs":[],"execution_count":null},{"source":"print(ytrain.shape, yval.shape)","metadata":{"_uuid":"7d9fcd77dcbcdc47b70136a3b91208aa30d70199","_cell_guid":"a0ff04e0-61bc-426b-abf5-fa0c62a3ac7a"},"cell_type":"code","outputs":[],"execution_count":null},{"source":"clf = RandomForestClassifier(n_estimators=150, n_jobs=-1)\nmetrics.log_loss(yval, clf.fit(vecs_tr, ytrain).predict(vecs_te))","metadata":{"_uuid":"7519fe4f6296998870f2396bbfff332c54b4452b","_cell_guid":"ffb70af2-4f84-4d03-9a64-7149086b968b"},"cell_type":"code","outputs":[],"execution_count":null},{"source":"fig = plt.figure(1, figsize=(8, 6))\nax = Axes3D(fig, elev=-150, azim=110)\nX_reduced = PCA(n_components=3).fit_transform(vecs_tr)\nax.scatter(X_reduced[:, 0], X_reduced[:, 1], X_reduced[:, 2], c=ytrain,\n           cmap=plt.cm.Blues, edgecolor='k', s=40)\nax.set_title(\"First three PCA directions\")\nax.set_xlabel(\"1st eigenvector\")\nax.w_xaxis.set_ticklabels([])\nax.set_ylabel(\"2nd eigenvector\")\nax.w_yaxis.set_ticklabels([])\nax.set_zlabel(\"3rd eigenvector\")\nax.w_zaxis.set_ticklabels([])\nplt.show()","metadata":{"_uuid":"676120a6a915e9734eebc879be37d073091d9634","_cell_guid":"4af338e4-7ca6-4e43-8b5e-6910074c20e8"},"cell_type":"code","outputs":[],"execution_count":null},{"source":"","metadata":{"collapsed":true,"_uuid":"e36d33a3acdf939962af6971fa3e4e9783f56488","_cell_guid":"30f80db8-eaff-4a45-b5ae-2d85e87076a1"},"cell_type":"code","outputs":[],"execution_count":null}],"metadata":{"kernelspec":{"language":"python","name":"python3","display_name":"Python 3"},"language_info":{"file_extension":".py","version":"3.6.3","nbconvert_exporter":"python","mimetype":"text/x-python","name":"python","pygments_lexer":"ipython3","codemirror_mode":{"version":3,"name":"ipython"}}},"nbformat":4}