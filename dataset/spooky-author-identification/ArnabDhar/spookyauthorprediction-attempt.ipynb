{"cells":[{"cell_type":"markdown","source":"**I tried using the TF-IDF Tranform and then fed it to MultinomialNaive Bayes classifier but the accuracy did not get more than 79%. So I have done as below which gives a decent 83%. Please let me know more ways to increase accuracy. Also the final probability distributions would have been corrected to smaller decimal places.**","metadata":{"_cell_guid":"b6667c6b-a051-41fb-9bcc-a6309c88667d","_uuid":"68581555677187e41621424d170e6afd1d1000ad"}},{"execution_count":null,"cell_type":"code","source":"from nltk import word_tokenize\nimport pandas as pd\nfrom nltk.classify.util import apply_features,accuracy\nfrom nltk.classify.scikitlearn import SklearnClassifier\nfrom sklearn.naive_bayes import MultinomialNB\nfrom nltk.corpus import stopwords\nfrom random import shuffle\nimport numpy as np\nimport seaborn as sns\nfrom matplotlib import pyplot as plt","outputs":[],"metadata":{"_cell_guid":"be4453a1-64ef-42ac-a2a7-ad267dc7e464","collapsed":true,"_uuid":"d422d0c4de1a5ec4b785d44aa78b359f05be716c"}},{"execution_count":null,"cell_type":"code","source":"df = pd.read_csv(\"../input/train.csv\")","outputs":[],"metadata":{"_cell_guid":"5c1dba64-8c53-409f-b752-d80413430de7","collapsed":true,"_uuid":"7bdfdda70952b2f38be058fc9b211ba997e284f2"}},{"execution_count":null,"cell_type":"code","source":"X_text=df['text']\ny_author=df['author']","outputs":[],"metadata":{"_cell_guid":"cf13c579-c25a-4417-8595-6c11fdd9e6e2","collapsed":true,"_uuid":"5a27e3d7d24be0f49066799d49765d6e3b3bf840"}},{"execution_count":null,"cell_type":"code","source":"def getTextAuthorFeatures(words):\n    uniqueWords=set(words)\n    return dict({word:True for word in uniqueWords})","outputs":[],"metadata":{"_cell_guid":"10a72d6e-d4eb-4131-8cc4-5e17f57d527f","collapsed":true,"_uuid":"db95a3e1204a104bf698f8c16f6bf8a036cee356"}},{"cell_type":"markdown","source":"**splitting the data as 70% training and 30% test**","metadata":{"_cell_guid":"f610d499-faff-4210-994f-11400efb955c","_uuid":"50cf03794fe9a1e4bfcfec999e6c2cd86e62b2de"}},{"execution_count":null,"cell_type":"code","source":"sw=set(stopwords.words('english'))\nfrom nltk.stem import PorterStemmer\nfeatureList=[]\nps=PorterStemmer()\nfor entry,author in zip(X_text,y_author):\n    wordList=[ps.stem(w.lower()) for w in word_tokenize(entry) if len(w)>=2 and w not in sw]\n    featureList.append([getTextAuthorFeatures(wordList),author])\n\nshuffle(featureList)\nX_train=apply_features(getTextAuthorFeatures,featureList[:int(len(featureList)*0.70)])\nX_test=apply_features(getTextAuthorFeatures,featureList[int(len(featureList)*0.70):])\n\nmnb=SklearnClassifier(MultinomialNB())\nmnb.train(X_train)\nprint('Accuracy%:',accuracy(mnb,X_test)*100)\n","outputs":[],"metadata":{"_cell_guid":"80e7086f-c9c7-4632-aed4-03f8a19b6e08","collapsed":true,"_uuid":"39cb0d85bc8679f7804be8a438d165281b8080d6"}},{"cell_type":"markdown","source":"**Results from real test in test.zip.We got a decent accuracy, so let's calculate the probability of each author per entry.**","metadata":{"_cell_guid":"5c299ac3-7458-46df-952d-7ebf59da9dc3","_uuid":"4938c43a6cb64fb63d1c635de7f96cad04adaa50"}},{"execution_count":null,"cell_type":"code","source":"test=pd.read_csv(\"../input/test.csv\")\nfinalProbList=[]\nfor idx,row in test.iterrows():\n    text=row['text']\n    predicted_author=mnb.classify(getTextAuthorFeatures([w.lower() for w in word_tokenize(text) ]))\n    predicted_proba=mnb.prob_classify(getTextAuthorFeatures([w.lower() for w in word_tokenize(text) ]))\n    finalProbList.append([row['id'],[predicted_proba.prob(i) for i in predicted_proba.samples()]])","outputs":[],"metadata":{"_cell_guid":"b7609596-6c38-496c-8b9c-d09664725962","collapsed":true,"_uuid":"cba9e2e5385e6538dd8ab8308e223cab035a17c2"}},{"execution_count":null,"cell_type":"code","source":"finalProbList","outputs":[],"metadata":{"_cell_guid":"ec4c37ee-31d5-42bc-882f-91f14bdb665b","collapsed":true,"_uuid":"7c71020ecd26be99ed206a16a3267aa6f9cf4c0d"}},{"execution_count":null,"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer","outputs":[],"metadata":{"collapsed":true}},{"execution_count":null,"cell_type":"code","source":"vec=TfidfVectorizer(ngram_range=(1,3),stop_words='english')","outputs":[],"metadata":{"collapsed":true}},{"execution_count":null,"cell_type":"code","source":"vecText=vec.fit_transform(X_text)","outputs":[],"metadata":{"collapsed":true}},{"execution_count":null,"cell_type":"code","source":"from sklearn.decomposition import TruncatedSVD","outputs":[],"metadata":{}},{"execution_count":null,"cell_type":"code","source":"svd=TruncatedSVD(n_components=np.unique(y_author).shape[0])","outputs":[],"metadata":{}},{"execution_count":null,"cell_type":"code","source":"reducedX=svd.fit_transform(vecText)","outputs":[],"metadata":{"collapsed":true}},{"execution_count":null,"cell_type":"code","source":"dfSVD=pd.DataFrame(reducedX)\ndfSVD['author']=y_author","outputs":[],"metadata":{}},{"execution_count":null,"cell_type":"code","source":"X=vecText\ny=dfSVD['author']","outputs":[],"metadata":{}},{"execution_count":null,"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler","outputs":[],"metadata":{}},{"execution_count":null,"cell_type":"code","source":"scaler=StandardScaler()","outputs":[],"metadata":{}},{"execution_count":null,"cell_type":"code","source":"from sklearn.naive_bayes import MultinomialNB\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.model_selection import train_test_split","outputs":[],"metadata":{}},{"execution_count":null,"cell_type":"code","source":"mnb=MultinomialNB()","outputs":[],"metadata":{"collapsed":true}},{"execution_count":null,"cell_type":"code","source":"X_train,X_test,y_train,y_test=train_test_split(X,y,train_size=0.70,random_state=42)","outputs":[],"metadata":{}},{"execution_count":null,"cell_type":"code","source":"mlpClf=MLPClassifier(random_state=42,verbose=Tr)","outputs":[],"metadata":{}},{"execution_count":null,"cell_type":"code","source":"mlpClf.fit(X_train,y_train)","outputs":[],"metadata":{}},{"execution_count":null,"cell_type":"code","source":"mlpClf.score(X_test,y_test)*100","outputs":[],"metadata":{}}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"nbconvert_exporter":"python","pygments_lexer":"ipython3","name":"python","codemirror_mode":{"version":3,"name":"ipython"},"version":"3.6.3","file_extension":".py","mimetype":"text/x-python"}},"nbformat_minor":1,"nbformat":4}