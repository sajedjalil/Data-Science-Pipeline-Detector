{"cells":[{"source":"# Introduction\n\nThis is a simple LSTM based classifier.\n\nCurrently, I have nothing for preprocessing.","cell_type":"markdown","metadata":{"_cell_guid":"d0659a2a-8fd5-49f0-b09c-f30dccbedd1e","_uuid":"c7f541c706b0e6ad1ded324b1867c60577d4f5aa","collapsed":true}},{"outputs":[],"metadata":{"_cell_guid":"1a2d449c-2b3a-425b-a9cf-cb13ab0a4e33","_uuid":"c42e0493931797763cd4e7df061e8c9fdaa65a3a","collapsed":true},"cell_type":"code","source":"!mkdir log\n!mkdir model\n!mkdir submission","execution_count":1},{"outputs":[],"metadata":{"_cell_guid":"93051e5c-5bef-4457-9016-97b20917d4de","_uuid":"2c92a83bd9093e7b5dc574a3b6f3f2b2aeba044b"},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelBinarizer\nfrom sklearn.metrics import log_loss\n\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.text import text_to_word_sequence\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers import Embedding\nfrom keras.layers import Bidirectional\nfrom keras.layers import LSTM\nfrom keras.layers import Dense\nfrom keras.layers import Dropout\nfrom keras.backend import tensorflow_backend\nfrom keras.callbacks import EarlyStopping\nfrom keras.callbacks import ModelCheckpoint\nfrom keras.callbacks import CSVLogger\n\nfrom gensim.models import KeyedVectors\n\nimport pandas\nimport numpy","execution_count":2},{"source":"## Build the neural network using Keras\n\nI use [Keras](http://github.com/fchollet/keras) to build neural network.\n\nMy network is very simple: one Embedding layer, one Bidirectional LSTM layer and one Fully-connected layer.\n\nI use Adam whose parameters are default values to optimize the network.","cell_type":"markdown","metadata":{"_cell_guid":"d59fffaf-b625-4cc9-b37d-56a3d0bfabdf","_uuid":"35d1a7d96f18d4bfb19f46c8e9f469256e5adc5f"}},{"outputs":[],"metadata":{"_cell_guid":"0d70440a-20d3-4c1e-a818-04b934c51d41","_uuid":"df7222478c0353d4520a3272fffe207855ebad83","collapsed":true},"cell_type":"code","source":"def build_model(n_word, n_dim, n_hidden, syn0=None, trainable=True):\n    model = Sequential()\n\n    if syn0 is not None:\n        model.add(Embedding(input_dim=n_word+1, output_dim=n_dim, weights=[syn0], trainable=trainable))\n        \n    else:\n        model.add(Embedding(input_dim=n_word+1, output_dim=n_dim, trainable=trainable))\n\n    model.add(Dropout(0.5))\n    model.add(Bidirectional(LSTM(n_hidden)))    \n    model.add(Dense(50))\n    model.add(Dense(3, activation='softmax'))\n    return model","execution_count":3},{"source":"## Build the embedding matrix\n\nNext, We create a embedding matrix.\n\nEmbedding matrix is initiarized randomly.\n\nI use [Gensim](https://github.com/RaRe-Technologies/gensim) for preparing word-embedding.","cell_type":"markdown","metadata":{"_cell_guid":"4e409cde-44f8-4fa0-9a43-0de5be3d1973","_uuid":"d60badf66525f2a7f414755c0531c586cf23c258"}},{"outputs":[],"metadata":{"_cell_guid":"c3e6cd90-0d69-49aa-a4e9-91beadef5ca9","_uuid":"b4f7c0f0b6b8081a6738e1da1dd9bafe0e037e6f","collapsed":true},"cell_type":"code","source":"def build_embedding(n_word, n_dim, pretrain=False):\n    syn0 = numpy.random.random((n_word+1, n_dim))\n\n    if pretrain:\n        embedding_model = KeyedVectors.load(f'embedding/glove.6B.{n_dim}d')\n        for word, index in tokenizer.word_index.items():\n            try:\n                vector = embedding_model.word_vec(word)\n                index = tokenizer.word_index[word]\n                syn0[index, :] = vector\n\n            except Exception as e:\n                pass\n\n    return syn0","execution_count":4},{"source":"## Run experiment\n\nTrain, evaluate, create submission!\n\nYou can see the log of training in log directory.","cell_type":"markdown","metadata":{"_cell_guid":"c7bee84a-5b85-4385-ab6e-2d35f88bf18f","_uuid":"3192a58449a1037f3e0441819ec140279fb0dc15"}},{"outputs":[],"metadata":{"_cell_guid":"e4c1fc11-d5b6-4df3-9a05-d6d1e121f197","_uuid":"a9fb5cfc6da07270de9ec32e42712f24081c324e","collapsed":true},"cell_type":"code","source":"def experiment(n_word, n_dim, n_hidden, pretrain=True, trainable=True, batch_size=128):\n    syn0  = build_embedding(n_word, n_dim, pretrain=pretrain)\n    \n    if pretrain:\n        model = build_model(n_word, n_dim, n_hidden, syn0=syn0, trainable=trainable)\n    \n    else:\n        model = build_model(n_word, n_dim, n_hidden, trainable=trainable)\n\n    model_name = f'modelBiLSTM.embedding{n_dim}.n_hidden{n_hidden}.trainable{trainable}.pretrain{pretrain}'  # NOQA\n    print()\n    print()\n    print('# params')\n    print(f'n_word     : {n_word}')\n    print(f'n_dim      : {n_dim}')\n    print(f'n_hidden   : {n_hidden}')\n    print(f'pretrain   : {pretrain}')\n    print(f'trainable  : {trainable}')\n    print(f'destination: {model_name}')\n    \n    callbacks = []\n    callbacks.append(EarlyStopping(patience=3))\n    callbacks.append(ModelCheckpoint(filepath=f'model/{model_name}.hdf5', save_best_only=True))\n    callbacks.append(CSVLogger(filename=f'log/{model_name}.csv'))\n\n    model.summary()\n    model.compile('adam', 'categorical_crossentropy')\n    model.fit(X_train, y_train, batch_size=batch_size, epochs=100,\n              validation_data=[X_val, y_val], callbacks=callbacks)\n    model.load_weights(f'model/{model_name}.hdf5')\n    \n    y_val_pred_prob = model.predict(X_val, batch_size=batch_size)\n    val_loss = log_loss(y_val, y_val_pred_prob)\n    \n    print(f'{model_name}: {val_loss}')\n    \n    y_test_prob = model.predict(X_test, batch_size=batch_size)\n    test_data['EAP'] = y_test_prob[:, 0]\n    test_data['HPL'] = y_test_prob[:, 1]\n    test_data['MWS'] = y_test_prob[:, 2]\n\n    test_data[['id', 'EAP', 'HPL', 'MWS']].to_csv(f'submission/{model_name}.csv', index=False)\n    \n    return 0","execution_count":5},{"outputs":[],"metadata":{"_cell_guid":"368124bf-c30a-4ca2-b4a9-1827b50a754f","_uuid":"c699179c9da01e4b9197ba1d95da21bf17a22d9d","collapsed":true},"cell_type":"code","source":"train_data = pandas.read_csv('../input/train.csv', index_col=False)\ntest_data  = pandas.read_csv('../input/test.csv',  index_col=False)","execution_count":6},{"outputs":[],"metadata":{"_cell_guid":"6a1c0a96-c807-4c4d-92bf-955f23922c43","_uuid":"5fafab8f51fde498c2e2ac15e5c1f80077b778a2"},"cell_type":"code","source":"train_data.head()","execution_count":7},{"outputs":[],"metadata":{"_cell_guid":"67bc2eac-3128-4623-a45a-eefbb90d5230","_uuid":"8c09e3beee4d6486dad5ca05076fbb5f0f6b2d09"},"cell_type":"code","source":"test_data.head()","execution_count":8},{"outputs":[],"metadata":{"_cell_guid":"8d407d31-2a51-4eba-9f17-163460a96d98","_uuid":"275a5dfe8f3b60ab38c07378742b8ebca99e32e7"},"cell_type":"code","source":"all_text = pandas.concat([train_data.text, test_data.text])\nn_train = train_data.shape[0]\n\nprint(f'n_train: {n_train}')","execution_count":9},{"outputs":[],"metadata":{"_cell_guid":"f263451a-745f-4638-ac79-fffbcdc7990e","_uuid":"4a53f3eb95e2fc4e3dfac54642d3f583e324ebb6","collapsed":true},"cell_type":"code","source":"tokenizer = Tokenizer()\ntokenizer.fit_on_texts(all_text)\n\nlabelbinarizer = LabelBinarizer()\nlabelbinarizer.fit(train_data['author'])\n\nX = tokenizer.texts_to_sequences(train_data.text)\nX = pad_sequences(X)\n\ny = labelbinarizer.fit_transform(train_data['author'])","execution_count":10},{"source":"## Create validation dataset","cell_type":"markdown","metadata":{"_cell_guid":"0cf19a57-67d9-46f1-befa-0e8cc3c0a5ef","_uuid":"6766cf51c2d7e87fb84d3e9aaa9ab51c7454df4f"}},{"outputs":[],"metadata":{"_cell_guid":"43a4fd77-d54c-4563-abb2-fdc8a00b637c","_uuid":"8f7465494fd053f29105664981ca7fc1d21086c2"},"cell_type":"code","source":"X_train, X_val, y_train, y_val = train_test_split(X, y)\nn_word   = len(tokenizer.word_index)\n\nprint(f'vocaburary size: {n_word}')","execution_count":11},{"outputs":[],"metadata":{"_cell_guid":"b21f807d-7ded-4c29-b90b-3a39b91d7329","_uuid":"0c8d6f2203a0725fb0645782eb3e5ae804efc3cd","collapsed":true},"cell_type":"code","source":"X_test = tokenizer.texts_to_sequences(test_data.text)\nX_test = pad_sequences(X_test)","execution_count":12},{"outputs":[],"metadata":{"_cell_guid":"0ec70a47-d14c-4f7b-8ebb-8fbb3187770f","_uuid":"ec332723c49b0252cbf2634554cf8f8bc1b1c20b"},"cell_type":"code","source":"experiment(n_word, n_dim=50, n_hidden=50, pretrain=False, trainable=True)","execution_count":13},{"source":"## Pretrained Embedding [optional]\n\nYou want to use pretrained word-embeddings? OK, go on to the next!\n\n\n### Repos overview\n\nThe directory tree of my repos is following.\n\n```\n.                                                                            \n├── embedding                                                                                   ├── libexec                                                               \n│   └── binary_convert.py  bn# describe later\n├── log                                                                                         ├── model                                                                   \n├── spooky.ipynb  # this notebook\n└── submission\n```\n\n\n#### Download and unzip\n\nI use GloVe 6 billion embeddings.\n\nPlease download [GloVe](https://nlp.stanford.edu/projects/glove/) embeddings.\n\nTo down load them, exec following commands.\n\n__NOTE__: I don't test this on kernel because glove.6B.zip is a large file. If you want test, please do it on your computer.\n\n```bash\nmkdir embedding\ncd embedding\n\nwget http://nlp.stanford.edu/data/glove.6B.zip\nunzip glove.6B.zip\n\ncd ..\n\nfor fname in `ls embedding/*.txt`; do\n  python -m gensim.scripts.glove2word2vec -i $fname -o `echo $fname | sed -s 's/txt/w2v/'`;\ndone\n```\n\n\n#### Convert text file to binary models\n\nNow, I have pretrained embeddings in `repo/embedding`.\n\nThen, I create gensim binary objects by `python libexec/convert.py`.\n\n`libexec/convert.py` looks below.\n\n\n```python\nfrom gensim.models import KeyedVectors\nimport glob\n\nfor fname in glob.glob('embedding/glove.6B.*.w2v'):\n    e = KeyedVectors.load_word2vec_format(fname)\n    e.save(fname.replace('.w2v', ''))\n```\n\n(exec `rm embedding/*.txt embedding/*.w2v embedding/glove.6B.zip` to cleanup.)\n\n\n## Enjoy!!\n\nAll preparations complete!\n\nWe can use pre-trained word-embeddings to train our model.\n\nBut currently, as [this discussion](https://www.kaggle.com/c/spooky-author-identification/discussion/42316) says,\nI cannot improve the score (It seems supress over-fitting).","cell_type":"markdown","metadata":{"_cell_guid":"a398775f-5c23-4627-96dd-f1ecf34bcd19","_uuid":"c73b5d7155dc9005c2cd1bd944e7140bcd2b16a6"}},{"outputs":[],"metadata":{"_cell_guid":"a58207dd-2f88-425c-88f6-e5b8d0c6baab","_uuid":"078b46954b31639460150f16eb466d0ebdee9415","collapsed":true},"cell_type":"code","source":"# use pre-trained word-embeddings\nexperiment(n_word, n_dim=50, n_hidden=50, pretrain=True, trainable=True) # not tested on kernel","execution_count":null}],"nbformat_minor":1,"nbformat":4,"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"version":3,"name":"ipython"},"nbconvert_exporter":"python","version":"3.6.3","file_extension":".py","pygments_lexer":"ipython3","mimetype":"text/x-python","name":"python"}}}