{"nbformat_minor":1,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"mimetype":"text/x-python","nbconvert_exporter":"python","pygments_lexer":"ipython3","name":"python","file_extension":".py","version":"3.6.3","codemirror_mode":{"name":"ipython","version":3}}},"cells":[{"source":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport nltk\nfrom nltk.corpus import stopwords as sw\nfrom nltk.corpus import wordnet as wn\nimport plotly.offline as py\nimport plotly.graph_objs as go\n%matplotlib inline\npy.init_notebook_mode(connected=True)","outputs":[],"metadata":{"_cell_guid":"9108985c-d16f-44e9-9a5f-345161fb716b","_uuid":"756c1b72444ee4427978aa81758a95a4eda7a836"},"execution_count":null,"cell_type":"code"},{"source":"# Reading the train and test set with pandas\ntrain = pd.read_csv(\"../input/train.csv\")\ntest = pd.read_csv(\"../input/test.csv\")\ntrain.head()","outputs":[],"metadata":{"_cell_guid":"080f4698-f848-4948-96e3-6d7c352c465a","_uuid":"5610bcd60c831fa34aa418715f1523d5e25eb93c"},"execution_count":null,"cell_type":"code"},{"source":"fig_sizes = {'S' : (6.5,4),\n             'M' : (9.75,6),\n             'L' : (13,8)}\n    \n#function that prepares plot \n#input:\n#    ax:seaborn plot\n#    f_size:figure size (defaulted as medium size)\n#    plot_title: plot title\n#    x_title: x title\n#    y_title: y title\ndef show_plot(f_size=fig_sizes['M'],plot_title=\"\",x_title=\"\",y_title=\"\"):\n    plt.figure(figsize=f_size)\n    plt.xlabel(x_title)\n    plt.ylabel(y_title)\n    plt.title(plot_title)\n\nauth_counts = train.author.value_counts()\nax_bp = show_plot(fig_sizes['M'],'Author Barplot','Author','Count')\nsns.barplot(x=auth_counts.index, y=auth_counts.values,ax=ax_bp)","outputs":[],"metadata":{"_cell_guid":"22e7b016-729d-45a9-91c3-a4d4b67b522c","_uuid":"929e3e5cf57460e34390382ac7f621c26368a51c"},"execution_count":null,"cell_type":"code"},{"source":"Lets take a look of a sample from each author:","metadata":{"_cell_guid":"f1d6f098-f99a-4d2b-ba78-8bd7d31d4fa0","_uuid":"e278aacb8e607bb03e6a1df0f70e30b191d29508"},"cell_type":"markdown"},{"source":"#Edgar Allan Poe Sample\ntrain.loc[train.author=='EAP','text'].iloc[:5]","outputs":[],"metadata":{"_cell_guid":"aafb5586-0955-40b6-97e8-68b30e30bc0c","_uuid":"7fd00532a45f6b85fd8962b289a8a016e5065ccb"},"execution_count":null,"cell_type":"code"},{"source":"#Mary Shelley Sample\ntrain.loc[train.author=='MWS','text'].iloc[:5]","outputs":[],"metadata":{"_cell_guid":"9ed63c20-a4e9-4c91-b4c7-592795f19327","_uuid":"a1e8acaf22efff649ab2d20575e2a6ac5e431b4d"},"execution_count":null,"cell_type":"code"},{"source":"#Mary Shelley Sample\ntrain.loc[train.author=='HPL','text'].iloc[:5]","outputs":[],"metadata":{"_cell_guid":"56bbc10b-0635-42a3-a634-42427f81a0a1","_uuid":"32d434625995d9e38bf9e06cae8eb156fb62bc50"},"execution_count":null,"cell_type":"code"},{"source":"w_counts = train['text'].str.split(expand=True).unstack().value_counts()","outputs":[],"metadata":{"_cell_guid":"85bca37b-ead0-4dc6-bebc-b037d85f0d09","_uuid":"92f41bb7357f3fbb6438c64eaec7bd89b075f117","collapsed":true},"execution_count":null,"cell_type":"code"},{"source":"ax_bp = show_plot(fig_sizes['M'],'Word Frequency','Word','Count')\nsns.barplot(x=w_counts.index[:20], y=w_counts.values[:20],ax=ax_bp)","outputs":[],"metadata":{"_cell_guid":"3cf8c632-8e25-4dea-bad3-395ceb3c6ba5","_uuid":"74b3c7668ea3423b579bb914514e05b3c17a2cb5"},"execution_count":null,"cell_type":"code"},{"source":"We can see that the most frequent words are mostly 'Stopwords', and they do not add info. Lets repeat this plot, now removing the Stopwords.","metadata":{"_cell_guid":"c3ad1e35-518c-499b-af7d-aebff1bf2a05","_uuid":"b66f712e6485a87dbc9af8aed4917e8c47fc28e2"},"cell_type":"markdown"},{"source":"sw_en = sw.words('english')\n#we will generate a regex that captures any stopword\nreg_match_sw = '|'.join(sw_en)\nreg_match_sw","outputs":[],"metadata":{"_cell_guid":"14b5a6f4-b798-48d6-9807-fa9c21240e85","_uuid":"b0950c775cf8c9f7efc1687d150417f47b55818d"},"execution_count":null,"cell_type":"code"},{"source":"#to lower so the regex can handle properly\ntrain.loc[:,'text_lower'] = train['text'].str.lower()\ntrain.loc[:,'text_lower'] = train.text_lower.str.replace(r'(\\b)('+(reg_match_sw)+r')(\\b)','')\ntrain.loc[:,'text_lower'] = train.text_lower.str.replace(r'[,\\.\\'\\\"-?;!]','')","outputs":[],"metadata":{"_cell_guid":"9d0d7ce5-0eda-4b07-ba35-c2b8fd5487f2","_uuid":"5f0ce13a49e754c1e00b0b199ecead1e39baa4ae","collapsed":true},"execution_count":null,"cell_type":"code"},{"source":"train.text_lower.iloc[:10]","outputs":[],"metadata":{"_cell_guid":"7b3667f6-4aca-44a8-b50e-d62acc33d9bc","_uuid":"3a504c7d5996fbaccc540800bee93487dbbf4a08"},"execution_count":null,"cell_type":"code"},{"source":"w_counts_wosw = train['text_lower'].str.split(expand=True).unstack().value_counts()\nax_bp = show_plot(fig_sizes['L'],'Word Frequency - WO Stopwords','Word','Count')\nsns.barplot(x=w_counts_wosw.index[:20], y=w_counts_wosw.values[:20],ax=ax_bp)","outputs":[],"metadata":{"_cell_guid":"e33826f3-f55b-4478-8615-e932d198f7ce","_uuid":"5818c57925e42bd27648549d6a01646a0aff479f"},"execution_count":null,"cell_type":"code"},{"source":"All right, no more 'the, 'of, 'and'... let's go one step further. ","metadata":{"_cell_guid":"5c7d8638-2390-4801-b710-f41d62f000eb","_uuid":"8d5b5b1221b61f0a22e40bfb0343f62e4f493cf9"},"cell_type":"markdown"},{"source":"Let's generate a 3D plot, where each axis value is the frecuency of a specific word for each author. We will pick the top 250 frequent words.","metadata":{"_cell_guid":"647af817-fa26-4ed4-ab2d-65f626705819","_uuid":"145cf1d21b80be0ec4bc4cedce3052b036d0685e"},"cell_type":"markdown"},{"source":"counts_eap = train.loc[train.author==\"EAP\",'text_lower'].str.split(expand=True).unstack().value_counts()\ncounts_wms = train.loc[train.author==\"MWS\",'text_lower'].str.split(expand=True).unstack().value_counts()\ncounts_hpl =  train.loc[train.author==\"HPL\",'text_lower'].str.split(expand=True).unstack().value_counts()\ncounts_eap = counts_eap.loc[[idx for idx in counts_eap.index if (idx in w_counts_wosw.iloc[:250].index)]].sort_index()\ncounts_wms = counts_wms.loc[[idx for idx in counts_wms.index if (idx in w_counts_wosw.iloc[:250].index)]].sort_index()\ncounts_hpl = counts_hpl.loc[[idx for idx in counts_hpl.index if (idx in w_counts_wosw.iloc[:250].index)]].sort_index()","outputs":[],"metadata":{"_cell_guid":"9a4a5ca6-9d55-4b4f-b377-d163355343ad","_uuid":"71456cfdf63f473db1588be8d95b78901b635ac1","collapsed":true},"execution_count":null,"cell_type":"code"},{"source":"common_words = set(counts_eap.index).intersection(set(counts_wms.index)).intersection(set(counts_hpl.index))\ncounts_eap = counts_eap.loc[counts_eap.index.isin(common_words)]\ncounts_wms = counts_wms.loc[counts_wms.index.isin(common_words)]\ncounts_hpl = counts_hpl.loc[counts_hpl.index.isin(common_words)]\nprint(counts_eap.shape)\nprint(counts_wms.shape)\nprint(counts_hpl.shape)","outputs":[],"metadata":{"_cell_guid":"11dfbd5b-05b5-4209-9199-6771c740c67c","_uuid":"883b6fdf8c932308355fecd1515008bce61065fb"},"execution_count":null,"cell_type":"code"},{"source":"freqs_eqp = 100*counts_eap/np.sum(counts_eap)\nfreqs_wms = 100*counts_wms/np.sum(counts_wms)\nfreqs_hpl = 100*counts_hpl/np.sum(counts_hpl)\n\n#distance from the average \nmean_freq = (freqs_eqp+freqs_wms+freqs_hpl)/3\ndist_mean = (np.abs(freqs_eqp-mean_freq)+np.abs(freqs_wms-mean_freq)+np.abs(freqs_hpl-mean_freq))/mean_freq","outputs":[],"metadata":{"_cell_guid":"93b6298d-fab3-4845-bad1-243698c125e4","_uuid":"f2defde9ea8e1b3f1f2dddbaf05cc7b093d1bc08","collapsed":true},"execution_count":null,"cell_type":"code"},{"source":"%matplotlib inline\ntrace1 = go.Scatter3d(\n    x=np.log(freqs_eqp),\n    y=np.log(freqs_wms),\n    z=np.log(freqs_hpl),\n    text=counts_eap.index,\n    mode='markers',\n    marker=dict(\n        size=np.log2(counts_eap+counts_wms+counts_hpl),\n        color=dist_mean,\n        colorscale='RdBu',  \n        opacity=0.8\n    )\n)\n\ndata = [trace1]\nlayout = go.Layout(\n    margin=dict(\n        l=0,\n        r=0,\n        b=0,\n        t=0\n    )\n)\n\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig)","outputs":[],"metadata":{"_cell_guid":"60505c0f-c7c6-4c05-83f4-03b66409e3d6","_uuid":"a1e20babc5fa941efbaebf7d107651fc7effb948"},"execution_count":null,"cell_type":"code"},{"source":"So here it is the plot, the bigger the point is, the more frequent it is, and the closer to red, the more deviation there is between authors frecuency for that word. Some red and orange points (words) are: \"turned\", \"love\", \"de\", \"however\", \"mr\", and the dot punctuation mark.","metadata":{"_cell_guid":"17519fd1-093c-4ede-9ed7-c4733400f182","_uuid":"c29109978250b4a2292491de64d4c1ea5f454055"},"cell_type":"markdown"},{"source":"What about that 'de' word? it happens in 2.5% MWS of times for and 3.3%  for HPL****, lets take a look at some samples:","metadata":{"_cell_guid":"638c83c5-879c-44af-bc8a-c74d292406a7","_uuid":"0edf1953025967a9c021bbd422cfc67832141501"},"cell_type":"markdown"},{"source":"for sample in train.loc[train.loc[:,'text_lower'].str.contains('\\sde\\s'),'text_lower'].iloc[:10]:\n    print(sample)","outputs":[],"metadata":{"_cell_guid":"10ffb422-9e12-45aa-97f6-c14321f1cb91","_uuid":"bbc4b5c2685f5b6e9fa10a16b4f4f7001ebdfbd9"},"execution_count":null,"cell_type":"code"},{"source":"![](http://)ah, so it seems that at texts, there are some french words, it might be useful to build further features. Also, de is used as substitute for 'the' at some points.","metadata":{"_cell_guid":"f1a14aae-7452-4a5f-ad41-ea800901b4e8","_uuid":"0c5f5f782182f02aa8c095055bb71e5be3607338"},"cell_type":"markdown"},{"source":"Now, lets generate some more interesting features, like:\n* Number of characters in text\n* Number of words in text\n* Number of special characters in text","metadata":{"_cell_guid":"40d77462-d6b9-4a1b-b629-7f6f6158dfb0","_uuid":"e5a765cbc94277be3bad26505f08e7fe3018a8f8"},"cell_type":"markdown"},{"source":"import re\ndef gen_meta_features(df,text_col):\n    #N chars in text\n    df[\"n_chars\"+\"_\"+text_col] = df[text_col].str.len()\n    #N words in text\n    df[\"n_words\"+\"_\"+text_col] = df[text_col].str.split().apply(lambda x:len(x))\n    #N punctuations chars in text\n    df[\"n_punct\"+\"_\"+text_col] = df[text_col].apply(lambda x: len(re.findall(r'[,.:\\'\\\"-?;!]',str(x))))\n    return df","outputs":[],"metadata":{"_cell_guid":"84cf9a84-a97a-4a14-a151-ad5988952d11","_uuid":"c889b5632b05fadb1b9e82dd8c16015570733bb2","collapsed":true},"execution_count":null,"cell_type":"code"},{"source":"Lets take a look at this feature, first at the raw text version, and then to the lower case stopwords free version","metadata":{"_cell_guid":"30179765-60d6-49a1-8d9a-ea78a982c8b6","_uuid":"b44651c8d8d1781da8b8d0714420529ca525fe21"},"cell_type":"markdown"},{"source":"#generate features for raw text\ntrain = gen_meta_features(train,'text')\n#generate features for lower wo stopwords text\ntrain = gen_meta_features(train,'text_lower')","outputs":[],"metadata":{"_cell_guid":"10dd621c-53f9-4211-ae66-dac0dbc34bfe","_uuid":"a0c0ea7e4a9a38fd63c6c7ce98e03fa464a21b72","collapsed":true},"execution_count":null,"cell_type":"code"},{"source":"Let's first take a look at the number of characters features","metadata":{"_cell_guid":"dbab980b-d85c-4b77-9166-ffbfe086e0e9","_uuid":"05ead4356ba5e3a07354651b3b21a426a42057f9"},"cell_type":"markdown"},{"source":"ax_box = show_plot(fig_sizes['M'],'N. Char for raw text','Author','N. Char distribution')\nsns.boxplot(x=train.loc[train.n_chars_text<1000,'author'], y=train.loc[train.n_chars_text<1000,'n_chars_text'],ax=ax_box)\nax_box = show_plot(fig_sizes['M'],'N. Char for lower text','Author','N. Char distribution')\nsns.boxplot(x=train.loc[train.n_chars_text_lower<1000,'author'], y=train.loc[train.n_chars_text_lower<1000,'n_chars_text_lower'],ax=ax_box)","outputs":[],"metadata":{"_cell_guid":"70d90447-c476-457a-8ffe-f80446c7843f","_uuid":"6fe1d00783c30c787da8726329222a6bb3453858"},"execution_count":null,"cell_type":"code"},{"source":"The distributions are really similar, maybe for our friend Edgar, we have a meadian value a bit lower than his peers, this can be appretiated in both versions of texts. Now let's look at the number of words:","metadata":{"_cell_guid":"f1c288a9-9fd8-426c-9ab1-e90d74717af2","_uuid":"75094156f3a3a06e9980b2e32037a00418002d2a"},"cell_type":"markdown"},{"source":"ax_box = show_plot(fig_sizes['M'],'N. Words for raw text','Author','N. Words distribution')\nsns.boxplot(x=train.loc[train.n_words_text<1000,'author'], y=train.loc[train.n_chars_text<1000,'n_words_text'],ax=ax_box)\nax_box = show_plot(fig_sizes['M'],'N. Words for lower text','Author','N. Char distribution')\nsns.boxplot(x=train.loc[train.n_chars_text_lower<1000,'author'], y=train.loc[train.n_chars_text_lower<1000,'n_words_text_lower'],ax=ax_box)","outputs":[],"metadata":{"_cell_guid":"056fec5f-ccff-437c-bf08-eaa9ebf4f47f","_uuid":"097fa4f8d7fcba68dddbe1d62df85df93b770c62"},"execution_count":null,"cell_type":"code"},{"source":"In this case, it is the median value of the number of words for Howard is sligtly bigger. This pattern appears for both versions of the text. Now we will take a look at the punctuation characters, this time only in the raw version of the text, as we removed the punctuation marks from the text_lower version.","metadata":{"_cell_guid":"1f7a5650-1744-4094-89d7-bba1f0eae623","_uuid":"12cf0e8147e17ceaa5f1db4b5cb64aedb035bd41"},"cell_type":"markdown"},{"source":"ax_box = show_plot(fig_sizes['M'],'N. Punctuation chars for raw text','Author','N. Puntctuation chars distribution')\nsns.boxplot(x=train.loc[train.n_punct_text<20,'author'], y=train.loc[train.n_punct_text<20,'n_punct_text'],ax=ax_box)","outputs":[],"metadata":{"_cell_guid":"4f6b2b1b-8a5b-4e2d-88a3-6f0f3c497b28","_uuid":"dd38d4ca6fded21c4fb7e8b9376bfc7c1855bb18"},"execution_count":null,"cell_type":"code"},{"source":"For the punctuation feature, Howard has a narrower distribution and a lower median value","metadata":{"_cell_guid":"10048563-a032-4249-b118-65c7d60d67c2","_uuid":"cc09d482a6113a1e765c79105cfb92c2d9de93d4"},"cell_type":"markdown"},{"source":"Before we saw some french words, let's try to capture non-english words and generate a feature from that.","metadata":{"_cell_guid":"cbf8a682-36fe-4c11-81ae-7749fc5a463d","_uuid":"62f087cb7ba8daa07a808f9afae42a6125c625a1"},"cell_type":"markdown"},{"source":"#set of english words\nenglish_vocab = set(w.lower() for w in nltk.corpus.words.words())\n#set of words that do not appear in the nltk engluish vocabulary\nnot_in_vocab = set(train.loc[:,'text_lower'].str.split(expand=True).unstack()).difference(english_vocab)\nprint(len(not_in_vocab))\nlist(not_in_vocab)[:15]","outputs":[],"metadata":{"_cell_guid":"cb389cab-660b-4c8f-bdb9-4b9fb1d57935","_uuid":"90010820aca0de53f8ee05e3770c4d7278b563cb"},"execution_count":null,"cell_type":"code"},{"source":"In the previous set, we picked some frech words, but mostly english words with some suffix. Anywaws, lets see if they make any difference between the three autors.","metadata":{"_cell_guid":"df712a54-1001-42ab-b467-76df845b6d99","_uuid":"a1a48ec4986ad77ef93c1b88bbec2b211aa35045"},"cell_type":"markdown"},{"source":"train[\"n_non_voc\"] = train['text_lower'].str.split().apply(lambda x:len(set(x).intersection(not_in_vocab)))\nax_box = show_plot(fig_sizes['M'],'N. Words not in nltk corpus','Author','N. Words not in nltk corpus distribution')\nsns.boxplot(x=train.loc[train.n_non_voc<50,'author'], y=train.loc[train.n_non_voc<50,'n_non_voc'],ax=ax_box)","outputs":[],"metadata":{"_cell_guid":"f55a1970-6dd0-484e-a560-7dab86238b94","_uuid":"f76a8e929c0ac324e77acd06a5a0d6a81e94185c"},"execution_count":null,"cell_type":"code"},{"source":"Let's see what happens with this feature if we first lemmatize our words","metadata":{"_cell_guid":"5f30d70f-ac6d-4244-8510-27e0be9aa6db","_uuid":"71e1d977366f0c02032211f6020a46a0c19855ce"},"cell_type":"markdown"},{"source":"from nltk.stem import WordNetLemmatizer\nwordnet_lemmatizer = WordNetLemmatizer()\n#example with the plural sufix\nprint(wordnet_lemmatizer.lemmatize('planets'))\nnot_in_vocab_lem = set([wordnet_lemmatizer.lemmatize(w) for w in set(train.loc[:,'text_lower'].str.split(expand=True).unstack()) if w is not None]).difference(english_vocab)\nprint(len(not_in_vocab_lem))\nlist(not_in_vocab_lem)[:15]","outputs":[],"metadata":{"_cell_guid":"6a775a0d-e601-4e06-9fee-56318c282456","_uuid":"c92d51b3a76a13278dc7e3a381d99ea3b78934af"},"execution_count":null,"cell_type":"code"},{"source":"train[\"n_non_voc_lem\"] = train['text_lower'].str.split().apply(lambda x:len(set(x).intersection(not_in_vocab_lem)))\nax_box = show_plot(fig_sizes['M'],'N. Words not in lemmatized nltk corpus','Author','N. Words not in lemmatized nltk corpus distribution')\nsns.violinplot(x=train.loc[train.n_non_voc_lem<10,'author'], y=train.loc[train.n_non_voc_lem<10,'n_non_voc_lem'],ax=ax_box)","outputs":[],"metadata":{"_cell_guid":"f74c8a10-07f4-4a29-8a96-6220a31c9a44","_uuid":"6273c89c7d670e4b71d97e617887471e8aa0f945"},"execution_count":null,"cell_type":"code"},{"source":"That handled a lot of the plurarls sufixes we saw before, but we still have a lot of 'ing's and 'ed's. Let's handle this using a stemmer in both the train dataset, and the vocabulary dataset.","metadata":{"_cell_guid":"dd15c959-802c-4866-9c95-30f8b0986504","_uuid":"6f2d39033031e5f1bfe12bc722fbf36c18dbb5df"},"cell_type":"markdown"},{"source":"from nltk.stem.porter import *\nstemmer = PorterStemmer()\nnot_in_vocab_stem = set([stemmer.stem(w) for w in set(train.loc[:,'text_lower'].str.split(expand=True).unstack()) if w is not None]).difference(set([stemmer.stem(v)for v in english_vocab]))\nprint(len(not_in_vocab_stem))\nlist(not_in_vocab_stem)[:15]","outputs":[],"metadata":{"_cell_guid":"c668f7fd-c2c1-499b-ba18-86e58e59ae23","_uuid":"c07e6ac6cfcff266a50ca368f763da56300ad657"},"execution_count":null,"cell_type":"code"},{"source":"train[\"n_non_voc_stem\"] = train['text_lower'].str.split().apply(lambda x:len(set(x).intersection(not_in_vocab_stem)))\nax_box = show_plot(fig_sizes['M'],'N. Words not in stemmed nltk corpus','Author','N. Words not in stemmed nltk corpus distribution V2')\nsns.violinplot(x=train.loc[train.n_non_voc_stem<5,'author'], y=train.loc[train.n_non_voc_stem<5,'n_non_voc_stem'],ax=ax_box)","outputs":[],"metadata":{"_cell_guid":"39e3a1ae-3efb-4ac2-859f-c9e1d4e23454","_uuid":"76dbb39398af0968877bb56681a7eec50daa75d1"},"execution_count":null,"cell_type":"code"},{"source":"train.loc[train.n_non_voc_stem<5].groupby(['author','n_non_voc_stem']).size()/train.shape[0]","outputs":[],"metadata":{"_cell_guid":"7e9c7473-30ba-4ae6-8719-dd591bd152d8","_uuid":"c7d03bec13361d9ae288189e0b9bbcf7ce5a614d"},"execution_count":null,"cell_type":"code"},{"source":"So, theese spooky authors meet star wars, as we will use Vader (Valence Aware Dictionary and sEntiment Reasoner) to capture the sentiment of the texts we have","metadata":{},"cell_type":"markdown"},{"source":"from nltk.sentiment.vader import SentimentIntensityAnalyzer\nanalyser = SentimentIntensityAnalyzer()\n\nprint(analyser.polarity_scores(\"this is a very bad day\")['compound'])\nprint(analyser.polarity_scores(\"this is a very nice day\")['compound'])","outputs":[],"metadata":{"_cell_guid":"2b0c2b2a-f88d-40e9-8396-a08d26308e4f","_uuid":"b84edb7ac009193ab026c612965848302b1dfcb7"},"execution_count":null,"cell_type":"code"},{"source":"train.loc[:,'text_sent'] = train['text_lower'].apply(lambda x:analyser.polarity_scores(x)['compound'])\nax_box = show_plot(fig_sizes['M'],'Text sentiment','Author','Text sentiment distribution')\nsns.violinplot(x=train['author'], y=train['text_sent'],ax=ax_box)","outputs":[],"metadata":{},"execution_count":null,"cell_type":"code"},{"source":"ax_sent = show_plot(fig_sizes['L'],'Sentiment Distribution','','')\nsns.distplot([train.loc[train.author=='EAP','text_sent']],ax=ax_sent)\nsns.distplot([train.loc[train.author=='HPL','text_sent']],ax=ax_sent)\nsns.distplot([train.loc[train.author=='MWS','text_sent']],ax=ax_sent)","outputs":[],"metadata":{},"execution_count":null,"cell_type":"code"},{"source":"We can see that the distributions are simmilar, some differences are that Edgar tends to have more neutral texts, while, Mary Shelley tend to have more positive texts than her counterparts","metadata":{},"cell_type":"markdown"},{"source":"Next steps:\n* POS tagging\n* Text similarity\n* Modeling and evaluation","metadata":{},"cell_type":"markdown"},{"source":"","outputs":[],"metadata":{"collapsed":true},"execution_count":null,"cell_type":"code"}],"nbformat":4}