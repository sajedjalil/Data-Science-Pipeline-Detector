{"metadata":{"language_info":{"codemirror_mode":{"version":3,"name":"ipython"},"mimetype":"text/x-python","name":"python","version":"3.6.3","nbconvert_exporter":"python","pygments_lexer":"ipython3","file_extension":".py"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat_minor":1,"cells":[{"metadata":{"_uuid":"eeb7ef3e0b151809f08c634e170412d0eefd9d70","_cell_guid":"f6b9f25b-d4d0-48e7-b914-9d32f1c5d3a4"},"execution_count":null,"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output.","outputs":[]},{"metadata":{"_uuid":"bfe7d916676a810c56dd322e579700b63b5131a1","_cell_guid":"7ab89cd0-bd1b-4214-96ff-b2e27bcc4c3d"},"execution_count":null,"cell_type":"code","source":"train = pd.read_csv('../input/train.csv')\ntrain.head()","outputs":[]},{"metadata":{"_uuid":"791b042c537f8c50ab33b005f8a626f1aa9ba95d","_cell_guid":"25d90664-d0de-4118-9f15-82c31776832d"},"cell_type":"markdown","source":"Lot of tuning has to be done. This is just a base version of the code."},{"metadata":{"_uuid":"dff505b82cbeb5aae36962aae596287ee2789d19","collapsed":true,"_cell_guid":"95fc3c9d-83e1-412b-9628-943eddc694a2"},"execution_count":null,"cell_type":"code","source":"import nltk as nl","outputs":[]},{"metadata":{"_uuid":"7d692afa363652baf9ac5558d7985996be9a581c","collapsed":true,"_cell_guid":"1dde1616-a345-4b79-b6cb-4e3ae0aceee4"},"execution_count":null,"cell_type":"code","source":"train['tokens'] = [nl.word_tokenize(sentences) for sentences in train.text]","outputs":[]},{"metadata":{"_uuid":"d18fcc14cb4e097eca4cb75155122f073a5eac3e","_cell_guid":"6d69ad7b-31dc-412d-80a4-ebff315882b3"},"execution_count":null,"cell_type":"code","source":"words = []\nfor item in train.tokens:\n    words.extend(item)","outputs":[]},{"metadata":{"_uuid":"a457c2105d024ee5e83f3878d1f8cc0d6e4a1739","collapsed":true,"_cell_guid":"cd2bd09a-a983-4160-900c-5fb939fd8987"},"execution_count":null,"cell_type":"code","source":"stemmer = nl.stem.lancaster.LancasterStemmer()","outputs":[]},{"metadata":{"_uuid":"ea9674beaa63a1a652a735c84ee6e1d57c3f514b","collapsed":true,"_cell_guid":"434b7463-d50a-4999-be0f-7b8f818827e4"},"execution_count":null,"cell_type":"code","source":"words = [stemmer.stem(word) for word in words]\nwords = set(words)","outputs":[]},{"metadata":{"_uuid":"089132d877932d908a6ecaac92884f004cf42877","collapsed":true,"_cell_guid":"02b89b37-5fd8-4b50-b909-e1a606218c8b"},"execution_count":null,"cell_type":"code","source":"training = []\nfor index,item in train.iterrows():\n    onehot = []\n    token_words = [stemmer.stem(word) for word in item['tokens']]\n    for w in words:\n        onehot.append(1) if w in token_words else onehot.append(0)\n    \n    training.append([onehot,item['author']])","outputs":[]},{"metadata":{"_uuid":"cec844aa875888a80f91127d4ffa5e476afd2015","_cell_guid":"8b83d9b2-1198-4c7d-8ca5-b594d3080b95"},"execution_count":null,"cell_type":"code","source":"training_new = np.array(training)","outputs":[]},{"metadata":{"_uuid":"faa4d0a168501a22a0970c8eaf16a87d29b25c15","_cell_guid":"395b4f31-fe94-40dc-8fa1-5034269e1ad5"},"execution_count":null,"cell_type":"code","source":"from numpy import array\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import OneHotEncoder\n\n# integer encode\nlabel_encoder = LabelEncoder()\ninteger_encoded = label_encoder.fit_transform(training_new[:,1])\n\n# binary encode\nonehot_encoder = OneHotEncoder(sparse=False)\ninteger_encoded = integer_encoded.reshape(len(integer_encoded), 1)\nonehot_encoded = onehot_encoder.fit_transform(integer_encoded)\n\ntrain_y = onehot_encoded","outputs":[]},{"metadata":{"_uuid":"6b5a1587a2875b842b3702242a8bb7e2db7b4bea","collapsed":true,"_cell_guid":"83b44b8f-f2ab-4bdb-994b-d9b006485d2b"},"execution_count":null,"cell_type":"code","source":"\ntrain_x = list(training_new[:,0])","outputs":[]},{"metadata":{"_uuid":"3fee1ea472ad1abe00e42267098e8b0f9a7c9de8","_cell_guid":"1c003286-8706-4b8a-b2ce-804794c56ca0"},"execution_count":null,"cell_type":"code","source":"import tensorflow as tf\nimport tflearn\n# reset underlying graph data\ntf.reset_default_graph()\n# Build neural network\nnet = tflearn.input_data(shape=[None, len(train_x[0])])\nnet = tflearn.fully_connected(net, 8)\nnet = tflearn.fully_connected(net, 8)\nnet = tflearn.fully_connected(net, len(train_y[0]), activation='softmax')\nnet = tflearn.regression(net)\n \n# Define model and setup tensorboard\nmodel = tflearn.DNN(net, tensorboard_dir='tflearn_logs')\n# Start training (apply gradient descent algorithm)\nmodel.fit(train_x, train_y, n_epoch=10, batch_size=8, show_metric=True)\nmodel.save('model.tflearn')","outputs":[]},{"metadata":{"_uuid":"617a9fdac01f97876d4e762285dc2a2aaabff07f","_cell_guid":"f97b7512-1b8f-4cfc-a942-5f00cbf8057e"},"execution_count":null,"cell_type":"code","source":"test = pd.read_csv('../input/test.csv')\ntest.head()","outputs":[]},{"metadata":{"_uuid":"ccacdb41132c8784c79a8831d2881249560aa74e","collapsed":true,"_cell_guid":"5ef9b0f5-cfe2-469a-b047-4ad5caf7baec"},"execution_count":null,"cell_type":"code","source":"test['tokens'] = [nl.word_tokenize(sentences) for sentences in test.text]","outputs":[]},{"metadata":{"_uuid":"1e2569285304f4549a2b385ad2ff84ceb4035713","collapsed":true,"_cell_guid":"02f2b972-2a87-41c2-b60a-07e690f8af7d"},"execution_count":null,"cell_type":"code","source":"testing = []\nfor index,item in test.iterrows():\n    onehot = []\n    token_words = [stemmer.stem(word) for word in item['tokens']]\n    for w in words:\n        onehot.append(1) if w in token_words else onehot.append(0)\n    \n    testing.append(onehot)","outputs":[]},{"metadata":{"_uuid":"e5f13522cd518f5d4d7b5e9772e64399a32cf1e4","collapsed":true,"_cell_guid":"c966c4d0-edd0-417f-9cdb-0ce72ff9ee49"},"execution_count":null,"cell_type":"code","source":"testing = list(np.array(testing))","outputs":[]},{"metadata":{"_uuid":"dd456b56b15eff249c5e9a6f7424b611bc518b5e","collapsed":true,"_cell_guid":"0c667b9d-264b-4f43-bb11-8c481f7d4789"},"execution_count":null,"cell_type":"code","source":"predicted = model.predict(X=testing)","outputs":[]},{"metadata":{"_uuid":"1f4b65bdfe038fcc67c8a7bc2b22e945b7cf652d","_cell_guid":"2cc66872-0087-4741-8260-7af7209a5528"},"execution_count":null,"cell_type":"code","source":"result_val = round(pd.DataFrame(predicted),6)\nresult_val.columns = [\"EAP\",\"HPL\",\"MWS\"]","outputs":[]},{"metadata":{"_uuid":"f921f80aaec15f10edd95e47f0d27b47d9556972","_cell_guid":"cb3bcc22-cdc6-4807-93c1-fa913e140dd7"},"execution_count":null,"cell_type":"code","source":"result = pd.DataFrame(columns=['id'])\nresult['id'] = test['id']","outputs":[]},{"metadata":{"_uuid":"a653074af5a617c0bbc54e7d126c392bd6a493f2","_cell_guid":"ae1175c7-2a8e-4bd6-ab85-2d2e4fdfb887"},"execution_count":null,"cell_type":"code","source":"result['EAP'] = result_val['EAP']\nresult['HPL'] = result_val['HPL']\nresult['MWS'] = result_val['MWS']","outputs":[]},{"metadata":{"_uuid":"2a5db004864ff1b8cbaf98c36f6a556b11f0a511","_cell_guid":"a182d263-1e27-4d54-9cf2-915ac6293830"},"execution_count":null,"cell_type":"code","source":"result.head()","outputs":[]},{"metadata":{"_uuid":"096bc5003560959198f4dbf14ab5edd1e27d992c","collapsed":true,"_cell_guid":"703e5561-7e46-4715-a26d-77e546da15a8"},"execution_count":null,"cell_type":"code","source":"","outputs":[]}],"nbformat":4}