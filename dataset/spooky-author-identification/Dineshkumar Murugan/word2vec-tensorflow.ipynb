{"nbformat_minor":1,"cells":[{"source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output.","outputs":[],"execution_count":null,"metadata":{"_uuid":"e5b7147a607eadc89778d8045cc4ff040036fd71","_cell_guid":"edbfce39-fa2a-4616-a8d4-87362749e8f4"},"cell_type":"code"},{"source":"train = pd.read_csv('../input/train.csv')\ntrain.head()","outputs":[],"execution_count":null,"metadata":{"_uuid":"d9e403e9ebb31423e8a8303ec5b645d8c90e92f0","_cell_guid":"a12c2147-d021-41c7-9de3-04e52c70c477"},"cell_type":"code"},{"source":"import nltk as nl\ntrain['tokens'] = [nl.word_tokenize(sentences) for sentences in train.text]\nwords = []\nfor item in train.tokens:\n    words.extend(item)\n\nstemmer = nl.stem.lancaster.LancasterStemmer()\nwords = [stemmer.stem(word) for word in words]\n\n\nfiltered_words = [word for word in words if word not in nl.corpus.stopwords.words('english')]\n\n\n\nimport gensim\n# let X be a list of tokenized texts (i.e. list of lists of tokens)\nmodel = gensim.models.Word2Vec(filtered_words, size=100)\nw2v = dict(zip(model.wv.index2word, model.wv.syn0))","outputs":[],"execution_count":null,"metadata":{"_uuid":"3e066e8c2827e3c690314c124f1cce3086817980","_cell_guid":"75898742-f35d-4d55-a9ef-45d145de1c88"},"cell_type":"code"},{"source":"i = 0\nfor index,item in train.iterrows():\n    if(i < len(item['tokens'])):\n        i = len(item['tokens'])\nprint(i)","outputs":[],"execution_count":null,"metadata":{"_uuid":"eea842f492662909bb6a584a802a1fba82b4cf20","_cell_guid":"3300183c-6963-4ea8-a231-869abdcf7932"},"cell_type":"code"},{"source":"training = []\ni = 0\nfor index,item in train.iterrows():\n    vec = np.zeros(100)\n    token_words = [stemmer.stem(word) for word in item['tokens']]\n    token_words = [word for word in token_words if word not in nl.corpus.stopwords.words('english')]\n    for w in token_words:\n        if w in w2v:\n            vec += w2v[w]\n    norm = np.linalg.norm(vec)\n    if norm != 0:\n        vec /= np.linalg.norm(vec)\n    \n    \n    training.append([vec,item['author']])","outputs":[],"execution_count":null,"metadata":{"_uuid":"2e1435c03a40c5fe0235916b3ad86ca1807baf72","_cell_guid":"370a0bbf-0e11-4e63-b3d3-37df8753290d"},"cell_type":"code"},{"source":"training_new = np.array(training)","outputs":[],"execution_count":null,"metadata":{"collapsed":true,"_uuid":"ff3c22c35aa3e3750ae9d0e2325ac13d72891a59","_cell_guid":"9c2a2c3b-5fca-4ff9-9f54-f72c8d43a856"},"cell_type":"code"},{"source":"from numpy import array\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import OneHotEncoder\n\n# integer encode\nlabel_encoder = LabelEncoder()\ninteger_encoded = label_encoder.fit_transform(training_new[:,1])\n\n# binary encode\nonehot_encoder = OneHotEncoder(sparse=False)\ninteger_encoded = integer_encoded.reshape(len(integer_encoded), 1)\nonehot_encoded = onehot_encoder.fit_transform(integer_encoded)\n\ntrain_y = onehot_encoded","outputs":[],"execution_count":null,"metadata":{"collapsed":true,"_uuid":"d833100c1b53eede1421cec6eed956a801c2f9de","_cell_guid":"61c0e064-2231-4541-b2ba-05ea335c850c"},"cell_type":"code"},{"source":"train_x = list(training_new[:,0])\n","outputs":[],"execution_count":null,"metadata":{"collapsed":true,"_uuid":"a629b7311fce00d00f4d31e6a728d9bf67b38ca3","_cell_guid":"b582166f-7cd2-4cae-89f0-2a107604fcf1"},"cell_type":"code"},{"source":"import tensorflow as tf\nimport tflearn\n# reset underlying graph data\ntf.reset_default_graph()\n# Build neural network\nnet = tflearn.input_data(shape=[None, len(train_x[0])])\nnet = tflearn.fully_connected(net, 8)\nnet = tflearn.fully_connected(net, 8)\nnet = tflearn.fully_connected(net, len(train_y[0]), activation='softmax')\nnet = tflearn.regression(net)\n \n# Define model and setup tensorboard\nmodel = tflearn.DNN(net, tensorboard_dir='tflearn_logs')\n# Start training (apply gradient descent algorithm)\nmodel.fit(train_x, train_y, n_epoch=10, batch_size=8, show_metric=True)\nmodel.save('model.tflearn')","outputs":[],"execution_count":null,"metadata":{"_uuid":"3c0242f2c05d4d8468a4b2e28c724edba16a5688","_cell_guid":"acf6ef90-839c-4079-818f-d221ea7ccfde"},"cell_type":"code"},{"source":"test = pd.read_csv('../input/test.csv')\ntest.head()","outputs":[],"execution_count":null,"metadata":{"_uuid":"e57c9859e4a2662d0c2a51c63d7b18f3873bfbb6","_cell_guid":"528f3b95-b24c-4cf0-9b97-9b245164331c"},"cell_type":"code"},{"source":"test['tokens'] = [nl.word_tokenize(sentences) for sentences in test.text]\n","outputs":[],"execution_count":null,"metadata":{"collapsed":true,"_uuid":"0b589ae71f2fa24084a6f7147624d80d652717c7","_cell_guid":"cdd0f422-535f-4f6d-97d4-2444c39c455c"},"cell_type":"code"},{"source":"testing = []\nfor index,item in test.iterrows():\n    vec = np.zeros(100)\n    token_words = [stemmer.stem(word) for word in item['tokens']]\n    token_words = [word for word in token_words if word not in nl.corpus.stopwords.words('english')]\n    for w in token_words:\n        if w in w2v:\n            vec += w2v[w]\n    norm = np.linalg.norm(vec)\n    if norm != 0:\n        vec /= np.linalg.norm(vec)\n    \n    \n    testing.append(vec)","outputs":[],"execution_count":null,"metadata":{"collapsed":true,"_uuid":"d567109523a2ab77d04b170c571e015248e98d1f","_cell_guid":"69666c23-393b-4c15-8c9d-296bbfc49f4f"},"cell_type":"code"},{"source":"testing = list(np.array(testing))\npredicted = model.predict(X=testing)\nresult_val = pd.DataFrame(predicted)\nresult_val.columns = [\"EAP\",\"HPL\",\"MWS\"]\nresult = pd.DataFrame(columns=['id'])\nresult['id'] = test['id']\nresult['EAP'] = result_val['EAP']\nresult['HPL'] = result_val['HPL']\nresult['MWS'] = result_val['MWS']\nresult.head()","outputs":[],"execution_count":null,"metadata":{"_uuid":"f63c5cde89989e92b98a0bf66d2d4108a4ba7cb6","_cell_guid":"dbf489be-6225-4a41-9bb7-a3bf19e7b077"},"cell_type":"code"},{"source":"","outputs":[],"execution_count":null,"metadata":{"collapsed":true,"_uuid":"ed72f7ea81f40dd057bde7830f104136f330f535","_cell_guid":"5eb6f709-5d26-4d21-bbfa-21f54a896477"},"cell_type":"code"}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","pygments_lexer":"ipython3","mimetype":"text/x-python","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"nbconvert_exporter":"python","version":"3.6.3"}},"nbformat":4}