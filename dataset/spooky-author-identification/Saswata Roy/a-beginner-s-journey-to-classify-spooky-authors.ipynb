{"cells":[{"metadata":{"_uuid":"969b21b25098032441c1248c13d6af766f972fcd"},"cell_type":"markdown","source":"##### Problem:\nIn this year's Halloween playground competition, you're challenged to predict the author of excerpts from horror stories by Edgar Allan Poe, Mary Shelley, and HP Lovecraft"},{"metadata":{"_uuid":"2107bb5510c92a7178c3e166a317b1e95cb6a6c0"},"cell_type":"markdown","source":"# Import packages"},{"metadata":{"ExecuteTime":{"end_time":"2019-02-04T15:58:47.564083Z","start_time":"2019-02-04T15:58:47.552692Z"},"trusted":true,"_uuid":"6a8244ca1cf3b2bce845d0c17801f8c9d0990db3"},"cell_type":"code","source":"import re\nimport pandas as pd\nimport string\nimport multiprocessing\nfrom nltk.corpus import stopwords\nfrom flashtext.keyword import KeywordProcessor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\nimport nltk\n# libraries for dataset preparation, feature engineering, model training \nfrom sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn import decomposition, ensemble\n\n#import pandas, xgboost, numpy, textblob, string\nfrom keras.preprocessing import text, sequence\nfrom keras import layers, models, optimizers\n\nnltk.download('stopwords')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6ea59367fedc92a92fbf6da0d70d9b67d91ce306"},"cell_type":"markdown","source":"# Import Dataset"},{"metadata":{"ExecuteTime":{"end_time":"2019-02-04T16:09:35.528699Z","start_time":"2019-02-04T16:09:35.434124Z"},"trusted":true,"_uuid":"398adb27caf642f51c092126c3139686e14f2b77"},"cell_type":"code","source":"df=pd.read_csv('../input/train.csv',encoding='utf-8')\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fe9f683f7214bc4e791c5039088c720dc3f2430e"},"cell_type":"markdown","source":"## Check the different values available for the column 'author'"},{"metadata":{"ExecuteTime":{"end_time":"2019-02-04T16:09:35.695882Z","start_time":"2019-02-04T16:09:35.684342Z"},"trusted":true,"_uuid":"acdb620f462d0c3ad646ea5c99160da11514f0d1"},"cell_type":"code","source":"df['author'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e9054ac4252827ec5bd54de036217602af9f4e3d"},"cell_type":"markdown","source":"# Preprocessing"},{"metadata":{"ExecuteTime":{"end_time":"2019-02-04T16:09:37.872413Z","start_time":"2019-02-04T16:09:37.848419Z"},"trusted":true,"_uuid":"802b3e15c6c65e1688a5b3ff7f437d03df86b8c3"},"cell_type":"code","source":"# Define a function for removing regex\ndef regex_filtering(text):\n        if text:\n            #removing all email metadata fix it for email terms only\n            text=re.sub(r\"^(sender|to|copy|from|sent|subject|date|cc|e|von|datum|an|importance|bcc):.*$\",\" \",text,flags=re.M)\n            #removing all mail ids\n            text=re.sub(r\"\\S*@\\S*\\s?\",\" \",text)\n            #removing all links\n            text=re.sub(r\"(((https?|ftp|file):\\/\\/)|www\\\\.)\\\\S+\", ' ', text, flags=re.MULTILINE)\n            text=re.sub(r\"\\w*\\.\\w{1,4}\", '', text, flags=re.MULTILINE)\n            #removing all non word character\n            text=re.sub(r\"([^a-zA-Z0-9\\\\u00C0-\\\\u00FF@]|[Ã£Ã¢])+\",' ',text)\n            #removing words with numbers \n            text=re.sub(r'\\w*\\d\\w*', ' ', text)\n            #removing single characters\n            text=re.sub(r'\\b\\S{1}\\s+',' ',text)\n            #removing words with repeating characters\n            text=re.sub(r'\\b(\\w)\\1{1,}\\s+',' ',text)\n            #removing punkt\n            text = text.translate(str.maketrans('','',string.punctuation))\n            #removing extra whitespace\n            text=re.sub(r\"\\s\\s+\",' ',text)\n            #removing repeating words\n            text=re.sub(r\"(\\w+\\s+)\\1{1,}\",' ',text)\n            #removing whitespaces\n            text=text.strip()\n            return text","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2019-02-04T16:09:38.590202Z","start_time":"2019-02-04T16:09:38.583706Z"},"trusted":true,"_uuid":"94349b35dd4caf7cf3d3e02ec28af32e1ad30374"},"cell_type":"code","source":"#Tokenize Terms and remove stopwords\ndef tokenize_term(x):\n        predefined_stopwords='horror perfectly'\n        english_stopwords=stopwords.words(\"english\")\n        german_stopwords=stopwords.words(\"german\")\n        stopwords_list=(list(predefined_stopwords.split(' '))+english_stopwords+german_stopwords)         \n        keyword_processor_stopwords = KeywordProcessor()\n        for each in stopwords_list:\n            keyword_processor_stopwords.add_keyword(each,' ')   \n        sentence=keyword_processor_stopwords.replace_keywords(x)\n        return sentence.strip()","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2019-02-04T16:09:38.990639Z","start_time":"2019-02-04T16:09:38.986352Z"},"trusted":true,"_uuid":"1556c344fdc8a08a06dcf942fe0de08651bdd401"},"cell_type":"code","source":"#Combined function\ndef preprocess(text):\n    return regex_filtering(tokenize_term(text))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2fe3d50094899d8800ba950068f3ecfd7e2ccdce"},"cell_type":"markdown","source":"## We'll check the preprocessing steps for a single text"},{"metadata":{"ExecuteTime":{"end_time":"2019-02-04T16:09:42.049179Z","start_time":"2019-02-04T16:09:42.033958Z"},"trusted":true,"_uuid":"ac3f0e612785f06a39fcbec51426c0a5d950310a"},"cell_type":"code","source":"x=df['text'][0]\nprint('Before processing: '+x+'\\n')\ny=preprocess(x)\nprint('After processing: '+y)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a80d0d9a6c8b5b459d49086d7f236778e0261946"},"cell_type":"markdown","source":"### Now we'll remove all the null and empty texts from dataframe"},{"metadata":{"ExecuteTime":{"end_time":"2019-02-04T16:09:43.795267Z","start_time":"2019-02-04T16:09:43.766707Z"},"trusted":true,"_uuid":"37b7d801aec6c5c60c092bcb3e1cc14a7aef42cd"},"cell_type":"code","source":"print('Before removing null/empty records from data, size of dataframe is {}'.format(df.shape))\ndf=df.loc[~df['text'].isnull()]\ndf=df.loc[df['text']!='']\ndf=df.loc[~df['author'].isnull()]\ndf=df.loc[df['author']!='']\nprint('After removing null/empty records from data, size of dataframe is {}'.format(df.shape))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c808fc7046a7d840e320a470db3f46dff4f8199e"},"cell_type":"markdown","source":"#### As can be seen , there were no text records present"},{"metadata":{"ExecuteTime":{"end_time":"2019-02-04T16:13:16.981144Z","start_time":"2019-02-04T16:12:38.332406Z"},"scrolled":true,"trusted":true,"_uuid":"c53a1464080b29be8b64d42cbead0b267045671d"},"cell_type":"code","source":"df['text_preprocessed']=df['text'].apply(preprocess)\ndf","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2019-02-04T16:11:13.256421Z","start_time":"2019-02-04T16:11:13.249763Z"},"_uuid":"85f2f29effcdf574917ebf3862ff8b4a824e62d4"},"cell_type":"markdown","source":"## Now we have to remove the empty processed text rows "},{"metadata":{"ExecuteTime":{"end_time":"2019-02-04T16:14:23.468978Z","start_time":"2019-02-04T16:14:23.442894Z"},"trusted":true,"_uuid":"4339a06c5a5f23006162870284253ac347f0be32"},"cell_type":"code","source":"print('Before removing null/empty preprocessed texts from data, size of dataframe is {}'.format(df.shape))\ndf=df.loc[~df['text_preprocessed'].isnull()]\ndf=df.loc[df['text_preprocessed']!='']\nprint('After removing null/empty preprocessed texts from data, size of dataframe is {}'.format(df.shape))","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2019-02-04T16:14:56.576416Z","start_time":"2019-02-04T16:14:56.568563Z"},"_uuid":"1cef118dac4cc8dc8cd38a3f0c8212417772cc2d"},"cell_type":"markdown","source":"#### We saw seven records were removed which were either null or empty after the preprocessing"},{"metadata":{"ExecuteTime":{"end_time":"2019-02-04T16:15:39.357733Z","start_time":"2019-02-04T16:15:39.325666Z"},"trusted":true,"_uuid":"7b0b2937fe05407149e92de4d8dc9f9e1b95f8f9"},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e01e429335ee2083d844b3a926238cca4f065046"},"cell_type":"code","source":"#Lemmatize different forms of words\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.tokenize import word_tokenize \n\nwordnet_lemmatizer = WordNetLemmatizer() \nlemmatized = [[wordnet_lemmatizer.lemmatize(word,pos='v') for word in word_tokenize(s)]\n              for s in df['text_preprocessed']]\ndf['text_lemmatized']=[\" \".join(i) for i in lemmatized]\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4510a7f28a41b186cd5c35bdb4af68943c7d2472"},"cell_type":"code","source":"texts=df['text_lemmatized'].values\nlabels=df['author'].values\ndata = list(labels + '\\t' + texts)\nprint('Total count of unique labels : {} \\n Authors are : {}'.format(len(set(labels)),df['author'].value_counts()))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ec3056e62b39cae9875d3fee5422f9a74e2438e3"},"cell_type":"markdown","source":"### Splitting Data into train and test set"},{"metadata":{"ExecuteTime":{"end_time":"2019-02-04T16:16:11.09598Z","start_time":"2019-02-04T16:16:10.946077Z"},"trusted":true,"_uuid":"fcb2173e3e3e5cfebbc8b43b5cb1ce567b50699e"},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(texts, labels, test_size=0.1, random_state=0)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"eb9b6d92a5bfbed2cef53c401b37ce46d25dbeb2"},"cell_type":"markdown","source":"## Generate Tf-Idf vectorizer with different params"},{"metadata":{"trusted":true,"_uuid":"08bc6ba910922327093bb933551714c94c74fdde"},"cell_type":"code","source":"# word level tf-idf\ntfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=5000)\ntfidf_vect.fit(texts)\nX_train_tfidf =  tfidf_vect.transform(X_train)\nX_test_tfidf =  tfidf_vect.transform(X_test)\n\n# ngram level tf-idf \ntfidf_vect_ngram = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(2,3), max_features=5000)\ntfidf_vect_ngram.fit(texts)\nX_train_tfidf_ngram =  tfidf_vect_ngram.transform(X_train)\nX_test_tfidf_ngram =  tfidf_vect_ngram.transform(X_test)\n\n# characters level tf-idf\ntfidf_vect_ngram_chars = TfidfVectorizer(analyzer='char', token_pattern=r'\\w{1,}', ngram_range=(2,3), max_features=5000)\ntfidf_vect_ngram_chars.fit(texts)\nX_train_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(X_train) \nX_test_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(X_test) ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9b4a7df950e1a26c9ea398271d20722e63930d70"},"cell_type":"markdown","source":"### Create a function to generate accuracy dynamically"},{"metadata":{"trusted":true,"_uuid":"56cd6d215989434edfa2c6e4dedbbf022d7cdb06"},"cell_type":"code","source":"def train_model(classifier, feature_vector_train, label, feature_vector_valid, is_neural_net=False):\n    # fit the training dataset on the classifier\n    classifier.fit(feature_vector_train, label)\n    \n    # predict the labels on validation dataset\n    predictions = classifier.predict(feature_vector_valid)\n    \n    if is_neural_net:\n        predictions = predictions.argmax(axis=-1)\n    \n    return metrics.accuracy_score(predictions, y_test)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c4c74cba2a6b313f10262b4f9bcf0bd38672292c"},"cell_type":"markdown","source":"### Let's try Naive-Bayes classifier first \n\nImplementing a naive bayes model using sklearn implementation with different features\n\nNaive Bayes is a classification technique based on Bayes’ Theorem with an assumption of independence among predictors. A Naive Bayes classifier assumes that the presence of a particular feature in a class is unrelated to the presence of any other feature here ."},{"metadata":{"trusted":true,"_uuid":"5c57e343d966fc0be440bea2f1f2fe069eb15ba7"},"cell_type":"code","source":"# Naive Bayes on Word Level TF IDF Vectors\naccuracy = train_model(naive_bayes.MultinomialNB(), X_train_tfidf, y_train, X_test_tfidf)\nprint(\"Naive Bayes, WordLevel TF-IDF: {}\".format(accuracy))\n\n# Naive Bayes on Ngram Level TF IDF Vectors\naccuracy = train_model(naive_bayes.MultinomialNB(), X_train_tfidf_ngram, y_train, X_test_tfidf_ngram)\nprint(\"Naive Bayes,  N-Gram Vectors: {}\".format(accuracy))\n\n# Naive Bayes on Character Level TF IDF Vectors\naccuracy = train_model(naive_bayes.MultinomialNB(), X_train_tfidf_ngram_chars, y_train, X_test_tfidf_ngram_chars)\nprint(\"Naive Bayes,CharLevel Vectors: {}\".format(accuracy))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"75ad24f082935406d1c8a61bd50e5b8fdade1665"},"cell_type":"markdown","source":"### Now, we will be trying out linear classifiers with all three types of tf-idf vectorizers\n\nLogistic regression measures the relationship between the categorical dependent variable and one or more independent variables by estimating probabilities using a logistic/sigmoid function. "},{"metadata":{"trusted":true,"_uuid":"d9011828640f44e3e132a3e81b3d8b472d8a4ad8"},"cell_type":"code","source":"# Linear Classifier on Word Level TF IDF Vectors\naccuracy = train_model(linear_model.LogisticRegression(), X_train_tfidf, y_train, X_test_tfidf)\nprint (\"Logistic Regression, WordLevel TF-IDF: \"+ str(accuracy))\n\n# Linear Classifier on Ngram Level TF IDF Vectors\naccuracy = train_model(linear_model.LogisticRegression(), X_train_tfidf_ngram, y_train, X_test_tfidf_ngram)\nprint(\"Logistic Regression, N-Gram Vectors: \"+ str(accuracy))\n\n# Linear Classifier on Character Level TF IDF Vectors\naccuracy = train_model(linear_model.LogisticRegression(), X_train_tfidf_ngram_chars, y_train, X_test_tfidf_ngram_chars)\nprint(\"Logistic Regression, CharLevel Vectors: \"+ str(accuracy))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"97ed5a1ec21df78c552ebf516c449f556a145211"},"cell_type":"markdown","source":"### Let us try SVC now\n\nSupport Vector Machine (SVM) is a supervised machine learning algorithm which can be used for both \nclassification or regression challenges. The model extracts a best possible hyper-plane / line that segregates the two classes. "},{"metadata":{"trusted":true,"_uuid":"84bfc7d715c6f22dc50cacc0fcfe2de8017425da"},"cell_type":"code","source":"# SVM on word Level TF IDF Vectors\naccuracy = train_model(svm.SVC(),X_train_tfidf, y_train, X_test_tfidf)\nprint(\"SVM, N-Gram Vectors: {}\".format(accuracy))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"be9eda12d401e3d033a70242072eb1ff2eb6615c"},"cell_type":"markdown","source":"### Using Random Forest \nRandom Forest models are a type of ensemble models, particularly bagging models. They are part of the tree based model family."},{"metadata":{"trusted":true,"_uuid":"2a674f4186396ed31f0d1ddcf53e0adcc7bbf00f"},"cell_type":"code","source":"# RF on Word Level TF IDF Vectors\naccuracy = train_model(ensemble.RandomForestClassifier(), X_train_tfidf, y_train, X_test_tfidf)\nprint(\"Random Forest, WordLevel TF-IDF: {}\".format(accuracy))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"93077574b9e8cbdce1ac53f73981fa85696eb528"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"82b6ce9f4549290fb8ba53f3c5d0dc0a93061a62"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"anaconda-cloud":{},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{},"toc_section_display":true,"toc_window_display":false},"varInspector":{"cols":{"lenName":16,"lenType":16,"lenVar":40},"kernels_config":{"python":{"delete_cmd_postfix":"","delete_cmd_prefix":"del ","library":"var_list.py","varRefreshCmd":"print(var_dic_list())"},"r":{"delete_cmd_postfix":") ","delete_cmd_prefix":"rm(","library":"var_list.r","varRefreshCmd":"cat(var_dic_list()) "}},"types_to_exclude":["module","function","builtin_function_or_method","instance","_Feature"],"window_display":false}},"nbformat":4,"nbformat_minor":1}