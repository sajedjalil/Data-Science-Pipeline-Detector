{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"version":3,"name":"ipython"},"version":"3.6.3","mimetype":"text/x-python","name":"python","file_extension":".py","pygments_lexer":"ipython3","nbconvert_exporter":"python"}},"nbformat":4,"nbformat_minor":1,"cells":[{"metadata":{"_uuid":"2191454f5b0045b06960bcab8a10a1c028c43592","_cell_guid":"48f4df09-ea68-4262-bc08-a4ca13b0623e"},"source":"# Simple solution (Naive Bayes)\n\nSimple ideas are often good ones! In this kernel you will discover how we can\nproduce a good model with only a few lines of code.","cell_type":"markdown"},{"metadata":{"_uuid":"f7cd5e3ca8aa50e7b83e11d1f888a44efbbbd1b7","_cell_guid":"282ae0d4-4999-4939-9302-6e686e7e3d35","collapsed":true},"execution_count":null,"outputs":[],"source":"import pandas as pd\nimport numpy as np\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.preprocessing import LabelBinarizer\nfrom sklearn.metrics import confusion_matrix, accuracy_score, log_loss\n\n%matplotlib inline\nmatplotlib.style.use('ggplot')","cell_type":"code"},{"metadata":{"_uuid":"aa39b75bae0350e826bc1dc4e98e7d33fed40f33","_cell_guid":"f44cee35-0404-4cb0-8309-ff8fd09d9488","collapsed":true},"execution_count":null,"outputs":[],"source":"df_train = pd.read_csv('../input/train.csv')\ndf_test = pd.read_csv('../input/test.csv')\n\nauthors = df_train['author'].unique()\nprint('Authors are:', authors)","cell_type":"code"},{"metadata":{"_uuid":"05c304e388b1d800b53f65400c0186704fbeeb7a","_cell_guid":"a923b53b-5675-4107-9097-99c3cdf7405e"},"source":"We just loaded our datasets, now let's split our data in two buckets: a train set (90% of the original data) and a test set (10% of the data).","cell_type":"markdown"},{"metadata":{"_uuid":"6695d154eafbc1a42e22da34c07091af018fd9db","_cell_guid":"bd73127f-1d72-4f46-9c86-30e37fa40328","collapsed":true},"execution_count":null,"outputs":[],"source":"X_train, X_test, y_train, y_test = train_test_split(\n    df_train['text'].values,\n    df_train['author'].values,\n    test_size=0.2,\n    random_state=42\n)","cell_type":"code"},{"metadata":{"_uuid":"7b6cc8893f58c182a0f4dc98adc52b60500431e2","_cell_guid":"0ae1586d-6c2e-469c-8eb8-d6edeb55a990"},"source":"Now that we are all set, we can train our model.\n\nWe are going to use a technique named Multinomial Naive Bayes.\n\nIt will automatically train a model by extracting the most used words for each authors and define a metric to classify new examples.\n\nLet's see how it goes.","cell_type":"markdown"},{"metadata":{"_uuid":"965cdaca789451ff30f1f4cbf0ce9e80ab7f95f8","_cell_guid":"e53038cb-bc31-4985-95db-e74cc47d30e8","collapsed":true},"execution_count":null,"outputs":[],"source":"vectorizer = CountVectorizer()\ncounts = vectorizer.fit_transform(X_train)\n\nclassifier = MultinomialNB()\nclassifier.fit(counts, y_train)","cell_type":"code"},{"metadata":{"_uuid":"232206f8fe151cc18243a88ce2de8eb031f078eb","_cell_guid":"befb49af-9ff6-433a-bd70-f1e1561906cf"},"source":"Done. That was simple, isn't it?\n\nWe have now a working solution, but how does it perform?","cell_type":"markdown"},{"metadata":{"_uuid":"3736ee56c8d8ff2eca77ce1250134ec61506cf05","_cell_guid":"483a828c-7219-4c6e-80fe-fb30f34045c3","collapsed":true},"execution_count":null,"outputs":[],"source":"examples = ['How peculiar!', \"That is a monster!\", \"the old man hadn't much time\"]\nexample_counts = vectorizer.transform(examples)\npred = classifier.predict(example_counts)\npred","cell_type":"code"},{"metadata":{"_uuid":"60e92cca9ac79b99c3d1c6603a02adfd941cd7c1","_cell_guid":"493e1b16-296f-4223-b4a8-f3f18926197c","collapsed":true},"execution_count":null,"outputs":[],"source":"test_counts = vectorizer.transform(X_test)\ny_pred = classifier.predict(test_counts)\n\naccu = accuracy_score(y_test, y_pred)\nprint(\"Accuracy: %.02lf\" % (100.*accu))","cell_type":"code"},{"metadata":{"_uuid":"203cd7bc05726f3f437426a362f59dd83bbd5d81","_cell_guid":"52d448f9-3fd3-4f1c-9d15-b1a6c4179968","collapsed":true},"execution_count":null,"outputs":[],"source":"y_pred_proba = classifier.predict_proba(test_counts)\ny_label = LabelBinarizer().fit_transform(y_test)\nloss = log_loss(y_label, y_pred_proba)\n\nprint(\"Log-loss: %.04lf\" % loss)","cell_type":"code"},{"metadata":{"_uuid":"2847bb058bb3820c579982028c3e5d3a05480820","_cell_guid":"2810bf85-daf1-4652-9b21-ba26d258bb64","collapsed":true},"execution_count":null,"outputs":[],"source":"conf = confusion_matrix(y_test, y_pred)\nconf = pd.DataFrame(\n    conf.astype(np.float)/conf.sum(axis=1),\n    index=authors,\n    columns=authors\n)\n\nplt.figure()\nplt.title('Confusion matrix of the predictions')\ncmap = sns.cubehelix_palette(as_cmap=True)\nsns.heatmap(conf, annot=True, cmap=cmap)\nplt.xlabel('Predicted label')\nplt.ylabel('True label')\nplt.show()","cell_type":"code"},{"metadata":{"_uuid":"92d24ea2f69640b48f3949daa7b75819c624612c","_cell_guid":"39878e53-bc14-44f8-8a5d-ef6817455610"},"source":"Finally we just create a new DataFrame with Pandas and we just get the result ready to send to Kaggle.\nHave fun!","cell_type":"markdown"},{"metadata":{"_uuid":"d39d574747d93bc9e831971bd6e76d7926125362","_cell_guid":"fd9d9abe-6a21-41ba-a6c1-c5a568d5f1ee","collapsed":true},"execution_count":null,"outputs":[],"source":"final_counts = vectorizer.transform(df_test['text'])\nresult = pd.DataFrame(classifier.predict_proba(final_counts), columns=authors)\nresult.insert(0, 'id', df_test['id'])\nresult.to_csv('kaggle_solution.csv', index=False, float_format='%.15f')","cell_type":"code"}]}