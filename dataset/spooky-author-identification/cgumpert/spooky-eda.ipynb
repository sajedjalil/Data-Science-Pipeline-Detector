{"nbformat_minor":1,"cells":[{"cell_type":"markdown","source":"Last updated: 31.10.2017\n# Spooky NLP problem\nRecently I started looking into ML algorithms for doing NLP tasks, mostly information retrieval, topic modelling and content classification. Therefore, I find this competition very interesting. Especially the objective - estimate the author from a snippet of text - is quite appealing because this focuses more on _style_ (choice of words, sentence structure etc) than on the actual content. Something quite different from the problems I am usually working on.\nOk, enough talking. Let's get started. In this kernel I will touch on the following topics (with a varying level of detail):\n- label encoding\n- text representation\n- sentence and word tokenization\n- stopword removal\n- stemming and lemmatizing\n- cross validation\n\nWhile doing this, I also try to give a brief introduction to the following python packages. This might be interesting for people who want to get started on doing NLP in python.\n- nltk\n- textblob\n- pandas\n- seaborn\n\nThese lists are evolving and will grow with future updates of this notebook.\n\nEnjoy reading and please comment! Feedback is very much appreciated!\n\nContents\n1. [Explorative Data Analysis](#Explorative-Data-Analysis)\n    1. [Required packages](#Required-packages)\n    1. [The training data](#The-training-data)\n    1. [Target label encoding](#Target-label-encoding)\n    1. [Training set balance](#Training-set-balance)\n    1. [Sentences](#Sentences)\n    1. [Vocabulary](#Vocabulary)\n    1. [Special characters](#Special-characters)\n1. [Reference model](#Reference-model)","metadata":{"_uuid":"150b44193117cac4a33e3018683c8cbaeaf43ad9","_cell_guid":"1694712c-30ee-448d-88b6-1ef7b23a5089"}},{"cell_type":"markdown","source":"## Explorative Data Analysis","metadata":{"_uuid":"dde4ad5340faab597737be19da2b9ef535bd36e3","_cell_guid":"1390e8ed-7bb7-47c9-a77b-35ac8592f90b"}},{"cell_type":"markdown","source":"### Required packages\nImport the most commonly used python packages:\n- [matplotlib.pyplot](http://matplotlib.org/api/pyplot_api.html) for basic plotting and figure styling\n- [numpy](http://www.numpy.org/)  is a powerful library for handling N-dimensional arrays, performing linear algebra operations, vectorizing transformations, random number generation and more.  I am going to use only very basic functionality of this powerful package in this kernel.\n- [pandas](http://pandas.pydata.org/) is the de-facto standard for handling datasets in the python data science community. It is so feature-rich that it is hard to describe in a sentence. If I had to, I would probably go with: _\"A python library for handling tabular/labelled data supporting the usual SQL-like operations (grouping, aggregating, transforming, NA handling...), add some fancy time series functionality, plus some easy-but-still-appealing visualisations and a superb documentation.\"_\n- [seaborn](http://seaborn.pydata.org/) a great visualisation library built on-top of `matplotlib` with a concise API geared towards statistical plots.\n- [sklearn](http://scikit-learn.org/stable/index.html) I can't  write a short intro without mentioning `scikit-learn`. In my opinion, this is the most commonly used python package for doing ML. It comes with heaps of standard supervised classification and regression algorithms as well as unsupervised clusterisation algorithms. In addition, it provides very neat preprocessing functionality and has many evaluation metrics defined. I will make heavy use of this package in this kernel. However, for specialized tasks you often find better suited/more performant libraries (e.g. xgboost or keras to mention two other common suspects on kaggle).\n- [nltk](http://www.nltk.org/) I would say _the_ standard library for doing Natural Language Processing in education and research. It is very modular and comes with a lot of algorithms and configuration options which leads to a rather steep learning curve.\n- [textblob](https://textblob.readthedocs.io/en/dev/) is built on-top of `nltk` and provides a more easily-accessible interface. If you don't need highly optimized performance, this might be your best starting point.\n\nFor a more detailed disucssion on python NLP packages, I would recommend you reading this \n[article](https://elitedatascience.com/python-nlp-libraries).\n","metadata":{"_uuid":"f67c6aa8eef4d816c8b64680b0cc543816b4cf1b","_cell_guid":"0034132a-6d4b-4a02-9cd1-7cd2e7791b99"}},{"cell_type":"code","outputs":[],"source":"import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport sklearn as sk\nimport nltk\nfrom textblob import TextBlob as TB, Word","execution_count":null,"metadata":{"_uuid":"e4f30ad5c8a74e5e2181d9a61d8af2e19444537e","collapsed":true,"_cell_guid":"f9642c07-a593-4d54-96ec-3cde121d0e0f"}},{"cell_type":"markdown","source":"### The training data\nLet's load the training data:","metadata":{"_uuid":"74ece3a2fd5aae037484ca23df57e28878cbc7e0","collapsed":true,"_cell_guid":"4c4b10db-6c13-44a6-98f1-af46fabc61a2"}},{"cell_type":"code","outputs":[],"source":"df = pd.read_csv(\"../input/train.csv\")\nprint('loaded %d samples with %d features' %(df.shape))","execution_count":null,"metadata":{"_uuid":"269fd898de122cd7a723298e08aa9426d5e653d6","_cell_guid":"83ad5a44-b2a9-41d5-8441-603b4b183ea1"}},{"cell_type":"markdown","source":"We can inspect the head and tail of our training set using `pandas.DataFrame`'s methods [head()](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.head.html) and [tail()](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.tail.html)","metadata":{"_uuid":"fa5d93d101ca7a25a2b43001263f56433eea3d55","_cell_guid":"d359fb95-05a6-49f0-ac77-393d45aaf5e4"}},{"cell_type":"code","outputs":[],"source":"print(df.head())\nprint(df.tail())","execution_count":null,"metadata":{"_uuid":"2d3d38d0111511351ac1a3b27cfe4c9bd8505eba","_cell_guid":"4a061f91-8a9c-4590-9973-494cccf0e36f"}},{"cell_type":"markdown","source":"As expected, we see that there is an ID which we need for the submission, some text snipped and the author.\n\n### Target label encoding\nThe author is the target label we want to predict in this competition. However, not all algorithms handle non-numeric data well. It is easy to construct a one-to-one mapping for author -> code and then use the numeric code as label. Because this task is quite common, there is a convience function in the `sklearn` package doing exactly this: [LabelEncoder](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html). Because we are going to need it later anyway, we can add a new column with these numeric labels.","metadata":{"_uuid":"f3789d1dd88638465f76be7f7dac74a512f4fb12","_cell_guid":"8a3f521d-c481-470d-a412-1c49be35739c"}},{"cell_type":"code","outputs":[],"source":"df['label'] = sk.preprocessing.LabelEncoder().fit_transform(df.author)\ndf.head()","execution_count":null,"metadata":{"_uuid":"4ca95e728ae5d0d2c068a88b190c6d2de0371678","_cell_guid":"1a0d4968-1975-4d01-b2cc-54b97448b31e"}},{"cell_type":"markdown","source":"We can quickly check that each author is mapped to exactly one label by grouping by the `author` column and counting how often different labels appear.","metadata":{"_uuid":"ab5e549f7fed6a7ad08aed57fcaa73879ad3b778","_cell_guid":"eab400a1-54ca-49b4-a049-fad6b8847a90"}},{"cell_type":"code","outputs":[],"source":"df.groupby('author').label.value_counts()","execution_count":null,"metadata":{"_uuid":"8e8519af61f47747bcfff16df4d6a5d0cdf5eaa5","_cell_guid":"580515b3-aa0b-42bd-93f0-3579c114896a"}},{"cell_type":"markdown","source":"As expected we see exactly one label per author. Selecting on `label == 0` is equal to selecting on `author == 'EAP'` and similarly for the other authors.\n\n### Training set balance\nLet's see how many samples we have per author. In general, the performance of supervised learning algorithms can be impacted negatively if you have a very imbalanced distribution of target labels.\n\nTo check the balance of the training set we group the samples by author and count how many texts we find in each group. Then we plot this as bar chart. Thanks to `pandas` slim API this is a one-liner:","metadata":{"_uuid":"1599bb35fc516eb0d2a4c05e7a629bcfaa044ca1","_cell_guid":"51e7a1cb-c405-41f3-b80e-576fb801d75c"}},{"cell_type":"code","outputs":[],"source":"df.groupby('author').text.count().plot.bar(title = 'training set balance');","execution_count":null,"metadata":{"_uuid":"736057831fbe9b3c676724175d2e46f7a706879a","_cell_guid":"09a0a4e0-fdca-479d-84ff-5639ee2c6260"}},{"cell_type":"markdown","source":"Things don't look too bad. We have more text snippets for Edgar while almost equally many for HP and Mary. The difference is not dramatic, but we will keep this in mind for later.\n\nAnother way of looking at this is: What does having a balanced training set mean? Having the same number of text snippets? Text snippets may vary in length, so the above comparison might not be the most useful. Two short text snippets may carry less information than one really long one. Instead of looking the raw number of training samples per author, one could compare different metrics. The most obvious to me are:\n- total number of sentences\n- total number of words\n- total number of characters\n\nThe last one is easy to get: just get the string length of each text (`len(text`) while for the other two one needs to split texts into sentences/words. At first glance, these seem to be trivial tasks (e.g. split on punctuation or white spaces respectively). However, it might be difficult to implement all the corner cases (e.g. punctuation inside quotations, apostrophes etc). Luckily we can employ the tokenizers of the `nltk` package which handle all these cases:\n- `nltk.sentence_tokenize(text)` will split the text into a list of sentences. Optionally, you can pass a `language` option to use special punctuation characters (e.g. Spanish). The default is English.\n- `nltk.word_tokenize(sentence)` will split the sentence in a list of words.\n\nFeel free to check the documentation of `nltk.tokenize` for other tokenization options.","metadata":{"_uuid":"234d4501fec92480920841eeb43f1f3bf51d7eb4","_cell_guid":"922b4cee-0af0-402a-b5a0-c61ca3d9a76c"}},{"cell_type":"code","outputs":[],"source":"# calculate different measures of the quantity of information\ndf['n_sentences'] = df.text.transform(lambda x: len(nltk.sent_tokenize(x)))\ndf['n_words'] = df.text.transform(lambda x: len(nltk.word_tokenize(x)))\ndf['text_len'] = df.text.transform(lambda x: len(x))","execution_count":null,"metadata":{"_uuid":"6216217115bcb75001464d50208ea2ec80b09cbf","collapsed":true,"_cell_guid":"864450b7-729c-4847-9b58-9c1995596a34"}},{"cell_type":"markdown","source":"Having calculated the the numbers above per text snippet, we need to group again by author and `sum()` them up (instead of counting as we did previously). The we compare the _amount of information_ per author again using bar plots. The key here is to pass the `subplots = True` option to the plotting function. Otherwise, all bars are drawn in one figure which is not optimal in this case due to the very different y-scales between the individual. For better readability, I split the one-line call voer many lines and added some inline comments.","metadata":{"_uuid":"7c4a53431772e440fdc6147f97cc38c59ef9bc64","_cell_guid":"1835d620-2377-4023-a790-a6da623dc82c"}},{"cell_type":"code","outputs":[],"source":"# first group by author\n(df.groupby('author')\n # select the columns we are interested in (note the list inside the []-operator)\n [['n_sentences','n_words','text_len']]\n # we calculate the sum for each column within each author group\n .sum()\n # finally, plot as bar chart in different figures\n .plot.bar(subplots = True, layout = (1,3), figsize = (18,6)));","execution_count":null,"metadata":{"_uuid":"f3f4476ee336e162d2934d788165e904ae98b5dd","_cell_guid":"47a8645d-ad54-482e-b3a6-54baf5817519"}},{"cell_type":"markdown","source":"Starting from the left, the total number of sentences per author gives roughly the same picture as already obtained from the number of text snippets (looking at the y-scale we can guess that most text snippets actually correspond to one sentence -> we can check this in a minute). Looking at the total number of words, the difference is still clearly visible but already reduced (EAP has about 30% more words than HPL). Considering the raw text length, the difference is even less pronounced (EAP has about 20% more characters in total than HPL).\n\nEven thought the initial picture did not change significantly, I think it was still worth checking it. In the end, we are still doing exploration ;-)\n\n### Sentences\nOk, let's quickly check the assumption that the text snippets are made out of individual sentences. We already calculated the number of sentences for each text snippet. So let's just look at the value counts of `n_sentences`.","metadata":{"_uuid":"e983b1d2c26e4a5ec55009699898859e99da8241","_cell_guid":"3b3f050a-ce76-436c-b3ce-d33947044f8c"}},{"cell_type":"code","outputs":[],"source":"print(df.n_sentences.value_counts())\ndf.n_sentences.plot.hist(log = True)\nprint('out of %d text snippets %d contain more than one sentence' %\n      (len(df), (df.n_sentences > 1).sum()))","execution_count":null,"metadata":{"_uuid":"01f04ec30cbb50ce0d29be6f338366ba122b5506","_cell_guid":"d19c1490-dbfc-4a5c-96cd-4387c8cf5f93"}},{"cell_type":"markdown","source":"Our assumption seems to be true for most of the samples. Nevertheless, there are 419 texts (about 2%) which can be splitted into multiple sentences, up to nine. Let's have a look at this peculiar text snippet.","metadata":{"_uuid":"e3d263e7b33609b26baf6826ffb7465566418c06","_cell_guid":"eb8ecb80-d17f-445e-aff1-8c26aec04af1"}},{"cell_type":"code","outputs":[],"source":"print(df[df.n_sentences == 9].text.iloc[0])","execution_count":null,"metadata":{"_uuid":"6f462d3cf5d395fc2cd1518eb0e9e1fe72fea9da","_cell_guid":"dc50245e-8500-4035-b190-fac4c13705c9"}},{"cell_type":"markdown","source":"Some serious bargaining happening here :-)\n\n### Vocabulary\nIf you are still with me, we should dare the next step and have  a look at the vocabulary. The objective sounds simple: _\"How often an author is using which word?\"_. Yet, we need to do some gymnastics to get this information nicely displayed. But I will walk you through this daunting section step by step.\n\n1. Determine the vocabulary (all words appearing in any of the texts).\n1. Count the number of occurences of each word (= _term_ in `sklearn` speak) for each document (yields the term-document matrix).\n1. Sum the term counts over all documents belonging to one author.\n1. Getting the most common words per author and plot them.\n\nSteps 1. and 2. can be done using the [CountVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) from `sklearn.feature_extraction.text`. This algorithms has many options for steering the text preprocessing, tokenization, limiting the vocabulary etc. Here we can use the default settings which will produce word counts per document in a way you would naively expect. Step 1. (determining the vocabulary) is done be calling the `fit` method while calculating term counts per document can be done by calling `transform` of the already fitted `CountVectorizer`. If the input to both methods is the same, you can combine those to steps and call `fit_transform` directly.","metadata":{"_uuid":"2952eb101dc8b8fd09e22c2bbf9f71174ce9dcf8","_cell_guid":"776f8aae-ff63-4925-99da-348673781e8b"}},{"cell_type":"code","outputs":[],"source":"# initialize count vectorizer\ncv = sk.feature_extraction.text.CountVectorizer()\n# learn vocabulary and calculate term-document frequncies in one go\nX = cv.fit_transform(df.text)\nprint('learned a vocabulary of size %d' % len(cv.vocabulary_))\nprint('first 5 terms in vocabulary (ordered alphabetically):')\nprint(cv.get_feature_names()[:5])\nprint('shape of term-document matrix is [n_samples, vocabulary_size]: ', X.shape)","execution_count":null,"metadata":{"_uuid":"72b5b20e4ce75328daea84180d7a96cf759642e5","_cell_guid":"8786aab1-fd98-4b47-8c2d-e47e573664af"}},{"cell_type":"markdown","source":"As a next step, we want to accumulate the word counts for all documents belonging to one author. This is, we want to sum over all rows (in `numpy` speak along `axis = 0`) but only include rows for a given author. Here we are going to use a trick: We will binarize the the author labels in a one-vs-all fashion using [label_binarize](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.label_binarize.html) from `sklearn.preprocessing`. As input we give the list of author labels and as a second argument the set of possible class labels (i.e. the three different author acronyms).","metadata":{"_uuid":"007783207194c82aa52cd3ecc7d0cdc1a9725b94","_cell_guid":"8bde05fa-570c-4cd2-a3f3-87f265378663"}},{"cell_type":"code","outputs":[],"source":"Y = sk.preprocessing.label_binarize(df.author, df.author.unique())\nprint('shape of Y: ',Y.shape)\nprint('The first 5 rows of Y:')\nprint(Y[:5,:])\nprint('class labels: ', df.author.unique())","execution_count":null,"metadata":{"_uuid":"5110afbc933ef4e2254c24e7634920fc8e756318","_cell_guid":"9c2aab63-d059-4b54-829d-fb0f5a50203b"}},{"cell_type":"markdown","source":"You can see that the resulting matrix has shape `[n_samples, n_classes]` where `n_classes` is the number of different class labels (3 different authors in our case). Every row contains exactly one 1 and otherwise only 0. A 1 in the first column means that this sample belongs to author `EAP`, a 1 in the second column corresponds to `HPL` and in the third column to `MWS`. If you remember a bit of linear algebra, you will figure out that summing the word counts per author can now be written as a simple dot-product `Y.T * X`.","metadata":{"_uuid":"a756ef13adddb9925395e8d267e008c2a3a4ad9e","_cell_guid":"f6ac2634-4656-4ad1-b579-d4ba71bbc64a"}},{"cell_type":"code","outputs":[],"source":"counts = Y.T * X\nprint('shape of counts is [n_classes, vocabulary_size]: ', counts.shape)","execution_count":null,"metadata":{"_uuid":"b0359a3ef5708c564abb34d21142f75b70948e7a","_cell_guid":"4b75f8fc-d4f2-4798-98d3-fd596ec093a7"}},{"cell_type":"markdown","source":"Now we have the full information about how often each word was used by each author. We put this back into a `pandas.DataFrame` to benefit from their great sorting and visualisation functionality. ","metadata":{"_uuid":"961d15c493c870e22a2ee151332f57d26235dccc","_cell_guid":"9c3871ec-f2fd-4d62-8e0f-140537aa3b00"}},{"cell_type":"code","outputs":[],"source":"count_df = pd.DataFrame(data = counts.T,\n                        columns = df.author.unique(),\n                        index = cv.get_feature_names())\ncount_df.head(10)","execution_count":null,"metadata":{"_uuid":"f9f312c08f52f4eb6bdc55f6cac69f338a15a046","_cell_guid":"517bbbef-0e18-4b3e-a9e4-dcd1b3a30f2c"}},{"cell_type":"markdown","source":"We can also make a bar plot showing the counts of the most frequent words easily using this dataframe.","metadata":{"_uuid":"691eec9e3eeace337119d455d3f29b1866ecdaad","_cell_guid":"a89066f4-65f6-4b1f-9372-c4b56fe14729"}},{"cell_type":"code","outputs":[],"source":"_, (ax1, ax2, ax3) = plt.subplots(ncols = 3, figsize = (16,6))\ntopn = 10\ncount_df.EAP.sort_values(ascending = False).iloc[topn::-1].plot.barh(title = 'EAP', ax = ax1)\ncount_df.HPL.sort_values(ascending = False).iloc[topn::-1].plot.barh(title = 'HPL', ax = ax2)\ncount_df.MWS.sort_values(ascending = False).iloc[topn::-1].plot.barh(title = 'MWS', ax = ax3);","execution_count":null,"metadata":{"_uuid":"737f01e46a99e52df19ca2669e4031183eb8d8f2","_cell_guid":"b21a3123-fa05-4e42-968e-55c4f2044467"}},{"cell_type":"markdown","source":"Well, this looks discouraging. All words appearing in the plots above don't carry much information. These are so-called stopwords. But we are lucky that `nltk` comes with predefined stopword lists. One must be careful using those since the definition of a stopword depends heavily on the specific task. For now we go with default list and see what we get.","metadata":{"_uuid":"dc2f22bb192f24aaff464346770ac0f24ae43381","_cell_guid":"5a7dc40a-cb36-4205-8d06-f4303b005cb0"}},{"cell_type":"code","outputs":[],"source":"english_sw = nltk.corpus.stopwords.words('english')\nprint('loaded %d stopwords for english' % len(english_sw))","execution_count":null,"metadata":{"_uuid":"f1f1372f8735cba924557f5e80539f6318af45b9","_cell_guid":"4eb9addb-554e-46c2-b7f7-e973c6697b21"}},{"cell_type":"code","outputs":[],"source":"count_df_no_sw = count_df[~count_df.index.isin(english_sw)]\n_, (ax1, ax2, ax3) = plt.subplots(ncols = 3, figsize = (16,6))\ntopn = 10\ncount_df_no_sw.EAP.sort_values(ascending = False).iloc[topn::-1].plot.barh(title = 'EAP', ax = ax1)\ncount_df_no_sw.HPL.sort_values(ascending = False).iloc[topn::-1].plot.barh(title = 'HPL', ax = ax2)\ncount_df_no_sw.MWS.sort_values(ascending = False).iloc[topn::-1].plot.barh(title = 'MWS', ax = ax3);","execution_count":null,"metadata":{"_uuid":"f39894dfe7f8434b23f6d48762bd409c46e01b6b","_cell_guid":"8b6a2153-e671-472d-b2e2-54f047b01f5a"}},{"cell_type":"markdown","source":"### Special characters\nSo far we have been looking at words and their occurences. Maybe the authors also have a preference for certain special characters? We can do the same analysis as above on individual characters instead of words by passing the option `analyze = 'char'` to the `CountVectorizer`.","metadata":{"_uuid":"a5b58e4a996e6f43b7ee7698fd405c45eefc1749","_cell_guid":"b142701e-7a42-4efb-bcc6-a24d9a14e758"}},{"cell_type":"code","outputs":[],"source":"cv2 = sk.feature_extraction.text.CountVectorizer(analyzer = 'char')\nX2 = cv2.fit_transform(df.text)\nchar_counts = pd.DataFrame(data = (X2.T * Y),\n                           columns = df.author.unique(),\n                           index = cv2.get_feature_names())","execution_count":null,"metadata":{"_uuid":"8316d874744b83f821821504309c9dd181135d0d","collapsed":true,"_cell_guid":"6566c2d5-58e4-4b66-9bed-10f4de268ce3"}},{"cell_type":"markdown","source":"Now we can inspect how often an author is using which character.","metadata":{"_uuid":"f9cd693faaa7d6d5d8bf06bbf7b742ddffc021c1","_cell_guid":"0f5d4a04-39cd-4a26-b18a-bb934490d460"}},{"cell_type":"code","outputs":[],"source":"char_counts","execution_count":null,"metadata":{"_uuid":"4c71bfe2cd0f12fed2ec7834dafea23a334ba7fc","_cell_guid":"a27bff04-3d98-41ca-b83b-94c54d58ec12"}},{"cell_type":"markdown","source":"This apart from the fact that EAP and HPL are using some French and Spanish special characters, we can't learn much from the table. We should normalize it by the total number of characters per author (which will give us an estimate for the conditional probability $P(character|author)$).","metadata":{"_uuid":"d93daec20bf51553a980e572746b7492ca71958c","_cell_guid":"6e020494-b322-4db8-bd6a-e391980b57e7"}},{"cell_type":"code","outputs":[],"source":"char_counts /= char_counts.sum()\nchar_counts.head(10)","execution_count":null,"metadata":{"_uuid":"f0cd5079081d0625c0fe5eb263e10f22c5fcfb14","_cell_guid":"ff69b17d-f196-46e1-8ba3-eacb763871a2"}},{"cell_type":"markdown","source":"In order to plot this data easily with `seaborn` we need to convert it to _long-format_.","metadata":{"_uuid":"13a794b0d48649bf424fc32897ac9ff992ced20f","_cell_guid":"5c16328c-c725-4d5f-bd8b-f44c529773e5"}},{"cell_type":"code","outputs":[],"source":"char_counts = (char_counts.stack()\n               .reset_index()\n               .rename(columns = {'level_0': 'char',\n                                  'level_1': 'author',\n                                  0: 'probability'\n                                 }))","execution_count":null,"metadata":{"_uuid":"127bd133261b589acff41ebf24464da628056250","collapsed":true,"_cell_guid":"25b583c9-4ad3-40ac-9ce2-696d11009d6d"}},{"cell_type":"code","outputs":[],"source":"char_counts.head(10)","execution_count":null,"metadata":{"_uuid":"9529707658ab5dc64e3d709b725f519d63ec274d","_cell_guid":"42e34c19-6a59-4208-9aaa-21a7bdced69f"}},{"cell_type":"markdown","source":"After this modification we can easily plot the conditional probabilities for each character given an author. For the sake of simplicity I restricted the set of plotted characters to those which have a probability higher than 1 in 100,000 (which basically removes the characters with accents). The absolute values on the y-scale are not that much of importance at the moment. Look for characters were the probabilites have different values.","metadata":{"_uuid":"c526a7c8f6a7ce3e9874fd1ec1ab045158fabcea","_cell_guid":"58a69aae-7795-4dc8-9bce-fc0391e2be14"}},{"cell_type":"code","outputs":[],"source":"sns.factorplot(col = 'char', x = 'author', y = 'probability',\n               data = char_counts[char_counts.probability > 1e-5], kind = 'bar', col_wrap = 4, size = 3, log = True);","execution_count":null,"metadata":{"_uuid":"012361064ba140c53b7effabe671f0cc43185fca","_cell_guid":"5842a023-bc63-4d11-8bef-ce45dcc7b961","scrolled":false}},{"cell_type":"code","outputs":[],"source":"def text2POS(text):\n    tags = TB(text).tags\n    return ' '.join([t for _,t in tags])","execution_count":null,"metadata":{"collapsed":true}},{"cell_type":"code","outputs":[],"source":"df['tags'] = df.text.transform(text2POS)","execution_count":null,"metadata":{}},{"cell_type":"code","outputs":[],"source":"c = sk.feature_extraction.text.CountVectorizer(tokenizer = lambda x: x.split())\nX3 = c.fit_transform(df.tags)\na = pd.DataFrame(data = X3.T * Y, columns = df.author.unique(), index = c.get_feature_names())","execution_count":null,"metadata":{}},{"cell_type":"code","outputs":[],"source":"X3.todense()[0,:]","execution_count":null,"metadata":{}},{"cell_type":"code","outputs":[],"source":"df.tags.iloc[0]","execution_count":null,"metadata":{}},{"cell_type":"code","outputs":[],"source":"b = a / a.sum()","execution_count":null,"metadata":{}},{"cell_type":"code","outputs":[],"source":"b = b.stack().reset_index().rename(columns = {'level_0': 'POS', 'level_1': 'author', 0: 'probability'})","execution_count":null,"metadata":{}},{"cell_type":"code","outputs":[],"source":"a.div(a.max(axis = 1), axis = 'index').min(axis = 1)","execution_count":null,"metadata":{}},{"cell_type":"code","outputs":[],"source":"stop_tags = a.index[a.div(a.max(axis = 1), axis = 'index').min(axis = 1) > 0.9]\nstop_tags","execution_count":null,"metadata":{}},{"cell_type":"code","outputs":[],"source":"sns.factorplot(col = 'POS', x = 'author', y = 'probability',\n               data = b, kind = 'bar', col_wrap = 4, size = 3, log = True);","execution_count":null,"metadata":{"scrolled":false}},{"cell_type":"markdown","source":"While most of the letters show very similar probabilites for each of three classes, there are some significant differences for some special characters like quotation marks, semi-colons, double-colons, question marks. That is something to keep in mind for the feature engineering and selection we are going to do later.\n\n## Reference model\nBefore diving further into feature engineering, I would like to build a simple model which can serve as a reference to judge further improvements. A very simple yet powerful approach to text classification is the [_Bag-of-words_](https://en.wikipedia.org/wiki/Bag-of-words_model) representation in conjunction with the [naive Bayes model](https://en.wikipedia.org/wiki/Naive_Bayes_classifier). The bag of words model represents a text by a list of words (or tokens) together with their document frequency (= absolute counts). It does not store any information about the grammar or the initial word order.\nThe naive Bayes model assumes that all input features are independent and that one can therefore write the posterior probability distribution as a product of individual conditional probability densities divided by some normalization factor. Despite the crude assumptions (words in texts are usually not independent), this approach can lead to quite satisfying classification results. Due to its historical importance and its simplicity, we shall use it as the reference model.\n\nFirst we need to define the input features for our model. Here I am going to use the word counts and the character counts as discussed above with the modification that I require some minimal threshold. Tokens which appear less often are not considered and just dropped. \n\nThe `sklearn` package comes with a handy tool to _stack_ and process several features - so-called [Pipeline]( http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html)s and [FeatureUnion](http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.FeatureUnion.html)s. You can think of a `FeatureUnion` as concatenating feature matrices next to each other (along `axis = 1`) and a pipeline as an object which calls `fit_transform` on its input and passes the output to the next stage in the pipeline. The last stage might be an estimator in which case it is only fitted (and no transformation is applied). The whole pipeline can then be used with the same semantics which are provided by the last step (e.g. as classifier, as regressor or as transformer).","metadata":{"_uuid":"af10c4f67f677068da21d2ca44e89e8b14fe2238","_cell_guid":"bb6d1b4b-67cf-41cb-869a-ba0a1d65e924"}},{"cell_type":"code","outputs":[],"source":"from sklearn.pipeline import make_pipeline, make_union\nfrom sklearn.naive_bayes import MultinomialNB\n\nreference = make_pipeline(\n    make_union(\n        make_pipeline(\n            sk.preprocessing.FunctionTransformer(func = lambda df: df.text, validate = False),\n            make_union(\n                sk.feature_extraction.text.CountVectorizer(min_df = 5),\n                sk.feature_extraction.text.CountVectorizer(analyzer = 'char', min_df = 10),\n            )\n        ),\n        make_pipeline(\n             sk.preprocessing.FunctionTransformer(func = lambda df: df.lemma, validate = False),\n             sk.feature_extraction.text.CountVectorizer(tokenizer = lambda x: x.split(),\n                                                        #stop_words = ['cc','nns'],\n                                                        min_df = 5\n                                                       )\n        )\n    ),\n    MultinomialNB(fit_prior = False)\n)","execution_count":null,"metadata":{"_uuid":"440119048fecc4e013dd4a525e735c229826b8c3","_cell_guid":"cf67b02e-d477-4187-b4ea-4221531c68c9"}},{"cell_type":"markdown","source":"Now we have our pipeline defined which can be used as a classifier. We will use [cross validation](https://en.wikipedia.org/wiki/Cross-validation) to train and evaluate it on random splits of the training sample.\n\nCross-validation is an important concept. If you train your model to closely to your training data, you will start relying on specific features or artifacts which are only present in the training set. This is meant when people talk about _overfitting_: your model does not generalize well to unseen data. In order to avoid this, you can train your model only on a subst of the available training set and evaluate it on the remaining set. For a more detailed discussion on this topic, check [sklearn's user guide](http://scikit-learn.org/stable/modules/cross_validation.html#cross-validation).\n\nThe evaluation metric is set to the logarithmic loss (which is used in the evaluation of this competition). In order to use this metric, we have to wrap it into a `scorer` object specifying that less is better and that we need probability predictions for each class instead of simple prediction... I know ... technicalities. Ah wait, one more thing: `sklearn` cross-validation framework always tries to maximise the target function. That's why the returned scores are negative (a log-loss can't be negative). Just revert the sign and you get the actual scores.","metadata":{"_uuid":"014675b312a5642a702f9ca3883c463bbcf6c23b","_cell_guid":"2dfc2bfb-c0e8-43de-b6c7-a81e05ef2982"}},{"cell_type":"code","outputs":[],"source":"# wrap the metric in a scorer function\nscore_func = sk.metrics.make_scorer(sk.metrics.log_loss,\n                                    greater_is_better = False,\n                                    needs_proba = True)\n\n# run the K-fold cross-validation with K = 5\nscores = sk.model_selection.cross_validate(reference,\n                                           df,\n                                           df.label,\n                                           cv = 5,\n                                           scoring = score_func,\n                                           return_train_score = True)\n# output the performance\ntrain_scores = scores['train_score']\ntest_scores = scores['test_score']\nprint('log-loss score of your model = %.2f +/- %.2f (training: %.2f +/- %.2f)' % (-np.mean(test_scores), np.std(test_scores, ddof = 1),-np.mean(train_scores), np.std(train_scores, ddof = 1)))","execution_count":null,"metadata":{"_uuid":"db2bd1e14c4c761f93fe9cf644fcb3abee6f36c0","_cell_guid":"d79bf3c4-8c3d-4f3a-a125-6fc73e4035f2"}},{"cell_type":"markdown","source":"Congratulations, you have trained a text classifier. The performance is not great, but remember that we got this result almost out-of-the-box without much feature engineering and tuning.\n\nFor now I will make a break and continue this evening with feature engineering. Things I have in mind include\n- imputing missing punctuation\n- stemming and lemmatizing\n- sentiment\n\nSo stay tuned and leave comment if you liked it!","metadata":{"_uuid":"587e478bbb418fdc1a096051db7083b2523c2751","_cell_guid":"150312f3-6930-40dc-9a8e-5c0d38b57e00"}},{"cell_type":"code","outputs":[],"source":"def _penn_to_wordnet(tag):\n    '''Converts the corpus tag into a Wordnet tag.'''\n    if tag in (\"NN\", \"NNS\", \"NNP\", \"NNPS\"):\n        return nltk.wordnet.NOUN\n    if tag in (\"JJ\", \"JJR\", \"JJS\"):\n        return nltk.wordnet.wordnet.ADJ\n    if tag in (\"VB\", \"VBD\", \"VBG\", \"VBN\", \"VBP\", \"VBZ\"):\n        return nltk.wordnet.wordnet.VERB\n    if tag in (\"RB\", \"RBR\", \"RBS\"):\n        return nltk.wordnet.wordnet.ADV\n    return None\n\ndef lemmatize(text):\n    b = TB(text)\n    return ' '.join([Word(w).lemmatize(_penn_to_wordnet(t)) for w,t in b.tags])","execution_count":null,"metadata":{}},{"cell_type":"code","outputs":[],"source":"df['lemma'] = df.text.transform(lemmatize)","execution_count":null,"metadata":{"collapsed":true}},{"cell_type":"code","outputs":[],"source":"c.fit_transform(df.lemma)\nprint(len(c.vocabulary_))","execution_count":null,"metadata":{}},{"cell_type":"code","outputs":[],"source":"csw = sk.feature_extraction.text.CountVectorizer(vocabulary = english_sw)\nX = csw.fit_transform(df.text)\nsw_df = pd.DataFrame(data = X.T * Y, columns = df.author.unique(), index = csw.get_feature_names())","execution_count":null,"metadata":{"collapsed":true}},{"cell_type":"code","outputs":[],"source":"a = pd.DataFrame(data = X.todense(), columns = csw.get_feature_names(), index = df.index)","execution_count":null,"metadata":{}},{"cell_type":"code","outputs":[],"source":"a['len'] = df.n_words","execution_count":null,"metadata":{}},{"cell_type":"code","outputs":[],"source":"d = a.div(df.n_words, axis = 'index') * 100","execution_count":null,"metadata":{}},{"cell_type":"code","outputs":[],"source":"d['author'] = df.author","execution_count":null,"metadata":{"collapsed":true}},{"cell_type":"code","outputs":[],"source":"sns.pairplot(data = d, vars = ['but', 'then', 'and', 'or'], hue = 'author');","execution_count":null,"metadata":{}},{"cell_type":"code","outputs":[],"source":"d.columns","execution_count":null,"metadata":{}},{"cell_type":"code","outputs":[],"source":"b = TB(df.text.iloc[2])\nprint(b)\nb.noun_phrases","execution_count":null,"metadata":{}},{"cell_type":"code","outputs":[],"source":"b","execution_count":null,"metadata":{}},{"cell_type":"code","outputs":[],"source":"b.sentiment","execution_count":null,"metadata":{}},{"cell_type":"code","outputs":[],"source":"df['polarity'] = df.text.transform(lambda x: TB(x).sentiment.polarity)","execution_count":null,"metadata":{"collapsed":true}},{"cell_type":"code","outputs":[],"source":"df['subjectivity'] = df.text.transform(lambda x: TB(x).sentiment.subjectivity)","execution_count":null,"metadata":{"collapsed":true}},{"cell_type":"code","outputs":[],"source":"sns.violinplot(x = 'author', y = 'subjectivity', data = df, inner = 'quartil')","execution_count":null,"metadata":{}},{"cell_type":"code","outputs":[],"source":"","execution_count":null,"metadata":{"collapsed":true}}],"nbformat":4,"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3","language":"python"},"language_info":{"file_extension":".py","pygments_lexer":"ipython3","version":"3.6.3","name":"python","nbconvert_exporter":"python","mimetype":"text/x-python","codemirror_mode":{"version":3,"name":"ipython"}}}}