{"metadata":{"language_info":{"pygments_lexer":"ipython3","version":"3.6.3","name":"python","file_extension":".py","codemirror_mode":{"version":3,"name":"ipython"},"mimetype":"text/x-python","nbconvert_exporter":"python"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat_minor":1,"cells":[{"cell_type":"markdown","source":"# Not So Scary Data Science\nThe purpose of this notebook is to show an intro level approach to the problem, help myself grow as a new data scientist and try provide some simple techniques and guidance to others who may also be just beginning.  At first I was intimidated by the subject of artificial intelligence, but after a few weeks of researching, videos and kernel overview I found out python can make tasks not seem as frightening.  A few notes before getting started: I am new to both python and machine learning, so if I interpret something incorrectly, code something wrong, ect. don't hesitate to tell me.  As mentioned above I am using this notebook to also help myself learn.\n\nEnough of the intro...let's dive into some code","metadata":{"_cell_guid":"617d52ff-e742-4d5f-b0fb-74cc9667a458","_uuid":"368bfa85a07e18f38efe715a06ed8403fdcfed5a"}},{"cell_type":"markdown","source":"## Imports","metadata":{"_cell_guid":"e88828d4-fcf4-456c-bf50-9bb2b37e69a7","_uuid":"219a6b77827d698b93b7606309b253e7d67f9d0d"}},{"cell_type":"code","source":"import itertools\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import log_loss\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n%matplotlib inline","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"352310cd-d124-4dfd-a902-f5f427bbcb25","collapsed":true,"_uuid":"f056d7f80dadbc5262d8a35e222e5036bf3fc359"}},{"cell_type":"markdown","source":"## Load and Inspect the Data","metadata":{"_cell_guid":"f67bcbcd-0320-4c8c-89aa-ffae34aa260c","_uuid":"88a640955713c2872486d5acf7d3ff26b39241cf"}},{"cell_type":"code","source":"train = pd.read_csv(\"../input/train.csv\")","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"334672ea-871f-4c91-81d2-39b2f3024bf2","collapsed":true,"_uuid":"ff12b369a5dae782cfc1d1764d8e580cd7de2206"}},{"cell_type":"code","source":"train.shape","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"53e7fe26-abad-45c2-9583-862d4b8a596e","_uuid":"a341b653febf61987724c031a63030e33001782e"}},{"cell_type":"code","source":"train.head(5)","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"6831d98e-55fe-4f96-81c8-c7348470170f","_uuid":"a4e0dd3ef45c846efca52d7342734e9d8ea889b1"}},{"cell_type":"code","source":"train.tail(5)","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"6469a57b-5706-41fb-8176-d044b8373487","scrolled":true,"_uuid":"3b6a7655bdfa84669e213aad09ed6438f4dd675d"}},{"cell_type":"code","source":"# Check for null values\ntrain.isnull().values.any()","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"8e68bd1f-a3a9-4df1-b6fb-bfa674b8ee32","scrolled":true,"_uuid":"65b5b2367b140f14e3d4fc257877d80883f203df"}},{"cell_type":"code","source":"# Check for any empty string values\nfor text_id, text, author in zip(train.id, train.text, train.author):\n    if text == '' or author == '':\n        print(text_id)","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"b38535a2-6cb4-459e-bf3a-1ccba2aa25b7","collapsed":true,"_uuid":"db00eaaca6ae20c3bd43895c872c4ade9a889f84","scrolled":true}},{"cell_type":"code","source":"num_total = len(train)\nnum_eap = len(train.loc[train.author == 'EAP'])\nnum_hpl = len(train.loc[train.author == 'HPL'])\nnum_mws = len(train.loc[train.author == 'MWS'])\n\nfig, ax = plt.subplots()\neap, hpl, mws = plt.bar(np.arange(1, 4), [(num_eap/num_total)*100, (num_hpl/num_total)*100, (num_mws/num_total)*100])\nax.set_xticks(np.arange(1, 4))\nax.set_xticklabels(['Edgar Allan Poe', 'H.P. Lovecraft', 'Mary Shelley'])\nax.set_ylim([0, 60])\nax.set_ylabel('Percentage of Entries', fontsize=12)\nax.set_xlabel('Author Name', fontsize=12)\nax.set_title('Percentage of Text Entries by Author')\nplt.show()","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"a9fefcc8-1a0f-4710-99a4-2cf96d53d294","scrolled":false,"_uuid":"67ef698216a0b1ebb04b1df4236c9175d15380d0"}},{"cell_type":"markdown","source":"This looks like a pretty good distribution of authors.","metadata":{"_cell_guid":"c88c0b5d-a31a-4287-a575-2c8377cc6174","_uuid":"ccaa1116933c7552e56ce230b9adf9adab19d475"}},{"cell_type":"markdown","source":"## Split the Data\n80% for training, 20% for testing","metadata":{"_cell_guid":"faa2bf4b-24ab-4178-a783-b302db99c988","_uuid":"0b21969aa0bede8077a95c9e28e2b5b34e90067d"}},{"cell_type":"code","source":"feature_col_names = ['text']\npredicted_class_name = ['author']\nx = train[feature_col_names].values\ny = train[predicted_class_name].values\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=.2, random_state=42)\n\n# Flatten the arrays for later use\nx_train = x_train.ravel()\nx_test = x_test.ravel()\ny_train = y_train.ravel()\ny_test = y_test.ravel()","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"78a467ea-e39b-4cdf-9d54-7b1c919536fc","collapsed":true,"_uuid":"82eaee2f04897327775b2cb0b961b6af23d8057b"}},{"cell_type":"markdown","source":"As seen above there are no null or empty values, so no additional data preparation is needed.","metadata":{"_cell_guid":"3940ce21-63bb-45e6-a648-5382c7b2a266","_uuid":"5cdd1953c76a696d3e96c7b0079efcf68bcadc41"}},{"cell_type":"markdown","source":"## Tokenize and Train an Initial Algorithm - MultinomialNB","metadata":{"_cell_guid":"5dbf0b85-d8c4-4186-a3a6-4654742f54da","_uuid":"b9c4444c611fe042c542c9964f048f968c131eef"}},{"cell_type":"code","source":"word_vectorizer = TfidfVectorizer()\nx_train_word = word_vectorizer.fit_transform(x_train)\nx_test_word = word_vectorizer.transform(x_test)\n\nword_clf = MultinomialNB()\nword_clf.fit(x_train_word, y_train)\n\nprint(\"Accuracy of word_clf: {0}\".format(accuracy_score(y_test, word_clf.predict(x_test_word))))\nprint(\"log_loss of word_clf: {0}\".format(log_loss(y_test, word_clf.predict_proba(x_test_word))))","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"aaf82683-d701-43f4-a7e6-d55383c676ab","_uuid":"95fbb045b399deaa70a17fc16263da1b42910798"}},{"cell_type":"markdown","source":"Just plug and chug gets an initial accuracy of 81% and a log loss of .6.  Not bad for a first attempt, but both the vectorizer and the classifier has some parameters we can adjust to try and improve our score.","metadata":{"_cell_guid":"8e11d7b1-1728-4b2a-9582-50f633f67c3a","_uuid":"b040d5ca2fa652575249891bd54e6014467a5084"}},{"cell_type":"code","source":"word_clf = MultinomialNB(alpha=.5)\nword_clf.fit(x_train_word, y_train)\n\nprint(\"Accuracy of word_clf with alpha=.5: {0}\".format(accuracy_score(y_test, word_clf.predict(x_test_word))))\nprint(\"log_loss of word_clf with alpha=.5: {0}\".format(log_loss(y_test, word_clf.predict_proba(x_test_word))))\n\nword_clf = MultinomialNB(alpha=.1)\nword_clf.fit(x_train_word, y_train)\n\nprint(\"Accuracy of word_clf with alpha=.1: {0}\".format(accuracy_score(y_test, word_clf.predict(x_test_word))))\nprint(\"log_loss of word_clf with alpha=.1: {0}\".format(log_loss(y_test, word_clf.predict_proba(x_test_word))))","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"57a063d5-68d5-48bd-878c-fa3b9e3d2d32","_uuid":"96717352356576bfdefcebede44b071a2ed91042"}},{"cell_type":"markdown","source":"Adjusting the alpha is improving the accuracy and log loss, but there must be a better way than manually adjusting the values by hand.  This is where SKLearn's Pipeline and GridSearchCV comes in handy.  It can automate all this testing by hand.","metadata":{"_cell_guid":"bac1965e-1968-4212-b6d5-e6d5dfc34de4","_uuid":"6e924ed9c1a4e7ab158824cd06c0942ecf029786"}},{"cell_type":"code","source":"pipeline = Pipeline(steps=[('tfidf', TfidfVectorizer()), ('mnb', MultinomialNB())])\n    \nparameters = {'tfidf__ngram_range': [(1,1), (1,2), (1,3)],\n              'mnb__alpha': [.1, .05, .005]}\n\n# For more updates on progress change verbose to something like 10\ngrid = GridSearchCV(pipeline, parameters, scoring='accuracy', cv=10, verbose=1, n_jobs=-1)\n\ngrid.fit(x_train, y_train)","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"7fe8001f-8343-4016-b61e-3abc6a6355a2","scrolled":false,"_uuid":"a52da02dd89efca12c752cf6679371f8f236496a","_kg_hide-output":true}},{"cell_type":"code","source":"print('Best score: {0}'.format(grid.best_score_))\nprint('Best parameters: {0}'.format(grid.best_params_))","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"76ff7711-5a1e-4e19-a359-f3e524ff3fbf","scrolled":true,"_uuid":"0f517bb7e04298d3cc9437a1a93ba689f720e372"}},{"cell_type":"markdown","source":"Other things that are testable: a mixture of different vectorizers, classifiers and all of their settings.  So far from all my testing I found out using TfidfVectorizer with the MultinomialNB is best.  Now, time to retrain with these values and output the performance.","metadata":{"_cell_guid":"d62906f3-ca00-4e87-a980-37731afff516","_uuid":"e8f5d642f791f3d9735b7487c72ca60f299df409"}},{"cell_type":"code","source":"word_vectorizer = TfidfVectorizer(ngram_range=(1, 2))\nx_train_word = word_vectorizer.fit_transform(x_train)\nx_test_word = word_vectorizer.transform(x_test)\n\nword_clf = MultinomialNB(alpha=.05)\nword_clf.fit(x_train_word, y_train)\n\nword_clf_pred_proba = word_clf.predict_proba(x_test_word)\n\nprint(\"Accuracy of word_clf: {0}\".format(accuracy_score(y_test, word_clf.predict(x_test_word))))\nprint(\"log_loss of word_clf: {0}\".format(log_loss(y_test, word_clf_pred_proba)))","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"4fad8c38-92c9-4fc6-a8aa-7c5f42a20a09","_uuid":"d8d8bae95c4c40799728bb1cc1540304a59497a4"}},{"cell_type":"markdown","source":"Looks like we bumped up the accuracy and log loss even further, excellent.  Let's try something else.  By default the TfidfVectorizer tokenizes by words, let's try characters instead.  Note: due to the increased amount of time character analysis takes I am skipping the Pipeline testing and returning the result I found to be best so far.","metadata":{"_cell_guid":"62007981-9b49-4a11-b85f-a015dbafffcf","_uuid":"bf4519b55e5491601a6defbc9785ceac7835edac"}},{"cell_type":"code","source":"char_vectorizer = TfidfVectorizer(analyzer='char', ngram_range=(2, 9))\nx_train_char = char_vectorizer.fit_transform(x_train)\nx_test_char = char_vectorizer.transform(x_test)\n\nchar_clf = MultinomialNB(alpha=.005)\nchar_clf.fit(x_train_char, y_train)\n\nchar_clf_pred_proba = char_clf.predict_proba(x_test_char)\n\nprint(\"Accuracy of char_clf: {0}\".format(accuracy_score(y_test, char_clf.predict(x_test_char))))\nprint(\"log_loss of char_clf: {0}\".format(log_loss(y_test, char_clf_pred_proba)))","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"229e7a9d-5dd8-49d4-95e1-16fa34438d9b","_uuid":"d8cee82690372a167d082bfade59fe05b1474827"}},{"cell_type":"markdown","source":"I found this pretty interesting, I increased the accuracy, but at the expense of log loss.  If I understand this correctly, the character analyzer is predicting correctly more times than the word analyzer, but it is less confident on each of the results.\n\nNow that two different classifiers let's look at some confusion matrices.","metadata":{"_cell_guid":"c3cf2c60-480f-42e9-832e-18c7d1c81620","_uuid":"b45fc652895e5aa0758e349170f48721cc8147bb"}},{"cell_type":"code","source":"# From http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html#sphx-glr-auto-examples-model-selection-plot-confusion-matrix-py\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    if normalize:\n        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n        #print(\"Normalized confusion matrix\")\n    #else:\n        #print('Confusion matrix, without normalization')\n\n    #print(cm)\n\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() / 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"726760e4-5813-433a-8d41-31141df8e791","_kg_hide-input":true,"_uuid":"881b2f725cc6248195236cda69a462002941a232","_kg_hide-output":true,"collapsed":true}},{"cell_type":"code","source":"word_conf_mat = confusion_matrix(y_test, word_clf.predict(x_test_word))\nplot_confusion_matrix(word_conf_mat, classes=['EAP', 'HPL', 'MWS'], title='Author Confusion Matrix - Word Classifier')","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"be3f21ac-bc1b-46e6-9bae-59df9ce7bb86","_uuid":"ae38632f2b838ff458cdde7e9b81f9839e6b6642"}},{"cell_type":"code","source":"char_conf_mat = confusion_matrix(y_test, char_clf.predict(x_test_char))\nplot_confusion_matrix(char_conf_mat, classes=['EAP', 'HPL', 'MWS'], title='Author Confusion Matrix - Character Classifier')","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"6c442333-ff6a-4add-8f4c-03a297e6d76c","_uuid":"e52a384c143219f5a0f292afd3742c0ec0d677bc"}},{"cell_type":"markdown","source":"Looks like each is better at predicting different aspects, what happens if we try to combine both classifiers (ensembling)?","metadata":{"_cell_guid":"86e063bf-7602-45a3-832e-58f24e42c9e7","_uuid":"6f3c4f96a9782a50a5c6382f9b975e5666abeb56"}},{"cell_type":"code","source":"combined_proba = []\ncombined_predictions = []\nfor w, c in zip(word_clf_pred_proba, char_clf_pred_proba):\n    prob = (w[0]+c[0])/2, (w[1]+c[1])/2, (w[2]+c[2])/2\n    combined_proba.append(prob)\n    if prob[0] > prob[1] and prob[0] > prob[2]:\n        combined_predictions.append('EAP')\n    elif prob[1] > prob[2]:\n        combined_predictions.append('HPL')\n    else:\n        combined_predictions.append('MWS')\n        \nprint(\"Accuracy of combined classifiers: {0}\".format(accuracy_score(y_test, combined_predictions)))\nprint(\"log_loss of combined classifiers: {0}\".format(log_loss(y_test, combined_proba)))","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"0578c28c-bdae-4e52-9a16-3d41d30a7bd2","_uuid":"701b10d8ed733bec4a556a8d6305b68c8810e622"}},{"cell_type":"code","source":"combined_conf_mat = confusion_matrix(y_test, combined_predictions)\nplot_confusion_matrix(combined_conf_mat, classes=['EAP', 'HPL', 'MWS'], title='Author Confusion Matrix - Combined Classifiers')","outputs":[],"execution_count":null,"metadata":{"_uuid":"b75887b578c080d15eaf9cf56601484a2d775076"}},{"cell_type":"markdown","source":"## The End....Maybe?\nAs I mentioned before this notebook is designed to both help others and myself learn.  If you have any questions, comments, suggestions or improvements please mention them.","metadata":{"_uuid":"6f5dd512d2d61503a5b3d54ecd3830f751ca4601"}}],"nbformat":4}