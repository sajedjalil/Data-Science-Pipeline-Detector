{"nbformat":4,"cells":[{"source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n","metadata":{"_cell_guid":"36612a2d-9ddd-4c0c-ab54-373a88022773","_uuid":"969e01894d6bb8facc5db88fe5122978a2909df3","collapsed":true},"execution_count":null,"cell_type":"code","outputs":[]},{"source":"train = pd.read_csv('../input/train.csv')\ntest = pd.read_csv('../input/test.csv')\nsub = pd.read_csv('../input/sample_submission.csv')","metadata":{"_cell_guid":"01a41492-dd35-4c4d-aa93-39a2f3f931b2","_uuid":"66c9bb1f59bd22a0ce7305bb9e83e02b0f1cbab2","collapsed":true},"execution_count":null,"cell_type":"code","outputs":[]},{"source":"We are going to analyze all the grams and use those as features for our classifer, and we are going to assume a largely linear association between the indicators and the labels.","metadata":{"_cell_guid":"e3db8cd4-e6b7-4609-83d0-54a7bedc4940","_uuid":"160d4b196389a1a13d1603b0b926754430810067"},"cell_type":"markdown"},{"source":"# Feature Extraction","metadata":{"_cell_guid":"95f31e10-65bd-492f-a89d-b4407c154567","_uuid":"252f1a58ae05254c33f830c5451c05b4507e0f82"},"cell_type":"markdown"},{"source":"Here, we are going to using the count vectorizer and only consider uni-grams, bi-grams, and tri-grams. We will also normalize the counts, and feed that into each classifer..","metadata":{"_cell_guid":"2ebe6d35-f863-4408-ab98-982ad7f2b0e1","_uuid":"d9b095b1acfd8d809133e04255b05a1c4d146e89"},"cell_type":"markdown"},{"source":"word = []\n\nfor text in train['text']:\n    word.append( text )\n\nfor text in test['text']:\n    word.append( text )","metadata":{"_cell_guid":"469358bb-6696-4b16-b241-b83923e3b6a7","_uuid":"a4610ddd246b29695926ed9623b12ac618e0e3d0","collapsed":true},"execution_count":null,"cell_type":"code","outputs":[]},{"source":"from sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer\n\ncount_vec = CountVectorizer( ngram_range = (1, 3) )\ntfid_ = TfidfTransformer( )\n\nprint('Extracing Count Information')\ncount_vec.fit(word)\ntrain_sparse = count_vec.transform( train['text'] )\ntest_sparse = count_vec.transform( test['text'] )\n\nprint('Normalizing Count Information')\ntfid_.fit( train_sparse )\n\ntrain_tfid = tfid_.transform( train_sparse )\ntest_tfid = tfid_.transform( test_sparse )","metadata":{"_cell_guid":"000a4dfe-d1b2-4cf7-80dc-c338c9db80bc","_uuid":"526872d409f011af56885b125d574cb914937be7","collapsed":true},"execution_count":null,"cell_type":"code","outputs":[]},{"source":"author_dict = { 'EAP' : 0, 'HPL' : 1, 'MWS' : 2 }\n\nauthor_labels = train['author'].apply(author_dict.get)\ntrain = train.drop('author', axis = 1)\ntrain.drop('id', axis = 1, inplace = True)","metadata":{"_cell_guid":"2e558024-c407-4159-8c36-b70b0c9044dd","_uuid":"31bee561bf5986772a7c5df48900494da008db4a","collapsed":true},"execution_count":null,"cell_type":"code","outputs":[]},{"source":"## Models\n\nWe will perform LogisticRegression, Stochastic Gradient Descent and Naive Bayes on the dataset.","metadata":{"_cell_guid":"aecf434b-e49a-4ba5-ba4c-5852c02814e9","_uuid":"2c09074f57bdee36717c6a34fdde9094d559a7ce"},"cell_type":"markdown"},{"source":"test_preds = {}","metadata":{"_cell_guid":"82586e92-1c46-40cd-86ad-1c63c2e2b6ac","_uuid":"95b27008e1567d9abe5bf22b85b5ae2702954278","collapsed":true},"execution_count":null,"cell_type":"code","outputs":[]},{"source":"## SGD w/log loss","metadata":{"_cell_guid":"681044c6-c85b-4dd0-9c92-19ad9247799f","_uuid":"6cc6ea5117bcc7e796b9f996f53b761013e79905"},"cell_type":"markdown"},{"source":"from sklearn.linear_model import SGDClassifier\n\nsgd_clf = SGDClassifier(loss = 'log', max_iter = 2000, n_jobs = -1)\n\nsgd_clf.fit( train_tfid, author_labels )\n\ntest_preds['sgd_clf'] = sgd_clf.predict_proba( test_tfid )","metadata":{"_cell_guid":"b6610741-baff-4dc3-807e-c79a1032d4a2","_uuid":"8e4c31f5cfab46445d038e61e5a2ecd3843258dc","collapsed":true},"execution_count":null,"cell_type":"code","outputs":[]},{"source":"## Naive Bayes","metadata":{"_cell_guid":"6d10a059-272d-43cd-a28c-eda4feda9813","_uuid":"39a927bd097bbdba8a5b82fc82045a6779931763"},"cell_type":"markdown"},{"source":"from sklearn.naive_bayes import MultinomialNB\n\nnb = MultinomialNB( )\n\nnb.fit( train_tfid, author_labels )\n\ntest_preds['nb_clf'] = nb.predict_proba( test_tfid )","metadata":{"_cell_guid":"7d30c4ef-1616-4dc9-860a-b23280555e46","_uuid":"99e924935478c8a1ca69b9baa7c21bd5f5692f55","collapsed":true},"execution_count":null,"cell_type":"code","outputs":[]},{"source":"## Logistic Regression","metadata":{"_cell_guid":"3714316e-f544-4b7f-b260-441256373743","_uuid":"96e708bf80b24e301f6b9c90a11419974a44576f"},"cell_type":"markdown"},{"source":"from sklearn.linear_model import LogisticRegression\n\nlog_clf = LogisticRegression( solver = 'saga', multi_class = 'multinomial', \n                             max_iter = 500, n_jobs = -1)\n\nlog_clf.fit( train_tfid, author_labels )\n\ntest_preds['log_clf'] = log_clf.predict_proba( test_tfid )","metadata":{"_cell_guid":"9e486d3a-d5b3-4a28-90f2-05ff186ed715","_uuid":"75024296a0594f2513e3d0686a1646d390e32a58","collapsed":true},"execution_count":null,"cell_type":"code","outputs":[]},{"source":"## Stack\n\nFor the stack, we will use the harmonic mean to average our values, and to squash any outliers in the predictions.","metadata":{"_cell_guid":"cd761c15-c426-4454-94c8-45754b29cbe0","_uuid":"a0cbf4e5877ca34e878145609b8941adf1fb045e"},"cell_type":"markdown"},{"source":"cols = ['EAP', 'HPL', 'MWS']\nsub[cols] = 0.0\n\nn = len( test_preds.keys() )\n\nfor key in test_preds.keys():\n    sub[cols] += (1.0/n)*( test_preds.get(key) ** -1.0)\n    \nsub[cols] = ( sub[cols].values ) ** -1.0","metadata":{"_cell_guid":"d47f82f6-10af-4b9a-9140-d45a7abda44d","_uuid":"7e446cb68419c5e2815495c74c505d68613aabb8","collapsed":true},"execution_count":null,"cell_type":"code","outputs":[]},{"source":"Time to submit.","metadata":{"_cell_guid":"b05bbc37-3921-4268-9bb3-ac32b455c76c","_uuid":"84efdc57f972dcf5d0b99d7661e636216b2859f0"},"cell_type":"markdown"},{"source":"sub.to_csv('sub.csv', index = False)","metadata":{"_cell_guid":"babd81e2-9e66-4f6a-892e-45432040b1f4","_uuid":"29542db529896ff551e67e485812513c362f50fd","collapsed":true},"execution_count":null,"cell_type":"code","outputs":[]},{"source":"## Conclusion\n\nThese parameters in each of the feature extraction models and the classifers are not optimized.  With some tinkering with the parameters, it's possible to get the LB score.  Hope this help! ","metadata":{"_cell_guid":"cd5da61c-dbdc-49ab-badc-c69cf9da4c60","_uuid":"1a40ceb7cc6aa09145aa62d772017b60a56435f0"},"cell_type":"markdown"}],"nbformat_minor":1,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"mimetype":"text/x-python","nbconvert_exporter":"python","version":"3.6.3","name":"python","file_extension":".py","pygments_lexer":"ipython3","codemirror_mode":{"version":3,"name":"ipython"}}}}