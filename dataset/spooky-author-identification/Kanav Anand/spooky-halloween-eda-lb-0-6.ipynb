{"nbformat_minor":1,"nbformat":4,"cells":[{"source":"## 1)EDA\n##### 1.1)Importing LIbraries.\n##### 1.2)Exploration and Wordclouds.\n##### 1.3)Making some new features.\n## 2)LB~(0.6)\n##### 2.1)Countvectorizers\n##### 2.1)Baseline Logistic Regression Model","metadata":{"_uuid":"cd620708e85d3963d63a416b1f6e0ad8455ba29e","_cell_guid":"f44d96e9-1d32-41f0-8067-e7ccd318cd8d"},"cell_type":"markdown"},{"outputs":[],"source":"import matplotlib.pyplot as plt\nimport seaborn as sns\nimport nltk\nfrom wordcloud import WordCloud, STOPWORDS\nfrom nltk.corpus import stopwords\nfrom wordcloud import WordCloud, STOPWORDS\nimport pandas as pd\nimport numpy as np\nfrom sklearn.feature_extraction.text import CountVectorizer\n\ntrain = pd.read_csv('../input/train.csv')\ntest = pd.read_csv('../input/test.csv')\n","execution_count":null,"metadata":{"_uuid":"e1a38479b7d02df2a8a086de9d8ec60bfffe5427","collapsed":true,"_cell_guid":"0ad9554d-7295-4588-93a9-6b80d425538a"},"cell_type":"code"},{"source":"##### Glimpse of Data","metadata":{"_uuid":"1648f8542570c26f14ce823ee9c13e089ba2643a","_cell_guid":"76f636fa-9c16-4269-9f55-e704373d31a9"},"cell_type":"markdown"},{"outputs":[],"source":"train.head()","execution_count":null,"metadata":{"_uuid":"6139d50ee519427a46eef37d22d721bc959e6998","_cell_guid":"a729a474-3e05-47d1-b7ea-65727ad9b420"},"cell_type":"code"},{"outputs":[],"source":"print(train.id[0],'---')\nprint(train.text[0])","execution_count":null,"metadata":{"_uuid":"8090781d8df981b8c74a1a92ba70e3ccc4d01794","_cell_guid":"06c84804-b900-4a41-99ae-031bfdb476b8"},"cell_type":"code"},{"source":"## Is it Balanced Data?","metadata":{"_uuid":"9806cb013b117c5a46c08314123a86555e788f29","_cell_guid":"5ef5ef83-43e1-4275-8dd8-5dea0f0a6531"},"cell_type":"markdown"},{"outputs":[],"source":"plt.figure(figsize=(10,6))\nsns.countplot(train['author']);\nplt.title('Countplot for Authours_');\nplt.xlabel('Authors_',fontsize=20);","execution_count":null,"metadata":{"_uuid":"0dbf7b1856e4ecf89448a15779a06a48fd5ad61f","_cell_guid":"d503ed56-a260-40a5-90eb-91784c1c1557"},"cell_type":"code"},{"source":"So the anwer is yes!","metadata":{"_uuid":"7ce2494a3ad2de2aec75c9587e060a15df9ad71c","_cell_guid":"016e5237-6dd8-4a34-b4a7-3d7f75a2be22"},"cell_type":"markdown"},{"outputs":[],"source":"df_eap = train[train.author=='EAP']\ndf_hpl = train[train.author=='HPL']\ndf_mws = train[train.author=='MWS']","execution_count":null,"metadata":{"_uuid":"8634dc03b954d16dd49c25c25011a8ca1f4b2e7e","collapsed":true,"_cell_guid":"bcc6e155-79fa-478f-b252-70a3fb3a5210"},"cell_type":"code"},{"source":"## Generating the word cloud from Authors Dictionary!\n1)EAP","metadata":{"_uuid":"ae2aa8b6f2cf2e27cad811cae6b96d89ccb55141","_cell_guid":"0d59115b-be81-4d23-ba3f-aff30412e49d"},"cell_type":"markdown"},{"outputs":[],"source":"df_eap.text\ndic= (' '.join(df_eap['text']))\n\nwordcloud = WordCloud(width = 1000, height = 500,stopwords=STOPWORDS).generate(dic)\n\nplt.figure(figsize=(15,5));\nplt.imshow(wordcloud);\nplt.axis('off');\nplt.title('Word Cloud for EAP');","execution_count":null,"metadata":{"_uuid":"2b68c5c1bc683690600ce4fdbb869b719dbedd41","_cell_guid":"a4680cff-9b49-4dc9-95f9-974e37a583e9"},"cell_type":"code"},{"source":"*2)HPL*","metadata":{"_uuid":"63aafed071a3b4744b817f3fbf57ecc3140f674a","_cell_guid":"56635a01-e45c-42db-9f9f-d8c6f1aad38e"},"cell_type":"markdown"},{"outputs":[],"source":"dic= (' '.join(df_hpl['text']))\n\nwordcloud = WordCloud(width = 1000, height = 500,stopwords=STOPWORDS).generate(dic)\n\nplt.figure(figsize=(15,5));\nplt.imshow(wordcloud);\nplt.axis('off');\nplt.title('Word Cloud for HPL');","execution_count":null,"metadata":{"_uuid":"e8fa851039f623d278d3346caf66c0ab4379927b","_cell_guid":"5fd32776-57d6-4ae1-a60b-277e187cef5a"},"cell_type":"code"},{"source":"*MWS*","metadata":{"_uuid":"61ec4eae8e513ed5e6ba90ce5db859dabf73f0fb","_cell_guid":"3dd0d753-1629-49ad-8197-3826a5c1ed46"},"cell_type":"markdown"},{"outputs":[],"source":"\ndic= (' '.join(df_mws['text']))\n\nwordcloud = WordCloud(width = 1000, height = 500,stopwords=STOPWORDS).generate(dic)\n\nplt.figure(figsize=(15,5));\nplt.imshow(wordcloud);\nplt.axis('off');\nplt.title('Word Cloud for MWS');","execution_count":null,"metadata":{"_uuid":"40bde56ec80892d9c1837d9c5657036b3786c759","_cell_guid":"1bfab6e2-4d18-4c34-a598-a5646c71a1ce"},"cell_type":"code"},{"source":"### Its Enough EDA author wise, Now lets jump to train DataSet and get sum features.","metadata":{"_uuid":"9918b8c3ca9241f8cf810d4593622f79eeee654a","_cell_guid":"e6f5af80-59c7-431e-b7ce-48e7f41f160e"},"cell_type":"markdown"},{"source":"**1)Lenght of Words**","metadata":{"_uuid":"847ea8d3e701aaf3a3b87bec83ffbd7db0bcd97a","_cell_guid":"3b2bb7b2-dfa1-419d-b4d2-794f7ef79b10"},"cell_type":"markdown"},{"outputs":[],"source":"train[\"length\"] = train[\"text\"].apply(lambda x: len(str(x).split()))\ntest[\"length\"] = test[\"text\"].apply(lambda x: len(str(x).split()))\nprint(train['length'].head())","execution_count":null,"metadata":{"_uuid":"3a5afe4652e242ee7722f9afe96ffeb36581c02c","_cell_guid":"8aff4d4a-58de-4898-9f80-5709faef31ad"},"cell_type":"code"},{"source":"**2)Removing the Stopwords**","metadata":{"_uuid":"50e314a322b64431674614bb5c101f89ce16b510","_cell_guid":"3fd662a4-7c25-49f8-bde4-418a1c29af5c"},"cell_type":"markdown"},{"outputs":[],"source":"train[\"stp_len\"] = train[\"text\"].apply(lambda x: len([i for i in str(x).lower().split() if i in stopwords.words(\"english\")]))\ntest[\"stp_len\"] = test[\"text\"].apply(lambda x: len([i for i in str(x).lower().split() if i in stopwords.words(\"english\")]))\nprint(train['stp_len'].head())","execution_count":null,"metadata":{"_uuid":"ecab45bd7f76d4337e532e267eeb020c0f71d33c","_cell_guid":"42353aae-0cff-49ab-8c0d-6f4ba04d20fc"},"cell_type":"code"},{"source":"## Lets see mean length of each writter.","metadata":{"_uuid":"836f4871305ff0cd766325bf3e036d92f3512212","_cell_guid":"6195bddc-a780-43b1-8e78-521aed2e4840"},"cell_type":"markdown"},{"outputs":[],"source":"print(train.groupby(by=['author'])['length'].mean())\ntrain.groupby(by=['author'])['length'].mean().plot(kind='bar');","execution_count":null,"metadata":{"_uuid":"d0b36b272c1daf562e2625c887c08e4811d3d45c","_cell_guid":"5def86fe-df2b-4816-bb77-058f3bedef47"},"cell_type":"code"},{"source":"## Lets see which user has widest dictionary!","metadata":{"_uuid":"5b7cb54cfa67b58d59ec2a48b72047c56951af32","_cell_guid":"bfabff61-a331-42d8-8b9b-7c93ffa0c27a"},"cell_type":"markdown"},{"outputs":[],"source":"print(train.groupby(by=['author'])['stp_len'].mean())\ntrain.groupby(by=['author'])['stp_len'].mean().plot(kind='bar');","execution_count":null,"metadata":{"_uuid":"4705fca7ea3942807a4d9679d44319bcf21e4c4b","_cell_guid":"710de250-7bd8-4ad6-ae43-935fe3c07ffd"},"cell_type":"code"},{"source":"## In our analysis the punctuation marks can play a very role","metadata":{"_uuid":"53e791fc4920c222653d855e56b215b9ac7ede23","_cell_guid":"2696cde2-c31d-460f-ad5d-f9be802ea734"},"cell_type":"markdown"},{"outputs":[],"source":"import string\ntrain[\"punct\"] =train['text'].apply(lambda x: len([i for i in str(x) if i in string.punctuation]) )\ntest[\"punct\"] =test['text'].apply(lambda x: len([i for i in str(x) if i in string.punctuation]) )","execution_count":null,"metadata":{"_uuid":"56448b07c9948e315de82210e82f363359faabd5","collapsed":true,"_cell_guid":"fcdb12b9-8e08-4cf8-b82a-33e9a08d2ba1"},"cell_type":"code"},{"outputs":[],"source":"import matplotlib.pyplot as plt\n%matplotlib inline\nprint(train.groupby(by=['author'])['punct'].mean())\ntrain.groupby(by=['author'])['punct'].mean().plot(kind='bar');","execution_count":null,"metadata":{"_uuid":"0df6fa18b3be021100d1a82590e91064a214e090","_cell_guid":"ad7b1e1d-466a-4001-b991-a7769b09bc20"},"cell_type":"code"},{"source":"# Predictions on test Data-","metadata":{"_uuid":"68fd45712516ef05ac865cef1d97c6be258b7f8d","_cell_guid":"1d912ed3-e679-4cb7-a1e2-bb2bf1b56c55"},"cell_type":"markdown"},{"source":"### CountVectorizer\n<p>We call <strong>vectorization</strong> the general process of turning a collection\nof text documents into numerical feature vectors. This specific strategy\n(tokenization, counting and normalization) is called the <strong>Bag of Words</strong>\nor “Bag of n-grams” representation. Documents are described by word\noccurrences while completely ignoring the relative position information\nof the words in the document.</p>","metadata":{"_uuid":"8841c3ab69d4e348fb78509343974ca81a61852a","_cell_guid":"5eff38fb-7470-4c9b-af6d-103734c6f5f2"},"cell_type":"markdown"},{"outputs":[],"source":"vect = CountVectorizer(ngram_range=(1,2),min_df=5).fit(train['text'])\ntrain_vectorized = vect.transform(train['text'])\nlen(vect.get_feature_names())","execution_count":null,"metadata":{"_uuid":"8d0bcb7b6add777b800326e1a9cccb9acff88e97","_cell_guid":"53cb658e-b014-49a7-a31f-16dd972a3abd"},"cell_type":"code"},{"source":"### This technique called Stacking will help us to combine our extracted features with X_test","metadata":{"_uuid":"1ae96ebb695bba877a815548ccee49f584c18744","_cell_guid":"124622ab-6836-4407-b257-3b9f43f35c18"},"cell_type":"markdown"},{"outputs":[],"source":"k=['length', 'stp_len', 'punct']\narr = np.array(train[k])\nimport scipy\nstack = train[['length', 'stp_len', 'punct']]\ntrain_vectorized = scipy.sparse.hstack([train_vectorized, stack])","execution_count":null,"metadata":{"_uuid":"ad3d32a1bda33411dcb5d2b477201716bb2fb048","collapsed":true,"_cell_guid":"d2717a59-b00d-4390-a6af-80e5b7764a0a"},"cell_type":"code"},{"outputs":[],"source":"from sklearn.linear_model import LogisticRegression\nmodel = LogisticRegression(C=.03,n_jobs=-1)\nmodel.fit(train_vectorized, train['author'])","execution_count":null,"metadata":{"_uuid":"aadbf0ef29258d760cddf28e794792b5134d2741","_cell_guid":"b5481aa0-408b-43a7-a6b8-7af790640f86"},"cell_type":"code"},{"source":"### This technique called Stacking will help us to combine our extracted features with X_test","metadata":{"_uuid":"18454c93e93554bcfbe70f31507be1d387ba5ede","_cell_guid":"ae324a29-703c-4135-8ea6-0f86bfba1ba2"},"cell_type":"markdown"},{"outputs":[],"source":"k=np.array(test[['length', 'stp_len', 'punct']])\nX_test = vect.transform(test['text'] )\nX_test = scipy.sparse.hstack([X_test, k])","execution_count":null,"metadata":{"_uuid":"9ecb7f8ebfa3beadcd158395f44871311d6668f6","collapsed":true,"_cell_guid":"daca33c7-9f0a-443f-8d51-7e9ef8c2e483"},"cell_type":"code"},{"source":"## Making Predictions on test set","metadata":{"_uuid":"3c6aa7a72aa2d539661db6647152ecfe7fbb28eb","_cell_guid":"8cb4e604-3b33-4fd7-8a74-8c24c1a1814b"},"cell_type":"markdown"},{"outputs":[],"source":"df = pd.DataFrame(model.predict_proba(X_test),index=test['id'],columns=['EAP','HPL','MWS'])\ndf.head()","execution_count":null,"metadata":{"_uuid":"e2aa4b7aeea7a1e904983eed4099a70e68f68971","_cell_guid":"7880d56c-08b7-46d5-a2dc-3501a8fb06ac"},"cell_type":"code"},{"source":"1. ## Saving as Submission file LB ~0.6","metadata":{"_uuid":"0f1b2f9f06fd2c46e2f95031edb3b9a5a18e74a6","_cell_guid":"4a55b115-f522-4b47-8508-1412148ab860"},"cell_type":"markdown"},{"outputs":[],"source":"df.to_csv('Submission1.csv')","execution_count":null,"metadata":{"_uuid":"52923a5c32b98b164b034eb0c99e0a2ee78186c3","collapsed":true,"_cell_guid":"d72d3236-ee15-4df3-918e-fe122f258ad5"},"cell_type":"code"},{"source":"#### This was baseline score.\n#### I will keep on updating it.","metadata":{"_uuid":"de9f076a2f1a5f2c62321681abb1f93ee285b644","_cell_guid":"a085235d-bf6d-4509-92b2-be2d1b33c9e9"},"cell_type":"markdown"},{"outputs":[],"source":"","execution_count":null,"metadata":{"_uuid":"1c91b96caf7c6e67f1023ea5c11ec7299ea478a9","collapsed":true,"_cell_guid":"93d1a87d-033e-4b53-8298-1dded49addc6"},"cell_type":"code"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"nbconvert_exporter":"python","file_extension":".py","name":"python","mimetype":"text/x-python","codemirror_mode":{"version":3,"name":"ipython"},"pygments_lexer":"ipython3","version":"3.6.3"}}}