{"metadata":{"language_info":{"version":"3.6.3","codemirror_mode":{"version":3,"name":"ipython"},"mimetype":"text/x-python","nbconvert_exporter":"python","file_extension":".py","pygments_lexer":"ipython3","name":"python"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat_minor":1,"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"ea8a94f4-0d60-4fff-960c-ea45a9aa5495","_uuid":"fbd285f3ca449c813592b2df2c41f27c0e5295db"},"source":"Model: Bidrectional LSTM over 2 types of word embeddings (from chars using RNN, and from one-hot [as in the paper](https://arxiv.org/abs/1604.05529)). Additional featrues from [\"Simple Feature Engg Notebook - Spooky Author\"](https://www.kaggle.com/sudalairajkumar/simple-feature-engg-notebook-spooky-author) and sentence embedding from LSTM output are used as input for Dense layer. \n\nLB: around 0.38"},{"cell_type":"code","metadata":{"_cell_guid":"ab3f0d14-8342-4d6a-aa08-0e4887d87740","scrolled":true,"_uuid":"2d6e105e3d31be0d71e9ba09d41c35be6194ec5e"},"execution_count":null,"source":"import csv\nimport re\nimport os\nimport pickle\nimport copy\nimport string\nfrom collections import Counter\n\nimport pandas as pd\nimport numpy as np\nimport nltk\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import log_loss\n\nfrom keras.models import Model, load_model\nfrom keras.callbacks import Callback, EarlyStopping, ModelCheckpoint\nfrom keras.preprocessing import sequence\nfrom keras.layers import LSTM, Bidirectional, Dropout, Dense, Input, Embedding, BatchNormalization, TimeDistributed\nfrom keras.layers.merge import concatenate\n\nRANDOM_SEED = 43\nnp.random.seed(RANDOM_SEED)\n\n# nltk.download('punkt')\n# nltk.download('stopwords')","outputs":[]},{"cell_type":"code","metadata":{"_cell_guid":"4163875e-49cf-44ae-ae9e-1b1d01a3279f","scrolled":true,"_uuid":"3fe90819d67e477197f48b279b9ab21b08d69f4c"},"execution_count":null,"source":"# Reading dataset\nDATA_TRAIN = \"../input/train.csv\"\nDATA_TEST = \"../input/test.csv\"\n\ntrain = pd.read_csv(DATA_TRAIN, delimiter=',', quotechar='\"')\ntest = pd.read_csv(DATA_TEST, delimiter=',', quotechar='\"')\n\nauthor_to_index = {\n    \"EAP\": 0, \n    \"HPL\": 1, \n    \"MWS\": 2\n}\ntrain[\"author\"] = train[\"author\"].map(author_to_index)\n\nprint(\"Train samples: {}\".format(train.shape[0]))\nprint(train.head())\nprint()\nprint(\"Test samples: {}\".format(test.shape[0]))\nprint(test.head())","outputs":[]},{"cell_type":"code","metadata":{"_cell_guid":"69c0f743-9b81-4cff-9117-b1c708fb8cf4","_uuid":"e23285c17af74534121035cca87f1cdb9810d000"},"execution_count":null,"source":"# Tokenizer - nltk.word_tokenize (punkt module of NLTK)\ntokenize = nltk.word_tokenize\nprint(train[\"text\"][0])\nprint(tokenize(train[\"text\"][0]))","outputs":[]},{"cell_type":"code","metadata":{"_cell_guid":"c34c7098-0ce8-4cc2-b5e8-9a8c3f9061e3","_uuid":"1891c08363fadade8360571294618d491f169417","collapsed":true},"execution_count":null,"source":"class Vocabulary(object):\n    def __init__(self, dump_filename):\n        self.dump_filename = dump_filename\n        self.word_to_index = {}\n        self.index_to_word = []\n        self.counter = Counter()\n        self.reset()\n\n        if os.path.isfile(self.dump_filename):\n            self.load()\n\n    def save(self):\n        with open(self.dump_filename, \"wb\") as f:\n            pickle.dump(self, f, pickle.HIGHEST_PROTOCOL)\n\n    def load(self):\n        with open(self.dump_filename, \"rb\") as f:\n            vocab = pickle.load(f)\n            self.__dict__.update(vocab.__dict__)\n\n    def add_word(self, word):\n        self.counter[word] += 1\n        if self.word_to_index.get(word) is None:\n            self.index_to_word.append(word)\n            index = len(self.index_to_word) -1\n            self.word_to_index[word] = index\n            return index\n        return self.word_to_index[word]\n\n    def get_word_index(self, word):\n        if self.word_to_index.get(word) is not None:\n            return self.word_to_index[word]\n        return len(self.word_to_index)\n\n    def get_word(self, index):\n        return self.index_to_word[index]\n\n    def size(self):\n        return len(self.index_to_word)\n    \n    def reset(self):\n        self.word_to_index = {}\n        self.index_to_word = []\n        self.counter = Counter()\n        self.word_to_index[\"NotAWord\"] = 0\n        self.index_to_word.append(\"NotAWord\")\n        self.counter[\"NotAWord\"] = 1\n    \n    def shrink(self, num):\n        pairs = self.counter.most_common(num)\n        self.reset()\n        for word, count in pairs:\n            self.add_word(word)","outputs":[]},{"cell_type":"code","metadata":{"_cell_guid":"a8bdfed6-5b1a-414d-ac2f-d396c5ab6352","_uuid":"ca58e4317fb8f1cf8570ea856c11d8b37f689554","collapsed":true},"execution_count":null,"source":"def bow(train_texts, test_texts, tokenizer=nltk.word_tokenize, preprocessor=None,\n        use_tfidf=False, max_features=None, bow_ngrams=(1, 1), analyzer='word'):\n    train = copy.deepcopy(train_texts)\n    test = copy.deepcopy(test_texts)\n\n    if use_tfidf:\n        vectorizer = TfidfVectorizer(analyzer=analyzer, ngram_range=bow_ngrams, tokenizer=tokenizer,\n                                     preprocessor=preprocessor, max_features=max_features)\n    else:\n        vectorizer = CountVectorizer(analyzer=analyzer, ngram_range=bow_ngrams, tokenizer=tokenizer,\n                                     preprocessor=preprocessor, max_features=max_features)\n    data = train + test\n    data = vectorizer.fit_transform(data)\n    train_data = data[:len(train)]\n    test_data = data[len(train):]\n    return train_data, test_data\n\ndef run_bow_nb(train_sentences, train_answers, test_sentences):\n    train_data, test_data = bow(train_sentences, test_sentences)\n    nb = MultinomialNB()\n    clf = GridSearchCV(estimator=nb, \n                       param_grid={\"alpha\": [0.1, 0.3, 0.6, 0.9, 1.0]}, \n                       scoring=\"neg_log_loss\", cv=5)\n    clf.fit(train_data, train_answers)\n    print(\"CV: {}\".format(clf.best_score_))\n    return  clf.predict_proba(train_data), clf.predict_proba(test_data)\n\ndef run_boc_nb(train_sentences, train_answers, test_sentences):\n    train_data, test_data = bow(train_sentences, test_sentences, tokenizer=None, use_tfidf=True, analyzer='char')\n    nb = MultinomialNB()\n    clf = GridSearchCV(estimator=nb, \n                       param_grid={\"alpha\": [0.1, 0.3, 0.6, 0.9, 1.0]}, \n                       scoring=\"neg_log_loss\", cv=5)\n    clf.fit(train_data, train_answers)\n    print(\"CV: {}\".format(clf.best_score_))\n    return  clf.predict_proba(train_data), clf.predict_proba(test_data)\n\ndef collect_additional_features(train, test):\n    train_df = train.copy()\n    test_df = test.copy()\n    eng_stopwords = set(nltk.corpus.stopwords.words(\"english\"))\n    \n    train_df[\"words\"] =  train_df[\"text\"].apply(lambda text: text.split())\n    test_df[\"words\"] = test_df[\"text\"].apply(lambda text: text.split())\n    \n    train_df[\"num_words\"] = train_df[\"words\"].apply(lambda words: len(words))\n    test_df[\"num_words\"] = test_df[\"words\"].apply(lambda words: len(words))\n    \n    train_df[\"num_unique_words\"] = train_df[\"words\"].apply(lambda words: len(set(words)))\n    test_df[\"num_unique_words\"] = test_df[\"words\"].apply(lambda words: len(set(words)))\n    \n    train_df[\"num_chars\"] = train_df[\"text\"].apply(lambda text: len(text))\n    test_df[\"num_chars\"] = test_df[\"text\"].apply(lambda text: len(text))\n    \n    train_df[\"num_stopwords\"] = train_df[\"words\"].apply(lambda words: len([w for w in words if w in eng_stopwords]))\n    test_df[\"num_stopwords\"] = test_df[\"words\"].apply(lambda words: len([w for w in words if w in eng_stopwords]))\n    \n    train_df[\"num_punctuations\"] = train_df['text'].apply(lambda text: len([c for c in text if c in string.punctuation]))\n    test_df[\"num_punctuations\"] =test_df['text'].apply(lambda text: len([c for c in text if c in string.punctuation]))\n    \n    train_df[\"num_words_upper\"] = train_df[\"words\"].apply(lambda words: len([w for w in words if w.isupper()]))\n    test_df[\"num_words_upper\"] = test_df[\"words\"].apply(lambda words: len([w for w in words if w.isupper()]))\n    \n    train_df[\"num_words_title\"] = train_df[\"words\"].apply(lambda words: len([w for w in words if w.istitle()]))\n    test_df[\"num_words_title\"] = test_df[\"words\"].apply(lambda words: len([w for w in words if w.istitle()]))\n    \n    train_df[\"mean_word_len\"] = train_df[\"words\"].apply(lambda words: np.mean([len(w) for w in words]))\n    test_df[\"mean_word_len\"] = test_df[\"words\"].apply(lambda words: np.mean([len(w) for w in words]))\n    \n    pred_train, pred_test = run_bow_nb(train_df[\"text\"].tolist(), train_df[\"author\"].tolist(), test_df[\"text\"].tolist())\n    train_df[\"nb_count_eap\"] = pred_train[:,0]\n    train_df[\"nb_count_hpl\"] = pred_train[:,1]\n    train_df[\"nb_count_mws\"] = pred_train[:,2]\n    test_df[\"nb_count_eap\"] = pred_test[:,0]\n    test_df[\"nb_count_hpl\"] = pred_test[:,1]\n    test_df[\"nb_count_mws\"] = pred_test[:,2]\n    \n    pred_train, pred_test = run_boc_nb(train_df[\"text\"].tolist(), train_df[\"author\"].tolist(), test_df[\"text\"].tolist())\n    train_df[\"nb_count_chars_eap\"] = pred_train[:,0]\n    train_df[\"nb_count_chars_hpl\"] = pred_train[:,1]\n    train_df[\"nb_count_chars_mws\"] = pred_train[:,2]\n    test_df[\"nb_count_chars_eap\"] = pred_test[:,0]\n    test_df[\"nb_count_chars_hpl\"] = pred_test[:,1]\n    test_df[\"nb_count_chars_mws\"] = pred_test[:,2]\n    \n    train_df.drop([\"text\", \"id\", \"words\"], axis=1, inplace=True)\n    test_df.drop([\"text\", \"id\", \"words\"], axis=1, inplace=True)\n    if \"author\" in train_df.columns:\n        train_df.drop([\"author\"], axis=1, inplace=True)\n    if \"author\" in test_df.columns:\n        test_df.drop([\"author\"], axis=1, inplace=True)\n    \n    scaler = MinMaxScaler()\n    train_df = scaler.fit_transform(train_df)\n    test_df = scaler.transform(test_df)\n    return train_df, test_df","outputs":[]},{"cell_type":"code","metadata":{"_cell_guid":"8c97a4cf-60a4-4e88-8c8d-7124cac8ecb8","_uuid":"28f3fe21c76a688fbcd828ad193d1ad2decbd425"},"execution_count":null,"source":"VOCAB_PATH = \"vocab.pickle\"\n\ndef prepare_vocabulary(vocab_path, train, test, shrink_border=None):\n    vocabulary = Vocabulary(vocab_path)\n    if vocabulary.size() <= 1:\n        for sentence in train['text'].tolist():\n            for word in tokenize(sentence):\n                vocabulary.add_word(word)\n        print(\"Train vocabulary size: {}\".format(vocabulary.size()))\n        for sentence in test['text'].tolist():\n            for word in tokenize(sentence):\n                vocabulary.add_word(word) \n        print(\"Train+test vocabulary size: {}\".format(vocabulary.size()))\n        vocabulary.save()\n\n    print(\"Vocabulary size: {}\".format(vocabulary.size()))\n    if shrink_border is not None:\n        vocabulary.shrink(shrink_border)\n        print(\"Vocabulary size after shrink: {}\".format(vocabulary.size()))\n    return vocabulary\n\nvocabulary = prepare_vocabulary(VOCAB_PATH, train, test)","outputs":[]},{"cell_type":"code","metadata":{"_cell_guid":"df033a1b-f903-4be3-ad17-4b8207dcf440","_uuid":"7fb13ac4887dddc73ee37960add081aea76d914f"},"execution_count":null,"source":"CHAR_SET = \" abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ.,-'\\\"\"\n\ndef get_samples(sentences, vocabulary, word_max_count, max_word_len):\n    n = len(sentences)\n    word_matrix = np.zeros((n, word_max_count), dtype='int')\n    char_matrix = np.zeros((n, word_max_count, max_word_len), dtype=np.int)\n    for i, sentence in enumerate(sentences):\n        words = tokenize(sentence)[:word_max_count]\n        word_matrix[i, -len(words):] = [vocabulary.get_word_index(word) for word in words]\n        char_vectors = []\n        for word in words:\n            char_indices = np.zeros(max_word_len)\n            word_char_indices = [CHAR_SET.index(ch) if ch in CHAR_SET else len(CHAR_SET) for ch in word]\n            char_indices[-min(len(word), max_word_len):] = word_char_indices[:max_word_len]\n            char_vectors.append(char_indices)\n        char_matrix[i, -len(words):] = char_vectors\n    return word_matrix, char_matrix\n\ndef get_train_val_test_sets(x, y, x_test, vocabulary, word_max_count=80, max_word_len=30, val_part=0.1):\n    word_matrix, char_matrix = get_samples(x[\"text\"].tolist(), vocabulary, word_max_count, max_word_len)\n\n    n = x.shape[0]\n    np.random.seed(RANDOM_SEED)\n    perm = np.random.permutation(n)\n    idx_train = perm[:int(n*(1-val_part))]\n    idx_val = perm[int(n*(1-val_part)):]\n\n    additional_features_matrix_train, additional_features_matrix_val = \\\n        collect_additional_features(x.iloc[idx_train], x.iloc[idx_val])\n\n    word_matrix_train = word_matrix[idx_train]\n    char_matrix_train = char_matrix[idx_train]\n    y_train = np.array(y, dtype='int32')[idx_train]\n\n    word_matrix_val = word_matrix[idx_val]\n    char_matrix_val = char_matrix[idx_val]\n    y_val = np.array(y, dtype='int32')[idx_val]\n\n    word_matrix_test, char_matrix_test = get_samples(x_test[\"text\"].tolist(), \n                                                     vocabulary, word_max_count, max_word_len)\n    _, additional_features_matrix_test = collect_additional_features(x.iloc[idx_train], x_test)\n\n    return (word_matrix_train, char_matrix_train, additional_features_matrix_train, y_train), \\\n        (word_matrix_val, char_matrix_val, additional_features_matrix_val, y_val), \\\n        (word_matrix_test, char_matrix_test, additional_features_matrix_test)\n        \ndata_train, data_val, data_test = get_train_val_test_sets(train, train['author'].tolist(), test, vocabulary)","outputs":[]},{"cell_type":"code","metadata":{"_cell_guid":"df01fc1d-df71-4c84-9539-224e7eb1ec79","_uuid":"562e6bdb8a73a01f74941953ee9c97f69dbb4898","collapsed":true},"execution_count":null,"source":"class SpookyRNN:\n    def __init__(self, rnn_units=64, dense_units=32, dropout=0.4, batch_size=256, \n                 embeddings_dimensions=150, char_embeddings_dimension=20, max_word_len=30,\n                 char_lstm_output_dim=64):\n        self.batch_size = batch_size\n        self.dropout = dropout\n        self.rnn_units = rnn_units\n        self.dense_units = dense_units\n        self.embeddings_dimensions = embeddings_dimensions\n        self.char_embeddings_dimension = char_embeddings_dimension\n        self.char_lstm_output_dim =char_lstm_output_dim\n        self.max_word_len = max_word_len\n\n        self.model = None\n\n    def build(self, n_additional_features, vocabulary_size):\n        word_index_input = Input(shape=(None,), dtype=\"int32\", name=\"word_index_input\")\n        word_embeddings = Embedding(vocabulary_size + 1, \n                                    self.embeddings_dimensions, name=\"word_embeddings\")(word_index_input)\n        \n        char_input = Input(shape=(None, self.max_word_len), dtype=\"int32\", name=\"char_input\")\n        char_embeddings = Embedding(len(CHAR_SET) + 1, \n                                    self.char_embeddings_dimension, name='char_embeddings')(char_input)\n        word_from_char_embeddings = TimeDistributed(Bidirectional(\n            LSTM(self.char_lstm_output_dim // 2, dropout=self.dropout, \n                 recurrent_dropout=self.dropout, name='CharLSTM')))(char_embeddings)\n        \n        additional_features_input = Input(shape=(n_additional_features, ), dtype='float32', name='add_input')\n        \n        lstm_input = concatenate([word_embeddings, word_from_char_embeddings], name=\"lstm_input\")\n        lstm_layer = Bidirectional(LSTM(self.rnn_units // 2, dropout=self.dropout, \n                                        recurrent_dropout=self.dropout))(lstm_input)\n        \n        layer = concatenate([lstm_layer, additional_features_input], name=\"dense_input\")\n        dense = Dense(self.dense_units, activation='relu')(layer)\n        dense = Dropout(self.dropout)(dense)\n        \n        predictions = Dense(3, activation='softmax')(dense)\n        model = Model(inputs=[word_index_input, char_input, additional_features_input], outputs=predictions)\n        \n        model.compile(loss='sparse_categorical_crossentropy', optimizer='nadam', metrics=['accuracy'])\n                                                                                          \n        print(model.summary())\n        self.model = model\n\n    def train(self, data_train, data_val, model_filename, enable_checkpoints=True):\n        word_matrix_train, char_matrix_train, additional_features_matrix_train, y_train = data_train\n        word_matrix_val, char_matrix_val, additional_features_matrix_val, y_val = data_val\n        \n        print(\"Train example:\")\n        print(word_matrix_train[0])\n        print(char_matrix_train[0])\n        print(additional_features_matrix_train[0])\n        print(y_train[0])\n        \n        # Callback to prevent overfitting.\n        callbacks = [EarlyStopping(monitor='val_loss', patience=0)]\n\n        # Callback to save best only model.\n        if enable_checkpoints:\n            callbacks.append(ModelCheckpoint(model_filename, monitor='val_loss', save_best_only=True))\n\n        self.model.fit([word_matrix_train, char_matrix_train, additional_features_matrix_train], y_train, \n                       validation_data=([word_matrix_val, char_matrix_val, additional_features_matrix_val], y_val),\n                       epochs=50,\n                       batch_size=self.batch_size,\n                       shuffle=True, \n                       callbacks=callbacks,\n                       verbose=1)\n\n    def load(self, filename: str) -> None:\n        self.model = load_model(filename)\n        print(self.model.summary())\n\n    def predict(self, data_test, answer_filename):\n        word_matrix, char_matrix, additional_features_matrix = data_test\n        \n        print(\"Test example: \")\n        print(word_matrix[0])\n        print(char_matrix[0])\n        print(additional_features_matrix[0])\n        preds = self.model.predict([word_matrix, char_matrix, additional_features_matrix], \n                                   batch_size=self.batch_size, verbose=1)\n        index_to_author = { 0: \"EAP\", 1: \"HPL\", 2: \"MWS\" }\n        submission = pd.DataFrame({\"id\": test[\"id\"], index_to_author[0]: preds[:, 0], \n                                   index_to_author[1]: preds[:, 1], index_to_author[2]: preds[:, 2]})\n        submission.to_csv(answer_filename, index=False)","outputs":[]},{"cell_type":"code","metadata":{"_cell_guid":"9cad4050-7aba-4503-a740-3b0cf5254519","_uuid":"99600905d1ce4af7f926263e530a2a499b31e5b4","collapsed":true},"execution_count":null,"source":"MODEL_FILENAME = \"model.h5\"","outputs":[]},{"cell_type":"code","metadata":{"_cell_guid":"5452978d-e96b-4eaf-b5fc-acd359ef42c0","_uuid":"b832f812966f89f5acf57b014fcb18771fa5d212"},"execution_count":null,"source":"rnn = SpookyRNN()\nrnn.build(data_train[2].shape[1], vocabulary.size())\nrnn.train(data_train, data_val, MODEL_FILENAME)","outputs":[]},{"cell_type":"code","metadata":{"_cell_guid":"36ba6b33-754e-4c61-b927-0430adebd867","_uuid":"af0d5634ee909cc325b1b709bb84c56db6a44533"},"execution_count":null,"source":"rnn = SpookyRNN()\nrnn.load(MODEL_FILENAME)\nrnn.predict(data_test, 'answer.csv')","outputs":[]}],"nbformat":4}