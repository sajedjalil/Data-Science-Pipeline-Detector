{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"mimetype":"text/x-python","file_extension":".py","nbconvert_exporter":"python","pygments_lexer":"ipython3","name":"python","version":"3.6.3","codemirror_mode":{"name":"ipython","version":3}}},"nbformat_minor":1,"nbformat":4,"cells":[{"cell_type":"markdown","metadata":{"_uuid":"5484dedb6db529f4dc58d0cb433186b3d823f429","_cell_guid":"73969602-c633-450e-9364-9481a21a879c"},"source":"# **This notebook's best result: val_acc is 0.8779, val_loss is 0.3129**"},{"cell_type":"markdown","metadata":{"_uuid":"7ccc3b4516a4fcde346a162ae4f9461016bbfbbf","_cell_guid":"d5736ce6-a0b0-4cf0-beaf-a73837398da9"},"source":"# **1. Few Preprocessings**\n# **2. Model: FastText by Keras**\n## **2.1** Change Preprocessings:\n- Do lower case "},{"cell_type":"code","metadata":{"_kg_hide-input":false,"_uuid":"b05ef71268db76a4e2565177bf6a5668a5fc428e","collapsed":false,"_kg_hide-output":true,"_cell_guid":"93e00783-a024-4e87-a5e1-6709cb8cc981"},"outputs":[],"execution_count":null,"source":"import numpy as np\n\nimport pandas as pd\n\nfrom collections import defaultdict\n\nimport keras\nimport keras.backend as K\nfrom keras.layers import Dense, GlobalAveragePooling1D, Embedding\nfrom keras.callbacks import EarlyStopping\nfrom keras.models import Sequential\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.utils import to_categorical\n\nfrom sklearn.model_selection import train_test_split\n\nnp.random.seed(7)"},{"cell_type":"code","metadata":{"_uuid":"d700f739101e37903112e1de293323dcfbb577be","collapsed":true,"_cell_guid":"a5cc2c3e-7960-482e-b548-c447b89925ec"},"outputs":[],"execution_count":null,"source":"df = pd.read_csv('./../input/train.csv')\na2c = {'EAP': 0, 'HPL' : 1, 'MWS' : 2}\ny = np.array([a2c[a] for a in df.author])\ny = to_categorical(y)"},{"cell_type":"markdown","metadata":{"_uuid":"a01bab31ed7b8a55820612063576963488d99eb6","_cell_guid":"a45cb3ba-d1bc-48e0-956c-27d0f49a9943"},"source":"# 1. **Few Preprocessings**\n\nIn traditional NLP tasks, preprocessings play an important role, but...\n\n## **Low-frequency words**\nIn my experience, fastText is very fast, but I need to delete rare words to avoid overfitting.\n\n**NOTE**:\nSome keywords are rare words, such like *Cthulhu* in *Cthulhu Mythos* of *Howard Phillips Lovecraft*.\nBut these are useful for this task.\n\n## **Removing Stopwords**\n\nNothing.\nTo identify author from a sentence, some stopwords play an important role because one has specific usages of them.\n\n## **Stemming and Lowercase**\n\nNothing.\nThis reason is the same for stopwords removing.\nAnd I guess some stemming rules provided by libraries is bad for this task because all author is the older author.\n\n## **Cutting long sentence**\n\nToo long documents are cut.\n\n## **Punctuation**\n\nBecause I guess each author has unique punctuations's usage in the novel, I separate them from words.\n\ne.g. `Don't worry` -> `Don ' t worry`\n\n## **Is it slow?**\n\nDon't worry! FastText is a very fast algorithm if it runs on CPU. "},{"cell_type":"markdown","metadata":{"_uuid":"0023cd1542d866d931deb8472f8a0d6fb0262d9a","_cell_guid":"8182b25a-f490-4b41-9865-ee1c04afecee"},"source":"# **Let's check character distribution per author**"},{"cell_type":"code","metadata":{"_uuid":"246a428ca3a063294c15c8c08d234ecf01e4ddbb","collapsed":false,"_kg_hide-output":true,"_cell_guid":"c1d00b0d-90e0-4f19-842c-51a82de42a10"},"outputs":[],"execution_count":null,"source":"counter = {name : defaultdict(int) for name in set(df.author)}\nfor (text, author) in zip(df.text, df.author):\n    text = text.replace(' ', '')\n    for c in text:\n        counter[author][c] += 1\n\nchars = set()\nfor v in counter.values():\n    chars |= v.keys()\n    \nnames = [author for author in counter.keys()]\n\nprint('c ', end='')\nfor n in names:\n    print(n, end='   ')\nprint()\nfor c in chars:    \n    print(c, end=' ')\n    for n in names:\n        print(counter[n][c], end=' ')\n    print()\n"},{"cell_type":"markdown","metadata":{"_uuid":"8e72d6f22587780364ed24cae13ece4a403479dd","_cell_guid":"7a3fdf4e-039d-4c93-bc21-9bad7dfc6ff8"},"source":"# **Summary of character distribution**\n\n- HPL and EAP used non ascii characters like a `Ã¤`.\n- The number of punctuations seems to be good feature\n"},{"cell_type":"markdown","metadata":{"_uuid":"fee49fd9139b78ae03603d7d37eafa38f3cb29dc","_cell_guid":"ce97fc0a-b85c-4f34-92c5-ae66a0730ace"},"source":"# **Preprocessing**\n\nMy preproceeings are \n\n- Separate punctuation from words\n- Remove lower frequency words ( <= 2)\n- Cut a longer document which contains `256` words"},{"cell_type":"code","metadata":{"_uuid":"999012010cd8b9b20d3c5b16c11a2374a5ce44c0","collapsed":true,"_cell_guid":"72ff2ff5-0945-4f39-8b02-39e4d5df16c5"},"outputs":[],"execution_count":null,"source":"def preprocess(text):\n    text = text.replace(\"' \", \" ' \")\n    signs = set(',.:;\"?!')\n    prods = set(text) & signs\n    if not prods:\n        return text\n\n    for sign in prods:\n        text = text.replace(sign, ' {} '.format(sign) )\n    return text"},{"cell_type":"code","metadata":{"_uuid":"53f325a090a44f7109f0537022398797704cdc80","collapsed":true,"_cell_guid":"f123742f-540f-438d-aba3-ebbca69235be"},"outputs":[],"execution_count":null,"source":"def create_docs(df, n_gram_max=2):\n    def add_ngram(q, n_gram_max):\n            ngrams = []\n            for n in range(2, n_gram_max+1):\n                for w_index in range(len(q)-n+1):\n                    ngrams.append('--'.join(q[w_index:w_index+n]))\n            return q + ngrams\n        \n    docs = []\n    for doc in df.text:\n        doc = preprocess(doc).split()\n        docs.append(' '.join(add_ngram(doc, n_gram_max)))\n    \n    return docs"},{"cell_type":"code","metadata":{"_uuid":"150f9f6643e6753386b2021ac812ecc0cac66202","collapsed":true,"_cell_guid":"888047de-806e-4ad2-9fff-18b4d6583d30"},"outputs":[],"execution_count":null,"source":"min_count = 2\n\ndocs = create_docs(df)\ntokenizer = Tokenizer(lower=False, filters='')\ntokenizer.fit_on_texts(docs)\nnum_words = sum([1 for _, v in tokenizer.word_counts.items() if v >= min_count])\n\ntokenizer = Tokenizer(num_words=num_words, lower=False, filters='')\ntokenizer.fit_on_texts(docs)\ndocs = tokenizer.texts_to_sequences(docs)\n\nmaxlen = 256\n\ndocs = pad_sequences(sequences=docs, maxlen=maxlen)"},{"cell_type":"markdown","metadata":{"_uuid":"b9e353b548b0dfbd4b42a40d8a2643efeb359a20","_cell_guid":"f9ebc033-2a26-4656-9472-8990c1a27c79"},"source":"# **2. Model: FastText by Keras**\n\nFastText is very fast and strong baseline algorithm for text classification based on Continuous Bag-of-Words model a.k.a Word2vec.\n\nFastText contains only three layers:\n\n1. Embeddings layer: Input words (and word n-grams) are all words in a sentence/document\n2. Mean/AveragePooling Layer: Taking average vector of Embedding vectors\n3. Softmax layer\n\nThere are some implementations of FastText:\n\n- Original library provided by Facebook AI research: https://github.com/facebookresearch/fastText\n- Keras: https://github.com/fchollet/keras/blob/master/examples/imdb_fasttext.py\n- Gensim: https://radimrehurek.com/gensim/models/wrappers/fasttext.html\n\nOriginal Paper: https://arxiv.org/abs/1607.01759 : More detail information about fastText classification model"},{"cell_type":"markdown","metadata":{"_uuid":"8b56b2ef90e519b939b7bf9ec5a146f749807b02","_cell_guid":"636eb75e-6fba-413e-996d-1395609b422c"},"source":"# My FastText parameters are:\n\n- The dimension of word vector is 20\n- Optimizer is `Adam`\n- Inputs are words and word bi-grams\n  - you can change this parameter by passing the max n-gram size to argument of `create_docs` function.\n"},{"cell_type":"code","metadata":{"_uuid":"bba1d1a6416876e74ed688f56e4d5bc4990ec12a","collapsed":true,"_cell_guid":"393d1ddb-0a87-42a3-8575-53ff7abff1da"},"outputs":[],"execution_count":null,"source":"input_dim = np.max(docs) + 1\nembedding_dims = 20"},{"cell_type":"code","metadata":{"_uuid":"e6c16572e6b32923af39dfd29467e32b52561bb1","collapsed":true,"_cell_guid":"2e3e1e3e-22f4-4727-ba6c-67f7b3e80d2f"},"outputs":[],"execution_count":null,"source":"def create_model(embedding_dims=20, optimizer='adam'):\n    model = Sequential()\n    model.add(Embedding(input_dim=input_dim, output_dim=embedding_dims))\n    model.add(GlobalAveragePooling1D())\n    model.add(Dense(3, activation='softmax'))\n\n    model.compile(loss='categorical_crossentropy',\n                  optimizer=optimizer,\n                  metrics=['accuracy'])\n    return model"},{"cell_type":"code","metadata":{"scrolled":true,"collapsed":false,"_kg_hide-output":true,"_cell_guid":"0db889db-0b3e-4025-8847-e3eb5f853f37","_kg_hide-input":false,"_uuid":"22e57e010206a3044adf7b82160c7c3ca78030f8"},"outputs":[],"execution_count":null,"source":"epochs = 25\nx_train, x_test, y_train, y_test = train_test_split(docs, y, test_size=0.2)\n\nmodel = create_model()\nhist = model.fit(x_train, y_train,\n                 batch_size=16,\n                 validation_data=(x_test, y_test),\n                 epochs=epochs,\n                 callbacks=[EarlyStopping(patience=2, monitor='val_loss')])"},{"cell_type":"markdown","metadata":{},"source":"### **Result**\n\n- Best val_loss is 0.3409\n- Best val_acc is 0.8700\n\n"},{"cell_type":"markdown","metadata":{},"source":"# **2.1 Change Preprocessings**\n\nNext, I change some parameters and preprocessings to improve fastText model.\n## **2.1.1 Do lower case**"},{"cell_type":"code","metadata":{"collapsed":true},"outputs":[],"execution_count":null,"source":"docs = create_docs(df)\ntokenizer = Tokenizer(lower=True, filters='')\ntokenizer.fit_on_texts(docs)\nnum_words = sum([1 for _, v in tokenizer.word_counts.items() if v >= min_count])\n\ntokenizer = Tokenizer(num_words=num_words, lower=True, filters='')\ntokenizer.fit_on_texts(docs)\ndocs = tokenizer.texts_to_sequences(docs)\n\nmaxlen = 256\n\ndocs = pad_sequences(sequences=docs, maxlen=maxlen)\n\ninput_dim = np.max(docs) + 1"},{"cell_type":"code","metadata":{"collapsed":false},"outputs":[],"execution_count":null,"source":"epochs = 16\nx_train, x_test, y_train, y_test = train_test_split(docs, y, test_size=0.2)\n\nmodel = create_model()\nhist = model.fit(x_train, y_train,\n                 batch_size=16,\n                 validation_data=(x_test, y_test),\n                 epochs=epochs,\n                 callbacks=[EarlyStopping(patience=2, monitor='val_loss')])"},{"cell_type":"markdown","metadata":{},"source":"**Result**\n\n- Best val_loss is 0.3129\n- Best val_acc is 0.8787"},{"cell_type":"code","metadata":{"collapsed":false},"outputs":[],"execution_count":null,"source":"test_df = pd.read_csv('../input/test.csv')\ndocs = create_docs(test_df)\ndocs = tokenizer.texts_to_sequences(docs)\ndocs = pad_sequences(sequences=docs, maxlen=maxlen)\ny = model.predict_proba(docs)\n\nresult = pd.read_csv('../input/sample_submission.csv')\nfor a, i in a2c.items():\n    result[a] = y[:, i]\n"},{"cell_type":"code","metadata":{"collapsed":true},"outputs":[],"execution_count":null,"source":"result.to_csv('fastText_result.csv', index=False)"},{"cell_type":"code","metadata":{"collapsed":true},"outputs":[],"execution_count":null,"source":""}]}