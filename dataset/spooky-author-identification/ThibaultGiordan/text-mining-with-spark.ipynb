{"nbformat_minor":1,"metadata":{"language_info":{"file_extension":".py","version":"3.6.3","codemirror_mode":{"version":3,"name":"ipython"},"mimetype":"text/x-python","name":"python","pygments_lexer":"ipython3","nbconvert_exporter":"python"},"kernelspec":{"display_name":"Python 3","name":"python3","language":"python"}},"cells":[{"metadata":{"_uuid":"a8bd8014c05a6352a3ecd75024354c5cddbe6310","_cell_guid":"db9be573-d8ae-4c47-a3ec-165180ba3fba"},"source":"Hi everyone !<br/>\nFor this Kernel I want to make a tutorial about Text-mining with Spark. What's Spark ? What's Text-mining ? Why Spark could be interesting in Kaggle challenge ? And why Spark and Text-mining are good together ? Are the questions I will try to anwser.\n<br/>\nI will not make data visualization in this tutorial, I'm pretty sure a lot of people are far better than me for visualization. Unfortunatelly this notebook will not run on Kaggle because you need to install Spark.\n<br/>\n<h1> What's Apache Spark </h1>\nSpark ( https://spark.apache.org/ ) is a framework for distributed operations (basically the opposite of using one core on one computer) it works with several language : Java,Scala,R and Python. You use it with Hadoop when you have a too many data for one computer. So why create a tutorial for this challenge with only few data ? Well, when I look at all big challenge on Kaggle I'm pretty sure that some people do feature engineering and even ML with Spark in order to gain some time, and I really think that Spark will be more and more popular for ML and Kaggle challenge and I want to help people to begin with Spark.<br/>\n<br/>\nFirst you need to know that generelly Spark run with Hadoop on several computer but you can easly install it on your own computer (there are a lot of tutorial on internet) but remember the main purpose of Spark is to run on several computer.<br/>\n<br/>\n<h4> MapReduce operation </h4>\nSpark use the programming model mapreduce. A MapReduce program is composed of a Map() procedure (method) and a Reduce() procedure (method). In our case you (for data manipulation on dataset) Map is an operation that apply a function on each row of your dataset (example : replace the id by 1 on every line) and Reduce an operation that apply a function for combining your row (example : sum all the id of our data).<br/>\n<br/>\nIn this notebook I would only use map so basically when you read :<br/>\nexample_2=example_1.map(lambda x : normalize_token(x))<br/>\nIt mean \"apply function normalize_token on every row of dataset example_1 this new dataset name is example_2\"<br/>\n<br/>\nSo if I have 1 computer with 4 core the function will apply on 4 lines simultaneously, if I have 5 computer with 4 core on each the function will apply on 20 lines simultaneously !<br/>\n<br/>\n<h4> RDD VS Dataframe </h4>\nSince the beginning I'm speaking about dataset but it's not really correct, you need to know they are two different way to store your data in spark, both are distributed (your data are on several computer ).<br/>\nBasically Dataframe are similar to Pandas dataframe, with column, row a lot of function that already exist and are fast(example : droping NA) but you can't store every type and do what you want (a line with more element than an other etc..)<br/>\nOn the other side you have RDD (Resilient Distributed Dataset) basically it's just a big array and every element(row) is a tuple(similar to an array), you can create your own function with MapReduce, all element(row) can have different size, and no one care about the type of your object in your RDD.<br/>\nYou can easly switch RDD to DF and more easly DF to RDD, we will use both because both have advantages.<br/>\n<br/>\n<h1> Text-mining 101 </h1>\n<br/>\nIn order to apply a Machine-learning algorithms you need a vector of number, so we need to transform our text into a vector. Classique methods that do this transformation need a BoW (bag of words) basically an array with word that help to classify the sentence.<br/>\nBut before tranforming our text into vector we need to clean it. <br/>\n<br/>\n<h4> Cleaning your text </h4>\nIndeed there are some useless words like \"the\", \"I\",\"of\" etc... they are use too often or they doesn't mean precise things, they are call \"stopwords\". <br/>\nIt's not the only transformation we can do, it's also important to change word to their lemma, basically change 'is','are'... to \"be\" and change \"rows\" to \"row\", this way word with the same meaning would write the same way.<br/>\n<br/>\nWe can do more transformation like replace some word/acronym by other but I think you get the idea : you need to keep/extract information and reduce noise of your data.<br/>\n<br/>\nAll those transformation exist with NLTK (natural language toolkit) a free library for text-mining in python<br/>\n<br/>\nAfter all this transformation you get an array of word (or BoW) now you can transform it into a vector of word<br/>\n<br/>\n<h4> BoW to vector </h4>\n<br/>\nThere are two popular method for this, word2vec and tf-idf. I will not explain them they are a lot of explaination on internet and english is not my main language. But you juste need to know that :<br/>\n- W2V use the way word are used in order to change word into vectors and have similar vector for word that are used in a similar way<br/>\n- tf-idf mean term frequency inverse document frequency, for each phrase it create a vector of the size of your vocabulary and give them an important weight if they appear few time.<br/>\n<br/>\nBoth methods can be good (like every machine learning algorithms)<br/>\nOnce you have change your BoW to a vector you just need to apply a RandomForest or other and it's done.<br/>\n<br/>\n<h1> Why Spark is good with text-mining </h1>\nBy reading this tutorial you probably realise that we transform a lot our data for text-mining challenge, and all those transformation are easy to parallelize. Indeed it's just applying a function on each row (so a map). So if you start a Text-mining challenge with billion of line, Spark would be really faster than using one core.<br/>\n<br/>\nAfter explaining the basics lets have a look at the code","cell_type":"markdown"},{"source":"import nltk\nimport numpy as np\nfrom nltk.tokenize import TreebankWordTokenizer\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nfrom nltk.stem import WordNetLemmatizer\nimport string\nsc","metadata":{"collapsed":true,"_uuid":"b9a769df73c1d699b0b7b47bf526293e9704e705","_cell_guid":"d87f92d5-cd21-408d-87ad-67eeccb1efdc"},"outputs":[],"cell_type":"code","execution_count":null},{"metadata":{"_uuid":"3a4588e8a90bbae51fc600b9b21024e01c179718","_cell_guid":"c84dcc2a-d7e4-4961-a8ec-3d5112b5841b"},"source":"First let's load our training data","cell_type":"markdown"},{"source":"df=spark.read.csv(\"train.csv\",header=True)\ndf.show()","metadata":{"collapsed":true,"_uuid":"8e1e100f183bf5d28b010ba70b1568a802ad86c3","_cell_guid":"e2bb89a3-07c2-4bfb-ae54-82765c0f7085","scrolled":true},"outputs":[],"cell_type":"code","execution_count":null},{"metadata":{"_uuid":"3d5dd0ec71930ad1f073aa389266bdcd68f743e5","_cell_guid":"f189dd48-911b-4b55-9268-ce558a64996d"},"source":"In this function we create a Bag of word with our sentence, and count the number of char (it could be an interessting feature)","cell_type":"markdown"},{"source":"def normalize_token(x) :\n    tokenizer = TreebankWordTokenizer()\n    lower=x.text.lower()\n    text_token=tokenizer.tokenize(lower)\n    count_char=len(lower)\n    return x+(count_char,text_token,)\n\nrdd_normalize_token=df.rdd.map(lambda x : normalize_token(x))\nrdd_normalize_token.first()","metadata":{"collapsed":true,"_uuid":"4911bc0c82564b7fcb56853fd7f690cfb7ad3a94","_cell_guid":"a2fd1ee9-3266-4408-9912-915a357f3f32","scrolled":true},"outputs":[],"cell_type":"code","execution_count":null},{"metadata":{"_uuid":"4930b48716143f01d615ac0d8e1acd95abf9d59c","_cell_guid":"b11ba34b-a6ae-4240-ae0d-37db8abac3c4"},"source":"In this function we remove stopword, little word and punctuation, we count the number of character that are punctuation and replace all number by \"number\" (I want to keep the information that an author can use a lot of number) in order to normalize them","cell_type":"markdown"},{"source":"def remove_stop_word_and_changer_number(x,seuil,stop_words,list_punct) :\n    tab_result=[]\n    count_punct=0\n    for elt in x[4] :\n        if elt in list_punct :\n            count_punct=count_punct+1\n        try :\n            number=float(elt)\n            tab_result.append(\"number\")\n        except :\n            if (len(elt)>seuil) &(elt not in stop_words) :\n                tab_result.append(elt)            \n    return x[:4]+(count_punct,tab_result,)\n    \nseuil=1\nstop_words=set(stopwords.words('english'))\nlist_punct=list(string.punctuation)\nrdd_stop_word=rdd_normalize_token.map(lambda x : remove_stop_word_and_changer_number(x,seuil,stop_words,list_punct))\nrdd_stop_word.first()","metadata":{"collapsed":true,"_uuid":"a3d82c9dccbf2dc33d98bec53174fbb6bea26057","_cell_guid":"8a7caa4e-213b-485f-a9ac-8a1f25e846db","scrolled":true},"outputs":[],"cell_type":"code","execution_count":null},{"metadata":{"_uuid":"39068aec3fac550b79292b62eab97cedd7fd40a3","_cell_guid":"1dbbb563-68e6-4634-b053-24a0f4df61f1"},"source":"We lemmatize our BoW with NLTK, we use POSTagging to help lemmatization","cell_type":"markdown"},{"source":"def lemmatisation(x,dict_cor) :\n    tab_result=[]\n    wordnet_lemmatizer = WordNetLemmatizer()\n    pos_tmp = nltk.pos_tag(x[5])\n    for elt in pos_tmp :\n        if elt[1][0] in dict_cor :\n            attrib=dict_cor[elt[1][0]]\n        else :\n            attrib = \"n\"\n        tab_result.append(wordnet_lemmatizer.lemmatize(elt[0], pos=attrib))\n    return x[:5]+(tab_result,)\n        \ndict_cor={\n    \"N\" : \"n\",\n    \"V\" : \"v\",\n    \"J\" : \"r\",\n    \"A\" : \"a\",\n}\n\nrdd_lemma=rdd_stop_word.map(lambda x : lemmatisation(x,dict_cor))\nrdd_lemma.first()","metadata":{"collapsed":true,"_uuid":"bfe2f68368b51f4892ec816785864dd70e39154a","_cell_guid":"8941e9c7-f9d2-46e7-a0e5-a3974b631db8","scrolled":true},"outputs":[],"cell_type":"code","execution_count":null},{"metadata":{"_uuid":"077cc4694630eb15eb610f1df30f21c4e4e72b77","_cell_guid":"b02874f7-2742-4635-8b46-a7de6a17675c"},"source":"We calcul the number of word in our BoW (it could be an interessting feature)","cell_type":"markdown"},{"source":"def size_word(x) :\n    nb_word=len(x[5])\n    return x+(nb_word,)\n\nrdd_size=rdd_lemma.map(lambda x :size_word(x))\nrdd_size.first()","metadata":{"collapsed":true,"_uuid":"facfb721712d108c8342bdf2514de9fea4be18cd","_cell_guid":"c6ae4f0b-d08f-4e49-8679-c5b9d3f610f7","scrolled":true},"outputs":[],"cell_type":"code","execution_count":null},{"metadata":{"_uuid":"798861516b11770b38492a845881954d6a852fc7","_cell_guid":"47164835-6de1-4b6b-bfbb-d30cd11458bc"},"source":"After all our modification we transform our RDD to a dataframe","cell_type":"markdown"},{"source":"df_final=rdd_size.toDF([\"id\",\"phrase\",\"Author\",\"nb_carac\",\"nb_punct\",\"words\",\"size\"])\nprint df_final.count()\ndf_final.show()","metadata":{"collapsed":true,"_uuid":"5ffa687a73fa4742048d68da9ac541969a9e71f7","_cell_guid":"606b18ba-a894-486a-96e3-ed951650836e","scrolled":true},"outputs":[],"cell_type":"code","execution_count":null},{"metadata":{"_uuid":"cd4869cee3b1801fde514126ad1e977da7e1b2cd","_cell_guid":"8a5ba84f-a7c6-443d-91d3-a8945ba2ec3c"},"source":"Now we can save our DF to parquet (a distributed format, more efficient than csv)","cell_type":"markdown"},{"source":"df_final.write.parquet(\"tokenize_03_12_v3\",mode=\"overwrite\")","metadata":{"collapsed":true,"_uuid":"999be78b3b623905e52c9d4b7cb1c28a7a433c9f","_cell_guid":"ce9547e4-2b95-49ac-bc41-65cebdb96dd9","scrolled":false},"outputs":[],"cell_type":"code","execution_count":null},{"metadata":{"_uuid":"2e48905fb5bdc2375301e5cb2ee491904be26ea2","_cell_guid":"757dbfd1-364c-4d21-ba0d-5f9f0f53c2cd"},"source":"All those previous transformation where on training set, let's do the same on test set","cell_type":"markdown"},{"source":"df_test=spark.read.csv(\"test.csv\",header=True)\n #The map is important in order to have the same number of column as train set\ndf_test_1=df_test.rdd.map(lambda x : x+(1,)).toDF([\"id\",\"text\",\"author\"])\ndf_test_2=df_test_1.rdd.map(lambda x : normalize_token(x))\ndf_test_3=df_test_2.map(lambda x : remove_stop_word_and_changer_number(x,seuil,stop_words,list_punct))\ndf_test_4=df_test_3.map(lambda x : lemmatisation(x,dict_cor))\ndf_test_5=df_test_4.map(lambda x :size_word(x))\ndf_final_test=df_test_5.toDF([\"id\",\"phrase\",\"Author\",\"nb_carac\",\"nb_punct\",\"words\",\"size\"]).drop(\"Author\")\nprint df_final_test.count()\nprint df_final_test.show()\ndf_final_test.write.parquet(\"tokenize_test_03_12_v3\",mode=\"overwrite\")","metadata":{"collapsed":true,"_uuid":"a76f9f443bcc7f03f5a9b1533768f01ba82ac5fb","_cell_guid":"69b23e46-6d59-4614-8ccf-b717300c6057"},"outputs":[],"cell_type":"code","execution_count":null},{"metadata":{"collapsed":true,"_uuid":"dd267e9da9534c06667776130cac15eba8d55ed6","_cell_guid":"a906452a-efdf-4b22-a575-96e8a1928445"},"source":"Now we can start Machine-learning","cell_type":"markdown"},{"source":"import nltk\nimport numpy as np\nfrom nltk.tokenize import TreebankWordTokenizer\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nfrom nltk.stem import WordNetLemmatizer\nfrom pyspark.ml.feature import HashingTF, IDF, Tokenizer\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.classification import RandomForestClassifier\nfrom pyspark.ml.feature import IndexToString, StringIndexer, VectorIndexer\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator\nfrom pyspark.ml.classification import NaiveBayes\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator\nfrom pyspark.ml.feature import Word2Vec\nfrom pyspark.ml.regression import RandomForestRegressor\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.classification import LogisticRegression\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator,  RegressionEvaluator\nfrom pyspark.ml.feature import HashingTF, Tokenizer\nfrom pyspark.ml.tuning import CrossValidator, ParamGridBuilder\nfrom pyspark.ml.linalg import Vectors, VectorUDT","metadata":{"collapsed":true,"_uuid":"a7b761a4234b2361dbe6516afc08bf45bfc99eaa","_cell_guid":"0f269929-7f64-4df8-b658-da185d4f93b4"},"outputs":[],"cell_type":"code","execution_count":null},{"metadata":{"_uuid":"3b8fe605ebc9ea9cce9b77e41ae47be2e7aace1c","_cell_guid":"cca510f3-2f42-4926-9924-098ea37136e3"},"source":"We load our data","cell_type":"markdown"},{"source":"df = spark.read.parquet(\"tokenize_03_12_v3\")\ndf_test = spark.read.parquet(\"tokenize_test_03_12_v3\")\ndf.show()","metadata":{"collapsed":true,"_uuid":"482151269b9077c0398000b23620ab1f7cb11ba9","_cell_guid":"f303bd20-0299-4b6f-9eb2-ed8bc1260254"},"outputs":[],"cell_type":"code","execution_count":null},{"metadata":{"_uuid":"32dc6b9ea6cca5fca1f6c03bacb3304abdb16c52","_cell_guid":"a3335a2e-ad58-4de6-b70e-ba047a996ac1"},"source":"Let's create different categorie for each author, we would then create one model for each author","cell_type":"markdown"},{"source":"def add_label(x) :\n    if x.Author==\"EAP\" :\n        EAP=1\n        HPL=0\n        MWS=0\n    elif x.Author==\"HPL\" :\n        EAP=0\n        HPL=1\n        MWS=0\n    elif x.Author==\"MWS\" :\n        EAP=0\n        HPL=0\n        MWS=1\n    else :\n        EAP=0\n        HPL=0\n        MWS=0\n    return x+(EAP,HPL,MWS)\n\nrdd_label=df.rdd.map(lambda x : add_label(x))\ndf_add_label=rdd_label.toDF([\"id\",\"phrase\",\"Author\",\"nb_carac\",\"nb_punct\",\"words\",\"size\",\"label_EAP\",\"label_HPL\",\"label_MWS\"])\ndf_add_label.show()","metadata":{"collapsed":true,"_uuid":"fb75fc099b8ad4f9c39ebcb8d99cd4a887d2ee74","_cell_guid":"08c8fe09-7f02-44dd-8f21-9f54cf670919"},"outputs":[],"cell_type":"code","execution_count":null},{"metadata":{"_uuid":"ec1c92fa759ce0b17c6e7e77eec7703a9d8594d5","_cell_guid":"392e10be-d885-47ad-a6a3-3a583b6eea21"},"source":"They are several way to transform a BoW to an vector, tf-idf and word2Vec are pretty popular, I get better result whith w2v (train on test corpus and train corpus). But you can try with tf-idf, w2v or even both (concatenation is just after)","cell_type":"markdown"},{"source":"method=\"both\"\n\nif method==\"tf-idf\" :\n    hashingTF = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\", numFeatures=256)\n    featurizedData = hashingTF.transform(df_add_label)\n\n    idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n    idfModel = idf.fit(featurizedData)\n    rescaledData = idfModel.transform(featurizedData)\n\nelif method==\"w2v\" :\n    word2Vec = Word2Vec(vectorSize=100, minCount=1, inputCol=\"words\", outputCol=\"features\",seed=42, maxIter=20)\n    df_all_words=df_test.select(\"words\").union(df_add_label.select(\"words\"))\n    model_w2v = word2Vec.fit(df_all_words)\n\n    rescaledData = model_w2v.transform(df_add_label)\n    \nelif method==\"both\" :\n    hashingTF = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\", numFeatures=256)\n    featurizedData = hashingTF.transform(df_add_label)\n\n    idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features_tf_idf\")\n    idfModel = idf.fit(featurizedData)\n    rescaledData_tmp = idfModel.transform(featurizedData)\n    \n    word2Vec = Word2Vec(vectorSize=100, minCount=1, inputCol=\"words\", outputCol=\"features\",seed=42, maxIter=20)\n    df_all_words=df_test.select(\"words\").union(df_add_label.select(\"words\"))\n    model_w2v = word2Vec.fit(df_all_words)\n\n    rescaledData = model_w2v.transform(rescaledData_tmp)\n    \nrescaledData.show()","metadata":{"collapsed":true,"_uuid":"d0cb92f878eb5106736b265717da41b7aaf15c82","_cell_guid":"42bc930d-118b-4486-b3a4-a2fed3bc94d8"},"outputs":[],"cell_type":"code","execution_count":null},{"metadata":{"_uuid":"4a93fb0bafea193b77ee38b0ed3db013c96fdc11","_cell_guid":"22972c72-6f15-4945-86bb-b100d802d12a"},"source":"Concatenate all feature (from w2v and/or tf-idf and external caracter) in order to train a model","cell_type":"markdown"},{"source":"if method==\"both\" :\n    df_ML=rescaledData.rdd.map(lambda x : x[:-1]+(Vectors.dense(list(x.features)+list(x.features_tf_idf) + [x.size,x.nb_carac,x.nb_punct]),))\\\n    .toDF(rescaledData.columns)\nelse :\n    df_ML=rescaledData.rdd.map(lambda x : x[:-1]+(Vectors.dense(list(x.features) + [x.size,x.nb_carac,x.nb_punct]),))\\\n    .toDF(rescaledData.columns)\ndf_ML.first()","metadata":{"collapsed":true,"_uuid":"4f4d232f4613b0ffd0385b5c5223feedcad57dd2","_cell_guid":"d9bbc63d-2b3c-4281-8ba3-e1bedc5abbd6"},"outputs":[],"cell_type":"code","execution_count":null},{"metadata":{"_uuid":"d0495c42945221287f7ff2bfce59d0dd52be8797","_cell_guid":"b69d92ad-e123-4e94-b9e2-319edeb4a4a0"},"source":"Now we train a model for each author, with cross validation and parameter tuning","cell_type":"markdown"},{"source":"trainingData=rescaledData\n#print trainingData.first()\n\nnb_eap = RandomForestRegressor(featuresCol=\"features\", predictionCol=\"EAP\",labelCol=\"label_EAP\", maxDepth=10, numTrees=20)\n\nparamGrid = ParamGridBuilder() \\\n    .addGrid(nb_eap.numTrees, [ 40,50,60]) \\\n    .addGrid(nb_eap.maxDepth, [ 5,10]) \\\n    .build()\n\ncrossval = CrossValidator(estimator=nb_eap,\n                          estimatorParamMaps=paramGrid,\n                          evaluator= RegressionEvaluator(predictionCol=\"EAP\",labelCol=\"label_EAP\"),\n                          numFolds=3)\n\nmodel_eap=crossval.fit(trainingData)","metadata":{"collapsed":true,"_uuid":"b2769d1227909153f3a3b58fbd083360fb0b8718","_cell_guid":"c2912f03-e525-4a9f-a1b8-75fd84b95ceb"},"outputs":[],"cell_type":"code","execution_count":null},{"source":"trainingData=rescaledData\n#print trainingData.first()\n\nnb_hpl = RandomForestRegressor(featuresCol=\"features\",labelCol=\"label_HPL\", predictionCol=\"HPL\", maxDepth=10, numTrees=20)\n\nparamGrid = ParamGridBuilder() \\\n    .addGrid(nb_hpl.numTrees, [40,50,60]) \\\n    .addGrid(nb_hpl.maxDepth, [ 5,10]) \\\n    .build()\n\ncrossval = CrossValidator(estimator=nb_hpl,\n                          estimatorParamMaps=paramGrid,\n                          evaluator= RegressionEvaluator(labelCol=\"label_HPL\", predictionCol=\"HPL\"),\n                          numFolds=3)\n\nmodel_hpl=crossval.fit(trainingData)","metadata":{"collapsed":true,"_uuid":"5146cc65b121133ea1d4f67a3a74f4e84380020f","_cell_guid":"dec1ba1d-823f-4ce7-8424-349a0d3d3270"},"outputs":[],"cell_type":"code","execution_count":null},{"source":"trainingData=rescaledData\n#print trainingData.first()\n\nnb_mws = RandomForestRegressor(featuresCol=\"features\",labelCol=\"label_MWS\", predictionCol=\"MWS\", maxDepth=10, numTrees=20)\n\nparamGrid = ParamGridBuilder() \\\n    .addGrid(nb_mws.numTrees, [40,50,60]) \\\n    .addGrid(nb_mws.maxDepth,  [5,10]) \\\n    .build()\n\ncrossval = CrossValidator(estimator=nb_mws,\n                          estimatorParamMaps=paramGrid,\n                          evaluator=RegressionEvaluator(labelCol=\"label_MWS\", predictionCol=\"MWS\"),\n                          numFolds=3)\n\nmodel_mws=crossval.fit(trainingData)","metadata":{"collapsed":true,"_uuid":"b98a5580e46ae977dd06a0b27f703d8c613858b9","_cell_guid":"5aca0488-2349-47a5-a525-23d1bdc6b972"},"outputs":[],"cell_type":"code","execution_count":null},{"source":"print model_eap.bestModel \nprint model_hpl.bestModel \nprint model_mws.bestModel ","metadata":{"collapsed":true,"_uuid":"4f91e6ae6d6da141c006080dc37079b7282db3c4","_cell_guid":"b4b746d8-7747-4123-9895-1a7e4c02517e"},"outputs":[],"cell_type":"code","execution_count":null},{"metadata":{"_uuid":"2fe7273e185ea08cf43c55438681ca31591e7f1c","_cell_guid":"eaa34482-e0b1-46e7-b002-b2fb5b390d45"},"source":"Now we apply our model to the test set","cell_type":"markdown"},{"source":"df_test.show()\nif method==\"tf-idf\" :\n    featurizedData_test = hashingTF.transform(df_test)\n    listfeaturized=featurizedData_test.collect()\n    rescaledData_test = idfModel.transform(featurizedData_test)\n    listidfMode=rescaledData_test.collect()\n    featureIndexer = VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\").fit(rescaledData_test)\n    df_to_predict=featureIndexer.transform(rescaledData_test)\n     df_to_predict=df_to_predict_tmp.rdd.map(lambda x : x[:-1]+(Vectors.dense(list(x.features_tf_idf) + [x.size,x.nb_carac,x.nb_punct]),))\\\n    .toDF(df_to_predict_tmp.columns)\n\nelif method==\"w2v\" :\n    df_to_predict_tmp= model_w2v.transform(df_test)\n    df_to_predict=df_to_predict_tmp.rdd.map(lambda x : x[:-1]+(Vectors.dense(list(x.features) + [x.size,x.nb_carac,x.nb_punct]),))\\\n    .toDF(df_to_predict_tmp.columns)\n    print df_to_predict.first()\n    \nelif method==\"both\" :\n    list0=df_test.collect()\n    featurizedData_test = hashingTF.transform(df_test)\n    rescaledData_test = idfModel.transform(featurizedData_test)\n    featureIndexer = VectorIndexer(inputCol=\"features_tf_idf\", outputCol=\"indexedFeatures\").fit(rescaledData_test)\n    df_to_predict=featureIndexer.transform(rescaledData_test)\n    df_to_predict_tmp= model_w2v.transform(df_to_predict)\n    df_to_predict=df_to_predict_tmp.rdd.map(lambda x : x[:-1]+(Vectors.dense(list(x.features)+list(x.features_tf_idf) + [x.size,x.nb_carac,x.nb_punct]),))\\\n    .toDF(df_to_predict_tmp.columns)\n    print df_to_predict.first()\n\ndf_test_1 = model_eap.transform(df_to_predict).drop('rawPrediction').drop('probability')\ndf_test_2 = model_hpl.transform(df_test_1).drop('rawPrediction').drop('probability')\ndf_test_3 = model_mws.transform(df_test_2).drop('phrase').drop('words').drop('rawFeatures').drop('rawPrediction').drop('probability').drop('features').drop('indexedFeatures')\nprint df_test_3.show()","metadata":{"collapsed":true,"_uuid":"5818cb615724d5c33d47dc8e9d95a9c484cedd95","_cell_guid":"3b9b46d3-7298-4bd1-9a98-813839f46d99"},"outputs":[],"cell_type":"code","execution_count":null},{"metadata":{"_uuid":"946f47a708de6b9ae7dd6e268beef37d3f1f6a1b","_cell_guid":"a6f310b7-6c70-457a-a9d0-0f30ab063e55"},"source":"Just to be sure that we haven't huge value, then select only column for Kaggle","cell_type":"markdown"},{"source":"def change_val(x) :\n    EAP=x.EAP\n    HPL=x.HPL\n    MWS=x.MWS\n    if x.EAP>1 :\n        EAP=1\n    if x.HPL>1 :\n        HPL=1\n    if x.MWS>1 :\n        MWS=1\n    if x.EAP<0 :\n        EAP=0\n    if x.HPL<0 :\n        HPL=0\n    if x.MWS<0 :\n        MWS=0\n    return (x.id,EAP,HPL,MWS)\n\ndf_save=df_test_3.rdd.map(lambda x : change_val(x)).toDF(['id','EAP','HPL','MWS'])","metadata":{"collapsed":true,"_uuid":"cdbc9ad10f0cf115151f490b8d25f163d9dc0193","_cell_guid":"23126f08-cb7c-4b8e-a85b-6029400774c2"},"outputs":[],"cell_type":"code","execution_count":null},{"metadata":{"_uuid":"5f4a82ef94e3818aa563a2344f137e4d09a15f8f","_cell_guid":"ee737c94-8607-4cec-ade3-df6ea5966ea0"},"source":"Now we reduce the distribution of the DF to 1 (in ordre to have one csv) then we save it !","cell_type":"markdown"},{"source":"df_save=df_test_3.coalesce(1)\nprint df_save.count()\nprint df_save.first()","metadata":{"collapsed":true,"_uuid":"64f99565ffd9763c18c38618b78e66b455d061ef","_cell_guid":"fdf921d8-50a1-4562-80d6-2f3b1bb6c850"},"outputs":[],"cell_type":"code","execution_count":null},{"source":"df_save.select(['id','EAP','HPL','MWS']).write.csv(\"result_test_03_12_v4\",sep=\",\",header=True,mode=\"overwrite\")","metadata":{"collapsed":true,"_uuid":"797eee17f06b4cfeb8a0c1c9857e1825613dc7e0","_cell_guid":"058ce720-ca0e-4211-b53c-d9eac049b163"},"outputs":[],"cell_type":"code","execution_count":null},{"metadata":{"_uuid":"af21c94ccc8f32c685dbe23bac2a8b78b77d465b","_cell_guid":"3d8e8d04-2399-4fd1-9822-6e8f1e65c5c3"},"source":"Well I hope this notebook will help you for your text-mining project, I also hope you learn things and want to try Spark, If you have question I will try to anwser them. Thanks for reading !","cell_type":"markdown"},{"source":"","metadata":{"collapsed":true,"_uuid":"7fd98d602a358a552d603a7af29030ebc875a8f4","_cell_guid":"979f4680-268b-4ae5-9f65-0768147ed17b"},"outputs":[],"cell_type":"code","execution_count":null}],"nbformat":4}