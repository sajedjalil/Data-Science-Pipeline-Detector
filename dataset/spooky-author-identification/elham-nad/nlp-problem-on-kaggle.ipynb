{"cells":[{"metadata":{},"cell_type":"markdown","source":"Importing Libraries."},{"metadata":{"_cell_guid":"d46ba3fd-26f1-4635-b2f9-fca916ff3066","_uuid":"21f3ccd962d1556dc2346699d45a29e9ef791367","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport xgboost as xgb\nfrom tqdm import tqdm\nfrom sklearn.svm import SVC\nfrom keras.models import Sequential\nfrom keras.layers.recurrent import LSTM, GRU\nfrom keras.layers.core import Dense, Activation, Dropout\nfrom keras.layers.embeddings import Embedding\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.utils import np_utils\nfrom sklearn import preprocessing, decomposition, model_selection, metrics, pipeline\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import MultinomialNB\nfrom keras.layers import GlobalMaxPooling1D, Conv1D, MaxPooling1D, Flatten, Bidirectional, SpatialDropout1D\nfrom keras.preprocessing import sequence, text\nfrom keras.callbacks import EarlyStopping\nfrom nltk import word_tokenize\nfrom nltk.corpus import stopwords\nstop_words = stopwords.words('english')\nfrom wordcloud import WordCloud, STOPWORDS \nimport matplotlib.pyplot as plt \nimport pandas as pd \nimport nltk \nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\nfrom nltk.stem import SnowballStemmer\nimport matplotlib.pyplot as plt\n#%matplotlib inline  \nimport matplotlib\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\nimport numpy as np\n\nfrom collections import defaultdict\n\nfrom sklearn.naive_bayes import MultinomialNB\nimport matplotlib.pyplot as plt\nfrom sklearn import svm, datasets\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import label_binarize\nfrom sklearn.metrics import roc_curve, auc\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom itertools import cycle\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.dummy import DummyClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import classification_report\n#Learning a classification model\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import LabelEncoder\nimport seaborn # make prettier graphs\nimport re\n#LDA\nfrom gensim.models import LdaMulticore, TfidfModel, CoherenceModel\nfrom gensim.corpora import Dictionary\nimport time # to know how long training took\nimport multiprocessing \nimport matplotlib.pyplot as plt\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.cluster import KMeans\nfrom sklearn.decomposition import PCA\nfrom sklearn.manifold import TSNE\nimport matplotlib.pyplot as plt\n%matplotlib inline  \nimport matplotlib\nfrom sklearn.manifold import TSNE\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom matplotlib import colors\nimport seaborn as sb","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"60326be1-82d1-4677-8ef8-da5b1eac475c","_uuid":"adb496504ab8453ce2b4f91dd6e5f17cbdaf4f68"},"cell_type":"markdown","source":"Let's load the datasets"},{"metadata":{"_cell_guid":"367e0329-7aeb-4f39-b1a9-d7395bdca993","_uuid":"d6ea63db0ad0db09b25c35601391b71564601699","trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/spooky/train.csv')\ntest = pd.read_csv('../input/spooky/test.csv')\ntraincleaned=pd.read_csv('../input/dataclean/data.csv')\n#traincleaned is the train data after cleaning, so I saved and load it here due to simplisity.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#traincleaned[:3].drop(traincleaned.columns[1], axis=1)\ntraincleaned.drop('Unnamed: 0', axis=1,inplace=True)  \ntraincleaned[:3]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"traincleaned.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\n\ndef custom_colours(*args, **kwargs):\n    word = args[0]  # First argument is the word itself\n    if \"o\" in word: \n        return \"#FFD700\"\n    return \"#CCCCCC\"\n\ncomment_words = '' \nstopwords = set(STOPWORDS) \n  \n# iterate through the csv file \nfor val in traincleaned.clean_text: \n      \n    # typecaste each val to string \n    val = str(val) \n  \n    # split the value \n    tokens = val.split() \n      \n    # Converts each token into lowercase \n    for i in range(len(tokens)): \n        tokens[i] = tokens[i].lower() \n      \n    comment_words += \" \".join(tokens)+\" \"\n  \nwordcloud = WordCloud(width = 800, height = 400, \n                stopwords = stopwords, \n                min_font_size = 8,background_color=\"white\",colormap=\"winter\").generate(comment_words) \n  \n# plot the WordCloud image                        \nplt.figure(figsize = (8, 8), facecolor = None) \nplt.imshow(wordcloud) \nplt.axis(\"off\") \nplt.tight_layout(pad = 0) \n  \nplt.show() ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#covid.drop(covid.columns[0], axis=1, inplace=True)\n#covid.head(7)\n\n#covid1=covid[:5000]\n#covid1.shape\n\n#covid1[\"author\"]=\"covid\"\n#covid1[\"id\"]=\"1\"\n#covid1\n\n#data_merge=covid1.append(train, ignore_index=True)\n#data_merge.head(2)\n#data_merge.shape","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"e171d134-bb33-4578-800c-2d65c2edf9c1","_uuid":"c02f2a3e039aad543bc789188fe08422dd78f5c0"},"cell_type":"markdown","source":"A quick look at the data"},{"metadata":{"_cell_guid":"1a9da2ba-2c8c-466d-8ceb-cb1fdb4351a8","_uuid":"0fccc28ade4d126b5279d52b0f24300f8c18e69b","trusted":true},"cell_type":"code","source":"print(train.head(2))\nprint(test.head(1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nstop_words = stopwords.words('english')\nstemmer = SnowballStemmer('english')\n\ntext_cleaning_re = \"@\\S+|https?:\\S+|http?:\\S|[^A-Za-z0-9]+\"\ndef preprocess(text, stem=False):\n  text = re.sub(text_cleaning_re, ' ', str(text).lower()).strip()\n  #text = re.sub(r\"(.)\\1+\", \"\\\\1\", str(text).lower()).strip()\n  tokens = []\n  for token in text.split():\n    if token not in stop_words:\n      if stem:\n        tokens.append(stemmer.stem(token))\n      else:\n        tokens.append(token)\n  return \" \".join(tokens)\n\n\n\n\n\n#train['text'] =  train['text'].apply(str)\ntest['text'] =  test['text'].apply(str)\n\n#train['text'] = train['text'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>2]))\ntest['text'] = test['text'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>2]))\n\n#train['text'] = train['text'].str.replace(\"[^a-zA-Z#]\", \" \")\ntest['text'] = test['text'].str.replace(\"[^a-zA-Z#]\", \" \")\n\nimport re\n#train.text = train.text.apply(lambda x: preprocess(x))\ntest.text = test.text.apply(lambda x: preprocess(x))\n\n\n\n\nimport spacy\nnlp = spacy.load('en', disable=['parser', 'ner'])\n\ndef clean_text(text):\n    '''reduce text to lower-case lexicon entry'''\n    lemmas = [token.lemma_ for token in nlp(text) \n              if token.pos_ in {'NOUN', 'VERB', 'ADJ', 'ADV', 'PROPN'}]\n    return ' '.join(lemmas)\n\n\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#train['clean_text'] = train['text'].apply(clean_text)\n#test['clean_text'] = test['text'].apply(clean_text)\n#test['clean_text'].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#path = \"/Users/elhamnadimi/Downloads/nlp\"\n\n#test.to_csv('testclean.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_sentence=traincleaned[:15663]\nprint(train_sentence.shape)\nevalu = traincleaned.tail(3916)\nprint(evalu.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_list=train_sentence.clean_text.apply(str).tolist()\ntest_list=evalu.clean_text.apply(str).tolist()\ntest_list[:2]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_list_1=traincleaned.clean_text.apply(str).tolist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_sentence['clean_text'] = train_sentence['clean_text'].astype(str)\n#train_sentence","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"instances = traincleaned.clean_text.astype(str).apply(str.split)\n\ninstances.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"type(instances)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"instances2 = train_sentence.clean_text.astype(str).apply(str.split)\n\ninstances2.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import cufflinks as cf\ncf.go_offline()\ncf.set_config_file(offline=False, world_readable=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef get_top_n_bigram(corpus, n=None):\n    vec = CountVectorizer(ngram_range=(2, 2)).fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]\ncommon_words = get_top_n_bigram(traincleaned['clean_text'].astype(str), 20)\n#for word, freq in common_words:\n    #print(word, freq)\ndf3 = pd.DataFrame(common_words, columns = ['ReviewText' , 'count'])\ndf3.set_index(\"ReviewText\", inplace = True)\n\n#df3.groupby('ReviewText').sum()['count'].sort_values(ascending=False).iplot(kind='bar', yTitle='Count', linecolor='black', title='Top 20 bigrams in review before removing stop words')\nplt.figure(figsize=(50,10)) \n\n\ndf3.plot(kind=\"bar\",width=0.5, color=(0.1, 0.1, 0.1, 0.4))\nplt.title(\"Top 20 bigram\",fontsize=15)\nplt.xlabel(\"Bigrams\",fontsize=15)\nplt.ylabel(\"Frequency\",fontsize=15)\n\nfig = matplotlib.pyplot.gcf()\nfig.set_size_inches(10.5, 5.5)\n\nplt.xticks(fontsize=15)\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_top_n_words(corpus, n=None):\n    vec = CountVectorizer(stop_words = 'english').fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]\ncommon_words = get_top_n_words(traincleaned['clean_text'].astype(str), 20)\n#for word, freq in common_words:\n#    print(word, freq)\ndf2 = pd.DataFrame(common_words, columns = ['ReviewText' , 'count'])\ndf2.set_index(\"ReviewText\", inplace = True)\n#df3.groupby('ReviewText').sum()['count'].sort_values(ascending=False).iplot(kind='bar', yTitle='Count', linecolor='black', title='Top 20 bigrams in review before removing stop words')\nplt.figure(figsize=(50,10)) \ndf2.plot(kind=\"bar\",width=0.5, color=(0.1, 0.1, 0.1, 0.4))\nplt.title(\"Top 20 unigram\",fontsize=15)\nplt.xlabel(\"Unigram\",fontsize=15)\nplt.ylabel(\"Frequency\",fontsize=15)\nfig = matplotlib.pyplot.gcf()\nfig.set_size_inches(10.5, 5.5)\nplt.xticks(fontsize=15)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tfidf = TfidfVectorizer(analyzer='word',\n                        ngram_range=(1,2),min_df = 0.001,\n                        max_df = 0.80,max_features = 8000,\n                        stop_words = 'english',use_idf=True)\ntfidf_results = tfidf.fit_transform(data_list_1)\nprint(tfidf_results.shape)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_tf = tfidf_results\ny_tf = traincleaned[\"clean_text\"].values\ny=traincleaned[\"clean_text\"].astype(str)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\nselector = SelectKBest(chi2, k=1500).fit(tfidf_results,y)\nX_sel = selector.transform(tfidf_results)\nprint(X_sel.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_sel.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(tfidf.vocabulary_['bad'])\nprint(tfidf.get_feature_names()[905])\n#print(len(train_sentence[train_sentence.clean_text.str.contains('bad')]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":" # this also works with sparse matrices\n\n# set number of latent components\nk = 30\n\nsvd = TruncatedSVD(n_components=k)\n%time U = svd.fit_transform(X_sel)\nS = svd.singular_values_\nV = svd.components_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_sel","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(U.shape, S.shape, V.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef show_topics(A, vocabulary, topn=5):\n    \"\"\"\n    find the top N words for each of the latent dimensions (=rows) in a\n    \"\"\"\n    topic_words = ([[vocabulary[i] for i in np.argsort(t)[:-topn-1:-1]]\n                    for t in A]) # for each row\n    return [', '.join(t) for t in topic_words]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\nt = V[0]\ntopn = 10\nvocabulary = tfidf.get_feature_names()\n[vocabulary[i] for i in np.argsort(t)[:-topn-1:-1]]\n\n# t[1937]\n# t[2366]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#terms = tfidf.get_feature_names()\n\n#show_topics(V, terms)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# most important words for each topic\nvocab = tfidf.get_feature_names()\n\nfor i, comp in enumerate(svd.components_):\n    vocab_comp = zip(vocab, comp)\n    sorted_words = sorted(vocab_comp, key= lambda x:x[1], reverse=True)[:5]\n    print(\"Topic \"+str(i)+\": \")\n    for t in sorted_words:\n        print(t[\n            0],end=\" \")\n    print(\"\\n\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\nsb.set_context('notebook')\n\ndef plot_vectors(vectors, title='VIZ', labels=None, dimensions=3):\n    \"\"\"\n    plot the vectors in 2 or 3 dimensions. \n    If labels are supplied, use them to color the data accordingly\n    \"\"\"\n    # set up graph\n    fig = plt.figure(figsize=(10,10))\n\n    # create data frame\n    df = pd.DataFrame(data={'x':vectors[:,0], 'y': vectors[:,1]})\n    # add labels, if supplied\n    if labels is not None:\n        df['label'] = labels\n    else:\n        df['label'] = [''] * len(df)\n\n    # assign colors to labels\n    cm = plt.get_cmap('tab10') # choose the color palette\n    n_labels = len(df.label.unique())\n    label_colors = [cm(1. * i/n_labels) for i in range(n_labels)]\n    cMap = colors.ListedColormap(label_colors)\n        \n    # plot in 3 dimensions\n    if dimensions == 3:\n        # add z-axis information\n        df['z'] = vectors[:,2]\n        # define plot\n        ax = fig.add_subplot(111, projection='3d')\n        frame1 = plt.gca() \n        # remove axis ticks\n        frame1.axes.xaxis.set_ticklabels([])\n        frame1.axes.yaxis.set_ticklabels([])\n        frame1.axes.zaxis.set_ticklabels([])\n\n        # plot each label as scatter plot in its own color\n        for l, label in enumerate(df.label.unique()):\n            df2 = df[df.label == label]\n            color_values = [label_colors[l]] * len(df2)\n            ax.scatter(df2['x'], df2['y'], df2['z'], \n                       c=color_values, \n                       cmap=cMap, \n                       edgecolor='black', \n                       label=label, \n                       alpha=0.4, \n                       s=100)\n      \n    # plot in 2 dimensions\n    elif dimensions == 2:\n        ax = fig.add_subplot(111)\n        frame1 = plt.gca() \n        frame1.axes.xaxis.set_ticklabels([])\n        frame1.axes.yaxis.set_ticklabels([])\n\n        for l, label in enumerate(df.label.unique()):\n            df2 = df[df.label == label]\n            color_values = [label_colors[l]] * len(df2)\n            ax.scatter(df2['x'], df2['y'], \n                       c=color_values, \n                       cmap=cMap, \n                       edgecolor='black', \n                       label=label, \n                       alpha=0.4, \n                       s=100)\n\n    else:\n        raise NotImplementedError()\n\n    plt.title(title)\n#     plt.legend()\n    plt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.manifold import TSNE\n\n\nZ = X_sel[:400].toarray()\ntsne_lsa_model = TSNE(n_components=2, perplexity=50, learning_rate=100, \n                        n_iter=2000, verbose=1, random_state=0, angle=0.75)\ntsne_lsa_vectors = tsne_lsa_model.fit_transform(Z)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\nZ = X_sel[:500].toarray()\ncategories = traincleaned.author[:500]\nX_embedded = TSNE(n_components=3).fit_transform(Z)\nplot_vectors(X_embedded, title='TSNE', labels=categories, dimensions=2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\nnum_clusters = 10\nnum_seeds = 10\nmax_iterations = 300\nlabels_color_map = {\n    0: '#20b2aa', 1: '#ff7373', 2: '#ffe4e1', 3: '#005073', 4: '#4d0404',\n    5: '#ccc0ba', 6: '#4700f9', 7: '#f6f900', 8: '#00f91d', 9: '#da8c49'\n}\npca_num_components = 2\ntsne_num_components = 2\n\n# texts_list = some array of strings for which TF-IDF is being computed\n\n# calculate tf-idf of texts\n#tf_idf_vectorizer = TfidfVectorizer(analyzer=\"word\", use_idf=True, smooth_idf=True, ngram_range=(2, 3))\n#tf_idf_matrix = tf_idf_vectorizer.fit_transform(texts_list)\n\n# create k-means model with custom config\nclustering_model = KMeans(\n    n_clusters=num_clusters,\n    max_iter=max_iterations,\n    precompute_distances=\"auto\",\n    n_jobs=-1\n)\n\nlabels = clustering_model.fit_predict(X_sel[:4500])\n# print labels\n\nX = X_sel[:4500].todense()\n\n# ----------------------------------------------------------------------------------------------------------------------\n\nreduced_data = PCA(n_components=pca_num_components).fit_transform(X)\n# print reduced_data\nfig = plt.figure(figsize = (10, 10))\nax = plt.axes()\nfor index, instance in enumerate(reduced_data):\n    # print instance, index, labels[index]\n   \n    \n    pca_comp_1, pca_comp_2 = reduced_data[index]\n    color = labels_color_map[labels[index]]\n    ax.scatter(pca_comp_1, pca_comp_2, c=color)\n    ax.legend()\n    plt.title(\"Clustring topics -Kmean\",fontsize=20)\n    plt.xlabel(\"PCA 0\",fontsize=20)\n    plt.ylabel(\"PCA 1\",fontsize=20)\n    \nplt.show()\n\n\n\n\n\n    \n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# t-SNE plot\n#embeddings = TSNE(n_components=tsne_num_components)\n\n#Y = embeddings.fit_transform(X[:500])\n#plt.scatter(Y[:, 0], Y[:, 1], cmap=plt.cm.Spectral)\n#plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_features = traincleaned.iloc[:,1:23]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\n# read in instances and create Dictionary object w information about frequencies etc. \ndictionary = Dictionary(instances)\n# get rid of words that are too rare or too frequent\ndictionary.filter_extremes(no_below=50, no_above=0.5)\nprint(dictionary, flush=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#replace words by their numerical IDs and their frequency\nprint(\"translating corpus to IDs\", flush=True)\nldacorpus = [dictionary.doc2bow(text) for text in instances]\n# learn TFIDF values from corpus\nprint(\"tf-idf transformation\", flush=True)\ntfidfmodel = TfidfModel(ldacorpus)\n# transform raw frequencies into TFIDF\nmodel_corpus = tfidfmodel[ldacorpus]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(instances[0])\nprint(ldacorpus[0]) \nprint(model_corpus[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#list(dictionary.items())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"coherence_values = []\n\ndev_size = 14000\neval_size = 5000\n\nfor num_topics in range(5, 15):\n    model = LdaMulticore(corpus=model_corpus[:dev_size], \n                         id2word=dictionary, \n                         num_topics=num_topics)\n\n    coherencemodel_umass = CoherenceModel(model=model, \n                                          texts=instances[dev_size:dev_size+eval_size], \n                                          dictionary=dictionary, \n                                          coherence='u_mass')\n\n    coherencemodel_cv = CoherenceModel(model=model, \n                                       texts=instances[dev_size:dev_size+eval_size], \n                                       dictionary=dictionary, \n                                       coherence='c_v')\n\n    umass_score = coherencemodel_umass.get_coherence()\n    cv_score = coherencemodel_cv.get_coherence()\n    \n    print(num_topics, umass_score, cv_score)\n    coherence_values.append((num_topics, umass_score, cv_score))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nscores = pd.DataFrame(coherence_values, columns=['num_topics', 'UMass', 'CV'])\n\nplt.figure(figsize=(50,10)) \nscores.plot(kind=\"line\",x='num_topics',\n                 y='UMass',marker='o', markerfacecolor='blue', markersize=12, color='skyblue', linewidth=4)\n\nplt.title(\"Topic coherance: Determining optimal number of topics \",fontsize=15)\nplt.ylabel(\"Coherance score\",fontsize=15)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nplt.figure(figsize=(50,10)) \nscores.plot(kind=\"line\",x='num_topics', y='CV',marker='o', markerfacecolor='blue', markersize=12, color='skyblue', linewidth=4)\n\nplt.title(\"Topic coherance: Determining optimal number of topics \",fontsize=15)\nplt.ylabel(\"Coherance score\",fontsize=15)\nfig = matplotlib.pyplot.gcf()\nfig.set_size_inches(10.5, 5.5)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"num_topics = 6\n\n# find chunksize to make about 200 updates\nnum_passes = 10\nchunk_size = len(model_corpus) * num_passes/200\nprint(chunk_size)\n\nstart = time.time()\nprint(\"fitting model\", flush=True)\nmodel = LdaMulticore(num_topics=num_topics, # number of topics\n                     corpus=model_corpus, # what to train on \n                     id2word=dictionary, # mapping from IDs to words\n                     workers=min(10, multiprocessing.cpu_count()-1), # choose 10 cores, or whatever computer has\n                     passes=num_passes, # make this many passes over data\n                     chunksize=chunk_size, # update after this many instances\n                     alpha=0.5\n                    )\n    \nprint(\"done in {}\".format(time.time()-start), flush=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# transform the data into topic distros\ntopic_corpus = model[model_corpus]\n\ntopic_corpus[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.print_topics()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\n# get the topic descritions\ntopic_sep = re.compile(r\"0\\.[0-9]{3}\\*\") # getting rid of useless formatting\n# extract a list of tuples with topic number and descriptors from the model\nmodel_topics = [(topic_no, re.sub(topic_sep, '', model_topic).split(' + ')) for topic_no, model_topic in\n                model.print_topics(num_topics=num_topics, num_words=5)]\n\ndescriptors = []\nfor i, m in model_topics:\n    print(i+1, \", \".join(m[:10]))\n    descriptors.append(\", \".join(m[:2]).replace('\"', ''))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"descriptors","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{"trusted":true},"cell_type":"code","source":"target_category = 'author'\nlimit = 1000\n# get a list of all the topic scores for each document\nscores = [[t[1] for t in topic_corpus[entry]] for entry in range(limit)]\n# turn that into a data frame with N rows and K columns, each with the score of the corresponding topic\ntopic_distros = pd.DataFrame(data=scores, columns=descriptors)\n# add the review category of each document (so we can aggregate)\ntopic_distros['category'] = train[target_category][:limit]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"topic_distros.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt # make graphs\n\n\nseaborn.set_context('poster') # use large font\n\nfig, ax = plt.subplots(figsize=(20, 10)) # set graph size\n\n\n# aggregate topics by categories\naggregate_by_category = topic_distros[topic_distros.category.isin('EAP HPL MWS'.split())]\naggregate_by_category = aggregate_by_category.groupby(aggregate_by_category.category).mean()\n# plot the graph\naggregate_by_category[descriptors].plot.bar(ax=ax,alpha=0.8,colormap=\"Set2\");\n# move the legend out\nplt.legend(loc='center left', bbox_to_anchor=(1.0, 0.5),fontsize=20);\nax.set_ylabel(\"Count\",fontsize=25)\nax.set_xlabel(\"Authors\",fontsize=25)\nplt.title(\"Topics per Authors\",fontsize=30)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Text classification :","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\n# transform labels into numbers\nlabels2numbers = LabelEncoder()\n\ny1 = labels2numbers.fit_transform(train_sentence['author'])\nprint(train_sentence['author'][:10], y1[:10], len(y1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train_sentence.shape)\nprint(y1.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tfidf = TfidfVectorizer(analyzer='word',\n                        ngram_range=(1,2),min_df = 0.001,\n                        max_df = 0.80,max_features = 8000,\n                        stop_words = 'english',use_idf=True)\ntfidf_results_cl = tfidf.fit_transform(data_list)\nprint(tfidf_results_cl.shape)\n\n\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\n\nselector = SelectKBest(chi2, k=1500).fit(tfidf_results_cl,y1)\nX_sel_lg = selector.transform(tfidf_results_cl)\nprint(X_sel_lg.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"labels2numbers.inverse_transform([1,1,1,0,0,1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\nclassifier_lg = LogisticRegression(class_weight='balanced', solver='lbfgs', multi_class='ovr')\n%time classifier_lg.fit(tfidf_results_cl , y1)\nprint(classifier_lg)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"coefs = classifier_lg.coef_\ncoefs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"k = tfidf.vocabulary_['bad'] # column position for the word\nprint(tfidf.get_feature_names()[k], classifier_lg.coef_[0, k])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\nfor k in [2,3,5,10,15,20]:\n    cv = cross_val_score(LogisticRegression(), tfidf_results_cl, y=y1, cv=k, n_jobs=-1, scoring=\"accuracy\")\n    fold_size = tfidf_results_cl.shape[0]/k\n    \n    print(\"F1 with {} folds for bag-of-words is {}\".format(k, cv.mean()))\n    print(\"Training on {} instances/fold, testing on {}\".format(fold_size*(k-1), fold_size))\n    print()\n    \n    \nscores = pd.Series(cv)\nprint(scores.min(), scores.mean(), scores.max())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nmost_frequent = DummyClassifier(strategy='most_frequent')\n\nprint(cross_val_score(most_frequent, tfidf_results_cl, y=y1, cv=5, n_jobs=-1, scoring=\"f1_micro\").mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y1.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"new_y = labels2numbers.transform(evalu['author'].astype(str))\nnew_y[:30]\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#evalu\nnew_X = tfidf.transform(evalu['clean_text'].astype(str))\nnew_y = labels2numbers.transform(evalu['author'].astype(str))\n\n\n\n# use the old classifier to predict and evaluate\nnew_predictions = classifier_lg.predict(new_X)\nprint(new_predictions)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#test_x = tfidf.transform(test['text'])\n# use the old classifier to predict and evaluate\n#test_predictions = classifier.predict(test_x)\n#print(test_predictions)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"probabilities_lg = classifier_lg.predict_proba(new_X)\nprint(probabilities_lg)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"prediction_lg = classifier_lg.predict(new_X)\nprediction_lg","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"prediction_lg = classifier_lg.predict(new_X)\nprediction_df = pd.DataFrame(data={'author': evalu['author'], 'new_predictions': labels2numbers.inverse_transform(prediction_lg),'truth':evalu['author']})\nprediction_df[:5]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#from sklearn.metrics import confusion_matrix\n#import seaborn as sns\n\n#Get the confusion matrix\n#cf_matrix = confusion_matrix(new_y, new_predictions)\n#sns.heatmap(cf_matrix, annot=True)\n#sns.heatmap(cf_matrix/np.sum(cf_matrix), annot=True, \n#            fmt='.2%', cmap='Blues')\n#print(cf_matrix)\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\n\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    import itertools\n    if normalize:\n        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    print(cm)\n\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() / 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    plt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"new_y2=labels2numbers.inverse_transform(new_y)\nnew_y2\neval_predictions_lg=labels2numbers.inverse_transform(prediction_lg)\neval_predictions_lg\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\ncnf_matrix = confusion_matrix(new_y2, eval_predictions_lg,labels=['EAP', 'HPL', 'MWS'])\nnp.set_printoptions(precision=2)\n\n# Plot non-normalized confusion matrix\nplt.figure()\nplot_confusion_matrix(cnf_matrix, classes=['EAP', 'HPL', 'MWS'],\n                      title='Confusion matrix')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"count_misclassified = (new_y != prediction_lg).sum()\nprint('Misclassified samples: {}'.format(count_misclassified))\n\naccuracy = metrics.accuracy_score(new_y, prediction_lg)\nprint(\"\\nPredicting with {:.2f}% accuracy\".format(accuracy))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y1[:7]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"new_y[:7]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nplt.style.use('ggplot')\n\n\nww = label_binarize(y1, classes=[0, 1, 2])\nn_classes = ww.shape[1]\nnew_y22=label_binarize(new_y, classes=[0, 1, 2])\n\n# Compute ROC curve and ROC area for each class\nfpr = dict()\ntpr = dict()\nroc_auc = dict()\nfor i in range(n_classes):\n    fpr[i], tpr[i], _ = roc_curve(new_y22[:, i], probabilities_lg[:, i])\n    roc_auc[i] = auc(fpr[i], tpr[i])\n# Compute micro-average ROC curve and ROC area\nfpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(new_y22.ravel(), probabilities_lg.ravel())\nroc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n\n# Plot of a ROC curve for a specific class\nplt.figure()\nplt.plot(fpr[2], tpr[2], label='ROC curve (area = %0.2f)' % roc_auc[2])\nplt.plot([0, 1], [0, 1], 'k--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic example')\nplt.legend(loc=\"lower right\")\nplt.show()\n\n# Plot ROC curve\nplt.figure()\nplt.plot(fpr[\"micro\"], tpr[\"micro\"],\n         label='micro-average ROC curve (area = {0:0.2f})'\n               ''.format(roc_auc[\"micro\"]))\nfor i in range(n_classes):\n    plt.plot(fpr[i], tpr[i], label='ROC curve of class {0} (area = {1:0.2f})'\n                                   ''.format(i, roc_auc[i]))\n\nplt.plot([0, 1], [0, 1], 'k--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Some extension of Receiver operating characteristic to multi-class')\nplt.legend(loc=\"lower right\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Naive Bayes for Sentiment Analysis","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\nnb = MultinomialNB()\n%time nb.fit(tfidf_results_cl, y1)\nprint(nb)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"coefnb = nb.coef_\ncoefnb","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"k = tfidf.vocabulary_['bad'] # column position for the word\nprint(tfidf.get_feature_names()[k], nb.coef_[0, k])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\n\nfor k in [2,3,5,10,15,20]:\n    cv2 = cross_val_score(MultinomialNB(), tfidf_results_cl, y=y1, cv=k, n_jobs=-1, scoring=\"f1_micro\")\n    fold_size = tfidf_results_cl.shape[0]/k\n    \n    print(\"F1 with {} folds for bag-of-words is {}\".format(k, cv.mean()))\n    print(\"Training on {} instances/fold, testing on {}\".format(fold_size*(k-1), fold_size))\n    print()\n    \n    scores = pd.Series(cv2)\nprint(scores.min(), scores.mean(), scores.max())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# use the old classifier to predict and evaluate, evaluation set\npredictions_nb = nb.predict(new_X)\nprint(predictions_nb)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(new_y, predictions_nb))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"count_misclassified_nb = (new_y != predictions_nb).sum()\nprint('Misclassified samples: {}'.format(count_misclassified_nb))\n\naccuracy_nb = metrics.accuracy_score(new_y, predictions_nb)\nprint(\"\\nPredicting with {:.2f}% accuracy\".format(accuracy_nb))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#y_pred = model.predict(X_test)\n#f1score = f1_score(y_test, y_pred)\n#print(f\"Tfidf Model Score: {f1score * 100} %\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"probabilities_nb = nb.predict_proba(new_X)\nprint(probabilities_nb)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions_nb = nb.predict(new_X)\nprediction_df = pd.DataFrame(data={'author': evalu['author'], 'new_predictions': labels2numbers.inverse_transform(predictions_nb),'truth':evalu['author']})\nprediction_df[:5]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions_nb","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"new_predictions_nb_=labels2numbers.inverse_transform(predictions_nb)\nnew_predictions_nb_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"new_predictions_nb_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cnf_matrix = confusion_matrix(new_y2, new_predictions_nb_,labels=['EAP', 'HPL', 'MWS'])\nnp.set_printoptions(precision=2)\n\n# Plot non-normalized confusion matrix\nplt.figure()\nplot_confusion_matrix(cnf_matrix, classes=['EAP', 'HPL', 'MWS'],\n                      title='Confusion matrix')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nfrom sklearn import svm, datasets\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import label_binarize\nfrom sklearn.metrics import roc_curve, auc\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom itertools import cycle\nplt.style.use('ggplot')\n\n\nww = label_binarize(y1, classes=[0, 1, 2])\nn_classes = ww.shape[1]\nnew_y22=label_binarize(new_y, classes=[0, 1, 2])\n\n#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.5, random_state=0)\n#y_score = new_probabilities\nplt.figure(figsize=(10,5)) \nfpr = dict()\ntpr = dict()\nroc_auc = dict()\nfor i in range(n_classes):\n    fpr[i], tpr[i], _ = roc_curve(new_y22[:, i], probabilities_nb[:, i])\n    roc_auc[i] = auc(fpr[i], tpr[i])\ncolors = cycle(['navy', 'aqua', 'deeppink'])\nfor i, color in zip(range(n_classes), colors):\n    plt.plot(fpr[i], tpr[i], color=color, lw=1.5,\n             label='ROC curve of class {0} (area = {1:0.2f})'\n             ''.format(i, roc_auc[i]))\n\nplt.plot([0, 1], [0, 1], 'k--', lw=1.5)\nplt.xlim([-0.05, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC curves for LogisticRegression')\nplt.legend(loc=\"lower right\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Your code here\nclass LM:\n \n  def __init__(self, smoothing, n_grams_size):\n    self.smoothing = smoothing\n    self.n_grams_size = n_grams_size\n    self.counts = defaultdict(lambda: defaultdict(lambda: smoothing))\n    self.START = '_***_'\n    self.STOP = '_STOP_'\n\n  def fit(self, document):\n    \"\"\"\n    fit on a given document and collect counts\n    \"\"\"\n    corpus = [line.strip().split() for line in document]\n\n    for sentence in corpus:\n      tokens = [self.START for _ in range(self.n_grams_size)] + sentence + [self.STOP]\n      for n_grams_tuple in nltk.ngrams(tokens, self.n_grams_size):\n        self.counts[n_grams_tuple[:-1]][n_grams_tuple[-1]] += 1\n#[:-1] .It slices the string to omit the last character\n  def _sample_next_word(self, history_tuple, iters=50):\n    \"\"\"\n    sample next word based on the history\n    \"\"\"\n    for _ in range(iters):\n      keys, values = zip(*self.counts[history_tuple].items())\n      values = np.array(values)\n      values /= values.sum()\n      sample = np.random.multinomial(1, values) \n\n    return keys[np.argmax(sample)]\n\n  def generate(self, words_list):\n    \"\"\"\n    generate a new sentence that begins with the given words\n    \"\"\"\n    # check number of words\n    if len(words_list) < self.n_grams_size:\n      raise AttributeError(\"wrong number of words\")\n\n    result = [self.START for _ in range(self.n_grams_size)]\n    result.extend(words_list)\n    history_tuple = tuple([result[i] for i in range(-self.n_grams_size+1, 0, 1)])\n    next_word = self._sample_next_word(history_tuple)\n    result.append(next_word)\n\n    while next_word != self.STOP:\n      history_tuple = tuple(result[i] for i in range(-self.n_grams_size+1, 0, 1))\n      next_word = self._sample_next_word(history_tuple)\n      result.append(next_word)\n\n    return ' '.join(result[self.n_grams_size:-1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\ntweets = [line.strip() for line in open('../input/spooky/train.csv', encoding='utf8')]\n \nlm = LM(smoothing=0.001, n_grams_size=2)\nlm.fit(document=tweets)\nu=np.unique([lm.generate([\"lovely\",\"winter\"]) for _ in range(10)])\ntest_subset=u.tolist()\ntest_subset","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import re\nREPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\nBAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\nSTOPWORDS = set(stopwords.words('english'))\n\ndef clean_text(text):\n    \"\"\"\n        text: a string\n        \n        return: modified initial string\n    \"\"\"\n    text = text.lower() # lowercase text\n    text = REPLACE_BY_SPACE_RE.sub(' ', text) # replace REPLACE_BY_SPACE_RE symbols by space in text. substitute the matched string in REPLACE_BY_SPACE_RE with space.\n    text = BAD_SYMBOLS_RE.sub('', text) # remove symbols which are in BAD_SYMBOLS_RE from text. substitute the matched string in BAD_SYMBOLS_RE with nothing. \n    text = text.replace('eap', '')\n    text = text.replace('mws', '')\n\n\n#    text = re.sub(r'\\W+', '', text)\n    text = ' '.join(word for word in text.split() if word not in STOPWORDS) # remove stopwors from text\n    return text\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"map_object = map(clean_text, test_subset)\nout_put = list(map_object)\nout_put","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenized_sents = [word_tokenize(i) for i in out_put]\nfrom itertools import chain\ntokens_output = list(chain(*tokenized_sents))\ntokens_output[:8]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"###################################","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"traincleaned[:3]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = traincleaned.clean_text\nY = traincleaned.author\n\nX_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\nprint(X_train.shape)\nprint(X_test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"1. 1. The problem here is, since there are different numbers in the same column, the model will misunderstand the data to be in some kind of order, 0 < 1 < 2. But this isnâ€™t the case at all. To overcome this problem, we use One Hot Encoder.\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\n\nlabel_encoder = LabelEncoder()\ninteger_encoded1 = label_encoder.fit_transform(y_train)\ninteger_encoded2 = label_encoder.fit_transform(y_test)\n\n\nle_name_mapping = dict(zip(label_encoder.transform(label_encoder.classes_),label_encoder.classes_))\nprint(\"Label Encoding Classes as \")\nprint(le_name_mapping)\n\n\n#Converts a class vector (integers) to binary class matrix.\ny_train=np_utils.to_categorical(integer_encoded1,num_classes=3)\ny_val=np_utils.to_categorical(integer_encoded2,num_classes=3)\nprint(\"One Hot Encoded class shape \")\nprint(y_train.shape)\nprint(y_train[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"integer_encoded2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_val","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train0 = X_train.values.astype(str)\n#y_train = df0_train['sentiment'].values\n\nX_val0 = X_test.values.astype(str)\n#y_val = df0_val['sentiment'].values\nprint('df_train shape: {}'.format(X_train0.shape))\nprint('df_val shape: {}'.format(X_val0.shape))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.python.keras.preprocessing import sequence\nfrom tensorflow.python.keras.preprocessing import text\n\n# Vectorization parameters\n# Limit on the number of features. We use the top 20K features.\nTOP_K = 20000\n\n# Limit on the length of text sequences. Sequences longer than this\n# will be truncated.\nMAX_SEQUENCE_LENGTH = 500\n\n#def sequence_vectorize(train_texts, val_texts):\n\n    # Create vocabulary with training texts.\ntokenizer = text.Tokenizer(num_words=TOP_K)\ntokenizer.fit_on_texts(X_train0)\n\n    # Vectorize training and validation texts.\nx_train = tokenizer.texts_to_sequences(X_train0)\nx_val = tokenizer.texts_to_sequences(X_val0)\n\n    # Get max sequence length.\nmax_length = len(max(x_train, key=len))\nif max_length > MAX_SEQUENCE_LENGTH:\n    max_length = MAX_SEQUENCE_LENGTH\n\n    # Fix sequence length to max value. Sequences shorter than the length are\n    # padded in the beginning and sequences longer are truncated\n    # at the beginning.\nx_train = sequence.pad_sequences(x_train, maxlen=max_length)\nx_val = sequence.pad_sequences(x_val, maxlen=max_length)\n\n\nvocab_size = len(tokenizer.word_index) + 1  # Adding 1 because of reserved 0 index\n\n\nprint(x_train)\nprint(max_length)\nprint(x_train.shape)\n\nprint(vocab_size)\nprint(x_train[2])\nprint(X_train0[1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#from tensorflow.keras.preprocessing.text import Tokenizer\n#from tensorflow.keras.preprocessing.sequence import pad_sequences    \n#tokenizer = Tokenizer(num_words=20000)\n#tokenizer.fit_on_texts(X_train0)\n#X_train = tokenizer.texts_to_sequences(X_train0)\n#X_test = tokenizer.texts_to_sequences(X_val0)\n#from keras.preprocessing.sequence import pad_sequences\n#maxlen = 300\n#padded_sequence_train = pad_sequences(X_train, padding='post', maxlen=maxlen)\n#padded_sequence_test = pad_sequences(X_test, padding='post', maxlen=maxlen)\n#print(vocab_size)\n#print(X_train[1])\n#print(X_train0[1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"word_index = tokenizer.word_index\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(tokenizer.word_index) + 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import LSTM,Dense, Dropout,Bidirectional\nfrom tensorflow.keras.layers import SpatialDropout1D\nfrom tensorflow.keras.layers import Embedding\nfrom tensorflow.keras.preprocessing.text import Tokenizer\n\n\nembedding_vector_length = 100\n\nmodel = Sequential()\n\nmodel.add(Embedding(len(tokenizer.word_index) + 1, embedding_vector_length,     \n                                         input_length=409) )\n\n#model.add(SpatialDropout1D(0.25))\nmodel.add(LSTM(2, dropout=0.2, recurrent_dropout=0.3))\n\n#model.add(Dropout(0.2))\n\nmodel.add(Dense(3, activation='softmax'))\nmodel.compile(loss='binary_crossentropy',optimizer='adam', \n                           metrics=['accuracy'])\nprint(model.summary())\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import LSTM,Dense, Dropout,Bidirectional\nfrom tensorflow.keras.layers import SpatialDropout1D\nfrom tensorflow.keras.layers import Embedding\nfrom tensorflow.keras.preprocessing.text import Tokenizer\n\n\n\n\n\nembedding_vector_length = 100\n\nmodel2 = Sequential(name=\"bi_lstm_20 NODES\")\n\nmodel2.add(Embedding(len(tokenizer.word_index) + 1, embedding_vector_length,     \n                                         input_length=409,name=\"Bi-LSTM\") )\n\nmodel2.add(Bidirectional(LSTM(20, return_sequences=True, recurrent_dropout=0.4)))\nmodel2.add(Dropout(0.5))\nmodel2.add(Bidirectional(LSTM(20, recurrent_dropout=0.2)))\n\n#model.add(Dropout(dropout))\n#model2.add(Dense(256, activation='relu'))\n\n\n#model.add(Dropout(0.2))\n\nmodel2.add(Dense(3, activation='softmax'))\nmodel2.compile(loss='binary_crossentropy',optimizer='adam', \n                           metrics=['accuracy'])\nprint(model2.summary())\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#history = model.fit(padded_sequence_train, y_train,validation_data=(padded_sequence_test, y_val),epochs=20,batch_size=70)\n\nfrom keras.callbacks import EarlyStopping\nes = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=2)\nhistory2=model2.fit(x_train, y_train,\n batch_size=500,\n epochs=20,\n validation_data=(x_val, y_val),\n callbacks=[es])\n#We save this model so that we can use in own web app\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"loss, accuracy = model2.evaluate(x_train, y_train, verbose=1)\nprint(\"Training Accuracy: {:.4f}\".format(accuracy))\nloss, accuracy = model2.evaluate(x_val, y_val, verbose=1)\nprint(\"Testing Accuracy:  {:.4f}\".format(accuracy))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_score = model2.predict(x_val,verbose=1)\ny_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_score2 = model2.predict_classes(x_val,verbose=1)\ny_score2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_val","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vv=np_utils.to_categorical(y_score2,num_classes=3)\nvv","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"probabilities = model2.predict_proba(x_val,verbose=1)\nprint(probabilities)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"prediction_model2 = model2.predict(x_val)\nprediction_model2 = pd.DataFrame(data={'author': y_test, 'new_predictions': labels2numbers.inverse_transform(y_score2),'truth':y_test})\nprediction_model2[:4]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nrr=labels2numbers.inverse_transform(integer_encoded2)\n\nuu=labels2numbers.inverse_transform(y_score2)\n\n\n\n\ncnf_matrix = confusion_matrix(rr, uu,labels=['EAP', 'HPL', 'MWS'])\nnp.set_printoptions(precision=2)\n\n# Plot non-normalized confusion matrix\nplt.figure()\nplot_confusion_matrix(cnf_matrix, classes=['EAP', 'HPL', 'MWS'],\n                      title='Confusion matrix')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"eee=label_binarize(y_score2, classes=[0, 1, 2])\neee","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nfrom sklearn import svm, datasets\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import label_binarize\nfrom sklearn.metrics import roc_curve, auc\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom itertools import cycle\nplt.style.use('ggplot')\n\n\n#qqq = label_binarize(integer_encoded2, classes=[0, 1, 2])\nn_classes = 3\neee=label_binarize(y_score2, classes=[0, 1, 2])\n\n#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.5, random_state=0)\n#y_score = new_probabilities\nplt.figure(figsize=(10,5)) \nfpr = dict()\ntpr = dict()\nroc_auc = dict()\nfor i in range(n_classes):\n    fpr[i], tpr[i], _ = roc_curve(eee[:, i], probabilities[:, i])\n    roc_auc[i] = auc(fpr[i], tpr[i])\ncolors = cycle(['navy', 'aqua', 'deeppink'])\nfor i, color in zip(range(n_classes), colors):\n    plt.plot(fpr[i], tpr[i], color=color, lw=1.5,\n             label='ROC curve of class {0} (area = {1:0.2f})'\n             ''.format(i, roc_auc[i]))\n\nplt.plot([0, 1], [0, 1], 'k--', lw=1.5)\nplt.xlim([-0.05, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC curves for LSTM')\nplt.legend(loc=\"lower right\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nplt.style.use('ggplot')\n\n\nqqq = label_binarize(integer_encoded2, classes=[0, 1, 2])\nn_classes = qqq.shape[1]\neee=label_binarize(y_score2, classes=[0, 1, 2])\n\n# Compute ROC curve and ROC area for each class\nfpr = dict()\ntpr = dict()\nroc_auc = dict()\nfor i in range(n_classes):\n    fpr[i], tpr[i], _ = roc_curve(eee[:, i], y_score[:, i])\n    roc_auc[i] = auc(fpr[i], tpr[i])\n# Compute micro-average ROC curve and ROC area\nfpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(eee.ravel(), y_score.ravel())\nroc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n\n# Plot of a ROC curve for a specific class\nplt.figure()\nplt.plot(fpr[2], tpr[2], label='ROC curve (area = %0.2f)' % roc_auc[2])\nplt.plot([0, 1], [0, 1], 'k--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic example')\nplt.legend(loc=\"lower right\")\nplt.show()\n\n# Plot ROC curve\nplt.figure()\nplt.plot(fpr[\"micro\"], tpr[\"micro\"],\n         label='micro-average ROC curve (area = {0:0.2f})'\n               ''.format(roc_auc[\"micro\"]))\nfor i in range(n_classes):\n    plt.plot(fpr[i], tpr[i], label='ROC curve of class {0} (area = {1:0.2f})'\n                                   ''.format(i, roc_auc[i]))\n\nplt.plot([0, 1], [0, 1], 'k--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Some extension of Receiver operating characteristic to multi-class')\nplt.legend(loc=\"lower right\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_score\n#The output for each sentence is a probability distribution over labels. Nice if we want it,\n#but here, we are actually more interested in the one best tag for each token.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#history = model.fit(padded_sequence_train, y_train,validation_data=(padded_sequence_test, y_val),epochs=20,batch_size=70)\n\nfrom keras.callbacks import EarlyStopping\nes = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=3)\nhistory=model.fit(x_train, y_train,\n batch_size=250,\n epochs=30,\n validation_data=(x_val, y_val),\n callbacks=[es])\n#We save this model so that we can use in own web app\nmodel.save('authors_res.h5')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"loss, accuracy = model.evaluate(x_train, y_train, verbose=1)\nprint(\"Training Accuracy: {:.4f}\".format(accuracy))\nloss, accuracy = model.evaluate(x_val, y_val, verbose=1)\nprint(\"Testing Accuracy:  {:.4f}\".format(accuracy))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nplt.style.use('ggplot')\n\ndef plot_history(history):\n    acc = history.history['acc']\n    val_acc = history.history['val_acc']\n    loss = history.history['loss']\n    val_loss = history.history['val_loss']\n    x = range(1, len(acc) + 1)\n\n    plt.figure(figsize=(12, 5))\n    plt.subplot(1, 2, 1)\n    plt.plot(x, acc, 'b', label='Training acc')\n    plt.plot(x, val_acc, 'r', label='Validation acc')\n    plt.title('Training and validation accuracy')\n    plt.legend()\n    plt.subplot(1, 2, 2)\n    plt.plot(x, loss, 'b', label='Training loss')\n    plt.plot(x, val_loss, 'r', label='Validation loss')\n    plt.title('Training and validation loss')\n    plt.legend()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_history(history2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nplt.style.use('ggplot')\n\n\nww = label_binarize(y1, classes=[0, 1, 2])\nn_classes = ww.shape[1]\nnew_y22=label_binarize(new_y, classes=[0, 1, 2])\n\n# Compute ROC curve and ROC area for each class\nfpr = dict()\ntpr = dict()\nroc_auc = dict()\nfor i in range(n_classes):\n    fpr[i], tpr[i], _ = roc_curve(new_y22[:, i], probabilities_lg[:, i])\n    roc_auc[i] = auc(fpr[i], tpr[i])\n# Compute micro-average ROC curve and ROC area\nfpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(new_y22.ravel(), probabilities_lg.ravel())\nroc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n\n# Plot of a ROC curve for a specific class\nplt.figure()\nplt.plot(fpr[2], tpr[2], label='ROC curve (area = %0.2f)' % roc_auc[2])\nplt.plot([0, 1], [0, 1], 'k--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic example')\nplt.legend(loc=\"lower right\")\nplt.show()\n\n# Plot ROC curve\nplt.figure()\nplt.plot(fpr[\"micro\"], tpr[\"micro\"],\n         label='micro-average ROC curve (area = {0:0.2f})'\n               ''.format(roc_auc[\"micro\"]))\nfor i in range(n_classes):\n    plt.plot(fpr[i], tpr[i], label='ROC curve of class {0} (area = {1:0.2f})'\n                                   ''.format(i, roc_auc[i]))\n\nplt.plot([0, 1], [0, 1], 'k--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Some extension of Receiver operating characteristic to multi-class')\nplt.legend(loc=\"lower right\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"testdata = pd.read_csv('../input/testclean/testclean.csv')\n\ntestdata.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"testset_x= testdata.clean_text.astype(str)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.python.keras.preprocessing import sequence\nfrom tensorflow.python.keras.preprocessing import text\n\n\nTOP_K = 20000\nMAX_SEQUENCE_LENGTH = 409\n\ntokenizer = text.Tokenizer(num_words=TOP_K)\ntokenizer.fit_on_texts(testset_x)\ntest_x0 = tokenizer.texts_to_sequences(testset_x)\n\nmax_length = len(max(testset_x, key=len))\nif max_length > MAX_SEQUENCE_LENGTH:\n    max_length = MAX_SEQUENCE_LENGTH\n\ntest_x0 = sequence.pad_sequences(test_x0, maxlen=max_length)\n\nvocab_size2 = len(tokenizer.word_index) + 1  # Adding 1 because of reserved 0 index\n\n\nprint(max_length)\nprint(vocab_size2)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_test_pred = model.predict(test_x0,verbose=1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(y_test_pred .shape)\nprint(testset_x[1])\nprint(y_test_pred[1])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#lets use glove word embeding:\n\nfrom numpy import array\nfrom numpy import asarray\nfrom numpy import zeros\n\nembeddings_dictionary = dict()\nglove_file = open('../input/glove100/glove.6B.100d.txt', encoding=\"utf8\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will create a dictionary that will contain words as keys and the corresponding 100 dimensional vectors as values, in the form of an array. Execute the following script:\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"#!wget http://nlp.stanford.edu/data/glove.6B.zip\n#!unzip -q glove.6B.zip","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# load the GloVe vectors in a dictionary:\n\nembeddings_index = {}\nf = open('../input/glove100/glove.6B.100d.txt', encoding='utf8')\nfor line in tqdm(f):\n    values = line.split()\n    word = ''.join(values[:-100])\n    coefs = np.asarray(values[-100:], dtype='float32')\n    embeddings_index[word] = coefs\nf.close()\n\nprint('Found %s word vectors.' % len(embeddings_index))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"words = ['sad']\nfor w in words:\n    if w in embeddings_index.keys():\n        print('Found the word {} in the dictionary'.format(w))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"To feed this into an Embedding layer, we need to build a matrix containing the words in the data set and their representative word embedding. So this matrix will be of shape (NB_WORDS, GLOVE_DIM)"},{"metadata":{"trusted":true},"cell_type":"code","source":"NB_WORDS=17666 \nGLOVE_DIM=100\nemb_matrix = np.zeros((NB_WORDS, GLOVE_DIM))\n\nfor w, i in tokenizer.word_index.items():\n    # The word_index contains a token for all words of the training data so we need to limit that\n    if i < NB_WORDS:\n        vect = embeddings_index.get(w)\n        # Check if the word from the training data occurs in the GloVe word embeddings\n        # Otherwise the vector is kept with only zeros\n        if vect is not None:\n            emb_matrix[i] = vect\n    else:\n        break","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"emb_matrix.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import LSTM,Dense, Dropout\nfrom tensorflow.keras.layers import SpatialDropout1D\nfrom tensorflow.keras.layers import Embedding\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras import layers\n\n\n\n\nembedding_vector_length = 100\n\nmodel2 = Sequential()\n\nmodel2.add(Embedding(17666, embedding_vector_length,     \n                                         input_length=409) )\n\n#model.add(SpatialDropout1D(0.25))\nmodel2.add(LSTM(10, dropout=0.4, recurrent_dropout=0.3))\nmodel2.add(Dropout(0.4))\n#model2.add(layers.Flatten())\n#model.add(Dense(units=output_labels))\n\nmodel2.add(Dense(3, activation='softmax'))\nmodel2.compile(loss='binary_crossentropy',optimizer='adam', \n                           metrics=['accuracy'])\nprint(model2.summary())\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model2.layers[0].set_weights([emb_matrix])\nmodel2.layers[0].trainable = False\nprint(model2.summary())\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#history = model.fit(padded_sequence_train, y_train,validation_data=(padded_sequence_test, y_val),epochs=20,batch_size=70)\n\nfrom keras.callbacks import EarlyStopping\nes = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=3)\nhistory=model2.fit(x_train, y_train,\n batch_size=400,\n epochs=30,\n validation_data=(x_val, y_val),\n callbacks=[es])\n#We save this model so that we can use in own web app\nmodel2.save('authors_res.h5')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#using attention LSTM","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class attention(Layer):\n    def __init__(self,**kwargs):\n        super(attention,self).__init__(**kwargs)\n\n    def build(self,input_shape):\n        self.W=self.add_weight(name=\"att_weight\",shape=(input_shape[-1],1),initializer=\"normal\")\n        self.b=self.add_weight(name=\"att_bias\",shape=(input_shape[1],1),initializer=\"zeros\")        \n        super(attention, self).build(input_shape)\n\n    def call(self,x):\n        et=K.squeeze(K.tanh(K.dot(x,self.W)+self.b),axis=-1)\n        at=K.softmax(et)\n        at=K.expand_dims(at,axis=-1)\n        output=x*at\n        return K.sum(output,axis=1)\n\n    def compute_output_shape(self,input_shape):\n        return (input_shape[0],input_shape[-1])\n\n    def get_config(self):\n        return super(attention,self).get_config()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.models import Model\nfrom keras.layers import Input, Embedding\nfrom keras.layers import Bidirectional, LSTM\nfrom keras.layers import Dropout, Dense, Activation\nimport numpy as np\n\n\nfeatures=409\n\ninputs = Input((409, ), \n               name='word_IDs')\nembeddings = Embedding(input_dim=len(tokenizer.word_index)+1, \n                       output_dim=32,name='embeddings',input_length=409)(inputs)\n#wrap the LSTM in a Bidirectional wrapper\nbilstm = LSTM(100,return_sequences=True,name=\"Bi-LSTM\",dropout=0.4, recurrent_dropout=0.3)(embeddings)\natt_out=attention()(bilstm)\n\noutput2 = Dense(3, activation='softmax')(att_out)\n\nmodel_bilstm = Model(inputs=[inputs], outputs=[output2])\n\nmodel_bilstm.compile(loss='binary_crossentropy',optimizer='adam', \n                           metrics=['accuracy'])\nmodel_bilstm.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.callbacks import EarlyStopping\nes = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=3)\nhistory=model_bilstm.fit(x_train, y_train,\n batch_size=300,\n epochs=30,\n validation_data=(x_val, y_val),\n callbacks=[es])\n#We save this model so that we can use in own web app\nmodel2.save('authors_res.h5')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Attention(tf.keras.Model):\n\n\tdef __init__(self, units):\n\t\tsuper(Attention, self).__init__()\n\t\tself.W1 = tf.keras.layers.Dense(units)\n\t\tself.W2 = tf.keras.layers.Dense(units)\n\t\tself.V = tf.keras.layers.Dense(1)\n\n\tdef call(self, features, hidden):\n\t\t# hidden shape == (batch_size, hidden size)\n\t\t# hidden_with_time_axis shape == (batch_size, 1, hidden size)\n\t\t# we are doing this to perform addition to calculate the score\n\t\thidden_with_time_axis = tf.expand_dims(hidden, 1)\n\t\t  \n\t\t# score shape == (batch_size, max_length, 1)\n\t\t# we get 1 at the last axis because we are applying score to self.V\n\t\t# the shape of the tensor before applying self.V is (batch_size, max_length, units)\n\t\tscore = tf.nn.tanh(\n\t\t\tself.W1(features) + self.W2(hidden_with_time_axis))\n\t\t# attention_weights shape == (batch_size, max_length, 1)\n\t\tattention_weights = tf.nn.softmax(self.V(score), axis=1)\n\t\t  \n\t\t# context_vector shape after sum == (batch_size, hidden_size)\n\t\tcontext_vector = attention_weights * features\n\t\tcontext_vector = tf.reduce_sum(context_vector, axis=1)\n\t\treturn context_vector, attention_weights","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nimport pickle\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport nltk\nfrom collections import defaultdict\nimport re\n\nimport tensorflow as tf\nimport tensorflow.keras as keras\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.layers import Concatenate, Dense, Input, LSTM, Embedding, Dropout, Activation, GRU, Flatten\nfrom tensorflow.keras.layers import Bidirectional, GlobalMaxPool1D\nfrom tensorflow.keras.models import Model, Sequential\nfrom tensorflow.keras.layers import Convolution1D\nfrom tensorflow.keras import initializers, regularizers, constraints, optimizers, layers\nfrom keras.callbacks import Callback\n\n\nmaxlen=409\nembed_size=100\nmax_features= len(tokenizer.word_index) +1\n\ninp = Input(shape=(maxlen,))\nx = Embedding(max_features, embed_size, weights=[emb_matrix])(inp)\nlstm = Bidirectional(LSTM(50, return_sequences=True), name=\"bi_lstm_0\")(x)\n\n(lstm, forward_h, forward_c, backward_h, backward_c) = Bidirectional(LSTM(40,return_state=True,\n                                                                          return_sequences=True),\n                                                                          name=\"bi_lstm_1\")(lstm)\nstate_h = Concatenate()([forward_h, backward_h])\nstate_c = Concatenate()([forward_c, backward_c])\ncontext_vector, attention_weights = Attention(20)(lstm, state_h)\n\n#dense1 = Dense(20, activation=\"relu\")(context_vector)\n\noutput = Dense(3, activation=\"softmax\")(context_vector)\n  \nmodel = keras.Model(inputs=inp, outputs=output)\n\nmodel.compile(loss='binary_crossentropy',optimizer='adam', \n                           metrics=['accuracy'])\n\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.callbacks import EarlyStopping\nes = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=3)\nhistory=model.fit(x_train, y_train,\n batch_size=300,\n epochs=30,\n validation_data=(x_val, y_val),\n callbacks=[es])\n#We save this model so that we can use in own web app\nmodel2.save('authors_res.h5')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### from keras.layers import merge\nimport os\nimport pickle\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport nltk\nfrom collections import defaultdict\nimport re\nimport tensorflow as tf\nimport tensorflow.keras as keras\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.layers import Concatenate, Dense, Input, LSTM, Embedding, Dropout, Activation, GRU, Flatten,Permute,RepeatVector,Lambda\nfrom tensorflow.keras.layers import Bidirectional, GlobalMaxPool1D\nfrom tensorflow.keras.models import Model, Sequential\nfrom tensorflow.keras.layers import Convolution1D\nfrom tensorflow.keras import initializers, regularizers, constraints, optimizers, layers\nfrom keras.callbacks import Callback\nfrom keras import backend as K\n\n\nmaxlen=409\nembed_size=100\nmax_features= len(tokenizer.word_index) +1\n\ninp =Input(shape=(maxlen,), dtype='float32')\nx = Embedding(max_features, embed_size, weights=[emb_matrix])(inp)\nlstm = Bidirectional(LSTM(50, return_sequences=True), name=\"bi_lstm_0\")(x)\n\nattention = Dense(1, activation='tanh')(lstm)\nattention= Flatten()(attention)\nattention= Activation('softmax')(attention)\nattention=RepeatVector(max_features*2)(attention)\nattention= Permute([2,1])(attention)\n\nsent_representation = Concatenate([lstm, attention])\n\nsent_representation = Lambda(lambda xin: K.sum(xin, axis=-1))(sent_representation)\n\n#output = Dense(3, activation=\"softmax\")(sent_representation)\nprint (sent_representation.get_shape())    \n#probabilities = Dense(3, activation='softmax')(sent_representation)      #Expected (5000,)\n#model = Model(inputs=input_, outputs=probabilities)\n#model.summary()\n\n\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":""}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}