{"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3","language":"python"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","nbconvert_exporter":"python","mimetype":"text/x-python","pygments_lexer":"ipython3","name":"python","version":"3.6.3"}},"cells":[{"cell_type":"markdown","metadata":{"_uuid":"9dacb7a2a5a9e582d7e597be4ea479cedd79489a","_cell_guid":"d49b7eaa-0bf7-48e0-9fe6-51ba03036ba1"},"source":"# Who wrote that? - Spooky author identification\nStarted on 30 Oct 2017\n\nThis notebook is inspired by:\n* Machine Learning: Classification - Coursera course by University of Washington,\nhttps://www.coursera.org/learn/ml-classification\n* Machine Learning with Text in scikit-learn - Kevin Markham's tutorial at Pycon 2016, \nhttps://m.youtube.com/watch?t=185s&v=ZiKMIuYidY0\n* Kernel by bshivanni - \"Predict the author of the story\", \nhttps://www.kaggle.com/bsivavenu/predict-the-author-of-the-story\n* Kernel by SRK - \"Simple Engg Feature Notebook - Spooky Author\",\nhttps://www.kaggle.com/sudalairajkumar/simple-feature-engg-notebook-spooky-author"},{"cell_type":"markdown","metadata":{"_uuid":"8f9dbcf8df12e40c0daee0f48d91fd07cd084cae","_cell_guid":"06e8c4a5-feea-41ea-8831-13ab7089dcc5"},"source":"Comments:\n\n* In this kernel, I will also do a weighted averaging of the 'proba' of the 2 models to see the performance.\n"},{"cell_type":"code","execution_count":null,"outputs":[],"source":"import numpy as np\nimport pandas as pd\nfrom matplotlib import pyplot as plt\n%matplotlib inline","metadata":{"_uuid":"5c5f4cc8865644748e11336736bbe584adebe7b1","collapsed":true,"_cell_guid":"8f6a95ee-cc95-4c9f-a8f7-72ae58ec13d6"}},{"cell_type":"markdown","metadata":{"_uuid":"4f65d03ddbfd127307d3e415003346eb898b4d6b","_cell_guid":"80d61838-9025-4cba-bb0e-58175586b21b"},"source":"## Read \"train.csv\" and \"test.csv into pandas"},{"cell_type":"code","execution_count":null,"outputs":[],"source":"train_df = pd.read_csv('../input/train.csv')\ntest_df = pd.read_csv('../input/test.csv')","metadata":{"_uuid":"4e35cd5fcae1581dbd6bc51f14728e27fe63fe70","collapsed":true,"_cell_guid":"094fff47-db10-447c-965e-08056f718bde"}},{"cell_type":"markdown","metadata":{"_uuid":"89d1c9a4f9598427e8a20d66fa9e56796ad720f6","_cell_guid":"09986b08-eda6-4438-9cbe-52a61d8d57fa"},"source":"## Examine the train data"},{"cell_type":"code","execution_count":null,"outputs":[],"source":"train_df.shape","metadata":{"_uuid":"4cf1c0650ee00566fa75e9c100393cac437d48d1","_cell_guid":"e8a8b1d3-36c0-4026-b91d-1181dda8bbfb"}},{"cell_type":"code","execution_count":null,"outputs":[],"source":"train_df.head()","metadata":{"_uuid":"9e53b7599d707a9420a75c37c7ac6d05bed9df7b","_cell_guid":"c4c7137d-6bc7-4b41-b50a-e511883155e9"}},{"cell_type":"code","execution_count":null,"outputs":[],"source":"# check the class distribution for the author label in train_df?\ntrain_df['author'].value_counts()","metadata":{"_uuid":"2ff9ed83ba328872d446add97695285dc49f4165","_cell_guid":"981dff09-3acf-4014-b38a-22afc02a6654"}},{"cell_type":"markdown","metadata":{"_uuid":"d2bd8ddc6445aad024d327d7b78004f80cfd83f6","_cell_guid":"391dfeb1-5549-4f85-ac4c-4ed66e9cce6b"},"source":"#### The class distribution looks balanced."},{"cell_type":"code","execution_count":null,"outputs":[],"source":"# compute the text length for the rows and record these\ntrain_df['text_length'] = train_df['text'].apply(lambda x: len(str(x).split()))\ntrain_df.head()","metadata":{"_uuid":"1e97432af65b6b75b436daabc83bdf57775a59c1","_cell_guid":"b26588a7-9a7f-4183-98b2-fb94a70bedaa"}},{"cell_type":"code","execution_count":null,"outputs":[],"source":"# look at the histogram plot for text length\ntrain_df.hist()\nplt.show()","metadata":{"_uuid":"448c3492fc2fe24f30bd7b97047d69f16b58ca2f","_cell_guid":"d5ac5111-5bba-44c9-a039-7bb01a5bfd59"}},{"cell_type":"code","execution_count":null,"outputs":[],"source":"# number of text length that are greater than 100, 150 & 200\nG100 = sum(i > 100 for i in train_df['text_length'])\nG150 = sum(i > 150 for i in train_df['text_length'])\nG200 = sum(i > 200 for i in train_df['text_length'])\nprint('Text length greater than 100, 150 & 200 are ',G100,',',G150,'&',G200, ' respectively.')\nprint('In percentages, they are %.2f, %.2f & %.2f' %(G100/len(train_df)*100, \n      G150/len(train_df)*100, G200/len(train_df)*100))","metadata":{"_uuid":"0bccd37457d2688f5709b48c1101eb452fb02e5b","_cell_guid":"71c554b8-96f5-4910-9c99-0b9e27ea2210"}},{"cell_type":"markdown","metadata":{"_uuid":"d28a801b4057518f87e20c46a77f9dc8757ab944","_cell_guid":"5b482de7-5fd1-4ef7-b7b4-2abf84ebdc25"},"source":"#### Most of the text length are within 200 words and less. Let's look at the summary statistics of the text lengths by author."},{"cell_type":"code","execution_count":null,"outputs":[],"source":"EAP = train_df[train_df['author'] =='EAP']['text_length']\nEAP.describe()","metadata":{"_uuid":"d5d5bf3219d1a53fae0bfab69844e118cda32bab","_cell_guid":"5da0987d-d871-4f53-b605-8f89ab4dfd94"}},{"cell_type":"code","execution_count":null,"outputs":[],"source":"EAP.hist()\nplt.show()","metadata":{"_uuid":"4600bd930b16cd5e576701c7baa4304c6b6b0836","_cell_guid":"4f17e03d-50dd-4aae-b579-a8f0d68ead5a"}},{"cell_type":"code","execution_count":null,"outputs":[],"source":"MWS = train_df[train_df['author'] == 'MWS']['text_length']\nMWS.describe()","metadata":{"_uuid":"a3e02aec85a3817124145d6c4ca602b9e0a73223","_cell_guid":"a9d8733d-c134-4f9b-8184-26492b995602"}},{"cell_type":"code","execution_count":null,"outputs":[],"source":"MWS.hist()\nplt.show()","metadata":{"_uuid":"487062ed9cca7724c86631174501c4bf2c2b90ad","_cell_guid":"4befa810-7786-451d-b476-b41dc5036869"}},{"cell_type":"code","execution_count":null,"outputs":[],"source":"HPL = train_df[train_df['author'] == 'HPL']['text_length']\nHPL.describe()","metadata":{"_uuid":"e3be2b92e83f2a4336bd20677395930597fea6c6","_cell_guid":"73db4ab6-3395-4972-be02-36d4bc393899"}},{"cell_type":"code","execution_count":null,"outputs":[],"source":"HPL.hist()\nplt.show()","metadata":{"_uuid":"f8cf435292bd01efe234667d55239a8733b60950","_cell_guid":"6f58ad06-6ce6-4921-bcbf-536cc7c009b3"}},{"cell_type":"markdown","metadata":{"_uuid":"71efe8c7cf1fa93eec845046b031852fcc4ed8ac","_cell_guid":"7c25d701-f75d-4902-b51d-6ed46cdb27a8"},"source":"## Similarly examine the text length & distribution in test data"},{"cell_type":"code","execution_count":null,"outputs":[],"source":"test_df.shape","metadata":{"_uuid":"8359291e49919941659a26679e19630192707508","_cell_guid":"d0776a75-0bac-4508-b60f-66e0dc6735d4"}},{"cell_type":"code","execution_count":null,"outputs":[],"source":"test_df.head()","metadata":{"_uuid":"5b17f1fccfe69cda599ad77b061481b6c3c1a590","_cell_guid":"9006b12c-110c-49e4-a5b7-cd824f9306ac"}},{"cell_type":"code","execution_count":null,"outputs":[],"source":"# examine the text length in test_df and record these\ntest_df['text_length'] = test_df['text'].apply(lambda x: len(str(x).split()))\ntest_df.head()","metadata":{"_uuid":"36e4d00a5afc15e6b04ffa1e79421396d051f614","_cell_guid":"0993d06b-495e-428a-933a-6ffec6bdcef3"}},{"cell_type":"code","execution_count":null,"outputs":[],"source":"test_df.hist()\nplt.show()","metadata":{"_uuid":"1cc05875b88e54b5a1079eafc088207536dea2f3","_cell_guid":"828b9990-d78e-46c7-b011-1c66e6e6be79"}},{"cell_type":"code","execution_count":null,"outputs":[],"source":"# number of text length that are greater than 100, 150 & 200\nG100 = sum(i > 100 for i in test_df['text_length'])\nG150 = sum(i > 150 for i in test_df['text_length'])\nG200 = sum(i > 200 for i in test_df['text_length'])\nprint('Text length greater than 100, 150 & 200 are ',G100,',',G150,'&',G200, ' respectively.')\nprint('In percentages, they are {:.2f}, {:.2f} & {:.2f}'.format(G100/len(test_df)*100, \n      G150/len(test_df)*100, G200/len(test_df)*100))","metadata":{"_uuid":"d51acd1488ff54db3037cd0259ffe51aa755c092","_cell_guid":"cda4bb66-766e-4219-87be-45aed5036114"}},{"cell_type":"markdown","metadata":{"_uuid":"db69eb685cd1be44de2224c399cbdeabb5ceaa06","_cell_guid":"b2bbf246-f098-425a-99e3-2eb2126b975c"},"source":"#### The proportion of text which are long in the test data is very similar to that in the train data."},{"cell_type":"markdown","metadata":{"_uuid":"d0709c2d7c606f27b7b1abeeed821aaeee00fb6e","_cell_guid":"3c3f238d-1b6a-48a0-bcee-40031e4a2536"},"source":"## Some preprocessing of the target variable to facilitate modelling"},{"cell_type":"code","execution_count":null,"outputs":[],"source":"# convert author labels into numerical variables\ntrain_df['author_num'] = train_df.author.map({'EAP':0, 'HPL':1, 'MWS':2})\n# Check conversion for first 5 rows\ntrain_df.head()","metadata":{"_uuid":"46438c851ef26de2bd7a3736c13abeb938519800","_cell_guid":"1a1a7fb8-9dfc-47c9-9114-f4708d109854"}},{"cell_type":"markdown","metadata":{"_uuid":"cae81f6b1d9bb475fc486d5fbb81981025cc3672","_cell_guid":"f5abe72a-13a7-41f6-ae8e-1c34dca97110"},"source":"## Define X and y from train data for use in tokenization by CountVectorizer"},{"cell_type":"code","execution_count":null,"outputs":[],"source":"X = train_df['text']\ny = train_df['author_num']\nprint(X.shape)\nprint(y.shape)","metadata":{"_uuid":"061d59552c6ef83bea8ecf9ffbf203286aeab6f8","_cell_guid":"49547fd7-9633-4f6a-bd46-84d8966f1e8b"}},{"cell_type":"markdown","metadata":{"_uuid":"903c319c5b83198edf0af59d49818bd9071ec8dc","_cell_guid":"4a6e6a69-91b7-43a9-8e91-3c8fd20d2790"},"source":"## Split train data into a training and a test set"},{"cell_type":"code","execution_count":null,"outputs":[],"source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\nprint(X_train.shape)\nprint(X_test.shape)\nprint(y_train.shape)\nprint(y_test.shape)","metadata":{"_uuid":"32e74067044fb018d0a71ed97c3326d578b1c0a6","_cell_guid":"438ceb39-2687-41b5-b7cb-8784a1aab35d"}},{"cell_type":"code","execution_count":null,"outputs":[],"source":"# examine the class distribution in y_train and y_test\nprint(y_train.value_counts())\nprint(y_test.value_counts())","metadata":{"_uuid":"40e308981530947ddb7b68ac0075b36c7decde43","_cell_guid":"7010f557-ccce-4d8a-a289-d081d2c3c0c9"}},{"cell_type":"markdown","metadata":{"_uuid":"cc4c2aeef221f5a97e1bd5ea1154052172175351","_cell_guid":"b2d898ae-79bc-48b8-8544-fb077f876c67"},"source":"## Vectorize the data using CountVectorizer"},{"cell_type":"code","execution_count":null,"outputs":[],"source":"# import and instantiate CountVectorizer\nfrom sklearn.feature_extraction.text import CountVectorizer\nvect = CountVectorizer(lowercase=False, token_pattern=r'(?u)\\b\\w+\\b|\\,|\\.|\\;|\\:')\n# vect = CountVectorizer(lowercase=False, token_pattern=r'(?u)\\b\\w+\\b|\\,|\\.|\\?|\\;|\\:|\\!|\\'')\nvect","metadata":{"_uuid":"7b7adf15d16408eb99689884906d0d687c2f8407","_cell_guid":"9be5a4f2-0e85-4f9a-ac00-b8e9916116cb"}},{"cell_type":"code","execution_count":null,"outputs":[],"source":"# learn the vocabulary in the training data, then use it to create a document-term matrix\nX_train_dtm = vect.fit_transform(X_train)\n# examine the document-term matrix created from X_train\nX_train_dtm","metadata":{"_uuid":"b755b1d4db58eeb5b0ab668d1aaf4a651d3de441","_cell_guid":"283c1d48-c267-431b-834e-37c8d9222b3c"}},{"cell_type":"code","execution_count":null,"outputs":[],"source":"# transform the test data using the earlier fitted vocabulary, into a document-term matrix\nX_test_dtm = vect.transform(X_test)\n# examine the document-term matrix from X_test\nX_test_dtm","metadata":{"_uuid":"408301ccb78e3f4056a6d2ebbd239594d1a59da0","_cell_guid":"54050711-560a-47cf-b4f9-2fbaf59bc2e4"}},{"cell_type":"markdown","metadata":{"_uuid":"bfd0cc30c28d8750c2b4fc395c00d9f3b186a4fe","_cell_guid":"74e5e92e-9964-47bd-89fb-d39b62730954"},"source":"## Build and evaluate an author classification model using Multinomial Naive Bayes"},{"cell_type":"code","execution_count":null,"outputs":[],"source":"# import and instantiate the Multinomial Naive Bayes model\nfrom sklearn.naive_bayes import MultinomialNB\nnb = MultinomialNB()\nnb","metadata":{"_uuid":"5498742f48ffcb39e09d347f5bf315fd882627c0","_cell_guid":"24e76bcf-3f0c-46b0-8370-e47ec1377b31"}},{"cell_type":"code","execution_count":null,"outputs":[],"source":"# train the model using X_train_dtm & y_train\nnb.fit(X_train_dtm, y_train)","metadata":{"_uuid":"2eb2a105d06304cc43486e396c5e57bcfa6a5da5","_cell_guid":"21f72378-5d13-4c70-9ac5-74cede2bfc34"}},{"cell_type":"code","execution_count":null,"outputs":[],"source":"# make author (class) predictions for X_test_dtm\ny_pred_test = nb.predict(X_test_dtm)","metadata":{"_uuid":"20b7974ad1b6b3bd4c03d0110950410c491366e6","collapsed":true,"_cell_guid":"521690d7-19e4-408e-b837-76582be3b408"}},{"cell_type":"code","execution_count":null,"outputs":[],"source":"# compute the accuracy of the predictions with y_test\nfrom sklearn import metrics\nmetrics.accuracy_score(y_test, y_pred_test)","metadata":{"_uuid":"75c9bf8f66093512b8ca8275e88f38750c6d6128","_cell_guid":"022f4b59-ef5e-413e-b4ef-090d81ac2223"}},{"cell_type":"code","execution_count":null,"outputs":[],"source":"# compute the accuracy of training data predictions\ny_pred_train = nb.predict(X_train_dtm)\nmetrics.accuracy_score(y_train, y_pred_train)","metadata":{"_uuid":"280a7405b142bba27d5a9b9cd2f778191158649b","_cell_guid":"9a538513-0860-48db-8626-5117499ea237"}},{"cell_type":"code","execution_count":null,"outputs":[],"source":"# look at the confusion matrix for y_test\nmetrics.confusion_matrix(y_test, y_pred_test)","metadata":{"_uuid":"be747bd888c6f48c9e1581c5341fa0eaba4b9753","_cell_guid":"6d92d210-7255-4c33-ba42-bee0c32de1bb"}},{"cell_type":"code","execution_count":null,"outputs":[],"source":"# calculate predicted probabilities for X_test_dtm\ny_pred_prob = nb.predict_proba(X_test_dtm)\ny_pred_prob[:10]","metadata":{"_uuid":"3b87ae14e8f809a509207fee3c3e29925f19a2e4","_cell_guid":"fa8ac435-be40-48a6-8b46-b6e3860dbbee"}},{"cell_type":"code","execution_count":null,"outputs":[],"source":"# compute the log loss number\nmetrics.log_loss(y_test, y_pred_prob)","metadata":{"_uuid":"9fed846b715fb77ee2549c63b02e3adce66b4f5f","_cell_guid":"48b380e8-5822-4376-aeea-295b229d62e9"}},{"cell_type":"markdown","metadata":{"_uuid":"a989e534fe575ebeb3d5952e221c5b445631738a","_cell_guid":"04ab10ee-4c43-41a2-87a8-ea3c3400a546"},"source":"## Build and evaluate an author classification model using Logistic Regression"},{"cell_type":"code","execution_count":null,"outputs":[],"source":"# import and instantiate the Logistic Regression model\nfrom sklearn.linear_model import LogisticRegression\nlogreg = LogisticRegression()\nlogreg","metadata":{"_uuid":"e7e5707b19c35c8371a0cff7351bc8aadc33acd1","_cell_guid":"e30be87f-e0a6-4fd7-bff2-b3c37737cbfe"}},{"cell_type":"code","execution_count":null,"outputs":[],"source":"# train the model using X_train_dtm and y_train\nlogreg.fit(X_train_dtm, y_train)","metadata":{"_uuid":"b0273bf7105237551f62d03cd9a46bc245e99b8b","_cell_guid":"fb392d0d-0d4b-4c0b-9690-2eac8cacc607"}},{"cell_type":"code","execution_count":null,"outputs":[],"source":"# make class predictions for X_test_dtm\ny_pred_test = logreg.predict(X_test_dtm)","metadata":{"_uuid":"98f1dfbb75b68b65d49fa9f8fa69ebf64554e894","collapsed":true,"_cell_guid":"4a298037-97bc-49eb-8cf7-bf52d4894f11"}},{"cell_type":"code","execution_count":null,"outputs":[],"source":"# compute the accuracy of the predictions\nmetrics.accuracy_score(y_test, y_pred_test)","metadata":{"_uuid":"5000bb48f01b5632c5b3d4387154e32b6be4410a","_cell_guid":"9c459dae-783d-49e0-9b0c-dffd35419dbc"}},{"cell_type":"code","execution_count":null,"outputs":[],"source":"# compute the accuracy of predictions with the training data\ny_pred_train = logreg.predict(X_train_dtm)\nmetrics.accuracy_score(y_train, y_pred_train)","metadata":{"_uuid":"847ef102dcb57e88f06003603a4ec7bd7cdde87f","_cell_guid":"816ea321-cdfd-41d0-b2f8-b89fcc513c43"}},{"cell_type":"code","execution_count":null,"outputs":[],"source":"# look at the confusion matrix for y_test\nmetrics.confusion_matrix(y_test, y_pred_test)","metadata":{"_uuid":"302632e4ecd6790e1cbb92baab53f98517028bcc","_cell_guid":"c6f4a678-7072-44c4-a415-de7ba1d55d52"}},{"cell_type":"code","execution_count":null,"outputs":[],"source":"# compute the predicted probabilities for X_test_dtm\ny_pred_prob = logreg.predict_proba(X_test_dtm)\ny_pred_prob[:10]","metadata":{"_uuid":"c6a1543f17f4031e31b250aef5d98ddb6b1f1d0d","_cell_guid":"8a238463-87eb-44c5-afcd-794accd04d71"}},{"cell_type":"code","execution_count":null,"outputs":[],"source":"# compute the log loss number\nmetrics.log_loss(y_test, y_pred_prob)","metadata":{"_uuid":"ad9cfb160671a6fbcf92b8ca6e800440948cf232","_cell_guid":"91e879ac-744b-4d10-b798-c99535e5c23b"}},{"cell_type":"markdown","metadata":{"_uuid":"ef4f2b854c0a2c346a6615ac8de40e5e4bcbe868","_cell_guid":"962928a6-503e-4757-8518-b7b23f74d09a"},"source":"## Train the Logistic Regression model with the entire dataset from \"train.csv\""},{"cell_type":"code","execution_count":null,"outputs":[],"source":"# Learn the vocabulary in the entire training data, and create the document-term matrix\nX_dtm = vect.fit_transform(X)\n# Examine the document-term matrix created from X_train\nX_dtm","metadata":{"_uuid":"d7e09c9fe6524a9f964d50d8048c5226a4a67475","_cell_guid":"5018c932-1fc4-401b-8993-2f1a48f2415e"}},{"cell_type":"code","execution_count":null,"outputs":[],"source":"# Train the Logistic Regression model using X_dtm & y\nlogreg.fit(X_dtm, y)","metadata":{"_uuid":"ab43f652017097166901c83c679485b444c6c192","_cell_guid":"69cacde4-b731-40a8-b754-e3ecd4decff7"}},{"cell_type":"code","execution_count":null,"outputs":[],"source":"# Compute the accuracy of training data predictions\ny_pred_train = logreg.predict(X_dtm)\nmetrics.accuracy_score(y, y_pred_train)","metadata":{"_uuid":"65f85a9921ce7d0664762fddb1b592a8bb005cae","_cell_guid":"48c91a38-87d7-4b72-a418-829730c9ee37"}},{"cell_type":"markdown","metadata":{"_uuid":"968c394fe3ba44506a2a65606317b8d1983d24ee","_cell_guid":"8bc6812d-6592-412e-80a5-ae339488a0bf"},"source":"## Make predictions on the test data and compute the probabilities for submission"},{"cell_type":"code","execution_count":null,"outputs":[],"source":"test_df.head()","metadata":{"_uuid":"e0398012e2ba55ec849fe618eaf8a89c4d796623","_cell_guid":"34f000be-b71e-4e4b-8af9-9962217e66b8"}},{"cell_type":"code","execution_count":null,"outputs":[],"source":"# transform the test data using the earlier fitted vocabulary, into a document-term matrix\ntest_dtm = vect.transform(test_df['text'])\n# examine the document-term matrix from X_test\ntest_dtm","metadata":{"_uuid":"2e45db51b1b8d7ea951db649770e33cc725608eb","_cell_guid":"4b22b19b-fbf5-429b-b980-fc954000dd1c"}},{"cell_type":"code","execution_count":null,"outputs":[],"source":"# make author (class) predictions for test_dtm\nLR_y_pred = logreg.predict(test_dtm)\nprint(LR_y_pred)","metadata":{"_uuid":"3290ddf6fe948d9b4a0ed26419ef94e35f295e84","_cell_guid":"19cc4c8b-ac52-4ed8-b7b3-9d0d1d96099d"}},{"cell_type":"code","execution_count":null,"outputs":[],"source":"# calculate predicted probabilities for test_dtm\nLR_y_pred_prob = logreg.predict_proba(test_dtm)\nLR_y_pred_prob[:10]","metadata":{"_uuid":"abaefe7c47cac96b429315d480900e8e9e800f98","_cell_guid":"0c051ec9-b17b-42b6-8449-6b692431d6c5"}},{"cell_type":"markdown","metadata":{"_uuid":"7ab689bd05df65f3a5e09fcefe7d80ba66615add","_cell_guid":"73999e46-b1f2-47ed-8c13-2871dfbd8541"},"source":"## Train the Naive Bayes model with the entire dataset \"train.csv\""},{"cell_type":"code","execution_count":null,"outputs":[],"source":"nb.fit(X_dtm, y)","metadata":{"_uuid":"2bca382cdca372e9ba4b294b54336efcfb15c181","_cell_guid":"7c9287c2-87f0-4344-8cf2-8cdf56c5e943"}},{"cell_type":"code","execution_count":null,"outputs":[],"source":"# compute the accuracy of training data predictions\ny_pred_train = nb.predict(X_dtm)\nmetrics.accuracy_score(y, y_pred_train)","metadata":{"_uuid":"9a6a36102a4269e6e182375eeabcf4ce63ac94ee","_cell_guid":"c3e5dd33-a75e-44f7-b211-1cced31c033d"}},{"cell_type":"markdown","metadata":{"_uuid":"7e9ed7c4b05fed9aafcec960e4414ddaaac852c3","_cell_guid":"326c0751-6036-451b-8b23-fdec9ef4631a"},"source":"## Make predictions on test data"},{"cell_type":"code","execution_count":null,"outputs":[],"source":"# make author (class) predictions for test_dtm\nNB_y_pred = nb.predict(test_dtm)\nprint(NB_y_pred)","metadata":{"_uuid":"6336049258e441b9507def0b9173570abd17e04c","_cell_guid":"fe35c157-77cf-48a3-9791-023725cd9784"}},{"cell_type":"code","execution_count":null,"outputs":[],"source":"# calculate predicted probablilities for test_dtm\nNB_y_pred_prob = nb.predict_proba(test_dtm)\nNB_y_pred_prob[:10]","metadata":{"_uuid":"db1ff0ec20bcbdead790ace8ac5254775ca60e95","_cell_guid":"ae1ce383-766a-4ef8-99cf-9df8f2bdfcf9"}},{"cell_type":"markdown","metadata":{"_uuid":"ee6d942b861235107e0d94174a8d21c2bad0e9ea","_cell_guid":"5d0970eb-bc44-4fad-91b7-e2a23c2f986d"},"source":"## Create submission file"},{"cell_type":"code","execution_count":null,"outputs":[],"source":"alpha = 0.6\ny_pred_prob = ((1-alpha)*LR_y_pred_prob + alpha*NB_y_pred_prob)\ny_pred_prob[:10]","metadata":{"_uuid":"a2a24232c56e0d0567493401972c11df9fbdeec9","_cell_guid":"4865057d-c375-4f3d-a8b3-fa36b18a3f7c","scrolled":false}},{"cell_type":"code","execution_count":null,"outputs":[],"source":"result = pd.DataFrame(y_pred_prob, columns=['EAP','HPL','MWS'])\nresult.insert(0, 'id', test_df['id'])\nresult.head()","metadata":{"_uuid":"ee067d4b61d323f1589970031ca121364e52a9f6","_cell_guid":"ef99474c-ee91-40af-a4e0-7f86445e6841"}},{"cell_type":"code","execution_count":null,"outputs":[],"source":"# Generate submission file in csv format\nresult.to_csv('rhodium_submission_14.csv', index=False, float_format='%.20f')","metadata":{"_uuid":"86d7d2fa2b5671183438fe0769cfb7594eb4efa5","collapsed":true,"_cell_guid":"bef9209e-c843-4ca6-be4f-35820ffa258d"}},{"cell_type":"markdown","metadata":{"_uuid":"cbbce014f73f54e9bf3e88e096ad66b89b88fcf4","_cell_guid":"7a012382-ad72-4c76-b53c-1f756544e23e"},"source":"### Will work on this further.\n### Comments and tips are most welcomed.\n### Please upvote if you find it useful. Cheers!"},{"cell_type":"code","execution_count":null,"outputs":[],"source":"","metadata":{"_uuid":"21a21a9ff3c174753c8d48495e058a1782bf7a76","collapsed":true,"_cell_guid":"2b370cff-d22e-4393-94d5-98b4883a32d8"}}],"nbformat_minor":1,"nbformat":4}