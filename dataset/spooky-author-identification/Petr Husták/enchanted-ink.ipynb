{"cells":[{"cell_type":"markdown","metadata":{"_uuid":"d6d3a825995d0d1d14f6449e17aaead8c678c051","_cell_guid":"4dd8a702-7735-480e-8c92-f374d1b694d9"},"source":"*... With my heart racing and limbs frozen, I surveyed the floor littered with crumpled scraps of paper. How am I ever going sort through this disarray? Just as my mind was smoothly transitioning from sudden shock to sheer hopelesness, a distant memory occured to me. Many a years ago, I learned an ink-enchanting spell. If I can just channel the spirits of those three frightening wordsmiths into the physical words on these pages, perhaps they will reveal some hint to who authored each sentence. I focus my mind, and slowly, word by word, spell out the incantation. As I am finishing the last syllable of the last spell, the letters on the pages come alive with a soft glow. I lean to look closer, only to be startled by a sudden flurry of motion and bright colorful flashes. Once the excitement settled down, I could see that the text transformed, and each word took on its own hue and magnitude. This must be the hint I was hoping for!*\n![Glowing Book](https://ichef.bbci.co.uk/childrens-responsive-ichef-ck/400xn/amz/cbbc/book-swap-index.jpg)"},{"cell_type":"markdown","metadata":{"_uuid":"ddd13e06b87265a3525b04022a62f783654cb8b9","_cell_guid":"c6eb5548-4e9a-4cbe-a0b8-590d70cec84a"},"source":"# 1. About This Kernel\nThis kernel does not classify data for submission, but is focused purely on visualization. Any supplied text is formatted by mapping calculated metrics on font size, color, and tooltips to show how individual words indicate each of the three authors. This type of visualization can be used in multiple ways.\n\n1. ** As a tool for facilitated manual classification:** An expert can read the enhanced text to sort the data combining the calculated data with their personal knowledge to make the final decision.\n2. **For analysis of classification approaches:** For example a look at misclassified sentences displayed using this formatting can reveal the reason for given sample to be misclassified, and inspire modifiction to the approach. Note that for this use case, the metrics mapped to the formatting parameters (color, size) do not necessarily have to be the same as presented in this kernel. \n3. **Just for fun:** Browse the data and explore what words are typical for which author. Or write your own sentence and see in who's style it is written!\n\n# 2. Motivational Example\nHere is a sample sentence to illustrate the technique. As you look at this, try to hover your mouse over individual words to show additional info."},{"outputs":[],"source":"from IPython.core.display import display, HTML # To show formatted HTML in IPython notebook  \nHTML(\"\"\"<font size=5 color='blue' title='\\\"Frankenstein\\\" Score (frequency)\nMWS: 3.0 (16)\nEAP: 0.0 (0)\nHPL: 0.0 (0)'>Frankenstein</font>, <font size=5 color='green' title='\\\"Cthulhu\\\" Score (frequency)\nHPL: 3.0 (14)\nEAP: 0.0 (0)\nMWS: 0.0 (0)'>Cthulhu</font>, <font size=1 color='green' title='\\\"and\\\" Score (frequency)\nHPL: 1.138 (5494)\nMWS: 1.055 (5506)\nEAP: 0.807 (5184)'>and</font> <font size=5 color='red' title='\\\"Dupin\\\" Score (frequency)\nEAP: 3.0 (52)\nHPL: 0.0 (0)\nMWS: 0.0 (0)'>Dupin</font> <font size=2 color='green' title='\\\"walk\\\" Score (frequency)\nHPL: 1.673 (22)\nMWS: 0.984 (14)\nEAP: 0.343 (6)'>walk</font> <font size=1 color='red' title='\\\"into\\\" Score (frequency)\nEAP: 1.23 (417)\nHPL: 1.004 (256)\nMWS: 0.765 (211)'>into</font> <font size=1 color='red' title='\\\"a\\\" Score (frequency)\nEAP: 1.137 (4256)\nHPL: 1.065 (2996)\nMWS: 0.798 (2428)'>a</font> <font size=2 color='red' title='\\\"bar\\\" Score (frequency)\nEAP: 1.669 (5)\nHPL: 1.331 (3)\nMWS: 0.0 (0)'>bar</font>.\"\"\")","cell_type":"code","metadata":{"_kg_hide-input":true,"_uuid":"727b87693ff19dd7849799dc44896197764fdcd1","_cell_guid":"5d38cbf8-7d3f-4bd2-ab14-c1af65c1fcc0"},"execution_count":18},{"cell_type":"markdown","metadata":{"_uuid":"a2bf02b31212616f819ebc33b35c24d9e60e0953","_cell_guid":"68feb8bc-0516-4a34-a2c3-26715eee8844"},"source":"Each author is assigned a color: blue for M. W. Shelley, green for H. P. Lovecraft and red for E. A. Poe. A cursory look at the formatted text should give a reader an idea about who wrote the sentence, albeit in this example it would be rather challenging. Let's look at the first few  words in the above sentence.\n- **Frankenstein** is blue and large, meaning it is a very \"MWS\" word. When hovering mouse over it, the tooltip shows that MWS used Frankenstein 16 times while the other authors did not use it at all. This results in a max author score of 3.0, also shown in the tooltip.\n- **Cthulhu** is green and large, meaning it is a very \"HPL\" word. Again, it has author score of 3.0, since it is exclusive to HPL.\n- **and** is written in small green font. Green means that it is most likely for HPL, nonetheless its small size shows that other authors use it almost as much. Look at the tooltip reveals that HPL's winning author score is only 1.138.\n- ..."},{"cell_type":"markdown","metadata":{"_uuid":"6829d01b7921440b2914ea271320178ef3f6e8c7","_cell_guid":"0dfda4c4-c719-452a-a3d3-aa8aaa2ff579"},"source":"# 3. Word Metrics Calculation\n\nAs hinted above, the size and color of individual words is determined by the author score metric. It is be calculated sing the following simple calculation.  \n\nFirst, we define the propensity of word $w$ in the corpus of author $a$.\n$$ p_w^a = \\frac{f_w^a}{N^a} $$\nHere, $N^a$ is total number of words in author $a$'s data sample and $f_w^a$ is number of times word $w$ appeared therein.\n\nFinal score for word $w$ and author $a$ is then calculated as propensity for given author divided by the average of all the authors' propensities. \n$$ Score_w^a = \\frac{p_w^a}{\\frac{1}{3}\\left( p_w^{EAP}+p_w^{MWS}+p_w^{HPL}\\right)}  $$\n\nWith the three author scores ready, we can apply formating to word $w$.\n- The author with the highest score is mapped to font color (*Who is the most likely author of this word?*)\n- The highest score is mapped to font size (*How strongly does the word indicate the author selected above?*)\n- All three author scores along with word frequencies are displayed in the tooltip   "},{"cell_type":"markdown","metadata":{"_uuid":"1e5bdcd87c829ec5a5c104791ce506a9711685b4","_cell_guid":"a0792f67-0448-4e10-83bc-f0f1c0c812f1"},"source":"# 4. Implementation\n\n## 4.1 - Read in the data\n\nCreate three dataframes ...\n*  **Training data** - Randomly selected 90% of the labeled input data.\n* **Validation data** - Randomly selected 10% of the labeled input data.\n* **Test data** - All of the unlabeled input data."},{"outputs":[],"source":"import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport nltk # the natural langauage toolkit, open-source NLP\nfrom IPython.core.display import display, HTML # To show formatted HTML in IPython notebook  \nfrom sklearn.model_selection import train_test_split # To create a validation dataset\n\n# read the full labeled dataset\ntexts = pd.read_csv(\"../input/train.csv\")\n# Create a 10% validation and 90% training datasets\ntexts_train, texts_valid = train_test_split(texts, test_size=0.1, random_state=5675)\n\n# read the unlabeled dataset\ntexts_test = pd.read_csv(\"../input/test.csv\")","cell_type":"code","metadata":{"_uuid":"6745dcaa1e534812d18d62fa537a842a18464318","collapsed":true,"_cell_guid":"1db301f5-c040-4254-ab77-08ff08465c2b"},"execution_count":19},{"cell_type":"markdown","metadata":{"_uuid":"98d3f718c2ea06539806bee5ebffab9f5e247685","_cell_guid":"79ece14c-7a19-4bc7-8326-48ddbbaff3ea"},"source":"## 4.2 - Calculate Word Frequencies by Author\n\nThis process feeds the training sentences into nltk's ConditionalFreqDist object, so that we can easily find out how many times was a word used by each of the three authors. This code is taken directly from the [sample kernel for this challenge](https://www.kaggle.com/rtatman/beginner-s-tutorial-python). Thank you!"},{"outputs":[],"source":"# split the data by author\nbyAuthor = texts_train.groupby(\"author\")\n\n# word frequency by author\nwordFreqByAuthor = nltk.probability.ConditionalFreqDist()\n\n# for each author...\nfor name, group in byAuthor:\n    # get all of the sentences they wrote and collapse them into a\n    # single long string\n    sentences = group['text'].str.cat(sep = ' ')\n    # convert everything to lower case (so \"The\" and \"the\" get counted as \n    # the same word rather than two different words)\n    sentences = sentences.lower()\n    # split the text into individual tokens    \n    tokens = nltk.tokenize.word_tokenize(sentences)\n    # calculate the frequency of each token\n    frequency = nltk.FreqDist(tokens)\n    # add the frequencies for each author to our dictionary\n    wordFreqByAuthor[name] = (frequency)\n    \n# now we have a dictionary where each entry is the frequency distrobution\n# of words for a specific author.","cell_type":"code","metadata":{"scrolled":true,"_uuid":"78be278343141f32482f9154ccc30e62f1f0ccdb","collapsed":true,"_cell_guid":"041d11af-17bb-4d1a-894d-386ee53adef4"},"execution_count":20},{"cell_type":"markdown","metadata":{"_uuid":"373055d1a7fc93e3a7f9b1a491d2a37a46482c47","_cell_guid":"48da04bd-0377-4ff6-9c73-0121c05c8e9a"},"source":"## 4.3 Word Score Calculation Example\n\nThis section is not part of the final implementation, but illustates on examples how author scores are calculated. The calculation can be done easily thanks to the nltk's ConditionalFreqDist class. Previously, we have created its instance in the object wordFreqByAuthor and loaded it with our training data. Word propensity, as we defined it above, can be directly retrieved using the method wordFreqByAuthor[*author*].freq(*word*), so the score calculation is straightforward. We will use the following function to show the calculation on a few example words."},{"outputs":[],"source":"def illustrateScoreCalculation(word):\n    \"\"\"Calculate word scores for all three authors, print results in console.\"\"\"\n    authors = ['EAP','HPL','MWS']\n    # Get word frequency for all three authors\n    frequencies = {a:wordFreqByAuthor[a][word] for a in authors}\n    # Get propensity for all three authors\n    propensities = {a:wordFreqByAuthor[a].freq(word) for a in authors}\n    # Calculate average propensity \n    avgPropensity =sum(propensities.values())/3\n    # Calculate final scores by dividing author's propensity by average propensity\n    scores = {a:propensities[a]/avgPropensity for a in authors}\n    # Show what we have learned\n    for a in authors:\n        print(\"{0} used the word \\\"{1}\\\" {2}x, final score is {3:.6f}/{4:.6f} = {5:.2f}\".format(\n                a,word,frequencies[a],propensities[a],avgPropensity,scores[a]))","cell_type":"code","metadata":{"_uuid":"953dcfd586719c11bf248f9313eadbf86fa2a091","collapsed":true,"_cell_guid":"e04b57ec-26a0-47fc-8235-aff63dc4247b"},"execution_count":21},{"outputs":[],"source":"### Example word 1 ###\nillustrateScoreCalculation(\"drawer\")","cell_type":"code","metadata":{"_kg_hide-input":false,"_kg_hide-output":false,"_uuid":"442d6dea0e87d0f72df0c2769012623d23bd3e2d","_cell_guid":"421c3902-0bd5-4e5b-bba0-987c4c4a8e7d"},"execution_count":22},{"cell_type":"markdown","metadata":{"_uuid":"b5b3385d7dcdc6eab727d443eb3dd91864cff6f3","_cell_guid":"b171d928-0240-4a6b-9ad6-a86316ae998e"},"source":"Obviously E. A. Poe uses the word \"drawer\" much more than the other two authors. This reflects in his score being 2.76, close to the max value of 3.0. "},{"outputs":[],"source":"### Example word 2 ###\nillustrateScoreCalculation(\"card\")","cell_type":"code","metadata":{"_uuid":"2e1d04c246fd6716df094906fa2cd06513ac6636","_cell_guid":"ee919612-3fef-4781-9e12-cfa71e2c230e"},"execution_count":23},{"cell_type":"markdown","metadata":{"_uuid":"01d76de29745cdeec9bc2c8342bed1126a926cf7","collapsed":true,"_cell_guid":"2003090f-7ebf-4655-b8b1-7a17b4197737"},"source":"The word \"card\" was only used by one author in the training set - E. A. Poe. This means his score is the maximum value of 3.00."},{"outputs":[],"source":"### Example word 3 ###\nillustrateScoreCalculation(\"and\")","cell_type":"code","metadata":{"_uuid":"1c95fd6c698ff790d8a7e08fb6d605dff72de291","_cell_guid":"89a4074d-01a1-4db9-8521-1a677a08bb15"},"execution_count":24},{"cell_type":"markdown","metadata":{"_uuid":"807454f79aa3ad7e28bd7755d49023db1a1c1516","_cell_guid":"a70dd306-dbf8-4a1a-8a9c-0d5d2f322072"},"source":"H. P. Lovecraft uses the word \"and\" most frequently in his writing. Nonetheless, the other two authors use it almost as much, therefore the winning score is only 1.14, close to the minimum of 1.0."},{"cell_type":"markdown","metadata":{"_uuid":"0ee15f414505dd59dc27d1ccaf7e953b1f302215","_cell_guid":"daeb125b-6d51-4f94-91a6-ab11d4344baa"},"source":"## 4.4 - Define EnchantedInkPrinter Class\n\nThis class combines two main functions: calculation of author scores as described in section 3 above, and generation of formatted html code that maps the calculated author scores to printed font properties.\n"},{"outputs":[],"source":"class EnchantedInkPrinter:\n    # Colors to use for the three authors\n    colors = [\"red\",\"green\",\"blue\"]\n    # What color to print words that did not appear in the training data\n    neutralColor = \"black\" \n    # Text sizes to be mapped to the author score metric\n    textSizeSteps = [5,4,3,2,1]\n    # Add a constant to all of the textSizeSteps to make output larger\n    textSizeOffset = 0\n    # Format author's code (EAP, MWS, HPL) if it appears in text\n    formatClassNames = True\n    # Add color and size legend to printed html text\n    showLegend = True\n    # Experimental: Calculate the most likely author of the whole\n    # sentence based on summary of individual word scores\n    showSentenceSummaryScore = False\n    \"\"\"Provides methods to print an input string or dataset as a formatted \n    HTML output that shows the most likely author for each word based on\n    provided ConditionalFreqDist object.\"\"\"\n    \n    def __init__(self, classFreq):\n        \"\"\"parameter classFreq: nltk.probability.ConditionalFreqDist containing word \n        frequencies for the analyzed authors\"\"\"\n        self.classFreq = classFreq\n        # Number of authors\n        classCount = len(classFreq.conditions())\n        # Only can have as many authors as elements in self.colors (this can be expanded easily)\n        if classCount > len(self.colors):\n            raise ValueError('Maximum number of classes exceeded: {0} > {1}'.format(classCount,len(self.colors)))\n        elif classCount < 2:\n            raise ValueError('There must be at least two classes')\n        # Assign a color to each class\n        self.palette = {a:b for a,b in zip(classFreq.conditions(),self.colors)}\n        self.palette[''] = self.neutralColor\n        # Calculate text size cutoffs by spreading number of self.textSizes\n        # over the whole max author score spectrum of 1.0 - 3.0 (# of authors)\n        self.maxScore= float(classCount)\n        scoreStep = (classCount-1.0)/(len(self.textSizeSteps)-1)\n        self.textSizes = []\n        for i in range(len(self.textSizeSteps)-1):\n            self.textSizes.append( (self.maxScore-scoreStep*i,self.textSizeSteps[i]) )\n        self.textSizes.append((0.0,self.textSizeSteps[-1]))\n    \n    def __calcWordScore(self,word):\n        \"\"\"Calculate author metrics for one word\n        parameter word: string with a single word\n        returns: all word metrics in this structure:\n                 {'word':<word>,\n                  'classScores':[ {'class':<author1 name>, \n                                   'score':<author1 score>, \n                                   'frequency':<author1 frequency> }, \n                                  {'class':<author2 name>, \n                                   'score':<author2 score>, \n                                   'frequency':<author2 frequency>}\n                                  {'class':<author3 name>, \n                                   'score':<author3 score>, \n                                   'frequency':<author3 frequency>}]}\n                Note: the 'classScores' list is sorted by descending author score, so the most\n                      probable author is always on the first position.\n        \"\"\"\n        scoreList = None\n        # Get average propensity of given word among all authors\n        overallWordFreq = self.__getOverallWordFreq(word)\n        # For author codes (EAP, MWS, HPL), create synthetic score\n        if self.formatClassNames and word in self.classFreq.conditions():\n            scoreList = [{'class':word,'score':self.textSizes[0][0]+0.1}]\n        # If the file was not used in the training data, return empty score\n        elif overallWordFreq==0:\n            scoreList = [{'class':'','score':0.0}]\n        # Otherwise calculate author scores\n        else:\n            scoreList = []\n            for a in self.classFreq.conditions():\n                # Score calculation\n                score = self.classFreq[a].freq(word.lower())/overallWordFreq\n                # Frequency calculation\n                frequency = self.classFreq[a][word.lower()]\n                # For terms that were used only once in the whole training data,\n                # adjust score by subtracting 0.01\n                if score == self.maxScore and frequency == 1:\n                    score -= 0.01\n                scoreList.append({'class':a,'score':score,'frequency':frequency})\n            # Sort by descending score\n            scoreList.sort(key=lambda x:x['score'],reverse=True)\n        return {'word':word,'classScores':scoreList}\n    \n    def __getOverallWordFreq(self,word):\n        \"\"\"Calculate average propensity of given word among all authors\"\"\" \n        sumScore = 0.0\n        for a in self.classFreq.conditions():\n            sumScore += self.classFreq[a].freq(word.lower())\n        return sumScore / len(self.classFreq.conditions())\n\n    def __calcSentenceScore(self,text):\n        \"\"\"Take sentence provided in parameter text, tokenize it to individual \n        words, get metrics for each of the words (in structure as described \n        in __calcWordScore) and then return an array of all word metrics. \n        \"\"\" \n        scores = []\n        for word in nltk.tokenize.word_tokenize(text):\n            score = self.__calcWordScore(word)\n            # append the returned word score dictionary to the scores array\n            scores.append(score)\n        # Experimental: Summarize scores for all words and add an extra word \n        # to the return list with the summary information\n        if self.showSentenceSummaryScore:\n            summaryScore =self.__aggregateSentenceScore(scores)\n            scores.append({'word':'['+summaryScore[0]['class']+']','classScores':summaryScore})\n        return scores\n    \n    def __aggregateSentenceScore(self,scores):\n        \"\"\"Experimental: Based on list of scores for all words in sentence,\n        calculate overall sentence score that should indicate the most likely \n        author for the whole sentence. Return a synthetic word score with the\n        summary information that can be added to the sentence when printing.\"\"\"\n        # In the summary score, score represents sum of scores \n        # of all words in the sentence for given author         \n        scoreTotals = {a:0.0 for a in self.classFreq.conditions()}\n        # In the summary score, frequency represents the number \n        # of words that given author 'has won' in the sentence\n        freqTotals = {a:0 for a in self.classFreq.conditions()}\n        for score in scores:\n            if score[\"classScores\"][0]['class'] != '':                 \n                freqTotals[score[\"classScores\"][0]['class']] += 1\n                for a in score[\"classScores\"]:\n                    scoreTotals[a['class']] += a['score']\n        scoreList = [{'class':c,'score':scoreTotals[c],'frequency':freqTotals[c]} for c in scoreTotals.keys()]\n        scoreList.sort(key=lambda x:x['score'],reverse=True)\n        return scoreList\n\n    def __getSentenceHtml(self,wordScores):\n        \"\"\"Based on the input list of word scores in wordScores, create formatted html\n        code that maps given scores to font color and size. The element of wordScores\n        are word score dicts as described in __calcWordScore \"\"\"\n        line = \"\"\n        # This loops over the words in the sentence\n        for wordScore in wordScores:\n            word = wordScore['word']\n            scores = wordScore['classScores']\n            bestClass = scores[0]['class']\n            bestScore = scores[0]['score']\n            # Do not format interpunction, just add it to the ouptut\n            if word in (\",\",\";\",\".\",\"'\",'\"'):\n                line += word\n            else:\n                # Find the text size for this word\n                textSize = 0\n                for scoreCutoff,size in self.textSizes:\n                    if bestScore >= scoreCutoff:\n                        textSize = size\n                        break\n                # Construct the tooltip text with all information\n                tooltipText = \"\"\n                if len(scores) > 1:\n                    tooltipText = \"title='\\\"\" + word + \"\\\" Score (frequency)\"\n                    for classScore in scores:\n                        tooltipText += \"\\n{0}: {1} ({2})\".format(classScore['class'],\\\n                                                                 round(classScore['score'],3),\\\n                                                                 classScore['frequency'])\n                    tooltipText += \"'\"\n                # Create the final html code for this word including the proper <font> tags\n                line += \" <font size={0} color='{1}' {3}>{2}</font>\"\\\n                    .format(textSize + self.textSizeOffset,self.palette[bestClass],word,tooltipText)\n        return line\n    \n    def __getCombinedLegendHtml(self):\n        \"\"\"Create HTML code with table with color and size legend next to each other\"\"\"\n        htmlCode = '<table><tr><td>Color Legend (Author with highest score)</td><td>Size Legend</td></tr><tr><td>'\n        htmlCode += self.__getColorLegendHtml() + '</td><td>' \n        htmlCode += self.__getSizeLegendHtml() + '</td></tr></table>'\n        return htmlCode\n        \n    def __getColorLegendHtml(self):\n        \"\"\"Create HTML code for color legend\"\"\"\n        # Create synthetic word score list, then use __getSentenceHtml to generate the actual HTML code\n        wordScoreList = [{'word':w+'<br>','classScores':[{'class':w,'score':self.textSizes[0][1]}]} for w in self.classFreq.conditions()]\n        return self.__getSentenceHtml(wordScoreList)\n        \n    def __getSizeLegendHtml(self):\n        \"\"\"Create HTML code for size legend\"\"\"\n        # Create synthetic word score list, then use __getSentenceHtml to generate the actual HTML code\n        sizeLegendElements = []\n        lastScore = None\n        for score,size in self.textSizes:\n            if lastScore is None:\n                sizeLegendElements.insert(0, {'word':'score {0}'.format(score),\\\n                                              'classScores':[{'class':'','score':score}]})\n            else:\n                sizeLegendElements.insert(0, {'word':'score between {0} and {1}<br>'.format(score,lastScore),\\\n                                              'classScores':[{'class':'','score':score}]})\n            lastScore = score\n        return self.__getSentenceHtml(sizeLegendElements)\n    \n    def printString(self,text):\n        \"\"\"Take a word or a sentence in the parameter text, calculate author scores,\n        and print it formatted according to the author scores \"\"\"\n        scoredWords = self.__calcSentenceScore(text)\n        htmlCode =self.__getSentenceHtml(scoredWords)\n        if self.showLegend:\n            htmlCode += self.__getCombinedLegendHtml()\n        display(HTML(htmlCode))\n        \n    def printDataFrame(self,data):\n        \"\"\"Take a data frame in the parameter data. Print the whole data frame,\n        formatting all text columns using calculated author scores\"\"\"\n        htmlCode = \"\"\n        if self.showLegend:\n            htmlCode = self.__getCombinedLegendHtml()\n        htmlCode += '<table><tr>'\n        for colName in data.columns:\n            htmlCode += '<td>' + colName + '</td>'\n        htmlCode += '</tr>'\n        for index,row in data.iterrows():\n            htmlCode += \"<tr>\"\n            for value in row:\n                if isinstance(value, str):\n                    htmlCode += '<td>' + self.__getSentenceHtml(self.__calcSentenceScore(value)) + '</td>'\n                else:\n                    htmlCode += str(value)\n            htmlCode += \"</tr>\"\n        display(HTML(htmlCode))\n","cell_type":"code","metadata":{"_uuid":"7fcf13b3c9e8bd00e080d8368324a41700efc584","collapsed":true,"_cell_guid":"53c7e046-4f6f-4513-91e7-6d0de12f393d"},"execution_count":25},{"cell_type":"markdown","metadata":{"_uuid":"273c34b0f2082660443eda879b4723897c35a005","_cell_guid":"ebc683c4-25cf-4a69-96e0-cbb880777b90"},"source":"## 4.5 Try out the EnchantedInkPrinter Class\n\nThe following code snippet shows the usage of the EnchantedInkPrinter class. "},{"outputs":[],"source":"# Initialize an instance of EnchantedInkPrinter\n# The constructor parameter is wordFreqByAuthor that we loaded with training data earlier\nprinter = EnchantedInkPrinter(wordFreqByAuthor)\n\n# The class exposes two public methods\n#   - printString(<str>) To print a sentence or word in a string.\n#   - printDataFrame(<DataFrame>) To print a whole dataframe, formatting all character columns\n\n# Let's try out the former function on the motivational example from section 2\nprinter.printString(\"Frankenstein, Cthulhu and Dupin walk into a bar.\")","cell_type":"code","metadata":{"_uuid":"be4e03cf59e4b4c13cde7e0850193a026b917211","_cell_guid":"53539067-1529-45b9-b4f9-eaf341b92817"},"execution_count":26},{"cell_type":"markdown","metadata":{"_uuid":"b6575c3cc2b1378f0da2bb0ebb59fc8a29d39fce","collapsed":true,"_cell_guid":"f9b8268b-007b-4fb7-be3b-178d60193aa0"},"source":"# 5. Challenge Data Visualization\n\nWith all the explanations out of the way, we can now look at the actual data.\n\n## 5.1 Validation Data Visualization\nIn section 4.1, we have taken random 10% of the challenge training data to use as a validation dataset. That means we can now use it to show the visualization on a fresh set of data (that was not used for training) and see what the actual labels are. You can browse through the data and see how well the formatting indicates the author of each of the sentences."},{"outputs":[],"source":"# Print the first 20 rows of the validation dataset\n# Modify the subset cutoffs as desired to show other data rows! \nprinter.printDataFrame(texts_valid[0:30])","cell_type":"code","metadata":{"_uuid":"56945cf6d76714f3da17892212cae9f5914e473f","_cell_guid":"17a9e552-246e-49a6-a738-ed8b0884e3ee"},"execution_count":27},{"cell_type":"markdown","metadata":{"_uuid":"50a156a6a05b0584d329e58f8a8bec20da7a5108","_cell_guid":"b9b1f8bb-d590-4007-857a-1026a503aa34"},"source":"## 5.2 Test Data Visualization\nHere are some rows from the challenge test dataset with no labels attached. Can you guess what the labels should be?"},{"outputs":[],"source":"# Print the first 20 rows of the validation dataset\n# Modify the subset cutoffs as desired to show other data rows! \nprinter.printDataFrame(texts_test[0:30])","cell_type":"code","metadata":{"_uuid":"91d149da2caefa729038e944659867f5f0c551a4","_cell_guid":"3588efd5-dea1-4411-9cd4-1758eab15024"},"execution_count":28}],"nbformat":4,"metadata":{"language_info":{"nbconvert_exporter":"python","pygments_lexer":"ipython3","name":"python","file_extension":".py","mimetype":"text/x-python","version":"3.6.3","codemirror_mode":{"version":3,"name":"ipython"}},"kernelspec":{"display_name":"Python 3","name":"python3","language":"python"}},"nbformat_minor":1}