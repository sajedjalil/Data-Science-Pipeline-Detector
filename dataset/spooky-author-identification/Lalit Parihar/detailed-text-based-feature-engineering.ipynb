{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"version":"3.6.3","nbconvert_exporter":"python","mimetype":"text/x-python","file_extension":".py","name":"python","pygments_lexer":"ipython3"}},"cells":[{"cell_type":"markdown","metadata":{"_uuid":"b8dca1448ff85da52dd499b0a9cae4293d53e368","_kg_hide-output":false,"_cell_guid":"c7255b6d-fead-43ea-8469-b4ceba1cf05b","_kg_hide-input":false,"collapsed":true},"source":"######  ABOUT THIS KERNEL ####\nHello Everyone!!!\nI understand it is very late for publishing a Kernel for this competition as most of the possible options have already been tried and worked upon.\n\nBut still I will try to give a chance and do some of the brain-storming on the 'Text based' feature engineering as per my limited wisdom on the topic.\n\nMy perspective of feature creation, will be - to find writing patterns which we see in day - to - day life.\n\nI am pretty sure that some of these are not tried yet and may help you in this - or any other similar tasks.\n\nI will also try to write this Kernel in a tutorial form so as to help anyone who might try to benefit from this.\n"},{"cell_type":"markdown","metadata":{"_uuid":"62d4fe7a5affb94abcb274a44e2c60bb4876b0ed","_cell_guid":"efa4e605-c1e7-45a5-9b53-4d6e2ab3e2b9","collapsed":true},"source":"######  ABOUT THE COMPETION ####\nIn this competition, text strings written by authors 1- Edgar Allan Poe (EAP),2- HP Lovecraft (HPL) & 3-Mary Wollstonecraft Shelley (MWS) are given as inputs with the respective author of the 'string'.\n\nThe test data contains a group of string and we have to provide the \"probability for each of the three classes(EAP,HPL,MWS)\". In simple terms we are supposed to identify the author of the string."},{"cell_type":"code","metadata":{"_uuid":"093149c7c1c8f3e18a0afe5288d26b398a3c24a9","_cell_guid":"883c28fd-7afc-4226-abd4-bb2a3cfdd73a","collapsed":true},"outputs":[],"execution_count":null,"source":"#As usual, in the first step we will import the libraries which we are gonna use.\nimport pandas as pd #pandas will help us reading the csv data to dataframes(df) and then working on the df.\nimport matplotlib.pyplot as plt #matplotlib will be used to visualize the data in the form of graphs.\nimport numpy as np # linear algebra\nimport string #for text pre-processing\nfrom nltk.corpus import stopwords #for removing stopwords\nimport re #Regular expression operations\nimport xgboost as xgb #For predicting the values\nfrom sklearn.model_selection import KFold #for cross validations(CV)\nfrom sklearn import metrics #for getting CV score\nfrom collections import Counter #counting of words in the texts\nimport operator\nfrom nltk import ngrams\nimport nltk\nfrom nltk import word_tokenize\n\n"},{"cell_type":"code","metadata":{"_uuid":"370ae0a4f1b4def7117a2b648587252707fc6ef9","_cell_guid":"bddcb0e9-75ff-4b40-a9f4-8b29086e79a7"},"outputs":[],"execution_count":null,"source":"#Now we will read the input data and store them in dataframes for further processing.\ntraining_df = pd.read_csv(\"../input/spooky-author-identification/train.csv\")\ntesting_df = pd.read_csv(\"../input/spooky-author-identification/test.csv\")\n\n#Once the data is loaded, we need to check the data and if it is loaded correctly. Executing the below\n#command will display the top 5 rows of the training_df dataframe.\ntraining_df.head(5)"},{"cell_type":"code","metadata":{"_uuid":"bb94139335df110283fe1ce6bf2d13d9c2be2eb1","_cell_guid":"ee631d96-7dc4-46de-9acb-719f7a1b779a"},"outputs":[],"execution_count":null,"source":"#similarly we will check the testing dataframe.\ntesting_df.head(5)"},{"cell_type":"markdown","metadata":{"_uuid":"70023d8e2deda344cec16ef09c0119182172edc8","_cell_guid":"3e31e515-7dc1-465e-8055-aa0228ea62e0"},"source":"As expected the testing data does not have the author name. :-)\nAs we can see in the training_df, the three authors(classes) have been mentioned against the strings(text).\n\nWe need to check if the classes are balanced - i.e. the ratio of inputs provided for each author. \n\nIn the below line we have created new dataframe, in which grouping is done on the basis of author."},{"cell_type":"code","metadata":{"_uuid":"b801c59ef6b29cec90e3921a2ae33a59452b997a","_cell_guid":"cc4f4d2e-d6be-444c-afbf-6b3099e80067"},"outputs":[],"execution_count":null,"source":"training_author_df = training_df.groupby('author',as_index=False).count()\ntraining_author_df.head()"},{"cell_type":"code","metadata":{"_uuid":"1f969952257bf82fc4621e6bb5ef17aa4b68203c","_cell_guid":"21f2a4a6-0e7e-4932-8f78-73e18447dab7"},"outputs":[],"execution_count":null,"source":"#Though it is clear that there is no much difference between the inputs for each classes(authors),\n#we can still have a look at bar-chart for better visualisation.\n\nobjects = training_author_df['author']  #storing values of authors in objects\ny_pos = np.arange(len(objects)) #creating numpy array for the count of authors\nids = training_author_df['id'] #assignig values of id for each author\n \nplt.bar(y_pos, ids, align='center', alpha=0.4,color = 'bgk') #basic configuration for bar chart\nplt.xticks(y_pos, objects) #assigning Labels to be displayed on X-axis\nplt.ylabel('Input Count') #Labels for Y-axis\nplt.title('Inputs per author') #Label for Chart\n \nplt.show()\n"},{"cell_type":"markdown","metadata":{"_uuid":"d30e70e4ab5e43375de79824a2207c39028323ad","_cell_guid":"faebd0bf-1099-431a-98b9-02f09fbd6135"},"source":"As it is clear for above chart and training_author_df, there is no much difference between inputs-we shall proceed with feature creation for the inputs.\n\nBefore feature creation it is important to do some pre-processing for the data. As a part of pre-processing,we will be removing punctuations and stopwords from text using string library.\n\nLets pick a string from data, for easy re-presentation.\n"},{"cell_type":"code","metadata":{"_uuid":"9fda5ef80bd516954f44756186d412c0e63f2ba6","_cell_guid":"ad0e709c-d488-4a0f-8697-7875cee62831"},"outputs":[],"execution_count":null,"source":"test_string = training_df.iloc[0]['text']\n\ntest_string"},{"cell_type":"markdown","metadata":{"_uuid":"0235d12e6fa1fe886436c8deca9792e869e2d74f","_cell_guid":"16a1545f-a38a-4e06-a3c2-cf57bf38553e"},"source":"For effective processing of the text, it is better to remove punctuations and stopwords from the text.\nTo remove punctuations we will write a function: remove_punctuations_from_string\n\nLets see what all punctuations are present in string.punctuation\n"},{"cell_type":"code","metadata":{"_uuid":"00cc88c00d7a929df5135f9429d4f47526af551a","_cell_guid":"14ddb4bc-3b4c-46df-a6e1-2dea8d095003"},"outputs":[],"execution_count":null,"source":"string.punctuation"},{"cell_type":"code","metadata":{"_uuid":"b3b5b854ef1f5895c9775a3a0ae045fa1c7a49c3","_cell_guid":"467f95c4-2762-4ed1-b3c6-70933bcde9d6"},"outputs":[],"execution_count":null,"source":"#Function for removing punctuations from string\ndef remove_punctuations_from_string(string1):\n    string1 = string1.lower() #changing to lower case\n    translation_table = dict.fromkeys(map(ord, string.punctuation), ' ') #creating dictionary of punc & None\n    string2 = string1.translate(translation_table) #translating string1\n    return string2\n#lets check the function on our test string.\n\nprint('After processing')\ntest_string = remove_punctuations_from_string(test_string)\ntest_string\n"},{"cell_type":"code","metadata":{"_uuid":"8f98ba8a0c9e891af3d6e844128e44b9c4898076","_cell_guid":"66b80949-3b15-4b34-a678-f8f2837f25e0"},"outputs":[],"execution_count":null,"source":"#The punctuations have been removed from the string. Lets write a similar function for removing\n#stopwords.\n\ndef remove_stopwords_from_string(string1):\n    pattern = re.compile(r'\\b(' + r'|'.join(stopwords.words('english')) + r')\\b\\s*') #compiling all stopwords.\n    string2 = pattern.sub('', string1) #replacing the occurrences of stopwords in string1\n    return string2\n\nprint('After processing')\ntest_string = remove_stopwords_from_string(test_string)\ntest_string\n"},{"cell_type":"code","metadata":{"_uuid":"4d35c606d98e385bff267e7beffef937c00fe38d","_cell_guid":"93b65b74-21ce-48f3-9b1c-f7d3bef9c8d2"},"outputs":[],"execution_count":null,"source":"#This seems to be fine. Now we can apply above functions on our dataframe. Lets check our dataframe\n#again.\n\ntraining_df.head(5)"},{"cell_type":"code","metadata":{"_uuid":"72ab1bf9d1d9b328e07d4f8b4d82d49cd4ed7ac6","_cell_guid":"f5d8237d-5fd5-4549-85a3-8e022b4c531a"},"outputs":[],"execution_count":null,"source":"#Lets take backup of un-processed text, we might need it for future functions.\n#We will perform all actions on testing_df aswell to avoid any errors in future.\ntraining_df[\"text_backup\"] = training_df[\"text\"] #Creating new column text_backup same as text.\ntesting_df[\"text_backup\"] = testing_df[\"text\"] #Creating new column text_backup same as text.\n\n\n#Applying above made functions on text.\ntraining_df[\"text\"] = training_df[\"text\"].apply(lambda x:remove_punctuations_from_string(x))\ntraining_df[\"text\"] = training_df[\"text\"].apply(lambda x:remove_stopwords_from_string(x))\ntesting_df[\"text\"] = testing_df[\"text\"].apply(lambda x:remove_punctuations_from_string(x))\ntesting_df[\"text\"] = testing_df[\"text\"].apply(lambda x:remove_stopwords_from_string(x))\n\ntraining_df.head(5)"},{"cell_type":"code","metadata":{"_uuid":"de8840fdcf07ec6c56a1fcbbd5cad63b47f2d4c3","_cell_guid":"cedf0a4c-7870-4fce-b495-730ba4f921fc"},"outputs":[],"execution_count":null,"source":"#Now we have processed and pre-processed text in our dataframe. Lets start making features from\n#the above data.\n\n#Initially we will create the basic features: 1 - Count of words in a statement(Vocab size), \n#2 - Count of characters in a statement & 3 - Diversity_score.\n\n#In most of the cases above 3 features display the variations between writing styles of the authors.\n\n#Feature 1 - Length of the input OR count of the words in the statement(Vocab size).\ntraining_df['Feature_1']= training_df[\"text_backup\"].apply(lambda x: len(str(x).split()))\ntesting_df['Feature_1']= testing_df[\"text_backup\"].apply(lambda x: len(str(x).split()))\n\n#Feature 2 - Count of characters in a statement\ntraining_df['Feature_2'] = training_df[\"text_backup\"].apply(lambda x: len(str(x)))\ntesting_df['Feature_2'] = testing_df[\"text_backup\"].apply(lambda x: len(str(x)))\n\n#Feature 3-Diversity_score i.e. Average length of words used in statement\ntraining_df['Feature_3'] = training_df['Feature_2'] / training_df['Feature_1']\ntesting_df['Feature_3'] = testing_df['Feature_2'] / testing_df['Feature_1']\n\ntraining_df.head(5)"},{"cell_type":"code","metadata":{"_uuid":"e73a9d672b47549ebafd99270cdcab5d7cf5619c","_cell_guid":"181a088c-4f37-47d3-9d6b-080c987c546c","collapsed":true},"outputs":[],"execution_count":null,"source":"#The usage of stop words can be another writing pattern. So the fourth feature is count of stopwords.\n#Feature_4 = Count of stopwords in the sentence.\nstop_words = set(stopwords.words('english'))\ntraining_df['Feature_4'] = training_df[\"text_backup\"].apply(lambda x: len([w for w in str(x).lower().split() if w in stop_words]) )\ntesting_df['Feature_4'] = testing_df[\"text_backup\"].apply(lambda x: len([w for w in str(x).lower().split() if w in stop_words]) )\n\n#Let us identify the highest used words(other than stopwords) in our input data for further feature generation.\n\n#getting all text in single list: Though there are several other quicker options to do this, but\n#this is the most accurate and convinient of them.\nall_text_without_sw = ''\nfor i in training_df.itertuples():\n    all_text_without_sw = all_text_without_sw +  str(i.text)\n#getting counts of each words:\ncounts = Counter(re.findall(r\"[\\w']+\", all_text_without_sw))\n#deleting ' from counts\ndel counts[\"'\"]\n#getting top 50 used words:\nsorted_x = dict(sorted(counts.items(), key=operator.itemgetter(1),reverse=True)[:50])\n\n#Feature-5: The count of top used words.\ntraining_df['Feature_5'] = training_df['text'].apply(lambda x: len([w for w in str(x).lower().split() if w in sorted_x]) )\ntesting_df['Feature_5'] = testing_df['text'].apply(lambda x: len([w for w in str(x).lower().split() if w in sorted_x]) )\n"},{"cell_type":"code","metadata":{"_uuid":"e6edb52a8ff1e1bda44c090b582fa853b40c2ead","_cell_guid":"2272cd7f-abc0-4897-a608-79fafa088edf","collapsed":true},"outputs":[],"execution_count":null,"source":"#Similarly lets identify the least used words:\nreverted_x = dict(sorted(counts.items(), key=operator.itemgetter(1))[:10000])\n#Feature-6: The count of least used words.\ntraining_df['Feature_6'] = training_df['text'].apply(lambda x: len([w for w in str(x).lower().split() if w in reverted_x]) )\ntesting_df['Feature_6'] = testing_df['text'].apply(lambda x: len([w for w in str(x).lower().split() if w in reverted_x]) )\n\n#Feature-7: Count of punctuations in the input.\ntraining_df['Feature_7'] = training_df['text_backup'].apply(lambda x: len([w for w in str(x) if w in string.punctuation]) )\ntesting_df['Feature_7'] = testing_df['text_backup'].apply(lambda x: len([w for w in str(x) if w in string.punctuation]) )\n"},{"cell_type":"code","metadata":{},"outputs":[],"execution_count":null,"source":"#Let's plot these features on a chart. To view these feature we will write a function:\ndef plot_bar_chart_from_dataframe(dataframe1,key_column,columns_to_be_plotted):\n    import pandas as pd\n    test_df1 = dataframe1.groupby(key_column).sum()\n    test_df2 = pd.DataFrame()\n    for column in columns_to_be_plotted:\n        test_df2[column] = round(test_df1[column]/ test_df1[column].sum()*100,2)\n    test_df2 = test_df2.T \n    ax = test_df2.plot(kind='bar', stacked=True, figsize =(10,5),legend = 'reverse',title = '% usage for each author')\n    for p in ax.patches:\n        a = p.get_x()+0.4\n        ax.annotate(str(p.get_height()), (a, p.get_y()), xytext=(5, 10), textcoords='offset points')\n\nkey_column = 'author'\ncolumns_to_be_plotted = ['Feature_4','Feature_5','Feature_6','Feature_7']\nplot_bar_chart_from_dataframe(training_df,key_column,columns_to_be_plotted)"},{"cell_type":"code","metadata":{"_uuid":"6f313023706d639d6b48a84f6aaf87750182704c","_cell_guid":"4339e00d-540a-4ae9-ab1f-ffbd592a2a87","collapsed":true},"outputs":[],"execution_count":null,"source":"#Feature-8: Count of UPPER case words.\ntraining_df['Feature_8'] = training_df['text_backup'].apply(lambda x: len([w for w in str(x).replace('I','i').replace('A','a').split() if w.isupper() == True]) )\ntesting_df['Feature_8'] = testing_df['text_backup'].apply(lambda x: len([w for w in str(x).replace('I','i').replace('A','a').split() if w.isupper() == True]) )\n\n#Feature-9: Count of Title case words.\ntraining_df['Feature_9'] = training_df['text_backup'].apply(lambda x: len([w for w in str(x).replace('I','i').replace('A','a').split() if w.istitle() == True]) )\ntesting_df['Feature_9'] = testing_df['text_backup'].apply(lambda x: len([w for w in str(x).replace('I','i').replace('A','a').split() if w.istitle() == True]) )\n\n#The above features are common features which can indicate a writing pattern. There might be a possibility\n#that a writer is using words which START WITH or END WITH particular characters. Lets try to identify them.\n\nstarting_words = sorted(list(map(lambda word : word[:2],filter(lambda word : len(word) > 3,all_text_without_sw.split()))))\nsw_counts = Counter(starting_words)\ntop_30_sw = dict(sorted(sw_counts.items(), key=operator.itemgetter(1),reverse=True)[:30])\n\n#Feature-10: Count of (Most words start with)\ntraining_df['Feature_10'] = training_df['text'].apply(lambda x: len([w for w in str(x).lower().split() if w[:2] in top_30_sw and w not in stop_words]) )\ntesting_df['Feature_10'] = testing_df['text'].apply(lambda x: len([w for w in str(x).lower().split() if w[:2] in top_30_sw and w not in stop_words]) )\n\n#Feature-11: Count of (Most words end with)\nending_words = sorted(list(map(lambda word : word[-2:],filter(lambda word : len(word) > 3,all_text_without_sw.split()))))\new_counts = Counter(ending_words)\ntop_30_ew = dict(sorted(sw_counts.items(), key=operator.itemgetter(1),reverse=True)[:30])\ntraining_df['Feature_11'] = training_df['text'].apply(lambda x: len([w for w in str(x).lower().split() if w[:2] in top_30_ew and w not in stop_words]) )\ntesting_df['Feature_11'] = testing_df['text'].apply(lambda x: len([w for w in str(x).lower().split() if w[:2] in top_30_ew and w not in stop_words]) )\n\n"},{"cell_type":"markdown","metadata":{"_uuid":"9f907a20d838fce0d1027dcd2d1d4cf74d566930","_cell_guid":"066c6c5c-e7c4-4a7b-b6a5-400328be5aa5","collapsed":true},"source":"It is a possibility that some author uses reference of particular locations in their text, it can be a city or country. To check this, we will create features for City and Country names used. For this we need City and Country corpus, but I did not find any readily available corpus.\n\nSo I created a list in excel file and have used as input for these features."},{"cell_type":"code","metadata":{"_uuid":"9e3eff67cf6003f5b3b5e1b8f24e8e7638184253","_cell_guid":"5ee4d930-1509-48ba-a9b9-e5314eb742da"},"outputs":[],"execution_count":null,"source":"list_of_cities_excel_file ='../input/city-country/list_of_cities.xlsx'\ncity_df = pd.read_excel(open(list_of_cities_excel_file,'rb'), sheet_name='Sheet1')\n\ncity_df.head(5)"},{"cell_type":"markdown","metadata":{"_uuid":"060d21dd267657ab19997e8692201d8a7a8e74a9","_cell_guid":"7fa93173-8bd9-43b1-8425-f87506c40d9e"},"source":"As the list of City & Country is available in dataframe, now we will create Features dependent on them."},{"cell_type":"code","metadata":{"_uuid":"6a2664aeb10e9c679aa9b147d5296fd9122e9da6","_cell_guid":"6661d061-4164-46c7-ad74-adda54ff99a0","collapsed":true},"outputs":[],"execution_count":null,"source":"city_list = city_df['City'].tolist()\ncity_list = [x.lower() for x in city_list]\n\n#We have list of cities used in the input text. We will create our Feature 12 on the basis of this list.\n\n#Feature-12: Count of City names used.\ntraining_df['Feature_12'] = training_df['text'].apply(lambda x: len([w for w in str(x).lower().split() if w in city_list]) )\ntesting_df['Feature_12'] = testing_df['text'].apply(lambda x: len([w for w in str(x).lower().split() if w in city_list]) )\n\n"},{"cell_type":"code","metadata":{"_uuid":"0b44aa7a59e474369de4a16fe7160b5dfab5d8e9","_cell_guid":"c467fd0d-87d7-48a3-9d87-2e96036f3bda","collapsed":true},"outputs":[],"execution_count":null,"source":"country_list = city_df['Country'].tolist()\ncountry_list = [x.lower() for x in country_list]\n\n#Feature-13: Count of Country names used.\ntraining_df['Feature_13'] = training_df['text'].apply(lambda x: len([w for w in str(x).lower().split() if w in country_list]) )\ntesting_df['Feature_13'] = testing_df['text'].apply(lambda x: len([w for w in str(x).lower().split() if w in country_list]) )\n\n"},{"cell_type":"markdown","metadata":{"_uuid":"9d5a8231c9318ff73d129c34b5f1696b9eec10be","_cell_guid":"18c89b32-c722-4558-a69f-773655308ea2"},"source":"Another possible option to identify writing patterns can be Trigrams. So we will identify top 10 trigrams from the text and check for its usage and make our 14th Feature."},{"cell_type":"code","metadata":{"_uuid":"066afec6bf3db6524b715bae34746f12d32c83e1","_cell_guid":"3c63bbd7-5baa-49cb-bc32-b92dbe75445d","collapsed":true},"outputs":[],"execution_count":null,"source":"#Function for getting Trigram from text:\ndef ngram_list_from_string(string1,count_of_words_in_ngram):\n    string1 = string1.lower()\n    string1 = string1.replace('.','. ')\n    all_grams = ngrams(string1.split(), count_of_words_in_ngram)\n    grams_list = []\n    for grams in all_grams:\n        grams_list.append(grams)\n    return(grams_list)"},{"cell_type":"code","metadata":{"_uuid":"e97fd544087b0280de68c62f2502c3f281e85e66","_cell_guid":"cf79045a-e409-43b8-8307-d99693bc1341","collapsed":true},"outputs":[],"execution_count":null,"source":"#Getting Trigram for text:\nngram_list = ngram_list_from_string(all_text_without_sw,3)\n\n#Getting count for every ngram:\nngram_counts = Counter(ngram_list)\n\n#Getting top 10 ngram as per highest count:\nsorted_ngram = dict(sorted(ngram_counts.items(), key=operator.itemgetter(1),reverse=True)[:10])\n\n#Feature-14: Top 10 trigram occurence:\ntraining_df['Feature_14'] = training_df['text_backup'].apply(lambda x: len([w for w in ngram_list_from_string(x,3)if w in sorted_ngram]) )\ntesting_df['Feature_14'] = testing_df['text_backup'].apply(lambda x: len([w for w in ngram_list_from_string(x,3)if w in sorted_ngram]) )\n\n"},{"cell_type":"markdown","metadata":{"_uuid":"615b2bf6be7edc8eb243f81529af70b12c28b6c7","_cell_guid":"932d33be-ae38-4df2-b8e9-83ea5195cb07"},"source":"Now we will identify the Part Of Speech from the written text. We will create 5 features for Nouns, Pronouns, Verbs, Adverbs, Adjectives.\n"},{"cell_type":"code","metadata":{"_uuid":"3ce286ee97cb4249c490d0f678f624f8305bfa36","_cell_guid":"57e87c48-7dc9-4f35-8613-5a307d272366","collapsed":true},"outputs":[],"execution_count":null,"source":"tokenized_all_text = word_tokenize(all_text_without_sw) #tokenize the text\nlist_of_tagged_words = nltk.pos_tag(tokenized_all_text) #adding POS Tags to tokenized words\n\nset_pos  = (set(list_of_tagged_words)) # set of POS tags & words\n\nnouns = ['NN','NNS','NNP','NNPS'] #POS tags of nouns\nlist_of_words = set(map(lambda tuple_2 : tuple_2[0], filter(lambda tuple_2 : tuple_2[1] in  nouns, set_pos)))\ntraining_df['Feature_15'] = training_df['text'].apply(lambda x: len([w for w in str(x).lower().split() if w in list_of_words]) )\ntesting_df['Feature_15'] = testing_df['text'].apply(lambda x: len([w for w in str(x).lower().split() if w in list_of_words]) )\n\npronouns = ['PRP','PRP$','WP','WP$'] # POS tags of pronouns\nlist_of_words = set(map(lambda tuple_2 : tuple_2[0], filter(lambda tuple_2 : tuple_2[1] in  pronouns, set_pos)))\ntraining_df['Feature_16'] = training_df['text'].apply(lambda x: len([w for w in str(x).lower().split() if w in list_of_words]) )\ntesting_df['Feature_16'] = testing_df['text'].apply(lambda x: len([w for w in str(x).lower().split() if w in list_of_words]) )\n\nverbs = ['VB','VBD','VBG','VBN','VBP','VBZ'] #POS tags of verbs\nlist_of_words = set(map(lambda tuple_2 : tuple_2[0], filter(lambda tuple_2 : tuple_2[1] in  verbs, set_pos)))\ntraining_df['Feature_17'] = training_df['text'].apply(lambda x: len([w for w in str(x).lower().split() if w in list_of_words]) )\ntesting_df['Feature_17'] = testing_df['text'].apply(lambda x: len([w for w in str(x).lower().split() if w in list_of_words]) )\n\nadverbs = ['RB','RBR','RBS','WRB'] #POS tags of adverbs\nlist_of_words = set(map(lambda tuple_2 : tuple_2[0], filter(lambda tuple_2 : tuple_2[1] in  adverbs, set_pos)))\ntraining_df['Feature_18'] = training_df['text'].apply(lambda x: len([w for w in str(x).lower().split() if w in list_of_words]) )\ntesting_df['Feature_18'] = testing_df['text'].apply(lambda x: len([w for w in str(x).lower().split() if w in list_of_words]) )\n\nadjectives = ['JJ','JJR','JJS'] #POS tags of adjectives\nlist_of_words = set(map(lambda tuple_2 : tuple_2[0], filter(lambda tuple_2 : tuple_2[1] in  adjectives, set_pos)))\ntraining_df['Feature_19'] = training_df['text'].apply(lambda x: len([w for w in str(x).lower().split() if w in list_of_words]) )\ntesting_df['Feature_19'] = testing_df['text'].apply(lambda x: len([w for w in str(x).lower().split() if w in list_of_words]) )\n"},{"cell_type":"markdown","metadata":{"_uuid":"7ecf4a1f875fcad14b7853c84124792c3a9f0368","_cell_guid":"19ba5044-3ff9-450c-a417-bb5f8e49e18d"},"source":"Currently, I am publishing the above code with 19 Text based features, though have some more in mind but will do in recent future.\n\nAlso this is my FIRST Kernel,request you to please comment with your advise, suggestions and honest feedback on the same.\n"}],"nbformat_minor":1,"nbformat":4}