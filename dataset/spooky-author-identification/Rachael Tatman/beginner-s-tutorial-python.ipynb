{"nbformat":4,"nbformat_minor":1,"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"4ea712d0-9f9d-4d68-a572-6ac2b119006a","_uuid":"ab9733987f40fa168b7c1e84964dca054f6c3fed"},"source":"In this tutorial we're going to get started with some basic natural language processing (NLP) tasks. We're going to:\n\n* Read in some helpful NLP libraries & our dataset\n* Find out how often each author uses each word\n* Use that to guess which author wrote a sentence\n\nReady? Let's get started! :D"},{"cell_type":"markdown","metadata":{"_cell_guid":"10814c6c-e718-43e8-90a4-aaac69e6422f","_uuid":"c70d2e764647d7ac1fe0c97210051f9ce98ef417"},"source":"## General intuition \n\nFor this tutorial, we're going to be guessing which author wrote a string of text based on the normalized unigram frequency. That's just a fancy way of saying that we're going to count how often each author uses every word in our training data and then divide by the number of total words they wrote. Then, if our test sentence has words that we've seen one author use a lot more than the others, we will guess that that person is probably the author.\n\nLet's imagine that this is our training corpus:\n\n* Author one: \"A very spooky thing happened. The thing was so spooky I screamed.\"\n* Author two: \"I ate a tasty candy apple. It was delicious\"\n\nAnd that this is our test sentence that we want to figure out who wrote:\n\n* Author ???: \"What a spooky thing!\"\n\nJust looking at it, it seems more likely that author one wrote this sentence. Author ones says both \"spooky\" and \"thing\" a lot, while author two does not (at least, based on our training data). Since we see both \"spooky\" and \"thing\" in our test sentence, it seems more likely that it was written by author one than author two--even though the test sentence does have the word \"a\" in it, which we have seen author two use too.\n\nIn the rest of this tutorial we're going to figure out how to translate this intution into code."},{"cell_type":"markdown","metadata":{"_cell_guid":"52783943-8073-4d7d-815d-868500ecf850","_uuid":"12b7cdc56b317df2ef4392c3b304a51ceda10caf"},"source":"## Read in some helpful NLP libraries & our dataset\n\nFor this tutorial, I'm going to be using the Natural Language Toolkit, also called the \"NLTK\". It's an open-source Python library for analyzing language data. The really nice thing about the NLTK is that it has a really helpful book that goes step-by-step through a lot of the common NLP tasks. Even better: you can get the book for free [here](http://www.nltk.org/book/)."},{"metadata":{"_cell_guid":"e5d6cdb9-0a56-44b2-99da-ae2432f89b9f","collapsed":true,"_uuid":"eb67d93a8e3ae84d499f706568d7611627f5308a"},"outputs":[],"cell_type":"code","execution_count":null,"source":"# read in some helpful libraries\nimport nltk # the natural langauage toolkit, open-source NLP\nimport pandas as pd # dataframes\n\n### Read in the data\n\n# read our data into a dataframe\ntexts = pd.read_csv(\"../input/train.csv\")\n\n# look at the first few rows\ntexts.head()"},{"cell_type":"markdown","metadata":{"_cell_guid":"75f43a03-d83d-4551-ac9a-4ac4d0764b32","_uuid":"668599ac03c8b70093377c7250cd34e93ba6cffe"},"source":"## Find out how often each author uses each word\n\nA lot of NLP applications rely on counting how often certain words are used. (The fancy term for this is \"word frequency\".) Let's look at the word frequency for each of the authors in our dataset. The NLTK has lots of nice built-in functions and data structures for this that we can make use of."},{"metadata":{"_cell_guid":"0456a8ef-0aca-4b40-8199-0e4fdec81671","collapsed":true,"_uuid":"025540f501355cfb33e6bc256ad3c0980793d0fd"},"outputs":[],"cell_type":"code","execution_count":null,"source":"### Split data\n\n# split the data by author\nbyAuthor = texts.groupby(\"author\")\n\n### Tokenize (split into individual words) our text\n\n# word frequency by author\nwordFreqByAuthor = nltk.probability.ConditionalFreqDist()\n\n# for each author...\nfor name, group in byAuthor:\n    # get all of the sentences they wrote and collapse them into a\n    # single long string\n    sentences = group['text'].str.cat(sep = ' ')\n    \n    # convert everything to lower case (so \"The\" and \"the\" get counted as \n    # the same word rather than two different words)\n    sentences = sentences.lower()\n    \n    # split the text into individual tokens    \n    tokens = nltk.tokenize.word_tokenize(sentences)\n    \n    # calculate the frequency of each token\n    frequency = nltk.FreqDist(tokens)\n\n    # add the frequencies for each author to our dictionary\n    wordFreqByAuthor[name] = (frequency)\n    \n# now we have an dictionary where each entry is the frequency distrobution\n# of words for a specific author.     "},{"cell_type":"markdown","metadata":{"_cell_guid":"a0d3c955-c7ac-4460-befa-7743d7ba37ec","_uuid":"1c10083be2d0bb1d8123e71cda822e35f7809bae"},"source":"Now we can look at how often each writer uses specific words. Since this is a Halloween competition, how about \"blood\", \"scream\" and \"fear\"? üëªüò®üßõ‚Äç‚ôÄÔ∏è"},{"metadata":{"_cell_guid":"56d4ab79-3361-4410-8d89-47b014b08357","collapsed":true,"_uuid":"356327c380228999a0476a82076486535891d021"},"outputs":[],"cell_type":"code","execution_count":null,"source":"# see how often each author says \"blood\"\nfor i in wordFreqByAuthor.keys():\n    print(\"blood: \" + i)\n    print(wordFreqByAuthor[i].freq('blood'))\n\n# print a blank line\nprint()\n\n# see how often each author says \"scream\"\nfor i in wordFreqByAuthor.keys():\n    print(\"scream: \" + i)\n    print(wordFreqByAuthor[i].freq('scream'))\n    \n# print a blank line\nprint()\n\n# see how often each author says \"fear\"\nfor i in wordFreqByAuthor.keys():\n    print(\"fear: \" + i)\n    print(wordFreqByAuthor[i].freq('fear'))"},{"cell_type":"markdown","metadata":{"_cell_guid":"bcf7a4fe-48c7-4351-bf43-f9e1d9272592","_uuid":"eb4e859307aa90d239ed9066180a0df958916e19"},"source":"## Use word frequency to guess which author wrote a sentence\n\nThe general idea is is that different people tend to use different words more or less often. (I had a beloved college professor that was especially fond of \"gestalt\".) If you're not sure who said something but it has a lot of words one person uses a lot in it, then you might guess that they were the one who wrote it. \n\nLet's use this general principle to guess who might have been more likely to write the sentence \"It was a dark and stormy night.\""},{"metadata":{"_cell_guid":"221c3934-12f9-4e8f-8e91-035c7fd97b28","collapsed":true,"_uuid":"9f17c73bd55582b518ddbcfdba4053b1dc0182f3"},"outputs":[],"cell_type":"code","execution_count":null,"source":"# One way to guess authorship is to use the joint probabilty that each \n# author used each word in a given sentence.\n\n# first, let's start with a test sentence\ntestSentence = \"It was a dark and stormy night.\"\n\n# and then lowercase & tokenize our test sentence\npreProcessedTestSentence = nltk.tokenize.word_tokenize(testSentence.lower())\n\n# create an empy dataframe to put our output in\ntestProbailities = pd.DataFrame(columns = ['author','word','probability'])\n\n# For each author...\nfor i in wordFreqByAuthor.keys():\n    # for each word in our test sentence...\n    for j  in preProcessedTestSentence:\n        # find out how frequently the author used that word\n        wordFreq = wordFreqByAuthor[i].freq(j)\n        # and add a very small amount to every prob. so none of them are 0\n        smoothedWordFreq = wordFreq + 0.000001\n        # add the author, word and smoothed freq. to our dataframe\n        output = pd.DataFrame([[i, j, smoothedWordFreq]], columns = ['author','word','probability'])\n        testProbailities = testProbailities.append(output, ignore_index = True)\n\n# empty dataframe for the probability that each author wrote the sentence\ntestProbailitiesByAuthor = pd.DataFrame(columns = ['author','jointProbability'])\n\n# now let's group the dataframe with our frequency by author\nfor i in wordFreqByAuthor.keys():\n    # get the joint probability that each author wrote each word\n    oneAuthor = testProbailities.query('author == \"' + i + '\"')\n    jointProbability = oneAuthor.product(numeric_only = True)[0]\n    \n    # and add that to our dataframe\n    output = pd.DataFrame([[i, jointProbability]], columns = ['author','jointProbability'])\n    testProbailitiesByAuthor = testProbailitiesByAuthor.append(output, ignore_index = True)\n\n# and our winner is...\ntestProbailitiesByAuthor.loc[testProbailitiesByAuthor['jointProbability'].idxmax(),'author']"},{"cell_type":"markdown","metadata":{"_cell_guid":"fca66518-050f-4484-b572-272ed911b805","_uuid":"e6eb3ec205947d5a15a8288c6eefc4f811eafeb4"},"source":"So based on what we've seen in our training data, it looks like of our three authors, H.P. Lovecraft was the most likely to write the sentence \"It was a dark and stormy night\"."},{"cell_type":"markdown","metadata":{},"source":"## Ready for more?\n\nNow that you've got your feet wet, why not head over to [Sohier's intermediate tutorial](https://www.kaggle.com/sohier/intermediate-tutorial-python/), which includes lots of tips on optimizing your code and getting ready to submit to the competition. "}],"metadata":{"kernelspec":{"name":"python3","language":"python","display_name":"Python 3"},"language_info":{"file_extension":".py","mimetype":"text/x-python","pygments_lexer":"ipython3","name":"python","nbconvert_exporter":"python","codemirror_mode":{"name":"ipython","version":3},"version":"3.6.3"}}}