{"cells":[{"source":"Uses SRKs wonderful features so don't forget to vote for him!","cell_type":"markdown","metadata":{}},{"outputs":[],"execution_count":null,"source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import StandardScaler\nimport nltk\nfrom nltk.corpus import stopwords\nimport string\nimport xgboost as xgb\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn import ensemble, metrics, model_selection, naive_bayes\ncolor = sns.color_palette()\n\n%matplotlib inline\n\neng_stopwords = set(stopwords.words(\"english\"))\npd.options.mode.chained_assignment = None","cell_type":"code","metadata":{"_uuid":"d362e082042808a0a79a4ba5d73ecb4d43442489","collapsed":true,"_cell_guid":"8c77bdad-b474-4265-b008-9e795a0ff33b"}},{"outputs":[],"execution_count":null,"source":"def runXGB(train_X, train_y, test_X, test_y=None, test_X2=None, seed_val=0, child=1, colsample=0.3):\n    param = {}\n    param['objective'] = 'multi:softprob'\n    param['eta'] = 0.1\n    param['max_depth'] = 3\n    param['silent'] = 1\n    param['num_class'] = 3\n    param['eval_metric'] = \"mlogloss\"\n    param['min_child_weight'] = child\n    param['subsample'] = 0.8\n    param['colsample_bytree'] = colsample\n    param['seed'] = seed_val\n    num_rounds = 2000\n\n    plst = list(param.items())\n    xgtrain = xgb.DMatrix(train_X, label=train_y)\n\n    if test_y is not None:\n        xgtest = xgb.DMatrix(test_X, label=test_y)\n        watchlist = [ (xgtrain,'train'), (xgtest, 'test') ]\n        model = xgb.train(plst, xgtrain, num_rounds, watchlist, early_stopping_rounds=50, verbose_eval=20)\n    else:\n        xgtest = xgb.DMatrix(test_X)\n        model = xgb.train(plst, xgtrain, num_rounds)\n\n    pred_test_y = model.predict(xgtest, ntree_limit = model.best_ntree_limit)\n    if test_X2 is not None:\n        xgtest2 = xgb.DMatrix(test_X2)\n        pred_test_y2 = model.predict(xgtest2, ntree_limit = model.best_ntree_limit)\n    return pred_test_y, pred_test_y2, model\n\ndef runMNB(train_X, train_y, test_X, test_y, test_X2):\n    model = naive_bayes.MultinomialNB()\n    model.fit(train_X, train_y)\n    pred_test_y = model.predict_proba(test_X)\n    pred_test_y2 = model.predict_proba(test_X2)\n    return pred_test_y, pred_test_y2, model","cell_type":"code","metadata":{"collapsed":true}},{"outputs":[],"execution_count":null,"source":"train_df = pd.read_csv(\"../input/train.csv\")\ntest_df = pd.read_csv(\"../input/test.csv\")\n## Number of words in the text ##\ntrain_df[\"num_words\"] = train_df[\"text\"].apply(lambda x: len(str(x).split()))\ntest_df[\"num_words\"] = test_df[\"text\"].apply(lambda x: len(str(x).split()))\n\n## Number of unique words in the text ##\ntrain_df[\"num_unique_words\"] = train_df[\"text\"].apply(lambda x: len(set(str(x).split())))\ntest_df[\"num_unique_words\"] = test_df[\"text\"].apply(lambda x: len(set(str(x).split())))\n\n## Number of characters in the text ##\ntrain_df[\"num_chars\"] = train_df[\"text\"].apply(lambda x: len(str(x)))\ntest_df[\"num_chars\"] = test_df[\"text\"].apply(lambda x: len(str(x)))\n\n## Number of stopwords in the text ##\ntrain_df[\"num_stopwords\"] = train_df[\"text\"].apply(lambda x: len([w for w in str(x).lower().split() if w in eng_stopwords]))\ntest_df[\"num_stopwords\"] = test_df[\"text\"].apply(lambda x: len([w for w in str(x).lower().split() if w in eng_stopwords]))\n\n## Number of punctuations in the text ##\ntrain_df[\"num_punctuations\"] =train_df['text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]) )\ntest_df[\"num_punctuations\"] =test_df['text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]) )\n\n## Number of title case words in the text ##\ntrain_df[\"num_words_upper\"] = train_df[\"text\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\ntest_df[\"num_words_upper\"] = test_df[\"text\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\n\n## Number of title case words in the text ##\ntrain_df[\"num_words_title\"] = train_df[\"text\"].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\ntest_df[\"num_words_title\"] = test_df[\"text\"].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\n\n## Average length of the words in the text ##\ntrain_df[\"mean_word_len\"] = train_df[\"text\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\ntest_df[\"mean_word_len\"] = test_df[\"text\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\n\nauthor_mapping_dict = {'EAP':0, 'HPL':1, 'MWS':2}\ntrain_y = train_df['author'].map(author_mapping_dict)\ntrain_id = train_df['id'].values\ntest_id = test_df['id'].values\n\n### recompute the trauncated variables again ###\ntrain_df[\"num_words\"] = train_df[\"text\"].apply(lambda x: len(str(x).split()))\ntest_df[\"num_words\"] = test_df[\"text\"].apply(lambda x: len(str(x).split()))\ntrain_df[\"mean_word_len\"] = train_df[\"text\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\ntest_df[\"mean_word_len\"] = test_df[\"text\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\n\ncols_to_drop = ['id', 'text']\ntrain_X = train_df.drop(cols_to_drop+['author'], axis=1)\ntest_X = test_df.drop(cols_to_drop, axis=1)\n\ntfidf_vec = TfidfVectorizer(stop_words='english', ngram_range=(1,3))\nfull_tfidf = tfidf_vec.fit_transform(train_df['text'].values.tolist() + test_df['text'].values.tolist())\ntrain_tfidf = tfidf_vec.transform(train_df['text'].values.tolist())\ntest_tfidf = tfidf_vec.transform(test_df['text'].values.tolist())\n\nn_comp = 20\nsvd_obj = TruncatedSVD(n_components=n_comp, algorithm='arpack')\nsvd_obj.fit(full_tfidf)\ntrain_svd = pd.DataFrame(svd_obj.transform(train_tfidf))\ntest_svd = pd.DataFrame(svd_obj.transform(test_tfidf))\n    \ntrain_svd.columns = ['svd_word_'+str(i) for i in range(n_comp)]\ntest_svd.columns = ['svd_word_'+str(i) for i in range(n_comp)]\ntrain_df = pd.concat([train_df, train_svd], axis=1)\ntest_df = pd.concat([test_df, test_svd], axis=1)\ndel full_tfidf, train_tfidf, test_tfidf, train_svd, test_svd\n\ntfidf_vec = CountVectorizer(stop_words='english', ngram_range=(1,3))\ntfidf_vec.fit(train_df['text'].values.tolist() + test_df['text'].values.tolist())\ntrain_tfidf = tfidf_vec.transform(train_df['text'].values.tolist())\ntest_tfidf = tfidf_vec.transform(test_df['text'].values.tolist())\n\ncv_scores = []\npred_full_test = 0\npred_train = np.zeros([train_df.shape[0], 3])\nkf = model_selection.KFold(n_splits=5, shuffle=True, random_state=2017)\nfor dev_index, val_index in kf.split(train_X):\n    dev_X, val_X = train_tfidf[dev_index], train_tfidf[val_index]\n    dev_y, val_y = train_y[dev_index], train_y[val_index]\n    pred_val_y, pred_test_y, model = runMNB(dev_X, dev_y, val_X, val_y, test_tfidf)\n    pred_full_test = pred_full_test + pred_test_y\n    pred_train[val_index,:] = pred_val_y\n    cv_scores.append(metrics.log_loss(val_y, pred_val_y))\nprint(\"Mean cv score : \", np.mean(cv_scores))\npred_full_test = pred_full_test / 5.\n\n# add the predictions as new features #\ntrain_df[\"nb_cvec_eap\"] = pred_train[:,0]\ntrain_df[\"nb_cvec_hpl\"] = pred_train[:,1]\ntrain_df[\"nb_cvec_mws\"] = pred_train[:,2]\ntest_df[\"nb_cvec_eap\"] = pred_full_test[:,0]\ntest_df[\"nb_cvec_hpl\"] = pred_full_test[:,1]\ntest_df[\"nb_cvec_mws\"] = pred_full_test[:,2]\n\n### Fit transform the tfidf vectorizer ###\ntfidf_vec = CountVectorizer(ngram_range=(1,7), analyzer='char')\ntfidf_vec.fit(train_df['text'].values.tolist() + test_df['text'].values.tolist())\ntrain_tfidf = tfidf_vec.transform(train_df['text'].values.tolist())\ntest_tfidf = tfidf_vec.transform(test_df['text'].values.tolist())\n\ncv_scores = []\npred_full_test = 0\npred_train = np.zeros([train_df.shape[0], 3])\nkf = model_selection.KFold(n_splits=5, shuffle=True, random_state=2017)\nfor dev_index, val_index in kf.split(train_X):\n    dev_X, val_X = train_tfidf[dev_index], train_tfidf[val_index]\n    dev_y, val_y = train_y[dev_index], train_y[val_index]\n    pred_val_y, pred_test_y, model = runMNB(dev_X, dev_y, val_X, val_y, test_tfidf)\n    pred_full_test = pred_full_test + pred_test_y\n    pred_train[val_index,:] = pred_val_y\n    cv_scores.append(metrics.log_loss(val_y, pred_val_y))\nprint(\"Mean cv score : \", np.mean(cv_scores))\npred_full_test = pred_full_test / 5.\n\n# add the predictions as new features #\ntrain_df[\"nb_cvec_char_eap\"] = pred_train[:,0]\ntrain_df[\"nb_cvec_char_hpl\"] = pred_train[:,1]\ntrain_df[\"nb_cvec_char_mws\"] = pred_train[:,2]\ntest_df[\"nb_cvec_char_eap\"] = pred_full_test[:,0]\ntest_df[\"nb_cvec_char_hpl\"] = pred_full_test[:,1]\ntest_df[\"nb_cvec_char_mws\"] = pred_full_test[:,2]\n\n### Fit transform the tfidf vectorizer ###\ntfidf_vec = TfidfVectorizer(ngram_range=(1,5), analyzer='char')\ntfidf_vec.fit(train_df['text'].values.tolist() + test_df['text'].values.tolist())\ntrain_tfidf = tfidf_vec.transform(train_df['text'].values.tolist())\ntest_tfidf = tfidf_vec.transform(test_df['text'].values.tolist())\n\ncv_scores = []\npred_full_test = 0\npred_train = np.zeros([train_df.shape[0], 3])\nkf = model_selection.KFold(n_splits=5, shuffle=True, random_state=2017)\nfor dev_index, val_index in kf.split(train_X):\n    dev_X, val_X = train_tfidf[dev_index], train_tfidf[val_index]\n    dev_y, val_y = train_y[dev_index], train_y[val_index]\n    pred_val_y, pred_test_y, model = runMNB(dev_X, dev_y, val_X, val_y, test_tfidf)\n    pred_full_test = pred_full_test + pred_test_y\n    pred_train[val_index,:] = pred_val_y\n    cv_scores.append(metrics.log_loss(val_y, pred_val_y))\nprint(\"Mean cv score : \", np.mean(cv_scores))\npred_full_test = pred_full_test / 5.\n\n# add the predictions as new features #\ntrain_df[\"nb_tfidf_char_eap\"] = pred_train[:,0]\ntrain_df[\"nb_tfidf_char_hpl\"] = pred_train[:,1]\ntrain_df[\"nb_tfidf_char_mws\"] = pred_train[:,2]\ntest_df[\"nb_tfidf_char_eap\"] = pred_full_test[:,0]\ntest_df[\"nb_tfidf_char_hpl\"] = pred_full_test[:,1]\ntest_df[\"nb_tfidf_char_mws\"] = pred_full_test[:,2]","cell_type":"code","metadata":{}},{"outputs":[],"execution_count":null,"source":"cols_to_drop = ['id', 'text']\ntrain_X = train_df.drop(cols_to_drop+['author'], axis=1)\ntest_X = test_df.drop(cols_to_drop, axis=1)","cell_type":"code","metadata":{"collapsed":true}},{"outputs":[],"execution_count":null,"source":"ss = StandardScaler()\nss.fit(pd.concat([train_X,test_X]))\nfeatures = train_X.columns\ntrain_X[features] = ss.transform(train_X[features])\ntest_X[features] = ss.transform(test_X[features])","cell_type":"code","metadata":{"collapsed":true}},{"outputs":[],"execution_count":null,"source":"def GPClusterX(data):\n    v = pd.DataFrame()\n    v[\"0\"] = np.tanh((((data[\"nb_cvec_eap\"] - ((data[\"svd_word_0\"] + data[\"svd_word_12\"]) + data[\"nb_cvec_char_hpl\"])) * 2.0) * 2.0))\n    v[\"1\"] = np.tanh(((data[\"nb_tfidf_char_eap\"] + ((((data[\"svd_word_1\"] * 2.0) * 2.0) - data[\"nb_cvec_char_hpl\"]) * 2.0)) * 2.0))\n    v[\"2\"] = np.tanh((((data[\"svd_word_1\"] - (data[\"svd_word_0\"] - (data[\"svd_word_17\"] - data[\"nb_cvec_char_hpl\"]))) * 2.0) * 2.0))\n    v[\"3\"] = np.tanh(((data[\"nb_tfidf_char_eap\"] - ((data[\"svd_word_0\"] + (data[\"nb_cvec_char_hpl\"] + data[\"svd_word_12\"])) * 2.0)) * 2.0))\n    v[\"4\"] = np.tanh((((((data[\"nb_cvec_eap\"] - data[\"svd_word_0\"]) * 2.0) - data[\"nb_cvec_char_hpl\"]) * 2.0) * 2.0))\n    v[\"5\"] = np.tanh(((((data[\"nb_cvec_char_eap\"] - data[\"num_words\"]) - (data[\"svd_word_12\"] + data[\"nb_cvec_char_hpl\"])) * 2.0) * 2.0))\n    v[\"6\"] = np.tanh((((((data[\"svd_word_1\"] * 2.0) - data[\"nb_cvec_char_hpl\"]) * 2.0) + data[\"svd_word_13\"]) * 2.0))\n    v[\"7\"] = np.tanh((((data[\"nb_cvec_char_eap\"] + ((data[\"svd_word_17\"] - data[\"nb_cvec_char_hpl\"]) - data[\"num_stopwords\"])) * 2.0) * 2.0))\n    v[\"8\"] = np.tanh(((((data[\"nb_cvec_eap\"] - data[\"svd_word_0\"]) - data[\"svd_word_0\"]) - data[\"nb_cvec_char_hpl\"]) * 2.0))\n    v[\"9\"] = np.tanh((((data[\"nb_tfidf_char_eap\"] - (data[\"svd_word_12\"] + data[\"num_unique_words\"])) - data[\"nb_cvec_hpl\"]) * 2.0))\n    v[\"10\"] = np.tanh((((((data[\"nb_cvec_eap\"] - data[\"num_words\"]) - data[\"svd_word_0\"]) * 2.0) - data[\"nb_tfidf_char_hpl\"]) * 2.0))\n    v[\"11\"] = np.tanh((((((-(data[\"nb_tfidf_char_hpl\"])) - data[\"svd_word_0\"]) - data[\"num_words\"]) * 2.0) - data[\"num_words_title\"]))\n    v[\"12\"] = np.tanh(((((data[\"nb_cvec_eap\"] - data[\"nb_cvec_char_hpl\"]) - data[\"num_punctuations\"]) - data[\"svd_word_12\"]) * 2.0))\n    v[\"13\"] = np.tanh(((((((data[\"svd_word_1\"] * 2.0) * 2.0) - data[\"num_chars\"]) + data[\"nb_tfidf_char_eap\"]) * 2.0) * 2.0))\n    v[\"14\"] = np.tanh((((data[\"svd_word_17\"] - data[\"nb_cvec_char_hpl\"]) + data[\"nb_cvec_char_eap\"]) * 2.0))\n    v[\"15\"] = np.tanh((((-(((data[\"svd_word_0\"] + data[\"svd_word_12\"]) + data[\"svd_word_8\"]))) - data[\"nb_cvec_char_hpl\"]) * 2.0))\n    v[\"16\"] = np.tanh((((((data[\"nb_tfidf_char_eap\"] - data[\"num_unique_words\"]) - data[\"svd_word_12\"]) - data[\"svd_word_2\"]) * 2.0) * 2.0))\n    v[\"17\"] = np.tanh((((data[\"svd_word_17\"] - data[\"svd_word_12\"]) - (data[\"nb_tfidf_char_hpl\"] + data[\"svd_word_0\"])) - data[\"num_unique_words\"]))\n    v[\"18\"] = np.tanh(((((data[\"svd_word_10\"] + data[\"nb_cvec_char_eap\"]) - data[\"svd_word_8\"]) - data[\"num_punctuations\"]) - data[\"svd_word_2\"]))\n    v[\"19\"] = np.tanh((data[\"svd_word_1\"] - (data[\"svd_word_0\"] + (data[\"num_words_upper\"] + (data[\"svd_word_12\"] + data[\"nb_tfidf_char_hpl\"])))))\n    v[\"20\"] = np.tanh(((((data[\"nb_cvec_char_eap\"] - data[\"svd_word_15\"]) - data[\"svd_word_8\"]) - data[\"svd_word_18\"]) * 2.0))\n    v[\"21\"] = np.tanh(((((((data[\"svd_word_1\"] * 2.0) * 2.0) - data[\"num_words\"]) - data[\"num_punctuations\"]) * 2.0) * 2.0))\n    v[\"22\"] = np.tanh((data[\"nb_cvec_char_eap\"] + (data[\"nb_cvec_eap\"] * ((data[\"num_unique_words\"] * -3.0) - data[\"nb_cvec_char_eap\"]))))\n    v[\"23\"] = np.tanh(((data[\"svd_word_17\"] - (data[\"svd_word_0\"] + data[\"nb_tfidf_char_hpl\"])) - (data[\"svd_word_3\"] + data[\"svd_word_8\"])))\n    v[\"24\"] = np.tanh((((-((data[\"svd_word_18\"] + (data[\"nb_cvec_char_eap\"] * data[\"num_unique_words\"])))) * 2.0) - data[\"svd_word_8\"]))\n    v[\"25\"] = np.tanh(((((-1.0 - (data[\"num_words_title\"] + data[\"num_words\"])) - data[\"svd_word_0\"]) * 2.0) * 2.0))\n    v[\"26\"] = np.tanh((((data[\"svd_word_17\"] - ((data[\"svd_word_3\"] + data[\"svd_word_12\"]) + data[\"num_words_upper\"])) * 2.0) * 2.0))\n    v[\"27\"] = np.tanh((-((((data[\"svd_word_7\"] * 2.0) * data[\"svd_word_7\"]) + (data[\"svd_word_4\"] + data[\"svd_word_8\"])))))\n    v[\"28\"] = np.tanh((((data[\"svd_word_0\"] * data[\"num_unique_words\"]) - (data[\"nb_cvec_mws\"] * data[\"nb_tfidf_char_mws\"])) * 2.0))\n    v[\"29\"] = np.tanh(((0.78320163488388062) - (data[\"svd_word_5\"] + (data[\"svd_word_8\"] + (data[\"svd_word_7\"] * data[\"svd_word_7\"])))))\n    v[\"30\"] = np.tanh((-1.0 + (data[\"svd_word_0\"] * (((data[\"nb_cvec_char_eap\"] * 2.0) * 2.0) * data[\"num_unique_words\"]))))\n    v[\"31\"] = np.tanh(((data[\"nb_cvec_char_eap\"] * ((data[\"svd_word_17\"] - data[\"num_words_upper\"]) - data[\"num_stopwords\"])) - data[\"svd_word_8\"]))\n    v[\"32\"] = np.tanh((((data[\"num_stopwords\"] * data[\"svd_word_0\"]) - (data[\"svd_word_7\"] * data[\"svd_word_7\"])) - data[\"svd_word_5\"]))\n    v[\"33\"] = np.tanh((((data[\"svd_word_11\"] * (data[\"svd_word_6\"] - data[\"svd_word_11\"])) - data[\"svd_word_9\"]) - data[\"svd_word_6\"]))\n    v[\"34\"] = np.tanh(((data[\"svd_word_8\"] * (data[\"nb_tfidf_char_eap\"] - data[\"svd_word_8\"])) + (data[\"num_words_upper\"] * data[\"num_unique_words\"])))\n    v[\"35\"] = np.tanh(((((data[\"svd_word_6\"] * data[\"svd_word_19\"]) + data[\"svd_word_7\"])/2.0) - (data[\"svd_word_6\"] * data[\"svd_word_6\"])))\n    v[\"36\"] = np.tanh(((((data[\"svd_word_1\"] * data[\"nb_cvec_char_eap\"]) * 2.0) - ((data[\"svd_word_6\"] + data[\"svd_word_9\"])/2.0)) * 2.0))\n    v[\"37\"] = np.tanh((((data[\"svd_word_12\"] * (data[\"svd_word_13\"] - data[\"svd_word_12\"])) - data[\"nb_cvec_char_mws\"]) - data[\"svd_word_12\"]))\n    v[\"38\"] = np.tanh(((data[\"nb_tfidf_char_eap\"] * data[\"nb_cvec_hpl\"]) - (data[\"svd_word_9\"] + (data[\"svd_word_8\"] - data[\"nb_cvec_hpl\"]))))\n    v[\"39\"] = np.tanh((data[\"svd_word_14\"] - (data[\"svd_word_4\"] + ((data[\"svd_word_11\"] * data[\"svd_word_11\"]) + data[\"svd_word_6\"]))))\n    v[\"40\"] = np.tanh(((data[\"svd_word_12\"] * data[\"svd_word_4\"]) - (data[\"svd_word_9\"] + (data[\"num_words_upper\"] * data[\"nb_cvec_char_eap\"]))))\n    v[\"41\"] = np.tanh(((data[\"svd_word_1\"] * data[\"svd_word_8\"]) - ((data[\"svd_word_2\"] + (data[\"svd_word_6\"] + data[\"svd_word_19\"]))/2.0)))\n    v[\"42\"] = np.tanh((data[\"svd_word_7\"] - ((data[\"svd_word_6\"] + (data[\"svd_word_7\"] * 2.0)) * (data[\"svd_word_7\"] * 2.0))))\n    v[\"43\"] = np.tanh(((((data[\"num_chars\"] + data[\"svd_word_11\"])/2.0) * (data[\"num_chars\"] - data[\"svd_word_11\"])) - data[\"svd_word_6\"]))\n    v[\"44\"] = np.tanh((((data[\"svd_word_11\"] - (data[\"num_unique_words\"] + data[\"num_stopwords\"])) - data[\"svd_word_9\"]) - data[\"nb_cvec_eap\"]))\n    v[\"45\"] = np.tanh(((data[\"nb_cvec_char_hpl\"] * (data[\"svd_word_11\"] - (data[\"svd_word_13\"] - data[\"nb_cvec_mws\"]))) - data[\"svd_word_9\"]))\n    v[\"46\"] = np.tanh((((data[\"num_unique_words\"] * data[\"nb_cvec_char_eap\"]) * (data[\"num_unique_words\"] - data[\"mean_word_len\"])) - data[\"nb_cvec_eap\"]))\n    v[\"47\"] = np.tanh(((((data[\"num_words\"] - (data[\"svd_word_0\"] * data[\"mean_word_len\"])) + data[\"svd_word_11\"])/2.0) - data[\"svd_word_9\"]))\n    v[\"48\"] = np.tanh((((data[\"svd_word_0\"] - (data[\"svd_word_0\"] * data[\"nb_cvec_char_hpl\"])) - data[\"svd_word_17\"]) * data[\"num_words\"]))\n    v[\"49\"] = np.tanh(((data[\"svd_word_18\"] * data[\"svd_word_11\"]) - (data[\"svd_word_15\"] + (data[\"nb_cvec_char_mws\"] * data[\"nb_cvec_mws\"]))))\n\n    return v.sum(axis=1)\n\n\ndef GPClusterY(data):\n    v = pd.DataFrame()\n    v[\"0\"] = np.tanh((((data[\"svd_word_12\"] + (data[\"nb_cvec_char_mws\"] * 2.0)) * 2.0) + (data[\"svd_word_7\"] + data[\"svd_word_11\"])))\n    v[\"1\"] = np.tanh((((data[\"svd_word_8\"] + ((data[\"nb_cvec_char_mws\"] + data[\"svd_word_2\"]) + data[\"nb_cvec_mws\"])) * 2.0) * 2.0))\n    v[\"2\"] = np.tanh((((data[\"nb_cvec_char_mws\"] - data[\"nb_cvec_char_hpl\"]) + (data[\"svd_word_8\"] + data[\"nb_cvec_mws\"])) * 2.0))\n    v[\"3\"] = np.tanh((((data[\"svd_word_2\"] + ((data[\"nb_cvec_char_mws\"] + data[\"nb_cvec_mws\"]) - data[\"nb_cvec_char_hpl\"])) * 2.0) * 2.0))\n    v[\"4\"] = np.tanh(((((data[\"svd_word_2\"] - (data[\"nb_cvec_char_hpl\"] + data[\"nb_cvec_eap\"])) * 2.0) + data[\"svd_word_12\"]) * 2.0))\n    v[\"5\"] = np.tanh(((((data[\"svd_word_3\"] + (data[\"svd_word_8\"] + data[\"nb_cvec_char_mws\"])) + data[\"nb_cvec_char_mws\"]) * 2.0) * 2.0))\n    v[\"6\"] = np.tanh(((((data[\"svd_word_2\"] + (data[\"nb_cvec_mws\"] - data[\"nb_cvec_char_hpl\"])) * 2.0) + data[\"svd_word_12\"]) * 2.0))\n    v[\"7\"] = np.tanh(((((data[\"num_words_upper\"] + data[\"nb_cvec_char_mws\"]) + (data[\"svd_word_12\"] - data[\"nb_cvec_char_hpl\"])) * 2.0) * 2.0))\n    v[\"8\"] = np.tanh((((data[\"num_words_upper\"] + ((-(data[\"nb_cvec_eap\"])) - (data[\"nb_cvec_char_hpl\"] * 2.0))) * 2.0) * 2.0))\n    v[\"9\"] = np.tanh((((data[\"nb_cvec_char_mws\"] + (data[\"svd_word_7\"] + (data[\"svd_word_12\"] + data[\"svd_word_2\"]))) * 2.0) * 2.0))\n    v[\"10\"] = np.tanh(((((data[\"num_words_upper\"] - (data[\"nb_cvec_eap\"] + (data[\"nb_cvec_char_hpl\"] * 2.0))) * 2.0) * 2.0) * 2.0))\n    v[\"11\"] = np.tanh((((data[\"nb_cvec_char_mws\"] + (data[\"svd_word_7\"] + data[\"svd_word_8\"])) * 2.0) * 2.0))\n    v[\"12\"] = np.tanh((((data[\"nb_tfidf_char_mws\"] - data[\"nb_cvec_char_hpl\"]) * 2.0) - (data[\"mean_word_len\"] + data[\"nb_cvec_char_hpl\"])))\n    v[\"13\"] = np.tanh(((((((data[\"nb_tfidf_char_mws\"] - data[\"nb_cvec_char_hpl\"]) - data[\"svd_word_4\"]) * 2.0) * 2.0) * 2.0) * 2.0))\n    v[\"14\"] = np.tanh(((((data[\"svd_word_12\"] - (data[\"svd_word_4\"] + data[\"nb_cvec_char_hpl\"])) * 2.0) - data[\"nb_cvec_eap\"]) * 2.0))\n    v[\"15\"] = np.tanh(((1.0 + ((data[\"svd_word_0\"] + (data[\"nb_cvec_char_mws\"] + data[\"svd_word_12\"])) * 2.0)) * 2.0))\n    v[\"16\"] = np.tanh(((data[\"svd_word_2\"] * (data[\"nb_cvec_char_hpl\"] * (9.78357696533203125))) - ((data[\"nb_cvec_char_hpl\"] * 2.0) * 2.0)))\n    v[\"17\"] = np.tanh((((((data[\"svd_word_3\"] * 2.0) - data[\"nb_cvec_hpl\"]) - data[\"nb_cvec_eap\"]) * 2.0) - data[\"nb_tfidf_char_hpl\"]))\n    v[\"18\"] = np.tanh((((data[\"nb_cvec_char_mws\"] + ((data[\"svd_word_8\"] + data[\"svd_word_15\"]) + data[\"svd_word_7\"])) * 2.0) * 2.0))\n    v[\"19\"] = np.tanh(((((data[\"svd_word_4\"] * (data[\"nb_cvec_char_hpl\"] * 2.0)) - data[\"nb_cvec_char_hpl\"]) * 2.0) * 2.0))\n    v[\"20\"] = np.tanh((((data[\"svd_word_0\"] - ((data[\"svd_word_0\"] * 2.0) * data[\"nb_cvec_char_mws\"])) - data[\"svd_word_17\"]) * 2.0))\n    v[\"21\"] = np.tanh(((((data[\"nb_cvec_char_hpl\"] * 2.0) * (data[\"svd_word_0\"] * 2.0)) + 1.0) * 2.0))\n    v[\"22\"] = np.tanh((((data[\"nb_tfidf_char_mws\"] + (data[\"svd_word_15\"] - data[\"nb_cvec_char_hpl\"])) * 2.0) + data[\"svd_word_18\"]))\n    v[\"23\"] = np.tanh((((((data[\"nb_cvec_char_mws\"] + data[\"svd_word_6\"]) - data[\"svd_word_17\"]) + data[\"svd_word_11\"]) * 2.0) * 2.0))\n    v[\"24\"] = np.tanh((((data[\"nb_cvec_char_hpl\"] * 2.0) * ((data[\"nb_cvec_char_hpl\"] * data[\"svd_word_0\"]) * 2.0)) - -3.0))\n    v[\"25\"] = np.tanh(((((data[\"num_chars\"] * data[\"nb_cvec_char_hpl\"]) - (data[\"svd_word_0\"] * data[\"nb_cvec_char_mws\"])) * 2.0) * 2.0))\n    v[\"26\"] = np.tanh(((data[\"svd_word_14\"] + data[\"num_words_upper\"]) + (data[\"nb_tfidf_char_eap\"] * (data[\"svd_word_17\"] - data[\"nb_cvec_char_mws\"]))))\n    v[\"27\"] = np.tanh(((data[\"svd_word_0\"] * data[\"nb_cvec_char_hpl\"]) + (data[\"nb_tfidf_char_mws\"] + (data[\"svd_word_12\"] * data[\"nb_cvec_char_hpl\"]))))\n    v[\"28\"] = np.tanh(((((data[\"svd_word_12\"] - data[\"svd_word_7\"]) + data[\"svd_word_4\"]) + data[\"num_unique_words\"]) * data[\"nb_cvec_char_hpl\"]))\n    v[\"29\"] = np.tanh((data[\"num_words\"] + (data[\"nb_tfidf_char_mws\"] * (data[\"svd_word_7\"] - (data[\"svd_word_0\"] * 2.0)))))\n    v[\"30\"] = np.tanh((((data[\"nb_tfidf_char_mws\"] * data[\"nb_tfidf_char_mws\"]) + (data[\"num_stopwords\"] + data[\"nb_cvec_char_eap\"])) + data[\"svd_word_6\"]))\n    v[\"31\"] = np.tanh(((data[\"nb_tfidf_char_mws\"] + data[\"svd_word_18\"]) + ((data[\"svd_word_8\"] + data[\"svd_word_0\"]) - data[\"svd_word_17\"])))\n    v[\"32\"] = np.tanh((((data[\"nb_cvec_hpl\"] * 2.0) * 2.0) * ((data[\"svd_word_5\"] + data[\"num_chars\"]) - data[\"svd_word_7\"])))\n    v[\"33\"] = np.tanh((((data[\"mean_word_len\"] + data[\"svd_word_15\"]) + (data[\"nb_cvec_char_hpl\"] + data[\"nb_tfidf_char_mws\"])) * data[\"nb_cvec_char_mws\"]))\n    v[\"34\"] = np.tanh((((((data[\"num_unique_words\"] * data[\"nb_cvec_char_mws\"]) * data[\"num_words_upper\"]) * 2.0) * 2.0) + data[\"nb_cvec_hpl\"]))\n    v[\"35\"] = np.tanh(((data[\"nb_cvec_char_hpl\"] * (-((data[\"svd_word_7\"] - (data[\"num_punctuations\"] - data[\"svd_word_16\"]))))) * 2.0))\n    v[\"36\"] = np.tanh(((data[\"nb_cvec_char_eap\"] * data[\"svd_word_17\"]) + ((data[\"svd_word_8\"] - data[\"svd_word_4\"]) - data[\"nb_cvec_char_hpl\"])))\n    v[\"37\"] = np.tanh(((np.tanh(data[\"nb_cvec_hpl\"]) - (data[\"nb_cvec_char_mws\"] * (data[\"svd_word_0\"] + data[\"num_unique_words\"]))) * 2.0))\n    v[\"38\"] = np.tanh((((-(data[\"svd_word_16\"])) * data[\"nb_cvec_eap\"]) + ((-(data[\"mean_word_len\"])) * data[\"nb_cvec_eap\"])))\n    v[\"39\"] = np.tanh((((data[\"nb_cvec_char_hpl\"] * data[\"svd_word_5\"]) + ((data[\"nb_cvec_char_hpl\"] * data[\"svd_word_8\"]) - data[\"nb_cvec_char_hpl\"]))/2.0))\n    v[\"40\"] = np.tanh(((((data[\"num_words\"] * data[\"svd_word_0\"]) * data[\"nb_cvec_mws\"]) + ((data[\"svd_word_7\"] + data[\"nb_cvec_hpl\"])/2.0))/2.0))\n    v[\"41\"] = np.tanh((((((data[\"svd_word_17\"] / 2.0) * data[\"nb_cvec_char_eap\"]) + (data[\"svd_word_1\"] * data[\"nb_cvec_char_eap\"]))/2.0) * 2.0))\n    v[\"42\"] = np.tanh((((data[\"mean_word_len\"] * data[\"num_words_upper\"]) - data[\"nb_cvec_mws\"]) - (data[\"svd_word_18\"] * data[\"nb_cvec_char_mws\"])))\n    v[\"43\"] = np.tanh((data[\"nb_cvec_char_hpl\"] * ((data[\"svd_word_5\"] + (data[\"svd_word_8\"] - data[\"nb_cvec_char_hpl\"])) - data[\"svd_word_7\"])))\n    v[\"44\"] = np.tanh((((data[\"svd_word_16\"] + np.tanh((data[\"nb_tfidf_char_mws\"] / 2.0)))/2.0) * data[\"nb_tfidf_char_mws\"]))\n    v[\"45\"] = np.tanh((data[\"nb_tfidf_char_mws\"] * (data[\"nb_cvec_hpl\"] * data[\"svd_word_0\"])))\n    v[\"46\"] = np.tanh((data[\"mean_word_len\"] * (data[\"nb_cvec_char_eap\"] * data[\"svd_word_0\"])))\n    v[\"47\"] = np.tanh((((data[\"mean_word_len\"] * data[\"svd_word_0\"]) * (-(data[\"nb_tfidf_char_mws\"]))) - data[\"nb_tfidf_char_mws\"]))\n    v[\"48\"] = np.tanh(((data[\"svd_word_14\"] + (data[\"nb_cvec_char_hpl\"] - data[\"num_words_upper\"])) * (data[\"svd_word_12\"] + data[\"svd_word_12\"])))\n    v[\"49\"] = np.tanh(((data[\"nb_cvec_char_mws\"] * data[\"num_unique_words\"]) * (((data[\"nb_cvec_mws\"] * data[\"num_words\"]) + data[\"num_words\"])/2.0)))\n    return v.sum(axis=1)","cell_type":"code","metadata":{"collapsed":true}},{"outputs":[],"execution_count":null,"source":"colors = ['red', 'green','blue']\nplt.figure(figsize=(15,15))\nplt.scatter(GPClusterX(train_X),GPClusterY(train_X),s=10, color=[colors[o]for o in train_y])","cell_type":"code","metadata":{}},{"outputs":[],"execution_count":null,"source":"","cell_type":"code","metadata":{"collapsed":true}}],"nbformat":4,"metadata":{"kernelspec":{"name":"python3","language":"python","display_name":"Python 3"},"language_info":{"name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","file_extension":".py","mimetype":"text/x-python","version":"3.6.3","codemirror_mode":{"name":"ipython","version":3}}},"nbformat_minor":1}