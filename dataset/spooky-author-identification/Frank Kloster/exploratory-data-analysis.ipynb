{"cells":[{"metadata":{"_uuid":"075ab0f3fc310e293828b3681f1d80642f88c106","_cell_guid":"a3cb0ee3-7bca-4b2b-8a27-be198d18818e"},"cell_type":"markdown","source":"# Introduction\n\n![](https://www.telegraph.co.uk/content/dam/films/2016/10/28/cthulhu_trans_NvBQzQNjv4BqeWq0Odl7YRxHNYM74_QBWlbFJiGQSGUwQFXFdwSXZiw.jpg?imwidth=1400)\n\nHere, I'll be looking at Kaggle's Spooky Author dataset. This is largely of interest to me for two reasons. First, as an excuse to learn some basic techniques in NLP. Many of these techniques I've learned over these past few months go unused, such as latent Dirichlet allocations and recurrent neural networks. Second, as someone interested in the horror literature as a whole, figured this dataset would be a good place to start.\n\nI will be using several different libraries.\n* **Numpy**, **Pandas**, and **Sklearn** for data analysis and machine learning.\n* **Plotly** for plotting data. I will also be using **Word Cloud** for generating wordclouds.\n---\n\nTo begin, let me start with a following quote.\n\n> \"The most merciful thing in the world, I think, is the inability of the human mind to correlate all its contents.\"\n> > HP Lovecraft\n\nWell, I guess he never heard of deep learning techniques! Oh well."},{"metadata":{"_uuid":"97ee6de5cf262e0dcff70fb9bc74466b1ffe1bd8","_cell_guid":"624cc24b-893e-470d-83b1-2a3f585f6394"},"cell_type":"markdown","source":"# Loading the Data"},{"metadata":{"_uuid":"e218baffbaf0680606b4c16c8567c735527f13e4","_cell_guid":"deba45ed-39c9-4e33-8a50-54d97f5d31dc","trusted":true},"cell_type":"code","source":"import base64\nimport numpy as np\nimport pandas as pd\nfrom collections import Counter\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d67debbca889e81a65d3f23aecaf268b2c0b8d69","_cell_guid":"6e3053b9-1f53-4aad-9b46-38780eaf7417","trusted":true},"cell_type":"code","source":"# Loading in the training data with Pandas\ndf = pd.read_csv(\"../input/train.csv\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"eb95fa1ac69bed793fede225b746844925f59407","_cell_guid":"1fa5935e-ee07-4b51-a1c0-ec010fed4d85"},"cell_type":"markdown","source":"First, let us take a look at he data itself."},{"metadata":{"_kg_hide-input":true,"_uuid":"b0c55dbcfcd55a3bfd7f5521161a2fb21f56eaa6","_cell_guid":"3bbe47be-677c-4925-8f4b-fcae4dfa792c","trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b440e9f59cd6c1afa59b19fa68a0bf1d2cdab80d","_cell_guid":"09b40772-6ce1-471b-8c1d-ce7bddfd10e3"},"cell_type":"markdown","source":"So we observe that each entry has three attributes, namely\n* an identification number,\n* the text itself,\n* and an abbreviation of the author.\nThe id entry we can safely discount for the rest of the analysis. The rest will however be useful.\n\nJust to get an idea of what each text entry looks like."},{"metadata":{"_uuid":"dde020a5f7164ab9c8bfd9447c731f899ced7a90","_cell_guid":"35d0cb77-8d34-4049-a9c5-09bc8dbe88d3","trusted":true},"cell_type":"code","source":"# Reading the full text of the first entry.\ndf['text'][0]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f3707a1c99a408bd51efed94e11cc1c508b598f5"},"cell_type":"markdown","source":"This is written by none other than Edgar Allan Poe, as indicated by the dataframe."},{"metadata":{"_uuid":"6f5243ee6a7ced54516884e76d2672a7adafa6fe","_cell_guid":"44225931-3d30-48a4-82c9-5e100dfd3f32"},"cell_type":"markdown","source":"This consists of one fairly long sentence. So this is exactly what we will be looking at. Next, let us check how many entries we have to deal with."},{"metadata":{"_uuid":"4b952afe083bc2f64ce7d5f1a912f428f06dc59a","_cell_guid":"fa664a99-7259-4de0-bf35-e892b55f7bc0","trusted":true},"cell_type":"code","source":"print(df.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"296cf5b19cc6b6fdb64804b9b91a205a7acf78c5","_cell_guid":"2f3ed627-ae08-4a0d-b194-1504f005296c"},"cell_type":"markdown","source":"Hence our training data consists of 19579 entries, each with three tuples attached to them (the latter we already knew)."},{"metadata":{"_uuid":"aa3c93c40e62d21d3762651b35c25edcb5d3d746","_cell_guid":"178ee8ea-a97e-49bf-be63-38396ee454b2"},"cell_type":"markdown","source":"Note that the authors are labelled by their initials. Each author has their own distinct style, which is summerized below.\n\n1. **[EAP - Edgar Allen Poe](https://en.wikipedia.org/wiki/Edgar_Allan_Poe)** : American writer who wrote poetry and short stories that revolved around tales of mystery and the grisly and the grim. Arguably the origin of \"detective fiction,\" especially with his work in \"The Masque of the Red Death.\"\n\n2. **[HPL - HP Lovecraft](https://en.wikipedia.org/wiki/H._P._Lovecraft)** : Best known for his \"Cthulu mythos.\" His writing style focuses heavily on fear of the unknown.\n\n3. **[MWS - Mary Shelley](https://en.wikipedia.org/wiki/Mary_Shelley)** : Probably the most diverse of the three - she was a novelist, dramatist, travel-writer, and biographer. Her most well known work is \"Frankenstein.\""},{"metadata":{"_uuid":"e90aef77086b604ed50757bb54a74e37d92a2a1e","_cell_guid":"cf0a6e25-b953-40df-be64-f0316e420954"},"cell_type":"markdown","source":"# Exploratory Data Analysis\n\n## Summary statistics of the training set\n\nHere we can visualize some basic statistics in the data, like the distribution of entries for each author. For this purpose, I will invoke the handy Plot.ly visualisation library and plot some simple bar plots. Plot.ly is probably one of the better looking graphics libraries out there. It does come at a cost of being a little hard to code compared to libraries such as *Seaborn*."},{"metadata":{"_uuid":"a56a2e1158be7e99a20c57692c5db6af1ee66bfa","trusted":true},"cell_type":"code","source":"import plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.tools as tls","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":false,"_uuid":"69c96d17112d5b7dd8b748955a818eaf6e13e172","_cell_guid":"5cefd7ac-9a38-438a-9c78-5ff5c4610af1","trusted":true},"cell_type":"code","source":"z = {'EAP': 'Edgar Allen Poe', 'MWS': 'Mary Shelley', 'HPL': 'HP Lovecraft'}\ndata = [go.Bar(\n            x = df.author.map(z).unique(),\n            y = df.author.value_counts().values,\n            marker= dict(colorscale='Jet',\n                         color = df.author.value_counts().values\n                        ),\n            text='Text entries attributed to Author'\n    )]\n\nlayout = go.Layout(\n    title='Target variable distribution'\n)\n\nfig = go.Figure(data=data, layout=layout)\n\npy.iplot(fig, filename='basic-bar')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"58dd795e28474ff99c36b3196ee1767dcf206eed","_cell_guid":"311422ae-29c8-4694-9041-aa5c1601b8a0"},"cell_type":"markdown","source":"This gives us a pretty good idea now of the amount we have to work with in terms of each author."},{"metadata":{"_uuid":"fe3ad42b8d423aafe9190490f180db59c0669e8a","_cell_guid":"bbfe7efe-74a4-45ad-bfba-d926ed9819d4"},"cell_type":"markdown","source":"## WordClouds\n\nOne very handy visualization tool for a data scientist when it comes to any sort of natural language processing is plotting \"Word Cloud\". A word cloud (as the name suggests) is an image that is made up of a mixture of distinct words which may make up a text or book and where the size of each word is proportional to its word frequency in that text (number of times the word appears). Here instead of dealing with an actual book or text, our words can simply be taken from the column \"text\"\n\n**Store the text of each author in  a Python list**\n\nWe first create three different python lists that store the texts of Edgar Allen Poe, HP Lovecraft and Mary Shelley respectively as follows:"},{"metadata":{"_uuid":"ed26dd3c5f3e1ffcd28dd16bfee53ba5501aaa4f","trusted":true},"cell_type":"code","source":"from wordcloud import WordCloud, STOPWORDS\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"810aad4dd6dd8118bd3c8ef8233c67bc1c9aa641"},"cell_type":"code","source":"def generate_word_cloud(text, title):\n    # Generate word cloud.\n    wc = WordCloud(background_color='black', max_words=1000,\n                  stopwords=STOPWORDS, max_font_size=40)\n    wc.generate(\" \".join(text))\n    \n    # Plot word cloud using matplotlib.\n    plt.figure(figsize=(16, 13))\n    plt.title(title, fontsize=20)\n    plt.imshow(wc.recolor(colormap='Pastel2', random_state=42), alpha=0.98)\n    plt.axis('off')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e8a631a23de198d48d472b10111448972f11aee5","_cell_guid":"78d367b4-7efe-43b2-9ec2-b1a602cbf132","trusted":true},"cell_type":"code","source":"eap = df[df.author==\"EAP\"][\"text\"].values\nhpl = df[df.author==\"HPL\"][\"text\"].values\nmws = df[df.author==\"MWS\"][\"text\"].values","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"65a507d0be3fa40703577dd456d8c2a8582772ba"},"cell_type":"markdown","source":"## HP Lovecraft"},{"metadata":{"trusted":true,"_uuid":"61516be2ca1b30bec1ebe7e08255642cf0c41f7a"},"cell_type":"code","source":"generate_word_cloud(hpl, \"HP Lovecraft\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"176073b7cfde7998a51ba71c2548769573c056a4"},"cell_type":"markdown","source":"As one can see, we can see certain Lovecraftian themes present. Good examples would be \"dark,\" \"thought,\" and \"strange.\" These themes are buried in the middle of several common words, such as \"place\" and \"seemed.\" Despite trying to remove certain stopwords, it can be kind of inevitble that certain common words will dominate."},{"metadata":{"_uuid":"f8e66ab84ed6a6444bdcdbf53b6a6fbec0f1b6ce"},"cell_type":"markdown","source":"## Edgar Allen Poe"},{"metadata":{"trusted":true,"_uuid":"7532abc06a09e63e4eee1e39a8cb31ae4692328a"},"cell_type":"code","source":"generate_word_cloud(eap, \"Edgar Allen Poe\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c7c74529230baa4c913a48cd02eef3ddcc7e5433"},"cell_type":"markdown","source":"## Mary Shelley"},{"metadata":{"trusted":true,"_uuid":"fcd76eabaf80c987190c0061c9ac16e45594bd0c"},"cell_type":"code","source":"generate_word_cloud(mws, \"Mary Shelley\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a026400479d03567e4c6455b7d90215d2ff2f8cf","_cell_guid":"d67381d1-d765-449e-a9b9-6e3b127580aa"},"cell_type":"markdown","source":"On the other hand, one can see that Mary Shelley's words revolve around primal instincts and themes of morality which range from the positive to negative ends of the spectrum, such as \"friend\", \"fear\", \"hope\", \"spirit\" etc. One common word that stands out, is \"Raymond.\" I had to dig through a little to figure out why such a word appeared, to be honest. If you look at the lesser-known work by Mary Shelley, you will find [The Last Man](https://en.wikipedia.org/wiki/The_Last_Man), one character from which is Lord Raymond."},{"metadata":{"_uuid":"c50a9903005de438405ee583ddee2882c5756e68","_cell_guid":"bdde4aac-a975-4429-ae9d-c93ec0692f09"},"cell_type":"markdown","source":"**Term frequencies**\n\nAn alternative strategy is simply to use a histogram in order to plot term frequencies."},{"metadata":{"_kg_hide-input":true,"_uuid":"2ca4b9012ee62e9a87623ea0737f1ec556fcfa91","_cell_guid":"d7c8263b-1161-4648-82d3-a72b012712d9","trusted":true},"cell_type":"code","source":"def plot_frequent_word(count_vec, author, feature_names):\n    zipped = list(zip(feature_names, count_vec))\n    x, y = (list(x) for x in zip(*sorted(zipped, key=lambda x: x[1], reverse=True)))\n    X = np.concatenate([x[0:15], x[-16:-1]])\n    Y = np.concatenate([y[0:15], y[-16:-1]])\n    # Plotting the Plot.ly plot for the Top 50 word frequencies\n    data = [go.Bar(\n                x = x[0:50],\n                y = y[0:50],\n                marker= dict(colorscale='Jet',\n                             color = y[0:50]\n                            ),\n                text='Word counts'\n        )]\n    layout = go.Layout(\n    title='Top 50 Word frequencies (%s)' % (author)\n    )\n\n    fig = go.Figure(data=data, layout=layout)\n\n    py.iplot(fig, filename='basic-bar')\n\ntf_vectorizer_hpl = CountVectorizer(max_df=0.95, \n                                     min_df=2,\n                                     stop_words='english',\n                                     decode_error='ignore')\n\ntf_vectorizer_eap = CountVectorizer(max_df=0.95, \n                                     min_df=2,\n                                     stop_words='english',\n                                     decode_error='ignore')\n\ntf_vectorizer_mws = CountVectorizer(max_df=0.95, \n                                     min_df=2,\n                                     stop_words='english',\n                                     decode_error='ignore')\n\ntf_hpl = tf_vectorizer_hpl.fit_transform(hpl)\ntf_eap = tf_vectorizer_eap.fit_transform(eap)\ntf_mws = tf_vectorizer_mws.fit_transform(mws)\n    \nfeature_names_hpl = tf_vectorizer_hpl.get_feature_names()\nfeature_names_eap = tf_vectorizer_eap.get_feature_names()\nfeature_names_mws = tf_vectorizer_mws.get_feature_names()\n\n\ncount_vec_hpl = np.asarray(tf_hpl.sum(axis=0)).ravel()\ncount_vec_eap = np.asarray(tf_eap.sum(axis=0)).ravel()\ncount_vec_mws = np.asarray(tf_mws.sum(axis=0)).ravel()\n\nplot_frequent_word(count_vec_hpl, 'HP Lovecraft', feature_names_hpl)\nplot_frequent_word(count_vec_eap, 'Edgar Allan Poe', feature_names_eap)\nplot_frequent_word(count_vec_mws, 'Mary Shelly', feature_names_mws)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Bigram Analysis\n\nWe turn our attention to now studyign bigrams. Specifically, we shall be looking for more 'surpising' bigrams. Specifically, we shall be scoring each bigram by\n\n$$\nS(w_1 w_2) = \\frac{N(w_1 w_2)}{[N(w_1) + c][N(w_2) + c]},\n$$\n\nwhere $N(w_1 w_2)$ counts the number of occurances of the bigram $w_1 w_2$, $N(w)$ counts the number of occurances of the word $w$, and $c$ is what's called a Bayesian smoothing parameter."},{"metadata":{"trusted":true},"cell_type":"code","source":"eap","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def surprising_bigrams(text_vec, bayesian_smoothing):\n    # Counts all the bigrams.\n    bigram_vectorizer = CountVectorizer(ngram_range=(2, 2), stop_words='english')\n    X = bigram_vectorizer.fit_transform(text_vec)\n    sum_bigrams = X.sum(axis=0)\n    bigram_freq = [(bigram, sum_bigrams[0, idx]) for bigram, idx in bigram_vectorizer.vocabulary_.items()]\n    bigram_freq = sorted(bigram_freq, key = lambda x: x[1], reverse=True)\n    \n    # Counts all the words\n    word_vectorizer = CountVectorizer(ngram_range=(1, 1), stop_words='english')\n    X = word_vectorizer.fit_transform(text_vec)\n    sum_words = X.sum(axis=0)\n    words_freq = {word: sum_words[0, idx] for word, idx in word_vectorizer.vocabulary_.items()}\n    \n    words_freq_list = [(word, sum_words[0, idx]) for word, idx in word_vectorizer.vocabulary_.items()]\n\n    words_freq_list = sorted(words_freq_list, key=lambda x: x[1], reverse=True)\n    \n    bayesian_smoothing_parameter = 30\n\n    bigram_freq_normalized = []\n    for bigram in bigram_freq:\n        words_temp = bigram[0].split()\n        word_freq_temp = [words_freq[word] for word in words_temp]\n        bigram_freq_normalized.append((bigram[0], bigram[1] / ((word_freq_temp[0] + bayesian_smoothing_parameter) * (word_freq_temp[1] + bayesian_smoothing_parameter))))\n\n    bigram_freq_normalized = sorted(bigram_freq_normalized, key=lambda x: x[1], reverse=True)\n    \n    return bigram_freq_normalized","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bigram_freq_eap = surprising_bigrams(eap, 60)\nbigram_freq_mws = surprising_bigrams(mws, 60)\nbigram_freq_hpl = surprising_bigrams(hpl, 60)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_bigram(bigram_freq, author, num):\n    bigram_freq = bigram_freq[:num]\n    x = [bigram[0] for bigram in bigram_freq]\n    y = [bigram[1] for bigram in bigram_freq]\n    # Plotting the Plot.ly plot for the Top 50 word frequencies\n    data = [go.Bar(\n                x = x,\n                y = y,\n                marker= dict(colorscale='Jet',\n                             color = y\n                            ),\n                text='Word counts'\n        )]\n    layout = go.Layout(\n    title='Top 50 Word frequencies (%s)' % (author)\n    )\n\n    fig = go.Figure(data=data, layout=layout)\n\n    py.iplot(fig, filename='basic-bar')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_bigram(bigram_freq_eap, 'Edgar Allan Poe', 50)\nplot_bigram(bigram_freq_mws, 'Mary Shelley', 50)\nplot_bigram(bigram_freq_hpl, 'HP Lovecraft', 50)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"739edef2e127c1db90b9ee774056ff4daea4271b","_cell_guid":"8d99cd60-8a17-4c14-81da-fb845d9912b2","trusted":false},"cell_type":"markdown","source":"# Latent Dirichlet Allocation"},{"metadata":{"_uuid":"f476c625f0ac6ad3b57b10adc34466f2d35900c1"},"cell_type":"markdown","source":"Latent Dirichlet Allocation, denoted by LDA from here on (not to be confused with [Linear Discriminant Analysis](https://en.wikipedia.org/wiki/Linear_discriminant_analysis)), is a very vital way of topic extraction in a corpus of documents. A fairly technical, although understandable, explanation can be found in the [original paper by Michael I. Jordan et al.](http://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf).\n\nWithout going into too much detail about the math behind the algorithm, there are two basic assumptions behind the algorithm.\n\n1. A corpus is a collection of topics,\n2. and a topic is a collection to keywords.\n\nLike other forms of unsupervised learning, we don't get names for our topics. It is usually easy to figure out based off the keywords provided. Also, we have to manually set the number of topics we expect. In this case, we choose 10 topics for each author.\n\nOnce we have a set of topics, we need a way to represent what each of those topics looks like. We use a tool called pyLDAvis to visually represent each of our topics."},{"metadata":{"trusted":true,"_uuid":"0e8d91d37bc8a1de8eaf3d14d65e40b54d03b210"},"cell_type":"code","source":"from sklearn.decomposition import NMF, LatentDirichletAllocation\nimport pyLDAvis\nimport pyLDAvis.sklearn\npyLDAvis.enable_notebook()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"598c930f137285a9c616047a5df1274cf7404911"},"cell_type":"markdown","source":"## Mary Shelley"},{"metadata":{"trusted":true,"_uuid":"b0d4d700391cacaccdfaf5af1f6cbc78b98fcf8c"},"cell_type":"code","source":"lda = LatentDirichletAllocation(n_components=10, max_iter=5,\n                                learning_method='online',\n                                learning_offset=50.,\n                                random_state=0)\n\nlda.fit(tf_mws)\n\ntf_feature_names = tf_vectorizer_mws.get_feature_names()\n\npyLDAvis.sklearn.prepare(lda, tf_mws, tf_vectorizer_mws)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"278664d126af6d69ec17ae17a555de3f9cb397cc"},"cell_type":"markdown","source":"While I am not intimantly familiar with Mary Shelley's work, there are a few things that pop out at me.\n\n* **Topic #4**: Lord Raymond and Andian are both characters in The Last Man."},{"metadata":{"_uuid":"88cdf564a88a37ca7707708f85f2ca80b274b200"},"cell_type":"markdown","source":"## HP Lovecraft"},{"metadata":{"trusted":true,"_uuid":"734e0575d91a3bf3ba59852b937a96470a17fee1"},"cell_type":"code","source":"lda = LatentDirichletAllocation(n_components=10, max_iter=5,\n                                learning_method='online',\n                                learning_offset=50.,\n                                random_state=0)\n\nlda.fit(tf_hpl)\n\ntf_feature_names = tf_vectorizer_hpl.get_feature_names()\n\npyLDAvis.sklearn.prepare(lda, tf_hpl, tf_vectorizer_hpl)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2e07bc85835151b7216c3e7fbbfc5f301fced788"},"cell_type":"markdown","source":"## Edgar Allan Poe"},{"metadata":{"trusted":true,"_uuid":"70e6e2c69bebad34ea827e6d4a677107c69aeba2"},"cell_type":"code","source":"lda = LatentDirichletAllocation(n_components=10, max_iter=5,\n                                learning_method='online',\n                                learning_offset=50.,\n                                random_state=0)\n\nlda.fit(tf_eap)\n\ntf_feature_names = tf_vectorizer_eap.get_feature_names()\n\npyLDAvis.sklearn.prepare(lda, tf_eap, tf_vectorizer_eap)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":1}