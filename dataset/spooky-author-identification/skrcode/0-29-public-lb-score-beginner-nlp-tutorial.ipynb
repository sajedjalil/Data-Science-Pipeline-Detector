{"cells":[{"source":"# **0.29 LB Score + Beginner NLP Tutorial**","cell_type":"markdown","metadata":{"_cell_guid":"f5d98a99-0ce2-4c2c-90db-276451d24dd1","_uuid":"b0f30f1df9d61f870994c75a8b28b85b67d35611"}},{"source":"> ## Introduction\n\nThis is my first Kaggle Competition on NLP. This notebook is heavily inspired from the kernels of \n1. abhishek : https://www.kaggle.com/abhishek/approaching-almost-any-nlp-problem-on-kaggle\n2. nzw0301 : https://www.kaggle.com/nzw0301/simple-keras-fasttext-val-loss-0-31\n3. sudalairajkumar : https://www.kaggle.com/sudalairajkumar/simple-feature-engg-notebook-spooky-author\nThank you for your brilliant kernels and tutorials.\n\nApproach followed :\n1. Simple Feature Engineering - Punctuation,Stop Words,Glove Sentence vectors\n2. Creating stack features from\n - Simple Features such as  tfidf and count vectors for words and chars and applying multinomial naive bayes(mnb)- tfidf+words+mnb,tfidf+chars+mnb,count+words+mnb,count+chars+mnb\n - Conv Nets on keras texttosequence, NNs on glove sentence vectors and Fast Text\n3. XGBoost, which is the final model which will use the simple features and stack features as input\n\nFive Fold Crossvalidation is used in all the models along with early stopping for those involving epochs.\n","cell_type":"markdown","metadata":{"_cell_guid":"767f48ca-574b-4db6-91b4-9812599d5f40","_uuid":"bcdfdea94c61aea5d1078e00e7062683e8761e33"}},{"source":"> ## Imports\n\nIn particular, the important ones are scikit and keras which will deal with the data modelling and feature extraction","cell_type":"markdown","metadata":{"_cell_guid":"1d7a3e12-6514-4169-9c26-28b0b3baf847","_uuid":"6d0924303ccb6cdfc67fd97652288f6ceb8b7b26"}},{"source":"# Imports\nimport pandas as pd\nimport sys\nimport glob\nimport errno\nimport csv\nimport numpy as np\nfrom nltk.corpus import stopwords\nimport re\nimport nltk.data\nimport nltk\nimport os\nfrom collections import OrderedDict\nfrom subprocess import check_call\nfrom shutil import copyfile\nfrom sklearn.metrics import log_loss\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport mpld3\nmpld3.enable_notebook()\nimport seaborn as sns\nfrom collections import Counter\nfrom sklearn.cross_validation import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn import ensemble, metrics, model_selection, naive_bayes\nfrom sklearn.preprocessing import LabelEncoder\nimport xgboost as xgb\nfrom tqdm import tqdm\nfrom sklearn import preprocessing, decomposition, model_selection, metrics, pipeline\nfrom keras.layers import GlobalAveragePooling1D,Merge,Lambda,Input,GlobalMaxPooling1D, Conv1D, MaxPooling1D, Flatten, Bidirectional, SpatialDropout1D,TimeDistributed\nfrom keras.preprocessing import sequence, text\nfrom keras.callbacks import EarlyStopping\nfrom nltk import word_tokenize\nfrom keras.layers.merge import concatenate\nfrom keras.utils import np_utils\nfrom keras.models import Sequential\nfrom keras.layers.recurrent import LSTM, GRU\nfrom keras.layers.core import Dense, Activation, Dropout\nfrom keras.layers.embeddings import Embedding\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras import initializers\nfrom keras import backend as K\nfrom sklearn.linear_model import SGDClassifier as sgd\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.callbacks import EarlyStopping","outputs":[],"cell_type":"code","metadata":{"_cell_guid":"dc5b9252-7d0b-49b7-819b-bd323d20043b","_uuid":"ed5c80c84c14e92ff4de5e4f3f0211778a04e299","collapsed":true},"execution_count":null},{"source":"> ## Read data","cell_type":"markdown","metadata":{"_cell_guid":"d213e432-a134-4d85-8b88-16fed3db8a26","_uuid":"6fa28aaff53727a340d68331a4d267c0b3c98e53"}},{"source":"# Read data\ntrain = \"../input/spooky-author-identification/train.csv\"\ntest = \"../input/spooky-author-identification/test.csv\"\nwv = \"../input/glove-global-vectors-for-word-representation/glove.6B.100d.txt\"\nX_train = pd.read_csv( train, header=0,delimiter=\",\" )\nX_test = pd.read_csv( test, header=0,delimiter=\",\" )\n\nauthors = ['EAP','MWS','HPL']\nY_train = LabelEncoder().fit_transform(X_train['author'])","outputs":[],"cell_type":"code","metadata":{"_cell_guid":"b81d83fe-af27-45e4-a877-88701472233f","_uuid":"ae0cc89a2c64db19c2526960a9e7ea5b2f3e207d","collapsed":true},"execution_count":null},{"source":">## Clean Data\n\nThis is to extract the pure words from the texts","cell_type":"markdown","metadata":{"_cell_guid":"57b0752e-87d9-4753-bcae-e3d490ea724a","_uuid":"160fb283cd791532c99c571f719536eed5a44ddb"}},{"source":"# Clean data\ndef clean(X_train,X_test):\n    X_train['words'] = [re.sub(\"[^a-zA-Z]\",\" \", data).lower().split() for data in X_train['text']]\n    X_test['words'] = [re.sub(\"[^a-zA-Z]\",\" \", data).lower().split() for data in X_test['text']]\n    return X_train,X_test\nX_train,X_test = clean(X_train,X_test)\n","outputs":[],"cell_type":"code","metadata":{"_cell_guid":"8a57b8ce-f62f-4384-9ede-9b36d5bd6f3a","_uuid":"5823c0a3cb183422e3babf23204c3440ef8e9aed","collapsed":true},"execution_count":null},{"source":"> # Simple Feature Engineering\n\nI am using viz. three types of features\n1. Punctuations - Each author favours a particular type(s) of punctuation. Hence I've taken sets of punctuations which are similar in behaviour. They are \";:\" , \",.\" , \"?\" , \"'\" , \"\"\" and the combined set of all of these.\n2. Stop word percentage- Stop word percentage of each text \n\n","cell_type":"markdown","metadata":{"_cell_guid":"206c8c20-4b47-406c-b58b-6d8cd0214d02","_uuid":"c1001d9ef26eeb9863b26e21cb910e886baa702c"}},{"source":"# Feature Engineering\n# Punctuation\npunctuations = [{\"id\":1,\"p\":\"[;:]\"},{\"id\":2,\"p\":\"[,.]\"},{\"id\":3,\"p\":\"[?]\"},{\"id\":4,\"p\":\"[\\']\"},{\"id\":5,\"p\":\"[\\\"]\"},{\"id\":6,\"p\":\"[;:,.?\\'\\\"]\"}]\nfor p in punctuations:\n    punctuation = p[\"p\"]\n    _train =  [ sentence.split() for sentence in X_train['text'] ]\n    X_train['punc_'+str(p[\"id\"])] = [len([word for word in sentence if bool(re.search(punctuation, word))])*100.0/len(sentence) for sentence in _train]    \n\n    _test =  [ sentence.split() for sentence in X_test['text'] ]\n    X_test['punc_'+str(p[\"id\"])] = [len([word for word in sentence if bool(re.search(punctuation, word))])*100.0/len(sentence) for sentence in _test]    \n\n","outputs":[],"cell_type":"code","metadata":{"_cell_guid":"b75a91dd-84e8-432b-96d2-8fd440eeda58","_uuid":"d0dcf1ed3522a379795823c1d04aa3ed1c15c8a3","collapsed":true},"execution_count":null},{"source":"# Feature Engineering\n# Stop Words\n_dist_train = [x for x in X_train['words']]\nX_train['stop_word'] = [len([word for word in sentence if word in stopwords.words('english')])*100.0/len(sentence) for sentence in _dist_train]\n\n_dist_test = [x for x in X_test['words']]\nX_test['stop_word'] = [len([word for word in sentence if word in stopwords.words('english')])*100.0/len(sentence) for sentence in _dist_test]    ","outputs":[],"cell_type":"code","metadata":{"_cell_guid":"d4683ee4-4d6c-4579-a114-1a2506466ba5","_uuid":"a146530189fa92a757865581cd2fbbe4d25e4ae6","collapsed":true},"execution_count":null},{"source":"> ## Stack Features\n\nThanks sudalairajkumar, for explaining this technique.\n\nStack features are features which are in itself predictions from existing features. Existing features which have many dimensions are good usecases to try out feature stacking on.\n\nThe motivation for using stack features is:\n1. To create features with respect to documents such as tfidf and counts, both word based and character based.\n2. To reduce the dimensionality of the huge feature vectors of the above so as to fit the data better.\n3. Multinomial Bayes works well due to the multinomial distribution and strong independence assumptions in the model.\n\n","cell_type":"markdown","metadata":{"_cell_guid":"20413657-fe05-4eea-b4b5-1aac0728eb63","_uuid":"1cb4be139a9c38085f96f6b2aaaa15dd974589d2"}},{"source":"# Feature Engineering\n# tfidf - words - nb\ndef tfidfWords(X_train,X_test):\n    tfidf_vec = TfidfVectorizer(stop_words='english', ngram_range=(1,3))\n    full_tfidf = tfidf_vec.fit_transform(X_train['text'].values.tolist() + X_test['text'].values.tolist())\n    train_tfidf = tfidf_vec.transform(X_train['text'].values.tolist())\n    test_tfidf = tfidf_vec.transform(X_test['text'].values.tolist())\n    return train_tfidf,test_tfidf,full_tfidf\n    \ndef runMNB(train_X, train_y, test_X, test_y, test_X2):\n    model = naive_bayes.MultinomialNB()\n    model.fit(train_X, train_y)\n    pred_test_y = model.predict_proba(test_X)\n    pred_test_y2 = model.predict_proba(test_X2)\n    return pred_test_y, pred_test_y2, model\n\ndef do_tfidf_MNB(X_train,X_test,Y_train):\n    train_tfidf,test_tfidf,full_tfidf = tfidfWords(X_train,X_test)\n    cv_scores = []\n    pred_full_test = 0\n    pred_train = np.zeros([X_train.shape[0], 3])\n    kf = model_selection.KFold(n_splits=5, shuffle=True, random_state=2017)\n    for dev_index, val_index in kf.split(X_train):\n        dev_X, val_X = train_tfidf[dev_index], train_tfidf[val_index]\n        dev_y, val_y = Y_train[dev_index], Y_train[val_index]\n        pred_val_y, pred_test_y, model = runMNB(dev_X, dev_y, val_X, val_y, test_tfidf)\n        pred_full_test = pred_full_test + pred_test_y\n        pred_train[val_index,:] = pred_val_y\n        cv_scores.append(metrics.log_loss(val_y, pred_val_y))\n    print(\"Mean cv score : \", np.mean(cv_scores))\n    pred_full_test = pred_full_test / 5.\n    return pred_train,pred_full_test\n\npred_train,pred_test = do_tfidf_MNB(X_train,X_test,Y_train)\nX_train[\"tfidf_words_nb_eap\"] = pred_train[:,0]\nX_train[\"tfidf_words_nb_hpl\"] = pred_train[:,1]\nX_train[\"tfidf_words_nb_mws\"] = pred_train[:,2]\nX_test[\"tfidf_words_nb_eap\"] = pred_test[:,0]\nX_test[\"tfidf_words_nb_hpl\"] = pred_test[:,1]\nX_test[\"tfidf_words_nb_mws\"] = pred_test[:,2]","outputs":[],"cell_type":"code","metadata":{"_cell_guid":"fd55f994-9a63-42ef-bf22-329ac518aaba","_uuid":"dc67057a457e2ce9fbc5d63881039616164362ac","collapsed":true},"execution_count":null},{"source":"# Feature Engineering\n# tfidf - chars - nb\ndef tfidfWords(X_train,X_test):\n    tfidf_vec = TfidfVectorizer(stop_words='english', ngram_range=(1,5),analyzer='char')\n    full_tfidf = tfidf_vec.fit_transform(X_train['text'].values.tolist() + X_test['text'].values.tolist())\n    train_tfidf = tfidf_vec.transform(X_train['text'].values.tolist())\n    test_tfidf = tfidf_vec.transform(X_test['text'].values.tolist())\n    return train_tfidf,test_tfidf\n    \ndef runMNB(train_X, train_y, test_X, test_y, test_X2):\n    model = naive_bayes.MultinomialNB()\n    model.fit(train_X, train_y)\n    pred_test_y = model.predict_proba(test_X)\n    pred_test_y2 = model.predict_proba(test_X2)\n    return pred_test_y, pred_test_y2, model\n\ndef do(X_train,X_test,Y_train):\n    train_tfidf,test_tfidf = tfidfWords(X_train,X_test)\n    cv_scores = []\n    pred_full_test = 0\n    pred_train = np.zeros([X_train.shape[0], 3])\n    kf = model_selection.KFold(n_splits=5, shuffle=True, random_state=2017)\n    for dev_index, val_index in kf.split(X_train):\n        dev_X, val_X = train_tfidf[dev_index], train_tfidf[val_index]\n        dev_y, val_y = Y_train[dev_index], Y_train[val_index]\n        pred_val_y, pred_test_y, model = runMNB(dev_X, dev_y, val_X, val_y, test_tfidf)\n        pred_full_test = pred_full_test + pred_test_y\n        pred_train[val_index,:] = pred_val_y\n        cv_scores.append(metrics.log_loss(val_y, pred_val_y))\n    print(\"Mean cv score : \", np.mean(cv_scores))\n    pred_full_test = pred_full_test / 5.\n    return pred_train,pred_full_test\npred_train,pred_test = do(X_train,X_test,Y_train)\nX_train[\"tfidf_chars_nb_eap\"] = pred_train[:,0]\nX_train[\"tfidf_chars_nb_hpl\"] = pred_train[:,1]\nX_train[\"tfidf_chars_nb_mws\"] = pred_train[:,2]\nX_test[\"tfidf_chars_nb_eap\"] = pred_test[:,0]\nX_test[\"tfidf_chars_nb_hpl\"] = pred_test[:,1]\nX_test[\"tfidf_chars_nb_mws\"] = pred_test[:,2]","outputs":[],"cell_type":"code","metadata":{"_cell_guid":"de63c172-df39-4bb9-a453-d0dcf3095977","_uuid":"544570b7813f55003e70baa51e31ad53cfa83a51","collapsed":true},"execution_count":null},{"source":"# Feature Engineering\n# count - words - nb\ndef countWords(X_train,X_test):\n    count_vec = CountVectorizer(stop_words='english', ngram_range=(1,3))\n    count_vec.fit(X_train['text'].values.tolist() + X_test['text'].values.tolist())\n    train_count = count_vec.transform(X_train['text'].values.tolist())\n    test_count = count_vec.transform(X_test['text'].values.tolist())\n    return train_count,test_count\n    \ndef runMNB(train_X, train_y, test_X, test_y, test_X2):\n    model = naive_bayes.MultinomialNB()\n    model.fit(train_X, train_y)\n    pred_test_y = model.predict_proba(test_X)\n    pred_test_y2 = model.predict_proba(test_X2)\n    return pred_test_y, pred_test_y2, model\n\ndef do_count_MNB(X_train,X_test,Y_train):\n    train_count,test_count=countWords(X_train,X_test)\n    cv_scores = []\n    pred_full_test = 0\n    pred_train = np.zeros([X_train.shape[0], 3])\n    kf = model_selection.KFold(n_splits=5, shuffle=True, random_state=2017)\n    for dev_index, val_index in kf.split(X_train):\n        dev_X, val_X = train_count[dev_index], train_count[val_index]\n        dev_y, val_y = Y_train[dev_index], Y_train[val_index]\n        pred_val_y, pred_test_y, model = runMNB(dev_X, dev_y, val_X, val_y, test_count)\n        pred_full_test = pred_full_test + pred_test_y\n        pred_train[val_index,:] = pred_val_y\n        cv_scores.append(metrics.log_loss(val_y, pred_val_y))\n    print(\"Mean cv score : \", np.mean(cv_scores))\n    pred_full_test = pred_full_test / 5.\n    return pred_train,pred_full_test\n\npred_train,pred_test = do_count_MNB(X_train,X_test,Y_train)\nX_train[\"count_words_nb_eap\"] = pred_train[:,0]\nX_train[\"count_words_nb_hpl\"] = pred_train[:,1]\nX_train[\"count_words_nb_mws\"] = pred_train[:,2]\nX_test[\"count_words_nb_eap\"] = pred_test[:,0]\nX_test[\"count_words_nb_hpl\"] = pred_test[:,1]\nX_test[\"count_words_nb_mws\"] = pred_test[:,2]","outputs":[],"cell_type":"code","metadata":{"_cell_guid":"22686d4f-f176-48a4-bdf3-ccc48c97c2ae","_uuid":"0a0b350127e8095b863d4d498bf83a930ec77a03","collapsed":true},"execution_count":null},{"source":"# Feature Engineering\n# count - chars - nb\ndef countChars(X_train,X_test):\n    count_vec = CountVectorizer(ngram_range=(1,7),analyzer='char')\n    count_vec.fit(X_train['text'].values.tolist() + X_test['text'].values.tolist())\n    train_count = count_vec.transform(X_train['text'].values.tolist())\n    test_count = count_vec.transform(X_test['text'].values.tolist())\n    return train_count,test_count\n    \ndef runMNB(train_X, train_y, test_X, test_y, test_X2):\n    model = naive_bayes.MultinomialNB()\n    model.fit(train_X, train_y)\n    pred_test_y = model.predict_proba(test_X)\n    pred_test_y2 = model.predict_proba(test_X2)\n    return pred_test_y, pred_test_y2, model\n\ndef do_count_chars_MNB(X_train,X_test,Y_train):\n    train_count,test_count=countChars(X_train,X_test)\n    cv_scores = []\n    pred_full_test = 0\n    pred_train = np.zeros([X_train.shape[0], 3])\n    kf = model_selection.KFold(n_splits=5, shuffle=True, random_state=2017)\n    for dev_index, val_index in kf.split(X_train):\n        dev_X, val_X = train_count[dev_index], train_count[val_index]\n        dev_y, val_y = Y_train[dev_index], Y_train[val_index]\n        pred_val_y, pred_test_y, model = runMNB(dev_X, dev_y, val_X, val_y, test_count)\n        pred_full_test = pred_full_test + pred_test_y\n        pred_train[val_index,:] = pred_val_y\n        cv_scores.append(metrics.log_loss(val_y, pred_val_y))\n    print(\"Mean cv score : \", np.mean(cv_scores))\n    pred_full_test = pred_full_test / 5.\n    return pred_train,pred_full_test\n\npred_train,pred_test = do_count_chars_MNB(X_train,X_test,Y_train)\nX_train[\"count_chars_nb_eap\"] = pred_train[:,0]\nX_train[\"count_chars_nb_hpl\"] = pred_train[:,1]\nX_train[\"count_chars_nb_mws\"] = pred_train[:,2]\nX_test[\"count_chars_nb_eap\"] = pred_test[:,0]\nX_test[\"count_chars_nb_hpl\"] = pred_test[:,1]\nX_test[\"count_chars_nb_mws\"] = pred_test[:,2]","outputs":[],"cell_type":"code","metadata":{"_cell_guid":"75994cc6-655d-46fd-a519-5ae41577792e","_uuid":"a19ae104c208f554c167ec2d01cf94b037a3ecfd","collapsed":true},"execution_count":null},{"source":"> ## Creating sentence vectors from individual word vectors\n\nThanks abhishek, for explaining this technique.\nsent2vec creates a normalized vector for the whole sentence from Glove embeddings. This works better than simply averaging or summing up the individual word vectors to obtain sentence vectors.\n\nThese are simple features.","cell_type":"markdown","metadata":{"_cell_guid":"02c65896-4c23-4e32-b7f7-970fcf5fbdb1","_uuid":"bb5335590476c9eba3a29719be5ff4ddb5fed5e8"}},{"source":"# load the GloVe vectors in a dictionary:\n\ndef loadWordVecs():\n    embeddings_index = {}\n    f = open(wv)\n    for line in f:\n        values = line.split()\n        word = values[0]\n        coefs = np.asarray(values[1:], dtype='float32')\n        embeddings_index[word] = coefs\n    f.close()\n    print('Found %s word vectors.' % len(embeddings_index))\n    return embeddings_index\n\ndef sent2vec(embeddings_index,s): # this function creates a normalized vector for the whole sentence\n    words = str(s).lower()\n    words = word_tokenize(words)\n    words = [w for w in words if not w in stopwords.words('english')]\n    words = [w for w in words if w.isalpha()]\n    M = []\n    for w in words:\n        try:\n            M.append(embeddings_index[w])\n        except:\n            continue\n    M = np.array(M)\n    v = M.sum(axis=0)\n    if type(v) != np.ndarray:\n        return np.zeros(100)\n    return v / np.sqrt((v ** 2).sum())\n\ndef doGlove(x_train,x_test):\n    embeddings_index = loadWordVecs()\n    # create sentence vectors using the above function for training and validation set\n    xtrain_glove = [sent2vec(embeddings_index,x) for x in tqdm(x_train)]\n    xtest_glove = [sent2vec(embeddings_index,x) for x in tqdm(x_test)]\n    xtrain_glove = np.array(xtrain_glove)\n    xtest_glove = np.array(xtest_glove)\n    return xtrain_glove,xtest_glove,embeddings_index\n\nglove_vecs_train,glove_vecs_test,embeddings_index = doGlove(X_train['text'],X_test['text'])\nX_train[['sent_vec_'+str(i) for i in range(100)]] = pd.DataFrame(glove_vecs_train.tolist())\nX_test[['sent_vec_'+str(i) for i in range(100)]] = pd.DataFrame(glove_vecs_test.tolist())\n\n","outputs":[],"cell_type":"code","metadata":{"_cell_guid":"6d4fd041-8719-4924-b1da-0a1bd51456e4","_uuid":"30874cc591fe99222b9aa1b4afaaef9bb7359d7d","collapsed":true},"execution_count":null},{"source":"> ## Using Neural Networks and Facebook's Fasttext\n\n1. NN => Conv Net which uses keras text to sequences to obtain input features\n2. NN_Glove =>  Simple Neural Net which uses sentence glove features\n3. Fasttext => Uses fasttext implementation via keras. Thanks nzw0301, for explaining this technique\n\nAll of the above are used to obtain stack features.","cell_type":"markdown","metadata":{"_cell_guid":"ca087c6a-98b7-45b8-a6dd-be44e8a98e53","_uuid":"f505cda13922f67ca448492497a18d7bbde3ef79"}},{"source":"# Using Neural Networks and Facebook's Fasttext\nearlyStopping=EarlyStopping(monitor='val_loss', patience=0, verbose=0, mode='auto')\n\n# NN\ndef doAddNN(X_train,X_test,pred_train,pred_test):\n    X_train[\"nn_eap\"] = pred_train[:,0]\n    X_train[\"nn_hpl\"] = pred_train[:,1]\n    X_train[\"nn_mws\"] = pred_train[:,2]\n    X_test[\"nn_eap\"] = pred_test[:,0]\n    X_test[\"nn_hpl\"] = pred_test[:,1]\n    X_test[\"nn_mws\"] = pred_test[:,2]\n    return X_train,X_test\n\ndef initNN(nb_words_cnt,max_len):\n    model = Sequential()\n    model.add(Embedding(nb_words_cnt,32,input_length=max_len))\n    model.add(Dropout(0.3))\n    model.add(Conv1D(64,\n                     5,\n                     padding='valid',\n                     activation='relu'))\n    model.add(Dropout(0.3))\n    model.add(MaxPooling1D())\n    model.add(Flatten())\n    model.add(Dense(800, activation='relu'))\n    model.add(Dropout(0.5))\n    model.add(Dense(3, activation='softmax'))\n\n    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics = ['accuracy'])\n    return model\n\ndef doNN(X_train,X_test,Y_train):\n    max_len = 70\n    nb_words = 10000\n    \n    print('Processing text dataset')\n    texts_1 = []\n    for text in X_train['text']:\n        texts_1.append(text)\n\n    print('Found %s texts.' % len(texts_1))\n    test_texts_1 = []\n    for text in X_test['text']:\n        test_texts_1.append(text)\n    print('Found %s texts.' % len(test_texts_1))\n    \n    tokenizer = Tokenizer(num_words=nb_words)\n    tokenizer.fit_on_texts(texts_1 + test_texts_1)\n    sequences_1 = tokenizer.texts_to_sequences(texts_1)\n    word_index = tokenizer.word_index\n    print('Found %s unique tokens.' % len(word_index))\n\n    test_sequences_1 = tokenizer.texts_to_sequences(test_texts_1)\n\n    xtrain_pad = pad_sequences(sequences_1, maxlen=max_len)\n    xtest_pad = pad_sequences(test_sequences_1, maxlen=max_len)\n    del test_sequences_1\n    del sequences_1\n    nb_words_cnt = min(nb_words, len(word_index)) + 1\n\n    # we need to binarize the labels for the neural net\n    ytrain_enc = np_utils.to_categorical(Y_train)\n    \n    kf = model_selection.KFold(n_splits=5, shuffle=True, random_state=2017)\n    cv_scores = []\n    pred_full_test = 0\n    pred_train = np.zeros([xtrain_pad.shape[0], 3])\n    for dev_index, val_index in kf.split(xtrain_pad):\n        dev_X, val_X = xtrain_pad[dev_index], xtrain_pad[val_index]\n        dev_y, val_y = ytrain_enc[dev_index], ytrain_enc[val_index]\n        model = initNN(nb_words_cnt,max_len)\n        model.fit(dev_X, y=dev_y, batch_size=32, epochs=4, verbose=1,validation_data=(val_X, val_y),callbacks=[earlyStopping])\n        pred_val_y = model.predict(val_X)\n        pred_test_y = model.predict(xtest_pad)\n        pred_full_test = pred_full_test + pred_test_y\n        pred_train[val_index,:] = pred_val_y\n    return doAddNN(X_train,X_test,pred_train,pred_full_test/5)\n\n## NN Glove\n\ndef doAddNN_glove(X_train,X_test,pred_train,pred_test):\n    X_train[\"nn_glove_eap\"] = pred_train[:,0]\n    X_train[\"nn_glove_hpl\"] = pred_train[:,1]\n    X_train[\"nn_glove_mws\"] = pred_train[:,2]\n    X_test[\"nn_glove_eap\"] = pred_test[:,0]\n    X_test[\"nn_glove_hpl\"] = pred_test[:,1]\n    X_test[\"nn_glove_mws\"] = pred_test[:,2]\n    return X_train,X_test\n\ndef initNN_glove():\n    # create a simple 3 layer sequential neural net\n    model = Sequential()\n\n    model.add(Dense(128, input_dim=100, activation='relu'))\n    model.add(Dropout(0.3))\n    model.add(BatchNormalization())\n\n    model.add(Dense(128, activation='relu'))\n    model.add(Dropout(0.3))\n    model.add(BatchNormalization())\n\n    model.add(Dense(3))\n    model.add(Activation('softmax'))\n\n    # compile the model\n    model.compile(loss='categorical_crossentropy', optimizer='adam')\n    return model\n\ndef doNN_glove(X_train,X_test,Y_train,xtrain_glove,xtest_glove):\n    # scale the data before any neural net:\n    scl = preprocessing.StandardScaler()\n    ytrain_enc = np_utils.to_categorical(Y_train)\n    kf = model_selection.KFold(n_splits=5, shuffle=True, random_state=2017)\n    cv_scores = []\n    pred_full_test = 0\n    xtrain_glove = scl.fit_transform(xtrain_glove)\n    xtest_glove = scl.fit_transform(xtest_glove)\n    pred_train = np.zeros([xtrain_glove.shape[0], 3])\n    \n    for dev_index, val_index in kf.split(xtrain_glove):\n        dev_X, val_X = xtrain_glove[dev_index], xtrain_glove[val_index]\n        dev_y, val_y = ytrain_enc[dev_index], ytrain_enc[val_index]\n        model = initNN_glove()\n        model.fit(dev_X, y=dev_y, batch_size=32, epochs=10, verbose=1,validation_data=(val_X, val_y),callbacks=[earlyStopping])\n        pred_val_y = model.predict(val_X)\n        pred_test_y = model.predict(xtest_glove)\n        pred_full_test = pred_full_test + pred_test_y\n        pred_train[val_index,:] = pred_val_y\n    return doAddNN_glove(X_train,X_test,pred_train,pred_full_test/5)\n\n# Fast Text\n\ndef doAddFastText(X_train,X_test,pred_train,pred_test):\n    X_train[\"ff_eap\"] = pred_train[:,0]\n    X_train[\"ff_hpl\"] = pred_train[:,1]\n    X_train[\"ff_mws\"] = pred_train[:,2]\n    X_test[\"ff_eap\"] = pred_test[:,0]\n    X_test[\"ff_hpl\"] = pred_test[:,1]\n    X_test[\"ff_mws\"] = pred_test[:,2]\n    return X_train,X_test\n\n\ndef initFastText(embedding_dims,input_dim):\n    model = Sequential()\n    model.add(Embedding(input_dim=input_dim, output_dim=embedding_dims))\n    model.add(GlobalAveragePooling1D())\n    model.add(Dense(3, activation='softmax'))\n\n    model.compile(loss='categorical_crossentropy',\n                  optimizer='adam',\n                  metrics=['accuracy'])\n    return model\n\ndef preprocessFastText(text):\n    text = text.replace(\"' \", \" ' \")\n    signs = set(',.:;\"?!')\n    prods = set(text) & signs\n    if not prods:\n        return text\n\n    for sign in prods:\n        text = text.replace(sign, ' {} '.format(sign) )\n    return text\n\ndef create_docs(df, n_gram_max=2):\n    def add_ngram(q, n_gram_max):\n            ngrams = []\n            for n in range(2, n_gram_max+1):\n                for w_index in range(len(q)-n+1):\n                    ngrams.append('--'.join(q[w_index:w_index+n]))\n            return q + ngrams\n        \n    docs = []\n    for doc in df.text:\n        doc = preprocessFastText(doc).split()\n        docs.append(' '.join(add_ngram(doc, n_gram_max)))\n    \n    return docs\n\ndef doFastText(X_train,X_test,Y_train):\n    min_count = 2\n\n    docs = create_docs(X_train)\n    tokenizer = Tokenizer(lower=False, filters='')\n    tokenizer.fit_on_texts(docs)\n    num_words = sum([1 for _, v in tokenizer.word_counts.items() if v >= min_count])\n\n    tokenizer = Tokenizer(num_words=num_words, lower=False, filters='')\n    tokenizer.fit_on_texts(docs)\n    docs = tokenizer.texts_to_sequences(docs)\n\n    maxlen = 300\n\n    docs = pad_sequences(sequences=docs, maxlen=maxlen)\n    input_dim = np.max(docs) + 1\n    embedding_dims = 20\n\n    # we need to binarize the labels for the neural net\n    ytrain_enc = np_utils.to_categorical(Y_train)\n\n    docs_test = create_docs(X_test)\n    docs_test = tokenizer.texts_to_sequences(docs_test)\n    docs_test = pad_sequences(sequences=docs_test, maxlen=maxlen)\n    xtrain_pad = docs\n    xtest_pad = docs_test\n    \n    kf = model_selection.KFold(n_splits=5, shuffle=True, random_state=2017)\n    cv_scores = []\n    pred_full_test = 0\n    pred_train = np.zeros([xtrain_pad.shape[0], 3])\n    for dev_index, val_index in kf.split(xtrain_pad):\n        dev_X, val_X = xtrain_pad[dev_index], xtrain_pad[val_index]\n        dev_y, val_y = ytrain_enc[dev_index], ytrain_enc[val_index]\n        model = initFastText(embedding_dims,input_dim)\n        model.fit(dev_X, y=dev_y, batch_size=32, epochs=25, verbose=1,validation_data=(val_X, val_y),callbacks=[earlyStopping])\n        pred_val_y = model.predict(val_X)\n        pred_test_y = model.predict(docs_test)\n        pred_full_test = pred_full_test + pred_test_y\n        pred_train[val_index,:] = pred_val_y\n    return doAddFastText(X_train,X_test,pred_train,pred_full_test/5)\n\nX_train,X_test = doFastText(X_train,X_test,Y_train)\nX_train,X_test = doNN(X_train,X_test,Y_train)\nX_train,X_test = doNN_glove(X_train,X_test,Y_train,glove_vecs_train,glove_vecs_test)\n","outputs":[],"cell_type":"code","metadata":{"_cell_guid":"3da63bdc-d29e-4e29-b687-8c2090ec4206","_uuid":"f3155dca771fadc880140e85756cc2bef4a70099","collapsed":true},"execution_count":null},{"source":"> ## Final model \n\nUses XGBoost which contains all the simple and stack features as input","cell_type":"markdown","metadata":{"_cell_guid":"729ee8c9-0cbf-4aba-9937-e0820afe502a","_uuid":"3013562a3a0ad8e8981a0c9f4f1761acd748696b"}},{"source":"# Final Model\n# XGBoost\ndef runXGB(train_X, train_y, test_X, test_y=None, test_X2=None, seed_val=0, child=1, colsample=0.3):\n    param = {}\n    param['objective'] = 'multi:softprob'\n    param['eta'] = 0.1\n    param['max_depth'] = 3\n    param['silent'] = 1\n    param['num_class'] = 3\n    param['eval_metric'] = \"mlogloss\"\n    param['min_child_weight'] = child\n    param['subsample'] = 0.8\n    param['colsample_bytree'] = colsample\n    param['seed'] = seed_val\n    num_rounds = 2000\n\n    plst = list(param.items())\n    xgtrain = xgb.DMatrix(train_X, label=train_y)\n\n    if test_y is not None:\n        xgtest = xgb.DMatrix(test_X, label=test_y)\n        watchlist = [ (xgtrain,'train'), (xgtest, 'test') ]\n        model = xgb.train(plst, xgtrain, num_rounds, watchlist, early_stopping_rounds=50, verbose_eval=20)\n    else:\n        xgtest = xgb.DMatrix(test_X)\n        model = xgb.train(plst, xgtrain, num_rounds)\n\n    pred_test_y = model.predict(xgtest, ntree_limit = model.best_ntree_limit)\n    if test_X2 is not None:\n        xgtest2 = xgb.DMatrix(test_X2)\n        pred_test_y2 = model.predict(xgtest2, ntree_limit = model.best_ntree_limit)\n    return pred_test_y, pred_test_y2, model\n\ndef do(X_train,X_test,Y_train):\n    drop_columns=[\"id\",\"text\",\"words\"]\n    x_train = X_train.drop(drop_columns+['author'],axis=1)\n    x_test = X_test.drop(drop_columns,axis=1)\n    y_train = Y_train\n    \n    kf = model_selection.KFold(n_splits=5, shuffle=True, random_state=2017)\n    cv_scores = []\n    pred_full_test = 0\n    pred_train = np.zeros([x_train.shape[0], 3])\n    for dev_index, val_index in kf.split(x_train):\n        dev_X, val_X = x_train.loc[dev_index], x_train.loc[val_index]\n        dev_y, val_y = y_train[dev_index], y_train[val_index]\n        pred_val_y, pred_test_y, model = runXGB(dev_X, dev_y, val_X, val_y, x_test, seed_val=0, colsample=0.7)\n        pred_full_test = pred_full_test + pred_test_y\n        pred_train[val_index,:] = pred_val_y\n        cv_scores.append(metrics.log_loss(val_y, pred_val_y))\n    print(\"cv scores : \", cv_scores)\n    return pred_full_test/5\nresult = do(X_train,X_test,Y_train)","outputs":[],"cell_type":"code","metadata":{"_cell_guid":"dee100c6-9d47-4919-9385-ca66ab7b5e87","_uuid":"598a66e5c0c4d9cf3b303cf54bfca2dcf08a30c3","collapsed":true},"execution_count":null}],"nbformat":4,"metadata":{"language_info":{"nbconvert_exporter":"python","mimetype":"text/x-python","version":"3.6.3","name":"python","file_extension":".py","pygments_lexer":"ipython3","codemirror_mode":{"version":3,"name":"ipython"}},"anaconda-cloud":{},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat_minor":1}