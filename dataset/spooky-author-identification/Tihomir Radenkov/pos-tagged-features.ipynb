{"nbformat":4,"nbformat_minor":1,"cells":[{"cell_type":"markdown","source":"To find the author of text we need two things: features and classificator. My approach here is to use one classifier for tagged words and one for feature classification. First I take treebank POS tagged words form NLTK packages. With this I train the tagger. POS means Part OF Speach, so we get a word and must define it is verb (VB) for example or adjective (JJ). The idea is to use tagged words ( or sequence of them) as features.","metadata":{"_uuid":"fc8e9cebbac1662ca841662d7f2e286fd34cec02","_cell_guid":"10c002ab-1615-431a-9bf6-ef8b5404ecfd"}},{"execution_count":null,"cell_type":"code","outputs":[],"source":"from nltk.corpus import treebank\nfrom nltk.tag.sequential import ClassifierBasedPOSTagger","metadata":{"_uuid":"f186de777437746380a94131581fedc742718b5c","_kg_hide-output":true,"collapsed":true,"_cell_guid":"828c71eb-de3e-457f-9095-0bdcdf50ea51"}},{"cell_type":"markdown","source":"Now other imports:","metadata":{"_uuid":"c5bab4465f0769900743bb6cfa52bb1f1e60c33b","_cell_guid":"9d85d4bc-4ccd-49c1-ba49-6fa5bbe422d0"}},{"execution_count":null,"cell_type":"code","outputs":[],"source":"import pandas as pd\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction import DictVectorizer\nfrom sklearn.naive_bayes import BernoulliNB\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.pipeline import make_pipeline\nimport numpy as np\nimport mglearn\nfrom sklearn.metrics import confusion_matrix\nimport matplotlib.pyplot as plt\nimport nltk\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem.snowball import SnowballStemmer\nimport itertools","metadata":{"_uuid":"584a2491a19f92920b7597ee1b52d86abd0746f5","_kg_hide-output":true,"collapsed":true,"_cell_guid":"663a7668-8477-431d-9ccb-cb4d44319773"}},{"execution_count":null,"cell_type":"code","outputs":[],"source":"# -------------- Main code\ntrain = pd.read_csv('train.csv')\ntrain_sents = treebank.tagged_sents()\ntagger = ClassifierBasedPOSTagger(train=train_sents)\nstemmer = SnowballStemmer('english')","metadata":{"_uuid":"c6535147a78221914f0bd3dc905e0194ce319a3e","_kg_hide-output":true,"collapsed":true,"_cell_guid":"34df873a-6cad-4112-ba77-afb10fe55947"}},{"cell_type":"markdown","source":"NLTK has a good parser RegexpParser, it takes as argument taggs in appropriate format. For more info follow this link:\n\n[nltk book](http://http://www.nltk.org/api/nltk.chunk.htm)\n\nTags can give as information about style of give author for example. I use this and search features as \"tags sequences\", with other words: sequence of words with given tag. One sequence is one unique feature. It can be more or less usefull but I beleave this approach can gvie as many possibilities: see get_sequence_tags()","metadata":{"_uuid":"0b26e523e1431b47a4d7ae770e5afe4f078a4b0a","_cell_guid":"c5cbad5c-a758-4829-841c-6d50a1473d6c"}},{"execution_count":null,"cell_type":"code","outputs":[],"source":"# Define tag sequences\nSEQ_1 = \"SEQ_1: {<DT|PP>?<JJ>*}\"\nSEQ_2 = \"SEQ_2: {<NN><DT|PP\\$>?<JJ>}\"\nSEQ_3 = \"SEQ_3: {<NP>?<VERB>?<NP|JJ>}\"\nSEQ_4 = \"SEQ_4: {<VB.*><NP|PP|CLAUSE>+$}\"\n\ncp1 = nltk.RegexpParser(SEQ_1)\ncp2 = nltk.RegexpParser(SEQ_2)\ncp3 = nltk.RegexpParser(SEQ_3)\ncp4 = nltk.RegexpParser(SEQ_4)\n\nlst_seq = list([cp1, cp2, cp3, cp4])","metadata":{"_uuid":"eb3407e471c307006186a98a1dda2333b6e76af2","_kg_hide-output":true,"collapsed":true,"_cell_guid":"1deec3c1-a076-4a0a-9705-9e947eba2e90"}},{"cell_type":"markdown","source":"In the code above I define foor sequnces which then I send as arguments to nltk.RegexpParser(), then I collect the foor nltk.RegexpParser objects. This list is ised as you can see in function get_sequence_tags. Here is the place to show all functions we need to get features, except function plot_confusion_matrix(), the others are for gewtting features.","metadata":{"_uuid":"7140d501840206fd935845e9b125d627edc6378e","_kg_hide-output":true,"_cell_guid":"0f1a36be-7557-4b49-882c-b1ff02003c56"}},{"execution_count":null,"cell_type":"code","outputs":[],"source":"def plot_confusion_matrix(cm, classes, normalize=False, title='Confusion matrix', cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    if normalize:\n        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    print(cm)\n\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() / 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n\n\ndef get_number_of_spaces(sentence):\n    return sentence.count(' ')\n\n\ndef get_number_of_capitals(sentence):\n    n = sum(1 for c in sentence if c.isupper())\n    return n\n\n\ndef get_number_of_nouns(taged_tokens):\n    n = sum(1 for word, tag in taged_tokens if tag == 'NN' or tag == 'NNS' \\\n            or tag == 'NNP' or tag == 'NNP')\n    return n\n\n\ndef get_number_of_adjectives(taged_tokens):\n    n = sum(1 for word, tag in taged_tokens if tag == 'JJ' or tag == 'JJR' or tag == 'JJS')\n    return n\n\n\ndef get_count_of_tagged(taged_tokens, tag_in):\n    n = sum(1 for word, tag in taged_tokens if tag == tag_in)\n    return n\n\n\ndef is_past_tense(taged_tokens):\n    n = sum(1 for word, tag in taged_tokens if tag == 'VBD')\n    return (n > 0)\n\n\ndef is_modal(taged_tokens):\n    n = sum(1 for word, tag in taged_tokens if tag == 'MD')\n    return (n > 0)\n\n\ndef vocab_richness(sentence):\n    unique = set(sentence.split())\n    count_uniques = len(unique)\n    return count_uniques\n\n\ndef get_first_words(sentence, count):\n    arr_words = sentence.split()\n    ret_words = arr_words[:count]\n    str_ret = ' '.join(ret_words)\n    return str_ret\n\n\ndef get_one_word(sentence, position):\n    arr_words = sentence.split()\n    if len(arr_words) >= (position + 1):\n        ret_word = arr_words[position]\n        return ret_word\n    else:\n        return False\n\n\ndef exists_she(sentense):\n    if 'she' in sentense.lower():\n        return True\n    else:\n        return False\n\n\ndef exists_he(sentense):\n    if 'he' in sentense.lower():\n        return True\n    else:\n        return False\n\n\n\ndef first_tag(taged_tokens):\n    return str(taged_tokens[0][1])\n\n\ndef second_tag(taged_tokens):\n    if len(taged_tokens) > 1:\n        return str(taged_tokens[1][1])\n    else:\n        return False\n\n\ndef third_tag(taged_tokens):\n    if len(taged_tokens) > 2:\n        return str(taged_tokens[2][1])\n    else:\n        return False\n\ndef get_consonant_letters(sentence):\n    consonants = 0\n    for word in sentence:\n        for letter in word:\n            if letter in 'bcdfghjklmnpqrstvwxz':\n                consonants += 1\n\n    return consonants\n\n\ndef get_sonant_letters(sentence):\n    sonants = 0\n    for word in sentence:\n        for letter in word:\n            if letter in 'aieouy':\n                sonants += 1\n\n    return sonants\n\n\ndef lexical_diversity(text):\n    return len(set(text)) / len(text)\n\n\ndef get_sequence_tags(taged_tokens, n_sequence):\n    countSequence = 0\n    cp = lst_seq[n_sequence-1]\n    result = cp.parse(taged_tokens)\n\n    for tre in result:\n        if isinstance(tre, nltk.tree.Tree):\n            if tre.label() ==  cp._stages[0]._chunk_label:\n                countSequence += 1\n\n    return (countSequence > 0)","metadata":{"_uuid":"d38e5877f0425a371c49638abe5e2feb77bfa63d","_kg_hide-output":true,"collapsed":true,"_cell_guid":"78deed95-8c01-48ad-b7a7-bbc180dad366"}},{"cell_type":"markdown","source":"The most important function, is get_sentence_features(), lets comment a litle bit the code.\nHere is the right place to use the NLTK.SnowballStemmer(), when the input sentence com in the function\nfirst it is transformed with the stemmer.What it does? It simply gets the word and give as output the grammatical stem\nof it. then the sentence is tokenized from nltk.wordpunct_tokenize and finaly the token string\ncommes to the tagger. The variable taged_tokens is used as argument to functions to get features.","metadata":{"_uuid":"860a133340ac008f878d4e1f085241abab3fb408","_cell_guid":"42cdd0f6-0979-42b5-b871-bb2434766bca"}},{"execution_count":null,"cell_type":"code","outputs":[],"source":"def get_sentence_features(sentens_in):\n    stemmed_words = list()\n    for w in sentens_in.split():\n        stemmed_words.append(stemmer.stem(w))\n\n    sentence = ' '.join(stemmed_words)\n    word_tokens = nltk.wordpunct_tokenize(sentence)\n\n    taged_tokens = tagger.tag(word_tokens)\n\n    X_dict = {}\n\n    X_dict['seq_01'] = get_sequence_tags(taged_tokens, 1)\n    X_dict['seq_02'] = get_sequence_tags(taged_tokens, 2)\n    X_dict['seq_03'] = get_sequence_tags(taged_tokens, 3)\n    X_dict['seq_04'] = get_sequence_tags(taged_tokens, 4)\n\n    X_dict['lexical_diversity'] = lexical_diversity(sentence.lower())\n    X_dict['get_consonant_letters'] = get_consonant_letters(sentence.lower())\n    X_dict['get_sonant_letters'] = get_sonant_letters(sentence.lower())\n\n    X_dict['count_of_spaces'] = get_number_of_spaces(sentence)\n    X_dict['count_capitals'] = get_number_of_capitals(sentence)\n    X_dict['count_nouns'] = get_number_of_nouns(taged_tokens)\n    X_dict['count_adjectives'] = get_number_of_adjectives(taged_tokens)\n\n    X_dict['count_numbers'] = get_count_of_tagged(taged_tokens, 'CD')\n    X_dict['count_NNS'] = get_count_of_tagged(taged_tokens, 'NNS')\n    X_dict['count_NNP'] = get_count_of_tagged(taged_tokens, 'NNP')\n    X_dict['count_NNPS'] = get_count_of_tagged(taged_tokens, 'NNPS')\n    X_dict['count_RBS'] = get_count_of_tagged(taged_tokens, 'RBS')\n    X_dict['count_RBR'] = get_count_of_tagged(taged_tokens, 'RBR')\n    X_dict['count_WP'] = get_count_of_tagged(taged_tokens, 'WP')\n    X_dict['count_WP$'] = get_count_of_tagged(taged_tokens, 'WP$')\n    X_dict['count_WRB'] = get_count_of_tagged(taged_tokens, 'WRB')\n    X_dict['count_PRP'] = get_count_of_tagged(taged_tokens, 'PRP')\n    X_dict['count_POS'] = get_count_of_tagged(taged_tokens, 'POS')\n    X_dict['count_FW'] = get_count_of_tagged(taged_tokens, 'FW')\n    X_dict['count_VB'] = get_count_of_tagged(taged_tokens, 'VB')\n    X_dict['count_VBD'] = get_count_of_tagged(taged_tokens, 'VBD')\n    X_dict['count_VBG'] = get_count_of_tagged(taged_tokens, 'VBG')\n    X_dict['count_VBN'] = get_count_of_tagged(taged_tokens, 'VBN')\n    X_dict['count_CC'] = get_count_of_tagged(taged_tokens, 'CC')\n\n    X_dict['count_DT']         = get_count_of_tagged(taged_tokens, 'DT')\n    X_dict['count_UH']         = get_count_of_tagged(taged_tokens, 'UH')\n    X_dict['count_SYM']        = get_count_of_tagged(taged_tokens, 'SYM')\n    X_dict['count_PDT']        = get_count_of_tagged(taged_tokens, 'PDT')\n    X_dict['count_LS']         = get_count_of_tagged(taged_tokens, 'LS')\n\n    X_dict['count_3rd person'] = get_count_of_tagged(taged_tokens, 'VBZ')\n    X_dict['count_gerund'] = get_count_of_tagged(taged_tokens, 'VBG')\n\n    X_dict['is_past_tense'] = is_past_tense(taged_tokens)\n    X_dict['is_modal'] = is_modal(taged_tokens)\n    X_dict['vocab_richness'] = vocab_richness(sentence)\n    X_dict['first_tag'] = first_tag(taged_tokens)\n    X_dict['second_tag'] = second_tag(taged_tokens)\n    X_dict['third_tag'] = third_tag(taged_tokens)\n    \n    X_dict['first_one_word'] = get_one_word(sentence, 0)\n    X_dict['second_one_word'] = get_one_word(sentence, 1)\n    X_dict['third_one_word'] = get_one_word(sentence, 2)\n    X_dict['forth_one_word'] = get_one_word(sentence, 3)\n    X_dict['fifth_one_word'] = get_one_word(sentence, 4)\n    X_dict['sixth_one_word'] = get_one_word(sentence, 5)\n    X_dict['seventh_one_word'] = get_one_word(sentence, 6)\n    X_dict['eith_one_word'] = get_one_word(sentence, 7)\n    X_dict['ninth_one_word'] = get_one_word(sentence, 8)\n    X_dict['tenth_one_word'] = get_one_word(sentence, 9)\n\n    X_dict['first_6_word'] = get_first_words(sentence, 6)\n    X_dict['first_5_word'] = get_first_words(sentence, 5)\n    X_dict['first_4_word'] = get_first_words(sentence, 4)\n    X_dict['first_3_word'] = get_first_words(sentence, 3)\n    X_dict['first_2_word'] = get_first_words(sentence, 2)\n\n    X_dict['exists_she'] = exists_she(sentence)\n    X_dict['exists_he']  = exists_he(sentence)\n\n    X_dict['first_word_is_the'] = ('the' == get_first_words(sentence.lower(), 1))\n    X_dict['first_word_is_she'] = ('she' == get_first_words(sentence.lower(), 1))\n    X_dict['first_word_is_he']  = ('he' == get_first_words(sentence.lower(), 1))\n    X_dict['first_word_is_it']  = ('it' == get_first_words(sentence.lower(), 1))\n    X_dict['first_word_is_this'] = ('this' == get_first_words(sentence.lower(), 1))\n    X_dict['first_word_is_you']  = ('you' == get_first_words(sentence.lower(), 1))\n\n\n    X_dict['Raymond'] = ('raymond' in sentence.lower())\n    X_dict['Perdita'] = ('perdita' in sentence.lower())\n    X_dict['Idris']   = ('idris' in sentence.lower())\n    X_dict['Adrian']  = ('adrian' in sentence.lower())\n    X_dict['Chapter'] = ('chapter' in sentence.lower())\n    X_dict['sinister'] = ('sinister' in sentence.lower())\n    X_dict['weird']    = ('weird' in sentence.lower())\n    X_dict['horrible'] = ('horrible' in sentence.lower())\n\n    return X_dict","metadata":{"_uuid":"d108956fd226d1d0c9cc4c84cd8656b5769e898d","_kg_hide-output":true,"collapsed":true,"_cell_guid":"1f3341a3-295b-423b-8b94-a620bbe6a9e1"}},{"cell_type":"markdown","source":" \nThis function is expected to return dictionary with features which are used later for classification. Let explain the method I use to extract features from a sentence. I mentioned above the tag sequences. What is a tag? This is grammatical description of a given word. The full name of these tags are POS or Part Of Speech tags. It marks a word as verb or adjective for example. In my opinion every author of text, not only horror authors use different style and structure of the written text. This style can be described as feature.\n\nThe full list of POS tags can be viewed here:\n\n[penn_treebank_pos](http:///www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html)\n\nNext variables: SEQ_1, SEQ_2, SEQ_3, SEQ_4 are just strings used as argument for nltk.RegexpParser(). Here you can find detailed information how to used this parser:\n\n[nltk.org book](http://www.nltk.org/book/ch07.html)\n\nTake attention to functions I use for feature extraction: get_sonant_letters(): All letters that are sonant in english language: \"aieouy\" Another function to get consonant letters is: get_consonant_letters() theese letters are : \"bcdfghjklmnpqrstvwxz\" Getting the count of these types of lettes are our features.\n\nAnother important feature is vocab_richness: it gives the count of unoque words in sentence. Also lexical_diversity feature that gives the divercity of given sentence. The function get_count_of_tagged(taged_tokens, 'CD') returns the count of \"CD Cardinal number\" numbers, the function get_count_of_tagged(taged_tokens, 'NNP') returns the count of nouns. So in this way we can get count of different POS tags in the sentence. So, we can find the use of \" 3rd person\" or if the sentence is in past tense. Additionaly I take first, second etc... words evey as feature. Last features are just words form train.csv. They are choosen emiricaly , not used any special method, just words with many counts. \n\nHowever they do not give as much information about the author, we cannot rely on them, it is not sure that these names are used in any text from the authors.","metadata":{"_uuid":"5356becdfbd15598b11ff19e9b8fb3044d72b397","_cell_guid":"8229f255-f870-4c85-911f-2e46f0bc7d1b"}},{"cell_type":"markdown","source":"Now, using LbelEncoder I will transform \"y\" lables as binary with values : [0,1,2]. I realy need this because our classifier will understand only these values.","metadata":{"_uuid":"48ee37938ff392f93ccd5305a3590a41b3738d08","_cell_guid":"a4fdf7ce-368c-461e-ae8c-eec2b6f3344c"}},{"execution_count":null,"cell_type":"code","outputs":[],"source":"lbl_enc = preprocessing.LabelEncoder()\ny = lbl_enc.fit_transform(train.author.values)","metadata":{"_uuid":"6e7bd5f8b74c7bbe3dfff9d15bd535bf0d5d781d","_kg_hide-output":true,"collapsed":true,"_cell_guid":"9f8b8f2d-013d-4577-86d9-330a7d4044d5"}},{"cell_type":"markdown","source":"\nThe i split the input data to foor variables, two for train data and two for validation data.","metadata":{"_uuid":"327b250491428968c01d4b18e51502e44014520f","_cell_guid":"786ad883-2471-41e2-81dc-538099172686"}},{"execution_count":null,"cell_type":"code","outputs":[],"source":"xtrain, xvalid, ytrain, yvalid = train_test_split(train.text.values, y,\n                                                  stratify=y,\n                                                  random_state=32,\n                                                  test_size=0.2, shuffle=True)\nprint(xtrain.shape)\nprint(xvalid.shape)","metadata":{"_uuid":"3dc9ffcb760d2978e0f147281e92db7898b746b9","_kg_hide-output":true,"collapsed":true,"_cell_guid":"57e66f94-1f38-414d-9bb1-a8c204be58f4"}},{"cell_type":"markdown","source":"\nNext I need is to get stopwords. Frquently used for NLP tasks stopwords usualy increase positive percent of classifications. Here i just added an array with additional values '.', ',', '\"', \"'\", '?', '!', ':', ';', '(', ')', '[', ']', '{', '}']","metadata":{"_uuid":"616ec587b8ab666eb7371703f55cbd9fa7d3b0e9","_cell_guid":"34a2545b-b4ea-4de0-ae22-a7cd635ee1b0"}},{"execution_count":null,"cell_type":"code","outputs":[],"source":"from nltk.corpus import stopwords\n\nstop_words = set(stopwords.words('english'))\nstop_words.update( ['.', ',', '\"', \"'\", '?', '!', ':', ';', '(', ')', '[', ']', '{', '}'])","metadata":{"_uuid":"70f499a14df1ba546d5597f21f40f935eb8f35aa","_kg_hide-output":true,"collapsed":true,"_cell_guid":"849f09d0-93d9-4855-b703-0008f48fbc7c"}},{"cell_type":"markdown","source":"\nThe main processs for extracting features begins here: For all data in input IN_x we try to extract features. The list Out_x contains feature vectors for all input data (IN_x), Out_y is a list with labels. As final result we have two pairs:\n\n1. X_Train, Y_train -> for train data;\n2. X_valid, Y_valid -> for validation;","metadata":{"_uuid":"485261086540ddb2d49f5ab1ed5382119a46e0b9","_cell_guid":"463400bf-9450-437f-908b-e8f81b82eaf8"}},{"execution_count":null,"cell_type":"code","outputs":[],"source":"def get_train_features(IN_x, IN_y):\n    Out_x, Out_y = [], []\n    index = 0\n\n    for sentens_edna in IN_x:\n        word_tokens1 = [i for i in word_tokenize(sentens_edna) if i not in stop_words]\n        sentens_in = ' '.join(word_tokens1)\n        X_feat_dict = get_sentence_features(sentens_in)\n        Out_x.append(X_feat_dict)\n        Out_y.append(IN_y[index])\n        index += 1\n\n    return Out_x, Out_y\n\n\nX_Train, Y_train = get_train_features(xtrain, ytrain)\nX_valid, Y_valid = get_train_features(xvalid, yvalid)","metadata":{"_uuid":"8247fd8e6f60b87d2de8cc8a43a1ba254fac343d","_kg_hide-output":true,"collapsed":true,"_cell_guid":"0d21ac57-aad7-41f1-8f8e-84ccc9aa23f6"}},{"cell_type":"markdown","source":"We make pipeline and cross validation to achieve best results. \nNext step is to transform Xtrain with DictVectorizer() from sklearn.feature_extraction. Our classifier is Naive Bayes or the variant implemention BernoulliNB(). On cross validation, GridSearchCV sends different alpha parameters to BernoulliNB(), the best result is shown.\n\nWe need Xtrain_ctv to train the classifier and X_valid_ctv for our confusion matrix","metadata":{"_uuid":"37cf9bc70274cfe70aa512ab88b6718e264ab447","_cell_guid":"35a8467b-307a-4b8a-8dcb-53d97df741bb"}},{"execution_count":null,"cell_type":"code","outputs":[],"source":"clf = grid.best_estimator_.named_steps['bernoullinb']\ncoef = grid.best_estimator_.named_steps['bernoullinb'].coef_\nbest_alpha = grid.best_estimator_.named_steps['bernoullinb'].alpha\nprint(\"Best cross-validation alpha: {:.2f}\".format(best_alpha))","metadata":{"_uuid":"a68c4e2f215ccd0c1a9730e773abcbac040f3d7b","_kg_hide-output":true,"collapsed":true,"_cell_guid":"a140c033-716d-497a-9f2d-7125d53a1fa7"}},{"cell_type":"markdown","source":"We have to determine which of the features are important or what is the \"coef\" of them. \nFirst we get the feature names, then with help the great tool mglearn we draw 3 grapchics , one for every class.","metadata":{"_uuid":"508a947b3a384de40e8e02a5df96ad2210d60850","_cell_guid":"42ecce21-e89d-4caa-8bca-4464db559159"}},{"execution_count":null,"cell_type":"code","outputs":[],"source":"feature_names = np.array(dict_vect.get_feature_names())\n\nmglearn.tools.visualize_coefficients(coef[0], feature_names, n_top_features=25)\nmglearn.tools.visualize_coefficients(coef[1], feature_names, n_top_features=25)\nmglearn.tools.visualize_coefficients(coef[2], feature_names, n_top_features=25)","metadata":{"_uuid":"e20ad8f057410a20fc29234d5b982a70115224cd","_kg_hide-output":true,"collapsed":true,"_cell_guid":"0e839239-a688-4545-9461-0f18c9ae225c"}},{"cell_type":"markdown","source":"mglearn.tools.visualize_coefficients method takes as argument coef - it is array with 3 dimensions, one for every class, next argument is feature names and n_top_features=25 means we need 25 from all features with best coeficient. Next 3 pictures show the these features.\nThe coeficients are negative and you can see on the rigth most important features. Some of them are:\n\"get_consonant_letters\",\n\"lexical_diversity\",\n\"vocab_richness\"\nand other.\n First graphics is most important features for  EAP.","metadata":{"_uuid":"ed299369609c7d59672bd595ee18c6822a8f8bcd","_cell_guid":"41d9de32-d770-4c0a-ad35-e61c7b349777"}},{"cell_type":"markdown","source":"![](https://i.imgur.com/gIp6PWo.png)","metadata":{"_uuid":"4ebdc30b4e5cb2441f5abdf2f86abcaadbe2cc88","_cell_guid":"02024062-d42d-47a0-b0e9-b6a4df5bfcd0"}},{"cell_type":"markdown","source":"Next is graphic for second author HPL. we see that the most important features here are almost the same.","metadata":{"_uuid":"46f611f091d9d33cdff0da87e1e969a8c9366306","_cell_guid":"b66afa59-09f1-40e3-ba98-fffea711f466"}},{"cell_type":"markdown","source":"![](https://i.imgur.com/D9kPt9i.png)","metadata":{"_uuid":"10ab92d2d4be3d9adfb723b178341190df4821e9","_cell_guid":"4d67bf32-7ba0-4879-be70-90d9059c8715"}},{"cell_type":"markdown","source":"Last graphic is for last author MWS:\nThe most important features are litle bit different here.","metadata":{"_uuid":"95a32f6661b86cbfd345fa240ed38565144ab758","_cell_guid":"d0b03bc6-f8af-4a5f-b070-3616a08cc6d2"}},{"cell_type":"markdown","source":"![](https://i.imgur.com/ufNyje6.png)","metadata":{"_uuid":"d2bed94d0313072b01e08f401954bde5882810be","_cell_guid":"33c5fc34-9d39-4d03-908e-a3b6ed74b344"}},{"cell_type":"markdown","source":"","metadata":{"_uuid":"3d78a12d91df37cee9feda56eb1531052b3d640c","_cell_guid":"c14082d2-1cb1-4985-b4ec-3cbcab0d96cb"}},{"cell_type":"markdown","source":"I must explain that features are comming for DictVectorizer(), so it generates ( when transform) many, many features.\nSo the features we can see in function are only basic features. The real features you can see in the pcituires above.\nExactly these features takes the classifier. \n","metadata":{"_uuid":"837562f48d953d592d2f01365da4d01e74887ab2","_cell_guid":"9935833b-b772-490e-9ac2-0bfbec05bd0f"}},{"cell_type":"markdown","source":"Confusion matrix shows the percent we achieve or the score of predicted sentences for every one author:  the higher the diagonal values of the confusion matrix - the better indicating many correct predictions.","metadata":{"_uuid":"416e1b07b4443e8e793fe7ea6ea8ab8ff811c4c6","_cell_guid":"02607858-e956-4c59-ba1e-1b2e95b13355"}},{"execution_count":null,"cell_type":"code","outputs":[],"source":"predictions = clf.predict_proba(X_valid_ctv)\npredicted_lables = clf.predict(X_valid_ctv)\n\ncnf_matrix = confusion_matrix(Y_valid, predicted_lables)\n\n# Plot normalized confusion matrix\nplt.figure()\nplot_confusion_matrix(cnf_matrix, classes=class_names, normalize=True,\n                      title='Normalized confusion matrix')\n\nplt.show()","metadata":{"_uuid":"c67ba47a039b402976d359b5bd008babf2779dd8","_kg_hide-output":true,"collapsed":true,"_cell_guid":"e599b4c9-3011-4cac-9cc8-c8778b30c5b6"}},{"cell_type":"markdown","source":"![](https://i.imgur.com/GKha9Zv.png)","metadata":{"_uuid":"dba6551a56d105c23340b60a0913c826b23c8662","_cell_guid":"396995d2-c727-43d8-9c28-a32dfcfdfa99"}},{"cell_type":"markdown","source":"Take a look to the diagonal form left upper corner. First is EAP with 75% predicted results score. HPL is 60% and MWS is 66%. Also we can see here that most of the predictions are biased to EAP.\n\nNext is Receiver Operating Characteristic curve (or ROC curve.)\nThe ROC curve is created by plotting the true positive rate  against the false positive rate.\nThe true-positive rate is also known as sensitivity. ROC is bumary classification so we have one curve per class.\nOn the graphic you see them with different colors. The legend shows AUC ot Area under the Curve.\nIf we can say with some words AUC as representing the probability that a classifier will rank a randomly chosen positive observation higher than a randomly chosen negative observation.\nHere you find good explanation of ROC and AUC:\n","metadata":{"_uuid":"b22a93999499011bf216f0e478c0394bb8eb07f8","_cell_guid":"ddcc5c35-36d0-4846-bb1e-8918dff67182"}},{"cell_type":"markdown","source":"[www.dataschool.io](http://www.dataschool.io/roc-curves-and-auc-explained/)","metadata":{"_uuid":"757d7641c87120d3d6f2c4af8d63bee3fdd8ffbd","_cell_guid":"6c8e8947-45fe-43ba-a829-23c82087970b"}},{"cell_type":"markdown","source":"![](https://i.imgur.com/Te4Iaye.png)","metadata":{"_uuid":"8807b1904f6edcb2784910fd64098a60c5a700f3","_cell_guid":"180ad57d-7cf2-41a5-b4c8-f53a9be9f616"}},{"execution_count":null,"cell_type":"code","outputs":[],"source":"from sklearn.metrics import roc_curve, auc\nn_classes = len(class_names)\nfrom sklearn.preprocessing import label_binarize\n\n# Binarize the output\nY_valid = label_binarize(Y_valid, classes=[0, 1, 2])\n\n# Compute ROC curve and ROC area for each class\nfpr     = dict()\ntpr     = dict()\nroc_auc = dict()\n\nplt.figure(1)\nplt.plot([0, 1], [0, 1], 'k--')\n\n\nfor i in range(n_classes):\n    fpr[i], tpr[i], _ = roc_curve(  Y_valid[:,i] , predictions[:, i] )\n    roc_auc[i] = auc(fpr[i], tpr[i])\n    plt.plot(fpr[i], tpr[i], label=class_names[i] + 'ROC curve (area = %0.2f)' % roc_auc[i])\n\nprint('EAP ROC curve (area = %0.2f)' % roc_auc[0])\nprint('HPL ROC curve (area = %0.2f)' % roc_auc[1])\nprint('MWS ROC curve (area = %0.2f)' % roc_auc[2])\n\nplt.xlabel('False positive rate')\nplt.ylabel('True positive rate')\nplt.title('ROC curve')\nplt.legend(loc=\"lower right\")\nplt.show()\n","metadata":{"_uuid":"946eddca0857ce3ce346f87d2880136e3507b971","_kg_hide-output":true,"collapsed":true,"_cell_guid":"30ec2ac1-e8cf-4de3-955c-6a6541ce307a"}}],"metadata":{"language_info":{"name":"python","mimetype":"text/x-python","nbconvert_exporter":"python","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"version":"3.6.3","pygments_lexer":"ipython3"},"kernelspec":{"display_name":"Python 3","name":"python3","language":"python"}}}