{"nbformat":4,"nbformat_minor":1,"cells":[{"cell_type":"markdown","source":"# Spooky Author Identification\n_____\n\n![cat eyes](http://www.sciencealert.com/images/articles/processed/catseyes_1024.jpg)","metadata":{"_cell_guid":"3009d60d-d118-48c9-8e16-9a80a074f73e","_uuid":"4c0fafc10c904ddad08c5cf2edcf55226dd953a2"}},{"cell_type":"markdown","source":"## Introduction\n___\nIn this year's Kaggle Halloween playground competition, we are being challenged to predict the author of excerpts from horror stories by Edgar Allan Poe, Mary Shelley, and HP Lovecraft.\n\n## Approach\n___\nUsing libraries such as [string](https://docs.python.org/2/library/string.html), [re](https://docs.python.org/2/library/re.html) and [nltk](http://www.nltk.org), we begin by cleaning the corpus - removing punctuation marks and numbers, converting all letters to lowercase and removing stopwords. \n\nWith the help of visualisation libraries such as [matplotlib](https://matplotlib.org), [seaborn](http://seaborn.pydata.org) and [wordcloud](https://github.com/amueller/word_cloud), we are able to identify the most frequent terms used by the authors in their writing.\n\nWe will also use [nltk's inbuilt Part-of-Speech Tagging function](http://www.nltk.org/book/ch05.html) to classify the words into their parts of speech. The reason for doing so is because we might expect authors to use specific tags relatively more than their counterparts. If this is the case, then the POS tags will be a useful tool to identify which author wrote the specific sentence.\n\nWe use [sklearn's](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) `CountVectorizer`  to convert the cleaned corpus into a matrix, with each row being a particular document, and each column a particular term. The term counts (how frequent a term appears in the corpus) for a particular document will be the corresponding in the value in the dataframe. Next, we use [sklearn's](http://scikit-learn.org/dev/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) `TfidfVectorizer` to account for the importance of a particular word term - intutively, words that occur frequently in a document (think 'Messi' in a soccer article) but do not frequently occur in the corpus itself tends to be an important term. On the other hand, words that frequently occur in all documents (think 'and', 'if', 'the') are deemed to have relative low importance.\n\nFollowing the generation of the term frequency dataframe, we conducting Topic Modelling using [gensim](https://radimrehurek.com/gensim/index.html). By identifying the underlying topics within the corpus, we were able to identify the 'hidden' topics within the corpus. Following the identification of such topics, we then allocate topics to each document depending on the terms present in the document.\n\nFinally, we pulled all our features together, and used a simple [Logistic Regression model](https://en.wikipedia.org/wiki/Logistic_regression) using [sklearn's GridSearchCV](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) to search for the best model by varying the regularisation term, C. On the holdout testing dataset, the preliminary model were able to obtain a testing score of 0.445 (as we are using the metric of logarithmic loss, lower is better). After which, we re-fitted the model, using the best parameters we found (C=10), using the whole training dataset.\n\n## Evaluation Metric\n___\nSimilar to Kaggle's evaluation metric, we will use the [multi-class logarithmic loss](https://www.kaggle.com/c/spooky-author-identification#evaluation).\n\n\n## Afternote\n___\nOur model scored 0.42827. The score was good enough to place us at 214 of 584 teams (37th percentile). As it turns out, a simple logistic regression model with good features will outperform a sophisticated machine learning algorithm with poor features. \n\nIn the event that you found this useful, please visit the other kernels that were instrumental in helping me formulate hypotheses, generate new features and challenging some of my beliefs:\n\n1. [Abhishek - Approaching (Almost) Any NLP Problem on Kaggle](https://www.kaggle.com/abhishek/approaching-almost-any-nlp-problem-on-kaggle)\n2. [Anisotropic - Spooky NLP and Topic Modelling tutorial](https://www.kaggle.com/arthurtok/spooky-nlp-and-topic-modelling-tutorial)\n3. [Heads or Tails - Treemap House of Horror: Spooky EDA/LDA/Features](https://www.kaggle.com/headsortails/treemap-house-of-horror-spooky-eda-lda-features)","metadata":{"_cell_guid":"0111fcc1-088e-40cc-8901-ebdf6f7c7c8a","_uuid":"16c3616b604be8c764a43bac6ba2749e7e9d5e8f"}},{"cell_type":"markdown","source":"### Importing key libraries and reading dataframe\n\nTo facilitate data processing and cleaning, we will import the following libraries:\n\n* [pandas](http://pandas.pydata.org) - pandas is an open source, BSD-licensed library providing high-performance, easy-to-use data structures and data analysis tools for the Python programming language.\n\n* [numpy](http://www.numpy.org) - NumPy is the fundamental package for scientific computing with Python.\n\n* [nltk](http://www.nltk.org) - NLTK is a leading platform for building Python programs to work with human language data. It provides easy-to-use interfaces to over 50 corpora and lexical resources such as WordNet, along with a suite of text processing libraries for classification, tokenization, stemming, tagging, parsing, and semantic reasoning, wrappers for industrial-strength NLP libraries, and an active discussion forum.\n\n* [re](https://docs.python.org/2/library/re.html) - This module provides regular expression matching operations similar to those found in Perl. Both patterns and strings to be searched can be Unicode strings as well as 8-bit strings.\n\n* [Matplotlib](https://matplotlib.org) - Matplotlib is a Python 2D plotting library which produces publication quality figures in a variety of hardcopy formats and interactive environments across platforms. Matplotlib can be used in Python scripts, the Python and IPython shell, the jupyter notebook, web application servers, and four graphical user interface toolkits.\n\n* [seaborn](http://seaborn.pydata.org) - Seaborn is a Python visualization library based on matplotlib. It provides a high-level interface for drawing attractive statistical graphics.\n\n* [wordcloud](https://github.com/amueller/word_cloud) - Wordcloud is a little word cloud generator in Python.","metadata":{"_cell_guid":"110d0e02-5b33-4601-a7fe-0f2b9490af7f","_uuid":"1f44b1ecf1a909001bd6b861fa1e2d97f5f9c46a"}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport nltk\nimport re\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom wordcloud import WordCloud","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"63e64a75-2569-4145-a4b7-d73cccc49c79","collapsed":true,"_uuid":"0b0eade1f34ad1b9efccfdaf5900db0ab8086205"}},{"cell_type":"code","source":"df_train = pd.read_csv('../input/train.csv')\ndf_test = pd.read_csv('../input/test.csv')","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"9e222cd4-2a06-494b-b420-401ff0e64f9d","collapsed":true,"_uuid":"0d82a6e80b3aa79f1b9c5dea2b80800eb3696f50"}},{"cell_type":"markdown","source":"Let's combine the two dataframes.","metadata":{"_cell_guid":"ebf8513e-14e7-47f4-8375-4e15b0096e7d","_uuid":"03d804841d2aea4d8254bd5e6e4dffb85d778ad3"}},{"cell_type":"code","source":"combined = pd.concat([df_train, df_test]).reset_index(drop=True)","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"b8d0bd80-276d-4b6d-b4ec-dd5ba8e04a0e","collapsed":true,"_uuid":"b03932bbc08c69796cda826b1ea2dbdb26a0350a"}},{"cell_type":"markdown","source":"### Exploratory Data Analysis\n\nAfter reading the dataframes, let's take a look at the first 5 rows of the combined dataframe!","metadata":{"_cell_guid":"afa6f9c7-790c-4d98-b26f-2dadecc4cdac","_uuid":"8c1f85e6a401efb6212a9a4dcd94376e6afc2a33"}},{"cell_type":"code","source":"combined.head()","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"09235cd8-53f1-4cfb-bb87-c980f183a777","collapsed":true,"_uuid":"bcae839d0f5b210a6e6eec4b25748df62871a280"}},{"cell_type":"markdown","source":"In this instance, we have 3 columns:\n\n* author - This is our target label: we are supposed to predict the author from the text.\n* id - This is probably a unique identifier that has no correlation with the author.\n* text - This is our independent variable/feature to predict the target label, author.","metadata":{"_cell_guid":"3a355561-66ae-41d1-9d8a-09f4069cead2","collapsed":true,"_uuid":"f47a03b7c24004a483aab8793d92b63ea07955a6"}},{"cell_type":"markdown","source":"What is the dimension of the dataframe?","metadata":{"_cell_guid":"c3952745-ea8a-41b8-86b0-14fdf09c5762","_uuid":"aa76037bf18b34b0e302aa27fd54285bef31ea0c"}},{"cell_type":"code","source":"print('The training dataset has %d rows and %d columns' % (df_train.shape[0], df_train.shape[1]))\nprint('The testing dataset has %d rows and %d columns' % (df_test.shape[0], df_test.shape[1]))\nprint('The combined dataset has %d rows and %d columns' % (combined.shape[0], combined.shape[1]))","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"15cc59da-8e1b-40c2-ab5c-526caf63a692","collapsed":true,"_uuid":"c0212871945e5c9fca34e10ebaed4aabc82dab1e"}},{"cell_type":"code","source":"np.sum(pd.isnull(combined))","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"07577f29-3dcf-45bf-a2f4-d95a16029c75","collapsed":true,"_uuid":"33e91c2b5e688d5f72a0681d32b31947dd80a9c9"}},{"cell_type":"markdown","source":"It appears that there are no NA values in the id and text columns. Also, we note that there are 8,392 missing author values, similar to the number of rows in the testing dataset. Nothing seems to be too alarming here.","metadata":{"_cell_guid":"79c39371-3128-4b78-84ce-a96bb93bc90c","_uuid":"ce75709e7d6a4083e37a9bfcd70222cb68fc953d"}},{"cell_type":"markdown","source":"How many unique authors are there in the dataset, and how does the author distribution look like?","metadata":{"_cell_guid":"99e961bc-a528-4f47-b9cb-89b86be10a55","_uuid":"24e289f6d12292daa3e106cb70e8165a84be6add"}},{"cell_type":"code","source":"combined.author.value_counts()","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"97563ecb-44e5-4f9e-baf0-0ba7a040277d","collapsed":true,"_uuid":"e6e4bac4f7bd384b73cbf2d4ae3321b6dea0bc7b"}},{"cell_type":"markdown","source":"It turns out that there are 3 authors in our dataset, and the author distribution looks to be **quite** uniform.\n\nThe authors in the dataset are respectively:\n\n1. **EAP** - Edgar Allen Poe\n2. **MWS** - Mary Shelley\n3. **HPL** - H.P. Lovecraft","metadata":{"_cell_guid":"2aea1af9-9747-455a-88dc-aa0ecdb5298b","_uuid":"7aedaae5af4079ab818acab49beebcb4050d1a82"}},{"cell_type":"markdown","source":"#### Most Common Words\n\nWhat are the most common words used by the author in this corpus? Let's find out.","metadata":{"_cell_guid":"e6ceb915-f7df-4e85-98f9-ad61a386ffe2","_uuid":"943f90fb034e6221ce38d278fbd0b54628fa6c8a"}},{"cell_type":"code","source":"import string\nimport operator\nfrom collections import OrderedDict\nsns.set(font_scale=1.25)\n\ndef top_20_words(author):\n    # Return a cleaned series of lists of words\n    common_words_df = (combined[combined['author'] == author].text\n                       .apply(lambda x: ''.join([word for word in x if word not in string.punctuation]))\n                       .str.lower()\n                       .str.split(' '))\n    \n    # Returns a dictionary where key = words and values = word counts\n    dict_of_word_count = {}\n    for text in common_words_df:\n        for word in text:\n            dict_of_word_count[word] = dict_of_word_count.get(word, 0) + 1\n\n    return sorted(dict_of_word_count.items(), key=operator.itemgetter(1), reverse=True)[:20]\n\ndef plot_top_20_words(author):\n    plt.figure(figsize=(20, 12))\n    topwords = top_20_words(author)\n    \n    words, freq = list(zip(*topwords))[0], list(zip(*topwords))[1]\n    \n    x_pos = np.arange(len(words)) \n    \n    sns.barplot(x_pos, freq)\n    plt.xticks(x_pos, words)\n    plt.title('Top 20 words of: ' + author)\n    plt.show()","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"b138b4b7-02a6-4d22-9fbb-c31358ae5318","collapsed":true,"_uuid":"9b37d732b7ff37a0da859aa1cd54df7352b959b0"}},{"cell_type":"markdown","source":"##### EAP","metadata":{"_cell_guid":"0febdd87-8981-4fd2-ad86-017e9ae50588","_uuid":"6e430628545aa64dc993ace39b53caab775b9797"}},{"cell_type":"code","source":"plot_top_20_words('EAP')","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"481e9408-2f4d-4755-ae6f-1f7e38a072ba","collapsed":true,"_uuid":"cc08a20c5066d3246d4694256d7a636c939d6795"}},{"cell_type":"code","source":"wordcloud = WordCloud().generate(str(combined[combined.author=='EAP'].text.tolist()))\n\nplt.figure(figsize=(20, 15))\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis(\"off\")\nplt.show()","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"37136b47-a91f-4f62-b4c2-a037ebd3fdfd","collapsed":true,"_uuid":"f09a82091a2deaa050a75e99f297ba8309eafdb9"}},{"cell_type":"markdown","source":"##### HPL","metadata":{"_cell_guid":"3a70a01e-b0b5-496c-8130-ca340fe4e653","_uuid":"b7100382f56188d78dd629d7b84ece8c2286f5a4"}},{"cell_type":"code","source":"plot_top_20_words('HPL')","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"3b07f194-f0b8-4d6b-a5c0-610e5293a541","collapsed":true,"_uuid":"3581641145b444ee152d8259aacefd99887a2821"}},{"cell_type":"code","source":"wordcloud = WordCloud().generate(str(combined[combined.author=='HPL'].text.tolist()))\n\nplt.figure(figsize=(20, 15))\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis(\"off\")\nplt.show()","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"8d8e95cd-3bcf-44a0-8f5a-a50a433e2189","collapsed":true,"_uuid":"4307917922c92a7dacd657342b500d11b17a4b08"}},{"cell_type":"markdown","source":"##### MWS","metadata":{"_cell_guid":"b728bbcb-9968-4f77-8a9d-a2e33c7c71b6","_uuid":"f3db7f13808fc92f42c5282d80639cad536d33bd"}},{"cell_type":"code","source":"plot_top_20_words('MWS')","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"98f6e736-bd3d-42c5-abda-4d93e6331cab","collapsed":true,"_uuid":"5f1ea3d8d014957575193e30a8d0393a10e75c24"}},{"cell_type":"code","source":"wordcloud = WordCloud().generate(str(combined[combined.author=='MWS'].text.tolist()))\n\nplt.figure(figsize=(20, 15))\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis(\"off\")\nplt.show()","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"3cbde996-82b7-44d9-ab63-2151a58ae92a","collapsed":true,"_uuid":"842722452e966c60dd15da82d5d803c1bba30eb2"}},{"cell_type":"markdown","source":"Judging by the top 20 most common words used by the 3 authors, it appears that they are very similar. Words such as 'the', 'and', 'of', and 'to' and 'a' tend to come up very frequently. It turns out that simply using the most frequency words alone will not help in differentiating across the 3 different authors.","metadata":{"_cell_guid":"bcdc1f32-581e-4e39-be2e-f05f35db4262","_uuid":"33669b1dc163309b03ebbf89a550ef59841f393f"}},{"cell_type":"markdown","source":"### Feature Engineering with re\n\nBefore using the Natural Language ToolKit (NLTK), let's think about some features we can use to predict the author which wrote the text. Here are some examples of features we can generate:\n\n1. Length of sentence - If we believe certain authors tend to be more verbose, then the length of sentence can help us identify these authors better.\n2. Number of words - Similar to the length of sentence, this captures the author's verbosity.\n3. Number of punctuation marks - If some authors are more likely to use exclaimation marks (!) or apostrophes (') in general, then the number of punctuation marks will be a good feature that we can consider.\n4. Number of capital letters - Some authors might use more capital letters in their writing.\n5. Average word length - Some authors tend to use longer words compared to their counterparts.","metadata":{"_cell_guid":"4c335bd7-0926-441e-8efd-6ea4cd060615","_uuid":"fb82a19eaf89fcb6c554bc3c750401d8e8a35a13"}},{"cell_type":"code","source":"combined['sent_length'] = combined.text.apply(lambda x: len(x))","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"9bbeab8f-bee0-4b67-acd9-3f1edbc44445","collapsed":true,"_uuid":"386e8367e4707be222622b8552f9e067a02b2cbb"}},{"cell_type":"code","source":"def word_count(text):\n    return len(''.join([word.lower() for word in text if word not in string.punctuation]).split(' '))\n\ncombined['word_length'] = combined.text.apply(word_count)","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"e18bc1ef-4dc4-4882-aa16-910337740e6f","collapsed":true,"_uuid":"cbb66f6bbe95a4d923143ceabf4285b51b425527"}},{"cell_type":"code","source":"combined['punc_marks'] = (combined\n                          .text\n                          .apply(lambda x: \n                                 len(re.findall('[!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~]', x))) /\n                          combined.sent_length)","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"a22102d2-2e03-4d95-835e-9247ccf2b0f5","collapsed":true,"_uuid":"6b71234cd5ca5ac515cf9d320b73c294b7c8b3bb"}},{"cell_type":"code","source":"combined['cap_letter'] = (combined\n                          .text\n                          .apply(lambda x: \n                                 len(re.findall('[A-Z]', x))) /\n                          combined.sent_length)","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"ed4a3598-23a5-4d5d-9ebe-62c2614b9fbc","collapsed":true,"_uuid":"cd9a1bd9d9d21fab0956d52da05010d08eb89fd9"}},{"cell_type":"code","source":"combined['avg_word_length'] = combined.sent_length / combined.word_length","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"6cdea9da-4fe1-453c-9275-25b918f262a9","collapsed":true,"_uuid":"38d99092465c2f587f75bfbf7a071995538f4f66"}},{"cell_type":"markdown","source":"After the creation of these features, let's take a look at their distributions according to the 3 authors!","metadata":{"_cell_guid":"daf777da-2df5-4e77-aea1-3a29adec3067","_uuid":"bc441f67dfa21c73d7deccf7b3371bfbb41c2852"}},{"cell_type":"markdown","source":"##### Sentence Length","metadata":{"_cell_guid":"3b5fd384-fc29-425e-83aa-016fca7607c7","_uuid":"471e1447911592c839221040953a0b0b09f95cf0"}},{"cell_type":"code","source":"plt.figure(figsize=(20, 12))\n\nsns.distplot(combined[combined.author == 'EAP'].sent_length, color = 'salmon', \n             bins=np.linspace(0, 1000, 101), kde=False, norm_hist=True, label = 'EAP')\nsns.distplot(combined[combined.author == 'HPL'].sent_length, color = 'steelblue', \n             bins=np.linspace(0, 1000, 101), kde=False, norm_hist=True, label = 'HPL')\nsns.distplot(combined[combined.author == 'MWS'].sent_length, color = 'seagreen', \n             bins=np.linspace(0, 1000, 101), kde=False, norm_hist=True, label = 'MWS')\n\nplt.title('Sentence Length')\nplt.legend()\nplt.show()","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"7ca1bab6-5ead-4335-b1b6-71855dcad120","collapsed":true,"_uuid":"1c4c7ff813e98fd034dd4694aadf4d6ed226ea0b"}},{"cell_type":"markdown","source":"On average, the 3 authors tend to write sentences of equal length (about 200 - 300 characters). We do note that the author, Lovecraft writes relatively long sentences, while Poe writes relatively short sentences. ","metadata":{"_cell_guid":"999286f4-8a81-4e72-b6eb-85486cce8a95","_uuid":"47d29df60db6a98130b0abf331cd5f18984c2137"}},{"cell_type":"markdown","source":"##### Word Length","metadata":{"_cell_guid":"be84b224-37ef-4a04-a070-52fcce065759","_uuid":"a66412569c4568caff5ad8ff4da47c543262d9d8"}},{"cell_type":"code","source":"plt.figure(figsize=(20, 12))\n\nsns.distplot(combined[combined.author == 'EAP'].word_length, color = 'salmon',\n             bins=np.linspace(0, 200, 101), kde=False, norm_hist=True, label = 'EAP')\nsns.distplot(combined[combined.author == 'HPL'].word_length, color = 'steelblue', \n             bins=np.linspace(0, 200, 101), kde=False, norm_hist=True, label = 'HPL')\nsns.distplot(combined[combined.author == 'MWS'].word_length, color = 'seagreen', \n             bins=np.linspace(0, 200, 101), kde=False, norm_hist=True, label = 'MWS')\n\nplt.title('Word Length')\nplt.legend()\nplt.show()","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"ff16dd75-d6f3-41c8-80ea-e1cd140c5c6c","collapsed":true,"_uuid":"bc0f591f271acd848513286044626cece8ad7ff4"}},{"cell_type":"markdown","source":"The word length feature appears to be strongly correlated with the sentence length feature. This isn't surprisingly at all. (**Why?**)","metadata":{"_cell_guid":"fa6e243c-ecd8-4cbc-be95-d65cbea90895","_uuid":"c9f48bf608f7d04ae54c0f1768ef27234a60c6ae"}},{"cell_type":"markdown","source":"##### Number of Punctuations","metadata":{"_cell_guid":"b879ff9d-64bb-4a8c-b3cf-8a128000c114","_uuid":"7587d2a61c5cd376b4ec46cf9e14e0976dca17d1"}},{"cell_type":"code","source":"plt.figure(figsize=(20, 12))\n\nsns.distplot(combined[combined.author == 'EAP'].punc_marks, color='salmon', label = 'EAP',\n             bins = np.linspace(0, 0.2, 101), kde=False, norm_hist=True)\nsns.distplot(combined[combined.author == 'HPL'].punc_marks, color='steelblue', label = 'HPL',\n             bins = np.linspace(0, 0.2, 101), kde=False, norm_hist=True)\nsns.distplot(combined[combined.author == 'MWS'].punc_marks, color='seagreen', label = 'MWS',\n             bins = np.linspace(0, 0.2, 101), kde=False, norm_hist=True)\nplt.title(\"Average Number of Punctuation Marks Used\")\nplt.legend()\n\nplt.show()","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"ff4167e4-9513-45e6-8f6d-51e26c0ee935","collapsed":true,"_uuid":"a85b3a4371208142a2dcf6fb325a0c55c723b9eb"}},{"cell_type":"markdown","source":"As it turns out, the number of punctuation marks used appears to be quite good at separating between the 3 authors. We note that the Poe and Shelley tend to use more punctuation marks in their sentence (controlled for the number of words), compared to Lovecraft.","metadata":{"_cell_guid":"021b62e2-35e2-4ba3-8ec1-6ebb8c013b51","_uuid":"154b53cd646d442ae10790d239aaae7b4a1b0871"}},{"cell_type":"markdown","source":"##### Number of Capital Letters","metadata":{"_cell_guid":"10d0095b-6107-41c8-8c77-e3a9ec14cde0","_uuid":"1fa80862ff09476acd607e3928c30bc16a4a20f8"}},{"cell_type":"code","source":"plt.figure(figsize=(20, 12))\n\nsns.distplot(combined[combined.author == 'EAP'].cap_letter, color='salmon', label = 'EAP',\n             bins = np.linspace(0, 0.2, 101), kde=False, norm_hist=True)\nsns.distplot(combined[combined.author == 'HPL'].cap_letter, color='steelblue', label = 'HPL',\n             bins = np.linspace(0, 0.2, 101), kde=False, norm_hist=True)\nsns.distplot(combined[combined.author == 'MWS'].cap_letter, color='seagreen', label = 'MWS',\n             bins = np.linspace(0, 0.2, 101), kde=False, norm_hist=True)\nplt.title(\"Average Number of Capital Letters Used\")\nplt.xlabel('Average Number of Capital Letters')\nplt.legend()\n\nplt.show()","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"9d3a4f55-19a0-426a-ba43-6b7311ed721f","collapsed":true,"_uuid":"100747d000469e1c999886f35931694b8bbaff84"}},{"cell_type":"markdown","source":"On average, the number of capital letters used in a sentence doesn't seem to be terribly informative of the target label. We do note that on average, all 3 authors use similar amounts of capital letters.","metadata":{"_cell_guid":"906e1c58-8443-432f-a57a-96acd0dd8443","_uuid":"7213ae0658e387d6a23adcaa58c3b6fd9f99853e"}},{"cell_type":"markdown","source":"##### Average Word Length","metadata":{"_cell_guid":"fb077189-697b-4ccd-b4c6-bf1c97fb2c16","_uuid":"1b51a825cb2cfbe999b7f6c13879087bcc61bf61"}},{"cell_type":"code","source":"plt.figure(figsize=(20, 12))\n\nsns.distplot(combined[combined.author == 'EAP'].avg_word_length, color='salmon', label = 'EAP',\n             bins = np.linspace(0, 12, 61), kde=False, norm_hist=True)\nsns.distplot(combined[combined.author == 'HPL'].avg_word_length, color='steelblue', label = 'HPL',\n             bins = np.linspace(0, 12, 61), kde=False, norm_hist=True)\nsns.distplot(combined[combined.author == 'MWS'].avg_word_length, color='seagreen', label = 'MWS',\n             bins = np.linspace(0, 12, 61), kde=False, norm_hist=True)\nplt.title(\"Average Word Length\")\nplt.xlabel('Average Word Length')\nplt.legend()\n\nplt.show()","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"646d18cc-7f63-4d3c-b071-099ad99159e7","collapsed":true,"_uuid":"b1034c3bef331e019120cfa29811c80aadb49fed"}},{"cell_type":"markdown","source":"On average, we note that the average word length across the 3 authors don't seem to deviate too much. Most of the words fall between 4 to 8 characters.","metadata":{"_cell_guid":"143d4584-3caa-45f0-85f6-a6c32be02227","collapsed":true,"_uuid":"d3094e1703f131b46ff9202b55a27c6b2e99c72e"}},{"cell_type":"markdown","source":"### Feature Engineering with NLTK\n\nLet's use the Natural Language Toolkit library in Python to generate more features!","metadata":{"_cell_guid":"f20419cb-490c-494a-9e70-794b94a23d8a","_uuid":"4c630d051eb5edfcfb362234aaea427c7ac1432f"}},{"cell_type":"markdown","source":"#### Stopwords Stemming\n\nFirst, we remove punctuations from the text column of our combined dataframe, and convert capital letters into small letters. After which, we remove stopwords from the cleaned text and *lemmatize* the cleaned text using NLTK's inbuilt `WordNetLemmatizer`. In addition, we will also conduct word stemming using NLTK's inbuilt `PorterStemmer`.","metadata":{"_cell_guid":"e43539c1-4eb9-4059-8201-14b0667def53","_uuid":"236737dc6a60b2dd0842bf26f317ea4eebebf98c"}},{"cell_type":"code","source":"# Cleaning text - removing punctuation, and converting capital letters to small letters\ndef list_of_words(text):\n    return ''.join([word.lower() for word in text if word not in string.punctuation]).split(' ')\n\nwordlist = combined.text.apply(list_of_words)","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"0ad24dac-e9d0-44b5-934e-db0f36f1eb0e","collapsed":true,"_uuid":"64135b62c76f7bc52da6ab8041b74adb0f9133ea"}},{"cell_type":"code","source":"# Cleaning text - removing non-alphanumeric characters\ndef remove_spaces(text):\n    return ' '.join([re.sub('[^a-zA-Z0-9]', ' ', word) for word in text]).split(' ')\n\nwordlist = wordlist.apply(remove_spaces)","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"3ba75f4a-67c5-45fa-b7c0-bde86cb897d8","collapsed":true,"_uuid":"5e86812223fa420af38ae31a1b7d844d8e2985ca"}},{"cell_type":"code","source":"# Removing stopwords from the list of words - warning: takes a long time \nfrom nltk.corpus import stopwords\n\ndef list_of_nonstopwords(text):\n    return [word for word in text if word not in stopwords.words('english')]\n\nnonstopword_list = wordlist.apply(list_of_nonstopwords)","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"1e517ffc-f7ab-464f-ba45-03c885d1ba9d","collapsed":true,"_uuid":"65a4b9794dcb22922cee978fd449154779e70c1b"}},{"cell_type":"code","source":"# Lemmatizing the list of non-stop-words\nfrom nltk.stem.wordnet import WordNetLemmatizer\nlemmatizer = WordNetLemmatizer()\n\ndef lemmatized_words(text):\n    return ' '.join([lemmatizer.lemmatize(word) for word in text])\n\ncombined['lemmatized_words'] = nonstopword_list.apply(lemmatized_words)","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"4f5561bd-658c-4b73-bf5b-fb3dfc329979","collapsed":true,"_uuid":"32f1cd0126ca83658a48cc40034ff1c21dcc3633"}},{"cell_type":"code","source":"# Stemming the list of non-stop-words\nfrom nltk.stem import PorterStemmer\nstemmer = PorterStemmer()\n\ndef stemmed_words(text):\n    return ' '.join([stemmer.stem(word) for word in text])\n\ncombined['stemmed_words'] = nonstopword_list.apply(stemmed_words)","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"54b90c3e-317a-4098-b819-b7023e4a5b54","collapsed":true,"_uuid":"e0fc386878b25bb023c4d53d4e54636144cc06e3"}},{"cell_type":"markdown","source":"Let's take a look at the first 5 rows of our dataframe.","metadata":{"_cell_guid":"322fbbba-ad7e-4dbf-ac70-7a5652ce1f08","_uuid":"cd68d99cb90e7ad0dd5395a89e6c73d9514ec1cd"}},{"cell_type":"code","source":"combined.head()","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"d09274eb-aebb-45ae-870a-7ee9bd7b190b","collapsed":true,"_uuid":"3798fc233efc65799769260cbeffbfec8f2d21d6"}},{"cell_type":"markdown","source":"#### Most Common Lemmatized Words\n\nAfter using WordNetLemmatizer to lemmatize the text in the combined dataframe, let's take a look at the most common lemmatized terms.","metadata":{"_cell_guid":"d98d0de3-501d-47b8-82f1-d418411f6ac1","_uuid":"03ca062152946d1f73db7091bc5d561cc1466d89"}},{"cell_type":"code","source":"def top_20_words_lemmatized(author):\n    # Return a cleaned series of lists of words\n    author_stemmed = (combined[combined.author == author].lemmatized_words\n                      .apply(lambda text: ''.join([word for word in text])).str.split(' '))\n\n    # Returns a dictionary where key = words and values = word counts\n    dict_of_word_count = {}\n    for text in author_stemmed:\n        for word in text: dict_of_word_count[word] = dict_of_word_count.get(word, 0) + 1\n\n    return sorted(dict_of_word_count.items(), key=operator.itemgetter(1), reverse=True)[:20]\n\ndef plot_top_20_words_lemmatized(author):\n    plt.figure(figsize=(20, 12))\n    topwords = top_20_words_lemmatized(author)\n    \n    words, freq = list(zip(*topwords))[0], list(zip(*topwords))[1]\n    \n    x_pos = np.arange(len(words)) \n    sns.barplot(x_pos, freq)\n    plt.xticks(x_pos, words)\n    plt.title('Top 20 lemmatized_words terms of: ' + author)\n    plt.show()","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"86a6e121-8a22-4dd9-b1df-dcb1f89c778e","collapsed":true,"_uuid":"bd7e66bdd547b74c95beea564994c93e701d1139"}},{"cell_type":"code","source":"# Plotting top lemmatized words for Edgar Allen Poe\nplot_top_20_words_lemmatized('EAP')","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"4ac5c665-236a-4c6f-ab20-545d0d345aa8","collapsed":true,"_uuid":"dcec7d15b9438ed55a14572451a8154ab39594b1"}},{"cell_type":"code","source":"wordcloud = WordCloud().generate(str(combined[combined.author=='EAP'].lemmatized_words.tolist()))\n\nplt.figure(figsize=(20, 15))\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis(\"off\")\nplt.show()","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"9677d2d4-1a82-4c20-b9d7-6ee61932313a","collapsed":true,"_uuid":"3fe9467c550413450506fa8efe9251822b100a4c"}},{"cell_type":"code","source":"# Plotting top lemmatized words for H.P. Lovecraft\nplot_top_20_words_lemmatized('HPL')","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"1de908a4-2396-4666-91fa-abe036e07086","collapsed":true,"_uuid":"03e6470cb13bbb83acda69280a31521a1c99ce5f"}},{"cell_type":"code","source":"wordcloud = WordCloud().generate(str(combined[combined.author=='HPL'].lemmatized_words.tolist()))\n\nplt.figure(figsize=(20, 15))\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis(\"off\")\nplt.show()","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"5f4c00a4-3e24-48eb-ae2d-f76394fdf91f","collapsed":true,"_uuid":"7d40e10b09ea09a8dc97f2c00a9d1cc3eba18f64"}},{"cell_type":"code","source":"# Plotting top lemmatized words for Mary Shelley\nplot_top_20_words_lemmatized('MWS')","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"a9ae2c37-2e8b-459b-bd6f-f73072bb2068","collapsed":true,"_uuid":"e3eb4ce2b6b8d6446cc0bee5290d620e09e6a34a"}},{"cell_type":"code","source":"wordcloud = WordCloud().generate(str(combined[combined.author=='MWS'].lemmatized_words.tolist()))\n\nplt.figure(figsize=(20, 15))\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis(\"off\")\nplt.show()","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"39bc123e-f17f-4e82-bcbd-78c76dd9846e","collapsed":true,"_uuid":"9abcc8a43202bd1f94724b940381b1ccadcdb6ca"}},{"cell_type":"markdown","source":"After removing stopwords and conducting lemmatizing the cleaned dataframe on our dataframe, there are still many common words, such as 'one' and 'seemed'. As these words frequently occur in the corpus, they are not \"good\" indicators of the authors.","metadata":{"_cell_guid":"bf2a66a9-8d65-4580-9b2d-f0aa4782de71","_uuid":"a25124f0daa009af104f7f7ed8945b559c012e96"}},{"cell_type":"markdown","source":"#### Part-of-Speech Tagging\n\nAccording to NLTK, part-of-speech tagging, or POS-tagging is the process of classifying words into their parts of speech and labeling them accordingly. Parts of speech are also known as word classes or lexical categories. The collection of tags used for a particular task is known as a tagset.\n\nFor our analysis, we can look at the number of nouns, verbs and adjectives in each sentence, and potentially use them as features. Intuitively, the writing style differs from authors to authors. We would expect certain authors to use more nouns, verbs or adjectives relative to other authors. Let's make use of this observation to create more features.","metadata":{"_cell_guid":"d260de25-8e1f-49de-ae17-816c14724756","_uuid":"27aa712cae5c15ac54bd280f985ce48d2f154bca"}},{"cell_type":"code","source":"# Conduct POS Tagging (takes a bit of time to run this code)\npos_tags = (combined.text.apply(lambda text: nltk.pos_tag(nltk.word_tokenize(text))))","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"695eeeb6-75ef-4d11-a8e4-70be5dd878e2","collapsed":true,"_uuid":"45e361a8052b6f3142616e324d3f2a8c211943db"}},{"cell_type":"code","source":"def pos_tag_count(list_of_postag):\n    # Return dictionary of dataframes with postags as keys and counts as values\n    dict_of_postags = {}\n    for tag in list_of_postag:\n        dict_of_postags[tag[1]] = dict_of_postags.get(tag[1], 0) + 1\n    return dict_of_postags\n        \npostags_df = pd.DataFrame(pos_tags.apply(pos_tag_count).to_dict()).T","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"23c239f7-892f-4e77-ab4f-dbaabac37d4e","collapsed":true,"_uuid":"0ef9f307350e7e5024b02f6fe6d504644b595c91"}},{"cell_type":"markdown","source":"Let's remove the punctuations from the POS Tagging dataframe, as they should not be very informative of the target label.","metadata":{"_cell_guid":"71c96242-d55b-4ade-a732-1fe5eb707bd5","_uuid":"ca92c0ff14299985f89fa87929929ca4767f8c05"}},{"cell_type":"code","source":"pos_tag_col = [col for col in postags_df.columns if re.findall('[A-Z]+', col)]\n\npostags_df_ = postags_df[pos_tag_col].fillna(0)","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"f56a919a-7d39-4ed4-9501-65c5d82e1598","collapsed":true,"_uuid":"aedeb15886a0ba7a74853b147773e734d27a2692"}},{"cell_type":"markdown","source":"To check which tags are most indicative of the target label, we can use a correlation matrix to estimate their importance.","metadata":{"_cell_guid":"9c1e9d25-8c9b-4da6-808c-6bc6f77bd7f7","_uuid":"bfafe8bbe257491af163ed42edecca7cb8336697"}},{"cell_type":"code","source":"postags_df_['EAP'] = pd.get_dummies(df_train.author).EAP\npostags_df_['HPL'] = pd.get_dummies(df_train.author).HPL\npostags_df_['MWS'] = pd.get_dummies(df_train.author).MWS\n\nsns.set(font_scale=1)\nplt.figure(figsize=(20,12))\nsns.heatmap(postags_df_.corr())\nplt.show()","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"6ea1f9a4-3464-4546-a84a-fb5b9313bb5f","collapsed":true,"_uuid":"382983152e9fa904cbda4e4913065cdddce7d252"}},{"cell_type":"markdown","source":"Let's include these features in our model!","metadata":{"_cell_guid":"78b5ba3e-c81a-4528-8ad5-143210b6b9e6","_uuid":"04db658f9b88cf18d14870f92d4e4f22ea466741"}},{"cell_type":"code","source":"del postags_df_['EAP']\ndel postags_df_['HPL']\ndel postags_df_['MWS']\n\ncombined = pd.merge(combined, postags_df_,\n                    left_index=True, right_index=True)","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"9a4fa822-6f75-4565-9f4d-187a155d4ced","collapsed":true,"_uuid":"518d07ddf3580e20361c254758719a5ee077a45e"}},{"cell_type":"markdown","source":"### Feature Engineering with Scikit-Learn\n\nAfter we have engineered features with re and NLTK, let's try to generate more features using Scikit-Learn! We can use the CountVectorizer to find words that occur a minimum amount of times in the dataframe. In our case, we will only focus only on unigrams, bigrams and trigrams which occur at least 3 times.","metadata":{"_cell_guid":"3a85d9cf-2402-4e65-889a-e68644f68255","_uuid":"a86ad252f63ceddd1f014fdff6fd4a6470d7e81c"}},{"cell_type":"code","source":"X_train = combined.iloc[:df_train.shape[0]]\nX_test = combined.iloc[df_train.shape[0]:]","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"7ac91f05-1df9-4629-abaa-7187a8c6e097","collapsed":true,"_uuid":"32cf64d6e62463f466743afbe9bb502d1302386d"}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\n\n# Use CountVectorizor to remove stop_words, remove tokens that don't appear in at least 3 documents,\n# and remove tokens that appear in more than 10% of the documents\nvect = CountVectorizer(min_df=3, ngram_range=(1, 10))\n\ntrain_counts_transformed = vect.fit_transform(X_train.stemmed_words)\ntest_counts_transformed = vect.transform(X_test.stemmed_words)","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"63bb7b4e-54b6-4400-ad20-6cb4c96dd896","collapsed":true,"_uuid":"ddd96c193bb4697b852d476f6774e34ff9d3fcf1"}},{"cell_type":"markdown","source":"Let's take a quick look at the dimensions of our dataframe.","metadata":{"_cell_guid":"4c7f7b9a-b306-4e2a-b67f-a7ea4292b14a","_uuid":"bf6a037f3ef7849193d7a4d8beb3f51651110d88"}},{"cell_type":"code","source":"train_counts_transformed","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"72acf16b-a477-417b-b188-6ea07b3bc6aa","collapsed":true,"_uuid":"6f4a14604a8bcdc69906ae1a20134e15dac465f9"}},{"cell_type":"markdown","source":"After vectorizing the n-gram counts, let's use the TfidfTransformer to convert our bag of words! Basically, the [tf-idf](https://en.wikipedia.org/wiki/Tfâ€“idf) (term frequency-inverse document freqency) is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus. \n\nIntuitively, you would expect relatively important words to occur frequently within a specific text, but does not appear frequently across the corpus. Think of the words 'Sherlock' and 'Holmes' - these words shouldn't occur in many different texts! On the other hand, words such as 'and', 'the' and 'of' appears frequently occur in many different texts, and are unimportant.","metadata":{"_cell_guid":"1b918da5-f09a-403f-90a0-ed57322437fa","_uuid":"cec67096b0f594057f81597429efbd6c31ed21e5"}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfTransformer\n\ntfidf = TfidfTransformer(use_idf=True)\n\ntrain_tfidf = tfidf.fit_transform(train_counts_transformed)\ntest_tfidf = tfidf.transform(test_counts_transformed)","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"26991569-8d0a-409d-86e9-3b804dd92080","collapsed":true,"_uuid":"da3d4605f8f111a5c9fce5439d8591f31d496837"}},{"cell_type":"markdown","source":"As it turns out, 14701 features fit the bill. Let's merge the dataframes together.","metadata":{"_cell_guid":"71cb10be-a4fe-451a-b5f2-faea0e9f9be3","_uuid":"4e6b5fb78f759457ba653a91d0aaa0d5a9e99f46"}},{"cell_type":"code","source":"X_train_ = pd.merge(X_train, pd.DataFrame(train_tfidf.toarray()),\n                    left_index=True, right_index=True)\n\nX_test_ = pd.merge(X_test.reset_index(drop=True), pd.DataFrame(test_tfidf.toarray()), \n                   left_index=True, right_index=True)","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"43c23b74-444c-4ac0-8710-3f23bb1e88b6","collapsed":true,"_uuid":"0ba56b1f093880fceec6abc390378c89ca826cd0"}},{"cell_type":"markdown","source":"### Topic Modelling with Gensim\n\nAfter feature engineering, let's conduct topic modelling to identify potential topics in the corpus! Intuitively, certain authors tend to write about certain topics - things which are closer to their hearts. \n\nHence, the identification of such topics could potentially help to improve our prediction rates.","metadata":{"_cell_guid":"4bb0cd9f-2022-4dfa-a0c5-dd1a1569e854","_uuid":"ff11e73ca20a2a356325df86b4b7d38c0af9d130"}},{"cell_type":"code","source":"import gensim\nfrom sklearn.feature_extraction.text import CountVectorizer\n\nvect = CountVectorizer(min_df=3, ngram_range=(1,5), stop_words='english')\n\n# Fit and transform\ntext_train = vect.fit_transform(X_train.lemmatized_words)\n\n# Convert sparse matrix to gensim corpus.\ncorpus = gensim.matutils.Sparse2Corpus(text_train, documents_columns=False)\n\n# Mapping from word IDs to words (To be used in LdaModel's id2word parameter)\nid_map = dict((v, k) for k, v in vect.vocabulary_.items())","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"f8b953f8-ce35-4130-9fb4-155820a543fd","collapsed":true,"_uuid":"f05efe3abb35ddb78818664b607591b0b730fca9"}},{"cell_type":"markdown","source":"In order to carry out Topic Modelling, we have to impute a parameter, $k$, which tells gensim the number of topics in the corpus. In our case, we settle on 6 different topics. In addition, let's set a random seed to ensure that the analysis is reproducible.","metadata":{"_cell_guid":"5b7c2c82-136c-46c6-929f-ad3acea22cb2","_uuid":"f9d0d88dd527253062c15c9b053ebfbde203d73f"}},{"cell_type":"code","source":"# Use the gensim.models.ldamodel.LdaModel constructor to estimate \n# LDA model parameters on the corpus, and save to the variable `ldamodel`\n\nrandom_state = 9410\n\nldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics = 6,\n                                           id2word = id_map, passes = 6,\n                                           random_state = random_state)","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"6b6fdc72-f9c2-46e6-b0ef-d255c28e6a4a","collapsed":true,"_uuid":"f41f606bd7bacb850192a1942a8c6eaed2dba4da"}},{"cell_type":"markdown","source":"What are these topics?","metadata":{"_cell_guid":"ac4536f1-027d-4a59-a40f-27c93f34a4da","_uuid":"1f2a1ceebbf8b0697649670988d8e5ce23faa31c"}},{"cell_type":"code","source":"ldamodel.show_topics(num_topics=6)","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"a212fe28-8fb0-475d-bca1-e3409a867140","collapsed":true,"_uuid":"78e7d813d7926bbd930d411dba0b4e0edd47aa1b"}},{"cell_type":"markdown","source":"Let's write a function that returns the most likely topic, given the text.","metadata":{"_cell_guid":"f769586c-016a-4e43-a334-16cbf2fc05aa","_uuid":"51a4487c04ee5d316d50b9db997d3dcfbf3cf3c5"}},{"cell_type":"code","source":"def most_probable_topic(text):\n    \n    # Transform text into Corpus\n    X = vect.transform(text)\n    corpus = gensim.matutils.Sparse2Corpus(X, documents_columns=False)\n    \n    # Return topic distribution\n    topic_dist =  ldamodel.inference(corpus)[0]\n    \n    topics = [max(enumerate(corpus), key=operator.itemgetter(1))[0] for corpus in topic_dist]\n    \n    return topics","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"4cfa911a-59f5-4a1e-93d4-d9fe3e7ebc6a","collapsed":true,"_uuid":"67daa40669b95e45dd8e13767ea8a24ed8235592"}},{"cell_type":"markdown","source":"Now that we have obtained the topic numbers, let's convert the topics found using One-Hot Encoding. We can use pandas' inbuilt function `pd.get_dummies()` to do so.","metadata":{"_cell_guid":"01ab276a-f26a-450f-8a8b-7491555ee99a","_uuid":"0bc2f5429731a6ddec4657a257f3b35447adf606"}},{"cell_type":"code","source":"train_topics = pd.get_dummies(most_probable_topic(X_train.lemmatized_words), prefix='topic')\ntest_topics = pd.get_dummies(most_probable_topic(X_test.lemmatized_words), prefix='topic')","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"2211134d-e7a4-4453-bcda-77cbf11c48a6","collapsed":true,"_uuid":"f536709d90b731357dc198cb94a85879fc746333"}},{"cell_type":"code","source":"train_topics.head()","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"29cb91f6-2572-426e-9bd6-1d8614b5fa26","collapsed":true,"_uuid":"8860e51a04ed4b97dfca71e4cbf9b504733c5fa1"}},{"cell_type":"code","source":"X_train = pd.merge(X_train_, train_topics, left_index=True, right_index=True)\nX_test = pd.merge(X_test_, test_topics, left_index=True, right_index=True)","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"b620887c-d501-49bf-ac74-ae2dd912b2a2","collapsed":true,"_uuid":"1de2c9ee8873d7d8be66f4b4d84ab861188d8b75"}},{"cell_type":"markdown","source":"What are the dimensions of our training and testing dataframe? Let's take a look.","metadata":{"_cell_guid":"5af42182-6266-4dd8-9159-4950a26dd5a6","_uuid":"340905ca3a372c956e9300869ac84e1dc67d31fa"}},{"cell_type":"code","source":"print('Training Dimension: ', X_train.shape)\nprint('Testing Dimension: ', X_test.shape)","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"2243fdfa-fb97-4acb-b27d-5668782958a6","collapsed":true,"_uuid":"53f708bc4911a0b6a50804b3196dc2e602b09505"}},{"cell_type":"markdown","source":"#### Distribution of Topic Numbers according to the different Authors","metadata":{"_cell_guid":"2f09ee6c-aeb9-4bef-96e4-c7f4a3b9c82d","_uuid":"3fdf1d88702894a389c3ac0a63e9641cd97c18b4"}},{"cell_type":"code","source":"plt.figure(figsize=(20,12))\n\ntopics = pd.DataFrame(most_probable_topic(X_train.lemmatized_words))\n\nsns.distplot(topics.iloc[combined[combined.author=='EAP'].index.values],\n             bins=range(0, 7, 1), kde=False, norm_hist=True, color='steelblue', label='EAP')\nsns.distplot(topics.iloc[combined[combined.author=='HPL'].index.values],\n             bins=range(0, 7, 1), kde=False, norm_hist=True, color='seagreen', label='HPL')\nsns.distplot(topics.iloc[combined[combined.author=='MWS'].index.values],\n             bins=range(0, 7, 1), kde=False, norm_hist=True, color='salmon', label='MWS')\n\nplt.title('Topic Distribution')\nplt.legend()\nplt.show()","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"61cfc507-23a3-45ba-a6dd-ef357339b382","collapsed":true,"_uuid":"d2390c1e5cc858bee0852b64ed30c6cfb3a5864e"}},{"cell_type":"markdown","source":"### Model Fitting\n\nLet's select the features which we will be using for our prediction.","metadata":{"_cell_guid":"d6ed875a-e1a5-4b07-ad90-38f779b9f5ea","_uuid":"df019aebf95527f90369fc6e805561eb4519b5fa"}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX_train.columns = [str(feat) for feat in X_train.columns.tolist()]\nX_test.columns = [str(feat) for feat in X_test.columns.tolist()]\nfeatures = [feat for feat in X_train.columns.tolist() \n            if feat not in ['author', 'id', 'text', 'stemmed_words', 'lemmatized_words']]\n\nX, y = X_train[features], X_train.author.values.ravel()\nX_test = X_test[features]\n\nX_subtrain, X_subtest, y_subtrain, y_subtest = train_test_split(X, y, test_size=0.2,\n                                                                random_state=random_state)","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"df9ff5f5-a92b-4f6e-a4f8-7282429d63e8","collapsed":true,"_uuid":"a785b3b5097cdc23b0bcad25a7055ebbc6353613"}},{"cell_type":"markdown","source":"#### Logistic Regression\n\nIn our case, we use the Logistic Regression Model as our baseline model.","metadata":{"_cell_guid":"54149088-b952-43ac-b743-c5b0f97a3a71","_uuid":"94ec2e9ee528affaada9f360ce2738538bf5f641"}},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import GridSearchCV\n\nlogregr = LogisticRegression(random_state=random_state)\n\nparam_grid = {'C': np.logspace(-2, 2, 5)}\n\nclf = GridSearchCV(logregr, param_grid=param_grid, scoring='neg_log_loss', cv=3)\nclf.fit(X_subtrain, y_subtrain)","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"9d2ac3d6-0b9d-43b0-b91d-eedfcb57a698","collapsed":true,"_uuid":"c19325da37cc8c736206c250c9432275f855515f"}},{"cell_type":"markdown","source":"How well does our model generalize to the hold-out testing dataset? Let's find out.","metadata":{"_cell_guid":"b6309483-fad8-4ec2-b0d3-7e5d352e48e6","_uuid":"bc80a57d833602c03e8949f234e03f614eb2fc9c"}},{"cell_type":"code","source":"from sklearn.metrics import log_loss\n\nlog_loss(y_subtest, clf.predict_proba(X_subtest))","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"af2678c6-4da5-4fa4-b910-46330639cef3","collapsed":true,"_uuid":"79208329176880772716e2396a790c703cf250ea"}},{"cell_type":"markdown","source":"As it turns out, our model performed pretty well on our holdout testing set, with a logarithmic loss of 0.445. Let's see how well our model generalizes to the testing dataset. Let's fit our model using the whole training dataset (including the holdout dataset).","metadata":{"_cell_guid":"31e2b5d5-e23f-4c89-bb8f-c0d6eef7513a","_uuid":"820edbe7246967a5ca8f3bbf7ade8a4fa8c43eb8"}},{"cell_type":"code","source":"clf.fit(X, y)","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"42d32662-b6aa-4dd4-8dea-40554994a617","collapsed":true,"_uuid":"45a338c19df7c1eb01067a273dd8d21d60dafd11"}},{"cell_type":"markdown","source":"#### Submitting our results","metadata":{"_cell_guid":"9f6bcf47-9f12-464e-8374-bc4971ff8ca9","_uuid":"a8385150cc0175e48e9b591a7e319570ef51b1fa"}},{"cell_type":"code","source":"X_test_pred = pd.DataFrame(clf.predict_proba(X_test), \n                           columns = ['EAP', 'HPL', 'MWS'])","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"be73bc8a-5ef3-41ba-a577-17443ad4cb2c","collapsed":true,"_uuid":"3c0a82e85b1980466b2d7e26c8e9d566e8074ef5"}},{"cell_type":"code","source":"submission = pd.read_csv('submission.csv')\n\nX_test_pred['id'] = submission['id']\n(X_test_pred.set_index('id')\n .reset_index()\n .to_csv('submission_.csv', index=False))","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"132ca24a-34b3-46e4-95d7-131db944453a","collapsed":true,"_uuid":"a3f495e6530f9223be171530663eb73af23ac576"}},{"cell_type":"markdown","source":"As it turns out, our model was able to generalize pretty well to the test set, scoring a logarithmic loss of 0.42827 (lower is better). This was enough to place us at rank 214 out of 585 (the 37th percentile in the competition)! (As on 14 Nov 2017)","metadata":{"_cell_guid":"ee31c0e5-ecc0-478a-9a85-38a24220df5e","_uuid":"fc1ef99fdc9f8fd5c0f72852495c617aa705111c"}}],"metadata":{"language_info":{"name":"python","codemirror_mode":{"name":"ipython","version":3},"version":"3.6.3","file_extension":".py","pygments_lexer":"ipython3","mimetype":"text/x-python","nbconvert_exporter":"python"},"kernelspec":{"language":"python","name":"python3","display_name":"Python 3"}}}