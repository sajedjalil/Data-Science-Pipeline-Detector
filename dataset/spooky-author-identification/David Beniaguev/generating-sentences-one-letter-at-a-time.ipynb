{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","nbconvert_exporter":"python","mimetype":"text/x-python","file_extension":".py","version":"3.6.3","pygments_lexer":"ipython3","codemirror_mode":{"name":"ipython","version":3}}},"nbformat_minor":1,"nbformat":4,"cells":[{"metadata":{"_cell_guid":"00c95d81-b7a3-43d6-8992-6a85db4f3698","_uuid":"bea0b8c41aa3f6cd77f8cdd6fc18b2e88b30bc34"},"cell_type":"markdown","source":"# Generating Sentences One Letter at a Time\nIn this script we will create several character based language models using very simple conditional probability distributions (CPDs) by calculating histograms from the data (these are simple generative models that we will sample sentences from). We will also build basic logistic regression classfiers over bag of character and/or bag of word features (these are simple discriminative models that we will use to identify authors).  \n\n## Generative Part\nIn the main part of the script, we will use **Markov Models** that remember either 0, 1, 2, 3 or 4 previous characters and emit a probability distrubiton over the next character in the sequence.  \nMarkov Models are perhaps the simplest and most streight forward way to model temporal sequences of discrete symbols (in our case, we will use characters as our discrete symbols).  \n\nWe will first see how discriminative these generative models can be for the task of author identification (spoiler: dispite their simplicity they are surprisingly good).  \n\nWe then continue to generate text in the style of the authors using those models to see what they have learned (another spoiler: they learn quite a bit, but not quite at the \"famous author\" level yet).   \n\nIn the end we will create a submission using our best markov model for the use of everyone who wishes to tweak it.\n\n## Discriminative Part\n\nIn the very last part of the script, we employ a fully discriminative approach that is either character based or word based bag of word features:\n1. Extract **Bag of Character n-grams** features\n1. Create a submission for **Logistic Regression over *BagOfChar***\n1. Extract **Bag of Word n-grams** features\n1. Create a submission for **Logistic Regression over *BagOfWord***\n1. Create a submission for **Logistic Regression over both *BagOfWord and BagOfChar***\n\n\n**Note: ** I use only pandas, numpy and native python here to make it simpler to \"get into\" the code for those of you who wish to do so (only in the last discriminative part I use sklearn as well)\n\n![nice animation](https://cdn-images-1.medium.com/max/1600/1*MbHRwYNA8F29hzes8EPHiQ.gif)\n\nCheck out this [blog post](https://hackernoon.com/from-what-is-a-markov-model-to-here-is-how-markov-models-work-1ac5f4629b71) from which I stole this nice animation above. It explains the basics of markov models and even contains code to apply markov models at the word level as basic discrete symbols. In this script, however, we will use characters as the basic discrete symbols. "},{"metadata":{"_cell_guid":"67a8db5f-392e-4c6f-b905-85bfb979b7ed","_uuid":"842777125de010e71d5dfe076a7e7a41d1ab091e"},"cell_type":"markdown","source":"# Short Math Introduction\nIn this script we try to model the distribution of sentences $$P(Sentence)$$  \nA sentence is just a sequence of $n$ characters and therrefore we can write $$P(Sentence) = P(c_1,c_2,c_3,...,c_n)$$  \n\n**In this script** we will model this sequence of characters using short term memory conditional distributions.  \n* **no memory** (this is also known as naive bayes): \n$$P(c_1,c_2,c_3,...,c_n) = P(c_1)P(c_2)P(c_3)...P(c_n) = \\prod_{t=1}^{n}P(c_t)$$ \n* **1 time step memory** (this is the classical markov chain): \n$$P(c_1,c_2,c_3,...,c_n) = P(c_1)P(c_2|c_1)P(c_3|c_2)...P(c_n|c_{n-1}) = P(c_1)\\prod_{t=2}^{n}P(c_t|c_{t-1})$$ \n* **2 time step memory**: \n$$P(c_1,c_2,c_3,...,c_n) = P(c_1)P(c_2|c_1)\\prod_{t=3}^{n}P(c_t|c_{t-1},c_{t-2})$$ \n* **3 time step memory**: \n$$P(c_1,c_2,c_3,...,c_n) = P(c_1)P(c_2|c_1)P(c_3|c_2,c_1)\\prod_{t=4}^{n}P(c_t|c_{t-1},c_{t-2},c_{t-3})$$ \n* **4 time step memory**: \n$$P(c_1,c_2,c_3,...,c_n) = P(c_1)P(c_2|c_1)P(c_3|c_2,c_1)P(c_4|c_3,c_2,c_1)\\prod_{t=5}^{n}P(c_t|c_{t-1},c_{t-2},c_{t-3},c_{t-4})$$ \n\n## Note: \nIn this corpus, the authors use 34 total characters (regular characters plus punctuation marks).  \nWhat this means is that:  \nStoring the $P(c_t)$ distribution will involve storing $34$ numbers.  \nStoring the $P(c_t|c_{t-1})$ distribution will involve storing $34^2 = 1,156$ numbers.  \nStoring the $P(c_t|c_{t-1},c_{t-2})$ distribution will involve storing $34^3 = 39,304$ numbers.  \nStoring the $P(c_t|c_{t-1},c_{t-2},c_{t-3})$ distribution will involve storing $34^4 = 1,336,336$ numbers.  \nStoring the $P(c_t|c_{t-1},c_{t-2},c_{t-3},c_{t-4})$ distribution will involve storing $34^5 = 45,435,424$ numbers.  \n\nKeep in mind also that the entire corpus of this competition contains about ~3M characters so that we should start encountering substantial finite sample size effects for 3 and 4 history conditional probability distributions."},{"metadata":{"_cell_guid":"6410c90a-ecf3-47cb-9365-0ab609f679e5","_uuid":"a8c18745f3bc90df325475338a4569a50aa051cb","_kg_hide-input":false,"collapsed":true},"cell_type":"code","execution_count":null,"outputs":[],"source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib\nfrom sklearn import model_selection, preprocessing, linear_model\nfrom sklearn.metrics import log_loss, accuracy_score\nfrom sklearn.feature_extraction.text import CountVectorizer\n\nmatplotlib.style.use('fivethirtyeight')"},{"metadata":{"_cell_guid":"62929ec9-f685-4c67-bf93-e5809b9c1abe","_uuid":"91b1fbf4352ccdeef3feda46413b68723117637c"},"cell_type":"markdown","source":"## Load training data and seperate it into a train and validation sets"},{"metadata":{"_cell_guid":"fbd33ca3-320a-483f-9030-f6a7dfde89e3","_uuid":"837f7b120f60725150db4f9c22f14c9f594504a2","collapsed":true},"cell_type":"code","execution_count":null,"outputs":[],"source":"#%% load and organize data\ndata = pd.read_csv('../input/train.csv')\n\nstratifiedCV = model_selection.StratifiedShuffleSplit(n_splits=1, test_size=0.10, random_state=1)\ntrainInds, validInds = next(stratifiedCV.split(data['text'], data['author']))\n\ntrainText  = data.loc[trainInds,'text'].reset_index(drop=True)\nvalidText  = data.loc[validInds,'text'].reset_index(drop=True)\ntrainLabel = data.loc[trainInds,'author'].reset_index(drop=True)\nvalidLabel = data.loc[validInds,'author'].reset_index(drop=True)"},{"metadata":{"_cell_guid":"43dac597-d6c9-4703-8aa4-c330c184462f","_uuid":"a7e4bc1d9d1c0f571b59171d5ab50fac5b500c78"},"cell_type":"markdown","source":"## Collect all chars into one large string for each author"},{"metadata":{"_cell_guid":"03008fd9-3115-4297-8a19-85b909a5677c","_uuid":"df3c3cf18de9fc66d51c55ccbeff90164c868de4","_kg_hide-input":true,"collapsed":true},"cell_type":"code","execution_count":null,"outputs":[],"source":"#%% some utility code\n# dictionary to manually converts greek/spanish chars into closest english chars\ntoEnglishDict = {}\nsrcStr = ['à','â','ä','å','æ','ç','è','é','ê','ë','ï','î','ñ','ô','ö','õ','ü','û','α','δ','ν','ο','π','ς','υ','ἶ']\ndstStr = ['a','a','a','a','a','c','e','e','e','e','i','i','n','o','o','o','u','u','a','d','n','o','p','s','y','i']\nfor src,dst in zip(srcStr,dstStr):\n    toEnglishDict[src] = dst\n    \n# function that converts all non english chars to their closest english char counterparts\ndef myunidecode(inString):\n    outString = ''\n    for ch in inString:\n        if ch in toEnglishDict.keys():\n            outString += toEnglishDict[ch]\n        else:\n            outString += ch\n    return outString"},{"metadata":{"_cell_guid":"97eda04f-6386-4deb-96e9-2fd60bc7ed94","_uuid":"8103a9cf3f33f9fa88d3d2b294583767558fcc14","_kg_hide-input":false,"collapsed":true},"cell_type":"code","execution_count":null,"outputs":[],"source":"#%% go over all train data and collect one large char sequence for each author\ncharsDict = {}\nfor key in ['all','EAP','HPL','MWS']:\n    charsDict[key] = []\n\nfor k, (sentence, author) in enumerate(zip(trainText,trainLabel)):\n    # the decoding is done for spanish/greek chars to be converted to close english chars\n    decodedSentence = myunidecode(sentence.lower())\n    chars = [char for char in decodedSentence]\n    \n    charsDict['all']  += chars\n    charsDict[author] += chars"},{"metadata":{"_cell_guid":"671379ac-fd3a-45c1-bcdf-2432c9960435","_uuid":"62dc53e002586c98788811c756bbf5cc0a8f86d0"},"cell_type":"markdown","source":"# Show the Char usage Distribution for each Author\n$$P(c_t|Author)$$"},{"metadata":{"_cell_guid":"b14c8208-d894-40ee-a4d0-e004e93ab615","_uuid":"7dc31fd5032fc5501bff3616729f837bc6581eaf","_kg_hide-input":true,"collapsed":true},"cell_type":"code","execution_count":null,"outputs":[],"source":"#%% show char usage histogram for the three authors\ncharEncoder = preprocessing.LabelEncoder()\ncharEncoder.fit(charsDict['all'])\n\ncharCounts_EAP = np.histogram(charEncoder.transform(charsDict['EAP']),range(len(charEncoder.classes_)+1),density=True)[0]\ncharCounts_HPL = np.histogram(charEncoder.transform(charsDict['HPL']),range(len(charEncoder.classes_)+1),density=True)[0]\ncharCounts_MWS = np.histogram(charEncoder.transform(charsDict['MWS']),range(len(charEncoder.classes_)+1),density=True)[0]\n\n# sort the char classes by their usage frequency\nsortedChars = np.flipud(np.argsort(charCounts_EAP + charCounts_HPL + charCounts_MWS))"},{"metadata":{"_cell_guid":"436abae9-7c73-44d6-9d0e-0da539e10a34","_uuid":"49fa8c587b2a8dbf87075479d81099bc1843214b","_kg_hide-output":false,"_kg_hide-input":true,"collapsed":true},"cell_type":"code","execution_count":null,"outputs":[],"source":"barWidth = 0.21\nx = np.arange(len(charCounts_EAP))\n\nplt.figure(figsize=(12,7)); plt.title('Character Usage Frequncy - $P(C_t)$ ',fontsize=25);\nplt.bar(x-barWidth, charCounts_EAP[sortedChars], barWidth, color='r', label='Edgar Allen Poe');\nplt.bar(x         , charCounts_HPL[sortedChars], barWidth, color='g', label='Howard Phillips Lovecraft');\nplt.bar(x+barWidth, charCounts_MWS[sortedChars], barWidth, color='b', label='Mary Wollstonecraft Shelley');\nplt.legend(fontsize=24); plt.ylabel('Usage Frequncy - $P(C_t)$', fontsize=20); plt.xlabel('$C_t$');\nplt.xticks(x,[\"'%s'\" %(charEncoder.classes_[i]) for i in sortedChars], fontsize=13);"},{"metadata":{"_cell_guid":"128558f1-b145-454b-bbb1-8824ca67f7bb","_uuid":"c7d182278e2993dde5b8c68bd8fc0da511413677"},"cell_type":"markdown","source":"Interestingly, there are differences between the authors here!  \n## Lets look at the same plot only with log scale on the y axis"},{"metadata":{"_cell_guid":"d6f28df1-30e9-49b9-90dc-43194aab88d9","_uuid":"d9f049d48d1207655e4534465b40201db7e042d2","_kg_hide-output":false,"_kg_hide-input":true,"collapsed":true},"cell_type":"code","execution_count":null,"outputs":[],"source":"barWidth = 0.21\nx = np.arange(len(charCounts_EAP))\n\nplt.figure(figsize=(12,7)); \nplt.title('Character Usage Frequncy - $P(C_t)$ ',fontsize=25);\nplt.bar(x-barWidth, charCounts_EAP[sortedChars], barWidth, color='r', label='Edgar Allen Poe');\nplt.bar(x         , charCounts_HPL[sortedChars], barWidth, color='g', label='Howard Phillips Lovecraft');\nplt.bar(x+barWidth, charCounts_MWS[sortedChars], barWidth, color='b', label='Mary Wollstonecraft Shelley');\nplt.legend(fontsize=21); plt.ylabel('Usage Frequncy - $P(C_t)$', fontsize=20); \nplt.yscale(\"log\", nonposy='clip'); plt.xlabel('$C_t$');\nplt.xticks(x,[\"'%s'\" %(charEncoder.classes_[i]) for i in sortedChars], fontsize=11);"},{"metadata":{"_cell_guid":"f289e60a-97dd-4e1a-8a77-b2741fc9d109","_uuid":"35fa3098ccd7a53be2e46efe665f059e195be3c3"},"cell_type":"markdown","source":"Very large differences can be seen in usage of punctuation marks.  \nFor example, the \":\" character is used much more extensivley by MWS and least used by HPL.\n\nEven though these differences look small, they are extreemly statistically significant since there are about 2,600,000 chars in the training set. These differences are not there by chance, and perhaps we can utilize these differences for classification. Lets try."},{"metadata":{"_cell_guid":"cc39acfc-bffe-4d14-9a3b-9cc052d42a02","_uuid":"9d88f4dbc77786fb57f5249f5e53934ce1e8d87f"},"cell_type":"markdown","source":"# How do we Identify the Author of each Sentence using our CPDs?\nIn this competition specifically, we would like to model the conditional probability distribution of the author given a sentence: $$P(Author | Sentence) = P(Author | c_1,c_2,c_3,...,c_n)$$  \n\nWe have three different authors here, so this boils down to:  \n\n$$P(EAP | c_1,c_2,c_3,...,c_n)$$    \n$$P(HPC | c_1,c_2,c_3,...,c_n)$$    \n$$P(MWS | c_1,c_2,c_3,...,c_n)$$  \n\nUsing the bayes rule we can invert this into $$P(Author | Sentence) = \\frac{P(Sentence | Author)P(Author)}{P(Sentence)}$$  \nThe prior distribution $P(Author)$ over authors is extreemly simple to calcualte (it's just the frequency of occurence of each author in the training set).  \n\nFor any specific sentence, the probability of that sentence $P(Sentence)$ is identical for all authors and therefore doesn't create any additional discrimination between them, so when creating a classifier we can just ignore this and compare the 3 following quantities:  \n\n$$P(c_1,c_2,c_3,...,c_n | EAP)P(EAP)$$    \n$$P(c_1,c_2,c_3,...,c_n | HPC)P(HPC)$$    \n$$P(c_1,c_2,c_3,...,c_n | MWS)P(MWS)$$  \nmeaning:\n$$  \\mathbf{predicted\\:Author} = argmax\\:\\{{P(c_1,c_2,c_3,...,c_n|Author)P(Author)}\\}   $$"},{"metadata":{"_cell_guid":"ad5a4e32-5649-4503-9406-7e042e4fb5b4","_uuid":"cfba7885c7f8ffddcc91f1de380116c237864cf0"},"cell_type":"markdown","source":"# Calculate Classification Accuracy based only on single char distribution\n$$  \\mathbf{predicted\\:Author} = argmax\\:\\{ \\prod_{t=1}^{n}P(c_t|Author)\\}   $$  \n\n**Note:** we assume here an equal prior just for simplicity (i.e. $P(Author) = \\frac{1}{3}$ for all authors)  \n"},{"metadata":{"_cell_guid":"cc90a92e-2146-4499-9c8c-a498d173c745","_uuid":"b1f01ca0cb494790b9e18fb230041491ee9c9783","collapsed":true},"cell_type":"code","execution_count":null,"outputs":[],"source":"#%% meassure classification accuracy on validation set using only character frequncy\nauthorsList = ['EAP','HPL','MWS']\nauthorPredictionList = []\nfor k, (sentence, author) in enumerate(zip(validText,validLabel)):\n    chars = [char for char in myunidecode(sentence.lower())]\n    # convert to log so we can sum probabilities instead of multiply\n    logP_EAP = sum([np.log(charCounts_EAP[charEncoder.classes_ == ch]) for ch in chars])\n    logP_HPL = sum([np.log(charCounts_HPL[charEncoder.classes_ == ch]) for ch in chars])\n    logP_MWS = sum([np.log(charCounts_MWS[charEncoder.classes_ == ch]) for ch in chars])\n    \n    authorPredictionList.append(authorsList[np.argmax(np.array([logP_EAP,logP_HPL,logP_MWS]))])\n\nprint(52*'-')\nprint('==> Validation Set Classification Accuracy = %.1f%s' %(100*(validLabel == authorPredictionList).mean(),'%'))\nprint(52*'-')"},{"metadata":{"_cell_guid":"4579bad2-3fc7-40de-b54f-9a2a1006558e","_uuid":"e92fd40c9f187f8591552e1e933af333ca2d6cbb"},"cell_type":"markdown","source":"Interestingly, even though this is perhaps the stupidest model one can think of, the discrimination accuracy is well above chance level."},{"metadata":{"_cell_guid":"bb5bdc3a-b469-4aa7-9538-89979ecc77b1","_uuid":"0095450bd3298e5f7d7717a4a109ce3b4d918e9a"},"cell_type":"markdown","source":"## Generate Sample Text for each author using the independent chars model\n$$  c_t \\: {\\raise.17ex\\hbox{$\\scriptstyle\\mathtt{\\sim}$}} \\: P(c_t|Author)   $$"},{"metadata":{"_cell_guid":"b33348ab-2110-4075-89f3-5bf1a70add34","_uuid":"81970ad3dcaba2cb91817b150b97131be7069005","_kg_hide-input":false,"collapsed":true},"cell_type":"code","execution_count":null,"outputs":[],"source":"#%% generate sample text by sampling one charachter at a time for the independent character model\nnp.random.seed(1234)\n\nmaxSentenceLength = 95\nnumSentencesPerAuthor = 5\n\ncharProbModel = {}\ncharProbModel['all'] = (charCounts_EAP + charCounts_HPL + charCounts_MWS)/3.0\ncharProbModel['EAP'] = charCounts_EAP\ncharProbModel['HPL'] = charCounts_HPL\ncharProbModel['MWS'] = charCounts_MWS\n\nfor author in ['EAP','HPL','MWS','all']:\n    print((6+maxSentenceLength)*'-')\n    print('Author %s:' %(author))\n    print(12*'-')\n    for i in range(numSentencesPerAuthor):\n        generatedSentence = ''\n        for j in range(maxSentenceLength):\n            newChar = charEncoder.classes_[np.random.choice(range(len(charCounts_EAP)),size=1,p=charProbModel[author])][0]\n            generatedSentence += newChar\n            \n            if (newChar == '.') or (j == maxSentenceLength):\n                break\n                \n        print('%d: \"%s\"' %(i+1,generatedSentence))\nprint((4+maxSentenceLength)*'-')"},{"metadata":{"_cell_guid":"6e842a31-dcff-4dc5-b6dc-bb7caf1f422e","_uuid":"3a468424e8fa9eb9c45cc61ad2dcff91cdd4ad12"},"cell_type":"markdown","source":"Well, as expected, we can see that this really doesn't resemble any human generated text.  \nBut perhaps we can do better with some memory?"},{"metadata":{"_cell_guid":"25dfb527-09d4-4025-b942-a82243067be4","_uuid":"4e9b55fdacfbd8fdea121948c35c7d7295d85c8f"},"cell_type":"markdown","source":"## Gather DataFrame with \"Author\", \"History\" and \"Next Char\" Fields\nUse history of 1 character "},{"metadata":{"_cell_guid":"8cf627b5-9dab-4efb-b075-590ce3b2b0fb","_uuid":"41ef0c50c4b894cab80f044df754f7e6e35e72cf","_kg_hide-input":true,"collapsed":true},"cell_type":"code","execution_count":null,"outputs":[],"source":"#%% gather all pairs of characters into a single dataframe\nhistoryLength = 1\n\nhistoryList  = []\nnextCharList = []\nauthorList   = []\nfor k, (sentence, author) in enumerate(zip(trainText,trainLabel)):\n    decodedSentence = myunidecode(sentence.lower())\n    charSequences = [decodedSentence[k:k+historyLength+1] for k in range(len(decodedSentence)-historyLength)]\n    \n    history  = [seq[:-1] for seq in charSequences]\n    nextChar = [seq[ -1] for seq in charSequences]\n    sentenceDF = pd.DataFrame(columns=['author','history','next char'])\n    \n    historyList  += history\n    nextCharList += nextChar\n    authorList   += [author]*len(history)\n        \ncorpusDF = pd.DataFrame(columns=['author','history','next char'])\ncorpusDF['author']    = authorList\ncorpusDF['history']   = historyList\ncorpusDF['next char'] = nextCharList\n\ncorpusDF.head(8)"},{"metadata":{"_cell_guid":"df23a5bf-799d-46fd-a176-4d75044939c4","_uuid":"71849b9a60dead2d0e94b5abdcf8153855ee1201"},"cell_type":"markdown","source":"## Build Markov Model that Remebers only the previous char\n$$P(c_t|c_{t-1},Author)$$"},{"metadata":{"_cell_guid":"714a10dd-1eb9-4b49-8d86-b271a937c18a","_uuid":"4152aaaeae091574bf8168901c481f36eb48b941","_kg_hide-input":true,"collapsed":true},"cell_type":"code","execution_count":null,"outputs":[],"source":"#%% generate P(c(t)|c(t-1)) model (Markov Model with memory of 1 time step)\ncharCondProbModel_H1 = {}\nfor author in ['EAP','HPL','MWS']:\n    charCondProbModel_H1[author] = np.zeros( (1+historyLength)*[charEncoder.classes_.shape[0]] )\n    \ncharCondCountModel_H1 = {}\nfor author in ['EAP','HPL','MWS']:\n    charCondCountModel_H1[author] = np.zeros( (1+historyLength)*[charEncoder.classes_.shape[0]] )\n\ncorpusGroupedByAuthor = corpusDF.groupby(by='author',axis=0)\nfor author in corpusDF['author'].unique():\n    authorCorpusDF = corpusGroupedByAuthor.get_group(author).loc[:,['history','next char']].reset_index(drop=True)\n    authorCorpusGroupedByHistory = authorCorpusDF.groupby(by='history',axis=0)\n    for history in authorCorpusDF['history'].unique():\n        authorHistoryDF = authorCorpusGroupedByHistory.get_group(history).reset_index(drop=True).loc[:,'next char'].reset_index(drop=True)\n\n        encodedHistory = charEncoder.transform([history])[0]\n        encodedNextCharCounts = np.histogram(charEncoder.transform(authorHistoryDF),range(len(charEncoder.classes_)+1),density=False)[0]\n        encodedNextCharProb   = np.histogram(charEncoder.transform(authorHistoryDF),range(len(charEncoder.classes_)+1),density=True)[0]\n\n        charCondProbModel_H1[author][encodedHistory,:]  = encodedNextCharProb\n        charCondCountModel_H1[author][encodedHistory,:] = encodedNextCharCounts\n\n    condCount = charCondCountModel_H1[author]\n    print('%s Sparsity level = %.1f%s' %(author, 100*(condCount < 1).sum() / (condCount > -1).sum().astype(float),'%'))\n\ncharCondProbModel_H1['all']  = (charCondProbModel_H1['EAP']  + charCondProbModel_H1['HPL']  + charCondProbModel_H1['MWS'] )/3.0\ncharCondCountModel_H1['all'] =  charCondCountModel_H1['EAP'] + charCondCountModel_H1['HPL'] + charCondCountModel_H1['MWS']\n\nprint('average Sparsity level = %.1f%s' %(100*(charCondCountModel_H1['all'] < 1).sum() / (condCount > -1).sum().astype(float),'%'))"},{"metadata":{"_cell_guid":"e548e874-0455-4141-a26f-ad5afaa53bf8","_uuid":"ad3802458bf07cd0c51b7305be7896e21d2494ea"},"cell_type":"markdown","source":"# Show the Conditional Probability Distribution of the entire corpus \n$$P(c_t|c_{t-1}) $$"},{"metadata":{"_cell_guid":"0f03a50a-bd60-4ebd-91b3-267e811aab9b","_uuid":"5a7e3709126d42e2d2fbe87db1491cc50a21a2eb","_kg_hide-output":false,"_kg_hide-input":true,"collapsed":true},"cell_type":"code","execution_count":null,"outputs":[],"source":"condProb = charCondProbModel_H1['all']\n\nplt.figure(figsize=(12,10))\nplt.imshow(condProb, cmap='hot');  plt.colorbar(); plt.clim(0,1);\nplt.grid('off'); plt.title('P(next char | prev char) for all Authors - $P(c_t|c_{t-1})$', fontsize=22);\nplt.xlabel('$c_t$ - next character', fontsize=18); plt.ylabel('$c_{t-1}$ - previous character', fontsize=18);\nplt.xticks(range(condProb.shape[0]),[\"'%s'\" %(ch) for ch in charEncoder.classes_]);\nplt.yticks(range(condProb.shape[0]),[\"'%s'\" %(ch) for ch in charEncoder.classes_]);"},{"metadata":{"_cell_guid":"91c86238-36da-49e7-a2a9-78b2dd1c1031","_uuid":"63cc4955b46d125b0f8253632de5b2ded1913c67"},"cell_type":"markdown","source":"There are a few clear \"columns\" in the dataset - for the \"a\",\"e\",\"i\" and \"o\" characters (which are vowels, by the way). What this means is that these characters are quite likely to come after many other characters. Contrast that with \"y\" that is likely to occur mostly after \"l\", \"m\" and \"b\" (to form the pairs \"ly\", \"my\" and \"by\").\n\nNote that we now have about 25% of character pairs that never occur in the training set at all. Even though we only have ~1,150 possible charachter pairs and ~2,600,000 pairs.  \nIn this particular case, it's safe to assume that the pairs that don't occur are simply very rare or non existent in the languge, but it's important to keep in mind that some of these zeros are perhaps due to finite sample size. This can be quanitfied of course, but we will skip this in this tutorial."},{"metadata":{"_cell_guid":"37a9ae24-bf09-4999-8690-231132bad2ec","_uuid":"5742f9eca7acbaace3603e4f2c96ec0a17676958"},"cell_type":"markdown","source":"# Show the Author Specific Conditional Probability Distributions \n$$P(c_t|c_{t-1},Author) $$"},{"metadata":{"_cell_guid":"8302ad72-5aba-4679-be35-22f61e1c5c81","_uuid":"fc3f9745af783421a98ba7d6213faaf2ebc76920","_kg_hide-input":true,"collapsed":true},"cell_type":"code","execution_count":null,"outputs":[],"source":"shortToFullNameDict = {}\nshortToFullNameDict['EAP'] = 'Edgar Allen Poe'\nshortToFullNameDict['HPL'] = 'Howard Phillips Lovecraft'\nshortToFullNameDict['MWS'] = 'Mary Wollstonecraft Shelley'\n\nplt.figure(figsize=(13,28))\nfor k, author in enumerate(['EAP','HPL','MWS']):\n    condProb = charCondProbModel_H1[author]\n    plt.subplot(3,1,k+1); plt.imshow(condProb, cmap='hot'); \n    plt.grid('off'); plt.colorbar(); plt.clim(0,1);\n    plt.title('P(next char | prev char, %s) - $P(c_t|c_{t-1},Author)$' %(shortToFullNameDict[author]), fontsize=17);\n    plt.xlabel('$c_t$ - next character', fontsize=15); plt.ylabel('$c_{t-1}$ - previous character', fontsize=15);\n    plt.xticks(range(condProb.shape[0]),[\"'%s'\" %(ch) for ch in charEncoder.classes_]);\n    plt.yticks(range(condProb.shape[0]),[\"'%s'\" %(ch) for ch in charEncoder.classes_]);\nplt.tight_layout();"},{"metadata":{"_cell_guid":"8c79df0f-cc69-4ed7-a6e6-ee0a97dbf71e","_uuid":"895c40f95fb79a4fc648590beed98aa0220b3cce"},"cell_type":"markdown","source":"We can see a few differences between the authors, especially in the top few rows that indicate the different usage of punctuation marks by our authors."},{"metadata":{"_cell_guid":"12b34b21-03be-4387-b067-0a0ffb29f1ad","_uuid":"767f805254bc829dd7862f90f5c108d697dcc680"},"cell_type":"markdown","source":"## Calculate Classification Accuracy of a Classic Markov Model\n$$  \\mathbf{predicted\\:Author} = argmax\\:\\{ \\prod_{t=2}^{n}P(c_t|c_{t-1},Author)\\}   $$"},{"metadata":{"_cell_guid":"574c5390-e94c-4456-9a06-2cb4db072513","_uuid":"c071ab1bca3d5e23c51a989de10120d10b599689","_kg_hide-input":true,"collapsed":true},"cell_type":"code","execution_count":null,"outputs":[],"source":"#%% meassure classification accuracy on validation set using Markov Model with memory of 1 time step\nuniformPriorFraction    = 0.0001\nallAuthorsPriorFraction = 0.0001\n\nprior = np.array([1.0-uniformPriorFraction-allAuthorsPriorFraction, allAuthorsPriorFraction, uniformPriorFraction])\nuniformPriorValue = 1.0/len(charEncoder.classes_)\n\ncondP_H1 = {}\nauthorsList = ['EAP','HPL','MWS']\nfor author in authorsList:\n    condP_H1[author]  = prior[0]*charCondProbModel_H1[author]\n    condP_H1[author] += prior[1]*charCondProbModel_H1['all']\n    condP_H1[author] += prior[2]*uniformPriorValue\n\nauthorPredictionList = []\nfor k, (sentence, author) in enumerate(zip(validText,validLabel)):\n    decodedSentence = myunidecode(sentence.lower())\n    charSequences = [decodedSentence[k:k+historyLength+1] for k in range(len(decodedSentence)-historyLength)]\n    \n    history  = [seq[:-1] for seq in charSequences]\n    nextChar = [seq[ -1] for seq in charSequences]\n    \n    logP_EAP = 0.0; logP_HPL = 0.0; logP_MWS = 0.0\n    for histChar, nextChar in zip(history,nextChar):\n        encodedHistChar = charEncoder.transform([histChar])[0]\n        encodedNextChar = charEncoder.transform([nextChar])[0]\n        \n        logP_EAP += np.log(condP_H1['EAP'][encodedHistChar,encodedNextChar])\n        logP_HPL += np.log(condP_H1['HPL'][encodedHistChar,encodedNextChar])\n        logP_MWS += np.log(condP_H1['MWS'][encodedHistChar,encodedNextChar])\n    \n    authorPredictionList.append(authorsList[np.argmax([logP_EAP,logP_HPL,logP_MWS])])\n    \nprint(52*'-')\nprint('==> Validation Set Classification Accuracy = %.1f%s' %(100*(validLabel == authorPredictionList).mean(),'%'))\nprint(52*'-')"},{"metadata":{"_cell_guid":"571beabf-823a-461b-9478-6bb0b5d93d7b","_uuid":"6897ce2802aec902e273ba0b2a27ff97875883d3"},"cell_type":"markdown","source":"The classification accuracy increases some more. By just looking at the distribution of pairs of charachters. "},{"metadata":{"_cell_guid":"a45dd146-248d-4c7f-9911-6e185c375c2b","_uuid":"0ffdfde68444a8f14b08d6e103a4098f9d313239"},"cell_type":"markdown","source":"## Generate Sample Text for each Author using our Markov Model\n$$  c_t\\: {\\raise.17ex\\hbox{$\\scriptstyle\\mathtt{\\sim}$}} \\:  P(c_t|c_{t-1},Author)   $$"},{"metadata":{"_cell_guid":"6fff75d0-958e-46c7-9a4b-4d4016c25660","_uuid":"c3f10b0dafd59eb4351aa8a43eb6ad30d87c6ef0","_kg_hide-input":true,"collapsed":true},"cell_type":"code","execution_count":null,"outputs":[],"source":"#%% generate sample text by sampling one charachter at a time from the 1 time step memory Markov Model\nnp.random.seed(123)\n\nmaxSentenceLength = 90\nnumSentencesPerAuthor = 6\n\nuniformPriorFraction    = 0.0001\nallAuthorsPriorFraction = 0.0009\n\nprior = np.array([1.0-uniformPriorFraction-allAuthorsPriorFraction, allAuthorsPriorFraction, uniformPriorFraction])\nuniformPriorValue = 1.0/(len(charEncoder.classes_))\n\ncondP_H1 = {}\nauthorsList = ['EAP','HPL','MWS']\nfor author in authorsList:\n    condP_H1[author]  = prior[0]*charCondProbModel_H1[author]\n    condP_H1[author] += prior[1]*charCondProbModel_H1['all']\n    condP_H1[author] += prior[2]*uniformPriorValue\n\ncondP_H1['all']  = (prior[0]+prior[1])*charCondProbModel_H1['all']\ncondP_H1['all'] += prior[2]*uniformPriorValue\n\nfor author in ['EAP','HPL','MWS','all']:\n    print((6+maxSentenceLength)*'-')\n    print('Author %s:' %(author))\n    print(12*'-')\n    for i in range(numSentencesPerAuthor):\n        firstChar = charEncoder.classes_[np.random.choice(range(len(charCounts_EAP)),size=1,p=charProbModel[author])][0]\n        generatedSentence = firstChar\n        for j in range(maxSentenceLength-1):\n            encodedHistChar = charEncoder.transform([generatedSentence[-1]])[0]\n            newChar = charEncoder.classes_[np.random.choice(range(len(charCounts_EAP)),size=1,p=condP_H1[author][encodedHistChar,:])][0]\n            generatedSentence += newChar\n            \n            if (newChar == '.') or (j == maxSentenceLength):\n                break\n        print('%d: \"%s\"' %(i+1,generatedSentence))\nprint((4+maxSentenceLength)*'-')"},{"metadata":{"_cell_guid":"cb701825-ac17-47ff-9358-1e7000a23f9b","_uuid":"75e3e7c6300686e6e54789d1c1d01ac7cda65e6a"},"cell_type":"markdown","source":"This already has a text like feeling to it. After punctioation we see a whitespace, every now and then we see the letter \"a\" and \"i\" seperated by whitespaces and the short word \"he\" appears several times in the text. we are getting somewhere, let's add a little bit more memory."},{"metadata":{"_cell_guid":"593e93af-5b0a-41c0-ae22-c6ecc11d9dfb","_uuid":"643fd26611349382c4fc4544b8e385d7b075a05b"},"cell_type":"markdown","source":"## Gather DataFrame with \"Author\", \"History\" and \"Next Char\" Fields\nUse history of size 2 characters"},{"metadata":{"_cell_guid":"a249b9aa-5aca-45a0-a211-000448884afb","_uuid":"d3c4d1ab9195ae1a30a14ad3b1a790a4c9d65373","_kg_hide-input":true,"collapsed":true},"cell_type":"code","execution_count":null,"outputs":[],"source":"#%% gather all triplets of characters into a single dataframe\nhistoryLength = 2\n\nhistoryList  = []\nnextCharList = []\nauthorList   = []\nfor k, (sentence, author) in enumerate(zip(trainText,trainLabel)):\n    decodedSentence = myunidecode(sentence.lower())\n    charSequences = [decodedSentence[k:k+historyLength+1] for k in range(len(decodedSentence)-historyLength)]\n    \n    history  = [seq[:-1] for seq in charSequences]\n    nextChar = [seq[ -1] for seq in charSequences]\n    sentenceDF = pd.DataFrame(columns=['author','history','next char'])\n    \n    historyList  += history\n    nextCharList += nextChar\n    authorList   += [author]*len(history)\n        \ncorpusDF = pd.DataFrame(columns=['author','history','next char'])\ncorpusDF['author']    = authorList\ncorpusDF['history']   = historyList\ncorpusDF['next char'] = nextCharList\n\ncorpusDF.head(8)"},{"metadata":{"_cell_guid":"2fee8167-079b-42b2-b2a8-63dc49836c8f","_uuid":"d0cb56d041591094019058ca4e8fea0c6b5cd079"},"cell_type":"markdown","source":"## Build Markov Model that remebers the Two previous chars\n$$ P(c_t|c_{t-1},c_{t-2},Author)  $$"},{"metadata":{"_cell_guid":"76b40bfe-d801-482a-8e8b-86b2d870fde3","_uuid":"f944b738eba926fce7ebe9e3ea6e3e669ff9f4ba","_kg_hide-input":true,"collapsed":true},"cell_type":"code","execution_count":null,"outputs":[],"source":"#%% generate P(c(t)|c(t-1),c(t-2)) model (Markov Model with memory of 2 time steps)\nhistoryLength = 2\n\ncharCondProbModel_H2 = {}\nfor author in ['EAP','HPL','MWS']:\n    charCondProbModel_H2[author] = np.zeros( (1+historyLength)*[charEncoder.classes_.shape[0]] )\n    \ncharCondCountModel_H2 = {}\nfor author in ['EAP','HPL','MWS']:\n    charCondCountModel_H2[author] = np.zeros( (1+historyLength)*[charEncoder.classes_.shape[0]] )\n\ncorpusGroupedByAuthor = corpusDF.groupby(by='author',axis=0)\nfor author in corpusDF['author'].unique():\n    authorCorpusDF = corpusGroupedByAuthor.get_group(author).loc[:,['history','next char']].reset_index(drop=True)\n    authorCorpusGroupedByHistory = authorCorpusDF.groupby(by='history',axis=0)\n    for history in authorCorpusDF['history'].unique():\n        authorHistoryDF = authorCorpusGroupedByHistory.get_group(history).reset_index(drop=True).loc[:,'next char'].reset_index(drop=True)\n\n        encodedHistory = charEncoder.transform([ch for ch in history])\n        encodedNextCharCounts = np.histogram(charEncoder.transform(authorHistoryDF),range(len(charEncoder.classes_)+1),density=False)[0]\n        encodedNextCharProb   = np.histogram(charEncoder.transform(authorHistoryDF),range(len(charEncoder.classes_)+1),density=True)[0]\n\n        charCondProbModel_H2[author][encodedHistory[0],encodedHistory[1],:]  = encodedNextCharProb\n        charCondCountModel_H2[author][encodedHistory[0],encodedHistory[1],:] = encodedNextCharCounts\n\n    condCount = charCondCountModel_H2[author]\n    print('%s Sparsity level = %.1f%s' %(author, 100*(condCount < 1).sum() / (condCount > -1).sum().astype(float),'%'))\n\ncharCondProbModel_H2['all']  = (charCondProbModel_H2['EAP']  + charCondProbModel_H2['HPL']  + charCondProbModel_H2['MWS'] )/3.0\ncharCondCountModel_H2['all'] =  charCondCountModel_H2['EAP'] + charCondCountModel_H2['HPL'] + charCondCountModel_H2['MWS']\n\ncondCount = charCondCountModel_H2['all']\nprint('average Sparsity level = %.1f%s' %(100*(condCount < 1).sum() / (condCount > -1).sum().astype(float),'%'))"},{"metadata":{"_cell_guid":"3b173b26-5f30-43bc-9a64-f51fd71b4b77","_uuid":"9d57c1709e10e0fefa6aa6791ff5adab9916c67a"},"cell_type":"markdown","source":"Note the Sparsity Levels increase quite a bit. This is due to the fact that there are 34^3 (~40,000) possible triplets, and most of them are illegal combinations in the english language.  \n\nBut remember what we discussed earlier, here the problem of finite sample size is much more pronounced so we can be much less \"confident\" in these zeros. meaning, we can't be sure they are actually zeros and not simply very rare events that just didn't happen to occur in the particular realization of the training sample.    \n\nFor this reason we will add a small constant number to the conditional probability distribution."},{"metadata":{"_cell_guid":"dc93dd2f-9d1b-49af-85de-11560a5d368e","_uuid":"a20e1faa027521f1f1cc6f2c29c5c966f8258ebe"},"cell_type":"markdown","source":"## Calculate Classification Accuracy of our 2 time step Markov Model\n$$  \\mathbf{predicted\\:Author} = argmax\\:\\{ \\prod_{t=3}^{n}P(c_t|c_{t-1},c_{t-2},Author)\\}   $$"},{"metadata":{"_cell_guid":"d4b81598-7c0a-465f-aafc-40666049732d","_uuid":"f78d9c85287fb88551a2b9dea9feeda7e117b48c","_kg_hide-input":true,"collapsed":true},"cell_type":"code","execution_count":null,"outputs":[],"source":"#%% meassure classification accuracy on validation set using Markov Model with memory of 2 time steps\nuniformPriorFraction    = 0.0001\nallAuthorsPriorFraction = 0.0001\n\nprior = np.array([1.0-uniformPriorFraction-allAuthorsPriorFraction, allAuthorsPriorFraction, uniformPriorFraction])\nuniformPriorValue = 1.0/len(charEncoder.classes_)\n\ncondP_H2 = {}\nauthorsList = ['EAP','HPL','MWS']\nfor author in authorsList:\n    condP_H2[author]  = prior[0]*charCondProbModel_H2[author]\n    condP_H2[author] += prior[1]*charCondProbModel_H2['all']\n    condP_H2[author] += prior[2]*uniformPriorValue\n\nauthorPredictionList = []\nfor k, (sentence, author) in enumerate(zip(validText,validLabel)):\n    decodedSentence = myunidecode(sentence.lower())\n    charSequences = [decodedSentence[k:k+historyLength+1] for k in range(len(decodedSentence)-historyLength)]\n    \n    history  = [seq[:-1] for seq in charSequences]\n    nextChar = [seq[ -1] for seq in charSequences]\n    \n    logP_EAP = 0.0; logP_HPL = 0.0; logP_MWS = 0.0\n    for histChars, nextChar in zip(history,nextChar):\n        encodedHistChars = charEncoder.transform([ch for ch in histChars])\n        encodedNextChar  = charEncoder.transform([nextChar])[0]\n        \n        logP_EAP += np.log(condP_H2['EAP'][encodedHistChars[0],encodedHistChars[1],encodedNextChar])\n        logP_HPL += np.log(condP_H2['HPL'][encodedHistChars[0],encodedHistChars[1],encodedNextChar])\n        logP_MWS += np.log(condP_H2['MWS'][encodedHistChars[0],encodedHistChars[1],encodedNextChar])\n    \n    authorPredictionList.append(authorsList[np.argmax([logP_EAP,logP_HPL,logP_MWS])])\n\nprint(52*'-')\nprint('==> Validation Set Classification Accuracy = %.1f%s' %(100*(validLabel == authorPredictionList).mean(),'%'))\nprint(52*'-')"},{"metadata":{"_cell_guid":"2198807e-f7e6-4b49-8e38-14fb957956a0","_uuid":"dba804b17c65bfd7264803179c0458e7ef8829fa"},"cell_type":"markdown","source":"The Classification Accuracy keeps rising, which is quite nice.  \nWe are now quite capable in distinguishing between the three authors.  \n***Does this also mean that our model has the ability to write text like our authors?***"},{"metadata":{"_cell_guid":"cd99de2a-a6d4-40f5-b357-f1d3e7069aa0","_uuid":"37c7b66cc0bd98c84e90ce232256b51552b699bf"},"cell_type":"markdown","source":"## Generate Sample Text for each Author using our 2 time step Markov Model\n$$  c_t\\: {\\raise.17ex\\hbox{$\\scriptstyle\\mathtt{\\sim}$}} \\:  P(c_t|c_{t-1},c_{t-2},Author)   $$"},{"metadata":{"_cell_guid":"12cb1539-4646-4a28-9956-b471e3125c2e","_uuid":"61a7396a5966fc8f7be33c06c75c2fa0c8bf0bf1","_kg_hide-input":true,"collapsed":true},"cell_type":"code","execution_count":null,"outputs":[],"source":"#%% generate sample text by sampling one charachter at a time from the 2 time step Markov Model\nnp.random.seed(1000)\n\nmaxSentenceLength = 95\nnumSentencesPerAuthor = 9\n\nuniformPriorFraction    = 0.0001\nallAuthorsPriorFraction = 0.0009\n\nprior = np.array([1.0-uniformPriorFraction-allAuthorsPriorFraction, allAuthorsPriorFraction, uniformPriorFraction])\nuniformPriorValue = 1.0/(len(charEncoder.classes_))\n\ncondP_H2 = {}\nauthorsList = ['EAP','HPL','MWS']\nfor author in authorsList:\n    condP_H2[author]  = prior[0]*charCondProbModel_H2[author]\n    condP_H2[author] += prior[1]*charCondProbModel_H2['all']\n    condP_H2[author] += prior[2]*uniformPriorValue\n\ncondP_H2['all']  = (prior[0]+prior[1])*charCondProbModel_H2['all']\ncondP_H2['all'] += prior[2]*uniformPriorValue\n\nfor author in ['EAP','HPL','MWS','all']:\n    print((6+maxSentenceLength)*'-')\n    print('Author %s:' %(author))\n    print(12*'-')\n    for i in range(numSentencesPerAuthor):\n        firstChar = charEncoder.classes_[np.random.choice(range(len(charCounts_EAP)),size=1,p=charProbModel[author])][0]\n        encodedFirstChar = charEncoder.transform([firstChar])[0]\n        secondChar = charEncoder.classes_[np.random.choice(range(len(charCounts_EAP)),size=1,p=condP_H1[author][encodedFirstChar,:])][0]\n        generatedSentence = firstChar + secondChar\n        \n        for j in range(maxSentenceLength-1):\n            encodedHistChars = charEncoder.transform([ch for ch in generatedSentence[-2:]])            \n            currCondProb = condP_H2[author][encodedHistChars[0],encodedHistChars[1],:]\n            currCondProb = currCondProb/currCondProb.sum() # just in case the probabilities don't sum directly to 1\n            newChar = charEncoder.classes_[np.random.choice(range(len(charCounts_EAP)),size=1,p=currCondProb)][0]\n            generatedSentence += newChar\n            \n            if (newChar == '.') or (j == maxSentenceLength):\n                break\n        print('%d: \"%s\"' %(i+1,generatedSentence))\nprint((4+maxSentenceLength)*'-')"},{"metadata":{"_cell_guid":"878e207c-616f-4848-a9ff-d3aa34190687","_uuid":"75020628183f847872f054e1049022d07814e7fc"},"cell_type":"markdown","source":"Clearly the answer is no.  Our model is not as capable as our authors.  \nNevertheless, we see that the text looks even better now. A lot of short 2-4 letter words like  \"on\", \"to\", \"we\", \"me\", \"of\", \"the\", \"for\", \"now\", \"age\", \"hate\", \"thin\", \"eyes\" appear quite often.   \nA completely different world relative to the independent model we saw first that looked like a complete jumble."},{"metadata":{"_cell_guid":"c6d5c483-8071-4492-8e4c-401e217e104c","_uuid":"090481c2cbe50010dbb83256f8d1be9f1648e683"},"cell_type":"markdown","source":"# Let's Repeat the process with History size of 3 chars\n## Gather DataFrame with \"Author\", \"History\" and \"Next Char\" Fields\nUse history of size 3 characters"},{"metadata":{"_cell_guid":"24be1e75-f405-433a-bf8a-290a00241b0d","_uuid":"29a2f8516b8ef49dfcdc0edd6491b98a2e5ee1eb","_kg_hide-input":true,"collapsed":true},"cell_type":"code","execution_count":null,"outputs":[],"source":"#%% gather all quadruplets of characters into a single dataframe\nhistoryLength = 3\n\nhistoryList  = []\nnextCharList = []\nauthorList   = []\nfor k, (sentence, author) in enumerate(zip(trainText,trainLabel)):\n    decodedSentence = myunidecode(sentence.lower())\n    charSequences = [decodedSentence[k:k+historyLength+1] for k in range(len(decodedSentence)-historyLength)]\n    \n    history  = [seq[:-1] for seq in charSequences]\n    nextChar = [seq[ -1] for seq in charSequences]\n    sentenceDF = pd.DataFrame(columns=['author','history','next char'])\n    \n    historyList  += history\n    nextCharList += nextChar\n    authorList   += [author]*len(history)\n        \ncorpusDF = pd.DataFrame(columns=['author','history','next char'])\ncorpusDF['author']    = authorList\ncorpusDF['history']   = historyList\ncorpusDF['next char'] = nextCharList\n\ncorpusDF.head(8)"},{"metadata":{"_cell_guid":"8abc1939-7943-4905-8ad1-8c5fe0424466","_uuid":"363f7bd9b27b467950281a88e34e90baba21fbfe"},"cell_type":"markdown","source":"## Build Markov Model that remebers the 3 previous chars\n$$ P(c_t|c_{t-1},c_{t-2},c_{t-3},Author)  $$"},{"metadata":{"_cell_guid":"c13672f5-07f1-45e3-9445-74b0fab58d05","_uuid":"0cb0f67b2d748a518a59316f992cea4b6777aafe","_kg_hide-input":true,"collapsed":true},"cell_type":"code","execution_count":null,"outputs":[],"source":"#%% generate P(c(t)|c(t-1),c(t-2),c(t-3)) model (Markov Model with memory of 3 time steps)\nhistoryLength = 3\n\ncharCondProbModel_H3 = {}\nfor author in ['EAP','HPL','MWS']:\n    charCondProbModel_H3[author] = np.zeros( (1+historyLength)*[charEncoder.classes_.shape[0]] )\n    \ncharCondCountModel_H3 = {}\nfor author in ['EAP','HPL','MWS']:\n    charCondCountModel_H3[author] = np.zeros( (1+historyLength)*[charEncoder.classes_.shape[0]] )\n\ncorpusGroupedByAuthor = corpusDF.groupby(by='author',axis=0)\nfor author in corpusDF['author'].unique():\n    authorCorpusDF = corpusGroupedByAuthor.get_group(author).loc[:,['history','next char']].reset_index(drop=True)\n    authorCorpusGroupedByHistory = authorCorpusDF.groupby(by='history',axis=0)\n    for history in authorCorpusDF['history'].unique():\n        authorHistoryDF = authorCorpusGroupedByHistory.get_group(history).reset_index(drop=True).loc[:,'next char'].reset_index(drop=True)\n\n        encodedHistory = charEncoder.transform([ch for ch in history])\n        encodedNextCharCounts = np.histogram(charEncoder.transform(authorHistoryDF),range(len(charEncoder.classes_)+1),density=False)[0]\n        encodedNextCharProb   = np.histogram(charEncoder.transform(authorHistoryDF),range(len(charEncoder.classes_)+1),density=True)[0]\n\n        charCondProbModel_H3[author][encodedHistory[0],encodedHistory[1],encodedHistory[2],:]  = encodedNextCharProb\n        charCondCountModel_H3[author][encodedHistory[0],encodedHistory[1],encodedHistory[2],:] = encodedNextCharCounts\n\n    condCount = charCondCountModel_H3[author]\n    print('%s Sparsity level = %.1f%s' %(author, 100*(condCount < 1).sum() / (condCount > -1).sum().astype(float),'%'))\n\ncharCondProbModel_H3['all']  = (charCondProbModel_H3['EAP']  + charCondProbModel_H3['HPL']  + charCondProbModel_H3['MWS'] )/3.0\ncharCondCountModel_H3['all'] =  charCondCountModel_H3['EAP'] + charCondCountModel_H3['HPL'] + charCondCountModel_H3['MWS']\n\ncondCount = charCondCountModel_H3['all']\nprint('average Sparsity level = %.1f%s' %(100*(condCount < 1).sum() / (condCount > -1).sum().astype(float),'%'))"},{"metadata":{"_cell_guid":"39bf2f05-a0ce-4054-a03b-6befaec7b9d3","_uuid":"a2b4493ddf69fe541806e1daab0fee0c8d6d937a"},"cell_type":"markdown","source":"Note the sparsity is increasing, as one would expect."},{"metadata":{"_cell_guid":"1228ccd9-9c1d-468a-9a19-0dd8ff571e7c","_uuid":"0897b456b66d62d2f2540558559f5e93ea136a3e"},"cell_type":"markdown","source":"## Calculate Classification Accuracy of Markov Model that remebers 3 time steps back\n$$  \\mathbf{predicted\\:Author} = argmax\\:\\{ \\prod_{t=4}^{n}P(c_t|c_{t-1},c_{t-2},c_{t-3},Author)\\}   $$"},{"metadata":{"_cell_guid":"579b827a-2c51-4d3b-bec5-ae5fadddb82a","_uuid":"caf41f65851b54282fd9fdfa6c7b1ec84df07953","_kg_hide-input":true,"collapsed":true},"cell_type":"code","execution_count":null,"outputs":[],"source":"#%% meassure classification accuracy on validation set using Markov Model with memory of 3 time steps\nuniformPriorFraction    = 0.05\nallAuthorsPriorFraction = 0.05\n\nprior = np.array([1.0-uniformPriorFraction-allAuthorsPriorFraction, allAuthorsPriorFraction, uniformPriorFraction])\nuniformPriorValue = 1.0/(len(charEncoder.classes_))\n\ncondP_H3 = {}\nauthorsList = ['EAP','HPL','MWS']\nfor author in authorsList:\n    condP_H3[author]  = prior[0]*charCondProbModel_H3[author]\n    condP_H3[author] += prior[1]*charCondProbModel_H3['all']\n    condP_H3[author] += prior[2]*uniformPriorValue\n\ncondP_H3['all']  = (prior[0]+prior[1])*charCondProbModel_H3['all']\ncondP_H3['all'] += prior[2]*uniformPriorValue\n\nauthorPredictionList = []\nfor k, (sentence, author) in enumerate(zip(validText,validLabel)):\n    decodedSentence = myunidecode(sentence.lower())\n    charSequences = [decodedSentence[k:k+historyLength+1] for k in range(len(decodedSentence)-historyLength)]\n    \n    history  = [seq[:-1] for seq in charSequences]\n    nextChar = [seq[ -1] for seq in charSequences]\n    \n    logP_EAP = 0.0; logP_HPL = 0.0; logP_MWS = 0.0\n    for histChars, nextChar in zip(history,nextChar):\n        encodedHistChars = charEncoder.transform([ch for ch in histChars])\n        encodedNextChar  = charEncoder.transform([nextChar])[0]\n        \n        logP_EAP += np.log(condP_H3['EAP'][encodedHistChars[0],encodedHistChars[1],encodedHistChars[2],encodedNextChar])\n        logP_HPL += np.log(condP_H3['HPL'][encodedHistChars[0],encodedHistChars[1],encodedHistChars[2],encodedNextChar])\n        logP_MWS += np.log(condP_H3['MWS'][encodedHistChars[0],encodedHistChars[1],encodedHistChars[2],encodedNextChar])\n    \n    authorPredictionList.append(authorsList[np.argmax([logP_EAP,logP_HPL,logP_MWS])])\n\nprint(52*'-')\nprint('==> Validation Set Classification Accuracy = %.1f%s' %(100*(validLabel == authorPredictionList).mean(),'%'))\nprint(52*'-')"},{"metadata":{"_cell_guid":"3e9fad7f-1fcc-4972-8ebf-5381ca9059a3","_uuid":"2529de1d007245f95bc2cce1fb6a6d4f75a72e8b"},"cell_type":"markdown","source":"The accuracy is already quite high!"},{"metadata":{"_cell_guid":"9337aee6-6fc8-46d7-8153-2e1a644e7946","_uuid":"77b5f0ee4c108baa2d21f55eb97e0730e6e1f02e"},"cell_type":"markdown","source":"## Generate Sample Text for each Author using our 3 time step Markov Model\n$$  c_t\\: {\\raise.17ex\\hbox{$\\scriptstyle\\mathtt{\\sim}$}} \\:  P(c_t|c_{t-1},c_{t-2},c_{t-3},Author)   $$"},{"metadata":{"_cell_guid":"965200a8-b2d4-4e16-a405-b04fddf2ffed","_uuid":"f58e6ac023d3b201769d9e4a04f58ca23f08c2b3","_kg_hide-input":true,"collapsed":true},"cell_type":"code","execution_count":null,"outputs":[],"source":"#%% generate sample text by sampling one charachter at a time from the 3 time step Markov Model\nnp.random.seed(123)\n\nmaxSentenceLength = 95\nnumSentencesPerAuthor = 9\n\nuniformPriorFraction    = 0.05\nallAuthorsPriorFraction = 0.05\n\nprior = np.array([1.0-uniformPriorFraction-allAuthorsPriorFraction, allAuthorsPriorFraction, uniformPriorFraction])\nuniformPriorValue = 1.0/(len(charEncoder.classes_))\n\ncondP_H3 = {}\nauthorsList = ['EAP','HPL','MWS']\nfor author in authorsList:\n    condP_H3[author]  = prior[0]*charCondProbModel_H3[author]\n    condP_H3[author] += prior[1]*charCondProbModel_H3['all']\n    condP_H3[author] += prior[2]*uniformPriorValue\n\ncondP_H3['all']  = (prior[0]+prior[1])*charCondProbModel_H3['all']\ncondP_H3['all'] += prior[2]*uniformPriorValue\n\nfor author in ['EAP','HPL','MWS','all']:\n    print((6+maxSentenceLength)*'-')\n    print('Author %s:' %(author))\n    print(12*'-')\n    for i in range(numSentencesPerAuthor):\n        # sample c(1) ~ P(c(t))\n        firstChar  = charEncoder.classes_[np.random.choice(range(len(charCounts_EAP)),size=1,p=charProbModel[author])][0]\n        encodedFirstChar = charEncoder.transform([firstChar])[0]\n        # sample c(2) ~ P(c(t)|c(t-1))\n        secondChar = charEncoder.classes_[np.random.choice(range(len(charCounts_EAP)),size=1,p=condP_H1[author][encodedFirstChar,:])][0]\n        encodedSecondChar = charEncoder.transform([secondChar])[0]\n        # sample c(3) ~ P(c(t)|c(t-1),c(t-2))\n        thirdChar  = charEncoder.classes_[np.random.choice(range(len(charCounts_EAP)),size=1,p=condP_H2[author][encodedFirstChar,encodedSecondChar,:])][0]\n        generatedSentence = firstChar + secondChar + thirdChar\n        \n        for j in range(maxSentenceLength-1):\n            encodedHistChars = charEncoder.transform([ch for ch in generatedSentence[-historyLength:]])            \n            currCondProb = condP_H3[author][encodedHistChars[0],encodedHistChars[1],encodedHistChars[2],:]\n            currCondProb = currCondProb/currCondProb.sum() # just in case the probabilities don't sum directly to 1\n            \n            # sample c(t) ~ P(c(t)|c(t-1),c(t-2),c(t-3))\n            newChar = charEncoder.classes_[np.random.choice(range(len(charCounts_EAP)),size=1,p=currCondProb)][0]\n            generatedSentence += newChar\n            \n            if (newChar == '.') or (j == maxSentenceLength):\n                break\n        print('%d: \"%s\"' %(i+1,generatedSentence))\nprint((4+maxSentenceLength)*'-')"},{"metadata":{"_cell_guid":"38b13990-c9d2-4a40-ad4a-157984a7f553","_uuid":"5d9a744204864d9c96f8120bfa6088aa86a28243"},"cell_type":"markdown","source":"Note the large number of legal english words in the generated text.  \nOur probabalistic model has managed to learn a lot of english words.  \n\nAlso note the relativley **long** 6+ letter words the model generates, such as \"**exceeded**\", \"**remain**\", \"**expect**\" \"**danger**\" and \"**struct**\", this while our model only remebers directly 4 character sequences, it manages to concatenate several such sequences together to form a longer coherent sequence at least some of the time."},{"metadata":{"_cell_guid":"4efd64c2-ea95-4599-9649-1084ee771256","_uuid":"e296a3eac1d745db4bd37c90941316d0e4d3d390"},"cell_type":"markdown","source":"# Let's Repeat the process one last time with History size of 4 chars\n## Gather DataFrame with \"Author\", \"History\" and \"Next Char\" Fields\nUse history of size 4 characters"},{"metadata":{"_cell_guid":"00109242-cc73-44c2-809c-bde0a19fa9e7","_uuid":"a4681bb35a7bedaa0e5079188bf3a2d86af105ec","_kg_hide-input":true,"collapsed":true},"cell_type":"code","execution_count":null,"outputs":[],"source":"#%% gather all 5-wise of characters into a single dataframe\nhistoryLength = 4\n\nhistoryList  = []\nnextCharList = []\nauthorList   = []\nfor k, (sentence, author) in enumerate(zip(trainText,trainLabel)):\n    decodedSentence = myunidecode(sentence.lower())\n    charSequences = [decodedSentence[k:k+historyLength+1] for k in range(len(decodedSentence)-historyLength)]\n    \n    history  = [seq[:-1] for seq in charSequences]\n    nextChar = [seq[ -1] for seq in charSequences]\n    sentenceDF = pd.DataFrame(columns=['author','history','next char'])\n    \n    historyList  += history\n    nextCharList += nextChar\n    authorList   += [author]*len(history)\n        \ncorpusDF = pd.DataFrame(columns=['author','history','next char'])\ncorpusDF['author']    = authorList\ncorpusDF['history']   = historyList\ncorpusDF['next char'] = nextCharList\n\ncorpusDF.head(15)"},{"metadata":{"_cell_guid":"3503925f-95ac-4c5e-8da8-34df34485080","_uuid":"9eb02fb69ffac995b8d656015d68f2bf794a9c12"},"cell_type":"markdown","source":"## Build Markov Model that remebers the 4 previous chars\n$$ P(c_t|c_{t-1},c_{t-2},c_{t-3},c_{t-4},Author)  $$"},{"metadata":{"_cell_guid":"43277f04-2c10-44c5-9628-8a68e10d381a","_uuid":"965171de0597c9066be53f085a19a92660cef0fa","_kg_hide-input":true,"collapsed":true},"cell_type":"code","execution_count":null,"outputs":[],"source":"#%% generate P(c(t)|c(t-1),c(t-2),c(t-3),c(t-4)) model (Markov Model with memory of 4 time steps)\nhistoryLength = 4\n\ncharCondProbModel_H4 = {}\nfor author in ['EAP','HPL','MWS']:\n    charCondProbModel_H4[author] = np.zeros( (1+historyLength)*[charEncoder.classes_.shape[0]] )\n    \ncharCondCountModel_H4 = {}\nfor author in ['EAP','HPL','MWS']:\n    charCondCountModel_H4[author] = np.zeros( (1+historyLength)*[charEncoder.classes_.shape[0]] )\n\ncorpusGroupedByAuthor = corpusDF.groupby(by='author',axis=0)\nfor author in corpusDF['author'].unique():\n    authorCorpusDF = corpusGroupedByAuthor.get_group(author).loc[:,['history','next char']].reset_index(drop=True)\n    authorCorpusGroupedByHistory = authorCorpusDF.groupby(by='history',axis=0)\n    for history in authorCorpusDF['history'].unique():\n        authorHistoryDF = authorCorpusGroupedByHistory.get_group(history).reset_index(drop=True).loc[:,'next char'].reset_index(drop=True)\n\n        encodedHistory = charEncoder.transform([ch for ch in history])\n        encodedNextCharCounts = np.histogram(charEncoder.transform(authorHistoryDF),range(len(charEncoder.classes_)+1),density=False)[0]\n        encodedNextCharProb   = np.histogram(charEncoder.transform(authorHistoryDF),range(len(charEncoder.classes_)+1),density=True)[0]\n\n        charCondProbModel_H4[author][encodedHistory[0],encodedHistory[1],encodedHistory[2],encodedHistory[3],:]  = encodedNextCharProb\n        charCondCountModel_H4[author][encodedHistory[0],encodedHistory[1],encodedHistory[2],encodedHistory[3],:] = encodedNextCharCounts\n\n    condCount = charCondCountModel_H4[author]\n    print('%s Sparsity level = %.2f%s' %(author, 100*(condCount < 1).sum() / (condCount > -1).sum().astype(float),'%'))\n\ncharCondProbModel_H4['all']  = (charCondProbModel_H4['EAP']  + charCondProbModel_H4['HPL']  + charCondProbModel_H4['MWS'] )/3.0\ncharCondCountModel_H4['all'] =  charCondCountModel_H4['EAP'] + charCondCountModel_H4['HPL'] + charCondCountModel_H4['MWS']\n\ncondCount = charCondCountModel_H4['all']\nprint('average Sparsity level = %.2f%s' %(100*((condCount < 1).sum() / (condCount > -1).sum().astype(float)),'%'))"},{"metadata":{"_cell_guid":"54449f88-f9ca-45ab-9207-a7491d080de1","_uuid":"4817556fab2da9afaee1434546cec142dc69f82b"},"cell_type":"markdown","source":"## Calculate Classification Accuracy of Markov Model that remebers 4 time steps back\n$$  \\mathbf{predicted\\:Author} = argmax\\:\\{ \\prod_{t=5}^{n}P(c_t|c_{t-1},c_{t-2},c_{t-3},c_{t-4},Author)P(Author)\\}   $$  \n**Note:** I've added also the prior over authors, to give a small additional performance boost"},{"metadata":{"_cell_guid":"fa3de034-ee60-4a18-85f5-33b32953ce34","_uuid":"3204432d89d1da58937044b3b25f39e53fc870a8","_kg_hide-input":true,"collapsed":true},"cell_type":"code","execution_count":null,"outputs":[],"source":"#%% meassure classification accuracy on validation set using Markov Model with memory of 4 time steps\ncondP_H4_PriorWeight_specific = 70\ncondP_H4_PriorWeight_all      = 30\n\ncondP_H3_PriorWeight_specific = 70\ncondP_H3_PriorWeight_all      = 30\n\nuniformPriorWeight            = 10\n\nlogP_EAP_prior = np.log((trainLabel == 'EAP').mean())\nlogP_HPL_prior = np.log((trainLabel == 'HPL').mean())\nlogP_MWS_prior = np.log((trainLabel == 'MWS').mean())\n\nnumChars = len(charEncoder.classes_)\nprior = np.array([condP_H4_PriorWeight_specific, condP_H4_PriorWeight_all, \n                  condP_H3_PriorWeight_specific, condP_H3_PriorWeight_all, uniformPriorWeight])\nprior = prior.astype(float) / prior.sum()\n\nuniformPriorValue = 1.0/numChars\n\ncondP_H4 = {}\nauthorsList = ['EAP','HPL','MWS']\nfor author in authorsList:\n    # get P(c(t)|c(t-1),c(t-2),c(t-3),c(t-4))\n    condP_H4[author]  = prior[0]*charCondProbModel_H4[author]\n    condP_H4[author] += prior[1]*charCondProbModel_H4['all']\n    \n    # get \"prior\" from P(c(t)|c(t-1),c(t-2),c(t-3))\n    condP_H4_from_CondP_H3_specific = np.tile(charCondProbModel_H3[author][np.newaxis,:,:,:],[numChars,1,1,1,1])\n    condP_H4_from_CondP_H3_all      = np.tile(charCondProbModel_H3['all'][np.newaxis,:,:,:],[numChars,1,1,1,1])\n    condP_H4[author] += prior[2]*condP_H4_from_CondP_H3_specific\n    condP_H4[author] += prior[3]*condP_H4_from_CondP_H3_all\n\n    condP_H4[author] += prior[4]*uniformPriorValue\n\ncondP_H4['all']  = (condP_H4['EAP'] + condP_H4['HPL'] + condP_H4['MWS'])  / 3.0\n\nauthorPredictionList = []\nlogProbGivenAuthor = np.zeros((len(validLabel),3))\nfor i, (sentence, author) in enumerate(zip(validText,validLabel)):\n    decodedSentence = myunidecode(sentence.lower())\n    charSequences = [decodedSentence[k:k+historyLength+1] for k in range(len(decodedSentence)-historyLength)]\n    \n    history  = [seq[:-1] for seq in charSequences]\n    nextChar = [seq[ -1] for seq in charSequences]\n    \n    logP_EAP = logP_EAP_prior; logP_HPL = logP_HPL_prior; logP_MWS = logP_MWS_prior;\n    for histChars, nextChar in zip(history,nextChar):\n        encodedHistChars = charEncoder.transform([ch for ch in histChars])\n        encodedNextChar  = charEncoder.transform([nextChar])[0]\n        \n        logP_EAP += np.log(condP_H4['EAP'][encodedHistChars[0],encodedHistChars[1],encodedHistChars[2],encodedHistChars[3],encodedNextChar])\n        logP_HPL += np.log(condP_H4['HPL'][encodedHistChars[0],encodedHistChars[1],encodedHistChars[2],encodedHistChars[3],encodedNextChar])\n        logP_MWS += np.log(condP_H4['MWS'][encodedHistChars[0],encodedHistChars[1],encodedHistChars[2],encodedHistChars[3],encodedNextChar])\n        \n        logProbGivenAuthor[i,:] = [logP_EAP,logP_HPL,logP_MWS]\n        \n    authorPredictionList.append(authorsList[np.argmax([logP_EAP,logP_HPL,logP_MWS])])\n\nprint(52*'-')\nprint('==> Validation Set Classification Accuracy = %.1f%s' %(100*(validLabel == authorPredictionList).mean(),'%'))\nprint(52*'-')"},{"metadata":{"_cell_guid":"aba94fa4-03f8-4a91-b5ea-732d52188f6c","_uuid":"bbdd8d5fea84254da5ee54cd960588e8075ad3b0"},"cell_type":"markdown","source":"The identification accuracy has reached quite a high level now.   \n\nBut we do see a saturation effect here. The improvment from history of 3 characters to 4 characters is not as large as the improvment from 2 character history to 3 character history."},{"metadata":{"_cell_guid":"dd26fe14-5e96-4cbf-b951-4cba641d9720","_uuid":"079a4d50d312f0e7be40cd71abb1985dbd0fdffe"},"cell_type":"markdown","source":"## Let's calculate also the log loss\nIn order to relate this to LB results"},{"metadata":{"_cell_guid":"34e8eaad-799b-419a-a40e-fdfb1586e923","_uuid":"c128219c6d471257596c3d5d16c95a6f0d4c67cc","_kg_hide-input":true,"collapsed":true},"cell_type":"code","execution_count":null,"outputs":[],"source":"#%% calculate log loss\nminimalLogP = -15.0\nuniformPriorWeight = 0.09\n\nauthorLogProb_norm = logProbGivenAuthor - np.tile(logProbGivenAuthor.max(axis=1)[:,np.newaxis], [1,3])\nauthorLogProb_norm[authorLogProb_norm < minimalLogP] = minimalLogP\n\nauthorProb = np.exp(authorLogProb_norm)\nauthorProb_norm = authorProb / np.tile(authorProb.sum(axis=1)[:,np.newaxis],[1, 3])\n\ny_Hat = uniformPriorWeight*(1/3.0) + (1.0-uniformPriorWeight)*authorProb_norm\n\nlabelEncoder = preprocessing.LabelEncoder()\ny_GT = labelEncoder.fit_transform(validLabel)\n\nprint(34*'-')\nprint('Validation Set Log Loss = %.5f' %(log_loss(y_GT, y_Hat)))\nprint(34*'-')"},{"metadata":{"_cell_guid":"a0e6a97f-e745-4518-978b-60025bcb816c","_uuid":"f1d84fc1136fe625a005e6fd7b97e0748a021015"},"cell_type":"markdown","source":"## Generate Sample Text for each Author using our 4 time step Markov Model\n$$  c_t\\: {\\raise.17ex\\hbox{$\\scriptstyle\\mathtt{\\sim}$}} \\:  P(c_t|c_{t-1},c_{t-2},c_{t-3},c_{t-4},Author)   $$  \n**Just for fun**, let's start all sentences with 'disp' and see how they evolve from there"},{"metadata":{"_cell_guid":"c1b25e6f-1e32-4791-9cba-9ad5ab81c862","_uuid":"7fb851655e4985542daf35d1473e528306645551","_kg_hide-input":true,"collapsed":true},"cell_type":"code","execution_count":null,"outputs":[],"source":"np.random.seed(1000)\n\nmaxSentenceLength = 95\nnumSentencesPerAuthor = 9\n\ncondP_H4_PriorWeight_specific = 10\ncondP_H4_PriorWeight_all      = 10\n\ncondP_H3_PriorWeight_specific = 1\ncondP_H3_PriorWeight_all      = 1\n\nuniformPriorWeight            = 1\n\nnumChars = len(charEncoder.classes_)\nprior = np.array([condP_H4_PriorWeight_specific, condP_H4_PriorWeight_all, \n                  condP_H3_PriorWeight_specific, condP_H3_PriorWeight_all, uniformPriorWeight])\nprior = prior.astype(float) / prior.sum()\n\nuniformPriorValue = 1.0/numChars\n\ncondP_H4 = {}\nauthorsList = ['EAP','HPL','MWS']\nfor author in authorsList:\n    # get P(c(t)|c(t-1),c(t-2),c(t-3),c(t-4))\n    condP_H4[author]  = prior[0]*charCondProbModel_H4[author]\n    condP_H4[author] += prior[1]*charCondProbModel_H4['all']\n    \n    # get prior from P(c(t)|c(t-1),c(t-2),c(t-3))\n    condP_H4_from_CondP_H3_specific = np.tile(charCondProbModel_H3[author][np.newaxis,:,:,:],[numChars,1,1,1,1])\n    condP_H4_from_CondP_H3_all      = np.tile(charCondProbModel_H3['all'][np.newaxis,:,:,:],[numChars,1,1,1,1])\n    condP_H4[author] += prior[2]*condP_H4_from_CondP_H3_specific\n    condP_H4[author] += prior[3]*condP_H4_from_CondP_H3_all\n\n    condP_H4[author] += prior[4]*uniformPriorValue\n\ncondP_H4['all']  = (condP_H4['EAP'] + condP_H4['HPL'] + condP_H4['MWS'])  / 3.0\n\nfor author in ['EAP','HPL','MWS','all']:\n    print((6+maxSentenceLength)*'-')\n    print('Author %s:' %(author))\n    print(12*'-')\n    for i in range(numSentencesPerAuthor):\n        generatedSentence = 'disp'\n        for j in range(maxSentenceLength-1):\n            encodedHistChars = charEncoder.transform([ch for ch in generatedSentence[-historyLength:]])            \n            currCondProb = condP_H4[author][encodedHistChars[0],encodedHistChars[1],encodedHistChars[2],encodedHistChars[3],:]\n            currCondProb = currCondProb/currCondProb.sum() # just in case the probabilities don't sum exactly to 1\n            \n            # sample c(t) ~ P(c(t)|c(t-1),c(t-2),c(t-3))\n            newChar = charEncoder.classes_[np.random.choice(range(len(charCounts_EAP)),size=1,p=currCondProb)][0]\n            generatedSentence += newChar\n            \n            if (newChar == '.') or (j == maxSentenceLength):\n                break\n        print('%d: \"%s\"' %(i+1,generatedSentence))\nprint((4+maxSentenceLength)*'-')"},{"metadata":{"_cell_guid":"6c36d7db-d4b8-4543-be5f-571375d7b483","_uuid":"96c7373a14fd3e16f601106174bb50a1d751d2df"},"cell_type":"markdown","source":"Here we can almost see sentences:  \n\"...and i says: \"yes, the...\"  \n\"...when i spoken to put to think till my feel the also...\"   \n\"...displayed on the done dark happines atter...\"  \n\"...\"why, so much as countain, and sent...\"\n\nSince we really know exactly what the model entails and it's simplicity, i.e. just storing conditional probabilities, and we see the generative performance of this model, that can remember english words and almost construct something that looks like actual sentences, this might make some of us wonder \"could it be that our brains are just a somewhat better probability estimation machine?\"  \nMy answer to this question would be \"most likely yes\" :-)"},{"metadata":{"_cell_guid":"2161e250-74f1-4cea-9b62-de9616ca74f3","_uuid":"3f3ada711f84bd0ad1c66c81ad9a2ef23ed39a57"},"cell_type":"markdown","source":"# Create a Submission on the Test Set\nThis submission might be useful for an ensemble if you haven't used any char based models yet"},{"metadata":{"_cell_guid":"7738dff7-aded-4a31-84e9-34349d4995cc","_uuid":"fdc57136c72cf8708ee2e303df4a759236c90f43","_kg_hide-input":true,"collapsed":true},"cell_type":"code","execution_count":null,"outputs":[],"source":"#%% create a submission\n# load test data\ntestData = pd.read_csv('../input/test.csv')\ntestText = testData.loc[:,'text'].reset_index(drop=True)\n\n# calculate log prob predictions\nlogProbGivenAuthor = np.zeros((len(testText),3))\nfor i, sentence in enumerate(testText):\n    decodedSentence = myunidecode(sentence.lower())\n    charSequences = [decodedSentence[k:k+historyLength+1] for k in range(len(decodedSentence)-historyLength)]\n    \n    history  = [seq[:-1] for seq in charSequences]\n    nextChar = [seq[ -1] for seq in charSequences]\n    \n    logP_EAP = logP_EAP_prior; logP_HPL = logP_HPL_prior; logP_MWS = logP_MWS_prior;\n    for histChars, nextChar in zip(history,nextChar):\n        encodedHistChars = charEncoder.transform([ch for ch in histChars])\n        encodedNextChar  = charEncoder.transform([nextChar])[0]\n        \n        logP_EAP += np.log(condP_H4['EAP'][encodedHistChars[0],encodedHistChars[1],encodedHistChars[2],encodedHistChars[3],encodedNextChar])\n        logP_HPL += np.log(condP_H4['HPL'][encodedHistChars[0],encodedHistChars[1],encodedHistChars[2],encodedHistChars[3],encodedNextChar])\n        logP_MWS += np.log(condP_H4['MWS'][encodedHistChars[0],encodedHistChars[1],encodedHistChars[2],encodedHistChars[3],encodedNextChar])\n    \n        logProbGivenAuthor[i,:] = [logP_EAP,logP_HPL,logP_MWS]"},{"metadata":{"_cell_guid":"bfeb3c3f-29cd-4906-a974-e8f18c2ed2e5","_uuid":"01fee243bf426df9372514b1d40ef69dadb66eb0","collapsed":true},"cell_type":"code","execution_count":null,"outputs":[],"source":"# convert log probabilities to final predictions\nminimalLogP = -15.0\nuniformPriorWeight = 0.09\n\nauthorLogProb_norm = logProbGivenAuthor - np.tile(logProbGivenAuthor.max(axis=1)[:,np.newaxis], [1,3])\nauthorLogProb_norm[authorLogProb_norm < minimalLogP] = minimalLogP\nauthorProb = np.exp(authorLogProb_norm)\nauthorProb_norm = authorProb / np.tile(authorProb.sum(axis=1)[:,np.newaxis],[1, 3])\ny_Hat = uniformPriorWeight*(1/3.0) + (1.0-uniformPriorWeight)*authorProb_norm\n\n# write a submission\nsubmission = pd.read_csv('../input/sample_submission.csv')\nsubmission.loc[:,['EAP', 'HPL', 'MWS']] = y_Hat\nsubmission.to_csv(\"Markov_char_given_4charHistory.csv\", index=False)\nsubmission.head(10)"},{"metadata":{"_cell_guid":"0a892f17-360a-41e5-a7a7-f40b1955dda6","_uuid":"c0367ce89cd1def7d8a5a55c1982f90e2f6413e8"},"cell_type":"markdown","source":"# Apply Fully Discriminative Approach\n1. Extract **Bag of Character n-grams** features\n1. Create a submission for **Logistic Regression over *BagOfChar***\n1. Extract **Bag of Word n-grams** features\n1. Create a submission for **Logistic Regression over *BagOfWord***\n1. Create a submission for **Logistic Regression over both *BagOfWord and BagOfChar***"},{"metadata":{"_cell_guid":"91a7fa24-c908-4410-b0ff-447c15eeb6f4","_uuid":"6e4a5e8c61b98a16a97cf20b9c1af0e803984cb9"},"cell_type":"markdown","source":"### 1. Extract **Bag of Character n-grams** features"},{"metadata":{"_cell_guid":"3102d943-bbf7-4513-8112-aa70a013b51a","_uuid":"0a16e192a725f5bb817847b9ec5e417900d8df4c","_kg_hide-input":true,"collapsed":true},"cell_type":"code","execution_count":null,"outputs":[],"source":"import time\nimport scipy\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n#%% Create a Bag of Char n-grams + logistic regression model\nngramLength = 5\n\nfeatureExtractionStartTime = time.time()\nprint('-'*52)\nprint('fitting \"CountVectorizer()\" for bag of char %d-grams' %(ngramLength))\n\nBagOfCharsExtractor = CountVectorizer(min_df=8, max_features=250000, \n                                      analyzer='char', ngram_range=(1,ngramLength), \n                                      binary=False,lowercase=True)\n\nBagOfCharsExtractor.fit(pd.concat((trainText,validText,testText)))\n\nX_train_char = BagOfCharsExtractor.transform(trainText)\nX_valid_char = BagOfCharsExtractor.transform(validText)\nX_test_char  = BagOfCharsExtractor.transform(testText)\n\nfeatureExtractionDurationInMinutes = (time.time()-featureExtractionStartTime)/60.0\nprint(\"feature extraction took %.2f minutes\" % (featureExtractionDurationInMinutes))\nprint('number of \"bag of char %d-gram\" features = %d' %(ngramLength, X_train_char.shape[1]))\nprint('-'*52)\n\n# scale inputs so that they will be in similar value range\nstdScaler = preprocessing.StandardScaler(with_mean=False)\nstdScaler.fit(scipy.sparse.vstack(((X_train_char,X_valid_char,X_test_char))))\n\nX_train_norm = stdScaler.transform(X_train_char)\nX_valid_norm = stdScaler.transform(X_valid_char)\nX_test_norm  = stdScaler.transform(X_test_char)\n\n# create labels for classification\nyLabelEncoder = preprocessing.LabelEncoder()\nyLabelEncoder.fit(pd.concat((trainLabel,validLabel)))\n\ny_train = yLabelEncoder.transform(trainLabel)\ny_valid = yLabelEncoder.transform(validLabel)\n\n##%% check performance on validation set\nvalidationStartTime = time.time()\nprint('-'*42)\nprint('fitting \"LogisticRegression()\" classifier')\n\nlogisticRegressor_char = linear_model.LogisticRegression(C=0.01, solver='sag')\nlogisticRegressor_char.fit(X_train_norm, y_train)\n\ntrainAccuracy = accuracy_score(y_train, logisticRegressor_char.predict(X_train_norm))\nvalidAccuracy = accuracy_score(y_valid, logisticRegressor_char.predict(X_valid_norm))\ntrainLogLoss  = log_loss(y_train, logisticRegressor_char.predict_proba(X_train_norm))\nvalidLogLoss  = log_loss(y_valid, logisticRegressor_char.predict_proba(X_valid_norm))\n\nvalidationDurationInMinutes = (time.time()-validationStartTime)/60.0\n\nprint('Validation took %.2f minutes' % (validationDurationInMinutes))\nprint('Train: %.1f%s Accuracy, log loss = %.4f' % (100*trainAccuracy,'%',trainLogLoss))\nprint('Valid: %.1f%s Accuracy, log loss = %.4f' % (100*validAccuracy,'%',validLogLoss))\nprint('-'*42)"},{"metadata":{"_cell_guid":"36f18513-a280-4a63-81e8-75fef2740be4","_uuid":"06412dd5b3bcf5674bf7536b59a295740e52a05d"},"cell_type":"markdown","source":"### 2. Create a submission for **Logistic Regression over *BagOfChar***\n"},{"metadata":{"_cell_guid":"1fd3a31d-7e77-40f9-9307-092ea4ee2861","_uuid":"20ede82fde50cc4430d11459b1d4ccdd3fa85db3","_kg_hide-input":true,"collapsed":true},"cell_type":"code","execution_count":null,"outputs":[],"source":"# write a submission\nsubmission = pd.read_csv('../input/sample_submission.csv')\nsubmission.loc[:,yLabelEncoder.classes_.tolist()] = logisticRegressor_char.predict_proba(X_test_norm)\nsubmission.to_csv(\"LogisticRegression_Over_BagOfCharNGrams.csv\", index=False)\nsubmission.head(10)"},{"metadata":{"_cell_guid":"debe90b5-8ff6-46e1-be50-c9448bc0f5ef","_uuid":"98f4cb50c786298a01cef42e3e47539a06368ac8"},"cell_type":"markdown","source":"### 3. Extract **Bag of Word n-grams** features"},{"metadata":{"_cell_guid":"02dcd5a2-9333-4003-baec-2dd7f70293c5","_uuid":"b1915889af860022ec2294afe5965f77997e4fa2","_kg_hide-input":true,"collapsed":true},"cell_type":"code","execution_count":null,"outputs":[],"source":"ngramLength = 2\n\nfeatureExtractionStartTime = time.time()\nprint('-'*52)\nprint('fitting \"CountVectorizer()\" for bag of word %d-grams' %(ngramLength))\n\nBagOfWordsExtractor = CountVectorizer(min_df=5, max_features=250000, \n                                      analyzer='word', ngram_range=(1,ngramLength), \n                                      binary=False,lowercase=True)\n\nBagOfWordsExtractor.fit(pd.concat((trainText,validText,testText)))\n\nX_train_word = BagOfWordsExtractor.transform(trainText)\nX_valid_word = BagOfWordsExtractor.transform(validText)\nX_test_word  = BagOfWordsExtractor.transform(testText)\n\nfeatureExtractionDurationInMinutes = (time.time()-featureExtractionStartTime)/60.0\nprint(\"feature extraction took %.2f minutes\" % (featureExtractionDurationInMinutes))\nprint('number of \"bag of word %d-gram\" features = %d' %(ngramLength, X_train_word.shape[1]))\nprint('-'*52)\n\n# scale inputs so that they will be in similar value range\nstdScaler = preprocessing.StandardScaler(with_mean=False)\nstdScaler.fit(scipy.sparse.vstack(((X_train_word,X_valid_word,X_test_word))))\n\nX_train_norm = stdScaler.transform(X_train_word)\nX_valid_norm = stdScaler.transform(X_valid_word)\nX_test_norm  = stdScaler.transform(X_test_word)\n\n#£%% check performance on validation set\nvalidationStartTime = time.time()\nprint('-'*42)\nprint('fitting \"LogisticRegression()\" classifier')\n\nlogisticRegressor_word = linear_model.LogisticRegression(C=0.01, solver='sag')\nlogisticRegressor_word.fit(X_train_norm, y_train)\n\ntrainAccuracy = accuracy_score(y_train, logisticRegressor_word.predict(X_train_norm))\nvalidAccuracy = accuracy_score(y_valid, logisticRegressor_word.predict(X_valid_norm))\ntrainLogLoss  = log_loss(y_train, logisticRegressor_word.predict_proba(X_train_norm))\nvalidLogLoss  = log_loss(y_valid, logisticRegressor_word.predict_proba(X_valid_norm))\n\nvalidationDurationInMinutes = (time.time()-validationStartTime)/60.0\n\nprint('Validation took %.2f minutes' % (validationDurationInMinutes))\nprint('Train: %.1f%s Accuracy, log loss = %.4f' % (100*trainAccuracy,'%',trainLogLoss))\nprint('Valid: %.1f%s Accuracy, log loss = %.4f' % (100*validAccuracy,'%',validLogLoss))\nprint('-'*42)"},{"metadata":{"_cell_guid":"57a32d34-6062-4478-9012-51d2c314d1f9","_uuid":"4c2260d9410952cf8acfbb9c9a44281c8fb26799"},"cell_type":"markdown","source":"### 4. Create a submission for **Logistic Regression over *BagOfWord***"},{"metadata":{"_cell_guid":"2cb3a48a-7dfc-47ec-9e41-278cd8458d6e","_uuid":"5e50837618a6e9575e9dd2d7fa31156a358e7640","_kg_hide-input":true,"collapsed":true},"cell_type":"code","execution_count":null,"outputs":[],"source":"# write a submission\nsubmission = pd.read_csv('../input/sample_submission.csv')\nsubmission.loc[:,yLabelEncoder.classes_.tolist()] = logisticRegressor_word.predict_proba(X_test_norm)\nsubmission.to_csv(\"LogisticRegression_Over_BagOfWordNGrams.csv\", index=False)\nsubmission.head(10)"},{"metadata":{"_cell_guid":"d808a158-3f77-4893-80df-a6b4e23d139b","_uuid":"2d85f23089cc64f4aa837e0f17c3fab79019ead1"},"cell_type":"markdown","source":"### 5. Create a submission for **Logistic Regression over both *BagOfWord and BagOfChar***"},{"metadata":{"_cell_guid":"4ea035b3-511f-4b37-82f5-e7a879228b0c","_uuid":"ff600f404b0c60a6a223cb173d6cc9fbe5b70f22","_kg_hide-input":true,"collapsed":true},"cell_type":"code","execution_count":null,"outputs":[],"source":"#%% combine word and char features\n\n# combine and scale features \nX_train = scipy.sparse.hstack((X_train_word,X_train_char))\nX_valid = scipy.sparse.hstack((X_valid_word,X_valid_char))\nX_test  = scipy.sparse.hstack((X_test_word,X_test_char))\n\nstdScaler = preprocessing.StandardScaler(with_mean=False)\nstdScaler.fit(scipy.sparse.vstack(((X_train,X_valid,X_test))))\n\nX_train = stdScaler.transform(X_train)\nX_valid = stdScaler.transform(X_valid)\nX_test  = stdScaler.transform(X_test)\n\n##%% check performance on validation set\n\nvalidationStartTime = time.time()\nprint('-'*42)\nprint('fitting \"LogisticRegression()\" classifier')\n\nlogisticRegressor = linear_model.LogisticRegression(C=0.01, solver='sag')\nlogisticRegressor.fit(X_train, y_train)\n\ntrainAccuracy = accuracy_score(y_train, logisticRegressor.predict(X_train))\ntrainLogLoss = log_loss(y_train, logisticRegressor.predict_proba(X_train))\nvalidAccuracy = accuracy_score(y_valid, logisticRegressor.predict(X_valid))\nvalidLogLoss = log_loss(y_valid, logisticRegressor.predict_proba(X_valid))\n\nvalidationDurationInMinutes = (time.time()-validationStartTime)/60.0\n\nprint('Validation took %.2f minutes' % (validationDurationInMinutes))\nprint('Train: %.1f%s Accuracy, log loss = %.4f' % (100*trainAccuracy,'%',trainLogLoss))\nprint('Valid: %.1f%s Accuracy, log loss = %.4f' % (100*validAccuracy,'%',validLogLoss))\nprint('-'*42)\n\n# write a submission\nsubmission = pd.read_csv('../input/sample_submission.csv')\nsubmission.loc[:,yLabelEncoder.classes_.tolist()] = logisticRegressor.predict_proba(X_test)\nsubmission.to_csv(\"LogisticRegression_Over_BagOfWord_BagOfChar.csv\", index=False)\nsubmission.head(10)"},{"metadata":{"_cell_guid":"f3d87f19-538e-4e88-a0f6-de91e97976b3","_uuid":"4330eef31c9afd2759ebaf627b34ef0dfc25f4dc","collapsed":true},"cell_type":"code","execution_count":null,"outputs":[],"source":""}]}