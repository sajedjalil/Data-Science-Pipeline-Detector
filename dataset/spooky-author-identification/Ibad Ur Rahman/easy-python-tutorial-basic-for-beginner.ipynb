{"cells":[{"metadata":{"_cell_guid":"eab2b800-00c3-43bd-b080-3bea260d2562","_uuid":"d3a0b4f9cb885a49d1c65693a0efef726045ca5b"},"cell_type":"markdown","source":"Upvote ! if you find it useful \n\n\n**TUTORIAL for BEGINNERS ON HOW TO SOLVE ALMOST ANY  TEXT CLASSIFICATION PROBLEM USING NAIVE BAYES**\n**So This is a very basic TUTORIAL, For newcomers. I will be using simple naive bayes along with Count Vectorization to solve this problem**\n\nSo Lets Start.\n\nLets Understand the hot terms first:\n\n**Naive Bayes Classifier**\n\n**It is a simple classifier based on bayes theorem with full independence between different features.**  I know it passed right from above the head.\n\nLets find a simple explanation:\n* So we have a dataset with [text, label] e.g.[\"This process, however, afforded me no means of...\"\t,EAP] where EAP represents the author so we have a text and we have to find out which author out of three wrote that text.\n* Now First of all computer does not understand alphabets or words, so we convert the words into numbers so make computer understand it\n* For that we simply assign a number for each unique word, e.g.  \"to be or not to be\" will be assigned numbers like to:1, be:2, or:3, not:4.\n* Now we can send input to the computer like [1,2,3,4,1,2] \n* But naive bayes does not want this it just wants the count of each words.\n* Naive bayes like the count of \"to\" or \"1\" in document1 or row1 and so on\n* So we give naive bayes input like this [0,1]:2   [0,2]:2   [0,3]:1 and so on. We are giving it the count of each word and each document so zero represents first document.\n* So we are going to have a big two dimensional matrix with lots of columns and rows equal to the number of documents given\n\nNow lets look at naive bayes again, Naive bayes considers each words as independant and does not value the position of each word: (the postion of word have no role in classifying or training in naive bayes)\n\nSecondly it is based on probability and bayes theorem, I am not going to explain it here as it will take more than necessary space so Chapter 13 of **An Introduction to information retreival by Christopher D manning , Prabhakar Raghavan\nHinrich Sch√ºtze ** Section 13.2 will explain us the naive bayes in detail\n\nhttps://nlp.stanford.edu/IR-book/pdf/13bayes.pdf\n\n\nNow lets come back to the dataset\n\n\nSo lets see what we got, we got a training data set with author labels and testing datasets without labels. In this we just take the training data set, Split it by using 70% for training the Naive Bayes classifier and other 30% for testing it.\nHere is the basic code and easiest one.\n\n\nSo first of all we import libraries then we load the csv file into a pandas dataframe, or in layman terms, into a python variable so that we can process it.\n\nThen we see a sample of the data using training_data.head().\n"},{"metadata":{"_cell_guid":"baa4621c-632c-4370-913d-9e373eb2a34f","collapsed":true,"_uuid":"7e4b1390a70253b31fb5da682abcca53c02d143f","trusted":false},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n#READING INPUT\ntraining_data = pd.read_csv(\"../input/train.csv\")\ntesting_data=pd.read_csv(\"../input/test.csv\")\ntraining_data.head()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"7a1092a3-e399-4e4e-9417-908194e50781","_uuid":"783a3f145d547a8f22486b8068f331c02b245e43"},"cell_type":"markdown","source":"Now by just looking at the top 4 entries, it must be clear that does id have any role in determining who the author is?\nNo. Id is just used as a identifier for text NOT for author. So one thing is clear over here, that id is useless and would not help in any way to our model to learn.\n\nSo now we simple omit the id.\nSimilarly to avoid clutterness we map \"EAP\" to 0 \"HPL\" to 1 and \"MWS\" to 2 as it will be more convenient for our classifier. \nIn other words we are just telling our computer that if classifier predicts 0 for the text then it means that it is preicting \"EAP\", if 1 then it means that it is predicting \"HPL\", if 2 then it means that it is predicting 2.\n\nNext we take all the rows under the column named \"text\" and put it in X ( a variable in python)\n\nSimilarly we take all rows under the column named \"author_num\" and put it in y (a variable in python)"},{"metadata":{"_cell_guid":"ac7c99f0-8dbb-4325-ba16-561821a40355","collapsed":true,"_uuid":"e18421f1e4cbc94ae771804ee7851ca773c94793","trusted":false},"cell_type":"code","source":"training_data['author_num'] = training_data.author.map({'EAP':0, 'HPL':1, 'MWS':2})\nX = training_data['text']\ny = training_data['author_num']\nprint (X.head())\nprint (y.head())\n\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"e9c627b9-1d2f-4ea9-a9fc-c950e6815c74","_uuid":"441cc367f1ab4a81af28b32e4f18b4bee214b5d6"},"cell_type":"markdown","source":"Now we got the data, we got the text and the corresponing label, Now we need to split the data into training set and testing set.\nTesting set is the one which we will never show to the computer, we will take it and keep it in a safe and only use it to test the model.\n\nSo we are going to split it into 70% for training and 30% for testing."},{"metadata":{"_cell_guid":"9159cab8-4eee-4e23-9f48-89b59e22e778","collapsed":true,"_uuid":"a48996b8f1273b24a51d1f727a132076b982eaf5","trusted":false},"cell_type":"code","source":"per=int(float(0.7)* len(X))\nX_train=X[:per]\nX_test=X[per:]\ny_train=y[:per]\ny_test=y[per:]","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"7fd81da0-5dee-4e16-8f2f-4688a3a77a9c","_uuid":"cae27e89722e4ae2209b31e4f94aa851ae5b4a48"},"cell_type":"markdown","source":"Here are some libraries we are going to need"},{"metadata":{"_cell_guid":"3e906dd6-0080-497e-8ab4-defe8249bc3f","collapsed":true,"_uuid":"f523dd13e0873798820f0789eb7fa6d19179923b","trusted":false},"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.naive_bayes import MultinomialNB","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"32e1ca38-e3bf-4305-a72c-be0355dc32b7","_uuid":"48425a6a44175b8cecf785c98e34139e1a56e512"},"cell_type":"markdown","source":"**Now comes the most important part.\nVectorization**\n\nWe see that computers get crazy with text, It only understands numbers, but we have got to classify text. Now what do we do?\nWe do tokenization and vectorization to save the count of each word. Confused right.\nkeep on reading believe me you will get it in the end.\nNow let say you have got a text like \"My name is computer My life\". so what exactly does the vectorizer do.??\n\nLets see,\nfirst it breaks it into tokens something like this. \n\nMy\n\nname\n\nis\n\ncomputer\n\nMy\n\nlife\n\nPretty easy right\nNow it first creates a vocabulary from it\ne.g\nMy:0\ncomputer:1\nis:2\nlife:3\nname:4\n\nStill very easy\n\n\n\nNow we see that we create a sparse matrix out of it,  \n\n\n\nWhich have 1 row and 5 columns are there were 5 unique tokens in our text\n\n\n\nwe see that matrix[0,0] is 2 which specifies that 0 item in dictionary which is My came 2 times and so on.\n"},{"metadata":{"_cell_guid":"81c2acb1-2222-4756-abf3-e0a2318a3135","collapsed":true,"_uuid":"899cfbdc7c6fe865d9296ef39e24282fd3bc9cef","trusted":false},"cell_type":"code","source":"#toy example\ntext=[\"My name is computer My life\"]\ntoy = CountVectorizer(lowercase=False, token_pattern=r'\\w+|\\,')\ntoy.fit_transform(text)\nprint (toy.vocabulary_)\nmatrix=toy.transform(text)\nprint (matrix[0,0])\nprint (matrix[0,1])\nprint (matrix[0,2])\nprint (matrix[0,3])\nprint (matrix[0,4])\n\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"bc773376-b322-4798-bf6c-35119a68c255","_uuid":"04fab449654c6216152d6102ae910cd4375be8e9"},"cell_type":"markdown","source":"Exacly like the above toy example we vectorize the text"},{"metadata":{"_cell_guid":"3088f721-682f-4b22-8dad-bbdfacc52fe4","collapsed":true,"_uuid":"2446f83e52889c77616628d8b637373b35b358c1","trusted":false},"cell_type":"code","source":"vect = CountVectorizer(lowercase=False, token_pattern=r'\\w+|\\,')\nX_cv=vect.fit_transform(X)\nX_train_cv = vect.transform(X_train)\nX_test_cv = vect.transform(X_test)\nprint (X_train_cv.shape)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"1c84fc29-5309-443f-b58d-8303559b892d","_uuid":"3b5274db4ab68f921fddde3e9ab0948b05f189d4"},"cell_type":"markdown","source":"Here comes the final step\nWe give the data to the clf.fit for training and test it for score.\nWe have not used log score over here for simplicity."},{"metadata":{"_cell_guid":"265e2ec4-0f55-48b9-85d5-a62c69c7cb64","collapsed":true,"_uuid":"8e83616eafc6f6734c3bc2c749be302eb5ab5459","trusted":false},"cell_type":"code","source":"clf=MultinomialNB()\nclf.fit(X_train_cv, y_train)\nclf.score(X_test_cv, y_test)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"835c6e9d-2719-4751-9df2-590caf7109f6","_uuid":"9b4397c5a76bfd828d30ea60585976ee11495c95"},"cell_type":"markdown","source":"Now we saw the accuracy on our training data we made for ourself, Now we will let the kaggle test our accuracy. So first of all we will update our vocabulary and transform raw TEST data from kaggle into vectorized form"},{"metadata":{"_cell_guid":"8db3b00f-0095-4a0f-9514-c5796da715cf","collapsed":true,"_uuid":"cd122cf728d7eec41b89b86853abeb87dfa6fffc","trusted":false},"cell_type":"code","source":"X_test=vect.transform(testing_data[\"text\"])\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"99ffae66-e2da-421d-a3db-c6e6e3508941","_uuid":"95a02567299b372eca9f63e78213eb8f7f673887"},"cell_type":"markdown","source":"Now we have successfully vectorized the data given by kaggle Now we fit the whole training data without any split into our Naive Bayes Model\nNext we give it the testing vectorized data to predict the probabilities"},{"metadata":{"_cell_guid":"fe9d635a-f847-45cc-adfd-d5bb9a0ff7c0","collapsed":true,"_uuid":"e5bd16df37ee535c8da3d4658e7b12edd06ac036","trusted":false},"cell_type":"code","source":"clf=MultinomialNB()\nclf.fit(X_cv, y)\npredicted_result=clf.predict_proba(X_test)\npredicted_result.shape","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"a4bc45db-8a97-4971-a07f-d6a7b59afd4c","_uuid":"b3e80f38e96182cccd31b3fc474424d0e0c3e8a9"},"cell_type":"markdown","source":"We see that we got a result with 8392 rows presenting each text and 3 columns each column representing probability of each author."},{"metadata":{"_cell_guid":"06525527-53ee-4b81-80e7-5968868a4237","collapsed":true,"_uuid":"4d40835e7ba3aa4698a8382b69e0d3d09a4dd26a","trusted":false},"cell_type":"code","source":"#NOW WE CREATE A RESULT DATA FRAME AND ADD THE COLUMNS NECESSARY TO SUBMIT HERE\nresult=pd.DataFrame()\nresult[\"id\"]=testing_data[\"id\"]\nresult[\"EAP\"]=predicted_result[:,0]\nresult[\"HPL\"]=predicted_result[:,1]\nresult[\"MWS\"]=predicted_result[:,2]\nresult.head()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"6b21da61-edc0-4d00-a9c3-00a89d49891f","_uuid":"12e3bb4580a7364b62e91d709f4429410493f8e8"},"cell_type":"markdown","source":"FINALLY WE SUBMIT THE RESULT TO KAGGLE FOR EVALUATION"},{"metadata":{"_cell_guid":"dd5be74b-94c3-4ce4-af92-d5c643571159","collapsed":true,"_uuid":"a13b01920608da7604bbb87d126c1f6e71e71259","trusted":false},"cell_type":"code","source":"result.to_csv(\"TO_SUBMIT.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"366002b321935dda3c544c6f5c9a418f89070130"},"cell_type":"markdown","source":"****PLEASE UPVOTE IF YOU FIND IT USEFUL****"}],"metadata":{"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":1}