{"cells":[{"source":"# Consist of following  sections\n* Loading Data\n* NLP preprocessing\n* Spliting of tran and validation\n* Data Analysis\n* Scikit Model tryouts\n* Tensorflow Model tryouts\n","cell_type":"markdown","metadata":{"_uuid":"aec0c6d8845ced3259bff9d02be52eb205daabc8","_cell_guid":"bd98906b-429e-467d-abff-46dc4377599f"}},{"outputs":[],"execution_count":null,"source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.tools as tls\n\nimport matplotlib.pyplot as plt\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output.","cell_type":"code","metadata":{"_uuid":"96828822bc867dc2fb2bf88a5a17e7b186d43613","_cell_guid":"a2faeb2b-7d41-4c18-b5b9-2d4bf5d688e5"}},{"outputs":[],"execution_count":null,"source":"train_df = pd.read_csv(\"../input/train.csv\")\ntest_df = pd.read_csv(\"../input/test.csv\")\ntrain_df.head()","cell_type":"code","metadata":{"_uuid":"a598ff44187cf66d549be0361ecb742976c1f034","_cell_guid":"b3a91f32-f28c-4ac1-bfba-38d3d770cf67"}},{"outputs":[],"execution_count":null,"source":"category_col = \"author\"","cell_type":"code","metadata":{"_uuid":"7454908c5f2a1444fe15bdcd8476720b87cfd6a6","collapsed":true,"_cell_guid":"07adabd2-2010-4336-baea-496a01066e94"}},{"outputs":[],"execution_count":null,"source":"from sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import LabelBinarizer\n","cell_type":"code","metadata":{"_uuid":"d60f8cd7bb0a0851c5be6a6b04efddcf081af97a","collapsed":true,"_cell_guid":"39ca6121-2b94-4575-8265-2607e7ea7d47"}},{"outputs":[],"execution_count":null,"source":"le = LabelEncoder().fit(train_df[\"author\"].as_matrix())\nlabel_binarizer = LabelBinarizer().fit(train_df[\"author\"].as_matrix()) # For TensorFlow","cell_type":"code","metadata":{"_uuid":"e7b2468892f940babb5b3a39d68cb0679eb81d74","collapsed":true,"_cell_guid":"ac01d096-032a-4eb1-b307-0cad3747a4ca"}},{"outputs":[],"execution_count":null,"source":"def get_one_hot_target_label(df):\n    print('Labels and their document counts based on', end=' ')\n    print(df.groupby(category_col)[category_col].count())\n\n    return label_binarizer.transform(df[category_col].as_matrix())","cell_type":"code","metadata":{"_uuid":"9b0db7a31a757c1bbaf329224f6daacc5162c4a8","collapsed":true,"_cell_guid":"3c4f18b7-e4dc-4cc1-8215-ebb5e536b41f"}},{"outputs":[],"execution_count":null,"source":"get_one_hot_target_label(train_df)","cell_type":"code","metadata":{"_uuid":"81f73f7401e6fe960ebceee0b2a943d6d90da8ae","_cell_guid":"61470284-9657-4c7e-95ac-79697567e7ca"}},{"source":"# NLP","cell_type":"markdown","metadata":{"_uuid":"587f0d1ec3e9a7b276fa52c376cebffe6fa0555c","collapsed":true,"_cell_guid":"2bfca921-b32c-4f60-8d8b-2f15dd0f4069"}},{"outputs":[],"execution_count":null,"source":"import nltk\nimport spacy","cell_type":"code","metadata":{"_uuid":"6514985e102a2257900a3c7538e3e21a919bec19","collapsed":true,"_cell_guid":"23eaa6c5-7b0e-47e3-a227-5ed145c86d3b"}},{"outputs":[],"execution_count":null,"source":"def extract_lemmas(df: pd.DataFrame, text_col, nlp=spacy.load('en')):\n    stopwords = nltk.corpus.stopwords.words('english')\n\n    def cleaning(sentence):\n        sentence = nlp(sentence)\n        tokens = [token.lemma_ for token in sentence if not token.is_punct | token.is_space | token.is_bracket | (token.text in stopwords)]\n        return ' '.join(tokens)\n\n    df = df.assign(nlp_processed = lambda rows : rows[text_col].map(lambda row: cleaning(row)))\n\n    return df\n    \n    ","cell_type":"code","metadata":{"_uuid":"eeee55874e9b3673bcc634815ef9183a6b2de450","collapsed":true,"_cell_guid":"4f87a62f-ec4a-4f8e-9e39-1197c93040c4"}},{"outputs":[],"execution_count":null,"source":"train_df = extract_lemmas(train_df, \"text\")\ntest_df = extract_lemmas(test_df, \"text\")","cell_type":"code","metadata":{"_uuid":"fcacf6aeb9d4838a548edce5f88127af8abdf64e","collapsed":true,"_cell_guid":"1f52e933-8cc1-43ff-8c5b-a113074a20e8"}},{"source":"# Data Analysis","cell_type":"markdown","metadata":{"_uuid":"923b3fb8d15968c2e54c9006d38495730603c5b8","_cell_guid":"9f990f06-75cc-4533-8180-4c6befc1c7e6"}},{"outputs":[],"execution_count":null,"source":"text_col = \"nlp_processed\"","cell_type":"code","metadata":{"_uuid":"6c45a22698587a892d5124f139171a1a044de787","collapsed":true,"_cell_guid":"d72b23ee-97b2-48ff-822c-85b6c6982747"}},{"outputs":[],"execution_count":null,"source":"train_df = train_df.assign(label = lambda rows : rows.author.map(lambda author: le.transform([author])[0]))","cell_type":"code","metadata":{"_uuid":"011a8c0ce4162cdab40e6f855652aaaa58e4f828","collapsed":true,"_cell_guid":"1c857b5b-4d5c-4e0e-a668-6feda59cb564"}},{"outputs":[],"execution_count":null,"source":"train_df = train_df.assign(length = lambda rows: rows.nlp_processed.map(lambda sent: len(sent.split(' '))) )","cell_type":"code","metadata":{"_uuid":"6305b533d986a7c24e219606dc2a09be87a7fffe","collapsed":true,"_cell_guid":"718a1cae-4722-4eb9-a1e2-315030db7e6d"}},{"outputs":[],"execution_count":null,"source":"train_df.head()","cell_type":"code","metadata":{"_uuid":"d197f0410e238a37a805ecac85b9f3bd01792ec0","_cell_guid":"3328f399-5489-495e-994a-cbbc39b7c3a2"}},{"outputs":[],"execution_count":null,"source":"# train_df = train_df[train_df[\"length\"] > 10]","cell_type":"code","metadata":{"_uuid":"00c311d4e55ca4db820054410025350d58617545","collapsed":true,"_cell_guid":"046d74d3-2ac7-46d0-ac28-2f305bc17ec4"}},{"source":"# Sratified Sampling\n- Sampling same number of samples from each class, for a balanced dataset","cell_type":"markdown","metadata":{"_uuid":"2147cf0ef6e0ccf8a50e0d22d27930306d978be8","_cell_guid":"bd8664f0-f8f4-4ca2-b9ff-e0eef62b2b64"}},{"outputs":[],"execution_count":null,"source":"def _get_train_val_split(df, category_col='author'):\n        print('Splitting the data set(stratified sampling)...')\n\n        def train_validate_test_split(df, train_percent=.8, seed=42):\n            np.random.seed(seed)\n            perm = np.random.permutation(df.index)\n            m = len(df)\n            train_end = int(train_percent * m)\n            train = df.loc[perm[:train_end]]\n            validate = df.loc[perm[train_end:]]\n            return train, validate\n\n        #Make list of sampled dataframe for each category\n        dfs = [train_validate_test_split(df[df[category_col] == label]) for label in le.classes_]\n\n        #Now the values are grouped to form a Dataframe\n        train_dfs = []\n        val_dfs = []\n        for train_df, val_df in dfs:\n            train_dfs.append(train_df)\n            val_dfs.append(val_df)\n\n        train_df = pd.concat(train_dfs)\n        val_df = pd.concat(val_dfs)\n\n        #Shuffle the data\n        train_df = train_df.sample(frac=1).reset_index(drop=True)\n        val_df = val_df.sample(frac=1).reset_index(drop=True)\n\n        print('Done!')\n\n        return train_df, val_df","cell_type":"code","metadata":{"_uuid":"d32aae971623e4655c86a9a0da5ec8c2838ef566","collapsed":true,"_cell_guid":"b22da011-a58f-4bc8-8c6b-36e033385106"}},{"outputs":[],"execution_count":null,"source":"train_df1, val_df = _get_train_val_split(train_df)","cell_type":"code","metadata":{"_uuid":"f0083a89c636b8cb6986525f5d6dffef63fe04ed","_cell_guid":"15f783d9-fcb0-434b-88ba-ad7b915fe3f7"}},{"outputs":[],"execution_count":null,"source":"from sklearn.model_selection import train_test_split\ntrain_df1, val_df = train_test_split(train_df,test_size=0.2) ","cell_type":"code","metadata":{"_uuid":"fc5692100925825013a3a0bb087a94442236a047","collapsed":true,"_cell_guid":"4913eb18-e6f5-421f-85ce-1d81bff71a7d"}},{"outputs":[],"execution_count":null,"source":"train_df.shape, train_df1.shape, val_df.shape\n#((19579, 6), (15663, 6), (3916, 6))","cell_type":"code","metadata":{"_uuid":"5c6a06692dd5a83171481f883f126b474713f296","_cell_guid":"678b48f1-395f-4a11-88cf-e0708d1f1010"}},{"outputs":[],"execution_count":null,"source":"train_df[[text_col, 'author']].groupby('author').count().plot(kind='bar')","cell_type":"code","metadata":{"_uuid":"3c90e64f36596b05963c26e4ee62850585de7fe1","_cell_guid":"24e2d51b-42c0-4e83-bc1b-671bdd606105"}},{"outputs":[],"execution_count":null,"source":"z = {'EAP': 'Edgar Allen Poe', 'MWS': 'Mary Shelley', 'HPL': 'HP Lovecraft'}\ndata = [go.Bar(\n            x = train_df.author.map(z).unique(),\n            y = train_df.author.value_counts().values,\n            marker= dict(colorscale='Jet',\n                         color = train_df.author.value_counts().values\n                        ),\n            text='Text entries attributed to Author'\n    )]\n\nlayout = go.Layout(\n    title='Target variable distribution'\n)\n\nfig = go.Figure(data=data, layout=layout)\n\npy.iplot(fig, filename='basic-bar')","cell_type":"code","metadata":{"_uuid":"ea6bf8c4781e4d621c9263592f7e1f59116c268f","_cell_guid":"9d9d9a29-423c-444e-93e7-80e9df6f781f"}},{"outputs":[],"execution_count":null,"source":"","cell_type":"code","metadata":{"_uuid":"ca94a2d7c32d3d50944ab09b61b0c620895b3b7e","collapsed":true,"_cell_guid":"70cda7d8-efdb-4d44-a408-34b21c06124d"}},{"source":"# Evaluation","cell_type":"markdown","metadata":{"_uuid":"cfafbdf18f54df13e1d3efe3f8777f617f5e759c","_cell_guid":"89d75ef0-1ac7-40b8-b121-d700840cfdfd"}},{"outputs":[],"execution_count":null,"source":"from sklearn.pipeline import Pipeline\nfrom sklearn.metrics import classification_report\n\ndef evaluate(pipeline, val_df=val_df, text_col=text_col, label_col='label'):\n    validation_prediction = pipeline.predict(val_df[text_col].as_matrix())\n    validation_actual = val_df[label_col].as_matrix()\n    val_acc = np.mean(validation_prediction == validation_actual)\n    print(\"Model Accuracy is {}\".format(val_acc))\n\n    val_report = classification_report(validation_actual, validation_prediction, target_names=list(le.classes_))\n    print(val_report)\n    \ndef fit_n_evaluate(stages):\n    pipeline = Pipeline(stages)\n    pipeline.fit(train_df1['text'].as_matrix(), train_df1['label'].as_matrix())\n    evaluate(pipeline)","cell_type":"code","metadata":{"_uuid":"6e0a0e078a3d581eace07281e3defc0f6439f874","collapsed":true,"_cell_guid":"382eaa4f-547a-4cfd-936f-ae7f34d2b43f"}},{"outputs":[],"execution_count":null,"source":"from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\nfrom sklearn.feature_extraction.text import TfidfVectorizer","cell_type":"code","metadata":{"_uuid":"affe96bca1f858ee763c1b4fafef2ebc525ffe4f","collapsed":true,"_cell_guid":"1d1504f8-8a26-469c-b69f-660ff1614d22"}},{"source":"# LDA","cell_type":"markdown","metadata":{"_uuid":"baa0cd1e01da9d0555bd91e4bb8ccba74164df7d","_cell_guid":"263fff9a-668b-481f-bcec-65178b8bec67"}},{"outputs":[],"execution_count":null,"source":"# Define helper function to print top words\ndef print_top_words(model, feature_names, n_top_words):\n    for index, topic in enumerate(model.components_):\n        message = \"\\nTopic #{}:\".format(index)\n        message += \" \".join([feature_names[i] for i in topic.argsort()[:-n_top_words - 1 :-1]])\n        print(message)\n        print(\"=\"*70)","cell_type":"code","metadata":{"_uuid":"0e307919f8271ed1d11ae71e039021c0a697aab0","collapsed":true,"_cell_guid":"cf9fe484-29c8-412d-a316-7725cede09ec"}},{"outputs":[],"execution_count":null,"source":"from sklearn.decomposition import NMF, LatentDirichletAllocation\n\ntf_vectorizer = CountVectorizer(max_df=0.95, min_df=2,\n                                stop_words='english')\ntf = tf_vectorizer.fit_transform(train_df[text_col].as_matrix())\n\nlda = LatentDirichletAllocation(n_components=3, max_iter=5,\n                                learning_method = 'online',\n                                learning_offset = 50.,\n                                random_state = 0)\n\nlda.fit(tf)","cell_type":"code","metadata":{"_uuid":"3ce6a9d24d0eab6aab4e0de2ccea52922e0fc05e","_cell_guid":"8b9cac8f-2c68-47ae-a520-78f36717815b"}},{"outputs":[],"execution_count":null,"source":"n_top_words = 20\nprint(\"\\nTopics in LDA model: \")\ntf_feature_names = tf_vectorizer.get_feature_names()\nprint_top_words(lda, tf_feature_names, n_top_words)","cell_type":"code","metadata":{"_uuid":"c70f996a53676ac2d8ab2e8eaf2497d67344da29","_cell_guid":"477fb162-2b85-4280-b7c7-e648a98cc87b"}},{"outputs":[],"execution_count":null,"source":"first_topic = lda.components_[0]\nsecond_topic = lda.components_[1]\nthird_topic = lda.components_[2]","cell_type":"code","metadata":{"_uuid":"f67e378b9289ecb37a2739672130ab332f282e36","collapsed":true,"_cell_guid":"f45eec71-20d7-460a-be75-c7f386daca8a"}},{"outputs":[],"execution_count":null,"source":"first_topic_words = [tf_feature_names[i] for i in first_topic.argsort()[:-50 - 1 :-1]]\nsecond_topic_words = [tf_feature_names[i] for i in second_topic.argsort()[:-50 - 1 :-1]]\nthird_topic_words = [tf_feature_names[i] for i in third_topic.argsort()[:-50 - 1 :-1]]","cell_type":"code","metadata":{"_uuid":"f7ba07b129beedc956acd1b88e627a84908fdb2f","collapsed":true,"_cell_guid":"879e31f1-7823-46c0-8649-da389d86dd1f"}},{"outputs":[],"execution_count":null,"source":"from wordcloud import WordCloud, STOPWORDS","cell_type":"code","metadata":{"_uuid":"564467118583b603b78fe93163c1a50a14e6289c","collapsed":true,"_cell_guid":"970bf327-21d2-4f77-b3c3-b500b0eaac3e"}},{"source":"**Word cloud of First Topic**\n","cell_type":"markdown","metadata":{"_uuid":"f121f2890d789c7ae3ea0acebc976eadc37675be","_cell_guid":"8a4141da-4811-4fe8-bf85-38bd1b6112f6"}},{"outputs":[],"execution_count":null,"source":"# Generating the wordcloud with the values under the category dataframe\nfirstcloud = WordCloud(\n                          stopwords=STOPWORDS,\n                          background_color='black',\n                          width=2500,\n                          height=1800\n                         ).generate(\" \".join(first_topic_words))\nplt.imshow(firstcloud)\nplt.axis('off')\nplt.show()","cell_type":"code","metadata":{"_uuid":"1a5a34e397dfb6f93e446016ba6d1fe36c7a49e9","_cell_guid":"0cefbb2e-3bc6-4901-9601-0fc3f154e55f"}},{"outputs":[],"execution_count":null,"source":"# Generating the wordcloud with the values under the category dataframe\nfirstcloud = WordCloud(\n                          stopwords=STOPWORDS,\n                          background_color='black',\n                          width=2500,\n                          height=1800\n                         ).generate(\" \".join(second_topic_words))\nplt.imshow(firstcloud)\nplt.axis('off')\nplt.show()","cell_type":"code","metadata":{"_uuid":"122451bae14bd8e280752c62c20c77edc1843268","_cell_guid":"9be2793a-711e-4282-a116-fc570e1f92e8"}},{"outputs":[],"execution_count":null,"source":"# Generating the wordcloud with the values under the category dataframe\nfirstcloud = WordCloud(\n                          stopwords=STOPWORDS,\n                          background_color='black',\n                          width=2500,\n                          height=1800\n                         ).generate(\" \".join(third_topic_words))\nplt.imshow(firstcloud)\nplt.axis('off')\nplt.show()","cell_type":"code","metadata":{"_uuid":"dad9a302a579b481f6788f6355a97a0b7f4d4293","_cell_guid":"71e511a1-35ab-4904-b9e5-9ae5d5e536f9"}},{"source":"# RandomForest Model","cell_type":"markdown","metadata":{"_uuid":"ab975f3c84c58e62bb6ffefd15b7f7906a24f7ca","_cell_guid":"c441754c-3497-4f1c-b1f6-9d2ad3ccdc23"}},{"outputs":[],"execution_count":null,"source":"from sklearn.ensemble import RandomForestClassifier","cell_type":"code","metadata":{"_uuid":"8cdbfa2bc6ce4878fb54ad6d19d215f4dde9e0be","collapsed":true,"_cell_guid":"623eebd2-829d-4a05-b034-aafe925c2127"}},{"outputs":[],"execution_count":null,"source":"rf_stages = [\n                        ('counter', CountVectorizer(analyzer='word', ngram_range=(1,1), max_df=0.95, min_df=2, stop_words='english')),\n                        ('vectorizer', TfidfTransformer()),\n#                         ('vectorizer', TfidfVectorizer(min_df=0, max_df=1, ngram_range=(1, 3), stop_words='english')),\n                        ('clf', RandomForestClassifier(n_jobs=8,\n                                              n_estimators=100,\n                                              min_samples_leaf=4,\n                                              oob_score=True,\n                                              max_depth=20,\n                                              max_features=0.8, #Not much difference with log2\n                                              random_state=42))\n              ]\n","cell_type":"code","metadata":{"_uuid":"e89a0cdc665ed0441a453c9ab0943c2c0d0bc14b","collapsed":true,"_cell_guid":"df50f734-0d45-4b3c-87c5-08b11db5b8c3"}},{"outputs":[],"execution_count":null,"source":"fit_n_evaluate(rf_stages)","cell_type":"code","metadata":{"_uuid":"3059e4785ff09ffcb5093c78df19ceebbdbc2766","_cell_guid":"e2155b47-bf55-4c13-8009-83a0b85a846a"}},{"source":"# XGBoost","cell_type":"markdown","metadata":{"_uuid":"3ee7c3912a76eb1a2bcab880bf06c8af7e5fb266","_cell_guid":"daaefd71-5f41-446c-8edd-43685b05beaa"}},{"outputs":[],"execution_count":null,"source":"from xgboost import XGBClassifier\nfrom sklearn.feature_selection import SelectKBest","cell_type":"code","metadata":{"_uuid":"25365abeca3a39358646350b83775b4d3a9baa99","collapsed":true,"_cell_guid":"41ce7b1b-bae3-4471-af81-d83a07cf84f1"}},{"outputs":[],"execution_count":null,"source":"xgboost_stages = [\n    ('vectorizer', TfidfVectorizer(min_df=2, max_df=0.95, ngram_range=(1, 2), stop_words='english')),\n    (\"kbest\",SelectKBest(k=300)), \n     ('clf', XGBClassifier())\n]","cell_type":"code","metadata":{"_uuid":"5b6694aff83eac51e48e61354572f1db6f06258c","collapsed":true,"_cell_guid":"10a543c1-cd23-4e89-bc13-85aa12d8e4d1"}},{"outputs":[],"execution_count":null,"source":"fit_n_evaluate(xgboost_stages)","cell_type":"code","metadata":{"_uuid":"99368520909204d548b1e133c574b7e1c9b8e353","_cell_guid":"04432691-bc3e-455e-99e8-5c7bf4dcdaea"}},{"source":"# SVM","cell_type":"markdown","metadata":{"_uuid":"3a5170f77b05c2067623c6d3e58276087d0d4d2e","_cell_guid":"d67e9c6d-7863-4772-b468-05612689fcc7"}},{"outputs":[],"execution_count":null,"source":"from sklearn import svm\n\nsvm_stages = [('vect', CountVectorizer()),\n                     ('tfidf', TfidfTransformer()),\n                     ('clf', svm.LinearSVC())\n                    ]\nfit_n_evaluate(svm_stages)","cell_type":"code","metadata":{"_uuid":"950b652fe3da4a6f996d8e8aed3e0fcc523fec5a","_cell_guid":"6ce91af6-a85f-4b58-8a74-cf7efd472004"}},{"source":"# MultiNomial Bayes","cell_type":"markdown","metadata":{"_uuid":"e5c9f54ad4d1710b6d0897e9c65503036e218752","_cell_guid":"b968eaab-d09e-4f2a-9643-0c121e27ba1d"}},{"outputs":[],"execution_count":null,"source":"from sklearn.naive_bayes import MultinomialNB\nmnb_stages = [('vect', CountVectorizer()),\n                     ('tfidf', TfidfTransformer()),\n                     ('clf',  MultinomialNB())\n                    ]\n\nfit_n_evaluate(mnb_stages)","cell_type":"code","metadata":{"_uuid":"49bb3e546d99cbae493826873c1bc5faaa09f343","_cell_guid":"a92d0ca4-eef9-436d-91d0-71467f49c51f"}},{"outputs":[],"execution_count":null,"source":"","cell_type":"code","metadata":{"_uuid":"d4819d7681e91c7ee9ff3ed90245aa95e071357b","collapsed":true,"_cell_guid":"0c28b2e2-746d-4dd5-91ad-dc6aedddaadb"}},{"outputs":[],"execution_count":null,"source":"","cell_type":"code","metadata":{"_uuid":"8c2de2112744abee9d97359267a7f4017b5e3f09","collapsed":true,"_cell_guid":"3287a631-1925-4115-a561-ff386b8539cb"}},{"source":"# Tensorflow\n\nBare with for the following lengthy code, as I am Ctrl + C and Ctrl + V from my data science workbench, since it helps me to maintain the changes. :)\n\nMost of the code used here extracted form the reference links given below, IMO the reference links have explained the topic to great extend! So I am going a head puting only the code to try out!\n\nI have adopted below archirecture, so that I can keep the dataset and TF models as modular as possible and extensible for new try outs with Tensorflow low level APIs.\n\nWhat if the kernel runtime takes more than 60mins? Dont worry jus re-run the \"model.run()\" cell again given below, it will start from where it stopped. Remember the session should be alive with respect to Kaggle kernel enviroonment.\n\nIf you are trying this in your machine, then no worries! Stop at your will and start at your will the learning will continuw from where it left!!! superb isn't it?\n\n**IMHO if you have any Tensorflow/Deep Learning papers which you wanted to try out for text classification, then this will be a playground for that!!**","cell_type":"markdown","metadata":{"_uuid":"9b6e1586824db27f4eb402d2c73ea376f209c0ef","_cell_guid":"38779c97-39f5-401e-a73f-c795c262f3a7"}},{"source":"**WARNING: Lengthy Code!!!** ","cell_type":"markdown","metadata":{"_uuid":"195c23b4fea2600a47fc3fca4fa0761e16473827","_cell_guid":"a57fa371-3fac-488b-b201-29e4a23aeee8"}},{"source":"> ## Preprating Tensorflow Dataset","cell_type":"markdown","metadata":{"_uuid":"4ff15a81fef7581fe69131f70dea2109978a92a2","_cell_guid":"9ef7d7e8-5aed-4baf-b8d6-953e2b6d7722"}},{"outputs":[],"execution_count":null,"source":"import tensorflow as tf\nfrom tensorflow.python.platform import gfile\nfrom tqdm import tqdm_notebook as tqdm\nimport tensorflow.contrib.learn as tflearn","cell_type":"code","metadata":{"_uuid":"e256cbaeb1a50025a0492afb492dd85b5b18e9cc","collapsed":true,"_cell_guid":"dca9d829-da07-4f09-aa44-24d5c3f08a28"}},{"outputs":[],"execution_count":null,"source":"# Define data loaders\nclass IteratorInitializerHook(tf.train.SessionRunHook):\n    \"\"\"Hook to initialise data iterator after Session is created.\"\"\"\n\n    def __init__(self):\n        super(IteratorInitializerHook, self).__init__()\n        self.iterator_initializer_func = None\n\n    def after_create_session(self, session, coord):\n        \"\"\"Initialise the iterator after the session has been created.\"\"\"\n        self.iterator_initializer_func(session)\n\ndef save_vocab(lines, outfilename, MAX_DOCUMENT_LENGTH, PADWORD='ZYXW'):\n    # the text to be classified\n    vocab_processor = tflearn.preprocessing.VocabularyProcessor(MAX_DOCUMENT_LENGTH,\n                                                                min_frequency=0)\n    vocab_processor.fit(lines)\n\n    with gfile.Open(outfilename, 'wb') as f:\n        f.write(\"{}\\n\".format(PADWORD))\n        for word, index in tqdm(vocab_processor.vocabulary_._mapping.items()):\n            f.write(\"{}\\n\".format(word))\n\n    nwords = len(vocab_processor.vocabulary_)\n    print('{} words into {}'.format(nwords, outfilename))\n\n    return nwords + 2  # UNKNOWN + PADWORD\n\n# Define the inputs\ndef setup_input_graph(features, labels, batch_size, scope='train-data'):\n    \"\"\"Return the input function to get the training data.\n\n    Args:\n        batch_size (int): Batch size of training iterator that is returned\n                          by the input function.\n        mnist_data (Object): Object holding the loaded mnist data.\n\n    Returns:\n        (Input function, IteratorInitializerHook):\n            - Function that returns (features, labels) when called.\n            - Hook to initialise input iterator.\n    \"\"\"\n    iterator_initializer_hook = IteratorInitializerHook()\n\n\n    def inputs():\n        \"\"\"Returns training set as Operations.\n\n        Returns:\n            (features, labels) Operations that iterate over the dataset\n            on every evaluation\n        \"\"\"\n        with tf.name_scope(scope):\n\n            # Define placeholders\n            features_placeholder = tf.placeholder(tf.string, features.shape)\n            labels_placeholder = tf.placeholder(labels.dtype, labels.shape)\n\n            # Build dataset iterator\n            dataset = tf.contrib.data.Dataset.from_tensor_slices((features_placeholder,\n                                                                  labels_placeholder))\n            dataset = dataset.repeat(None)  # Infinite iterations\n            dataset = dataset.shuffle(buffer_size=10000)\n            dataset = dataset.batch(batch_size)\n            iterator = dataset.make_initializable_iterator()\n\n            # Set runhook to initialize iterator\n            iterator_initializer_hook.iterator_initializer_func = \\\n                lambda sess: sess.run(\n                    iterator.initializer,\n                    feed_dict={features_placeholder: features,\n                               labels_placeholder: labels})\n\n            next_example, next_label = iterator.get_next()\n\n            # Return batched (features, labels)\n            return next_example, next_label\n\n    # Return function and hook\n    return inputs, iterator_initializer_hook","cell_type":"code","metadata":{"_uuid":"da3750fc3015579f33db73bdff27e68c46392d49","collapsed":true,"_cell_guid":"b83e61d6-b6fb-4956-818e-0fb0b9bc8680"}},{"source":"## Tensorflow Estimators","cell_type":"markdown","metadata":{"_uuid":"a042faf241755252a8075464494496c86eae2a29","collapsed":true,"_cell_guid":"6eb58e44-5d50-471f-8a7f-2191b9b252e5"}},{"outputs":[],"execution_count":null,"source":"import tensorflow as tf\nfrom tensorflow.contrib import lookup\nfrom tensorflow.contrib.learn import learn_runner\nfrom sklearn.base import BaseEstimator, TransformerMixin, ClassifierMixin\n\n# Set default flags for the output directories\nFLAGS = tf.app.flags.FLAGS","cell_type":"code","metadata":{"_uuid":"5d0525a32b176ee4cd617cf0a161b6815ee9536d","collapsed":true,"_cell_guid":"2059748f-fa59-42dd-a8c3-99500c8d7b0d"}},{"source":"I have made a small abstract class to maintain different models and datasets.\n\n![](https://cdn-images-1.medium.com/max/800/1*zoNZvvuJb06yAghetc6BfQ.png)","cell_type":"markdown","metadata":{"_uuid":"bbc9ebfa495a49641d085bec0cc8218c9c0bcc92","_cell_guid":"18c7bf31-8501-45f2-a059-aeb65b60b361"}},{"outputs":[],"execution_count":null,"source":"\nclass TFBaseEstimator(BaseEstimator, TransformerMixin, ClassifierMixin):\n    def __init__(self, train_input_fn,\n                 train_input_hook,\n                 eval_input_fn,\n                 eval_input_hook,\n                 learning_rate,\n                 train_steps,\n                 min_eval_frequency):\n        self.train_input_fn = train_input_fn\n        self.train_input_hook = train_input_hook\n        self.eval_input_fn = eval_input_fn\n        self.eval_input_hook =  eval_input_hook\n\n        self.learning_rate = learning_rate\n        self.train_steps = train_steps\n        self.min_eval_frequency = min_eval_frequency\n\n    def estimator_spec(self, *args):\n        return NotImplementedError\n\n    def fit(self, X, y):\n        return NotImplementedError\n\n    def predict(self,X):\n        return NotImplementedError\n\n    def get_estimator(self, run_config, params):\n        \"\"\"Return the model as a Tensorflow Estimator object.\n\n        Args:\n             run_config (RunConfig): Configuration for Estimator run.\n             params (HParams): hyperparameters.\n        \"\"\"\n        return tf.estimator.Estimator(\n            model_fn=self.estimator_spec,  # First-class function\n            params=params,  # HParams\n            config=run_config  # RunConfig\n        )\n\n    def experiment_fn(self, run_config, params):\n        \"\"\"Create an experiment to train and evaluate the model.\n    \n        Args:\n            run_config (RunConfig): Configuration for Estimator run.\n            params (HParam): Hyperparameters\n    \n        Returns:\n            (Experiment) Experiment for training the mnist model.\n        \"\"\"\n        # You can change a subset of the run_config properties as\n        run_config = run_config.replace(save_checkpoints_steps=params.min_eval_frequency)\n\n        # Define the mnist classifier\n        estimator = self.get_estimator(run_config, params)\n\n        # Define the experiment\n        experiment = tf.contrib.learn.Experiment(\n            estimator=estimator,  # Estimator\n            train_input_fn=self.train_input_fn,  # First-class function\n            eval_input_fn=self.eval_input_fn,  # First-class function\n            train_steps=params.train_steps,  # Minibatch steps\n            eval_steps=100,  # Use evaluation feeder until its empty\n            min_eval_frequency=params.min_eval_frequency,  # Eval frequency\n            train_monitors=[self.train_input_hook],  # Hooks for training\n            eval_hooks=[self.eval_input_hook],  # Hooks for evaluation\n            #         export_strategies=[saved_model_export_utils.make_export_strategy(\n            #                                 serving_input_fn,\n            #                                 default_output_alternative_key=None,\n            #                                 exports_to_keep=1\n            #                                 )],\n\n        )\n\n        return experiment\n\n    def run_experiment(self, argv=None):\n        \"\"\"Run the training experiment.\"\"\"\n        # Define model parameters\n        params = tf.contrib.training.HParams(\n            learning_rate=self.learning_rate,\n            train_steps=self.train_steps,\n            min_eval_frequency=self.min_eval_frequency\n        )\n\n        # Set the run_config and the directory to save the model and stats\n        run_config = tf.contrib.learn.RunConfig()\n        run_config = run_config.replace(model_dir=FLAGS.model_dir)\n\n        learn_runner.run(\n            experiment_fn=self.experiment_fn,  # First-class function\n            run_config=run_config,  # RunConfig\n            schedule=\"train_and_evaluate\",  # What to run\n            hparams=params  # HParams\n        )\n\n    def run(self):\n        tf.app.run(main=self.run_experiment)","cell_type":"code","metadata":{"_uuid":"774654ec1913300423e16a54fa33acc23725b310","collapsed":true,"_cell_guid":"9b03dacc-15b7-446b-854a-8bb69183b0df"}},{"outputs":[],"execution_count":null,"source":"def get_sequence_length(sequence):\n    '''\n    Returns the sequence length, droping out all the zeros if the sequence is padded\n    :param sequence: Tensor(shape=[batch_size, doc_length, feature_dim])\n    :return: Array of Document lengths of size batch_size\n    '''\n    used = tf.sign(tf.reduce_max(tf.abs(sequence), 2))\n    length = tf.reduce_sum(used,1)\n    length = tf.cast(length, tf.int32)\n    return length\n","cell_type":"code","metadata":{"_uuid":"e1ed4da17894bf6e4af6d16d637995bfb64a1e5c","collapsed":true,"_cell_guid":"d1e102cd-ad8a-4ee2-906c-ea1826a5e88f"}},{"source":"Following code uses variable length LSTMS\n\n### Train data -> NLP cleansing -> Tensors of String -> Split on space -> Table lookup -> Tensors of Ids -> LSTM -> Dropout Layer -> Fully Connected Layers -> Output","cell_type":"markdown","metadata":{}},{"outputs":[],"execution_count":null,"source":"import tensorflow as tf\nfrom tensorflow.contrib.learn import ModeKeys\nfrom tensorflow.contrib import lookup\n\n# import tensorflow.contrib.rnn.LSTMStateTuple\n\n\nclass TextCNNRNN(TFBaseEstimator):\n    def __init__(self,\n                 vocab_file,\n                 vocab_size,\n                 train_input_fn,\n                 train_input_hook,\n                 eval_input_fn,\n                 eval_input_hook,\n                 max_doc_length,\n                 learning_rate=0.001,\n                 train_steps=40000,\n                 min_eval_frequency=500):\n        super().__init__(train_input_fn,\n                         train_input_hook,\n                         eval_input_fn,\n                         eval_input_hook,\n                         learning_rate,\n                         train_steps,\n                         min_eval_frequency)\n\n        self.VOCAB_FILE = vocab_file\n        self.VOCAB_SIZE = vocab_size\n        self.PADWORD = 'PADXYZ'\n        self.MAX_DOCUMENT_LENGTH = max_doc_length\n        self.EMBEDDING_SIZE = 300\n\n        self.WINDOW_SIZE = self.EMBEDDING_SIZE\n        self.STRIDE = int(self.WINDOW_SIZE / 2)\n\n        self.NUM_CLASSES = 3\n\n        self.num_lstm_layers = 1\n        self.output_keep_prob = 0.5\n\n    def estimator_spec(self, features, labels, mode, params):\n        \"\"\"Model function used in the estimator.\n\n        Args:\n            features : Tensor(shape=[?], dtype=string) Input features to the model.\n            labels : Tensor(shape=[?, n], dtype=Float) Input labels.\n            mode (ModeKeys): Specifies if training, evaluation or prediction.\n            params (HParams): hyperparameters.\n\n        Returns:\n            (EstimatorSpec): Model to be run by Estimator.\n        \"\"\"\n        is_training = mode == ModeKeys.TRAIN\n\n\n        # Define model's architecture\n        with tf.variable_scope(\"sentence-2-words\"):\n            table = lookup.index_table_from_file(vocabulary_file=self.VOCAB_FILE,\n                                                 num_oov_buckets=1,\n                                                 default_value=-1,\n                                                 name=\"table\")\n            tf.logging.info('table info: {}'.format(table))\n\n            # string operations\n            text_lines = tf.squeeze(features)\n            words = tf.string_split(text_lines)\n            densewords = tf.sparse_tensor_to_dense(words, default_value=self.PADWORD)\n            numbers = table.lookup(densewords)\n            sliced = numbers\n            # padding = tf.constant([[0, 0], [0, self.MAX_DOCUMENT_LENGTH]])\n            # padded = tf.pad(numbers, padding)\n            # sliced = tf.slice(padded, [0, 0], [-1, self.MAX_DOCUMENT_LENGTH])\n\n        with tf.device('/cpu:0'), tf.name_scope(\"embed-layer\"):\n            # layer to take the words and convert them into vectors (embeddings)\n            # This creates embeddings matrix of [n_words, EMBEDDING_SIZE] and then\n            # maps word indexes of the sequence into\n            # [batch_size, MAX_DOCUMENT_LENGTH, EMBEDDING_SIZE].\n            word_vectors = tf.contrib.layers.embed_sequence(sliced,\n                                                      vocab_size=self.VOCAB_SIZE,\n                                                      embed_dim=self.EMBEDDING_SIZE)\n\n            # [?, self.MAX_DOCUMENT_LENGTH, self.EMBEDDING_SIZE]\n            tf.logging.debug('words_embed={}'.format(word_vectors))\n\n            # Split into list of embedding per word, while removing doc length dim.\n            # word_list results to be a list of tensors [batch_size, EMBEDDING_SIZE].\n            # word_list = tf.unstack(word_vectors, axis=1)\n\n        with tf.name_scope(\"lstm-layer\"):\n                # LSTM cell\n            lstm = tf.contrib.rnn.LSTMCell(self.EMBEDDING_SIZE, state_is_tuple=True)\n            tf.logging.info('lstm: ------> {}'.format(lstm))\n\n            # Add dropout to the cell\n            # cell =  SwitchableDropoutWrapper(\n            #     lstm,\n            #     is_training,\n            #     output_keep_prob=self.output_keep_prob)\n            if is_training:\n                cell = tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=self.output_keep_prob)\n            else:\n                cell = tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=1.0)\n\n                # Stack up multiple LSTM layers, for deep learning\n            cell = tf.contrib.rnn.MultiRNNCell([cell] * self.num_lstm_layers)\n            tf.logging.info('cell: ------> {}'.format(cell))\n            #\n            outputs, encoding = tf.nn.dynamic_rnn(cell, word_vectors, dtype=tf.float32,\n                                                  sequence_length=get_sequence_length(word_vectors))\n            # LSTMStateTuple https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/LSTMStateTuple\n            encoding = encoding[0][0]\n\n            # Create a Gated Recurrent Unit cell with hidden size of EMBEDDING_SIZE.\n            # cell = tf.nn.rnn_cell.GRUCell(self.EMBEDDING_SIZE)\n\n            # Create an unrolled Recurrent Neural Networks to length of\n            # MAX_DOCUMENT_LENGTH and passes word_list as inputs for each unit.\n            # _, encoding = tf.nn.static_rnn(cell, word_list, dtype=tf.float32)\n            # [?, EMBEDDING_SIZE]\n            tf.logging.info('encoding: ------> {}'.format(encoding))\n\n\n        with tf.name_scope(\"hidden-mlp-layer\"):\n            # [batch_size, 100]\n            hidden_layer = tf.contrib.layers.fully_connected(encoding, 100,\n                                                                 activation_fn=tf.nn.relu)\n            tf.logging.info('hidden_layer: ------> {}'.format(hidden_layer))\n            hidden_layer = tf.contrib.layers.fully_connected(hidden_layer, 50,\n                                                             activation_fn=tf.nn.relu)\n            tf.logging.info('hidden_layer: ------> {}'.format(hidden_layer))\n\n        with tf.name_scope(\"logits-layer\"):\n            # [?, self.NUM_CLASSES]\n            logits = tf.contrib.layers.fully_connected(hidden_layer, self.NUM_CLASSES,\n                                                                 activation_fn=tf.sigmoid)\n            tf.logging.info('logits: ------> {}'.format(logits))\n\n        with tf.name_scope(\"output-layer\"):\n            # [?,1]\n            predictions = tf.argmax(logits, axis=-1)\n            tf.logging.info('predictions: ------> {}'.format(predictions))\n\n            # Loss, training and eval operations are not needed during inference.\n        loss = None\n        train_op = None\n        eval_metric_ops = {}\n\n        if mode != ModeKeys.INFER:\n            loss = tf.nn.softmax_cross_entropy_with_logits(\n                labels=labels,\n                logits=logits,\n                name='softmax_entropy')\n\n            loss = tf.reduce_mean(loss)\n\n            train_op = tf.contrib.layers.optimize_loss(\n                loss=loss,\n                global_step=tf.contrib.framework.get_global_step(),\n                optimizer=tf.train.AdamOptimizer,\n                learning_rate=params.learning_rate)\n\n            eval_metric_ops = {\n                'RPXAccuracy': tf.metrics.accuracy(\n                    labels=tf.argmax(labels, 1, name='target_argmax'),\n                    predictions=predictions,\n                    name='accuracy')\n            }\n\n        return tf.estimator.EstimatorSpec(\n            mode=mode,\n            predictions=predictions,\n            loss=loss,\n            train_op=train_op,\n            eval_metric_ops=eval_metric_ops\n        )\n\n","cell_type":"code","metadata":{"_uuid":"745d198985f3f759e58facfbf68dd6d4179ed141","collapsed":true,"_cell_guid":"baa9fe52-bdc0-4c2f-8397-86190774b392"}},{"outputs":[],"execution_count":null,"source":"# Show debugging output\ntf.logging.set_verbosity(tf.logging.DEBUG)\n\n# Set default flags for the output directories\nFLAGS = tf.app.flags.FLAGS\n\ntf.app.flags.DEFINE_string(\n    flag_name='model_dir', \n    default_value='./spooky_tf_models_cnnrnn',\n    docstring='Output directory for model and training stats.')\n\ntf.app.flags.DEFINE_string(\n    flag_name='data_dir', \n    default_value='./spooky_data_cnnrnn',\n    docstring='Directory to download the data to.')\n\nBATCH_SIZE = 64\nMAX_DOCUMENT_LENGTH = 350","cell_type":"code","metadata":{"_uuid":"79556cb39749fd2a75d3f66aea707ad40c13209f","collapsed":true,"_cell_guid":"0e9cb73d-dd67-4487-b9e9-addd007f01f0"}},{"outputs":[],"execution_count":null,"source":"VOCAB_SIZE = save_vocab(train_df1[text_col].as_matrix(), \n                       outfilename='horror_vocab.tsv', \n                       MAX_DOCUMENT_LENGTH=MAX_DOCUMENT_LENGTH)","cell_type":"code","metadata":{"_uuid":"810e74a0df1d1abf3453f23d6cf99802fe76f3a6","_cell_guid":"efb894cc-4a43-472b-bcbe-be73fae04bd6"}},{"outputs":[],"execution_count":null,"source":"train_input_fn, train_input_hook = setup_input_graph(train_df1[text_col].as_matrix(), \n                                                     get_one_hot_target_label(train_df1),\n                                                    batch_size=BATCH_SIZE, \n                                                     scope='train-data')","cell_type":"code","metadata":{"_uuid":"2d42aa920f198ea45d397368be1a01645a2d7ef6","_cell_guid":"384ed243-6237-4b5b-93a0-9db59f6d1138"}},{"outputs":[],"execution_count":null,"source":"eval_input_fn, eval_input_hook =  setup_input_graph(val_df[text_col].as_matrix(), \n                                                     get_one_hot_target_label(val_df),\n                                                    batch_size=BATCH_SIZE, \n                                                    scope='eval-data')","cell_type":"code","metadata":{"_uuid":"4a6d2db6572bb91f65dc77b1d76c8376455f9e3d","_cell_guid":"0c876328-03da-4ffd-beaf-67b0a48bb07e"}},{"outputs":[],"execution_count":null,"source":"model = TextCNNRNN(\"horror_vocab.tsv\", \n                 VOCAB_SIZE, \n                 train_input_fn, \n                 train_input_hook, \n                 eval_input_fn, \n                 eval_input_hook,\n                  max_doc_length=MAX_DOCUMENT_LENGTH)","cell_type":"code","metadata":{"_uuid":"ded616d6f1e5dd5ac6eeca5c6fdddca2fce0dbb7","collapsed":true,"_cell_guid":"dbbd6b1a-f1b5-47af-bd00-51f4c423d9b3"}},{"outputs":[],"execution_count":null,"source":"%time\nmodel.run()","cell_type":"code","metadata":{"_uuid":"46cbc263f356f0ad55467f59e22b9bade9e38430","_cell_guid":"bfc8ee60-30d2-4720-a106-9dac6118b648"}},{"source":"# in my machine I was able to reach 78 to 80 % accuracy","cell_type":"markdown","metadata":{"_uuid":"16620a3f98c47bd3d58982d9d52813ac83d45f00","_cell_guid":"ebba8972-6feb-4d4d-980f-7c26872eb2e9"}},{"source":"# TODO \n- Create submission\n- Clean the notebook and make more readble","cell_type":"markdown","metadata":{"_uuid":"22dd54378e03246b784c0a72f96bcc92e84722a5","collapsed":true,"_cell_guid":"b3966e8d-020e-423d-abeb-afd60ddc7ec9"}},{"outputs":[],"execution_count":null,"source":"","cell_type":"code","metadata":{"_uuid":"0ead453a215a790b327fe9ade631bbe73e18d403","collapsed":true,"_cell_guid":"d23e1319-6924-4792-ad9e-fa4cbe9d4105"}},{"source":"# TensorFlow Urls:\nhttps://www.tensorflow.org/api_docs/python/tf/contrib/data/Dataset  \nhttps://www.tensorflow.org/api_docs/python/tf/contrib/learn/Experiment  \nhttps://www.tensorflow.org/api_docs/python/tf/estimator/Estimator  \n\n# Refernces\n- https://terrytangyuan.github.io/data/papers/tf-estimators-kdd-paper.pdf\n- https://medium.com/onfido-tech/higher-level-apis-in-tensorflow-67bfb602e6c0\n- https://www.kaggle.com/arthurtok/spooky-nlp-and-topic-modelling-tutorial\n- https://medium.com/towards-data-science/how-to-do-text-classification-using-tensorflow-word-embeddings-and-cnn-edae13b3e575\n- https://github.com/GoogleCloudPlatform/training-data-analyst/tree/master/blogs/textclassification","cell_type":"markdown","metadata":{"_uuid":"2148e22609a54b3cefc231de4da52aef1883d94c","collapsed":true,"_cell_guid":"65d571c3-17cd-4e2b-99f6-1ee33b10260e"}},{"outputs":[],"execution_count":null,"source":"","cell_type":"code","metadata":{"_uuid":"bfea0eecc56266194dadf1a573911c39495b538e","collapsed":true,"_cell_guid":"4761b540-b961-4560-a74e-a77390b99ec5"}}],"nbformat":4,"metadata":{"kernelspec":{"name":"python3","language":"python","display_name":"Python 3"},"language_info":{"name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","file_extension":".py","mimetype":"text/x-python","version":"3.6.3","codemirror_mode":{"name":"ipython","version":3}}},"nbformat_minor":1}