{"cells":[{"outputs":[],"execution_count":null,"metadata":{"_cell_guid":"52308f52-f7d1-47ef-899f-11cc8a4f2256","_uuid":"c4a005af2a942f6e85f9bb2070ae2790c74b4d70","collapsed":true},"source":"import numpy as np\nnp.random.seed(666)\nimport pandas as pd\nfrom sklearn.cross_validation import train_test_split\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n\n#LOAD DATA\ntrain = pd.read_csv(\"../input/train.csv\")\ntest = pd.read_csv(\"../input/test.csv\")\nsample = pd.read_csv(\"../input/sample_submission.csv\")\n\n#CREATE TARGET VARIABLE\ntrain[\"EAP\"] = (train.author==\"EAP\")*1\ntrain[\"HPL\"] = (train.author==\"HPL\")*1\ntrain[\"MWS\"] = (train.author==\"MWS\")*1\ntrain.drop(\"author\", 1, inplace=True)\ntarget_vars = [\"EAP\", \"HPL\", \"MWS\"]\ntrain.head(2)","cell_type":"code"},{"outputs":[],"execution_count":null,"metadata":{"_cell_guid":"9505edbb-956a-4e25-9291-b63b3ac5fbfa","_uuid":"b170d6dc306e3e8b8f72ff2d520685befa96d648","collapsed":true},"source":"from nltk.corpus import stopwords\neng_stopwords = set(stopwords.words(\"english\"))\nimport string\n## Number of words in the text ##\ntrain[\"num_words\"] = train[\"text\"].apply(lambda x: len(str(x).split()))\ntest[\"num_words\"] = test[\"text\"].apply(lambda x: len(str(x).split()))\n\n## Number of unique words in the text ##\ntrain[\"num_unique_words\"] = train[\"text\"].apply(lambda x: len(set(str(x).split())))\ntest[\"num_unique_words\"] = test[\"text\"].apply(lambda x: len(set(str(x).split())))\n\n## Number of characters in the text ##\ntrain[\"num_chars\"] = train[\"text\"].apply(lambda x: len(str(x)))\ntest[\"num_chars\"] = test[\"text\"].apply(lambda x: len(str(x)))\n\n## Number of stopwords in the text ##\ntrain[\"num_stopwords\"] = train[\"text\"].apply(lambda x: len([w for w in str(x).lower().split() if w in eng_stopwords]))\ntest[\"num_stopwords\"] = test[\"text\"].apply(lambda x: len([w for w in str(x).lower().split() if w in eng_stopwords]))\n\n## Number of punctuations in the text ##\ntrain[\"num_punctuations\"] =train['text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]) )\ntest[\"num_punctuations\"] =test['text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]) )\n\n## Number of title case words in the text ##\ntrain[\"num_words_upper\"] = train[\"text\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\ntest[\"num_words_upper\"] = test[\"text\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\n\n## Number of title case words in the text ##\ntrain[\"num_words_title\"] = train[\"text\"].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\ntest[\"num_words_title\"] = test[\"text\"].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\n\n## Average length of the words in the text ##\ntrain[\"mean_word_len\"] = train[\"text\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\ntest[\"mean_word_len\"] = test[\"text\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\n\nnum_vars = [\"mean_word_len\", \"num_words_title\", \"num_punctuations\", \"num_chars\"\n            , \"num_stopwords\", \"num_chars\", \"num_unique_words\", \"num_words\"]","cell_type":"code"},{"outputs":[],"execution_count":null,"metadata":{"_cell_guid":"c4ed6f7e-187d-4552-8e53-f534ab849c46","_uuid":"49b0149267c17e41bddbaf489c83c21c14928933","collapsed":true},"source":"#STEMMING WORDS\nimport nltk.stem as stm\nimport re\nstemmer = stm.SnowballStemmer(\"english\")\ntrain[\"stem_text\"] = train.text.apply(lambda x: (\" \").join([stemmer.stem(z) for z in re.sub(\"[^a-zA-Z0-9]\",\" \", x).split(\" \")]))\ntest[\"stem_text\"] = test.text.apply(lambda x: (\" \").join([stemmer.stem(z) for z in re.sub(\"[^a-zA-Z0-9]\",\" \", x).split(\" \")]))\n\n#PROCESS TEXT: RAW\nfrom keras.preprocessing.text import Tokenizer\ntok_raw = Tokenizer()\ntok_raw.fit_on_texts(train.text.str.lower())\ntok_stem = Tokenizer()\ntok_stem.fit_on_texts(train.stem_text)\ntrain[\"seq_text_stem\"] = tok_stem.texts_to_sequences(train.stem_text)\ntest[\"seq_text_stem\"] = tok_stem.texts_to_sequences(test.stem_text)\n\n","cell_type":"code"},{"outputs":[],"execution_count":null,"metadata":{"_cell_guid":"a4bca788-12ea-4870-8c9a-1b602db94546","_uuid":"4930dcfbc90520bb5bc05331b25f8a3ddc27fe73","collapsed":true},"source":"#EXTRACT DATA FOR KERAS MODEL\nfrom keras.preprocessing.sequence import pad_sequences\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.pipeline import Pipeline\n\ndef get_keras_data(dataset, maxlen=20, scaler=None, tdfidf=None):\n    if scaler==None:\n        scaler = StandardScaler()\n        scaler.fit(dataset[num_vars])\n    if tdfidf==None:\n        tdfidf = Pipeline(steps=[('tdfidf', TfidfVectorizer(analyzer='word', binary=False\n                                , ngram_range=(1, 4), stop_words=\"english\"))\n                            , ('svd', TruncatedSVD(algorithm='randomized', n_components=20, n_iter=10,\n                                       random_state=None, tol=0.0)\n                                )])\n        tdfidf.fit(dataset.text)\n    X = {\n        \"stem_input\": pad_sequences(dataset.seq_text_stem, maxlen=maxlen),\n        \"num_input\": scaler.transform(dataset[num_vars]),\n        \"svd_vect\": tdfidf.transform(dataset.text)\n    }\n    return X, scaler, tdfidf\n\n\nmaxlen = 60\ndtrain, dvalid = train_test_split(train, random_state=123, train_size=0.9)\nprint(\"processing train...\")\nX_train, scaler, tdfidf = get_keras_data(dtrain, maxlen)\ny_train = np.array(dtrain[target_vars])\nprint(\"processing valid...\")\nX_valid, _, _ = get_keras_data(dvalid, maxlen, scaler, tdfidf)\ny_valid = np.array(dvalid[target_vars])\nprint(\"processing test...\")\nX_test, _, _ = get_keras_data(test, maxlen, scaler, tdfidf)\n\nn_stem_seq = np.max( [np.max(X_valid[\"stem_input\"]), np.max(X_train[\"stem_input\"])])+1","cell_type":"code"},{"outputs":[],"execution_count":null,"metadata":{"_cell_guid":"e107482a-53f8-42da-a3ab-6dbaab951756","_uuid":"1ea4dbac31590e0ccc5d98402c1932f00bf1c872","collapsed":true},"source":"#KERAS MODEL DEFINITION\nfrom keras.layers import Dense, Dropout, Embedding\nfrom keras.layers import Flatten, Input, SpatialDropout1D, Concatenate\nfrom keras.models import Model\nfrom keras.optimizers import Adam \nfrom keras.callbacks import ModelCheckpoint, Callback, EarlyStopping\n\ndef get_callbacks(filepath, patience=2):\n    es = EarlyStopping('val_loss', patience=patience, mode=\"min\")\n    msave = ModelCheckpoint(filepath, save_best_only=True)\n    return [es, msave]\n\ndef get_model():\n    embed_dim = 30\n    dropout_rate = 0.9\n    emb_dropout_rate = 0.9\n   \n    input_text = Input(shape=[maxlen], name=\"stem_input\")\n    \n    input_num = Input(shape=[X_train[\"num_input\"].shape[1]], name=\"num_input\")\n    \n    input_svd = Input(shape=[X_train[\"svd_vect\"].shape[1]], name=\"svd_vect\")\n    \n    emb_lstm = SpatialDropout1D(emb_dropout_rate) (Embedding(n_stem_seq, embed_dim\n                                                ,input_length = maxlen\n                                                               ) (input_text))\n    concatenate = Concatenate()([(Flatten() (emb_lstm)), input_num, input_svd])\n    dense = Dropout(dropout_rate) (Dense(256) (concatenate))\n    \n    output = Dense(3, activation=\"softmax\")(dense)\n\n    model = Model([input_text, input_num, input_svd], output)\n\n    optimizer = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n    model.compile(loss=\"categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n    return model\n\nmodel = get_model()\nmodel.summary()","cell_type":"code"},{"outputs":[],"execution_count":null,"metadata":{"_cell_guid":"fa3bca6c-13d4-4e1b-8e48-68c0afe7f5b3","scrolled":true,"_uuid":"3fc1de3e53843b081df15a4418595b8784cb32e2","collapsed":true},"source":"#TRAIN KERAS MODEL\nfile_path = \".model_weights.hdf5\"\ncallbacks = get_callbacks(filepath=file_path, patience=5)\n\nmodel = get_model()\nmodel.fit(X_train, y_train, epochs=150\n          , validation_data=[X_valid, y_valid]\n         , batch_size=512\n         , callbacks = callbacks)","cell_type":"code"},{"outputs":[],"execution_count":null,"metadata":{"_cell_guid":"f0d09781-e25a-4a89-a037-e2d0f4d6ced3","_uuid":"0a36663461f74bb0e1e1df4defbdd7e78b92c9c6","collapsed":true},"source":"#MODEL EVALUATION\nfrom sklearn.metrics import log_loss\n\nmodel = get_model()\nmodel.load_weights(file_path)\n\npreds_train = model.predict(X_train)\npreds_valid = model.predict(X_valid)\n\nprint(log_loss(y_train, preds_train))\nprint(log_loss(y_valid, preds_valid))","cell_type":"code"},{"outputs":[],"execution_count":null,"metadata":{"_cell_guid":"9d823373-aec8-4db7-a9de-8763feca62f2","_uuid":"4ec1a7c7f6824b6d929d612aa923749ddd7136bd","collapsed":true},"source":"#PREDICTION\npreds = pd.DataFrame(model.predict(X_test), columns=target_vars)\nsubmission = pd.concat([test[\"id\"],preds], 1)\nsubmission.to_csv(\"./submission.csv\", index=False)\nsubmission.head()","cell_type":"code"},{"metadata":{"_cell_guid":"8b647350-9f06-4bb4-b5c4-1bd0aa675672","_uuid":"3099d9906ea396643a9c6cdb5506a9dfe60f11f5","collapsed":true},"source":"Thanks to [SRK](https://www.kaggle.com/sudalairajkumar) for these good features of  his [kernel](https://www.kaggle.com/sudalairajkumar/simple-feature-engg-notebook-spooky-author?scriptVersionId=1667963) !!!","cell_type":"markdown"}],"nbformat":4,"nbformat_minor":1,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"nbconvert_exporter":"python","pygments_lexer":"ipython3","name":"python","file_extension":".py","version":"3.6.3","mimetype":"text/x-python"}}}