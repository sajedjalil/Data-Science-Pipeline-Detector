{"cells":[{"cell_type":"markdown","source":"## Spooky Spooky Exploration\n\nAn analysis revolving around Intelligibility\n\nBy Nick Brooks","metadata":{"_cell_guid":"b235f198-4449-4236-8a3a-ad3e06fa0620","_uuid":"3af1bd2c28670291d94009198da27e8988568481"}},{"outputs":[],"cell_type":"code","metadata":{"_kg_hide-input":true,"collapsed":true,"_cell_guid":"c4ab602c-7323-4f03-ae3e-f4b5a316d56e","_uuid":"842a12ccf02b42343cc9b4fb9c2e749f0bd19498"},"execution_count":null,"source":"# Packages\nimport os\nimport numpy as np\nimport pandas as pd\nimport nltk\nimport random\n\n# Pre-Processing\nimport string\nfrom nltk.tokenize import RegexpTokenizer\nfrom nltk.corpus import stopwords\nimport re\nfrom nltk.stem import PorterStemmer\nfrom nltk.stem.lancaster import LancasterStemmer\nfrom nltk.stem.porter import *\n\n# Sentiment Analysis\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\nfrom nltk.sentiment.util import *\nimport matplotlib.pyplot as plt\n\n# Visualization\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom subprocess import check_output\nfrom wordcloud import WordCloud, STOPWORDS\nimport seaborn as sns\n\n# N- Grams\nfrom nltk.util import ngrams\nfrom collections import Counter\n\n# Topic Modeling\nfrom nltk import WordNetLemmatizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.decomposition import NMF, LatentDirichletAllocation\n\n# Word 2 Vec\nfrom gensim.models import Word2Vec\nfrom sklearn.decomposition import PCA\n\n# Models\nimport datetime\nfrom nltk import naivebayes\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")"},{"cell_type":"markdown","source":"## Goal:\n\nAttempt to understand the themes and style of each author using statistical methods, instead of reading the books like a normal person..\n\n\n## Introduction:\n\nThis dataset is comprised of authors who mostly lived in the 19ths century. Earliest of the bunch, Mary Shelley is most famous for her work Frankenstein. Her Gothic Horror style refers to the architectural setting of her stories, which is an iconic style of the medieval era. I can easily imagine a dark stone castle in the night being confronted by a angry mob of villagers with torches and pitchforks! Through this environment, Shelley also explores the contemporary themes of the role of the individual in a secular government, carrying Locke and Rousseau’s burden into the world of fiction.\nAlthough Edgar Allan Poe is also known of residing in the realm of Gothic Horror, his writing is perhaps more centered around an urban setting. Furthermore, while Shelley resided in England and Europe, Poe was a Bostonian who was orphaned at an early age, and lacked the funds and parental guidance of Shelley.\nH.P Lovecraft, our final member of the Spooky club, also lived his life in the American Northeast. Similar to Poe, he also struggled to support himself as an author, and his health took a serious toll because of it. Lovecraft's early work is said to be influenced by Poe’s Macabre style. However, in the end, Lovecraft’s obsession with Astronomy led him to center his stories in a cosmic setting, exploring themes of human insignificance amongst ancient gods and prophecies. Do you hear its Call?\n\nSo what?\nThis brief introduction of the author’s artistic style indicate that there is a lot of overlap in terms of the setting and influence between these three authors. It would be interesting to test the integrity of this hypothesis by analysing the unsupervised technique of word vector of themes such as: Astronomy, Medieval Ages, Gods, Nihilism, Life. \n","metadata":{"collapsed":true,"_cell_guid":"54c86000-6374-42c1-a4b0-17cc5b08290b","_uuid":"cf7694dddbc7355629b4a3d8553726e372375770"}},{"outputs":[],"cell_type":"code","metadata":{"collapsed":true,"_cell_guid":"3d059977-9bbc-4794-aebc-609a34735aa6","_uuid":"de362e8c1dea8258261277b0f92249b305f8b92a"},"execution_count":null,"source":"import sys\nsys.version"},{"outputs":[],"cell_type":"code","metadata":{"collapsed":true,"_cell_guid":"ab0ae274-f457-4bf7-8581-73fb35bdb8cc","_uuid":"552b09b60a5122bda8b9b8e73d7794c99ed51bca"},"execution_count":null,"source":"# Read Data\ndf = pd.read_csv(\"../input/train.csv\", index_col=\"id\")\ntest = pd.read_csv(\"../input/test.csv\", index_col=\"id\")"},{"outputs":[],"cell_type":"code","metadata":{"collapsed":true,"_cell_guid":"1d065f08-a75c-4de9-8a2e-b26fbb5a6735","_uuid":"f4ace0d9496ffe6e64852080c1da2447668a9578","scrolled":false},"execution_count":null,"source":"pd.set_option('max_colwidth', 500)\ndf.text= df.text.astype(str)\ndf.author = pd.Categorical(df.author)\ndf.iloc[:20,:]"},{"cell_type":"markdown","source":"## First look at sentence samples:\nWow, these sentences sure are Spooky. Except for the last of the bunch, where Lovecraft comments on the mature look of a chinless child. From these samples so far, the writing style definitely seems drawn out and convoluted. Something a modern like me is not completely accustomed to..\n","metadata":{"_cell_guid":"52f22aab-d8fe-4e9c-976c-fc6cea1aa7f6","_uuid":"a9cbdee43f030d8a53c53f0b9f8c6b3b758451da"}},{"cell_type":"markdown","source":"## Pre-Processing\n\n- Lexicon Processing and Normalization\n- Lowercasing and Removing Punctuation\n- Tokenization\n- Removing Stopwords\n","metadata":{"_cell_guid":"76083173-14b1-498d-b35d-5a241ac27576","_uuid":"0ef77c7861dd2c3ead6c40c3d51ece36ec263b4a"}},{"outputs":[],"cell_type":"code","metadata":{"_kg_hide-input":true,"collapsed":true,"_cell_guid":"cdafd8bc-c4bd-40a5-84c8-afa17e3af70d","_uuid":"15cb312aff83cff4b04d4dc8b90f829177ce620b"},"execution_count":null,"source":"from nltk.stem.lancaster import LancasterStemmer\nfrom nltk.stem.porter import *\n#ps = LancasterStemmer()\nps = PorterStemmer()\n\ntokenizer = RegexpTokenizer(r'\\w+')\nstop_words = set(stopwords.words('english'))\n\ndef preprocessing(data):\n    txt = data.str.lower().str.cat(sep=' ') #1\n    words = tokenizer.tokenize(txt) #2\n    words = [w for w in words if not w in stop_words] #3\n    #ords = [ps.stem(w) for w in words] #4\n    return words\n\ndef wordfreqviz(text, x):\n    word_dist = nltk.FreqDist(text)\n    top_N = x\n    rslt = pd.DataFrame(word_dist.most_common(top_N),\n                    columns=['Word', 'Frequency']).set_index('Word')\n    matplotlib.style.use('ggplot')\n    rslt.plot.bar(rot=0)\n\ndef wordfreq(text, x):\n    word_dist = nltk.FreqDist(text)\n    top_N = x\n    rslt = pd.DataFrame(word_dist.most_common(top_N),\n                    columns=['Word', 'Frequency']).set_index('Word')\n    print(rslt)"},{"cell_type":"markdown","source":"## Sentiment Analysis\n\nI am hoping to get an idea of the general mood of each author. Understanding the mood could potentially point to the most “macabre” of them all.","metadata":{"_cell_guid":"c1c72d5d-6bc4-4fc5-a527-93e4a8611942","_uuid":"9b39978b4590034c829449020f53eb497c41ec78"}},{"outputs":[],"cell_type":"code","metadata":{"_kg_hide-input":true,"collapsed":true,"_cell_guid":"b5c87308-2f2e-4aa7-9cd9-4e4fe92a6072","_uuid":"db9b29a603c93f9036a157483fdb2f932aa03a39"},"execution_count":null,"source":"# Pre-Processing\nSIA = SentimentIntensityAnalyzer()\n\n# Applying Model, Variable Creation\nsentiment = df.copy()\nsentiment['polarity_score']=sentiment.text.apply(lambda x:SIA.polarity_scores(x)['compound'])\nsentiment['neutral_score']=sentiment.text.apply(lambda x:SIA.polarity_scores(x)['neu'])\nsentiment['negative_score']=sentiment.text.apply(lambda x:SIA.polarity_scores(x)['neg'])\nsentiment['positive_score']=sentiment.text.apply(lambda x:SIA.polarity_scores(x)['pos'])\nsentiment['sentiment']=''\nsentiment.loc[sentiment.polarity_score>0,'sentiment']='POSITIVE'\nsentiment.loc[sentiment.polarity_score==0,'sentiment']='NEUTRAL'\nsentiment.loc[sentiment.polarity_score<0,'sentiment']='NEGATIVE'\n\n# Normalize for Size\nauth_sent= sentiment.groupby(['author','sentiment'])[['text']].count().reset_index()\nfor x in ['EAP','HPL','MWS']:\n    auth_sent.text[auth_sent.author == x] = (auth_sent.text[auth_sent.author == x]/\\\n        auth_sent[auth_sent.author ==x].text.sum())*100"},{"outputs":[],"cell_type":"code","metadata":{"collapsed":true,"_cell_guid":"a36ff893-ddc0-4e65-a1d2-54518e340c36","_uuid":"66782e1424b2cee2f7eacbd7a2e81e700c94f7a5","scrolled":false},"execution_count":null,"source":"ax= sns.barplot(x='sentiment', y='text',hue='author',data=auth_sent)\nax.set(xlabel='Author', ylabel='Sentiment Percentage')\nax.figure.suptitle(\"Author by Sentiment\", fontsize = 24)\nplt.show()"},{"cell_type":"markdown","source":"After normalizing for sentence size by author, I am surprised to find that Lovecraft is the most negative writer, and that Shelley is the most polarized, with the least amount of neutral sentences. This suggests a high volatility between sentences, an author not afraid of taking her readers through a rollercoaster of emotions.\n\nAfter reading each author’s wikipedia page, I was really expecting Poe to top the negativity chart, since his writing seemed centered around bleak city streets. Now that I think about it, there isn’t anything too cheerful about the evil space squids, Cthulhu.\n","metadata":{"_cell_guid":"e0ca602a-c760-433d-915f-ed44a7c0184c","_uuid":"517f0ba2c85978013449d180b18b172e69f0ffd4"}},{"cell_type":"markdown","source":"## WordCloud\n\nWord clouds are great at shedding light on the author’s prefered language.","metadata":{"_cell_guid":"188c6547-56b0-4e74-89b9-97370a2b94cf","_uuid":"c904f64b10cb610d45ac3f03c5f06ac7696a0c90"}},{"outputs":[],"cell_type":"code","metadata":{"_kg_hide-input":true,"collapsed":true,"_cell_guid":"c551dc2b-c50f-46c9-ad91-730d5b824845","_uuid":"b59430083d5a79bc5017aeb3cca52028859b7e9a"},"execution_count":null,"source":"# Function\ndef cloud(text, title):\n    # Setting figure parameters\n    mpl.rcParams['figure.figsize']=(10.0,10.0)    #(6.0,4.0)\n    #mpl.rcParams['font.size']=12                #10 \n    mpl.rcParams['savefig.dpi']=100             #72 \n    mpl.rcParams['figure.subplot.bottom']=.1 \n    \n    # Processing Text\n    stopwords = set(STOPWORDS) # Redundant\n    wordcloud = WordCloud(width=1400, height=800,\n                          background_color='black',\n                          #stopwords=stopwords,\n                         ).generate(\" \".join(text))\n    \n    # Output Visualization\n    plt.figure(figsize=(20,10), facecolor='k')\n    plt.imshow(wordcloud)\n    plt.axis('off')\n    plt.tight_layout(pad=0)\n    plt.title(title, fontsize=50,color='y')\n    #plt.imshow(plt.recolor( colormap= 'Pastel1_r' , random_state=17), alpha=0.98)\n    #fig.savefig(\"wordcloud.png\", dpi=900)"},{"outputs":[],"cell_type":"code","metadata":{"collapsed":true,"_cell_guid":"3cea0578-e18d-4a88-955d-c567b098f26e","_uuid":"30c03ee1f4f73282fdb2027fe000eab6190750bf"},"execution_count":null,"source":"x = \"EAP\"\nprint(cloud(df[df.author == x]['text'].values,x))"},{"cell_type":"markdown","source":"This word cloud brings to light some of the propositional words, as well as some simple vocabulary. Unfortunately, the most defining characteristics of Poe do not emerge.","metadata":{"collapsed":true,"_cell_guid":"d86b97ee-0382-468c-8dbd-01dca86aa328","_uuid":"91af219a3e8f6ed160aece3024dc5785f999cabc"}},{"outputs":[],"cell_type":"code","metadata":{"collapsed":true,"_cell_guid":"314b18ae-28d6-4fe9-b79e-1ac78345cb6c","_uuid":"5be78bc8d8cf45e7a1db0467d45d1cf461803c53"},"execution_count":null,"source":"x = \"HPL\"\nprint(cloud(df[df.author == x]['text'].values,x))"},{"cell_type":"markdown","source":"\"Seemed\", \"Thing\", \"Old\", \"Night\" are words I would expect in the Lovecraft mythos.","metadata":{"_cell_guid":"83bb2d99-ac26-49b7-b4e2-4a7174426006","_uuid":"352543fc2916e0aaca333c6a3e59b0e0ce4f6d48"}},{"outputs":[],"cell_type":"code","metadata":{"collapsed":true,"_cell_guid":"019b2a1c-4cc1-47ef-8cd3-02753bbe0320","_uuid":"b5282d3a8a76a61c49ea78cc45b5f09cbcc911e4"},"execution_count":null,"source":"x = \"MWS\"\nprint(cloud(df[df.author == x]['text'].values,x))"},{"cell_type":"markdown","source":"My suspicions that Shelley would focus on existential themes is correct. “Life”, “heart”, “love”, “soul”, “death” seem right out of Frankenstein. Unfortunately, the dataset does not indicate the novels included.","metadata":{"_cell_guid":"d2b36d52-77b4-48ee-a38d-11d0c037791d","_uuid":"1168a46c4834fab3860c2c14c1348f42d611f86a"}},{"cell_type":"markdown","source":"## N Grams\n\nAnother tool leveraging statistical frequency.. N-Grams! This method finds the most common sequence of words by N length. While the word cloud has a sequence length of 1, N-Grams may provide insight into the prose of the author.","metadata":{"_cell_guid":"9c690025-fc1f-41ec-bbbc-3f90f91c7f61","_uuid":"3243d4e9c0382d6b5f47aad865a949363fd62f36"}},{"outputs":[],"cell_type":"code","metadata":{"_kg_hide-input":true,"collapsed":true,"_cell_guid":"5a2063bc-c993-4e27-88bd-eb3dce2e8f44","_uuid":"dee5c78b6d9a37b6f482755186daf19096fba855"},"execution_count":null,"source":"## Helper Functions\ndef get_ngrams(text, n):\n    n_grams = ngrams((text), n)\n    return [ ' '.join(grams) for grams in n_grams]\n\ndef gramfreq(text,n,num):\n    # Extracting bigrams\n    result = get_ngrams(text,n)\n    # Counting bigrams\n    result_count = Counter(result)\n    # Converting to the result to a data frame\n    df = pd.DataFrame.from_dict(result_count, orient='index')\n    df = df.rename(columns={'index':'words', 0:'frequency'}) # Renaming index column name\n    return df.sort_values([\"frequency\"],ascending=[0])[:num]\n\ndef gram_table(x, gram, length):\n    out = pd.DataFrame(index=None)\n    for i in gram:\n        table = pd.DataFrame(gramfreq(preprocessing(df[df.author == x]['text']),i,length).reset_index())\n        table.columns = [\"{}-Gram\".format(i),\"Occurence\"]\n        out = pd.concat([out, table], axis=1)\n    return out"},{"cell_type":"markdown","source":"#### Edgar Allan Poe\n\nMr. Crab? (#13 on 2-Gram), did a Spongebob text sample get sneaked in here? Interestingly, 2-Grams has a few French words such as “L’etoile”, and “Espanaye”. Other than that, “ha ha” also stands out, since I was under the impression that this was unique to millennial texting.\n\nIn terms of 3-Grams, this sequence length gives a good idea of some of the major characters and locations.\n\nFinally, 4-Grams also give insight in some strange sayings, like “ugh ugh ugh ugh”, and “mille mille mille mille” which is just four times “thousand” in French.","metadata":{"_cell_guid":"1a4ec41c-1a78-40ca-9467-3d646f021746","_uuid":"92ef90814705aa1b69352933fe093433443beee4"}},{"outputs":[],"cell_type":"code","metadata":{"collapsed":true,"_cell_guid":"9ea53e31-bd4f-450a-af6f-8f42168a60e2","_uuid":"63768f82a9116c13ca99df8f23454396ced8d528"},"execution_count":null,"source":"gram_table(x=\"EAP\", gram=[1,2,3,4], length=20)"},{"cell_type":"markdown","source":"#### HP Lovecraft\n\n“Heh heh heh heh..” said the old women small furry. Lovecraft’s N-Grams could have its own Card Against Humanities edition. His N-Grams give a good amount of insight on his characters and iconic objects, such as the necronomicon, and a slew of characters with strange, ominous features.\n","metadata":{"_cell_guid":"e2cbdde5-0cbf-4fa8-a341-11f58b64a560","_uuid":"046bfb3851ba4db650e23cb3d18ee4b4dc4faf9e"}},{"outputs":[],"cell_type":"code","metadata":{"collapsed":true,"_cell_guid":"145a1d4e-c749-4670-8899-0d2f23bbe9a4","_uuid":"879929b630acc24ecfd138156b96c62ed6b1ed8a"},"execution_count":null,"source":"gram_table(x=\"HPL\", gram=[1,2,3,4], length=20)"},{"cell_type":"markdown","source":"#### Mary Shelley\n\nThe theme of life and mortality is not evident in much of the higher N-Grams. It is interesting to note that the occurrences of 3 and 4 Grams are very low, suggesting that Shelley does not make use of drawn out phrases. \n","metadata":{"_cell_guid":"9380b72b-e73f-4c9d-b50b-791a218efad8","_uuid":"939ed47fdbf2231810ebd725e7cb82cc172826e0"}},{"outputs":[],"cell_type":"code","metadata":{"collapsed":true,"_cell_guid":"78924925-6a59-4fb4-ab5f-5dbf0ca020a7","_uuid":"978cf1a70a0f3433b303decd69c356ea98fcc18b"},"execution_count":null,"source":"gram_table(x=\"MWS\", gram=[1,2,3,4], length=20)"},{"cell_type":"markdown","source":"#### N-Gram conclusion\n\nWithin this genre of horror, there seems to be a strange interest in “old man”.\n","metadata":{"_cell_guid":"f7ff4ce4-8915-413e-be91-eb5d4690a044","_uuid":"d7afc336023201b39a5a17ef25a993f6fef61eb3"}},{"cell_type":"markdown","source":"## Unsupervised Learning/ Topic Modeling\n\nMy next goal is to try to use more advanced methods to extract some of the themes, or topics within each author. ","metadata":{"_cell_guid":"0d56e60f-f1dc-40dc-932e-a43806e5c97d","_uuid":"8be50b2d5a7390438cfb306d637dbffbbedfd9c9"}},{"cell_type":"markdown","source":"## Latent Dirichlet Allocation\n*Full Credit goes to Anisotropic! Check out his amazing notebook*. I was very impressed by his analysis and wanted to see if topics could be applied within each author.\n\nhttps://www.kaggle.com/arthurtok/spooky-nlp-and-topic-modelling-tutorial\n\nThe LDA model uses probability to output the top words of each cluster.","metadata":{"_cell_guid":"8e512765-0bc9-47c3-8f9c-3abbb999e56b","_uuid":"e14096a48f34bf015472f5a5630fefa674a07611"}},{"outputs":[],"cell_type":"code","metadata":{"_kg_hide-input":true,"collapsed":true,"_cell_guid":"c12501fd-307c-419e-aea7-dc42b7583f2d","_uuid":"b6a73c6395554794f37f1e786297566a75bbf2d5"},"execution_count":null,"source":"lemm = WordNetLemmatizer()\nclass LemmaCountVectorizer(CountVectorizer):\n    def build_analyzer(self):\n        analyzer = super(LemmaCountVectorizer, self).build_analyzer()\n        return lambda doc: (lemm.lemmatize(w) for w in analyzer(doc))\n    \n# Define helper function to print top words\ndef print_top_words(model, feature_names, n_top_words):\n    for index, topic in enumerate(model.components_):\n        message = \"\\nTopic #{}:\".format(index)\n        message += \" \".join([feature_names[i] for i in topic.argsort()[:-n_top_words - 1 :-1]])\n        print(message)\n        print(\"=\"*70)\n    \ndef LDA(data):\n    # Storing the entire training text in a list\n    text = list(data.values)\n    # Calling our overwritten Count vectorizer\n    tf_vectorizer = LemmaCountVectorizer(max_df=0.95, min_df=2,\n                                              stop_words='english',\n                                              decode_error='ignore')\n    tf = tf_vectorizer.fit_transform(text)\n\n\n    lda = LatentDirichletAllocation(n_topics=6, max_iter=5,\n                                    learning_method = 'online',\n                                    learning_offset = 50.,\n                                    random_state = 0)\n\n    lda.fit(tf)\n\n    n_top_words = 10\n    print(\"\\nTopics in LDA model: \")\n    tf_feature_names = tf_vectorizer.get_feature_names()\n    print_top_words(lda, tf_feature_names, n_top_words)"},{"cell_type":"markdown","source":"#### Edgar Allan Poe","metadata":{"_cell_guid":"e3a891b4-48f2-458e-954f-13f656f21e0e","_uuid":"029f1af5dac5f3e4a7b1e3351b3b49f580c91625"}},{"outputs":[],"cell_type":"code","metadata":{"collapsed":true,"_cell_guid":"b9cab8f0-a31c-409d-befa-ac68d30fc0a9","_uuid":"80a2a49c7c8f698361e802df7740e8a248e8e0b9","scrolled":false},"execution_count":null,"source":"x = \"EAP\"\nLDA(df.text[df.author==x])"},{"cell_type":"markdown","source":"To me, topic #1 seems the most interesting since it suggest some kind of existential theme.\n\n\n#### Mary Shelley","metadata":{"_cell_guid":"7a00cf96-7c7f-4d13-bb1b-963311209e0c","_uuid":"2b9e1b3213b04df3e93146c735b228316f6f7d11"}},{"outputs":[],"cell_type":"code","metadata":{"collapsed":true,"_cell_guid":"b392c596-9268-4fce-99e9-9bfffcdc3361","_uuid":"5d7c1cb20303fb2239abc34ff673dfd8abc00a87"},"execution_count":null,"source":"x = \"MWS\"\nLDA(df.text[df.author==x])"},{"cell_type":"markdown","source":"Words such as \"Love\", \"Life\", \"Hope\" reinforce the observations made from the N-Grams and my prior assumptions about Shelley's themes.\n\n#### HP Lovecraft","metadata":{"_cell_guid":"87f7ef27-e88b-4bca-b750-9c108a10828f","_uuid":"d4c345039ff48775def2ca76fa08a40b0bea3218"}},{"outputs":[],"cell_type":"code","metadata":{"collapsed":true,"_cell_guid":"c1cb347f-ec82-4845-816d-414bca84a676","_uuid":"39ccea1ba25abbeb909e63d6cf9c4b2f17675ecf"},"execution_count":null,"source":"x = \"HPL\"\nLDA(df.text[df.author==x])"},{"cell_type":"markdown","source":"Mostly setting centric topics.\n\n## Word 2 Vec","metadata":{"_cell_guid":"c8b66ef1-512f-4ec1-9efb-be84700e2d40","_uuid":"d6ad3d60bbba678539dd149e94e1ffb29257914d"}},{"outputs":[],"cell_type":"code","metadata":{"_kg_hide-input":true,"collapsed":true,"_cell_guid":"00896219-a0c6-4f1f-818e-e1081a61b912","_uuid":"1b26a720e02a94592f599e726397488871ecb45d"},"execution_count":null,"source":"def model_prep(df_in):\n    df_in['tokenized'] = df_in.text.astype(str).str.lower() # turn into lower case text\n    df_in['tokenized'] = df_in.apply(lambda row: tokenizer.tokenize(row['tokenized']), axis=1) # apply tokenize to each row\n    df_in['tokenized'] = df_in['tokenized'].apply(lambda x: [w for w in x if not w in stop_words]) # remove stopwords from each row\n    #df_in['tokenized'] = df_in['tokenized'].apply(lambda x: [ps.stem(w) for w in x]) # apply stemming to each row\n    return df_in\n\ndef w2vec(data,yrange):\n    wvec = model_prep(data)\n    model = Word2Vec(data.tokenized, min_count=1, max_vocab_size=250)\n    # model.save('model.bin')\n    # new_model = Word2Vec.load('model.bin')\n    \n    # summarize the loaded model\n    print(model)\n\n    X = model[model.wv.vocab]\n    pca = PCA(n_components=2)\n    result = pca.fit_transform(X)\n    # create a scatter plot of the projection\n    plt.rcParams[\"figure.figsize\"] = [16,9]\n\n    plt.scatter(result[:, 0], result[:, 1])\n    words = list(model.wv.vocab)\n    for i, word in enumerate(words):\n        plt.annotate(word, xy=(result[i, 0], result[i, 1]))\n\n    plt.ylim(yrange)   \n\n    plt.show()\n    \n"},{"outputs":[],"cell_type":"code","metadata":{"collapsed":true,"_cell_guid":"cb9d2fc8-67a7-4ee1-bc8e-c3323afc05a6","_uuid":"8fed81866ac35a0927d7e6497a9276a96400afbf"},"execution_count":null,"source":"x = \"MWS\"\nw2vec(df[df.author==x],[-.015,.015])"},{"outputs":[],"cell_type":"code","metadata":{"collapsed":true,"_cell_guid":"871d5b86-4c75-4ad9-8260-139268f1de96","_uuid":"0993150e511fe799d3bc5de8c2f1608becb5ae43"},"execution_count":null,"source":"x = \"EAP\"\nprint(\"\\n\",x)\nw2vec(df[df.author==x],[-.014,.014])"},{"outputs":[],"cell_type":"code","metadata":{"collapsed":true,"_cell_guid":"e6109094-7d4d-4123-bd54-d905dacb144c","_uuid":"6a18839681d52d110689173ced59dc54cdc24c57"},"execution_count":null,"source":"x = \"HPL\"\nprint(\"\\n\",x)\nw2vec(df[df.author==x],[-.015,.015])"},{"cell_type":"markdown","source":"## Naive Bayes\n\nGenerative Models based off Bayes' Rule and Conditional Probabilities","metadata":{"_cell_guid":"a69a139c-4a2c-4461-b51c-078f94bcad56","_uuid":"8e5b0caf01a45f03dac9672a70191ba35f2ba15c"}},{"outputs":[],"cell_type":"code","metadata":{"collapsed":true,"_cell_guid":"26d1e80b-7207-440f-a572-5466d2bca125","_uuid":"4462e2f97ca67da00052bc36fe7c154eb992f542"},"execution_count":null,"source":"print(\"Train Vocabulary Size: {}\".format(len(nltk.FreqDist(preprocessing(df['text'])))))\nprint(\"Train Size: {}\".format(len(df)))\nprint(\"Test Vocabulary Size: {}\".format(len(nltk.FreqDist(preprocessing(test['text'])))))\nprint(\"Test Size: {}\".format(len(test)))"},{"outputs":[],"cell_type":"code","metadata":{"_kg_hide-input":true,"collapsed":true,"_cell_guid":"9660e866-bc2b-4b5f-963e-e93df0767dd9","_uuid":"e57d52545c638712d6406e91449c4c969b92cc94"},"execution_count":null,"source":"# Number of features\nall_words = nltk.FreqDist(preprocessing(df['text'])) # Calculate word occurence from whole block of text\nword_features = list(all_words.keys())[:20000] \n# Number of columns (can't exceed vocab, only shrink it) from largest to smallest\n\n# Helper Functions\n# for each review, records which uniqeue words out of the whole text body are present\ndef find_features(document):\n    words = set(document)\n    features = {}\n    for w in word_features:\n        features[w] = (w in words)\n\n    return features\n\n# Function to create model features\ndef model_prep(state, df_in):\n    df_in['tokenized'] = df_in.text.astype(str).str.lower() # turn into lower case text\n    df_in['tokenized'] = df_in.apply(lambda row: tokenizer.tokenize(row['tokenized']), axis=1) # apply tokenize to each row\n    df_in['tokenized'] = df_in['tokenized'].apply(lambda x: [w for w in x if not w in stop_words]) # remove stopwords from each row\n    df_in['tokenized'] = df_in['tokenized'].apply(lambda x: [ps.stem(w) for w in x]) # apply stemming to each row\n    if state == \"Train\":\n        print(\"{} Word Features: {}\".format(state, len(word_features)))\n        print(\"All Possible words in {} set: {}\".format(state, len(all_words)))\n        # Bag of Words with Label\n        featuresets = [(find_features(text), LABEL) for (text, LABEL) in list(zip(df_in.tokenized, (df_in.author)))]\n        print(\"Train Set Size: {}\".format(len(featuresets)))\n        print(\"Train Set Ready\")\n        return featuresets, word_features\n    else:\n        # Bag of Words without Labels\n        featuresets = [(find_features(text)) for (text) in list(df_in.tokenized)]\n        print(\"Submission Set Size: {}\".format(len(featuresets)))\n        print(\"Submission Set Ready\")\n        return featuresets"},{"outputs":[],"cell_type":"code","metadata":{"collapsed":true,"_cell_guid":"3424fc5d-ffe3-4edc-a2aa-a95a18675895","_uuid":"dee64c1bb0e2749fe79e391c01fb10800afa30e9"},"execution_count":null,"source":"trainset, word_features= model_prep(\"Train\", df_in=df)"},{"outputs":[],"cell_type":"code","metadata":{"collapsed":true,"_cell_guid":"98a04513-135c-4b00-ac3f-de051c99464b","_uuid":"221740d62977040f27bcc2ed8fa2a0f13eb211f9"},"execution_count":null,"source":"submissionset = model_prep(\"Test\", df_in=test)"},{"outputs":[],"cell_type":"code","metadata":{"collapsed":true,"_cell_guid":"6a77c12b-a159-49d7-a8f0-40f9cc0f4828","_uuid":"b959d48d96d79c67065e392c1769a6be0ae0599f"},"execution_count":null,"source":"training_set = trainset[:15000]\ntesting_set = trainset[15000:]\ndel trainset"},{"cell_type":"markdown","source":"## Execute Model","metadata":{"_cell_guid":"9dc700aa-e99a-488a-84e4-b2764e55f0c4","_uuid":"427611725ecbd379945c27f56c61566ebbe61d9a"}},{"outputs":[],"cell_type":"code","metadata":{"collapsed":true,"_cell_guid":"5c454544-f85b-48bb-bbf5-a979d59e28dd","_uuid":"8bdd9d0da10f0fb58fdb89c94c58e9ecb32a9c24"},"execution_count":null,"source":"start = time.time()\nclassifier = nltk.NaiveBayesClassifier.train(training_set)\n# Posterior = prior_occurence * likelihood / evidence\nend = time.time()\nprint(\"Model took %0.2f seconds to train\"%(end - start))"},{"outputs":[],"cell_type":"code","metadata":{"collapsed":true,"_cell_guid":"07ffb111-974c-4721-84f7-08f909ac9532","_uuid":"dc3b9f47047bdd5cb260db2a2e1033c9fc44a61f"},"execution_count":null,"source":"# Edgar Allan Poe [EAP], Mary Shelley[MWS], and HP Lovecraft[HPL]"},{"outputs":[],"cell_type":"code","metadata":{"collapsed":true,"_cell_guid":"210517b5-cb9f-47b4-862c-cda1cc72918d","_uuid":"6164ec398cd40b72c75fc2f119d48b0ff7967a10","scrolled":false},"execution_count":null,"source":"print(\"Classifier Test Accuracy:\",(nltk.classify.accuracy(classifier, testing_set))*100)\nprint(classifier.show_most_informative_features(40))"},{"outputs":[],"cell_type":"code","metadata":{"collapsed":true,"_cell_guid":"efc5bcb1-9da7-484c-a9b9-9a6e56124671","_uuid":"f7e169ad81739f7f1deb3d7521e856eab547564b"},"execution_count":null,"source":"classifier.labels()"},{"outputs":[],"cell_type":"code","metadata":{"collapsed":true,"_cell_guid":"8ed6f754-712c-4b2b-a2f5-fd6c04c621f9","_uuid":"43411648ea57864bff445d0cb693e3d83694682c"},"execution_count":null,"source":"labels = classifier.labels()\nsubmission = pd.DataFrame(columns=labels)\nfor x in submissionset:\n    dist = classifier.prob_classify(x)\n    submission= submission.append({labels[0]:dist.prob(labels[0]),\n                                   labels[1]:dist.prob(labels[1]),\n                                   labels[2]:dist.prob(labels[2])},ignore_index=True)\nsubmission[\"id\"] = test.index\nsubmission= submission[[\"id\", \"EAP\",\"HPL\",\"MWS\"]]"},{"outputs":[],"cell_type":"code","metadata":{"collapsed":true,"_cell_guid":"7b63af9a-3621-4f34-bc31-0f33e2dee308","_uuid":"e83955381a202243f7fc0b58d7a5483828f52108"},"execution_count":null,"source":"submission.head()"},{"outputs":[],"cell_type":"code","metadata":{"collapsed":true,"_cell_guid":"631ae152-2f33-4a59-83a1-8b670cbf6a04","_uuid":"14f99d06c0d6387f049980a29d3d80b384dcc538"},"execution_count":null,"source":"submission.to_csv(\"naive_spooky.csv\", index=False)"}],"nbformat":4,"metadata":{"language_info":{"name":"python","codemirror_mode":{"name":"ipython","version":3},"nbconvert_exporter":"python","mimetype":"text/x-python","file_extension":".py","version":"3.6.3","pygments_lexer":"ipython3"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"}},"nbformat_minor":1}