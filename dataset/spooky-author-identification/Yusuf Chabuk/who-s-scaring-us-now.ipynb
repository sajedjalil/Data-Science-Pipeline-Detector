{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Introduction\n\nThe objective of this task is to identify the author of excerpts from the works of three famous horror writers, Edgar Allan Poe, Mary Shelley, and HP Lovecraft. \n\nA brief summary of the code: I'll begin with the preparation of the text data (read, convert into a dataframe, clean and lemmatize), then convert the text data into vectors, determine the features and create the models. I'll use Countvectorizer to vectorize the text data and MultinomialNB, LogisticRegression and XGBClassifier for prediction models. \n\nLet me begin by importing the libs/mods I'll be using in this notebook:"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport re\nimport string\nfrom string import punctuation \nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nfrom collections import Counter\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.preprocessing import LabelEncoder \nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.compose import make_column_transformer\nfrom sklearn import model_selection\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.linear_model import LogisticRegression\nfrom xgboost import XGBClassifier\n\nlabelencoder = LabelEncoder()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_set=pd.read_csv(\"../input/spooky-author-identification/train.zip\")\ntest_set=pd.read_csv(\"../input/spooky-author-identification/test.zip\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_set.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We convert the data into a dataframe and see how it looks like (as usual (:)"},{"metadata":{"trusted":true},"cell_type":"code","source":"char_set = \"[!#$%&'()*+,./:;<=>?@[\\]^_`{|}„“~—\\\"\\-]–+«»…\"\nstopwords_en = stopwords.words(\"english\")+[\"one\",\"le\",\"de\",\"u\",\"us\"]\n                   \nlemmatizer=WordNetLemmatizer()\n\ndef lemmatize(sent):\n\n    sent = re.sub('\\w*\\d\\w*', '', sent)\n    \n    nopunct=[ch for ch in sent if ch not in char_set]\n    nopunct=''.join(nopunct)\n    \n    lemmas = []\n    for lemma in nopunct.split():\n        lemma=lemma.lower()\n        lemma = lemma.strip()\n        if lemma not in stopwords_en and len(lemma)!=1:\n            lemma = lemmatizer.lemmatize(lemma)\n            lemmas.append(lemma)\n       \n    return (lemmas)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This is the code I'll be using for cleaning the text data; getting rid of the punctuation marks and stopwords, and also for lemmatizing it."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_set['lems']=''\ntrain_set['lemphrases']=''\n    \nfor i,j in enumerate(train_set['text']):\n    train_set['lems'][i]=lemmatize(j)\n    train_set['lemphrases'][i]=\" \".join(train_set['lems'][i])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_set.sample(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_words={\"EAP\":[], \"HPL\":[], \"MWS\":[]}\naw=[]\n\nfor auth in all_words.keys():\n    for line in train_set[train_set[\"author\"]==auth][\"lems\"]:\n        for w in line:\n            all_words[auth].append(w)\n            aw.append(w)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Counter(aw).most_common()[:10]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Some of the most common words may actually be added to the list of stopwords, but I personally think such words may give a clue about the style and wording of the author."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"all_words_unique={}\nall_words_unique[\"EAP\"]=set(all_words['EAP'])-(set(all_words['HPL'])| set(all_words['MWS']))\nall_words_unique[\"HPL\"]=  set(all_words['HPL'])-(set(all_words['EAP']) | set(all_words['MWS']))\nall_words_unique[\"MWS\"]= set(all_words['MWS'])-(set(all_words['EAP'])| set(all_words['HPL']))\n\nauth_counts={\"EAP\": len(all_words_unique[\"EAP\"]),\n             \"HPL\": len(all_words_unique[\"HPL\"]),\n             \"MWS\":len(all_words_unique[\"MWS\"])\n             }\n\nfig_sizes = {'S' : (6.5,4),\n             'M' : (9.75,6),\n             'L' : (13,8)}\n\ndef show_plot(f_size=(6.5,4),plot_title=\"\",x_title=\"\",y_title=\"\"):\n    plt.figure(figsize=f_size)\n    plt.xlabel(x_title)\n    plt.ylabel(y_title)\n    plt.title(plot_title)\n\nax_bp = show_plot((6.5,4),'Unique Vocabulary by Author','Author','Count')\n#sns.barplot(x=list(auth_counts.keys()), y=list(auth_counts.values()), ax=ax_bp)\nsns.barplot(x=list(all_words_unique.keys()), y=list(len(i) for i in all_words_unique.values()), ax=ax_bp)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We see that out of the three authors Mary Shelley has got the least unique vocabulary. I'm not going to use the uniqueness of the vocabulary by author in thie notebook, but it may be worth thinking about how that could be transformed into a feature.\n\n\nNow I'll test several models, with the original text, lemmatized phrases and also lemmatized phrases including the  stopwords, and also a combination of all of them in a pipeline."},{"metadata":{},"cell_type":"markdown","source":"### MultinomialNB"},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"training_scores=[]\nvalidation_scores=[]\n\ndef model_selector(data):\n\n    y=data['author']\n    \n    y = labelencoder.fit_transform(y)\n    \n    \n    for i in ['text', 'lemphrases']:\n        \n        X=data[i]\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True, random_state=42)\n\n        CV=CountVectorizer(ngram_range=(1,4), min_df=2)\n        transformer=CV.fit(X_train)\n\n        text_train=transformer.transform(X_train)\n        text_test=transformer.transform(X_test)\n\n        model = MultinomialNB()\n        model = model.fit(text_train, y_train)\n        \n        score=model.score(text_train, y_train)\n        training_scores.append(score)        \n \n        score=model.score(text_test, y_test)\n        validation_scores.append(score)      \n\n    Xp=data.drop(['id','author', 'lems'], axis=1)\n    yp=data['author']\n    yp = labelencoder.fit_transform(yp)\n    Xp_train, Xp_test, yp_train, yp_test = train_test_split(Xp, yp, test_size=0.2, shuffle=True, random_state=42)\n    \n    p_transformer=make_column_transformer((CountVectorizer(ngram_range=(1,4), min_df=2), \"text\"),\n                                            (CountVectorizer(ngram_range=(1,3), min_df=2), \"lemphrases\")\n                                            )\n\n    p_model=make_pipeline(p_transformer, MultinomialNB())\n    p_model.fit(Xp_train, yp_train)\n    \n    score=p_model.score(Xp_train, yp_train)\n    training_scores.append(score)\n    \n    score=p_model.score(Xp_test, yp_test)\n    validation_scores.append(score)   \n     \n    \nmodel_selector(train_set) \n\nresults=pd.DataFrame()\nresults[\"Features\"]=[\"Original Text\",\"Lemmatized\",\"Pipeline\"]\nresults[\"Training\"]=training_scores\nresults[\"Validation\"]=validation_scores\n\nresults","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"I've used a quite basic method here: \n- Vectorize the text data (both the original text and also the lemmatized one (\"lemphrases\") with Countvectorizer;\n- Turn the target values (y) into numerical labels with LabelEncoder;\n- Train and test both the original texts and lemmatized phrases separately;\n- Train and test both the original texts and lemmatized phrases together by creating a pipeline.\n\nI almost always get better results by the pipeline though it's usually by around 1% only. I've also used larger ngrams for the original text.."},{"metadata":{},"cell_type":"markdown","source":"### XGBClassifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"def xgb_prediction(X,y):\n\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n    CV=CountVectorizer(ngram_range=(1,4), min_df=2)\n    transformer=CV.fit(X_train)\n\n    text_train=transformer.transform(X_train)\n    text_test=transformer.transform(X_test)\n\n    xgb_model = XGBClassifier(learning_rate=0.1, max_depth=8, n_estimators=250, random_state=42)\n    xgb_model.fit(text_train, y_train)\n        \n    from sklearn.calibration import CalibratedClassifierCV\n    cal_model = CalibratedClassifierCV(xgb_model, method=\"sigmoid\", cv=3)\n    cal_model.fit(text_train, y_train)\n        \n    #print(f\"Training score: {xgb_model.score(text_train, y_train)}\") \n    print(f\"Validation score: {xgb_model.score(text_test, y_test)}\")\n    print(f\"Calibrated Validation score: {cal_model.score(text_test, y_test)}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X=train_set[\"text\"]\ny=train_set[\"author\"]\ny=labelencoder.fit_transform(y)\nxgb_prediction(X,y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I'm trying the model for the original texts first and later I'll repeat it for the lemmatized phrases as well. It's interesting to note that we get better results with non-lemmatized text input without calibration than the lemmatized input (with or without calibration)."},{"metadata":{"trusted":true},"cell_type":"code","source":"X=train_set[\"lemphrases\"]\nxgb_prediction(X,y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I've already used Naive Bayes' MultinomialNB above and here I'm using XGBClassifier as an estimator. It's possible to do some cross validation and also check the results for various parameters by using, for example, GridSearchCV. I've also implemented some calibration here and I've obtained better results with the original texts (non-cleaned data :)) It's possible to get much better results with bigger n_estimators and max_depth (like 250 or 500 to 10-12, for example),but it takes significantly longer to train and test the model."},{"metadata":{"trusted":true},"cell_type":"code","source":"test_set['lems']=''\ntest_set['lemphrases']=''\n    \nfor i,j in enumerate(test_set['text']):\n    test_set['lems'][i]=lemmatize(j)\n    test_set['lemphrases'][i]=\" \".join(test_set['lems'][i])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We process the test data the same way as we did with the train/validation data."},{"metadata":{},"cell_type":"markdown","source":"### LogisticRegression (with a pipeline)"},{"metadata":{"trusted":true},"cell_type":"code","source":"yp=train_set['author']\nXp=train_set.drop(['id','author', 'lems'], axis=1)\nyp=labelencoder.fit_transform(yp)\n\nXp_train, Xp_test, yp_train, yp_test = train_test_split(Xp, yp, test_size=0.2, shuffle=True, random_state=42)\n\np_transformer=make_column_transformer((CountVectorizer(ngram_range=(1,4), min_df=2), \"text\"),\n                                        (CountVectorizer(ngram_range=(1,3), min_df=2), \"lemphrases\")\n                                        )\n\np_model=make_pipeline(p_transformer, LogisticRegression(penalty=\"l2\", max_iter=2000, solver=\"newton-cg\"))\np_model.fit(Xp_train,yp_train)\np_model.score(Xp_train,yp_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"p_model.score(Xp_test,yp_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### LogisticRegression (Kfold cross validation)"},{"metadata":{"trusted":true},"cell_type":"code","source":"kfold = model_selection.KFold(n_splits=3, shuffle=True, random_state=2323)\nresults = model_selection.cross_val_score(p_model, Xp, yp, cv=kfold)\nprint(\"Accuracy: %.1f%%\" % (results.mean()*100.0))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f_model=p_model.fit(Xp, yp) #final model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Xt=test_set.drop(['id','lems'], axis=1)\nprobs=f_model.predict_proba(Xt)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i,j in enumerate(probs):\n    for k,m in enumerate(j):\n        if m>0.99:\n            probs[i][k]=0.98\n        if m<0.01:\n            probs[i][k]=0.01\n#We do this in order to get rid of too small or too big values so as to reduce the high variance.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.set_option('display.float_format', lambda x: '%.10f' % x)\npreds=pd.DataFrame(data=probs, columns = [\"EAP\",\"HPL\",\"MWS\"])\nresults3 = pd.concat([test_set[['id']], preds], axis=1)\n\nresults3.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Et, voila, les resultats! :)) They aren't the best and obviously far from perfection, but it's possible to improve with more complex models and by more careful calibrations. For example taking the average of MultinomialNb and LogisticRegression model results gives better predictions. It's also possible to try this by taking the average of the results from the three models above. Alternatively, and even better, to use Simple Linear Regression with the results from 2-3 different models."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}