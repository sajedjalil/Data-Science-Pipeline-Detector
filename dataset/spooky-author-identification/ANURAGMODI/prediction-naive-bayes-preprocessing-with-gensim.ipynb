{"cells":[{"source":"# Introduction\n\nIn this notebook, we will be representing our documents as a TF-IDF vector , which is a way to model your text data into vectors , because all the libraries which you will generally use, tends to represent the data  into mathematical models. Then we will train a classifer(naiveBayes, SVM) on the dataset and then will evaluate our model. The tutorial will progress as follows:\n\n\n1. **Preprocess your Data with Gensim** - We will be using gensim library along with nltk, to quickly and efficiently preprocess our data. \n\n2. **Building the Model** - We will be training Naive Bayes classifier on our data\n\n3. **Train the Model** - We will be training Naive Bayes classifier on our data\n\n4. **Prediction**\n\n5. **Evaluating the Model** - We will be evaluating our model\n\n**Note:** I won't be doing a lot of Exploratory Data analysis as there are already public kernels available. Please go through **Spooky NLP and Topic Modelling tutorial** for data visualization purpose","cell_type":"markdown","metadata":{"_cell_guid":"9e222406-13a5-4b68-9a6f-8aa1dc7357b3","_uuid":"b3289510811141b46a0a923361f9b84f9a6a1611"}},{"outputs":[],"metadata":{"_cell_guid":"f528cef0-7c8c-4c87-bcc4-be3cc215fbb2","_uuid":"86d55df4366588c592aa929b419fc83cb66b14f4","collapsed":true},"cell_type":"code","source":"# read in some helpful libraries\nimport nltk                       # the natural langauage toolkit, open-source NLP\nimport pandas as pd               # pandas dataframe\nimport re                         # regular expression\nfrom nltk.corpus import stopwords  \nfrom gensim import parsing        # Help in preprocessing the data, very efficiently\nimport gensim\nimport numpy as np\n\n# Loading in the training data with Pandas\ndf_train = pd.read_csv(\"../input/train.csv\")","execution_count":2},{"outputs":[],"metadata":{"_cell_guid":"7f8673ec-ad86-4ae3-a4f7-4fc9d57febd1","_uuid":"71b523d00ef5c1b0b84400944d55eff35674e191"},"cell_type":"code","source":"# look at the first few rows and how the text looks like\nprint (df_train['text'][2]) , '\\n'\ndf_train.head()","execution_count":3},{"outputs":[],"metadata":{"_cell_guid":"a1379b65-2bcb-4d2b-9678-9f103212d2bb","_uuid":"36f8b401b35888936af29ab799be4e52ab89d311"},"cell_type":"code","source":"## check the dimensions of the table\nprint (\"Shape:\", df_train.shape, '\\n')\n\n## Check if there is any NULL values inside the dataset\nprint (\"Null Value Statistics:\", '\\n \\n', df_train.isnull().sum()) ## Sum will tell the total number of NULL values inside the dataset\nprint ('\\n')\n\n## Explore the data types of your dataset\nprint (\"Data Type of All Columns:\" '\\n \\n', df_train.dtypes)","execution_count":4},{"outputs":[],"metadata":{"_cell_guid":"60e1ef11-9ee4-4544-94da-1731fb9c438b","_uuid":"66596dd7e693966f5a7ce4049fc227a6ebf50172"},"cell_type":"code","source":"## Collect all unique author names from author column\nauthor_names = df_train['author'].unique()\nprint (author_names)","execution_count":5},{"source":"For performance reason it is good to convert the target variable(author name) in **some sort of coding format** ","cell_type":"markdown","metadata":{"_cell_guid":"afc6df98-c926-4d01-b13a-b65dd0b46d0e","_uuid":"c8f583e89a5a103afd0c24e7b8998922c0d28b3c"}},{"outputs":[],"metadata":{"_cell_guid":"741abbf8-189a-45b9-b54d-d905bd29418d","_uuid":"110d06796aa8c411294d01f4cad656006c4c62ec"},"cell_type":"code","source":"\"\"\"\nMWS 2\nEAP 0\nHPL 1\n\"\"\" \nauthorname_to_id = {}\nassign_id = 0\nfor name in author_names:\n    authorname_to_id[name] = assign_id\n    assign_id += 1  ## Get a new id for new author\n    \n##  Print the dictionary created\nfor key, values in authorname_to_id.items():\n    print (key, values)","execution_count":6},{"outputs":[],"metadata":{"_cell_guid":"6fe6e541-ba72-4333-85d5-2a5743839cd8","_uuid":"d85960d4bc8a3075588db56e9bc7556faeaf19d5"},"cell_type":"code","source":"## convert the author name to id --> So when we predict the result humans can understand\n\"\"\"\n0 EAP\n1 HPL\n2 MWS\n\"\"\" \nid_to_author_name = {v: k for k, v in authorname_to_id.items()}\nfor key, values in id_to_author_name.items():\n    print (key, values)","execution_count":7},{"outputs":[],"metadata":{"_cell_guid":"c54e47b3-99a3-4ba6-8769-d7c3ab746334","_uuid":"ae0284a84c7abb3f6957ffe0b5cf842bf4738135","collapsed":true},"cell_type":"code","source":"## Add a new column to pandas dataframe, with the author name mapping\ndef get_author_id(author_name):\n    return authorname_to_id[author_name]\n\ndf_train['author_id'] = df_train['author'].map(get_author_id)","execution_count":8},{"outputs":[],"metadata":{"_cell_guid":"5d7e431d-ee32-45b1-a82c-29fcd5636ddc","_uuid":"88c6113db1f09a935424327b7c93bfe0982e8285"},"cell_type":"code","source":"df_train.head()","execution_count":9},{"source":"## 1. Preprocessing the Data","cell_type":"markdown","metadata":{"_cell_guid":"5dde30b9-5bd4-4971-aa5f-370c6d5710b2","_uuid":"fecce442fe2dc3ac996f93fe7a39c2ba34ebe71d"}},{"outputs":[],"metadata":{"_cell_guid":"12d6cb4f-728e-4c67-8b1c-43d65b1bb73d","_uuid":"f5b25b9237f6669fb9cdb5e8c1fb7cf02b0aeffa","collapsed":true},"cell_type":"code","source":"def transformText(text):\n    \n    stops = set(stopwords.words(\"english\"))\n    \n    # Convert text to lower\n    text = text.lower()\n    # Removing non ASCII chars    \n    text = re.sub(r'[^\\x00-\\x7f]',r' ',text)\n    \n    # Strip multiple whitespaces\n    text = gensim.corpora.textcorpus.strip_multiple_whitespaces(text)\n    \n    # Removing all the stopwords\n    filtered_words = [word for word in text.split() if word not in stops]\n    \n    # Removing all the tokens with lesser than 3 characters\n    filtered_words = gensim.corpora.textcorpus.remove_short(filtered_words, minsize=3)\n    \n    # Preprocessed text after stop words removal\n    text = \" \".join(filtered_words)\n    \n    # Remove the punctuation\n    text = gensim.parsing.preprocessing.strip_punctuation2(text)\n    \n    # Strip all the numerics\n    text = gensim.parsing.preprocessing.strip_numeric(text)\n    \n    # Strip multiple whitespaces\n    text = gensim.corpora.textcorpus.strip_multiple_whitespaces(text)\n    \n    # Stemming\n    return gensim.parsing.preprocessing.stem_text(text)","execution_count":10},{"outputs":[],"metadata":{"_cell_guid":"56297a46-d6de-40a7-9ea2-f6c987c78594","_uuid":"bb8fa68a7d30696f0467d2517c330043e78f9d86","collapsed":true},"cell_type":"code","source":"df_train['text'] = df_train['text'].map(transformText)","execution_count":11},{"outputs":[],"metadata":{"_cell_guid":"1f3921e2-6ce3-42cb-b303-14f3098affcb","_uuid":"86cda089fe81da4b0150bddb901454f588ff245e"},"cell_type":"code","source":"## Print a couple of rows after the preprocessing of the data is done\n\nprint (df_train['text'][0] , '\\n')\nprint (df_train['text'][1] , '\\n')\nprint (df_train['text'][2])","execution_count":12},{"source":"### We will be dividing our training data into **(Train, Test)**. So that we can evaluate the model","cell_type":"markdown","metadata":{"_cell_guid":"53b5d611-1754-40e1-a7e5-f0689420c814","_uuid":"b62b2b8c42a46094e3973828a5e01aae687e7698"}},{"outputs":[],"metadata":{"_cell_guid":"6c08ecc9-a5f6-4f44-a435-e3783185627d","_uuid":"c1f27b1f953869150f66fc11158c0b03a1cd5e63","collapsed":true},"cell_type":"code","source":"## Split the data \nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(df_train['text'], df_train['author_id'], \n                                                    test_size=0.33, random_state=42)","execution_count":13},{"outputs":[],"metadata":{"_cell_guid":"84694c85-9871-4ea1-8d45-65b3d815d215","_uuid":"5418839ceb96d24881279e4e9bf4466f6f802d1a"},"cell_type":"code","source":"print (\"Training Sample Size:\", len(X_train), ' ', \"Test Sample Size:\" ,len(X_test))","execution_count":14},{"source":"## 2. Building the Model","cell_type":"markdown","metadata":{"_cell_guid":"662d46ec-01d7-439c-8235-77dfebc8e810","_uuid":"b8d8f1b42718c05b0b57c6ad6631a33a34b80444"}},{"outputs":[],"metadata":{"_cell_guid":"b75d3944-d141-4552-9822-b4e0185ccf90","_uuid":"7f8713fa43f2579cdc55149849ec232bf3bf65cc"},"cell_type":"code","source":"## Get the word vocabulary out of the data\nfrom sklearn.feature_extraction.text import CountVectorizer\ncount_vect = CountVectorizer()\nX_train_counts = count_vect.fit_transform(X_train)\nX_train_counts.shape\n\n## Count of 'mistak' in corpus (mistake -> mistak after stemming)\nprint ('mistak appears:', count_vect.vocabulary_.get(u'mistak') , 'in the corpus')","execution_count":15},{"outputs":[],"metadata":{"_cell_guid":"b17dabb2-42bc-403a-af23-32ec7301693c","_uuid":"852b61e4f1fcd834dc02ab7eed54b8e0537a537f"},"cell_type":"code","source":"## Get the TF-IDF vector representation of the data\nfrom sklearn.feature_extraction.text import TfidfTransformer\ntfidf_transformer = TfidfTransformer()\nX_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\nprint ('Dimension of TF-IDF vector :' , X_train_tfidf.shape)","execution_count":16},{"source":"## 3. Training a classifier","cell_type":"markdown","metadata":{"_cell_guid":"01a8c859-362a-46f8-ae30-b193913dc83d","_uuid":"d56be71925c003ce43127ee425ce861255c7c8eb"}},{"outputs":[],"metadata":{"_cell_guid":"3c5cae4e-8a77-45df-83d4-0ec05aeb461f","_uuid":"df3cc66ac7bea646148caac88ba0c0b786bd46a9","collapsed":true},"cell_type":"code","source":"from sklearn.naive_bayes import MultinomialNB\nclf = MultinomialNB().fit(X_train_tfidf, y_train)","execution_count":17},{"source":"## 4. Prediction","cell_type":"markdown","metadata":{"_cell_guid":"25b06766-4d93-4b42-a3e8-b6dcf2894b68","_uuid":"1788457e6ddfcb6e7d1e11e984f5aba64e2778a6"}},{"outputs":[],"metadata":{"_cell_guid":"7d4086f0-55ec-4f30-b170-22a529e180c9","_uuid":"cc7d05c7ec10eee14e13de3c3b22be36f51441ec","collapsed":true},"cell_type":"code","source":"## Prediction part\n\nX_new_counts = count_vect.transform(X_test)\nX_new_tfidf = tfidf_transformer.transform(X_new_counts)\n\npredicted = clf.predict(X_new_tfidf)","execution_count":18},{"outputs":[],"metadata":{"_cell_guid":"292e151c-4cf9-4784-96ef-3ea6f91e5a8d","_uuid":"1a2fe39f7554cbcaecc3a7f5871bc1e14acd2157"},"cell_type":"code","source":"## predictions for first 10 test samples\n\ncounter  = 0\nfor doc, category in zip(X_test, predicted):\n    print('%r => %s' % (doc, id_to_author_name[category]))\n    if(counter == 10):\n        break\n    counter += 1    ","execution_count":19},{"source":"## 5. Evaluation \n\nWe will be doing simple evaluation scheme and will conclude  mean of the **correct predictions** as our accuracy","cell_type":"markdown","metadata":{"_cell_guid":"38420227-83e9-484e-bbae-329a8cde5bd1","_uuid":"24c7df3e9f0ea00c1f61556952a9e4dd69226491"}},{"outputs":[],"metadata":{"_cell_guid":"ea161d8c-5e6d-477a-8284-790649979340","_uuid":"fe6e0e9c320b4d56ce719f0166f863f41b8eff94"},"cell_type":"code","source":"np.mean(predicted == y_test) ## 80% sounds good only ","execution_count":20},{"source":"# Where to go from Here:\nHere are my couple of ideas to try:\n1. Use a better classifier (SVM)\n2. Go with word2vec representation of the data rather than TF-IDF\n\n**Note:** These are my possible ideas, feel free to comment and let me know the sections of the code which \n                    can be improved.","cell_type":"markdown","metadata":{"_cell_guid":"c1e92cd9-daab-477b-b478-d43490a65493","_uuid":"24cc73bea878dc8a94a8a46778c7a10ec316dd93"}}],"nbformat_minor":1,"nbformat":4,"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"version":3,"name":"ipython"},"nbconvert_exporter":"python","version":"3.6.3","file_extension":".py","pygments_lexer":"ipython3","mimetype":"text/x-python","name":"python"}}}