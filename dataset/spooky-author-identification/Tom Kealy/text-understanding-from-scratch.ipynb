{"metadata":{"kernelspec":{"display_name":"Python [default]","name":"python3","language":"python"},"language_info":{"file_extension":".py","name":"python","codemirror_mode":{"version":3,"name":"ipython"},"version":"3.5.4","mimetype":"text/x-python","nbconvert_exporter":"python","pygments_lexer":"ipython3"}},"nbformat":4,"nbformat_minor":1,"cells":[{"metadata":{},"source":"# Text Understanding from Scratch","cell_type":"markdown"},{"metadata":{},"source":"This note book is a small demonstration of using Convolutional Neural Networks to do a texct classification task. The basic idea is to one-hot convert the characters into a vector and then runa small CNN on the resulting vectors.","cell_type":"markdown"},{"execution_count":null,"metadata":{},"source":"import pandas as pd\nimport numpy as np\nimport os\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder, LabelBinarizer\nfrom sklearn.cross_validation import train_test_split\nfrom nltk import sent_tokenize, word_tokenize\nimport string\nfrom keras.models import Model\nfrom keras.optimizers import SGD\nfrom keras.layers import Input, Dense, Dropout, Flatten\nfrom keras.layers.convolutional import Convolution1D, MaxPooling1D\nimport matplotlib.pyplot as plt\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.utils import to_categorical\n%matplotlib inline\nprint(os.getcwd())","outputs":[],"cell_type":"code"},{"metadata":{},"source":"First we read the data, convert the text to a series of vectors, and then one-hot encode the targets","cell_type":"markdown"},{"execution_count":null,"metadata":{},"source":"texts = pd.read_csv( '../input/train.csv')","outputs":[],"cell_type":"code"},{"execution_count":null,"metadata":{"scrolled":true},"source":"texts.head()","outputs":[],"cell_type":"code"},{"metadata":{},"source":"","cell_type":"markdown"},{"execution_count":null,"metadata":{"collapsed":true},"source":"authors = texts['author']\ntexts = texts['text']","outputs":[],"cell_type":"code"},{"execution_count":null,"metadata":{"collapsed":true},"source":"def create_vocab_set():\n    #https://github.com/johnb30/py_crepe/\n    #This alphabet is 69 chars vs. 70 reported in the paper since they include two\n    # '-' characters. See https://github.com/zhangxiangxiao/Crepe#issues.\n\n    alphabet = (list(string.ascii_lowercase) + list(string.digits) +\n                list(string.punctuation) + ['\\n'])\n    vocab_size = len(alphabet)\n    check = set(alphabet)\n\n    vocab = {}\n    reverse_vocab = {}\n    for ix, t in enumerate(alphabet):\n        vocab[t] = ix\n        reverse_vocab[ix] = t\n\n    return vocab, reverse_vocab, vocab_size, check","outputs":[],"cell_type":"code"},{"execution_count":null,"metadata":{"collapsed":true},"source":"def encode_data(x, maxlen, vocab, vocab_size, check):\n    #https://github.com/johnb30/py_crepe/\n    #Iterate over the loaded data and create a matrix of size maxlen x vocabsize\n    #In this case that will be 1014x69. This is then placed in a 3D matrix of size\n    #data_samples x maxlen x vocab_size. Each character is encoded into a one-hot\n    #array. Chars not in the vocab are encoded into an all zero vector.\n\n    input_data = np.zeros((len(x), maxlen, vocab_size))\n    for dix, sent in enumerate(x):\n        counter = 0\n        sent_array = np.zeros((maxlen, vocab_size))\n        chars = list(sent.lower().replace(' ', ''))\n        for c in chars:\n            if counter >= maxlen:\n                pass\n            else:\n                char_array = np.zeros(vocab_size, dtype=np.int)\n                if c in check:\n                    ix = vocab[c]\n                    char_array[ix] = 1\n                sent_array[counter, :] = char_array\n                counter += 1\n        input_data[dix, :, :] = sent_array\n\n    return input_data","outputs":[],"cell_type":"code"},{"execution_count":null,"metadata":{"collapsed":true},"source":"maxlen = 140\nvocab, reverse_vocab, vocab_size, check = create_vocab_set()\nencoded = encode_data(texts, maxlen, vocab, vocab_size, check)","outputs":[],"cell_type":"code"},{"execution_count":null,"metadata":{"collapsed":true},"source":"lb = LabelBinarizer()\nlb.fit(authors)\ntargets = lb.transform(authors)","outputs":[],"cell_type":"code"},{"execution_count":null,"metadata":{},"source":"encoded.shape","outputs":[],"cell_type":"code"},{"execution_count":null,"metadata":{},"source":"targets.shape","outputs":[],"cell_type":"code"},{"execution_count":null,"metadata":{},"source":"print(encoded[0])\nprint(targets[0])","outputs":[],"cell_type":"code"},{"execution_count":null,"metadata":{"collapsed":true},"source":"X_train, X_test, y_train, y_test = train_test_split(encoded, targets, test_size=0.2, random_state=1234) ","outputs":[],"cell_type":"code"},{"metadata":{},"source":"Now we build the CNN model to do our training! This CNN has 7 conv layers (some followed by  maxpooling) and then 2 fully connected layers to finish off. ","cell_type":"markdown"},{"execution_count":null,"metadata":{"collapsed":true},"source":"nb_filter = 256\ndense_outputs = 1024\ncat_output = 3\nbatch_size = 80\nnb_epoch = 10","outputs":[],"cell_type":"code"},{"execution_count":null,"metadata":{},"source":"inputs = Input(shape=(maxlen, vocab_size), name='input', dtype='float32')\nconv0 = Convolution1D(nb_filter=nb_filter, filter_length=7, border_mode='valid', activation='relu', input_shape=(maxlen, vocab_size))(inputs)\nconv0 = MaxPooling1D(pool_length=2)(conv0)\n\nconv1 = Convolution1D(nb_filter=nb_filter, filter_length=7, border_mode='valid', activation='relu', input_shape=(maxlen, vocab_size))(conv0)\nconv1 = MaxPooling1D(pool_length=2)(conv1)\n\nconv2 = Convolution1D(nb_filter=nb_filter, filter_length=4, border_mode='valid', activation='relu', input_shape=(maxlen, vocab_size))(conv1)\n\nconv3 = Convolution1D(nb_filter=nb_filter, filter_length=4, border_mode='valid', activation='relu', input_shape=(maxlen, vocab_size))(conv2)\n\nconv4 = Convolution1D(nb_filter=nb_filter, filter_length=4, border_mode='valid', activation='relu', input_shape=(maxlen, vocab_size))(conv3)\n\nconv5 = Convolution1D(nb_filter=nb_filter, filter_length=4, border_mode='valid', activation='relu', input_shape=(maxlen, vocab_size))(conv4)\n\nconv6 = Convolution1D(nb_filter=nb_filter, filter_length=4, border_mode='valid', activation='relu', input_shape=(maxlen, vocab_size))(conv5)\nconv6 = MaxPooling1D(pool_length=2)(conv6)\nconv6 = Flatten()(conv5)\n\ndense0 = Dropout(0.5)(Dense(dense_outputs, activation='relu')(conv6))\ndense1 = Dropout(0.5)(Dense(dense_outputs, activation='relu')(dense0))\n\npred = Dense(cat_output, activation='softmax', name='output')(dense1)\n\nmodel = Model(input=inputs, output=pred)\n\nsgd = SGD(lr=0.01, momentum=0.9)\nmodel.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])","outputs":[],"cell_type":"code"},{"metadata":{},"source":"We'll only train this model for 5 epochs, but you can train it for far longer if you wish!","cell_type":"markdown"},{"execution_count":null,"metadata":{},"source":"model.fit(x=X_train, y=y_train, epochs=5)","outputs":[],"cell_type":"code"},{"execution_count":null,"metadata":{"collapsed":true},"source":"y_pred = model.predict(X_test)","outputs":[],"cell_type":"code"},{"execution_count":null,"metadata":{},"source":"y_pred","outputs":[],"cell_type":"code"},{"metadata":{},"source":"This evaluation won't be that great - the network needs to be trained for much longer (if yo have a mchine with a CUDA enabled gpu, the training will be significantly faster.)","cell_type":"markdown"},{"execution_count":null,"metadata":{},"source":"model.evaluate(X_test, y_test)","outputs":[],"cell_type":"code"},{"execution_count":null,"metadata":{},"source":"a2c = {'EAP': 0, 'HPL' : 1, 'MWS' : 2}\nresult = pd.read_csv('../input/sample_submission.csv')\nfor a, i in a2c.items():\n    result[a] = y_pred[:, i]\n#to_submit=result","outputs":[],"cell_type":"code"},{"execution_count":null,"metadata":{"collapsed":true},"source":"","outputs":[],"cell_type":"code"}]}