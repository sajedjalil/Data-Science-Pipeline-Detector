{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd \n\n# Plotly imports\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.tools as tls\nimport spacy\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Other imports\nfrom collections import Counter\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.decomposition import NMF, LatentDirichletAllocation\nfrom matplotlib import pyplot as plt\n%matplotlib inline\n\nfrom wordcloud import WordCloud, STOPWORDS","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"all credits to https://www.kaggle.com/arthurtok/spooky-nlp-and-topic-modelling-tutorial\n"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/spooky-author-identification/train.zip')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are 3 authors - EAP(edgar alan poe), HPL(HP Lovecraft), MWS(Mary Shelley)"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"n = {'EAP': 'Edgar Alan Poe', 'HPL':'HP Lovecraft', 'MWS':'Mary SHelley'}\n\ndata = [go.Bar(\n            x = train.author.map(n).unique(),\n            y = train.author.value_counts().values,\n            marker = dict(colorscale = 'darkmint',\n                         color = train.author.value_counts().values\n                        ),\n            text ='Texts per author'\n    )]\n\nlayout = go.Layout(\n    title = 'Distribution of target variable'\n)\n\nfig = go.Figure(data = data, layout = layout)\n\npy.iplot(fig, filename='basic-bar')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"words = train['text'].str.split(expand = True).unstack().value_counts()\n\ndata = [go.Bar(\n            x = words.index.values[2:80],\n            y = words.values[2:80],\n            marker = dict(colorscale = 'darkmint',\n                         color = words.values[2:160]\n                         ),\n            text = 'Word counts'\n    )]\n\nlayout = go.Layout(\n    title='Top 80 Words'\n)\n\nfig = go.Figure(data=data, layout = layout)\n\npy.iplot(fig, filename='basic-bar')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"WordClouds"},{"metadata":{"trusted":true},"cell_type":"code","source":"eap = train[train.author=='EAP']['text'].values\nhpl = train[train['author']=='HPL']['text'].values\nmws = train[train['author']=='MWS']['text'].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (16, 13))\nwc = WordCloud(background_color = 'black', max_words = 10000,\n              stopwords=STOPWORDS, max_font_size=40)\nwc.generate(' '.join(hpl))\nplt.title('HP Lovecraft word cloud', fontsize=20)\nplt.imshow(wc.recolor( colormap= 'Pastel2' , random_state=17), alpha=0.98)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (16, 13))\nwc = WordCloud(background_color = 'black', max_words = 10000,\n              stopwords=STOPWORDS, max_font_size=40)\nwc.generate(' '.join(eap))\nplt.title('E A Poe word cloud')\nplt.imshow(wc.recolor( colormap= 'Pastel2' , random_state=17), alpha=0.98)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (16, 13))\nwc = WordCloud(background_color = 'black', max_words = 10000,\n              stopwords=STOPWORDS, max_font_size=40)\nwc.generate(' '.join(mws))\nplt.title('M Sherry word cloud')\nplt.imshow(wc.recolor( colormap= 'Pastel2' , random_state=17), alpha=0.98)\nplt.axis('off')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**PREPROCESSING**\n* Tokenization\n* Stopwords\n* Stemming\n* Vectorization"},{"metadata":{"trusted":true},"cell_type":"code","source":"import nltk","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"TOKENIZATION"},{"metadata":{"trusted":true},"cell_type":"code","source":"first_text = train.text.values[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"first_text_token = nltk.word_tokenize(first_text)\nprint(first_text_token)\nprint('\\nNumber of words: {}'.format(len(first_text_token)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nlp = spacy.load('en_core_web_sm')\ndoc = nlp('My new phone is made by Apple. Apple is company from San Francisco')\nspacy.displacy.render(doc, style = 'ent')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"STOPWORDS REMOVAL"},{"metadata":{"trusted":true},"cell_type":"code","source":"stopwords = nltk.corpus.stopwords.words('english')\nlen(stopwords)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"first_text_clean = [word for word in first_text_token if word.lower() not in stopwords]\nprint(first_text_clean)\nprint('\\nNumber of words: {}'.format(len(first_text_clean)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"STEMMING AND LEMMATIZATION\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"stemmer = nltk.stem.PorterStemmer()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(stemmer.stem('working'))\nprint(stemmer.stem('leaves'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.stem import WordNetLemmatizer","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lemmatizer = WordNetLemmatizer()\nprint(lemmatizer.lemmatize('leaves'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Vectorazing Raw Text**"},{"metadata":{},"cell_type":"markdown","source":"The Bag of Words"},{"metadata":{"trusted":true},"cell_type":"code","source":"text_test = ['I like trading stocks', 'I like reading books']\nvectorizer = CountVectorizer(min_df=0)\ntext_transform = vectorizer.fit_transform(text_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vectorizer.get_feature_names()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"text_transform.toarray()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"text_transform","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**TOPIC modelling**\n* Latent Dirichlet Allocation \n* Non-negative Matrix Factorization"},{"metadata":{"trusted":true},"cell_type":"code","source":"def print_top_words(model, feature_names, n_top_words):\n    for index, topic in enumerate(model.components_):\n        message = \"\\nTopic #{}:\".format(index)\n        message += \" \".join([feature_names[i] for i in topic.argsort()[:-n_top_words - 1 :-1]])\n        print(message)\n        print(\"=\"*70)                    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lemm = WordNetLemmatizer()\nclass LemmaCountVectorizer(CountVectorizer):\n    def build_analyzer(self):\n        analyzer = super(LemmaCountVectorizer, self).build_analyzer()\n        return lambda doc: (lemm.lemmatize(w) for w in analyzer(doc))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Storing the entire training text in a list\ntext = list(train.text.values)\n# Calling our overwritten Count vectorizer\ntf_vectorizer = LemmaCountVectorizer(max_df=0.95, \n                                     min_df=2,\n                                     stop_words='english',\n                                     decode_error='ignore')\ntf = tf_vectorizer.fit_transform(text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_names = tf_vectorizer.get_feature_names()\ncount_vec = np.asarray(tf.sum(axis=0)).ravel()\nzipped = list(zip(feature_names, count_vec))\nx, y = (list(x) for x in zip(*sorted(zipped, key=lambda x: x[1], reverse=True)))\n# Now I want to extract out on the top 15 and bottom 15 words\nY = np.concatenate([y[0:15], y[-16:-1]])\nX = np.concatenate([x[0:15], x[-16:-1]])\n\n# Plotting the Plot.ly plot for the Top 50 word frequencies\ndata = [go.Bar(\n            x = x[0:50],\n            y = y[0:50],\n            marker= dict(colorscale='Jet',\n                         color = y[0:50]\n                        ),\n            text='Word counts'\n    )]\n\nlayout = go.Layout(\n    title='Top 50 Word frequencies after Preprocessing'\n)\n\nfig = go.Figure(data=data, layout=layout)\n\npy.iplot(fig, filename='basic-bar')\n\n# Plotting the Plot.ly plot for the Top 50 word frequencies\ndata = [go.Bar(\n            x = x[-100:],\n            y = y[-100:],\n            marker= dict(colorscale='Portland',\n                         color = y[-100:]\n                        ),\n            text='Word counts'\n    )]\n\nlayout = go.Layout(\n    title='Bottom 100 Word frequencies after Preprocessing'\n)\n\nfig = go.Figure(data=data, layout=layout)\n\npy.iplot(fig, filename='basic-bar')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lda = LatentDirichletAllocation(n_components=11, max_iter=5,\n                                learning_method = 'online',\n                                learning_offset = 50.,\n                                random_state = 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lda.fit(tf)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"n_top_words = 40\nprint(\"\\nTopics in LDA model: \")\ntf_feature_names = tf_vectorizer.get_feature_names()\nprint_top_words(lda, tf_feature_names, n_top_words)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"first_topic = lda.components_[0]\nsecond_topic = lda.components_[1]\nthird_topic = lda.components_[2]\nfourth_topic = lda.components_[3]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Word Cloud visualizations of the topicsÂ¶\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"first_topic_words = [tf_feature_names[i] for i in first_topic.argsort()[:-50 - 1 :-1]]\nsecond_topic_words = [tf_feature_names[i] for i in second_topic.argsort()[:-50 - 1 :-1]]\nthird_topic_words = [tf_feature_names[i] for i in third_topic.argsort()[:-50 - 1 :-1]]\nfourth_topic_words = [tf_feature_names[i] for i in fourth_topic.argsort()[:-50 - 1 :-1]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"firstcloud = WordCloud(\n                          stopwords=STOPWORDS,\n                          background_color='black',\n                          width=2500,\n                          height=1800\n                         ).generate(\" \".join(first_topic_words))\nplt.imshow(firstcloud)\nplt.axis('off')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cloud = WordCloud(\n                          stopwords=STOPWORDS,\n                          background_color='black',\n                          width=2500,\n                          height=1800\n                         ).generate(\" \".join(second_topic_words))\nplt.imshow(cloud)\nplt.axis('off')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cloud = WordCloud(\n                          stopwords=STOPWORDS,\n                          background_color='black',\n                          width=2500,\n                          height=1800\n                         ).generate(\" \".join(third_topic_words))\nplt.imshow(cloud)\nplt.axis('off')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cloud = WordCloud(\n                          stopwords=STOPWORDS,\n                          background_color='black',\n                          width=2500,\n                          height=1800\n                         ).generate(\" \".join(fourth_topic_words))\nplt.imshow(cloud)\nplt.axis('off')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"extending the countvectorizer class with a lemmatizer"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"raw","source":""},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}