{"metadata":{"language_info":{"mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","file_extension":".py","version":"3.6.3"},"kernelspec":{"display_name":"Python 3","name":"python3","language":"python"}},"nbformat":4,"nbformat_minor":1,"cells":[{"cell_type":"code","execution_count":null,"outputs":[],"source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np\nimport os\nimport pandas as pd\nimport sys\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.feature_extraction.text import TfidfVectorizer,HashingVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.svm import LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.linear_model import LogisticRegression,SGDClassifier\nfrom nltk.corpus import wordnet as wn\nfrom nltk.corpus import stopwords\nfrom nltk.stem.snowball import SnowballStemmer\nfrom nltk.stem import PorterStemmer\nimport nltk\nfrom nltk import word_tokenize, ngrams\nfrom nltk.classify import SklearnClassifier\nfrom wordcloud import WordCloud,STOPWORDS\nimport xgboost as xgb\nnp.random.seed(25)\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\ntrain = pd.read_csv(\"../input/train.csv\")\ntest = pd.read_csv(\"../input/test.csv\")\n# Any results you write to the current directory are saved as output.","metadata":{"_uuid":"2ed1ca776494fcf1a1a3b3e15dc45a0028084612","_cell_guid":"2cb8c4af-1f0f-45c5-9835-bcaa0884fc60"}},{"cell_type":"code","execution_count":null,"outputs":[],"source":"train.head()","metadata":{"_uuid":"e748224a9020db4fb4937dbff66cc52af8b9b960","_cell_guid":"0fe92c4c-4dcc-4a05-af1e-0c7da467736d"}},{"cell_type":"code","execution_count":null,"outputs":[],"source":"# Target Mapping\nmapping_target = {'EAP':0, 'HPL':1, 'MWS':2}\ntrain = train.replace({'author':mapping_target})","metadata":{"_uuid":"5fd18f20b2a2132e51005616c059b2fbe83611d0","collapsed":true,"_cell_guid":"6de5dde1-0376-41f2-a77e-f5a06db6390e"}},{"cell_type":"code","execution_count":null,"outputs":[],"source":"train.head()","metadata":{"_uuid":"e9ea83988b68cedb52ac3a0cbeb5a583c27db8a8","_cell_guid":"f1656e9e-578a-4fd2-93c7-6bd88d5f9f5e"}},{"cell_type":"code","execution_count":null,"outputs":[],"source":"test_id = test['id']\ntarget = train['author']","metadata":{"_uuid":"b16cfbca25e8311963c7ef2496701678720c9fb1","collapsed":true,"_cell_guid":"597d190a-8c7e-4beb-a909-0e19fc9ccab8"}},{"cell_type":"code","execution_count":null,"outputs":[],"source":"# function to clean data\nimport string\nimport itertools \nimport re\nfrom nltk.stem import WordNetLemmatizer\nfrom string import punctuation\n\ndef preprocess(text):\n    text = text.strip()\n    text = text.replace(\"' \", \" ' \")\n    signs = set(',.:;\"?!')\n    prods = set(text) & signs\n    if not prods:\n        return text\n\n    for sign in prods:\n        text = text.replace(sign, ' {} '.format(sign) )\n    return text\n\ndef cleanData(text, lowercase = False, remove_stops = False, stemming = False, lemmatization = False):\n    \n    txt = str(text)\n    \n    txt = re.sub(r'[^A-Za-z\\s]',r' ',txt)\n    \n     \n    if lowercase:\n        txt = \" \".join([w.lower() for w in txt.split()])\n        \n    if remove_stops:\n        txt = \" \".join([w for w in txt.split() if w not in stops])\n    if stemming:\n        st = PorterStemmer()\n        txt = \" \".join([st.stem(w) for w in txt.split()])\n    \n    if lemmatization:\n        wordnet_lemmatizer = WordNetLemmatizer()\n        txt = \" \".join([wordnet_lemmatizer.lemmatize(w, pos='v') for w in txt.split()])\n\n    return txt","metadata":{"_uuid":"8c987eff81376f41ffdb8e4e4e53e208e8663460","collapsed":true,"_cell_guid":"9b40c219-8b32-4a2d-8c38-1bc4691c71ab"}},{"cell_type":"code","execution_count":null,"outputs":[],"source":"train['text'] = train['text'].map(lambda x: preprocess(x))\ntest['text'] = test['text'].map(lambda x: preprocess(x))","metadata":{"_uuid":"0196ba72a8cf5a3e2d95b8252143b5e20b57ce6f","collapsed":true,"_cell_guid":"628ddc08-8e61-407f-8c73-617e1a39e5e2"}},{"cell_type":"code","execution_count":null,"outputs":[],"source":"# clean text\ntrain['text'] = train['text'].map(lambda x: cleanData(x, lowercase=True, remove_stops=False, stemming=False, lemmatization = False))\ntest['text'] = test['text'].map(lambda x: cleanData(x, lowercase=True, remove_stops=False, stemming=False, lemmatization = False))","metadata":{"_uuid":"d4b0e8b202f9954a52082e1011907f8f12b3eb9b","collapsed":true,"_cell_guid":"161467ed-5748-4b0b-b2f8-aeba6221e283"}},{"cell_type":"markdown","source":"# Using Neural Network","metadata":{"_uuid":"23d266451f858f76cf82d070c2914961e75229ac","collapsed":true,"_cell_guid":"e4d0ceef-3a1f-40f4-ba47-857c3f5c81e5"}},{"cell_type":"code","execution_count":null,"outputs":[],"source":"np.random.seed(25)\nfrom keras.models import Sequential\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.utils.np_utils import to_categorical\nfrom keras.layers import Dense, Input, Flatten, merge, LSTM, Lambda, Dropout\nfrom keras.layers import Conv1D, MaxPooling1D, Embedding\nfrom keras.models import Model\nfrom keras.layers.wrappers import TimeDistributed, Bidirectional\nfrom keras.layers.normalization import BatchNormalization\nfrom keras import backend as K\nfrom keras.layers import Convolution1D, GlobalMaxPooling1D, GlobalAveragePooling1D\nfrom keras.layers import GlobalMaxPooling1D, Conv1D, MaxPooling1D, Flatten, Bidirectional, SpatialDropout1D\nfrom keras.layers.merge import concatenate\nfrom keras.layers.core import Dense, Activation, Dropout\nimport codecs","metadata":{"_uuid":"6811b5188a429fca473f1006bc5e06e384583777","_cell_guid":"5168bc65-7ee1-49c9-a4f4-1d50032e337b"}},{"cell_type":"code","execution_count":null,"outputs":[],"source":"MAX_SEQUENCE_LENGTH = 256\nMAX_NB_WORDS = 200000","metadata":{"_uuid":"f04bb2ae8618b5b4d5ad7cd7ee12bc1f640cf962","collapsed":true,"_cell_guid":"61aea7ce-e90c-4d07-b2ac-24ed5cc8e0dc"}},{"cell_type":"code","execution_count":null,"outputs":[],"source":"def add_ngram(q, n_gram_max):\n            ngrams = []\n            for n in range(2, n_gram_max+1):\n                for w_index in range(len(q)-n+1):\n                    ngrams.append('--'.join(q[w_index:w_index+n]))\n            return q + ngrams","metadata":{"_uuid":"7dc2c4bbd4edb1c210389c70fd9ec84cd133d22b","collapsed":true,"_cell_guid":"3defc665-523f-44e7-80ea-a7cdfeeee706"}},{"cell_type":"code","execution_count":null,"outputs":[],"source":"n_gram_max = 2\nprint('Processing text dataset')\ntexts_1 = []\nfor text in train['text']:\n    text = text.split()\n    texts_1.append(' '.join(add_ngram(text, n_gram_max)))\n    \n\nlabels = train['author']  # list of label ids\n\nprint('Found %s texts.' % len(texts_1))\ntest_texts_1 = []\nfor text in test['text']:\n    text = text.split()\n    test_texts_1.append(' '.join(add_ngram(text, n_gram_max)))\nprint('Found %s texts.' % len(test_texts_1))","metadata":{"_uuid":"256cb736ba230aaa9a81d8eb0c9388204ab7b4f3","_cell_guid":"1877b98d-c7e1-4d1e-8f02-5685154ab261"}},{"cell_type":"code","execution_count":null,"outputs":[],"source":"min_count = 2\ntokenizer = Tokenizer(lower=False, filters='')\ntokenizer.fit_on_texts(texts_1)\nnum_words = sum([1 for _, v in tokenizer.word_counts.items() if v >= min_count])\n\ntokenizer = Tokenizer(num_words=num_words, lower=True, filters='')\ntokenizer.fit_on_texts(texts_1)\n\n\nsequences_1 = tokenizer.texts_to_sequences(texts_1)\n# word_index = tokenizer.word_index\n# print('Found %s unique tokens.' % len(word_index))\n\ntest_sequences_1 = tokenizer.texts_to_sequences(test_texts_1)\n\ndata_1 = pad_sequences(sequences_1, maxlen=MAX_SEQUENCE_LENGTH)\nlabels = np.array(labels)\nprint('Shape of data tensor:', data_1.shape)\nprint('Shape of label tensor:', labels.shape)\n\ntest_data_1 = pad_sequences(test_sequences_1, maxlen=MAX_SEQUENCE_LENGTH)\n#test_labels = np.array(test_labels)\ndel test_sequences_1\ndel sequences_1\nimport gc\ngc.collect()","metadata":{"_uuid":"9e5a87606694be4317c8dfabea0367bc30f40626","_cell_guid":"97dfbeef-98e4-44c6-a3f5-4b4944b2fd17"}},{"cell_type":"code","execution_count":null,"outputs":[],"source":"nb_words = np.max(data_1) + 1 #min(MAX_NB_WORDS, len(word_index)) + 1","metadata":{"_uuid":"1994e200be8a3f3c3aa2e3f5fda60aaf8f40ccef","collapsed":true,"_cell_guid":"6a36b388-fc58-4f5e-be9b-5d0aed587254"}},{"cell_type":"code","execution_count":null,"outputs":[],"source":"nb_words","metadata":{"_uuid":"5649820ad79c83da2301ab97d426b9c734f99769","_cell_guid":"096dcf66-890a-499c-b4b8-fb67ab61160e"}},{"cell_type":"code","execution_count":null,"outputs":[],"source":"from keras.layers.recurrent import LSTM, GRU\nmodel = Sequential()\nmodel.add(Embedding(nb_words,20,input_length=MAX_SEQUENCE_LENGTH))\n# model.add(Flatten())\n# model.add(Dense(100, activation='relu'))\n# model.add(Dropout(0.3))\n# model.add(Conv1D(64,\n#                  5,\n#                  padding='valid',\n#                  activation='relu'))\n# model.add(Dropout(0.3))\nmodel.add(GlobalAveragePooling1D())\n# model.add(Flatten())\n# model.add(Dense(100, activation='relu'))\n# model.add(Dropout(0.5))\nmodel.add(Dense(3, activation='softmax'))\n\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics = ['accuracy'])","metadata":{"_uuid":"edab633734e3f18ee93738d90d3f50538caba730","collapsed":true,"_cell_guid":"5fc2074d-c97d-42c7-9520-287a82a46dbb"}},{"cell_type":"code","execution_count":null,"outputs":[],"source":"model.fit(data_1, to_categorical(labels), validation_split=0.2, nb_epoch=15, batch_size=16)","metadata":{"_uuid":"a18313a64627ac15ffcd8491028a65f2abd072f4","_cell_guid":"eb081f32-9b55-40ed-ae98-ecb3bdd6fa0c"}},{"cell_type":"code","execution_count":null,"outputs":[],"source":"preds = model.predict(test_data_1)","metadata":{"_uuid":"656bf99e26d40ae97d32b4b78841bd9709369a90","collapsed":true,"_cell_guid":"8fb234fc-3415-4f6d-8a31-a7a2523ff739"}},{"cell_type":"code","execution_count":null,"outputs":[],"source":"result = pd.DataFrame()\nresult['id'] = test_id\nresult['EAP'] = [x[0] for x in preds]\nresult['HPL'] = [x[1] for x in preds]\nresult['MWS'] = [x[2] for x in preds]\n\nresult.to_csv(\"result.csv\", index=False)","metadata":{"_uuid":"dbc9c5a25da1e427b3e0a863b686a5c0a4305ef1","collapsed":true,"_cell_guid":"179acab2-3204-4b38-81e8-b6d4d325d351"}},{"cell_type":"code","execution_count":null,"outputs":[],"source":"result.head()","metadata":{"_uuid":"69d807a1bab626c5f7da9a438706dc374c41a902","_cell_guid":"7ec1d3f0-52da-4bae-a5d1-7315fa2984f5"}},{"cell_type":"code","execution_count":null,"outputs":[],"source":"","metadata":{"_uuid":"bc1a7b899fb8df036071d66aa8e3cf1fdab0b096","collapsed":true,"_cell_guid":"4547b418-1226-4255-8c4d-0383cce52123"}}]}