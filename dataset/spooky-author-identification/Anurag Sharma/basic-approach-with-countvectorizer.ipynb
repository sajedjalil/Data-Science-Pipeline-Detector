{"nbformat_minor":1,"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"version":"3.6.3","pygments_lexer":"ipython3","nbconvert_exporter":"python","mimetype":"text/x-python","codemirror_mode":{"version":3,"name":"ipython"},"name":"python","file_extension":".py"}},"cells":[{"outputs":[],"execution_count":null,"source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np\nimport os\nimport pandas as pd\nimport sys\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.feature_extraction.text import TfidfVectorizer,HashingVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.svm import LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.linear_model import LogisticRegression,SGDClassifier\nfrom nltk.corpus import wordnet as wn\nfrom nltk.corpus import stopwords\nfrom nltk.stem.snowball import SnowballStemmer\nfrom nltk.stem import PorterStemmer\nimport nltk\nfrom nltk import word_tokenize, ngrams\nfrom nltk.classify import SklearnClassifier\nfrom wordcloud import WordCloud,STOPWORDS\nimport xgboost as xgb\nnp.random.seed(25)\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\ntrain = pd.read_csv(\"../input/train.csv\")\ntest = pd.read_csv(\"../input/test.csv\")\n# Any results you write to the current directory are saved as output.","metadata":{"_uuid":"f57eae421649a1a54e3d77dcdcbd8451bc3bf05f","_cell_guid":"5c58e60b-2f86-4972-b5fa-1470c975bbab"},"cell_type":"code"},{"outputs":[],"execution_count":null,"source":"train.head()","metadata":{"_uuid":"264dee8d28a7a7aa48bccb48e00c8e1bcdfc434a","_cell_guid":"40d8f19c-ffb9-4a83-b26d-85a0b99f1470"},"cell_type":"code"},{"outputs":[],"execution_count":null,"source":"# Target Mapping\nmapping_target = {'EAP':0, 'HPL':1, 'MWS':2}\ntrain = train.replace({'author':mapping_target})","metadata":{"_uuid":"018fdcaf3f40a28610d7d6f5b86ea1039ad758fd","collapsed":true,"_cell_guid":"5bc97bf7-83c2-48d9-afc2-233f67c1b381"},"cell_type":"code"},{"outputs":[],"execution_count":null,"source":"train.head()","metadata":{"_uuid":"45cb41466f9fdb88e0955e0ee8270c28a7ddf694","_cell_guid":"a17bf366-ac47-49af-bef9-ef814c3fd87f"},"cell_type":"code"},{"outputs":[],"execution_count":null,"source":"test_id = test['id']\ntarget = train['author']","metadata":{"_uuid":"2233c58f025a9d8bc241da4896dd9429db03db7a","collapsed":true,"_cell_guid":"fad2ebb8-4c7b-4c4e-97c6-ca5c3ae68885"},"cell_type":"code"},{"outputs":[],"execution_count":null,"source":"# function to clean data\nimport string\nimport itertools \nimport re\nfrom nltk.stem import WordNetLemmatizer\nfrom string import punctuation\n\nstops = ['the','a','an','and','but','if','or','because','as','what','which','this','that','these','those','then',\n              'just','so','than','such','both','through','about','for','is','of','while','during','to','What','Which',\n              'Is','If','While','This']\n# punct = list(string.punctuation)\n# punct.append(\"''\")\n# punct.append(\":\")\n# punct.append(\"...\")\n# punct.append(\"@\")\n# punct.append('\"\"')\ndef cleanData(text, lowercase = False, remove_stops = False, stemming = False, lemmatization = False):\n    \n    txt = str(text)\n    \n    txt = re.sub(r'[^A-Za-z\\s]',r' ',txt)\n    \n     \n    if lowercase:\n        txt = \" \".join([w.lower() for w in txt.split()])\n        \n    if remove_stops:\n        txt = \" \".join([w for w in txt.split() if w not in stops])\n    if stemming:\n        st = PorterStemmer()\n        txt = \" \".join([st.stem(w) for w in txt.split()])\n    \n    if lemmatization:\n        wordnet_lemmatizer = WordNetLemmatizer()\n        txt = \" \".join([wordnet_lemmatizer.lemmatize(w, pos='v') for w in txt.split()])\n\n    return txt","metadata":{"_uuid":"c7710ed8a476f2e705bc8ebf3da20edc61bc419a","collapsed":true,"_cell_guid":"c099a9ef-555e-4eec-8e0c-a1b2efe9766f"},"cell_type":"code"},{"outputs":[],"execution_count":null,"source":"# def fraction_noun(row):\n#     \"\"\"function to give us fraction of noun over total words \"\"\"\n#     text = row['text']\n#     text_splited = text.split(' ')\n#     text_splited = [''.join(c for c in s if c not in string.punctuation) for s in text_splited]\n#     text_splited = [s for s in text_splited if s]\n#     word_count = text_splited.__len__()\n#     pos_list = nltk.pos_tag(text_splited)\n#     noun_count = len([w for w in pos_list if w[1] in ('NN','NNP','NNPS','NNS')])\n#     return (noun_count/word_count)\n\n# def fraction_adj(row):\n#     \"\"\"function to give us fraction of adjectives over total words in given text\"\"\"\n#     text = row['text']\n#     text_splited = text.split(' ')\n#     text_splited = [''.join(c for c in s if c not in string.punctuation) for s in text_splited]\n#     text_splited = [s for s in text_splited if s]\n#     word_count = text_splited.__len__()\n#     pos_list = nltk.pos_tag(text_splited)\n#     adj_count = len([w for w in pos_list if w[1] in ('JJ','JJR','JJS')])\n#     return (adj_count/word_count)\n\n# def fraction_verbs(row):\n#     \"\"\"function to give us fraction of verbs over total words in given text\"\"\"\n#     text = row['text']\n#     text_splited = text.split(' ')\n#     text_splited = [''.join(c for c in s if c not in string.punctuation) for s in text_splited]\n#     text_splited = [s for s in text_splited if s]\n#     word_count = text_splited.__len__()\n#     pos_list = nltk.pos_tag(text_splited)\n#     verbs_count = len([w for w in pos_list if w[1] in ('VB','VBD','VBG','VBN','VBP','VBZ')])\n#     return (verbs_count/word_count)","metadata":{"_uuid":"c9c0fa3e866493a19683f73182e844a4b46ad5f6","collapsed":true,"_cell_guid":"ad2ca37d-d052-4679-b938-c312bb06c5ca"},"cell_type":"code"},{"outputs":[],"execution_count":null,"source":"# train['fraction_noun'] = train.apply(lambda row: fraction_noun(row), axis =1)\n# train['fraction_adj'] = train.apply(lambda row: fraction_adj(row), axis =1)\n# train['fraction_verbs'] = train.apply(lambda row: fraction_verbs(row), axis =1)\n\n# test['fraction_noun'] = test.apply(lambda row: fraction_noun(row), axis =1)\n# test['fraction_adj'] = test.apply(lambda row: fraction_adj(row), axis =1)\n# test['fraction_verbs'] = test.apply(lambda row: fraction_verbs(row), axis =1)","metadata":{"_uuid":"180aaac8707e139d2d4e4a353c2bdb57fff94f1b","collapsed":true,"_cell_guid":"0c5176fc-42b9-4c5a-9e86-241c492a6343"},"cell_type":"code"},{"outputs":[],"execution_count":null,"source":"## Number of words in the text ##\ntrain[\"num_words\"] = train[\"text\"].apply(lambda x: len(str(x).split()))\ntest[\"num_words\"] = test[\"text\"].apply(lambda x: len(str(x).split()))\n\n## Number of unique words in the text ##\ntrain[\"num_unique_words\"] = train[\"text\"].apply(lambda x: len(set(str(x).split())))\ntest[\"num_unique_words\"] = test[\"text\"].apply(lambda x: len(set(str(x).split())))\n\n## Number of characters in the text ##\ntrain[\"num_chars\"] = train[\"text\"].apply(lambda x: len(str(x)))\ntest[\"num_chars\"] = test[\"text\"].apply(lambda x: len(str(x)))\n\n## Number of stopwords in the text ##\ntrain[\"num_stopwords\"] = train[\"text\"].apply(lambda x: len([w for w in str(x).lower().split() if w in stops]))\ntest[\"num_stopwords\"] = test[\"text\"].apply(lambda x: len([w for w in str(x).lower().split() if w in stops]))\n\n## Number of punctuations in the text ##\ntrain[\"num_punctuations\"] =train['text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]) )\ntest[\"num_punctuations\"] =test['text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]) )\n\n## Number of title case words in the text ##\ntrain[\"num_words_upper\"] = train[\"text\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\ntest[\"num_words_upper\"] = test[\"text\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\n\n## Number of title case words in the text ##\ntrain[\"num_words_title\"] = train[\"text\"].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\ntest[\"num_words_title\"] = test[\"text\"].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\n\n## Average length of the words in the text ##\ntrain[\"mean_word_len\"] = train[\"text\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\ntest[\"mean_word_len\"] = test[\"text\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))","metadata":{"_uuid":"2b2c032cc12f119ec9bee54272027dd5a76b88ab","collapsed":true,"_cell_guid":"b2942fed-8a79-4995-9899-0e1f44eb3ebd"},"cell_type":"code"},{"outputs":[],"execution_count":null,"source":"# clean text\ntrain['text'] = train['text'].map(lambda x: cleanData(x, lowercase=True, remove_stops=True, stemming=True, lemmatization = False))\ntest['text'] = test['text'].map(lambda x: cleanData(x, lowercase=True, remove_stops=True, stemming=True, lemmatization = False))","metadata":{"_uuid":"58685087a0322fcda8706880104b09d577ea7ab2","collapsed":true,"_cell_guid":"1eb905b0-d6af-49a6-aa51-532e4d73ae9a"},"cell_type":"code"},{"outputs":[],"execution_count":null,"source":"test['author'] = np.nan\nalldata = pd.concat([train, test]).reset_index(drop=True)","metadata":{"_uuid":"4b94a2f380b03a6f9d085783ad32a36436b8be40","collapsed":true,"_cell_guid":"f5640a15-ee88-435f-b7af-17bdac551a27"},"cell_type":"code"},{"outputs":[],"execution_count":null,"source":"#tfidfvec = TfidfVectorizer(analyzer='word', ngram_range = (1,1))\ntfidfvec = CountVectorizer(analyzer='word', ngram_range = (1,1),min_df = 1, max_features= 5000)\ntfidfdata = tfidfvec.fit_transform(alldata['text'])","metadata":{"_uuid":"9579e4cba026c878d0f8c3ebf105c351969c6568","collapsed":true,"_cell_guid":"f3f35dfc-90ec-42cf-abf7-46bf720cf3c1"},"cell_type":"code"},{"outputs":[],"execution_count":null,"source":"tfidfdata.shape","metadata":{"_uuid":"7e5d6d40fa269f6c22bf8fa7acfa55975a71d034","_cell_guid":"ca2d71fc-0d10-433d-8785-ee13b7967d32"},"cell_type":"code"},{"outputs":[],"execution_count":null,"source":"# create dataframe for features\ntfidf_df = pd.DataFrame(tfidfdata.todense())","metadata":{"_uuid":"1fb0fcc606ddef4c81235e2df4792f7a6b920aec","collapsed":true,"_cell_guid":"ce32b9b5-00fd-4900-b11e-3590f3a1d323"},"cell_type":"code"},{"outputs":[],"execution_count":null,"source":"tfidf_df.columns = ['col' + str(x) for x in tfidf_df.columns]","metadata":{"_uuid":"a64faf9836953130b5f864b72442343337406f47","collapsed":true,"_cell_guid":"8db3c9d0-65e2-48e8-a470-ffe544d58997"},"cell_type":"code"},{"outputs":[],"execution_count":null,"source":"tfid_df_train = tfidf_df[:len(train)]\ntfid_df_test = tfidf_df[len(train):]","metadata":{"_uuid":"ab28dedf854ceda85667fab196e56b16e15030be","collapsed":true,"_cell_guid":"3c5ffc8d-cd76-4c9d-85be-a434d433dc62"},"cell_type":"code"},{"outputs":[],"execution_count":null,"source":"# merge into a new data frame with features\ntrain_feats2 = pd.concat([tfid_df_train], axis=1)\ntest_feats2 = pd.concat([tfid_df_test], axis=1)","metadata":{"_uuid":"41c8520f729039377d3c57a8482a17032d81c916","collapsed":true,"_cell_guid":"eeedc511-2859-40df-a60f-2e7846d04c65"},"cell_type":"code"},{"outputs":[],"execution_count":null,"source":"# lgb\nimport lightgbm as lgb\n# default parameters\nparams = {'objective':'multi:softprob',\n          'gamma':1,\n           'eval_metric':'mlogloss',\n          'max_depth': 13,\n          'seed':2017,\n          'num_class':3,\n          'subsample':0.5,\n          'eta':0.5\n         }","metadata":{"_uuid":"ab7cd2637abe745be619c0ae13ae00d824923bdf","collapsed":true,"_cell_guid":"2ec79f6f-295a-4243-9d4c-cb447267f68f"},"cell_type":"code"},{"outputs":[],"execution_count":null,"source":"X_train, X_valid, y_train, y_valid = train_test_split(train_feats2, target, train_size = 0.7, stratify = target, random_state = 2017)","metadata":{"_uuid":"dc182dc3921155ea857c5afddb0ac096622b99cb","_cell_guid":"9153a316-78e6-4518-8e9b-5ff9f9dbf1c3"},"cell_type":"code"},{"outputs":[],"execution_count":null,"source":"dtrain = xgb.DMatrix(data=X_train, label=y_train)\ndvalid = xgb.DMatrix(data=X_valid, label=y_valid)\ndtest = xgb.DMatrix(data=test_feats2)\nwatchlist = [(dtrain, 'train'),(dvalid, 'eval')]","metadata":{"_uuid":"17c6a99adc42d11c51961b33793b1640ec6915ee","collapsed":true,"_cell_guid":"32644949-ea6f-4123-837c-58fb07908d64"},"cell_type":"code"},{"outputs":[],"execution_count":null,"source":"model = xgb.train(params, dtrain, 1000, watchlist, maximize=False, verbose_eval=20, early_stopping_rounds=40)","metadata":{"_uuid":"030e37f6c9e96d12e07666dd2b5c3b8cc6405ec4","_cell_guid":"1bcea2e0-d3b3-4d54-9ffe-24c197534522"},"cell_type":"code"},{"outputs":[],"execution_count":null,"source":"# from sklearn.linear_model import LogisticRegression,SGDClassifier\n# from sklearn.ensemble import VotingClassifier\n# from sklearn.naive_bayes import MultinomialNB,BernoulliNB, GaussianNB\n# from sklearn.svm import SVC\n# from sklearn.tree import DecisionTreeClassifier\n# from sklearn.ensemble import ExtraTreesClassifier\n\n# # clf1 = LogisticRegression(penalty='l1', dual=False, tol=0.0005, C=1, fit_intercept=True, intercept_scaling=1.0, class_weight=None, random_state=1)\n# # #clf2 = LogisticRegression(penalty='l2', dual=False, tol=0.0005, C=1, fit_intercept=True, intercept_scaling=1.0, class_weight=None, random_state=2)\n# # #clf3 = LogisticRegression(penalty='l2', dual=False, tol=0.0005, C=1, fit_intercept=True, intercept_scaling=0.2, class_weight=None, random_state=25)\n# # #clf1 = BernoulliNB()\n# # #clf2 =  GaussianNB()\n# # clf3 = MultinomialNB()\n# # model = VotingClassifier(estimators=[('lr', clf1), ('svc', clf3)],weights=[3,3], voting='soft')\n# model = LogisticRegression(penalty='l1', dual=False, tol=0.0005, C=1, fit_intercept=True, intercept_scaling=1.0, class_weight=None, random_state=2)\n\n","metadata":{"_uuid":"118ceb938e4272111344a149e5a98dceb78d2323","collapsed":true,"_cell_guid":"cffe35aa-5d79-4a53-8743-1a48ade0a4d5"},"cell_type":"code"},{"outputs":[],"execution_count":null,"source":"# # MultinomialNB - term counts is giving higher CV score\n# from sklearn.model_selection import cross_val_score\n# from sklearn.metrics import accuracy_score, make_scorer, log_loss\n# print(cross_val_score(model, train_feats2, target, cv=5, scoring=make_scorer(accuracy_score)))","metadata":{"_uuid":"22dd12d1b822cdcb1b567ddffa4ddf1d33d0014c","collapsed":true,"_cell_guid":"7c420103-dfe9-4e5e-9f19-1c0eed2fb4d9"},"cell_type":"code"},{"outputs":[],"execution_count":null,"source":"# model.fit(train_feats2, target)","metadata":{"_uuid":"8dba7e3c23c09db6db41ec8d222f80ed1c2458c0","collapsed":true,"_cell_guid":"cc8a44fc-a829-4292-b66e-3047e01eb6a7"},"cell_type":"code"},{"outputs":[],"execution_count":null,"source":"preds = model.predict(dtest)","metadata":{"_uuid":"846a8f9ad054a0cbb0ccfca485b8473da84efbaf","collapsed":true,"_cell_guid":"2ebd2663-5e20-4089-8f69-11fa81e87811"},"cell_type":"code"},{"outputs":[],"execution_count":null,"source":"result = pd.DataFrame()\nresult['id'] = test_id\nresult['EAP'] = [x[0] for x in preds]\nresult['HPL'] = [x[1] for x in preds]\nresult['MWS'] = [x[2] for x in preds]\n\nresult.to_csv(\"result.csv\", index=False)","metadata":{"_uuid":"78970d433c0a03813b93eb74dcdd121ccc9a555a","collapsed":true,"_cell_guid":"892831b5-a329-41bd-bd8a-1a0a085b1d62"},"cell_type":"code"},{"outputs":[],"execution_count":null,"source":"","metadata":{"_uuid":"ffb50ad0046407131d58f9cb706da2f30c7e95b5","collapsed":true,"_cell_guid":"47422afe-ba78-4268-a98b-54920c6c484c"},"cell_type":"code"}],"nbformat":4}