{"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3","language":"python"},"language_info":{"file_extension":".py","name":"python","codemirror_mode":{"version":3,"name":"ipython"},"version":"3.6.3","mimetype":"text/x-python","nbconvert_exporter":"python","pygments_lexer":"ipython3"}},"nbformat":4,"nbformat_minor":1,"cells":[{"metadata":{"_uuid":"2ec1c3d467a209df0a2b45f322f6699758db14ac","_cell_guid":"ecb08636-c4b4-406c-bdcd-78dfdf598444"},"source":"## Predicting Spooky Authors: Part 1\n### Author: Blake Conrad","cell_type":"markdown"},{"execution_count":null,"metadata":{"_uuid":"7cab997a81343819a159eb5ec98efb09ee6e2511","_cell_guid":"da4dd525-0ce9-4f01-91dc-5306d600aef3","collapsed":true},"source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\ndf_train = pd.read_csv(\"../input/train.csv\")\ndf_test = pd.read_csv(\"../input/test.csv\")","outputs":[],"cell_type":"code"},{"metadata":{"_uuid":"e3c8f0f8726d97d46b5083ca03ceeddf3de74095","_cell_guid":"2556eef7-0b7f-421d-8365-a34f307616ce"},"source":"## Bag of words\nText preprocessing, tokenizing and filtering of stopwords are included in a high level component that is able to build a dictionary of features and transform documents to feature vectors:\n\nOccurrence count is a good start but there is an issue: longer documents will have higher average count values than shorter documents, even though they might talk about the same topics.\nTo avoid these potential discrepancies it suffices to divide the number of occurrences of each word in a document by the total number of words in the document: these new features are called tf for Term Frequencies.\nAnother refinement on top of tf is to downscale weights for words that occur in many documents in the corpus and are therefore less informative than those that occur only in a smaller portion of the corpus.\nThis downscaling is called tf–idf for “Term Frequency times Inverse Document Frequency”.","cell_type":"markdown"},{"execution_count":null,"metadata":{"_uuid":"fb840512cca511eddd1f5fa01d6125046899c542","_cell_guid":"9c187752-2432-45d1-af25-33ff08e603c5","collapsed":true},"source":"from sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer\n\n# Feed: \n#  1. A list of sentences\n#  2. A pandas dataframe that represents a list of sentences\n# E.g., [\"This is the first sentence, yes.\",\n#        \"Now youre getting the idea, aren't you?\",\n#        ...]\ndef get_bag_of_words(X):\n    \n    count_vect = CountVectorizer()\n    X_counts = count_vect.fit_transform(X)\n    #X_counts.shape\n\n    tf_transformer = TfidfTransformer(use_idf=False).fit(X_counts)\n    X_tf = tf_transformer.transform(X_counts)\n    #X_tf.shape\n\n    tfidf_transformer = TfidfTransformer()\n    X_tfidf = tfidf_transformer.fit_transform(X_counts)\n    #X_tfidf.shape\n    #X_tfidf.data\n    \n    print(\"Bag of words created!\")\n    return X_tfidf","outputs":[],"cell_type":"code"},{"metadata":{"_uuid":"e4f3549083e048476079917b3919899f2f3645e5","_cell_guid":"fb3ded62-1ee6-48c8-b77f-f73e6ee9e9a3"},"source":"## Split training sets\nIt is important to remember when training and testing that upon the split in the data sets prior to building our term frequencies and term frequency inverse document frequencies, the vocabulary is subject to change with the data fed into it. To avoid any dimensionality mismatching, we just build the universal vocabulary with our training and testing set, but only train the model on the observations in the data related to the training data points. Likewise when testing, we only use the testing points. This is loosely managed by the `pd.concat([df1,df2,...,dfn])` function which is the same as a `rbind(df1,df2)` in R, By putting X_train in front of the bag of words for the training term frequency inverse document frequencies, we know to only seek from `0:NumberOfTrainingRows == 0:len(X_train)`, similarly for the testing; since we put it first in the `pd.concat`; `0:len(X_test)`. We use this logic again later when actually building a predictor for the model and cross validation, so it is good to understand the little trick now. ","cell_type":"markdown"},{"execution_count":null,"metadata":{"_uuid":"6fc2d72dfce482b3913ba0a84865b0de54a8af92","_cell_guid":"93fd46a1-d1ca-4c6b-a5d4-7be2131239e3"},"source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(df_train[\"text\"],\n                                                    df_train['author'],\n                                                    test_size=0.33, random_state=42)\n\nX_train_tfidf = get_bag_of_words(pd.concat([X_train, X_test]))\nX_test_tfidf = get_bag_of_words(pd.concat([X_test, X_train]))","outputs":[],"cell_type":"code"},{"metadata":{"_uuid":"cc08345b979dadfd966593ac77bb6148abf300a5","_cell_guid":"b2cca014-e0d3-4618-9097-e7040af90fd2"},"source":"## Train a Naive Bayes Model to predict Authors\n\nMultinomialNB implements the naive Bayes algorithm for multinomially distributed data, and is one of the two classic naive Bayes variants used in text classification (where the data are typically represented as word vector counts, although tf-idf vectors are also known to work well in practice). The distribution is parametrized by vectors \\theta_y = (\\theta_{y1},\\ldots,\\theta_{yn}) for each class y, where n is the number of features (in text classification, the size of the vocabulary) and \\theta_{yi} is the probability P(x_i \\mid y) of feature i appearing in a sample belonging to class y.","cell_type":"markdown"},{"execution_count":null,"metadata":{"_uuid":"0846f0b35290299c13f1acbb6817df41cd53d8fd","_cell_guid":"cd1adb3b-34de-46ba-845f-4d9983c3a449"},"source":"from sklearn.naive_bayes import MultinomialNB\n\n# Fit on our term frequency inverse document frequency\nclf = MultinomialNB().fit(X_train_tfidf[:len(X_train)], y_train)\n\n# Build the test data set\ny_pred = clf.predict(X_test_tfidf[:len(X_test)])\nprint(\"Top 5 Predictions on X_test: \", y_pred[:5])\n\n","outputs":[],"cell_type":"code"},{"metadata":{"_uuid":"b2f7fecd65d670faab880f6dec813e5d6eea91ec","_cell_guid":"ce6dee72-4b8a-4750-b1f4-89f237905fde"},"source":"## Performance\nNow we are interesting in seeing how well we actually predicted. I import some precious libraries, then build a little helper function to get a quick glance at how well we did in some standard areas of performance analysis. I typically like to use `accuracy` as a benchmark, however `log_loss` or `entropy` and others are used depending on the type of problem being dealt with.\n\n## Report\n\nNot too bad, we got a 79% Accuracy  on our first shot. Lets take a look at building a `Pipeline`, `Parameter Grid Optimization`, and `Cross Validation` to see what our best model looks like. Additionally, we can start to look at other models (I.e., New pipelines) for `SVM`, `Random Forest`,  `Adaptive Boosting`, and `Ensemble`.","cell_type":"markdown"},{"execution_count":null,"metadata":{"_uuid":"0cb9541c9351dc283fcff22bd4f738a2defc6583","_cell_guid":"3341ebdc-d8be-44e2-af85-c0d10f04f340"},"source":"#import entropy/log loss as a metric\nfrom sklearn.metrics import precision_score, \\\n    recall_score, confusion_matrix, classification_report, \\\n    accuracy_score, f1_score\nfrom sklearn.metrics import log_loss\nfrom sklearn.metrics import classification_report\n\ndef generate_results(y_true, y_pred):\n    \n    print ('Accuracy:\\n', accuracy_score(y_test, y_pred))\n    print ('F1 score:\\n', f1_score(y_test, y_pred, average='macro'))\n    print ('Recall:\\n', recall_score(y_test, y_pred, average='macro'))\n    print ('Precision:\\n', precision_score(y_test, y_pred, average='macro'))\n    print ('clasification report:\\n', classification_report(y_test,y_pred))\n    print ('confussion matrix:\\n',confusion_matrix(y_test, y_pred))\n    #print ('log loss:\\n',log_loss(y_test, y_pred))\n    #print entropy/log_loss as a metric\n\ngenerate_results(y_test, y_pred)\n\nprint(classification_report(y_test, y_pred, target_names=clf.classes_))","outputs":[],"cell_type":"code"},{"metadata":{"_uuid":"1e04afe420fb98edd3b5a55a138f6a268ee9caac","_cell_guid":"1eae3624-f1d1-408c-a59b-ce2ef3d435b4"},"source":"## Actually do a Kaggle Prediction\n1. `X_train_tfidf = get_bag_of_words(pd.concat([df_train[\"text\"], df_test['text']]))`Again, we build vocabulary on the training and testing data","cell_type":"markdown"},{"execution_count":null,"metadata":{"_uuid":"b4a78acc4ff3a1dc5fe6605125187ca67955f0a0","_cell_guid":"a43d2739-4821-48c7-bd6d-77ae3df2f392","collapsed":true},"source":"X_train_tfidf = get_bag_of_words(pd.concat([df_train[\"text\"], df_test['text']]))\ny_train = df_train[\"author\"]\nX_test_tfidf = get_bag_of_words(pd.concat([df_test[\"text\"], df_train[\"text\"]]))\ny_pred = []\n\nclf = MultinomialNB().fit(X_train_tfidf[:len(df_train)], y_train)\ny_pred = clf.predict_proba(X_test_tfidf[:len(df_test)])\nresults = pd.DataFrame({'id':df_test[\"id\"]})\nresults[clf.classes_] = pd.DataFrame(y_pred)\n\n# For the results, I need a table like the following\n#\n# id | P(author1) | P(author2) | P(author3)\n#\nresults.head()\n","outputs":[],"cell_type":"code"},{"metadata":{"_uuid":"f23f06ab184a92a91dafa6dcb266d480226883e8","_cell_guid":"bb90c5e9-7640-40b0-b26c-86844b9bc1fc"},"source":"## Consider K-Fold Cross Validation","cell_type":"markdown"},{"execution_count":null,"metadata":{"_uuid":"4bcc050e9d17aec59bca1180e84c235030c19293","_cell_guid":"1fc3f71c-9b2d-43f5-90a4-2045b638e90c","collapsed":true},"source":"from sklearn.model_selection import KFold\n\n# Create 10 folds to test our data on\nkf = KFold(n_splits=10)\nscores=[]\nfor train_index, test_index in kf.split(df_train):\n    \n    # Foldi Train/Test Data\n    #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n    \n    X_traini, X_testi = df_train.loc[train_index,\"text\"], df_train.loc[test_index,\"text\"]\n    y_traini, y_testi = df_train.loc[train_index,\"author\"], df_train.loc[test_index,\"author\"]\n    \n    # Foldi Train/Test Bag of words\n    X_train_tfidfi = get_bag_of_words(pd.concat([X_traini, X_testi]))\n    X_test_tfidfi = get_bag_of_words(pd.concat([X_testi, X_traini]))\n    \n    # Foldi Model\n    clfi = MultinomialNB().fit(X_train_tfidfi[:len(X_traini)], y_traini)\n\n    # Test Foldi Model on Foldi held out data\n    y_predi = clfi.predict(X_test_tfidfi[:len(X_testi)])\n    \n    # Append results, iterate\n    scores.append(accuracy_score(y_testi, y_predi))\nprint(\"Accuracy Scores After 10-Fold Cross Validation:\")\nprint(scores)\nprint(\"Average Accuracyy After 10 Folds:\")\nprint(np.mean(scores))","outputs":[],"cell_type":"code"},{"execution_count":null,"metadata":{"_uuid":"80e2e07ea23841dfd140b2eed4106c311d7bbd9f","_cell_guid":"6a41e252-8472-4348-afd6-b95a36ccccb6","collapsed":true},"source":"","outputs":[],"cell_type":"code"},{"metadata":{"_uuid":"35e6a71c5ef3aafecb782070819813a734da7ccc","_cell_guid":"17cfbd6f-a438-4227-8913-6c7d3f6f7cb8"},"source":"## SVM\nSo lets take a look at SVM. Some important things to note:\n<ul>\n<li>The Hyper Parameter `C=1.0` in our model is at default. This is fine, but the area around the hyperplane that we accept/reject from might be more predictive if we allow some wiggle room. This allows for a natural and prescribed amount of error in the training phase to mispredict, which may be helpful for the <em>real world situations</em> we will encounter (I.e., avoids overfitting).</li>\n<li>The Parameter `kernel=’rbf’ in our model is at default. This could be more predictive if we mapped into higher dimensions with different kernel functions. Also, `degree=3` is the initial, but this will tell the degree of the polynomial we wish to map to.</li>\n\n</ul\n<p> It turns out SVM predicts very poorly in this case! So it has been removed :) </p>","cell_type":"markdown"},{"metadata":{"_uuid":"88a58fa66d94beb60cd9e9673f033e1bd8276f2a","_cell_guid":"2441c660-4803-4371-bdcf-0909670c3e62"},"source":"## Parameter Tuning\nWe will use grid search for this, which will make great use for the SVM model we just build (and our Naive Bayes model too!). So for starters, lets do a grid search on the best `alpha` value for the Naive Bayes model, then do a grid search on the best `C` value for the SVM.\n\nObviously, such an exhaustive search can be expensive. If we have multiple CPU cores at our disposal, we can tell the grid searcher to try these eight parameter combinations in parallel with the n_jobs parameter. If we give this parameter a value of -1, grid search will detect how many cores are installed and uses them all:","cell_type":"markdown"},{"metadata":{"_uuid":"0bd83f814a4d8da3c1cd96b861632ad8099b72ba","_cell_guid":"22fdf7a8-b2bc-40ef-b6c6-55fc170e4f2d"},"source":"## Pipeline objects\nEach of the pipeline objects acts as an sklearn estimator (I.e., pipelineObject.fit(X,y)) and the coolest part about them is that you can throw them into a GridSearch CV object and it will optimize the best parameter for each layer of the pipe. Pretty cool eh?","cell_type":"markdown"},{"execution_count":null,"metadata":{"_uuid":"b8334b2ab1456bf28c10de2845b3687e7975ec9d","_cell_guid":"9fa629ee-59c6-4724-be50-126096c3e36f"},"source":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.pipeline import Pipeline\n\nnb_parameters = {#'vect__ngram_range': [(1, 1), (1, 2),(1,3)],\n                 #'tfidf__use_idf': (True, False),\n                 'alpha': (1,0,0.01, 0.001, 0.0001)}\n\nsvm_parameters = {#'vect__ngram_range': [(1, 1), (1, 2),(1,3)],\n                 #'tfidf__use_idf': (True, False),\n                 'C': (1, 10, 100, 1000),\n                 'kernel': ('linear', 'rbf'),\n                 'gamma': (0.0001, 0.001, 0.01, 0.1)}\nxgb_parameters = {#'vect__ngram_range': [(1, 1), (1, 2),(1,3)],\n                 #'tfidf__use_idf': (True, False),\n                 'n_estimators':[1000,1500,2000],\n                 'max_depth':[3],\n                 'subsample':[0.5],\n                 'learning_rate':[0.01, 0.02, 0.03, 0.04, 0.05],\n                 'min_samples_leaf': [1],\n                 'random_state': [3]}\n\n\nfrom sklearn import ensemble\nfrom sklearn.svm import SVC\n\ndef getXGBPipe():\n    clf_xgb_pipe = Pipeline([('vect', CountVectorizer()), \n                              ('tfidf', TfidfTransformer()),\n                              ('clf', ensemble.GradientBoostingClassifier())])\ndef getNaiveBayesPipe():\n    clf_nb_pipe = Pipeline([('vect', CountVectorizer()), \n                              ('tfidf', TfidfTransformer()),\n                              ('clf', MultinomialNB())])\n    return clf_nb_pipe\n\n\ndef getSVMPipe():\n    clf_svm_pipe = Pipeline([('vect', CountVectorizer()), \n                              ('tfidf', TfidfTransformer()),\n                              ('clf', SVC())])\n    return clf_svm_pipe\n\n\n","outputs":[],"cell_type":"code"},{"metadata":{"_uuid":"dd2dcc5b3fc66c1bddf71007ead65e5fa3a14c87","_cell_guid":"b7b36162-c6c9-4c63-b459-b0c1e8f499c3"},"source":"## Grid Search for Naive Bayes Parameters\n\n## Predict on the optimal estimator","cell_type":"markdown"},{"execution_count":null,"metadata":{"_uuid":"aa5d78fbe9d2e285105bff8a3d140f5acd3c13e1","_cell_guid":"3b9dc9b1-ae2d-4f00-bcea-474d59d5a8f1"},"source":"gs_clf_nb = GridSearchCV(MultinomialNB(), nb_parameters, n_jobs=-1, cv=10)\ngs_clf_nb_fit = gs_clf_nb.fit(X_train_tfidf[:len(X_train)], y_train)\nbest_nb_clf_fit = gs_clf_nb_fit.best_estimator_\nprint(\"The best alpha: \", best_nb_clf_fit.alpha)\ny_pred = best_nb_clf_fit.predict_proba(X_test_tfidf[:len(X_test)])\ngenerate_results(y_pred, y_test)\n","outputs":[],"cell_type":"code"},{"metadata":{"_uuid":"392d521eb93b33c865e06f0dc0ea3abdaa54f6a1","_cell_guid":"d9f30c2a-e940-4ab0-9713-5fb70734309c"},"source":"## Submittion for the best NB","cell_type":"markdown"},{"execution_count":null,"metadata":{"_uuid":"44601775c9216365d894ad809064c482e9dd65a0","_cell_guid":"78acef98-d41d-4579-a40b-e85e16ebb22a","collapsed":true},"source":"X_train_tfidf = get_bag_of_words(pd.concat([df_train[\"text\"], df_test['text']]))\ny_train = df_train[\"author\"]\nX_test_tfidf = get_bag_of_words(pd.concat([df_test[\"text\"], df_train[\"text\"]]))\ny_pred = []\n\nclf = best_nb_clf_fit.fit(X_train_tfidf[:len(df_train)], y_train)\ny_pred = clf.predict_proba(X_test_tfidf[:len(df_test)])\nresults = pd.DataFrame({'id':df_test[\"id\"]})\nresults[clf.classes_] = pd.DataFrame(y_pred)\n\n# For the results, I need a table like the following\n#\n# id | P(author1) | P(author2) | P(author3)\n#\nresults.to_csv(\"11102017_2_bestNB.csv\")","outputs":[],"cell_type":"code"},{"metadata":{"_uuid":"cf48b00a0becf0024423758c742592b7215d759f","_cell_guid":"93123552-f35c-4ade-ae55-aa58c0285a2b"},"source":"## What if we could do more with our NB, like support it with other features?\nConsider the following: Each author as a unique person, will have a unique language, vocabulary, and choose their words uniquely.\nIf the postulate is true, then the most unfrequent words could be predictive in revealing who is who by just their words.\nDoes K-means reveal any secrets about 3 unique groups? Lets try!\nDoes another 50 columns with booleans for least frequent words (and booleans to represent if we saw it or not) help identify these spooky authors? Lets try!","cell_type":"markdown"},{"metadata":{"_uuid":"248b396686fb964d68331ed2d78ea14fcb3457a2","_cell_guid":"5c445520-aef2-474f-b4f1-f5e4f031e9e6"},"source":"## Kmeans | k=3\n\nCreate the kmeans for","cell_type":"markdown"},{"execution_count":null,"metadata":{"_uuid":"7e5b4f111d1db46c1adfac2c32e8879aa15ba07e","_cell_guid":"288f2631-c3c4-4c1c-9890-788dc1907b96"},"source":"from sklearn.cluster import KMeans\n#log_loss(y_pred, y_true)\n\nkmeans_column_train = KMeans(n_clusters=3, random_state=0).fit(X_train_tfidf[:len(X_train)]).labels_\nkmeans_column_test = KMeans(n_clusters=3, random_state=0).fit(X_test_tfidf[:len(X_test)]).labels_\n\n# Cast into a usable form to append as a column\nkmeans_column_train = np.matrix(kmeans_column_train).T\nkmeans_column_test = np.matrix(kmeans_column_test).T\n\nprint(\"Kmeans columns created!\")","outputs":[],"cell_type":"code"},{"metadata":{"_uuid":"dfa16ed29db6de1d5ec345b09625defdb82336d7","_cell_guid":"6bae9289-64ca-4aa7-8e1c-19cc802563b4"},"source":"## Feature Engineering | K-means with K=3\n\nLets append the kmeans results columns to our actually data matrices `X_train_tfidf` and `X_test_tfidf` then calculate some metrics and see if it is responsive with our best estimator above (Naive Bayes).\n\nSo it looks pretty promising, lets submit it again and see what we get.","cell_type":"markdown"},{"execution_count":null,"metadata":{"_uuid":"32cd3be6a70904384c1c15a2705c3fe45b5e71c1","_cell_guid":"d52557ee-91d4-4159-ab81-d105d7e31d07","collapsed":true},"source":"# Append the columns\nX_train_raw = np.matrix(X_train_tfidf[:len(X_train)].toarray())\nX_test_raw = np.matrix(X_test_tfidf[:len(X_test)].toarray())\n\nX_train_new = np.concatenate((X_train_raw, kmeans_column_train), axis=1)\nX_test_new = np.concatenate((X_test_raw, kmeans_column_test), axis=1)\n\nclf = best_nb_clf_fit.fit(X_train_new, y_train)\ny_pred = clf.predict_proba(X_test_new)\n\ngenerate_results(y_pred, y_test)","outputs":[],"cell_type":"code"},{"metadata":{},"source":"## Feature Engineering | PCA + Kmeans + Naive Bayes","cell_type":"markdown"},{"execution_count":null,"metadata":{},"source":"\n# 1. PCA on X_train_tfidf and X_test_tfidf\nfrom sklearn.decomposition import PCA\npca = PCA().fit(np.matrix(X_train_tfidf[:len(X_train)].toarray()))\npca\n\n# 2. Visualize to pick the top K components that maximize variance\n\n# 3. Kmeans on our top K columns\n\n# 4. with PCA + Kmeans run NB on it\n\n# 5. Predict on NB and report out how well we did\n\n","outputs":[],"cell_type":"code"},{"metadata":{"_uuid":"54380df89170531d28f12a81055dab1a9ac7786e","_cell_guid":"1b51e0c3-c600-4d19-bc8e-9b57f4a0b4e2"},"source":"## Submit NB with Kmeans Support Column\n\nThis did not actually improve the score! The clusters may not be finding the 3 authors as I suspected to begin with ... ","cell_type":"markdown"},{"execution_count":null,"metadata":{"_uuid":"e2927cad4d4983b4ab3f54a418532885a7b5e834","_cell_guid":"5d137142-a8d8-42ec-bedc-513dbed8d858","collapsed":true},"source":"from sklearn.cluster import KMeans\n\nX_train_tfidf = get_bag_of_words(pd.concat([df_train[\"text\"], df_test['text']]))\ny_train = df_train[\"author\"]\nX_test_tfidf = get_bag_of_words(pd.concat([df_test[\"text\"], df_train[\"text\"]]))\ny_pred = []\nprint(\"Data matrix created!\")\n\nkmeans_column_train = KMeans(n_clusters=3, random_state=0).fit(X_train_tfidf[:len(df_train[\"text\"])]).labels_\nkmeans_column_test = KMeans(n_clusters=3, random_state=0).fit(X_test_tfidf[:len(df_test[\"text\"])]).labels_\nkmeans_column_train = np.matrix(kmeans_column_train).T\nkmeans_column_test = np.matrix(kmeans_column_test).T\n\nprint(\"Kmeans columns created!\")\n\n\nX_train_raw = np.matrix(X_train_tfidf[:len(df_train[\"text\"])].toarray())\nX_train_new = np.concatenate((X_train_raw, kmeans_column_train), axis=1)\nX_test_raw = np.matrix(X_test_tfidf[:len(X_test)].toarray())\nX_test_new = np.concatenate((X_test_raw, kmeans_column_test), axis=1)\nprint(\"Column appended!\")\n\nclf = best_nb_clf_fit.fit(X_train_new, y_train)\nprint(\"Estimator built!\")\ny_pred = clf.predict_proba(X_test_new)\nprint(\"Predictions casted!\")\n\n\nresults = pd.DataFrame({'id':df_test[\"id\"]})\nresults[clf.classes_] = pd.DataFrame(y_pred)\n\n# For the results, I need a table like the following\n#\n# id | P(author1) | P(author2) | P(author3)\n#\nresults.to_csv(\"11102017_2_kmeans.csv\")\nprint(\"Writing out!\")","outputs":[],"cell_type":"code"},{"metadata":{"_uuid":"d91f9573969563071e3ff45d658032ac77a5ea65","_cell_guid":"c2137e37-a2a6-4daa-bae2-4d0ae863636f"},"source":"## Repeat for XGB\n\nThis takes quite a long time to complete .. so I will hold off for now during my experimentation stages :) ","cell_type":"markdown"},{"execution_count":null,"metadata":{"_uuid":"d0e644d26e21da659ff6815908b044aac1138013","_cell_guid":"b943cd90-b703-47a1-8d45-120a977db5d2","collapsed":true},"source":"gs_clf_xgb = GridSearchCV(ensemble.GradientBoostingClassifier(), xgb_parameters, n_jobs=-1, cv=3)\ngs_clf_xgb_fit = gs_clf_xgb.fit(np.matrix(X_train_tfidf[:len(X_train)], y_train)\nbest_xgb_clf_fit = gs_clf_xgb_fit.best_estimator_\nprint(\"The best xgb: \", best_nb_clf_fit)\ny_pred = best_xgb_clf_fit.predict_proba(X_test_tfidf[:len(X_test)])\ngenerate_results(y_pred, y_test)","outputs":[],"cell_type":"code"},{"metadata":{"_uuid":"99fab9ec488b23bc6a0927876dc24d97a3eaaf08","_cell_guid":"3c51b2e8-e4da-42ae-8933-9eb4f85f79da"},"source":"## Run a baseline XGB model\n\nThis run takes a while, but do it next time!!","cell_type":"markdown"},{"execution_count":null,"metadata":{"_uuid":"b2a99472a43e1ee72682a814643839aca5650194","_cell_guid":"40a95288-795d-4b1f-b7c9-d9a2e402345e","collapsed":true},"source":"gs_clf_xgb = ensemble.GradientBoostingClassifier().fit(np.matrix(X_train_tfidf[:len(X_train)].toarray()),\n                                                       y_train)\nprint(\"Classifier fit!\")\ny_pred = best_xgb_clf_fit.predict_proba(np.matrix(X_test_tfidf[:len(X_test)].toarray()))\nprint(\"Classifier predictions casted!!\")\ngenerate_results(y_pred, y_test)","outputs":[],"cell_type":"code"},{"metadata":{"_uuid":"8d282a80332d81b9814b9ce220fb85e5e4e6d0c6","_cell_guid":"26ae9731-6c12-4cd7-b6a6-3de7c6de8b45"},"source":"## Feature Engineering | Naive Bayes + XGB\n\nWell so far we have a pretty promising Naive Bayes model prediciting quite well. The Kmeans didn't help us out any, but maybe XGB will. My next consideration is to see the following:\n\n<ul> \n<li> Consider how well XGB does independently. </li>\n<li> Consider an average between the XGB result set and the NB result set, test to see how predictive it is.</li>\n\n</ul>","cell_type":"markdown"},{"metadata":{"_uuid":"5b3c4dbc1389aff2f3f30a8a8036db3a13d2ad3a","_cell_guid":"afe5814e-ec50-49bb-ab90-6318cd2adc68"},"source":"## Feature Engineering | Most Unfrequent Words\n\nConsidering an assumption I made above that individuals think and rationalize uniquely (I.e., behavioral economics), their individual syntax should be unique as well. So the likelihood of two individuals choosing the exact same phrase is unlikely. So the question I want to pose is as follows: Are words that the vocabulary sees less able to identify who said them? Some things to consider for this postulate:\n\n<ul> \n<li> What is the count of unique words that fits an appropriate threshold? </li>\n<li> Should they be considered as boolean values individually appended to their columns (I.e., did this uncommon word appear in this sentence (for M new words, M new columns)?</li>\n\n</ul>","cell_type":"markdown"},{"metadata":{"_uuid":"5c4cc941f05ef91acbc6edcee9525143045b9c28","_cell_guid":"8c8e0313-f09a-49ad-bf5b-4218bc6447df"},"source":"## Try a XGB Model to see performance gain\n## Try other relavent models to see performance gain\n## Do an ensemble combinations to see performance gain\n## Feature engineering to see performance gain\n\n## To be continued ..","cell_type":"markdown"},{"metadata":{"_uuid":"32affba2211f4fc146bb9060a0d8301a2fc678b5","_cell_guid":"c310628d-0838-4105-8b29-0b6485ca0079"},"source":"","cell_type":"markdown"},{"execution_count":null,"metadata":{"_uuid":"a7463c6a68c6ec972ea756c98a46bcd43aa1c82f","_cell_guid":"7468d57d-750d-4793-8414-7c9cb3e33b5c","collapsed":true},"source":"","outputs":[],"cell_type":"code"},{"execution_count":null,"metadata":{"_uuid":"86ad0717c1794d2e05434907dcd04b8b6b40c3c5","_cell_guid":"a7096c5d-5980-4ee5-b684-e10e57a0d6b8","collapsed":true},"source":"","outputs":[],"cell_type":"code"}]}