{"cells":[{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{},"cell_type":"raw","source":""},{"metadata":{},"cell_type":"markdown","source":"# NLP Course-Quora Binary Classification\n\n\nThis is a binary classification (traditional)/semantic classification approach by applying different techniques -EDA(feature engineering,gram analysis),embeddings,deep neural networks,transformer architectures and evaluation.\n\n\n## Important Points\n\n1. EDA- Data Analysis, Statistical Analysis, Tensor Decomposition Strategies, non semantic inference, gram analysis\n2. Embeddings- Static Embeddings - word2vec, glove, wikitext,keras Embeddings\n3. Preliminary Deep Models- using Keras, TF, important aspects on LSTM,Convolution Networks.\n4. Extensive Embedding Architectures- Transformers - Variants of Bert, GPT\n5. Evaluation- Checking accuracy and predictions\n\n## Salient resources\n\nResources for each part will be shared along with very important kernels and other notebooks which are relevant pertaining to this course.\n\n"},{"metadata":{},"cell_type":"markdown","source":"# EDA- Analysis and Benchmarking\n\n\nThe theme of the kernel is to abstract and segregate potentially insincere Quora questions. This module will be used as a starting point for initial analysis of semantics,grams. For an extended implication and requirements of the kernel,please refer to : [Quora Insincere Question Classification](https://www.kaggle.com/c/quora-insincere-questions-classification/overview). \n\nIn this module we will be looking into the following aspects:\n\n1. The training and test data \n2. Simplified Statistics and Analysis-vectorization, gram analysis\n3. Classic Reduction Techniques- PCA,SVD, TSNE\n4. Creating a benchmark with classic statistical models"},{"metadata":{},"cell_type":"markdown","source":"## Evaluation of the Dataset\n\nThis submodule will assist us in understanding the data as well as have an inital idea of the same.\nSome resources for this include:\n\n1. [PCA](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html),[TSNE](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html),[SVD](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html)\n2. [tfidfvectorization](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html),[CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)\n3. [WordCloud](https://pypi.org/project/wordcloud/)\n4. [Gram-Analysis](https://www.nltk.org/)\n5. [Eda-notebook](https://www.kaggle.com/abhilash1910/tweet-analysis-eda-cleaning-tsne-glove-tf)\n\n\nPCA works by eigenvector decomposition strategy as shown in the sample:\n\n<img src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_ica_vs_pca_thumb.png\"></img>\n\n\n"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"#see the input\n\n!ls ../input\n!ls ../input/quora-insincere-questions-classification","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#import libraries useful for the entire module\nimport pandas as pd \nimport numpy as np\nimport xgboost as xgb\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as mpatches\nimport matplotlib\nfrom tqdm import tqdm\nfrom sklearn.svm import SVC\nfrom keras.models import Sequential\nfrom keras.layers.recurrent import LSTM, GRU\nfrom keras.layers.core import Dense, Activation, Dropout\nfrom keras.layers.embeddings import Embedding\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.utils import np_utils\nfrom sklearn import preprocessing, decomposition, model_selection, metrics, pipeline\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import MultinomialNB\nfrom keras.layers import GlobalMaxPooling1D, Conv1D, MaxPooling1D, Flatten, Bidirectional, SpatialDropout1D\nfrom keras.preprocessing import sequence, text\nfrom keras.callbacks import EarlyStopping\nfrom nltk import word_tokenize\nfrom nltk.corpus import stopwords\nstop_words = stopwords.words('english')\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as mpatches\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.decomposition import PCA, TruncatedSVD,SparsePCA\nfrom sklearn.metrics import classification_report,confusion_matrix\nfrom nltk.tokenize import word_tokenize\nfrom collections import defaultdict\nfrom collections import Counter\nimport seaborn as sns\nfrom wordcloud import WordCloud,STOPWORDS\nimport nltk\nfrom nltk.corpus import stopwords\nimport string\nfrom plotly import tools\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import preprocessing,metrics,manifold\nfrom sklearn.manifold import TSNE\nfrom sklearn.model_selection import train_test_split,cross_val_score,GridSearchCV,cross_val_predict\nfrom imblearn.over_sampling import ADASYN,SMOTE\nfrom imblearn.under_sampling import NearMiss\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.svm import SVC\nimport collections\nimport keras as k\nimport matplotlib.patches as mpatches\nfrom sklearn.metrics import accuracy_score\n%matplotlib inline\nfrom sklearn.preprocessing import RobustScaler\nimport xgboost\nfrom imblearn.metrics import classification_report_imbalanced\nfrom sklearn.metrics import classification_report,roc_auc_score,roc_curve,r2_score,recall_score,confusion_matrix,precision_recall_curve\nfrom collections import Counter\nfrom sklearn.model_selection import StratifiedKFold,KFold,StratifiedShuffleSplit\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df=pd.read_csv('../input/quora-insincere-questions-classification/train.csv')\ntest_df=pd.read_csv('../input/quora-insincere-questions-classification/test.csv')\nprint(train_df.head())\nprint(test_df.head())\nprint(\"===========\")\nprint(\"Training Shape\".format(),train_df.shape)\nprint(\"Testing Shape\".format(),test_df.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"The type of columns in the dataset\")\nprint(\"Columns\".format(),train_df.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n#Insincere question test\ntrain_ext=train_df[train_df['target']==1]['question_text']\nprint(train_ext)\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Analysis 1- Word statistics\n\nThis analysis would be done on both the training and testing datasets.\nA fundamental part of any NLP pipeline before applying transformations"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Training data statistics\n\n#Estimate the value counts\ncount_types=train_df['target'].value_counts()\nprint(\"Extracting counts\".format(),count_types)\n#Count targets with value 1\n\ncount_ones=train_df[train_df['target']==1].shape[0]\nprint(count_ones)\ncount_zeros=train_df[train_df['target']==0].shape[0]\nprint(count_zeros)\n\n#Matplot to plot the amount of questions from either types\ndef plot_counts(count_ones,count_zeros):\n    plt.rcParams['figure.figsize']=(6,6)\n    plt.bar(0,count_ones,width=0.6,label='InSincere Questions',color='Red')\n    plt.legend()\n    plt.bar(2,count_zeros,width=0.6,label='Sincere Questions',color='Green')\n    plt.legend()\n    plt.ylabel('Count of Questions (in M)')\n    plt.xlabel('Types of Questions')\n    plt.show()\n\n    \n#Seaborn Dist plot for analysing the length of a question sentence    \ndef plot_wordcount(count_ones_length,count_zeros_length):\n    fig,(ax1,ax2)=plt.subplots(1,2,figsize=(15,5))\n    sns.distplot(count_zeros_length,ax=ax1,color='Blue')\n    ax1.set_title('Sincere Question Length')\n    sns.distplot(count_ones_length,ax=ax2,color='Red')\n    ax2.set_title('Insincere Question Length')\n    fig.suptitle('Average Length of Words in Question')\n    plt.show()    \n\n#Generic Plotter\ndef plot_count(count_punct_ones,count_punct_zeros,title_1,title_2,subtitle):\n    fig,(ax1,ax2)=plt.subplots(1,2,figsize=(15,5))\n    sns.distplot(count_punct_zeros,ax=ax1,color='Blue')\n    ax1.set_title(title_1)\n    sns.distplot(count_punct_ones,ax=ax2,color='Red')\n    ax2.set_title(title_2)\n    fig.suptitle(subtitle)\n    plt.show()    \n\n\n#preliminary word cloud statistics\ndef display_cloud(data):\n    stopwords=set(STOPWORDS)\n    wordcloud=WordCloud(stopwords=stopwords,max_font_size=120,max_words=300,width=800,height=400,background_color='white',min_font_size=5).generate(str(data))\n    plt.figure(figsize=(24,8))\n    plt.imshow(wordcloud)\n    plt.axis(\"off\")\n    plt.title(\"Word Cloud of the questions\")\n    plt.show()\n\n\n    \n    \n#Extract the length of the insincere-sincere questions\ndef word_length(x):\n    return len(x)\n\n\ncount_ones_length=train_df[train_df['target']==1]['question_text'].str.split().apply(lambda z:word_length(z))\nprint(\"Length of each insincere questions\".format(),count_ones_length[:5])\ncount_zeros_length=train_df[train_df['target']==0]['question_text'].str.split().apply(lambda z: word_length(z))\nprint(\"Length of each sincere questions\".format(),count_zeros_length[:5])\n\n#Plots\nplot_counts(count_ones,count_zeros)\nplot_wordcount(count_ones_length,count_zeros_length)\ndisplay_cloud(train_df['question_text'])\ndisplay_cloud(train_df[train_df['target']==1]['question_text'])\ndisplay_cloud(train_df[train_df['target']==0]['question_text'])\n#Other analysis\nstops=set(stopwords.words('english'))\ncount_punct_ones=train_df[train_df['target']==1]['question_text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))\ncount_punct_zeros=train_df[train_df['target']==0]['question_text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))\ntitle_1='Sincere Question Punctuations'\ntitle_2='Insincere Question Punctuations'\nsubtitle='Punctuations in Questions'\nplot_count(count_punct_ones,count_punct_zeros,title_1,title_2,subtitle)\n\ncount_avg_ones=train_df[train_df['target']==1]['question_text'].apply(lambda z : np.mean([len(z) for w in str(z).split()]))\ncount_avg_zeros=train_df[train_df['target']==0]['question_text'].apply(lambda z: np.mean([len(z) for w in str(z).split()]))\ntitle_1='Insincere Question Average Length'\ntitle_2='Sincere Question Average Length'\nsubtitle='Average Length'\nplot_count(count_avg_ones,count_avg_zeros,title_1,title_2,subtitle)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Test dataset Word Statistics\n\ndef plot_testcount(count_test,title1,subtitle):\n    fig,(ax1)=plt.subplots(1,figsize=(15,5))\n    sns.distplot(count_test,ax=ax1,color='Orange')\n    ax1.set_title(title_1)\n    fig.suptitle(subtitle)\n    plt.show()    \n\ncount_test=test_df['question_text'].str.split().apply(lambda z:word_length(z))\nprint(\"Length of each test questions\".format(),count_test[:5])\n\n#Plots\nplot_testcount(count_test,'Questions in test','Total Questions')\ndisplay_cloud(test_df['question_text'])\n#Other analysis\nstops=set(stopwords.words('english'))\ncount_puncttest=test_df['question_text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))\ntitle_1='Test Question Punctuations'\nsubtitle='Punctuations in Questions'\nplot_testcount(count_puncttest,title_1,subtitle)\n\ncount_avg_test=test_df['question_text'].apply(lambda z : np.mean([len(z) for w in str(z).split()]))\ntitle_1='Test Question Average Length'\nsubtitle='Average Length'\nplot_testcount(count_avg_test,title_1,subtitle)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Analysis 2- Gram Statistics\n\nThis will be useful for analysing the gram (bi,tri) in the words in the questions. It is an important step before cleaning.\nResources:\n\n1. [Seaborn](https://seaborn.pydata.org/index.html)\n2. [Gram Analysis](https://albertauyeung.github.io/2018/06/03/generating-ngrams.html)\n\nA pictorial representation of gram analysis is provided here:\n\n<img src=\"https://miro.medium.com/max/536/1*vZhxrBkCz-yN_rzZBqSKiA.png\"></img>"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Gram analysis on Training set\nstopword=set(stopwords.words('english'))\ndef gram_analysis(data,gram):\n    tokens=[t for t in data.lower().split(\" \") if t!=\"\" if t not in stopword]\n    ngrams=zip(*[tokens[i:] for i in range(gram)])\n    final_tokens=[\" \".join(z) for z in ngrams]\n    return final_tokens\n\n#analyse most common Sentences\ndef mostcommon_words(data):\n    counter=Counter(data)\n    commonwords=counter.most_common()\n    x_coord,y_coord=[],[]\n    for words,occ in commonwords[:20]:\n        if words not in stopword:\n            x_coord.append(occ)\n            y_coord.append(words)\n            \n    sns.barplot(x=x_coord,y=y_coord,saturation=1,orient=\"h\")\n\n#Create frequency grams for analysis\n    \ndef create_dict(data,grams):\n    freq_dict=defaultdict(int)\n    for sentence in data:\n        for tokens in gram_analysis(sentence,grams):\n            freq_dict[tokens]+=1\n    return freq_dict\n\ndef horizontal_bar_chart(df, color):\n    trace = go.Bar(\n        y=df[\"n_gram_words\"].values[::-1],\n        x=df[\"n_gram_frequency\"].values[::-1],\n        showlegend=False,\n        orientation = 'h',\n        marker=dict(\n            color=color,\n        ),\n    )\n    return trace\n\n\n\ndef create_new_df(freq_dict,):\n    freq_df=pd.DataFrame(sorted(freq_dict.items(),key=lambda z:z[1])[::-1])\n    freq_df.columns=['n_gram_words','n_gram_frequency']\n    #print(freq_df.head())\n    #plt.barh(freq_df['n_gram_words'][:20],freq_df['n_gram_frequency'][:20],linewidth=0.3)\n    #plt.show()\n    trace=horizontal_bar_chart(freq_df[:20],'blue')\n    return trace\n    \ndef plot_grams(trace_zero,trace_one):\n    fig = tools.make_subplots(rows=1, cols=2, vertical_spacing=0.04,\n                          subplot_titles=[\"Frequent words of sincere questions\", \n                                          \"Frequent words of insincere questions\"])\n    fig.append_trace(trace_zero, 1, 1)\n    fig.append_trace(trace_ones, 1, 2)\n    fig['layout'].update(height=1200, width=900, paper_bgcolor='rgb(233,233,233)', title=\"Word Count Plots\")\n    py.iplot(fig, filename='word-plots')\n    \n    \ntrain_df_zero=train_df[train_df['target']==0]['question_text']\ntrain_df_ones=train_df[train_df['target']==1]['question_text']\n\n#mostcommon_words(train_df_zero)\nprint(\"Bi-gram analysis\")\nfreq_train_df_zero=create_dict(train_df_zero[:200],2)\n#print(freq_train_df_zero)\ntrace_zero=create_new_df(freq_train_df_zero)\nfreq_train_df_ones=create_dict(train_df_ones[:200],2)\n#print(freq_train_df_zero)\ntrace_ones=create_new_df(freq_train_df_ones)\nplot_grams(trace_zero,trace_ones)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Tri-gram analysis\")\nfreq_train_df_zero=create_dict(train_df_zero[:200],3)\n#print(freq_train_df_zero)\ntrace_zero=create_new_df(freq_train_df_zero)\nfreq_train_df_ones=create_dict(train_df_ones[:200],3)\n#print(freq_train_df_zero)\ntrace_ones=create_new_df(freq_train_df_ones)\nplot_grams(trace_zero,trace_ones)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Sentence Analysis\nmostcommon_words(train_df_zero)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Sentence Analysis\nmostcommon_words(train_df_ones)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Similar gram analysis using nltk after regexing\n\nimport re\nfrom nltk.util import ngrams\ndef gram_analyse(s):\n    \n    s = s.lower()\n    s = re.sub(r'[^a-zA-Z0-9\\s]', ' ', s)\n    tokens = [token for token in s.split(\" \") if token != \"\"]\n    output = list(ngrams(tokens, 5))\n    return output\n\nfor j in range(0,10):\n\n    out=gram_analyse(train_df_ones.iloc[j])\n    print(\"Output grams\",out)\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Inference From Initial Word Analysis\n\nFrom the analysis, we see Sincere questions are in a larger quantity as compared to the Insincere questions. This may lead us to think the dataset is imbalanced. The gram statistics shows that there is a good correlation in trigram analysis of Insincere questions as compared to the Sincere ones. The number of words in insincere questions is larger as compared to the sincere ones, and the same can be deduced from the average sentence length of them as well.\n\nFor balancing certain strategies are important such as synthetic oversampling\n\n1. [SMOTE](https://imbalanced-learn.readthedocs.io/en/stable/generated/imblearn.over_sampling.SMOTE.html)\n2. [ADASYN](https://imbalanced-learn.readthedocs.io/en/stable/generated/imblearn.over_sampling.ADASYN.html)\n\n\n\nThere are other undersampling and oversampling methods as well:\n\n1. [UnderSampling](https://imbalanced-learn.readthedocs.io/en/stable/under_sampling.html)\n2. [OverSampling](https://imbalanced-learn.readthedocs.io/en/stable/over_sampling.html)\n\nThis cannot be directly applied to textual data as it can be applied to vectorized(float) data.\n\n\nSMOTE for imbalanced data can be previewed as follows (with orange samples being the synthetically generated class):\n\n<img src=\"data:image/png;base64,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\"></img>\n\nSynthetic Data points generated by density functions in Adasyn is represented below:\n\n<img src=\"https://glemaitre.github.io/imbalanced-learn/_images/sphx_glr_plot_adasyn_001.png\"></img>\n"},{"metadata":{},"cell_type":"markdown","source":"## Theoretical Diagram of SMOTE/ADASYN operation\n\nSmote and Adasyn employ distance metrics of KNN/Kmeans for generating synthetic points. The difference lies in how the points are generated- Smote employs weighted distance metrics whereas Adasyn performs probabilistic density distributions for generating new points.\n\n<img src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAANIAAADwCAMAAABCI8pNAAABNVBMVEX///8AAAD4+Pj8/Pzw8PD29vbNzc3v7+/q6urz8/Ph4eHc3Nzo6OjAwMDs7OzZ2dnR0dGxsbGpqanDw8Obm5uGhoa6urptbW13d3e1tbWsrKyjo6OTk5NmZmaVlZVfX19XV1d9fX1RUVFra2v9AABCQkI7OztLS0szMzMrKyvgAADvAAALCwv///mPAAAeHh7VAADByb+kmphnAACtAACRk4heFxecAABMLi9vUlK2AABlERBwXFw9NTFcOTiDAAByAACEd3ieoJbU08dubWOVm4nAv7PBsrWvsag5QDXk4NRud2l8e3Lp7+CInIRbXFGiqpdIT0SDeWZpPj5ZSku3uaWHjYHH0sOgkJKXq5V4hm01LypuTEwwAABaKyvExrCYl4E4LS6RhnxsZVmwtptKRUphVU3Yn21WAAAZxUlEQVR4nO1diXvbxrEfYYElcYO474M6KNlmHCvRy2vjNLVkKapixU4bK077mvi5r/3//4S3AC/cBA9ZzvfxF9mxSBDc2ZmdnWsHADvssMMOO2RA6KFHsE3wus2BHUUSGkgPPZZtAGNwlF7KIvJHtK3fPbOEyC8yBgE2mC1+QR8PFMCi0tviPdtAU1K/8AJLkb+UGG/l7hQ2dMCeKYA80BXQXWErt22FPKILv3tHe08yciiK2vjmHFmifF6Ke2LCwT2LtRIUvgAN9zKE5N/90YYzquwndaNnY4/d7MbtKHHC3ptCI7/Qw81ocpvWjhhudN9WiGrxd2FG0d4o/XUT0fP81rf9fuvba4OPSi+Ic5L2JhpPX/POKB60XyBEavsFa0Ivy7SzIGkiNKKx1o0RLN8DtqNRl0KZU/Rk+goxKVbHoNtEGJur1BLomi8+npE0WwnrWH1++zKagxutM19tN7QMrTLeGZuO5+8Iq39t50/0nJXv3QIqyMRLLL+unKSvR4tFJu2veOfKAm2DsD0TGR1O2VGhCbA7KEzz/mr7or6SkuyPtrae9LkaWHpLvNIOwnVcRzMIw9Wub8bJXLVpixed6Hi0kthUsboyoZdf0u2bF/uPN3uNmph3T+TyxSvMey9YfSysu/pnapAjaX7DYPZKed72u4t7tI5RaCtrfKjmu+ckzVTBYpctc8XuvJj4qrLpgngr3oY8G/9cUuYKY++wdC3XlUvrjmxL/pM7Hf7cFjPmJJ2ULsUdpYkqG8GdIZjrfrIAf/h079BbTJA6J6msVv2O1oCxviEab8UnJNtNgeELV8krXWl32+GpFXekPHh7/c+2wJ8tJavkyVQtwe1jK1yStPIrE5qGLDhxZW9ajsFm/s825q1mMXNe4mcDY2xj8Q1CJ/eTijcbjlMW93VgtykyZThnolphZx28Tle1IN5CLJRv30HDYBrfCbbue9aCnxstnEYh21AR2pxImqHyjOvHE1nopMK1zb9+IuoUWIZDQU9QEB0Fq+5Xesm20hO9eAtnyBPKutyKiVf87hpg4miJo8KYKAyrxdHZsoI4osAs6AKiJko7RsP622CXXSCgNL2i9/i4+loLvMKUCI4Rcv2SqaDsFcIDvlGvmLYSwErqjV5t/ZAvVijCotyQkexhGKYxf366UlzDq10z29iLkd24WYireB8VO0QX+em/eBB9nLKMi1zRsrKpSsR+rXEUdaOJspHVaFxhDeSGHZ4KVtghcIkmhKeqC8fpnLHDbKxOxFFUb5DGyWpVkNwxJMsMArbJ9EmnTLKaPmmsIAdu/cRQeroZzbMxLLEm6OC4acUYXaXdDIDTa1ejl0ldc2h3pQgpV9WSs3yGtJASJf6vL7/67w3XjIWTENy6JYOT7H/1Yp29s0o0kRuVGaVPEl1eQQH94dHBwR91u44opasZnZkhqM5RnN2iec78VZQqlRTlBmXrhRsWsino64ODg0cGF9dYRzFCPZqlJz/N38Nmq1Rza8Y2nxOvUbcxK3pjfCLP50ch/2JEhimN/fnnBwdfHnI1SkmwQbeSwyT5xjZGo6YxDQ6fPBmlk1XHiHk8hVsv91MH3vegz9EoNcEpSCy38sXCn7798wtiTUBF3HkefOlcgnMeri7EBvnYn7iX9c4rXmiMloqLNbJ3mmFr4D815HL+dgo+lU8lPqnRkWEiwbsLMK+gTqzSC2ZBgNrtVF9IfosC2l8zLouWWqn6UVFFph4isaLOL+DZM7h4WUsSNY9rPF32/ZMJo7WBUqbOWZIqbQDTIXIiRoWdhRhN4NOXF3B1BT9f1wveIuy5V7PzCHzul8wQyQJwTzONy/AzA5JaL7/WzaI287GJ1MAIX93AdyYo1w2Chxck8dV3zfz0h0S/WNNrU22CY3ejvBqKu1ylclSysFDSufM1EK/gwgXhL7UkLQJqezN9jRYL1st/RtZyaf30YoMwS80motFcakO/k7jKZItQRtONOCPNZ00RNBVuzpx6NsezQc6MADRcaHtHarh4bwCsq8ZKL8lI2kLEgB24TvYzGBTEZcLLaWzCSLkUnj4DxYXbSzDrSZovprlqiZkG136AFrkvA3oaaMhIstWbrFMoVlSx2FC+1y5eXSvnZ8UdcDKr/UxNZOT5VyA7cPsDjP9VSxI9VLJE6miuT5mhezRjTjFuoSpHc5JmFkOYLSd7nfhlMeGFzVMuDXlhhy2SNDOLnSE/ecc3jG+887+ch6PXM5Job/EZakhIUQaDnFLhLYhn+0yY3+m48GReqLBXdJPWySKwxc1deX0Db8h287bsEDkzg3YWwqQ4QeC49EeYUstYsMgWxFU1p4r8vDRqMLsdKyaRgTE3o+io+Jl1jCW2KDXYhbdk97wiq75oM+YMGGdYHzBQTXMyP4QV+zV2H2cvPsenXEJyGFjTqtrBhKIn269GxI5qAv4OlNNecYJyWfMA6UHd9uwmakQUlPynP/9tf7mXzbl2pM8sZyKfClF6T5PS0hHWCNSXdbjy4wXga5B1uCmSpCxS1kFas1XnpzKpeme+fnTw+Ov2qCWrGUeJlhu+kiopplex9vx1Urp0aS35gFXgT4H9a1HwzHkEYpICaMx04M+Il/VlLZfw858GRNr0wHb7akHgew1b6noFEsWbYfMi49Ht+36JS2Vrq5DpyIH7MyHp2xobCPAXjx9/FgVhZpz2i99bteXSi6jVU9tyYhiHRopkOgblJdlACUU/jG+blA2ebehKXPuNwWeff/ai5nX0nPj9j/82+7VIdGl2KP3w6ZG/jmfhYdljOL/X02dWPDaA80D4MEYvioK3WDu5AJ6+X1UT2MZhlVTeC+yfCiQVGaMWg8An9enWDlCJQcwRz1GllClJXGTFVvoTFas6uWT2L5Tfm6tqgo8r3yKZSeQT3ipfPHr87aIspRAswfn7oJkt8XT13IgaAqsboLKg11vzlM3oGfv1+fczxStLaqI/LNqZlOIHiTk11PDz57kPo2HO9SwEIBZm+epsUmVKB19nwWlwUOhEMTIfXm70ppkkV9vJjvIbC6dagVenKCYouIH5mUjmJMWtw6+DmmZCXAYGgtLgYrgGRkGhEBdXlJMysSYonqej+cTTmaGzxOjMiV7+SmtOUrloZjlcskI8FjkceA1upBxkwenc/mDU+M6pNZE52pNjAkgJY9vpkHcTozlzrBxN/pyk1T1AVSYUgWOBGDaQlFCp6ZhXYVadUyZY0xV9TPXd/UDvasfw8zHnZ4pvMMu7QFWIZhgImqHUC14WlKVsLh9Oq9/95hGup4a2miMqTi4vBDBmkrdGpZ9LmDAQQFOK4Q0ZY2Xyk812hyEuIlx7q+pdPKyUG1r6RPTWqcjxRFrFtOPQOJ/HRrF598o8f+lcf5/KOn2Yp4mt1asLUdlb3dAUKcFhZxFd2TDTOey7hrpWwEs2ZohytgJKuGuQPgCcjQ0G+MOC+18fwc6RVFpGjKbh7EfTmuOFlGNZhpLYHpg+v2H2ZwFvMRQUv4XbOxj/cAv7CHpFucO1XKIaueQYYuyIwaloWK37JrJBqDoXG0KeKT1kwc3deHw5huug4kJI9eIwL1a0/MjJT4KD39zA2wv4+Rnfbgokre+uB+RNx4+Snj1Gp+Pxu9OTTkUdaFFWf0zcQCfKaWQz4uFUA+27Bv7eM1gBfJMQ8focUada+KpfswkNylzSXTfV8SjjUzK5njMiZxZR0uD6tzQ8c3PXTtI6Lnkn9BxbQsdW8EHwEgmuf6yQVEk7o0MO2FSDUDK/KOehZqxy8PUzIG7lzfkSwVvxTMRqQMn4lIJzAd5cVE/aWuVv5oaY0qZbS8Hf5o10VYlDLU1tE9d/ieB1TtGvA2Sdj9F5H365gFcVkso6dqALEcvNNuriVBNW+fo1EIK5U+i9bi9bVO/18G3MwHkvjbkqh92MmzlJYtkS4ILYdbNgxp3yEOphhsB6/dqKRlb0TbXmvvbULJ4xoFoNIL5SLeJl3p6N5SXq4WOd/K5gv307rLgepg79a/3y72NYovGaawXuG0vOSsjlUgUttr6x7B+Pjv7xj/YIlveRTnNWsWwRx5VX0CSkxRmBmZ8OWS8eh8Y5t70/0FE/CgzoNbv5Hw8tk00N0r1K/fXXVALZRoeVo5lE5NL0DzHPjXs6XZwDWubD9Fsv4I3RZ48ff0b0IvZLBSLTGpheYhX1BAvuPbNqqaO5JLuahVz/h4z00Cjaj4OJUy5WDSPJtu+1xnspSWZ7Ojsj6Z81miL11JmG+Wg/TSU5pmgSOKa51rn+pbvHkhNN+ItHj77QnLhi/2Cu7WR0TUx6joH+7LWmvbzGdw2FxRsjaVeKsp2dA+STqDLKtrCf3VwRP8BndJqJur5YYhk3IFjqecode1VQbuznqMd8u3vRrPmc72/hBxne/gzmWiQtsR5SxJ1vJieBOLufd7JEsdFNPBzgtEDr6hkod2tp/A6nU3Wlu4JasCpZdtaCa9JMA0wocjBo19xaXOowWvmfX/y6Qph0yipxaQCwqX7L/A8P11rqZ16vxSVmueD96fODx1+sEoRj3DjsYLSyDXx0BnCdlZxdfViLJH0m8ZzMyzJP/sglDSd8e3Bw8McVT2yLTyNx3ZDXAF9paanGb2/X03iz43/sofvSUJNADaPSRsR+QUj6asXEsS5HxAVeYhOz9eljx07XEfx2Dc9KJJmBHdt2sG9bbX2wxOlYaYPMC3cO4/NxyQBF/reff/mHFafcS51H2Z4rQMpJEneSftYcZbYt1BsXAxXkAXx3Cnz5VIrOXbyF28sxey52KNFjR1dw8Q71zqhxURP1jkFLVu3qkY48u6s6YZXyNIsKZounNyJOCZuRWn9EEUf7Q2t/aO//u7x7e9dpsG1MnY2bUoIpZrW9tAU3l8BcjsdvCiQhOSVn1QKtXGqRKEBtHmLPSIgV8IzMEFu17ZN3DjfvgSKDtFsqS6Wp80Anwntgz8bjSzoveLJupIHK8vm8ZQhlaaFP2MGTGUknadEBd8h54UTFrxjr04WbdIRk2tuMaWoqU+xP4VjyKeSzhaqB0YQari7CLYe6p09+yhZzIsm5tByTz4MglDrFWjbLS721EjznA2KMMRg93LaWpoNlh9A7AzjrwVm8eJOPlMnxoLqFHGr4FXcRKNwrv7zDaMjNLQNuQdJ08YTOhEsrpgW9F4DOKHQmjV93KGmmE2RQ47M+vL/JCR4yBG8SN67ZFnXlcow+0HB+q1Xf5XLVNVIltTPTn8uN5uI38tQHdnzWG9+11tBzk/mig1NCEQuXN1Cn3piaF/UEjb/vwakM52WS9CJTD+cklUhYkUu68oGCM3r8/rZN481OcbOHAKcIfrmFq7jusppOJTo3vuPSkydvK4KXsIXFNy+bL+0zK6+lF8QyZOGvY7hrI0mbfA1zZI9i+z8/JsH/1m5CuHqoRZc/3BKpI65aRfB8uXiTaf+QsrWAVizy11XlhYmHDn6x30aSMNW2iEJUhoYje9WtyXvOkrGDq8FpmSRBLqlIzjg83LwLjuC6zmCQFb5vo+mWX3FSdRneETdAgat/VSxA1Ckirt1T+KujK1TdmnSeaMNrDM+ucUXj1R44rOC+0mrNJ3+LqJwl9mwuDW399jZ1b0qwu+Q16TVMx07o1t6CLPGydOliStHV/wH/ukLSqL3GXTbF/tqnmZaD65hiqES9NcuybHs/sQOrvCaY1g7MclZzZbPb7Ge8HrpKaAqqodwqxbzL1f2la7yOVskKhQ491HLYbX7EZN3uq8uRdJ397l6TLQEaNtx2caiw3KVre6is+yYYdmQNOrE0Jb4p9FjpK3sP6JqaSyZT20FN0ZN0qFpLvrkg6V47Q3fArFLqSeeBKLWyt/CeRtsYluyHoR/q6U8uIBV2SeMsRrL88DyeskeozRDsz260XnPBEkL87FK6uLu5ucu3TViSFZvAnZN0tOzSfLVEzXRJnaemC3TRA/rVGM7GWo6kXhe3ZVEZ/XSZhshXFnBBeeRUpIQnROq2wiNCUgi9OzQ+k+AubwZ1ObKy4NLxskuVgoHHg2LI028QvDjzMND2LIdQuL0bj8+I51ZoSdalq8kqa6kEJIcODOLYARGvGyhvhI6J1BljeHebF7xu7W3mtaFE44kSbjZ26e00ce0I/SXAhx6cc0XB65bom/T8f5pWZYixKDXGSjtv3VtByI8NCU4FuP6+QJLeycxXjGja6wOdIJDru04R0+HjdK6bIuR8Bs44eHOhbTKVZqhGvShsyLV83Doh3WbgzS388nOaOcxjNQIFCiSUSPV64iM/qybETMizp9eU9q8iSWsceB2o9c74ihHUTYFt207IT2KV+lQJaxzxqB96bUbgQbCddr+QS/6uC45Pkf69rTblDw1+5L18pb96rfuvN23Xu6V2v8qmU8u7N1fQvwTkjzcdkr5p09YJ6hqGrQT++2u4OaOYMxat1Tsmh+ZKzlWAN37eD2/AzeWYORvD3caCs5U9f/PngvGqcAcoTclKm6+FLUjeFrYk3n6HegY99iVmc7mxN7dkKodSVgf/fEy/I+6CNN5c8EBa44xoEV17cLaBV5FPwVl//Hd6C0p4Y8kzthCY40OfJeto/P62XPmzFj6FpyPyMYJTKtV419sgKazlkxLbw8CKIisYtedWt1IvzA2NnwLj35bx07+3Yi0O6/gkajdk99PH4zdya2BeXa+fXxkUQ5D9tRWh0erYpPku2QARkQWhjaRevI0RfBxoPgjvic3FjM/aSNpCg/d7QV1xBRG8NFI2hkulhST5k32crFtd45rvI9YgewXLNZPEbO9RWVtHUvEOtJdE6ih0KsFdM0nRfaWS7wWiRmU2F5xfN5P0EZ73uj4qdojonqb1VfDmolHjlQuxPx44PENbQMgtDVwjtt95D365gSYu0aMHszviwV0yOHs9OH85bPOnw6KpJhqa5YmvLrXwPw0kxQ+n7SxFhd77rD1Hu4FZGCJtpqe+RJH8NMjXA+5IMTFs7hB1R0PUHvWo68PYiPs92bcEAUw3zfNgSSCn+6OO0CqPRtg+rIlbD7/cLvNsulcsKQ9KEcQfMqk7E2AZl9Ki9y535O6t+UZHROO0cJxsmtej5RFEr0OZoDJ8sH4BU1jUGZXy6PKii0u9/EEv/YcP6e8bxK0X0vTSq06zi1uHjB+sV0AOMQWnDJz+DMqP3ULXuPmwL20nn4LtvW+NhlY0sqP/Pey4OyKWqj6JAdKWK8Kn0OdgTZhxRfc50fYfzP1xIQCOPBlommKANxMMKxy0/XRBEdtdtAIRsPlJe0Y77LDDDjvssMMOO+ywww477LDDDjvssMMOO+zwuwPyJ49V6XZWrP3I6UMfhZ+CtTVOw7I6yUOmzynrZe3a5w/G1fLJPKstrS8dfQplyISkmFWUvkhPSHKOAbwoLZFWWKCyEfKSKiOUPdiYSfu/UFm7vawelp6ctkDTLm9pIxu223noewVryErfRM6kmN4JNNazqDCgbUVW7b6x7xs48NT00Wyg6bqKB4ba831ZT5+3p4quMegPvciL2SNvxEZgqu5A3dqps3VJGvKEIpGfcMkTYoffBzmQXSWmZINwRVd8DgJRAMlIG7lzpi0dK1lu2UgozkkggiH4XAyiut+LTdMvP8L045OU9AhFnDnhkg+RJwV9K5RVJerLoYcJSQYPOqFYOgTJUwJsC9yRIh8BklU/UlKS4owkjw/IJbSsbuH0xGYkRZhoANzLuETbmGdNi7OIiPGS4VG6Cr6IbaSkCtGJcSgOjET0HS09WKaHfdVIlESyOUMeGgqTcE6keOFDl6qyR46t2oOgXHlRPMvrL9VlW+n8sSUoOP0PlzLDxcZm/aUNhYUunYt32GGH3xlY43A4Oy8lZXZOS91TWY0UQD9sYecc1Emuk9kg7Xt93N/PE+XL7MIe8Nssdmr1B7DeC7xJo5aJsdkbEUvBAo1S0seZiybwWJQFW8dmzyWmqaO5GhpwgE0QCUeYAceoLIsdXlQQFkViRtAOK7kPfXZn0qll+vBI3rQ5J4YhY4kGNjhsqLbiYwMLI8kjRp2kqJonR5ylOCFxQ2wskzfZo/6xcIiOuEiOIDENS3noOtzps1gnzaZEKbJRDPuMwbn4CKih6ICn+DzYHgN0lHaXxEQ4jX0qIJYcd6SJupHZrRH5TxQDLugh5fihd91eRtHUHjJEzZNO6JizFX2gD0TX88B3dB/h1BaNPSd0Rs6hiW3Cq7SvsD5SY+EQjtFhb4RD5og+dDXf/KjNhOrADfeeTC06JBOTs88LfJ+XOLJkFCB/cxylgJJqOqRRHEfzAoMlpBGV0cdALuvxtNzjpVgBhigSDMKnUIq73L/uLS2Blp7+zuoH0VK3Dq3aLH+HHXb4nYHJ6y1zsWc2KMWah8J9EuiZ852E8wbzJgY8uNN/c2nvmFq42+kAsG3IX3/51fPpvx0fOFcSFU0TqWPN1D0WBIc5EvvKpKedJgLWyBZsch4xXD3KdACLsFF3ufvAHx4dHHw1i5BaQzbiDOakP6Rj1vSVRPB1L2KI5arJNGBD0ZQhNeID03NUHdui6Zix29yG+YHw6+ODgz9OT8wLoFlyMiB2qJ2Ggx1kOYRREfBh9vA+ZCV9N6b2U5vcM0UIRDGSGcp6+ONVRRhfPp4/nMsTsQpHLDqGiItF1eNiYaRyEbFTtUgVwJSt0I+1uGcrZmgajuKprq068qpP77lvoBdf/zpbDBSW0z4mLM/wPYkTOIkHTgGJJ/9Pn+WZGqtKr8/T5K2+4/dSqxYLSHvwZEUFRQ1tdHR3jO107f0Y6HdMfNEPfeJyhx122GGH3z3+H3XF2ObBZu53AAAAAElFTkSuQmCC\"></img>"},{"metadata":{},"cell_type":"markdown","source":"## Some cleaning\n\nThis is mainly used for removing certain punctuations, stopwords from the corpus. Though the data is quite clean, we will clean this further with regex. \n\n1. [Regex](https://docs.python.org/3/library/re.html)\n\nWe can also stem the corpus but that will not be helpful in the long run, though lemmatizing may  be helpful to a certain extent.\n\n1. [Stem](https://www.nltk.org/howto/stem.html)\n2. [Lemmatize](http://www.nltk.org/api/nltk.stem.html?highlight=lemmatizer)\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Regex cleaning\n\ndef remove_punctuations(data):\n    punct_tag=re.compile(r'[^\\w\\s]')\n    data=punct_tag.sub(r'',data)\n    return data\ndef remove_html(data):\n    html_tag=re.compile(r'<.*?>')\n    data=html_tag.sub(r'',data)\n    return data\n\ndef remove_url(data):\n    url_clean= re.compile(r\"https://\\S+|www\\.\\S+\")\n    data=url_clean.sub(r'',data)\n    return data\ndef clean_data(data):\n    emoji_clean= re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    data=emoji_clean.sub(r'',data)\n    url_clean= re.compile(r\"https://\\S+|www\\.\\S+\")\n    data=url_clean.sub(r'',data)\n    return data\n\n\ntrain_df['question_text']=train_df['question_text'].apply(lambda z : remove_url(z))\ntrain_df['question_text']=train_df['question_text'].apply(lambda z: clean_data(z))\ntrain_df['question_text']=train_df['question_text'].apply(lambda z: remove_html(z))\ntrain_df['question_text']=train_df['question_text'].apply(lambda z: remove_punctuations(z))\n\nprint(\"Cleaned Train Insincere Question Set\")\nprint(train_df[train_df['target']==1]['question_text'].head())\nprint(\"Cleaned Train Sincere Question Set\")\nprint(train_df[train_df['target']==0]['question_text'].head())\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Lemmatizing the corpus as a backup\n\nfrom nltk.stem import WordNetLemmatizer\n\n\ndef lemma_traincorpus(data):\n    lemmatizer=WordNetLemmatizer()\n    out_data=\"\"\n    for words in data:\n        out_data+= lemmatizer.lemmatize(words)\n    return out_data\n\ntrain_df['question_text']=train_df['question_text'].apply(lambda z: lemma_traincorpus(z))\nprint(train_df.head())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Vectorization and Benchmarking\n\nIn this contextt, we will look into vectorizing the training set with tfidf vectorizer and count vectorizer. Post that we will be fitting a statistical model from the sklearn library to get an initial benchmark . This is a very important step as an initial statistical benchmark should always be done before going in to deep models and advanced architectures.\n\nIn this section we will be vectorizing the train corpus first and then we will be using the following models:\n\n1. [LogisticRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)\n2. [Naive Bayes](https://scikit-learn.org/stable/modules/naive_bayes.html)\n3. [SupportVectorMachines](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html)\n4. [LinearDiscriminantAnalysis](https://scikit-learn.org/stable/modules/generated/sklearn.discriminant_analysis.LinearDiscriminantAnalysis.html)\n5. [DecisionTrees](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html)\n6. [RandomForest](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)\n7. [XGBoost](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html)\n8. [LightGBM](https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.LGBMClassifier.html) \n"},{"metadata":{"trusted":true},"cell_type":"code","source":"#tfidf vectorization\n\ntfidf_vect=TfidfVectorizer(stop_words='english',ngram_range=(1,3))\n#tfidf_vect.fit_transform(train_df['question_text'].values.tolist() + test_df['question_text'].values.tolist())\ntrain_tfidf=tfidf_vect.fit_transform(train_df['question_text'].values.tolist())\ntest_tfidf=tfidf_vect.fit_transform(test_df['question_text'].values.tolist())\nprint(train_tfidf)\n\n#count vectorization\ndef count_vectorize(train_data,test_data):\n    count_vectorize=CountVectorizer(stop_words='english',ngram_range=(1,3),analyzer='word',token_pattern='r\\w{1,}')\n    count_vectorize.fit_transform(train_data['question_text'].values.tolist() + test_data['question_text'].values.tolist())\n    train_count=count_vectorize.transform(train_data['question_text'].values.tolist())\n    test_count=count_vectorize.transform(test_data['question_text'].values.tolist())\n    return train_count,test_count\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Creating Statistical Models \n\nIn this section, we will be analysing the outcomes of different models mentioned above using k fold statistics from sklearn.\n\n1. [KFold](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html)\n2. [StratifiedKFold](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedKFold.html)\n3. [Kernel Using KFold](https://www.kaggle.com/abhilash1910/credit-card-fraud)"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_y=train_df['target']\n\ntrain_x=train_df['question_text']\nprint(train_y)\nprint(train_x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Some preliminary models for sequential evaluation on the training set\n\n#train_y=train_df['target'].values\n\n# models=[]\n# models.append(('LR',LogisticRegression()))\n# models.append(('KNN',KNeighborsClassifier()))\n# models.append(('LDA',LinearDiscriminantAnalysis()))\n# models.append(('DT',DecisionTreeClassifier()))\n# #models.append(('SVC',SVC()))\n# model_result=[]\n# scoring='accuracy'\n# for name,model in models:\n#     kfold=KFold(n_splits=10,random_state=7)\n#     results=cross_val_score(model,train_tfidf,train_y,cv=kfold,scoring=scoring)\n#     print(\"Classifiers: \",name, \"Has a training score of\", round(results.mean(), 2) * 100, \"% accuracy score\")\n#     model_result.append(results.mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Partitioning the Set\n\nSince there is no validation set to benchmark against , we create an internal partition of training and testing sets from the tfidf transformed data.\n\n1.[TrainTestSplit](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)"},{"metadata":{"trusted":true},"cell_type":"code","source":"#split the training set into train and test evaluation sets\nprint(train_tfidf.shape)\n\ntrain_x,test_x,train_y,test_y=train_test_split(train_tfidf,train_y,test_size=0.2,random_state=42)\nprint(train_x.shape)\nprint(train_y.shape)\nprint(test_x.shape)\nprint(test_y.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# models=[]\n# #models.append(('LR',LogisticRegression()))\n# models.append(('KNN',KNeighborsClassifier()))\n# models.append(('LDA',LinearDiscriminantAnalysis()))\n# models.append(('DT',DecisionTreeClassifier()))\n\n# #Now trying out different stats models\n# def model_training(model,train_x,test_x,train_y,test_y):\n#     model.fit(train_x,train_y)\n#     pred=model.predict(test_x)\n#     print(\"Evaluate confusion matrix\")\n#     print(confusion_matrix(test_y,pred))\n#     print(accuracy_score(test_y,pred))\n#     return accuracy_score(test_y,pred)\n    \n# for name,mods in models:\n#     accuracy_score=model_training(mods,train_x,test_x,train_y,test_y)\n#     print(accuracy_score)\n    \n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Logistic Regression\n\nThe logistic regression relies on Sigmoid function for classifications:\n\n<img src=\"https://res.cloudinary.com/practicaldev/image/fetch/s--xoKf0Xfi--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://cdn-images-1.medium.com/max/2000/1%2AXisqJi764DTxragiRFexSQ.png\"></img>"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Logistic Regression\nmodel=LogisticRegression()\nmodel.fit(train_x,train_y)\npred=model.predict(test_x)\nprint(\"Evaluate confusion matrix\")\nprint(confusion_matrix(test_y,pred))\nprint(accuracy_score(test_y,pred))\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{"trusted":true},"cell_type":"code","source":"#Naive Bayes\n\nmodel=MultinomialNB()\nmodel.fit(train_x,train_y)\npred=model.predict(test_x)\nprint(\"Evaluate confusion matrix\")\nprint(confusion_matrix(test_y,pred))\nprint(accuracy_score(test_y,pred))\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## LDA OverView:\n\nA Generic LDA, can be viewed as follows:\n\n<img src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_lda_qda_0011.png\"></img>"},{"metadata":{"trusted":true},"cell_type":"code","source":"#LDA- Linear Disciminant Analysis\n\nmodel=LinearDiscriminantAnalysis()\nx_train=train_x.toarray()\nmodel.fit(x_train,train_y)\npred=model.predict(test_x)\nprint(\"Evaluate confusion matrix\")\nprint(confusion_matrix(test_y,pred))\nprint(accuracy_score(test_y,pred))\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Idea on Boosted Trees Architecture\n\nThis [link](https://xgboost.readthedocs.io/en/latest/tutorials/model.html) provides a good material for boosted trees , which can be shown below as :\n\n<img src=\"https://raw.githubusercontent.com/dmlc/web-data/master/xgboost/model/struct_score.png\"></img>"},{"metadata":{"trusted":true},"cell_type":"code","source":"#XGBoost\nfrom xgboost import XGBClassifier as xg\nmodel_xgb= xg(n_estimators=100,random_state=42)\nmodel_xgb.fit(train_x,train_y)\ny_pred_xgb=model_xgb.predict(test_x)\nprint(\"Confusion matrix\")\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Boosting Architecture Summary\n\nThis provides a proper outline of a generic boosting model:\n\n<img src=\"https://miro.medium.com/max/1554/1*FLshv-wVDfu-i54OqvZdHg.png\"></img>"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(confusion_matrix(test_y,y_pred_xgb))\nprint(accuracy_score(test_y,y_pred_xgb.round()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#LightGBM\nfrom lightgbm import LGBMClassifier as lg\nmodel_lgbm= lg(n_estimators=100,random_state=42)\nmodel_lgbm.fit(train_x,train_y)\ny_pred_lgbm=model_lgbm.predict(test_x)\nprint(\"Confusion matrix\")\nprint(confusion_matrix(test_y,y_pred_lgbm))\nprint(accuracy_score(test_y,y_pred_lgbm.round()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Decision Trees\nmodel_dt=DecisionTreeClassifier(random_state=42)\nmodel_dt.fit(train_x,train_y)\ny_pred=model_dt.predict(test_x)\nprint(\"Confusion matrix\")\nprint(confusion_matrix(test_y,y_pred))\nprint(accuracy_score(test_y,y_pred.round()))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Random Forest Architecture\n\nA random forest architecture with majority voting mechanism appears as follows:\n\n<img src=\"https://www.researchgate.net/profile/Hung_Cao12/publication/333438248/figure/fig6/AS:763710377299970@1559094151459/Random-Forest-model-with-majority-voting.ppm\"></img>"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Random Forest\nmodel_dt=RandomForestClassifier(random_state=42)\nmodel_dt.fit(train_x,train_y)\ny_pred=model_dt.predict(test_x)\nprint(\"Confusion matrix\")\nprint(confusion_matrix(test_y,y_pred))\nprint(accuracy_score(test_y,y_pred.round()))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Some facts on SVM\n\nSVM uses a kernel(basis) for marginalizing the classes and hence it can take up a lot of time based on certain kernels. An overview of certain terms in SVM classification can be viewed as follows:\n\n<img src=\"https://static.javatpoint.com/tutorial/machine-learning/images/support-vector-machine-algorithm.png\"></img>"},{"metadata":{},"cell_type":"markdown","source":"### SVM takes a lot of time to train and evaluate\n\nSince SVM takes time for vectorized datasets, we will be seeing it after the reduction/decomposition techniques- PCA/TSNE/SVD."},{"metadata":{},"cell_type":"markdown","source":"## Inference From Statistical Models\n\nWe see that statistical models provide a good benchmark  in terms of performance. Hyperparamater tuning using gridsearch,random search can also help to increase the performance. \n\n1. [GridSearch](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html)\n2. [RandomizedSearch](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html)\n\nThere are other models which are important in this context, namely SVM.SVM generally takes a lot of time to train and hence dimensionality reduction is very important before using SVM for statistical modelling. \n"},{"metadata":{},"cell_type":"markdown","source":"## Dimensionality Reduction Techniques\n\nIn this context , we will get into PCA, SVD and TSNE transformation of the input vectors to generate finite central points . These techniques involve statistical tensor/matrix decomposition algorithms which try to preserve the features based on the importance of the eigen vectors of the matrix."},{"metadata":{"trusted":true},"cell_type":"code","source":"def dimen_reduc_plot(test_data,test_label,option):\n    tsvd= TruncatedSVD(n_components=2,algorithm=\"randomized\",random_state=42)\n    tsne=TSNE(n_components=2,random_state=42) #not recommended instead use PCA\n    pca=SparsePCA(n_components=2,random_state=42)\n    if(option==1):\n        tsvd_result=tsvd.fit_transform(test_data)\n        plt.figure(figsize=(10,8))\n        colors=['orange','red']\n        \n        sns.scatterplot(x=tsvd_result[:,0],y=tsvd_result[:,1],hue=test_label        )\n        \n        plt.show()\n        plt.figure(figsize=(10,10))\n        plt.scatter(tsvd_result[:,0],tsvd_result[:,1],c=test_label,cmap=matplotlib.colors.ListedColormap(colors))\n        color_red=mpatches.Patch(color='red',label='False_Tweet')\n        color_orange=mpatches.Patch(color='orange',label='Real_Tweet')\n        plt.legend(handles=[color_orange,color_red])\n        plt.title(\"TSVD\")\n        plt.show()\n    if(option==2):\n        tsne_result=tsne.fit_transform(test_data)\n        plt.figure(figsize=(10,8))\n        colors=['orange','red']\n        sns.scatterplot(x=tsne_result[:,0],y=tsne_result[:,1],hue=test_label)\n        plt.show()\n        plt.figure(figsize=(10,10))\n        plt.scatter(x=tsne_result[:,0],y=tsne_result[:,1],c=test_label,cmap=matplotlib.colors.ListedColormap(colors))\n        color_red=mpatches.Patch(color='red',label='False_tweet')\n        color_orange=mpatches.Patch(color='orange',label='Real_Tweet')\n        plt.legend(handles=[color_orange,color_red])\n        plt.title(\"PCA\")\n        plt.show() \n    if(option==3):\n        pca_result=pca.fit_transform(test_data.toarray())\n        plt.figure(figsize=(10,8))\n        colors=['orange','red']\n        sns.scatterplot(x=pca_result[:,0],y=pca_result[:,1],hue=test_label)\n        plt.show()\n        plt.figure(figsize=(10,10))\n        plt.scatter(x=pca_result[:,0],y=pca_result[:,1],c=test_label,cmap=matplotlib.colors.ListedColormap(colors))\n        color_red=mpatches.Patch(color='red',label='False_tweet')\n        color_orange=mpatches.Patch(color='orange',label='Real_Tweet')\n        plt.legend(handles=[color_orange,color_red])\n        plt.title(\"TSNE\")\n        plt.show()\n\ndimen_reduc_plot(train_x,train_y,1)\ndimen_reduc_plot(train_x,train_y,3)\ndimen_reduc_plot(train_x,train_y,2)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def dimen_reduc_plot(test_data,test_label,option):\n    tsvd= TruncatedSVD(n_components=2,algorithm=\"randomized\",random_state=42)\n    tsne=TSNE(n_components=2,random_state=42) #not recommended instead use PCA\n    pca=SparsePCA(n_components=2,random_state=42)\n    if(option==1):\n        tsvd_result=tsvd.fit_transform(test_data)\n        plt.figure(figsize=(10,8))\n        colors=['orange','red']\n        \n        sns.scatterplot(x=tsvd_result[:,0],y=tsvd_result[:,1],hue=test_label        )\n        \n        plt.show()\n        plt.figure(figsize=(10,10))\n        plt.scatter(tsvd_result[:,0],tsvd_result[:,1],c=test_label,cmap=matplotlib.colors.ListedColormap(colors))\n        color_red=mpatches.Patch(color='red',label='False_Tweet')\n        color_orange=mpatches.Patch(color='orange',label='Real_Tweet')\n        plt.legend(handles=[color_orange,color_red])\n        plt.title(\"TSVD\")\n        plt.show()\n    if(option==2):\n        tsne_result=tsne.fit_transform(test_data)\n        plt.figure(figsize=(10,8))\n        colors=['orange','red']\n        sns.scatterplot(x=tsne_result[:,0],y=tsne_result[:,1],hue=test_label)\n        plt.show()\n        plt.figure(figsize=(10,10))\n        plt.scatter(x=tsne_result[:,0],y=tsne_result[:,1],c=test_label,cmap=matplotlib.colors.ListedColormap(colors))\n        color_red=mpatches.Patch(color='red',label='False_tweet')\n        color_orange=mpatches.Patch(color='orange',label='Real_Tweet')\n        plt.legend(handles=[color_orange,color_red])\n        plt.title(\"PCA\")\n        plt.show() \n    if(option==3):\n        pca_result=pca.fit_transform(test_data.toarray())\n        plt.figure(figsize=(10,8))\n        colors=['orange','red']\n        sns.scatterplot(x=pca_result[:,0],y=pca_result[:,1],hue=test_label)\n        plt.show()\n        plt.figure(figsize=(10,10))\n        plt.scatter(x=pca_result[:,0],y=pca_result[:,1],c=test_label,cmap=matplotlib.colors.ListedColormap(colors))\n        color_red=mpatches.Patch(color='red',label='False_tweet')\n        color_orange=mpatches.Patch(color='orange',label='Real_Tweet')\n        plt.legend(handles=[color_orange,color_red])\n        plt.title(\"TSNE\")\n        plt.show()\n\ndimen_reduc_plot(test_x,test_y,1)\ndimen_reduc_plot(test_x,test_y,3)\ndimen_reduc_plot(test_x,test_y,2)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Statistical Learning and Inference - Conclusion\n\nThrough these methods, we have evaluated the importance of statistical models in nlp classification tasks just by sampling words, tokens .While these approaches are often overlooked upon , it is very important to mention that  these benchmarks are important to validate against deep learning models. [Abhishek Thakur's Kernel](https://www.kaggle.com/abhishek/approaching-almost-any-nlp-problem-on-kaggle) provides a good idea on this, and can be used for reference. "},{"metadata":{},"cell_type":"markdown","source":"# Neural Networks\n\nIn this context, we will be building a preliminary deep model using sophisticated neural networks and variants of RNNs.\nWe will be building a simple LSTM model for validating the influence of deep models with respect to the statistical ones. In the first case, we will be using the Keras Embedding layer and visualize the results before using the embedding models.\n\n1. [Keras LSTM](https://keras.io/api/layers/recurrent_layers/lstm/)\n2. [Keras](https://keras.io/about/)\n3. [Keras Starter Guides](https://keras.io/examples/nlp/)\n4. [Tensorflow Starter](https://www.tensorflow.org/tutorials/keras/text_classification)\n5. [Tensorflow Hub](https://www.tensorflow.org/tutorials/keras/text_classification_with_hub)\n6. [Jason's Blog-Best practises](https://machinelearningmastery.com/best-practices-document-classification-deep-learning/)\n7. [Jason's Blog-Convolution Networks](https://machinelearningmastery.com/develop-word-embedding-model-predicting-movie-review-sentiment/)\n\nThese are some starter resources for creating preliminary networks for sentiment analysis, text/intent classifications.\nThere will be some advanced architectures which will be focussed later.\n\n\n\nLSTMs are a modified variant of RNNs having 4 gates (i,f,c,o) with the task of alleviating exploding and vanishing gradients.LSTM architecture can be represented as follows:\n\n<img src=\"https://upload.wikimedia.org/wikipedia/commons/3/3b/The_LSTM_cell.png\"></img>\n\n\nThe following are the mathematical formulations of a LSTM cell:\n\n<img src=\"https://www.researchgate.net/profile/Savvas_Varsamopoulos/publication/329362532/figure/fig5/AS:699592479870977@1543807253596/Structure-of-the-LSTM-cell-and-equations-that-describe-the-gates-of-an-LSTM-cell.jpg\"></img>\n\n\nGRUs are another variant of LSTM cells with reduced gates and can be represented as follows:\n\n<img src=\"https://www.data-blogger.com/wp-content/uploads/2017/08/gru.png\"></img>"},{"metadata":{},"cell_type":"markdown","source":"## Convolution Architecture for Image Processing\n\nA classical Convolution 2D architecture for image processing and recognition is shown below:\n\n<img src=\"https://miro.medium.com/max/3288/1*uAeANQIOQPqWZnnuH-VEyw.jpeg\"></img>\n"},{"metadata":{},"cell_type":"markdown","source":"## Simple Dense Model with Embedding Architecture\n\nA simplified architecture of a neural network model with Embeddings and a MLP Dense Neuron is shown below:\n\n<img src=\"https://cdn-images-1.medium.com/max/800/1*4Uil1zWWF5-jlt-FnRJgAQ.png\"></img>"},{"metadata":{},"cell_type":"markdown","source":"## Creating a Preliminary LSTM/CNN model \n\nIn this stage, let us create a preliminary model without any sophisticated embeddings or architectures. We will be using the Keras Sequential API for creating our models. Let us follow the pipeline for creating any network for text classification:\n\n1. Tokenize the input features- This implies converting the input data into tokens (by using one hot encoding/tokenizing) . \n2. Tokenize the targets- This can be done with the help of Label Encoder(sklearn) or using \"values()\" from python\n3. Padd the tokenized features- To ensure that the length of the tokenized feature is same across all the entries(post padding)\n4. Create a simple model- Build a Sequential Network with Keras Embedding layer as the starting point\n5. Add Layers to the model- Add either Conv/LSTM/Bi-LSTM/RNN/GRU layers with different activations- relu is recommended\n6. Add the Dense layer to the model- At the end , we have to add the Dense layer by flattening the output of the previous layer\n7. Add necessary activation functions- Sigmoid for Binary Classification , Softmax for Multi-class classification\n8. Launch Tensorboard- Launch the Tensorboard and visualize the training parameters-loss,accuracy etc.\n\nThis forms the fundamental steps to build a basic but fundamental pipeline for any language modelling task. Sophistications include adding custom embeddings before the keras Embedding layer and then adding certain other layers(transformer architectures) before the LSTM.\n"},{"metadata":{},"cell_type":"markdown","source":"## Sigmoid LSTM/Embedding Architecture\n\nThis diagram provides an outline how the processing of words are done in LSTM - Embedding layers with Sigmoid activation:\n\n<img src=\"https://miro.medium.com/max/489/1*27JmK8VBdphpSCWNb4MhNA.png\"></img>"},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow import keras\nfrom keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.layers import LSTM, Dense,Flatten,Conv2D,Conv1D,GlobalMaxPooling1D\nfrom keras.optimizers import Adam\n#import MiniAttention.MiniAttention as ma\nimport numpy as np  \nimport pandas as pd \nimport re           \nfrom bs4 import BeautifulSoup \nfrom keras.preprocessing.sequence import pad_sequences\nfrom nltk.corpus import stopwords   \nfrom tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Concatenate, TimeDistributed, Bidirectional\nfrom tensorflow.keras.models import Model,Sequential\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.model_selection import train_test_split\nfrom keras.utils import to_categorical\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import LabelBinarizer, LabelEncoder","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Load the input features\ntrain_df=pd.read_csv('../input/quora-insincere-questions-classification/train.csv')\ntest_df=pd.read_csv('../input/quora-insincere-questions-classification/test.csv')\ntrain_set,test_set=train_test_split(train_df,test_size=0.2,random_state=2017)\nprint(train_set.shape)\nprint(test_set.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Tokenize the Input Features\n\nIn this stage we will be tokenizing the inputs using Keras Tokenizers."},{"metadata":{"trusted":true},"cell_type":"code","source":"maxlen=1000\nmax_features=5000 \nembed_size=300\n\n#clean some null words or use the previously cleaned & lemmatized corpus\n\ntrain_x=train_set['question_text'].fillna('_na_').values\nval_x=test_set['question_text'].fillna('_na_').values\n\n#Tokenizing steps- must be remembered\ntokenizer=Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(list(train_x))\ntrain_x=tokenizer.texts_to_sequences(train_x)\nval_x=tokenizer.texts_to_sequences(val_x)\n\n#Pad the sequence- To allow same length for all vectorized words\ntrain_x=pad_sequences(train_x,maxlen=maxlen)\nval_x=pad_sequences(val_x,maxlen=maxlen)\n\n\n#get the target values - either using values or using Label Encoder\ntrain_y=train_set['target'].values\nval_y=test_set['target'].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Padded and Tokenized Training Sequence\".format(),train_x.shape)\nprint(\"Target Values Shape\".format(),train_y.shape)\nprint(\"Padded and Tokenized Training Sequence\".format(),val_x.shape)\nprint(\"Target Values Shape\".format(),val_y.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train_x[2])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Creating a basic Model \n\nIn this step, we create a basic model with help of Keras Embedding Layer. Here we are using hte sequential API from Keras. Some resources related to the activation functions and layers:\n\n1. [Sequential](https://keras.io/guides/sequential_model/)\n2. [Tensorflow Activations](https://www.tensorflow.org/api_docs/python/tf/keras/activations)\n3. [Keras Loss](https://keras.io/api/losses/)\n4. [Keras Optimizers](https://keras.io/api/optimizers/)\n5. [Keras Metrics](https://keras.io/api/metrics/)\n6. [Dense Layer](https://keras.io/api/layers/core_layers/dense/)\n7. [Core Layers](https://keras.io/api/layers/core_layers/)\n\nThese should be sufficient for getting started and creating models."},{"metadata":{},"cell_type":"markdown","source":"## Flow of layers in Sequential Models\n\nParticularly in NLP, the flow of the layers is as follows:\n\n1. Embedding Layer\n2. LSTM/CuDNNLSTM/ Bidirectional LSTM/CuDNNGRU/GRU layers\n2. Convolution/CNN\n3. GlobalMaxPooling Layer -Optional\n4. Dense Layer\n5. Final Dense Layer (with sigmoid activation for binary and softmax activation for multiclass)\n6. Model Compile - loss- binary_crossentropy (binary classification) or categorical_crossentropy(categorical classification)\n7. Optimizers for Model.compile - Adam ,Adagrad,Adadelta ,Nadam,Nesterov,SGD,RMSProp\n8. Metrics- accuracy\n9. Model fit - Input features\n\nThis is the general flow for building any network in NLP \n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Creating a basic model- without pretrained embeddings\nmodel=Sequential()\nmodel.add(Embedding(max_features,embed_size,input_length=maxlen))\nmodel.add(LSTM(60))\nmodel.add(Dense(16,activation='relu'))\nmodel.add(Dense(1,activation='sigmoid'))\nmodel.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Fit the model with the inputs\n\nmodel.fit(train_x,train_y,batch_size=512,epochs=2,verbose=2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#fit and validate together\nmodel.fit(train_x,train_y,batch_size=128,epochs=3,verbose=2,validation_data=(val_x,val_y))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Creating the model without using Sequential API\n\nIn this case, we will be using a different technique to build models . We will specify the inputs explicitly at each layer of the models for this purpose.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, Conv1D\nfrom keras.layers import Bidirectional, GlobalMaxPool1D\nfrom keras.models import Model\nfrom keras import initializers, regularizers, constraints, optimizers, layers\nimport math\nfrom tqdm import tqdm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install --upgrade keras","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#CuDNN uses the Nvidia GPU for faster training\n\ninp=Input(shape=(maxlen,))\nz=Embedding(max_features,embed_size)(inp)\nz=Bidirectional(LSTM(60,return_sequences='True'))(z)\nz=GlobalMaxPool1D()(z)\nz=Dense(16,activation='relu')(z)\nz=Dense(1,activation='sigmoid')(z)\nmodel=Model(inputs=inp,outputs=z)\nmodel.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\nmodel.summary()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#fit and validate together\nmodel.fit(train_x,train_y,batch_size=512,epochs=1,verbose=2,validation_data=(val_x,val_y))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Embeddings\n\nIn this context, we will explore certain embeddings which may increase the performance of the model. Pre-trained embeddings provide a better representation of word vectors, and we will be analysing the word2vec, glove embeddigns as our starting point\n\n1. [Glove](https://nlp.stanford.edu/projects/glove/)\n2. [Word2Vec](https://radimrehurek.com/gensim/models/word2vec.html)\n\n\nWord2Vec employs cosine distance measure between word vectors:\n\n<img src=\"https://3qeqpr26caki16dnhd19sv6by6v-wpengine.netdna-ssl.com/wp-content/uploads/2017/08/Scatter-Plot-of-PCA-Projection-of-Word2Vec-Model.png\"></img>\n\n\nThis excellent resource by Jalammar will help in [understandability](http://jalammar.github.io/illustrated-word2vec/) of skipgram and common bag of words models\n\nGlobal Vectors or glove also employs distance metric for comparison of the word vectors in an unsupervised manner:\n\n<img src=\"https://nlp.stanford.edu/projects/glove/images/man_woman.jpg\"></img>\n\n\nWe will be exploring more as we progress"},{"metadata":{"trusted":true},"cell_type":"code","source":"#see the input\n\n!ls ../input\n!ls ../input/quora-insincere-questions-classification","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!ls ../input/wikinews300d1msubwordvec","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Visualization of Embeddings\n\nWe can also visualize the embeddings which appears like a wave. \nA presentation of this is done in the notebook:\n\n1. [Twitter Analysis](https://www.kaggle.com/abhilash1910/tweet-analysis-eda-cleaning-tsne-glove-tf)\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"#visualize the embeddings\nplt.plot(embedding_matrix[10])\nplt.plot(embedding_matrix[5])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Rebuilding the model with pre-trained embeddings from Glove\n\ninp=Input(shape=(maxlen,))\nz=Embedding(max_features,embed_size,weights=[embedding_matrix])(inp)\nz=Bidirectional(LSTM(60,return_sequences='True'))(z)\nz=GlobalMaxPool1D()(z)\nz=Dense(16,activation='relu')(z)\nz=Dense(1,activation='sigmoid')(z)\nmodel=Model(inputs=inp,outputs=z)\nmodel.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\nmodel.summary()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#fit and validate together- with glove embedding\nmodel.fit(train_x,train_y,batch_size=512,epochs=1,verbose=2,validation_data=(val_x,val_y))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\n\n#Fast Text Embeddings for training \nEMBEDDING_FILE = '../input/wikinews300d1msubwordvec/wiki-news-300d-1M-subword.vec'\ndef get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\nembeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE) if len(o)>100)\n\nall_embs = np.stack(embeddings_index.values())\nemb_mean,emb_std = all_embs.mean(), all_embs.std()\nembed_size = all_embs.shape[1]\n\nword_index = tokenizer.word_index\nnb_words = min(max_features, len(word_index))\nembedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\nfor word, i in word_index.items():\n    if i >= max_features: continue\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#visualize the embeddings-FastText\nplt.plot(embedding_matrix[10])\nplt.plot(embedding_matrix[5])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"inp=Input(shape=(maxlen,))\nz=Embedding(max_features,embed_size,weights=[embedding_matrix])(inp)\nz=Bidirectional(LSTM(60,return_sequences='True'))(z)\nz=GlobalMaxPool1D()(z)\nz=Dense(16,activation='relu')(z)\nz=Dense(1,activation='sigmoid')(z)\nmodel=Model(inputs=inp,outputs=z)\nmodel.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\nmodel.summary()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#fit and validate together- with glove embedding\nmodel.fit(train_x,train_y,batch_size=512,epochs=1,verbose=2,validation_data=(val_x,val_y))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"! ls ../input\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!ls ../input/googlenewsvectorsnegative300","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!ls ../input/paragram-300-sl999","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"! ls ../input/word2vec-sample","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!ls ../input","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\n\n#Paragram Embeddings for training \nEMBEDDING_FILE = '../input/paragram-300-sl999/paragram_300_sl999.txt'\ndef get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\nembeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE) if len(o)>100)\n\nall_embs = np.stack(embeddings_index.values())\nemb_mean,emb_std = all_embs.mean(), all_embs.std()\nembed_size = all_embs.shape[1]\n\nword_index = tokenizer.word_index\nnb_words = min(max_features, len(word_index))\nembedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\nfor word, i in word_index.items():\n    if i >= max_features: continue\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def add_corpus(data):\n    corpus=[]\n    for i in tqdm(data):\n        words=[word for word in (i)]\n        corpus.append(words)\n    return corpus\ndef create_embedding(data):\n    embedding_map={}\n    file=open('../input/paragram-300-sl999/paragram_300_sl999.txt','r')\n    for  f in file:\n        values=f.split(' ')\n        word=values[0]\n        coef=np.asarray(values[1:],dtype='float32')\n        embedding_map[word]=coef\n    file.close()\n    return embedding_map\ndef  embedding_preprocess(data):\n    #max_word_length=1000\n    max_sequence_length=100\n    tokenizer=Tokenizer()\n    tokenizer.fit_on_texts(data)\n    sequences=tokenizer.texts_to_sequences(data)\n    \n    word_idx=tokenizer.word_index\n    data_pad=pad_sequences(sequences,padding=\"post\",maxlen=max_sequence_length)\n    emb_dim=data.get('a').shape[0]\n    num_length=len(word_idx)+1\n    emb_mat=np.zeros((num_length,emb_dim))\n    for word,idx in tqdm(word_idx.items()):\n        if idx > num_length:\n            continue\n        elif idx < num_length:\n            emb_vector=data.get(word)\n            if emb_vector is not None: \n                emb_mat[idx]=emb_vector\n    \n    return emb_mat,word_idx,data_pad,num_length\n    \n    \ncorpus_train_data=add_corpus(train_x)\nprint(\"corpus created\")\n\nembedding_map= create_embedding(corpus_train_data)\nprint(\"Embedding matrix created\")\nemb_mat,word_idx,pad_data,num_words=embedding_preprocess(embedding_map)\nprint(pad_data.shape)\n\nprint(\"Visualise embedded vectors\")\nplt.plot(emb_mat[10])\nplt.plot(emb_mat[20])\nplt.plot(emb_mat[50])\nplt.title(\"Embedding Vectors\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from gensim.models import KeyedVectors\nEMBEDDING_FILE = '../input/googlenewsvectorsnegative300/GoogleNews-vectors-negative300.bin'\nembeddings_index = KeyedVectors.load_word2vec_format(EMBEDDING_FILE, binary=True)\n\nword_index = tokenizer.word_index\nnb_words = min(max_features, len(word_index))\nembedding_matrix = (np.random.rand(nb_words, embed_size) - 0.5) / 5.0\nfor word, i in word_index.items():\n    if i >= max_features: continue\n    if word in embeddings_index:\n        embedding_vector = embeddings_index.get_vector(word)\n        embedding_matrix[i] = embedding_vector","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"inp=Input(shape=(maxlen,))\nz=Embedding(max_features,embed_size,weights=[embedding_matrix])(inp)\nz=Bidirectional(LSTM(60,return_sequences='True'))(z)\nz=GlobalMaxPool1D()(z)\nz=Dense(16,activation='relu')(z)\nz=Dense(1,activation='sigmoid')(z)\nmodel=Model(inputs=inp,outputs=z)\nmodel.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\nmodel.summary()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#fit and validate together- with glove embedding\nmodel.fit(train_x,train_y,batch_size=512,epochs=1,verbose=2,validation_data=(val_x,val_y))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Word2vec examples\n\nfrom gensim.models import Word2Vec\n\nmodel = Word2Vec.load_word2vec_format('../input/googlenewsvectorsnegative300/GoogleNews-vectors-negative300.bin', binary=True, norm_only=True)\n\n\ndog = model['dog']\n\nprint(model.most_similar(positive=['woman', 'king'], negative=['man']))\n\nprint(model.doesnt_match(\"breakfast cereal dinner lunch\".split()))\n\nprint(model.similarity('woman', 'man'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Embeddings Conclusion\n\nThere are many embedding libraries which are present in the Kaggle datasets. This provides a preview using some of the most popular and commonly used word embeddings and running the deep models using it.\nHere is an important kernel which may be of help:\n\n1. [SRK's kernel](https://www.kaggle.com/sudalairajkumar/a-look-at-different-embeddings)\n\nThis concludes today's session -------\n\n\nThe next session on Transformers is [here](https://www.kaggle.com/abhilash1910/nlp-workshop-2-ml-india)\n"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}