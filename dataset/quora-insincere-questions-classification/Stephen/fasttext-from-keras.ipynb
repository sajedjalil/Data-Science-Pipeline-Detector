{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":1,"outputs":[{"output_type":"stream","text":"['embeddings', 'train.csv', 'sample_submission.csv', 'test.csv']\n","name":"stdout"}]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"from __future__ import print_function\nimport numpy as np\nimport pandas as pd\nimport re\n\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing import sequence\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import Embedding\nfrom keras.layers import GlobalAveragePooling1D\n\nfrom sklearn.model_selection import train_test_split","execution_count":2,"outputs":[{"output_type":"stream","text":"Using TensorFlow backend.\n","name":"stderr"}]},{"metadata":{"trusted":true,"_uuid":"c114749b578dac79c59397d03805464f7c44d03c"},"cell_type":"code","source":"def create_ngram_set(input_list, ngram_value=2):\n    \"\"\"\n    Extract a set of n-grams from a list of integers.\n    >>> create_ngram_set([1, 4, 9, 4, 1, 4], ngram_value=2)\n    {(4, 9), (4, 1), (1, 4), (9, 4)}\n    >>> create_ngram_set([1, 4, 9, 4, 1, 4], ngram_value=3)\n    [(1, 4, 9), (4, 9, 4), (9, 4, 1), (4, 1, 4)]\n    \"\"\"\n    return set(zip(*[input_list[i:] for i in range(ngram_value)]))","execution_count":3,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"523c567997be8bf27d6e7f9ff110049df7c0bf8d"},"cell_type":"code","source":"def add_ngram(sequences, token_indice, ngram_range=2):\n    \"\"\"\n    Augment the input list of list (sequences) by appending n-grams values.\n    Example: adding bi-gram\n    >>> sequences = [[1, 3, 4, 5], [1, 3, 7, 9, 2]]\n    >>> token_indice = {(1, 3): 1337, (9, 2): 42, (4, 5): 2017}\n    >>> add_ngram(sequences, token_indice, ngram_range=2)\n    [[1, 3, 4, 5, 1337, 2017], [1, 3, 7, 9, 2, 1337, 42]]\n    Example: adding tri-gram\n    >>> sequences = [[1, 3, 4, 5], [1, 3, 7, 9, 2]]\n    >>> token_indice = {(1, 3): 1337, (9, 2): 42, (4, 5): 2017, (7, 9, 2): 2018}\n    >>> add_ngram(sequences, token_indice, ngram_range=3)\n    [[1, 3, 4, 5, 1337, 2017], [1, 3, 7, 9, 2, 1337, 42, 2018]]\n    \"\"\"\n    new_sequences = []\n    for input_list in sequences:\n        new_list = input_list[:]\n        for ngram_value in range(2, ngram_range + 1):\n            for i in range(len(new_list) - ngram_value + 1):\n                ngram = tuple(new_list[i:i + ngram_value])\n                if ngram in token_indice:\n                    new_list.append(token_indice[ngram])\n        new_sequences.append(new_list)\n\n    return new_sequences","execution_count":4,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a948bb1ccc733865377c187e47af54217bc558fa"},"cell_type":"code","source":"# Set parameters:\n# ngram_range = 2 will add bi-grams features\nngram_range = 2\nmax_features = 20000\nmaxlen = 80\nbatch_size = 32\nembedding_dims = 50\nepochs = 5","execution_count":5,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1d251121114a1af7f44d932bb1844531477f086d"},"cell_type":"code","source":"train = pd.read_csv(\"../input/train.csv\", index_col=None)\nprint(train.shape)\n\ndisplay(train.head())\n\ntest = pd.read_csv(\"../input/test.csv\", index_col=None)\nprint(test.shape)\n\ndisplay(test.head())","execution_count":6,"outputs":[{"output_type":"stream","text":"(1306122, 3)\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"                    qid  ...   target\n0  00002165364db923c7e6  ...        0\n1  000032939017120e6e44  ...        0\n2  0000412ca6e4628ce2cf  ...        0\n3  000042bf85aa498cd78e  ...        0\n4  0000455dfa3e01eae3af  ...        0\n\n[5 rows x 3 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>qid</th>\n      <th>question_text</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>00002165364db923c7e6</td>\n      <td>How did Quebec nationalists see their province...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>000032939017120e6e44</td>\n      <td>Do you have an adopted dog, how would you enco...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0000412ca6e4628ce2cf</td>\n      <td>Why does velocity affect time? Does velocity a...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>000042bf85aa498cd78e</td>\n      <td>How did Otto von Guericke used the Magdeburg h...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0000455dfa3e01eae3af</td>\n      <td>Can I convert montra helicon D to a mountain b...</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}},{"output_type":"stream","text":"(375806, 2)\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"                    qid                                      question_text\n0  0000163e3ea7c7a74cd7  Why do so many women become so rude and arroga...\n1  00002bd4fb5d505b9161  When should I apply for RV college of engineer...\n2  00007756b4a147d2b0b3  What is it really like to be a nurse practitio...\n3  000086e4b7e1c7146103                             Who are entrepreneurs?\n4  0000c4c3fbe8785a3090   Is education really making good people nowadays?","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>qid</th>\n      <th>question_text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0000163e3ea7c7a74cd7</td>\n      <td>Why do so many women become so rude and arroga...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>00002bd4fb5d505b9161</td>\n      <td>When should I apply for RV college of engineer...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>00007756b4a147d2b0b3</td>\n      <td>What is it really like to be a nurse practitio...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>000086e4b7e1c7146103</td>\n      <td>Who are entrepreneurs?</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0000c4c3fbe8785a3090</td>\n      <td>Is education really making good people nowadays?</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true,"_uuid":"74869fbe1b7de3ea2bafd05725bd34d3fd8a7004"},"cell_type":"code","source":"puncts = [',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', \"'\", '$', '&', '/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', '•',  '~', '@', '£', \n '·', '_', '{', '}', '©', '^', '®', '`',  '<', '→', '°', '€', '™', '›',  '♥', '←', '×', '§', '″', '′', 'Â', '█', '½', 'à', '…', \n '“', '★', '”', '–', '●', 'â', '►', '−', '¢', '²', '¬', '░', '¶', '↑', '±', '¿', '▾', '═', '¦', '║', '―', '¥', '▓', '—', '‹', '─', \n '▒', '：', '¼', '⊕', '▼', '▪', '†', '■', '’', '▀', '¨', '▄', '♫', '☆', 'é', '¯', '♦', '¤', '▲', 'è', '¸', '¾', 'Ã', '⋅', '‘', '∞', \n '∙', '）', '↓', '、', '│', '（', '»', '，', '♪', '╩', '╚', '³', '・', '╦', '╣', '╔', '╗', '▬', '❤', 'ï', 'Ø', '¹', '≤', '‡', '√', ]","execution_count":7,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ba7660619d5b9d03f45f03521307a03ba677f53a"},"cell_type":"code","source":"mispell_dict = {\"aren't\" : \"are not\",\n\"can't\" : \"cannot\",\n\"couldn't\" : \"could not\",\n\"didn't\" : \"did not\",\n\"doesn't\" : \"does not\",\n\"don't\" : \"do not\",\n\"hadn't\" : \"had not\",\n\"hasn't\" : \"has not\",\n\"haven't\" : \"have not\",\n\"he'd\" : \"he would\",\n\"he'll\" : \"he will\",\n\"he's\" : \"he is\",\n\"i'd\" : \"I would\",\n\"i'd\" : \"I had\",\n\"i'll\" : \"I will\",\n\"i'm\" : \"I am\",\n\"isn't\" : \"is not\",\n\"it's\" : \"it is\",\n\"it'll\":\"it will\",\n\"i've\" : \"I have\",\n\"let's\" : \"let us\",\n\"mightn't\" : \"might not\",\n\"mustn't\" : \"must not\",\n\"shan't\" : \"shall not\",\n\"she'd\" : \"she would\",\n\"she'll\" : \"she will\",\n\"she's\" : \"she is\",\n\"shouldn't\" : \"should not\",\n\"that's\" : \"that is\",\n\"there's\" : \"there is\",\n\"they'd\" : \"they would\",\n\"they'll\" : \"they will\",\n\"they're\" : \"they are\",\n\"they've\" : \"they have\",\n\"we'd\" : \"we would\",\n\"we're\" : \"we are\",\n\"weren't\" : \"were not\",\n\"we've\" : \"we have\",\n\"what'll\" : \"what will\",\n\"what're\" : \"what are\",\n\"what's\" : \"what is\",\n\"what've\" : \"what have\",\n\"where's\" : \"where is\",\n\"who'd\" : \"who would\",\n\"who'll\" : \"who will\",\n\"who're\" : \"who are\",\n\"who's\" : \"who is\",\n\"who've\" : \"who have\",\n\"won't\" : \"will not\",\n\"wouldn't\" : \"would not\",\n\"you'd\" : \"you would\",\n\"you'll\" : \"you will\",\n\"you're\" : \"you are\",\n\"you've\" : \"you have\",\n\"'re\": \" are\",\n\"wasn't\": \"was not\",\n\"we'll\":\" will\",\n\"didn't\": \"did not\",\n\"tryin'\":\"trying\"}","execution_count":8,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f32b12281617b525e4ac0260bb0a206457e9ce68"},"cell_type":"code","source":"def clean_text(x):\n    x = str(x)\n    for punct in puncts:\n        x = x.replace(punct, f' {punct} ')\n    return x\n\ndef clean_numbers(x):\n    x = str(x)\n    x = re.sub(r'[0-9]{5,}', '#####', x)\n    x = re.sub(r'[0-9]{4}', '####', x)\n    x = re.sub(r'[0-9]{3}', '###', x)\n    x = re.sub(r'[0-9]{2}', '##', x)\n    \n    return x\n\ndef _get_mispell(mispell_dict):\n    mispell_re = re.compile('(%s)' % '|'.join(mispell_dict.keys()))\n    return mispell_dict, mispell_re\n\nmispellings, mispellings_re = _get_mispell(mispell_dict)\n\ndef replace_typical_misspell(text):\n    def replace(match):\n        return mispellings[match.group(0)]\n    \n    text = mispellings_re.sub(replace, text)\n    return text","execution_count":9,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c70e13e70a66675b180f35b0662550e3c32fd485"},"cell_type":"code","source":"train[\"question_text\"] = train[\"question_text\"].apply(lambda x: x.lower())\ntest[\"question_text\"] = test[\"question_text\"].apply(lambda x: x.lower())\n\ntrain[\"question_text\"] = train[\"question_text\"].map(clean_text)\ntest[\"question_text\"] = test[\"question_text\"].map(clean_text)\n\ntrain[\"question_text\"] = train[\"question_text\"].map(clean_numbers)\ntest[\"question_text\"] = test[\"question_text\"].map(clean_numbers)\n\ntrain[\"question_text\"] = train[\"question_text\"].map(replace_typical_misspell)\ntest[\"question_text\"] = test[\"question_text\"].map(replace_typical_misspell)","execution_count":10,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e8f82cd0912967213a01d71654046f44169a7ab8"},"cell_type":"code","source":"df = pd.concat([train ,test],sort=True)\n\ntrain, dev = train_test_split(train, test_size=0.1, random_state=2019)\n\ntrain_X = train[\"question_text\"].fillna(\"_na_\").values\ndev_X = dev[\"question_text\"].fillna(\"_na_\").values\ntest_X = test[\"question_text\"].fillna(\"_na_\").values\n\ntokenizer = Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(list(df['question_text']))\nx_train = tokenizer.texts_to_sequences(train_X)\nx_test = tokenizer.texts_to_sequences(dev_X)\n\ntest_X = tokenizer.texts_to_sequences(test_X)\n\n#train_X = pad_sequences(train_X, maxlen=maxlen)\n#dev_X = pad_sequences(dev_X, maxlen=maxlen)\n#test_X = pad_sequences(test_X, maxlen=maxlen)\n\ny_train = train['target'].values\ny_test = dev['target'].values","execution_count":11,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"52277aba96875874801328bb3f73c16fe37816e5"},"cell_type":"code","source":"print(len(x_train), 'train sequences')\nprint(len(x_test), 'validation sequences')\nprint(len(test_X), 'test sequences')\nprint('Average train sequence length: {}'.format(\n    np.mean(list(map(len, x_train)), dtype=int)))\nprint('Average validation sequence length: {}'.format(\n    np.mean(list(map(len, x_test)), dtype=int)))\nprint('Average test sequence length: {}'.format(\n    np.mean(list(map(len, test_X)), dtype=int)))","execution_count":12,"outputs":[{"output_type":"stream","text":"1175509 train sequences\n130613 validation sequences\n375806 test sequences\nAverage train sequence length: 12\nAverage validation sequence length: 12\nAverage test sequence length: 12\n","name":"stdout"}]},{"metadata":{"trusted":true,"_uuid":"e8591a6495468578918a5783992d7c327136fced"},"cell_type":"code","source":"if ngram_range > 1:\n    print('Adding {}-gram features'.format(ngram_range))\n    # Create set of unique n-gram from the training set.\n    ngram_set = set()\n    for input_list in x_train:\n        for i in range(2, ngram_range + 1):\n            set_of_ngram = create_ngram_set(input_list, ngram_value=i)\n            ngram_set.update(set_of_ngram)\n\n    # Dictionary mapping n-gram token to a unique integer.\n    # Integer values are greater than max_features in order\n    # to avoid collision with existing features.\n    start_index = max_features + 1\n    token_indice = {v: k + start_index for k, v in enumerate(ngram_set)}\n    indice_token = {token_indice[k]: k for k in token_indice}\n\n    # max_features is the highest integer that could be found in the dataset.\n    max_features = np.max(list(indice_token.keys())) + 1\n\n    # Augmenting x_train and x_test with n-grams features\n    x_train = add_ngram(x_train, token_indice, ngram_range)\n    x_test = add_ngram(x_test, token_indice, ngram_range)\n    test_X = add_ngram(test_X, token_indice, ngram_range)\n    print('Average train sequence length: {}'.format(\n        np.mean(list(map(len, x_train)), dtype=int)))\n    print('Average validation sequence length: {}'.format(\n        np.mean(list(map(len, x_test)), dtype=int)))\n    print('Average test sequence length: {}'.format(\n        np.mean(list(map(len, test_X)), dtype=int)))","execution_count":13,"outputs":[{"output_type":"stream","text":"Adding 2-gram features\nAverage train sequence length: 24\nAverage validation sequence length: 23\nAverage test sequence length: 23\n","name":"stdout"}]},{"metadata":{"trusted":true,"_uuid":"8f79fb24aa10d5b5b8bd164b62f0a3cc824afd5e"},"cell_type":"code","source":"print('Pad sequences (samples x time)')\nx_train = sequence.pad_sequences(x_train, maxlen=maxlen)\nx_test = sequence.pad_sequences(x_test, maxlen=maxlen)\ntest_X = sequence.pad_sequences(test_X, maxlen=maxlen)\nprint('x_train shape:', x_train.shape)\nprint('x_test shape:', x_test.shape)\nprint('test_X shape:', test_X.shape)","execution_count":14,"outputs":[{"output_type":"stream","text":"Pad sequences (samples x time)\nx_train shape: (1175509, 80)\nx_test shape: (130613, 80)\ntest_X shape: (375806, 80)\n","name":"stdout"}]},{"metadata":{"trusted":true,"_uuid":"53dc1ebffabbb51fa9be5bf500673f9bea54e294"},"cell_type":"code","source":"print('Build model...')\nmodel = Sequential()\n\n# we start off with an efficient embedding layer which maps\n# our vocab indices into embedding_dims dimensions\nmodel.add(Embedding(max_features,\n                    embedding_dims,\n                    input_length=maxlen))\n\n# we add a GlobalAveragePooling1D, which will average the embeddings\n# of all words in the document\nmodel.add(GlobalAveragePooling1D())\n\n# We project onto a single unit output layer, and squash it with a sigmoid:\nmodel.add(Dense(1, activation='sigmoid'))\n\nmodel.compile(loss='binary_crossentropy',\n              optimizer='adam',\n              metrics=['accuracy'])\nmodel.summary()","execution_count":15,"outputs":[{"output_type":"stream","text":"Build model...\nWARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nColocations handled automatically by placer.\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding_1 (Embedding)      (None, 80, 50)            104571200 \n_________________________________________________________________\nglobal_average_pooling1d_1 ( (None, 50)                0         \n_________________________________________________________________\ndense_1 (Dense)              (None, 1)                 51        \n=================================================================\nTotal params: 104,571,251\nTrainable params: 104,571,251\nNon-trainable params: 0\n_________________________________________________________________\n","name":"stdout"}]},{"metadata":{"trusted":true,"_uuid":"69239bd8ea4a668c55d4035e63b33aa58de4d54a"},"cell_type":"code","source":"model.fit(x_train, y_train,\n          batch_size=batch_size,\n          epochs=2,\n          validation_data=(x_test, y_test))","execution_count":16,"outputs":[{"output_type":"stream","text":"WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse tf.cast instead.\n","name":"stdout"},{"output_type":"stream","text":"/opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:107: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 104571200 elements. This may consume a large amount of memory.\n  num_elements)\n","name":"stderr"},{"output_type":"stream","text":"Train on 1175509 samples, validate on 130613 samples\nEpoch 1/2\n1175509/1175509 [==============================] - 1279s 1ms/step - loss: 0.1311 - acc: 0.9516 - val_loss: 0.1244 - val_acc: 0.9521\nEpoch 2/2\n1175509/1175509 [==============================] - 1273s 1ms/step - loss: 0.1045 - acc: 0.9603 - val_loss: 0.1244 - val_acc: 0.9537\n","name":"stdout"},{"output_type":"execute_result","execution_count":16,"data":{"text/plain":"<keras.callbacks.History at 0x7ff16aa01780>"},"metadata":{}}]},{"metadata":{"trusted":true,"_uuid":"a22f21d4d3c7ac9b7ae3c3223843d79bd8dc3ff4"},"cell_type":"code","source":"score, acc = model.evaluate(x_test, y_test, batch_size=batch_size)\nprint('Test score:', score)\nprint('Test accuracy:', acc)","execution_count":17,"outputs":[{"output_type":"stream","text":"130613/130613 [==============================] - 4s 33us/step\nTest score: 0.12443095594230413\nTest accuracy: 0.9537488611403092\n","name":"stdout"}]},{"metadata":{"trusted":true,"_uuid":"fe526ec775f0c2b15c9ad345c7879c40d19937cf"},"cell_type":"code","source":"pred_test_y = model.predict([test_X], batch_size=1024, verbose=1)\npred_test_y = (pred_test_y > .5).astype(int)\n\ntest['prediction'] = pred_test_y\ncols = ['qid','prediction']\ntest=test[cols]\ntest.to_csv('submission.csv', index=False) ","execution_count":18,"outputs":[{"output_type":"stream","text":"375806/375806 [==============================] - 0s 1us/step\n","name":"stdout"}]},{"metadata":{"trusted":true,"_uuid":"96e77920041ac4b49cb44b5be54cf20104283254"},"cell_type":"code","source":"pred_test_y","execution_count":19,"outputs":[{"output_type":"execute_result","execution_count":19,"data":{"text/plain":"array([[1],\n       [0],\n       [0],\n       ...,\n       [0],\n       [0],\n       [0]])"},"metadata":{}}]},{"metadata":{"trusted":true,"_uuid":"4fc4fe426b972f9ce1950afba4f69d43140e2f12"},"cell_type":"code","source":"print(os.listdir(\".\"))","execution_count":22,"outputs":[{"output_type":"stream","text":"['.ipynb_checkpoints', 'submission.csv', '__notebook_source__.ipynb']\n","name":"stdout"}]},{"metadata":{"trusted":true,"_uuid":"5a72e5be321a30a9b93f6186d38121f309c98b6b"},"cell_type":"code","source":"!head submission.csv","execution_count":21,"outputs":[{"output_type":"stream","text":"qid,prediction\r\n0000163e3ea7c7a74cd7,1\r\n00002bd4fb5d505b9161,0\r\n00007756b4a147d2b0b3,0\r\n000086e4b7e1c7146103,0\r\n0000c4c3fbe8785a3090,0\r\n000101884c19f3515c1a,0\r\n00010f62537781f44a47,0\r\n00012afbd27452239059,0\r\n00014894849d00ba98a9,0\r\n","name":"stdout"}]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}