{"nbformat":4,"nbformat_minor":1,"metadata":{"_is_fork":false,"language_info":{"nbconvert_exporter":"python","pygments_lexer":"ipython3","mimetype":"text/x-python","name":"python","codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","version":"3.6.1"},"_change_revision":0,"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"}},"cells":[{"source":"# Starting kit for PyTorch Deep Learning\n\nWelcome to this tutorial to get started on PyTorch for this competition.\nPyTorch is a promising port of Facebook's Torch to Python.\n\nIt's only 3 months old but has an already promising feature set.\nUnfortunately it's very very raw, and I had a lot of troubles to get started with very basic things:\n- data loading\n- building a basic CNN\n- training\n\nHopefully this will help you getting started using PyTorch on this dataset.","metadata":{"_uuid":"40766e895680e624e4c630b5d36022efa79c1cde","_cell_guid":"82cb34f6-d7d9-5938-c2f9-2b231d073c04"},"cell_type":"markdown"},{"source":"## Importing libraries\nPlease note that we do not import numpy but PyTorch wrapper for Numpy","metadata":{"_uuid":"196aea9e92998b1d0426c92cd792cbf9f75ebecc","_cell_guid":"3763a794-0a61-f0ab-9215-56de74bf29df"},"cell_type":"markdown"},{"execution_count":null,"outputs":[],"metadata":{"_uuid":"e2bcfdd95c26d7fcb2c2ac74fa5de5c1c8b69ee1","collapsed":true,"_cell_guid":"f3ee9f39-55e1-ee69-2bb6-25c095155e1d"},"cell_type":"code","source":"import pandas as pd\nfrom torch import np # Torch wrapper for Numpy\n\nimport os\nfrom PIL import Image\n\nimport torch\nfrom torch.utils.data.dataset import Dataset\nfrom torch.utils.data import DataLoader\nfrom torchvision import transforms\nfrom torch import nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.autograd import Variable\n\nfrom sklearn.preprocessing import MultiLabelBinarizer"},{"source":"## Setting up global variables","metadata":{"_uuid":"b15503e332e53c61dd09d270d89b45c0626905a7","_cell_guid":"e6fde4f7-e8f3-3782-673a-62ce72b652fa"},"cell_type":"markdown"},{"execution_count":null,"outputs":[],"metadata":{"_uuid":"97dcfadb41118534815a39441cd547be0271ee49","collapsed":true,"_cell_guid":"45d63034-a44c-47e8-7376-2deb00af03a9"},"cell_type":"code","source":"IMG_PATH = '../input/train-jpg/'\nIMG_EXT = '.jpg'\nTRAIN_DATA = '../input/train_v2.csv'"},{"source":"## Loading the data - first part - DataSet\n\nThis is probably the most obscure part of PyTorch. Most examples use well known datasets (MNIST ...) and have a custom loader or forces you to have a specific folder structure similar to this:\n\n* data\n    * train\n          * dogs\n          * cats\n     * validation\n          * dogs\n          * cats\n     * test\n          * test\n\nData loading in PyTorch is in 2 parts\n\nFirst the data must be wrapped in a __Dataset__ class with a getitem method that from an index return X_train[index] and y_train[index] and a length method. A Dataset is basically a data storage.\n\nThe following solution loads the image name from a CSV and file path + extension and can be adapted easily for most Kaggle challenges. You won't have to write your own ;).\n\nThe code will:\n\n- Check that all images in CSV exist in the folder\n- Use ScikitLearn MultiLabelBinarizer to OneHotEncode the labels, mlb.inverse_transform(predictions) can be used to get back the textual labels from the predictions\n- Apply PIL transformations to the images. See [here](http://pytorch.org/docs/torchvision/transforms.html) for the supported list.\n- Use ToTensor() to convert from an image with color scale 0-255 to a Tensor with color scale 0-1.\n\nNote: We use PIL instead of OpenCV because it's Torch default image loader and is compatible with `ToTensor()` method. An fast loader called accimage is currently in development and was published 3 days ago [here](https://github.com/pytorch/accimage).\n\nNote 2: This only provides a mapping to the data, **the data is not loaded in memory at this point**. The next part will show you how to load only what is needed for the batch in memory. This is a huge advantage compared to kernels that must load all images at once.","metadata":{"_uuid":"98d2fc3cf62fe4231af852d9d8b649161f466127","_cell_guid":"1aa8e64e-f2eb-f570-bfd3-6098638c5f40"},"cell_type":"markdown"},{"execution_count":null,"outputs":[],"metadata":{"_uuid":"f961efd73fa939ea267f9cfde69991c05ba1e194","collapsed":true,"_cell_guid":"08a005ca-d963-5434-d60d-72d399cb7fe3"},"cell_type":"code","source":"class KaggleAmazonDataset(Dataset):\n    \"\"\"Dataset wrapping images and target labels for Kaggle - Planet Amazon from Space competition.\n\n    Arguments:\n        A CSV file path\n        Path to image folder\n        Extension of images\n        PIL transforms\n    \"\"\"\n\n    def __init__(self, csv_path, img_path, img_ext, transform=None):\n    \n        tmp_df = pd.read_csv(csv_path)\n        assert tmp_df['image_name'].apply(lambda x: os.path.isfile(img_path + x + img_ext)).all(), \\\n\"Some images referenced in the CSV file were not found\"\n        \n        self.mlb = MultiLabelBinarizer()\n        self.img_path = img_path\n        self.img_ext = img_ext\n        self.transform = transform\n\n        self.X_train = tmp_df['image_name']\n        self.y_train = self.mlb.fit_transform(tmp_df['tags'].str.split()).astype(np.float32)\n\n    def __getitem__(self, index):\n        img = Image.open(self.img_path + self.X_train[index] + self.img_ext)\n        img = img.convert('RGB')\n        if self.transform is not None:\n            img = self.transform(img)\n        \n        label = torch.from_numpy(self.y_train[index])\n        return img, label\n\n    def __len__(self):\n        return len(self.X_train.index)"},{"execution_count":null,"outputs":[],"metadata":{"_uuid":"5cb1b0c274c7576f8316614fd7502a664434c4f8","_cell_guid":"98a20a0b-d39e-21a6-232b-990e916f6756"},"cell_type":"code","source":"transformations = transforms.Compose([transforms.Scale(32),transforms.ToTensor()])\n\ndset_train = KaggleAmazonDataset(TRAIN_DATA,IMG_PATH,IMG_EXT,transformations)"},{"source":"## Loading the data - second part - DataLoader\n\nAs was said, loading the data is in 2 parts, we provided PyTorch with a data storage, and we have to tell it how to load it. This is done with __DataLoader__\n\nThe DataLoader defines how you retrieve the images + labels from the dataset. You can tell it to:\n\n* Set the batch size.\n* Shuffle and sample the data randomly, hence implementing __train_test_split__ (check SubsetRandomSampler [here](http://pytorch.org/docs/data.html?highlight=sampler))\n* Improve performance by loading data via  separate thread `num_worker` and using `pin_memory` for CUDA. Documentation [here](http://pytorch.org/docs/notes/cuda.html?highlight=dataloader).","metadata":{"_uuid":"ffc2817a854540ab117b540e89b9b9061d043318","_cell_guid":"2db00aac-0fb9-a1ab-1687-f373875de6bb"},"cell_type":"markdown"},{"execution_count":null,"outputs":[],"metadata":{"_uuid":"1119f5298649ecb89c2c5f852897c2f668c57daf","collapsed":true,"_cell_guid":"a2d57750-80fc-c8fe-9640-f276681f5549"},"cell_type":"code","source":"train_loader = DataLoader(dset_train,\n                          batch_size=256,\n                          shuffle=True,\n                          num_workers=4 # 1 for CUDA\n                         # pin_memory=True # CUDA only\n                         )"},{"source":"## Creating your Neural Network\n\nThis is tricky, you need  to compute yourself the in_channels and out_channels of your filters hence the 2304 input for the Dense layer. The first input 3 corresponds to the number of channels of your image, the 17 output corresponds to the number of target labels.","metadata":{"_uuid":"b160fcc8285bf7bde19b86dddf51088927ee3e9d","_cell_guid":"1a27f04f-d260-46ec-698b-21aba8631f71"},"cell_type":"markdown"},{"execution_count":null,"outputs":[],"metadata":{"_uuid":"c193d2e40bcfe1469a7e532e0f106cc3d798d145","collapsed":true,"_cell_guid":"c9a86c3d-b977-856a-7b71-5bf0cd509691"},"cell_type":"code","source":"class Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.conv1 = nn.Conv2d(3, 32, kernel_size=3)\n        self.conv2 = nn.Conv2d(32, 64, kernel_size=3)\n        self.conv2_drop = nn.Dropout2d()\n        self.fc1 = nn.Linear(2304, 256)\n        self.fc2 = nn.Linear(256, 17)\n\n    def forward(self, x):\n        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n        x = x.view(x.size(0), -1) # Flatten layer\n        x = F.relu(self.fc1(x))\n        x = F.dropout(x, training=self.training)\n        x = self.fc2(x)\n        return F.sigmoid(x)\n\nmodel = Net() # On CPU\n# model = Net().cuda() # On GPU"},{"source":"## Defining your training function","metadata":{"_uuid":"c0ec28fd632b8179f4acbd452a6609955715db11","_cell_guid":"51e51a88-e8fc-467c-98cd-cab80f5e8679"},"cell_type":"markdown"},{"execution_count":null,"outputs":[],"metadata":{"_uuid":"e2e45e9e48ac8fa50d0f233af2e840c081060e19","collapsed":true,"_cell_guid":"7c18ddb7-cd5a-86d3-b3b9-4c6bc467e7ea"},"cell_type":"code","source":"optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5)"},{"execution_count":null,"outputs":[],"metadata":{"_uuid":"637199af63cb44cf516ed56f5d3201d10720980e","collapsed":true,"_cell_guid":"745377b3-d942-a03a-76a9-e27cce51e01d"},"cell_type":"code","source":"def train(epoch):\n    model.train()\n    for batch_idx, (data, target) in enumerate(train_loader):\n        #Â data, target = data.cuda(async=True), target.cuda(async=True) # On GPU\n        data, target = Variable(data), Variable(target)\n        optimizer.zero_grad()\n        output = model(data)\n        loss = F.binary_cross_entropy(output, target)\n        loss.backward()\n        optimizer.step()\n        if batch_idx % 10 == 0:\n            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n                epoch, batch_idx * len(data), len(train_loader.dataset),\n                100. * batch_idx / len(train_loader), loss.data[0]))"},{"source":"## Training your model","metadata":{"_uuid":"5340681ab6f92b0f7d20481d291a210e7c04caeb","_cell_guid":"65a8fce1-f2b6-28ea-a807-216db7011267"},"cell_type":"markdown"},{"execution_count":null,"outputs":[],"metadata":{"_uuid":"7e23cad6ee3132b1de0cf9f220e7c15181e4e2ea","_cell_guid":"5e7ff060-19da-1b01-28ce-bd2e72430fee"},"cell_type":"code","source":"for epoch in range(1, 2):\n    train(epoch)"},{"source":"# Thank you for your attention\n\nHopefully that will help you get started. I still have a lot to figure out in PyTorch like:\n\n* Implementing the train / validation split\n* Figure out data augmentation (and not just random transformations or images)\n* Implementing early stopping\n* Automating computation of intermediate layers\n* Improving the display of each epochs\n\nIf you liked the kernel don't forget to vote and don't hesitate to comment.","metadata":{"_uuid":"980a38d3a688c235bae36e7d7382d3c5a675694d","_cell_guid":"2e306e2f-87f0-f753-ab41-841a3b097afa"},"cell_type":"markdown"},{"source":"## Full code\n\nI have published my full code of the competition in my [GitHub](https://github.com/mratsim/Amazon_Forest_Computer_Vision). (Note: I only worked on it in the first 2 weeks, so it probably lacks the latest findings)\n\nYou will find:\n  - [A script that output the mean and stddev of your image if you want to train from scratch](https://github.com/mratsim/Amazon_Forest_Computer_Vision/blob/master/compute-mean-std.py#L28)\n\n  - [Using weighted loss function](https://github.com/mratsim/Amazon_Forest_Computer_Vision/blob/master/main_pytorch.py#L61)\n\n  - [Logging your experiment](https://github.com/mratsim/Amazon_Forest_Computer_Vision/blob/master/main_pytorch.py#L89)\n\n  - [Composing data augmentations](https://github.com/mratsim/Amazon_Forest_Computer_Vision/blob/master/main_pytorch.py#L103), also [here](https://github.com/mratsim/Amazon_Forest_Computer_Vision/blob/master/src/p_data_augmentation.py#L181).\nNote use [Pillow-SIMD](https://python-pillow.org/pillow-perf/) instead of PIL/Pillow. It is even faster than OpenCV\n\n  - [Loading from a CSV that contains image path - 61 lines yeah](https://github.com/mratsim/Amazon_Forest_Computer_Vision/blob/master/src/p2_dataload.py#L23)\n\n  - [Equivalent in Keras - 216 lines ugh](https://github.com/mratsim/Amazon_Forest_Computer_Vision/blob/master/src/k_dataloader.py). Note: so much lines were needed because by default in Keras you either have the data augmentation with ImageDataGenerator or lazy loading of images with \"flow_from_directory\" and there is no flow_from_csv\n\n  - [Model finetuning with custom PyCaffe weights](https://github.com/mratsim/Amazon_Forest_Computer_Vision/blob/master/src/p_neuro.py#L139)\n\n  - Train_test_split, [PyTorch version](https://github.com/mratsim/Amazon_Forest_Computer_Vision/blob/master/src/p_model_selection.py#L4) and [Keras version](https://github.com/mratsim/Amazon_Forest_Computer_Vision/blob/master/src/k_model_selection.py#L4)\n\n- [Weighted sampling training so that the model view rare cases more often](https://github.com/mratsim/Amazon_Forest_Computer_Vision/blob/master/main_pytorch.py#L131-L140)\n\n - [Custom Sampler creation, example for the balanced sampler](https://github.com/mratsim/Amazon_Forest_Computer_Vision/blob/master/src/p_sampler.py)\n\n - [Saving snapshots each epoch](https://github.com/mratsim/Amazon_Forest_Computer_Vision/blob/master/main_pytorch.py#L171)\n\n - [Loading the best snapshot for prediction](https://github.com/mratsim/Amazon_Forest_Computer_Vision/blob/master/pytorch_predict_only.py#L83)\n\n - [Failed word embeddings experiments](https://github.com/mratsim/Amazon_Forest_Computer_Vision/blob/master/Embedding-RNN-Autoencoder.ipynb) to [combine image and text data](https://github.com/mratsim/Amazon_Forest_Computer_Vision/blob/master/Dual_Feed_Image_Label.ipynb)\n\n - [Combined weighted loss function (softmax for unique weather tags, BCE for multilabel tags)](https://github.com/mratsim/Amazon_Forest_Computer_Vision/blob/master/src/p2_loss.py#L36)\n\n - [Selecting the best F2-threshold](https://github.com/mratsim/Amazon_Forest_Computer_Vision/blob/master/src/p2_metrics.py#L38) via stochastic search at the end of each epoch to [maximize validation score](https://github.com/mratsim/Amazon_Forest_Computer_Vision/blob/526128239a6abcbb32fbf5b34ed8cc7a3cd87c4e/src/p2_validation.py#L49). This is then saved along model parameter.\n\n  - [CNN-RNN combination (work in progress)](https://github.com/mratsim/Amazon_Forest_Computer_Vision/blob/master/src/p3_neuroRNN.py#L10)","metadata":{"_uuid":"048d866ee8581bddd981ecd9c68f201200a3c4eb","_cell_guid":"16f21935-088c-8590-9233-2700afeb3922"},"cell_type":"markdown"}]}