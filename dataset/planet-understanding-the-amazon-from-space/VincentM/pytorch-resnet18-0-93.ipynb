{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Overview\n\nThis competition is a joint initiative by Planet and SCCON, showcasing how satellite imagery can be leveraged to monitor deforestation and climate change overtime. In this challenge, we need to design a multilabel classifier to help detect both small and large scale interventionism within the Amazon forest.\n\nThe stakes of this project are paramount to share knowledge on computer vision applied to forestry and generalize these techniques to other forest monitoring use-cases. Datasets and AI projects on forests are still scarced, so there is a growing need to create massive catalog and gather intelligence about biodiversity. Ultimately, it will benefits and attract more company like [Pachama](https://pachama.com) or [Avalanche](https://avalanche.green/) to protect and get a deeper understanding of our forests.\n\nThe quantity of labelized images in this challenge is limited (about 40,000), so we need to retrain existing models on this small Planet dataset. Resnet has shown to be a strong candidate on previous challenges among pretrained models like VGG, Inception and DenseNet (full list of pretrained torchvision models [here](https://pytorch.org/vision/stable/models.html)). New deep learning techniques are emerging and ideas worth exploring include the following:\n- [Cossim CNN](https://www.rpisoni.dev/posts/cossim-convolution/)\n- [Vision Transformers](https://github.com/lucidrains/vit-pytorch)\n- [Self-supervised learning](https://docs.lightly.ai/tutorials/package/tutorial_simsiam_esa.html)\n\nThis dataset initially contained 4 bands satellite imagery —R, G, B and Near-Infrared (NIR)—, available through torrent download. The torrents seem to have been disabled but fortunately, a Kaggler have saved [the entire RGB .png dataset](https://www.kaggle.com/nikitarom/planets-dataset). The .tif files containing the NIR dataset however, aren't available anymore. NIR is especially useful to compute greenness indexes such as the NDVI. It allows a finer visualisation of the tree canopee and measurement of the carbon absorption, although we would need to recreate a model from scratch to use it, since all models have been pretrained on RGB only. \n\nThe dataset contains a wide class imbalance, with primary forest presence on more than 90% of the labels, whereas conventional mining only account for 0.25%. We need to monitor the scores for these rare classes, as they are the key to an overall good score and an efficient model to detect ponctual human activity.","metadata":{}},{"cell_type":"markdown","source":"## Packages and setup","metadata":{}},{"cell_type":"code","source":"import os\nimport random\nfrom time import time\nfrom glob import glob\nimport pandas as pd\nimport numpy as np\nfrom tqdm.notebook import tqdm\nfrom collections import Counter\nimport dill as pickle\n\nfrom plotly import graph_objects as go\nimport plotly.express as px\nimport plotly.figure_factory as ff\nfrom plotly.subplots import make_subplots\n\nfrom matplotlib import pyplot as plt\n%matplotlib inline\n\nimport cv2\n\nfrom sklearn.manifold import TSNE\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MultiLabelBinarizer\nfrom sklearn.metrics import fbeta_score, confusion_matrix\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, Dataset\nfrom torchvision import transforms as T, models\nfrom torch.optim import Adam\nfrom torch.optim.lr_scheduler import StepLR\n!pip install -q torchsummary --user\nfrom torchsummary import summary\n\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"device: {device}\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-03-09T14:23:00.419449Z","iopub.execute_input":"2022-03-09T14:23:00.419785Z","iopub.status.idle":"2022-03-09T14:23:15.789591Z","shell.execute_reply.started":"2022-03-09T14:23:00.419706Z","shell.execute_reply":"2022-03-09T14:23:15.788649Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We set random seeding for reproducibility purposes.","metadata":{}},{"cell_type":"code","source":"random.seed(101)\nnp.random.seed(101)\ntorch.manual_seed(101);","metadata":{"execution":{"iopub.status.busy":"2022-03-09T14:23:15.79185Z","iopub.execute_input":"2022-03-09T14:23:15.793738Z","iopub.status.idle":"2022-03-09T14:23:15.80149Z","shell.execute_reply.started":"2022-03-09T14:23:15.793695Z","shell.execute_reply":"2022-03-09T14:23:15.800712Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!ls ../input/planets-dataset/planet/planet","metadata":{"execution":{"iopub.status.busy":"2022-03-09T14:23:15.807521Z","iopub.execute_input":"2022-03-09T14:23:15.808006Z","iopub.status.idle":"2022-03-09T14:23:16.482517Z","shell.execute_reply.started":"2022-03-09T14:23:15.807968Z","shell.execute_reply":"2022-03-09T14:23:16.481757Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Exploration\n\n### Labels distribution\n\nLet's first load the training dataset and quantify the label distributions.","metadata":{}},{"cell_type":"code","source":"path = \"../input/planets-dataset/planet/planet/\"\npath_train = os.path.join(path, \"train-jpg\")\npath_test = os.path.join(path, \"test-jpg\")\nprint(\n    f\"train files: {len(os.listdir(path_train))}, \"\n    f\"test files: {len(os.listdir(path_test))}\"\n)","metadata":{"execution":{"iopub.status.busy":"2022-03-09T14:23:16.484421Z","iopub.execute_input":"2022-03-09T14:23:16.484694Z","iopub.status.idle":"2022-03-09T14:23:18.570187Z","shell.execute_reply.started":"2022-03-09T14:23:16.484657Z","shell.execute_reply":"2022-03-09T14:23:18.569434Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"path_class = os.path.join(path, \"train_classes.csv\")\ndf_class = pd.read_csv(path_class)\nprint(df_class.shape)\ndf_class.head()","metadata":{"execution":{"iopub.status.busy":"2022-03-09T14:23:18.572093Z","iopub.execute_input":"2022-03-09T14:23:18.572623Z","iopub.status.idle":"2022-03-09T14:23:18.643731Z","shell.execute_reply.started":"2022-03-09T14:23:18.572583Z","shell.execute_reply":"2022-03-09T14:23:18.64305Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Simple counter of individual label, by splitting them from tags.","metadata":{}},{"cell_type":"code","source":"df_class[\"list_tags\"] = df_class.tags.str.split(\" \")\nrow_tags = df_class.list_tags.values\ntags = [tag for row in row_tags for tag in row]\ncounter_tags = Counter(tags)\ndf_tags = pd.DataFrame(\n    {\"tag\": counter_tags.keys(), \"total\": counter_tags.values()}\n).sort_values(\"total\")\n\nfig = px.bar(df_tags, x=\"total\", y=\"tag\", orientation=\"h\", \n             color=\"total\",\n)\nfig.update_layout(title=\"Class distribution\")\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-09T14:23:18.646431Z","iopub.execute_input":"2022-03-09T14:23:18.646781Z","iopub.status.idle":"2022-03-09T14:23:19.645804Z","shell.execute_reply.started":"2022-03-09T14:23:18.646753Z","shell.execute_reply":"2022-03-09T14:23:19.645144Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As expected, some classes are largely representated whereas some are barely present in this dataset, in a Pareto distribution fashion.\nThere is an important risk that our model barely learn the rare classes or even to exclude them from the training data upon splitting between training and validating sets.\nWe identify clearly theses classes for latter investigation.","metadata":{}},{"cell_type":"code","source":"RARE_CLASSES = [\n    \"bare_ground\", \"selective_logging\", \"artisinal_mine\", \"blooming\", \"slash_burn\", \"blow_down\", \"conventional_mine\"\n]","metadata":{"execution":{"iopub.status.busy":"2022-03-09T14:23:19.647205Z","iopub.execute_input":"2022-03-09T14:23:19.647479Z","iopub.status.idle":"2022-03-09T14:23:19.651644Z","shell.execute_reply.started":"2022-03-09T14:23:19.647443Z","shell.execute_reply":"2022-03-09T14:23:19.650982Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Class visualisation\n\nLet's now observe each label invidually. Each image is mapped to a list of labels, with a total of 17 different labels.\nAll 17 labels are \"almost\" independant, meaning that `primary` can be found along with `slash burn`. I said \"almost\" because `cloudy` affects visibility, so that no other label can be found in the same image.\n\nFor sake of clarity, I only displayed one label per image on the figure below. When images are associated to multiple labels, I displayed them multiple times (e.g. `primary` and `haze`).","metadata":{}},{"cell_type":"code","source":"all_tags = list(set(tags))\nN_tags = len(all_tags)\nfig, axes = plt.subplots(4, (N_tags//4)+1, figsize=(20, 20))\nfor idx, tag in enumerate(all_tags):\n    filename = df_class.loc[df_class.tags.str.contains(tag)].image_name.values[0]\n    img = cv2.imread(os.path.join(path_train, filename+\".jpg\"))\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    idx_col = idx // 4\n    idx_row = idx % 4\n    axes[idx_row][idx_col].set_title(tag)\n    axes[idx_row][idx_col].imshow(img)\naxes[1][-1].remove()\naxes[2][-1].remove()\naxes[3][-1].remove()","metadata":{"execution":{"iopub.status.busy":"2022-03-09T14:23:19.653242Z","iopub.execute_input":"2022-03-09T14:23:19.653806Z","iopub.status.idle":"2022-03-09T14:23:22.722449Z","shell.execute_reply.started":"2022-03-09T14:23:19.65377Z","shell.execute_reply":"2022-03-09T14:23:22.720677Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can make few remarks here:\n- Some labels like \"water\" or \"road\" are challenging to differenciate\n- Some rare labels like selecting logging and blooming are also hard to discriminate, and are barely visible at all\n- Strong correlations can be expected between labels like habitation, road and cultivations","metadata":{}},{"cell_type":"markdown","source":"### T-SNE and dimension shrinking for visualisation\n\nT-SNE allows us to cluster our dataset by shrinking the image dimensions to only 2. It is useful for a quick visualisation of our dataset.\n\nScikit-learn documentation:\n> <i>T-distributed Stochastic Neighbor Embedding</i>.\n>\n> <i>t-SNE [1] is a tool to visualize high-dimensional data. It converts similarities between data points to joint probabilities and tries to minimize the Kullback-Leibler divergence between the joint probabilities of the low-dimensional embedding and the high-dimensional data. t-SNE has a cost function that is not convex, i.e. with different initializations we can get different results.</i>","metadata":{}},{"cell_type":"markdown","source":"We need to convert each image to a normalized 1D vector that we can stake onto each other. This matrix will be the input of our TSNE.","metadata":{}},{"cell_type":"code","source":"def load_img(path_file):\n    img = cv2.imread(path_file)\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    img = cv2.resize(img, (100, 100), cv2.INTER_LINEAR).astype(float)\n    img = cv2.normalize(img, None, 0.0, 1.0, cv2.NORM_MINMAX)\n    img = img.reshape(1, -1)\n    return img","metadata":{"execution":{"iopub.status.busy":"2022-03-09T14:23:22.725291Z","iopub.execute_input":"2022-03-09T14:23:22.725628Z","iopub.status.idle":"2022-03-09T14:23:22.732085Z","shell.execute_reply.started":"2022-03-09T14:23:22.725596Z","shell.execute_reply":"2022-03-09T14:23:22.731003Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We only sample 600 images as an estimation of our dataset diversity.","metadata":{}},{"cell_type":"code","source":"filenames = df_class.image_name.sample(600).values\npath_files = [os.path.join(path_train, filename+\".jpg\") for filename in filenames]\nX_train_sample = np.vstack([load_img(path_file) for path_file in path_files])\nX_train_sample.shape","metadata":{"execution":{"iopub.status.busy":"2022-03-09T14:23:22.733438Z","iopub.execute_input":"2022-03-09T14:23:22.733767Z","iopub.status.idle":"2022-03-09T14:23:28.26176Z","shell.execute_reply.started":"2022-03-09T14:23:22.733643Z","shell.execute_reply":"2022-03-09T14:23:28.261015Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tsne = TSNE(\n    n_components=2,\n    init=\"random\",\n    random_state=101,\n    method=\"barnes_hut\",\n    n_iter=500,\n    verbose=2,\n)\nX_embedded = tsne.fit_transform(X_train_sample)\nX_embedded.shape","metadata":{"execution":{"iopub.status.busy":"2022-03-09T14:23:28.263865Z","iopub.execute_input":"2022-03-09T14:23:28.264333Z","iopub.status.idle":"2022-03-09T14:23:53.136905Z","shell.execute_reply.started":"2022-03-09T14:23:28.264294Z","shell.execute_reply":"2022-03-09T14:23:53.136194Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def fetch_img(path_file, h, w):\n    img = cv2.imread(path_file)\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    img = cv2.resize(img, (h*2, w*2), cv2.INTER_LINEAR)\n    return img","metadata":{"execution":{"iopub.status.busy":"2022-03-09T14:23:53.140105Z","iopub.execute_input":"2022-03-09T14:23:53.140323Z","iopub.status.idle":"2022-03-09T14:23:53.147609Z","shell.execute_reply.started":"2022-03-09T14:23:53.140296Z","shell.execute_reply":"2022-03-09T14:23:53.146654Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now that we have the embedding matrix (i.e. `x` and `y` for each image in the 2D space output), we just have to draw a white canva, and place pictures according to their embedding on it. We also need to rescale them based on the canva dimensions.","metadata":{}},{"cell_type":"code","source":"size_img = 1000\noffset_img = 50\nh = w = int(offset_img / 2)\n\nX_scaled = (X_embedded - X_embedded.min(0)) / (X_embedded.max(0) - X_embedded.min(0))\nX_scaled = (X_scaled * size_img).astype(int)\nX_scaled = np.clip(X_scaled, offset_img, size_img-offset_img)\n\nimg_tsne = np.ones((size_img+2*offset_img, size_img+2*offset_img, 3), dtype=np.uint8) * 255\nfor idx in range(X_scaled.shape[0]):\n    x, y = X_scaled[idx][0], X_scaled[idx][1]\n    img = fetch_img(path_files[idx], h, w)\n    img_tsne[x-w:x+w, y-h:y+h, :] = img\n\nfig = plt.figure(figsize=(12, 12))\nplt.imshow(img_tsne);\nplt.axis(\"off\");","metadata":{"execution":{"iopub.status.busy":"2022-03-09T14:23:53.151048Z","iopub.execute_input":"2022-03-09T14:23:53.152703Z","iopub.status.idle":"2022-03-09T14:23:55.0196Z","shell.execute_reply.started":"2022-03-09T14:23:53.15267Z","shell.execute_reply":"2022-03-09T14:23:55.018992Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model training","metadata":{}},{"cell_type":"markdown","source":"### Transformations\n\nFor optimal performances, [resnet18 need input shape that are multiple of 32](https://discuss.pytorch.org/t/input-image-shape-for-resnet18-architecture/80693) and in our case we have input of size 256. From 256, the closest multiple of 32 is 224.\n\nTherefore, we rescale our input data using this multiple, and we also normalize our dataset based on resnet pretrained mean and standard deviation intensity values. `ToTensor()` is useful to normalize our image values from 0-255 range to 0-1 range.","metadata":{}},{"cell_type":"code","source":"def get_transforms():\n    transform_train = T.Compose([\n      T.ToPILImage(),\n      T.Resize(224),\n      T.ToTensor(),\n      T.Normalize(\n          mean=[0.485, 0.456, 0.406],\n          std=[0.229, 0.224, 0.225],\n      )\n    ])\n    transform_val = T.Compose([\n      T.ToPILImage(),\n      T.Resize(224),\n      T.ToTensor(),\n      T.Normalize(\n        mean=[0.485, 0.456, 0.406],\n        std=[0.229, 0.224, 0.225],\n      )\n    ])\n    return transform_train, transform_val","metadata":{"execution":{"iopub.status.busy":"2022-03-09T14:23:55.020745Z","iopub.execute_input":"2022-03-09T14:23:55.02142Z","iopub.status.idle":"2022-03-09T14:23:55.028624Z","shell.execute_reply.started":"2022-03-09T14:23:55.021374Z","shell.execute_reply":"2022-03-09T14:23:55.027993Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We define our custom Dataset class to manipulate batches of data between RAM and Disk more easily. Some point of attentions:\n- `__init__`: we pass the dataframe along with the target, the transformation, the file path and `is_train` flag. It is important to distinguish the training phase from the testing phase because we use test augmentation. <br> Test augmentation (TTA) is helpful to diversify our training dataset and build a more robust model. It is applied on each image for each batch, meaning that is doesn't increase the length of our training dataset per say, but it transforms each image randomly during execution time.\n- `__getitem__`: we define what the dataset return upon iteration. It needs to load both image and target.\n- `collate_fn`: we use this function within the following DataLoader instance. It corresponds to the batch manipulation. This is were `transform` is called. We also proceed to train and test augmentation there.","metadata":{}},{"cell_type":"code","source":"class AmazonDatasetError(Exception):\n    pass\n\n\nclass AmazonDataset(Dataset):\n    def __init__(self, df, ohe_tags, transform, path, is_train=True, idx_tta=None):\n        super().__init__()\n        self.df = df\n        self.ohe_tags = ohe_tags\n        self.transform = transform\n        if isinstance(path, str):\n            self.paths = [path]\n        elif isinstance(path, (list, tuple)):\n            self.paths = path\n        else:\n            raise AmazonDatasetError(f\"Path type must be str, list or tuple, got: {type(path)}\")\n        self.is_train = is_train\n        if not is_train:\n            if not idx_tta in list(range(6)):\n                raise AmazonDatasetError(\n                    f\"In test mode, 'idx_tta' must be an int belonging to [0, 5], got: {repr(idx_tta)}\"\n                )\n            self.idx_tta = idx_tta\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        filename = self.df.iloc[idx].image_name + \".jpg\"\n        for path in self.paths:\n            if filename in os.listdir(path):\n                file_path = os.path.join(path, filename)\n                break\n        else:\n            raise AmazonDatasetError(f\"Can't fetch {filename} among {self.paths}\")\n        img = cv2.imread(file_path)\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        label = self.ohe_tags[idx]\n        return img, label\n\n    def collate_fn(self, batch):\n        imgs, labels = [], []\n        for (img, label) in batch:\n            img = self.custom_augment(img)\n            img = torch.tensor(img)\n            img = img.permute(2, 0, 1)\n            img = self.transform(img)\n            imgs.append(img[None])\n            labels.append(label)\n        imgs = torch.cat(imgs).float().to(device)\n        labels = torch.tensor(labels).float().to(device)\n        return imgs, labels\n\n    def load_img(self, idx, ax=None):\n        img, ohe_label = self[idx]\n        label = self.df.iloc[idx].tags\n        title = f\"{label} - {ohe_label}\"\n        if ax is None:\n            plt.imshow(img)\n            plt.title(title)\n        else:\n            ax.imshow(img)\n            ax.set_title(title)\n    \n    def custom_augment(self, img):\n        \"\"\"\n        Discrete rotation and horizontal flip.\n        Random during training and non random during testing for TTA.\n        Not implemented in torchvision.transforms, hence this function.\n        \"\"\"\n        choice = np.random.randint(0, 6) if self.is_train else self.idx_tta\n        if choice == 0:\n            # Rotate 90\n            img = cv2.rotate(img, rotateCode=cv2.ROTATE_90_CLOCKWISE)\n        if choice == 1:\n            # Rotate 90 and flip horizontally\n            img = cv2.rotate(img, rotateCode=cv2.ROTATE_90_CLOCKWISE)\n            img = cv2.flip(img, flipCode=1)\n        if choice == 2:\n            # Rotate 180\n            img = cv2.rotate(img, rotateCode=cv2.ROTATE_180)\n        if choice == 3:\n            # Rotate 180 and flip horizontally\n            img = cv2.rotate(img, rotateCode=cv2.ROTATE_180)\n            img = cv2.flip(img, flipCode=1)\n        if choice == 4:\n            # Rotate 90 counter-clockwise\n            img = cv2.rotate(img, rotateCode=cv2.ROTATE_90_COUNTERCLOCKWISE)\n        if choice == 5:\n            # Rotate 90 counter-clockwise and flip horizontally\n            img = cv2.rotate(img, rotateCode=cv2.ROTATE_90_COUNTERCLOCKWISE)\n            img = cv2.flip(img, flipCode=1)\n        return img","metadata":{"execution":{"iopub.status.busy":"2022-03-09T14:23:55.029866Z","iopub.execute_input":"2022-03-09T14:23:55.030284Z","iopub.status.idle":"2022-03-09T14:23:55.052238Z","shell.execute_reply.started":"2022-03-09T14:23:55.030248Z","shell.execute_reply":"2022-03-09T14:23:55.051507Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We are now ready for all full data pipeline! We define our target encoder first before, wrapping our custom train and validation dataset within `DataLoaders`, with a batch size of 64 —a good trade-off between RAM usage and speed in our case.","metadata":{}},{"cell_type":"code","source":"def get_data(df_train, df_val):\n\n    encoder = MultiLabelBinarizer()\n    ohe_tags_train = encoder.fit_transform(df_train.list_tags.values)\n    ohe_tags_val = encoder.transform(df_val.list_tags.values)\n\n    transform_train, transform_val = get_transforms()\n    ds_train = AmazonDataset(df_train, ohe_tags_train, transform_train, path=path_train)\n    ds_val = AmazonDataset(df_val, ohe_tags_val, transform_val, path=path_train)\n\n    dl_train = DataLoader(\n      ds_train,\n      batch_size=64,\n      shuffle=True,\n      collate_fn=ds_train.collate_fn\n    )\n    dl_val = DataLoader(\n      ds_val,\n      batch_size=64,\n      shuffle=True,\n      collate_fn=ds_val.collate_fn\n    )\n\n    return ds_train, ds_val, dl_train, dl_val, encoder","metadata":{"execution":{"iopub.status.busy":"2022-03-09T14:23:55.053756Z","iopub.execute_input":"2022-03-09T14:23:55.054013Z","iopub.status.idle":"2022-03-09T14:23:55.066878Z","shell.execute_reply.started":"2022-03-09T14:23:55.053979Z","shell.execute_reply":"2022-03-09T14:23:55.065898Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Sanity check: we expect `imgs` to be a batch of our chosen batch size (64), with 3 channels and of chosen image dimensions (224x224). <br>\n`labels` are also a batch of our chosen size (64) with 17 different classes.","metadata":{}},{"cell_type":"code","source":"df_train, df_val = train_test_split(df_class, test_size=.2)\n\nds_train, ds_val, dl_train, dl_val, encoder = get_data(df_train, df_val)\n\nimgs, labels = next(iter(dl_train))\nimgs.shape, labels.shape","metadata":{"execution":{"iopub.status.busy":"2022-03-09T14:23:55.068211Z","iopub.execute_input":"2022-03-09T14:23:55.068641Z","iopub.status.idle":"2022-03-09T14:24:00.015826Z","shell.execute_reply.started":"2022-03-09T14:23:55.068606Z","shell.execute_reply":"2022-03-09T14:24:00.015105Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ds_train.load_img(5)","metadata":{"execution":{"iopub.status.busy":"2022-03-09T14:24:00.017002Z","iopub.execute_input":"2022-03-09T14:24:00.017276Z","iopub.status.idle":"2022-03-09T14:24:00.282822Z","shell.execute_reply.started":"2022-03-09T14:24:00.017237Z","shell.execute_reply":"2022-03-09T14:24:00.282193Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Model definition","metadata":{}},{"cell_type":"markdown","source":"We download directly weights from the pretrained resnet18, and freeze all weights. We overwrite the last fully connected layer, by adding two dense layers followed by a sigmoid. This `fc` last part is the only layer to be trained.","metadata":{}},{"cell_type":"code","source":"def get_model():\n    model = models.resnet18(pretrained=True)\n    for param in model.parameters():\n        param.require_grad = False\n    model.avgpool = nn.AdaptiveAvgPool2d(output_size=(1, 1))\n    model.fc = nn.Sequential(\n      nn.Flatten(),\n      nn.Linear(512, 128), # 512 for resnet18 or 2048 for resnet 50\n      nn.ReLU(inplace=True),\n      nn.Dropout(.2),\n      nn.Linear(128, 17),\n      nn.Sigmoid()\n    )\n    optimizer = Adam(model.parameters(), lr=1e-4)\n    loss_fn = nn.BCELoss()\n\n    return model.to(device), optimizer, loss_fn","metadata":{"execution":{"iopub.status.busy":"2022-03-09T14:24:00.283869Z","iopub.execute_input":"2022-03-09T14:24:00.284293Z","iopub.status.idle":"2022-03-09T14:24:00.292496Z","shell.execute_reply.started":"2022-03-09T14:24:00.284259Z","shell.execute_reply":"2022-03-09T14:24:00.291797Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training","metadata":{}},{"cell_type":"code","source":"def train_batch(X, Y, model, loss_fn, optimizer):\n    model.train()\n    optimizer.zero_grad()\n    Y_hat = model(X)\n    batch_loss = loss_fn(Y_hat, Y)\n    batch_loss.backward()\n    optimizer.step()\n    Y_hat = Y_hat.detach().float().cpu().numpy()\n    \n    return Y_hat, batch_loss.item()\n\n\n@torch.no_grad()\ndef compute_val_loss(X, Y, model, loss_fn):\n    model.eval()\n    Y_hat = model(X)\n    batch_loss = loss_fn(Y_hat, Y)\n    Y_hat = Y_hat.detach().float().cpu().numpy()\n    \n    return Y_hat, batch_loss.item()","metadata":{"execution":{"iopub.status.busy":"2022-03-09T14:24:00.293849Z","iopub.execute_input":"2022-03-09T14:24:00.29413Z","iopub.status.idle":"2022-03-09T14:24:00.30292Z","shell.execute_reply.started":"2022-03-09T14:24:00.294092Z","shell.execute_reply":"2022-03-09T14:24:00.302188Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We choose to train our model for 21 epochs, while decreasing our learning rate by 10x every 7 batches. We monitor the validation loss are our key metrics. The validation score is just useful as side indication, because we choose the classification threshold quite randomly (0.2). <br>\nLater on, we will find the most adapted threshold for each target.","metadata":{}},{"cell_type":"code","source":"def train_model(dl_train, dl_val, idx_fold):\n    model, optimizer, loss_fn = get_model()\n    lr_scheduler = StepLR(optimizer, step_size=7, gamma=0.1)\n\n    loss_train, loss_val = [], []\n    score_train, score_val = [], []\n\n    Y_hat_val = None\n    best_loss_val = np.inf\n\n    epochs = 14\n    for idx in range(epochs):\n        loss_train_epoch, loss_val_epoch = [], []\n        Y_hat_train_epoch, Y_hat_val_epoch = [], []\n        Y_train_epoch, Y_val_epoch = [], []\n\n        for X, Y in tqdm(dl_train, leave=False):\n            Y_hat, batch_loss = train_batch(X, Y, model, loss_fn, optimizer)\n            loss_train_epoch.append(batch_loss)\n            Y_hat_train_epoch.extend(Y_hat)\n            Y_train_epoch.extend(Y.detach().float().cpu().numpy())\n\n        for X, Y in tqdm(dl_val, leave=False):\n            Y_hat, batch_loss = compute_val_loss(X, Y, model, loss_fn)\n            loss_val_epoch.append(batch_loss)\n            Y_hat_val_epoch.extend(Y_hat)\n            Y_val_epoch.extend(Y.detach().float().cpu().numpy())\n                \n        avg_loss_train = np.mean(loss_train_epoch)\n        avg_loss_val = np.mean(loss_val_epoch)\n\n        Y_hat_train_epoch = np.array(Y_hat_train_epoch)\n        Y_hat_val_epoch = np.array(Y_hat_val_epoch)\n        Y_thresh_train_epoch = (Y_hat_train_epoch > .2).astype(float)\n        Y_thresh_val_epoch = (Y_hat_val_epoch > .2).astype(float)\n        Y_train_epoch = np.array(Y_train_epoch)\n        Y_val_epoch = np.array(Y_val_epoch)\n        \n        score_train_epoch = fbeta_score(Y_train_epoch, Y_thresh_train_epoch, beta=2, average=\"samples\")\n        score_val_epoch = fbeta_score(Y_val_epoch, Y_thresh_val_epoch, beta=2, average=\"samples\")\n               \n        # saving values for debugging\n        if avg_loss_val < best_loss_val:\n            best_loss_val = avg_loss_val\n            Y_hat_val = Y_hat_val_epoch\n            Y_thresh_val = Y_thresh_val_epoch\n            Y_val = Y_val_epoch\n            \n        loss_train.append(avg_loss_train)\n        loss_val.append(avg_loss_val)\n        score_train.append(score_train_epoch)\n        score_val.append(score_val_epoch)\n\n        print(\n            f\"epoch: {idx}/{epochs} -- train loss: {avg_loss_train}, \" \\\n            f\"val loss: {avg_loss_val}\" \\\n            f\" -- train fbeta_score: {score_train_epoch}, \" \\\n            f\"val fbeta_score: {score_val_epoch}\"\n        )\n        \n        lr_scheduler.step()\n\n    train_results = {\n        \"loss_train\": loss_train,\n        \"loss_val\": loss_val,\n        \"score_train\": score_train,\n        \"score_val\": score_val,\n        \"Y_hat_val\": Y_hat_val,\n        \"Y_thresh_val\": Y_thresh_val,\n        \"Y_val\": Y_val,\n    }\n        \n    torch.save(model, f\"resnet18_fold{idx_fold}.pth\")\n    pickle.dump(train_results, open(f\"train_results_fold{idx_fold}.pkl\", \"wb\"))","metadata":{"execution":{"iopub.status.busy":"2022-03-09T14:24:00.304231Z","iopub.execute_input":"2022-03-09T14:24:00.304645Z","iopub.status.idle":"2022-03-09T14:24:00.320713Z","shell.execute_reply.started":"2022-03-09T14:24:00.304608Z","shell.execute_reply":"2022-03-09T14:24:00.319991Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There is a risk that we have created an imbalance within our rare classes, by splitting randomly our dataset between train and validation. Let's ensure that rare classes representation are similar between both datasets.","metadata":{}},{"cell_type":"code","source":"for rare_class in RARE_CLASSES:\n    total_train = df_train.loc[df_train.tags.str.contains(rare_class)].shape[0]\n    total_val = df_val.loc[df_val.tags.str.contains(rare_class)].shape[0]\n    print(f\"train {rare_class}: {100 * total_train / df_train.shape[0]:.4f}% ({total_train})\")\n    print(f\"val {rare_class}: {100 * total_val / df_val.shape[0]:.4f}% ({total_val})\\n\")","metadata":{"execution":{"iopub.status.busy":"2022-03-09T14:24:00.322016Z","iopub.execute_input":"2022-03-09T14:24:00.322741Z","iopub.status.idle":"2022-03-09T14:24:00.469515Z","shell.execute_reply.started":"2022-03-09T14:24:00.3227Z","shell.execute_reply":"2022-03-09T14:24:00.468822Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_model(dl_train, dl_val, 0)","metadata":{"execution":{"iopub.status.busy":"2022-03-09T14:24:00.471118Z","iopub.execute_input":"2022-03-09T14:24:00.471544Z","iopub.status.idle":"2022-03-09T14:43:00.279037Z","shell.execute_reply.started":"2022-03-09T14:24:00.471509Z","shell.execute_reply":"2022-03-09T14:43:00.278107Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = torch.load(\"resnet18_fold0.pth\")\ntrain_results = pickle.load(open(\"train_results_fold0.pkl\", \"rb\"))","metadata":{"execution":{"iopub.status.busy":"2022-03-09T14:43:00.280526Z","iopub.execute_input":"2022-03-09T14:43:00.282321Z","iopub.status.idle":"2022-03-09T14:43:00.326408Z","shell.execute_reply.started":"2022-03-09T14:43:00.282275Z","shell.execute_reply":"2022-03-09T14:43:00.325616Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"loss_train = train_results[\"loss_train\"]\nloss_val = train_results[\"loss_val\"]\nscore_train = train_results[\"score_train\"]\nscore_val = train_results[\"score_val\"]\n\nfig = make_subplots(rows=1, cols=2, subplot_titles=(\"Loss\", \"Fbeta scores\"))\nfig.add_trace(\n    go.Scatter(\n        x=list(range(len(loss_train))),\n        y=loss_train,\n        name=\"loss_train\",\n    ),\n    row=1, col=1\n)\nfig.add_trace(\n    go.Scatter(\n        x=list(range(len(loss_val))),\n        y=loss_val,\n        name=\"loss_val\",\n    ),\n    row=1, col=1\n)\nfig.add_trace(\n    go.Scatter(\n        x=list(range(len(score_train))),\n        y=score_train,\n        name=\"score_train\",\n    ),\n    row=1, col=2\n)\nfig.add_trace(\n    go.Scatter(\n        x=list(range(len(score_val))),\n        y=score_val,\n        name=\"score_val\",\n    ),\n    row=1, col=2\n)\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-09T14:43:00.327668Z","iopub.execute_input":"2022-03-09T14:43:00.327944Z","iopub.status.idle":"2022-03-09T14:43:00.38417Z","shell.execute_reply.started":"2022-03-09T14:43:00.327909Z","shell.execute_reply":"2022-03-09T14:43:00.383331Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Understanding results\n\n### Marginal proba\n\nLet display the average of our Y_hat prediction proba, when the truth is Y = 1 or Y = 0.","metadata":{}},{"cell_type":"code","source":"Y_hat_val = np.array(train_results[\"Y_hat_val\"])\nY_val = np.array(train_results[\"Y_val\"])\n\npos_probas, neg_probas = [], []\nfor class_, idx in encoder._cached_dict.items():\n    pos_probas.append(Y_hat_val[np.where(Y_val[:, idx] != 0), idx].mean())\n    neg_probas.append(Y_hat_val[np.where(Y_val[:, idx] == 0), idx].mean())\ngo.Figure([\n    go.Bar(x=list(encoder._cached_dict), y=pos_probas, name=\"Y_hat proba | Y = 1\"),\n    go.Bar(x=list(encoder._cached_dict), y=neg_probas, name=\"Y_hat proba | Y = 0\")\n]).show()","metadata":{"execution":{"iopub.status.busy":"2022-03-09T14:43:00.385669Z","iopub.execute_input":"2022-03-09T14:43:00.386106Z","iopub.status.idle":"2022-03-09T14:43:00.406183Z","shell.execute_reply.started":"2022-03-09T14:43:00.386067Z","shell.execute_reply":"2022-03-09T14:43:00.405495Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Nice. We see that default thresholding at .2 might not always work. So we need to define the best threshold for each individual classes.","metadata":{}},{"cell_type":"code","source":"def find_best_thresholds(Y_hat, Y):\n    N_tags = Y.shape[1]\n    best_threshs = [0.2] * N_tags\n    resolution = 100\n    for jdx in tqdm(range(N_tags)):\n        best_score = 0\n        #threshs = np.zeros_like(best_threshs)\n        threshs = best_threshs.copy()\n        for kdx in range(resolution):\n            kdx /= resolution\n            threshs[jdx] = kdx\n            Y_hat_thresh = (Y_hat > threshs).astype(float)\n            score = fbeta_score(Y, Y_hat_thresh, beta=2, average=\"samples\")\n            if score > best_score:\n                best_score = score\n                best_threshs[jdx] = kdx\n    \n    global_best_score = fbeta_score(Y, (Y_hat > best_threshs).astype(float), beta=2, average=\"samples\")\n    print(f\"threshs: {best_threshs} -- best score: {global_best_score}\")\n    \n    return best_threshs","metadata":{"execution":{"iopub.status.busy":"2022-03-09T14:43:00.410061Z","iopub.execute_input":"2022-03-09T14:43:00.410304Z","iopub.status.idle":"2022-03-09T14:43:00.417228Z","shell.execute_reply.started":"2022-03-09T14:43:00.410278Z","shell.execute_reply":"2022-03-09T14:43:00.41649Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"threshs = find_best_thresholds(Y_hat_val, Y_val)","metadata":{"execution":{"iopub.status.busy":"2022-03-09T14:43:00.418789Z","iopub.execute_input":"2022-03-09T14:43:00.419301Z","iopub.status.idle":"2022-03-09T14:43:43.527415Z","shell.execute_reply.started":"2022-03-09T14:43:00.419264Z","shell.execute_reply":"2022-03-09T14:43:43.52663Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We improvemed our validaiton score slightly. Now let's observe the score by class.","metadata":{}},{"cell_type":"code","source":"class_scores = {}\nclasses = encoder.classes_\nfor jdx in range(Y_val.shape[1]):\n    y_val = Y_val[:, jdx].ravel()\n    y_hat_val = (Y_hat_val[:, jdx].ravel() > threshs[jdx]).astype(float)\n    score = fbeta_score(y_val, y_hat_val, beta=2)\n    class_scores[classes[jdx]] = round(score, 4)\n\ndf_score = pd.DataFrame(dict(\n    label=list(class_scores.keys()), score=list(class_scores.values()),\n)).sort_values(\"score\", ascending=False)\nfig = px.bar(df_score, x=\"label\", y=\"score\", color=\"score\")\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-09T14:43:43.528786Z","iopub.execute_input":"2022-03-09T14:43:43.52922Z","iopub.status.idle":"2022-03-09T14:43:43.65326Z","shell.execute_reply.started":"2022-03-09T14:43:43.52918Z","shell.execute_reply":"2022-03-09T14:43:43.6525Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Half of our rare classes are below the 50% score. There is room for improvement here. Let's observe all confusion matrices for each classes.","metadata":{}},{"cell_type":"markdown","source":"### Confusion matrixes","metadata":{}},{"cell_type":"code","source":"fig = make_subplots(cols=5, rows=4)\nfor jdx in range(Y_val.shape[1]):\n    y_val = Y_val[:, jdx].ravel()\n    y_hat_val = (Y_hat_val[:, jdx].ravel() > threshs[jdx]).astype(float)\n    tn, fp, fn, tp = confusion_matrix(y_val, y_hat_val).ravel()\n    mat = np.array([[fn, tn], [tp, fp]])\n    col = jdx // 4+1\n    row = jdx % 4+1\n    fig.add_trace(\n        go.Heatmap(\n            z=mat, text=[[f\"fn: {fn}\", f\"tn: {tn}\"], [f\"tp: {tp}\", f\"fp: {fp}\"]], \n            texttemplate=\"%{text}\", colorscale='Viridis', name=encoder.classes_[jdx],\n            showscale=False\n        ),\n        col=col, row=row, \n    )\n    fig.update_xaxes(title=encoder.classes_[jdx], showticklabels=False, row=row, col=col)\n    fig.update_yaxes(showticklabels=False, row=row, col=col)\n    \n\nfig.update_layout(\n    width=1200, height=800, title=\"Confusion matrices\", \n)\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-09T14:43:43.654595Z","iopub.execute_input":"2022-03-09T14:43:43.655021Z","iopub.status.idle":"2022-03-09T14:43:44.419195Z","shell.execute_reply.started":"2022-03-09T14:43:43.654983Z","shell.execute_reply":"2022-03-09T14:43:44.418495Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- `slash_burn` and `blowdown` scores contains more false negative (FN) than false positive (FP)\n- However, fbeta score is more sensitive to FP than FN by design.\n- Because we choose to shuffle our dataset in our dataloader, we can't display the original image of these fp and fn to visually understand our model errors. That's an issue of our implementation. So to furthering analysis, we might return filenames in the `__getitem__` and `collate_fn` methods of our custom Dataset class.","metadata":{}},{"cell_type":"markdown","source":"# Inference\n\nIt's time to check our model real performances!\n\nWe begin by checking the number of test files.","metadata":{}},{"cell_type":"code","source":"!echo $(ls ../input/planets-dataset/planet/planet/test-jpg | wc -l) + $(ls ../input/planets-dataset/test-jpg-additional/test-jpg-additional | wc -l)","metadata":{"execution":{"iopub.status.busy":"2022-03-09T14:43:44.420499Z","iopub.execute_input":"2022-03-09T14:43:44.420886Z","iopub.status.idle":"2022-03-09T14:43:46.872956Z","shell.execute_reply.started":"2022-03-09T14:43:44.42085Z","shell.execute_reply":"2022-03-09T14:43:46.872005Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_test_data(idx_tta):\n    path_test_table = \"../input/planets-dataset/planet/planet\"\n    path_test_file_1 = \"../input/planets-dataset/planet/planet/test-jpg\"\n    path_test_file_2 = \"../input/planets-dataset/test-jpg-additional/test-jpg-additional\"\n    file_count = len(os.listdir(path_test_file_1)) + len(os.listdir(path_test_file_2))\n    df_test = pd.read_csv(os.path.join(path_test_table, \"sample_submission.csv\"))\n    \n    assert df_test.shape[0] == file_count # sanity check\n    \n    ohe_tags_test = np.zeros((df_test.shape[0], 17))\n    _, transform_val = get_transforms()\n    ds_test = AmazonDataset(\n        df_test, ohe_tags_test, transform_val, path=[path_test_file_1, path_test_file_2],\n        is_train=False, idx_tta=idx_tta\n    )\n    dl_test = DataLoader(\n        ds_test, shuffle=False, batch_size=32, collate_fn=ds_test.collate_fn\n    )\n    \n    return dl_test, df_test","metadata":{"execution":{"iopub.status.busy":"2022-03-09T14:43:46.874873Z","iopub.execute_input":"2022-03-09T14:43:46.87519Z","iopub.status.idle":"2022-03-09T14:43:46.883133Z","shell.execute_reply.started":"2022-03-09T14:43:46.875146Z","shell.execute_reply":"2022-03-09T14:43:46.882203Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"@torch.no_grad()\ndef batch_predict(model, X):\n    model.eval()\n    Y = model(X)\n    return Y.detach().float().cpu().numpy()","metadata":{"execution":{"iopub.status.busy":"2022-03-09T14:43:46.88469Z","iopub.execute_input":"2022-03-09T14:43:46.885611Z","iopub.status.idle":"2022-03-09T14:43:46.894249Z","shell.execute_reply.started":"2022-03-09T14:43:46.88557Z","shell.execute_reply":"2022-03-09T14:43:46.893438Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Y_hat_test = []\nfor idx_tta in range(6):\n    Y_hat_test_tta = []\n    dl_test, df_test = get_test_data(idx_tta)\n    for X, _ in tqdm(dl_test):\n        Y_hat_test_batch = batch_predict(model, X)\n        Y_hat_test_tta.extend(Y_hat_test_batch)\n    Y_hat_test.append(Y_hat_test_tta)\nY_hat_test = np.mean(np.array(Y_hat_test), axis=0)\nY_hat_test = (Y_hat_test > threshs).astype(float)","metadata":{"execution":{"iopub.status.busy":"2022-03-09T14:43:46.895642Z","iopub.execute_input":"2022-03-09T14:43:46.896127Z","iopub.status.idle":"2022-03-09T14:44:20.933892Z","shell.execute_reply.started":"2022-03-09T14:43:46.896089Z","shell.execute_reply":"2022-03-09T14:44:20.932489Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Y_hat_test_inv = encoder.inverse_transform(Y_hat_test)\ntest_tags = []\nfor row in Y_hat_test_inv:\n    tags = \" \".join(row)\n    test_tags.append(tags)\n\ndf_test[\"tags\"] = test_tags\ndf_test.to_csv(\"my_sample_submission.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2022-03-09T14:44:20.935213Z","iopub.status.idle":"2022-03-09T14:44:20.935618Z","shell.execute_reply.started":"2022-03-09T14:44:20.935395Z","shell.execute_reply":"2022-03-09T14:44:20.935418Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Previous Results","metadata":{}},{"cell_type":"markdown","source":"| public score | private score | approx rank | model    | split mode            | lr                               | N epochs |\n|--------------|---------------|-------------|----------|-----------------------|----------------------------------|----------|\n| 0.92651      | 0.92455       | 237/936     | resnet50 | simple train test 20% | 1e-4, updated at 5e-5 and 2.5e-5 | 10       |\n| 0.92414      | 0.92244       | 270/936     | resnet18 | simple train test 20% | 1e-4                             | 10       |","metadata":{}}]}