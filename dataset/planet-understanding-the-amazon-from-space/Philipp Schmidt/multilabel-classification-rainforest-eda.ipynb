{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"01068536-7699-64a9-c695-20023c541de2"},"source":"# Who can save the rainforest?\n\nIn this competition we are given a **multilabel classification** problem, where we have to decide, given an image, which labels belong to it. From the evaluation section of the competition:\n\nFor each image listed in the test set, predict a space-delimited list of tags which you believe are associated with the image. There are 17 possible tags: agriculture, artisinal_mine, bare_ground, blooming, blow_down, clear, cloudy, conventional_mine, cultivation, habitation, haze, partly_cloudy, primary, road, selective_logging, slash_burn, water.\n\nIn this notebook we will:\n\n* generate a fun bernoulli trial sample submission\n* look at the actual images to get a first impression of the data.\n\nA standard approach to multilabel classification is to learn as many OVA (one vs all) models as there are distinct labels and then assign labels by the classifier output of each of the models, we'll get to that later.\n\n**If you like it, please upvote this :)**\n\nLet's dive right into the data."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"3e2d6bad-160f-2466-857f-a23bac3252eb"},"outputs":[],"source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom subprocess import check_output\nimport matplotlib.pyplot as plt\nfrom scipy.stats import bernoulli\nimport seaborn as sns\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n#print(check_output([\"ls\", \"../input/train-jpg\"]).decode(\"utf8\"))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"0b35a5f2-9aa1-01e0-62fe-f7fbcd259324"},"outputs":[],"source":"sample = pd.read_csv('../input/sample_submission.csv')\nsample.head()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"28193050-bd95-5ecc-0acd-e407c58b5b19"},"outputs":[],"source":"df = pd.read_csv('../input/train.csv')\ndf.head()"},{"cell_type":"markdown","metadata":{"_cell_guid":"08955642-f7fe-3c1c-68a2-88eb1035baff"},"source":"So, we are given around 40.000 training images."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d4bcd9aa-5156-011b-74d7-084820f6df8e"},"outputs":[],"source":"df.shape"},{"cell_type":"markdown","metadata":{"_cell_guid":"d3d7c9cd-ef05-ff3f-3aee-2bcbe097777e"},"source":"# Tag counts\n\nFirst, let's count all of the tags."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"1eb1b2f8-c1ae-c4bc-102a-9db91c60d71f"},"outputs":[],"source":"all_tags = [item for sublist in list(df['tags'].apply(lambda row: row.split(\" \")).values) for item in sublist]\nprint('total of {} non-unique tags in all training images'.format(len(all_tags)))\nprint('average number of labels per image {}'.format(1.0*len(all_tags)/df.shape[0]))"},{"cell_type":"markdown","metadata":{"_cell_guid":"4b328d0d-9b12-4b96-0471-e1dd31242985"},"source":"Now, lets do the actual counting. We're going to use pandas dataframe groupby method for that. In total, as we found in the description above, there are 17 distinct tags."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"8e499bc7-c231-437d-9bf5-c977c3d901d2"},"outputs":[],"source":"tags_counted_and_sorted = pd.DataFrame({'tag': all_tags}).groupby('tag').size().reset_index().sort_values(0, ascending=False)\ntags_counted_and_sorted.head()"},{"cell_type":"markdown","metadata":{"_cell_guid":"17b7ae1f-6d0a-06f0-9b06-a4b15f57bb5f"},"source":"There are only a few tags, that occur very often in the data:\n\n* primary\n* clear\n* agriculture\n* road\n* and water."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"da7d4cb0-9342-1b01-6544-9fcfecb630d2"},"outputs":[],"source":"tags_counted_and_sorted.plot.barh(x='tag', y=0, figsize=(12,8))"},{"cell_type":"markdown","metadata":{"_cell_guid":"d917a637-7fc7-af23-9ead-4e7ff2d4560e"},"source":"From this tag distribution it will most likely be relatively easy to predict the often occuring tags and comparatively very hard to get the low sampled tags correct."},{"cell_type":"markdown","metadata":{"_cell_guid":"9a65e304-4284-30ba-3ba6-2e4a289323ee"},"source":"# Submission from training tag counts\n\nLet's do something fun. We'll take the training tag distribution and sample from it as a prior for our test data. For that we will configure a bernoulli distribution for each sample with the observed training frequency and sample from that for each test image. With that we'll generate a submission without ever looking at the images. :)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"288bded4-c859-4404-9974-6dbdfa89570e"},"outputs":[],"source":"tag_probas = tags_counted_and_sorted[0].values/tags_counted_and_sorted[0].values.sum()\nindicators = np.hstack([bernoulli.rvs(p, 0, sample.shape[0]).reshape(sample.shape[0], 1) for p in tag_probas])\nindicators = np.array(indicators)\nindicators.shape"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"424a7d1a-801a-654c-0af8-4b9c502fedfa"},"outputs":[],"source":"indicators[:10,:]"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d2bed920-9a5c-275d-f262-9d3d9d7052b9"},"outputs":[],"source":"sorted_tags = tags_counted_and_sorted['tag'].values\nall_test_tags = []\nfor index in range(indicators.shape[0]):\n    all_test_tags.append(' '.join(list(sorted_tags[np.where(indicators[index, :] == 1)[0]])))\nlen(all_test_tags)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"2dfd33fb-261b-282d-dd2b-02a8aca8ef6a"},"outputs":[],"source":"sample['tags'] = all_test_tags\nsample.head()\nsample.to_csv('bernoulli_submission.csv', index=False)"},{"cell_type":"markdown","metadata":{"_cell_guid":"678d5bea-eba1-409e-73c0-1cad672042f5"},"source":"Ok, enough for the fun part, lets get serious :)."},{"cell_type":"markdown","metadata":{"_cell_guid":"b3ccdd2d-51c5-1ba8-79ec-21ddd4477c8c"},"source":"# Looking at the actual images"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"3695b343-5ce4-fe6b-3eee-ce1d788f7383"},"outputs":[],"source":"from glob import glob\nimage_paths = glob('../input/train-jpg/*.jpg')[0:1000]\nimage_names = list(map(lambda row: row.split(\"/\")[-1][:-4], image_paths))\nimage_names[0:10]"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c8ca0894-ca11-70a3-952d-45e84e515b50"},"outputs":[],"source":"plt.figure(figsize=(12,8))\nfor i in range(6):\n    plt.subplot(2,3,i+1)\n    plt.imshow(plt.imread(image_paths[i]))\n    plt.title(str(df[df.image_name == image_names[i]].tags.values))"},{"cell_type":"markdown","metadata":{"_cell_guid":"91b7640a-e004-a986-c825-05089f536e99"},"source":"It seems, that all of the images are of the same size, which would make preprocessing them much easier."},{"cell_type":"markdown","metadata":{"_cell_guid":"c00595fc-2096-3356-e195-0660bfc31c97"},"source":"# Image clustering\n\nWithout having to look at all of the images, a common technique is to cluster images by their native representation (pixel intensities) or some encoded version of it, e.g. by computing activations of a vision-based neural network.\n\nFor our purpose we will just use the pixel intensities and compute pairwise distances."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"29e139a7-af08-f405-2284-1b0aae8c9255"},"outputs":[],"source":"import cv2\n\nn_imgs = 600\n\nall_imgs = []\n\nfor i in range(n_imgs):\n    img = plt.imread(image_paths[i])\n    img = cv2.resize(img, (50, 50), cv2.INTER_LINEAR)\n    img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY).astype('float') / 255.0\n    img = img.reshape(1, -1)\n    all_imgs.append(img)\n\nimg_mat = np.vstack(all_imgs)\nimg_mat.shape"},{"cell_type":"markdown","metadata":{"_cell_guid":"7224bb6a-2ab0-6ca8-d7b5-424c80599c2e"},"source":"We can see frmo the line spectrum in the clustermap, that there are a few images that are very similar to each other using the pixel intensities."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"189431de-a11b-acd3-635f-e0008405e7bc"},"outputs":[],"source":"from scipy.spatial.distance import pdist, squareform\n\nsq_dists = squareform(pdist(img_mat))\nprint(sq_dists.shape)\nsns.clustermap(\n    sq_dists,\n    figsize=(12,12),\n    cmap=plt.get_cmap('viridis')\n)"},{"cell_type":"markdown","metadata":{"_cell_guid":"5e89343f-bcc9-c8d2-efdb-53f54867dfa2"},"source":""},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c4e7dafd-d90d-e28e-c372-ba5fc182b81d"},"outputs":[],"source":""}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":0}