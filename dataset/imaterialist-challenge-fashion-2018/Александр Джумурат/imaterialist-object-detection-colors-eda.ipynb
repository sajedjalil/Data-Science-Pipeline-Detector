{"cells":[{"metadata":{"_uuid":"ef527f58d578d6a83a361634acf38119b31f77c5","_cell_guid":"5c2d4e50-6a4a-4d40-b2f4-5eb2b31a2b2a"},"cell_type":"markdown","source":"\n![](https://theknclan.com/wp-content/uploads/2017/10/635980679147435890-488367249_FashionHeader.png)\n\n# Extensive EDA of iMaterialist (Fashion) Dataset with Object Detection and Color Analysis\n\nThis notebook contains the exploration of iMaterialist Challenge (Fashion) at FGVC5 [dataset](https://www.kaggle.com/c/imaterialist-challenge-fashion-2018)\n\nAbout the iMaterialist (Fashion) Competition - \n\nAs shoppers move online, it would be a dream come true to have products in photos classified automatically. But, automatic product recognition is tough because for the same product, a picture can be taken in different lighting, angles, backgrounds, and levels of occlusion. Meanwhile different fine-grained categories may look very similar, for example, royal blue vs turquoise in color. Many of today’s general-purpose recognition machines simply cannot perceive such subtle differences between photos, yet these differences could be important for shopping decisions.\n\nTackling issues like this is why the Conference on Computer Vision and Pattern Recognition (CVPR) has put together a workshop specifically for data scientists focused on fine-grained visual categorization called the FGVC5 workshop. As part of this workshop, CVPR is partnering with Google, Wish, and Malong Technologies to challenge the data science community to help push the state of the art in automatic image classification.\n\nIn this competition, FGVC workshop organizers with Wish and Malong Technologies challenge you to develop algorithms that will help with an important step towards automatic product detection – to accurately assign attribute labels for fashion images. Individuals/Teams with top submissions will be invited to present their work live at the FGVC5 workshop.  \n\n\n\n\n**Contents**\n\n**1. Descriptive Statistics**  \n    \n    1.1 Counts of Images and Labels  \n    1.2 Top Labels in the dataset  \n    1.3 Most Common Co-occuring Labels  \n    1.4 Images with maxium Labels  \n    1.5 Images with single Label  \n    1.6 Freq Dist of Images in different label count buckets  \n    \n**2. Colors Used in the Images**  \n    \n    2.1 Top Average Color of the images\n    2.2 Dominant Colors present in the images\n    2.3 Common Color Palletes\n    \n**3. Object Detection**\n    \n    3.1 Top Colors Detected in the images\n    3.2 Top Objects Detected in the images"},{"metadata":{"_uuid":"f1f8b429f61feb7c2425f4ce78aac2d3a9fabc16","_cell_guid":"2b988610-89eb-4d0f-a32e-4460252360c7"},"cell_type":"markdown","source":"## Dataset Preparation "},{"metadata":{"_uuid":"bcd29715fcb4f4d134ad0314411e054e4487ec59","_kg_hide-output":true,"_kg_hide-input":true,"_cell_guid":"7973e793-46ce-47e0-bab6-4c0652dc28fa","trusted":true},"cell_type":"code","source":"from IPython.core.display import HTML\nfrom IPython.display import Image\nfrom collections import Counter\nimport pandas as pd \nimport json\n\n\nfrom plotly.offline import init_notebook_mode, iplot\nimport matplotlib.pyplot as plt\nimport plotly.graph_objs as go\nfrom wordcloud import WordCloud\nfrom plotly import tools\nimport seaborn as sns\nfrom PIL import Image\n\nimport tensorflow as tf\nimport numpy as np\n\ninit_notebook_mode(connected=True)\n%matplotlib inline ","execution_count":1,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f3b3a1e2454d76d6468c4c26cfbe6dd0360f5fc7"},"cell_type":"code","source":"!ls","execution_count":3,"outputs":[]},{"metadata":{"_kg_hide-output":true,"_uuid":"1b8ee699cdf9d40b61a95a7fc6000e9687d48a51","_kg_hide-input":true,"_cell_guid":"1a76c5a6-b49e-47eb-a12a-37b7a8c3e137","trusted":true},"cell_type":"code","source":"## read the dataset \n\ntrain_path = '../input/imaterialist-challenge-fashion-2018/train.json'\ntest_path = '../input/imaterialist-challenge-fashion-2018/test.json'\nvalid_path = '../input/imaterialist-challenge-fashion-2018/validation.json'\n\ntrain_inp = open(train_path).read()\ntrain_inp = json.loads(train_inp)\n\ntest_inp = open(test_path).read()\ntest_inp = json.loads(test_inp)\n\nvalid_inp = open(valid_path).read()\nvalid_inp = json.loads(valid_inp)","execution_count":2,"outputs":[]},{"metadata":{"_uuid":"c7452f7f723674521eb44bf4b99e00c9def6b6be","_cell_guid":"6b2e0af3-ffdf-4b91-999f-fd1267909a0a"},"cell_type":"markdown","source":"## 1. Descriptive Statistics\n\n## 1.1 How many Images and how many distinct labels are there in the dataset?"},{"metadata":{"_uuid":"443c50e2b95f2ca514b6a389b3bc4bcf604e7a5a","_kg_hide-input":true,"_cell_guid":"9126a503-7931-471c-b1a9-83574aa99ee9","trusted":false,"collapsed":true},"cell_type":"code","source":"# how many images \ndef get_stats(data):\n    total_images = len(data['images'])\n\n    all_annotations = []\n    if 'annotations' in data:\n        for each in data['annotations']:\n            all_annotations.extend(each['labelId'])\n    total_labels = len(set(all_annotations))\n    return total_images, total_labels, all_annotations\n\ntotal_images, total_labels, train_annotations = get_stats(train_inp)\nprint (\"Total Images in the train:\", total_images)\nprint (\"Total Labels in the train:\", total_labels)\nprint (\"\")\n\ntotal_images, total_labels, test_annotations = get_stats(test_inp)\nprint (\"Total Images in the test:\", total_images)\nprint (\"Total Labels in the test:\", total_labels)\nprint (\"\")\n\ntotal_images, total_labels, valid_annotations = get_stats(valid_inp)\nprint (\"Total Images in the valid:\", total_images)\nprint (\"Total Labels in the valid:\", total_labels)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1f579316bda0134876400b8d0da547cfb4ae7fc4","_cell_guid":"fd59fd68-c93f-45c9-b2aa-30cb15b1e5d8"},"cell_type":"markdown","source":"There are about 1 Million images provided in the train dataset and there are 228 distinct labels which are used to label these images. There are two other sources of data as well - test data and validation data but in thie notebook I have only used images from train dataset.\n\n## 1.2 Which are the top used Labels in the dataset ?"},{"metadata":{"_uuid":"b4648f100090276d7aa525fee9c4dc9da5f0e056","_kg_hide-input":true,"scrolled":false,"_cell_guid":"e33a7250-839d-4d30-9166-65bf77a33af0","trusted":false,"collapsed":true},"cell_type":"code","source":"train_labels = Counter(train_annotations)\n\nxvalues = list(train_labels.keys())\nyvalues = list(train_labels.values())\n\ntrace1 = go.Bar(x=xvalues, y=yvalues, opacity=0.8, name=\"year count\", marker=dict(color='rgba(20, 20, 20, 1)'))\nlayout = dict(width=800, title='Distribution of different labels in the train dataset', legend=dict(orientation=\"h\"));\n\nfig = go.Figure(data=[trace1], layout=layout);\niplot(fig);","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8dab4cedb14941f4ac46cb71071d7f59aafb3ff5","_kg_hide-input":true,"_cell_guid":"90be7fb0-551c-4e6b-86d5-bc3420be0b38","trusted":false,"collapsed":true},"cell_type":"code","source":"valid_labels = Counter(valid_annotations)\n\nxvalues = list(valid_labels.keys())\nyvalues = list(valid_labels.values())\n\ntrace1 = go.Bar(x=xvalues, y=yvalues, opacity=0.8, name=\"year count\", marker=dict(color='rgba(20, 20, 20, 1)'))\nlayout = dict(width=800, title='Distribution of different labels in the valid dataset', legend=dict(orientation=\"h\"));\n\nfig = go.Figure(data=[trace1], layout=layout);\niplot(fig);","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cdc8029332c700bc30ee19a87224f0c5ed43744a","collapsed":true,"_kg_hide-input":true,"_cell_guid":"ee7ff814-a588-4651-ad4c-41d778ce11ab","trusted":false},"cell_type":"code","source":"def get_images_for_labels(labellist, data):\n    image_ids = []\n    for each in data['annotations']:\n        if all(x in each['labelId'] for x in labellist):\n            image_ids.append(each['imageId'])\n            if len(image_ids) == 2:\n                break\n    image_urls = []\n    for each in data['images']:\n        if each['imageId'] in image_ids:\n            image_urls.append(each['url'])\n    return image_urls","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8516dc1796f7b979aaac060fedf70cc3863656f6","_kg_hide-input":true,"_cell_guid":"b00b8167-9a2c-4c00-974a-43dc734e56bd","trusted":false,"collapsed":true},"cell_type":"code","source":"# most common labels \n\ntemps = train_labels.most_common(10)\nlabels_tr = [\"Label-\"+str(x[0]) for x in temps]\nvalues = [x[1] for x in temps]\n\ntrace1 = go.Bar(x=labels_tr, y=values, opacity=0.7, name=\"year count\", marker=dict(color='rgba(120, 120, 120, 0.8)'))\nlayout = dict(height=400, title='Top 10 Labels in the train dataset', legend=dict(orientation=\"h\"));\n\nfig = go.Figure(data=[trace1], layout=layout);\niplot(fig);","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7caddee2c18d5f114517fb6c1062b078b3b397b6","_cell_guid":"4ce22864-8045-41db-b904-c0580a2a6f93"},"cell_type":"markdown","source":"Label 66 is the most used label with almost 750K images tagged with this label in the training dataset"},{"metadata":{"_uuid":"fa1f420b73bae76b42192c4d6e43ff1d1aeacfe3","_cell_guid":"209031cd-7433-46ad-a928-620cff407222","trusted":false,"collapsed":true},"cell_type":"code","source":"temps = valid_labels.most_common(10)\nlabels_vl = [\"Label-\"+str(x[0]) for x in temps]\nvalues = [x[1] for x in temps]\n\ntrace1 = go.Bar(x=labels_vl, y=values, opacity=0.7, name=\"year count\", marker=dict(color='rgba(120, 120, 120, 0.8)'))\nlayout = dict(height=400, title='Top 10 Labels in the valid dataset', legend=dict(orientation=\"h\"));\n\nfig = go.Figure(data=[trace1], layout=layout);\niplot(fig);","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b54dccf0df42f58927e22e6ddb59b1adfce2a6b9","_kg_hide-output":true,"_kg_hide-input":true,"_cell_guid":"9c93a82a-bcbf-42e5-8104-85c4579c2256"},"cell_type":"markdown","source":"Again, in the validation dataset, Label 66 is the most used label but second most label used is label-17 not label-105 of training dataset"},{"metadata":{"_uuid":"eb7366687f30cc4ea04cc19490a8ca0545687e16","_cell_guid":"23160b0b-cc0b-4120-82aa-1e235623921f"},"cell_type":"markdown","source":"## 1.3 What are the most Common Co-Occuring Labels in the dataset\n\nSince every image can be classified into multiple labels, it will be interesting to note which lables have co-occured together"},{"metadata":{"_uuid":"a842fbc6fa760de7d63149861a67abf778368dc1","_kg_hide-input":true,"_cell_guid":"2a73cf6c-c8bc-454f-917d-bc6d8c40840b","trusted":false,"collapsed":true},"cell_type":"code","source":"# Most Commonly Occuring Labels \n\ndef cartesian_reduct(alist):\n    results = []\n    for x in alist:\n        for y in alist:\n            if x == y:\n                continue\n            srtd = sorted([int(x),int(y)])\n            srtd = \" AND \".join([str(x) for x in srtd])\n            results.append(srtd)\n    return results \n\nco_occurance = []\nfor i, each in enumerate(train_inp['annotations']):\n    prods = cartesian_reduct(each['labelId'])\n    co_occurance.extend(prods)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"52135727afb5e93b63e0b096eed0bc4addb0ad55","_kg_hide-input":true,"_cell_guid":"4e2115cb-42b8-493d-a778-e69f6b2957f0","trusted":false,"collapsed":true},"cell_type":"code","source":"coocur = Counter(co_occurance).most_common(10)\nlabels = list(reversed([\"Label: \"+str(x[0]) for x in coocur]))\nvalues = list(reversed([x[1] for x in coocur]))\n\ntrace1 = go.Bar(x=values, y=labels, opacity=0.7, orientation=\"h\", name=\"year count\", marker=dict(color='rgba(130, 130, 230, 0.8)'))\nlayout = dict(height=400, title='Most Common Co-Occuring Labels in the dataset', legend=dict(orientation=\"h\"));\n\nfig = go.Figure(data=[trace1], layout=layout);\niplot(fig);","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e126017764613c9f8f71252a0635c483abf62d51","collapsed":true,"_cell_guid":"af51d714-c1d6-4cac-a0ae-f43541fc0717"},"cell_type":"markdown","source":"From the above graph, (label 66 and label 105) and (label 66 and label 171) have been used most number of times while labelling the images, with the total count of 460K and 445K respectively. Apart from the most frequently occuring label \"66\", label 105 and label 153 have been used repeatedly in the dataset.\n\n## 1.4 Which Images are tagged with Maximum Labels\n\nSome images are labelled with single label but some images can have labels as high as 20. Lets get the images having the largest numbers of labels in the dataset"},{"metadata":{"_uuid":"1065f594debe3214532502ab9eb57bf938f7f378","_kg_hide-input":true,"_cell_guid":"3ca70481-f224-4489-95e5-edb36f3722ad","trusted":false,"collapsed":true},"cell_type":"code","source":"def get_image_url(imgid, data):\n    for each in data['images']:\n        if each['imageId'] == imgid:\n            return each['url']\n\nsrtedlist = sorted(train_inp['annotations'], key=lambda d: len(d['labelId']), reverse=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7dd171bd3b27bf5a64ac05cdae1f7410f54718ac","_kg_hide-output":false,"_kg_hide-input":true,"_cell_guid":"7fa5a40e-1445-4e53-a7f2-b0f3fc577423","trusted":false,"collapsed":true},"cell_type":"code","source":"for img in srtedlist[:5]:\n    iurl = get_image_url(img['imageId'], train_inp)  \n    labelpair = \", \".join(img['labelId'])\n    imghtml = \"\"\"Labels: \"\"\"+ str(labelpair) +\"\"\" &nbsp;&nbsp; <b>Total Labels: \"\"\"+ str(len(img['labelId'])) + \"\"\"</b><br>\"\"\" + \"<img src=\"+iurl+\" width=200px; style='float:left'>\"\n    display(HTML(imghtml))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5cd086ea03137d46973c8f85db823bdd1cdfa4e2","_cell_guid":"34185100-fc53-400c-b576-5f6f1d685377"},"cell_type":"markdown","source":"## 1.5 Which Images have perfect label ie. a Single Label\n\nLets get some of the images which has only one label"},{"metadata":{"_uuid":"fe2cc9aadffba1e94557bbece36200de69531d5f","_kg_hide-output":false,"_kg_hide-input":true,"_cell_guid":"e63b3fe2-9dbc-4da4-967e-98ae058c6985","trusted":false,"collapsed":true},"cell_type":"code","source":"# How many images are labelled with only 1 label \nfor img in srtedlist[-5:]:\n    iurl = get_image_url(img['imageId'], train_inp)  \n    labelpair = \", \".join(img['labelId'])\n    imghtml = \"\"\"<b> Label: \"\"\"+ str(labelpair) +\"\"\"</b><br>\"\"\" + \"<img src=\"+iurl+\" width=200px; height=200px; style='float:left'>\"\n    display(HTML(imghtml))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1dbe4eee0cf261fa80482547eb6251c0aded430a","_cell_guid":"cebbbf4b-d217-4f67-b751-cb72ddfbf426"},"cell_type":"markdown","source":"## 1.6 Frequency Distribution of Images with respective Labels Counts in the dataset\n\nLets visualize how many images are there in each label count bucket. "},{"metadata":{"_uuid":"a74791dc0373c6ee23384ecbddb975ae9bb1bdb8","_cell_guid":"89d66cb5-cdca-49c6-a21c-532588eace13","trusted":false,"collapsed":true},"cell_type":"code","source":"lbldst = Counter([len(x['labelId']) for x in srtedlist])\n\nlabels = list(lbldst.keys())\nvalues = list(lbldst.keys())\n\ntrace1 = go.Bar(x=labels, y=values, opacity=0.7, name=\"year count\", marker=dict(color='rgba(10, 80, 190, 0.8)'))\nlayout = dict(height=400, title='Frequency distribution of images with respective labels counts ', legend=dict(orientation=\"h\"));\n\nfig = go.Figure(data=[trace1], layout=layout);\niplot(fig);","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8b1a31c52d386d31bfd26005cf0f014fde9689a3","_cell_guid":"9a44f49f-f7cf-460e-9fcf-49919074110d"},"cell_type":"markdown","source":"Very few images are there in the dataset having single label, while there is an increasing trend in the number of images with more number of labels. There are no images in the dataset having 22 labels but there are images having 23 labels. \n\n## 2. Colors Used in the Images \n\nIn the e-commerce industry, colors play a very important role in the customer behaviours. Some people are more inclined towards soft colors while some prefer warm colors. In this section, lets visualize what type of colors are used in the images. \n\n## 2.1 Common Average Color of the Images "},{"metadata":{"_uuid":"32e37de659f609957b76bc9b535f1133c7d37d4f","collapsed":true,"_kg_hide-input":true,"_cell_guid":"5a71f987-f73d-40c8-898f-3937656006b5","trusted":false},"cell_type":"code","source":"import urllib\nfrom io import StringIO\n\ndef compute_average_image_color(img):\n    width, height = img.size\n    count, r_total, g_total, b_total = 0, 0, 0, 0\n    for x in range(0, width):\n        for y in range(0, height):\n            r, g, b = img.getpixel((x,y))\n            r_total += r\n            g_total += g\n            b_total += b\n            count += 1\n    return (r_total/count, g_total/count, b_total/count)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"_uuid":"fd1c286110ab4b6177dacba6a3bfbc8e82ea17c2","collapsed":true,"_kg_hide-input":true,"_cell_guid":"0f5e19ab-74a5-4a4e-afba-dcf824333941","trusted":false},"cell_type":"code","source":"import os \nimgpath = '../input/sampleimages/top_images/top_images/'\nread_from_disk = True\n\nif read_from_disk:\n    srtedlist = os.listdir(imgpath)\nelse:\n    srtedlist = sorted(inp['annotations'], key=lambda d: len(d['labelId']), reverse=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a237f9eb532504393411a8d6e480c993d3e8715d","collapsed":true,"_kg_hide-input":true,"_cell_guid":"5690268a-238c-4292-b509-17503a54cbbb","trusted":false},"cell_type":"code","source":"average_colors = {}\nfor img in srtedlist[:10]:\n    if read_from_disk:\n        img = Image.open(imgpath + img)\n    else:\n        iurli = get_image_url(img['imageId'])\n\n        ## download the images \n        # filename = iurli.split(\"/\")[-1].split(\"-large\")[0]\n        # urllib.urlretrieve(iurli, \"top_images/\"+filename)\n        \n        file = cStringIO.StringIO(urllib.urlopen(iurli).read())\n        img = Image.open(img)\n           \n    average_color = compute_average_image_color(img)\n    if average_color not in average_colors:\n        average_colors[average_color] = 0\n    average_colors[average_color] += 1","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"14a19c795ac1bd2303d4d94dc56cc71587d7ddca","_kg_hide-input":true,"_cell_guid":"30f53b7f-63c2-412c-987d-be3b5db06191","trusted":false,"collapsed":true},"cell_type":"code","source":"for average_color in average_colors:\n    average_color1 = (int(average_color[0]),int(average_color[1]),int(average_color[2]))\n    image_url = \"<span style='display:inline-block; min-width:200px; background-color:rgb\"+str(average_color1)+\";padding:10px 10px;'>\"+str(average_color1)+\"</span>\"\n#     print (image_url)\n    display(HTML(image_url))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bb2ac3a1899c1ba90f672834d28b3247e2660a70","_cell_guid":"2bb951c6-d66c-40f4-9bcd-03ba365bd2e1"},"cell_type":"markdown","source":"## 2.2 Most Dominant Colors Used in the Images "},{"metadata":{"_uuid":"c933fb6e9c02d906c37feb3472316755266f160a","_kg_hide-input":true,"_cell_guid":"975a6d0c-0ffe-46ed-af36-37ca41e44b95","trusted":false,"collapsed":true},"cell_type":"code","source":"## top used colors in images \nfrom colorthief import ColorThief\nimport urllib \n\npallets = []\nfor img in srtedlist[:10]:\n    \n    if read_from_disk:\n        img = imgpath + img\n    else:\n        iurli = get_image_url(img['imageId'])\n\n        ## download the images \n        # filename = iurli.split(\"/\")[-1].split(\"-large\")[0]\n        # urllib.urlretrieve(iurli, \"top_images/\"+filename)\n        \n        file = cStringIO.StringIO(urllib.urlopen(iurli).read())\n        img = Image.open(img)\n\n    color_thief = ColorThief(img)\n    dominant_color = color_thief.get_color(quality=1)\n    \n    image_url = \"<span style='display:inline-block; min-width:200px; background-color:rgb\"+str(dominant_color)+\";padding:10px 10px;'>\"+str(dominant_color)+\"</span>\"\n    display(HTML(image_url))\n    \n    palette = color_thief.get_palette(color_count=6)\n    pallets.append(palette)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bb2f0c409ac9cbc9e40e3576e7c02c6c7a37b32b","_cell_guid":"fb146bdb-ca56-44ad-8145-21302ecb513c"},"cell_type":"markdown","source":"## 2.3 Common Color Pallets of the Images"},{"metadata":{"_uuid":"58519381c0c120993b4d639eb4e759b810c85e43","_kg_hide-input":true,"_cell_guid":"b8d21ac0-9be8-4e48-abec-82337428e039","trusted":false,"collapsed":true},"cell_type":"code","source":"for pallet in pallets:\n    img_url = \"\"\n    for pall in pallet:\n        img_url += \"<span style='background-color:rgb\"+str(pall)+\";padding:20px 10px;'>\"+str(pall)+\"</span>\"\n    img_url += \"<br>\"\n    display(HTML(img_url))\n    print \n    ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7b9aa71f07bddb15fddfd6bfcaaa52adbd9771a7","_cell_guid":"7db10b16-9dae-4abd-bba1-71a775684848"},"cell_type":"markdown","source":"## 3. Object Detection using TensorFlow API \n\n\nI have used tensorflow API for object detection the code is given in the following cell.\n\n\n![title](https://github.com/tensorflow/models/raw/master/research/object_detection/g3doc/img/kites_detections_output.jpg)"},{"metadata":{"_uuid":"2221c4e57ef7a494df5b1a61e8b58d8046b9d28f","collapsed":true,"_kg_hide-input":false,"_cell_guid":"4223537b-7a6e-4369-bbf3-5a46acc5ca09","trusted":false},"cell_type":"code","source":"### UNCOMMENT THE FOLLOWING LINE AFTER DOWNLOADING THE UTILS FROM THIS LINK - https://github.com/tensorflow/models/tree/master/research/object_detection/utils\n\n# from utils import label_map_util\n\ndef DOWNLOAD_MODELS():\n    MODEL_NAME = 'ssd_mobilenet_v1_coco_2017_11_17'\n    MODEL_FILE = MODEL_NAME + '.tar.gz'\n    DOWNLOAD_BASE = 'http://download.tensorflow.org/models/object_detection/'\n    PATH_TO_CKPT = MODEL_NAME + '/frozen_inference_graph.pb'\n    PATH_TO_LABELS = os.path.join('data', 'mscoco_label_map.pbtxt')\n    \n    opener = urllib.request.URLopener()\n    opener.retrieve(DOWNLOAD_BASE + MODEL_FILE, MODEL_FILE)\n    tar_file = tarfile.open(MODEL_FILE)\n    for file in tar_file.getmembers():\n        file_name = os.path.basename(file.name)\n        if 'frozen_inference_graph.pb' in file_name:\n            tar_file.extract(file, os.getcwd())\n\ndef detect_object(filename):\n\n    def img2array(img):\n        (img_width, img_height) = img.size\n        return np.array(img.getdata()).reshape((img_width, img_height, 3)).astype(np.uint8)\n\n    categories, probabilities = [], []\n    PATH_TO_CKPT = 'frozen_inference_graph.pb'\n    PATH_TO_LABELS = 'mscoco_label_map.pbtxt'\n    detection_graph = tf.Graph()\n    with detection_graph.as_default():\n        od_graph_def = tf.GraphDef()\n        with tf.gfile.GFile(PATH_TO_CKPT, 'rb') as fid:\n            serialized_graph = fid.read()\n            od_graph_def.ParseFromString(serialized_graph)\n            tf.import_graph_def(od_graph_def, name='')\n\n\n    label_map = label_map_util.load_labelmap(PATH_TO_LABELS)\n    categories = label_map_util.convert_label_map_to_categories(label_map, max_num_classes=100, use_display_name=True)\n    category_index = label_map_util.create_category_index(categories)\n\n    with detection_graph.as_default():\n        with tf.Session(graph=detection_graph) as sess:\n            image_tensor = detection_graph.get_tensor_by_name('image_tensor:0')\n            detection_boxes = detection_graph.get_tensor_by_name('detection_boxes:0')\n            detection_scores = detection_graph.get_tensor_by_name('detection_scores:0')\n            detection_classes = detection_graph.get_tensor_by_name('detection_classes:0')\n            num_detections = detection_graph.get_tensor_by_name('num_detections:0')\n\n            image = Image.open(filename)\n            image_np = img2array(image)\n            image_np_expanded = np.expand_dims(image_np, axis=0)\n            (boxes, scores, classes, num) = sess.run([detection_boxes, detection_scores, detection_classes, num_detections], feed_dict={image_tensor: image_np_expanded})\n            for index,value in enumerate(classes[0]):\n                if float(scores[0,index]) > 0.1:\n                    temp =  category_index.get(value)['name']\n                    if temp not in categories:\n                        categories.append(temp)\n                        probabilities.append(scores[0,index])\n    return categories, probabilities\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"47621e2304534f75d151e16182c2b16dad5c7beb","collapsed":true,"_kg_hide-input":true,"_cell_guid":"f03c9c0b-873d-41f0-8355-452c71043efc","trusted":false},"cell_type":"code","source":"## UNCOMMENT THE FOLLOWING LINES TO RUN THE OBJECT DETECTION MODEL AND SAVE THE RESULTS \n\n# for img in srtedlist[:10]:\n#     iurli = get_image_url(img['imageId'])\n    \n#     file = cStringIO.StringIO(urllib.urlopen(iurli).read())\n#     objects = detect_object(file)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7ae283e11fb78106db7724bf28d1b78124d5d7d3","_cell_guid":"58b0a91b-8061-4d7e-abd2-28c91bffccfe"},"cell_type":"markdown","source":"- Reference: [TensorFlow Object Detection Notebook](https://github.com/tensorflow/models/blob/master/research/object_detection/object_detection_tutorial.ipynb)  \n- Pre-Trained Models Reference: [PreTrained Models](https://github.com/tensorflow/models/tree/676a4f70c20020ed41b533e0c331f115eeffe9a3/research/object_detection)  \n- Link to download the Utils: https://github.com/tensorflow/models/tree/master/research/object_detection/utils"},{"metadata":{"_uuid":"5c6e3c854babec0caa3858d9bb5b2101f71f49c8","_cell_guid":"2917cbc0-941b-4732-aadd-84e8a8b2dba6"},"cell_type":"markdown","source":"Since it would have taken a lot of time on kaggle kernals, I have pre-computed the objects in my local machine."},{"metadata":{"_uuid":"3b91ab41a99253c5f65fc77511789eadfad32508","collapsed":true,"_kg_hide-input":true,"_cell_guid":"bcff6d5e-75b7-48e9-955c-c5308f7e226c","trusted":false},"cell_type":"code","source":"objpath = '../input/precomputedobjects/objects.txt'\n\nobjs = open(objpath).read().strip().split(\"\\n\")\ncolors = [_ for _ in objs if \"color\" in _]\nnon_colors = [_ for _ in objs if \"color\" not in _]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"555c23073483ea9cbe4da0e9e677742fce55ba87","_cell_guid":"53fb49db-aa92-46e0-af08-70e70eb05592"},"cell_type":"markdown","source":"## 3.1 Top Objects detected using Object detection "},{"metadata":{"_uuid":"7668da79698d18ba6b1a20027c2a8a6c215c3452","_kg_hide-input":true,"_cell_guid":"a676f85c-1d81-43a0-9eba-a51ac94e44c3","trusted":false,"collapsed":true},"cell_type":"code","source":"txt = \"\"\nfor i, color in enumerate(Counter(non_colors).most_common(100)):\n    txt += color[0]+\" \"\nwordcloud = WordCloud(max_font_size=50, width=600, height=300).generate(txt)\nplt.figure(figsize=(15,8))\nplt.imshow(wordcloud)\nplt.title(\"Top Objects Detected in the images\", fontsize=15)\nplt.axis(\"off\")\nplt.show() ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"26afc2fc9b893404ad56a55ebb0361f5fdf2ed7d","_cell_guid":"22c818bb-376e-4249-a349-ec01b6c20222"},"cell_type":"markdown","source":"## 3.2 Top Colors Detected in the images"},{"metadata":{"_uuid":"f2d1296b9a522aeaf947df93e3e0e40f75813b6d","_kg_hide-input":true,"_cell_guid":"a93578c5-8d58-4c77-aeb7-87b4fa0ec4bf","trusted":false,"collapsed":true},"cell_type":"code","source":"txt = \"\"\nfor i, color in enumerate(Counter(colors).most_common(100)):\n    txt += (color[0] + \" \")\ntxt = txt.replace(\"color\", \" \")\nwordcloud = WordCloud(max_font_size=50, width=600, height=300, background_color='white').generate(txt)\nplt.figure(figsize=(15,8))\nplt.imshow(wordcloud)\nplt.title(\"Top Colors Used in the images\", fontsize=15)\nplt.axis(\"off\")\nplt.show() ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bc2904e7890dc7571c85eec2dae6f4191188f290","_cell_guid":"63c5788e-70ae-4401-bd89-473cdb5e605a"},"cell_type":"markdown","source":"Thanks for viewing the notebook. Hope You liked it, if liked it please upvote. "}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}