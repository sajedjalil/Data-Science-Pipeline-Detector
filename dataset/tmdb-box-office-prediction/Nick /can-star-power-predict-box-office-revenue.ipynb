{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"#load the data and merge test and train set \n\n# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.\n\ndf_train = pd.read_csv('../input/train.csv')\ndf_test = pd.read_csv('../input/test.csv')\ndf_submission = pd.read_csv ('../input/sample_submission.csv')\n\n\n\n#merge datasets for faster feature engineering \ndf_tmdb = df_train.append(df_test,ignore_index=True)\ndf_tmdb.shape\ndf_tmdb.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_tmdb.tail(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#insert some libraries\nimport json\nimport ast\nfrom pprint import pprint\nimport seaborn as sns \nfrom scipy.stats import norm,skew\nimport matplotlib.pyplot as plt\nimport matplotlib.mlab as mlab\nimport numpy as np\npd.set_option('display.max_columns', None)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def process_homepage(homepage):\n    \"\"\"\n    Returns 0 if homepage is empty and 1 otherwise\n    \"\"\"\n    if pd.isnull(homepage):\n        return 0\n    return 1\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def process_cast(cast):\n    \"\"\"\n    Extracts cast members names and returns string\n    containing these names delimited by ;\n    \"\"\"\n    if pd.isnull(cast):\n        return ''\n    cast_members = ast.literal_eval(cast)\n    members_names = [member['name'] for member in cast_members]\n    return ';'.join(members_names)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def extract_genres(genres, genres_list):\n    \"\"\"\n    Extracts genres from genres json\n    \"\"\"\n    if not pd.isnull(genres):\n        parsed_genres = ast.literal_eval(genres)\n        genres_list.update([genre['name'] for genre in parsed_genres])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def process_row(row, genres_list, df):\n    \"\"\"\n    Processes one row of the dataframe. Extracts genres,\n    fill columns with values for genres of the row, fill\n    homepage with 0 or 1 by the value in default homepage,\n    calculates average movie star power for the row(movie).\n    \"\"\"\n    if pd.isnull(row['genres']):\n        parsed_genres = []\n    else:\n        parsed_genres = ast.literal_eval(row['genres']) # parsed_genres from the json\n    homepage = process_homepage(row['homepage']) # 0 or 1 for the homepage\n    if len(row['cast_names']) > 0:\n        actors_list = row['cast_names'].split(';') # list of actors of the movie\n        total_star_power = 0\n        total_star_power_before = 0\n        for actor in actors_list:\n            appearances = df[df['cast_names'].str.contains(actor)] # only if actor appears in the movie\n            before = appearances[appearances['release_date'] <= row['release_date']] # only if actor apears BEFORE or ON release date of the movie\n            total_star_power += len(appearances)\n            total_star_power_before += len(before)\n        avg_star_power = total_star_power_before / len(actors_list) # calculate average star power BEFORE and ON release\n        avg_star_power_total = total_star_power / len(actors_list) # calculate average star power without taking release date into account\n    else:\n        avg_star_power = 0\n        avg_star_power_total = 0\n    new_row = {'id': row['id'], 'homepage': homepage,\n            'avg_star_power': avg_star_power,\n            'avg_star_power_total': avg_star_power_total}\n    for genre in parsed_genres:\n        new_row[genre['name']] = 1\n    return pd.Series(data=new_row)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_all_genres(df):\n    \"\"\"\n    Returns all genres of the films in dataframe\n    \"\"\"\n    genres_list = set()\n    df['genres'].apply(extract_genres, args=(genres_list,))\n    return genres_list\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def parse_database(df):\n    genres_list = list(get_all_genres(df)) # gets all genres, needed for the column names\n    columns = ['id', 'cast_names', 'avg_star_power', 'avg_star_power_total','homepage']\n    columns.extend(genres_list)\n    parsed_df = pd.DataFrame(columns=columns) # new dataframe that will be outputed\n\n    df['cast_names'] = df['cast'].apply(process_cast) # parse cast members\n\n    parsed_df = df.apply(process_row, axis=1, args=(genres_list, df)) # process rows of the dataframe\n\n    parsed_df = parsed_df.fillna(value=0) # fill NaN with zeros\n    columns.remove('cast_names') # remove parsed cast names\n    parsed_df = parsed_df[columns] # order columns\n\n    return parsed_df\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_tmdb_adjusted = parse_database(df_tmdb)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_tmdb_adjusted.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#drop original column 'homepage'\nfor columns in ['homepage']:\n    df_tmdb.drop(columns, axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#merge columns homepage, different genres, avg_star_power and _avg_star_power_total with df_tmdb\ndf_tmdb = pd.merge(df_tmdb, df_tmdb_adjusted, on='id')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_tmdb.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def process_belongs_to_collection(belongs_to_collection):\n    \"\"\"\n    Returns 0 if belongs_to_collection is empty and 1 otherwise\n    \"\"\"\n    if pd.isnull(belongs_to_collection):\n        return 0\n    return 1\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def process_crew(crew):\n    \"\"\"\n    Extracts crew members names and returns string\n    containing these names delimited by ;\n    \"\"\"\n    if pd.isnull(crew):\n        return ''\n    crew_members = ast.literal_eval(crew)\n    members_names = [member['name'] for member in crew_members]\n    return ';'.join(members_names)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def extract_spoken_languages(spoken_languages, spoken_languages_list):\n    \"\"\"\n    Extracts spoken languages from json\n    \"\"\"\n    if not pd.isnull(spoken_languages):\n        parsed_spoken_languages = ast.literal_eval(spoken_languages)\n        spoken_languages_list.update([spoken_languages['name'] for spoken_languages in parsed_spoken_languages])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def process_row(row, spoken_languages_list, df2):\n    \"\"\"\n    Processes one row of the dataframe. Extracts spoken_languages,\n    fill columns with values for spoken_languages of the row, fill\n    belongs_to_collection with 0 or 1,\n    calculates average crew star power for the row(movie).\n    \"\"\"\n    if pd.isnull(row['spoken_languages']):\n        parsed_spoken_languages = []\n    else:\n        parsed_spoken_languages = ast.literal_eval(row['spoken_languages']) # parsed_spoken_languages from the json\n    belongs_to_collection = process_belongs_to_collection(row['belongs_to_collection']) # 0 or 1 for belongs_to_collection\n    if len(row['crew_names']) > 0:\n        crews_list = row['crew_names'].split(';') # list of crew of the movie\n        total_crew_power = 0\n        total_crew_power_before = 0\n        for crew in crews_list:\n            appearances = df2[df2['crew_names'].str.contains(crew)] # only if crew member appears in the movies\n            before = appearances[appearances['release_date'] <= row['release_date']] # and only on or before release date of the movie\n            total_crew_power += len(appearances)\n            total_crew_power_before += len(before)\n        avg_crew_power = total_crew_power_before / len(crews_list) # calculate average star power BEFORE and ON release\n        avg_crew_power_total = total_crew_power / len(crews_list) # calculate average star power without counting in release date\n    else:\n        avg_crew_power = 0\n        avg_crew_power_total = 0\n    new_row = {'id': row['id'], 'belongs_to_collection': belongs_to_collection,\n            'avg_crew_power': avg_crew_power,\n            'avg_crew_power_total': avg_crew_power_total}\n    for spoken_languages in parsed_spoken_languages:\n        new_row[spoken_languages['name']] = 1\n    return pd.Series(data=new_row)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_all_spoken_languages(df2):\n    \"\"\"\n    Returns all spoken_languages of the films in dataframe\n    \"\"\"\n    spoken_languages_list = set()\n    df2['spoken_languages'].apply(extract_spoken_languages, args=(spoken_languages_list,))\n    return spoken_languages_list","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def parse_database(df2):\n    spoken_languages_list = list(get_all_spoken_languages(df2)) # gets all spoken_languages, needed for the column names\n    columns = ['id', 'crew_names', 'avg_crew_power', 'avg_crew_power_total','belongs_to_collection']\n    columns.extend(spoken_languages_list)\n    parsed_df2 = pd.DataFrame(columns=columns) # new dataframe that will be outputed\n\n    df2['crew_names'] = df2['crew'].apply(process_cast) # parse crew members\n\n    parsed_df2 = df2.apply(process_row, axis=1, args=(spoken_languages_list, df2)) # process rows of the dataframe\n\n    parsed_df2 = parsed_df2.fillna(value=0) # fill NaN with zeros\n    columns.remove('crew_names') # remove parsed cast names\n    parsed_df2 = parsed_df2[columns] # order columns\n\n    return parsed_df2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_tmdb_adjusted2 = parse_database(df_tmdb)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_tmdb_adjusted2.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#drop original column 'belongs_to_collection'\nfor columns in ['belongs_to_collection']:\n    df_tmdb.drop(columns, axis=1, inplace=True)\n   ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#RECODE SPOKENLANGUAGES TO ONLY ENGLISH, MIXED or OTHER. FIRST RECODE ENGLISH TO 100\ndef english(series):\n    if series == 1:\n        return 100\n    else:\n        return series\n    \n\ndf_tmdb_adjusted2['English'] = df_tmdb_adjusted2['English'].apply(english)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_tmdb_adjusted2.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#then sum over all spoken languages\ndf_tmdb_adjusted2['languages']= df_tmdb_adjusted2[list(df_tmdb_adjusted2.columns[4:68])].sum(axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_tmdb_adjusted2.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#then recode into three categories \ndef spoken_languages(series):\n    if series == 100:\n        return 'en'\n    if series > 100:\n        return 'en and other'\n    else:\n        return 'other'\n    \n\ndf_tmdb_adjusted2['spoken_langauages'] = df_tmdb_adjusted2['languages'].apply(spoken_languages)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_tmdb_adjusted2.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_tmdb_adjusted2= df_tmdb_adjusted2[['id', 'avg_crew_power', 'avg_crew_power_total', 'belongs_to_collection','spoken_langauages']]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_tmdb_adjusted2.head(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#merge columns belongs_to_collection, different languagues, avg_crew_power and _avg_crew_power_total with df_tmdb\ndf_tmdb = pd.merge(df_tmdb, df_tmdb_adjusted2, on='id')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#extract Keywords from Json column\ndf_tmdb['Keywords'] = df_tmdb['Keywords'].apply(lambda x: [''] if pd.isna(x) else [str(j['name']) for j in (eval(x))])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def process_production(production_companies):\n    \"\"\"\n    Extracts production_companies and returns string\n    containing these names delimited by ;\n    \"\"\"\n    if pd.isnull(production_companies):\n        return ''\n    production_members = ast.literal_eval(production_companies)\n    production_names = [member['name'] for member in production_members]\n    return ';'.join(production_names)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def process_row(row, df3):\n    \"\"\"\n    Processes one row of the dataframe. Extracts production_companies\n   calculates average production star power for the row(movie).\n    \"\"\"\n    if len(row['production_names']) > 0:\n        production_list = row['production_names'].split(';') # list of production_companies of the movie\n        total_production_power = 0\n        total_production_power_before = 0\n        for production_companies in production_list:\n            appearances = df3[df3['production_names'].str.contains(production_companies)] # only if production company appears in the movie\n            before = appearances[appearances['release_date'] <= row['release_date']] # and only on or before release date of the movie\n            total_production_power += len(appearances)\n            total_production_power_before += len(before)\n        avg_production_power = total_production_power_before / len(production_list) # calculate average production power BEFORE and ON release\n        avg_production_power_total = total_production_power / len(production_list) # calculate average production power without counting in release date\n    else:\n        avg_production_power = 0\n        avg_production_power_total = 0\n    new_row = {'id': row['id'],\n            'avg_production_power': avg_production_power,\n            'avg_production_power_total': avg_production_power_total}\n    return pd.Series(data=new_row)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def parse_database(df3):\n    columns = ['id', 'production_names', 'avg_production_power', 'avg_production_power_total']\n    parsed_df3 = pd.DataFrame(columns=columns) # new dataframe that will be outputed\n\n    df3['production_names'] = df3['production_companies'].apply(process_production) # parse production companies\n\n    parsed_df3 = df3.apply(process_row, axis=1, args=(df3,)) # process rows of the dataframe\n\n    parsed_df3 = parsed_df3.fillna(value=0) # fill NaN with zeros\n    columns.remove('production_names') # remove parsed production names\n    parsed_df3 = parsed_df3[columns] # order columns\n\n    return parsed_df3","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_tmdb_adjusted3 = parse_database(df_tmdb)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_tmdb_adjusted3.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#merge columns avg_production_power and avg_production_power_total with df_tmdb\ndf_tmdb = pd.merge(df_tmdb, df_tmdb_adjusted3, on='id')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_tmdb.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#extract production_countries from Json column\ndf_tmdb['production_countries'] = df_tmdb['production_countries'].apply(lambda x: [''] if pd.isna(x) else [str(j['name']) for j in (eval(x))])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#create column 'production_USA'. 1 is yes, 0 is no\ndf_tmdb['production_USA'] = df_tmdb['production_countries'].apply(lambda x: 1 if 'United States of America' in x else 0)\n","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":false,"trusted":true},"cell_type":"code","source":"df_tmdb.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#calculate number of cast, crew, production companies, production countries, keywords and number of letters/words in original title, title, overview and tagline\n\ndf_tmdb[\"cast_len\"] = df_tmdb.loc[df_tmdb[\"cast\"].notnull(),\"cast\"].apply(lambda x : len(x))\ndf_tmdb[\"crew_len\"] = df_tmdb.loc[df_tmdb[\"crew\"].notnull(),\"crew\"].apply(lambda x : len(x))\ndf_tmdb[\"production_companies_len\"]=df_tmdb.loc[df_tmdb[\"production_companies\"].notnull(),\"production_companies\"].apply(lambda x : len(x))\ndf_tmdb[\"production_countries_len\"]=df_tmdb.loc[df_tmdb[\"production_countries\"].notnull(),\"production_countries\"].apply(lambda x : len(x))\ndf_tmdb[\"keywords_len\"]=df_tmdb.loc[df_tmdb[\"Keywords\"].notnull(),\"Keywords\"].apply(lambda x : len(x))\ndf_tmdb[\"genres_len\"]=df_tmdb.loc[df_tmdb[\"genres\"].notnull(),\"genres\"].apply(lambda x : len(x))\ndf_tmdb['original_title_letter_count'] = df_tmdb['original_title'].str.len() \ndf_tmdb['original_title_word_count'] = df_tmdb['original_title'].str.split().str.len() \ndf_tmdb['title_word_count'] = df_tmdb['title'].str.split().str.len()\ndf_tmdb['overview_word_count'] = df_tmdb['overview'].str.split().str.len()\ndf_tmdb['tagline_word_count'] = df_tmdb['tagline'].str.split().str.len()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#look at missing values \ndf_tmdb.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#replace missing values of new columns with 'len' with mode\nfor column in ['runtime', 'cast_len', 'crew_len', 'production_companies_len', 'genres_len', 'overview_word_count', 'tagline_word_count']:\n    df_tmdb[column].fillna(df_tmdb[column].mode()[0], inplace=True)\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_tmdb.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#describe numeric variables  \ndf_tmdb.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Some movies have a budget of zero. That is hard to believe. Some of these will be looked up and corrected. The movie budgets with 'zero' that can not be found, will be replaced with the mean budget. The same goes for revenue. There are no movies with revenue zero but some of them have revenue 1. All movies with revenue less than $ 1000 will be replaced with the mean revenue. Popularity shows some strange outliers (max = 547, mean is 8.51). These will be looked at and maybe replaced with the mean. Some movies have runtime 0. These will also be replaced with the mean.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_tmdb[df_tmdb.dtypes[(df_tmdb.dtypes==\"float64\")|(df_tmdb.dtypes==\"int64\")]\n                        .index.values].hist(figsize=[20,20])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#all columns lookd skewed. What are the values?\n\ndf_tmdb.skew(axis = 0, skipna = True, numeric_only= True) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#for some skewed columns, I will add a log transform as a feature. First Popularity (skewness 19.96)\n#log transform popularity ( after missing values are)\ndf_tmdb['log_popularity'] = np.log1p(df_tmdb.popularity)\n#skewness down from 14.37 to -2.61\ndf_tmdb[\"log_popularity\"].skew(axis = 0) \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.hist(df_tmdb.log_popularity, bins=20)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#budget. First replace some missing values, found in different kernels.  \n\ndf_tmdb.loc[df_tmdb['id'] == 90,'budget'] = 30000000         # Sommersby          \ndf_tmdb.loc[df_tmdb['id'] == 118,'budget'] = 60000000        # Wild Hogs\ndf_tmdb.loc[df_tmdb['id'] == 149,'budget'] = 18000000        # Beethoven\ndf_tmdb.loc[df_tmdb['id'] == 464,'budget'] = 20000000        # Parenthood\ndf_tmdb.loc[df_tmdb['id'] == 470,'budget'] = 13000000        # The Karate Kid, Part II\ndf_tmdb.loc[df_tmdb['id'] == 513,'budget'] = 930000          # From Prada to Nada\ndf_tmdb.loc[df_tmdb['id'] == 797,'budget'] = 8000000         # Welcome to Dongmakgol\ndf_tmdb.loc[df_tmdb['id'] == 819,'budget'] = 90000000        # Alvin and the Chipmunks: The Road Chip\ndf_tmdb.loc[df_tmdb['id'] == 850,'budget'] = 90000000        # Modern Times\ndf_tmdb.loc[df_tmdb['id'] == 1112,'budget'] = 7500000        # An Officer and a Gentleman\ndf_tmdb.loc[df_tmdb['id'] == 1131,'budget'] = 4300000        # Smokey and the Bandit   \ndf_tmdb.loc[df_tmdb['id'] == 1359,'budget'] = 10000000       # Stir Crazy \ndf_tmdb.loc[df_tmdb['id'] == 1542,'budget'] = 1500000        # All at Once\ndf_tmdb.loc[df_tmdb['id'] == 1542,'budget'] = 15800000       # Crocodile Dundee II\ndf_tmdb.loc[df_tmdb['id'] == 1571,'budget'] = 4000000        # Lady and the Tramp\ndf_tmdb.loc[df_tmdb['id'] == 1714,'budget'] = 46000000       # The Recruit\ndf_tmdb.loc[df_tmdb['id'] == 1721,'budget'] = 17500000       # Cocoon\ndf_tmdb.loc[df_tmdb['id'] == 2268,'budget'] = 17500000       # Madea Goes to Jail budget\ndf_tmdb.loc[df_tmdb['id'] == 2602,'budget'] = 31000000       # Mr. Holland's Opus\ndf_tmdb.loc[df_tmdb['id'] == 2612,'budget'] = 15000000       # Field of Dreams\ndf_tmdb.loc[df_tmdb['id'] == 2696,'budget'] = 10000000       # Nurse 3-D\ndf_tmdb.loc[df_tmdb['id'] == 2801,'budget'] = 10000000       # Fracture\ndf_tmdb.loc[df_tmdb['id'] == 3033,'budget'] = 250 \ndf_tmdb.loc[df_tmdb['id'] == 3051,'budget'] = 50\ndf_tmdb.loc[df_tmdb['id'] == 3084,'budget'] = 337\ndf_tmdb.loc[df_tmdb['id'] == 3224,'budget'] = 4  \ndf_tmdb.loc[df_tmdb['id'] == 3594,'budget'] = 25  \ndf_tmdb.loc[df_tmdb['id'] == 3619,'budget'] = 500  \ndf_tmdb.loc[df_tmdb['id'] == 3831,'budget'] = 3  \ndf_tmdb.loc[df_tmdb['id'] == 3935,'budget'] = 500  \ndf_tmdb.loc[df_tmdb['id'] == 4049,'budget'] = 995946 \ndf_tmdb.loc[df_tmdb['id'] == 4424,'budget'] = 3  \ndf_tmdb.loc[df_tmdb['id'] == 4460,'budget'] = 8  \ndf_tmdb.loc[df_tmdb['id'] == 4555,'budget'] = 1200000 \ndf_tmdb.loc[df_tmdb['id'] == 4624,'budget'] = 30 \ndf_tmdb.loc[df_tmdb['id'] == 4645,'budget'] = 500 \ndf_tmdb.loc[df_tmdb['id'] == 4709,'budget'] = 450 \ndf_tmdb.loc[df_tmdb['id'] == 4839,'budget'] = 7\ndf_tmdb.loc[df_tmdb['id'] == 3125,'budget'] = 25 \ndf_tmdb.loc[df_tmdb['id'] == 3142,'budget'] = 1\ndf_tmdb.loc[df_tmdb['id'] == 3201,'budget'] = 450\ndf_tmdb.loc[df_tmdb['id'] == 3222,'budget'] = 6\ndf_tmdb.loc[df_tmdb['id'] == 3545,'budget'] = 38\ndf_tmdb.loc[df_tmdb['id'] == 3670,'budget'] = 18\ndf_tmdb.loc[df_tmdb['id'] == 3792,'budget'] = 19\ndf_tmdb.loc[df_tmdb['id'] == 3881,'budget'] = 7\ndf_tmdb.loc[df_tmdb['id'] == 3969,'budget'] = 400\ndf_tmdb.loc[df_tmdb['id'] == 4196,'budget'] = 6\ndf_tmdb.loc[df_tmdb['id'] == 4221,'budget'] = 11\ndf_tmdb.loc[df_tmdb['id'] == 4222,'budget'] = 500\ndf_tmdb.loc[df_tmdb['id'] == 4285,'budget'] = 11\ndf_tmdb.loc[df_tmdb['id'] == 4319,'budget'] = 1\ndf_tmdb.loc[df_tmdb['id'] == 4639,'budget'] = 10\ndf_tmdb.loc[df_tmdb['id'] == 4719,'budget'] = 45\ndf_tmdb.loc[df_tmdb['id'] == 4822,'budget'] = 22\ndf_tmdb.loc[df_tmdb['id'] == 4829,'budget'] = 20\ndf_tmdb.loc[df_tmdb['id'] == 4969,'budget'] = 20\ndf_tmdb.loc[df_tmdb['id'] == 5021,'budget'] = 40 \ndf_tmdb.loc[df_tmdb['id'] == 5035,'budget'] = 1 \ndf_tmdb.loc[df_tmdb['id'] == 5063,'budget'] = 14 \ndf_tmdb.loc[df_tmdb['id'] == 5119,'budget'] = 2 \ndf_tmdb.loc[df_tmdb['id'] == 5214,'budget'] = 30 \ndf_tmdb.loc[df_tmdb['id'] == 5221,'budget'] = 50 \ndf_tmdb.loc[df_tmdb['id'] == 4903,'budget'] = 15\ndf_tmdb.loc[df_tmdb['id'] == 4983,'budget'] = 3\ndf_tmdb.loc[df_tmdb['id'] == 5102,'budget'] = 28\ndf_tmdb.loc[df_tmdb['id'] == 5217,'budget'] = 75\ndf_tmdb.loc[df_tmdb['id'] == 5224,'budget'] = 3 \ndf_tmdb.loc[df_tmdb['id'] == 5469,'budget'] = 20 \ndf_tmdb.loc[df_tmdb['id'] == 5840,'budget'] = 1 \ndf_tmdb.loc[df_tmdb['id'] == 5960,'budget'] = 30\ndf_tmdb.loc[df_tmdb['id'] == 6506,'budget'] = 11 \ndf_tmdb.loc[df_tmdb['id'] == 6553,'budget'] = 280\ndf_tmdb.loc[df_tmdb['id'] == 6561,'budget'] = 7\ndf_tmdb.loc[df_tmdb['id'] == 6582,'budget'] = 218\ndf_tmdb.loc[df_tmdb['id'] == 6638,'budget'] = 5\ndf_tmdb.loc[df_tmdb['id'] == 6749,'budget'] = 8 \ndf_tmdb.loc[df_tmdb['id'] == 6759,'budget'] = 50 \ndf_tmdb.loc[df_tmdb['id'] == 6856,'budget'] = 10\ndf_tmdb.loc[df_tmdb['id'] == 6858,'budget'] =  100\ndf_tmdb.loc[df_tmdb['id'] == 6876,'budget'] =  250\ndf_tmdb.loc[df_tmdb['id'] == 6972,'budget'] = 1\ndf_tmdb.loc[df_tmdb['id'] == 7079,'budget'] = 8000000\ndf_tmdb.loc[df_tmdb['id'] == 7150,'budget'] = 118\ndf_tmdb.loc[df_tmdb['id'] == 6506,'budget'] = 118\ndf_tmdb.loc[df_tmdb['id'] == 7225,'budget'] = 6\ndf_tmdb.loc[df_tmdb['id'] == 7231,'budget'] = 85\ndf_tmdb.loc[df_tmdb['id'] == 5222,'budget'] = 5\ndf_tmdb.loc[df_tmdb['id'] == 5322,'budget'] = 90\ndf_tmdb.loc[df_tmdb['id'] == 5350,'budget'] = 70\ndf_tmdb.loc[df_tmdb['id'] == 5378,'budget'] = 10\ndf_tmdb.loc[df_tmdb['id'] == 5545,'budget'] = 80\ndf_tmdb.loc[df_tmdb['id'] == 5810,'budget'] = 8\ndf_tmdb.loc[df_tmdb['id'] == 5926,'budget'] = 300\ndf_tmdb.loc[df_tmdb['id'] == 5927,'budget'] = 4\ndf_tmdb.loc[df_tmdb['id'] == 5986,'budget'] = 1\ndf_tmdb.loc[df_tmdb['id'] == 6053,'budget'] = 20\ndf_tmdb.loc[df_tmdb['id'] == 6104,'budget'] = 1\ndf_tmdb.loc[df_tmdb['id'] == 6130,'budget'] = 30\ndf_tmdb.loc[df_tmdb['id'] == 6301,'budget'] = 150\ndf_tmdb.loc[df_tmdb['id'] == 6276,'budget'] = 100\ndf_tmdb.loc[df_tmdb['id'] == 6473,'budget'] = 100\ndf_tmdb.loc[df_tmdb['id'] == 6842,'budget'] = 30\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#replace budgets < 1000 with mean \nmean = df_tmdb.loc[df_tmdb['budget']<1000, 'budget'].mean()\ndf_tmdb.loc[df_tmdb.budget <1000, 'budget'] = np.nan\ndf_tmdb.fillna(mean,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#log transform budget. Skewness now between -1 and +1\n\ndf_tmdb['log_budget'] = np.log1p(df_tmdb.budget)\ndf_tmdb[\"log_budget\"].skew(axis = 0) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.hist(df_tmdb.log_budget, bins=20)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#replacing some missing revenue values \ndf_tmdb.loc[df_tmdb['id'] == 16,'revenue'] = 171052         # Skinning\ndf_tmdb.loc[df_tmdb['id'] == 117,'revenue'] = 55287687      # Back to 1942\ndf_tmdb.loc[df_tmdb['id'] == 270,'revenue'] = 20018         # Glass: A Portrait of Philip in Twelve Parts \ndf_tmdb.loc[df_tmdb['id'] == 281,'revenue'] = 10655191      # Bats\ndf_tmdb.loc[df_tmdb['id'] == 151,'revenue'] = 18000000      # Windwalker\ndf_tmdb.loc[df_tmdb['id'] == 313,'revenue'] = 11540112      # The Cookout \ndf_tmdb.loc[df_tmdb['id'] == 1282,'revenue'] = 46800000     # Death at a Funeral\ndf_tmdb.loc[df_tmdb['id'] == 451,'revenue'] = 12291275      # Chasing Liberty\ndf_tmdb.loc[df_tmdb['id'] == 1865,'revenue'] = 181185387    # Scooby-Doo 2: Monsters Unleashed\ndf_tmdb.loc[df_tmdb['id'] == 2491,'revenue'] = 6849998      # Never Talk to Strangers\ndf_tmdb.loc[df_tmdb['id'] == 2252,'revenue'] = 51119758     # Bodyguard\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mean = df_tmdb.loc[df_tmdb['revenue']<1000, 'revenue'].mean()\ndf_tmdb.loc[df_tmdb.revenue <1000, 'revenue'] = np.nan\ndf_tmdb.fillna(mean,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#since regression analysis is based on normal distribution, also log transform revenue\ndf_tmdb['log_revenue'] = np.log1p(df_tmdb.revenue)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.hist(df_tmdb.log_revenue, bins=20)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_tmdb.head(1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# also add log of some other features\ndf_tmdb['log_avg_star_power']= np.log1p(df_tmdb.avg_star_power)\ndf_tmdb['log_avg_star_power_total'] = np.log1p(df_tmdb.avg_star_power_total)\ndf_tmdb['log-avg_crew_power'] = np.log1p(df_tmdb.avg_crew_power)\ndf_tmdb['log_avg_crew_power_total'] = np.log1p(df_tmdb.avg_crew_power_total)\ndf_tmdb['log_avg_production_power'] = np.log1p(df_tmdb.avg_production_power)\ndf_tmdb['log_avg_production_power_total'] = np.log1p(df_tmdb.avg_production_power_total)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#log transformation decreased skewness of 'star-, crew, - and production power indices'\ndf_tmdb.skew(axis = 0, skipna = True, numeric_only= True) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Extract month, day and year of the release of the movie \ndf_tmdb[['release_month','release_day','release_year']]=df_tmdb['release_date'].str.split('/',expand=True).replace(np.nan, -1).astype(int)\n\ndf_tmdb.loc[ (df_tmdb['release_year'] <= 19) & (df_tmdb['release_year'] < 100), \"release_year\"] += 2000\ndf_tmdb.loc[ (df_tmdb['release_year'] > 19)  & (df_tmdb['release_year'] < 100), \"release_year\"] += 1900\n\nreleaseDate = pd.to_datetime(df_tmdb['release_date']) \ndf_tmdb['release_month'] = releaseDate.dt.month\ndf_tmdb['release_dayofweek'] = releaseDate.dt.dayofweek\ndf_tmdb['release_quarter'] = releaseDate.dt.quarter\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#drop some unneccessary columns \nfor columns in [\"genres\", \"imdb_id\", \"original_language\", \"original_title\", \"overview\", \"poster_path\", \"production_companies\",\n               \"production_countries\", \"spoken_languages\", \"status\", \"tagline\", \"title\", \"Keywords\", \"cast\", \"crew\",\n               \"cast_names\", \"crew_names\", \"production_names\"]:\n    df_tmdb.drop(columns, axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_tmdb.head(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#also drop revenue since log revenue will be predicted. Calculate expm1 before submission\nfor columns in [\"revenue\"]:\n    df_tmdb.drop(columns, axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#make dummy release_month\ndummies = pd.get_dummies(df_tmdb['release_month']).rename(columns=lambda x: 'release_month' + str(x))\ndf_tmdb = pd.concat([df_tmdb, dummies], axis=1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#dummy release_day\ndday = pd.get_dummies(df_tmdb['release_dayofweek']).rename(columns=lambda x: 'release_dayofweek' + str(x))\ndf_tmdb = pd.concat([df_tmdb, dday], axis=1)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#dummy spoken_languages\nlanguagedum= pd.get_dummies(df_tmdb['spoken_langauages']).rename(columns=lambda x: 'languages' + str(x))\ndf_tmdb = pd.concat([df_tmdb,languagedum], axis=1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#dummy release_quarter\nrq= pd.get_dummies(df_tmdb['release_quarter']).rename(columns=lambda x: 'release_quarter' + str(x))\ndf_tmdb = pd.concat([df_tmdb,rq], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_tmdb.rename(columns={'log-avg_crew_power':'log_avg_crew_power'},inplace=True) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#standardize the features\nfrom sklearn.preprocessing import StandardScaler\ndf = StandardScaler().fit_transform(df_tmdb[['budget', 'popularity', 'runtime', 'avg_star_power', \"avg_star_power_total\", \n                                             \"avg_crew_power\", \"avg_crew_power_total\", \"avg_production_power\", \"avg_production_power_total\",\n                                             \"cast_len\", \"crew_len\", \"production_companies_len\", \"production_countries_len\", \"keywords_len\",\n                                             \"genres_len\", \"original_title_letter_count\", \"original_title_word_count\", \"title_word_count\",\n                                             \"overview_word_count\", \"tagline_word_count\", \"log_popularity\", \"log_budget\", \"release_year\", \n                                             \"log_avg_star_power\", \"log_avg_star_power_total\", \"log_avg_crew_power\", \"log_avg_crew_power_total\", \n                                             \"log_avg_production_power\", \"log_avg_production_power_total\"]])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_tmdb[['budget', 'popularity', 'runtime', 'avg_star_power', \"avg_star_power_total\", \n        \"avg_crew_power\", \"avg_crew_power_total\", \"avg_production_power\", \"avg_production_power_total\",\n        \"cast_len\", \"crew_len\", \"production_companies_len\", \"production_countries_len\", \"keywords_len\",\n        \"genres_len\", \"original_title_letter_count\", \"original_title_word_count\", \"title_word_count\",\n        \"overview_word_count\", \"tagline_word_count\", \"log_popularity\", \"log_budget\", \"release_year\",\n        \"log_avg_star_power\", \"log_avg_star_power_total\", \"log_avg_crew_power\", \"log_avg_crew_power_total\", \n        \"log_avg_production_power\", \"log_avg_production_power_total\"]] = StandardScaler().fit_transform(df_tmdb[['budget',\n        'popularity', 'runtime', 'avg_star_power', \"avg_star_power_total\", \n        \"avg_crew_power\", \"avg_crew_power_total\", \"avg_production_power\", \"avg_production_power_total\",\n        \"cast_len\", \"crew_len\", \"production_companies_len\", \"production_countries_len\", \"keywords_len\",\n        \"genres_len\", \"original_title_letter_count\", \"original_title_word_count\", \"title_word_count\",\n        \"overview_word_count\", \"tagline_word_count\", \"log_popularity\", \"log_budget\", \"release_year\", \n        \"log_avg_star_power\", \"log_avg_star_power_total\", \"log_avg_crew_power\", \"log_avg_crew_power_total\", \n        \"log_avg_production_power\", \"log_avg_production_power_total\"]])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_tmdb.head(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"#delete final columns \nfor columns in [\"id\", \"release_date\", \"release_month\", \"release_day\", \"release_quarter\",\"release_dayofweek\", \"spoken_langauages\"]:\n    df_tmdb.drop(columns, axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_tmdb.to_csv('tmdb.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#end of ETLand feature engineering. Split data back into train and test set \n\ndf_train= df_tmdb.iloc[0:3000] # first 3000 rows of the tmdb dataframe\ndf_test=df_tmdb.iloc[3000:7398]\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#78 features are used to predict box office (log)revenue\ndf_train.shape, df_test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# look at correlation of most important feature to discover multicollinearity\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nplt.figure(figsize=(20,18))\nsns.heatmap(df_train[['log_revenue','log_budget','budget', 'popularity', 'log_popularity', 'avg_star_power',\n          'avg_crew_power','avg_production_power', 'log_avg_star_power', 'log_avg_crew_power', 'log_avg_production_power', \n          'production_USA', 'homepage', 'belongs_to_collection', 'release_year']].corr(), annot=True, fmt='.2', center=0.0, cmap='coolwarm')\n\n#in general, linear or regression models suffer from features that are highly correlated. Since non of the features correlate more than (-)0.5 with each other, all features will be used","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#split data into features and target. All features are either integer, float or dummy.\nfeatures = df_train.select_dtypes(include=['int64', 'float64', 'uint8', 'int8']).columns.tolist()\nfeatures.remove('log_revenue')\nfeatures_unseen = df_test.select_dtypes(include=['int64', 'float64', 'uint8', 'int8']).columns.tolist()\nfeatures_unseen.remove('log_revenue')\n\nX, y = df_train[features], df_train['log_revenue']\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#build simple lineair regression model. \nfrom sklearn import linear_model\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn import datasets, linear_model\n\nlm = linear_model.LinearRegression()\nmodel = lm.fit(X,y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#make predictions\ny_pred=model.predict(X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#check metrics, R_Squared is 0.58, rmse is 1,61\n\nfrom sklearn import metrics\nfrom sklearn.metrics import mean_squared_error\nfrom math import sqrt\nimport numpy as np\n\nrmse = sqrt(mean_squared_error(y, y_pred))\n\n\nprint (\"R-Squared is:\", metrics.r2_score(y, y_pred))\nprint (\"The rmse is:\", rmse)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## The line / model\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nplt.scatter(y, y_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#run linear regression model with test set \n\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n\nprint (X_train.shape, y_train.shape)\nprint (X_test.shape, y_test.shape)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lmA = linear_model.LinearRegression()\n\n#train model using training set \nlmA.fit(X_train, y_train)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#make predictions using the test set \ny_predA = lmA.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## The line / model\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nplt.scatter(y_test, y_predA)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#rmse and R-Squared down to 0.48, RMSE up to 2.32\nrmse = sqrt(mean_squared_error(y_test, y_predA))\n\n\nprint (\"R-Squared is:\", metrics.r2_score(y_test, y_predA))\nprint (\"The rmse is:\", rmse)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#look at actual and predicted values \ncompare = pd.DataFrame({'Actual': y_test, 'Predicted': y_predA})\ncompare.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# look at actual and predicted values of first 50 entries in the dataset\ncompare1 = compare.head(50)\ncompare1.plot(kind='bar',figsize=(30,10))\nplt.grid(which='major', linestyle='-', linewidth='0.5', color='green')\nplt.grid(which='minor', linestyle=':', linewidth='0.5', color='black')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#make prediction on unseen test dataset\nX_unseen=df_test[features_unseen]\nprediction_unseen= lmA.predict(X_unseen)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"prediction_unseen","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_submission['revenue'] = np.expm1(prediction_unseen)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_submission.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_submission[['id','revenue']].to_csv('submission_linear.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"kaggle competitions submit -c tmdb-box-office-prediction -f submission_linear.csv -m \"Message\"","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Random Forest Regressor model\n\n# Import the model\nfrom sklearn.ensemble import RandomForestRegressor\n\n# Instantiate model with 1000 decision trees\n\nrf = RandomForestRegressor(n_estimators = 1000, random_state = 42)\n\n# Train the model on training data\nrf.fit(X, y);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#make prediction\ny_pred2=rf.predict(X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#check metrics, R-Squared 0.93, RMSE 0.79\nrmse = sqrt(mean_squared_error(y, y_pred2))\n\n\nprint (\"R-Squared is:\", metrics.r2_score(y, y_pred2))\nprint (\"The rmse is:\", rmse)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#run Random Forrest Regressor model with test set\n\nX2_train, X2_test, y2_train, y2_test = train_test_split(X, y, test_size=0.3)\nprint (X2_train.shape, y2_train.shape)\nprint (X2_test.shape, y2_test.shape)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# fit a model\n# Import the model\nfrom sklearn.ensemble import RandomForestRegressor\n\n# Instantiate model with 1000 decision trees\n\nrf_test = RandomForestRegressor(n_estimators = 1000, random_state = 42)\n\n# Train the model on training data\nrf_test.fit(X2_train, y2_train);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#make prediction on test set \ny_predC=rf_test.predict(X2_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#rmse and R-Squared down to 0.51, RMSE up to 2.23\nrmse = sqrt(mean_squared_error(y2_test, y_predC))\n\n\nprint (\"R-Squared is:\", metrics.r2_score(y2_test, y_predC))\nprint (\"The rmse is:\", rmse)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#look at actual and predicted values \ncompare = pd.DataFrame({'Actual': y2_test, 'Predicted': y_predC})\ncompare.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# look at actual and predicted values of first 50 entries in the dataset\ncompare1 = compare.head(50)\ncompare1.plot(kind='bar',figsize=(30,10))\nplt.grid(which='major', linestyle='-', linewidth='0.5', color='green')\nplt.grid(which='minor', linestyle=':', linewidth='0.5', color='black')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#make prediction on unseen test dataset\nX_unseen2=df_test[features_unseen]\nprediction_unseen2= rf_test.predict(X_unseen2)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_submission['revenue'] = np.expm1(prediction_unseen2)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_submission.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_submission[['id','revenue']].to_csv('submission_rf.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}