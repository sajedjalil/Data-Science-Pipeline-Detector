{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Look at the big picture ðŸž"},{"metadata":{},"cell_type":"markdown","source":"**Given a set of information about different movies our task is to create an algorithm that predicts the worldwide box office revenue.**\n\nTo make things a bit more fun lets pretend that that we were employeed by a movie producer to see if his next movie is going to be a commercial success ðŸ’¸ðŸ’¸ðŸ’¸"},{"metadata":{},"cell_type":"markdown","source":"**The main goals of our model:**\n\n1. It needs to be accurate. We need the producer to trust our prediction\n\n2. It need to make predictions fast. The producer wants to try many different parameters like genre, actors, etc. to see what gives him the most money"},{"metadata":{},"cell_type":"markdown","source":"**Frame the problem**\n\nThe problem can be expressed as a supervised learning univariate regression task. Meaning we will use a labeled training set from which we take multiple different parameters/features to predict a single value, which is the revenue of the movie"},{"metadata":{},"cell_type":"markdown","source":"*The performance of our model will be measured by using root mean square error, RMSE for short*\n\n![](https://miro.medium.com/max/966/1*lqDsPkfXPGen32Uem1PTNg.png)"},{"metadata":{},"cell_type":"markdown","source":"# Get the data"},{"metadata":{},"cell_type":"markdown","source":"> Load the data we will for training and testing our model"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\n# Load training and test data using pandas\ntrain_data = pd.read_csv('/kaggle/input/tmdb-box-office-prediction/train.csv')\ntest_data = pd.read_csv('/kaggle/input/tmdb-box-office-prediction/test.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> Seems we do not have much data to go around, so we will need to engineer some"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.shape, test_data.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> Lets take a quick look at the data ðŸ§"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Discover and Visualize Data"},{"metadata":{},"cell_type":"markdown","source":"> In order to get some insight from data we need to take a look at them"},{"metadata":{"trusted":true},"cell_type":"code","source":"%matplotlib inline \n\nimport matplotlib.pyplot as plt\n\ntrain_data.drop('id', axis=1).hist(bins=50, figsize=(20, 15))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.drop('id', axis=1).plot(kind='scatter', x='budget', y='popularity', alpha=0.4,\n                                  s=train_data['runtime'], label='runtime', figsize=(10, 7),\n                                  c='revenue', cmap=plt.get_cmap('jet'), colorbar=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> Popularity and budget seem to important in correlation to the revenue"},{"metadata":{"trusted":true},"cell_type":"code","source":"corr_matrix = train_data.corr()\ncorr_matrix['revenue'].sort_values(ascending=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature Engineering"},{"metadata":{},"cell_type":"markdown","source":"> The numerical data available in the dataset are very limited, but using feature engineering it is possible to create new data for the ML-model to train with"},{"metadata":{"trusted":true},"cell_type":"code","source":"from ast import literal_eval\nfrom sklearn.base import BaseEstimator, TransformerMixin\n\nclass TextToDictTransformer(BaseEstimator, TransformerMixin):\n    def __init__(self, features):\n        self.features = features\n    \n    def fit(self, X, y=None):\n        return self\n    \n    def transform(self, X):\n        for column in self.features:\n            X[column] = X[column].apply(lambda x: {} if pd.isna(x) else literal_eval(x))\n        return X","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.base import BaseEstimator, TransformerMixin\n\nclass BooleanTransformer(BaseEstimator, TransformerMixin):\n    '''\n    Transform binary values to boolean\n    '''\n    def __init__(self, features):\n        self.features = features\n    \n    def fit(self, X, y=None):\n        return self\n    \n    def transform(self, X):\n        try:\n            for column in self.features:\n                X[column] = X[column].apply(lambda x: 1 if x != {} and pd.isna(x) == False else 0)\n        except Exception as ex:\n            print(\"Boolean transformer error:\", ex)\n        return X","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.base import BaseEstimator, TransformerMixin\n\nclass OneHotTransformer(BaseEstimator, TransformerMixin):\n    '''\n    One hot objects using the name attribute of the dict\n    '''\n    def __init__(self, features, top_values):\n        self.features = features\n        self.top_values = top_values\n    \n    def fit(self, X, y=None):\n        return self\n    \n    def transform(self, X):\n        try:\n            i = 0\n            for feature in self.features:\n                for name in self.top_values[i]:\n                    X[f'{feature}_{name}'] = X[feature].apply(lambda x: 1 if name in str(x) else 0)\n                i += 1\n                    \n            X = X.drop(self.features, axis=1)\n        except Exception as ex:\n            print(\"One hot tansformer error:\", ex)\n        return X","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from collections import Counter\nfrom sklearn.base import BaseEstimator, TransformerMixin\n    \n\nclass CastTransformer(BaseEstimator, TransformerMixin):\n    '''\n    Create columns for most common names and characters and also the size of the whole cast\n    as well as the size divided to different genders\n    '''\n    def __init__(self, top_cast_names, top_cast_chars):\n        self.top_cast_names = top_cast_names\n        self.top_cast_chars = top_cast_chars\n        \n    def fit(self, X, y=None):\n        return self\n    \n    def transform(self, X):\n        try:\n            X['cast_len'] = X['cast'].apply(lambda x: len(x) if x != {} else 0)\n            \n            for name in self.top_cast_names:\n                X[f'cast_name_{name}'] = X['cast'].apply(lambda x: 1 if name in str(x) else 0)\n                \n            for name in self.top_cast_chars:\n                X[f'cast_char_{name}'] = X['cast'].apply(lambda x: 1 if name in str(x) else 0)\n            \n            X['cast_gender_undef'] = X['cast'].apply(lambda x: sum([1 for i in x if i['gender'] == 0]))\n            X['cast_gender_male'] = X['cast'].apply(lambda x: sum([1 for i in x if i['gender'] == 1]))\n            X['cast_gender_female'] = X['cast'].apply(lambda x: sum([1 for i in x if i['gender'] == 2]))\n            \n            X = X.drop('cast', axis=1)\n        except Exception as ex:\n            print(\"Cast transformer error:\", ex)\n        return X","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from collections import Counter\nfrom sklearn.base import BaseEstimator, TransformerMixin\n\n\nclass CrewTransformer(BaseEstimator, TransformerMixin):\n    '''\n    Create columns for most common names, jobs and departments and also the size of the whole cast\n    as well as the size divided to different genders\n    '''\n    def __init__(self, top_crew_names, top_crew_jobs, top_crew_departments):\n        self.top_crew_names = top_crew_names\n        self.top_crew_jobs = top_crew_jobs\n        self.top_crew_departments = top_crew_departments\n    \n    def fit(self, X, y=None):\n        return self\n    \n    def transform(self, X):\n        try:\n            X['crew_len'] = X['crew'].apply(lambda x: len(x) if x != {} else 0)\n            \n            for name in self.top_crew_names:\n                X[f'crew_name_{name}'] = X['crew'].apply(lambda x: 1 if name in str(x) else 0)\n                \n            for name in self.top_crew_jobs:\n                X[f'crew_job_{name}'] = X['crew'].apply(lambda x: 1 if name in str(x) else 0)\n                \n            for name in self.top_crew_departments:\n                X[f'crew_department_{name}'] = X['crew'].apply(lambda x: 1 if name in str(x) else 0)\n            \n            X['crew_gender_undef'] = X['crew'].apply(lambda x: sum([1 for i in x if i['gender'] == 0]))\n            X['crew_gender_male'] = X['crew'].apply(lambda x: sum([1 for i in x if i['gender'] == 1]))\n            X['crew_gender_female'] = X['crew'].apply(lambda x: sum([1 for i in x if i['gender'] == 2]))\n            \n            X = X.drop('crew', axis=1)\n        except Exception as ex:\n            print(\"Crew transformer error:\", ex)\n        return X","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.base import BaseEstimator, TransformerMixin\n\nclass DateTransformer(BaseEstimator, TransformerMixin):        \n    '''\n    Breaks date to 3 different values for year, month and day\n    '''\n    def fit(self, X, y=None):\n        return self\n    \n    def transform(self, X):\n        try:            \n            X['year'] = pd.Series(pd.DatetimeIndex(X['release_date']).year)\n            X['month'] = pd.Series(pd.DatetimeIndex(X['release_date']).month)\n            X['day'] = pd.Series(pd.DatetimeIndex(X['release_date']).day)\n            X = X.drop('release_date', axis=1)\n        except Exception as ex:\n            print(\"Date transformer pipeline error:\", ex)\n        return X","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.base import BaseEstimator, TransformerMixin\n\nclass FixRevenueTransformer(BaseEstimator, TransformerMixin):\n    '''\n    Fix some values that are problematic\n    '''\n    def fit(self, X, y=None):\n        return self\n    \n    def transform(self, X):\n        try:\n            X.loc[X['id'] == 16,'revenue'] = 192864          # Skinning\n            X.loc[X['id'] == 90,'budget'] = 30000000         # Sommersby          \n            X.loc[X['id'] == 118,'budget'] = 60000000        # Wild Hogs\n            X.loc[X['id'] == 149,'budget'] = 18000000        # Beethoven\n            X.loc[X['id'] == 313,'revenue'] = 12000000       # The Cookout \n            X.loc[X['id'] == 451,'revenue'] = 12000000       # Chasing Liberty\n            X.loc[X['id'] == 464,'budget'] = 20000000        # Parenthood\n            X.loc[X['id'] == 470,'budget'] = 13000000        # The Karate Kid, Part II\n            X.loc[X['id'] == 513,'budget'] = 930000          # From Prada to Nada\n            X.loc[X['id'] == 797,'budget'] = 8000000         # Welcome to Dongmakgol\n            X.loc[X['id'] == 819,'budget'] = 90000000        # Alvin and the Chipmunks: The Road Chip\n            X.loc[X['id'] == 850,'budget'] = 90000000        # Modern Times\n            X.loc[X['id'] == 1112,'budget'] = 7500000        # An Officer and a Gentleman\n            X.loc[X['id'] == 1131,'budget'] = 4300000        # Smokey and the Bandit   \n            X.loc[X['id'] == 1359,'budget'] = 10000000       # Stir Crazy \n            X.loc[X['id'] == 1542,'budget'] = 1              # All at Once\n            X.loc[X['id'] == 1570,'budget'] = 15800000       # Crocodile Dundee II\n            X.loc[X['id'] == 1571,'budget'] = 4000000        # Lady and the Tramp\n            X.loc[X['id'] == 1714,'budget'] = 46000000       # The Recruit\n            X.loc[X['id'] == 1721,'budget'] = 17500000       # Cocoon\n            X.loc[X['id'] == 1865,'revenue'] = 25000000      # Scooby-Doo 2: Monsters Unleashed\n            X.loc[X['id'] == 2268,'budget'] = 17500000       # Madea Goes to Jail budget\n            X.loc[X['id'] == 2491,'revenue'] = 6800000       # Never Talk to Strangers\n            X.loc[X['id'] == 2602,'budget'] = 31000000       # Mr. Holland's Opus\n            X.loc[X['id'] == 2612,'budget'] = 15000000       # Field of Dreams\n            X.loc[X['id'] == 2696,'budget'] = 10000000       # Nurse 3-D\n            X.loc[X['id'] == 2801,'budget'] = 10000000       # Fracture\n            X.loc[X['id'] == 3889,'budget'] = 15000000       # Colossal\n            X.loc[X['id'] == 6733,'budget'] = 5000000        # The Big Sick\n            X.loc[X['id'] == 3197,'budget'] = 8000000        # High-Rise\n            X.loc[X['id'] == 6683,'budget'] = 50000000       # The Pink Panther 2\n            X.loc[X['id'] == 5704,'budget'] = 4300000        # French Connection II\n            X.loc[X['id'] == 6109,'budget'] = 281756         # Dogtooth\n            X.loc[X['id'] == 7242,'budget'] = 10000000       # Addams Family Values\n            X.loc[X['id'] == 7021,'budget'] = 17540562       #  Two Is a Family\n            X.loc[X['id'] == 5591,'budget'] = 4000000        # The Orphanage\n            X.loc[X['id'] == 4282,'budget'] = 20000000       # Big Top Pee-wee\n\n            power_six = X.id[X.budget > 1000][X.revenue < 100]\n\n            for k in power_six :\n                X.loc[X['id'] == k,'revenue'] =  X.loc[X['id'] == k,'revenue'] * 1000000\n                \n            return X\n        \n        except Exception as ex:\n            print(\"Fix revenue transformer error:\", ex)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from collections import Counter\nfrom sklearn.base import BaseEstimator, TransformerMixin\n\nclass DropFeaturesTransformer(BaseEstimator, TransformerMixin):\n    '''\n    Drop unwanted features\n    '''\n    def __init__(self, features):\n        self.features = features\n    \n    def fit(self, X, y=None):\n        return self\n    \n    def transform(self, X):\n        try:\n            return X.drop(self.features, axis=1)\n        except Exception as ex:\n            print(\"Drop features transformer error:\", ex)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.base import BaseEstimator, TransformerMixin\n\nclass TrainTestTransformer(BaseEstimator, TransformerMixin):        \n    def __init__(self, impute=False, normalize=False):\n        self.impute = impute\n        self.normalize = normalize\n        \n    def fit(self, X, y=None):\n        \n        if self.impute:\n            X = X.fillna(X.median())\n    \n        self.X = X.drop('revenue', axis=1)    \n        self.y = X['revenue']\n        \n        if self.normalize:\n            self.X = MinMaxScaler().fit_transform(self.X)\n        \n        return self\n    \n    def transform(self, X):\n        return train_test_split(self.X, self.y, test_size=0.10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def top_values(X, column, attribute):\n    '''\n    Find the most common values for a column in the dataset\n    '''\n    try:\n        values = X[column].apply(lambda x: [i[attribute] for i in x] if x != {} else []).values\n        top_values = Counter([j for i in values for j in i]).most_common(30)\n        top_values = [i[0] for i in top_values]\n        return top_values\n    except Exception as ex:\n        print(ex)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Prepare Data for Machine Learning algorithms"},{"metadata":{},"cell_type":"markdown","source":"> Creating pipelines will allow us to handle data easily and make model testing and selection easier"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import train_test_split\n\n\ntext_to_dict = ['belongs_to_collection', 'genres', 'production_companies',\n                'production_countries', 'spoken_languages', 'Keywords', 'cast', 'crew']\n\nboolean_features = ['homepage', 'belongs_to_collection']\n\none_hot_objects = ['genres', 'production_countries', 'spoken_languages', 'production_companies']\n\ndrop_features = ['id',\n                'original_language',\n                'Keywords',\n                'imdb_id',\n                'status',\n                'poster_path', \n                'original_title',\n                'overview',\n                'tagline',\n                'title'\n                ]\n\n\nengineered_data = TextToDictTransformer(text_to_dict).transform(train_data.copy())\n\none_hot_top_values = [top_values(engineered_data, i, 'name')  for i in one_hot_objects]\n\nfeature_engineering_pipeline = Pipeline([\n    ('boolean_transformer', BooleanTransformer(boolean_features)), \n    ('one_hot_transformer', OneHotTransformer(one_hot_objects, one_hot_top_values)),\n    ('date_transformer', DateTransformer()),\n    ('cast_transformer', CastTransformer(top_values(engineered_data, 'cast', 'name'),\n                                         top_values(engineered_data, 'cast', 'character'))),\n    ('crew_transformer', CrewTransformer(top_values(engineered_data, 'crew', 'name'),\n                                         top_values(engineered_data, 'crew', 'job'),\n                                         top_values(engineered_data, 'crew', 'department'))),\n    ('fix_revenue_transformer', FixRevenueTransformer()),\n    ('drop_features_transformers', DropFeaturesTransformer(drop_features)),\n])\n\nengineered_data = feature_engineering_pipeline.fit_transform(engineered_data)\nX_train, X_valid, y_train, y_valid = TrainTestTransformer(impute=True, normalize=False).fit_transform(engineered_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('X_train', X_train.shape)\nprint('y_train', y_train.shape)\nprint('X_test', X_valid.shape)\nprint('y_train', y_valid.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Select and Train a Model"},{"metadata":{},"cell_type":"markdown","source":"> Now that the pipeline is ready and the data are prepared, it is time to train our regression models and select the best on predicting movie revenues"},{"metadata":{},"cell_type":"markdown","source":"**Linear regression model**"},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_data = X_valid[:]\nsample_labels = np.log1p(y_valid[:])\n\n\nnum_models = 5\nlin_reg_models = []\ntree_reg_models = []\nforest_reg_models = []","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\n\nfor i in range(num_models):\n    lin_reg = LinearRegression()\n    lin_reg.fit(X_train, np.log1p(y_train))\n\n    preds = lin_reg.predict(sample_data)\n    lin_mse = mean_squared_error(sample_labels, preds)\n    lin_rmse = np.sqrt(lin_mse)\n    \n    lin_reg_models.append((lin_reg, lin_rmse))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Decision Tree model**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeRegressor\nfrom sklearn.metrics import mean_squared_error\n\nfor i in range(num_models):\n    tree_reg = DecisionTreeRegressor()\n    tree_reg.fit(X_train, np.log1p(y_train))\n    \n    preds = tree_reg.predict(sample_data)\n    tree_mse = mean_squared_error(sample_labels, preds)\n    tree_rmse = np.sqrt(tree_mse)\n    \n    tree_reg_models.append((tree_reg, tree_rmse))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Random Forest Regressor**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error\n\nfor i in range(num_models):\n    forest_reg = RandomForestRegressor(n_estimators=100)\n    forest_reg.fit(X_train, np.log1p(y_train))\n\n    preds = forest_reg.predict(sample_data)\n    forest_mse = mean_squared_error(sample_labels, preds)\n    forest_rmse = np.sqrt(forest_mse)\n    \n    forest_reg_models.append((forest_reg, forest_rmse))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"res = pd.DataFrame({'Linear': [i[1] for i in lin_reg_models], 'Tree': [i[1] for i in tree_reg_models], 'Forest': [i[1] for i in forest_reg_models]})\nres","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# The best model is \"Random Forest\""},{"metadata":{},"cell_type":"markdown","source":"> The best model is produced using random forest, now lets use grid search to look check different hyperparameters so we can get even better results"},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\n\ndef plot_feature_importance(feature_columns, importance_values):\n    feature_imp = [col for col in zip(feature_columns, importance_values)]\n    feature_imp.sort(key=lambda x:x[1], reverse=True)\n\n    imp = pd.DataFrame(feature_imp[0:20], columns=['feature', 'importance'])\n    plt.figure(figsize=(10, 8))\n    sns.barplot(y='feature', x='importance', data=imp)\n    plt.title('20 Most Important Features', fontsize=16)\n    plt.ylabel(\"Feature\", fontsize=16)\n    plt.xlabel(\"\")\n    plt.show()\n    return imp","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get the best model\nbest_forest_model, best_forest_model_rmse = sorted(forest_reg_models, key=lambda x: x[1])[0]\nmodel_importances = plot_feature_importance(X_train.columns, best_forest_model.feature_importances_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Grid Search**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\n\nparam_grid = [\n    {'bootstrap': [False], 'n_estimators': [200, 250, 300], 'max_features': [60, 80, 100]},\n    {'oob_score': [True, False], 'n_estimators': [150, 180, 200], 'max_features': [40, 50, 60]},\n]\n\ngrid_search = GridSearchCV(RandomForestRegressor(), param_grid, cv=10, scoring='neg_mean_squared_error', return_train_score=True)\ngrid_search.fit(X_train, np.log1p(y_train))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grid_search.best_estimator_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grid_search_model = grid_search.best_estimator_\n\npreds = grid_search_model.predict(sample_data)\ngrid_mse = mean_squared_error(sample_labels, preds)\ngrid_rmse = np.sqrt(forest_mse)\ngrid_rmse","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"importances = plot_feature_importance(X_train.columns, grid_search_model.feature_importances_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> Compare our best forest model with the results of grid search, and use the best model"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Random Forest model selected' if best_forest_model_rmse < grid_rmse else 'Grid search model selected')\nmodel = best_forest_model if best_forest_model_rmse < grid_rmse else grid_search_model","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*Note: The rmse is lower because we re-trained on the whole training set and that means we overfit the training data, which is not good. But this new training will come handy when making predictions on the test set*"},{"metadata":{"trusted":true},"cell_type":"code","source":"X, y = X_train.append(X_valid), y_train.append(y_valid)\n\nmodel.fit(X, np.log1p(y))\n\npreds = model.predict(sample_data)\nmodel_mse = mean_squared_error(sample_labels, preds)\nmodel_rmse = np.sqrt(model_mse)\nmodel_rmse","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Predictions"},{"metadata":{},"cell_type":"markdown","source":"> Create the submission file for the competition"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data = TextToDictTransformer(text_to_dict).transform(test_data.copy())\nengineered_test_data = feature_engineering_pipeline.fit_transform(test_data.copy())\n\netg = engineered_test_data.drop('revenue', axis=1)\netg = etg.fillna(etg.median())\n\npreds = model.predict(etg)\n\nsubmission = test_data.copy()\n\nsubmission['revenue'] = np.expm1(preds)\nsubmission['revenue'] = submission['revenue'].astype(str)\n\nsubmission[['id', 'title', 'budget', 'popularity', 'revenue']].head(50)\nsubmission[['id', 'revenue']].to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}