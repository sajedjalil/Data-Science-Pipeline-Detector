{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":false},"cell_type":"markdown","source":"# In Parameter Space, No One Can Hear You Scream\n\nI will try to keep it compact for readibility and visibility. I will also not use any external data and avoid too much parameter tuning.\n\nFor EDA, I took a lot of inspiration from Kernels Grandmaster Andrew Lukyanenko: https://www.kaggle.com/artgor/eda-feature-engineering-and-model-interpretation\n\n_Latest updates:_\n* Baseline model (linear regression revenue=f(budget)): **2.67065**\n* Random Forest model (no textual data, maxdepth tuning): **2.56128** (baseline model does almost as well!)\n* CatBoost vanilla model (no textual data, no tuning): **1.97972**"},{"metadata":{"trusted":true,"_uuid":"490fbc37301d5df9bb8100b3e76e5383d8c5fdbd"},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nfrom collections import Counter\n#from collections import deque\n\nimport matplotlib.pyplot as plt\nfrom matplotlib import cm\nfrom mpl_toolkits.mplot3d import Axes3D\nplt.style.use('ggplot')\n%matplotlib inline\n\nfrom wordcloud import WordCloud\nimport plotly as py\nimport plotly.graph_objs as go\nfrom plotly import tools\nimport seaborn as sns\n\nimport ast                               # ast.literal_eval() to reformat strings into dictionaries\nfrom urllib.request import urlopen\nfrom PIL import Image                    # display jpg files\n\nimport nltk\n#nltk.download('stopwords')\n#nltk.download('punkt')\n#from nltk.corpus import stopwords\n#from nltk.tokenize import word_tokenize\n#from nltk.stem import PorterStemmer\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n#from sklearn.model_selection import GridSearchCV\n#from sklearn.model_selection import RandomizedSearchCV\nfrom catboost import CatBoostRegressor","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"115fa1df38a114939525373d0a7ad652110d643a"},"cell_type":"code","source":"train = pd.read_csv('../input/train.csv')\ntest = pd.read_csv('../input/test.csv')\ntrain.shape, test.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"68de88aec4c302ef7475b2f6da036e28e1a8126a"},"cell_type":"markdown","source":"## I. Exploratory Data Analysis (EDA)\n\n### a. Sneak peak into the data"},{"metadata":{"trusted":true,"_uuid":"cf989e0dc8a442edc55490261bf8f34da5b1b994"},"cell_type":"code","source":"train.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"12655d7cbfb62a839aec699dda8ee1085b12a074"},"cell_type":"code","source":"# sanity check: target column\nx = list(train.columns.values)\ny = list(test.columns.values)\n[item for item in x if item not in y]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"560bc5d229271451747e2e8bf03eb2caa2759cbb"},"cell_type":"code","source":"train.head(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5bcb765e68ae5405acd35be7ee6942ebd1548298"},"cell_type":"code","source":"fig = plt.figure(figsize=(15, 5))\nplt.title(\"Distribution of NAN values\")\ntrain.isna().sum().sort_values(ascending = True).plot(kind = 'barh')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8babefd479d8fb7805cadcc1579592c235f4267d"},"cell_type":"markdown","source":"A priori a stimulating problem:\n- we only have 3000 samples for training while we need to test for c. 4400 test samples;\n- we must predict revenue (regression problem) based on a bunch of feature types:\n    - numerical data;\n    - categorical data (e.g. genre, keywords, cast);\n    - unstructured data (text strings);\n- we must deal with columns with many NAN values (but not in the case of numericals).\n\nThis sounds fun! Now, what movies are we talking about??\n\n### b. Data Cleansing & Preliminary Feature Definition (on training set)\n\nSome of the objects are string versions of dictionaries. We must reformat them before extracting their content."},{"metadata":{"trusted":true,"_uuid":"c9afefa55a9b3fa10f79d90b5671086429369643"},"cell_type":"code","source":"# reformat strings into dictionaries\n# ast.literal_eval() use instad of eval() inspired from https://www.kaggle.com/gravix/gradient-in-a-box\ndef refmt_str2dict(df, cols):\n    for col in cols:\n        df[col] = df[col].apply(lambda x: {} if pd.isna(x) else ast.literal_eval(x) )\n    return df\n\ndict_columns = ['belongs_to_collection', 'genres', 'production_companies', 'production_countries', \n                'spoken_languages', 'Keywords', 'cast', 'crew']\n\ntrain = refmt_str2dict(train, dict_columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fa62904c4f3737d59bde9370cc6d789eac07b0de"},"cell_type":"code","source":"# path found here: https://www.kaggle.com/artgor/eda-feature-engineering-and-model-interpretation\nTMDB_path = 'https://image.tmdb.org/t/p/w600_and_h900_bestv2/'","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8ac99bd38a7fedba12d81cb0363cfe078897bb1d"},"cell_type":"markdown","source":"#### 'title' & 'poster_path'"},{"metadata":{"trusted":true,"_uuid":"502784b25245bc2324e5184c93c4dbacd33a0900"},"cell_type":"code","source":"nrow = 3\nfig = plt.figure(figsize=(20, nrow*5))\nk = 0\nfor i in np.random.randint(train.shape[0], size=nrow*7):\n    ax = fig.add_subplot(nrow, 7, k+1, xticks=[], yticks=[])\n    img = Image.open(urlopen(TMDB_path + train['poster_path'][i]))\n    plt.imshow(img)\n    ax.set_title(f\"{train['title'][i][0:22]}\")\n    k += 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"17a322e84fbebd07b112e0c074c5fb3ab74761cb"},"cell_type":"code","source":"def display_posters(movies, nrow=1):\n    fig = plt.figure(figsize=(20, nrow*5))\n    max_plot = nrow*7\n    if len(movies) <= max_plot:\n        max_movies = len(movies)\n    else:\n        max_movies = max_plot\n    for i in range(max_movies):\n        ax = fig.add_subplot(nrow, 7, i+1, xticks=[], yticks=[])\n        img = Image.open(urlopen(TMDB_path + movies['poster_path'][i]))\n        plt.imshow(img)\n        ax.set_title(f\"{movies['title'][i][0:22]}\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e63012d66ff34d9a2761f868b39918b2ce44aa7b"},"cell_type":"markdown","source":"Let's get a better feeling of the data and possibilities. I feel like searching for:\n- The oldest movies in the training set (to check 'release_date');\n- Time travel movies (to check 'Keywords', 'genres');\n- Movies with Bill Murray (to check 'cast');\n- Movies by Christopher Nolan (to check 'crew');\n- Movies by Amblin Entertainment (to check 'production_companies', 'production_countries')\n\nand work from there\n\n#### 'release_date'"},{"metadata":{"trusted":true,"_uuid":"ca659734e03d15c1095357e0aaddeae69dfa3a40"},"cell_type":"code","source":"# correct 'release_date' year\ndef fix_date(x):\n    yr = x.split('/')[2]\n    if int(yr) <= 19:\n        return x[:-2] + '20' + yr\n    else:\n        return x[:-2] + '19' + yr\n\ntrain['release_date'] = train['release_date'].apply(lambda x: fix_date(x))\ntrain['release_date'] = pd.to_datetime(train['release_date'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4487e3f29e71ea741b78e9656e9cfcbc0f9814ac"},"cell_type":"code","source":"def process_date(df):\n    date_attrs = ['year', 'month', 'day', 'weekday', 'weekofyear', 'quarter']\n    for attr in date_attrs:\n        new_col = 'release_date_' + attr\n        df[new_col] = getattr(df['release_date'].dt, attr).astype(int)\n    return df\n\ntrain = process_date(train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3a1a7f2382e7087ca53f98bfa896ee4933eab18a"},"cell_type":"code","source":"oldies = train[train['release_date_year'] < 1930].reset_index()\ndisplay_posters(oldies)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"aa1ee1d42b47b5609280022f47033b4742770daa"},"cell_type":"markdown","source":"#### 'Keywords', 'genres'"},{"metadata":{"trusted":true,"_uuid":"66cf2103a6fde183016a2d2fbcb55633baacef53"},"cell_type":"code","source":"def WordCloud_fromDict(dict_name, key='name'):\n    list_dict = list(train[dict_name].apply(lambda x: [i[key] for i in x] if x != {} else []).values)\n#    list2txt = ' '.join([i for j in list_dict for i in j])\n    list2txt = ' '.join(['_'.join(i.split(' ')) for j in list_dict for i in j])\n    wordcloud = WordCloud(max_font_size = None, background_color = 'black', collocations = False,\n                      width = 1200, height = 1000).generate(list2txt)\n    return wordcloud","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b981a58987c2ca84ad6ed58eac31771f44714c50"},"cell_type":"code","source":"fig = plt.figure(figsize = (20, 10))\nax = fig.add_subplot(1,2,1)\nax.imshow(WordCloud_fromDict('genres'))\nax.set_title('GENRES')\nax.axis('off')\nax = fig.add_subplot(1,2,2)\nax.imshow(WordCloud_fromDict('Keywords'))\nax.set_title('KEYWORDS')\nax.axis('off')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ee5b7a5669d2f6a52a61c452933bbd4a72b0c750"},"cell_type":"code","source":"list_genres = list(train['genres'].apply(lambda x: [i['name'] for i in x] if x != {} else []).values)\nlist_keywords = list(train['Keywords'].apply(lambda x: [i['name'] for i in x] if x != {} else []).values)\ngenres = set([y for x in list_genres for y in x])\nkeywords = set([y for x in list_keywords for y in x])\nlen(genres), len(keywords)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"06b28b4e3474338ecd881915b7026d41f6fec41e"},"cell_type":"code","source":"timeTravelMovies = train[train['Keywords'].apply(lambda x: 'time travel' in [i['name'] for i in x])].reset_index()\ndisplay_posters(timeTravelMovies)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6a91ffa4a522eb6ee610fde3d8cfbc089ef75ea8"},"cell_type":"markdown","source":"#### 'cast'"},{"metadata":{"trusted":true,"_uuid":"a03a1968110158e8b2a256c530335accdab34659"},"cell_type":"code","source":"nrow = 2\n# checking lead cast 'cast_id'] == 1\nlist_cast_wURL = list(train['cast'].apply(\n    lambda x: [(i['name'], i['profile_path']) for i in x if i['cast_id'] == 1] if x != {} else []).values)\ntop_cast = Counter([y for x in list_cast_wURL for y in x]).most_common(7*nrow)\n\nfig = plt.figure(figsize=(20, nrow*5))\nk = 0\nfor i in top_cast:\n    ax = fig.add_subplot(nrow, 7, k+1, xticks=[], yticks=[])\n    img = Image.open(urlopen(TMDB_path + i[0][1]))\n    plt.imshow(img)\n    ax.set_title(i[0][0][0:22])\n    k += 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"af3b9c16979dc656c8194b4704d4ac86246c7cfe"},"cell_type":"code","source":"BillMurrayMovies = train[train['cast'].apply(lambda x: 'Bill Murray' in [i['name'] for i in x])].reset_index()\ndisplay_posters(BillMurrayMovies)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3da4b93c03daa7e35eacc57fb9731dc9269f4f16"},"cell_type":"markdown","source":"#### 'crew'"},{"metadata":{"trusted":true,"_uuid":"91c9d6cde83c154b718c175bb97d596bc62467ca"},"cell_type":"code","source":"NolanMovies = train[train['crew'].apply(lambda x: 'Christopher Nolan' in [i['name'] for i in x if\\\n                                                                          i['job'] == 'Director'])].reset_index()\ndisplay_posters(NolanMovies)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"786a926f41678e2231067f85cf4d9aa5f4bb84ec"},"cell_type":"code","source":"list_crew = list(train['crew'].apply(lambda x: [i['name'] for i in x] if x != {} else []).values)\ncast = set([y for x in list_cast_wURL for y in x])\ncrew = set([y for x in list_crew for y in x])\nlen(cast), len(crew)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"465393ea3785115eb902c9e30c6080f4adb54e6d"},"cell_type":"markdown","source":"#### 'production_companies' & 'production_countries'"},{"metadata":{"trusted":true,"_uuid":"11b0a3ce200b758033ba9c873e70917b2ba81b15"},"cell_type":"code","source":"fig = plt.figure(figsize = (20, 10))\nax = fig.add_subplot(1,2,1)\nax.imshow(WordCloud_fromDict('production_companies'))\nax.set_title('PRODUCTION COMPANIES')\nax.axis('off')\nax = fig.add_subplot(1,2,2)\nax.imshow(WordCloud_fromDict('production_countries'))\nax.set_title('PRODUCTION COUNTRIES')\nax.axis('off')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8103d5f2d6178c3cf0879e23164053c8be74e899"},"cell_type":"code","source":"list_prodCompanies = list(train['production_companies'].apply(\n    lambda x: [i['name'] for i in x] if x != {} else []).values)\nlist_prodCountries = list(train['production_countries'].apply(\n    lambda x: [i['name'] for i in x] if x != {} else []).values)\nprodCompanies = set([y for x in list_prodCompanies for y in x])\nprodCountries = set([y for x in list_prodCountries for y in x])\nlen(prodCompanies), len(prodCountries)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"59f1b808dc5d43ea92e2161511ab58fb1688de74"},"cell_type":"code","source":"AmblinMovies = train[train['production_companies'].apply(\n    lambda x: 'Amblin Entertainment' in [i['name'] for i in x])].reset_index()\ndisplay_posters(AmblinMovies)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"af72398904a75e2dcad27bd1da702181fd2564fe"},"cell_type":"markdown","source":"#### All numericals: 'budget', 'popularity', 'runtime', 'revenue'"},{"metadata":{"trusted":true,"_uuid":"1a8386a8b198bc87d7b259dbc1d7ba70c515c000"},"cell_type":"code","source":"train[['budget', 'revenue', 'runtime', 'popularity']].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"317a15addfccab454378e47fe19ddf2aec5066ec"},"cell_type":"code","source":"plt.figure(figsize=(20,8))\nplt.subplot(121)\nplt.hist(np.log1p(train['budget']), bins = 50)   #some zero values in 'budget'\nplt.title('budget_rescaled')\nplt.subplot(122)\nplt.hist(np.log1p(train['revenue']), bins = 50)\nplt.title('revenue_rescaled')\nplt.subplots_adjust(hspace=0.5)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bc3971bc466b8953ed3057ec60240c768d5a78c7"},"cell_type":"code","source":"yr_release = train['release_date_year'].value_counts().sort_index()\nyr_budget = train.groupby(['release_date_year'])['budget'].mean()\nyr_revenue = train.groupby(['release_date_year'])['revenue'].mean()\nyr_popularity = train.groupby(['release_date_year'])['popularity'].mean()\n\nplt.figure(figsize=(20,8))\nplt.subplot(311)\nplt.title('Movie count')\nplt.plot(yr_release.index, yr_release.values)\nplt.subplot(312)\nplt.plot(yr_budget.index, yr_budget.values)\nplt.plot(yr_revenue.index, yr_revenue.values)\nplt.title('$')\nplt.subplot(313)\nplt.plot(yr_popularity.index, yr_popularity.values)\nplt.title('Popularity')\nplt.subplots_adjust(hspace=0.3)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f7e723dee9bbac022df5883775c34e524f3784a7"},"cell_type":"markdown","source":"#### Unstructured textual data: 'overview' & 'tagline' (& 'keywords' again)\n\nSince we have few samples, let's keep it simple and only see those unstructured data as bags-of-words. Let's play first with one movie only; let's make it The Terminator. We will merge all text columns into one since some words are redundant and keywords do not seem always that relevant compared to the overview/tagline."},{"metadata":{"trusted":true,"_uuid":"67798a93e716c080e98805b77de35aa6237026e7"},"cell_type":"code","source":"OneMovie = train[train['title'] == 'The Terminator']\nkeywords = list(OneMovie['Keywords'].apply(lambda x: [i['name'] for i in x] if x != {} else []).values)[0]\nkeywords_str = ' '.join(keywords)\ntext_merged = list(OneMovie['overview'])[0] + ' | ' + list(OneMovie['tagline'])[0] + ' | ' + keywords_str\ntext_merged","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f10b54084c26a640477d9758a4562859c6710c90"},"cell_type":"markdown","source":"Let's apply the same procedure for the full training set:"},{"metadata":{"trusted":true,"_uuid":"efbd3a51c9640aa33b44bb2a5f3a95b11f16a762"},"cell_type":"code","source":"# The following will not be used, as removing stopwords, getting stem, etc. will be done via TfidfVectorizer...\n#stop_words = set(stopwords.words('english'))\n#tokens = word_tokenize(text_merged)\n#tokens_cleaned = [w for w in tokens if not w in stop_words]\n#print(' '.join(tokens_cleaned))\n\n#ps = PorterStemmer()\n#tokens_stem = [ps.stem(w) for w in tokens_cleaned]\n#tokens_stem_nopunct = [w.lower() for w in tokens_stem if w.isalpha()]\n#print(' '.join(tokens_stem_nopunct))\n\n#bagofwords = list(set(tokens_stem_nopunct))\n#','.join(bagofwords)\n\ntext_merged2train = []\nfor id in train['id']:\n    mov = train[train['id'] == id]\n    keywords = list(mov['Keywords'].apply(lambda x: [i['name'] for i in x] if x != {} else []).values)[0]\n    keywords_str = ' '.join(keywords)\n    if pd.isnull(list(mov['overview'])[0]):\n        overview = ''\n    else:\n        overview = list(mov['overview'])[0]\n    if pd.isnull(list(mov['tagline'])[0]):\n        tagline = ''\n    else:\n        tagline = list(mov['tagline'])[0]\n\n#    text_merged = overview + ' | ' + tagline + ' | ' + keywords_str\n    text_merged = keywords_str\n\n    text_merged2train.append(text_merged)\n#    tokens = word_tokenize(text_merged)\n#    tokens_cleaned = [w for w in tokens if not w in stop_words]\n#    tokens_stem = [ps.stem(w) for w in tokens_cleaned]\n#    tokens_stem_nopunct = [w.lower() for w in tokens_stem if w.isalpha()]\n#    bagofwords = list(set(tokens_stem_nopunct))\n#    text_merged2train.append(bagofwords)\n\ntrain['text_merged'] = text_merged2train","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6c65e49f87240056daef41540eaee82945db5fec"},"cell_type":"markdown","source":"After testing, top keywords lead to less overfitting than top keywords+tagline+overview."},{"metadata":{"trusted":true,"_uuid":"f3b9a8ee87fbb6846d81d5338e5a0de294a05feb"},"cell_type":"code","source":"#check\n#list(train[train['title'] == 'The Terminator']['text_merged'])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4d65ddf436f1e25717b2ea2020a149560b4d0956"},"cell_type":"markdown","source":"#### Loose ends: 'belongs_to_collection', 'homepage', 'imdb_id', 'original_language', 'original_title', 'spoken_languages', 'status'"},{"metadata":{"trusted":true,"_uuid":"22f730324d9d1b58dd4e2089859a41b9af86cd1b"},"cell_type":"code","source":"train['belongs_to_collection'].head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"16d15116bc7eff03709d84e2f1e8e2d87ff6737e"},"cell_type":"code","source":"train['belongs_to_collection'].apply(lambda x: 1 if x != {} else 0).value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3da278b7ba5fc4dbb471357d194831e7de032be9"},"cell_type":"code","source":"train['belongs2coll_yn'] = train['belongs_to_collection'].apply(lambda x: len(x) if x != {} else 0)\ntrain['homepage'].head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7228c54606b8b9998921082d25fa9ad3ab41e871"},"cell_type":"code","source":"train['homepage'].apply(lambda x: 1 if pd.isnull(x) == False else 0).value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3542a291fe5700d3c46157dbd9eb02d41bb71a43"},"cell_type":"code","source":"train['homepage_yn'] = train['homepage'].apply(lambda x: 1 if pd.isnull(x) == False else 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d82da2292023d8731967b332b31425dda66e67ab"},"cell_type":"code","source":"train['imdb_id'].head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ad99f3294220f021adba5cd04d78e70b1716a774"},"cell_type":"code","source":"' '.join(set(train['original_language'])), len(set(train['original_language']))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"52753059fce96207618a9362a9e5ab71ee69e618"},"cell_type":"code","source":"train['original_title'][0:5], train['title'][0:5]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"653abf8a4d37ede93f92baf29591699a7c49da59"},"cell_type":"code","source":"list_languages = list(train['spoken_languages'].apply(lambda x: [i['name'] for i in x] if x != {} else []).values)\nlanguages = set([y for x in list_languages for y in x])\nlen(languages), Counter([i for j in list_languages for i in j]).most_common(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2262adaf402bc8da9b07b505a2f4a5f452f5e61e"},"cell_type":"code","source":"train['status'].apply(lambda x: 1 if x == 'Released' else 0).value_counts(), set(train['status'])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"532cfde29663038f90817307243f17597ddf145e"},"cell_type":"markdown","source":"###  c. To Recap (training set only)\n\nHave we checked all data columns?\n\n**id**                         : Irrelevant<br>\n**belongs_to_collection**      : Turned into boolean 'belongs2coll_yn'<br>\n**budget**                     : Numerical<br>\n**genres**                     : 20 genres -> feature engineering (one-hot categoricals)<br>\n**homepage**                   : Turned into boolean 'homepage_yn'<br>\n**imdb_id**                    : Irrelevant (will not use IMDB data)<br>\n**original_language**          : 36 languages -> feature engineering<br>\n**original_title**             : Irrelevant (only difference with 'title' is language, which is already in 'original_language')<br>\n**overview**                   : Cleaned & merged with other text in new feature 'text_merged'<br>\n**popularity**                 : Numerical<br>\n**poster_path**                : Just for visualization<br>\n**production_companies**       : 3695 companies -> feature engineering<br>\n**production_countries**       : 74 countries -> feature engineering<br>\n**release_date**               : Cleaned & turned into 'year', 'month', 'day', 'weekday', 'weekofyear', 'quarter'<br>\n**runtime**                    : Numerical<br>\n**spoken_languages**           : 56 languages -> feature engineering<br>\n**status**                     : Irrelevant (only 4 'Rumored' instead of 'Released' out of 3000 samples)<br>\n**tagline**                    : Cleaned & merged with other text in new feature 'text_merged'<br>\n**title**                      : Not (yet) included in new feature 'text_merged'<br>\n**Keywords**                   : Cleaned & merged with other text in new feature 'text_merged'<br>\n**cast**                       : 38723 names -> feature engineering (incl. 'character', 'gender'...)<br>\n**crew**                       : 38554 names -> feature engineering (incl. 'department', 'gender', 'job'...)<br>\n**revenue**                    : TARGET<br>\n\nNew columns: **release_date_xxx**, **text_merged**, **belongs2coll_yn**, **homepage_yn**<br>\nBefore moving to feature engineering, let's check the consistency between training set and test set."},{"metadata":{"_uuid":"5ee0881bff6be672b94c4a54a845235b9dbffd06"},"cell_type":"markdown","source":"### d. Training set / Test set Balance\n\n#### Create new features in test set (as in training set)"},{"metadata":{"trusted":true,"_uuid":"650b9249f4e0b354d77b3941caa937cf7bf1cfde"},"cell_type":"code","source":"# strings to dictionaries\ntest = refmt_str2dict(test, dict_columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f4abaa2bf05d16cf34b2b49d0d99b6ef73f2d606"},"cell_type":"code","source":"# One release date missing for the test set, for movie 'Jails, Hospitals & Hip-Hop'\n# date '5/1/00' retrieved from https://www.imdb.com/title/tt0210130/\n# but I don't want to use any external data wo will use dummy date\ntest.at[pd.isnull(test['release_date']), 'release_date'] = '1/1/11'\ntest['release_date'] = test['release_date'].apply(lambda x: fix_date(x))\ntest['release_date'] = pd.to_datetime(test['release_date'])\n\n# create 'year', 'month', 'day', 'weekday', 'weekofyear', 'quarter' features\ntest = process_date(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"210783665243c51c6e7a415d1197252fd06a8be2"},"cell_type":"code","source":"text_merged2test = []\nfor id in test['id']:\n    mov = test[test['id'] == id]\n    keywords = list(mov['Keywords'].apply(lambda x: [i['name'] for i in x] if x != {} else []).values)[0]\n    keywords_str = ' '.join(keywords)\n    if pd.isnull(list(mov['overview'])[0]):\n        overview = ''\n    else:\n        overview = list(mov['overview'])[0]\n    if pd.isnull(list(mov['tagline'])[0]):\n        tagline = ''\n    else:\n        tagline = list(mov['tagline'])[0]\n\n#    text_merged = overview + ' | ' + tagline + ' | ' + keywords_str\n    text_merged = keywords_str\n\n    text_merged2test.append(text_merged)\n#    tokens = word_tokenize(text_merged)\n#    tokens_cleaned = [w for w in tokens if not w in stop_words]\n#    tokens_stem = [ps.stem(w) for w in tokens_cleaned]\n#    tokens_stem_nopunct = [w.lower() for w in tokens_stem if w.isalpha()]\n#    bagofwords = list(set(tokens_stem_nopunct))\n#    text_merged2test.append(bagofwords)\n    \ntest['text_merged'] = text_merged2test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"66ea2b0c016cf262ee0b07263b18aa3d7fd51276"},"cell_type":"code","source":"#list(test[test['title'] == 'Transcendence']['text_merged'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5cfff22b54a1750b8ac4390e4b2d93ada4de977e"},"cell_type":"code","source":"test['belongs2coll_yn'] = test['belongs_to_collection'].apply(lambda x: len(x) if x != {} else 0)\ntest['homepage_yn'] = test['homepage'].apply(lambda x: 1 if pd.isnull(x) == False else 0)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"df148e16181fc74e0d345c727de87791cbf8dac8"},"cell_type":"markdown","source":"#### Compare distributions"},{"metadata":{"trusted":true,"_uuid":"5c0e1e60055f5733d5d3d6d2cd9ac2e93e7a1448"},"cell_type":"code","source":"yr_release_test = test['release_date_year'].value_counts().sort_index()\nyr_budget_test = test.groupby(['release_date_year'])['budget'].mean()\n\nplt.figure(figsize=(20,8))\nplt.subplot(211)\nplt.title('Movie count')\nplt.plot(yr_release.index, yr_release.values)\nplt.plot(yr_release_test.index, yr_release_test.values)\nplt.subplot(212)\nplt.plot(yr_budget.index, yr_budget.values)\nplt.plot(yr_budget_test.index, yr_budget_test.values)\nplt.title('$')\nplt.subplots_adjust(hspace=0.3)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f6773ab089ebb32dbce566fad95f63f7343f0ac3"},"cell_type":"code","source":"test[test['release_date_year'] == 1927]\n#https://en.wikipedia.org/wiki/List_of_most_expensive_films\n#Metropolis, the 1927 German film directed by Fritz Lang, often erroneously reported as having cost\n#$200 million at the value of modern money. Metropolis cost $1.2â€“1.3 million at the time of its\n#production, which would be about $12 million at 2009 prices","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b01b1c7290d06f7a817d5c267f1315448a08685e"},"cell_type":"code","source":"nrow = 2\n# checking lead cast 'cast_id'] == 1\nlist_cast_INtest = list(test['cast'].apply(\n    lambda x: [(i['name'], i['profile_path']) for i in x if i['cast_id'] == 1] if x != {} else []).values)\ntop_cast_INtest = Counter([y for x in list_cast_INtest for y in x]).most_common(7*nrow)\n\nfig = plt.figure(figsize=(20, nrow*5))\nk = 0\nfor i in top_cast_INtest:\n    ax = fig.add_subplot(nrow, 7, k+1, xticks=[], yticks=[])\n    img = Image.open(urlopen(TMDB_path + i[0][1]))\n    plt.imshow(img)\n    ax.set_title(i[0][0][0:22])\n    k += 1","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f22105c2b9e6b1383528da78b7560267666d8bca"},"cell_type":"markdown","source":"#### Drop columns that won't be used"},{"metadata":{"trusted":true,"_uuid":"51c746c3ef73efb3d3f6dd83dcbb686d346e0aa3"},"cell_type":"code","source":"cols2drop = ['id','belongs_to_collection', 'homepage', 'imdb_id', 'original_title', 'overview',\n            'poster_path', 'release_date', 'spoken_languages', 'status','tagline', 'Keywords']\ntrain = train.drop(cols2drop, axis=1)\ntest = test.drop(cols2drop, axis=1)\n\n#features that remain:\ntrain.columns, test.columns","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f03e918d6d4f07a215deb68436be31183c8c1eaa"},"cell_type":"markdown","source":"## II. Feature Engineering\n\n### a. Feature Selection Based on Correlations\n\n#### Numericals"},{"metadata":{"trusted":true,"_uuid":"a4b0d1b0c74e62b652777501f557825a9030389e"},"cell_type":"code","source":"df4corr = train[['budget', 'popularity', 'runtime', 'release_date_year', 'release_date_month', 'release_date_day', \\\n                   'release_date_weekday', 'release_date_weekofyear', 'release_date_quarter', 'revenue']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f27cf11d3fcf4e91d956fb0449ce78ee990a07cc"},"cell_type":"code","source":"correlation = df4corr.corr()\nplt.figure(figsize=(12, 12))  \nsns.heatmap(correlation, annot=True, square=True, cmap='coolwarm')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0fa8018c5c90d768237cb14b58e6eafcf12ecc28"},"cell_type":"code","source":"_, axes = plt.subplots(2, 4, figsize=(20, 8))\nsns.scatterplot(x = 'budget', y = 'revenue', data = train, marker=\"+\", ax=axes[0,0])\nsns.scatterplot(x = np.log1p(train['budget']), y = np.log1p(train['revenue']), marker=\"+\", ax=axes[0,1])\nsns.scatterplot(x = 'runtime', y = 'revenue', data = train, marker=\"+\", ax=axes[0,2])\n#sns.scatterplot(x = train['budget']/train['runtime'], y = train['revenue'], marker=\"+\", ax=axes[0,2])\nsns.scatterplot(x = 'popularity', y = 'revenue', data = train, marker=\"+\", ax=axes[0,3])\n#sns.scatterplot(x = train['budget']/train['popularity'], y = train['revenue'], marker=\"+\", ax=axes[0,3])\n#sns.scatterplot(x = np.where(train['popularity'] < 40, train['popularity'], 40), y = train['revenue'], \\\n#               marker=\"+\", ax=axes[0,3])\nsns.scatterplot(x = 'release_date_year', y = 'revenue', data = train, marker=\"+\", ax=axes[1,0])\nsns.stripplot(x = 'release_date_weekday', y = 'revenue', data = train, ax=axes[1,1])\nsns.stripplot(x = 'release_date_month', y = 'revenue', data = train, ax=axes[1,2])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"17c091a858ff24293af83bd9351c022e07d056f5"},"cell_type":"code","source":"train['budget_yn'] = train['budget'].apply(lambda x: 0 if x == 0 else 1)\ntrain['budget_per_year'] = train['budget']/train['release_date_year']\n#train['budget_perRuntime'] = train['budget']/train['runtime']\n#train['popularity_clipped'] = np.where(train['popularity'] < 40, train['popularity'], 40)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8dc01dfb71eb3719ae51c70126cbcb4ea496cf4b"},"cell_type":"markdown","source":"#### boolean"},{"metadata":{"trusted":true,"_uuid":"e0127400b42824861694d42700a8ad13220f4aca"},"cell_type":"code","source":"_, axes = plt.subplots(1, 3, figsize=(20, 8))\nsns.stripplot(x = 'belongs2coll_yn', y = 'revenue', data = train, ax=axes[0])\nsns.stripplot(x = 'homepage_yn', y = 'revenue', data = train, ax=axes[1])\nsns.stripplot(x = 'budget_yn', y = 'revenue', data = train, ax=axes[2])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bfd3fe4b8a3ed2068acd1b3099811ef06097fc2a"},"cell_type":"markdown","source":"#### Categoricals"},{"metadata":{"trusted":true,"_uuid":"31be9b70b92c0bd9dfd01728f7de491582b1a56e"},"cell_type":"code","source":"train['n_genres'] = train['genres'].apply(lambda x: len(x) if x != {} else 0)\ntrain['n_production_companies'] = train['production_companies'].apply(lambda x: len(x) if x != {} else 0)\ntrain['n_production_countries'] = train['production_countries'].apply(lambda x: len(x) if x != {} else 0)\ntrain['n_cast'] = train['cast'].apply(lambda x: len(x) if x != {} else 0)\ntrain['n_crew'] = train['crew'].apply(lambda x: len(x) if x != {} else 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d329a1d0937a5cf5b5f4865c1a293d9e3e27dd02"},"cell_type":"code","source":"_, axes = plt.subplots(2, 3, figsize=(20, 8))\nsns.stripplot(x = 'n_genres', y = 'revenue', data = train, ax=axes[0,0])\nsns.stripplot(x = 'n_production_companies', y = 'revenue', data = train, ax=axes[0,1])\nsns.stripplot(x = 'n_production_countries', y = 'revenue', data = train, ax=axes[0,2])\nsns.regplot(x = 'n_cast', y = 'revenue', data = train, marker='+', ax=axes[1,0])\nsns.regplot(x = 'n_crew', y = 'revenue', data = train, marker='+', ax=axes[1,1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"926fcce4b9b6c9b9798fe2da6f1e9aae7d014407"},"cell_type":"code","source":"train['genres_collapsed'] = train['genres'].apply(lambda x: ' '.\\\n                                                  join(sorted([i['name'] for i in x])) if x != {} else '')\nfor g in genres:\n    train['genre_' + g] =  train['genres_collapsed'].apply(lambda x: 1 if g in x else 0)\n\nkeys_genre = ['genre_' + g for g in genres]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"97078925e624abcb365f68b85085cf4109149b77"},"cell_type":"code","source":"n_prodCompanies = 30\nCounter([i for j in list_prodCompanies for i in j]).most_common(n_prodCompanies)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"15b0e9e402bcc42cbe51695810b63fa7cebae1fc"},"cell_type":"code","source":"train['production_companies_collapsed'] = train['production_companies'].apply(lambda x: ' '.\\\n                                                    join(sorted([i['name'] for i in x])) if x != {} else '')\ntop_prodCompanies = [m[0] for m in Counter([i for j in list_prodCompanies for i in j]).most_common(n_prodCompanies)]\nfor comp in top_prodCompanies:\n    train['production_company_' + comp] = train['production_companies_collapsed'].\\\n            apply(lambda x: 1 if comp in x else 0)\n    \nkeys_production_company = ['production_company_' + comp for comp in top_prodCompanies]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6d79b808feae742b15e89a078b0333a95d5d10fb"},"cell_type":"code","source":"n_prodCountries = 30\nCounter([i for j in list_prodCountries for i in j]).most_common(n_prodCountries)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c27e2472ee628e2e3c40faaa0bde2ea87c0817a3"},"cell_type":"code","source":"train['production_countries_collapsed'] = train['production_countries'].apply(lambda x: ' '.\\\n                                                    join(sorted([i['name'] for i in x])) if x != {} else '')\ntop_prodCountries = [m[0] for m in Counter([i for j in list_prodCountries for i in j]).most_common(n_prodCountries)]\nfor comp in top_prodCountries:\n    train['production_country_' + comp] = train['production_countries_collapsed'].\\\n            apply(lambda x: 1 if comp in x else 0)\n\nkeys_production_country = ['production_country_' + comp for comp in top_prodCountries]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d71d7c9fc4a4c489fbda7009c790db71671c449d"},"cell_type":"code","source":"#train['EnglishLead_yn'] = train['original_language'].apply(lambda x: 1 if x == 'en' else 0)\ntop_languages = [m[0] for m in Counter(train['original_language']).most_common(5)]\nfor l in top_languages:\n    train['language_' + l] = train['original_language'].\\\n            apply(lambda x: 1 if l in x else 0)\n\nkeys_language = ['language_' + l for l in top_languages]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"56eaf04eac77123bb15d0086420f9f8632b3757c"},"cell_type":"code","source":"# use lead actor for top_cast\nn_lead = 50\nlist_lead = list(train['cast'].apply(lambda x: [i['name'] for i in x if i['cast_id'] == 1] if x != {} else []).values)\nCounter([i for j in list_lead for i in j]).most_common(n_lead)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1b8d86eba9fc2a98367c20d11326922be2b3c138"},"cell_type":"code","source":"# different top leads in training and test sets - used combined set\nfullset = pd.concat([train, test])\n\nlist_lead = list(fullset['cast'].apply(lambda x: [i['name'] for i in x if i['cast_id'] == 1] if x != {} else []).values)\nCounter([i for j in list_lead for i in j]).most_common(n_lead)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ddc880ddc0a11af7182dca51de12857da8e67fd1"},"cell_type":"code","source":"# find top_lead even if not lead in movie\ntrain['cast_collapsed'] = train['cast'].apply(lambda x: ' '.\\\n                                              join(sorted([i['name'] for i in x])) if x != {} else '')\ntop_lead = [m[0] for m in Counter([i for j in list_lead for i in j]).most_common(n_lead)]\nfor lead in top_lead:\n    train['cast_' + lead] = train['cast_collapsed'].apply(lambda x: 1 if lead in x else 0)\n    \nkeys_cast = ['cast_' + lead for lead in top_lead]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8669dc9f0ad247960ce04f0835ecd3f28d0173d5"},"cell_type":"code","source":"n_directors = 50\nlist_directors = list(train['crew'].apply(\n    lambda x: [i['name'] for i in x if i['job'] == 'Director'] if x != {} else []).values)\nCounter([i for j in list_directors for i in j]).most_common(n_directors)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"de338de3d4851dcf364a6b8fb6579d3ad299bd2b"},"cell_type":"code","source":"#same as cast - use combined set\nlist_directors = list(fullset['crew'].apply(\n    lambda x: [i['name'] for i in x if i['job'] == 'Director'] if x != {} else []).values)\nCounter([i for j in list_directors for i in j]).most_common(n_directors)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"234c91d82b85a67aa631331c419e7d50a76f216d"},"cell_type":"code","source":"train['directors_collapsed'] = train['crew'].apply(lambda x: ' '.\\\n        join(sorted([i['name'] for i in x if i['job'] == 'Director'])) if x != {} else '')\ntop_directors = [m[0] for m in Counter([i for j in list_directors for i in j]).most_common(n_directors)]\nfor d in top_directors:\n    train['director_' + d] = train['directors_collapsed'].apply(lambda x: 1 if d in x else 0)\n    \nkeys_director = ['director_' + d for d in top_directors]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ae37388fcd2b5a6fd4559acbaf2417ca9f22a7b7"},"cell_type":"markdown","source":"#### Textual data"},{"metadata":{"trusted":true,"_uuid":"4776235845f13efc9b1790ca1a21514bbc7b5db8"},"cell_type":"code","source":"# deprecated: was used when text transformation was already performed on 'text_merged'\n#tokens_train = [word for l in train['text_merged'] for word in l]\n#tokens_test = [word for l in test['text_merged'] for word in l]\n#tokens = tokens_train + tokens_test\n#lexicon = set(tokens)\n#len(lexicon)\n\n#lexicon10 = [m[0] for m in Counter(tokens).most_common(10)]\n#lexicon10","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9933d95228991f0f3bdde5b776efd345e86cc13f"},"cell_type":"code","source":"vectorizer = TfidfVectorizer(\n            analyzer = 'word',\n            stop_words = 'english',\n            ngram_range = (1, 2),\n            min_df = 10,\n            sublinear_tf = True)\n\noverview_transf = vectorizer.fit_transform(train['text_merged'])\noverview_transf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2088e998880378424887ec5a4cc915e1e3951ace"},"cell_type":"code","source":"lexicon = list(vectorizer.vocabulary_.keys())\n#lexicon","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"08079da64077870868551fabe042608f3fb3ce0a"},"cell_type":"code","source":"linreg = LinearRegression()\nlinreg.fit(overview_transf, train['revenue'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"25a4029e0e2ea837c519ef35c717df4492453367"},"cell_type":"code","source":"top = 100   #500: 2.06288, 100 : 2.01548, 50: 2.01763 (test set result)\nnegcorrTop = np.argsort(linreg.coef_)[0:top]\nposcorrTop = np.argsort(linreg.coef_)[len(linreg.coef_)-top:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9cc6e2b8661d9a730a060a3be653633d57f9746c"},"cell_type":"code","source":"keywords_newPos = [lexicon[i] for i in poscorrTop]\nkeywords_newNeg = [lexicon[i] for i in negcorrTop]\nkeywords_new = keywords_newPos + keywords_newNeg\nkeywords_new[0:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cca6c9aba26c63ab67b767e24232fdc92d02d16d"},"cell_type":"code","source":"for k in keywords_new:\n    train['txt_' + k] =  train['text_merged'].apply(lambda x: 1 if k in x else 0)\n\nkeys_txt = ['txt_' + s for s in keywords_new]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"96d5e3b8a52a72265897580c8fb30a2973ccbdfb"},"cell_type":"markdown","source":"### b. Apply to test set"},{"metadata":{"trusted":true,"_uuid":"49795a861bfcc4609231f3e56bdc48740381d3c6"},"cell_type":"code","source":"test['budget_yn'] = test['budget'].apply(lambda x: 0 if x == 0 else 1)\ntest['budget_per_year'] = test['budget']/test['release_date_year']\n\ntest['n_genres'] = test['genres'].apply(lambda x: len(x) if x != {} else 0)\ntest['n_production_companies'] = test['production_companies'].apply(lambda x: len(x) if x != {} else 0)\ntest['n_production_countries'] = test['production_countries'].apply(lambda x: len(x) if x != {} else 0)\ntest['n_cast'] = test['cast'].apply(lambda x: len(x) if x != {} else 0)\ntest['n_crew'] = test['crew'].apply(lambda x: len(x) if x != {} else 0)\n\n#test['EnglishLead_yn'] = test['original_language'].apply(lambda x: 1 if x == 'en' else 0)\nfor l in top_languages:\n    test['language_' + l] = test['original_language'].\\\n            apply(lambda x: 1 if l in x else 0)\n\ntest['genres_collapsed'] = test['genres'].apply(lambda x: ' '.join(sorted([i['name'] for i in x])) if x != {} else '')\nfor g in genres:\n    test['genre_' + g] =  test['genres_collapsed'].apply(lambda x: 1 if g in x else 0)\n\ntest['production_companies_collapsed'] = test['production_companies'].apply(lambda x: ' '.\\\n                                                    join(sorted([i['name'] for i in x])) if x != {} else '')\nfor comp in top_prodCompanies:\n    test['production_company_' + comp] = test['production_companies_collapsed'].\\\n              apply(lambda x: 1 if comp in x else 0)\n\ntest['production_countries_collapsed'] = test['production_countries'].apply(lambda x: ' '.\\\n                                                    join(sorted([i['name'] for i in x])) if x != {} else '')\nfor comp in top_prodCountries:\n    test['production_country_' + comp] = test['production_countries_collapsed'].\\\n            apply(lambda x: 1 if comp in x else 0)\n\ntest['cast_collapsed'] = test['cast'].apply(lambda x: ' '.\\\n                                              join(sorted([i['name'] for i in x])) if x != {} else '')\nfor lead in top_lead:\n    test['cast_' + lead] = test['cast_collapsed'].apply(lambda x: 1 if lead in x else 0)\n\ntest['directors_collapsed'] = test['crew'].apply(lambda x: ' '.\\\n        join(sorted([i['name'] for i in x if i['job'] == 'Director'])) if x != {} else '')\nfor d in top_directors:\n    test['director_' + d] = test['directors_collapsed'].apply(lambda x: 1 if d in x else 0)\n    \nfor k in keywords_new:\n    test['txt_' + k] =  test['text_merged'].apply(lambda x: 1 if k in x else 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e341a29e5591f8e0727ec4e4f4b3747bfdacb4de"},"cell_type":"code","source":"cols2drop = ['genres', 'production_companies', 'production_countries', 'genres_collapsed',\n             'production_companies_collapsed', 'production_countries_collapsed', 'cast_collapsed', \n             'directors_collapsed', 'n_cast', 'n_crew']\ntrain = train.drop(cols2drop, axis=1)\ntest = test.drop(cols2drop, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8adbc0c4cb21ca9d6bc3369b58d715d062e319f1"},"cell_type":"markdown","source":"## III. Modelling"},{"metadata":{"trusted":true,"_uuid":"de3d3387f9303f083496664e4b4f9552dadacc79"},"cell_type":"code","source":"def RMSE(y_obs, y_pred):\n    n = len(y_obs)\n    rmse = np.sqrt( 1/n*np.sum((y_pred-y_obs)**2) )\n    return rmse\n\ndef RMSLE(y_obs, y_pred):\n    n = len(y_obs)\n    rmsle = np.sqrt( 1/n*np.sum((np.log(y_pred)-np.log(y_obs))**2) )\n    return rmsle\n\ndef LinearRegression(X, y):\n    intercept_term = np.ones(shape = y.shape)\n    X = np.concatenate((intercept_term, X), 1)\n    #closed-form solution:\n    coeffs = np.linalg.inv(X.T.dot(X)).dot(X.T).dot(y)\n    return coeffs","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c86f51ab8e1016522f4291e7d774cc55cad10c30"},"cell_type":"markdown","source":"### a. Baseline Model (Linear Regression)\n\nOur baseline model is a linear regressor between some numericals (budget, etc.) & revenue. No need to define a validation set since we will clearly not overfit here.\n\nWe will use the closed-form solution, which is the cleanest approach for such simple case\n\ny = b + WX"},{"metadata":{"trusted":true,"_uuid":"e9fc81c1e0b8bbf9492144180ed96f4f46f32c73"},"cell_type":"code","source":"x = train['budget'].values\n#x = train['budget_per_year'].values\nX = np.reshape(x, (len(x),1))     #len(x) samples, 1 dimension\ny = train['revenue'].values\ny = np.reshape(y, (len(y),1))\nnp.shape(X), np.shape(y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a029828fc84d4d4c87de2e12c5486689e707bb91"},"cell_type":"code","source":"model_baseline_coeffs = LinearRegression(X, y)\nmodel_baseline_pred_train = model_baseline_coeffs[0] + model_baseline_coeffs[1]*X\nmodel_baseline_coeffs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"21c576c1b41901dbd28451210c4156d103db5c29"},"cell_type":"code","source":"plt.figure(figsize=(10,5))\nplt.title('Linear regression (1d)')\nplt.scatter(X, y)\nplt.plot(X, model_baseline_pred_train, c = 'black')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2a34770c5960c1b82c7507d08ba03fcda223201d"},"cell_type":"code","source":"#x=budget: 2.6656031635747532\n#x=budget_per_date_year: 2.661467020660518\nRMSLE(y, model_baseline_pred_train)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e1d75b99c678c414777f5b1e3788cf632b9072c1"},"cell_type":"markdown","source":"### b. Trees"},{"metadata":{"trusted":true,"_uuid":"f60adc2d0a2bd5653b3666c15fc234ba9ea27eb7"},"cell_type":"code","source":"train_selected = train[['budget', 'popularity', 'runtime',\n                        'release_date_year', 'release_date_weekday', 'release_date_month',\n                        'budget_yn', 'belongs2coll_yn', 'homepage_yn',\n                        'n_genres', 'n_production_companies', 'n_production_countries', \n                        'revenue']+keys_genre+keys_cast+keys_director+keys_production_company+\n                       keys_production_country+keys_language]\n\n#train_selected = train_selected.replace([np.inf, -np.inf], np.nan)\ntrain_selected = train_selected.dropna(axis = 0)\n\nX = train_selected.drop(['revenue'], axis=1)\ny = train_selected['revenue']","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"baf0079a4802e91b4f7e0db28103007abec32651"},"cell_type":"markdown","source":"#### Random Forest"},{"metadata":{"trusted":true,"_uuid":"33fe1daea0161c1b5117d51aada6d863b3f84bf2"},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4)\n\nmodel_RF = RandomForestRegressor(n_estimators = 100,\n                                 max_depth = 20)\nmodel_RF.fit(X_train, y_train)\nmodel_RF_pred_valset = model_RF.predict(X_test)\nRMSLE(y_test, model_RF_pred_valset)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4c2364d01c92cecfe4bfe4a5aae6d433ab52ecc3"},"cell_type":"code","source":"model_RF.fit(X, y)\nmodel_RF_pred_train = model_RF.predict(X)\nRMSLE(y, model_RF_pred_train)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"047b6c2260c300f971c8e6fe0eb73592104473e2"},"cell_type":"markdown","source":"#### CatBoost\n\nvanilla model"},{"metadata":{"trusted":true,"_uuid":"a798fb2dec319e114ccc9c77256ee4b7bd534841"},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, np.log1p(y), test_size=0.4)\n\nmodel_CatBoost = CatBoostRegressor(silent=True)\n\nmodel_CatBoost.fit(X_train, y_train)\nmodel_CatBoost_pred_valset = model_CatBoost.predict(X_test)\nRMSE(y_test, model_CatBoost_pred_valset)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"88846892768b6054a01afd9a103ec7170b41f60f","scrolled":true},"cell_type":"code","source":"model_CatBoost.fit(X, np.log1p(y))\nmodel_CatBoost_pred_train = model_CatBoost.predict(X)\nRMSE(np.log1p(y), model_CatBoost_pred_train)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d1bf1aff024ecfa629b291f920445c7ae6feaccd"},"cell_type":"markdown","source":"#### Gradient Boosting\n\nvanilla model"},{"metadata":{"trusted":true,"_uuid":"bace88c8eacf09adea329ef13d22011e64655d92"},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, np.log1p(y), test_size=0.4)\n\nmodel_gboost = GradientBoostingRegressor()\n\nmodel_gboost.fit(X_train, y_train)\nmodel_gboost_pred_valset = model_gboost.predict(X_test)\nRMSE(y_test, model_gboost_pred_valset)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2b2a2d06dc7d9838ab06d7e5b4e031beb2fc627e"},"cell_type":"code","source":"model_gboost.fit(X, np.log1p(y))\nmodel_gboost_pred_train = model_gboost.predict(X)\nRMSE(np.log1p(y), model_gboost_pred_train)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7ed5860fed032caa7241410edd8f4ad31ef2183d"},"cell_type":"markdown","source":"#### Ensembling CatBoost & Gradient Boosting"},{"metadata":{"trusted":true,"_uuid":"fb860c123fe8ce4c8225bfdeb3a1823abf035ee4"},"cell_type":"code","source":"model_ensemble_pred_train = (model_CatBoost_pred_train+model_gboost_pred_train)/2\nRMSE(np.log1p(y), model_ensemble_pred_train)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3b5fb3008f3e4cf612358cf18568e7bb736ba408"},"cell_type":"markdown","source":"### c. Submission\n\n#### First submission\n\nBaseline model based on simplest linear regression<br>\nYields a score of **2.67065** (ranked 186 as of 24 Feb 2019) to be compared to training set result of 2.66560."},{"metadata":{"trusted":true,"_uuid":"8e2293cd25bac4e03fc8bfc23839ce334e2e2c53"},"cell_type":"code","source":"x = test['budget'].values\nX = np.reshape(x, (len(x),1))     #len(x) samples, 1 dimension\nnp.shape(X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"422d9044ec7f38904c9ccea6d737137419e6f6e8"},"cell_type":"code","source":"model_baseline_pred_test = model_baseline_coeffs[0] + model_baseline_coeffs[1]*X","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c3b056223f4060f006c7eadd244f5acad243d4d4"},"cell_type":"code","source":"submission = pd.read_csv('../input/sample_submission.csv')\nsubmission['revenue'] = model_baseline_pred_test\nsubmission.to_csv('submission_baseline.csv', index = False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b1ad52d1690cf89c247c4c25354d025d81b3dfef"},"cell_type":"markdown","source":"#### Second submission\n\nSimple Random Forest model without textual data.<br>\nYields score of **2.56128** (ranked 180 as of 27 Feb 2019) to be compared to training set result of 2.19076."},{"metadata":{"trusted":true,"_uuid":"76926801e72d18054dfc67d870c5c014baa812ae"},"cell_type":"code","source":"test_features = test[['budget', 'popularity', 'runtime',\n                        'release_date_year', 'release_date_weekday', 'release_date_month',\n                        'budget_yn', 'belongs2coll_yn', 'homepage_yn',\n                        'n_genres', 'n_production_companies', 'n_production_countries']+\n                     keys_genre+keys_cast+keys_director+keys_production_company+keys_production_country+\n                    keys_language]\n\nX = test_features\ntest_features.columns[test_features.isna().any()].tolist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"81391ee08896b9dd2c37582cafa48b2fb872f5ee"},"cell_type":"code","source":"test_features['runtime'][test_features['runtime'].isna() == True]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f5c6ceb1298521bd825326ef3b6b61772691141f"},"cell_type":"code","source":"test_features['runtime'][test_features['runtime'].isna() == True] = test_features['runtime'].median()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"74b7980fd2d0f511645de167c19ee35a7c544c52"},"cell_type":"code","source":"model_RF_pred_test = model_RF.predict(X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ea4919d972bf4682ed00815105ba3589027822c6"},"cell_type":"code","source":"submission = pd.read_csv('../input/sample_submission.csv')\nsubmission['revenue'] = model_RF_pred_test\nsubmission.to_csv('submission_RF.csv', index = False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1ddaf00315ad2e7813a434a2ca36590172026d09"},"cell_type":"markdown","source":"#### Third submission\n\nCatBoost vanilla model without textual data.<br>\nYields **1.99677** (ranked 68 as of 27 Feb 2019) to be compared to training set result of 1.74703.<br>\nYields **1.98573** (ranked 73 as of 5 Mar 2019) - production countries, 5 top languages added compared to previous one."},{"metadata":{"trusted":true,"_uuid":"002b6cd6b2e4e21a25add81a1b8fd8d2d50adbbf"},"cell_type":"code","source":"model_CatBoost_pred_test = model_CatBoost.predict(X)\nmodel_CatBoost_pred_test = np.expm1(model_CatBoost_pred_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5dc828a193118e1950d98ec9d9b136fb07e639e1"},"cell_type":"code","source":"submission = pd.read_csv('../input/sample_submission.csv')\nsubmission['revenue'] = model_CatBoost_pred_test\nsubmission.to_csv('submission_CatBoost.csv', index = False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bae5002d90b4d9d728ce6ead3cb966eeec06d2ab"},"cell_type":"markdown","source":"---\n\n#### Other submissions (no improvement)\n\nGradient Boosting vanilla model without textual data."},{"metadata":{"trusted":true,"_uuid":"a9736e9188eb22921b48549a57073b77733ccc7b"},"cell_type":"code","source":"model_gboost_pred_test = model_gboost.predict(X)\nmodel_gboost_pred_test = np.expm1(model_gboost_pred_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4d270d488cab5bad92ef789654f3a86bae5efa60"},"cell_type":"code","source":"submission = pd.read_csv('../input/sample_submission.csv')\nsubmission['revenue'] = model_gboost_pred_test\nsubmission.to_csv('submission_gboost.csv', index = False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1b615ad1817c313b678febb4f6ea980c8e4c1efe"},"cell_type":"markdown","source":"Ensembling CatBoost & Gradient Boosting. Yields 2.02841"},{"metadata":{"trusted":true,"_uuid":"cc7c673b66319832cdd48b130f1e9a025b588245"},"cell_type":"code","source":"model_ensemble_pred_test = (model_CatBoost_pred_test + model_gboost_pred_test)/2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a5484285f6343136da15db1fc68b2da2c16074e6"},"cell_type":"code","source":"submission = pd.read_csv('../input/sample_submission.csv')\nsubmission['revenue'] = model_ensemble_pred_test\nsubmission.to_csv('submission_ensemble.csv', index = False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}