{"cells":[{"metadata":{"_uuid":"c2f4cb5827370294c9eb053cfef7c554f1fd6f55"},"cell_type":"markdown","source":"<a id='999'></a>\n# TMDB Box Office Prediction\n\nThis notebook shows all common processes for traditional machine learning step by step: including from data loading, feature engineering, feature selection, model selection, and to the final prediction. \n\nWe have a little data to predict TMDB movie's office box revenue: only 3,000 data items for the train and 4,400 for the test. It results in that using modern ML techniques is not appropriate for this model but we have to use traditional ML algorithms, where domain knowledge is needed for major steps of building model such as feature engineering and feature selection. \n\n<img src='https://storage.googleapis.com/kaggle-competitions/kaggle/10300/logos/thumb76_76.png' align='right' />\n\n1. [Loading Data](#1)\n1. [Feature Engineering](#2)\n    1. [Convert JSON features](#21)\n    1. [Drop some features](#22)\n    1. [Missing values](#23)\n    1. [Creation of new features](#24)\n1. [Feature selection](#3)\n    1. [Intuitive analysis](#31)\n        1. [Numerical features](#310)\n        1. [Text length features](#311)\n        1. [Release date features](#312)\n        1. [Genre features](#313)\n        1. [Count features](#314)\n        1. [Language, Country](#315)\n    1. [Summary of Intuitive analysis](#32)\n    1. [Automatic Feature Selection](#33)\n        1. [Univariate Selection](#331)\n        1. [Recursive Feature Elimination](#332)\n        1. [Principal Component Analysis](#333)\n        1. [Extra Trees Regressor](#334)\n    1. [Summary of Automatic Feature Selection](#34)\n1. [Model Selection](#4)\n    1. [Normalization of skewed data](#41)\n    1. [Model Selection](#42)\n    1. [Feature selection for the best model](#43)\n1. [Train and Predict](#5)\n\n\n6. **[Further Research Work](#6)**\n    1. [Can sub-prediction improve the overall accuracy?](#61)\n    1. [Additive Feature Selection](#62)"},{"metadata":{"_uuid":"3e77fe89648ec296a679f08619109c5568339e4d"},"cell_type":"markdown","source":"# 1. Loading Data<a id='1'></a>"},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport matplotlib.pylab as pylab\nimport seaborn as sns\n\nmpl.style.use('ggplot')\nsns.set_style('white')\npylab.rcParams['figure.figsize'] = 12,8\n\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import cross_validate\n\nfrom sklearn.feature_selection import RFECV\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\n\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.neural_network import MLPRegressor\n\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\nfrom catboost import CatBoostRegressor\n\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import StandardScaler\n\nimport random\nimport time\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0f93696e3cc85da713eb627bf7d94727581445db","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"train = pd.read_csv('../input/train.csv')\ntest = pd.read_csv('../input/test.csv')\n\nprint('train dataset size:', train.shape)\nprint('test dataset size:', test.shape)\ntrain.sample(4)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"df38b324e872d744e6d3855f2c2713384557a47f"},"cell_type":"markdown","source":"There are 8 JSON-style features, 4 numerical, 4 text, and 1 date feature."},{"metadata":{"_uuid":"d9dbe5eed2d5e3819f607316a2e5e1e3634ff947","trusted":true,"_kg_hide-output":false},"cell_type":"code","source":"train.info()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c392bbb483edc27a55c5f44e1b3e8fd03055709d"},"cell_type":"markdown","source":"<a id='2'></a>\n# [2. Feature Engineering](#999)\n\nAt first, convert JSON-styled features into string/category/list ones.\n\n<a id='21'></a>\n## [2.1 Convert JSON features](#999)\n\n* **`belongs_to_collection`**: convert `name` into string\n* **`genres`, `production_companies`**: convert `name` values into comma-separated string list\n* **`production_countries`**: convert `iso_3166_1` values into comma-separated string list\n* **`spoken_languages`**: convert `iso_639_1` values into comma-separated string list\n* **`Keywords`**: convert `name` values into comma-separated string list\n* **`cast`, `crew`**: get their lengths, as its detailed information is very unlikely relevant to the revenue "},{"metadata":{"_uuid":"1d52b6a799f44986a219f996c6a971b8d2add155","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def proc_json(string, key):\n    try:\n        data = eval(string)\n        return \",\".join([d[key] for d in data])\n    except:\n        return ''\n\ndef proc_json_len(string):\n    try:\n        data = eval(string)\n        return len(data)\n    except:\n        return 0\n\ntrain.belongs_to_collection = train.belongs_to_collection.apply(lambda x: proc_json(x, 'name'))\ntest.belongs_to_collection = test.belongs_to_collection.apply(lambda x: proc_json(x, 'name'))\n\ntrain.genres = train.genres.apply(lambda x: proc_json(x, 'name'))\ntest.genres = test.genres.apply(lambda x: proc_json(x, 'name'))\n\ntrain.production_companies = train.production_companies.apply(lambda x: proc_json(x, 'name'))\ntest.production_companies = test.production_companies.apply(lambda x: proc_json(x, 'name'))\n\ntrain.production_countries = train.production_countries.apply(lambda x: proc_json(x, 'iso_3166_1'))\ntest.production_countries = test.production_countries.apply(lambda x: proc_json(x, 'iso_3166_1'))\n\ntrain.spoken_languages = train.spoken_languages.apply(lambda x: proc_json(x, 'iso_639_1'))\ntest.spoken_languages = test.spoken_languages.apply(lambda x: proc_json(x, 'iso_639_1'))\n\ntrain.Keywords = train.Keywords.apply(lambda x: proc_json(x, 'name'))\ntest.Keywords = test.Keywords.apply(lambda x: proc_json(x, 'name'))\n\ntrain.cast = train.cast.apply(proc_json_len)\ntest.cast = test.cast.apply(proc_json_len)\n\ntrain.crew = train.crew.apply(proc_json_len)\ntest.crew = test.crew.apply(proc_json_len)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"165fe1211828c224695cea7de729d34fe21b0153"},"cell_type":"markdown","source":"Though belongs_to_collection has many missing values, movies in the same collection have similar budgets and revenues. Keep it just now and consider its relevance later."},{"metadata":{"_uuid":"9b45ad70c8e7b9fcf25077b23704abca16a00535","trusted":true},"cell_type":"code","source":"train.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"641a3bb142520aad8ec31e75d88d96c45a168444"},"cell_type":"markdown","source":"<a id='22'></a>\n## [2.2. Drop some features](#999)\nIt's clearly obvious that ID/URL features are not useful for the prediction of revenue and I will drop them.\n\nAnd text features such as `title` and `overview` are not likely useful by itself. For now I use only length information of text columns, but when I feel like to need more features, I can reconsider extraction some information from these columns through NLP."},{"metadata":{"_uuid":"f884391d4bd97b7cbbc2675c33ebc3185a6c08d2","trusted":true},"cell_type":"code","source":"# get lengths of text columns\ncolumns = ['original_title', 'title', 'overview', 'tagline']\nfor col in columns:\n    new_col = col + '_len'\n    train[new_col] = train[col].apply(lambda x: 0 if x is np.nan else len(x))\n    test[new_col] = test[col].apply(lambda x: 0 if x is np.nan else len(x))\n\n# drop ID/URL/text columns\ncolumns.extend(['homepage', 'imdb_id', 'poster_path'])\n\ntrain.drop(columns, axis=1, inplace=True)\ntest.drop(columns, axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"deff9034c45fea1491f4a25699b7f4c5876a0442"},"cell_type":"markdown","source":"<a id='23'></a>\n## [2.3. Missing values](#999)"},{"metadata":{"_uuid":"89c85ddf014643209af604ff096744d55300d818","trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"print('-'*30, '\\n', train.isnull().sum())\nprint('-'*30, '\\n', test.isnull().sum())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"308c88b9ccdd813ce1c0fbc1f88516c29e539190"},"cell_type":"markdown","source":"### - runtime\n`runtime` column has 2 and 4 missing values for the train and test dataset respectively. I can't understand what it means, but there are some 0 values of `runtime` column. So, fill the missing values of `runtime` column with 0."},{"metadata":{"_uuid":"e6e48c23feff5a92637af01bb6c58244848f0cc0","trusted":true},"cell_type":"code","source":"train.runtime = train.runtime.fillna(0)\ntest.runtime = test.runtime.fillna(0)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"01b70d147f29ccbd98fd379a5a167a4060c0fbfb"},"cell_type":"markdown","source":"### - status\n`status` column has 2 missing values in the test dataset. Fill it with 'Released', the mostly used value."},{"metadata":{"_uuid":"0bf877cc634c64e4589fae3ca81a8af4557c026a","trusted":true},"cell_type":"code","source":"test.loc[test.status.isnull(), 'status'] = 'Released'","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1d54e00f8103b4a562753222404c44c52a5a2b8b"},"cell_type":"markdown","source":"### - release_date\n`release_date` column has one missing value in the test dataset. Fill it with the mostly used value."},{"metadata":{"_uuid":"15f91cd509e6b71879d193c50ee6e619b347b3ac","trusted":true},"cell_type":"code","source":"test.loc[test.release_date.isnull(), 'release_date'] = test.release_date.mode()[0]\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2028c5c98255beaad6109bb639ac17385a51242b"},"cell_type":"markdown","source":"<a id='24'></a>\n## [2.4. Creation of new features](#999)\n\n### - Date related Values"},{"metadata":{"_uuid":"6efdb5586526de8816c97e886b06c63c2e641092","trusted":true},"cell_type":"code","source":"def expand_release_date(df):\n    df.release_date = pd.to_datetime(df.release_date)\n\n    df['release_year'] = df.release_date.dt.year\n    df['release_month'] = df.release_date.dt.month\n    df['release_day'] = df.release_date.dt.dayofweek\n    df['release_quarter'] = df.release_date.dt.quarter\n    \n    return df\n\ntrain = expand_release_date(train)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b9384aa0734c1221714089cc270673d949ea1762","trusted":true},"cell_type":"code","source":"train[['release_year', 'release_month', 'release_day', 'release_quarter']].describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"653bc4de796822a026efd134824bf0abb343f8fc"},"cell_type":"markdown","source":"# ðŸ˜² \nOoop! The maximum of `release_year` is 2068! \n\nThe year values have only two digits and the years before 1969 are denoted as ones of 2000's. Make it correct."},{"metadata":{"_uuid":"30f2a355ff3895fff93061ab40e5f03bf75d99c7","trusted":true},"cell_type":"code","source":"def expand_release_date(df):\n    df.release_date = pd.to_datetime(df.release_date)\n\n    df['release_year'] = df.release_date.dt.year\n    df['release_year'] = df.release_year.apply(lambda x: x-100 if x > 2020 else x)\n    \n    df['release_month'] = df.release_date.dt.month\n    df['release_day'] = df.release_date.dt.dayofweek\n    df['release_quarter'] = df.release_date.dt.quarter\n    \n    return df\n\ntrain = expand_release_date(train)\ntest = expand_release_date(test)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"87b742c43606a54d6d25c357beeeaa15d89496a5"},"cell_type":"markdown","source":"### - Genres\nAs 0+ genres for each movie, it is not a reasonable way to convert `genres` column into category type. It might make the same genres different, e.g. 'Drama,Romance' and 'Romance,Drama' would be categorized differently.\nTherefore I made dummy columns for all of the genres."},{"metadata":{"_uuid":"1f07d7b6e1079b665c62b45c8a00e6d23b3d24ab","trusted":true},"cell_type":"code","source":"# get total genres list\ngenres = []\nfor idx, val in train.genres.iteritems():\n    gen_list = val.split(',')\n    for gen in gen_list:\n        if gen == '':\n            continue\n\n        if gen not in genres:\n            genres.append(gen)\n            \n\ngenre_column_names = []\nfor gen in genres:\n    col_name = 'genre_' + gen.replace(' ', '_')\n    train[col_name] = train.genres.str.contains(gen).astype('uint8')\n    test[col_name] = test.genres.str.contains(gen).astype('uint8')\n    genre_column_names.append(col_name)\n\ntrain.sample(5)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"08d57caa28680610126f21a1d34ac26d025df0fe"},"cell_type":"markdown","source":"It's not certain if genre count is relevant to the revenue, but calculate it now and test later."},{"metadata":{"_uuid":"85cabc5e46099807586aa0c2fd0d6c1c26dcdba3","trusted":true},"cell_type":"code","source":"train['genre_count'] = train.genres.apply(lambda x: len(x.split(',')))\ntest['genre_count'] = test.genres.apply(lambda x: len(x.split(',')))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"954b04b07e114f0a4611a8ea25ca86be6541222f"},"cell_type":"markdown","source":"### - Country & Company"},{"metadata":{"_uuid":"72c4c2dd9bb0d71547115f3ec469988589ec99c1","trusted":true},"cell_type":"code","source":"train['country_us'] = train.production_countries.str.contains('US').astype('uint8')\ntrain['country_count'] = train.production_countries.apply(lambda x: len(x.split(',')))\ntrain['company_count'] = train.production_companies.apply(lambda x: len(x.split(',')))\n\ntest['country_us'] = test.production_countries.str.contains('US').astype('uint8')\ntest['country_count'] = test.production_countries.apply(lambda x: len(x.split(',')))\ntest['company_count'] = test.production_companies.apply(lambda x: len(x.split(',')))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"23dd0e1995ad1ac14fd3485c9328847e5a4bcbe0"},"cell_type":"markdown","source":"### - Language"},{"metadata":{"_uuid":"51bec0e00ed8c1d4321ed22e5226acb78121df0b","trusted":true},"cell_type":"code","source":"train['slc'] = train.spoken_languages.apply(lambda x: len(x.split(',')))\ntrain['orig_lang_code'] = train.original_language.astype('category').cat.codes\n\ntest['slc'] = test.spoken_languages.apply(lambda x: len(x.split(',')))\ntest['orig_lang_code'] = test.original_language.astype('category').cat.codes","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"60de83e42098817033af66f96a5e41d99d800697"},"cell_type":"markdown","source":"### - crew and cast\n`crew` and `cast` have been converted into the total number when analysing JSON values. I created a new feature by adding those 2 values."},{"metadata":{"_uuid":"75587260540a74a58cb38b63ee11b52f67cf8cb0","trusted":true},"cell_type":"code","source":"train['total_staff'] = train.cast + train.crew\ntest['total_staff'] = test.cast + test.crew","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"011d99d8b572bb707aaddd2d276687b1844feb27"},"cell_type":"markdown","source":"<a id='3'></a>\n# [3. Feature selection](#999)\n\nNow, I have nearly 50 columns in the train dataset.\nBut there are still some more places for new features: belongs_to_collection, status, Keywords, and text columns I have dropped at the beginning. \n\nThere are lots of ways for feature selection using sklearn, but I'd like to see charts to know potencial relevances of features to the target value. After looking at some charts directly and I can apply various methods to calculate feature importance.\n\nRelated references:\n- From Wikipedia, the free encyclopedia, [Feature selection](https://en.wikipedia.org/wiki/Feature_selection)\n- Jason Brownlee, [Feature Selection For Machine Learning in Python](https://machinelearningmastery.com/feature-selection-machine-learning-python/)\n- Sudharsan Asaithambi, [Why, How and When to apply Feature Selection](https://towardsdatascience.com/why-how-and-when-to-apply-feature-selection-e9c69adfabf2)\n- Kaggle Kernel, [6 Ways for Feature Selection](https://www.kaggle.com/sz8416/6-ways-for-feature-selection)\n- Matthew Mayo, [Step Forward Feature Selection: A Practical Example in Python](https://www.kdnuggets.com/2018/06/step-forward-feature-selection-python.html)\n\n<a id='31'></a>\n## [3.1 Intuitive analysis](#999)\n\n<a id='310'></a>\n### - [Numerical features](#999)\n\n    * Relationship between Numerical Features"},{"metadata":{"_uuid":"323a1a37dfc470fa4892dd8af80a95fa512902a7","scrolled":false,"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"numerical_data = train[['budget', 'popularity', 'runtime', 'cast', 'crew', 'total_staff', 'revenue']]\ng = sns.PairGrid(numerical_data)\ng = g.map_diag(plt.hist, bins=10)\ng = g.map_offdiag(plt.scatter, s=5, alpha=.9, linewidth=.5)\ng = g.add_legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5882e0099cfc9a466d46227cd71240d09c9af594","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"plt.figure(figsize=(8,6))\nsns.heatmap(numerical_data.corr(), annot=True, fmt='.2', center=0.0, cmap='RdBu_r')\nplt.title('Correlation between Numerical Features')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1074149a38f502e10a1d4d0c06fb74c602c372b0"},"cell_type":"markdown","source":"**TIPS**: All numerical features are highly related to the revenue. `total_staff`, which is calculated by adding `cast` and `crew`, shows some interesting characteristics. It is high relevant to the most of the numerical value.\n\n<a id='311'></a>\n### - [Text length features](#999)\n\n    * Distribution of Text Length Features"},{"metadata":{"_uuid":"c929191fdf0e9660492e101b770fb7e22825fc8b","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"cols = ['original_title_len', 'title_len', 'overview_len', 'tagline_len']\n\nplt.figure(figsize=(16, 8))\nfor idx, col in enumerate(cols):\n    plt.subplot(2, 4, idx+1)\n    plt.hist(train[col], bins=15)\n    plt.title(col)\n    \n    plt.subplot(2, 4, idx+5)\n    plt.scatter(x=train[col], y=train.revenue, alpha=.4, marker='+')\n    \nplt.show()   ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f4ec8b8662744568a2d52680889c9fdfdcaca7cb","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"cols.append('revenue')\nplt.figure(figsize=(8,6))\nsns.heatmap(train[cols].corr(), annot=True, fmt='.2', center=0.0, cmap='RdBu_r')\nplt.title('Correlation of Text Length Features')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"85d1b9858723bb2391de9e08021f7424f0512033"},"cell_type":"markdown","source":"**Tips**: Text lengths except `original_title_len` are not likely relevant to the revenue. `original_title_len` and `title_len` are highly related to each other."},{"metadata":{"_uuid":"5647815fe0e0c20f68b5a843e300bccf326080a4"},"cell_type":"markdown","source":"<a id='312'></a>\n### - [Release date features](#999)"},{"metadata":{"_uuid":"00a4c01a48ea9f3f5236dfa4f4d00674eee98866","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"plt.figure(figsize=(16,3))\nsns.set(style=\"whitegrid\")\nsns.barplot(x='release_year', y='revenue', errwidth=0.5, data=train)\nplt.xticks(rotation=90)\nplt.title('Average Revenue per Year')\nplt.show()\n\nplt.figure(figsize=(16,3))\nax = plt.subplot(131)\nsns.barplot(x='release_month', y='revenue', data=train, ax=ax)\nax.set_title('Average Revenue per Month')\n\nax = plt.subplot(132)\nsns.barplot(x='release_day', y='revenue', data=train, ax=ax)\nax.set_title('Average Revenue per Weekday')\n\nax = plt.subplot(133)\nsns.barplot(x='release_quarter', y='revenue', data=train, ax=ax)\nax.set_title('Average Revenue per Quarter')\nplt.show()\n\nplt.figure(figsize=(8,6))\nsns.heatmap(train[['release_year', 'release_quarter', 'release_month', 'release_day', 'revenue']].corr(), annot=True, fmt='.2', center=0.0, cmap='RdBu_r')\nplt.title('Correlation between Date Features')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"08ad07260089ae50e128e8cbfd331596beac1026"},"cell_type":"markdown","source":"**Tips**: only `release_year` seems to be relevant to the revenue. `release_quater` and `release_month` are highly related to each other.\n\n<a id='313'></a>\n### - [Genre features](#999)"},{"metadata":{"_uuid":"8fa67c816b0ade4edf8e8113abf259b03dcac13e","trusted":true},"cell_type":"code","source":"genre_counts = []\ngenre_avg_revenues = []\n\nfor col in genre_column_names:\n    genre_counts.append(train[col].sum())\n    genre_avg_revenues.append(train.loc[train[col] == 1, 'revenue'].mean())\n\ngenre_df = pd.DataFrame({'genre': genres, 'counts': genre_counts, 'revenue': genre_avg_revenues})    \n\n_, (ax0, ax1) = plt.subplots(nrows=1, ncols=2, sharey=True, figsize=(8, 8))\ngenre_df.plot.barh(x='genre', y='counts', ax=ax0, legend=False)\nax0.set_title('Movie counts')\n\ngenre_df.plot.barh(x='genre', y='revenue', ax=ax1, legend=False)\nax1.set_title('Average Revenues')\n\nplt.subplots_adjust(wspace=0.01)\nplt.show()\n\ncols = genre_column_names[:]\ncols.append('revenue')\nplt.figure(figsize=(20, 12))\nsns.heatmap(train[cols].corr(), annot=True, fmt='.2g', center=0.0, cmap='RdBu_r')\nplt.title('Movie Genres\\' Correlation with Revenue')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7a93acea19ff1c1a3c23a5a48187dc6691f8338f"},"cell_type":"markdown","source":"**TIPS**: Few genres with high relevance to the revenue have relatively large number of movies and high revenue. They are Adventure, Action, Fantasy, Drama, Family, Animation, and Science Fiction.\n\n<a id='314'></a>\n### - [Count features](#999)\nI have made 4 count features: genre_count, country_count, company_count, and slc(spoken language count). Let's see them."},{"metadata":{"_uuid":"1f70ad2436e36910e3f838f1a7bef138387b3462","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"_, ((ax0, ax1), (ax2, ax3)) = plt.subplots(nrows=2, ncols=2, figsize=(16, 10))\nsns.boxplot(x='genre_count', y='revenue', ax=ax0, data=train)\nsns.boxplot(x='slc', y='revenue', ax=ax1, data=train)\nsns.boxplot(x='country_count', y='revenue', ax=ax2, data=train)\nsns.boxplot(x='company_count', y='revenue', ax=ax3, data=train)\nplt.show()\n\nplt.figure(figsize=(8,6))\nsns.heatmap(train[['genre_count', 'country_count', 'company_count', 'slc', 'revenue']].corr(), annot=True, fmt='.2', center=0.0, cmap='coolwarm')\nplt.title('Correlation between Count Features')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"873be0b6a54d1e5641a409f1390ec0f13d837676"},"cell_type":"markdown","source":"**TIPS**: all the count-related features seem to be useless. :-(\n\n<a id='315'></a>\n### - [Language, Country](#999)\n"},{"metadata":{"_uuid":"668aca2acae762309af67da34acefd56272f6d15","scrolled":false,"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"_, (ax0, ax1) = plt.subplots(nrows=1, ncols=2, figsize=(16, 8))\nsns.boxplot(x='country_us', y='revenue', ax=ax0, data=train)\nax0.set_title('Revenue per Country_US')\nsns.boxplot(x='orig_lang_code', y='revenue', ax=ax1, data=train)\nax1.set_title('Revenue per Original Language Code')\nplt.show()\n\nplt.figure(figsize=(6, 4))\nsns.heatmap(train[['country_us', 'orig_lang_code', 'revenue']].corr(), annot=True, fmt='.2', center=0.0, cmap='coolwarm')\nplt.title('Correlation of Language Features with Revenue')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bd46945535d9f5f3869c450ba4c9b65ef2d030b9"},"cell_type":"markdown","source":"**TIPS**: US movies produce high revenues and country_us is likey a useful feature. Language is not relevent to the revenue.\n\n<a id='32'></a>\n## [3.2 Summary of Intuitive analysis](#999)\n\n* `budget`, `popularity`, `total_staff`, and `runtime` are great features for the model.(`cast` and `crew` are better to be ignored as they give duplicated information with `total_staff`.\n* `original_title_len` is a little bit related to the revenue. \n* `release_year` and `release_day` are OK.\n* Selected genres are `Adventure`, `Action`, `Fantasy`, `Drama`, `Family`, `Animation`, and `Science Fiction`.\n* `genre_count` and `company_count` are needed to be considered more.\n* `country_us` is good."},{"metadata":{"_uuid":"6fa7775184bfff1bfdfd6eea8d961842061cc35a","trusted":true},"cell_type":"code","source":"manually_selected_features = ['budget', 'popularity', 'total_staff', 'runtime', 'original_title_len', \n                              'release_year', 'release_day', 'genre_Adventure', 'genre_Action', 'genre_Fantasy', \n                              'genre_Drama', 'genre_Family', 'genre_Animation', 'genre_Science_Fiction',\n                              'genre_count', 'company_count', 'country_us']","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cfd176a05223fd8f428dc91c364bec5dabc06c6b"},"cell_type":"markdown","source":"<a id='33'></a>\n## [3.3 Automatic Feature Selection](#999)\n\nThere are several ways using sklearn library to select suitable features for the model. I personally like this blog '[Feature Selection For Machine Learning in Python](https://machinelearningmastery.com/feature-selection-machine-learning-python/)' written by Jason Brownlee. I applied 4 methods mentioned in the blog.\n\n<a id='331'></a>\n### - [Univariate Selection](#999)"},{"metadata":{"_uuid":"1bfd200b3635ff6d73543e4210910292bd1ea820","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"features = train.select_dtypes(include=['int64', 'float64', 'uint8', 'int8']).columns.tolist()\nfeatures.remove('id')\nfeatures.remove('revenue')\n\nX, Y = train[features], train['revenue']\n\nuni_test = SelectKBest(score_func=chi2, k='all')\nfit = uni_test.fit(X, Y)\n\nfeature_df = pd.DataFrame({'feature': features, 'importance': np.log10(fit.scores_)})\nfeature_df.sort_values(by='importance', ascending=True, inplace=True)\n\nfeature_df.plot.barh(x='feature', y='importance', figsize=(12, 12))\nplt.title('Feature Importance by Univariate Selection')\n\nfeatures1 = feature_df.feature[-20:].tolist()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"66b6575213e70b476d6a6dc49ea7d7763917dbbe"},"cell_type":"markdown","source":"The result shown in the chart is a little different from my intuitive analysis. Let's go on with other methods.\n\n<a id='332'></a>\n### - [Recursive Feature Elimination](#999)\n\nRFECV(RandomForestRegressor) makes different results for every run. So I iterated this process several times and aggregate the result."},{"metadata":{"_uuid":"e04a58d3321c52ac67aa624d0b757904b0df0fc8","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"from tqdm import tqdm\n\nimportances = np.zeros(len(features))\n\nfor i in tqdm(range(10)):\n    model = RandomForestRegressor()\n    rfecv = RFECV(model, cv=5)\n    fit = rfecv.fit(X, Y)\n\n    selected = np.array(fit.support_)\n    importances = importances + selected\n\nfeature_df = pd.DataFrame({'feature': features, 'importance': importances})\nfeature_df.sort_values(by='importance', ascending=True, inplace=True)\n\nfeature_df.plot.barh(x='feature', y='importance', figsize=(10, 12))\nplt.title('Feature Importance by RFECV & random forest regressor')    \n\nfeatures2 = feature_df.loc[feature_df.importance==10, 'feature'].tolist()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7f1747799f9a15cce7e16e445484f274a528be41"},"cell_type":"markdown","source":"9 features are always selected in 10 tests.\n\n<a id='333'></a>\n### - [Principal Component Analysis](#999)\n\nPrincipal Component Analysis (or PCA) uses linear algebra to transform the dataset into a compressed form.\n\nGenerally this is called a data reduction technique. A property of PCA is that you can choose the number of dimensions or principal component in the transformed result."},{"metadata":{"_uuid":"a0d3ad8aeaa25fb99069f1ae36b3431828b79296","trusted":true},"cell_type":"code","source":"from sklearn.decomposition import PCA\npca = PCA(n_components=3)\nfit = pca.fit(X)\n\nprint(pca.explained_variance_ratio_)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f9bec1377fce6ad0bdd6ebcdb3e2644d4ac9ccae"},"cell_type":"markdown","source":"The first PC shows the full variance, so we calculated feature importances from the first PC."},{"metadata":{"_uuid":"0c432d40df0be6d9680439c7b2304f7d8d0c1950","trusted":true},"cell_type":"code","source":"feature_df = pd.DataFrame({'feature': features, 'importance': abs( pca.components_[0])})\nfeature_df.sort_values(by='importance', ascending=False, inplace=True)\n\nfeatures3 = feature_df.feature[:20]\nfeatures3","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"61fcfef9ba32c1ed56b0797affb4573f1cf43ab8"},"cell_type":"markdown","source":"<a id='334'></a>\n### - [Extra Trees Regressor](#999)\n\nBagged decision trees like Random Forest and Extra Trees can be used to estimate the importance of features.\n\nTop 20 important features are:"},{"metadata":{"_uuid":"95e8dd375e702e476da2b4d4ab8375a0c3e7320e","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"from sklearn.ensemble import ExtraTreesRegressor\nmodel = ExtraTreesRegressor()\nmodel.fit(X, Y)\n\nfeature_df = pd.DataFrame({'feature': features, 'importance': model.feature_importances_})\nfeature_df.sort_values(by='importance', ascending=False, inplace=True)\nfeatures4 = feature_df.feature[:20]\nfeatures4","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4f058a5595237c9f2678142ade71b0d6c5374217"},"cell_type":"markdown","source":"<a id='34'></a>\n## 3.4 [Summary of Automatic Feature Selection](#999)\n\nThe more features, the more likely overfitting, and the more time training consume. From the feature selection results, I tried to select less than 10 features.\n\nAs we can see above, the selected features are slightly different according to the model. That's why we should do feature selection along with model selection.\n\nFinally, we decided the automatically selected feature as the intersection of  4 feature sets calculated above:"},{"metadata":{"_uuid":"5d6f090c95278df336dda8a7ad1ac4baa1dd17d2","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"automatically_selected_features = list( set(features1) & set(features2) & set(features3) & set(features4) )\nautomatically_selected_features","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"75d4c1eeb3fceb309cfbe3de79602164843f2bd4"},"cell_type":"markdown","source":"#### features selected by correlation\n\nAnd, as one of the automatic feature selection process, we get the features which have relatively high correlation with the revenue:"},{"metadata":{"_uuid":"f00d2067d335806599fcadc77abb66cb19defe4d","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"features = train.select_dtypes(include=['int64', 'float64', 'uint8', 'int8']).columns.tolist()\nfeatures.remove('id')\nfeatures.remove('revenue')\n\ntarget = 'revenue'\ncorr_features = features[:]\ncorr_features.append(target)\ncorrs = abs(train[corr_features].corr()['revenue']).sort_values(ascending=False)\ncorr_selected_features = corrs[:20].index.tolist()\ncorr_selected_features.remove('revenue')\ncorr_selected_features","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"21c3ecd3e06f1178501e275b1dc99b08cab858fd"},"cell_type":"markdown","source":"## Summary of Feature Selection\nThrough the feature selection process, we got 3 lists of features selected by difference approaches.\n* **manually_selected_features**\n* **automatically_selected_features**\n* **corr_selected_features**\n\nBut feature selection process is not finished yet. I can't say clearly which is the best one just now. I would use corr_selected_features to select the model and after the model selection consider the features once more."},{"metadata":{"_uuid":"f4e6e932e1a1f6c49e1a3c38ac8cffcf028cae53"},"cell_type":"markdown","source":"<a id='4'></a>\n# 4. [Model Selection](#999)"},{"metadata":{"_uuid":"7a145bcf6ef8ce67c1b239ceac9ac15a43aa7ceb"},"cell_type":"markdown","source":"<a id='41'></a>\n## 4.1 [Normalization of skewed data](#999)\n`budget` and `revenue` are highly skewed and they need to be normalized by logarithm."},{"metadata":{"_uuid":"97b457aa9890f2f4d9e194b1783c8e0913381224","trusted":true},"cell_type":"code","source":"train['revenue'] = np.log1p(train['revenue'])\ntrain['budget'] = np.log1p(train['budget'])\n\ntest['budget'] = np.log1p(test['budget'])\n\nX = train[corr_selected_features]\ny = train['revenue']","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f8b51a0e941c7ebd61a8a9ef84b5af71804281c2"},"cell_type":"markdown","source":"<a id='42'></a>\n## 4.2 [Model Selection](#999)\nFrom various regression models, the most accurate model was selected."},{"metadata":{"_uuid":"fb31d6b9b2f08b3db40f3058a5ebaa0a6c276ef9","trusted":false},"cell_type":"code","source":"def select_model(X, Y):\n\n    best_models = {}\n    models = [\n        {\n            'name': 'LinearRegression',\n            'estimator': LinearRegression(),\n            'hyperparameters': {},\n        },\n        {\n            'name': 'KNeighbors',\n            'estimator': KNeighborsRegressor(),\n            'hyperparameters':{\n                'n_neighbors': range(3,50,3),\n                'weights': ['distance', 'uniform'],\n                'algorithm': ['auto'],\n                'leaf_size': list(range(10,51,10)),\n                }\n        },\n        {\n            'name': 'GradientBoostingRegressor',\n            'estimator': GradientBoostingRegressor(),\n            'hyperparameters':{\n                'n_estimators': range(70, 150, 10),\n                'criterion': ['friedman_mse'],\n                'max_depth': [3, 5, 7, 9],\n                'max_features': ['log2', 'sqrt'],\n                'min_samples_leaf': [1, 2, 4],\n                'min_samples_split': [3, 5, 7]\n            }\n            \n        },\n\n        {\n            'name': 'XGBoost',\n            'estimator': XGBRegressor(),\n            'hyperparameters':{\n                'booster': ['gbtree', 'gblinear', 'dart'],\n                'max_depth': range(10, 51, 10),\n                'n_estimators': [200],\n                'nthread': [4],\n                'min_child_weight': range(1, 8, 2),\n                'learning_rate': [.05, .1, .15],\n            }\n        },\n        {\n            'name': 'Light GBM',\n            'estimator': LGBMRegressor(),\n            'hyperparameters':{\n                'max_depth': range(20, 85, 15),\n                'learning_rate': [.01, .05, .1],\n                'num_leaves': [300, 600, 900, 1200],\n                'n_estimators': [200]\n            }\n        },\n        {\n            'name': 'Cat Boost',\n            'estimator': CatBoostRegressor(),\n            'hyperparameters':{\n                'depth': [4, 7, 10],\n                'learning_rate': [.03, .06, .1, .15],\n                'l2_leaf_reg': [1, 4, 7, 9],\n                'iterations': [300]\n            }\n        }\n        \n    ]\n    \n    for model in tqdm(models):\n        # print('\\n', '-'*20, '\\n', model['name'])\n        start = time.perf_counter()\n        grid = GridSearchCV(model['estimator'], param_grid=model['hyperparameters'], cv=5, scoring = \"explained_variance\", verbose=False, n_jobs=-1)\n        grid.fit(X, Y)\n        best_models[model['name']] = {'score': grid.best_score_, 'params': grid.best_params_}\n        run = time.perf_counter() - start\n        # print('accuracy: {}\\n{} --{:.2f} seconds.'.format(str(grid.best_score_), str(grid.best_params_), run))\n        \n    return best_models\n\n#best = select_model(X, y)\n#best","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"54773f7e615737b1c2bfea55043a5ce300da718d"},"cell_type":"markdown","source":"The best model is GradientBoostingRegressor. Now test which feature list is best.\n\n<a id='43'></a>\n## 4.3 [Feature selection for the best model](#999)"},{"metadata":{"_uuid":"379b4b0e51dde5181be18e5f04a38c8205296270","trusted":false},"cell_type":"code","source":"def get_accuracy(features):\n    X, y = train[features], train['revenue']\n    \n    model = GradientBoostingRegressor(criterion='mse', max_depth=6, max_features='sqrt', \n                                      min_samples_leaf=4, min_samples_split=9, n_estimators=110, loss='huber')\n    result = cross_validate(model, X, y, cv=10, scoring=\"explained_variance\", verbose=False, n_jobs=-1)\n    return np.mean(result['test_score'])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c94b05cd45a07d8b5765264c1f1720bff73e119c","trusted":false},"cell_type":"code","source":"all_features = train.select_dtypes(include=['int64', 'float64', 'uint8', 'int8']).columns.tolist()\nall_features.remove('id')\nall_features.remove('revenue')\n\nbest_features = None\nbest_accuracy = 0\n\nfeature_candidates = [all_features, manually_selected_features, automatically_selected_features, corr_selected_features]\nfor flist in feature_candidates:\n    acc = get_accuracy(flist)\n    if acc > best_accuracy:\n        best_accuracy = acc\n        best_features = flist\n        \nprint('The best accuracy is', best_accuracy)\nbest_features","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ca2541512621438305d4faf5266ac18a9067a9d6"},"cell_type":"markdown","source":"<a id='5'></a>\n# 5. [Train and Predict](#999)"},{"metadata":{"_uuid":"bf2f304becbdb17e2f041582f2a2a6c2fb9d4186","trusted":false},"cell_type":"code","source":"model = GradientBoostingRegressor(criterion='mse', max_depth=6, max_features='sqrt', \n                                      min_samples_leaf=4, min_samples_split=9, n_estimators=110, loss='huber')\nmodel.fit(train[best_features], train['revenue'])\npredict = model.predict(test[best_features])\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b94ddf6e0b519fa699023ebdf58bd2b0de4e2fc9","trusted":false},"cell_type":"code","source":"submit = pd.DataFrame({'id': test.id, 'revenue':np.expm1(predict)})\nsubmit.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e5870fc0d6ff3d01e3badb125aa3a9d13ac85516"},"cell_type":"markdown","source":"The accuracy of the model described so far was 2.10761 at most.\n\n\n\n<a id='6'></a>\n# [6. Further Research Work](#999)\n\nThough I have tried pretty much works of feature engineering/selection and model selection, it is only the introduction of the whole work and the accuracy of my model is not high enough. There are still so many gaps in this kernel and now I'd like to study all capabilities of improvement one by one.\n\nPossible parts to be improved are:\n\n* **feature engineering**\n    * there are lots of features to be created.\n    * many missing values\n    * outliers\n* **feature selection**\n    * more feature selection methods\n    * consideration of validation of selected features\n* **model selection**\n    * other models including neural networks\n    * how to improve the performance of a model(specialize for the selected model)\n    * model parameter tunning"},{"metadata":{"_uuid":"d24808b15504fc93be2b1680dda2321f4fc669f7"},"cell_type":"markdown","source":"<a id='61'></a>\n## [6.1 Can sub-prediction improve the overall accuracy?](#999)\n\nWe wonder if the overall accuracy can be improved by a sub-prediction, which means an internal prediction of the system.  The target of a sub-predidction might be missing/outlier values of existing features or an other abstract, intermediate feature.\n\nOf course, there can't be a perfect predictor(if exists, it's not a predictor but an equation), so these sub-predictions may result in accumulation of error.\n\nFor the case of `budget` feature in this competition, which is the most important feature for the prediction of `revenue`, nearly 25% are 0 and this is not normal but kind of missing values. If we can guess these 0 budgets with more than 50% of accuracy, could the overall accuracy be improved even a little? Will guessing be better than taking them as they are?\n\nIt depends. Let't go for it.\n\n------------------------------------------------------------------\nWe had some experiments for the prediction of `budget` outside of this kernel. Its process is really similar to this kernel itself and we made a function which predicts `budget` values.\n\nHere, we used the GradientBoostingRegressor again but it is not because that I am a fan of GBR. We have used the GBR first in this kernel. In this competition the result of our model selection is always the Gradient Boosting Regressor, we are a little bit strange for it, and it shows implicitly that my approach for model selection has a problem. "},{"metadata":{"_uuid":"4cc067c4dfc4de238ddb638823d403ecc4b3cba4","trusted":false},"cell_type":"code","source":"def fill_budget(in_train, in_test):\n\n\n    def fb_proc_json_len(string):\n        try:\n            data = eval(string)\n            return len(data)\n        except:\n            return 0\n\n    test = in_test.copy()\n    test['revenue'] = np.nan\n    total = pd.concat([in_train, test], axis=0)\n\n    ###############################################################\n    # this is for the time after data loading\n    # here, in the last part of kernel, these steps are already done in the previous steps\n    # so, comment them\n    \n    # total.drop(['belongs_to_collection', 'homepage', 'imdb_id', 'poster_path', 'original_title', 'overview', 'status', 'tagline', 'title'], axis=1, inplace=True)    \n    # total.loc[total.runtime.isnull(), 'runtime'] = 0\n    # total.loc[total.release_date.isnull(), 'release_date'] = total.release_date.mode()[0]\n\n    # total['genre_count'] = total.genres.apply(fb_proc_json_len)\n    # total['company_count'] = total.production_companies.apply(fb_proc_json_len)\n    # total.cast = total.cast.apply(fb_proc_json_len)\n    # total.crew = total.crew.apply(fb_proc_json_len)\n\n    # total.release_date = pd.to_datetime(total.release_date)\n    # total['release_year'] = total.release_date.dt.year\n    # total['release_month'] = total.release_date.dt.month\n    ################################################################\n\n    # these are the result of a small feature selection.\n    fb_features = ['popularity', 'runtime', 'cast', 'crew', 'genre_count', \n                   'company_count', 'release_year', 'release_month']\n\n    fb_target = 'budget'\n\n    train = total[total.budget > 0]\n    pred = total[total.budget == 0]\n\n    # and this is the result of a small model selection.\n    model = GradientBoostingRegressor(criterion='mse', learning_rate=0.15, loss='huber', max_depth=6, \n                                  max_features='sqrt', min_samples_leaf=4, min_samples_split=9, n_estimators=110)\n    model.fit(train[fb_features], train[fb_target])\n\n\n    predict = model.predict(pred[fb_features])\n    pred[fb_target] = predict\n\n    out_train = in_train.copy()\n    out_test = in_test.copy()\n\n    for idx, row in out_train[out_train.budget == 0].iterrows():\n        mid = row.id\n        out_train.loc[out_train.id == mid, 'budget'] = pred.loc[pred.id == mid, 'budget']\n\n\n    for idx, row in out_test[out_test.budget == 0].iterrows():\n        mid = row.id\n        out_test.loc[out_test.id == mid, 'budget'] = pred.loc[pred.id == mid, 'budget']\n\n    return (out_train, out_test)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"17a65a7435cdfd88625c4b13f0b299d384ea7f31"},"cell_type":"markdown","source":"- Predict `budget` missing values(zeroes)"},{"metadata":{"_uuid":"8e9c219d37627bca84269b91c94e2b2bab003441","trusted":false},"cell_type":"code","source":"# train, test = fill_budget(train, test)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3a8e8c2a180285e2b8762823f3b9eaad7253729e"},"cell_type":"markdown","source":"- Predict `revenue`s(equal to Chapter 5.)"},{"metadata":{"_uuid":"12aaed398bdd24291d74d4ce89f1a4893cc95239","trusted":false},"cell_type":"code","source":"# model = GradientBoostingRegressor(criterion='mse', max_depth=6, max_features='sqrt', \n#                                   min_samples_leaf=4, min_samples_split=9, n_estimators=110, loss='huber')\n# model.fit(train[best_features], train['revenue'])\n# predict = model.predict(test[best_features])\n\n# submit = pd.DataFrame({'id': test.id, 'revenue':np.expm1(predict)})\n# submit.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fab9828970368c068459ff286dc153ff4f3fcc22"},"cell_type":"markdown","source":"We have tested this sub-prediction, but the result was bad. It downed the accuracy.\n\n** Sub-prediction dosen't improve the overall accuracy.**"},{"metadata":{"_uuid":"8d62145d2798d586cc7457ae8cf04553eda22f0a"},"cell_type":"markdown","source":"<a id='62'></a>\n## [6.2 Forward Feature Selection](#999)\n We have thought and thought how to improve the feature selection and found 2 ways. One is \"Forward feature selection\" and the other is \"Backward feature selection\". But soon after, we have found that they have been already invented and studied by others. Their name is SFS & SBS and you can see its detail in \"[A survey on feature selection methods](http://romisatriawahono.net/lecture/rm/survey/machine%20learning/Chandrashekar%20-%20Feature%20Selection%20Methods%20-%202014.pdf)\".\n \n We have implemented and tested those two methods and found that SFS(adding feature selection) contributes but SBS doesn't.\n \n Here, we will introduce the SFS method."},{"metadata":{"_uuid":"486cc6f66ee65c7f70a1edd0600d0e861e5c7f45"},"cell_type":"markdown","source":"### Adding Feature Selection"},{"metadata":{"_uuid":"15842bdd97f4a6bd223d1b10158396e7ddc9c48b","trusted":false},"cell_type":"code","source":"from mlxtend.feature_selection import SequentialFeatureSelector as SFS\nfrom mlxtend.plotting import plot_sequential_feature_selection as plot_sfs\n\ncandidates = train.select_dtypes(include=['int64', 'float64', 'uint8', 'int8']).columns.tolist()\ncandidates.remove('id')\ncandidates.remove('revenue')\n\nX, y = train[candidates], train['revenue']\n\nmodel = GradientBoostingRegressor(criterion='mse', max_depth=6, max_features='sqrt', \n                                  min_samples_leaf=4, min_samples_split=9, n_estimators=110, loss='huber')\n    \nsfs = SFS(estimator=model, \n           k_features=(3, 9),\n           forward=True, \n           floating=False, \n           scoring='neg_mean_squared_error',\n           cv=5)\n\nsfs.fit(X, y, custom_feature_names=candidates)\n\nprint('best combination (ACC: %.3f): %s\\n' % (sfs.k_score_, sfs.k_feature_idx_))\nprint('all subsets:\\n', sfs.subsets_)\n\nfig = plot_sfs(sfs.get_metric_dict(), kind='std_err')\nplt.title('Sequential Forward Selection (w. StdErr)')\nplt.grid()\nplt.show()\n\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b1dea09ba143a4beec2b373e91779426c793beee","trusted":false},"cell_type":"code","source":"sfs_fatures = ['budget', 'popularity', 'runtime', 'tagline_len', 'release_year', 'genre_Drama', 'genre_Family', 'genre_Thriller', 'genre_Crime']\nmodel = GradientBoostingRegressor(criterion='friedman_mse', max_depth=5, max_features='sqrt', \n                                      min_samples_leaf=4, min_samples_split=3, n_estimators=110, random_state=1)\nmodel.fit(train[sfs_fatures], train['revenue'])\npredict = model.predict(test[sfs_fatures])\n\nsubmit = pd.DataFrame({'id': test.id, 'revenue':np.expm1(predict)})\nsubmit.to_csv('submission.csv', index=False)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6db38c2c4a26fde1c563864c399fa8f666c33209"},"cell_type":"markdown","source":"It result in a little improvement in the public score. But SBS didn't give us any pleasure."},{"metadata":{"_uuid":"ac83e74d74efd4a436dfcf8503805c613207cf41"},"cell_type":"markdown","source":"** Our research goes on..**"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}