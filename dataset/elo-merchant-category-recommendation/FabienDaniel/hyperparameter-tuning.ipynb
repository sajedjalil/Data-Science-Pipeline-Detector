{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom datetime import datetime\nimport gc\nimport warnings\nfrom bayes_opt import BayesianOptimization\n\nimport numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport lightgbm as lgb\nfrom sklearn.model_selection import KFold\nimport warnings\nimport time\nimport sys\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import mean_squared_error\nfrom datetime import datetime\nwarnings.simplefilter(action='ignore', category=FutureWarning)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4840a851241d5e17992b5cd1fbd8b4d663855213"},"cell_type":"code","source":"def timer(start_time=None):\n    if not start_time:\n        start_time = datetime.now()\n        return start_time\n    elif start_time:\n        thour, temp_sec = divmod(\n            (datetime.now() - start_time).total_seconds(), 3600)\n        tmin, tsec = divmod(temp_sec, 60)\n        print('\\n Time taken: %i hours %i minutes and %s seconds.' % (thour, tmin, round(tsec, 2)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d99aaf3dd579d277d385d37171253e4340fe9bdf"},"cell_type":"code","source":"def reduce_mem_usage(df):\n    start_mem = df.memory_usage().sum() / 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() / 1024**2\n    print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bee6068c8748de461b05978525ff4fc3ebc58aaf"},"cell_type":"code","source":"train = pd.read_csv(\"../input/elo-world/train.csv\", index_col=0)\ntrain = reduce_mem_usage(train)\n\ntarget = train['target']\ndel train['target']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3c6830e69f9a6398d28e2b6a99d83b4c7ea3f81c"},"cell_type":"code","source":"unimportant_features = [\n    'auth_category_2_1.0_mean',\n    'auth_category_2_2.0_mean',\n    'auth_category_2_3.0_mean',\n    'auth_category_2_5.0_mean',\n    'hist_category_2_3.0_mean',\n    'hist_category_2_4.0_mean',\n    'hist_category_2_5.0_mean',\n    'hist_category_3_A_mean',\n    'hist_installments_min',\n    'hist_installments_std',\n    'hist_month_lag_std',\n    'hist_purchase_amount_max',\n    'hist_purchase_month_max',\n    'hist_purchase_month_min',\n    'hist_purchase_month_std',\n    'installments_min_mean',\n    'new_category_2_1.0_mean',\n    'new_category_2_2.0_mean',\n    'new_category_2_3.0_mean',\n    'new_category_2_5.0_mean',\n    'new_city_id_nunique',\n    'new_installments_std',\n    'new_state_id_nunique',\n    'purchase_amount_mean_mean'\n]\nfeatures = [c for c in train.columns if c not in ['card_id', 'first_active_month']]\n#features = [f for f in features if f not in unimportant_features]\ncategorical_feats = [c for c in features if 'feature_' in c]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b0b150eb17144cd74023face62e4fc7e4a0b02f8"},"cell_type":"code","source":"def LGB_CV(\n          max_depth,\n          num_leaves,\n          min_data_in_leaf,\n          feature_fraction,\n          bagging_fraction,\n          lambda_l1\n         ):\n    \n    folds = KFold(n_splits=5, shuffle=True, random_state=15)\n    oof = np.zeros(train.shape[0])\n\n    for fold_, (trn_idx, val_idx) in enumerate(folds.split(train.values, target.values)):\n        print(\"fold nÂ°{}\".format(fold_))\n        trn_data = lgb.Dataset(train.iloc[trn_idx][features],\n                               label=target.iloc[trn_idx],\n                               categorical_feature=categorical_feats)\n        val_data = lgb.Dataset(train.iloc[val_idx][features],\n                               label=target.iloc[val_idx],\n                               categorical_feature=categorical_feats)\n    \n        param = {\n            'num_leaves': int(num_leaves),\n            'min_data_in_leaf': int(min_data_in_leaf), \n            'objective':'regression',\n            'max_depth': int(max_depth),\n            'learning_rate': 0.01,\n            \"boosting\": \"gbdt\",\n            \"feature_fraction\": feature_fraction,\n            \"bagging_freq\": 1,\n            \"bagging_fraction\": bagging_fraction ,\n            \"bagging_seed\": 11,\n            \"metric\": 'rmse',\n            \"lambda_l1\": lambda_l1,\n            \"verbosity\": -1\n        }\n    \n        clf = lgb.train(param,\n                        trn_data,\n                        10000,\n                        valid_sets = [trn_data, val_data],\n                        verbose_eval=500,\n                        early_stopping_rounds = 200)\n        \n        oof[val_idx] = clf.predict(train.iloc[val_idx][features],\n                                   num_iteration=clf.best_iteration)\n        \n        del clf, trn_idx, val_idx\n        gc.collect()\n        \n    return -mean_squared_error(oof, target)**0.5","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"34f35c9381e52cd5c215d31366c8fd4af2af3165"},"cell_type":"code","source":"LGB_BO = BayesianOptimization(LGB_CV, {\n    'max_depth': (4, 10),\n    'num_leaves': (5, 130),\n    'min_data_in_leaf': (10, 150),\n    'feature_fraction': (0.7, 1.0),\n    'bagging_fraction': (0.7, 1.0),\n    'lambda_l1': (0, 6)\n    })","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3c986659aadadb0c046c58cf104b18544a4f1975","scrolled":true},"cell_type":"code","source":"print('-'*126)\n\nstart_time = timer(None)\nwith warnings.catch_warnings():\n    warnings.filterwarnings('ignore')\n    LGB_BO.maximize(init_points=2, n_iter=20, acq='ei', xi=0.0)\ntimer(start_time)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3bc35b2074f14549015f6e5e55995395b74f8ed6"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}