{"cells":[{"metadata":{"_uuid":"cd061b24306ee279ecd757119990ca70b3509d9f"},"cell_type":"markdown","source":"# Background\n- Delicate sampling is needed to avoid unwanted bias.\n- Let's find the good features which give well-distributed folds.\n- To experiment, I forked my kernel and use same features, random number,  parameters and model."},{"metadata":{"_uuid":"1ebfdd1d0b83e3f0a56ba79ed5f56b9b9c59c90e"},"cell_type":"markdown","source":"# I referred a very helpful kernel. Thanks!\n- https://www.kaggle.com/fabiendaniel/elo-world\n- https://www.kaggle.com/youhanlee/stratified-sampling-for-regression-lb-1-4627"},{"metadata":{"_uuid":"24b34f7ceb08c676b2e2567feed962b95864f9c3"},"cell_type":"markdown","source":"- This kernel shows \n- how to use 'new_merchant_transactions.csv' \n- how to use lgb and xgb\n- how to ensemble them."},{"metadata":{"ExecuteTime":{"end_time":"2018-11-27T18:56:52.172046Z","start_time":"2018-11-27T18:56:48.781792Z"},"trusted":true,"_uuid":"404cf8e0e81d03dae437ff45fbcad13b65481bd4","_kg_hide-input":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport lightgbm as lgb\nfrom sklearn.model_selection import KFold\nimport warnings\nimport time\nimport sys\nimport datetime\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import mean_squared_error\nwarnings.simplefilter(action='ignore', category=FutureWarning)\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4965e9fcd356569f7031cadc8b078f39075bf164"},"cell_type":"markdown","source":"# New transactions"},{"metadata":{"ExecuteTime":{"end_time":"2018-11-27T18:56:55.549404Z","start_time":"2018-11-27T18:56:52.176437Z"},"scrolled":true,"trusted":true,"_uuid":"d3ae8e3ebefe121750ab7cbddcdf41d44541abae"},"cell_type":"code","source":"new_transactions = pd.read_csv('../input/new_merchant_transactions.csv')\n\nnew_transactions['authorized_flag'] = \\\n    new_transactions['authorized_flag'].map({'Y':1, 'N':0})\n\ndef aggregate_new_transactions(new_trans):    \n    agg_func = {\n        'authorized_flag': ['sum', 'mean'],\n        'merchant_id': ['nunique'],\n        'city_id': ['nunique'],\n        'purchase_amount': ['sum', 'median', 'max', 'min', 'std'],\n        'installments': ['sum', 'median', 'max', 'min', 'std'],\n        'month_lag': ['min', 'max']\n        }\n    agg_new_trans = new_trans.groupby(['card_id']).agg(agg_func)\n    agg_new_trans.columns = ['new_' + '_'.join(col).strip() \n                           for col in agg_new_trans.columns.values]\n    agg_new_trans.reset_index(inplace=True)\n    \n    df = (new_trans.groupby('card_id')\n          .size()\n          .reset_index(name='new_transactions_count'))\n    \n    agg_new_trans = pd.merge(df, agg_new_trans, on='card_id', how='left')\n    \n    return agg_new_trans\n\nnew_trans = aggregate_new_transactions(new_transactions)\n\n# historical_transactions\n\nhistorical_transactions = pd.read_csv('../input/historical_transactions.csv')\n\nhistorical_transactions['authorized_flag'] = \\\n    historical_transactions['authorized_flag'].map({'Y':1, 'N':0})\n\ndef aggregate_historical_transactions(history):\n    \n    history.loc[:, 'purchase_date'] = pd.DatetimeIndex(history['purchase_date']).\\\n                                      astype(np.int64) * 1e-9\n    \n    agg_func = {\n        'authorized_flag': ['sum', 'mean'],\n        'merchant_id': ['nunique'],\n        'city_id': ['nunique'],\n        'purchase_amount': ['sum', 'median', 'max', 'min', 'std'],\n        'installments': ['sum', 'median', 'max', 'min', 'std'],\n        'purchase_date': [np.ptp],\n        'month_lag': ['min', 'max']\n        }\n    agg_history = history.groupby(['card_id']).agg(agg_func)\n    agg_history.columns = ['hist_' + '_'.join(col).strip() \n                           for col in agg_history.columns.values]\n    agg_history.reset_index(inplace=True)\n    \n    df = (history.groupby('card_id')\n          .size()\n          .reset_index(name='hist_transactions_count'))\n    \n    agg_history = pd.merge(df, agg_history, on='card_id', how='left')\n    \n    return agg_history\n\nhistory = aggregate_historical_transactions(historical_transactions)\n\ndef read_data(input_file):\n    df = pd.read_csv(input_file)\n    df['first_active_month'] = pd.to_datetime(df['first_active_month'])\n    df['year'] = df['first_active_month'].dt.year\n    df['month'] = df['first_active_month'].dt.month\n    df['elapsed_time'] = (datetime.date(2018, 2, 1) - df['first_active_month'].dt.date).dt.days\n    return df\ntrain = read_data('../input/train.csv')\ntest = read_data('../input/test.csv')\n\ntarget = train['target']\ndel train['target']\n\n# Merge historical transactions and new transactions\n\ntrain = pd.merge(train, history, on='card_id', how='left')\ntest = pd.merge(test, history, on='card_id', how='left')\n\ntrain = pd.merge(train, new_trans, on='card_id', how='left')\ntest = pd.merge(test, new_trans, on='card_id', how='left')\n\n\n# Feature preparation\n\nuse_cols = [col for col in train.columns if col not in ['card_id', 'first_active_month']]\n\ntrain = train[use_cols]\ntest = test[use_cols]\n\nfeatures = list(train[use_cols].columns)\ncategorical_feats = [col for col in features if 'feature_' in col]\n\nfor col in categorical_feats:\n    print(col, 'have', train[col].value_counts().shape[0], 'categories.')\n\n\nfrom sklearn.preprocessing import LabelEncoder\nfor col in categorical_feats:\n    print(col)\n    lbl = LabelEncoder()\n    lbl.fit(list(train[col].values.astype('str')) + list(test[col].values.astype('str')))\n    train[col] = lbl.transform(list(train[col].values.astype('str')))\n    test[col] = lbl.transform(list(test[col].values.astype('str')))\n\ndf_all = pd.concat([train, test])\ndf_all = pd.get_dummies(df_all, columns=categorical_feats)\n\nlen_train = train.shape[0]\n\ntrain = df_all[:len_train]\ntest = df_all[len_train:]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"28d454626a0accd80ccd88e5884532dd9a0af2f0"},"cell_type":"markdown","source":"# ===================== Let's start ====================="},{"metadata":{"_uuid":"22036ffbdeb17f4e5039ae8ebce6844d5601604e"},"cell_type":"markdown","source":"- To get more stable sampling, we can compare the distribution of target with the distribution of target for each fold.\n- Simply, I used the Kolmogorovâ€“Smirnov test.\n"},{"metadata":{"_uuid":"1565c88fa419ab6f183e8772b7d626c8f24270d8"},"cell_type":"markdown","source":"# 1.  Random sampling\n- At first, we use random-sampling to get a control case."},{"metadata":{"trusted":true,"_uuid":"060d0cd11cc73e65aa1a3f87f63452a92ed56833"},"cell_type":"code","source":"from scipy.stats import ks_2samp","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cf9cb16f09e2e09b6db1f3e0a9dbb90044dabedd"},"cell_type":"code","source":"FOLDs = KFold(n_splits=5, shuffle=True, random_state=1989)\ntrn_idxs = []\nval_idxs = []\nfor fold_, (trn_idx, val_idx) in enumerate(FOLDs.split(train)):\n    trn_idxs.append(trn_idx)\n    val_idxs.append(val_idx)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1004b84bbe7beecbcf1f20d149d33dd9815ccad9"},"cell_type":"code","source":"print(ks_2samp(target, target[trn_idxs[0]]))\nprint(ks_2samp(target, target[trn_idxs[1]]))\nprint(ks_2samp(target, target[trn_idxs[2]]))\nprint(ks_2samp(target, target[trn_idxs[3]]))\nprint(ks_2samp(target, target[trn_idxs[4]]))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5d4dc0fc2032eb9acab47f9ccaae7429497e4cdc"},"cell_type":"markdown","source":"- Statistics should go to zero and P-value should go to 1.\n- K-Fold using random sampling shows well-distributed sampling."},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"df3a2aa4469593546aa2276f2ead93fb4f33754e"},"cell_type":"code","source":"lgb_params = {\"objective\" : \"regression\", \"metric\" : \"rmse\", \n               \"max_depth\": 7, \"min_child_samples\": 20, \n               \"reg_alpha\": 1, \"reg_lambda\": 1,\n               \"num_leaves\" : 64, \"learning_rate\" : 0.001, \n               \"subsample\" : 0.8, \"colsample_bytree\" : 0.8, \n               \"verbosity\": -1}\n\nFOLDs = KFold(n_splits=5, shuffle=True, random_state=1989)\n\noof_lgb = np.zeros(len(train))\npredictions_lgb = np.zeros(len(test))\n\nfeatures_lgb = list(train.columns)\nfeature_importance_df_lgb = pd.DataFrame()\n\nfor fold_, (trn_idx, val_idx) in enumerate(FOLDs.split(train)):\n    trn_data = lgb.Dataset(train.iloc[trn_idx], label=target.iloc[trn_idx])\n    val_data = lgb.Dataset(train.iloc[val_idx], label=target.iloc[val_idx])\n\n    print(\"LGB \" + str(fold_) + \"-\" * 50)\n    num_round = 2000\n    clf = lgb.train(lgb_params, trn_data, num_round, valid_sets = [trn_data, val_data], verbose_eval=200, early_stopping_rounds = 2000)\n    oof_lgb[val_idx] = clf.predict(train.iloc[val_idx], num_iteration=clf.best_iteration)\n\n    fold_importance_df_lgb = pd.DataFrame()\n    fold_importance_df_lgb[\"feature\"] = features_lgb\n    fold_importance_df_lgb[\"importance\"] = clf.feature_importance()\n    fold_importance_df_lgb[\"fold\"] = fold_ + 1\n    feature_importance_df_lgb = pd.concat([feature_importance_df_lgb, fold_importance_df_lgb], axis=0)\n    predictions_lgb += clf.predict(test, num_iteration=clf.best_iteration) / FOLDs.n_splits\n    \n\nprint(np.sqrt(mean_squared_error(oof_lgb, target)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"26793c576a5732201cd273d37dc8574ac0d1e292"},"cell_type":"code","source":"corr_result = pd.concat([train, target], axis=1)\nabs(corr_result.corr()['target']).sort_values(ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4a92158ab4416943dbc07531b54cae9022bebf74"},"cell_type":"markdown","source":"- After checking the correlation with target, we know that 'new_purchase_amount_max' and 'elapsed_time' are good candidate."},{"metadata":{"_uuid":"f684862c7c1eed5deaf1e0d62f45098b9106372a"},"cell_type":"markdown","source":"# 2. new_purchase_amount_max"},{"metadata":{"trusted":true,"_uuid":"7fd41408f650dedaab746c245ce29c6f92719165"},"cell_type":"code","source":"from sklearn.model_selection import StratifiedKFold","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"04e3d68bb264a70b1bb0d746ccbf2322d8a8da66"},"cell_type":"markdown","source":"- Statistics should go to zero and P-value should go to 1."},{"metadata":{"trusted":true,"_uuid":"4033e905f3c70460fb4885218df8bf79ac808df7"},"cell_type":"code","source":"new_cat = np.ceil(train['new_purchase_amount_max'].fillna(0)/20).values\n\nFOLDs = StratifiedKFold(n_splits=5, shuffle=True, random_state=1989)\ntrn_idxs = []\nval_idxs = []\nfor fold_, (trn_idx, val_idx) in enumerate(FOLDs.split(train,new_cat)):\n    trn_idxs.append(trn_idx)\n    val_idxs.append(val_idx)\n\nprint(ks_2samp(target, target[trn_idxs[0]]))\nprint(ks_2samp(target, target[trn_idxs[1]]))\nprint(ks_2samp(target, target[trn_idxs[2]]))\nprint(ks_2samp(target, target[trn_idxs[3]]))\nprint(ks_2samp(target, target[trn_idxs[4]]))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4074b3d5e862fcbac76771c9e6a6e010db83b18f"},"cell_type":"markdown","source":"## After manual categorization, 20 is selected as denominator. "},{"metadata":{"trusted":true,"_uuid":"2c985f31a2291a15eb0de34e8fbe99e1ec7a9646"},"cell_type":"code","source":"lgb_params = {\"objective\" : \"regression\", \"metric\" : \"rmse\", \n               \"max_depth\": 7, \"min_child_samples\": 20, \n               \"reg_alpha\": 1, \"reg_lambda\": 1,\n               \"num_leaves\" : 64, \"learning_rate\" : 0.001, \n               \"subsample\" : 0.8, \"colsample_bytree\" : 0.8, \n               \"verbosity\": -1}\n\nFOLDs = StratifiedKFold(n_splits=5, shuffle=True, random_state=1989)\n\noof_lgb = np.zeros(len(train))\npredictions_lgb = np.zeros(len(test))\n\nfeatures_lgb = list(train.columns)\nfeature_importance_df_lgb = pd.DataFrame()\n\nfor fold_, (trn_idx, val_idx) in enumerate(FOLDs.split(train, new_cat)):\n    trn_data = lgb.Dataset(train.iloc[trn_idx], label=target.iloc[trn_idx])\n    val_data = lgb.Dataset(train.iloc[val_idx], label=target.iloc[val_idx])\n\n    print(\"LGB \" + str(fold_) + \"-\" * 50)\n    num_round = 2000\n    clf = lgb.train(lgb_params, trn_data, num_round, valid_sets = [trn_data, val_data], verbose_eval=200, early_stopping_rounds = 2000)\n    oof_lgb[val_idx] = clf.predict(train.iloc[val_idx], num_iteration=clf.best_iteration)\n\n    fold_importance_df_lgb = pd.DataFrame()\n    fold_importance_df_lgb[\"feature\"] = features_lgb\n    fold_importance_df_lgb[\"importance\"] = clf.feature_importance()\n    fold_importance_df_lgb[\"fold\"] = fold_ + 1\n    feature_importance_df_lgb = pd.concat([feature_importance_df_lgb, fold_importance_df_lgb], axis=0)\n    predictions_lgb += clf.predict(test, num_iteration=clf.best_iteration) / FOLDs.n_splits\n    \n\nprint(np.sqrt(mean_squared_error(oof_lgb, target)))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a727490743f6af0c15ec234fe9a79d5b8287bc6c"},"cell_type":"markdown","source":"# 3. elapsed_time "},{"metadata":{"trusted":true,"_uuid":"04c8b45cefe1dd1831fad91b13016ce2efdbd3a1"},"cell_type":"code","source":"new_cat = np.ceil(train['elapsed_time'] / 10).values\n\nFOLDs = StratifiedKFold(n_splits=5, shuffle=True, random_state=1989)\ntrn_idxs = []\nval_idxs = []\nfor fold_, (trn_idx, val_idx) in enumerate(FOLDs.split(train,new_cat)):\n    trn_idxs.append(trn_idx)\n    val_idxs.append(val_idx)\n\nprint(ks_2samp(target, target[trn_idxs[0]]))\nprint(ks_2samp(target, target[trn_idxs[1]]))\nprint(ks_2samp(target, target[trn_idxs[2]]))\nprint(ks_2samp(target, target[trn_idxs[3]]))\nprint(ks_2samp(target, target[trn_idxs[4]]))","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2018-11-27T19:12:11.199094Z","start_time":"2018-11-27T19:05:17.422791Z"},"trusted":true,"scrolled":false,"_uuid":"ea646382df220eed41b31955a8f8abcc242f7915"},"cell_type":"code","source":"lgb_params = {\"objective\" : \"regression\", \"metric\" : \"rmse\", \n               \"max_depth\": 7, \"min_child_samples\": 20, \n               \"reg_alpha\": 1, \"reg_lambda\": 1,\n               \"num_leaves\" : 64, \"learning_rate\" : 0.001, \n               \"subsample\" : 0.8, \"colsample_bytree\" : 0.8, \n               \"verbosity\": -1}\n\nFOLDs = StratifiedKFold(n_splits=5, shuffle=True, random_state=1989)\n\noof_lgb = np.zeros(len(train))\npredictions_lgb = np.zeros(len(test))\n\nfeatures_lgb = list(train.columns)\nfeature_importance_df_lgb = pd.DataFrame()\n\nfor fold_, (trn_idx, val_idx) in enumerate(FOLDs.split(train, new_cat)):\n    trn_data = lgb.Dataset(train.iloc[trn_idx], label=target.iloc[trn_idx])\n    val_data = lgb.Dataset(train.iloc[val_idx], label=target.iloc[val_idx])\n\n    print(\"LGB \" + str(fold_) + \"-\" * 50)\n    num_round = 2000\n    clf = lgb.train(lgb_params, trn_data, num_round, valid_sets = [trn_data, val_data], verbose_eval=200, early_stopping_rounds = 2000)\n    oof_lgb[val_idx] = clf.predict(train.iloc[val_idx], num_iteration=clf.best_iteration)\n\n    fold_importance_df_lgb = pd.DataFrame()\n    fold_importance_df_lgb[\"feature\"] = features_lgb\n    fold_importance_df_lgb[\"importance\"] = clf.feature_importance()\n    fold_importance_df_lgb[\"fold\"] = fold_ + 1\n    feature_importance_df_lgb = pd.concat([feature_importance_df_lgb, fold_importance_df_lgb], axis=0)\n    predictions_lgb += clf.predict(test, num_iteration=clf.best_iteration) / FOLDs.n_splits\n    \n\nprint(np.sqrt(mean_squared_error(oof_lgb, target)))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"198a1f77fa99754ef2b53d5bf3a11f84ee855b08"},"cell_type":"markdown","source":"# XGB"},{"metadata":{"ExecuteTime":{"start_time":"2018-11-27T19:13:04.678Z"},"trusted":true,"_uuid":"48f17e6cae4bf8823bf6a18fd21f82695f106997"},"cell_type":"code","source":"import xgboost as xgb\n\nxgb_params = {'eta': 0.001, 'max_depth': 7, 'subsample': 0.8, 'colsample_bytree': 0.8, \n          'objective': 'reg:linear', 'eval_metric': 'rmse', 'silent': True}\n\n\n\nFOLDs = StratifiedKFold(n_splits=5, shuffle=True, random_state=1989)\n\noof_xgb = np.zeros(len(train))\npredictions_xgb = np.zeros(len(test))\n\n\nfor fold_, (trn_idx, val_idx) in enumerate(FOLDs.split(train, new_cat)):\n    trn_data = xgb.DMatrix(data=train.iloc[trn_idx], label=target.iloc[trn_idx])\n    val_data = xgb.DMatrix(data=train.iloc[val_idx], label=target.iloc[val_idx])\n    watchlist = [(trn_data, 'train'), (val_data, 'valid')]\n    print(\"xgb \" + str(fold_) + \"-\" * 50)\n    num_round = 2000\n    xgb_model = xgb.train(xgb_params, trn_data, num_round, watchlist, early_stopping_rounds=100, verbose_eval=200)\n    oof_xgb[val_idx] = xgb_model.predict(xgb.DMatrix(train.iloc[val_idx]), ntree_limit=xgb_model.best_ntree_limit+50)\n\n    predictions_xgb += xgb_model.predict(xgb.DMatrix(test), ntree_limit=xgb_model.best_ntree_limit+50) / FOLDs.n_splits\n\nnp.sqrt(mean_squared_error(oof_xgb, target))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5b959baeb6b995adc039ad81d49437b37363e4d0"},"cell_type":"code","source":"print('lgb', np.sqrt(mean_squared_error(oof_lgb, target)))\nprint('xgb', np.sqrt(mean_squared_error(oof_xgb, target)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"725d30677a5ad53aee78f8afa7dd9d2f61d999a3"},"cell_type":"code","source":"total_sum = 0.5 * oof_lgb + 0.5 * oof_xgb\nprint(\"CV score: {:<8.5f}\".format(mean_squared_error(total_sum, target)**0.5))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f8cf16263c0da352f0c2b514dd3b8163f650f29f"},"cell_type":"code","source":"cols = (feature_importance_df_lgb[[\"feature\", \"importance\"]]\n        .groupby(\"feature\")\n        .mean()\n        .sort_values(by=\"importance\", ascending=False)[:1000].index)\n\nbest_features = feature_importance_df_lgb.loc[feature_importance_df_lgb.feature.isin(cols)]\n\nplt.figure(figsize=(14,14))\nsns.barplot(x=\"importance\",\n            y=\"feature\",\n            data=best_features.sort_values(by=\"importance\",\n                                           ascending=False))\nplt.title('LightGBM Features (avg over folds)')\nplt.tight_layout()\nplt.savefig('lgbm_importances.png')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6e2a7b9ef480b2246d4d1fd392420437b1c06183"},"cell_type":"code","source":"sub_df = pd.read_csv('../input/sample_submission.csv')\nsub_df[\"target\"] = 0.5 * predictions_lgb + 0.5 * predictions_xgb\nsub_df.to_csv(\"submission.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"2481f0210e50fa05482b9777833cb05c6df4b5ba"},"cell_type":"markdown","source":"- Compared to my previous kernel, the cv score is improved.\n- 3.73413  -> 3.73340 "},{"metadata":{"trusted":true,"_uuid":"f6400d8afdf40a0510b5252f531a1ece45de17c9"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"df4b05483540a97d06e5b4266f391a07f64e34fc"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{},"toc_section_display":true,"toc_window_display":false}},"nbformat":4,"nbformat_minor":1}