{"cells":[{"metadata":{},"cell_type":"markdown","source":"# ELO Merchant Category Recommendation\n\n![](https://storage.googleapis.com/kaggle-competitions/kaggle/10445/logos/thumb76_76.png?t=2018-10-24-17-14-05)\n\n\nImagine being hungry in an unfamiliar part of town and getting restaurant recommendations served up, based on your personal preferences, at just the right moment. The recommendation comes with an attached discount from your credit card provider for a local place around the corner!\n\nRight now, Elo, one of the largest payment brands in Brazil, has built partnerships with merchants in order to offer promotions or discounts to cardholders. But do these promotions work for either the consumer or the merchant? Do customers enjoy their experience? Do merchants see repeat business? Personalization is key.\n\nElo has built machine learning models to understand the most important aspects and preferences in their customers’ lifecycle, from food to shopping. But so far none of them is specifically tailored for an individual or profile. This is where you come in.\n\nIn this competition, Kagglers will develop algorithms to identify and serve the most relevant opportunities to individuals, by uncovering signal in customer loyalty. Your input will improve customers’ lives and help Elo reduce unwanted campaigns, to create the right experience for customers.\n"},{"metadata":{},"cell_type":"markdown","source":"### imports"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os\nimport gc\nimport time\nimport warnings\nimport datetime\nimport numpy as np\nimport pandas as pd\nimport xgboost as xgb\nimport seaborn as sns\nfrom math import sqrt\nimport lightgbm as lgb\nimport matplotlib.pyplot as plt\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\nfrom scipy.optimize import minimize\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import KFold, StratifiedKFold\n\nprint(os.listdir(\"../input\"))\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Reduce Memory Usage do to high memory req."},{"metadata":{"trusted":true,"_uuid":"acec24cf0a801a7e34dc7e47d849e79b40f40eef"},"cell_type":"code","source":"def reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() / 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() / 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n    return df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### to be able to view all the columns"},{"metadata":{"trusted":true,"_uuid":"3c82ed001f52cafa2ab6ab20ab8eb7d431bd2629"},"cell_type":"code","source":"def display_all(df):\n    with pd.option_context(\"display.max_rows\", 1000, \"display.max_columns\", 1000): \n        display(df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### generic missing percentage viewer for each column"},{"metadata":{"trusted":true,"_uuid":"6c48545987ac33e9d1867f7ae0978d90fdde57ce"},"cell_type":"code","source":"def missing_percentage(df):\n    total = df.isnull().sum().sort_values(ascending = False)\n    percent = (df.isnull().sum()/df.isnull().count()*100).sort_values(ascending = False)\n    missing_data  = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n    return missing_data.head(20)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### loading the data..."},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"train        = pd.read_csv('../input/train.csv', parse_dates=[\"first_active_month\"])\nsample       = pd.read_csv('../input/sample_submission.csv')\ntest         = pd.read_csv('../input/test.csv', parse_dates=[\"first_active_month\"])\nht           = pd.read_csv('../input/historical_transactions.csv',parse_dates=['purchase_date'])\nmerchant     = pd.read_csv('../input/merchants.csv')\nnew_merchant = pd.read_csv('../input/new_merchant_transactions.csv',parse_dates=[\"purchase_date\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### reducing the memory for each file & overwriting"},{"metadata":{"trusted":true,"_uuid":"19b838e030e6acceb5df599574ebfce42163d1e3"},"cell_type":"code","source":"train        = reduce_mem_usage(train)\ntest         = reduce_mem_usage(test)\nht           = reduce_mem_usage(ht)\nnew_merchant = reduce_mem_usage(new_merchant)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## EDA START"},{"metadata":{"_uuid":"d3232e018f3569d69ce4ea1a18925e3fb495b80e"},"cell_type":"markdown","source":"## Missing Value Exploration"},{"metadata":{"trusted":true,"_uuid":"c09ebc81e31d8401ef6ddd0debc180470b7b2965"},"cell_type":"code","source":"for df in [ht,new_merchant]:\n    df['category_2'].fillna(1.0,inplace=True)\n    df['category_3'].fillna('A',inplace=True)\n    df['merchant_id'].fillna('M_ID_00a6ca8a8a',inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bd8431a7488911608a00ddeb22c9a9e6fb6274c4","_kg_hide-input":true},"cell_type":"code","source":"feature_1 = train.loc[train['feature_1'] == 1]\nfeature_2 = train.loc[train['feature_1'] == 2]\nfeature_3 = train.loc[train['feature_1'] == 3]\nfeature_4 = train.loc[train['feature_1'] == 4]\nfeature_5 = train.loc[train['feature_1'] == 5]\n\nplt.figure(figsize=(10, 6))\nplt.title('Feature_2 Distribution based on Feature_1 values')\nsns.despine()\nsns.set_context(\"notebook\", font_scale=1.5, rc={\"lines.linewidth\": 2.5})\n\nsns.distplot(feature_1['feature_2'], hist=False, rug=False,label='1')\nsns.distplot(feature_2['feature_2'], hist=False, rug=False,label='2')\nsns.distplot(feature_3['feature_2'], hist=False, rug=False,label='3')\nsns.distplot(feature_4['feature_2'], hist=False, rug=False,label='4')\nsns.distplot(feature_5['feature_2'], hist=False, rug=False,label='5')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bf16369bc492d75f748604d1875bfe857fdac537","_kg_hide-input":true},"cell_type":"code","source":"feature_1 = train.loc[train['feature_2'] == 1]\nfeature_2 = train.loc[train['feature_2'] == 2]\nfeature_3 = train.loc[train['feature_2'] == 3]\n\nplt.figure(figsize=(10, 6))\nplt.title('Feature_1 Distribution based on Feature_2 values')\nsns.despine()\nsns.set_context(\"notebook\", font_scale=1.5, rc={\"lines.linewidth\": 2.5})\n\nsns.distplot(feature_1['feature_1'], hist=False, rug=False,label='1')\nsns.distplot(feature_2['feature_1'], hist=False, rug=False,label='2')\nsns.distplot(feature_3['feature_1'], hist=False, rug=False,label='3')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4fcb0d016df45b6e51bd1243a0d9a3674b270ea8","_kg_hide-input":true},"cell_type":"code","source":"feature_1 = train.loc[train['feature_3'] == 0]\nfeature_2 = train.loc[train['feature_3'] == 1]\n\nplt.figure(figsize=(10, 6))\nplt.title('Feature_1 Distribution based on Feature_3 values')\nsns.despine()\nsns.set_context(\"notebook\", font_scale=1.5, rc={\"lines.linewidth\": 2.5})\n\nsns.distplot(feature_1['feature_1'], hist=False, rug=False,label='0')\nsns.distplot(feature_2['feature_1'], hist=False, rug=False,label='1')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e506b7a7412e6c7539cf7dc4ba64e9ec16fbef55","_kg_hide-input":true},"cell_type":"code","source":"feature_1 = train.loc[train['feature_3'] == 0]\nfeature_2 = train.loc[train['feature_3'] == 1]\n\nplt.figure(figsize=(10, 6))\nplt.title('Feature_2 Distribution based on Feature_3 values')\nsns.despine()\nsns.set_context(\"notebook\", font_scale=1.5, rc={\"lines.linewidth\": 2.5})\n\nsns.distplot(feature_1['feature_2'], hist=False, rug=False,label='0')\nsns.distplot(feature_2['feature_2'], hist=False, rug=False,label='1')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5d77b74f17cd6d4255f1b20c7c24d5eef29550a9","_kg_hide-input":true},"cell_type":"code","source":"# thanks to this kernel @ https://www.kaggle.com/artgor/elo-eda-and-models\nfig, ax = plt.subplots(3, 1, figsize = (12, 12))\ntrain['feature_1'].value_counts().sort_index().plot(kind='bar', ax=ax[0], color='teal')\ntrain['feature_2'].value_counts().sort_index().plot(kind='bar', ax=ax[1], color='brown')\ntrain['feature_3'].value_counts().sort_index().plot(kind='bar', ax=ax[2], color='gold');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3b112632eddc050abdb9b0b885281d96728d35bd","_kg_hide-input":true},"cell_type":"code","source":"plt.figure(figsize=(10, 6))\nplt.title('Target Distribution')\nsns.despine()\nsns.set_context(\"notebook\", font_scale=1.5, rc={\"lines.linewidth\": 2.5})\n\nsns.distplot(train['target'], hist=True, rug=False,norm_hist=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## EDA START"},{"metadata":{"_uuid":"20b9bf4c4867752cc0938857c0dade20fc3c3bbf"},"cell_type":"markdown","source":"## categorical modifications + aggregations"},{"metadata":{"trusted":true,"_uuid":"9158093e34c1740f17358d9a40374ed3b0f367b4"},"cell_type":"code","source":"categorical_feats = ['feature_1', 'feature_2', 'feature_3']\n\nfor col in categorical_feats:\n    \n    lbl = LabelEncoder()\n    lbl.fit(list(train[col].values.astype('str')) + list(test[col].values.astype('str')))\n    train[col] = lbl.transform(list(train[col].values.astype('str')))\n    test[col]  = lbl.transform(list(test[col].values.astype('str')))\n    \ndf_all = pd.concat([train, test])\ndf_all = pd.get_dummies(df_all, columns=categorical_feats)\n\nlen_train = train.shape[0]\n\ntrain = df_all[:len_train]\ntest  = df_all[len_train:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"23fc4e74cd0f7ac3eee9149150ddfc0fbcdb8b1a"},"cell_type":"code","source":"ht['authorized_flag'] = ht['authorized_flag'].map({'Y':1, 'N':0})\nht['category_1']      = ht['category_1'].map({'Y': 1, 'N': 0})\nht['category_2x1']    = (ht['category_2'] == 1) + 0\nht['category_2x2']    = (ht['category_2'] == 2) + 0\nht['category_2x3']    = (ht['category_2'] == 3) + 0\nht['category_2x4']    = (ht['category_2'] == 4) + 0\nht['category_2x5']    = (ht['category_2'] == 5) + 0\nht['category_3A']     = (ht['category_3'].astype(str) == 'A') + 0\nht['category_3B']     = (ht['category_3'].astype(str) == 'B') + 0\nht['category_3C']     = (ht['category_3'].astype(str) == 'C') + 0\n\nht['purchase_date'] = pd.to_datetime(ht['purchase_date'])\nht['year']          = ht['purchase_date'].dt.year\nht['weekofyear']    = ht['purchase_date'].dt.weekofyear\nht['month']         = ht['purchase_date'].dt.month\nht['dayofweek']     = ht['purchase_date'].dt.dayofweek\nht['weekend']       = (ht.purchase_date.dt.weekday >=5).astype(int)\nht['hour']          = ht['purchase_date'].dt.hour\n\n\nht['Christmas_Day_2017'] = (pd.to_datetime('2017-12-25') - ht['purchase_date']).dt.days.apply(lambda x: x if x > 0 and x < 100 else 0)\nht['fathers_day_2017']   = (pd.to_datetime('2017-08-13') - ht['purchase_date']).dt.days.apply(lambda x: x if x > 0 and x < 100 else 0)\nht['Children_day_2017']  = (pd.to_datetime('2017-10-12') - ht['purchase_date']).dt.days.apply(lambda x: x if x > 0 and x < 100 else 0)\nht['Black_Friday_2017']  = (pd.to_datetime('2017-11-24') - ht['purchase_date']).dt.days.apply(lambda x: x if x > 0 and x < 100 else 0)\nht['Valentine_day_2017'] = (pd.to_datetime('2017-06-12') - ht['purchase_date']).dt.days.apply(lambda x: x if x > 0 and x < 100 else 0)\nht['Mothers_Day_2018']   = (pd.to_datetime('2018-05-13') - ht['purchase_date']).dt.days.apply(lambda x: x if x > 0 and x < 100 else 0)\n\ndef aggregate_historical_ht(history):\n    \n    history.loc[:, 'purchase_date'] = pd.DatetimeIndex(history['purchase_date']).\\\n                                      astype(np.int64) * 1e-9\n    \n    agg_func = {\n        'authorized_flag': ['sum', 'mean'],\n        'category_1': ['sum', 'mean'],\n        'category_2': ['nunique'],\n        'category_3A': ['sum'],\n        'category_3B': ['sum'],\n        'category_3C': ['sum'],\n        'category_2x1': ['sum','mean'],\n        'category_2x2': ['sum','mean'],\n        'category_2x3': ['sum','mean'],\n        'category_2x4': ['sum','mean'],\n        'category_2x5': ['sum','mean'],   \n        'merchant_id': ['nunique'],\n        'merchant_category_id': ['nunique'],\n        'city_id': ['nunique'],\n        'purchase_amount': ['sum', 'median', 'max', 'min', 'std'],\n        'installments': ['sum', 'median', 'max', 'min', 'std'],\n        'purchase_date': [np.ptp, 'max', 'min'],\n        'weekofyear': ['nunique'],\n        'weekend' : ['sum', 'mean'],\n        'dayofweek': ['nunique'],\n        'hour': ['nunique'],\n        'year': ['nunique'],\n        'card_id' : ['size'],\n        'month': ['nunique'],\n        'state_id': ['nunique'],\n        'subsector_id': ['nunique'],\n        'Christmas_Day_2017':['mean'],\n        'fathers_day_2017':['mean'],\n        'Children_day_2017':['mean'],\n        'Black_Friday_2017':['mean'],\n        'Valentine_day_2017':['mean'],\n        'Mothers_Day_2018':['mean']\n        }\n    agg_history = history.groupby(['card_id']).agg(agg_func)\n    agg_history.columns = ['hist_' + '_'.join(col).strip() \n                           for col in agg_history.columns.values]\n    agg_history.reset_index(inplace=True)\n    \n    df = (history.groupby('card_id')\n          .size()\n          .reset_index(name='hist_ht_count'))\n    \n    agg_history = pd.merge(df, agg_history, on='card_id', how='left')\n    \n    return agg_history\n\nhistory = aggregate_historical_ht(ht)\ndel ht\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"67f8c687a395d88d5837c4cf4559e688f1bd57b4"},"cell_type":"code","source":"train = pd.merge(train, history, on='card_id', how='left')\ntest  = pd.merge(test, history, on='card_id', how='left')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Feature Generation for time related feautures"},{"metadata":{"trusted":true,"_uuid":"de737ae03570a7505515f09d7a7cc8d0cd86fa6a"},"cell_type":"code","source":"# thanks to the kernel @ https://www.kaggle.com/yhn112/data-exploration-lightgbm-catboost-lb-3-760\n\nfor df in [train,test]:\n    df['first_active_month'] = pd.to_datetime(df['first_active_month'])\n    df['year'] = df['first_active_month'].dt.year\n    df['month'] = df['first_active_month'].dt.month\n    df['week'] = df['first_active_month'].dt.week\n    df['elapsed_time'] = (datetime.date(2018, 2, 1) - df['first_active_month'].dt.date).dt.days\n\ntrain.drop('first_active_month',axis=1,inplace=True)\ntest.drop('first_active_month',axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### more aggregations"},{"metadata":{"trusted":true,"_uuid":"90f45209a0cbb4d9829ebcc347cab38d28efb589"},"cell_type":"code","source":"new_merchant['purchase_date'] = pd.DatetimeIndex(new_merchant['purchase_date']).\\\n                                astype(np.int64) * 1e-9\n\nnew_merchant['authorized_flag'] = new_merchant['authorized_flag'].map({'Y':1, 'N':0})\nnew_merchant['category_1']      = new_merchant['category_1'].map({'Y':1, 'N':0})\nnew_merchant['category_3A']     = (new_merchant['category_3'].astype(str) == 'A') + 0\nnew_merchant['category_3B']     = (new_merchant['category_3'].astype(str) == 'B') + 0\nnew_merchant['category_3C']     = (new_merchant['category_3'].astype(str) == 'C') + 0\nnew_merchant['category_2x1']    = (new_merchant['category_2'] == 1) + 0\nnew_merchant['category_2x2']    = (new_merchant['category_2'] == 2) + 0\nnew_merchant['category_2x3']    = (new_merchant['category_2'] == 3) + 0\nnew_merchant['category_2x4']    = (new_merchant['category_2'] == 4) + 0\nnew_merchant['category_2x5']    = (new_merchant['category_2'] == 5) + 0\n\ndef aggregate_new_transactions(new_trans):    \n    agg_func = {\n        'authorized_flag': ['sum', 'mean'],\n        'category_1':   ['sum', 'mean'],\n        'category_2':   ['nunique'],\n        'category_3A':  ['sum'],\n        'category_3B':  ['sum'],\n        'category_3C':  ['sum'],     \n        'category_2x1': ['sum','mean'],\n        'category_2x2': ['sum','mean'],\n        'category_2x3': ['sum','mean'],\n        'category_2x4': ['sum','mean'],\n        'category_2x5': ['sum','mean'],  \n        'merchant_id': ['nunique'],\n        'merchant_category_id': ['nunique'],\n        'city_id': ['nunique'],\n        'purchase_amount': ['sum', 'median', 'max', 'min', 'std'],\n        'purchase_date': [np.ptp, 'max', 'min'],\n        'installments': ['sum', 'median', 'max', 'min', 'std'],\n        'month_lag': ['min', 'max'],\n        'state_id': ['nunique'],\n        'subsector_id': ['nunique']\n        }\n    agg_new_trans = new_trans.groupby(['card_id']).agg(agg_func)\n    agg_new_trans.columns = ['new_' + '_'.join(col).strip() \n                           for col in agg_new_trans.columns.values]\n    agg_new_trans.reset_index(inplace=True)\n    \n    df = (new_trans.groupby('card_id')\n          .size()\n          .reset_index(name='new_transactions_count'))\n    \n    agg_new_trans = pd.merge(df, agg_new_trans, on='card_id', how='left')\n    \n    return agg_new_trans\n\nnew_trans = aggregate_new_transactions(new_merchant)\n\ndel new_merchant\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"74731a3a51ad68ea9cedd90e8246262f6e94e7ce"},"cell_type":"code","source":"train = pd.merge(train, new_trans, on='card_id', how='left')\ntest  = pd.merge(test, new_trans, on='card_id', how='left')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Reducing again after all these aggregations"},{"metadata":{"trusted":true,"_uuid":"7ea5301f945d5ced7b0c0f205ff5de934dee1fe9"},"cell_type":"code","source":"train = reduce_mem_usage(train)\ntest  = reduce_mem_usage(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3533476d2dc560ae77c3ca1682b23fbd8a12f72b"},"cell_type":"code","source":"y     = train['target']\ntrain = train.drop(['target'],axis=1)\ntest  = test.drop(['target'],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6d582c92152fb6901702eb580934718a1cf716bd"},"cell_type":"markdown","source":"# Light GBM (Model 1)"},{"metadata":{"trusted":true,"_uuid":"5d52e3bd91f8330f84525163e823b7b41776eb84"},"cell_type":"code","source":"id_train = train['card_id'].copy()\nid_test  = test['card_id'].copy()\n\ntrain = train.drop('card_id', axis = 1)\ntest  = test.drop('card_id', axis = 1)\n\nnfolds = 10\nfolds = KFold(n_splits=5, shuffle=True, random_state=4590)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Parameters"},{"metadata":{"trusted":true,"_uuid":"44325ae3eb0a084090f75333b075f0351dea49bf"},"cell_type":"code","source":"param = {'num_leaves': 129,\n         'min_data_in_leaf': 148, \n         'objective':'regression',\n         'max_depth': 9,\n         'learning_rate': 0.005,\n         \"min_child_samples\": 24,\n         \"boosting\": \"gbdt\",\n         \"feature_fraction\": 0.7202,\n         \"bagging_freq\": 1,\n         \"bagging_fraction\": 0.8125 ,\n         \"bagging_seed\": 11,\n         \"metric\": 'rmse',\n         \"lambda_l1\": 0.3468,\n         \"verbosity\": -1}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Feature Importance"},{"metadata":{"trusted":true,"scrolled":false,"_uuid":"e392b7f031bc677260bc9a4ccb1984e4cc3bfcf6"},"cell_type":"code","source":"feature_importance_df = np.zeros((train.shape[1], nfolds))\nmvalid = np.zeros(len(train))\nmfull  = np.zeros(len(test))\n\nfor fold_, (trn_idx, val_idx) in enumerate(folds.split(train.values, train.values)):\n    print('----')\n    print(\"fold n°{}\".format(fold_))\n    \n    x0,y0 = train.iloc[trn_idx], y[trn_idx]\n    x1,y1 = train.iloc[val_idx], y[val_idx]\n    \n    trn_data = lgb.Dataset(x0, label= y0); val_data = lgb.Dataset(x1, label= y1)\n    \n    num_round = 10000\n    clf = lgb.train(param, trn_data, num_round, valid_sets = [trn_data, val_data], \n                    verbose_eval=500, early_stopping_rounds = 150)\n    mvalid[val_idx] = clf.predict(x1, num_iteration=clf.best_iteration)\n    \n    feature_importance_df[:, fold_] = clf.feature_importance()\n    \n    mfull += clf.predict(test, num_iteration=clf.best_iteration) / folds.n_splits\n    \nnp.sqrt(mean_squared_error(mvalid, y))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f98ee6f4e9c45853c0efab3ac1da96d1dcb475aa"},"cell_type":"code","source":"ximp = pd.DataFrame()\nximp['feature'] = train.columns\nximp['importance'] = feature_importance_df.mean(axis = 1)\n\nplt.figure(figsize=(14,14))\nsns.barplot(x=\"importance\",\n            y=\"feature\",\n            data=ximp.sort_values(by=\"importance\",\n                                           ascending=False))\nplt.title('LightGBM Features (avg over folds)')\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" ### to check the distribution "},{"metadata":{"trusted":true,"_uuid":"860b27d81feaaecd303194ef1eac103128a2bb49"},"cell_type":"code","source":"plt.figure(figsize=(10, 6))\nplt.title('Target Distribution')\nsns.despine()\nsns.set_context(\"notebook\", font_scale=1.5, rc={\"lines.linewidth\": 2.5})\nsns.distplot(mfull,hist=True, rug=True,norm_hist=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"eeadfb1c47b87047c57547055e716ded2c856c1f"},"cell_type":"code","source":"submission = pd.DataFrame()\nsubmission['card_id']  = id_test\nsubmission['target'] = mfull\nsubmission.to_csv('submission_lgb.csv', index = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"de5e8e61c02e69c6f6ca57254e7663812fa54f67"},"cell_type":"markdown","source":"# *to be continued*\n### different models & blending will be added soon"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}