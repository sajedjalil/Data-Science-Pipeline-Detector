{"metadata":{"language_info":{"nbconvert_exporter":"python","version":"3.6.3","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python","pygments_lexer":"ipython3"},"kernelspec":{"language":"python","name":"python3","display_name":"Python 3"}},"nbformat":4,"cells":[{"metadata":{"_cell_guid":"d95099e0-7a39-40a8-bacf-2e4c3cfcfb95","_uuid":"feaf5f77881a33c7fb250a7675a52f03e6a2be03"},"source":"# **// Part 0 // First Kaggle Competition - 'Hello World'**\n\nThis is the first competition I enter and the first real-world problem that I am publishing on. I hope to learn a lot myself and why not help some of my fellow learners along the way.\n\n# **// Part 1 // Problem Description**\n\nThe competition is called : **Corporacion Favorita Grocery Sales Forecasting**.\n\nThe task is to predict sales in the stores of an Ecuadorian supermarket chain so that they can avoid overstocking, which would reduce waste and loss, and minimize understocking, which induces opportunity cost and lower customer satisfaction. For the sake of both the buisiness and the environment, better predictions are highly desirable. This increased efficiency could also result in higher profits for the stakeholders and/or a better pricepoint for customers, depending on the choices made by the chain.\n\nThe given data is a table with the following variables/features: date, store id, item id, sales volume, promotion.\nWhereas store id and item id are integers, promotion is a boolean and sales volume is a float (integers for discrete items, float for volume/weight). We can see the data as N time series, one per (store, item) combination. Many of these time series are most likely correlated to each other and some sort of dimensional reduction will be most welcome here. \n\nThe company also offers some other data sets, such as a list of stores with their location, a time series of daily transactions per store, a list of holidays and events, a list of products by category, and the price of oil, of which a good chunk of the ecuadorian economy is allegedly tied to. These are additional tools to simplify and/or enhance the predictions, and some other external data could also be used in this regard. We will most likely do so in future kernels.\n\nThe primary task is therefore forecasting. For the sake of simplicity, and as it is our first dip in time series, this kernel will focus on the daily transactions data of a single store instead of the sales data for all stores.\n","cell_type":"markdown"},{"metadata":{"_cell_guid":"ff2fd268-839a-4483-8681-3c26914f7d9b","_uuid":"89210ac8934aeaeeaaa2b194439c712aded9e688"},"source":"# // Set-up\n\n  Kaggle allows for the data to be used from their server directly and has most libraries available, it is therefore a great place to work on your project instead of using you own machine. Combined with Jupyter, it becomes an extremely intuitive tool for exploratory work, doing things step-by-step and tuning our approach as we go. Moreover it allows readers to learn, fork the project and get their hands dirty. We are really looking forward to publishing!\n  \n  Let's then start by loading the libraries and methods we need:  ","cell_type":"markdown"},{"source":"# DATA MANIPULATION\nimport numpy as np # linear algebra\nimport random as rd # generating random numbers\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport datetime # manipulating date formats\nfrom operator import add # elementwise addition\n\n# VIZUALIZATION\nimport matplotlib.pyplot as plt # basic plotting\nimport seaborn # for prettier plots\nimport folium # plotting data on interactive maps\n\n# UNSUPERVISED LEARNING\nfrom sklearn.cluster import AgglomerativeClustering as AggClust # Hierarchical Clustering\nfrom scipy.cluster.hierarchy import ward,dendrogram # Hierarchical Clustering + Dendograms\n\n# TIME SERIES","metadata":{"_cell_guid":"75fd2f9a-7297-4203-b8f7-1502f66e0c9e","_uuid":"54cda7088daf8d27c5627cb80f9bef15fa30f6e7","collapsed":true},"execution_count":null,"outputs":[],"cell_type":"code"},{"metadata":{"_cell_guid":"e15e0b3e-2e6f-4be8-a8c5-879f3228656c","_uuid":"0eb792941c43ec9306fa07c35a55611101b4e052"},"source":"We can also import the data. First let us look at a simple version of the problem and see if it can guide us. One of the files is a record of the number of transactions per store per day. This can be seen as a simpler problem - predicting the traffic in a given store.","cell_type":"markdown"},{"source":"# Reading daily transfers per store\nsales=pd.read_csv('../input/transactions.csv')\n\n# Reading store list\nstores=pd.read_csv('../input/stores.csv')\nstores.type=stores.type.astype('category')\n\n# Adding information about the stores\nsales=pd.merge(sales,stores,how='left')\n\n# Reading the holiday and events schedule\nholidays=pd.read_csv('../input/holidays_events.csv')\n\n# Formatting the dates properly\nsales['date']=sales.date.apply(lambda x:datetime.datetime.strptime(x, '%Y-%m-%d'))\nholidays['date']=holidays.date.apply(lambda x:datetime.datetime.strptime(x, '%Y-%m-%d'))\n\n# Isolating events that do not correspond to holidays\nevents=holidays.loc[holidays.type=='Event']\nholidays=holidays.loc[holidays.type!='Event']\n\n# Extracting year, week and day\nsales['year'],sales['week'],sales['day']=list(zip(*sales.date.apply(lambda x: x.isocalendar())))\n\n# Creating a categorical variable showing weekends\nsales['dayoff']=[x in [6,7] for x in sales.day]\n\n# Adjuusting this variable to show all holidays\nfor (d,t,l,n) in zip(holidays.date,holidays.type,holidays.locale,holidays.locale_name):\n  if t!='Work Day':\n    if l=='National':\n      sales.loc[sales.date==d,'dayoff']=True\n    elif l=='Regional':\n      sales.loc[(sales.date==d)&(sales.state==n),'dayoff']=True\n    else:\n      sales.loc[(sales.date==d)&(sales.city==n),'dayoff']=True\n  else:\n    sales.loc[(sales.date==d),'dayoff']=False\n","metadata":{"_cell_guid":"6eccd50b-c197-4fbb-8f21-dac64e7165d4","_uuid":"2d3b84468f9166e001f7ddf20700ac61030101ee","collapsed":true},"execution_count":null,"outputs":[],"cell_type":"code"},{"metadata":{"_cell_guid":"9896392a-e519-40de-8979-9d383cbd492c","_uuid":"f6258c70c12aa2f48b482ed1abd5a96f708cd39a"},"source":"# // Exploratory Analysis\n\nLet us start by seeing what our tables look like. We'll use head() and info() on the **sales** table:","cell_type":"markdown"},{"source":"sales.info()","metadata":{"_cell_guid":"1c6a9def-31c1-48ea-96bf-b44753df47e4","_uuid":"0d50f51e8c9a07edbdb89bac0ba7a8927cb257e9","_kg_hide-input":true},"execution_count":null,"outputs":[],"cell_type":"code"},{"source":"sales.head(20)","metadata":{"_cell_guid":"129726cb-2a42-4b05-bfb1-79a5b110e497","_uuid":"07fc58e6032faade715096ec74b15bd4769674e0","_kg_hide-input":true},"execution_count":null,"outputs":[],"cell_type":"code"},{"metadata":{"_cell_guid":"e01a22ed-afa5-4525-97c8-49f83d8f1248","_uuid":"3cffaf345ed32fb7ec7d904ca37e59ebfa387879"},"source":"### **Store #47**\nThe chain established itselft in Quito in 1952 ([Wikipedia](https://es.wikipedia.org/wiki/Corporaci%C3%B3n_Favorita)), so let's pick a shop in Quito as a starting point, as the brand is well established there. Let us pick #47 and plot the corresponding transactions time series. \nWith a well established store, we can predict that the time series will be almost stationary. High seasonality is expected too, as people consume more during celebration periods. Let's see what we get:","cell_type":"markdown"},{"source":"ts=sales.loc[sales['store_nbr']==47,['date','transactions']].set_index('date')\nplt.figure(figsize=(12,12))\nplt.title('Daily transactions in store #47')\nplt.xlabel('time')\nplt.ylabel('Number of transactions')\nplt.plot(ts);","metadata":{"_cell_guid":"e71aaaa4-27a3-4ff3-8a50-76577d059749","_uuid":"ad0de98e86f562c47958c3d9e4f3e609e016aea7","_kg_hide-input":true},"execution_count":null,"outputs":[],"cell_type":"code"},{"metadata":{"_cell_guid":"5f8d67a5-ee54-4035-9893-fed92f7e47b8","_uuid":"e72886f2f0262e8b872b59ebc9a643bf76a6a108"},"source":"### ** Seasonality and Outliers **\nWe can easily identify a peak around Christmas, a very low volume day mid 2015 and a fairly stable behavior in between. The surge in volume around Christmas was expected, but what happened mid 2015? As it turns out it corresponds to a giant national protest in Ecuador, which will most likely impact other stores as well. [Wikipedia](https://en.wikipedia.org/wiki/2015_Ecuadorian_protests#24_June)\n\n  We can also see that outside of this holiday period, the transactions have quite a lot of variance, the volume oscillating between 3000 and 5000. Let's check the rolling mean and standard deviation with a window of a month::","cell_type":"markdown"},{"source":"plt.figure(figsize=(12,12))\nplt.plot(ts.rolling(window=7,center=False).mean(),label='Rolling Mean');\nplt.plot(ts.rolling(window=7,center=False).std(),label='Rolling sd');\nplt.legend();","metadata":{"_cell_guid":"53fd7336-5259-4f3b-831e-9b5eb606f41e","_uuid":"f9953c2056b9b6a292c47919f492874510866689","_kg_hide-input":true},"execution_count":null,"outputs":[],"cell_type":"code"},{"metadata":{},"source":"# **// Part 2 // Analysis Approach**\n\n  We will start by using an ARIMA model on this time series and see how successful we are. ARIMA stands for AutoRegressive Integrated Moving Average","cell_type":"markdown"},{"metadata":{"_cell_guid":"b1e9585f-a8eb-4f80-a194-98412c93cc67","_uuid":"919a5fbc34a9d6b37fb6847f924e7f3e499b91af"},"source":"### ** Two time Series ?**\n\nThe fact that holidays transactions volume is higher than the rest of the year is giving us a hint that this behavior may be more generalized. Afterall people have more time to shop when they don't work. Let's then see if it is true. We'll plot working days and days off with two different colors to see if there is a trend.\nAs you can see below, we can definitely see that there is a horizontal trend line at ~4700 transactions for days off and at ~3400 transactions for working days, repeated year after year. This could be good news for our model. Indeed, being able to separate 'apples' from 'oranges' will remove a lot of the apparent randomness that is contained in the time series as it stands. We may want to build a proper clustering engine for our days before forecasting and split the time series so as to improve our predictive power.","cell_type":"markdown"},{"source":"def plot_store_transactions(store_viz,n=30,split=False):\n    temp=sales.loc[sales.store_nbr==store_viz].set_index('date')\n    plt.figure(figsize=(12,6))\n    if split:\n        ax1=plt.subplot(1,2,1)\n        plt.scatter(temp.loc[~temp.dayoff].index,\n                    temp.loc[~temp.dayoff].transactions,label='working days')\n        plt.scatter(temp.loc[temp.dayoff].index,\n                    temp.loc[temp.dayoff].transactions,label='off days')\n        plt.legend()\n        plt.title('Daily transactions. Store {}, Type {}, Cluster {}'.format(store_viz,\n                                                                        list(stores.loc[stores.store_nbr==store_viz,'type'])[0],\n                                                                        list(stores.loc[stores.store_nbr==store_viz,'cluster'])[0])\n                 )\n        ax2=plt.subplot(1,2,2,sharey=ax1,sharex=ax1)\n        plt.plot(temp.loc[~temp.dayoff,'transactions'].rolling(window=n).mean(),label='working days')\n        plt.plot(temp.loc[temp.dayoff,'transactions'].rolling(window=n).mean(),label='off days')\n        plt.legend()\n        plt.title('Store {}: {} day rolling means'.format(store_viz,n))\n        plt.setp(ax2.get_yticklabels(), visible=False)\n    else:\n        ax1=plt.subplot(1,2,1)\n        plt.scatter(temp.index,temp.transactions)\n        plt.title('Daily transactions. Store {}, Type {}, Cluster {}'.format(store_viz,\n                                                                        list(stores.loc[stores.store_nbr==store_viz,'type'])[0],\n                                                                        list(stores.loc[stores.store_nbr==store_viz,'cluster'])[0])\n                 )\n        ax2=plt.subplot(1,2,2,sharey=ax1)\n        plt.plot(temp.transactions.rolling(window=n).mean())\n        plt.title('Store {}: {} day rolling means'.format(store_viz,n))\n        plt.setp(ax2.get_yticklabels(), visible=False)\nplt.show()","metadata":{"_cell_guid":"1bc76208-5e8d-4f5e-809f-4a957a3bede6","_uuid":"8c392b66dc8a1d264c1d182995fb20c53f1f16ce","_kg_hide-input":true},"execution_count":null,"outputs":[],"cell_type":"code"},{"source":"plot_store_transactions(47,30,True)","metadata":{"_cell_guid":"a3df06c6-1257-4359-98ea-abb5fadf08d9","_uuid":"40df40bb95e0a1c9ae8f75aca1d15f9f8d84bebe","_kg_hide-input":false},"execution_count":null,"outputs":[],"cell_type":"code"},{"metadata":{"_cell_guid":"24157c70-5890-46ba-9232-bcd475585a9a","_uuid":"e439691669a9c32f5444fcedddb1b2a31ec573b9"},"source":"### ** Is 'type' a good classifier? **\nThis is the biggest chain in the country and there are therefore many stores to analyse, 54 in total. Are they all behaving like store #47? The company provided two different classifications of their stores without any indications of what they correspond to: type and cluster. Would they by chance correspond to patterns in the transaction volume time series? \n\nLet's plot them grouped by type to see if a pattern emerges or if we ought to seek our own classification. The following function creates a grid of plots for all stores of a given type.","cell_type":"markdown"},{"source":"def plot_store_transactions_type(typ):\n    typ_stores=stores.loc[stores.type==typ,'store_nbr']\n    n=len(typ_stores)\n    m=1\n    for x in range(1,6):\n        if (n-1) in range((x-1)**2,x**2):\n            m=x\n    plt.figure(figsize=(15,15))\n    for x in range(n):\n        nbr=typ_stores.iloc[x]\n        ax1 = plt.subplot(m,m,x+1)\n        ax1.scatter(sales.loc[(~sales.dayoff)&(sales.store_nbr==nbr),'date'].values,\n                sales.loc[(~sales.dayoff)&(sales.store_nbr==nbr),'transactions'])\n        ax1.scatter(sales.loc[(sales.dayoff)&(sales.store_nbr==nbr),'date'].values,\n                sales.loc[(sales.dayoff)&(sales.store_nbr==nbr),'transactions'])\n        plt.title('Store {}, Type {}, Cluster {}'.format(nbr,\n                                                         list(stores.loc[stores.store_nbr==nbr,'type'])[0],\n                                                         list(stores.loc[stores.store_nbr==nbr,'cluster'])[0])\n             )\n        plt.suptitle(' Type {} stores'.format(typ),fontsize=25)\n    plt.show()","metadata":{"_cell_guid":"817de8b1-452d-4a79-8011-6f6be9810dc5","_uuid":"0fae2c6291f0059b88a235cb37a5514a77001d19","_kg_hide-input":false},"execution_count":null,"outputs":[],"cell_type":"code"},{"source":"plot_store_transactions_type('A')","metadata":{"_cell_guid":"15e916aa-2ced-4c89-9704-a47f0456478f","_uuid":"f63bb3a52dfb6748fe17706147d66012835defbd","_kg_hide-input":true},"execution_count":null,"outputs":[],"cell_type":"code"},{"metadata":{"_cell_guid":"22a7bc76-8151-4d04-81cd-2a442a65d1bd","_uuid":"40052fa78ae0a413f95215f14eafea4d90f13262"},"source":"It seems that all show a similar pattern to store #47, which is part of this group. This is encouraging. Note some slight difference though: Store #49 shows a distinctive upward trend, while store #51 seems to barely have any difference between working days and off days. Store 52 is brand new and has very few data points.\n\nLet us now see type 'B'.","cell_type":"markdown"},{"source":"plot_store_transactions_type('B')","metadata":{"_cell_guid":"d76dbc7b-6fff-4650-9908-70953c9f361a","_uuid":"2867b5062c913e22aa0315f4660594328d3fb74b","_kg_hide-input":true},"execution_count":null,"outputs":[],"cell_type":"code"},{"metadata":{"_cell_guid":"4a01ee1d-a7cd-458d-b2ec-433222fd679d","_uuid":"e89a82131f5b4e10c56e86ecca38944b620afb3b"},"source":"The stores with a clear difference between working days and off days are the minority here. Note that store #20 and #21 show a surge of volume at opening before stabilizing after a few months. There must have entered a market with a lack of service. Note that store #18 has a 'hole' in the data for about a 4 months late 2016. Probably some renovation.\n\nLet's check type 'C'.","cell_type":"markdown"},{"source":"plot_store_transactions_type('C')","metadata":{"_cell_guid":"5a1f2da9-f3f8-450d-a1b0-6a0c0fb2adb3","_uuid":"05ab8fdc9bc416e332fb10c75f78ec14e3c7171f","_kg_hide-input":true},"execution_count":null,"outputs":[],"cell_type":"code"},{"metadata":{"_cell_guid":"2411d348-5680-44f6-b111-1f2b439e866e","_uuid":"a5b6a6d8bff9beafb2abc91987cd743eb1a3daee"},"source":"Type 'C' is even less consistant as the previous ones. Stores #13, #14 and #19 have the off days split into two distinct groups. Stores #15 and #40 have slightly less transactions on off days than working days. There must be a geographical reason for these behaviors, such as stores situatied in industrial zones. It is something worth investigating later.","cell_type":"markdown"},{"source":"plot_store_transactions_type('D')","metadata":{"_cell_guid":"eb96074b-714f-46b4-9fb5-7bded4dc0825","_uuid":"a907d08532cc76aa9ba5eafd7a6406f5162c35a3","_kg_hide-input":true},"execution_count":null,"outputs":[],"cell_type":"code"},{"metadata":{"_cell_guid":"02b9e7a3-c3c0-4b98-9569-599a11524606","_uuid":"963a4f05499a48c0072180b9d8d76ef751544e34"},"source":"These show a mix of behaviors previously seen. Store #1 shows its off days both split and below working days. Store #24 has a 'hole' of 3 months mid 2015 while store #53 has a clear discontinuity in April 2016.\nThe discontinuity was explained by the company, as the consequence of the relief efforts from the population after an earthquate of magnitude 7.8 in the region.","cell_type":"markdown"},{"source":"plot_store_transactions_type('E')","metadata":{"_cell_guid":"274f4458-5b25-4b9c-9f3f-3558386962c7","_uuid":"dfe05495eaad1c14548f246c48bd7c12f7550db2","_kg_hide-input":true},"execution_count":null,"outputs":[],"cell_type":"code"},{"metadata":{"_cell_guid":"49b4421c-6350-4bc2-b4b9-69b5cb752067","_uuid":"508e92b2199887c8aea3db44c2d6caa318f68f68"},"source":"This group is a little more homogenius with common transactions volume. Note that stores #36 and #43 both have discontinuities.\n\n## ** Clusters are no better **\n\n  Another classification provided by the company is what they called 'clusters'. The fonction below does the same as previously but grouping by clusters.","cell_type":"markdown"},{"source":"def plot_store_transactions_cluster(clust):\n    clust_stores=stores.loc[stores.cluster==clust,'store_nbr']\n    n=len(clust_stores)\n    m=1\n    for x in range(1,6):\n        if (n-1) in range((x-1)**2,x**2):\n            m=x\n    plt.figure(figsize=(15,15))\n    for x in range(n):\n        nbr=clust_stores.iloc[x]\n        ax1 = plt.subplot(m,m,x+1)\n        ax1.scatter(sales.loc[(~sales.dayoff)&(sales.cluster==clust)&(sales.store_nbr==nbr),'date'].values,\n                sales.loc[(~sales.dayoff)&(sales.cluster==clust)&(sales.store_nbr==nbr),'transactions'])\n        ax1.scatter(sales.loc[(sales.dayoff)&(sales.cluster==clust)&(sales.store_nbr==nbr),'date'].values,\n                sales.loc[(sales.dayoff)&(sales.cluster==clust)&(sales.store_nbr==nbr),'transactions'])\n        plt.title('Store {}, Type {}, Cluster {}'.format(nbr,\n                                                         list(stores.loc[stores.store_nbr==nbr,'type'])[0],\n                                                         list(stores.loc[stores.store_nbr==nbr,'cluster'])[0])\n             )\n        plt.suptitle(' Cluster {} stores'.format(clust),fontsize=25)\n    plt.show()","metadata":{"_cell_guid":"f66a99b5-60b9-4497-bb1d-29e323b428df","_uuid":"2ad9c92130025636eac3d2104e526b91a8f5a6c7","collapsed":true},"execution_count":null,"outputs":[],"cell_type":"code"},{"metadata":{"_cell_guid":"8d425401-9599-4533-b13b-7c0c9df7d161","_uuid":"0163a9b5597bd6887967e033f96f34928f7ea93e"},"source":"  The gaphs below belong to stores of the cluster #13, which show 4 very different behaviors. Clusters are therefore not a good classification for forcasting models either","cell_type":"markdown"},{"source":"plot_store_transactions_cluster(13)","metadata":{"_cell_guid":"6e27c5aa-6484-4c5f-b64a-ad454effc61a","_uuid":"80d700c4b543dd896015a23a968510658e00f6f4","_kg_hide-input":true},"execution_count":null,"outputs":[],"cell_type":"code"},{"metadata":{"_cell_guid":"dc48ef88-2519-4349-8405-a804fb7113f4","_uuid":"20e8fc53c995592afddb361932cd44d405e9f655"},"source":"","cell_type":"markdown"},{"metadata":{"_cell_guid":"7aa5c102-06f2-4b5f-b4d2-d7f1c7039709","_uuid":"4015942b7abb27a5c1c9640f58236254cc2c60b8"},"source":"# **// Part 2 // Analysis Approach**\n\n  From what I understand, we are not just trying to predict a number but find patterns in the clients' consuming patterns. It is important then to understand the impulses that make people shop or not shop. Since precise location isn't available not is weather, let's focus on a simple factor : working day / holiday.\n  \n We would like to get a better classification of the stores so as to adapt the models to them, this is a good example of unsupervised learning and we will start with getting clusters with these simple features:\n- Mean and standard deviation of working day transactions\n- Mean and standard deviation of holiday transactions\n- Mean and standard deviation of week day transactions\n\nLet us build the table\n","cell_type":"markdown"},{"source":"sales_by_store=sales.groupby(['store_nbr']).transactions.sum()/sales.groupby(['store_nbr']).transactions.count()\n\nMeans1=sales.groupby(['store_nbr','dayoff']).transactions.agg(['mean','std']).unstack(level=1)\nMeans2=sales.groupby(['store_nbr','day']).transactions.agg(['mean','std']).unstack(level=1)\n\n# Creating a new columns with ratio of transactions of the day / daily average\nsales['normalized']=[v/sales_by_store[s] for (s,v) in zip(sales.store_nbr,sales.transactions)]\nMeans1_norm=sales.groupby(['store_nbr','dayoff']).normalized.agg(['mean','std']).unstack(level=1)\nMeans2_norm=sales.groupby(['store_nbr','day']).normalized.agg(['mean','std']).unstack(level=1)\n\nsales","metadata":{"_cell_guid":"725eea58-aaca-4069-be5d-30d3a42c5bb9","_uuid":"919c40d20782745030edbcb94d76b2841cd76c3f","scrolled":true},"execution_count":null,"outputs":[],"cell_type":"code"},{"metadata":{"_cell_guid":"938011eb-9c9d-47cd-ae10-88124ec95fc3","_uuid":"4eb01cae200fc84eefb047b6fc82594d6203605b"},"source":"Let us plot the new features to see if any clustering appear obvious:","cell_type":"markdown"},{"source":"plt.figure(figsize=(12,16))\nplt.subplot(2,2,1)\nplt.scatter(Means1.iloc[:,0],Means1.iloc[:,1])\nplt.xlabel('Means of working days')\nplt.ylabel('Means of holidays')\nplt.plot([0,5000],[0,5000])\nplt.title('Comparing mean by type of day')\nplt.subplot(2,2,2)\nplt.scatter(Means1.iloc[:,2],Means1.iloc[:,3])\nplt.xlabel('Standard dev. of working days')\nplt.ylabel('Standard dev. of holidays')\nplt.plot([0,1000],[0,1000])\nplt.title('Comparing std by type of day');\nplt.subplot(2,2,3)\nplt.scatter(Means1.iloc[:,0],Means1.iloc[:,2])\nplt.xlabel('Means of working days')\nplt.ylabel('Standard dev. of working days')\nplt.plot([0,5000],[0,500])\nplt.title('Comparing mand and std for working days')\nplt.subplot(2,2,4)\nplt.scatter(Means1.iloc[:,1],Means1.iloc[:,3])\nplt.xlabel('Means of holidays days')\nplt.ylabel('Standard dev. of holidays')\nplt.plot([0,5000],[0,1000]);\nplt.title('Comparing mand and std for holidays');","metadata":{"_cell_guid":"c4223140-bf41-4907-a279-3dd01e7416d6","_uuid":"4a1e447b9abc31fec8a65cbe41613dc2b3812dc5","_kg_hide-input":true},"execution_count":null,"outputs":[],"cell_type":"code"},{"source":"plt.figure(figsize=(12,6))\nplt.subplot(1,2,1)\nseaborn.heatmap(Means2.iloc[:,0:7],cmap='Oranges');\nplt.subplot(1,2,2)\nseaborn.heatmap(Means2.iloc[:,7:14],cmap='Oranges');","metadata":{"_cell_guid":"2193cea7-684b-4b60-95cd-a8755e174dc2","_uuid":"3d8c907bf9d99fcd848a0c9cd950b2db92a73fff"},"execution_count":null,"outputs":[],"cell_type":"code"},{"source":"plt.figure(figsize=(12,16))\nplt.subplot(2,2,1)\nplt.scatter(Means1_norm.iloc[:,0],Means1_norm.iloc[:,1])\nplt.xlabel('Means of working days')\nplt.ylabel('Means of holidays')\nplt.title('Comparing mean by type of day')\nplt.subplot(2,2,2)\nplt.scatter(Means1_norm.iloc[:,2],Means1_norm.iloc[:,3])\nplt.xlabel('Standard dev. of working days')\nplt.ylabel('Standard dev. of holidays')\nplt.title('Comparing std by type of day');\nplt.subplot(2,2,3)\nplt.scatter(Means1_norm.iloc[:,0],Means1_norm.iloc[:,2])\nplt.xlabel('Means of working days')\nplt.ylabel('Standard dev. of working days')\nplt.title('Comparing mand and std for working days')\nplt.subplot(2,2,4)\nplt.scatter(Means1_norm.iloc[:,1],Means1_norm.iloc[:,3])\nplt.xlabel('Means of holidays days')\nplt.ylabel('Standard dev. of holidays')\nplt.title('Comparing mand and std for holidays');","metadata":{"_cell_guid":"f01c39d7-071d-4e16-87a6-3493918c5949","_uuid":"62c2545fb2ac8bccc5662b0df9d6aba397aa29ce"},"execution_count":null,"outputs":[],"cell_type":"code"},{"metadata":{},"source":"Note that the fact that the means are linearly correlated should be obvious the slope should correspond to the ratio of days off / working days. It gives us a very simple clustering of the stores, there is the one in the bottom right and the rest. We expect that lonely store to be store #1.","cell_type":"markdown"},{"source":"plt.figure(figsize=(12,6))\nplt.subplot(1,2,1)\nseaborn.heatmap(Means2_norm.iloc[:,0:7],cmap='Oranges');\nplt.subplot(1,2,2)\nseaborn.heatmap(Means2_norm.iloc[:,7:14],cmap='Oranges');","metadata":{"_cell_guid":"d21df8e1-bb7f-4acb-9192-e08b176f736b","_uuid":"8cdc7278160c067f32cf0814892689283348bf4a"},"execution_count":null,"outputs":[],"cell_type":"code"},{"source":"clustering=AggClust(n_clusters=6)\ncluster=clustering.fit_predict(Means2_norm)\nstores['new_cluster']=cluster\n\ndef plot_store_transactions_new_cluster(clust):\n    clust_stores=stores.loc[stores['new_cluster']==clust,'store_nbr']\n    n=len(clust_stores)\n    m=1\n    for x in range(1,10):\n        if (n-1) in range((abs(x-1))**2,x**2):\n            m=x\n    plt.figure(figsize=(15,15))\n    for x in range(n):\n        nbr=clust_stores.iloc[x]\n        ax1 = plt.subplot(m,m,x+1)\n        ax1.scatter(sales.loc[(~sales.dayoff)&(sales.store_nbr==nbr),'date'].values,\n                sales.loc[(~sales.dayoff)&(sales.store_nbr==nbr),'transactions'],\n                color='blue')\n        ax1.scatter(sales.loc[(sales.dayoff)&(sales.store_nbr==nbr),'date'].values,\n                sales.loc[(sales.dayoff)&(sales.store_nbr==nbr),'transactions'],\n                color='red')\n        plt.title('Store {}, Type {}, Cluster {}'.format(nbr,\n                                                         list(stores.loc[stores.store_nbr==nbr,'type'])[0],\n                                                         list(stores.loc[stores.store_nbr==nbr,'cluster'])[0])\n             )\n    plt.show()","metadata":{"_cell_guid":"9646833f-7aa3-4215-bc19-a40c5c3c584e","_uuid":"fe6bab6bad05f9efbd766b3b11f1fd823f147b8f","collapsed":true},"execution_count":null,"outputs":[],"cell_type":"code"},{"source":"plot_store_transactions_new_cluster(2)","metadata":{"_cell_guid":"87eb87c8-90a3-4107-b4ae-cdcf50f86cd5","_uuid":"cc8b1a5e9626ccd66ce9aab0ea3f60cafdd7fa88","collapsed":true},"execution_count":null,"outputs":[],"cell_type":"code"},{"source":"fig=plt.figure(figsize=(15,15))\nax = fig.add_subplot(1, 1, 1)\ndendrogram(ward(Means2_norm),ax=ax)\nax.tick_params(axis='x', which='major', labelsize=15)\nax.tick_params(axis='y', which='major', labelsize=8)\nplt.show()","metadata":{"_cell_guid":"8fc20eb1-7846-4ff6-a2dc-42288c075c84","_uuid":"b4a78bd94fa4d63214898893e1b418bafa0f0950","collapsed":true},"execution_count":null,"outputs":[],"cell_type":"code"},{"metadata":{"_cell_guid":"a20bcb9a-fafa-4fcf-afaa-c90beb6f7a49","_uuid":"cf582790609da088bb803758233866a7ac7ca32e"},"source":"# **// Part 3 // Initial Solution**\n\n# **// Part 4 // Initial Solution Analysis**\n\n# **// Part 5 // Revised Solution Analysis**","cell_type":"markdown"},{"metadata":{},"source":"##  ** Geography **\n\n  Let's plot all stores using their location to see if we can find more hints about what we see. Unfortunately the locations are not provided at a granularity below city and states. We will therefore use the city names and **add some noise** to the GPS coordinates so as to separate the icons for visibility.\n  As it turns out, the data is far from representing all stores owned by the company so we cannot use a simple method as a google search to identify the type of neighborhood/environment the stores are located in.\n\n\n  To plot the map, we will be using Folium, which is really nice and allows very interactive content.","cell_type":"markdown"},{"source":"store_locations={\n 'Ambato' : [-1.2543408,-78.6228504],\n 'Babahoyo' : [-1.801926,-79.53464589999999],\n 'Cayambe' : [0.025,-77.98916659999998],\n 'Cuenca' : [-2.9001285,-79.0058965],\n 'Daule' : [-1.86218,-79.97766899999999],\n 'El Carmen' : [25.9375976,-100.36382520000001],\n 'Esmeraldas' : [0.9681788999999998,-79.6517202],\n 'Guaranda' : [-1.5904721,-78.9995154],\n 'Guayaquil' : [-2.1709979,-79.92235920000002],\n 'Ibarra' : [0.3391763,-78.12223360000002],\n 'Latacunga' : [-0.7754954,-78.52064999999999],\n 'Libertad' : [-2.2344458,-79.91122430000001],\n 'Loja' : [-4.0078909,-79.21127690000003],\n 'Machala' : [-3.2581112,-79.9553924],\n 'Manta' : [-0.9676533,-80.70891010000003],\n 'Playas' : [-2.6284683,-80.38958860000002],\n 'Puyo' : [-1.4923925,-78.00241340000002],\n 'Quevedo' : [-1.0225124,-79.46040349999998],\n 'Quito' : [-0.1806532,-78.46783820000002],\n 'Riobamba' : [-1.6635508,-78.65464600000001],\n 'Salinas' : [-2.2233633,-80.958462],\n 'Santo Domingo' : [-0.2389045,-79.17742679999998]\n}\n\n# Defining a color dictionary\ncol={'A':'red','B':'blue','C':'green','D':'brown','E':'yellow'}\n\n#\ndef add_city_map(name,typ):\n    folium.Marker(\n         location=list(map(add,store_locations.get(name),[(0.5-rd.random())/20,(0.5-rd.random())/20])),\n         icon=folium.Icon(color=col.get(typ), icon='shopping-cart'),\n    ).add_to(map_Ecuador)\n\nmap_Ecuador=folium.Map(location=[-1.233333, -78.516667],zoom_start=7)\n\n# Enabling clustering (also replace map_ecuador by store_cluster in the add_city_map function)\n# from folium.plugins import MarkerCluster\n#store_cluster=MarkerCluster().add_to(map_Ecuador)\n\n[add_city_map(x,y) for x,y in zip(stores.city,stores.type)]\nmap_Ecuador","metadata":{},"execution_count":null,"outputs":[],"cell_type":"code"},{"metadata":{},"source":"The map does not teach us anything about the 'type' classification and based on what we have seen so far, I am very tempted to drop it altogether.\n\nNote that to change the icon to a intuitive one, I checked the documentation for the icon method (see below), and followed the link provided, sending me to the github page of the Leaflet plugin that Folium is built upon. ","cell_type":"markdown"},{"source":"help(folium.Icon)","metadata":{},"execution_count":null,"outputs":[],"cell_type":"code"},{"metadata":{},"source":"## ** Weather **\n\nWeather could have a major impact on sales, with people chosing to stay home and delay their purchases during heavy rain or extreme heat/cold. Let us pull the data for store 47's location and overlay temperature and precipitation to the transaction activity.\n\nUnfortunately I am currently unable to find past weather data for ecuadorian cities.","cell_type":"markdown"},{"metadata":{},"source":"### ** Decomposition**","cell_type":"markdown"},{"source":"from statsmodels.tsa.seasonal import seasonal_decompose\ndecomposition = seasonal_decompose(ts,freq=365,model='additive')\n\ntrend = decomposition.trend\nseasonal = decomposition.seasonal\nresidual = decomposition.resid\n\nplt.figure(figsize=(12,12))\n\nplt.subplot(411)\nplt.plot(ts, label='Original')\nplt.legend(loc='best')\nplt.subplot(412)\nplt.plot(trend, label='Trend')\nplt.legend(loc='best')\nplt.subplot(413)\nplt.plot(seasonal,label='Seasonality')\nplt.legend(loc='best')\nplt.subplot(414)\nplt.plot(residual, label='Residuals')\nplt.legend(loc='best')\nplt.tight_layout()","metadata":{},"execution_count":null,"outputs":[],"cell_type":"code"}],"nbformat_minor":1}