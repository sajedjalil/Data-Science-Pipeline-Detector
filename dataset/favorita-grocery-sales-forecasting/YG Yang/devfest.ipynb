{"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3","language":"python"},"language_info":{"pygments_lexer":"ipython3","name":"python","mimetype":"text/x-python","file_extension":".py","nbconvert_exporter":"python","codemirror_mode":{"version":3,"name":"ipython"},"version":"3.6.3"}},"nbformat_minor":1,"nbformat":4,"cells":[{"metadata":{"_cell_guid":"2887fbc8-496d-4033-b85d-02d545903f90","_uuid":"6f2519b19ab9cfbbf515ed611d4a2438010bfc66"},"source":"# Prepare","cell_type":"markdown"},{"metadata":{"_cell_guid":"791934a3-dfef-470e-93ba-5316e7b0f4cc","_uuid":"fb6ed10e5af3dc13e8a610d268104d49f8b4c2e9"},"outputs":[],"source":"# -*- coding: utf-8 -*-\nimport datetime\nfrom datetime import timedelta\n\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\n\nfrom tensorflow.contrib.timeseries.python.timeseries import NumpyReader\nfrom tensorflow.contrib.timeseries.python.timeseries import estimators as tfts_estimators\nfrom tensorflow.contrib.timeseries.python.timeseries import model as tfts_model\n\nimport matplotlib\nimport matplotlib.pyplot as plt\n%matplotlib inline\n","execution_count":null,"cell_type":"code"},{"metadata":{"_cell_guid":"3d621ab2-370a-4540-9b07-5355774c1533","_uuid":"cbbd94a687b28f986dce3d613117c00e2095a806"},"source":"# Read Train data","cell_type":"markdown"},{"metadata":{"collapsed":true,"_cell_guid":"d86b91d8-e8d2-4b62-862c-406079d14161","_uuid":"dd4bf13cfa82f19be4093faad05216d78aea4b26"},"outputs":[],"source":"dtypes = {'id':'int64', 'item_nbr':'int32', 'store_nbr':'int8'}\n\ntrain = pd.read_csv('../input/train.csv', usecols=[1,2,3,4], dtype=dtypes, parse_dates=['date'], \n                    skiprows=range(1, 101688780) #Skip initial dates \n)\n\ntrain.loc[(train.unit_sales < 0),'unit_sales'] = 0 # eliminate negatives\ntrain['unit_sales'] =  train['unit_sales'].apply(pd.np.log1p) #logarithm conversion\ntrain['dow'] = train['date'].dt.dayofweek ","execution_count":null,"cell_type":"code"},{"metadata":{"collapsed":true,"_cell_guid":"39576949-21f1-4740-80bb-073edc1a49ac","_uuid":"2fdb26c33eff6454f9d2f8e6d228c957969bf624"},"outputs":[],"source":"# creating records for all items, in all markets on all dates\n# for correct calculation of daily unit sales averages.\nu_dates = train.date.unique()\nu_stores = train.store_nbr.unique()\nu_items = train.item_nbr.unique()\ntrain.set_index(['date', 'store_nbr', 'item_nbr'], inplace=True)\ntrain = train.reindex(\n    pd.MultiIndex.from_product(\n        (u_dates, u_stores, u_items),\n        names=['date','store_nbr','item_nbr']\n    )\n)","execution_count":null,"cell_type":"code"},{"metadata":{"collapsed":true,"_cell_guid":"f07b4373-73d7-4421-a0bd-9937087000a0","_uuid":"1811aa3be385237df174140b0b6f72313f5e4c3a"},"outputs":[],"source":"train.loc[:, 'unit_sales'].fillna(0, inplace=True) # fill NaNs\ntrain.reset_index(inplace=True) # reset index and restoring unique columns  \nlastdate = train.iloc[train.shape[0]-1].date","execution_count":null,"cell_type":"code"},{"metadata":{"_cell_guid":"9e15054b-16f5-4ee2-997c-131763de4191","scrolled":true,"_uuid":"98303cb457d329e701b46115c9f8d3f5d89c0719"},"outputs":[],"source":"train.head()","execution_count":null,"cell_type":"code"},{"metadata":{"_cell_guid":"d6404245-80f8-44e9-9305-58c56e026fbf","_uuid":"dd2dc6c5f6b1835b34e7f4711cc80fc5c9b9f8d2"},"outputs":[],"source":"tmp = train[['item_nbr','store_nbr','dow','unit_sales']]\nma_dw = tmp.groupby(['item_nbr','store_nbr','dow'])['unit_sales'].mean().to_frame('madw')\nma_dw.reset_index(inplace=True)\nma_dw.head()","execution_count":null,"cell_type":"code"},{"metadata":{"_cell_guid":"47ced6e6-a4aa-4c50-91be-00cc3a136d16","_uuid":"1d09db10e17856079f746ec3cf8c90bced7bb4df"},"outputs":[],"source":"tmp = ma_dw[['item_nbr','store_nbr','madw']]\nma_wk = tmp.groupby(['item_nbr', 'store_nbr'])['madw'].mean().to_frame('mawk')\nma_wk.reset_index(inplace=True)\nma_wk.head()","execution_count":null,"cell_type":"code"},{"metadata":{"_cell_guid":"a90fe1e3-2423-475d-aee3-cb3004154469","_uuid":"e2dff589f16d6b44e7cebb68331e4b7f983e6815"},"source":"# Moving Average - Our Basic Model","cell_type":"markdown"},{"metadata":{"collapsed":true,"_cell_guid":"bd12a883-ad02-41fe-b1bd-a64db8346a95","_uuid":"dbadfeb78b48f8dfb98546f0dba24788b22c2cfe"},"outputs":[],"source":"tmp = train[['item_nbr','store_nbr','unit_sales']]\nma_is = tmp.groupby(['item_nbr', 'store_nbr'])['unit_sales'].mean().to_frame('mais226')","execution_count":null,"cell_type":"code"},{"metadata":{"collapsed":true,"_cell_guid":"9896f7ee-6c6b-40b9-94a8-e7ff5760beee","_uuid":"3de8cfb1926c7f9a627e16de48377e2c6a93ea7f"},"outputs":[],"source":"for i in [112,56,28,14,7,3,1]:\n    tmp = train[train.date>lastdate-timedelta(int(i))]\n    tmpg = tmp.groupby(['item_nbr','store_nbr'])['unit_sales'].mean().to_frame('mais'+str(i))\n    ma_is = ma_is.join(tmpg, how='left')\n\ndel tmp,tmpg","execution_count":null,"cell_type":"code"},{"metadata":{"collapsed":true,"_cell_guid":"60b8a105-c89e-4471-8a37-658bac2c8dc1","_uuid":"22c535688c57e0bfa8e25b0fc2cadf8c3287bd85"},"outputs":[],"source":"ma_is['mais']=ma_is.median(axis=1)\nma_is.reset_index(inplace=True)","execution_count":null,"cell_type":"code"},{"metadata":{"_cell_guid":"208298dc-623e-4ff9-b467-d7635eaead19","_uuid":"514ab8a0e1a42944a22212c661d3667f776465ca"},"outputs":[],"source":"ma_is.head()","execution_count":null,"cell_type":"code"},{"metadata":{"_cell_guid":"ee38b647-ea41-4353-b26b-7c0ad5da9ff0","_uuid":"9c8772b5d69b06666c0e609ccbcabdad3f3db44a"},"source":"# Tensorflow Timesereies - ARRegressor","cell_type":"markdown"},{"metadata":{"collapsed":true,"_cell_guid":"04ca2f07-70ca-4d40-b95e-ba106a0ffbf2","_uuid":"ea1c2a838e196738b0abd6a2e1040b3fb1ea2188"},"outputs":[],"source":"def data_to_npreader(store_nbr: int, item_nbr: int) -> NumpyReader:\n    unit_sales = train[np.logical_and(train[\"store_nbr\"] == store_nbr,\n                                      train['item_nbr'] == item_nbr)].unit_sales\n\n    x = np.asarray(range(len(unit_sales)))\n    y = np.asarray(unit_sales)\n\n    dataset = {\n        tf.contrib.timeseries.TrainEvalFeatures.TIMES: x,\n        tf.contrib.timeseries.TrainEvalFeatures.VALUES: y,\n    }\n\n    reader = NumpyReader(dataset)\n    return x, y, reader","execution_count":null,"cell_type":"code"},{"metadata":{"_cell_guid":"3760145a-fde5-4bca-947f-3475c0a36046","_uuid":"6cc396718a2430981db7e0d690e9c9b60cdb7034"},"outputs":[],"source":"x, y, reader = data_to_npreader(store_nbr=1, item_nbr=105574)\n\ntrain_input_fn = tf.contrib.timeseries.RandomWindowInputFn(\n        reader, batch_size=32, window_size=40)\n\nar = tf.contrib.timeseries.ARRegressor(\n    periodicities=21, input_window_size=30, output_window_size=10,\n    num_features=1,\n    loss=tf.contrib.timeseries.ARModel.NORMAL_LIKELIHOOD_LOSS\n)\n\nar.train(input_fn=train_input_fn, steps=16000)","execution_count":null,"cell_type":"code"},{"metadata":{"_cell_guid":"1cdc09b0-88be-494c-bc0b-ee266db7b568","_uuid":"9f737adbef3da70ca4a0e395d443c72b720448f5"},"outputs":[],"source":"evaluation_input_fn = tf.contrib.timeseries.WholeDatasetInputFn(reader)\n# keys of evaluation: ['covariance', 'loss', 'mean', 'observed', 'start_tuple', 'times', 'global_step']\nevaluation = ar.evaluate(input_fn=evaluation_input_fn, steps=1)\n\n(ar_predictions,) = tuple(ar.predict(\n    input_fn=tf.contrib.timeseries.predict_continuation_input_fn(\n        evaluation, steps=16)))","execution_count":null,"cell_type":"code"},{"metadata":{"_cell_guid":"95b6f2c8-f005-41d7-bddb-14ce7f7a1705","_uuid":"5bee10b5b487fcc904582dd79be811c1278a4ca2"},"outputs":[],"source":"plt.figure(figsize=(15, 5))\nplt.plot(x.reshape(-1), y.reshape(-1), label='origin')\nplt.plot(evaluation['times'].reshape(-1), evaluation['mean'].reshape(-1), label='evaluation')\nplt.plot(ar_predictions['times'].reshape(-1), ar_predictions['mean'].reshape(-1), label='prediction')\nplt.xlabel('time_step')\nplt.ylabel('values')\nplt.legend(loc=4)\nplt.show()","execution_count":null,"cell_type":"code"},{"metadata":{"_cell_guid":"a623f6f2-2eed-4c47-bea5-0145e801072f","_uuid":"ce9d93549be5c55eb35ef9ab33afdeea59d5eced"},"source":"# Tensorflow Timesereies - LSTM","cell_type":"markdown"},{"metadata":{"collapsed":true,"_cell_guid":"96fd1e2c-5cea-4917-b1f6-527beb5ee644","_uuid":"b6cc4f1f4dd58df829e3264e502c638b6293b711"},"outputs":[],"source":"class _LSTMModel(tfts_model.SequentialTimeSeriesModel):\n    \"\"\"A time series model-building example using an RNNCell.\"\"\"\n    \n    def __init__(self, num_units, num_features, dtype=tf.float32):\n        \"\"\"Initialize/configure the model object.\n        Note that we do not start graph building here. Rather, this object is a\n        configurable factory for TensorFlow graphs which are run by an Estimator.\n        Args:\n          num_units: The number of units in the model's LSTMCell.\n          num_features: The dimensionality of the time series (features per\n            timestep).\n          dtype: The floating point data type to use.\n        \"\"\"\n        \n        super(_LSTMModel, self).__init__(\n            # Pre-register the metrics we'll be outputting (just a mean here).\n            train_output_names=[\"mean\"],\n            predict_output_names=[\"mean\"],\n            num_features=num_features,\n            dtype=dtype)\n        self._num_units = num_units\n        # Filled in by initialize_graph()\n        self._lstm_cell = None\n        self._lstm_cell_run = None\n        self._predict_from_lstm_output = None\n\n    def initialize_graph(self, input_statistics):\n        \"\"\"Save templates for components, which can then be used repeatedly.\n        This method is called every time a new graph is created. It's safe to start\n        adding ops to the current default graph here, but the graph should be\n        constructed from scratch.\n        Args:\n          input_statistics: A math_utils.InputStatistics object.\n        \"\"\"\n        \n        super(_LSTMModel, self).initialize_graph(input_statistics=input_statistics)\n        self._lstm_cell = tf.nn.rnn_cell.LSTMCell(num_units=self._num_units)\n        # Create templates so we don't have to worry about variable reuse.\n        self._lstm_cell_run = tf.make_template(\n            name_=\"lstm_cell\",\n            func_=self._lstm_cell,\n            create_scope_now_=True)\n        # Transforms LSTM output into mean predictions.\n        self._predict_from_lstm_output = tf.make_template(\n            name_=\"predict_from_lstm_output\",\n            func_=\n            lambda inputs: tf.layers.dense(inputs=inputs, units=self.num_features),\n            create_scope_now_=True)\n\n    def get_start_state(self):\n        \"\"\"Return initial state for the time series model.\"\"\"\n        return (\n            # Keeps track of the time associated with this state for error checking.\n            tf.zeros([], dtype=tf.int64),\n            # The previous observation or prediction.\n            tf.zeros([self.num_features], dtype=self.dtype),\n            # The state of the RNNCell (batch dimension removed since this parent\n            # class will broadcast).\n            [tf.squeeze(state_element, axis=0)\n             for state_element\n             in self._lstm_cell.zero_state(batch_size=1, dtype=self.dtype)])\n\n    def _filtering_step(self, current_times, current_values, state, predictions):\n        \"\"\"Update model state based on observations.\n        Note that we don't do much here aside from computing a loss. In this case\n        it's easier to update the RNN state in _prediction_step, since that covers\n        running the RNN both on observations (from this method) and our own\n        predictions. This distinction can be important for probabilistic models,\n        where repeatedly predicting without filtering should lead to low-confidence\n        predictions.\n        Args:\n          current_times: A [batch size] integer Tensor.\n          current_values: A [batch size, self.num_features] floating point Tensor\n            with new observations.\n          state: The model's state tuple.\n          predictions: The output of the previous `_prediction_step`.\n        Returns:\n          A tuple of new state and a predictions dictionary updated to include a\n          loss (note that we could also return other measures of goodness of fit,\n          although only \"loss\" will be optimized).\n        \"\"\"\n        state_from_time, prediction, lstm_state = state\n        with tf.control_dependencies(\n            [tf.assert_equal(current_times, state_from_time)]):\n          # Subtract the mean and divide by the variance of the series.  Slightly\n          # more efficient if done for a whole window (using the normalize_features\n          # argument to SequentialTimeSeriesModel).\n          transformed_values = self._scale_data(current_values)\n          # Use mean squared error across features for the loss.\n          predictions[\"loss\"] = tf.reduce_mean(\n              (prediction - transformed_values) ** 2, axis=-1)\n          # Keep track of the new observation in model state. It won't be run\n          # through the LSTM until the next _imputation_step.\n          new_state_tuple = (current_times, transformed_values, lstm_state)\n        return (new_state_tuple, predictions)\n\n    def _prediction_step(self, current_times, state):\n        \"\"\"Advance the RNN state using a previous observation or prediction.\"\"\"\n        _, previous_observation_or_prediction, lstm_state = state\n        lstm_output, new_lstm_state = self._lstm_cell_run(\n            inputs=previous_observation_or_prediction, state=lstm_state)\n        next_prediction = self._predict_from_lstm_output(lstm_output)\n        new_state_tuple = (current_times, next_prediction, new_lstm_state)\n        return new_state_tuple, {\"mean\": self._scale_back_data(next_prediction)}\n\n    def _imputation_step(self, current_times, state):\n        \"\"\"Advance model state across a gap.\"\"\"\n        # Does not do anything special if we're jumping across a gap. More advanced\n        # models, especially probabilistic ones, would want a special case that\n        # depends on the gap size.\n        return state\n\n    def _exogenous_input_step(\n        self, current_times, current_exogenous_regressors, state):\n        \"\"\"Update model state based on exogenous regressors.\"\"\"\n        raise NotImplementedError(\n            \"Exogenous inputs are not implemented for this example.\")","execution_count":null,"cell_type":"code"},{"metadata":{"_cell_guid":"120a99df-9b77-4cdf-b38f-a63994d8dc5e","_uuid":"3792f03defbbe5c1980bb8d9fb5c0f727070b177"},"outputs":[],"source":"x, y, reader = data_to_npreader(store_nbr=2, item_nbr=105574)\n\ntrain_input_fn = tf.contrib.timeseries.RandomWindowInputFn(\n      reader, batch_size=16, window_size=21)\n\nestimator = tfts_estimators.TimeSeriesRegressor(\n      model=_LSTMModel(num_features=1, num_units=32),\n      optimizer=tf.train.AdamOptimizer(0.001))\n\nestimator.train(input_fn=train_input_fn, steps=16000)\nevaluation_input_fn = tf.contrib.timeseries.WholeDatasetInputFn(reader)\nevaluation = estimator.evaluate(input_fn=evaluation_input_fn, steps=1)","execution_count":null,"cell_type":"code"},{"metadata":{"_cell_guid":"0fad5946-6e01-4314-ae25-c0390123c543","_uuid":"0044cad7222be25f719c6ad2537f58b59b6a0571"},"outputs":[],"source":"(lstm_predictions,) = tuple(estimator.predict(\n      input_fn=tf.contrib.timeseries.predict_continuation_input_fn(\n          evaluation, steps=16)))","execution_count":null,"cell_type":"code"},{"metadata":{"_cell_guid":"c9d318ff-ebf9-4b1f-be27-984742315c92","_uuid":"1d41fbd86bcfd4c355e6995a8db14131e741a84b"},"outputs":[],"source":"plt.figure(figsize=(15, 5))\nplt.plot(x.reshape(-1), y.reshape(-1), label='origin')\nplt.plot(evaluation['times'].reshape(-1), evaluation['mean'].reshape(-1), label='evaluation')\nplt.plot(lstm_predictions['times'].reshape(-1), lstm_predictions['mean'].reshape(-1), label='prediction')\nplt.xlabel('time_step')\nplt.ylabel('values')\nplt.legend(loc=4)\nplt.show()","execution_count":null,"cell_type":"code"},{"metadata":{"_cell_guid":"3e64d9f4-5789-4ca9-a31f-2307b4528b66","_uuid":"5364f70e08646a689374e90b6f5c8f94d712b5b7"},"source":"# Forecasting Test data","cell_type":"markdown"},{"metadata":{"collapsed":true,"_cell_guid":"67df9b2d-499a-4a73-b02a-32f72aa07cd4","_uuid":"19a8725a462fa90c7b98a1b9075d19b5e2e72d7c"},"outputs":[],"source":"# Read test dataset\ntest = pd.read_csv('../input/test.csv', dtype=dtypes, parse_dates=['date'])\ntest['dow'] = test['date'].dt.dayofweek\n\n# Moving Average\ntest = pd.merge(test, ma_is, how='left', on=['item_nbr','store_nbr'])\ntest = pd.merge(test, ma_wk, how='left', on=['item_nbr','store_nbr'])\ntest = pd.merge(test, ma_dw, how='left', on=['item_nbr','store_nbr','dow'])\ntest['unit_sales'] = test.mais\n\n# Autoregressive\nar_predictions['mean'][ar_predictions['mean'] < 0] = 0\ntest.loc[np.logical_and(test['store_nbr'] == 1, test['item_nbr'] == 105574), 'unit_sales'] = ar_predictions['mean']\n\n# LSTM\nlstm_predictions['mean'][lstm_predictions['mean'] < 0] = 0\ntest.loc[np.logical_and(test['store_nbr'] == 2, test['item_nbr'] == 105574), 'unit_sales'] = lstm_predictions['mean']","execution_count":null,"cell_type":"code"},{"metadata":{"collapsed":true,"_cell_guid":"ee48e38c-aef9-4131-9498-689636a0f665","_uuid":"4f5f0edbbcff0e2ba99f0c07a6dd001cff516ed2"},"outputs":[],"source":"pos_idx = test['mawk'] > 0\ntest_pos = test.loc[pos_idx]\ntest.loc[pos_idx, 'unit_sales'] = test_pos['unit_sales'] * test_pos['madw'] / test_pos['mawk']","execution_count":null,"cell_type":"code"},{"metadata":{"collapsed":true,"_cell_guid":"f82e2a4f-8b01-4a05-a695-fb6ec3cc5447","_uuid":"2347f4b4dc4f50b20ee073143e77e3b4f27a4d08"},"outputs":[],"source":"test.loc[:, \"unit_sales\"].fillna(0, inplace=True)\ntest['unit_sales'] = test['unit_sales'].apply(pd.np.expm1) # restoring unit values \ntest['mais'] = test['mais'].apply(pd.np.expm1) # restoring unit values ","execution_count":null,"cell_type":"code"},{"metadata":{"collapsed":true,"_cell_guid":"e8932480-8a30-4087-819e-a64b49f3ab8c","_uuid":"087eab72f32ac3c0bec3f6b82bbeb61d456868da"},"outputs":[],"source":"holiday = pd.read_csv('../input/holidays_events.csv', parse_dates=['date'])\nholiday = holiday.loc[holiday['transferred'] == False]\n\ntest = pd.merge(test, holiday, how = 'left', on =['date'] )\ntest['transferred'].fillna(True, inplace=True)\n\ntest.loc[test['transferred'] == False, 'unit_sales'] *= 1.2\ntest.loc[test['onpromotion'] == True, 'unit_sales'] *= 1.5","execution_count":null,"cell_type":"code"},{"metadata":{"_cell_guid":"3f12bfe1-0815-4232-b81a-7aa8107eb96e","_uuid":"a300657ff597ecb5f953bc7a3ece5f40203ca5e6"},"outputs":[],"source":"test.loc[np.logical_and(test['store_nbr'] == 1, test['item_nbr'] == 105574)]","execution_count":null,"cell_type":"code"},{"metadata":{"collapsed":true,"_cell_guid":"d94181f1-0909-4eae-adc6-e864b8fd5fe6","_uuid":"671d5646b0defc79a1ad0272a4714347772ffb7b"},"outputs":[],"source":"test[['id','unit_sales']].to_csv('submission.csv.gz', index=False, compression='gzip')","execution_count":null,"cell_type":"code"}]}