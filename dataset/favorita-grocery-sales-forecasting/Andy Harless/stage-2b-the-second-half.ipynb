{"cells":[{"source":"This is part of an experiment:  since the full run times out on Kaggle, how about running two halves separately and then combining the results?  We'll see.  This is the second half.  (The is the second half of stage 2.  Stage 1 was data preparation, as described in the next paragraph.  Stage 3 will be combining the results from stage 2.)","cell_type":"markdown","metadata":{"_uuid":"75e7f910a9e0bb6857a29a3a63aa962216b682bc","_cell_guid":"27470270-7b03-40dc-8db9-0c0def53f919"}},{"source":"Running a neural network using the data prepared by [this script](https://www.kaggle.com/aharless/preparing-data-for-lgbm-or-something-else/output).  The data represent the fitting and validation scheme derived from Ceshine's [LGBM starter](https://www.kaggle.com/ceshine/lgbm-starter) script with the additional features created in [Lingzhi's upgraded version](https://www.kaggle.com/vrtjso/lgbm-one-step-ahead).  So far I've made no attempt to preprocess them further for optimal use by the neural network.  (I used [kaggleslayer's code](https://www.kaggle.com/kaggleslayer/grocery-prediction-with-neural-network) as a starting point for the structure of the network itself, but now I've made some revisions.)\n","cell_type":"markdown","metadata":{"_uuid":"97d6551fade88ca26ef8da160f00f086da88a157","_cell_guid":"73cf795d-9fbb-4844-b11f-7ca191241a0a"}},{"source":"N_EPOCHS = 10\nN_DAYS = 8  # Limit to first half","execution_count":null,"outputs":[],"cell_type":"code","metadata":{"collapsed":true,"_uuid":"aa8103a83aab37756721ec8d32e17c0d44fe495e","_cell_guid":"7343183e-c8c9-4177-85c3-25502c3aaf4e"}},{"source":"from datetime import date, timedelta\nimport pandas as pd\nimport numpy as np\nfrom sklearn.metrics import mean_squared_error\nimport lightgbm as lgb\nfrom keras.models import Sequential\nfrom keras.layers.core import Dense, Activation\nfrom keras import callbacks\nfrom keras.callbacks import ModelCheckpoint\nfrom keras.layers import Dropout, BatchNormalization\nfrom keras.layers.advanced_activations import PReLU\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))","execution_count":null,"outputs":[],"cell_type":"code","metadata":{"collapsed":true,"_uuid":"8ec47e6264e7a62a2f555101331b5ef20545bb95","_cell_guid":"1e632d4a-2fe8-4a4e-bc96-5b7c985ad3a5"}},{"source":"indir = '../input/preparing-data-for-lgbm-or-something-else/'\nindir2 = '../input/favorita-grocery-sales-forecasting/'","execution_count":null,"outputs":[],"cell_type":"code","metadata":{"collapsed":true,"_uuid":"d87c09bff2d91506fb431c64864c2ca85a1d345c","_cell_guid":"6ddda393-3bf0-4427-8090-4afd29fed0ae"}},{"source":"X_test = pd.read_csv(indir + 'X_test.csv')\nX_val = pd.read_csv(indir + 'X_val.csv')\nX_train = pd.read_csv(indir + 'X_train.csv')\ny_train = np.array(pd.read_csv(indir + 'y_train.csv'))\ny_val = np.array(pd.read_csv(indir + 'y_val.csv'))\nstores_items = pd.read_csv(indir + 'stores_items.csv', index_col=['store_nbr','item_nbr'])\ntest_ids = pd.read_csv( indir + 'test_ids.csv',  parse_dates=['date']).set_index(\n                        ['store_nbr', 'item_nbr', 'date'] )\nitems = pd.read_csv( indir2 + 'items.csv' ).set_index(\"item_nbr\")\nitems = items.reindex( stores_items.index.get_level_values(1) )","execution_count":null,"outputs":[],"cell_type":"code","metadata":{"collapsed":true,"_uuid":"12b8b208eea67cc7d63b812f7f74ea859c970cb2","_cell_guid":"e0a3cdab-1e83-4eac-8a65-9087b1c7bcf1"}},{"source":"model = Sequential()\nmodel.add(Dense(32, kernel_initializer='normal', input_shape=(X_train.shape[1],)))\nmodel.add(PReLU())\nmodel.add(BatchNormalization())\nmodel.add(Dropout(.25))\nmodel.add(Dense(16, kernel_initializer='normal'))\nmodel.add(PReLU())\nmodel.add(BatchNormalization())\nmodel.add(Dropout(.18))\nmodel.add(Dense(8, kernel_initializer='normal'))\nmodel.add(PReLU())\nmodel.add(BatchNormalization())\nmodel.add(Dropout(.12))\nmodel.add(Dense(1, kernel_initializer='normal'))\nmodel.compile(loss = 'mse', optimizer='adam', metrics=['mse'])","execution_count":null,"outputs":[],"cell_type":"code","metadata":{"collapsed":true,"_uuid":"9bf8c811452e5d4ad2bf4cbbec7d6e38e6a16ca5","_cell_guid":"82cf743b-d35b-4d76-bdc0-bef713bd7b95"}},{"source":"Note: See this [stack overflow post](https://stackoverflow.com/questions/47802601/nonetype-error-on-saving-model-in-keras) on the problem that led me to comment out the early stopping code in the block below.","cell_type":"markdown","metadata":{"_uuid":"a00783d1c33626026f02c01b0a85f9483abbde37","_cell_guid":"228e291b-df30-4c2b-b6b9-1dab7ed8ca8c"}},{"source":"val_pred = []\ntest_pred = []\n# wtpath = 'weights.hdf5'  # To save best epoch. But need Keras bug to be fixed first.\nsample_weights=np.array( pd.concat([items[\"perishable\"]] * 6) * 0.25 + 1 )\nfor i in range(N_DAYS, 16):\n    print(\"=\" * 50)\n    print(\"Step %d\" % (i+1))\n    print(\"=\" * 50)\n    y = y_train[:, i]\n    xv = np.array(X_val)\n    yv = y_val[:, i]\n#    bestepoch = ModelCheckpoint( filepath=wtpath, verbose=1, save_best_only=True )\n    model.fit( np.array(X_train), y, batch_size = 128, epochs = N_EPOCHS, verbose=2,\n               sample_weight=sample_weights, validation_data=(xv,yv) ) \n             #, callbacks=[bestepoch] # bestepoch doesn't work: keras bug\n#    model.load_weights( wtpath )\n    val_pred.append(model.predict(X_val))\n    test_pred.append(model.predict(X_test))","execution_count":null,"outputs":[],"cell_type":"code","metadata":{"collapsed":true,"_uuid":"c94a579ef3229e57fe8d01fd066e0ae29f0c1bea","_cell_guid":"d8d0fedb-c005-4c84-821d-38594af832cf"}},{"source":"weights=pd.concat([items[\"perishable\"]]) * 0.25 + 1\nprint(\"Unweighted validation mse: \", mean_squared_error(\n    y_val[:,N_DAYS:], np.array(val_pred).squeeze(axis=2).transpose()) )\nprint(\"Partial weighted mse:      \", mean_squared_error(\n    y_val[:,N_DAYS:], np.array(val_pred).squeeze(axis=2).transpose(), sample_weight=weights) )","execution_count":null,"outputs":[],"cell_type":"code","metadata":{"collapsed":true,"_uuid":"945900465e5c76fed6228c59787041b4b775c6de","_cell_guid":"2ce03313-40c7-44bf-904e-f60cdc6bd17b"}},{"source":"pd.DataFrame(np.array(val_pred).squeeze(axis=2).transpose()).to_csv('nn_val_second_half.csv', \n                                                                    float_format='%.5f', \n                                                                    index=None)","execution_count":null,"outputs":[],"cell_type":"code","metadata":{"collapsed":true,"_uuid":"1bb18392bd3ad310623ba691735fbfb936c77a95","_cell_guid":"ab5098b9-416c-4d31-ad0b-cff6fadf9f53"}},{"source":"START = \"2017-08-24\"\ndays = 16 - N_DAYS\ny_test = np.array(test_pred).squeeze(axis=2).transpose()\ndf_preds = pd.DataFrame(\n    y_test, index=stores_items.index,\n    columns=pd.date_range(START, periods=days)\n).stack().to_frame(\"unit_sales\")\ndf_preds.index.set_names([\"store_nbr\", \"item_nbr\", \"date\"], inplace=True)","execution_count":null,"outputs":[],"cell_type":"code","metadata":{"collapsed":true,"_uuid":"f66a8fb43fca35e195122ed62ef2eaba9b696f7d","_cell_guid":"3dd23e34-fe1b-43f8-a4c8-9e2f775e8220"}},{"source":"submission = test_ids.join(df_preds, how=\"left\").fillna(0)\nsubmission[\"unit_sales\"] = np.clip(np.expm1(submission[\"unit_sales\"]), 0, 1000)\nsubmission.to_csv('nn_sub_second_half.csv', float_format='%.5f', index=None)","execution_count":null,"outputs":[],"cell_type":"code","metadata":{"collapsed":true,"_uuid":"96ccc1e0484c7796758e74e98fac428cdcdcd000","_cell_guid":"dfa7b3d1-0506-4b49-859d-54152c839a76"}}],"nbformat_minor":1,"nbformat":4,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"version":"3.6.3","pygments_lexer":"ipython3","nbconvert_exporter":"python","mimetype":"text/x-python","name":"python","codemirror_mode":{"version":3,"name":"ipython"},"file_extension":".py"}}}