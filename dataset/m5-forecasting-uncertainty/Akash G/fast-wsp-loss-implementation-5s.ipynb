{"cells":[{"metadata":{},"cell_type":"markdown","source":"The M5 competition loss is a little ugly, epecially when it comes to the Uncertainty's one with it's WSPL! So far, there is no, if not little, public implementation of that *beast* (I still talking about the WSPL and not *the american  crime drama series*).\n\nAs I was saying it on [M5  Accuracy](https://www.kaggle.com/kneroma/faster-loss-function-with-lb-position-500ms) my implementation is based on a deep inspection of the loss combined with an intelligent choice of pandas' components. \n\n**Too much talk, let's dive in it !**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<h3 style=\"color: red;\">If you find this work helpful, please don't forget upvoting in order to get me motivated in sharing my hard work<h3/>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from pathlib import Path\nimport pandas as pd, numpy as np\nnp.set_printoptions(suppress=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"! ls ../input","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"DATA_ROOT = Path(\"../input/m5-forecasting-uncertainty\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Let's configure everything here","execution_count":null},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"class M5Config:\n    def __init__(self,cat_cols=None,sales_path=None, add_fake_categories=True, start=1,\n                     end = 1913, days=None, evaluation=False,\n                read_calendar=True, read_sales=True, read_prices=True, read_sample_submission=False):\n        self.cat_cols = [\"id\", \"cat_id\", \"state_id\", \"dept_id\", \"store_id\", \"item_id\"] if cat_cols is None else cat_cols\n        self.col_groups = [\n                ('Total', 'X'),\n                ('cat_id', 'X'),\n                ('state_id', 'X'),\n                ('dept_id', 'X'),\n                ('store_id', 'X'),\n                ('item_id', 'X'),\n                ('state_id', 'cat_id'),\n                ('state_id', 'dept_id'),\n                ('store_id', 'cat_id'),\n                ('store_id', 'dept_id'),\n                ('state_id','item_id'),\n                ('item_id', 'store_id')]\n\n        self.evaluation = False\n        self.suffix = \"evaluation\" if self.evaluation else \"validation\"\n\n        self.sales_path = DATA_ROOT/f'sales_train_{self.suffix}.csv' if sales_path is None else sales_path\n        self.calendar_path = DATA_ROOT/\"calendar.csv\"\n        self.prices_path = DATA_ROOT/\"sell_prices.csv\"\n        self.sample_submission_path = DATA_ROOT/\"sample_submission.csv\"\n\n        self.add_fake_categories = add_fake_categories\n\n        self.start = start\n        self.end = end\n\n        \n        if days is None:\n            self.set_days()\n        else:\n            self.days = days\n\n        assert end > 28\n        self.set_weight_days()\n        \n        self.read_calendar = read_calendar\n        self.read_sales = read_sales\n        self.read_prices = read_prices\n        self.read_sample_submission = False\n        \n    def set_days(self):\n        self.days = [f\"d_{i}\" for i in range(self.start,self.end+1)]\n    \n    def set_weight_days(self):\n        self.weight_days = [f\"d_{i}\" for i in range(self.end-27, self.end+1)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# And what if we make a generic class to laod the data when needed ?","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"class M5Data:\n    def __init__(self, config=None):\n        self.config = config if config is not None else M5Config()\n        \n        self.cal = self.read(self.config.calendar_path) if self.config.read_calendar else None\n        self.sales = self.read(self.config.sales_path, usecols = self.config.cat_cols\n                               + self.config.days) if self.config.read_sales else None\n        self.prices = self.read(self.config.prices_path) if self.config.read_prices else None\n        self.sample_submission = self.read(self.config.sample_submission_path)\\\n                                            if self.config.read_sample_submission else None\n        \n        if self.config.add_fake_categories:\n            self.add_fake_categories()\n            \n        \n    def read(self, path, usecols=None):\n        return pd.read_csv(path.as_posix(), usecols=usecols)\n    \n    \n    def add_fake_categories(self):\n        self.sales[\"Total\"] = \"Total\"\n        self.sales[\"X\"] = \"X\"\n        \n        \n        \n    \n    def _to_42840(self, group):\n        assert group in self.config.col_groups\n#         print(group, self.sales.columns)\n        group = list(group)\n        df = self.sales[group+self.config.days].groupby(group)[self.config.days].sum()\n        df.reset_index(inplace=True)\n        df.rename(columns={group[0]:\"level1_val\", group[1]:\"level2_val\"}, inplace=True)\n        df[\"level1_name\"] = group[0]\n        df[\"level2_name\"] = group[1]\n        df[\"group_id\"] = self.config.col_groups.index(tuple(group))\n        df = df[[\"group_id\", \"level1_name\", \"level2_name\", \"level1_val\", \"level2_val\"]+self.config.days]\n        \n        return df\n    \n    def to_42840(self):\n        \n        df = pd.concat([self._to_42840(group) for group in self.config.col_groups], axis=0,sort=False)\n        df.sort_values([ \"level1_val\", \"level2_val\"], inplace=True)\n        df.reset_index(drop=True, inplace=True)\n        \n        return df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### You can give a try to the M5Data API here :","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\ndata = M5Data()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.sales.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\ndata._to_42840((\"item_id\", \"store_id\")).head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Powerful API for Weights & Scales computation","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"class WeightAndScaleComputer:\n    def __init__(self, config=None, data=None):\n        self.config = config if config is not None else M5Config()\n        self.data = data if data is not None else M5Data(config=self.config)\n        \n        self.df_42840 = self.data.to_42840()\n        \n    def get_prices(self):\n        prices = self.data.prices.copy()\n        cal = self.data.cal\n\n        prices[\"id\"] = [\"{}_{}_{}\".format(item_id, store_id, self.config.suffix) \n                            for item_id, store_id in zip(prices.item_id, prices.store_id)] \n        \n        day_count = cal[\"d\"].str.replace(\"d_\", \"\").astype(int)\n\n        prices = prices[[\"wm_yr_wk\", \"id\", \"sell_price\"]].merge(\n                cal.loc[(day_count>= self.config.end-27) & (day_count<= self.config.end), [\"wm_yr_wk\", \"d\"]],\n                                                on = [\"wm_yr_wk\"])\n        prices = prices.set_index([\"id\", \"d\"]).sell_price.fillna(0.).unstack().fillna(0.)\n        return prices\n        \n    \n    def get_weights(self):\n        \n        # Backup old data\n        sales_backup = self.data.sales\n        df_42840_backup  = self.df_42840\n\n        data = self.data\n        sales = data.sales\n        \n        sales.sort_values(\"id\",inplace=True)\n        sales.reset_index(inplace=True, drop=True)\n        \n        prices = self.get_prices()\n        prices.sort_index(inplace=True)\n        prices.reset_index(inplace=True, drop=True)\n        prices = prices[self.config.weight_days]\n        \n        for i,col in enumerate(self.config.weight_days):\n            sales[col] = sales[col]*prices[col].values\n            \n        data.sales = sales\n        df_42840 = data.to_42840()\n        \n        df_42840[\"turnover\"] = df_42840[self.config.weight_days].sum(axis=1)\n        df_42840[\"level_turnover\"] = df_42840.groupby([\"level1_name\",\"level2_name\"]).turnover.transform(\"sum\")\n        df = df_42840[[\"group_id\", \"level1_name\", \"level2_name\", \"level1_val\", \"level2_val\"]].copy()\n        df[\"weights\"] = df_42840[\"turnover\"]/df_42840[\"level_turnover\"].values\n        \n        df.sort_values([\"level1_val\", \"level2_val\"], inplace=True)\n        df.reset_index(drop=True,inplace=True)\n        \n        # Restore old data\n        self.data.sales = sales_backup\n        self.df_42840 = df_42840_backup\n        \n        return df\n    \n    \n    def get_scales(self, kind=\"mae\"): # kind in ['mae', 'mse']\n        assert kind in ['mae', 'mse']\n        \n        df = self.df_42840[self.config.days].values\n        \n        diff = (np.abs if kind == \"mae\" else np.square )(df[:, 1:] - df[:, :-1]) \n        \n        is_start = df[:, :-1].cumsum(1) >= 1\n        \n        diff *= is_start\n        \n        starts = is_start.argmax(1)\n        size = df.shape[1] - starts - 1\n        \n        scales = diff.sum(1)/size\n        \n        df = self.df_42840[[\"level1_val\", \"level2_val\"]].copy()\n        df[\"scales\"] = scales\n        \n        df.sort_values([\"level1_val\", \"level2_val\"], inplace=True)\n        df.reset_index(drop=True,inplace=True)\n        \n        return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Weighted Scaled Pinball Loss","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"class WeightedScaledPinballLoss:\n        \"\"\"A fast routine for the  Weighted Scaled Pinball Loss (WSPL).\n\nThis might be slow (one to two minutes) at initialisation in order to initiate all the routines required \nto accelerate on-the-fly WSPL computation.\n\"\"\"\n        \n        def __init__(self, scales, weights, data=None, qs=None):\n            self.set_weights_and_scales(weights = weights, scales =  scales)\n            self._scales = self.weights_and_scales[\"scales\"].values\n            self._weights = self.weights_and_scales[\"weights\"].values\n            \n            self.qs = np.array([0.005, 0.025, 0.165, 0.25 , 0.5  , 0.75 , 0.835, 0.975, 0.995])\\\n                                                        if qs is None else np.array(qs)\n            \n            if data is None:\n                config = M5Config(sales_path=DATA_ROOT/\"sales_train_evaluation.csv\", start = 1914, end=1914+27,\n                                        read_calendar=False,read_prices=False,read_sample_submission=False)\n                data = M5Data(config)\n                data.sales[\"id\"] = data.sales[\"id\"].str.replace(\"evaluation\", \"validation\")\n                \n            self.data = data\n            \n            df_42840 = data.to_42840()\n\n            df_42840[\"id_no_q\"] = df_42840[\"level1_val\"].str.cat(df_42840[\"level2_val\"], \n                                                                 sep=\"_\").str.cat([\"_validation\"]*len(df_42840))\n            df_42840 = df_42840.sort_values(\"id_no_q\").reset_index(drop=True)\n\n            self.df_42840 = df_42840[self.data.config.days].values\n            \n            self.submission_config = M5Config(cat_cols = [\"id\"], days = [f\"F{i}\" for  i in range(1,29)],\n                                read_calendar=False,read_prices=False,read_sample_submission=False)\n            \n            \n        def set_weights_and_scales(self, weights, scales):\n            weights = weights.copy()\n            weights = weights.merge(scales,on=[\"level1_val\", \"level2_val\"])\n            weights[\"id_no_q\"] = weights[\"level1_val\"].str.cat(weights[\"level2_val\"], \n                                                               sep=\"_\").str.cat([\"_validation\"]*len(weights))\n            weights = weights.sort_values(\"id_no_q\").reset_index(drop=True)\n            \n            self.weights_and_scales = weights\n                \n            \n        def score(self, y_pred ):\n            \"\"\"Compute the WSPL.\n            \n            Parameters:\n            -----------\n            y_true: pd.DataFrame, Path,str-path \n                pd.DataFram or path to a pd.DataFrame that consists of daily 42840x28 evaluation data.\n                This dataframe must includes the 'id' column. \n                \n            y_pred: pd.DataFrame, Path,str-path \n                pd.DataFram or path to a pd.DataFrame that consists of daily 4(2840x9)x28 prediction data.\n                This dataframe must includes the 'id' column.\n            \"\"\"\n            y_true = self.df_42840\n            y_pred = self.get_sub_data(y_pred)\n            assert (9, *y_true.shape) == y_pred.shape\n            \n            diff =   y_true[None] - y_pred\n            diff = np.maximum(diff*self.qs[:, None, None], diff*(self.qs[:, None, None]-1)).mean(2)\n            wspl = np.sum(diff*self._weights[None]/self._scales[None])/(9*12)\n            \n            return wspl\n        \n        def get_sub_data(self, data):\n            \n            sales = data\n        \n            if isinstance(sales, (str, Path)):\n                sales = pd.read_csv(Path(sales).as_posix())\n            else:\n                assert isinstance(sales, pd.DataFrame)\n                \n            sales = sales[sales[\"id\"].str.endswith(\"validation\")].copy()\n            \n            if not \"_q_\" in sales.columns:\n                sales[\"_q_\"] = sales[\"id\"].str.extract(r\"_(\\d+\\.\\d+)_\").astype(float)\n            if not \"id_no_q\" in sales.columns:\n                sales[\"id_no_q\"] = sales[\"id\"].str.replace(r\"_(\\d+\\.\\d+)_\", \"_\")\n\n            sales.sort_values([\"_q_\", \"id_no_q\"], inplace=True)\n            \n            sales = sales[[f\"F{i}\" for i in range(1, 29)]].values.reshape((9, 42840, 28 ))\n\n            return sales","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Some tests for our loss API","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\nwsc = WeightAndScaleComputer()\nwspl_scales = wsc.get_scales(kind=\"mae\")\nweights = wsc.get_weights()\nwspl_scales.shape, weights.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"weights[(weights.level1_name==\"state_id\")&(weights.level2_name==\"cat_id\")]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Let's test it !\nNow we can give a real-world test to our API by using [this best uncertainty public kernels dataset](https://www.kaggle.com/kneroma/m5-uncertainty-best-public-lbs). Please upvote the dataset to make it more visible for all.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"WSPL =  WeightedScaledPinballLoss(scales = wspl_scales, weights=weights )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"! ls ../input/m5-uncertainty-best-public-lbs","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### My older submission (public kernel)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"kkiller_017921 = pd.read_csv(\"../input/m5-uncertainty-best-public-lbs/Kkiller_FromPointToUncertainty_017921.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\nWSPL.score(kkiller_017921)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###  Ulrich's one","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"ulrich_012565 = pd.read_csv(\"../input/m5-uncertainty-weights/submission.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\nWSPL.score(ulrich_012565)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Krisztiansz's one","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"krisztiansz_015905 = pd.read_csv(\"../input/m5-uncertainty-best-public-lbs/\"+\n                                 \"KrisztianSz_PointToUncertaintyDifferentRangesPerLevel_015905.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\nWSPL.score(krisztiansz_015905)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Notes\n* Even if I put a lot of efforts making the evaluation function faster, there could still some room for improvements so your contribs' would be highly appreciated\n* There still some tiny differences between our API results and the Kaggle's one. I'm on it !\n* I'm hard-working currently to port this implementation on Pytorch/Tensorflow in order to be able to use it directly as loss function. Stay tuned !","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<p style=\"text-align:center;size:1.2rem;weight:200\">Thank you for reading my Kernel through the end!</p>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<h1 style=\"color:pink;text-align:center\">Good Luck Folks</h1>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}