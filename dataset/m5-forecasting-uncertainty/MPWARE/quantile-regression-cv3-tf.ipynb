{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Quantile Regression - Tensorflow CPU - CV3\n\n> This kernel is mainly a fork of Quantile-Regression-with-Keras interesting kernel from Ulrich GOUE. It adds TimeSeries split cross validation on the last 3 folds for better regularization.\n\n- Fork#1 from: https://www.kaggle.com/ulrich07/quantile-regression-with-keras\n- Fork#2 from: https://www.kaggle.com/chrisrichardmiles/m5u-wsplevaluator-weighted-scaled-pinball-loss\n- Some code optimization to make it simple.\n- LabelEncoder added.\n- Embedding dimensions rules updated.\n- Data starting from 2014-03-28. Float16 removed.\n- SCALED option added.\n- CV3 added for regularization.\n\n- v1.0: 2xDense64+1xDense32, dt_week_end category added, CV3: Last 3 folds (28 days each), EPOCH=20, LB=0.1720","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import mean_squared_error\nfrom tqdm.notebook import tqdm\nimport gc, os\nfrom scipy.sparse import csr_matrix\nfrom datetime import datetime, timedelta\nfrom sklearn.preprocessing import LabelEncoder\npd.set_option('display.max_colwidth', None)\npd.set_option('display.max_columns', 500)\npd.set_option('display.max_rows', 4000)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow.keras.layers as L\nimport tensorflow.keras.models as M\nimport tensorflow.keras.backend as K\nfrom tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\nimport tensorflow as tf\ntf.random.set_seed(2020)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def add_days(date_str, days=1, fmt=\"%Y-%m-%d\"):\n    date = datetime.strptime(date_str, fmt)\n    date = date + timedelta(days=days)\n    return datetime.strftime(date, fmt)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"os.listdir(\"/kaggle/input/m5-forecasting-uncertainty\")\n#os.listdir(\"/kaggle/input/m5ubasicfeatures-testscaled\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() / 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float32) # float16\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() / 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n    return df\n\ndef autocorrelation(ys, t=1):\n    return np.corrcoef(ys[:-t], ys[t:])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"SCALED = True","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Datasets with all levels timeseries + time features + scale. Starting on 2014-03-28 (to fit in kernel memory)\n# For level12, all leading zeros have been removed (according to calendar)\n# scale is computed on all data (since 2011) with train_pd[\"scale\"] = train_pd.groupby(['id'])[\"sales\"].transform(lambda x: np.abs(x - x.shift(1)).mean() )\nsales_train = pd.read_pickle(\"/kaggle/input/m5ubasicsfeaturesscaled/basic_features_2014-03-28.pkl\")\nsales_test = pd.read_pickle(\"/kaggle/input/m5ubasicfeatures-testscaled/basic_features_test_2014-03-28.pkl\")\n\nsales = pd.concat([sales_train, sales_test])\n    \nsales = pd.concat([sales_train, sales_test]).set_index([\"id\", \"date\"]).sort_index().reset_index()\n\n# Save memory now as we're not going to use these features.\nsales.drop(columns=['sell_price',\n 'event_type_1',\n 'event_name_2',\n 'event_type_2',\n 'dt_weekofyear',\n 'dt_quarter',\n 'dt_month_cursor'], inplace=True)\n\n# Add start date for each time series.\nsales[\"start\"] = sales.groupby('id')[\"date\"].transform(lambda x: x.min())\ndel sales_train, sales_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for c in sales.columns:\n    if 'dt_' in c:\n        sales[c] = sales[c].astype(np.int8) # Wrong for dt_year but no impact as using as categorical\nsales.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 13\ncategoricals_col = [\"snap_CA\", \"snap_TX\", \"snap_WI\", \"dt_weekday\", \"dt_month\", \"dt_year\", \"event_name_1\", \"dt_dayofmonth\", \"dt_weekend\",\n                   \"item_id\", \"dept_id\", \"cat_id\", \"store_id\", \"state_id\"]\nfor cat in tqdm(categoricals_col):\n    if (cat == \"event_name_1\") or (\"_id\" in cat):\n        sales[cat] = sales[cat].astype(str)\n    le_tmp = LabelEncoder()\n    le_tmp.fit(sales[cat])\n    sales[cat] = le_tmp.transform(sales[cat])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if SCALED == True:\n    sales[\"sales\"] = sales[\"sales\"] / sales[\"scale\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sales = reduce_mem_usage(sales)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"numericals = []\n\nLAGS = [28, 35, 42, 49, 56, 63]\nfor lag in tqdm(LAGS):\n    sales[f\"x_{lag}\"] = sales[[\"id\", \"sales\"]].groupby(\"id\")[\"sales\"].shift(lag)\n    numericals.append(f\"x_{lag}\")\n\nROLLS = [7, 14, 28]\nfor roll in tqdm(ROLLS):\n    for q in [0.95]:\n        sales[\"xr_q%.3f_%d\" % (q, roll)] = sales[[\"id\", \"sales\"]].groupby(\"id\")[\"sales\"].transform(lambda x: x.shift(28).rolling(roll).quantile(q))\n        numericals.append(\"xr_q%.3f_%d\" % (q, roll))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Additional drop if needed\ndrop_cols = [c for c in sales.columns if c not in [\"sales\", \"scale\", \"id\", \"date\", \"start\"] + numericals + categoricals_col]\nprint(drop_cols)\nsales.drop(columns=drop_cols, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Embedding rules. Max dim is 50, otherwise half of the uniques.\ncategoricals_info = {}\nfor c in categoricals_col:\n    total_unique = sales[c].nunique()\n    categoricals_info[c] = (total_unique, min(50, (total_unique + 1) // 2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def make_data(df, idx, numericals_cols, categoricals_cols):\n    x = {}\n    x[\"num\"] = df[idx][numericals_cols].values\n    for cat in categoricals_cols:\n        x[cat] = df[idx][cat].values\n    t = df[idx][\"sales\"].values\n    m = df[idx][[\"id\", \"date\", \"scale\"]]\n    return x, t, m","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Train/Valid/Test\nDATE = \"date\"\nSTART = \"start\"\nMAX_LAG = max(LAGS)\nMAX_ROLL = max(ROLLS) + 28\nFINAL_MAX = np.maximum(MAX_LAG, MAX_ROLL)\nprint(\"FINAL_MAX\", FINAL_MAX)\n# Add max lag to drop NaN due to rolling windows\nsales[START] = sales[START] + pd.DateOffset(days=FINAL_MAX)\nsales.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def qloss(y_true, y_pred):\n    # Pinball loss for multiple quantiles\n    qs = [0.005, 0.025, 0.165, 0.250, 0.500, 0.750, 0.835, 0.975, 0.995]\n    q = tf.constant(np.array([qs]), dtype=tf.float32)\n    e = y_true - y_pred\n    v = tf.maximum(q*e, (q-1)*e)\n    return K.mean(v)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def make_model(n_in, categoricals_info):    \n    num = L.Input((n_in,), name=\"num\")\n    inp = {\"num\": num}\n    p = []\n    for key, value in categoricals_info.items():\n        tmp_inp = L.Input((1,), name=key)\n        inp[key] = tmp_inp\n        p.append(L.Embedding(value[0], value[1], name=\"%s_3d\" % key)(tmp_inp))\n        \n    emb = L.Concatenate(name=\"embds\")(p)\n    context = L.Flatten(name=\"context\")(emb)\n    \n    x = L.Concatenate(name=\"x1\")([context, num])\n    x = L.Dense(64, activation=\"relu\", name=\"d1\")(x)\n    x = L.Dense(64, activation=\"relu\", name=\"d2\")(x)\n    x = L.Dense(32, activation=\"relu\", name=\"d3\")(x)\n    \n    preds = L.Dense(9, activation=\"linear\", name=\"preds\")(x)\n    \n    model = M.Model(inp, preds, name=\"M1\")\n    model.compile(loss=qloss, optimizer=\"adam\")\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(numericals)\nprint(categoricals_info)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Time Series split validation","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"FIRST_DATE = '2014-03-28'\nFOLD_LIST = [\n    (1,  {\"train_start\": FIRST_DATE, \"train_stop\": \"2016-02-01\", \"valid_start\": \"2016-02-01\", \"valid_stop\": \"2016-02-29\"}),\n    (2,  {\"train_start\": FIRST_DATE, \"train_stop\": \"2016-02-29\", \"valid_start\": \"2016-02-29\", \"valid_stop\": \"2016-03-28\"}), \n    (3,  {\"train_start\": FIRST_DATE, \"train_stop\": \"2016-03-28\", \"valid_start\": \"2016-03-28\", \"valid_stop\": \"2016-04-25\"}),    \n]\n\nTEST_START = '2016-04-25'\nTEST_STOP = '2016-05-23'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"EPOCH = 20\nBATCH_SIZE = 50_000\n\nfor i, FOLD in enumerate(FOLD_LIST):\n    \n    xt, yt, mt = make_data(sales, ((sales[DATE] >= FOLD[1][\"train_start\"]) & (sales[DATE] < FOLD[1][\"valid_start\"]) & (sales[DATE] >= sales[START]) ), numericals, categoricals_col)\n    xv, yv, mv = make_data(sales, ((sales[DATE] >= FOLD[1][\"valid_start\"]) & (sales[DATE] < FOLD[1][\"valid_stop\"]) & (sales[DATE] >= sales[START])), numericals, categoricals_col)\n    \n    net = make_model(len(numericals), categoricals_info)\n    ckpt = ModelCheckpoint(\"w_%d.h5\" % FOLD[0], monitor='val_loss', verbose=1, save_best_only=True,mode='min')\n    #reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.001)\n    #es = EarlyStopping(monitor='val_loss', patience=3)\n    if i == 0:\n        print(net.summary())\n        tf.keras.utils.plot_model(net, to_file='model.png', show_shapes=True, show_layer_names=True)\n        \n    print(\"Train:\", mt[\"date\"].min(), mt[\"date\"].max(), xt[\"num\"].shape, xt[\"num\"].dtype, xt[\"dt_weekday\"].shape, xt[\"dt_weekday\"].dtype, yt.shape, yt.dtype, \"NaN\", np.count_nonzero(np.isnan(xt[\"num\"])))\n    print(\"Valid:\", mv[\"date\"].min(), mv[\"date\"].max(), xv[\"num\"].shape, xv[\"num\"].dtype, xv[\"dt_weekday\"].shape, xv[\"dt_weekday\"].dtype, yv.shape, yv.dtype, \"NaN\", np.count_nonzero(np.isnan(xv[\"num\"])))\n    \n    net.fit(xt, yt, batch_size=BATCH_SIZE, epochs=EPOCH, validation_data=(xv, yv), callbacks=[ckpt]) # [ckpt, reduce_lr, es]\n    del net, xt, xv, yt, yv, mt, mv\n    gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Reload each model and Test\nxe, ye, me = make_data(sales, ((sales[DATE] >= TEST_START) & (sales[DATE] < TEST_STOP) & (sales[DATE] >= sales[START])), numericals, categoricals_col)\npe = []\nfor i, FOLD in enumerate(FOLD_LIST):    \n    nett = make_model(len(numericals), categoricals_info)\n    nett.load_weights(\"w_%d.h5\" % FOLD[0])\n    fold_pe = nett.predict(xe, batch_size=BATCH_SIZE, verbose=1)\n    pe.append(fold_pe)\npe = np.array(pe)    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Average of per-fold prediction\npe = np.mean(pe, axis=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pe = pe.reshape((-1, 28, 9))\nse = me[\"scale\"].values.reshape((-1, 28))\nif SCALED == False:\n    se = np.ones_like(se)\nids = me[\"id\"].values.reshape((-1, 28))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"names = [f\"F{i+1}\" for i in range(28)]\npiv = pd.DataFrame(ids[:, 0], columns=[\"id\"])\nQUANTILES = [\"0.005\", \"0.025\", \"0.165\", \"0.250\", \"0.500\", \"0.750\", \"0.835\", \"0.975\", \"0.995\"]\nVALID = []\nEVAL = []\n\nfor i, quantile in tqdm(enumerate(QUANTILES)):\n    t1 = pd.DataFrame(pe[:,:, i]*se, columns=names)\n    t1 = piv.join(t1)\n    t1[\"id\"] = t1[\"id\"] + f\"_{quantile}_validation\"\n    t2 = pd.DataFrame(pe[:,:, i]*se, columns=names)\n    t2 = piv.join(t2)\n    t2[\"id\"] = t2[\"id\"] + f\"_{quantile}_evaluation\"\n    VALID.append(t1)\n    EVAL.append(t2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub = pd.DataFrame()\nsub = sub.append(VALID + EVAL)\ndel VALID, EVAL, t1, t2, sales","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub.to_csv(\"submission.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_rollup(train_df):\n    \"\"\"Gets a sparse roll up matrix for aggregation and \n    an index to align weights and scales.\"\"\"\n    \n    # Take the transpose of each dummy matrix to correctly orient the matrix\n    dummy_frames = [\n        pd.DataFrame({'Total': np.ones((30490,)).astype('int8')}, index=train_df.index).T, \n        pd.get_dummies(train_df.state_id, dtype=np.int8).T,                                             \n        pd.get_dummies(train_df.store_id, dtype=np.int8).T,\n        pd.get_dummies(train_df.cat_id, dtype=np.int8).T,\n        pd.get_dummies(train_df.dept_id, dtype=np.int8).T,\n        pd.get_dummies(train_df.state_id + '_' + train_df.cat_id, dtype=np.int8).T,\n        pd.get_dummies(train_df.state_id + '_' + train_df.dept_id, dtype=np.int8).T,\n        pd.get_dummies(train_df.store_id + '_' + train_df.cat_id, dtype=np.int8).T,\n        pd.get_dummies(train_df.store_id + '_' + train_df.dept_id, dtype=np.int8).T,\n        pd.get_dummies(train_df.item_id, dtype=np.int8).T,\n        pd.get_dummies(train_df.item_id + '_' + train_df.state_id, dtype=np.int8).T,\n        pd.get_dummies(train_df.item_id + '_' + train_df.store_id, dtype=np.int8).T\n    ]\n\n    rollup_matrix = pd.concat(dummy_frames, keys=range(1,13), names=['Level', 'id'])\n\n    # Save the index for later use \n    rollup_index = rollup_matrix.index\n\n    # Sparse format will save space and calculation time\n    rollup_matrix_csr = csr_matrix(rollup_matrix)\n    \n    return rollup_matrix_csr, rollup_index\n\ndef get_w_df(train_df, cal_df, prices_df, rollup_index, rollup_matrix_csr, start_test=1914): \n    \"\"\"Returns the weight, scale, and scaled weight of all series, \n    in a dataframe aligned with the rollup_index, created in get_rollup()\"\"\"\n    \n    d_cols = [f'd_{i}' for i in range(start_test - 28, start_test)]\n    df = train_df[['store_id', 'item_id'] + d_cols]\n    df = df.melt(id_vars=['store_id', 'item_id'],\n                           var_name='d', \n                           value_name = 'sales')\n    df = df.merge(cal_df[['d', 'wm_yr_wk']], on='d', how='left')\n    df = df.merge(prices_df, on=['store_id', 'item_id', 'wm_yr_wk'], how='left')\n    df['dollar_sales'] = df.sales * df.sell_price\n\n    # Now we will get the total dollar sales \n    dollar_sales = df.groupby(['store_id', 'item_id'], sort=False)['dollar_sales'].sum()\n    del df\n\n    # Build a weight, scales, and scaled weight columns \n    # that are aligned with rollup_index. \n    w_df = pd.DataFrame(index = rollup_index)\n    w_df['dollar_sales'] = rollup_matrix_csr * dollar_sales\n    w_df['weight'] = w_df.dollar_sales / w_df.dollar_sales[0]\n    del w_df['dollar_sales']\n\n    ##################### Scaling factor #######################\n    \n    df = train_df.loc[:, 'd_1':f'd_{start_test-1}']\n    agg_series = rollup_matrix_csr * df.values\n    no_sale = np.cumsum(agg_series, axis=1) == 0\n    agg_series = np.where(no_sale, np.nan, agg_series)\n    scale = np.nanmean(np.diff(agg_series, axis=1) ** 2, axis=1)\n\n    w_df['scale'] = scale\n    w_df['scaled_weight'] = w_df.weight / np.sqrt(w_df.scale)\n    \n    ################# spl_scale ####################\n    # Now we can compute the scale and add \n    # it as a column to our w_df\n    scale = np.nanmean(np.abs(np.diff(agg_series)), axis = 1)\n    scale.shape\n    w_df['spl_scale'] = scale\n\n    # It may also come in handy to have a scaled_weight \n    # on hand.  \n    w_df['spl_scaled_weight'] = w_df.weight / w_df.spl_scale\n     \n    ############## sub_id for uncertainty submission ##############\n    w_df = add_sub_id(w_df)\n    \n    return w_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class WSPLEvaluator(): \n    \"\"\" Will generate w_df and ability to score prediction for any start_test period \"\"\"\n    def __init__(self, train_df, cal_df, prices_df, start_test=1914):\n        self.rollup_matrix_csr, self.rollup_index = get_rollup(train_df)\n                        \n        self.w_df = get_w_df(\n                        train_df,\n                        cal_df,\n                        prices_df,\n                        self.rollup_index,\n                        self.rollup_matrix_csr,\n                        start_test=start_test,\n                    )\n        \n        self.quantiles = [0.005, 0.025, 0.165, 0.25, 0.5, 0.75, 0.835, 0.975, 0.995]\n        level_12_actuals = train_df.loc[:, f'd_{start_test}': f'd_{start_test + 27}']\n        self.actuals = self.rollup_matrix_csr * level_12_actuals.values\n        self.actuals_tiled = np.tile(self.actuals.T, 9).T\n        \n        \n    def score_all(self, preds): \n        scores_df, total = wspl(self.actuals_tiled, preds, self.w_df)\n        self.scores_df = scores_df\n        self.total_score = total\n        print(f\"Total score is {total}\")\n\n\n############## spl scaling factor function ###############\ndef add_spl_scale(w_df, train_df, rollup_matrix_csr): \n    # We calculate scales for days preceding \n    # the start of the testing/scoring period. \n    start_test = 1914\n    df = train_df.loc[:, 'd_1':f'd_{start_test-1}']\n\n    # We will need to aggregate all series \n    agg_series = rollup_matrix_csr * df.values\n\n    # Make sure leading zeros are not included in calculations\n    agg_series = h.nan_leading_zeros(agg_series)\n\n    # Now we can compute the scale and add \n    # it as a column to our w_df\n    scale = np.nanmean(np.abs(np.diff(agg_series)), axis = 1)\n    scale.shape\n    w_df['spl_scale'] = scale\n\n    # It may also come in handy to have a scaled_weight \n    # on hand.  \n    w_df['spl_scaled_weight'] = w_df.weight / w_df.spl_scale\n    \n    return w_df\n\n########## Function for all level pinball loss for quantile u ############\ndef spl_u(actuals, preds, u, w_df):\n    \"\"\"Returns the scaled pinball loss for each series\"\"\"\n    pl = np.where(actuals >= preds, (actuals - preds) * u, (preds - actuals) * (1 - u)).mean(axis=1)\n\n    # Now calculate the scaled pinball loss.  \n    all_series_spl = pl / w_df.spl_scale\n    return all_series_spl\n\n########## wspl for all quantiles ############\ndef wspl(actuals, preds, w_df): \n    \"\"\"\n    :acutals:, 9 vertical copies of the ground truth for all series. \n    :preds: predictions for all series and all quantiles. Same \n    shape as actuals\"\"\"\n    quantiles = [0.005, 0.025, 0.165, 0.25, 0.5, 0.75, 0.835, 0.975, 0.995]\n    scores = []\n    \n    # In this case, preds has every series for every  \n    # quantile T, so it has 42840 * 9 rows. We first \n    # break it up into 9 parts to get the wspl_T for each.\n    # We also do the same for actuals. \n    preds_list = np.split(preds, 9)\n    actuals_list = np.split(actuals, 9)\n    \n    for i in range(9):\n        scores.append(spl_u(actuals_list[i], preds_list[i], quantiles[i], w_df))\n        \n    # Store all our results in a dataframe\n    scores_df = pd.DataFrame(dict(zip(quantiles, [w_df.weight * score for score in scores])))\n    \n    #  We divide score by 9 \n    # to get the average wspl of each quantile. \n    spl = sum(scores) / 9\n    wspl_by_series = (w_df.weight * spl)\n    total = wspl_by_series.sum() / 12\n    \n    return scores_df, total\n\n####################################################################################\n############################ formatting for submission #############################\n\n############## sub_id function ################\ndef add_sub_id(w_df):\n    \"\"\" adds a column 'sub_id' which will match the \n    labels in the sample_submission 'id' column. Next \n    step will be adding '_{quantile}_validation/evaluation'\n    onto the sub_id column. This will be done in another \n    function. \n    \n    :w_df: dataframe with the multi-index that is \n    genereated by get_rollup()\n    \n    Returns w_df with added 'sub_id' column\"\"\"\n    # Lets add a sub_id col to w_df that \n    # we will build to match the submission file. \n    w_df['sub_id'] = w_df.index.get_level_values(1)\n\n    ###### level 1-5, 10 change ########\n    w_df.loc[1:5, 'sub_id'] = w_df.sub_id + '_X'\n    w_df.loc[10, 'sub_id'] = w_df.sub_id + '_X'\n\n    ######## level 11 change ##########\n    splits = w_df.loc[11, 'sub_id'].str.split('_')\n    w_df.loc[11, 'sub_id'] = (splits.str[3] + '_' + \\\n                              splits.str[0] + '_' + \\\n                              splits.str[1] + '_' + \\\n                              splits.str[2]).values\n    \n    return w_df\n\n\n\n################## add quantile function ################\ndef add_quantile_to_sub_id(w_df, u): \n    \"\"\"Used to format 'sub_id' column in w_df. w_df must \n    already have a 'sub_id' column. This used to match \n    the 'id' column of the submission file.\"\"\"\n    # Make sure not to affect global variable if we \n    # don't want to. \n    w_df = w_df.copy()\n    w_df['sub_id'] = w_df.sub_id + f\"_{u:.3f}_validation\"\n    return w_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = pd.read_csv(\"/kaggle/input/m5-forecasting-uncertainty/sales_train_evaluation.csv\")\ncal_df = pd.read_csv(\"/kaggle/input/m5-forecasting-uncertainty/calendar.csv\")\nprices_df = pd.read_csv(\"/kaggle/input/m5-forecasting-uncertainty/sell_prices.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"e = WSPLEvaluator(train_df, \n                  cal_df,\n                  prices_df,\n                  start_test=1914)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub = pd.read_csv(\"submission.csv\").iloc[:42840 * 9]\nsub.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"QUANTS = [0.005,0.025,0.165,0.25, 0.5, 0.75, 0.835, 0.975, 0.995]\ncopies = [add_quantile_to_sub_id(e.w_df, QUANTS[i]) for i in range(9)]\nw_df_9 = pd.concat(copies, axis = 0)\nsorted_sub = sub.set_index('id').reindex(w_df_9.sub_id)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"e.score_all(sorted_sub.values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"e.total_score","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}