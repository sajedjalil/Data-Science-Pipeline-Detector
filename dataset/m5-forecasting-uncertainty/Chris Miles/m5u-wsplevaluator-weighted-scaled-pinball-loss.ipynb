{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Versions unpdates: \n\nVersion 14: changed the object so that scores_df is an attribute that shows scores for all series and all quantiles. I also showed a few examples of how to visualize these scores, but I didn't even scratch the surface into other things you can do with this dataframe of scores, namely compare to other models and find which performs best on which series/quantile combinations. \n\nversion 11: added an easy to use WSPLEvaluator object to my utility script. You can find this at the end of the notebook. ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# TLDR: \nWe successfuly make a function to calculate the WSPL for our predictions that we can use for cross validation. We create helper functions that build on the [M5-helpers](https://www.kaggle.com/chrisrichardmiles/m5-helpers/edit/run/35419204) utility script, which is publicly available (you will find helpful functions for both competitions). You can also go strait to the end of this notebook to get all the functions, but they will only work in conjunction with other functions from M5-helpers. \n\n## Uncertainty has no ground truth\nUsually, the ground truth of a prediction can be measured. If the ground truth can't be measured, it feels kind of weird to try to predict it. But this is the exact situation that we have with uncertainty. We are trying to make predictions about a variable, sales at walmart on a certain day, which itself does have a ground truth value. The ground truth of  total sales of a product or aggregation of products is precicely one number, the total sales. But we are not tasked with delivering a prediction of total sales. Rather, we are tasked with delivering information about the distribution from which total sales is drawn. To be more precise, we are trying to trying to predict 9 specific quantiles for total sales. \n\n\n## Confidence interval representation\nWe will deliver our confidence intervals by delivering quantile forecasts for 9 different quantiles, namely .005, .025, .165, .25, .5, .75, .835, .975, .995. By doing this, it can be inferred exactly what our confidence intervals are. For instance, my central 50% confidence interval, goes from my quantile prediction for .25 up to my quantile prediction for .75. I had to specify that the interval is centered around the median, because there is also a 50% confidence interval that goes from zero to my qunatile prediction for .5. In fact, many different confidence intervals can be inferred from our predictions. We just usually think of a confidence interval being centered around a mean.\n\nSo how can  We are trying to predict a possible range of values that will\n\n\n\n# How to measure our performance \nSince we don't have any ground truth to measure our predictions against, we need a loss metric that will optimize for the quantile we are predicting. This is accomplished by the scaled pinball loss function (hereby referred to as SPL). \n\n# Scaled Pinball Loss\n\n\\begin{eqnarray}\nSPL_{\\tau}(y,z) & = & (y - z) \\tau & \\textrm{ if } y \\geq z \\\\\\\n & = & (z - y) (1 - \\tau) & \\textrm{ if } z > y\n\\end{eqnarray}\n\nFor:\n* quantile ${\\tau}$\n* true value ${y}$\n* quantile prediction ${z}$\n\n# Overall metric: weighted scaled pinball loss\n\n\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import m5_helpers as h\nimport pandas as pd \nimport numpy as np","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"help(h)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"MAIN_PATH = '/kaggle/input/m5-forecasting-uncertainty/'\n\ntrain_df = pd.read_csv(F'{MAIN_PATH}sales_train_evaluation.csv')\nprices_df = pd.read_csv(F'{MAIN_PATH}sell_prices.csv')\ncal_df = pd.read_csv(F'{MAIN_PATH}calendar.csv')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"################ pinball loss example ###################\nactuals = train_df.head().iloc[0, -28:]\n\n# Add some random noise to simulate predictions \npreds = np.clip(actuals + np.random.normal(scale=.2, size=28), 0, None)\n\n# Lets say we are predicting for the .25 quantile \nu = .25\n\n# So then the SPL for this series and quantile would be\npl = np.where(actuals >= preds, (actuals - preds) * u, (preds - actuals) * (1 - u)).mean()\npl","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"################## simple pl example function #####################\n# This is just for example. We will not use this for our score \n# calculations. \ndef pl(actuals, preds, u): \n    return np.where(series >=actuals, (actuals - preds) * u, (preds - actuals) * (1 - u)).mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"############# weighted scaled pinball loss ################\n# To calculate the wspl we will need both the weights and \n# the scaling factor for each series. To that end, we will \n# have to aggregate the data in train_df into all the series.\n# We will need a rollup matrix and index to aggregate all \n# series. \nrollup_matrix_csr, rollup_index = h.get_rollup(train_df)\n\n# A weight_dataframe with an index for all the series with the \n# weights and scales would be nice to have. The weights \n# are the same for accuracy and uncertainty, and I have \n# made a function, get_w_df that gets those weights, but\n# as of now, get_w_df only gives scaling factors for the wrmsse. \n# We will need different scaling factors the wspl.\nw_df = h.get_w_df(\n    train_df,\n    cal_df,\n    prices_df,\n    rollup_index,\n    rollup_matrix_csr,\n    start_test=1914,\n)\nw_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"###################### spl scaling factor #######################\n# We calculate scales for days preceding \n# the start of the testing/scoring period. \nstart_test = 1914\ndf = train_df.loc[:, 'd_1':f'd_{start_test-1}']\n\n# We will need to aggregate all series \nagg_series = rollup_matrix_csr * df.values\n\n# Make sure leading zeros are not included in calculations\nagg_series = h.nan_leading_zeros(agg_series)\n\n# Now we can compute the scale and add \n# it as a column to our w_df\nscale = np.nanmean(np.abs(np.diff(agg_series)), axis = 1)\nscale.shape\nw_df['spl_scale'] = scale\n\n# It may also come in handy to have a scaled_weight \n# on hand. \nw_df['spl_scaled_weight'] = w_df.weight / w_df.spl_scale\ndisplay(w_df.head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"############## spl scaling factor function ###############\ndef add_spl_scale(w_df, train_df, rollup_matrix_csr): \n    # We calculate scales for days preceding \n    # the start of the testing/scoring period. \n    start_test = 1914\n    df = train_df.loc[:, 'd_1':f'd_{start_test-1}']\n\n    # We will need to aggregate all series \n    agg_series = rollup_matrix_csr * df.values\n\n    # Make sure leading zeros are not included in calculations\n    agg_series = h.nan_leading_zeros(agg_series)\n\n    # Now we can compute the scale and add \n    # it as a column to our w_df\n    scale = np.nanmean(np.abs(np.diff(agg_series)), axis = 1)\n    scale.shape\n    w_df['spl_scale'] = scale\n\n    # It may also come in handy to have a scaled_weight \n    # on hand.  \n    w_df['spl_scaled_weight'] = w_df.weight / w_df.spl_scale\n    \n    return w_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"############## spl for all series with quantile u = .25 ##################\nstart_test = 1914\nu = .25\nlevel_12_acutals = train_df.loc[:, f'd_{start_test}': f'd_{start_test + 27}']\n\n# But we need actuals for all series\nactuals = rollup_matrix_csr * level_12_acutals.values\n\n# We will just use zero predictions for example\npreds = np.zeros(actuals.shape)\n\n# Lets calculate the pinball loss\npl = np.where(actuals >= preds, (actuals - preds) * u, (preds - actuals) * (1 - u)).mean(axis=1)\n\n# Now calculate the scaled pinball loss.  \nall_series_spl = pl / w_df.spl_scale\nall_series_spl","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"########## Function for all level pinball loss for quantile u ############\ndef spl_u(actuals, preds, u):\n    \"\"\"Returns the scaled pinball loss for each series\"\"\"\n    pl = np.where(actuals >= preds, (actuals - preds) * u, (preds - actuals) * (1 - u)).mean(axis=1)\n\n    # Now calculate the scaled pinball loss.  \n    all_series_spl = pl / w_df.spl_scale\n    return all_series_spl","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"########## wspl for all quantiles ############\ndef wspl(actuals, preds): \n    \"\"\"\n    :acutals:, 9 vertical copies of the ground truth for all series. \n    :preds: predictions for all series and all quantiles. Same \n    shape as actuals\"\"\"\n    quantiles = [0.005, 0.025, 0.165, 0.25, 0.5, 0.75, 0.835, 0.975, 0.995]\n    scores = []\n    \n    # In this case, preds has every series for every  \n    # quantile T, so it has 42840 * 9 rows. We first \n    # break it up into 9 parts to get the wspl_T for each.\n    # We also do the same for actuals. \n    preds_list = np.split(preds, 9)\n    actuals_list = np.split(actuals, 9)\n    \n    for i in range(9):\n        scores.append(spl_u(actuals_list[i], preds_list[i], quantiles[i]))\n        \n    #  We divide score by 9 \n    # to get the average wspl of each quantile. \n    spl = sum(scores) / 9\n    wspl = (w_df.weight * spl).sum() / 12\n    return wspl","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"################ Test on zero predictions #################\nstart_test = 1914\nlevel_12_acutals = train_df.loc[:, f'd_{start_test}': f'd_{start_test + 27}']\nactuals = rollup_matrix_csr * level_12_acutals.values\nactuals = np.tile(actuals.T, 9).T\npreds = np.zeros((42840 * 9, 28))\nwspl(actuals, preds)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Load in a submission we know the score of. \n# This one scored .17921\n# We only want the _validaiton portion to test our wspl\nsub = pd.read_csv('/kaggle/input/from-point-to-uncertainty-prediction/submission.csv').iloc[:42840 * 9, 1:]\nprint(wspl(actuals, sub.values), 'is supposed to be close to .17921')\nprint('This is not good. The problem is our depends on the predictions to be in a specific order.')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Creating index formatting functions to match submission id column.\nTo make a submission, we need our columns to have the exact same labels as the submission column. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Lets get our index the way we need it \n# to work with our rollup matrix. This is the \n# oder our predictions will naturally be in. \n# If we want to test other sample submissions \n# with our wspl function, they will need to \n# be reindexed to match our index. Therefore, we \n# will need to build a column that matched the \n# formatting of the sample submission exactly.\nrollup_matrix_csr, rollup_index = h.get_rollup(train_df)\nw_df = h.get_w_df(\n    train_df,\n    cal_df,\n    prices_df,\n    rollup_index,\n    rollup_matrix_csr,\n    start_test=1914,\n)\n\n# I think the submission index was created \n# by combining the Agg_Level_1 and Agg_Level_2\n# columns. \nw = pd.read_csv('/kaggle/input/m5methods/validation/weights_validation.csv')\n    \n# We will also need the sample submission file. \nss = pd.read_csv('/kaggle/input/m5-forecasting-uncertainty/sample_submission.csv')\n\n# Lets just take the first 42840 rows of \n# submissino file for now. \nss = ss.iloc[:42840]\n\n# Give it w_df index so we can easily \n# look at different levels without \n# having to count rows. \nss.index = w_df.index\n\n# We will need to see both indexes \n# at different levels, along with the \n# sample submission. \ndef p(i, head_length=2): \n    display(w[w.Level_id == f'Level{i}'].head(head_length))\n    display(w_df.loc[i].head(head_length))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"p(1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Lets add a sub_id col to w_df that \n# we will build to match the submission \n# file. \nw_df['sub_id'] = w_df.index.get_level_values(1)\n\n###### level 1-5, 10 change ########\nw_df.loc[1:5, 'sub_id'] = w_df.sub_id + '_X'\nw_df.loc[10, 'sub_id'] = w_df.sub_id + '_X'\n\n######## level 11 change ##########\nsplits = w_df.loc[11, 'sub_id'].str.split('_')\nw_df.loc[11, 'sub_id'] = (splits.str[3] + '_' + \\\n                          splits.str[0] + '_' + \\\n                          splits.str[1] + '_' + \\\n                          splits.str[2]).values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Lets observe our work\nfor i in list(range(1, 6)) + [10]: \n    p(i)\nprint('Looks good')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Add '_0.005_validation' to sub_id column\n# We will need to do this for every quantile. \n# For now, we are just doing it for 0.005\nw_df['sub_id'] = w_df.sub_id + '_0.005_validation'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"w_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Verify we have all the right labels \n# in our index. This should be the null set.\nset(ss.id).difference(set(w_df.sub_id))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Are they in the right order?\n(w_df.index == ss.id).all()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Lets reindex w_df and see if they match. \nw_df = w_df.set_index('sub_id')\nw_df = w_df.reindex(ss.id)\n(w_df.index == ss.id).all()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"############## sub_id function ################\ndef add_sub_id(w_df):\n    \"\"\" adds a column 'sub_id' which will match the \n    labels in the sample_submission 'id' column. Next \n    step will be adding '_{quantile}_validation/evaluation'\n    onto the sub_id column. This will be done in another \n    function. \n    \n    :w_df: dataframe with the multi-index that is \n    genereated by get_rollup()\n    \n    Returns w_df with added 'sub_id' column\"\"\"\n    # Lets add a sub_id col to w_df that \n    # we will build to match the submission file. \n    w_df['sub_id'] = w_df.index.get_level_values(1)\n\n    ###### level 1-5, 10 change ########\n    w_df.loc[1:5, 'sub_id'] = w_df.sub_id + '_X'\n    w_df.loc[10, 'sub_id'] = w_df.sub_id + '_X'\n\n    ######## level 11 change ##########\n    splits = w_df.loc[11, 'sub_id'].str.split('_')\n    w_df.loc[11, 'sub_id'] = (splits.str[3] + '_' + \\\n                              splits.str[0] + '_' + \\\n                              splits.str[1] + '_' + \\\n                              splits.str[2]).values\n    \n    return w_df\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"############## Adding '_{quantile}_validation' ##############\n# Start with a fresh w_df. \nw_df = h.get_w_df(\n    train_df,\n    cal_df,\n    prices_df,\n    rollup_index,\n    rollup_matrix_csr,\n    start_test=1914,\n)\n\n# Use tao = 0.005\nT = 0.005\n\n# Add 'sub_id' column \nw_df = add_sub_id(w_df)\n\n# Add tao onto sub_id\nw_df['sub_id'] = w_df.sub_id + f\"_{T}_validation\"\nw_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"################## add quantile function ################\ndef add_quantile_to_sub_id(w_df, u): \n    \"\"\"Used to format 'sub_id' column in w_df. w_df must \n    already have a 'sub_id' column. This used to match \n    the 'id' column of the submission file.\"\"\"\n    # Make sure not to affect global variable if we \n    # don't want to. \n    w_df = w_df.copy()\n    w_df['sub_id'] = w_df.sub_id + f\"_{u:.3f}_validation\"\n    return w_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Using our id formatting functions to test our wspl function","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Load in a submission we know the score of. \n# This one scored .17921\n# We only want the _validaiton portion to test our wspl\nsub = pd.read_csv('/kaggle/input/from-point-to-uncertainty-prediction/submission.csv').iloc[:42840 * 9]\nsub.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# The id column is not in the right order for \n# our wspl function. Lets fix that. \n# First we start with a fresh w_df. Then we add \n# on the 'sub_id' column. Then we extend the dataframe \n# to be 9 times as long with the appropriate \n# \"_{quantile}_validation\" lable added to 'sub_id'\nw_df = h.get_w_df(\n    train_df,\n    cal_df,\n    prices_df,\n    rollup_index,\n    rollup_matrix_csr,\n    start_test=1914,\n)\n\n# We need the spl scale\nw_df = add_spl_scale(w_df, train_df, rollup_matrix_csr)\nw_df = add_sub_id(w_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"quantiles = [0.005, 0.025, 0.165, 0.250, 0.500, 0.750, 0.835, 0.975, 0.995]\ncopies = [add_quantile_to_sub_id(w_df, quantiles[i]) for i in range(9)]\nw_df_9 = pd.concat(copies, axis = 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We need the submission file to be in the \n# the same order as our w_df index to use \n# our function. \nsorted_sub = sub.set_index('id').reindex(w_df_9.sub_id)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We need the true values\nstart_test = 1914\nlevel_12_acutals = train_df.loc[:, f'd_{start_test}': f'd_{start_test + 27}']\nactuals = rollup_matrix_csr * level_12_acutals.values\nactuals = np.tile(actuals.T, 9).T","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Successful test","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(wspl(actuals, sorted_sub.values))\nprint('BOOM!')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# New helper functions ready to put into helper file. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from m5_helpers import *","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"####################################################################################\n############################ WSPL and helpers ######################################\n\n############################ WSPLEvaluator Object ##################################\nclass WSPLEvaluator(): \n    \"\"\" Will generate w_df and ability to score prediction for any start_test period \"\"\"\n    def __init__(self, train_df, cal_df, prices_df, start_test=1914):\n        self.rollup_matrix_csr, self.rollup_index = get_rollup(train_df)\n                        \n        self.w_df = get_w_df(\n                        train_df,\n                        cal_df,\n                        prices_df,\n                        self.rollup_index,\n                        self.rollup_matrix_csr,\n                        start_test=start_test,\n                    )\n        \n        self.quantiles = [0.005, 0.025, 0.165, 0.25, 0.5, 0.75, 0.835, 0.975, 0.995]\n        level_12_actuals = train_df.loc[:, f'd_{start_test}': f'd_{start_test + 27}']\n        self.actuals = self.rollup_matrix_csr * level_12_actuals.values\n        self.actuals_tiled = np.tile(self.actuals.T, 9).T\n        \n        \n    def score_all(self, preds): \n        scores_df, total = wspl(self.actuals_tiled, preds, self.w_df)\n        self.scores_df = scores_df\n        self.total_score = total\n        print(f\"Total score is {total}\")\n\n\n############## spl scaling factor function ###############\ndef add_spl_scale(w_df, train_df, rollup_matrix_csr): \n    # We calculate scales for days preceding \n    # the start of the testing/scoring period. \n    start_test = 1914\n    df = train_df.loc[:, 'd_1':f'd_{start_test-1}']\n\n    # We will need to aggregate all series \n    agg_series = rollup_matrix_csr * df.values\n\n    # Make sure leading zeros are not included in calculations\n    agg_series = h.nan_leading_zeros(agg_series)\n\n    # Now we can compute the scale and add \n    # it as a column to our w_df\n    scale = np.nanmean(np.abs(np.diff(agg_series)), axis = 1)\n    scale.shape\n    w_df['spl_scale'] = scale\n\n    # It may also come in handy to have a scaled_weight \n    # on hand.  \n    w_df['spl_scaled_weight'] = w_df.weight / w_df.spl_scale\n    \n    return w_df\n\n########## Function for all level pinball loss for quantile u ############\ndef spl_u(actuals, preds, u, w_df):\n    \"\"\"Returns the scaled pinball loss for each series\"\"\"\n    pl = np.where(actuals >= preds, (actuals - preds) * u, (preds - actuals) * (1 - u)).mean(axis=1)\n\n    # Now calculate the scaled pinball loss.  \n    all_series_spl = pl / w_df.spl_scale\n    return all_series_spl\n\n########## wspl for all quantiles ############\ndef wspl(actuals, preds, w_df): \n    \"\"\"\n    :acutals:, 9 vertical copies of the ground truth for all series. \n    :preds: predictions for all series and all quantiles. Same \n    shape as actuals\"\"\"\n    quantiles = [0.005, 0.025, 0.165, 0.25, 0.5, 0.75, 0.835, 0.975, 0.995]\n    scores = []\n    \n    # In this case, preds has every series for every  \n    # quantile T, so it has 42840 * 9 rows. We first \n    # break it up into 9 parts to get the wspl_T for each.\n    # We also do the same for actuals. \n    preds_list = np.split(preds, 9)\n    actuals_list = np.split(actuals, 9)\n    \n    for i in range(9):\n        scores.append(spl_u(actuals_list[i], preds_list[i], quantiles[i], w_df))\n        \n    # Store all our results in a dataframe\n    scores_df = pd.DataFrame(dict(zip(quantiles, [w_df.weight * score for score in scores])))\n    \n    #  We divide score by 9 \n    # to get the average wspl of each quantile. \n    spl = sum(scores) / 9\n    wspl_by_series = (w_df.weight * spl)\n    total = wspl_by_series.sum() / 12\n    \n    return scores_df, total\n\n####################################################################################\n############################ formatting for submission #############################\n\n############## sub_id function ################\ndef add_sub_id(w_df):\n    \"\"\" adds a column 'sub_id' which will match the \n    labels in the sample_submission 'id' column. Next \n    step will be adding '_{quantile}_validation/evaluation'\n    onto the sub_id column. This will be done in another \n    function. \n    \n    :w_df: dataframe with the multi-index that is \n    genereated by get_rollup()\n    \n    Returns w_df with added 'sub_id' column\"\"\"\n    # Lets add a sub_id col to w_df that \n    # we will build to match the submission file. \n    w_df['sub_id'] = w_df.index.get_level_values(1)\n\n    ###### level 1-5, 10 change ########\n    w_df.loc[1:5, 'sub_id'] = w_df.sub_id + '_X'\n    w_df.loc[10, 'sub_id'] = w_df.sub_id + '_X'\n\n    ######## level 11 change ##########\n    splits = w_df.loc[11, 'sub_id'].str.split('_')\n    w_df.loc[11, 'sub_id'] = (splits.str[3] + '_' + \\\n                              splits.str[0] + '_' + \\\n                              splits.str[1] + '_' + \\\n                              splits.str[2]).values\n    \n    return w_df\n\n\n\n################## add quantile function ################\ndef add_quantile_to_sub_id(w_df, u): \n    \"\"\"Used to format 'sub_id' column in w_df. w_df must \n    already have a 'sub_id' column. This used to match \n    the 'id' column of the submission file.\"\"\"\n    # Make sure not to affect global variable if we \n    # don't want to. \n    w_df = w_df.copy()\n    w_df['sub_id'] = w_df.sub_id + f\"_{u:.3f}_validation\"\n    return w_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# WSPLEvaluator object in action\nYou must add [this utility script](https://www.kaggle.com/chrisrichardmiles/m5-helpers/edit/run/35537552) to your notebook. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Must add m5_helpers utility script\nfrom m5_helpers import WSPLEvaluator","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\ne = WSPLEvaluator(train_df, cal_df, prices_df, start_test=1914)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"e.score_all(sorted_sub.values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Now we can get the total score \ne.total_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We can also see scores for all series \n# and all quantiles\n# with a multi-index for organization. \nscores_df = e.scores_df\nscores_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We can look at each level and quantile \n# combination. \nscores_df.groupby(level=0).sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We can also see scores by levels\nscores_df.groupby(level=0).sum().mean(axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Or scores by quantile\nscores_df.groupby(level=0).sum().mean(axis=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scores_df.loc[3]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nscores_df.loc[3].plot(title='scores by store for each quantile')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scores_df.loc[3].T.plot(title='scores for quantile for each store', figsize=(14,8))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scores_df.loc[1].T.plot()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}