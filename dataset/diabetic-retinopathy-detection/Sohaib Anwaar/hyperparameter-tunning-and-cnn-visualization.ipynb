{"cells":[{"metadata":{"_uuid":"75ba011a5cda52db2d167e75b84056b644ec8d51"},"cell_type":"markdown","source":"##### **Installing Libraries**"},{"metadata":{"trusted":true,"_uuid":"0088907b62b25efcb3729c03beba9dda7f5ff6a4"},"cell_type":"code","source":"!pip install talos # hyperparameter tuning\n!pip install --upgrade pip","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d476e1d5219d18bec51481f83fd0b98c99bc6cb2"},"cell_type":"markdown","source":"First of All I am going to Import **libraries** that will be usefull for this project"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt # showing and rendering figures\n# io related\nimport seaborn as sns # for Visualizing my data\nfrom keras.utils import to_categorical # Using keras to_categorical because I need to convert by labels \n# into categorical form\nimport os # for taking input to my dataframe\nimport cv2 # for resizing my iamges\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n# not needed in Kaggle, but required in Jupyter\n%matplotlib inline \n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c669aa79a36ce3e03064c1b67b6ab0ae97f88307"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c52014240e6f425e79109cfb4575941eba5704a2","scrolled":true},"cell_type":"code","source":"# Here I am building my dataframe (taking patient id  from image coloum and seperating side of image \n# left or right specefying path of image and at last converting my labels into categorical labels)\ntemp_df=pd.read_csv('../input/diabetic-retinopathy-detection/trainLabels.csv') # uploading csv to my pandas dataframe\nprint(temp_df.head()) # displaying first 5 objects in dataframe\nimage=temp_df['image'].str.split('_',n=1,expand=True) #splitting Side and Patient ID \ndf = pd.DataFrame()# creating new dataframe object\ndf['eye_side']=image[1] #taking side of Image\ndf['patient_id']=image[0]#taking patient id of an Image\n\ndf['path']='../input/diabetic-retinopathy-detection/'#Giving paths of the images \ndf['path']=df['path'].str.cat(temp_df['image']+'.jpeg')#adding Image path and format \ndf['exists'] = df['path'].map(os.path.exists)\ndf=df[df['exists']]\ndf['level']=temp_df['level']# taking levels of Image\ndf['level_cat'] = df['level'].map(lambda x: to_categorical(x, 1+df['level'].max()))#converting my \n# labels to categorical_labels\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3a19b4836926d19a2dcf33a87901c66aadbb354f"},"cell_type":"markdown","source":"**Visualization**\n"},{"metadata":{"trusted":true,"_uuid":"4c8e1274a77cb213cc1be4e7b0b24e29476edb51"},"cell_type":"code","source":"im = plt.imread(df.path.values[2]) # reading Image from its path\n\nplt.imshow(im)# show the image\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c02989e6c1b1cd24484588928dbd580146a5fba1"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"624db2a543159b9a9ccfe2a02036fc755927bb8d","scrolled":true},"cell_type":"code","source":"sizes = df['level'].values #taking values from series because I only want to visualize levels nt index\nprint(sizes[0:5])#printing first 5 values\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5c1d2931363854e96d6e0623a7e750f8f640d31d"},"cell_type":"code","source":"sns.distplot(sizes, kde=False); # Visualizing levels in dataset","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"50cee6f84bec0ab61acc2d715a9c94911b92fb12"},"cell_type":"code","source":"pd.value_counts(sizes) # viewing the values of levels","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4996d2adaaa90dd3718f2847fd42e4842d3f97e0"},"cell_type":"code","source":"import PIL\nfrom PIL import Image\nbaseheight = 128\nimg = Image.open('../input/diabetic-retinopathy-detection/1192_right.jpeg')\n\nwsize = 128\nimg = img.resize((wsize, baseheight), PIL.Image.ANTIALIAS)\nimg.save('resized_image.jpg')# i need this for storing my previous image also ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b6dcb91e1199e15a8e18bcacca658b2e97bf4212"},"cell_type":"code","source":"im = plt.imread('resized_image.jpg') # reading Image from its path\n\nplt.imshow(im)# show the image\nplt.title(\"Resized Image\")\nplt.show()\n\nim = plt.imread('../input/diabetic-retinopathy-detection/1192_right.jpeg') # reading Image from its path\n\nplt.imshow(im)# show the image\nplt.title(\"Orignal Image\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"44b0d93a24fe74e7ab50ae44f80e8a2f056d3992"},"cell_type":"code","source":"#total Examples of LEVEL [1,2,3,4] So that we are able to make balanced dataset\nsum_E=0\nfor i in range (1,5):\n    L1_df=pd.DataFrame()# creating new dataframe object\n    L1_df =df [df.level==i]\n    x=len(L1_df)\n    sum_E=x+sum_E\nprint(sum_E)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cba3a39a844b5f27d29027dec691c8c11f917c89"},"cell_type":"markdown","source":"So we will take **261** **examples from eye having** **level 0 and 261 examples from  level[1,2,3,4]**"},{"metadata":{"_uuid":"11514d19f2b3599735161d6b30ed18bf8eff341a"},"cell_type":"markdown","source":"**almost 70% of 0 DR and 30% of 1DR **"},{"metadata":{"_uuid":"77bdc41a532371c4bebfc63c9fe4fd795c7eac40"},"cell_type":"markdown","source":"**Making of Binary dataframe**"},{"metadata":{"trusted":true,"_uuid":"5c47c7853ae9e05f4159d2b2957c9ce165582b5c"},"cell_type":"markdown","source":"\nB_df=pd.DataFrame()\n#making secondary dataset having small number of sample \n#taking examples where level=0\n\nL0_df=df[df.level==0 ]\n\n\n\n\n\nL1_df=pd.DataFrame()# creating new dataframe object\nL1_df=df[df.level==1 ]\nL1_df=L1_df.drop('level_cat',axis=1)\n\n\n\nL2_df=pd.DataFrame()# creating new dataframe object\nL2_df=df[df.level==2 ]\n\n\n\n\nL3_df=pd.DataFrame()# creating new dataframe object\nL3_df=df[df.level==3] \nL3_df=L3_df.drop('level_cat',axis=1)\n\n\n\nL4_df=pd.DataFrame()# creating new dataframe object\nL4_df=df[df.level==4 ]\nL4_df=L4_df.drop('level_cat',axis=1)\n\n\n#Combining Examples of level [1,2,3,4]\nframes = [ L1_df, L2_df,L3_df,L4_df]\nL1_df = pd.concat(frames)\nL1_df=L1_df.replace(({'level' : { 2 : 1, 3 : 1, 4 : 1 }}))\n\n\nframes = [ L0_df,L1_df]\nB_df = pd.concat(frames,sort=True)\nB_df=B_df.drop('level_cat',axis=1)\nB_df['level_cat'] = B_df['level'].map(lambda x: to_categorical(x, 1+B_df['level'].max()))#converting my \n# labels to categorical_labels\n#Adding New Categorical Lables according to Binary Dataset\n\nprint(\"Total Numbe of examples in Binary DataFrame = \" + str(len(B_df)))  # Total Examples"},{"metadata":{"trusted":true,"_uuid":"ba2eae7059c2be1bf37c590df0221a2647ff9dfa"},"cell_type":"markdown","source":"sizes =B_df['level'].values\nsns.distplot(sizes, kde=False); # Visualizing levels in dataset"},{"metadata":{"trusted":true,"_uuid":"66c8b5da4ba34056586e6932581a63b610f033f2"},"cell_type":"markdown","source":"**Saving My Binary dataset and binary sizes (90,128,264) of images which are on same indexs **"},{"metadata":{"_uuid":"ca5ca3be501ce8b263cf526603cfa1e991adfe3c"},"cell_type":"markdown","source":"**Saving Binary Dataset**"},{"metadata":{"trusted":true,"_uuid":"95c7f688589815e10bc96ea3a6e648f60b3636a3"},"cell_type":"markdown","source":"len(B_df)"},{"metadata":{"trusted":true,"_uuid":"d9d38cbbf6905f7260966d2f25ed01ebb6546009"},"cell_type":"markdown","source":"B_df.to_csv(\"Binary Dataframe\")\n"},{"metadata":{"_uuid":"4b8d54b2382a2bc2b23313eab24bef46f047db68"},"cell_type":"markdown","source":"**Resizing image array to (90,90)**"},{"metadata":{"trusted":true,"_uuid":"e9a361fcfb2863d3b7f6fed4f6c824f8c9ad5fae"},"cell_type":"markdown","source":"#resizing my training examples to (90,90)\n#train examples\nfrom PIL import Image\nfrom skimage.transform import resize \n\nimage_list=[]\n\nfor i in range (len(B_df)):\n    image=plt.imread(B_df['path'].iloc[i])\n    image = resize(image, (90, 90))\n    image_list.append(image)\n    "},{"metadata":{"_uuid":"ee63a3855cee90c1a2fac4f497962c3647299911"},"cell_type":"markdown","source":"**Resizing image array to (128,128)**"},{"metadata":{"trusted":true,"_uuid":"33bdec73450c06d1567ed66a9ade1fc69e748cb5"},"cell_type":"markdown","source":"#resizing my training examples to (128,128)\n#train examples\nfrom PIL import Image\nfrom skimage.transform import resize \n\nimage_list_128=[]\nfor i in B_df['path']:\n    image=plt.imread(i)\n    image = resize(image, (128, 128))\n    image_list_128.append(image)"},{"metadata":{"_uuid":"d135dd0e9b79058f3afc56e748dac9eefebd33ae"},"cell_type":"markdown","source":"**Resizing image array to (264,264)**"},{"metadata":{"trusted":true,"_uuid":"aa6d541e843acef6ac322ad56b991c3e6f4a8a23"},"cell_type":"markdown","source":"#resizing my training examples to (264,264)\n#train examples\nfrom PIL import Image\nfrom skimage.transform import resize \n\nimage_list_264=[]\nfor i in B_df['path']:\n    image=plt.imread(i)\n    image = resize(image, (264, 264))\n    image_list_264.append(image)"},{"metadata":{"_uuid":"babb44a1b6465f8184fcf329792f8bb41860e252"},"cell_type":"markdown","source":"**Only resizing took my 3:00 hours in Kaggle** So I am storing my preprocess data"},{"metadata":{"_uuid":"ec120e018055d10fe791f59114714abe269c7f4b"},"cell_type":"markdown","source":"**Saving Numpy array to files**"},{"metadata":{"trusted":true,"_uuid":"c0abcc5ea1916e3a4084adadd7dcca349ad836b1"},"cell_type":"markdown","source":"img_list=np.asarray(image_list)\nimg_list=img_list.reshape(1000,90*90*3)\nprint(img_list.shape)\ntype(img_list)\nnp.savez_compressed('1000_Binary_images_data_90', a=img_list)\n\nimg_list_128=np.asarray(image_list_128)\nimg_list_128=img_list_128.reshape(1000,128*128*3)\nprint(img_list_128.shape)\ntype(img_list_128)\nnp.savez_compressed('1000_Binary_images_data_128', a=img_list_128)\n\nimg_list_264=np.asarray(image_list_264)\nimg_list_264=img_list_264.reshape(1000,264*264*3)\nprint(img_list_264.shape)\ntype(img_list_264)\nnp.savez_compressed('1000_Binary_images_data_264', a=img_list_264)"},{"metadata":{"_uuid":"960b877a59081a2445f343cc84473f152a58276c"},"cell_type":"markdown","source":"**loading my saving Binary data**"},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"ab834012666408e5568e484721784baad278b651"},"cell_type":"code","source":"B_df=pd.read_csv('../input/prepossessed-arrays-of-binary-data/1000_Binary Dataframe')\nB_df=B_df.drop('Unnamed: 0',axis=1)\nB_df.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c4780a38f50a2fde101dfb4dcdb3787c74e578c1"},"cell_type":"code","source":"sizes =B_df['level'].values\nsns.distplot(sizes, kde=False); # Visualizing levels in dataset","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a6da57f88204abf9fa7635edb1c43b3e0919449f"},"cell_type":"markdown","source":"**loading My numpy array which I saved**"},{"metadata":{"trusted":true,"_uuid":"3c831d9a6bd26e2daf869fe63f331140a05a1c04"},"cell_type":"code","source":"Binary_90 = np.load('../input/prepossessed-arrays-of-binary-data/1000_Binary_images_data_90.npz')\nX_90=Binary_90['a']\nBinary_128 = np.load('../input/prepossessed-arrays-of-binary-data/1000_Binary_images_data_128.npz')\nX_128=Binary_128['a']\nBinary_264 = np.load('../input/prepossessed-arrays-of-binary-data/1000_Binary_images_data_264.npz')\nX_264=Binary_264['a']\ny=B_df['level'].values\n\n\nprint(X_90.shape)\nprint(X_128.shape)\nprint(X_264.shape)\nprint(y.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7315b6c277900108ec972a851987109ac5b62840"},"cell_type":"code","source":"# we need to resize our X because we load array in 2 diminsional and we need it in 4 diminsional\nprint(\"Shape before reshaping X_90\" +str(X_90.shape))\nX_90=X_90.reshape(1000,90,90,3)\nprint(\"Shape after reshaping X_90\" +str(X_90.shape))\nprint(\"\\n\\n\")\n\nprint(\"Shape before reshaping X_128\" +str(X_128.shape))\nX_128=X_128.reshape(1000,128,128,3)\nprint(\"Shape after reshaping X_128\" +str(X_128.shape))\nprint(\"\\n\\n\")\n\nprint(\"Shape before reshaping X_264\" +str(X_264.shape))\nX_264=X_264.reshape(1000,264,264,3)\nprint(\"Shape after reshaping X_264\" +str(X_264.shape))\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bc7d08ede0dc563bd221e6004aa8898bae6d5c9e"},"cell_type":"markdown","source":"**Confirming that image path on 1st index of data frame is same as the image at 1st index of the Np Arrays**"},{"metadata":{"trusted":true,"_uuid":"3b87ec5b0bdfed8e7371c648f3a792adc7b2023f"},"cell_type":"code","source":"im = plt.imread(B_df['path'][1]) # reading Image from its path\n\nplt.imshow(im)# show the image\nplt.title(\"Orignal Image\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9632d11c1430af93ff6f7b2e63d45ea2b266264e"},"cell_type":"code","source":"plt.title(\"90*90*3 Image\")\nplt.imshow(X_90[1])\nplt.show()\n\nplt.title(\"128*128*3 Image\")\nplt.imshow(X_128[1])\nplt.show()\n\nplt.title(\"264*264*3 Image\")\nplt.imshow(X_264[1])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e769cdf35a7cddeeefd8ce29ea71e7e9a6bea9f8"},"cell_type":"markdown","source":"**Its seem to be same now thats good for us**"},{"metadata":{"_uuid":"a3605da2dd2e8140f7b14fa5b12bdeb488280052"},"cell_type":"markdown","source":"**Now I have my X and Y. Now its time for spliting and training**"},{"metadata":{"trusted":true,"_uuid":"9947540f03908f2340b1de637fd306f15c52d455"},"cell_type":"code","source":"y.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e08529b5824b56a61fd1cc99c985c12345906258"},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(\n    X_128,y, test_size=0.10, random_state=42)\ny_train = to_categorical(y_train, num_classes=2)\ny_test_Categorical=to_categorical(y_test)\ny_categorical =to_categorical(y)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"295eb6aff84b1018e66376d105ead5990e55d7cf"},"cell_type":"markdown","source":"**CNN MODEL **"},{"metadata":{"trusted":true,"_uuid":"f448c61a56313d50de76aa71a032b79169397f44"},"cell_type":"code","source":"from keras.models import Sequential,Model\nfrom keras.layers import Conv2D,MaxPooling2D,Dense,Flatten,Dropout,Activation\nfrom keras import losses\nfrom keras.optimizers import Adam, Adagrad\nfrom keras.callbacks import EarlyStopping\nfrom keras import regularizers\nfrom sklearn.model_selection import GridSearchCV\nimport keras\n#import talos as ta","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1b611e55d174313671d5eb5c3e1653f157218459"},"cell_type":"markdown","source":"**Talos Model For HyperParameter Optimization**"},{"metadata":{"_uuid":"afcdca406413d88630c9fde82e80ebdc83fb1707"},"cell_type":"markdown","source":"**Talos for hyperparameter tuning**"},{"metadata":{"trusted":true,"_uuid":"36badd189ef2fb415aeffb4803ed766fa4d492a9"},"cell_type":"code","source":"\ndef Talos_Model(X_train, y_train, X_test, y_test, params):\n    #parameters defined\n    lr = params['lr']\n    epochs=params['epochs']\n    dropout_rate=params['dropout']\n    optimizer=params['optimizer']\n    loss=params['loss']\n    last_activation=params['last_activation']\n    activation=params['activation']\n    clipnorm=params['clipnorm']\n    decay=params['decay']\n    momentum=params['momentum']\n    l1=params['l1']\n    l2=params['l2']\n    No_of_CONV_and_Maxpool_layers=params['No_of_CONV_and_Maxpool_layers']\n    No_of_Dense_Layers =params['No_of_Dense_Layers']\n    No_of_Units_in_dense_layers=params['No_of_Units_in_dense_layers']\n    Kernal_Size=params['Kernal_Size']\n    Conv2d_filters=params['Conv2d_filters']\n    pool_size_p=params['pool_size']\n    padding_p=params['padding']\n    \n    #model sequential\n    model=Sequential()\n    \n    for i in range(0,No_of_CONV_and_Maxpool_layers):\n        model.add(Conv2D(Conv2d_filters, Kernal_Size ,padding=padding_p))\n        model.add(Activation(activation))\n        model.add(MaxPooling2D(pool_size=pool_size_p,strides=(2,2)))\n    \n    \n    model.add(Flatten())\n    \n    for i in range (0,No_of_Dense_Layers):\n        model.add(Dense(units=No_of_Units_in_dense_layers,activation=activation, kernel_regularizer=regularizers.l2(l2),\n                  activity_regularizer=regularizers.l1(l1)))\n    \n    \n    model.add(Dense(units=20,activation=activation))\n    \n    model.add(Dense(units=2,activation=activation))\n    if optimizer==\"Adam\":\n        opt=keras.optimizers.Adam(lr=lr, decay=decay, beta_1=0.9, beta_2=0.999)\n    if optimizer==\"Adagrad\":\n        opt=keras.optimizers.Adagrad(lr=lr, epsilon=None, decay=decay)\n    if optimizer==\"sgd\":\n        opt=keras.optimizers.SGD(lr=lr, momentum=momentum, decay=decay, nesterov=False)\n    \n    model.compile(loss=loss,optimizer=opt,\n                 metrics=['accuracy'])\n    \n    out = model.fit(X_train, y_train, epochs=params['epochs'])\n\n    return out,model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d4632d3c06aeb8f495084281364cffc0228ca1e8"},"cell_type":"markdown","source":"**Hyperparameter Tuning  by using Talos**\n\n[https://github.com/autonomio/talos/blob/master/examples/Hyperparameter%20Optimization%20with%20Keras%20for%20the%20Iris%20Prediction.ipynb](http://)"},{"metadata":{"trusted":true,"_uuid":"358358559fd08bf1270875e1ca43915894f4d072"},"cell_type":"code","source":"\n\nparams = {'lr': (0.1, 0.01,1 ),\n     'epochs': [10,5,15],\n     'dropout': (0, 0.40, 0.8),\n     'optimizer': [\"Adam\",\"Adagrad\",\"sgd\"],\n     'loss': [\"binary_crossentropy\",\"mean_squared_error\",\"mean_absolute_error\"],\n     'last_activation': [\"softmax\",\"sigmoid\"],\n     'activation' :[\"relu\",\"selu\",\"linear\"],\n     'clipnorm':(0.0,0.5,1),\n     'decay':(1e-6,1e-4,1e-2),\n     'momentum':(0.9,0.5,0.2),\n     'l1': (0.01,0.001,0.0001),\n     'l2': (0.01,0.001,0.0001),\n     'No_of_CONV_and_Maxpool_layers':[2,3],\n     'No_of_Dense_Layers': [2,3,4],\n     'No_of_Units_in_dense_layers':[128,64,32,256],\n     'Kernal_Size':[(2,2),(4,4),(6,6)],\n     'Conv2d_filters':[60,40,80,120],\n     'pool_size':[(2,2),(4,4)],\n     'padding':[\"valid\",\"same\"]\n    }\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4a53dff46155af7fee746f669a6290c28822c766"},"cell_type":"markdown","source":"Talos take 1500 different scans which is so much expansive even my kaggle kernal die because of talos scans "},{"metadata":{"trusted":true,"_uuid":"255495748dd201773313531d2786cde58f6cafef","_kg_hide-output":true},"cell_type":"code","source":"import talos as ta\nh = ta.Scan(X_train, y_train, params=params,\n            model=Talos_Model,\n            dataset_name='DR',\n            experiment_no='1',\n            grid_downsample=.01)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8cff4198a9b3a0e12126b41f96b2ccd4acdd0cca"},"cell_type":"markdown","source":"**Reporting of talos library**"},{"metadata":{"trusted":true,"_uuid":"306898dced4467b8d74b0e450c29a0dcf6a37a13"},"cell_type":"code","source":"accessing the results data frame\nh.data.head()\n\naccessing epoch entropy values for each round\nh.peak_epochs_df\n\naccess the summary details\nh.details","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b52ee79b2832ca502ffa038247b9e918e6129d13"},"cell_type":"code","source":"r = ta.Reporting(h)\n\nr.best_params()\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"99270f00dfbe4315b472e374f6c49988bf9f6b71"},"cell_type":"markdown","source":"**Randomized Search For Hyperparameter tuning**"},{"metadata":{"_uuid":"304d6da3a83c0c5d9504906b54833293dc9f10f5"},"cell_type":"markdown","source":"**Model**"},{"metadata":{"trusted":true,"_uuid":"172cc2216ae33cc288cd8dfb6cb12ab1d762b68b"},"cell_type":"code","source":"\ndef Randomized_Model(lr=0.01,dropout=0.5,optimizer=\"adam\",loss='mean_squared_error',\n                    last_activation=\"softmax\",activation=\"relu\",clipnorm=0.1,\n                    decay=1e-2,momentum=0.5,l1=0.01,l2=0.001,No_of_CONV_and_Maxpool_layers=3,\n                    No_of_Dense_Layers=3,No_of_Units_in_dense_layers=24,Conv2d_filters=60):\n       \n    \n    \n    #model sequential\n    model=Sequential()\n    \n    for i in range(0,No_of_CONV_and_Maxpool_layers):\n        model.add(Conv2D(Conv2d_filters, (2,2) ,padding=\"same\"))\n        model.add(Activation(activation))\n        model.add(MaxPooling2D(pool_size=(2,2),strides=(2,2)))\n    \n    \n    model.add(Flatten())\n    \n    for i in range (0,No_of_Dense_Layers):\n        model.add(Dense(units=No_of_Units_in_dense_layers,activation=activation, kernel_regularizer=regularizers.l2(l2),\n                  activity_regularizer=regularizers.l1(l1)))\n    \n    model.add(Dropout(dropout))\n    model.add(Dense(units=20,activation=activation))\n    \n    model.add(Dense(units=2,activation=activation))\n    if optimizer==\"Adam\":\n        opt=keras.optimizers.Adam(lr=lr, decay=decay, beta_1=0.9, beta_2=0.999)\n    if optimizer==\"Adagrad\":\n        opt=keras.optimizers.Adagrad(lr=lr, epsilon=None, decay=decay)\n    if optimizer==\"sgd\":\n        opt=keras.optimizers.SGD(lr=lr, momentum=momentum, decay=decay, nesterov=False)\n    \n    model.compile(loss=loss,optimizer=opt,\n                 metrics=['accuracy'])\n    \n    \n\n    return model","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fdfc5be86a0e060f0cf09a5aef262d0afd6f6a70"},"cell_type":"markdown","source":"**Parameters of Randomized Search**"},{"metadata":{"trusted":true,"_uuid":"d9f91b57dc9936c42dcdde953dac7031999acd81"},"cell_type":"code","source":"\n\nparams = {'lr': (0.1, 0.01,1,0.001 ),\n     'epochs': [10,5,15,30],\n     'dropout': (0, 0.40, 0.8),\n     'optimizer': [\"Adam\",\"Adagrad\",\"sgd\"],\n     'loss': [\"binary_crossentropy\",\"mean_squared_error\",\"mean_absolute_error\"],\n     'last_activation': [\"softmax\",\"sigmoid\"],\n     'activation' :[\"relu\",\"selu\",\"linear\"],\n     'clipnorm':(0.0,0.5,1),\n     'decay':(1e-6,1e-4,1e-2),\n     'momentum':(0.9,0.5,0.2),\n     'l1': (0.01,0.001,0.0001),\n     'l2': (0.01,0.001,0.0001),\n     'No_of_CONV_and_Maxpool_layers':[2,3],\n     'No_of_Dense_Layers': [2,3,4,5],\n     'No_of_Units_in_dense_layers':[128,64,32,256],\n     \n     'Conv2d_filters':[60,40,80,120,220]\n     \n     \n    }\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1733ab33fa158b7094409b9a2baf67fda6f42fa1","_kg_hide-output":true},"cell_type":"code","source":"from keras.wrappers.scikit_learn import KerasClassifier\nfrom sklearn.model_selection import RandomizedSearchCV, KFold\nfrom sklearn.metrics import make_scorer\n# model class to use in the scikit random search CV \nmodel = KerasClassifier(build_fn=Randomized_Model, epochs=10, batch_size=20, verbose=1)\ngrid = RandomizedSearchCV(estimator=model, cv=KFold(3), param_distributions=params, \n                          verbose=20,  n_iter=10, n_jobs=1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"48f586502c8b81b5ace354be9aac4210e8ab83c2","_kg_hide-output":true},"cell_type":"code","source":"grid_result = grid.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3579cc3b86fa17ff2b42cf815ba374fe9f7bd77c"},"cell_type":"markdown","source":"**Best Params of Randomized search**"},{"metadata":{"trusted":true,"_uuid":"5395aa0cd00cdc8d31358558d27a403e2b5ec557"},"cell_type":"code","source":"best_params=grid_result.best_params_\nbest_params","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"414baf56512334fab8c2fdd35534ba472175ce89"},"cell_type":"code","source":"\nfrom sklearn.metrics import accuracy_score\n\ny=grid_result.predict(X_test)\nrandom=accuracy_score(y, y_test)\nprint(\"Base Accuracy \",random)\n\nbest_random = grid_result.best_estimator_\ny1=best_random.predict(X_test)\nBest=accuracy_score(y1, y_test)\nprint(\"Best Accuracy \" ,Best)\n\n\nprint('Improvement of {:0.2f}%.'.format( 100 * (Best - random) / random))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"db0d47e241f94bb58c8dfd4dcd1a5a46ea918e7f"},"cell_type":"markdown","source":""},{"metadata":{"trusted":true,"_uuid":"b6da09f65eaf59710c2ec0201f7257b780856588"},"cell_type":"code","source":"def Best_param_Model(best_params):\n       \n    lr=best_params[\"lr\"]\n    dropout=best_params[\"dropout\"]\n    optimizer=best_params[\"optimizer\"]\n    loss=best_params[\"loss\"]\n    last_activation=best_params[\"last_activation\"]\n    activation=best_params[\"activation\"]\n    clipnorm=best_params[\"clipnorm\"]\n    decay=best_params[\"decay\"]\n    momentum=best_params[\"momentum\"]\n    l1=best_params[\"l1\"]\n    l2=best_params[\"l2\"]\n    No_of_CONV_and_Maxpool_layers=best_params[\"No_of_CONV_and_Maxpool_layers\"]\n    No_of_Dense_Layers=best_params[\"No_of_Dense_Layers\"]\n    No_of_Units_in_dense_layers=best_params[\"No_of_Units_in_dense_layers\"]\n    Conv2d_filters=best_params[\"Conv2d_filters\"]\n    \n    #model sequential\n    model=Sequential()\n    \n    for i in range(0,No_of_CONV_and_Maxpool_layers):\n        model.add(Conv2D(Conv2d_filters, (2,2) ,padding=\"same\"))\n        model.add(Activation(activation))\n        model.add(MaxPooling2D(pool_size=(2,2),strides=(2,2)))\n    \n    \n    model.add(Flatten())\n    \n    for i in range (0,No_of_Dense_Layers):\n        model.add(Dense(units=No_of_Units_in_dense_layers,activation=activation, kernel_regularizer=regularizers.l2(l2),\n                  activity_regularizer=regularizers.l1(l1)))\n    \n    \n    model.add(Dense(units=20,activation=activation))\n    \n    model.add(Dense(units=2,activation=activation))\n    if optimizer==\"Adam\":\n        opt=keras.optimizers.Adam(lr=lr, decay=decay, beta_1=0.9, beta_2=0.999)\n    if optimizer==\"Adagrad\":\n        opt=keras.optimizers.Adagrad(lr=lr, epsilon=None, decay=decay)\n    if optimizer==\"sgd\":\n        opt=keras.optimizers.SGD(lr=lr, momentum=momentum, decay=decay, nesterov=False)\n    \n    model.compile(loss=loss,optimizer=opt,\n                 metrics=['accuracy'])\n    \n    \n\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d44860874f77e57d6097dfe2e8a508577fc1c8d0","_kg_hide-output":true},"cell_type":"code","source":"\nBinary_model=Best_param_Model(best_params)\nhistory =Binary_model.fit(X_train, y_train, epochs=100, validation_data=(X_test,y_test_Categorical))\n\n# Plot training & validation accuracy values\nplt.plot(history.history['acc'])\nplt.plot(history.history['val_acc'])\n\n\nplt.title('Model accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.show()\n\n# Plot training & validation loss values\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\n\nplt.title('Model loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2455607daf304f9c4f31cf4000f1a8152ea2aea0"},"cell_type":"code","source":"Binary_model.evaluate(X_test,y_test_Categorical)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c4a94e221c52734002a8854909da7fdce98d2d40"},"cell_type":"markdown","source":"**Visualizing my CNN model**"},{"metadata":{"trusted":true,"_uuid":"fa21ffdc50acfb243ee5ede6945dad7959f49bbc"},"cell_type":"code","source":"y=B_df['level'].values\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(\n    X_128,y, test_size=0.10, random_state=42)\ny_train = to_categorical(y_train, num_classes=2)\ny_test_Categorical=to_categorical(y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9701d9d546a13ec669d58b28e5466696131d8062","scrolled":true,"_kg_hide-output":true},"cell_type":"code","source":"\nmodel = Sequential()\nmodel.add(Conv2D(16,kernel_size = (5,5),activation = 'relu', activity_regularizer=regularizers.l2(1e-8)))\nmodel.add(Conv2D(32,kernel_size = (5,5),activation = 'relu', activity_regularizer = regularizers.l2(1e-8)))\nmodel.add(MaxPooling2D(3,3))\nmodel.add(Conv2D(64,kernel_size = (5,5),activation = 'relu', activity_regularizer = regularizers.l2(1e-8)))\nmodel.add(MaxPooling2D(3,3))\nmodel.add(Conv2D(128,activation = 'relu',kernel_size = (3,3),activity_regularizer = regularizers.l2(1e-8)))\nmodel.add(Flatten())\nmodel.add(Dropout(0.4))\nmodel.add(Dense(64,activation = 'tanh',activity_regularizer = regularizers.l2(1e-8)))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(16,activation = 'tanh',activity_regularizer = regularizers.l2(1e-8)))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(2,activation = 'softmax'))\nmodel.compile(loss=keras.losses.binary_crossentropy, optimizer=\"adam\", metrics=[\"accuracy\"])\nmodel.fit(X_train,y_train, epochs = 10 ,batch_size = 16,validation_data=(X_test,y_test_Categorical))\nmodel.summary()\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3803469addf95c901556379672e89b2ce527d364"},"cell_type":"markdown","source":"Accuracy Measures\n[Helping Material](http://comprna.upf.edu/courses/Master_AGB/2_ClassificationAlgorithms/Lecture_Accuracy.pdf)"},{"metadata":{"trusted":true,"_uuid":"458012a41631422880d372941458b0f633751dfc"},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\nprediction=model.predict(X_test)\ny_pred=[]\nfor i in prediction:\n    y_pred.append(i.argmax())\ny_pred=np.asarray(y_pred)\ntrue_negative,false_positive,false_negative,true_positive=confusion_matrix(y_test, y_pred).ravel()\n\nprint(\"true_negative: \",true_negative)\nprint(\"false_positive: \",false_positive)\nprint(\"false_negative: \",false_negative)\nprint(\"true_positive: \",true_positive)\nprint(\"\\n\\n Accuracy Measures\\n\\n\")\nSensitivity=true_positive/(true_positive+false_negative)\nprint(\"Sensitivity: \",Sensitivity)\n\nFalse_Positive_Rate=false_positive/(false_positive+true_negative)\nprint(\"False_Positive_Rate: \",False_Positive_Rate)\n\nSpecificity=true_negative/(false_positive + true_negative)\nprint(\"Specificity: \",Specificity)\n\n#FDR Ã  0 means that very few of our predictions are wrong\nFalse_Discovery_Rate=false_positive/(false_positive+true_positive)\nprint(\"False_Discovery_Rate: \",False_Discovery_Rate)\n\nPositive_Predictive_Value =true_positive/(true_positive+false_positive)\nprint(\"Positive_Predictive_Value: \",Positive_Predictive_Value)\n\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c02f3b61b1409acd5b9dd40a5cda0b6203e91a35"},"cell_type":"markdown","source":"**Model is Giving 0% accuracy After all of this If someone know the solution please let me know**\n1. According to me less images will be the problem so that cnn is not getting enough images to train their weights well"},{"metadata":{"trusted":true,"_uuid":"79b3e2128244f39f30951b1c8666005f38a1b32c"},"cell_type":"code","source":"a=np.expand_dims( X_train[10],axis=0)\na.shape\nlayer_outputs = [layer.output for layer in model.layers]\nactivation_model = Model(inputs=model.input, outputs=layer_outputs)\nactivations = activation_model.predict(a)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5490b4618d2ae30cf8cd1caccc5e6920965177b9"},"cell_type":"code","source":"def display_activation(activations, col_size, row_size, act_index): \n    activation = activations[act_index]\n    activation_index=0\n    fig, ax = plt.subplots(row_size, col_size, figsize=(row_size*2.5,col_size*1.5))\n    for row in range(0,row_size):\n        for col in range(0,col_size):\n            ax[row][col].imshow(activation[0, :, :, activation_index])\n            activation_index += 1\ndisplay_activation(activations, 4, 4,1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"95fd6b70f17d00cae18897c418edf0aa34772940"},"cell_type":"markdown","source":"please visualize 1 and 3 shape their will be a 6d array of filters but first 16 filter not present in the weights array"},{"metadata":{"_uuid":"00efcf8725c192d03ef4bce4e6de81aa9907a8db"},"cell_type":"markdown","source":"**Visualizing 15th filter of my first cov2d layer**"},{"metadata":{"trusted":true,"_uuid":"beaf8921ae1e14443593369db699a718a3e1fb6a","_kg_hide-input":false,"_kg_hide-output":false},"cell_type":"code","source":"top_layer = model.layers[0]\nplt.imshow(top_layer.get_weights()[0][:, :, :,15 ])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d0f57d2b784f5c3fb18b13c76c1f8ff0e314f6df"},"cell_type":"markdown","source":"**Achiving 76% accuracy till now on 1000 examples**"},{"metadata":{"trusted":true,"_uuid":"f02b05fd7c2cbd9dea3401be6d638c8d02e11b9d"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ee961f2a64b0297754b7a29b6fc10f00aae3c532"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"63e2c71231aecdff344100184c023500c46c2a75"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"66be5c8384be88e9c0318b73697475c0c195fcfa"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fcf5040e64ac5be05d8f1ed7c1d9c1e793ed91f4"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"46c08ee16c7feb872a50cf670229f431024c8a67"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a3d659bdabf20e7d628ccd20e6f5551c826c8f2c"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"95641de3868bc92dec3bbaec5da24c8e9ea38c09"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bc6d1769767e0340475403d49ec3068e1b02e83a"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"826df147bd60e45328cd5eb6e09b5e0e88de2f48"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f124a6d59166b3f6873a467350b8fe0348ae59d1"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"07cc76663f534840e971eeb4b624cbdf027b16fd"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"84351f1cac74cd2cee37250a7618485faaf8cddc"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1f94ed31bb943f75c51c5f928893807261fe27b1"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"27709611d7631bc72afcd815e39d7c4c119ed068"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5fa9e93039e0030d72b281dcc86703bdd85fc98e"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bca6bc0b6347ebca6ddbb2dd6a7180a03d3af3b5"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ac5743b49a4ce23f65c36d4bbc89413ab7dcc3da"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4fdec00c80402bd9f80acd68ef8d7a3fea243ee6"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"283f1c0c7a607dae900ca76c4a4be5a5eebb7ef8"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"774e0212f020859222ddcf8190020a23b2ead60b"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2137bbe15c8116c1315c443e5ed77e99e4901c11"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bcaf6b844fe2dca4aebc559f786880365498bd8c"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"91446c036a72435a98add4bda38afb8d2e5bd0a2"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"233f927707ae6e39d5ac63903914f658fce693be"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9d0cc613c5b5fd2c2de90e5cb95ce15f0c9a6e9f"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f8c2bfcb0fd9a7f773328f45dd969da95f9ebd1a"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1a6f7eaaa33eed466408fb1c0cd48755fed5ab87"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b2ab13035ab58eaea7af50fd570144507b5113ea"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c8e7293cd9e13119a800ab9c9c3c77e6779715ee"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4d3771b82c9f93a82a01bf62b9d436cc02e322a8"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"792fe17723e14576bfb6799f171a44e507629181"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"692b855e9c5d595fc422d2a63b6ee283ae941eb7"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"84c5185f7b6b88ea30346be08dc9df5bda230896"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ca3022b4aa4055814cfd15767dccb121bb7d730d"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bed497e9b6ea6c464a12cfa7979880f8211cf448"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"32f2a537bc9ac4799323aa6d4e3068243275b679"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"90a5ac1fd834d0fd3289192f6890a7569a84a335"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"885a9d3eb1f6331828f18ff955bd9ebde22d8547"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"20b98099d52f93ee830fedb3b4c38c8f58c4f034"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0aae4ff7dc1afc3c04c2d145ff74db4d1e2d1d8e"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2c01beb17b0155987f318bd43a04c400f6e13f1e"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4a7da8f1ba9f2d636eb48f582def85304d46dce2"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"50d5323c3ef48c5d5a094d09ff7575dbf31e1958"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"30b083d2f1122ba5e102846349410efb84bffbb1"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"824cb0e6173ff82e4b49f07526f62162e8fa184d"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"263ea4a86b204d9834746c9698674be07ad49410"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9ba46beb817981810d55e2309dc3135fb5eb6c57"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dcf0852e95fed4fc042884cd36d34cb8c91eb555"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"209e709458625b96caadf43263acd7dda180e76b"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"274833d17d7c56ad6d4b8d81b1b6118c4a2c9d0a"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3a325b8355f6441995253fcef2027faba22c4fab"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f3fef267a69895b1073f12387ddbbd417f918c24"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9a1bf97eaf4c8a3e2bae3eb84e5e054aed273e58"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"554adf271ddb9fd7eef49be24b11dd453088c962"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5e60f147239ce70e146099640d8c752a74ae1b29"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"28db9aed501eb3076917d58f34e40a1e6a7b6383"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"826966cabe2b420345404774a19ce09f8d860564"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7284855ff1a5e6027f80f0d4ff28f909e940370c"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ce5d0ef576951a2b44a103ffcf0ae1e861cfa65d"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e213b129ce5b07a3573546097c9c8a2e057a5332"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}