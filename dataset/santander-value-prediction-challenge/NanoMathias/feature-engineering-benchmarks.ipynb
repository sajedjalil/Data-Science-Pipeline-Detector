{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"collapsed":true},"cell_type":"markdown","source":"# Objective\nTo get an overview of all the possible feature engineering possible in this competition, I'll try to collect and benchmark everything I can find in other kernels and everything I come up with myself in this notebook. It'll be a work-in-progress as I do not have that much time on my hands as to do it in one go."},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true,"_kg_hide-input":true,"collapsed":true},"cell_type":"code","source":"import gc \nimport warnings\n\nimport scipy\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nfrom copy import deepcopy\n\nfrom scipy.stats import skew, kurtosis, gmean, ks_2samp\n\nimport hdbscan\nfrom sklearn.cluster import KMeans, MeanShift\n\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.linear_model import ElasticNetCV\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import LeaveOneOut\nfrom sklearn.decomposition import TruncatedSVD, FastICA, NMF, FactorAnalysis\nfrom sklearn.decomposition import PCA, SparsePCA, MiniBatchSparsePCA, KernelPCA, IncrementalPCA\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.manifold import TSNE\nfrom sklearn.random_projection import GaussianRandomProjection\nfrom sklearn.random_projection import SparseRandomProjection\nfrom sklearn.manifold import TSNE\nfrom sklearn.preprocessing import scale\n\nfrom mlxtend.feature_selection import SequentialFeatureSelector as SFS\n\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin\nfrom sklearn.externals.joblib import Parallel, delayed\nfrom sklearn.base import clone, is_classifier\nfrom sklearn.model_selection._split import check_cv\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport scikitplot as skplt\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom matplotlib.ticker import NullFormatter\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"41d836946df80423691f23bd307e739ac1a6a031"},"cell_type":"markdown","source":"# 1. Data Loading & Pre-processing\nCurrent pre-processing pipeline follows learning from this notebook:<br />\nhttps://www.kaggle.com/c/santander-value-prediction-challenge/kernels\n\nSummary:\n* Log-transform all columns\n* Mean-variance scale all columns excepting sparse entries\n* Remove \"ugly\" rows from test - this is based on the assumptions in [this discussion](https://www.kaggle.com/c/santander-value-prediction-challenge/discussion/61288). Could be dangerous for final score!\n\nI'm not removing zero-variance and duplicate columns, since these are not constant/duplicates in test, and they could contain information when combined with other columns.\n\n* NOTE: I'm only sampling 1000 rows from test and train, so as to experiment quicker in this kernel. I'm running it on the entire dataset locally."},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"7891baeb7f9a3eb1578a5b3ecea05834fe3aba37","collapsed":true},"cell_type":"code","source":"# How many rows to include in this kernel (to make iteration speedier)\nSAMPLES = 1000\n\n# Read train and test files\ntrain_df = pd.read_csv('../input/train.csv').sample(SAMPLES)\ntest_df = pd.read_csv('../input/test.csv').sample(SAMPLES)\n\n# Get the combined data\ntotal_df = pd.concat([train_df.drop('target', axis=1), test_df], axis=0).drop('ID', axis=1)\n\n# Get the target\ny = np.log1p(train_df.target)\n\n# Log-transform all column\ntotal_df.loc[:, :] = np.log1p(total_df.values)\n\n# Scale values. Since it's time-series we shouldn't scale column-wise\nmean = np.mean(total_df.values)\nstd = np.std(total_df.values)\ntotal_df.loc[:, :] = ((total_df.values - mean) / std)\n    \n# Train and test\ntrain_idx = range(0, len(train_df))\ntest_idx = range(len(train_df), len(total_df))\n\n# Longest list of sorted columns I could find, from:\n# https://www.kaggle.com/johnfarrell/giba-s-property-extended-extended-result\nordered_cols = [\n    'f190486d6', '58e2e02e6', 'eeb9cd3aa', '9fd594eec', '6eef030c1', '15ace8c9f', \n    'fb0f5dbfe', '58e056e12', '20aa07010', '024c577b9', 'd6bb78916', 'b43a7cfd5', \n    '58232a6fb', '1702b5bf0', '324921c7b', '62e59a501', '2ec5b290f', '241f0f867', \n    'fb49e4212', '66ace2992', 'f74e8f13d', '5c6487af1', '963a49cdc', '26fc93eb7', \n    '1931ccfdd', '703885424', '70feb1494', '491b9ee45', '23310aa6f', 'e176a204a', \n    '6619d81fc', '1db387535', 'fc99f9426', '91f701ba2', '0572565c2', '190db8488', \n    'adb64ff71', 'c47340d97', 'c5a231d81', '0ff32eb98'\n][::-1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"baa7a8420cc1362737fae2ad5d9532edec0af2de"},"cell_type":"markdown","source":"## 1.1. Aggregations and Functions\nSeveral aggregation features have been suggested, the ones I've read through:\n* https://www.kaggle.com/samratp/aggregates-sumvalues-sumzeros-k-means-pca\n* https://www.kaggle.com/mortido/digging-into-the-data-time-series-theory\n* https://www.kaggle.com/ianchute/geometric-mean-of-each-row-lb-1-55\n* https://www.kaggle.com/sggpls/pipeline-kernel-xgb-fe-lb1-39/code\n\nThe code below is especially inspired by Sergey's notebook."},{"metadata":{"trusted":true,"_uuid":"54e8d445e046d600cd31dfe742f07cea436a8f18","_kg_hide-input":true,"collapsed":true},"cell_type":"code","source":"aggregate_df = pd.DataFrame()\n\n# Wrapper function\ndef diff2(x):\n    return np.diff(x, n=2)\n\n# Different pre-processing to be used before each primary function\npreprocess_steps = [\n    [],\n    [np.diff], [diff2],\n    [np.unique], [np.unique, np.diff], [np.unique, diff2]    \n]\n\n# Different statistics to calculate on each preprocessed step\nstats = [len, np.min, np.max, np.median, np.std, skew, kurtosis] + 19 * [np.percentile]\nstats_kwargs = [{} for i in range(7)] + [{'q': np.round(i, 2)} for i in np.linspace(0.05, 0.95, 19)]\n\n# Only operate on non-nulls\nfor funs in preprocess_steps:\n    \n    # Apply pre-processing steps\n    x = total_df[total_df != 0]\n    for f in funs:\n        x = f(x)\n        \n    # Go through our set of stat functions\n    for stat, stat_kwargs in zip(stats, stats_kwargs):\n        \n        # Construct feature name\n        name_components = [\n            stat.__name__,\n            \"_\".join([f.__name__ for f in funs]),\n            \"_\".join([\"{}={}\".format(k, v) for k,v in stat_kwargs.items()])\n        ]\n        feature_name = \"-\".join([e for e in name_components if e])\n\n        # Calc and save new feature in our dataframe\n        aggregate_df[feature_name] = total_df.apply(lambda x: stat(x, **stat_kwargs), axis=1)\n        \n# Extra features\naggregate_df['number_of_different'] = total_df.nunique(axis=1)\naggregate_df['non_zero_count'] = total_df.astype(bool).sum(axis=1) \naggregate_df['sum_zeros'] = (total_df == 0).astype(int).sum(axis=1)\naggregate_df['non_zero_fraction'] = total_df.shape[1] / total_df.astype(bool).sum(axis=1) \naggregate_df['geometric_mean'] = total_df.apply(\n    lambda x: np.exp(np.log(x[x>0]).mean()), axis=1\n)\naggregate_df.reset_index(drop=True, inplace=True)\naggregate_df['geometric_mean'] = aggregate_df['geometric_mean'].replace(np.nan, 0)\naggregate_df['non_zero_fraction'] = aggregate_df['non_zero_fraction'].replace(np.inf, 0)\n\n# Show user which aggregates were created\nprint(f\">> Created {len(aggregate_df.columns)} features for; {aggregate_df.columns.tolist()}\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"acf6ca30388ac0b6432fb77cebe3c216d2eae13d"},"cell_type":"markdown","source":"## 1.2. Decomposition Methods\nLots of people have been using decomposition methods to reduce the number of features. The ones I've read through so far:\n* https://www.kaggle.com/shivamb/introduction-to-dataset-decomposition-techniques\n* https://www.kaggle.com/yekenot/baseline-with-decomposition-components\n\nFrom my trials in [this notebook](https://www.kaggle.com/nanomathias/linear-regression-with-elastic-net), it seems like often it's only the first 10-20 components that are actually important for the modeling. Since we are testing features now, here I'll include 10 of each decomposition method.\n\n* Note: some of the methods I only fit on training dataset, due to kernel limitations. I believe they should be fitted on the entire dataset."},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"663e156528c534a8c64a39fb21553627b678ae63","collapsed":true},"cell_type":"code","source":"COMPONENTS = 10\n\n# Convert to sparse matrix\nsparse_matrix = scipy.sparse.csr_matrix(total_df.values)\n\n# Data to be passed to t-SNE\ntsvd = TruncatedSVD(n_components=1000).fit_transform(sparse_matrix)\n\n# V1 List of decomposition methods\nmethods = [\n    {'method': KernelPCA(n_components=2, kernel=\"rbf\"), 'data': 'total'},\n    {'method': FactorAnalysis(n_components=COMPONENTS), 'data': 'total'},\n    {'method': TSNE(n_components=3, init='pca'), 'data': 'tsvd'},\n    {'method': TruncatedSVD(n_components=COMPONENTS), 'data': 'sparse'},\n    {'method': PCA(n_components=COMPONENTS), 'data': 'total'},\n    {'method': FastICA(n_components=COMPONENTS), 'data': 'total'},\n    {'method': GaussianRandomProjection(n_components=COMPONENTS, eps=0.1), 'data': 'total'},\n    {'method': SparseRandomProjection(n_components=COMPONENTS, dense_output=True), 'data': 'total'}\n]\n\n# Run all the methods\nembeddings = []\nfor run in methods:\n    name = run['method'].__class__.__name__\n    \n    # Run method on appropriate data\n    if run['data'] == 'sparse':\n        embedding = run['method'].fit_transform(sparse_matrix)\n    elif run['data'] == 'tsvd':\n        embedding = run['method'].fit_transform(tsvd)\n    else:\n        embedding = run['method'].fit_transform(total_df)\n        \n    # Save in list of all embeddings\n    embeddings.append(\n        pd.DataFrame(embedding, columns=[f\"{name}_{i}\" for i in range(embedding.shape[1])])\n    )\n    print(f\">> Ran {name}\")\n    gc.collect()    \n    \n# Put all components into one dataframe\ncomponents_df = pd.concat(embeddings, axis=1).reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a57cbc94ac6bdb5e7fa53802f371fcb5aa26caf0"},"cell_type":"markdown","source":"## 1.3. Dense Autoencoder\nI saw a few people use autoencoders, but here I just implement a very simple one. From empirical tests it seems that the components I extract from this it doesn't make sense to have an embedded dimension higher than about 5-10. I've tried tuning the dense autoencoder in terms of layers, dropout, batch normalization, and learning rate, but I usually do not get anything much better than the one presented below. "},{"metadata":{"trusted":true,"_uuid":"d0194d5f53a8b942290250896182ceb4587c0367","_kg_hide-input":true,"collapsed":true},"cell_type":"code","source":"from keras.layers import *\nfrom keras.optimizers import *\nfrom keras.callbacks import *\nfrom keras.models import Model, Sequential\n\nenc_input = Input((total_df.shape[1], ))\nenc_output = Dense(512, activation='relu')(enc_input)\nenc_output = Dropout(0.5)(enc_output)\nenc_output = Dense(5, activation='relu')(enc_output)\n\ndec_input = Dense(512, activation='relu')(enc_output)\ndec_output = Dropout(0.5)(dec_input)\ndec_output = Dense(total_df.shape[1], activation='relu')(dec_output)\n\n# This model maps an input to its reconstruction\nvanilla_encoder = Model(enc_input, enc_output)\nvanilla_autoencoder = Model(enc_input, dec_output)\nvanilla_autoencoder.compile(optimizer=Adam(0.0001), loss='mean_squared_error')\nvanilla_autoencoder.summary()\n\n# Fit the autoencoder\nvanilla_autoencoder.fit(\n    total_df.values, total_df.values,\n    epochs=6, # INCREASE THIS ONE\n    batch_size=256,\n    shuffle=True,\n    callbacks=[\n        ReduceLROnPlateau(monitor='loss', patience=5, verbose=1),\n        EarlyStopping(monitor='loss', patience=10, mode='min', min_delta=1e-5)\n    ]\n)\n\n# Put into dataframe\ndense_ae_df = pd.DataFrame(\n    vanilla_encoder.predict(total_df.values, batch_size=256), \n    columns=['dense_AE_{}'.format(i) for i in range(5)]\n).reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"513753bf0c80e17f343f70304278d541df9adc36"},"cell_type":"markdown","source":"## 1.4. Supervised Learning features\n### 1.4.1 Classifier Features\nThe code for extracting these features is taken directly from Sergey's kernel:\n* https://www.kaggle.com/sggpls/pipeline-kernel-xgb-fe-lb1-39"},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"0f5c8fa42d94face0eefe4cf59569e8dc5d14033","collapsed":true},"cell_type":"code","source":"# Define regressors and class-levels to go through\nclasses = range(2, 7)\nregressors = [\n    ExtraTreesClassifier(\n        n_estimators=100, max_features=0.5,\n        max_depth=None, max_leaf_nodes=270,\n        min_impurity_decrease=0.0001,\n        n_jobs=-1, class_weight='balanced'\n    ),\n    LogisticRegression(\n        class_weight='balanced'\n    )\n]\n\nclass ClassifierTransformer(BaseEstimator, TransformerMixin):\n    \"\"\"https://www.kaggle.com/sggpls/pipeline-kernel-xgb-fe-lb1-39\"\"\"\n    \n    def __init__(self, estimator=None, n_classes=2, cv=3):\n        self.estimator = estimator\n        self.n_classes = n_classes\n        self.cv = cv\n        \n    @staticmethod\n    def _get_labels(y, n_classes):\n        y_labels = np.zeros(len(y))\n        y_us = np.sort(np.unique(y))\n        step = int(len(y_us) / n_classes)\n        \n        for i_class in range(n_classes):\n            if i_class + 1 == n_classes:\n                y_labels[y >= y_us[i_class * step]] = i_class\n            else:\n                y_labels[\n                    np.logical_and(\n                        y >= y_us[i_class * step],\n                        y < y_us[(i_class + 1) * step]\n                    )\n                ] = i_class\n        return y_labels\n        \n    def fit(self, X, y):\n        y_labels = self._get_labels(y, self.n_classes)\n        cv = check_cv(self.cv, y_labels, classifier=is_classifier(self.estimator))\n        self.estimators_ = []\n        \n        for train, _ in cv.split(X, y_labels):\n            self.estimators_.append(\n                clone(self.estimator).fit(X[train], y_labels[train])\n            )\n        return self\n    \n    def transform(self, X, y=None):\n        cv = check_cv(self.cv, y, classifier=is_classifier(self.estimator))\n        \n        X_prob = np.zeros((X.shape[0], self.n_classes))\n        X_pred = np.zeros(X.shape[0])\n        \n        for estimator, (_, test) in zip(self.estimators_, cv.split(X)):\n            X_prob[test] = estimator.predict_proba(X[test])\n            X_pred[test] = estimator.predict(X[test])\n        return np.hstack([X_prob, np.array([X_pred]).T])\n\n# Put all features into one dataframe (i.e. aggregate, timeseries, components)\nfeature_df = pd.concat([components_df, aggregate_df, dense_ae_df], axis=1).fillna(0)    \n    \n# Collect predictions\nclf_features = []\nclf_columns = []\nfor n in tqdm(classes):\n    for regr in regressors:\n        clf = ClassifierTransformer(regr, n_classes=n, cv=5)\n        clf.fit(tsvd[train_idx], y)\n        clf_features.append(\n            clf.transform(tsvd)\n        )\n        clf_columns += [f\"{n}-{regr.__class__.__name__}_pred{i}\" for i in range(n+1)]\n\n# Save into dataframe\nclf_features = np.concatenate(clf_features, axis=1)\nclassifier_df = pd.DataFrame(clf_features, columns=clf_columns)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1bf016d66049a22aac78b2652a3b6ad1fc7243be"},"cell_type":"markdown","source":"### 1.4.2. Linear Discriminant Analysis\nBy splitting the target into categories we can apply Linear Discriminant Analysis, which i similar to PCA, but in addition to tryin to maximize the variance in our component axes we are also trying to maximize the separation between the target classes. See [scikit-learn reference](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.discriminant_analysis) for details."},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"8f9a1de781b910bf54c9e3f1839bd8625660b6a7","collapsed":true},"cell_type":"code","source":"# Final results\nlda_df = []\n\n# Don't show warnings about collinearity issues\nwith warnings.catch_warnings():    \n    warnings.simplefilter(\"ignore\") \n    \n    # Try for different splits of classes\n    n_classes = [2, 3, 4, 5, 6]\n    for classes in tqdm(n_classes):\n\n        # Create labels / target categories\n        labels = ClassifierTransformer._get_labels(y, classes)\n\n        # Run LDA\n        lda = LinearDiscriminantAnalysis()\n        lda.fit(total_df.values[train_idx], labels)\n        lda_trafo = lda.transform(total_df)\n\n        # The transformed data is in a numpy matrix. This may be inconvenient and we therefore\n        # put transformed/projected data into new dataframe, where we specify column names and index\n        lda_df.append(pd.DataFrame(\n            lda_trafo,\n            columns=[\"LDA{}_Comp{}\".format(classes, i+1) for i in range(lda_trafo.shape[1])]\n        ))\n\n# Put into one dataframe\nlda_df = pd.concat(lda_df, axis=1)\n\n# Plot the LDAs with unique labels 2, 3, 4 in the component space\n# _, axes = plt.subplots(1, 3, figsize=(20, 5))\nfig = plt.figure(figsize=(20, 5))\n\n# Plot 1D plot with label colors\nax = fig.add_subplot(131)\nax.scatter(lda_df.loc[train_idx, 'LDA2_Comp1'], np.zeros(len(train_idx)), c=labels)\nax.xaxis.set_major_formatter(NullFormatter())\nax.yaxis.set_major_formatter(NullFormatter())\nax.set_title(\"LDA with 2 labels\")\nax.set_xlabel(\"Component 1\")\n\n# Plot 2D plot with label colors\nax = fig.add_subplot(132)\nax.scatter(lda_df.loc[train_idx, 'LDA3_Comp1'], lda_df.loc[train_idx, 'LDA3_Comp2'], c=labels)\nax.xaxis.set_major_formatter(NullFormatter())\nax.yaxis.set_major_formatter(NullFormatter())\nax.set_title(\"LDA with 3 labels\")\nax.set_xlabel(\"Component 1\")\nax.set_ylabel(\"Component 2\")\n\n# Plot 3D plot with label colors\nax = fig.add_subplot(133, projection='3d')\nax.scatter(lda_df.loc[train_idx, 'LDA4_Comp1'], lda_df.loc[train_idx, 'LDA4_Comp2'], lda_df.loc[train_idx, 'LDA4_Comp3'], c=labels)\nax.xaxis.set_major_formatter(NullFormatter())\nax.yaxis.set_major_formatter(NullFormatter())\nax.zaxis.set_major_formatter(NullFormatter())\nax.set_title(\"LDA with 4 labels\")\nax.set_xlabel(\"Component 1\")\nax.set_ylabel(\"Component 2\")\nax.set_zlabel(\"Component 3\")\n\n# Show plot\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fb891ae26133ee3fd3ba384a6f77c26c8d9d7674"},"cell_type":"markdown","source":"### 1.4.2. LSTM Regression\nGiven that we have timeseries data, it might be interesting to do a LSTM prediction of the target based on the ordered columns in the dataset. This prediction could potentially be stacked into a subsequent model as a feature."},{"metadata":{"trusted":true,"_uuid":"6472d4059bc10557aa13f84d8645241d7ef89b0d","collapsed":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom keras.callbacks import *\n\n# Split into train & test\nX_train, X_val, y_train, y_val = train_test_split(\n    total_df.iloc[train_idx][ordered_cols],\n    y.values, \n    test_size=0.1,\n    random_state=42,\n    shuffle=True\n)\n\n# Loss function\ndef rmse(y_true, y_pred):\n    return K.sqrt(K.mean(K.square(y_pred - y_true), axis=-1)) \n\n# Simple LSTM model\nmodel = Sequential()\nmodel.add(LSTM(64, return_sequences=True, input_shape=(len(ordered_cols), 1)))\nmodel.add(LSTM(64, return_sequences=True))\nmodel.add(LSTM(32))\nmodel.add(Dense(1, activation='linear'))\nmodel.compile(loss=rmse, optimizer='adam')\n\n# Fit model to train data, validate on validation data\nmodel.fit(\n    np.expand_dims(X_train, 2), y_train,\n    epochs=50,\n    validation_data=(np.expand_dims(X_val, 2), y_val),\n    callbacks=[\n        ReduceLROnPlateau(patience=2, verbose=1),\n        EarlyStopping(patience=3, min_delta=1e-5)\n    ]\n)\n\n# Save regression result in dataframe\nlstm_regr_df = pd.DataFrame(\n    model.predict(np.expand_dims(total_df[ordered_cols].values, 2)),\n    columns=['LSTM_regression']\n)\nlstm_regr_df.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5ddbff96292edc6a98d1d853dda9edea028d1822"},"cell_type":"markdown","source":"Model seems to work decently, in the sense that it does not seem to significantly overfit the data. I would train on the entire training set, but for the purpose of this notebook I've just opted for using this model trained on 90% of our subsetted training data."},{"metadata":{"_uuid":"d605b44e1ae7f7a33c991752ecd90e084017a913"},"cell_type":"markdown","source":"## 1.5. Clustering features\nThe idea here is to cluster the train dataset, predict clusters in test, and use cluster IDs as features. In the following I'll just use KMeans as a simple clustering method, where I can fit the clustering algos on the training data, and then predict clusters on test. For algorithms where we can't fit on train and then predict on test, fitting the algos on could lead to clusters only found in the test dataset and not train, which would not be good. Idea taken from:\n\n* https://www.kaggle.com/samratp/aggregates-sumvalues-sumzeros-k-means-pca"},{"metadata":{"trusted":true,"_uuid":"368242e02161c3ad93c225302463d96972bcef29","_kg_hide-input":true,"collapsed":true},"cell_type":"code","source":"n_clusters = [4, 8, 16, 32, 64]\n\nskplt.cluster.plot_elbow_curve(\n    KMeans(random_state=42),\n    sparse_matrix[train_idx],\n    cluster_ranges=n_clusters,\n    figsize=(20, 5)\n)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"59c60a9a188dbceccdc5622e9f105af1d4dbfd59"},"cell_type":"markdown","source":"Seems reasonable to run KMeans with these different numbers of clusters. We'll easily end up overfitting at the end (when we get to predicting value) if we include too many clusters trained on only train data, but here I'm just testing things out, so I'll include features all the way up to 64 clusters. For visual inspection I also plot where the clusters are located in the t-SNE embedding, as well as the average value for each cluster in the training set."},{"metadata":{"trusted":true,"_uuid":"93767b695f4225595014169124c2917ff0226154","_kg_hide-input":true,"collapsed":true},"cell_type":"code","source":"# Clustering algos\ncluster_methods = {'KMeans-{}'.format(n): KMeans(n_clusters=n) for n in n_clusters}\n\n# Target statistics to derive for each cluster\nstats = [len, np.min, np.max, np.mean, np.median, np.std, skew, kurtosis]\n\n# Put results into this dataframe\ncluster_df = pd.DataFrame()\n\n# Create plot\n_, axes = plt.subplots(2, len(cluster_methods), figsize=(20, 7))\n\n# Convenience function\ndef get_cluster_colors(clusterer, palette='hls'):\n    \"\"\"Create cluster colors based on labels and probability assignments\"\"\"\n    n_clusters = len(np.unique(clusterer.labels_))\n    color_palette = sns.color_palette(palette, n_clusters)\n    cluster_colors = [color_palette[x] if x >= 0 else (0.5, 0.5, 0.5) for x in clusterer.labels_]\n    if hasattr(clusterer, 'probabilities_'):\n        cluster_colors = [sns.desaturate(x, p) for x, p in zip(cluster_colors, clusterer.probabilities_)]\n    return cluster_colors\n    \n# Loop through all the clustering methods\nfor i, (name, method) in tqdm(enumerate(cluster_methods.items())):\n    \n    # Perform clustering on SVD data\n    method.fit(tsvd[train_idx])\n    \n    # Plot on t-SNE for visual inspectino\n    axes[0][i].scatter(\n        components_df['TSNE_0'], components_df['TSNE_1'], \n        s=50, linewidth=0, \n        c=get_cluster_colors(method), \n        alpha=0.25\n    )\n    axes[0][i].set_title(name+\" on t-SNE\")\n    \n    # Plot barplots to see average target in clusters\n    unique_clusters = np.unique(method.labels_)\n    \n    # Predict on entire datasest\n    predicted_clusters = method.predict(tsvd)\n    cluster_df[name] = predicted_clusters\n    \n    # Cluster targets\n    cluster_targets = {cluster: y.iloc[train_idx][method.labels_ == cluster] for cluster in unique_clusters}\n    \n    # Go through each function we want to run on the clusters\n    for fun in stats:\n        cluster2fun = {}\n        \n        # Go through each cluster\n        for cluster in unique_clusters:\n            cluster2fun[cluster] = fun(cluster_targets[cluster])\n        \n        # Map result to final dataframe\n        cluster_df[name+\" \"+fun.__name__] = cluster_df[name].map(cluster2fun)\n        \n    # Show target values for cluster algo\n    means = [np.mean(cluster_targets[c]) for c in unique_clusters]\n    stds = [np.std(cluster_targets[c]) for c in unique_clusters]\n    axes[1][i].bar(unique_clusters, means, 0.5, yerr=stds)\n    axes[1][i].set_xlabel('Cluster')\n    axes[1][i].set_ylabel('Target Value')\n\n# Show plot\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"daac92e100911dbcb768633b3deab509bb91c0cb"},"cell_type":"markdown","source":"## 1.6. Genetic Programming Features\nIt was [suggested to me](https://www.kaggle.com/nanomathias/genetic-programming-to-find-giba-s-property#358927) that genetic algorithms could also be used to generate features. I found that the `gplearn` package makes this very easy to do, so here we go:"},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"6fc954484ed45101f93fc7aa7baa3bba394a8750","collapsed":true},"cell_type":"code","source":"from gplearn.genetic import SymbolicTransformer\n\nfunction_set = ['add', 'sub', 'mul', 'div',\n                'inv', 'log', 'abs', 'neg', \n                'sqrt', 'max', 'min']\n\ngp = SymbolicTransformer(\n    generations=10, population_size=50000,\n    hall_of_fame=100, n_components=10,\n    function_set=function_set,\n    parsimony_coefficient=0.0005,\n    max_samples=0.9, verbose=1,\n    random_state=42, n_jobs=4\n)\n\n# Fit & save to dataframe\ngp.fit(total_df.iloc[train_idx], y)\ngp_features = gp.transform(total_df)\ngenetic_df = pd.DataFrame(gp_features, columns=[f'Genetic_{i}' for i in range(gp_features.shape[1])])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b3a9ddb93a08250e895ac8f5aff78764ca43758a"},"cell_type":"markdown","source":"## 1.6. Time-series features\n### 1.6.1. Timseries Models\nI've had this section on here from the get-go, since I think a lot of us suspected time-series to be involved, however columns being all scrambled up I didn't really spend time on it. Now with Giba's property out, we have the columns sorted in a way that may correspond to the time series, and thus we can start extracting time series predictions from these series of values. \n\nIn the following I'm just trying something quick; linear regression of the entire sequence of columns, and then creating a supervised learning problem with lagged timesteps."},{"metadata":{"trusted":true,"_uuid":"c164f13f4f189e9f3af22633a09dd6b035e76092","_kg_hide-input":true,"collapsed":true},"cell_type":"code","source":"def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n    \"\"\"\n    Frame a time series as a supervised learning dataset.\n    Taken from: https://machinelearningmastery.com/convert-time-series-supervised-learning-problem-python/\n    Arguments:\n        data: Sequence of observations as a list or NumPy array.\n        n_in: Number of lag observations as input (X).\n        n_out: Number of observations as output (y).\n        dropnan: Boolean whether or not to drop rows with NaN values.\n    Returns:\n        Pandas DataFrame of series framed for supervised learning.\n    \"\"\"\n    n_vars = 1 if type(data) is list else data.shape[1]\n    df = pd.DataFrame(data)\n    cols, names = list(), list()\n    # input sequence (t-n, ... t-1)\n    for i in range(n_in, 0, -1):\n        cols.append(df.shift(i))\n        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n    # forecast sequence (t, t+1, ... t+n)\n    for i in range(0, n_out):\n        cols.append(df.shift(-i))\n        if i == 0:\n            names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n        else:\n            names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n    # put it all together\n    agg = pd.concat(cols, axis=1)\n    agg.columns = names\n    # drop rows with NaN values\n    if dropnan:\n        agg.dropna(inplace=True)\n    return agg\n\n# Forecast storage\nforecasts = {k: [] for k in ['ElasticNet_idx', 'RandomForest_lags3']}\n\n# Run various forecasts\nfor i, row in tqdm(total_df[ordered_cols].iterrows(), total=len(total_df)):\n    \n    # Linear regression 2 steps into the future based on index\n    regr = ElasticNetCV(cv=LeaveOneOut())\n    regr.fit(\n        np.arange(0, len(row)).reshape(-1, 1), \n        row.values.reshape(-1, 1).ravel()\n    )\n    forecasts['ElasticNet_idx'].append(regr.predict([[len(row)+2]])[0])\n    \n    # Random forest based on 3 lagged features\n    sdf = series_to_supervised(row.values.tolist(), 3, 2)\n    regr = ExtraTreesRegressor(n_estimators=10)\n    regr.fit(sdf[['var1(t-3)', 'var1(t-2)', 'var1(t-1)']], sdf['var1(t+1)'])\n    forecasts['RandomForest_lags3'].append(\n        regr.predict([row.values[-3:]])[0]\n    )\n    \n# put into dataframe\nforecasts_df = pd.DataFrame(forecasts)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"44ad83b083ad3b35ef59d5f5723d36e2605e620c"},"cell_type":"markdown","source":"### 1.6.2. LSTM Autoencoder\nNow that we have a sequence of columns supposedly corresponding to timesteps in a sequence, maybe it makes sense to crease an LSTM autoencoder; i.e. a unsupervised feature extraction with the use of LSTM layers on those sorted columns only."},{"metadata":{"trusted":true,"scrolled":false,"_uuid":"b1aa7319a3aee987129fd11d812b677ec3baad3d","collapsed":true,"_kg_hide-input":true},"cell_type":"code","source":"latent_dim = 5\ntimesteps = len(ordered_cols)\n\n# Define model (based on https://blog.keras.io/building-autoencoders-in-keras.html)\ninputs = Input(shape=(timesteps, 1))\nencoded = LSTM(5)(inputs)\ndecoded = RepeatVector(timesteps)(encoded)\ndecoded = LSTM(1, return_sequences=True)(decoded)\nsequence_autoencoder = Model(inputs, decoded)\nsequence_encoder = Model(inputs, encoded)\n\n# Compile with MSE\nsequence_autoencoder.compile(\n    optimizer=Adam(0.001),\n    loss='mean_squared_error'\n)\n\n# Fitting only on sorted columns\nlstm_X = np.expand_dims(total_df[ordered_cols].values, 2)\n\n# Fit to all data (test + train)\nsequence_autoencoder.fit(\n    lstm_X, lstm_X,\n    epochs=100,\n    batch_size=256,\n    shuffle=True,\n    verbose=1,\n    callbacks=[\n        ReduceLROnPlateau(monitor='loss', patience=5, verbose=1),\n        EarlyStopping(monitor='loss', patience=10, mode='min', min_delta=1e-5)\n    ]\n)\n\n# Put encoded result into dataframe\nlstm_ae_df = pd.DataFrame(\n    sequence_encoder.predict(lstm_X, batch_size=16), \n    columns=['lstm_AE_{}'.format(i) for i in range(5)]\n).reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bd5adc50079640b25ea58bd20d62a004c77e8a8c"},"cell_type":"markdown","source":"# 2. Feature Benchmarking / Importance Testing\nTo test these features, I'll probe them both individually and in combinations against the target value with local CV scores\n\n## 2.1. Individual Feature Testing\nHere I'm running 10-fold CV scores against target using one feature at a time, in order to see which features perform the best by themselves. I'll use a basic random forest regressor in all my tests."},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"974cbd1322d5665ec80e3d06c439b617ec55fb68","collapsed":true},"cell_type":"code","source":"# Put all features into one dataframe (i.e. aggregate, timeseries, components)\nfeature_df = pd.concat([\n    components_df,\n    aggregate_df,\n    dense_ae_df,\n    classifier_df,\n    lda_df,\n    cluster_df,\n    genetic_df,\n    forecasts_df,\n    lstm_ae_df,\n    lstm_regr_df\n], axis=1).fillna(0)\n\n# Go through each feature\nresults = []\nfor col in tqdm(feature_df.columns):\n    \n    # Get the column values in training\n    X = feature_df.iloc[train_idx][col].values.reshape(-1, 1)\n    \n    # Get CV scores\n    scores = cross_val_score(\n        ExtraTreesRegressor(n_estimators=30),\n        X, y,\n        scoring='neg_mean_squared_error',\n        cv=10\n    )\n    scores = np.sqrt(-scores)\n    for score in scores:\n        results.append({'feature': col, 'score': score, 'mean_score': np.mean(scores)})\n        \n# Put results in dataframe\nresults = pd.DataFrame(results).sort_values('mean_score')\n\n# Function for plotting feature scores. Will be used again later\ndef plot_feature_scores(results, max_cols=100, feature_list=None, title=\"\"):\n    \n    # Save copy\n    results_subset = results.copy()\n    \n    # Only get subset of features for plotting\n    if len(np.unique(results_subset.feature)) > max_cols:\n        results_subset = results_subset[results_subset.mean_score < np.sort(np.unique(results_subset.mean_score))[100]]\n        \n    # Only select certain features\n    if feature_list is not None:\n        results_subset = results_subset[results_subset.feature.isin(feature_list)]\n\n    # Create plot of scores\n    _, axes = plt.subplots(1, 1, figsize=(20, 5))\n    sns.barplot(x='feature', y='score', data=results_subset, ax=axes)\n    plt.xticks(rotation=90)\n    plt.title(title)\n    plt.show()\n\n# Plot the feature scores\nplot_feature_scores(results)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"15439bb33c7954ac79c4a58cfcfe9735b5c278df"},"cell_type":"markdown","source":"I'm pretty sure a lot of these features are overfitting the training data, especially the clusters & classifier ones, but I'll not go into that analysis in this notebook though."},{"metadata":{"_uuid":"55d1303ebdec864570f21b4feca98a77cafde63d"},"cell_type":"markdown","source":"## 2.2. Feature Selection\nThere seems to be some differences between the training and test dataset - at least in terms of it being fairly easy to distinguish test samples from training samples when fitting a classifier. It's easy to imagine that if the distribution in a given feature is significantly different between test and training set, the final model may overfit on training data. Therefore in the following I implement the following two functions:\n* One for recursively eliminating features in the training set based on feature importance, until adversarial validation scores reaches a certain threshold; i.e. I am removing features that aid in the a classifier in predicting whether a sample is from test or train.\n* One where features with Kolmogorov–Smirnov statistics above a certain threshold between train & test are removed"},{"metadata":{"trusted":true,"_uuid":"5ae92ed4dcf42c5bbe9dddeb89420d47d031deb8","_kg_hide-input":true,"collapsed":true},"cell_type":"code","source":"# Train / test bool target\ny_clf = np.zeros(len(total_df))\ny_clf[test_idx] = 1\n\ndef get_safe_KS(df, thr=0.1):\n    \"\"\"Use KS to determine columns with KS statistic above threshold between train & test\"\"\"\n\n    # Book-keeping\n    drop_features = []\n\n    # Go through all columns\n    with tqdm() as pbar:\n        for col in feature_df.columns:\n\n            # Columns left\n            cols_left = [c for c in feature_df.columns if c not in drop_features]\n            pbar.update(1)\n\n            # Look at distribution in feature\n            statistic, pvalue = ks_2samp(\n                feature_df.loc[train_idx, col].values, \n                feature_df.loc[test_idx, col].values\n            )\n            if pvalue < 0.05 and statistic > 0.1:\n                pbar.set_description(f\"Dropping: {col}. KS: {statistic}. p-value: {pvalue}. {len(cols_left)} features left.\")\n                drop_features.append(col)\n            \n    # Return columns to keep\n    return cols_left\n\ndef get_safe_adversarial(df, thr=0.7):\n    \"\"\"Recursively eliminate features from adversarial validation with highest feature importance,\n    Continues untill the accuracy of the oob-score for a random forest decreases below given threshold\n    \"\"\"\n    \n    # Book-keeping\n    current_score = np.inf\n    drop_features = []\n    \n    # Start eliminating features\n    with tqdm() as pbar:\n        while current_score > thr:\n\n            # Columns left\n            cols_left = [c for c in df.columns if c not in drop_features]\n\n            # Fit random forest model\n            regr = ExtraTreesClassifier(n_estimators=100, oob_score=True, bootstrap=True)\n            regr.fit(df[cols_left], y_clf)\n            current_score = regr.oob_score_\n            pbar.update(1)\n\n            # Get most important feature for classification\n            best_feature = cols_left[np.argmax(regr.feature_importances_)]\n\n            # Add to drop and inform user\n            if current_score > thr:\n                pbar.set_description(f\"Acc: {regr.oob_score_}. Dropping: {best_feature}.\")\n                drop_features.append(best_feature)\n            else:\n                pbar.set_description(f\"Adversarial Elimination reached threshold acc of {thr}. {len(cols_left)} features left.\")\n    return cols_left\n\n# Create plot for KS elimination\ncols_left = get_safe_KS(feature_df, 0.1)\nplot_feature_scores(results, feature_list=cols_left, title=\"After Kolmogorov–Smirnov feature elimination\")\n\n# Create plot for adversarial elimination\ncols_left = get_safe_adversarial(feature_df, 0.7)\nplot_feature_scores(results, feature_list=cols_left, title=\"After adversarial feature elimination\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5d5ec31490c60302e39b54a9e4b8b0ee061f0be6"},"cell_type":"markdown","source":"I won't make any claims as to whether it's reasonable to remove these features use any of the above methods in terms of getting a good public LB score, I've not tried it out."},{"metadata":{"_uuid":"526a42c5d3ed3802ce30a2bb33637a456c776ce7"},"cell_type":"markdown","source":"## 2.3. Feature Combination Testing\nFeature combinations can be tested and evaluated in a myriad of ways, but when we are looking at small datasets like in this case, I especially like to use forward/backward feature selection algorithms. So I'll start out with those, and then see how things go - mlxtend comes with a nice package for performing these sequential feature selections, see [here](https://rasbt.github.io/mlxtend/user_guide/feature_selection/SequentialFeatureSelector/)\n\n### 2.2.1. Sequential Floating Forward Selection\nIn floating forward selection we first attempt a regression one feature at a time, and then we pick the best one. Afterwards we try combinations of this first feature with all of the other features one at a time, and pick the best combination, and we do this till a specified threshold in number of features are chosen. Floating refers to the fact that when we have 3 or more features in our \"chosen\" feature set, we also try removing each of these features from the set to see if that increases the score."},{"metadata":{"trusted":true,"_uuid":"9338192d56974f6677ed51649559ac7eb0f1e681","_kg_hide-input":true,"collapsed":true},"cell_type":"code","source":"# Create forward feature selector\nselector = SFS(\n    ExtraTreesRegressor(n_estimators=30),\n    k_features=(1,15),\n    forward=True,\n    scoring='neg_mean_squared_error',\n    cv=10,\n    n_jobs=-1, \n    verbose=0\n)\n\n# Fit model and get best features\nselector.fit(feature_df[cols_left].values[train_idx], y)\n\n# Plot results\nresults = []\ncurrent_features = []\nfor step, info in selector.subsets_.items():\n\n    # What was added / removed on this iteration\n    added_feature = [i for i in info['feature_idx'] if i not in current_features][0]\n    removed_feature = [i for i in current_features if i not in info['feature_idx']]    \n    \n    # Update book-keeping\n    current_features.append(added_feature)\n    \n    # Save for plotting\n    label = f\"Added {feature_df.columns[added_feature]}\"\n    if removed_feature:\n        label += f\". Removed {feature_df.columns[removed_feature[0]]}\"\n        current_features.remove(removed_feature[0])\n    scores = np.sqrt(-info['cv_scores'])\n    for score in scores:\n        results.append({'label': label, 'score': score, 'mean_score': np.mean(scores)})\n        \n# Put results in dataframe\nresults = pd.DataFrame(results)\n\n# Create plot of scores\n_, axes = plt.subplots(1, 1, figsize=(20, 5))\nsns.barplot(x='label', y='score', data=results, ax=axes)\naxes.set_ylim((results.score.min(), results.score.max()))\nplt.xticks(rotation=90)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"0d535ff4956b603f14ddd10575a583fc8805e57c"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}