{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"collapsed":true},"cell_type":"markdown","source":"# Train vs. Test dataset distributions\nBefore getting started on this competition I quickly wanted to check the distributions of the test dataset against that of the training dataset, and if possible see how different from each other they are."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"74e62825d49c5732ca0e64fb1479d00998d8c1e1","_kg_hide-input":true},"cell_type":"code","source":"import gc\nimport itertools\nfrom copy import deepcopy\n\nimport numpy as np\nimport pandas as pd\n\nfrom tqdm import tqdm\n\nfrom scipy.stats import ks_2samp\n\nfrom sklearn.preprocessing import scale, MinMaxScaler\nfrom sklearn.manifold import TSNE\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.decomposition import FastICA\nfrom sklearn.decomposition import PCA\nfrom sklearn.random_projection import GaussianRandomProjection\nfrom sklearn.random_projection import SparseRandomProjection\n\nfrom sklearn import manifold\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.model_selection import cross_val_predict\nfrom sklearn.metrics import classification_report\nfrom sklearn.model_selection import StratifiedKFold\n\nimport matplotlib.pyplot as plt\nfrom matplotlib.ticker import NullFormatter\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"10a8a7d796dc8f9c1d409586a9196ec6a419a547"},"cell_type":"markdown","source":"# 1. t-SNE Distribution Overview\nTo start out I'll take out an equal amount of samples from the train and test dataset (4459 samples from both, i.e. entire training set and sample of test set), and perform a t-SNE on the combined data. I'm scaling all the data with mean-variance, but for columns where we have outliers (> 3x standard deviation) I also do a log-transform prior to scaling.\n\n## 1.0. Data Pre-Processing\nCurrent pre-processing procedure:\n* Get 4459 rows from training set and test set and concatenate them\n* Columns with standard deviation of 0 in training set removed\n* Columns which are duplicate in training set removed\n* Log-transform all columns which have significant outliers (> 3x standard deviation)\n* Create datasets with: \n    * Mean-variance scale all columns including 0-values!\n    * Mean-variance scale all columns **excluding** 0-values!\n    "},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"%%time \n\n# How many samples to take from both train and test\nSAMPLE_SIZE = 4459\n\n# Read train and test files\ntrain_df = pd.read_csv('../input/train.csv').sample(SAMPLE_SIZE)\ntest_df = pd.read_csv('../input/test.csv').sample(SAMPLE_SIZE)\n\n# Get the combined data\ntotal_df = pd.concat([train_df.drop('target', axis=1), test_df], axis=0).drop('ID', axis=1)\n\n# Columns to drop because there is no variation in training set\nzero_std_cols = train_df.drop(\"ID\", axis=1).columns[train_df.std() == 0]\ntotal_df.drop(zero_std_cols, axis=1, inplace=True)\nprint(f\">> Removed {len(zero_std_cols)} constant columns\")\n\n# Removing duplicate columns\n# Taken from: https://www.kaggle.com/scirpus/santander-poor-mans-tsne\ncolsToRemove = []\ncolsScaned = []\ndupList = {}\ncolumns = total_df.columns\nfor i in range(len(columns)-1):\n    v = train_df[columns[i]].values\n    dupCols = []\n    for j in range(i+1,len(columns)):\n        if np.array_equal(v, train_df[columns[j]].values):\n            colsToRemove.append(columns[j])\n            if columns[j] not in colsScaned:\n                dupCols.append(columns[j]) \n                colsScaned.append(columns[j])\n                dupList[columns[i]] = dupCols\ncolsToRemove = list(set(colsToRemove))\ntotal_df.drop(colsToRemove, axis=1, inplace=True)\nprint(f\">> Dropped {len(colsToRemove)} duplicate columns\")\n\n# Go through the columns one at a time (can't do it all at once for this dataset)\ntotal_df_all = deepcopy(total_df)              \nfor col in total_df.columns:\n    \n    # Detect outliers in this column\n    data = total_df[col].values\n    data_mean, data_std = np.mean(data), np.std(data)\n    cut_off = data_std * 3\n    lower, upper = data_mean - cut_off, data_mean + cut_off\n    outliers = [x for x in data if x < lower or x > upper]\n    \n    # If there are crazy high values, do a log-transform\n    if len(outliers) > 0:\n        non_zero_idx = data != 0\n        total_df.loc[non_zero_idx, col] = np.log(data[non_zero_idx])\n    \n    # Scale non-zero column values\n    nonzero_rows = total_df[col] != 0\n    total_df.loc[nonzero_rows, col] = scale(total_df.loc[nonzero_rows, col])\n    \n    # Scale all column values\n    total_df_all[col] = scale(total_df_all[col])\n    gc.collect()\n    \n# Train and test\ntrain_idx = range(0, len(train_df))\ntest_idx = range(len(train_df), len(total_df))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e47174cd98bb0995a23343b3c78f206eda4e3755"},"cell_type":"markdown","source":"With that I end up with two dataframe, pre-processed slightly differently in terms of either scaling with sparse entries or without."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"4633da67fa6a464cfc2856e3d1af35aa9b81261d"},"cell_type":"markdown","source":"## 1.1. Performing PCA\nSince we have so many features, I thought it'd be a good idea to perform PCA prior to the t-SNE to reduce the dimensionality a bit. Arbitrarily I chose to include 1000 PCA components, which includes about 80% of the variation in the dataset, which I think it allright for saying something about the distributions, but also speeding up t-SNE a bit. In the following I show just the visualize only the plots from PCA on the dataset where scaling was performed excluding zeroes."},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"51482eed2e5b4d0675a06c338b2f823b48aea33f"},"cell_type":"code","source":"def test_pca(data, create_plots=True):\n    \"\"\"Run PCA analysis, return embedding\"\"\"\n    \n    # Create a PCA object, specifying how many components we wish to keep\n    pca = PCA(n_components=1000)\n\n    # Run PCA on scaled numeric dataframe, and retrieve the projected data\n    pca_trafo = pca.fit_transform(data)    \n\n    # The transformed data is in a numpy matrix. This may be inconvenient if we want to further\n    # process the data, and have a more visual impression of what each column is etc. We therefore\n    # put transformed/projected data into new dataframe, where we specify column names and index\n    pca_df = pd.DataFrame(\n        pca_trafo,\n        index=total_df.index,\n        columns=[\"PC\" + str(i + 1) for i in range(pca_trafo.shape[1])]\n    )\n\n    # Only construct plots if requested\n    if create_plots:\n        \n        # Create two plots next to each other\n        _, axes = plt.subplots(2, 2, figsize=(20, 15))\n        axes = list(itertools.chain.from_iterable(axes))\n\n        # Plot the explained variance# Plot t \n        axes[0].plot(\n            pca.explained_variance_ratio_, \"--o\", linewidth=2,\n            label=\"Explained variance ratio\"\n        )\n\n        # Plot the cumulative explained variance\n        axes[0].plot(\n            pca.explained_variance_ratio_.cumsum(), \"--o\", linewidth=2,\n            label=\"Cumulative explained variance ratio\"\n        )\n\n        # Show legend\n        axes[0].legend(loc=\"best\", frameon=True)\n\n        # Show biplots\n        for i in range(1, 4):\n\n            # Components to be plottet\n            x, y = \"PC\"+str(i), \"PC\"+str(i+1)\n\n            # Plot biplots\n            settings = {'kind': 'scatter', 'ax': axes[i], 'alpha': 0.2, 'x': x, 'y': y}\n            pca_df.iloc[train_idx].plot(label='Train', c='#ff7f0e', **settings)\n            pca_df.iloc[test_idx].plot(label='Test',  c='#1f77b4', **settings)    \n\n        # Show the plot\n        plt.show()\n    \n    return pca_df\n\n# Run the PCA and get the embedded dimension\npca_df = test_pca(total_df)\npca_df_all = test_pca(total_df_all, create_plots=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"02008acaf74f0b048af4e34e4276c77ccfca0f34"},"cell_type":"markdown","source":"I included to plot the biplots just for fun, even though only very few percent of the variation are described by those components. Looks fun, and also hints at the training data being more spread out in those components than the test data, which seems more tightly clustered around the center.\n\n## 1.2. Running t-SNE\nHaving reduced the dimensionality a bit it's now possible to run the t-SNE in about 5min or so, and subsequently plot both training and test data in the embedded 2D space. In the following I do that for both the dataset cases I have prepared to see any differences."},{"metadata":{"trusted":true,"_uuid":"e79561861ea50ca8b60e55bafa63a86a2fe26396","_kg_hide-output":false,"_kg_hide-input":true},"cell_type":"code","source":"def test_tsne(data, ax=None, title='t-SNE'):\n    \"\"\"Run t-SNE and return embedding\"\"\"\n\n    # Run t-SNE\n    tsne = TSNE(n_components=2, init='pca')\n    Y = tsne.fit_transform(data)\n\n    # Create plot\n    for name, idx in zip([\"Train\", \"Test\"], [train_idx, test_idx]):\n        ax.scatter(Y[idx, 0], Y[idx, 1], label=name, alpha=0.2)\n        ax.set_title(title)\n        ax.xaxis.set_major_formatter(NullFormatter())\n        ax.yaxis.set_major_formatter(NullFormatter())\n    ax.legend()        \n    return Y\n\n# Run t-SNE on PCA embedding\n_, axes = plt.subplots(1, 2, figsize=(20, 8))\n\ntsne_df = test_tsne(\n    pca_df, axes[0],\n    title='t-SNE: Scaling on non-zeros'\n)\ntsne_df_unique = test_tsne(\n    pca_df_all, axes[1],\n    title='t-SNE: Scaling on all entries'\n)\n\nplt.axis('tight')\nplt.show()  ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"387f0a921f28b4426468973f7c319a572e8eaa95"},"cell_type":"markdown","source":"From this is seems like if scaling is performed only on non-zero entries, then the training and test set look more similar. If scaling is performed on all entries it seems like the two datasets are more separated from each other. In a previous notebook I didn't remove duplicate columns or columns with zero standard deviation - in that case even more significant differences were observed. Of course, it's still always important to be careful with t-SNE intepretation in my experience, and it might be worth looking into in more detail; both in terms of t-SNE parameters, pre-processing, etc.\n\n### 1.2.1. t-SNE colored by row-index or zero-count\n@avloss commented on this kernel about the fact that the data is time separated, so I thought it'd be interesting to look a bit more into why the t-SNE looks as it does. The two most obvious measures to investigate, that I could come up with off the top of my head, were the index of the rows (as a measure of 'time', assuming data is not shuffled), and the number of zeros for the given rows."},{"metadata":{"trusted":true,"_uuid":"5e2de080a765e628b53f10f0e2e120ca41dcba0a","_kg_hide-input":true},"cell_type":"code","source":"gc.collect()\n# Get our color map\ncm = plt.cm.get_cmap('RdYlBu')\n\n# Create plot\nfig, axes = plt.subplots(1, 2, figsize=(20, 8))\nsc = axes[0].scatter(tsne_df[:, 0], tsne_df[:, 1], alpha=0.2, c=range(len(tsne_df)), cmap=cm)\ncbar = fig.colorbar(sc, ax=axes[0])\ncbar.set_label('Entry index')\naxes[0].set_title(\"t-SNE colored by index\")\naxes[0].xaxis.set_major_formatter(NullFormatter())\naxes[0].yaxis.set_major_formatter(NullFormatter())\n\nzero_count = (total_df == 0).sum(axis=1).values\nsc = axes[1].scatter(tsne_df[:, 0], tsne_df[:, 1], alpha=0.2, c=zero_count, cmap=cm)\ncbar = fig.colorbar(sc, ax=axes[1])\ncbar.set_label('#sparse entries')\naxes[1].set_title(\"t-SNE colored by number of zeros\")\naxes[1].xaxis.set_major_formatter(NullFormatter())\naxes[1].yaxis.set_major_formatter(NullFormatter())\n ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d6798fe990cbd386f3d3b551ed1e8e30a4ce67bd"},"cell_type":"markdown","source":"Looks pretty interesting - seems like the higher-index rows are located more at the center of the plot. Also, we see a small cluster of rows with few zero-entries, as well as a few more clusters in the right-hand figure.\n\n### 1.2.2. t-SNE with different perplexities\nt-SNE can give some pretty tricky to intepret results depending on the perplexity parameter used. So just to be sure in the following I check for a few different values of the perplexity parameter."},{"metadata":{"trusted":true,"_uuid":"7c5f4dcf20415ba5f4752c24565d41aeec0e5f20","_kg_hide-input":true},"cell_type":"code","source":"_, axes = plt.subplots(1, 4, figsize=(20, 5))\nfor i, perplexity in enumerate([5, 30, 50, 100]):\n    \n    # Create projection\n    Y = TSNE(init='pca', perplexity=perplexity).fit_transform(pca_df)\n    \n    # Plot t-SNE\n    for name, idx in zip([\"Train\", \"Test\"], [train_idx, test_idx]):\n        axes[i].scatter(Y[idx, 0], Y[idx, 1], label=name, alpha=0.2)\n    axes[i].set_title(\"Perplexity=%d\" % perplexity)\n    axes[i].xaxis.set_major_formatter(NullFormatter())\n    axes[i].yaxis.set_major_formatter(NullFormatter())\n    axes[i].legend() \n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3f9c25da6c1268764a86ee0a17523cae4c4cfa0a"},"cell_type":"markdown","source":"Overall these all look pretty similar and show the same trend, so no need to worry about the perplexity parameter it seems.\n\n### 1.2.3. t-SNE colored by target\nFor the training set it may be interesting to see how the different target values are separated on the embedded two dimensions."},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"65939e3398f7fd2f08c118836b36e72f90bf2b65"},"cell_type":"code","source":"# Create plot\nfig, axes = plt.subplots(1, 1, figsize=(10, 8))\nsc = axes.scatter(tsne_df[train_idx, 0], tsne_df[train_idx, 1], alpha=0.2, c=np.log1p(train_df.target), cmap=cm)\ncbar = fig.colorbar(sc, ax=axes)\ncbar.set_label('Log1p(target)')\naxes.set_title(\"t-SNE colored by target\")\naxes.xaxis.set_major_formatter(NullFormatter())\naxes.yaxis.set_major_formatter(NullFormatter())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4890e9e35099768b480e261f9d7466fd59c0d9e8"},"cell_type":"markdown","source":"Clearly the different train target values are located at different locations in the t-SNE plot."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"2200f484196cbc0a16b3b30622255a30dac21c72"},"cell_type":"markdown","source":"# 2. Classification of Test vs. Train\nAnother good check is to see how well we can classify whether a given entry belongs to test or training dataset - if it is possible to do this reasonably well, that is an indication of differences between the two dataset distributions. I'll just run a simple shuffled 10-fold cross-validation with a basic random forest model to see how well it performs this task. First let's try that classification on the case where scaling is performed on all entries:"},{"metadata":{"trusted":true,"_uuid":"02d0c7f07263540d44dbd614d2b84f38cade790c","_kg_hide-input":true},"cell_type":"code","source":"def test_prediction(data):\n    \"\"\"Try to classify train/test samples from total dataframe\"\"\"\n\n    # Create a target which is 1 for training rows, 0 for test rows\n    y = np.zeros(len(data))\n    y[train_idx] = 1\n\n    # Perform shuffled CV predictions of train/test label\n    predictions = cross_val_predict(\n        ExtraTreesClassifier(n_estimators=100, n_jobs=4),\n        data, y,\n        cv=StratifiedKFold(\n            n_splits=10,\n            shuffle=True,\n            random_state=42\n        )\n    )\n\n    # Show the classification report\n    print(classification_report(y, predictions))\n    \n# Run classification on total raw data\ntest_prediction(total_df_all)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f98ae91208fdd5c7e6455bdb1bdd109336687dd6"},"cell_type":"markdown","source":"On the current notebook this gives about a 0.71 f1 score, which means we can do this prediction quite well, indicating some significant differences between the datasets. Let us try on the dataset where we only scaled non-zero values:"},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"de999ae5131bf6724257245db42451367e469ed5"},"cell_type":"code","source":"test_prediction(total_df)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e8f1d9f3bf8854e8834b741394e1648d8c2cd004"},"cell_type":"markdown","source":"This reduced the f1 score a little bit down to 0.68; corresponding to what we observed in the t-SNE analysis, but still it's apparently quite easy for the model to decently well distinguish between train and test - considering the very simple classifcation model used here."},{"metadata":{"_uuid":"f4e2c2831f3eaf3aeadc70bed9e69e186d2ada90"},"cell_type":"markdown","source":"# 3. Feature-by-feature distribution similarity\nNext let us try to look at the problem feature-by-feature, and perform Kolomogorov-Smirnov tests to see if the distribution in test and training set is similar. I'll use the function [scipy.stats.ks_2samp](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.ks_2samp.html#scipy.stats.ks_2samp) from scipy to run the tests. For all those features where the distributions are highly distinguishable, we may benefit from ignoring those columns, so as to avoid overfitting on training data. In the following I just identify those columns, and plot the distributions as a sanity check for some of the features"},{"metadata":{"trusted":true,"scrolled":false,"_uuid":"99773124700f0cbe482ef3be2960a6f8f4465d4a","_kg_hide-input":true},"cell_type":"code","source":"def get_diff_columns(train_df, test_df, show_plots=True, show_all=False, threshold=0.1):\n    \"\"\"Use KS to estimate columns where distributions differ a lot from each other\"\"\"\n\n    # Find the columns where the distributions are very different\n    diff_data = []\n    for col in tqdm(train_df.columns):\n        statistic, pvalue = ks_2samp(\n            train_df[col].values, \n            test_df[col].values\n        )\n        if pvalue <= 0.05 and np.abs(statistic) > threshold:\n            diff_data.append({'feature': col, 'p': np.round(pvalue, 5), 'statistic': np.round(np.abs(statistic), 2)})\n\n    # Put the differences into a dataframe\n    diff_df = pd.DataFrame(diff_data).sort_values(by='statistic', ascending=False)\n\n    if show_plots:\n        # Let us see the distributions of these columns to confirm they are indeed different\n        n_cols = 7\n        if show_all:\n            n_rows = int(len(diff_df) / 7)\n        else:\n            n_rows = 2\n        _, axes = plt.subplots(n_rows, n_cols, figsize=(20, 3*n_rows))\n        axes = [x for l in axes for x in l]\n\n        # Create plots\n        for i, (_, row) in enumerate(diff_df.iterrows()):\n            if i >= len(axes):\n                break\n            extreme = np.max(np.abs(train_df[row.feature].tolist() + test_df[row.feature].tolist()))\n            train_df.loc[:, row.feature].apply(np.log1p).hist(\n                ax=axes[i], alpha=0.5, label='Train', density=True,\n                bins=np.arange(-extreme, extreme, 0.25)\n            )\n            test_df.loc[:, row.feature].apply(np.log1p).hist(\n                ax=axes[i], alpha=0.5, label='Test', density=True,\n                bins=np.arange(-extreme, extreme, 0.25)\n            )\n            axes[i].set_title(f\"Statistic = {row.statistic}, p = {row.p}\")\n            axes[i].set_xlabel(f'Log({row.feature})')\n            axes[i].legend()\n\n        plt.tight_layout()\n        plt.show()\n        \n    return diff_df\n\n# Get the columns which differ a lot between test and train\ndiff_df = get_diff_columns(total_df.iloc[train_idx], total_df.iloc[test_idx])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"69504594b40dfebfa045cd27fd4743fb5b952862"},"cell_type":"markdown","source":"On my run it dropped about 150 features. Let's try a classification report again to see if we can distinguish test from train."},{"metadata":{"trusted":true,"_uuid":"bf73040c943595cd9d05aa263e38a26a8d55ac46"},"cell_type":"code","source":"# Run classification on total raw data\nprint(f\">> Dropping {len(diff_df)} features based on KS tests\")\ntest_prediction(\n    total_df.drop(diff_df.feature.values, axis=1)\n)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2a6c1b47675e37a5969b9a94a3383dcb8fb17c07"},"cell_type":"markdown","source":"Here we actually see lower score, down from 68% to 62%, meaning train and test are more similar. I've not tested these things with any regressors yet, but I'd think it might be interesting to drop some if not all of these features which may enable the model to overfit on training data. I'm not sure Kolmogorovâ€“Smirnov is neccesarily the absolute best statistical test for comparing these kinda-discrete distributions - I've tried only running it on non-zero entries, but in that case we end up removing many more features, while still allowing the model to easily distinguish between train and test based on the zeroes. Suggestions on how to approach this more thoroughly would be appreciated.\n\n# 4. Decomposition Feature\nSo far I've only looked at PCA components, but most kernels look at several decomposition methods, so it may be interesting to look at t-SNE of these 10-50 components of each method instead of 1000 PCA components. Furthermore, it's interesting to see how well we can classify test/train based on this reduced feature space.\n\n"},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"6613edd43f0f255110f4ae55c8da47286e666005"},"cell_type":"code","source":"COMPONENTS = 20\n\n# List of decomposition methods to use\nmethods = [\n    TruncatedSVD(n_components=COMPONENTS),\n    PCA(n_components=COMPONENTS),\n    FastICA(n_components=COMPONENTS),\n    GaussianRandomProjection(n_components=COMPONENTS, eps=0.1),\n    SparseRandomProjection(n_components=COMPONENTS, dense_output=True)    \n]\n\n# Run all the methods\nembeddings = []\nfor method in methods:\n    name = method.__class__.__name__    \n    embeddings.append(\n        pd.DataFrame(method.fit_transform(total_df), columns=[f\"{name}_{i}\" for i in range(COMPONENTS)])\n    )\n    print(f\">> Ran {name}\")\n    \n# Put all components into one dataframe\ncomponents_df = pd.concat(embeddings, axis=1)\n\n# Prepare plot\n_, axes = plt.subplots(1, 3, figsize=(20, 5))\n\n# Run t-SNE on components\ntsne_df = test_tsne(\n    components_df, axes[0],\n    title='t-SNE: with decomposition features'\n)\n\n# Color by index\nsc = axes[1].scatter(tsne_df[:, 0], tsne_df[:, 1], alpha=0.2, c=range(len(tsne_df)), cmap=cm)\ncbar = fig.colorbar(sc, ax=axes[1])\ncbar.set_label('Entry index')\naxes[1].set_title(\"t-SNE colored by index\")\naxes[1].xaxis.set_major_formatter(NullFormatter())\naxes[1].yaxis.set_major_formatter(NullFormatter())\n\n# Color by target\nsc = axes[2].scatter(tsne_df[train_idx, 0], tsne_df[train_idx, 1], alpha=0.2, c=np.log1p(train_df.target), cmap=cm)\ncbar = fig.colorbar(sc, ax=axes[2])\ncbar.set_label('Log1p(target)')\naxes[2].set_title(\"t-SNE colored by target\")\naxes[2].xaxis.set_major_formatter(NullFormatter())\naxes[2].yaxis.set_major_formatter(NullFormatter())\n\nplt.axis('tight')\nplt.show()  ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4be45cfbd04e3f0ec6d027a1e49f4e2aa00084a5"},"cell_type":"markdown","source":"Let us check how well we can classify train from test with these feature:"},{"metadata":{"trusted":true,"_uuid":"53b0e8ed8f24b21c0833d153bc470b48d851041f"},"cell_type":"code","source":"test_prediction(components_df)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5739e7c5e26cc78013abf5883409da6b779e74f0"},"cell_type":"markdown","source":"So here we get a classification f1 score of about 0.83, which is pretty bad I would say. Clearly the test and training are very different from each other looking at these components. Let us try to use the KS tests again to eliminate columns that are significantly different from each other."},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"0273a69e14149458becc3016d684b617c70ceeea"},"cell_type":"code","source":"# Get the columns which differ a lot between test and train\ndiff_df = get_diff_columns(\n    components_df.iloc[train_idx], components_df.iloc[test_idx],\n    threshold=0.1\n)\n\n# Run classification on total raw data\nprint(f\">> Dropping {len(diff_df)} features based on KS tests\")\ntest_prediction(\n    components_df.drop(diff_df.feature.values, axis=1)\n)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"388045ef8e8b9ad91b2765f0ad29ec6e6b662701"},"cell_type":"markdown","source":"So by dropping 78 features we're down to an f1 score of 0.6. I've not tried testing any of this against either local CV score or public LB score, and probably all these features should not be dropped, but I imagine some of them could be leading to overfitting on the training set.\n\nTo be updated."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.5","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}