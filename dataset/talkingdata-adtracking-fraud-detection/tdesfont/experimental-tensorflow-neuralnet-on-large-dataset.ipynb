{"cells":[{"metadata":{"_uuid":"6aaa2d4fd9bfcf958fad1aefc9647c0e7caded26","_cell_guid":"bf210e52-ab11-4c18-9cc8-910bf6b7ba6e"},"cell_type":"markdown","source":"## Import packages"},{"metadata":{"_uuid":"801c1d9136b399bedee6217b5d008228008b7759","_cell_guid":"8b16d551-7490-4494-8325-c5f11c6da7b3","trusted":false,"collapsed":true},"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom tensorflow.contrib.learn.python.learn.datasets.mnist import read_data_sets\nfrom tensorflow.python.framework import ops","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"22289c63b78910ce7854ef9b964eeb5242bd7659","collapsed":true,"_cell_guid":"83b26a4c-135c-4320-b68d-cff27e7d8da8","trusted":false},"cell_type":"code","source":"import datetime\nimport time","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c9f58fe5ed1806aa482c616d0e1261ced4ee68bd","_cell_guid":"4ebff117-4c68-41d5-8761-44d86c1d8970"},"cell_type":"markdown","source":"## Import data"},{"metadata":{"_uuid":"9d2c20ce7e740d08ecdc4d26777edd3e643c1578","collapsed":true,"_cell_guid":"76ad41b9-cbae-423f-970a-8dc5acfd5267","trusted":false},"cell_type":"code","source":"import_path = '../input/'","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d3be68c3d200bf6207fa656f6e89a7097244c204","collapsed":true,"_cell_guid":"4a62e25b-3515-4746-9cbb-ba8945d82218","trusted":false},"cell_type":"code","source":"train_df = pd.read_csv(import_path + 'train.csv', skiprows=1, header=None, \\\n            names=['ip', 'app', 'device','os', 'channel', 'click_time', 'attributed_time', 'is_attributed'],\\\n            chunksize=1000000)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cc301102735a9e2c53110893a8a695209b47513d","collapsed":true,"_cell_guid":"c25d7b04-78cb-4db0-9b30-87564c691a8a","trusted":false},"cell_type":"code","source":"def process_chunk(sample_chunk):\n    \n    sample_chunk.index = pd.DatetimeIndex(sample_chunk['click_time'])\n    sample_chunk['wday'] = sample_chunk.index.map(lambda x: x.weekday())\n    sample_chunk['hour'] = sample_chunk.index.map(lambda x: x.hour)\n\n    sample_chunk['(ip, wday, hour)'] = sample_chunk.apply(lambda row: (row['ip'], row['wday'], row['hour']), axis=1)\n    sample_chunk['freq(ip, wday, hour)'] = sample_chunk.groupby(['(ip, wday, hour)'])['(ip, wday, hour)'].transform('count')\n    sample_chunk = sample_chunk.drop(['(ip, wday, hour)'], axis=1)\n\n    sample_chunk['(ip, hour, channel)'] = sample_chunk.apply(lambda row: (row['ip'], row['hour'], row['channel']), axis=1)\n    sample_chunk['freq(ip, hour, channel)'] = sample_chunk.groupby(['(ip, hour, channel)'])['(ip, hour, channel)'].transform('count')\n    sample_chunk = sample_chunk.drop(['(ip, hour, channel)'], axis=1)\n\n    sample_chunk['(ip, hour, os)'] = sample_chunk.apply(lambda row: (row['ip'], row['hour'], row['os']), axis=1)\n    sample_chunk['freq(ip, hour, os)'] = sample_chunk.groupby(['(ip, hour, os)'])['(ip, hour, os)'].transform('count')\n    sample_chunk = sample_chunk.drop(['(ip, hour, os)'], axis=1)\n\n    sample_chunk['(ip, hour, device)'] = sample_chunk.apply(lambda row: (row['ip'], row['hour'], row['device']), axis=1)\n    sample_chunk['freq(ip, hour, device)'] = sample_chunk.groupby(['(ip, hour, device)'])['(ip, hour, device)'].transform('count')\n    sample_chunk = sample_chunk.drop(['(ip, hour, device)'], axis=1)\n    \n    sample_chunk = sample_chunk.drop(['ip','click_time', 'attributed_time', 'wday'], axis=1)\n    \n    return sample_chunk","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2ef47b5f49ba1e20fe6bce899e765da48b930f83","collapsed":true,"_cell_guid":"3d457771-e500-4b2a-b4c1-ba69a2ad876f","trusted":false},"cell_type":"code","source":"ops.reset_default_graph()\nsess = tf.Session()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9a95f2abe35073d34c36096fc78b53b2476a89f6","collapsed":true,"_cell_guid":"0b98657b-85ad-4cb5-a5c9-fd35bd472cb3","trusted":false},"cell_type":"code","source":"# Set RNN parameters\nepochs = 25\nbatch_size = 100000\nmax_sequence_length = 9\nrnn_size = 10\nembedding_size = 50\nlearning_rate = 0.005\ndropout_keep_prob = tf.placeholder(tf.float32)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"06622ea31e92a632440b0e6237ff27c9c9cabdf8","collapsed":true,"_cell_guid":"825ec64c-5b68-4f1c-87bb-3ac0a069df11","trusted":false},"cell_type":"code","source":"# Normalize by column (min-max norm to be between 0 and 1)\ndef normalize_cols(m):\n    col_max = m.max(axis=0)\n    col_min = m.min(axis=0)\n    return (m-col_min) / (col_max - col_min)\n    \n\n# Define Variable Functions (weights and bias)\ndef init_weight(shape, st_dev):\n    weight = tf.Variable(tf.random_normal(shape, stddev=st_dev))\n    return(weight)\n    \n\ndef init_bias(shape, st_dev):\n    bias = tf.Variable(tf.random_normal(shape, stddev=st_dev))\n    return(bias)\n    \n    \n# Create Placeholders\nx_data = tf.placeholder(shape=[None, 9], dtype=tf.float32)\ny_target = tf.placeholder(shape=[None, 1], dtype=tf.float32)\n\n\n# Create a fully connected layer:\ndef fully_connected(input_layer, weights, biases):\n    layer = tf.add(tf.matmul(input_layer, weights), biases)\n    return(tf.nn.relu(layer))\n\n\n#--------Create the first layer (50 hidden nodes)--------\nweight_1 = init_weight(shape=[9, 50], st_dev=10.0)\nbias_1   = init_bias(shape=[50], st_dev=10.0)\nlayer_1  = fully_connected(x_data, weight_1, bias_1)\n\n#--------Create second layer (25 hidden nodes)--------\nweight_2 = init_weight(shape=[50, 25], st_dev=10.0)\nbias_2 = init_bias(shape=[25], st_dev=10.0)\nlayer_2 = fully_connected(layer_1, weight_2, bias_2)\n\n\n#--------Create third layer (5 hidden nodes)--------\nweight_3 = init_weight(shape=[25, 10], st_dev=10.0)\nbias_3 = init_bias(shape=[10], st_dev=10.0)\nlayer_3 = fully_connected(layer_2, weight_3, bias_3)\n\n\n#--------Create output layer (1 output value)--------\nweight_4 = init_weight(shape=[10, 1], st_dev=10.0)\nbias_4 = init_bias(shape=[1], st_dev=10.0)\nfinal_output = fully_connected(layer_3, weight_4, bias_4)\n\n# Declare loss function:\nloss = tf.reduce_mean(tf.square(y_target - final_output))\n\n# Declare optimizer\nmy_opt = tf.train.AdamOptimizer(0.005)\ntrain_step = my_opt.minimize(loss)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"_uuid":"715d43d6129bcf919f597caa9de15fcb4e6be7f5","_cell_guid":"cda3e079-cbc9-46dc-913a-b1ee9e878e50","trusted":false,"collapsed":true},"cell_type":"code","source":"%matplotlib inline\n\n# Initialize Variables\ninit = tf.global_variables_initializer()\nsess.run(init)\n\n# Training loop\nloss_vec = []\ntest_loss = []\n\nfor idx_chunk, chunk in enumerate(train_df):\n    print('chunk nÂ°{} is being processing...'.format(idx_chunk))\n    chunk = process_chunk(chunk)\n    print('chunk processed.')\n    \n    x_vals = np.array(chunk.drop(['is_attributed'], axis=1))\n    y_vals = np.array(chunk['is_attributed'])\n    \n    shuffled_ix = np.random.permutation(np.arange(len(x_vals)))\n    x_shuffled = x_vals[shuffled_ix]\n    y_shuffled = y_vals[shuffled_ix]\n    \n    # Split train/test set\n    ix_cutoff = int(len(y_vals)*0.80)\n    x_vals_train, x_vals_test = x_shuffled[:ix_cutoff], x_shuffled[ix_cutoff:]\n    y_vals_train, y_vals_test = y_shuffled[:ix_cutoff], y_shuffled[ix_cutoff:]\n    \n    x_vals_train = np.nan_to_num(normalize_cols(x_vals_train))\n    x_vals_test = np.nan_to_num(normalize_cols(x_vals_test))\n\n    for i in range(epochs):\n        rand_index = np.random.choice(len(x_vals_train), size=batch_size)\n        rand_x = x_vals_train[rand_index]\n        rand_y = np.transpose([y_vals_train[rand_index]])\n        sess.run(train_step, feed_dict={x_data: rand_x, y_target: rand_y})\n\n        temp_loss = sess.run(loss, feed_dict={x_data: rand_x, y_target: rand_y})\n        loss_vec.append(temp_loss)\n\n        test_temp_loss = sess.run(loss, feed_dict={x_data: x_vals_test, y_target: np.transpose([y_vals_test])})\n        test_loss.append(test_temp_loss)\n        if (i+1) % 10 == 0:\n            print('Generation: ' + str(i+1) + '. Loss = ' + str(temp_loss))\n\n    if idx_chunk % 10 == 0:\n        # Plot loss (MSE) over time\n        plt.plot(loss_vec, 'k-', label='Train Loss')\n        plt.plot(test_loss, 'r--', label='Test Loss')\n        plt.semilogy()\n        plt.title('Loss (MSE) per Generation')\n        plt.legend(loc='upper right')\n        plt.xlabel('Generation')\n        plt.ylabel('Loss')\n        plt.show()\n    \n    if idx_chunk == 50: # Equivalent to 50 Mo lines of processed rows\n        break","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.5","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}