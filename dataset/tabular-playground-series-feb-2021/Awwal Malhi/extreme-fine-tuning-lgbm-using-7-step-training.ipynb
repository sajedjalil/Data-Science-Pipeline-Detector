{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Extreme Fine Tuning of LGBM using Incremental training\n\n\nIn my efforts to push leaderboard i stumbled across a small trick to improve predictions in 4th to 5th decimal using same parameters and a single model, essentially it is a trick to improve prediction of your best parameter, squeezing more out of them!!. Trick is executed in following steps:\n\n* Find the best parameters for your LGBM, manually or using optimization methods of your choice.\n\n\n* train the model to the best RMSE you can get in one training round using high early stopping.\n\n\n* train the model for 1 or 2 rounds with reduced learning rate.\n\n\n* once the first few rounds are over, start reducing regularization params by a factor at each incremental training iteration, you will start observing improvements in 5th decimal place... which is enough to get 5th decimal improvement on your models leaderboard score.\n\nAt the top of leaderboard this make a huge difference, i pushed my rank from `39` at **0.84202** to my best `6th place`(17th Feb 2021) with **0.84193**\n\nLets check out."},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import KFold, GridSearchCV, train_test_split\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import LabelEncoder\n\nfrom lightgbm import LGBMRegressor\n\nimport optuna\nfrom functools import partial\n\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/tabular-playground-series-feb-2021/train.csv')\ntest = pd.read_csv('../input/tabular-playground-series-feb-2021/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = train.drop(['id', 'target'], axis=1)\ny_train = train.target\nX_test = test.drop(['id'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_cols = [feature for feature in train.columns if 'cat' in feature]\n\ndef label_encoder(df):\n    for feature in cat_cols:\n        le = LabelEncoder()\n        le.fit(df[feature])\n        df[feature] = le.transform(df[feature])\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = label_encoder(X_train)\nX_test = label_encoder(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"split = KFold(n_splits=5, random_state=2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def objective(trial, X, y, name='xgb'):\n        \n    params = {'max_depth':trial.suggest_int('max_depth', 5, 50),\n              'n_estimators':200000,\n              #'boosting':trial.suggest_categorical('boosting', ['gbdt', 'dart', 'goss']),\n              'subsample': trial.suggest_uniform('subsample', 0.2, 1.0),\n              'colsample_bytree':trial.suggest_uniform('colsample_bytree', 0.2, 1.0),\n              'learning_rate':trial.suggest_uniform('learning_rate', 0.007, 0.02),\n              'reg_lambda':trial.suggest_uniform('reg_lambda', 0.01, 50),\n              'reg_alpha':trial.suggest_uniform('reg_alpha', 0.01, 50),\n              'min_child_samples':trial.suggest_int('min_child_samples', 5, 100),\n              'num_leaves':trial.suggest_int('num_leaves', 10, 200),\n              'n_jobs' : -1,\n              'metric':'rmse',\n              'max_bin':trial.suggest_int('max_bin', 300, 1000),\n              'cat_smooth':trial.suggest_int('cat_smooth', 5, 100),\n              'cat_l2':trial.suggest_loguniform('cat_l2', 1e-3, 100)}\n\n    model = LGBMRegressor(**params)\n                  \n    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=0)\n    \n\n    model.fit(X_train, y_train, eval_set=[(X_val, y_val)],\n              eval_metric=['rmse'],\n              early_stopping_rounds=250, \n              categorical_feature=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n              #callbacks=[optuna.integration.LightGBMPruningCallback(trial, metric='rmse')],\n              verbose=0)\n\n    train_score = np.round(np.sqrt(mean_squared_error(y_train, model.predict(X_train))), 5)\n    test_score = np.round(np.sqrt(mean_squared_error(y_val, model.predict(X_val))), 5)\n                  \n    print(f'TRAIN RMSE : {train_score} || TEST RMSE : {test_score}')\n                  \n    return test_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"optimize = partial(objective, X=X_train, y=y_train)\n\nstudy_lgbm = optuna.create_study(direction='minimize')\n#study_lgbm.optimize(optimize, n_trials=300)\n\n# i have commented out the trials so as to cut short the notebook execution time.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#From the above optuna trials the best parameters i could find were the following ones!\n\nlgbm_params = {'max_depth': 16, \n                'subsample': 0.8032697250789377, \n                'colsample_bytree': 0.21067140508531404, \n                'learning_rate': 0.009867383057779643,\n                'reg_lambda': 10.987474846877767, \n                'reg_alpha': 17.335285595031994, \n                'min_child_samples': 31, \n                'num_leaves': 66, \n                'max_bin': 522, \n                'cat_smooth': 81, \n                'cat_l2': 0.029690334194270022, \n                'metric': 'rmse', \n                'n_jobs': -1, \n                'n_estimators': 20000}","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"preds_list_base = []\npreds_list_final_iteration = []\npreds_list_all = []\n\nfor train_idx, val_idx in split.split(X_train):\n            X_tr = X_train.iloc[train_idx]\n            X_val = X_train.iloc[val_idx]\n            y_tr = y_train.iloc[train_idx]\n            y_val = y_train.iloc[val_idx]\n            \n            Model = LGBMRegressor(**lgbm_params).fit(X_tr, y_tr, eval_set=[(X_val, y_val)],\n                          eval_metric=['rmse'],\n                          early_stopping_rounds=250, \n                          categorical_feature=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n                          #callbacks=[optuna.integration.LightGBMPruningCallback(trial, metric='rmse')],\n                          verbose=0)\n            \n            preds_list_base.append(Model.predict(X_test))\n            preds_list_all.append(Model.predict(X_test))\n            print(f'RMSE for Base model is {np.sqrt(mean_squared_error(y_val, Model.predict(X_val)))}')\n            first_rmse = np.sqrt(mean_squared_error(y_val, Model.predict(X_val)))\n            params = lgbm_params.copy()\n            \n            for i in range(1, 8):\n                if i >2:    \n                    \n                    # reducing regularizing params if \n                    \n                    params['reg_lambda'] *= 0.9\n                    params['reg_alpha'] *= 0.9\n                    params['num_leaves'] += 40\n                    \n                params['learning_rate'] = 0.003\n                Model = LGBMRegressor(**params).fit(X_tr, y_tr, eval_set=[(X_val, y_val)],\n                          eval_metric=['rmse'],\n                          early_stopping_rounds=200, \n                          categorical_feature=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n                          #callbacks=[optuna.integration.LightGBMPruningCallback(trial, metric='rmse')],\n                          verbose=0,\n                          init_model=Model)\n                \n                preds_list_all.append(Model.predict(X_test))\n                print(f'RMSE for Incremental trial {i} model is {np.sqrt(mean_squared_error(y_val, Model.predict(X_val)))}')\n            last_rmse = np.sqrt(mean_squared_error(y_val, Model.predict(X_val)))\n            print('',end='\\n\\n')\n            print(f'Improvement of : {first_rmse - last_rmse}')\n            print('-' * 100)\n            preds_list_final_iteration.append(Model.predict(X_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Great!! we can see that we have observed some further improvement in all the folds. Lets point out few findings:\n\n* The first few iterations are just using very low learning_rate.. after the 2nd iteration we can see that there are iterations with very good improvement, observed by reducing regularization.\n\n\n* There are also iterations where loss increased at later iterations slightly compared to previous iteration, showing that we have reached the limit in few iterations before the max iteration.\n\n\n* If you try setting verbose=1, you will observe that these improvements are observed only in first few trees created... after that loss starts to increase, LGBM keeps the best model. But reducing regularization does improve loss for first few trees!!!!"},{"metadata":{},"cell_type":"markdown","source":"I have 3 different sets of predictions, one for only the base model and one for all the predictions done and last one for only final iteration.\n\n* `y_preds_base` : **0.84196 - 0.84199** (keeps jumping between these)\n\n\n* `y_preds_all` : **0.84195 - 0.84196**\n\n\n* `y_preds_final_iteration` : **0.84193**"},{"metadata":{"trusted":false},"cell_type":"code","source":"y_preds_base = np.array(preds_list_base).mean(axis=0)\ny_preds_base","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"y_preds_all = np.array(preds_list_all).mean(axis=0)\ny_preds_all","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"y_preds_final_iteration = np.array(preds_list_final_iteration).mean(axis=0)\ny_preds_final_iteration","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"submission = pd.DataFrame({'id':test.id,\n              'target':y_preds_final_iteration})","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"submission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"pd.read_csv('submission.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Finding the right regularization reducing factors using optuna"},{"metadata":{},"cell_type":"markdown","source":"you may even try reducing or increasing few params and find the best mix of factors using optuna, it may even be possible to improve results more than achieved above, an example of the technique is shown below... "},{"metadata":{"trusted":true},"cell_type":"code","source":"# creating a pre trained model to use in objective.\n\nX_tr, X_val, y_tr, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)\nlgbm = LGBMRegressor(**lgbm_params).fit(X_tr, y_tr, eval_set=[(X_val, y_val)],\n                          eval_metric=['rmse'],\n                          early_stopping_rounds=250, \n                          categorical_feature=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n                          verbose=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def objective(trial, model, X, y, iterations=5):\n\n    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=0)\n    f1 = trial.suggest_uniform('f1', 0.1, 1.0)\n    f2 = trial.suggest_uniform('f2', 0.1, 3)\n    f3 = trial.suggest_int('f3', 20, 100)\n    f4 = trial.suggest_int('f4', 20, 50)\n    f5 = trial.suggest_int('f5', 1, 5)\n    lr_factor = trial.suggest_uniform('lr_factor', 0.1, 0.7)\n    \n    \n    params = lgbm_params.copy()\n    print(f'RMSE for base model is {np.sqrt(mean_squared_error(y_val, Model.predict(X_val)))}')\n\n    for i in range(1, iterations):\n        if i > 2:\n            params['reg_lambda'] *=  f1\n            params['reg_alpha'] += f2\n            params['num_leaves'] += f3\n            params['min_child_samples'] -= f4\n            params['cat_smooth'] -= f5\n            params['learning_rate'] *= lr_factor\n            #params['max_depth'] += f5\n\n       \n        params['learning_rate'] = params['learning_rate'] if params['learning_rate'] > 0.0009 else 0.0009\n        # need to stop learning rate to reduce to a very insignificant value, hence we use this threshold\n        \n        Model = model(**params).fit(X_train, y_train, eval_set=[(X_val, y_val)],\n                          eval_metric=['rmse'],\n                          early_stopping_rounds=200, \n                          categorical_feature=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n                          verbose=1000,\n                          init_model=Model if i > 1 else lgbm)# we will use pre trained model for first iteration\n     \n        print(f'RMSE for {i}th model is {np.sqrt(mean_squared_error(y_val, Model.predict(X_val)))}')\n           \n              \n    RMSE = mean_squared_error(y_val, Model.predict(X_val), squared=False)\n    return RMSE\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"study = optuna.create_study(direction='minimize')\noptimize = partial(objective, X=X_train, y=y_train, model=LGBMRegressor)\n#study.optimize(optimize, n_trials=100)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Finally, i am still working on and experimenting why this actually works, few things i have found are:\n\n* using only iterations with some reduction in learning rate will not give you results as good as reducing regularization at each iteration, loss improvement plateaus pretty qucikly and starts worsening in after few iterations, reducing regularizatioj too forces some more loss improvement and helps get more iterations in.\n\n\n* Reducing regularization slowly forecefully improves loss at every next iteration until a bottleneck is reached, where this trick just does not work anymore, The reason for this can be maybe for the first few trees added at a new iteration with reduced regularization, the minute changes they bring to decision boundary even though overfit inducing should help generalize for just first few trees, after that overfitting starts to increase and loss shots up during each iteration, its great that lgbm iternally stores the best loss at each training!!\n\n**Although a small trick this work has been a hardwork of few days, so if you like the work and find it useful, show your support by upvoting!!** "},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}