{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"scrolled":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Hello there!**  \n  This is my not only first competition on Kaggle, but the first off-course assignment. My goal is to test some methods, that I'v learned so far, and also to try data visualization techniques.  \n  So I would be very glad to get some feedback "},{"metadata":{},"cell_type":"markdown","source":"# Importing Libraries"},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"# Basics\nimport numpy as np \nimport pandas as pd\n\n# Plotting\nimport matplotlib.pyplot as plt\nimport matplotlib.cm as cm\nimport squarify\nimport seaborn as sns\n\n# Model stuff\nfrom sklearn.feature_extraction import DictVectorizer as DV\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import model_selection\nfrom sklearn.metrics import accuracy_score, mean_squared_error\n\n# Appearence\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom colorama import Fore","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Task Detail \n\n## Goal\nFor this competition, you will be predicting a **continuous target** based on a number of feature columns given in the data. All of the feature columns, **cat0 - cat9 are categorical**, and the feature columns **cont0 - cont13 are continuous**.  \n  \nThe dataset is used for this competition is synthetic, but based on a real dataset and generated using a CTGAN. The original dataset deals with predicting the amount of an insurance claim. Although the features are **anonymized**, they have properties relating to real-world features.  \n  \n  ## Metric\nSubmissions are scored on the root mean squared error. RMSE is defined as:\n$$\\text{RMSE} = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2}$$\n\nwhere  is the predicted value,  is the original value, and  is the number of rows in the test data."},{"metadata":{},"cell_type":"markdown","source":"# First Look On The Data  \n## Train.csv"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_csv = pd.read_csv('../input/tabular-playground-series-feb-2021/train.csv')\nprint(train_csv.info(verbose = True))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"train_csv.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Test.csv"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_csv = pd.read_csv('../input/tabular-playground-series-feb-2021/test.csv')\nprint(test_csv.info(verbose = True))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we see there is no null data and all continuous features are normalized"},{"metadata":{},"cell_type":"markdown","source":"# Data Visualization üìä"},{"metadata":{},"cell_type":"markdown","source":"## Target  \nWe can try to visualize target data with boxplot. That would show us such characteristics as percentiles, min (max) values and the set of extreme values "},{"metadata":{"trusted":true},"cell_type":"code","source":"target_data = train_csv['target']\n\nplt.figure(figsize=(9,9))\nplt.title('Target Boxplot', size = 14)\nsns.boxplot(data = target_data, color = 'royalblue')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see, there are some extreme points in our dataset. These points may correspond to reality, in the case of a *large number of them*.  \nBut if there are only few of them, it would be better to **get rid of them**, because the submissions are scored on the root mean squared error, which is very sensitive to outliers in the data"},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"the max value = 10.3, but 25% quartile is ~6.8 so we have a few count of outliers in our data\"\"\"\ntarget_data.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Lets count how many outliers are there**"},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"lower boundary\"\"\"\nlower_whisker = plt.boxplot(target_data)['whiskers'][0].get_ydata()[1]\nprint(sum(1 for i in target_data if i <= (lower_whisker - 0.1)), 'value(s) is(are) below lower whisker') # 0.1 is the \"tolerance\" in case the test data will be shifted\n\n\"\"\"higher boundary\"\"\"\nhigher_whisker = plt.boxplot(target_data)['whiskers'][1].get_ydata()[1]\nprint(sum(1 for i in target_data if i >= (higher_whisker + 0.1)), 'value(s) is(are) above the upper whisker')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**258** values are only **0.086%** of all data, so we can delete these rows in our DataFrame"},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"creating list with indexes\"\"\"\nindex_list = []\nfor value in target_data:\n    if value <= (lower_whisker - 0.1) or value >= (higher_whisker + 0.1):\n        index_list.append(list(target_data).index(value))\n\n\"\"\"rows deletion\"\"\"\ntrain_csv.drop(index_list, axis=0, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(9,9))\nplt.title('Target Boxplot', size = 14)\nsns.boxplot(data = train_csv['target'], color = 'royalblue')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Categorial  \nLets create a treemap of our categorial data"},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"def treemap(DataFrame = train_csv):\n    f, axes = plt.subplots(nrows = 3, ncols= 3, figsize = (24,16))\n    \n    for i in range(train_csv.select_dtypes(include='object').shape[1] - 1):\n        \n        # Set labels & size for treemap\n        df = DataFrame.groupby('cat{}'.format(i)).size().reset_index(name = 'counts')\n        labels = df.apply( lambda x: str(x[0]) + '\\n (' + str(x[1]) + ')', axis = 1)\n        size = df['counts'].values\n        \n        \n        colors = [plt.cm.coolwarm(i/float(len(labels))) for i in range(len(labels))]\n        squarify.plot(sizes=size, label=labels, color = colors, alpha = 0.8, ax = axes[i//3, i%3])\n        \n        # Decorate\n        axes[i//3, i%3].set_title('Treemap of Cat{}'.format(i))\n        axes[i//3, i%3].axis('off')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"treemap()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.catplot(x=\"cat6\", kind=\"count\", palette=\"coolwarm\", data=train_csv)\nplt.title('cat6 in train', size = 13.5)\n\nsns.catplot(x=\"cat6\", kind=\"count\", palette=\"coolwarm\", data=test_csv)\nplt.title('cat6 in test', size = 13.5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see, there is no much complexity in categorial features. We have  \n* 5 columns, where one value is dominating others\n* 3 columns, where two values are dominating others  \n* 1 column with with a wide variability  \n  \n**Another moment that we need to notice is that the** `number of cat features in test != number of cat features in train` "},{"metadata":{},"cell_type":"markdown","source":"## Continuous"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Select cont data only\ncont_data = train_csv.select_dtypes(include =['float64', 'int64'])\n\nnum_rows, num_cols = 4, 4\nf, axes = plt.subplots(nrows = num_rows, ncols= num_cols, figsize = (24,16))\n\nfor i, col_name in enumerate(cont_data):\n    \n    sns.kdeplot(cont_data[col_name], fill=True, color = 'royalblue',\n                   alpha=.5, linewidth=0, ax = axes[i // num_rows, i % num_cols])\n\nf.delaxes(axes[0, 0])\nf.delaxes(axes[3, 3])\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Correlation between features"},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot heatmap\nplt.figure(figsize=(15,15))\nmask = np.triu(cont_data.corr())\nsns.heatmap(cont_data.corr(), annot=True, mask = mask, fmt=\".2f\", cmap='coolwarm',\n            cbar_kws={\"shrink\": .8}, vmin=0, vmax=1)\n# yticks\nplt.yticks(rotation=0)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see, there is no high correlation between variables"},{"metadata":{},"cell_type":"markdown","source":"# Feature Modify üß∞"},{"metadata":{"trusted":true},"cell_type":"code","source":"def modify_df(df):\n    df['cat4'] = df['cat4'].apply(lambda x: x if x == 'B' else 'Z')\n    df['cat5'] = df['cat5'].apply(lambda x: x if x in ['B', 'D'] else 'Z')\n    df['cat6'] = df['cat6'].apply(lambda x: x if x == 'A' else 'Z')\n    df['cat7'] = df['cat7'].apply(lambda x: x if x in ['E', 'D'] else 'Z')\n    df['cat8'] = df['cat8'].apply(lambda x: x if x in ['E', 'C', 'G', 'A'] else 'Z')\n    \n    df['cont001'] = df['cont8'] * df['cont0']\n    df['cont002'] = df['cont9'] * df['cont0']\n    df['cont003'] = df['cont9'] * df['cont5']\n    df['cont004'] = df['cont8'] * df['cont5']\n    df['cont005'] = df['cont2'] * df['cont4']\n    df['cont006'] = df['cont1'] * df['cont3']\n    df['cont007'] = df['cont13'] * df['cont1']\n\n    return df\n\nmod_train_csv = modify_df(train_csv.copy())\nmod_test_csv = modify_df(test_csv.copy())\n\ntreemap(DataFrame = mod_train_csv)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Train"},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_cols = mod_train_csv.drop(['id', 'target'], axis=1).columns\n\nX_cat = mod_train_csv[feature_cols].select_dtypes(include = 'object')\nX_cont = mod_train_csv[feature_cols].select_dtypes(exclude = 'object')\ny = mod_train_csv['target']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Test"},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_cols = mod_test_csv.drop(['id'], axis=1).columns\n\nX_cat_TEST = mod_test_csv[feature_cols].select_dtypes(include = 'object')\nX_cont_TEST = mod_test_csv[feature_cols].select_dtypes(exclude = 'object')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# One-Hot-Encoding üìü"},{"metadata":{},"cell_type":"markdown","source":"## Train"},{"metadata":{"trusted":true},"cell_type":"code","source":"encoder = DV(sparse = False)\nX_cat_oh = encoder.fit_transform(X_cat.T.to_dict().values())\n\n\"\"\"merging data\"\"\"\nX = np.hstack((X_cont, X_cat_oh))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Test"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_cat_oh_TEST = encoder.fit_transform(X_cat_TEST.T.to_dict().values())\n\n\"\"\"merging data\"\"\"\nX_TEST = np.hstack((X_cont_TEST, X_cat_oh_TEST))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Train Test Split ü™ì\nFor checking our models we have to split our data "},{"metadata":{"trusted":true},"cell_type":"code","source":"(X_train, \n X_test, \n y_train, y_test) = train_test_split(X, y, \n                                     test_size=0.2, \n                                     random_state=0,\n                                     )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Linear Model üìâ"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import library\nfrom sklearn import linear_model\n\n# Model set\nestimator1 = linear_model.SGDRegressor(random_state = 42)\n\n\"\"\"Grid Search\"\"\"\n\nparameters_grid = {\n    'penalty' : ['l1', 'l2', 'elasticnet'],\n    'alpha' : np.linspace(0.0001, 0.001, num = 5),\n}\n\ncv = model_selection.ShuffleSplit(n_splits = 5, test_size = 0.2, random_state = 42)\n\ngrid_cv_LM = model_selection.GridSearchCV(estimator1, parameters_grid, scoring = 'neg_root_mean_squared_error', cv = cv)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\ngrid_cv_LM.fit(X_train, y_train)\n\npredictions_LM = grid_cv_LM.best_estimator_.predict(X_test)\n\nscore_rmse_LM = (mean_squared_error(y_test, predictions_LM))**0.5\nprint(Fore.GREEN + 'Base Linear SGDRegressor RMSE: {}'.format(score_rmse_LM))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Random Forest üå≥"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import library\nfrom sklearn.ensemble import RandomForestRegressor\n\nestimator_RF = RandomForestRegressor(n_jobs=-1, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nestimator_RF.fit(X_train, y_train)\n\n# Test\npredictions_RF = estimator_RF.predict(X_test)\n\nscore_rmse_RF = (mean_squared_error(y_test, predictions_RF))**0.5\nprint(Fore.GREEN + 'RandomForest RMSE: {}'.format(score_rmse_RF))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# XGBoost üèÉüèª"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nfrom xgboost import XGBRegressor\nXGB_default = XGBRegressor(random_state=42, tree_method='gpu_hist');\n\nXGB_default.fit(X_train, y_train);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions_XGB = XGB_default.predict(X_test)\n\nscore_rmse_XGB = (mean_squared_error(y_test, predictions_XGB))**0.5\nprint(Fore.GREEN + 'Base XGBoost RMSE: {}'.format(score_rmse_XGB))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# XGBoost Parameter Tuning üöµüèª"},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"best params\"\"\"\n\nxgb_params = {\n    'booster':'gbtree',\n    'n_estimators':10000,\n    'max_depth':7, \n    'eta':0.01,\n    'gamma':1.8,\n    'objective':'reg:squarederror',\n    'verbosity':0,\n    'subsample':0.85,\n    'colsample_bytree':0.4,\n    'lambda':2.7,\n    'alpha':6,\n    'scale_pos_weight':1,\n    'objective':'reg:squarederror',\n    'eval_metric':'rmse',\n    'seed': 42,\n    'tree_method':'gpu_hist',\n    'gpu_id':0\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\nXGB_tune = XGBRegressor(**xgb_params);\nXGB_tune.fit(X_train, y_train);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"predict\"\"\"\n\npredictions_XGB = XGB_tune.predict(X_test)\n\nscore_rmse_XGB = (mean_squared_error(y_test, predictions_XGB))**0.5\nprint(Fore.GREEN + 'Tune XGB RMSE: {}'.format(score_rmse_XGB))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# kNN üëØ‚Äç"},{"metadata":{"trusted":true},"cell_type":"code","source":"# import library\nfrom sklearn.neighbors import KNeighborsRegressor\n\nestimator_kNN = KNeighborsRegressor()\nestimator_kNN.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"predict\"\"\"\n\npredictions_kNN = estimator_kNN.predict(X_test)\n\nscore_rmse_kNN = (mean_squared_error(y_test, predictions_kNN))**0.5\nprint(Fore.GREEN + 'Base kNN RMSE: {}'.format(score_rmse_kNN))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# LGMB\nwanna try LGBM model with optimize params that I found on https://www.kaggle.com/andreshg/tps-feb-a-complete-study#5.-Optimized-LGBM-CrossValidated-%F0%9F%A7%AE"},{"metadata":{"trusted":true},"cell_type":"code","source":"from lightgbm import LGBMRegressor\nimport lightgbm as lgb\n\nbest_params = {\n    'reg_lambda': 0.015979956459638782,\n    'reg_alpha': 9.103977313355028,\n    'colsample_bytree': 0.3,\n    'subsample': 1.0,\n    'learning_rate': 0.009,\n    'n_estimators': 3000,\n    'max_depth': 15,\n    'min_child_samples': 142,\n    'num_leaves': 84,\n    'random_state': 42, \n    'device': 'gpu',\n}\n\n# Instantiate model with 100 decision trees\nestimator_LGBM = LGBMRegressor(**best_params)\n\nestimator_LGBM.fit(X_train, y_train)\n\n# Use the forest's predict method on the test data\npredictions_LGBM = estimator_LGBM.predict(X_test)\n\nscore_rmse_LGBM = (mean_squared_error(y_test, predictions_LGBM))**0.5\nprint(Fore.GREEN + 'Tuned LGBM RMSE: {}'.format(score_rmse_LGBM))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results = {'score': [score_rmse_LM, score_rmse_RF, score_rmse_XGB, score_rmse_kNN, score_rmse_LGBM], 'model': ['Linear', 'RF', 'XGB', 'kNN', 'LGBM']}\nprint(Fore.WHITE + 'Results:\\n')\nfor i in range(5):\n    print(Fore.GREEN + list(results.values())[1][i], '\\t', list(results.values())[0][i], '\\n')\n    \nprint(Fore.WHITE + 'LGBM is the best model')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Train on full data & Submit üî©"},{"metadata":{"trusted":true},"cell_type":"code","source":"estimator_LGBM.fit(X, y)\n\npredictions = estimator_LGBM.predict(X_TEST)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"XGB_tune.fit(X, y)\n\npredictions2 = XGB_tune.predict(X_TEST)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Prepare Submission File üìù"},{"metadata":{"trusted":true},"cell_type":"code","source":"my_submission = pd.DataFrame({'id': test_csv.id, 'target': predictions})\n\nmy_submission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"my_submission2 = pd.DataFrame({'id': test_csv.id, 'target': predictions2})\n\nmy_submission2.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Another way to Feature Modify"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_csv['cat6'] = train_csv['cat6'].apply(lambda x: x if x in ['A', 'B', 'C', 'D', 'E', 'I', 'H']  else 'A')\nsns.catplot(x=\"cat6\", kind=\"count\", palette=\"coolwarm\", data=train_csv)\nplt.title('cat6 in train', size = 13.5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Train"},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_cols = train_csv.drop(['id', 'target'], axis=1).columns\n\nX_cat = train_csv[feature_cols].select_dtypes(include = 'object')\nX_cont = train_csv[feature_cols].select_dtypes(exclude = 'object')\ny = train_csv['target']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Test"},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_cols = test_csv.drop(['id'], axis=1).columns\n\nX_cat_TEST = test_csv[feature_cols].select_dtypes(include = 'object')\nX_cont_TEST = test_csv[feature_cols].select_dtypes(exclude = 'object')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Train O-H-E"},{"metadata":{"trusted":true},"cell_type":"code","source":"encoder = DV(sparse = False)\nX_cat_oh = encoder.fit_transform(X_cat.T.to_dict().values())\n\n\"\"\"merging data\"\"\"\nX = np.hstack((X_cont, X_cat_oh))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Test O-H-E"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_cat_oh_TEST = encoder.fit_transform(X_cat_TEST.T.to_dict().values())\n\n\"\"\"merging data\"\"\"\nX_TEST = np.hstack((X_cont_TEST, X_cat_oh_TEST))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Train test split"},{"metadata":{"trusted":true},"cell_type":"code","source":"(X_train, \n X_test, \n y_train, y_test) = train_test_split(X, y, \n                                     test_size=0.2, \n                                     random_state=0,\n                                     )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## XGB"},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"best params\"\"\"\n\nxgb_params = {\n    'booster':'gbtree',\n    'n_estimators':10000,\n    'max_depth':7, \n    'eta':0.01,\n    'gamma':1.8,\n    'objective':'reg:squarederror',\n    'verbosity':0,\n    'subsample':0.85,\n    'colsample_bytree':0.4,\n    'lambda':2.7,\n    'alpha':6,\n    'scale_pos_weight':1,\n    'objective':'reg:squarederror',\n    'eval_metric':'rmse',\n    'seed': 42,\n    'tree_method':'gpu_hist',\n    'gpu_id':0\n}\n\nXGB_tune1 = XGBRegressor(**xgb_params);\nXGB_tune1.fit(X_train, y_train);\n\n\"\"\"predict\"\"\"\n\npredictions_XGB1 = XGB_tune1.predict(X_test)\n\nscore_rmse_XGB1 = (mean_squared_error(y_test, predictions_XGB1))**0.5\nprint(Fore.GREEN + 'Tune XGB RMSE: {}'.format(score_rmse_XGB1))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## LGMB"},{"metadata":{"trusted":true},"cell_type":"code","source":"best_params = {\n    'reg_lambda': 0.015979956459638782,\n    'reg_alpha': 9.103977313355028,\n    'colsample_bytree': 0.3,\n    'subsample': 1.0,\n    'learning_rate': 0.009,\n    'n_estimators': 3000,\n    'max_depth': 15,\n    'min_child_samples': 142,\n    'num_leaves': 84,\n    'random_state': 42, \n    'device': 'gpu',\n}\n\n# Instantiate model with 100 decision trees\nestimator_LGBM1 = LGBMRegressor(**best_params)\n\nestimator_LGBM1.fit(X_train, y_train)\n\n# Use the forest's predict method on the test data\npredictions_LGBM1 = estimator_LGBM1.predict(X_test)\n\nscore_rmse_LGBM1 = (mean_squared_error(y_test, predictions_LGBM1))**0.5\nprint(Fore.GREEN + 'Tuned LGBM RMSE: {}'.format(score_rmse_LGBM1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"estimator_LGBM1.fit(X, y)\n\npredictions3 = estimator_LGBM1.predict(X_TEST)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"my_submission3 = pd.DataFrame({'id': test_csv.id, 'target': predictions3})\n\nmy_submission3.to_csv('submission3.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"my_submission3","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"estimator_LGBM1.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions4 = estimator_LGBM1.predict(X_TEST)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"my_submission4 = pd.DataFrame({'id': test_csv.id, 'target': predictions4})\n\nmy_submission4.to_csv('submission4.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}