{"cells":[{"metadata":{},"cell_type":"markdown","source":"##  Hi again, my beautiful kagglers.\n\nLet's jump into the second playground competition of the year 2021.  \nAs from today, I decided to share my monthly work with you. \nPlease, don't hesitate to comment and upvote.  \n\n---\n\nLONG TIME AGO: basic and advanced feature engineering, catboost and lgbm models, xgboost   \n\nIN THIS VERSION:  xgb and lightgbm predictors, feature transformation. LGBM and catboost fit process are disabled to reduce a computation time. Shap is also disabled.  \n\nUP NEXT: more complex models, shap.\n\n---"},{"metadata":{},"cell_type":"markdown","source":"LGBM prediction `sub_305` is made by my other [kernel](https://www.kaggle.com/kirillklyukvin/tps-feb-lgbm) based on beautiful [work](https://www.kaggle.com/maunish/lgbm-goes-brrr) by Mister [Maunish](https://www.kaggle.com/maunish).  \n\nSome ideas I took from [this notebook](https://www.kaggle.com/tunguz/ensembling-starter-tps-feb-2021) by Mister [Bojan](https://www.kaggle.com/tunguz).   \n\nFriends, let's show our appreciation and upvote their notebooks too!\n\n---"},{"metadata":{},"cell_type":"markdown","source":"## Data and libs"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport re\nimport sys, gc, os\nfrom IPython.display import display\n\nfrom scipy import stats\n\nimport shap\nshap.initjs()\nimport featuretools as ft\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style('whitegrid')\n\nfrom tqdm import tqdm\nfrom sklearn.model_selection import KFold\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.svm import LinearSVR\nfrom sklearn.model_selection import train_test_split, KFold\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import MinMaxScaler, Normalizer, MaxAbsScaler\nfrom sklearn.preprocessing import (StandardScaler, PowerTransformer, QuantileTransformer ,LabelEncoder, \n                                   OneHotEncoder, OrdinalEncoder)\n\nimport catboost as cb\nimport lightgbm as lgb\nfrom xgboost import XGBRegressor","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train = pd.read_csv('../input/tabular-playground-series-feb-2021/train.csv')\ndf_test = pd.read_csv('../input/tabular-playground-series-feb-2021/test.csv')\ndf_sub = pd.read_csv('../input/tabular-playground-series-feb-2021/sample_submission.csv')\n\nSEED = 1991\n\n\n### My and another lovely kagglers latest submissions with the best score  \n\nsub_295 = pd.read_csv('../input/tp-f-my-subs/submission_001.csv')\nsub_202 = pd.read_csv('../input/feb84202/FEB84202.csv')\nsub_222 = pd.read_csv('../input/tp-f-my-subs/submission_021.csv')\nsub_216 = pd.read_csv('../input/tp-f-my-subs/submission_216.csv')\nsub_257 = pd.read_csv('../input/tp-f-my-subs/submission_257.csv')\nsub_279 = pd.read_csv('../input/tp-f-my-subs/submission_012.csv')\nsub_305 = pd.read_csv('../input/tp-f-my-subs/submission_lgb_305.csv')\nsub_212 = pd.read_csv('../input/tp-f-my-subs/submission_037.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"## EDA"},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"def simple_eda(df):\n    \n    \"\"\"\n    This function helps us with simple data analysis.\n    We may explore the common information about the dataset, missing values, features distribution and duplicated rows\n    \"\"\"\n    \n    # applying info() method\n    print('---')\n    print('Common information')\n    print('---')\n    print()\n    print(df.info())\n    \n    # missing values\n    print()\n    print('---')\n    if df.isna().sum().sum() == 0:\n        print('There are no missing values')\n        print('---')\n    else:\n        print('Detected')\n        display(df.isna().sum())\n    \n    \n    # applying describe() method for categorical features\n    print()\n    print('---')\n    print('Categorical columns')\n    print('Total {}'.format(len(df.select_dtypes(include='object').columns)))\n    print('---')\n    display(df.describe(include = 'object'))\n    \n    # same describe() but for continious features\n    print('---')\n    print('Continuous columns')\n    print('Total {}'.format(len(df.select_dtypes(include=['int', 'float']).columns)))\n    print('---')\n    display(df.describe())\n    \n    #checking for duplicated rows\n    if df.duplicated().sum() == 0:\n        print('---')\n        print('There are no duplicates')\n        print('---')\n    else:\n        print('---')\n        print('Duplicates found')\n        print('---')\n        display(df[df.duplicated()])\n    \n    print()\n    print('---')\n    print('End of the report')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":false,"_kg_hide-input":false},"cell_type":"code","source":"simple_eda(df_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"simple_eda(df_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Unlike the first tabular competition, here we have to deal with both categorical and continuous features.  \n\nWe have pretty similar features distribution (by the first look) on both datasets. No missing values and duplicates are spotted.  \n\nA few categorical columns have a huge class disbalance. For example, 'cat0' contains approximately 93% of values 'A'. I suppose such columns are non-informative.\n\n---"},{"metadata":{},"cell_type":"markdown","source":"### Exploring categorical features \n\nBefore we start, let's drop the target columns."},{"metadata":{"trusted":true},"cell_type":"code","source":"target = df_train['target']\ndf_train.drop('target', axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"CAT= df_train.select_dtypes(include='object').columns.tolist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"idx = 0\nf, axes = plt.subplots(5, 2, sharex=True, figsize=(12,14))\nplt.suptitle('Categorical features distribution', size=16, y=(0.94))\n\nfor row in range(5):\n    for col in range(2):\n        data = df_train[CAT[idx]].value_counts()\n        sns.barplot(x = data.values, y = data.index, palette='deep', ax=axes[row, col])\n        axes[row,col].set_title(CAT[idx])\n        idx += 1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Seems like the next columns are non-informative: [cat0, cat2, cat4, cat6, cat7]. \nFor the first time we will use all default columns for prediction. Later we try to remove \"unimodal' ones.  \n\n---"},{"metadata":{},"cell_type":"markdown","source":"### Exploring continuous features "},{"metadata":{"trusted":true},"cell_type":"code","source":"NUM = df_train.select_dtypes('float64').columns.tolist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(16,5))\nsns.violinplot(data=df_train[NUM], color='slategray')\nplt.title('Continuous features distribution');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Not so bad. There are no columns with a huge amount of outliers. Four features have a highly skewed distribution [cont3, cont5, cont7 and cont1].  \n\nNext step - check the correlations."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,10))\nsns.heatmap(df_train[NUM].join(target).corr(), square=True, linewidths=0.7, cmap=\"bone_r\");","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have a few multicollinear columns. As I said previously, in this version I try to use these features by default. Next, we will try to use some more complicated algorithms.  \n\n---"},{"metadata":{},"cell_type":"markdown","source":"### Exploring target distribution"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,5))\nsns.histplot(target, color='slategray', stat='frequency');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have almost the same target bimodal distribution as in January playground competition.  \nAlso let's see how many values bellow mark 4 do we have here."},{"metadata":{"trusted":true},"cell_type":"code","source":"len(target[target <= 4])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Just 34 \"outliers\". Most likely we may drop them without a huge impact on a final score."},{"metadata":{"trusted":true},"cell_type":"code","source":"to_drop = target[target <= 4].index\ntarget.drop(to_drop, inplace=True)\ndf_train.drop(to_drop, inplace=True)\n\nplt.figure(figsize=(10,5))\nsns.histplot(target, color='slategray', stat='frequency');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Much better."},{"metadata":{},"cell_type":"markdown","source":"## Machine learning\n\nAt this version we will use almost raw datasets with a touch of basic preprocessing.\n\n---\n\n### Categorical features encoding"},{"metadata":{"trusted":true},"cell_type":"code","source":"CAT_01 = list(set(CAT).difference(set(['cat6'])))\nCAT_01","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Simple one-hot encoding"},{"metadata":{"trusted":true},"cell_type":"code","source":"#dummies_train = pd.get_dummies(df_train[CAT_01])\n#dummies_test = pd.get_dummies(df_test[CAT_01])\n\n#train = df_train[NUM].join(dummies_train)\n#test = df_test[NUM].join(dummies_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Feature 'cat6' has a label 'G' in training set, which is missed in test set. So after simply dummy transformation we have to drop 'cat6_G' feature from the updated train set."},{"metadata":{"trusted":true},"cell_type":"code","source":"#set(train.columns.tolist()).difference(set(test.columns.tolist()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#train.drop('cat6_G', axis=1, inplace=True)\n#train.shape[1], test.shape[1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"#### Label encoding"},{"metadata":{"trusted":true},"cell_type":"code","source":"#for feature in CAT_01:\n#    le = LabelEncoder()\n#    le.fit(df_train[feature])\n#    df_train[feature] = le.transform(df_train[feature])\n#    df_test[feature] = le.transform(df_test[feature])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = df_train.copy()\ntest = df_test.copy()\n\ntrain.drop(['id'], axis=1, inplace=True)\ntest.drop(['id'], axis=1, inplace=True)\n\ntrain.shape, test.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Feature engineering \n\n#### Scaling and encoding"},{"metadata":{"trusted":true},"cell_type":"code","source":"cols = train.columns.tolist()\n\nct = ColumnTransformer([('onehot',OrdinalEncoder(), slice(len(CAT))),\n                        ('quantile',QuantileTransformer(random_state=SEED, n_quantiles=1500),\n                         slice(len(CAT),len(CAT) + len(NUM) + 5))])\n\ntrain = ct.fit_transform(train)\ntest = ct.transform(test)\n\ntrain = pd.DataFrame(train, columns = cols)\ntest = pd.DataFrame(test, columns = cols)\n\ntrain[CAT] = train[CAT] / 10\ntest[CAT] = test[CAT] / 10","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Manual feature egineering"},{"metadata":{"trusted":true},"cell_type":"code","source":"def feat_eng(df):\n    \n    # Manually multiply and drop specific columns\n    \n    #df['cont_003'] = df['cont0'] * df['cont8']\n\n    df['cont001'] = df['cont8'] * df['cont0']\n    df['cont002'] = df['cont9'] * df['cont0']\n    df['cont003'] = df['cont9'] * df['cont5']\n    df['cont004'] = df['cont8'] * df['cont5']\n    df['cont005'] = df['cont2'] * df['cont4']\n    df['cont006'] = df['cont1'] * df['cont3']\n    df['cont007'] = df['cont13'] * df['cont1']\n    \n    #df['cat005'] = df['cat2'] * df['cat1']\n   # df['cat006'] = df['cat2'] * df['cat4']\n    \n    #df.drop('cont5', axis=1, inplace=True)\n    #df.drop('cont9', axis=1, inplace=True)\n    \n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = feat_eng(train)\ntest = feat_eng(test)\n\n#train.drop('id', axis=1, inplace=True)\n#test.drop('id', axis=1, inplace=True)\n\ntrain.shape, test.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### FeatureTools"},{"metadata":{"trusted":true},"cell_type":"code","source":"#to_transform = ['cont0', 'cont1', 'cont4', 'cont5', 'cont8', 'cont9', 'cont12'] ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def feat_eng_01(df):\n    \n    es = ft.EntitySet(id = 'data')\n\n    original_cols = to_transform\n\n    es = es.entity_from_dataframe(entity_id = 'data', \n                              dataframe = df[original_cols], \n                              index = 'id', \n                              time_index = None)\n    \n    new_features, new_feature_names = ft.dfs(entityset = es, target_entity = 'data', \n                                 trans_primitives = ['multiply_numeric'])\n    \n    new_features.reset_index(drop=True, inplace=True)\n    new_features.drop(original_cols, axis=1, inplace=True)\n    \n    return new_features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#train_fe = feat_eng_01(train)\n#test_fe = feat_eng_01(test)\n\n#train_fe.index = train.index\n#test_fe.index = test.index\n\n#train = train.join(train_fe)\n#test = test.join(test_fe)\n\n#train.shape, test.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Feature selection"},{"metadata":{"trusted":true},"cell_type":"code","source":"#X_train, X_valid, y_train, y_valid = train_test_split(train, target, test_size=0.1, random_state=SEED)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### FitPredict  \n\n**Catboost with kFold**"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"catboost_params = { \n            'iterations':10000,\n            'learning_rate':0.004,\n            'depth':9,\n            'num_leaves':111,\n            'random_strength':3,\n            'min_data_in_leaf':10,\n            'l2_leaf_reg':5.2,\n            'loss_function':'RMSE',\n            'random_seed':SEED,\n            'eval_metric':'RMSE',\n            'grow_policy':'Depthwise',\n            'max_bin':512,\n            'task_type': 'GPU',\n            'od_type':'Iter',\n            'od_wait':50,\n            'metric_period':50\n            }"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"N_FOLDS = 5\n\nkf = KFold(n_splits = N_FOLDS)\noof = np.zeros(len(target))\n\npreds_folds = np.zeros(len(test))\n\nfor train_ind, test_ind in tqdm(kf.split(train)):\n    X_train = train.iloc[train_ind]\n    X_val = train.iloc[test_ind]\n    y_train = target.iloc[train_ind]\n    y_val = target.iloc[test_ind]\n\n    model = cb.CatBoostRegressor(**catboost_params)\n    \n    train_pool = cb.Pool(X_train, y_train)\n    val_pool = cb.Pool(X_val, y_val)\n    \n    model.fit(X_train, y=y_train, \n              eval_set = (X_val, y_val),  \n              early_stopping_rounds=100, \n              verbose_eval=500, \n              use_best_model=True, \n              cat_features=train[CAT].columns.tolist(),\n              plot=False)\n    \n    p = model.predict(X_val)\n    oof[test_ind] = p\n    preds_folds += model.predict(test)/N_FOLDS\n    \n    #print(np.round(mean_squared_error(y_val, p, squared=False),5))\n        \nprint(f'mean square error on training data: {np.round(mean_squared_error(target, oof, squared=False),5)}')"},{"metadata":{},"cell_type":"markdown","source":"0.84351\n\ncb best RMSE on oof 0.8427"},{"metadata":{},"cell_type":"markdown","source":"**LGBm with kfold**"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"lgb_params_kfold = {\n \n    'n_estimators':6000,\n    'learning_rate': 0.004,\n    'min_child_samples':285,\n    'boosting_type': 'gbdt',\n    'num_leaves': 256,\n    'max_depth': -1,\n    'lambda_l1': 4.5,\n    'lambda_l2': 1.2,\n    'subsample':0.8,\n    #'categorical_feature':CAT_FEAT,\n    \n    'device_type':'gpu',\n    'metric': 'rmse',\n    'cat_smooth': 39,\n    'silent': True,\n    'importance_type': 'split',\n    'feature_pre_filter': False,\n    'bagging_fraction': 0.85,\n    'min_data_in_leaf': 100,\n    #'min_data_per_group': 5,\n    #'min_sum_hessian_in_leaf': 0.01,\n    #'bagging_freq': 5,\n    #'feature_fraction': 0.7,\n    #'min_child_samples': 20,\n    \n    'n_jobs': -1,\n    'random_state': SEED}"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"lgb_params_kfold = {\n     'reg_alpha': 6.147694913504962,\n     'reg_lambda': 0.002457826062076097,\n     'colsample_bytree': 0.3,\n     'subsample': 0.8,\n     'learning_rate': 0.001,\n     'max_depth': 20,\n     'num_leaves': 111,\n     'min_child_samples': 285,\n     'random_state':SEED,\n     'verbose':-1,\n     'n_estimators': 30000,\n     'metric': 'rmse',\n     'cat_smooth': 39\n}"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"N_FOLDS = 5\n\nkf = KFold(n_splits = N_FOLDS)\noof = np.zeros(len(target))\n\npreds_folds_lgb = np.zeros(len(test))\n\n\nfor train_ind, test_ind in tqdm(kf.split(train)):\n    X_train = train.iloc[train_ind]\n    X_val = train.iloc[test_ind]\n    y_train = target.iloc[train_ind]\n    y_val = target.iloc[test_ind]\n\n    model = lgb.LGBMRegressor(**lgb_params_kfold)\n    \n    model.fit(X_train, y_train, eval_set = ((X_val, y_val)), early_stopping_rounds = 200, verbose = 1000)\n    p = model.predict(X_val)\n    oof[test_ind] = p\n\n    preds_folds_lgb += model.predict(test)/N_FOLDS\n        \nprint(f'mean square error on training data: {np.round(mean_squared_error(target, oof, squared=False),5)}')"},{"metadata":{},"cell_type":"markdown","source":"best attempt - RMSE on training data: 0.84215"},{"metadata":{},"cell_type":"markdown","source":"**xgb**"},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb_params = {\n    'booster':'gbtree',\n    'n_estimators':20000,\n    'max_depth':5, \n    'eta':0.008,\n    'gamma':3.5,\n    'objective':'reg:squarederror',\n    'verbosity':0,\n    'subsample':0.75,\n    'colsample_bytree':0.35,\n    'reg_lambda':0.23,\n    'reg_alpha':0.52,\n    'scale_pos_weight':1,\n    'objective':'reg:squarederror',\n    'eval_metric':'rmse',\n    #'seed':SEED,\n    'tree_method':'gpu_hist',\n    'gpu_id':0\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"N_FOLDS = 10\n\nkf = KFold(n_splits = N_FOLDS)\noof = np.zeros(len(target))\n\npreds_folds_xgb = np.zeros(len(test))\n\n\nfor train_ind, test_ind in tqdm(kf.split(train)):\n    X_train = train.iloc[train_ind]\n    X_val = train.iloc[test_ind]\n    y_train = target.iloc[train_ind]\n    y_val = target.iloc[test_ind]\n\n    model = XGBRegressor(**xgb_params)\n    \n    model.fit(X_train, y_train, eval_set = [(X_val, y_val)], early_stopping_rounds = 50, verbose = False)\n    p = model.predict(X_val)\n    oof[test_ind] = p\n\n    preds_folds_xgb += model.predict(test)/N_FOLDS","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f'rmse on oof: {np.round(mean_squared_error(target, oof, squared=False),5)}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"best rmse:  \n0.84252  \n0.84194"},{"metadata":{},"cell_type":"markdown","source":"Using shape to get some info from XGB model and training features."},{"metadata":{"trusted":true},"cell_type":"code","source":"#shap_values = shap.TreeExplainer(model).shap_values(X_train)\n#shap.summary_plot(shap_values, X_train)\n#shap_interactions = model.predict(test)\n#shap_preds = model.predict(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_top_k_interactions(feature_names, shap_interactions, k):\n    # Get the mean absolute contribution for each feature interaction\n    aggregate_interactions = np.mean(np.abs(shap_interactions[:, :-1, :-1]), axis=0)\n    interactions = []\n    for i in range(aggregate_interactions.shape[0]):\n        for j in range(aggregate_interactions.shape[1]):\n            if j < i:\n                interactions.append(\n                    (feature_names[i] + \"-\" + feature_names[j], aggregate_interactions[i][j] * 2))\n    # sort by magnitude\n    interactions.sort(key=lambda x: x[1], reverse=True)\n    interaction_features, interaction_values = map(tuple, zip(*interactions))\n    plt.bar(interaction_features[:k], interaction_values[:k])\n    plt.xticks(rotation=90)\n    plt.tight_layout()\n    plt.show()\n    \n#plot_top_k_interactions(test.columns.tolist(), shap_interactions, 10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#to_drop_shap = ['cat8_G', 'cat1_B', 'cat3_A', 'cont12', 'cont7', 'cat3_C', 'cat9_L', 'cat9_I', 'cont10']\n\n#train_shap = train.copy()\n#test_shap = test.copy()\n\n#train_shap.drop(to_drop_shap, axis=1, inplace=True)\n#test_shap.drop(to_drop_shap, axis=1, inplace=True)\n#train_shap.shape, train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"N_FOLDS = 10\n\nkf = KFold(n_splits = N_FOLDS)\noof = np.zeros(len(target))\n\npreds_folds_xgb = np.zeros(len(test))\n\n\nfor train_ind, test_ind in tqdm(kf.split(train_shap)):\n    X_train = train.iloc[train_ind]\n    X_val = train.iloc[test_ind]\n    y_train = target.iloc[train_ind]\n    y_val = target.iloc[test_ind]\n\n    model = XGBRegressor(**xgb_params)\n    \n    model.fit(X_train, y_train, eval_set = [(X_val, y_val)], early_stopping_rounds = 50, verbose = False)\n    p = model.predict(X_val)\n    oof[test_ind] = p\n\n    preds_folds_xgb += model.predict(test)/N_FOLDS\n        \nprint(f'mean square error on training data: {np.round(mean_squared_error(target, oof, squared=False),5)}')"},{"metadata":{},"cell_type":"markdown","source":"## Submission"},{"metadata":{},"cell_type":"markdown","source":"Let's try to mix catboost and LGBM prediction from a previous version. You may find the code above."},{"metadata":{"trusted":true},"cell_type":"code","source":"avg_pred_01 = sub_202['target'] * 0.6 + sub_212['target'] * 0.4\navg_pred_02 = sub_202['target'] * 1.4 - sub_212['target'] * 0.4\navg_pred_03 = sub_202['target'] * 1.4 - sub_305['target'] * 0.4\navg_pred_04 = sub_202['target'] * 1.3 - preds_folds_xgb * 0.3\n#avg_pred_05 = sub_216['target'] * 0.5 + preds_folds_xgb * 0.2 + sub_212['target'] * 0.3\n#mixed_pred = preds_folds_xgb * 0.6 + sub_84295['target'] * 0.4","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_sub['target'] = avg_pred_01\ndf_sub.to_csv('submission_045.csv', index=False)\n\ndf_sub['target'] = avg_pred_02\ndf_sub.to_csv('submission_046.csv', index=False)\n\ndf_sub['target'] = avg_pred_03\ndf_sub.to_csv('submission_047.csv', index=False)\n\ndf_sub['target'] = avg_pred_04\ndf_sub.to_csv('submission_048.csv', index=False)\n\n#df_sub['target'] = avg_pred_05\n#df_sub.to_csv('submission_044.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#plt.figure(figsize=(8,5))\n#sns.histplot(avg_pred_01, color='slategray');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Scoreboard and notes"},{"metadata":{},"cell_type":"markdown","source":"**02/02/21**  *Public 0.84295*  \n\nSimple EDA and preprocessing, tuned LGBR\n\n**04/02/21** *Public: 0.84309 / 0.84322 / 0.84361*  \n\nBasic feature engineering doesn't improve the score  \n\n**05 and 06 02/21** *Public 0.84279*  \n\nXgb with shap feature impotrance, xgb + lbgm predictions  \npred xgb oof == 0.84194  pub_sub == 0.84279 \n\n**10/02/21** *Public 0.84217*  \nMixed lightbgm and two different kfold XGB predictors  \n\n**11/02/21** Publick 0.84212*  \nLGBM 10 folds 0.83 public and average XGB preds from three different kernels;   \nTouch of feature engineering, quantile transformation.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"### best params  \n\nlgb_params_kfold = {\n \n    'n_estimators':3000,\n    'learning_rate': 0.004,\n    'min_data_per_group': 5,\n    'boosting_type': 'gbdt',\n    'num_leaves': 256,\n    'max_depth': -1,\n    'lambda_l1': 4.5,\n    'lambda_l2': 1.2,\n    \n    'metric': 'rmse',\n    'cat_smooth': 1.0,\n    'silent': True,\n    'importance_type': 'split',\n    'feature_pre_filter': False,\n    'bagging_fraction': 0.85,\n    'min_data_in_leaf': 100,\n    'min_sum_hessian_in_leaf': 0.001,\n    'bagging_freq': 7,\n    'feature_fraction': 0.5,\n    'min_gain_to_split': 0.0,\n    'min_child_samples': 20,\n    \n    'n_jobs': -1,\n    'random_state': SEED}\n\n\nxgb_params = {\n    'booster':'gbtree',\n    'n_estimators':10000,\n    'max_depth':5, \n    'eta':0.006,\n    'gamma':1.2,\n    'objective':'reg:squarederror',\n    'verbosity':0,\n    'subsample':0.85,\n    'colsample_bytree':0.8,\n    'lambda':4.5,\n    'alpha':1.2,\n    'scale_pos_weight':1,\n    'objective':'reg:squarederror',\n    'eval_metric':'rmse',\n    'seed':SEED,\n    'tree_method':'gpu_hist',\n    'gpu_id':0\n}\n\nxgb_params = {\n    'booster':'gbtree',\n    'n_estimators':10000,\n    'max_depth':7, \n    'eta':0.01,\n    'gamma':1.8,\n    'objective':'reg:squarederror',\n    'verbosity':0,\n    'subsample':0.85,\n    'colsample_bytree':0.4,\n    'lambda':2.7,\n    'alpha':6,\n    'scale_pos_weight':1,\n    'objective':'reg:squarederror',\n    'eval_metric':'rmse',\n    'seed':SEED,\n    'tree_method':'gpu_hist',\n    'gpu_id':0\n}","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}