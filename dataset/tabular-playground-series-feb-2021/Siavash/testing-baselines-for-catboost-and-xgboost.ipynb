{"cells":[{"metadata":{"execution":{"iopub.execute_input":"2021-02-24T21:36:16.840243Z","iopub.status.busy":"2021-02-24T21:36:16.839321Z","iopub.status.idle":"2021-02-24T21:36:20.437367Z","shell.execute_reply":"2021-02-24T21:36:20.435955Z"},"papermill":{"duration":3.629733,"end_time":"2021-02-24T21:36:20.437719","exception":false,"start_time":"2021-02-24T21:36:16.807986","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"!pip install --no-warn-conflicts -q --upgrade xgboost","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.01654,"end_time":"2021-02-24T21:36:20.472013","exception":false,"start_time":"2021-02-24T21:36:20.455473","status":"completed"},"tags":[]},"cell_type":"markdown","source":"This is a fun competition for testing ideas and probably **Overfiting**! Using baseline predictions for Catboost and XGboost might improve their performance. I'm testing this idea in this notebook. XGboost parameters are from [https://www.kaggle.com/tunguz/tps-02-21-feature-importance-with-xgboost-and-shap](https://www.kaggle.com/tunguz/tps-02-21-feature-importance-with-xgboost-and-shap) and the LightGBM model is from [https://www.kaggle.com/awwalmalhi/extreme-fine-tuning-lgbm-using-7-step-training](https://www.kaggle.com/awwalmalhi/extreme-fine-tuning-lgbm-using-7-step-training)"},{"metadata":{"execution":{"iopub.execute_input":"2021-02-24T21:36:20.516212Z","iopub.status.busy":"2021-02-24T21:36:20.51447Z","iopub.status.idle":"2021-02-24T21:36:27.883496Z","shell.execute_reply":"2021-02-24T21:36:27.881767Z"},"papermill":{"duration":7.39412,"end_time":"2021-02-24T21:36:27.883726","exception":false,"start_time":"2021-02-24T21:36:20.489606","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"from sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import LabelEncoder\nfrom catboost import CatBoostRegressor, Pool\nfrom sklearn.model_selection import KFold\nfrom sklearn.linear_model import RidgeCV\nfrom lightgbm import LGBMRegressor\nimport matplotlib.pyplot as plt\nfrom functools import partial\nfrom xgboost import DMatrix\nimport lightgbm as lgbm\nimport xgboost as xgb \nimport seaborn as sns\nimport pandas as pd\nimport numpy as np\nimport warnings\n\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2021-02-24T21:36:27.929826Z","iopub.status.busy":"2021-02-24T21:36:27.928968Z","iopub.status.idle":"2021-02-24T21:36:33.238963Z","shell.execute_reply":"2021-02-24T21:36:33.238205Z"},"papermill":{"duration":5.336759,"end_time":"2021-02-24T21:36:33.239188","exception":false,"start_time":"2021-02-24T21:36:27.902429","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/tabular-playground-series-feb-2021/train.csv')\ntest = pd.read_csv('../input/tabular-playground-series-feb-2021/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2021-02-24T21:36:33.380463Z","iopub.status.busy":"2021-02-24T21:36:33.34413Z","iopub.status.idle":"2021-02-24T21:36:33.383421Z","shell.execute_reply":"2021-02-24T21:36:33.382705Z"},"papermill":{"duration":0.124638,"end_time":"2021-02-24T21:36:33.383629","exception":false,"start_time":"2021-02-24T21:36:33.258991","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"X_train = train.drop(['id', 'target'], axis=1)\ny_train = train.target\nX_test = test.drop(['id'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2021-02-24T21:36:33.532473Z","iopub.status.busy":"2021-02-24T21:36:33.44261Z","iopub.status.idle":"2021-02-24T21:36:35.403301Z","shell.execute_reply":"2021-02-24T21:36:35.401798Z"},"papermill":{"duration":2.001991,"end_time":"2021-02-24T21:36:35.403534","exception":false,"start_time":"2021-02-24T21:36:33.401543","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"cat_cols = [feature for feature in train.columns if 'cat' in feature]\ncont_cols = [feature for feature in train.columns if 'con' in feature]\n\nfor feature in cat_cols:\n    le = LabelEncoder()\n    le.fit(train[feature])\n    X_train[feature] = le.transform(X_train[feature])\n    X_test[feature] = le.transform(X_test[feature])\n","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2021-02-24T21:36:35.445698Z","iopub.status.busy":"2021-02-24T21:36:35.444621Z","iopub.status.idle":"2021-02-24T21:36:35.449832Z","shell.execute_reply":"2021-02-24T21:36:35.449194Z"},"papermill":{"duration":0.028882,"end_time":"2021-02-24T21:36:35.450052","exception":false,"start_time":"2021-02-24T21:36:35.42117","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"seed = 0\nn_splits = 6\nshuffle=True\niterations = 50000\nearly_stopping_rounds = 400\nverbose_eval = 0\nbaseline_rounds = 1\ncb_learning_rate = 0.006\nxgb_learning_rate = 0.01","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2021-02-24T21:36:35.494176Z","iopub.status.busy":"2021-02-24T21:36:35.492485Z","iopub.status.idle":"2021-02-24T21:36:35.495439Z","shell.execute_reply":"2021-02-24T21:36:35.496257Z"},"papermill":{"duration":0.028777,"end_time":"2021-02-24T21:36:35.496492","exception":false,"start_time":"2021-02-24T21:36:35.467715","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"split = KFold(n_splits=n_splits, shuffle=True, random_state=seed)","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2021-02-24T21:36:35.548665Z","iopub.status.busy":"2021-02-24T21:36:35.542263Z","iopub.status.idle":"2021-02-24T21:36:35.551717Z","shell.execute_reply":"2021-02-24T21:36:35.551104Z"},"papermill":{"duration":0.037983,"end_time":"2021-02-24T21:36:35.551915","exception":false,"start_time":"2021-02-24T21:36:35.513932","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"cb_params = {'iterations':iterations,\n             'learning_rate':cb_learning_rate,\n             'depth':7,\n             'bootstrap_type':'Bernoulli',\n             'random_strength':1,\n             'min_data_in_leaf':10,\n             'l2_leaf_reg':3,\n             'loss_function':'RMSE', \n             'eval_metric':'RMSE',\n             'random_seed':seed,\n             'grow_policy':'Depthwise',\n             'max_bin':1024, \n             'model_size_reg': 0,\n             'task_type': 'GPU',\n             'od_type':'IncToDec',\n             'od_wait':100,\n             'metric_period':500,\n             'verbose':verbose_eval,\n             'subsample':0.8,\n             'od_pval':1e-10,\n             'max_ctr_complexity': 8,\n             'has_time': False,\n             'simple_ctr' : 'FeatureFreq',\n             'combinations_ctr': 'FeatureFreq'\n            }\n\nxgb_params= {'objective': 'reg:squarederror',\n             'max_depth': 6,\n             'eta': xgb_learning_rate,\n             'colsample_bytree': 0.4,\n             'subsample': 0.6,\n             'reg_alpha' : 6,\n             'min_child_weight': 100,\n             'n_jobs': 2,\n             'seed': 2001,\n             'tree_method': 'gpu_hist',\n             'gpu_id': 0,\n             'predictor': 'gpu_predictor',\n            }\n\nlgbm_params = {'max_depth': 16,\n               'subsample': 0.8032697250789377, \n               'colsample_bytree': 0.21067140508531404,\n               'learning_rate': 0.009867383057779643,\n               'reg_lambda': 10.987474846877767,\n               'reg_alpha': 17.335285595031994,\n               'min_child_samples': 31, \n               'num_leaves': 66,\n               'max_bin': 522,\n               'cat_smooth': 81,\n               'cat_l2': 0.029690334194270022,\n               'metric': 'rmse',\n               'n_jobs': -1, \n               'verbose':-1,\n               'n_estimators': iterations\n              }\n","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2021-02-24T21:36:35.799973Z","iopub.status.busy":"2021-02-24T21:36:35.785952Z","iopub.status.idle":"2021-02-25T03:26:05.579101Z","shell.execute_reply":"2021-02-25T03:26:05.578394Z"},"papermill":{"duration":20970.010089,"end_time":"2021-02-25T03:26:05.579288","exception":false,"start_time":"2021-02-24T21:36:35.569199","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"preds_list = []\noof_cb = np.zeros((len(train)))\noof_xgb = np.zeros((len(train)))\noof_cbx = np.zeros((len(train)))\noof_xgbx = np.zeros((len(train)))\noof_lgb = np.zeros((len(train)))\noof_lgb_incremental = np.zeros((len(train)))\nstack_oof = np.zeros((len(train)))\nstack_preds = np.zeros((len(test)))\n\nfor fold, (train_idx, val_idx) in enumerate(split.split(X_train)):\n    print(f'Fold {fold+1}')\n    X_tr = X_train.iloc[train_idx]\n    X_val = X_train.iloc[val_idx]\n    y_tr = y_train.iloc[train_idx]\n    y_val = y_train.iloc[val_idx]\n    fold_stack_oof = np.zeros((len(X_val), 6))\n    fold_stack_preds = np.zeros((len(test), 6))\n    ptrain = Pool(data=X_tr, label=y_tr, cat_features=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n    pvalid = Pool(data=X_val, label=y_val, cat_features=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n    ptest = Pool(data=X_test, cat_features=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n    CModel = CatBoostRegressor(**cb_params)\n    CModel.fit(ptrain,\n               eval_set=pvalid,\n               use_best_model=True,\n               early_stopping_rounds=early_stopping_rounds)\n    temp_fold_preds = CModel.predict(pvalid)\n    oof_cb[val_idx] = temp_fold_preds\n    first_cb_rmse = mean_squared_error(y_val, temp_fold_preds, squared=False)\n    print(f'RMSE of CB model is {first_cb_rmse}')\n    baseline_preds_tr_cb = CModel.predict(ptrain)\n    baseline_preds_vl_cb = temp_fold_preds\n    test_preds_cb = CModel.predict(ptest)   \n    fold_stack_oof[:,0] = temp_fold_preds\n    fold_stack_preds[:,0] = test_preds_cb\n    xtrain = DMatrix(data=X_tr, label=y_tr, nthread=2)\n    xvalid = DMatrix(data=X_val, label=y_val, nthread=2)\n    xtest = DMatrix(data=X_test, nthread=2)\n    XModel = xgb.train(xgb_params, xtrain,\n                       evals=[(xvalid,'validation')],\n                       verbose_eval=verbose_eval,\n                       early_stopping_rounds=early_stopping_rounds,\n                       xgb_model=None,\n                       num_boost_round=iterations)\n    temp_fold_preds = XModel.predict(xvalid)\n    oof_xgb[val_idx] = temp_fold_preds\n    first_xgb_rmse = mean_squared_error(y_val, temp_fold_preds, squared=False)\n    print(f'RMSE of XGB model is {first_xgb_rmse}')\n    baseline_preds_tr_xgb = XModel.predict(xtrain)\n    baseline_preds_vl_xgb = temp_fold_preds\n    test_preds_xgb = XModel.predict(xtest)\n    fold_stack_oof[:,1] = temp_fold_preds\n    fold_stack_preds[:,1] = test_preds_xgb\n    ltrain = lgbm.Dataset(X_tr, label=y_tr, init_score=None, categorical_feature=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9], free_raw_data=False)\n    lvalid = lgbm.Dataset(X_val, label=y_val, init_score=None, categorical_feature=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9], free_raw_data=False)\n    ltest =  lgbm.Dataset(X_test, label=y_val, init_score=None, categorical_feature=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9], free_raw_data=False)\n    LModel = lgbm.train(lgbm_params,\n                        train_set=ltrain,\n                        num_boost_round=iterations,\n                        valid_sets=lvalid, \n                        init_model=None,\n                        early_stopping_rounds=early_stopping_rounds,\n                        verbose_eval=verbose_eval)           \n    temp_fold_preds = LModel.predict(X_val)\n    oof_lgb[val_idx] = temp_fold_preds\n    fold_stack_oof[:,2] = temp_fold_preds\n    fold_stack_preds[:,2] = LModel.predict(X_test)\n    first_lgb_rmse = mean_squared_error(y_val, temp_fold_preds, squared=False)\n    print(f'RMSE of LGBM model is {first_lgb_rmse}')\n    params = lgbm_params.copy()     \n    params.update({'learning_rate': 0.003})\n    for i in range(1, 9):\n        if i > 2:                      \n            params['reg_lambda'] *= 0.9\n            params['reg_alpha']  *= 0.9\n            params['num_leaves'] += 40                   \n        \n        LModel = lgbm.train(lgbm_params,\n                            train_set=ltrain,\n                            num_boost_round=iterations,\n                            valid_sets=lvalid, \n                            init_model=LModel,\n                            early_stopping_rounds=early_stopping_rounds,\n                            verbose_eval=verbose_eval)           \n    temp_fold_preds = LModel.predict(X_val)\n    oof_lgb_incremental[val_idx] = temp_fold_preds\n    second_lgb_rmse = mean_squared_error(y_val, temp_fold_preds, squared=False)\n    print(f'RMSE of LGBM model is {second_lgb_rmse}')\n    print(f'LGBM improvement using Incremental Improvements {first_lgb_rmse - second_lgb_rmse}')\n    baseline_preds_tr_lgb = LModel.predict(X_tr)\n    baseline_preds_vl_lgb = temp_fold_preds\n    test_preds_lgb = LModel.predict(X_test)\n    fold_stack_oof[:,3] = temp_fold_preds\n    fold_stack_preds[:,3] = test_preds_lgb\n    \n    baseline_train = (baseline_preds_tr_xgb+baseline_preds_tr_lgb+baseline_preds_tr_cb)/3\n    baseline_valid = (baseline_preds_vl_xgb+baseline_preds_vl_lgb+baseline_preds_vl_cb)/3\n    baseline_test = (test_preds_xgb+test_preds_lgb+test_preds_cb)/3\n    \n    for baseline in range(baseline_rounds):\n        ptrain = Pool(data=X_tr, label=y_tr, cat_features=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9], baseline=baseline_train)\n        pvalid = Pool(data=X_val, label=y_val, cat_features=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9], baseline=baseline_valid)\n        ptest = Pool(data=X_test, cat_features=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9], baseline=baseline_test)\n        cb_params_ = cb_params.copy()\n        cb_params_.update({'learning_rate': cb_learning_rate*(1/(baseline+1))})\n        CModel = CatBoostRegressor(**cb_params_)\n        CModel.fit(ptrain, \n                   eval_set=pvalid,\n                   use_best_model=True,\n                   early_stopping_rounds=early_stopping_rounds)\n        temp_fold_preds = CModel.predict(pvalid)\n        oof_cbx[val_idx] = temp_fold_preds\n        second_cb_rmse = mean_squared_error(y_val, temp_fold_preds, squared=False)\n        print(f'RMSE of CB model with baseline round {baseline+1} {second_cb_rmse}')   \n        baseline_train = CModel.predict(ptrain)\n        baseline_valid = CModel.predict(pvalid)\n        baseline_test = CModel.predict(ptest)\n        if baseline == baseline_rounds - 1:\n            fold_stack_oof[:,4] = temp_fold_preds\n            fold_stack_preds[:,4] = baseline_test\n        \n        xtrain = DMatrix(data=X_tr, label=y_tr, base_margin=baseline_train)\n        xvalid = DMatrix(data=X_val, label=y_val, base_margin=baseline_valid)\n        xtest =  DMatrix(data=X_test, base_margin=baseline_test)\n        xgb_params_ = xgb_params.copy()\n        xgb_params_.update({'learning_rate': xgb_learning_rate*(1/(baseline+1))})\n        XModel = xgb.train(xgb_params_, xtrain,\n                           evals=[(xvalid,'validation')],\n                           verbose_eval=verbose_eval,\n                           early_stopping_rounds=early_stopping_rounds,\n                           xgb_model=None,\n                           num_boost_round=iterations)\n        temp_fold_preds = XModel.predict(xvalid)\n        oof_xgbx[val_idx] = temp_fold_preds\n        baseline_train = XModel.predict(xtrain)\n        baseline_valid = temp_fold_preds\n        baseline_test = XModel.predict(xtest)\n        if baseline == baseline_rounds - 1:\n            fold_stack_oof[:,5] = temp_fold_preds\n            fold_stack_preds[:,5] = baseline_test\n        second_xgb_rmse = mean_squared_error(y_val, temp_fold_preds, squared=False)\n        print(f'RMSE of XGB model with baseline round {baseline+1} {second_xgb_rmse}')\n        print(f'CB Improvement  using Baseline round {baseline+1}: {first_cb_rmse - second_cb_rmse}')\n        print(f'XGB Improvement using Baseline round {baseline+1}: {first_xgb_rmse - second_xgb_rmse}')\n        first_cb_rmse = second_cb_rmse\n        first_xgb_rmse = second_xgb_rmse\n    \n    stacker = RidgeCV().fit(fold_stack_oof, y_val)\n    temp_stack_fold_pred = stacker.predict(fold_stack_oof)\n    stack_oof[val_idx] = temp_stack_fold_pred\n    stack_rmse = mean_squared_error(y_val, temp_stack_fold_pred, squared=False)\n    print(f'RMSE of stack model  {stack_rmse}')\n    stack_preds += stacker.predict(fold_stack_preds)/n_splits\n    print('-' * 100)\n    print('',end='\\n')\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lightgbm also has an **init_score** in it's **Dataset** but it's not working the same way as Catboost and XGBoost and providing high score predictions would lead to interference with model's learning process(It's based on my tests and maybe somebody could make it work!)"},{"metadata":{"execution":{"iopub.execute_input":"2021-02-25T03:26:05.658128Z","iopub.status.busy":"2021-02-25T03:26:05.657487Z","iopub.status.idle":"2021-02-25T03:26:05.682534Z","shell.execute_reply":"2021-02-25T03:26:05.681501Z"},"papermill":{"duration":0.067635,"end_time":"2021-02-25T03:26:05.682717","exception":false,"start_time":"2021-02-25T03:26:05.615082","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"first_cb_rmse = mean_squared_error(y_train, oof_cb, squared=False)\nfirst_xgb_rmse = mean_squared_error(y_train, oof_xgb, squared=False)\nfirst_lgb_rmse = mean_squared_error(y_train, oof_lgb, squared=False)\nsecond_cb_rmse = mean_squared_error(y_train, oof_cbx, squared=False)\nsecond_xgb_rmse = mean_squared_error(y_train, oof_xgbx, squared=False)\nsecond_lgb_rmse = mean_squared_error(y_train, oof_lgb_incremental, squared=False)\nprint(f'RMSE for CB model is {first_cb_rmse}')\nprint(f'RMSE for XGB model is {first_xgb_rmse}')\nprint(f'RMSE for LGBM model is {first_lgb_rmse}')\nprint(f'RMSE for CB model with XGB baseline is {second_cb_rmse}')\nprint(f'RMSE for XGB model with CB baseline is {second_xgb_rmse}')\nprint(f'RMSE for LGBM model with Incremental Improvement is {second_lgb_rmse}')\nprint(f'RMSE for CB and XGB blend is {mean_squared_error(y_train, (oof_cbx+oof_xgbx)/2, squared=False)}')\nprint(f'RMSE for CB, XGB and LGBM blend is {mean_squared_error(y_train, (oof_cbx+oof_xgbx+oof_lgb_incremental)/3, squared=False)}')\nprint(f'RMSE for CB, XGB and LGBM stack is {mean_squared_error(y_train, stack_oof, squared=False)}')","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.036065,"end_time":"2021-02-25T03:26:05.83746","exception":false,"start_time":"2021-02-25T03:26:05.801395","status":"completed"},"tags":[]},"cell_type":"markdown","source":"Final predictions are a simple **stack** of **XGBoost**, **LightGBM** and **Catboost** models."},{"metadata":{"execution":{"iopub.execute_input":"2021-02-25T03:26:05.917599Z","iopub.status.busy":"2021-02-25T03:26:05.916122Z","iopub.status.idle":"2021-02-25T03:26:06.854692Z","shell.execute_reply":"2021-02-25T03:26:06.853445Z"},"papermill":{"duration":0.980934,"end_time":"2021-02-25T03:26:06.854882","exception":false,"start_time":"2021-02-25T03:26:05.873948","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"preds = np.mean(preds_list, axis=0)\nsubmission_mean = pd.DataFrame({'id':test.id,'target':stack_preds})\nsubmission_mean.to_csv('submission_mean.csv', index=False)\nsubmission_mean.head()","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.036241,"end_time":"2021-02-25T03:26:07.897803","exception":false,"start_time":"2021-02-25T03:26:07.861562","status":"completed"},"tags":[]},"cell_type":"markdown","source":"Another nice idea for more **overfit**. Blending with [**top public blend**](https://www.kaggle.com/somayyehgholami/comparative-method-tabular-feb-301)!"},{"metadata":{"execution":{"iopub.execute_input":"2021-02-25T03:26:07.983345Z","iopub.status.busy":"2021-02-25T03:26:07.982565Z","iopub.status.idle":"2021-02-25T03:26:08.85885Z","shell.execute_reply":"2021-02-25T03:26:08.857652Z"},"papermill":{"duration":0.924982,"end_time":"2021-02-25T03:26:08.859045","exception":false,"start_time":"2021-02-25T03:26:07.934063","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"sub_best = pd.read_csv('../input/comparative-method-tabular-feb-301/submission.csv')\nsub_final = submission_mean.copy()\nsub_final['target'] = sub_best.target * 0.9 + submission_mean.target * 0.1\nsub_final.to_csv('blend_mean.csv', index=False)\nsub_final.head()","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.036849,"end_time":"2021-02-25T03:26:09.746188","exception":false,"start_time":"2021-02-25T03:26:09.709339","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}