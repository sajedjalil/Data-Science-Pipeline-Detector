{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Introduction\n\nA simple LightGBM model using `LabelEncoder` for categorical values. Further tuned by hand to find a good balance between fit and LB score. Note that the model is very likely overfit. This model may be most useful if combined with other ensemble methods. Note that no significant feature engineering was employed by this model.\n\n## Credits\n\n* [LGBM Goes brrr!](https://www.kaggle.com/maunish/lgbm-goes-brrr) for initial tuning of LGBM parameters."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\ntrain = pd.read_csv(\"../input/tabular-playground-series-feb-2021/train.csv\")\ntest = pd.read_csv(\"../input/tabular-playground-series-feb-2021/test.csv\")\ntrain","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature Definitions"},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_features = [\n    \"cat0\", \"cat1\", \"cat2\", \"cat3\", \"cat4\", \"cat5\", \"cat6\", \"cat7\", \n    \"cat8\", \"cat9\"\n]\n\ncont_features = [\n    \"cont0\", \"cont1\", \"cont2\", \"cont3\", \"cont4\",\n    \"cont5\", \"cont6\", \"cont7\", \"cont8\", \"cont9\", \"cont10\", \n    \"cont11\", \"cont12\", \"cont13\"\n]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Categories must be converted to `int` types for LightGBM to use them as categorical. We'll use `LabelEncoder` to do this."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\n\nnew_cat_features = []\n\ndef label_encode(train_df, test_df, column):\n    le = LabelEncoder()\n    new_feature = \"{}_le\".format(column)\n    le.fit(train_df[column])\n    train_df[new_feature] = le.transform(train_df[column])\n    test_df[new_feature] = le.transform(test_df[column])\n    return new_feature\n\nfor feature in cat_features:\n    new_cat_features.append(label_encode(train, test, feature))\n\ntrain","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Generate Model\n\nHere we'll use `KFold` cross validation, but produce predictions out-of-fold for each fold."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\nfrom lightgbm import LGBMRegressor\n\nn_folds = 10\n\nskf = KFold(n_splits=n_folds, random_state=2021, shuffle=True)\n\ntrain_oof = np.zeros((300000,))\ntest_preds = 0\n\nfull_features = []\nfull_features.extend(new_cat_features)\nfull_features.extend(cont_features)\n\nlgbm_params = {\n    \"random_state\": 2021,\n    \"metric\": \"rmse\",\n    \"n_jobs\": 6,\n    \"early_stopping_round\": 200,\n    \"cat_features\": [x for x in range(len(new_cat_features))],\n    \"reg_alpha\": 9.03513073170552,\n    \"reg_lambda\": 0.024555737897445917,\n    \"colsample_bytree\": 0.2185112060137363,\n    \"learning_rate\": 0.003049106861273527,\n    \"max_depth\": 65,\n    \"num_leaves\": 51,\n    \"min_child_samples\": 177,\n    \"n_estimators\": 1600000,\n    \"cat_smooth\": 93.60968300634175,\n    \"max_bin\": 537,\n    \"min_data_per_group\": 117,\n    \"bagging_freq\": 1,\n    \"bagging_fraction\": 0.6709049555262285,\n    \"cat_l2\": 7.5586732660804445,\n}\n\nfor fold, (train_index, test_index) in enumerate(skf.split(train, train[\"target\"])):\n    print(\"-------> Fold {} <--------\".format(fold + 1))\n    x_train, x_valid = pd.DataFrame(train.iloc[train_index]), pd.DataFrame(train.iloc[test_index])\n    y_train, y_valid = train[\"target\"].iloc[train_index], train[\"target\"].iloc[test_index]\n    \n    x_train_features = pd.DataFrame(x_train[full_features])\n    x_valid_features = pd.DataFrame(x_valid[full_features])\n\n    model = LGBMRegressor(\n        **lgbm_params\n    )\n    model.fit(\n        x_train_features[full_features], \n        y_train,\n        eval_set=[(x_valid_features[full_features], y_valid)],\n        verbose=100,\n    )\n    oof_preds = model.predict(x_valid_features[full_features])\n    test_preds += model.predict(test[full_features]) / n_folds\n    train_oof[test_index] = oof_preds\n    print(\"\")\n    \nprint(\"--> Overall results for out of fold predictions\")\nprint(\": RMSE = {}\".format(mean_squared_error(train_oof, train[\"target\"], squared=False)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Generate Results\n\nThe test predictions were generated from each fold. Collect them here and build submission file."},{"metadata":{"trusted":true},"cell_type":"code","source":"preds = test_preds.tolist()\ntest_ids = test[\"id\"].tolist()\n\nsubmission = pd.DataFrame({\"id\": test_ids, \"target\": preds})\nsubmission.to_csv(\"submission.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"If you find this kernel useful, please upvote!"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}