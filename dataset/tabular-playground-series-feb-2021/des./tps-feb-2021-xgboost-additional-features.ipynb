{"cells":[{"metadata":{},"cell_type":"markdown","source":"### Trying to create new features when the features are anonymised, is taking a stab in the dark. It is more of a hope than expectations. So I did!\n\n#### Highlights: \n* Features I combined: **cat1, cat8, cat9** (cat9 x cat1, cat8 x cat1)\n* Cat features are label encoded first\n\n||5 fold local CV|||\n|---|---|---|---|\n| Model|lr =0.1 | lr =0.01 |lr=0.005|\n| base-model | 0.843853 | 0.842680 |0.842935|\n| with new features | 0.843737 | 0.842648 |0.842927|\n|score gain|0.000116|0.000032|0.000008|\n\n### Remark:\nThe result is based on realtively coarse model. Outcome may be different for fine-tuned models. If you find something different with your models please let me know. \n\nThe score-gained due to the additional features has decreased with lowering the learning rate. Why?  \n\n         \n"},{"metadata":{},"cell_type":"markdown","source":"# 0. Set-up"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\nfrom lightgbm import LGBMRegressor\nfrom xgboost import XGBRegressor\nfrom sklearn.preprocessing import LabelEncoder\n\nimport numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 1. Load data"},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv(r'/kaggle/input/tabular-playground-series-feb-2021/train.csv', index_col= 'id')\ntest = pd.read_csv(r'/kaggle/input/tabular-playground-series-feb-2021/test.csv', index_col= 'id')\nsubmission = pd.read_csv(r'/kaggle/input/tabular-playground-series-feb-2021/sample_submission.csv', index_col= 'id')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2. Data processing"},{"metadata":{"trusted":true},"cell_type":"code","source":"target = train.pop('target')\ny = target","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_features = [col for col in train.columns if train[col].dtype=='object']\nnum_features = [col for col in train.columns if train[col].dtype=='float']\n\n\nle = LabelEncoder()\n\nle_train = train.copy()\nle_test = test.copy()\n\nfor col in cat_features:\n    le_train[col] = le.fit_transform(train[col])\n    le_test[col] = le.transform(test[col])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = le_train\ntest = le_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features = train.columns\nlen(features)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3. Base model: xgboost"},{"metadata":{"trusted":true},"cell_type":"code","source":"Nfold = 5\nSEED = 100\n\nkfold = KFold(n_splits=Nfold, shuffle=True, random_state=SEED)\noof_preds = np.zeros(train.shape[0])\nsubm_preds_xgb = np.zeros(test.shape[0])\n\nfor n_fold, (trn_idx, val_idx) in enumerate(kfold.split(train)):\n    trn_x, trn_y = train[features].iloc[trn_idx], y.iloc[trn_idx]\n    val_x, val_y = train[features].iloc[val_idx], y.iloc[val_idx]\n    \n    xgb = XGBRegressor(max_depth=6,\n        learning_rate=0.005,\n        n_estimators=5000,\n        verbosity=1,\n        silent=None,\n        objective='reg:squarederror',\n        booster='gbtree',\n        n_jobs=-1,\n        nthread=None,\n        gamma=0.0,\n        min_child_weight= 133, \n        subsample=0.8,\n        colsample_bytree=0.5,\n        reg_alpha=7.5,\n        reg_lambda=0.25,\n        random_state=SEED,\n        tree_method = 'gpu_hist',\n        predictor = 'gpu_predictor',\n    )                      \n\n    \n    xgb.fit(trn_x, trn_y,\n           eval_set =[(trn_x, trn_y), (val_x, val_y)],\n           eval_metric=\"rmse\", verbose=1000, early_stopping_rounds=40\n           )\n   \n    oof_preds[val_idx] = xgb.predict(val_x)\n    subm_preds_xgb += xgb.predict(test[features])/kfold.n_splits\n    \n    print('Fold {} MSE : {:.6f}'.format(n_fold + 1, mean_squared_error(val_y, oof_preds[val_idx], squared=False)))   \n      \n    \nprint(\"*****************************************************************\")\nprint('{} fold local CV= {:.6f}'.format(Nfold, mean_squared_error(y, oof_preds, squared=False)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 4. Feature engineering\n## 4.1 Mutual Info Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"# the following two code snippets are adapted from the \"feature engineering kaggle min-course\"\n\nfrom sklearn.feature_selection import mutual_info_regression\n\nfeatures = train.dtypes == int\n\ndef make_mi_scores(train, y, discrete_features):\n    mi_scores = mutual_info_regression(train, y, discrete_features=features)\n    mi_scores = pd.Series(mi_scores, name=\"MI Scores\", index=train.columns)\n    mi_scores = mi_scores.sort_values(ascending=False)\n    return mi_scores\n\nmi_scores = make_mi_scores(train, y, features)\nmi_scores","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_utility_scores(scores):\n    y = scores.sort_values(ascending=True)\n    width = np.arange(len(y))\n    ticks = list(y.index)\n    plt.barh(width, y, color='#d1aeab', alpha=0.9)\n    plt.yticks(width, ticks)\n    plt.grid()\n    plt.title(\"Mutual Information Scores\")\n\n\nplt.figure(dpi=100, figsize=(8, 5))\nplot_utility_scores(mi_scores)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4.2 Additional Features Created"},{"metadata":{"trusted":true},"cell_type":"code","source":"# I tried other combinations of features based on the mutual information score \n# but these two gave the best improvement\n\ntrain['9t1'] = train['cat9']*train['cat1']\ntrain['8t1'] = train['cat8']*train['cat1']\n\ntest['9t1'] = test['cat9']*test['cat1']\ntest['8t1'] = test['cat8']*test['cat1']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features = train.columns\nlen(features)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 5. Model with the additional features"},{"metadata":{"trusted":true},"cell_type":"code","source":"Nfold = 5\nSEED = 100\n\nkfold = KFold(n_splits=Nfold, shuffle=True, random_state=SEED)\noof_preds = np.zeros(train.shape[0])\nsubm_preds_xgb = np.zeros(test.shape[0])\n\nfor n_fold, (trn_idx, val_idx) in enumerate(kfold.split(train)):\n    trn_x, trn_y = train[features].iloc[trn_idx], y.iloc[trn_idx]\n    val_x, val_y = train[features].iloc[val_idx], y.iloc[val_idx]\n    \n    xgb = XGBRegressor(max_depth=6,\n        learning_rate=0.005,\n        n_estimators=5000,\n        verbosity=1,\n        silent=None,\n        objective='reg:squarederror',\n        booster='gbtree',\n        n_jobs=-1,\n        nthread=None,\n        gamma=0.0,\n        min_child_weight= 133, \n        subsample=0.8,\n        colsample_bytree=0.5,\n        reg_alpha=7.5,\n        reg_lambda=0.25,\n        random_state=SEED,\n        tree_method = 'gpu_hist',\n        predictor = 'gpu_predictor',\n    )                      \n\n    \n    xgb.fit(trn_x, trn_y,\n           eval_set =[(trn_x, trn_y), (val_x, val_y)],\n           eval_metric=\"rmse\", verbose=1000, early_stopping_rounds=40\n           )\n   \n    oof_preds[val_idx] = xgb.predict(val_x)\n    subm_preds_xgb += xgb.predict(test[features])/kfold.n_splits\n    \n    print('Fold {} MSE : {:.6f}'.format(n_fold + 1, mean_squared_error(val_y, oof_preds[val_idx], squared=False)))   \n      \n    \nprint(\"*****************************************************************\")\nprint('{} fold local CV= {:.6f}'.format(Nfold, mean_squared_error(y, oof_preds, squared=False)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Thank you for your interest in this notebook!"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}