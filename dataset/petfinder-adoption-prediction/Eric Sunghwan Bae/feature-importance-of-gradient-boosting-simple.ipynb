{"cells":[{"metadata":{"trusted":true,"_uuid":"a9f0576d564693bb6c30b55c39af2ae5d1f24963"},"cell_type":"code","source":"# The score of V7 was 0.327.\n# V7 code uses Gradient Boosting (sklearn) and verify feature importance as input data.\n\n# This version will add more features (images, image metadata, and sentiment data).\n# Reference: https://www.kaggle.com/wrosinski/baselinemodeling\n# Our goal: Integration of more features will enable us to possibly improve the score.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"57e31184d023c18e72eb56794835e5e623e8c5dd"},"cell_type":"code","source":"import gc\nimport glob\nimport os\nimport json\nimport matplotlib.pyplot as plt\nimport pprint\n\nimport numpy as np\nimport pandas as pd\n\nfrom joblib import Parallel, delayed\nfrom tqdm import tqdm\nfrom PIL import Image\n\n%matplotlib inline\n\npd.options.display.max_rows = 128\npd.options.display.max_columns = 128\nplt.rcParams['figure.figsize'] = (12, 9)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cec10bf37b229c85a2b7c5b29e6adad078f5ba1e"},"cell_type":"code","source":"# load core DFs (train and test):\ntrain = pd.read_csv('../input/train/train.csv')\nprint('train shape:', train.shape)\ntest = pd.read_csv('../input/test/test.csv')\nprint('test shape:', test.shape)\nsample_submission = pd.read_csv('../input/test/sample_submission.csv')\n\n# load mapping dictionaries:\nlabels_breed = pd.read_csv('../input/breed_labels.csv')\nlabels_state = pd.read_csv('../input/color_labels.csv')\nlabels_color = pd.read_csv('../input/state_labels.csv')\n\n# add additional features (The type is LIST and is has directory of files):\ntrain_image_files = sorted(glob.glob('../input/train_images/*.jpg'))\ntrain_metadata_files = sorted(glob.glob('../input/train_metadata/*.json'))\ntrain_sentiment_files = sorted(glob.glob('../input/train_sentiment/*.json'))\nprint('num of train images files: {}'.format(len(train_image_files)))\nprint('num of train metadata files: {}'.format(len(train_metadata_files)))\nprint('num of train sentiment files: {}'.format(len(train_sentiment_files)))\ntest_image_files = sorted(glob.glob('../input/test_images/*.jpg'))\ntest_metadata_files = sorted(glob.glob('../input/test_metadata/*.json'))\ntest_sentiment_files = sorted(glob.glob('../input/test_sentiment/*.json'))\nprint('num of test images files: {}'.format(len(test_image_files)))\nprint('num of test metadata files: {}'.format(len(test_metadata_files)))\nprint('num of test sentiment files: {}'.format(len(test_sentiment_files)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"babe92b8ae520a3e3e0bd93c826115f8fc69a006"},"cell_type":"code","source":"# plt.rcParams['figure.figsize'] = (12, 9)\nplt.style.use('ggplot')\n\n# Images:\ntrain_df_ids = train[['PetID']]\nprint('length of train data is', train_df_ids.shape[0])\nprint()\ntrain_df_imgs = pd.DataFrame(train_image_files)\ntrain_df_imgs.columns = ['image_filename']\ntrain_imgs_pets = train_df_imgs['image_filename'].apply(lambda x: x.split('/')[-1].split('-')[0])\ntrain_df_imgs = train_df_imgs.assign(PetID=train_imgs_pets)\nprint('# of train data that has images is', len(train_imgs_pets.unique()))\npets_with_images = len(np.intersect1d(train_imgs_pets.unique(), train_df_ids['PetID'].unique()))\nprint('fraction of pets with images: {:.3f}'.format(pets_with_images / train_df_ids.shape[0]))\n\n# Metadata:\ntrain_df_ids = train[['PetID']]\ntrain_df_metadata = pd.DataFrame(train_metadata_files)\ntrain_df_metadata.columns = ['metadata_filename']\ntrain_metadata_pets = train_df_metadata['metadata_filename'].apply(lambda x: x.split('/')[-1].split('-')[0])\ntrain_df_metadata = train_df_metadata.assign(PetID=train_metadata_pets)\nprint(len(train_metadata_pets.unique()))\npets_with_metadatas = len(np.intersect1d(train_metadata_pets.unique(), train_df_ids['PetID'].unique()))\nprint('fraction of pets with metadata: {:.3f}'.format(pets_with_metadatas / train_df_ids.shape[0]))\n\n# Sentiment:\ntrain_df_ids = train[['PetID']]\ntrain_df_sentiment = pd.DataFrame(train_sentiment_files)\ntrain_df_sentiment.columns = ['sentiment_filename']\ntrain_sentiment_pets = train_df_sentiment['sentiment_filename'].apply(lambda x: x.split('/')[-1].split('.')[0])\ntrain_df_sentiment = train_df_sentiment.assign(PetID=train_sentiment_pets)\nprint(len(train_sentiment_pets.unique()))\npets_with_sentiments = len(np.intersect1d(train_sentiment_pets.unique(), train_df_ids['PetID'].unique()))\nprint('fraction of pets with sentiment: {:.3f}'.format(pets_with_sentiments / train_df_ids.shape[0]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ced7be65590385fda3e7c1274ef610a35bebd59a"},"cell_type":"code","source":"# for test\n# Images:\ntest_df_ids = test[['PetID']]\nprint(test_df_ids.shape)\ntest_df_imgs = pd.DataFrame(test_image_files)\ntest_df_imgs.columns = ['image_filename']\ntest_imgs_pets = test_df_imgs['image_filename'].apply(lambda x: x.split('/')[-1].split('-')[0])\ntest_df_imgs = test_df_imgs.assign(PetID=test_imgs_pets)\nprint(len(test_imgs_pets.unique()))\npets_with_images = len(np.intersect1d(test_imgs_pets.unique(), test_df_ids['PetID'].unique()))\nprint('fraction of pets with images: {:.3f}'.format(pets_with_images / test_df_ids.shape[0]))\n\n# Metadata:\ntest_df_ids = test[['PetID']]\ntest_df_metadata = pd.DataFrame(test_metadata_files)\ntest_df_metadata.columns = ['metadata_filename']\ntest_metadata_pets = test_df_metadata['metadata_filename'].apply(lambda x: x.split('/')[-1].split('-')[0])\ntest_df_metadata = test_df_metadata.assign(PetID=test_metadata_pets)\nprint(len(test_metadata_pets.unique()))\npets_with_metadatas = len(np.intersect1d(test_metadata_pets.unique(), test_df_ids['PetID'].unique()))\nprint('fraction of pets with metadata: {:.3f}'.format(pets_with_metadatas / test_df_ids.shape[0]))\n\n# Sentiment:\ntest_df_ids = test[['PetID']]\ntest_df_sentiment = pd.DataFrame(test_sentiment_files)\ntest_df_sentiment.columns = ['sentiment_filename']\ntest_sentiment_pets = test_df_sentiment['sentiment_filename'].apply(lambda x: x.split('/')[-1].split('.')[0])\ntest_df_sentiment = test_df_sentiment.assign(PetID=test_sentiment_pets)\nprint(len(test_sentiment_pets.unique()))\npets_with_sentiments = len(np.intersect1d(test_sentiment_pets.unique(), test_df_ids['PetID'].unique()))\nprint('fraction of pets with sentiment: {:.3f}'.format(pets_with_sentiments / test_df_ids.shape[0]))\nprint()\n# are distributions the same?\nprint('images and metadata distributions the same? {}'.format(\n    np.all(test_metadata_pets == test_imgs_pets)))","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":false,"_kg_hide-output":false,"trusted":true,"_uuid":"0e78823f3d317407622db92d89deb77e607b81b6"},"cell_type":"code","source":"# data parsing & feature extraction:\n# After taking a look at the data, we know its structure and can use it to extract additional features and concatenate them with basic train/test DFs.\nclass PetFinderParser(object):\n    \n    def __init__(self, debug=False):\n        \n        self.debug = debug\n        self.sentence_sep = ' '\n        \n        # Does not have to be extracted because main DF already contains description\n        self.extract_sentiment_text = False\n        \n        \n    def open_metadata_file(self, filename):\n        \"\"\"\n        Load metadata file.\n        \"\"\"\n        with open(filename, 'r') as f:\n            metadata_file = json.load(f)\n        return metadata_file\n            \n    def open_sentiment_file(self, filename):\n        \"\"\"\n        Load sentiment file.\n        \"\"\"\n        with open(filename, 'r') as f:\n            sentiment_file = json.load(f)\n        return sentiment_file\n            \n    def open_image_file(self, filename):\n        \"\"\"\n        Load image file.\n        \"\"\"\n        image = np.asarray(Image.open(filename))\n        return image\n    \n    def parse_sentiment_file(self, file):\n        \"\"\"\n        Parse sentiment file. Output DF with sentiment features.\n        \"\"\"\n        \n        file_sentiment = file['documentSentiment']\n        file_entities = [x['name'] for x in file['entities']]\n        file_entities = self.sentence_sep.join(file_entities)\n\n        if self.extract_sentiment_text:\n            file_sentences_text = [x['text']['content'] for x in file['sentences']]\n            file_sentences_text = self.sentence_sep.join(file_sentences_text)\n        file_sentences_sentiment = [x['sentiment'] for x in file['sentences']]\n        \n        file_sentences_sentiment = pd.DataFrame.from_dict(\n            file_sentences_sentiment, orient='columns').sum()\n        file_sentences_sentiment = file_sentences_sentiment.add_prefix('document_').to_dict()\n        \n        file_sentiment.update(file_sentences_sentiment)\n        \n        df_sentiment = pd.DataFrame.from_dict(file_sentiment, orient='index').T\n        if self.extract_sentiment_text:\n            df_sentiment['text'] = file_sentences_text\n            \n        df_sentiment['entities'] = file_entities\n        df_sentiment = df_sentiment.add_prefix('sentiment_')\n        \n        return df_sentiment\n    \n    def parse_metadata_file(self, file):\n        \"\"\"\n        Parse metadata file. Output DF with metadata features.\n        \"\"\"\n        \n        file_keys = list(file.keys())\n        \n        if 'labelAnnotations' in file_keys:\n            file_annots = file['labelAnnotations'][:int(len(file['labelAnnotations']) * 0.3)]\n            file_top_score = np.asarray([x['score'] for x in file_annots]).mean()\n            file_top_desc = [x['description'] for x in file_annots]\n        else:\n            file_top_score = np.nan\n            file_top_desc = ['']\n        \n        file_colors = file['imagePropertiesAnnotation']['dominantColors']['colors']\n        file_crops = file['cropHintsAnnotation']['cropHints']\n\n        file_color_score = np.asarray([x['score'] for x in file_colors]).mean()\n        file_color_pixelfrac = np.asarray([x['pixelFraction'] for x in file_colors]).mean()\n\n        file_crop_conf = np.asarray([x['confidence'] for x in file_crops]).mean()\n        \n        if 'importanceFraction' in file_crops[0].keys():\n            file_crop_importance = np.asarray([x['importanceFraction'] for x in file_crops]).mean()\n        else:\n            file_crop_importance = np.nan\n\n        df_metadata = {\n            'annots_score': file_top_score,\n            'color_score': file_color_score,\n            'color_pixelfrac': file_color_pixelfrac,\n            'crop_conf': file_crop_conf,\n            'crop_importance': file_crop_importance,\n            'annots_top_desc': self.sentence_sep.join(file_top_desc)\n        }\n        \n        df_metadata = pd.DataFrame.from_dict(df_metadata, orient='index').T\n        df_metadata = df_metadata.add_prefix('metadata_')\n        \n        return df_metadata\n    \n# Helper function for parallel data processing:\ndef extract_additional_features(pet_id, mode='train'):\n    \n    sentiment_filename = '../input/{}_sentiment/{}.json'.format(mode, pet_id)\n    try:\n        sentiment_file = pet_parser.open_sentiment_file(sentiment_filename)\n        df_sentiment = pet_parser.parse_sentiment_file(sentiment_file)\n        df_sentiment['PetID'] = pet_id\n    except FileNotFoundError:\n        df_sentiment = []\n\n    dfs_metadata = []\n    metadata_filenames = sorted(glob.glob('../input/{}_metadata/{}*.json'.format(mode, pet_id)))\n    if len(metadata_filenames) > 0:\n        for f in metadata_filenames:\n            metadata_file = pet_parser.open_metadata_file(f)\n            df_metadata = pet_parser.parse_metadata_file(metadata_file)\n            df_metadata['PetID'] = pet_id\n            dfs_metadata.append(df_metadata)\n        dfs_metadata = pd.concat(dfs_metadata, ignore_index=True, sort=False)\n    dfs = [df_sentiment, dfs_metadata]\n    \n    return dfs\n\n\npet_parser = PetFinderParser()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6a33f3f9fde3ed061c3da7cc4ac6a48d601b2caf"},"cell_type":"code","source":"# Unique IDs from train and test:\ndebug = False\ntrain_pet_ids = train.PetID.unique()\ntest_pet_ids = test.PetID.unique()\n\nif debug:\n    train_pet_ids = train_pet_ids[:1000]\n    test_pet_ids = test_pet_ids[:500]\n\n\n# Train set:\n# Parallel processing of data:\ndfs_train = Parallel(n_jobs=6, verbose=1)(\n    delayed(extract_additional_features)(i, mode='train') for i in train_pet_ids)\n\n# Extract processed data and format them as DFs:\ntrain_dfs_sentiment = [x[0] for x in dfs_train if isinstance(x[0], pd.DataFrame)]\ntrain_dfs_metadata = [x[1] for x in dfs_train if isinstance(x[1], pd.DataFrame)]\n\ntrain_dfs_sentiment = pd.concat(train_dfs_sentiment, ignore_index=True, sort=False)\ntrain_dfs_metadata = pd.concat(train_dfs_metadata, ignore_index=True, sort=False)\n\nprint(train_dfs_sentiment.shape, train_dfs_metadata.shape)\n\n\n# Test set:\n# Parallel processing of data:\ndfs_test = Parallel(n_jobs=6, verbose=1)(\n    delayed(extract_additional_features)(i, mode='test') for i in test_pet_ids)\n\n# Extract processed data and format them as DFs:\ntest_dfs_sentiment = [x[0] for x in dfs_test if isinstance(x[0], pd.DataFrame)]\ntest_dfs_metadata = [x[1] for x in dfs_test if isinstance(x[1], pd.DataFrame)]\n\ntest_dfs_sentiment = pd.concat(test_dfs_sentiment, ignore_index=True, sort=False)\ntest_dfs_metadata = pd.concat(test_dfs_metadata, ignore_index=True, sort=False)\n\nprint(test_dfs_sentiment.shape, test_dfs_metadata.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6aae7ae910c02edee82cb06f02464901cbef1f59"},"cell_type":"code","source":"train_dfs_metadata.head(1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3d2aa556b286fb1572eef47d117abf7ee1e0e001"},"cell_type":"code","source":"train_dfs_sentiment.head(1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"87eeba7e85490245d624dc83f7bb0f3caac3f0f7"},"cell_type":"code","source":"# group extracted features by PetID:\n# Extend aggregates and improve column naming\naggregates = ['mean', 'sum']\n\n# Train\ntrain_metadata_desc = train_dfs_metadata.groupby(['PetID'])['metadata_annots_top_desc'].unique()\ntrain_metadata_desc = train_metadata_desc.reset_index()\ntrain_metadata_desc[\n    'metadata_annots_top_desc'] = train_metadata_desc[\n    'metadata_annots_top_desc'].apply(lambda x: ' '.join(x))\n\nprefix = 'metadata'\ntrain_metadata_gr = train_dfs_metadata.drop(['metadata_annots_top_desc'], axis=1)\nfor i in train_metadata_gr.columns:\n    if 'PetID' not in i:\n        train_metadata_gr[i] = train_metadata_gr[i].astype(float)\ntrain_metadata_gr = train_metadata_gr.groupby(['PetID']).agg(aggregates)\ntrain_metadata_gr.columns = pd.Index(['{}_{}_{}'.format(\n            prefix, c[0], c[1].upper()) for c in train_metadata_gr.columns.tolist()])\ntrain_metadata_gr = train_metadata_gr.reset_index()\n\n\ntrain_sentiment_desc = train_dfs_sentiment.groupby(['PetID'])['sentiment_entities'].unique()\ntrain_sentiment_desc = train_sentiment_desc.reset_index()\ntrain_sentiment_desc[\n    'sentiment_entities'] = train_sentiment_desc[\n    'sentiment_entities'].apply(lambda x: ' '.join(x))\n\nprefix = 'sentiment'\ntrain_sentiment_gr = train_dfs_sentiment.drop(['sentiment_entities'], axis=1)\nfor i in train_sentiment_gr.columns:\n    if 'PetID' not in i:\n        train_sentiment_gr[i] = train_sentiment_gr[i].astype(float)\ntrain_sentiment_gr = train_sentiment_gr.groupby(['PetID']).agg(aggregates)\ntrain_sentiment_gr.columns = pd.Index(['{}_{}_{}'.format(\n            prefix, c[0], c[1].upper()) for c in train_sentiment_gr.columns.tolist()])\ntrain_sentiment_gr = train_sentiment_gr.reset_index()\n\n# Test\ntest_metadata_desc = test_dfs_metadata.groupby(['PetID'])['metadata_annots_top_desc'].unique()\ntest_metadata_desc = test_metadata_desc.reset_index()\ntest_metadata_desc[\n    'metadata_annots_top_desc'] = test_metadata_desc[\n    'metadata_annots_top_desc'].apply(lambda x: ' '.join(x))\n\nprefix = 'metadata'\ntest_metadata_gr = test_dfs_metadata.drop(['metadata_annots_top_desc'], axis=1)\nfor i in test_metadata_gr.columns:\n    if 'PetID' not in i:\n        test_metadata_gr[i] = test_metadata_gr[i].astype(float)\ntest_metadata_gr = test_metadata_gr.groupby(['PetID']).agg(aggregates)\ntest_metadata_gr.columns = pd.Index(['{}_{}_{}'.format(\n            prefix, c[0], c[1].upper()) for c in test_metadata_gr.columns.tolist()])\ntest_metadata_gr = test_metadata_gr.reset_index()\n\n\ntest_sentiment_desc = test_dfs_sentiment.groupby(['PetID'])['sentiment_entities'].unique()\ntest_sentiment_desc = test_sentiment_desc.reset_index()\ntest_sentiment_desc[\n    'sentiment_entities'] = test_sentiment_desc[\n    'sentiment_entities'].apply(lambda x: ' '.join(x))\n\nprefix = 'sentiment'\ntest_sentiment_gr = test_dfs_sentiment.drop(['sentiment_entities'], axis=1)\nfor i in test_sentiment_gr.columns:\n    if 'PetID' not in i:\n        test_sentiment_gr[i] = test_sentiment_gr[i].astype(float)\ntest_sentiment_gr = test_sentiment_gr.groupby(['PetID']).agg(aggregates)\ntest_sentiment_gr.columns = pd.Index(['{}_{}_{}'.format(\n            prefix, c[0], c[1].upper()) for c in test_sentiment_gr.columns.tolist()])\ntest_sentiment_gr = test_sentiment_gr.reset_index()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"41dc43aae70b80db1119f2fd510c7e248a80cfc2"},"cell_type":"code","source":"# merge processed DFs with base train/test DF:\n# Train merges:\ntrain_proc = train.copy()\ntrain_proc = train_proc.merge(\n    train_sentiment_gr, how='left', on='PetID')\ntrain_proc = train_proc.merge(\n    train_metadata_gr, how='left', on='PetID')\ntrain_proc = train_proc.merge(\n    train_metadata_desc, how='left', on='PetID')\ntrain_proc = train_proc.merge(\n    train_sentiment_desc, how='left', on='PetID')\n\n# Test merges:\ntest_proc = test.copy()\ntest_proc = test_proc.merge(\n    test_sentiment_gr, how='left', on='PetID')\ntest_proc = test_proc.merge(\n    test_metadata_gr, how='left', on='PetID')\ntest_proc = test_proc.merge(\n    test_metadata_desc, how='left', on='PetID')\ntest_proc = test_proc.merge(\n    test_sentiment_desc, how='left', on='PetID')\n\n\nprint(train_proc.shape, test_proc.shape)\nassert train_proc.shape[0] == train.shape[0]\nassert test_proc.shape[0] == test.shape[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0b9a25e892876a124f0831225802b926d5209cda"},"cell_type":"code","source":"train_proc.head(1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"36de383fcfe4fdba6baa370d5aaaabdb9089fcbe"},"cell_type":"code","source":"# add breed mapping:\ntrain_breed_main = train_proc[['Breed1']].merge(\n    labels_breed, how='left',\n    left_on='Breed1', right_on='BreedID',\n    suffixes=('', '_main_breed'))\n\ntrain_breed_main = train_breed_main.iloc[:, 2:]\ntrain_breed_main = train_breed_main.add_prefix('main_breed_')\n\ntrain_breed_second = train_proc[['Breed2']].merge(\n    labels_breed, how='left',\n    left_on='Breed2', right_on='BreedID',\n    suffixes=('', '_second_breed'))\n\ntrain_breed_second = train_breed_second.iloc[:, 2:]\ntrain_breed_second = train_breed_second.add_prefix('second_breed_')\n\ntrain_proc = pd.concat(\n    [train_proc, train_breed_main, train_breed_second], axis=1)\n\n\ntest_breed_main = test_proc[['Breed1']].merge(\n    labels_breed, how='left',\n    left_on='Breed1', right_on='BreedID',\n    suffixes=('', '_main_breed'))\n\ntest_breed_main = test_breed_main.iloc[:, 2:]\ntest_breed_main = test_breed_main.add_prefix('main_breed_')\n\ntest_breed_second = test_proc[['Breed2']].merge(\n    labels_breed, how='left',\n    left_on='Breed2', right_on='BreedID',\n    suffixes=('', '_second_breed'))\n\ntest_breed_second = test_breed_second.iloc[:, 2:]\ntest_breed_second = test_breed_second.add_prefix('second_breed_')\n\n\ntest_proc = pd.concat(\n    [test_proc, test_breed_main, test_breed_second], axis=1)\n\nprint(train_proc.shape, test_proc.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4762c00ada791a34d3f897de92a48020180bf658"},"cell_type":"code","source":"train_proc.head(1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2700d1cefa5ddebc56a579063a80ed5f7d0bc5c2"},"cell_type":"code","source":"# concatenate train & test:\n# Inspect NaN structure of the processed data: AdoptionSpeed is the target column.\nX = pd.concat([train_proc, test_proc], ignore_index=True, sort=False)\n# print('NaN structure:\\n{}'.format(np.sum(pd.isnull(X))))\n\n# extract different column types:\n# integer columns are usually categorical features, which do not need encoding\n# float columns are numerical features\n# object columns are categorical features, which should be encoded\ncolumn_types = X.dtypes\nint_cols = column_types[column_types == 'int']\nfloat_cols = column_types[column_types == 'float']\ncat_cols = column_types[column_types == 'object']\nprint('\\tinteger columns:\\n{}'.format(int_cols))\nprint('\\n\\tfloat columns:\\n{}'.format(float_cols))\nprint('\\n\\tto encode categorical columns:\\n{}'.format(cat_cols))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4e746c185aeb08455094aa3b12d2bb4ddb5db161"},"cell_type":"code","source":"print(X.shape)\nX.main_breed_Type[0:10] == X.Type[0:10]\n# same feature?!","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d8b271ef1dc20cb662082a86d3a32a0dc6d8a720"},"cell_type":"code","source":"# feature engineering:\n\n# Copy original X DF for easier experimentation,\n# all feature engineering will be performed on this one:\nX_temp = X.copy()\n\n\n# Select subsets of columns:\ntext_columns = ['Description', 'metadata_annots_top_desc', 'sentiment_entities']\ncategorical_columns = ['main_breed_BreedName', 'second_breed_BreedName']\n\n# Names are all unique, so they can be dropped by default\n# Same goes for PetID, it shouldn't be used as a feature\nto_drop_columns = ['PetID', 'Name', 'RescuerID']\n# RescuerID will also be dropped, as a feature based on this column will be extracted independently\n\n# Count RescuerID occurrences:\nrescuer_count = X.groupby(['RescuerID'])['PetID'].count().reset_index()\nrescuer_count.columns = ['RescuerID', 'RescuerID_COUNT']\n\n# Merge as another feature onto main DF:\nX_temp = X_temp.merge(rescuer_count, how='left', on='RescuerID')\n\n# Factorize categorical columns:\nfor i in categorical_columns:\n    X_temp.loc[:, i] = pd.factorize(X_temp.loc[:, i])[0]\n    \n# Subset text features:\nX_text = X_temp[text_columns]\n\nfor i in X_text.columns:\n    X_text.loc[:, i] = X_text.loc[:, i].fillna('<MISSING>')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f8560a8bc37c443a91eab5f6ec01c13777a74f62"},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.decomposition import SparsePCA, TruncatedSVD, LatentDirichletAllocation, NMF\n\nn_components = 5\ntext_features = []\n\n\n# Generate text features:\nfor i in X_text.columns:\n    \n    # Initialize decomposition methods:\n    print('generating features from: {}'.format(i))\n    svd_ = TruncatedSVD(\n        n_components=n_components, random_state=1337)\n    nmf_ = NMF(\n        n_components=n_components, random_state=1337)\n    \n    tfidf_col = TfidfVectorizer().fit_transform(X_text.loc[:, i].values)\n    svd_col = svd_.fit_transform(tfidf_col)\n    svd_col = pd.DataFrame(svd_col)\n    svd_col = svd_col.add_prefix('SVD_{}_'.format(i))\n    \n    nmf_col = nmf_.fit_transform(tfidf_col)\n    nmf_col = pd.DataFrame(nmf_col)\n    nmf_col = nmf_col.add_prefix('NMF_{}_'.format(i))\n    \n    text_features.append(svd_col)\n    text_features.append(nmf_col)\n    \n# Combine all extracted features:\ntext_features = pd.concat(text_features, axis=1)\n\n# Concatenate with main DF:\nX_temp = pd.concat([X_temp, text_features], axis=1)\n\n# Remove raw text columns:\nfor i in X_text.columns:\n    X_temp = X_temp.drop(i, axis=1)\n    \n# Remove unnecessary columns:\nX_temp = X_temp.drop(to_drop_columns, axis=1)\n\n# Check final df shape:\nprint('X shape: {}'.format(X_temp.shape))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"45f619514fca2b8585f777c08a13fc32f3d6a385"},"cell_type":"code","source":"print(X_temp.shape)\n# print(X_temp.keys())\nprint(X_temp.main_breed_Type[0:10] == X_temp.Type[0:10])\nX_temp.head(1)\n# X.Breed1[0:1]\n# X.main_breed_BreedName[0:1]\n# X.Breed2[0:1]\n# X.second_breed_BreedName[0:1]\n# still same feature?!","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"348c5dc5fe5c20dcc072223fd35fc68cca139cf4"},"cell_type":"code","source":"X_temp_column_types = X_temp.dtypes\n\nX_temp_int_cols = X_temp_column_types[X_temp_column_types == 'int']\nX_temp_float_cols = X_temp_column_types[X_temp_column_types == 'float']\nX_temp_cat_cols = X_temp_column_types[X_temp_column_types == 'object']\n\nprint('\\tinteger columns:\\n{}'.format(X_temp_int_cols))\nprint('\\n\\tfloat columns:\\n{}'.format(X_temp_float_cols))\nprint('\\n\\tto encode categorical columns:\\n{}'.format(X_temp_cat_cols))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1cda600fed7acf6381f342c484eb49fb4d92e854"},"cell_type":"code","source":"X_temp.tail(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d6efa713c96c7c47abea323267d3c7d33b1be252"},"cell_type":"code","source":"# train/test split:\n\n# Split into train and test again:\nX_train = X_temp.loc[np.isfinite(X_temp.AdoptionSpeed), :]\nX_test = X_temp.loc[~np.isfinite(X_temp.AdoptionSpeed), :]\n\n# Remove missing target column from test:\nX_test = X_test.drop(['AdoptionSpeed'], axis=1)\n\n\nprint('X_train shape: {}'.format(X_train.shape))\nprint('X_test shape: {}'.format(X_test.shape))\n\nassert X_train.shape[0] == train.shape[0]\nassert X_test.shape[0] == test.shape[0]\n\n\n# Check if columns between the two DFs are the same:\ntrain_cols = X_train.columns.tolist()\ntrain_cols.remove('AdoptionSpeed')\n\ntest_cols = X_test.columns.tolist()\n\nassert np.all(train_cols == test_cols)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1c91c8881e719e9f50f54e872335995b8494c68d"},"cell_type":"code","source":"# at this time, simply drop columns with missing values for GBM\ntrain_data = X_train.dropna(axis=1)\ntest_data = X_test.dropna(axis=1)\ntest_data = test_data.drop(['main_breed_Type'], axis=1)\nprint(train_data.shape)\nprint(test_data.shape)\n# train.keys() == test.keys()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"206c60cbcfe05c2e9b4e19bc0807e970924d0489"},"cell_type":"code","source":"# applying GBM\n# Gradient Boosting Classifier https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html#sklearn.svm.SVR\nfrom sklearn.datasets import make_hastie_10_2 \nfrom sklearn.ensemble import GradientBoostingClassifier\n\n# Plot\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4ab6d90c0a4492fdaa98eeecf66f5f608be3db50"},"cell_type":"code","source":"# splitting step - training set and validation set \nfrom sklearn.model_selection import train_test_split\ntrain_df, val_df = train_test_split(train_data, test_size = .2)\nprint (train_df.shape)\nprint (val_df.shape)\ntrain_df.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"201114ce82b7525ff6f4f5f34a567e812667409d"},"cell_type":"code","source":"# setting up targets (labels)\ntr_y = train_df[\"AdoptionSpeed\"]\ntr_x = train_df.drop(['AdoptionSpeed'], axis=1)\nval_y = val_df[\"AdoptionSpeed\"]\nval_x = val_df.drop(['AdoptionSpeed'], axis=1)\nprint(tr_x.shape)\nprint(tr_y.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"91270b3b6363d9ff0b976395d19c25c1c289b68e"},"cell_type":"code","source":"# # np.sum(pd.isnull(tr_x))\n# np.sum(pd.isnull(test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"df576709d703e02c02d7f7deb298bc748baac288"},"cell_type":"code","source":"# # test_temp = test\n# # data_without_missing_values = test_temp.dropna(axis=1)\n# np.sum(pd.isnull(test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a5d0c2bcd66591781f4f1c7720283779f44832b1"},"cell_type":"code","source":"# parameters setting for model\n\n# GradientBoostingClassifier(loss=’deviance’, learning_rate=0.1, n_estimators=100, \n#                            subsample=1.0, criterion=’friedman_mse’, min_samples_split=2, \n#                            min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_depth=3, \n#                            min_impurity_decrease=0.0, min_impurity_split=None, init=None, \n#                            random_state=None, max_features=None, verbose=0, max_leaf_nodes=None, \n#                            warm_start=False, presort=’auto’, validation_fraction=0.1, n_iter_no_change=None, tol=0.0001)\n\nclf = GradientBoostingClassifier(n_estimators=100, learning_rate=0.05, max_depth=5, random_state=0)\nclf.fit(tr_x, tr_y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6dd504b9b385ffe375cfbf32adb879c9c497f96d"},"cell_type":"code","source":"# prediction for validation data\nval_prediction = clf.predict(val_x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4fe8857c35128001198fd5b2e4f03053ecbbeaeb"},"cell_type":"code","source":"from sklearn.metrics import cohen_kappa_score\ncohen_kappa_score(val_y, val_prediction, weights = \"quadratic\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cc8186473f2238e9d20587b1ccdbc786b8d3d151"},"cell_type":"code","source":"# Plot feature importance https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html#sklearn.svm.SVR\nfeature_importance = clf.feature_importances_\n# make importances relative to max importance\nfeature_importance = 100.0 * (feature_importance / feature_importance.max())\nsorted_idx = np.argsort(feature_importance)\npos = np.arange(sorted_idx.shape[0]) + .5\n# plt.subplot(1, 2, 2)\nplt.figure(figsize=(8, 18))\nplt.barh(pos, feature_importance[sorted_idx], align='center')\nplt.yticks(pos, tr_x.keys()[sorted_idx])\nplt.xlabel('Relative Importance')\nplt.title('Variable Importance')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"49e7773d57d88dd8d1ad8325f6c5b43e74cffe22"},"cell_type":"code","source":"# tr_x.keys()\n# tr_x.main_breed_BreedName.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1d8c10c30720ffe48b3adc9d2fda698d4b750fee"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# # This Python 3 environment comes with many helpful analytics libraries installed\n# # It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# # For example, here's several helpful packages to load in \n\n# import numpy as np # linear algebra\n# import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# # Input data files are available in the \"../input/\" directory.\n# # For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\n# import os\n# print(os.listdir(\"../input\"))\n\n# # Any results you write to the current directory are saved as output.\n# import warnings\n# warnings.filterwarnings(\"ignore\")\n\n# # Gradient Boosting Classifier https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html#sklearn.svm.SVR\n# from sklearn.datasets import make_hastie_10_2 \n# from sklearn.ensemble import GradientBoostingClassifier\n\n# # Plot\n# import matplotlib.pyplot as plt\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"# # read data\n# breeds = pd.read_csv('../input/breed_labels.csv')\n# colors = pd.read_csv('../input/color_labels.csv')\n# states = pd.read_csv('../input/state_labels.csv')\n\n# train = pd.read_csv('../input/train/train.csv')\n# test = pd.read_csv('../input/test/test.csv')\n# sub = pd.read_csv('../input/test/sample_submission.csv')\n\n# train['dataset_type'] = 'train'\n# test['dataset_type'] = 'test'\n# all_data = pd.concat([train, test])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"040ed6ffea4d1d88d44a428de24d874e501a1eeb"},"cell_type":"code","source":"# # given shape\n# print(train.shape)\n# train.head(1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2466085354bc5faaee3202982866ec044bba6def"},"cell_type":"code","source":"# # drop some features\n# list_to_drop = [\"Name\", \"RescuerID\", \"Description\", \"PetID\", 'dataset_type']\n# train.drop(list_to_drop, axis = 1, inplace = True)\n# print(train.shape)\n# train.head(1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d64ad4c4a515c9f88da1cf84b10d779866f8a885"},"cell_type":"code","source":"# # splitting step - training set and validation set \n# from sklearn.model_selection import train_test_split\n# train_df, val_df = train_test_split(train, test_size = .2)\n# print (train_df.shape)\n# print (val_df.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a38cbdc0e9ffe7e2212d48332c54152ec51b3242"},"cell_type":"code","source":"# train_df.head(1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7140585eb09ef321ffd2d2c64af0f0332166b1fd"},"cell_type":"code","source":"# # setting up targets (labels)\n# tr_y = train_df[\"AdoptionSpeed\"]\n# tr_x = train_df.iloc[:,0:19]\n# val_y = val_df[\"AdoptionSpeed\"]\n# val_x = val_df.iloc[:,0:19]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bbab445b5b297c2dbf17e0b62e34015864039d03"},"cell_type":"code","source":"# # parameters setting for model\n\n# # GradientBoostingClassifier(loss=’deviance’, learning_rate=0.1, n_estimators=100, \n# #                            subsample=1.0, criterion=’friedman_mse’, min_samples_split=2, \n# #                            min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_depth=3, \n# #                            min_impurity_decrease=0.0, min_impurity_split=None, init=None, \n# #                            random_state=None, max_features=None, verbose=0, max_leaf_nodes=None, \n# #                            warm_start=False, presort=’auto’, validation_fraction=0.1, n_iter_no_change=None, tol=0.0001)\n\n# clf = GradientBoostingClassifier(n_estimators=100, learning_rate=0.05, max_depth=5, random_state=0)\n# clf.fit(tr_x, tr_y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ca2233c78cd48fd6c346fb35170d838127e911a7"},"cell_type":"code","source":"# # prediction for validation data\n# val_prediction = clf.predict(val_x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7c1cd590fdd3eac02bad1116dbc9dadd021f35c2"},"cell_type":"code","source":"# from sklearn.metrics import cohen_kappa_score\n# cohen_kappa_score(val_y, val_prediction, weights = \"quadratic\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e9fd4f28c51ddd1b9cc65af4093d17cd99f74124"},"cell_type":"code","source":"# # Plot feature importance https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html#sklearn.svm.SVR\n# feature_importance = clf.feature_importances_\n# # make importances relative to max importance\n# feature_importance = 100.0 * (feature_importance / feature_importance.max())\n# sorted_idx = np.argsort(feature_importance)\n# pos = np.arange(sorted_idx.shape[0]) + .5\n# # plt.subplot(1, 2, 2)\n# plt.figure(figsize=(12, 6))\n# plt.barh(pos, feature_importance[sorted_idx], align='center')\n# plt.yticks(pos, tr_x.keys()[sorted_idx])\n# plt.xlabel('Relative Importance')\n# plt.title('Variable Importance')\n# plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6fef6f9ef0c00f3f6a1079444170b4d228a97e72"},"cell_type":"code","source":"# for submission, we use all data (train + validation)\nall_x = pd.concat([tr_x, val_x])\nall_y = pd.concat([tr_y, val_y])\nprint (all_x.shape)\nprint (all_y.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"47eeb1c6fd1295c97bcde32ca1c2d022a7f75235"},"cell_type":"code","source":"# train again for submission purpose based on all data \nclf_submit = GradientBoostingClassifier(n_estimators=100, learning_rate=0.05, max_depth=5, random_state=0)\nclf_submit.fit(all_x, all_y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"46edffc446bc56bd9c35ca191e118e2c38ad99df"},"cell_type":"code","source":"# # make test data as input features\n# test.drop(list_to_drop, axis = 1, inplace = True)\n# # see input features\n# print(test.shape)\n# test.head(1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"04e194e55d10f3534bcf850d3544255aa7991ce7"},"cell_type":"code","source":"# # predction for all data\n# prediction = clf_submit.predict(test)\n# # Create submission data\n# submission = pd.DataFrame({'PetID': sub.PetID, 'AdoptionSpeed': [int(i) for i in prediction]})\n# print(submission.head())\n# # Create submission file\n# submission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2bffb4de7bb12dbaceba38033bf942106a18a5bc"},"cell_type":"code","source":"# test.keys()\n# X_test.keys()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"239bddf81c1cd97d3596acd05c02f22524ccc982"},"cell_type":"code","source":"# predction for all data\nprediction = clf_submit.predict(test_data)\n# Create submission data\nsubmission = pd.DataFrame({'PetID': test['PetID'].values, 'AdoptionSpeed': [int(i) for i in prediction]})\nprint(submission.head())\n# Create submission file\nsubmission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f1a0def27276832ff6a0d915ddd4fabbd69edc65"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"95a09076c8e8fd1e0ec5f7ac4ac651473c10d70f"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"13e93b09a9761aa70166088c00c8862207e4733c"},"cell_type":"code","source":"## in case for xgboost\n# import xgboost as xgb\n# # read in data\n# dtrain = xgb.DMatrix('demo/data/agaricus.txt.train')\n# dtest = xgb.DMatrix('demo/data/agaricus.txt.test')\n# # specify parameters via map\n# param = {'max_depth':2, 'eta':1, 'silent':1, 'objective':'binary:logistic' }\n# num_round = 2\n# bst = xgb.train(param, dtrain, num_round)\n# # make prediction\n# preds = bst.predict(dtest)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0da21f2204de3776c67a4d4f07b2f47f60848a2b"},"cell_type":"code","source":"# import xgboost as xgb\n# dtrain = xgb.DMatrix(tr_x, tr_y)\n# params = {\n#     'booster': \"gbtree\",\n#     'objective': 'multi:softmax',\n#     'eval_metric': 'merror',\n#     'eta' : 0.01,\n#     'lambda': 2.0,\n#     'alpha': 1.0,\n#     'lambda_bias': 6.0,\n#     'num_class': 5,\n# #     'n_jobs' : 4,\n#     'silent': 1,\n# #     'n_estimators':100\n#     'max_depth': 12\n# }\n\n# %time booster = xgb.train(params, dtrain, num_boost_round=100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e1393bbf121a0a28a24f182e92c422fb5b06ad54"},"cell_type":"code","source":"# dtest = xgb.DMatrix(val_x)\n# result = booster.predict(dtest)\n# from sklearn.metrics import cohen_kappa_score\n# cohen_kappa_score(val_y, result, weights = \"quadratic\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2ebccb77998ccd1adc010cebaabb23d777f95d39"},"cell_type":"code","source":"# n_estimators=1000, learning_rate=0.01, max_depth=7\n# 0.32785083936122394\n# n_estimators=500, learning_rate=0.01, max_depth=5\n# 0.32597593350337895\n# 0.30082779006306737\n# n_estimators=500, learning_rate=0.01, max_depth=10\n# 0.3031026793030307\n# n_estimators=100, learning_rate=0.05, max_depth=5\n# 0.33081184760966265\n# 0.3408957404611054\n# n_estimators=500, learning_rate=0.01, max_depth=7\n# 0.33095254066990065\n# 0.3202743639855826\n\n# 0.32387427192480744 (random state x)\n# 0.32099448579490575 (random state 1)\n# 0.32543273785368976 (random state 1 with 1000)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c836fdbb84132a2eedaeac9faab210aa01e37d3b"},"cell_type":"code","source":"# # clf = GradientBoostingClassifier(n_estimators=100, learning_rate=0.5, max_depth=7, random_state=0)\n# from sklearn.metrics import cohen_kappa_score\n# cohen_kappa_score(val_y, val_prediction, weights = \"quadratic\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a6d10a227d70b40dba17105ad835c5c3b3985f2c"},"cell_type":"code","source":"# # clf = GradientBoostingClassifier(n_estimators=500, learning_rate=0.5, max_depth=5, random_state=0)\n# from sklearn.metrics import cohen_kappa_score\n# cohen_kappa_score(val_y, val_prediction, weights = \"quadratic\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ef80a8a22538e7ad1be68cc7e3e4b6ffbc60d049"},"cell_type":"code","source":"# # clf = GradientBoostingClassifier(n_estimators=1000, learning_rate=0.5, max_depth=5, random_state=0)\n# from sklearn.metrics import cohen_kappa_score\n# cohen_kappa_score(val_y, val_prediction, weights = \"quadratic\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8f0effa471da7869ee70cd3007f1cbd66a79bb6c"},"cell_type":"code","source":"# # clf = GradientBoostingClassifier(n_estimators=150, learning_rate=0.5, max_depth=12, random_state=0)\n# from sklearn.metrics import cohen_kappa_score\n# cohen_kappa_score(val_y, val_prediction, weights = \"quadratic\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8597469584a765bf010908821595e1a5dcc6320a"},"cell_type":"code","source":"# # clf = GradientBoostingClassifier(n_estimators=100, learning_rate=0.5, max_depth=5, random_state=0)\n# from sklearn.metrics import cohen_kappa_score\n# cohen_kappa_score(val_y, val_prediction, weights = \"quadratic\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"903338b595c351e987ff59d07f58fa35e16a27ca"},"cell_type":"code","source":"# # Plot feature importance https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html#sklearn.svm.SVR\n# feature_importance = clf.feature_importances_\n# # make importances relative to max importance\n# feature_importance = 100.0 * (feature_importance / feature_importance.max())\n# sorted_idx = np.argsort(feature_importance)\n# pos = np.arange(sorted_idx.shape[0]) + .5\n# # plt.subplot(1, 2, 2)\n# plt.figure(figsize=(12, 6))\n# plt.barh(pos, feature_importance[sorted_idx], align='center')\n# plt.yticks(pos, tr_x.keys()[sorted_idx])\n# plt.xlabel('Relative Importance')\n# plt.title('Variable Importance')\n# plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"790dcf9c2db3eb1df4e95bc1e6b030fe7ba695b2"},"cell_type":"code","source":"# # for submission, we use all data (train + validation)\n# all_x = pd.concat([tr_x, val_x])\n# all_y = pd.concat([tr_x, val_y])\n# print (all_x)\n# print (all_y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"07eb2a132eaec0232c0017551d0499c0a9d16add"},"cell_type":"code","source":"# # train again for submission purpose based on all data \n# clf_submit = GradientBoostingClassifier(n_estimators=500, learning_rate=0.01, max_depth=7, random_state=1)\n# clf_submit.fit(tr_x, tr_y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7db2dae754e8afc7d47c2037d464f8ab54a73bff"},"cell_type":"code","source":"# # make test data as input features\n# test.drop(list_to_drop, axis = 1, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fb41ef0526c3d6c2c0c1fd8e9906d369ec6e9fda"},"cell_type":"code","source":"# # see input features\n# test.head(1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d64d64d879f58b929b177f635fe28cf335c4992e"},"cell_type":"code","source":"# # prediction\n# # prediction = clf.predict(test)\n# predction for all data\n# prediction = clf_submit.predict(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a4ebf61a354661055b7e81cad0f7f70acd266eb2"},"cell_type":"code","source":"# # Create submission data\n# submission = pd.DataFrame({'PetID': sub.PetID, 'AdoptionSpeed': [int(i) for i in prediction]})\n# submission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f7ae40a7836bee8d4aceb066bc60cbbecdf0ace2"},"cell_type":"code","source":"# # Create submission file\n# submission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"abec70f961bd11f2e577bc527b82916f90f62d2c"},"cell_type":"code","source":"# print(train.iloc[:,[0,2,3,4,5,6,7,8,9,11,12,13,14,19,22]].shape)\n# train.iloc[:,[0,2,3,4,5,6,7,8,9,11,12,13,14,19,22]][0:10]\n# # tr_y = (train.iloc[:,[23]]).values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"32a18c5d53d5f366d0f46592ece9c9f2685b5caf"},"cell_type":"code","source":"# print(train.shape)\n# pr = int(train.shape[0]*0.8)\n# print(pr, '+', train.shape[0] - pr, '=', train.shape[0])\n# tr = train[0:pr]\n# val = train[pr:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6190f861cbacf1272e994921b2ea0743d72ac778"},"cell_type":"code","source":"# # tr_y = (train.iloc[:,[23]]).values\n# train.Breed1.values\n# train.Breed2.values\n# # just go for breed1\n# TR_BR = train.iloc[:,[0,3,23]].values\n# TR_BR\n\n# # TR_BR[0][1]\n# # for i in range(len(TR_BR)):\n# #     for j in range(len(BRS_cn)):\n# #         if (TR_BR[i][0] == BRS_cn[j][1]) and (TR_BR[i][1] == BRS_cn[j][0]):\n# #             BRS_cn[j][TR_BR[i][2]+2] += 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1b832a72aa2134e8d8fb1869744cca73eebcdeb8"},"cell_type":"code","source":"# target encoding....","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f7eae815a5209b7f8c038efa3830ff7a8a47f1de"},"cell_type":"code","source":"# breeds[210:270]\n# breeds[0:10]\n# BRS = breeds.iloc[:,[0,1]].values\n# # BRS.shape\n# # np.zeros([BRS.shape[0]]).shape\n# BRS_cn = np.concatenate((BRS,np.zeros([BRS.shape[0],5])), axis=1)\n# print(BRS_cn.shape)\n# BRS_cn","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"555a44c6cfdb8ab7af7d51d50ed7ac6f776249d8"},"cell_type":"code","source":"# for i in range(len(TR_BR)):\n#     for j in range(len(BRS_cn)):\n#         if (TR_BR[i][0] == BRS_cn[j][1]) and (TR_BR[i][1] == BRS_cn[j][0]):\n#             BRS_cn[j][TR_BR[i][2]+2] += 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1774219b24632cc3a277e9c136e9c2238db3b072"},"cell_type":"code","source":"# MT[0][0]*0 + MT[0][1]*1 + MT[0][2]*2 + MT[0][3]*3 + MT[0][4]*4\n# sum","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3b20b925b7357ba406c55f22e58ddafca3fa0c53"},"cell_type":"code","source":"# Mean = np.zeros([BRS_cn[:,2:].shape[0]])\n# MT = BRS_cn[:,2:]\n# MT\n# for k in range(len(MT)):\n#     if (MT[k][0] + MT[k][1] + MT[k][2] + MT[k][3] + MT[k][4]) == 0:\n#         Mean[k] = 2\n#     else:\n#         Mean[k] = (MT[k][0]*0 + MT[k][1]*1 + MT[k][2]*2 + MT[k][3]*3 + MT[k][4]*4) / (MT[k][0] + MT[k][1] + MT[k][2] + MT[k][3] + MT[k][4])\n    \n\n# # for i in range(len(BRS_cn)):\n# #     for j in range\n# #     if BRS_cn[i][0] == ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"55b0797058e75eaa88339c9e8004e244e97efce8"},"cell_type":"code","source":"# Mean.reshape(Mean.shape[0],1)\n# # BRS_1 = np.concatenate((BRS,Mean), axis=1)\n# BRS_1 = np.concatenate((BRS,Mean.reshape(Mean.shape[0],1)), axis=1)\n# BRS_1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c5f3ec4761fc132d14c39a3b1cd0331c8c313c1b"},"cell_type":"code","source":"# # TR_BR.shape[0]\n# BR_1 = np.zeros([TR_BR.shape[0],1])\n# for i in range(len(TR_BR)):\n#     for j in range(len(BRS_1)):\n#         if (TR_BR[i][0] == BRS_1[j][1]) and (TR_BR[i][1] == BRS_1[j][0]):\n#             BR_1[i] = BRS_1[j][2]\n# #             BRS_cn[j][TR_BR[i][2]+2] += 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false,"_uuid":"65150924336a4f8a6ee9cd8c4990f4138ea78405"},"cell_type":"code","source":"# print(BR_1.shape)\n# BR_1[0:100]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d92922461a695aa34a390ac71a366dfc26e15ada"},"cell_type":"code","source":"# import matplotlib.pyplot as plt\n# # rng = np.random.RandomState(10)  # deterministic random data\n# # a = np.hstack((BR_1.normal(size=1000),BR_1.normal(loc=5, scale=2, size=1000)))\n# plt.figure(figsize=(15, 6))\n# plt.hist(BR_1, bins='auto')  # arguments are passed to np.histogram\n# plt.title(\"Histogram with 'auto' bins\")\n# plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e7e866015242bc4d9a2c9bb01e85573fd6a813f3"},"cell_type":"code","source":"# import matplotlib.pyplot as plt\n# # rng = np.random.RandomState(10)  # deterministic random data\n# # a = np.hstack((BR_1.normal(size=1000),BR_1.normal(loc=5, scale=2, size=1000)))\n# plt.figure(figsize=(15, 6))\n# plt.hist(train.iloc[:,[3]].values, bins='auto')  # arguments are passed to np.histogram\n# plt.title(\"Histogram with 'auto' bins\")\n# plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d4c5ab7d3d2115d04c6a26986bdf22b0c14d17ef"},"cell_type":"code","source":"# new_train = np.concatenate((train.iloc[:,[2,4,5,6,7,8,9,11,12,13,14,19,22]].values,BR_1), axis=1)\n# # print(new_train.shape)\n# # print(new_train[0])\n# # train.iloc[:,[2,3,4,5,6,7,8,9,11,12,13,14,19,22]].head(1)\n# ## train.iloc[:,[0,2,3,4,5,6,7,8,9,11,12,13,14,19,22]].shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"06fe8c9a929d7715f1497d9a80530c2abf03d55a"},"cell_type":"code","source":"# print(train.shape)\n# pr = int(train.shape[0]*0.8)\n# print(pr, '+', train.shape[0] - pr, '=', train.shape[0])\n# tr = train[0:pr]\n# val = train[pr:]\n\n# new_tr = train[0:pr]\n# new_val = train[pr:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cc740c75f55c3546c565f00027c99bd823f5f8bb"},"cell_type":"code","source":"# # train input data (x) and train labels (y), at this time no seperation for validating, no normalization\n# # tr_x = train.iloc[:,[2,3,4,5,6,7,8,9,11,12,13,14,19,22]].values\n# tr_y = (train.iloc[:,[23]]).values\n\n# # features as input\n# # 1. Age - Age of pet when listed, in months\n# # 2. Breed1 - Primary breed of pet (Refer to BreedLabels dictionary)\n# # 3. Breed2 - Secondary breed of pet, if pet is of mixed breed (Refer to BreedLabels dictionary)\n# # 4. Gender - Gender of pet (1 = Male, 2 = Female, 3 = Mixed, if profile represents group of pets)\n# # 5. Color1 - Color 1 of pet (Refer to ColorLabels dictionary)\n# # 6. Color2 - Color 2 of pet (Refer to ColorLabels dictionary)\n# # 7. Color3 - Color 3 of pet (Refer to ColorLabels dictionary)\n# # 8. MaturitySize - Size at maturity (1 = Small, 2 = Medium, 3 = Large, 4 = Extra Large, 0 = Not Specified)\n# # 9. Vaccinated - Pet has been vaccinated (1 = Yes, 2 = No, 3 = Not Sure)\n# # 10. Dewormed - Pet has been dewormed (1 = Yes, 2 = No, 3 = Not Sure)\n# # 11. Sterilized - Pet has been spayed / neutered (1 = Yes, 2 = No, 3 = Not Sure)\n# # 12. Health - Health Condition (1 = Healthy, 2 = Minor Injury, 3 = Serious Injury, 0 = Not Specified)\n# # 13. VideoAmt - Total uploaded videos for this pet\n# # 14. PhotoAmt - Total uploaded photos for this pet","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f4a2350dcbcd838b1d80a44b4e8673dc95fefec1"},"cell_type":"code","source":"# # Training for Gradient Boosting Classifier\n# # clf = GradientBoostingClassifier(n_estimators=150, learning_rate=0.5, max_depth=12, random_state=0).fit(tr_x, tr_y)\n# clf = GradientBoostingClassifier(n_estimators=150, learning_rate=0.5, max_depth=12, random_state=0).fit(new_train, tr_y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ecec2988f4b5d901f0c0ce12433f0a9c5c025e8c"},"cell_type":"code","source":"# # Plot feature importance https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html#sklearn.svm.SVR\n# feature_importance = clf.feature_importances_\n# # make importances relative to max importance\n# feature_importance = 100.0 * (feature_importance / feature_importance.max())\n# sorted_idx = np.argsort(feature_importance)\n# pos = np.arange(sorted_idx.shape[0]) + .5\n# # plt.subplot(1, 2, 2)\n# plt.figure(figsize=(12, 6))\n# plt.barh(pos, feature_importance[sorted_idx], align='center')\n# plt.yticks(pos, train.iloc[:,[2,4,5,6,7,8,9,11,12,13,14,19,22,3]].keys()[sorted_idx])\n# plt.xlabel('Relative Importance')\n# plt.title('Variable Importance')\n# plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"88201ec258384193939862d7dcb17348dd1b3843"},"cell_type":"code","source":"# train.iloc[:,[2,4,5,6,7,8,9,11,12,13,14,19,22,3]].keys()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f16c80d210e310b68a03f7515409eb4e14008270"},"cell_type":"code","source":"# >>> from sklearn.metrics import cohen_kappa_score\n# >>> y_true = [2, 0, 2, 2, 0, 1]\n# >>> y_pred = [0, 0, 2, 2, 0, 2]\n# >>> cohen_kappa_score(y_true, y_pred)\n# 0.4285714285714286","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"39c050188f7b7ff3122f8ee0e3a0fc99c2f1a7ed"},"cell_type":"code","source":"# This is for Target Encoding Results [only for Breed1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"de8741e2c939b4142f51608b8b9f2b6db7b5972f"},"cell_type":"code","source":"# # # TR_BR.shape[0]\n# # BR_1 = np.zeros([TR_BR.shape[0],1])\n# # for i in range(len(TR_BR)):\n# #     for j in range(len(BRS_1)):\n# #         if (TR_BR[i][0] == BRS_1[j][1]) and (TR_BR[i][1] == BRS_1[j][0]):\n# #             BR_1[i] = BRS_1[j][2]\n# # #             BRS_cn[j][TR_BR[i][2]+2] += 1\n\n# # TR_BR.shape[0]\n# BR_1 = np.zeros([TR_BR.shape[0],1])\n# for i in range(len(TR_BR)):\n#     for j in range(len(BRS_1)):\n#         if (TR_BR[i][0] == BRS_1[j][1]) and (TR_BR[i][1] == BRS_1[j][0]):\n#             BR_1[i] = BRS_1[j][2]\n# #             BRS_cn[j][TR_BR[i][2]+2] += 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3e98d06569c2f310f5d5a69a6b984f541fb00a52"},"cell_type":"code","source":"# # TE_BR = test.iloc[:,[0,3,23]].values\n# # TE_BR\n# TE_BR = test.iloc[:,[0,3]].values\n# TE_BR_1 = np.zeros([TE_BR.shape[0],1])\n# for i in range(len(TE_BR)):\n#     for j in range(len(BRS_1)):\n#         if (TE_BR[i][0] == BRS_1[j][1]) and (TE_BR[i][1] == BRS_1[j][0]):\n#             TE_BR_1[i] = BRS_1[j][2]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0e046d706e39ca44551b7065632e1bad383cd00a"},"cell_type":"code","source":"# TE_BR_1[0:1000]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"73c50c114fd4a5d076eaa3b3de0aec69eb71f7ab"},"cell_type":"code","source":"# new_test = np.concatenate((test.iloc[:,[2,4,5,6,7,8,9,11,12,13,14,19,22]].values,TE_BR_1), axis=1)\n# prediction = clf.predict(new_test)\n# # clf.score(tr_x, tr_y) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e857f30c09c34f51353f15b51b146bd33389b5f8"},"cell_type":"code","source":"# # For submission\n# test_x = test.iloc[:,[2,3,4,5,6,7,8,9,11,12,13,14,19,22]].values\n# prediction = clf.predict(test_x)\n# clf.score(tr_x, tr_y) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b6c196e74a2b1203bdd19ce926c8a3526e445c44"},"cell_type":"code","source":"# # Create submission data\n# submission = pd.DataFrame({'PetID': sub.PetID, 'AdoptionSpeed': [int(i) for i in prediction]})\n# submission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fa62cf0cfe0f549d00d90fcd4a24a04444ccd1f9"},"cell_type":"code","source":"# # Create submission file\n# submission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c32a88666a4b6d3be503d81383a834294999b1c4"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}