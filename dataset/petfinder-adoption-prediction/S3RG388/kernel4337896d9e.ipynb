{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"scrolled":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nfrom sklearn import preprocessing\nimport json\nfrom pandas.io.json import json_normalize\nimport seaborn as sns # Beautiful plots\n\n## Kfold for cross-validation\nfrom sklearn.model_selection import KFold, StratifiedKFold\n\n## Classifier of XGBosst\nfrom   xgboost import XGBClassifier\n##import xgboost as xgb\n\n## Package used for fine tuning\nfrom hyperopt import hp, fmin, tpe, STATUS_OK, Trials\n\n# Any results you write to the current directory are saved as output.\nimport os\nprint(os.listdir(\"../input/\"))\n\nimport nltk\nimport string\nfrom gensim.models import word2vec\nfrom tqdm import tqdm\nfrom keras.preprocessing.text import text_to_word_sequence\nfrom nltk.corpus import stopwords","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e2a62322cba1f63a12612d2d092f6a942654977c","scrolled":true},"cell_type":"code","source":"trainframe = pd.read_csv('../input/train/train.csv')\ntestframe = pd.read_csv('../input/test/test.csv')\n\ntrain_id = trainframe['PetID']\ntest_id = testframe['PetID']\n\ntrainframe.AdoptionSpeed.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b3634d5bf5327d8082f36d23604831aa6af98a8b"},"cell_type":"code","source":"## Feature engineering for simpler features\n\n## Create a list of bins for the state variable\nbins = trainframe[\"State\"].value_counts().index.tolist()\nbins.sort()\nbins\ntrainframe['State_Binned'] = pd.cut(trainframe['State'], bins)\nlabels = list(range(0,len(bins)-1))\nlabels\ntrainframe['State_Binned'] = pd.cut(trainframe['State'], bins=bins, labels=labels)\n\n## Clipping the amount of photos tail using a cut-off value of 5\nmaxVal = 5\ntrainframe['PhotoAmt_Clipped'] = trainframe['PhotoAmt'].where(trainframe['PhotoAmt'] <= maxVal, maxVal)\n\n## Clipping the amount of videos to no video - 0 or one or more videos - 1\nmaxVal = 1\ntrainframe['VideoAmt_Clipped'] = trainframe['VideoAmt'].where(trainframe['VideoAmt'] <= maxVal, maxVal)\n\n## Clipping the quantity of pets with a cut-off of 5\nmaxVal = 5\ntrainframe['Quantity_Clipped'] = trainframe['Quantity'].where(trainframe['Quantity'] <= maxVal, maxVal)\n\n## Normalizing breed labels and fee column into a -1 to 1 distribution\ntrainframe[\"Breed1_Normalized\"] = (trainframe[\"Breed1\"] - trainframe[\"Breed1\"].mean()) / (trainframe[\"Breed1\"].max() - trainframe[\"Breed1\"].min())\ntrainframe[\"Breed2_Normalized\"] = (trainframe[\"Breed2\"] - trainframe[\"Breed2\"].mean()) / (trainframe[\"Breed2\"].max() - trainframe[\"Breed2\"].min())\ntrainframe[\"Fee_Normalized\"] = (trainframe[\"Fee\"] - trainframe[\"Fee\"].mean()) / (trainframe[\"Fee\"].max() - trainframe[\"Fee\"].min())\n\n## Transform the age feature into years to avoid large values\ntrainframe[\"Age_Years\"] = (trainframe[\"Age\"] / 12).round(1)\n\n## Set PetID as Index\n\ntrainframe = trainframe.set_index(\"PetID\")\n\ntrainframe.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"603949b30bfdeae4667cd9efcd29191e3bb53700"},"cell_type":"code","source":"## Feature engineering for simpler features\n## For testing\n\n## Create a list of bins for the state variable\nbins = testframe[\"State\"].value_counts().index.tolist()\nbins.sort()\nbins\ntestframe['State_Binned'] = pd.cut(testframe['State'], bins)\nlabels = list(range(0,len(bins)-1))\nlabels\ntestframe['State_Binned'] = pd.cut(testframe['State'], bins=bins, labels=labels)\n\n## Clipping the amount of photos tail using a cut-off value of 5\nmaxVal = 5\ntestframe['PhotoAmt_Clipped'] = testframe['PhotoAmt'].where(testframe['PhotoAmt'] <= maxVal, maxVal)\n\n## Clipping the amount of videos to no video - 0 or one or more videos - 1\nmaxVal = 1\ntestframe['VideoAmt_Clipped'] = testframe['VideoAmt'].where(testframe['VideoAmt'] <= maxVal, maxVal)\n\n## Clipping the quantity of pets with a cut-off of 5\nmaxVal = 5\ntestframe['Quantity_Clipped'] = testframe['Quantity'].where(testframe['Quantity'] <= maxVal, maxVal)\n\n## Normalizing breed labels and fee column into a -1 to 1 distribution\ntestframe[\"Breed1_Normalized\"] = (testframe[\"Breed1\"] - testframe[\"Breed1\"].mean()) / (testframe[\"Breed1\"].max() - testframe[\"Breed1\"].min())\ntestframe[\"Breed2_Normalized\"] = (testframe[\"Breed2\"] - testframe[\"Breed2\"].mean()) / (testframe[\"Breed2\"].max() - testframe[\"Breed2\"].min())\ntestframe[\"Fee_Normalized\"] = (testframe[\"Fee\"] - testframe[\"Fee\"].mean()) / (testframe[\"Fee\"].max() - testframe[\"Fee\"].min())\n\n## Transform the age feature into years to avoid large values\ntestframe[\"Age_Years\"] = (testframe[\"Age\"] / 12).round(1)\n\n## Set PetID as Index\n\ntestframe = testframe.set_index(\"PetID\")\n\ntestframe.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d58ffabed280606cea8402a3f3db460be8f79475","scrolled":true},"cell_type":"code","source":"## Add image data - courtesy of Peter Hurford's Kernel found at\n# https://www.kaggle.com/peterhurford/pets-lightgbm-baseline-with-all-the-data\n\n\nvertex_xs = []\nvertex_ys = []\nbounding_confidences = []\nbounding_importance_fracs = []\ndominant_blues = []\ndominant_greens = []\ndominant_reds = []\ndominant_pixel_fracs = []\ndominant_scores = []\nlabel_descriptions = []\nlabel_scores = []\nnf_count = 0\nnl_count = 0\nfor pet in train_id:\n    try:\n        with open('../input/train_metadata/' + pet + '-1.json', 'r') as f:\n            data = json.load(f)\n        vertex_x = data['cropHintsAnnotation']['cropHints'][0]['boundingPoly']['vertices'][2]['x']\n        vertex_xs.append(vertex_x)\n        vertex_y = data['cropHintsAnnotation']['cropHints'][0]['boundingPoly']['vertices'][2]['y']\n        vertex_ys.append(vertex_y)\n        bounding_confidence = data['cropHintsAnnotation']['cropHints'][0]['confidence']\n        bounding_confidences.append(bounding_confidence)\n        bounding_importance_frac = data['cropHintsAnnotation']['cropHints'][0].get('importanceFraction', -1)\n        bounding_importance_fracs.append(bounding_importance_frac)\n        dominant_blue = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['color']['blue']\n        dominant_blues.append(dominant_blue)\n        dominant_green = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['color']['green']\n        dominant_greens.append(dominant_green)\n        dominant_red = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['color']['red']\n        dominant_reds.append(dominant_red)\n        dominant_pixel_frac = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['pixelFraction']\n        dominant_pixel_fracs.append(dominant_pixel_frac)\n        dominant_score = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['score']\n        dominant_scores.append(dominant_score)\n        if data.get('labelAnnotations'):\n            label_description = data['labelAnnotations'][0]['description']\n            label_descriptions.append(label_description)\n            label_score = data['labelAnnotations'][0]['score']\n            label_scores.append(label_score)\n        else:\n            nl_count += 1\n            label_descriptions.append('nothing')\n            label_scores.append(-1)\n    except FileNotFoundError:\n        nf_count += 1\n        vertex_xs.append(-1)\n        vertex_ys.append(-1)\n        bounding_confidences.append(-1)\n        bounding_importance_fracs.append(-1)\n        dominant_blues.append(-1)\n        dominant_greens.append(-1)\n        dominant_reds.append(-1)\n        dominant_pixel_fracs.append(-1)\n        dominant_scores.append(-1)\n        label_descriptions.append('nothing')\n        label_scores.append(-1)\n\nprint(nf_count)\nprint(nl_count)\ntrainframe.loc[:, 'vertex_x'] = vertex_xs\ntrainframe.loc[:, 'vertex_y'] = vertex_ys\ntrainframe.loc[:, 'bounding_confidence'] = bounding_confidences\ntrainframe.loc[:, 'bounding_importance'] = bounding_importance_fracs\ntrainframe.loc[:, 'dominant_blue'] = dominant_blues\ntrainframe.loc[:, 'dominant_green'] = dominant_greens\ntrainframe.loc[:, 'dominant_red'] = dominant_reds\ntrainframe.loc[:, 'dominant_pixel_frac'] = dominant_pixel_fracs\ntrainframe.loc[:, 'dominant_score'] = dominant_scores\ntrainframe.loc[:, 'label_description'] = label_descriptions\ntrainframe.loc[:, 'label_score'] = label_scores\n\n\nvertex_xs = []\nvertex_ys = []\nbounding_confidences = []\nbounding_importance_fracs = []\ndominant_blues = []\ndominant_greens = []\ndominant_reds = []\ndominant_pixel_fracs = []\ndominant_scores = []\nlabel_descriptions = []\nlabel_scores = []\nnf_count = 0\nnl_count = 0\nfor pet in test_id:\n    try:\n        with open('../input/test_metadata/' + pet + '-1.json', 'r') as f:\n            data = json.load(f)\n        vertex_x = data['cropHintsAnnotation']['cropHints'][0]['boundingPoly']['vertices'][2]['x']\n        vertex_xs.append(vertex_x)\n        vertex_y = data['cropHintsAnnotation']['cropHints'][0]['boundingPoly']['vertices'][2]['y']\n        vertex_ys.append(vertex_y)\n        bounding_confidence = data['cropHintsAnnotation']['cropHints'][0]['confidence']\n        bounding_confidences.append(bounding_confidence)\n        bounding_importance_frac = data['cropHintsAnnotation']['cropHints'][0].get('importanceFraction', -1)\n        bounding_importance_fracs.append(bounding_importance_frac)\n        dominant_blue = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['color']['blue']\n        dominant_blues.append(dominant_blue)\n        dominant_green = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['color']['green']\n        dominant_greens.append(dominant_green)\n        dominant_red = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['color']['red']\n        dominant_reds.append(dominant_red)\n        dominant_pixel_frac = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['pixelFraction']\n        dominant_pixel_fracs.append(dominant_pixel_frac)\n        dominant_score = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['score']\n        dominant_scores.append(dominant_score)\n        if data.get('labelAnnotations'):\n            label_description = data['labelAnnotations'][0]['description']\n            label_descriptions.append(label_description)\n            label_score = data['labelAnnotations'][0]['score']\n            label_scores.append(label_score)\n        else:\n            nl_count += 1\n            label_descriptions.append('nothing')\n            label_scores.append(-1)\n    except FileNotFoundError:\n        nf_count += 1\n        vertex_xs.append(-1)\n        vertex_ys.append(-1)\n        bounding_confidences.append(-1)\n        bounding_importance_fracs.append(-1)\n        dominant_blues.append(-1)\n        dominant_greens.append(-1)\n        dominant_reds.append(-1)\n        dominant_pixel_fracs.append(-1)\n        dominant_scores.append(-1)\n        label_descriptions.append('nothing')\n        label_scores.append(-1)\n\nprint(nf_count)\ntestframe.loc[:, 'vertex_x'] = vertex_xs\ntestframe.loc[:, 'vertex_y'] = vertex_ys\ntestframe.loc[:, 'bounding_confidence'] = bounding_confidences\ntestframe.loc[:, 'bounding_importance'] = bounding_importance_fracs\ntestframe.loc[:, 'dominant_blue'] = dominant_blues\ntestframe.loc[:, 'dominant_green'] = dominant_greens\ntestframe.loc[:, 'dominant_red'] = dominant_reds\ntestframe.loc[:, 'dominant_pixel_frac'] = dominant_pixel_fracs\ntestframe.loc[:, 'dominant_score'] = dominant_scores\ntestframe.loc[:, 'label_description'] = label_descriptions\ntestframe.loc[:, 'label_score'] = label_scores\n\ntrainframe.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"423f0463717ceb9c283f261b84c41137931d8883","scrolled":true},"cell_type":"code","source":"## Create a dataframe with total sentiment scores for all available descriptions; total score for each description = total description score * total description magnitude\n\ntrainlist = []\nfor i in os.listdir(\"../input/train_sentiment/\"):\n    with open('../input/train_sentiment/' + str(i)) as f:\n        data = json.load(f)\n        score = data.get('documentSentiment').get('magnitude') * data.get('documentSentiment').get('score')\n        trainlist.append((str(i).replace('.json',''), score))\n\nsentiment_scores = pd.DataFrame(trainlist, columns=['PetID','Total_Sentiment_Score']).set_index('PetID')\n\n## Add sentiment scores into the dataframe\ntrainframe = sentiment_scores.join(trainframe, how='outer')\n## Replace missing sentiment scores with neutral 0s\ntrainframe.fillna(0, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"39fbbaaab83d837f06c54bb6ea58a8d50011e8d1"},"cell_type":"code","source":"## Create a dataframe with total sentiment scores for all available descriptions; total score for each description = total description score * total description magnitude\n## For testing\n\ntestlist = []\nfor i in os.listdir(\"../input/test_sentiment/\"):\n    with open('../input/test_sentiment/' + str(i)) as f:\n        data = json.load(f)\n        score = data.get('documentSentiment').get('magnitude') * data.get('documentSentiment').get('score')\n        testlist.append((str(i).replace('.json',''), score))\n\nsentiment_scores = pd.DataFrame(testlist, columns=['PetID','Total_Sentiment_Score']).set_index('PetID')\n\n## Add sentiment scores into the dataframe\ntestframe = sentiment_scores.join(testframe, how='outer')\n## Replace missing sentiment scores with neutral 0s\ntestframe.fillna(0, inplace=True)\n\ntestframe.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"be36dc14720311bc3480e711800f1a01fbe3a253"},"cell_type":"code","source":"## Normalize numeric features into a single normalized frame\n\n## Map numeric features + engineered features\ntrainframe_all = trainframe[['Total_Sentiment_Score','Type','Age_Years','Breed1_Normalized','Breed2_Normalized',\n                             'Gender','Color1','Color2','Color3','MaturitySize','FurLength','Vaccinated','Dewormed','Sterilized','Health',\n                             'Quantity_Clipped','Fee','Fee_Normalized','PhotoAmt_Clipped','AdoptionSpeed',\n                             'vertex_x','vertex_y','bounding_confidence', 'bounding_importance',\n                             'dominant_blue', 'dominant_green', 'dominant_red','dominant_pixel_frac',\n                             'dominant_score','label_score', 'Breed1', 'Breed2']]\n\n## Map labels into a separate dataset\ntrain_labels = pd.DataFrame(trainframe, columns=['AdoptionSpeed'])\n\n## Reset index for training features to prepare for normalization and re-indexation\ntrainframe_all = trainframe_all.reset_index()\n\n## Create new normalized frames for normalization\nnormalized_frame_all = pd.DataFrame()\nnormalized_frame_all = trainframe_all\n\n\n##Run normalization on both training features excluding non-numeric IDs\n\n##Add column names\n#columnNames = list(trainframe_all.head(0))\n\n#for i in columnNames:\n   # if i != 'PetID' and i != 'AdoptionSpeed':\n      #  normalized_frame_all[i] = preprocessing.scale(trainframe_all[i].astype('float64'))\n\n## Add IDs and re-index both normalized feature sets to prepare for label merging\n\n#normalized_frame_all['PetID'] = trainframe_all['PetID']\n#normalized_frame_all = normalized_frame_all.set_index('PetID')\n\n#normalized_frame_all.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d398cbe17e3f355c1e3e898be15da287aca1b7f0"},"cell_type":"code","source":"## Engineer feature crosses for the train set\n\nnormalized_frame_all['Breed1 x MaturitySize'] = normalized_frame_all['Breed1'] *  normalized_frame_all['MaturitySize']\nnormalized_frame_all['Breed1 x Gender'] = normalized_frame_all['Breed1'] *  normalized_frame_all['Gender']\nnormalized_frame_all['MaturitySize x Gender'] = normalized_frame_all['MaturitySize'] *  normalized_frame_all['Gender']\nnormalized_frame_all['Type x Gender'] = normalized_frame_all['Type'] *  normalized_frame_all['Gender']\nnormalized_frame_all['Type x MaturitySize'] = normalized_frame_all['Type'] *  normalized_frame_all['MaturitySize']\nnormalized_frame_all['Type x FurLength'] = normalized_frame_all['Type'] *  normalized_frame_all['FurLength']\nnormalized_frame_all['Gender x FurLength'] = normalized_frame_all['Gender'] *  normalized_frame_all['FurLength']\nnormalized_frame_all['Type x Health'] = normalized_frame_all['Type'] *  normalized_frame_all['Health']\nnormalized_frame_all['Fee x Health'] = normalized_frame_all['Fee'] *  normalized_frame_all['Health']\nnormalized_frame_all['Vaccinated x Dewormed x Sterilized'] = normalized_frame_all['Vaccinated'] *  normalized_frame_all['Dewormed'] * normalized_frame_all['Sterilized']\nnormalized_frame_all['Type x Color1'] = normalized_frame_all['Type'] *  normalized_frame_all['Color1']\nnormalized_frame_all['Breed1 x Color1'] = normalized_frame_all['Breed1'] *  normalized_frame_all['Color1']\nnormalized_frame_all['Type x Age_Years'] = normalized_frame_all['Type'] *  normalized_frame_all['Age_Years']\nnormalized_frame_all['Health x Age_Years'] = normalized_frame_all['Health'] * normalized_frame_all['Age_Years']\nnormalized_frame_all['Fee x Age_Years'] = normalized_frame_all['Fee'] * normalized_frame_all['Age_Years']\n\n##Round 2 of engineering\nnormalized_frame_all['Total_Sentiment_Score x Health'] = normalized_frame_all['Total_Sentiment_Score'] * normalized_frame_all['Health']\nnormalized_frame_all['Total_Sentiment_Score x Gender'] = normalized_frame_all['Total_Sentiment_Score'] * normalized_frame_all['Gender']\nnormalized_frame_all['Total_Sentiment_Score x Breed1'] = normalized_frame_all['Total_Sentiment_Score'] * normalized_frame_all['Breed1']\nnormalized_frame_all['Total_Sentiment_Score x PhotoAmt_Clipped'] = normalized_frame_all['Total_Sentiment_Score'] * normalized_frame_all['PhotoAmt_Clipped']\nnormalized_frame_all['Total_Sentiment_Score x Fee'] = normalized_frame_all['Total_Sentiment_Score'] * normalized_frame_all['Fee']\nnormalized_frame_all['PhotoAmt_Clipped x Health'] = normalized_frame_all['PhotoAmt_Clipped'] * normalized_frame_all['Health']\nnormalized_frame_all['PhotoAmt_Clipped x Breed1'] = normalized_frame_all['PhotoAmt_Clipped'] * normalized_frame_all['Breed1']\nnormalized_frame_all['PhotoAmt_Clipped x MaturitySize'] = normalized_frame_all['PhotoAmt_Clipped'] * normalized_frame_all['MaturitySize']\nnormalized_frame_all['PhotoAmt_Clipped x Gender'] = normalized_frame_all['PhotoAmt_Clipped'] * normalized_frame_all['Gender']\nnormalized_frame_all['Fee x PhotoAmt_Clipped'] = normalized_frame_all['Fee'] * normalized_frame_all['PhotoAmt_Clipped']\n\n\nnormalized_frame_all.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e5474007327f41169e6df4bbf34177c8b7aeac2e"},"cell_type":"code","source":"## Normalize numeric features into a single normalized frame\n## For testing\n\n## Map numeric features + engineered features\ntestframe_all = testframe[['Total_Sentiment_Score','Type','Age_Years','Breed1_Normalized','Breed2_Normalized',\n                             'Gender','Color1','Color2','Color3','MaturitySize','FurLength','Vaccinated','Dewormed','Sterilized','Health',\n                             'Quantity_Clipped','Fee','Fee_Normalized','PhotoAmt_Clipped',\n                             'vertex_x','vertex_y','bounding_confidence', 'bounding_importance',\n                             'dominant_blue', 'dominant_green', 'dominant_red','dominant_pixel_frac',\n                             'dominant_score','label_score', 'Breed1', 'Breed2']]\n\n\n## Reset index for testing features to prepare for normalization and re-indexation\ntestframe_all = testframe_all.reset_index()\n\n## Create new normalized frames for normalization\nnormalized_frame_test_all = pd.DataFrame()\nnormalized_frame_test_all = testframe_all\n\n##Add column names\n#columnNames = list(testframe_all.head(0))\n\n##Run normalization on both testing features excluding non-numeric IDs\n#for i in columnNames:\n    #if i != 'PetID':\n      #  normalized_frame_test_all[i] = preprocessing.scale(testframe_all[i].astype('float64'))\n\n\n## Add IDs and re-index both normalized feature sets to prepare for label merging\n\n#normalized_frame_test_all['PetID'] = testframe_all['PetID']\n#normalized_frame_test_all = normalized_frame_test_all.set_index('PetID')\n\n\n\n#normalized_frame_test_all.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ccc56c999783dbf7050cb9c6a2606149bf0837eb","_kg_hide-output":true},"cell_type":"code","source":"## Engineer feature crosses for the test set\n\nnormalized_frame_test_all['Breed1 x MaturitySize'] = normalized_frame_test_all['Breed1'] *  normalized_frame_test_all['MaturitySize']\nnormalized_frame_test_all['Breed1 x Gender'] = normalized_frame_test_all['Breed1'] *  normalized_frame_test_all['Gender']\nnormalized_frame_test_all['MaturitySize x Gender'] = normalized_frame_test_all['MaturitySize'] *  normalized_frame_test_all['Gender']\nnormalized_frame_test_all['Type x Gender'] = normalized_frame_test_all['Type'] *  normalized_frame_test_all['Gender']\nnormalized_frame_test_all['Type x MaturitySize'] = normalized_frame_test_all['Type'] *  normalized_frame_test_all['MaturitySize']\nnormalized_frame_test_all['Type x FurLength'] = normalized_frame_test_all['Type'] *  normalized_frame_test_all['FurLength']\nnormalized_frame_test_all['Gender x FurLength'] = normalized_frame_test_all['Gender'] *  normalized_frame_test_all['FurLength']\nnormalized_frame_test_all['Type x Health'] = normalized_frame_test_all['Type'] *  normalized_frame_test_all['Health']\nnormalized_frame_test_all['Fee x Health'] = normalized_frame_test_all['Fee'] *  normalized_frame_test_all['Health']\nnormalized_frame_test_all['Vaccinated x Dewormed x Sterilized'] = normalized_frame_test_all['Vaccinated'] *  normalized_frame_test_all['Dewormed'] * normalized_frame_test_all['Sterilized']\nnormalized_frame_test_all['Type x Color1'] = normalized_frame_test_all['Type'] *  normalized_frame_test_all['Color1']\nnormalized_frame_test_all['Breed1 x Color1'] = normalized_frame_test_all['Breed1'] *  normalized_frame_test_all['Color1']\nnormalized_frame_test_all['Type x Age_Years'] = normalized_frame_test_all['Type'] *  normalized_frame_test_all['Age_Years']\nnormalized_frame_test_all['Health x Age_Years'] = normalized_frame_test_all['Health'] * normalized_frame_test_all['Age_Years']\nnormalized_frame_test_all['Fee x Age_Years'] = normalized_frame_test_all['Fee'] * normalized_frame_test_all['Age_Years']\n\n##Round 2 of engineering\nnormalized_frame_test_all['Total_Sentiment_Score x Health'] = normalized_frame_test_all['Total_Sentiment_Score'] * normalized_frame_test_all['Health']\nnormalized_frame_test_all['Total_Sentiment_Score x Gender'] = normalized_frame_test_all['Total_Sentiment_Score'] * normalized_frame_test_all['Gender']\nnormalized_frame_test_all['Total_Sentiment_Score x Breed1'] = normalized_frame_test_all['Total_Sentiment_Score'] * normalized_frame_test_all['Breed1']\nnormalized_frame_test_all['Total_Sentiment_Score x PhotoAmt_Clipped'] = normalized_frame_test_all['Total_Sentiment_Score'] * normalized_frame_test_all['PhotoAmt_Clipped']\nnormalized_frame_test_all['Total_Sentiment_Score x Fee'] = normalized_frame_test_all['Total_Sentiment_Score'] * normalized_frame_test_all['Fee']\nnormalized_frame_test_all['PhotoAmt_Clipped x Health'] = normalized_frame_test_all['PhotoAmt_Clipped'] * normalized_frame_test_all['Health']\nnormalized_frame_test_all['PhotoAmt_Clipped x Breed1'] = normalized_frame_test_all['PhotoAmt_Clipped'] * normalized_frame_test_all['Breed1']\nnormalized_frame_test_all['PhotoAmt_Clipped x MaturitySize'] = normalized_frame_test_all['PhotoAmt_Clipped'] * normalized_frame_test_all['MaturitySize']\nnormalized_frame_test_all['PhotoAmt_Clipped x Gender'] = normalized_frame_test_all['PhotoAmt_Clipped'] * normalized_frame_test_all['Gender']\nnormalized_frame_test_all['Fee x PhotoAmt_Clipped'] = normalized_frame_test_all['Fee'] * normalized_frame_test_all['PhotoAmt_Clipped']\n\n\nnormalized_frame_test_all.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7bf83b033fd36e81b9249dad69908c4d372a047e"},"cell_type":"code","source":"## Converting description data into word2vec representation - with the assistance of takuok's code, found at https://www.kaggle.com/takuok/word2vec\n\neng_stopwords = set(stopwords.words(\"english\"))\nremove_punctuation_map = dict((ord(char), ' ') for char in string.punctuation)\n#stemmer = nltk.stem.snowball.SnowballStemmer('english')\nstemmer = nltk.stem.porter.PorterStemmer()\n\ndef stem_tokens(tokens):\n    lst = [stemmer.stem(item) for item in tokens]\n    return ' '.join(lst)\n\ndef get_textfeats(df, col, flag=True):\n    df[col] = df[col].fillna('none').astype(str)\n    df[col] = df[col].str.lower()\n    df[col] = df[col].apply(lambda x: stem_tokens(nltk.word_tokenize(x.translate(remove_punctuation_map))))\n    \n    return df\n\ndef load_text(train, test):\n    train = get_textfeats(train, \"Description\")\n    test = get_textfeats(test, \"Description\")\n    train_desc = train['Description'].values\n    test_desc = test['Description'].values\n\n    train_corpus = [text_to_word_sequence(text) for text in tqdm(train_desc)]\n    test_corpus = [text_to_word_sequence(text) for text in tqdm(test_desc)]\n    \n    return train_corpus, test_corpus\n\ndef get_result(corpus, model):\n    result = []\n    for text in corpus:\n        n_skip = 0\n        for n_w, word in enumerate(text):\n            try:\n                vec_ = model.wv[word]\n            except:\n                n_skip += 1\n                continue\n            if n_w == 0:\n                vec = vec_\n            else:\n                vec = vec + vec_\n        vec = vec / (n_w - n_skip + 1)\n        result.append(vec)\n        \n    return result\n\ntrain_corpus, test_corpus = load_text(trainframe, testframe)\nmodel = word2vec.Word2Vec(train_corpus+test_corpus, size=200, window=10, max_vocab_size=50000, seed=0)\ntrain_result = get_result(train_corpus, model)\ntest_result = get_result(test_corpus, model)\n\nw2v_cols = [\"wv{}\".format(i) for i in range(1, 201)]\ntrain_result = pd.DataFrame(train_result)\ntrain_result.columns = w2v_cols\ntest_result = pd.DataFrame(test_result)\ntest_result.columns = w2v_cols\n\nnormalized_frame_all = pd.concat((normalized_frame_all, train_result), axis=1)\nnormalized_frame_test_all = pd.concat((normalized_frame_test_all, test_result), axis=1)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1cac025bf18bf84db1ca661f3427abfeca157578"},"cell_type":"code","source":"## Select final features and targets for training and validation\n\nfeature_selection = []\n\ncolumnNames = list(normalized_frame_all.head(0))\n\nfor i in columnNames:\n    if i != 'PetID' and i != 'AdoptionSpeed':\n        feature_selection.append(i)\n                        \ntarget_train = trainframe_all['AdoptionSpeed'].values\ntrain  = np.array(normalized_frame_all[feature_selection])\n\n## Prepare cross-validation\n\nX = train\ny = target_train\nK = 10\nkf = StratifiedKFold(n_splits = K, random_state = 3228, shuffle = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9ea67e260bc51a992173553dc9e7307e11e5e54c"},"cell_type":"markdown","source":"feature_selection = ['Total_Sentiment_Score','Type','Age_Years','Breed1_Normalized','Gender','FurLength','Vaccinated',\n                     'Dewormed','Sterilized','Quantity_Clipped','PhotoAmt_Clipped','Breed1_Normalized x MaturitySize',\n 'Breed1_Normalized x Gender',\n 'MaturitySize x Gender',\n 'Type x Gender',\n 'Type x MaturitySize',\n 'Type x FurLength',\n 'Gender x FurLength',\n 'Type x Health',\n 'Fee x Health',\n 'Vaccinated x Dewormed x Sterilized',\n 'Type x Color1',\n 'Breed1_Normalized x Color1',\n 'Type x Age_Years',\n 'Health x Age_Years',\n 'Fee x Age_Years']\n\ntarget_train = trainframe_all.AdoptionSpeed\ntrain  = normalized_frame_all[feature_selection]\n\n## Splitting in train and valid\ntrain_size      = int(len(train)*0.7)\nfeatures_train  = train[: train_size]\nlabels_train    = target_train[: train_size]\nfeatures_valid  = train[train_size :]\nlabels_valid    = target_train[train_size :]"},{"metadata":{"trusted":true,"_uuid":"df0e7c27d1f6525b7e821aee31ec7886795a0735"},"cell_type":"code","source":"## Kappa calculation taken from ulissesdias https://www.kaggle.com/ulissesdias/xgboost-all-data-hyperopt-parameter-tuning/notebook competition entry by the way of\n## Hamner's github repository\n# https://github.com/benhamner/Metrics\n\n\ndef confusion_matrix(rater_a, rater_b, min_rating=None, max_rating=None):\n    \"\"\"\n    Returns the confusion matrix between rater's ratings\n    \"\"\"\n    assert(len(rater_a) == len(rater_b))\n    if min_rating is None:\n        min_rating = min(rater_a + rater_b)\n    if max_rating is None:\n        max_rating = max(rater_a + rater_b)\n    num_ratings = int(max_rating - min_rating + 1)\n    conf_mat = [[0 for i in range(num_ratings)]\n                for j in range(num_ratings)]\n    for a, b in zip(rater_a, rater_b):\n        conf_mat[a - min_rating][b - min_rating] += 1\n    return conf_mat\n\n\ndef histogram(ratings, min_rating=None, max_rating=None):\n    \"\"\"\n    Returns the counts of each type of rating that a rater made\n    \"\"\"\n    if min_rating is None:\n        min_rating = min(ratings)\n    if max_rating is None:\n        max_rating = max(ratings)\n    num_ratings = int(max_rating - min_rating + 1)\n    hist_ratings = [0 for x in range(num_ratings)]\n    for r in ratings:\n        hist_ratings[r - min_rating] += 1\n    return hist_ratings\n\n\ndef quadratic_weighted_kappa(y, y_pred):\n    \"\"\"\n    Calculates the quadratic weighted kappa\n    axquadratic_weighted_kappa calculates the quadratic weighted kappa\n    value, which is a measure of inter-rater agreement between two raters\n    that provide discrete numeric ratings.  Potential values range from -1\n    (representing complete disagreement) to 1 (representing complete\n    agreement).  A kappa value of 0 is expected if all agreement is due to\n    chance.\n    quadratic_weighted_kappa(rater_a, rater_b), where rater_a and rater_b\n    each correspond to a list of integer ratings.  These lists must have the\n    same length.\n    The ratings should be integers, and it is assumed that they contain\n    the complete range of possible ratings.\n    quadratic_weighted_kappa(X, min_rating, max_rating), where min_rating\n    is the minimum possible rating, and max_rating is the maximum possible\n    rating\n    \"\"\"\n    rater_a = y\n    rater_b = y_pred\n    min_rating=None\n    max_rating=None\n    rater_a = np.array(rater_a, dtype=int)\n    rater_b = np.array(rater_b, dtype=int)\n    assert(len(rater_a) == len(rater_b))\n    if min_rating is None:\n        min_rating = min(min(rater_a), min(rater_b))\n    if max_rating is None:\n        max_rating = max(max(rater_a), max(rater_b))\n    conf_mat = confusion_matrix(rater_a, rater_b,\n                                min_rating, max_rating)\n    num_ratings = len(conf_mat)\n    num_scored_items = float(len(rater_a))\n\n    hist_rater_a = histogram(rater_a, min_rating, max_rating)\n    hist_rater_b = histogram(rater_b, min_rating, max_rating)\n\n    numerator = 0.0\n    denominator = 0.0\n\n    for i in range(num_ratings):\n        for j in range(num_ratings):\n            expected_count = (hist_rater_a[i] * hist_rater_b[j]\n                              / num_scored_items)\n            d = pow(i - j, 2.0) / pow(num_ratings - 1, 2.0)\n            numerator += d * conf_mat[i][j] / num_scored_items\n            denominator += d * expected_count / num_scored_items\n\n    return (1.0 - numerator / denominator)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a9a8f98c9fdbfb138c4ef98601a38ebb6b1e5fc5"},"cell_type":"markdown","source":"## Modified version of the classifier from ulissesdias\n## https://www.kaggle.com/ulissesdias/xgboost-all-data-hyperopt-parameter-tuning/notebook\n## Including regularization and 10 strat k-fold validation\n\n## Set hyperopt parameter space including L1 and L2 reg\n\nspace ={\n    'max_depth'      : 10,\n   # 'max_depth'       : hp.quniform('max_depth', 10, 30, 0.5),\n    #'min_child_weight': 20,\n    'min_child_weight': hp.quniform('min_child_weight', 10, 30,0.5),\n    'subsample'       : 0.9,\n    #'subsample'       : hp.quniform('subsample', 0, 1, 0.05),\n    #'colsample_bytree': hp.quniform('colsample_bytree', 0, 1, 0.05),\n    'colsample_bytree': 0.6,\n    'alpha': hp.quniform('alpha', 0, 20, 0.5),\n    #'alpha': 2.25,\n   # 'lambda': 6,\n    'lambda': hp.quniform('lambda', 0, 10, 0.25),\n        #'gamma': hp.quniform('gamma', 8, 18,0.25)\n    'gamma': 12\n    }\n\n## Create summary list of training outcomes\noutcomes = []\n\ndef print_feature_importance(clf) :\n    sorted_idx = np.argsort(clf.feature_importances_)[::-1]\n    importance = \"Importance = [\"\n    for index in sorted_idx[:15] :\n        importance += feature_selection[index] + \",\"\n        #print([features[index], clf.feature_importances_[index]])\n    print(importance + \"]\")\n    \n## Objective function - XGBClassifier with 10 strat k-fold validation\n\ndef objective(space):\n    clf = XGBClassifier(\n        nthread          = 40,\n        #n_estimators     = 10000,\n\n        max_depth        = int(space['max_depth']),\n        min_child_weight = float(space['min_child_weight']),\n        subsample        = float(space['subsample']),\n        colsample_bytree = float(space['colsample_bytree']),\n        reg_alpha = float(space['alpha']),\n        reg_lambda = float(space['lambda']),\n        reg_gamma = float(space['gamma'])\n            )\n\n    eval_set  = [(features_train, labels_train.ravel()), ( features_valid, labels_valid.ravel())]\n    clf.fit(features_train, labels_train.ravel(), eval_set = eval_set, eval_metric=\"merror\", early_stopping_rounds=30,verbose = False)   \n    \n    prediction_train = clf.predict(features_train)\n    prediction_valid = clf.predict(features_valid)\n    \n    kappa_train = quadratic_weighted_kappa(labels_train, prediction_train)\n    kappa_valid = quadratic_weighted_kappa(labels_valid, prediction_valid)\n    \n    print(\"space: %s, Kappa Train: %.3f, Kappa Valid: %.3f\" % (str(space), kappa_train, kappa_valid))\n    print(\"\")\n    outcomes.append(str(\"space: %s, Kappa Train: %.3f, Kappa Valid: %.3f\" % (str(space), kappa_train, kappa_valid)))\n    return{'loss':1-kappa_valid, 'status': STATUS_OK }"},{"metadata":{"trusted":true,"_uuid":"1236da199e3649e702ce9ffa4bf0c758b7c1d67c"},"cell_type":"code","source":"## Modified version of the classifier from ulissesdias\n## https://www.kaggle.com/ulissesdias/xgboost-all-data-hyperopt-parameter-tuning/notebook\n## Including regularization and 10 strat k-fold validation\n\n## Set hyperopt parameter space including L1 and L2 reg\n\nspace ={\n    #'max_depth'      : 23,\n    'max_depth'       : hp.quniform('max_depth', 12, 25, 0.5),\n    'min_child_weight': 21,\n    #'min_child_weight': hp.quniform('min_child_weight', 18,30,0.5),\n    'subsample'       : 0.6000000000000001,\n    #'subsample'       : hp.quniform('subsample', 0.2, 0.9, 0.05),\n    #'colsample_bytree': hp.quniform('colsample_bytree', 0, 0.99, 0.05),\n    'colsample_bytree': 0.65,\n   # 'alpha': hp.quniform('alpha', 0, 20, 0.5),\n     #'alpha': 7.3,\n    #'lambda': 15,\n    #'lambda': hp.quniform('lambda', 10, 15, 0.5),\n    #'gamma': hp.quniform('gamma', 15,25,0.5)\n    #'gamma': 22.5\n    'seed': 1337,\n    'eta': 0.0123\n    }\n\n## Create summary list of training outcomes\noutcomes = []\nbestkappa = []\nbestparams = []\n\ndef print_feature_importance(clf) :\n    sorted_idx = np.argsort(clf.feature_importances_)[::-1]\n    importance = \"Importance = [\"\n    for index in sorted_idx[:15] :\n        importance += feature_selection[index] + \",\"\n        #print([features[index], clf.feature_importances_[index]])\n    print(importance + \"]\")\n    return importance\n    \n\n## Objective function - XGBClassifier with 10 strat k-fold validation\n\ndef objective(space):\n    \n    clf = XGBClassifier(\n        nthread          = 40,\n            #n_estimators     = 10000,\n            #objective = 'multi:softmax', \n            #num_class = 5, \n        max_depth        = int(space['max_depth']),\n        min_child_weight = float(space['min_child_weight']),\n        subsample        = float(space['subsample']),\n        colsample_bytree = float(space['colsample_bytree']),\n        seed = int(space['seed']),\n        eta = int(space['eta'])\n                )\n    \n        ## Apply cross-validation\n    for train_index, test_index in kf.split(X, y):\n        print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n        X_train, X_test = X[train_index], X[test_index]\n        y_train, y_test = y[train_index], y[test_index]\n    \n        train_features = pd.DataFrame(X_train)\n        train_targets = pd.DataFrame(y_train).values\n        valid_features = pd.DataFrame(X_test)\n        valid_targets = pd.DataFrame(y_test).values\n\n        eval_set  = [(train_features, train_targets.ravel()), ( valid_features, valid_targets.ravel())]\n        clf.fit(train_features, train_targets.ravel(), eval_set = eval_set, eval_metric=\"merror\", early_stopping_rounds=50,verbose = False)\n        \n    importance = print_feature_importance(clf)\n    prediction_train = clf.predict(train_features)\n    \n    kappa_valid_frame = normalized_frame_all\n    kappa_valid_frame = kappa_valid_frame.reset_index()\n    kappa_valid_frame['AdoptionSpeed'] = trainframe_all['AdoptionSpeed']\n    kappa_valid_frame = kappa_valid_frame.sample(6000)\n\n    kappa_valid_features = pd.DataFrame(np.array(kappa_valid_frame[feature_selection]))\n    kappa_valid_targets = kappa_valid_frame['AdoptionSpeed'].values\n\n    prediction_valid = clf.predict(valid_features)\n    kappa_train = quadratic_weighted_kappa(train_targets.ravel(), prediction_train)\n    kappa_valid = quadratic_weighted_kappa(valid_targets.ravel(), prediction_valid)\n    \n    if not bestkappa:\n        bestkappa.append(kappa_valid)\n        bestparams.append(space['max_depth'])\n        bestparams.append(space['max_depth'])\n        bestparams.append(space['max_depth'])\n    elif bestkappa[0] < kappa_valid:\n        bestkappa[0] = kappa_valid\n        bestparams[0] = space['max_depth']\n        bestparams[1] = space['max_depth']\n        bestparams[2] = space['max_depth']\n    \n    print(\"space: %s, Kappa Train: %.3f, Kappa Valid: %.3f, Feature Importance: %s\" % (str(space), kappa_train, kappa_valid, importance))\n    print(importance)\n    print(\"\")\n    outcomes.append(str(\"space: %s, Kappa Train: %.3f, Kappa Valid: %.3f, Feature Importance: %s, Best Kappa: %.3f\" % (str(space), kappa_train, kappa_valid, importance, int(bestkappa[0]))))\n    print(bestkappa)\n    print(bestparams)\n    return{'loss':1-kappa_valid, 'status': STATUS_OK }\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4be5cc1312f57d75b36b7f6989db126c57d70a2f"},"cell_type":"code","source":"trials = Trials()\nbest = fmin(fn=objective,\n            space=space,\n            algo=tpe.suggest,\n            max_evals=18,\n            trials=trials)\n\n\n## Write hyperparameter performance in a csv file\noutcomereport = pd.DataFrame()\noutcomereport['Outcomes'] = outcomes\noutcomereport\n\noutcomereport.to_csv('outcomes.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"57e44e258b7bb5b245d0b7e582f762fe106159af"},"cell_type":"code","source":"## Optimal hyperparameters from the tuning iterations\n\nspace ={\n    #'max_depth'      : 17.5,\n    'max_depth'       : bestparams[0],\n    'min_child_weight': 21,\n    #'min_child_weight': hp.quniform('min_child_weight', 15, 25,0.5),\n    'subsample'       : 0.6000000000000001,\n    #'subsample'       : hp.quniform('subsample', 0, 0.99, 0.05),\n    #'colsample_bytree': hp.quniform('colsample_bytree', 0, 0.99, 0.05),\n    'colsample_bytree': 0.65,\n    #'alpha': hp.quniform('alpha', 0, 20, 0.5),\n    #'alpha': bestparams[1],\n    #'lambda': bestparams[1],\n    #'lambda': 15,\n   # 'gamma': hp.quniform('gamma', 0, 20,0.5)\n    #'gamma': 22.5\n    'seed': 1337,\n    'eta': 0.0123\n    }\n\n## Objective function - XGBClassifier with 10 strat k-fold validation\n\nclf = XGBClassifier(\n    thread          = 40,\n    #objective = 'multi:softmax', \n    #num_class = 5,\n    max_depth = int(space['max_depth']),\n    min_child_weight = float(space['min_child_weight']),\n    subsample = float(space['subsample']),\n    colsample_bytree = float(space['colsample_bytree']),\n    seed = int(space['seed']),\n    eta = int(space['eta'])\n            )\n    ## Apply cross-validation\n\nfor train_index, test_index in kf.split(X, y):\n    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n    X_train, X_test = X[train_index], X[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n    \n    train_features = pd.DataFrame(X_train)\n    train_targets = pd.DataFrame(y_train).values\n    valid_features = pd.DataFrame(X_test)\n    valid_targets = pd.DataFrame(y_test).values\n\n    eval_set  = [(train_features, train_targets.ravel()), ( valid_features, valid_targets.ravel())]\n    clf.fit(train_features, train_targets.ravel(), eval_set = eval_set, eval_metric=\"merror\", early_stopping_rounds=50,verbose = False)\n    \nprint_feature_importance(clf)\nprediction_train = clf.predict(train_features)\nprediction_valid = clf.predict(valid_features)\n    \nkappa_train = quadratic_weighted_kappa(train_targets.ravel(), prediction_train)\nkappa_valid = quadratic_weighted_kappa(valid_targets.ravel(), prediction_valid)\n    \nprint(\"space: %s, Kappa Train: %.3f, Kappa Valid: %.3f\" % (str(space), kappa_train, kappa_valid))\nprint(\"\")\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9765becf68427e28b0956df664ddae77be0e33f4"},"cell_type":"markdown","source":"## Optimal hyperparameters from the tuning iterations\n\nspace ={\n    'max_depth'       : 10,\n    'min_child_weight': 20,\n    'subsample'       : 0.9,\n    'colsample_bytree': 0.6,\n    'alpha': 2.25,\n    'lambda': 6,\n    'gamma' : 12\n    }\n\nclf = XGBClassifier(\n    nthread          = 40,\n    #n_estimators     = 10000,\n\n    max_depth        = int(space['max_depth']),\n    min_child_weight = float(space['min_child_weight']),\n    subsample        = float(space['subsample']),\n    colsample_bytree = float(space['colsample_bytree']),\n    reg_alpha = float(space['alpha']),\n    reg_lambda = float(space['lambda']),\n    reg_gamma = float(space['gamma'])\n)\n\neval_set  = [(features_train, labels_train.ravel()), ( features_valid, labels_valid.ravel())]\nclf.fit(features_train, labels_train.ravel(), eval_set = eval_set, eval_metric=\"merror\", early_stopping_rounds=30,verbose = False)   \n    \nprediction_train = clf.predict(features_train)\nprediction_valid = clf.predict(features_valid)\n    \nkappa_train = quadratic_weighted_kappa(labels_train, prediction_train)\nkappa_valid = quadratic_weighted_kappa(labels_valid, prediction_valid)\n    \nprint(\"space: %s, Kappa Train: %.3f, Kappa Valid: %.3f\" % (str(space), kappa_train, kappa_valid))\nprint(\"\")\noutcomes.append(str(\"space: %s, Kappa Train: %.3f, Kappa Valid: %.3f\" % (str(space), kappa_train, kappa_valid)))\nreturn{'loss':1-kappa_valid, 'status': STATUS_OK }"},{"metadata":{"trusted":true,"_uuid":"a3606a9dcd76bc2b60b5cddfbad3aa7a8498fd5c"},"cell_type":"code","source":"## Creaate test set for prediction and submission\ntest  = pd.DataFrame(np.array(normalized_frame_test_all[feature_selection]))\nnormalized_frame_test_all[feature_selection].head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f99197000645b6849fe52619c92d0ecf1f7276a3"},"cell_type":"code","source":"normalized_frame_all[feature_selection].head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9033a3e46dc46c78a412929e25a28251124b5630"},"cell_type":"code","source":"## Creat submission and save to csv\n\nprediction_test = clf.predict(test)\n\nsubmission = pd.DataFrame(\n    { \n        'PetID'         : testframe_all.PetID, \n        'AdoptionSpeed' : prediction_test\n    }\n)\n\nsubmission.to_csv('submission.csv',index=False)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"71903e60260e93ec3e50f0055081295ee9ad5a13"},"cell_type":"code","source":"testframe.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d1e5916d6e6710db8260746ebacb35150d68eb07"},"cell_type":"code","source":"trainframe.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"64440465a2462c6c3bb67955123f59256994e85b"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}