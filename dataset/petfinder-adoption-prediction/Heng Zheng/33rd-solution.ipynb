{"cells":[{"metadata":{},"cell_type":"markdown","source":"We have 3 feature tables and one from public kernel, and just merging them together."},{"metadata":{},"cell_type":"markdown","source":"# imports"},{"metadata":{"trusted":true,"_uuid":"1252c7abebd4f3b33eb4a1b3e1e29ee4ac452210"},"cell_type":"code","source":"import gc\nimport os\nimport json\nimport re\nimport glob\nfrom joblib import Parallel, delayed\n\nimport scipy as sp\nimport pandas as pd\nimport numpy as np\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\npd.set_option('max_columns', 500)\npd.set_option('max_rows', 500)\n\nfrom functools import partial\nfrom math import sqrt\n\nfrom sklearn.metrics import cohen_kappa_score, mean_squared_error\nfrom sklearn.metrics import confusion_matrix as sk_cmatrix\nfrom sklearn.model_selection import StratifiedKFold, train_test_split\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.decomposition import TruncatedSVD, NMF\nfrom sklearn.decomposition import LatentDirichletAllocation\nfrom sklearn.preprocessing import MultiLabelBinarizer\n\nfrom collections import Counter\n\nimport xgboost as xgb\nimport lightgbm as lgb\n\nnp.random.seed(1029)\n\nfrom tqdm import tqdm, tqdm_notebook\n\nimport cv2\nfrom keras.applications.densenet import preprocess_input, DenseNet121\nfrom keras.models import Model\nfrom keras.layers import GlobalAveragePooling2D, Input, Lambda, AveragePooling1D\nimport keras.backend as K\n\nfrom scipy import stats\n\nfrom PIL import Image","execution_count":1,"outputs":[{"output_type":"stream","text":"Using TensorFlow backend.\n","name":"stderr"}]},{"metadata":{"_uuid":"3d176daedc7bb27dd1717b9193500d11bd498b89"},"cell_type":"markdown","source":"# nfolds"},{"metadata":{"trusted":true,"_uuid":"805a41ec46e550cd935b9e646a408117733f9b61"},"cell_type":"code","source":"N_FOLDS = 4\nFOLDS = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=42)","execution_count":2,"outputs":[]},{"metadata":{"_uuid":"8e21e261bcf7ced65dbaf066d316f5d7cc637b34"},"cell_type":"markdown","source":"# metric functions and OptimizedRounder(s)"},{"metadata":{"trusted":true,"_uuid":"dc5ed1bd82d0dd7db9aa7f7f5936f16c673292a1"},"cell_type":"code","source":"def get_chi2(obs, exp):\n    diff = set(exp) - set(obs)\n    f_obs = obs.value_counts()\n    f_exp = exp.value_counts()\n    if diff:\n        for i in diff:\n            f_obs[i] = 0\n    f_obs = f_obs.sort_index()\n    f_exp = f_exp.sort_index()\n    chi2, _ = stats.chisquare(f_obs.values,f_exp.values)\n    return chi2\n\ndef confusion_matrix(rater_a, rater_b, min_rating=None, max_rating=None):\n    assert(len(rater_a) == len(rater_b))\n    if min_rating is None:\n        min_rating = min(rater_a + rater_b)\n    if max_rating is None:\n        max_rating = max(rater_a + rater_b)\n    num_ratings = int(max_rating - min_rating + 1)\n    conf_mat = [[0 for i in range(num_ratings)]\n                for j in range(num_ratings)]\n    for a, b in zip(rater_a, rater_b):\n        conf_mat[a - min_rating][b - min_rating] += 1\n    return conf_mat\n\ndef histogram(ratings, min_rating=None, max_rating=None):\n    if min_rating is None:\n        min_rating = min(ratings)\n    if max_rating is None:\n        max_rating = max(ratings)\n    num_ratings = int(max_rating - min_rating + 1)\n    hist_ratings = [0 for x in range(num_ratings)]\n    for r in ratings:\n        hist_ratings[r - min_rating] += 1\n    return hist_ratings\n\ndef quadratic_weighted_kappa(y_true, y_pred):\n    rater_a = y_true\n    rater_b = y_pred\n    min_rating=None\n    max_rating=None\n    rater_a = np.array(rater_a, dtype=int)\n    rater_b = np.array(rater_b, dtype=int)\n    assert(len(rater_a) == len(rater_b))\n    if min_rating is None:\n        min_rating = min(min(rater_a), min(rater_b))\n    if max_rating is None:\n        max_rating = max(max(rater_a), max(rater_b))\n    conf_mat = confusion_matrix(rater_a, rater_b,\n                                min_rating, max_rating)\n    num_ratings = len(conf_mat)\n    num_scored_items = float(len(rater_a))\n\n    hist_rater_a = histogram(rater_a, min_rating, max_rating)\n    hist_rater_b = histogram(rater_b, min_rating, max_rating)\n\n    numerator = 0.0\n    denominator = 0.0\n\n    for i in range(num_ratings):\n        for j in range(num_ratings):\n            expected_count = (hist_rater_a[i] * hist_rater_b[j]\n                              / num_scored_items)\n            d = pow(i - j, 2.0) / pow(num_ratings - 1, 2.0)\n            numerator += d * conf_mat[i][j] / num_scored_items\n            denominator += d * expected_count / num_scored_items\n\n    return (1.0 - numerator / denominator)\n\ndef get_class_bounds(y, y_pred, N=5, class0_fraction=-1):\n    ysort = np.sort(y)\n    predsort = np.sort(y_pred)\n    bounds = []\n    for ibound in range(N-1):\n        iy = len(ysort[ysort <= ibound])\n        if (ibound == 0) and (class0_fraction >= 0.0) :\n            iy = int(class0_fraction * iy)\n        bounds.append(predsort[iy])\n    return bounds\n\ndef assign_class(y_pred, boundaries):\n    y_classes = np.zeros(len(y_pred))\n    for iclass, bound in enumerate(boundaries):\n        y_classes[y_pred >= bound] = iclass + 1\n    return y_classes.astype(int)\n\ndef get_init_coefs(y_test_pred, y_test):\n    kappas = []\n    coefs = []\n    cl0fracs = np.array(np.arange(0.01,30,0.01))\n    for cl0frac in cl0fracs:\n        coef = get_class_bounds(y_test, y_test_pred, class0_fraction=cl0frac)\n        coefs.append(coef)\n        y_test_k = assign_class(y_test_pred, coef)\n        kappa = cohen_kappa_score(y_test, y_test_k, weights='quadratic')\n        kappas.append(kappa)\n    ifmax = np.array(kappas).argmax()\n    best_frac = cl0fracs[ifmax]\n    best_coef = coefs[ifmax] \n    return best_coef\n\ndef rmse(actual, predicted):\n    return sqrt(mean_squared_error(actual, predicted))","execution_count":3,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class OptimizedRounder(object):\n    def __init__(self,initial_coefs = None):\n        if(initial_coefs == None):\n            self.initial_coefs = [1.775, 2.1057, 2.4438, 2.7892]\n        else:\n            self.initial_coefs = initial_coefs.copy()\n        self.coef_ = 0\n\n    def _kappa_loss(self, coef, X, y):\n        X_p = np.copy(X)\n        for i, pred in enumerate(X_p):\n            if pred < coef[0]:\n                X_p[i] = 0\n            elif pred >= coef[0] and pred < coef[1]:\n                X_p[i] = 1\n            elif pred >= coef[1] and pred < coef[2]:\n                X_p[i] = 2\n            elif pred >= coef[2] and pred < coef[3]:\n                X_p[i] = 3\n            else:\n                X_p[i] = 4\n        ll = quadratic_weighted_kappa(y, X_p)\n        return -ll\n\n    def fit(self, X, y):\n        loss_partial = partial(self._kappa_loss, X=X, y=y)\n        self.coef_ = sp.optimize.minimize(loss_partial, self.initial_coefs, method='nelder-mead')\n\n    def predict(self, X, coef):\n        X_p = np.copy(X)\n        for i, pred in enumerate(X_p):\n            if pred < coef[0]:\n                X_p[i] = 0\n            elif pred >= coef[0] and pred < coef[1]:\n                X_p[i] = 1\n            elif pred >= coef[1] and pred < coef[2]:\n                X_p[i] = 2\n            elif pred >= coef[2] and pred < coef[3]:\n                X_p[i] = 3\n            else:\n                X_p[i] = 4\n        return X_p\n\n    def coefficients(self):\n        return self.coef_['x']\n\nclass OptimizedRounder_v2(object):\n    def __init__(self, initial_coefs = None):\n        if(initial_coefs == None):\n            self.initial_coefs = [1.775, 2.1057, 2.4438, 2.7892]\n        else:\n            self.initial_coefs = initial_coefs.copy()\n        self.coef_ = 0\n    \n    def _kappa_loss(self, coef, X, y):\n        X_p = pd.cut(X, [-np.inf] + list(np.sort(coef)) + [np.inf], labels = [0, 1, 2, 3, 4])\n        ll = cohen_kappa_score(y, X_p, weights = 'quadratic')    \n        chi2 =  get_chi2(X_p, y)\n        ll = ll - chi2 * (1.0 / 25000)\n        return -ll\n    \n    def fit(self, X, y):\n        loss_partial = partial(self._kappa_loss, X = X, y = y)\n        self.coef_ = sp.optimize.minimize(loss_partial, self.initial_coefs, method = 'nelder-mead')\n    \n    def predict(self, X, coef):\n        preds = pd.cut(X, [-np.inf] + list(np.sort(coef)) + [np.inf], labels = [0, 1, 2, 3, 4])\n        return preds\n    \n    def coefficients(self):\n        return self.coef_['x']\n\n\nclass OptimizedRounder_v3(object):\n    def __init__(self):\n        self.coef_ = 0\n\n    def _kappa_loss(self, coef, X, y):\n        X_p = np.copy(X)\n        for i, pred in enumerate(X_p):\n            if pred < coef[0]:\n                X_p[i] = 0\n            elif pred >= coef[0] and pred < coef[1]:\n                X_p[i] = 1\n            elif pred >= coef[1] and pred < coef[2]:\n                X_p[i] = 2\n            elif pred >= coef[2] and pred < coef[3]:\n                X_p[i] = 3\n            else:\n                X_p[i] = 4\n\n        ll = quadratic_weighted_kappa(y, X_p)\n        return -ll\n\n    def fit(self, X, y):\n        loss_partial = partial(self._kappa_loss, X=X, y=y)\n        initial_coef = [0.5, 1.5, 2.5, 3.5]\n        self.coef_ = sp.optimize.minimize(loss_partial, initial_coef, method='nelder-mead')\n\n    def predict(self, X, coef, len_0=410):\n        X_p = np.copy(X)\n        temp = sorted(list(X_p))\n        threshold = temp[int(0.9*len_0)-1]\n        for i, pred in enumerate(X_p):\n            if pred < threshold:\n                X_p[i] = 0\n            elif pred >= threshold and pred < coef[1]:\n                X_p[i] = 1\n            elif pred >= coef[1] and pred < coef[2]:\n                X_p[i] = 2\n            elif pred >= coef[2] and pred < coef[3]:\n                X_p[i] = 3\n            else:\n                X_p[i] = 4\n        return X_p\n\n    def coefficients(self):\n        return self.coef_['x']","execution_count":4,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# image processing functions"},{"metadata":{"trusted":true,"_uuid":"df57919e6c9a665f035406395c6efe1862000bf6"},"cell_type":"code","source":"def resize_to_square(im):\n    old_size = im.shape[:2]\n    ratio = float(img_size)/max(old_size)\n    new_size = tuple([int(x*ratio) for x in old_size])\n    im = cv2.resize(im, (new_size[1], new_size[0]))\n    delta_w = img_size - new_size[1]\n    delta_h = img_size - new_size[0]\n    top, bottom = delta_h//2, delta_h-(delta_h//2)\n    left, right = delta_w//2, delta_w-(delta_w//2)\n    color = [0, 0, 0]\n    new_im = cv2.copyMakeBorder(im, top, bottom, left, right, cv2.BORDER_CONSTANT,value=color)\n    return new_im\n\ndef load_image(path, pet_id):\n    image = cv2.imread(f'{path}{pet_id}-1.jpg')\n    new_image = resize_to_square(image)\n    new_image = preprocess_input(new_image)\n    return new_image","execution_count":5,"outputs":[]},{"metadata":{"_uuid":"c5d5c0cd887647b81e2f9255b2fdd2cd981c3dba"},"cell_type":"markdown","source":"# Feature table 1"},{"metadata":{},"cell_type":"markdown","source":"## load data"},{"metadata":{"trusted":true,"_uuid":"00cccd1f2607cf0d5b2957e9a28ceac8e591e931"},"cell_type":"code","source":"train = pd.read_csv(\"../input/petfinder-adoption-prediction/train/train.csv\")\ntest = pd.read_csv(\"../input/petfinder-adoption-prediction/test/test.csv\")\n\nbreeds = pd.read_csv(\"../input/petfinder-adoption-prediction/breed_labels.csv\")\ncolors = pd.read_csv(\"../input/petfinder-adoption-prediction/color_labels.csv\")\nstates = pd.read_csv(\"../input/petfinder-adoption-prediction/state_labels.csv\")","execution_count":6,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"89fa84cdd31b260ca2335214a3020116079ab51a"},"cell_type":"code","source":"origin_train = train[list(train.columns)]\norigin_test = test[list(test.columns)]","execution_count":7,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8ab95181d459b746c944c06cc4f94f369aa5f8a9"},"cell_type":"code","source":"breedid_map = dict(zip(breeds['BreedID'], breeds['BreedName'].map(lambda x:x.lower())))\ncolor_map = dict(zip(colors['ColorID'], colors['ColorName'].map(lambda x:x)))\nstate_map = dict(zip(states['StateID'], states['StateName'].map(lambda x:x)))","execution_count":8,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7ece1c37bd58937e7b788f86d8d023b60e4a8f57"},"cell_type":"code","source":"train_id = train['PetID']\ntest_id = test['PetID']","execution_count":9,"outputs":[]},{"metadata":{"_uuid":"894347ebcedebf336608600974dc27cefe7ce34b"},"cell_type":"markdown","source":"## common feature"},{"metadata":{"trusted":true,"_uuid":"689e47ac466057a5df72cbeb91eacfc578e01809"},"cell_type":"code","source":"def sentiment_feature(data, ids, path):\n    doc_sent_mag = []\n    doc_sent_score = []\n    doc_sent_len = []\n    doc_sent_mags = []\n    doc_sent_scores = []\n\n    doc_entity_len = []\n    doc_entity_sali = []\n\n    nf_count = 0\n\n    for pet in ids:\n        try:\n            with open('../input/petfinder-adoption-prediction/%s/' % path + pet + '.json', 'r') as f:\n                sentiment = json.load(f)\n            doc_sent_mag.append(sentiment['documentSentiment']['magnitude'])\n            doc_sent_score.append(sentiment['documentSentiment']['score'])\n            \n            doc_sent_len.append(len(sentiment['sentences']))\n            if len(sentiment['sentences']) == 0:\n                doc_sent_mags.append([-999])\n                doc_sent_scores.append([-999])\n            else:\n                doc_sent_mags.append([sent['sentiment']['magnitude'] for sent in sentiment['sentences']])\n                doc_sent_scores.append([sent['sentiment']['score'] for sent in sentiment['sentences']])\n            \n            doc_entity_len.append(len(sentiment['entities']))\n            if len(sentiment['entities']) == 0:\n                doc_entity_sali.append([-999])\n            else:\n                doc_entity_sali.append([entity['salience'] for entity in sentiment['entities']])\n        except FileNotFoundError:\n            nf_count += 1\n            doc_sent_mag.append(-1)\n            doc_sent_score.append(-1)\n            doc_sent_len.append(-1)\n            doc_sent_mags.append([-1000])\n            doc_sent_scores.append([-1000])\n            doc_entity_len.append(-1)\n            doc_entity_sali.append([-1000])\n\n    data.loc[:, 'doc_sent_mag'] = doc_sent_mag\n    data.loc[:, 'doc_sent_score'] = doc_sent_score\n\n    return data\n\ntrain = sentiment_feature(train, train_id, 'train_sentiment')\ntest = sentiment_feature(test, test_id, 'test_sentiment')","execution_count":10,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7960234e10f15a12bc59c6e61bb11719f1f3f483"},"cell_type":"code","source":"def gen_meta_f(df, ids, meta_path):\n    vertex_xs = []\n    vertex_ys = []\n    bounding_confidences = []\n    bounding_importance_fracs = []\n    dominant_blues = []\n    dominant_greens = []\n    dominant_reds = []\n    dominant_pixel_fracs = []\n    dominant_scores = []\n    \n    dominant_blues1 = []\n    dominant_greens1 = []\n    dominant_reds1 = []\n    dominant_pixel_fracs1 = []\n    dominant_scores1 = []\n\n    label_descriptions = []\n    label_descriptions1 = []\n    label_descriptions2 = []\n    label_descriptions3 = []\n    \n    label_scores = []\n    label_scores1 = []\n    label_scores2 = []\n    label_scores3 = []\n    \n    nf_count = 0\n    nl_count = 0\n    label_data = {}\n    for idx, pet in enumerate(ids):\n        try:\n            with open('../input/petfinder-adoption-prediction/%s/' % meta_path + pet + '-1.json', 'r') as f:\n                data = json.load(f)\n            vertex_x = data['cropHintsAnnotation']['cropHints'][0]['boundingPoly']['vertices'][2]['x']\n            vertex_xs.append(vertex_x)\n            vertex_y = data['cropHintsAnnotation']['cropHints'][0]['boundingPoly']['vertices'][2]['y']\n            vertex_ys.append(vertex_y)\n            bounding_confidence = data['cropHintsAnnotation']['cropHints'][0]['confidence']\n            bounding_confidences.append(bounding_confidence)\n            bounding_importance_frac = data['cropHintsAnnotation']['cropHints'][0].get('importanceFraction', -1)\n            bounding_importance_fracs.append(bounding_importance_frac)\n            # 0\n            dominant_blue = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['color']['blue']\n            dominant_blues.append(dominant_blue)\n            dominant_green = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['color']['green']\n            dominant_greens.append(dominant_green)\n            dominant_red = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['color']['red']\n            dominant_reds.append(dominant_red)\n            dominant_pixel_frac = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['pixelFraction']\n            dominant_pixel_fracs.append(dominant_pixel_frac)\n            dominant_score = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['score']\n            dominant_scores.append(dominant_score)\n            # 1\n            if len(data['imagePropertiesAnnotation']['dominantColors']['colors']) > 1 and len(data['imagePropertiesAnnotation']['dominantColors']['colors'][1]['color']) == 3:\n                dominant_blue1 = data['imagePropertiesAnnotation']['dominantColors']['colors'][1]['color']['blue']\n                dominant_blues1.append(dominant_blue1)\n                dominant_green1 = data['imagePropertiesAnnotation']['dominantColors']['colors'][1]['color']['green']\n                dominant_greens1.append(dominant_green1)\n                dominant_red1 = data['imagePropertiesAnnotation']['dominantColors']['colors'][1]['color']['red']\n                dominant_reds1.append(dominant_red1)\n                dominant_pixel_frac1 = data['imagePropertiesAnnotation']['dominantColors']['colors'][1]['pixelFraction']\n                dominant_pixel_fracs1.append(dominant_pixel_frac1)\n                dominant_score1 = data['imagePropertiesAnnotation']['dominantColors']['colors'][1]['score']\n                dominant_scores1.append(dominant_score1)\n        \n            else:\n                dominant_blues1.append(-1)\n                dominant_greens1.append(-1)\n                dominant_reds1.append(-1)\n                dominant_pixel_fracs1.append(-1)\n                dominant_scores1.append(-1)\n                \n            if data.get('labelAnnotations'):\n                label_description = data['labelAnnotations'][0]['description']\n                label_descriptions.append(label_description)\n                label_score = data['labelAnnotations'][0]['score']\n                label_scores.append(label_score)\n\n                if len(data['labelAnnotations']) > 1:\n                    label_description1 = data['labelAnnotations'][1]['description']\n                    label_descriptions1.append(label_description1)\n                    label_score1 = data['labelAnnotations'][1]['score']\n                    label_scores1.append(label_score1)\n                else:\n                    label_descriptions1.append('nothing')\n                    label_scores1.append(-1)\n                \n                if len(data['labelAnnotations']) > 2:\n                    label_description2 = data['labelAnnotations'][2]['description']\n                    label_descriptions2.append(label_description2)\n                    label_score2 = data['labelAnnotations'][2]['score']\n                    label_scores2.append(label_score2)\n                else:\n                    label_descriptions2.append('nothing')\n                    label_scores2.append(-1)\n\n                if len(data['labelAnnotations']) > 3:\n                    label_description3 = data['labelAnnotations'][3]['description']\n                    label_descriptions3.append(label_description3)\n                    label_score3 = data['labelAnnotations'][3]['score']\n                    label_scores3.append(label_score3)\n                else:\n                    label_descriptions3.append('nothing')\n                    label_scores3.append(-1)\n\n            else:\n                nl_count += 1\n                label_descriptions.append('nothing')\n                label_descriptions1.append(label_description1)\n                label_descriptions2.append(label_description2)\n                label_descriptions3.append(label_description3)\n                \n                label_scores.append(-1)\n                label_scores1.append(-1)\n                label_scores2.append(-1)\n                label_scores3.append(-1)\n                                                            \n        except FileNotFoundError:\n            nf_count += 1\n            vertex_xs.append(-1)\n            vertex_ys.append(-1)\n            bounding_confidences.append(-1)\n            bounding_importance_fracs.append(-1)\n            dominant_blues.append(-1)\n            dominant_greens.append(-1)\n            dominant_reds.append(-1)\n            dominant_pixel_fracs.append(-1)\n            dominant_scores.append(-1)\n            \n            dominant_blues1.append(-1)\n            dominant_greens1.append(-1)\n            dominant_reds1.append(-1)\n            dominant_pixel_fracs1.append(-1)\n            dominant_scores1.append(-1)\n\n            label_descriptions.append('nothing')\n            label_descriptions1.append('nothing')\n            label_descriptions2.append('nothing')\n            label_descriptions3.append('nothing')\n            label_scores.append(-1)\n            label_scores1.append(-1)\n            label_scores2.append(-1)\n            label_scores3.append(-1)\n\n    prefix = 'meta_'\n    df.loc[:, prefix+'vertex_x'] = vertex_xs\n    df.loc[:, prefix+'vertex_y'] = vertex_ys\n    df.loc[:, prefix+'bounding_confidence'] = bounding_confidences\n    df.loc[:, prefix+'bounding_importance'] = bounding_importance_fracs\n    df.loc[:, prefix+'dominant_blue'] = dominant_blues\n    df.loc[:, prefix+'dominant_green'] = dominant_greens\n    df.loc[:, prefix+'dominant_red'] = dominant_reds\n    df.loc[:, prefix+'dominant_pixel_frac'] = dominant_pixel_fracs\n    df.loc[:, prefix+'dominant_score'] = dominant_scores\n    \n    df.loc[:, prefix+'label_description'] = label_descriptions\n    df.loc[:, prefix+'label_description1'] = label_descriptions1\n    df.loc[:, prefix+'label_description2'] = label_descriptions2\n\n    df.loc[:, prefix+'label_score'] = label_scores\n    df.loc[:, prefix+'label_score1'] = label_scores1\n    df.loc[:, prefix+'label_score2'] = label_scores1\n    cate_cols = [prefix+col for col in ['label_description','label_description1','label_description2']]\n    df.loc[:, cate_cols] = df[cate_cols].astype('category')\n\ngen_meta_f(train, train_id, 'train_metadata')\ngen_meta_f(test, test_id, 'test_metadata')","execution_count":11,"outputs":[]},{"metadata":{"_uuid":"c454456e65545c754da3c13d0d6c2d856ea354eb"},"cell_type":"markdown","source":"## origin feature"},{"metadata":{"trusted":true,"_uuid":"22321b6528b77d6f4b1d63471b24a412ed61f03e"},"cell_type":"code","source":"def rescue_feature(df):\n    rescue_count = df.groupby('RescuerID')['Quantity'].count()\n    rescue_count.name = 'rescue_count'\n    rescue_num = df.groupby('RescuerID')['Quantity'].sum()\n    rescue_num.name = 'rescue_num'\n    rescue_unique_type = df.drop_duplicates(['RescuerID', 'Type']).groupby('RescuerID')['RescuerID'].count()\n    rescue_unique_type.name = 'rescue_unique_type'\n    df = df.join(rescue_count, on='RescuerID')\n    df = df.join(rescue_num, on='RescuerID') \n    df['rescue_rank'] = df.RescuerID.map(df.RescuerID.value_counts().rank()/df.RescuerID.unique().shape[0])\n    return df\n\ndef pure_breed_encode(data):\n    data['pure_breed1'] = np.where((data['Breed1'] != 307) , '0', '1')\n    data['pure_breed2'] = np.where((data['Breed2'] == 0) , '0', np.where(data['Breed2'] != 307, '1', '2'))\n    data['pure_breed3'] = (data['pure_breed1'] + data['pure_breed2'])\n    data['pure_animal_pure_breed4'] = np.where((data['Type'].astype(np.str)=='1') & (data['pure_breed3']=='00'), '100', np.where((data['Type'].astype(np.str)=='2') & (data['pure_breed3']=='00'), '200', '333'))\n    for col in ['pure_breed1', 'pure_breed2', 'pure_breed3', 'pure_animal_pure_breed4']:\n        data[col] = data[col].astype('category')\n    del data['pure_animal_pure_breed4']\n    return data\n\ndef call_name_f(data):\n    is_call_name = []\n    for name, desc in zip(data['Name'], data['Description']):\n        clean_desc = str(desc).lower()\n        clean_name = str(name).lower()\n        if clean_name == 'nan':\n            is_call_name.append(0)\n        else:\n            num = len(clean_desc.split(clean_name))\n            is_call_name.append(num)\n    data['call_name_num'] = is_call_name\n    return data\n\ntrain = rescue_feature(train)\ntest = rescue_feature(test)\n\ntrain = pure_breed_encode(train)\ntest = pure_breed_encode(test)","execution_count":12,"outputs":[]},{"metadata":{"_uuid":"985767be5219fb4c04d9d3d30c8bf2df0983392d"},"cell_type":"markdown","source":"## description feature"},{"metadata":{"trusted":true,"_uuid":"d0fc7a6f96302f9ddc0a0c722a7ee70fc43a0af6"},"cell_type":"code","source":"def language_type(desc):\n    desc = str(desc)\n    if desc=='nan':\n        return 0\n    zhmodel = re.compile(u'[\\u4e00-\\u9fa5]')\n    enmodel = re.compile(u'[a-zA-Z]')\n    zhmatch = zhmodel.search(desc)\n    enmatch = enmodel.search(desc)\n    if zhmatch and enmatch:\n        return 3\n    elif zhmatch:\n        return 3\n    elif enmatch:\n        return 2\n    else:\n        return 1\n\ndef malaiyu_type(desc):\n    desc = str(desc)\n    malai = [' la x ' , ' nk ',' nie ', ' umur ', ' di ', 'teruk', ' satu ',' dh ', ' ni ',' tp ', ' yg ', 'mmg', 'msj', ' utk ' ,'neh' ]\n    for ma_tag in malai:\n        if desc.find(ma_tag) > -1:\n            return ma_tag,1\n    \n    return \"\", 0\n\nlang_prefix = 'lang_'\ntrain[lang_prefix+'language_type'] = train.Description.map(lambda x:language_type(x))\ntrain[lang_prefix+'malaiyu_type'] = train.Description.map(lambda x:malaiyu_type(x)[1])\n\ntest[lang_prefix+'language_type'] = test.Description.map(lambda x:language_type(x))\ntest[lang_prefix+'malaiyu_type'] = test.Description.map(lambda x:malaiyu_type(x)[1])","execution_count":13,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4b3010de8bd1f282c9fe5f03bdeeec8bbcb82933","_kg_hide-input":false,"_kg_hide-output":false,"scrolled":true},"cell_type":"code","source":"def obtain_text(df):\n    breed1_text = df['Breed1'].map(lambda x:breedid_map.get(x, 'unknown_breed'))\n    breed2_text = df['Breed2'].map(lambda x:breedid_map.get(x, 'unknown_breed'))\n    color1_text = df['Color1'].map(lambda x:color_map.get(x, 'unknown_color'))\n    color2_text = df['Color2'].map(lambda x:color_map.get(x, 'unknown_color'))\n    color3_text = df['Color3'].map(lambda x:color_map.get(x, 'unknown_color'))\n\n    text = df['Name'].fillna(\"none\") + \" \" \\\n           + breed1_text  + \" \" \\\n           + breed2_text + \" \" \\\n           + color1_text + \" \" \\\n           + color2_text + \" \" \\\n           + color3_text + \" \" \\\n           + df['Description'].fillna(\"none\")\n    \n    return text\n\ntrain_desc = train.Description.fillna(\"none\").values\ntest_desc = test.Description.fillna(\"none\").values\n\ntfv = TfidfVectorizer(min_df=3,  max_features=10000,\n        strip_accents='unicode', analyzer='word', token_pattern=r'\\w{1,}',\n        ngram_range=(1, 3), use_idf=1, smooth_idf=1, sublinear_tf=1,\n        stop_words = 'english')\n    \n# Fit TFIDF\ntfv.fit(list(train_desc))\nX =  tfv.transform(train_desc)\nX_test = tfv.transform(test_desc)\n\ncomponents = 120\nsvd = TruncatedSVD(n_components=components)\nsvd.fit(X)\n\nX = svd.transform(X)\nX = pd.DataFrame(X, columns=['svd_{}'.format(i) for i in range(components)])\ntrain = pd.concat((train, X), axis=1)\nX_test = svd.transform(X_test)\nX_test = pd.DataFrame(X_test, columns=['svd_{}'.format(i) for i in range(components)])\ntest = pd.concat((test, X_test), axis=1)","execution_count":14,"outputs":[]},{"metadata":{"_uuid":"41824c84db0ec2988a3d5c67a6350732595c0a97"},"cell_type":"markdown","source":"## NMF LDA"},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"551925fdc6ba4258b3842948a3a894b3c5835846"},"cell_type":"code","source":"def nmf_lda_feature(train, test, train_text, test_text):\n    tfv = TfidfVectorizer(min_df=3,  max_features=10000,\n        strip_accents='unicode', analyzer='word', token_pattern=r'\\w{1,}',\n        ngram_range=(1, 3), use_idf=1, smooth_idf=1, sublinear_tf=1,\n        stop_words = 'english')\n\n    tfv.fit(list(train_text)+list(test_text))\n    X =  tfv.transform(train_text)\n    X_test = tfv.transform(test_text)\n\n    components = 20\n    nmf = NMF(n_components=components, random_state=100).fit(X)\n    nmf_x = nmf.transform(X)\n    nmf_x = pd.DataFrame(nmf_x, columns=['nmf_{}'.format(i) for i in range(components)])\n    train = pd.concat((train, nmf_x), axis=1)\n    nmf_x_test = nmf.transform(X_test)\n    nmf_x_test = pd.DataFrame(nmf_x_test, columns=['nmf_{}'.format(i) for i in range(components)])\n    test = pd.concat((test, nmf_x_test), axis=1)\n\n    components = 12\n    lda = LatentDirichletAllocation(n_components=components, max_iter=120, n_jobs=-1)\n    lda.fit(X)\n    lda_x = lda.transform(X)\n    lda_x = pd.DataFrame(lda_x, columns=['lda_{}'.format(i) for i in range(components)])\n    train = pd.concat((train, lda_x), axis=1)\n    lda_x_test = lda.transform(X_test)\n    lda_x_test = pd.DataFrame(lda_x_test, columns=['lda_{}'.format(i) for i in range(components)])\n    test = pd.concat((test, lda_x_test), axis=1)\n    \n    return train, test\n\ntrain_text = obtain_text(train)\ntest_text = obtain_text(test)\n\ntrain, test = nmf_lda_feature(train, test, train_text, test_text)","execution_count":15,"outputs":[]},{"metadata":{"_uuid":"485b57a71ef900c8afbc846cda8ce876c29eb29c"},"cell_type":"markdown","source":"## image feature"},{"metadata":{"trusted":true,"_uuid":"5dae812f463ae3568f845e953d8a58b3ae06c175"},"cell_type":"code","source":"train_df = pd.read_csv(\"../input/petfinder-adoption-prediction/train/train.csv\")\ntest_df = pd.read_csv('../input/petfinder-adoption-prediction/test/test.csv')\ntest_pet_ids = test_df['PetID'].values\ntrain_pet_ids = train_df['PetID'].values\ntarget = train_df['AdoptionSpeed'].values","execution_count":16,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.models import Model\nfrom keras.layers import GlobalAveragePooling2D, Input, Lambda, AveragePooling1D,Dense,Dropout\nimport keras.backend as K\nfrom keras.optimizers import Adam\nfrom keras.applications.densenet import preprocess_input, DenseNet121\nfrom keras.applications.resnet50 import preprocess_input as res_preprocess, ResNet50","execution_count":17,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### ResNet50 meta feature"},{"metadata":{"trusted":true,"_uuid":"696082194e0962116d539bcbae005ee814efb3aa"},"cell_type":"code","source":"batch_size = 128\ndef BASE_MODEL():\n    inp = Input((128,128,3))\n    backbone = ResNet50(input_tensor = inp, \n                           weights=\"../input/resnet50/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\",\n                           include_top = False)\n    x = backbone.output\n    x = GlobalAveragePooling2D()(x)\n    x = Dense(512)(x)\n    x = Dropout(0.5)(x)\n    output = Dense(1,activation='linear')(x)\n    return Model(inp,output)\n\ndef new_load_image(path, pet_id):\n    image = cv2.imread(f'{path}{pet_id}-1.jpg')\n    try:\n        new_image = cv2.resize(image,(128,128))\n    except:\n        new_image = np.zeros((128,128,3))\n    new_image = res_preprocess(new_image)\n    return new_image\n\n\ndef train_gen(batch_size=128, shuffle=True, pet_list=None, pet_labels=None, use_labels=True):\n    images_df = pd.DataFrame({'img_id':pet_list,'label':pet_labels})\n    while True:\n        if shuffle:\n            images_df = images_df.sample(frac=1.0).reset_index(drop=True)\n        for start in range(0, len(images_df), batch_size):\n            x_batch = []\n            y_batch = []\n            end = min(start + batch_size,len(images_df))\n            for _id in range(start,end):\n                image_row = images_df.iloc[_id]\n                image_id = image_row['img_id']\n                img = new_load_image(\"../input/petfinder-adoption-prediction/train_images/\", image_id)\n                if use_labels:\n                    img_label = image_row['label']\n                    y_batch.append(img_label)\n                else:\n                    y_batch.append(-1.0)\n                x_batch.append(img)\n            yield np.array(x_batch),np.array(y_batch)\n            \ndef test_gen(batch_size=128,shuffle=True,pet_list=None,pet_labels=None,use_labels=True):\n    images_df = pd.DataFrame({'img_id':pet_list,'label':pet_labels})\n    while True:\n        if shuffle:\n            images_df = images_df.sample(frac=1.0).reset_index(drop=True)\n        for start in range(0, len(images_df), batch_size):\n            x_batch = []\n            y_batch = []\n            end = min(start + batch_size,len(images_df))\n            for _id in range(start,end):\n                image_row = images_df.iloc[_id]\n                image_id = image_row['img_id']\n                img = new_load_image(\"../input/petfinder-adoption-prediction/test_images/\", image_id)\n                if use_labels:\n                    img_label = image_row['label']\n                    y_batch.append(img_label)\n                else:\n                    y_batch.append(-1.0)\n                x_batch.append(img)\n            yield np.array(x_batch),np.array(y_batch)","execution_count":18,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false,"_uuid":"75e03ad05db1381cac084c431b2be0b1115133c9"},"cell_type":"code","source":"test_img_prob = np.zeros(shape=(test_df.shape[0],1))\ntrain_img_prob = np.zeros(shape=(train_df.shape[0],1))\n\nfor tr_idx,te_idx in FOLDS.split(train_pet_ids, target):\n    gen_tr = train_gen(batch_size=batch_size,\n                       shuffle=True,\n                       pet_list=train_pet_ids[tr_idx],\n                       pet_labels=target[tr_idx])\n    gen_te = train_gen(batch_size=batch_size,\n                       shuffle=False,\n                       pet_list=train_pet_ids[te_idx],\n                       pet_labels=target[te_idx])\n    gen_test = test_gen(batch_size=batch_size,\n                        shuffle=False,\n                        pet_list=test_pet_ids,\n                        pet_labels=None,\n                        use_labels=False)\n    \n    model = BASE_MODEL()\n    model.compile(optimizer='adam', loss='mse')\n    \n    model.fit_generator(gen_tr,\n                        steps_per_epoch=int(np.ceil(len(tr_idx)*1.0/batch_size)),\n                        epochs=3,verbose=1,\n                        validation_data=gen_te,\n                        validation_steps=int(np.ceil(len(te_idx)*1.0/batch_size)))\n    _test_prob = model.predict_generator(gen_test,\n                                         steps=int(np.ceil(len(test_df)*1.0/(batch_size))))\n    _val_prob = model.predict_generator(gen_te,\n                                        steps=int(np.ceil(len(te_idx)*1.0/(batch_size))))\n    \n    train_img_prob[te_idx,:] = _val_prob \n    test_img_prob += _test_prob\n\ntest_img_prob /= N_FOLDS","execution_count":19,"outputs":[{"output_type":"stream","text":"WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nColocations handled automatically by placer.\nWARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\nInstructions for updating:\nPlease use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\nWARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse tf.cast instead.\nEpoch 1/3\n88/88 [==============================] - 98s 1s/step - loss: 10.4753 - val_loss: 1.4209\nEpoch 2/3\n88/88 [==============================] - 64s 729ms/step - loss: 1.4055 - val_loss: 1.3576\nEpoch 3/3\n88/88 [==============================] - 64s 723ms/step - loss: 1.3358 - val_loss: 1.9851\nEpoch 1/3\n88/88 [==============================] - 81s 917ms/step - loss: 10.2023 - val_loss: 1.7216\nEpoch 2/3\n88/88 [==============================] - 64s 726ms/step - loss: 1.4147 - val_loss: 1.4109\nEpoch 3/3\n88/88 [==============================] - 65s 741ms/step - loss: 1.3028 - val_loss: 1.4474\nEpoch 1/3\n88/88 [==============================] - 86s 974ms/step - loss: 11.6259 - val_loss: 1.7567\nEpoch 2/3\n88/88 [==============================] - 65s 738ms/step - loss: 1.4120 - val_loss: 1.3919\nEpoch 3/3\n88/88 [==============================] - 68s 775ms/step - loss: 1.3341 - val_loss: 1.4087\nEpoch 1/3\n88/88 [==============================] - 90s 1s/step - loss: 15.9591 - val_loss: 5.5198\nEpoch 2/3\n88/88 [==============================] - 65s 740ms/step - loss: 1.6569 - val_loss: 1.4355\nEpoch 3/3\n88/88 [==============================] - 66s 752ms/step - loss: 1.4077 - val_loss: 1.4172\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"### DenseNet121 extracted 256 dim image features"},{"metadata":{"trusted":true,"_uuid":"c13634ed4a2f319218bc6c14bcef986427f7b45a"},"cell_type":"code","source":"img_size = 256\nbatch_size = 16\n\ntrain_df = pd.read_csv(\"../input/petfinder-adoption-prediction/train/train.csv\")\npet_ids = train_df['PetID'].values\nn_batches = len(pet_ids) // batch_size + 1\n\nfrom keras.models import Model\nfrom keras.layers import GlobalAveragePooling2D, Input, Lambda, AveragePooling1D\nimport keras.backend as K\ninp = Input((256,256,3))\nbackbone = DenseNet121(input_tensor = inp, \n                       weights=\"../input/densenet-keras/DenseNet-BC-121-32-no-top.h5\",\n                       include_top = False)\nx = backbone.output\nx = GlobalAveragePooling2D()(x)\nx = Lambda(lambda x: K.expand_dims(x,axis = -1))(x)\nx = AveragePooling1D(4)(x)\nout = Lambda(lambda x: x[:,:,0])(x)\n\nm = Model(inp,out)\n\nfeatures = {}\nfor b in tqdm_notebook(range(n_batches)):\n    start = b*batch_size\n    end = (b+1)*batch_size\n    batch_pets = pet_ids[start:end]\n    batch_images = np.zeros((len(batch_pets),img_size,img_size,3))\n    for i,pet_id in enumerate(batch_pets):\n        try:\n            batch_images[i] = load_image(\"../input/petfinder-adoption-prediction/train_images/\", pet_id)\n        except:\n            pass\n    batch_preds = m.predict(batch_images)\n    for i,pet_id in enumerate(batch_pets):\n        features[pet_id] = batch_preds[i]\n\ntrain_feats = pd.DataFrame.from_dict(features, orient='index')\ntrain_feats.columns = ['pic_'+str(i) for i in range(train_feats.shape[1])]","execution_count":20,"outputs":[{"output_type":"display_data","data":{"text/plain":"HBox(children=(IntProgress(value=0, max=938), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7a2a22eb55b44391b838db17e1d3f374"}},"metadata":{}},{"output_type":"stream","text":"\n","name":"stdout"}]},{"metadata":{"trusted":true,"_uuid":"d363871abb9624583f262467f403a6fccbe3e269"},"cell_type":"code","source":"test_df = pd.read_csv('../input/petfinder-adoption-prediction/test/test.csv')\n\npet_ids = test_df['PetID'].values\nn_batches = len(pet_ids) // batch_size + 1\n\nfeatures = {}\nfor b in tqdm_notebook(range(n_batches)):\n    start = b*batch_size\n    end = (b+1)*batch_size\n    batch_pets = pet_ids[start:end]\n    batch_images = np.zeros((len(batch_pets),img_size,img_size,3))\n    for i,pet_id in enumerate(batch_pets):\n        try:\n            batch_images[i] = load_image(\"../input/petfinder-adoption-prediction/test_images/\", pet_id)\n        except:\n            pass\n    batch_preds = m.predict(batch_images)\n    for i,pet_id in enumerate(batch_pets):\n        features[pet_id] = batch_preds[i]\n        \ntest_feats = pd.DataFrame.from_dict(features, orient='index')\ntest_feats.columns = ['pic_'+str(i) for i in range(test_feats.shape[1])]\n\ntest_feats = test_feats.reset_index()\ntest_feats.rename({'index': 'PetID'}, axis='columns', inplace=True)\n\ntrain_feats = train_feats.reset_index()\ntrain_feats.rename({'index': 'PetID'}, axis='columns', inplace=True)\n\ntest_feats.head()\ntrain = pd.merge(train, train_feats, how='left', on='PetID')\ntest = pd.merge(test, test_feats, how='left', on='PetID')","execution_count":21,"outputs":[{"output_type":"display_data","data":{"text/plain":"HBox(children=(IntProgress(value=0, max=249), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c613dc8113004d1d836ee41ce5f75f12"}},"metadata":{}},{"output_type":"stream","text":"\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_feats_256 = train_feats.copy()\ntest_feats_256 = test_feats.copy()","execution_count":22,"outputs":[]},{"metadata":{"_uuid":"31147ec092e6204ca2faa85278829a760246f87c"},"cell_type":"markdown","source":"# Feature table 2"},{"metadata":{"trusted":true},"cell_type":"code","source":"breed_id_map = dict(zip(breeds.BreedID.values,breeds.BreedName.values))\nbreed_type_map = dict(zip(breeds.BreedID.values,breeds.Type.values))\ncolor_id_map = dict(zip(colors.ColorID.values,colors.ColorName.values))","execution_count":23,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## colors"},{"metadata":{"trusted":true,"_uuid":"eecdb60d031a60b11f33672924537ea024d7033c"},"cell_type":"code","source":"train['Breed1_text'] = train['Breed1'].map(lambda x:breed_id_map.get(x,'UNK_Breed1'))\ntrain['Breed2_text'] = train['Breed2'].map(lambda x:breed_id_map.get(x,'UNK_Breed2'))\ntrain['Color1_text'] = train['Color1'].map(lambda x:color_id_map.get(x,'UNK_Color1'))\ntrain['Color2_text'] = train['Color2'].map(lambda x:color_id_map.get(x,'UNK_Color2'))\ntrain['Color3_text'] = train['Color3'].map(lambda x:color_id_map.get(x,'UNK_Color3'))\n\ntest['Breed1_text'] = test['Breed1'].map(lambda x:breed_id_map.get(x,'UNK_Breed1'))\ntest['Breed2_text'] = test['Breed2'].map(lambda x:breed_id_map.get(x,'UNK_Breed2'))\ntest['Color1_text'] = test['Color1'].map(lambda x:color_id_map.get(x,'UNK_Color1'))\ntest['Color2_text'] = test['Color2'].map(lambda x:color_id_map.get(x,'UNK_Color2'))\ntest['Color3_text'] = test['Color3'].map(lambda x:color_id_map.get(x,'UNK_Color3'))","execution_count":24,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## all raw text"},{"metadata":{"trusted":true},"cell_type":"code","source":"train['raw_text'] =  train['Name'] + ' ' \\\n                    + train['Breed1_text'] + ' ' + train['Breed2_text'] + ' ' \\\n                    + train['Color1_text'] + ' ' + train['Color2_text'] + ' ' \\\n                    + train['Color3_text'] + ' ' \\\n                    + train['Description']\n\ntest['raw_text'] =  test['Name'] + ' ' \\\n                    + test['Breed1_text'] + ' ' + test['Breed2_text'] + ' ' \\\n                    + test['Color1_text'] + ' ' + test['Color2_text'] + ' ' \\\n                    + test['Color3_text'] + ' ' \\\n                    + test['Description']","execution_count":25,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## rescuer, breeds and description"},{"metadata":{"trusted":true,"_uuid":"1dd735332ac57a6c1a8c786c2fe2e56c91bbcfce"},"cell_type":"code","source":"gzf_prefix = 'gzf_'\n\ntrain[gzf_prefix+'RescureID_rank'] = train.RescuerID.map(train.RescuerID.value_counts().rank()/train.RescuerID.unique().shape[0])\ntrain[gzf_prefix+'Description_len'] = train.Description.map(lambda x:len(x) if type(x)!=float else 0)\ntrain[gzf_prefix+'Description_word_len'] = train.Description.map(lambda x:len(x.strip().split()) if type(x)!=float else 0)\ntrain[gzf_prefix+'Description_distinct_word_len'] = train.Description.map(lambda x:len(set(x.lower().strip().split())) if type(x)!=float else 0)\ntrain[gzf_prefix+'Description_distinct_word_ratio'] = train[gzf_prefix+'Description_distinct_word_len'] / (train[gzf_prefix+'Description_word_len'] + 1.0)\n\ntest[gzf_prefix+'RescureID_rank'] = test.RescuerID.map(test.RescuerID.value_counts().rank()/test.RescuerID.unique().shape[0])\ntest[gzf_prefix+'Description_len'] = test.Description.map(lambda x:len(x) if type(x)!=float else 0)\ntest[gzf_prefix+'Description_word_len'] = test.Description.map(lambda x:len(x.strip().split()) if type(x)!=float else 0)\ntest[gzf_prefix+'Description_distinct_word_len'] = test.Description.map(lambda x:len(set(x.lower().strip().split())) if type(x)!=float else 0)\ntest[gzf_prefix+'Description_distinct_word_ratio'] = test[gzf_prefix+'Description_distinct_word_len'] / (test[gzf_prefix+'Description_word_len'] + 1.0)","execution_count":26,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9d128c1201b2c3ebe59983668364f75cecf48d42"},"cell_type":"code","source":"X = pd.concat([train,test],axis=0,ignore_index=True)\nlen_train = len(train)","execution_count":27,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2abeea315ff576c2abfdecbb2a7bbf40015a032d"},"cell_type":"code","source":"X[gzf_prefix+'is_pure'] = ((X.Breed1!=307) & (X.Breed2!=307) & (X.Breed2!=0)).astype(float)\nX[gzf_prefix+'is_pure_breed1'] = (X.Breed1!=307).astype(float)\nX[gzf_prefix+'is_pure_breed2'] = ((X.Breed2!=307) & (X.Breed2!=0)).astype(float)","execution_count":28,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"41c5b3b02db665003a48bd99b7f3d27a718955c7"},"cell_type":"code","source":"agg_num_feature = ['Age','Health','PhotoAmt','Quantity',\n                   'doc_sent_mag', 'doc_sent_score', \n                   'meta_dominant_score', 'meta_label_score',gzf_prefix+'Description_len']\nagg_rescureid_1 = X.groupby(['RescuerID'])[agg_num_feature].mean()\nagg_rescureid_1.columns = ['Age_id','Health_id','PhotoAmt_id','Quantity_id',\n                   'doc_sent_mag_id', 'doc_sent_score_id', \n                   'dominant_score_id', 'label_score_id','Description_len_id']\nagg_rescureid_2 = X.groupby(['RescuerID'])['Breed1'].aggregate({'307_ratio':lambda x:(x==307).mean()})\nagg_rescureid = pd.concat([agg_rescureid_1,agg_rescureid_2],axis=1)\nagg_rescureid.columns = [gzf_prefix+x for x in agg_rescureid.columns ]\nX = pd.merge(X,agg_rescureid,left_on='RescuerID',right_index=True,how='left')","execution_count":29,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## TFIDF NMF LDA"},{"metadata":{"trusted":true,"_uuid":"e206c257054a213edb8184d20a40fd86476c2fe0"},"cell_type":"code","source":"SVD_FEATURES = 120\nNMF_FEATURES = 20\nLDA_FEATURES = 12\n\ndesc = X.raw_text.fillna(\"none\").values\ntfidf = TfidfVectorizer(min_df=3,  max_features=10000,\n        strip_accents='unicode', analyzer='word', token_pattern=r'\\w{1,}',\n        ngram_range=(1, 3), use_idf=1, smooth_idf=1, sublinear_tf=1,\n        stop_words = 'english')\n    \n# Fit TFIDF\nX_tfidf = tfidf.fit_transform(list(desc))\n\nsvd = TruncatedSVD(n_components=SVD_FEATURES)\nsvd.fit(X_tfidf)\nX_svd = svd.fit_transform(X_tfidf)\n\nX_svd = pd.DataFrame(X_svd, columns=[gzf_prefix+'sdv_{}'.format(i) for i in range(SVD_FEATURES)])\nX = pd.concat((X, X_svd), axis=1)\n\nnmf = NMF(n_components=NMF_FEATURES)\nnmf.fit(X_tfidf)\nX_nmf = nmf.fit_transform(X_tfidf)\n\nX_nmf = pd.DataFrame(X_nmf, columns=[gzf_prefix+'mnf_{}'.format(i) for i in range(NMF_FEATURES)])\nX = pd.concat((X, X_nmf), axis=1)\n\nlda = LatentDirichletAllocation(n_components=LDA_FEATURES, n_jobs=-1,max_iter=120)\nlda.fit(X_tfidf)\nX_lda = lda.fit_transform(X_tfidf)\n\nX_lda = pd.DataFrame(X_lda, columns=[gzf_prefix+'lad_{}'.format(i) for i in range(LDA_FEATURES)])\nX = pd.concat((X, X_lda), axis=1)\n","execution_count":30,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6e0e91b04f806a9a92eae2cb3a9ec80fcd0ed71f"},"cell_type":"code","source":"cat_cols = ['Health',\n 'Breed1', 'Breed2',\n 'Type', 'Gender',\n 'Color3', 'Color2', 'Color1',\n 'Vaccinated','Sterilized',  'Dewormed',\n 'MaturitySize', 'FurLength',\n 'State','meta_label_description','meta_label_description1','meta_label_description2']\nX.loc[:, cat_cols] = X[cat_cols].astype('category')","execution_count":31,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"37ae613cab00f1345db46aacf797e1d8da3a0a5a"},"cell_type":"code","source":"foo = train.dtypes\ncat_feature_names = foo[foo == \"category\"].index.values\ncat_features = [i for i in range(X.shape[1]) if X.columns[i] in cat_feature_names]","execution_count":32,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c6ae29181c8a582e48371b7bf100c46fbf30ffd3"},"cell_type":"code","source":"train = X[:len_train]\ntest = X[len_train:]\ntrain.index = range(len_train)\ntest.index = range(test.shape[0])\n\ntarget = train['AdoptionSpeed']\nrescue_id = train['RescuerID']\n\ntrain.shape, target.shape","execution_count":33,"outputs":[{"output_type":"execute_result","execution_count":33,"data":{"text/plain":"((14993, 633), (14993,))"},"metadata":{}}]},{"metadata":{"trusted":true,"_uuid":"63ab7c50d5c6826311c613dcb800cba7e1242080"},"cell_type":"markdown","source":"# training functions for FT 1 and 2"},{"metadata":{"trusted":true,"_uuid":"da8c42ee4530d5f11f4247fe8d2d9b39d772952b"},"cell_type":"code","source":"def obtain_train_mse_and_kappa(train_predictions, target):\n    optR = OptimizedRounder()\n    optR.fit(train_predictions, target)\n    coefficients_ = optR.coefficients()\n    rmse_score1 = rmse(target, train_predictions)\n    train_predictions = optR.predict(train_predictions, optR.coefficients()).astype(int)\n    qwk_score = quadratic_weighted_kappa(target, train_predictions)\n    rmse_score2 = rmse(target, train_predictions)\n    \n    return rmse_score1, rmse_score2, qwk_score\n\ndef run_cv_model(train, test, target, weight, model_fn, params={}, eval_fn=None, label='model'):\n    kf = FOLDS\n    n_splits = N_FOLDS\n    \n    fold_splits = kf.split(train, target)\n    cv_scores = []\n    qwk_scores = []\n    pred_full_test = 0\n    pred_train = np.zeros((train.shape[0], n_splits))\n    pred_test = np.zeros((origin_test.shape[0], n_splits))\n    \n    all_coefficients = np.zeros((n_splits, 4))\n    i = 1\n    for dev_index, val_index in fold_splits:\n        print('Started ' + label + ' fold ' + str(i) + '/{}'.format(n_splits))\n        if isinstance(train, pd.DataFrame):\n            dev_X, val_X = train.iloc[dev_index], train.iloc[val_index]\n            dev_y, val_y = target[dev_index], target[val_index]\n            dev_weight, val_weight = weight[dev_index], weight[val_index]\n        else:\n            dev_X, val_X = train[dev_index], train[val_index]\n            dev_y, val_y = target[dev_index], target[val_index]\n            dev_weight, val_weight = weight[dev_index], weight[val_index]\n            \n        params2 = params.copy()\n        pred_val_y, pred_test_y, importances, coefficients, qwk = model_fn(dev_X, dev_y, val_X, val_y, dev_weight, val_weight, test, params2)\n        pred_full_test = pred_full_test + pred_test_y\n        pred_train[val_index] = pred_val_y\n        pred_test[:, i-1] = pred_test_y.reshape(-1)\n        \n        all_coefficients[i-1, :] = coefficients\n        if eval_fn is not None:\n            cv_score = eval_fn(val_y, pred_val_y)\n            cv_scores.append(cv_score)\n            qwk_scores.append(qwk)\n            print(label + ' cv score {}: RMSE {} QWK {}'.format(i, cv_score, qwk))\n        i += 1\n    train_rmse1,  train_rmse2, train_qwk = obtain_train_mse_and_kappa([r[0] for r in pred_train], target)\n    print('{} cv RMSE scores : {}'.format(label, cv_scores))\n    print('{} cv mean        RMSE score : {}'.format(label, np.mean(cv_scores)))\n    print('{} cv recalculate RMSE1 score : {}'.format(label, train_rmse1))\n    print('{} cv recalculate RMSE2 score : {}'.format(label, train_rmse2))\n    print('{} cv std RMSE score : {}'.format(label, np.std(cv_scores)))\n    print('{} cv QWK scores : {}'.format(label, qwk_scores))\n    print('{} cv mean        QWK score : {}'.format(label, np.mean(qwk_scores)))\n    print('{} cv recalculate QWK score : {}'.format(label, train_qwk))\n    print('{} cv std QWK score : {}'.format(label, np.std(qwk_scores)))\n    pred_full_test = pred_full_test / float(n_splits)\n    results = {'label': label,\n               'train': pred_train, 'test': pred_full_test, 'test_value':pred_test,\n                'cv': cv_scores, 'qwk': qwk_scores,\n               'coefficients': all_coefficients}\n    return results\n\ndef runLGB(train_X, train_y, test_X, test_y, dev_weight, val_weight, test_X2, params):\n    print('Prep LGB')\n\n    d_train = lgb.Dataset(train_X, label=train_y, weight=dev_weight)\n    d_valid = lgb.Dataset(test_X, label=test_y, weight=val_weight)\n    watchlist = [d_train, d_valid]\n    print('Train LGB')\n    num_rounds = params.pop('num_rounds')\n    verbose_eval = params.pop('verbose_eval')\n    early_stop = None\n    if params.get('early_stop'):\n        early_stop = params.pop('early_stop')\n    model = lgb.train(params,\n                      train_set=d_train,\n                      num_boost_round=num_rounds,\n                      valid_sets=watchlist,\n                      verbose_eval=verbose_eval,\n                      early_stopping_rounds=early_stop)\n    print('Predict 1/2')\n    pred_test_y = model.predict(test_X, num_iteration=model.best_iteration)\n    pred_test_y2 = model.predict(test_X2, num_iteration=model.best_iteration)\n    importances = model.feature_importance()\n    optR = OptimizedRounder_v3()\n    len_0 = test_y[test_y==0].shape[0]\n    optR.fit(pred_test_y, test_y)\n    coefficients = optR.coefficients()\n    pred_test_y_k = optR.predict(pred_test_y, coefficients, len_0)\n    print(\"Valid Counts = \", Counter(test_y))\n    print(\"Predicted Counts = \", Counter(pred_test_y_k))\n    print(\"Coefficients = \", coefficients)\n    qwk = quadratic_weighted_kappa(test_y, pred_test_y_k)\n    print(\"QWK = \", qwk)\n    print('Predict 2/2')\n    return np.array(pred_test_y).reshape(-1, 1), np.array(pred_test_y2).reshape(-1, 1), importances, coefficients, qwk","execution_count":34,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"98d67a5b617ef54ffd60f437c6acce75f41e6432"},"cell_type":"code","source":"def get_cols(totals, prefixs):\n    if isinstance(prefixs, list):\n        cols = []\n        for prefix in prefixs:\n            cols += [col for col in totals if col.find(prefix) > -1]\n        return cols\n    else:\n        return [col for col in totals if col.find(prefixs) > -1]\n\norigin_cols = [\n    \"Type\",\"Age\",\n    \"Breed1\",\"Breed2\",\"Gender\",\n    \"Color1\",\"Color2\",\"Color3\",\n    \"MaturitySize\",\"FurLength\",\n    \"Vaccinated\",\"Dewormed\",\"Sterilized\",\"Health\",\n    \"Quantity\",\"Fee\",\"State\",\n    \"VideoAmt\",\"PhotoAmt\"\n]\n\ndoc_cols = get_cols(train.columns, 'doc_')\nmeta_cols = get_cols(train.columns, 'meta_')\npure_cols = get_cols(train.columns, 'pure_')\nrescue_cols = get_cols(train.columns, 'rescue_')\nlang_cols = get_cols(train.columns, 'lang_')\nsml_cols = get_cols(train.columns, ['svd_', 'lda_', 'nmf_'])\npic_cols = get_cols(train.columns, 'pic_')","execution_count":35,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3d600d917d061d2db2cd7d738d57683fd0456802"},"cell_type":"code","source":"train['ResNet_meta'] = train_img_prob.flatten()\ntest['ResNet_meta'] = test_img_prob.flatten()","execution_count":36,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3efb23cf9be8789f7b0adb3676225526cadeb4de"},"cell_type":"code","source":"gzf_cols = doc_cols + lang_cols + origin_cols + pic_cols + [\n                'meta_dominant_blue', 'meta_dominant_green','meta_dominant_pixel_frac', \n                'meta_dominant_red', 'meta_dominant_score', 'meta_label_score', \n                'meta_vertex_x', 'meta_vertex_y'] + [\n                gzf_prefix+'RescureID_rank',gzf_prefix+'Description_len',\n                gzf_prefix+'Description_word_len',gzf_prefix+'Description_distinct_word_len',\n                gzf_prefix+'Description_distinct_word_ratio',\n                gzf_prefix+'is_pure',gzf_prefix+'is_pure_breed1',gzf_prefix+'is_pure_breed2',\n                gzf_prefix+'Quantity_id',gzf_prefix+'307_ratio'\n                ] + [gzf_prefix+'sdv_{}'.format(i) for i in range(SVD_FEATURES)] \\\n                  + [gzf_prefix+'mnf_{}'.format(i) for i in range(NMF_FEATURES)] \\\n                  + [gzf_prefix+'lad_{}'.format(i) for i in range(LDA_FEATURES)] \\\n                  + ['ResNet_meta']","execution_count":37,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b5c92d3ca8b7bd764b90107366956110b424348a"},"cell_type":"code","source":"zkr_cols = origin_cols + doc_cols + meta_cols + pure_cols + rescue_cols + lang_cols + sml_cols + pic_cols + ['ResNet_meta']","execution_count":38,"outputs":[]},{"metadata":{"_uuid":"5ae8178d84b542125fbd0c9a8eed69fbe139d838"},"cell_type":"markdown","source":"# Training FT 1 and 2"},{"metadata":{"trusted":true,"_uuid":"7486217ec895cdef8991f9b3c025f81d12881afe"},"cell_type":"code","source":"train_gzf = train[gzf_cols]\ntest_gzf = test[gzf_cols]\n\ntrain_zkr = train[zkr_cols]\ntest_zkr = test[zkr_cols]\n\nprint(train_gzf.shape, test_gzf.shape, train_zkr.shape, test_zkr.shape)","execution_count":39,"outputs":[{"output_type":"stream","text":"(14993, 452) (3972, 452) (14993, 457) (3972, 457)\n","name":"stdout"}]},{"metadata":{"_uuid":"6012b936da8650cdd0eca14befdea8e498224f1c"},"cell_type":"markdown","source":"## LGB"},{"metadata":{"trusted":true,"_uuid":"07b7ee7267ff3bb7421dcf5df366cd339eeaf7fa"},"cell_type":"code","source":"params = {'application': 'regression',\n          'boosting': 'gbdt',\n          'metric': 'rmse',\n          'num_leaves': 80,\n          'max_depth': 9,\n          'learning_rate': 0.01,\n          'bagging_fraction': 0.9,\n          'bagging_freq': 3,\n          'feature_fraction': 0.85,\n          'min_split_gain': 0.01,\n          'min_child_samples': 150,\n          'min_child_weight': 0.1,\n          'verbosity': -1,\n          'data_random_seed': 3,\n          'early_stop': 100,\n          'verbose_eval': 500,\n          'num_rounds': 5000\n         }\n\nweight = pd.Series(np.where(train['Type']==2, 1.0, 1.0))\nlgb_gzf = run_cv_model(train[gzf_cols], test[gzf_cols], target, weight, runLGB, params, rmse, 'lgb')","execution_count":40,"outputs":[{"output_type":"stream","text":"Started lgb fold 1/4\nPrep LGB\nTrain LGB\nTraining until validation scores don't improve for 100 rounds.\n[500]\ttraining's rmse: 0.832749\tvalid_1's rmse: 1.04387\n[1000]\ttraining's rmse: 0.707013\tvalid_1's rmse: 1.03652\nEarly stopping, best iteration is:\n[1272]\ttraining's rmse: 0.651996\tvalid_1's rmse: 1.03519\nPredict 1/2\nValid Counts =  Counter({4.0: 1050, 2.0: 1010, 3.0: 815, 1.0: 773, 0.0: 103})\nPredicted Counts =  Counter({2.0: 1270, 4.0: 1019, 1.0: 714, 3.0: 657, 0.0: 91})\nCoefficients =  [0.49110651 2.05002231 2.56081551 2.86398469]\nQWK =  0.4617867479103991\nPredict 2/2\nlgb cv score 1: RMSE 1.0351866548278965 QWK 0.4617867479103991\nStarted lgb fold 2/4\nPrep LGB\nTrain LGB\nTraining until validation scores don't improve for 100 rounds.\n[500]\ttraining's rmse: 0.83197\tvalid_1's rmse: 1.04014\n[1000]\ttraining's rmse: 0.703062\tvalid_1's rmse: 1.03191\n[1500]\ttraining's rmse: 0.602803\tvalid_1's rmse: 1.02984\nEarly stopping, best iteration is:\n[1511]\ttraining's rmse: 0.600867\tvalid_1's rmse: 1.02977\nPredict 1/2\nValid Counts =  Counter({4.0: 1049, 2.0: 1009, 3.0: 815, 1.0: 773, 0.0: 103})\nPredicted Counts =  Counter({2.0: 1224, 3.0: 1201, 4.0: 863, 1.0: 370, 0.0: 91})\nCoefficients =  [0.49641263 1.86797011 2.41368967 2.98063877]\nQWK =  0.4524020809875716\nPredict 2/2\nlgb cv score 2: RMSE 1.0297698540904212 QWK 0.4524020809875716\nStarted lgb fold 3/4\nPrep LGB\nTrain LGB\nTraining until validation scores don't improve for 100 rounds.\n[500]\ttraining's rmse: 0.834522\tvalid_1's rmse: 1.02274\n[1000]\ttraining's rmse: 0.705992\tvalid_1's rmse: 1.01574\nEarly stopping, best iteration is:\n[1053]\ttraining's rmse: 0.695573\tvalid_1's rmse: 1.01546\nPredict 1/2\nValid Counts =  Counter({4.0: 1049, 2.0: 1009, 3.0: 815, 1.0: 772, 0.0: 102})\nPredicted Counts =  Counter({2.0: 1751, 4.0: 1216, 1.0: 690, 0.0: 90})\nCoefficients =  [0.5188965  2.04905357 2.74870938 2.06497036]\nQWK =  0.47807723795151436\nPredict 2/2\nlgb cv score 3: RMSE 1.0154597540724297 QWK 0.47807723795151436\nStarted lgb fold 4/4\nPrep LGB\nTrain LGB\nTraining until validation scores don't improve for 100 rounds.\n[500]\ttraining's rmse: 0.832408\tvalid_1's rmse: 1.03151\n[1000]\ttraining's rmse: 0.700057\tvalid_1's rmse: 1.02393\nEarly stopping, best iteration is:\n[1371]\ttraining's rmse: 0.624415\tvalid_1's rmse: 1.02281\nPredict 1/2\nValid Counts =  Counter({4.0: 1049, 2.0: 1009, 3.0: 814, 1.0: 772, 0.0: 102})\nPredicted Counts =  Counter({3.0: 1102, 2.0: 1044, 4.0: 967, 1.0: 543, 0.0: 90})\nCoefficients =  [0.52095592 1.95950246 2.38466513 2.87723243]\nQWK =  0.47302579687570645\nPredict 2/2\nlgb cv score 4: RMSE 1.0228056549091529 QWK 0.47302579687570645\nlgb cv RMSE scores : [1.0351866548278965, 1.0297698540904212, 1.0154597540724297, 1.0228056549091529]\nlgb cv mean        RMSE score : 1.0258054794749751\nlgb cv recalculate RMSE1 score : 1.0258354924378006\nlgb cv recalculate RMSE2 score : 1.2140652832964334\nlgb cv std RMSE score : 0.007412079644479627\nlgb cv QWK scores : [0.4617867479103991, 0.4524020809875716, 0.47807723795151436, 0.47302579687570645]\nlgb cv mean        QWK score : 0.46632296593129785\nlgb cv recalculate QWK score : 0.4721546275266236\nlgb cv std QWK score : 0.009968188964703624\n","name":"stdout"}]},{"metadata":{"trusted":true,"_uuid":"2af8cc081cbec43a01cf3a2ba4fbc7ae11986aaa","scrolled":false},"cell_type":"code","source":"params = {'application': 'regression',\n          'boosting': 'gbdt',\n          'metric': 'rmse',\n          'num_leaves': 80,\n          'max_depth': 9,\n          'learning_rate': 0.01,\n          'bagging_fraction': 0.9,\n          'bagging_freq': 3,\n          'feature_fraction': 0.84,\n          'min_split_gain': 0.01,\n          'min_child_samples': 150,\n          'min_child_weight': 0.1,\n          'verbosity': -1,\n          'data_random_seed': 3,\n          'verbose_eval': 500,\n          'num_rounds': 1500,\n         }\n\nweight = pd.Series(np.where(train['Type']==2, 1.0, 1.0))\nlgb_zkr = run_cv_model(train[zkr_cols], test[zkr_cols], target, weight, runLGB, params, rmse, 'lgb')","execution_count":null,"outputs":[{"output_type":"stream","text":"Started lgb fold 1/4\nPrep LGB\nTrain LGB\n[500]\ttraining's rmse: 0.828353\tvalid_1's rmse: 1.04354\n[1000]\ttraining's rmse: 0.700263\tvalid_1's rmse: 1.03543\n[1500]\ttraining's rmse: 0.597154\tvalid_1's rmse: 1.033\nPredict 1/2\nValid Counts =  Counter({4.0: 1050, 2.0: 1010, 3.0: 815, 1.0: 773, 0.0: 103})\nPredicted Counts =  Counter({2.0: 1748, 4.0: 1246, 1.0: 666, 0.0: 91})\nCoefficients =  [0.52215962 2.0265292  2.75981032 1.58504158]\nQWK =  0.4576383478618269\nPredict 2/2\nlgb cv score 1: RMSE 1.0330003567066113 QWK 0.4576383478618269\nStarted lgb fold 2/4\nPrep LGB\nTrain LGB\n[500]\ttraining's rmse: 0.827086\tvalid_1's rmse: 1.03639\n[1000]\ttraining's rmse: 0.693852\tvalid_1's rmse: 1.02738\n[1500]\ttraining's rmse: 0.591903\tvalid_1's rmse: 1.02481\nPredict 1/2\nValid Counts =  Counter({4.0: 1049, 2.0: 1009, 3.0: 815, 1.0: 773, 0.0: 103})\nPredicted Counts =  Counter({2.0: 1699, 4.0: 948, 3.0: 751, 1.0: 260, 0.0: 91})\nCoefficients =  [0.52093116 1.79473986 2.56482092 2.93222952]\nQWK =  0.46864051063799317\nPredict 2/2\nlgb cv score 2: RMSE 1.0248102501490413 QWK 0.46864051063799317\nStarted lgb fold 3/4\nPrep LGB\nTrain LGB\n","name":"stdout"}]},{"metadata":{"_uuid":"bee31d2749404d7823a8e5760b211d3ee6c8e8d0"},"cell_type":"markdown","source":"# Feature table 3"},{"metadata":{},"cell_type":"markdown","source":"## reload"},{"metadata":{"trusted":true,"_uuid":"c46adc48f8e281f8b711532fd5939dc435a92758"},"cell_type":"code","source":"del train, test\ngc.collect()\n\ntrain = pd.read_csv(\"../input/petfinder-adoption-prediction/train/train.csv\")\ntest = pd.read_csv(\"../input/petfinder-adoption-prediction/test/test.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## color"},{"metadata":{"trusted":true,"_uuid":"ceb695e4c6261af2b3e8ec45b0127a8fbf0bafd0"},"cell_type":"code","source":"train['Color'] = train.Color1 * 100 + train.Color2 * 10 + train.Color3\ntrain.drop(['Color1', 'Color2', 'Color3'], axis=1, inplace=True)\n\ntest['Color'] = test.Color1 * 100 + test.Color2 * 10 + test.Color3\ntest.drop(['Color1', 'Color2', 'Color3'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1bc1883b1fcea53b2512319211cecc6ead627fc9"},"cell_type":"code","source":"target = train['AdoptionSpeed']\ntrain_id = train['PetID']\ntest_id = test['PetID']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## sentiment data"},{"metadata":{"trusted":true,"_uuid":"e80ed6377975f9af61baa893d364d6e80f2897e3"},"cell_type":"code","source":"doc_sent_mag = []\ndoc_sent_score = []\nnf_count = 0\nfor pet in train_id:\n    try:\n        with open('../input/petfinder-adoption-prediction/train_sentiment/' + pet + '.json', 'r') as f:\n            sentiment = json.load(f)\n        doc_sent_mag.append(sentiment['documentSentiment']['magnitude'])\n        doc_sent_score.append(sentiment['documentSentiment']['score'])\n    except FileNotFoundError:\n        nf_count += 1\n        doc_sent_mag.append(-1)\n        doc_sent_score.append(-1)\n\ntrain.loc[:, 'doc_sent_mag'] = doc_sent_mag\ntrain.loc[:, 'doc_sent_score'] = doc_sent_score\ntrain[\"doc_sentiment\"] = train.doc_sent_mag * train.doc_sent_score\n\ndoc_sent_mag = []\ndoc_sent_score = []\nnf_count = 0\nfor pet in test_id:\n    try:\n        with open('../input/petfinder-adoption-prediction/test_sentiment/' + pet + '.json', 'r') as f:\n            sentiment = json.load(f)\n        doc_sent_mag.append(sentiment['documentSentiment']['magnitude'])\n        doc_sent_score.append(sentiment['documentSentiment']['score'])\n    except FileNotFoundError:\n        nf_count += 1\n        doc_sent_mag.append(-1)\n        doc_sent_score.append(-1)\n\ntest.loc[:, 'doc_sent_mag'] = doc_sent_mag\ntest.loc[:, 'doc_sent_score'] = doc_sent_score\ntest[\"doc_sentiment\"] = test.doc_sent_mag * test.doc_sent_score","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## TFIDF"},{"metadata":{"trusted":true,"_uuid":"d66570d07ebdf9669e41eca61126ab50394cd6a1"},"cell_type":"code","source":"n_components = 150\n\ntrain_desc = train.Description.fillna(\"none\").values\ntest_desc = test.Description.fillna(\"none\").values\n\ntfv = TfidfVectorizer(min_df=3,  max_features=None,\n        strip_accents='unicode', analyzer='word', token_pattern=r'\\w{1,}',\n        ngram_range=(1, 3), use_idf=1, smooth_idf=1, sublinear_tf=1,\n        stop_words='english')\n\ntfv.fit(list(train_desc))\nX = tfv.transform(train_desc)\nX_test = tfv.transform(test_desc)\n\nsvd = TruncatedSVD(n_components=n_components)\nsvd.fit(X)\nX = svd.transform(X)\n\nX = pd.DataFrame(X, columns=['svd_{}'.format(i) for i in range(n_components)])\ntrain = pd.concat((train, X), axis=1)\nX_test = svd.transform(X_test)\nX_test = pd.DataFrame(X_test, columns=['svd_{}'.format(i) for i in range(n_components)])\ntest = pd.concat((test, X_test), axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## image metadata"},{"metadata":{"trusted":true,"_uuid":"4ec54e72c80fc72db27ff11281fd99eb1f1f88a0"},"cell_type":"code","source":"img_xs = []\nimg_ys = []\nvertex_xs = []\nvertex_ys = []\nbounding_confidences = []\nbounding_importance_fracs = []\ndominant_blues = []\ndominant_greens = []\ndominant_reds = []\ndominant_pixel_fracs = []\ndominant_scores = []\nlabel_descriptions = []\nlabel_scores = []\nnf_count = 0\nnl_count = 0\nfor pet in train_id:\n    try:\n        im = Image.open('../input/petfinder-adoption-prediction/train_images/%s-1.jpg' % pet)\n        width, height = im.size\n        img_xs.append(width)\n        img_ys.append(height)\n        with open('../input/petfinder-adoption-prediction/train_metadata/' + pet + '-1.json', 'r') as f:\n            data = json.load(f)\n        vertex_x = data['cropHintsAnnotation']['cropHints'][0]['boundingPoly']['vertices'][2]['x']\n        vertex_xs.append(vertex_x)\n        vertex_y = data['cropHintsAnnotation']['cropHints'][0]['boundingPoly']['vertices'][2]['y']\n        vertex_ys.append(vertex_y)\n        bounding_confidence = data['cropHintsAnnotation']['cropHints'][0]['confidence']\n        bounding_confidences.append(bounding_confidence)\n        bounding_importance_frac = data['cropHintsAnnotation']['cropHints'][0].get('importanceFraction', -1)\n        bounding_importance_fracs.append(bounding_importance_frac)\n        dominant_blue = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['color']['blue']\n        dominant_blues.append(dominant_blue)\n        dominant_green = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['color']['green']\n        dominant_greens.append(dominant_green)\n        dominant_red = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['color']['red']\n        dominant_reds.append(dominant_red)\n        dominant_pixel_frac = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['pixelFraction']\n        dominant_pixel_fracs.append(dominant_pixel_frac)\n        dominant_score = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['score']\n        dominant_scores.append(dominant_score)\n        if data.get('labelAnnotations'):\n            label_description = data['labelAnnotations'][0]['description']\n            label_descriptions.append(label_description)\n            label_score = data['labelAnnotations'][0]['score']\n            label_scores.append(label_score)\n        else:\n            nl_count += 1\n            label_descriptions.append('nothing')\n            label_scores.append(-1)\n    except FileNotFoundError:\n        nf_count += 1\n        img_xs.append(-1)\n        img_ys.append(-1)\n        vertex_xs.append(-1)\n        vertex_ys.append(-1)\n        bounding_confidences.append(-1)\n        bounding_importance_fracs.append(-1)\n        dominant_blues.append(-1)\n        dominant_greens.append(-1)\n        dominant_reds.append(-1)\n        dominant_pixel_fracs.append(-1)\n        dominant_scores.append(-1)\n        label_descriptions.append('nothing')\n        label_scores.append(-1)\n\ntrain.loc[:, 'img_x'] = img_xs\ntrain.loc[:, 'img_y'] = img_ys\ntrain.loc[:, 'vertex_x'] = vertex_xs\ntrain.loc[:, 'vertex_y'] = vertex_ys\ntrain.loc[:, 'bounding_confidence'] = bounding_confidences\ntrain.loc[:, 'bounding_importance'] = bounding_importance_fracs\ntrain.loc[:, 'dominant_blue'] = dominant_blues\ntrain.loc[:, 'dominant_green'] = dominant_greens\ntrain.loc[:, 'dominant_red'] = dominant_reds\ntrain.loc[:, 'dominant_pixel_frac'] = dominant_pixel_fracs\ntrain.loc[:, 'dominant_score'] = dominant_scores\ntrain.loc[:, 'label_description'] = label_descriptions\ntrain.loc[:, 'label_score'] = label_scores\n\nimg_xs = []\nimg_ys = []\nvertex_xs = []\nvertex_ys = []\nbounding_confidences = []\nbounding_importance_fracs = []\ndominant_blues = []\ndominant_greens = []\ndominant_reds = []\ndominant_pixel_fracs = []\ndominant_scores = []\nlabel_descriptions = []\nlabel_scores = []\nnf_count = 0\nnl_count = 0\nfor pet in test_id:\n    try:\n        im = Image.open('../input/petfinder-adoption-prediction/test_images/%s-1.jpg' % pet)\n        width, height = im.size\n        img_xs.append(width)\n        img_ys.append(height)\n        with open('../input/petfinder-adoption-prediction/test_metadata/' + pet + '-1.json', 'r') as f:\n            data = json.load(f)\n        vertex_x = data['cropHintsAnnotation']['cropHints'][0]['boundingPoly']['vertices'][2]['x']\n        vertex_xs.append(vertex_x)\n        vertex_y = data['cropHintsAnnotation']['cropHints'][0]['boundingPoly']['vertices'][2]['y']\n        vertex_ys.append(vertex_y)\n        bounding_confidence = data['cropHintsAnnotation']['cropHints'][0]['confidence']\n        bounding_confidences.append(bounding_confidence)\n        bounding_importance_frac = data['cropHintsAnnotation']['cropHints'][0].get('importanceFraction', -1)\n        bounding_importance_fracs.append(bounding_importance_frac)\n        dominant_blue = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['color']['blue']\n        dominant_blues.append(dominant_blue)\n        dominant_green = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['color']['green']\n        dominant_greens.append(dominant_green)\n        dominant_red = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['color']['red']\n        dominant_reds.append(dominant_red)\n        dominant_pixel_frac = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['pixelFraction']\n        dominant_pixel_fracs.append(dominant_pixel_frac)\n        dominant_score = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['score']\n        dominant_scores.append(dominant_score)\n        if data.get('labelAnnotations'):\n            label_description = data['labelAnnotations'][0]['description']\n            label_descriptions.append(label_description)\n            label_score = data['labelAnnotations'][0]['score']\n            label_scores.append(label_score)\n        else:\n            nl_count += 1\n            label_descriptions.append('nothing')\n            label_scores.append(-1)\n    except FileNotFoundError:\n        nf_count += 1\n        img_xs.append(-1)\n        img_ys.append(-1)\n        vertex_xs.append(-1)\n        vertex_ys.append(-1)\n        bounding_confidences.append(-1)\n        bounding_importance_fracs.append(-1)\n        dominant_blues.append(-1)\n        dominant_greens.append(-1)\n        dominant_reds.append(-1)\n        dominant_pixel_fracs.append(-1)\n        dominant_scores.append(-1)\n        label_descriptions.append('nothing')\n        label_scores.append(-1)\n\ntest.loc[:, 'img_x'] = img_xs\ntest.loc[:, 'img_y'] = img_ys\ntest.loc[:, 'vertex_x'] = vertex_xs\ntest.loc[:, 'vertex_y'] = vertex_ys\ntest.loc[:, 'bounding_confidence'] = bounding_confidences\ntest.loc[:, 'bounding_importance'] = bounding_importance_fracs\ntest.loc[:, 'dominant_blue'] = dominant_blues\ntest.loc[:, 'dominant_green'] = dominant_greens\ntest.loc[:, 'dominant_red'] = dominant_reds\ntest.loc[:, 'dominant_pixel_frac'] = dominant_pixel_fracs\ntest.loc[:, 'dominant_score'] = dominant_scores\ntest.loc[:, 'label_description'] = label_descriptions\ntest.loc[:, 'label_score'] = label_scores","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1d77f0bc5eb591afbd209212e4e033acf3e22fcd"},"cell_type":"code","source":"train[\"vertex_x_ratio\"] = train.vertex_x / train.img_x\ntrain[\"vertex_y_ratio\"] = train.vertex_y / train.img_y\n\ntest[\"vertex_x_ratio\"] = test.vertex_x / test.img_x\ntest[\"vertex_y_ratio\"] = test.vertex_y / test.img_y","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## name"},{"metadata":{"trusted":true,"_uuid":"facbbb3998c6cdf908ef85c7f975be180137d18e"},"cell_type":"code","source":"train.Name = train.Name.fillna('')\ntest.Name = test.Name.fillna('')\ntrain[\"Name\"] = train.Name.apply(lambda x: str(x).lower())\ntest[\"Name\"] = test.Name.apply(lambda x: str(x).lower())\n\ntrain[\"name_length\"] = train.Name.apply(lambda x: len(str(x)))\ntest[\"name_length\"] = test.Name.apply(lambda x: len(str(x)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5f9817c1ffc542b80205a0fb54f77c6a8e357962"},"cell_type":"code","source":"all_data = pd.concat((train, test))\n\nname_idx, name_val = all_data.Name.value_counts().index, all_data.Name.value_counts().values\nname_map = dict()\nfor idx, val in zip(name_idx, name_val):\n    name_map.update({idx: val})\n\ntrain[\"name_cnt\"] = train.Name.map(name_map)\ntest[\"name_cnt\"] = test.Name.map(name_map)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## description"},{"metadata":{"trusted":true,"_uuid":"f9757e92ef33b48ed6e2ab7d8288bfb346cbc470"},"cell_type":"code","source":"train['Description'] = train['Description'].fillna('')\ntest['Description'] = test['Description'].fillna('')\n\ntrain['desc_length'] = train['Description'].apply(lambda x: len(x))\ntrain['desc_words'] = train['Description'].apply(lambda x: len(x.split()))\n\ntest['desc_length'] = test['Description'].apply(lambda x: len(x))\ntest['desc_words'] = test['Description'].apply(lambda x: len(x.split()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ae07ed429a63978116f8391e2eca56f1243f8466"},"cell_type":"code","source":"puncts = [',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', \"'\", '$', '&', '/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', '',  '~', '@', '', \n '', '_', '{', '}', '^']\n\ndef lexical_density(x):\n    for punct in puncts:\n        x = x.replace(punct, \"\")\n    li = x.split(\" \")\n    return len(set(li)) / len(li) if len(li) != 0 else 0\n\ntrain[\"desc_lexical_density\"] = train.Description.apply(lambda x: lexical_density(x))\ntest[\"desc_lexical_density\"] = test.Description.apply(lambda x: lexical_density(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b82a805d4966a5400aee74f0637c68d7a6c05599"},"cell_type":"code","source":"def sentences_count(x):\n    return len(re.split(r'[.!?]+', x))\n\ntrain[\"sentences_count\"] = train.Description.apply(lambda x: sentences_count(x))\ntest[\"sentences_count\"] = test.Description.apply(lambda x: sentences_count(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ac708a00a85ec3391ec826258b9fd883fdbc71c7"},"cell_type":"code","source":"def find_capitals(x):\n    return len(re.findall('[A-Z]', x))\n\ntrain[\"desc_capitals\"] = train.Description.apply(lambda x: find_capitals(x))\ntest[\"desc_capitals\"] = test.Description.apply(lambda x: find_capitals(x))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## rescuer"},{"metadata":{"trusted":true,"_uuid":"942380bb413fc4d76e81f7d0e8089a38ac5b6438"},"cell_type":"code","source":"rescuer_idx, rescuer_val = all_data.RescuerID.value_counts().index, all_data.RescuerID.value_counts().values\nrescuer_map = dict()\nfor idx, val in zip(rescuer_idx, rescuer_val):\n    rescuer_map.update({idx: val})\n\ntrain[\"rescuer_cnt\"] = train.RescuerID.map(rescuer_map)\ntest[\"rescuer_cnt\"] = test.RescuerID.map(rescuer_map)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## state"},{"metadata":{"trusted":true,"_uuid":"74db8e1bdb137b6599b9e83e603924803b083d77"},"cell_type":"code","source":"# state GDP: https://en.wikipedia.org/wiki/List_of_Malaysian_states_by_GDP\nstate_gdp = {\n    41336: 116.679,\n    41325: 40.596,\n    41367: 23.02,\n    41401: 190.075,\n    41415: 5.984,\n    41324: 37.274,\n    41332: 42.389,\n    41335: 52.452,\n    41330: 67.629,\n    41380: 5.642,\n    41327: 81.284,\n    41345: 80.167,\n    41342: 121.414,\n    41326: 280.698,\n    41361: 32.270\n}\n\n# state population: https://zh.wikipedia.org/wiki/%E9%A9%AC%E6%9D%A5%E8%A5%BF%E4%BA%9A\nstate_population = {\n    41336: 33.48283,\n    41325: 19.47651,\n    41367: 15.39601,\n    41401: 16.74621,\n    41415: 0.86908,\n    41324: 8.21110,\n    41332: 10.21064,\n    41335: 15.00817,\n    41330: 23.52743,\n    41380: 2.31541,\n    41327: 15.61383,\n    41345: 32.06742,\n    41342: 24.71140,\n    41326: 54.62141,\n    41361: 10.35977\n}\n\n# state area\nstate_area = {\n    41336: 19.210,\n    41325: 9.500,\n    41367: 15.099,\n    41401: 0.243,\n    41415: 0.091,\n    41324: 1.664,\n    41332: 6.686,\n    41335: 36.137,\n    41330: 21.035,\n    41380: 2.31541,\n    41327: 0.821,\n    41345: 73.631,\n    41342: 124.450,\n    41326: 8.104,\n    41361: 13.035\n}\n\ntrain[\"state_gdp\"] = train.State.map(state_gdp)\ntrain[\"state_population\"] = train.State.map(state_population)\ntrain[\"state_area\"] = train.State.map(state_area)\ntest[\"state_gdp\"] = test.State.map(state_gdp)\ntest[\"state_population\"] = test.State.map(state_population)\ntest[\"state_area\"] = test.State.map(state_area)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## pure breed"},{"metadata":{"trusted":true,"_uuid":"63cd3e2c5def81a6c277b8ad79842fe6b7993b00"},"cell_type":"code","source":"# {\"Domestic Long Hair\": 264, \"Domestic Medium Hair\": 265, \"Domestic Short Hair\": 266, \"Mixed Breed\": 307}\n\ntrain['Pure_breed'] = 1\ntrain.loc[train['Breed2'] != 0, 'Pure_breed'] = 0\ntrain.loc[train['Breed1'] == 264, 'Pure_breed'] = 0\ntrain.loc[train['Breed1'] == 265, 'Pure_breed'] = 0\ntrain.loc[train['Breed1'] == 266, 'Pure_breed'] = 0\ntrain.loc[train['Breed1'] == 307, 'Pure_breed'] = 0\n\ntest['Pure_breed'] = 1\ntest.loc[test['Breed2'] != 0, 'Pure_breed'] = 0\ntest.loc[test['Breed1'] == 264, 'Pure_breed'] = 0\ntest.loc[test['Breed1'] == 265, 'Pure_breed'] = 0\ntest.loc[test['Breed1'] == 266, 'Pure_breed'] = 0\ntest.loc[test['Breed1'] == 307, 'Pure_breed'] = 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b3f0e071d133083e45461232a01899f4de86a059"},"cell_type":"code","source":"# drop some not so impantance features\n\ntrain.drop(['vertex_x', 'vertex_y', 'bounding_confidence'], axis=1, inplace=True)\ntest.drop(['vertex_x', 'vertex_y', 'bounding_confidence'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## DenseNet121 extracted 128 dim image features"},{"metadata":{"trusted":true,"_uuid":"eae52236ea117bcd1bbd10e5581abd310c36123b"},"cell_type":"code","source":"n_img_features = 128\n\nimg_size = 256\nbatch_size = 16\n\ninp = Input((img_size, img_size, 3))\nbackbone = DenseNet121(input_tensor=inp, \n                       weights=\"../input/densenet-keras/DenseNet-BC-121-32-no-top.h5\",\n                       include_top = False)\nx = backbone.output\nx = GlobalAveragePooling2D()(x)\nx = Lambda(lambda x: K.expand_dims(x,axis = -1))(x)\nx = AveragePooling1D(1024//n_img_features)(x)\nout = Lambda(lambda x: x[:,:,0])(x)\n\nm = Model(inp,out)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"17e963fa7c917aab382242276e5e6b37f0dcfbd0"},"cell_type":"code","source":"pet_ids = train_id.values\nn_batches = len(pet_ids) // batch_size + 1\n\nfeatures = {}\nfor b in tqdm_notebook(range(n_batches)):\n    start = b*batch_size\n    end = (b+1)*batch_size\n    batch_pets = pet_ids[start:end]\n    batch_images = np.zeros((len(batch_pets),img_size,img_size,3))\n    for i,pet_id in enumerate(batch_pets):\n        try:\n            batch_images[i] = load_image(\"../input/petfinder-adoption-prediction/train_images/\", pet_id)\n        except:\n            pass\n    batch_preds = m.predict(batch_images)\n    for i,pet_id in enumerate(batch_pets):\n        features[pet_id] = batch_preds[i]\n        \ntrain_feats = pd.DataFrame.from_dict(features, orient='index')\ntrain_feats.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"61c093a449cd1d14486bac8bd791f09e37421d38"},"cell_type":"code","source":"pet_ids = test_id.values\nn_batches = len(pet_ids) // batch_size + 1\n\nfeatures = {}\nfor b in tqdm_notebook(range(n_batches)):\n    start = b*batch_size\n    end = (b+1)*batch_size\n    batch_pets = pet_ids[start:end]\n    batch_images = np.zeros((len(batch_pets),img_size,img_size,3))\n    for i,pet_id in enumerate(batch_pets):\n        try:\n            batch_images[i] = load_image(\"../input/petfinder-adoption-prediction/test_images/\", pet_id)\n        except:\n            pass\n    batch_preds = m.predict(batch_images)\n    for i,pet_id in enumerate(batch_pets):\n        features[pet_id] = batch_preds[i]\n        \ntest_feats = pd.DataFrame.from_dict(features, orient='index')\ntest_feats.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"49c1c8cc0730b7f93b38d1798f66dae6940aec7a"},"cell_type":"code","source":"train_feats.columns = [\"img_feat{}\".format(i) for i in range(n_img_features)]\ntest_feats.columns = [\"img_feat{}\".format(i) for i in range(n_img_features)]\n\ntrain_feats[\"PetID\"] = train_feats.index\ntest_feats[\"PetID\"] = test_feats.index\n\ntrain = pd.merge(train, train_feats, on=\"PetID\")\ntest = pd.merge(test, test_feats, on=\"PetID\")\n\nprint(train.shape, test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"36c070d5e964ee142985e4cbfc8c551dea68e81e"},"cell_type":"code","source":"train.drop(['AdoptionSpeed', 'PetID'], axis=1, inplace=True)\ntest.drop(['PetID'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6b2ea3e1cbb6fdac6069f3a0b8d73920911081ed"},"cell_type":"markdown","source":"## LGB"},{"metadata":{"trusted":true,"_uuid":"64584c98c2dbb6392adc3cd8d9fe58f8bddccfd1"},"cell_type":"code","source":"train.drop(['Name', 'RescuerID', 'Description'], axis=1, inplace=True)\ntest.drop(['Name', 'RescuerID', 'Description'], axis=1, inplace=True)\n\n# rearrange columns again\nc = ['Type', 'Age', 'Breed1', 'Breed2', 'Gender', 'MaturitySize', 'FurLength', 'Vaccinated', 'Dewormed', 'Sterilized', 'Health', 'Quantity', 'Fee', 'State', 'VideoAmt', 'PhotoAmt', 'Color'] +  [\"img_feat{}\".format(i) for i in range(n_img_features)] + ['doc_sent_mag', 'doc_sent_score'] + ['svd_{}'.format(i) for i in range(n_components)] + ['img_x', 'img_y', 'bounding_importance', 'dominant_blue', 'dominant_green', 'dominant_red', 'dominant_pixel_frac', 'dominant_score','label_description', 'label_score', 'vertex_x_ratio', 'vertex_y_ratio', 'name_length', 'name_cnt', 'desc_length', 'desc_words', 'desc_lexical_density', 'sentences_count', 'desc_capitals', 'rescuer_cnt', 'state_gdp', 'state_population', 'Pure_breed']\ntrain = train[c]\ntest = test[c]\n\nnumeric_cols = ['Age', 'Quantity', 'Fee', 'VideoAmt', 'PhotoAmt', 'doc_sent_mag', 'doc_sent_score', 'dominant_score', 'dominant_pixel_frac', 'dominant_red', 'dominant_green', 'dominant_blue', 'bounding_importance', 'img_x', 'img_y', 'vertex_x_ratio', 'vertex_y_ratio', 'label_score', 'desc_length', 'desc_words', 'desc_lexical_density', 'sentences_count', 'desc_capitals', 'rescuer_cnt', 'state_gdp', 'state_population', 'Pure_breed', 'name_length', 'name_cnt'] + ['svd_{}'.format(i) for i in range(n_components)] + [\"img_feat{}\".format(i) for i in range(n_img_features)]\ncat_cols = list(set(train.columns) - set(numeric_cols))\n\ntrain.loc[:, cat_cols] = train[cat_cols].astype('category')\ntest.loc[:, cat_cols] = test[cat_cols].astype('category')\n\nfoo = train.dtypes\ncat_feature_names = foo[foo == \"category\"]\ncat_features = [train.columns.get_loc(c) for c in train.columns if c in cat_feature_names]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d15e605c71a60c8f81dd77430e504be889bd1bc1"},"cell_type":"code","source":"del run_cv_model\ngc.collect()\n\ndef run_cv_model(train, test, target, model_fn, params={}, eval_fn=None, label='model'):\n    kf = FOLDS\n    n_splits = N_FOLDS\n    \n    fold_splits = kf.split(train, target)\n    cv_scores = []\n    qwk_scores = []\n    pred_full_test = 0\n    pred_train = np.zeros((train.shape[0], n_splits))\n    all_coefficients = np.zeros((n_splits, 4))\n    feature_importance_df = pd.DataFrame()\n    i = 1\n    for dev_index, val_index in fold_splits:\n        print('Started ' + label + ' fold ' + str(i) + '/' + str(n_splits))\n        if isinstance(train, pd.DataFrame):\n            dev_X, val_X = train.iloc[dev_index], train.iloc[val_index]\n            dev_y, val_y = target[dev_index], target[val_index]\n        else:\n            dev_X, val_X = train[dev_index], train[val_index]\n            dev_y, val_y = target[dev_index], target[val_index]\n        params2 = params.copy()\n        pred_val_y, pred_test_y, importances, coefficients, qwk = model_fn(dev_X, dev_y, val_X, val_y, test, params2)\n        pred_full_test = pred_full_test + pred_test_y\n        pred_train[val_index] = pred_val_y\n        all_coefficients[i-1, :] = coefficients\n        if eval_fn is not None:\n            cv_score = eval_fn(val_y, pred_val_y)\n            cv_scores.append(cv_score)\n            qwk_scores.append(qwk)\n            print(label + ' cv score {}: RMSE {} QWK {}'.format(i, cv_score, qwk))\n        fold_importance_df = pd.DataFrame()\n        fold_importance_df['feature'] = train.columns.values\n        fold_importance_df['importance'] = importances\n        fold_importance_df['fold'] = i\n        feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n        i += 1\n    print('{} cv RMSE scores : {}'.format(label, cv_scores))\n    print('{} cv mean RMSE score : {}'.format(label, np.mean(cv_scores)))\n    print('{} cv std RMSE score : {}'.format(label, np.mean(cv_scores)))\n    print('{} cv QWK scores : {}'.format(label, qwk_scores))\n    print('{} cv mean QWK score : {}'.format(label, np.mean(qwk_scores)))\n    print('{} cv std QWK score : {}'.format(label, np.std(qwk_scores)))\n    pred_full_test = pred_full_test / float(n_splits)\n    results = {'label': label,\n               'train': pred_train, 'test': pred_full_test,\n                'cv': cv_scores, 'qwk': qwk_scores,\n               'importance': feature_importance_df,\n               'coefficients': all_coefficients}\n    return results","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c855930ec4501953a8b18a8fc10e602e89e229b9"},"cell_type":"code","source":"del runLGB\ngc.collect()\n\ndef runLGB(train_X, train_y, test_X, test_y, test_X2, params):\n    d_train = lgb.Dataset(train_X, label=train_y)\n    d_valid = lgb.Dataset(test_X, label=test_y)\n    watchlist = [d_train, d_valid]\n    print('Train LGB')\n    try:\n        num_rounds = params.pop('num_rounds')\n    except:\n        pass\n    verbose_eval = params.pop('verbose_eval')\n    early_stop = None\n    if params.get('early_stop'):\n        early_stop = params.pop('early_stop')\n    model = lgb.train(params,\n                      train_set=d_train,\n                      num_boost_round=10000,\n                      valid_sets=watchlist,\n                      verbose_eval=verbose_eval,\n                      callbacks=[lgb.reset_parameter(learning_rate=[0.005]*1000+[0.003]*1000+[0.001]*8000)],\n                      early_stopping_rounds=early_stop)\n\n    print('Predict 1/2')\n    pred_test_y = model.predict(test_X, num_iteration=model.best_iteration)\n    init_coef = get_init_coefs(pred_test_y, test_y)\n    optR = OptimizedRounder_v2(initial_coefs=init_coef)\n    optR.fit(pred_test_y, test_y)\n    coefficients = optR.coefficients()\n    pred_test_y_k = optR.predict(pred_test_y, coefficients)\n    chi2 = get_chi2(pred_test_y_k, test_y)\n    print(\"Valid Counts = {}\".format(Counter(test_y)))\n    print(\"Predicted Counts = {}\".format(Counter(pred_test_y_k)))\n    print(\"Coefficients = {}\".format(coefficients))\n    print(\"Chi2 = {}\".format(chi2))\n    qwk = quadratic_weighted_kappa(test_y, pred_test_y_k)\n    print(\"QWK = {}\".format(qwk))\n    print('Predict 2/2')\n    pred_test_y2 = model.predict(test_X2, num_iteration=model.best_iteration)\n    return pred_test_y.reshape(-1, 1), pred_test_y2.reshape(-1, 1), model.feature_importance(), coefficients, qwk","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f93e40673af588c956a81a785791b96725e6dda2"},"cell_type":"code","source":"param = {'application': 'regression',\n         'boosting': 'gbdt', \n         'metric': 'rmse', \n         'num_leaves': 149, \n         'max_depth': 11, \n         'max_bin': 37, \n         'bagging_fraction': 0.975419815153193, \n         'bagging_freq': 1, \n         'feature_fraction': 0.2705570927694394, \n         'min_split_gain': 0.7636472013417633, \n         'min_child_samples': 29, \n         'min_child_weight': 0.13126728393897313, \n         'lambda_l2': 0.841358003322472, \n         'verbosity': -1, \n         'data_random_seed': 1029, \n         'early_stop': 100, \n         'verbose_eval': 2000, \n         'num_rounds': 10000}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false,"_uuid":"7afbe7c002b1e2999348d1c7ed4872313e5af739"},"cell_type":"code","source":"lgb_zyl = run_cv_model(train, test, target, runLGB, param, rmse, 'lgb')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c4423eb273fd68e78b62bb98f2cf78afeccda55d"},"cell_type":"markdown","source":"# Feature table 3\n\nhttps://www.kaggle.com/ranjoranjan/single-xgboost-model"},{"metadata":{},"cell_type":"markdown","source":"## reload"},{"metadata":{"trusted":true,"_uuid":"232ecf92e020ca40bf18de33baeeccf1c9217ff8"},"cell_type":"code","source":"del train, test\ngc.collect()\n\ntrain = pd.read_csv('../input/petfinder-adoption-prediction/train/train.csv')\ntest = pd.read_csv('../input/petfinder-adoption-prediction/test/test.csv')\n\nlabels_breed = pd.read_csv('../input/petfinder-adoption-prediction/breed_labels.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## features"},{"metadata":{"trusted":true,"_uuid":"368130e50ffdbe332f52a8dd576348eb1d1e180b"},"cell_type":"code","source":"all_ids = pd.concat([train, test], axis=0, ignore_index=True, sort=False)[['PetID']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"edc65a21c0797ac84624706173fd9fd59bec9c9f"},"cell_type":"code","source":"n_components = 32\nsvd_ = TruncatedSVD(n_components=n_components, random_state=1337)\n\nfeatures_df = pd.concat([train_feats_256, test_feats_256], axis=0)\nfeatures = features_df[[f'pic_{i}' for i in range(256)]].values\n\nsvd_col = svd_.fit_transform(features)\nsvd_col = pd.DataFrame(svd_col)\nsvd_col = svd_col.add_prefix('IMG_SVD_')\n\nimg_features = pd.concat([all_ids, svd_col], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6431035e3c414c5f40913c1fe5ece1b47a7b0041"},"cell_type":"code","source":"train_image_files = sorted(glob.glob('../input/petfinder-adoption-prediction/train_images/*.jpg'))\ntrain_metadata_files = sorted(glob.glob('../input/petfinder-adoption-prediction/train_metadata/*.json'))\ntrain_sentiment_files = sorted(glob.glob('../input/petfinder-adoption-prediction/train_sentiment/*.json'))\n\ntest_image_files = sorted(glob.glob('../input/petfinder-adoption-prediction/test_images/*.jpg'))\ntest_metadata_files = sorted(glob.glob('../input/petfinder-adoption-prediction/test_metadata/*.json'))\ntest_sentiment_files = sorted(glob.glob('../input/petfinder-adoption-prediction/test_sentiment/*.json'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"05acad8dfc7ec249c0eb5a1291d02742b9367073"},"cell_type":"code","source":"split_char = '/'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2275c76dcc0085761e61a42694f54254004ea162"},"cell_type":"code","source":"train_df_ids = train[['PetID']]\n\ntrain_df_ids = train[['PetID']]\ntrain_df_metadata = pd.DataFrame(train_metadata_files)\ntrain_df_metadata.columns = ['metadata_filename']\ntrain_metadata_pets = train_df_metadata['metadata_filename'].apply(lambda x: x.split(split_char)[-1].split('-')[0])\ntrain_df_metadata = train_df_metadata.assign(PetID=train_metadata_pets)\n\ntrain_df_ids = train[['PetID']]\ntrain_df_sentiment = pd.DataFrame(train_sentiment_files)\ntrain_df_sentiment.columns = ['sentiment_filename']\ntrain_sentiment_pets = train_df_sentiment['sentiment_filename'].apply(lambda x: x.split(split_char)[-1].split('.')[0])\ntrain_df_sentiment = train_df_sentiment.assign(PetID=train_sentiment_pets)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"34a100d54acaa3577da2d1aad6775a7f4dc16ba8"},"cell_type":"code","source":"test_df_ids = test[['PetID']]\n\ntest_df_metadata = pd.DataFrame(test_metadata_files)\ntest_df_metadata.columns = ['metadata_filename']\ntest_metadata_pets = test_df_metadata['metadata_filename'].apply(lambda x: x.split(split_char)[-1].split('-')[0])\ntest_df_metadata = test_df_metadata.assign(PetID=test_metadata_pets)\n\ntest_df_sentiment = pd.DataFrame(test_sentiment_files)\ntest_df_sentiment.columns = ['sentiment_filename']\ntest_sentiment_pets = test_df_sentiment['sentiment_filename'].apply(lambda x: x.split(split_char)[-1].split('.')[0])\ntest_df_sentiment = test_df_sentiment.assign(PetID=test_sentiment_pets)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1ac4c4ec5ccb59846ca1eb22f849713cd87668b9"},"cell_type":"code","source":"class PetFinderParser(object):\n    \n    def __init__(self, debug=False):        \n        self.debug = debug\n        self.sentence_sep = ' '        \n        self.extract_sentiment_text = False\n    \n    def open_json_file(self, filename):\n        with open(filename, 'r', encoding='utf-8') as f:\n            json_file = json.load(f)\n        return json_file\n        \n    def parse_sentiment_file(self, file):\n        file_sentiment = file['documentSentiment']\n        file_entities = [x['name'] for x in file['entities']]\n        file_entities = self.sentence_sep.join(file_entities)       \n        file_sentences_sentiment = [x['sentiment'] for x in file['sentences']]        \n        file_sentences_sentiment = pd.DataFrame.from_dict(\n            file_sentences_sentiment, orient='columns')\n        file_sentences_sentiment_df = pd.DataFrame(\n            {\n                'magnitude_sum': file_sentences_sentiment['magnitude'].sum(axis=0),\n                'score_sum': file_sentences_sentiment['score'].sum(axis=0),\n                'magnitude_mean': file_sentences_sentiment['magnitude'].mean(axis=0),\n                'score_mean': file_sentences_sentiment['score'].mean(axis=0),\n                'magnitude_var': file_sentences_sentiment['magnitude'].var(axis=0),\n                'score_var': file_sentences_sentiment['score'].var(axis=0),\n            }, index=[0]\n        )        \n        df_sentiment = pd.DataFrame.from_dict(file_sentiment, orient='index').T\n        df_sentiment = pd.concat([df_sentiment, file_sentences_sentiment_df], axis=1)            \n        df_sentiment['entities'] = file_entities\n        df_sentiment = df_sentiment.add_prefix('sentiment_')        \n        return df_sentiment\n    \n    def parse_metadata_file(self, file):\n        file_keys = list(file.keys())        \n        if 'labelAnnotations' in file_keys:\n            file_annots = file['labelAnnotations']\n            file_top_score = np.asarray([x['score'] for x in file_annots]).mean()\n            file_top_desc = [x['description'] for x in file_annots]\n        else:\n            file_top_score = np.nan\n            file_top_desc = ['']        \n        file_colors = file['imagePropertiesAnnotation']['dominantColors']['colors']\n        file_crops = file['cropHintsAnnotation']['cropHints']\n        file_color_score = np.asarray([x['score'] for x in file_colors]).mean()\n        file_color_pixelfrac = np.asarray([x['pixelFraction'] for x in file_colors]).mean()\n        file_crop_conf = np.asarray([x['confidence'] for x in file_crops]).mean()        \n        if 'importanceFraction' in file_crops[0].keys():\n            file_crop_importance = np.asarray([x['importanceFraction'] for x in file_crops]).mean()\n        else:\n            file_crop_importance = np.nan\n        df_metadata = {\n            'annots_score': file_top_score,\n            'color_score': file_color_score,\n            'color_pixelfrac': file_color_pixelfrac,\n            'crop_conf': file_crop_conf,\n            'crop_importance': file_crop_importance,\n            'annots_top_desc': self.sentence_sep.join(file_top_desc)\n        }        \n        df_metadata = pd.DataFrame.from_dict(df_metadata, orient='index').T\n        df_metadata = df_metadata.add_prefix('metadata_')        \n        return df_metadata\n    \ndef extract_additional_features(pet_id, mode='train'):\n    sentiment_filename = f'../input/petfinder-adoption-prediction/{mode}_sentiment/{pet_id}.json'\n    try:\n        sentiment_file = pet_parser.open_json_file(sentiment_filename)\n        df_sentiment = pet_parser.parse_sentiment_file(sentiment_file)\n        df_sentiment['PetID'] = pet_id\n    except FileNotFoundError:\n        df_sentiment = []\n    dfs_metadata = []\n    metadata_filenames = sorted(glob.glob(f'../input/petfinder-adoption-prediction/{mode}_metadata/{pet_id}*.json'))\n    if len(metadata_filenames) > 0:\n        for f in metadata_filenames:\n            metadata_file = pet_parser.open_json_file(f)\n            df_metadata = pet_parser.parse_metadata_file(metadata_file)\n            df_metadata['PetID'] = pet_id\n            dfs_metadata.append(df_metadata)\n        dfs_metadata = pd.concat(dfs_metadata, ignore_index=True, sort=False)\n    dfs = [df_sentiment, dfs_metadata]    \n    return dfs\n\npet_parser = PetFinderParser()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5c97527c16c4d32b83e831813f3c41dce34e4445"},"cell_type":"code","source":"train_pet_ids = train.PetID.unique()\ntest_pet_ids = test.PetID.unique()\n\ndfs_train = Parallel(n_jobs=-1, verbose=1)(\n    delayed(extract_additional_features)(i, mode='train') for i in train_pet_ids)\ntrain_dfs_sentiment = [x[0] for x in dfs_train if isinstance(x[0], pd.DataFrame)]\ntrain_dfs_metadata = [x[1] for x in dfs_train if isinstance(x[1], pd.DataFrame)]\ntrain_dfs_sentiment = pd.concat(train_dfs_sentiment, ignore_index=True, sort=False)\ntrain_dfs_metadata = pd.concat(train_dfs_metadata, ignore_index=True, sort=False)\nprint(train_dfs_sentiment.shape, train_dfs_metadata.shape)\n\ndfs_test = Parallel(n_jobs=-1, verbose=1)(\n    delayed(extract_additional_features)(i, mode='test') for i in test_pet_ids)\ntest_dfs_sentiment = [x[0] for x in dfs_test if isinstance(x[0], pd.DataFrame)]\ntest_dfs_metadata = [x[1] for x in dfs_test if isinstance(x[1], pd.DataFrame)]\ntest_dfs_sentiment = pd.concat(test_dfs_sentiment, ignore_index=True, sort=False)\ntest_dfs_metadata = pd.concat(test_dfs_metadata, ignore_index=True, sort=False)\nprint(test_dfs_sentiment.shape, test_dfs_metadata.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"875ea01cd56d9f4a8113ab27291309ba6ef6c876"},"cell_type":"code","source":"aggregates = ['sum', 'mean', 'var']\nsent_agg = ['sum']\n\ntrain_metadata_desc = train_dfs_metadata.groupby(['PetID'])['metadata_annots_top_desc'].unique()\ntrain_metadata_desc = train_metadata_desc.reset_index()\ntrain_metadata_desc[\n    'metadata_annots_top_desc'] = train_metadata_desc[\n    'metadata_annots_top_desc'].apply(lambda x: ' '.join(x))\n\nprefix = 'metadata'\ntrain_metadata_gr = train_dfs_metadata.drop(['metadata_annots_top_desc'], axis=1)\nfor i in train_metadata_gr.columns:\n    if 'PetID' not in i:\n        train_metadata_gr[i] = train_metadata_gr[i].astype(float)\ntrain_metadata_gr = train_metadata_gr.groupby(['PetID']).agg(aggregates)\ntrain_metadata_gr.columns = pd.Index([f'{c[0]}_{c[1].upper()}' for c in train_metadata_gr.columns.tolist()])\ntrain_metadata_gr = train_metadata_gr.reset_index()\n\ntrain_sentiment_desc = train_dfs_sentiment.groupby(['PetID'])['sentiment_entities'].unique()\ntrain_sentiment_desc = train_sentiment_desc.reset_index()\ntrain_sentiment_desc[\n    'sentiment_entities'] = train_sentiment_desc[\n    'sentiment_entities'].apply(lambda x: ' '.join(x))\n\nprefix = 'sentiment'\ntrain_sentiment_gr = train_dfs_sentiment.drop(['sentiment_entities'], axis=1)\nfor i in train_sentiment_gr.columns:\n    if 'PetID' not in i:\n        train_sentiment_gr[i] = train_sentiment_gr[i].astype(float)\ntrain_sentiment_gr = train_sentiment_gr.groupby(['PetID']).agg(sent_agg)\ntrain_sentiment_gr.columns = pd.Index([f'{c[0]}' for c in train_sentiment_gr.columns.tolist()])\ntrain_sentiment_gr = train_sentiment_gr.reset_index()\n\n\ntest_metadata_desc = test_dfs_metadata.groupby(['PetID'])['metadata_annots_top_desc'].unique()\ntest_metadata_desc = test_metadata_desc.reset_index()\ntest_metadata_desc[\n    'metadata_annots_top_desc'] = test_metadata_desc[\n    'metadata_annots_top_desc'].apply(lambda x: ' '.join(x))\n\nprefix = 'metadata'\ntest_metadata_gr = test_dfs_metadata.drop(['metadata_annots_top_desc'], axis=1)\nfor i in test_metadata_gr.columns:\n    if 'PetID' not in i:\n        test_metadata_gr[i] = test_metadata_gr[i].astype(float)\ntest_metadata_gr = test_metadata_gr.groupby(['PetID']).agg(aggregates)\ntest_metadata_gr.columns = pd.Index([f'{c[0]}_{c[1].upper()}' for c in test_metadata_gr.columns.tolist()])\ntest_metadata_gr = test_metadata_gr.reset_index()\n\ntest_sentiment_desc = test_dfs_sentiment.groupby(['PetID'])['sentiment_entities'].unique()\ntest_sentiment_desc = test_sentiment_desc.reset_index()\ntest_sentiment_desc[\n    'sentiment_entities'] = test_sentiment_desc[\n    'sentiment_entities'].apply(lambda x: ' '.join(x))\n\nprefix = 'sentiment'\ntest_sentiment_gr = test_dfs_sentiment.drop(['sentiment_entities'], axis=1)\nfor i in test_sentiment_gr.columns:\n    if 'PetID' not in i:\n        test_sentiment_gr[i] = test_sentiment_gr[i].astype(float)\ntest_sentiment_gr = test_sentiment_gr.groupby(['PetID']).agg(sent_agg)\ntest_sentiment_gr.columns = pd.Index([f'{c[0]}' for c in test_sentiment_gr.columns.tolist()])\ntest_sentiment_gr = test_sentiment_gr.reset_index()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e14b17f128df3a506b88138466c584fdfb77d316"},"cell_type":"code","source":"train_proc = train.copy()\ntrain_proc = train_proc.merge(\n    train_sentiment_gr, how='left', on='PetID')\ntrain_proc = train_proc.merge(\n    train_metadata_gr, how='left', on='PetID')\ntrain_proc = train_proc.merge(\n    train_metadata_desc, how='left', on='PetID')\ntrain_proc = train_proc.merge(\n    train_sentiment_desc, how='left', on='PetID')\n\ntest_proc = test.copy()\ntest_proc = test_proc.merge(\n    test_sentiment_gr, how='left', on='PetID')\ntest_proc = test_proc.merge(\n    test_metadata_gr, how='left', on='PetID')\ntest_proc = test_proc.merge(\n    test_metadata_desc, how='left', on='PetID')\ntest_proc = test_proc.merge(\n    test_sentiment_desc, how='left', on='PetID')\n\nassert train_proc.shape[0] == train.shape[0]\nassert test_proc.shape[0] == test.shape[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c6a46f86abd30df4fc6c613bc5cce4b518b82a7f"},"cell_type":"code","source":"train_breed_main = train_proc[['Breed1']].merge(\n    labels_breed, how='left',\n    left_on='Breed1', right_on='BreedID',\n    suffixes=('', '_main_breed'))\ntrain_breed_main = train_breed_main.iloc[:, 2:]\ntrain_breed_main = train_breed_main.add_prefix('main_breed_')\ntrain_breed_second = train_proc[['Breed2']].merge(\n    labels_breed, how='left',\n    left_on='Breed2', right_on='BreedID',\n    suffixes=('', '_second_breed'))\ntrain_breed_second = train_breed_second.iloc[:, 2:]\ntrain_breed_second = train_breed_second.add_prefix('second_breed_')\ntrain_proc = pd.concat(\n    [train_proc, train_breed_main, train_breed_second], axis=1)\n\ntest_breed_main = test_proc[['Breed1']].merge(\n    labels_breed, how='left',\n    left_on='Breed1', right_on='BreedID',\n    suffixes=('', '_main_breed'))\ntest_breed_main = test_breed_main.iloc[:, 2:]\ntest_breed_main = test_breed_main.add_prefix('main_breed_')\ntest_breed_second = test_proc[['Breed2']].merge(\n    labels_breed, how='left',\n    left_on='Breed2', right_on='BreedID',\n    suffixes=('', '_second_breed'))\ntest_breed_second = test_breed_second.iloc[:, 2:]\ntest_breed_second = test_breed_second.add_prefix('second_breed_')\ntest_proc = pd.concat(\n    [test_proc, test_breed_main, test_breed_second], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"936333e2ddcf97e697f2653581eb101f1175f249"},"cell_type":"code","source":"X = pd.concat([train_proc, test_proc], ignore_index=True, sort=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"921f2615dd33c8ca4c1e4b269097676560de620c"},"cell_type":"code","source":"X_temp = X.copy()\n\ntext_columns = ['Description', 'metadata_annots_top_desc', 'sentiment_entities']\ncategorical_columns = ['main_breed_BreedName', 'second_breed_BreedName']\n\nto_drop_columns = ['PetID', 'Name', 'RescuerID']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9faf88981566aad9050270839e26551dc4702127"},"cell_type":"code","source":"rescuer_count = X.groupby(['RescuerID'])['PetID'].count().reset_index()\nrescuer_count.columns = ['RescuerID', 'RescuerID_COUNT']\n\nX_temp = X_temp.merge(rescuer_count, how='left', on='RescuerID')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cc54e76b0e08bbfa56620048394ac3f80828d044"},"cell_type":"code","source":"for i in categorical_columns:\n    X_temp.loc[:, i] = pd.factorize(X_temp.loc[:, i])[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cad5744f4acab6aa9e2ff0116d978006e3d03745"},"cell_type":"code","source":"X_text = X_temp[text_columns]\n\nfor i in X_text.columns:\n    X_text.loc[:, i] = X_text.loc[:, i].fillna('none')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b3a5e69067caf2f88a4fb17e51a9347774004a53"},"cell_type":"code","source":"X_temp['Length_Description'] = X_text['Description'].map(len)\nX_temp['Length_metadata_annots_top_desc'] = X_text['metadata_annots_top_desc'].map(len)\nX_temp['Lengths_sentiment_entities'] = X_text['sentiment_entities'].map(len)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d064795c722b641d260925af1e6bfba99ef34a00"},"cell_type":"code","source":"n_components = 16\ntext_features = []\n\nfor i in X_text.columns:\n    print(f'generating features from: {i}')\n    tfv = TfidfVectorizer(min_df=2,  max_features=None,\n                          strip_accents='unicode', analyzer='word', token_pattern=r'(?u)\\b\\w+\\b',\n                          ngram_range=(1, 3), use_idf=1, smooth_idf=1, sublinear_tf=1)\n    svd_ = TruncatedSVD(\n        n_components=n_components, random_state=1337)\n    tfidf_col = tfv.fit_transform(X_text.loc[:, i].values)    \n    svd_col = svd_.fit_transform(tfidf_col)\n    svd_col = pd.DataFrame(svd_col)\n    svd_col = svd_col.add_prefix('TFIDF_{}_'.format(i))    \n    text_features.append(svd_col)\n    \ntext_features = pd.concat(text_features, axis=1)\n\nX_temp = pd.concat([X_temp, text_features], axis=1)\n\nfor i in X_text.columns:\n    X_temp = X_temp.drop(i, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"77fb8cdbfb4e5d33aa0cf13de6b052f879157039"},"cell_type":"code","source":"X_temp = X_temp.merge(img_features, how='left', on='PetID')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8d1b7d4e203ae17789a3ccef5fcaf447417e8087"},"cell_type":"code","source":"train_df_ids = train[['PetID']]\ntest_df_ids = test[['PetID']]\n\ntrain_df_imgs = pd.DataFrame(train_image_files)\ntrain_df_imgs.columns = ['image_filename']\ntrain_imgs_pets = train_df_imgs['image_filename'].apply(lambda x: x.split(split_char)[-1].split('-')[0])\n\ntest_df_imgs = pd.DataFrame(test_image_files)\ntest_df_imgs.columns = ['image_filename']\ntest_imgs_pets = test_df_imgs['image_filename'].apply(lambda x: x.split(split_char)[-1].split('-')[0])\n\ntrain_df_imgs = train_df_imgs.assign(PetID=train_imgs_pets)\ntest_df_imgs = test_df_imgs.assign(PetID=test_imgs_pets)\n\ndef getSize(filename):\n    st = os.stat(filename)\n    return st.st_size\n\ndef getDimensions(filename):\n    img_size = Image.open(filename).size\n    return img_size \n\ntrain_df_imgs['image_size'] = train_df_imgs['image_filename'].apply(getSize)\ntrain_df_imgs['temp_size'] = train_df_imgs['image_filename'].apply(getDimensions)\ntrain_df_imgs['width'] = train_df_imgs['temp_size'].apply(lambda x : x[0])\ntrain_df_imgs['height'] = train_df_imgs['temp_size'].apply(lambda x : x[1])\ntrain_df_imgs = train_df_imgs.drop(['temp_size'], axis=1)\n\ntest_df_imgs['image_size'] = test_df_imgs['image_filename'].apply(getSize)\ntest_df_imgs['temp_size'] = test_df_imgs['image_filename'].apply(getDimensions)\ntest_df_imgs['width'] = test_df_imgs['temp_size'].apply(lambda x : x[0])\ntest_df_imgs['height'] = test_df_imgs['temp_size'].apply(lambda x : x[1])\ntest_df_imgs = test_df_imgs.drop(['temp_size'], axis=1)\n\naggs = {\n    'image_size': ['sum', 'mean', 'var'],\n    'width': ['sum', 'mean', 'var'],\n    'height': ['sum', 'mean', 'var'],\n}\nagg_train_imgs = train_df_imgs.groupby('PetID').agg(aggs)\nnew_columns = [\n    k + '_' + agg for k in aggs.keys() for agg in aggs[k]\n]\nagg_train_imgs.columns = new_columns\nagg_train_imgs = agg_train_imgs.reset_index()\n\nagg_test_imgs = test_df_imgs.groupby('PetID').agg(aggs)\nnew_columns = [\n    k + '_' + agg for k in aggs.keys() for agg in aggs[k]\n]\nagg_test_imgs.columns = new_columns\nagg_test_imgs = agg_test_imgs.reset_index()\n\nagg_imgs = pd.concat([agg_train_imgs, agg_test_imgs], axis=0).reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fac322e21458cbbd236e0316b00b74fc6a9d178c"},"cell_type":"code","source":"X_temp = X_temp.merge(agg_imgs, how='left', on='PetID')\n\nX_temp = X_temp.drop(to_drop_columns, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8a24d680d4f50c5a70bf422a990b70bc1ce2bafb"},"cell_type":"code","source":"X_train = X_temp.loc[np.isfinite(X_temp.AdoptionSpeed), :]\nX_test = X_temp.loc[~np.isfinite(X_temp.AdoptionSpeed), :]\n\nX_test = X_test.drop(['AdoptionSpeed'], axis=1)\n\nassert X_train.shape[0] == train.shape[0]\nassert X_test.shape[0] == test.shape[0]\n\ntrain_cols = X_train.columns.tolist()\ntrain_cols.remove('AdoptionSpeed')\n\ntest_cols = X_test.columns.tolist()\n\nassert np.all(train_cols == test_cols)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9ea0415a381b321e8bb341a605552bf56e6a1979"},"cell_type":"code","source":"X_train_non_null = X_train.fillna(-1)\nX_test_non_null = X_test.fillna(-1)\nX_train_non_null['ResNet_meta'] = train_img_prob.flatten()         # ADD IMG ResNet50 metafeature\nX_test_non_null['ResNet_meta'] = test_img_prob.flatten()           # ADD IMG ResNet50 metafeature","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"65f111d010d8ea785b769f97552590ad82e6247f"},"cell_type":"code","source":"X_train_non_null.isnull().any().any(), X_test_non_null.isnull().any().any()\nX_train_non_null.shape, X_test_non_null.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c83b64002c92e506b81ea74066677c944c68cad7"},"cell_type":"markdown","source":"## XGB"},{"metadata":{"trusted":true,"_uuid":"58b5ba6bd0198b61f5b5ffe812f65a6050a8319c"},"cell_type":"code","source":"xgb_params = {\n    'eval_metric': 'rmse',\n    'seed': 1337,\n    'eta': 0.0123,\n    'subsample': 0.8,\n    'colsample_bytree': 0.85,\n    'tree_method': 'gpu_hist',\n    'device': 'gpu',\n    'silent': 1,\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"514c20f24c31c5e5edd31c9ebda5a02bafd75482"},"cell_type":"code","source":"def run_xgb(params, X_train, X_test):\n    kf = FOLDS\n    n_splits = N_FOLDS\n    \n    verbose_eval = 1000\n    num_rounds = 60000\n    early_stop = 500\n\n    oof_train = np.zeros((X_train.shape[0]))\n    oof_test = np.zeros((X_test.shape[0], n_splits))\n\n    i = 0\n\n    for train_idx, valid_idx in kf.split(X_train, X_train['AdoptionSpeed'].values):\n\n        X_tr = X_train.iloc[train_idx, :]\n        X_val = X_train.iloc[valid_idx, :]\n\n        y_tr = X_tr['AdoptionSpeed'].values\n        X_tr = X_tr.drop(['AdoptionSpeed'], axis=1)\n\n        y_val = X_val['AdoptionSpeed'].values\n        X_val = X_val.drop(['AdoptionSpeed'], axis=1)\n\n        d_train = xgb.DMatrix(data=X_tr, label=y_tr, feature_names=X_tr.columns)\n        d_valid = xgb.DMatrix(data=X_val, label=y_val, feature_names=X_val.columns)\n\n        watchlist = [(d_train, 'train'), (d_valid, 'valid')]\n        model = xgb.train(dtrain=d_train, num_boost_round=num_rounds, evals=watchlist,\n                         early_stopping_rounds=early_stop, verbose_eval=verbose_eval, params=params)\n\n        valid_pred = model.predict(xgb.DMatrix(X_val, feature_names=X_val.columns), ntree_limit=model.best_ntree_limit)\n        test_pred = model.predict(xgb.DMatrix(X_test, feature_names=X_test.columns), ntree_limit=model.best_ntree_limit)\n\n        oof_train[valid_idx] = valid_pred\n        oof_test[:, i] = test_pred\n\n        i += 1\n    return model, oof_train, oof_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0a2d026c70a3ab522fd8129fcca6c4abdde6ce7d"},"cell_type":"code","source":"model, oof_train, oof_test = run_xgb(xgb_params, X_train_non_null, X_test_non_null)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9752f157d9fdcaa4155495c87b962592d5d650f3"},"cell_type":"code","source":"xgb_453_train_pred = oof_train\nxgb_453_test_pred = np.mean(oof_test, axis=1)\nxgb_453_train_pred.shape, xgb_453_test_pred.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"65d4b62b5717e5c31ef96643ee13b9538d601d48"},"cell_type":"markdown","source":"# Corr"},{"metadata":{"trusted":true,"_uuid":"86eb23134bee46334acbb26c57af726e92c8f664"},"cell_type":"code","source":"gzf_lgb = lgb_gzf[\"test\"].reshape(-1)\nzkr_lgb = lgb_zkr[\"test\"].reshape(-1)\nzyl_lgb = lgb_zyl[\"test\"].reshape(-1)\n\ndfa = pd.DataFrame({\"gzf_lgb\":gzf_lgb, \n                    \"zkr_lgb\":zkr_lgb, \n                    \"zyl_lgb\":zyl_lgb, \n                    \"453_xgb\":xgb_453_test_pred})\ndfa.corr()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"67ead707709e414b26d42d0889c8fa2fac513c9a"},"cell_type":"markdown","source":"# Stacking"},{"metadata":{"trusted":true,"_uuid":"12f9eee9bcaa0387620a3a7dd0f9825c21635087"},"cell_type":"code","source":"gzf_lgb_train_pred = np.mean(lgb_gzf['train'], axis=1)\ngzf_lgb_test_pred = np.mean(lgb_gzf['test'], axis=1)\n\nzkr_lgb_train_pred = np.mean(lgb_zkr['train'], axis=1)\nzkr_lgb_test_pred = np.mean(lgb_zkr['test'], axis=1)\n\nzyl_lgb_train_pred = np.mean(lgb_zyl['train'], axis=1)\nzyl_lgb_test_pred = np.mean(lgb_zyl['test'], axis=1)\n\n\ntrain_meta = np.concatenate([gzf_lgb_train_pred.reshape(-1,1),\n                             zkr_lgb_train_pred.reshape(-1,1),\n                             zyl_lgb_train_pred.reshape(-1,1),\n                             xgb_453_train_pred.reshape(-1,1)\n                            ], axis=1)\ntest_meta = np.concatenate([gzf_lgb_test_pred.reshape(-1,1),\n                            zkr_lgb_test_pred.reshape(-1,1),\n                            zyl_lgb_test_pred.reshape(-1,1),\n                            xgb_453_test_pred.reshape(-1,1)\n                           ], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"977e8c61c2149fb4afc9a4e7e1275a4797744664"},"cell_type":"code","source":"from sklearn.linear_model import Ridge","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"06c42389b7a4a1f4fb17de6330215387af251902"},"cell_type":"code","source":"clf = Ridge(alpha=0.1)\n\nclf.fit(train_meta, target)\ntrain_pred = clf.predict(train_meta)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4d40fcdf8b0bb43806b1f358302b0e9eb726b8b2"},"cell_type":"code","source":"print(clf.coef_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3f8984d7656a80e3d3932e24dca0963c421fd6ab"},"cell_type":"code","source":"init_coef = get_init_coefs(train_pred,  target)\noptR = OptimizedRounder_v2(initial_coefs=init_coef)\noptR.fit(train_pred, target)\ncoefficients = optR.coefficients()\nprint(\"coefficients: \", coefficients, \"\\n\")\n\nprint(\"True Counter: \", Counter(target))\n\noptR = OptimizedRounder_v2()\ntrain_predictions = optR.predict(train_pred, coefficients).astype(int)\nprint(\"Train Counter: \", Counter(train_predictions))\n\nprint(\"\\nTrain QWK: \", quadratic_weighted_kappa(target, train_predictions))\nprint(\"Train RMSE: \", rmse(target, train_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f98732821cd1ed5817ac133efd89b23866669e88"},"cell_type":"code","source":"predictions = clf.predict(test_meta)\n\noptR = OptimizedRounder_v3()\ntest_predictions = optR.predict(predictions, coefficients, 110).astype(int)\nprint(\"Test Counter: \", Counter(test_predictions), \"\\n\")\n\nprint(\"True Distribution:\")\nprint(pd.value_counts(target, normalize=True).sort_index())\nprint(\"Train Predicted Distribution:\")\nprint(pd.value_counts(train_predictions, normalize=True).sort_index())\nprint(\"Test Predicted Distribution:\")\nprint(pd.value_counts(test_predictions, normalize=True).sort_index())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"36ee65ded47bdbde1d8033c30224e65abdc17f43"},"cell_type":"code","source":"submission = pd.DataFrame({'PetID': test_id, 'AdoptionSpeed': test_predictions})\nsubmission.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4cd97ac32dfe34fb81e6e2e19af373cbd8509e29"},"cell_type":"code","source":"submission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4166d8db9a2dade4cd47febf966b80b56b3d65cf"},"cell_type":"code","source":"!head submission.csv","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b2becea193304df8748f921b43e4e286cc70be09"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}