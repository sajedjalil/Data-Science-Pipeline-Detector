{"cells":[{"metadata":{"_uuid":"efc8f31b8f57b14c535dabe6a9ddb1f7423e6065"},"cell_type":"markdown","source":"## Libraries"},{"metadata":{"trusted":true,"_uuid":"1223e84fee63f03f38138cc256ada0b6e76032ad"},"cell_type":"code","source":"import os\nimport cv2\nimport json\nimport pickle\nimport random\nimport scipy as sp\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\n\nimport xgboost as xgb\nimport lightgbm as lgb\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.utils.data as data\n\nimport torchvision.models as models\nimport torchvision.transforms as transforms\n\nfrom pathlib import Path\nfrom datetime import datetime as dt\nfrom functools import partial\nfrom collections import Counter, defaultdict\n\nfrom PIL import Image\n\nfrom joblib import Parallel, delayed\n\nfrom sklearn.model_selection import StratifiedKFold, KFold, GroupKFold\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.metrics import cohen_kappa_score\nfrom sklearn.preprocessing import StandardScaler\n\nfrom tqdm import tqdm\nfrom fastprogress import master_bar, progress_bar\n\npd.options.display.max_columns = 128\ntorch.multiprocessing.set_start_method(\"spawn\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fdd9dd8f815b98f600c7e65668b12ff5b321799f"},"cell_type":"markdown","source":"## Data Loading"},{"metadata":{"trusted":true,"_uuid":"31dd1db3ae9a48171c45d2b74bedf81a176fcb8c"},"cell_type":"code","source":"input_dir = Path(\"../input/petfinder-adoption-prediction/\")\ntrain = pd.read_csv(input_dir / \"train/train.csv\")\ntest = pd.read_csv(input_dir / \"test/test.csv\")\nsample_submission = pd.read_csv(input_dir / \"test/sample_submission.csv\")\n\ntrain_pet_ids = train.PetID.unique()\ntest_pet_ids = test.PetID.unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.PhotoAmt.mean(), test.PhotoAmt.mean()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"15150b4893d23dd21cafa51d889713e4c50a1d1f"},"cell_type":"markdown","source":"## Image model loading"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"!cp ../input/pytorch-pretrained-image-models/* ./\n!ls ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a04adc838cfa9aa01f74052dc8eb9ea29af59406"},"cell_type":"markdown","source":"## Metadata and Sentiment data"},{"metadata":{"trusted":true,"_uuid":"64d5f41ce9fbcf68d024e396f7b6e7b5c733fc45"},"cell_type":"code","source":"def jopen(path):\n    with open(path, \"r\", encoding=\"utf-8\") as f:\n        json_file = json.load(f)\n    return json_file\n\n\ndef parse_sentiment_file(path):\n    file = jopen(path)\n    language: str = file[\"language\"]\n\n    sentiment: list = file[\"documentSentiment\"]\n    entities: list = [x[\"name\"] for x in file[\"entities\"]]\n    entity = \" \".join(entities)\n\n    sentence_sentiment: list = [x[\"sentiment\"] for x in file[\"sentences\"]]\n    magnitude: np.ndarray = np.array(\n        [x[\"magnitude\"] for x in sentence_sentiment])\n    score: np.ndarray = np.array([x[\"score\"] for x in sentence_sentiment])\n\n    return_js = {\n        \"magnitude_sum\": magnitude.sum(),\n        \"magnitude_mean\": magnitude.mean(),\n        \"magnitude_var\": magnitude.var(),\n        \"score_sum\": score.sum(),\n        \"score_mean\": score.mean(),\n        \"score_var\": score.var(),\n        \"language\": language,\n        \"entity\": entity,\n        \"document_magnitude\": sentiment[\"magnitude\"],\n        \"document_score\": sentiment[\"score\"]\n    }\n    return return_js\n\n\ndef parse_metadata(path):\n    file: dict = jopen(path)\n    file_keys = list(file.keys())\n    name_specified = 0\n    if \"labelAnnotations\" in file_keys:\n        file_annots = file[\"labelAnnotations\"]\n        file_mean_score = np.asarray([x[\"score\"] for x in file_annots]).mean()\n        file_desc = \" \".join([x[\"description\"] for x in file_annots])\n        if \"cat\" in file_desc or \"dog\" in file_desc:\n            name_specified = 1\n    else:\n        file_mean_score = np.nan\n        file_desc = \"\"\n\n    file_colors: list = file[\"imagePropertiesAnnotation\"][\"dominantColors\"][\n        \"colors\"]\n    file_crops: list = file[\"cropHintsAnnotation\"][\"cropHints\"]\n\n    color_score = np.asarray([x[\"score\"] for x in file_colors]).mean()\n    pixel_frac = np.asarray([x[\"pixelFraction\"] for x in file_colors]).mean()\n    crop_conf = np.asarray([x[\"confidence\"] for x in file_crops]).mean()\n\n    if \"importanceFraction\" in file_crops[0].keys():\n        crop_importance = np.asarray(\n            [x[\"importanceFraction\"] for x in file_crops]).mean()\n    else:\n        crop_importance = np.nan\n    metadata = {\n        \"annot_score\": file_mean_score,\n        \"color_score\": color_score,\n        \"pixel_frac\": pixel_frac,\n        \"crop_conf\": crop_conf,\n        \"crop_importance\": crop_importance,\n        \"desc\": file_desc,\n        \"specified\": name_specified\n    }\n    return metadata\n\n\ndef additinal_features_per_id(pet_id, sentiment_path: Path, meta_path: Path):\n    sentiment_path = sentiment_path / f\"{pet_id}.json\"\n    try:\n        sentiment = parse_sentiment_file(sentiment_path)\n        sentiment[\"pet_id\"] = pet_id\n    except FileNotFoundError:\n        sentiment = {}\n\n    meta_files = sorted(meta_path.glob(f\"{pet_id}*.json\"))\n    metadata_list = []\n    if len(meta_files) > 0:\n        for f in meta_files:\n            metadata = parse_metadata(f)\n            metadata[\"pet_id\"] = pet_id\n            metadata_list.append(metadata)\n    return sentiment, metadata_list\n\n\ndef load_additional_features(ped_ids: list, sentiment_path: Path,\n                             meta_path: Path):\n    features = Parallel(\n        n_jobs=-1, verbose=1)(\n            delayed(additinal_features_per_id)(i, sentiment_path, meta_path)\n            for i in ped_ids)\n    sentiments = [x[0] for x in features if len(x[0]) > 0]\n    metadatas = [x[1] for x in features if len(x[1]) > 0]\n    sentiment_keys = sentiments[0].keys()\n    metadata_keys = metadatas[0][0].keys()\n    sentiment_dict = {}\n    metadata_dict = {}\n    for key in sentiment_keys:\n        sentiment_dict[key] = [x[key] for x in sentiments]\n\n    for key in metadata_keys:\n        meta_list = []\n        for meta_per_pid in metadatas:\n            meta_list += [meta[key] for meta in meta_per_pid]\n        metadata_dict[key] = meta_list\n\n    sentiment_df = pd.DataFrame(sentiment_dict)\n    metadata_df = pd.DataFrame(metadata_dict)\n    return sentiment_df, metadata_df\n\n\ndef aggregate_metadata(metadata_df: pd.DataFrame,\n                       aggregates=[\"sum\", \"mean\", \"var\"]):\n    meta_desc: pd.DataFrame = metadata_df.groupby([\"pet_id\"])[\"desc\"].unique()\n    meta_desc = meta_desc.reset_index()\n    meta_desc[\"desc\"] = meta_desc[\"desc\"].apply(lambda x: \" \".join(x))\n\n    meta_gr: pd.DataFrame = metadata_df.drop([\"desc\"], axis=1)\n    for i in meta_gr.columns:\n        if \"pet_id\" not in i:\n            meta_gr[i] = meta_gr[i].astype(float)\n    meta_gr = meta_gr.groupby([\"pet_id\"]).agg(aggregates)\n    meta_gr.columns = pd.Index(\n        [f\"{c[0]}_{c[1].upper()}\" for c in meta_gr.columns.tolist()])\n    meta_gr = meta_gr.reset_index()\n    return meta_gr, meta_desc\n\n\ndef aggregate_sentiment(sentiment_df: pd.DataFrame, aggregates=[\"sum\"]):\n    sentiment_desc: pd.DataFrame = sentiment_df.groupby(\n        [\"pet_id\"])[\"entity\"].unique()\n    sentiment_desc = sentiment_desc.reset_index()\n    sentiment_desc[\"entity\"] = sentiment_desc[\"entity\"].apply(\n        lambda x: \" \".join(x))\n    sentiment_lang = sentiment_df.groupby(\n        [\"pet_id\"])[\"language\"].unique()\n    sentiment_lang = sentiment_lang.reset_index()\n    sentiment_lang[\"language\"] = sentiment_lang[\"language\"].apply(\n        lambda x: \" \".join(x))\n    sentiment_desc = sentiment_desc.merge(\n        sentiment_lang, how=\"left\", on=\"pet_id\")\n    \n\n    sentiment_gr: pd.DataFrame = sentiment_df.drop([\"entity\", \"language\"],\n                                                   axis=1)\n    for i in sentiment_gr.columns:\n        if \"pet_id\" not in i:\n            sentiment_gr[i] = sentiment_gr[i].astype(float)\n    sentiment_gr = sentiment_gr.groupby([\"pet_id\"]).agg(aggregates)\n    sentiment_gr.columns = pd.Index(\n        [f\"{c[0]}\" for c in sentiment_gr.columns.tolist()])\n    sentiment_gr = sentiment_gr.reset_index()\n    return sentiment_gr, sentiment_desc","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":false},"cell_type":"markdown","source":"## Load data"},{"metadata":{"trusted":true,"_uuid":"a2a404fac56478b68955bfb8f2ab4cd17a34d5cb"},"cell_type":"code","source":"input_dir = Path(\"../input/petfinder-adoption-prediction/\")\ntrain = pd.read_csv(input_dir / \"train/train.csv\")\ntest = pd.read_csv(input_dir / \"test/test.csv\")\nsample_submission = pd.read_csv(input_dir / \"test/sample_submission.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5bd906850dc0ed9476140b7fa0835bcef8c57b64"},"cell_type":"code","source":"sp_train = input_dir / Path(\"train_sentiment/\")\nmp_train = input_dir / Path(\"train_metadata/\")\nsp_test = input_dir / Path(\"test_sentiment/\")\nmp_test = input_dir / Path(\"test_metadata/\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5a60150bcae8f4bef0ad91c425ecc41e1f97b02d"},"cell_type":"code","source":"train_pet_ids = train.PetID.unique()\ntest_pet_ids = test.PetID.unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"80ea8fba883636520ffc0e200278a5097ca20ee6"},"cell_type":"code","source":"train_sentiment_df, train_metadata_df = load_additional_features(\n    train_pet_ids, sp_train, mp_train)\n\ntest_sentiment_df, test_metadata_df = load_additional_features(\n    test_pet_ids, sp_test, mp_test)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9e363a35cd0ef7aac5b7dabdd376478565bcaeea"},"cell_type":"markdown","source":"## Aggregate sentiment data and metadata"},{"metadata":{"trusted":true,"_uuid":"ddc55fc2f3b0ee50ff60d58908f9eb8d7fcad780"},"cell_type":"code","source":"train_meta_gr, train_meta_desc = aggregate_metadata(train_metadata_df)\ntest_meta_gr, test_meta_desc = aggregate_metadata(test_metadata_df)\ntrain_sentiment_gr, train_sentiment_desc = \\\n    aggregate_sentiment(train_sentiment_df)\ntest_sentiment_gr, test_sentiment_desc = \\\n    aggregate_sentiment(test_sentiment_df)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2badeaca2206cdf6a224e7225385ef9440b8a3dd"},"cell_type":"markdown","source":"## Merge processed DataFrames with base train/test DataFrame"},{"metadata":{"trusted":true,"_uuid":"7c9807588b2fe960b1f96eeb07b908505dd9b3f0"},"cell_type":"code","source":"train_proc = train.copy()\ntrain_proc = train_proc.merge(\n    train_sentiment_gr, how=\"left\", left_on=\"PetID\", right_on=\"pet_id\")\ntrain_proc = train_proc.merge(\n    train_meta_gr, how=\"left\", left_on=\"PetID\", right_on=\"pet_id\")\ntrain_proc = train_proc.merge(\n    train_sentiment_desc, how=\"left\", left_on=\"PetID\", right_on=\"pet_id\")\ntrain_proc = train_proc.merge(\n    train_meta_desc, how=\"left\", left_on=\"PetID\", right_on = \"pet_id\")\n\ntest_proc = test.copy()\ntest_proc = test_proc.merge(\n    test_sentiment_gr, how=\"left\", left_on=\"PetID\", right_on=\"pet_id\")\ntest_proc = test_proc.merge(\n    test_meta_gr, how=\"left\", left_on=\"PetID\", right_on=\"pet_id\")\ntest_proc = test_proc.merge(\n    test_sentiment_desc, how=\"left\", left_on=\"PetID\", right_on=\"pet_id\")\ntest_proc = test_proc.merge(\n    test_meta_desc, how=\"left\", left_on=\"PetID\", right_on = \"pet_id\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2468fdc9fe6a99059fc3ec87213638fcd843b4d8"},"cell_type":"code","source":"print(train_proc.shape, test_proc.shape)\nassert train_proc.shape[0] == train.shape[0]\nassert test_proc.shape[0] == test.shape[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5e5cbe9287fd8150b83a3044f6f5c344bd68ac72"},"cell_type":"code","source":"train_proc.drop(train_proc.filter(\n    regex=\"pet_id\", axis=1).columns.tolist(), \n    axis=1, \n    inplace=True)\n\ntest_proc.drop(test_proc.filter(\n    regex=\"pet_id\", axis=1).columns.tolist(),\n    axis=1,\n    inplace=True)\n\ntrain_proc.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a4d3c3cd67a16af0bcdea52392a1a107995cd0a2"},"cell_type":"code","source":"train_proc.language.fillna(\"\", inplace=True)\ntest_proc.language.fillna(\"\", inplace=True)\n\nlangs = train_proc.language.unique()\nencode_dict = {k: i for i, k in enumerate(langs)}\n\ntrain_proc.language = train_proc.language.map(encode_dict)\ntest_proc.language = test_proc.language.map(encode_dict)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3ab3b3a25300ebc10720f61612358f52007caec0"},"cell_type":"markdown","source":"## Add Breed Mapping"},{"metadata":{"trusted":true,"_uuid":"6d6199ceadd4b7c2734ca0df0801338faefd64a4"},"cell_type":"code","source":"labels_breed = pd.read_csv(\"../input/petfinder-adoption-prediction/breed_labels.csv\")\nlabels_state = pd.read_csv(\"../input/petfinder-adoption-prediction/state_labels.csv\")\nlabels_color = pd.read_csv(\"../input/petfinder-adoption-prediction/color_labels.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c2193d9fb9f2e9eb2449c2dffd245227ab9d299a"},"cell_type":"code","source":"train_breed_main = train_proc[[\"Breed1\"]].merge(\n    labels_breed, how=\"left\",\n    left_on=\"Breed1\", right_on=\"BreedID\",\n    suffixes=(\"\", \"_main_breed\"))\ntrain_breed_main = train_breed_main.iloc[:, 2:]\ntrain_breed_main = train_breed_main.add_prefix(\"main_breed_\")\n\ntrain_breed_second = train_proc[[\"Breed2\"]].merge(\n    labels_breed, how=\"left\",\n    left_on=\"Breed2\", right_on=\"BreedID\",\n    suffixes=(\"\", \"_second_breed\"))\ntrain_breed_second = train_breed_second.iloc[:, 2:]\ntrain_breed_second = train_breed_second.add_prefix(\"second_breed_\")\n\ntrain_proc = pd.concat([\n    train_proc, train_breed_main, train_breed_second\n], axis=1)\n\ntest_breed_main = test_proc[[\"Breed1\"]].merge(\n    labels_breed, how=\"left\",\n    left_on=\"Breed1\", right_on=\"BreedID\",\n    suffixes=(\"\", \"_main_breed\"))\ntest_breed_main = test_breed_main.iloc[:, 2:]\ntest_breed_main = test_breed_main.add_prefix(\"main_breed_\")\n\ntest_breed_second = test_proc[[\"Breed2\"]].merge(\n    labels_breed, how=\"left\",\n    left_on=\"Breed2\", right_on=\"BreedID\",\n    suffixes=(\"\", \"_second_breed\"))\ntest_breed_second = test_breed_second.iloc[:, 2:]\ntest_breed_second = test_breed_second.add_prefix(\"second_breed_\")\n\ntest_proc = pd.concat([\n    test_proc, test_breed_main, test_breed_second\n], axis=1)\n\nprint(train_proc.shape, test_proc.shape)\ntrain_proc.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c492c2a290bb067d0dc656733054d0ae44ab73bf"},"cell_type":"markdown","source":"## Feature engineering"},{"metadata":{"trusted":true,"_uuid":"ce8d622672d743f5e746549d3ac85478f21c0d31"},"cell_type":"code","source":"X = pd.concat([train_proc, test_proc], ignore_index=True, sort=False)\nX_temp = X.copy()\n\ntext_columns = [\n    \"Description\",\n    \"entity\",\n    \"desc\"]\ncategorical_columns = [\n    \"Type\", \"Breed1\", \"Breed2\", \"Gender\",\n    \"Color1\", \"Color2\", \"Color3\", \"MaturitySize\",\n    \"FurLength\", \"Vaccinated\", \"Dewormed\", \"Sterilized\",\n    \"State\", \"language\", \"main_breed_BreedName\", \"second_breed_BreedName\"\n]\ncat_c = [\"main_breed_BreedName\", \"second_breed_BreedName\"]\ndrop_columns = [\n    \"PetID\", \"Name\", \"RescuerID\"\n]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8f85feb5de68ba8edb643319f2f3d652d1ebfa1f"},"cell_type":"code","source":"for i in cat_c:\n    X_temp.loc[:, i] = pd.factorize(X_temp.loc[:, i])[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c65e045184826428d9074a13caa185ffd8793999"},"cell_type":"code","source":"X_text = X_temp[text_columns]\nfor i in X_text.columns:\n    X_text[i] = X_text[i].fillna(\"none\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"000e0e05491ac479349ffe40d8146f912d00c1d5"},"cell_type":"code","source":"X_temp[\"len_description\"] = X_text[\"Description\"].map(len)\nX_temp[\"len_meta_desc\"] = X_text[\"desc\"].map(len)\nX_temp[\"len_entity\"] = X_text[\"entity\"].map(len)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import re\n\nfrom nltk.corpus import stopwords\nimport spacy\nfrom spacy.lang.en.stop_words import STOP_WORDS\nfrom spacy.lang.en import English\nfrom nltk.tokenize import RegexpTokenizer\nimport nltk.stem as stm\nfrom nltk import WordNetLemmatizer, word_tokenize\n\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\n\npuncts = [',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', \"'\", '$', '&', '/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', '•',  '~', '@', '£', \n '·', '_', '{', '}', '©', '^', '®', '`',  '<', '→', '°', '€', '™', '›',  '♥', '←', '×', '§', '″', '′', 'Â', '█', '½', 'à', '…', \n '“', '★', '”', '–', '●', 'â', '►', '−', '¢', '²', '¬', '░', '¶', '↑', '±', '¿', '▾', '═', '¦', '║', '―', '¥', '▓', '—', '‹', '─', \n '▒', '：', '¼', '⊕', '▼', '▪', '†', '■', '’', '▀', '¨', '▄', '♫', '☆', 'é', '¯', '♦', '¤', '▲', 'è', '¸', '¾', 'Ã', '⋅', '‘', '∞', \n '∙', '）', '↓', '、', '│', '（', '»', '，', '♪', '╩', '╚', '³', '・', '╦', '╣', '╔', '╗', '▬', '❤', 'ï', 'Ø', '¹', '≤', '‡', '√', ]\n\ndef clean_text(x):\n    x = str(x)\n    for punct in puncts:\n        x = x.replace(punct, f' {punct} ')\n    return x\n\ndef clean_numbers(x):\n    x = re.sub('[0-9]{5,}', '#####', x)\n    x = re.sub('[0-9]{4}', '####', x)\n    x = re.sub('[0-9]{3}', '###', x)\n    x = re.sub('[0-9]{2}', '##', x)\n    return x\n\nmispell_dict = {\"aren't\" : \"are not\",\n\"can't\" : \"cannot\",\n\"couldn't\" : \"could not\",\n\"didn't\" : \"did not\",\n\"doesn't\" : \"does not\",\n\"don't\" : \"do not\",\n\"hadn't\" : \"had not\",\n\"hasn't\" : \"has not\",\n\"haven't\" : \"have not\",\n\"he'd\" : \"he would\",\n\"he'll\" : \"he will\",\n\"he's\" : \"he is\",\n\"i'd\" : \"I would\",\n\"i'd\" : \"I had\",\n\"i'll\" : \"I will\",\n\"i'm\" : \"I am\",\n\"isn't\" : \"is not\",\n\"it's\" : \"it is\",\n\"it'll\":\"it will\",\n\"i've\" : \"I have\",\n\"let's\" : \"let us\",\n\"mightn't\" : \"might not\",\n\"mustn't\" : \"must not\",\n\"shan't\" : \"shall not\",\n\"she'd\" : \"she would\",\n\"she'll\" : \"she will\",\n\"she's\" : \"she is\",\n\"shouldn't\" : \"should not\",\n\"that's\" : \"that is\",\n\"there's\" : \"there is\",\n\"they'd\" : \"they would\",\n\"they'll\" : \"they will\",\n\"they're\" : \"they are\",\n\"they've\" : \"they have\",\n\"we'd\" : \"we would\",\n\"we're\" : \"we are\",\n\"weren't\" : \"were not\",\n\"we've\" : \"we have\",\n\"what'll\" : \"what will\",\n\"what're\" : \"what are\",\n\"what's\" : \"what is\",\n\"what've\" : \"what have\",\n\"where's\" : \"where is\",\n\"who'd\" : \"who would\",\n\"who'll\" : \"who will\",\n\"who're\" : \"who are\",\n\"who's\" : \"who is\",\n\"who've\" : \"who have\",\n\"won't\" : \"will not\",\n\"wouldn't\" : \"would not\",\n\"you'd\" : \"you would\",\n\"you'll\" : \"you will\",\n\"you're\" : \"you are\",\n\"you've\" : \"you have\",\n\"'re\": \" are\",\n\"wasn't\": \"was not\",\n\"we'll\":\" will\",\n\"didn't\": \"did not\",\n\"tryin'\":\"trying\"}\n\ndef _get_mispell(mispell_dict):\n    mispell_re = re.compile('(%s)' % '|'.join(mispell_dict.keys()))\n    return mispell_dict, mispell_re\n\nmispellings, mispellings_re = _get_mispell(mispell_dict)\ndef replace_typical_misspell(text):\n    def replace(match):\n        return mispellings[match.group(0)]\n    return mispellings_re.sub(replace, text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_text[\"cleaned_text\"] = X_text[\"Description\"].map(lambda x: x.lower())\nX_text[\"cleaned_text\"] = X_text[\"cleaned_text\"].map(lambda x: clean_text(x))\nX_text[\"cleaned_text\"] = X_text[\"cleaned_text\"].map(lambda x: clean_numbers(x))\nX_text[\"cleaned_text\"] = X_text[\"cleaned_text\"].map(lambda x: replace_typical_misspell(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"eng_stopwords = set(stopwords.words(\"english\"))\nimport string\n\nX_temp[\"len_description\"] = X_text[\"Description\"].map(len)\nX_temp[\"len_meta_desc\"] = X_text[\"desc\"].map(len)\nX_temp[\"len_entity\"] = X_text[\"entity\"].map(len)\n\nX_temp[\"num_description_words\"] = X_text[\"Description\"].map(lambda x: len(str(x).split()))\nX_temp[\"num_desc_words\"] = X_text[\"desc\"].map(lambda x: len(str(x).split()))\nX_temp[\"num_entity_words\"] = X_text[\"entity\"].map(lambda x: len(str(x).split()))\n\nX_temp[\"uniq_description_words\"] = X_text[\"Description\"].map(lambda x: len(set(str(x).split())))\nX_temp[\"uniq_desc_words\"] = X_text[\"desc\"].map(lambda x: len(set(str(x).split())))\nX_temp[\"uniq_entity_words\"] = X_text[\"entity\"].map(lambda x: len(set(str(x).split())))\n\nX_temp[\"num_description_stopwords\"] = X_text[\"Description\"].map(lambda x: len([\n    w for w in str(x).lower().split() if w in eng_stopwords]))\nX_temp[\"num_desc_stopwords\"] = X_text[\"desc\"].map(lambda x: len([\n    w for w in str(x).lower().split() if w in eng_stopwords]))\nX_temp[\"num_entity_stopwords\"] = X_text[\"entity\"].map(lambda x: len([\n    w for w in str(x).lower().split() if w in eng_stopwords]))\n\nX_temp[\"num_description_punctuation\"] = X_text[\"Description\"].map(lambda x: len([\n    c for c in str(x) if c in string.punctuation]))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"aa54bd185c17eb6a8006f97764a8d5e945642ca6"},"cell_type":"markdown","source":"### State stats"},{"metadata":{"trusted":true,"_uuid":"af908305a8452fc51a1019f2c80ad3ead58fb24e"},"cell_type":"code","source":"state_gdp = {\n    41336: 116.679,\n    41325: 40.596,\n    41367: 23.02,\n    41401: 190.075,\n    41415: 5.984,\n    41324: 37.274,\n    41332: 42.389,\n    41335: 52.452,\n    41330: 67.629,\n    41380: 5.642,\n    41327: 81.284,\n    41345: 80.167,\n    41342: 121.414,\n    41326: 280.698,\n    41361: 32.270\n}\n\nstate_population = {\n    41336: 33.48283,\n    41325: 19.47651,\n    41367: 15.39601,\n    41401: 16.74621,\n    41415: 0.86908,\n    41324: 8.21110,\n    41332: 10.21064,\n    41335: 15.00817,\n    41330: 23.52743,\n    41380: 2.31541,\n    41327: 15.61383,\n    41345: 32.06742,\n    41342: 24.71140,\n    41326: 54.62141,\n    41361: 10.35977\n}\n\nstate_area ={\n    41336:19102,\n    41325:9500,\n    41367:15099,\n    41401:243,\n    41415:91,\n    41324:1664,\n    41332:6686,\n    41335:36137,\n    41330:21035,\n    41380:821,\n    41327:1048,\n    41345:73631,\n    41342:124450,\n    41326:8104,\n    41361:13035\n}\nX_temp[\"state_gdp\"] = X_temp.State.map(state_gdp)\nX_temp[\"state_population\"] = X_temp.State.map(state_population)\nX_temp[\"state_area\"] = X_temp.State.map(state_area)\n\nX_temp[\"state_gdp_per_person\"] = X_temp[\"state_gdp\"] / X_temp[\"state_population\"] * 1e4\nX_temp[\"fee_per_gdp_per_person\"] = X_temp.Fee / X_temp[\"state_gdp_per_person\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dcf68f94944d5a105f30c3847f34fd36e1ad71fb"},"cell_type":"code","source":"X_temp.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e48c8f21713b9ad4e554f087335395677252af20"},"cell_type":"markdown","source":"### Name features"},{"metadata":{"trusted":true,"_uuid":"138a56a2df54cbbfb704937b43f924b42147e71c"},"cell_type":"code","source":"import re\ndef has_name(x):\n    if isinstance(x, float):\n        return 0\n    if \"no name\" in x.lower():\n        return 0\n    return 1\n\n\ndef num_name_words(x):\n    if isinstance(x, float):\n        return 0\n    name_words = x.split(\" \")\n    return len(name_words)\n\n\ndef contains_amp(x):\n    if isinstance(x, float):\n        return 0\n    if \"&\" in x:\n        return 1\n    if \"and\" in x.lower():\n        return 1\n    if \"+\" in x.lower():\n        return 1\n    return 0\n\n\ndef contains_comma(x):\n    if isinstance(x, float):\n        return 0\n    if \",\" in x:\n        return 1\n    return 0\n\n\ndef start_with_number(x):\n    if isinstance(x, float):\n        return 0\n    match = re.match(f\"\\d\", x)\n    if match:\n        return int(match.group())\n    return 0\n\n\ndef contains_paren(x):\n    if isinstance(x, float):\n        return 0\n    if \"(\" in x:\n        return 1\n    if \")\" in x:\n        return 1\n    return 0\n\n\ndef contains_number(x):\n    if isinstance(x, float):\n        return 0\n    if re.match(r\".*\\d\", x):\n        return 1\n    return 0\n\n\ndef safe_calc_len(x):\n    if isinstance(x, float):\n        return 1\n    return len(x)\n\n\ndef num_unlike_letters(x):\n    if isinstance(x, float):\n        return 0\n    letters = {\n        ',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', \"'\", '$', \n        '&', '/', '[', ']', '>', '%', '=', '#', '*', '+',  \n        '•',  '~', '@', '£', '·', '_', '{', '}', '©', '^', '®', '`',\n        '<', '→', '°', '€', '™', '›',  '♥', '←', '×', '§', '″', '′', \n        'Â', '█', '½', 'à', '…', '“', '★', '”', '–', '●', 'â', '►', \n        '−', '¢', '²', '¬', '░', '¶', '↑', '±', '¿', '▾', '═', '¦', \n        '║', '―', '¥', '▓', '—', '‹', '─', '▒', '：', '¼', '⊕', '▼', \n        '▪', '†', '■', '’', '▀', '¨', '▄', '♫', '☆', 'é', '¯', '♦', \n        '¤', '▲', 'è', '¸', '¾', 'Ã', '⋅', '‘', '∞', '∙', '）', '↓', '、', \n        '│', '（', '»', '，', '♪', '╩', '╚', '³', '・', '╦', '╣', '╔', '╗', \n        '▬', '❤', 'ï', 'Ø', '¹', '≤', '‡', '√', \"ã\", \"ç\", \"å\", \"ä\",\n        \"¶\", \"ð\"}\n\n    letter_in = set(x)\n    intersection = letter_in.intersection(letters)\n    if len(intersection) == 0:\n        return 0\n    else:\n        unlike_num = 0\n        for l in intersection:\n            unlike_num += len(re.findall(re.escape(l), x))\n        return unlike_num\n\nX_temp[\"num_name_words\"] = X_temp.Name.map(lambda x: num_name_words(x))\nX_temp[\"contains_amp\"] = X_temp.Name.map(lambda x: contains_amp(x))\nX_temp[\"contains_comma\"] = X_temp.Name.map(lambda x: contains_comma(x))\nX_temp[\"start_with_number\"] = X_temp.Name.map(lambda x: start_with_number(x))\nX_temp[\"contains_paren\"] = X_temp.Name.map(lambda x: contains_paren(x))\nX_temp[\"contains_number\"] = X_temp.Name.map(lambda x: contains_number(x))\nX_temp[\"name_length\"] = X_temp.Name.map(lambda x: safe_calc_len(x))\nX_temp[\"num_unlike_letters\"] = X_temp.Name.map(lambda x: num_unlike_letters(x))\nX_temp[\"rate_unlike_letters\"] = X_temp.num_unlike_letters / X_temp.name_length\nX_temp.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4aa6be7a80659bf78178766f53dfea7af91d06b6"},"cell_type":"markdown","source":"## Tfidf"},{"metadata":{"trusted":true,"_uuid":"95de8074d85b1b8c42c3b209dfed4a8903d5a673"},"cell_type":"code","source":"n_components = 16\ntext_features = []\n\nfor i in text_columns:\n    print(f\"generating features from: {i}\")\n    tfv = TfidfVectorizer(\n        min_df=2,\n        strip_accents=\"unicode\",\n        analyzer=\"word\",\n        token_pattern=r\"(?u)\\b\\w+\\b\",\n        ngram_range=(1, 3),\n        use_idf=1,\n        smooth_idf=1,\n        sublinear_tf=1)\n    svd = TruncatedSVD(\n        n_components=n_components,\n        random_state=1337)\n    tfidf_col = tfv.fit_transform(X_text.loc[:, i].values)\n    svd_col = svd.fit_transform(tfidf_col)\n    svd_col = pd.DataFrame(svd_col)\n    svd_col = svd_col.add_prefix(\"Tfidf_{}_\".format(i))\n    \n    text_features.append(svd_col)\n\ntext_features = pd.concat(text_features, axis=1)\nX_temp = pd.concat([X_temp, text_features], axis=1)\n\nfor i in text_columns:\n    X_temp.drop(i, axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4f43400c8d2ce34ef6db94049bc215be6de528fd"},"cell_type":"markdown","source":"## Image size features"},{"metadata":{"trusted":true,"_uuid":"190f69f684f76dc03a93040cda0f12bb9ef190b8"},"cell_type":"code","source":"import os\nimport glob\n\ntrain_image_files = sorted(\n    glob.glob(\"../input/petfinder-adoption-prediction/train_images/*.jpg\"))\ntest_image_files = sorted(\n    glob.glob(\"../input/petfinder-adoption-prediction/test_images/*.jpg\"))\n\ntrain_df_imgs = pd.DataFrame(train_image_files)\ntest_df_imgs = pd.DataFrame(test_image_files)\ntrain_df_imgs.columns = [\"image_file_name\"]\ntest_df_imgs.columns = [\"image_file_name\"]\n\ntrain_imgs_pets = train_df_imgs[\"image_file_name\"].apply(\n    lambda x: x.split(\"/\")[-1].split(\"-\")[0])\ntest_imgs_pets = test_df_imgs[\"image_file_name\"].apply(\n    lambda x: x.split(\"/\")[-1].split(\"-\")[0])\ntrain_df_imgs = train_df_imgs.assign(PetID=train_imgs_pets)\ntest_df_imgs = test_df_imgs.assign(PetID=test_imgs_pets)\n\ndef get_size(filename):\n    st = os.stat(filename)\n    return st.st_size\n\ndef get_dimensions(filename):\n    img_size = Image.open(filename).size\n    return img_size\n\ntrain_df_imgs[\"image_size\"] = train_df_imgs[\"image_file_name\"].apply(get_size)\ntest_df_imgs[\"image_size\"] = test_df_imgs[\"image_file_name\"].apply(get_size)\ntrain_df_imgs[\"temp_size\"] = train_df_imgs[\"image_file_name\"].apply(get_dimensions)\ntest_df_imgs[\"temp_size\"] = test_df_imgs[\"image_file_name\"].apply(get_dimensions)\ntrain_df_imgs[\"width\"] = train_df_imgs[\"temp_size\"].apply(lambda x: x[0])\ntest_df_imgs[\"width\"] = test_df_imgs[\"temp_size\"].apply(lambda x: x[0])\ntrain_df_imgs[\"height\"] = train_df_imgs[\"temp_size\"].apply(lambda x: x[1])\ntest_df_imgs[\"height\"] = test_df_imgs[\"temp_size\"].apply(lambda x: x[1])\ntrain_df_imgs.drop([\"temp_size\"], axis=1, inplace=True)\ntest_df_imgs.drop([\"temp_size\"], axis=1, inplace=True)\n\naggs = {\n    \"image_size\": [\"sum\", \"mean\", \"var\"],\n    \"width\": [\"sum\", \"mean\", \"var\"],\n    \"height\": [\"sum\", \"mean\", \"var\"]\n}\nagg_train_imgs = train_df_imgs.groupby(\"PetID\").agg(aggs)\nnew_columns = [\n    k + \"_\" + agg for k in aggs.keys() for agg in aggs[k]\n]\nagg_test_imgs = test_df_imgs.groupby(\"PetID\").agg(aggs)\nagg_train_imgs.columns = new_columns\nagg_train_imgs = agg_train_imgs.reset_index()\n\nagg_test_imgs.columns = new_columns\nagg_test_imgs = agg_test_imgs.reset_index()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f1e67fa6e8b1c1deac413010b7ed9b9d43234f92"},"cell_type":"code","source":"agg_imgs = pd.concat([agg_train_imgs, agg_test_imgs], axis=0).reset_index(drop=True)\nX_temp = X_temp.merge(agg_imgs, how=\"left\", on=\"PetID\")\nX_temp.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Sanitize"},{"metadata":{"trusted":true},"cell_type":"code","source":"hasnans = []\nfor c in X_temp.columns:\n    if X_temp[c].hasnans:\n        hasnans.append(c)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sanitize_col = list(\n    set(hasnans) - {\n        \"Name\", \"AdoptionSpeed\"\n    })\nX_temp[sanitize_col] = X_temp[sanitize_col].fillna(0.0)\nX_temp.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"big_num_cols = [\"len_description\", \"len_meta_desc\", \"len_entity\",\n                \"num_description_words\", \"num_desc_words\", \"num_entity_words\", \n                \"uniq_description_words\", \"uniq_desc_words\", \"uniq_entity_words\",\n                \"num_description_stopwords\", \"num_desc_stopwords\", \"num_entity_stopwords\",\n                \"num_description_punctuation\",\n                \"state_gdp\", \"state_population\", \"state_area\", \"state_gdp_per_person\",\n                \"image_size_sum\", \"image_size_mean\", \"image_size_var\", \"width_sum\",\n                \"width_mean\", \"width_var\", \"height_sum\", \"height_mean\", \"height_var\"]\nfor c in big_num_cols:\n    X_temp[c] = X_temp[c].map(lambda x: np.log1p(x))\nX_temp.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"629ec15f165fb4faf8051671f07abfaa2e6872df"},"cell_type":"markdown","source":"## Drop columns"},{"metadata":{"trusted":true,"_uuid":"d720a05abf06196c89ba7e21df7f4387f6cf8e72"},"cell_type":"code","source":"train_resc = train.RescuerID\ntest_resc = test.RescuerID","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0034fb4f78cd5cc6c26a762642deb73276303c9e"},"cell_type":"code","source":"X_temp.drop(drop_columns, axis=1, inplace=True)\nX_temp.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.shape[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2fd332396f3d3e5bcecf987cb038805b812bfd7d"},"cell_type":"code","source":"train.shape\nn_train = train.shape[0]\nX_train = X_temp.loc[:n_train-1, :]\nX_test = X_temp.loc[n_train:, :]\nX_test.drop([\"AdoptionSpeed\"], axis=1, inplace=True)\n\nassert X_train.shape[0] == train.shape[0]\nassert X_test.shape[0] == test.shape[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6bd7866671af382a793076e81bc9b6732b66dbd4"},"cell_type":"code","source":"train_cols = X_train.columns.tolist()\ntrain_cols.remove(\"AdoptionSpeed\")\n\ntest_cols = X_test.columns.tolist()\nassert np.all(train_cols == test_cols)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ebbbcb8c426fd7ca62fd54a8334c5bad226dd78e"},"cell_type":"code","source":"X_train_non_null = X_train.fillna(-1)\nX_test_non_null = X_test.fillna(-1)\n\nX_train_non_null.isnull().any().any(), X_test_non_null.isnull().any().any()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4fa273d5d3538864c5839a6de6aba0a343a8d530"},"cell_type":"code","source":"cat_features = [\"Type\", \"Breed1\", \"Breed2\", \"Gender\", \"Color1\", \"Color2\", \"Color3\",\n                \"MaturitySize\", \"FurLength\", \"Vaccinated\", \"Dewormed\", \"Sterilized\",\n                \"Health\", \"Quantity\", \"State\", \"language\", \"main_breed_BreedName\", \n                \"second_breed_BreedName\"]\n\nX_train_cat = X_train_non_null.loc[:, cat_features]\nX_test_cat = X_test_non_null.loc[:, cat_features]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"22aa79408edbd3799d3a139f4becb3bd12dc6ae2"},"cell_type":"markdown","source":"## GroupStratifiedKFold"},{"metadata":{"trusted":true,"_uuid":"e3889cb5951b66961abacdaa83cf2afc23be7493"},"cell_type":"code","source":"def stratified_group_k_fold(X, y, groups, k, seed=None):\n    labels_num = np.max(y) + 1\n    y_counts_per_group = defaultdict(lambda: np.zeros(labels_num))\n    y_distr = Counter()\n    for label, g in zip(y, groups):\n        y_counts_per_group[g][label] += 1\n        y_distr[label] += 1\n\n    y_counts_per_fold = defaultdict(lambda: np.zeros(labels_num))\n    groups_per_fold = defaultdict(set)\n\n    def eval_y_counts_per_fold(y_counts, fold):\n        y_counts_per_fold[fold] += y_counts\n        std_per_label = []\n        for label in range(labels_num):\n            label_std = np.std([y_counts_per_fold[i][label] / y_distr[label] for i in range(k)])\n            std_per_label.append(label_std)\n        y_counts_per_fold[fold] -= y_counts\n        return np.mean(std_per_label)\n    \n    groups_and_y_counts = list(y_counts_per_group.items())\n    random.Random(seed).shuffle(groups_and_y_counts)\n\n    for g, y_counts in sorted(groups_and_y_counts, key=lambda x: -np.std(x[1])):\n        best_fold = None\n        min_eval = None\n        for i in range(k):\n            fold_eval = eval_y_counts_per_fold(y_counts, i)\n            if min_eval is None or fold_eval < min_eval:\n                min_eval = fold_eval\n                best_fold = i\n        y_counts_per_fold[best_fold] += y_counts\n        groups_per_fold[best_fold].add(g)\n\n    all_groups = set(groups)\n    for i in range(k):\n        train_groups = all_groups - groups_per_fold[i]\n        test_groups = groups_per_fold[i]\n\n        train_indices = [i for i, g in enumerate(groups) if g in train_groups]\n        test_indices = [i for i, g in enumerate(groups) if g in test_groups]\n\n        yield train_indices, test_indices","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"058b7fbb860473d9a65f65400de195cddd6d932c"},"cell_type":"markdown","source":"## Categorical Target Encoding"},{"metadata":{"trusted":true,"_uuid":"6105f6ef169df736d238d7a978c040a2f6a8cd16"},"cell_type":"code","source":"y = X_train_non_null.AdoptionSpeed\nX_train_cat_y = X_train_cat.copy()\nX_train_cat_y[\"AdoptionSpeed\"] = y\n\nX_train_cat_encoded = np.zeros((X_train_cat.shape[0], len(cat_features) * 2))\nX_test_cat_encoded = np.zeros((X_test_cat.shape[0], len(cat_features) * 2))\nk = 10\nfold = StratifiedKFold(n_splits=k, shuffle=True, random_state=1213)\n\nfor trn_idx, val_idx in fold.split(X_train_cat, y.values.astype(int)):\n    X_trn, X_val = X_train_cat_y.loc[trn_idx, :], X_train_cat_y.loc[val_idx, :]\n    for j, c in enumerate(cat_features):\n        X_trn_enc = X_trn.groupby(c).agg({\n            \"AdoptionSpeed\": [\"mean\", \"std\"]\n        })\n        cte_columns = [f\"{c}_{x}\" for x in [\"mean\", \"std\"]]\n        X_trn_enc.columns = cte_columns\n        X_temp = np.zeros((X_test_cat.shape[0], 2))\n        X_temp_df = pd.DataFrame(data=X_temp, columns=cte_columns)\n        for x in X_trn_enc.columns:\n            X_val[x] = X_val[c].map(X_trn_enc[x])\n            X_temp_df[x] = X_test_cat[c].map(X_trn_enc[x]).reset_index(drop=True)\n        X_train_cat_encoded[val_idx, 2 * j:2 * (j + 1)] = X_val[X_trn_enc.columns].values\n        X_test_cat_encoded[:, 2 * j: 2 * (j + 1)] += X_temp_df.values / k","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4f29fd092db829af7253267c2ef4dc6c92f48e73"},"cell_type":"code","source":"X_train_cat_encoded.shape, X_test_cat_encoded.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"17d6dba910e9ef8689a059fe81f3a151a6ce8935"},"cell_type":"code","source":"columns = [f\"{c}_{x}\" for c in cat_features for x in [\"mean\", \"std\"]]\nX_train_cat_encoded_df = pd.DataFrame(data=X_train_cat_encoded, columns=columns)\nX_test_cat_encoded_df = pd.DataFrame(data=X_test_cat_encoded, columns=columns)\nX_test_cat_encoded_df.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a45e4eb9cee5516325edf586e4a9edd9d1d12e31"},"cell_type":"markdown","source":"## Numerical Features"},{"metadata":{"trusted":true,"_uuid":"8d91fecdf01da92b21e2d05a8ba6d67243b7f9da"},"cell_type":"code","source":"X_train_num = X_train_non_null.drop(cat_features + [\"AdoptionSpeed\"], axis=1)\nX_test_num = X_test_non_null.drop(cat_features, axis=1)\n\ntarget = X_train_non_null[\"AdoptionSpeed\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5ef772ea61eddacbd04b03c29614b94d2d6babf5"},"cell_type":"code","source":"X_train_num = pd.concat([X_train_num, X_train_cat_encoded_df], axis=1)\n\nX_test_cat_encoded_df.index = X_test_num.index\nX_test_num = pd.concat([X_test_num, X_test_cat_encoded_df], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2fab8a9f6506d6369c4cd0b3d813b16b2ce63440"},"cell_type":"code","source":"X_train_num.fillna(0.0, inplace=True)\nX_test_num.fillna(0.0, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7022fe088c563feee029b5a9fa599c4faca5a806"},"cell_type":"code","source":"X_train_num.replace(np.inf, np.nan).fillna(0.0, inplace=True)\nX_test_num.replace(np.inf, np.nan).fillna(0.0, inplace=True)\n\nX_train_num.replace(-np.inf, np.nan).fillna(0.0, inplace=True)\nX_test_num.replace(-np.inf, np.nan).fillna(0.0, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"037ada6259e60182176ad199d08bbe0ebf53fa35"},"cell_type":"code","source":"X_train_num.values[np.isinf(X_train_num)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3943eac9c119e7c778677861e30428a657264dc8"},"cell_type":"code","source":"X_all_num = pd.concat([X_train_num, X_test_num], axis=0)\nss = StandardScaler()\nss.fit(X_all_num)\n\nX_train_ss = ss.transform(X_train_num)\nX_test_ss = ss.transform(X_test_num)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1b20c6270c1a1e13ccf32364b043d6891814f511"},"cell_type":"code","source":"X_train_ss[np.isnan(X_train_ss)] = 0.0\nX_test_ss[np.isnan(X_test_ss)] = 0.0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c61099ad98de01c893e42f0691ff4d365ebb9216"},"cell_type":"code","source":"cat_cat = pd.concat([X_train_cat, X_test_cat])\n\nn_breed1 = cat_cat[\"Breed1\"].nunique()\nn_breed2 = cat_cat[\"Breed2\"].nunique()\nn_langs = cat_cat[\"language\"].nunique()\nn_color1 = cat_cat[\"Color1\"].nunique()\nn_color2 = cat_cat[\"Color2\"].nunique()\nn_color3 = cat_cat[\"Color3\"].nunique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e1f47565b9a8ff3d8eeb5f1b34154dde3e71af90"},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nfor c in X_train_cat.columns:\n        le = LabelEncoder()\n        le.fit(cat_cat[c])\n        X_train_cat[c] = le.transform(X_train_cat[c])\n        X_test_cat[c] = le.transform(X_test_cat[c])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Sequences"},{"metadata":{"trusted":true},"cell_type":"code","source":"import gc\ndef load_fasttext(word_index):    \n    EMBEDDING_FILE = '../input/fasttext-crawl-300d-2m/crawl-300d-2M.vec'\n    def get_coefs(word, *arr): return word, np.asarray(arr, dtype='float32')\n    embeddings_index = dict(get_coefs(*o.rstrip().rsplit(' ')) for o in open(EMBEDDING_FILE))\n    all_embs = np.stack(embeddings_index.values())\n    emb_mean,emb_std = all_embs.mean(), all_embs.std()\n    embed_size = all_embs.shape[1]\n\n    # word_index = tokenizer.word_index\n    nb_words = min(100000, len(word_index)) + 1\n    embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n    words_list = []\n    for word, i in word_index.items():\n        if i >= 100000: continue\n        embedding_vector = embeddings_index.get(word)\n        if embedding_vector is not None: \n            embedding_matrix[i] = embedding_vector\n            words_list.append(word)\n    return embedding_matrix, set(words_list)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer = Tokenizer(num_words=100000)\ntokenizer.fit_on_texts(X_text[\"cleaned_text\"] + \" \" + X_text[\"desc\"] + \" \" + X_text[\"entity\"])\nlen(tokenizer.word_index)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_text, test_text = X_text.loc[:n_train-1, :], X_text.loc[n_train:, :]\ntrain_text.shape, test_text.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train_text = tokenizer.texts_to_sequences(train_text[\"cleaned_text\"])\nx_test_text = tokenizer.texts_to_sequences(test_text[\"cleaned_text\"])\n\nx_train_text = pad_sequences(x_train_text, maxlen=70)\nx_test_text = pad_sequences(x_test_text, maxlen=70)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nembedding_matrix, words_set = load_fasttext(tokenizer.word_index)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def calc_vector(text, word_index, words_set, embedding_matrix):\n    words = set(text.split())\n    n_skip = 0\n    vec = np.zeros((embedding_matrix.shape[1],))\n    if len(words) == 0:\n        return vec\n    for n_w, word in enumerate(words):\n        if word in words_set:\n            idx = word_index.get(word)\n            vec_ = embedding_matrix[idx, :]\n        else:\n            n_skip += 1\n            continue\n        if n_w == 0:\n            vec = vec_\n        else:\n            vec = vec + vec_\n    vec = vec / (n_w - n_skip + 1)\n    return vec","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tqdm.pandas()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_entity = train_text.entity.progress_apply(lambda x: calc_vector(\n    x, tokenizer.word_index, words_set, embedding_matrix))\ntest_entity = test_text.entity.progress_apply(lambda x: calc_vector(\n    x, tokenizer.word_index, words_set, embedding_matrix))\n\ntrain_entity = np.vstack(train_entity.values.tolist())\ntest_entity = np.vstack(test_entity.values.tolist())\n\ntrain_entity[np.isnan(train_entity)] = 0.0\ntest_entity[np.isnan(test_entity)] = 0.0\n\ntrain_desc = train_text.desc.progress_apply(lambda x: calc_vector(\n    x, tokenizer.word_index, words_set, embedding_matrix))\ntest_desc = test_text.desc.progress_apply(lambda x: calc_vector(\n    x, tokenizer.word_index, words_set, embedding_matrix))\n\ntrain_desc = np.vstack(train_desc.values.tolist())\ntest_desc = np.vstack(test_desc.values.tolist())\n\ntrain_desc[np.isnan(train_desc)] = 0.0\ntest_desc[np.isnan(test_desc)] = 0.0","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Images"},{"metadata":{"trusted":true},"cell_type":"code","source":"normalize = transforms.Normalize(\n    mean=[0.485, 0.456, 0.406],\n    std=[0.229, 0.224, 0.225]\n)\nds_trans = transforms.Compose([transforms.Resize(224),\n                               transforms.CenterCrop(224),\n                               transforms.ToTensor(),\n                               normalize])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class ImageDataset(data.Dataset):\n    def __init__(self, pet_ids, root_dir, transform):\n        self.pet_ids = pet_ids\n        self.root_dir = root_dir\n        self.transform = transform\n        \n    def __len__(self):\n        return len(self.pet_ids)\n    \n    def __getitem__(self, idx):\n        imgs = torch.zeros((4, 3, 224, 224))\n        for i in range(4):\n            img_name = f\"{self.pet_ids[idx]}-{i+1}.jpg\"\n            fullname = self.root_dir / Path(img_name)\n            try:\n                image = Image.open(fullname).convert(\"RGB\")\n            except FileNotFoundError:\n                image = np.zeros((3, 224, 224), dtype=np.uint8).transpose(1, 2, 0)\n                image = Image.fromarray(np.uint8(image))\n            if self.transform:\n                image = self.transform(image)\n            imgs[i, :, :, :] = image\n        return [self.pet_ids[idx], imgs]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Classifier(nn.Module):\n    def __init__(self):\n        super(Classifier, self).__init__()\n        \n    def forward(self, x):\n        return x\n\nclass ImagePretrained(nn.Module):\n    def __init__(self, path):\n        super(ImagePretrained, self).__init__()\n        self.densenet121 = models.densenet121()\n        self.densenet121.load_state_dict(torch.load(path))\n        self.densenet121.classifier = Classifier()\n        dense = nn.Sequential(*list(self.densenet121.children())[:-1])\n        for param in dense.parameters():\n            param.requires_grad = False\n            \n    def forward(self, x):\n        converted = torch.zeros(x.size(0), 4, 1024)\n        for i in range(4):\n            out = self.densenet121(x[:, i, :, :, :])\n            converted[:, i, :] = out\n        return converted","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_img_dataset = ImageDataset(train_pet_ids,\n                             \"../input/petfinder-adoption-prediction/train_images/\",\n                             transform=ds_trans)\ntest_img_dataset = ImageDataset(test_pet_ids,\n                            \"../input/petfinder-adoption-prediction/test_images/\",\n                            transform=ds_trans)\ntrain_img_loader = data.DataLoader(train_img_dataset, batch_size=128, shuffle=False)\ntest_img_loader = data.DataLoader(test_img_dataset, batch_size=128, shuffle=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_pids = []\ntrain_img_matrix = np.zeros((len(train_pet_ids), 4, 1024))\nmodel = ImagePretrained(\"densenet121.pth\")\nmodel.to(\"cuda:0\")\nfor i, (pid, tensor) in tqdm(enumerate(train_img_loader)):\n    train_pids += [*pid]\n    tensor = tensor.to(\"cuda:0\")\n    pred = model(tensor).detach().cpu().numpy()\n    train_img_matrix[i * 128:(i + 1) * 128, :, :] = pred\n    \ntest_pids = []\ntest_img_matrix = np.zeros((len(test_pet_ids), 4, 1024))\nfor i, (pid, tensor) in tqdm(enumerate(test_img_loader)):\n    test_pids += [*pid]\n    tensor = tensor.to(\"cuda:0\")\n    pred = model(tensor).detach().cpu().numpy()\n    test_img_matrix[i * 128:(i + 1) * 128, :, :] = pred","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3b9bd342bf82e03749abc2534f49187794e890a4"},"cell_type":"markdown","source":"## Metrics"},{"metadata":{"trusted":true,"_uuid":"e352a3d956a76b441ff4fdfb0a3d82230f08e48a"},"cell_type":"code","source":"class OptimizedRounder(object):\n    def __init__(self):\n        self.coef_ = 0\n    \n    def _kappa_loss(self, coef, X, y):\n        preds = pd.cut(X, [-np.inf] + list(np.sort(coef)) + [np.inf], labels = [0, 1, 2, 3, 4])\n        return -cohen_kappa_score(y, preds, weights='quadratic')\n    \n    def fit(self, X, y, initial_coef=[]):\n        loss_partial = partial(self._kappa_loss, X = X, y = y)\n        initial_coef = initial_coef\n        self.coef_ = sp.optimize.minimize(loss_partial, initial_coef, method='nelder-mead')\n    \n    def predict(self, X, coef):\n        preds = pd.cut(X, [-np.inf] + list(np.sort(coef)) + [np.inf], labels = [0, 1, 2, 3, 4])\n        return preds\n    \n    def coefficients(self):\n        return self.coef_['x']\n    \ndef confusion_matrix(rater_a, rater_b, min_rating=None, max_rating=None):\n    assert len(rater_a) == len(rater_b)\n\n    if min_rating is None:\n        min_rating = min(rater_a + rater_b)\n    if max_rating is None:\n        max_rating = max(rater_a + rater_b)\n    num_rating = int(max_rating - min_rating + 1)\n    conf_mat = [[0 for i in range(num_rating)] for j in range(num_rating)]\n    for a, b in zip(rater_a, rater_b):\n        conf_mat[a - min_rating][b - min_rating] += 1\n    return conf_mat\n\n\ndef histogram(ratings, min_rating=None, max_rating=None):\n    if min_rating is None:\n        min_rating = min(ratings)\n    if max_rating is None:\n        max_rating = max(ratings)\n    num_ratings = int(max_rating - min_rating + 1)\n    hist_ratings = [0 for x in range(num_ratings)]\n    for r in ratings:\n        hist_ratings[r - min_rating] += 1\n    return hist_ratings\n\n\ndef quadratic_weighted_kappa(y, y_pred):\n    rater_a = y\n    rater_b = y_pred\n    min_rating = None\n    max_rating = None\n    rater_a = np.array(rater_a, dtype=int)\n    rater_b = np.array(rater_b, dtype=int)\n\n    assert len(rater_a) == len(rater_b)\n\n    min_rating = min(min(rater_a), min(rater_b))\n    max_rating = max(max(rater_a), max(rater_b))\n\n    conf_mat = confusion_matrix(rater_a, rater_b, min_rating, max_rating)\n    num_ratings = len(conf_mat)\n    num_scored_items = float(len(rater_a))\n\n    hist_rater_a = histogram(rater_a, min_rating, max_rating)\n    hist_rater_b = histogram(rater_b, min_rating, max_rating)\n\n    numerator = 0.0\n    denominator = 0.0\n\n    for i in range(num_ratings):\n        for j in range(num_ratings):\n            expected_count = (\n                hist_rater_a[i] * hist_rater_b[j]) / num_scored_items\n            d = pow(i - j, 2.0) / pow(num_ratings - 1, 2.0)\n            numerator += d * conf_mat[i][j] / num_scored_items\n            denominator += d * expected_count / num_scored_items\n\n    return (1.0 - numerator / denominator)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"843c82870b00649db3d27335587d388b6c09f5ca"},"cell_type":"markdown","source":"## DataLoader"},{"metadata":{"trusted":true,"_uuid":"c783d7639653290fd2654b6f940de6e5476a8af7"},"cell_type":"code","source":"class PetDataset(data.Dataset):\n    def __init__(self, \n                 img_tensor,\n                 cat_features, \n                 num_features,\n                 entity_tensor,\n                 desc_tensor,\n                 seq_tensor,\n                 labels):\n        self.img_tensor = img_tensor\n        self.cat_features = cat_features\n        self.num_features = num_features\n        self.entity_tensor = entity_tensor\n        self.desc_tensor = desc_tensor\n        self.seq_tensor = seq_tensor\n        if labels is not None:\n            self.labels = labels\n        else:\n            self.labels = None\n        \n    def __len__(self):\n        return len(self.cat_features)\n    \n    def __getitem__(self, idx):\n        img_feature = self.img_tensor[idx]\n        cat_feature = self.cat_features[idx]\n        num_feature = self.num_features[idx]\n        entity_feature = self.entity_tensor[idx]\n        desc_feature = self.desc_tensor[idx]\n        seq_feature = self.seq_tensor[idx]\n\n        if self.labels is not None:\n            label = self.labels[idx]\n            return [\n                img_feature, cat_feature, num_feature, \n                entity_feature, desc_feature, seq_feature, label]\n        else:\n            return [img_feature, cat_feature, num_feature,\n                    entity_feature, desc_feature, seq_feature]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"03c9f5c917def1297f4685e009ef70b21faca3ad"},"cell_type":"markdown","source":"## Trainer"},{"metadata":{"trusted":true,"_uuid":"0dc8f13b4e1311519e85048912cc7701863c6145"},"cell_type":"code","source":"def get_n_params(model):\n    pp=0\n    for p in list(model.parameters()):\n        nn=1\n        for s in list(p.size()):\n            nn = nn*s\n        pp += nn\n    return pp\n\ndef seed_torch(seed=1029):\n    random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n\ndef run_xgb(params,\n            X,\n            y,\n            X_test,\n            resc,\n            n_splits=10,\n            num_rounds=60000,\n            early_stop=500,\n            verbose_eval=1000):\n    oof_train = np.zeros((X.shape[0]))\n    oof_test = np.zeros((X_test.shape[0], n_splits))\n    fold = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=1213)\n\n    for i, (trn_index, val_index) in enumerate(fold.split(X, y.astype(int))):\n        X_tr = X.iloc[trn_index, :]\n        X_val = X.iloc[val_index, :]\n\n        y_tr = y[trn_index]\n        y_val = y[val_index]\n        d_train = xgb.DMatrix(\n            data=X_tr, label=y_tr, feature_names=X_tr.columns)\n        d_valid = xgb.DMatrix(\n            data=X_val, label=y_val, feature_names=X_val.columns)\n\n        watchlist = [(d_train, \"train\"), (d_valid, \"valid\")]\n        model = xgb.train(\n            params=params,\n            dtrain=d_train,\n            num_boost_round=num_rounds,\n            evals=watchlist,\n            early_stopping_rounds=early_stop,\n            verbose_eval=verbose_eval)\n        valid_pred = model.predict(\n            xgb.DMatrix(X_val, feature_names=X_val.columns),\n            ntree_limit=model.best_ntree_limit)\n        test_pred = model.predict(\n            xgb.DMatrix(X_test, feature_names=X_test.columns),\n            ntree_limit=model.best_ntree_limit)\n        oof_train[val_index] = valid_pred\n        oof_test[:, i] = test_pred\n    return model, oof_train, oof_test\n\n\nclass Trainer:\n    def __init__(self, \n                 model,\n                 resc,\n                 n_splits=5, \n                 seed=42, \n                 device=\"cuda:0\", \n                 train_batch=16,\n                 val_batch=32,\n                 kwargs={}):\n        self.model = model\n        self.n_splits = n_splits\n        self.seed = seed\n        self.device = device\n        self.train_batch = train_batch\n        self.val_batch = val_batch\n        self.kwargs = kwargs\n        self.resc = resc\n        \n        self.best_score = None\n        self.tag = dt.now().strftime(\"%Y-%m-%d-%H-%M-%S\")\n        \n        self.loss_fn = nn.MSELoss(reduction=\"mean\").to(self.device)\n        path = Path(f\"bin/{self.tag}\")\n        path.mkdir(exist_ok=True, parents=True)\n        self.path = path\n        \n    def fit(self, \n            img_feats, \n            cat_feats, \n            num_feats, \n            entity_feats,\n            desc_feats,\n            seq_feats, \n            answer, \n            n_epochs=30):\n        self.train_preds = np.zeros((train.shape[0]))\n        answer = answer.values.astype(int)\n        fold = StratifiedKFold(n_splits=self.n_splits, shuffle=True, random_state=self.seed)\n        cat_feats = cat_feats.values\n        for i, (trn_idx, val_idx) in enumerate(fold.split(img_feats, \n                                                          answer)):\n            self.fold_num = i\n            print(f\"Fold: {i+1}\")\n            img_train, img_val = img_feats[trn_idx], img_feats[val_idx]\n            cat_train, cat_val = cat_feats[trn_idx], cat_feats[val_idx]\n            num_train, num_val = num_feats[trn_idx], num_feats[val_idx]\n            ent_train, ent_val = entity_feats[trn_idx], entity_feats[val_idx]\n            dsc_train, dsc_val = desc_feats[trn_idx], desc_feats[val_idx]\n            seq_train, seq_val = seq_feats[trn_idx], seq_feats[val_idx]\n            y_train, y_val = answer[trn_idx] / 4, answer[val_idx] / 4\n            \n            valid_preds = self._fit(img_train, \n                                    cat_train, \n                                    num_train,\n                                    ent_train,\n                                    dsc_train,\n                                    seq_train,\n                                    y_train,\n                                    n_epochs,\n                                    img_val,\n                                    cat_val,\n                                    num_val,\n                                    ent_val,\n                                    dsc_val,\n                                    seq_val,\n                                    y_val)\n            self.train_preds[val_idx] = valid_preds\n        \n    def _fit(self, \n             img, \n             cat, \n             num,\n             ent,\n             dsc,\n             seq,\n             y, \n             n_epochs, \n             img_val, \n             cat_val, \n             num_val,\n             ent_val,\n             dsc_val,\n             seq_val,\n             y_val):\n        seed_torch(self.seed)\n        img_tensor = torch.tensor(img, dtype=torch.float32).to(self.device)\n        cat_tensor = torch.tensor(cat, dtype=torch.long).to(self.device)\n        num_tensor = torch.tensor(num, dtype=torch.float32).to(self.device)\n        ent_tensor = torch.tensor(ent, dtype=torch.float32).to(self.device)\n        dsc_tensor = torch.tensor(dsc, dtype=torch.float32).to(self.device)\n        seq_tensor = torch.tensor(seq, dtype=torch.long).to(self.device)\n        y_tensor = torch.tensor(y[:, np.newaxis], dtype=torch.float32).to(self.device)\n        train = PetDataset(img_tensor, \n                           cat_tensor, \n                           num_tensor,\n                           ent_tensor,\n                           dsc_tensor,\n                           seq_tensor,\n                           y_tensor)\n        train_loader = data.DataLoader(train, \n                                       batch_size=self.train_batch, shuffle=True)\n        img_eval = torch.tensor(img_val, dtype=torch.float32).to(self.device)\n        cat_eval = torch.tensor(cat_val, dtype=torch.long).to(self.device)\n        num_eval = torch.tensor(num_val, dtype=torch.float32).to(self.device)\n        ent_eval = torch.tensor(ent_val, dtype=torch.float32).to(self.device)\n        dsc_eval = torch.tensor(dsc_val, dtype=torch.float32).to(self.device)\n        seq_eval = torch.tensor(seq_val, dtype=torch.long).to(self.device)\n        y_eval = torch.tensor(y_val[:, np.newaxis], dtype=torch.float32).to(self.device)\n        eval_ = PetDataset(img_eval,\n                           cat_eval,\n                           num_eval,\n                           ent_eval,\n                           dsc_eval,\n                           seq_eval,\n                           y_eval)\n        eval_loader = data.DataLoader(eval_,\n                                      batch_size=self.val_batch, shuffle=False)\n        \n        model = self.model(**self.kwargs)\n        model = model.to(self.device)\n        optimizer = optim.Adam(model.parameters())\n        best_score = np.inf\n        mb = master_bar(range(n_epochs))\n        \n        for epoch in mb:\n            model.train()\n            avg_loss = 0.\n            for i_batch, c_batch, n_batch, e_batch, d_batch, s_batch, y_batch in progress_bar(train_loader, parent=mb):\n                y_pred = model(i_batch, c_batch, n_batch, e_batch, d_batch, s_batch)\n                loss = self.loss_fn(y_pred, y_batch)\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n                avg_loss += loss.item() / len(train_loader)\n            valid_preds, avg_val_loss = self._val(eval_loader, model)\n            print(f\"epoch {epoch+1}/{n_epochs}\")\n            print(f\"avg_loss: {avg_loss:.4f}\")\n            print(f\"avg_val_loss: {avg_val_loss:.4f}\")\n            if best_score > avg_val_loss:\n                torch.save(model.state_dict(),\n                           self.path / f\"best{self.fold_num}.pt\")\n                print(f\"Save model on epoch {epoch + 1}\")\n                best_score = avg_val_loss\n        model.load_state_dict(torch.load(self.path / f\"best{self.fold_num}.pt\"))\n        valid_preds, avg_val_loss = self._val(eval_loader, model)\n        print(f\"Validation loss: {avg_val_loss}\")\n        return valid_preds\n    \n    def _val(self, loader, model):\n        model.eval()\n        valid_preds = np.zeros(loader.dataset.cat_features.size(0))\n        avg_val_loss = 0.\n\n        for i, (i_batch, c_batch, n_batch, e_batch, d_batch, s_batch, y_batch) in enumerate(loader):\n            with torch.no_grad():\n                y_pred = model(i_batch, c_batch, n_batch, e_batch, d_batch, s_batch).detach()\n                avg_val_loss += self.loss_fn(y_pred,\n                                             y_batch).item() / len(loader)\n                valid_preds[i * self.val_batch:(i + 1) * self.val_batch] = y_pred.cpu().numpy()[:, 0]\n        return valid_preds, avg_val_loss","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Attention(nn.Module):\n    def __init__(self, feature_dims, step_dims, n_middle, n_attention,\n                 **kwargs):\n        super(Attention, self).__init__(**kwargs)\n        self.support_masking = True\n        self.feature_dims = feature_dims\n        self.step_dims = step_dims\n        self.n_middle = n_middle\n        self.n_attention = n_attention\n        self.features_dim = 0\n\n        self.lin1 = nn.Linear(feature_dims, n_middle, bias=False)\n        self.lin2 = nn.Linear(n_middle, n_attention, bias=False)\n\n    def forward(self, x, mask=None):\n        step_dims = self.step_dims\n\n        eij = self.lin1(x)\n        eij = torch.tanh(eij)\n        eij = self.lin2(eij)\n\n        a = torch.exp(eij).reshape(-1, self.n_attention, step_dims)\n\n        if mask is not None:\n            a = a * mask\n\n        a = a / torch.sum(a, 2, keepdim=True) + 1e-10\n\n        weighted_input = torch.bmm(a, x)\n        return torch.sum(weighted_input, 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"81f45cdbc8ee4bd0f11c545461b9ea090f358f8e"},"cell_type":"code","source":"class NeuralNet(nn.Module):\n    def __init__(self, \n                 emb_dims, \n                 num_dims, \n                 img_linear,\n                 ent_linear,\n                 dsc_linear,\n                 seq_linear,\n                 linear_size,\n                 embedding_matrix,\n                 hidden_size,\n                 maxlen,\n                 n_attention):\n        super(NeuralNet, self).__init__()\n        self.img_linear = img_linear\n        n_features, embed_size = embedding_matrix.shape\n        self.img_lin = nn.Linear(1024, img_linear)\n        self.img_attn = Attention(img_linear, 4, 2, 2)\n        self.ent_lin = nn.Linear(300, ent_linear)\n        self.dsc_lin = nn.Linear(300, dsc_linear)\n        self.seq_emb = nn.Embedding(n_features, embed_size)\n        self.seq_emb.weight = nn.Parameter(torch.tensor(\n            embedding_matrix, dtype=torch.float32))\n        self.seq_emb.weight.requires_grad = False\n        self.seq_emb_dropout = nn.Dropout2d(0.2)\n        self.lstm = nn.LSTM(\n            embed_size, hidden_size, bidirectional=True, batch_first=True)\n        self.attn = Attention(hidden_size * 2, maxlen, n_attention,\n                              n_attention)\n        self.seq_lin = nn.Linear(hidden_size * 2, seq_linear)\n\n        self.embeddings = nn.ModuleList(\n            [nn.Embedding(x, y) for x, y in emb_dims])\n        n_emb_out = sum([y for x, y in emb_dims])\n        self.fc1 = nn.Linear(\n            img_linear + n_emb_out + num_dims + ent_linear + dsc_linear + seq_linear, \n            linear_size)\n        self.bn1 = nn.BatchNorm1d(linear_size)\n        self.fc2 = nn.Linear(linear_size, 1)\n        self.drop = nn.Dropout(0.2)\n        self.relu = nn.ReLU()\n        self.tanh = nn.Tanh()\n\n    def forward(self, i, c, n, e, d, s):\n        imgs = torch.zeros(i.size(0), 4, self.img_linear).to(\"cuda:0\")\n        for j in range(4):\n            img_f = self.img_lin(i[:, j, :])\n            imgs[:, j, :] = img_f\n        img_feats = self.drop(self.tanh(self.img_attn(imgs)))\n        ent_feats = self.drop(self.tanh(self.ent_lin(e)))\n        dsc_feats = self.drop(self.tanh(self.dsc_lin(d)))\n        emb = [\n            emb_layer(c[:, j]) for j, emb_layer in enumerate(self.embeddings)\n        ]\n        emb = self.drop(self.tanh(torch.cat(emb, 1)))\n        \n        h_seq = self.seq_emb(s)\n        h_seq = torch.squeeze(\n            self.seq_emb_dropout(torch.unsqueeze(h_seq, 0)))\n        h_lstm, _ = self.lstm(h_seq)\n        h_attn = self.attn(h_lstm)\n        h_lin = self.tanh(self.seq_lin(h_attn))\n        data = torch.cat([img_feats, emb, n, ent_feats, dsc_feats, h_lin], 1)\n        out = self.relu(self.fc1(data))\n        # out = self.drop(out)\n        out = self.bn1(out)\n        out = self.fc2(out)\n        return out","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4ac9701aff07421363ecbbe2d9dc3720f06e1eaa"},"cell_type":"markdown","source":"## Train"},{"metadata":{"trusted":true,"_uuid":"6e889da94644054a0d19c79a2b202784cb04eb37"},"cell_type":"code","source":"emb_dims = [(2, 1), (n_breed1, 3), (n_breed2, 3), (3, 1), (n_color1, 1),\n            (n_color2, 1), (n_color3, 1), (4, 1), (3, 1), (3, 1),\n            (3, 1), (3, 1), (3, 1), (19, 1), (14, 1),(n_langs, 1)]\nnum_dims = X_test_ss.shape[1]\ntrainer = Trainer(\n    NeuralNet,\n    resc=train_resc,\n    n_splits=10,\n    train_batch=64,\n    val_batch=512,\n    seed=328,\n    kwargs={\n        \"emb_dims\": emb_dims,\n        \"num_dims\": num_dims,\n        \"img_linear\": 48,\n        \"linear_size\": 144,\n        \"ent_linear\": 10,\n        \"dsc_linear\": 10,\n        \"seq_linear\": 20,\n        \"embedding_matrix\": embedding_matrix,\n        \"hidden_size\": 64,\n        \"maxlen\": 70,\n        \"n_attention\": 20\n    })","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9686ef36b564f8f0684e62f44305bda521cfbdd9"},"cell_type":"code","source":"trainer.fit(train_img_matrix, \n            X_train_cat, \n            X_train_ss, \n            train_entity, \n            train_desc, \n            x_train_text, target, 10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"7771e8ce83030684751573b9b62617bca34ccfdd"},"cell_type":"code","source":"bin_path = trainer.path\ntest_preds = np.zeros((X_test_cat.shape[0]))\ni_tensor = torch.tensor(test_img_matrix, dtype=torch.float32).to(trainer.device)\nc_tensor = torch.tensor(X_test_cat.values, dtype=torch.long).to(trainer.device)\nn_tensor = torch.tensor(X_test_ss, dtype=torch.float32).to(trainer.device)\ne_tensor = torch.tensor(test_entity, dtype=torch.float32).to(trainer.device)\nd_tensor = torch.tensor(test_desc, dtype=torch.float32).to(trainer.device)\ns_tensor = torch.tensor(x_test_text, dtype=torch.long).to(trainer.device)\ntest_dataset = PetDataset(i_tensor, \n                          c_tensor, \n                          n_tensor,\n                          e_tensor,\n                          d_tensor,\n                          s_tensor,\n                          labels=None)\ntest_loader = data.DataLoader(test_dataset, batch_size=512, shuffle=False)\n\nfor path in bin_path.iterdir():\n    print(f\"using {str(path)}\")\n    model = NeuralNet(**trainer.kwargs)\n    model.to(\"cuda:0\")\n    model.load_state_dict(torch.load(path))\n\n    model.eval()\n    temp = np.zeros((X_test_cat.shape[0]))\n    for i, (i_batch, c_batch, n_batch, e_batch, d_batch, s_batch) in enumerate(test_loader):\n        i_batch = i_batch.to(trainer.device)\n        with torch.no_grad():\n            y_pred = model(i_batch, c_batch, n_batch, e_batch, d_batch, s_batch).detach()\n            temp[i * 512:(i + 1) * 512] = y_pred.cpu().numpy()[:, 0]\n    test_preds += temp / trainer.n_splits","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_preds.mean()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1b5f8367580c618fe393e3f7703038bd7fdb1304"},"cell_type":"markdown","source":"## Image"},{"metadata":{"trusted":true,"_uuid":"a49e0a579debd3cd7ca95793fed9ac51be81983f"},"cell_type":"code","source":"class ImageDataset(data.Dataset):\n    def __init__(self, imat):\n        self.imat = imat\n        \n    def __len__(self):\n        return len(self.imat)\n    \n    def __getitem__(self, idx):\n        image = self.imat[idx]\n        \n        return [image]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f4f183fc0120f4bc4a8fff9598d26e27ac6d5b6a"},"cell_type":"code","source":"train_i_tensor = torch.tensor(train_img_matrix, dtype=torch.float32).to(\"cuda:0\")\ntest_i_tensor = torch.tensor(test_img_matrix, dtype=torch.float32).to(\"cuda:0\")\ntrain_dataset = ImageDataset(train_i_tensor)\nbatch = 256\nn_img_dim = 48\ntrain_loader = data.DataLoader(train_dataset,\n                               batch_size=batch,\n                               shuffle=False)\nX_train_img = np.zeros((len(train_pet_ids), n_img_dim))\n\ntest_dataset = ImageDataset(test_i_tensor)\ntest_loader = data.DataLoader(test_dataset,\n                              batch_size=batch,\n                              shuffle=False)\nX_test_img = np.zeros((len(test_pet_ids), n_img_dim))\nbin_path = trainer.path\nfor path in bin_path.iterdir():\n    model = NeuralNet(**trainer.kwargs)\n    model.to(\"cuda:0\")\n    model.load_state_dict(torch.load(path))\n    model.eval()\n    temp = np.zeros((len(train_pet_ids), n_img_dim))\n    \n    for i, (i_batch, ) in tqdm(enumerate(train_loader)):\n        with torch.no_grad():\n            imgs = torch.zeros(i_batch.size(0), 4, n_img_dim).to(\"cuda:0\")\n            for j in range(4):\n                pre = model.img_lin(i_batch[:, j, :])\n                imgs[:, j, :] = pre\n            y_pred = model.img_attn(imgs).detach()\n            temp[i * batch:(i + 1) * batch, :] = y_pred.cpu().numpy()\n    X_train_img += temp / trainer.n_splits\n    \n    temp = np.zeros((len(test_pet_ids), n_img_dim))\n    for i, (i_batch, ) in tqdm(enumerate(test_loader)):\n        with torch.no_grad():\n            imgs = torch.zeros(i_batch.size(0), 4, n_img_dim).to(\"cuda:0\")\n            for j in range(4):\n                pre = model.img_lin(i_batch[:, j, :])\n                imgs[:, j, :] = pre\n            y_pred = model.img_attn(imgs).detach()\n            temp[i * batch:(i + 1) * batch, :] = y_pred.cpu().numpy()\n    X_test_img += temp / trainer.n_splits","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"398fff9d745229e06a4d4e40be9cb6a2b85e3ea5"},"cell_type":"code","source":"X_train_img.shape, X_test_img.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dea3e37fd67308876118a9f9eecafa3f4aff97c8"},"cell_type":"code","source":"train_img = pd.DataFrame(data=X_train_img, columns=[\n    f\"img{i}\" for i in range(X_train_img.shape[1])\n])\ntest_img = pd.DataFrame(data=X_test_img, columns=[\n    f\"img{i}\" for i in range(X_test_img.shape[1])\n])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a30d386881498532f29122ba3c2595bcbdb54e94"},"cell_type":"code","source":"num_columns = X_train_num.columns\nX_train_num_df = pd.DataFrame(data=X_train_ss, columns=num_columns)\nX_test_num_df = pd.DataFrame(data=X_test_ss, columns=num_columns)\n\nX_test_num_df.index = X_test_cat.index\ntest_img.index = X_test_cat.index\n\nX_train_all = pd.concat([X_train_num_df, X_train_cat, train_img], axis=1)\nX_test_all = pd.concat([X_test_num_df, X_test_cat, test_img], axis=1)\n\nprint(X_train_all.shape, X_test_all.shape)\nX_train_all.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"65ff0d0d7c69e4bf64f8288a48b8d8c0ad56a697"},"cell_type":"code","source":"\"AdoptionSpeed\" in X_train_all.columns, \"AdoptionSpeed\" in X_test_all.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8c26f564679be986e701c8db293068ab85e8b654"},"cell_type":"code","source":"X_train_all.columns.tolist() == X_test_all.columns.tolist()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"eec84836e1cbfe38066808c7a132aff19e6306ad"},"cell_type":"markdown","source":"## Category Embedding"},{"metadata":{"trusted":true,"_uuid":"5bced7bce730a041d4b9feb6b550235ade506d42"},"cell_type":"code","source":"class CategoryDataset(data.Dataset):\n    def __init__(self, category):\n        self.category = category\n        \n    def __len__(self):\n        return len(self.category)\n    \n    def __getitem__(self, idx):\n        category = self.category[idx, :]\n        return [category]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"76de7b14d557c7fece51f94d3444f3370ab027ec"},"cell_type":"code","source":"c_train = torch.tensor(X_train_cat.values, dtype=torch.long).to(\"cuda:0\")\nc_test = torch.tensor(X_test_cat.values, dtype=torch.long).to(\"cuda:0\")\ntrain_dataset = CategoryDataset(c_train)\ntest_dataset = CategoryDataset(c_test)\ntrain_loader = data.DataLoader(train_dataset, batch_size=128, shuffle=False)\ntest_loader = data.DataLoader(test_dataset, batch_size=128, shuffle=False)\n\nX_train_cat_ = np.zeros((len(train_pet_ids), 20))\nX_test_cat_ = np.zeros((len(test_pet_ids), 20))\nbin_path = trainer.path\nfor path in bin_path.iterdir():\n    model = NeuralNet(**trainer.kwargs)\n    model.to(\"cuda:0\")\n    model.load_state_dict(torch.load(path))\n    model.eval()\n    temp = np.zeros((len(train_pet_ids), 20))\n    for i, (c_batch, ) in tqdm(enumerate(train_loader)):\n        with torch.no_grad():\n            y_pred = [model.embeddings[i](c_batch[:, i]) for i in range(len(model.embeddings))]\n            y_pred = torch.cat(y_pred, 1).detach()\n            temp[i * 128:(i + 1) * 128, :] = y_pred.cpu().numpy()\n    X_train_cat_ += temp / trainer.n_splits\n    temp = np.zeros((len(test_pet_ids), 20))\n    for i, (c_batch, ) in tqdm(enumerate(test_loader)):\n        with torch.no_grad():\n            y_pred = [model.embeddings[i](c_batch[:, i]) for i in range(len(model.embeddings))]\n            y_pred = torch.cat(y_pred, 1).detach()\n            temp[i * 128:(i + 1) * 128, :] = y_pred.cpu().numpy()\n    X_test_cat_ += temp / trainer.n_splits","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"317123c7f3d065a9b88bd159456e845d5bc75d28"},"cell_type":"code","source":"X_train_cat_.shape, X_test_cat_.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3d09ea0c48c79c8b8076041b789054c6c1c9cf93"},"cell_type":"code","source":"train_emb = pd.DataFrame(data=X_train_cat_, columns=[\n    f\"emb{i}\" for i in range(X_train_cat_.shape[1])\n])\ntest_emb = pd.DataFrame(data=X_test_cat_, columns=[\n    f\"emb{i}\" for i in range(X_test_cat_.shape[1])\n])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3e22008286acb662454c4b48e9d754e5005b4f55"},"cell_type":"code","source":"test_emb.index = X_test_all.index\n\nX_train_all = pd.concat([X_train_all, train_emb], axis=1)\nX_test_all = pd.concat([X_test_all, test_emb], axis=1)\n\nprint(X_train_all.shape, X_test_all.shape)\nX_train_all.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Entity and Desc"},{"metadata":{"trusted":true},"cell_type":"code","source":"class WordVecDataset(data.Dataset):\n    def __init__(self, wv):\n        self.wv = wv\n        \n    def __len__(self):\n        return len(self.wv)\n    \n    def __getitem__(self, idx):\n        wv = self.wv[idx, :]\n        return [wv]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"e_train = torch.tensor(train_entity, dtype=torch.float32).to(\"cuda:0\")\ne_test = torch.tensor(test_entity, dtype=torch.float32).to(\"cuda:0\")\ntrain_dataset = WordVecDataset(e_train)\ntest_dataset = WordVecDataset(e_test)\ntrain_loader = data.DataLoader(train_dataset, batch_size=128, shuffle=False)\ntest_loader = data.DataLoader(test_dataset, batch_size=128, shuffle=False)\n\nX_train_ent = np.zeros((len(train_pet_ids), 10))\nX_test_ent = np.zeros((len(test_pet_ids), 10))\nbin_path = trainer.path\nfor path in bin_path.iterdir():\n    model = NeuralNet(**trainer.kwargs)\n    model.to(\"cuda:0\")\n    model.load_state_dict(torch.load(path))\n    model.eval()\n    temp = np.zeros((len(train_pet_ids), 10))\n    for i, (e_batch, ) in tqdm(enumerate(train_loader), ascii=True):\n        with torch.no_grad():\n            y_pred = model.ent_lin(e_batch).detach()\n            temp[i * 128:(i + 1) * 128, :] = y_pred.cpu().numpy()\n    X_train_ent += temp / trainer.n_splits\n    temp = np.zeros((len(test_pet_ids), 10))\n    for i, (e_batch, ) in tqdm(enumerate(test_loader), ascii=True):\n        with torch.no_grad():\n            y_pred = model.ent_lin(e_batch).detach()\n            temp[i * 128:(i + 1) * 128, :] = y_pred.cpu().numpy()\n    X_test_ent += temp / trainer.n_splits","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_ent.shape, X_test_ent.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_ent = pd.DataFrame(data=X_train_ent, columns=[\n    f\"ent{i}\" for i in range(X_train_ent.shape[1])\n])\ntest_ent = pd.DataFrame(data=X_test_ent, columns=[\n    f\"ent{i}\" for i in range(X_test_ent.shape[1])\n])\ntest_ent.index = X_test_all.index\n\nX_train_all = pd.concat([X_train_all, train_ent], axis=1)\nX_test_all = pd.concat([X_test_all, test_ent], axis=1)\n\nprint(X_train_all.shape, X_test_all.shape)\nX_train_all.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"d_train = torch.tensor(train_desc, dtype=torch.float32).to(\"cuda:0\")\nd_test = torch.tensor(test_desc, dtype=torch.float32).to(\"cuda:0\")\ntrain_dataset = WordVecDataset(d_train)\ntest_dataset = WordVecDataset(d_test)\ntrain_loader = data.DataLoader(train_dataset, batch_size=128, shuffle=False)\ntest_loader = data.DataLoader(test_dataset, batch_size=128, shuffle=False)\n\nX_train_dsc = np.zeros((len(train_pet_ids), 10))\nX_test_dsc = np.zeros((len(test_pet_ids), 10))\nbin_path = trainer.path\nfor path in bin_path.iterdir():\n    model = NeuralNet(**trainer.kwargs)\n    model.to(\"cuda:0\")\n    model.load_state_dict(torch.load(path))\n    model.eval()\n    temp = np.zeros((len(train_pet_ids), 10))\n    for i, (d_batch, ) in tqdm(enumerate(train_loader), ascii=True):\n        with torch.no_grad():\n            y_pred = model.dsc_lin(d_batch).detach()\n            temp[i * 128:(i + 1) * 128, :] = y_pred.cpu().numpy()\n    X_train_dsc += temp / trainer.n_splits\n    temp = np.zeros((len(test_pet_ids), 10))\n    for i, (d_batch, ) in tqdm(enumerate(test_loader), ascii=True):\n        with torch.no_grad():\n            y_pred = model.dsc_lin(d_batch).detach()\n            temp[i * 128:(i + 1) * 128, :] = y_pred.cpu().numpy()\n    X_test_dsc += temp / trainer.n_splits","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_dsc.shape, X_test_dsc.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_dsc = pd.DataFrame(data=X_train_dsc, columns=[\n    f\"dsc{i}\" for i in range(X_train_dsc.shape[1])\n])\ntest_dsc = pd.DataFrame(data=X_test_dsc, columns=[\n    f\"dsc{i}\" for i in range(X_test_dsc.shape[1])\n])\ntest_dsc.index = X_test_all.index\n\nX_train_all = pd.concat([X_train_all, train_dsc], axis=1)\nX_test_all = pd.concat([X_test_all, test_dsc], axis=1)\n\nprint(X_train_all.shape, X_test_all.shape)\nX_train_all.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Sequences"},{"metadata":{"trusted":true},"cell_type":"code","source":"class SeqDataset(data.Dataset):\n    def __init__(self, seq):\n        self.seq = seq\n        \n    def __len__(self):\n        return len(self.seq)\n    \n    def __getitem__(self, idx):\n        seq = self.seq[idx, :]\n        return [seq]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"s_train = torch.tensor(x_train_text, dtype=torch.long).to(\"cuda:0\")\ns_test = torch.tensor(x_test_text, dtype=torch.long).to(\"cuda:0\")\ntrain_dataset = SeqDataset(s_train)\ntest_dataset = SeqDataset(s_test)\ntrain_loader = data.DataLoader(train_dataset, batch_size=128, shuffle=False)\ntest_loader = data.DataLoader(test_dataset, batch_size=128, shuffle=False)\n\nX_train_seq = np.zeros((len(train_pet_ids), 20))\nX_test_seq = np.zeros((len(test_pet_ids), 20))\nbin_path = trainer.path\nfor path in bin_path.iterdir():\n    model = NeuralNet(**trainer.kwargs)\n    model.to(\"cuda:0\")\n    model.load_state_dict(torch.load(path))\n    model.eval()\n    temp = np.zeros((len(train_pet_ids), 20))\n    for i, (s_batch, ) in tqdm(enumerate(train_loader), ascii=True):\n        with torch.no_grad():\n            h_emb = model.seq_emb(s_batch)\n            h_emb = torch.squeeze(model.seq_emb_dropout(torch.unsqueeze(h_emb, 0)))\n            h_lstm, _ = model.lstm(h_emb)\n            h_attn = model.attn(h_lstm)\n            y_pred = model.seq_lin(h_attn).detach()\n            temp[i * 128:(i + 1) * 128, :] = y_pred.cpu().numpy()\n    X_train_seq += temp / trainer.n_splits\n    temp = np.zeros((len(test_pet_ids), 20))\n    for i, (s_batch, ) in tqdm(enumerate(test_loader), ascii=True):\n        with torch.no_grad():\n            h_emb = model.seq_emb(s_batch)\n            h_emb = torch.squeeze(model.seq_emb_dropout(torch.unsqueeze(h_emb, 0)))\n            h_lstm, _ = model.lstm(h_emb)\n            h_attn = model.attn(h_lstm)\n            y_pred = model.seq_lin(h_attn).detach()\n            temp[i * 128:(i + 1) * 128, :] = y_pred.cpu().numpy()\n    X_test_seq += temp / trainer.n_splits","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_seq.shape, X_test_seq.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_seq = pd.DataFrame(data=X_train_seq, columns=[\n    f\"seq{i}\" for i in range(X_train_seq.shape[1])\n])\ntest_seq = pd.DataFrame(data=X_test_seq, columns=[\n    f\"seq{i}\" for i in range(X_test_seq.shape[1])\n])\ntest_seq.index = X_test_all.index\n\nX_train_all = pd.concat([X_train_all, train_seq], axis=1)\nX_test_all = pd.concat([X_test_all, test_seq], axis=1)\n\nprint(X_train_all.shape, X_test_all.shape)\nX_train_all.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9c65a159205f1ae72cf2de8a9816726705350d72"},"cell_type":"markdown","source":"## XGBoost"},{"metadata":{"trusted":true,"_uuid":"c2a120d1514d355b4a9bd53221c18eb38b857bb1"},"cell_type":"code","source":"xgb_params = {\n    \"eval_metric\": \"rmse\",\n    \"seed\": 1337,\n    \"eta\": 0.0123,\n    \"subsample\": 0.8,\n    \"colsample_bytree\": 0.85,\n    \"tree_method\": \"gpu_hist\",\n    \"device\": \"gpu\",\n    \"silent\": 1\n}\n\nxgb_X = X_train_all\nxgb_y = target\nxgb_X_test = X_test_all\n\nmodel, oof_train_xgb, oof_test_xgb= run_xgb(\n    xgb_params, \n    xgb_X, \n    xgb_y, \n    xgb_X_test,\n    resc=train_resc,\n    n_splits=10,\n    num_rounds=10000)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b9f56b5ede55e2272d63f782be65140374528992"},"cell_type":"markdown","source":"## Run LGBM"},{"metadata":{"trusted":true,"_uuid":"a6101913115d0b45f6eb3539ea3fb7622038d8a3"},"cell_type":"code","source":"def run_lgb(params,\n            X,\n            y,\n            X_test,\n            resc,\n            cat_features,\n            n_splits=10,\n            early_stop=500):\n    oof_train = np.zeros((X.shape[0]))\n    oof_test = np.zeros((X_test.shape[0], n_splits))\n    fold = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=229)\n\n    for i, (trn_index, val_index) in enumerate(fold.split(X, \n                                                        y.astype(int))):\n        X_tr = X.iloc[trn_index, :]\n        X_val = X.iloc[val_index, :]\n\n        y_tr = y[trn_index]\n        y_val = y[val_index]\n        model = lgb.LGBMRegressor(**params)\n        model.fit(X_tr, \n                  y_tr, \n                  eval_set=(X_val, y_val),\n                  verbose=500,\n                  early_stopping_rounds=early_stop,\n                  categorical_feature=cat_features)\n        valid_pred = model.predict(X_val, num_iteration=model.best_iteration_)\n        test_pred = model.predict(X_test, num_iteration=model.best_iteration_)\n        oof_train[val_index] = valid_pred\n        oof_test[:, i] = test_pred\n    return model, oof_train, oof_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"41baedb9113c5e5febb62683416620e2875806b5"},"cell_type":"code","source":"lgb_params = {\n    \"boosting_type\": \"gbdt\",\n    \"num_leaves\": 146,\n    \"max_depth\": 12,\n    \"max_bin\": 32,\n    \"learning_rate\": 0.01,\n    \"n_estimators\": 10000,\n    \"subsample\": 0.9212945843023237,\n    \"subsample_freq\": 2,\n    \"colsample_bytree\": 0.6334740217238963,\n    \"reg_lambda\": 1.543309192604612,\n    \"min_child_samples\": 45,\n    \"min_child_weight\": 0.5878240657385082,\n    \"min_split_gain\": 0.004619759404679957,\n    \"n_jobs\": -1\n}\n\nmodel, oof_train, oof_test = run_lgb(\n    lgb_params, \n    xgb_X, \n    xgb_y, \n    xgb_X_test,\n    train_resc,\n    cat_features,\n    n_splits=6,\n    early_stop=500)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb_X.columns[np.argwhere(model.feature_importances_ > 2000).reshape(-1)]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a8fe014f27d97225fe8182507ea2b25a56608d2f"},"cell_type":"markdown","source":"## Post process"},{"metadata":{"trusted":true,"_uuid":"96cd36acf1527937b353850b5127e9fdb5f1672b"},"cell_type":"code","source":"def plot_pred(pred):\n    sns.distplot(pred, kde=True, hist_kws={\"range\": [0, 5]})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4bb9e7887a96f658a9d5346213bd102ee053776b"},"cell_type":"code","source":"plot_pred(oof_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7e603980305c9a07e7e8e47e63157218f270464c"},"cell_type":"code","source":"plot_pred(oof_train_xgb)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3978700644bc02faa416bec586c6c5235f5e191d"},"cell_type":"code","source":"oof_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c93732aae83759455f5dd583bd691fe1b1c55fd9"},"cell_type":"code","source":"plot_pred(oof_test.mean(axis=1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"392d689d823857cb65635a9a704958dfb08384b6"},"cell_type":"code","source":"plot_pred(oof_test_xgb.mean(1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"461512ba773c98a9e08db4847cd95cbfbab1baba"},"cell_type":"code","source":"plot_pred(trainer.train_preds * 4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"81534882627892ea4d9cfb4e4ecc3e999e0901e2"},"cell_type":"code","source":"plot_pred(test_preds * 4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d17db450bf99cfb3c0e5e0ce8af73e86a21a7cab"},"cell_type":"code","source":"lgb_xgb = 0.5 * oof_train + 0.5 * oof_train_xgb\nlgb_xgb_test = 0.5 * oof_test.mean(1) + 0.5 * oof_test_xgb.mean(1)\nplot_pred(lgb_xgb)\nplot_pred(lgb_xgb_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"61c55611dea702680998f99728ec594687f53225"},"cell_type":"code","source":"nn_preds = np.clip(trainer.train_preds, a_min=0.0, a_max=1.0)\nnn_preds_test = np.clip(test_preds, a_min=0.0, a_max=1.0)\nplot_pred(nn_preds * 4)\nplot_pred(nn_preds_test * 4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"599af7b9c36fe329f2d53a3fda11938727172a81"},"cell_type":"code","source":"lgb_xgb_nn = 0.9 * lgb_xgb + 0.1 * (nn_preds * 4)\nlgb_xgb_nn_test = 0.9 * lgb_xgb_test + 0.1 * nn_preds_test * 4","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d3e315bd1f8cacada2fbb38361f900de290d2052"},"cell_type":"code","source":"plot_pred(lgb_xgb_nn)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6554c06c2fd88e0066e9a9b9ccb873d458840135"},"cell_type":"code","source":"plot_pred(lgb_xgb_nn_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e5c7fc7a4fad7fd8fbdb8f573a6bdc84999c8867"},"cell_type":"code","source":"opt = OptimizedRounder()\nopt.fit(lgb_xgb, target, [1.6, 2.1, 2.8, 3.5])\ncoeff = opt.coefficients()\nvalid_pred = opt.predict(lgb_xgb, coeff)\nqwk = quadratic_weighted_kappa(xgb_y, valid_pred)\nprint(\"QWK = \", qwk)\ncoeffs = coeff.copy()\ntrain_predictions = opt.predict(lgb_xgb, coeffs).astype(np.int8)\nprint(f\"train_preds: {Counter(train_predictions)}\")\ntest_predictions = opt.predict(lgb_xgb_test, coeffs).astype(np.int8)\nprint(f\"test_preds: {Counter(test_predictions)}\")\nsubmission = pd.DataFrame({\"PetID\": test.PetID.values, \"AdoptionSpeed\": test_predictions})\nsubmission.to_csv(\"submission_tree.csv\", index=False)\nsubmission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b531b191b4961866fd6b86c4a2d2be2b9d65ae51"},"cell_type":"code","source":"opt = OptimizedRounder()\nopt.fit(lgb_xgb_nn, target, [1.5, 2.0, 2.5, 3.5])\ncoeff = opt.coefficients()\nvalid_pred = opt.predict(lgb_xgb_nn, coeff)\nqwk = quadratic_weighted_kappa(xgb_y, valid_pred)\nprint(\"QWK = \", qwk)\ncoeffs = coeff.copy()\ntrain_predictions = opt.predict(lgb_xgb_nn, coeffs).astype(np.int8)\nprint(f\"train_preds: {Counter(train_predictions)}\")\ntest_predictions = opt.predict(lgb_xgb_nn_test, coeffs).astype(np.int8)\nprint(f\"test_preds: {Counter(test_predictions)}\")\nsubmission = pd.DataFrame({\"PetID\": test.PetID.values, \"AdoptionSpeed\": test_predictions})\nsubmission.to_csv(\"submission.csv\", index=False)\nsubmission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0fa0faa2b48eb4a8502d34f24675fbc511fa5a67"},"cell_type":"code","source":"opt = OptimizedRounder()\nopt.fit(nn_preds * 4, target, [1.5, 2.0, 2.5, 3.5])\ncoeff = opt.coefficients()\nvalid_pred = opt.predict(nn_preds * 4, coeff)\nqwk = quadratic_weighted_kappa(target, valid_pred)\nprint(\"QWK = \", qwk)\ncoeffs = coeff.copy()\ntrain_predictions = opt.predict(nn_preds * 4, coeffs).astype(np.int8)\nprint(f\"train_preds: {Counter(train_predictions)}\")\ntest_predictions = opt.predict(nn_preds_test * 4, coeffs).astype(np.int8)\nprint(f\"test_preds: {Counter(test_predictions)}\")\nsubmission = pd.DataFrame({\"PetID\": test.PetID.values, \"AdoptionSpeed\": test_predictions})\nsubmission.to_csv(\"submission_nn.csv\", index=False)\nsubmission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b5715f11666185cc44d0425a90b6f702b430cce5"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}