{"cells":[{"cell_type":"markdown","metadata":{},"source":"# Feature importance exploration and scikit-learn baseline\nWe'll do feature importance exploration for the training data CSV with [Pandas](https://pandas.pydata.org/) and [scikit-learn pipelines](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html). The benefit of pipelines is that we **never** modify our training data in-place. Therefore, our results are independent of the order in which the cells are executed (as long as all variables, functions and imports are defined). One can also easily run parameter tuning on the preprocessing steps as well (see the [example](https://scikit-learn.org/stable/auto_examples/compose/plot_compare_reduction.html)).\n\n### TODO\n- [ ] Compute quadratic weighted kappa score in parameter tuning\n- [x] Write a transformer for picking the best features from feature importances\n- [ ] Sweep over preprocessing parameters in parameter search\n- [ ] Automatically choose the estimators chosen to the voting classifier"},{"cell_type":"markdown","metadata":{},"source":"### Define imports and load data"},{"cell_type":"code","execution_count":33,"metadata":{},"outputs":[],"source":"import os\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport lightgbm as lgb\n\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler\nfrom sklearn.ensemble import VotingClassifier, RandomForestClassifier, GradientBoostingClassifier, ExtraTreesClassifier\nfrom sklearn.metrics import make_scorer, cohen_kappa_score, accuracy_score\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV, cross_val_score\n\nfrom xgboost import XGBClassifier\n\n%matplotlib inline\nplt.rc('figure', figsize=(20.0, 10.0))"},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":"INPUT_DIR = \"../input\"\nprint(os.listdir(INPUT_DIR))"},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":"train_df = pd.read_csv(os.path.join(INPUT_DIR, 'train', 'train.csv'))\nX_test = pd.read_csv(os.path.join(INPUT_DIR, 'test', 'test.csv'))"},{"cell_type":"markdown","metadata":{},"source":"## Data description (copied from [competition description](https://www.kaggle.com/c/petfinder-adoption-prediction/data))"},{"cell_type":"markdown","metadata":{},"source":"<i>\nIn this competition you will predict the speed at which a pet is adopted, based on the petâ€™s listing on PetFinder. Sometimes a profile represents a group of pets. In this case, the speed of adoption is determined by the speed at which all of the pets are adopted. The data included text, tabular, and image data. See below for details. \nThis is a Kernels-only competition. At the end of the competition, test data will be replaced in their entirety with new data of approximately the same size, and your kernels will be rerun on the new data.\n\n### File descriptions\n- train.csv - Tabular/text data for the training set\n- test.csv - Tabular/text data for the test set\n- sample_submission.csv - A sample submission file in the correct format\n- breed_labels.csv - Contains Type, and BreedName for each BreedID. Type 1 is dog, 2 is cat.\n- color_labels.csv - Contains ColorName for each ColorID\n- state_labels.csv - Contains StateName for each StateID\n\n### Data Fields\n- PetID - Unique hash ID of pet profile\n- AdoptionSpeed - Categorical speed of adoption. Lower is faster. This is the value to predict. See below section for more info.\n- Type - Type of animal (1 = Dog, 2 = Cat)\n- Name - Name of pet (Empty if not named)\n- Age - Age of pet when listed, in months\n- Breed1 - Primary breed of pet (Refer to BreedLabels dictionary)\n- Breed2 - Secondary breed of pet, if pet is of mixed breed (Refer to BreedLabels dictionary)\n- Gender - Gender of pet (1 = Male, 2 = Female, 3 = Mixed, if profile represents group of pets)\n- Color1 - Color 1 of pet (Refer to ColorLabels dictionary)\n- Color2 - Color 2 of pet (Refer to ColorLabels dictionary)\n- Color3 - Color 3 of pet (Refer to ColorLabels dictionary)\n- MaturitySize - Size at maturity (1 = Small, 2 = Medium, 3 = Large, 4 = Extra Large, 0 = Not Specified)\n- FurLength - Fur length (1 = Short, 2 = Medium, 3 = Long, 0 = Not Specified)\n- Vaccinated - Pet has been vaccinated (1 = Yes, 2 = No, 3 = Not Sure)\n- Dewormed - Pet has been dewormed (1 = Yes, 2 = No, 3 = Not Sure)\n- Sterilized - Pet has been spayed / neutered (1 = Yes, 2 = No, 3 = Not Sure)\n- Health - Health Condition (1 = Healthy, 2 = Minor Injury, 3 = Serious Injury, 0 = Not Specified)\n- Quantity - Number of pets represented in profile\n- Fee - Adoption fee (0 = Free)\n- State - State location in Malaysia (Refer to StateLabels dictionary)\n- RescuerID - Unique hash ID of rescuer\n- VideoAmt - Total uploaded videos for this pet\n- PhotoAmt - Total uploaded photos for this pet\n- Description - Profile write-up for this pet. The primary language used is English, with some in Malay or Chinese.\n- AdoptionSpeed Contestants are required to predict this value. The value is determined by how quickly, if at all, a pet is adopted. The values are determined in the following way: \n    0 - Pet was adopted on the same day as it was listed. \n    1 - Pet was adopted between 1 and 7 days (1st week) after being listed. \n    2 - Pet was adopted between 8 and 30 days (1st month) after being listed. \n    3 - Pet was adopted between 31 and 90 days (2nd & 3rd month) after being listed. \n    4 - No adoption after 100 days of being listed. (There are no pets in this dataset that waited between 90 and 100 days).\n\n### Images\n\nFor pets that have photos, they will be named in the format of PetID-ImageNumber.jpg. Image 1 is the profile (default) photo set for the pet. For privacy purposes, faces, phone numbers and emails have been masked.\n\n### Image Metadata\nWe have run the images through Google's Vision API, providing analysis on Face Annotation, Label Annotation, Text Annotation and Image Properties. You may optionally utilize this supplementary information for your image analysis.\n\nFile name format is PetID-ImageNumber.json.\n\nSome properties will not exist in JSON file if not present, i.e. Face Annotation. Text Annotation has been simplified to just 1 entry of the entire text description (instead of the detailed JSON result broken down by individual characters and words). Phone numbers and emails are already anonymized in Text Annotation.\n\nGoogle Vision API reference: https://cloud.google.com/vision/docs/reference/rest/v1/images/annotate\n\n### Sentiment Data\nWe have run each pet profile's description through Google's Natural Language API, providing analysis on sentiment and key entities. You may optionally utilize this supplementary information for your pet description analysis. There are some descriptions that the API could not analyze. As such, there are fewer sentiment files than there are rows in the dataset.\n\nFile name format is PetID.json.\n\nGoogle Natural Language API reference: https://cloud.google.com/natural-language/docs/basics\n\nWhat will change in the 2nd stage of the competition?\nIn the second stage of the competition, we will re-run your selected Kernels. The following files will be swapped with new data:\n\ntest.zip including test.csv and sample_submission.csv\ntest_images.zip\ntest_metadata.zip\ntest_sentiment.zip\n\nIn stage 2, all data will be replaced with approximately the same amount of different data. The stage 1 test data will not be available when kernels are rerun in stage 2.\n</i>"},{"cell_type":"markdown","metadata":{},"source":"## Preprocessing"},{"cell_type":"markdown","metadata":{},"source":"### What we'll do here  for each column\n- `PetId`: Drop\n- `Type`: One-hot encode into `Type_Cat` field, drop `Type_Dog`\n- `Name`: Create a field for if name exists or not, drop *Name* column\n- `Age`: Leave as-is\n- `Breed1`: Keep `N` most frequent categories, one-hot encode rest\n- `Breed2`: Keep `N` most frequent categories, one-hot encode rest\n- `Gender`: One-hot encode\n- `Color1`, `Color2`, `Color3`: One-hot encode all of them\n- `MaturitySize`: Leave as-is\n- `FurLength`: One-hot encode (should account for zero?)\n- `Vaccinated`: One-hot encode (should account for 3?)\n- `Dewormed`: One-hot encode (should account for 3?)\n- `Sterilized`: One-hot encode (should account for 3?)\n- `Health`: One-hot encode, accounting for 0 (not specified)\n- `Quantity`: Leave as is\n- `Fee`: Leave as is\n- `State`: Keep `N` most frequent categories, one-hot encode\n- `VideoAmt`, `PhotosAmt`: Leave as is\n- `Description`: Leave as is"},{"cell_type":"markdown","metadata":{},"source":"### Quick look at the distributions"},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":"train_df.hist(figsize=(20, 10))\nplt.tight_layout()"},{"cell_type":"markdown","metadata":{},"source":"### Define transformers\nWe'll use [scikit-learn pipelines](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html) to define our data preprocessing transforms. We'll use a few custom transformers for the purpose:\n- `DataFrameColumnMapper`: Map DataFrame column to a new column (similar to `DataFrameMapper` from [sklearn-pandas](https://github.com/scikit-learn-contrib/sklearn-pandas))\n- `CategoricalTruncator`: Keep only `N` most frequent categories for a given column, replace others with \"Other\"\n- `CategoricalOneHotEncoder`: One-hot encode columns\n- `DataFrameColumnDropper`: Drop given columns\n- `ColumnByFeatureImportancePicker`: Pick `N` most important columns based on a classifier feature importance\n- `DataFrameToValuesTransformer`: Map DataFrame to NumPy array, used before predictors"},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":"class DataFrameColumnMapper(BaseEstimator, TransformerMixin):\n    \"\"\"\n    Map DataFrame column to a new column (similar to DataFrameMapper from sklearn-pandas)\n    \n    Attributes:\n        column_name (str): Column name to transform\n        mapping_func (func): Function to apply to given column values\n        new_column_name (str): Name for the new column, leave empty if replacing `column_name`\n        drop_original (bool): Drop original column if true and new_column_name != column_name\n    \"\"\"\n    def __init__(self, column_name, mapping_func, new_column_name=None, drop_original=True):\n        \"\"\"\n        \"\"\"\n        self.column_name = column_name\n        self.mapping_func = mapping_func\n        self.new_column_name = new_column_name if new_column_name is not None else self.column_name\n        self.drop_original = drop_original\n\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        transformed_column = X.transform({self.column_name: self.mapping_func})\n        Y = X.copy()\n        Y = Y.assign(**{self.new_column_name: transformed_column})\n        if self.column_name != self.new_column_name and self.drop_original:\n            Y = Y.drop(self.column_name, axis=1)\n        return Y"},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":"class CategoricalToOneHotEncoder(BaseEstimator, TransformerMixin):\n    \"\"\"\n    One-hot encode given columns.\n    \n    Attributes:\n        columns (List[str]): Columns to one-hot encode.\n        mappings_ (Dict[str, Dict]): Mapping from original column name to the one-hot-encoded column names\n    \"\"\"\n    def __init__(self, columns=None):\n        self.columns = columns\n        self.mappings_ = None\n    def fit(self, X, y=None):\n        # Pick all categorical attributes if no columns to transform were specified\n        if self.columns is None:\n            self.columns = X.select_dtypes(exclude='number')\n        \n        # Keep track of which categorical attributes are assigned to which integer. This is important \n        # when transforming the test set.\n        mappings = {}\n        \n        for col in self.columns:\n            labels, uniques = X.loc[:, col].factorize() # Assigns unique integers for all categories\n            int_and_cat = list(enumerate(uniques))\n            cat_and_int = [(x[1], x[0]) for x in int_and_cat]\n            mappings[col] = {'int_to_cat': dict(int_and_cat), 'cat_to_int': dict(cat_and_int)}\n    \n        self.mappings_ = mappings\n        return self\n\n    def transform(self, X):\n        Y = X.copy()\n        for col in self.columns:\n            transformed_col = Y.loc[:, col].transform(lambda x: self.mappings_[col]['cat_to_int'][x])\n            for key, val in self.mappings_[col]['cat_to_int'].items():\n                one_hot = (transformed_col == val) + 0 # Cast boolean to int by adding zero\n                Y = Y.assign(**{'{}_{}'.format(col, key): one_hot})\n            Y = Y.drop(col, axis=1)\n        return Y"},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":"class CategoricalTruncator(BaseEstimator, TransformerMixin):\n    \"\"\"\n    Keep only N most frequent categories for a given column, replace others with \"Other\"\n    \n    Attributes:\n        column_name (str): Column for which to truncate categories\n        n_values_to_keep (int): How many of the most frequent values to keep (1 for keeping only most frequent, etc.)\n        values_ (List[str]): List of category names to keep, others are replaced with \"Other\"\n    \"\"\"\n    def __init__(self, column_name, n_values_to_keep=5):\n        self.column_name = column_name\n        self.n_values_to_keep = n_values_to_keep\n        self.values_ = None\n    def fit(self, X, y=None):\n        # Here we must ensure that the test set is transformed similarly in the later phase and that the same values are kept\n        self.values_ = list(X[self.column_name].value_counts()[:self.n_values_to_keep].keys())\n        return self\n    def transform(self, X):\n        transform = lambda x: x if x in self.values_ else 'Other'\n        Y = X.copy()\n        y = Y.transform({self.column_name: transform})\n        return Y.assign(**{self.column_name: y})"},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":"class DataFrameColumnDropper(BaseEstimator, TransformerMixin):\n    \"\"\"\n    Drop given columns.\n    \n    Attributes:\n        column_names (List[Str]): List of columns to drop\n    \"\"\"\n    def __init__(self, column_names):\n        self.column_names = column_names\n    def fit(self, X, y=None):\n        return self\n    def transform(self, X):\n        return X.copy().drop(self.column_names, axis=1)"},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":"class ColumnByFeatureImportancePicker(BaseEstimator, TransformerMixin):\n    \"\"\"\n    Pick columns by feature importance\n    Attributes:\n        n_features (Optional[int]): How many most important features to keep, None for noop transformation\n        classifier (: Classifier, must have `feature_importances_` available after `fit` has been called\n    \"\"\"\n    def __init__(self, n_features: int = 20, classifier=RandomForestClassifier(n_estimators=100, random_state=42)):\n        self.n_features = n_features\n        self.classifier = classifier\n        self.attributes_ = None\n        \n    def fit_and_compute_importances(self, X_df, y):\n        \"\"\"\n        :return: Sorted list of tuples containing column name and its feature importance\n        \"\"\"\n        X_numeric = X_df.select_dtypes(include='number')\n        X = X_numeric.values\n        self.classifier.fit(X, y)\n        feature_importances = self.classifier.feature_importances_\n        feature_names = list(X_numeric)\n        feature_importances_with_names = [(feature_name, feature_importance) for feature_name, feature_importance in zip(feature_names, feature_importances)]\n        feature_importances_with_names.sort(key=lambda x: x[1], reverse=True)\n        return feature_importances_with_names\n        \n    def fit(self, X, y=None):\n        if self.n_features is None:\n            # Do nothing but keep the order\n            self.attributes_ = list(X)\n            return self\n        \n        assert y is not None, \"Feature importances cannot be computed without y!\"\n        feature_importances_with_names = self.fit_and_compute_importances(X, y)\n        self.attributes_ = [feature_name for feature_name, _ in feature_importances_with_names[:self.n_features]]\n        return self\n    \n    def transform(self, X):\n        return X.copy().loc[:, self.attributes_]"},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":"class DataFrameToValuesTransformer(BaseEstimator, TransformerMixin):\n    \"\"\"\n    Transform DataFrame to NumPy array.\n    \n    Attributes:\n        attributes_ (List[str]): List of DataFrame column names\n    \"\"\"\n    def __init__(self):\n        self.attributes_ = None\n        pass\n    def fit(self, X, y=None):\n        # Remember the order of attributes before converting to NumPy to ensure the columns\n        # are included in the same order when transforming validation or test dataset\n        self.attributes_ = list(X)\n        return self\n    def transform(self, X):\n        return X.loc[:, self.attributes_].values"},{"cell_type":"markdown","metadata":{},"source":"### Split training data into training and validation set\nWe split `train_df` into two sets. `X_train` is used for the cross-validation, `X_val` is used at the end of the notebook to estimate the generalization error."},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":"from sklearn.model_selection import train_test_split\n\ndef to_features_and_labels(df):\n    y = df['AdoptionSpeed'].values\n    X = df.drop('AdoptionSpeed', axis=1)\n    return X, y\n\nX_train_val, y_train_val = to_features_and_labels(train_df)\n\nX_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.20, random_state=42,\n                                                  stratify=y_train_val)\n\nprint(\"Shape of X_train:\", X_train.shape)\nprint(\"Shape of X_val:\", X_val.shape)\nprint(\"Shape of y_train:\", y_train.shape)\nprint(\"Shape of y_val:\", y_val.shape)"},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":"X_train.head()"},{"cell_type":"markdown","metadata":{},"source":"### Define preprocessing pipeline\nBuild [scikit-learn pipeline](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html) mapping `X_train` to `X_train_preprocessed`. Note that we **never** modify `X_train`: This ensures that our results are independendent of the order in which cells are executed (as long as all variables and functions are defined)."},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":"def has_field_transformer(column_name, new_column_name=None, is_missing_func=pd.notna) -> TransformerMixin:\n    return DataFrameColumnMapper(column_name=column_name,\n                                 mapping_func=lambda name: np.int(is_missing_func(name)),\n                                 drop_original=True,\n                                 new_column_name=new_column_name if new_column_name is not None else column_name)\n\ndef value_matches_transformer(column_name, new_column_name=None, matches=pd.notna) -> TransformerMixin:\n    return DataFrameColumnMapper(column_name=column_name,\n                                 mapping_func=lambda value: np.int(matches(value)),\n                                 drop_original=False,\n                                 new_column_name=new_column_name if new_column_name is not None else column_name)\n\ndef map_categories(column_name, mapping_dict) -> TransformerMixin:\n    return DataFrameColumnMapper(column_name=column_name,\n                                 mapping_func=lambda x: mapping_dict[x])\n\ndef onehot_encode(columns) -> TransformerMixin:\n    return CategoricalToOneHotEncoder(columns=columns)\n\ndef truncate_categorical(column_name, n_values_to_keep=10):\n    return CategoricalTruncator(column_name=column_name, n_values_to_keep=n_values_to_keep)\n\nONEHOT_ENCODED_COLUMNS = [\"Type\", \"Breed1\", \"Breed2\", \"Gender\", \"Color1\", \"Color2\", \"Color3\", \"Health\",\n                          \"FurLength\", \"Vaccinated\", \"Dewormed\", \"Sterilized\", \"State\", \"RescuerID\"]\n\ndef build_preprocessing_pipeline() -> Pipeline:\n     return Pipeline([\n        ('add_has_name', has_field_transformer(column_name=\"Name\", new_column_name=\"hasName\")),\n        ('add_is_free', value_matches_transformer(column_name=\"Fee\", new_column_name=\"isFree\",\n                                                  matches=lambda value: value < 1)),\n        ('map_type_to_species', map_categories(column_name=\"Type\", mapping_dict={1: 'dog', 2: 'cat'})),\n        ('map_gender_to_names', map_categories(column_name=\"Gender\", mapping_dict={1: 'male', 2: 'female', 3: 'mixed'})),\n        ('truncate_breed1', truncate_categorical(column_name=\"Breed1\", n_values_to_keep=10)),\n        ('truncate_breed2', truncate_categorical(column_name=\"Breed2\", n_values_to_keep=10)),\n        ('truncate_state', truncate_categorical(column_name=\"State\", n_values_to_keep=10)),\n        ('truncate_rescuer_id', truncate_categorical(column_name=\"RescuerID\", n_values_to_keep=10)),\n        ('onehot_encode', CategoricalToOneHotEncoder(columns=ONEHOT_ENCODED_COLUMNS)),\n        ('drop_unused_columns', DataFrameColumnDropper(\n            column_names=['PetID', 'Description', 'Type_dog'\n        ])),\n        ('pick_columns_by_importance', ColumnByFeatureImportancePicker(n_features=None))\n    ])\n\npreprocessing_pipeline = build_preprocessing_pipeline()\nX_train_preprocessed = preprocessing_pipeline.fit_transform(X_train, y_train)\nX_val_preprocessed = preprocessing_pipeline.transform(X_val)\n\nX_train_preprocessed.head(10)"},{"cell_type":"markdown","metadata":{},"source":"### Print the columns:"},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[],"source":"print(\"Number of features:\", len(list(X_train_preprocessed)))\nprint(\"\")\n\nprint(\"Numerical columns:\", list(X_train_preprocessed.select_dtypes(include=\"number\")))\nprint(\"\")\n\nprint(\"Non-numerical columns:\", list(X_train_preprocessed.select_dtypes(exclude=\"number\")))"},{"cell_type":"markdown","metadata":{},"source":"### Check that only numerical fields exist in the preprocessed DataFrame"},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[],"source":"X_train_preprocessed.info()"},{"cell_type":"markdown","metadata":{},"source":"## Run classifier"},{"cell_type":"markdown","metadata":{},"source":"First define helper factory functions for building pipelines:"},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[],"source":"def build_preparation_pipeline():\n    return Pipeline([\n        ('to_numpy', DataFrameToValuesTransformer()),\n        ('scaler', StandardScaler())\n    ])\n\ndef build_full_pipeline(classifier=None):\n    preprocessing_pipeline = build_preprocessing_pipeline()\n    preparation_pipeline = build_preparation_pipeline()\n    return Pipeline([\n        ('preprocessing', preprocessing_pipeline),\n        ('preparation', preparation_pipeline),\n        ('classifier', classifier)  # Expected to be filled by parameter search\n    ])"},{"cell_type":"markdown","metadata":{},"source":"### Analyze feature importance\nTrain [RandomForestClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) to compute ``feature_importances_``. Note that one can access the attributes of any transformer in the pipeline via `named_steps` attribute containing a dictionary of, well, named steps of the pipeline. For example, to access the column names via the `DataFrameToValuesTransformer` class and its `attributes_` attribute:\n```python\npipeline.named_steps['preparation'].named_steps['to_numpy'].attributes_\n```"},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[],"source":"def compute_feature_importances(classifier):\n    \"\"\"\n    :param classifier: Classifier to use for computing feature importances, must have `feature_importances_` attribute\n    :return: List of tuples containing column name and its feature importance\n    \"\"\"\n    pipeline = build_full_pipeline(classifier=classifier)\n    pipeline.fit(X_train, y_train)\n    assert hasattr(classifier, 'feature_importances_')\n    feature_importances = classifier.feature_importances_\n    feature_names = pipeline.named_steps['preparation'].named_steps['to_numpy'].attributes_\n    feature_importances_with_names = [(feature_name, feature_importance) for feature_name, feature_importance in zip(feature_names, feature_importances)]\n    feature_importances_with_names.sort(key=lambda x: x[1], reverse=True)\n    return feature_importances_with_names\n\nrf_classifier = RandomForestClassifier(n_estimators=100, max_depth=None)\nfeature_importances_with_names = compute_feature_importances(rf_classifier)\n\nN_MOST_IMPORTANT_TO_SHOW = 50\nprint(\"Feature importances (top {}):\".format(N_MOST_IMPORTANT_TO_SHOW))\nfor feature_name, feature_importance in feature_importances_with_names[:N_MOST_IMPORTANT_TO_SHOW]:\n    print(\"{} -> {}\".format(feature_name, feature_importance))\n    \nrf_pipeline = build_full_pipeline(classifier=rf_classifier)\ncross_val_score(rf_pipeline, X_train, y_train, cv=5, scoring=make_scorer(cohen_kappa_score))"},{"cell_type":"markdown","metadata":{},"source":"Let us check the cross validation score for the simple Random Forest classifier with `cohen_kappa_score`:"},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[],"source":"rf_classifier = RandomForestClassifier(n_estimators=100)\nrf_pipeline = build_full_pipeline(classifier=rf_classifier)\ncross_val_score(rf_pipeline, X_train, y_train, cv=5, scoring=make_scorer(cohen_kappa_score))"},{"cell_type":"markdown","metadata":{},"source":"### Investigate confusion matrix"},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[],"source":"from sklearn.model_selection import cross_val_predict\nfrom sklearn.metrics import confusion_matrix\n\n# From https://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    import itertools\n    if normalize:\n        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    # print(cm)\n\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() / 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    plt.tight_layout()\n    \n    \ny_pred = cross_val_predict(rf_pipeline, X=X_train, y=y_train, cv=5)\n\ncnf_matrix = confusion_matrix(y_true=y_train, y_pred=y_pred)\n\nplt.figure(figsize=(15, 7))\nplot_confusion_matrix(cnf_matrix, classes=range(0, 5),\n                      title='Confusion matrix, without normalization')"},{"cell_type":"markdown","metadata":{},"source":"## Parameter tuning"},{"cell_type":"markdown","metadata":{},"source":"The performance on the test set is clearly quite bad. Let us do some randomized search to see how much we can improve.\n\nFirst define helper functions:"},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[],"source":"def build_search(pipeline, param_distributions, n_iter=10):\n    return RandomizedSearchCV(pipeline, param_distributions=param_distributions, \n                              cv=5, return_train_score=True, refit='cohen_kappa',\n                              n_iter=n_iter,\n                              scoring={\n                                    'accuracy': make_scorer(accuracy_score),\n                                    'cohen_kappa': make_scorer(cohen_kappa_score)\n                               },\n                              verbose=1, random_state=42)\n\ndef pretty_cv_results(cv_results, \n                      sort_by='rank_test_cohen_kappa',\n                      sort_ascending=True,\n                      n_rows=20):\n    df = pd.DataFrame(cv_results)\n    cols_of_interest = [key for key in df.keys() if key.startswith('param_') \n                        or key.startswith('mean_train') \n                        or key.startswith('mean_test_')\n                        or key.startswith('rank')]\n    return df.loc[:, cols_of_interest].sort_values(by=sort_by, ascending=sort_ascending).head(n_rows)\n\ndef run_search(search):\n    search.fit(X_train, y_train)\n    print('Best score is:', search.best_score_)\n    return pretty_cv_results(search.cv_results_)"},{"cell_type":"markdown","metadata":{},"source":"Let us first check the effect of the number of features on the performance:"},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"param_distributions = {\n        'preprocessing__pick_columns_by_importance__n_features': [None, 50],\n        'classifier': [RandomForestClassifier(n_estimators=250, random_state=42, max_depth=10)],\n        'classifier__max_depth': [10, None]\n    }\n\nrf_feature_search = build_search(build_full_pipeline(), param_distributions=param_distributions, n_iter=4)\nrf_feature_cv_results = run_search(search=rf_feature_search)\nrf_feature_cv_results"},{"cell_type":"markdown","metadata":{},"source":"As it's not obvious from this quick parameter search whether the number of features should be restricted, we'll keep it as a search parameter below.\n\nLet's search for the best `RandomForestClassifier`:"},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[],"source":"param_distributions = {\n        'preprocessing__pick_columns_by_importance__n_features': [None],\n        'classifier': [RandomForestClassifier(n_estimators=500, random_state=42)],\n        'classifier__n_estimators': [500],\n        'classifier__max_features': ['auto', 'log2'],\n        'classifier__max_depth': [None, 10],\n        'classifier__bootstrap': [False, True],\n        'classifier__min_samples_leaf': [1, 5, 10],\n        'classifier__min_samples_split': [2, 5, 10],\n        'classifier__criterion': ['gini', 'entropy'],\n    }\n\nrf_search = build_search(build_full_pipeline(), param_distributions=param_distributions, n_iter=50)\nrf_cv_results = run_search(search=rf_search)\nrf_cv_results"},{"cell_type":"markdown","metadata":{},"source":"## [Extra-trees classifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesClassifier.html)"},{"cell_type":"code","execution_count":24,"metadata":{},"outputs":[],"source":"param_distributions = {\n        'preprocessing__pick_columns_by_importance__n_features': [None],\n        'classifier': [ExtraTreesClassifier(n_estimators=500, random_state=42)],\n        'classifier__n_estimators': [500],\n        'classifier__max_features': ['auto', 'log2'],\n        'classifier__max_depth': [None],\n        # 'classifier__bootstrap': [False, True],\n        # 'classifier__min_samples_leaf': [0.001, 0.01, 0.05, 1, 5, 10],\n        'classifier__min_samples_split': [2, 5, 10],\n        'classifier__criterion': ['gini', 'entropy'],\n    }\n\net_search = build_search(build_full_pipeline(), param_distributions=param_distributions, n_iter=50)\net_cv_results = run_search(search=et_search)\net_cv_results"},{"cell_type":"markdown","metadata":{},"source":"## [Logistic regression classifier](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)"},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"from sklearn.linear_model import LogisticRegression\n\nparam_distributions = {\n        'preprocessing__pick_columns_by_importance__n_features': [None, 50],\n        'classifier': [LogisticRegression(solver='lbfgs', random_state=42)],\n        'classifier__multi_class': ['ovr', 'multinomial'],\n        'classifier__C': np.logspace(-3, 0, 4),\n    }\n\nlogistic_search = build_search(build_full_pipeline(), param_distributions=param_distributions, n_iter=20)\nlogistic_cv_results = run_search(search=logistic_search)\nlogistic_cv_results"},{"cell_type":"markdown","metadata":{},"source":"## [Multilayer perceptron](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html#sklearn.neural_network.MLPClassifier)"},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"from sklearn.neural_network import MLPClassifier\n\nparam_distributions = {\n        'preprocessing__pick_columns_by_importance__n_features': [None, 50],\n        'preparation__scaler': [MinMaxScaler()],\n        'classifier': [MLPClassifier(hidden_layer_sizes=(100, ), random_state=42)],\n        'classifier__hidden_layer_sizes': [[10], [10, 10,], [10, 10, 10]],\n        'classifier__alpha': np.logspace(-4, -2, 3),\n        'classifier__solver': ['adam'],\n        'classifier__tol': np.logspace(-4, -2, 3),\n        'classifier__learning_rate_init': np.logspace(-3, -1, 3),\n        'classifier__activation': ['relu', 'tanh'],\n    }\n\nmlp_search = build_search(build_full_pipeline(), param_distributions=param_distributions, n_iter=5)\nmlp_cv_results = run_search(search=mlp_search)\nmlp_cv_results"},{"cell_type":"markdown","metadata":{},"source":"## [Gaussian process classifier](https://scikit-learn.org/stable/modules/generated/sklearn.gaussian_process.GaussianProcessClassifier.html)\nThis turned out to be too heavy so it's skipped."},{"cell_type":"markdown","metadata":{},"source":"## [SVC](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html)\nDoes not scale well to large datasets so skipped."},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"\"\"\"\nfrom sklearn.svm import SVC\nparam_distributions = { \n        'preprocessing__pick_columns_by_importance__n_features': [None],\n        'classifier': [ SVC(random_state=42, probability=True) ], # Probability to use soft voting later\n        'classifier__C': np.logspace(-1, 1, 3),\n        'classifier__kernel': ['linear', 'poly', 'rbf'],\n        'classifier__gamma': ['auto', 'scale']\n    }\n\n\nsvm_search = build_search(pipeline=build_full_pipeline(), param_distributions=param_distributions, n_iter=1)\nsvm_cv_results = run_search(search=svm_search)\nsvm_cv_results\n\"\"\""},{"cell_type":"markdown","metadata":{},"source":"### [Gradient boosting](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html)"},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"from sklearn.ensemble import GradientBoostingClassifier\n\nparam_distributions = { \n        'preprocessing__pick_columns_by_importance__n_features': [None],\n        'classifier': [ GradientBoostingClassifier(random_state=42) ],\n        'classifier__loss': ['deviance'],\n        'classifier__n_estimators': [100, 300],\n        'classifier__max_features': ['log2', None],\n        'classifier__max_depth': [5, 10],\n        'classifier__min_samples_leaf': [1, 5, 10],\n        'classifier__min_samples_split': [2, 5, 10],\n        'classifier__learning_rate': [0.1, 0.2],\n        'classifier__subsample': [0.75, 0.90, 1.0]\n    }\n\ngb_search = build_search(pipeline=build_full_pipeline(), param_distributions=param_distributions, n_iter=20)\ngb_cv_results = run_search(search=gb_search)\ngb_cv_results"},{"cell_type":"markdown","metadata":{},"source":"### [LightGBM](https://lightgbm.readthedocs.io/en/latest/Python-API.html#scikit-learn-api)\n"},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"param_distributions = {\n    'preprocessing__pick_columns_by_importance__n_features': [None],\n    'classifier': [ lgb.sklearn. LGBMClassifier(random_state=42, objective='multiclass') ],\n    'classifier__boosting_type': ['gbdt', 'dart'],\n    'classifier__num_leaves': [20, 31, 50],\n    'classifier__max_depth': [-1],\n    'classifier__learning_rate': [0.1, 0.2],\n    'classifier__n_estimators': [100, 300],\n    'classifier__subsample': [1.0, 0.9],\n    'classifier__reg_alpha': [0.0, *np.logspace(-3, -2, 2)],\n    'classifier__reg_lambda': [0.0],\n}\n\n# cross_val_score(lgbm_pipeline, X_train, y_train, cv=5, scoring=make_scorer(cohen_kappa_score))\n# compute_feature_importances(classifier=lgbm_classifier)\n\nlgbm_search = build_search(pipeline=build_full_pipeline(), param_distributions=param_distributions, n_iter=20)\nlgbm_cv_results = run_search(search=lgbm_search)\nlgbm_cv_results"},{"cell_type":"markdown","metadata":{},"source":"## [XGBoost](https://xgboost.readthedocs.io/en/latest/python/python_api.html#module-xgboost.sklearn)"},{"cell_type":"code","execution_count":34,"metadata":{},"outputs":[],"source":"param_distributions = {\n    'preprocessing__pick_columns_by_importance__n_features': [None],\n    'classifier': [ XGBClassifier(random_state=42) ],\n    'classifier__max_depth': [3, 5],\n    'classifier__learning_rate': [0.1, 0.2],\n    'classifier__n_estimators': [100, 200],\n    'classifier__reg_alpha': [0, 1e-3],\n    'classifier__lambda': [1],\n}\n\n# cross_val_score(lgbm_pipeline, X_train, y_train, cv=5, scoring=make_scorer(cohen_kappa_score))\n# compute_feature_importances(classifier=lgbm_classifier)\n\nxgb_search = build_search(pipeline=build_full_pipeline(), param_distributions=param_distributions, n_iter=20)\nxgb_cv_results = run_search(search=xgb_search)\nxgb_cv_results"},{"cell_type":"markdown","metadata":{},"source":"## Submission"},{"cell_type":"markdown","metadata":{},"source":"Create a stacking ensemble from the best estimators."},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[],"source":"def cross_val_predictions(classifiers, X, y):\n    \"\"\"\n    Stack all cross validation prediction probabilities from classifiers into a single matrix.\n    Predictions are computed using `cross_val_predict`, ensuring that predictions are clean.\n    \"\"\"\n    return np.hstack([cross_val_predict(classifier, X, y, cv=5, method='predict_proba') for classifier in classifiers])\n \ndef first_level_predictions(classifiers, X):\n    \"\"\"\n    Stack all prediction probabilities from classifier probability predictions.\n    \"\"\"\n    return np.hstack([classifier.predict_proba(X) for classifier in classifiers])\n\nbest_estimators = [\n    rf_search.best_estimator_,\n    et_search.best_estimator_,\n    gb_search.best_estimator_,\n    lgbm_search.best_estimator_,\n    xgb_search.best_estimator_\n]\n\nX_train_second_level = cross_val_predictions(classifiers=best_estimators, X=X_train, y=y_train)\n\nstacking_classifier = GradientBoostingClassifier(random_state=42)\n\nstacking_classifier.fit(X_train_second_level, y_train)"},{"cell_type":"markdown","metadata":{},"source":"Check the performance on the hold-out set `(X_val, y_val)` when trained on `(X_train, y_train)`:"},{"cell_type":"code","execution_count":26,"metadata":{},"outputs":[],"source":"for estimator in best_estimators:\n    estimator.fit(X_train, y_train)\n\nX_val_second_level = first_level_predictions(best_estimators, X_val)\ny_val_pred = stacking_classifier.predict(X_val_second_level)\n\nprint(\"Performance of stacking classifier on the hold-out set:\", cohen_kappa_score(y_val, y_val_pred))"},{"cell_type":"markdown","metadata":{},"source":"Train stacking classifier with all data available:"},{"cell_type":"code","execution_count":29,"metadata":{},"outputs":[],"source":"X_train_val_second_level = cross_val_predictions(best_estimators, X=X_train_val, y=y_train_val)\nstacking_classifier.fit(X_train_val_second_level, y_train_val)\n\nfor estimator in best_estimators:\n    estimator.fit(X_train_val, y_train_val)\n\nX_test_second_level = first_level_predictions(best_estimators, X=X_test)"},{"cell_type":"markdown","metadata":{},"source":"Evaluate predictions on the test set:"},{"cell_type":"code","execution_count":30,"metadata":{},"outputs":[],"source":"def get_predictions(estimator, X):\n    predictions = estimator.predict(X)\n    indices = X_test.loc[:, 'PetID']\n    as_dict = [{'PetID': index, 'AdoptionSpeed': prediction} for index, prediction in zip(indices, predictions)]\n    df = pd.DataFrame.from_dict(as_dict)\n    df = df.reindex(['PetID', 'AdoptionSpeed'], axis=1)\n    return df\n\npredictions = get_predictions(stacking_classifier, X=X_test_second_level)"},{"cell_type":"markdown","metadata":{},"source":"Write `submission.csv`:"},{"cell_type":"code","execution_count":31,"metadata":{},"outputs":[],"source":"def write_submission(predictions):\n    submission_folder = '.'\n    dest_file = os.path.join(submission_folder, 'submission.csv')\n    predictions.to_csv(dest_file, index=False)\n    print(\"Wrote to {}\".format(dest_file))\n    \nwrite_submission(predictions)"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.5"}},"nbformat":4,"nbformat_minor":2}