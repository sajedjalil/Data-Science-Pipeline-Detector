{"cells":[{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"from keras import backend as K\nK.tensorflow_backend._get_available_gpus()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"import gc\nfrom collections import Counter\nimport glob\nimport os\nimport json\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pprint\nimport warnings\n\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.decomposition import TruncatedSVD\nfrom joblib import Parallel, delayed\nfrom tqdm import tqdm, tqdm_notebook\n\nimport scipy as sp \nfrom functools import partial\nfrom math import sqrt\n\nfrom sklearn.metrics import cohen_kappa_score\n\n%matplotlib inline\n\nnp.random.seed(seed=1337)\nwarnings.filterwarnings('ignore')\n\nsplit_char = '/'\n\npd.set_option('display.max_columns', 1000)\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import PolynomialFeatures\n\nimport string\n\nimport spacy","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.set_option('display.max_columns', 50)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Import Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/petfinder-adoption-prediction/train/train.csv')\ntest = pd.read_csv('../input/petfinder-adoption-prediction/test/test.csv')\nsample_submission = pd.read_csv('../input/petfinder-adoption-prediction/test/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"labels_breed = pd.read_csv('../input/petfinder-adoption-prediction/breed_labels.csv')\nlabels_color = pd.read_csv('../input/petfinder-adoption-prediction/color_labels.csv')\nlabels_state = pd.read_csv('../input/petfinder-adoption-prediction/state_labels.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_all = pd.concat([train, test], ignore_index=True, sort=False)\n\ndf_all.loc[np.isfinite(df_all.AdoptionSpeed), 'is_train'] = 1\ndf_all.loc[~np.isfinite(df_all.AdoptionSpeed), 'is_train'] = 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"all_columns = []","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" # MetaData&Sentiment"},{"metadata":{"trusted":false},"cell_type":"code","source":"train_image_files = sorted(glob.glob('../input/petfinder-adoption-prediction/train_images/*.jpg'))\ntrain_metadata_files = sorted(glob.glob('../input/petfinder-adoption-prediction/train_metadata/*.json'))\ntrain_sentiment_files = sorted(glob.glob('../input/petfinder-adoption-prediction/train_sentiment/*.json'))\n\n\ntest_image_files = sorted(glob.glob('../input/petfinder-adoption-prediction/test_images/*.jpg'))\ntest_metadata_files = sorted(glob.glob('../input/petfinder-adoption-prediction/test_metadata/*.json'))\ntest_sentiment_files = sorted(glob.glob('../input/petfinder-adoption-prediction/test_sentiment/*.json'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Metadata:\ntrain_df_ids = train[['PetID']]\ntrain_df_metadata = pd.DataFrame(train_metadata_files)\ntrain_df_metadata.columns = ['metadata_filename']\ntrain_metadata_pets = train_df_metadata['metadata_filename'].apply(lambda x: x.split(split_char)[-1].split('-')[0])\ntrain_df_metadata = train_df_metadata.assign(PetID=train_metadata_pets)\nprint(len(train_metadata_pets.unique()))\n\npets_with_metadatas = len(np.intersect1d(train_metadata_pets.unique(), train_df_ids['PetID'].unique()))\nprint(f'fraction of pets with metadata: {pets_with_metadatas / train_df_ids.shape[0]:.3f}')\n\n# Sentiment:\ntrain_df_ids = train[['PetID']]\ntrain_df_sentiment = pd.DataFrame(train_sentiment_files)\ntrain_df_sentiment.columns = ['sentiment_filename']\ntrain_sentiment_pets = train_df_sentiment['sentiment_filename'].apply(lambda x: x.split(split_char)[-1].split('.')[0])\ntrain_df_sentiment = train_df_sentiment.assign(PetID=train_sentiment_pets)\nprint(len(train_sentiment_pets.unique()))\n\npets_with_sentiments = len(np.intersect1d(train_sentiment_pets.unique(), train_df_ids['PetID'].unique()))\nprint(f'fraction of pets with sentiment: {pets_with_sentiments / train_df_ids.shape[0]:.3f}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Images:\ntest_df_ids = test[['PetID']]\nprint(test_df_ids.shape)\n\n# Metadata:\ntest_df_metadata = pd.DataFrame(test_metadata_files)\ntest_df_metadata.columns = ['metadata_filename']\ntest_metadata_pets = test_df_metadata['metadata_filename'].apply(lambda x: x.split(split_char)[-1].split('-')[0])\ntest_df_metadata = test_df_metadata.assign(PetID=test_metadata_pets)\nprint(len(test_metadata_pets.unique()))\n\npets_with_metadatas = len(np.intersect1d(test_metadata_pets.unique(), test_df_ids['PetID'].unique()))\nprint(f'fraction of pets with metadata: {pets_with_metadatas / test_df_ids.shape[0]:.3f}')\n\n# Sentiment:\ntest_df_sentiment = pd.DataFrame(test_sentiment_files)\ntest_df_sentiment.columns = ['sentiment_filename']\ntest_sentiment_pets = test_df_sentiment['sentiment_filename'].apply(lambda x: x.split(split_char)[-1].split('.')[0])\ntest_df_sentiment = test_df_sentiment.assign(PetID=test_sentiment_pets)\nprint(len(test_sentiment_pets.unique()))\n\npets_with_sentiments = len(np.intersect1d(test_sentiment_pets.unique(), test_df_ids['PetID'].unique()))\nprint(f'fraction of pets with sentiment: {pets_with_sentiments / test_df_ids.shape[0]:.3f}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"class PetFinderParser(object):\n    \n    def __init__(self, debug=False):\n        \n        self.debug = debug\n        self.sentence_sep = ' '\n        \n        self.extract_sentiment_text = False\n    \n    def open_json_file(self, filename):\n        with open(filename, 'r', encoding='utf-8') as f:\n            json_file = json.load(f)\n        return json_file\n        \n    def parse_sentiment_file(self, file):\n        \"\"\"\n        Parse sentiment file. Output DF with sentiment features.\n        \"\"\"\n        \n        file_sentiment = file['documentSentiment']\n        file_entities = [x['name'] for x in file['entities']]\n        file_entities = self.sentence_sep.join(file_entities)\n        \n        file_sentences_sentiment = [x['sentiment'] for x in file['sentences']]\n        \n        file_sentences_sentiment = pd.DataFrame.from_dict(\n            file_sentences_sentiment, orient='columns')\n        file_sentences_sentiment_df = pd.DataFrame(\n            {\n                'magnitude_sum': file_sentences_sentiment['magnitude'].sum(axis=0),\n                'score_sum': file_sentences_sentiment['score'].sum(axis=0),\n                'magnitude_mean': file_sentences_sentiment['magnitude'].mean(axis=0),\n                'score_mean': file_sentences_sentiment['score'].mean(axis=0),\n                'magnitude_var': file_sentences_sentiment['magnitude'].var(axis=0),\n                'score_var': file_sentences_sentiment['score'].var(axis=0),\n            }, index=[0]\n        )\n        \n        df_sentiment = pd.DataFrame.from_dict(file_sentiment, orient='index').T\n        df_sentiment = pd.concat([df_sentiment, file_sentences_sentiment_df], axis=1)\n            \n        df_sentiment['entities'] = file_entities\n        df_sentiment = df_sentiment.add_prefix('sentiment_')\n        \n        return df_sentiment\n    \n    def parse_metadata_file(self, file):\n        \"\"\"\n        Parse metadata file. Output DF with metadata features.\n        \"\"\"\n        \n        file_keys = list(file.keys())\n        \n        if 'labelAnnotations' in file_keys:\n            file_annots = file['labelAnnotations']\n            file_top_score = np.asarray([x['score'] for x in file_annots]).mean()\n            file_top_desc = [x['description'] for x in file_annots]\n        else:\n            file_top_score = np.nan\n            file_top_desc = ['']\n        \n        file_colors = file['imagePropertiesAnnotation']['dominantColors']['colors']\n        file_crops = file['cropHintsAnnotation']['cropHints']\n\n        file_color_score = np.asarray([x['score'] for x in file_colors]).mean()\n        file_color_pixelfrac = np.asarray([x['pixelFraction'] for x in file_colors]).mean()\n\n        file_crop_conf = np.asarray([x['confidence'] for x in file_crops]).mean()\n        \n        if 'importanceFraction' in file_crops[0].keys():\n            file_crop_importance = np.asarray([x['importanceFraction'] for x in file_crops]).mean()\n        else:\n            file_crop_importance = np.nan\n\n        df_metadata = {\n            'annots_score': file_top_score,\n            'color_score': file_color_score,\n            'color_pixelfrac': file_color_pixelfrac,\n            'crop_conf': file_crop_conf,\n            'crop_importance': file_crop_importance,\n            'annots_top_desc': self.sentence_sep.join(file_top_desc)\n        }\n        \n        df_metadata = pd.DataFrame.from_dict(df_metadata, orient='index').T\n        df_metadata = df_metadata.add_prefix('metadata_')\n        \n        return df_metadata\n    \n\ndef extract_additional_features(pet_id, mode='train'):\n    \n    sentiment_filename = f'../input/petfinder-adoption-prediction/{mode}_sentiment/{pet_id}.json'\n    try:\n        sentiment_file = pet_parser.open_json_file(sentiment_filename)\n        df_sentiment = pet_parser.parse_sentiment_file(sentiment_file)\n        df_sentiment['PetID'] = pet_id\n    except FileNotFoundError:\n        df_sentiment = []\n\n    dfs_metadata = []\n    metadata_filenames = sorted(glob.glob(f'../input/petfinder-adoption-prediction/{mode}_metadata/{pet_id}*.json'))\n    if len(metadata_filenames) > 0:\n        for f in metadata_filenames:\n            metadata_file = pet_parser.open_json_file(f)\n            df_metadata = pet_parser.parse_metadata_file(metadata_file)\n            df_metadata['PetID'] = pet_id\n            dfs_metadata.append(df_metadata)\n        dfs_metadata = pd.concat(dfs_metadata, ignore_index=True, sort=False)\n    dfs = [df_sentiment, dfs_metadata]\n    \n    return dfs\n\n\npet_parser = PetFinderParser()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"debug = False\ntrain_pet_ids = train.PetID.unique()\ntest_pet_ids = test.PetID.unique()\n\nif debug:\n    train_pet_ids = train_pet_ids[:1000]\n    test_pet_ids = test_pet_ids[:500]\n\n\ndfs_train = Parallel(n_jobs=-1, verbose=1)(\n    delayed(extract_additional_features)(i, mode='train') for i in train_pet_ids)\n\ntrain_dfs_sentiment = [x[0] for x in dfs_train if isinstance(x[0], pd.DataFrame)]\ntrain_dfs_metadata = [x[1] for x in dfs_train if isinstance(x[1], pd.DataFrame)]\n\ntrain_dfs_sentiment = pd.concat(train_dfs_sentiment, ignore_index=True, sort=False)\ntrain_dfs_metadata = pd.concat(train_dfs_metadata, ignore_index=True, sort=False)\n\nprint(train_dfs_sentiment.shape, train_dfs_metadata.shape)\n\n\ndfs_test = Parallel(n_jobs=-1, verbose=1)(\n    delayed(extract_additional_features)(i, mode='test') for i in test_pet_ids)\n\ntest_dfs_sentiment = [x[0] for x in dfs_test if isinstance(x[0], pd.DataFrame)]\ntest_dfs_metadata = [x[1] for x in dfs_test if isinstance(x[1], pd.DataFrame)]\n\ntest_dfs_sentiment = pd.concat(test_dfs_sentiment, ignore_index=True, sort=False)\ntest_dfs_metadata = pd.concat(test_dfs_metadata, ignore_index=True, sort=False)\n\nprint(test_dfs_sentiment.shape, test_dfs_metadata.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Aggregations"},{"metadata":{"trusted":false},"cell_type":"code","source":"aggregates = ['sum', 'mean', 'max', 'min', 'std'] # enis: I added 'max', 'min', 'std'\nsent_agg = ['sum']\n\n\n# Train\ntrain_metadata_desc = train_dfs_metadata.groupby(['PetID'])['metadata_annots_top_desc'].unique()\ntrain_metadata_desc = train_metadata_desc.reset_index()\ntrain_metadata_desc[\n    'metadata_annots_top_desc'] = train_metadata_desc[\n    'metadata_annots_top_desc'].apply(lambda x: ' '.join(x))\n\nprefix = 'metadata'\ntrain_metadata_gr = train_dfs_metadata.drop(['metadata_annots_top_desc'], axis=1)\nfor i in train_metadata_gr.columns:\n    if 'PetID' not in i:\n        train_metadata_gr[i] = train_metadata_gr[i].astype(float)\ntrain_metadata_gr = train_metadata_gr.groupby(['PetID']).agg(aggregates)\ntrain_metadata_gr.columns = pd.Index([f'{c[0]}_{c[1].upper()}' for c in train_metadata_gr.columns.tolist()])\ntrain_metadata_gr = train_metadata_gr.reset_index()\n\n\ntrain_sentiment_desc = train_dfs_sentiment.groupby(['PetID'])['sentiment_entities'].unique()\ntrain_sentiment_desc = train_sentiment_desc.reset_index()\ntrain_sentiment_desc[\n    'sentiment_entities'] = train_sentiment_desc[\n    'sentiment_entities'].apply(lambda x: ' '.join(x))\n\nprefix = 'sentiment'\ntrain_sentiment_gr = train_dfs_sentiment.drop(['sentiment_entities'], axis=1)\nfor i in train_sentiment_gr.columns:\n    if 'PetID' not in i:\n        train_sentiment_gr[i] = train_sentiment_gr[i].astype(float)\ntrain_sentiment_gr = train_sentiment_gr.groupby(['PetID']).agg(sent_agg)\ntrain_sentiment_gr.columns = pd.Index([f'{c[0]}' for c in train_sentiment_gr.columns.tolist()])\ntrain_sentiment_gr = train_sentiment_gr.reset_index()\n\n\n# Test\ntest_metadata_desc = test_dfs_metadata.groupby(['PetID'])['metadata_annots_top_desc'].unique()\ntest_metadata_desc = test_metadata_desc.reset_index()\ntest_metadata_desc[\n    'metadata_annots_top_desc'] = test_metadata_desc[\n    'metadata_annots_top_desc'].apply(lambda x: ' '.join(x))\n\nprefix = 'metadata'\ntest_metadata_gr = test_dfs_metadata.drop(['metadata_annots_top_desc'], axis=1)\nfor i in test_metadata_gr.columns:\n    if 'PetID' not in i:\n        test_metadata_gr[i] = test_metadata_gr[i].astype(float)\ntest_metadata_gr = test_metadata_gr.groupby(['PetID']).agg(aggregates)\ntest_metadata_gr.columns = pd.Index([f'{c[0]}_{c[1].upper()}' for c in test_metadata_gr.columns.tolist()])\ntest_metadata_gr = test_metadata_gr.reset_index()\n\n\ntest_sentiment_desc = test_dfs_sentiment.groupby(['PetID'])['sentiment_entities'].unique()\ntest_sentiment_desc = test_sentiment_desc.reset_index()\ntest_sentiment_desc[\n    'sentiment_entities'] = test_sentiment_desc[\n    'sentiment_entities'].apply(lambda x: ' '.join(x))\n\nprefix = 'sentiment'\ntest_sentiment_gr = test_dfs_sentiment.drop(['sentiment_entities'], axis=1)\nfor i in test_sentiment_gr.columns:\n    if 'PetID' not in i:\n        test_sentiment_gr[i] = test_sentiment_gr[i].astype(float)\ntest_sentiment_gr = test_sentiment_gr.groupby(['PetID']).agg(sent_agg)\ntest_sentiment_gr.columns = pd.Index([f'{c[0]}' for c in test_sentiment_gr.columns.tolist()])\ntest_sentiment_gr = test_sentiment_gr.reset_index()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"all_sentiment_gr = pd.concat([train_sentiment_gr, test_sentiment_gr], ignore_index=True, sort=False)\nall_metadata_gr = pd.concat([train_metadata_gr, test_metadata_gr], ignore_index=True, sort=False)\nall_metadata_desc = pd.concat([train_metadata_desc, test_metadata_desc], ignore_index=True, sort=False)\nall_sentiment_desc = pd.concat([train_sentiment_desc, test_sentiment_desc], ignore_index=True, sort=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# merges:\ndf_all = df_all.merge(\n    all_sentiment_gr, how='left', on='PetID')\ndf_all = df_all.merge(\n    all_metadata_gr, how='left', on='PetID')\ndf_all = df_all.merge(\n    all_metadata_desc, how='left', on='PetID')\ndf_all = df_all.merge(\n    all_sentiment_desc, how='left', on='PetID')\n\n\nall_columns.extend(list(all_sentiment_gr.columns[1:]) +list(all_metadata_gr.columns[1:]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"text_columns = ['Description', 'metadata_annots_top_desc', 'sentiment_entities']\n\nX_text = df_all[text_columns].copy()\n\nfor i in X_text.columns:\n    X_text.loc[:, i] = X_text.loc[:, i].fillna('none')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"n_components = 16\ntext_features = []\n\n# Generate text features:\nfor i in X_text.columns:\n    \n    # Initialize decomposition methods:\n    print(f'generating features from: {i}')\n    tfv = TfidfVectorizer(min_df=2,  max_features=None,\n                          strip_accents='unicode', analyzer='word', token_pattern=r'(?u)\\b\\w+\\b',\n                          ngram_range=(1, 3), use_idf=1, smooth_idf=1, sublinear_tf=1)\n    svd_ = TruncatedSVD(\n        n_components=n_components, random_state=1337)\n    \n    tfidf_col = tfv.fit_transform(X_text.loc[:, i].values)\n    \n    svd_col = svd_.fit_transform(tfidf_col)\n    svd_col = pd.DataFrame(svd_col)\n    svd_col = svd_col.add_prefix('TFIDF_{}_'.format(i))\n    \n    text_features.append(svd_col)\n    \ntext_features = pd.concat(text_features, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df_all = pd.concat([df_all, text_features.reset_index(drop=True)], axis=1)\n\nall_columns.extend(text_features.columns)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Image Aggregations"},{"metadata":{"trusted":false},"cell_type":"code","source":"train_image_nums = Counter([i.split('/')[-1].split('-')[0] for i in train_image_files])\ntest_image_nums = Counter([i.split('/')[-1].split('-')[0] for i in test_image_files])\n\ntrain['image_num'] = train.PetID.apply(lambda x: train_image_nums[x] if x in train_image_nums else 0)\ntest['image_num'] = test.PetID.apply(lambda x: test_image_nums[x] if x in test_image_nums else 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"from PIL import Image\ntrain_df_ids = train[['PetID']]\ntest_df_ids = test[['PetID']]\n\ntrain_df_imgs = pd.DataFrame(train_image_files)\ntrain_df_imgs.columns = ['image_filename']\ntrain_imgs_pets = train_df_imgs['image_filename'].apply(lambda x: x.split(split_char)[-1].split('-')[0])\n\ntest_df_imgs = pd.DataFrame(test_image_files)\ntest_df_imgs.columns = ['image_filename']\ntest_imgs_pets = test_df_imgs['image_filename'].apply(lambda x: x.split(split_char)[-1].split('-')[0])\n\ntrain_df_imgs = train_df_imgs.assign(PetID=train_imgs_pets)\ntest_df_imgs = test_df_imgs.assign(PetID=test_imgs_pets)\n\ndef getSize(filename):\n    st = os.stat(filename)\n    return st.st_size\n\ndef getDimensions(filename):\n    img_size = Image.open(filename).size\n    return img_size \n\ntrain_df_imgs['image_size'] = train_df_imgs['image_filename'].apply(getSize)\ntrain_df_imgs['temp_size'] = train_df_imgs['image_filename'].apply(getDimensions)\ntrain_df_imgs['width'] = train_df_imgs['temp_size'].apply(lambda x : x[0])\ntrain_df_imgs['height'] = train_df_imgs['temp_size'].apply(lambda x : x[1])\ntrain_df_imgs = train_df_imgs.drop(['temp_size'], axis=1)\n\ntest_df_imgs['image_size'] = test_df_imgs['image_filename'].apply(getSize)\ntest_df_imgs['temp_size'] = test_df_imgs['image_filename'].apply(getDimensions)\ntest_df_imgs['width'] = test_df_imgs['temp_size'].apply(lambda x : x[0])\ntest_df_imgs['height'] = test_df_imgs['temp_size'].apply(lambda x : x[1])\ntest_df_imgs = test_df_imgs.drop(['temp_size'], axis=1)\n\naggs = {\n    'image_size': ['sum', 'mean', 'max', 'min', 'std'], # enis: I added 'max', 'min', 'std'\n    'width': ['sum', 'mean', 'max', 'min', 'std'], # enis: I added 'max', 'min', 'std'\n    'height': ['sum', 'mean', 'max', 'min', 'std'], # enis: I added 'max', 'min', 'std'\n}\n\nagg_train_imgs = train_df_imgs.groupby('PetID').agg(aggs)\nnew_columns = [\n    k + '_' + agg for k in aggs.keys() for agg in aggs[k]\n]\nagg_train_imgs.columns = new_columns\nagg_train_imgs = agg_train_imgs.reset_index()\n\nagg_test_imgs = test_df_imgs.groupby('PetID').agg(aggs)\nnew_columns = [\n    k + '_' + agg for k in aggs.keys() for agg in aggs[k]\n]\nagg_test_imgs.columns = new_columns\nagg_test_imgs = agg_test_imgs.reset_index()\n\nagg_imgs = pd.concat([agg_train_imgs, agg_test_imgs], axis=0).reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df_all = pd.merge(df_all,agg_imgs,on='PetID',how='left')\n\nall_columns.extend(new_columns + ['image_num'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# BREED"},{"metadata":{},"cell_type":"markdown","source":"#### Mix Breed "},{"metadata":{"trusted":false},"cell_type":"code","source":"breed_job = df_all[['Breed1','Breed2']]\nbreed_job['mixed_breed'] = ((breed_job[['Breed1','Breed2']]==307).sum(axis=1)>0).astype('int')\nbreed_job.loc[breed_job['Breed1']==307,'Breed1'] = 0\nbreed_job.loc[breed_job['Breed2']==307,'Breed2'] = 0\n\nbreed_job.loc[breed_job['mixed_breed']==1,'Breed1'] = breed_job.loc[breed_job['mixed_breed']==1,'Breed1']+breed_job.loc[breed_job['mixed_breed']==1,'Breed2']\nbreed_job.loc[breed_job['mixed_breed']==1,'Breed2'] = 0\n\nbreed_job.loc[(breed_job['mixed_breed']==0)&(breed_job['Breed2']!=0),'extra_mixed'] = 1\n\nbreed_job.loc[breed_job['extra_mixed']==1,'Breed2'] = 0\n\nbreed_job['mixed_breed'] = breed_job['mixed_breed'] + breed_job['extra_mixed'].fillna(0)\n\ndf_all = df_all.drop(['Breed1', 'Breed2',],axis=1)\n\ndf_all = pd.concat(\n    [df_all, breed_job[['Breed1','mixed_breed']]], axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Breed Type"},{"metadata":{"trusted":false},"cell_type":"code","source":"df_all['Type'] = df_all['Type'].map({1:1,2:0})\n\ndf_all = df_all.rename(columns={'Type':'is_dog'})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Breed Prediction"},{"metadata":{"_kg_hide-input":false,"trusted":false},"cell_type":"code","source":"from keras.preprocessing import image                  \n\ndef path_to_tensor(img_path):\n    img = image.load_img(img_path, target_size=(299, 299))\n    x = image.img_to_array(img)\n    return np.expand_dims(x, axis=0)\n\ndef paths_to_tensor(img_paths):\n    list_of_tensors = [path_to_tensor(img_path) for img_path in img_paths]\n    return np.vstack(list_of_tensors)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"predicted_df = df_all[(df_all.Breed1 == 0) & (df_all.mixed_breed == 1) & df_all.is_dog][['PetID', 'is_train']].copy().reset_index(drop=True)\npredicted_df['img_path'] = predicted_df.apply(lambda x: f\"../input/petfinder-adoption-prediction/{'train' if x['is_train'] else 'test'}_images/{x['PetID']}-1.jpg\", axis=1)\n                                              \npredicted_df = predicted_df[predicted_df.img_path.map(os.path.exists)].reset_index(drop=True)\n                                              \npredicted_df['pred_breed'] = 0\n                                              \nfns = predicted_df.img_path.values","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"from PIL import ImageFile\nImageFile.LOAD_TRUNCATED_IMAGES = True \n\ndef create_generator(batch_size):\n    while True:\n        for i in range(0, len(fns), batch_size):\n            chunk = fns[i:i+batch_size]                            \n            x = paths_to_tensor(chunk).astype('float32')/255\n            \n            yield x","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"batchsize = 256\n\npred_generator = create_generator(batchsize)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"from keras.models import Model, Input\n\nimport keras.backend as K\n\nfrom keras.applications.inception_resnet_v2 import InceptionResNetV2\n\nfrom keras.layers import GlobalAveragePooling2D, Dense, Dropout\n\ninp_ten = Input(shape=(299, 299, 3))\nx = InceptionResNetV2(\n    weights='../input/inceptionresnetv2/inception_resnet_v2_weights_tf_dim_ordering_tf_kernels_notop.h5', \n    include_top=False, \n    pooling=None\n)(inp_ten)\nx = GlobalAveragePooling2D()(x)\nout = Dense(133, activation='softmax')(x)\n\nmodel = Model(inp_ten, out)\n\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n\nmodel.summary()\n\nmodel.load_weights('../input/dog-identification-pretrained/dog_identification_model.hdf5')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"breed_dict = {i:j \n              for i, j in zip(\n                  labels_breed[labels_breed.Type == 1].BreedName.apply(lambda x: x.lower().replace(' ', '_')).values,\n                  labels_breed[labels_breed.Type == 1].BreedID.values)}","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"dog_names = ['in/001.Affenpinscher', 'in/002.Afghan_hound', 'in/003.Airedale_terrier', 'in/004.Akita', 'in/005.Alaskan_malamute', 'in/006.American_eskimo_dog', 'in/007.American_foxhound', 'in/008.American_staffordshire_terrier', 'in/009.American_water_spaniel', 'in/010.Anatolian_shepherd_dog', 'in/011.Australian_cattle_dog', 'in/012.Australian_shepherd', 'in/013.Australian_terrier', 'in/014.Basenji', 'in/015.Basset_hound', 'in/016.Beagle', 'in/017.Bearded_collie', 'in/018.Beauceron', 'in/019.Bedlington_terrier', 'in/020.Belgian_malinois', 'in/021.Belgian_sheepdog', 'in/022.Belgian_tervuren', 'in/023.Bernese_mountain_dog', 'in/024.Bichon_frise', 'in/025.Black_and_tan_coonhound', 'in/026.Black_russian_terrier', 'in/027.Bloodhound', 'in/028.Bluetick_coonhound', 'in/029.Border_collie', 'in/030.Border_terrier', 'in/031.Borzoi', 'in/032.Boston_terrier', 'in/033.Bouvier_des_flandres', 'in/034.Boxer', 'in/035.Boykin_spaniel', 'in/036.Briard', 'in/037.Brittany', 'in/038.Brussels_griffon', 'in/039.Bull_terrier', 'in/040.Bulldog', 'in/041.Bullmastiff', 'in/042.Cairn_terrier', 'in/043.Canaan_dog', 'in/044.Cane_corso', 'in/045.Cardigan_welsh_corgi', 'in/046.Cavalier_king_charles_spaniel', 'in/047.Chesapeake_bay_retriever', 'in/048.Chihuahua', 'in/049.Chinese_crested', 'in/050.Chinese_shar-pei', 'in/051.Chow_chow', 'in/052.Clumber_spaniel', 'in/053.Cocker_spaniel', 'in/054.Collie', 'in/055.Curly-coated_retriever', 'in/056.Dachshund', 'in/057.Dalmatian', 'in/058.Dandie_dinmont_terrier', 'in/059.Doberman_pinscher', 'in/060.Dogue_de_bordeaux', 'in/061.English_cocker_spaniel', 'in/062.English_setter', 'in/063.English_springer_spaniel', 'in/064.English_toy_spaniel', 'in/065.Entlebucher_mountain_dog', 'in/066.Field_spaniel', 'in/067.Finnish_spitz', 'in/068.Flat-coated_retriever', 'in/069.French_bulldog', 'in/070.German_pinscher', 'in/071.German_shepherd_dog', 'in/072.German_shorthaired_pointer', 'in/073.German_wirehaired_pointer', 'in/074.Giant_schnauzer', 'in/075.Glen_of_imaal_terrier', 'in/076.Golden_retriever', 'in/077.Gordon_setter', 'in/078.Great_dane', 'in/079.Great_pyrenees', 'in/080.Greater_swiss_mountain_dog', 'in/081.Greyhound', 'in/082.Havanese', 'in/083.Ibizan_hound', 'in/084.Icelandic_sheepdog', 'in/085.Irish_red_and_white_setter', 'in/086.Irish_setter', 'in/087.Irish_terrier', 'in/088.Irish_water_spaniel', 'in/089.Irish_wolfhound', 'in/090.Italian_greyhound', 'in/091.Japanese_chin', 'in/092.Keeshond', 'in/093.Kerry_blue_terrier', 'in/094.Komondor', 'in/095.Kuvasz', 'in/096.Labrador_retriever', 'in/097.Lakeland_terrier', 'in/098.Leonberger', 'in/099.Lhasa_apso', 'in/100.Lowchen', 'in/101.Maltese', 'in/102.Manchester_terrier', 'in/103.Mastiff', 'in/104.Miniature_schnauzer', 'in/105.Neapolitan_mastiff', 'in/106.Newfoundland', 'in/107.Norfolk_terrier', 'in/108.Norwegian_buhund', 'in/109.Norwegian_elkhound', 'in/110.Norwegian_lundehund', 'in/111.Norwich_terrier', 'in/112.Nova_scotia_duck_tolling_retriever', 'in/113.Old_english_sheepdog', 'in/114.Otterhound', 'in/115.Papillon', 'in/116.Parson_russell_terrier', 'in/117.Pekingese', 'in/118.Pembroke_welsh_corgi', 'in/119.Petit_basset_griffon_vendeen', 'in/120.Pharaoh_hound', 'in/121.Plott', 'in/122.Pointer', 'in/123.Pomeranian', 'in/124.Poodle', 'in/125.Portuguese_water_dog', 'in/126.Saint_bernard', 'in/127.Silky_terrier', 'in/128.Smooth_fox_terrier', 'in/129.Tibetan_mastiff', 'in/130.Welsh_springer_spaniel', 'in/131.Wirehaired_pointing_griffon', 'in/132.Xoloitzcuintli', 'in/133.Yorkshire_terrier']","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"preds = model.predict_generator(pred_generator, steps=len(fns) // batchsize + 1, verbose=1)\n\npreds = np.argmax(preds, axis=1)\n\nmapping = {\n    'american_foxhound': 'foxhound',\n    'anatolian_shepherd_dog': 'anatolian_shepherd',\n    'australian_cattle_dog': 'australian_cattle_dog/blue_heeler',\n    'belgian_malinois': 'belgian_shepherd_malinois',\n    'belgian_sheepdog': 'belgian_shepherd_dog_sheepdog',\n    'belgian_tervuren': 'belgian_shepherd_tervuren',\n    'bouvier_des_flandres': 'bouvier_des_flanders',\n    'brittany': 'brittany_spaniel',\n    'bulldog': 'american_bulldog',\n    'cane_corso': 'cane_corso_mastiff',\n    'cardigan_welsh_corgi': 'welsh_corgi',\n    'chinese_crested': 'chinese_crested_dog',\n    'chinese_shar-pei': 'shar_pei',\n    'dandie_dinmont_terrier': 'dandi_dinmont_terrier',\n    'entlebucher_mountain_dog': 'entlebucher',\n    'icelandic_sheepdog': 'shetland_sheepdog_sheltie',\n    'irish_red_and_white_setter': 'irish_setter',\n    'miniature_schnauzer': 'schnauzer',\n    'newfoundland': 'newfoundland_dog',\n    'nova_scotia_duck_tolling_retriever': 'nova_scotia_duck-tolling_retriever',\n    'parson_russell_terrier': 'jack_russell_terrier',\n    'pembroke_welsh_corgi': 'welsh_corgi',\n    'plott': 'plott_hound',\n    'wirehaired_pointing_griffon': 'german_wirehaired_pointer',\n    'xoloitzcuintli': 'xoloitzcuintle/mexican_hairless',\n    'yorkshire_terrier': 'yorkshire_terrier_yorkie'\n}\n\ndef get_breed_id(prediction):\n    dog_name = dog_names[prediction].split('.')[-1].lower()\n    dog_name = dog_name if dog_name not in mapping else mapping[dog_name]\n    return breed_dict[dog_name]\n\npredicted_df['pred_breed'] = [get_breed_id(x) for x in preds]\n\ndel model; gc.collect()\n\nK.clear_session()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"predicted_df = predicted_df[['PetID', 'pred_breed']]\n\ndf_all = df_all.merge(predicted_df, on='PetID', how='left')\n\ndel predicted_df; gc.collect()\n\ndf_all.pred_breed = df_all.pred_breed.fillna(0)\nget_values = lambda i, j: int(j) if j != 0 else i\ndf_all.pred_breed = [get_values(i, j) for i,j in zip(df_all.Breed1.values, df_all.pred_breed.values)]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Breed Names"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_all_breed_names = df_all[['Breed1']].merge(\n    labels_breed[['BreedID','BreedName']], how='left',\n    left_on='Breed1', right_on='BreedID',\n    suffixes=('', '_main_breed'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"for e in ['hair','domestic', 'short', 'medium', 'retriever', 'terrier', 'tabby', 'long']:\n    df_all_breed_names['Breed_{}'.format(e)] = (df_all_breed_names.BreedName.apply(lambda x: e in str(x).lower())).astype(int)\n\nbreed_detail_features = ['Breed_{}'.format(e) for e in ['hair','domestic', 'short', 'medium', 'retriever', 'terrier', 'tabby', 'long']]\n\ndf_all = pd.concat(\n    [df_all, df_all_breed_names[breed_detail_features]], axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Label Encode"},{"metadata":{"trusted":false},"cell_type":"code","source":"le = LabelEncoder()\ndf_all['Breed1_le'] = le.fit_transform(df_all['Breed1'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"breed_columns = ['is_dog', 'Breed1_le', 'mixed_breed']\n\nbreed_columns.extend(breed_detail_features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"all_columns = []\n\nall_columns.extend(breed_columns)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Age\nAge - Age of pet when listed, in months"},{"metadata":{"trusted":false},"cell_type":"code","source":"df_all['Age_bins'] = pd.cut(np.log1p(df_all.Age),9,labels=['Age_{}'.format(e) for e in range(9)])\n# df_all['age_bin_1'] = pd.cut((df_all.Age),10,labels=['age_{}'.format(e) for e in range(10)])\n\n# df_all.groupby('Age_bins').Age.describe()\n\nle = LabelEncoder()\ndf_all['Age_bins'] = le.fit_transform(df_all['Age_bins'])\n\nage_columns = ['Age_bins']","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"all_columns.extend(age_columns)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Name"},{"metadata":{"trusted":false},"cell_type":"code","source":"# na features\ndf_all['Name_isna'] = df_all['Name'].isna().astype(int)\n# name lens\ndf_all.loc[~df_all['Name'].isna(),'Name_len'] = df_all.loc[~df_all['Name'].isna(),'Name'].apply(lambda x : len(str(x).split()))\ndf_all['Name_len'] = df_all['Name_len'].fillna(0)\n# name with numbers\nname_with_numbers = [e for e in df_all.Name.unique() if len(set([str(t) for t in range(10)]).intersection(set(str(e).split())))>0]\ndf_all['Name_with_numbers'] = (df_all.Name.isin(name_with_numbers)).astype(int)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"name_columns = ['Name_len','Name_isna',\"Name_with_numbers\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"all_columns.extend(name_columns)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Color\n-given breed, how is the color"},{"metadata":{"trusted":false},"cell_type":"code","source":"df_all['Color_range'] = (df_all[['Color1','Color2','Color3',]]>0).sum(axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"color_columns = ['Color_range']\n\nall_columns.extend(color_columns)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Fee"},{"metadata":{"trusted":false},"cell_type":"code","source":"df_all['Fee_bins']= pd.cut(np.log1p(df_all.Fee),5,labels=['Fee_{}'.format(e) for e in range(5)])\n\nle = LabelEncoder()\ndf_all['Fee_bins'] = le.fit_transform(df_all['Fee_bins'])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"fee_columns = ['Fee_bins']","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"all_columns.extend(fee_columns)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# State"},{"metadata":{"trusted":false},"cell_type":"code","source":"# state GDP: https://en.wikipedia.org/wiki/List_of_Malaysian_states_by_GDP\nstate_gdp = {\n    41336: 116.679,\n    41325: 40.596,\n    41367: 23.02,\n    41401: 190.075,\n    41415: 5.984,\n    41324: 37.274,\n    41332: 42.389,\n    41335: 52.452,\n    41330: 67.629,\n    41380: 5.642,\n    41327: 81.284,\n    41345: 80.167,\n    41342: 121.414,\n    41326: 280.698,\n    41361: 32.270\n}\n\n# state population: https://en.wikipedia.org/wiki/Malaysia\nstate_population = {\n    41336: 33.48283,\n    41325: 19.47651,\n    41367: 15.39601,\n    41401: 16.74621,\n    41415: 0.86908,\n    41324: 8.21110,\n    41332: 10.21064,\n    41335: 15.00817,\n    41330: 23.52743,\n    41380: 2.31541,\n    41327: 15.61383,\n    41345: 32.06742,\n    41342: 24.71140,\n    41326: 54.62141,\n    41361: 10.35977\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# External Data from Wiki\ndf_all[\"state_gdp\"] = df_all.State.map(state_gdp)\ndf_all[\"state_population\"] = df_all.State.map(state_population)\n# label encoding states\nle = LabelEncoder()\ndf_all['State_le'] = le.fit_transform(df_all['State'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"state_columns = ['state_gdp',\n        'state_population',\n        'State_le',]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"all_columns.extend(state_columns)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Video Photo"},{"metadata":{"trusted":false},"cell_type":"code","source":"# any video?\ndf_all['is_VideoAmt'] =(df_all.VideoAmt>0).astype('int')\n# bin \ndf_all['PhotoAmt_bins']= pd.cut(np.log1p(df_all.PhotoAmt),5,labels=['PhotoAmt_{}'.format(e) for e in range(5)])\n\n# df_all.groupby(['PhotoAmt_bins'])['PhotoAmt'].describe()\n\nle = LabelEncoder()\ndf_all['PhotoAmt_bins'] = le.fit_transform(df_all['PhotoAmt_bins'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"video_photo_columns = ['is_VideoAmt','PhotoAmt_bins']","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"all_columns.extend(video_photo_columns)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Description"},{"metadata":{"trusted":false},"cell_type":"code","source":"# na features\ndf_all['Description_isna'] = df_all['Description'].isna().astype(int)\n# no of words \ndf_all['len_words_Description'] = df_all.Description.apply(lambda x: len(str(x).split()))\n# create bins\nn = 6\ndf_all['len_words_Description_bins']= pd.cut(np.log1p(df_all.len_words_Description),n,labels=['Description_{}'.format(e) for e in range(n)])\n# df_all.groupby(['len_words_Description_bins'])['len_words_Description'].describe()\nle = LabelEncoder()\ndf_all['len_words_Description_bins'] = le.fit_transform(df_all['len_words_Description_bins'])\n\n# number of uppercase \ndf_all['len_isupper_Description'] = df_all.Description.apply(lambda x: len([c for c in str(x) if c.isupper()]))\n# number of lowercase\ndf_all['len_islower_Description'] = df_all.Description.apply(lambda x: len([c for c in str(x) if c.islower()]))\n\n# some numbers \ncount = lambda l1, l2: len(list(filter(lambda c: c in l2, l1)))\n\ndf_all['len_punctuation_Description'] = df_all.Description.apply(lambda x: count(str(x), string.punctuation))\ndf_all['len_letters_Description'] = df_all.Description.apply(lambda x: count(str(x), string.ascii_letters))\ndf_all['len_characters_Description'] = df_all.Description.apply(lambda x: len(str(x)))\n\n# ratios to words\ndf_all['len_isupper_Description_to_words'] = df_all['len_isupper_Description'] / df_all['len_words_Description']\ndf_all['len_islower_Description_to_words'] = df_all['len_islower_Description'] / df_all['len_words_Description']\ndf_all['len_punctuation_Description_to_words'] = df_all['len_punctuation_Description'] / df_all['len_words_Description']\ndf_all['len_letters_Description_to_words'] = df_all['len_letters_Description'] / df_all['len_words_Description']\ndf_all['len_characters_Description_to_words'] = df_all['len_characters_Description'] / df_all['len_words_Description']\n\n# ratios to letters\ndf_all['len_isupper_Description_to_characters'] = df_all['len_isupper_Description'] / df_all['len_characters_Description']\ndf_all['len_islower_Description_to_characters'] = df_all['len_islower_Description'] / df_all['len_characters_Description']\ndf_all['len_punctuation_Description_to_characters'] = df_all['len_punctuation_Description'] / df_all['len_characters_Description']\ndf_all['len_letters_Description_to_characters'] = df_all['len_letters_Description'] / df_all['len_characters_Description']","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"description_columns = [\n    'Description_isna',\n    'len_words_Description',\n    'len_words_Description_bins',\n    'len_isupper_Description',\n    'len_islower_Description',\n    'len_punctuation_Description',\n    'len_letters_Description',\n    'len_characters_Description',\n    'len_isupper_Description_to_words',\n    'len_islower_Description_to_words',\n    'len_punctuation_Description_to_words',\n    'len_letters_Description_to_words',\n    'len_isupper_Description_to_characters',\n    'len_islower_Description_to_characters',\n    'len_punctuation_Description_to_characters',\n    'len_letters_Description_to_characters',\n    'len_characters_Description_to_words',\n                      ]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"all_columns.extend(description_columns)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Word - Sentence Embeddings"},{"metadata":{"trusted":true},"cell_type":"code","source":"nlp = spacy.load('../input/spacyen-vectors-web-lg/spacy-en_vectors_web_lg/en_vectors_web_lg')\n\ndesc_embd_cols = ['Describe_spacy_{}'.format(e) for e in range(300)]\n\ndesc_embd = pd.DataFrame(np.vstack(df_all[['Description']].dropna().apply(lambda x: nlp(x['Description']).vector,axis=1).values),\n                         index=df_all[['Description']].dropna().index)\n\ndesc_embd.columns= desc_embd_cols\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"del nlp; gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"Describe_SVD = True\n\nif Describe_SVD:\n    n_components = 32\n\n    svd_ = TruncatedSVD(n_components=n_components, random_state=1881)\n    svd_col = svd_.fit_transform(desc_embd)\n    svd_col_df = pd.DataFrame(svd_col,index=df_all[['Description']].dropna().index)\n    svd_col_df = svd_col_df.add_prefix('Describe_spacy_SVD')\n\n    df_all  = df_all.join(svd_col_df)\n    all_columns.extend(svd_col_df.columns)\n    \nelse:\n    \n    df_all  = df_all.join(desc_embd)\n    all_columns.extend(desc_embd_cols)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"del svd_col_df\ndel desc_embd; gc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Image Features"},{"metadata":{"trusted":false},"cell_type":"code","source":"images_df = df_all[['PetID', 'is_train', 'is_dog']].copy()\n\nimages_df['img_path'] = images_df.apply(lambda x: f\"../input/petfinder-adoption-prediction/{'train' if x['is_train'] else 'test'}_images/{x['PetID']}-1.jpg\", axis=1)\n                                           \nimages_df = images_df[images_df.img_path.map(os.path.exists)].reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"from keras.preprocessing import image                  \n\ndef load_image(img_path, preprocess_fn, img_size):\n    image = cv2.imread(img_path)\n    image = cv2.resize(image, (img_size, img_size))\n    image = preprocess_fn(image)\n    return image","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"images_df = images_df.set_index('PetID')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### DenseNet121"},{"metadata":{"trusted":false},"cell_type":"code","source":"from keras.models import Model\nfrom keras.layers import GlobalAveragePooling2D, Input, Lambda, AveragePooling1D\n\nfrom keras.applications.densenet import preprocess_input, DenseNet121\n\ninp = Input((256,256,3))\nbackbone = DenseNet121(input_tensor = inp, \n                       weights=\"../input/densenet-keras/DenseNet-BC-121-32-no-top.h5\",\n                       include_top = False)\nx = backbone.output\nx = GlobalAveragePooling2D()(x)\nx = Lambda(lambda x: K.expand_dims(x,axis = -1))(x)\nx = AveragePooling1D(4)(x)\nout = Lambda(lambda x: x[:,:,0])(x)\n\nm = Model(inp,out)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"import cv2\n\nbatch_size = 256\n\npet_ids = images_df.index.values\nn_batches = len(pet_ids) // batch_size + 1\n\nfeatures = {}\nfor b in tqdm(range(n_batches)):\n    start = b*batch_size\n    end = (b+1)*batch_size\n    batch_pets = pet_ids[start:end]\n    batch_images = np.zeros((len(batch_pets),256,256,3))\n    for i,pet_id in enumerate(batch_pets):\n        image_ins = images_df.loc[pet_id]\n        batch_images[i] = load_image(image_ins['img_path'], preprocess_input, 256)\n    batch_preds = m.predict(batch_images)\n    for i,pet_id in enumerate(batch_pets):\n        features[pet_id] = batch_preds[i]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"img_feats = pd.DataFrame.from_dict(features, orient='index')\nimg_feats.columns = [f'pic_{i}' for i in range(img_feats.shape[1])]\n\nimg_feats = img_feats.reset_index()\nimg_feats.rename({'index': 'PetID'}, axis='columns', inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"all_ids = img_feats[['PetID']]\nall_ids.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"n_components = 32\nsvd_ = TruncatedSVD(n_components=n_components, random_state=1337)\n\nfeatures = img_feats[[f'pic_{i}' for i in range(len(img_feats.columns) - 1)]].values\n\nsvd_col = svd_.fit_transform(features)\nsvd_col = pd.DataFrame(svd_col)\nsvd_col = svd_col.add_prefix('IMG_SVD_Dense_')\n\nimg_feats = pd.concat([all_ids, svd_col], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df_all = df_all.merge(img_feats, on='PetID', how='left')\n\nall_columns.extend(img_feats.columns.values)\n\ndel img_feats\n\ndel m; gc.collect()\nK.clear_session()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Inception V3"},{"metadata":{"trusted":false},"cell_type":"code","source":"from keras.models import Model\nfrom keras.layers import GlobalAveragePooling2D, Input, Lambda, AveragePooling1D\n\nfrom keras.applications.inception_v3 import InceptionV3, preprocess_input\n\ninp = Input(shape=(224, 224, 3))\nbackbone = InceptionV3(\n    input_tensor=inp, \n    weights='../input/keras-pretrained-models/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5', \n    include_top=False, \n    pooling=None,\n)\n\nx = backbone.output\nx = GlobalAveragePooling2D()(x)\nx = Lambda(lambda x: K.expand_dims(x,axis = -1))(x)\nx = AveragePooling1D(4)(x)\nout = Lambda(lambda x: x[:,:,0])(x)\n\nm = Model(inp,out)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"batch_size = 256\n\npet_ids = images_df.index.values\nn_batches = len(pet_ids) // batch_size + 1\n\nfeatures = {}\nfor b in tqdm(range(n_batches)):\n    start = b*batch_size\n    end = (b+1)*batch_size\n    batch_pets = pet_ids[start:end]\n    batch_images = np.zeros((len(batch_pets),224, 224,3))\n    for i,pet_id in enumerate(batch_pets):\n        image_ins = images_df.loc[pet_id]\n        batch_images[i] = load_image(image_ins['img_path'], preprocess_input, 224)\n    batch_preds = m.predict(batch_images)\n    for i,pet_id in enumerate(batch_pets):\n        features[pet_id] = batch_preds[i]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"img_feats = pd.DataFrame.from_dict(features, orient='index')\nimg_feats.columns = [f'pic_inception{i}' for i in range(img_feats.shape[1])]\n\nimg_feats = img_feats.reset_index()\nimg_feats.rename({'index': 'PetID'}, axis='columns', inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"all_ids = img_feats[['PetID']]\nall_ids.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"n_components = 32\nsvd_ = TruncatedSVD(n_components=n_components, random_state=1337)\n\nfeatures = img_feats[[f'pic_inception{i}' for i in range(len(img_feats.columns) - 1)]].values\n\nsvd_col = svd_.fit_transform(features)\nsvd_col = pd.DataFrame(svd_col)\nsvd_col = svd_col.add_prefix('IMG_SVD_Incep_')\n\nimg_feats = pd.concat([all_ids, svd_col], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df_all = df_all.merge(img_feats, on='PetID', how='left')\n\nall_columns.extend(img_feats.columns.values)\n\ndel img_feats\n\ndel m; gc.collect()\nK.clear_session()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Xception"},{"metadata":{"trusted":false},"cell_type":"code","source":"from keras.models import Model\nfrom keras.layers import GlobalAveragePooling2D, Input, Lambda, AveragePooling1D\n\nfrom keras.applications.xception import Xception, preprocess_input\n\ninp = Input(shape=(224, 224, 3))\nbackbone = Xception(\n    input_tensor=inp, \n    weights='../input/keras-pretrained-models/xception_weights_tf_dim_ordering_tf_kernels_notop.h5', \n    include_top=False, \n    pooling=None,\n)\n\nx = backbone.output\nx = GlobalAveragePooling2D()(x)\nx = Lambda(lambda x: K.expand_dims(x,axis = -1))(x)\nx = AveragePooling1D(4)(x)\nout = Lambda(lambda x: x[:,:,0])(x)\n\nm = Model(inp,out)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"batch_size = 256\n\npet_ids = images_df.index.values\nn_batches = len(pet_ids) // batch_size + 1\n\nfeatures = {}\nfor b in tqdm(range(n_batches)):\n    start = b*batch_size\n    end = (b+1)*batch_size\n    batch_pets = pet_ids[start:end]\n    batch_images = np.zeros((len(batch_pets),224, 224,3))\n    for i,pet_id in enumerate(batch_pets):\n        image_ins = images_df.loc[pet_id]\n        batch_images[i] = load_image(image_ins['img_path'], preprocess_input, 224)\n    batch_preds = m.predict(batch_images)\n    for i,pet_id in enumerate(batch_pets):\n        features[pet_id] = batch_preds[i]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"img_feats = pd.DataFrame.from_dict(features, orient='index')\nimg_feats.columns = [f'pic_xception{i}' for i in range(img_feats.shape[1])]\n\nimg_feats = img_feats.reset_index()\nimg_feats.rename({'index': 'PetID'}, axis='columns', inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"all_ids = img_feats[['PetID']]\nall_ids.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"n_components = 32\nsvd_ = TruncatedSVD(n_components=n_components, random_state=1337)\n\nfeatures = img_feats[[f'pic_xception{i}' for i in range(len(img_feats.columns) - 1)]].values\n\nsvd_col = svd_.fit_transform(features)\nsvd_col = pd.DataFrame(svd_col)\nsvd_col = svd_col.add_prefix('IMG_SVD_Xcep_')\n\nimg_feats = pd.concat([all_ids, svd_col], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df_all = df_all.merge(img_feats, on='PetID', how='left')\n\nall_columns.extend(img_feats.columns.values)\n\ndel img_feats\ndel images_df\ndel m; gc.collect()\nK.clear_session()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# External Data"},{"metadata":{},"cell_type":"markdown","source":"## Petfinder.com"},{"metadata":{"trusted":false},"cell_type":"code","source":"df_external_1 = pd.read_csv('../input/petfindercomexternal/petfinder.com_external.csv')\ndf_external_1 = df_external_1.drop('attr_other_names',axis=1)\ndf_external_1 = df_external_1.rename(columns={'BreedID':'pred_breed'})","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df_all = df_all.merge(df_external_1,how='left',on='pred_breed')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"all_columns.extend(df_external_1.columns[:].values)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Cat and dog breeds parameters from [here](https://www.kaggle.com/hocop1/cat-and-dog-breeds-parameters)"},{"metadata":{"trusted":false},"cell_type":"code","source":"# add features from ratings \nwith open('../input/cat-and-dog-breeds-parameters/rating.json', 'r') as f:\n        ratings = json.load(f)\ncat_ratings = ratings['cat_breeds']\ndog_ratings = ratings['dog_breeds']\n\nbreed_id = {}\nfor id_,name in zip(labels_breed.BreedID,labels_breed.BreedName):\n    breed_id[name] = id_\n\nbreed_names_1 = [i for i in cat_ratings.keys()]\nbreed_names_2 = [i for i in dog_ratings.keys()]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df_cat_breeds = pd.DataFrame(cat_ratings).T\ndf_dog_breeds = pd.DataFrame(dog_ratings).T\ndf_detail_breeds = pd.concat([df_cat_breeds,df_dog_breeds],axis=1)\ndf_detail_breeds = df_detail_breeds.reset_index().rename(columns={'index':'Breed1'})","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df_detail_breeds = df_detail_breeds.append({'Breed1':'Terrier',\n    'Affectionate with Family': 4.606060606060606,\n 'Friendly Toward Strangers': 3.757575757575758,\n 'General Health': 4.0606060606060606,\n 'Intelligence': 4.090909090909091,\n ' Adaptability': 3.212121212121212,\n ' All Around Friendliness': 4.121212121212121,\n ' Exercise Needs': 4.454545454545454,\n ' Health Grooming': 2.8181818181818183,\n ' Trainability': 3.606060606060606,\n 'Adapts Well to Apartment Living': 3.9696969696969697,\n 'Amount Of Shedding': 2.515151515151515,\n 'Dog Friendly': 3.1818181818181817,\n 'Drooling Potential': 1.2424242424242424,\n 'Easy To Groom': 3.1515151515151514,\n 'Easy To Train': 3.3636363636363638,\n 'Energy Level': 4.212121212121212,\n 'Exercise Needs': 4.090909090909091,\n 'Good For Novice Owners': 3.0606060606060606,\n 'Incredibly Kid Friendly Dogs': 4.212121212121212,\n 'Intensity': 4.03030303030303,\n 'Potential For Mouthiness': 2.8484848484848486,\n 'Potential For Playfulness': 4.666666666666667,\n 'Potential For Weight Gain': 3.3333333333333335,\n 'Prey Drive': 3.5757575757575757,\n 'Sensitivity Level': 3.5454545454545454,\n 'Size': 2.0,\n 'Tendency To Bark Or Howl': 3.212121212121212,\n 'Tolerates Being Alone': 2.1818181818181817,\n 'Tolerates Cold Weather': 3.0303030303030303,\n 'Tolerates Hot Weather': 3.242424242424242,\n 'Wanderlust Potential': 3.6363636363636362}, ignore_index=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"breed_mapper = {\"White German Shepherd\":\"German Shepherd Dog\",\n    \"Jack Russell Terrier (Parson Russell Terrier)\": \"Jack Russell Terrier\",\n\"Belgian Shepherd Dog Sheepdog\": \"Belgian Sheepdog\",\n\"Shetland Sheepdog Sheltie\": \"Shetland Sheepdog\",\n\"English Pointer\": \"Pointer\",\n\"Appenzell Mountain Dog\": \"Bernese Mountain Dog\",\n\"Yorkshire Terrier Yorkie\": \"Yorkshire Terrier\",\n\"Thai Ridgeback\": \"Rhodesian Ridgeback\",\n\"Spitz\": \"Finnish Spitz\",\n\"Standard Poodle\": \"Poodle\",\n\"Havana\": \"Havana Brown\",\n\"Munsterlander\": \"Small Munsterlander Pointer\",\n\"English Bulldog\": \"Bulldog\",\n\"Husky\": \"Siberian Husky\",\n\"Entlebucher\": \"Entlebucher Mountain Dog\",\n\"Wire-haired Pointing Griffon\": \"Wirehaired Pointing Griffon\",\n\"Chocolate Labrador Retriever\": \"Labrador Retriever\",\n\"Belgian Shepherd Malinois\": \"Belgian Sheepdog\",\n\"German Spitz\": \"Finnish Spitz\",\n\"German Spitz\": \"Japanese Spitz\",\n\"Smooth Fox Terrier\": \"Fox Terrier\",\n\"Belgian Shepherd Tervuren\": \"Belgian Tervuren\",\n\"Wirehaired Terrier\": \"German Wirehaired Pointer\",\n\"Galgo Spanish Greyhound\": \"Greyhound\",\n\"Welsh Corgi\": \"Cardigan Welsh Corgi\",\n\"Eskimo Dog\": \"American Eskimo Dog\",\n\"Sheep Dog\": \"Old English Sheepdog\",\n\"American Hairless Terrier\": \"American Staffordshire Terrier\",\n\"Retriever\": \"Labrador Retriever\",\n\"Caucasian Sheepdog (Caucasian Ovtcharka)\": \"Caucasian Shepherd Dog\",\n\"Oriental Tabby\": \"Oriental\",\n\"Flat-coated Retriever\": \"Flat-Coated Retriever\",\n\"Oriental Short Hair\": \"Oriental\",\n\"Exotic Shorthair\": \"American Shorthair\",\n\"Spaniel\": \"Cocker Spaniel\",\n\"Wire Fox Terrier\": \"Fox Terrier\",\n\"Oriental Long Hair\": \"Oriental\",\n\"Chinese Crested Dog\": \"Chinese Crested\",\n\"Applehead Siamese\": \"Siamese Cat\",\n\"Klee Kai\": \"Alaskan Klee Kai\",\n\"Dandi Dinmont Terrier\": \"Dandie Dinmont Terrier\",\n\"Yellow Labrador Retriever\": \"Labrador Retriever\",\n\"Bobtail\": \"American Bobtail\",\n\"Anatolian Shepherd\": \"Anatolian Shepherd Dog\",\n\"Cane Corso Mastiff\": \"Cane Corso\",\n\"Bengal\": \"Bengal Cats\",\n\"Pit Bull Terrier\": \"American Pit Bull Terrier\",\n\"Shepherd\": \"German Shepherd Dog\",\n\"Scottish Terrier Scottie\": \"Scottish Terrier\",\n\"Mountain Dog\": \"Bernese Mountain Dog\",\n\"Jindo\": \"Korean Jindo Dog\",\n\"Foxhound\": \"American Foxhound\",\n\"Bouvier des Flanders\": \"Bouvier des Flandres\",\n\"Schnauzer\": \"Standard Schnauzer\",\n\"Newfoundland Dog\": \"Newfoundland\",\n\"Cattle Dog\": \"Australian Cattle Dog\",\n\"West Highland White Terrier Westie\": \"West Highland White Terrier\",\n\"Australian Cattle Dog/Blue Heeler\": \"Australian Cattle Dog\",\n\"Maremma Sheepdog\": \"Belgian Sheepdog\",\n\"Ragdoll\": \"Ragdoll Cats\",\n\"Wheaten Terrier\": \"Soft Coated Wheaten Terrier\",\n\"Setter\": \"English Setter\",\n\"Siamese\": \"Siamese Cat\",\n\"Black Labrador Retriever\": \"Labrador Retriever\",\n\"Norwegian Forest Cat\": \"Norwegian Forest\",\n\"English Coonhound\": \"American English Coonhound\",\n\"Coonhound\": \"American English Coonhound\",\n\"English Shepherd\": \"Old English Sheepdog\",\n\"Plott Hound\": \"Plott\",\n\"Brittany Spaniel\": \"Brittany\",\n\"Corgi\": \"Cardigan Welsh Corgi\",\n\"Illyrian Sheepdog\": \"Old English Sheepdog\",\n\"Patterdale Terrier (Fell Terrier)\": \"Terrier\",\n\"Nova Scotia Duck-Tolling Retriever\": \"Nova Scotia Duck Tolling Retriever\",\n\"Hound\": \"Afghan Hound\",\n\"Belgian Shepherd Laekenois\": \"Belgian Sheepdog\",\n\"Sphynx (hairless cat)\": \"Sphynx\",\n\"Mountain Cur\": \"Bernese Mountain Dog\",\n\"Kai Dog\": \"Alaskan Klee Kai\",}","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"labels_breed_temp = labels_breed.copy()\n\nchoose = labels_breed_temp.BreedName.isin(list(breed_mapper.keys()))\nlabels_breed_temp['NewBreedName'] = labels_breed_temp['BreedName']\nlabels_breed_temp.loc[choose,'NewBreedName'] = labels_breed_temp.loc[choose,'BreedName'].map(breed_mapper)\n\nbreed_mapper_all = labels_breed_temp.set_index('BreedID')['NewBreedName'].to_dict()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df_all_temp = df_all.copy()\n\ndf_all_temp.Breed1 = df_all_temp.pred_breed.map(breed_mapper_all)\n\ndf_all_temp = df_all_temp.merge(df_detail_breeds,on='Breed1',how='left')\n\ndf_detail_breeds_cols = df_detail_breeds.columns[1:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df_all = pd.concat((df_all,df_all_temp[df_detail_breeds_cols]),axis=1)\n\nall_columns.extend(list(df_detail_breeds.columns.values[1:]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Pet Breed Characteristics from [here](https://www.kaggle.com/rturley/pet-breed-characteristics)"},{"metadata":{"trusted":false},"cell_type":"code","source":"df_external_3_cat = pd.read_csv('../input/pet-breed-characteristics/cat_breed_characteristics.csv')\ndf_external_3_dog = pd.read_csv('../input/pet-breed-characteristics/dog_breed_characteristics.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"cat_dog_cols = list(df_external_3_dog.columns)+list(set(df_external_3_cat.columns)- set(df_external_3_dog.columns))\n\ndf_external_3_all = pd.concat((df_external_3_dog,df_external_3_cat),axis=0)[cat_dog_cols]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"breed_id = {}\nfor id_,name in zip(labels_breed.BreedID,labels_breed.BreedName):\n    breed_id[name] = id_","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df_external_3_all['BreedName']= df_external_3_all.BreedName.map(breed_id)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df_external_3_all = df_external_3_all.rename(columns={\"BreedName\":'pred_breed'})","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df_all = df_all.merge(df_external_3_all,how='left',on='pred_breed')\n\n# all_columns.extend(list(df_external_3_all.columns.values[1:]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df_all_temp = df_all[['Temperment']].copy()\n\ndf_all_temp['Temperment'] = df_all_temp['Temperment'].apply(lambda x: [e.strip(',') for e in str(x).split()])\n\ndf_all_temp = df_all_temp.drop('Temperment', 1).join(df_all_temp.Temperment.str.join('|').str.get_dummies())\n\n\ndf_all_temp = df_all_temp.add_prefix('Temperment_')\n\ndf_all = pd.concat((df_all,df_all_temp),axis=1)\n\ndel df_all['Temperment']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> # Rescuer ID"},{"metadata":{"trusted":false},"cell_type":"code","source":"rescuer_count = df_all.groupby(['RescuerID'])['PetID'].count().reset_index()\nrescuer_count.columns = ['RescuerID', 'RescuerID_COUNT']\n\ndf_all = df_all.merge(rescuer_count, how='left', on='RescuerID')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Aggregations"},{"metadata":{"trusted":false},"cell_type":"code","source":"groupby_columns = [\n    ['is_dog','Breed1'],\n    ['State'],\n    ['is_dog','Gender','Age_bins'],\n    ['is_dog','MaturitySize'],\n    ['is_dog','FurLength'],\n    ['is_dog','Breed1','Vaccinated', 'Dewormed', 'Sterilized', 'Health',]\n]\n\naggregate_columns = [\n    'Age',\n    'Name_len',\n    'Color1',\n    'Color2',\n    'Color3',\n    'MaturitySize',\n    'FurLength',\n    'Vaccinated',\n    'Dewormed',\n    'Sterilized',\n    'Health',\n    'Quantity',\n    'Fee',\n    'VideoAmt',\n    'PhotoAmt',\n    'state_gdp',\n    'state_population',\n    'len_words_Description', \n    'len_isupper_Description_to_characters',\n    'len_islower_Description_to_characters',\n    'len_punctuation_Description_to_characters',\n    'len_letters_Description_to_characters',\n    'len_characters_Description_to_words',\n    'RescuerID_COUNT',\n]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"for _groupby in groupby_columns:\n    \n    _aggregate = [e for e in aggregate_columns if e not in _groupby]\n    _aggregate = {e:['mean','std','min','max'] for e in _aggregate}\n\n    gr = df_all.groupby(_groupby).agg(_aggregate)\n    gr.columns = ['{}_{}_{}'.format(\"_\".join(_groupby),e[0],e[1]) for e in gr.columns]\n    all_columns.extend(gr.columns)\n    gr = gr.reset_index()\n    df_all = pd.merge(df_all,gr,on=_groupby,how='left')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"other_columns =[\n        'PetID', \n        'AdoptionSpeed',\n        'Age',\n        'Color1',\n        'Color2',\n        'Color3',\n        'Dewormed',\n        'Fee',\n        'FurLength',\n        'Gender',\n        'Health',\n        'MaturitySize',\n        'PhotoAmt',\n        'Quantity',\n        'Sterilized',\n        'Vaccinated',\n        'VideoAmt',]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"categorical_columns = ['Gender', 'Color1', 'Color2', 'Color3','Vaccinated', 'Dewormed',\n       'Sterilized', 'State','Breed1_le', 'RescuerID']\n    \nfreq_categorical_columns = []\nfor c in tqdm(categorical_columns):\n    df_all[c+'_freq'] = df_all[c].map(df_all.groupby(c).size() / df_all.shape[0])\n    freq_categorical_columns.append(c+'_freq')\n    indexer = pd.factorize(df_all[c], sort=True)[1]\n    df_all[c] = indexer.get_indexer(df_all[c])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"del train \ndel test\n\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"excl_columns = list(df_all.dtypes[df_all.dtypes == 'object'].index.values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df_all = df_all.loc[:, ~df_all.columns.isin(excl_columns)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"train = df_all.loc[df_all.is_train==1, ~df_all.columns.duplicated()].drop('is_train', 1)\ntest =  df_all.loc[df_all.is_train==0, ~df_all.columns.duplicated()].drop('is_train', 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# train.to_csv('27_03_lite_feature_engineered_train.csv',index=False)\n# test.to_csv('27_03_lite_feature_engineered_test.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model"},{"metadata":{"trusted":false},"cell_type":"code","source":"# train = pd.read_csv('27_03_lite_feature_engineered_train.csv')\n# test = pd.read_csv('27_03_lite_feature_engineered_test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"feat_cols = list(train.columns[\n    (train.columns.values != 'AdoptionSpeed')\n].values)\ny_col = 'AdoptionSpeed'","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"X_train = train[feat_cols]\nX_test = test[feat_cols]\n\ny_train = train[y_col]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print(f\"X_train shape: {X_train.shape} \\nX_test shape: {X_test.shape}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.metrics import cohen_kappa_score\n\nclass OptimizedRounder(object):\n        def __init__(self):\n            self.coef_ = 0\n\n        def _loss(self, coef, X, y, idx):\n            X_p = np.array([to_bins(pred, coef) for pred in X])\n            ll = -cohen_kappa_score(y, X_p, weights='quadratic')\n            return ll\n\n        def fit(self, X, y):\n            coef = [1.5, 2.0, 2.5, 3.0]\n            golden1 = 0.618\n            golden2 = 1 - golden1\n            ab_start = [(1, 2), (1.8, 2.5), (2, 2.8), (2.5, 3.0)]\n            for it1 in range(10):\n                for idx in range(4):\n                    # golden section search\n                    a, b = ab_start[idx]\n                    # calc losses\n                    coef[idx] = a\n                    la = self._loss(coef, X, y, idx)\n                    coef[idx] = b\n                    lb = self._loss(coef, X, y, idx)\n                    for it in range(20):\n                        # choose value\n                        if la > lb:\n                            a = b - (b - a) * golden1\n                            coef[idx] = a\n                            la = self._loss(coef, X, y, idx)\n                        else:\n                            b = b - (b - a) * golden2\n                            coef[idx] = b\n                            lb = self._loss(coef, X, y, idx)\n            self.coef_ = {'x': coef}\n\n        def predict(self, X, coef):\n            X_p = np.array([to_bins(pred, coef) for pred in X])\n            return X_p\n\n        def coefficients(self):\n            return self.coef_['x']\n        \n# put some numerical values to bins\ndef to_bins(x, borders):\n    for i in range(len(borders)):\n        if x <= borders[i]:\n            return i\n    return len(borders)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## XGBoost Model"},{"metadata":{"trusted":false},"cell_type":"code","source":"import xgboost as xgb\nfrom sklearn.model_selection import StratifiedKFold\n\nfrom bayes_opt import BayesianOptimization","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# dtrain = xgb.DMatrix(X_train, label=y_train)\n# dtest = xgb.DMatrix(X_test)\n\n# def xgb_evaluate(max_depth, gamma, colsample_bytree, eta):\n#     params = {'eval_metric': 'rmse',\n#                 'max_depth': int(max_depth),\n#                 'subsample': 0.8,\n#                 'eta': eta,\n#                 'gamma': gamma,\n#                 'tree_method': 'gpu_hist',\n#                 'device': 'gpu',\n#                 'colsample_bytree': colsample_bytree}\n#     # Used around 1000 boosting rounds in the full model\n#     cv_result = xgb.cv(params, dtrain, num_boost_round=1000, nfold=5)    \n    \n#     # Bayesian optimization only knows how to maximize, not minimize, so return the negative RMSE\n#     return -1.0 * cv_result['test-rmse-mean'].iloc[-1]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# xgb_bo = BayesianOptimization(xgb_evaluate, {'max_depth': (3, 7), \n#                                              'gamma': (0, 1),\n#                                              'colsample_bytree': (0.6, 1.0),\n#                                             'eta': (0.1, 0.005)})\n# # Use the expected improvement acquisition function to handle negative numbers\n# # Optimally needs quite a few more initiation points and number of iterations\n# xgb_bo.maximize(init_points=3, n_iter=5, acq='ei')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# best_params = xgb_bo.max['params']\n# best_params['max_depth'] = int(best_params['max_depth'])\n\n# best_params","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"import xgboost as xgb\nfrom sklearn.model_selection import StratifiedKFold\n\nxgb_params = {\n    'eval_metric': 'rmse',\n    'seed': 1337,\n    'subsample': 0.8,\n    'tree_method': 'gpu_hist',\n    'device': 'gpu',\n    'colsample_bytree': 0.8152926875727884,\n    'eta': 0.03402795645208036,\n    'gamma': 0.9209917577458885,\n    'max_depth': 6,\n    'silent': 1,\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"def run_xgb(params, X_train, X_test):\n    n_splits = 5\n    verbose_eval = 1000\n    num_rounds = 60000\n    early_stop = 500\n\n    kf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=1337)\n\n    oof_train = np.zeros((X_train.shape[0]))\n    oof_test = np.zeros((X_test.shape[0], n_splits))\n    oof_coefficients = []\n\n\n    i = 0\n\n    for train_idx, valid_idx in kf.split(X_train, y_train.values):\n\n        X_tr = X_train.iloc[train_idx, :]\n        X_val = X_train.iloc[valid_idx, :]\n\n        y_tr = y_train.values[train_idx]\n        y_val = y_train.values[valid_idx]\n\n        d_train = xgb.DMatrix(data=X_tr, label=y_tr, feature_names=X_tr.columns)\n        d_valid = xgb.DMatrix(data=X_val, label=y_val, feature_names=X_val.columns)\n\n        watchlist = [(d_train, 'train'), (d_valid, 'valid')]\n        model = xgb.train(dtrain=d_train, num_boost_round=num_rounds, evals=watchlist,\n                         early_stopping_rounds=early_stop, verbose_eval=verbose_eval, params=params)\n\n        valid_pred = model.predict(xgb.DMatrix(X_val, feature_names=X_val.columns), ntree_limit=model.best_ntree_limit)\n        test_pred = model.predict(xgb.DMatrix(X_test, feature_names=X_test.columns), ntree_limit=model.best_ntree_limit)\n        \n        optR = OptimizedRounder()\n        optR.fit(valid_pred, y_val)\n        coefficients = optR.coefficients()\n#         coefficients = [1.6287721, 2.09219494, 2.480343, 2.8293866]\n        oof_coefficients.append(coefficients)\n        pred_val_y_k = optR.predict(valid_pred, coefficients)\n        qwk = cohen_kappa_score(y_val, pred_val_y_k, weights='quadratic')\n\n        print(\"QWK = \", qwk)\n\n        oof_train[valid_idx] = valid_pred\n        oof_test[:, i] = test_pred\n\n        i += 1\n    return model, oof_train, oof_test, oof_coefficients","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"model, oof_train, oof_test, oof_coefficients = run_xgb(xgb_params, X_train, X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"optR = OptimizedRounder()\nmean_coeffs = np.mean(np.array(oof_coefficients), axis=0)\nvalid_pred = optR.predict(oof_train, mean_coeffs)\nqwk = cohen_kappa_score(y_train.values, valid_pred, weights='quadratic')\nprint(\"QWK = \", qwk)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## LGBM Model"},{"metadata":{"trusted":false},"cell_type":"code","source":"# from lightgbm import LGBMRegressor\n# import lightgbm as lgb\n# from sklearn.model_selection import StratifiedKFold\n# from sklearn.metrics import  mean_squared_error\n\n\n# def lgbm_evaluate(**params_range):  \n#     params_range['num_leaves'] = int(params_range['num_leaves'])\n#     params_range['max_depth'] = int(params_range['max_depth'])\n#     params_range['application'] = 'regression'\n#     params_range['metric'] = 'rmse'\n    \n#     clf = LGBMRegressor(**params_range, n_estimators=1000, n_jobs=-1, )\n        \n#     mse_scores = np.zeros(5)\n    \n#     kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=1337)\n\n#     for i, (f_ind, outf_ind) in enumerate(kf.split(X_train, y_train.values)):\n\n#         X_tr = X_train.loc[f_ind].copy()\n#         X_val = X_train.loc[outf_ind].copy()\n\n#         y_tr, y_val = y_train[f_ind], y_train[outf_ind]\n        \n#         clf.fit(X_tr,\n#                 y_tr,\n#                eval_set=[(X_val, y_val)],\n#                eval_metric='rmse',\n#                verbose=False)\n\n#         val_preds = clf.predict(X_val, num_iteration = clf.best_iteration_)\n#         mse_scores[i] = mean_squared_error(y_val, val_preds)\n#         gc.collect()\n\n#     return -np.mean(mse_scores)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# params_range = {\n#     'learning_rate': (.01, .001), \n#     'num_leaves': (60, 90), \n#     'subsample': (0.8, 1), \n#     'max_depth': (7, 18), \n#     'reg_alpha': (.03, .05), \n#     'reg_lambda': (.06, .08), \n#     'min_split_gain': (.01, .03),\n#     'min_child_weight': (38, 50),\n#     'feature_fraction': (0.7, 1.),\n#     'bagging_fraction': (0.8, 1)\n# }","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# from bayes_opt import BayesianOptimization\n\n\n# bo = BayesianOptimization(lgbm_evaluate, params_range)\n# bo.maximize(init_points=3, n_iter=5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# import operator\n# results = bo.res\n# results.sort(key=operator.itemgetter('target'), reverse=True)\n\n# best_params = results[0]['params']\n# best_params['num_leaves'] = int(best_params['num_leaves'])\n# best_params['max_depth'] = int(best_params['max_depth'])\n\n# best_params","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"import lightgbm as lgb\n\nparams = {\n    'application': 'regression',\n    'boosting': 'gbdt',\n    'metric': 'rmse',\n    'num_leaves': 60,\n    'max_depth': 11,\n    'learning_rate': 0.0076,\n    'bagging_fraction': 0.9208175607947006,\n    'feature_fraction': 0.7512774855381017,\n    'learning_rate': 0.01,\n    'max_depth': 17,\n    'min_child_weight': 49.90252232871143,\n    'min_split_gain': 0.021286271431087116,\n    'num_leaves': 76,\n    'reg_alpha': 0.043051815788046566,\n    'reg_lambda': 0.07902389530425666,\n    'subsample': 0.9253685895179343,\n    'verbosity': -1,\n    'data_random_seed': 17\n}\n\n# Additional parameters:\nearly_stop = 500\n\nverbose_eval = 1000\nnum_rounds = 10000\nn_splits = 5","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"oof_train_lgb = np.zeros((X_train.shape[0]))\noof_test_lgb = np.zeros((X_test.shape[0], n_splits))\nqwk_scores = []\noof_coefficients_lgb = []\n\nkf = StratifiedKFold(n_splits=5, shuffle=True, random_state=1337)\n\nfor i, (f_ind, outf_ind) in enumerate(kf.split(X_train, y_train.values)):\n\n    X_tr = X_train.loc[f_ind].copy()\n    X_val = X_train.loc[outf_ind].copy()\n\n    y_tr, y_val = y_train[f_ind], y_train[outf_ind]\n    \n    print('\\ny_tr distribution: {}'.format(Counter(y_tr)))\n    \n    d_train = lgb.Dataset(X_tr, label=y_tr)\n    d_valid = lgb.Dataset(X_val, label=y_val)\n    watchlist = [d_train, d_valid]\n    \n    print('training LGB:')\n    model = lgb.train(params,\n                      train_set=d_train,\n                      num_boost_round=num_rounds,\n                      valid_sets=watchlist,\n                      verbose_eval=verbose_eval,\n                      early_stopping_rounds=early_stop)\n    \n    val_pred = model.predict(X_val, num_iteration=model.best_iteration)\n    optR = OptimizedRounder()\n    optR.fit(val_pred, y_val)\n    coefficients = optR.coefficients()\n    oof_coefficients_lgb.append(coefficients)\n    \n    pred_val_y_k = optR.predict(val_pred, coefficients)\n    print(\"Valid Counts = \", Counter(y_val))\n    print(\"Predicted Counts = \", Counter(pred_val_y_k))\n    print(\"Coefficients = \", coefficients)\n    qwk = cohen_kappa_score(y_val, pred_val_y_k, weights='quadratic')\n    qwk_scores.append(qwk)\n    print(\"QWK = \", qwk)\n    \n    test_pred = model.predict(X_test, num_iteration=model.best_iteration)\n    \n    oof_train_lgb[outf_ind] = val_pred\n    oof_test_lgb[:, i] = test_pred\n    \nprint('{} cv mean QWK score : {}, coeffs {}'.format('LGBM', np.mean(qwk_scores)\n                                                    ,np.mean(np.array(oof_coefficients_lgb), axis=0)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"optR = OptimizedRounder()\nmean_coeffs = np.mean(np.array(oof_coefficients_lgb), axis=0)\nvalid_pred = optR.predict(oof_train_lgb, mean_coeffs)\nqwk = cohen_kappa_score(y_train.values, valid_pred, weights='quadratic')\nprint(\"QWK = \", qwk)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Ensemble LGBM + XGBoost"},{"metadata":{"trusted":false},"cell_type":"code","source":"mean_coeff_m = (np.array(oof_coefficients_lgb) + np.array(oof_coefficients)) / 2","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"optR = OptimizedRounder()\nmean_coeffs = np.mean(np.array(mean_coeff_m), axis=0)\nvalid_pred = optR.predict((oof_train_lgb + oof_train) / 2, mean_coeffs)\nqwk = cohen_kappa_score(y_train.values, valid_pred, weights='quadratic')\nprint(\"QWK = \", qwk)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"train_predictions = optR.predict((oof_train_lgb + oof_train) / 2, mean_coeffs).astype(np.int8)\nprint(f'train pred distribution: {Counter(train_predictions)}')\ntest_predictions = optR.predict((oof_test_lgb.mean(axis=1) + oof_test.mean(axis=1)) / 2, mean_coeffs).astype(np.int8)\nprint(f'test pred distribution: {Counter(test_predictions)}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test = pd.read_csv('../input/petfinder-adoption-prediction/test/test.csv', usecols=['PetID'])\n\nsubmission = pd.DataFrame({'PetID': test['PetID'].values, 'AdoptionSpeed': test_predictions})\nsubmission.to_csv('submission.csv', index=False)\nsubmission.head()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"}},"nbformat":4,"nbformat_minor":1}