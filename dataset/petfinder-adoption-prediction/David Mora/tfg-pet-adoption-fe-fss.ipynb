{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# PetFinder.my Adoption Prediction: Feature Engineering\n\n### David Mora Garrido, Bachelor Dissertation (2nd part)","metadata":{}},{"cell_type":"code","source":"!pip install rfpimp\n\nimport category_encoders as ce\nimport glob\nimport json\nimport matplotlib.pyplot as plt\nimport math\nimport numpy as np\nimport os\nimport pandas as pd\nimport random\nimport rfpimp\nimport seaborn as sns\nimport string\nimport time\nimport utils_tfg_pet_adoption_eda as utils\nimport warnings\nimport xgboost as xgb\n\nfrom collections import defaultdict\nfrom IPython.display import clear_output\nfrom itertools import cycle\n# from scipy.stats import skew\nfrom sklearn.base import BaseEstimator, TransformerMixin, clone\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.feature_selection import f_classif, mutual_info_classif, mutual_info_regression\nfrom sklearn.impute import IterativeImputer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay, plot_confusion_matrix\nfrom sklearn.metrics import accuracy_score, cohen_kappa_score, mean_squared_error, roc_curve, auc, roc_auc_score\nfrom sklearn.metrics import precision_recall_curve, average_precision_score\nfrom sklearn.model_selection import StratifiedKFold, train_test_split\nfrom sklearn.pipeline import make_pipeline, Pipeline\nfrom sklearn.preprocessing import FunctionTransformer, KBinsDiscretizer, StandardScaler, label_binarize\n# from sklearn.preprocessing import PowerTransformer, QuantileTransformer\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeRegressor\nfrom tqdm import tqdm\n\npd.set_option('display.max_columns', None)\n# plt.style.available","metadata":{"execution":{"iopub.status.busy":"2021-06-29T07:15:48.63535Z","iopub.execute_input":"2021-06-29T07:15:48.635821Z","iopub.status.idle":"2021-06-29T07:16:03.944118Z","shell.execute_reply.started":"2021-06-29T07:15:48.635722Z","shell.execute_reply":"2021-06-29T07:16:03.9428Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"seed = 27912","metadata":{"execution":{"iopub.status.busy":"2021-06-29T07:16:03.94634Z","iopub.execute_input":"2021-06-29T07:16:03.946659Z","iopub.status.idle":"2021-06-29T07:16:03.951543Z","shell.execute_reply.started":"2021-06-29T07:16:03.94663Z","shell.execute_reply":"2021-06-29T07:16:03.950194Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"os.environ['PYTHONHASHSEED'] = str(seed)\nrandom.seed(seed)\nnp.random.seed(seed)","metadata":{"execution":{"iopub.status.busy":"2021-06-29T07:16:03.952852Z","iopub.execute_input":"2021-06-29T07:16:03.953193Z","iopub.status.idle":"2021-06-29T07:16:03.968456Z","shell.execute_reply.started":"2021-06-29T07:16:03.953151Z","shell.execute_reply":"2021-06-29T07:16:03.967073Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv('../input/petfinder-adoption-prediction/train/train.csv')\nbreeds = pd.read_csv('../input/petfinder-adoption-prediction/PetFinder-BreedLabels.csv')\ncolors = pd.read_csv('../input/petfinder-adoption-prediction/PetFinder-ColorLabels.csv')\nstates = pd.read_csv('../input/petfinder-adoption-prediction/PetFinder-StateLabels.csv')","metadata":{"execution":{"iopub.status.busy":"2021-06-29T07:16:03.970222Z","iopub.execute_input":"2021-06-29T07:16:03.970541Z","iopub.status.idle":"2021-06-29T07:16:04.311964Z","shell.execute_reply.started":"2021-06-29T07:16:03.970511Z","shell.execute_reply":"2021-06-29T07:16:04.310807Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"breeds_dict = {0: np.nan}\nfor index, row in breeds.iterrows():\n    breeds_dict[row[\"BreedID\"]] = row[\"BreedName\"]\n    \ncolors_dict = {0: np.nan}\nfor index, row in colors.iterrows():\n    colors_dict[row[\"ColorID\"]] = row[\"ColorName\"]\n    \nstates_dict = {}\nfor index, row in states.iterrows():\n    states_dict[row[\"StateID\"]] = row[\"StateName\"]","metadata":{"execution":{"iopub.status.busy":"2021-06-29T07:16:04.315265Z","iopub.execute_input":"2021-06-29T07:16:04.315606Z","iopub.status.idle":"2021-06-29T07:16:04.362643Z","shell.execute_reply.started":"2021-06-29T07:16:04.315572Z","shell.execute_reply":"2021-06-29T07:16:04.36178Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"target = \"AdoptionSpeed\"","metadata":{"execution":{"iopub.status.busy":"2021-06-29T07:16:04.366075Z","iopub.execute_input":"2021-06-29T07:16:04.366485Z","iopub.status.idle":"2021-06-29T07:16:04.370704Z","shell.execute_reply.started":"2021-06-29T07:16:04.366449Z","shell.execute_reply":"2021-06-29T07:16:04.369641Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = train.drop(target, axis=1)\ny = train[target]","metadata":{"execution":{"iopub.status.busy":"2021-06-29T07:16:04.372291Z","iopub.execute_input":"2021-06-29T07:16:04.372685Z","iopub.status.idle":"2021-06-29T07:16:04.391253Z","shell.execute_reply.started":"2021-06-29T07:16:04.372641Z","shell.execute_reply":"2021-06-29T07:16:04.390286Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class LeftJoinReplace(BaseEstimator, TransformerMixin):\n    def __init__(self, values_dict, variables):\n        self.values_dict = values_dict\n        self.variables = variables\n    \n    def fit(self, X, y):\n        return self\n    \n    def transform(self, X, y=None):\n        X = X.copy()\n        for var in self.variables:\n            X[var].replace(self.values_dict, inplace=True)\n        return X\n    \n    def fit_transform(self, X, y=None):\n        return self.fit(X, y).transform(X, y)","metadata":{"execution":{"iopub.status.busy":"2021-06-29T07:16:04.392617Z","iopub.execute_input":"2021-06-29T07:16:04.39314Z","iopub.status.idle":"2021-06-29T07:16:04.400598Z","shell.execute_reply.started":"2021-06-29T07:16:04.393096Z","shell.execute_reply":"2021-06-29T07:16:04.399766Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# To be passed to FunctionTransformer\ndef replace_integers_by_strings(X):\n    X = X.copy()\n    replace_dict = {\n        'Type': {1: 'Dog', 2: 'Cat'},\n        'Gender': {1: 'Male', 2: 'Female', 3: 'Mixed'},\n        'MaturitySize': {1: 'Small', 2: 'Medium', 3: 'Large', 4: 'Extra Large', 0: 'Not Specified'},\n        'FurLength': {1: 'Short', 2: 'Medium', 3: 'Long', 0: 'Not Specified'},\n        'Vaccinated': {1: 'Yes', 2: 'No', 3: 'Not Sure'},\n        'Dewormed': {1: 'Yes', 2: 'No', 3: 'Not Sure'},\n        'Sterilized': {1: 'Yes', 2: 'No', 3: 'Not Sure'},\n        'Health': {1: 'Healthy', 2: 'Minor Injury', 3: 'Serious Injury', 0: 'Not Specified'}\n    }\n    utils.replace_val_categorical(X, replace_dict)\n    return X","metadata":{"execution":{"iopub.status.busy":"2021-06-29T07:16:04.401917Z","iopub.execute_input":"2021-06-29T07:16:04.402457Z","iopub.status.idle":"2021-06-29T07:16:04.413035Z","shell.execute_reply.started":"2021-06-29T07:16:04.402424Z","shell.execute_reply":"2021-06-29T07:16:04.411922Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# To be passed to FunctionTransformer\ndef has_significant_name(X):\n    X = X.copy()\n    no_name_equivalents = set(filter(lambda x: 'no name' in str(x).lower() or len(str(x)) < 3\n                                     or x is np.nan, list(X[\"Name\"].unique())))\n    X[\"HasName\"] = 1\n    X.loc[X[\"Name\"].isin(no_name_equivalents), \"HasName\"] = 0\n    return X #.drop([\"Name\"], axis=1)","metadata":{"execution":{"iopub.status.busy":"2021-06-29T07:16:04.414237Z","iopub.execute_input":"2021-06-29T07:16:04.414692Z","iopub.status.idle":"2021-06-29T07:16:04.430205Z","shell.execute_reply.started":"2021-06-29T07:16:04.414623Z","shell.execute_reply":"2021-06-29T07:16:04.428828Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class BreedFrequencyEncoding(BaseEstimator, TransformerMixin):\n        \n    def fit(self, X, y):\n        self.count_encoder = ce.CountEncoder()\n        self.count_encoder.fit(X[\"Breed1\"])\n        self.most_common_value = X[\"Breed1\"].value_counts().value_counts().keys()[0]\n        return self\n\n    def transform(self, X, y=None):\n        X = X.copy()\n        X[\"Breed1_freq_encode\"] = self.count_encoder.transform(X[\"Breed1\"])\n        X.loc[X[\"Breed1_freq_encode\"].isna(), \"Breed1_freq_encode\"] = self.most_common_value\n        return X #.drop([\"Breed1\"], axis=1)\n\n    def fit_transform(self, X, y=None):\n        return self.fit(X, y).transform(X, y)\n\n    \nclass BreedTargetEncoding(BaseEstimator, TransformerMixin):\n    def __init__(self, minimum_group_threshold=1, smoothing=1.0):\n        self.minimum_group_threshold = minimum_group_threshold\n        self.smoothing = smoothing\n        \n    def fit(self, X, y):\n        X = X.copy()\n        y = y.copy().astype(str)\n        self.fitted_target_encoders = {}\n        y_encoder = ce.OneHotEncoder(use_cat_names=True)\n        y_encoder.fit(y)\n        y_onehot = y_encoder.transform(y)\n        y_column_names = y_onehot.columns\n        for class_name in y_column_names:\n            target_encoder = ce.TargetEncoder(min_samples_leaf=self.minimum_group_threshold,\n                                              smoothing=self.smoothing)\n            self.fitted_target_encoders[class_name] = target_encoder.fit(X[\"Breed1\"], y_onehot[class_name])\n        return self\n\n    def transform(self, X, y=None):\n        X = X.copy()\n        for class_name, target_encoder in self.fitted_target_encoders.items():\n            X[f'Breed1_{str(class_name)}'] = target_encoder.transform(X[\"Breed1\"])\n        return X #.drop([\"Breed1\"], axis=1)\n    \n    def fit_transform(self, X, y=None):\n        return self.fit(X, y).transform(X, y)\n    \n\nclass BreedsOneHotSVD(BaseEstimator, TransformerMixin):\n    def __init__(self, svd_n_components=10, seed=27912):\n        self.svd_n_components = svd_n_components\n        self.seed = seed\n    \n    def fit(self, X, y):\n        self.encoder = ce.OneHotEncoder(use_cat_names=True)\n        self.svd = TruncatedSVD(n_components=self.svd_n_components, random_state=self.seed)\n        X_breeds_transformed = self.encoder.fit_transform(X[[\"Breed1\", \"Breed2\"]])\n        self.svd.fit(X_breeds_transformed)\n        return self\n        \n    def transform(self, X, y=None):\n        X = X.copy()\n        X_breeds_transformed = self.encoder.transform(X[[\"Breed1\", \"Breed2\"]])\n        breed_svd_columns = [f\"Breed_svd_{i}\" for i in range(self.svd_n_components)]\n        X_breeds_svd = pd.DataFrame(self.svd.transform(X_breeds_transformed),\n                                    index=X.index.copy(),\n                                    columns=breed_svd_columns)\n        X = X.merge(X_breeds_svd, left_index=True, right_index=True, how=\"left\")\n        return X\n    \n\nclass BreedEncoding(BaseEstimator, TransformerMixin):\n    def __init__(self, enc_type=\"target_and_frequency\", minimum_group_threshold=1, smoothing=1.0):\n        self.enc_type = enc_type\n        self.minimum_group_threshold = minimum_group_threshold\n        self.smoothing = smoothing\n    \n    def fit(self, X, y):\n        self.encoders = []\n        if self.enc_type == \"target_and_frequency\":\n            self.encoders.append(BreedTargetEncoding(self.minimum_group_threshold,\n                                                     self.smoothing))\n            self.encoders.append(BreedFrequencyEncoding())\n        elif self.enc_type == \"one-hot_svd\":\n            self.encoders.append(BreedsOneHotSVD())\n        else:\n            raise ValueError(f\"{self.enc_type} is not a valid value for 'enc_type' (target_and_frequency/one-hot_svd)\")\n            \n        for encoder in self.encoders:\n            encoder.fit(X,y)\n        return self\n    \n    def transform(self, X, y=None):\n        X = X.copy()\n        for encoder in self.encoders:\n            X_mod = encoder.transform(X, y)\n            X_columns = set(X.columns)\n            added_columns = list(set(X_mod.columns) - X_columns)\n            X = pd.merge(X, X_mod[added_columns], left_index=True, right_index=True)\n        \n        return X #.drop([\"Breed1\"], axis=1)\n        \n    def fit_transform(self, X, y=None):\n        return self.fit(X, y).transform(X, y)","metadata":{"execution":{"iopub.status.busy":"2021-06-29T07:16:04.431768Z","iopub.execute_input":"2021-06-29T07:16:04.432429Z","iopub.status.idle":"2021-06-29T07:16:04.559064Z","shell.execute_reply.started":"2021-06-29T07:16:04.432379Z","shell.execute_reply":"2021-06-29T07:16:04.557774Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def has_pure_breed(X):\n    X = X.copy()\n    domestic_x_hair = {\"Domestic Short Hair\", \"Domestic Medium Hair\", \"Domestic Long Hair\"}\n    X[\"PureBreed\"] = 1\n    X.loc[(X[\"Breed1\"] == \"Mixed Breed\") | (X[\"Breed2\"] == \"Mixed Breed\") |\n          (X[\"Breed1\"].isin(domestic_x_hair)) | (X[\"Breed2\"].isin(domestic_x_hair)) |\n          ((X[\"Breed1\"].notna()) & (X[\"Breed2\"].notna())), \"PureBreed\"] = 0\n    return X","metadata":{"execution":{"iopub.status.busy":"2021-06-29T07:16:04.560451Z","iopub.execute_input":"2021-06-29T07:16:04.56079Z","iopub.status.idle":"2021-06-29T07:16:04.574078Z","shell.execute_reply.started":"2021-06-29T07:16:04.560758Z","shell.execute_reply":"2021-06-29T07:16:04.572773Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def breed_matches_fur_length(X):\n    X = X.copy()\n    domestic_x_hair = {\"Domestic Short Hair\", \"Domestic Medium Hair\", \"Domestic Long Hair\"}\n    indexes_fur_length_inconsistencies = []\n    for index, row in X.iterrows():\n        fur_length = str(row[\"FurLength\"]).lower()\n        breed_name = str(row[\"Breed1\"])\n        if breed_name in domestic_x_hair and fur_length not in breed_name.lower():\n            indexes_fur_length_inconsistencies.append(index)\n    X[\"BreedMatchesFurLength\"] = 1\n    X.loc[indexes_fur_length_inconsistencies, \"BreedMatchesFurLength\"] = 0\n    return X","metadata":{"execution":{"iopub.status.busy":"2021-06-29T07:16:04.575963Z","iopub.execute_input":"2021-06-29T07:16:04.576447Z","iopub.status.idle":"2021-06-29T07:16:04.588091Z","shell.execute_reply.started":"2021-06-29T07:16:04.576401Z","shell.execute_reply":"2021-06-29T07:16:04.586983Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class BreedImputer(BaseEstimator, TransformerMixin):\n        \n    def fit(self, X, y):\n        self.most_common_dog_breed = X.loc[X[\"Type\"] == \"Dog\", \"Breed1\"].value_counts().keys()[0]\n        self.most_common_cat_breed = X.loc[X[\"Type\"] == \"Cat\", \"Breed1\"].value_counts().keys()[0]\n        return self\n    \n    def transform(self, X, y=None):\n        X = X.copy()\n        X.loc[(X[\"Breed1\"].isna()) & (X[\"Breed2\"].notna()) & (X[\"Type\"] == \"Dog\"), \"Breed1\"] = \\\n            X.loc[(X[\"Breed1\"].isna()) & (X[\"Breed2\"].notna()) & (X[\"Type\"] == \"Dog\"), \"Breed2\"]\n        X.loc[(X[\"Breed1\"].isna()) & (X[\"Breed2\"].notna()) & (X[\"Type\"] == \"Cat\"), \"Breed1\"] = \\\n            X.loc[(X[\"Breed1\"].isna()) & (X[\"Breed2\"].notna()) & (X[\"Type\"] == \"Cat\"), \"Breed2\"]\n        X.loc[(X[\"Breed1\"].isna()) & (X[\"Breed2\"].isna()) &\n              (X[\"Type\"] == \"Dog\"), \"Breed1\"] = self.most_common_dog_breed\n        X.loc[(X[\"Breed1\"].isna()) & (X[\"Breed2\"].isna()) &\n              (X[\"Type\"] == \"Cat\"), \"Breed1\"] = self.most_common_cat_breed\n        return X\n    \n    def fit_transform(self, X, y=None):\n        return self.fit(X, y).transform(X, y)","metadata":{"execution":{"iopub.status.busy":"2021-06-29T07:16:04.589252Z","iopub.execute_input":"2021-06-29T07:16:04.589606Z","iopub.status.idle":"2021-06-29T07:16:04.602922Z","shell.execute_reply.started":"2021-06-29T07:16:04.589573Z","shell.execute_reply":"2021-06-29T07:16:04.602117Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CustomOneHotEncoder(BaseEstimator, TransformerMixin):\n    def __init__(self, columns):\n        self.columns = columns\n        \n    def fit(self, X, y):\n        self.one_hot_encoder = ce.OneHotEncoder(use_cat_names=True)\n        self.one_hot_encoder.fit(X[self.columns], y)\n        return self\n    \n    def transform(self, X, y=None):\n        X = X.copy()\n        if \"Type\" in X.columns:\n            X[\"Type\"] = np.where(X[\"Type\"] == \"Cat\", 0, 1)\n        return X.merge(self.one_hot_encoder.transform(X[self.columns]), left_index=True, right_index=True)","metadata":{"execution":{"iopub.status.busy":"2021-06-29T07:16:04.604086Z","iopub.execute_input":"2021-06-29T07:16:04.604548Z","iopub.status.idle":"2021-06-29T07:16:04.621912Z","shell.execute_reply.started":"2021-06-29T07:16:04.604495Z","shell.execute_reply":"2021-06-29T07:16:04.620797Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class OrdinalVariableEncoder(BaseEstimator, TransformerMixin):\n    def __init__(self, columns, enc_type=\"ordinal\", mapping=None):\n        self.columns = columns\n        self.enc_type = enc_type\n        self.mapping = mapping\n        \n    \n    def fit(self, X, y):\n        if self.enc_type == \"ordinal\":\n            self.encoder = ce.OrdinalEncoder(mapping=self.mapping)\n        elif self.enc_type == \"one-hot\":\n            self.encoder = ce.OneHotEncoder(use_cat_names=True)\n        else:\n            raise ValueError(f\"{self.enc_type} is not a valid value for 'enc_type' (ordinal/onehot)\")\n            \n        self.encoder.fit(X[self.columns], y)\n        if self.enc_type == \"ordinal\":\n            self.most_common_values = {}\n            for i, column in enumerate(self.columns):\n                most_common_value_column = list(filter(lambda x: x in self.mapping[i][\"mapping\"],\n                                                       X[column].value_counts().keys()))[0]\n                self.most_common_values[column] = float(self.mapping[i][\"mapping\"][most_common_value_column])\n        return self\n    \n    def transform(self, X, y=None):\n        X = X.copy()\n        columns_encoding = self.encoder.transform(X[self.columns])\n        if self.enc_type == \"ordinal\":\n            for column in self.columns:\n                # If it is Not Specified, then the ordinal condition is harder to hold,\n                # so in those cases we set the most repeated value in trainig\n                columns_encoding.loc[columns_encoding[column] == -1.0, column] = \\\n                    self.most_common_values[column]\n            columns_encoding.rename(lambda x: f\"{x}_ordinal\", axis=1, inplace=True)\n        X = X.merge(columns_encoding, left_index=True, right_index=True)\n        return X","metadata":{"execution":{"iopub.status.busy":"2021-06-29T07:16:04.623386Z","iopub.execute_input":"2021-06-29T07:16:04.623712Z","iopub.status.idle":"2021-06-29T07:16:04.637726Z","shell.execute_reply.started":"2021-06-29T07:16:04.623662Z","shell.execute_reply":"2021-06-29T07:16:04.636645Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ordinal_vars_mapping = [\n    {\"col\": \"MaturitySize\", \"mapping\": {1: 0, 2: 1, 3: 2, 4: 3}},\n    {\"col\": \"FurLength\", \"mapping\": {1: 0, 2: 1, 3: 2}},\n    {\"col\": \"Health\", \"mapping\": {1: 0, 2: 1, 3: 2}},\n]\n# X_train[\"Health\"].value_counts().keys()\nfloat(ordinal_vars_mapping[0][\"mapping\"][\n    list(filter(lambda x: x in ordinal_vars_mapping[0][\"mapping\"],\n                X[\"MaturitySize\"].value_counts().keys()))[0]])","metadata":{"execution":{"iopub.status.busy":"2021-06-29T07:16:04.638782Z","iopub.execute_input":"2021-06-29T07:16:04.639067Z","iopub.status.idle":"2021-06-29T07:16:04.664778Z","shell.execute_reply.started":"2021-06-29T07:16:04.639041Z","shell.execute_reply":"2021-06-29T07:16:04.663577Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ReplaceState(BaseEstimator, TransformerMixin):\n    def __init__(self, gdp_per_capita, impute_nan_value):\n        self.gdp_per_capita = gdp_per_capita\n        self.impute_nan_value = impute_nan_value\n        \n    def fit(self, X, y):\n        return self\n    \n    def transform(self, X, y=None):\n        X = X.copy()\n        X[\"State\"].replace({\"Melaka\": \"Malacca\", \"Pulau Pinang\": \"Penang\"}, inplace=True)\n        X[\"StateGDP\"] = pd.to_numeric(X[\"State\"].replace(self.gdp_per_capita))\n        X.loc[X[\"StateGDP\"].isna(), \"StateGDP\"] = self.impute_nan_value\n        return X","metadata":{"execution":{"iopub.status.busy":"2021-06-29T07:16:04.666816Z","iopub.execute_input":"2021-06-29T07:16:04.667147Z","iopub.status.idle":"2021-06-29T07:16:04.675763Z","shell.execute_reply.started":"2021-06-29T07:16:04.667114Z","shell.execute_reply":"2021-06-29T07:16:04.67454Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ReplaceRescuerID(BaseEstimator, TransformerMixin):\n \n    def fit(self, X, y):\n        self.count_encoder = ce.CountEncoder()\n        self.count_encoder.fit(X[\"RescuerID\"])\n        self.most_common_value = X[\"RescuerID\"].value_counts().value_counts().keys()[0]\n        return self\n    \n    def transform(self, X, y=None):\n        X = X.copy()\n        X[\"RescuerCount\"] = self.count_encoder.transform(X[\"RescuerID\"])\n        X.loc[X[\"RescuerCount\"].isna(), \"RescuerCount\"] = self.most_common_value\n        return X","metadata":{"execution":{"iopub.status.busy":"2021-06-29T07:16:04.677224Z","iopub.execute_input":"2021-06-29T07:16:04.677864Z","iopub.status.idle":"2021-06-29T07:16:04.689904Z","shell.execute_reply.started":"2021-06-29T07:16:04.677818Z","shell.execute_reply":"2021-06-29T07:16:04.688884Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CustomDiscretizer(BaseEstimator, TransformerMixin):\n    def __init__(self, bins_age=-1, quantity=True, fee=True, video_amt=True, photo_amt=True):\n        self.bins_age = bins_age\n        self.quantity = quantity\n        self.fee = fee\n        self.video_amt = video_amt\n        self.photo_amt = photo_amt            \n            \n    def fit(self, X, y):\n        if self.bins_age > 1:\n            self.age_discretizer = KBinsDiscretizer(strategy='kmeans', encode='ordinal',\n                                                    n_bins=self.bins_age)\n            self.age_discretizer.fit(X[\"Age\"].values.reshape(-1, 1))\n        return self\n    \n    def transform(self, X, y=None):\n        X = X.copy()\n        if self.bins_age > 1:\n            X[\"Age_disc\"] = self.age_discretizer.transform(X[\"Age\"].values.reshape(-1, 1))\n            X[\"Age_disc\"] = X[\"Age_disc\"].astype('int64')\n        if self.quantity:\n            X.loc[X[\"Quantity\"] <= 1, \"Quantity_disc\"] = 0\n            X.loc[(X[\"Quantity\"] >= 2) & (X[\"Quantity\"] <= 6), \"Quantity_disc\"] = 1\n            X.loc[X[\"Quantity\"] >= 7, \"Quantity_disc\"] = 2\n        if self.fee:\n            X[\"HasFee\"] = np.where(X[\"Fee\"] > 0, 1, 0)\n        if self.video_amt:\n            X[\"HasVideo\"] = np.where(X[\"VideoAmt\"] > 0, 1, 0)\n        if self.photo_amt:\n            X[\"HasPhoto\"] = np.where(X[\"PhotoAmt\"] > 0, 1, 0)   \n        return X\n            ","metadata":{"execution":{"iopub.status.busy":"2021-06-29T07:16:04.691305Z","iopub.execute_input":"2021-06-29T07:16:04.692026Z","iopub.status.idle":"2021-06-29T07:16:04.707816Z","shell.execute_reply.started":"2021-06-29T07:16:04.691981Z","shell.execute_reply":"2021-06-29T07:16:04.706665Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ColumnRemover(BaseEstimator, TransformerMixin):\n    def __init__(self, columns):\n        self.columns = columns\n    \n    def fit(self, X, y):\n        return self\n    \n    def transform(self, X, y=None):\n        X = X.copy()\n        return X.drop(self.columns, axis=1)","metadata":{"execution":{"iopub.status.busy":"2021-06-29T07:16:04.709247Z","iopub.execute_input":"2021-06-29T07:16:04.709954Z","iopub.status.idle":"2021-06-29T07:16:04.725422Z","shell.execute_reply.started":"2021-06-29T07:16:04.709905Z","shell.execute_reply":"2021-06-29T07:16:04.724357Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CustomStandardScaler(BaseEstimator, TransformerMixin):\n    def __init__(self, numeric_columns):\n        self.numeric_columns = numeric_columns\n        \n    def fit(self, X, y):\n        self.columns_to_standardize = self.numeric_columns + \\\n            list(filter(\n            lambda x: (\"Breed\" in str(x) and \"Fur\" not in str(x) and \n                       \"Pure\" not in str(x)) \\\n                        or \"_ordinal\" in str(x) or \"img_\" in str(x) \\\n                        or \"desc_\" in str(x) or \"_num\" in str(x) \\\n                        or \"_mean\" in str(x) or \"_sum\" in str(x) \\\n                        or \"_var\" in str(x),\n                        X.columns))\n        self.means = {}\n        self.stds = {}\n        for column in self.columns_to_standardize:\n            self.means[column] = X[column].mean()\n            self.stds[column] = X[column].std()\n        return self\n    \n    def transform(self, X, y=None):\n        X = X.copy()\n        for column in self.columns_to_standardize:\n            mean = self.means[column]\n            std = self.stds[column]\n            X[column] = (X[column] - mean) / std if std > 0 else 0\n        return X","metadata":{"execution":{"iopub.status.busy":"2021-06-29T07:18:33.794755Z","iopub.execute_input":"2021-06-29T07:18:33.795272Z","iopub.status.idle":"2021-06-29T07:18:33.806456Z","shell.execute_reply.started":"2021-06-29T07:18:33.795218Z","shell.execute_reply":"2021-06-29T07:18:33.80541Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class PrintStep(BaseEstimator, TransformerMixin):\n    def fit(self, X, y):\n        return self\n    \n    def transform(self, X, y=None):\n        display(X)\n        display(X.info())\n        return X.copy()\n    \n    def fit_transform(self, X, y=None):\n        return self.fit(X, y).transform(X, y)","metadata":{"execution":{"iopub.status.busy":"2021-06-28T08:38:44.022311Z","iopub.execute_input":"2021-06-28T08:38:44.02286Z","iopub.status.idle":"2021-06-28T08:38:44.037262Z","shell.execute_reply.started":"2021-06-28T08:38:44.022818Z","shell.execute_reply":"2021-06-28T08:38:44.036139Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gdp_per_capita = {\n    \"Kuala Lumpur\": 129472,\n    \"Labuan\": 77798,\n    \"Penang\": 55243,\n    \"Selangor\": 54995,\n    \"Sarawak\": 53358,\n    \"Malacca\": 49172,\n    \"Negeri Sembilan\": 45373,\n    \"Johor\": 37342,\n    \"Pahang\": 36474,\n    \"Perak\": 31668,\n    \"Terengganu\": 30933,\n    \"Perlis\": 25656,\n    \"Sabah\": 25326,\n    \"Kedah\": 22412,\n    \"Kelantan\": 14300\n}\n\nordinal_vars_mapping = [\n    {\"col\": \"MaturitySize\", \"mapping\": {\"Small\": 0, \"Medium\": 1, \"Large\": 2, \"Extra Large\": 3}},\n    {\"col\": \"FurLength\", \"mapping\": {\"Short\": 0, \"Medium\": 1, \"Long\": 2}},\n    {\"col\": \"Health\", \"mapping\": {\"Healthy\": 0, \"Minor Injury\": 1, \"Serious Injury\": 2}},\n]\n\n\ncolumns_to_be_removed_1 = [\"Name\", \"Breed1\", \"Breed2\", \"Gender\", \"Color1\", \"Color2\", \"Color3\",\n                         \"Vaccinated\", \"Dewormed\", \"Sterilized\", \"State\", \"RescuerID\", \"Description\",\n                         \"PetID\", \"MaturitySize\", \"FurLength\", \"Health\"]\n\nnumeric_columns_1 = [\"Age\", \"Quantity\", \"Fee\", \"VideoAmt\", \"PhotoAmt\", \"StateGDP\", \"RescuerCount\"]\n\n\npipeline_1_transformers = [\n    ('replace_breeds', LeftJoinReplace(values_dict=breeds_dict,\n                                      variables=[\"Breed1\", \"Breed2\"])),\n    ('replace_colors', LeftJoinReplace(values_dict=colors_dict,\n                                      variables=[\"Color1\", \"Color2\", \"Color3\"])),\n    ('replace_states', LeftJoinReplace(values_dict=states_dict,\n                                     variables=[\"State\"])),\n    ('replace_by_strings', FunctionTransformer(func=replace_integers_by_strings)),\n    ('has_name', FunctionTransformer(func=has_significant_name)),\n    ('pure_breed', FunctionTransformer(func=has_pure_breed)),\n    ('breed_matches_fur_length', FunctionTransformer(func=breed_matches_fur_length)),\n    ('impute_breed', BreedImputer()),\n    ('encode_breed', BreedEncoding(enc_type=\"target_and_frequency\", minimum_group_threshold=1, smoothing=1.0)),\n    ('one_hot_encoder', CustomOneHotEncoder(columns=[\"Gender\", \"Color1\", \"Color2\",\n                                \"Color3\", \"Vaccinated\", \"Dewormed\", \"Sterilized\"])),\n    ('ordinal_vars_encoder', OrdinalVariableEncoder(columns=[\"MaturitySize\", \"FurLength\", \"Health\"],\n                                enc_type=\"ordinal\", mapping=ordinal_vars_mapping)),\n    ('state_gdp', ReplaceState(gdp_per_capita=gdp_per_capita, impute_nan_value=46450)),\n    ('rescuer_count', ReplaceRescuerID()),\n    ('discretizer', CustomDiscretizer(bins_age=-1, quantity=False, fee=False, video_amt=False,\n                                     photo_amt=False)),\n    ('drop_columns', ColumnRemover(columns=columns_to_be_removed_1)),\n    ('custom_standard_scaler', CustomStandardScaler(numeric_columns_1))\n]\n\npipeline = Pipeline(steps=pipeline_1_transformers)\nresult = pipeline.fit_transform(X,y)\n# clear_output(wait=True)\ndisplay(result)","metadata":{"execution":{"iopub.status.busy":"2021-06-29T07:18:38.86521Z","iopub.execute_input":"2021-06-29T07:18:38.865584Z","iopub.status.idle":"2021-06-29T07:18:41.10319Z","shell.execute_reply.started":"2021-06-29T07:18:38.865544Z","shell.execute_reply":"2021-06-29T07:18:41.102037Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**average classification report -->** https://stackoverflow.com/a/42567557\n\n**https://arxiv.org/pdf/2008.05756.pdf**","metadata":{}},{"cell_type":"code","source":"def plot_precision_recall_curves(ax, y_test, y_score):\n    y_test = label_binarize(y_test, classes=[0, 1, 2, 3, 4])\n    n_classes = y_test.shape[1]\n    # For each class\n    precision = dict()\n    recall = dict()\n    average_precision = dict()\n    for i in range(n_classes):\n        precision[i], recall[i], _ = precision_recall_curve(y_test[:, i],\n                                                            y_score[:, i])\n        average_precision[i] = average_precision_score(y_test[:, i], y_score[:, i])\n\n    # A \"micro-average\": quantifying score on all classes jointly\n    precision[\"micro\"], recall[\"micro\"], _ = precision_recall_curve(y_test.ravel(),\n        y_score.ravel())\n    average_precision[\"micro\"] = average_precision_score(y_test, y_score,\n                                                         average=\"micro\")\n    # setup plot details\n    colors = cycle(['navy', 'turquoise', 'darkorange', 'cornflowerblue', 'teal'])\n\n    f_scores = np.linspace(0.2, 0.8, num=4)\n    lines = []\n    labels = []\n    for f_score in f_scores:\n        x = np.linspace(0.01, 1)\n        y = f_score * x / (2 * x - f_score)\n        l, = ax.plot(x[y >= 0], y[y >= 0], color='gray', alpha=0.2)\n        ax.annotate('f1={0:0.1f}'.format(f_score), xy=(0.9, y[45] + 0.02))\n\n    lines.append(l)\n    labels.append('iso-f1 curves')\n    l, = ax.plot(recall[\"micro\"], precision[\"micro\"], color='gold', lw=2)\n    lines.append(l)\n    labels.append('micro-average Precision-recall (area = {0:0.2f})'\n                  ''.format(average_precision[\"micro\"]))\n\n    for i, color in zip(range(n_classes), colors):\n        l, = ax.plot(recall[i], precision[i], color=color, lw=2)\n        lines.append(l)\n        labels.append('Precision-recall for class {0} (area = {1:0.2f})'\n                      ''.format(i, average_precision[i]))\n\n    ax.set_xlim([0.0, 1.0])\n    ax.set_ylim([0.0, 1.05])\n    ax.set_xlabel('Recall')\n    ax.set_ylabel('Precision')\n    ax.set_title('Multi-class Precision-Recall curve')\n    ax.legend(lines, labels, loc=(0.1, -.5), prop=dict(size=12))\n\n    \ndef round_reg_predictions(y_pred, coefficients):\n    y_pred = y_pred.copy()\n    for i, predicted_value in enumerate(y_pred):\n        if predicted_value < coefficients[0]:\n            y_pred[i] = 0\n        elif predicted_value < coefficients[1]:\n            y_pred[i] = 1\n        elif predicted_value < coefficients[2]:\n            y_pred[i] = 2\n        elif predicted_value < coefficients[3]:\n            y_pred[i] = 3\n        else:\n            y_pred[i] = 4\n    y_pred = y_pred.astype('int64')\n    return y_pred\n\n\ndef evaluate_model(model, cv, X, y, model_type, display_results=True,\n                   display_plots=True, coefficients=[0.5, 1.5, 2.5, 3.5]):\n    orig_model = model\n    fit_times = []\n    if model_type == 'regression':\n        rmse_values = []\n    accuracy_scores = []\n    kappa_scores = []\n    confusion_matrices = []\n    \n    # Variables for average classification report\n    original_class = []\n    predicted_class = []\n\n    if model_type == \"classification\":\n        all_y_test = []\n        all_y_score = None\n    \n    for train_index, test_index in cv.split(X, y):\n        print(f\"CV Iteration {len(fit_times)+1}\")\n        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n        \n        model = clone(orig_model)\n        start = time.time() \n        with warnings.catch_warnings():\n            warnings.simplefilter(\"ignore\")\n            model.fit(X_train, y_train)\n        end = time.time()\n        \n        fit_times.append(end-start)\n        \n        y_pred = model.predict(X_test)\n        \n        if model_type == 'regression':\n            rmse_values.append(mean_squared_error(y_test, y_pred, squared=False))\n            y_pred = round_reg_predictions(y_pred, coefficients)\n        else:\n            all_y_test = np.concatenate((all_y_test, y_test))\n            if all_y_score is None:\n                all_y_score = model.predict_proba(X_test)\n            else:\n                all_y_score = np.concatenate((all_y_score, model.predict_proba(X_test)))\n        \n        original_class.extend(y_test)\n        predicted_class.extend(y_pred)\n        accuracy_scores.append(accuracy_score(y_test, y_pred))\n        \n        kappa_scores.append(cohen_kappa_score(y_test, y_pred, weights='quadratic'))\n        \n        confusion_matrices.append(confusion_matrix(y_test, y_pred, normalize='true'))\n    \n    if display_results:\n        print(\"-----------------RESULTS-----------------\")\n        print(f\"Mean fit time: {np.mean(fit_times)} s\")\n        if model_type == 'regression':\n            print(\"RMSE:\", rmse_values)\n            print(\"Average RMSE:\", np.mean(rmse_values))\n        print(\"\\nAccuracy:\", accuracy_scores)\n        print(\"QWK:\", kappa_scores)\n        print(\"\\nAverage accuracy:\", np.mean(accuracy_scores))\n        print(\"Average QWK:\", np.mean(kappa_scores))\n        print(\"\\nAverage classification report:\")\n        print(classification_report(original_class, predicted_class)) \n        \n        if display_plots:\n            disp = ConfusionMatrixDisplay(confusion_matrix=np.mean(confusion_matrices, axis=0))\n            plt.style.use('default')\n            nrows = 1\n            ncols = 1 if model_type == \"regression\" else 2\n            figsize = (18,6) if model_type == \"classification\" else (6,5)\n            _, ax = plt.subplots(nrows=nrows, ncols=ncols, figsize=figsize)\n            ax = ax.flatten() if model_type == \"classification\" else [ax]\n            ax[0].set_title(\"\\nAverage confusion matrix\", fontsize='18')\n            disp.plot(ax=ax[0])\n            ax[0].grid(False)\n\n            if model_type == \"classification\":\n                plot_precision_recall_curves(ax[1], all_y_test, all_y_score)\n\n            plt.show()\n    \n    if model_type == \"classification\":\n        return np.mean(fit_times), np.mean(accuracy_scores), np.mean(kappa_scores)\n    else:\n        return np.mean(fit_times), np.mean(rmse_values), np.mean(accuracy_scores), np.mean(kappa_scores)","metadata":{"execution":{"iopub.status.busy":"2021-06-29T14:33:49.991315Z","iopub.execute_input":"2021-06-29T14:33:49.991685Z","iopub.status.idle":"2021-06-29T14:33:50.02834Z","shell.execute_reply.started":"2021-06-29T14:33:49.991634Z","shell.execute_reply":"2021-06-29T14:33:50.027065Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We will evaluate the extensions of the previous base pipeline in a 5-CV with 4 different models, just to have an estimation of how much performance improves and also in order to determine which models are better to be used in hyperparameter tuning. Moreover, as the CNN that we will see in later sections are trained on a single training-validation split, we will also evaluate the performance on that split:","metadata":{}},{"cell_type":"code","source":"cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=seed)","metadata":{"execution":{"iopub.status.busy":"2021-06-29T07:18:54.676715Z","iopub.execute_input":"2021-06-29T07:18:54.677066Z","iopub.status.idle":"2021-06-29T07:18:54.681755Z","shell.execute_reply.started":"2021-06-29T07:18:54.677036Z","shell.execute_reply":"2021-06-29T07:18:54.680711Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"https://scikit-learn.org/stable/auto_examples/model_selection/plot_roc.html\n\n**Why precision-recall instead of ROC in this case --> https://machinelearningmastery.com/roc-curves-and-precision-recall-curves-for-classification-in-python/**\n\nhttps://scikit-learn.org/stable/auto_examples/model_selection/plot_precision_recall.html\n\nhttps://scikit-learn.org/stable/modules/generated/sklearn.metrics.average_precision_score.html#sklearn.metrics.average_precision_score\n\nhttps://scikit-learn.org/stable/modules/model_evaluation.html#precision-recall-f-measure-metrics\n\nhttps://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_recall_curve.html#sklearn.metrics.precision_recall_curve","metadata":{}},{"cell_type":"markdown","source":"https://buildmedia.readthedocs.org/media/pdf/xgboost/latest/xgboost.pdf **1.5.9 --> non-determinism... (see also https://stackoverflow.com/questions/65523909/what-features-of-xgboost-are-affected-by-seed-random-state)**","metadata":{}},{"cell_type":"markdown","source":"**(all the cells below need X_train_CNN, y_train_CNN, X_val_CNN and y_val_CNN, defined at the beginning of the Training + image features extraction section)**","metadata":{}},{"cell_type":"code","source":"xgb_classifier = xgb.XGBClassifier(eval_metric='mlogloss', random_state=seed, n_jobs=-1,\n                          use_label_encoder=False)\n\nrandom_forest_clf = RandomForestClassifier(n_jobs=-1, random_state=seed)\n\nsvc_clf = SVC(break_ties=True, probability=True, random_state=seed)\n\nlogistic_regression = LogisticRegression(n_jobs=-1, random_state=seed)\n\nmodels = {\n    \"XGBClassifier\": xgb_classifier,\n    \"RandomForestClassifier\": random_forest_clf,\n    \"SVC (rbf kernel)\": svc_clf,\n    \"Logistic Regression\": logistic_regression\n}\n\nglobal_evaluation_results_1 = pd.DataFrame(\n    [],\n    columns=[\"Pipeline\", \"Model\", \"Average fit time\", \"Average accuracy\",\n             \"Average QWK\", \"Single split accuracy\", \"Single split QWK\"]\n)\n\nfor model_desc, model in models.items():\n    print(f\"--------------------- MODEL: {model_desc} ---------------------\")\n    avg_fit_time, avg_acc, avg_qwk = evaluate_model(\n        Pipeline(steps=pipeline_1_transformers + [('model', model)]),\n        cv, X, y, model_type=\"classification\")\n    \n    single_accuracy, single_QWK = evaluate_model_single_split(\n                Pipeline(steps=pipeline_1_transformers + [('model', model)]),\n                X_train_CNN, X_val_CNN, y_train_CNN, y_val_CNN,\n                model_type=\"classification\", display_results=True)\n    \n    global_evaluation_results_1 = global_evaluation_results_1.append({\n        \"Pipeline\": 1,\n        \"Model\": model_desc,\n        \"Average fit time\": avg_fit_time,\n        \"Average accuracy\": avg_acc,\n        \"Average QWK\": avg_qwk,\n        \"Single split accuracy\": single_accuracy,\n        \"Single split QWK\": single_QWK\n    }, ignore_index=True)","metadata":{"execution":{"iopub.status.busy":"2021-06-28T08:40:00.844796Z","iopub.execute_input":"2021-06-28T08:40:00.845193Z","iopub.status.idle":"2021-06-28T08:51:41.74543Z","shell.execute_reply.started":"2021-06-28T08:40:00.845158Z","shell.execute_reply":"2021-06-28T08:51:41.744503Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Adding description metadata","metadata":{}},{"cell_type":"code","source":"def include_description_length(X):\n    X = X.copy()\n    X[\"DescriptionLength\"] = X[\"Description\"].fillna('').map(lambda x: len(str(x)) if str(x) else 0)\n    return X","metadata":{"execution":{"iopub.status.busy":"2021-06-29T07:20:04.747006Z","iopub.execute_input":"2021-06-29T07:20:04.747594Z","iopub.status.idle":"2021-06-29T07:20:04.754546Z","shell.execute_reply.started":"2021-06-29T07:20:04.747543Z","shell.execute_reply":"2021-06-29T07:20:04.753396Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# path = '../input/tfg-pet-adoption-data/train_description_metadata.json'\n# description_metadata_json = utils.load_json(path)\n# description_metadata = pd.DataFrame.from_dict(description_metadata_json, orient='index')\n# description_metadata.drop([\"DescriptionNumEntities\"], axis=1, inplace=True)\n# description_metadata.rename(lambda x: x if x == \"DescriptionLanguage\" else x + \"_num\", axis=1, inplace=True)\n# description_metadata.to_csv(\"train_description_metadata.csv\")\n\ndescription_metadata = pd.read_csv(\n    \"../input/tfg-pet-adoption-data/train_description_metadata.csv\",\n    index_col=0)\ndescription_metadata","metadata":{"execution":{"iopub.status.busy":"2021-06-29T07:20:05.288087Z","iopub.execute_input":"2021-06-29T07:20:05.288564Z","iopub.status.idle":"2021-06-29T07:20:05.353049Z","shell.execute_reply.started":"2021-06-29T07:20:05.28852Z","shell.execute_reply":"2021-06-29T07:20:05.351611Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class IncludeDescriptionMetadata(BaseEstimator, TransformerMixin):\n    def __init__(self, description_metadata):\n        self.description_metadata = description_metadata\n        \n    def fit(self, X, y):\n        return self\n    \n    def transform(self, X, y=None):\n        X = X.copy()\n        X = X.merge(self.description_metadata, left_index=False, right_index=True,\n                    left_on=\"PetID\", how=\"left\")\n        X.loc[\n            (X[\"Description\"].isna()) | (X[\"Description\"] == ''),\n            [\"DescriptionScore_num\", \"DescriptionMagnitude_num\", \"DescriptionMaxSalience_num\"]\n        ] = 0.0\n        return X","metadata":{"execution":{"iopub.status.busy":"2021-06-29T07:20:05.991418Z","iopub.execute_input":"2021-06-29T07:20:05.991796Z","iopub.status.idle":"2021-06-29T07:20:05.999637Z","shell.execute_reply.started":"2021-06-29T07:20:05.991765Z","shell.execute_reply":"2021-06-29T07:20:05.998448Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CorrectDescriptionLanguage(BaseEstimator, TransformerMixin):\n    def __init__(self, minimum_group_size=10):\n        self.minimum_group_size = minimum_group_size\n        self.languages_freq = {}\n        \n    def fit(self, X, y):\n        self.languages_freq = dict(X[\"DescriptionLanguage\"].value_counts())\n        self.languages_freq[\"ms\"] = X[(X[\"Description\"].notna()) &\n                                         (X[\"DescriptionLanguage\"].isna())].shape[0]\n        freq_other_languages = 0\n        languages_to_remove = []\n        for language, freq in self.languages_freq.items():\n            if freq < self.minimum_group_size:\n                freq_other_languages += freq\n                languages_to_remove.append(language)\n        for language in languages_to_remove:\n            del self.languages_freq[language]\n        self.languages_freq[\"others\"] = freq_other_languages\n        return self\n    \n    def transform(self, X, y=None):\n        X = X.copy()\n        X.loc[(X[\"Description\"].notna()) & (X[\"DescriptionLanguage\"].isna()),\n              \"DescriptionLanguage\"] = \"ms\" # ISO code for Malay language\n        X.loc[~(X[\"DescriptionLanguage\"].isin(self.languages_freq)) &\n              (X[\"Description\"].notna()) &\n              (X[\"Description\"] != ''), \"DescriptionLanguage\"] = \"others\"\n        return X","metadata":{"execution":{"iopub.status.busy":"2021-06-29T07:20:07.006834Z","iopub.execute_input":"2021-06-29T07:20:07.007235Z","iopub.status.idle":"2021-06-29T07:20:07.018924Z","shell.execute_reply.started":"2021-06-29T07:20:07.007198Z","shell.execute_reply":"2021-06-29T07:20:07.017711Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Is it worth to impute missing values (Malay descriptions) using a strategy like KNN or iterative imputing using a decision tree, given that there are more than 500 such cases and some of these description metadata's variables are skewed, it could be worth. We can try different strategies, also using the mean, and see how convenient each one could be.","metadata":{}},{"cell_type":"code","source":"class CustomIterativeImputer(BaseEstimator, TransformerMixin):\n    def __init__(self, estimator=DecisionTreeRegressor(max_depth=5, random_state=seed)):\n        self.estimator = estimator\n        \n        \n    def fit(self, X, y):\n        self.estimator = clone(self.estimator)\n        self.imputer = IterativeImputer(estimator=self.estimator,\n                                        skip_complete=True,\n                                        random_state=seed)\n        self.columns_to_impute = list(filter(\n            lambda x: \"img_\" not in str(x) and \"desc_\" not in str(x), X.columns))\n        self.imputer.fit(X[self.columns_to_impute])\n        return self\n    \n    \n    def transform(self, X, y=None):\n        X = X.copy()\n        X[self.columns_to_impute] = self.imputer.transform(X[self.columns_to_impute])\n        return X","metadata":{"execution":{"iopub.status.busy":"2021-06-29T07:20:09.110669Z","iopub.execute_input":"2021-06-29T07:20:09.11109Z","iopub.status.idle":"2021-06-29T07:20:09.11935Z","shell.execute_reply.started":"2021-06-29T07:20:09.111049Z","shell.execute_reply":"2021-06-29T07:20:09.118226Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"columns_to_be_removed_2 = [\"Name\", \"Breed1\", \"Breed2\", \"Gender\", \"Color1\", \"Color2\", \"Color3\",\n                         \"Vaccinated\", \"Dewormed\", \"Sterilized\", \"State\", \"RescuerID\", \"PetID\",\n                        \"MaturitySize\", \"FurLength\", \"Health\", \"DescriptionLanguage\", \"Description\"]\n\nnumeric_columns_2 = [\"Age\", \"Quantity\", \"Fee\", \"VideoAmt\", \"PhotoAmt\", \"StateGDP\", \"RescuerCount\",\n                   \"DescriptionLength\"]\n\npipeline_2_transformers = [\n    ('replace_breeds', LeftJoinReplace(values_dict=breeds_dict,\n                                      variables=[\"Breed1\", \"Breed2\"])),\n    ('replace_colors', LeftJoinReplace(values_dict=colors_dict,\n                                      variables=[\"Color1\", \"Color2\", \"Color3\"])),\n    ('replace_states', LeftJoinReplace(values_dict=states_dict,\n                                     variables=[\"State\"])),\n    ('replace_by_strings', FunctionTransformer(func=replace_integers_by_strings)),\n    ('has_name', FunctionTransformer(func=has_significant_name)),\n    ('pure_breed', FunctionTransformer(func=has_pure_breed)),\n    ('breed_matches_fur_length', FunctionTransformer(func=breed_matches_fur_length)),\n    ('impute_breed', BreedImputer()),\n    ('encode_breed', BreedEncoding(enc_type=\"target_and_frequency\")),\n    ('ordinal_vars_encoder', OrdinalVariableEncoder(columns=[\"MaturitySize\", \"FurLength\", \"Health\"],\n                                enc_type=\"ordinal\", mapping=ordinal_vars_mapping)),\n    ('state_gdp', ReplaceState(gdp_per_capita=gdp_per_capita, impute_nan_value=46450)),\n    ('rescuer_count', ReplaceRescuerID()),\n    ('discretizer', CustomDiscretizer(bins_age=-1, quantity=False, fee=False, video_amt=False,\n                                     photo_amt=False)),\n    ('description_length', FunctionTransformer(func=include_description_length)),\n    ('include_desc_metadata', IncludeDescriptionMetadata(description_metadata=description_metadata)),\n    ('correct_desc_language', CorrectDescriptionLanguage()),\n    ('one_hot_encoder', CustomOneHotEncoder(columns=[\"Gender\", \"Color1\", \"Color2\",\n                            \"Color3\", \"Vaccinated\", \"Dewormed\", \"Sterilized\", \"DescriptionLanguage\"])),\n    ('drop_columns', ColumnRemover(columns=columns_to_be_removed_2)),\n    ('impute_malay_desc', CustomIterativeImputer()),\n    ('custom_standard_scaler', CustomStandardScaler(numeric_columns_2)),\n]\n\npipeline = Pipeline(steps=pipeline_2_transformers)\nwith warnings.catch_warnings():\n    warnings.simplefilter(\"ignore\")\n    result = pipeline.fit_transform(X,y)\ndisplay(result)","metadata":{"execution":{"iopub.status.busy":"2021-06-29T07:20:11.837357Z","iopub.execute_input":"2021-06-29T07:20:11.837778Z","iopub.status.idle":"2021-06-29T07:20:15.132872Z","shell.execute_reply.started":"2021-06-29T07:20:11.837741Z","shell.execute_reply":"2021-06-29T07:20:15.131811Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xgb_classifier = xgb.XGBClassifier(eval_metric='mlogloss', random_state=seed, n_jobs=-1,\n                          use_label_encoder=False)\n\nrandom_forest_clf = RandomForestClassifier(n_jobs=-1, random_state=seed)\n\nsvc_clf = SVC(break_ties=True, probability=True, random_state=seed)\n\nlogistic_regression = LogisticRegression(n_jobs=-1, random_state=seed)\n\nmodels = {\n    \"XGBClassifier\": xgb_classifier,\n    \"RandomForestClassifier\": random_forest_clf,\n    \"SVC (rbf kernel)\": svc_clf,\n    \"Logistic Regression\": logistic_regression\n}\n\nglobal_evaluation_results_2 = pd.DataFrame(\n    [],\n    columns=[\"Pipeline\", \"Model\", \"Average fit time\", \"Average accuracy\",\n             \"Average QWK\", \"Single split accuracy\", \"Single split QWK\"]\n)\n\nfor model_desc, model in models.items():\n    print(f\"--------------------- MODEL: {model_desc} ---------------------\")\n    avg_fit_time, avg_acc, avg_qwk = evaluate_model(\n        Pipeline(steps=pipeline_2_transformers + [('model', model)]),\n        cv, X, y, model_type=\"classification\")\n    \n    single_accuracy, single_QWK = evaluate_model_single_split(\n                Pipeline(steps=pipeline_2_transformers + [('model', model)]),\n                X_train_CNN, X_val_CNN, y_train_CNN, y_val_CNN,\n                model_type=\"classification\", display_results=True)\n    \n    global_evaluation_results_2 = global_evaluation_results_2.append({\n        \"Pipeline\": 2,\n        \"Model\": model_desc,\n        \"Average fit time\": avg_fit_time,\n        \"Average accuracy\": avg_acc,\n        \"Average QWK\": avg_qwk,\n        \"Single split accuracy\": single_accuracy,\n        \"Single split QWK\": single_QWK\n    }, ignore_index=True)","metadata":{"execution":{"iopub.status.busy":"2021-06-28T08:51:44.65602Z","iopub.execute_input":"2021-06-28T08:51:44.656415Z","iopub.status.idle":"2021-06-28T09:05:15.453551Z","shell.execute_reply.started":"2021-06-28T08:51:44.656385Z","shell.execute_reply":"2021-06-28T09:05:15.452658Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Adding profile image metadata","metadata":{}},{"cell_type":"code","source":"# path = '../input/tfg-pet-adoption-data/train_profile_images_metadata.json'\n# profile_image_metadata_json = utils.load_json(path)\n# profile_image_metadata = pd.DataFrame.from_dict(profile_image_metadata_json, orient='index')\n# profile_image_metadata.drop([\"faces\"], axis=1, inplace=True)\n# profile_image_metadata.rename(columns={\n#     'sum_pixelFraction':'ImageMetadataSumPixelFraction_num',\n#     'max_pet_topicality':'ImageMetadataMaxPetTopicality_num',\n#     'num_entities':'ImageMetadataNumEntities_num',\n#     'desc_concatenation':'ImageMetadataDescription',\n#     'has_text':'ImageMetadataHasText'}, inplace=True)\n# profile_image_metadata.to_csv(\"train_profile_images_metadata.csv\")\n\nprofile_image_metadata = pd.read_csv(\n    \"../input/tfg-pet-adoption-data/train_profile_images_metadata.csv\",\n    index_col=0)\nprofile_image_metadata","metadata":{"execution":{"iopub.status.busy":"2021-06-29T07:21:58.663201Z","iopub.execute_input":"2021-06-29T07:21:58.663584Z","iopub.status.idle":"2021-06-29T07:21:58.783858Z","shell.execute_reply.started":"2021-06-29T07:21:58.663548Z","shell.execute_reply":"2021-06-29T07:21:58.782935Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# path = \"../input/tfg-pet-adoption-data/train_all_images_metadata.json\"\n# all_train_images_metadata_json = utils.load_json(path)\n# all_train_images_metadata = pd.DataFrame.from_dict(all_train_images_metadata_json, orient='index')\n# all_train_images_metadata.drop([\"faces\"], axis=1, inplace=True)\n# all_train_images_metadata.rename(columns={\n#     'sum_pixelFraction':'ImageMetadataSumPixelFraction',\n#     'max_pet_topicality':'ImageMetadataMaxPetTopicality',\n#     'num_entities':'ImageMetadataNumEntities',\n#     'desc_concatenation':'ImageMetadataDescription',\n#     'has_text':'ImageMetadataHasText'}, inplace=True)\n# all_train_images_metadata.to_csv(\"all_train_images_metadata.csv\")\n\nall_images_metadata = pd.read_csv(\n    \"../input/tfg-pet-adoption-data/all_train_images_metadata.csv\",\n    index_col=0)\nall_images_metadata","metadata":{"execution":{"iopub.status.busy":"2021-06-29T07:21:59.935392Z","iopub.execute_input":"2021-06-29T07:21:59.935818Z","iopub.status.idle":"2021-06-29T07:22:00.347854Z","shell.execute_reply.started":"2021-06-29T07:21:59.93578Z","shell.execute_reply":"2021-06-29T07:22:00.346808Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def aggregate_images_metadata(all_images_metadata, type_data=\"train\"):\n    all_images_metadata = all_images_metadata.reset_index()\n    all_images_metadata[\"ImageMetadataHasText\"] = all_images_metadata[\n        \"ImageMetadataHasText\"].astype('int64')\n    all_images_metadata.rename(columns={\"index\": \"PetID\"}, inplace=True)\n    all_images_metadata[\"PetID\"] = all_images_metadata[\n        \"PetID\"].apply(lambda x: str(x)[:str(x).rindex('-')]) #filename to PetID\n    \n    # Split numbers, boolean (ImageHasText) and text (ImageDescription)\n    all_images_metadata_num = all_images_metadata[\n        [\"PetID\", \"ImageMetadataSumPixelFraction\", \"ImageMetadataMaxPetTopicality\",\n         \"ImageMetadataNumEntities\"]\n    ].copy()\n    all_images_metadata_bool = all_images_metadata[[\"PetID\", \"ImageMetadataHasText\"]].copy()\n    all_images_metadata_text = all_images_metadata[\n        [\"PetID\", \"ImageMetadataDescription\"]].copy()\n    all_images_metadata_text[\"ImageMetadataDescription\"] = \\\n        all_images_metadata_text[\"ImageMetadataDescription\"].astype(str)\n    \n    # Aggregate numbers\n    all_images_metadata_num_agg = all_images_metadata_num.groupby(\n        [\"PetID\"]).agg([\"mean\", \"sum\", \"var\"])\n    all_images_metadata_num_agg.columns = \\\n        [f\"{x}_{y}\" if y != \"\" else f\"{x}\" for x,y in all_images_metadata_num_agg.columns.to_flat_index()]\n    all_images_metadata_num_agg.fillna(0, inplace=True)\n    \n    # Aggregate booleans (sum)\n    all_images_metadata_bool_agg = all_images_metadata_bool.groupby(\n        [\"PetID\"]).sum()\n    \n    # Aggregate texts\n    all_images_metadata_text_agg = \\\n        all_images_metadata_text.groupby([\"PetID\"])[\"ImageMetadataDescription\"].apply(lambda x: \" \".join(x))\n    \n    all_images_metadata_agg = all_images_metadata_num_agg.merge(\n        all_images_metadata_text_agg, left_index=True, right_index=True)\n    all_images_metadata_agg = all_images_metadata_agg.merge(\n        all_images_metadata_bool_agg, left_index=True, right_index=True)\n    \n    all_images_metadata_agg.rename(columns={\"ImageMetadataHasText\": \"ImageMetadataHasText_sum\"},\n                                   inplace=True)\n    all_images_metadata_agg.to_csv(f\"all_{type_data}_images_metadata_agg.csv\")\n    \n    return all_images_metadata_agg","metadata":{"execution":{"iopub.status.busy":"2021-06-29T07:22:00.702965Z","iopub.execute_input":"2021-06-29T07:22:00.703362Z","iopub.status.idle":"2021-06-29T07:22:00.715917Z","shell.execute_reply.started":"2021-06-29T07:22:00.703324Z","shell.execute_reply":"2021-06-29T07:22:00.71448Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# all_train_images_metadata_text_agg = aggregate_images_metadata(all_train_images_metadata)\nall_images_metadata_text_agg = pd.read_csv(\n    \"../input/tfg-pet-adoption-data/all_train_images_metadata_agg.csv\",\n    index_col=0)\nall_images_metadata_text_agg","metadata":{"execution":{"iopub.status.busy":"2021-06-29T07:22:02.089428Z","iopub.execute_input":"2021-06-29T07:22:02.090016Z","iopub.status.idle":"2021-06-29T07:22:02.395825Z","shell.execute_reply.started":"2021-06-29T07:22:02.089964Z","shell.execute_reply":"2021-06-29T07:22:02.394822Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class IncludeProfileImageMetadata(BaseEstimator, TransformerMixin):\n    def __init__(self, profile_image_metadata, all_images_metadata_agg=None,\n                 aggregate_metadata=False):\n        self.profile_image_metadata = profile_image_metadata\n        self.all_images_metadata_agg = all_images_metadata_agg\n        self.aggregate_metadata = aggregate_metadata\n    \n    def fit(self, X, y):\n        return self\n    \n    def transform(self, X, y=None):\n        X = X.copy()\n        if self.aggregate_metadata:\n            metadata = self.all_images_metadata_agg\n        else:\n            metadata = self.profile_image_metadata\n        X = X.merge(metadata, left_index=False, right_index=True,\n                    left_on=\"PetID\", how=\"left\")\n        columns = list(filter(lambda x: \"ImageMetadata\" in x and x != \"ImageMetadataDescription\", X.columns))\n        X.loc[(X[\"PhotoAmt\"] == 0), columns] = 0\n        X.loc[(X[\"PhotoAmt\"] == 0), \"ImageMetadataDescription\"] = ''\n        return X","metadata":{"execution":{"iopub.status.busy":"2021-06-29T07:22:03.048427Z","iopub.execute_input":"2021-06-29T07:22:03.04881Z","iopub.status.idle":"2021-06-29T07:22:03.058837Z","shell.execute_reply.started":"2021-06-29T07:22:03.048777Z","shell.execute_reply":"2021-06-29T07:22:03.057983Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CorrectWrongType(BaseEstimator, TransformerMixin):\n    def __init__(self, breeds_file,\n                 cat_equivalents={'cat', 'kitty', 'kitten', 'cats', 'kitties', 'kittens'},\n                 dog_equivalents={'dog', 'doggie', 'doggy', 'doggo', 'pup', 'puppy',\n                                 'dogs', 'pups', 'puppies'},\n                 delete_characters_desc=set(string.punctuation)):\n        \n        self.breeds_file = breeds_file\n        self.cat_equivalents = cat_equivalents\n        self.dog_equivalents = dog_equivalents\n        self.delete_characters_desc = delete_characters_desc\n       \n    \n    def fit(self, X, y):\n        self.breeds_file = self.breeds_file.copy()\n        utils.replace_val_categorical(self.breeds_file,\n                                      replace_dict={'Type': {1: 'Dog', 2: 'Cat'}})\n        self.most_common_breed = {}\n        self.most_common_breed[\"Cat\"] = X.loc[X[\"Type\"] == \"Cat\",\n                                              \"Breed1\"].value_counts().keys()[0]\n        self.most_common_breed[\"Dog\"] = X.loc[X[\"Type\"] == \"Dog\",\n                                              \"Breed1\"].value_counts().keys()[0]\n        return self\n    \n    \n    def transform(self, X, y=None):\n        X = X.copy()\n        for index, row in X[(X[\"ImageMetadataDescription\"].notna()) |\n                            (X[\"ImageMetadataDescription\"] != '')].iterrows():\n            if str(row[\"Type\"]).lower() not in str(row[\"ImageMetadataDescription\"]).lower() \\\n                and ((\"ImageMetadataMaxPetTopicality\" in X.columns and\n                      row[\"ImageMetadataMaxPetTopicality\"] > 0) or \n                     (\"ImageMetadataMaxPetTopicality_mean\" in X.columns and\n                      row[\"ImageMetadataMaxPetTopicality_mean\"] > 0)):\n                \n                type_from_breed = self.breeds_file.loc[\n                        self.breeds_file[\"BreedName\"] == row[\"Breed1\"], \"Type\"].values[0]\n                name_words = set(str(row[\"Name\"]).lower().split())\n                cat_equivalent_in_name = len(self.cat_equivalents & name_words) > 0\n                dog_equivalent_in_name = len(self.dog_equivalents & name_words) > 0\n                \n                # Check if a 'cat' or 'dog' equivalent is in the profile name\n                if cat_equivalent_in_name:\n                    X.loc[index, \"Type\"] = \"Cat\"\n                elif dog_equivalent_in_name:\n                    X.loc[index, \"Type\"] = \"Dog\"\n                else:\n                    # Check if a 'cat' or 'dog' equivalent is in the profile description\n                    # Delete punctuation characters in order to find type equivalents\n                    # ('cat' not found if we leave 'cat.' in the description, for example)\n                    description = str(row[\"Description\"]).lower()\n                    for char in self.delete_characters_desc:\n                        description = description.replace(char, ' ')\n                    description_words = description.split()\n                    num_cat_equivalents = sum(list(map(lambda x: description_words.count(x),\n                                                       self.cat_equivalents)))\n                    num_dog_equivalents = sum(list(map(lambda x: description_words.count(x),\n                                                       self.dog_equivalents)))\n                    \n                    if num_cat_equivalents > num_dog_equivalents:\n                        X.loc[index, \"Type\"] = \"Cat\"\n                    elif num_dog_equivalents > num_cat_equivalents:\n                        X.loc[index, \"Type\"] = \"Dog\"\n                    # Get the Type from the external breeds file\n                    elif type_from_breed == row[\"Type\"]:\n                        X.loc[index, \"Type\"] = \"Cat\"\n                    else:\n                        X.loc[index, \"Type\"] = type_from_breed\n\n                if type_from_breed != X.loc[index, \"Type\"]:\n                    X.loc[index, \"Breed1\"] = self.most_common_breed[X.loc[index, \"Type\"]]\n        \n        return X","metadata":{"execution":{"iopub.status.busy":"2021-06-29T07:22:04.054401Z","iopub.execute_input":"2021-06-29T07:22:04.05496Z","iopub.status.idle":"2021-06-29T07:22:04.080991Z","shell.execute_reply.started":"2021-06-29T07:22:04.054925Z","shell.execute_reply":"2021-06-29T07:22:04.079549Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"columns_to_be_removed_3 = [\"Name\", \"Breed1\", \"Breed2\", \"Gender\", \"Color1\", \"Color2\", \"Color3\",\n                         \"Vaccinated\", \"Dewormed\", \"Sterilized\", \"State\", \"RescuerID\", \"PetID\",\n                         \"MaturitySize\", \"FurLength\", \"Health\", \"DescriptionLanguage\",\n                         \"Description\", \"ImageMetadataDescription\"]\n\nnumeric_columns_3 = [\"Age\", \"Quantity\", \"Fee\", \"VideoAmt\", \"PhotoAmt\", \"StateGDP\", \"RescuerCount\",\n                   \"DescriptionLength\"]\n\npipeline_3_transformers = [\n    ('replace_breeds', LeftJoinReplace(values_dict=breeds_dict,\n                                      variables=[\"Breed1\", \"Breed2\"])),\n    ('replace_colors', LeftJoinReplace(values_dict=colors_dict,\n                                      variables=[\"Color1\", \"Color2\", \"Color3\"])),\n    ('replace_states', LeftJoinReplace(values_dict=states_dict,\n                                     variables=[\"State\"])),\n    ('replace_by_strings', FunctionTransformer(func=replace_integers_by_strings)),\n    ('has_name', FunctionTransformer(func=has_significant_name)),\n    ('pure_breed', FunctionTransformer(func=has_pure_breed)),\n    ('breed_matches_fur_length', FunctionTransformer(func=breed_matches_fur_length)),\n    ('impute_breed', BreedImputer()),\n    ('include_prof_im_metadata', IncludeProfileImageMetadata(profile_image_metadata)),\n    ('correct_wrong_type', CorrectWrongType(breeds)),\n    ('encode_breed', BreedEncoding(enc_type=\"target_and_frequency\")),\n    ('ordinal_vars_encoder', OrdinalVariableEncoder(columns=[\"MaturitySize\", \"FurLength\", \"Health\"],\n                                enc_type=\"ordinal\", mapping=ordinal_vars_mapping)),\n    ('state_gdp', ReplaceState(gdp_per_capita=gdp_per_capita, impute_nan_value=46450)),\n    ('rescuer_count', ReplaceRescuerID()),\n    ('discretizer', CustomDiscretizer(bins_age=-1, quantity=False, fee=False, video_amt=False,\n                                     photo_amt=False)),\n    ('description_length', FunctionTransformer(func=include_description_length)),\n    ('include_desc_metadata', IncludeDescriptionMetadata(description_metadata=description_metadata)),\n    ('correct_desc_language', CorrectDescriptionLanguage()),\n    ('one_hot_encoder', CustomOneHotEncoder(columns=[\"Gender\", \"Color1\", \"Color2\",\n                            \"Color3\", \"Vaccinated\", \"Dewormed\", \"Sterilized\", \"DescriptionLanguage\"])),\n    ('drop_columns', ColumnRemover(columns=columns_to_be_removed_3)),\n    ('impute_malay_desc', CustomIterativeImputer()),\n    ('custom_standard_scaler', CustomStandardScaler(numeric_columns_3))\n]\n\npipeline = Pipeline(steps=pipeline_3_transformers)\nwith warnings.catch_warnings():\n    warnings.simplefilter(\"ignore\")\n    result = pipeline.fit_transform(X,y)\ndisplay(result)","metadata":{"execution":{"iopub.status.busy":"2021-06-29T07:22:06.436898Z","iopub.execute_input":"2021-06-29T07:22:06.43726Z","iopub.status.idle":"2021-06-29T07:22:11.185508Z","shell.execute_reply.started":"2021-06-29T07:22:06.437231Z","shell.execute_reply":"2021-06-29T07:22:11.184422Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xgb_classifier = xgb.XGBClassifier(eval_metric='mlogloss', random_state=seed, n_jobs=-1,\n                          use_label_encoder=False)\n\nrandom_forest_clf = RandomForestClassifier(n_jobs=-1, random_state=seed)\n\nsvc_clf = SVC(break_ties=True, probability=True, random_state=seed)\n\nlogistic_regression = LogisticRegression(n_jobs=-1, random_state=seed)\n\nmodels = {\n    \"XGBClassifier\": xgb_classifier,\n    \"RandomForestClassifier\": random_forest_clf,\n    \"SVC (rbf kernel)\": svc_clf,\n    \"Logistic Regression\": logistic_regression\n}\n\nglobal_evaluation_results_3 = pd.DataFrame(\n    [],\n    columns=[\"Pipeline\", \"Model\", \"Average fit time\", \"Average accuracy\",\n             \"Average QWK\", \"Single split accuracy\", \"Single split QWK\"]\n)\n\nfor model_desc, model in models.items():\n    print(f\"--------------------- MODEL: {model_desc} ---------------------\")\n    avg_fit_time, avg_acc, avg_qwk = evaluate_model(\n        Pipeline(steps=pipeline_3_transformers + [('model', model)]),\n        cv, X, y, model_type=\"classification\")\n    \n    single_accuracy, single_QWK = evaluate_model_single_split(\n                Pipeline(steps=pipeline_3_transformers + [('model', model)]),\n                X_train_CNN, X_val_CNN, y_train_CNN, y_val_CNN,\n                model_type=\"classification\", display_results=True)\n    \n    global_evaluation_results_3 = global_evaluation_results_3.append({\n        \"Pipeline\": 3,\n        \"Model\": model_desc,\n        \"Average fit time\": avg_fit_time,\n        \"Average accuracy\": avg_acc,\n        \"Average QWK\": avg_qwk,\n        \"Single split accuracy\": single_accuracy,\n        \"Single split QWK\": single_QWK\n    }, ignore_index=True)","metadata":{"execution":{"iopub.status.busy":"2021-06-28T09:05:20.604187Z","iopub.execute_input":"2021-06-28T09:05:20.604472Z","iopub.status.idle":"2021-06-28T09:18:34.146103Z","shell.execute_reply.started":"2021-06-28T09:05:20.604442Z","shell.execute_reply":"2021-06-28T09:18:34.14517Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Adding profile image properties","metadata":{}},{"cell_type":"code","source":"# path = '../input/tfg-pet-adoption-data/train_profile_images_properties.json'\n# profile_image_properties_json = utils.load_json(path)\n# profile_image_properties = pd.DataFrame.from_dict(profile_image_properties_json, orient='index')\n# profile_image_properties.rename(columns={\n#     \"dullness\": \"ImagePropertyDullness_num\",\n#     \"whiteness\": \"ImagePropertyWhiteness_num\",\n#     \"blurrness\": \"ImagePropertyBlurrness_num\",\n#     \"size\": \"ImagePropertySize_num\",\n#     \"width\": \"ImagePropertyWidth_num\",\n#     \"height\": \"ImagePropertyHeight_num\"\n# }, inplace=True)\n# profile_image_properties.to_csv(\"train_profile_images_properties.csv\")\n\nprofile_image_properties = pd.read_csv(\n    \"../input/tfg-pet-adoption-data/train_profile_images_properties.csv\",\n    index_col=0)\nprofile_image_properties","metadata":{"execution":{"iopub.status.busy":"2021-06-29T07:23:08.474816Z","iopub.execute_input":"2021-06-29T07:23:08.475215Z","iopub.status.idle":"2021-06-29T07:23:08.553981Z","shell.execute_reply.started":"2021-06-29T07:23:08.475179Z","shell.execute_reply":"2021-06-29T07:23:08.553115Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# path = '../input/tfg-pet-adoption-data/all_train_images_properties.json'\n# all_images_properties_json = utils.load_json(path)\n# all_images_properties = pd.DataFrame.from_dict(all_images_properties_json, orient='index')\n# all_images_properties.drop([\"PetID\"], axis=1, inplace=True)\n# all_images_properties.rename(columns={\n#     \"dullness\": \"ImagePropertyDullness\",\n#     \"whiteness\": \"ImagePropertyWhiteness\",\n#     \"blurrness\": \"ImagePropertyBlurrness\",\n#     \"size\": \"ImagePropertySize\",\n#     \"width\": \"ImagePropertyWidth\",\n#     \"height\": \"ImagePropertyHeight\"\n# }, inplace=True)\n# rounded_width = round(all_images_properties[\"ImagePropertyWidth\"]/100)*100\n# rounded_height = round(all_images_properties[\"ImagePropertyHeight\"]/100)*100\n# all_images_properties[\"ImagePropertyAspectRatio\"] = np.divide(rounded_width, rounded_height, out=np.zeros_like(rounded_width), where=rounded_height!=0)\n# all_images_properties.to_csv(\"all_train_images_properties.csv\")\n\nall_images_properties = pd.read_csv(\n    \"../input/tfg-pet-adoption-data/all_train_images_properties.csv\",\n    index_col=0)\nall_images_properties","metadata":{"execution":{"iopub.status.busy":"2021-06-29T07:23:09.929741Z","iopub.execute_input":"2021-06-29T07:23:09.930244Z","iopub.status.idle":"2021-06-29T07:23:10.125507Z","shell.execute_reply.started":"2021-06-29T07:23:09.930213Z","shell.execute_reply":"2021-06-29T07:23:10.124569Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def aggregate_images_properties(all_images_properties, type_data=\"train\"):\n    all_images_properties = all_images_properties.reset_index()\n    all_images_properties.rename(columns={\"index\": \"PetID\"}, inplace=True)\n    all_images_properties[\"PetID\"] = all_images_properties[\n        \"PetID\"].apply(lambda x: str(x)[:str(x).rindex('-')]) #filename to PetID\n    \n    all_images_properties_agg = all_images_properties.groupby(\n        [\"PetID\"]).agg([\"mean\", \"sum\", \"var\"])\n    all_images_properties_agg.columns = \\\n        [f\"{x}_{y}\" if y != \"\" else f\"{x}\" for x,y in all_images_properties_agg.columns.to_flat_index()]\n    all_images_properties_agg.fillna(0, inplace=True)\n    \n    all_images_properties_agg.to_csv(f\"all_{type_data}_images_properties_agg.csv\")\n    \n    return all_images_properties_agg","metadata":{"execution":{"iopub.status.busy":"2021-06-29T07:23:11.19292Z","iopub.execute_input":"2021-06-29T07:23:11.193387Z","iopub.status.idle":"2021-06-29T07:23:11.202352Z","shell.execute_reply.started":"2021-06-29T07:23:11.193349Z","shell.execute_reply":"2021-06-29T07:23:11.200026Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# all_images_properties_agg = aggregate_images_properties(all_images_properties)\nall_images_properties_agg = pd.read_csv(\n    \"../input/tfg-pet-adoption-data/all_train_images_properties_agg.csv\",\n    index_col=0)\nall_images_properties_agg","metadata":{"execution":{"iopub.status.busy":"2021-06-29T07:23:12.328882Z","iopub.execute_input":"2021-06-29T07:23:12.32927Z","iopub.status.idle":"2021-06-29T07:23:12.496849Z","shell.execute_reply.started":"2021-06-29T07:23:12.329223Z","shell.execute_reply":"2021-06-29T07:23:12.495754Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class IncludeProfileImageProperties(BaseEstimator, TransformerMixin):\n    def __init__(self, profile_image_properties,\n                 aggregated_images_properties=None,\n                 aggregated_properties=False):\n        self.profile_image_properties = profile_image_properties\n        self.aggregated_images_properties = aggregated_images_properties\n        self.aggregated_properties = aggregated_properties\n        \n    def fit(self, X, y):\n        return self\n    \n    def transform(self, X, y=None):\n        X = X.copy()\n        if self.aggregated_properties:\n            properties = self.aggregated_images_properties\n        else:\n            properties = self.profile_image_properties\n        X = X.merge(properties, left_index=False, right_index=True,\n                    left_on=\"PetID\", how=\"left\")\n        columns_nan = list(filter(lambda x: \"ImageProperty\" in x and (\n            \"Dullness\" in x or \"Whiteness\" in x or \"Blurrness\" in x), X.columns))\n        columns_zero = list(filter(lambda x: \"ImageProperty\" in x and (\n            \"Size\" in x or \"Width\" in x or \"Height\" in x or \"AspectRatio\" in x),\n                                   X.columns))\n        X.loc[(X[\"PhotoAmt\"] == 0), columns_nan] = np.nan\n        X.loc[(X[\"PhotoAmt\"] == 0), columns_zero] = 0\n        return X","metadata":{"execution":{"iopub.status.busy":"2021-06-29T07:23:13.647211Z","iopub.execute_input":"2021-06-29T07:23:13.647637Z","iopub.status.idle":"2021-06-29T07:23:13.657911Z","shell.execute_reply.started":"2021-06-29T07:23:13.64759Z","shell.execute_reply":"2021-06-29T07:23:13.656834Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def include_aspect_ratio(X):\n    X = X.copy()\n    if \"ImagePropertyAspectRatio_mean\" not in X.columns:\n        rounded_width = round(X[\"ImagePropertyWidth_num\"]/100)*100\n        rounded_height = round(X[\"ImagePropertyHeight_num\"]/100)*100\n        X[\"ImagePropertyAspectRatio_num\"] = np.divide(\n            rounded_width, rounded_height,\n            out=np.zeros_like(rounded_width),\n            where=rounded_height!=0)\n    return X","metadata":{"execution":{"iopub.status.busy":"2021-06-29T07:23:14.872794Z","iopub.execute_input":"2021-06-29T07:23:14.873191Z","iopub.status.idle":"2021-06-29T07:23:14.880036Z","shell.execute_reply.started":"2021-06-29T07:23:14.873144Z","shell.execute_reply":"2021-06-29T07:23:14.878788Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"columns_to_be_removed_4 = [\"Name\", \"Breed1\", \"Breed2\", \"Gender\", \"Color1\", \"Color2\", \"Color3\",\n                         \"Vaccinated\", \"Dewormed\", \"Sterilized\", \"State\", \"RescuerID\", \"PetID\",\n                         \"MaturitySize\", \"FurLength\", \"Health\", \"DescriptionLanguage\",\n                         \"Description\", \"ImageMetadataDescription\"]\n\nnumeric_columns_4 = [\"Age\", \"Quantity\", \"Fee\", \"VideoAmt\", \"PhotoAmt\", \"StateGDP\", \"RescuerCount\",\n                   \"DescriptionLength\"]\n    \npipeline_4_transformers = [\n    ('replace_breeds', LeftJoinReplace(values_dict=breeds_dict,\n                                      variables=[\"Breed1\", \"Breed2\"])),\n    ('replace_colors', LeftJoinReplace(values_dict=colors_dict,\n                                      variables=[\"Color1\", \"Color2\", \"Color3\"])),\n    ('replace_states', LeftJoinReplace(values_dict=states_dict,\n                                     variables=[\"State\"])),\n    ('replace_by_strings', FunctionTransformer(func=replace_integers_by_strings)),\n    ('has_name', FunctionTransformer(func=has_significant_name)),\n    ('pure_breed', FunctionTransformer(func=has_pure_breed)),\n    ('breed_matches_fur_length', FunctionTransformer(func=breed_matches_fur_length)),\n    ('impute_breed', BreedImputer()),\n    ('include_prof_im_metadata', IncludeProfileImageMetadata(profile_image_metadata)),\n    ('correct_wrong_type', CorrectWrongType(breeds)),\n    ('encode_breed', BreedEncoding(enc_type=\"target_and_frequency\")),\n    ('ordinal_vars_encoder', OrdinalVariableEncoder(columns=[\"MaturitySize\", \"FurLength\", \"Health\"],\n                                enc_type=\"ordinal\", mapping=ordinal_vars_mapping)),\n    ('state_gdp', ReplaceState(gdp_per_capita=gdp_per_capita, impute_nan_value=46450)),\n    ('rescuer_count', ReplaceRescuerID()),\n    ('discretizer', CustomDiscretizer(bins_age=-1, quantity=False, fee=False, video_amt=False,\n                                     photo_amt=False)),\n    ('description_length', FunctionTransformer(func=include_description_length)),\n    ('include_desc_metadata', IncludeDescriptionMetadata(description_metadata=description_metadata)),\n    ('correct_desc_language', CorrectDescriptionLanguage()),\n    ('one_hot_encoder', CustomOneHotEncoder(columns=[\"Gender\", \"Color1\", \"Color2\",\n                            \"Color3\", \"Vaccinated\", \"Dewormed\", \"Sterilized\", \"DescriptionLanguage\"])),\n    ('include_prof_im_properties', IncludeProfileImageProperties(profile_image_properties)),\n    ('drop_columns', ColumnRemover(columns=columns_to_be_removed_4)),\n    ('round_im_dims_aspect_ratio', FunctionTransformer(func=include_aspect_ratio)),\n    ('impute_malay_desc_missing_prof_im_props', CustomIterativeImputer()),\n    ('custom_standard_scaler', CustomStandardScaler(numeric_columns_4))\n]\n\npipeline = Pipeline(steps=pipeline_4_transformers)\nwith warnings.catch_warnings():\n    warnings.simplefilter(\"ignore\")\n    result = pipeline.fit_transform(X,y)\ndisplay(result)","metadata":{"execution":{"iopub.status.busy":"2021-06-29T07:23:16.779073Z","iopub.execute_input":"2021-06-29T07:23:16.779588Z","iopub.status.idle":"2021-06-29T07:23:23.079542Z","shell.execute_reply.started":"2021-06-29T07:23:16.779556Z","shell.execute_reply":"2021-06-29T07:23:23.078072Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xgb_classifier = xgb.XGBClassifier(eval_metric='mlogloss', random_state=seed, n_jobs=-1,\n                          use_label_encoder=False)\n\nrandom_forest_clf = RandomForestClassifier(n_jobs=-1, random_state=seed)\n\nsvc_clf = SVC(break_ties=True, probability=True, random_state=seed)\n\nlogistic_regression = LogisticRegression(n_jobs=-1, random_state=seed)\n\nmodels = {\n    \"XGBClassifier\": xgb_classifier,\n    \"RandomForestClassifier\": random_forest_clf,\n    \"SVC (rbf kernel)\": svc_clf,\n    \"Logistic Regression\": logistic_regression\n}\n\nglobal_evaluation_results_4 = pd.DataFrame(\n    [],\n    columns=[\"Pipeline\", \"Model\", \"Average fit time\", \"Average accuracy\",\n             \"Average QWK\", \"Single split accuracy\", \"Single split QWK\"]\n)\n\nfor model_desc, model in models.items():\n    print(f\"--------------------- MODEL: {model_desc} ---------------------\")\n    avg_fit_time, avg_acc, avg_qwk = evaluate_model(\n        Pipeline(steps=pipeline_4_transformers + [('model', model)]),\n        cv, X, y, model_type=\"classification\")\n    \n    single_accuracy, single_QWK = evaluate_model_single_split(\n                Pipeline(steps=pipeline_4_transformers + [('model', model)]),\n                X_train_CNN, X_val_CNN, y_train_CNN, y_val_CNN,\n                model_type=\"classification\", display_results=True)\n    \n    global_evaluation_results_4 = global_evaluation_results_4.append({\n        \"Pipeline\": 4,\n        \"Model\": model_desc,\n        \"Average fit time\": avg_fit_time,\n        \"Average accuracy\": avg_acc,\n        \"Average QWK\": avg_qwk,\n        \"Single split accuracy\": single_accuracy,\n        \"Single split QWK\": single_QWK\n    }, ignore_index=True)","metadata":{"execution":{"iopub.status.busy":"2021-06-28T09:18:40.333575Z","iopub.execute_input":"2021-06-28T09:18:40.333953Z","iopub.status.idle":"2021-06-28T09:34:12.449321Z","shell.execute_reply.started":"2021-06-28T09:18:40.333921Z","shell.execute_reply":"2021-06-28T09:34:12.44828Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"display(global_evaluation_results_1)\ndisplay(global_evaluation_results_2)\ndisplay(global_evaluation_results_3)\ndisplay(global_evaluation_results_4)","metadata":{"execution":{"iopub.status.busy":"2021-06-28T09:34:12.450766Z","iopub.execute_input":"2021-06-28T09:34:12.451065Z","iopub.status.idle":"2021-06-28T09:34:12.495273Z","shell.execute_reply.started":"2021-06-28T09:34:12.451033Z","shell.execute_reply":"2021-06-28T09:34:12.494195Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# _, ax = plt.subplots(figsize=(15,15))\n# xgb.plot_importance(model, ax=ax)","metadata":{"execution":{"iopub.status.busy":"2021-06-28T09:34:12.496725Z","iopub.execute_input":"2021-06-28T09:34:12.497114Z","iopub.status.idle":"2021-06-28T09:34:12.500792Z","shell.execute_reply.started":"2021-06-28T09:34:12.497072Z","shell.execute_reply":"2021-06-28T09:34:12.500036Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"https://stats.stackexchange.com/questions/421545/multiple-imputation-by-chained-equations-mice-explained#:~:text=MICE%20is%20a%20multiple%20imputation,are%20missing%20completely%20at%20random).\n\nhttps://cran.r-project.org/web/packages/miceRanger/vignettes/miceAlgorithm.html\n\nhttps://contrib.scikit-learn.org/category_encoders/targetencoder.html (https://contrib.scikit-learn.org/category_encoders/_modules/category_encoders/target_encoder.html#TargetEncoder.fit) y https://dl.acm.org/citation.cfm?id=507538\n\nhttps://www.kaggle.com/aroraaman/quadratic-kappa-metric-explained-in-5-simple-steps","metadata":{}},{"cell_type":"markdown","source":"## Extracting features from the images using CNN","metadata":{}},{"cell_type":"markdown","source":"**Base source code to extract image features from: https://www.kaggle.com/christofhenkel/extract-image-features-from-pretrained-nn**","metadata":{}},{"cell_type":"code","source":"import cv2\nimport keras.backend as K\nimport shutil\nimport tensorflow as tf\n\nfrom IPython.display import Image\n\nfrom keras.applications import DenseNet121, DenseNet201, InceptionResNetV2, NASNetLarge, VGG16, Xception\nfrom keras.applications.densenet import preprocess_input as preprocess_input_densenet\nfrom keras.applications.inception_resnet_v2 import preprocess_input as preprocess_input_inceptionresnetv2\nfrom keras.applications.nasnet import preprocess_input as preprocess_input_nasnet\nfrom keras.applications.vgg16 import preprocess_input as preprocess_input_vgg16\nfrom keras.applications.xception import preprocess_input as preprocess_input_xception\nfrom keras.callbacks import ModelCheckpoint, EarlyStopping\nfrom keras.layers import Add, Average, AveragePooling1D, Concatenate, Dense, Dropout, GlobalAveragePooling2D, Input, Lambda, Maximum\nfrom keras.models import Model, load_model\nfrom keras.optimizers import Adam\nfrom keras.preprocessing.image import array_to_img, ImageDataGenerator\nfrom keras.utils import to_categorical, plot_model\n\nfrom sklearn.model_selection import train_test_split\n# from sklearn.utils.validation import check_is_fitted\n\n!pip install tensorflow-addons\nfrom tensorflow_addons.metrics import CohenKappa as cohen_kappa_keras","metadata":{"execution":{"iopub.status.busy":"2021-06-29T07:25:23.368998Z","iopub.execute_input":"2021-06-29T07:25:23.3694Z","iopub.status.idle":"2021-06-29T07:25:38.259023Z","shell.execute_reply.started":"2021-06-29T07:25:23.369365Z","shell.execute_reply":"2021-06-29T07:25:38.25792Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ImageFeatureExtractor(BaseEstimator, TransformerMixin):\n    def __init__(self, construct_from_cnn_backbone=None, from_image=None, cnn_backbone=None,\n                 images_directory=None, preprocess_input=None, img_size=None, average=False,\n                 cnn_backbone_weights_file=None, input_features_file=None, model=None,\n                 save=False, model_name=None, svd_n_components=None, include_feats=True,\n                 loaded_features=None, multiple_instances_per_petid=False,\n                 use_utils_only=False, debug=False):\n        self.construct_from_cnn_backbone = construct_from_cnn_backbone\n        self.from_image = from_image\n        self.cnn_backbone = cnn_backbone\n        self.images_directory = images_directory\n        self.preprocess_input = preprocess_input\n        self.img_size = img_size\n        self.average = average\n        self.cnn_backbone_weights_file = cnn_backbone_weights_file\n        self.input_features_file = input_features_file\n        self.model = model\n        self.save = save\n        self.model_name = model_name\n        self.svd_n_components = svd_n_components\n        self.include_feats = include_feats\n        self.loaded_features = loaded_features\n        self.multiple_instances_per_petid = multiple_instances_per_petid\n        self.use_utils_only = use_utils_only\n        self.debug = debug\n                \n        \n    def resize_to_square(self, im):\n        old_size = im.shape[:2] # old_size is in (height, width) format\n        ratio = float(self.img_size)/max(old_size)\n        new_size = tuple([int(x*ratio) for x in old_size])\n        # new_size should be in (width, height) format\n        im = cv2.resize(im, (new_size[1], new_size[0]))\n        delta_w = self.img_size - new_size[1]\n        delta_h = self.img_size - new_size[0]\n        top, bottom = delta_h//2, delta_h-(delta_h//2)\n        left, right = delta_w//2, delta_w-(delta_w//2)\n        color = [0, 0, 0]\n        new_im = cv2.copyMakeBorder(im, top, bottom, left, right,\n                                    cv2.BORDER_CONSTANT, value=color)\n        return new_im\n\n    \n    def load_image(self, pet_id, num_photo=None):\n        image = cv2.imread(f'{self.images_directory}/{pet_id}-{1 if num_photo is None else num_photo}.jpg')\n        new_image = self.resize_to_square(image)\n        new_image = new_image[...,::-1].astype(np.float32) # BGR to RGB + uint8 to float32\n        new_image = self.preprocess_input(new_image)\n        return new_image\n    \n    \n    def create_model(self):\n        inp = Input((self.img_size,self.img_size,3))\n        if self.cnn_backbone_weights_file is not None:\n            weights = self.cnn_backbone_weights_file\n        else:\n            weights = \"imagenet\"\n        backbone = self.cnn_backbone(input_tensor=inp, include_top=False, weights=weights)\n        x = backbone.output\n        x = GlobalAveragePooling2D()(x)\n        if self.average:\n            x = Lambda(lambda x: K.expand_dims(x, axis=-1))(x)\n            x = AveragePooling1D(4)(x)\n            out = Lambda(lambda x: x[:,:,0])(x)\n        else:\n            out = x\n            \n        self.model = Model(inp,out)\n    \n    \n    def get_input_features(self, pet_id, num_photo=None):\n        return self.input_features_df.loc[f\"{pet_id}{'-'+str(num_photo)+'.jpg' if num_photo is not None else ''}\", :]\n    \n    \n    def extract(self, pet_ids_or_filenames, input_type):\n        if input_type == \"features\":\n            self.input_features_df = pd.read_csv(self.input_features_file, index_col=0)\n            get_input = self.get_input_features\n        elif input_type == \"image\":\n            get_input = self.load_image\n        \n        batch_size = 16\n        n_batches = len(pet_ids_or_filenames) // batch_size + 1\n        features = {}\n        for b in tqdm(range(n_batches)):\n            start = b * batch_size\n            end = (b+1) * batch_size\n            batch_pets = pet_ids_or_filenames[start:end]\n            if input_type == \"image\":\n                batch_inputs = np.zeros((len(batch_pets), self.img_size, self.img_size, 3))\n            elif input_type == \"features\":\n                batch_inputs = np.zeros((len(batch_pets), self.input_features_df.shape[1]))\n            \n            for i, pet_id_or_filename in enumerate(batch_pets):\n                try:\n                    if self.multiple_instances_per_petid:\n                        split_index = pet_id_or_filename.rindex('-')\n                        pet_id = pet_id_or_filename[:split_index]\n                        num_photo = pet_id_or_filename[split_index + 1:-4] #-4 to not include .jpg\n                        batch_inputs[i] = get_input(pet_id, num_photo)\n                    else:\n                        batch_inputs[i] = get_input(pet_id_or_filename)\n                except:\n                    pass\n            \n            batch_preds = self.model.predict(batch_inputs)\n            for i, pet_id_or_filename in enumerate(batch_pets):\n                features[pet_id_or_filename] = batch_preds[i]\n        \n        return pd.DataFrame.from_dict(features, orient='index')\n        \n          \n    def select_method_and_extract(self, pet_ids_or_filenames):\n        if self.loaded_features is not None:\n            features_df = self.loaded_features.copy()\n        # Only the above option is recommended when this method is called from self.fit and\n        # we are doing cross validation, grid search, etc. The options below should only be\n        # executed if we have not extracted and saved features before\n        elif self.construct_from_cnn_backbone:\n            features_df = self.extract(pet_ids_or_filenames, input_type=\"image\")\n        elif self.model is not None:\n            if self.from_image:\n                features_df = self.extract(pet_ids_or_filenames, input_type=\"image\")\n            else:\n                features_df = self.extract(pet_ids_or_filenames, input_type=\"features\")\n        else:\n            features_df = pd.read_csv(self.input_features_file, index_col=0)\n            \n        return features_df\n    \n    \n    def get_filenames(self, pet_ids, photo_amts):\n        filenames = []\n        for pet_id, photo_amt in zip(pet_ids, photo_amts):\n            filenames.append(f\"{pet_id}-1.jpg\")\n            for i in range(2, int(photo_amt)+1):\n                filenames.append(f\"{pet_id}-{i}.jpg\")\n        return filenames\n    \n    \n    def fit(self, X, y):\n        if not self.use_utils_only:\n            if self.construct_from_cnn_backbone is None:\n                raise ValueError(\"\"\"'construct_from_cnn_backbone' cannot be None if this\n                            transformer is not used just for its utils\"\"\")\n            if self.construct_from_cnn_backbone:\n                if self.cnn_backbone is None or self.images_directory is None \\\n                    or self.preprocess_input is None or self.img_size is None:\n                    raise ValueError(\"\"\"Not enough parameters given that\n                            'construct_from_cnn_backbone' is True\"\"\")\n            else:\n                if self.model is None and (self.input_features_file is None and\n                                           self.loaded_features is None):\n                    raise ValueError(\"\"\"If you don't specify 'model', either 'input_features_file'\n                            or 'loaded_features' must be passed as parameter\"\"\")\n                if self.from_image and (self.model is None or self.images_directory is None\n                                        or self.preprocess_input is None or self.img_size is None):\n                    raise ValueError(\"\"\"Not enough parameters given that 'construct_from\n                            _cnn_backbone' is False and 'from_image' is True\"\"\")\n                if self.model is not None and self.input_features_file is None and not self.from_image:\n                    raise ValueError(\"\"\"'input_features_file' cannot be None given that 'construct_from\n                            _cnn_backbone' is False and 'from_image' is False\"\"\")\n                if self.save and self.model is not None and self.model_name is None:\n                    raise ValueError(\"\"\"You have to specify the name of the model if you want to store\n                                the features\"\"\")\n        else:\n            raise RuntimeError(\"Cannot fit transformer as only its utils can be used\")\n            \n        if self.construct_from_cnn_backbone:\n            self.create_model()\n            \n        if self.svd_n_components is not None:\n            self.svd = TruncatedSVD(n_components=self.svd_n_components,\n                                    random_state=seed)\n            pet_ids = X[\"PetID\"].values\n            \n            if self.multiple_instances_per_petid:\n                photo_amts = X[\"PhotoAmt\"].values\n                filenames = self.get_filenames(pet_ids, photo_amts)\n                pet_ids_or_filenames = filenames\n            else:\n                pet_ids_or_filenames = pet_ids\n            \n            features_df = self.select_method_and_extract(pet_ids_or_filenames)\n            self.svd.fit(features_df.loc[pet_ids_or_filenames, :])\n        \n        return self\n    \n    \n    def save_features(self, features_df, prefix=None):\n        svd_str = f\"_svd-{self.svd.components_.shape[0]}\" if self.svd_n_components is not None else \"\"\n        if self.construct_from_cnn_backbone:\n            average_str = \"_avg\" if self.average else \"\"\n            new_filename = f'{prefix + \"_\" if prefix is not None else \"\"}image_features_{self.cnn_backbone.__name__}_in-{self.img_size}{average_str}{svd_str}.csv'\n        elif self.model is not None:\n            new_filename = f'{prefix + \"_\" if prefix is not None else \"\"}image_features_{self.model_name}_in-{self.img_size}{svd_str}.csv'\n        elif svd_str != \"\":\n            new_filename = f'{self.input_features_file[self.input_features_file.rindex(\"/\")+1:-4]}{svd_str}.csv'\n\n        try:\n            features_df.to_csv(new_filename)\n            if self.debug:\n                print(f\"Image features file {new_filename} succesfully saved\")\n        except Exception as e:\n            print(e)\n    \n    \n    def transform(self, X, y=None):\n        if self.use_utils_only:\n            raise RuntimeError(\"Cannot use transformer as only its utils can be used\")\n        X = X.copy()\n        pet_ids = X[\"PetID\"].values\n        \n        if self.multiple_instances_per_petid:\n            photo_amts = X[\"PhotoAmt\"].values\n            filenames = self.get_filenames(pet_ids, photo_amts)\n            pet_ids_or_filenames = filenames\n        else:\n            pet_ids_or_filenames = pet_ids\n        \n        features_df = self.select_method_and_extract(pet_ids_or_filenames)\n\n        if self.svd_n_components is not None and features_df is not None:\n            features_df = pd.DataFrame(self.svd.transform(features_df.loc[pet_ids_or_filenames, :]), index=pet_ids_or_filenames)\n        \n        if self.debug:\n            print(f\"Number of image features: {features_df.shape[1]}\\n\")\n            display(features_df.head())\n            \n        if self.multiple_instances_per_petid:\n            if self.save:\n                self.save_features(features_df, prefix=\"ALL\")\n            features_df = features_df.reset_index()\n            features_df.rename(columns={\"index\": \"PetID\"}, inplace=True)\n            features_df[\"PetID\"] = features_df[\"PetID\"].apply(lambda x: str(x)[:str(x).rindex('-')]) #filename to PetID\n            features_df = features_df.groupby([\"PetID\"]).agg([\"mean\", \"sum\", \"var\"])\n            features_df.columns = [f\"{x}_{y}\" if y != \"\" else f\"{x}\" for x,y in features_df.columns.to_flat_index()]\n            features_df.fillna(0, inplace=True)\n            if self.debug:\n                print(f\"Aggregated features:\\n\")\n                display(features_df.head())\n            if self.save:\n                self.save_features(features_df, prefix=\"AGGREGATED\")\n        elif self.save:\n            self.save_features(features_df)\n            \n        if self.include_feats:\n            features_df.rename(lambda x: f\"img_{x}\", axis=1, inplace=True)\n            X = X.merge(features_df, left_index=False, right_index=True,\n                        left_on=\"PetID\", how=\"left\")\n#             if \"PetID\" in X.columns:\n#                 X.drop([\"PetID\"], axis=1, inplace=True)\n                \n        return X","metadata":{"execution":{"iopub.status.busy":"2021-06-29T07:25:38.261833Z","iopub.execute_input":"2021-06-29T07:25:38.262153Z","iopub.status.idle":"2021-06-29T07:25:38.322595Z","shell.execute_reply.started":"2021-06-29T07:25:38.262123Z","shell.execute_reply":"2021-06-29T07:25:38.321166Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**The code to load an image from https://www.kaggle.com/christofhenkel/extract-image-features-from-pretrained-nn is not correct, since cv2 loads images in BGR format but preprocess_input expects them to be in RGB, plus the type should be float32 and cv2 loads uint8. See https://github.com/keras-team/keras/blob/be4cef42ab21d85398fb6930ec5419a3de8a7d71/keras/applications/densenet.py#L364, https://github.com/keras-team/keras/blob/be4cef42ab21d85398fb6930ec5419a3de8a7d71/keras/applications/imagenet_utils.py#L166, https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/img_to_array, https://stackoverflow.com/questions/50746096/how-to-match-cv2-imread-to-the-keras-image-img-load-output, https://github.com/keras-team/keras/issues/10279 and https://stackoverflow.com/questions/48677128/what-is-the-right-way-to-preprocess-images-in-keras-while-fine-tuning-pre-traine.**","metadata":{}},{"cell_type":"code","source":"ife = ImageFeatureExtractor(construct_from_cnn_backbone=True, cnn_backbone=DenseNet121,\n                 images_directory=\"../input/petfinder-adoption-prediction/train_images\",\n                 preprocess_input=preprocess_input_densenet, img_size=256)\n\n\ngreatest_aspect_ratios_ids = X.loc[result.sort_values('ImagePropertyAspectRatio_num',\n                                    ascending=False).head(5).index, \"PetID\"].values\nsmallest_aspect_ratios_ids = X.loc[result.loc[result[\"PhotoAmt\"] > result[\"PhotoAmt\"].min()\n                        ].sort_values('ImagePropertyAspectRatio_num').head(5).index, \"PetID\"].values\n\n_, ax = plt.subplots(nrows=1,ncols=5,figsize=(20,4))\nax = ax.flatten()\nfor i, pet_id in enumerate(greatest_aspect_ratios_ids):\n    image = cv2.imread(f'{ife.images_directory}/{pet_id}-1.jpg')\n    new_image = ife.resize_to_square(image)\n    new_image = new_image[...,::-1] # BGR to RGB\n    ax[i].set_xticks([]) \n    ax[i].set_yticks([]) \n    ax[i].imshow(new_image)\n\nplt.suptitle(\"Resized images that had the greatest aspect ratios\", fontsize=20)\nplt.show()\n\n\n_, ax = plt.subplots(nrows=1,ncols=5,figsize=(20,4))\nax = ax.flatten()\nfor i, pet_id in enumerate(smallest_aspect_ratios_ids):\n    image = cv2.imread(f'{ife.images_directory}/{pet_id}-1.jpg')\n    new_image = ife.resize_to_square(image)\n    new_image = new_image[...,::-1] # BGR to RGB\n    ax[i].set_xticks([]) \n    ax[i].set_yticks([]) \n    ax[i].imshow(new_image)\n\nplt.suptitle(\"Resized images that had the smallest aspect ratios\", fontsize=20)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-29T07:25:38.324178Z","iopub.execute_input":"2021-06-29T07:25:38.324465Z","iopub.status.idle":"2021-06-29T07:25:39.30063Z","shell.execute_reply.started":"2021-06-29T07:25:38.324437Z","shell.execute_reply":"2021-06-29T07:25:39.29933Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"physical_devices = tf.config.list_physical_devices(\"GPU\")\n\nif not physical_devices:\n    print(\"No GPU devices are used in the host.\")","metadata":{"execution":{"iopub.status.busy":"2021-06-29T07:25:39.301931Z","iopub.execute_input":"2021-06-29T07:25:39.302232Z","iopub.status.idle":"2021-06-29T07:25:39.312866Z","shell.execute_reply.started":"2021-06-29T07:25:39.302203Z","shell.execute_reply":"2021-06-29T07:25:39.31146Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.set_option('display.max_columns', 32)","metadata":{"execution":{"iopub.status.busy":"2021-06-29T07:25:39.315352Z","iopub.execute_input":"2021-06-29T07:25:39.315764Z","iopub.status.idle":"2021-06-29T07:25:39.330138Z","shell.execute_reply.started":"2021-06-29T07:25:39.31573Z","shell.execute_reply":"2021-06-29T07:25:39.328846Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def to_latex(df, filename, groupby_keys=[]):\n    df.index += 1\n    if len(groupby_keys) > 0:\n        df = df.groupby(groupby_keys).mean().reset_index()\n    df.to_latex(f\"{filename}.tex\", index=False, column_format='c'*df.shape[1])\n    tex_file = []\n    with open(f\"{filename}.tex\", 'r') as f:\n        tex_file = f.readlines()\n    with open(f\"{filename}.tex\", 'w') as f:\n        f.writelines([\"\\\\begin{table}[H]\\n\", \"\\\\centering\\n\"] + tex_file + [\"\\\\caption{}\\n\", \"\\\\end{table}\\n\"])","metadata":{"execution":{"iopub.status.busy":"2021-06-29T07:25:39.331861Z","iopub.execute_input":"2021-06-29T07:25:39.332202Z","iopub.status.idle":"2021-06-29T07:25:39.34547Z","shell.execute_reply.started":"2021-06-29T07:25:39.332172Z","shell.execute_reply":"2021-06-29T07:25:39.34454Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_history(results, variables, nrows, ncols, figsize):\n    plt.style.use('default')\n    _, ax = plt.subplots(nrows=nrows, ncols=ncols, figsize=figsize)\n    ax = ax.flatten() if nrows*ncols > 1 else [ax]\n    x = list(range(1,results.shape[0]+1))\n    for i in range(len(ax)):\n        if i < len(variables):\n            ax[i].plot(x, results[variables[i]])\n            ax[i].plot(x, results[f'val_{variables[i]}'])\n            ax[i].set_title(f'Model {variables[i]}')\n            ax[i].set_ylabel(variables[i])\n            ax[i].set_xlabel('epoch')\n            ax[i].legend(['train', 'val'], loc='upper left')\n        else:\n            ax[i].set_axis_off()\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-29T07:25:39.346457Z","iopub.execute_input":"2021-06-29T07:25:39.346764Z","iopub.status.idle":"2021-06-29T07:25:39.371137Z","shell.execute_reply.started":"2021-06-29T07:25:39.346732Z","shell.execute_reply":"2021-06-29T07:25:39.369783Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### DenseNet121","metadata":{}},{"cell_type":"markdown","source":"(run of Version 34 of the cell below was done with the BGR to RGB modification)","metadata":{}},{"cell_type":"code","source":"ife = ImageFeatureExtractor(construct_from_cnn_backbone=True, cnn_backbone=DenseNet121,\n                 images_directory=\"../input/petfinder-adoption-prediction/train_images\",\n                 preprocess_input=preprocess_input_densenet, img_size=256, average=False,\n                 save=True, debug=True, include_feats=False)\n\n_ = ife.fit_transform(X, y)","metadata":{"execution":{"iopub.status.busy":"2021-06-03T10:33:42.768767Z","iopub.execute_input":"2021-06-03T10:33:42.76913Z","iopub.status.idle":"2021-06-03T10:36:51.204523Z","shell.execute_reply.started":"2021-06-03T10:33:42.769095Z","shell.execute_reply":"2021-06-03T10:36:51.203559Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image_features_densenet121_bgr = pd.read_csv(\n        \"../input/tfg-pet-adoption-data/image_features_DenseNet121_in-256_BGR.csv\",\n        index_col=0)\nimage_features_densenet121_bgr.head(5)","metadata":{"execution":{"iopub.status.busy":"2021-06-03T10:36:51.208031Z","iopub.execute_input":"2021-06-03T10:36:51.208305Z","iopub.status.idle":"2021-06-03T10:36:54.757738Z","shell.execute_reply.started":"2021-06-03T10:36:51.208279Z","shell.execute_reply":"2021-06-03T10:36:54.756812Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"By simply looking at some of the values of the two previous dataframes, we can see that having changed the input format to `preprocess_input` from BGR to RGB has had some effect.","metadata":{}},{"cell_type":"markdown","source":"### DenseNet201","metadata":{}},{"cell_type":"code","source":"ife = ImageFeatureExtractor(construct_from_cnn_backbone=True, cnn_backbone=DenseNet201,\n                 images_directory=\"../input/petfinder-adoption-prediction/train_images\",\n                 preprocess_input=preprocess_input_densenet, img_size=256, average=False,\n                 save=True, debug=True, include_feats=False)\n\n_ = ife.fit_transform(X, y)","metadata":{"execution":{"iopub.status.busy":"2021-06-03T11:23:44.375184Z","iopub.execute_input":"2021-06-03T11:23:44.375532Z","iopub.status.idle":"2021-06-03T11:28:11.980264Z","shell.execute_reply.started":"2021-06-03T11:23:44.375501Z","shell.execute_reply":"2021-06-03T11:28:11.97931Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### InceptionResNetV2","metadata":{}},{"cell_type":"code","source":"ife = ImageFeatureExtractor(construct_from_cnn_backbone=True, cnn_backbone=InceptionResNetV2,\n                 images_directory=\"../input/petfinder-adoption-prediction/train_images\",\n                 preprocess_input=preprocess_input_inceptionresnetv2, img_size=256,\n                 average=False, save=True, debug=True, include_feats=False)\n\n_ = ife.fit_transform(X, y)","metadata":{"execution":{"iopub.status.busy":"2021-06-03T11:28:11.982269Z","iopub.execute_input":"2021-06-03T11:28:11.982789Z","iopub.status.idle":"2021-06-03T11:32:36.339358Z","shell.execute_reply.started":"2021-06-03T11:28:11.982748Z","shell.execute_reply":"2021-06-03T11:32:36.337772Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### NASNetLarge","metadata":{}},{"cell_type":"code","source":"ife = ImageFeatureExtractor(construct_from_cnn_backbone=True, cnn_backbone=NASNetLarge,\n                 images_directory=\"../input/petfinder-adoption-prediction/train_images\",\n                 preprocess_input=preprocess_input_nasnet, img_size=256, average=False,\n                 save=True, debug=True, include_feats=False)\n\n_ = ife.fit_transform(X, y)","metadata":{"execution":{"iopub.status.busy":"2021-06-03T11:32:36.340803Z","iopub.execute_input":"2021-06-03T11:32:36.341193Z","iopub.status.idle":"2021-06-03T11:39:22.192095Z","shell.execute_reply.started":"2021-06-03T11:32:36.341154Z","shell.execute_reply":"2021-06-03T11:39:22.191121Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### VGG16","metadata":{}},{"cell_type":"code","source":"ife = ImageFeatureExtractor(construct_from_cnn_backbone=True, cnn_backbone=VGG16,\n                 images_directory=\"../input/petfinder-adoption-prediction/train_images\",\n                 preprocess_input=preprocess_input_vgg16, img_size=256, average=False,\n                 save=True, debug=True, include_feats=False)\n\n_ = ife.fit_transform(X, y)","metadata":{"execution":{"iopub.status.busy":"2021-06-03T11:39:22.193679Z","iopub.execute_input":"2021-06-03T11:39:22.194195Z","iopub.status.idle":"2021-06-03T11:42:06.771482Z","shell.execute_reply.started":"2021-06-03T11:39:22.194155Z","shell.execute_reply":"2021-06-03T11:42:06.770364Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Xception","metadata":{}},{"cell_type":"code","source":"ife = ImageFeatureExtractor(construct_from_cnn_backbone=True, cnn_backbone=Xception,\n                 images_directory=\"../input/petfinder-adoption-prediction/train_images\",\n                 preprocess_input=preprocess_input_xception, img_size=256, average=False,\n                 save=True, debug=True, include_feats=False)\n\n_ = ife.fit_transform(X, y)","metadata":{"execution":{"iopub.status.busy":"2021-06-03T11:42:06.773785Z","iopub.execute_input":"2021-06-03T11:42:06.774159Z","iopub.status.idle":"2021-06-03T11:45:53.287585Z","shell.execute_reply.started":"2021-06-03T11:42:06.77412Z","shell.execute_reply":"2021-06-03T11:45:53.286604Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**If we take a look at the distributions of some of the extracted features, we can see that DenseNets give us normally distributed variables, while the rest are right skewed...**","metadata":{}},{"cell_type":"markdown","source":"As the number of features that we get from each CNN is different, we will apply SVD to reduce the number of features and also to determine which pretrained CNN we should focus on.","metadata":{}},{"cell_type":"code","source":"features_dfs = {}\nfor model in [DenseNet121, DenseNet201, InceptionResNetV2, NASNetLarge, VGG16, Xception]:\n    features_dfs[model.__name__] = pd.read_csv(\n        f\"../input/tfg-pet-adoption-data/image_features_{model.__name__}_in-256.csv\",\n        index_col=0)","metadata":{"execution":{"iopub.status.busy":"2021-06-27T17:08:13.508679Z","iopub.execute_input":"2021-06-27T17:08:13.50912Z","iopub.status.idle":"2021-06-27T17:09:30.125723Z","shell.execute_reply.started":"2021-06-27T17:08:13.509085Z","shell.execute_reply":"2021-06-27T17:09:30.124545Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's also define the columns to be removed and the base numeric columns that will be passed to the corresponding transformers in the pipelines that we will use to evaluate the effect of the extracted image features along with the previous data:","metadata":{}},{"cell_type":"code","source":"columns_to_be_removed_image_feats_eval = [\"Name\", \"Breed1\", \"Breed2\", \"Gender\", \"Color1\",\n                                          \"Color2\", \"Color3\", \"Vaccinated\", \"Dewormed\",\n                                          \"Sterilized\", \"State\", \"RescuerID\", #\"PetID\",\n                                          \"MaturitySize\", \"FurLength\", \"Health\",\n                                          \"DescriptionLanguage\", \"Description\",\n                                          \"ImageMetadataDescription\"]\n\nnumeric_columns_image_feats_eval = [\"Age\", \"Quantity\", \"Fee\", \"VideoAmt\", \"PhotoAmt\",\n                                    \"StateGDP\", \"RescuerCount\", \"DescriptionLength\"]","metadata":{"execution":{"iopub.status.busy":"2021-06-29T07:26:04.532165Z","iopub.execute_input":"2021-06-29T07:26:04.532559Z","iopub.status.idle":"2021-06-29T07:26:04.538734Z","shell.execute_reply.started":"2021-06-29T07:26:04.532524Z","shell.execute_reply":"2021-06-29T07:26:04.5375Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pipeline_transformers = [\n    ('replace_breeds', LeftJoinReplace(values_dict=breeds_dict,\n                                      variables=[\"Breed1\", \"Breed2\"])),\n    ('replace_colors', LeftJoinReplace(values_dict=colors_dict,\n                                      variables=[\"Color1\", \"Color2\", \"Color3\"])),\n    ('replace_states', LeftJoinReplace(values_dict=states_dict,\n                                     variables=[\"State\"])),\n    ('replace_by_strings', FunctionTransformer(func=replace_integers_by_strings)),\n    ('has_name', FunctionTransformer(func=has_significant_name)),\n    ('pure_breed', FunctionTransformer(func=has_pure_breed)),\n    ('breed_matches_fur_length', FunctionTransformer(func=breed_matches_fur_length)),\n    ('impute_breed', BreedImputer()),\n    ('include_prof_im_metadata', IncludeProfileImageMetadata(profile_image_metadata)),\n    ('correct_wrong_type', CorrectWrongType(breeds)),\n    ('encode_breed', BreedEncoding(enc_type=\"target_and_frequency\")),\n    ('ordinal_vars_encoder', OrdinalVariableEncoder(columns=[\"MaturitySize\", \"FurLength\", \"Health\"],\n                                enc_type=\"ordinal\", mapping=ordinal_vars_mapping)),\n    ('state_gdp', ReplaceState(gdp_per_capita=gdp_per_capita, impute_nan_value=46450)),\n    ('rescuer_count', ReplaceRescuerID()),\n    ('discretizer', CustomDiscretizer(bins_age=-1, quantity=False, fee=False, video_amt=False,\n                                     photo_amt=False)),\n    ('description_length', FunctionTransformer(func=include_description_length)),\n    ('include_desc_metadata', IncludeDescriptionMetadata(description_metadata=description_metadata)),\n    ('correct_desc_language', CorrectDescriptionLanguage()),\n    ('one_hot_encoder', CustomOneHotEncoder(columns=[\"Gender\", \"Color1\", \"Color2\",\n                            \"Color3\", \"Vaccinated\", \"Dewormed\", \"Sterilized\", \"DescriptionLanguage\"])),\n    ('include_prof_im_properties', IncludeProfileImageProperties(profile_image_properties)),\n    ('drop_columns', ColumnRemover(columns=columns_to_be_removed_image_feats_eval)),\n    ('round_im_dims_aspect_ratio', FunctionTransformer(func=include_aspect_ratio)),\n    None,\n    ('drop_petid', ColumnRemover(columns=[\"PetID\"])),\n    ('impute_malay_desc_missing_prof_im_props', CustomIterativeImputer()),\n    ('custom_standard_scaler', CustomStandardScaler(numeric_columns_image_feats_eval))\n]\n\nevaluation_results = pd.DataFrame([], columns=[\"CNN\", \"SVD-n_components\",\n                        \"Average fit time\", \"Average accuracy\", \"Average QWK\"])\n\nfor cnn in [DenseNet121, DenseNet201, InceptionResNetV2, NASNetLarge, VGG16, Xception]:\n    for svd_n_components in [16, 32, 64]:\n#         for model_desc, model in models.items():\n        ife = ImageFeatureExtractor(construct_from_cnn_backbone=False,\n                            loaded_features=features_dfs[cnn.__name__],\n                            svd_n_components=svd_n_components)\n        \n        pipeline_transformers[-4] = ('image_features_extractor', ife)\n        \n        model = xgb.XGBClassifier(eval_metric='mlogloss', random_state=seed, n_jobs=-1,\n                              use_label_encoder=False)\n        print(f\"\\n\\n*************** XGBClassifier with image features from {cnn.__name__}, SVD {svd_n_components} ***************\")\n        avg_fit_time, avg_accuracy, avg_QWK = evaluate_model(\n                    Pipeline(steps=pipeline_transformers + [('model', model)]),\n                    cv, X, y, model_type=\"classification\", display_results=False)\n        \n        evaluation_results = evaluation_results.append({\n            \"CNN\": cnn.__name__,\n            \"SVD-n_components\": svd_n_components,\n            \"Average fit time\": avg_fit_time,\n            \"Average accuracy\": avg_accuracy,\n            \"Average QWK\": avg_QWK\n        },ignore_index=True)\n        \ndisplay(evaluation_results)","metadata":{"execution":{"iopub.status.busy":"2021-06-27T17:13:14.568519Z","iopub.execute_input":"2021-06-27T17:13:14.568917Z","iopub.status.idle":"2021-06-27T18:00:33.440859Z","shell.execute_reply.started":"2021-06-27T17:13:14.568886Z","shell.execute_reply":"2021-06-27T18:00:33.439697Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"evaluation_results.groupby([\"CNN\"]).mean()","metadata":{"execution":{"iopub.status.busy":"2021-06-27T18:02:00.738972Z","iopub.execute_input":"2021-06-27T18:02:00.739447Z","iopub.status.idle":"2021-06-27T18:02:00.75633Z","shell.execute_reply.started":"2021-06-27T18:02:00.739406Z","shell.execute_reply":"2021-06-27T18:02:00.754998Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see that in terms of both average accuracy and average QWK, the configurations that gives better results are DenseNet121 with 16 SVD components and NASNetLarge with the same number of components. However, as the number of features that NASNetLarge yields is greater than that of DenseNet121, the average fit time is over 60% higher; consequently, and taking into account that the training time (both forward and backward propagation) would be significantly higher due to the complexity of the network, the best option to continue is DenseNet121.","metadata":{}},{"cell_type":"code","source":"del features_dfs\nto_latex(evaluation_results, \"evaluation_results_XGBoostClassifier_varying-CNNs_SVD\")","metadata":{"execution":{"iopub.status.busy":"2021-06-27T18:00:33.442599Z","iopub.execute_input":"2021-06-27T18:00:33.442873Z","iopub.status.idle":"2021-06-27T18:00:33.477494Z","shell.execute_reply.started":"2021-06-27T18:00:33.442846Z","shell.execute_reply":"2021-06-27T18:00:33.476415Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### SVD n_components?","metadata":{}},{"cell_type":"markdown","source":"As saw previously, when we evaluated our model with image features extracted from different pretrained CNNs, the best results were obtained using a smaller number of components applying SVD (in particular, 16), plus more features means more fitting time. We also saw that using the raw features (without Average Pooling) is not feasible in terms of time (1096 features in total...) and better results were obtained applying SVD with 16, 32 or 64 components.\n\nLet's see now what is the result of applying SVD with a smaller number of components to the image features:","metadata":{}},{"cell_type":"code","source":"image_features_densenet121 = pd.read_csv(\n        \"../input/tfg-pet-adoption-data/image_features_DenseNet121_in-256.csv\",\n        index_col=0)","metadata":{"execution":{"iopub.status.busy":"2021-06-22T16:49:19.490246Z","iopub.execute_input":"2021-06-22T16:49:19.490519Z","iopub.status.idle":"2021-06-22T16:49:22.635478Z","shell.execute_reply.started":"2021-06-22T16:49:19.490484Z","shell.execute_reply":"2021-06-22T16:49:22.634646Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pipeline_transformers = [\n    ('replace_breeds', LeftJoinReplace(values_dict=breeds_dict,\n                                      variables=[\"Breed1\", \"Breed2\"])),\n    ('replace_colors', LeftJoinReplace(values_dict=colors_dict,\n                                      variables=[\"Color1\", \"Color2\", \"Color3\"])),\n    ('replace_states', LeftJoinReplace(values_dict=states_dict,\n                                     variables=[\"State\"])),\n    ('replace_by_strings', FunctionTransformer(func=replace_integers_by_strings)),\n    ('has_name', FunctionTransformer(func=has_significant_name)),\n    ('pure_breed', FunctionTransformer(func=has_pure_breed)),\n    ('breed_matches_fur_length', FunctionTransformer(func=breed_matches_fur_length)),\n    ('impute_breed', BreedImputer()),\n    ('include_prof_im_metadata', IncludeProfileImageMetadata(profile_image_metadata)),\n    ('correct_wrong_type', CorrectWrongType(breeds)),\n    ('encode_breed', BreedEncoding(enc_type=\"target_and_frequency\")),\n    ('ordinal_vars_encoder', OrdinalVariableEncoder(columns=[\"MaturitySize\", \"FurLength\", \"Health\"],\n                                enc_type=\"ordinal\", mapping=ordinal_vars_mapping)),\n    ('state_gdp', ReplaceState(gdp_per_capita=gdp_per_capita, impute_nan_value=46450)),\n    ('rescuer_count', ReplaceRescuerID()),\n    ('discretizer', CustomDiscretizer(bins_age=-1, quantity=False, fee=False, video_amt=False,\n                                     photo_amt=False)),\n    ('description_length', FunctionTransformer(func=include_description_length)),\n    ('include_desc_metadata', IncludeDescriptionMetadata(description_metadata=description_metadata)),\n    ('correct_desc_language', CorrectDescriptionLanguage()),\n    ('one_hot_encoder', CustomOneHotEncoder(columns=[\"Gender\", \"Color1\", \"Color2\",\n                            \"Color3\", \"Vaccinated\", \"Dewormed\", \"Sterilized\", \"DescriptionLanguage\"])),\n    ('include_prof_im_properties', IncludeProfileImageProperties(profile_image_properties)),\n    ('drop_columns', ColumnRemover(columns=columns_to_be_removed_image_feats_eval)),\n    ('round_im_dims_aspect_ratio', FunctionTransformer(func=include_aspect_ratio)),\n    None,\n    ('drop_petid', ColumnRemover(columns=[\"PetID\"])),\n    ('impute_malay_desc_missing_prof_im_props', CustomIterativeImputer()),\n    ('custom_standard_scaler', CustomStandardScaler(numeric_columns_image_feats_eval))\n]\n\n\nevaluation_results = pd.DataFrame([], columns=[\"SVD-n_components\",\n                        \"Average fit time\", \"Average accuracy\", \"Average QWK\"])\n\nfor svd_n_components in [4, 8, 12, 16, 20, 24, 28]:\n    ife = ImageFeatureExtractor(construct_from_cnn_backbone=False,\n                                loaded_features=image_features_densenet121,\n                                svd_n_components=svd_n_components)\n\n    pipeline_transformers[-4] = ('image_features_extractor', ife)\n\n    model = xgb.XGBClassifier(eval_metric='mlogloss', random_state=seed, n_jobs=-1,\n                          use_label_encoder=False)\n    print(f\"\\n\\n*************** XGBClassifier with image features from DenseNet121, SVD {svd_n_components} ***************\")\n    avg_fit_time, avg_accuracy, avg_QWK = evaluate_model(\n                Pipeline(steps=pipeline_transformers + [('model', model)]),\n                cv, X, y, model_type=\"classification\", display_results=True,\n                display_plots=False)\n\n    evaluation_results = evaluation_results.append({\"SVD-n_components\": svd_n_components,\n                        \"Average fit time\": avg_fit_time, \"Average accuracy\": avg_accuracy,\n                        \"Average QWK\": avg_QWK}, ignore_index=True)\n        \n\ndisplay(evaluation_results)\nto_latex(evaluation_results, \"evaluation_results_XGBClassifier_DenseNet121_varying-SVD-n_components\")","metadata":{"execution":{"iopub.status.busy":"2021-06-22T16:49:22.641362Z","iopub.execute_input":"2021-06-22T16:49:22.641673Z","iopub.status.idle":"2021-06-22T17:08:56.381796Z","shell.execute_reply.started":"2021-06-22T16:49:22.641642Z","shell.execute_reply":"2021-06-22T17:08:56.380715Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It seems that the best balance between average accuracy and average QWK, reducing the dimensionality with SVD to 4 or 16 components are the best options.","metadata":{}},{"cell_type":"markdown","source":"### Average?","metadata":{}},{"cell_type":"markdown","source":"Let's try to apply now an additional layer of Average Pooling with a size of 4 (so instead of 1024 features we get 256), as the author of the base source code originally proposes, and see if it is better than not applying it:","metadata":{}},{"cell_type":"code","source":"image_features_densenet121_avg = pd.read_csv(\n        \"../input/tfg-pet-adoption-data/image_features_DenseNet121_in-256_avg.csv\",\n        index_col=0)","metadata":{"execution":{"iopub.status.busy":"2021-06-22T17:08:56.383635Z","iopub.execute_input":"2021-06-22T17:08:56.383885Z","iopub.status.idle":"2021-06-22T17:08:58.023833Z","shell.execute_reply.started":"2021-06-22T17:08:56.38386Z","shell.execute_reply":"2021-06-22T17:08:58.022678Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pipeline_transformers = [\n    ('replace_breeds', LeftJoinReplace(values_dict=breeds_dict,\n                                      variables=[\"Breed1\", \"Breed2\"])),\n    ('replace_colors', LeftJoinReplace(values_dict=colors_dict,\n                                      variables=[\"Color1\", \"Color2\", \"Color3\"])),\n    ('replace_states', LeftJoinReplace(values_dict=states_dict,\n                                     variables=[\"State\"])),\n    ('replace_by_strings', FunctionTransformer(func=replace_integers_by_strings)),\n    ('has_name', FunctionTransformer(func=has_significant_name)),\n    ('pure_breed', FunctionTransformer(func=has_pure_breed)),\n    ('breed_matches_fur_length', FunctionTransformer(func=breed_matches_fur_length)),\n    ('impute_breed', BreedImputer()),\n    ('include_prof_im_metadata', IncludeProfileImageMetadata(profile_image_metadata)),\n    ('correct_wrong_type', CorrectWrongType(breeds)),\n    ('encode_breed', BreedEncoding(enc_type=\"target_and_frequency\")),\n    ('ordinal_vars_encoder', OrdinalVariableEncoder(columns=[\"MaturitySize\", \"FurLength\", \"Health\"],\n                                enc_type=\"ordinal\", mapping=ordinal_vars_mapping)),\n    ('state_gdp', ReplaceState(gdp_per_capita=gdp_per_capita, impute_nan_value=46450)),\n    ('rescuer_count', ReplaceRescuerID()),\n    ('discretizer', CustomDiscretizer(bins_age=-1, quantity=False, fee=False, video_amt=False,\n                                     photo_amt=False)),\n    ('description_length', FunctionTransformer(func=include_description_length)),\n    ('include_desc_metadata', IncludeDescriptionMetadata(description_metadata=description_metadata)),\n    ('correct_desc_language', CorrectDescriptionLanguage()),\n    ('one_hot_encoder', CustomOneHotEncoder(columns=[\"Gender\", \"Color1\", \"Color2\",\n                            \"Color3\", \"Vaccinated\", \"Dewormed\", \"Sterilized\", \"DescriptionLanguage\"])),\n    ('include_prof_im_properties', IncludeProfileImageProperties(profile_image_properties)),\n    ('drop_columns', ColumnRemover(columns=columns_to_be_removed_image_feats_eval)),\n    ('round_im_dims_aspect_ratio', FunctionTransformer(func=include_aspect_ratio)),\n    None,\n    ('drop_petid', ColumnRemover(columns=[\"PetID\"])),\n    ('impute_malay_desc_missing_prof_im_props', CustomIterativeImputer())\n    ('custom_standard_scaler', CustomStandardScaler(numeric_columns_image_feats_eval))\n]\n\n\nevaluation_results = pd.DataFrame([], columns=[\"SVD-n_components\",\n                        \"Average fit time\", \"Average accuracy\", \"Average QWK\"])\n\nfor svd_n_components in [4, 8, 12, 16, 20, 24, 28, 32]:\n    ife = ImageFeatureExtractor(construct_from_cnn_backbone=False,\n                                loaded_features=image_features_densenet121_avg,\n                                svd_n_components=svd_n_components)\n\n    pipeline_transformers[-4] = ('image_features_extractor', ife)\n\n    model = xgb.XGBClassifier(eval_metric='mlogloss', random_state=seed, n_jobs=-1,\n                          use_label_encoder=False)\n    print(f\"\\n\\n*************** XGBClassifier with image features from DenseNet121, AVG + SVD {svd_n_components} ***************\")\n    avg_fit_time, avg_accuracy, avg_QWK = evaluate_model(\n                Pipeline(steps=pipeline_transformers + [('model', model)]),\n                cv, X, y, model_type=\"classification\", display_results=True,\n                display_plots=False)\n\n    evaluation_results = evaluation_results.append({\"SVD-n_components\": svd_n_components,\n                        \"Average fit time\": avg_fit_time, \"Average accuracy\": avg_accuracy,\n                        \"Average QWK\": avg_QWK}, ignore_index=True)\n        \ndisplay(evaluation_results)\nto_latex(evaluation_results, \"evaluation_results_XGBoostClassifier_DenseNet121__avg__varying-SVD-n_components\")","metadata":{"execution":{"iopub.status.busy":"2021-06-22T17:08:58.025186Z","iopub.execute_input":"2021-06-22T17:08:58.02544Z","iopub.status.idle":"2021-06-22T17:28:43.806692Z","shell.execute_reply.started":"2021-06-22T17:08:58.025415Z","shell.execute_reply":"2021-06-22T17:28:43.80568Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"If we look at the obtained results, both raw features and reduzed using SVD, and compare them to those obtained before without applying Average Pooling, we can see that it is better to continue working without applying this last operation, as the results are worse in the average QWK (maybe we are losing information by losing some important dimensions).","metadata":{}},{"cell_type":"markdown","source":"### Image size?","metadata":{}},{"cell_type":"markdown","source":"Let's try to vary now the input image size, which we will resize every image to, and see whether 256x256, the one we have used so far, is a good option.","metadata":{}},{"cell_type":"code","source":"for img_size in [224, 384, 512]:\n    print(f\"\\nDenseNet121 output features extracted from {img_size}x{img_size} resized images:\")\n    ife = ImageFeatureExtractor(construct_from_cnn_backbone=True, cnn_backbone=DenseNet121,\n                     images_directory=\"../input/petfinder-adoption-prediction/train_images\",\n                     preprocess_input=preprocess_input_densenet, img_size=img_size, average=False,\n                     save=True, debug=True, include_feats=False)\n\n    _ = ife.fit_transform(X, y)","metadata":{"execution":{"iopub.status.busy":"2021-06-22T18:00:35.782728Z","iopub.execute_input":"2021-06-22T18:00:35.783163Z","iopub.status.idle":"2021-06-22T18:00:35.78768Z","shell.execute_reply.started":"2021-06-22T18:00:35.783118Z","shell.execute_reply":"2021-06-22T18:00:35.786688Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"densenet121_var_img_size_dfs = {}\nfor img_size in [224, 256, 384, 512]:\n    densenet121_var_img_size_dfs[img_size] = pd.read_csv(\n        f\"../input/tfg-pet-adoption-data/image_features_DenseNet121_in-{img_size}.csv\",\n        index_col=0)","metadata":{"execution":{"iopub.status.busy":"2021-06-22T18:00:41.252025Z","iopub.execute_input":"2021-06-22T18:00:41.252664Z","iopub.status.idle":"2021-06-22T18:01:01.637992Z","shell.execute_reply.started":"2021-06-22T18:00:41.252611Z","shell.execute_reply":"2021-06-22T18:01:01.637104Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pipeline_transformers = [\n    ('replace_breeds', LeftJoinReplace(values_dict=breeds_dict,\n                                      variables=[\"Breed1\", \"Breed2\"])),\n    ('replace_colors', LeftJoinReplace(values_dict=colors_dict,\n                                      variables=[\"Color1\", \"Color2\", \"Color3\"])),\n    ('replace_states', LeftJoinReplace(values_dict=states_dict,\n                                     variables=[\"State\"])),\n    ('replace_by_strings', FunctionTransformer(func=replace_integers_by_strings)),\n    ('has_name', FunctionTransformer(func=has_significant_name)),\n    ('pure_breed', FunctionTransformer(func=has_pure_breed)),\n    ('breed_matches_fur_length', FunctionTransformer(func=breed_matches_fur_length)),\n    ('impute_breed', BreedImputer()),\n    ('include_prof_im_metadata', IncludeProfileImageMetadata(profile_image_metadata)),\n    ('correct_wrong_type', CorrectWrongType(breeds)),\n    ('encode_breed', BreedEncoding(enc_type=\"target_and_frequency\")),\n    ('ordinal_vars_encoder', OrdinalVariableEncoder(columns=[\"MaturitySize\", \"FurLength\", \"Health\"],\n                                enc_type=\"ordinal\", mapping=ordinal_vars_mapping)),\n    ('state_gdp', ReplaceState(gdp_per_capita=gdp_per_capita, impute_nan_value=46450)),\n    ('rescuer_count', ReplaceRescuerID()),\n    ('discretizer', CustomDiscretizer(bins_age=-1, quantity=False, fee=False, video_amt=False,\n                                     photo_amt=False)),\n    ('description_length', FunctionTransformer(func=include_description_length)),\n    ('include_desc_metadata', IncludeDescriptionMetadata(description_metadata=description_metadata)),\n    ('correct_desc_language', CorrectDescriptionLanguage()),\n    ('one_hot_encoder', CustomOneHotEncoder(columns=[\"Gender\", \"Color1\", \"Color2\",\n                            \"Color3\", \"Vaccinated\", \"Dewormed\", \"Sterilized\", \"DescriptionLanguage\"])),\n    ('include_prof_im_properties', IncludeProfileImageProperties(profile_image_properties)),\n    ('drop_columns', ColumnRemover(columns=columns_to_be_removed_image_feats_eval)),\n    ('round_im_dims_aspect_ratio', FunctionTransformer(func=include_aspect_ratio)),\n    None,\n    ('drop_petid', ColumnRemover(columns=[\"PetID\"])),\n    ('impute_malay_desc_missing_prof_im_props', CustomIterativeImputer())\n    ('custom_standard_scaler', CustomStandardScaler(numeric_columns_image_feats_eval))\n]\n\nevaluation_results = pd.DataFrame([], columns=[\"Input size\", \"SVD-n_components\",\n                        \"Average fit time\", \"Average accuracy\", \"Average QWK\"])\n\nfor img_size in [224, 256, 384, 512]:\n    for svd_n_components in [4, 8, 12, 16, 20, 24, 28]:\n        ife = ImageFeatureExtractor(construct_from_cnn_backbone=False,\n                                    loaded_features=densenet121_var_img_size_dfs[img_size],\n                                    svd_n_components=svd_n_components)\n        \n        pipeline_transformers[-4] = ('image_features_extractor', ife)\n        \n        model = xgb.XGBClassifier(eval_metric='mlogloss', random_state=seed, n_jobs=-1,\n                              use_label_encoder=False)\n        print(f\"\\n\\n*************** XGBClassifier with image features from DenseNet121, {img_size}x{img_size} input, SVD {svd_n_components} ***************\")\n        avg_fit_time, avg_accuracy, avg_QWK = evaluate_model(\n                    Pipeline(steps=pipeline_transformers + [('model', model)]),\n                    cv, X, y, model_type=\"classification\", display_results=False)\n        \n        evaluation_results = evaluation_results.append({\"Input size\": img_size,\n                            \"SVD-n_components\": svd_n_components, \"Average fit time\": avg_fit_time,\n                            \"Average accuracy\": avg_accuracy, \"Average QWK\": avg_QWK},\n                                                       ignore_index=True)\n        \ndisplay(evaluation_results)","metadata":{"execution":{"iopub.status.busy":"2021-06-22T18:01:01.640008Z","iopub.execute_input":"2021-06-22T18:01:01.640641Z","iopub.status.idle":"2021-06-22T19:19:08.84828Z","shell.execute_reply.started":"2021-06-22T18:01:01.640596Z","shell.execute_reply":"2021-06-22T19:19:08.847282Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"evaluation_results.groupby([\"Input size\"]).mean()[[\"Average accuracy\", \"Average QWK\"]]","metadata":{"execution":{"iopub.status.busy":"2021-06-22T19:19:08.849538Z","iopub.execute_input":"2021-06-22T19:19:08.849785Z","iopub.status.idle":"2021-06-22T19:19:08.863961Z","shell.execute_reply.started":"2021-06-22T19:19:08.849759Z","shell.execute_reply":"2021-06-22T19:19:08.862823Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del densenet121_var_img_size_dfs\nto_latex(evaluation_results, \"evaluation_results_XGBoostClassifier_DenseNet121__var-img_size__var-SVD-n_components\")\nto_latex(evaluation_results, \"evaluation_results_XGBoostClassifier_DenseNet121__var-img_size__var-SVD-n_components__GROUPED\", [\"Input size\"])","metadata":{"execution":{"iopub.status.busy":"2021-06-22T19:19:08.865662Z","iopub.execute_input":"2021-06-22T19:19:08.866225Z","iopub.status.idle":"2021-06-22T19:19:08.91682Z","shell.execute_reply.started":"2021-06-22T19:19:08.866183Z","shell.execute_reply":"2021-06-22T19:19:08.915565Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we can see, the best balance between the average accuracy and average QWK is obtained with an image size of 384x384, but the results are very close to those obtained with 256x256 images, so there is no need to change of input image size. Moreover, if we want to perform transfer learning and train the FC layers (top) of a pretrained CNN, it will be better to resize to 256x256 instead of 384x384 or 512x512, as in those latter cases it will take more time to preprocess, plus the fact that if we want to avoid resizing the images every time we train the CNN, we could resize every image once and save them into a new dataset, so a size of 256x256 will be better in terms of disk storage.","metadata":{}},{"cell_type":"markdown","source":"### Training + extraction?","metadata":{}},{"cell_type":"markdown","source":"Now, we will perform transfer learning: taking the architecture of the DenseNet121 convolutional layers, the ones that allow us to extract high-level features from input images, and then add some Dense (fully connected) layers on top, in order to perform training on those, while freezing the weights of the bottom layers so that they still retain the weights used for imagenet (that is, we are changing the previous classifier, for imagenet, by another one, that is really what is going on). The goal now is to check whether the Dense layers can give us better features than the ones obtained previously (the best so far, evaluated with XGBoostClassifier and 5-CV, are those obtained from the raw image features, that is, no Average Pooling after the GlobalAveragePooling2D that condenses and flattens the output of the series of, mainly, convolutions, and then reducing those 1024 features to 16 using SVD), by means of training the weights between the layers on top to our target, AdoptionSpeed, or even some of the most representative predictor variables.","metadata":{}},{"cell_type":"markdown","source":"First, we will resize all the train images (we will use all the images, not just the profile images) and we will save them in order to create a new dataset, so that we can avoid the resizing step in the preprocessing function:","metadata":{}},{"cell_type":"code","source":"def resize_and_save_train_images(directory, img_size):\n    try:\n        if not os.path.isdir(directory):\n            os.mkdir(directory)\n        ife = ImageFeatureExtractor(use_utils_only=True, img_size=img_size)\n        for image_path in tqdm(glob.iglob(\"../input/petfinder-adoption-prediction/train_images/*.jpg\")):\n            new_image_path = f\"{directory}/{image_path[image_path.rindex('/')+1:]}\"\n            if not os.path.exists(new_image_path):\n                image = cv2.imread(image_path)\n                resized_image = ife.resize_to_square(image)\n                resized_image = resized_image[...,::-1].astype(np.float32) # BGR to RGB + uint8 to float32\n                pil_image = array_to_img(resized_image)\n                pil_image.save(new_image_path)\n        shutil.make_archive(\"resized_train_images\", \"zip\", directory)\n    except Exception as e:\n        print(e)","metadata":{"execution":{"iopub.status.busy":"2021-06-05T10:41:00.426933Z","iopub.execute_input":"2021-06-05T10:41:00.427303Z","iopub.status.idle":"2021-06-05T10:41:00.43641Z","shell.execute_reply.started":"2021-06-05T10:41:00.427271Z","shell.execute_reply":"2021-06-05T10:41:00.435311Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# resize_and_save_train_images(\"./resized_train_images\", 256)","metadata":{"execution":{"iopub.status.busy":"2021-06-05T10:41:09.408901Z","iopub.execute_input":"2021-06-05T10:41:09.40945Z","iopub.status.idle":"2021-06-05T10:57:50.205814Z","shell.execute_reply.started":"2021-06-05T10:41:09.409403Z","shell.execute_reply":"2021-06-05T10:57:50.20432Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now, as we will use the class `ImageDataGenerator` in order to try different data augmentation methods and its `flow_from_dataframe` method, we will have to create a `DataFrame` with a column for the name of each file and other columns for the target values (one-hot encoded for classification, raw for regression).","metadata":{}},{"cell_type":"code","source":"def get_dataframe_CNN_training_AdoptionSpeed(directory, train_df):\n    filenames_and_target_values = {}\n    \n    for j, image_path in tqdm(enumerate(glob.iglob(f\"{directory}/*.jpg\"))):\n        filename = image_path[image_path.rindex('/')+1:]\n        pet_id = filename[:filename.rindex('-')]\n        y_value = train_df.loc[train_df[\"PetID\"] == pet_id, \"AdoptionSpeed\"].values[0]\n        filenames_and_target_values[j] = {}\n        filenames_and_target_values[j][\"filename\"] = filename\n        filenames_and_target_values[j][\"AdoptionSpeed\"] = y_value\n        for i in range(5):\n            filenames_and_target_values[j][f\"AdoptionSpeed_{i}\"] = 0\n        filenames_and_target_values[j][f\"AdoptionSpeed_{y_value}\"] = 1\n    \n    filenames_and_target_values_df = pd.DataFrame.from_dict(filenames_and_target_values, orient=\"index\")\n    filenames_and_target_values_df = filenames_and_target_values_df.reset_index()\n    filenames_and_target_values_df.drop([\"index\"], axis=1, inplace=True)\n    filenames_and_target_values_df.to_csv(\"dataframe_CNN_training.csv\", index=False)\n    \n    return filenames_and_target_values_df","metadata":{"execution":{"iopub.status.busy":"2021-06-05T18:47:27.884515Z","iopub.execute_input":"2021-06-05T18:47:27.884895Z","iopub.status.idle":"2021-06-05T18:47:27.894204Z","shell.execute_reply.started":"2021-06-05T18:47:27.884864Z","shell.execute_reply":"2021-06-05T18:47:27.892873Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# filenames_and_target_values = get_dataframe_CNN_training_AdoptionSpeed(\n#     \"../input/tfg-pet-adoption-resized-train-images\", train)","metadata":{"execution":{"iopub.status.busy":"2021-06-05T18:47:29.321513Z","iopub.execute_input":"2021-06-05T18:47:29.321878Z","iopub.status.idle":"2021-06-05T18:49:57.247327Z","shell.execute_reply.started":"2021-06-05T18:47:29.321848Z","shell.execute_reply":"2021-06-05T18:49:57.246049Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"filenames_and_target_values = pd.read_csv(\"../input/tfg-pet-adoption-data/dataframe_CNN_training.csv\")\nfilenames_and_target_values[\"PetID\"] = filenames_and_target_values[\"filename\"].apply(lambda x: str(x)[:str(x).rindex('-')])\nfilenames_and_target_values","metadata":{"execution":{"iopub.status.busy":"2021-06-29T07:19:16.844417Z","iopub.execute_input":"2021-06-29T07:19:16.844851Z","iopub.status.idle":"2021-06-29T07:19:17.020313Z","shell.execute_reply.started":"2021-06-29T07:19:16.844818Z","shell.execute_reply":"2021-06-29T07:19:17.019156Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now, we will create to `ImageDataGenerator` instances: one for training, with data augmentation, and another for validation, without data augmentation. This way we will validate on unseen images.","metadata":{}},{"cell_type":"code","source":"train_generator = ImageDataGenerator(\n    horizontal_flip=True,\n    preprocessing_function=preprocess_input_densenet\n)\n\nval_generator = ImageDataGenerator(\n    preprocessing_function=preprocess_input_densenet\n)","metadata":{"execution":{"iopub.status.busy":"2021-06-28T07:25:37.865811Z","iopub.execute_input":"2021-06-28T07:25:37.866317Z","iopub.status.idle":"2021-06-28T07:25:37.871461Z","shell.execute_reply.started":"2021-06-28T07:25:37.86626Z","shell.execute_reply":"2021-06-28T07:25:37.870255Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now, we will divide the DataFrame with the filenames and their correspondant target value (both 'raw' and one-hot encoded) into a training set and a validation set, in a stratified manner on the variable PetID (this way, a single instance will have all its images either in the training or in the validation set, but not in both, so that our validation estimation is not optimistic, as happened in Version 58 and before, since some instances could be validated with a model trained on some of its images), especially in order to have instances with outcome 0 in the validation set:","metadata":{}},{"cell_type":"code","source":"pet_ids_train, pet_ids_val, _, _ = train_test_split(\n    train[[\"PetID\", \"AdoptionSpeed\"]], train[\"AdoptionSpeed\"],\n    test_size=0.2, stratify=train[\"AdoptionSpeed\"], random_state=seed)","metadata":{"execution":{"iopub.status.busy":"2021-06-29T07:19:20.608928Z","iopub.execute_input":"2021-06-29T07:19:20.609303Z","iopub.status.idle":"2021-06-29T07:19:20.635416Z","shell.execute_reply.started":"2021-06-29T07:19:20.609263Z","shell.execute_reply":"2021-06-29T07:19:20.634304Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pet_ids_train[\"AdoptionSpeed\"].value_counts(normalize=True)","metadata":{"execution":{"iopub.status.busy":"2021-06-29T07:19:21.042229Z","iopub.execute_input":"2021-06-29T07:19:21.042615Z","iopub.status.idle":"2021-06-29T07:19:21.052832Z","shell.execute_reply.started":"2021-06-29T07:19:21.042566Z","shell.execute_reply":"2021-06-29T07:19:21.051643Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pet_ids_val[\"AdoptionSpeed\"].value_counts(normalize=True)","metadata":{"execution":{"iopub.status.busy":"2021-06-29T07:19:21.583592Z","iopub.execute_input":"2021-06-29T07:19:21.583975Z","iopub.status.idle":"2021-06-29T07:19:21.594272Z","shell.execute_reply.started":"2021-06-29T07:19:21.583934Z","shell.execute_reply":"2021-06-29T07:19:21.593306Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"set_pet_ids_train = set(pet_ids_train[\"PetID\"])\nfilenames_and_target_values_train_no_instance_overlap = filenames_and_target_values.loc[\n    filenames_and_target_values[\"PetID\"].isin(set_pet_ids_train)\n].copy()\nfilenames_and_target_values_val_no_instance_overlap = filenames_and_target_values.loc[\n    ~filenames_and_target_values[\"PetID\"].isin(set_pet_ids_train)\n].copy()","metadata":{"execution":{"iopub.status.busy":"2021-06-29T07:19:22.066638Z","iopub.execute_input":"2021-06-29T07:19:22.067011Z","iopub.status.idle":"2021-06-29T07:19:22.115202Z","shell.execute_reply.started":"2021-06-29T07:19:22.06698Z","shell.execute_reply":"2021-06-29T07:19:22.114186Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"filenames_and_target_values_train_no_instance_overlap","metadata":{"execution":{"iopub.status.busy":"2021-06-29T07:19:22.353588Z","iopub.execute_input":"2021-06-29T07:19:22.353993Z","iopub.status.idle":"2021-06-29T07:19:22.374762Z","shell.execute_reply.started":"2021-06-29T07:19:22.353956Z","shell.execute_reply":"2021-06-29T07:19:22.373498Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"filenames_and_target_values_val_no_instance_overlap","metadata":{"execution":{"iopub.status.busy":"2021-06-29T07:19:23.29349Z","iopub.execute_input":"2021-06-29T07:19:23.29385Z","iopub.status.idle":"2021-06-29T07:19:23.31496Z","shell.execute_reply.started":"2021-06-29T07:19:23.29382Z","shell.execute_reply":"2021-06-29T07:19:23.313478Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(set(filenames_and_target_values_train_no_instance_overlap[\"PetID\"]) & set(filenames_and_target_values_val_no_instance_overlap[\"PetID\"]))\nprint(set(filenames_and_target_values_train_no_instance_overlap[\"filename\"]) & set(filenames_and_target_values_val_no_instance_overlap[\"filename\"]))","metadata":{"execution":{"iopub.status.busy":"2021-06-29T07:19:23.706088Z","iopub.execute_input":"2021-06-29T07:19:23.706438Z","iopub.status.idle":"2021-06-29T07:19:23.743415Z","shell.execute_reply.started":"2021-06-29T07:19:23.706407Z","shell.execute_reply":"2021-06-29T07:19:23.742384Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"filenames_and_target_values_train_no_instance_overlap[\"AdoptionSpeed\"].value_counts(normalize=True)","metadata":{"execution":{"iopub.status.busy":"2021-06-29T07:19:24.408263Z","iopub.execute_input":"2021-06-29T07:19:24.408655Z","iopub.status.idle":"2021-06-29T07:19:24.417925Z","shell.execute_reply.started":"2021-06-29T07:19:24.408619Z","shell.execute_reply":"2021-06-29T07:19:24.416788Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"filenames_and_target_values_val_no_instance_overlap[\"AdoptionSpeed\"].value_counts(normalize=True)","metadata":{"execution":{"iopub.status.busy":"2021-06-29T07:19:24.955785Z","iopub.execute_input":"2021-06-29T07:19:24.956289Z","iopub.status.idle":"2021-06-29T07:19:24.965782Z","shell.execute_reply.started":"2021-06-29T07:19:24.956255Z","shell.execute_reply":"2021-06-29T07:19:24.964954Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Look how the proportion of images of 4 is considerably smaller than the original (28%), since we know that when profiles don't have images, it is very likely that the outcome is 4, plus the fact that we will train over all the images, not just the profile images (so the dicrease is more than 4.4%, while the instances with no profile image whose outcome is 4 is approximately the 1.4% of the total; this is due to the fact that many of the profiles have more than one photo).**","metadata":{}},{"cell_type":"markdown","source":"Now, we have to take into account that if we want to test how well the extracted features perform using some model like XGBClassifier in a 5-CV validation strategy, as before, we may obtain optimistic results, since we will train the models with a single Training+Validation split (that is, in each iteration of the 5-CV we may validate extracting features from instances whose profile image was used in the CNN training). Of course we could perform a 5-CV for the training of the CNN, but this would take so much time, that it would be impossible to train some of the models that we will see (which train with multiple data augmentation strategies) without exceding Kaggle's session time limit of 9 hours. Hence, we keep the single split training+validation of the CNNs, but in addition to the 5-CV training and validation strategy, we will also validate the single split of the cell above:","metadata":{}},{"cell_type":"code","source":"X_train_CNN = X.loc[X[\"PetID\"].isin(set_pet_ids_train)].copy().sample(frac=1, random_state=seed)\ny_train_CNN = train.loc[train[\"PetID\"].isin(set_pet_ids_train), \"AdoptionSpeed\"].copy().sample(frac=1, random_state=seed)\nX_val_CNN = X.loc[~X[\"PetID\"].isin(set_pet_ids_train)].copy().sample(frac=1, random_state=seed)\ny_val_CNN = train.loc[~train[\"PetID\"].isin(set_pet_ids_train), \"AdoptionSpeed\"].copy().sample(frac=1, random_state=seed)","metadata":{"execution":{"iopub.status.busy":"2021-06-29T07:19:26.635701Z","iopub.execute_input":"2021-06-29T07:19:26.636312Z","iopub.status.idle":"2021-06-29T07:19:26.67857Z","shell.execute_reply.started":"2021-06-29T07:19:26.636276Z","shell.execute_reply":"2021-06-29T07:19:26.677611Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train_CNN.head(5)","metadata":{"execution":{"iopub.status.busy":"2021-06-29T07:19:27.602113Z","iopub.execute_input":"2021-06-29T07:19:27.602778Z","iopub.status.idle":"2021-06-29T07:19:27.624418Z","shell.execute_reply.started":"2021-06-29T07:19:27.602739Z","shell.execute_reply":"2021-06-29T07:19:27.623292Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train_CNN.head(5)","metadata":{"execution":{"iopub.status.busy":"2021-06-29T07:19:28.139372Z","iopub.execute_input":"2021-06-29T07:19:28.139734Z","iopub.status.idle":"2021-06-29T07:19:28.146374Z","shell.execute_reply.started":"2021-06-29T07:19:28.139701Z","shell.execute_reply":"2021-06-29T07:19:28.145667Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_val_CNN.head(5)","metadata":{"execution":{"iopub.status.busy":"2021-06-29T07:19:28.70828Z","iopub.execute_input":"2021-06-29T07:19:28.708642Z","iopub.status.idle":"2021-06-29T07:19:28.729334Z","shell.execute_reply.started":"2021-06-29T07:19:28.70861Z","shell.execute_reply":"2021-06-29T07:19:28.728603Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_val_CNN.head(5)","metadata":{"execution":{"iopub.status.busy":"2021-06-29T07:19:29.319392Z","iopub.execute_input":"2021-06-29T07:19:29.319997Z","iopub.status.idle":"2021-06-29T07:19:29.327587Z","shell.execute_reply.started":"2021-06-29T07:19:29.319943Z","shell.execute_reply":"2021-06-29T07:19:29.326669Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def evaluate_model_single_split(model, X_train, X_test, y_train, y_test, model_type,\n                           display_results=True, coefficients=[0.5, 1.5, 2.5, 3.5]):\n    orig_model = model\n    model = clone(orig_model)\n    start = time.time() \n    with warnings.catch_warnings():\n        warnings.simplefilter(\"ignore\")\n        model.fit(X_train, y_train)\n    end = time.time()\n\n    y_pred = model.predict(X_test)\n\n    if model_type == 'regression':\n        rmse_value = mean_squared_error(y_test, y_pred, squared=False)\n        y_pred = round_reg_predictions(y_pred, coefficients)\n\n    accuracy_value = accuracy_score(y_test, y_pred)\n\n    qwk_value = cohen_kappa_score(y_test, y_pred, weights='quadratic')\n\n    if display_results:\n        print(\"\\n----------------- RESULTS SINGLE TRAIN-VAL SPLIT -----------------\")\n        print(f\"Fit time: {end-start} s\")\n        if model_type == 'regression':\n            print(\"\\nRMSE:\", rmse_value)\n            print(\"Accuracy:\", accuracy_value)\n        else:\n            print(\"\\nAccuracy:\", accuracy_value)\n        print(\"QWK:\", qwk_value)\n        print(\"\\nClassification report:\")\n        print(classification_report(y_test, y_pred))\n\n    return accuracy_value, qwk_value","metadata":{"execution":{"iopub.status.busy":"2021-06-29T07:19:29.781797Z","iopub.execute_input":"2021-06-29T07:19:29.78248Z","iopub.status.idle":"2021-06-29T07:19:29.793131Z","shell.execute_reply.started":"2021-06-29T07:19:29.782423Z","shell.execute_reply":"2021-06-29T07:19:29.792228Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Training on AdoptionSpeed","metadata":{}},{"cell_type":"markdown","source":"In this section, we will train three different models on the target variable, AdoptionSpeed. These models treat the predictions as classification, regression and ordinal regression, respectively.","metadata":{}},{"cell_type":"markdown","source":"##### Classification","metadata":{}},{"cell_type":"markdown","source":"First of all, we define the iterators that we will use to extract batches of training and validation images:","metadata":{}},{"cell_type":"code","source":"batch_size = 32\ntarget_size = (256,256)\nclassification_target_columns = [f\"AdoptionSpeed_{i}\" for i in range(5)]\n\nclassification_train_iterator = train_generator.flow_from_dataframe(\n    dataframe=filenames_and_target_values_train_no_instance_overlap,\n    directory=\"../input/tfg-pet-adoption-resized-train-images/\",\n    x_col=\"filename\",\n    y_col=classification_target_columns,\n    target_size=target_size,\n    class_mode=\"raw\",\n    batch_size=batch_size,\n    shuffle=True,\n    seed=seed\n)\n\nclassification_val_iterator = val_generator.flow_from_dataframe(\n    dataframe=filenames_and_target_values_val_no_instance_overlap,\n    directory=\"../input/tfg-pet-adoption-resized-train-images/\",\n    x_col=\"filename\",\n    y_col=classification_target_columns,\n    target_size=target_size,\n    class_mode=\"raw\",\n    batch_size=batch_size,\n    shuffle=False,\n    seed=seed\n)","metadata":{"execution":{"iopub.status.busy":"2021-06-15T19:02:42.160886Z","iopub.execute_input":"2021-06-15T19:02:42.161136Z","iopub.status.idle":"2021-06-15T19:04:46.827844Z","shell.execute_reply.started":"2021-06-15T19:02:42.161113Z","shell.execute_reply":"2021-06-15T19:04:46.826958Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"But first of all, as we are going to compare these three models using the results (from the 5-CV) obtained when running XGBClassifier using the extracted image features, we will first define a common setup for the Dense layers that we are going to add on top of DenseNet121's backbone. We will define the number of Dense layers (1 or 2) and the number of neurons of each one (16, 32 or 64) by training and validating the classification model on 5 epochs:\n\n**https://www.tensorflow.org/addons/api_docs/python/tfa/metrics/CohenKappa (https://github.com/tensorflow/addons/tree/master)**","metadata":{}},{"cell_type":"code","source":"combinations = [\n    [16],\n    [32],\n    [64],\n    [64,32],\n    [64,16],\n    [32,16]\n]\n\nevaluation_results = pd.DataFrame([], columns=[\"Combination\", \"loss\",\n                        \"accuracy\", \"qwk\", \"val_loss\", \"val_accuracy\", \"val_qwk\"])\n\nfor combination in combinations:\n    print(f\"\\n\\n{len(combination)} layer{'s' if len(combination) > 1 else ''} {tuple(combination)}\")\n    inp = Input((256,256,3))\n    backbone = DenseNet121(input_tensor=inp, include_top=False, weights=\"imagenet\")\n    \n    x = backbone.output\n    x = GlobalAveragePooling2D()(x)\n    \n    for i in range(len(combination)):\n        x = Dense(combination[i], activation=\"relu\")(x)\n    \n    out = Dense(5, activation=\"softmax\")(x)\n\n    # Freezing DenseNet121 backbone\n    for layer in backbone.layers:\n        layer.trainable = False\n\n    model = Model(inp,out)\n    \n    optimizer = Adam(lr=0.001)\n    metrics = [\"accuracy\", cohen_kappa_keras(num_classes=5, name=\"qwk\", weightage=\"quadratic\")]\n    loss = \"categorical_crossentropy\"\n    \n    model.compile(loss=loss, metrics=metrics, optimizer=optimizer)\n    \n    epochs = 5\n    batch_size = 32\n    history = model.fit(x=classification_train_iterator,\n                        validation_data=classification_val_iterator,\n                        batch_size=batch_size, epochs=epochs)\n    \n    train_loss = history.history[\"loss\"]\n    train_accuracy = history.history[\"accuracy\"]\n    train_qwk = history.history[\"qwk\"]\n    validation_loss = history.history[\"val_loss\"]\n    validation_accuracy = history.history[\"val_accuracy\"]\n    validation_qwk = history.history[\"val_qwk\"]\n    \n    for i in range(len(train_loss)):\n        evaluation_results = evaluation_results.append({\n            \"Combination\": f\"{len(combination)}_{tuple(combination)}\",\n            \"loss\": train_loss[i], \"accuracy\": train_accuracy[i], \"qwk\": train_qwk[i],\n            \"val_loss\": validation_loss[i], \"val_accuracy\": validation_accuracy[i],\n            \"val_qwk\": validation_qwk[i]\n        }, ignore_index=True)\n\nevaluation_results.to_csv(\"tuning_n-layers_size_DenseNet121_transfer-learning.csv\", index=False)\nto_latex(evaluation_results, \"tuning_n-layers_size_DenseNet121_transfer-learning\")\nto_latex(evaluation_results, \"tuning_n-layers_size_DenseNet121_transfer-learning__GROUPED\", [\"Combination\"])","metadata":{"execution":{"iopub.status.busy":"2021-06-15T19:04:46.829193Z","iopub.execute_input":"2021-06-15T19:04:46.82969Z","iopub.status.idle":"2021-06-15T20:59:11.251232Z","shell.execute_reply.started":"2021-06-15T19:04:46.82965Z","shell.execute_reply":"2021-06-15T20:59:11.25046Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"evaluation_results = pd.read_csv(\"../input/tfg-pet-adoption-data/tuning_n-layers_size_DenseNet121_transfer-learning.csv\")\nevaluation_results","metadata":{"execution":{"iopub.status.busy":"2021-06-25T17:28:11.232248Z","iopub.execute_input":"2021-06-25T17:28:11.232645Z","iopub.status.idle":"2021-06-25T17:28:11.236213Z","shell.execute_reply.started":"2021-06-25T17:28:11.232592Z","shell.execute_reply":"2021-06-25T17:28:11.235019Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"evaluation_results.groupby([\"Combination\"]).mean()","metadata":{"execution":{"iopub.status.busy":"2021-06-25T17:28:23.736489Z","iopub.execute_input":"2021-06-25T17:28:23.736933Z","iopub.status.idle":"2021-06-25T17:28:23.740831Z","shell.execute_reply.started":"2021-06-25T17:28:23.736882Z","shell.execute_reply":"2021-06-25T17:28:23.739745Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we can see, the best setup in terms of all the validation metrics is the one with 2 layers, the first one 64 and the second one (penultimate layer of the CNN) 16. Thus, we will create the three models that will be trained on more epochs using this architecture of dense layers:","metadata":{}},{"cell_type":"markdown","source":"Now, let's define the classification CNN model with the aforementioned Dense layers setup:","metadata":{}},{"cell_type":"code","source":"inp = Input((256,256,3))\nbackbone = DenseNet121(input_tensor=inp, include_top=False, weights=\"imagenet\")\nx = backbone.output\nx = GlobalAveragePooling2D()(x)\nx = Dense(64, activation=\"relu\")(x)\nx = Dense(16, activation=\"relu\")(x)\nout = Dense(5, activation=\"softmax\")(x)\n\n# Freezing DenseNet121 backbone\nfor layer in backbone.layers:\n    layer.trainable = False\n\nclassification_model = Model(inp,out)\n\noptimizer = Adam(lr=0.001)\nmetrics = [\"accuracy\", cohen_kappa_keras(num_classes=5, name=\"qwk\", weightage=\"quadratic\")]\nloss = \"categorical_crossentropy\"\n    \nclassification_model.compile(loss=loss, metrics=metrics, optimizer=optimizer)","metadata":{"execution":{"iopub.status.busy":"2021-06-13T21:05:49.5531Z","iopub.execute_input":"2021-06-13T21:05:49.55343Z","iopub.status.idle":"2021-06-13T21:05:56.24625Z","shell.execute_reply.started":"2021-06-13T21:05:49.553401Z","shell.execute_reply":"2021-06-13T21:05:56.245486Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"epochs = 5\nbatch_size = 32\n\nclassification_evaluation_results = pd.DataFrame([], columns=[\"loss\",\n                        \"accuracy\", \"qwk\", \"val_loss\", \"val_accuracy\", \"val_qwk\"])\n    \nclassification_history = classification_model.fit(x=classification_train_iterator,\n                    validation_data=classification_val_iterator,\n                    batch_size=batch_size, epochs=epochs)\n\ntrain_loss = classification_history.history[\"loss\"]\ntrain_accuracy = classification_history.history[\"accuracy\"]\ntrain_qwk = classification_history.history[\"qwk\"]\nvalidation_loss = classification_history.history[\"val_loss\"]\nvalidation_accuracy = classification_history.history[\"val_accuracy\"]\nvalidation_qwk = classification_history.history[\"val_qwk\"]\n\nfor i in range(len(train_loss)):\n    classification_evaluation_results = classification_evaluation_results.append({\n        \"loss\": train_loss[i], \"accuracy\": train_accuracy[i], \"qwk\": train_qwk[i],\n        \"val_loss\": validation_loss[i], \"val_accuracy\": validation_accuracy[i],\n        \"val_qwk\": validation_qwk[i]\n    }, ignore_index=True)\n\nclassification_model.save(f\"DenseNet121_classification_64-16_{epochs}-epochs.h5\")\nclassification_evaluation_results.to_csv(f\"DenseNet121_classification_64-16_{epochs}-epochs.csv\", index=False)\nto_latex(classification_evaluation_results, f\"DenseNet121_classification_64-16_{epochs}-epochs\")","metadata":{"execution":{"iopub.status.busy":"2021-06-13T21:06:05.203295Z","iopub.execute_input":"2021-06-13T21:06:05.203624Z","iopub.status.idle":"2021-06-13T21:27:31.438289Z","shell.execute_reply.started":"2021-06-13T21:06:05.203594Z","shell.execute_reply":"2021-06-13T21:27:31.437396Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"classification_evaluation_results = pd.read_csv(\"../input/tfg-pet-adoption-data/DenseNet121_classification_64-16_5-epochs.csv\")\nplot_history(classification_evaluation_results, [\"loss\", \"accuracy\", \"qwk\"], 1, 3, (20,5))","metadata":{"execution":{"iopub.status.busy":"2021-06-22T19:19:09.416097Z","iopub.execute_input":"2021-06-22T19:19:09.416422Z","iopub.status.idle":"2021-06-22T19:19:09.906375Z","shell.execute_reply.started":"2021-06-22T19:19:09.416393Z","shell.execute_reply":"2021-06-22T19:19:09.905328Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"classification_model = load_model(\"../input/tfg-pet-adoption-data/DenseNet121_classification_64-16_5-epochs.h5\")","metadata":{"execution":{"iopub.status.busy":"2021-06-15T20:59:11.301597Z","iopub.execute_input":"2021-06-15T20:59:11.301956Z","iopub.status.idle":"2021-06-15T20:59:16.199732Z","shell.execute_reply.started":"2021-06-15T20:59:11.301917Z","shell.execute_reply":"2021-06-15T20:59:16.198979Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = Model(inputs=classification_model.input, outputs=classification_model.layers[-3].output)\nife = ImageFeatureExtractor(construct_from_cnn_backbone=False, model=model,\n                 images_directory=\"../input/petfinder-adoption-prediction/train_images\",\n                 from_image=True, preprocess_input=preprocess_input_densenet, img_size=256,\n                 model_name=\"DenseNet121_classification_layer-64_5-epochs\",\n                 save=True, debug=True, include_feats=False)\n\n_ = ife.fit_transform(X, y)","metadata":{"execution":{"iopub.status.busy":"2021-06-13T21:29:31.073243Z","iopub.execute_input":"2021-06-13T21:29:31.073613Z","iopub.status.idle":"2021-06-13T21:34:02.152913Z","shell.execute_reply.started":"2021-06-13T21:29:31.073578Z","shell.execute_reply":"2021-06-13T21:34:02.151767Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"classification_model_layer_64_features_5e = pd.read_csv(\n    \"../input/tfg-pet-adoption-data/image_features_DenseNet121_classification_layer-64_5-epochs_in-256.csv\",\n    index_col=0)\ndisplay(classification_model_layer_64_features_5e.head(5))\nclassification_model_layer_64_features_5e.describe().loc[\"std\",:].value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-06-22T19:19:09.907821Z","iopub.execute_input":"2021-06-22T19:19:09.908256Z","iopub.status.idle":"2021-06-22T19:19:10.275287Z","shell.execute_reply.started":"2021-06-22T19:19:09.908203Z","shell.execute_reply":"2021-06-22T19:19:10.274294Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = Model(inputs=classification_model.input, outputs=classification_model.layers[-2].output)\nife = ImageFeatureExtractor(construct_from_cnn_backbone=False, model=model,\n                 images_directory=\"../input/petfinder-adoption-prediction/train_images\",\n                 from_image=True, preprocess_input=preprocess_input_densenet, img_size=256,\n                 model_name=\"DenseNet121_classification_layer-16_5-epochs\",\n                 save=True, debug=True, include_feats=False)\n\n_ = ife.fit_transform(X, y)","metadata":{"execution":{"iopub.status.busy":"2021-06-13T21:34:02.402258Z","iopub.execute_input":"2021-06-13T21:34:02.402629Z","iopub.status.idle":"2021-06-13T21:36:34.518825Z","shell.execute_reply.started":"2021-06-13T21:34:02.402591Z","shell.execute_reply":"2021-06-13T21:36:34.51721Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"classification_model_layer_16_features_5e = pd.read_csv(\n    \"../input/tfg-pet-adoption-data/image_features_DenseNet121_classification_layer-16_5-epochs_in-256.csv\",\n    index_col=0)\ndisplay(classification_model_layer_16_features_5e.head(5))\nclassification_model_layer_16_features_5e.describe().loc[\"std\",:].value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-06-22T19:19:10.276585Z","iopub.execute_input":"2021-06-22T19:19:10.276845Z","iopub.status.idle":"2021-06-22T19:19:10.407999Z","shell.execute_reply.started":"2021-06-22T19:19:10.276818Z","shell.execute_reply":"2021-06-22T19:19:10.407003Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Regression","metadata":{}},{"cell_type":"code","source":"batch_size = 32\ntarget_size = (256,256)\n\nregression_train_iterator = train_generator.flow_from_dataframe(\n    dataframe=filenames_and_target_values_train_no_instance_overlap,\n    directory=\"../input/tfg-pet-adoption-resized-train-images/\",\n    x_col=\"filename\",\n    y_col=\"AdoptionSpeed\",\n    target_size=target_size,\n    class_mode=\"raw\",\n    batch_size=batch_size,\n    shuffle=True,\n    seed=seed\n)\n\nregression_val_iterator = val_generator.flow_from_dataframe(\n    dataframe=filenames_and_target_values_val_no_instance_overlap,\n    directory=\"../input/tfg-pet-adoption-resized-train-images/\",\n    x_col=\"filename\",\n    y_col=\"AdoptionSpeed\",\n    target_size=target_size,\n    class_mode=\"raw\",\n    batch_size=batch_size,\n    shuffle=False,\n    seed=seed\n)","metadata":{"execution":{"iopub.status.busy":"2021-06-13T21:45:42.163667Z","iopub.execute_input":"2021-06-13T21:45:42.164075Z","iopub.status.idle":"2021-06-13T21:45:46.272572Z","shell.execute_reply.started":"2021-06-13T21:45:42.164039Z","shell.execute_reply":"2021-06-13T21:45:46.271637Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"inp = Input((256,256,3))\nbackbone = DenseNet121(input_tensor=inp, include_top=False, weights=\"imagenet\")\nx = backbone.output\nx = GlobalAveragePooling2D()(x)\nx = Dense(64, activation=\"relu\")(x)\nx = Dense(16, activation=\"relu\")(x)\nout = Dense(1, activation=\"linear\")(x)\n\n# Freezing DenseNet121 backbone\nfor layer in backbone.layers:\n    layer.trainable = False\n\nregression_model = Model(inp,out)\n\noptimizer = Adam(lr=0.001)\nmetrics = [\"mean_absolute_error\"]\nloss = \"mean_squared_error\"\n    \nregression_model.compile(loss=loss, metrics=metrics, optimizer=optimizer)","metadata":{"execution":{"iopub.status.busy":"2021-06-13T21:45:46.274137Z","iopub.execute_input":"2021-06-13T21:45:46.274651Z","iopub.status.idle":"2021-06-13T21:45:48.630387Z","shell.execute_reply.started":"2021-06-13T21:45:46.274612Z","shell.execute_reply":"2021-06-13T21:45:48.629645Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"epochs = 5\nbatch_size = 32\n\nregression_evaluation_results = pd.DataFrame([], columns=[\"loss\",\n                        \"mean_absolute_error\", \"val_loss\",\n                        \"val_mean_absolute_error\"])\n    \nregression_history = regression_model.fit(x=regression_train_iterator,\n                    validation_data=regression_val_iterator,\n                    batch_size=batch_size, epochs=epochs)\n\ntrain_loss = regression_history.history[\"loss\"]\ntrain_mae = regression_history.history[\"mean_absolute_error\"]\nvalidation_loss = regression_history.history[\"val_loss\"]\nvalidation_mae = regression_history.history[\"val_mean_absolute_error\"]\n\nfor i in range(len(train_loss)):\n    regression_evaluation_results = regression_evaluation_results.append({\n        \"loss\": train_loss[i],\n        \"mean_absolute_error\": train_mae[i],\n        \"val_loss\": validation_loss[i],\n        \"val_mean_absolute_error\": validation_mae[i]\n    }, ignore_index=True)\n\nregression_model.save(f\"DenseNet121_regression_64-16_{epochs}-epochs.h5\")\nregression_evaluation_results.to_csv(f\"DenseNet121_regression_64-16_{epochs}-epochs.csv\", index=False)\nto_latex(regression_evaluation_results, f\"DenseNet121_regression_64-16_{epochs}-epochs\")","metadata":{"execution":{"iopub.status.busy":"2021-06-13T21:45:51.205668Z","iopub.execute_input":"2021-06-13T21:45:51.20599Z","iopub.status.idle":"2021-06-13T22:02:25.55402Z","shell.execute_reply.started":"2021-06-13T21:45:51.205955Z","shell.execute_reply":"2021-06-13T22:02:25.553216Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"regression_evaluation_results = pd.read_csv(\"../input/tfg-pet-adoption-data/DenseNet121_regression_64-16_5-epochs.csv\")\nplot_history(regression_evaluation_results, [\"loss\", \"mean_absolute_error\"], 1, 2, (12,3))","metadata":{"execution":{"iopub.status.busy":"2021-06-22T19:19:10.409241Z","iopub.execute_input":"2021-06-22T19:19:10.409505Z","iopub.status.idle":"2021-06-22T19:19:10.739055Z","shell.execute_reply.started":"2021-06-22T19:19:10.409477Z","shell.execute_reply":"2021-06-22T19:19:10.73814Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"regression_model = load_model(\"../input/tfg-pet-adoption-data/DenseNet121_regression_64-16_5-epochs.h5\")","metadata":{"execution":{"iopub.status.busy":"2021-06-15T20:59:16.939867Z","iopub.execute_input":"2021-06-15T20:59:16.940215Z","iopub.status.idle":"2021-06-15T20:59:20.388248Z","shell.execute_reply.started":"2021-06-15T20:59:16.94018Z","shell.execute_reply":"2021-06-15T20:59:20.387426Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = Model(inputs=regression_model.input, outputs=regression_model.layers[-3].output)\nife = ImageFeatureExtractor(construct_from_cnn_backbone=False, model=model,\n                 images_directory=\"../input/petfinder-adoption-prediction/train_images\",\n                 from_image=True, preprocess_input=preprocess_input_densenet, img_size=256,\n                 model_name=\"DenseNet121_regression_layer-64_5-epochs\",\n                 save=True, debug=True, include_feats=False)\n\n_ = ife.fit_transform(X, y)","metadata":{"execution":{"iopub.status.busy":"2021-06-13T22:02:25.555694Z","iopub.execute_input":"2021-06-13T22:02:25.556021Z","iopub.status.idle":"2021-06-13T22:04:56.728025Z","shell.execute_reply.started":"2021-06-13T22:02:25.555992Z","shell.execute_reply":"2021-06-13T22:04:56.725747Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"regression_model_layer_64_features_5e = pd.read_csv(\n    \"../input/tfg-pet-adoption-data/image_features_DenseNet121_regression_layer-64_5-epochs_in-256.csv\",\n    index_col=0)\ndisplay(regression_model_layer_64_features_5e.head(5))\nregression_model_layer_64_features_5e.describe().loc[\"std\",:].value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-06-22T19:19:10.740495Z","iopub.execute_input":"2021-06-22T19:19:10.740764Z","iopub.status.idle":"2021-06-22T19:19:11.089608Z","shell.execute_reply.started":"2021-06-22T19:19:10.740735Z","shell.execute_reply":"2021-06-22T19:19:11.088604Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = Model(inputs=regression_model.input, outputs=regression_model.layers[-2].output)\nife = ImageFeatureExtractor(construct_from_cnn_backbone=False, model=model,\n                 images_directory=\"../input/petfinder-adoption-prediction/train_images\",\n                 from_image=True, preprocess_input=preprocess_input_densenet, img_size=256,\n                 model_name=\"DenseNet121_regression_layer-16_5-epochs\",\n                 save=True, debug=True, include_feats=False)\n\n_ = ife.fit_transform(X, y)","metadata":{"execution":{"iopub.status.busy":"2021-06-13T22:04:57.054919Z","iopub.execute_input":"2021-06-13T22:04:57.055263Z","iopub.status.idle":"2021-06-13T22:07:31.513366Z","shell.execute_reply.started":"2021-06-13T22:04:57.055228Z","shell.execute_reply":"2021-06-13T22:07:31.512434Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"regression_model_layer_16_features_5e = pd.read_csv(\n    \"../input/tfg-pet-adoption-data/image_features_DenseNet121_regression_layer-16_5-epochs_in-256.csv\",\n    index_col=0)\ndisplay(regression_model_layer_16_features_5e.head(5))\nregression_model_layer_16_features_5e.describe().loc[\"std\",:].value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-06-22T19:19:11.09079Z","iopub.execute_input":"2021-06-22T19:19:11.091072Z","iopub.status.idle":"2021-06-22T19:19:11.218729Z","shell.execute_reply.started":"2021-06-22T19:19:11.091041Z","shell.execute_reply":"2021-06-22T19:19:11.217957Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Ordinal regression","metadata":{}},{"cell_type":"code","source":"batch_size = 32\ntarget_size = (256,256)\n\nordinal_regression_train_iterator = train_generator.flow_from_dataframe(\n    dataframe=filenames_and_target_values_train_no_instance_overlap,\n    directory=\"../input/tfg-pet-adoption-resized-train-images/\",\n    x_col=\"filename\",\n    y_col=\"AdoptionSpeed\",\n    target_size=target_size,\n    class_mode=\"raw\",\n    batch_size=batch_size,\n    shuffle=True,\n    seed=seed\n)\n\nordinal_regression_val_iterator = val_generator.flow_from_dataframe(\n    dataframe=filenames_and_target_values_val_no_instance_overlap,\n    directory=\"../input/tfg-pet-adoption-resized-train-images/\",\n    x_col=\"filename\",\n    y_col=\"AdoptionSpeed\",\n    target_size=target_size,\n    class_mode=\"raw\",\n    batch_size=batch_size,\n    shuffle=False,\n    seed=seed\n)","metadata":{"execution":{"iopub.status.busy":"2021-06-13T22:12:48.862712Z","iopub.execute_input":"2021-06-13T22:12:48.863089Z","iopub.status.idle":"2021-06-13T22:13:07.664852Z","shell.execute_reply.started":"2021-06-13T22:12:48.863056Z","shell.execute_reply":"2021-06-13T22:13:07.663304Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**https://arxiv.org/pdf/1901.07884.pdf, https://github.com/ck37/coral-ordinal, https://colab.research.google.com/drive/1AQl4XeqRRhd7l30bmgLVObKt5RFPHttn**\n\n**Issue https://github.com/ck37/coral-ordinal/issues/1 solved in github repo, but not in pip**","metadata":{}},{"cell_type":"code","source":"!cp -r ../input/tfg-pet-adoption-data/coral-ordinal-master/coral-ordinal-master/* ./\n!python setup.py install\nimport coral_ordinal as coral","metadata":{"execution":{"iopub.status.busy":"2021-06-23T07:53:21.682429Z","iopub.execute_input":"2021-06-23T07:53:21.682983Z","iopub.status.idle":"2021-06-23T07:53:26.744954Z","shell.execute_reply.started":"2021-06-23T07:53:21.682939Z","shell.execute_reply":"2021-06-23T07:53:26.743738Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"inp = Input((256,256,3))\nbackbone = DenseNet121(input_tensor=inp, include_top=False, weights=\"imagenet\")\nx = backbone.output\nx = GlobalAveragePooling2D()(x)\nx = Dense(64, activation=\"relu\")(x)\nx = Dense(16, activation=\"relu\")(x)\nout = coral.CoralOrdinal(num_classes=5)(x)\n\n# Freezing DenseNet121 backbone\nfor layer in backbone.layers:\n    layer.trainable = False\n\nordinal_regression_model = Model(inp,out)\n\noptimizer = Adam(lr=0.001)\nmetrics = [coral.MeanAbsoluteErrorLabels()]\nloss = coral.OrdinalCrossEntropy(num_classes=5)                              \n        \nordinal_regression_model.compile(loss=loss, metrics=metrics, optimizer=optimizer)","metadata":{"execution":{"iopub.status.busy":"2021-06-13T22:13:11.851701Z","iopub.execute_input":"2021-06-13T22:13:11.851989Z","iopub.status.idle":"2021-06-13T22:13:14.51899Z","shell.execute_reply.started":"2021-06-13T22:13:11.851961Z","shell.execute_reply":"2021-06-13T22:13:14.518233Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"epochs = 5\nbatch_size = 32\n\nordinal_regression_evaluation_results = pd.DataFrame([], columns=[\"loss\",\n                        \"mean_absolute_error_labels\", \"val_loss\",\n                        \"val_mean_absolute_error_labels\"])\n    \nordinal_regression_history = ordinal_regression_model.fit(\n                    x=ordinal_regression_train_iterator,\n                    validation_data=ordinal_regression_val_iterator,\n                    batch_size=batch_size, epochs=epochs)\n\ntrain_loss = ordinal_regression_history.history[\"loss\"]\ntrain_mae = ordinal_regression_history.history[\"mean_absolute_error_labels\"]\nvalidation_loss = ordinal_regression_history.history[\"val_loss\"]\nvalidation_mae = ordinal_regression_history.history[\"val_mean_absolute_error_labels\"]\n\nfor i in range(len(train_loss)):\n    ordinal_regression_evaluation_results = ordinal_regression_evaluation_results.append({\n        \"loss\": train_loss[i],\n        \"mean_absolute_error_labels\": train_mae[i],\n        \"val_loss\": validation_loss[i],\n        \"val_mean_absolute_error_labels\": validation_mae[i]\n    }, ignore_index=True)\n\nordinal_regression_model.save(f\"DenseNet121_ordinal_regression_64-16_{epochs}-epochs.h5\")\nordinal_regression_evaluation_results.to_csv(f\"DenseNet121_ordinal_regression_64-16_{epochs}-epochs.csv\", index=False)\nto_latex(ordinal_regression_evaluation_results, f\"DenseNet121_ordinal_regression_64-16_{epochs}-epochs\")","metadata":{"execution":{"iopub.status.busy":"2021-06-13T22:13:14.521342Z","iopub.execute_input":"2021-06-13T22:13:14.521698Z","iopub.status.idle":"2021-06-13T22:32:01.462102Z","shell.execute_reply.started":"2021-06-13T22:13:14.521661Z","shell.execute_reply":"2021-06-13T22:32:01.461261Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ordinal_regression_evaluation_results = pd.read_csv(\"../input/tfg-pet-adoption-data/DenseNet121_ordinal_regression_64-16_5-epochs.csv\")\nplot_history(ordinal_regression_evaluation_results, [\"loss\", \"mean_absolute_error_labels\"], 1, 2, (10,3))","metadata":{"execution":{"iopub.status.busy":"2021-06-22T19:19:11.219927Z","iopub.execute_input":"2021-06-22T19:19:11.220188Z","iopub.status.idle":"2021-06-22T19:19:11.489992Z","shell.execute_reply.started":"2021-06-22T19:19:11.220162Z","shell.execute_reply":"2021-06-22T19:19:11.48913Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ordinal_regression_model = load_model(\n    \"../input/tfg-pet-adoption-data/DenseNet121_ordinal_regression_64-16_5-epochs.h5\",\n    custom_objects={\"CoralOrdinal\": coral.CoralOrdinal,\n                    \"OrdinalCrossEntropy\": coral.OrdinalCrossEntropy,\n                    \"MeanAbsoluteErrorLabels\": coral.MeanAbsoluteErrorLabels}\n)","metadata":{"execution":{"iopub.status.busy":"2021-06-23T07:57:04.536607Z","iopub.execute_input":"2021-06-23T07:57:04.536986Z","iopub.status.idle":"2021-06-23T07:57:09.153468Z","shell.execute_reply.started":"2021-06-23T07:57:04.536954Z","shell.execute_reply":"2021-06-23T07:57:09.152417Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = Model(inputs=ordinal_regression_model.input, outputs=ordinal_regression_model.layers[-3].output)\nife = ImageFeatureExtractor(construct_from_cnn_backbone=False, model=model,\n                 images_directory=\"../input/petfinder-adoption-prediction/train_images\",\n                 from_image=True, preprocess_input=preprocess_input_densenet, img_size=256,\n                 model_name=\"DenseNet121_ordinal_regression_layer-64_5-epochs\",\n                 save=True, debug=True, include_feats=False)\n\n_ = ife.fit_transform(X, y)","metadata":{"execution":{"iopub.status.busy":"2021-06-13T22:32:01.720888Z","iopub.execute_input":"2021-06-13T22:32:01.721134Z","iopub.status.idle":"2021-06-13T22:34:34.267639Z","shell.execute_reply.started":"2021-06-13T22:32:01.721109Z","shell.execute_reply":"2021-06-13T22:34:34.265713Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ordinal_regression_model_layer_64_features_5e = pd.read_csv(\n    \"../input/tfg-pet-adoption-data/image_features_DenseNet121_ordinal_regression_layer-64_5-epochs_in-256.csv\",\n    index_col=0)\ndisplay(ordinal_regression_model_layer_64_features_5e.head(5))\nordinal_regression_model_layer_64_features_5e.describe().loc[\"std\",:].value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-06-22T19:19:11.491231Z","iopub.execute_input":"2021-06-22T19:19:11.491499Z","iopub.status.idle":"2021-06-22T19:19:11.853373Z","shell.execute_reply.started":"2021-06-22T19:19:11.491471Z","shell.execute_reply":"2021-06-22T19:19:11.852332Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = Model(inputs=ordinal_regression_model.input, outputs=ordinal_regression_model.layers[-2].output)\nife = ImageFeatureExtractor(construct_from_cnn_backbone=False, model=model,\n                 images_directory=\"../input/petfinder-adoption-prediction/train_images\",\n                 from_image=True, preprocess_input=preprocess_input_densenet, img_size=256,\n                 model_name=\"DenseNet121_ordinal_regression_layer-16_5-epochs\",\n                 save=True, debug=True, include_feats=False)\n\n_ = ife.fit_transform(X, y)","metadata":{"execution":{"iopub.status.busy":"2021-06-13T22:34:34.540865Z","iopub.execute_input":"2021-06-13T22:34:34.541234Z","iopub.status.idle":"2021-06-13T22:37:06.819558Z","shell.execute_reply.started":"2021-06-13T22:34:34.541181Z","shell.execute_reply":"2021-06-13T22:37:06.817973Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ordinal_regression_model_layer_16_features_5e = pd.read_csv(\n    \"../input/tfg-pet-adoption-data/image_features_DenseNet121_ordinal_regression_layer-16_5-epochs_in-256.csv\",\n    index_col=0)\ndisplay(ordinal_regression_model_layer_16_features_5e.head(5))\nordinal_regression_model_layer_16_features_5e.describe().loc[\"std\",:].value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-06-22T19:19:11.854873Z","iopub.execute_input":"2021-06-22T19:19:11.855273Z","iopub.status.idle":"2021-06-22T19:19:11.992202Z","shell.execute_reply.started":"2021-06-22T19:19:11.855232Z","shell.execute_reply":"2021-06-22T19:19:11.991148Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's check how well XGBClassifier performs with the extracted image features from the three previous models (it can handle useless variables, that is, those with std = 0 or very close to 0, but we will create a transformer in order to get ride of those variables without predicting potencial):","metadata":{}},{"cell_type":"code","source":"class UselessVariablesRemover(BaseEstimator, TransformerMixin):\n    def __init__(self, tolerance):\n        self.tolerance = tolerance\n        self.columns_to_remove = []\n        \n    def fit(self, X, y):\n        X_description = X.describe()\n        self.columns_to_remove = X_description.columns[X_description.loc[\"std\"] < self.tolerance]\n        return self\n    \n    def transform(self, X, y=None):\n        X = X.copy()\n        return X.drop(self.columns_to_remove, axis=1)","metadata":{"execution":{"iopub.status.busy":"2021-06-29T07:26:31.693474Z","iopub.execute_input":"2021-06-29T07:26:31.69386Z","iopub.status.idle":"2021-06-29T07:26:31.701346Z","shell.execute_reply.started":"2021-06-29T07:26:31.693827Z","shell.execute_reply":"2021-06-29T07:26:31.699516Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pipeline_transformers = [\n    ('replace_breeds', LeftJoinReplace(values_dict=breeds_dict,\n                                      variables=[\"Breed1\", \"Breed2\"])),\n    ('replace_colors', LeftJoinReplace(values_dict=colors_dict,\n                                      variables=[\"Color1\", \"Color2\", \"Color3\"])),\n    ('replace_states', LeftJoinReplace(values_dict=states_dict,\n                                     variables=[\"State\"])),\n    ('replace_by_strings', FunctionTransformer(func=replace_integers_by_strings)),\n    ('has_name', FunctionTransformer(func=has_significant_name)),\n    ('pure_breed', FunctionTransformer(func=has_pure_breed)),\n    ('breed_matches_fur_length', FunctionTransformer(func=breed_matches_fur_length)),\n    ('impute_breed', BreedImputer()),\n    ('include_prof_im_metadata', IncludeProfileImageMetadata(profile_image_metadata)),\n    ('correct_wrong_type', CorrectWrongType(breeds)),\n    ('encode_breed', BreedEncoding(enc_type=\"target_and_frequency\")),\n    ('ordinal_vars_encoder', OrdinalVariableEncoder(columns=[\"MaturitySize\", \"FurLength\", \"Health\"],\n                                enc_type=\"ordinal\", mapping=ordinal_vars_mapping)),\n    ('state_gdp', ReplaceState(gdp_per_capita=gdp_per_capita, impute_nan_value=46450)),\n    ('rescuer_count', ReplaceRescuerID()),\n    ('discretizer', CustomDiscretizer(bins_age=-1, quantity=False, fee=False, video_amt=False,\n                                     photo_amt=False)),\n    ('description_length', FunctionTransformer(func=include_description_length)),\n    ('include_desc_metadata', IncludeDescriptionMetadata(description_metadata=description_metadata)),\n    ('correct_desc_language', CorrectDescriptionLanguage()),\n    ('one_hot_encoder', CustomOneHotEncoder(columns=[\"Gender\", \"Color1\", \"Color2\",\n                            \"Color3\", \"Vaccinated\", \"Dewormed\", \"Sterilized\", \"DescriptionLanguage\"])),\n    ('include_prof_im_properties', IncludeProfileImageProperties(profile_image_properties)),\n    ('drop_columns', ColumnRemover(columns=columns_to_be_removed_image_feats_eval)),\n    ('round_im_dims_aspect_ratio', FunctionTransformer(func=include_aspect_ratio)),\n    None,\n    ('drop_petid', ColumnRemover(columns=[\"PetID\"])),\n    ('useless_vars_remover', UselessVariablesRemover(tolerance=0.000001)),\n    ('impute_malay_desc_missing_prof_im_props', CustomIterativeImputer()),\n    ('custom_standard_scaler', CustomStandardScaler(numeric_columns_image_feats_eval))\n]\n\nevaluation_results = pd.DataFrame([], columns=[\"Model description\",\n                        \"Average fit time\", \"Average accuracy\", \"Average QWK\",\n                        \"Single split accuracy\", \"Single split QWK\"])\n\nimage_features_dataframes = {\n    \"AdoptionSpeed, classification_model_layer-64\": classification_model_layer_64_features_5e,\n    \"AdoptionSpeed, classification_model_layer-16\": classification_model_layer_16_features_5e,\n    \"AdoptionSpeed, regression_model_layer-64\": regression_model_layer_64_features_5e,\n    \"AdoptionSpeed, regression_model_layer-16\": regression_model_layer_16_features_5e,\n    \"AdoptionSpeed, ordinal_regression_model_layer-64\": ordinal_regression_model_layer_64_features_5e,\n    \"AdoptionSpeed, ordinal_regression_model_layer-16\": ordinal_regression_model_layer_16_features_5e\n}\n\nfor features_description, loaded_features in image_features_dataframes.items():\n    ife = ImageFeatureExtractor(construct_from_cnn_backbone=False,\n                                loaded_features=loaded_features)\n    pipeline_transformers[-5] = ('image_features_extractor', ife)\n\n    model = xgb.XGBClassifier(eval_metric='mlogloss', random_state=seed, n_jobs=-1,\n                          use_label_encoder=False)\n    model_description = f\"XGBClassifier with image features from DenseNet121, {features_description}\"\n    print(f\"\\n\\n*************** {model_description} ***************\")\n    avg_fit_time, avg_accuracy, avg_QWK = evaluate_model(\n                Pipeline(steps=pipeline_transformers + [('model', model)]),\n                cv, X, y, model_type=\"classification\", display_results=True,\n                display_plots=False)\n    \n    single_accuracy, single_QWK = evaluate_model_single_split(\n                Pipeline(steps=pipeline_transformers + [('model', model)]),\n                X_train_CNN, X_val_CNN, y_train_CNN, y_val_CNN,\n                model_type=\"classification\", display_results=True)\n\n    evaluation_results = evaluation_results.append({\"Model description\": model_description,\n                        \"Average fit time\": avg_fit_time, \"Average accuracy\": avg_accuracy,\n                        \"Average QWK\": avg_QWK, \"Single split accuracy\": single_accuracy,\n                        \"Single split QWK\": single_QWK}, ignore_index=True)\n\npd.set_option('display.max_colwidth', None)\ndisplay(evaluation_results)\npd.set_option('display.max_colwidth', 50)\nto_latex(evaluation_results, \"evaluation_results_XGBClassifier_DenseNet121_AdoptionSpeed_5-epochs\")","metadata":{"execution":{"iopub.status.busy":"2021-06-22T19:19:12.022957Z","iopub.execute_input":"2021-06-22T19:19:12.023379Z","iopub.status.idle":"2021-06-22T19:31:56.131111Z","shell.execute_reply.started":"2021-06-22T19:19:12.023321Z","shell.execute_reply":"2021-06-22T19:31:56.130285Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The best balance between accuracy and QWK in the single split validation is obtained with the regression model, while the ordinal regression performs significantly worse in this split. Moreover, this is not a matter of \"classification and ordinal regression may need more epochs than regression\", since, for example, the average QWK in 5-CV is better using features from both layers of the ordinal regression model than the regression model.","metadata":{}},{"cell_type":"markdown","source":"#### Training on predictor variables","metadata":{}},{"cell_type":"markdown","source":"First of all, we will create a class in order to include it in the pipeline and get the mutual information between the predictor variables and the target in each 5-CV iteration (we will create a dataframe with as many rows as the number of predictor variables and two columns: classification and regression, as we will consider the target as both using the functions `mutual_info_classif` and `mutual_info_regression`, respectively; then, in each iteration, we just sum up the mutual information with the target of each variable):","metadata":{}},{"cell_type":"code","source":"mutual_info_df_5cv = None\n\nclass MutualInfoPredictorVars(BaseEstimator, TransformerMixin):\n    def __init__(self, numeric_columns, seed):\n        self.numeric_columns = numeric_columns\n        self.seed = seed\n        self.df = mutual_info_df_5cv\n    \n    def fit(self, X, y):\n        self.numeric_columns += list(filter(\n            lambda x: \"Breed\" in str(x) or \"_ordinal\" in str(x) or \"img_\" in str(x) or \"desc_\" in str(x),\n            X.columns))\n        self.discrete_columns = list(filter(lambda x: x not in self.numeric_columns,\n                                            X.columns))\n        self.index_discrete_columns = [i for i, x in enumerate(X.columns)\n                                            if x in self.discrete_columns]\n        mutual_info_target_classif = mutual_info_classif(X, y,\n                            discrete_features=self.index_discrete_columns,\n                            random_state=self.seed)\n        \n        for i, column in enumerate(X.columns):\n            self.df.loc[column, \"classification\"] += mutual_info_target_classif[i]\n        \n        mutual_info_target_regression = mutual_info_regression(X, y,\n                            discrete_features=self.index_discrete_columns,\n                            random_state=self.seed)\n        \n        for i, column in enumerate(X.columns):\n            self.df.loc[column, \"regression\"] += mutual_info_target_regression[i]\n        \n        return self\n        \n    def transform(self, X, y=None):\n        X = X.copy()\n        return X","metadata":{"execution":{"iopub.status.busy":"2021-06-22T19:31:56.132336Z","iopub.execute_input":"2021-06-22T19:31:56.132737Z","iopub.status.idle":"2021-06-22T19:31:56.141392Z","shell.execute_reply.started":"2021-06-22T19:31:56.132707Z","shell.execute_reply":"2021-06-22T19:31:56.140534Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"columns_to_be_removed = [\"Name\", \"Breed1\", \"Breed2\", \"Gender\", \"Color1\", \"Color2\", \"Color3\",\n                         \"Vaccinated\", \"Dewormed\", \"Sterilized\", \"State\", \"RescuerID\", \"Description\",\n                         \"PetID\", \"MaturitySize\", \"FurLength\", \"Health\"]\n\nnumeric_columns = [\"Age\", \"Quantity\", \"Fee\", \"VideoAmt\", \"PhotoAmt\", \"StateGDP\", \"RescuerCount\"]\n\ncolumns_mutual_info = [\n    'Type', 'Age', 'MaturitySize_ordinal', 'FurLength_ordinal', 'Health_ordinal',\n    'Quantity', 'Fee', 'VideoAmt', 'PhotoAmt', 'HasName', 'PureBreed',\n    'Breed1_AdoptionSpeed_0', 'Breed1_AdoptionSpeed_2', 'Breed1_AdoptionSpeed_3',\n    'Breed1_AdoptionSpeed_1', 'Breed1_AdoptionSpeed_4', 'Breed1_freq_encode',\n    'Gender_Male', 'Gender_Female', 'Gender_Mixed', 'Color1_Black', 'Color1_Brown',\n    'Color1_Cream', 'Color1_Gray', 'Color1_Golden', 'Color1_White', 'Color1_Yellow',\n    'Color2_White', 'Color2_Brown', 'Color2_nan', 'Color2_Gray', 'Color2_Cream',\n    'Color2_Yellow', 'Color2_Golden', 'Color3_nan', 'Color3_White', 'Color3_Cream',\n    'Color3_Gray', 'Color3_Yellow', 'Color3_Golden', 'Vaccinated_No',\n    'Vaccinated_Not Sure', 'Vaccinated_Yes', 'Dewormed_No', 'Dewormed_Not Sure',\n    'Dewormed_Yes', 'Sterilized_No', 'Sterilized_Not Sure', 'Sterilized_Yes',\n    'StateGDP', 'RescuerCount'\n]\n\nmutual_info_df_5cv = pd.DataFrame(0, index=columns_mutual_info,\n                                  columns=[\"classification\", \"regression\"])\n\npipeline_transformers = [\n    ('replace_breeds', LeftJoinReplace(values_dict=breeds_dict,\n                                      variables=[\"Breed1\", \"Breed2\"])),\n    ('replace_colors', LeftJoinReplace(values_dict=colors_dict,\n                                      variables=[\"Color1\", \"Color2\", \"Color3\"])),\n    ('replace_states', LeftJoinReplace(values_dict=states_dict,\n                                     variables=[\"State\"])),\n    ('replace_by_strings', FunctionTransformer(func=replace_integers_by_strings)),\n    ('has_name', FunctionTransformer(func=has_significant_name)),\n    ('pure_breed', FunctionTransformer(func=has_pure_breed)),\n    ('breed_matches_fur_length', FunctionTransformer(func=breed_matches_fur_length)),\n    ('impute_breed', BreedImputer()),\n    ('encode_breed', BreedEncoding(enc_type=\"target_and_frequency\")),\n    ('one_hot_encoder', CustomOneHotEncoder(columns=[\"Gender\", \"Color1\", \"Color2\",\n                                \"Color3\", \"Vaccinated\", \"Dewormed\", \"Sterilized\"])),\n    ('ordinal_vars_encoder', OrdinalVariableEncoder(columns=[\"MaturitySize\", \"FurLength\", \"Health\"],\n                                enc_type=\"ordinal\", mapping=ordinal_vars_mapping)),\n    ('state_gdp', ReplaceState(gdp_per_capita=gdp_per_capita, impute_nan_value=46450)),\n    ('rescuer_count', ReplaceRescuerID()),\n    ('discretizer', CustomDiscretizer(bins_age=-1, quantity=False, fee=False, video_amt=False,\n                                     photo_amt=False)),\n    ('drop_columns', ColumnRemover(columns=columns_to_be_removed)),\n    ('custom_standard_scaler', CustomStandardScaler(numeric_columns)),\n    ('get_mutual_info', MutualInfoPredictorVars(numeric_columns,\n                                                seed))\n]\n\n\nmodel = xgb.XGBClassifier(eval_metric='mlogloss', random_state=seed, n_jobs=-1,\n                          use_label_encoder=False)\n_ = evaluate_model(Pipeline(steps=pipeline_transformers + [('model', model)]),\n               cv, X, y, model_type=\"classification\", display_results=False)\n\ndisplay(mutual_info_df_5cv)","metadata":{"execution":{"iopub.status.busy":"2021-06-22T19:31:56.142585Z","iopub.execute_input":"2021-06-22T19:31:56.143016Z","iopub.status.idle":"2021-06-22T19:32:55.712468Z","shell.execute_reply.started":"2021-06-22T19:31:56.142986Z","shell.execute_reply":"2021-06-22T19:32:55.711169Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"list(mutual_info_df_5cv.sort_values('classification', ascending=False).head(10).index)","metadata":{"execution":{"iopub.status.busy":"2021-06-22T19:32:55.713965Z","iopub.execute_input":"2021-06-22T19:32:55.714346Z","iopub.status.idle":"2021-06-22T19:32:55.72179Z","shell.execute_reply.started":"2021-06-22T19:32:55.714314Z","shell.execute_reply":"2021-06-22T19:32:55.720728Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"list(mutual_info_df_5cv.sort_values('regression', ascending=False).head(10).index)","metadata":{"execution":{"iopub.status.busy":"2021-06-22T19:32:55.723417Z","iopub.execute_input":"2021-06-22T19:32:55.723809Z","iopub.status.idle":"2021-06-22T19:32:55.738626Z","shell.execute_reply.started":"2021-06-22T19:32:55.723768Z","shell.execute_reply":"2021-06-22T19:32:55.737448Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we can see, the predictor variables that, by themselves (each one), have the greatest predicting power considering the target categorical or continuous are: RescuerCount, Age and Breed1. Thus, we will create a dataframe with the filename of each image (all training images, not just profile images) and the values of the previous variables (including the logarithmic transformation of RescuerCount and Age, and Breed1 will be one-hot encoded).","metadata":{}},{"cell_type":"code","source":"def get_dataframe_CNN_training_predictors(directory, train_X_df, train_y_df, columns_before,\n                               columns_after, transformations, drop_after):  \n    train_X_df = train_X_df[[\"PetID\"] + columns_before].copy()\n    add_y = False\n    for column in columns_before + columns_after:\n        if column == train_y_df.name and column in transformations:\n            for transformation in transformations[column]:\n                train_y_df = transformation.fit_transform(train_y_df)\n            add_y = True\n        if column in transformations:\n            for transformation in transformations[column]:\n                train_X_df = transformation.fit_transform(train_X_df, train_y_df)\n            \n    train_df = train_X_df\n    train_df.drop(drop_after, axis=1, inplace=True)\n    if add_y:\n        if isinstance(train_y_df, pd.DataFrame):\n            train_df[train_y_df.columns] = train_y_df\n        else:\n            train_df[train_y_df.name] = train_y_df\n    \n    train_df.set_index(\"PetID\", inplace=True)\n    \n    filenames_and_target_values = {}\n    for pet_id in tqdm(train_df.index):\n        values_to_include = dict(train_df.loc[pet_id, :])\n        for image_path in glob.iglob(f\"{directory}/{pet_id}*.jpg\"):\n            filename = image_path[image_path.rindex('/')+1:]\n            filenames_and_target_values[filename] = values_to_include\n    \n    filenames_and_target_values_df = pd.DataFrame.from_dict(filenames_and_target_values, orient=\"index\")\n    filenames_and_target_values_df = filenames_and_target_values_df.reset_index()\n    filenames_and_target_values_df.rename(columns={'index': 'filename'}, inplace=True)\n    filenames_and_target_values_df.to_csv(\"dataframe_CNN_training_predictors.csv\", index=False)\n    \n    return filenames_and_target_values_df","metadata":{"execution":{"iopub.status.busy":"2021-06-16T09:35:42.708244Z","iopub.execute_input":"2021-06-16T09:35:42.708601Z","iopub.status.idle":"2021-06-16T09:35:42.720961Z","shell.execute_reply.started":"2021-06-16T09:35:42.708564Z","shell.execute_reply":"2021-06-16T09:35:42.719978Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Necessary to use BreedImputer below\ndef replace_type_integers(X):\n    X = X.copy()\n    replace_dict = {\n        'Type': {1: 'Dog', 2: 'Cat'}\n    }\n    utils.replace_val_categorical(X, replace_dict)\n    return X","metadata":{"execution":{"iopub.status.busy":"2021-06-09T12:57:19.820345Z","iopub.execute_input":"2021-06-09T12:57:19.820706Z","iopub.status.idle":"2021-06-09T12:57:19.825744Z","shell.execute_reply.started":"2021-06-09T12:57:19.820675Z","shell.execute_reply":"2021-06-09T12:57:19.82462Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# One-hot encode Breed1 after ce.OrdinalEncoder (starts from 1)\n# and save it in one column for multioutput training\ndef one_hot_encoder_breed1(X):\n    X = X.copy()\n    X['Breed1'] = X['Breed1'] - 1\n    X[\"Breed1_onehot\"] = to_categorical(X['Breed1'].values).tolist()\n    return X","metadata":{"execution":{"iopub.status.busy":"2021-06-09T12:57:20.383166Z","iopub.execute_input":"2021-06-09T12:57:20.383532Z","iopub.status.idle":"2021-06-09T12:57:20.389904Z","shell.execute_reply.started":"2021-06-09T12:57:20.383502Z","shell.execute_reply":"2021-06-09T12:57:20.388566Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Transformer to include logarithmic transformation of RescuerCount and Age\n#(we will train on the raw data, and also the transformed ones):\nclass LogTransform(BaseEstimator, TransformerMixin):\n    def __init__(self, columns):\n        self.columns = columns\n    \n    def fit(self, X, y):\n        return self\n    \n    def transform(self, X, y=None):\n        X = X.copy()\n        for column in self.columns:\n            X[f\"{column}_log\"] = np.log(1 + X[column])\n        return X","metadata":{"execution":{"iopub.status.busy":"2021-06-09T12:57:21.209184Z","iopub.execute_input":"2021-06-09T12:57:21.2097Z","iopub.status.idle":"2021-06-09T12:57:21.216731Z","shell.execute_reply.started":"2021-06-09T12:57:21.209653Z","shell.execute_reply":"2021-06-09T12:57:21.215419Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ordinal_encoder = ce.OrdinalEncoder(cols=[\"Breed1\"])\n\n# filenames_and_targets_predictors = get_dataframe_CNN_training_predictors(\n#     directory=\"../input/tfg-pet-adoption-resized-train-images\",\n#     train_X_df=X,\n#     train_y_df=y,\n#     columns_before=[\"Age\", \"Type\", \"RescuerID\", \"Breed1\", \"Breed2\"],\n#     columns_after=[\"RescuerCount\"],\n#     transformations={\n#         \"Type\": [FunctionTransformer(func=replace_type_integers)],\n#         \"RescuerID\": [ReplaceRescuerID()],\n#         \"Breed1\": [LeftJoinReplace(values_dict=breeds_dict,\n#                                    variables=[\"Breed1\", \"Breed2\"]),\n#                    BreedImputer(),\n#                    ordinal_encoder,\n#                    FunctionTransformer(func=one_hot_encoder_breed1)],\n#         \"Age\": [LogTransform([\"Age\"])],\n#         \"RescuerCount\": [LogTransform([\"RescuerCount\"])]\n#     },\n#     drop_after=[\"Type\", \"RescuerID\", \"Breed1\", \"Breed2\"]\n# )","metadata":{"execution":{"iopub.status.busy":"2021-06-09T12:57:21.645319Z","iopub.execute_input":"2021-06-09T12:57:21.645711Z","iopub.status.idle":"2021-06-09T13:21:24.017752Z","shell.execute_reply.started":"2021-06-09T12:57:21.645674Z","shell.execute_reply":"2021-06-09T13:21:24.01675Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"filenames_and_targets_predictors = pd.read_csv(\"../input/tfg-pet-adoption-data/dataframe_CNN_training_predictors.csv\")\nfilenames_and_targets_predictors[\"Breed1_onehot\"] = \\\n            filenames_and_targets_predictors[\"Breed1_onehot\"].apply(json.loads)\nfilenames_and_targets_predictors[\"PetID\"] = filenames_and_targets_predictors[\"filename\"].apply(lambda x: str(x)[:str(x).rindex('-')])\nfilenames_and_targets_predictors","metadata":{"execution":{"iopub.status.busy":"2021-06-22T19:32:55.739893Z","iopub.execute_input":"2021-06-22T19:32:55.740297Z","iopub.status.idle":"2021-06-22T19:32:58.612367Z","shell.execute_reply.started":"2021-06-22T19:32:55.740263Z","shell.execute_reply":"2021-06-22T19:32:58.611474Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"filenames_and_targets_predictors.info()","metadata":{"execution":{"iopub.status.busy":"2021-06-22T19:34:01.487606Z","iopub.execute_input":"2021-06-22T19:34:01.488558Z","iopub.status.idle":"2021-06-22T19:34:01.515275Z","shell.execute_reply.started":"2021-06-22T19:34:01.488493Z","shell.execute_reply":"2021-06-22T19:34:01.514217Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see that all the values of the column Breed1_onehot have a length of 175, the number of observed different breeds in the training dataset:","metadata":{}},{"cell_type":"code","source":"filenames_and_targets_predictors[\"Breed1_onehot\"].apply(len).value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-06-22T19:34:40.03115Z","iopub.execute_input":"2021-06-22T19:34:40.031596Z","iopub.status.idle":"2021-06-22T19:34:40.064239Z","shell.execute_reply.started":"2021-06-22T19:34:40.031543Z","shell.execute_reply":"2021-06-22T19:34:40.063069Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The following is the correspondence between each ordinal encoded value (position `value` - 1 in the list) and the Breed (see Version 48):","metadata":{}},{"cell_type":"code","source":"# ordinal_encoder.mapping[0][\"mapping\"]","metadata":{"execution":{"iopub.status.busy":"2021-06-09T16:50:36.251875Z","iopub.execute_input":"2021-06-09T16:50:36.25445Z","iopub.status.idle":"2021-06-09T16:50:36.258805Z","shell.execute_reply.started":"2021-06-09T16:50:36.254404Z","shell.execute_reply":"2021-06-09T16:50:36.257825Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now, let's create the training and validation split, the same as before. As the order in which the filenames appear in the dataframe with the AdoptionSpeed values is not the same one as the order of this dataframe, we will look into the `filename` values of the training and validation sets of that dataframe:","metadata":{}},{"cell_type":"code","source":"filenames_and_targets_predictors_train_no_instance_overlap = filenames_and_targets_predictors.loc[\n    filenames_and_targets_predictors[\"PetID\"].isin(set_pet_ids_train)\n].copy()\nfilenames_and_targets_predictors_val_no_instance_overlap = filenames_and_targets_predictors.loc[\n    ~filenames_and_targets_predictors[\"PetID\"].isin(set_pet_ids_train)\n].copy()","metadata":{"execution":{"iopub.status.busy":"2021-06-22T19:34:47.958887Z","iopub.execute_input":"2021-06-22T19:34:47.959314Z","iopub.status.idle":"2021-06-22T19:34:48.007418Z","shell.execute_reply.started":"2021-06-22T19:34:47.959268Z","shell.execute_reply":"2021-06-22T19:34:48.006155Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(sorted(filenames_and_target_values_train_no_instance_overlap[\"filename\"].values) == sorted(filenames_and_targets_predictors_train_no_instance_overlap[\"filename\"].values))\nprint(sorted(filenames_and_target_values_val_no_instance_overlap[\"filename\"].values) == sorted(filenames_and_targets_predictors_val_no_instance_overlap[\"filename\"].values))\nprint(set(filenames_and_targets_predictors_train_no_instance_overlap[\"filename\"].values) & set(filenames_and_targets_predictors_val_no_instance_overlap[\"filename\"].values))\nprint(set(filenames_and_targets_predictors_train_no_instance_overlap[\"PetID\"].values) & set(filenames_and_targets_predictors_val_no_instance_overlap[\"PetID\"].values))","metadata":{"execution":{"iopub.status.busy":"2021-06-22T19:34:48.513873Z","iopub.execute_input":"2021-06-22T19:34:48.514296Z","iopub.status.idle":"2021-06-22T19:34:48.591512Z","shell.execute_reply.started":"2021-06-22T19:34:48.514259Z","shell.execute_reply":"2021-06-22T19:34:48.590484Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**https://keras.io/api/models/model_training_apis/#compile-method --> The loss value that will be minimized by the model will then be the weighted sum of all individual losses, weighted by the loss_weights coefficients.**\n\nAs we have multiple outputs of different type, we will define different loss functions; each one will be weighted according to the mean mutual information value (classification and regression), and then the weights of the FC network will be updated on backpropagation according to that weighted sum.","metadata":{}},{"cell_type":"code","source":"weight_rescuercount = mutual_info_df_5cv.loc[\"RescuerCount\", :].mean()\nweight_age = mutual_info_df_5cv.loc[\"Age\", :].mean()\nbreed1_rows = list(filter(lambda x: \"Breed1_AdoptionSpeed_\" in x, mutual_info_df_5cv.index))\nweight_breed1 = mutual_info_df_5cv.loc[breed1_rows, :].mean().mean()\n\nloss_weights = np.array([weight_rescuercount, weight_age, weight_breed1])\n# Normalize so that they sum 1.0\nloss_weights /= loss_weights.sum()\ndisplay(loss_weights)","metadata":{"execution":{"iopub.status.busy":"2021-06-22T19:34:52.025262Z","iopub.execute_input":"2021-06-22T19:34:52.025826Z","iopub.status.idle":"2021-06-22T19:34:52.036335Z","shell.execute_reply.started":"2021-06-22T19:34:52.02579Z","shell.execute_reply":"2021-06-22T19:34:52.035119Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now, let's create the multioutput model and compile it using the appropiate loss functions and the weight of each one, that we have just computed:","metadata":{}},{"cell_type":"code","source":"inp = Input((256,256,3))\nbackbone = DenseNet121(input_tensor=inp, include_top=False, weights=\"imagenet\")\nx = backbone.output\nx = GlobalAveragePooling2D()(x)\nx = Dense(64, activation=\"relu\")(x)\nx = Dense(16, activation=\"relu\")(x)\nout1 = Dense(1, activation=\"linear\", name=\"out_rescuer\")(x) #RescuerCount (either same or log)\nout2 = Dense(1, activation=\"linear\", name=\"out_age\")(x) #Age (either same or log)\nout3 = Dense(175, activation=\"softmax\", name=\"out_breed\")(x) #Breed1\n\n# Freezing DenseNet121 backbone\nfor layer in backbone.layers:\n    layer.trainable = False\n\nmultioutput_model = Model(inputs=inp, outputs=[out1,out2,out3])\n\noptimizer = Adam(lr=0.001)\nmetrics = {\"out_breed\": \"accuracy\"}\nloss = {\"out_rescuer\": \"mean_squared_error\",\n        \"out_age\": \"mean_squared_error\",\n        \"out_breed\": \"categorical_crossentropy\"}\nloss_weights_dict = {\"out_rescuer\": loss_weights[0],\n                \"out_age\": loss_weights[1],\n                \"out_breed\": loss_weights[2]}\n\nmultioutput_model.compile(loss=loss, loss_weights=loss_weights_dict, metrics=metrics, optimizer=optimizer)","metadata":{"execution":{"iopub.status.busy":"2021-06-15T22:45:35.30364Z","iopub.execute_input":"2021-06-15T22:45:35.304099Z","iopub.status.idle":"2021-06-15T22:45:37.633985Z","shell.execute_reply.started":"2021-06-15T22:45:35.304065Z","shell.execute_reply":"2021-06-15T22:45:37.633069Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The following are the last layers of our multioutput model:","metadata":{}},{"cell_type":"code","source":"# from keras.utils import plot_model\n# plot_model(multioutput_model, show_shapes=True)","metadata":{"execution":{"iopub.status.busy":"2021-06-15T21:38:45.672977Z","iopub.execute_input":"2021-06-15T21:38:45.673305Z","iopub.status.idle":"2021-06-15T21:38:45.676752Z","shell.execute_reply.started":"2021-06-15T21:38:45.673275Z","shell.execute_reply":"2021-06-15T21:38:45.675668Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Image(\"../input/tfg-pet-adoption-data/model_zoom_FC.png\")","metadata":{"execution":{"iopub.status.busy":"2021-06-22T19:34:57.177096Z","iopub.execute_input":"2021-06-22T19:34:57.177524Z","iopub.status.idle":"2021-06-22T19:34:57.201536Z","shell.execute_reply.started":"2021-06-22T19:34:57.177489Z","shell.execute_reply":"2021-06-22T19:34:57.200763Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As in previous training processes, we have to define the ImageDataGenerator instances for training and validation (the first one with data augmentation, for now just horizontal flipping of images) and the corresponding iterators:","metadata":{}},{"cell_type":"code","source":"multioutput_train_generator = ImageDataGenerator(\n    horizontal_flip=True,\n    preprocessing_function=preprocess_input_densenet\n)\n\nmultioutput_val_generator = ImageDataGenerator(\n    preprocessing_function=preprocess_input_densenet\n)","metadata":{"execution":{"iopub.status.busy":"2021-06-15T21:39:00.932788Z","iopub.execute_input":"2021-06-15T21:39:00.933134Z","iopub.status.idle":"2021-06-15T21:39:00.937677Z","shell.execute_reply.started":"2021-06-15T21:39:00.933104Z","shell.execute_reply":"2021-06-15T21:39:00.936756Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"batch_size = 32\ntarget_size = (256,256)\n\nmultioutput_train_iterator = multioutput_train_generator.flow_from_dataframe(\n    dataframe=filenames_and_targets_predictors_train_no_instance_overlap,\n    directory=\"../input/tfg-pet-adoption-resized-train-images/\",\n    x_col=\"filename\",\n    y_col=[\"RescuerCount\", \"Age\", \"Breed1_onehot\"],\n    target_size=target_size,\n    class_mode=\"multi_output\",\n    batch_size=batch_size,\n    shuffle=True,\n    seed=seed\n)\n\nmultioutput_val_iterator = multioutput_val_generator.flow_from_dataframe(\n    dataframe=filenames_and_targets_predictors_val_no_instance_overlap,\n    directory=\"../input/tfg-pet-adoption-resized-train-images/\",\n    x_col=\"filename\",\n    y_col=[\"RescuerCount\", \"Age\", \"Breed1_onehot\"],\n    target_size=target_size,\n    class_mode=\"multi_output\",\n    batch_size=batch_size,\n    shuffle=False,\n    seed=seed\n)","metadata":{"execution":{"iopub.status.busy":"2021-06-15T21:39:02.450766Z","iopub.execute_input":"2021-06-15T21:39:02.451114Z","iopub.status.idle":"2021-06-15T21:40:16.143681Z","shell.execute_reply.started":"2021-06-15T21:39:02.451082Z","shell.execute_reply":"2021-06-15T21:40:16.142053Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**?? https://stackoverflow.com/questions/50571641/compilation-options-of-a-multi-output-model-multiple-losses-loss-weighting, https://github.com/keras-team/keras/issues/10306**","metadata":{}},{"cell_type":"markdown","source":"**The training below was done using the scaled versions of RescuerCount and Age (x-mean/std):**","metadata":{}},{"cell_type":"code","source":"epochs = 30\nbatch_size = 32\n\nmultioutput_evaluation_results = pd.DataFrame([], columns=[\"loss\",\n                        \"out_rescuer_loss\", \"out_age_loss\", \"out_breed_loss\",\n                        \"out_breed_accuracy\", \"val_loss\", \"val_out_rescuer_loss\",\n                        \"val_out_age_loss\", \"val_out_breed_loss\",\n                        \"val_out_breed_accuracy\"])\n\nearly_stopping = EarlyStopping(monitor='val_loss', restore_best_weights=True, patience=5)\ncheckpoint_callback = ModelCheckpoint(\n    \"DenseNet121_multioutput_64-16_{epoch:02d}-epoch_val_loss-{val_loss:02f}.h5\",\n    monitor='val_loss', save_best_only=True)\n\nmultioutput_history = multioutput_model.fit(\n                    x=multioutput_train_iterator,\n                    validation_data=multioutput_val_iterator,\n                    batch_size=batch_size, epochs=epochs,\n                    callbacks=[checkpoint_callback, early_stopping])\n\ntrain_loss = multioutput_history.history[\"loss\"]\ntrain_rescuer_loss = multioutput_history.history[\"out_rescuer_loss\"]\ntrain_age_loss = multioutput_history.history[\"out_age_loss\"]\ntrain_breed_loss = multioutput_history.history[\"out_breed_loss\"]\ntrain_breed_accuracy = multioutput_history.history[\"out_breed_accuracy\"]\nvalidation_loss = multioutput_history.history[\"val_loss\"]\nvalidation_rescuer_loss = multioutput_history.history[\"val_out_rescuer_loss\"]\nvalidation_age_loss = multioutput_history.history[\"val_out_age_loss\"]\nvalidation_breed_loss = multioutput_history.history[\"val_out_breed_loss\"]\nvalidation_breed_accuracy = multioutput_history.history[\"val_out_breed_accuracy\"]\n\nfor i in range(len(train_loss)):\n    multioutput_evaluation_results = multioutput_evaluation_results.append({\n        \"loss\": train_loss[i],\n        \"out_rescuer_loss\": train_rescuer_loss[i],\n        \"out_age_loss\": train_age_loss[i],\n        \"out_breed_loss\": train_breed_loss[i],\n        \"out_breed_accuracy\": train_breed_accuracy[i],\n        \"val_loss\": validation_loss[i],\n        \"val_out_rescuer_loss\": validation_rescuer_loss[i],\n        \"val_out_age_loss\": validation_age_loss[i],\n        \"val_out_breed_loss\": validation_breed_loss[i],\n        \"val_out_breed_accuracy\": validation_breed_accuracy[i]\n    }, ignore_index=True)\n\n# multioutput_model.save(f\"DenseNet121_multioutput_64-16_{epochs}-epochs.h5\")\nmultioutput_evaluation_results.to_csv(f\"DenseNet121_multioutput__scaled__64-16_{epochs}-epochs.csv\", index=False)\nto_latex(multioutput_evaluation_results, f\"DenseNet121_multioutput__scaled__64-16_{epochs}-epochs\")\n\n# Zipping the checkpoints\n!mkdir scaled_no_log\n!mv ./DenseNet121_multioutput_64*.h5 scaled_no_log\nshutil.make_archive(\"DenseNet121_multioutput_scaled_no_log_checkpoints\", \"zip\", \"./scaled_no_log\")","metadata":{"execution":{"iopub.status.busy":"2021-06-15T21:45:03.427675Z","iopub.execute_input":"2021-06-15T21:45:03.428059Z","iopub.status.idle":"2021-06-15T22:30:09.726971Z","shell.execute_reply.started":"2021-06-15T21:45:03.428022Z","shell.execute_reply":"2021-06-15T22:30:09.726101Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"multioutput_evaluation_results = pd.read_csv(\"../input/tfg-pet-adoption-data/DenseNet121_multioutput__scaled__64-16_30-epochs.csv\")\nplot_history(multioutput_evaluation_results,\n    [\"loss\", \"out_rescuer_loss\", \"out_age_loss\", \"out_breed_loss\",\n     \"out_breed_accuracy\"],\n    nrows=3, ncols=2, figsize=(10,16))","metadata":{"execution":{"iopub.status.busy":"2021-06-22T19:35:05.727523Z","iopub.execute_input":"2021-06-22T19:35:05.728125Z","iopub.status.idle":"2021-06-22T19:35:06.506674Z","shell.execute_reply.started":"2021-06-22T19:35:05.728077Z","shell.execute_reply":"2021-06-22T19:35:06.50537Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Beyond epoch 8 the validation loss does not improve, so let's load the model after that epoch:","metadata":{}},{"cell_type":"code","source":"multioutput_model_scaled = load_model(\n    \"../input/tfg-pet-adoption-data/DenseNet121_multioutput__scaled__64-16_08-epoch_val_loss-0.919037.h5\")","metadata":{"execution":{"iopub.status.busy":"2021-06-16T09:35:46.537966Z","iopub.execute_input":"2021-06-16T09:35:46.538282Z","iopub.status.idle":"2021-06-16T09:35:53.736259Z","shell.execute_reply.started":"2021-06-16T09:35:46.538248Z","shell.execute_reply":"2021-06-16T09:35:53.735451Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"multioutput_model_scaled.layers[-10:]","metadata":{"execution":{"iopub.status.busy":"2021-06-16T09:35:53.73747Z","iopub.execute_input":"2021-06-16T09:35:53.737819Z","iopub.status.idle":"2021-06-16T09:35:53.743798Z","shell.execute_reply.started":"2021-06-16T09:35:53.737781Z","shell.execute_reply":"2021-06-16T09:35:53.742993Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = Model(inputs=multioutput_model_scaled.input, outputs=multioutput_model_scaled.layers[-5].output)\nife = ImageFeatureExtractor(construct_from_cnn_backbone=False, model=model,\n                 images_directory=\"../input/petfinder-adoption-prediction/train_images\",\n                 from_image=True, preprocess_input=preprocess_input_densenet, img_size=256,\n                 model_name=f\"DenseNet121_multioutput__scaled__layer-64_8-epochs\",\n                 save=True, debug=True, include_feats=False)\n\n_ = ife.fit_transform(X, y)","metadata":{"execution":{"iopub.status.busy":"2021-06-15T22:35:02.767987Z","iopub.execute_input":"2021-06-15T22:35:02.768332Z","iopub.status.idle":"2021-06-15T22:39:24.987235Z","shell.execute_reply.started":"2021-06-15T22:35:02.768304Z","shell.execute_reply":"2021-06-15T22:39:24.98632Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"multioutput_model_scaled_layer_64_features_8e = pd.read_csv(\n    \"../input/tfg-pet-adoption-data/image_features_DenseNet121_multioutput__scaled__layer-64_8-epochs_in-256.csv\",\n    index_col=0)\ndisplay(multioutput_model_scaled_layer_64_features_8e.head(5))\nmultioutput_model_scaled_layer_64_features_8e.describe().loc[\"std\",:].value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-06-22T19:35:14.939644Z","iopub.execute_input":"2021-06-22T19:35:14.940059Z","iopub.status.idle":"2021-06-22T19:35:15.351832Z","shell.execute_reply.started":"2021-06-22T19:35:14.940002Z","shell.execute_reply":"2021-06-22T19:35:15.350864Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = Model(inputs=multioutput_model_scaled.input, outputs=multioutput_model_scaled.layers[-4].output)\nife = ImageFeatureExtractor(construct_from_cnn_backbone=False, model=model,\n                 images_directory=\"../input/petfinder-adoption-prediction/train_images\",\n                 from_image=True, preprocess_input=preprocess_input_densenet, img_size=256,\n                 model_name=f\"DenseNet121_multioutput__scaled__layer-16_8-epochs\",\n                 save=True, debug=True, include_feats=False)\n\n_ = ife.fit_transform(X, y)","metadata":{"execution":{"iopub.status.busy":"2021-06-15T22:39:24.98899Z","iopub.execute_input":"2021-06-15T22:39:24.989369Z","iopub.status.idle":"2021-06-15T22:41:54.691893Z","shell.execute_reply.started":"2021-06-15T22:39:24.989329Z","shell.execute_reply":"2021-06-15T22:41:54.690363Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"multioutput_model_scaled_layer_16_features_8e = pd.read_csv(\n    \"../input/tfg-pet-adoption-data/image_features_DenseNet121_multioutput__scaled__layer-16_8-epochs_in-256.csv\",\n    index_col=0)\ndisplay(multioutput_model_scaled_layer_16_features_8e.head(5))\nmultioutput_model_scaled_layer_16_features_8e.describe().loc[\"std\",:].value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-06-22T19:35:16.984233Z","iopub.execute_input":"2021-06-22T19:35:16.984647Z","iopub.status.idle":"2021-06-22T19:35:17.189269Z","shell.execute_reply.started":"2021-06-22T19:35:16.984607Z","shell.execute_reply":"2021-06-22T19:35:17.188261Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"batch_size = 32\ntarget_size = (256,256)\n\nmultioutput_log_train_iterator = multioutput_train_generator.flow_from_dataframe(\n    dataframe=filenames_and_targets_predictors_train_no_instance_overlap,\n    directory=\"../input/tfg-pet-adoption-resized-train-images/\",\n    x_col=\"filename\",\n    y_col=[\"RescuerCount_log\", \"Age_log\", \"Breed1_onehot\"],\n    target_size=target_size,\n    class_mode=\"multi_output\",\n    batch_size=batch_size,\n    shuffle=True,\n    seed=seed\n)\n\nmultioutput_log_val_iterator = multioutput_val_generator.flow_from_dataframe(\n    dataframe=filenames_and_targets_predictors_val_no_instance_overlap,\n    directory=\"../input/tfg-pet-adoption-resized-train-images/\",\n    x_col=\"filename\",\n    y_col=[\"RescuerCount_log\", \"Age_log\", \"Breed1_onehot\"],\n    target_size=target_size,\n    class_mode=\"multi_output\",\n    batch_size=batch_size,\n    shuffle=False,\n    seed=seed\n)","metadata":{"execution":{"iopub.status.busy":"2021-06-15T22:46:13.918924Z","iopub.execute_input":"2021-06-15T22:46:13.919368Z","iopub.status.idle":"2021-06-15T22:46:35.775687Z","shell.execute_reply.started":"2021-06-15T22:46:13.919333Z","shell.execute_reply":"2021-06-15T22:46:35.774976Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**The training below was done using the the logarithmic transformation of RescuerCount and Age (log(1+x)), without scaling:**","metadata":{}},{"cell_type":"code","source":"epochs = 30\nbatch_size = 32\n\nearly_stopping = EarlyStopping(monitor='val_loss', restore_best_weights=True, patience=5)\ncheckpoint_callback = ModelCheckpoint(\n    \"DenseNet121_multioutput__log__64-16_{epoch:02d}-epoch_val_loss-{val_loss:02f}.h5\",\n    monitor='val_loss', save_best_only=True)\n\nmultioutput_evaluation_results = pd.DataFrame([], columns=[\"loss\",\n                        \"out_rescuer_loss\", \"out_age_loss\", \"out_breed_loss\",\n                        \"out_breed_accuracy\", \"val_loss\", \"val_out_rescuer_loss\",\n                        \"val_out_age_loss\", \"val_out_breed_loss\",\n                        \"val_out_breed_accuracy\"])\n    \nmultioutput_history = multioutput_model.fit(\n                    x=multioutput_log_train_iterator,\n                    validation_data=multioutput_log_val_iterator,\n                    batch_size=batch_size, epochs=epochs,\n                    callbacks=[checkpoint_callback, early_stopping])\n\ntrain_loss = multioutput_history.history[\"loss\"]\ntrain_rescuer_loss = multioutput_history.history[\"out_rescuer_loss\"]\ntrain_age_loss = multioutput_history.history[\"out_age_loss\"]\ntrain_breed_loss = multioutput_history.history[\"out_breed_loss\"]\ntrain_breed_accuracy = multioutput_history.history[\"out_breed_accuracy\"]\nvalidation_loss = multioutput_history.history[\"val_loss\"]\nvalidation_rescuer_loss = multioutput_history.history[\"val_out_rescuer_loss\"]\nvalidation_age_loss = multioutput_history.history[\"val_out_age_loss\"]\nvalidation_breed_loss = multioutput_history.history[\"val_out_breed_loss\"]\nvalidation_breed_accuracy = multioutput_history.history[\"val_out_breed_accuracy\"]\n\nfor i in range(len(train_loss)):\n    multioutput_evaluation_results = multioutput_evaluation_results.append({\n        \"loss\": train_loss[i],\n        \"out_rescuer_loss\": train_rescuer_loss[i],\n        \"out_age_loss\": train_age_loss[i],\n        \"out_breed_loss\": train_breed_loss[i],\n        \"out_breed_accuracy\": train_breed_accuracy[i],\n        \"val_loss\": validation_loss[i],\n        \"val_out_rescuer_loss\": validation_rescuer_loss[i],\n        \"val_out_age_loss\": validation_age_loss[i],\n        \"val_out_breed_loss\": validation_breed_loss[i],\n        \"val_out_breed_accuracy\": validation_breed_accuracy[i]\n    }, ignore_index=True)\n\n# multioutput_model.save(f\"DenseNet121_multioutput__log__64-16_{epochs}-epochs.h5\")\nmultioutput_evaluation_results.to_csv(f\"DenseNet121_multioutput__log__64-16_{epochs}-epochs.csv\", index=False)\nto_latex(multioutput_evaluation_results, f\"DenseNet121_multioutput__log__64-16_{epochs}-epochs\")\n\n# Zipping the checkpoints\n!mkdir log_no_scaled\n!mv ./DenseNet121_multioutput__log__*.h5 log_no_scaled\nshutil.make_archive(\"DenseNet121_multioutput_log_no_scaled_checkpoints\", \"zip\", \"./log_no_scaled\")","metadata":{"execution":{"iopub.status.busy":"2021-06-15T22:47:14.867023Z","iopub.execute_input":"2021-06-15T22:47:14.86734Z","iopub.status.idle":"2021-06-15T23:30:17.409871Z","shell.execute_reply.started":"2021-06-15T22:47:14.867313Z","shell.execute_reply":"2021-06-15T23:30:17.40896Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"multioutput_evaluation_results = pd.read_csv(\"../input/tfg-pet-adoption-data/DenseNet121_multioutput__log__64-16_30-epochs.csv\")\nplot_history(multioutput_evaluation_results,\n    [\"loss\", \"out_rescuer_loss\", \"out_age_loss\", \"out_breed_loss\",\n     \"out_breed_accuracy\"],\n    nrows=3, ncols=2, figsize=(10,16))","metadata":{"execution":{"iopub.status.busy":"2021-06-22T19:35:22.591578Z","iopub.execute_input":"2021-06-22T19:35:22.591993Z","iopub.status.idle":"2021-06-22T19:35:24.00133Z","shell.execute_reply.started":"2021-06-22T19:35:22.591947Z","shell.execute_reply":"2021-06-22T19:35:23.999866Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In this case, we will load the model after the 10th epoch as beyond that the validation loss does not improve:","metadata":{}},{"cell_type":"code","source":"multioutput_model_log = load_model(\n    \"../input/tfg-pet-adoption-data/DenseNet121_multioutput__log__64-16_08-epoch_val_loss-1.582031.h5\")","metadata":{"execution":{"iopub.status.busy":"2021-06-16T09:37:31.255046Z","iopub.execute_input":"2021-06-16T09:37:31.255407Z","iopub.status.idle":"2021-06-16T09:37:34.708983Z","shell.execute_reply.started":"2021-06-16T09:37:31.25536Z","shell.execute_reply":"2021-06-16T09:37:34.708206Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = Model(inputs=multioutput_model_log.input, outputs=multioutput_model_log.layers[-5].output)\nife = ImageFeatureExtractor(construct_from_cnn_backbone=False, model=model,\n                 images_directory=\"../input/petfinder-adoption-prediction/train_images\",\n                 from_image=True, preprocess_input=preprocess_input_densenet, img_size=256,\n                 model_name=f\"DenseNet121_multioutput__log__layer-64_8-epochs\",\n                 save=True, debug=True, include_feats=False)\n\n_ = ife.fit_transform(X, y)","metadata":{"execution":{"iopub.status.busy":"2021-06-15T23:34:18.52442Z","iopub.execute_input":"2021-06-15T23:34:18.524752Z","iopub.status.idle":"2021-06-15T23:36:46.943113Z","shell.execute_reply.started":"2021-06-15T23:34:18.524721Z","shell.execute_reply":"2021-06-15T23:36:46.94232Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"multioutput_model_log_layer_64_features_8e = pd.read_csv(\n    \"../input/tfg-pet-adoption-data/image_features_DenseNet121_multioutput__log__layer-64_8-epochs_in-256.csv\",\n    index_col=0)\ndisplay(multioutput_model_log_layer_64_features_8e.head(5))\nmultioutput_model_log_layer_64_features_8e.describe().loc[\"std\",:].value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-06-22T19:35:24.895241Z","iopub.execute_input":"2021-06-22T19:35:24.895601Z","iopub.status.idle":"2021-06-22T19:35:25.26635Z","shell.execute_reply.started":"2021-06-22T19:35:24.89557Z","shell.execute_reply":"2021-06-22T19:35:25.265272Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = Model(inputs=multioutput_model_log.input, outputs=multioutput_model_log.layers[-4].output)\nife = ImageFeatureExtractor(construct_from_cnn_backbone=False, model=model,\n                 images_directory=\"../input/petfinder-adoption-prediction/train_images\",\n                 from_image=True, preprocess_input=preprocess_input_densenet, img_size=256,\n                 model_name=f\"DenseNet121_multioutput__log__layer-16_8-epochs\",\n                 save=True, debug=True, include_feats=False)\n\n_ = ife.fit_transform(X, y)","metadata":{"execution":{"iopub.status.busy":"2021-06-15T23:36:46.945236Z","iopub.execute_input":"2021-06-15T23:36:46.945839Z","iopub.status.idle":"2021-06-15T23:39:15.743061Z","shell.execute_reply.started":"2021-06-15T23:36:46.9458Z","shell.execute_reply":"2021-06-15T23:39:15.741971Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"multioutput_model_log_layer_16_features_8e = pd.read_csv(\n    \"../input/tfg-pet-adoption-data/image_features_DenseNet121_multioutput__log__layer-16_8-epochs_in-256.csv\",\n    index_col=0)\ndisplay(multioutput_model_log_layer_16_features_8e.head(5))\nmultioutput_model_log_layer_16_features_8e.describe().loc[\"std\",:].value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-06-22T19:35:27.702507Z","iopub.execute_input":"2021-06-22T19:35:27.702839Z","iopub.status.idle":"2021-06-22T19:35:27.846324Z","shell.execute_reply.started":"2021-06-22T19:35:27.702811Z","shell.execute_reply":"2021-06-22T19:35:27.845277Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pipeline_transformers = [\n    ('replace_breeds', LeftJoinReplace(values_dict=breeds_dict,\n                                      variables=[\"Breed1\", \"Breed2\"])),\n    ('replace_colors', LeftJoinReplace(values_dict=colors_dict,\n                                      variables=[\"Color1\", \"Color2\", \"Color3\"])),\n    ('replace_states', LeftJoinReplace(values_dict=states_dict,\n                                     variables=[\"State\"])),\n    ('replace_by_strings', FunctionTransformer(func=replace_integers_by_strings)),\n    ('has_name', FunctionTransformer(func=has_significant_name)),\n    ('pure_breed', FunctionTransformer(func=has_pure_breed)),\n    ('breed_matches_fur_length', FunctionTransformer(func=breed_matches_fur_length)),\n    ('impute_breed', BreedImputer()),\n    ('include_prof_im_metadata', IncludeProfileImageMetadata(profile_image_metadata)),\n    ('correct_wrong_type', CorrectWrongType(breeds)),\n    ('encode_breed', BreedEncoding(enc_type=\"target_and_frequency\")),\n    ('ordinal_vars_encoder', OrdinalVariableEncoder(columns=[\"MaturitySize\", \"FurLength\", \"Health\"],\n                                enc_type=\"ordinal\", mapping=ordinal_vars_mapping)),\n    ('state_gdp', ReplaceState(gdp_per_capita=gdp_per_capita, impute_nan_value=46450)),\n    ('rescuer_count', ReplaceRescuerID()),\n    ('discretizer', CustomDiscretizer(bins_age=-1, quantity=False, fee=False, video_amt=False,\n                                     photo_amt=False)),\n    ('description_length', FunctionTransformer(func=include_description_length)),\n    ('include_desc_metadata', IncludeDescriptionMetadata(description_metadata=description_metadata)),\n    ('correct_desc_language', CorrectDescriptionLanguage()),\n    ('one_hot_encoder', CustomOneHotEncoder(columns=[\"Gender\", \"Color1\", \"Color2\",\n                            \"Color3\", \"Vaccinated\", \"Dewormed\", \"Sterilized\", \"DescriptionLanguage\"])),\n    ('include_prof_im_properties', IncludeProfileImageProperties(profile_image_properties)),\n    ('drop_columns', ColumnRemover(columns=columns_to_be_removed_image_feats_eval)),\n    ('round_im_dims_aspect_ratio', FunctionTransformer(func=include_aspect_ratio)),\n    None,\n    ('drop_petid', ColumnRemover(columns=[\"PetID\"])),\n    ('useless_vars_remover', UselessVariablesRemover(tolerance=0.000001)),\n    ('impute_malay_desc_missing_prof_im_props', CustomIterativeImputer()),\n    ('custom_standard_scaler', CustomStandardScaler(numeric_columns_image_feats_eval))\n]\n\nevaluation_results = pd.DataFrame([], columns=[\"Model description\",\n                        \"Average fit time\", \"Average accuracy\", \"Average QWK\",\n                        \"Single split accuracy\", \"Single split QWK\"])\n\nimage_features_dataframes = {\n    \"multioutput_model_scaled_layer-64\": multioutput_model_scaled_layer_64_features_8e,\n    \"multioutput_model_scaled_layer-16\": multioutput_model_scaled_layer_16_features_8e,\n    \"multioutput_model_log_layer-64\": multioutput_model_log_layer_64_features_8e,\n    \"multioutput_model_log_layer-16\": multioutput_model_log_layer_16_features_8e\n}\n\nfor features_description, loaded_features in image_features_dataframes.items():\n    ife = ImageFeatureExtractor(construct_from_cnn_backbone=False,\n                                loaded_features=loaded_features)\n    pipeline_transformers[-5] = ('image_features_extractor', ife)\n\n    model = xgb.XGBClassifier(eval_metric='mlogloss', random_state=seed, n_jobs=-1,\n                          use_label_encoder=False)\n    model_description = f\"XGBClassifier with image features from DenseNet121, {features_description}\"\n    print(f\"\\n\\n*************** {model_description} ***************\")\n    avg_fit_time, avg_accuracy, avg_QWK = evaluate_model(\n                Pipeline(steps=pipeline_transformers + [('model', model)]),\n                cv, X, y, model_type=\"classification\", display_results=True,\n                display_plots=False)\n    \n    single_accuracy, single_QWK = evaluate_model_single_split(\n                Pipeline(steps=pipeline_transformers + [('model', model)]),\n                X_train_CNN, X_val_CNN, y_train_CNN, y_val_CNN,\n                model_type=\"classification\", display_results=True)\n\n    evaluation_results = evaluation_results.append({\n        \"Model description\": model_description,\n        \"Average fit time\": avg_fit_time,\n        \"Average accuracy\": avg_accuracy,\n        \"Average QWK\": avg_QWK,\n        \"Single split accuracy\": single_accuracy,\n        \"Single split QWK\": single_QWK\n    }, ignore_index=True)\n\npd.set_option('display.max_colwidth', None)\ndisplay(evaluation_results)\npd.set_option('display.max_colwidth', 50)\nto_latex(evaluation_results, \"evaluation_results_XGBClassifier_DenseNet121_multioutput_8-epochs-all\")","metadata":{"execution":{"iopub.status.busy":"2021-06-22T19:35:28.191647Z","iopub.execute_input":"2021-06-22T19:35:28.191978Z","iopub.status.idle":"2021-06-22T19:44:45.79253Z","shell.execute_reply.started":"2021-06-22T19:35:28.191947Z","shell.execute_reply":"2021-06-22T19:44:45.79146Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Using some of the predictor variables with the highest mutual information with AdoptionSpeed as target variables in the CNN does not seem to yield better results than using AdoptionSpeed as the CNN's target variable.","metadata":{}},{"cell_type":"markdown","source":"#### Data augmentation and Dropout","metadata":{}},{"cell_type":"markdown","source":"First of all, let's train the regression model using more data augmentation strategies:","metadata":{}},{"cell_type":"code","source":"adoptionspeed_train_generator_da = ImageDataGenerator(\n    horizontal_flip=True,\n    zoom_range=[0.5, 1.0],\n#     rotation_range=30,\n#     brightness_range=[0.7, 1.0],\n    width_shift_range=0.3,\n    height_shift_range=0.3,\n    preprocessing_function=preprocess_input_densenet\n)\n\nadoptionspeed_val_generator = ImageDataGenerator(\n    preprocessing_function=preprocess_input_densenet\n)","metadata":{"execution":{"iopub.status.busy":"2021-06-21T19:14:39.600726Z","iopub.execute_input":"2021-06-21T19:14:39.601235Z","iopub.status.idle":"2021-06-21T19:14:39.608037Z","shell.execute_reply.started":"2021-06-21T19:14:39.601193Z","shell.execute_reply":"2021-06-21T19:14:39.606823Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"target_size = (256,256)\nbatch_size = 32\n\nregression_adoptionspeed_train_iterator_da = adoptionspeed_train_generator_da.flow_from_dataframe(\n    dataframe=filenames_and_target_values_train_no_instance_overlap,\n    directory=\"../input/tfg-pet-adoption-resized-train-images/\",\n    x_col=\"filename\",\n    y_col=\"AdoptionSpeed\",\n    target_size=target_size,\n    class_mode=\"raw\",\n    batch_size=batch_size,\n    shuffle=True,\n    seed=seed\n)\n\nregression_adoptionspeed_val_iterator = adoptionspeed_val_generator.flow_from_dataframe(\n    dataframe=filenames_and_target_values_val_no_instance_overlap,\n    directory=\"../input/tfg-pet-adoption-resized-train-images/\",\n    x_col=\"filename\",\n    y_col=\"AdoptionSpeed\",\n    target_size=target_size,\n    class_mode=\"raw\",\n    batch_size=batch_size,\n    shuffle=False,\n    seed=seed\n)","metadata":{"execution":{"iopub.status.busy":"2021-06-21T19:14:39.609569Z","iopub.execute_input":"2021-06-21T19:14:39.610048Z","iopub.status.idle":"2021-06-21T19:16:48.780171Z","shell.execute_reply.started":"2021-06-21T19:14:39.609994Z","shell.execute_reply":"2021-06-21T19:16:48.77924Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"inp = Input((256,256,3))\nbackbone = DenseNet121(input_tensor=inp, include_top=False, weights=\"imagenet\")\nx = backbone.output\nx = GlobalAveragePooling2D()(x)\nx = Dense(64, activation=\"relu\")(x)\nx = Dense(16, activation=\"relu\")(x)\nout = Dense(1, activation=\"linear\")(x)\n\n# Freezing DenseNet121 backbone\nfor layer in backbone.layers:\n    layer.trainable = False\n\nregression_model_data_aug = Model(inp,out)\n\noptimizer = Adam(lr=0.001)\nmetrics = [\"mean_absolute_error\"]\nloss = \"mean_squared_error\"\n    \nregression_model_data_aug.compile(loss=loss, metrics=metrics, optimizer=optimizer)","metadata":{"execution":{"iopub.status.busy":"2021-06-16T09:40:48.992911Z","iopub.execute_input":"2021-06-16T09:40:48.993471Z","iopub.status.idle":"2021-06-16T09:40:52.353114Z","shell.execute_reply.started":"2021-06-16T09:40:48.993431Z","shell.execute_reply":"2021-06-16T09:40:52.352325Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now, we will increase the number of epochs as the inputs will vary more than before:","metadata":{}},{"cell_type":"code","source":"epochs = 30\nbatch_size = 32\n\nregression_da_training_results = pd.DataFrame([], columns=[\"loss\",\n                        \"mean_absolute_error\", \"val_loss\",\n                        \"val_mean_absolute_error\"])\n\nearly_stopping = EarlyStopping(monitor='val_loss', restore_best_weights=True, patience=5)\nmodel_checkpoint = ModelCheckpoint(\n    'DenseNet121_regression__data-aug__64-16_{epoch:02d}-epochs_val_loss-{val_loss:02f}.h5',\n    monitor='val_loss', save_best_only=True)\n\nregression_da_history = regression_model_data_aug.fit(\n                    x=regression_adoptionspeed_train_iterator_da,\n                    validation_data=regression_adoptionspeed_val_iterator,\n                    batch_size=batch_size, epochs=epochs,\n                    callbacks=[model_checkpoint, early_stopping])\n\ntrain_loss = regression_da_history.history[\"loss\"]\ntrain_mae = regression_da_history.history[\"mean_absolute_error\"]\nvalidation_loss = regression_da_history.history[\"val_loss\"]\nvalidation_mae = regression_da_history.history[\"val_mean_absolute_error\"]\n\nfor i in range(len(train_loss)):\n    regression_da_training_results = regression_da_training_results.append({\n        \"loss\": train_loss[i],\n        \"mean_absolute_error\": train_mae[i],\n        \"val_loss\": validation_loss[i],\n        \"val_mean_absolute_error\": validation_mae[i]\n    }, ignore_index=True)\n\nregression_model_data_aug.save(f\"DenseNet121_regression__data-aug__64-16_{epochs}-epochs.h5\")\nregression_da_training_results.to_csv(f\"DenseNet121_regression__data-aug__64-16_{epochs}-epochs.csv\", index=False)\nto_latex(regression_da_training_results, f\"DenseNet121_regression__data-aug__64-16_{epochs}-epochs\")\n\n# Zipping the checkpoints\n!mkdir regression_da_no_dropout\n!mv ./DenseNet121_regression__data-aug__*.h5 regression_da_no_dropout\nshutil.make_archive(\"DenseNet121_regression__data-aug__no-dropout__checkpoints\", \"zip\", \"./regression_da_no_dropout\")","metadata":{"execution":{"iopub.status.busy":"2021-06-16T09:41:05.80678Z","iopub.execute_input":"2021-06-16T09:41:05.80709Z","iopub.status.idle":"2021-06-16T11:46:57.774405Z","shell.execute_reply.started":"2021-06-16T09:41:05.807061Z","shell.execute_reply":"2021-06-16T11:46:57.772564Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"regression_da_training_results = pd.read_csv(\"../input/tfg-pet-adoption-data/DenseNet121_regression__data-aug__64-16_30-epochs.csv\")\nplot_history(regression_da_training_results, [\"loss\", \"mean_absolute_error\"], 1, 2, (12,3))","metadata":{"execution":{"iopub.status.busy":"2021-06-22T19:44:45.795222Z","iopub.execute_input":"2021-06-22T19:44:45.795526Z","iopub.status.idle":"2021-06-22T19:44:46.081106Z","shell.execute_reply.started":"2021-06-22T19:44:45.795489Z","shell.execute_reply":"2021-06-22T19:44:46.080339Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The training process stopped after epoch 13 as the validation loss didn't improve since epoch 8. Let's load the model after epoch 8:","metadata":{}},{"cell_type":"code","source":"regression_model_data_aug = load_model(\"../input/tfg-pet-adoption-data/DenseNet121_regression__data-aug__64-16_06-epochs_val_loss-1.171272.h5\")","metadata":{"execution":{"iopub.status.busy":"2021-06-11T11:57:39.599235Z","iopub.execute_input":"2021-06-11T11:57:39.599615Z","iopub.status.idle":"2021-06-11T11:57:43.696929Z","shell.execute_reply.started":"2021-06-11T11:57:39.599581Z","shell.execute_reply":"2021-06-11T11:57:43.695785Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = Model(inputs=regression_model_data_aug.input, outputs=regression_model_data_aug.layers[-3].output)\nife = ImageFeatureExtractor(construct_from_cnn_backbone=False, model=model,\n                 images_directory=\"../input/petfinder-adoption-prediction/train_images\",\n                 from_image=True, preprocess_input=preprocess_input_densenet, img_size=256,\n                 model_name=f\"DenseNet121_regression__data-aug__layer-64_6-epochs\",\n                 save=True, debug=True, include_feats=False)\n\n_ = ife.fit_transform(X, y)","metadata":{"execution":{"iopub.status.busy":"2021-06-16T12:06:49.56767Z","iopub.execute_input":"2021-06-16T12:06:49.567997Z","iopub.status.idle":"2021-06-16T12:09:50.698938Z","shell.execute_reply.started":"2021-06-16T12:06:49.567967Z","shell.execute_reply":"2021-06-16T12:09:50.69682Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"regression_model_data_aug_layer_64_features = pd.read_csv(\n    \"../input/tfg-pet-adoption-data/image_features_DenseNet121_regression__data-aug__layer-64_6-epochs_in-256.csv\",\n    index_col=0)\ndisplay(regression_model_data_aug_layer_64_features.head(5))\nregression_model_data_aug_layer_64_features.describe().loc[\"std\",:].value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-06-22T19:44:46.082207Z","iopub.execute_input":"2021-06-22T19:44:46.082632Z","iopub.status.idle":"2021-06-22T19:44:46.402056Z","shell.execute_reply.started":"2021-06-22T19:44:46.082602Z","shell.execute_reply":"2021-06-22T19:44:46.401121Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = Model(inputs=regression_model_data_aug.input, outputs=regression_model_data_aug.layers[-2].output)\nife = ImageFeatureExtractor(construct_from_cnn_backbone=False, model=model,\n                 images_directory=\"../input/petfinder-adoption-prediction/train_images\",\n                 from_image=True, preprocess_input=preprocess_input_densenet, img_size=256,\n                 model_name=f\"DenseNet121_regression__data-aug__layer-16_6-epochs\",\n                 save=True, debug=True, include_feats=False)\n\n_ = ife.fit_transform(X, y)","metadata":{"execution":{"iopub.status.busy":"2021-06-16T12:09:50.701298Z","iopub.execute_input":"2021-06-16T12:09:50.70171Z","iopub.status.idle":"2021-06-16T12:12:14.47277Z","shell.execute_reply.started":"2021-06-16T12:09:50.701647Z","shell.execute_reply":"2021-06-16T12:12:14.471778Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"regression_model_data_aug_layer_16_features = pd.read_csv(\n    \"../input/tfg-pet-adoption-data/image_features_DenseNet121_regression__data-aug__layer-16_6-epochs_in-256.csv\",\n    index_col=0)\ndisplay(regression_model_data_aug_layer_16_features.head(5))\nregression_model_data_aug_layer_16_features.describe().loc[\"std\",:].value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-06-22T19:44:46.403406Z","iopub.execute_input":"2021-06-22T19:44:46.403661Z","iopub.status.idle":"2021-06-22T19:44:46.536475Z","shell.execute_reply.started":"2021-06-22T19:44:46.403635Z","shell.execute_reply":"2021-06-22T19:44:46.535475Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now, let's train it without data augmentation (just horizontal flipping, as before) but with Dropout layers, with the same number of epochs as was done originally, 5, in order to see whether more features are useful (we saw that the outputs of layer Dense 16 were all 0 except one feature). We will test 3 setups: dropout before layer Dense 64, dropout before layer Dense 16 and dropout before both.","metadata":{}},{"cell_type":"code","source":"target_size = (256,256)\nbatch_size = 32\n\nadoptionspeed_train_generator = ImageDataGenerator(\n    horizontal_flip=True,\n    preprocessing_function=preprocess_input_densenet\n)\n\nregression_adoptionspeed_train_iterator = adoptionspeed_train_generator.flow_from_dataframe(\n    dataframe=filenames_and_target_values_train_no_instance_overlap,\n    directory=\"../input/tfg-pet-adoption-resized-train-images/\",\n    x_col=\"filename\",\n    y_col=\"AdoptionSpeed\",\n    target_size=target_size,\n    class_mode=\"raw\",\n    batch_size=batch_size,\n    shuffle=True,\n    seed=seed\n)","metadata":{"execution":{"iopub.status.busy":"2021-06-16T12:12:14.818582Z","iopub.execute_input":"2021-06-16T12:12:14.818949Z","iopub.status.idle":"2021-06-16T12:12:31.238088Z","shell.execute_reply.started":"2021-06-16T12:12:14.818913Z","shell.execute_reply":"2021-06-16T12:12:31.23715Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"inp = Input((256,256,3))\nbackbone = DenseNet121(input_tensor=inp, include_top=False, weights=\"imagenet\")\nx = backbone.output\nx = GlobalAveragePooling2D()(x)\nx = Dropout(rate=0.25, seed=seed)(x)\nx = Dense(64, activation=\"relu\")(x)\nx = Dense(16, activation=\"relu\")(x)\nout = Dense(1, activation=\"linear\")(x)\n\n# Freezing DenseNet121 backbone\nfor layer in backbone.layers:\n    layer.trainable = False\n\nregression_model_dropout_64 = Model(inp,out)\n\noptimizer = Adam(lr=0.001)\nmetrics = [\"mean_absolute_error\"]\nloss = \"mean_squared_error\"\n    \nregression_model_dropout_64.compile(loss=loss, metrics=metrics, optimizer=optimizer)","metadata":{"execution":{"iopub.status.busy":"2021-06-16T12:12:31.239365Z","iopub.execute_input":"2021-06-16T12:12:31.239875Z","iopub.status.idle":"2021-06-16T12:12:34.506297Z","shell.execute_reply.started":"2021-06-16T12:12:31.239837Z","shell.execute_reply":"2021-06-16T12:12:34.505503Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"inp = Input((256,256,3))\nbackbone = DenseNet121(input_tensor=inp, include_top=False, weights=\"imagenet\")\nx = backbone.output\nx = GlobalAveragePooling2D()(x)\nx = Dense(64, activation=\"relu\")(x)\nx = Dropout(rate=0.25, seed=seed)(x)\nx = Dense(16, activation=\"relu\")(x)\nout = Dense(1, activation=\"linear\")(x)\n\n# Freezing DenseNet121 backbone\nfor layer in backbone.layers:\n    layer.trainable = False\n\nregression_model_dropout_16 = Model(inp,out)\n\noptimizer = Adam(lr=0.001)\nmetrics = [\"mean_absolute_error\"]\nloss = \"mean_squared_error\"\n    \nregression_model_dropout_16.compile(loss=loss, metrics=metrics, optimizer=optimizer)","metadata":{"execution":{"iopub.status.busy":"2021-06-16T12:12:34.507566Z","iopub.execute_input":"2021-06-16T12:12:34.507929Z","iopub.status.idle":"2021-06-16T12:12:36.91837Z","shell.execute_reply.started":"2021-06-16T12:12:34.507896Z","shell.execute_reply":"2021-06-16T12:12:36.917614Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"inp = Input((256,256,3))\nbackbone = DenseNet121(input_tensor=inp, include_top=False, weights=\"imagenet\")\nx = backbone.output\nx = GlobalAveragePooling2D()(x)\nx = Dropout(rate=0.25, seed=seed)(x)\nx = Dense(64, activation=\"relu\")(x)\nx = Dropout(rate=0.25, seed=seed)(x)\nx = Dense(16, activation=\"relu\")(x)\nout = Dense(1, activation=\"linear\")(x)\n\n# Freezing DenseNet121 backbone\nfor layer in backbone.layers:\n    layer.trainable = False\n\nregression_model_dropout_64_16 = Model(inp,out)\n\noptimizer = Adam(lr=0.001)\nmetrics = [\"mean_absolute_error\"]\nloss = \"mean_squared_error\"\n    \nregression_model_dropout_64_16.compile(loss=loss, metrics=metrics, optimizer=optimizer)","metadata":{"execution":{"iopub.status.busy":"2021-06-16T12:12:36.919635Z","iopub.execute_input":"2021-06-16T12:12:36.919999Z","iopub.status.idle":"2021-06-16T12:12:39.324085Z","shell.execute_reply.started":"2021-06-16T12:12:36.919951Z","shell.execute_reply":"2021-06-16T12:12:39.32322Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"epochs = 5\nbatch_size = 32\n\nregression_models_dropout = {\n    \"regression__dropout_64\": regression_model_dropout_64,\n    \"regression__dropout_16\": regression_model_dropout_16,\n    \"regression__dropout_64-16\": regression_model_dropout_64_16\n}\n\nfor model_desc, regression_model in regression_models_dropout.items():\n    regression_dropout_training_results = pd.DataFrame([], columns=[\"loss\",\n                        \"mean_absolute_error\", \"val_loss\",\n                        \"val_mean_absolute_error\"])\n    \n    regression_history = regression_model.fit(\n                        x=regression_adoptionspeed_train_iterator,\n                        validation_data=regression_adoptionspeed_val_iterator,\n                        batch_size=batch_size, epochs=epochs)\n\n    train_loss = regression_history.history[\"loss\"]\n    train_mae = regression_history.history[\"mean_absolute_error\"]\n    validation_loss = regression_history.history[\"val_loss\"]\n    validation_mae = regression_history.history[\"val_mean_absolute_error\"]\n\n    for i in range(len(train_loss)):\n        regression_dropout_training_results = regression_dropout_training_results.append({\n            \"loss\": train_loss[i],\n            \"mean_absolute_error\": train_mae[i],\n            \"val_loss\": validation_loss[i],\n            \"val_mean_absolute_error\": validation_mae[i]\n        }, ignore_index=True)\n\n    regression_model.save(f\"DenseNet121_{model_desc}__64-16_{epochs}-epochs.h5\")\n    regression_dropout_training_results.to_csv(f\"DenseNet121_{model_desc}__64-16_{epochs}-epochs.csv\", index=False)\n    to_latex(regression_dropout_training_results, f\"DenseNet121_{model_desc}__64-16_{epochs}-epochs\")","metadata":{"execution":{"iopub.status.busy":"2021-06-16T12:12:39.325305Z","iopub.execute_input":"2021-06-16T12:12:39.325625Z","iopub.status.idle":"2021-06-16T13:01:58.559966Z","shell.execute_reply.started":"2021-06-16T12:12:39.325591Z","shell.execute_reply":"2021-06-16T13:01:58.559155Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"regression_models_dropout = [\"dropout_64\", \"dropout_16\", \"dropout_64-16\"]\nfor model_desc in regression_models_dropout:\n    regression_training_results_dropout = pd.read_csv(f\"../input/tfg-pet-adoption-data/DenseNet121_regression__{model_desc}__64-16_5-epochs.csv\")\n    plot_history(regression_training_results_dropout, [\"loss\", \"mean_absolute_error\"], 1, 2, (12,3))","metadata":{"execution":{"iopub.status.busy":"2021-06-22T19:44:46.539132Z","iopub.execute_input":"2021-06-22T19:44:46.539381Z","iopub.status.idle":"2021-06-22T19:44:47.493476Z","shell.execute_reply.started":"2021-06-22T19:44:46.539356Z","shell.execute_reply":"2021-06-22T19:44:47.492676Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's extract the outputs of the 64 and 16 neurons layers of each model with dropout:","metadata":{}},{"cell_type":"code","source":"regression_model_dropout_64 = load_model(\"../input/tfg-pet-adoption-data/DenseNet121_regression__dropout_64__64-16_5-epochs.h5\")","metadata":{"execution":{"iopub.status.busy":"2021-06-11T12:07:59.317834Z","iopub.execute_input":"2021-06-11T12:07:59.318193Z","iopub.status.idle":"2021-06-11T12:08:03.390761Z","shell.execute_reply.started":"2021-06-11T12:07:59.31816Z","shell.execute_reply":"2021-06-11T12:08:03.389687Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = Model(inputs=regression_model_dropout_64.input, outputs=regression_model_dropout_64.layers[-3].output)\nife = ImageFeatureExtractor(construct_from_cnn_backbone=False, model=model,\n                 images_directory=\"../input/petfinder-adoption-prediction/train_images\",\n                 from_image=True, preprocess_input=preprocess_input_densenet, img_size=256,\n                 model_name=f\"DenseNet121_regression__dropout_64__layer-64_5-epochs\",\n                 save=True, debug=True, include_feats=False)\n\n_ = ife.fit_transform(X, y)","metadata":{"execution":{"iopub.status.busy":"2021-06-16T13:02:00.103703Z","iopub.execute_input":"2021-06-16T13:02:00.103955Z","iopub.status.idle":"2021-06-16T13:04:25.943766Z","shell.execute_reply.started":"2021-06-16T13:02:00.103929Z","shell.execute_reply":"2021-06-16T13:04:25.942826Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"regression_model_dropout_64_layer_64_features = pd.read_csv(\n    \"../input/tfg-pet-adoption-data/image_features_DenseNet121_regression__dropout_64__layer-64_5-epochs_in-256.csv\",\n    index_col=0)\ndisplay(regression_model_dropout_64_layer_64_features.head(5))\nregression_model_dropout_64_layer_64_features.describe().loc[\"std\",:].value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-06-22T19:44:47.496727Z","iopub.execute_input":"2021-06-22T19:44:47.497624Z","iopub.status.idle":"2021-06-22T19:44:47.848924Z","shell.execute_reply.started":"2021-06-22T19:44:47.497557Z","shell.execute_reply":"2021-06-22T19:44:47.847867Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = Model(inputs=regression_model_dropout_64.input, outputs=regression_model_dropout_64.layers[-2].output)\nife = ImageFeatureExtractor(construct_from_cnn_backbone=False, model=model,\n                 images_directory=\"../input/petfinder-adoption-prediction/train_images\",\n                 from_image=True, preprocess_input=preprocess_input_densenet, img_size=256,\n                 model_name=f\"DenseNet121_regression__dropout_64__layer-16_5-epochs\",\n                 save=True, debug=True, include_feats=False)\n\n_ = ife.fit_transform(X, y)","metadata":{"execution":{"iopub.status.busy":"2021-06-16T13:04:26.234619Z","iopub.execute_input":"2021-06-16T13:04:26.234967Z","iopub.status.idle":"2021-06-16T13:06:50.68504Z","shell.execute_reply.started":"2021-06-16T13:04:26.234931Z","shell.execute_reply":"2021-06-16T13:06:50.683038Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"regression_model_dropout_64_layer_16_features = pd.read_csv(\n    \"../input/tfg-pet-adoption-data/image_features_DenseNet121_regression__dropout_64__layer-16_5-epochs_in-256.csv\",\n    index_col=0)\ndisplay(regression_model_dropout_64_layer_16_features.head(5))\nregression_model_dropout_64_layer_16_features.describe().loc[\"std\",:].value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-06-22T19:44:47.849899Z","iopub.execute_input":"2021-06-22T19:44:47.850177Z","iopub.status.idle":"2021-06-22T19:44:47.966814Z","shell.execute_reply.started":"2021-06-22T19:44:47.850151Z","shell.execute_reply":"2021-06-22T19:44:47.965628Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"regression_model_dropout_16 = load_model(\"../input/tfg-pet-adoption-data/DenseNet121_regression__dropout_16__64-16_5-epochs.h5\")","metadata":{"execution":{"iopub.status.busy":"2021-06-11T12:08:07.480491Z","iopub.execute_input":"2021-06-11T12:08:07.480981Z","iopub.status.idle":"2021-06-11T12:08:11.836027Z","shell.execute_reply.started":"2021-06-11T12:08:07.48094Z","shell.execute_reply":"2021-06-11T12:08:11.835034Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = Model(inputs=regression_model_dropout_16.input, outputs=regression_model_dropout_16.layers[-4].output)\nife = ImageFeatureExtractor(construct_from_cnn_backbone=False, model=model,\n                 images_directory=\"../input/petfinder-adoption-prediction/train_images\",\n                 from_image=True, preprocess_input=preprocess_input_densenet, img_size=256,\n                 model_name=f\"DenseNet121_regression__dropout_16__layer-64_5-epochs\",\n                 save=True, debug=True, include_feats=False)\n\n_ = ife.fit_transform(X, y)","metadata":{"execution":{"iopub.status.busy":"2021-06-16T13:06:50.772468Z","iopub.execute_input":"2021-06-16T13:06:50.772853Z","iopub.status.idle":"2021-06-16T13:09:18.110199Z","shell.execute_reply.started":"2021-06-16T13:06:50.772818Z","shell.execute_reply":"2021-06-16T13:09:18.10925Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"regression_model_dropout_16_layer_64_features = pd.read_csv(\n    \"../input/tfg-pet-adoption-data/image_features_DenseNet121_regression__dropout_16__layer-64_5-epochs_in-256.csv\",\n    index_col=0)\ndisplay(regression_model_dropout_16_layer_64_features.head(5))\nregression_model_dropout_16_layer_64_features.describe().loc[\"std\",:].value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-06-22T19:44:47.967945Z","iopub.execute_input":"2021-06-22T19:44:47.968223Z","iopub.status.idle":"2021-06-22T19:44:48.295532Z","shell.execute_reply.started":"2021-06-22T19:44:47.968196Z","shell.execute_reply":"2021-06-22T19:44:48.294511Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = Model(inputs=regression_model_dropout_16.input, outputs=regression_model_dropout_16.layers[-2].output)\nife = ImageFeatureExtractor(construct_from_cnn_backbone=False, model=model,\n                 images_directory=\"../input/petfinder-adoption-prediction/train_images\",\n                 from_image=True, preprocess_input=preprocess_input_densenet, img_size=256,\n                 model_name=f\"DenseNet121_regression__dropout_16__layer-16_5-epochs\",\n                 save=True, debug=True, include_feats=False)\n\n_ = ife.fit_transform(X, y)","metadata":{"execution":{"iopub.status.busy":"2021-06-16T13:09:18.357934Z","iopub.execute_input":"2021-06-16T13:09:18.358287Z","iopub.status.idle":"2021-06-16T13:11:42.699344Z","shell.execute_reply.started":"2021-06-16T13:09:18.358248Z","shell.execute_reply":"2021-06-16T13:11:42.698349Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"regression_model_dropout_16_layer_16_features = pd.read_csv(\n    \"../input/tfg-pet-adoption-data/image_features_DenseNet121_regression__dropout_16__layer-16_5-epochs_in-256.csv\",\n    index_col=0)\ndisplay(regression_model_dropout_16_layer_16_features.head(5))\nregression_model_dropout_16_layer_16_features.describe().loc[\"std\",:].value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-06-22T19:44:48.296666Z","iopub.execute_input":"2021-06-22T19:44:48.296911Z","iopub.status.idle":"2021-06-22T19:44:48.413709Z","shell.execute_reply.started":"2021-06-22T19:44:48.296886Z","shell.execute_reply":"2021-06-22T19:44:48.412134Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"regression_model_dropout_64_16 = load_model(\"../input/tfg-pet-adoption-data/DenseNet121_regression__dropout_64-16__64-16_5-epochs.h5\")","metadata":{"execution":{"iopub.status.busy":"2021-06-11T12:08:12.297846Z","iopub.execute_input":"2021-06-11T12:08:12.298234Z","iopub.status.idle":"2021-06-11T12:08:16.060474Z","shell.execute_reply.started":"2021-06-11T12:08:12.298195Z","shell.execute_reply":"2021-06-11T12:08:16.059597Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = Model(inputs=regression_model_dropout_64_16.input, outputs=regression_model_dropout_64_16.layers[-4].output)\nife = ImageFeatureExtractor(construct_from_cnn_backbone=False, model=model,\n                 images_directory=\"../input/petfinder-adoption-prediction/train_images\",\n                 from_image=True, preprocess_input=preprocess_input_densenet, img_size=256,\n                 model_name=f\"DenseNet121_regression__dropout_64-16__layer-64_5-epochs\",\n                 save=True, debug=True, include_feats=False)\n\n_ = ife.fit_transform(X, y)","metadata":{"execution":{"iopub.status.busy":"2021-06-16T13:11:42.787257Z","iopub.execute_input":"2021-06-16T13:11:42.787604Z","iopub.status.idle":"2021-06-16T13:14:09.915811Z","shell.execute_reply.started":"2021-06-16T13:11:42.787569Z","shell.execute_reply":"2021-06-16T13:14:09.915039Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"regression_model_dropout_64_16_layer_64_features = pd.read_csv(\n    \"../input/tfg-pet-adoption-data/image_features_DenseNet121_regression__dropout_64-16__layer-64_5-epochs_in-256.csv\",\n    index_col=0)\ndisplay(regression_model_dropout_64_16_layer_64_features.head(5))\nregression_model_dropout_64_16_layer_64_features.describe().loc[\"std\",:].value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-06-22T19:44:48.414795Z","iopub.execute_input":"2021-06-22T19:44:48.415049Z","iopub.status.idle":"2021-06-22T19:44:48.736076Z","shell.execute_reply.started":"2021-06-22T19:44:48.415005Z","shell.execute_reply":"2021-06-22T19:44:48.73486Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = Model(inputs=regression_model_dropout_64_16.input, outputs=regression_model_dropout_64_16.layers[-2].output)\nife = ImageFeatureExtractor(construct_from_cnn_backbone=False, model=model,\n                 images_directory=\"../input/petfinder-adoption-prediction/train_images\",\n                 from_image=True, preprocess_input=preprocess_input_densenet, img_size=256,\n                 model_name=f\"DenseNet121_regression__dropout_64-16__layer-16_5-epochs\",\n                 save=True, debug=True, include_feats=False)\n\n_ = ife.fit_transform(X, y)","metadata":{"execution":{"iopub.status.busy":"2021-06-16T13:14:10.160952Z","iopub.execute_input":"2021-06-16T13:14:10.161317Z","iopub.status.idle":"2021-06-16T13:16:34.950905Z","shell.execute_reply.started":"2021-06-16T13:14:10.161279Z","shell.execute_reply":"2021-06-16T13:16:34.950092Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"regression_model_dropout_64_16_layer_16_features = pd.read_csv(\n    \"../input/tfg-pet-adoption-data/image_features_DenseNet121_regression__dropout_64-16__layer-16_5-epochs_in-256.csv\",\n    index_col=0)\ndisplay(regression_model_dropout_64_16_layer_16_features.head(5))\nregression_model_dropout_64_16_layer_16_features.describe().loc[\"std\",:].value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-06-22T19:44:48.739405Z","iopub.execute_input":"2021-06-22T19:44:48.739683Z","iopub.status.idle":"2021-06-22T19:44:48.861822Z","shell.execute_reply.started":"2021-06-22T19:44:48.739656Z","shell.execute_reply":"2021-06-22T19:44:48.860796Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pipeline_transformers = [\n    ('replace_breeds', LeftJoinReplace(values_dict=breeds_dict,\n                                      variables=[\"Breed1\", \"Breed2\"])),\n    ('replace_colors', LeftJoinReplace(values_dict=colors_dict,\n                                      variables=[\"Color1\", \"Color2\", \"Color3\"])),\n    ('replace_states', LeftJoinReplace(values_dict=states_dict,\n                                     variables=[\"State\"])),\n    ('replace_by_strings', FunctionTransformer(func=replace_integers_by_strings)),\n    ('has_name', FunctionTransformer(func=has_significant_name)),\n    ('pure_breed', FunctionTransformer(func=has_pure_breed)),\n    ('breed_matches_fur_length', FunctionTransformer(func=breed_matches_fur_length)),\n    ('impute_breed', BreedImputer()),\n    ('include_prof_im_metadata', IncludeProfileImageMetadata(profile_image_metadata)),\n    ('correct_wrong_type', CorrectWrongType(breeds)),\n    ('encode_breed', BreedEncoding(enc_type=\"target_and_frequency\")),\n    ('ordinal_vars_encoder', OrdinalVariableEncoder(columns=[\"MaturitySize\", \"FurLength\", \"Health\"],\n                                enc_type=\"ordinal\", mapping=ordinal_vars_mapping)),\n    ('state_gdp', ReplaceState(gdp_per_capita=gdp_per_capita, impute_nan_value=46450)),\n    ('rescuer_count', ReplaceRescuerID()),\n    ('discretizer', CustomDiscretizer(bins_age=-1, quantity=False, fee=False, video_amt=False,\n                                     photo_amt=False)),\n    ('description_length', FunctionTransformer(func=include_description_length)),\n    ('include_desc_metadata', IncludeDescriptionMetadata(description_metadata=description_metadata)),\n    ('correct_desc_language', CorrectDescriptionLanguage()),\n    ('one_hot_encoder', CustomOneHotEncoder(columns=[\"Gender\", \"Color1\", \"Color2\",\n                            \"Color3\", \"Vaccinated\", \"Dewormed\", \"Sterilized\", \"DescriptionLanguage\"])),\n    ('include_prof_im_properties', IncludeProfileImageProperties(profile_image_properties)),\n    ('drop_columns', ColumnRemover(columns=columns_to_be_removed_image_feats_eval)),\n    ('round_im_dims_aspect_ratio', FunctionTransformer(func=include_aspect_ratio)),\n    None,\n    ('drop_petid', ColumnRemover(columns=[\"PetID\"])),\n    ('useless_vars_remover', UselessVariablesRemover(tolerance=0.000001)),\n    ('impute_malay_desc_missing_prof_im_props', CustomIterativeImputer()),\n    ('custom_standard_scaler', CustomStandardScaler(numeric_columns_image_feats_eval))\n]\n\nevaluation_results = pd.DataFrame([], columns=[\"Model description\",\n                        \"Average fit time\", \"Average accuracy\", \"Average QWK\",\n                        \"Single split accuracy\", \"Single split QWK\"])\n\nimage_features_dataframes = {\n    \"AdoptionSpeed, regression_model_data-aug_layer-64_6e\": regression_model_data_aug_layer_64_features,\n    \"AdoptionSpeed, regression_model_data-aug_layer-16_6e\": regression_model_data_aug_layer_16_features,\n    \"AdoptionSpeed, regression_model_dropout-64_layer-64_5e\": regression_model_dropout_64_layer_64_features,\n    \"AdoptionSpeed, regression_model_dropout-64_layer-16_5e\": regression_model_dropout_64_layer_16_features,\n    \"AdoptionSpeed, regression_model_dropout-16_layer-64_5e\": regression_model_dropout_16_layer_64_features,\n    \"AdoptionSpeed, regression_model_dropout-16_layer-16_5e\": regression_model_dropout_16_layer_16_features,\n    \"AdoptionSpeed, regression_model_dropout-64-16_layer-64_5e\": regression_model_dropout_64_16_layer_64_features,\n    \"AdoptionSpeed, regression_model_dropout-64-16_layer-16_5e\": regression_model_dropout_64_16_layer_16_features\n}\n\nfor features_description, loaded_features in image_features_dataframes.items():\n    ife = ImageFeatureExtractor(construct_from_cnn_backbone=False,\n                                loaded_features=loaded_features)\n    pipeline_transformers[-5] = ('image_features_extractor', ife)\n\n    model = xgb.XGBClassifier(eval_metric='mlogloss', random_state=seed, n_jobs=-1,\n                          use_label_encoder=False)\n    model_description = f\"XGBClassifier, DenseNet121, {features_description}\"\n    print(f\"\\n\\n*************** {model_description} ***************\")\n    avg_fit_time, avg_accuracy, avg_QWK = evaluate_model(\n                Pipeline(steps=pipeline_transformers + [('model', model)]),\n                cv, X, y, model_type=\"classification\", display_results=True,\n                display_plots=False)\n    \n    single_accuracy, single_QWK = evaluate_model_single_split(\n                Pipeline(steps=pipeline_transformers + [('model', model)]),\n                X_train_CNN, X_val_CNN, y_train_CNN, y_val_CNN,\n                model_type=\"classification\", display_results=True)\n\n    evaluation_results = evaluation_results.append({\n        \"Model description\": model_description,\n        \"Average fit time\": avg_fit_time,\n        \"Average accuracy\": avg_accuracy,\n        \"Average QWK\": avg_QWK,\n        \"Single split accuracy\": single_accuracy,\n        \"Single split QWK\": single_QWK\n    }, ignore_index=True)\n\npd.set_option('display.max_colwidth', None)\ndisplay(evaluation_results)\npd.set_option('display.max_colwidth', 50)\nto_latex(evaluation_results, \"evaluation_results_XGBClassifier_DenseNet121_regression__data-aug__dropout\")","metadata":{"execution":{"iopub.status.busy":"2021-06-22T19:44:48.863133Z","iopub.execute_input":"2021-06-22T19:44:48.863386Z","iopub.status.idle":"2021-06-22T20:01:13.403727Z","shell.execute_reply.started":"2021-06-22T19:44:48.863361Z","shell.execute_reply":"2021-06-22T20:01:13.402848Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It is no surprise that in a limited number of epochs the models with one Dropout layer perform better than that with two dropout layer. However, as we will unfreeze some of the DenseNet121 backbone layers in the next section and train with data augmentation, it will be better to use two Dropout layers in order to generalize better. ","metadata":{}},{"cell_type":"markdown","source":"#### Data augmentation + Dropout + Fine-tuning","metadata":{}},{"cell_type":"markdown","source":"Let's train the regression model with 2 Dropout layers using the training iterator with several data augmentation methods on a few epochs (we will monitor the validation loss, when it does not improve, the training is finished):","metadata":{}},{"cell_type":"code","source":"inp = Input((256,256,3))\nbackbone = DenseNet121(input_tensor=inp, include_top=False, weights=\"imagenet\")\nx = backbone.output\nx = GlobalAveragePooling2D()(x)\nx = Dropout(rate=0.25, seed=seed)(x)\nx = Dense(64, activation=\"relu\")(x)\nx = Dropout(rate=0.25, seed=seed)(x)\nx = Dense(16, activation=\"relu\")(x)\nout = Dense(1, activation=\"linear\")(x)\n\n# Freezing DenseNet121 backbone\nfor layer in backbone.layers:\n    layer.trainable = False\n\nregression_model_dropout_64_16 = Model(inp,out)\n\noptimizer = Adam(lr=0.001)\nmetrics = [\"mean_absolute_error\"]\nloss = \"mean_squared_error\"\n    \nregression_model_dropout_64_16.compile(loss=loss, metrics=metrics, optimizer=optimizer)","metadata":{"execution":{"iopub.status.busy":"2021-06-16T13:47:44.323639Z","iopub.execute_input":"2021-06-16T13:47:44.324015Z","iopub.status.idle":"2021-06-16T13:47:46.751286Z","shell.execute_reply.started":"2021-06-16T13:47:44.323985Z","shell.execute_reply":"2021-06-16T13:47:46.750521Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"epochs = 30\nbatch_size = 32\n\nregression_dropout_data_aug_training_results = pd.DataFrame([], columns=[\"loss\",\n                        \"mean_absolute_error\", \"val_loss\",\n                        \"val_mean_absolute_error\"])\n\nearly_stopping = EarlyStopping(monitor='val_loss', restore_best_weights=True, patience=5)\nmodel_checkpoint = ModelCheckpoint(\n    'DenseNet121_regression__dropout_64-16__data-aug__64-16_{epoch:02d}-epochs_val_loss-{val_loss:02f}.h5',\n    monitor='val_loss', save_best_only=True)\n\nregression_dropout_data_aug_history = regression_model_dropout_64_16.fit(\n                    x=regression_adoptionspeed_train_iterator_da,\n                    validation_data=regression_adoptionspeed_val_iterator,\n                    batch_size=batch_size, epochs=epochs,\n                    callbacks=[model_checkpoint, early_stopping])\n\ntrain_loss = regression_dropout_data_aug_history.history[\"loss\"]\ntrain_mae = regression_dropout_data_aug_history.history[\"mean_absolute_error\"]\nvalidation_loss = regression_dropout_data_aug_history.history[\"val_loss\"]\nvalidation_mae = regression_dropout_data_aug_history.history[\"val_mean_absolute_error\"]\n\nfor i in range(len(train_loss)):\n    regression_dropout_data_aug_training_results = regression_dropout_data_aug_training_results.append({\n        \"loss\": train_loss[i],\n        \"mean_absolute_error\": train_mae[i],\n        \"val_loss\": validation_loss[i],\n        \"val_mean_absolute_error\": validation_mae[i]\n    }, ignore_index=True)\n\n# regression_model_dropout_64_16.save(f\"DenseNet121_regression__dropout_64-16__data-aug__64-16_{epochs}-epochs.h5\")\nregression_dropout_data_aug_training_results.to_csv(f\"DenseNet121_regression__dropout_64-16__data-aug__64-16_{epochs}-epochs.csv\", index=False)\nto_latex(regression_dropout_data_aug_training_results, f\"DenseNet121_regression__dropout_64-16__data-aug__64-16_{epochs}-epochs\")\n\n# Zipping the checkpoints\n!mkdir regression_dropout_data_aug\n!mv ./DenseNet121_regression__dropout_64-16__data-aug*.h5 regression_dropout_data_aug\nshutil.make_archive(\"DenseNet121_regression__dropout_64-16__data-aug__checkpoints\", \"zip\", \"./regression_dropout_data_aug\")","metadata":{"execution":{"iopub.status.busy":"2021-06-16T13:49:18.167428Z","iopub.execute_input":"2021-06-16T13:49:18.167765Z","iopub.status.idle":"2021-06-16T15:47:12.38818Z","shell.execute_reply.started":"2021-06-16T13:49:18.167734Z","shell.execute_reply":"2021-06-16T15:47:12.387261Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"regression_dropout_data_aug_training_results = pd.read_csv(\"../input/tfg-pet-adoption-data/DenseNet121_regression__dropout_64-16__data-aug__64-16_30-epochs.csv\")\nplot_history(regression_dropout_data_aug_training_results, [\"loss\", \"mean_absolute_error\"], 1, 2, (12,3))","metadata":{"execution":{"iopub.status.busy":"2021-06-22T20:01:13.404972Z","iopub.execute_input":"2021-06-22T20:01:13.405282Z","iopub.status.idle":"2021-06-22T20:01:13.676333Z","shell.execute_reply.started":"2021-06-22T20:01:13.405253Z","shell.execute_reply":"2021-06-22T20:01:13.675184Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"regression_model_dropout_64_16_6e = load_model(\"../input/tfg-pet-adoption-data/DenseNet121_regression__dropout_64-16__data-aug__64-16_06-epochs_val_loss-1.171635.h5\")","metadata":{"execution":{"iopub.status.busy":"2021-06-13T02:21:35.285652Z","iopub.execute_input":"2021-06-13T02:21:35.286015Z","iopub.status.idle":"2021-06-13T02:21:39.247663Z","shell.execute_reply.started":"2021-06-13T02:21:35.285984Z","shell.execute_reply":"2021-06-13T02:21:39.246622Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = Model(inputs=regression_model_dropout_64_16_6e.input, outputs=regression_model_dropout_64_16_6e.layers[-4].output)\nife = ImageFeatureExtractor(construct_from_cnn_backbone=False, model=model,\n                 images_directory=\"../input/petfinder-adoption-prediction/train_images\",\n                 from_image=True, preprocess_input=preprocess_input_densenet, img_size=256,\n                 model_name=f\"DenseNet121_regression__dropout_64-16__data-aug__layer-64_6-epochs\",\n                 save=True, debug=True, include_feats=False)\n\n_ = ife.fit_transform(X, y)","metadata":{"execution":{"iopub.status.busy":"2021-06-16T15:51:03.449792Z","iopub.execute_input":"2021-06-16T15:51:03.450112Z","iopub.status.idle":"2021-06-16T15:53:43.618591Z","shell.execute_reply.started":"2021-06-16T15:51:03.450083Z","shell.execute_reply":"2021-06-16T15:53:43.617706Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"regression_model_dropout_64_16_data_aug_layer_64_features_6e = pd.read_csv(\n    \"../input/tfg-pet-adoption-data/image_features_DenseNet121_regression__dropout_64-16__data-aug__layer-64_6-epochs_in-256.csv\",\n    index_col=0)\ndisplay(regression_model_dropout_64_16_data_aug_layer_64_features_6e.head(5))\nregression_model_dropout_64_16_data_aug_layer_64_features_6e.describe().loc[\"std\",:].value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-06-22T20:01:13.677802Z","iopub.execute_input":"2021-06-22T20:01:13.678256Z","iopub.status.idle":"2021-06-22T20:01:14.019465Z","shell.execute_reply.started":"2021-06-22T20:01:13.678213Z","shell.execute_reply":"2021-06-22T20:01:14.0182Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = Model(inputs=regression_model_dropout_64_16_6e.input, outputs=regression_model_dropout_64_16_6e.layers[-2].output)\nife = ImageFeatureExtractor(construct_from_cnn_backbone=False, model=model,\n                 images_directory=\"../input/petfinder-adoption-prediction/train_images\",\n                 from_image=True, preprocess_input=preprocess_input_densenet, img_size=256,\n                 model_name=f\"DenseNet121_regression__dropout_64-16__data-aug__layer-16_6-epochs\",\n                 save=True, debug=True, include_feats=False)\n\n_ = ife.fit_transform(X, y)","metadata":{"execution":{"iopub.status.busy":"2021-06-16T15:56:35.323249Z","iopub.execute_input":"2021-06-16T15:56:35.323569Z","iopub.status.idle":"2021-06-16T15:59:00.370387Z","shell.execute_reply.started":"2021-06-16T15:56:35.323539Z","shell.execute_reply":"2021-06-16T15:59:00.369494Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"regression_model_dropout_64_16_data_aug_layer_16_features_6e = pd.read_csv(\n    \"../input/tfg-pet-adoption-data/image_features_DenseNet121_regression__dropout_64-16__data-aug__layer-16_6-epochs_in-256.csv\",\n    index_col=0)\ndisplay(regression_model_dropout_64_16_data_aug_layer_16_features_6e.head(5))\nregression_model_dropout_64_16_data_aug_layer_16_features_6e.describe().loc[\"std\",:].value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-06-22T20:01:14.020448Z","iopub.execute_input":"2021-06-22T20:01:14.0207Z","iopub.status.idle":"2021-06-22T20:01:14.162585Z","shell.execute_reply.started":"2021-06-22T20:01:14.020675Z","shell.execute_reply":"2021-06-22T20:01:14.161424Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Now, let's unfreeze the last convolutional block of DenseNet121 backbone and compile the model with a smaller learning rate:**","metadata":{}},{"cell_type":"code","source":"for i, layer in enumerate(regression_model_dropout_64_16_6e.layers[375:], start=375):\n    print(i, layer.name)","metadata":{"execution":{"iopub.status.busy":"2021-06-16T15:59:54.452753Z","iopub.execute_input":"2021-06-16T15:59:54.453077Z","iopub.status.idle":"2021-06-16T15:59:54.463342Z","shell.execute_reply.started":"2021-06-16T15:59:54.45305Z","shell.execute_reply":"2021-06-16T15:59:54.462153Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for layer in regression_model_dropout_64_16_6e.layers[:418]:\n    layer.trainable = False\nfor layer in regression_model_dropout_64_16_6e.layers[418:]:\n    layer.trainable = True\n\noptimizer = Adam(lr=0.0001)\nmetrics = [\"mean_absolute_error\"]\nloss = \"mean_squared_error\"\n    \nregression_model_dropout_64_16_6e.compile(loss=loss, metrics=metrics, optimizer=optimizer)","metadata":{"execution":{"iopub.status.busy":"2021-06-16T16:00:34.176562Z","iopub.execute_input":"2021-06-16T16:00:34.17693Z","iopub.status.idle":"2021-06-16T16:00:34.212394Z","shell.execute_reply.started":"2021-06-16T16:00:34.1769Z","shell.execute_reply":"2021-06-16T16:00:34.21155Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"epochs = 15\nbatch_size = 32\n\nregression_fine_tune_dropout_data_aug_training_results = pd.DataFrame([], columns=[\"loss\",\n                        \"mean_absolute_error\", \"val_loss\",\n                        \"val_mean_absolute_error\"])\n\nearly_stopping = EarlyStopping(monitor='val_loss', restore_best_weights=True, patience=4)\nmodel_checkpoint = ModelCheckpoint(\n    'DenseNet121_regression__fine-tune__dropout_64-16__data-aug__64-16_{epoch:02d}-epochs_val_loss-{val_loss:02f}.h5',\n    monitor='val_loss', save_best_only=True)\n\nregression_fine_tune_dropout_data_aug_history = regression_model_dropout_64_16_6e.fit(\n                    x=regression_adoptionspeed_train_iterator_da,\n                    validation_data=regression_adoptionspeed_val_iterator,\n                    batch_size=batch_size, epochs=epochs,\n                    callbacks=[model_checkpoint, early_stopping])\n\ntrain_loss = regression_fine_tune_dropout_data_aug_history.history[\"loss\"]\ntrain_mae = regression_fine_tune_dropout_data_aug_history.history[\"mean_absolute_error\"]\nvalidation_loss = regression_fine_tune_dropout_data_aug_history.history[\"val_loss\"]\nvalidation_mae = regression_fine_tune_dropout_data_aug_history.history[\"val_mean_absolute_error\"]\n\nfor i in range(len(train_loss)):\n    regression_fine_tune_dropout_data_aug_training_results = regression_fine_tune_dropout_data_aug_training_results.append({\n        \"loss\": train_loss[i],\n        \"mean_absolute_error\": train_mae[i],\n        \"val_loss\": validation_loss[i],\n        \"val_mean_absolute_error\": validation_mae[i]\n    }, ignore_index=True)\n\nregression_fine_tune_dropout_data_aug_training_results.to_csv(f\"DenseNet121_regression__fine-tune__dropout_64-16__data-aug__64-16_{epochs}-epochs.csv\", index=False)\nto_latex(regression_fine_tune_dropout_data_aug_training_results, f\"DenseNet121_regression__fine-tune__dropout_64-16__data-aug__64-16_{epochs}-epochs\")\n\n# Zipping the checkpoints\n!mkdir regression_fine_tune_dropout_data_aug\n!mv ./DenseNet121_regression__fine-tune__dropout_64-16__data-aug*.h5 regression_fine_tune_dropout_data_aug\nshutil.make_archive(\"DenseNet121_regression__fine-tune__dropout_64-16__data-aug__checkpoints\", \"zip\", \"./regression_fine_tune_dropout_data_aug\")","metadata":{"execution":{"iopub.status.busy":"2021-06-16T16:04:47.000887Z","iopub.execute_input":"2021-06-16T16:04:47.00123Z","iopub.status.idle":"2021-06-16T17:41:55.922068Z","shell.execute_reply.started":"2021-06-16T16:04:47.001196Z","shell.execute_reply":"2021-06-16T17:41:55.920991Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"regression_fine_tune_dropout_data_aug_training_results = pd.read_csv(\"../input/tfg-pet-adoption-data/DenseNet121_regression__fine-tune__dropout_64-16__data-aug__64-16_15-epochs.csv\")\nplot_history(regression_fine_tune_dropout_data_aug_training_results, [\"loss\", \"mean_absolute_error\"], 1, 2, (12,3))","metadata":{"execution":{"iopub.status.busy":"2021-06-22T20:01:14.164233Z","iopub.execute_input":"2021-06-22T20:01:14.164637Z","iopub.status.idle":"2021-06-22T20:01:14.482212Z","shell.execute_reply.started":"2021-06-22T20:01:14.164593Z","shell.execute_reply":"2021-06-22T20:01:14.481048Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"regression_model_dropout_64_16_5e = load_model(\"../input/tfg-pet-adoption-data/DenseNet121_regression__fine-tune__dropout_64-16__data-aug__64-16_05-epochs_val_loss-1.150202.h5\")","metadata":{"execution":{"iopub.status.busy":"2021-06-13T19:08:29.081084Z","iopub.execute_input":"2021-06-13T19:08:29.081515Z","iopub.status.idle":"2021-06-13T19:08:34.037294Z","shell.execute_reply.started":"2021-06-13T19:08:29.081478Z","shell.execute_reply":"2021-06-13T19:08:34.036155Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = Model(inputs=regression_model_dropout_64_16_5e.input, outputs=regression_model_dropout_64_16_5e.layers[-4].output)\nife = ImageFeatureExtractor(construct_from_cnn_backbone=False, model=model,\n                 images_directory=\"../input/petfinder-adoption-prediction/train_images\",\n                 from_image=True, preprocess_input=preprocess_input_densenet, img_size=256,\n                 model_name=f\"DenseNet121_regression__fine-tune__dropout_64-16__data-aug__layer-64_5-epochs\",\n                 save=True, debug=True, include_feats=False)\n\n_ = ife.fit_transform(X, y)","metadata":{"execution":{"iopub.status.busy":"2021-06-16T17:48:22.05583Z","iopub.execute_input":"2021-06-16T17:48:22.056182Z","iopub.status.idle":"2021-06-16T17:50:48.610157Z","shell.execute_reply.started":"2021-06-16T17:48:22.056143Z","shell.execute_reply":"2021-06-16T17:50:48.609191Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"regression_model_fine_tune_dropout_64_16_data_aug_layer_64_features_5e = pd.read_csv(\n    \"../input/tfg-pet-adoption-data/image_features_DenseNet121_regression__fine-tune__dropout_64-16__data-aug__layer-64_5-epochs_in-256.csv\",\n    index_col=0)\ndisplay(regression_model_fine_tune_dropout_64_16_data_aug_layer_64_features_5e.head(5))\nregression_model_fine_tune_dropout_64_16_data_aug_layer_64_features_5e.describe().loc[\"std\",:].value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-06-22T20:01:14.484093Z","iopub.execute_input":"2021-06-22T20:01:14.484536Z","iopub.status.idle":"2021-06-22T20:01:14.821892Z","shell.execute_reply.started":"2021-06-22T20:01:14.48449Z","shell.execute_reply":"2021-06-22T20:01:14.821138Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = Model(inputs=regression_model_dropout_64_16_5e.input, outputs=regression_model_dropout_64_16_5e.layers[-2].output)\nife = ImageFeatureExtractor(construct_from_cnn_backbone=False, model=model,\n                 images_directory=\"../input/petfinder-adoption-prediction/train_images\",\n                 from_image=True, preprocess_input=preprocess_input_densenet, img_size=256,\n                 model_name=f\"DenseNet121_regression__fine-tune__dropout_64-16__data-aug__layer-16_5-epochs\",\n                 save=True, debug=True, include_feats=False)\n\n_ = ife.fit_transform(X, y)","metadata":{"execution":{"iopub.status.busy":"2021-06-16T17:50:48.871054Z","iopub.execute_input":"2021-06-16T17:50:48.871428Z","iopub.status.idle":"2021-06-16T17:53:15.801377Z","shell.execute_reply.started":"2021-06-16T17:50:48.871388Z","shell.execute_reply":"2021-06-16T17:53:15.800408Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"regression_model_fine_tune_dropout_64_16_data_aug_layer_16_features_5e = pd.read_csv(\n    \"../input/tfg-pet-adoption-data/image_features_DenseNet121_regression__fine-tune__dropout_64-16__data-aug__layer-16_5-epochs_in-256.csv\",\n    index_col=0)\ndisplay(regression_model_fine_tune_dropout_64_16_data_aug_layer_16_features_5e.head(5))\nregression_model_fine_tune_dropout_64_16_data_aug_layer_16_features_5e.describe().loc[\"std\",:].value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-06-22T20:01:14.822871Z","iopub.execute_input":"2021-06-22T20:01:14.823117Z","iopub.status.idle":"2021-06-22T20:01:14.974961Z","shell.execute_reply.started":"2021-06-22T20:01:14.823093Z","shell.execute_reply":"2021-06-22T20:01:14.974119Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pipeline_transformers = [\n    ('replace_breeds', LeftJoinReplace(values_dict=breeds_dict,\n                                      variables=[\"Breed1\", \"Breed2\"])),\n    ('replace_colors', LeftJoinReplace(values_dict=colors_dict,\n                                      variables=[\"Color1\", \"Color2\", \"Color3\"])),\n    ('replace_states', LeftJoinReplace(values_dict=states_dict,\n                                     variables=[\"State\"])),\n    ('replace_by_strings', FunctionTransformer(func=replace_integers_by_strings)),\n    ('has_name', FunctionTransformer(func=has_significant_name)),\n    ('pure_breed', FunctionTransformer(func=has_pure_breed)),\n    ('breed_matches_fur_length', FunctionTransformer(func=breed_matches_fur_length)),\n    ('impute_breed', BreedImputer()),\n    ('include_prof_im_metadata', IncludeProfileImageMetadata(profile_image_metadata)),\n    ('correct_wrong_type', CorrectWrongType(breeds)),\n    ('encode_breed', BreedEncoding(enc_type=\"target_and_frequency\")),\n    ('ordinal_vars_encoder', OrdinalVariableEncoder(columns=[\"MaturitySize\", \"FurLength\", \"Health\"],\n                                enc_type=\"ordinal\", mapping=ordinal_vars_mapping)),\n    ('state_gdp', ReplaceState(gdp_per_capita=gdp_per_capita, impute_nan_value=46450)),\n    ('rescuer_count', ReplaceRescuerID()),\n    ('discretizer', CustomDiscretizer(bins_age=-1, quantity=False, fee=False, video_amt=False,\n                                     photo_amt=False)),\n    ('description_length', FunctionTransformer(func=include_description_length)),\n    ('include_desc_metadata', IncludeDescriptionMetadata(description_metadata=description_metadata)),\n    ('correct_desc_language', CorrectDescriptionLanguage()),\n    ('one_hot_encoder', CustomOneHotEncoder(columns=[\"Gender\", \"Color1\", \"Color2\",\n                            \"Color3\", \"Vaccinated\", \"Dewormed\", \"Sterilized\", \"DescriptionLanguage\"])),\n    ('include_prof_im_properties', IncludeProfileImageProperties(profile_image_properties)),\n    ('drop_columns', ColumnRemover(columns=columns_to_be_removed_image_feats_eval)),\n    ('round_im_dims_aspect_ratio', FunctionTransformer(func=include_aspect_ratio)),\n    None,\n    ('drop_petid', ColumnRemover(columns=[\"PetID\"])),\n    ('useless_vars_remover', UselessVariablesRemover(tolerance=0.000001)),\n    ('impute_malay_desc_missing_prof_im_props', CustomIterativeImputer()),\n    ('custom_standard_scaler', CustomStandardScaler(numeric_columns_image_feats_eval))\n]\n\nevaluation_results = pd.DataFrame([], columns=[\"Model description\",\n                        \"Average fit time\", \"Average accuracy\", \"Average QWK\",\n                        \"Single split accuracy\", \"Single split QWK\"])\n\nimage_features_dataframes = {\n    \"AdoptionSpeed, regression_model_dropout-64-16_data-aug_layer-64_6e\": regression_model_dropout_64_16_data_aug_layer_64_features_6e,\n    \"AdoptionSpeed, regression_model_dropout-64-16_data-aug_layer-16_6e\": regression_model_dropout_64_16_data_aug_layer_16_features_6e,\n    \"AdoptionSpeed, regression_model_fine-tune_dropout-64-16_data-aug_layer-64_5e\": regression_model_fine_tune_dropout_64_16_data_aug_layer_64_features_5e,\n    \"AdoptionSpeed, regression_model_fine-tune_dropout-64-16_data-aug_layer-16_5e\": regression_model_fine_tune_dropout_64_16_data_aug_layer_16_features_5e\n}\n\nfor features_description, loaded_features in image_features_dataframes.items():\n    ife = ImageFeatureExtractor(construct_from_cnn_backbone=False,\n                                loaded_features=loaded_features)\n    pipeline_transformers[-5] = ('image_features_extractor', ife)\n\n    model = xgb.XGBClassifier(eval_metric='mlogloss', random_state=seed, n_jobs=-1,\n                          use_label_encoder=False)\n    model_description = f\"XGBClassifier, DenseNet121, {features_description}\"\n    print(f\"\\n\\n*************** {model_description} ***************\")\n    avg_fit_time, avg_accuracy, avg_QWK = evaluate_model(\n                Pipeline(steps=pipeline_transformers + [('model', model)]),\n                cv, X, y, model_type=\"classification\", display_results=True,\n                display_plots=False)\n\n    single_accuracy, single_QWK = evaluate_model_single_split(\n                Pipeline(steps=pipeline_transformers + [('model', model)]),\n                X_train_CNN, X_val_CNN, y_train_CNN, y_val_CNN,\n                model_type=\"classification\", display_results=True)\n\n    evaluation_results = evaluation_results.append({\n        \"Model description\": model_description,\n        \"Average fit time\": avg_fit_time,\n        \"Average accuracy\": avg_accuracy,\n        \"Average QWK\": avg_QWK,\n        \"Single split accuracy\": single_accuracy,\n        \"Single split QWK\": single_QWK\n    }, ignore_index=True)\n    \n\npd.set_option('display.max_colwidth', None)\ndisplay(evaluation_results)\npd.set_option('display.max_colwidth', 50)\nto_latex(evaluation_results, \"evaluation_results_XGBClassifier_DenseNet121_regression__fine-tune__data-aug__dropout\")","metadata":{"execution":{"iopub.status.busy":"2021-06-22T20:01:14.976879Z","iopub.execute_input":"2021-06-22T20:01:14.977254Z","iopub.status.idle":"2021-06-22T20:09:48.277646Z","shell.execute_reply.started":"2021-06-22T20:01:14.977219Z","shell.execute_reply":"2021-06-22T20:09:48.276623Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Aggregation of several images features?","metadata":{}},{"cell_type":"markdown","source":"Let's try to include the information from all the images of each profile using their extracted features:","metadata":{}},{"cell_type":"markdown","source":"#### Raw features (1024) + SVD 16","metadata":{}},{"cell_type":"code","source":"ife = ImageFeatureExtractor(construct_from_cnn_backbone=True, cnn_backbone=DenseNet121,\n                 images_directory=\"../input/petfinder-adoption-prediction/train_images\",\n                 preprocess_input=preprocess_input_densenet, img_size=256, average=False,\n                 save=True, debug=True, include_feats=False, multiple_instances_per_petid=True)\n\n_ = ife.fit_transform(X, y)","metadata":{"execution":{"iopub.status.busy":"2021-06-12T01:38:06.756368Z","iopub.execute_input":"2021-06-12T01:38:06.756704Z","iopub.status.idle":"2021-06-12T01:52:06.03354Z","shell.execute_reply.started":"2021-06-12T01:38:06.756673Z","shell.execute_reply":"2021-06-12T01:52:06.032611Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image_features_densenet121 = pd.read_csv(\n    \"../input/tfg-pet-adoption-data/image_features_DenseNet121_in-256.csv\",\n    index_col=0)\ndisplay(image_features_densenet121.head(5))","metadata":{"execution":{"iopub.status.busy":"2021-06-22T21:27:14.057234Z","iopub.execute_input":"2021-06-22T21:27:14.057847Z","iopub.status.idle":"2021-06-22T21:27:17.891719Z","shell.execute_reply.started":"2021-06-22T21:27:14.057809Z","shell.execute_reply":"2021-06-22T21:27:17.89072Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_image_features_densenet121 = pd.read_csv(\n    \"../input/tfg-pet-adoption-data/ALL_image_features_DenseNet121_in-256.csv\",\n    index_col=0)\ndisplay(all_image_features_densenet121.head(5))","metadata":{"execution":{"iopub.status.busy":"2021-06-22T21:27:17.893573Z","iopub.execute_input":"2021-06-22T21:27:17.893845Z","iopub.status.idle":"2021-06-22T21:27:33.07917Z","shell.execute_reply.started":"2021-06-22T21:27:17.893817Z","shell.execute_reply":"2021-06-22T21:27:33.078082Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"aggregated_image_features_densenet121 = pd.read_csv(\n    \"../input/tfg-pet-adoption-data/AGGREGATED_image_features_DenseNet121_in-256.csv\",\n    index_col=0)\ndisplay(aggregated_image_features_densenet121.head(5))","metadata":{"execution":{"iopub.status.busy":"2021-06-22T21:27:33.080797Z","iopub.execute_input":"2021-06-22T21:27:33.081118Z","iopub.status.idle":"2021-06-22T21:27:45.588927Z","shell.execute_reply.started":"2021-06-22T21:27:33.081083Z","shell.execute_reply":"2021-06-22T21:27:45.58784Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pipeline_transformers = [\n    ('replace_breeds', LeftJoinReplace(values_dict=breeds_dict,\n                                      variables=[\"Breed1\", \"Breed2\"])),\n    ('replace_colors', LeftJoinReplace(values_dict=colors_dict,\n                                      variables=[\"Color1\", \"Color2\", \"Color3\"])),\n    ('replace_states', LeftJoinReplace(values_dict=states_dict,\n                                     variables=[\"State\"])),\n    ('replace_by_strings', FunctionTransformer(func=replace_integers_by_strings)),\n    ('has_name', FunctionTransformer(func=has_significant_name)),\n    ('pure_breed', FunctionTransformer(func=has_pure_breed)),\n    ('breed_matches_fur_length', FunctionTransformer(func=breed_matches_fur_length)),\n    ('impute_breed', BreedImputer()),\n    ('include_prof_im_metadata', IncludeProfileImageMetadata(profile_image_metadata)),\n    ('correct_wrong_type', CorrectWrongType(breeds)),\n    ('encode_breed', BreedEncoding(enc_type=\"target_and_frequency\")),\n    ('ordinal_vars_encoder', OrdinalVariableEncoder(columns=[\"MaturitySize\", \"FurLength\", \"Health\"],\n                                enc_type=\"ordinal\", mapping=ordinal_vars_mapping)),\n    ('state_gdp', ReplaceState(gdp_per_capita=gdp_per_capita, impute_nan_value=46450)),\n    ('rescuer_count', ReplaceRescuerID()),\n    ('discretizer', CustomDiscretizer(bins_age=-1, quantity=False, fee=False, video_amt=False,\n                                     photo_amt=False)),\n    ('description_length', FunctionTransformer(func=include_description_length)),\n    ('include_desc_metadata', IncludeDescriptionMetadata(description_metadata=description_metadata)),\n    ('correct_desc_language', CorrectDescriptionLanguage()),\n    ('one_hot_encoder', CustomOneHotEncoder(columns=[\"Gender\", \"Color1\", \"Color2\",\n                            \"Color3\", \"Vaccinated\", \"Dewormed\", \"Sterilized\", \"DescriptionLanguage\"])),\n    ('include_prof_im_properties', IncludeProfileImageProperties(profile_image_properties)),\n    ('drop_columns', ColumnRemover(columns=columns_to_be_removed_image_feats_eval)),\n    ('round_im_dims_aspect_ratio', FunctionTransformer(func=include_aspect_ratio)),\n    None,\n    ('drop_petid', ColumnRemover(columns=[\"PetID\"])),\n    ('useless_vars_remover', UselessVariablesRemover(tolerance=0.000001)),\n    ('impute_malay_desc_missing_prof_im_props', CustomIterativeImputer()),\n    ('custom_standard_scaler', CustomStandardScaler(numeric_columns_image_feats_eval))\n]\n\nevaluation_results = pd.DataFrame([], columns=[\"Model description\",\n                        \"Average fit time\", \"Average accuracy\", \"Average QWK\",\n                        \"Single split accuracy\", \"Single split QWK\"])\n\nimage_features_dataframes = {\n    \"raw-1024_single\": image_features_densenet121,\n    \"raw-1024_all\": all_image_features_densenet121,\n    \"raw-1024_agg_mean_sum_var\": aggregated_image_features_densenet121\n}\n\nfor features_description, loaded_features in image_features_dataframes.items():\n    for svd_n_components in [4, 16]:\n        ife = ImageFeatureExtractor(construct_from_cnn_backbone=False,\n                                    loaded_features=loaded_features,\n                                    svd_n_components=svd_n_components,\n                                    multiple_instances_per_petid=\"all\" in features_description)\n        pipeline_transformers[-5] = ('image_features_extractor', ife)\n\n        model = xgb.XGBClassifier(eval_metric='mlogloss', random_state=seed, n_jobs=-1,\n                              use_label_encoder=False)\n        model_description = f\"XGBClassifier, DenseNet121, {features_description}, SVD {svd_n_components}\"\n        print(f\"\\n\\n*************** {model_description} ***************\")\n        avg_fit_time, avg_accuracy, avg_QWK = evaluate_model(\n                    Pipeline(steps=pipeline_transformers + [('model', model)]),\n                    cv, X, y, model_type=\"classification\", display_results=True,\n                    display_plots=False)\n\n        single_accuracy, single_QWK = evaluate_model_single_split(\n                    Pipeline(steps=pipeline_transformers + [('model', model)]),\n                    X_train_CNN, X_val_CNN, y_train_CNN, y_val_CNN,\n                    model_type=\"classification\", display_results=True)\n\n        evaluation_results = evaluation_results.append({\n            \"Model description\": model_description,\n            \"Average fit time\": avg_fit_time,\n            \"Average accuracy\": avg_accuracy,\n            \"Average QWK\": avg_QWK,\n            \"Single split accuracy\": single_accuracy,\n            \"Single split QWK\": single_QWK\n        }, ignore_index=True)\n    \n\npd.set_option('display.max_colwidth', None)\ndisplay(evaluation_results)\npd.set_option('display.max_colwidth', 50)\nto_latex(evaluation_results, \"evaluation_results_XGBClassifier_DenseNet121_raw-1024_single_and_aggregation_multiple_images_svd-4\")","metadata":{"execution":{"iopub.status.busy":"2021-06-22T21:34:09.313764Z","iopub.execute_input":"2021-06-22T21:34:09.314204Z","iopub.status.idle":"2021-06-22T21:58:57.656209Z","shell.execute_reply.started":"2021-06-22T21:34:09.314146Z","shell.execute_reply":"2021-06-22T21:58:57.655358Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del image_features_densenet121,\ndel all_image_features_densenet121,\ndel aggregated_image_features_densenet121","metadata":{"execution":{"iopub.status.busy":"2021-06-22T21:58:57.657659Z","iopub.execute_input":"2021-06-22T21:58:57.657914Z","iopub.status.idle":"2021-06-22T21:58:57.66311Z","shell.execute_reply.started":"2021-06-22T21:58:57.657889Z","shell.execute_reply":"2021-06-22T21:58:57.662112Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Fine-tuned model (dropout 64-16)","metadata":{}},{"cell_type":"code","source":"regression_model_dropout_64_16_fine_tune = load_model(\"../input/tfg-pet-adoption-data/DenseNet121_regression__fine-tune__dropout_64-16__data-aug__64-16_05-epochs_val_loss-1.150202.h5\")","metadata":{"execution":{"iopub.status.busy":"2021-06-21T14:00:20.336816Z","iopub.execute_input":"2021-06-21T14:00:20.339052Z","iopub.status.idle":"2021-06-21T14:00:24.246857Z","shell.execute_reply.started":"2021-06-21T14:00:20.339009Z","shell.execute_reply":"2021-06-21T14:00:24.245872Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = Model(inputs=regression_model_dropout_64_16_fine_tune.input,\n              outputs=regression_model_dropout_64_16_fine_tune.layers[-4].output)\nife = ImageFeatureExtractor(construct_from_cnn_backbone=False, model=model,\n                 images_directory=\"../input/petfinder-adoption-prediction/train_images\",\n                 from_image=True, preprocess_input=preprocess_input_densenet, img_size=256,\n                 model_name=f\"DenseNet121_regression__fine-tune__dropout_64-16__data-aug__64-16__layer-64_5-epochs\",\n                 save=True, debug=True, include_feats=False, multiple_instances_per_petid=True)\n\n_ = ife.fit_transform(X, y)","metadata":{"execution":{"iopub.status.busy":"2021-06-17T17:40:13.700519Z","iopub.execute_input":"2021-06-17T17:40:13.700905Z","iopub.status.idle":"2021-06-17T19:38:58.622009Z","shell.execute_reply.started":"2021-06-17T17:40:13.700873Z","shell.execute_reply":"2021-06-17T19:38:58.621097Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_image_features_regression_model_dropout_64_16_fine_tune_layer_64 = pd.read_csv(\n    \"../input/tfg-pet-adoption-data/ALL_image_features_DenseNet121_regression__fine-tune__dropout_64-16__data-aug__64-16__layer-64_5-epochs_in-256.csv\",\n    index_col=0)\ndisplay(all_image_features_regression_model_dropout_64_16_fine_tune_layer_64.head(5))","metadata":{"execution":{"iopub.status.busy":"2021-06-22T20:25:45.178769Z","iopub.execute_input":"2021-06-22T20:25:45.179049Z","iopub.status.idle":"2021-06-22T20:25:45.830175Z","shell.execute_reply.started":"2021-06-22T20:25:45.179008Z","shell.execute_reply":"2021-06-22T20:25:45.829136Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"aggregated_image_features_regression_model_dropout_64_16_fine_tune_layer_64 = pd.read_csv(\n    \"../input/tfg-pet-adoption-data/AGGREGATED_image_features_DenseNet121_regression__fine-tune__dropout_64-16__data-aug__64-16__layer-64_5-epochs_in-256.csv\",\n    index_col=0)\ndisplay(aggregated_image_features_regression_model_dropout_64_16_fine_tune_layer_64.head(5))","metadata":{"execution":{"iopub.status.busy":"2021-06-22T20:25:45.833061Z","iopub.execute_input":"2021-06-22T20:25:45.833334Z","iopub.status.idle":"2021-06-22T20:25:46.348148Z","shell.execute_reply.started":"2021-06-22T20:25:45.833307Z","shell.execute_reply":"2021-06-22T20:25:46.347141Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = Model(inputs=regression_model_dropout_64_16_fine_tune.input,\n              outputs=regression_model_dropout_64_16_fine_tune.layers[-2].output)\nife = ImageFeatureExtractor(construct_from_cnn_backbone=False, model=model,\n                 images_directory=\"../input/petfinder-adoption-prediction/train_images\",\n                 from_image=True, preprocess_input=preprocess_input_densenet, img_size=256,\n                 model_name=f\"DenseNet121_regression__fine-tune__dropout_64-16__data-aug__64-16__layer-16_5-epochs\",\n                 save=True, debug=True, include_feats=False, multiple_instances_per_petid=True)\n\n_ = ife.fit_transform(X, y)","metadata":{"execution":{"iopub.status.busy":"2021-06-17T17:40:13.700519Z","iopub.execute_input":"2021-06-17T17:40:13.700905Z","iopub.status.idle":"2021-06-17T19:38:58.622009Z","shell.execute_reply.started":"2021-06-17T17:40:13.700873Z","shell.execute_reply":"2021-06-17T19:38:58.621097Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_image_features_regression_model_dropout_64_16_fine_tune_layer_16 = pd.read_csv(\n    \"../input/tfg-pet-adoption-data/ALL_image_features_DenseNet121_regression__fine-tune__dropout_64-16__data-aug__64-16__layer-16_5-epochs_in-256.csv\",\n    index_col=0)\ndisplay(all_image_features_regression_model_dropout_64_16_fine_tune_layer_16.head(5))","metadata":{"execution":{"iopub.status.busy":"2021-06-22T22:01:23.851386Z","iopub.execute_input":"2021-06-22T22:01:23.852271Z","iopub.status.idle":"2021-06-22T22:01:24.226018Z","shell.execute_reply.started":"2021-06-22T22:01:23.852221Z","shell.execute_reply":"2021-06-22T22:01:24.225156Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"aggregated_image_features_regression_model_dropout_64_16_fine_tune_layer_16 = pd.read_csv(\n    \"../input/tfg-pet-adoption-data/AGGREGATED_image_features_DenseNet121_regression__fine-tune__dropout_64-16__data-aug__64-16__layer-16_5-epochs_in-256.csv\",\n    index_col=0)\ndisplay(aggregated_image_features_regression_model_dropout_64_16_fine_tune_layer_16.head(5))","metadata":{"execution":{"iopub.status.busy":"2021-06-22T22:01:25.234452Z","iopub.execute_input":"2021-06-22T22:01:25.235198Z","iopub.status.idle":"2021-06-22T22:01:25.554215Z","shell.execute_reply.started":"2021-06-22T22:01:25.235128Z","shell.execute_reply":"2021-06-22T22:01:25.552808Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pipeline_transformers = [\n    ('replace_breeds', LeftJoinReplace(values_dict=breeds_dict,\n                                      variables=[\"Breed1\", \"Breed2\"])),\n    ('replace_colors', LeftJoinReplace(values_dict=colors_dict,\n                                      variables=[\"Color1\", \"Color2\", \"Color3\"])),\n    ('replace_states', LeftJoinReplace(values_dict=states_dict,\n                                     variables=[\"State\"])),\n    ('replace_by_strings', FunctionTransformer(func=replace_integers_by_strings)),\n    ('has_name', FunctionTransformer(func=has_significant_name)),\n    ('pure_breed', FunctionTransformer(func=has_pure_breed)),\n    ('breed_matches_fur_length', FunctionTransformer(func=breed_matches_fur_length)),\n    ('impute_breed', BreedImputer()),\n    ('include_prof_im_metadata', IncludeProfileImageMetadata(profile_image_metadata)),\n    ('correct_wrong_type', CorrectWrongType(breeds)),\n    ('encode_breed', BreedEncoding(enc_type=\"target_and_frequency\")),\n    ('ordinal_vars_encoder', OrdinalVariableEncoder(columns=[\"MaturitySize\", \"FurLength\", \"Health\"],\n                                enc_type=\"ordinal\", mapping=ordinal_vars_mapping)),\n    ('state_gdp', ReplaceState(gdp_per_capita=gdp_per_capita, impute_nan_value=46450)),\n    ('rescuer_count', ReplaceRescuerID()),\n    ('discretizer', CustomDiscretizer(bins_age=-1, quantity=False, fee=False, video_amt=False,\n                                     photo_amt=False)),\n    ('description_length', FunctionTransformer(func=include_description_length)),\n    ('include_desc_metadata', IncludeDescriptionMetadata(description_metadata=description_metadata)),\n    ('correct_desc_language', CorrectDescriptionLanguage()),\n    ('one_hot_encoder', CustomOneHotEncoder(columns=[\"Gender\", \"Color1\", \"Color2\",\n                            \"Color3\", \"Vaccinated\", \"Dewormed\", \"Sterilized\", \"DescriptionLanguage\"])),\n    ('include_prof_im_properties', IncludeProfileImageProperties(profile_image_properties)),\n    ('drop_columns', ColumnRemover(columns=columns_to_be_removed_image_feats_eval)),\n    ('round_im_dims_aspect_ratio', FunctionTransformer(func=include_aspect_ratio)),\n    None,\n    ('drop_petid', ColumnRemover(columns=[\"PetID\"])),\n    ('useless_vars_remover', UselessVariablesRemover(tolerance=0.000001)),\n    ('impute_malay_desc_missing_prof_im_props', CustomIterativeImputer()),\n    ('custom_standard_scaler', CustomStandardScaler(numeric_columns_image_feats_eval))\n]\n\nevaluation_results = pd.DataFrame([], columns=[\"Model description\",\n                        \"Average fit time\", \"Average accuracy\", \"Average QWK\",\n                        \"Single split accuracy\", \"Single split QWK\"])\n\nimage_features_dataframes = {\n    \"regression_model_fine-tune_dropout-64-16_data-aug_layer-64_single\": regression_model_fine_tune_dropout_64_16_data_aug_layer_64_features_5e,\n    \"regression_model_fine-tune_dropout-64-16_data-aug_layer-64_agg_mean_sum_var\": aggregated_image_features_regression_model_dropout_64_16_fine_tune_layer_64,\n    \"regression_model_fine-tune_dropout-64-16_data-aug_layer-16_single\": regression_model_fine_tune_dropout_64_16_data_aug_layer_16_features_5e,\n    \"regression_model_fine-tune_dropout-64-16_data-aug_layer-16_agg_mean_sum_var\": aggregated_image_features_regression_model_dropout_64_16_fine_tune_layer_16\n}\n\nfor features_description, loaded_features in image_features_dataframes.items():\n    ife = ImageFeatureExtractor(construct_from_cnn_backbone=False,\n                                loaded_features=loaded_features,\n                                multiple_instances_per_petid=False)\n    pipeline_transformers[-5] = ('image_features_extractor', ife)\n\n    model = xgb.XGBClassifier(eval_metric='mlogloss', random_state=seed, n_jobs=-1,\n                          use_label_encoder=False)\n    model_description = f\"XGBClassifier, DenseNet121, {features_description}\"\n    print(f\"\\n\\n*************** {model_description} ***************\")\n    avg_fit_time, avg_accuracy, avg_QWK = evaluate_model(\n                Pipeline(steps=pipeline_transformers + [('model', model)]),\n                cv, X, y, model_type=\"classification\", display_results=True,\n                display_plots=False)\n\n    single_accuracy, single_QWK = evaluate_model_single_split(\n                Pipeline(steps=pipeline_transformers + [('model', model)]),\n                X_train_CNN, X_val_CNN, y_train_CNN, y_val_CNN,\n                model_type=\"classification\", display_results=True)\n\n    evaluation_results = evaluation_results.append({\n        \"Model description\": model_description,\n        \"Average fit time\": avg_fit_time,\n        \"Average accuracy\": avg_accuracy,\n        \"Average QWK\": avg_QWK,\n        \"Single split accuracy\": single_accuracy,\n        \"Single split QWK\": single_QWK\n    }, ignore_index=True)\n    \n\npd.set_option('display.max_colwidth', None)\ndisplay(evaluation_results)\npd.set_option('display.max_colwidth', 50)\nto_latex(evaluation_results, \"evaluation_results_XGBClassifier_DenseNet121_regression_fine-tune_dropout_64-16_single_and_aggregation_multiple_images\")","metadata":{"execution":{"iopub.status.busy":"2021-06-22T22:01:37.683732Z","iopub.execute_input":"2021-06-22T22:01:37.684191Z","iopub.status.idle":"2021-06-22T22:12:21.132241Z","shell.execute_reply.started":"2021-06-22T22:01:37.684149Z","shell.execute_reply":"2021-06-22T22:12:21.131339Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Multi-input model","metadata":{}},{"cell_type":"markdown","source":"**https://datascience.stackexchange.com/questions/38111/classification-problem-with-many-images-per-instance**","metadata":{}},{"cell_type":"code","source":"inputs = []\ncnns_outputs = []\n\nfor i in range(1, 6):\n    inp = Input((256,256,3))\n    inputs.append(inp)\n    backbone = DenseNet121(input_tensor=inp, include_top=False, weights=\"imagenet\")\n    x = backbone.output\n    x = GlobalAveragePooling2D()(x)\n    cnns_outputs.append(x)\n    \n    # Freezing DenseNet121 backbone layers\n    for layer in backbone.layers:\n        layer._name = layer.name + f\"_{i}\"\n        layer.trainable = False\n\naverage = Average()(cnns_outputs)\nmaximum = Maximum()(cnns_outputs)\naddition = Add()(cnns_outputs)\n\nconcatenation = Concatenate()([average, maximum, addition])\nx = Dense(256, activation=\"relu\")(concatenation)\nx = Dropout(rate=0.25, seed=seed)(x)\nx = Dense(64, activation=\"relu\")(x)\nx = Dropout(rate=0.25, seed=seed)(x)\nx = Dense(16, activation=\"relu\")(x)\nout = Dense(1, activation=\"linear\")(x)\n \nregression_model_dropout_64_16_multi_input = Model(inputs=inputs, outputs=out)\n\noptimizer = Adam(lr=0.001)\nmetrics = [\"mean_absolute_error\"]\nloss = \"mean_squared_error\"\n    \nregression_model_dropout_64_16_multi_input.compile(loss=loss, metrics=metrics, optimizer=optimizer)","metadata":{"execution":{"iopub.status.busy":"2021-06-18T12:35:03.798492Z","iopub.execute_input":"2021-06-18T12:35:03.798845Z","iopub.status.idle":"2021-06-18T12:35:18.799799Z","shell.execute_reply.started":"2021-06-18T12:35:03.798809Z","shell.execute_reply":"2021-06-18T12:35:18.798871Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plot_model(regression_model_dropout_64_16_multi_input, show_shapes=True)","metadata":{"execution":{"iopub.status.busy":"2021-06-18T09:55:37.623365Z","iopub.execute_input":"2021-06-18T09:55:37.62366Z","iopub.status.idle":"2021-06-18T09:55:37.628002Z","shell.execute_reply.started":"2021-06-18T09:55:37.623631Z","shell.execute_reply":"2021-06-18T09:55:37.626569Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Image(\"../input/tfg-pet-adoption-data/model_bottom.png\")","metadata":{"execution":{"iopub.status.busy":"2021-06-22T20:31:24.118304Z","iopub.execute_input":"2021-06-22T20:31:24.118689Z","iopub.status.idle":"2021-06-22T20:31:24.146256Z","shell.execute_reply.started":"2021-06-22T20:31:24.118648Z","shell.execute_reply":"2021-06-22T20:31:24.145304Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train[\"PhotoAmt\"].describe()","metadata":{"execution":{"iopub.status.busy":"2021-06-22T20:31:24.147966Z","iopub.execute_input":"2021-06-22T20:31:24.14835Z","iopub.status.idle":"2021-06-22T20:31:24.161709Z","shell.execute_reply.started":"2021-06-22T20:31:24.148311Z","shell.execute_reply":"2021-06-22T20:31:24.16075Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_dataframe_CNN_training_AdoptionSpeed_multi_input(train_df, num_inputs):\n    filenames_and_target_values = {}\n    \n    for i, (index, row) in tqdm(enumerate(train_df.iterrows())):\n        pet_id = row[\"PetID\"]\n        photo_amt = int(row[\"PhotoAmt\"])\n        adoption_speed = int(row[\"AdoptionSpeed\"])\n        if photo_amt > 0:\n            columns = {}\n            columns[\"PetID\"] = pet_id\n            columns[\"AdoptionSpeed\"] = adoption_speed\n            columns[\"PhotoAmt\"] = photo_amt\n            columns[f\"filename{random.randint(1, num_inputs)}\"] = f\"{pet_id}-1.jpg\"\n            remaining_files = list(range(2, photo_amt+1))\n            random.shuffle(remaining_files)\n            if photo_amt < num_inputs:\n                fill_remaining_files = list(range(1, photo_amt+1))\n                random.shuffle(fill_remaining_files)\n                num_remaining_files = len(remaining_files)\n                for _ in range(num_remaining_files, num_inputs-1):\n                    if len(fill_remaining_files) > 0:\n                        remaining_files.append(fill_remaining_files.pop())\n                    else:\n                        remaining_files.append(random.randint(1, photo_amt))\n            for j in range(1, num_inputs+1):\n                if f\"filename{j}\" not in columns:\n                    columns[f\"filename{j}\"] =  f\"{pet_id}-{remaining_files.pop()}.jpg\"\n            filenames_and_target_values[i] = columns\n        \n    filenames_and_target_values_df = pd.DataFrame.from_dict(filenames_and_target_values, orient=\"index\")\n    filenames_and_target_values_df.reset_index(drop=True, inplace=True)\n    filenames_and_target_values_df.to_csv(f\"dataframe_CNN_training_{num_inputs}-img_inputs.csv\", index=False)\n\n    return filenames_and_target_values_df","metadata":{"execution":{"iopub.status.busy":"2021-06-17T22:22:54.194487Z","iopub.execute_input":"2021-06-17T22:22:54.194886Z","iopub.status.idle":"2021-06-17T22:22:54.206492Z","shell.execute_reply.started":"2021-06-17T22:22:54.194853Z","shell.execute_reply":"2021-06-17T22:22:54.205391Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# filenames_and_target_values_5_img_inputs = get_dataframe_CNN_training_AdoptionSpeed_multi_input(\n#     train_df=train,\n#     num_inputs=5\n# )\nfilenames_and_target_values_5_img_inputs = pd.read_csv(\"../input/tfg-pet-adoption-data/dataframe_CNN_training_5-img_inputs.csv\")","metadata":{"execution":{"iopub.status.busy":"2021-06-22T20:31:24.163384Z","iopub.execute_input":"2021-06-22T20:31:24.163713Z","iopub.status.idle":"2021-06-22T20:31:24.239284Z","shell.execute_reply.started":"2021-06-22T20:31:24.163685Z","shell.execute_reply":"2021-06-22T20:31:24.238144Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"filenames_and_target_values_5_img_inputs.head(10)","metadata":{"execution":{"iopub.status.busy":"2021-06-22T20:31:24.240755Z","iopub.execute_input":"2021-06-22T20:31:24.241136Z","iopub.status.idle":"2021-06-22T20:31:24.260328Z","shell.execute_reply.started":"2021-06-22T20:31:24.241096Z","shell.execute_reply":"2021-06-22T20:31:24.259132Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"filenames_and_target_values_5_img_inputs_train = \\\n    filenames_and_target_values_5_img_inputs.loc[\n        filenames_and_target_values_5_img_inputs[\"PetID\"].isin(set_pet_ids_train)\n    ].copy()\nfilenames_and_target_values_5_img_inputs_val = \\\n    filenames_and_target_values_5_img_inputs.loc[\n        ~filenames_and_target_values_5_img_inputs[\"PetID\"].isin(set_pet_ids_train)\n    ].copy()","metadata":{"execution":{"iopub.status.busy":"2021-06-22T20:31:24.261896Z","iopub.execute_input":"2021-06-22T20:31:24.262178Z","iopub.status.idle":"2021-06-22T20:31:24.282707Z","shell.execute_reply.started":"2021-06-22T20:31:24.262151Z","shell.execute_reply":"2021-06-22T20:31:24.281737Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"filenames_and_target_values_5_img_inputs_train","metadata":{"execution":{"iopub.status.busy":"2021-06-22T20:31:24.288938Z","iopub.execute_input":"2021-06-22T20:31:24.289285Z","iopub.status.idle":"2021-06-22T20:31:24.313016Z","shell.execute_reply.started":"2021-06-22T20:31:24.289251Z","shell.execute_reply":"2021-06-22T20:31:24.311941Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"filenames_and_target_values_5_img_inputs_val","metadata":{"execution":{"iopub.status.busy":"2021-06-22T20:31:24.31472Z","iopub.execute_input":"2021-06-22T20:31:24.315115Z","iopub.status.idle":"2021-06-22T20:31:24.335026Z","shell.execute_reply.started":"2021-06-22T20:31:24.315073Z","shell.execute_reply":"2021-06-22T20:31:24.333937Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"filenames_and_target_values_5_img_inputs_train[\"AdoptionSpeed\"].value_counts(normalize=True)","metadata":{"execution":{"iopub.status.busy":"2021-06-22T20:31:24.337841Z","iopub.execute_input":"2021-06-22T20:31:24.338184Z","iopub.status.idle":"2021-06-22T20:31:24.347265Z","shell.execute_reply.started":"2021-06-22T20:31:24.338151Z","shell.execute_reply":"2021-06-22T20:31:24.346159Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"filenames_and_target_values_5_img_inputs_val[\"AdoptionSpeed\"].value_counts(normalize=True)","metadata":{"execution":{"iopub.status.busy":"2021-06-22T20:31:24.348692Z","iopub.execute_input":"2021-06-22T20:31:24.349079Z","iopub.status.idle":"2021-06-22T20:31:24.362658Z","shell.execute_reply.started":"2021-06-22T20:31:24.349011Z","shell.execute_reply":"2021-06-22T20:31:24.361502Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(set(filenames_and_target_values_5_img_inputs_train[\"PetID\"]) & set(filenames_and_target_values_5_img_inputs_val[\"PetID\"]))","metadata":{"execution":{"iopub.status.busy":"2021-06-22T20:31:24.364Z","iopub.execute_input":"2021-06-22T20:31:24.364425Z","iopub.status.idle":"2021-06-22T20:31:24.378314Z","shell.execute_reply.started":"2021-06-22T20:31:24.364382Z","shell.execute_reply":"2021-06-22T20:31:24.37725Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**https://stackoverflow.com/questions/49404993/how-to-use-fit-generator-with-multiple-inputs**","metadata":{}},{"cell_type":"code","source":"def get_train_iterator_multi_input(num_inputs, train_dataframe, target_size, batch_size, seed=seed):\n    train_generator = ImageDataGenerator(\n        horizontal_flip=True,\n        zoom_range=[0.5, 1.0],\n        width_shift_range=0.3,\n        height_shift_range=0.3,\n        preprocessing_function=preprocess_input_densenet\n    )\n    \n    train_iterators = {}\n    for i in range(1, num_inputs+1):\n        train_iterators[i] = train_generator.flow_from_dataframe(\n            dataframe=train_dataframe,\n            directory=\"../input/tfg-pet-adoption-resized-train-images/\",\n            x_col=f\"filename{i}\",\n            y_col=\"AdoptionSpeed\",\n            target_size=target_size,\n            class_mode=\"raw\",\n            batch_size=batch_size,\n            shuffle=True,\n            seed=seed\n        )\n    \n    while True:\n        X_i = []\n        for i in range(1, num_inputs+1):\n            X_i.append(next(train_iterators[i]))\n        yield random.sample([x[0] for x in X_i], num_inputs), X_i[0][1]","metadata":{"execution":{"iopub.status.busy":"2021-06-18T12:35:18.803384Z","iopub.execute_input":"2021-06-18T12:35:18.803649Z","iopub.status.idle":"2021-06-18T12:35:18.811133Z","shell.execute_reply.started":"2021-06-18T12:35:18.803621Z","shell.execute_reply":"2021-06-18T12:35:18.810308Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_val_iterator_multi_input(num_inputs, val_dataframe, target_size, batch_size, seed=seed):\n    val_generator = ImageDataGenerator(\n        preprocessing_function=preprocess_input_densenet\n    )\n    \n    val_iterators = {}\n    for i in range(1, num_inputs+1):\n        val_iterators[i] = val_generator.flow_from_dataframe(\n            dataframe=val_dataframe,\n            directory=\"../input/tfg-pet-adoption-resized-train-images/\",\n            x_col=f\"filename{i}\",\n            y_col=\"AdoptionSpeed\",\n            target_size=target_size,\n            class_mode=\"raw\",\n            batch_size=batch_size,\n            shuffle=False,\n            seed=seed\n        )\n    \n    while True:\n        X_i = []\n        for i in range(1, num_inputs+1):\n            X_i.append(next(val_iterators[i]))\n        yield [x[0] for x in X_i], X_i[0][1]","metadata":{"execution":{"iopub.status.busy":"2021-06-18T10:17:44.566741Z","iopub.execute_input":"2021-06-18T10:17:44.567083Z","iopub.status.idle":"2021-06-18T10:17:44.57369Z","shell.execute_reply.started":"2021-06-18T10:17:44.567051Z","shell.execute_reply":"2021-06-18T10:17:44.572712Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"epochs = 30\nbatch_size = 32\ntarget_size = (256,256)\n\nregression_multi_input_dropout_64_16_training_results = pd.DataFrame([], columns=[\"loss\",\n                        \"mean_absolute_error\", \"val_loss\",\n                        \"val_mean_absolute_error\"])\n\nearly_stopping = EarlyStopping(monitor='val_loss', restore_best_weights=True, patience=10)\nmodel_checkpoint = ModelCheckpoint(\n    'DenseNet121_regression__multi-input-5__dropout_64-16__{epoch:02d}-epochs_val_loss-{val_loss:02f}.h5',\n    monitor='val_loss', save_best_only=True)\n\ntrain_iterator_multi_input = get_train_iterator_multi_input(\n    num_inputs=5,\n    train_dataframe=filenames_and_target_values_5_img_inputs_train,\n    target_size=target_size,\n    batch_size=batch_size,\n    seed=seed\n)\n\nval_iterator_multi_input = get_val_iterator_multi_input(\n    num_inputs=5,\n    val_dataframe=filenames_and_target_values_5_img_inputs_val,\n    target_size=target_size,\n    batch_size=batch_size,\n    seed=seed\n)\n\nregression_multi_input_dropout_64_16_training_history = \\\n    regression_model_dropout_64_16_multi_input.fit(\n                    x=train_iterator_multi_input,\n                    validation_data=val_iterator_multi_input,\n                    batch_size=batch_size,\n                    steps_per_epoch=math.ceil(filenames_and_target_values_5_img_inputs_train.shape[0] / batch_size),\n                    validation_steps=math.ceil(filenames_and_target_values_5_img_inputs_val.shape[0] / batch_size),\n                    epochs=epochs,\n                    callbacks=[model_checkpoint, early_stopping])\n\ntrain_loss = regression_multi_input_dropout_64_16_training_history.history[\"loss\"]\ntrain_mae = regression_multi_input_dropout_64_16_training_history.history[\"mean_absolute_error\"]\nvalidation_loss = regression_multi_input_dropout_64_16_training_history.history[\"val_loss\"]\nvalidation_mae = regression_multi_input_dropout_64_16_training_history.history[\"val_mean_absolute_error\"]\n\nfor i in range(len(train_loss)):\n    regression_multi_input_dropout_64_16_training_results = regression_multi_input_dropout_64_16_training_results.append({\n        \"loss\": train_loss[i],\n        \"mean_absolute_error\": train_mae[i],\n        \"val_loss\": validation_loss[i],\n        \"val_mean_absolute_error\": validation_mae[i]\n    }, ignore_index=True)\n\nregression_multi_input_dropout_64_16_training_results.to_csv(f\"DenseNet121_regression__multi-input-5__dropout_64-16__{epochs}-epochs.h5.csv\", index=False)\nto_latex(regression_multi_input_dropout_64_16_training_results, f\"DenseNet121_regression__multi-input-5__dropout_64-16__{epochs}-epochs.h5.csv\")\n\n# Zipping the checkpoints\n!mkdir regression_multi_input_5_dropout\n!mv ./DenseNet121_regression__multi-input-5*.h5 regression_multi_input_5_dropout\nshutil.make_archive(\"DenseNet121_regression__multi-input-5__dropout_64-16__checkpoints\", \"zip\", \"./regression_multi_input_5_dropout\")","metadata":{"execution":{"iopub.status.busy":"2021-06-18T15:24:55.180469Z","iopub.execute_input":"2021-06-18T15:24:55.180821Z","iopub.status.idle":"2021-06-18T15:24:55.187933Z","shell.execute_reply.started":"2021-06-18T15:24:55.180784Z","shell.execute_reply":"2021-06-18T15:24:55.187001Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"regression_multi_input_dropout_64_16_training_results = pd.read_csv(\"../input/tfg-pet-adoption-data/DenseNet121_regression__multi-input-5__dropout_64-16__30-epochs.csv\")\nplot_history(regression_multi_input_dropout_64_16_training_results, [\"loss\", \"mean_absolute_error\"], 1, 2, (12,3))","metadata":{"execution":{"iopub.status.busy":"2021-06-22T20:31:24.379666Z","iopub.execute_input":"2021-06-22T20:31:24.379998Z","iopub.status.idle":"2021-06-22T20:31:24.675567Z","shell.execute_reply.started":"2021-06-22T20:31:24.379966Z","shell.execute_reply":"2021-06-22T20:31:24.674543Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The results are bad, the lowest validation loss is 1.38, which is considerably greater than those values that we obtained with single-input CNNs. The same happens when we add a Dense 256 layer before the 64 one just in case we are underfitting. Moreover, this is probably a wrong approach, since even if we just want to use the 5 first photos of a profile, in those cases where the number of photos is smaller, some photos will have more importance (if there are 3 photos, we would input 1 2 3 1 2, in whatever order, which is another problem, giving more importance to the profile photo, which is okay, but also to 2 over 3).","metadata":{}},{"cell_type":"markdown","source":"### Ensemble (single input)","metadata":{}},{"cell_type":"markdown","source":"We will create now a ensemble of three DenseNet121 backbones: the first one without any trainable layer, the second one with the last dense block as trainable and Dropout of 0.1 after the GlobalAveragePooling2D layer and the third one with the two last dense blocks as trainable and Dropout of 0.2 after the GlobalAveragePooling2D layer. Then, we aggregate the outputs using the average and the maximum and concatenate both.","metadata":{}},{"cell_type":"code","source":"inp = Input((256,256,3))\ncnns_outputs = []\n\nfor i in range(3):\n    backbone = DenseNet121(input_tensor=inp, include_top=False, weights=\"imagenet\")\n    x = backbone.output\n    x = GlobalAveragePooling2D()(x)\n    if i > 0:\n        x = Dropout(rate=0.1*i, seed=seed)(x)\n    cnns_outputs.append(x)\n    \n    # Freezing DenseNet121 backbone layers\n    for j, layer in enumerate(backbone.layers):\n        layer._name = f\"{layer.name}_{i}\"\n        if (i == 0) or (i == 1 and j < 418) or (i == 2 and j < 411):\n            layer.trainable = False\n        else:\n            layer.trainable = True\n            \naverage = Average()(cnns_outputs)\nmaximum = Maximum()(cnns_outputs)\n\nconcatenation = Concatenate()([average, maximum])\nx = Dropout(rate=0.25, seed=seed)(concatenation)\nx = Dense(64, activation=\"relu\")(x)\nx = Dropout(rate=0.25, seed=seed)(x)\nx = Dense(16, activation=\"relu\")(x)\nout = Dense(1, activation=\"linear\")(x)\n \nregression_model_dropout_64_16_ensemble = Model(inputs=inp, outputs=out)\n\n# To solve RuntimeError: Unable to create link (name already exists)\n# https://stackoverflow.com/questions/64118599/getting-the-runtimeerror-unable-to-create-link-name-already-exists-with-a-mul\nfor i, w in enumerate(regression_model_dropout_64_16_ensemble.weights):\n    split_name = w.name.split('/')\n    new_name = split_name[0] + '_' + str(i) + '/' + split_name[1] + '_' + str(i)\n    regression_model_dropout_64_16_ensemble.weights[i]._handle_name = new_name\n\noptimizer = Adam(lr=0.001)\nmetrics = [\"mean_absolute_error\"]\nloss = \"mean_squared_error\"\n    \nregression_model_dropout_64_16_ensemble.compile(loss=loss, metrics=metrics, optimizer=optimizer)","metadata":{"execution":{"iopub.status.busy":"2021-06-21T18:49:59.259767Z","iopub.execute_input":"2021-06-21T18:49:59.26059Z","iopub.status.idle":"2021-06-21T18:50:31.645128Z","shell.execute_reply.started":"2021-06-21T18:49:59.260535Z","shell.execute_reply":"2021-06-21T18:50:31.643982Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plot_model(regression_model_dropout_64_16_ensemble, show_shapes=True)\nImage(\"../input/tfg-pet-adoption-data/ensemble_model_zoom_output.png\")","metadata":{"execution":{"iopub.status.busy":"2021-06-22T20:31:24.676741Z","iopub.execute_input":"2021-06-22T20:31:24.677007Z","iopub.status.idle":"2021-06-22T20:31:24.697264Z","shell.execute_reply.started":"2021-06-22T20:31:24.676977Z","shell.execute_reply":"2021-06-22T20:31:24.696475Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"epochs = 30\nbatch_size = 32\n\nregression_ensemble_training_results = pd.DataFrame([], columns=[\"loss\",\n                        \"mean_absolute_error\", \"val_loss\",\n                        \"val_mean_absolute_error\"])\n\nearly_stopping = EarlyStopping(monitor='val_loss', restore_best_weights=True, patience=5)\nmodel_checkpoint = ModelCheckpoint(\n    'DenseNet121_regression__ensemble__{epoch:02d}-epochs_val_loss-{val_loss:02f}.h5',\n    monitor='val_loss', save_best_only=True)\n\nregression_model_dropout_64_16_ensemble_history = regression_model_dropout_64_16_ensemble.fit(\n                    x=regression_adoptionspeed_train_iterator_da,\n                    validation_data=regression_adoptionspeed_val_iterator,\n                    batch_size=batch_size, epochs=epochs,\n                    callbacks=[model_checkpoint, early_stopping])\n\ntrain_loss = regression_model_dropout_64_16_ensemble_history.history[\"loss\"]\ntrain_mae = regression_model_dropout_64_16_ensemble_history.history[\"mean_absolute_error\"]\nvalidation_loss = regression_model_dropout_64_16_ensemble_history.history[\"val_loss\"]\nvalidation_mae = regression_model_dropout_64_16_ensemble_history.history[\"val_mean_absolute_error\"]\n\nfor i in range(len(train_loss)):\n    regression_ensemble_training_results = regression_ensemble_training_results.append({\n        \"loss\": train_loss[i],\n        \"mean_absolute_error\": train_mae[i],\n        \"val_loss\": validation_loss[i],\n        \"val_mean_absolute_error\": validation_mae[i]\n    }, ignore_index=True)\n\nregression_ensemble_training_results.to_csv(f\"DenseNet121_regression__dropout_64-16__ensemble__{epochs}-epochs.csv\", index=False)\nto_latex(regression_ensemble_training_results, f\"DenseNet121_regression__dropout_64-16__ensemble__{epochs}-epochs\")\n\n# Zipping the checkpoints\n!mkdir regression_ensemble\n!mv ./DenseNet121_regression__dropout_64-16__ensemble*.h5 regression_ensemble\nshutil.make_archive(\"DenseNet121_regression__dropout_64-16__ensemble__checkpoints\", \"zip\", \"./regression_ensemble\")","metadata":{"execution":{"iopub.status.busy":"2021-06-21T08:42:24.335602Z","iopub.execute_input":"2021-06-21T08:42:24.335988Z","iopub.status.idle":"2021-06-21T11:41:13.585239Z","shell.execute_reply.started":"2021-06-21T08:42:24.335956Z","shell.execute_reply":"2021-06-21T11:41:13.584426Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"regression_ensemble_training_results = pd.read_csv(\"../input/tfg-pet-adoption-data/DenseNet121_regression__dropout_64-16__ensemble__30-epochs.csv\")\nplot_history(regression_ensemble_training_results, [\"loss\", \"mean_absolute_error\"], 1, 2, (12,3))","metadata":{"execution":{"iopub.status.busy":"2021-06-22T20:31:24.698483Z","iopub.execute_input":"2021-06-22T20:31:24.698763Z","iopub.status.idle":"2021-06-22T20:31:24.989114Z","shell.execute_reply.started":"2021-06-22T20:31:24.698735Z","shell.execute_reply":"2021-06-22T20:31:24.988366Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = Model(inputs=regression_model_dropout_64_16_ensemble.input,\n              outputs=regression_model_dropout_64_16_ensemble.layers[-4].output)\nife = ImageFeatureExtractor(construct_from_cnn_backbone=False, model=model,\n                 images_directory=\"../input/petfinder-adoption-prediction/train_images\",\n                 from_image=True, preprocess_input=preprocess_input_densenet, img_size=256,\n                 model_name=f\"DenseNet121_regression__ensemble__layer-64\",\n                 save=True, debug=True, include_feats=False)\n\n_ = ife.fit_transform(X, y)","metadata":{"execution":{"iopub.status.busy":"2021-06-21T12:01:47.023389Z","iopub.execute_input":"2021-06-21T12:01:47.023749Z","iopub.status.idle":"2021-06-21T12:08:10.231015Z","shell.execute_reply.started":"2021-06-21T12:01:47.023717Z","shell.execute_reply":"2021-06-21T12:08:10.229981Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image_features_regression_model_ensemble_layer_64_features = pd.read_csv(\n    \"../input/tfg-pet-adoption-data/image_features_DenseNet121_regression__ensemble__layer-64_in-256.csv\",\n    index_col=0)\ndisplay(image_features_regression_model_ensemble_layer_64_features.head(5))\nimage_features_regression_model_ensemble_layer_64_features.describe().loc[\"std\",:].value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-06-22T20:31:24.990234Z","iopub.execute_input":"2021-06-22T20:31:24.990683Z","iopub.status.idle":"2021-06-22T20:31:25.314978Z","shell.execute_reply.started":"2021-06-22T20:31:24.990652Z","shell.execute_reply":"2021-06-22T20:31:25.314177Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = Model(inputs=regression_model_dropout_64_16_ensemble.input,\n              outputs=regression_model_dropout_64_16_ensemble.layers[-2].output)\nife = ImageFeatureExtractor(construct_from_cnn_backbone=False, model=model,\n                 images_directory=\"../input/petfinder-adoption-prediction/train_images\",\n                 from_image=True, preprocess_input=preprocess_input_densenet, img_size=256,\n                 model_name=f\"DenseNet121_regression__ensemble__layer-16\",\n                 save=True, debug=True, include_feats=False)\n\n_ = ife.fit_transform(X, y)","metadata":{"execution":{"iopub.status.busy":"2021-06-21T12:08:10.232796Z","iopub.execute_input":"2021-06-21T12:08:10.233159Z","iopub.status.idle":"2021-06-21T12:11:48.833743Z","shell.execute_reply.started":"2021-06-21T12:08:10.233119Z","shell.execute_reply":"2021-06-21T12:11:48.832772Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image_features_regression_model_ensemble_layer_16_features = pd.read_csv(\n    \"../input/tfg-pet-adoption-data/image_features_DenseNet121_regression__ensemble__layer-16_in-256.csv\",\n    index_col=0)\ndisplay(image_features_regression_model_ensemble_layer_16_features.head(5))\nimage_features_regression_model_ensemble_layer_16_features.describe().loc[\"std\",:].value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-06-22T20:31:25.316157Z","iopub.execute_input":"2021-06-22T20:31:25.316632Z","iopub.status.idle":"2021-06-22T20:31:25.455241Z","shell.execute_reply.started":"2021-06-22T20:31:25.316599Z","shell.execute_reply":"2021-06-22T20:31:25.454147Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = Model(inputs=regression_model_dropout_64_16_ensemble.input,\n              outputs=regression_model_dropout_64_16_ensemble.layers[-4].output)\nife = ImageFeatureExtractor(construct_from_cnn_backbone=False, model=model,\n                 images_directory=\"../input/petfinder-adoption-prediction/train_images\",\n                 from_image=True, preprocess_input=preprocess_input_densenet, img_size=256,\n                 model_name=f\"DenseNet121_regression__ensemble__layer-64\",\n                 save=True, debug=True, include_feats=False, multiple_instances_per_petid=True)\n\n_ = ife.fit_transform(X, y)","metadata":{"execution":{"iopub.status.busy":"2021-06-21T12:13:02.332649Z","iopub.execute_input":"2021-06-21T12:13:02.332992Z","iopub.status.idle":"2021-06-21T12:34:46.592593Z","shell.execute_reply.started":"2021-06-21T12:13:02.33296Z","shell.execute_reply":"2021-06-21T12:34:46.591547Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_image_features_regression_model_ensemble_layer_64_features = pd.read_csv(\n    \"../input/tfg-pet-adoption-data/ALL_image_features_DenseNet121_regression__ensemble__layer-64_in-256.csv\",\n    index_col=0)\ndisplay(all_image_features_regression_model_ensemble_layer_64_features.head(5))\nall_image_features_regression_model_ensemble_layer_64_features.describe().loc[\"std\",:].value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-06-22T20:31:25.456553Z","iopub.execute_input":"2021-06-22T20:31:25.456926Z","iopub.status.idle":"2021-06-22T20:31:26.257576Z","shell.execute_reply.started":"2021-06-22T20:31:25.456884Z","shell.execute_reply":"2021-06-22T20:31:26.256576Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"aggregated_image_features_regression_model_ensemble_layer_64_features = pd.read_csv(\n    \"../input/tfg-pet-adoption-data/AGGREGATED_image_features_DenseNet121_regression__ensemble__layer-64_in-256.csv\",\n    index_col=0)\ndisplay(aggregated_image_features_regression_model_ensemble_layer_64_features.head(5))\naggregated_image_features_regression_model_ensemble_layer_64_features.describe().loc[\"std\",:].value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-06-22T20:31:26.258867Z","iopub.execute_input":"2021-06-22T20:31:26.259129Z","iopub.status.idle":"2021-06-22T20:31:27.085428Z","shell.execute_reply.started":"2021-06-22T20:31:26.259104Z","shell.execute_reply":"2021-06-22T20:31:27.084299Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = Model(inputs=regression_model_dropout_64_16_ensemble.input,\n              outputs=regression_model_dropout_64_16_ensemble.layers[-2].output)\nife = ImageFeatureExtractor(construct_from_cnn_backbone=False, model=model,\n                 images_directory=\"../input/petfinder-adoption-prediction/train_images\",\n                 from_image=True, preprocess_input=preprocess_input_densenet, img_size=256,\n                 model_name=f\"DenseNet121_regression__ensemble__layer-16\",\n                 save=True, debug=True, include_feats=False, multiple_instances_per_petid=True)\n\n_ = ife.fit_transform(X, y)","metadata":{"execution":{"iopub.status.busy":"2021-06-21T12:34:46.594659Z","iopub.execute_input":"2021-06-21T12:34:46.595051Z","iopub.status.idle":"2021-06-21T12:48:51.290517Z","shell.execute_reply.started":"2021-06-21T12:34:46.595011Z","shell.execute_reply":"2021-06-21T12:48:51.289546Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_image_features_regression_model_ensemble_layer_16_features = pd.read_csv(\n    \"../input/tfg-pet-adoption-data/ALL_image_features_DenseNet121_regression__ensemble__layer-16_in-256.csv\",\n    index_col=0)\ndisplay(all_image_features_regression_model_ensemble_layer_16_features.head(5))\nall_image_features_regression_model_ensemble_layer_16_features.describe().loc[\"std\",:].value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-06-22T20:31:27.087068Z","iopub.execute_input":"2021-06-22T20:31:27.087448Z","iopub.status.idle":"2021-06-22T20:31:27.399127Z","shell.execute_reply.started":"2021-06-22T20:31:27.087407Z","shell.execute_reply":"2021-06-22T20:31:27.398008Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"aggregated_image_features_regression_model_ensemble_layer_16_features = pd.read_csv(\n    \"../input/tfg-pet-adoption-data/AGGREGATED_image_features_DenseNet121_regression__ensemble__layer-16_in-256.csv\",\n    index_col=0)\ndisplay(aggregated_image_features_regression_model_ensemble_layer_16_features.head(5))\naggregated_image_features_regression_model_ensemble_layer_16_features.describe().loc[\"std\",:].value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-06-22T20:31:27.400058Z","iopub.execute_input":"2021-06-22T20:31:27.400307Z","iopub.status.idle":"2021-06-22T20:31:27.703895Z","shell.execute_reply.started":"2021-06-22T20:31:27.400281Z","shell.execute_reply":"2021-06-22T20:31:27.702509Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's create another ensemble, this time without Dropout layers in the output of each DenseNet121 backbone, but increasing (twice the value we had before) the Dropout layer before the Dense 64 one:","metadata":{}},{"cell_type":"code","source":"inp = Input((256,256,3))\ncnns_outputs = []\n\nfor i in range(3):\n    backbone = DenseNet121(input_tensor=inp, include_top=False, weights=\"imagenet\")\n    x = backbone.output\n    x = GlobalAveragePooling2D()(x)\n    cnns_outputs.append(x)\n    \n    # Freezing DenseNet121 backbone layers\n    for j, layer in enumerate(backbone.layers):\n        layer._name = f\"{layer.name}_{i}\"\n        if (i == 0) or (i == 1 and j < 418) or (i == 2 and j < 411):\n            layer.trainable = False\n        else:\n            layer.trainable = True\n            \naverage = Average()(cnns_outputs)\nmaximum = Maximum()(cnns_outputs)\n\nconcatenation = Concatenate()([average, maximum])\nx = Dropout(rate=0.5, seed=seed)(concatenation) # More dropout here\nx = Dense(64, activation=\"relu\")(x)\nx = Dropout(rate=0.25, seed=seed)(x)\nx = Dense(16, activation=\"relu\")(x)\nout = Dense(1, activation=\"linear\")(x)\n \nregression_model_dropout_64_16_ensemble_2 = Model(inputs=inp, outputs=out)\n\n# To solve RuntimeError: Unable to create link (name already exists)\n# https://stackoverflow.com/questions/64118599/getting-the-runtimeerror-unable-to-create-link-name-already-exists-with-a-mul\nfor i, w in enumerate(regression_model_dropout_64_16_ensemble_2.weights):\n    split_name = w.name.split('/')\n    new_name = split_name[0] + '_' + str(i) + '/' + split_name[1] + '_' + str(i)\n    regression_model_dropout_64_16_ensemble_2.weights[i]._handle_name = new_name\n\noptimizer = Adam(lr=0.001)\nmetrics = [\"mean_absolute_error\"]\nloss = \"mean_squared_error\"\n    \nregression_model_dropout_64_16_ensemble_2.compile(loss=loss, metrics=metrics, optimizer=optimizer)","metadata":{"execution":{"iopub.status.busy":"2021-06-21T19:16:48.781414Z","iopub.execute_input":"2021-06-21T19:16:48.781931Z","iopub.status.idle":"2021-06-21T19:17:16.587601Z","shell.execute_reply.started":"2021-06-21T19:16:48.781892Z","shell.execute_reply":"2021-06-21T19:17:16.586816Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plot_model(regression_model_dropout_64_16_ensemble_2, show_shapes=True)\nImage(\"../input/tfg-pet-adoption-data/ensemble_model_2_zoom.png\")","metadata":{"execution":{"iopub.status.busy":"2021-06-22T22:12:21.134212Z","iopub.execute_input":"2021-06-22T22:12:21.134484Z","iopub.status.idle":"2021-06-22T22:12:21.152939Z","shell.execute_reply.started":"2021-06-22T22:12:21.134457Z","shell.execute_reply":"2021-06-22T22:12:21.151425Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"epochs = 30\nbatch_size = 32\n\nregression_ensemble_2_training_results = pd.DataFrame([], columns=[\"loss\",\n                        \"mean_absolute_error\", \"val_loss\",\n                        \"val_mean_absolute_error\"])\n\nearly_stopping = EarlyStopping(monitor='val_loss', restore_best_weights=True, patience=5)\nmodel_checkpoint = ModelCheckpoint(\n    'DenseNet121_regression__ensemble__2__{epoch:02d}-epochs_val_loss-{val_loss:02f}.h5',\n    monitor='val_loss', save_best_only=True)\n\nregression_model_dropout_64_16_ensemble_2_history = regression_model_dropout_64_16_ensemble_2.fit(\n                    x=regression_adoptionspeed_train_iterator_da,\n                    validation_data=regression_adoptionspeed_val_iterator,\n                    batch_size=batch_size, epochs=epochs,\n                    callbacks=[model_checkpoint, early_stopping])\n\ntrain_loss = regression_model_dropout_64_16_ensemble_2_history.history[\"loss\"]\ntrain_mae = regression_model_dropout_64_16_ensemble_2_history.history[\"mean_absolute_error\"]\nvalidation_loss = regression_model_dropout_64_16_ensemble_2_history.history[\"val_loss\"]\nvalidation_mae = regression_model_dropout_64_16_ensemble_2_history.history[\"val_mean_absolute_error\"]\n\nfor i in range(len(train_loss)):\n    regression_ensemble_2_training_results = regression_ensemble_2_training_results.append({\n        \"loss\": train_loss[i],\n        \"mean_absolute_error\": train_mae[i],\n        \"val_loss\": validation_loss[i],\n        \"val_mean_absolute_error\": validation_mae[i]\n    }, ignore_index=True)\n\nregression_ensemble_2_training_results.to_csv(f\"DenseNet121_regression__dropout_64-16__ensemble__2__{epochs}-epochs.csv\", index=False)\nto_latex(regression_ensemble_2_training_results, f\"DenseNet121_regression__dropout_64-16__ensemble__2__{epochs}-epochs\")\n\n# Zipping the checkpoints\n!mkdir regression_ensemble_2\n!mv ./DenseNet121_regression__dropout_64-16__ensemble__2*.h5 regression_ensemble_2\nshutil.make_archive(\"DenseNet121_regression__dropout_64-16__ensemble__2__checkpoints\", \"zip\", \"./regression_ensemble_2\")","metadata":{"execution":{"iopub.status.busy":"2021-06-21T19:17:16.588933Z","iopub.execute_input":"2021-06-21T19:17:16.589259Z","iopub.status.idle":"2021-06-21T21:39:59.636162Z","shell.execute_reply.started":"2021-06-21T19:17:16.589224Z","shell.execute_reply":"2021-06-21T21:39:59.635346Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"regression_ensemble_2_training_results = pd.read_csv(\"../input/tfg-pet-adoption-data/DenseNet121_regression__dropout_64-16__ensemble__2__30-epochs.csv\")\nplot_history(regression_ensemble_2_training_results, [\"loss\", \"mean_absolute_error\"], 1, 2, (12,3))","metadata":{"execution":{"iopub.status.busy":"2021-06-22T20:31:27.705291Z","iopub.execute_input":"2021-06-22T20:31:27.705593Z","iopub.status.idle":"2021-06-22T20:31:27.99926Z","shell.execute_reply.started":"2021-06-22T20:31:27.705566Z","shell.execute_reply":"2021-06-22T20:31:27.998375Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"regression_model_dropout_64_16_ensemble_2 = load_model(\"../input/tfg-pet-adoption-data/DenseNet121_regression__ensemble__2__06-epochs_val_loss-1.160883.h5\")","metadata":{"execution":{"iopub.status.busy":"2021-06-22T20:57:04.268171Z","iopub.execute_input":"2021-06-22T20:57:04.268623Z","iopub.status.idle":"2021-06-22T20:57:16.76154Z","shell.execute_reply.started":"2021-06-22T20:57:04.268584Z","shell.execute_reply":"2021-06-22T20:57:16.760587Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = Model(inputs=regression_model_dropout_64_16_ensemble_2.input,\n              outputs=regression_model_dropout_64_16_ensemble_2.layers[-4].output)\nife = ImageFeatureExtractor(construct_from_cnn_backbone=False, model=model,\n                 images_directory=\"../input/petfinder-adoption-prediction/train_images\",\n                 from_image=True, preprocess_input=preprocess_input_densenet, img_size=256,\n                 model_name=f\"DenseNet121_regression__ensemble__2__layer-64\",\n                 save=True, debug=True, include_feats=False)\n\n_ = ife.fit_transform(X, y)","metadata":{"execution":{"iopub.status.busy":"2021-06-21T21:50:56.003202Z","iopub.execute_input":"2021-06-21T21:50:56.003559Z","iopub.status.idle":"2021-06-21T21:56:30.672237Z","shell.execute_reply.started":"2021-06-21T21:50:56.003526Z","shell.execute_reply":"2021-06-21T21:56:30.671303Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image_features_regression_model_ensemble_2_layer_64_features = pd.read_csv(\n    \"../input/tfg-pet-adoption-data/image_features_DenseNet121_regression__ensemble__2__layer-64_in-256.csv\",\n    index_col=0)\ndisplay(image_features_regression_model_ensemble_2_layer_64_features.head(5))\nimage_features_regression_model_ensemble_2_layer_64_features.describe().loc[\"std\",:].value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-06-22T20:31:28.000703Z","iopub.execute_input":"2021-06-22T20:31:28.001019Z","iopub.status.idle":"2021-06-22T20:31:28.393819Z","shell.execute_reply.started":"2021-06-22T20:31:28.000984Z","shell.execute_reply":"2021-06-22T20:31:28.393008Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = Model(inputs=regression_model_dropout_64_16_ensemble_2.input,\n              outputs=regression_model_dropout_64_16_ensemble_2.layers[-2].output)\nife = ImageFeatureExtractor(construct_from_cnn_backbone=False, model=model,\n                 images_directory=\"../input/petfinder-adoption-prediction/train_images\",\n                 from_image=True, preprocess_input=preprocess_input_densenet, img_size=256,\n                 model_name=f\"DenseNet121_regression__ensemble__2__layer-16\",\n                 save=True, debug=True, include_feats=False)\n\n_ = ife.fit_transform(X, y)","metadata":{"execution":{"iopub.status.busy":"2021-06-21T21:56:30.675632Z","iopub.execute_input":"2021-06-21T21:56:30.675903Z","iopub.status.idle":"2021-06-21T22:00:10.29022Z","shell.execute_reply.started":"2021-06-21T21:56:30.675875Z","shell.execute_reply":"2021-06-21T22:00:10.288166Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image_features_regression_model_ensemble_2_layer_16_features = pd.read_csv(\n    \"../input/tfg-pet-adoption-data/image_features_DenseNet121_regression__ensemble__2__layer-16_in-256.csv\",\n    index_col=0)\ndisplay(image_features_regression_model_ensemble_2_layer_16_features.head(5))\nimage_features_regression_model_ensemble_2_layer_16_features.describe().loc[\"std\",:].value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-06-22T20:31:28.394994Z","iopub.execute_input":"2021-06-22T20:31:28.395273Z","iopub.status.idle":"2021-06-22T20:31:28.594564Z","shell.execute_reply.started":"2021-06-22T20:31:28.395245Z","shell.execute_reply":"2021-06-22T20:31:28.593458Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = Model(inputs=regression_model_dropout_64_16_ensemble_2.input,\n              outputs=regression_model_dropout_64_16_ensemble_2.layers[-4].output)\nife = ImageFeatureExtractor(construct_from_cnn_backbone=False, model=model,\n                 images_directory=\"../input/petfinder-adoption-prediction/train_images\",\n                 from_image=True, preprocess_input=preprocess_input_densenet, img_size=256,\n                 model_name=f\"DenseNet121_regression__ensemble__2__layer-64\",\n                 save=True, debug=True, include_feats=False, multiple_instances_per_petid=True)\n\n_ = ife.fit_transform(X, y)","metadata":{"execution":{"iopub.status.busy":"2021-06-21T22:00:10.291646Z","iopub.execute_input":"2021-06-21T22:00:10.292023Z","iopub.status.idle":"2021-06-21T22:19:53.755153Z","shell.execute_reply.started":"2021-06-21T22:00:10.291984Z","shell.execute_reply":"2021-06-21T22:19:53.754202Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_image_features_regression_model_ensemble_2_layer_64_features = pd.read_csv(\n    \"../input/tfg-pet-adoption-data/ALL_image_features_DenseNet121_regression__ensemble__2__layer-64_in-256.csv\",\n    index_col=0)\ndisplay(all_image_features_regression_model_ensemble_2_layer_64_features.head(5))\nall_image_features_regression_model_ensemble_2_layer_64_features.describe().loc[\"std\",:].value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-06-22T20:31:28.597672Z","iopub.execute_input":"2021-06-22T20:31:28.597956Z","iopub.status.idle":"2021-06-22T20:31:29.337161Z","shell.execute_reply.started":"2021-06-22T20:31:28.597926Z","shell.execute_reply":"2021-06-22T20:31:29.336008Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"aggregated_image_features_regression_model_ensemble_2_layer_64_features = pd.read_csv(\n    \"../input/tfg-pet-adoption-data/AGGREGATED_image_features_DenseNet121_regression__ensemble__2__layer-64_in-256.csv\",\n    index_col=0)\ndisplay(aggregated_image_features_regression_model_ensemble_2_layer_64_features.head(5))\naggregated_image_features_regression_model_ensemble_2_layer_64_features.describe().loc[\"std\",:].value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-06-22T20:31:29.338173Z","iopub.execute_input":"2021-06-22T20:31:29.33843Z","iopub.status.idle":"2021-06-22T20:31:30.151224Z","shell.execute_reply.started":"2021-06-22T20:31:29.338405Z","shell.execute_reply":"2021-06-22T20:31:30.150304Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = Model(inputs=regression_model_dropout_64_16_ensemble_2.input,\n              outputs=regression_model_dropout_64_16_ensemble_2.layers[-2].output)\nife = ImageFeatureExtractor(construct_from_cnn_backbone=False, model=model,\n                 images_directory=\"../input/petfinder-adoption-prediction/train_images\",\n                 from_image=True, preprocess_input=preprocess_input_densenet, img_size=256,\n                 model_name=f\"DenseNet121_regression__ensemble__2__layer-16\",\n                 save=True, debug=True, include_feats=False, multiple_instances_per_petid=True)\n\n_ = ife.fit_transform(X, y)","metadata":{"execution":{"iopub.status.busy":"2021-06-21T22:19:53.756529Z","iopub.execute_input":"2021-06-21T22:19:53.757026Z","iopub.status.idle":"2021-06-21T22:33:55.299792Z","shell.execute_reply.started":"2021-06-21T22:19:53.756986Z","shell.execute_reply":"2021-06-21T22:33:55.298264Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_image_features_regression_model_ensemble_2_layer_16_features = pd.read_csv(\n    \"../input/tfg-pet-adoption-data/ALL_image_features_DenseNet121_regression__ensemble__2__layer-16_in-256.csv\",\n    index_col=0)\ndisplay(all_image_features_regression_model_ensemble_2_layer_16_features.head(5))\nall_image_features_regression_model_ensemble_2_layer_16_features.describe().loc[\"std\",:].value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-06-22T20:31:30.15238Z","iopub.execute_input":"2021-06-22T20:31:30.152623Z","iopub.status.idle":"2021-06-22T20:31:30.475796Z","shell.execute_reply.started":"2021-06-22T20:31:30.152598Z","shell.execute_reply":"2021-06-22T20:31:30.474843Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"aggregated_image_features_regression_model_ensemble_2_layer_16_features = pd.read_csv(\n    \"../input/tfg-pet-adoption-data/AGGREGATED_image_features_DenseNet121_regression__ensemble__2__layer-16_in-256.csv\",\n    index_col=0)\ndisplay(aggregated_image_features_regression_model_ensemble_2_layer_16_features.head(5))\naggregated_image_features_regression_model_ensemble_2_layer_16_features.describe().loc[\"std\",:].value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-06-29T07:31:29.282186Z","iopub.execute_input":"2021-06-29T07:31:29.282547Z","iopub.status.idle":"2021-06-29T07:31:29.702363Z","shell.execute_reply.started":"2021-06-29T07:31:29.282516Z","shell.execute_reply":"2021-06-29T07:31:29.701286Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pipeline_transformers = [\n    ('replace_breeds', LeftJoinReplace(values_dict=breeds_dict,\n                                      variables=[\"Breed1\", \"Breed2\"])),\n    ('replace_colors', LeftJoinReplace(values_dict=colors_dict,\n                                      variables=[\"Color1\", \"Color2\", \"Color3\"])),\n    ('replace_states', LeftJoinReplace(values_dict=states_dict,\n                                     variables=[\"State\"])),\n    ('replace_by_strings', FunctionTransformer(func=replace_integers_by_strings)),\n    ('has_name', FunctionTransformer(func=has_significant_name)),\n    ('pure_breed', FunctionTransformer(func=has_pure_breed)),\n    ('breed_matches_fur_length', FunctionTransformer(func=breed_matches_fur_length)),\n    ('impute_breed', BreedImputer()),\n    ('include_prof_im_metadata', IncludeProfileImageMetadata(profile_image_metadata)),\n    ('correct_wrong_type', CorrectWrongType(breeds)),\n    ('encode_breed', BreedEncoding(enc_type=\"target_and_frequency\")),\n    ('ordinal_vars_encoder', OrdinalVariableEncoder(columns=[\"MaturitySize\", \"FurLength\", \"Health\"],\n                                enc_type=\"ordinal\", mapping=ordinal_vars_mapping)),\n    ('state_gdp', ReplaceState(gdp_per_capita=gdp_per_capita, impute_nan_value=46450)),\n    ('rescuer_count', ReplaceRescuerID()),\n    ('discretizer', CustomDiscretizer(bins_age=-1, quantity=False, fee=False, video_amt=False,\n                                     photo_amt=False)),\n    ('description_length', FunctionTransformer(func=include_description_length)),\n    ('include_desc_metadata', IncludeDescriptionMetadata(description_metadata=description_metadata)),\n    ('correct_desc_language', CorrectDescriptionLanguage()),\n    ('one_hot_encoder', CustomOneHotEncoder(columns=[\"Gender\", \"Color1\", \"Color2\",\n                            \"Color3\", \"Vaccinated\", \"Dewormed\", \"Sterilized\", \"DescriptionLanguage\"])),\n    ('include_prof_im_properties', IncludeProfileImageProperties(profile_image_properties)),\n    ('drop_columns', ColumnRemover(columns=columns_to_be_removed_image_feats_eval)),\n    ('round_im_dims_aspect_ratio', FunctionTransformer(func=include_aspect_ratio)),\n    None,\n    ('drop_petid', ColumnRemover(columns=[\"PetID\"])),\n    ('useless_vars_remover', UselessVariablesRemover(tolerance=0.000001)),\n    ('impute_malay_desc_missing_prof_im_props', CustomIterativeImputer()),\n    ('custom_standard_scaler', CustomStandardScaler(numeric_columns_image_feats_eval))\n]\n\nevaluation_results = pd.DataFrame([], columns=[\"Model description\",\n                        \"Average fit time\", \"Average accuracy\", \"Average QWK\",\n                        \"Single split accuracy\", \"Single split QWK\"])\n\nimage_features_dataframes = {\n    \"regression_ensemble_model_layer-64_single\": image_features_regression_model_ensemble_layer_64_features,\n    \"regression_ensemble_model_layer-64_single_agg_mean_sum_var\": aggregated_image_features_regression_model_ensemble_layer_64_features,\n    \"regression_ensemble_model_layer-16_single\": image_features_regression_model_ensemble_layer_16_features,\n    \"regression_ensemble_model_layer-16_single_agg_mean_sum_var\": aggregated_image_features_regression_model_ensemble_layer_16_features,\n    \"regression_ensemble_2_model_layer-64_single\": image_features_regression_model_ensemble_2_layer_64_features,\n    \"regression_ensemble_2_model_layer-64_single_agg_mean_sum_var\": aggregated_image_features_regression_model_ensemble_2_layer_64_features,\n    \"regression_ensemble_2_model_layer-16_single\": image_features_regression_model_ensemble_2_layer_16_features,\n    \"regression_ensemble_2_model_layer-16_single_agg_mean_sum_var\": aggregated_image_features_regression_model_ensemble_2_layer_16_features\n}\n\nfor features_description, loaded_features in image_features_dataframes.items():\n    ife = ImageFeatureExtractor(construct_from_cnn_backbone=False,\n                                loaded_features=loaded_features,\n                                multiple_instances_per_petid=False)\n    pipeline_transformers[-5] = ('image_features_extractor', ife)\n\n    model = xgb.XGBClassifier(eval_metric='mlogloss', random_state=seed, n_jobs=-1,\n                          use_label_encoder=False)\n    model_description = f\"XGBClassifier, DenseNet121, {features_description}\"\n    print(f\"\\n\\n*************** {model_description} ***************\")\n    avg_fit_time, avg_accuracy, avg_QWK = evaluate_model(\n                Pipeline(steps=pipeline_transformers + [('model', model)]),\n                cv, X, y, model_type=\"classification\", display_results=True,\n                display_plots=False)\n\n    single_accuracy, single_QWK = evaluate_model_single_split(\n                Pipeline(steps=pipeline_transformers + [('model', model)]),\n                X_train_CNN, X_val_CNN, y_train_CNN, y_val_CNN,\n                model_type=\"classification\", display_results=True)\n\n    evaluation_results = evaluation_results.append({\n        \"Model description\": model_description,\n        \"Average fit time\": avg_fit_time,\n        \"Average accuracy\": avg_accuracy,\n        \"Average QWK\": avg_QWK,\n        \"Single split accuracy\": single_accuracy,\n        \"Single split QWK\": single_QWK\n    }, ignore_index=True)\n    \n\npd.set_option('display.max_colwidth', None)\ndisplay(evaluation_results)\npd.set_option('display.max_colwidth', 50)\nto_latex(evaluation_results, \"evaluation_results_XGBClassifier_DenseNet121_regression_ensemble_single_and_aggregation_multiple_images\")","metadata":{"execution":{"iopub.status.busy":"2021-06-22T20:31:30.762847Z","iopub.execute_input":"2021-06-22T20:31:30.763127Z","iopub.status.idle":"2021-06-22T20:49:22.558088Z","shell.execute_reply.started":"2021-06-22T20:31:30.763099Z","shell.execute_reply":"2021-06-22T20:49:22.557261Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The aggregation of features from every pet profile's images extracted from the layer 16 of ensemble 2 outclasses all the other features extracted from different models in the single split validation accuracy and QWK.\n\n**For the evaluation of all the extracted features, check Version 76.**","metadata":{}},{"cell_type":"code","source":"ife = ImageFeatureExtractor(\n    construct_from_cnn_backbone=False,\n    loaded_features=aggregated_image_features_regression_model_ensemble_2_layer_16_features\n)\n\npipeline_5_transformers = [\n    ('replace_breeds', LeftJoinReplace(values_dict=breeds_dict,\n                                      variables=[\"Breed1\", \"Breed2\"])),\n    ('replace_colors', LeftJoinReplace(values_dict=colors_dict,\n                                      variables=[\"Color1\", \"Color2\", \"Color3\"])),\n    ('replace_states', LeftJoinReplace(values_dict=states_dict,\n                                     variables=[\"State\"])),\n    ('replace_by_strings', FunctionTransformer(func=replace_integers_by_strings)),\n    ('has_name', FunctionTransformer(func=has_significant_name)),\n    ('pure_breed', FunctionTransformer(func=has_pure_breed)),\n    ('breed_matches_fur_length', FunctionTransformer(func=breed_matches_fur_length)),\n    ('impute_breed', BreedImputer()),\n    ('include_prof_im_metadata', IncludeProfileImageMetadata(profile_image_metadata)),\n    ('correct_wrong_type', CorrectWrongType(breeds)),\n    ('encode_breed', BreedEncoding(enc_type=\"target_and_frequency\")),\n    ('ordinal_vars_encoder', OrdinalVariableEncoder(columns=[\"MaturitySize\", \"FurLength\", \"Health\"],\n                                enc_type=\"ordinal\", mapping=ordinal_vars_mapping)),\n    ('state_gdp', ReplaceState(gdp_per_capita=gdp_per_capita, impute_nan_value=46450)),\n    ('rescuer_count', ReplaceRescuerID()),\n    ('discretizer', CustomDiscretizer(bins_age=-1, quantity=False, fee=False, video_amt=False,\n                                     photo_amt=False)),\n    ('description_length', FunctionTransformer(func=include_description_length)),\n    ('include_desc_metadata', IncludeDescriptionMetadata(description_metadata=description_metadata)),\n    ('correct_desc_language', CorrectDescriptionLanguage()),\n    ('one_hot_encoder', CustomOneHotEncoder(columns=[\"Gender\", \"Color1\", \"Color2\",\n                            \"Color3\", \"Vaccinated\", \"Dewormed\", \"Sterilized\", \"DescriptionLanguage\"])),\n    ('include_prof_im_properties', IncludeProfileImageProperties(profile_image_properties)),\n    ('drop_columns', ColumnRemover(columns=columns_to_be_removed_image_feats_eval)),\n    ('round_im_dims_aspect_ratio', FunctionTransformer(func=include_aspect_ratio)),\n    ('image_features_extractor', ife),\n    ('drop_petid', ColumnRemover(columns=[\"PetID\"])),\n    ('useless_vars_remover', UselessVariablesRemover(tolerance=0.000001)),\n    ('impute_malay_desc_missing_prof_im_props', CustomIterativeImputer()),\n    ('custom_standard_scaler', CustomStandardScaler(numeric_columns_image_feats_eval))\n]\n\n\nxgb_classifier = xgb.XGBClassifier(eval_metric='mlogloss', random_state=seed, n_jobs=-1,\n                          use_label_encoder=False)\n\nrandom_forest_clf = RandomForestClassifier(n_jobs=-1, random_state=seed)\n\nsvc_clf = SVC(break_ties=True, probability=True, random_state=seed)\n\nlogistic_regression = LogisticRegression(n_jobs=-1, random_state=seed)\n\nmodels = {\n    \"XGBClassifier\": xgb_classifier,\n    \"RandomForestClassifier\": random_forest_clf,\n    \"SVC (rbf kernel)\": svc_clf,\n    \"Logistic Regression\": logistic_regression\n}\n\nglobal_evaluation_results_5 = pd.DataFrame(\n    [],\n    columns=[\"Pipeline\", \"Model\", \"Average fit time\", \"Average accuracy\",\n             \"Average QWK\", \"Single split accuracy\", \"Single split QWK\"]\n)\n\nfor model_desc, model in models.items():\n    print(f\"--------------------- MODEL: {model_desc} ---------------------\")\n    avg_fit_time, avg_acc, avg_qwk = evaluate_model(\n        Pipeline(steps=pipeline_5_transformers + [('model', model)]),\n        cv, X, y, model_type=\"classification\")\n    \n    single_accuracy, single_QWK = evaluate_model_single_split(\n                Pipeline(steps=pipeline_5_transformers + [('model', model)]),\n                X_train_CNN, X_val_CNN, y_train_CNN, y_val_CNN,\n                model_type=\"classification\", display_results=True)\n    \n    global_evaluation_results_5 = global_evaluation_results_5.append({\n        \"Pipeline\": 5,\n        \"Model\": model_desc,\n        \"Average fit time\": avg_fit_time,\n        \"Average accuracy\": avg_acc,\n        \"Average QWK\": avg_qwk,\n        \"Single split accuracy\": single_accuracy,\n        \"Single split QWK\": single_QWK\n    }, ignore_index=True)","metadata":{"execution":{"iopub.status.busy":"2021-06-28T09:36:05.784018Z","iopub.execute_input":"2021-06-28T09:36:05.784353Z","iopub.status.idle":"2021-06-28T09:51:47.546883Z","shell.execute_reply.started":"2021-06-28T09:36:05.784324Z","shell.execute_reply":"2021-06-28T09:51:47.545852Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"display(global_evaluation_results_5)","metadata":{"execution":{"iopub.status.busy":"2021-06-28T09:51:47.55067Z","iopub.execute_input":"2021-06-28T09:51:47.550968Z","iopub.status.idle":"2021-06-28T09:51:47.565726Z","shell.execute_reply.started":"2021-06-28T09:51:47.550935Z","shell.execute_reply":"2021-06-28T09:51:47.564886Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Text processing","metadata":{}},{"cell_type":"markdown","source":"https://github.com/ssut/py-googletrans, https://github.com/ssut/py-googletrans/issues/121, https://github.com/ssut/py-googletrans/issues/280, https://py-googletrans.readthedocs.io/en/latest/","metadata":{}},{"cell_type":"code","source":"!pip install --upgrade language_tool_python\nimport language_tool_python","metadata":{"execution":{"iopub.status.busy":"2021-06-29T07:27:25.294312Z","iopub.execute_input":"2021-06-29T07:27:25.294739Z","iopub.status.idle":"2021-06-29T07:27:33.521503Z","shell.execute_reply.started":"2021-06-29T07:27:25.294697Z","shell.execute_reply":"2021-06-29T07:27:33.520478Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!cp -r ../input/tfg-pet-adoption-data/pycontractions-master/pycontractions-master/* ./\n!python setup.py install\nfrom pycontractions import Contractions","metadata":{"execution":{"iopub.status.busy":"2021-06-29T07:27:33.524183Z","iopub.execute_input":"2021-06-29T07:27:33.524506Z","iopub.status.idle":"2021-06-29T07:27:36.973772Z","shell.execute_reply.started":"2021-06-29T07:27:33.524472Z","shell.execute_reply":"2021-06-29T07:27:36.972387Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install emoji --upgrade\n!pip install googletrans==3.1.0a0\n\nimport emoji\nimport googletrans\nfrom gensim.test.utils import datapath, get_tmpfile\nfrom gensim.models import KeyedVectors\nfrom gensim.scripts.glove2word2vec import glove2word2vec\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer","metadata":{"execution":{"iopub.status.busy":"2021-06-29T07:27:36.975434Z","iopub.execute_input":"2021-06-29T07:27:36.975786Z","iopub.status.idle":"2021-06-29T07:27:57.733553Z","shell.execute_reply.started":"2021-06-29T07:27:36.97575Z","shell.execute_reply":"2021-06-29T07:27:57.732524Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are two options to perform TF-IDF: one of such process for each language (some of them are very uncommon, so this option can be discarded) or translating the non-English descriptions to English:","metadata":{}},{"cell_type":"code","source":"translator = googletrans.Translator(raise_exception=True)\ntranslations = translator.translate([\n    \"cute cat and adorable I just adopt him a few month back but she actually not very well adapt with my other cats, looking forward to responsible adopter who really love cats. p/s: there is macam ada isi lebih dekat ekor dia tapi kami dah check vet dan dia ni sihat. :) if you interested just call me at\",\n    \"Jumpa dekat kawasan semak, elsa baru sahaja beranak. Now her kittens are already 6 months. Trying to search for new owner that can give them love and attention. I cant afford to take care of them anymore as i already have 10 cats.\",\n    \"Jumpa depan pintu rumah.. Tak tahu siapa tinggalkan.. Sangat baik dan sopan.. Sudah divaksin, vitamin, cacing dan kurap.\"\n], src=\"ms\", dest=\"en\")\n\n\nfor translation in translations:\n    print(f\"{translation.origin} \\nvvvvvvvvvv\\n {translation.text}\\n\\n\")","metadata":{"execution":{"iopub.status.busy":"2021-06-29T07:27:57.735205Z","iopub.execute_input":"2021-06-29T07:27:57.735545Z","iopub.status.idle":"2021-06-29T07:27:59.21995Z","shell.execute_reply.started":"2021-06-29T07:27:57.735511Z","shell.execute_reply":"2021-06-29T07:27:59.218308Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pipeline_transformers = [\n    ('include_desc_metadata', IncludeDescriptionMetadata(description_metadata=description_metadata)),\n    ('correct_desc_language', CorrectDescriptionLanguage()),\n]\n\npipeline = Pipeline(steps=pipeline_transformers)\nwith warnings.catch_warnings():\n    warnings.simplefilter(\"ignore\")\n    result = pipeline.fit_transform(X,y)\n\npets_descriptions = result[[\"PetID\", \"DescriptionLanguage\", \"Description\"]]","metadata":{"execution":{"iopub.status.busy":"2021-06-29T07:27:59.224607Z","iopub.execute_input":"2021-06-29T07:27:59.225596Z","iopub.status.idle":"2021-06-29T07:27:59.299006Z","shell.execute_reply.started":"2021-06-29T07:27:59.225544Z","shell.execute_reply":"2021-06-29T07:27:59.297944Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pets_descriptions[\"DescriptionLanguage\"].value_counts(dropna=False)","metadata":{"execution":{"iopub.status.busy":"2021-06-29T07:27:59.301608Z","iopub.execute_input":"2021-06-29T07:27:59.302584Z","iopub.status.idle":"2021-06-29T07:27:59.318083Z","shell.execute_reply.started":"2021-06-29T07:27:59.302528Z","shell.execute_reply":"2021-06-29T07:27:59.31701Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**https://github.com/carpedm20/emoji, https://stackoverflow.com/a/51785357, https://stackoverflow.com/a/43146653**","metadata":{}},{"cell_type":"code","source":"def remove_emoji(text):\n    return emoji.get_emoji_regexp().sub(u'', text)\n\ndef extract_emojis(s):\n    return ''.join(c for c in s if c in emoji.UNICODE_EMOJI['en'])","metadata":{"execution":{"iopub.status.busy":"2021-06-29T07:27:59.319781Z","iopub.execute_input":"2021-06-29T07:27:59.320224Z","iopub.status.idle":"2021-06-29T07:27:59.333301Z","shell.execute_reply.started":"2021-06-29T07:27:59.32019Z","shell.execute_reply":"2021-06-29T07:27:59.332267Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"chinese_descriptions = pets_descriptions.loc[\n                pets_descriptions[\"DescriptionLanguage\"] == \"zh\", \"Description\"].tolist()","metadata":{"execution":{"iopub.status.busy":"2021-06-29T07:27:59.335013Z","iopub.execute_input":"2021-06-29T07:27:59.335461Z","iopub.status.idle":"2021-06-29T07:27:59.351898Z","shell.execute_reply.started":"2021-06-29T07:27:59.335426Z","shell.execute_reply":"2021-06-29T07:27:59.350737Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for desc in chinese_descriptions:\n    if extract_emojis(desc) != '':\n        print(f\"{desc}\\n\")\n        print(remove_emoji(desc))\n        print(\"\\n\\n\")","metadata":{"execution":{"iopub.status.busy":"2021-06-29T07:27:59.353197Z","iopub.execute_input":"2021-06-29T07:27:59.35353Z","iopub.status.idle":"2021-06-29T07:27:59.712426Z","shell.execute_reply.started":"2021-06-29T07:27:59.353502Z","shell.execute_reply":"2021-06-29T07:27:59.710946Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_copy = train.copy() \ntrain_copy[\"DescriptionHasEmoji\"] = train_copy[\"Description\"].apply(lambda x: extract_emojis(str(x)) != '')\ntrain_copy[\"DescriptionHasEmoji\"].value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-06-29T07:27:59.714921Z","iopub.execute_input":"2021-06-29T07:27:59.715418Z","iopub.status.idle":"2021-06-29T07:28:00.271849Z","shell.execute_reply.started":"2021-06-29T07:27:59.715368Z","shell.execute_reply":"2021-06-29T07:28:00.270729Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"utils.plot_vert_barplot(train_copy, \"DescriptionHasEmoji\", target, figsize=(10,4), display_numbers=False)","metadata":{"execution":{"iopub.status.busy":"2021-06-29T07:28:00.273195Z","iopub.execute_input":"2021-06-29T07:28:00.273797Z","iopub.status.idle":"2021-06-29T07:28:00.583311Z","shell.execute_reply.started":"2021-06-29T07:28:00.273735Z","shell.execute_reply":"2021-06-29T07:28:00.582348Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"googletrans.LANGUAGES","metadata":{"execution":{"iopub.status.busy":"2021-06-29T07:28:00.584692Z","iopub.execute_input":"2021-06-29T07:28:00.584981Z","iopub.status.idle":"2021-06-29T07:28:00.597039Z","shell.execute_reply.started":"2021-06-29T07:28:00.584954Z","shell.execute_reply":"2021-06-29T07:28:00.595824Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class DescriptionTransformer(BaseEstimator, TransformerMixin):\n    def __init__(self, transformations_df=None, kv_model=None, save=False, debug=False):\n        self.transformations_df = transformations_df\n        self.kv_model = kv_model\n        self.save = save\n        self.debug = debug\n    \n    \n    def remove_text_emojis(self, text):\n        return emoji.get_emoji_regexp().sub(u'', text)\n\n    def remove_all_emojis(self):\n        self.transformations_df[\"Description\"] = self.transformations_df[\"Description\"].apply(lambda x: x if x is np.nan else self.remove_text_emojis(str(x)))\n    \n    \n    def get_descriptions_translations(self, X):        \n        translator = googletrans.Translator(raise_exception=True)\n        self.transformations_df[\"translation\"] = ''\n        \n        for language in translations_df[\"DescriptionLanguage\"].unique():\n            if language is np.nan:\n                continue\n            if language == \"en\":\n                condition = self.transformations_df[\"DescriptionLanguage\"] == language\n                self.transformations_df.loc[condition, \"translation\"] = \\\n                    self.transformations_df.loc[condition, \"Description\"]\n                continue\n            \n            if language == \"others\":\n                src = \"auto\"\n            elif \"zh\" in language:\n                src = \"zh-cn\"\n            else:\n                src = language\n                \n            descriptions = self.transformations_df.loc[\n                self.transformations_df[\"DescriptionLanguage\"] == language, \"Description\"].tolist()\n            \n            if self.debug:\n                print(f\"Translating {len(descriptions)} descriptions, src='{src}', dest='en'\")\n            \n            try:\n                translations = translator.translate(descriptions, src=src, dest=\"en\")\n                self.transformations_df.loc[self.transformations_df[\"DescriptionLanguage\"] == language,\n                    \"translation\"] = list(map(lambda x: x.text, translations))\n            except Exception as e:\n                print(e)\n    \n    \n    def expand_contractions(self):\n        cont = Contractions(kv_model=self.kv_model)\n        cont.load_models()\n        self.transformations_df[\"expanded\"] = ''\n        self.transformations_df[\"expanded\"] = pd.Series(\n            list(cont.expand_texts(self.transformations_df[\"translation\"].values, precise=False)),\n            index=self.transformations_df.index.copy())\n    \n    \n    # Replace all punctuation symbols (except \" ' \") by a whitespace\n    def remove_punctuation(self, text, translator=str.maketrans(\n            string.punctuation.replace(\"'\", \"\"), \" \"*(len(string.punctuation)-1))):\n        return text.translate(translator)\n    \n    \n    def fit(self, X, y):\n        self.transformations_df = self.transformations_df.copy()\n        return self\n    \n    \n    def transform(self, X, y=None):\n        X = X.copy()\n        X[\"Description\"] = X[\"Description\"].replace(np.nan, '')\n        if self.transformations_df is None:\n            self.transformations_df = X.loc[[\"PetID\", \"DescriptionLanguage\", \"Description\"]].copy()\n            self.transformations_df.set_index(\"PetID\", inplace=True)\n            self.transformations_df[\"Description\"] = self.transformations_df[\"Description\"].replace(np.nan, '')\n            if self.debug:\n                print(\"Removing emojis from every description...\")\n            self.remove_all_emojis()\n            if self.debug:\n                print(\"Translating non-english descriptions...\")\n            self.get_descriptions_translations()\n            self.transformations_df[\"translation\"] = self.transformations_df[\"translation\"].apply(lambda x: str(x).replace(\"\", \"'\"))\n            if self.debug:\n                print(\"Expanding English language contractions...\")\n            self.expand_contractions()\n            if self.debug:\n                print(\"Removing punctuation...\")\n            self.transformations_df[\"expanded\"] = self.transformations_df[\"expanded\"].apply(lambda x: self.remove_punctuation(str(x)))\n        else:\n            self.transformations_df[\"Description\"] = self.transformations_df[\"Description\"].replace(np.nan, '')\n        \n        self.transformations_df[\"translation\"] = self.transformations_df[\"translation\"].replace(np.nan, '')\n        self.transformations_df[\"expanded\"] = self.transformations_df[\"expanded\"].replace(np.nan, '')\n        \n        if self.save:\n            if self.debug:\n                print(\"Saving transformations to .csv file...\")\n            self.transformations_df.to_csv(\"descriptions_transformations.csv\")\n\n        X = X.merge(self.transformations_df[\"expanded\"], left_index=False, right_index=True,\n                    left_on=\"PetID\", how=\"left\")\n        X.drop([\"Description\"], axis=1, inplace=True)\n        X.rename(columns={\"expanded\": \"Description\"}, inplace=True)\n        \n        return X","metadata":{"execution":{"iopub.status.busy":"2021-06-29T07:28:00.59879Z","iopub.execute_input":"2021-06-29T07:28:00.599238Z","iopub.status.idle":"2021-06-29T07:28:00.629622Z","shell.execute_reply.started":"2021-06-29T07:28:00.5992Z","shell.execute_reply":"2021-06-29T07:28:00.628233Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"translations_df = pd.read_csv(\"../input/tfg-pet-adoption-data/non-english_descriptions_translations.csv\", index_col=0)\ntranslations_df","metadata":{"execution":{"iopub.status.busy":"2021-06-29T07:28:00.631143Z","iopub.execute_input":"2021-06-29T07:28:00.631566Z","iopub.status.idle":"2021-06-29T07:28:00.705155Z","shell.execute_reply.started":"2021-06-29T07:28:00.631523Z","shell.execute_reply":"2021-06-29T07:28:00.704071Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"display(translations_df.loc[translations_df[\"DescriptionLanguage\"] == \"ms\"].sample(5,random_state=seed))\ndisplay(translations_df.loc[translations_df[\"DescriptionLanguage\"] == \"zh\"].sample(5,random_state=seed))\ndisplay(translations_df.loc[translations_df[\"DescriptionLanguage\"] == \"zh-Hant\"].sample(5,random_state=seed))","metadata":{"execution":{"iopub.status.busy":"2021-06-29T07:28:00.70756Z","iopub.execute_input":"2021-06-29T07:28:00.707886Z","iopub.status.idle":"2021-06-29T07:28:00.743848Z","shell.execute_reply.started":"2021-06-29T07:28:00.707855Z","shell.execute_reply":"2021-06-29T07:28:00.742726Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"https://github.com/SanDiegoMachineLearning/QuoraInsincere, https://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction, https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html, https://nlp.stanford.edu/IR-book/html/htmledition/sublinear-tf-scaling-1.html","metadata":{}},{"cell_type":"markdown","source":"Some of the emojis that are not detected as such are actually removed by CountVectorizer (which is used by TfidfVectorizer):","metadata":{}},{"cell_type":"code","source":"count_vectorizer = CountVectorizer(strip_accents='unicode')\ncount_vectorizer_analyzer = count_vectorizer.build_analyzer()\nexample_desc = translations_df.loc[\"f3fd77fe8\", \"translation\"]\nprint(example_desc)\nprint(\"\\nvvvvvvvvv\\n\")\nprint(*count_vectorizer_analyzer(example_desc), sep=', ')","metadata":{"execution":{"iopub.status.busy":"2021-06-29T07:28:00.745707Z","iopub.execute_input":"2021-06-29T07:28:00.746023Z","iopub.status.idle":"2021-06-29T07:28:00.765227Z","shell.execute_reply.started":"2021-06-29T07:28:00.745992Z","shell.execute_reply":"2021-06-29T07:28:00.76422Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"(why 'I' and 'a' are discarded: https://stackoverflow.com/questions/20717641/countvectorizer-i-not-showing-up-in-vectorized-text)\n\n**HOWEVER: Look how \"can't\" is left as \"can\" --> this is why we need to expand contractions**","metadata":{}},{"cell_type":"markdown","source":"https://medium.com/@lukei_3514/dealing-with-contractions-in-nlp-d6174300876b, https://github.com/ian-beaver/pycontractions, https://towardsdatascience.com/word-movers-distance-for-text-similarity-7492aeca71b0, https://radimrehurek.com/gensim/models/word2vec.html, https://github.com/RaRe-Technologies/gensim-data, https://nlp.stanford.edu/pubs/glove.pdf, https://nlp.stanford.edu/projects/glove/\n\n**error during pip install pycontractions --> https://github.com/ian-beaver/pycontractions/issues/17, solved in https://github.com/Martin36/pycontractions (https://github.com/ian-beaver/pycontractions/pull/19, https://pypi.org/project/language-tool-python/#description), pull request not completed**","metadata":{}},{"cell_type":"markdown","source":"**Issue: https://github.com/facebookresearch/fastText/issues/171, pycontractions expects a binary file instead of a strings file (line 314 in https://github.com/Martin36/pycontractions/blob/master/pycontractions/contractions.py), so I have to initialize the model outside using glove -> word2vec format: https://radimrehurek.com/gensim/scripts/glove2word2vec.html**","metadata":{}},{"cell_type":"code","source":"_ = glove2word2vec(\"../input/glove-twitter/glove.twitter.27B.25d.txt\", \"w2v_glove.twitter.27B.25d.txt\")\nkv_model = KeyedVectors.load_word2vec_format(\"./w2v_glove.twitter.27B.25d.txt\", binary=False)","metadata":{"execution":{"iopub.status.busy":"2021-06-29T07:28:00.767405Z","iopub.execute_input":"2021-06-29T07:28:00.767905Z","iopub.status.idle":"2021-06-29T07:29:12.753277Z","shell.execute_reply.started":"2021-06-29T07:28:00.76786Z","shell.execute_reply":"2021-06-29T07:29:12.75217Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cont = Contractions(kv_model=kv_model)\ncont.load_models()","metadata":{"execution":{"iopub.status.busy":"2021-06-29T07:29:12.754759Z","iopub.execute_input":"2021-06-29T07:29:12.75506Z","iopub.status.idle":"2021-06-29T07:29:56.309427Z","shell.execute_reply.started":"2021-06-29T07:29:12.755033Z","shell.execute_reply":"2021-06-29T07:29:56.306588Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"descriptions_sample = translations_df.sample(5, random_state=seed)\ndescriptions_sample[\"translation\"].values","metadata":{"execution":{"iopub.status.busy":"2021-06-29T07:29:56.312117Z","iopub.execute_input":"2021-06-29T07:29:56.312579Z","iopub.status.idle":"2021-06-29T07:29:56.329054Z","shell.execute_reply.started":"2021-06-29T07:29:56.31253Z","shell.execute_reply":"2021-06-29T07:29:56.32765Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"list(cont.expand_texts(descriptions_sample[\"translation\"].values, precise=False))","metadata":{"execution":{"iopub.status.busy":"2021-06-29T07:29:56.335337Z","iopub.execute_input":"2021-06-29T07:29:56.335877Z","iopub.status.idle":"2021-06-29T07:30:07.342942Z","shell.execute_reply.started":"2021-06-29T07:29:56.335826Z","shell.execute_reply":"2021-06-29T07:30:07.341936Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# desc_transformer = DescriptionTransformer(kv_model=kv_model, save=True, debug=True)\n# expanded_descs_df = desc_transformer.fit_transform(pets_descriptions, y=[])","metadata":{"execution":{"iopub.status.busy":"2021-06-29T07:30:07.344562Z","iopub.execute_input":"2021-06-29T07:30:07.344885Z","iopub.status.idle":"2021-06-29T07:30:07.351151Z","shell.execute_reply.started":"2021-06-29T07:30:07.344856Z","shell.execute_reply":"2021-06-29T07:30:07.348926Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"transformations_df = pd.read_csv(\"../input/tfg-pet-adoption-data/descriptions_transformations.csv\", index_col=0)\ntransformations_df[\"Description\"] = transformations_df[\"Description\"].replace(np.nan, '')\ntransformations_df[\"translation\"] = transformations_df[\"translation\"].replace(np.nan, '')\ntransformations_df[\"expanded\"] = transformations_df[\"expanded\"].replace(np.nan, '')\ntransformations_df","metadata":{"execution":{"iopub.status.busy":"2021-06-29T07:30:07.352714Z","iopub.execute_input":"2021-06-29T07:30:07.353226Z","iopub.status.idle":"2021-06-29T07:30:08.681176Z","shell.execute_reply.started":"2021-06-29T07:30:07.353181Z","shell.execute_reply":"2021-06-29T07:30:08.680267Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"transformations_df.loc[\"ef14861df\", \"Description\"]","metadata":{"execution":{"iopub.status.busy":"2021-06-29T07:30:08.682962Z","iopub.execute_input":"2021-06-29T07:30:08.683312Z","iopub.status.idle":"2021-06-29T07:30:08.691999Z","shell.execute_reply.started":"2021-06-29T07:30:08.683282Z","shell.execute_reply":"2021-06-29T07:30:08.691164Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"transformations_df.loc[\"ef14861df\", \"translation\"]","metadata":{"execution":{"iopub.status.busy":"2021-06-29T07:30:08.693494Z","iopub.execute_input":"2021-06-29T07:30:08.694187Z","iopub.status.idle":"2021-06-29T07:30:08.712214Z","shell.execute_reply.started":"2021-06-29T07:30:08.694138Z","shell.execute_reply":"2021-06-29T07:30:08.711069Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"transformations_df.loc[\"ef14861df\", \"expanded\"]","metadata":{"execution":{"iopub.status.busy":"2021-06-29T07:30:08.714245Z","iopub.execute_input":"2021-06-29T07:30:08.714732Z","iopub.status.idle":"2021-06-29T07:30:08.73244Z","shell.execute_reply.started":"2021-06-29T07:30:08.714656Z","shell.execute_reply":"2021-06-29T07:30:08.731497Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Need to replace symbol \" **** \" by \" **'** \", since it cannot be detected as a contraction:","metadata":{}},{"cell_type":"code","source":"num_symbol = 0\nfor index, row in transformations_df.iterrows():\n    if \"\" in str(row[\"Description\"]):\n        num_symbol += 1\n        if num_symbol < 5:\n            print(row[\"Description\"] + \"\\n\")\nnum_symbol","metadata":{"execution":{"iopub.status.busy":"2021-06-29T07:30:08.734182Z","iopub.execute_input":"2021-06-29T07:30:08.734525Z","iopub.status.idle":"2021-06-29T07:30:10.148086Z","shell.execute_reply.started":"2021-06-29T07:30:08.734491Z","shell.execute_reply":"2021-06-29T07:30:10.147075Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Replace was done using the code below, it is already included in the transformer but as expanding takes a long time, I just searched those descriptions or translations with this symbol (more than 141 replacings were done as some translations had also this symbol):","metadata":{}},{"cell_type":"code","source":"# for index, row in tqdm(transformations_df.iterrows()):\n#     pet_id = index\n#     if \"\" in str(row[\"translation\"]):\n#         transformations_df.loc[pet_id, \"translation\"] = row[\"translation\"].replace(\"\", \"'\")\n#         transformations_df.loc[pet_id, \"expanded\"] = list(cont.expand_texts(\n#             [transformations_df.loc[pet_id, \"translation\"]], precise=False))[0]\n# transformations_df.to_csv(\"descriptions_transformations.csv\")","metadata":{"execution":{"iopub.status.busy":"2021-06-29T07:30:10.149891Z","iopub.execute_input":"2021-06-29T07:30:10.15033Z","iopub.status.idle":"2021-06-29T07:30:10.156391Z","shell.execute_reply.started":"2021-06-29T07:30:10.150284Z","shell.execute_reply":"2021-06-29T07:30:10.154933Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Summing up, current transformations: replace np.nan descriptions by '', remove emojis, translate non-English descriptions, expand English contractions using word embeddings and Word Mover's Distance (pycontractions), replace punctuation (except \" ' \", as if the previous step didn't replace it, it was because it indicates genitive or other circunstances).**","metadata":{}},{"cell_type":"markdown","source":"### TF-IDF","metadata":{}},{"cell_type":"code","source":"class CustomTfidfVectorizer(BaseEstimator, TransformerMixin):\n    def __init__(self, ngram_range=(1,1), max_df=1.0, min_df=1, sublinear_tf=False,\n                 svd_n_components=16, seed=seed, debug=False):\n        self.ngram_range = ngram_range\n        self.max_df = max_df\n        self.min_df = min_df\n        self.sublinear_tf = sublinear_tf\n        self.svd_n_components = svd_n_components\n        self.seed = seed\n        self.debug = debug\n        \n    \n    def fit(self, X, y):\n        self.tfidf_vectorizer = TfidfVectorizer(strip_accents='unicode',\n                                                ngram_range=self.ngram_range,\n                                                max_df=self.max_df,\n                                                min_df=self.min_df,\n                                                use_idf=True,\n                                                smooth_idf=True,\n                                                sublinear_tf=self.sublinear_tf)\n        self.svd = TruncatedSVD(n_components=self.svd_n_components,\n                                random_state=self.seed)\n        \n        corpus = X[\"Description\"] #.replace(np.nan, '').values # TfidfVectorizer does not accept np.nan values\n        if self.debug:\n            print(\"Fitting TfidfVectorizer...\")\n        corpus_transformed = self.tfidf_vectorizer.fit_transform(corpus)\n        if self.debug:\n            print(f\"Shape of transformed training corpus: {corpus_transformed.shape}\")\n            print(f\"Fitting TruncatedSVD, {self.svd_n_components} components, with transformed training corpus...\")\n        self.svd.fit(corpus_transformed)\n        \n        return self\n    \n    \n    def transform(self, X, y=None):\n        X = X.copy()\n        corpus = X[\"Description\"] #.replace(np.nan, '').values\n        if self.debug:\n            print(\"Transforming corpus with TfidfVectorizer...\")\n        corpus_transformed = self.tfidf_vectorizer.transform(corpus)\n        desc_columns = [f\"desc_{i}\" for i in range(self.svd_n_components)]\n        if self.debug:\n            display(corpus_transformed)\n            print(f\"Transformed corpus shape: {corpus_transformed.shape}\")\n            print(f\"Using fitted TruncatedSVD on transformed corpus\")\n        desc_features_df = pd.DataFrame(self.svd.transform(corpus_transformed),\n                                        index=X.index.copy(),\n                                        columns=desc_columns)\n        X = X.merge(desc_features_df, left_index=True, right_index=True, how=\"left\")\n        return X","metadata":{"execution":{"iopub.status.busy":"2021-06-29T07:30:10.158308Z","iopub.execute_input":"2021-06-29T07:30:10.158856Z","iopub.status.idle":"2021-06-29T07:30:10.179598Z","shell.execute_reply.started":"2021-06-29T07:30:10.158804Z","shell.execute_reply":"2021-06-29T07:30:10.177616Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"columns_to_be_removed_desc_feats_eval = [\"Name\", \"Breed1\", \"Breed2\", \"Gender\", \"Color1\",\n                                          \"Color2\", \"Color3\", \"Vaccinated\", \"Dewormed\",\n                                          \"Sterilized\", \"State\", \"RescuerID\",\n                                          \"MaturitySize\", \"FurLength\", \"Health\",\n                                          \"ImageMetadataDescription\"]\n\nnumeric_columns_desc_feats_eval = [\"Age\", \"Quantity\", \"Fee\", \"VideoAmt\", \"PhotoAmt\",\n                                    \"StateGDP\", \"RescuerCount\", \"DescriptionLength\"]","metadata":{"execution":{"iopub.status.busy":"2021-06-29T07:30:10.181568Z","iopub.execute_input":"2021-06-29T07:30:10.181982Z","iopub.status.idle":"2021-06-29T07:30:10.196138Z","shell.execute_reply.started":"2021-06-29T07:30:10.181947Z","shell.execute_reply":"2021-06-29T07:30:10.194863Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's take the number of SVD componentes outside the hyperparameter tuning and select one value now in order to reduce the number of combination and the fit time:","metadata":{}},{"cell_type":"code","source":"ife = ImageFeatureExtractor(\n    construct_from_cnn_backbone=False,\n    loaded_features=aggregated_image_features_regression_model_ensemble_2_layer_16_features\n)\n\npipeline_transformers = [\n    ('replace_breeds', LeftJoinReplace(values_dict=breeds_dict,\n                                      variables=[\"Breed1\", \"Breed2\"])),\n    ('replace_colors', LeftJoinReplace(values_dict=colors_dict,\n                                      variables=[\"Color1\", \"Color2\", \"Color3\"])),\n    ('replace_states', LeftJoinReplace(values_dict=states_dict,\n                                     variables=[\"State\"])),\n    ('replace_by_strings', FunctionTransformer(func=replace_integers_by_strings)),\n    ('has_name', FunctionTransformer(func=has_significant_name)),\n    ('pure_breed', FunctionTransformer(func=has_pure_breed)),\n    ('breed_matches_fur_length', FunctionTransformer(func=breed_matches_fur_length)),\n    ('impute_breed', BreedImputer()),\n    ('include_prof_im_metadata', IncludeProfileImageMetadata(profile_image_metadata)),\n    ('correct_wrong_type', CorrectWrongType(breeds)),\n    ('encode_breed', BreedEncoding(enc_type=\"target_and_frequency\")),\n    ('ordinal_vars_encoder', OrdinalVariableEncoder(columns=[\"MaturitySize\", \"FurLength\", \"Health\"],\n                                enc_type=\"ordinal\", mapping=ordinal_vars_mapping)),\n    ('state_gdp', ReplaceState(gdp_per_capita=gdp_per_capita, impute_nan_value=46450)),\n    ('rescuer_count', ReplaceRescuerID()),\n    ('discretizer', CustomDiscretizer(bins_age=-1, quantity=False, fee=False, video_amt=False,\n                                     photo_amt=False)),\n    ('description_length', FunctionTransformer(func=include_description_length)),\n    ('include_desc_metadata', IncludeDescriptionMetadata(description_metadata=description_metadata)),\n    ('correct_desc_language', CorrectDescriptionLanguage()),\n    ('one_hot_encoder', CustomOneHotEncoder(columns=[\"Gender\", \"Color1\", \"Color2\",\n                            \"Color3\", \"Vaccinated\", \"Dewormed\", \"Sterilized\", \"DescriptionLanguage\"])),\n    ('include_prof_im_properties', IncludeProfileImageProperties(profile_image_properties)),\n    ('drop_columns', ColumnRemover(columns=columns_to_be_removed_desc_feats_eval)),\n    ('round_im_dims_aspect_ratio', FunctionTransformer(func=include_aspect_ratio)),\n    ('image_features_extractor', ife),\n    ('description_transformer', DescriptionTransformer(transformations_df=transformations_df)),\n    None,\n    ('drop_petid_desc', ColumnRemover(columns=[\"PetID\", \"DescriptionLanguage\", \"Description\"])),\n    ('useless_vars_remover', UselessVariablesRemover(tolerance=0.000001)),\n    ('impute_malay_desc_missing_prof_im_props', CustomIterativeImputer()),\n    ('custom_standard_scaler', CustomStandardScaler(numeric_columns_desc_feats_eval))\n]\n\nevaluation_results = pd.DataFrame([], columns=[\"Model description\",\n                        \"Average fit time\", \"Average accuracy\", \"Average QWK\",\n                        \"Single split accuracy\", \"Single split QWK\"])\n\nfor svd_n_components in [8, 16, 24, 32, 48]:\n    tfidf_vectorizer = CustomTfidfVectorizer(ngram_range=(1,2),\n                                             svd_n_components=svd_n_components,\n                                             seed=seed)\n    pipeline_transformers[-5] = ('tfidf_vectorizer', tfidf_vectorizer)\n\n    model = xgb.XGBClassifier(eval_metric='mlogloss', random_state=seed, n_jobs=-1,\n                          use_label_encoder=False)\n    model_description = f\"XGBClassifier, TF-IDF ngram (1,2) SVD-{svd_n_components}\"\n    print(f\"\\n\\n*************** {model_description} ***************\")\n    avg_fit_time, avg_accuracy, avg_QWK = evaluate_model(\n                Pipeline(steps=pipeline_transformers + [('model', model)]),\n                cv, X, y, model_type=\"classification\", display_results=True,\n                display_plots=False)\n\n    single_accuracy, single_QWK = evaluate_model_single_split(\n                Pipeline(steps=pipeline_transformers + [('model', model)]),\n                X_train_CNN, X_val_CNN, y_train_CNN, y_val_CNN,\n                model_type=\"classification\", display_results=True)\n\n    evaluation_results = evaluation_results.append({\n        \"Model description\": model_description,\n        \"Average fit time\": avg_fit_time,\n        \"Average accuracy\": avg_accuracy,\n        \"Average QWK\": avg_QWK,\n        \"Single split accuracy\": single_accuracy,\n        \"Single split QWK\": single_QWK\n    }, ignore_index=True)\n\n\npd.set_option('display.max_colwidth', None)\ndisplay(evaluation_results)\npd.set_option('display.max_colwidth', 50)\nto_latex(evaluation_results, \"evaluation_results_XGBClassifier_TFIDF_ngram-1-2_varying-SVD_n-components\")","metadata":{"execution":{"iopub.status.busy":"2021-06-28T09:54:24.476763Z","iopub.execute_input":"2021-06-28T09:54:24.477103Z","iopub.status.idle":"2021-06-28T10:18:49.325936Z","shell.execute_reply.started":"2021-06-28T09:54:24.477072Z","shell.execute_reply":"2021-06-28T10:18:49.325146Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We will fix the number of SVD components to 16 for the TF-IDF matrix.","metadata":{}},{"cell_type":"markdown","source":"### Pre-trained word embeddings + CNN","metadata":{}},{"cell_type":"markdown","source":"https://radimrehurek.com/gensim/models/word2vec.html\n\nhttps://towardsdatascience.com/word-embeddings-exploration-explanation-and-exploitation-with-code-in-python-5dac99d5d795\n\nhttps://www.kaggle.com/matleonard/word-vectors\n\nhttps://tfhub.dev/google/collections/universal-sentence-encoder/1, https://www.tensorflow.org/text/guide/word_embeddings (https://github.com/tensorflow/hub/issues/244, https://github.com/tensorflow/hub/issues/572)","metadata":{}},{"cell_type":"markdown","source":"**We will use this model: https://keras.io/examples/nlp/pretrained_word_embeddings/**","metadata":{}},{"cell_type":"code","source":"from keras.initializers import Constant\nfrom keras.layers import Conv1D, Embedding, GlobalMaxPooling1D, MaxPooling1D\nfrom keras.layers.experimental.preprocessing import TextVectorization\nfrom keras.wrappers.scikit_learn import KerasRegressor\nfrom sklearn.model_selection import RandomizedSearchCV","metadata":{"execution":{"iopub.status.busy":"2021-06-28T10:18:49.327808Z","iopub.execute_input":"2021-06-28T10:18:49.328234Z","iopub.status.idle":"2021-06-28T10:18:49.339654Z","shell.execute_reply.started":"2021-06-28T10:18:49.32819Z","shell.execute_reply":"2021-06-28T10:18:49.338598Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train_text = transformations_df.loc[X_train_CNN[\"PetID\"].values, \"expanded\"].copy().values\ny_train_text = y_train_CNN.copy().values\nX_val_text = transformations_df.loc[X_val_CNN[\"PetID\"].values, \"expanded\"].copy().values\ny_val_text = y_val_CNN.copy().values","metadata":{"execution":{"iopub.status.busy":"2021-06-28T10:18:49.34126Z","iopub.execute_input":"2021-06-28T10:18:49.341681Z","iopub.status.idle":"2021-06-28T10:18:49.35954Z","shell.execute_reply.started":"2021-06-28T10:18:49.34164Z","shell.execute_reply":"2021-06-28T10:18:49.358695Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(len(X_train_text))\nX_train_text[:5]","metadata":{"execution":{"iopub.status.busy":"2021-06-28T10:18:49.360548Z","iopub.execute_input":"2021-06-28T10:18:49.360943Z","iopub.status.idle":"2021-06-28T10:18:49.371549Z","shell.execute_reply.started":"2021-06-28T10:18:49.360913Z","shell.execute_reply":"2021-06-28T10:18:49.3705Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(len(y_train_text))\ny_train_text[:5]","metadata":{"execution":{"iopub.status.busy":"2021-06-28T10:18:49.372936Z","iopub.execute_input":"2021-06-28T10:18:49.373542Z","iopub.status.idle":"2021-06-28T10:18:49.386994Z","shell.execute_reply.started":"2021-06-28T10:18:49.373497Z","shell.execute_reply":"2021-06-28T10:18:49.385843Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(len(X_val_text))\nX_val_text[:5]","metadata":{"execution":{"iopub.status.busy":"2021-06-28T10:18:49.38857Z","iopub.execute_input":"2021-06-28T10:18:49.388978Z","iopub.status.idle":"2021-06-28T10:18:49.402557Z","shell.execute_reply.started":"2021-06-28T10:18:49.388935Z","shell.execute_reply":"2021-06-28T10:18:49.401413Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(len(y_val_text))\ny_val_text[:5]","metadata":{"execution":{"iopub.status.busy":"2021-06-28T10:18:49.403789Z","iopub.execute_input":"2021-06-28T10:18:49.404362Z","iopub.status.idle":"2021-06-28T10:18:49.416467Z","shell.execute_reply.started":"2021-06-28T10:18:49.404319Z","shell.execute_reply":"2021-06-28T10:18:49.415475Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_tokens_train_text = list(map(lambda x: len(x.split()), X_train_text))\nprint(f\"Mean number of tokens: {np.mean(num_tokens_train_text)}\")\nprint(f\"Max number of tokens: {np.max(num_tokens_train_text)}\")\nprint(f\"Median number of tokens: {np.median(num_tokens_train_text)}\")","metadata":{"execution":{"iopub.status.busy":"2021-06-28T10:18:49.417977Z","iopub.execute_input":"2021-06-28T10:18:49.418415Z","iopub.status.idle":"2021-06-28T10:18:49.484547Z","shell.execute_reply.started":"2021-06-28T10:18:49.418363Z","shell.execute_reply":"2021-06-28T10:18:49.483462Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vectorizer = TextVectorization(max_tokens=30000, output_sequence_length=200)\ntext_ds = tf.data.Dataset.from_tensor_slices(X_train_text).batch(128)\nvectorizer.adapt(text_ds)\nvectorizer.get_vocabulary()[:5]","metadata":{"execution":{"iopub.status.busy":"2021-06-28T10:18:49.485825Z","iopub.execute_input":"2021-06-28T10:18:49.486111Z","iopub.status.idle":"2021-06-28T10:18:50.302049Z","shell.execute_reply.started":"2021-06-28T10:18:49.48608Z","shell.execute_reply":"2021-06-28T10:18:50.301001Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"voc = vectorizer.get_vocabulary()\nword_index = dict(zip(voc, range(len(voc))))\n\nembeddings_index = {}\nwith open(\"../input/glove-twitter/glove.twitter.27B.100d.txt\") as f:\n    for line in f:\n        word, coefs = line.split(maxsplit=1)\n        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n        embeddings_index[word] = coefs\n\nprint(f\"Found {len(embeddings_index)} word vectors.\")","metadata":{"execution":{"iopub.status.busy":"2021-06-28T10:18:50.303396Z","iopub.execute_input":"2021-06-28T10:18:50.303674Z","iopub.status.idle":"2021-06-28T10:19:34.097296Z","shell.execute_reply.started":"2021-06-28T10:18:50.303647Z","shell.execute_reply":"2021-06-28T10:19:34.096333Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_tokens = len(voc) + 2\nembedding_dim = 100\nhits = 0\nmisses = 0\n\ncount_print = 0\n# Prepare embedding matrix\nembedding_matrix = np.zeros((num_tokens, embedding_dim))\nfor word, i in word_index.items():\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        # Words not found in embedding index will be all-zeros.\n        # This includes the representation for \"padding\" and \"OOV\"\n        embedding_matrix[i] = embedding_vector\n        hits += 1\n    else:\n        misses += 1\n        if count_print < 20:\n            print(word)\n            count_print += 1\n\nprint(f\"Converted {hits} words ({misses} misses)\")","metadata":{"execution":{"iopub.status.busy":"2021-06-28T10:19:34.098883Z","iopub.execute_input":"2021-06-28T10:19:34.099345Z","iopub.status.idle":"2021-06-28T10:19:34.160649Z","shell.execute_reply.started":"2021-06-28T10:19:34.099299Z","shell.execute_reply":"2021-06-28T10:19:34.159759Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"embedding_layer = Embedding(\n    num_tokens,\n    embedding_dim,\n    embeddings_initializer=Constant(embedding_matrix),\n    trainable=False,\n)","metadata":{"execution":{"iopub.status.busy":"2021-06-28T10:19:34.162094Z","iopub.execute_input":"2021-06-28T10:19:34.162401Z","iopub.status.idle":"2021-06-28T10:19:34.169331Z","shell.execute_reply.started":"2021-06-28T10:19:34.162371Z","shell.execute_reply":"2021-06-28T10:19:34.167995Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's try the same FC network we used in image features extraction at the end of the CNN:","metadata":{}},{"cell_type":"code","source":"int_sequences_input = Input(shape=(None,), dtype=\"int64\")\nembedded_sequences = embedding_layer(int_sequences_input)\nx = Conv1D(128, 5, activation=\"relu\")(embedded_sequences)\nx = MaxPooling1D(5)(x)\nx = Conv1D(128, 5, activation=\"relu\")(x)\nx = MaxPooling1D(5)(x)\nx = Conv1D(128, 5, activation=\"relu\")(x)\nx = GlobalMaxPooling1D()(x)\nx = Dense(128, activation=\"relu\")(x)\nx = Dropout(rate=0.5, seed=seed)(x)\nx = Dense(16, activation=\"relu\")(x)\npreds = Dense(1, activation=\"linear\")(x)\n\ncnn_word_embeddings_model = Model(int_sequences_input, preds)\n\noptimizer = Adam(lr=0.001)\nmetrics = [\"mean_absolute_error\"]\nloss = \"mean_squared_error\"\n    \ncnn_word_embeddings_model.compile(loss=loss, metrics=metrics, optimizer=optimizer)\n\ncnn_word_embeddings_model.summary()","metadata":{"execution":{"iopub.status.busy":"2021-06-28T10:19:34.171541Z","iopub.execute_input":"2021-06-28T10:19:34.171952Z","iopub.status.idle":"2021-06-28T10:19:34.317447Z","shell.execute_reply.started":"2021-06-28T10:19:34.171909Z","shell.execute_reply":"2021-06-28T10:19:34.316364Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train_text_index = vectorizer(np.array([[s] for s in X_train_text])).numpy()\nX_val_text_index = vectorizer(np.array([[s] for s in X_val_text])).numpy()\n\ny_train_text_index = np.array(y_train_text)\ny_val_text_index = np.array(y_val_text)","metadata":{"execution":{"iopub.status.busy":"2021-06-28T10:19:34.318473Z","iopub.execute_input":"2021-06-28T10:19:34.318732Z","iopub.status.idle":"2021-06-28T10:19:35.070283Z","shell.execute_reply.started":"2021-06-28T10:19:34.318705Z","shell.execute_reply":"2021-06-28T10:19:35.069366Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"epochs = 15\nbatch_size = 128\n\ncnn_word_embeddings_training_results = pd.DataFrame([], columns=[\"loss\",\n                        \"mean_absolute_error\", \"val_loss\",\n                        \"val_mean_absolute_error\"])\n\nearly_stopping = EarlyStopping(monitor='val_loss', restore_best_weights=True, patience=5)\nmodel_checkpoint = ModelCheckpoint(\n    'cnn_word_embeddings__{epoch:02d}-epochs_val_loss-{val_loss:02f}.h5',\n    monitor='val_loss', save_best_only=True)\n\ncnn_word_embeddings_history = cnn_word_embeddings_model.fit(\n    x=X_train_text_index,\n    y=y_train_text_index,\n    batch_size=batch_size,\n    epochs=epochs,\n    validation_data=(X_val_text_index, y_val_text_index),\n    callbacks=[model_checkpoint, early_stopping]\n)\n\ntrain_loss = cnn_word_embeddings_history.history[\"loss\"]\ntrain_mae = cnn_word_embeddings_history.history[\"mean_absolute_error\"]\nvalidation_loss = cnn_word_embeddings_history.history[\"val_loss\"]\nvalidation_mae = cnn_word_embeddings_history.history[\"val_mean_absolute_error\"]\n\nfor i in range(len(train_loss)):\n    cnn_word_embeddings_training_results = cnn_word_embeddings_training_results.append({\n        \"loss\": train_loss[i],\n        \"mean_absolute_error\": train_mae[i],\n        \"val_loss\": validation_loss[i],\n        \"val_mean_absolute_error\": validation_mae[i]\n    }, ignore_index=True)\n\ncnn_word_embeddings_training_results.to_csv(f\"cnn_word_embeddings__{epochs}-epochs.csv\", index=False)\nto_latex(cnn_word_embeddings_training_results, f\"cnn_word_embeddings__{epochs}-epochs\")\n\n# Zipping the checkpoints\n!mkdir cnn_word_embeddings\n!mv ./cnn_word_embeddings*.h5 cnn_word_embeddings\nshutil.make_archive(\"cnn_word_embeddings__checkpoints\", \"zip\", \"./cnn_word_embeddings\")","metadata":{"execution":{"iopub.status.busy":"2021-06-23T09:17:46.604076Z","iopub.execute_input":"2021-06-23T09:17:46.60447Z","iopub.status.idle":"2021-06-23T09:19:41.592081Z","shell.execute_reply.started":"2021-06-23T09:17:46.604434Z","shell.execute_reply":"2021-06-23T09:19:41.590782Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cnn_word_embeddings_training_results = pd.read_csv(\"../input/tfg-pet-adoption-data/cnn_word_embeddings__15-epochs.csv\")\nplot_history(cnn_word_embeddings_training_results, [\"loss\", \"mean_absolute_error\"], 1, 2, (12,3))","metadata":{"execution":{"iopub.status.busy":"2021-06-28T10:19:35.071773Z","iopub.execute_input":"2021-06-28T10:19:35.072469Z","iopub.status.idle":"2021-06-28T10:19:35.42218Z","shell.execute_reply.started":"2021-06-28T10:19:35.072423Z","shell.execute_reply":"2021-06-28T10:19:35.421315Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The model overfits, let's apply the idea proposed here: https://realpython.com/python-keras-text-classification/#hyperparameters-optimization in order to check different configurations of each layer:","metadata":{}},{"cell_type":"code","source":"def create_cnn_word_emb_model(num_filters, kernel_size, num_intermediate_conv_max,\n                              size_dense_1, dropout_rate, size_dense_2, lr):\n    int_sequences_input = Input(shape=(None,), dtype=\"int64\")\n    embedded_sequences = embedding_layer(int_sequences_input)\n    x = Conv1D(num_filters, kernel_size, activation=\"relu\")(embedded_sequences)\n    x = MaxPooling1D(kernel_size)(x)\n    for i in range(num_intermediate_conv_max):\n        x = Conv1D(num_filters, kernel_size, activation=\"relu\")(x)\n        x = MaxPooling1D(kernel_size)(x)\n    x = Conv1D(num_filters, kernel_size, activation=\"relu\")(x)\n    x = GlobalMaxPooling1D()(x)\n    x = Dense(size_dense_1, activation=\"relu\")(x)\n    x = Dropout(rate=dropout_rate, seed=seed)(x)\n    x = Dense(size_dense_2, activation=\"relu\")(x)\n    preds = Dense(1, activation=\"linear\")(x)\n\n    model = Model(int_sequences_input, preds)\n\n    optimizer = Adam(lr=lr)\n    metrics = [\"mean_absolute_error\"]\n    loss = \"mean_squared_error\"\n\n    model.compile(loss=loss, metrics=metrics, optimizer=optimizer)\n    return model","metadata":{"execution":{"iopub.status.busy":"2021-06-23T21:09:27.318719Z","iopub.execute_input":"2021-06-23T21:09:27.319034Z","iopub.status.idle":"2021-06-23T21:09:27.329673Z","shell.execute_reply.started":"2021-06-23T21:09:27.319003Z","shell.execute_reply":"2021-06-23T21:09:27.328259Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Parameter grid for grid search\nparam_grid = dict(num_filters=[64,128,192], kernel_size=[3,5], num_intermediate_conv_max=[0,1],\n                  size_dense_1=[64,128], dropout_rate=[0.25,0.5], size_dense_2=[16,32],\n                  lr=[0.001,0.0001])","metadata":{"execution":{"iopub.status.busy":"2021-06-23T21:09:27.331113Z","iopub.execute_input":"2021-06-23T21:09:27.331499Z","iopub.status.idle":"2021-06-23T21:09:27.343238Z","shell.execute_reply.started":"2021-06-23T21:09:27.33144Z","shell.execute_reply":"2021-06-23T21:09:27.342103Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The previous grid makes 192 possible combinations, so let's specify RandomizedSearchCV to select just 50 of them and run a stratified 4-CV inside X_train_text:","metadata":{}},{"cell_type":"code","source":"epochs = 6\nbatch_size = 128\n\nmodel = KerasRegressor(build_fn=create_cnn_word_emb_model,\n                        epochs=epochs, batch_size=128,\n                        verbose=False)\n\ngrid = RandomizedSearchCV(estimator=model, param_distributions=param_grid,\n                          cv=StratifiedKFold(n_splits=4), verbose=1,\n                          n_iter=50, random_state=seed)\n\nwith warnings.catch_warnings():\n    warnings.simplefilter(\"ignore\")\n    grid_result = grid.fit(X_train_text_index, y_train_text_index)\n\n# Evaluate testing set\ntest_accuracy = grid.score(X_val_text_index, y_val_text_index)\n\nrandom_search_results = pd.DataFrame.from_dict(grid.cv_results_)\nrandom_search_results.to_csv(\"random_search_cnn_word_emb_results.csv\")\n\ns = ('Best Loss: {:.4f}\\n{}\\nTest (X_val_text_index) Loss: {:.4f}\\n\\n')\noutput_string = s.format(\n    grid_result.best_score_,\n    grid_result.best_params_,\n    test_accuracy)\nprint(output_string)","metadata":{"execution":{"iopub.status.busy":"2021-06-23T21:09:27.358087Z","iopub.execute_input":"2021-06-23T21:09:27.358423Z","iopub.status.idle":"2021-06-23T23:56:24.553052Z","shell.execute_reply.started":"2021-06-23T21:09:27.358392Z","shell.execute_reply":"2021-06-23T23:56:24.552161Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"random_search_results = pd.read_csv(\"../input/tfg-pet-adoption-data/random_search_cnn_word_emb_results.csv\", index_col=0)\nrandom_search_results.sort_values([\"rank_test_score\"]).head(3)","metadata":{"execution":{"iopub.status.busy":"2021-06-28T10:19:35.423164Z","iopub.execute_input":"2021-06-28T10:19:35.423422Z","iopub.status.idle":"2021-06-28T10:19:35.456277Z","shell.execute_reply.started":"2021-06-28T10:19:35.423396Z","shell.execute_reply":"2021-06-28T10:19:35.45553Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"int_sequences_input = Input(shape=(None,), dtype=\"int64\")\nembedded_sequences = embedding_layer(int_sequences_input)\nx = Conv1D(64, 3, activation=\"relu\")(embedded_sequences)\nx = MaxPooling1D(3)(x)\nx = Conv1D(64, 3, activation=\"relu\")(x)\nx = MaxPooling1D(3)(x)\nx = Conv1D(64, 3, activation=\"relu\")(x)\nx = GlobalMaxPooling1D()(x)\nx = Dense(128, activation=\"relu\")(x)\nx = Dropout(rate=0.5, seed=seed)(x)\nx = Dense(16, activation=\"relu\")(x)\npreds = Dense(1, activation=\"linear\")(x)\n\ncnn_word_embeddings_model_hyper_params_tuned = Model(int_sequences_input, preds)\n\noptimizer = Adam(lr=0.001)\nmetrics = [\"mean_absolute_error\"]\nloss = \"mean_squared_error\"\n    \ncnn_word_embeddings_model_hyper_params_tuned.compile(loss=loss, metrics=metrics, optimizer=optimizer)\n\ncnn_word_embeddings_model_hyper_params_tuned.summary()","metadata":{"execution":{"iopub.status.busy":"2021-06-28T10:19:35.457233Z","iopub.execute_input":"2021-06-28T10:19:35.457612Z","iopub.status.idle":"2021-06-28T10:19:35.547Z","shell.execute_reply.started":"2021-06-28T10:19:35.457585Z","shell.execute_reply":"2021-06-28T10:19:35.54615Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"epochs = 30\nbatch_size = 128\n\ncnn_word_embeddings_training_results = pd.DataFrame([], columns=[\"loss\",\n                        \"mean_absolute_error\", \"val_loss\",\n                        \"val_mean_absolute_error\"])\n\nearly_stopping = EarlyStopping(monitor='val_loss', restore_best_weights=True, patience=5)\nmodel_checkpoint = ModelCheckpoint(\n    'cnn_word_embeddings__hyper-params-tuned__{epoch:02d}-epochs_val_loss-{val_loss:02f}.h5',\n    monitor='val_loss', save_best_only=True)\n\ncnn_word_embeddings_history = cnn_word_embeddings_model_hyper_params_tuned.fit(\n    x=X_train_text_index,\n    y=y_train_text_index,\n    batch_size=batch_size,\n    epochs=epochs,\n    validation_data=(X_val_text_index, y_val_text_index),\n    callbacks=[model_checkpoint, early_stopping]\n)\n\ntrain_loss = cnn_word_embeddings_history.history[\"loss\"]\ntrain_mae = cnn_word_embeddings_history.history[\"mean_absolute_error\"]\nvalidation_loss = cnn_word_embeddings_history.history[\"val_loss\"]\nvalidation_mae = cnn_word_embeddings_history.history[\"val_mean_absolute_error\"]\n\nfor i in range(len(train_loss)):\n    cnn_word_embeddings_training_results = cnn_word_embeddings_training_results.append({\n        \"loss\": train_loss[i],\n        \"mean_absolute_error\": train_mae[i],\n        \"val_loss\": validation_loss[i],\n        \"val_mean_absolute_error\": validation_mae[i]\n    }, ignore_index=True)\n\ncnn_word_embeddings_training_results.to_csv(f\"cnn_word_embeddings__hyper-params-tuned__{epochs}-epochs.csv\", index=False)\nto_latex(cnn_word_embeddings_training_results, f\"cnn_word_embeddings__hyper-params-tuned__{epochs}-epochs\")\n\n# Zipping the checkpoints\n!mkdir cnn_word_embeddings_hyper_params_tuned\n!mv ./cnn_word_embeddings__hyper*.h5 cnn_word_embeddings_hyper_params_tuned\nshutil.make_archive(\"cnn_word_embeddings__hyper-params-tuned__checkpoints\", \"zip\", \"./cnn_word_embeddings_hyper_params_tuned\")","metadata":{"execution":{"iopub.status.busy":"2021-06-24T00:14:03.162756Z","iopub.execute_input":"2021-06-24T00:14:03.163148Z","iopub.status.idle":"2021-06-24T00:15:42.474123Z","shell.execute_reply.started":"2021-06-24T00:14:03.163116Z","shell.execute_reply":"2021-06-24T00:15:42.472684Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cnn_word_embeddings_training_results = pd.read_csv(\"../input/tfg-pet-adoption-data/cnn_word_embeddings__hyper-params-tuned__30-epochs.csv\")\nplot_history(cnn_word_embeddings_training_results, [\"loss\", \"mean_absolute_error\"], 1, 2, (12,3))","metadata":{"execution":{"iopub.status.busy":"2021-06-28T10:19:35.547996Z","iopub.execute_input":"2021-06-28T10:19:35.548287Z","iopub.status.idle":"2021-06-28T10:19:35.845641Z","shell.execute_reply.started":"2021-06-28T10:19:35.548259Z","shell.execute_reply":"2021-06-28T10:19:35.844802Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The validation loss is still very high, and the best value didn't really improve the one of the original model. Let's check how much information the extracted features can give:","metadata":{}},{"cell_type":"code","source":"string_input = Input(shape=(1,), dtype=\"string\")\nx = vectorizer(string_input)\n\ncnn_word_embeddings_model_hyper_params_tuned_output_16 = Model(\n    inputs=cnn_word_embeddings_model_hyper_params_tuned.input,\n    outputs=cnn_word_embeddings_model_hyper_params_tuned.layers[-2].output\n)\n\npreds = cnn_word_embeddings_model_hyper_params_tuned_output_16(x)\nend_to_end_cnn_word_embeddings_model_hyper_params_tuned_output_16 = Model(string_input, preds)\nend_to_end_cnn_word_embeddings_model_hyper_params_tuned_output_16.save(\n    \"end_to_end_cnn_word_embeddings__hyper-params-tuned__layer-16__05-epochs_val_loss-1.356145.tf\",\n    save_format='tf')\nend_to_end_cnn_word_embeddings_model_hyper_params_tuned_output_16.summary()","metadata":{"execution":{"iopub.status.busy":"2021-06-24T00:21:47.602369Z","iopub.execute_input":"2021-06-24T00:21:47.602808Z","iopub.status.idle":"2021-06-24T00:22:11.057555Z","shell.execute_reply.started":"2021-06-24T00:21:47.602723Z","shell.execute_reply":"2021-06-24T00:22:11.056394Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"end_to_end_cnn_word_embeddings_model_hyper_params_tuned_output_16.layers[-1].layers","metadata":{"execution":{"iopub.status.busy":"2021-06-24T00:22:36.68793Z","iopub.execute_input":"2021-06-24T00:22:36.6883Z","iopub.status.idle":"2021-06-24T00:22:36.697231Z","shell.execute_reply.started":"2021-06-24T00:22:36.688269Z","shell.execute_reply":"2021-06-24T00:22:36.695697Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"string_input = Input(shape=(1,), dtype=\"string\")\nx = vectorizer(string_input)\n\ncnn_word_embeddings_model_hyper_params_tuned_output_128 = Model(\n    inputs=cnn_word_embeddings_model_hyper_params_tuned.input,\n    outputs=cnn_word_embeddings_model_hyper_params_tuned.layers[-4].output\n)\n\npreds = cnn_word_embeddings_model_hyper_params_tuned_output_128(x)\nend_to_end_cnn_word_embeddings_model_hyper_params_tuned_output_128 = Model(string_input, preds)\nend_to_end_cnn_word_embeddings_model_hyper_params_tuned_output_128.save(\n    \"end_to_end_cnn_word_embeddings__hyper-params-tuned__layer-128__05-epochs_val_loss-1.356145.tf\",\n    save_format='tf')\nend_to_end_cnn_word_embeddings_model_hyper_params_tuned_output_128.summary()","metadata":{"execution":{"iopub.status.busy":"2021-06-24T00:24:11.320618Z","iopub.execute_input":"2021-06-24T00:24:11.321036Z","iopub.status.idle":"2021-06-24T00:24:36.489959Z","shell.execute_reply.started":"2021-06-24T00:24:11.321004Z","shell.execute_reply":"2021-06-24T00:24:36.488879Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# With save_format='tf', there are several files and subdirectories\nshutil.make_archive(\"end_to_end_cnn_word_embeddings__hyper-params-tuned__layer-16__05-epochs_val_loss-1.356145.tf\", \"zip\", \"./end_to_end_cnn_word_embeddings__hyper-params-tuned__layer-16__05-epochs_val_loss-1.356145.tf\")","metadata":{"execution":{"iopub.status.busy":"2021-06-24T00:25:47.372829Z","iopub.execute_input":"2021-06-24T00:25:47.373417Z","iopub.status.idle":"2021-06-24T00:26:03.504286Z","shell.execute_reply.started":"2021-06-24T00:25:47.373381Z","shell.execute_reply":"2021-06-24T00:26:03.503273Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"shutil.make_archive(\"end_to_end_cnn_word_embeddings__hyper-params-tuned__layer-128__05-epochs_val_loss-1.356145.tf\", \"zip\", \"./end_to_end_cnn_word_embeddings__hyper-params-tuned__layer-128__05-epochs_val_loss-1.356145.tf\")","metadata":{"execution":{"iopub.status.busy":"2021-06-24T00:26:03.505846Z","iopub.execute_input":"2021-06-24T00:26:03.506346Z","iopub.status.idle":"2021-06-24T00:26:19.669292Z","shell.execute_reply.started":"2021-06-24T00:26:03.506312Z","shell.execute_reply":"2021-06-24T00:26:19.66848Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"end_to_end_copy = load_model(\"./end_to_end_cnn_word_embeddings__hyper-params-tuned__layer-16__05-epochs_val_loss-1.356145.tf\")\nend_to_end_copy.summary()","metadata":{"execution":{"iopub.status.busy":"2021-06-24T00:27:16.1535Z","iopub.execute_input":"2021-06-24T00:27:16.154871Z","iopub.status.idle":"2021-06-24T00:27:32.058798Z","shell.execute_reply.started":"2021-06-24T00:27:16.154793Z","shell.execute_reply":"2021-06-24T00:27:32.057704Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Note: when loading the model, the embedding layer is left as trainable, so we have to revert that:**","metadata":{}},{"cell_type":"code","source":"end_to_end_copy.layers[-1].layers[1].trainable = False\nend_to_end_copy.summary()\ndel end_to_end_copy","metadata":{"execution":{"iopub.status.busy":"2021-06-24T00:27:34.548592Z","iopub.execute_input":"2021-06-24T00:27:34.549045Z","iopub.status.idle":"2021-06-24T00:27:34.645135Z","shell.execute_reply.started":"2021-06-24T00:27:34.549011Z","shell.execute_reply":"2021-06-24T00:27:34.643827Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class DescriptionFeatureExtractor(BaseEstimator, TransformerMixin):\n    def __init__(self, model=None, model_name=None, loaded_features=None,\n                 save=False, include_feats=True, debug=False):\n        self.model = model\n        self.model_name = model_name\n        self.save = save\n        self.include_feats = include_feats\n        self.loaded_features = loaded_features\n        self.debug = debug       \n    \n    \n    def extract(self, X):        \n        features = {}\n        iterable = X.iterrows()\n        if self.debug:\n            iterable = tqdm(iterable, total=X.shape[0])\n        for index, row in iterable:\n            pet_id = row[\"PetID\"]\n            desc_features = self.model.predict([[row[\"Description\"]]])[0]\n            features[pet_id] = desc_features\n        return pd.DataFrame.from_dict(features, orient='index')\n    \n    \n    def fit(self, X, y):\n        if self.model is None and self.loaded_features is None:\n            raise ValueError(\"'model' and 'loaded_features' cannot be None at the same time\")\n        if self.model is not None and self.model_name is None:\n            raise ValueError(\"'model_name' cannot be None if 'model is not None'\")\n        \n        return self\n    \n    \n    def save_features(self, features_df):\n        if self.model is not None:\n            new_filename = f'description_features_{self.model_name}.csv'\n        try:\n            features_df.to_csv(new_filename)\n            if self.debug:\n                print(f\"File {new_filename} succesfully saved\")\n        except Exception as e:\n            print(e)\n    \n    \n    def transform(self, X, y=None):\n        X = X.copy()\n        X[\"Description\"].replace(np.nan, '', inplace=True)\n        if self.loaded_features is not None:\n            features_df = self.loaded_features.copy()\n        elif self.model is not None:\n            features_df = self.extract(X)\n        \n        if self.debug:\n            print(f\"Number of description features: {features_df.shape[1]}\\n\")\n            display(features_df.head())\n            \n        if self.save:\n            self.save_features(features_df)\n            \n        if self.include_feats:\n            features_df.rename(lambda x: f\"desc_{x}\", axis=1, inplace=True)\n            X = X.merge(features_df, left_index=False, right_index=True,\n                        left_on=\"PetID\", how=\"left\")\n#             if \"PetID\" in X.columns:\n#                 X.drop([\"PetID\"], axis=1, inplace=True)\n                \n        return X","metadata":{"execution":{"iopub.status.busy":"2021-06-28T10:19:35.847263Z","iopub.execute_input":"2021-06-28T10:19:35.8476Z","iopub.status.idle":"2021-06-28T10:19:35.862702Z","shell.execute_reply.started":"2021-06-28T10:19:35.847571Z","shell.execute_reply":"2021-06-28T10:19:35.861744Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dfe = DescriptionFeatureExtractor(\n    model=end_to_end_cnn_word_embeddings_model_hyper_params_tuned_output_16,\n    model_name=f\"cnn_word_embeddings_model__hyper-params-tuned__layer-16_05-epochs\",\n    save=True, debug=True, include_feats=False\n)\n\n_ = dfe.fit_transform(X, y)","metadata":{"execution":{"iopub.status.busy":"2021-06-24T00:31:39.368935Z","iopub.execute_input":"2021-06-24T00:31:39.369443Z","iopub.status.idle":"2021-06-24T00:47:03.499859Z","shell.execute_reply.started":"2021-06-24T00:31:39.369396Z","shell.execute_reply":"2021-06-24T00:47:03.498963Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"description_features_cnn_tuned_layer_16 = pd.read_csv(\n    \"../input/tfg-pet-adoption-data/description_features_cnn_word_embeddings_model__hyper-params-tuned__layer-16_05-epochs.csv\",\n    index_col=0)\ndisplay(description_features_cnn_tuned_layer_16.head(5))\ndescription_features_cnn_tuned_layer_16.describe().loc[\"std\",:].value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-06-28T10:19:35.863954Z","iopub.execute_input":"2021-06-28T10:19:35.864329Z","iopub.status.idle":"2021-06-28T10:19:36.010264Z","shell.execute_reply.started":"2021-06-28T10:19:35.864291Z","shell.execute_reply":"2021-06-28T10:19:36.009342Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dfe = DescriptionFeatureExtractor(\n    model=end_to_end_cnn_word_embeddings_model_hyper_params_tuned_output_128,\n    model_name=f\"cnn_word_embeddings_model__hyper-params-tuned__layer-128_05-epochs\",\n    save=True, debug=True, include_feats=False\n)\n\n_ = dfe.fit_transform(X, y)","metadata":{"execution":{"iopub.status.busy":"2021-06-24T00:47:48.447611Z","iopub.execute_input":"2021-06-24T00:47:48.448083Z","iopub.status.idle":"2021-06-24T01:03:22.07086Z","shell.execute_reply.started":"2021-06-24T00:47:48.448031Z","shell.execute_reply":"2021-06-24T01:03:22.069498Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"description_features_cnn_tuned_layer_128 = pd.read_csv(\n    \"../input/tfg-pet-adoption-data/description_features_cnn_word_embeddings_model__hyper-params-tuned__layer-128_05-epochs.csv\",\n    index_col=0)\ndisplay(description_features_cnn_tuned_layer_128.head(5))\ndescription_features_cnn_tuned_layer_128.describe().loc[\"std\",:].value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-06-28T10:19:36.011969Z","iopub.execute_input":"2021-06-28T10:19:36.012279Z","iopub.status.idle":"2021-06-28T10:19:36.857828Z","shell.execute_reply.started":"2021-06-28T10:19:36.012249Z","shell.execute_reply":"2021-06-28T10:19:36.856863Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ife = ImageFeatureExtractor(construct_from_cnn_backbone=False,\n        loaded_features=aggregated_image_features_regression_model_ensemble_2_layer_16_features)\n\npipeline_transformers = [\n    ('replace_breeds', LeftJoinReplace(values_dict=breeds_dict,\n                                      variables=[\"Breed1\", \"Breed2\"])),\n    ('replace_colors', LeftJoinReplace(values_dict=colors_dict,\n                                      variables=[\"Color1\", \"Color2\", \"Color3\"])),\n    ('replace_states', LeftJoinReplace(values_dict=states_dict,\n                                     variables=[\"State\"])),\n    ('replace_by_strings', FunctionTransformer(func=replace_integers_by_strings)),\n    ('has_name', FunctionTransformer(func=has_significant_name)),\n    ('pure_breed', FunctionTransformer(func=has_pure_breed)),\n    ('impute_breed', BreedImputer()),\n    ('include_prof_im_metadata', IncludeProfileImageMetadata(profile_image_metadata)),\n    ('correct_wrong_type', CorrectWrongType(breeds)),\n    ('encode_breed', BreedEncoding(enc_type=\"target_and_frequency\")),\n    ('ordinal_vars_encoder', OrdinalVariableEncoder(columns=[\"MaturitySize\", \"FurLength\", \"Health\"],\n                                enc_type=\"ordinal\", mapping=ordinal_vars_mapping)),\n    ('state_gdp', ReplaceState(gdp_per_capita=gdp_per_capita, impute_nan_value=46450)),\n    ('rescuer_count', ReplaceRescuerID()),\n    ('discretizer', CustomDiscretizer(bins_age=-1, quantity=False, fee=False, video_amt=False,\n                                     photo_amt=False)),\n    ('description_length', FunctionTransformer(func=include_description_length)),\n    ('include_desc_metadata', IncludeDescriptionMetadata(description_metadata=description_metadata)),\n    ('correct_desc_language', CorrectDescriptionLanguage()),\n    ('one_hot_encoder', CustomOneHotEncoder(columns=[\"Gender\", \"Color1\", \"Color2\",\n                            \"Color3\", \"Vaccinated\", \"Dewormed\", \"Sterilized\", \"DescriptionLanguage\"])),\n    ('include_prof_im_properties', IncludeProfileImageProperties(profile_image_properties)),\n    ('drop_columns', ColumnRemover(columns=columns_to_be_removed_desc_feats_eval)),\n    ('round_im_dims_aspect_ratio', FunctionTransformer(func=include_aspect_ratio)),\n    ('image_features_extractor', ife),\n    ('description_transformer', DescriptionTransformer(transformations_df=transformations_df)),\n    None,\n    ('drop_petid', ColumnRemover(columns=[\"PetID\", \"DescriptionLanguage\", \"Description\"])),\n    ('custom_standard_scaler', CustomStandardScaler(numeric_columns_desc_feats_eval)),\n    ('useless_vars_remover', UselessVariablesRemover(tolerance=0.000001)),\n    ('impute_malay_desc_missing_prof_im_props', CustomIterativeImputer())\n]\n\nevaluation_results = pd.DataFrame([], columns=[\"Model description\",\n                        \"Average fit time\", \"Average accuracy\", \"Average QWK\",\n                        \"Single split accuracy\", \"Single split QWK\"])\n\n\nfeatures_dict = {\n    \"cnn_n_filters-64_kernel_size-3_128-16_layer-16\": description_features_cnn_tuned_layer_16,\n    \"cnn_n_filters-64_kernel_size-3_128-16_layer-128\": description_features_cnn_tuned_layer_128\n}\n\nfor features_desc, features in features_dict.items():\n    dfe = DescriptionFeatureExtractor(loaded_features=features)\n    pipeline_transformers[-5] = ('DescriptionFeatureExtractor', dfe)\n\n    model = xgb.XGBClassifier(eval_metric='mlogloss', random_state=seed, n_jobs=-1,\n                          use_label_encoder=False)\n    model_description = f\"XGBClassifier, word embeddings {features_desc}\"\n    print(f\"\\n\\n*************** {model_description} ***************\")\n    avg_fit_time, avg_accuracy, avg_QWK = evaluate_model(\n                Pipeline(steps=pipeline_transformers + [('model', model)]),\n                cv, X, y, model_type=\"classification\", display_results=True,\n                display_plots=False)\n\n    single_accuracy, single_QWK = evaluate_model_single_split(\n                Pipeline(steps=pipeline_transformers + [('model', model)]),\n                X_train_CNN, X_val_CNN, y_train_CNN, y_val_CNN,\n                model_type=\"classification\", display_results=True)\n\n    evaluation_results = evaluation_results.append({\n        \"Model description\": model_description,\n        \"Average fit time\": avg_fit_time,\n        \"Average accuracy\": avg_accuracy,\n        \"Average QWK\": avg_QWK,\n        \"Single split accuracy\": single_accuracy,\n        \"Single split QWK\": single_QWK\n    }, ignore_index=True)\n\n\npd.set_option('display.max_colwidth', None)\ndisplay(evaluation_results)\npd.set_option('display.max_colwidth', 50)\nto_latex(evaluation_results, \"evaluation_results_XGBClassifier_cnn_word_embeddings_hyper-params-tuned\")","metadata":{"execution":{"iopub.status.busy":"2021-06-28T10:19:36.859315Z","iopub.execute_input":"2021-06-28T10:19:36.859686Z","iopub.status.idle":"2021-06-28T10:28:11.552105Z","shell.execute_reply.started":"2021-06-28T10:19:36.859655Z","shell.execute_reply":"2021-06-28T10:28:11.551033Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Given the bad results of the CNNs with word embeddings in the single split validation, we will definitively use TF-IDF to extract information from the Description text.","metadata":{}},{"cell_type":"code","source":"ife = ImageFeatureExtractor(\n    construct_from_cnn_backbone=False,\n    loaded_features=aggregated_image_features_regression_model_ensemble_2_layer_16_features\n)\n\ntfidf_vectorizer = CustomTfidfVectorizer(ngram_range=(1,2),\n                                         svd_n_components=16,\n                                         seed=seed)\n\npipeline_6_transformers = [\n    ('replace_breeds', LeftJoinReplace(values_dict=breeds_dict,\n                                      variables=[\"Breed1\", \"Breed2\"])),\n    ('replace_colors', LeftJoinReplace(values_dict=colors_dict,\n                                      variables=[\"Color1\", \"Color2\", \"Color3\"])),\n    ('replace_states', LeftJoinReplace(values_dict=states_dict,\n                                     variables=[\"State\"])),\n    ('replace_by_strings', FunctionTransformer(func=replace_integers_by_strings)),\n    ('has_name', FunctionTransformer(func=has_significant_name)),\n    ('pure_breed', FunctionTransformer(func=has_pure_breed)),\n    ('breed_matches_fur_length', FunctionTransformer(func=breed_matches_fur_length)),\n    ('impute_breed', BreedImputer()),\n    ('include_prof_im_metadata', IncludeProfileImageMetadata(profile_image_metadata)),\n    ('correct_wrong_type', CorrectWrongType(breeds)),\n    ('encode_breed', BreedEncoding(enc_type=\"target_and_frequency\")),\n    ('ordinal_vars_encoder', OrdinalVariableEncoder(columns=[\"MaturitySize\", \"FurLength\", \"Health\"],\n                                enc_type=\"ordinal\", mapping=ordinal_vars_mapping)),\n    ('state_gdp', ReplaceState(gdp_per_capita=gdp_per_capita, impute_nan_value=46450)),\n    ('rescuer_count', ReplaceRescuerID()),\n    ('discretizer', CustomDiscretizer(bins_age=-1, quantity=False, fee=False, video_amt=False,\n                                     photo_amt=False)),\n    ('description_length', FunctionTransformer(func=include_description_length)),\n    ('include_desc_metadata', IncludeDescriptionMetadata(description_metadata=description_metadata)),\n    ('correct_desc_language', CorrectDescriptionLanguage()),\n    ('one_hot_encoder', CustomOneHotEncoder(columns=[\"Gender\", \"Color1\", \"Color2\",\n                            \"Color3\", \"Vaccinated\", \"Dewormed\", \"Sterilized\", \"DescriptionLanguage\"])),\n    ('include_prof_im_properties', IncludeProfileImageProperties(profile_image_properties)),\n    ('drop_columns', ColumnRemover(columns=columns_to_be_removed_desc_feats_eval)),\n    ('round_im_dims_aspect_ratio', FunctionTransformer(func=include_aspect_ratio)),\n    ('image_features_extractor', ife),\n    ('description_transformer', DescriptionTransformer(transformations_df=transformations_df)),\n    ('tfidf_vectorizer', tfidf_vectorizer),\n    ('drop_petid_desc', ColumnRemover(columns=[\"PetID\", \"DescriptionLanguage\", \"Description\"])),\n    ('useless_vars_remover', UselessVariablesRemover(tolerance=0.000001)),\n    ('impute_malay_desc_missing_prof_im_props', CustomIterativeImputer()),\n    ('custom_standard_scaler', CustomStandardScaler(numeric_columns_desc_feats_eval))\n]\n\n\nxgb_classifier = xgb.XGBClassifier(eval_metric='mlogloss', random_state=seed, n_jobs=-1,\n                          use_label_encoder=False)\n\nrandom_forest_clf = RandomForestClassifier(n_jobs=-1, random_state=seed)\n\nsvc_clf = SVC(break_ties=True, probability=True, random_state=seed)\n\nlogistic_regression = LogisticRegression(n_jobs=-1, random_state=seed)\n\nmodels = {\n    \"XGBClassifier\": xgb_classifier,\n    \"RandomForestClassifier\": random_forest_clf,\n    \"SVC (rbf kernel)\": svc_clf,\n    \"Logistic Regression\": logistic_regression\n}\n\nglobal_evaluation_results_6 = pd.DataFrame(\n    [],\n    columns=[\"Pipeline\", \"Model\", \"Average fit time\", \"Average accuracy\",\n             \"Average QWK\", \"Single split accuracy\", \"Single split QWK\"]\n)\n\nfor model_desc, model in models.items():\n    print(f\"--------------------- MODEL: {model_desc} ---------------------\")\n    avg_fit_time, avg_acc, avg_qwk = evaluate_model(\n        Pipeline(steps=pipeline_6_transformers + [('model', model)]),\n        cv, X, y, model_type=\"classification\")\n    \n    single_accuracy, single_QWK = evaluate_model_single_split(\n                Pipeline(steps=pipeline_6_transformers + [('model', model)]),\n                X_train_CNN, X_val_CNN, y_train_CNN, y_val_CNN,\n                model_type=\"classification\", display_results=True)\n    \n    global_evaluation_results_6 = global_evaluation_results_6.append({\n        \"Pipeline\": 6,\n        \"Model\": model_desc,\n        \"Average fit time\": avg_fit_time,\n        \"Average accuracy\": avg_acc,\n        \"Average QWK\": avg_qwk,\n        \"Single split accuracy\": single_accuracy,\n        \"Single split QWK\": single_QWK\n    }, ignore_index=True)","metadata":{"execution":{"iopub.status.busy":"2021-06-28T10:28:11.553502Z","iopub.execute_input":"2021-06-28T10:28:11.553891Z","iopub.status.idle":"2021-06-28T10:49:26.934481Z","shell.execute_reply.started":"2021-06-28T10:28:11.55385Z","shell.execute_reply":"2021-06-28T10:49:26.933327Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"global_evaluation_results = global_evaluation_results_1.append(\n    [global_evaluation_results_2, global_evaluation_results_3,\n     global_evaluation_results_4, global_evaluation_results_5,\n     global_evaluation_results_6],\n    ignore_index=True\n)\nglobal_evaluation_results.to_csv(\"global_evaluation_results.csv\", index=False)\nglobal_evaluation_results","metadata":{"execution":{"iopub.status.busy":"2021-06-28T10:49:26.936207Z","iopub.execute_input":"2021-06-28T10:49:26.936818Z","iopub.status.idle":"2021-06-28T10:49:26.967682Z","shell.execute_reply.started":"2021-06-28T10:49:26.93677Z","shell.execute_reply":"2021-06-28T10:49:26.966503Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Feature importance and subset selection","metadata":{}},{"cell_type":"code","source":"!cp -r ../input/tfg-pet-adoption-data/rdc-master/rdc-master/* ./\n!python setup.py install\nfrom rdc import rdc","metadata":{"execution":{"iopub.status.busy":"2021-06-29T07:30:56.354304Z","iopub.execute_input":"2021-06-29T07:30:56.354754Z","iopub.status.idle":"2021-06-29T07:30:59.501941Z","shell.execute_reply.started":"2021-06-29T07:30:56.354711Z","shell.execute_reply":"2021-06-29T07:30:59.50042Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Implemented strategies: ranker (mutual information between each predictor an AdoptionSpeed, without interaction between predictor variables), rf_gini and rf_permutation (importance when fitting a Random Forest, with default parameters, using the Gini split criterion to compute the importances or the OOB importances estimation with permutations, respectively) and mrmr_fcq and mrmr_frq (two versions of MRMR, the first one with Pearson Correlation and the second one with the Randomized Dependent Coefficient, both to measure interaction between predictor variables).","metadata":{}},{"cell_type":"markdown","source":"https://scikit-learn.org/stable/modules/feature_selection.html#sequential-feature-selection\n\nhttps://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.mutual_info_regression.html#sklearn.feature_selection.mutual_info_regression\n\nhttps://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.mutual_info_classif.html#sklearn.feature_selection.mutual_info_classif\n\nhttps://github.com/scikit-learn/scikit-learn/blob/15a949460/sklearn/feature_selection/_sequential.py#L15\n\nhttps://towardsdatascience.com/feature-selection-how-to-throw-away-95-of-your-data-and-get-95-accuracy-ad41ca016877\n\nhttps://towardsdatascience.com/top-7-feature-selection-techniques-in-machine-learning-94e08730cd09\n\nhttps://es.wikipedia.org/wiki/Maldici%C3%B3n_de_la_dimensi%C3%B3n\n\n(Didn't use this, as it was not done on OOB: https://scikit-learn.org/stable/auto_examples/inspection/plot_permutation_importance.html#sphx-glr-auto-examples-inspection-plot-permutation-importance-py, https://scikit-learn.org/stable/modules/permutation_importance.html#permutation-importance)","metadata":{}},{"cell_type":"markdown","source":"**!!** https://explained.ai/rf-importance/index.html#3, https://github.com/parrt/random-forest-importances\n\n https://www.stat.berkeley.edu/~breiman/OOBestimation.pdf\n \n**(When doing OOB feauture importance, some of them are negative: the mean error is smaller after a random permutation! So they are far from being important features)**","metadata":{}},{"cell_type":"markdown","source":"https://towardsdatascience.com/mrmr-explained-exactly-how-you-wished-someone-explained-to-you-9cf4ed27458b\n\nhttps://arxiv.org/pdf/1908.05376.pdf\n\nhttps://arxiv.org/pdf/1304.7717.pdf\n\nhttps://github.com/garydoranjr/rdc\n\n**RDC summary in question -->** https://stats.stackexchange.com/questions/161397/about-the-randomized-dependence-coefficient","metadata":{}},{"cell_type":"code","source":"class FeatureSubsetSelection(BaseEstimator, TransformerMixin):\n    def __init__(self, strategy, base_numeric_columns, seed, k=None, frac=None,\n                 debug=False):\n        self.strategy = strategy\n        self.base_numeric_columns = base_numeric_columns\n        self.seed = seed\n        self.k = k\n        self.frac = frac\n        self.debug = debug\n        \n    \n    def ranker(self, X, y):\n        mutual_info_target = list(zip(\n            X.columns,\n            mutual_info_classif(X, y,\n                discrete_features=self.index_discrete_columns,\n                random_state=self.seed)))\n        self.ranking = sorted(mutual_info_target, key=lambda x: x[1], reverse=True)\n        \n        if self.debug:\n            display(self.ranking)\n         \n        self.selected = [x[0] for x in self.ranking[:self.k]]\n    \n    \n    def rf_gini(self, X, y):\n        rf = RandomForestClassifier(n_jobs=-1, random_state=self.seed)\n        rf.fit(X, y)\n        self.ranking = sorted(list(zip(X.columns, rf.feature_importances_)),\n                              key=lambda x: x[1],\n                              reverse=True)\n        if self.debug:\n            display(self.ranking)\n         \n        self.selected = [x[0] for x in self.ranking[:self.k]]\n    \n    \n    def rf_permutation(self, X, y):\n        rf = RandomForestClassifier(oob_score=True, n_jobs=-1, random_state=self.seed)\n        rf.fit(X, y)\n        imp = rfpimp.oob_importances(rf, X, y, n_samples=-1)\n        self.ranking = list(zip(imp.index, imp[\"Importance\"]))\n        \n        if self.debug:\n            display(self.ranking)\n         \n        self.selected = [x[0] for x in self.ranking[:self.k]]      \n\n    \n    def mrmr(self, X, y, correlation):\n        f_test = pd.Series(f_classif(X,y)[0], index=X.columns)\n        if self.debug:\n            print(\"F-values:\")\n            display(f_test)\n        # Pearson correlation or Randomized Dependence Coefficient:\n        self.dependence_predictors = pd.DataFrame(0.000001, index=X.columns, columns=X.columns)\n        self.selected = []\n        not_selected = list(X.columns)\n        \n        for i in range(self.k):\n            if i == 0:\n                feature_best_f_value = f_test.idxmax()\n                self.selected.append(feature_best_f_value)\n                not_selected.remove(feature_best_f_value)\n                if self.debug:\n                    print(f\"Selected feature {feature_best_f_value} (F-value: {f_test.max()})\")\n                continue\n            \n            last_added = self.selected[-1]\n            if correlation:\n                self.dependence_predictors.loc[not_selected, last_added] = \\\n                        X[not_selected].corrwith(X[last_added]).abs().clip(0.000001)\n            else:\n                for var in not_selected:\n                    self.dependence_predictors.loc[var, last_added] = \\\n                            rdc(X[var].to_numpy(), X[last_added].to_numpy())\n                self.dependence_predictors.loc[not_selected, last_added] = \\\n                    self.dependence_predictors.loc[not_selected, last_added].abs().clip(0.000001)\n            \n            scores = f_test.loc[not_selected] / self.dependence_predictors.loc[not_selected,\n                                                                    self.selected].mean(axis=1)\n            \n            best_feature_i = scores.index[scores.argmax()]\n            self.selected.append(best_feature_i)\n            not_selected.remove(best_feature_i)\n            \n            if self.debug:\n                print(f\"Selected feature {best_feature_i} ({scores.max()})\")\n            \n    \n    def fit(self, X, y):\n        if self.k is None and self.frac is None:\n            raise ValueError(f\"Parameters 'k' and 'frac' cannot be None at the same time\")\n        if self.k == 0 or self.frac == 0:\n            raise ValueError(f\"Neither 'k' nor 'frac' can be 0\")\n        if self.k is not None and self.k > len(X.columns):\n            raise ValueError(f\"Parameter 'k' cannot be greater than the total number of features\")\n        if self.strategy not in {\"ranker\", \"rf_gini\", \"rf_permutation\",\n                                 \"mrmr_fcq\", \"mrmr_frq\"}:\n            raise ValueError(f\"{self.strategy} is not a valid FSS strategy\")\n            \n        if self.strategy == \"ranker\":\n            self.numeric_columns = self.base_numeric_columns + \\\n                list(filter(lambda x: (\"Breed\" in str(x) and \"Fur\" not in str(x)) \\\n                            or \"_ordinal\" in str(x) or \"img_\" in str(x) \\\n                            or \"desc_\" in str(x) or \"_num\" in str(x) \\\n                            or \"_mean\" in str(x) or \"_sum\" in str(x) \\\n                            or \"_var\" in str(x),\n                            X.columns))\n            self.discrete_columns = list(filter(lambda x: x not in self.numeric_columns, X.columns))\n            self.index_discrete_columns = [i for i, x in enumerate(X.columns)\n                                                if x in self.discrete_columns]\n        \n        if self.frac is not None:\n            self.k = round(len(X.columns) * self.frac)\n        \n        if self.k == len(X.columns):\n            self.selected = X.columns\n        elif self.strategy == 'ranker':\n            self.ranker(X, y)\n        elif self.strategy == 'rf_gini':\n            self.rf_gini(X, y)\n        elif self.strategy == 'rf_permutation':\n            self.rf_permutation(X, y)\n        elif self.strategy == 'mrmr_fcq':\n            self.mrmr(X, y, correlation=True)\n        elif self.strategy == 'mrmr_frq':\n            self.mrmr(X, y, correlation=False)\n        \n        return self\n    \n    \n    def transform(self, X, y=None):\n        X = X.copy()\n        return X[self.selected]","metadata":{"execution":{"iopub.status.busy":"2021-06-29T07:30:59.50452Z","iopub.execute_input":"2021-06-29T07:30:59.504869Z","iopub.status.idle":"2021-06-29T07:30:59.538359Z","shell.execute_reply.started":"2021-06-29T07:30:59.504835Z","shell.execute_reply":"2021-06-29T07:30:59.536788Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ife = ImageFeatureExtractor(\n    construct_from_cnn_backbone=False,\n    loaded_features=aggregated_image_features_regression_model_ensemble_2_layer_16_features\n)\n\ntfidf_vectorizer = CustomTfidfVectorizer(ngram_range=(1,2),\n                                         svd_n_components=16,\n                                         seed=seed)\n\npipeline_transformers = [\n    ('replace_breeds', LeftJoinReplace(values_dict=breeds_dict,\n                                      variables=[\"Breed1\", \"Breed2\"])),\n    ('replace_colors', LeftJoinReplace(values_dict=colors_dict,\n                                      variables=[\"Color1\", \"Color2\", \"Color3\"])),\n    ('replace_states', LeftJoinReplace(values_dict=states_dict,\n                                     variables=[\"State\"])),\n    ('replace_by_strings', FunctionTransformer(func=replace_integers_by_strings)),\n    ('has_name', FunctionTransformer(func=has_significant_name)),\n    ('pure_breed', FunctionTransformer(func=has_pure_breed)),\n    ('breed_matches_fur_length', FunctionTransformer(func=breed_matches_fur_length)),\n    ('impute_breed', BreedImputer()),\n    ('include_prof_im_metadata', IncludeProfileImageMetadata(profile_image_metadata)),\n    ('correct_wrong_type', CorrectWrongType(breeds)),\n    ('encode_breed', BreedEncoding(enc_type=\"target_and_frequency\")),\n    ('ordinal_vars_encoder', OrdinalVariableEncoder(columns=[\"MaturitySize\", \"FurLength\", \"Health\"],\n                                enc_type=\"ordinal\", mapping=ordinal_vars_mapping)),\n    ('state_gdp', ReplaceState(gdp_per_capita=gdp_per_capita, impute_nan_value=46450)),\n    ('rescuer_count', ReplaceRescuerID()),\n    ('discretizer', CustomDiscretizer(bins_age=-1, quantity=False, fee=False, video_amt=False,\n                                     photo_amt=False)),\n    ('description_length', FunctionTransformer(func=include_description_length)),\n    ('include_desc_metadata', IncludeDescriptionMetadata(description_metadata=description_metadata)),\n    ('correct_desc_language', CorrectDescriptionLanguage()),\n    ('one_hot_encoder', CustomOneHotEncoder(columns=[\"Gender\", \"Color1\", \"Color2\",\n                            \"Color3\", \"Vaccinated\", \"Dewormed\", \"Sterilized\", \"DescriptionLanguage\"])),\n    ('include_prof_im_properties', IncludeProfileImageProperties(profile_image_properties)),\n    ('drop_columns', ColumnRemover(columns=columns_to_be_removed_desc_feats_eval)),\n    ('round_im_dims_aspect_ratio', FunctionTransformer(func=include_aspect_ratio)),\n    ('image_features_extractor', ife),\n    ('description_transformer', DescriptionTransformer(transformations_df=transformations_df)),\n    ('tfidf_vectorizer', tfidf_vectorizer),\n    ('drop_petid_desc', ColumnRemover(columns=[\"PetID\", \"DescriptionLanguage\", \"Description\"])),\n    ('useless_vars_remover', UselessVariablesRemover(tolerance=0.000001)),\n    ('impute_malay_desc_missing_prof_im_props', CustomIterativeImputer()),\n    ('custom_standard_scaler', CustomStandardScaler(numeric_columns_desc_feats_eval)),\n    None\n]\n\n\nxgb_classifier = xgb.XGBClassifier(eval_metric='mlogloss', random_state=seed, n_jobs=-1,\n                          use_label_encoder=False)\n\nrandom_forest_clf = RandomForestClassifier(n_jobs=-1, random_state=seed)\n\nsvc_clf = SVC(break_ties=True, probability=True, random_state=seed)\n\nlogistic_regression = LogisticRegression(n_jobs=-1, random_state=seed)\n\nmodels = {\n    \"XGBClassifier\": xgb_classifier,\n    \"RandomForestClassifier\": random_forest_clf,\n    \"SVC (rbf kernel)\": svc_clf,\n    \"Logistic Regression\": logistic_regression\n}\n\nfss_evaluation_results = pd.DataFrame(\n    [],\n    columns=[\"FSS strategy\", \"Model\", \"Fraction of features\", \"Average fit time\",\n             \"Average accuracy\", \"Average QWK\", \"Single split accuracy\",\n             \"Single split QWK\"]\n)\n\nfss_strategies = [\"ranker\", \"rf_gini\", \"rf_permutation\", \"mrmr_fcq\",\n                  \"mrmr_frq\"]\n\nfor model_desc, model in models.items():\n    for fss_strategy in fss_strategies:\n        for frac in [0.2, 0.4, 0.6, 0.8, 1]:\n            print(f\"--------------------- MODEL: {model_desc}, FSS STRATEGY: {fss_strategy}, FRACTION: {frac} ---------------------\")\n            \n            fss = FeatureSubsetSelection(\n                strategy=fss_strategy,\n                base_numeric_columns=numeric_columns_desc_feats_eval,\n                seed=seed,\n                frac=frac\n            )\n            pipeline_transformers[-1] = ('fss', fss)\n            \n            avg_fit_time, avg_acc, avg_qwk = evaluate_model(\n                Pipeline(steps=pipeline_transformers + [('model', model)]),\n                cv, X, y, model_type=\"classification\", display_results=False)\n\n            single_accuracy, single_QWK = evaluate_model_single_split(\n                    Pipeline(steps=pipeline_transformers + [('model', model)]),\n                    X_train_CNN, X_val_CNN, y_train_CNN, y_val_CNN,\n                    model_type=\"classification\", display_results=False)\n\n            fss_evaluation_results = fss_evaluation_results.append({\n                \"Model\": model_desc,\n                \"FSS strategy\": fss_strategy,\n                \"Fraction of features\": frac,\n                \"Average fit time\": avg_fit_time,\n                \"Average accuracy\": avg_acc,\n                \"Average QWK\": avg_qwk,\n                \"Single split accuracy\": single_accuracy,\n                \"Single split QWK\": single_QWK\n            }, ignore_index=True)","metadata":{"execution":{"iopub.status.busy":"2021-06-29T13:23:33.132623Z","iopub.execute_input":"2021-06-29T13:23:33.133057Z","iopub.status.idle":"2021-06-29T13:23:33.139466Z","shell.execute_reply.started":"2021-06-29T13:23:33.133022Z","shell.execute_reply":"2021-06-29T13:23:33.138554Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# fss_evaluation_results.to_csv(\"fss_evaluation_results.csv\")\nfss_evaluation_results = pd.read_csv(\"../input/tfg-pet-adoption-data/fss_evaluation_results.csv\")\nfss_evaluation_results","metadata":{"execution":{"iopub.status.busy":"2021-06-29T13:45:21.189626Z","iopub.execute_input":"2021-06-29T13:45:21.190148Z","iopub.status.idle":"2021-06-29T13:45:21.225883Z","shell.execute_reply.started":"2021-06-29T13:45:21.190115Z","shell.execute_reply":"2021-06-29T13:45:21.224787Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"_, ax = plt.subplots(nrows=4, ncols=5, figsize=(25,25))\nfor i, model_desc in enumerate(models):\n    data = fss_evaluation_results.loc[\n        fss_evaluation_results[\"Model\"] == model_desc]\n    sns.lineplot(data=data, x=\"Fraction of features\", y=\"Average fit time\",\n                 hue=\"FSS strategy\", ax=ax[i,0])\n    sns.lineplot(data=data, x=\"Fraction of features\", y=\"Average accuracy\",\n                           hue=\"FSS strategy\", ax=ax[i,1])\n    sns.lineplot(data=data, x=\"Fraction of features\", y=\"Average QWK\",\n                           hue=\"FSS strategy\", ax=ax[i,2])\n    ax[i,2].set_title(model_desc, fontsize=16)\n    sns.lineplot(data=data, x=\"Fraction of features\", y=\"Single split accuracy\",\n                 hue=\"FSS strategy\", ax=ax[i,3])\n    sns.lineplot(data=data, x=\"Fraction of features\", y=\"Single split QWK\",\n                           hue=\"FSS strategy\", ax=ax[i,4])\nplt.suptitle(\"FSS evaluation results\", fontsize=20, y=0.92)\nplt.subplots_adjust(hspace=0.5, wspace=0.3)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-29T14:08:56.277986Z","iopub.execute_input":"2021-06-29T14:08:56.278469Z","iopub.status.idle":"2021-06-29T14:09:02.332735Z","shell.execute_reply.started":"2021-06-29T14:08:56.278432Z","shell.execute_reply":"2021-06-29T14:09:02.331581Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Classification or ordinal regression?","metadata":{}},{"cell_type":"code","source":"ife = ImageFeatureExtractor(\n    construct_from_cnn_backbone=False,\n    loaded_features=aggregated_image_features_regression_model_ensemble_2_layer_16_features\n)\n\ntfidf_vectorizer = CustomTfidfVectorizer(ngram_range=(1,2),\n                                         svd_n_components=16,\n                                         seed=seed)\n\npipeline_6_transformers = [\n    ('replace_breeds', LeftJoinReplace(values_dict=breeds_dict,\n                                      variables=[\"Breed1\", \"Breed2\"])),\n    ('replace_colors', LeftJoinReplace(values_dict=colors_dict,\n                                      variables=[\"Color1\", \"Color2\", \"Color3\"])),\n    ('replace_states', LeftJoinReplace(values_dict=states_dict,\n                                     variables=[\"State\"])),\n    ('replace_by_strings', FunctionTransformer(func=replace_integers_by_strings)),\n    ('has_name', FunctionTransformer(func=has_significant_name)),\n    ('pure_breed', FunctionTransformer(func=has_pure_breed)),\n    ('breed_matches_fur_length', FunctionTransformer(func=breed_matches_fur_length)),\n    ('impute_breed', BreedImputer()),\n    ('include_prof_im_metadata', IncludeProfileImageMetadata(profile_image_metadata)),\n    ('correct_wrong_type', CorrectWrongType(breeds)),\n    ('encode_breed', BreedEncoding(enc_type=\"target_and_frequency\")),\n    ('ordinal_vars_encoder', OrdinalVariableEncoder(columns=[\"MaturitySize\", \"FurLength\", \"Health\"],\n                                enc_type=\"ordinal\", mapping=ordinal_vars_mapping)),\n    ('state_gdp', ReplaceState(gdp_per_capita=gdp_per_capita, impute_nan_value=46450)),\n    ('rescuer_count', ReplaceRescuerID()),\n    ('discretizer', CustomDiscretizer(bins_age=-1, quantity=False, fee=False, video_amt=False,\n                                     photo_amt=False)),\n    ('description_length', FunctionTransformer(func=include_description_length)),\n    ('include_desc_metadata', IncludeDescriptionMetadata(description_metadata=description_metadata)),\n    ('correct_desc_language', CorrectDescriptionLanguage()),\n    ('one_hot_encoder', CustomOneHotEncoder(columns=[\"Gender\", \"Color1\", \"Color2\",\n                            \"Color3\", \"Vaccinated\", \"Dewormed\", \"Sterilized\", \"DescriptionLanguage\"])),\n    ('include_prof_im_properties', IncludeProfileImageProperties(profile_image_properties)),\n    ('drop_columns', ColumnRemover(columns=columns_to_be_removed_desc_feats_eval)),\n    ('round_im_dims_aspect_ratio', FunctionTransformer(func=include_aspect_ratio)),\n    ('image_features_extractor', ife),\n    ('description_transformer', DescriptionTransformer(transformations_df=transformations_df)),\n    ('tfidf_vectorizer', tfidf_vectorizer),\n    ('drop_petid_desc', ColumnRemover(columns=[\"PetID\", \"DescriptionLanguage\", \"Description\"])),\n    ('useless_vars_remover', UselessVariablesRemover(tolerance=0.000001)),\n    ('impute_malay_desc_missing_prof_im_props', CustomIterativeImputer()),\n    ('custom_standard_scaler', CustomStandardScaler(numeric_columns_desc_feats_eval))\n]","metadata":{"execution":{"iopub.status.busy":"2021-06-29T14:15:05.234646Z","iopub.execute_input":"2021-06-29T14:15:05.235311Z","iopub.status.idle":"2021-06-29T14:15:05.250388Z","shell.execute_reply.started":"2021-06-29T14:15:05.235268Z","shell.execute_reply":"2021-06-29T14:15:05.249384Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Spot the difference between this:","metadata":{}},{"cell_type":"code","source":"model = xgb.XGBRegressor(objective='reg:squarederror', random_state=seed,\n                         seed=seed, n_jobs=-1)\n\n_ = evaluate_model(Pipeline(steps=pipeline_6_transformers + [('model', model)]),\n               cv, X, y, model_type=\"regression\", coefficients=[0.5,1.5,2.5,3.5])","metadata":{"execution":{"iopub.status.busy":"2021-06-29T14:34:43.997046Z","iopub.execute_input":"2021-06-29T14:34:43.997474Z","iopub.status.idle":"2021-06-29T14:36:30.289452Z","shell.execute_reply.started":"2021-06-29T14:34:43.997435Z","shell.execute_reply":"2021-06-29T14:36:30.288231Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"And this?:","metadata":{}},{"cell_type":"code","source":"model = xgb.XGBRegressor(objective='reg:squarederror', random_state=seed,\n                         seed=seed, n_jobs=-1)\n\n_ = evaluate_model(Pipeline(steps=pipeline_6_transformers + [('model', model)]),\n               cv, X, y, model_type=\"regression\", coefficients=[1.15,2.15,2.45,2.85])","metadata":{"execution":{"iopub.status.busy":"2021-06-29T14:36:30.291615Z","iopub.execute_input":"2021-06-29T14:36:30.291952Z","iopub.status.idle":"2021-06-29T14:38:16.496772Z","shell.execute_reply.started":"2021-06-29T14:36:30.291919Z","shell.execute_reply":"2021-06-29T14:38:16.495634Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Well, in both cases we are training XGBoost with regression as the objective function; then in order to estimate how accurate we have been, we round the obtained predictions to of the original class values: 0, 1, 2, 3 or 4. The main difference is that in the first case, we round up the values in a common way:\n\n* If it is smaller than or equal to 0.5, then we round to 0\n* Between 0.5 and 1.5 (included), 1\n* Between 1.5 and 2.5 (included), 2\n* Between 2.5 and 3.5 (included), 3\n* Greater than 3.5, 4\n\nHowever, we can see that the results are not very good: as we might expect, the most common outcome is between 2 and 3. If we look at the misses, we can manually adjust the coefficients (for example, few instances that are actually a '3' are predicted as '4', but many that are actually a '4' are predicted as '3'; solution: instead of 3.5, the last coefficient could be smaller, for example 2.85), and this is what we have done in the second execution. **However: manually adjusting coefficients is not a good way to achieve better performance, we can overfit to this data.**","metadata":{}},{"cell_type":"markdown","source":"Thus, we can try the following code, which is extracted from this post: https://www.kaggle.com/c/petfinder-adoption-prediction/discussion/76107. This solution is based on the Nelder-Mead optimization: https://codesachin.wordpress.com/2016/01/16/nelder-mead-optimization/","metadata":{}},{"cell_type":"code","source":"from functools import partial\nimport scipy as sp\n\nclass OptimizedRounder:\n    def _kappa_loss(self, coef, y, y_pred):\n        y_pred = np.copy(y_pred)\n        for i, pred in enumerate(y_pred):\n            if pred < coef[0]:\n                y_pred[i] = 0\n            elif pred >= coef[0] and pred < coef[1]:\n                y_pred[i] = 1\n            elif pred >= coef[1] and pred < coef[2]:\n                y_pred[i] = 2\n            elif pred >= coef[2] and pred < coef[3]:\n                y_pred[i] = 3\n            else:\n                y_pred[i] = 4\n\n        ll = cohen_kappa_score(y, y_pred, weights='quadratic')\n        return -ll\n\n    def fit(self, y, y_pred):\n        loss_partial = partial(self._kappa_loss, y=y, y_pred=y_pred)\n        initial_coef = [0.5, 1.5, 2.5, 3.5]\n        self.coefficients = sp.optimize.minimize(loss_partial, initial_coef, method='nelder-mead')['x']\n\n    def predict(self, y_pred):\n        y_pred = np.copy(y_pred)\n        for i, pred in enumerate(y_pred):\n            if pred < self.coefficients[0]:\n                y_pred[i] = 0\n            elif pred >= self.coefficients[0] and pred < self.coefficients[1]:\n                y_pred[i] = 1\n            elif pred >= self.coefficients[1] and pred < self.coefficients[2]:\n                y_pred[i] = 2\n            elif pred >= self.coefficients[2] and pred < self.coefficients[3]:\n                y_pred[i] = 3\n            else:\n                y_pred[i] = 4\n        return y_pred","metadata":{"execution":{"iopub.status.busy":"2021-06-29T15:08:19.667572Z","iopub.execute_input":"2021-06-29T15:08:19.667942Z","iopub.status.idle":"2021-06-29T15:08:19.68233Z","shell.execute_reply.started":"2021-06-29T15:08:19.667912Z","shell.execute_reply":"2021-06-29T15:08:19.681162Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def evaluate_regression_model(model, cv, X, y, display_results=True,\n                              display_plot=True):\n    orig_model = model\n    fit_times = []\n    coefficients = []\n    rmse_values = []\n    accuracy_scores = []\n    kappa_scores = []\n    confusion_matrices = []\n    \n    # Variables for average classification report\n    original_class = []\n    predicted_class = []\n    \n    for train_index, test_index in cv.split(X, y):\n        print(f\"CV Iteration {len(fit_times)+1}\")\n        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n        \n        model = clone(orig_model)\n        start = time.time() \n        with warnings.catch_warnings():\n            warnings.simplefilter(\"ignore\")\n            model.fit(X_train, y_train)\n        \n        # Fitting rounder:\n        optR = OptimizedRounder()\n        y_pred_train = model.predict(X_train)\n        optR.fit(y_train, y_pred_train)\n        coefficients.append(optR.coefficients)\n        \n        end = time.time()\n        \n        fit_times.append(end-start)\n        \n        y_pred = model.predict(X_test)\n        \n        rmse_values.append(mean_squared_error(y_test, y_pred, squared=False))\n        y_pred = optR.predict(y_pred)\n        \n        original_class.extend(y_test)\n        predicted_class.extend(y_pred)\n        accuracy_scores.append(accuracy_score(y_test, y_pred))\n        \n        kappa_scores.append(cohen_kappa_score(y_test, y_pred, weights='quadratic'))\n        \n        confusion_matrices.append(confusion_matrix(y_test, y_pred, normalize='true'))\n    \n    coefficients = np.array(coefficients)\n    \n    if display_results:\n        print(\"-----------------RESULTS-----------------\")\n        print(f\"Mean fit time: {np.mean(fit_times)} s\")\n        print(\"RMSE:\", rmse_values)\n        print(\"Average RMSE:\", np.mean(rmse_values))\n        print(\"\\nCoefficients:\", coefficients)\n        print(\"Average Coefficients:\", np.mean(coefficients, axis=0))\n        print(\"\\nAccuracy:\", accuracy_scores)\n        print(\"QWK:\", kappa_scores)\n        print(\"\\nAverage accuracy:\", np.mean(accuracy_scores))\n        print(\"Average QWK:\", np.mean(kappa_scores))\n        print(\"\\nAverage classification report:\")\n        print(classification_report(original_class, predicted_class)) \n        \n        if display_plot:\n            disp = ConfusionMatrixDisplay(confusion_matrix=np.mean(confusion_matrices, axis=0))\n            plt.style.use('default')\n            nrows = 1\n            ncols = 1\n            figsize = (6,5)\n            _, ax = plt.subplots(nrows=nrows, ncols=ncols, figsize=figsize)\n            ax.set_title(\"\\nAverage confusion matrix\", fontsize='16')\n            disp.plot(ax=ax)\n            ax.grid(False)\n\n            plt.show()\n    \n    return np.mean(fit_times), np.mean(coefficients, axis=0), np.mean(rmse_values), np.mean(accuracy_scores), np.mean(kappa_scores)","metadata":{"execution":{"iopub.status.busy":"2021-06-29T15:16:19.699046Z","iopub.execute_input":"2021-06-29T15:16:19.69957Z","iopub.status.idle":"2021-06-29T15:16:19.723057Z","shell.execute_reply.started":"2021-06-29T15:16:19.699526Z","shell.execute_reply":"2021-06-29T15:16:19.721757Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = xgb.XGBRegressor(objective='reg:squarederror', random_state=seed,\n                         seed=seed, n_jobs=-1)\n\n_ = evaluate_regression_model(\n    Pipeline(steps=pipeline_6_transformers + [('model', model)]),\n    cv, X, y)","metadata":{"execution":{"iopub.status.busy":"2021-06-29T15:16:21.202451Z","iopub.execute_input":"2021-06-29T15:16:21.202943Z","iopub.status.idle":"2021-06-29T15:21:44.108409Z","shell.execute_reply.started":"2021-06-29T15:16:21.202894Z","shell.execute_reply":"2021-06-29T15:21:44.107439Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = RandomForestRegressor(random_state=seed, n_jobs=-1)\n\n_ = evaluate_regression_model(\n    Pipeline(steps=pipeline_6_transformers + [('model', model)]),\n    cv, X, y)","metadata":{"execution":{"iopub.status.busy":"2021-06-29T15:33:59.245712Z","iopub.execute_input":"2021-06-29T15:33:59.246148Z","iopub.status.idle":"2021-06-29T15:38:15.139631Z","shell.execute_reply.started":"2021-06-29T15:33:59.246048Z","shell.execute_reply":"2021-06-29T15:38:15.138452Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we can see, this is a good way to convert a regression problem into an (ordinal) classification one. However, if we compare these results with the ones obtained with XGBClassifier and RandomForestClassifier, respectively, we can see that both classifiers give better average accuracy and also better f1-score for every class than the corresponding regressors, and since after the regressors we have to find the optimal coefficients (they are local, which is a disadvantage of the Nelder-Mead optimization), the average fit time increases considerably. It is true that the average QWK is better with the regressors, and this is the main metric, but if we do not have the competition 'mindset', there are sufficient reasons to continue working with classifiers for hyperparameter tuning.","metadata":{}}]}