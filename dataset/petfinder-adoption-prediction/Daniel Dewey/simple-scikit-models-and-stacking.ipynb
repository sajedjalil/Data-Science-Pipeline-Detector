{"cells":[{"metadata":{"_uuid":"bd4302311ae93397054dd70244987f219a82ae2d"},"cell_type":"markdown","source":"# PetFinder Simple Models and Stacking"},{"metadata":{"_uuid":"faf9fa642dc780ee1dad76ae24f8d4c831383167"},"cell_type":"markdown","source":"## <a id=\"Index\">Index</a>\n<a href=\"#Diary\">Description/Diary</a><br>\n<a href=\"#WhereRunning\">Where are we Running?</a><br>\n<br>\n<a href=\"#DataProcessing\">Reading and Processing csv Data Files</a><br>\n<a href=\"#SentimentValues\">Sentiment Values from Description</a><br>\n<a href=\"#ImageMetadata\">Metadata from Profile Images</a><br>\n<a href=\"#SVDImageMeta\">Use SVD on Image Metadata</a><br>\n<a href=\"#SVDDescription\">Use SVD on Description Words</a><br>\n<br>\n<a href=\"#FeatureSummary\">Summary of the Features</a><br>\n<a href=\"#CompareTrainTest\">Compare Train and Test Averages</a><br>\n<a href=\"#TheFeatures\">Select The Features to Use for ML</a><br>\n<br>\n<a href=\"#MachineLearning\">Machine Learning</a><br>\n<a href=\"#SetupData\">Setup the X,y Data</a><br>\n<a href=\"#StackingData\">Replace Data with Stacking Data?</a><br>\n<br>\n<a href=\"#Stacking\">Stacking Experiments...</a><br>\n<br>\n<a href=\"#SelectModel\">Select the Model</a><br>\n<a href=\"#DefineModels\">Define the Models and Parameters</a><br>\n<br>\n<a href=\"#HyperSearch\">Do the Hyper-Parameter Search</a><br>\n<a href=\"#CVResults\">Grid Search CV Results</a><br>\n<br>\n<a href=\"#FeatureImportance\">Feature Importance</a><br>\n<a href=\"#EvaluateBest\">Evaluate Best Model</a><br>\n<a href=\"#OutputKaggle\">Output Kaggle Predictions</a><br>\n<br>\n<a href=\"#TheEnd\">The End</a><br>\n<br>"},{"metadata":{"_uuid":"1a1249889b78dae4f4c2520f49f693d9a4ffa1f1"},"cell_type":"markdown","source":"## <a id=\"Diary\">Description/Diary</a>\nGo to <a href=\"#Index\">Index</a>\n\n4 April 2019: Forked from the last version of my (private, 'cause it's a historic mess) kernel for the PetFinder competition. Some simple EDA is done on the data to generate and look at a set of features. These features are input to three of the SciKit regressor models: GBR, SVR, and MLP (neural network) and the output from each saved in files. The three model outputs are then used to make input features for simple stacking.\n\nRun the different models on the dataset:<br>\n(v1) LB ~ 0.416,  Train = 0.555 (42.4%),   GSCV:  Test = 0.412, Train = 0.576 --- **GBR model** fit and **Show EDA plots**<br>\n(v2) LB ~ 0.401,  Train = 0.520 (40.0%),   GSCV:  Test = 0.378, Train = 0.837 --- **SVR model fit**  (No EDA plots) <br>\n(v3) cancelled. <br>\n(v4) LB = 0.???,  Train = 0.426 (37.2%),   GSCV:  Test = 0.401, Train = 0.449 --- **MLP model fit**  (No EDA plots) <br>\n<br>\n5 Apr 2019: Added a dataset with the outputs from the versions above; will read these in to do stacking.<br>\n(Irritating: adding the dataset requires another layer of dirs under input/; adjust dat_dir for this.)<br>\nThe Stacking Test score is very high - kind of strange... probably some kind of over fitting? v5 used alpha = 1.5, re-run v6 with alpha = 3.5 for a little more regularization. The Test values seem to be around 0.6 (mostly), or 0.5 (sometimes) or 0.4 (least frequently) -- Strange, maybe this depends on how the Group k-Fold divides the samples?<br>\n(v5) LB=0.???,  Train = 0.628 (46.0%),   GSCV:  Test = 0.627, Train = 0.629 --- **Stacking model fit**; uses the previous 3 fits outputs<br>\n(v6) LB=0.???,  Train = 0.612 (45.5%),   GSCV:  Test = 0.609, Train = 0.610 --- **Stacking model fit**; alpha ~ 3.5<br>\nFor stacking, changed the Group k-Fold to Repeat k-Fold w/3 repeats x 6 splits; gives less test variation but doesn't change the final all-training fit -- smallest alpha is best fit, usually. Do an alpha ~ 5 stacking fit:<br>\n(v7) LB=0.???,  Train = 0.600 (44.6%),   GSCV:  Test = 0.589, Train = 0.589 --- **Stacking model fit**; alpha ~ 5.0 <br>\n(v8,9) LB=0.???,  Train = 0.577 (44.8%),   GSCV:  Test = 0.557, Train = 0.557 --- **Stacking model fit**; alpha ~ 8.0 <br>\n**  - - - - - - - - - ** <br>\n17 Apr 2019: Re-run these using the new phase-2 test data (3972 vs 3948 values). <br>\n(v10) Private=0.372 (LB~0.416) Train = 0.534 (41.7%),   GSCV:  Test = 0.412, Train = 0.575 --- **GBR model** fit and **Show EDA plots**<br>\n(v11) Private=0.350 (LB~0.401) Train = 0.520 (40.0%),   GSCV:  Test = 0.378, Train = 0.837 --- **SVR model fit**  (No EDA plots) <br>\n(v12) Private=0.363 (LB=0.???) Train = 0.430 (37.0%),   GSCV:  Test = 0.400, Train = 0.444 --- **MLP model fit**  (No EDA plots) <br>\nStacking the previous 3 fits outputs; \"Demo2\" version is used to distinquish them from the first-round outputs. For stacking, use MLP (5,3) model, and changed the Group k-Fold to Repeat k-Fold w/3 repeats x 6 splits; gives less test variation but doesn't change the final all-training fit.<br>\nDo stacking with different values of alpha (listed here in alpha order):<br>\n(v21) Private=0.342 (LB=0.???) Train = 0.599 (45.7%), GSCV: Test = 0.600, Train = 0.601 --- **Stacking model fit**; alpha ~ 0.5 <br>\n(v13) Private=0.357 (LB=0.???) Train = 0.593 (44.2%),   GSCV:  Test = 0.593, Train = 0.593 --- **Stacking model fit**; alpha ~ 1.5 <br>\n(v14) Private=0.373 (LB=0.???) Train = 0.569 (43.1%),   GSCV:  Test = 0.575, Train = 0.575 --- **Stacking model fit**; alpha ~ 3.5<br>\n(v17) Private=0.373 (LB=0.???) Train = 0.552 (43.6%),   GSCV:  Test = 0.562, Train = 0.562 --- **Stacking model fit**; alpha ~ 4.5<br>\n(v15) Private=0.376 (LB=0.???) Train = 0.560 (42.7%),   GSCV:  Test = 0.553, Train = 0.553 --- **Stacking model fit**; alpha ~ 5.5 <br>\n(v16) Private=0.373 (LB=0.???) Train = 0.534 (41.4%), GSCV: Test = 0.539, Train = 0.539 --- **Stacking model fit**; alpha ~ 8.5 <br>\n(v22) Private=0.376 (LB=0.???) Train = 0.533 (41.4%), GSCV: Test = 0.534, Train = 0.534 --- **Stacking model fit**; alpha ~ 10.5 <br>\n(v18) **Private=0.378** (LB=0.???) Train = 0.524 (41.2%), GSCV: Test = 0.529, Train = 0.529 --- **Stacking model fit**; alpha ~ 12.5 <br>\n(v23) Private=0.xxx (LB=0.???) Train = 0.524 (41.0%), GSCV: Test = 0.500, Train = 0.497 --- **Stacking model fit**; alpha ~ 14.5 <br>\n(v19) Private=0.376 (LB=0.???) Train = 0.523 (41.0%), GSCV: Test = 0.504, Train = 0.504 --- **Stacking model fit**; alpha ~ 17.5 <br>\n(v20) Private=0.376 (LB=0.???) Train = 0.497 (39.6%), GSCV: Test = 0.480, Train = 0.477 --- **Stacking model fit**; alpha ~ 25.5 <br>\n\n\n`\n"},{"metadata":{"_uuid":"dbccc732761274e611170d0af97521488f5c9c98"},"cell_type":"markdown","source":"## <a id=\"WhereRunning\">Where are we Running?</a>\nBack to <a href=\"#Index\">Index</a>\n\nCan select some differences if running on Kaggle or my local machine."},{"metadata":{"_uuid":"73be1cc01b19d61c4c71785d4ae0264cdb3498cd","trusted":true},"cell_type":"code","source":"# Running on Kaggle (True), or on my local machine (False)\nLOCATION_KAGGLE = True\n\n# Put this here too, to easily run with/without OPTR:\n# Use the optimized rounder?\nUSE_OPTR = False\n\n# Generate all the various output plots related to the EDA portion\nSHOW_EDA = True\n\n# If not on Kaggle, is my memory stick available?\nGOT_STICK = True\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bece87ba3d17cba718969af7aa428eb27661c271","trusted":true},"cell_type":"code","source":"# version string to include in filenames of plots, etc. (except submission)\nversion_str = \"vDemo2\"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7d2dde5185a98ce0d5fd41ffcdf480aa58f7eaa0","trusted":true},"cell_type":"code","source":"# general things used for EDA steps\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\nimport os\nfrom time import time\nfrom time import strftime\n\nimport json\n\nfrom collections import Counter\n\nfrom sklearn.decomposition import TruncatedSVD","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# For fun, plot the Private score vs the stacking alpha parameter...\nstack_alpha =   [ 0.5,   1.5,   3.5,    4.5,    5.5,    8.5,    10.5,    12.5,   17.5,   25.5]\nstack_private = [0.342, 0.357, 0.3731, 0.3727, 0.3761, 0.3732,  0.3765,  0.3778, 0.3757, 0.3757]\nplt.plot(stack_alpha,stack_private,'o')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"55e99f96713a51c82dca48ff024da2666c117681","trusted":true},"cell_type":"code","source":"# show the files and dirs\nprint(os.listdir(\"../input/\"))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5f71c3f5468be985f0c2fa5164f7d55b4fba20e3","trusted":true},"cell_type":"code","source":"# Create an output directory for this version, if running locally;\n# otherwise use the current dir when on Kaggle.\nif LOCATION_KAGGLE:\n    out_dir = \".\"\nelse:\n    out_dir = \"Out_\"+version_str\n    try:\n        os.mkdir(out_dir)\n    except FileExistsError:\n        pass","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"98501aaa57616d8e1041799d21b552fc508efc32","trusted":true},"cell_type":"code","source":"# Repeatable or roll-the-dice?\n# The seed is set once here at beginning of notebook.\n#\n# A fixed value (changed occasionally with new submission)\nRANDOM_SEED = 233   # new for v7\n#\n# Uncomment this to get a time-based random value, 0 to 1023\n##RANDOM_SEED = int(time()) % 2**10\n#\n# in either case initialize the seed\nnp.random.seed(RANDOM_SEED)\nprint(RANDOM_SEED)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1c613ab97d006f3f10d79dafa2760adda684095a"},"cell_type":"markdown","source":"## <a id=\"DataProcessing\">Reading and Processing csv Data Files</a>\nBack to <a href=\"#Index\">Index</a>"},{"metadata":{"_uuid":"5760e349e6ac33d64fdc414017fa3f728c3cb25e","trusted":true},"cell_type":"code","source":"# Where are the data files\n# Data dir\n# With just the PetFinder data, get it from input directly:\nif LOCATION_KAGGLE:\n    ##dat_dir =\"../input/\"\n    # With the dataset petfinder-outputs-for-stacking added, get data from:\n    dat_dir =\"../input/petfinder-adoption-prediction/\"\nelse:\n    dat_dir =\"../input/\"\n    \n# CSV files\nbreed_csv = \"breed_labels.csv\"\ncolor_csv = \"color_labels.csv\"\nstate_csv = \"state_labels.csv\"\ntrain_csv = \"train/train.csv\"\ntest_csv = \"test/test.csv\"\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"41166a4fa8164cb03822d59e742b6270332255cb"},"cell_type":"markdown","source":"### Read the files and do basic feature processing\nThis is all done in this one cell to prevent getting out of synch."},{"metadata":{"_uuid":"18391bb8b351373908811165c0b3d9d5ccc2fa8f","trusted":true},"cell_type":"code","source":"# Read the files and do All the basic feature processing\n\n# time it\nt_preproc = time()\n\n# Read in the ancillary data\ndf_breed = pd.read_csv(dat_dir+breed_csv)\ndf_color = pd.read_csv(dat_dir+color_csv)\ndf_state = pd.read_csv(dat_dir+state_csv)\n\n# Read in the train and test data\n#\n# Train\ndf_train = pd.read_csv(dat_dir+train_csv)\n# one little correction: # 8852(dog, but listed with type=2)\ndf_train.loc[8852,'Type'] = 1\n#\n# Test\n# Read from the test csv file.\n# ... Or can have the test dataframe created from raw training data to\n# see if test features are all getting processed the same way as training.\n# Check that by looking at Compare Train and Test averages below:\n# the z-scores should mostly be -2 to 2 with no large (>5) values.\n# This is an infrequent but useful check.\nTEST_FROM_TRAIN = False\nif TEST_FROM_TRAIN:\n    # Get a test set from the input train data, size it close to real test set\n    n_known_test = 3946\n    rand_order = np.random.permutation(len(df_train))\n    df_test = df_train.loc[rand_order[0 : n_known_test]].copy().reset_index()\n    df_train = df_train.loc[rand_order[n_known_test : ]].copy().reset_index()\n    TEST_SUBDIR = 'train'\nelse:\n    # Read in the actual test data\n    df_test = pd.read_csv(dat_dir+test_csv)\n    # Add AdoptionSpeed column, to be filled in later\n    df_test['AdoptionSpeed']=-1\n    TEST_SUBDIR = 'test'\n\n    \n# Sections here below mess with various of the features\n# (either inplace or creating new columns)\n# to make them (what I think is) better for ML ;-)\n\n\n# - - -\n# The Breed dataframe:\n# Include the average class value for each Breed1 in the df_breed dataframe\n# This allows essentially \"target encoding\".\n#\n# Change the df_breed index to be the BreedID\ndf_breed = df_breed.set_index('BreedID')\n# Get the mean adoption speed and number for each breed\nbreed1_means = df_train.groupby('Breed1').AdoptionSpeed.agg(['mean'])\nbreed1_count = df_train.groupby('Breed1').AdoptionSpeed.agg(['count'])\n# Put the mean and count in new columns\ndf_breed['AveSpeed'] = breed1_means\ndf_breed['TrainCount'] = breed1_count\n# Calculate the average of all average speeds that have 4 or more counts:\nmin_count = 4\ngot_min = df_breed['TrainCount'] >= min_count\nave_avespeed = df_breed.loc[got_min,'AveSpeed'].mean()\n# quantize the got_min values to 0.05:\ndf_breed.loc[got_min,'AveSpeed'] = 0.05*(20.0*df_breed.loc[got_min,'AveSpeed']+0.5).astype(int)\n# and fill missing and small-count values\ndf_breed['AveSpeed'].fillna(ave_avespeed, inplace=True)\ndf_breed['TrainCount'].fillna(0.0, inplace=True)\n# For few-count breeds, replace the AveSpeed with the average average\ndf_breed.loc[df_breed['TrainCount'] < min_count,'AveSpeed'] = ave_avespeed \n# and add a 0 breed (for Breed2 mapping):\ndf_breed.loc[0] = ave_avespeed\n\n\n# - - -\n# Training and Test Dataframes\n\n# NaNs\n# All NaNs come from the Name or Description fields, so:\ndf_train[\"Name\"].fillna('?', inplace=True)\ndf_test[\"Name\"].fillna('?', inplace=True)\ndf_train[\"Description\"].fillna('?', inplace=True)\ndf_test[\"Description\"].fillna('?', inplace=True)\n\n# Name\n# Will some indication of the length of Name and Description be useful?\n# Create log-lengths; Nan ones will have '?' and give 0 for log(length).\n#\n# Flag Long names, ones that are more than 10 characters\ndf_train[\"NameLong\"] = 1 * (np.log(df_train[\"Name\"].apply(len)) > np.log(10.5))\n# Flag a Given name that is not Long\ndf_train[\"NameGiven\"] = 1 * ((np.log(df_train[\"Name\"].apply(len)) > np.log(2.5)) &\n                            ~(df_train[\"Name\"] == 'No Name') &\n                            ~(df_train[\"Name\"] == 'Puppy') &\n                            ~(df_train[\"Name\"] == 'Puppies') &\n                            ~(df_train[\"Name\"] == 'No Name Yet') &\n                            ~(df_train[\"Name\"] == 'Kitten') &\n                            ~(df_train[\"Name\"] == 'Kittens') &\n                            ~(df_train[\"NameLong\"] == 0))\n#\n# Same for Test\ndf_test[\"NameLong\"] = 1 * (np.log(df_test[\"Name\"].apply(len)) > np.log(10.5))\ndf_test[\"NameGiven\"] = 1 * ((np.log(df_test[\"Name\"].apply(len)) > np.log(2.5)) &\n                            ~(df_test[\"Name\"] == 'No Name') &\n                            ~(df_test[\"Name\"] == 'Puppy') &\n                            ~(df_test[\"Name\"] == 'Puppies') &\n                            ~(df_test[\"Name\"] == 'No Name Yet') &\n                            ~(df_test[\"Name\"] == 'Kitten') &\n                            ~(df_test[\"Name\"] == 'Kittens') &\n                            ~(df_test[\"NameLong\"] == 0))\n\n\n# Description\n# Most entries have descriptions, so encode the length of the description usefully.\n# First, assign numerical log-length values, will be 0 if it was NaN:\ndf_train[\"LenDescr\"] = np.log(df_train[\"Description\"].apply(len))\ndf_test[\"LenDescr\"] = np.log(df_test[\"Description\"].apply(len))\n# Using these it is possible that the (log-)lengths provide\n# a too fine-grained label letting ML memorize the answers, but it is not generalizable.\n# Instead break the description lengths into a discrete set of values based on percentile,\n# e.g., the log-lengths have 25% and 75% values of: 4.76, 6.06.\ndescr_percents = list(range(5, 99, 10))+[98]\nlevels = np.zeros(len(descr_percents))\nnp.percentile(df_train[\"LenDescr\"].values.copy(), descr_percents, out=levels, overwrite_input=True)\n##print(\"Description levels, percentile-log spaced, number of words:\", (np.exp(levels)/5.0).astype(int))\ndescr_lens = df_train[\"LenDescr\"].copy()\ndf_train[\"LenDescr\"] = 0\nfor il, level in enumerate(levels):\n    above = descr_lens > level\n    df_train.loc[above,\"LenDescr\"] = il+1\ndescr_lens = df_test[\"LenDescr\"].copy()\ndf_test[\"LenDescr\"] = 0\nfor il, level in enumerate(levels):\n    above = descr_lens > level\n    df_test.loc[above,\"LenDescr\"] = il+1\n# Use squared too?\ndf_train[\"LenDescrSq\"] = (df_train[\"LenDescr\"] - 5.0)**2\ndf_test[\"LenDescrSq\"] = (df_test[\"LenDescr\"] - 5.0)**2\n\n# State\n# Use state-based data to create other columns:\n# state GDP: https://en.wikipedia.org/wiki/List_of_Malaysian_states_by_GDP\nstate_gdp = {\n    41336: 116.679,\n    41325: 40.596,\n    41367: 23.02,\n    41401: 190.075,\n    41415: 5.984,\n    41324: 37.274,\n    41332: 42.389,\n    41335: 52.452,\n    41330: 67.629,\n    41380: 5.642,\n    41327: 81.284,\n    41345: 80.167,\n    41342: 121.414,\n    41326: 280.698,\n    41361: 32.270\n}\n# state population: https://en.wikipedia.org/wiki/Malaysia\nstate_population = {\n    41336: 33.48283,\n    41325: 19.47651,\n    41367: 15.39601,\n    41401: 16.74621,\n    41415: 0.86908,\n    41324: 8.21110,\n    41332: 10.21064,\n    41335: 15.00817,\n    41330: 23.52743,\n    41380: 2.31541,\n    41327: 15.61383,\n    41345: 32.06742,\n    41342: 24.71140,\n    41326: 54.62141,\n    41361: 10.35977\n}\ndf_train[\"StateGDP\"] = df_train.State.map(state_gdp)\ndf_train[\"StatePop\"] = df_train.State.map(state_population)\ndf_train[\"StateGDPpp\"] = df_train[\"StateGDP\"]/df_train[\"StatePop\"]\ndf_test[\"StateGDP\"] = df_test.State.map(state_gdp)\ndf_test[\"StatePop\"] = df_test.State.map(state_population)\ndf_test[\"StateGDPpp\"] = df_test[\"StateGDP\"]/df_test[\"StatePop\"]\n#\n# Finally, subtract 41300 from the State to be in a better range\ndf_train['State'] = df_train['State'] - 41300\ndf_test['State'] = df_test['State'] - 41300\n\n# Vaccinated, Dewormed, Sterilized:\n#\n# Three options for adjusting these...\n# i) Leave as-is: 1=Yes 2=No 3=Not Sure\n# ii) Change to just 0=No, Yes=1:\nif False:\n    # Initial thinking was that Not sure is essentially the same as No...\n    # - Turn 3 = Not Sure into 2 = No; then,\n    # - Change 1=Yes 2=No to 1=Yes 0=No\n    ns_cols = ['Vaccinated', 'Dewormed', 'Sterilized']\n    for col in ns_cols:\n        df_train.loc[df_train[col]==3, col] = 2\n        df_train[col] = 2 - df_train[col]\n        # and for Test\n        df_test.loc[df_test[col]==3, col] = 2\n        df_test[col] = 2 - df_test[col]\n# iii) Change to No=0, Not sure = 1, Yes=2\nif True:\n    # But after looking at https://www.kaggle.com/jaseziv83/extensive-pet-finder-eda\n    # keep the three values separate and/but put Not sure between the other two.\n    # - Change 3 (not sure) to 1.5\n    # - Scale/Flip/Offset/Integer to finally have: No=0, Not sure = 1, Yes=2\n    ns_cols = ['Vaccinated', 'Dewormed', 'Sterilized']\n    for col in ns_cols:\n        df_train.loc[df_train[col]==3, col] = 1.5\n        df_train[col] = 4 - 2*df_train[col]\n        df_train[col] = df_train[col].astype(int)\n        # and for Test\n        df_test.loc[df_test[col]==3, col] = 1.5\n        df_test[col] = 4 - 2*df_test[col]\n        df_test[col] = df_test[col].astype(int)\n              \n# Color1,2,3 - experiment with alternate Color variables\n# \"Please note that sequence of colors actually does not matter,\n#  it is just to indicate which color(s) are dominant within the pet.\"\n# Code: 1-Black, 2-Brown, 3-Golden, 4-Yellow, 5-Cream, 6-Gray, 7-White\n# Map color codes to rough brighness range\ndef clr_bright(clr_code):\n    bright_level = [0,0,2,5,5,9,8,10]\n    return bright_level[clr_code]\n# Train\n# Introduce ColorSolid and ColorMany flags\ndf_train['ColorSolid'] = 1 * (df_train['Color2'] == 0)\ndf_train['ColorMany'] = 1 * (df_train['Color3'] != 0)\n# General brightness and contrast of the animal\n# ColorBright\ndf_train['ColorBright'] = 0.5*(df_train['Color1'].apply(clr_bright) + df_train['Color2'].apply(clr_bright))\nsolids = (df_train['ColorSolid'] == 1)\ndf_train.loc[solids,'ColorBright'] = df_train.loc[solids,'Color1'].apply(clr_bright)\ndf_train['ColorBrightSq'] = (df_train['ColorBright'] - 6.0)**2\n# ColorContrast\ndf_train['ColorContrast'] = np.abs(df_train['Color1'].apply(clr_bright) - df_train['Color2'].apply(clr_bright))\ndf_train.loc[solids,'ColorContrast'] = 0\ndf_train['ColorContrastSq'] = (df_train['ColorContrast'] - 4.5)**2\n#\n# Test\ndf_test['ColorSolid'] = 1 * (df_test['Color2'] == 0)\ndf_test['ColorMany'] = 1 * (df_test['Color3'] != 0)\n# General brightness and contrast of the animal\ndf_test['ColorBright'] = 0.5*(df_test['Color1'].apply(clr_bright) + df_test['Color2'].apply(clr_bright))\nsolids = (df_test['ColorSolid'] == 1)\ndf_test.loc[solids,'ColorBright'] = df_test.loc[solids,'Color1'].apply(clr_bright)\ndf_test['ColorBrightSq'] = (df_test['ColorBright'] - 6.0)**2\ndf_test['ColorContrast'] = np.abs(df_test['Color1'].apply(clr_bright) - df_test['Color2'].apply(clr_bright))\ndf_test.loc[solids,'ColorContrast'] = 0\ndf_test['ColorContrastSq'] = (df_test['ColorContrast'] - 4.5)**2\n\n# Breed:\n# Learned about \"Mixed Breed\" breed thanks to https://www.kaggle.com/artgor/exploration-of-data-step-by-step\n# There are a handful of Train Breed1=0 values, replace with 307 Mixed Breed (Dog), 266 Domestic... (Cat):\nbreed0s = (df_train['Breed1'] == 0)\nif sum(1*breed0s) > 0:\n    # Can look at them: 4 Dogs, 1 Cat\n    ##print(df_train.loc[breed0s])\n    # They all have valid Breed2s so just do\n    df_train.loc[breed0s,'Breed1'] = 1 * df_train.loc[breed0s,'Breed2']\n# do the same for Test if there are any\nbreed0s = (df_test['Breed1'] == 0)\nif sum(1*breed0s) > 0:\n    df_test.loc[breed0s,'Breed1'] = 1 * df_test.loc[breed0s,'Breed2']\n#\n# Add MixedBreed and Domestic columns...\n# These are not simply Breed2 != 0 since\n# For Dogs \"Mixed Breed\"(307) is one of the Breed1,2 values?!\n# For Cats there are 3 \"Domestic...\" values: 264,265,266\n#\ndf_train['MixedBreed'] = 1 * ((df_train['Type'] == 1) & ( (df_train['Breed1'] == 307) |\n                        (df_train['Breed2'] == 307) )  )\ndf_test['MixedBreed'] = 1 * ((df_test['Type'] == 1) & ( (df_test['Breed1'] == 307) |\n                        (df_test['Breed2'] == 307) )  )\n\ndf_train['Domestic'] = 1 * ((df_train['Type'] == 2) & ( (df_train['Breed1'] == 266) |\n                        (df_train['Breed2'] == 266) | (df_train['Breed1'] == 265) |\n                        (df_train['Breed2'] == 265) | (df_train['Breed1'] == 264) |\n                        (df_train['Breed2'] == 264)  )  )\ndf_test['Domestic'] = 1 * ((df_test['Type'] == 2) & ( (df_test['Breed1'] == 266) |\n                        (df_test['Breed2'] == 266) | (df_test['Breed1'] == 265) |\n                        (df_test['Breed2'] == 265) | (df_test['Breed1'] == 264) |\n                        (df_test['Breed2'] == 264)  )  )\n\ndf_train['NamedBreed'] = 1 * ~( (df_train['MixedBreed'] == 1) | \n                            (df_train['Domestic'] == 1) )\ndf_test['NamedBreed'] = 1 * ~( (df_test['MixedBreed'] == 1) | \n                            (df_test['Domestic'] == 1) )\n#\n# Use the df_breed to add columns of  Breed1Speed, Breed2Speed\ndf_train['BreedSpeed'] = [ df_breed.loc[brID, 'AveSpeed'] for brID in list(df_train['Breed1']) ]\ndf_train['BreedSpeed2'] = [ df_breed.loc[brID, 'AveSpeed'] for brID in list(df_train['Breed2']) ]\ndf_test['BreedSpeed'] = [ df_breed.loc[brID, 'AveSpeed'] for brID in list(df_test['Breed1']) ]\ndf_test['BreedSpeed2'] = [ df_breed.loc[brID, 'AveSpeed'] for brID in list(df_test['Breed2']) ]\n\n# PhotoAmt\n# replace with np.log( +1)\ncol = 'PhotoAmt'\ndf_train[col] = np.log(df_train[col] + 1.0)\ndf_test[col] = np.log(df_test[col] + 1.0)\n# Lump 12 or more photos in one last bin:\ndf_train.loc[(df_train['PhotoAmt'] > 2.5),'PhotoAmt'] = 2.7\ndf_test.loc[(df_test['PhotoAmt'] > 2.5),'PhotoAmt'] = 2.7\n\n# VideoAmt\n# set a max of 3\ndf_train.loc[(df_train['VideoAmt'] > 3),'VideoAmt'] = 3\ndf_test.loc[(df_test['VideoAmt'] > 3),'VideoAmt'] = 3\n\n# Quantity: 10 or more are all set to 10\ndf_train.loc[(df_train['Quantity'] > 9),'Quantity'] = 10\ndf_test.loc[(df_test['Quantity'] > 9),'Quantity'] = 10\n\n# Age\n#replace with np.log( +1)\ncol = 'Age'\ndf_train[col] = np.log(df_train[col] + 1.0)\ndf_test[col] = np.log(df_test[col] + 1.0)\n# Put large values into 2 bins\n# 18.5 to 48.5\ndf_train.loc[(df_train['Age'] > np.log(18.5+1)) & \n             (df_train['Age'] < np.log(48.5+1)),'Age'] = np.log(30+1)\ndf_test.loc[(df_test['Age'] > np.log(18.5+1)) & \n             (df_test['Age'] < np.log(48.5+1)),'Age'] = np.log(30+1)\n# 48.5 +\ndf_train.loc[(df_train['Age'] > np.log(48.5+1)),'Age'] = np.log(60+1)\ndf_test.loc[(df_test['Age'] > np.log(48.5+1)),'Age'] = np.log(60+1)\n#\n# AgeSq\n# Add a 'squared' term = (log(Age) - 2.0)**2\ndf_train['AgeSq'] = (df_train['Age'] - 2.0)**2\ndf_test['AgeSq'] = (df_test['Age'] - 2.0)**2\n\n# Fee\n#replace with np.log( +1)\ncol = 'Fee'\ndf_train[col] = np.log(df_train[col] + 1.0)\ndf_test[col] = np.log(df_test[col] + 1.0)\n#\n# 2.5 to 10.5\ndf_train.loc[(df_train['Fee'] > np.log(2.5+1)) & \n             (df_train['Fee'] < np.log(10.5+1)),'Fee'] = np.log(6+1)\ndf_test.loc[(df_test['Fee'] > np.log(2.5+1)) & \n             (df_test['Fee'] < np.log(10.5+1)),'Fee'] = np.log(6+1)\nfor fee_low in np.arange(10.0, 50.0+1, 10.0):\n    # x0.5 to X0.5\n    df_train.loc[(df_train['Fee'] > np.log(fee_low+1.5)) & \n             (df_train['Fee'] < np.log(fee_low+11.5)),'Fee'] = np.log(fee_low+6)\n    df_test.loc[(df_test['Fee'] > np.log(fee_low+1.5)) & \n             (df_test['Fee'] < np.log(fee_low+11.5)),'Fee'] = np.log(fee_low+6)\nfor fee_low in np.arange(60.0, 360.0+1, 50.0):\n    # x0.5 to X0.5\n    df_train.loc[(df_train['Fee'] > np.log(fee_low+1.5)) & \n             (df_train['Fee'] < np.log(fee_low+51.5)),'Fee'] = np.log(fee_low+25)\n    df_test.loc[(df_test['Fee'] > np.log(fee_low+1.5)) & \n             (df_test['Fee'] < np.log(fee_low+51.5)),'Fee'] = np.log(fee_low+25)\n# > 410\ndf_train.loc[(df_train['Fee'] > np.log(410.5+1)),'Fee'] = np.log(600+1)\ndf_test.loc[(df_test['Fee'] > np.log(410.5+1)),'Fee'] = np.log(600+1)\n#\n# FeeSq\n# Add a 'squared' term = (log(Fee) - 4.1)**2\ndf_train['FeeSq'] = (df_train['Fee'] - 4.1)**2\ndf_test['FeeSq'] = (df_test['Fee'] - 4.1)**2\n\n# Add a TypeBlur with random spread of values:\n# Discrete blur values:\ndf_train['TypeBlur'] = 2.0*df_train['Type'] + 0.1*np.random.randint(0,11,size=len(df_train)) - 3.5\ndf_test['TypeBlur'] = 2.0*df_test['Type'] + 0.1*np.random.randint(0,11,size=len(df_test)) - 3.5\n\n# Add a SterilBlur with random spread of values:\n# Discrete blur values:\ndf_train['SterilBlur'] = 2.0*df_train['Sterilized'] + 0.1*np.random.randint(0,11,size=len(df_train)) - 2.5\ndf_test['SterilBlur'] = 2.0*df_test['Sterilized'] + 0.1*np.random.randint(0,11,size=len(df_test)) - 2.5\n\n# RescuerID - \"to use or not to use?\"\n# See the discussion at:\n# https://www.kaggle.com/c/petfinder-adoption-prediction/discussion/78511#486890\n# A valid reason to include features based on it is that it gives information on the rescuer.\n# Create RescuerActivity columns based on the code in: \n# https://www.kaggle.com/wrosinski/baselinemodeling\n#\n# Train\nrescuer_count = df_train.groupby(['RescuerID'])['PetID'].count().reset_index()\nrescuer_count.columns = ['RescuerID', 'RescuerActivity']\ndf_train = df_train.merge(rescuer_count, how='left', on='RescuerID')\n# Test\nrescuer_count = df_test.groupby(['RescuerID'])['PetID'].count().reset_index()\nrescuer_count.columns = ['RescuerID', 'RescuerActivity']\ndf_test = df_test.merge(rescuer_count, how='left', on='RescuerID')\n#\n# Do a log scaling on these, but first\n# Scale-down the train values by the test/train ratio\n# of the number of values above a low number (e.g., 2) \ncol = 'RescuerActivity'\n##test2train = len(df_test)/len(df_train)\nn_low = 2\ntest2train = len(df_test.loc[df_test[col] > n_low, col])/ \\\n                    len(df_train.loc[df_train[col] > n_low, col])\ndf_train.loc[df_train[col] > n_low, col] = n_low + 0.5 + \\\n                    test2train * (df_train.loc[df_train[col] > n_low, col] - n_low)\ndf_train[col] = np.log(df_train[col].astype(int))\n# Keep the test values as they are and do log()\ndf_test[col] = np.log(df_test[col])\n\n\nprint(time() - t_preproc)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9e2ed6d44421ca1f2c90b39795820ab2d7ba1652"},"cell_type":"markdown","source":"## <a id=\"SentimentValues\">Sentiment Values from Description</a>\nBack to <a href=\"#Index\">Index</a><br>\n\n\nGet the sentiment values (of the Descriptions) that are available in the json files. <br> \n`score` of the sentiment ranges between -1.0 (negative) and 1.0 (positive) and corresponds to the overall emotional leaning of the text. <br>\n`magnitude` indicates the overall strength of emotion (both positive and negative) within the given text, between 0.0 and +inf. Unlike score, magnitude is not normalized; each expression of emotion within the text (both positive and negative) contributes to the text's magnitude (so longer text blocks may have greater magnitudes). <br>\n`language` contains the language of the document, either passed in the initial request, or automatically detected if absent. <br>\nBased on the json-reading code in https://www.kaggle.com/abhishek/maybe-something-interesting-here ."},{"metadata":{"_uuid":"274d228e80420dd2af3f91395c2ef5cde20329f1","trusted":true},"cell_type":"code","source":"# On kaggle the files are in ../input/train[test]_metadata[sentiment]/<PetID>*.json\n# ( Or with another layer of dir: use data_dir as the starting point)\n# On my maching I have added a layer of directory structure using the PetID's first character:\n#   ../input/train[test]_metadata[sentiment]/<P>_/<PetID>*.json\n# LOCATION_KAGGLE is used to flag where we're running.\n# Additionally, on home machine these are possibly available on memory stick if GOT_STICK is true.\n\ndef get_sentiment(petid, test_train='train'):\n    \"\"\"\n    Get information from petid's (description) sentiment file.\n    Returns: sent_mag, sent_score, sent_lang\n    \"\"\"\n    global LOCATION_KAGGLE, GOT_STICK\n    if LOCATION_KAGGLE :\n        # Kaggle location:\n        filename = dat_dir + test_train + '_sentiment/' + petid + '.json'\n    else:\n        # My location (put in sub-dirs) and optionally read from memory stick\n        if GOT_STICK:\n            stick_loc = '/media/dd/dd_bigStick/PetFinder/input/'\n            filename = stick_loc + test_train + '_sentiment/' + petid[0] + \"_/\" + petid + '.json'\n        else:\n            filename = dat_dir + test_train + '_sentiment/' + petid[0] + \"_/\" + petid + '.json'\n    try:\n        with open(filename, 'r') as f:\n            sentiment = json.load(f)\n        sent_mag = sentiment['documentSentiment']['magnitude']\n        sent_score = sentiment['documentSentiment']['score']\n        sent_lang = sentiment['language']\n    except FileNotFoundError:\n        ##print('No sentiment for Filename:',filename)\n        # Defaults if no description:\n        sent_mag = 0.0     # nothing here\n        sent_score = 0.25  # 0.25 mild positive\n        sent_lang = 'en'\n    #\n    # Quantize sent_mag to 0.5 if above 4.0\n    if sent_mag > 4.0:\n        sent_mag = 0.5*(int(2.0*sent_mag+0.5))\n    # Quantize sent_mag to 0.2 if above 1.5\n    elif sent_mag >1.5:\n        sent_mag = 0.2*(int(5.0*sent_mag+0.5))\n    # otherwise leave as is\n    return sent_mag, sent_score, sent_lang\n\nt_sentiment = time()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"aa9a2ceb76c39245d8bc32f394a163ee092c3287","trusted":true},"cell_type":"code","source":"# TRAINING\n\n# Get the sentiment values and put them in the dataframe(s)\nsent_mags = []\nsent_scores = []\nsent_langs = []\nn_not_found = 0\nfor petid in df_train.PetID:\n    sent_mag, sent_score, sent_lang = get_sentiment(petid, test_train='train')\n    # count the not-found ones\n    if (sent_mag == 0.0) and (abs(sent_score - 0.25) < 0.001):\n        n_not_found += 1\n    sent_mags.append(sent_mag)\n    sent_scores.append(sent_score)\n    sent_langs.append(sent_lang)\nprint(\"Training Sentiment: missing {} out of {}\".format(n_not_found, len(df_train)))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4dba460571c2414912a7e992d5df3d1b47fbe3f8","trusted":true},"cell_type":"code","source":"# Put them in the dataframe; use log(1+ ) for magnitude.\n# SentMag\ndf_train['SentMag'] = np.log(1.0+np.array(sent_mags)) \ndf_train.loc[(df_train['SentMag'] > 2.2),'SentMag'] = 2.7\ndf_train['SentMagSq'] = (df_train['SentMag'] - 1.1)**2\n\n# SentScore\ndf_train['SentScore'] = sent_scores\ndf_train['SentScoreSq'] = (df_train['SentScore'] - 0.1)**2\n\n# SentLang\ndf_train['SentLang'] = sent_langs \n# and convert SentLang to a 0, 1 (0=en)\ndf_train['SentLang'] = 1 * (df_train['SentLang'] == 'en') ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5e97f7af23666c99e75968210108af60fb7d0419","trusted":true},"cell_type":"code","source":"# TEST\n\n# Get the sentiment values and put them in the dataframe(s)\nsent_mags = []\nsent_scores = []\nsent_langs = []\nn_not_found = 0\nfor petid in df_test.PetID:\n    sent_mag, sent_score, sent_lang = get_sentiment(petid, test_train=TEST_SUBDIR)\n    # count the not-found ones\n    if (sent_mag == 0.0) and (abs(sent_score - 0.25) < 0.001):\n        n_not_found += 1\n    sent_mags.append(sent_mag)\n    sent_scores.append(sent_score)\n    sent_langs.append(sent_lang)\nprint(\"Test Sentiment: missing {} out of {}\".format(n_not_found, len(df_test)))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f502046ff2f1a992e7d85df3280e3a590f63f3f5","trusted":true},"cell_type":"code","source":"# Put them in the dataframe; use log(1+ ) for magnitude.\n# SentMag\ndf_test['SentMag'] = np.log(1.0+np.array(sent_mags)) \ndf_test.loc[(df_test['SentMag'] > 2.2),'SentMag'] = 2.7\ndf_test['SentMagSq'] = (df_test['SentMag'] - 1.1)**2\n\n# SentScore\ndf_test['SentScore'] = sent_scores\ndf_test['SentScoreSq'] = (df_test['SentScore'] - 0.1)**2\n\n# SentLang\ndf_test['SentLang'] = sent_langs \n# and convert SentLang to a 0, 1 (0=en)\ndf_test['SentLang'] = 1 * (df_test['SentLang'] == 'en')  \n\n\n\nprint(time() - t_sentiment)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"11bc56cb4eaceb5e4d122c69bd7d561f58146887"},"cell_type":"markdown","source":"## <a id=\"ImageMetadata\">Get Metadata from Profile Images</a>\nBack to <a href=\"#Index\">Index</a><br>\n\nMetadata generated from the pet images are available in json files. Read these files and extract the annotations and values from the first two images (as available)."},{"metadata":{"_uuid":"931d9d7c50559e704a80db5a51a70bd1c7d4bb0b","trusted":true},"cell_type":"code","source":"# On kaggle the files are in ../input/train[test]_metadata[sentiment]/<PetID>*.json\n# On my maching I have added a layer of directory structure using the PetID's first character:\n#   ../input/train[test]_metadata[sentiment]/<P>_/<PetID>*.json\n# LOCATION_KAGGLE is used to flag where we're running.\n# Additionally, on home machine these are possibly available on memory stick if GOT_STICK is true.\n\ndef get_metadata(petid, test_train='train', verbose=False, imgnumstr='1'):\n    \"\"\"\n    Get information from petid's (photo) metadata file.\n    \"\"\"\n    global LOCATION_KAGGLE, GOT_STICK\n    if LOCATION_KAGGLE :\n        # Kaggle location:\n        filename = dat_dir + test_train + '_metadata/' + petid + '-'+imgnumstr+'.json'\n    else:\n        # My location (put in sub-dirs) and optionally read from memory stick\n        if GOT_STICK:\n            stick_loc = '/media/dd/dd_bigStick/PetFinder/input/'\n            filename = stick_loc + test_train + '_metadata/' + \\\n                        petid[0] + \"_/\" + petid + '-'+imgnumstr+'.json'\n        else:\n            filename = dat_dir + test_train + '_metadata/' + \\\n                        petid[0] + \"_/\" + petid + '-'+imgnumstr+'.json'\n    try:\n        with open(filename, 'r') as f:\n            thismeta = json.load(f)\n            try:\n                if verbose:\n                    print('Got metadata for Filename:', filename)\n                    if GOT_STICK:\n                        stick_loc = '/media/dd/dd_bigStick/PetFinder/input/'\n                        imgfilename = stick_loc + test_train + '_images/' + petid[0] + \\\n                                                        \"_/\" + petid + '-'+imgnumstr+'.jpg'\n                        print('View Image with:  eog', imgfilename)\n                # assemble the combined string of annotations and values\n                annots_str = ''\n                annots_valstr = ''\n                for ilab in range(len(thismeta['labelAnnotations'])):\n                            this_annot = thismeta['labelAnnotations'][ilab]['description']\n                            this_val = thismeta['labelAnnotations'][ilab]['score']\n                            annots_str += this_annot.replace(\" \",\"_\")+\" \"\n                            # Multiply by 100 and quantize to 1\n                            annots_valstr += str(int(100.0*this_val+0.5))+\" \"\n                            if verbose:\n                                print( str(ilab),\": \", \"{:.5f}  {}\".format(this_val, this_annot))\n            except:     # KeyError IndexError\n                annots_str = 'none '\n                annots_valstr = '0 '\n        #\n    except FileNotFoundError:\n        if verbose:\n            print('No metadata for Filename:', filename)\n        # Defaults if no description:\n        annots_str = 'none '\n        annots_valstr = '0 '\n    # return the string of values and string of annotations\n    return annots_valstr, annots_str\n\n\nt_metadata = time()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"85deb6adca0a7a49b7c263204cf50533044c12a6","trusted":true},"cell_type":"code","source":"# Put the metadata information into the dfs\n# Combine values and annotations from image-1 and image-2 (if present)\n\n# TRAINING\nmeta_values = []\nmeta_annots = []\nn_not_found = 0\nfor petid in df_train.PetID:\n    # get the first image info\n    annots_valstr, annots_str = get_metadata(petid, test_train='train', imgnumstr='1')\n    if annots_str == 'none ':\n        n_not_found += 1\n    # get second image info\n    ann2_valstr, ann2_str = get_metadata(petid, test_train='train', imgnumstr='2')\n    if ann2_str == 'none ':\n        meta_values.append(annots_valstr)\n        meta_annots.append(annots_str)\n    else:\n        # combine images 1 and 2 info\n        meta_values.append(annots_valstr + ann2_valstr)\n        meta_annots.append(annots_str + ann2_str)\nprint(\"Training Metadata-1: missing {} out of {}\".format(n_not_found, len(df_train)))\ndf_train['MetaValues'] = meta_values\ndf_train['MetaAnnots'] = meta_annots\n\n# TEST\nmeta_values = []\nmeta_annots = []\nn_not_found = 0\nfor petid in df_test.PetID:\n    annots_valstr, annots_str = get_metadata(petid, test_train=TEST_SUBDIR, imgnumstr='1')\n    if annots_str == 'none ':\n        n_not_found += 1\n    # get second image info\n    ann2_valstr, ann2_str = get_metadata(petid, test_train=TEST_SUBDIR, imgnumstr='2')\n    if ann2_str == 'none ':\n        meta_values.append(annots_valstr)\n        meta_annots.append(annots_str)\n    else:\n        # combine images 1 and 2 info\n        meta_values.append(annots_valstr + ann2_valstr)\n        meta_annots.append(annots_str + ann2_str)\nprint(\"Test Metadata-1: missing {} out of {}\".format(n_not_found, len(df_test)))\ndf_test['MetaValues'] = meta_values\ndf_test['MetaAnnots'] = meta_annots","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c5ec442720894d1f4d990b7ad77ac58942b9add3","trusted":true},"cell_type":"code","source":"# Demo of get_metadata: Look at a single one - two strings are returned:\n##get_metadata(df_train.loc[8852,'PetID'], test_train='train', verbose=True, imgnumstr='2')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"66fe29718313f0c0df8c21546eb1cd114d6bc297"},"cell_type":"markdown","source":"## <a id=\"SVDImageMeta\">Use SVD on Image Metadata</a>\nBack to <a href=\"#Index\">Index</a>"},{"metadata":{"_uuid":"22fb2c86806c5e92048c12b284e2104d0f80318a","trusted":true},"cell_type":"code","source":"# Get a list of all the common annots\nall_annots = ''\nfor annot in df_train['MetaAnnots']:\n    all_annots += annot\n\n# String with all the annotations separated by spaces:\nprint(\"All annotations, total characters:\",len(all_annots))\n# Dictionary of annotations and number of times they appeared in the corpus,\n# it's in undefined order.\nannots_dict = Counter(all_annots.split(\" \"))\nprint(\"Number of distince annotation strings:\",len(annots_dict))\n##annots_dict\n#\n# List of tuples, (number, annot), sorted by reverse number\nsorted_num_annot = sorted([(value, key) for (key, value) in annots_dict.items() if value > 30],reverse=True)\n# List of tuples, (annot, number), sorted alphabetically\nsorted_annot_num = sorted([(key, value) for (key, value) in annots_dict.items() if value > 30])\n\nprint(\"Annotations with >30 samples:\",len(sorted_num_annot))\n\n# Could show them all...\n##sorted_annots_num","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d7a42b9499525cb1dbc73a9a389366c85637bbd7","trusted":true},"cell_type":"code","source":"# Get the most popular annotation values\n# cut off at a larger number of appearances\nn_popular = 300\npop_annots = [anot for num, anot in sorted_num_annot if num > n_popular]\n\n# remove some very generic ones\nremove_annots = ['dog_like_mammal','dog_breed','dog','dog_breed_group',\n                 'cat','cat_like_mammal','mammal']\nfor remove_this in remove_annots:\n    try:\n        pop_annots.remove(remove_this)\n    except ValueError:\n        pass\n    \n# Show the list of popular ones\nif SHOW_EDA:\n    print(len(pop_annots))\n    pop_annots","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bb5f49f9be66ebdba55664a515fc39de5f2761f4","trusted":true},"cell_type":"code","source":"# Make a dictionary to map pop_annots values to their vector index\n# ( can use pop_annots[index] to go the other way )\npop_index_of_annot = {}\nfor ianno, this_anno in enumerate(pop_annots):\n    pop_index_of_annot[this_anno] = ianno\n    \n# Check going either way...\n##pop_annots[13]\n##pop_index_of_annot['companion_dog']","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cfcd0734a697309d1a0ddb464d5ff4c6553fcf85","trusted":true},"cell_type":"code","source":"# Routine to get lists of annotations and values from df item\ndef get_annots_values(df_row):\n    annots = df_row['MetaAnnots']\n    values = df_row['MetaValues']\n    vallist_str = values.split(\" \")[0:-1]\n    vallist = [int(this) for this in vallist_str]\n    annolist = annots.split(\" \")[0:-1]\n    return annolist, vallist\n\n# For each training sample create/populate a vector of the popular annotations\n# Get the AdoptionSpeed and Type also for color-coding below.\n# Use MetaValues to assign real values (rather than just 0,1)\ntrain_vectors = []\ntrain_speeds = []\ntrain_types = []\nfor dfindex in range(0,len(df_train)):\n    # start with all 0s list\n    this_vect = [ 0.0 for pop in pop_annots]\n    # Get the MetaAnnots and MetaValues for this df item\n    meta_annots, meta_values = get_annots_values(df_train.loc[dfindex])\n    for ianno, this_anno in enumerate(meta_annots):\n        try:\n            popind = pop_index_of_annot[this_anno]\n            this_vect[popind] += 0.01*meta_values[ianno]\n        except KeyError:\n            pass\n    ##print(this_vect)\n    train_vectors.append(this_vect)\n    train_speeds.append(df_train.loc[dfindex,'AdoptionSpeed'])\n    train_types.append(df_train.loc[dfindex,'Type'])\n    \n\n##train_vectors","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8b3282e34a8cc21576dd2faedaaad8deb6aaa7fb","trusted":true},"cell_type":"code","source":"# Reduce the dimensions using SVD and put in dataframe\n\n# Define number of output dimensions\nn_svd_comps = 10\nsvd = TruncatedSVD(n_components=n_svd_comps,algorithm='randomized',n_iter=20)\n\n# Use the training vectors to determine the transformation\nsvd.fit(train_vectors)\n\n# and get the reduced-dimension output vectors for the training set\nsvd_train = svd.transform(train_vectors)\n# put the components in individual columns of df_train\n# multiply by 100 and quantize to 10.\nfor icomp in range(n_svd_comps):\n    df_train['MetaSvd'+str(icomp)] = 10*((10.0*(svd_train[:,icomp]+0.05)).astype('int'))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5ec464dbf00e2c91dc022c1aec7b808e0fdbaec6","trusted":true},"cell_type":"code","source":"# Make plots using pairs of components\nif SHOW_EDA:\n    \n    # Separate the components for plotting\n    svd0 = svd_train[:,0]\n    svd1 = svd_train[:,1]\n    svd2 = svd_train[:,2]\n    svd3 = svd_train[:,3]\n    svd4 = svd_train[:,4]\n    svd5 = svd_train[:,5]\n    svd6 = svd_train[:,6]\n    svd7 = svd_train[:,7]\n    svd8 = svd_train[:,8]\n    svd9 = svd_train[:,9]\n\n    clr_of_speed = {0:'black', 1:'blue', 2:'purple', 3:'orange', 4:'red'}\n    clr_of_type = {1:'blue', 2:'red', 3:'purple'}\n\n    # Use Speed as color\n    plot_colors = [clr_of_speed[this_speed] for this_speed in train_speeds]\n    # Use Type as color\n    ##plot_colors = [clr_of_type[this_type] for this_type in train_types]\n\n    # set a transparency\n    plot_alpha = 0.2\n\n    plt.scatter(svd0,svd1,marker='.', c=plot_colors, alpha=plot_alpha)\n    plt.show()\n    plt.scatter(svd2,svd1,marker='.', c=plot_colors, alpha=plot_alpha)\n    plt.show()\n    plt.scatter(svd2,svd3,marker='.', c=plot_colors, alpha=plot_alpha)\n    plt.show()\n    plt.scatter(svd4,svd3,marker='.', c=plot_colors, alpha=plot_alpha)\n    plt.show()\n    plt.scatter(svd4,svd5,marker='.', c=plot_colors, alpha=plot_alpha)\n    plt.show()\n    plt.scatter(svd6,svd5,marker='.', c=plot_colors, alpha=plot_alpha)\n    plt.show()\n    plt.scatter(svd6,svd7,marker='.', c=plot_colors, alpha=plot_alpha)\n    plt.show()\n    plt.scatter(svd8,svd7,marker='.', c=plot_colors, alpha=plot_alpha)\n    plt.show()\n    plt.scatter(svd8,svd9,marker='.', c=plot_colors, alpha=plot_alpha)\n    plt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"189226653ea98cff540646afa835c14f12187838","trusted":true},"cell_type":"code","source":"# Look at the annotations that contribute most (pos or neg) in each SVD 'unit vector'\nif SHOW_EDA:\n    for ivec in range(n_svd_comps):\n        unit_vect = np.zeros(n_svd_comps)\n        unit_vect[ivec] = 1.0\n        unit_vect = np.array(unit_vect).reshape(-1, 1).T\n        ##print(unit_vect)\n        feature_vect = (100.0*svd.inverse_transform(unit_vect)).astype('int').reshape(-1)\n        print(feature_vect)\n        for ifeat, feat_val in enumerate(feature_vect):\n            if abs(feat_val) >=15:\n                print(pop_annots[ifeat], feat_val)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"11125ea4d23d59fcbfd45941fc40da1a214d8a3a","trusted":true},"cell_type":"code","source":"# Do the TEST  TEST  TEST  TEST  TEST  TEST set too\n\n# For each training sample create/populate a vector of the popular annotations\n# Get the AdoptionSpeed and Type also for color-coding below.\n# Use MetaValues to assign real values (rather than just 0,1)\ntest_vectors = []\ntest_speeds = []\ntest_types = []\nfor dfindex in range(0,len(df_test)):\n    # start with all 0s list\n    this_vect = [ 0.0 for pop in pop_annots]\n    # Get the MetaAnnots and MetaValues for this df item\n    meta_annots, meta_values = get_annots_values(df_test.loc[dfindex])\n    for ianno, this_anno in enumerate(meta_annots):\n        try:\n            popind = pop_index_of_annot[this_anno]\n            this_vect[popind] += 0.01*meta_values[ianno]\n        except KeyError:\n            pass\n    ##print(this_vect)\n    test_vectors.append(this_vect)\n    test_speeds.append(df_test.loc[dfindex,'AdoptionSpeed'])\n    test_types.append(df_test.loc[dfindex,'Type'])\n    \n# Get the reduced-dimension output vectors for the Test set\nsvd_test = svd.transform(test_vectors)\n# put the components in individual columns of df_test\n# multiply by 100 and quantize to 10.\nfor icomp in range(n_svd_comps):\n    df_test['MetaSvd'+str(icomp)] = 10*((10.0*(svd_test[:,icomp]+0.05)).astype('int'))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"aebae0ca11901b9496159140432a2aa12426f7c2"},"cell_type":"markdown","source":"## <a id=\"SVDDescription\">Use SVD on Description Words</a>\nBack to <a href=\"#Index\">Index</a>"},{"metadata":{"_uuid":"0631855ce35dea2feedfe380c4133502400cc953","trusted":true},"cell_type":"code","source":"# Get a list of all the common description words\nall_descripts = ''\n# Get and combine the descriptions\n# Use only the first part of the descriptions\nmax_words = 50\nfor descrip in df_train['Description']:\n    words = descrip.split(\" \")\n    if len(words) > max_words:\n        first_ones = ''\n        for wrd in words[0:max_words]:\n            first_ones += wrd+\" \"\n        all_descripts += first_ones\n    else:\n        all_descripts += descrip + ' '\n\n# Convert to all lower case\nall_descripts = all_descripts.lower()\n# Some other replacements\nall_descripts = all_descripts.replace(\".\",\"\")\nall_descripts = all_descripts.replace(\",\",\"\")\nall_descripts = all_descripts.replace(\"'\",\"_\")\nall_descripts = all_descripts.replace(\"no \",\"no_\")\nall_descripts = all_descripts.replace(\"not \",\"not_\")\nall_descripts = all_descripts.replace(\"not_a \",\"not_a_\")\nall_descripts = all_descripts.replace(\"can_t \",\"can_t_\")\nall_descripts = all_descripts.replace(\"don_t \",\"don_t_\")\nall_descripts = all_descripts.replace(\"doesn_t \",\"doesn_t_\")\n\n# String with all the words separated by spaces:\nprint(\"All Descriptions, total characters:\",len(all_descripts))\n\n# Dictionary of annotations and number of times they appeared in the corpus,\n# it's in undefined order.\ndescripts_dict = Counter(all_descripts.split(\" \"))\nprint(\"Number of distinct description words:\",len(descripts_dict))\n##descripts_dict","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5d9df1ea2890d80ccb952477fc42fd8388375a89","trusted":true},"cell_type":"code","source":"# Get the most popular/useful words\n\n# Only consider ones with a min number of characters\nmin_chars = 5\n# and which appear in the corpus at least this many times:\nntimes_popular = 150   # 150 = 1% of training samples\n# but not more than this many:\nntimes_ignore = 1000000  # no upper limit to times appearing\n\n# List of tuples, (number, annot), sorted by reverse number\nsorted_num_descrip = sorted([(value, key) for (key, value) in\n                           descripts_dict.items() if (\n                           (value > ntimes_popular) and\n                           (value < ntimes_ignore) and                                                      \n                            (len(key) >= min_chars))],reverse=True)\n\nprint(\"Words of >={} characters appearing >{} times: {}\".format(\n            min_chars, ntimes_popular, len(sorted_num_descrip)))\n\npop_descripts = [wrd for num, wrd in sorted_num_descrip if num > ntimes_popular]\n\n# Show the list of popular ones\nif SHOW_EDA:\n    print(len(pop_descripts))\n    pop_descripts","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d8bba527c3075257e5ed5fda720f7c47c75c9a32","trusted":true},"cell_type":"code","source":"# For each Description create/populate a vector of the popular words.\n# Get the AdoptionSpeed and Type also for color-coding below.\n# Vectors have 0 or 1 depending if the word is present (regardless of number of times)\ntrain_vectors = []\ntrain_speeds = []\ntrain_types = []\nfor dfindex in range(0,len(df_train)):\n    # start with all 0s list\n    this_vect = [ 0.0 for wrd in pop_descripts]\n    # get this description\n    this_descrip = df_train.loc[dfindex, 'Description']\n    # use only first words\n    words = this_descrip.split(\" \")\n    if len(words) > max_words:\n        first_ones = ''\n        for wrd in words[0:max_words]:\n            first_ones += wrd+\" \"\n        this_descrip = first_ones\n    # only changes needed is she's to she_s:\n    this_descrip = this_descrip.replace(\"she's\",\"she_s\")\n    # brute-force go through all the words and see if they are there\n    for iwrd, wrd in enumerate(pop_descripts):\n        if wrd in this_descrip:\n            this_vect[iwrd] = 1.0\n    ##print(this_vect)\n    train_vectors.append(this_vect)\n    train_speeds.append(df_train.loc[dfindex,'AdoptionSpeed'])\n    train_types.append(df_train.loc[dfindex,'Type'])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2d2dbe3e4721e2ce7698d1efa5e2031805d75d71","trusted":true},"cell_type":"code","source":"# Reduce the dimensions using SVD and put in dataframe\n\n# Define number of output dimensions\nn_svd_comps = 10\nsvd = TruncatedSVD(n_components=n_svd_comps,algorithm='randomized',n_iter=20)\n\n# Use the training vectors to determine the transformation\nsvd.fit(train_vectors)\n\n# and get the reduced-dimension output vectors for the training set\nsvd_train = svd.transform(train_vectors)\n# put the components in individual columns of df_train\n# multiply by 100 and quantize to 10.\nfor icomp in range(n_svd_comps):\n    df_train['DescripSvd'+str(icomp)] = 10*((10.0*(svd_train[:,icomp]+0.05)).astype('int'))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9adbe24759cb114f2e821c6091a18ad9510ad1b7","trusted":true},"cell_type":"code","source":"# - - - TEST TEST TEST TEST TEST TEST TEST TEST - - -\n# For each Description create/populate a vector of the popular words.\n# Get the AdoptionSpeed and Type also for color-coding below.\n# Vectors have 0 or 1 depending if the word is present (regardless of number of times)\ntest_vectors = []\ntest_speeds = []\ntest_types = []\nfor dfindex in range(0,len(df_test)):\n    # start with all 0s list\n    this_vect = [ 0.0 for wrd in pop_descripts]\n    # get this description\n    this_descrip = df_test.loc[dfindex, 'Description']\n    # use only first words\n    words = this_descrip.split(\" \")\n    if len(words) > max_words:\n        first_ones = ''\n        for wrd in words[0:max_words]:\n            first_ones += wrd+\" \"\n        this_descrip = first_ones\n    # only changes needed is she's to she_s:\n    this_descrip = this_descrip.replace(\"she's\",\"she_s\")\n    # brute-force go through all the words and see if they are there\n    for iwrd, wrd in enumerate(pop_descripts):\n        if wrd in this_descrip:\n            this_vect[iwrd] = 1.0\n    ##print(this_vect)\n    test_vectors.append(this_vect)\n    test_speeds.append(df_test.loc[dfindex,'AdoptionSpeed'])\n    test_types.append(df_test.loc[dfindex,'Type'])\n    \n# and get the reduced-dimension output vectors for the TEST set\nsvd_test = svd.transform(test_vectors)\n# put the components in individual columns of df_test\n# multiply by 100 and quantize to 10.\nfor icomp in range(n_svd_comps):\n    df_test['DescripSvd'+str(icomp)] = 10*((10.0*(svd_test[:,icomp]+0.05)).astype('int'))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e8fd2bbb29390a3f1ac5f42018c8065c119318ad","trusted":true},"cell_type":"code","source":"# Make plots using pairs of components\nif SHOW_EDA:\n    \n    # Separate the components for plotting\n    svd0 = svd_train[:,0]\n    svd1 = svd_train[:,1]\n    svd2 = svd_train[:,2]\n    svd3 = svd_train[:,3]\n    svd4 = svd_train[:,4]\n    svd5 = svd_train[:,5]\n    svd6 = svd_train[:,6]\n    svd7 = svd_train[:,7]\n    svd8 = svd_train[:,8]\n    svd9 = svd_train[:,9]\n\n    clr_of_speed = {0:'black', 1:'blue', 2:'purple', 3:'orange', 4:'red'}\n    clr_of_type = {1:'blue', 2:'red', 3:'purple'}\n\n    # Use Speed as color\n    plot_colors = [clr_of_speed[this_speed] for this_speed in train_speeds]\n    # Use Type as color\n    ##plot_colors = [clr_of_type[this_type] for this_type in train_types]\n\n    # set a transparency\n    plot_alpha = 0.2\n\n    plt.scatter(svd0,svd1,marker='.', c=plot_colors, alpha=plot_alpha)\n    plt.show()\n    plt.scatter(svd2,svd1,marker='.', c=plot_colors, alpha=plot_alpha)\n    plt.show()\n    plt.scatter(svd2,svd3,marker='.', c=plot_colors, alpha=plot_alpha)\n    plt.show()\n    plt.scatter(svd4,svd3,marker='.', c=plot_colors, alpha=plot_alpha)\n    plt.show()\n    plt.scatter(svd4,svd5,marker='.', c=plot_colors, alpha=plot_alpha)\n    plt.show()\n    plt.scatter(svd6,svd5,marker='.', c=plot_colors, alpha=plot_alpha)\n    plt.show()\n    plt.scatter(svd6,svd7,marker='.', c=plot_colors, alpha=plot_alpha)\n    plt.show()\n    plt.scatter(svd8,svd7,marker='.', c=plot_colors, alpha=plot_alpha)\n    plt.show()\n    plt.scatter(svd8,svd9,marker='.', c=plot_colors, alpha=plot_alpha)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a97f5ef96f424fcfba65eac20f1347d3118751be","trusted":true},"cell_type":"code","source":"# Look at the Descriptions that contribute most (pos or neg) in each SVD 'unit vector'\nif SHOW_EDA:\n    for ivec in range(n_svd_comps):\n        unit_vect = np.zeros(n_svd_comps)\n        unit_vect[ivec] = 1.0\n        unit_vect = np.array(unit_vect).reshape(-1, 1).T\n        ##print(unit_vect)\n        feature_vect = (100.0*svd.inverse_transform(unit_vect)).astype('int').reshape(-1)\n        print(feature_vect)\n        for ifeat, feat_val in enumerate(feature_vect):\n            if abs(feat_val) >=15:\n                print(pop_descripts[ifeat], feat_val)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3a12b638c364b832cf94baf2cbb8882a92d6f0d9"},"cell_type":"markdown","source":"## <a id=\"FeatureSummary\">Summary of the Features</a>\nBack to <a href=\"#Index\">Index</a><br>\n\nLook at the features in various ways."},{"metadata":{"_uuid":"dc8aefd6e58d87af924fc058dcd312f9b5818980","trusted":true},"cell_type":"code","source":"if SHOW_EDA:\n    # Look at a dataframe\n    # Select a dataframe\n    df_temp = df_train\n\n    print(df_temp.columns)\n\n    if len(df_temp) > 20:\n        # Show beginning and ending\n        print(df_temp.loc[list(range(5))+list(range(len(df_temp)-5,len(df_temp)))])\n    else:\n        print(df_temp)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"93949cdd2f9f997d2e8dff3cc7ad334365b198a0","trusted":true},"cell_type":"code","source":"if SHOW_EDA:\n    print(df_train.columns)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f68c04b4baad3d16a12ade5246527eec3a5ce9eb","trusted":true},"cell_type":"code","source":"# Look at a feature\nfeat_name = 'RescuerActivity'\nval_counts = df_train[feat_name].value_counts()\nfeat_values = np.sort(val_counts.index)\nprint(feat_name+\" has {} distinct values:\\n\".format(len(val_counts)))\n\n# Look at the histogram of this features training values\ndf_train[feat_name].hist(bins=100, figsize=(10,4), grid=False, color='orange')\nplt.xlabel(feat_name)\nplt.ylabel(\"Number of samples\")\nplt.show()\n\n# Mean and Median values for each feature value\ny_float_aves = []\npct_classes = []\npct = 0.50   # 0.50 is the median\nfor this_val in feat_values:\n    float_ave = 0.0\n    # number that have this feature value\n    num_this_val = len(df_train[(df_train[feat_name] == this_val)])\n    num_pct = pct*num_this_val\n    num_so_far = 0\n    for iclass in range(5):\n        # number in iclass for this feature value\n        num_this_class = len(df_train[(df_train[feat_name] == this_val) &\n                                      (df_train['AdoptionSpeed'] == iclass)])\n        # accumulate the average\n        float_ave += iclass*num_this_class\n        ##print(iclass, num_this_class)\n        num_so_far += num_this_class\n        if ((num_so_far >= num_pct) and ((num_so_far-num_this_class) < num_pct)):\n            # The pct-th one is in this class\n            val_pct = iclass\n            # what fraction along in the class\n            class_fraction = (num_pct - (num_so_far-num_this_class))/max([num_this_class, 1])\n            val_pct += class_fraction - 0.5\n    ##print(float_ave/num_this_val)\n    # save the average for this feature value\n    y_float_aves.append(float_ave/num_this_val)\n    # If there are a few number of counts, just use the mean\n    if num_this_val > 20:\n        pct_classes.append(val_pct)\n    else:\n        pct_classes.append(float_ave/num_this_val)\n  \n\nplt.figure(figsize=(10,4))\nplt.plot(feat_values, y_float_aves, marker=\"o\", linestyle='')\nplt.plot(feat_values, pct_classes, marker=\"o\", linestyle='', markersize=3)\nplt.xlabel(feat_name)\nplt.ylabel(\"Average Class (blue), Median (orange)\")\nplt.ylim(-0.1,4.1)\nplt.show()\n\nplt.figure(figsize=(10,4))\n# adding 0.01 to allow 0/0 to give 1:\nplt.plot(feat_values, (0.01+np.array(pct_classes)) /\n         (0.01+np.array(y_float_aves)), marker=\"o\", linestyle='',\n         markersize=3, color='green')\nplt.xlabel(feat_name)\nplt.ylabel(\"Median / Ave Class\")\n##plt.ylim(0.0,)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a37df4217415db1feda65452a4f162cf6ade5ec1","trusted":true},"cell_type":"code","source":"# Statistical summary of the numerical values\n##df_train['PhotoAmt'].describe()\nif SHOW_EDA:\n    print(df_train.describe())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3854b6f33f3e2d95e5b3b18ce0264b8688eff4cd","trusted":true},"cell_type":"code","source":"# and for Test\nif SHOW_EDA:\n    print(df_test.describe())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"25cd84f93f980b08340cf48536736ff1fb54dcf0","trusted":true},"cell_type":"code","source":"if SHOW_EDA:\n    # Look at histograms of the values, training and test\n    df_train.hist(bins=50, sharey=True, figsize=(15,10), grid=False)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"065ab17148c88c3a5141ca967eaed5d022068ff6","trusted":true},"cell_type":"code","source":"if SHOW_EDA:\n    df_test.hist(bins=50, sharey=True, figsize=(15,10), grid=False)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fa2e4328c4ee74524386ff9326d547939bac016e"},"cell_type":"markdown","source":"## <a id=\"CompareTrainTest\">Compare Train and Test Averages</a>\nBack to <a href=\"#Index\">Index</a>"},{"metadata":{"_uuid":"a014ae499df9a0bf8d0a58583b0af26b8ed12067","trusted":true},"cell_type":"code","source":"# Compare Train and Test averages\n# Using a z-score with standard error based on the number of samples\ndescr_train = df_train.describe()\ndescr_test = df_test.describe()\n# Number of samples in the test set\nn_test = descr_test.loc[\"count\",\"Type\"]\nn_train = descr_train.loc[\"count\",\"Type\"]\nif SHOW_EDA:\n    print(\"     --column--    z-score      Test Mean     Train Mean\")\nfor col in descr_test.columns:\n    ave_test = descr_test.loc[\"mean\",col]\n    ave_train = descr_train.loc[\"mean\",col]\n    std_train = descr_train.loc[\"std\",col]\n    if SHOW_EDA:\n        print(col.rjust(15), \n            '{:.4f}'.format((ave_test - ave_train)/\n                               (std_train*np.sqrt(1.0/n_test+1.0/n_train))).rjust(10),\n            '{:.4f}'.format(ave_test).rjust(14),\n            '{:.4f}'.format(ave_train).rjust(14))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e20d51d7cebc76286fc4171ff1c1f7f403866249","trusted":true},"cell_type":"code","source":"# Look at the correlation between the numerical values, includes AdoptionSpeed\n# Very little strong correlation seen\ncorr_df = df_train.corr()\n\n# Choose a way to display the values\n# whole matrix\n##print(corr_df)\n##corr_df\n# Just the correlations with AdoptionSpeed:\nif SHOW_EDA:\n    print(corr_df.AdoptionSpeed)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"449dd87f2e5ffbe45f9ca423da98addd6028f18b","trusted":true},"cell_type":"code","source":"# Check for NaN's anywhere in the data sets\n# They only appeared in Name and Description so those were fillna'ed above.\nn_train = len(df_train)\nnona_train = len(df_train.dropna(axis=0))\nprint(n_train, nona_train, \"  Fraction of NaNs:\", (nona_train-n_train)/n_train)\n\nn_test = len(df_test)\nnona_test = len(df_test.dropna(axis=0))\nprint(n_test, nona_test, \"  Fraction of NaNs:\", (nona_test-n_test)/n_test)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a7a91bf85ab2fe0c516a73ee87b6e462366c3c28"},"cell_type":"markdown","source":"## Comparing Target Classes and Adoption Times\nWe can use regression as well as classification since the classes have a monotonic numerical ordering. In fact the classes are roughly log-spaced with `Class = 0.75*np.log(Days)` as shown by the plot made here.\nNote that to better agree with the line, the first class has been plotted at a class numerical value of -1.0 instead of 0."},{"metadata":{"_uuid":"859b47970f6cbe9b031cc6890a70a494a89ae593","trusted":true},"cell_type":"code","source":"if SHOW_EDA:\n    fig, ax = plt.subplots()\n    ax.set_xlim((0.03, 500.0))\n    ax.set_ylim((-2.0, 5.0))\n\n    # The class adoption-time regions\n    class_nums = [-1.0,1,2,3,4]\n    t_mins = np.array([0.01, 1.1, 8.0, 31.0, 91.0])\n    t_maxs = np.array([1.0, 7.0, 30.0, 90.0, 1000.0])\n\n    for ireg in range(len(t_maxs)):\n        plt.semilogx([t_mins[ireg], t_maxs[ireg]],\n                 [class_nums[ireg],class_nums[ireg]], marker='o')\n\n    # a simple log approximation\n    # line extends from 2 hours to one year\n    t = np.exp(np.arange(np.log(2.0/24.0), np.log(365.0), 0.1))\n    cline = 0.75*np.log(t)\n\n    plt.semilogx(t, cline)\n    plt.xlabel('log(Days)')\n    plt.ylabel('Class')\n    plt.title('Class vs Days')\n    plt.grid(True)\n\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"10c86c516acf162c4ac20fd1a50a2541de4b9c04"},"cell_type":"markdown","source":"## <a id=\"TheFeatures\">Select The Features to Use for ML</a>\nBack to <a href=\"#Index\">Index</a>"},{"metadata":{"_uuid":"546febbaa8220d57d9f782fd750c32c4d642d70c","scrolled":true,"trusted":true},"cell_type":"code","source":"# All Features... the training numeric columns:\nall_features = descr_train.columns\n\n# Remove the 'answer' column:\nall_features = all_features.drop('AdoptionSpeed')\n\n# List all of these potential features\nprint(len(all_features),\"All features:\")\nprint(all_features)\n# print out append commands for each one\n##for feat in all_features:\n##    print(\"features.append('\"+feat+\"')\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7c36fe9b56ce6f28b9a319193c7959f01020e2f4","trusted":true},"cell_type":"code","source":"# Select the features to use.\n# All are listed here, roughly in importance order and grouped.\n# Undesired ones are commented out.\nfeatures = []\n\nfeatures.append('Age')     # TOP 2\nfeatures.append('AgeSq')\n\nfeatures.append('RescuerActivity')  # TOP 3\n\n##features.append('SterilBlur')  # v56 remove this one, keep Sterilized...\nfeatures.append('Sterilized')  # TOP 4\n\nfeatures.append('PhotoAmt')    # TOP 10\n\nfeatures.append('BreedSpeed')    # TOP 2\nfeatures.append('Breed1')        # TOP 6\nfeatures.append('BreedSpeed2')\nfeatures.append('Breed2')\nfeatures.append('MixedBreed')  # ?<0.007\nfeatures.append('NamedBreed')  # <0.007\n##features.append('Domestic')  # * <0.005\n\n# The SVD on Metadata annotation words\nfeatures.append('MetaSvd2')   # TOP 6\nfeatures.append('MetaSvd3')   # TOP 10\nfeatures.append('MetaSvd0')   # TOP 10\nfeatures.append('MetaSvd1')\nfeatures.append('MetaSvd4')\nfeatures.append('MetaSvd5')\nfeatures.append('MetaSvd6')\nfeatures.append('MetaSvd8')\nfeatures.append('MetaSvd7')\nfeatures.append('MetaSvd9')\n\n# The Description Sentiment values\nfeatures.append('SentMag')\nfeatures.append('SentMagSq')\nfeatures.append('SentScore')\nfeatures.append('SentScoreSq')\n##features.append('SentLang')  # * <0.005\n\n# The SVD on Description words\nfeatures.append('DescripSvd1')\nfeatures.append('DescripSvd7')\nfeatures.append('DescripSvd9')\nfeatures.append('DescripSvd0')\nfeatures.append('DescripSvd6')\nfeatures.append('DescripSvd3')\nfeatures.append('DescripSvd8')\nfeatures.append('DescripSvd2')\nfeatures.append('DescripSvd4')\nfeatures.append('DescripSvd5')\n\nfeatures.append('TypeBlur')\n##features.append('Type')  # * <0.005\n\nfeatures.append('ColorBright')\nfeatures.append('ColorBrightSq')\nfeatures.append('ColorContrast')\nfeatures.append('Color1')\nfeatures.append('ColorContrastSq')\nfeatures.append('Color2')\nfeatures.append('Color3')  # <0.007\n##features.append('ColorMany')  # * <0.005\n##features.append('ColorSolid')  # * <0.005\n\nfeatures.append('LenDescr')\nfeatures.append('LenDescrSq')\n\nfeatures.append('Quantity')    # TOP 6\n\nfeatures.append('StatePop')\nfeatures.append('StateGDP')\nfeatures.append('StateGDPpp')\nfeatures.append('State')\n\nfeatures.append('Vaccinated')\n\nfeatures.append('Gender')\n\nfeatures.append('Dewormed')\n\nfeatures.append('MaturitySize')\n\nfeatures.append('FurLength')  # ?<0.007\n\nfeatures.append('Fee')  # <0.007\nfeatures.append('FeeSq')  # <0.007\n\n##features.append('NameGiven')  # * <0.005\n##features.append('NameLong')  # * <0.005\n\n##features.append('VideoAmt')  # * <0.005\n\n##features.append('Health')  # * <0.005\n\n# - - -\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9ab162451bf540e33ddeb5f3bd08e4ade472a81a"},"cell_type":"markdown","source":" = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = ="},{"metadata":{"_uuid":"78fd3715a9518f3deae9fbe9a0b6bf5ad9fa8713"},"cell_type":"markdown","source":"## <a id=\"MachineLearning\">Machine Learning</a>\nBack to <a href=\"#Index\">Index</a>"},{"metadata":{"_uuid":"3506f2e7187288dc0e1b4421527205e073dbc3cd"},"cell_type":"markdown","source":"## Routines"},{"metadata":{"_uuid":"11283926059f51a26b4b41aaf17c20259897eaac","trusted":true},"cell_type":"code","source":"# Routines we might use\nfrom sklearn.preprocessing import StandardScaler\n\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import ShuffleSplit\nfrom sklearn.model_selection import RepeatedKFold\nfrom sklearn.model_selection import GroupKFold\n#\nfrom sklearn.model_selection import GridSearchCV\n# For train-validation split\nfrom sklearn.model_selection import train_test_split\n\n# Use this simple tree regressor\nfrom sklearn.tree import DecisionTreeRegressor\n# Use this more complex classifier\nfrom sklearn.ensemble import RandomForestRegressor\n# or...\nfrom sklearn.ensemble import GradientBoostingRegressor\n\nfrom sklearn.svm import SVR\n\nfrom sklearn.neural_network import MLPRegressor\n\n# Metric\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import cohen_kappa_score\nfrom sklearn.metrics import confusion_matrix\n#\nfrom sklearn.metrics import make_scorer\n\n# For OptimizedRounder(_v2)\nimport scipy as sp\nfrom functools import partial","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7235c173f6ae0fcc12a4deb590cb908d6559cbd9"},"cell_type":"markdown","source":"### Routine to get X,y data:  All, or Dogs, or Cats"},{"metadata":{"_uuid":"5e80cb342aa5f96d015fe4c83145250602feff4f","trusted":true},"cell_type":"code","source":"def get_Xy_values(df_in, features, type_sel=0):\n    # Extract and return the features, X dataframe, and target values, y (np.array).\n    # type_sel will select All(0), Dogs(1), or Cats(2)\n\n    if type_sel == 0:\n        # All of the pets\n        X = df_in[features].copy()\n        y = df_in.AdoptionSpeed.values\n        down_sel=(df_in['Type'] >= type_sel)\n    else:\n        # Down-select to just Cats or Dogs\n        down_sel = (df_in['Type'] == type_sel)\n        X = df_in.loc[down_sel,features].copy()\n        X = X.reset_index().drop('index',axis=1)\n        y = df_in[down_sel].AdoptionSpeed\n        y = y.values\n\n    print(\"\\nThe y target has {} values.\\n\".format(len(y)))\n    return X, y, down_sel\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b0fd3369ab16b79da96412b1fe1d58bf82724962"},"cell_type":"markdown","source":"### Routines for converting Regression Floats --> Class Integers\nBecause the y values (classes) have numerical meaning (larger is longer time), it is possible to use Regression models as well as multi-class classification models.The float output of the regression has to be turned into integer classes and the routines here do that in various ways."},{"metadata":{"_uuid":"99b2a72f3dfbd4c6e391f78eb6f10e011eb0d1ae","trusted":true},"cell_type":"code","source":"# Find boundary locations (equiv. to indices in sorted array)\n# for y_pred to match known y percentiles\ndef get_class_bounds(y, y_pred, N=5, class0_fraction=-1):\n    # Returns N-1 boundaries in y_pred values that separate y_pred\n    # into N classes (0, 1, 2, ..., N-1) with same percentiles as y has.\n    # Can adjust the fraction in Class 0 by the given factor (>=0), if desired. \n    ysort = np.sort(y)\n    predsort = np.sort(y_pred)\n    bounds = []\n    for ibound in range(N-1):\n        iy = len(ysort[ysort <= ibound])\n        # reduce the number of class 0 predictions?\n        if (ibound == 0) and (class0_fraction >= 0.0) :\n            iy = int(class0_fraction * iy)\n        bounds.append(predsort[iy])\n    return bounds\n\n# Given class boundaries in y_pred units, output integer class values\n# y_pred should be an np.array.\ndef assign_class(y_pred, boundaries):\n    y_classes = np.zeros(len(y_pred))\n    for iclass, bound in enumerate(boundaries):\n        y_classes[y_pred >= bound] = iclass + 1\n    return y_classes.astype(int)\n\ndef optimize_class0_fraction(train_y, train_meta, verbose=False, fraclimit=1.0):\n    # This cell calculates and plots the kappa (and MSE) vs the class0 fraction adjustment.\n    # Note that MSE prefers (lower MSE) a class0 fraction near/at 0,\n    # whereas kappa prefers (higher kappa) a fraction near 1.\n    # Then the class0 fraction that gives best training kappa is selected.\n    kappas = []\n    ##mses = []\n    # fractions to try... (could go larger than 1 if desired.)\n    cl0fracs = np.array(np.arange(0.01,fraclimit,0.01))\n    for cl0frac in cl0fracs:\n        boundaries = get_class_bounds(train_y, train_meta, class0_fraction=cl0frac)\n        train_meta_ints = assign_class(train_meta, boundaries)\n        kappa = cohen_kappa_score(train_y, train_meta_ints, weights='quadratic')\n        kappas.append(kappa)\n        ##mse = mean_squared_error(train_y, train_meta_ints)\n        ##mses.append(mse)\n    \n    # Use the class0 fraction that gives the highest training kappa\n    ifmax = np.array(kappas).argmax()\n    cl0frac = cl0fracs[ifmax]\n\n    if verbose:\n        # Show the kappa and MSE vs class0 fraction\n        plt.plot(cl0fracs, kappas)\n        plt.plot([cl0frac],[kappas[ifmax]],marker=\"o\",color=\"green\")\n        plt.title(\"Training: kappa[{:.4f}] vs class0_fraction[{}]\".format(kappas[ifmax],cl0frac))\n        plt.xlabel(\"class0_fraction\")\n        plt.ylabel(\"kappa\")\n        plt.show()\n\n        ##plt.plot(cl0fracs, mses)\n        ##plt.title(\"Training: MSE vs class0_fraction\")\n        ##plt.xlabel(\"class0_fraction\")\n        ##plt.ylabel(\"MSE\")\n        ##plt.show()\n\n    return cl0frac\n\n# This was the OptimizedRounder from https://www.kaggle.com/abhishek/maybe-something-interesting-here\n#   ... instead ...\n# Version from https://www.kaggle.com/naveenasaithambi/optimizedrounder-improved    \n# improved\nclass OptimizedRounder_v2(object):\n    def __init__(self):\n        self.coef_ = 0\n    \n    def _kappa_loss(self, coef, X, y):\n        preds = pd.cut(X, [-np.inf] + list(np.sort(coef)) + [np.inf], labels = [0, 1, 2, 3, 4])\n        return -cohen_kappa_score(y, preds, weights = 'quadratic')\n    \n    # Modify to have extra keywords:\n    def fit(self, X, y, initial_coef = [0.5, 1.5, 2.5, 3.5], tol=0.001):\n        loss_partial = partial(self._kappa_loss, X = X, y = y)\n        self.coef_ = sp.optimize.minimize(loss_partial, initial_coef, method = 'nelder-mead', tol=tol)\n    \n    def predict(self, X, coef):\n        preds = pd.cut(X, [-np.inf] + list(np.sort(coef)) + [np.inf], labels = [0, 1, 2, 3, 4])\n        return preds\n    \n    def coefficients(self):\n        return self.coef_['x']","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"10c86c516acf162c4ac20fd1a50a2541de4b9c04"},"cell_type":"markdown","source":"## <a id=\"SetupData\">Setup the X,y Data</a>\nBack to <a href=\"#Index\">Index</a>\n\nCan repeat from here to end to run various ML configurations, etc.\n\nUp to three X,y data sets are created here: **X,y** are all of the available training data, **Xloc,y_loc** are the Local Test data (if iit is selected), and **Xkag,y_kag** are the Kaggle competion (test) data."},{"metadata":{"_uuid":"6ff99f645ab884e0f25653e0c90446b2b6a2e481"},"cell_type":"markdown","source":"### Separate off a Local Test set?"},{"metadata":{"_uuid":"2fd49d3ff77932d57b73c1d395cf44a6f527156c","trusted":true},"cell_type":"code","source":"LOCAL_TEST=False\n\nn_local_test = 2000\n# specify the local-test size directly\n##n_local_test = 3950  # close to 3948 of test\n# or calc it from setting the remaining training size:\n##n_local_test = len(df_train) - 7000\n    \nif LOCAL_TEST:\n    # Hold out a local test set\n    # shuffle and choose at random, RescuerID groups are split.\n    rand_order = np.random.permutation(len(df_train))\n    df_localtest = df_train.loc[rand_order[0 : n_local_test]].copy().reset_index()\n    df_traintrain = df_train.loc[rand_order[n_local_test : ]].copy().reset_index()\nelse:\n    df_traintrain = df_train.copy()\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"105d573380b7dd0067b312a460b186d2692c4527","trusted":true},"cell_type":"code","source":"# Compare the Test and LocalTest histograms for all features\n#\nif False and LOCAL_TEST:\n    for col in features:\n        df_test[col].plot.hist(bins=50, color='lightblue')\n        df_localtest[col].plot.hist(bins=50, color='red', alpha=0.4)\n        plt.title(col+\" (Test-blue, LocalTest-red)\")\n        plt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6cb566455af7c345fbaaf5869b73208babf00df0"},"cell_type":"markdown","source":"### All Data... or just Dogs, or just Cats ??"},{"metadata":{"_uuid":"375a56a12b482da9f440118526aec23e673c4cbb","trusted":true},"cell_type":"code","source":"# Can down-select by Type if desired\n# All(0), Dog(1) or Cat(2)\n\ntype_sel = 0\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6ff99f645ab884e0f25653e0c90446b2b6a2e481"},"cell_type":"markdown","source":"### Extract the X,y data sets"},{"metadata":{"_uuid":"705b5b5a6208e02d7983a98e463b6072b6dd0b44","trusted":true},"cell_type":"code","source":"# List the selected features\nprint(len(features),\"Selected features:\")\nprint(features)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2fd49d3ff77932d57b73c1d395cf44a6f527156c","scrolled":true,"trusted":true},"cell_type":"code","source":"# Get all the training X, y\n# These ys are classes 0 to 4.\n\n# Create yv s that are used as regression targets;\n# yv = y except that the y=0 class is assigned a value:\nY0_VALUE = -1.0\n\nprint(\"Training:\")\nX, y, down_sel = get_Xy_values(df_traintrain, features, type_sel=type_sel)\n# Create a float y value for regression: 0 --> -1, others the same\nyv = 1.0*y\nyv[y == 0] = Y0_VALUE\n# Get the rescuerIDs for groups\ngrp_train = df_traintrain.loc[down_sel,'RescuerID']\n\n# Get the local test set, if one was created\nif LOCAL_TEST:\n    print(\"Local Test:\")\n    Xloc, y_loc, down_sel = get_Xy_values(df_localtest, features, type_sel=type_sel)\n    yv_loc = 1.0*y_loc\n    yv_loc[y_loc == 0] = Y0_VALUE\n    # Get the rescuerIDs for groups\n    grp_loc = df_localtest.loc[down_sel,'RescuerID']\n    \n# Get the Kaggle test set (Note: y_kag is not valid)\nprint(\"Kaggle Test:\")\nXkag, y_kag, down_sel = get_Xy_values(df_test, features, type_sel=type_sel)\n# Get the rescuerIDs for groups\ngrp_kag = df_test.loc[down_sel,'RescuerID']\n\n\n# Offset, Scale all features so that Train features have mean 0.0 and standard deviation 1.0:\nfor col in X.columns:\n    col_mean = X[col].mean()\n    col_std = X[col].std()\n    ##print(col_mean, col_std)\n    # X\n    X[col] = (X[col] - col_mean)/col_std\n    # Xloc (if present)\n    if LOCAL_TEST:\n        Xloc[col] = (Xloc[col] - col_mean)/col_std\n    # Xkag\n    Xkag[col] = (Xkag[col] - col_mean)/col_std\n    \n##X.var()\n\nX.head(5)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"caa6fe21cbff83b48dc33e22e9bdbf889bf844bd","trusted":true},"cell_type":"code","source":"# Can look at the makeup of RescuerID groups, if desired\n##grp_dict = Counter(grp_train)   # grp_train, grp_loc, or grp_kag\n##print(len(grp_dict))\n##sorted([(value, key) for (key, value) in grp_dict.items()],reverse=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# This variable is used to allow a single notebook to be run in different ways.\n# For single model fitting, continuing below (without the Stacking Experiments...)\nSTACKING = False","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"24696f7f6f87a410092314ad183ff81e8ea23dae","collapsed":true},"cell_type":"markdown","source":"## <a id=\"StackingData\">Replace Data with Stacking Data?</a>\nBack to <a href=\"#Index\">Index</a>\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# - - -\n# Moved the Stacking Experiements... section from the end of the notebook to below here:\n# this will replace the X,y etc data with the stacking outputs from the 3 fits.\n# These stacking data are then fit to produce the stacking results/prediction.\n# - - -\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## <a id=\"Stacking\">Stacking Experiments...</a>\nBack to <a href=\"#Index\">Index</a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Moved Stacking Experiments... from the end to here for Kaggle stacking versions.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Assemble stacking data?\n# STACKING variable is used in above cells to signal that the data are stacking data.\n\n# After running the desired models on the usual data,\n# Change this from False to True and run from here to end...\n\nSTACKING = True\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b2012b53b80c1f9145e640691014acec210f7dd2","trusted":true},"cell_type":"code","source":"# If multiple Xys files are available then...\nif STACKING:\n    # Hard-code the directory that has stacking files\n    ##stk_version = 'v76C'\n    ##stk_dir = \"Out_\"+stk_version\n    # Or use current-version directory:\n    ##stk_version = version_str\n    ##stk_dir = out_dir\n    # Or the Dataset of fit results in parallel with the dat_dir\n    stk_version = version_str\n    if LOCATION_KAGGLE:\n        stk_dir = dat_dir+\"../petfinder-outputs-for-stacking\"\n    else:\n        stk_dir = out_dir\n    \n    Xtrain_gbr = pd.read_csv(stk_dir+\"/Xys_train_\"+\"gbr\"+\"_\"+stk_version+\".csv\")\n    Xtrain_svr = pd.read_csv(stk_dir+\"/Xys_train_\"+\"svr\"+\"_\"+stk_version+\".csv\")\n    Xtrain_mlp = pd.read_csv(stk_dir+\"/Xys_train_\"+\"mlp\"+\"_\"+stk_version+\".csv\")\n    \n    Xkag_gbr = pd.read_csv(stk_dir+\"/Xys_Kag_\"+\"gbr\"+\"_\"+stk_version+\".csv\")\n    Xkag_svr = pd.read_csv(stk_dir+\"/Xys_Kag_\"+\"svr\"+\"_\"+stk_version+\".csv\")\n    Xkag_mlp = pd.read_csv(stk_dir+\"/Xys_Kag_\"+\"mlp\"+\"_\"+stk_version+\".csv\")\n    \n    # Combine the training into one\n    # start with all gbr columns\n    Xstk = Xtrain_gbr.copy()\n    # include y's from others\n    Xstk['ypv_svr'] = Xtrain_svr['ypv_svr']\n    Xstk['yp_svr'] = Xtrain_svr['yp_svr']\n    Xstk['y_svr'] = Xtrain_svr['y_svr']\n    #\n    Xstk['ypv_mlp'] = Xtrain_mlp['ypv_mlp']\n    Xstk['yp_mlp'] = Xtrain_mlp['yp_mlp']\n    Xstk['y_mlp'] = Xtrain_mlp['y_mlp']\n\n    # Combine the Kaggle into one\n    Xstkkag = Xkag_gbr.copy()\n    Xstkkag['ypv_svr'] = Xkag_svr['ypv_svr']\n    Xstkkag['yp_svr'] = Xkag_svr['yp_svr']\n    Xstkkag['ypv_mlp'] = Xkag_mlp['ypv_mlp']\n    Xstkkag['yp_mlp'] = Xkag_mlp['yp_mlp']\n    \n    # Offset, Scale stacking features to have mean 0.0 and standard deviation 1.0.\n    # Only need to do the new ypv columns (since other features are done already):\n    for col in ['ypv_svr', 'ypv_gbr', 'ypv_mlp']:\n        col_mean = Xstk[col].mean()\n        col_std = Xstk[col].std()\n        Xstk[col] = (Xstk[col] - col_mean)/col_std\n        Xstkkag[col] = (Xstkkag[col] - col_mean)/col_std","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d555a33626c8d2f94a6c8b5842c91b3203ff3a04","trusted":true},"cell_type":"code","source":"# Show SVR vs GBR values, color coded too.\nif STACKING:\n    # Plots with data separated by the known y value\n    clrcol = 'Age'\n    xylimits = (-4.0,4.0)\n    #\n    for iy in range(5):\n        f, ((ax1, ax2)) = plt.subplots(1, 2, sharex=True, sharey=True)\n        Xstk[(Xstk['y_gbr'] == iy)].plot.scatter('ypv_gbr','ypv_svr',\n                s=8, c=clrcol, colormap='jet', title=\"Training SVR vs GBR, for y = \"+str(iy),\n                figsize=(12,4), alpha=0.2, xlim=xylimits, ylim=xylimits, ax=ax1)\n        Xstk[(Xstk['y_gbr'] == iy)].plot.scatter('ypv_gbr','ypv_mlp',\n                s=8, c=clrcol, colormap='jet', title=\"Training MLP vs GBR, for y = \"+str(iy),\n                figsize=(12,4), alpha=0.2, xlim=xylimits, ylim=xylimits, ax=ax2)\n        plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# A compact set of plots\nif STACKING:\n    \n    clrcol = 'Age'\n    xylimits = (-4.0,4.0)\n    allfigsize = (18,8)\n        \n    f, axarray = plt.subplots(2, 4, sharex=True, sharey=True)\n    f.suptitle('Comparing ypv-values: SVR, MLP (y-axes)  vs  GBR (x-axis)      (Color-coded by '+clrcol+')')\n    \n    ax1 = axarray[0,0]\n    ax1r = axarray[0,1]\n    Xstk[Xstk['TypeBlur'] < 0.0].plot.scatter('ypv_gbr','ypv_svr', s=10, c=clrcol, colormap='jet',\n                figsize=allfigsize, alpha=0.10, xlim=xylimits, ylim=xylimits, colorbar=False, ax=ax1)\n    ax1.set_title(\"Dogs - Training Data\")\n    Xstkkag[Xstkkag['TypeBlur'] < 0.0].plot.scatter('ypv_gbr','ypv_svr', s=15, c=clrcol, colormap='jet',\n                figsize=allfigsize, alpha=0.15, xlim=xylimits, ylim=xylimits, colorbar=False, ax=ax1r)\n    ax1r.set_title(\"Dogs - Test Data\")\n    ax2 = axarray[0,2]\n    ax2r = axarray[0,3]    \n    Xstk[Xstk['TypeBlur'] > 0.0].plot.scatter('ypv_gbr','ypv_svr', s=10, c=clrcol, colormap='jet',\n                figsize=allfigsize, alpha=0.10, xlim=xylimits, ylim=xylimits, colorbar=False, ax=ax2)\n    ax2.set_title(\"Cats - Training Data\")\n    Xstkkag[Xstkkag['TypeBlur'] > 0.0].plot.scatter('ypv_gbr','ypv_svr', s=15, c=clrcol, colormap='jet',\n                figsize=allfigsize, alpha=0.15, xlim=xylimits, ylim=xylimits, colorbar=False, ax=ax2r)\n    ax2r.set_title(\"Cats - Test Data\")\n    \n    ax3 = axarray[1,0]\n    ax3r = axarray[1,1]\n    Xstk[Xstk['TypeBlur'] < 0.0].plot.scatter('ypv_gbr','ypv_mlp', s=10, c=clrcol, colormap='jet',\n                figsize=allfigsize, alpha=0.10, xlim=xylimits, ylim=xylimits, colorbar=False, ax=ax3)\n    ax3.set_title(\"Dogs - Training Data\")\n    Xstkkag[Xstkkag['TypeBlur'] < 0.0].plot.scatter('ypv_gbr','ypv_mlp', s=15, c=clrcol, colormap='jet',\n                figsize=allfigsize, alpha=0.15, xlim=xylimits, ylim=xylimits, colorbar=False, ax=ax3r)\n    ax3r.set_title(\"Dogs - Test Data\")\n    ax4 = axarray[1,2]\n    ax4r = axarray[1,3]\n    Xstk[Xstk['TypeBlur'] > 0.0].plot.scatter('ypv_gbr','ypv_mlp', s=10, c=clrcol, colormap='jet',\n                figsize=allfigsize, alpha=0.10, xlim=xylimits, ylim=xylimits, colorbar=False, ax=ax4)\n    ax4.set_title(\"Cats - Training Data\")\n    Xstkkag[Xstkkag['TypeBlur'] > 0.0].plot.scatter('ypv_gbr','ypv_mlp', s=15, c=clrcol, colormap='jet',\n                figsize=allfigsize, alpha=0.15, xlim=xylimits, ylim=xylimits, colorbar=False, ax=ax4r)\n    ax4r.set_title(\"Cats - Test Data\")\n    \n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Decide on the target and features\n# And redefine X, Xkag and y, yv.\nif STACKING:\n    # Get the y and yv values:\n    Y0_VALUE = -1.0\n    y = Xstk['y_gbr'].values\n    yv = 1.0*y\n    yv[y == 0] = Y0_VALUE\n    \n    # Select features and create X and Xkag\n    features = ['ypv_gbr','ypv_svr','ypv_mlp','Age','TypeBlur']\n    #\n    X = Xstk[features].copy()\n    #\n    Xkag = Xstkkag[features].copy()\n\n# Currently, there is not option for a separate local test of stacking data:\nLOCAL_TEST = False","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if STACKING:\n    print(X.head(10))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if STACKING:\n    print(y[0:10])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if STACKING:\n    print(yv[0:10])","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true},"cell_type":"markdown","source":"### Can now continue with <a href=\"#SelectModel\">Select the Model</a> to fit this stacked data"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2adab9b58e12a03be8c21f00410f212ef9f96465"},"cell_type":"markdown","source":"## <a id=\"SelectModel\">Select the Model</a>\nBack to <a href=\"#Index\">Index</a>\n\nSelect the model to use, from: dtr, rfr, gbr, svr, mlp"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Use these three for PetFinder:\n##regressor_name = 'gbr'\n##regressor_name = 'svr'\nregressor_name = 'mlp'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# The model to use if we're doing the STACKING:\nif STACKING:\n    ##regressor_name = 'rfr'\n    regressor_name = 'mlp'","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2e7809946fdeeb1142ae4fb387ec7a6d2273228e"},"cell_type":"markdown","source":"## <a id=\"DefineModels\">Define the Models and Parameters</a>\nBack to <a href=\"#Index\">Index</a>\n\nDefine the different models available and their parameters and GSCV parameters."},{"metadata":{"_uuid":"2c8dab0ec5db1a5b0758ed1a796575a6ea9eda52"},"cell_type":"markdown","source":"### DecisionTreeRegressor"},{"metadata":{"_uuid":"8ccd0b342d393aca9c5b76f79767c1c6f13a04d5","trusted":true},"cell_type":"code","source":"# Decision Tree Regressor\n\n# https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html\n\n# DecisionTreeRegressor(\n# criterion=mse, splitter=best, max_depth=None, min_samples_split=2, min_samples_leaf=1,\n# min_weight_fraction_leaf=0.0, max_features=None, random_state=None, max_leaf_nodes=None,\n# min_impurity_decrease=0.0, min_impurity_split=None, presort=False)\n\ndtr_params = {'max_depth': 25,\n                'min_samples_leaf': 5,\n                'min_impurity_decrease': 0.00200\n             }\n\ndtr_param_grid = {'max_depth': [10, 25],\n                'min_samples_leaf': [5, 20],\n                'min_impurity_decrease': [0.00200]\n             }","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"083269b920f3987a247847d0957eaa81fa4e6af0"},"cell_type":"markdown","source":"### RandomForestRegressor"},{"metadata":{"_uuid":"8945cd38bd6601d330cc75ca40d53594b2581ab8","trusted":true},"cell_type":"code","source":"# Random Forest Regressor\n\n# https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html\n\n# RandomForestRegressor(\n# n_estimators=warn, criterion=mse, max_depth=None, min_samples_split=2, min_samples_leaf=1,\n# min_weight_fraction_leaf=0.0, max_features=auto, max_leaf_nodes=None, min_impurity_decrease=0.0,\n# min_impurity_split=None, bootstrap=True, oob_score=False, n_jobs=None, random_state=None, verbose=0,\n# warm_start=False)\n\nrfr_params = {'n_estimators': 200,\n              'max_depth': 10,\n              'min_samples_leaf': 5,\n              'min_impurity_decrease': 0.00020\n             }\n\nrfr_param_grid = {'max_depth': [7, 10, 15],\n                  'min_samples_leaf': [5],\n                  'min_impurity_decrease': [0.00020]\n                 }\n\nif STACKING:\n    # Use different parameters for the stacking fitting...\n    rfr_params = {'n_estimators': 200,\n              'max_depth': 9,\n              'min_samples_leaf': 25,\n              'min_impurity_decrease': 0.00040\n             }\n    rfr_param_grid = [\n                    {'max_depth': [5],  #[5, 7, 9],\n                  'min_samples_leaf': [40],  #[15, 25, 40],\n                  'min_impurity_decrease': [0.00060]  #[0.00025, 0.00040, 0.00060]\n                 }\n    ]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4967fbfbc974719e1267bd74dc6e39383d67b1b1"},"cell_type":"markdown","source":"### GradientBoostingRegressor"},{"metadata":{"_uuid":"42e6c80820bade6f03b67f9162a3bf335936e464","trusted":true},"cell_type":"code","source":"# Gradient Boosting Regressor\n\n# https://scikit-learn.org/stable/auto_examples/ensemble/plot_gradient_boosting_regression.html\n\n# GradientBoostingRegressor(\n#      loss=ls,\n#      learning_rate=0.1, \n#      n_estimators=100,\n#      subsample=1.0,\n#      max_depth=3,\n#      min_samples_split=2, min_samples_leaf=2,\n#      min_impurity_decrease=0.0,\n#      max_features=None,\n#      n_iter_no_change=None, validation_fraction=0.1, tol=0.0001\n#      verbose=0,\n#\n#      criterion=friedman_mse,\n#      min_weight_fraction_leaf=0.0, \n#      min_impurity_decrease=0.0, min_impurity_split=None, init=None, random_state=None,\n#      alpha=0.9,\n#      max_leaf_nodes=None, warm_start=False, presort=auto, \n#      )\n\n# Parameters for this model\n\n# Use a larger validation fraction if there is no local test set:\nif LOCAL_TEST:\n    valid_fraction = 0.08\nelse:\n    valid_fraction = 0.20\n\n# possible loss functions:\n#  ls refers to least squares regression.\n#  lad (least absolute deviation) is a highly robust loss function solely based\n#        on order information of the input variables.\n#  huber is a combination of the two. \n\n# Other parameters for this model.   #***** are ones to focus on tuning...\n# Values here are updated to be the current 'best' ones.\ngbr_params = {'loss': 'ls',        # <-- try both ls and huber\n          'learning_rate': 0.013, # Smaller better but n_estimators grows\n          'n_estimators': 400,   # Early stopping will limit this, so just set a large value.\n          'max_depth': 80,      # Keep this large and let other parameters limit the depth\n          #\n          'max_features': 0.80,      #***** <1.0 reduces variance and increases bias\n          'min_impurity_decrease': 0.025,   #*****\n          'min_samples_leaf': 33,         # *****\n          'min_samples_split': 550,      #*****\n          'subsample': 0.60,            #***** less than 1.0 to reduce variance, increase bias\n          # early stopping:      \n          # allows not tuning the n_estimators parameter\n          'n_iter_no_change': 10,\n          'validation_fraction': valid_fraction, 'tol': 0.0005,\n          'verbose': 1\n          }\n\n# Setup hyper-parameter grid for the model:\n# . . .\n# Do a course 'scan' of each of some of the 'best' parameter values\ngbr_param_grid = [\n    {'max_features': [0.50, 0.80, 0.95],\n     'min_impurity_decrease': [0.025],\n     'min_samples_leaf': [33],\n     'min_samples_split': [550],\n     'subsample': [0.60]},\n    {'max_features': [0.80],\n     'min_impurity_decrease': [0.025],\n     'min_samples_leaf': [8, 33, 120],\n     'min_samples_split': [550],\n     'subsample': [0.60]},\n    {'max_features': [0.80],\n     'min_impurity_decrease': [0.025],\n     'min_samples_leaf': [33],\n     'min_samples_split': [550],\n     'subsample': [0.25, 0.60, 0.90]},\n]\n\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7984a2f3c4fab4fba098bb307a9c63a71d7321c4"},"cell_type":"markdown","source":"### SVR"},{"metadata":{"_uuid":"cfe058c5752d3bf91f5f5197a6b16a8930d6fdbe","trusted":true},"cell_type":"code","source":"# Support Vector Regression\n\n# https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html\n\n# SVR(\n# kernel=rbf, degree=3, gamma=auto_deprecated, coef0=0.0, tol=0.001,\n# C=1.0, epsilon=0.1, shrinking=True, cache_size=200, verbose=False, max_iter=-1)\n\nsvr_params = {'kernel': 'poly',\n              'degree': 3,\n              'coef0': 0.0,\n              'tol': 0.003,\n              #\n              'C': 1.4,\n              'epsilon': 0.60,\n              'gamma': 0.007,\n              #\n              'shrinking': True,\n              'cache_size': 500,\n              'verbose': True,\n              'max_iter': 100000\n             }\n\n# Setup hyper-parameter grid for the model:\n# . . .\n# Do a course 'scan' of each of some of the 'best' parameter values\nsvr_param_grid = [\n    {'C': [0.3, 1.4, 6.0],\n    'epsilon': [0.6],\n    'gamma': [0.007]\n    },\n    {'C': [1.4],\n    'epsilon': [0.2, 0.6, 2.0],\n    'gamma': [0.007]\n    },\n    {'C': [1.4],\n    'epsilon': [0.6],\n    'gamma': [0.001, 0.007, 0.070]\n    }\n]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Neural Network: MLP"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Neural Network Regressor\n\n# https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPRegressor.html\n\n# MLPRegressor(\n# hidden_layer_sizes=(100, ), activation=relu, solver=adam, alpha=0.0001,\n# batch_size=auto, learning_rate=constant, learning_rate_init=0.001, power_t=0.5,\n# max_iter=200, shuffle=True, random_state=None, tol=0.0001,\n# verbose=False, warm_start=False, momentum=0.9, nesterovs_momentum=True,\n# early_stopping=False, validation_fraction=0.1,\n# beta_1=0.9, beta_2=0.999, epsilon=1e-08, n_iter_no_change=10)\n\n# Use a larger validation fraction if there is no local test set:\nif LOCAL_TEST:\n    valid_fraction = 0.12\nelse:\n    valid_fraction = 0.20\n    \n    \nmlp_params = {'hidden_layer_sizes': (12, 4),\n              'alpha': 2.000,   # L2 regularization param\n              'learning_rate_init': 0.0030,\n              # less often changed parameters:\n              'batch_size': 50,  # bit better than auto=200\n              'momentum': 0.70,  # for sgd; 0.95 more erratic\n              #\n              'activation': 'relu',\n              'solver': 'sgd',\n              'learning_rate': 'constant',\n              'max_iter': 500, # number of epochs, uses < 100 for (12,4)\n              'tol': 0.0001,\n              'n_iter_no_change': 10,  # number of epochs\n              'early_stopping': True,\n              'validation_fraction': valid_fraction,\n              #\n              'verbose': False\n             }\n\n# Setup hyper-parameter grid for the model:\n# . . .\n# Do a course 'scan' of each of some of the 'best' parameter values\nmlp_param_grid = [\n    {'alpha': [0.02, 2.0, 20.0],  # L2 regularization param\n     'learning_rate_init': [0.0030],\n     'momentum': [0.70]\n    },\n    {'alpha': [2.0,],  # L2 regularization param\n     'learning_rate_init': [0.0003, 0.0030, 0.0300],\n     'momentum': [0.70]\n    },\n    {'alpha': [2.0,],  # L2 regularization param\n     'learning_rate_init': [0.0030],\n     'momentum': [0.25, 0.70, 0.95]\n    }\n]\n\n# If using MLP for the stacking then change the NN size:\nif STACKING:\n    # Use small NN size\n    mlp_params['hidden_layer_sizes'] = (5, 3)\n    mlp_params['alpha'] = 14.5\n    # Vary the L2 regularization, 3 times at 3 similar values \n    mlp_param_grid = [\n    {'alpha': [14.4, 14.5, 14.6],  # L2 regularization param\n     'learning_rate_init': [0.0030],\n     'momentum': [0.70]\n    },\n    {'alpha': [14.4, 14.5, 14.6],\n     'learning_rate_init': [0.0030],\n     'momentum': [0.70]\n    },\n    {'alpha': [14.4, 14.5, 14.6],\n     'learning_rate_init': [0.0030],\n     'momentum': [0.70]\n    }\n    ]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"79095ca20851d7549d8348b0262bc21d622c8808"},"cell_type":"markdown","source":"### Setup the model that was selected"},{"metadata":{"_uuid":"1bcbc1ce745ae4b0d97f76440b11d6193dcf26c9","trusted":true},"cell_type":"code","source":"# Setup the (regression) model:\n\nif regressor_name == 'dtr':\n    regressor = DecisionTreeRegressor(**dtr_params)\n    param_grid = dtr_param_grid\n\nelif regressor_name == 'rfr':\n    regressor = RandomForestRegressor(**rfr_params)\n    param_grid = rfr_param_grid\n\nelif regressor_name == 'gbr':\n    regressor = GradientBoostingRegressor(**gbr_params)\n    param_grid = gbr_param_grid\n    \nelif regressor_name == 'svr':\n    regressor = SVR(**svr_params)\n    param_grid = svr_param_grid\n    \nelif regressor_name == 'mlp':\n    regressor = MLPRegressor(**mlp_params)\n    param_grid = mlp_param_grid\n    \nelse:\n    invalid_regressor_name_stopping()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cd2ff97f104b137d0515084b532cc293bf49415c"},"cell_type":"markdown","source":"### Fit the Model with Nominal Hyper-Parameters\n"},{"metadata":{"_uuid":"9966b57a553b00d6278114a8e914a8270c02c78c","trusted":true},"cell_type":"code","source":"# Doing this fit here lets us skip over the Hyper-Parameter Search and continue\nif True:\n    best_regressor = regressor.fit(X,yv)\n    # Show these parameters\n    print(best_regressor.get_params())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3de077a81fdb3b1ca8d6dab5d61898bbbdfbc5e4"},"cell_type":"markdown","source":"# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n### Continue with the Hyper-Parameter Search that follows\n#### Or skip the whole Hyper-Parameter section, go to <a href=\"#FeatureImportance\">Feature Importance</a> <br>\n# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -"},{"metadata":{"_uuid":"d8df093b98ddb023242bd2c4b8f50e3438d3e9a5"},"cell_type":"markdown","source":"## <a id=\"HyperSearch\">Do the Hyper-Parameter Search</a>\nBack to <a href=\"#Index\">Index</a> <br>\n\nScan various combinations, or <br>\nJust do single/small numbers of values of the parameters."},{"metadata":{"_uuid":"1823d2db209ba8cbd0f91b8a1057fb2ea88db768","trusted":true},"cell_type":"code","source":"# Define scoring function(s)\n\n# Demo of make_scorer with simple example:\nmse_scorer = make_scorer(mean_squared_error, greater_is_better=False)\n\n# Calculate QWK from yv and yv_pred:\ndef qwk_score(yvtrue, yvpred):\n    model_boundaries = get_class_bounds(yvtrue, yvpred)\n    yclasspred = assign_class(yvpred, model_boundaries)\n    # need the actual 0,1,...,4 y class values\n    yclass = yvtrue.copy()\n    yclass[yclass <= 0.0] = 0.0\n    yclass = yclass.astype(int)\n    qwk_val = cohen_kappa_score(yclass, yclasspred, labels=None, weights=\"quadratic\")\n    return qwk_val\n\n# Then use that to create a scorer:\nqwk_scorer = make_scorer(qwk_score, greater_is_better=True)\n\ngscv_scorer = qwk_scorer\nscorer_name = 'QWK'","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"846c3b2877046867b0e630802cfd988848416c70","trusted":true},"cell_type":"code","source":"# Set the CV parameter or method:\n#\n# Use KFold with chosen number of folds: \n##cv_folds = 6\n##cv_param = cv_folds\n\n# Use GroupKFold; cv_param is the splits iterator\n# Use X, yv here though it is just for indices so y would be OK too.\n#\ncv_folds = 6\n#\nif not(STACKING):\n    # Use Group k-Fold for single-model fitting:\n    gkf = GroupKFold(n_splits=cv_folds)\n    cv_param = gkf.split(X, yv, groups=grp_train)\nelse:\n    # Try different train-test split methods for stacking:\n    cv_folds = 18\n    repeats = 3\n    rkf = RepeatedKFold(n_splits=int(cv_folds/repeats), n_repeats=repeats)\n    cv_param = rkf","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f7f4c1f5374f31e9822750a34e5385cfc6ea5203","trusted":true},"cell_type":"code","source":"# Do the Grid Search\n\nprint(\"\\nDoing Grid Search on model: \"+regressor_name+\".\\n\")\n\n# Select number of CPUs for GSCV depending if on Kaggle or not:\nif LOCATION_KAGGLE:\n    number_cpus = -1\nelse:\n    number_cpus = -2\n\n#GridSearchCV(estimator, param_grid, scoring=None,\n#             fit_params=None,\n#             n_jobs=None, iid=warn, refit=True, cv=warn,\n#             verbose=0,\n#             pre_dispatch=2*n_jobs, error_score=raise-deprecating,\n#             return_train_score=warn)\n\n# param_grid is:\n# Dictionary with parameters names (string) as keys and lists of parameter settings to try as values,\n# or a list of such dictionaries.\n    \ngscv = GridSearchCV(regressor, param_grid,\n            scoring=qwk_scorer,      #  or: mse_scorer  or  'neg_mean_squared_error',\n            n_jobs=number_cpus, #  -2: all CPUs but one are used\n            iid=False,  # independent identical distrib., \"agrees with standard defn of CV\"\n            cv=cv_param,\n            refit=True,\n            verbose=2,\n            return_train_score=True)\n\nt0 = time()\n\n_dummy = gscv.fit(X, yv)\n\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8ca402f3a19d1cc322868305b952a90cec39e652"},"cell_type":"markdown","source":"### Best model found by GS CV"},{"metadata":{"_uuid":"b79b07930d8c81fb24d2fd31515cabd238eee9d4","trusted":true},"cell_type":"code","source":"# Get the best model\nbest_regressor = gscv.best_estimator_\n\n# Fit it to all the training data - This was already done by the refit=True\n##best_gbr.fit(X, yv)\n\nif regressor_name == 'gbr':\n    print(\"\\nGSCV Fitting took {:.1f} minutes. (Final fit took {} iterations.)\".format(\n                                    (time()-t0)/60.0, len(best_regressor.train_score_)))\nelif regressor_name == 'mlp':\n    print(\"\\nGSCV Fitting took {:.1f} minutes. (Final fit took {} iterations.)\".format(\n                                    (time()-t0)/60.0, best_regressor.n_iter_))\nelse:\n    print(\"\\nGSCV Fitting took {:.1f} minutes.\".format((time()-t0)/60.0))\n\nprint(\"\")\ntrain_mse = mean_squared_error(yv, best_regressor.predict(X))\nprint(\"Final All-Train MSE: {:.5f}\".format(train_mse))\nif LOCAL_TEST:\n    loctest_mse = mean_squared_error(yv_loc, best_regressor.predict(Xloc))\n    print(\"Local-Test MSE: {:.5f}\".format(loctest_mse))\nelse:\n    loctest_mse = -1.0","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c749bb3b541b10ba78389c9a954ca660444e731b","trusted":true},"cell_type":"code","source":"# Show the parameters\nbest_regressor.get_params()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8c4bfdddfcadc70a7a4f9a60b87c0f5a527a7fe2"},"cell_type":"markdown","source":"### GBR training plot"},{"metadata":{"_uuid":"b8a5e7a23ed688b9e365340bea097e57d21394d4","trusted":true},"cell_type":"code","source":"# #############################################################################\n# Plot training deviance vs iteration - only for GBR\n\nif regressor_name == 'gbr':\n    # compute (local) test set deviance vs iteration number\n    if LOCAL_TEST:\n        loctest_score = np.zeros(len(best_regressor.train_score_), dtype=np.float64)\n        for i, ypv_loctest_iter in enumerate(best_regressor.staged_predict(Xloc)):\n            loctest_score[i] = best_regressor.loss_(yv_loc, ypv_loctest_iter)\n\n    plt.figure(figsize=(12, 6))\n    plt.title('Loss vs Iteration.  Final train/local-test MSE = {:.4f} / {:.4f}'.format(train_mse, loctest_mse))\n\n    x_estimators = np.arange(len(best_regressor.train_score_)) + 1\n    if LOCAL_TEST:\n        plt.plot([0,len(loctest_score)],[0.0,0.0],'--')\n        plt.plot(x_estimators, loctest_score, 'r-',\n             label='Local-Test Loss vs Iter.s')\n        plt.plot([0,len(loctest_score)],[loctest_mse,loctest_mse],'g--',label='Final Local-Test MSE')\n    # The training curve\n    plt.plot(x_estimators, best_regressor.train_score_, 'b-',\n         label='Training Loss vs Iter.s')\n    plt.plot([0,len(best_regressor.train_score_)],[train_mse,train_mse],'g--',label='Final All-Training MSE')\n\n    plt.legend(loc='center right')\n    plt.xlabel('Boosting Iterations')\n    plt.ylabel('Deviance')\n\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ab3ecbe4f9094502284e33fca6960e121cf4c335"},"cell_type":"markdown","source":"## <a id=\"CVResults\">Grid Search CV Results</a>\nBack to <a href=\"#Index\">Index</a>"},{"metadata":{"_uuid":"305fee5cb9b9146aa06de4a6f5defd4c97f54bce","trusted":true},"cell_type":"code","source":"# The GSCV results are given by the python dictionary:\n##gscv.cv_results_\n\n# Here the TEST refers to the out-of-fold/validation data in the CV process.\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bc6831e6da7f5e5ca58de0e7caea263a109a5022","trusted":true},"cell_type":"code","source":"# Put the grid search results in a pandas dataframe\n# Sort by a value, e.g, the test score which is -1*MSE\ndf_gscv = pd.DataFrame.from_dict(gscv.cv_results_).sort_values(by='mean_test_score',ascending=False)\n\n# Put the original order in an \"index\" column:\ndf_gscv = df_gscv.reset_index()\n\n# form the statistics of the columns\ngscv_stats = df_gscv.describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5ed4b62cf3b28efa9b6687a647ad625b3dec9af1","trusted":true},"cell_type":"code","source":"# Look into the std values given by the GSCV output...\nif False:\n    # The average std of the test scores is given as:\n    print(\"\\nAverage std_test_score = {:.4f}\".format(gscv_stats.loc['mean','std_test_score']))\n    print(\"This is much larger than the range of the mean_test_score s (further below.)\")\n\n    # The error bars are dominated by the differences in score from split-to-split:\n    print(\"\\nThere is large (but consistent) variation between the splits:\")\n    for itest in range(3):   # len(df_gscv)):\n        for isplit in range(cv_folds):\n            print(\"split {}: score = {:.4f}\".format(isplit, df_gscv.loc[itest,'split'+str(isplit)+'_test_score']))\n        print(\"mean test score = {:.4f}\".format(df_gscv.loc[itest,'mean_test_score']))\n        print(\" - - -\")\n    print(\" etc\")\n\n    # This is much larger than the variation of the same split due to fitting/parameter changes:\n    print(\"\\nIn contrast the variation within a single split is smaller:\")\n    for itest in range(6):   # len(df_gscv)):\n        for isplit in [0]:\n            print(\"split {}: score = {:.4f}\".format(isplit, df_gscv.loc[itest,'split'+str(isplit)+'_test_score']))\n    print(\" etc\")\n\n# Calculate the std of the each-split's scores,\n# and the average std of a split score.\n##print(\"\\nScore random variation is estimated from the std of each split:\")\nstd_split = 0.0\nfor isplit in range(cv_folds):\n    ##print(\"std split{} = {:.4f}\".format(isplit,gscv_stats.loc['std','split'+str(isplit)+'_test_score']))\n    std_split += (gscv_stats.loc['std','split'+str(isplit)+'_test_score'])**2\nstd_split = np.sqrt(std_split/cv_folds)\n\nprint(\"The average standard deviation of test-split scores is {:.4f}\".format(std_split))\n# The sterr expected from the split std is then:\nsterr_splits = std_split/np.sqrt(cv_folds)\nprint(\"Hence, the expected sterr of the test scores is {:.4f}\".format(sterr_splits))\n\n\n# Add a sterr_test_score column\n# not using this:\n##df_gscv['sterr_test_score'] = df_gscv['std_test_score']/np.sqrt(cv_folds)\n# but using the sterr_splits instead:\ndf_gscv['sterr_test_score'] = sterr_splits","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e7ec5f25a3c5fbe5c42d5bd474884a2f8f471b8b","trusted":true},"cell_type":"code","source":"# Save the dataframe to a file (in out_dir):\ntimestr = strftime(\"%m-%d-%y_%H-%M\")\n# include the regressor name too\ndf_gscv.to_csv(out_dir+\"/GSCV_\" + regressor_name +\n               \"_{}_{}.csv\".format(timestr, version_str), header=True, index=True)\n\n# Show all the rows\ndf_gscv\n# Show the first some number of rows\n##df_gscv[0:min([6,len(df_gscv)])]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"dd55e9c372b7fb4c1df56d081416357d0e41f052","trusted":true},"cell_type":"code","source":"# Update the gscv_stats variable with the new sterr column\ngscv_stats = df_gscv.describe()\n\nprint(\"\\nMean test score Min/50%/75%/Max: {:.4f}, {:.4f}, {:.4f}, {:.4f}\".format(\n                gscv_stats.loc['min','mean_test_score'],\n                gscv_stats.loc['50%','mean_test_score'], gscv_stats.loc['75%','mean_test_score'],\n                gscv_stats.loc['max','mean_test_score']))\nprint(\"\\nRange of mean_test_score = {:.4f}, sterr_test_score = {:.4f}\".format(\n                gscv_stats.loc['max','mean_test_score'] - gscv_stats.loc['min','mean_test_score'],\n                gscv_stats.loc['50%','sterr_test_score']))\n##gscv_stats\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0ceec22b1e4e89fd40a72fa1ca16101ee3afaaf0","trusted":true},"cell_type":"code","source":"# Get a list of all the parameters that were scanned, the keys from the dicts(s):\nif type(param_grid) ==  type({1:2}):\n    # it's a dictionary\n    param_keys = list(param_grid.keys())\nelse:\n    # it's a list of dictionaries\n    param_keys = []\n    for pdict in param_grid:\n        for key in pdict.keys():\n            param_keys.append(key)\n# make it a sorted, unique list\nparam_keys = list(set(param_keys))\nparam_keys.sort()\nprint(param_keys)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"670fb563718af69f5cf22ddfc5b07dd30881dd9f","trusted":true},"cell_type":"code","source":"# Show test score with error bars vs the GS original order (index)\n# Can color-code points by one of the parameters (or not.)\n# Pic a param\nthis_param = param_keys[0]\ndf_gscv.plot.scatter('index', 'mean_test_score', yerr='sterr_test_score', figsize=(15,4),\n                        title='Test Score ('+scorer_name+') vs Grid Search index',\n                        c=\"param_\"+this_param, colormap='plasma')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e5dfe593985af5e03cbd3ff03cd719f6cce1e7ea","trusted":true},"cell_type":"code","source":"# Make plots of the test values vs param values for the params\nnum_param_keys = len(param_keys)\nfig, axes = plt.subplots(1,num_param_keys,sharey=True,figsize=(15,5))\nfor iparam, this_param in enumerate(param_keys):\n    # Why is this needed to get scatter to work?!?\n    df_gscv[\"param_\"+this_param] = df_gscv[\"param_\"+this_param].astype(float)\n    if num_param_keys > 1:\n        ax = axes[iparam]\n    else:\n        ax = axes\n    # without or with the error bars\n    ##df_gscv.plot.scatter(\"param_\"+this_param,'mean_test_score',ax=ax)\n    df_gscv.plot.scatter(\"param_\"+this_param,'mean_test_score',ax=ax, yerr='sterr_test_score',\n                         # include this to get better scaling for small values...\n                         xlim=(0.000,1.1*max(df_gscv[\"param_\"+this_param])))\n    \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fe2520a353d040885af1ac631214bae179564fd6","trusted":true},"cell_type":"code","source":"# Show the test score vs train score with color-code by parameter values (if useful)\nif True:\n    for this_param in param_keys:\n        df_gscv.plot.scatter('mean_train_score','mean_test_score',\n                         c=\"param_\"+this_param, colormap='plasma',\n                         sharex=True, figsize=(15,4), yerr='sterr_test_score')\n\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"25679277d7f1fc37a2224d7e2a0d5199da5e1a72"},"cell_type":"markdown","source":"# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n###  Done with Hyper-parameters\n# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -"},{"metadata":{"_uuid":"8077ecf07cbb376adcf9f19f0c986623cb6dd653"},"cell_type":"markdown","source":"## <a id=\"FeatureImportance\">Feature Importance</a>\nBack to <a href=\"#Index\">Index</a>"},{"metadata":{"_uuid":"26d5c5e20ce92f3f416b69a0ae686594d96c627b","trusted":true},"cell_type":"code","source":"if regressor_name in ['dtr','rfr','gbr','mlp']:\n    # Plot feature importance\n    # Get feature importance\n    if regressor_name == 'mlp':\n        # For mlp regressor create a quasi-importance from the weights.\n        # \"The ith element in the list represents the weight matrix corresponding to layer i.\"\n        # Input layer weights\n        ##len(best_regressor.coefs_[0])\n        # sum of abs() of input weights for each feature\n        feature_importance = np.array([sum(np.abs(wgts)) for wgts in best_regressor.coefs_[0] ])\n    else:\n        # tree models have feature importance available:\n        feature_importance = best_regressor.feature_importances_\n    # make importances relative to max importance\n    max_import = feature_importance.max()\n    feature_importance = 100.0 * (feature_importance / max_import)\n    sorted_idx = np.argsort(feature_importance)\n    pos = np.arange(sorted_idx.shape[0]) + 0.5\n\n    plt.figure(figsize=(8, 15))\n    ##plt.subplot(1, 2, 2)\n    plt.barh(pos, feature_importance[sorted_idx], align='center')\n    plt.yticks(pos, X.columns[sorted_idx])\n    plt.xlabel(regressor_name.upper()+' -- Relative Importance')\n    plt.title('           '+regressor_name.upper()+\n              ' -- Variable Importance                  max --> {:.3f} '.format(max_import))\n\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7a615da5097bfa6c35efe62120665350a3dfdcc6"},"cell_type":"markdown","source":"## <a id=\"EvaluateBest\">Evaluate Best Model</a>\nBack to <a href=\"#Index\">Index</a>"},{"metadata":{"_uuid":"5d66fcab3e50711ba907fb948a4a1619ea325ee2"},"cell_type":"markdown","source":"### Training: QWK"},{"metadata":{"_uuid":"cede3ba83061bc5ddb21f22ed53ac8e7e563549f","scrolled":true,"trusted":true},"cell_type":"code","source":"# Make the model predictions on the Training data\nypv = best_regressor.predict(X)\n\n# Set the classification boundaries based on the predicted Training values\n#\n# Find boundaries to separate ypv into same class numbers as y:\nmodel_boundaries = get_class_bounds(y, ypv)\nif USE_OPTR:\n    # use optimized rounder too\n    optR = OptimizedRounder_v2()\n    optR.fit(ypv, y, initial_coef=model_boundaries, tol=0.0001)\n    model_boundaries = optR.coefficients()\n        \nprint(\"\\nUsing post-fit boundaries = \", model_boundaries)\n\n# Apply boundaries to training to get predicted classes\nyp = assign_class(ypv, model_boundaries)\ntrain_kappa = cohen_kappa_score(y, yp, labels=None, weights=\"quadratic\")\nprint(\"\\n--TRAIN-- kappa (Cohen, quadratic) = {:.5f}\".format(train_kappa))\n\nprint(\"\")\n\n# Confusion Matrix\nif True:\n    con_mat = confusion_matrix(y, yp, labels=None, sample_weight=None)\n    print(con_mat)\n    print(\"\\nAll-training kappa = {:.3f}\".format(train_kappa))\n    ##print(\"Mean-squared error = {:.4f}\".format(mean_squared_error(y, yp)))\n    print(\"Accuracy = {:.2f}%\".format(100.0*accuracy_score(y, yp))) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# --- Toward Stacking ---\n# If there was no LOCAL_TEST (so X contains all samples), then\n# Save the y, yp, and ypv as columns in the X dataframe and write it out:\nif not(LOCAL_TEST) and not(STACKING):\n    X['y_'+regressor_name] = y\n    X['yp_'+regressor_name] = yp\n    X['ypv_'+regressor_name] = ypv\n    # Write it out:\n    X.to_csv(out_dir+\"/Xys_train_\"+regressor_name+\"_\"+version_str+\".csv\")\n    # Remove the added columns:\n    X = X.drop(['y_'+regressor_name,'yp_'+regressor_name,'ypv_'+regressor_name],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d35627e8128a0b616601abee743362733fef13d6","trusted":true},"cell_type":"code","source":"# Can look at the QWK for just subsets based on the features,\n# e.g., Dogs vs Cats\ndef subset_qwk(X, y, ypv, subset_bool):\n    global USE_OPTR\n    Xcopy = X.copy()\n    Xcopy['y'] = y\n    Xcopy['ypv'] = ypv\n    subset_boundaries = get_class_bounds(Xcopy.loc[subset_bool,'y'], Xcopy.loc[subset_bool,'ypv'])\n    if USE_OPTR:\n        # use optimized rounder too\n        optR = OptimizedRounder_v2()\n        optR.fit(Xcopy.loc[subset_bool,'ypv'], Xcopy.loc[subset_bool,'y'],\n                 initial_coef=subset_boundaries, tol=0.0001)\n        subset_boundaries = optR.coefficients()\n    ##print(25*\" \"+str(subset_boundaries))\n    subset_yp = assign_class(Xcopy.loc[subset_bool,'ypv'], subset_boundaries)\n    subset_kappa = cohen_kappa_score(Xcopy.loc[subset_bool,'y'], \n                                     subset_yp, labels=None, weights=\"quadratic\")\n    ##print(\"\\nQWK of subset = {:.5f}\".format(subset_kappa))\n    return subset_kappa, subset_boundaries","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4d29a4e4d6800fc454c5c9029f14c864b5b2b685","trusted":true},"cell_type":"code","source":"all_qwk, all_bounds = subset_qwk(X, y, ypv, X['TypeBlur']>-10)\nprint(\"QWK for  All: {:.3f}\".format(all_qwk), all_bounds)\ndogs_qwk, dog_bounds = subset_qwk(X, y, ypv, X['TypeBlur']<0)\nprint(\"QWK for Dogs: {:.3f}\".format(dogs_qwk), dog_bounds)\ncats_qwk, cat_bounds = subset_qwk(X, y, ypv, X['TypeBlur']>0)\nprint(\"QWK for Cats: {:.3f}\".format(cats_qwk), cat_bounds)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"dff0acaa34d07938dd515b9b9f10e63b159ad781","trusted":true},"cell_type":"code","source":"# Show the ypv distribution by known AdoptionSpeed\n\n# Temporarily ... Put the model speed and ys in the X dataframe:\nX['preds'] = ypv\nX['AdoptionSpeed'] = y\n##X.groupby('AdoptionSpeed').preds.agg(['min', 'max', 'median', 'mean'])\n\nX.hist('preds', by='AdoptionSpeed', bins=100, sharex=True, sharey=True, layout=(5,1), figsize=(14,9))\n# Todo: overplot the boundaries\nplt.show()\n\n# Remove the added columns:\nX = X.drop(['AdoptionSpeed','preds'],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b53fe98b72368e35ffc6fa8951c525f4a037415d","trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5cd9597aad28ae60cb93142ac13c29410603bb27","trusted":true},"cell_type":"code","source":"if LOCAL_TEST:\n    # The regression Test predictions (floats)\n    ypv_loctest = best_regressor.predict(Xloc)\n    # Apply the boundaries to get final Test classes\n    yp_loctest = assign_class(ypv_loctest, model_boundaries)\n    loctest_kappa = cohen_kappa_score(y_loc, yp_loctest, labels=None, weights=\"quadratic\")\n    print(\"\\nLocal-Test kappa (Cohen, quadratic) = {:.5f}\\n\".format(loctest_kappa))\n    \n    # Confusion Matrix\n    if True:\n        con_mat = confusion_matrix(y_loc, yp_loctest, labels=None, sample_weight=None)\n        print(con_mat)\n        print(\"\\n\"+regressor_name.upper()+\n              \":  Local-Test = {:.3f},    Train = {:.3f},   GSCV:  Test = {:.3f}, Train = {:.3f}\".format(\n            loctest_kappa, train_kappa, gscv_stats.loc['max','mean_test_score'],\n              gscv_stats.loc['max','mean_train_score']))\n        ##print(\"Mean-squared error = {:.4f}\".format(mean_squared_error(y, yp)))\n        print(\"Accuracy = {:.2f}%\".format(100.0*accuracy_score(y_loc, yp_loctest))) \nelse:\n    # Summarize the other values anyway\n    print(\"\\n\"+regressor_name.upper()+\n              \":  Local-Test = - - -,    Train = {:.3f},   GSCV:  Test = {:.3f}, Train = {:.3f}\".format(\n            train_kappa, gscv_stats.loc['max','mean_test_score'],\n              gscv_stats.loc['max','mean_train_score']))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c98b28606a5fa426d4cfa5bd6559ac0828388cb3","trusted":true},"cell_type":"code","source":"# Histogram of the model's predicted values\n\nfixed_bins = 0.0 + np.array(range(5*50+1))/(5*10.0)\n\nplt.figure(figsize=(15, 6))\nplt.subplot(2, 1, 1)\nplt.hist([ypv]+[-1.0]+[5.0], bins=fixed_bins, histtype='stepfilled')\nplt.title(\"Train: Prediction Values\")\nif LOCAL_TEST:\n    plt.subplot(2, 1, 2)\n    plt.hist([ypv_loctest]+[-1.0]+[5.0], bins=fixed_bins, histtype='stepfilled')\n    plt.title(\"Local-Test: Prediction Values\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Bottom line for the Training/Test(s)\nprint(\"\\n\"+50*\" =\")\nprint(\"\\nThe Bottom Line on the Fitting:\\n\")\nif LOCAL_TEST:\n    print(\"\\n\"+regressor_name.upper()+\n              \":  Local-Test = {:.3f},    Train = {:.3f},   GSCV:  Test = {:.3f}, Train = {:.3f}\".format(\n            loctest_kappa, train_kappa, gscv_stats.loc['max','mean_test_score'],\n              gscv_stats.loc['max','mean_train_score']))\n    ##print(\"Mean-squared error = {:.4f}\".format(mean_squared_error(y, yp)))\n    print(\"Accuracy = {:.2f}%\".format(100.0*accuracy_score(y_loc, yp_loctest))) \nelse:\n    # Summarize the other values anyway\n    print(\"\\n\"+regressor_name.upper()+\n              \":  Local-Test = - - -,    Train = {:.3f} ({:.1f}%),   GSCV:  Test = {:.3f}, Train = {:.3f}\".format(\n                train_kappa, 100.0*accuracy_score(y, yp),\n                  gscv_stats.loc['max','mean_test_score'], gscv_stats.loc['max','mean_train_score']))\nprint(\"\\n\"+50*\" =\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"95f5fdc1524e245d0e4da5269c01082a83c2ba16"},"cell_type":"markdown","source":"## <a id=\"OutputKaggle\">Output Kaggle Predictions</a>\nBack to <a href=\"#Index\">Index</a>"},{"metadata":{"_uuid":"a95bc328322254e19f0dc4be3b6f1e2a0f48737d","trusted":true},"cell_type":"code","source":"# Make the Kaggle set predictions\nypv_kag = best_regressor.predict(Xkag)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"68a2f544d6e5540a59f5551ac4854c7d00550733","trusted":true},"cell_type":"code","source":"# Histogram of the Kaggle predictions\nplt.figure(figsize=(15, 6))\nplt.subplot(2, 1, 1)\nplt.hist([ypv_kag]+[-1.0]+[5.0], bins=fixed_bins, histtype='stepfilled')\nplt.title(\"Kaggle: Prediction Values\")\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a65ab524922022d5b56092b7031e1546754249af","trusted":true},"cell_type":"code","source":"# Apply the boundaries to get final Kaggle classes\nyp_kag = assign_class(ypv_kag, all_bounds)\n# Put these predictions into the original df_test which is the Kaggle test data\ndf_test['AdoptionSpeed'] = assign_class(ypv_kag, all_bounds)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c572d5ed8f675cc0264cbb266626ca99a51c5e41","trusted":true},"cell_type":"code","source":"# Apply separate boundaries for Dogs and Cats:\nif False:\n    # Make it clear that this was done:\n    print(60*\"=\"+\"\\n\")\n    print(\" - - -  Using Different Boundaries for Dogs and Cats - - -\")\n    print(\"\\n\"+60*\"=\")\n    # Reset the predicted classes\n    df_test['AdoptionSpeed'] = -1\n    # Put the ypv_kag into the original df test\n    df_test['ypv'] = ypv_kag\n    # Assign Dogs...\n    dog_sel = df_test['Type'] == 1\n    df_test.loc[dog_sel,'AdoptionSpeed'] = assign_class(df_test.loc[dog_sel,'ypv'], dog_bounds)\n    # Assign Cats...\n    cat_sel = df_test['Type'] == 2\n    df_test.loc[cat_sel,'AdoptionSpeed'] = assign_class(df_test.loc[cat_sel,'ypv'], cat_bounds) ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d62f187d06626398b82fc87cbcf59217eae7b8e7","trusted":true},"cell_type":"code","source":"# Any -1 s remaining for answers?\nall_answered = (df_test.AdoptionSpeed.min() >= 0)\nprint(\"All predictions made?  {}\".format(all_answered))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9c6b4690371cea989f9ae6f945899503bd9a2a71","trusted":true},"cell_type":"code","source":"if all_answered:\n    # Save the result as the submission (will over-write when stacking)\n    df_test[['PetID','AdoptionSpeed']].to_csv(\"submission.csv\",index=False)\n\n    # Output model results to use as a stacking-input file:\n    if not(STACKING) and not(LOCAL_TEST):\n        # --- Toward Stacking ---\n        # Save the yp_kag and ypv_kag as columns in the Xkag dataframe\n        # and write it out, label filename with the regressor.\n        Xkag['yp_'+regressor_name] = yp_kag\n        Xkag['ypv_'+regressor_name] = ypv_kag\n        # Write it out:\n        Xkag.to_csv(out_dir+\"/Xys_Kag_\"+regressor_name+\"_\"+version_str+\".csv\")\n        # Remove the added columns:\n        Xkag = Xkag.drop(['yp_'+regressor_name,'ypv_'+regressor_name],axis=1)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a983a395b7238e41079baa6bf2d002a24b0d8862","trusted":true},"cell_type":"code","source":"# that's all.\n!head -10 submission.csv","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d47ef61e9fff81db1deb6d67f684a41cbb9124c6","trusted":true},"cell_type":"code","source":"!tail -10 submission.csv","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"38f18e66b6dc73175f6035d7f811d2daa3bd3df9","trusted":true},"cell_type":"code","source":"RANDOM_SEED","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f6744561f78b14a4e5da2c0ff1def85d5c9a590c","collapsed":true},"cell_type":"markdown","source":"## <a id=\"TheEnd\">The End</a>\nBack to <a href=\"#Index\">Index</a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.8"}},"nbformat":4,"nbformat_minor":1}