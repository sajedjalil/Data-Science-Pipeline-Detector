{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"collapsed":true},"cell_type":"markdown","source":"![](http://)<H1>Problem Understanding</H1>\n\nThe YouTube8M challenge is a multi-class classification problem, where we are asked to predict for each video, given video & frame level audio and frame RGB features, to which group of categories it belongs to.\n\nI have divided entire task into two parts\n\n1. Simple Data Exploration,  Labels/classes study of sample videos. \n2. Created a Bi-LSTM multilabel neural model by randomly created sample data.\n\nLets first explore the labels for the training data, their distribution and frequent patterns and co-occurance of the most frequent label categories.\n\n**Since we have been given sample dataset  here, so all my exploration will be done on sample data, we can do the same anlaysis on large corpus using GCloud ML Engine**"},{"metadata":{"trusted":true,"_uuid":"80e7b21b56d4b092b4fb2a33088cddc8a099586f","collapsed":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport tensorflow as tf\nimport seaborn as sns\nfrom IPython.display import YouTubeVideo\nimport matplotlib.pyplot as plt\nimport plotly.plotly as py\n\nimport os\nprint(os.listdir(\"../input\"))\n# video level feature file\nprint(os.listdir(\"../input/video\"))\n# frame level features file\nprint(os.listdir(\"../input/frame\"))\n","execution_count":2,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":false},"cell_type":"markdown","source":"<H3>File descriptions</H3>\n1. ** labels_names_2018.csv**:  a mapping between label_id and label_name <br>\n\n1. **vocabularu.csv:** :  the full data dictionary for label names and their descriptions <br>\n\n1. ** video (video-level data):**  contains video level info for each video, these files are in TFRecords format, will explore it later let us first explore what features it has:<br>\n    a. `id`: unique id for the video, in train set it is a Youtube video id, and in test/validation they are anonymized<br>\n    b. `labels`: list of labels of that video<br>\n    c. `mean_rgb`: float array of length 1024<br>\n    d. `mean_audio`: float array of length 128<br>\n\n1. **frame (frame-level data) :** contains frame level info for each video, again files are given in TFRecords format, lets see features <br>\n    a. `id`: unique id for the video, in train set it is a YouTube video id, and in test/validation they are anonymized.<br>\n    b. `labels`: list of labels of that video. <br>\n    c. `rgb`: Each frame has float array of length 1024,<br>\n    d. `audio`: Each frame has float array of length 128<br>\n \n1. **sample_submission.csv ** a sample submission file in the correct format, each row has <br>\n   a. `VideoId` - the id of the video<br>\n   b. `LabelConfidencePairs`: space delimited label/prediction pairs ordered by descending confidence\n"},{"metadata":{"_uuid":"e91327d134d0f6331f48041f6b1ed633322d3fa2"},"cell_type":"markdown","source":"<H2>EDA </H2>"},{"metadata":{"_uuid":"10b5be2b9ebd2f0b4d20e31b81cdb75fb81b2a9b"},"cell_type":"markdown","source":"<h3>Let us first explore labels and their distributions</h3>"},{"metadata":{"trusted":true,"_uuid":"d196e6a37229117a27a81af4be759844c78dfc1d","collapsed":true},"cell_type":"code","source":"# total number of labels\nlabels_df = pd.read_csv('../input/label_names_2018.csv')\nprint(labels_df.head())\nprint(\"Total nubers of labels in sample dataset: %s\" %(len(labels_df['label_name'].unique())))","execution_count":3,"outputs":[]},{"metadata":{"_uuid":"dd3c3bfb7ef6ad0c8bf3917593402769077f40a7"},"cell_type":"markdown","source":"<h4>Exploring video level data</h4>"},{"metadata":{"trusted":true,"_uuid":"0d507c57cc551ae03684fa0b26f5531ae00b797f","collapsed":true},"cell_type":"code","source":"# distribution of labels\nvideo_files = [\"../input/video/{}\".format(i) for i in os.listdir(\"../input/video\")]\nprint(video_files)\n\nvid_ids = []\nlabels = []\nmean_rgb = []\nmean_audio = []\n\nfor file in video_files:\n    for example in tf.python_io.tf_record_iterator(file):\n        tf_example = tf.train.Example.FromString(example)\n\n        vid_ids.append(tf_example.features.feature['id'].bytes_list.value[0].decode(encoding='UTF-8'))\n        labels.append(tf_example.features.feature['labels'].int64_list.value)\n        mean_rgb.append(tf_example.features.feature['mean_rgb'].float_list.value)\n        mean_audio.append(tf_example.features.feature['mean_audio'].float_list.value)\n\nprint('Number of videos in Sample data set: %s' % str(len(vid_ids)))\nprint('Picking a youtube video id: %s' % vid_ids[13])\nprint('List of label ids for youtube video id %s, are - %s' % (vid_ids[13], str(labels[13])))\nprint('First 20 rgb feature of a youtube video (',vid_ids[13],'): are - %s' % str(mean_rgb[13][:20]))\n","execution_count":4,"outputs":[]},{"metadata":{"_uuid":"a65e9ea1d9e60c1f427b52c9ef8907a31715cd37"},"cell_type":"markdown","source":"> <h2>Lets have a look at the most common labels and the relation among thems </h2>"},{"metadata":{"trusted":true,"_uuid":"15625995cbdf2c522ab7fc17fe2883347b79d798","collapsed":true},"cell_type":"code","source":"# Lets convert labels for each video into their respective names\nlabels_name = []\nfor row in labels:\n    n_labels = []\n    for label_id in row:\n        # some labels ids are missing so have put try/except\n        try:\n            n_labels.append(str(labels_df[labels_df['label_id']==label_id]['label_name'].values[0]))\n        except:\n            continue\n    labels_name.append(n_labels)\n\nprint('List of label names for youtube video id %s, are - %s' % (vid_ids[13], str(labels_name[13])))","execution_count":5,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a512145ae5dd1ce3158d40aec5e2c3ddaeeb9553","collapsed":true},"cell_type":"code","source":"# creating labels count dictionary\nfrom collections import Counter\nimport operator\n\nall_labels = []\nfor each in labels_name:\n    all_labels.extend(each)\n\nlabels_count_dict = dict(Counter(all_labels))","execution_count":6,"outputs":[]},{"metadata":{"_uuid":"9d6b4a168db3da6ef4efa2f85136684c80b91b11"},"cell_type":"markdown","source":"Lets have a look at distribution of top 25 labels"},{"metadata":{"trusted":true,"_uuid":"a6e5e08fdbe224a958d24bf334017be06bc6785b","collapsed":true},"cell_type":"code","source":"# creating label count dataframe\nlabels_count_df = pd.DataFrame.from_dict(labels_count_dict, orient='index').reset_index()\nlabels_count_df.columns = ['label', 'count']\nsorted_labels_count_df = labels_count_df.sort_values('count', ascending=False)\n\n# plotting top 25 labels distribution\nTOP = 25\nTOP_labels = list(sorted_labels_count_df['label'])[:TOP]\nfig, ax = plt.subplots(figsize=(10,7))\nsns.barplot(y='label', x='count', data=sorted_labels_count_df.iloc[0:TOP, :])\nplt.title('Top {} labels with sample count'.format(TOP))\n","execution_count":7,"outputs":[]},{"metadata":{"_uuid":"9c649e544256608eabe649ed8a560336697930ad"},"cell_type":"markdown","source":"Lets explore most common occuring labels with these top 25 labels."},{"metadata":{"trusted":true,"_uuid":"bb97d50fe72d50777d08dc352c7e09ccb2a7e191","collapsed":true},"cell_type":"code","source":"# creating common occurs labels count dict\ncommon_occur_top_label_dict = {}\nfor row in labels_name:\n    for label in row:\n        if label in TOP_labels:\n            c_labels = [label + \"|\" + x for x in row if x != label]\n            for c_label in c_labels: \n                common_occur_top_label_dict[c_label] = common_occur_top_label_dict.get(c_label, 0) + 1\n\n# creating dataframe\ncommon_occur_top_label_df = pd.DataFrame.from_dict(common_occur_top_label_dict, orient='index').reset_index()\ncommon_occur_top_label_df.columns = ['common_label', 'count']\nsorted_common_occur_top_label_df = common_occur_top_label_df.sort_values('count', ascending=False)\n\n\n# plotting 25 common occurs labels from top labels\nTOP = 25\nfig, ax = plt.subplots(figsize=(10,7))\nsns.barplot(y='common_label', x='count', data=sorted_common_occur_top_label_df.iloc[0:TOP, :])\nplt.title('Top {} common occur labels with sample count'.format(TOP))\n","execution_count":8,"outputs":[]},{"metadata":{"_uuid":"5c82f70a5beca91ae31d3698ebed0bf7dcd45645"},"cell_type":"markdown","source":"This shows game and vehicle are most commonly occurs labels among sample youtube videos"},{"metadata":{"_uuid":"928167409a44bcfdd0b3285c6f240400e4cc5768"},"cell_type":"markdown","source":"<H2> Create Network Graph For Top Labels</H2>"},{"metadata":{"trusted":true,"_uuid":"bf64d437d1308ce8dae92cae14fcee3337160300","collapsed":true},"cell_type":"code","source":"# libraries\nimport pandas as pd\nimport numpy as np\n \ntop_cooccurance_label_dict = {}\nfor row in labels_name:\n    for label in row:\n        if label in TOP_labels:\n            top_label_siblings = [x for x in row if x != label]\n            for sibling in top_label_siblings:\n                if label not in top_cooccurance_label_dict:\n                    top_cooccurance_label_dict[label] = {}\n                top_cooccurance_label_dict[label][sibling] = top_cooccurance_label_dict.get(label, {}).get(sibling, 0) + 1\n\nfrom_label= []\nto_label = []\nvalue = []\nfor key, val in top_cooccurance_label_dict.items():\n    for key2, val2 in val.items():\n        from_label.append(key)\n        to_label.append(key2)\n        value.append(val2)\n\ndf = pd.DataFrame({ 'from': from_label, 'to': to_label, 'value': value})\nsorted_df = df.sort_values('value', ascending=False)\nsorted_df = sorted_df.iloc[:50, ]","execution_count":9,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"b0dc07e28be77b9b720ad96d8028c6306bde6bb3"},"cell_type":"code","source":"node_colors = ['turquoise', 'turquoise', 'green', 'crimson', 'grey', 'turquoise', 'turquoise', \n'grey', 'skyblue', 'crimson', 'yellow', 'green', 'turquoise', \n'skyblue', 'skyblue', 'green', 'green', 'lightcoral', 'grey', 'yellow', \n'turquoise', 'skyblue', 'orange', 'green', 'skyblue', 'green', 'turquoise', \n'lightcoral', 'yellow', 'lightcoral', 'green', 'turquoise', 'lightcoral', 'turquoise', \n'yellow', 'orange', 'lightcoral', 'grey', 'green', 'orange', 'crimson', \n'skyblue', 'lightcoral', 'lightcoral', 'skyblue', 'crimson', 'yellow', 'yellow', 'lightcoral', \n'yellow']\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7016240deba8e3dc84e762bcf9ce09ec928c97e2","collapsed":true},"cell_type":"code","source":"import networkx as nx\nimport matplotlib.pyplot as plt\n \ndf = sorted_df\n# Build your graph\nG=nx.from_pandas_dataframe(df, 'from', 'to', 'value', create_using=nx.Graph() )\nplt.figure(figsize = (10,10))\nnx.draw(G, pos=nx.circular_layout(G), node_size=1000, with_labels=True, node_color=node_colors)\nnx.draw_networkx_edge_labels(G, pos=nx.circular_layout(G), edge_labels=nx.get_edge_attributes(G, 'value'))\nplt.title('Network graph representing the co-occurance between the categories', size=20)\nplt.show()","execution_count":19,"outputs":[]},{"metadata":{"_uuid":"036031f1c43003361ecee12b023e30eff085dbcd"},"cell_type":"markdown","source":"\n<H3> let's  Explore frame-level data for videos </H3>"},{"metadata":{"trusted":true,"_uuid":"ce07fd19af2c6514c76faa0c4961d0e725b056e1","collapsed":true},"cell_type":"code","source":"frame_files = [\"../input/frame/{}\".format(i) for i in os.listdir(\"../input/frame\")]\nfeat_rgb = []\nfeat_audio = []\n\nfor file in frame_files:\n    for example in tf.python_io.tf_record_iterator(file):        \n        tf_seq_example = tf.train.SequenceExample.FromString(example)\n        n_frames = len(tf_seq_example.feature_lists.feature_list['audio'].feature)\n        sess = tf.InteractiveSession()\n        rgb_frame = []\n        audio_frame = []\n        # iterate through frames\n        for i in range(n_frames):\n            rgb_frame.append(tf.cast(tf.decode_raw(\n                    tf_seq_example.feature_lists.feature_list['rgb'].feature[i].bytes_list.value[0],tf.uint8)\n                           ,tf.float32).eval())\n            audio_frame.append(tf.cast(tf.decode_raw(\n                    tf_seq_example.feature_lists.feature_list['audio'].feature[i].bytes_list.value[0],tf.uint8)\n                           ,tf.float32).eval())\n\n\n        sess.close()\n        feat_rgb.append(rgb_frame)\n        feat_audio.append(audio_frame)\n        break","execution_count":20,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"de2961a2d440d8a189acea5ae38cd55af100335e","collapsed":true},"cell_type":"code","source":"print(\"No. of videos %d\" % len(feat_rgb))\nprint('The first video has %d frames' %len(feat_rgb[0]))\nprint(\"Max frame length is: %d\" % max([len(x) for x in feat_rgb]))","execution_count":217,"outputs":[]},{"metadata":{"_uuid":"3b0a1b63ecc438e66390efdd6cb968a261592189"},"cell_type":"markdown","source":"<H1> Bi-LSTM Multilabel classification </H1>"},{"metadata":{"_uuid":"ea8db223a71186e0f315b871cbd342712afb5a4c"},"cell_type":"markdown","source":"**Here we will be using deep learning model with below architecture, since frames are sequence data, we will be utilising bi-directional lstm to learn this frame data and merge their ourput with video level data which later will pass through output sigmoid layer with units equal to no. of features**"},{"metadata":{"_uuid":"9db209cc501c25d96e99de193e54f6a23847dc6f"},"cell_type":"markdown","source":"**Link Diagram ** - https://drive.google.com/file/d/1mGnPBya9eyKj0ZP6a4GUuFVXSr7pRw4j/view?usp=sharing"},{"metadata":{"trusted":true,"_uuid":"ed6c45f1f005c2e5efd06831a7686e85f956195e","collapsed":true},"cell_type":"code","source":"# keras imports\nfrom keras.layers import Dense, Input, LSTM, Dropout, Bidirectional\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.layers.embeddings import Embedding\nfrom keras.layers.merge import concatenate\nfrom keras.callbacks import TensorBoard\nfrom keras.models import load_model\nfrom keras.models import Model\nimport operator\nimport time \nimport gc\nimport os","execution_count":15,"outputs":[]},{"metadata":{"_uuid":"542288144f1803ba0718012c29c0be398299859d"},"cell_type":"markdown","source":"**creating training and dev set**"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"e2be53fb9bc19ef58bda8f21ea91b1ca8527969b"},"cell_type":"code","source":"def create_train_dev_dataset(video_rgb, video_audio, frame_rgb, frame_audio, labels):\n    \"\"\"\n    Method to created training and validation data\n    \"\"\"\n    shuffle_indices = np.random.permutation(np.arange(len(labels)))\n    video_rgb_shuffled = video_rgb[shuffle_indices]\n    video_audio_shuffled = video_audio[shuffle_indices]\n    frame_rgb_shuffled = frame_rgb[shuffle_indices]\n    frame_audio_shuffled = frame_audio[shuffle_indices]\n    labels_shuffled = labels[shuffle_indices]\n\n    dev_idx = max(1, int(len(labels_shuffled) * validation_split_ratio))\n\n    del video_rgb\n    del video_audio\n    del frame_rgb\n    del frame_audio\n    gc.collect()\n\n    train_video_rgb, val_video_rgb = video_rgb_shuffled[:-dev_idx], video_rgb_shuffled[-dev_idx:]\n    train_video_audio, val_video_audio = video_audio_shuffled[:-dev_idx], video_audio_shuffled[-dev_idx:]\n    \n    train_frame_rgb, val_frame_rgb = frame_rgb_shuffled[:-dev_idx], frame_rgb_shuffled[-dev_idx:]\n    train_frame_audio, val_frame_audio = frame_audio_shuffled[:-dev_idx], frame_audio_shuffled[-dev_idx:]\n    \n    train_labels, val_labels = labels_shuffled[:-dev_idx], labels_shuffled[-dev_idx:]\n    \n    del video_rgb_shuffled, video_audio_shuffled, frame_rgb_shuffled, frame_audio_shuffled, labels_shuffled\n    gc.collect()\n    \n    return (train_video_rgb, train_video_audio, train_frame_rgb, train_frame_audio, train_labels, val_video_rgb, val_video_audio, \n            val_frame_rgb, val_frame_audio, val_labels)\n    ","execution_count":16,"outputs":[]},{"metadata":{"_uuid":"78dabc3ef7258e578d8055a5683dd73663ebe1bf"},"cell_type":"markdown","source":"**Defining Model parameters and creating architecture**"},{"metadata":{"trusted":true,"_uuid":"32425df0c0f2215486c88f68e6a617b241c44c94","collapsed":true},"cell_type":"code","source":"max_frame_rgb_sequence_length = 10\nframe_rgb_embedding_size = 1024\n\nmax_frame_audio_sequence_length = 10\nframe_audio_embedding_size = 128\n\nnumber_dense_units = 1000\nnumber_lstm_units = 100\nrate_drop_lstm = 0.2\nrate_drop_dense = 0.2\nactivation_function='relu'\nvalidation_split_ratio = 0.2\nlabel_feature_size = 10\n\ndef create_model(video_rgb, video_audio, frame_rgb, frame_audio, labels):\n    \"\"\"Create and store best model at `checkpoint` path ustilising bi-lstm layer for frame level data of videos\"\"\"\n    train_video_rgb, train_video_audio, train_frame_rgb, train_frame_audio, train_labels, val_video_rgb, val_video_audio, val_frame_rgb, val_frame_audio, val_labels = create_train_dev_dataset(video_rgb, video_audio, frame_rgb, frame_audio, labels) \n    \n    # Creating 2 bi-lstm layer, one for rgb and other for audio level data\n    lstm_layer_1 = Bidirectional(LSTM(number_lstm_units, dropout=rate_drop_lstm, recurrent_dropout=rate_drop_lstm))\n    lstm_layer_2 = Bidirectional(LSTM(number_lstm_units, dropout=rate_drop_lstm, recurrent_dropout=rate_drop_lstm))\n    \n    # creating input layer for frame-level data\n    frame_rgb_sequence_input = Input(shape=(max_frame_rgb_sequence_length, frame_rgb_embedding_size), dtype='float32')\n    frame_audio_sequence_input = Input(shape=(max_frame_audio_sequence_length, frame_audio_embedding_size), dtype='float32')\n    \n    frame_x1 = lstm_layer_1(frame_rgb_sequence_input)\n    frame_x2 = lstm_layer_2(frame_audio_sequence_input)\n    \n    # creating input layer for video-level data\n    video_rgb_input = Input(shape=(video_rgb.shape[1],))\n    video_rgb_dense = Dense(int(number_dense_units/2), activation=activation_function)(video_rgb_input)\n    \n    video_audio_input = Input(shape=(video_audio.shape[1],))\n    video_audio_dense = Dense(int(number_dense_units/2), activation=activation_function)(video_audio_input)\n    \n    # merging frame-level bi-lstm output and later passed to dense layer by applying batch-normalisation and dropout\n    merged_frame = concatenate([frame_x1, frame_x2])\n    merged_frame = BatchNormalization()(merged_frame)\n    merged_frame = Dropout(rate_drop_dense)(merged_frame)\n    merged_frame_dense = Dense(int(number_dense_units/2), activation=activation_function)(merged_frame)\n    \n    # merging video-level dense layer output\n    merged_video = concatenate([video_rgb_dense, video_audio_dense])\n    merged_video = BatchNormalization()(merged_video)\n    merged_video = Dropout(rate_drop_dense)(merged_video)\n    merged_video_dense = Dense(int(number_dense_units/2), activation=activation_function)(merged_video)\n    \n    # merging frame-level and video-level dense layer output\n    merged = concatenate([merged_frame_dense, merged_video_dense])\n    merged = BatchNormalization()(merged)\n    merged = Dropout(rate_drop_dense)(merged)\n     \n    merged = Dense(number_dense_units, activation=activation_function)(merged)\n    merged = BatchNormalization()(merged)\n    merged = Dropout(rate_drop_dense)(merged)\n    preds = Dense(label_feature_size, activation='sigmoid')(merged)\n    \n    model = Model(inputs=[frame_rgb_sequence_input, frame_audio_sequence_input, video_rgb_input, video_audio_input], outputs=preds)\n    print(model.summary())\n    model.compile(loss='binary_crossentropy', optimizer='nadam', metrics=['acc'])\n    early_stopping = EarlyStopping(monitor='val_loss', patience=3)\n    \n    STAMP = 'lstm_%d_%d_%.2f_%.2f' % (number_lstm_units, number_dense_units, rate_drop_lstm, rate_drop_dense)\n\n    checkpoint_dir = 'checkpoints/' + str(int(time.time())) + '/'\n\n    if not os.path.exists(checkpoint_dir):\n        os.makedirs(checkpoint_dir)\n\n    bst_model_path = checkpoint_dir + STAMP + '.h5'\n    model_checkpoint = ModelCheckpoint(bst_model_path, save_best_only=True, save_weights_only=False)\n    tensorboard = TensorBoard(log_dir=checkpoint_dir + \"logs/{}\".format(time.time()))\n    \n    model.fit([train_frame_rgb, train_frame_audio, train_video_rgb, train_video_audio], train_labels,\n              validation_data=([val_frame_rgb, val_frame_audio, val_video_rgb, val_video_audio], val_labels),\n              epochs=200, batch_size=64, shuffle=True, callbacks=[early_stopping, model_checkpoint, tensorboard])    \n    return model","execution_count":17,"outputs":[]},{"metadata":{"_uuid":"1a622af9d8d296b645f174d19552e10028572a49"},"cell_type":"markdown","source":"**Creating random data set for training **\n\n![](http://)Here I am creating a sample dataset of same size and dimension of training sample and will train the model"},{"metadata":{"trusted":true,"_uuid":"ce8e12fcb40db9e4408597f47dffba880d7c2178","collapsed":true},"cell_type":"code","source":"import numpy as np\nimport random\n\nsample_length = 1000\n\nvideo_rgb = np.random.rand(sample_length, 1024)\nvideo_audio = np.random.rand(sample_length, 128)\n\nframe_rgb = np.random.rand(sample_length, 10, 1024)\nframe_audio = np.random.rand(sample_length, 10, 128)\n\n# Here I have considered i have only 10 labels\nlabels = np.zeros([sample_length,10])\nfor i in range(len(labels)):\n    j = random.randint(0,9)\n    labels[i][j] = 1 ","execution_count":18,"outputs":[]},{"metadata":{"_uuid":"036835fe69115df1cd1ba8b58d74712948b0c362"},"cell_type":"markdown","source":"<H2> Training Model </H2>"},{"metadata":{"trusted":true,"_uuid":"17570cf3fda9fc665d90f93a8a7856edda3ba3f9","collapsed":true},"cell_type":"code","source":"model = create_model(video_rgb, video_audio, frame_rgb, frame_audio, labels)","execution_count":6,"outputs":[]},{"metadata":{"_uuid":"30e6fb23365f900e80940091c9e0f8aa2c8aaeca"},"cell_type":"markdown","source":"<H3>Testing with created random test data</H3>"},{"metadata":{"trusted":true,"_uuid":"1fb19d21cbadbc5a15706fc388ea71cbb624915b","collapsed":true},"cell_type":"code","source":"test_video_rgb = np.random.rand(1, 1024)\ntest_video_audio = np.random.rand(1, 128)\n\ntest_frame_rgb = np.random.rand(1, 10, 1024)\ntest_frame_audio = np.random.rand(1, 10, 128)\n\npreds = list(model.predict([test_frame_rgb, test_frame_audio, test_video_rgb, test_video_audio], verbose=1).ravel())\nindex, value = max(enumerate(preds), key=operator.itemgetter(1))\nprint(\"Predicted Label - %s with probability - %s\" % (str(index), str(value)))","execution_count":19,"outputs":[]},{"metadata":{"_uuid":"0f757e254b92eeff5d80e4bf6f3529d7340da5c5"},"cell_type":"markdown","source":"<H1> Thanks !! Hopes that help </H1>"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"98000b065485a0d7ffa880df4579e278db26ebd4"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.5","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}