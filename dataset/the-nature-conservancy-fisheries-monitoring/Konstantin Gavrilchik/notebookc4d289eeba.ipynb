{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"286d52fd-be35-2016-a541-458a8d8be1e6"},"source":""},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c4cc3f0a-6054-46df-ad0b-53aeb446c6a4"},"outputs":[],"source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output.\n\nimport numpy as np\nnp.random.seed(1984)\n\nimport os\nimport glob\nimport cv2\nimport datetime\nimport pandas as pd\nimport time\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nfrom sklearn.cross_validation import KFold\nfrom keras.models import Sequential\nfrom keras.layers.core import Dense, Dropout, Flatten\nfrom keras.layers.convolutional import Convolution2D, MaxPooling2D, ZeroPadding2D, AveragePooling2D\nfrom keras.optimizers import SGD, Adagrad\nfrom keras.callbacks import EarlyStopping\nfrom keras.utils import np_utils\nfrom keras.constraints import maxnorm\nfrom sklearn.metrics import log_loss\nfrom keras import __version__ as keras_version\n\n\n\ndef get_im_cv2(path):\n    img = cv2.imread(path)\n    resized = cv2.resize(img, (64, 64), cv2.INTER_LINEAR)\n    return resized\n\n\ndef load_train():\n    X_train = []\n    X_train_id = []\n    y_train = []\n    start_time = time.time()\n\n    print('Read train images')\n    folders = ['ALB', 'BET', 'DOL', 'LAG', 'NoF', 'OTHER', 'SHARK', 'YFT']\n    for fld in folders:\n        index = folders.index(fld)\n        print('Load folder {} (Index: {})'.format(fld, index))\n        path = os.path.join('..', 'input', 'train', fld, '*.jpg')\n        files = glob.glob(path)\n        for fl in files:\n            flbase = os.path.basename(fl)\n            img = get_im_cv2(fl)\n            X_train.append(img)\n            X_train_id.append(flbase)\n            y_train.append(index)\n\n    print('Read train data time: {} seconds'.format(round(time.time() - start_time, 2)))\n    return X_train, y_train, X_train_id\n\n\ndef load_test():\n    path = os.path.join('..', 'input', 'test_stg1', '*.jpg')\n    files = sorted(glob.glob(path))\n\n    X_test = []\n    X_test_id = []\n    for fl in files:\n        flbase = os.path.basename(fl)\n        img = get_im_cv2(fl)\n        X_test.append(img)\n        X_test_id.append(flbase)\n\n    return X_test, X_test_id\n\n\ndef create_submission(predictions, test_id, info):\n    result1 = pd.DataFrame(predictions, columns=['ALB', 'BET', 'DOL', 'LAG', 'NoF', 'OTHER', 'SHARK', 'YFT'])\n    result1.loc[:, 'image'] = pd.Series(test_id, index=result1.index)\n    now = datetime.datetime.now()\n    sub_file = 'submission_' + info + '_' + str(now.strftime(\"%Y-%m-%d-%H-%M\")) + '.csv'\n    result1.to_csv(sub_file, index=False)\n\n\ndef read_and_normalize_train_data():\n    train_data, train_target, train_id = load_train()\n\n    print('Convert to numpy...')\n    train_data = np.array(train_data, dtype=np.uint8)\n    train_target = np.array(train_target, dtype=np.uint8)\n\n    print('Reshape...')\n    train_data = train_data.transpose((0, 3, 1, 2))\n\n    print('Convert to float...')\n    train_data = train_data.astype('float32')\n    train_data = train_data / 255\n    train_target = np_utils.to_categorical(train_target, 8)\n\n    print('Train shape:', train_data.shape)\n    print(train_data.shape[0], 'train samples')\n    return train_data, train_target, train_id\n\n\ndef read_and_normalize_test_data():\n    start_time = time.time()\n    test_data, test_id = load_test()\n\n    test_data = np.array(test_data, dtype=np.uint8)\n    test_data = test_data.transpose((0, 3, 1, 2))\n\n    test_data = test_data.astype('float32')\n    test_data = test_data / 255\n\n    print('Test shape:', test_data.shape)\n    print(test_data.shape[0], 'test samples')\n    print('Read and process test data time: {} seconds'.format(round(time.time() - start_time, 2)))\n    return test_data, test_id\n\n\ndef dict_to_list(d):\n    ret = []\n    for i in d.items():\n        ret.append(i[1])\n    return ret\n\n\ndef merge_several_folds_mean(data, nfolds):\n    a = np.array(data[0])\n    for i in range(1, nfolds):\n        a += np.array(data[i])\n    a /= nfolds\n    return a.tolist()\n\n\ndef create_model():\n    model = Sequential()\n    model.add(ZeroPadding2D((1, 1), input_shape=(3, 64, 64), dim_ordering='th'))\n    model.add(Convolution2D(8, 3, 3, activation='relu', dim_ordering='th', init='he_uniform'))\n    model.add(Dropout(0.2))\n    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2), dim_ordering='th'))\n    model.add(ZeroPadding2D((1, 1), dim_ordering='th'))\n    model.add(Convolution2D(16, 3, 3, activation='relu', dim_ordering='th', init='he_uniform'))\n    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2), dim_ordering='th'))\n    model.add(Dropout(0.2))\n    \n    model.add(Flatten())\n    model.add(Dense(96, activation='relu',init='he_uniform'))\n    model.add(Dropout(0.4))\n    model.add(Dense(24, activation='relu',init='he_uniform'))\n    model.add(Dropout(0.2))\n    model.add(Dense(8, activation='softmax'))\n\n    #sgd = SGD(lr=1e-2, decay=1e-4, momentum=0.89, nesterov=False)\n    model.compile(optimizer='adam', loss='categorical_crossentropy')\n\n    return model\n\n\ndef get_validation_predictions(train_data, predictions_valid):\n    pv = []\n    for i in range(len(train_data)):\n        pv.append(predictions_valid[i])\n    return pv\n\n\ndef run_cross_validation_create_models(nfolds=10):\n    # input image dimensions\n    batch_size = 32\n    nb_epoch = 8\n    random_state = 51\n    first_rl = 96\n\n    train_data, train_target, train_id = read_and_normalize_train_data()\n\n    yfull_train = dict()\n    kf = KFold(len(train_id), n_folds=nfolds, shuffle=True, random_state=random_state)\n    num_fold = 0\n    sum_score = 0\n    models = []\n    for train_index, test_index in kf:\n        model = create_model()\n        X_train = train_data[train_index]\n        Y_train = train_target[train_index]\n        X_valid = train_data[test_index]\n        Y_valid = train_target[test_index]\n\n        num_fold += 1\n        print('Start KFold number {} from {}'.format(num_fold, nfolds))\n        print('Split train: ', len(X_train), len(Y_train))\n        print('Split valid: ', len(X_valid), len(Y_valid))\n\n        callbacks = [\n            EarlyStopping(monitor='val_loss', patience=3, verbose=0),\n        ]\n        model.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=nb_epoch,\n              shuffle=True, verbose=2, validation_data=(X_valid, Y_valid),\n              callbacks=callbacks)\n\n        predictions_valid = model.predict(X_valid.astype('float32'), batch_size=batch_size, verbose=2)\n        score = log_loss(Y_valid, predictions_valid)\n        print('Score log_loss: ', score)\n        sum_score += score*len(test_index)\n\n        # Store valid predictions\n        for i in range(len(test_index)):\n            yfull_train[test_index[i]] = predictions_valid[i]\n\n        models.append(model)\n\n    score = sum_score/len(train_data)\n    print(\"Log_loss train independent avg: \", score)\n\n    info_string = '_' + str(np.round(score,3)) + '_flds_' + str(nfolds) + '_eps_' + str(nb_epoch) + '_fl_' + str(first_rl)\n    return info_string, models\n\n\ndef run_cross_validation_process_test(info_string, models):\n    batch_size = 24\n    num_fold = 0\n    yfull_test = []\n    test_id = []\n    nfolds = len(models)\n\n    for i in range(nfolds):\n        model = models[i]\n        num_fold += 1\n        print('Start KFold number {} from {}'.format(num_fold, nfolds))\n        test_data, test_id = read_and_normalize_test_data()\n        test_prediction = model.predict(test_data, batch_size=batch_size, verbose=2)\n        yfull_test.append(test_prediction)\n\n    test_res = merge_several_folds_mean(yfull_test, nfolds)\n    info_string = 'loss_' + info_string \\\n                + '_folds_' + str(nfolds)\n    create_submission(test_res, test_id, info_string)\n"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d6726657-5e75-9192-c94e-6a93872a4faa"},"outputs":[],"source":"if __name__ == '__main__':\n    print('Keras version: {}'.format(keras_version))\n    num_folds = 3\n    info_string, models = run_cross_validation_create_models(num_folds)\n    run_cross_validation_process_test(info_string, models)\n"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"25135aa7-e7b5-7dd5-dac3-4399a028f7ab"},"outputs":[],"source":"pd.read_csv('submission_loss__0.596_flds_3_eps_8_fl_96_folds_3_2017-03-03-10-32.csv')"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b0dba140-6cf8-14e9-3f2c-f4f16f72d863"},"outputs":[],"source":"pd.set_option('display.max_rows', len(data))\ndata = pd.read_csv('submission_loss__0.596_flds_3_eps_8_fl_96_folds_3_2017-03-03-10-32.csv')"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c01bd609-73da-e664-7b35-5872ce36bea2"},"outputs":[],"source":"data"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"facc16a8-d1c6-3061-e48b-cf4f19fa665a"},"outputs":[],"source":""}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":0}