{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"344dd1d4-fd11-8f03-417b-0195ce44b12e"},"outputs":[],"source":"#!/usr/bin/env python\n\n\"\"\"image_classification.py: Classify images to classify fish types\"\"\"\n\nimport os\nimport glob\nimport joblib\nimport cv2\nimport numpy as np\nfrom scipy.cluster import vq\n\nimport pandas as pd\n\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.preprocessing import StandardScaler\n\n\n__author__ = \"Pradeep Kumar A.V.\"\n\n\nCLASSES = {\n    'ALB': 1,\n    'BET': 2,\n    'DOL': 3,\n    'LAG': 4,\n    'NoF': 5,\n    'OTHER': 6,\n    'SHARK': 7,\n    'YFT': 8\n}\n\nCLASSES_REV = {value: key for key, value in CLASSES.items()}\n\n\nclass Saliency(object):\n    \"\"\"Generate saliency map from RGB images with the spectral residual method\n\n        This class implements an algorithm that is based on the spectral\n        residual approach (Hou & Zhang, 2007).\n    \"\"\"\n    def __init__(self, img, use_numpy_fft=True, gauss_kernel=(3, 3)):\n        \"\"\"Constructor\n\n            This method initializes the saliency algorithm.\n\n            :param img: an RGB input image\n            :param use_numpy_fft: flag whether to use NumPy's FFT (True) or\n                                  OpenCV's FFT (False)\n            :param gauss_kernel: Kernel size for Gaussian blur\n        \"\"\"\n        self.use_numpy_fft = use_numpy_fft\n        self.gauss_kernel = gauss_kernel\n        self.frame_orig = self._enhance_image(img)\n\n        # downsample image for processing\n        self.small_shape = (24, 24)\n        self.frame_small = cv2.resize(img, self.small_shape[1::-1])\n\n    @staticmethod\n    def _enhance_image(img):\n        \"\"\"\n        :param img: RGB color image\n        :return: enhanced image\n        \"\"\"\n        img_yuv = cv2.cvtColor(img, cv2.COLOR_RGB2YUV)\n\n        # equalize the histogram of the Y channel\n        img_yuv[:, :, 0] = cv2.equalizeHist(img_yuv[:, :, 0])\n\n        # convert the YUV image back to RGB format\n        img_output = cv2.cvtColor(img_yuv, cv2.COLOR_YUV2BGR)\n        return img_output\n\n    def get_saliency_map(self):\n        \"\"\"Returns a saliency map\n\n            This method generates a saliency map for the image that was\n            passed to the class constructor.\n\n            :returns: grayscale saliency map\n        \"\"\"\n        # haven't calculated saliency map for this image yet\n        # multiple channels: consider each channel independently\n        sal = np.zeros_like(self.frame_small).astype(np.float32)\n        for c in xrange(self.frame_small.shape[2]):\n            small = self.frame_small[:, :, c]\n            sal[:, :, c] = self._get_channel_sal_magn(small)\n\n        # overall saliency: channel mean\n        sal = np.mean(sal, 2)\n\n        # postprocess: blur, square, and normalize\n        if self.gauss_kernel is not None:\n            sal = cv2.GaussianBlur(sal, self.gauss_kernel, sigmaX=1,\n                                   sigmaY=0)\n        sal **= 2\n        sal = np.float32(sal)/np.max(sal)\n\n        # scale up\n        sal = cv2.resize(sal, self.frame_orig.shape[1::-1])\n\n        return sal\n\n    def _get_channel_sal_magn(self, channel):\n        \"\"\"Returns the log-magnitude of the Fourier spectrum\n\n            This method calculates the log-magnitude of the Fourier spectrum\n            of a single-channel image. This image could be a regular grayscale\n            image, or a single color channel of an RGB image.\n\n            :param channel: single-channel input image\n            :returns: log-magnitude of Fourier spectrum\n        \"\"\"\n        # do FFT and get log-spectrum\n        if self.use_numpy_fft:\n            img_dft = np.fft.fft2(channel)\n            magnitude, angle = cv2.cartToPolar(np.real(img_dft),\n                                               np.imag(img_dft))\n        else:\n            img_dft = cv2.dft(np.float32(channel),\n                              flags=cv2.DFT_COMPLEX_OUTPUT)\n            magnitude, angle = cv2.cartToPolar(img_dft[:, :, 0],\n                                               img_dft[:, :, 1])\n\n        # get log amplitude\n        log_ampl = np.log10(magnitude.clip(min=1e-9))\n\n        # blur log amplitude with avg filter\n        log_ampl_blur = cv2.blur(log_ampl, (3, 3))\n\n        # residual\n        residual = np.exp(log_ampl - log_ampl_blur)\n\n        # back to cartesian frequency domain\n        if self.use_numpy_fft:\n            real_part, imag_part = cv2.polarToCart(residual, angle)\n            img_combined = np.fft.ifft2(real_part + 1j*imag_part)\n            magnitude, _ = cv2.cartToPolar(np.real(img_combined),\n                                           np.imag(img_combined))\n        else:\n            img_dft[:, :, 0], img_dft[:, :, 1] = cv2.polarToCart(residual,\n                                                                 angle)\n            img_combined = cv2.idft(img_dft)\n            magnitude, _ = cv2.cartToPolar(img_combined[:, :, 0],\n                                           img_combined[:, :, 1])\n\n        return magnitude\n\n\nclass FishClassifierBOVW(object):\n    def __init__(self, pre_trained_model=None, descriptor='ORB',\n                 n_visual_words=5, use_saliency=False):\n        self.model = joblib.load(pre_trained_model) \\\n            if pre_trained_model else None\n        self.n_visual_words = n_visual_words\n        self.descriptor = descriptor\n        self.use_saliency = use_saliency\n\n    # Helper functions\n    @staticmethod\n    def _load_img(path):\n        \"\"\"\n        :param path: path of image to be loaded.\n        :return: cv2 image object\n        \"\"\"\n        img = cv2.imread(path)\n        # Convert the image from cv2 default BGR format to RGB\n        return cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n\n    @staticmethod\n    def _pretty_print(msg):\n            print()\n            print('=' * len(msg))\n            print(msg)\n            print('=' * len(msg))\n\n    def _detect_and_describe(self, image):\n        \"\"\"\n        :param image: Input RGB color image\n        :return: keypoints and features tuple\n        \"\"\"\n        # detect and extract features from the image\n        if self.descriptor == 'SIFT':\n            descriptor = cv2.xfeatures2d.SIFT_create()\n        elif self.descriptor == 'ORB':\n            descriptor = cv2.ORB_create()\n        else:\n            data = image.reshape((-1, 3))\n            data = np.float32(data)\n            return None, data\n        (kps, features) = descriptor.detectAndCompute(image, None)\n\n        # convert the keypoints from KeyPoint objects to NumPy\n        # arrays\n        kps = np.float32([kp.pt for kp in kps])\n        features = np.float32(features)\n\n        # return a tuple of keypoints and features\n        return kps, features\n\n    @staticmethod\n    def _kmeans_clustering(data, k=5):\n        \"\"\"\n        :param data: input data\n        :param k: K value\n        :return: k-Means clusters\n        \"\"\"\n        crit = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 10, 1.0)\n        flags = cv2.KMEANS_RANDOM_CENTERS\n        ret, label, centers = cv2.kmeans(data, k, None, crit, 10, flags)\n        return centers\n\n    #  Main wrapper methods\n\n    def _extract_img_features(self, img_data_dir, mode='train'):\n        \"\"\"\n        :param img_data_dir: directory path where the images reside.\n         The training images should reside in class named folders\n        :param: mode: 'train' or 'test'\n        :return:\n        \"\"\"\n        if mode == 'train':\n            files = glob.glob(\"%s/*/*\" % img_data_dir)\n        else:\n            files = glob.glob(\"%s/*\" % img_data_dir)\n        dataset_size = len(files)\n        resp = np.zeros((dataset_size, 1))\n\n        print(\"\\nProcessing images, and generating descriptors..\\n\")\n        ctr = 0\n        des_list = []\n        for f in files:\n            print(\"Processing image %s\" % f)\n            img = self._load_img(f)\n            if self.use_saliency:\n                sal = Saliency(img)\n                smap = sal.get_saliency_map()\n                img[:, :, 0] *= smap\n                img[:, :, 1] *= smap\n                img[:, :, 2] *= smap\n            kpts, des = self._detect_and_describe(img)\n            des_list.append((f, des))\n            if type == 'train':\n                resp[ctr] = CLASSES[f.split('/')[-2]]\n                ctr += 1\n\n        descriptors = des_list[0][1]\n        for image_path, descriptor in des_list[1:]:\n            descriptors = np.vstack((descriptors, descriptor))\n\n        print(\"\\nClustering the descriptors to form BOVW dictionary..\\n\")\n        centers = self._kmeans_clustering(descriptors, self.n_visual_words)\n        im_features = np.zeros((dataset_size, self.n_visual_words), \"float32\")\n        for i in range(dataset_size):\n            words, distance = vq.vq(des_list[i][1], centers)\n            for w in words:\n                im_features[i][w] += 1\n\n        # Scaling the values of features\n        slr = StandardScaler().fit(im_features)\n        im_features = slr.transform(im_features)\n\n        resp = np.float32(resp)\n        return files, im_features, resp\n\n    def train_classifier(self, train_data_dir, save_model=True):\n        \"\"\"\n        :param train_data_dir: training data directory\n        :param save_model: save classifier model as a pickle file\n        :return: None\n        \"\"\"\n        # Extract features and train the classifier\n        self._pretty_print(\"Extracting training image features\")\n        train_files, train_data, train_resp = \\\n            self._extract_img_features(train_data_dir)\n        self._pretty_print(\"Training the classifier\")\n        self.model = GaussianNB()\n        self.model.fit(train_data, train_resp)\n        if save_model:\n            joblib.dump(self.model, 'model.pkl', protocol=2)\n\n    def test_classifier(self, test_data_dir, submission_file=\"submission.csv\"):\n        \"\"\"\n        :param test_data_dir: test data directory\n        :param submission_file: file name to save predictions\n        :return: None\n        \"\"\"\n        self._pretty_print(\"Extracting testing image features\")\n        test_files, test_data, test_resp = \\\n            self._extract_img_features(test_data_dir, type='test')\n        self._pretty_print(\"Testing the classifier\")\n        predictions = self.model.predict_proba(test_data)\n\n        columns = [CLASSES_REV[int(entry)] for entry in self.model.classes_]\n        submission = pd.DataFrame(predictions, columns=columns)\n        images = [f.split('/')[-1] for f in test_files]\n        submission.insert(0, 'image', images)\n        submission.head()\n        submission.to_csv(submission_file, index=False)\n\n\ndef main():\n    \"\"\"\n    Main wrapper to call the classifier\n    :return: None\n    \"\"\"\n    pre_trained_model_file = 'model.pkl'\n    training_data_dir = '../input/train'\n    testing_data_dir = '../input/test_stg1'\n    submission_file_name = 'Bag_of_visual_words_ORB_NB.csv'\n\n    parameters = {\n        'descriptor': 'ORB',\n        'n_visual_words': 5,\n        'use_saliency': True\n    }\n\n    if os.path.exists(pre_trained_model_file):\n        cls = FishClassifierBOVW(pre_trained_model_file, **parameters)\n        cls.train_classifier(training_data_dir)\n    else:\n        cls = FishClassifierBOVW(**parameters)\n\n    cls.test_classifier(testing_data_dir, submission_file_name)\n\n\nif __name__ == '__main__':\n    main()\n"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c90be43c-7fd9-ad89-a9f0-359241900855"},"outputs":[],"source":"from subprocess import check_output\nprint(check_output([\"ls\"]).decode(\"utf8\"))"}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":0}