{"cells":[{"execution_count":null,"cell_type":"code","outputs":[],"metadata":{"trusted":false,"_uuid":"4273b730ba51dae871a2ec26fac0d9057331fb19","_cell_guid":"7f903488-acce-ad66-a3c4-e98d81350180"},"source":"import numpy as np\nimport pandas as pd\nnp.random.seed(2017)\nimport os\nimport glob\nimport cv2\nimport datetime\nimport pandas as pd\nimport time\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nfrom sklearn.cross_validation import KFold\nfrom keras.models import Sequential\nfrom keras.layers.core import Dense, Dropout, Flatten\nfrom keras.layers.convolutional import Convolution2D, MaxPooling2D, ZeroPadding2D, AveragePooling2D\nfrom keras.callbacks import EarlyStopping\nfrom keras.utils import np_utils\nfrom keras.constraints import maxnorm\nfrom sklearn.metrics import log_loss\nfrom keras import __version__ as keras_version\n\nfrom PIL import ImageFilter, ImageStat\nfrom PIL import Image, ImageDraw\nfrom sklearn.preprocessing import MinMaxScaler\n\nmin_max_s = MinMaxScaler(feature_range=(0, 255), copy=True)\n\ndef im_stats(im_stats_df):\n    im_stats_d = {}\n    for i in range(len(im_stats_df)):\n        im_stats_im_ = Image.open(im_stats_df['path'][i])\n        im_stats_d[im_stats_df['path'][i]] = {'Stats': ImageStat.Stat(im_stats_im_), 'Size': im_stats_im_.size}\n    im_stats_df['size_0'] = im_stats_df['path'].map(lambda x: im_stats_d[x]['Size'][0])\n    im_stats_df['size_1'] = im_stats_df['path'].map(lambda x: im_stats_d[x]['Size'][1])\n    im_stats_df['sum_0'] = im_stats_df['path'].map(lambda x: im_stats_d[x]['Stats'].sum[0])\n    im_stats_df['sum_1'] = im_stats_df['path'].map(lambda x: im_stats_d[x]['Stats'].sum[1])\n    im_stats_df['sum_2'] = im_stats_df['path'].map(lambda x: im_stats_d[x]['Stats'].sum[2])\n    im_stats_df['mean_0'] = im_stats_df['path'].map(lambda x: im_stats_d[x]['Stats'].mean[0])\n    im_stats_df['mean_1'] = im_stats_df['path'].map(lambda x: im_stats_d[x]['Stats'].mean[1])\n    im_stats_df['mean_2'] = im_stats_df['path'].map(lambda x: im_stats_d[x]['Stats'].mean[2])\n    im_stats_df['rms_0'] = im_stats_df['path'].map(lambda x: im_stats_d[x]['Stats'].rms[0])\n    im_stats_df['rms_1'] = im_stats_df['path'].map(lambda x: im_stats_d[x]['Stats'].rms[1])\n    im_stats_df['rms_2'] = im_stats_df['path'].map(lambda x: im_stats_d[x]['Stats'].rms[2])\n    im_stats_df['var_0'] = im_stats_df['path'].map(lambda x: im_stats_d[x]['Stats'].var[0])\n    im_stats_df['var_1'] = im_stats_df['path'].map(lambda x: im_stats_d[x]['Stats'].var[1])\n    im_stats_df['var_2'] = im_stats_df['path'].map(lambda x: im_stats_d[x]['Stats'].var[2])\n    im_stats_df['stddev_0'] = im_stats_df['path'].map(lambda x: im_stats_d[x]['Stats'].stddev[0])\n    im_stats_df['stddev_1'] = im_stats_df['path'].map(lambda x: im_stats_d[x]['Stats'].stddev[1])\n    im_stats_df['stddev_2'] = im_stats_df['path'].map(lambda x: im_stats_d[x]['Stats'].stddev[2])\n    return im_stats_df\n\ndef get_im_cv2(path):\n    img = cv2.imread(path)\n    resized = cv2.resize(img, (32, 32), cv2.INTER_LINEAR)\n    return resized\n\ndef load_train():\n    X_train = []\n    X_train_id = []\n    y_train = []\n    start_time = time.time()\n    train = []\n\n    print('Read train images')\n    folders = ['ALB', 'BET', 'DOL', 'LAG', 'NoF', 'OTHER', 'SHARK', 'YFT']\n    for fld in folders:\n        index = folders.index(fld)\n        print('Load folder {} (Index: {})'.format(fld, index))\n        path = os.path.join('..', 'input', 'train', fld, '*.jpg')\n        files = glob.glob(path) #limited\n        train += files\n        for fl in files:\n            flbase = os.path.basename(fl)\n            img = get_im_cv2(fl)\n            X_train.append(img)\n            X_train_id.append(flbase)\n            y_train.append(index)\n    train = im_stats(pd.DataFrame(train, columns=['path']))\n    train = min_max_s.fit_transform(train[[c for c in train.columns if c not in ['path']]]).astype(int)\n    for im in range(len(X_train)):\n        for i in range(len(train[im])):\n            X_train[im][i,0] = [train[im][i],0,0]\n            X_train[im][i,1] = [train[im][i],0,0]\n            X_train[im][i,2] = [train[im][i],0,0]\n    print('Read train data time: {} seconds'.format(round(time.time() - start_time, 2)))\n    return X_train, y_train, X_train_id\n\ndef load_test():\n    path = os.path.join('..', 'input', 'test_stg1', '*.jpg')\n    files = sorted(glob.glob(path))[:100] #limited\n\n    X_test = []\n    X_test_id = []\n    test = []\n    for fl in files:\n        flbase = os.path.basename(fl)\n        img = get_im_cv2(fl)\n        X_test.append(img)\n        X_test_id.append(flbase)\n    test = im_stats(pd.DataFrame(files, columns=['path']))\n    test = min_max_s.transform(test[[c for c in test.columns if c not in ['path']]]).astype(int)\n    for im in range(len(X_test)):\n        for i in range(len(test[im])):\n            X_test[im][i,0] = [test[im][i],0,0]\n            X_test[im][i,1] = [test[im][i],0,0]\n            X_test[im][i,2] = [test[im][i],0,0]\n    return X_test, X_test_id\n\ndef create_submission(predictions, test_id, info):\n    result1 = pd.DataFrame(predictions, columns=['ALB', 'BET', 'DOL', 'LAG', 'NoF', 'OTHER', 'SHARK', 'YFT'])\n    result1.loc[:, 'image'] = pd.Series(test_id, index=result1.index)\n    now = datetime.datetime.now()\n    sub_file = 'submission_' + info + '_' + str(now.strftime(\"%Y-%m-%d-%H-%M\")) + '.csv'\n    result1.to_csv(sub_file, index=False)\n\ndef read_and_normalize_train_data():\n    train_data, train_target, train_id = load_train()\n\n    print('Convert to numpy...')\n    train_data = np.array(train_data, dtype=np.uint8)\n    train_target = np.array(train_target, dtype=np.uint8)\n\n    print('Reshape...')\n    train_data = train_data.transpose((0, 3, 1, 2))\n\n    print('Convert to float...')\n    train_data = train_data.astype('float32')\n    train_data = train_data / 255\n    train_target = np_utils.to_categorical(train_target, 8)\n\n    print('Train shape:', train_data.shape)\n    print(train_data.shape[0], 'train samples')\n    return train_data, train_target, train_id\n\ndef read_and_normalize_test_data():\n    start_time = time.time()\n    test_data, test_id = load_test()\n\n    test_data = np.array(test_data, dtype=np.uint8)\n    test_data = test_data.transpose((0, 3, 1, 2))\n\n    test_data = test_data.astype('float32')\n    test_data = test_data / 255\n\n    print('Test shape:', test_data.shape)\n    print(test_data.shape[0], 'test samples')\n    print('Read and process test data time: {} seconds'.format(round(time.time() - start_time, 2)))\n    return test_data, test_id\n\ndef dict_to_list(d):\n    ret = []\n    for i in d.items():\n        ret.append(i[1])\n    return ret\n\ndef merge_several_folds_mean(data, nfolds):\n    a = np.array(data[0])\n    for i in range(1, nfolds):\n        a += np.array(data[i])\n    a /= nfolds\n    return a.tolist()\n\ndef create_model():\n    model = Sequential()\n    model.add(ZeroPadding2D((1, 1), input_shape=(3, 32, 32), dim_ordering='th'))\n    model.add(Convolution2D(8, 3, 3, activation='relu', dim_ordering='th', init='he_uniform'))\n    model.add(Dropout(0.2))\n    \n    model.add(Flatten())\n    model.add(Dense(16, activation='relu',init='he_uniform'))\n    model.add(Dropout(0.2))\n    model.add(Dense(8, activation='softmax'))\n\n    model.compile(optimizer='adadelta', loss='categorical_crossentropy')\n    return model\n\ndef get_validation_predictions(train_data, predictions_valid):\n    pv = []\n    for i in range(len(train_data)):\n        pv.append(predictions_valid[i])\n    return pv\n\ndef run_cross_validation_create_models(nfolds=5):\n    batch_size = 20\n    nb_epoch = 4\n    random_state = 0\n\n    train_data, train_target, train_id = read_and_normalize_train_data()\n\n    yfull_train = dict()\n    kf = KFold(len(train_id), n_folds=nfolds, shuffle=True, random_state=random_state)\n    num_fold = 0\n    sum_score = 0\n    models = []\n    for train_index, test_index in kf:\n        model = create_model()\n        X_train = train_data[train_index]\n        Y_train = train_target[train_index]\n        X_valid = train_data[test_index]\n        Y_valid = train_target[test_index]\n\n        num_fold += 1\n        print('Start KFold number {} from {}'.format(num_fold, nfolds))\n        print('Split train: ', len(X_train), len(Y_train))\n        print('Split valid: ', len(X_valid), len(Y_valid))\n\n        callbacks = [\n            EarlyStopping(monitor='val_loss', patience=3, verbose=0),\n        ]\n        model.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=nb_epoch,\n              shuffle=True, verbose=2, validation_data=(X_valid, Y_valid),\n              callbacks=callbacks)\n\n        predictions_valid = model.predict(X_valid.astype('float32'), batch_size=batch_size, verbose=2)\n        score = log_loss(Y_valid, predictions_valid)\n        print('Score log_loss: ', score)\n        sum_score += score*len(test_index)\n\n        # Store valid predictions\n        for i in range(len(test_index)):\n            yfull_train[test_index[i]] = predictions_valid[i]\n\n        models.append(model)\n\n    score = sum_score/len(train_data)\n    print(\"Log_loss train independent avg: \", score)\n\n    info_string = '_' + str(np.round(score,3)) + '_flds_' + str(nfolds) + '_eps_' + str(nb_epoch)\n    return info_string, models\n\ndef run_cross_validation_process_test(info_string, models):\n    batch_size = 20\n    num_fold = 0\n    yfull_test = []\n    test_id = []\n    nfolds = len(models)\n\n    for i in range(nfolds):\n        model = models[i]\n        num_fold += 1\n        print('Start KFold number {} from {}'.format(num_fold, nfolds))\n        test_data, test_id = read_and_normalize_test_data()\n        test_prediction = model.predict(test_data, batch_size=batch_size, verbose=2)\n        yfull_test.append(test_prediction)\n\n    test_res = merge_several_folds_mean(yfull_test, nfolds)\n    info_string = 'loss_' + info_string + '_folds_' + str(nfolds)\n    create_submission(test_res, test_id, info_string)\n\nif __name__ == '__main__':\n    print('Keras version: {}'.format(keras_version))\n    num_folds = 2\n    info_string, models = run_cross_validation_create_models(num_folds)\n    run_cross_validation_process_test(info_string, models)"},{"execution_count":null,"cell_type":"code","outputs":[],"metadata":{"trusted":false,"_uuid":"8a3de32a22885642bc1934bcd96af2c46e48594e","_cell_guid":"e71a8ca8-eca2-c5e9-775c-1f047322f373"},"source":"#Now to blend use the following with your various results\n\nimport numpy as np\n\ndf1 = pd.read_csv('../input/sample_submission_stg1.csv') #change these\ndf2 = pd.read_csv('../input/sample_submission_stg1.csv') #change these\nc = [c+'_' if c !='image' else c for  c in df2.columns]\ndf2.columns = c\ndf = pd.merge(df1, df2, on='image', how='inner')\n\nfor c in df1.columns:\n    if c != 'image':\n        df[c] = (df[c] + df[c+'_'])/2\ndf[df1.columns].to_csv('z11_sub_blend01.csv', index=False)"}],"nbformat":4,"metadata":{"language_info":{"version":"3.6.1","pygments_lexer":"ipython3","mimetype":"text/x-python","nbconvert_exporter":"python","file_extension":".py","name":"python","codemirror_mode":{"version":3,"name":"ipython"}},"_is_fork":false,"_change_revision":0,"kernelspec":{"display_name":"Python 3","name":"python3","language":"python"}},"nbformat_minor":0}