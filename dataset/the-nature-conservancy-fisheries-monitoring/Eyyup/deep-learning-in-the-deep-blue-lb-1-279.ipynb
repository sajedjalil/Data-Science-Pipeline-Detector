{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"17a737e2-ff67-6170-1229-bb95dbb2f65a"},"source":"# Start-to-Finish Solution in Keras\n\nHere is my basic method for getting a LB submission churned out. No parameter tuning or data augmentation has been attempted, which should increase the score significantly. "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e6a4aad9-f09f-8f20-a9d0-159f5ef6c922"},"outputs":[],"source":"import os, cv2, random\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import log_loss\nfrom sklearn.preprocessing import LabelEncoder\n\nimport matplotlib.pyplot as plt\nfrom matplotlib import ticker\nimport seaborn as sns\n%matplotlib inline \n\nfrom keras.models import Sequential\nfrom keras.layers import Dropout, Flatten, Convolution2D, MaxPooling2D, ZeroPadding2D, Dense, Activation\nfrom keras.optimizers import RMSprop, Adam\nfrom keras.callbacks import EarlyStopping\nfrom keras.utils import np_utils\nfrom keras import backend as K\n\nTRAIN_DIR = '../input/train/'\nTEST_DIR = '../input/test_stg1/'\nFISH_CLASSES = ['ALB', 'BET', 'DOL', 'LAG', 'NoF', 'OTHER', 'SHARK', 'YFT']\nROWS = 90  #720\nCOLS = 160 #1280\nCHANNELS = 3"},{"cell_type":"markdown","metadata":{"_cell_guid":"dec4ed7b-f0b8-0c30-5431-3bd652c190da"},"source":"# Loading and Preprocessing Data\n\nNot much processing, other than resizing to 90x160, but you will probably want to run larger images on a GPU for a higher score. I am also keeping track of the labels as I loop through each image folder.  "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d8f4e77d-ce91-48f7-c04a-ba225f893f0c"},"outputs":[],"source":"def get_images(fish):\n    \"\"\"Load files from train folder\"\"\"\n    fish_dir = TRAIN_DIR+'{}'.format(fish)\n    images = [fish+'/'+im for im in os.listdir(fish_dir)]\n    return images\n\ndef read_image(src):\n    \"\"\"Read and resize individual images\"\"\"\n    im = cv2.imread(src, cv2.IMREAD_COLOR)\n    im = cv2.resize(im, (COLS, ROWS), interpolation=cv2.INTER_CUBIC)\n    return im\n\n\nfiles = []\ny_all = []\n\nfor fish in FISH_CLASSES:\n    fish_files = get_images(fish)\n    files.extend(fish_files)\n    \n    y_fish = np.tile(fish, len(fish_files))\n    y_all.extend(y_fish)\n    print(\"{0} photos of {1}\".format(len(fish_files), fish))\n    \ny_all = np.array(y_all)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"7547d288-1ca9-c45e-0efb-75ea832cb318"},"outputs":[],"source":"X_all = np.ndarray((len(files), ROWS, COLS, CHANNELS), dtype=np.uint8)\n\nfor i, im in enumerate(files): \n    X_all[i] = read_image(TRAIN_DIR+im)\n    if i%1000 == 0: print('Processed {} of {}'.format(i, len(files)))\n\nprint(X_all.shape)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"18a6f374-8c0e-b332-3e46-46a732b96930"},"outputs":[],"source":"## Uncomment to check out a fish from each class\n#uniq = np.unique(y_all, return_index=True)\n# for f, i in zip(uniq[0], uniq[1]):\n    #plt.imshow(X_all[i])\n    #plt.title(f)\n    #plt.show()"},{"cell_type":"markdown","metadata":{"_cell_guid":"25a3b3b2-a00e-d618-9d2c-4dc18ad14744"},"source":"# Splitting the Training Data\n\nOne-Hot-Encode the labels, then create a stratified train/validation split. "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"0537e039-ae27-ea4e-82a6-23603d9aa007"},"outputs":[],"source":"# One Hot Encoding Labels\ny_all = LabelEncoder().fit_transform(y_all)\ny_all = np_utils.to_categorical(y_all)\n\nX_train, X_valid, y_train, y_valid = train_test_split(X_all, y_all, \n                                                    test_size=0.2, random_state=23, \n                                                    stratify=y_all)"},{"cell_type":"markdown","metadata":{"_cell_guid":"e2256352-79ff-78eb-24d8-f1874d32cbf4"},"source":"## The Model\n\nPretty typical CNN in Keras with a plenty of dropout regularization between the fully connected layers. Note: I set the epochs to 1 to avoid timing out - change it to around 20. "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"6e2396d3-d295-9616-d327-99cbe76b8851"},"outputs":[],"source":"optimizer = RMSprop(lr=1e-4)\nobjective = 'categorical_crossentropy'\n\ndef center_normalize(x):\n    return (x - K.mean(x)) / K.std(x)\n\nmodel = Sequential()\n\nmodel.add(Activation(activation=center_normalize, input_shape=(ROWS, COLS, CHANNELS)))\n\nmodel.add(Convolution2D(32, 5, 5, border_mode='same', activation='relu', dim_ordering='tf'))\nmodel.add(Convolution2D(32, 5, 5, border_mode='same', activation='relu', dim_ordering='tf'))\nmodel.add(MaxPooling2D(pool_size=(2, 2), dim_ordering='tf'))\n\nmodel.add(Convolution2D(64, 3, 3, border_mode='same', activation='relu', dim_ordering='tf'))\nmodel.add(Convolution2D(64, 3, 3, border_mode='same', activation='relu', dim_ordering='tf'))\nmodel.add(MaxPooling2D(pool_size=(2, 2), dim_ordering='tf'))\n\nmodel.add(Convolution2D(128, 3, 3, border_mode='same', activation='relu', dim_ordering='tf'))\nmodel.add(Convolution2D(128, 3, 3, border_mode='same', activation='relu', dim_ordering='tf'))\nmodel.add(MaxPooling2D(pool_size=(2, 2), dim_ordering='tf'))\n\nmodel.add(Convolution2D(256, 3, 3, border_mode='same', activation='relu', dim_ordering='tf'))\nmodel.add(Convolution2D(256, 3, 3, border_mode='same', activation='relu', dim_ordering='tf'))\nmodel.add(MaxPooling2D(pool_size=(2, 2), dim_ordering='tf'))\n\n\nmodel.add(Flatten())\nmodel.add(Dense(256, activation='relu'))\nmodel.add(Dropout(0.5))\n\nmodel.add(Dense(64, activation='relu'))\nmodel.add(Dropout(0.5))\n\nmodel.add(Dense(len(FISH_CLASSES)))\nmodel.add(Activation('sigmoid'))\n\nmodel.compile(loss=objective, optimizer=optimizer)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"461ff410-5bca-f26d-5b4b-722fb2681e33"},"outputs":[],"source":"early_stopping = EarlyStopping(monitor='val_loss', patience=4, verbose=1, mode='auto')        \n        \nmodel.fit(X_train, y_train, batch_size=64, nb_epoch=1,\n              validation_split=0.2, verbose=1, shuffle=True, callbacks=[early_stopping])"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"6c3c125c-e273-99a5-8360-8b717b9b9e58"},"outputs":[],"source":"preds = model.predict(X_valid, verbose=1)\nprint(\"Validation Log Loss: {}\".format(log_loss(y_valid, preds)))"},{"cell_type":"markdown","metadata":{"_cell_guid":"6020ea37-c78f-a6f3-0877-2196833f8790"},"source":"# Predicting the Test Set\n\nFinishing off with predictions on the test set. Scored LB 1.279 "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c4769ce7-367c-5240-b615-714972d8e596"},"outputs":[],"source":"test_files = [im for im in os.listdir(TEST_DIR)]\ntest = np.ndarray((len(test_files), ROWS, COLS, CHANNELS), dtype=np.uint8)\n\nfor i, im in enumerate(test_files): \n    test[i] = read_image(TEST_DIR+im)\n    \ntest_preds = model.predict(test, verbose=1)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"dfe3630b-9944-e91e-102d-d26efdcaa90f"},"outputs":[],"source":"submission = pd.DataFrame(test_preds, columns=FISH_CLASSES)\nsubmission.insert(0, 'image', test_files)\nsubmission.head()"}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":0}