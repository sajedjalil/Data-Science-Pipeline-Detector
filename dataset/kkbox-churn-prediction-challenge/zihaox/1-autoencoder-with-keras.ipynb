{"nbformat_minor":1,"nbformat":4,"cells":[{"source":"Note that the autoencoder code are borrowed from the following notebook: https://github.com/curiousily/Credit-Card-Fraud-Detection-using-Autoencoders-in-Keras/blob/master/fraud_detection.ipynb\n\nThe code used for summary statistics / dtype fixing belongs to ZihaoXu.","cell_type":"markdown","metadata":{"_uuid":"7d76e8d6f55dd51420857a11cba092c840b911a4","_cell_guid":"5216d274-8031-4b2f-9dc8-f2351992eaf5"}},{"source":"# important packages to import\nimport pandas as pd\nimport numpy as np\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport sys, os\nimport pickle\n\nfrom scipy import stats\nfrom pylab import rcParams\nfrom keras.models import Model, load_model\nfrom keras.layers import Input, Dense\nfrom keras.callbacks import ModelCheckpoint, TensorBoard\nfrom keras import regularizers\n\nfrom matplotlib import offsetbox\nfrom matplotlib.ticker import NullFormatter\nfrom sklearn import preprocessing, cross_validation, svm, manifold\nfrom sklearn.cross_validation import cross_val_score, KFold\nfrom sklearn.ensemble import RandomForestClassifier # Load scikit's random forest classifier library\nfrom sklearn.grid_search import GridSearchCV\n\nfrom time import time\nfrom datetime import datetime, timedelta\nfrom collections import defaultdict\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))","execution_count":null,"cell_type":"code","metadata":{"_uuid":"429a16b108ea544970eed55093467fa23b0254ec","_cell_guid":"596c0675-6567-459c-9fde-f08060ae37bd"},"outputs":[]},{"source":"# holistic summary of the given data set. \n# \"remove_bad_rowCol\" can be turned on to remove non-informative col / row\ndef holistic_summary(df, remove_bad_rowCol = False, verbose = True):\n    # remove non-informative columns\n    if(remove_bad_rowCol):\n        df = df.drop(df.columns[df.isnull().sum() >= .9 * len(df)], axis = 1)\n        df = df.drop(df.index[df.isnull().sum(axis = 1) >= .5* len(df.columns)], axis = 0)\n        \n    # fix column names:\n    df.columns = [c.replace(\" \", \"_\").lower() for c in df.columns]\n    \n    print('***************************************************************')\n    print('Begin holistic summary: ')\n    print('***************************************************************\\n')\n    \n    print('Dimension of df: ' + str(df.shape))\n    print('Percentage of good observations: ' + str(1 - df.isnull().any(axis = 1).sum()/len(df)))\n    print('---------------------------------------------------------------\\n')\n    \n    print(\"Rows with nan values: \" + str(df.isnull().any(axis = 1).sum()))\n    print(\"Cols with nan values: \" + str(df.isnull().any(axis = 0).sum()))\n    print('Breakdown:')\n    print(df.isnull().sum()[df.isnull().sum()!=0])\n    print('---------------------------------------------------------------\\n')\n    \n    print('Columns details: ')\n    print('Columns with known dtypes: ')\n    good_cols = pd.DataFrame(df.dtypes[df.dtypes!='object'], columns = ['type'])\n    good_cols['nan_num'] = [df[col].isnull().sum() for col in good_cols.index]\n    good_cols['unique_val'] = [df[col].nunique() for col in good_cols.index]\n    good_cols['example'] = [df[col][1] for col in good_cols.index]\n    good_cols = good_cols.reindex(good_cols['type'].astype(str).str.len().sort_values().index)\n    print(good_cols)\n    print('\\n')\n    \n    try:\n        print('Columns with unknown dtypes:')\n        bad_cols = pd.DataFrame(df.dtypes[df.dtypes=='object'], columns = ['type'])\n        bad_cols['nan_num'] = [df[col].isnull().sum() for col in bad_cols.index]\n        bad_cols['unique_val'] = [df[col].nunique() for col in bad_cols.index]\n        bad_cols['example(sliced)'] = [str(df[col][1])[:10] for col in bad_cols.index]\n        bad_cols = bad_cols.reindex(bad_cols['example(sliced)'].str.len().sort_values().index)\n        print(bad_cols)\n    except Exception as e:\n        print('No columns with unknown dtypes!')\n    print('_______________________________________________________________\\n\\n\\n')\n    #if not verbose: enablePrint()\n    return df\ndef memo(df):\n    mem = df.memory_usage(index=True).sum()\n    print(mem/ 1024**2,\" MB\")","execution_count":null,"cell_type":"code","metadata":{"_uuid":"61b81a75737450a0e07045a10e5fe74a8f67fd55","collapsed":true,"_cell_guid":"71148697-53e3-4095-b1c9-96ac40566689"},"outputs":[]},{"source":"trans = pd.read_csv('../input/transactions.csv')\n# Memory Reduction\ndef change_datatype(df):\n    int_cols = list(df.select_dtypes(include=['int']).columns)\n    for col in int_cols:\n        if ((np.max(df[col]) <= 127) and(np.min(df[col] >= -128))):\n            df[col] = df[col].astype(np.int8)\n        elif ((np.max(df[col]) <= 32767) and(np.min(df[col] >= -32768))):\n            df[col] = df[col].astype(np.int16)\n        elif ((np.max(df[col]) <= 2147483647) and(np.min(df[col] >= -2147483648))):\n            df[col] = df[col].astype(np.int32)\n        else:\n            df[col] = df[col].astype(np.int64)\n            \ndef change_datatype_float(df):\n    float_cols = list(df.select_dtypes(include=['float']).columns)\n    for col in float_cols:\n        df[col] = df[col].astype(np.float32)\n\nchange_datatype(trans)\nchange_datatype_float(trans)\nmemo(trans)\n","execution_count":null,"cell_type":"code","metadata":{"_uuid":"8b562d92cc9532e4b83713bc71a7a1ffca6e5e1a","_cell_guid":"eaf53791-1202-4566-9ddc-3056ea19d419"},"outputs":[]},{"source":"# fixing dtypes: time and numeric variables\ndef fix_dtypes(df, time_cols, num_cols):\n    \n    print('***************************************************************')\n    print('Begin fixing data types: ')\n    print('***************************************************************\\n')\n    \n    def fix_time_col(df, time_cols):\n        for time_col in time_cols:\n            df[time_col] = pd.to_datetime(df[time_col], errors = 'coerce', format = '%Y%m%d')\n        print('---------------------------------------------------------------')\n        print('The following time columns has been fixed: ')\n        print(time_cols)\n        print('---------------------------------------------------------------\\n')\n\n    def fix_num_col(df, num_cols):\n        for col in num_cols:\n            df[col] = pd.to_numeric(df[col], errors = 'coerce')\n        print('---------------------------------------------------------------')\n        print('The following number columns has been fixed: ')\n        print(num_cols)\n        print('---------------------------------------------------------------\\n')\n        \n    if(len(num_cols) > 0):\n        fix_num_col(df, num_cols)\n    fix_time_col(df, time_cols)\n\n    print('---------------------------------------------------------------')\n    print('Final data types:')\n    result = pd.DataFrame(df.dtypes, columns = ['type'])\n    result = result.reindex(result['type'].astype(str).str.len().sort_values().index)\n    print(result)\n    print('_______________________________________________________________\\n\\n\\n')\n    return df","execution_count":null,"cell_type":"code","metadata":{"_uuid":"98ef0cca74072861013f843f5d4aa9f8099265b6","collapsed":true,"_cell_guid":"82c42e27-5b03-4f14-87b5-e8db4e06271e"},"outputs":[]},{"source":"np.random.seed(47)\nsamp = trans.sample(frac = .01, replace = False)\ntrain = pd.read_csv('../input/train.csv')\ntrain = train.append(pd.read_csv('../input/train_v2.csv'))\ntrain.index = range(len(train))\n\ntest = pd.read_csv('../input/sample_submission_zero.csv')\ntest = test.append(pd.read_csv('../input/sample_submission_v2.csv'))\ntest.index = range(len(test))\n\nmembers = pd.read_csv('../input/members_v3.csv')\n\nsamp = samp.merge(train, on = 'msno', how = 'inner')\nsamp = samp.merge(members, on = 'msno', how = 'inner')\n\nsamp.head()","execution_count":null,"cell_type":"code","metadata":{"_uuid":"6d6197ebb2146593fa350860829cb72d15b07403","_cell_guid":"403e1451-ff1d-4648-af48-2ed88c3b37b9"},"outputs":[]},{"source":"samp = fix_dtypes(samp, time_cols = ['transaction_date', 'membership_expire_date', 'registration_init_time'], num_cols = [])\nsamp['year'] = [d.year for d in samp['transaction_date']]\nsamp['month'] = [d.month for d in samp['transaction_date']]\nsamp['day'] = [d.day for d in samp['transaction_date']]\nsamp['wday'] = [d.weekday() for d in samp['transaction_date']]\nsamp['transaction_date'] = [d.year + (d.month-1) / 12 + d.day / 365 for d in samp['transaction_date']]\nsamp['membership_expire_date'] = [d.year + (d.month-1) / 12 + d.day / 365 for d in samp['membership_expire_date']]\nsamp['registration_init_time'] = [d.year + (d.month-1) / 12 + d.day / 365 for d in samp['registration_init_time']]\nmemo(samp)","execution_count":null,"cell_type":"code","metadata":{"_uuid":"fb5ee3c90c3c6a86ebe80d128bb288edcca68b18","_cell_guid":"ef2f834c-a874-4b4e-94e7-4c83bbf1e09f"},"outputs":[]},{"source":"from multiprocessing import Pool, cpu_count\nimport gc; gc.enable()\n\ndef transform_df(df):\n    df = pd.DataFrame(df)\n    df = df.sort_values(by=['date'], ascending=[False])\n    df = df.reset_index(drop=True)\n    df = df.drop_duplicates(subset=['msno'], keep='first')\n    return df\n\ndef transform_df2(df):\n    df = df.sort_values(by=['date'], ascending=[False])\n    df = df.reset_index(drop=True)\n    df = df.drop_duplicates(subset=['msno'], keep='first')\n    return df\n\ndf_iter = pd.read_csv('../input/user_logs.csv', low_memory=False, iterator=True, chunksize=10000000)\nlast_user_logs = []\ni = 0 #~400 Million Records - starting at the end but remove locally if needed\nfor df in df_iter:\n    if i>35: # used to be 35, just testing\n        if len(df)>0:\n            print(df.shape)\n            p = Pool(cpu_count())\n            df = p.map(transform_df, np.array_split(df, cpu_count()))   \n            df = pd.concat(df, axis=0, ignore_index=True).reset_index(drop=True)\n            df = transform_df2(df)\n            p.close(); p.join()\n            last_user_logs.append(df)\n            print('...', df.shape)\n            df = []\n    i+=1\n\nlast_user_logs = pd.concat(last_user_logs, axis=0, ignore_index=True).reset_index(drop=True)\nlast_user_logs = transform_df2(last_user_logs)","execution_count":null,"cell_type":"code","metadata":{},"outputs":[]},{"source":"print(last_user_logs.shape)\nprint(list(last_user_logs))","execution_count":null,"cell_type":"code","metadata":{},"outputs":[]},{"source":"print(\"Len before the merge: \", len(samp))\nsamp = samp.merge(last_user_logs, on = 'msno', how = 'inner')\nprint(\"Len after the merge: \", len(samp))","execution_count":null,"cell_type":"code","metadata":{},"outputs":[]},{"source":"print(list(samp))\nprint(\"Number of observations: \" + str(len(samp)))\nsamp.head()","execution_count":null,"cell_type":"code","metadata":{"_uuid":"7d6f62b3cad3336953c90475c9170a69659921e3","_cell_guid":"9ef5661f-767d-415f-9316-3792e0e73f25"},"outputs":[]},{"source":"# Visualization of the \"churn\" class","cell_type":"markdown","metadata":{"_uuid":"1cd3ac048d76c1df78156286dad46c6e9b0fae33","_cell_guid":"fdab7743-63de-444c-8b15-dd635a6051ff"}},{"source":"# We see that only the gender column has quite a lot of NANs\ndf = samp\ndf = holistic_summary(df)","execution_count":null,"cell_type":"code","metadata":{"_uuid":"bba4e268c51de78a3492ee32dd80d8370ecf3fca","_cell_guid":"ee3a1996-150f-4eb3-a8de-6ccbdd7ee191"},"outputs":[]},{"source":"# impute the missing genders randomly\nimport random\nnp.random.seed(47)\n\ngender = []\nfor x in df['gender']:\n    if type(x) == np.float:\n        gender.append(random.choice(['female', 'male']))\n    else:\n        gender.append(x)\ndf['gender'] = gender\ndf['gender'].isnull().any()","execution_count":null,"cell_type":"code","metadata":{"_uuid":"ec8a302d25c08a3546875098d84a2e94f1aa1fac","_cell_guid":"ed7f412d-1cb1-423a-b5f2-ed5e5e62e6a8"},"outputs":[]},{"source":"sns.set(style = 'white')\nsns.countplot(data = df, x = 'is_churn')\nsns.despine()","execution_count":null,"cell_type":"code","metadata":{"_uuid":"5386bbc6e670e94881a4ff20bb94a4e372ce2d68","_cell_guid":"403157c0-6857-4431-bad8-8a8d3c2ae0e5"},"outputs":[]},{"source":"The dataset is quite imbalanced...","cell_type":"markdown","metadata":{"_uuid":"b2908e16012335fe1ff64ba5b3a67c65cebeeae0","_cell_guid":"c3430b8c-5f2d-4ed7-a1c2-7ceaf155c354"}},{"source":"print(\"Churn ratio\", len(df[df['is_churn'] == 1])/len(df[df['is_churn'] == 0]))","execution_count":null,"cell_type":"code","metadata":{"_uuid":"00f79f4c2c2f887d4700dacb52b981c5bb8ef20e","_cell_guid":"7534c489-63a3-40ba-b714-6bb05ba37fd2"},"outputs":[]},{"source":"churn = df[df['is_churn'] == 1]\nn_churn = df[df['is_churn'] == 0]\n\nf, (ax1, ax2) = plt.subplots(2, 1, sharex=True)\nf.suptitle('Amount per transaction by class')\n\nbins = 50\nax1.hist(churn.actual_amount_paid, bins = bins)\nax1.set_title('Churn')\n\nax2.hist(n_churn.actual_amount_paid, bins = bins)\nax2.set_title('Not Churn')\n\nplt.xlabel('Amount ($)')\nplt.ylabel('Number of Transactions')\nplt.show();","execution_count":null,"cell_type":"code","metadata":{"_uuid":"af9d82a0f0c30ffb4d9586d0aaaf40c6932fbdd4","_cell_guid":"045d9a5b-3dfb-4354-b562-68d81fecd025"},"outputs":[]},{"source":"time_cols = ['wday', 'membership_expire_date']\n\nfor t in time_cols:\n    f, (ax1, ax2) = plt.subplots(2, 1, sharex=True)\n    f.suptitle('Time of transaction vs Amount by class')\n    ax1.scatter(churn[t], churn['actual_amount_paid'], s = 2,alpha = .25)\n    ax1.set_title('Churn')\n\n    ax2.scatter(n_churn[t], n_churn['actual_amount_paid'], s = 2, alpha = .25)\n    ax2.set_title('Not Churn')\n\n    plt.xlabel('Time')\n    plt.ylabel('Amount')\n    plt.show()","execution_count":null,"cell_type":"code","metadata":{"_uuid":"ef7373559833cb8b4aa6e8248cbd8c56eb198b5c","_cell_guid":"9416b1cb-bcae-4592-86ab-89791a857876"},"outputs":[]},{"source":"# Keras autoencoder","cell_type":"markdown","metadata":{"_uuid":"70b99ac8b1432253d617f994a2842e4fade2ab20","collapsed":true,"_cell_guid":"472783b4-ff8a-413b-901f-ef952e8f1d23"}},{"source":"# Drop the 'msno', 'bd' cols since have no value\ndf = df.drop(['msno', 'bd'], axis = 1)","execution_count":null,"cell_type":"code","metadata":{"_uuid":"e27c92da6a4b36f59ba068cf17779a3a1d663e1d","collapsed":true,"_cell_guid":"96b9da72-60b5-4b55-8ad3-a2c4569dbeea"},"outputs":[]},{"source":"# Encode the gender col to 1 for male and 0 for female\ndf['gender'] = np.where(df['gender'] == 'male', 1, 0)","execution_count":null,"cell_type":"code","metadata":{"_uuid":"d401c977fe1aa22bf6c514a5994fc94c3f212b14","collapsed":true,"_cell_guid":"511c07be-8a75-4d6d-ba5f-e6124c41d340"},"outputs":[]},{"source":"# Plot the correlation matrix\ndef corr_plot(df, title = 'Correlation Matrix', annot=False, show = True):\n    sns.set(style = 'white')\n\n    # Compute the correlation matrix\n    corr = df.corr()\n\n    # Generate a mask for the upper triangle\n    mask = np.zeros_like(corr, dtype=np.bool)\n    mask[np.triu_indices_from(mask)] = True\n\n    # Set up the matplotlib figure\n    f, ax = plt.subplots(figsize=(11, 9))\n\n    # Generate a custom diverging colormap\n    cmap = sns.diverging_palette(220, 10, as_cmap=True)\n\n    # Draw the heatmap with the mask and correct aspect ratio\n    sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0, annot=annot,\n                square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\n    plt.title(title)\n    if show:\n        plt.show()\ncorr_plot(samp[sorted(list(df))])","execution_count":null,"cell_type":"code","metadata":{},"outputs":[]},{"source":"features = np.array(df[[col for col in df.columns if col != 'is_churn']])\nresponse = np.array(df[['is_churn']])\n\nprint(len(features))\nprint(len(response))","execution_count":null,"cell_type":"code","metadata":{"_uuid":"a771ac9bbd629b80118452269b454ef11b4dedb5","_cell_guid":"84bd8626-6ce0-4f05-9870-64db72fc89ca"},"outputs":[]},{"source":"features","execution_count":null,"cell_type":"code","metadata":{"_uuid":"41298a3a306ad7c2b320e776830f3f2d456be8bb","_cell_guid":"2f12d22c-3cfc-48f4-88d4-c7c11c79e23c"},"outputs":[]},{"source":"Let's first standardize the data and apply PCA.","cell_type":"markdown","metadata":{"_uuid":"e8d50f444223422227cb62d9e79390d65213a072","_cell_guid":"464c81b3-855e-4b8a-a33b-3d3e3baa015e"}},{"source":"from sklearn import decomposition\nfrom sklearn.preprocessing import StandardScaler\n\n# Fit the scaler to the features and transform\nfeatures_std = StandardScaler().fit_transform(features)","execution_count":null,"cell_type":"code","metadata":{"_uuid":"455800e97adba1a028fbc274626d75101d1bde3f","collapsed":true,"_cell_guid":"34a6d6e7-3ecd-423c-9966-51868ded7ce1"},"outputs":[]},{"source":"# Create a pca object with the 20 components as a parameter\npca = decomposition.PCA(n_components=20)\n\n# Fit the PCA and transform the data\nfeatures_pca = pca.fit_transform(features_std)","execution_count":null,"cell_type":"code","metadata":{"_uuid":"6f85ad72d0f6c5a577369b9478f84483a710aacb","collapsed":true,"_cell_guid":"62a8ce50-2fc4-4e48-93ff-26b6c18e5fea"},"outputs":[]},{"source":"features_pca.shape","execution_count":null,"cell_type":"code","metadata":{"_uuid":"27222d0c747d72cc67b5421b62d8068c596f1f3a","_cell_guid":"d9e17e12-6bb1-4c84-907f-4d8b75cccc2b"},"outputs":[]},{"source":"viz = pd.DataFrame(features_pca)[[0,1,2]]\nviz.columns = [str(c) for c in viz.columns]\nviz['is_churn'] = df['is_churn']\nsns.lmplot(data = viz, x = '0', y = '1', fit_reg=False, hue = 'is_churn')\nplt.show()","execution_count":null,"cell_type":"code","metadata":{"_uuid":"adb70cf0fa1ccf78f865a8e8b8937625d55eeea5","_cell_guid":"06782548-ff84-4f9d-bd27-fa8c7ecf979c"},"outputs":[]},{"source":"# Convert features_pca back to a df and add the is_churn column\nfeatures_pca = pd.DataFrame(features_pca)\nfeatures_pca['is_churn'] = df['is_churn']\n\nfeatures_std = pd.DataFrame(features_std)\nfeatures_std['is_churn'] = df['is_churn']","execution_count":null,"cell_type":"code","metadata":{"_uuid":"96ce67d1e0741805293f984bcdc4796bfbbbeb48","_cell_guid":"86b84c9c-cf00-4657-abff-a96f24a3c6da"},"outputs":[]},{"source":"from sklearn.cross_validation import train_test_split\nX_train, X_test = train_test_split(features_std, test_size=0.2, random_state=47)\nX_train = X_train[X_train['is_churn'] == 0]\nX_train = X_train.drop(['is_churn'], axis=1)\n\ny_test = X_test['is_churn']\nX_test = X_test.drop(['is_churn'], axis=1)\n\nX_train = X_train.values\nX_test = X_test.values","execution_count":null,"cell_type":"code","metadata":{"_uuid":"568c56e375415342fa5a2d69387eb4b17f7c3ed5","collapsed":true,"_cell_guid":"bb4d21f4-4021-4c74-8105-8d707afbe29e"},"outputs":[]},{"source":"X_train.shape","execution_count":null,"cell_type":"code","metadata":{"_uuid":"e58689c62a36e39d2da353c9afaea80d37694222","_cell_guid":"b9300c51-536c-4c10-8e8d-a467afc27df2"},"outputs":[]},{"source":"X_test.shape","execution_count":null,"cell_type":"code","metadata":{"_uuid":"0a5198af6c4727d33c2d5d64dc8972f3a68a66f6","_cell_guid":"11a612f9-c38b-4e14-98eb-19a790203c65"},"outputs":[]},{"source":"## Building the model","cell_type":"markdown","metadata":{"_uuid":"2035b7dbe6f6529630d411b1cf3511a29b97181a","_cell_guid":"1095a30e-2bdb-4661-82db-79a17c6c5efc"}},{"source":"input_dim = X_train.shape[1]\nencoding_dim = 14","execution_count":null,"cell_type":"code","metadata":{"_uuid":"b8a9c37b99b1775a8ebc36c67a57510c3a9252fa","collapsed":true,"_cell_guid":"7f85167c-50ba-4e16-b901-eee9821063d3"},"outputs":[]},{"source":"input_layer = Input(shape=(input_dim, ))\n\nencoder = Dense(encoding_dim, activation=\"tanh\", \n                activity_regularizer=regularizers.l1(10e-5))(input_layer)\nencoder = Dense(int(encoding_dim / 2), activation=\"relu\")(encoder)\n\ndecoder = Dense(int(encoding_dim / 2), activation='tanh')(encoder)\ndecoder = Dense(input_dim, activation='relu')(decoder)\n\nautoencoder = Model(inputs=input_layer, outputs=decoder)","execution_count":null,"cell_type":"code","metadata":{"_uuid":"760a5ac69247ae5ebe9676c91a7e2d8ccfcdc1d1","collapsed":true,"_cell_guid":"d5fd75bc-75ab-4cab-9285-862a65578c10"},"outputs":[]},{"source":"nb_epoch = 100\nbatch_size = 32\n\nautoencoder.compile(optimizer='adam', \n                    loss='mean_squared_error', \n                    metrics=['accuracy'])\n\ncheckpointer = ModelCheckpoint(filepath=\"model.h5\",\n                               verbose=0,\n                               save_best_only=True)\ntensorboard = TensorBoard(log_dir='./logs',\n                          histogram_freq=0,\n                          write_graph=True,\n                          write_images=True)\n\nhistory = autoencoder.fit(X_train, X_train,\n                    epochs=nb_epoch,\n                    batch_size=batch_size,\n                    shuffle=True,\n                    validation_data=(X_test, X_test),\n                    verbose=1,\n                    callbacks=[checkpointer, tensorboard]).history","execution_count":null,"cell_type":"code","metadata":{"scrolled":false,"_uuid":"025b0218858f700e1d5f6aa986bc7184838be59c","_cell_guid":"f94b0b96-12f1-4fc6-ac50-e4e04b7567b2"},"outputs":[]},{"source":"autoencoder = load_model('model.h5')","execution_count":null,"cell_type":"code","metadata":{"_uuid":"74c8bbcbacdb134ad61c4fe54bdbbf06e1459d86","collapsed":true,"_cell_guid":"30384add-4700-4084-983f-fe02e1effbc9"},"outputs":[]},{"source":"# Evaluate the Model","cell_type":"markdown","metadata":{"_uuid":"135d67f70737840367462cb09110f9455a11749e","_cell_guid":"9add179c-c60c-425e-98fa-accc87096fce"}},{"source":"plt.plot(history['loss'])\nplt.plot(history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper right');","execution_count":null,"cell_type":"code","metadata":{"_uuid":"38a6c8ea3e8989c08bb6d697f2e09905dffe86f9","_cell_guid":"8580fa90-01b7-4894-a126-37ccc149e458"},"outputs":[]},{"source":"predictions = autoencoder.predict(X_test)","execution_count":null,"cell_type":"code","metadata":{"_uuid":"dd2aeb568944d234651e930102892a17249eb72e","collapsed":true,"_cell_guid":"acfadaa5-91a3-4e88-a12d-7805a3523813"},"outputs":[]},{"source":"mse = np.mean(np.power(X_test - predictions, 2), axis=1)\nerror_df = pd.DataFrame({'reconstruction_error': mse,\n                        'true_class': y_test})","execution_count":null,"cell_type":"code","metadata":{"_uuid":"9f0a8fbd2d1c1e2643960d7dee2dfa55383c84f9","collapsed":true,"_cell_guid":"e244b189-cccf-443e-95b1-1b504cd767c0"},"outputs":[]},{"source":"error_df.describe()","execution_count":null,"cell_type":"code","metadata":{"_uuid":"3578fb3568fcba97406a4ad1fb0839befbf6048d","_cell_guid":"e0e4cf8e-808d-44b1-a902-201edfe58cb0"},"outputs":[]},{"source":"fig = plt.figure()\nax = fig.add_subplot(111)\nnormal_error_df = error_df[(error_df['true_class']== 0) & (error_df['reconstruction_error'] < 10)]\n_ = ax.hist(normal_error_df.reconstruction_error.values, bins=10)\nplt.title('Reconstruction error without fraud')\nsns.despine()","execution_count":null,"cell_type":"code","metadata":{"_uuid":"e74ed5a635e7318b7fef01293c593680ea1c9672","_cell_guid":"14a9ed40-0687-4458-a569-ea0611e79066"},"outputs":[]},{"source":"fig = plt.figure()\nax = fig.add_subplot(111)\nfraud_error_df = error_df[error_df['true_class'] == 1]\n_ = ax.hist(fraud_error_df.reconstruction_error.values, bins=10)\nplt.title('Reconstruction error with fraud')\nsns.despine()","execution_count":null,"cell_type":"code","metadata":{"_uuid":"059bf0d1b3311470c3cae44c237052e83c6033ee","_cell_guid":"3cc7e9c5-f97a-4055-8112-c4bcf34a6a20"},"outputs":[]},{"source":"from sklearn.metrics import (confusion_matrix, precision_recall_curve, auc,\n                             roc_curve, recall_score, classification_report, f1_score,\n                             precision_recall_fscore_support)","execution_count":null,"cell_type":"code","metadata":{"_uuid":"603728abbe2dd9fd4347ec5485233a5af4c1ee0f","collapsed":true,"_cell_guid":"1fe82725-135b-4107-b58e-980dd44e3d35"},"outputs":[]},{"source":"fpr, tpr, thresholds = roc_curve(error_df.true_class, error_df.reconstruction_error)\nroc_auc = auc(fpr, tpr)\n\nplt.title('Receiver Operating Characteristic')\nplt.plot(fpr, tpr, label='AUC = %0.4f'% roc_auc)\nplt.legend(loc='lower right')\nplt.plot([0,1],[0,1],'r--')\nplt.xlim([-0.001, 1])\nplt.ylim([0, 1.001])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show();","execution_count":null,"cell_type":"code","metadata":{"_uuid":"65b32585322b32c837cf69a9f46308c5266dc068","_cell_guid":"1359fb43-c33c-492e-af55-e8a151867cd9"},"outputs":[]},{"source":"precision, recall, th = precision_recall_curve(error_df.true_class, error_df.reconstruction_error)\nplt.plot(recall, precision, 'b', label='Precision-Recall curve')\nplt.title('Recall vs Precision')\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.show()","execution_count":null,"cell_type":"code","metadata":{"_uuid":"0c9ceededb3ce81e9708e349c5f95c15540db4b2","_cell_guid":"82e80b6b-5629-4aa2-9b3b-5f19c159bd14"},"outputs":[]},{"source":"plt.plot(th, precision[1:], 'b', label='Threshold-Precision curve')\nplt.title('Precision for different threshold values')\nplt.xlabel('Threshold')\nplt.ylabel('Precision')\nplt.show()","execution_count":null,"cell_type":"code","metadata":{"_uuid":"68ec29e59ffd13d6a616ed8fd223b708ac012f93","_cell_guid":"e22b457d-1d3a-405d-b599-5b0a18a234ab"},"outputs":[]},{"source":"plt.plot(th, recall[1:], 'b', label='Threshold-Recall curve')\nplt.title('Recall for different threshold values')\nplt.xlabel('Reconstruction error')\nplt.ylabel('Recall')\nplt.show()","execution_count":null,"cell_type":"code","metadata":{"_uuid":"63c35818d8658ba4773a72e64d7e2941fbf4bb60","_cell_guid":"7b4b6d81-30b3-4977-b130-c6b434d58759"},"outputs":[]},{"source":"# Prediction","cell_type":"markdown","metadata":{"_uuid":"fde86829e9efe58cce3c0d5d1181dcdb8a703136","_cell_guid":"b8246f8e-10fb-4093-b90a-f85edaba723e"}},{"source":"threshold = 2.9","execution_count":null,"cell_type":"code","metadata":{"_uuid":"0b88beb36b33ccebcdfefc80ee71a4e1f16da9eb","collapsed":true,"_cell_guid":"ad6c4ba9-157d-42bc-94bb-6e021faf7438"},"outputs":[]},{"source":"groups = error_df.groupby('true_class')\nfig, ax = plt.subplots()\n\nfor name, group in groups:\n    ax.plot(group.index, group.reconstruction_error, marker='o', ms=3.5, linestyle='',\n            label= \"churn\" if name == 1 else \"not churn\")\nax.hlines(threshold, ax.get_xlim()[0], ax.get_xlim()[1], colors=\"r\", zorder=100, label='Threshold')\nax.legend()\nplt.title(\"Reconstruction error for different classes\")\nplt.ylabel(\"Reconstruction error\")\nplt.xlabel(\"Data point index\")\nsns.despine()\nplt.show();","execution_count":null,"cell_type":"code","metadata":{"_uuid":"8c9fc84b5767744009a9a4de653a01a343b289df","_cell_guid":"63ef41f3-4bf2-42fa-bc0d-7abc8f79e87a"},"outputs":[]},{"source":"LABELS = ['churn', 'not churn']\n\ny_pred = [1 if e > threshold else 0 for e in error_df.reconstruction_error.values]\nconf_matrix = confusion_matrix(error_df.true_class, y_pred)\n\nplt.figure(figsize=(12, 12))\nsns.heatmap(conf_matrix, xticklabels=LABELS, yticklabels=LABELS, annot=True, fmt=\"d\");\nplt.title(\"Confusion matrix\")\nplt.ylabel('True class')\nplt.xlabel('Predicted class')\nplt.show()","execution_count":null,"cell_type":"code","metadata":{"_uuid":"7836721427595436909ecfef9d26b15886056d00","_cell_guid":"9b963411-257e-4bbd-b472-b26abbf0ac73"},"outputs":[]},{"source":"","execution_count":null,"cell_type":"code","metadata":{"_uuid":"f4d9eebe1fd131a7e5805468d07139589c85d2c2","collapsed":true,"_cell_guid":"e2eb55ff-8b42-4a11-b734-2a728bd89127"},"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3","language":"python"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","pygments_lexer":"ipython3","mimetype":"text/x-python","name":"python","version":"3.6.3","nbconvert_exporter":"python"}}}