{"cells":[{"metadata":{},"cell_type":"markdown","source":"While stage 1 is running, it is easy to create models that are artificially too good to be true. A quick look at the leaderboard shows that, [as every single time](https://www.kaggle.com/c/google-cloud-ncaa-march-madness-2020-division-1-mens-tournament/discussion/130649), a lot of us found a way to submit a perfect score.\n\nThis notebook is to give you some ideas on how to create a way to benchmark your models for both the classic competitions (where we predict winning probabilities) and the new ones (where we predict point spread)\n\nWe will make use of [**TubesML**](https://pypi.org/project/tubesml/), which helps in not worrying about information leakage during the validation process, no matter how complex the model pipeline gets (and it helps me developing and getting to the next release faster)"},{"metadata":{"_kg_hide-input":false,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"!pip install tubesml==0.2.0","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nimport tubesml as tml\n\nfrom sklearn.model_selection import StratifiedShuffleSplit, GridSearchCV, train_test_split\nfrom sklearn.linear_model import Ridge, LogisticRegression\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import accuracy_score, mean_squared_error, mean_absolute_error, log_loss\nfrom sklearn.model_selection import KFold\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nimport mm_data_manipulation as mm\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Quick data preparation\n\nThe goal of the notebook is just to give some ideas about model validation, not to build an actual model. Therefore, let's just make a simple training dataset, what follows should work with any training set and any model."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"def make_training_data(details, targets):\n    tmp = details.copy()\n    tmp.columns = ['Season', 'Team1'] + \\\n                ['T1_'+col for col in tmp.columns if col not in ['Season', 'TeamID']]\n    total = pd.merge(targets, tmp, on=['Season', 'Team1'], how='left')\n\n    tmp = details.copy()\n    tmp.columns = ['Season', 'Team2'] + \\\n                ['T2_'+col for col in tmp.columns if col not in ['Season', 'TeamID']]\n    total = pd.merge(total, tmp, on=['Season', 'Team2'], how='left')\n    \n    if total.isnull().any().any():\n        raise ValueError('Something went wrong')\n        \n    stats = [col[3:] for col in total.columns if 'T1_' in col and 'region' not in col]\n\n    for stat in stats:\n        total['delta_'+stat] = total['T1_'+stat] - total['T2_'+stat]\n        \n    try:\n        total['delta_off_edge'] = total['T1_off_rating'] - total['T2_def_rating']\n        total['delta_def_edge'] = total['T2_off_rating'] - total['T1_def_rating']\n    except KeyError:\n        pass\n        \n    return total\n\n\ndef add_seed(seed_location, total):\n    seed_data = pd.read_csv(seed_location)\n    seed_data['region'] = seed_data['Seed'].apply(lambda x: x[0])\n    seed_data['Seed'] = seed_data['Seed'].apply(lambda x: int(x[1:3]))\n    total = pd.merge(total, seed_data, how='left', on=['TeamID', 'Season'])\n    return total\n\n\ndef make_teams_target(data, league):\n    if league == 'men':\n        limit = 2003\n    else:\n        limit = 2010\n\n    df = data[data.Season >= limit].copy()\n\n    df['Team1'] = np.where((df.WTeamID < df.LTeamID), df.WTeamID, df.LTeamID)\n    df['Team2'] = np.where((df.WTeamID > df.LTeamID), df.WTeamID, df.LTeamID)\n    df['target'] = np.where((df['WTeamID'] < df['LTeamID']),1,0)\n    df['target_points'] = np.where((df['WTeamID'] < df['LTeamID']),df.WScore - df.LScore,df.LScore - df.WScore)\n    df.loc[df.WLoc == 'N', 'LLoc'] = 'N'\n    df.loc[df.WLoc == 'H', 'LLoc'] = 'A'\n    df.loc[df.WLoc == 'A', 'LLoc'] = 'H'\n    df['T1_Loc'] = np.where((df.WTeamID < df.LTeamID), df.WLoc, df.LLoc)\n    df['T2_Loc'] = np.where((df.WTeamID > df.LTeamID), df.WLoc, df.LLoc)\n    df['T1_Loc'] = df['T1_Loc'].map({'H': 1, 'A': -1, 'N': 0})\n    df['T2_Loc'] = df['T2_Loc'].map({'H': 1, 'A': -1, 'N': 0})\n\n    reverse = data[data.Season >= limit].copy()\n    reverse['Team1'] = np.where((reverse.WTeamID > reverse.LTeamID), reverse.WTeamID, reverse.LTeamID)\n    reverse['Team2'] = np.where((reverse.WTeamID < reverse.LTeamID), reverse.WTeamID, reverse.LTeamID)\n    reverse['target'] = np.where((reverse['WTeamID'] > reverse['LTeamID']),1,0)\n    reverse['target_points'] = np.where((reverse['WTeamID'] > reverse['LTeamID']),\n                                        reverse.WScore - reverse.LScore,\n                                        reverse.LScore - reverse.WScore)\n    reverse.loc[reverse.WLoc == 'N', 'LLoc'] = 'N'\n    reverse.loc[reverse.WLoc == 'H', 'LLoc'] = 'A'\n    reverse.loc[reverse.WLoc == 'A', 'LLoc'] = 'H'\n    reverse['T1_Loc'] = np.where((reverse.WTeamID > reverse.LTeamID), reverse.WLoc, reverse.LLoc)\n    reverse['T2_Loc'] = np.where((reverse.WTeamID < reverse.LTeamID), reverse.WLoc, reverse.LLoc)\n    reverse['T1_Loc'] = reverse['T1_Loc'].map({'H': 1, 'A': -1, 'N': 0})\n    reverse['T2_Loc'] = reverse['T2_Loc'].map({'H': 1, 'A': -1, 'N': 0})\n    \n    df = pd.concat([df, reverse], ignore_index=True)\n\n    to_drop = ['WScore','WTeamID', 'LTeamID', 'LScore', 'WLoc', 'LLoc', 'NumOT']\n    for col in to_drop:\n        del df[col]\n    \n    df.loc[:,'ID'] = df.Season.astype(str) + '_' + df.Team1.astype(str) + '_' + df.Team2.astype(str)\n    return df\n\n\ndef prepare_data(league):\n    save_loc = 'processed_data/' + league + '/'\n\n    if league == 'women':\n        regular_season = '/kaggle/input/ncaaw-march-mania-2021-spread/WRegularSeasonDetailedResults.csv'\n        playoff = '/kaggle/input/ncaaw-march-mania-2021/WNCAATourneyDetailedResults.csv'\n        playoff_compact = '/kaggle/input/ncaaw-march-mania-2021/WNCAATourneyCompactResults.csv'\n        seed = '/kaggle/input/ncaaw-march-mania-2021/WNCAATourneySeeds.csv'\n        save_loc = 'data/processed_women/'\n    else:\n        regular_season = '/kaggle/input/ncaam-march-mania-2021-spread/MRegularSeasonDetailedResults.csv'\n        playoff = '/kaggle/input/ncaam-march-mania-2021/MNCAATourneyDetailedResults.csv'\n        playoff_compact = '/kaggle/input/ncaam-march-mania-2021/MNCAATourneyCompactResults.csv'\n        seed = '/kaggle/input/ncaam-march-mania-2021/MNCAATourneySeeds.csv'\n        save_loc = 'data/processed_men/'\n    \n    # Season stats\n    reg = pd.read_csv(regular_season)\n    reg = mm.process_details(reg)\n    regular_stats = mm.full_stats(reg)\n    \n    regular_stats = add_seed(seed, regular_stats)    \n    \n    # Target data generation \n    target_data = pd.read_csv(playoff_compact)\n    target_data = make_teams_target(target_data, league)\n    \n    all_reg = make_training_data(regular_stats, target_data)\n    all_reg = all_reg[all_reg.DayNum >= 136]  # remove pre tourney \n    \n    return all_reg","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"train_men = prepare_data('men')[['Season', 'target', 'target_points', 'ID', 'delta_Seed', 'delta_Score']]\ntrain_women = prepare_data('women')[['Season', 'target', 'target_points', 'ID', 'delta_Seed', 'delta_Score']]\ntrain_men.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Random split of data\n\nThis method is fairly quick and the most basic one: set some test set aside, and evaluate your model. We make the following functions just to keep it clean."},{"metadata":{"trusted":true},"cell_type":"code","source":"def _clean_columns(train, test):\n    for col in ['target', 'target_points', 'ID', 'DayNum', \n                'Team1', 'Team2', 'Season', 'competitive', 'competitive_score']:\n        try:\n            del train[col]\n            del test[col]\n        except KeyError:\n            pass\n    return train, test\n\n\ndef _make_preds(train, y_train, test, model, kfolds, predict_proba):\n    # this function can be made fancier with, for example, the usual kfold \n    # with early stopping and prediction on the test set\n    # We keep it simpler here\n    \n    oof, imp_coef = tml.cv_score(data=train, target=y_train, estimator=model, \n                                 cv=kfolds, imp_coef=True, predict_proba=predict_proba)\n    \n    fit_model = model.fit(train, y_train)\n    if predict_proba:\n        predictions = fit_model.predict_proba(test)[:,1]\n    else:\n        predictions = fit_model.predict(test)\n    \n    return fit_model, oof, imp_coef, predictions\n\n\ndef random_split(data, model, kfolds, target, test_size=0.2, predict_proba=False, tune=False, param_grid=None):\n    \n    # split the data, it is possible to stratify on the years\n    train, test = tml.make_test(data, test_size=test_size, strat_feat='Season', random_state=324)\n    \n    y_train = train[target]\n    y_test = test[target]\n    \n    # make sure unwanted columns are not there\n    train, test = _clean_columns(train, test)\n    \n    if tune:  # optional if you like it\n        if predict_proba:\n            grid = GridSearchCV(model, param_grid=param_grid, n_jobs=-1, \n                                cv=5, scoring='neg_log_loss')\n        else:\n            grid = GridSearchCV(model, param_grid=param_grid, n_jobs=-1, \n                                cv=5, scoring='neg_mean_absolute_error')\n        grid.fit(train, y_train)\n        model = grid.best_estimator_\n        print(grid.best_score_)\n        print(grid.best_params_)\n    \n    # Cross validation with Kfold on train set + retraining and prediction on the test set\n    fit_model, oof, imp_coef, predictions = _make_preds(train, y_train, test, model, kfolds, predict_proba)\n    \n    return fit_model, oof, predictions, imp_coef, train, y_train, test, y_test","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can then build a very simple pipeline and predict **the point spread** with this validation set up"},{"metadata":{"trusted":true},"cell_type":"code","source":"pipe = Pipeline([('scl', tml.DfScaler()), ('ridge', Ridge())])\n\nkfolds = KFold(n_splits=5, shuffle=True, random_state=345)\n\nfitted, oof_pred, test_pred, imp_coef, train, y_train, test, y_test = random_split(train_men, pipe, kfolds, 'target_points')\n\nimp_coef","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Perfect, now we just need a function to evaluate what we have produced"},{"metadata":{"trusted":true},"cell_type":"code","source":"def report_points(train, test, y_train, y_test, oof, preds, plot=True):\n    mae_oof = round(mean_absolute_error(y_true=y_train, y_pred=oof), 4)\n    mae_test = round(mean_absolute_error(y_true=y_test, y_pred=preds), 4)\n    mse_oof = round(np.sqrt(mean_squared_error(y_true=y_train, y_pred=oof)), 4)\n    mse_test = round(np.sqrt(mean_squared_error(y_true=y_test, y_pred=preds)), 4)\n    acc_oof = round(accuracy_score(y_true=(y_train>0).astype(int), y_pred=(oof>0).astype(int)),4)\n    acc_test = round(accuracy_score(y_true=(y_test>0).astype(int), y_pred=(preds>0).astype(int)),4)\n    n_unsure_oof = round((abs(oof) < 3).mean() * 100, 2)\n    n_unsure_test = round((abs(preds) < 3).mean() * 100, 2)\n\n    if plot:\n        # plot predictions\n        tml.plot_regression_predictions(train, y_train, oof)\n        tml.plot_regression_predictions(test, y_test, preds)\n    \n    print(f'MAE train: \\t\\t\\t {mae_oof}')\n    print(f'MAE test: \\t\\t\\t {mae_test}')\n    print(f'RMSE train: \\t\\t\\t {mse_oof}')\n    print(f'RMSE test: \\t\\t\\t {mse_test}')\n    print(f'Accuracy train: \\t\\t {acc_oof}')\n    print(f'Accuracy test: \\t\\t\\t {acc_test}')\n    print(f'Unsure train: \\t\\t\\t {n_unsure_oof}%')\n    print(f'Unsure test: \\t\\t\\t {n_unsure_test}%')\n    \n    \nreport_points(train, test, y_train, y_test, oof_pred, test_pred)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Not a great model, but we knew this already. \n\nThe set up works also to predict **the probability of winning**"},{"metadata":{"trusted":true},"cell_type":"code","source":"\npipe = Pipeline([('scl', tml.DfScaler()), \n                 ('logit', LogisticRegression(solver='lbfgs', multi_class='auto'))])\n\nfitted, oof_pred, test_pred, imp_coef, train, y_train, test, y_test = random_split(train_women, pipe, \n                                                                                   kfolds, 'target', \n                                                                                   predict_proba=True)\n\nimp_coef","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Which neads a slightly different function for reporting"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"def plot_pred_prob(oof, test, y_train, y_test):\n    \n    fig, ax = plt.subplots(1,2, figsize=(15, 6))\n    \n    df = pd.DataFrame()\n    df['true'] = np.where(y_train > 0, 1, 0)\n    df['Prediction'] = oof\n    \n    df[df.true==1]['Prediction'].hist(bins=50, ax=ax[0], alpha=0.5, color='g', label='Victory')\n    df[df.true==0]['Prediction'].hist(bins=50, ax=ax[0], alpha=0.5, color='r', label='Loss')\n    \n    df = pd.DataFrame()\n    df['true'] = np.where(y_test > 0, 1, 0)\n    df['Prediction'] = test\n\n    df[df.true==1]['Prediction'].hist(bins=50, ax=ax[1], alpha=0.5, color='g', label='Victory')\n    df[df.true==0]['Prediction'].hist(bins=50, ax=ax[1], alpha=0.5, color='r', label='Loss')\n    \n    ax[0].axvline(0.5, color='k', linestyle='--')\n    ax[1].axvline(0.5, color='k', linestyle='--')\n    \n    ax[0].set_title('Training data')\n    ax[1].set_title('Test data')\n    ax[0].grid(False)\n    ax[1].grid(False)\n    ax[0].legend()\n    ax[1].legend()\n    fig.suptitle('Probabilities of victory', fontsize=15)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def report_victory(y_train, y_test, oof, preds, probs=True):\n    \n    if probs:\n        acc_oof = round(accuracy_score(y_true=y_train, y_pred=(oof>0.5).astype(int)),4)\n        acc_test = round(accuracy_score(y_true=y_test, y_pred=(preds>0.5).astype(int)),4)\n        n_unsure_oof = round((abs(oof - 0.5) < 0.1).mean() * 100, 4)\n        n_unsure_test = round((abs(preds - 0.5) < 0.1).mean() * 100, 4)\n        logloss_oof = round(log_loss(y_true=y_train, y_pred=oof), 4)\n        logloss_test = round(log_loss(y_true=y_test, y_pred=preds), 4)\n        \n        plot_pred_prob(oof, preds, y_train, y_test)\n    \n    print(f'Accuracy train: \\t\\t {acc_oof}')\n    print(f'Accuracy test: \\t\\t\\t {acc_test}')\n    print(f'Logloss train: \\t\\t\\t {logloss_oof}')\n    print(f'Logloss test: \\t\\t\\t {logloss_test}')\n    print(f'Unsure train: \\t\\t\\t {n_unsure_oof}%')\n    print(f'Unsure test: \\t\\t\\t {n_unsure_test}%')\n    \nreport_victory(y_train, y_test, oof_pred, test_pred)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Yearly split of the data\n\nIn the simple train/test split we are also using future tournaments to predict on the past, which is quick but it doesn't arguably give a good read on how the models will do this year.\n\nA different validation strategy is to **simulate this competion** by only training with a set of years and prediction on the next one. \n\nThe results on the test set with this strategy can be easily compared with last year's competitions"},{"metadata":{"trusted":true},"cell_type":"code","source":"def yearly_split(data, model, kfolds, target, predict_proba=False, tune=False, param_grid=None):\n    \n    fit_model = {}\n    oof = {}\n    imp_coef = {}\n    train = {}\n    test = {}\n    y_train = {}\n    y_test = {}\n    predictions = {}\n    \n    years = [2015, 2016, 2017, 2018, 2019]\n    \n    for year in years:\n        yr = str(year)\n        train[yr] = data[data.Season < year].copy()\n        test[yr] = data[data.Season == year].copy()\n    \n        y_train[yr] = train[yr][target]\n        y_test[yr] = test[yr][target]\n\n        train[yr], test[yr] = _clean_columns(train[yr], test[yr])\n        \n        if tune:\n            if predict_proba:\n                grid = GridSearchCV(model, param_grid=param_grid, n_jobs=-1, \n                                    cv=5, scoring='neg_log_loss')\n            else:\n                grid = GridSearchCV(model, param_grid=param_grid, n_jobs=-1, \n                                    cv=5, scoring='neg_mean_absolute_error')\n            grid.fit(train[yr], y_train[yr])\n            model = grid.best_estimator_\n            print(grid.best_score_)\n            print(grid.best_params_)\n        \n        fit_model[yr], oof[yr], imp_coef[yr], predictions[yr] = _make_preds(train[yr], \n                                                                            y_train[yr], \n                                                                            test[yr], \n                                                                            model, \n                                                                            kfolds, \n                                                                            predict_proba)\n    \n    return fit_model, oof, predictions, imp_coef, train, y_train, test, y_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pipe = Pipeline([('scl', tml.DfScaler()), ('ridge', Ridge())])\n\nfitted, oof_pred, test_pred, imp_coef, train, y_train, test, y_test = yearly_split(train_men, pipe, kfolds, 'target_points')\n\nfitted.keys()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The function is returning a dictionary with the results year by year.\n\nWe can then leverage the previous function and just wrap them to see a report by year"},{"metadata":{"trusted":true},"cell_type":"code","source":"def yearly_wrapper(train, test, y_train, y_test, oof, preds, proba=False):\n    y_train_total = []\n    y_test_total = []\n    oof_total = []\n    preds_total = []\n    for yr in train.keys():\n        print(yr)\n        print('\\n')\n        if proba:\n            report_victory(y_train[yr], y_test[yr], oof[yr], preds[yr], probs=True)\n        else:\n            report_points(train[yr], test[yr], y_train[yr], y_test[yr], oof[yr], preds[yr], plot=False)\n        print('\\n')\n        print('_'*40)\n        print('\\n')\n        y_train_total.append(y_train[yr])\n        y_test_total.append(y_test[yr])\n        oof_total += list(oof[yr])\n        preds_total += list(preds[yr])\n        \n    print('Total predictions')\n    print('\\n')\n    y_train_total = pd.concat(y_train_total, ignore_index=True)\n    y_test_total = pd.concat(y_test_total, ignore_index=True)\n    oof_total = pd.Series(oof_total)\n    preds_total = pd.Series(preds_total)\n    if proba:\n        report_victory(y_train_total, y_test_total, oof_total, preds_total)\n    else:\n        report_points(train[yr], test[yr], y_train_total, y_test_total, oof_total, preds_total, plot=False)\n    \n\nyearly_wrapper(train, test, y_train, y_test, oof_pred, test_pred)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"And, again, we can have a similar report when predicting the probability of winning"},{"metadata":{"trusted":true},"cell_type":"code","source":"pipe = Pipeline([('scl', tml.DfScaler()), \n                 ('logit', LogisticRegression(solver='lbfgs', multi_class='auto'))])\n\nfitted, oof_pred, test_pred, imp_coef, train, y_train, test, y_test = yearly_split(train_women, pipe, kfolds, 'target', predict_proba=True)\n\nyearly_wrapper(train, test, y_train, y_test, oof_pred, test_pred, proba=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*Note: this model would have got you at the 213th position in the 2019's competition and as you can see it takes about 35 seconds of work.*\n\n*Another note: this model outscored my submission (with a coding error, but still) by more than 70 positions...*\n\n# Conclusion\n\nAppropriately validating your model will give enable you to take informed modeling decisions without worrying about your results looking unnaturally better than they really are. The two methods presented here are simple but effective in simulating how the model will possibly behave in stage 2, when you will need to wait the actual game to happen to know your score. Each of the functions here presented are merely showing the concept and it is not difficult to increase their complexity to give you better insights on how the model is performing.\n\nGood luck and enjoy the best yearly competitions on Kaggle!\n\n\nP.s. For an even better model evaluation, moving the processing of the data inside of a pipeline might look like a lot of work but it also a great skill to master for real ML applications. For examples in how to do so, you can read [this notebook](https://www.kaggle.com/lucabasa/understand-and-use-a-pipeline)"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}