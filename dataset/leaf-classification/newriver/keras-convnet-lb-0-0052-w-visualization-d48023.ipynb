{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"59b5a974-1bd8-f13a-c60f-6212d8690227"},"source":"**NOTE:** I managed to get an LB of 0.0052 with this code, but due to some randomness still in the script, the score varies between 0.016 and 0.005. Using some form of kfold-validation reduces this variance.\n\n**EDIT:** I would run this somewhere other than Kaggle locally for 150 epochs instead of 89 like I have it set to below; 89 is the best I could do without the script timing out.\n\n# The Idea\n\nI started this competition by simply feeding the pre-extracted features into a multi-layer perceptron with one hidden layer and got surprisingly good results, but I still had all this image data that I wasn't using. My immediate thought then was to simply combine a convolutional neural network on the images with the pre-extracted features MLP and train the entire model end to end. Keras's functional API gives us a really easy way to do this. Below, I'll outline the process of getting this model working along, point out some nice resources to learning about convolutional nets, and do some visualization of what the neural network is actually doing. But before we do that, let's just get all the data loading out of the way."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f1761cd9-7522-3d88-0d4e-c7fa6c1eafa9"},"outputs":[],"source":"import os\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import StratifiedShuffleSplit\n\n# If you want to use Theano, all you need to change\n# is the dim ordering whenever you are dealing with\n# the image array. Instead of\n# (samples, rows, cols, channels) it should be\n# (samples, channels, rows, cols)\n\n# Keras stuff\nfrom keras.utils.np_utils import to_categorical\nfrom keras.preprocessing.image import img_to_array, load_img\n\n# A large amount of the data loading code is based on najeebkhan's kernel\n# Check it out at https://www.kaggle.com/najeebkhan/leaf-classification/neural-network-through-keras\nroot = '../input'\nnp.random.seed(2016)\nsplit_random_state = 7\nsplit = .9\n\n\ndef load_numeric_training(standardize=True):\n    \"\"\"\n    Loads the pre-extracted features for the training data\n    and returns a tuple of the image ids, the data, and the labels\n    \"\"\"\n    # Read data from the CSV file\n    data = pd.read_csv(os.path.join(root, 'train.csv'))\n    ID = data.pop('id')\n\n    # Since the labels are textual, so we encode them categorically\n    y = data.pop('species')\n    y = LabelEncoder().fit(y).transform(y)\n    # standardize the data by setting the mean to 0 and std to 1\n    X = StandardScaler().fit(data).transform(data) if standardize else data.values\n\n    return ID, X, y\n\n\ndef load_numeric_test(standardize=True):\n    \"\"\"\n    Loads the pre-extracted features for the test data\n    and returns a tuple of the image ids, the data\n    \"\"\"\n    test = pd.read_csv(os.path.join(root, 'test.csv'))\n    ID = test.pop('id')\n    # standardize the data by setting the mean to 0 and std to 1\n    test = StandardScaler().fit(test).transform(test) if standardize else test.values\n    return ID, test\n\n\ndef resize_img(img, max_dim=96):\n    \"\"\"\n    Resize the image to so the maximum side is of size max_dim\n    Returns a new image of the right size\n    \"\"\"\n    # Get the axis with the larger dimension\n    max_ax = max((0, 1), key=lambda i: img.size[i])\n    # Scale both axes so the image's largest dimension is max_dim\n    scale = max_dim / float(img.size[max_ax])\n    return img.resize((int(img.size[0] * scale), int(img.size[1] * scale)))\n\n\ndef load_image_data(ids, max_dim=96, center=True):\n    \"\"\"\n    Takes as input an array of image ids and loads the images as numpy\n    arrays with the images resized so the longest side is max-dim length.\n    If center is True, then will place the image in the center of\n    the output array, otherwise it will be placed at the top-left corner.\n    \"\"\"\n    # Initialize the output array\n    # NOTE: Theano users comment line below and\n    X = np.empty((len(ids), max_dim, max_dim, 1))\n    # X = np.empty((len(ids), 1, max_dim, max_dim)) # uncomment this\n    for i, idee in enumerate(ids):\n        # Turn the image into an array\n        x = resize_img(load_img(os.path.join(root, 'images', str(idee) + '.jpg'), grayscale=True), max_dim=max_dim)\n        x = img_to_array(x)\n        # Get the corners of the bounding box for the image\n        # NOTE: Theano users comment the two lines below and\n        length = x.shape[0]\n        width = x.shape[1]\n        # length = x.shape[1] # uncomment this\n        # width = x.shape[2] # uncomment this\n        if center:\n            h1 = int((max_dim - length) / 2)\n            h2 = h1 + length\n            w1 = int((max_dim - width) / 2)\n            w2 = w1 + width\n        else:\n            h1, w1 = 0, 0\n            h2, w2 = (length, width)\n        # Insert into image matrix\n        # NOTE: Theano users comment line below and\n        X[i, h1:h2, w1:w2, 0:1] = x\n        # X[i, 0:1, h1:h2, w1:w2] = x  # uncomment this\n    # Scale the array values so they are between 0 and 1\n    return np.around(X / 255.0)\n\n\ndef load_train_data(split=split, random_state=None):\n    \"\"\"\n    Loads the pre-extracted feature and image training data and\n    splits them into training and cross-validation.\n    Returns one tuple for the training data and one for the validation\n    data. Each tuple is in the order pre-extracted features, images,\n    and labels.\n    \"\"\"\n    # Load the pre-extracted features\n    ID, X_num_tr, y = load_numeric_training()\n    # Load the image data\n    X_img_tr = load_image_data(ID)\n    # Split them into validation and cross-validation\n    sss = StratifiedShuffleSplit(n_splits=1, train_size=split, random_state=random_state)\n    train_ind, test_ind = next(sss.split(X_num_tr, y))\n    X_num_val, X_img_val, y_val = X_num_tr[test_ind], X_img_tr[test_ind], y[test_ind]\n    X_num_tr, X_img_tr, y_tr = X_num_tr[train_ind], X_img_tr[train_ind], y[train_ind]\n    return (X_num_tr, X_img_tr, y_tr), (X_num_val, X_img_val, y_val)\n\n\ndef load_test_data():\n    \"\"\"\n    Loads the pre-extracted feature and image test data.\n    Returns a tuple in the order ids, pre-extracted features,\n    and images.\n    \"\"\"\n    # Load the pre-extracted features\n    ID, X_num_te = load_numeric_test()\n    # Load the image data\n    X_img_te = load_image_data(ID)\n    return ID, X_num_te, X_img_te\n\nprint('Loading the training data...')\n(X_num_tr, X_img_tr, y_tr), (X_num_val, X_img_val, y_val) = load_train_data(random_state=split_random_state)\ny_tr_cat = to_categorical(y_tr)\ny_val_cat = to_categorical(y_val)\nprint('Training data loaded!')"},{"cell_type":"markdown","metadata":{"_cell_guid":"cc89019d-f83c-5be3-f2e2-dc51318f9a76"},"source":"# Data Augmentation\n\nOne trick we are going to use to improve the robustness of our model is image data augmentation, allowing it to perform better on the test set. \n\nIf you take a look at [Rhyando Anggoro Adi's post](https://www.kaggle.com/c/leaf-classification/forums/t/24764/create-gif-based-on-leaf-class) on the forum containing a GIF of each training sample for each species, you'll notice that for a given species most of the leaves look very similar except that the leaf is rotated slightly or is slightly larger in scale. We'll try to emphasize this in our dataset by randomly performing a rotation or zoom transformation to each leaf image as the image is passed to the neural network. Below is the code for the data augmentation image generator along with a slight change to the source code to help us out later on.\n\n**NOTE:** the change to the source code is not the only way to get around the problem of matching the indices of our two inputs (images and pre-extracted features). You can also manually shuffle the indices, set the shuffle parameter for the ImageDataGenerator to False, and flow the generator from the manually shuffled images."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"93d3a473-43bf-038c-2569-a5d2d85b61a6"},"outputs":[],"source":"from keras.preprocessing.image import ImageDataGenerator, NumpyArrayIterator, array_to_img\n\n# A little hacky piece of code to get access to the indices of the images\n# the data augmenter is working with.\nclass ImageDataGenerator2(ImageDataGenerator):\n    def flow(self, X, y=None, batch_size=32, shuffle=True, seed=None,\n             save_to_dir=None, save_prefix='', save_format='jpeg'):\n        return NumpyArrayIterator2(\n            X, y, self,\n            batch_size=batch_size, shuffle=shuffle, seed=seed,\n            dim_ordering=self.dim_ordering,\n            save_to_dir=save_to_dir, save_prefix=save_prefix, save_format=save_format)\n\n\nclass NumpyArrayIterator2(NumpyArrayIterator):\n    def next(self):\n        # for python 2.x.\n        # Keeps under lock only the mechanism which advances\n        # the indexing of each batch\n        # see http://anandology.com/blog/using-iterators-and-generators/\n        with self.lock:\n            # We changed index_array to self.index_array\n            self.index_array, current_index, current_batch_size = next(self.index_generator)\n        # The transformation of images is not under thread lock so it can be done in parallel\n        batch_x = np.zeros(tuple([current_batch_size] + list(self.X.shape)[1:]))\n        for i, j in enumerate(self.index_array):\n            x = self.X[j]\n            x = self.image_data_generator.random_transform(x.astype('float32'))\n            x = self.image_data_generator.standardize(x)\n            batch_x[i] = x\n        if self.save_to_dir:\n            for i in range(current_batch_size):\n                img = array_to_img(batch_x[i], self.dim_ordering, scale=True)\n                fname = '{prefix}_{index}_{hash}.{format}'.format(prefix=self.save_prefix,\n                                                                  index=current_index + i,\n                                                                  hash=np.random.randint(1e4),\n                                                                  format=self.save_format)\n                img.save(os.path.join(self.save_to_dir, fname))\n        if self.y is None:\n            return batch_x\n        batch_y = self.y[self.index_array]\n        return batch_x, batch_y\n\nprint('Creating Data Augmenter...')\nimgen = ImageDataGenerator2(\n    rotation_range=20,\n    zoom_range=0.2,\n    horizontal_flip=True,\n    vertical_flip=True,\n    fill_mode='nearest')\nimgen_train = imgen.flow(X_img_tr, y_tr_cat, seed=np.random.randint(1, 10000))\nprint('Finished making data augmenter...')"},{"cell_type":"markdown","metadata":{"_cell_guid":"38d9594b-3efc-e653-7746-cc8100773f43"},"source":"# Combining the Image CNN with the Pre-Extracted Features MLP\n\nNow that we've gotten all the data preparation work out of the way, we can actually construct our model.\n\n## Wait!! I don't know what a convolutional neural network is!\n\nNo worries! I've linked below a few great places to get an overview of convnets. You can find more by just googling \"convolutional neural network explained\".\n\n* http://cs231n.github.io/convolutional-networks/\n* https://ujjwalkarn.me/2016/08/11/intuitive-explanation-convnets/\n* http://neuralnetworksanddeeplearning.com/chap6.html\n\n## Keras Functional API\n\nFor basic neural network architectures we can use Keras's Sequential API, but since we need to build a model that takes two different inputs (image and pre-extracted features) in two different locations in the model, we won't be able to use the Sequential API. Instead, we'll be using the Functional API. This API is just as straightforward, but instead of having a model we add layers to, we'll instead be passing an array through a layer, and passing that output through another layer, and so on. You can think of each layer as a function and the array we give it as its argument. Click [here](https://keras.io/getting-started/functional-api-guide/) for more info about the functional API."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a7a75734-2306-d300-c4ac-f4b910aecf84"},"outputs":[],"source":"from keras.models import Model\nfrom keras.layers import Dense, Dropout, Activation, Convolution2D, MaxPooling2D, Flatten, Input, merge\n\n\ndef combined_model():\n\n    # Define the image input\n    image = Input(shape=(96, 96, 1), name='image')\n    # Pass it through the first convolutional layer\n    x = Convolution2D(8, 5, 5, input_shape=(96, 96, 1), border_mode='same')(image)\n    x = (Activation('relu'))(x)\n    x = (MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))(x)\n\n    # Now through the second convolutional layer\n    x = (Convolution2D(32, 5, 5, border_mode='same'))(x)\n    x = (Activation('relu'))(x)\n    x = (MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))(x)\n\n    # Flatten our array\n    x = Flatten()(x)\n    # Define the pre-extracted feature input\n    numerical = Input(shape=(192,), name='numerical')\n    # Concatenate the output of our convnet with our pre-extracted feature input\n    concatenated = merge([x, numerical], mode='concat')\n\n    # Add a fully connected layer just like in a normal MLP\n    x = Dense(100, activation='relu')(concatenated)\n    x = Dropout(.5)(x)\n\n    # Get the final output\n    out = Dense(99, activation='softmax')(x)\n    # How we create models with the Functional API\n    model = Model(input=[image, numerical], output=out)\n    model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n\n    return model\n\nprint('Creating the model...')\nmodel = combined_model()\nprint('Model created!')"},{"cell_type":"markdown","metadata":{"_cell_guid":"89ff2bfa-f039-78c7-dddc-18f1551ba182"},"source":"Now we're finally ready to actually train the model! Running on Kaggle will take a while. It's MUCH faster to run it locally if you have a GPU, or on an AWS instance with a GPU."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"ace7901a-a7eb-9e45-9cc7-e8cc5a9ac1f9"},"outputs":[],"source":"from keras.callbacks import ModelCheckpoint\nfrom keras.models import load_model\n\n\ndef combined_generator(imgen, X):\n    \"\"\"\n    A generator to train our keras neural network. It\n    takes the image augmenter generator and the array\n    of the pre-extracted features.\n    It yields a minibatch and will run indefinitely\n    \"\"\"\n    while True:\n        for i in range(X.shape[0]):\n            # Get the image batch and labels\n            batch_img, batch_y = next(imgen)\n            # This is where that change to the source code we\n            # made will come in handy. We can now access the indicies\n            # of the images that imgen gave us.\n            x = X[imgen.index_array]\n            yield [batch_img, x], batch_y\n\n# autosave best Model\nbest_model_file = \"leafnet.h5\"\nbest_model = ModelCheckpoint(best_model_file, monitor='val_loss', verbose=1, save_best_only=True)\n\nprint('Training model...')\nhistory = model.fit_generator(combined_generator(imgen_train, X_num_tr),\n                              samples_per_epoch=X_num_tr.shape[0],\n                              nb_epoch=89,\n                              validation_data=([X_img_val, X_num_val], y_val_cat),\n                              nb_val_samples=X_num_val.shape[0],\n                              verbose=0,\n                              callbacks=[best_model])\n\nprint('Loading the best model...')\nmodel = load_model(best_model_file)\nprint('Best Model loaded!')"},{"cell_type":"markdown","metadata":{"_cell_guid":"e6bbbe7b-f2b2-f049-f9e0-78e84bf44370"},"source":"And now we create our submission. From the last version's submission created from running this on Kaggle I got a 0.01672 LB, but I had managed to get a 0.00520 LB score with this exact same code after running for 100 epochs (though the best model occurred at the 89th epoch for me) on an AWS p2.xlarge instance. I did set the random seeds but there is still randomness somewhere. This variance in the results could definitely be improved upon with some k-fold validation, but I'll leave the implementation up to the reader."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"6e413e66-b449-01a1-0ed8-f92e2257f1a3"},"outputs":[],"source":"# Get the names of the column headers\nLABELS = sorted(pd.read_csv(os.path.join(root, 'train.csv')).species.unique())\n\nindex, test, X_img_te = load_test_data()\n\nyPred_proba = model.predict([X_img_te, test])\n\n# Converting the test predictions in a dataframe as depicted by sample submission\nyPred = pd.DataFrame(yPred_proba,index=index,columns=LABELS)\n\nprint('Creating and writing submission...')\nfp = open('submit.csv', 'w')\nfp.write(yPred.to_csv())\nprint('Finished writing submission')\n# Display the submission\nyPred.tail()"},{"cell_type":"markdown","metadata":{"_cell_guid":"5c93a6aa-81b0-b87b-d290-d17c53b4a56c"},"source":"# Visualization\n\nGreat! So we've got our combined model working that incorporates both the raw binary images of the leaves and the pre-extracted features. But you might ask now, what is the neural network actually learning? One easy way to tell what the convolutional portion of the neural net is learning is through visualization of the hidden layers. First, we'll pick a few random leaves from our validation set and we'll pass each one through the neural network. As the leaf goes through, the convolutional neural net will apply many filters each looking for something in the image. Once the filter is applied we'll grab the new image of the leaf and the white portions of the image will tell us where the filter activated and the black will tell us where it didn't. If you take a look at our architecture for the neural net, you'll notice we created 8 filters for the first convolutional layer and 32 for the second one. Thus, for each leaf image we should get a set of 8 and another set of 32 new images.\n\nTo do this in Keras we'll build a Keras function as outlined in the [Keras FAQ](https://keras.io/getting-started/faq/#how-can-i-visualize-the-output-of-an-intermediate-layer)."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"36464aad-bc82-924f-d19c-a11759c99685"},"outputs":[],"source":"from math import sqrt\n\nimport matplotlib.pyplot as plt\nfrom keras import backend as K\n\nNUM_LEAVES = 3\nmodel_fn = 'leafnet.h5'\n\n# Function by gcalmettes from http://stackoverflow.com/questions/11159436/multiple-figures-in-a-single-window\ndef plot_figures(figures, nrows = 1, ncols=1, titles=False):\n    \"\"\"Plot a dictionary of figures.\n\n    Parameters\n    ----------\n    figures : <title, figure> dictionary\n    ncols : number of columns of subplots wanted in the display\n    nrows : number of rows of subplots wanted in the figure\n    \"\"\"\n\n    fig, axeslist = plt.subplots(ncols=ncols, nrows=nrows)\n    for ind,title in enumerate(sorted(figures.keys(), key=lambda s: int(s[3:]))):\n        axeslist.ravel()[ind].imshow(figures[title], cmap=plt.gray())\n        if titles:\n            axeslist.ravel()[ind].set_title(title)\n\n    for ind in range(nrows*ncols):\n        axeslist.ravel()[ind].set_axis_off()\n\n    if titles:\n        plt.tight_layout()\n    plt.show()\n\n\ndef get_dim(num):\n    \"\"\"\n    Simple function to get the dimensions of a square-ish shape for plotting\n    num images\n    \"\"\"\n\n    s = sqrt(num)\n    if round(s) < s:\n        return (int(s), int(s)+1)\n    else:\n        return (int(s)+1, int(s)+1)\n\n# Load the best model\nmodel = load_model(model_fn)\n\n# Get the convolutional layers\nconv_layers = [layer for layer in model.layers if isinstance(layer, MaxPooling2D)]\n\n# Pick random images to visualize\nimgs_to_visualize = np.random.choice(np.arange(0, len(X_img_val)), NUM_LEAVES)\n\n# Use a keras function to extract the conv layer data\nconvout_func = K.function([model.layers[0].input, K.learning_phase()], [layer.output for layer in conv_layers])\nconv_imgs_filts = convout_func([X_img_val[imgs_to_visualize], 0])\n# Also get the prediction so we know what we predicted\npredictions = model.predict([X_img_val[imgs_to_visualize], X_num_val[imgs_to_visualize]])\n\nimshow = plt.imshow #alias\n# Loop through each image disply relevant info\nfor img_count, img_to_visualize in enumerate(imgs_to_visualize):\n\n    # Get top 3 predictions\n    top3_ind = predictions[img_count].argsort()[-3:]\n    top3_species = np.array(LABELS)[top3_ind]\n    top3_preds = predictions[img_count][top3_ind]\n\n    # Get the actual leaf species\n    actual = LABELS[y_val[img_to_visualize]]\n\n    # Display the top 3 predictions and the actual species\n    print(\"Top 3 Predicitons:\")\n    for i in range(2, -1, -1):\n        print(\"\\t%s: %s\" % (top3_species[i], top3_preds[i]))\n    print(\"\\nActual: %s\" % actual)\n\n    # Show the original image\n    plt.title(\"Image used: #%d (digit=%d)\" % (img_to_visualize, y_val[img_to_visualize]))\n    # For Theano users comment the line below and\n    imshow(X_img_val[img_to_visualize][:, :, 0], cmap='gray')\n    # imshow(X_img_val[img_to_visualize][0], cmap='gray') # uncomment this\n    plt.tight_layout()\n    plt.show()\n\n    # Plot the filter images\n    for i, conv_imgs_filt in enumerate(conv_imgs_filts):\n        conv_img_filt = conv_imgs_filt[img_count]\n        print(\"Visualizing Convolutions Layer %d\" % i)\n        # Get it ready for the plot_figures function\n        # For Theano users comment the line below and\n        fig_dict = {'flt{0}'.format(i): conv_img_filt[:, :, i] for i in range(conv_img_filt.shape[-1])}\n        # fig_dict = {'flt{0}'.format(i): conv_img_filt[i] for i in range(conv_img_filt.shape[-1])} # uncomment this\n        plot_figures(fig_dict, *get_dim(len(fig_dict)))"},{"cell_type":"markdown","metadata":{"_cell_guid":"b27a6e30-3232-8ce0-108b-05402b5d57f2"},"source":"# Conclusion\n\nFor the first convolutional layer we can sort of tell that most of the filters are doing edge detection on the leaf. That actually makes a lot of sense since pretty much all of the species specific information of a leaf is stored in the shape of its edge. The second convolutional layer is also mainly edge detection along with some point and edge shape detection I noticed with some leaves that have particularly special shapes. This is actually pretty common with convnets. The first few layers will do really simple stuff like edge and shape detection, but the deeper you go the more abstract it gets. Since we don't really have enough data to go that deep most of our filters look pretty tame. Judging by our LB score though, I think we can assume what it's doing is fairly constructive.\n\nWell, that's all! If you've made it this far that means you've read my first kernel, and I hope it helps if you're stuck and don't know how to improve your score. In addition to questions, I'm very open to any feedback both in general about kernel writing and specifically about this kernel.\n\n## Thank you for reading!"}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":0}