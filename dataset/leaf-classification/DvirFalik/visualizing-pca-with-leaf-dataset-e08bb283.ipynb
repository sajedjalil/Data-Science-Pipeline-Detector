{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"}},"nbformat":4,"nbformat_minor":0,"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"3f7fe7df-1326-d944-2ec5-22c5c9155822","_active":false},"source":"***Visualizing PCA with Leaf Dataset***\n================================\n\nIn this script we will apply PCA on leaf images and try to get a feel for the distribution of leaf images using visualizations that (hopefully) clarify different aspects about how to interpret PCA results.\n\nWe will then continue to see if the PCA features are informative in terms of classifying leafs and determine how many of those we need.","execution_count":null,"outputs":[],"execution_state":"idle"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f0c7a02b-f9b6-f3e8-7b9a-67d34671ef7e","_active":false,"collapsed":false},"outputs":[],"source":"import os\nimport cv2\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib\nimport matplotlib.image as mpimg\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\n\nfrom sklearn import model_selection\nfrom sklearn import decomposition\nfrom sklearn import linear_model\nfrom sklearn import ensemble\nfrom sklearn import neighbors\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.neighbors import KernelDensity\nfrom sklearn.manifold import TSNE\nfrom sklearn.metrics import accuracy_score\nfrom sklearn import svm\n\nfrom skimage.transform import rescale\nfrom scipy import ndimage as ndi\n\nmatplotlib.style.use('fivethirtyeight')","execution_state":"idle"},{"cell_type":"markdown","metadata":{"_cell_guid":"347f2648-d405-9620-505c-c7d91d1f9b70","_active":false},"source":"**For the sake of the script not being too cluttered, I commented out all intermediate plots in the data loading and preparation phases**\n\nanyone who is interested is welcome to fork and uncomment to see what is going on.\n\n(the main assumption of this pre-processing stage is that the absolute sizes of the leafs matter, and not just their shape. i.e. leafs with different sizes are most definitely different types of leafs. not sure if it's actually important, but just in case)","execution_count":null,"outputs":[],"execution_state":"idle"},{"metadata":{"_cell_guid":"c3ec6416-99d9-99ce-036c-398d8f0d2428","_active":false,"collapsed":false},"source":"#%% load the data\ndataDir = '../input/'\ntrainData = pd.read_csv(dataDir + 'train.csv')\nclassEncoder = LabelEncoder()\ntrainLabels = classEncoder.fit_transform(trainData.ix[:,'species'])\ntrainIDs = np.array(trainData.ix[:,'id'])\n\ndat = np.array(trainData.ix[:,2:194])\n\n\n","execution_count":null,"cell_type":"code","outputs":[],"execution_state":"idle"},{"metadata":{"_cell_guid":"d1063640-1e4d-9bdf-492e-c4d2fa94e9d6","_active":false,"collapsed":false},"source":"#Extract HuMomments - Binary shape feature\nfrom sklearn import preprocessing\n\nnumImages = 1584\nn=100\nhumomments = np.zeros([numImages,7])\ni=0\nfor i in range (numImages):\n    imageFilename = dataDir + 'images/' + str(i+1) + '.jpg'\n    img = mpimg.imread(imageFilename)           \n    humomments[i-1,:] = np.transpose(cv2.HuMoments(cv2.moments(img)))\n    i = i+1\n    \n#print(humomments)\nnormmomments = preprocessing.normalize(humomments, axis=1 ,norm='l2')\nscaledmomments = preprocessing.scale(humomments,axis=1)","execution_count":null,"cell_type":"code","outputs":[],"execution_state":"idle"},{"metadata":{"_cell_guid":"620569e6-2144-b8cd-43c7-325c346f61c9","_active":true,"collapsed":false},"source":"print (normmomments.shape)\nprint(dat.shape)","execution_count":null,"cell_type":"code","outputs":[],"execution_state":"idle"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c80e25cd-829b-742e-f6a8-482aea76757a","_active":false,"collapsed":false},"outputs":[],"source":"#%% define GaussianModel class\n\nclass GaussianModel:\n    def __init__(self, X, numBasisFunctions=10, objectPixels=None):\n        '''\n        inputs: \n            X                    - numSamples x numDimentions matrix\n            numBasisFunctions       - number of basis function to use\n            objectPixels (optional) - an binnary mask image used for presentation\n                                      will be used as Im[objectPixels] = dataSample\n                                      must satisfy objectPixels.ravel().sum() = X.shape[1]\n        '''\n        \n        self.numBasisFunctions = numBasisFunctions        \n        if objectPixels == None:\n            self.objectPixels = np.ones((1,X.shape[1]),dtype=np.bool)\n        else:\n            self.objectPixels = objectPixels\n        assert(self.objectPixels.ravel().sum() == X.shape[1])\n\n        PCAModel = decomposition.PCA(n_components=numBasisFunctions, whiten=True)\n        self.dataRepresentation = PCAModel.fit_transform(X)\n        self.PCAModel = PCAModel\n\n    def RepresentUsingModel(self, X):\n        return self.PCAModel.transform(X)\n","execution_state":"idle"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c2a00d98-472a-e719-e4bc-37ae275a44a1","_active":false,"collapsed":false},"outputs":[],"source":"# train the Gaussian Model \n\n#sampleDim = np.shape(scaledDownImages)[0]*np.shape(scaledDownImages)[1]\n#X = scaledDownImages.reshape(sampleDim,-1).T\n#\n#objectPixelsMask = np.ones((np.shape(scaledDownImages)[0],np.shape(scaledDownImages)[1]))==1\n#leaf_PCAModel = GaussianModel(X, numBasisFunctions=100, objectPixels=objectPixelsMask)\nX=trainData.filter(items=trainData.columns[2:])\nleaf_PCAModel = GaussianModel(X, numBasisFunctions=192, )","execution_state":"idle"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"3e6549c4-a554-44f6-0c1e-1fba63c2bc4e","_active":false,"collapsed":false},"outputs":[],"source":"matplotlib.rcParams['font.size'] = 5\nmatplotlib.rcParams['figure.figsize'] = (8,5)\n\nX_PCA = leaf_PCAModel.RepresentUsingModel(X)\nX_PCA\n\nX_PCA_train = X_PCA     #   [trainIDs-1,:]\ny_train = trainLabels\n\ntSNE_PCAModel = TSNE(n_components=2, random_state=0)\nX_PCA_train_tSNE = tSNE_PCAModel.fit_transform(X_PCA_train) \n\nplt.figure()\nplt.subplot(1,2,1); plt.scatter(X_PCA_train[:,0],X_PCA_train[:,1],c=y_train,cmap='Paired',s=10,alpha=0.95)\nplt.title('PCA representation'); plt.xlabel('PC1 coeff'); plt.ylabel('PC2 coeff')\nplt.subplot(1,2,2); plt.scatter(X_PCA_train_tSNE[:,0],X_PCA_train_tSNE[:,1],c=y_train,cmap='Paired',s=10,alpha=0.95)\nplt.title('t-SNE representation'); plt.xlabel('t-SNE axis1'); plt.ylabel('t-SNE axis2')","execution_state":"idle"},{"cell_type":"markdown","metadata":{"_cell_guid":"1c1029eb-7f72-6e45-d936-e6c14e031360","_active":false},"source":"We can see that nearby points usually have similar color and this means they have similar leaf label. This makes us confident that we can achieve at least some classification accuracy from these PCA features.\n\n\n----------\n\n\nNow, let's see what is the **classification accuracy** using this PCA representation if we use **different amount of PCA coefficients** for several different types of classifiers.","execution_count":null,"outputs":[],"execution_state":"idle"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"3ccd8f59-2478-1dce-9513-958ff604edb1","_active":false,"collapsed":false},"outputs":[],"source":"#%% plot CV classification accuracy as function of num components used for 3 very different type of classifiers\nmatplotlib.rcParams['font.size'] = 7\nmatplotlib.rcParams['figure.figsize'] = (8,7)\n\nnumPCsToUse = [1,2,4,8,16,32,64,96,128,160,192]\n\nlogReg = linear_model.LogisticRegression(C=10.0)\nkNN = neighbors.KNeighborsClassifier(n_neighbors=7)\nRF = ensemble.RandomForestClassifier(n_estimators=100)\nSvm = svm.SVC(gamma=0.1, C=100.)\n\nClsDict={\n    'logReg': {'cls': logReg, 'Acc':[], 'meanAcc':[], 'AccStd':[]},\n    'kNN': {'cls': kNN, 'Acc':[], 'meanAcc':[], 'AccStd':[]},\n    'RF': {'cls': RF, 'Acc':[], 'meanAcc':[], 'AccStd':[]},\n    'Svm': {'cls': Svm, 'Acc':[], 'meanAcc':[], 'AccStd':[]},\n}\nlogRegMeanAccuracy = []; kNN_MeanAccuracy = []; RF_MeanAccuracy = []; Svm_MeanAccuracy = []\nlogRegAccuracyStd  = []; kNN_AccuracyStd  = []; RF_AccuracyStd  = []; Svm_AccuracyStd  = []\n\nfor numPCs in numPCsToUse:\n    stratifiedCV = model_selection.StratifiedKFold(n_splits=5, random_state=1)\n    for cls in ClsDict:\n        ClsDict[cls]['Acc']=[]\n    for trainInds, validInds in stratifiedCV.split(X_PCA_train, y_train):\n        X_train_cv = X_PCA_train[trainInds,:numPCs]\n        X_valid_cv = X_PCA_train[validInds,:numPCs]\n\n        y_train_cv = y_train[trainInds]\n        y_valid_cv = y_train[validInds]\n\n        for cls in ClsDict:\n            clsObj=ClsDict[cls]['cls']\n            clsObj.fit(X_train_cv, y_train_cv)\n            ClsDict[cls]['Acc'].append(accuracy_score(y_valid_cv, clsObj.predict(X_valid_cv)))\n        \n    for cls in ClsDict:\n        clsObj=ClsDict[cls]['cls']\n        clsObj.fit(X_train_cv, y_train_cv)\n        ClsDict[cls]['meanAcc'].append(np.array(ClsDict[cls]['Acc']).mean())\n        ClsDict[cls]['AccStd'].append(np.array(ClsDict[cls]['Acc']).std())\n        \nplt.figure()\nfor cls in ClsDict:\n    plt.errorbar(x=numPCsToUse, y=ClsDict[cls]['meanAcc'], yerr=ClsDict[cls]['AccStd'])\nplt.xlim(min(numPCsToUse)-1,max(numPCsToUse)+1); plt.legend(ClsDict.keys(),loc=2)\nplt.xlabel('num PCA components'); plt.ylabel('validation accuracy'); plt.title('accuracy as function of num PCs')","execution_state":"idle"},{"cell_type":"markdown","metadata":{"_cell_guid":"2fd63a23-eb3e-1ac0-1e99-cb63f631e783","_active":false},"source":"Overall, it's evident that all classifiers achieve approximately similar performance.\n\nBut it's interesting to note the somewhat different behavior of these different classifiers as a function of number of components used. \n\nFor example, the nearest neighbor classifier flattens out early and does not benefit from additional components beyond 8, whereas the logistic regression classifier continues to increase it's performance up to around 32 components.\nThe Random Forest classifier is consistently the best performing but it also flattens out at around 32 components.","execution_count":null,"outputs":[],"execution_state":"idle"},{"metadata":{"_cell_guid":"b268d1b9-05dc-60bf-2267-cc13001c85c8","_active":false,"collapsed":false},"source":"ClsDict","execution_count":null,"cell_type":"code","outputs":[],"execution_state":"idle"},{"metadata":{"_cell_guid":"5c070ef7-cb3c-6a07-467e-0370a8f78955","_active":false,"collapsed":false},"source":null,"execution_count":null,"cell_type":"code","outputs":[],"execution_state":"idle"}]}