{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"b58b9347-127e-c868-1dbe-b271ca7b85da"},"source":"In this post we will use PCA to simplify the data and optimise the training of the prediction models. We will also train 4 different models and analyse their accuracy and predictions before making a decision on the model to use for final predictions.\n\n**1- Load and inspection of data**  "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"02334169-5d63-8725-0c2b-613aef337177"},"outputs":[],"source":"import math\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom sklearn.decomposition import PCA\nfrom sklearn import preprocessing, cross_validation\n\ndf_test = pd.read_csv('../input/test.csv', index_col = 'id')\ndf = pd.read_csv('../input/train.csv', index_col = 'id')\nprint(df.head())\n"},{"cell_type":"markdown","metadata":{"_cell_guid":"be7085f6-a70c-b091-086d-32d35384347c"},"source":"We want to convert the column 'species' to a numeric format that we can pass to our model to train.\n"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e637abc6-e9b9-305b-8719-4088137bb73c"},"outputs":[],"source":"#create new column in dataframe equal to species\ndf['label'] = df['species']\n\n#create numpy array storing unique species values\nspecies = df.species.unique() \n\n#replace newly created column in dataframe with index of each species in the array. \nfor sp in species:\n    df['label'].replace(sp, species.tolist().index(sp), inplace = True)\n    \n#store label column in variable y \ny= df['label']"},{"cell_type":"markdown","metadata":{"_cell_guid":"e1d75c39-ca17-7725-12fa-e4f1c5d9de33"},"source":"We also saw that there's quite a number of features and the values of some of them are also quite small. In some cases, being 0.00000 for most instances. We can see this more clearly doing:"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"3aa2906d-a2dc-b473-415e-9e529783f45a"},"outputs":[],"source":"print(df.describe())"},{"cell_type":"markdown","metadata":{"_cell_guid":"3c74a421-8c92-b012-75fb-a39f358501c9"},"source":"Column “margin8” for instance, has 75% of the data population equal to 0.000000. And since we can't do this analysis individually for all fields due to the large number of features in this dataset, the best way to go about this is to do some dimensionality reduction with PCA. This will help us to retain the features with most variance and which will provide the most valuable information to train our model.\n\n**2- Scale and PCA**\n\nWe scale our data (training and test data):"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f1b39645-1bf5-b0b5-440f-7fa20628275e"},"outputs":[],"source":"scaled_X = preprocessing.scale(df.drop(['species','label'], 1))\nscaled_X_test = preprocessing.scale(df_test)"},{"cell_type":"markdown","metadata":{"_cell_guid":"8cda804d-969f-0320-4f84-f6d06a8178b5"},"source":"We first call PCA without reducing the number of our features, just to get the eigenvalue matrix. We need this matrix to evaluate the optimal number of features we can retain without losing too much information:"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"fb2f4de7-1d84-4beb-8633-763b78801d98"},"outputs":[],"source":"pca = PCA()\npca_X = pca.fit_transform(scaled_X)\neigenvectors_ini = pca.components_                                      #dim = (m x 192)\neigenvalues_ini = pca.explained_variance_"},{"cell_type":"markdown","metadata":{"_cell_guid":"3d4c33f9-834f-620f-ebfa-684b51eab063"},"source":"Now we apply PCA, to retain 120 features (out of the 192 we initially had) and then we check the variance retained between our new set of data and the initial data:\n"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"4f663574-c1a3-a6d0-2298-6be67a4bd7ae"},"outputs":[],"source":"pca = PCA(n_components = 120)\npca_X = pca.fit_transform(scaled_X)\neigenvectors = pca.components_                                          #dim = (m x 120)\neigenvalues = pca.explained_variance_\n\nvar_retained = np.sum(eigenvalues, axis =0)/np.sum(eigenvalues_ini, axis = 0)\nprint('Variance retained:', var_retained) "},{"cell_type":"markdown","metadata":{"_cell_guid":"00e229e4-3a7a-844a-0450-64f066ebd5ec"},"source":"The variance retained with this number of features is over 99%. This is an acceptable value, this way we have reduced the number of features without losing much information. We apply this to our test data to leave it ready for prediction phase. "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f04baee4-b503-745d-a03c-94e60c0d5fcb"},"outputs":[],"source":"pca_X_test = pca.fit_transform(scaled_X_test)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"43421af3-e013-e988-259d-39b742b63c93"},"outputs":[],"source":"pca_X_train, pca_X_cv, y_train, y_cv = cross_validation.train_test_split(pca_X, y, test_size=0.2)\nprint('Length training data',len(pca_X_train), '\\n', 'length cv data', len(pca_X_cv))"},{"cell_type":"markdown","metadata":{"_cell_guid":"ad1bf69c-a645-d75f-3014-b7e2e01ebac9"},"source":"**3- Visualise data**\n\nSometimes visualising the data can be useful to choose a prediction model to train, so we try and plot the 3 most significant dimensions of our reduced data:"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"4560c5c3-6b19-278a-578c-b00834f2ceff"},"outputs":[],"source":"colors = 100*['k','gray','m','r','orange','y','g','c','b','w']\n\n#Plot PCA'd data (reduced data)\nfig = plt.figure()\nax = fig.add_subplot(111, projection = '3d')\nfor i in range(len(df)):\n    ax.scatter(pca_X[i,0],pca_X[i,1],pca_X[i,2], color = colors[df['label'].iloc[i]])\nax.set_xlabel('PCA Dimension 1')\nax.set_ylabel('PCA Dimension 2')\nax.set_zlabel('PCA Dimension 3')\nax.set_title('Leaf training data visualisation (PCA = 3)')\nplt.show()"},{"cell_type":"markdown","metadata":{"_cell_guid":"8cb1fc25-03d2-d8d8-02b9-22d9f395008b"},"source":"From this visualisation the only conclusion to extract is that the species classification is not likely to respond to a linear model. (Can't see clear linear boundaries between species in the plotting, all seem to be somewhat tangled).\nSo, we will train 4 different models and then try and evaluate which one might perform best.\n\n**4- Train models & cross-validation evaluation** \n\nWe train 4 models and test against the cross-validation data. (Added some extra imports at the beginning of the script). "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"86284fd2-ca58-55ae-a40d-a2e2f72db0a0"},"outputs":[],"source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn import neighbors, svm\nfrom sklearn.ensemble import RandomForestClassifier\n\n#Logistic Regression\nclf_log = LogisticRegression(multi_class='multinomial', solver = 'newton-cg')\nclf_log.fit(pca_X_train, y_train)\naccuracy_log = clf_log.score(pca_X_cv, y_cv)\nprint('Logistic Regression accuracy:',accuracy_log)\n\n#K-Nearest Neighbours\nclf_knn = neighbors.KNeighborsClassifier()\nclf_knn.fit(pca_X_train, y_train)\naccuracy_knn = clf_knn.score(pca_X_cv, y_cv)\nprint('K-Nearest Neighbours accuracy:',accuracy_knn)\n\n#SVM \nclf_svm = svm.SVC(probability = True)\nclf_svm.fit(pca_X_train, y_train)\naccuracy_svm = clf_svm.score(pca_X_cv, y_cv)\nprint('SVM accuracy:',accuracy_svm)\n\n#Random Forest\nclf_rf = RandomForestClassifier(n_estimators=120)\nclf_rf.fit(pca_X_train, y_train)\naccuracy_rf = clf_rf.score(pca_X_cv, y_cv)\nprint('Random forest accuracy:',accuracy_rf)\n"},{"cell_type":"markdown","metadata":{"_cell_guid":"cdd36f47-ee03-c1b5-9f4e-074e7d675b8a"},"source":"Several runs of these models, give accuracies well over 90%. In most cases over 95%, with Logistic Regression and SVM consistently peaking over the other two. \nThe code below shows accuracies in a graph."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d7a2d1ef-06f2-b2d8-e7a3-ee3acb6a9bad"},"outputs":[],"source":"def plot_accuracy(accuracy_log, accuracy_knn, accuracy_svm, accuracy_rf):\n    fig, axes = plt.subplots()\n    ax=axes\n    colors = ['r','orange','y','g']\n    width = 0.5\n    x = range(0, 4)\n    y = [accuracy_log, accuracy_knn, accuracy_svm, accuracy_rf]\n    ind = ['Logistic Reg', 'K-Nearest Neighb', 'SVM', 'Random Forest']\n    ax.bar(x, y, width, align = 'center', color = colors)\n    for a, b in enumerate(y):\n        ax.text(x[a] - width/4, b - 0.2, str(round(b,3)), color = 'k')\n    ax.xaxis.set_ticks(x)\n    ax.set_xticklabels(ind)\n    ax.axhline(min(y), linewidth=1, color='b')\n    ax.set_title('Accuracies - CrossValidation data')\n    plt.show()\n\n#Plot accuracy\nplot_accuracy(accuracy_log, accuracy_knn, accuracy_svm, accuracy_rf)"},{"cell_type":"markdown","metadata":{"_cell_guid":"a128c024-f93f-5816-0776-657e9427cb97"},"source":"**5- Predictions of test data and analysis of predictions**\n\nLogistic Regression stands as favourite. However, a comparison of predictions per model on the test set might help get more comfort over the final choice of the model.\n\nWe get the labelled predictions for the test data per model:\n"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"eddc8528-8e1a-d151-3f98-f19ab1ddc785"},"outputs":[],"source":"#Logistic Regression\nlabels_log = clf_log.predict(pca_X_test)\n\n#K-Nearest Neighbours\nlabels_knn = clf_knn.predict(pca_X_test)\n\n#Random Forest\nlabels_rf = clf_rf.predict(pca_X_test)\n\n#SVM - can't use predict(X) as in previous cases\nprob_svm = clf_svm.predict_proba(pca_X_test)\nlabels_svm = prob_svm.argmax(axis = 1)"},{"cell_type":"markdown","metadata":{"_cell_guid":"c7bf087d-90f6-10dc-37a7-72cf701a79c1"},"source":"For SVM can't use the *predict(X)* method as in previous cases. An inspection of the predicted labels calculated through *predict(X)* against the probability values calculated through predict_proba(X) shows that labels outputted by *predict(X)* in SVM don't necessarily match the class with the highest probability value. Therefore, the accuracy value calculated for the SVM model might not be a conclusive measure of the accuracy of predictions submitted as probability values. (The accuracy calculated through *score()* method is defined as mean accuracy of *predict(X)* with respect to y). \nHence, to get the predictions consistent with probabilities we calculated the labels as previous shown.\n\n**Snapshot of predictions**\n\nNow, to get a snapshot of the predictions per model, we get the number times each species is predicted per model and we plot this summary in some graphs:"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b12204d9-d3ee-38be-bd25-f970f8b808cf"},"outputs":[],"source":"#Function to summarise predictions\ndef get_predictions_summary(species, labels_log, labels_knn, labels_svm, labels_rf, data):\n\n    df_pred = pd.DataFrame(data = data)\n    df_pred['logistic'] = species[labels_log]\n    df_pred['k-nn'] = species[labels_knn]\n    df_pred['svm'] = species[labels_svm]\n    df_pred['random_forest'] = species[labels_rf]\n    #df_pred.to_csv('All predictions.csv',sep=',')\n\n    df_pred_summary = pd.DataFrame({'logistic': df_pred['logistic'].value_counts()})\n    df_pred_summary['k-nn'] = pd.DataFrame(data = df_pred['k-nn'].value_counts())\n    df_pred_summary['svm'] = pd.DataFrame(data = df_pred['svm'].value_counts())\n    df_pred_summary['random_forest'] = pd.DataFrame(data = df_pred['random_forest'].value_counts())\n\n    return df_pred, df_pred_summary\n\n#Call function to summarise predictions\ndf_pred, df_pred_summary = get_predictions_summary(species, labels_log, labels_knn, labels_svm, labels_rf, df_test)\n\n\n#Function to plot top N predictions per model\ndef plot_predictions_summary(df_pred, df_pred_summary, top_n):\n    \n    cols = int(len(df_pred_summary.columns)/2)\n    rows =  len(df_pred_summary.columns) - cols\n    colors = ['r','orange','y','g']\n    \n    fig, axes = plt.subplots(nrows = rows, ncols = cols, figsize = (9,6))\n    ax_0 = [axes[0,0],axes[0,1],axes[1,0],axes[1,1]]\n\n    i = 0\n    for name in df_pred.columns[-4:]:\n        ax1 = df_pred_summary.sort_values(str(name), ascending = False)[str(name)].iloc[0:top_n].plot(ax = ax_0[i], kind = 'bar', color = colors[i])\n        ax1.set_xticklabels(ax1.xaxis.get_majorticklabels(), rotation=20, fontsize = 6)\n        ax1.set_title(str(name))\n        i+=1 \n\n    for column in df_pred_summary.columns:\n        print('Top', top_n, column, '\\n', df_pred_summary.sort_values(str(column), ascending = False)[str(column)].iloc[0:top_n])\n    \n    plt.show()\n\n#Call function to plot top-5 species\nplot_predictions_summary(df_pred, df_pred_summary, 5)\n"},{"cell_type":"markdown","metadata":{"_cell_guid":"9c9e7c9d-3c7b-e3b6-a971-399fd29af98f"},"source":"Unfortunately, none of predictions look anything similar (top 5 species don't match across models). We see, however, that SVM predicts an unexpectedly high number of samples (140, over 20% of test data) to the same species. This number is unexpectedly large in contrast with the numbers we see in the other models, so probably another reason to discard SVM as a prediction model based on highest probability value.  \n\n**Concurrent predictions**\n\nLastly we try and compare actual matches of predictions across models. This is, instances where the same species is predicted by 3 models simultaneously, by pairs of models (2 models and 2models), or by only two models."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b972bfab-006b-dbb4-8c94-67d148b8b919"},"outputs":[],"source":"#Nr of predictions that match across models (models predicting the same) & analyse concurrent predictions across models (in CV or test data).\ndef get_same_predictions_across_models(pred_log, pred_knn, pred_svm, pred_rf, *labels):\n\n    #store 2-model matches\n    matches_logvknn = matches_logvsvm = matches_logvrf = matches_knnvsvm = matches_knnvrf = matches_svmvrf = 0\n    #store 2-model matches with labels - CV only\n    ismatch2logvknn = ismatch2logvsvm = ismatch2logvrf = ismatch2knnvsvm = ismatch2knnvrf = ismatch2svmvrf = 0\n    #store 2-by-2-model matches\n    matches_2and2_logknn_svmrf = matches_2and2_logsvm_knnrf = matches_2and2_logrf_knnsvm = 0\n    #store 3-model matches\n    matches_logvknnvsvm = matches_logvknnvrf = matches_logvsvmvrf = matches_knnvsvmvrf = 0\n    #store 3-model matches with labels - CV only\n    isamatch3log = isamatch3knn = isamatch3svm = isamatch3rf = isamatchall4 = 0\n    #store 4-model matches\n    matches_all = 0\n\n    for i in range(len(pred_log)):\n        if (pred_log[i] == pred_knn[i]):    \n            if (pred_log[i] == pred_svm[i]):                \n                if (pred_log[i] == pred_rf[i]):             \n                    matches_all += 1                            #match all\n                    try:\n                        if pred_log[i] == labels[0][i]:\n                            isamatchall4 += 1                       #prediction match labels - CV only\n                    except:\n                        pass\n                else:\n                    matches_logvknnvsvm += 1                    #match 3 models (Log, Knn, SVM)\n                    try:\n                        if pred_log[i] == labels[0][i]:                   \n                            isamatch3rf += 1                        #prediction match labels - CV only\n                    except:\n                        pass\n            elif(pred_log[i] == pred_rf[i]):\n                matches_logvknnvrf += 1\n                try:\n                    if pred_log[i] == labels[0][i]:                    \n                        isamatch3svm += 1                           #prediction match labels - CV only\n                except:\n                    pass\n                \n            else:\n                if (pred_svm[i] == pred_rf[i]):              \n                    matches_2and2_logknn_svmrf += 1             #match 2 models (Log, Knn) and 2 models (SVM, Rf)\n                else:\n                    matches_logvknn += 1                        #match 2 models (Log, Knn)\n\n        elif (pred_log[i] == pred_svm[i]):\n            if(pred_svm[i] == pred_rf[i]):\n                matches_logvsvmvrf += 1                         #match 3 models (Log, SVM, Rf)\n                try:\n                    if pred_log[i] == labels[0][i]:\n                        isamatch3knn += 1                               #prediction match labels - CV only\n                except:\n                    pass\n            else:\n                if (pred_knn[i] == pred_rf[i]):\n                    matches_2and2_logsvm_knnrf += 1             #match 2 models (Log, SVM) and 2 models (Knn, Rf)\n                else:\n                    matches_logvsvm += 1                        #match 2 models (Log, SVM)\n\n        elif(pred_log[i] == pred_rf[i]):\n            if (pred_knn[i] == pred_svm[i]):\n                    matches_2and2_logrf_knnsvm += 1             #match 2 models (Log, Rf) and 2 models (Knn, SVM)\n            else:\n                matches_logvrf += 1                             #match 2 models (Log, Rf)\n            \n        elif (pred_knn[i] == pred_svm[i]):\n            if (pred_knn[i] == pred_rf[i]):\n                matches_knnvsvmvrf += 1                         #match 3 models (Knn, SVM, Rf)\n                try:\n                    if pred_knn[i] == labels[0][i]:\n                        isamatch3log += 1                               #prediction match labels - CV only\n                except:\n                    pass\n            else:\n                matches_knnvsvm += 1                            #match 2 models (Knn, SVM)\n        elif(pred_knn[i] == pred_rf[i]):\n            matches_knnvrf += 1                                 #match 2 models (Knn, Rf)\n\n            \n        elif (pred_svm[i] == pred_rf[i]):\n            matches_svmvrf += 1                                 #match 2 models (SVM, Rf)\n\n\n    matches3 = [matches_knnvsvmvrf, matches_logvsvmvrf, matches_logvknnvrf, matches_logvknnvsvm]\n    matches2and2 = [matches_2and2_logknn_svmrf, matches_2and2_logsvm_knnrf, matches_2and2_logrf_knnsvm]\n    matches2 = [matches_logvknn, matches_logvsvm, matches_logvrf, matches_knnvsvm, matches_knnvrf, matches_svmvrf]\n    actualmatches = [isamatch3log, isamatch3knn, isamatch3svm, isamatch3rf,\n                     ismatch2logvknn, ismatch2logvsvm, ismatch2logvrf, ismatch2knnvsvm, ismatch2knnvrf, ismatch2svmvrf]\n    tags3match = ['(no Log)','(no KNN)','(no SVM)','(no Rf)']\n    tags2match = ['Log-Knn','Log-SVM','Log-Rf','Knn-SVM','Knn-Rf','SVM-Rf']\n\n    print('All 4 model match',matches_all)\n    print('3 model match:')\n    for i in range(len(matches3)):\n        print(tags3match[i], matches3[i],'perc:',matches3[i]/len(pred_log))\n    print('2-and-2 model match:')\n    for i in range(len(matches2and2)):\n        print(tags2match[i], tags2match[len(matches2)-1-i], matches2and2[i],'perc:',matches2and2[i]/len(pred_log))\n    print('2 model match:')\n    for i in range(len(matches2)):\n        print(tags2match[i], matches2[i],'perc:',matches2[i]/len(pred_log))\n    print('Actual matches - 3 model (CV set only):')\n    for i in range(len(matches3)):\n        print(tags3match[i], actualmatches[i],'perc:',actualmatches[i]/len(pred_log))\n    print('Actual matches - 2 model (CV set only):')\n    for i in range(len(matches2)):\n        print(tags2match[i], actualmatches[i+4],'perc:',actualmatches[i+4]/len(pred_log))\n\n    return matches_all, isamatchall4, matches3, matches2and2, matches2, actualmatches\n\n#Call function to compare predictions across models - CV or Test data\nmatches_all, isamatchall4, matches3, \\\nmatches2and2, matches2, actualmatches = get_same_predictions_across_models(labels_log, labels_knn, labels_svm, labels_rf)\n\n#Function to plot matches\ndef plot_matches_across_models(matches3, matches2and2, matches2, actualmatches):\n    y_ini = [matches3, matches2and2, matches2]                                                                      #matches in predictions across models\n    ind_ini = [['-Log', '-KNN', '-SVM', '-R.For'], ['LgKn-SVRf', 'LgSV-KnRf', 'LgRf-KnSV'],\n           ['LgKn','LgSV','LgRf','KnSV','KnRf','SVRf']]                                              #tags for 3-model matches, 2-and-2 model matches & 2-model matches  \n    z_ini = [actualmatches[0:4], [0,0,0], actualmatches[4:10]]                                                      #matches across models and with labels - CV data only\n    \n    cols = len(y_ini)\n    y = []\n    x = []\n    x_2 = []\n    idx = []\n    z = []\n    ind = []\n\n    for match in y_ini:                                                                                             #populate data to plot for elements of y_ini different from zero \n        if all(v == 0 for v in match):\n            cols -= 1     \n        else:\n            index = y_ini.index(match)\n            idx.append(index)\n            y.append(match)\n            x.append(range(0,len(match)))\n            x_2.append(np.arange(0.5, (len(match)+0.5)))\n            z.append(z_ini[index])\n            ind.append(ind_ini[index])     \n                \n    fig, axes = plt.subplots(nrows = 1, ncols = cols, figsize = (9,6))\n    colors = ['r','orange','y','g']\n    width = 0.5\n\n    for i in range(len(y)):\n        axes[i].bar(x[i], y[i], width, align = 'center', color = colors)\n        for a, b in enumerate(y[i]):\n            axes[i].text(x[i][a] - width/4, b + b*0.02, str(round(b,3)), color = 'k')\n        if not (all(v == 0 for v in z[i])):                                                                         #only cv data (when there are labels to match predictions with)\n            axes[i].bar(x_2[i], z[i], width, align = 'center', color = 'c')\n            for a, b in enumerate(z[i]):\n                axes[i].text(x_2[i][a] - width/4, b + b*0.02, str(round(b,3)), color = 'k')\n        axes[i].xaxis.set_ticks(x[i])\n        axes[i].set_xticklabels(ind[i])\n        axes[i].tick_params(labelsize = 'small')\n        axes[i].set_title(str(math.ceil((idx[i]+3)/(idx[i]+1))) + ' models match', fontsize= 8)\n    plt.show()\n\n#Call function to plot matching predictions between models\nplot_matches_across_models(matches3, matches2and2, matches2, actualmatches)\n"},{"cell_type":"markdown","metadata":{"_cell_guid":"63ad0ea5-4954-07fe-37bb-9288fd08406e"},"source":"From graphs above, we see that for samples where 3 models predict the same species (graph on the left), only 6 instances out of the 142 concurrences don't involve Logistic Regression. In the rest of the cases, Logistic Regression seems to agree with other 2 models on the prediction. \nLikewise, in instances where only 2 models agree (graph on the far right), Logistic Regression seems to have the highest presence (193 concurrences with other models: 107 with Knn, 31 with SVM, 55 with Rf).\n\nThese two factors, along with the high accuracy value observed consistently over runs, give some reassurance that the Logistic Regression might give us best results out of the 4 models.\n\n\n**6- Submission of predictions**\n\nWe output the predictions of the Logistic Regression data:\n"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b09f30c6-ae34-99ba-facb-ea0c0115fbe3"},"outputs":[],"source":"def get_predictions_probabilities(species, df_data, pca_data):\n\n    df_prob = pd.DataFrame(data = np.nan, index = df_data.index, columns = [str(sp) for sp in species])\n\n    #Logistic Regression\n    prob_log = clf_log.predict_proba(pca_data)\n    for sp in species:\n        df_prob[str(sp)] = prob_log[:,species.tolist().index(sp)]\n\n    df_prob = df_prob[sorted(df_prob.columns)]\n    print(df_prob.tail())\n    df_prob.to_csv('submission.csv',sep=',')\n\n    return df_prob\n\ndf_prob = get_predictions_probabilities(species, df_test, pca_X_test)\n"}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"}},"nbformat":4,"nbformat_minor":0}