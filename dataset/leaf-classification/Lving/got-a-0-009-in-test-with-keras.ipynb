{"metadata":{"language_info":{"nbconvert_exporter":"python","name":"python","codemirror_mode":{"version":3,"name":"ipython"},"version":"3.6.3","mimetype":"text/x-python","file_extension":".py","pygments_lexer":"ipython3"},"kernelspec":{"display_name":"Python 3","name":"python3","language":"python"}},"nbformat_minor":1,"cells":[{"metadata":{"_uuid":"39eece483c5d0808e57bae6e0e5d719d34d902c2","_cell_guid":"97e9c07d-a7de-48d5-b33d-488c2a223e90"},"outputs":[],"source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport os\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import StratifiedShuffleSplit\n\nfrom keras.utils.np_utils import to_categorical\nfrom keras.preprocessing.image import img_to_array, load_img\n\nroot = '../input'\nnp.random.seed(2016)\nsplit_random_state = 7\nsplit = .9\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output.","cell_type":"code","execution_count":4},{"metadata":{},"source":" I found [note](https://www.kaggle.com/abhmul/keras-convnet-lb-0-0052-w-visualization) this code was not useable, so I modify the code , Now it can run again","cell_type":"markdown"},{"metadata":{},"outputs":[],"source":"def load_numeric_training(standardize=True):\n    \"\"\"\n    Loads the pre-extracted features for the training data\n    and returns a tuple of the image ids, the data, and the labels\n    \"\"\"\n    # Read data from the CSV file\n    data = pd.read_csv(os.path.join(root, 'train.csv'))\n    ID = data.pop('id')\n\n    # Since the labels are textual, so we encode them categorically\n    y = data.pop('species')\n    y = LabelEncoder().fit(y).transform(y)\n    # standardize the data by setting the mean to 0 and std to 1\n    X = StandardScaler().fit(data).transform(data) if standardize else data.values\n\n    return ID, X, y\n\n\ndef load_numeric_test(standardize=True):\n    \"\"\"\n    Loads the pre-extracted features for the test data\n    and returns a tuple of the image ids, the data\n    \"\"\"\n    test = pd.read_csv(os.path.join(root, 'test.csv'))\n    ID = test.pop('id')\n    # standardize the data by setting the mean to 0 and std to 1\n    test = StandardScaler().fit(test).transform(test) if standardize else test.values\n    return ID, test\n\n\ndef resize_img(img, max_dim=96):\n    \"\"\"\n    Resize the image to so the maximum side is of size max_dim\n    Returns a new image of the right size\n    \"\"\"\n    # Get the axis with the larger dimension\n    max_ax = max((0, 1), key=lambda i: img.size[i])\n    # Scale both axes so the image's largest dimension is max_dim\n    scale = max_dim / float(img.size[max_ax])\n    return img.resize((int(img.size[0] * scale), int(img.size[1] * scale)))\n\n\ndef load_image_data(ids, max_dim=96, center=True):\n    \"\"\"\n    Takes as input an array of image ids and loads the images as numpy\n    arrays with the images resized so the longest side is max-dim length.\n    If center is True, then will place the image in the center of\n    the output array, otherwise it will be placed at the top-left corner.\n    \"\"\"\n    # Initialize the output array\n    # NOTE: Theano users comment line below and\n    X = np.empty((len(ids), max_dim, max_dim, 1))\n    # X = np.empty((len(ids), 1, max_dim, max_dim)) # uncomment this\n    for i, idee in enumerate(ids):\n        # Turn the image into an array\n        x = resize_img(load_img(os.path.join(root, 'images', str(idee) + '.jpg'), grayscale=True), max_dim=max_dim)\n        x = img_to_array(x)\n        # Get the corners of the bounding box for the image\n        # NOTE: Theano users comment the two lines below and\n        length = x.shape[0]\n        width = x.shape[1]\n        # length = x.shape[1] # uncomment this\n        # width = x.shape[2] # uncomment this\n        if center:\n            h1 = int((max_dim - length) / 2)\n            h2 = h1 + length\n            w1 = int((max_dim - width) / 2)\n            w2 = w1 + width\n        else:\n            h1, w1 = 0, 0\n            h2, w2 = (length, width)\n        # Insert into image matrix\n        # NOTE: Theano users comment line below and\n        X[i, h1:h2, w1:w2, 0:1] = x\n        # X[i, 0:1, h1:h2, w1:w2] = x  # uncomment this\n    # Scale the array values so they are between 0 and 1\n    return np.around(X / 255.0)\n\n\ndef load_train_data(split=split, random_state=None):\n    \"\"\"\n    Loads the pre-extracted feature and image training data and\n    splits them into training and cross-validation.\n    Returns one tuple for the training data and one for the validation\n    data. Each tuple is in the order pre-extracted features, images,\n    and labels.\n    \"\"\"\n    # Load the pre-extracted features\n    ID, X_num_tr, y = load_numeric_training()\n    # Load the image data\n    X_img_tr = load_image_data(ID)\n    # Split them into validation and cross-validation\n    sss = StratifiedShuffleSplit(n_splits=1, train_size=split, random_state=random_state)\n    train_ind, test_ind = next(sss.split(X_num_tr, y))\n    X_num_val, X_img_val, y_val = X_num_tr[test_ind], X_img_tr[test_ind], y[test_ind]\n    X_num_tr, X_img_tr, y_tr = X_num_tr[train_ind], X_img_tr[train_ind], y[train_ind]\n    return (X_num_tr, X_img_tr, y_tr), (X_num_val, X_img_val, y_val)\n\n\ndef load_test_data():\n    \"\"\"\n    Loads the pre-extracted feature and image test data.\n    Returns a tuple in the order ids, pre-extracted features,\n    and images.\n    \"\"\"\n    # Load the pre-extracted features\n    ID, X_num_te = load_numeric_test()\n    # Load the image data\n    X_img_te = load_image_data(ID)\n    return ID, X_num_te, X_img_te\n\nprint('Loading the training data...')\n(X_num_tr, X_img_tr, y_tr), (X_num_val, X_img_val, y_val) = load_train_data(random_state=split_random_state)\ny_tr_cat = to_categorical(y_tr)\ny_val_cat = to_categorical(y_val)\nprint('Training data loaded!')","cell_type":"code","execution_count":5},{"metadata":{"collapsed":true},"outputs":[],"source":"from keras.preprocessing.image import ImageDataGenerator, NumpyArrayIterator, array_to_img\n\nclass ImageDataGenerator2(ImageDataGenerator):\n    def flow(self, X, y=None, batch_size=32, shuffle=True, seed=None,\n             save_to_dir=None, save_prefix='', save_format='jpeg'):\n        return NumpyArrayIterator2(\n            X, y, self,\n            batch_size=batch_size, shuffle=shuffle, seed=seed,\n            save_to_dir=save_to_dir, save_prefix=save_prefix, save_format=save_format)\n\nclass NumpyArrayIterator2(NumpyArrayIterator):\n    def next(self):\n        with self.lock:\n            self.my_index_array = next(self.index_generator)\n            return self._get_batches_of_transformed_samples(self.my_index_array)\n        \n","cell_type":"code","execution_count":9},{"metadata":{},"outputs":[],"source":"print('Creating Data Augmenter...')\nimgen = ImageDataGenerator2(rotation_range=20, zoom_range=0.2, horizontal_flip=True, vertical_flip=True, fill_mode='nearest')\nimgen_train = imgen.flow(X_img_tr, y_tr_cat, seed=np.random.randint(1, 10000))\nprint('Finished making data augmenter...')","cell_type":"code","execution_count":10},{"metadata":{},"outputs":[],"source":"from keras.models import Model\nfrom keras.layers import Dense, Dropout, Activation, Conv2D, MaxPooling2D, Flatten, Input, concatenate\n\n\ndef combined_model():\n\n    # Define the image input\n    image = Input(shape=(96, 96, 1), name='image')\n    # Pass it through the first convolutional layer\n    x = Conv2D(8, (5, 5), input_shape=(96, 96, 1), padding='same')(image)\n    x = (Activation('relu'))(x)\n    x = (MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))(x)\n\n    # Now through the second convolutional layer\n    x = (Conv2D(32, (5, 5), padding='same'))(x)\n    x = (Activation('relu'))(x)\n    x = (MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))(x)\n\n    # Flatten our array\n    x = Flatten()(x)\n    # Define the pre-extracted feature input\n    numerical = Input(shape=(192,), name='numerical')\n    # Concatenate the output of our convnet with our pre-extracted feature input\n    concatenated = concatenate([x, numerical], axis=-1)\n\n    # Add a fully connected layer just like in a normal MLP\n    x = Dense(100, activation='relu')(concatenated)\n    x = Dropout(.5)(x)\n\n    # Get the final output\n    out = Dense(99, activation='softmax')(x)\n    # How we create models with the Functional API\n    model = Model(input=[image, numerical], output=out)\n    model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n\n    return model","cell_type":"code","execution_count":12},{"metadata":{},"outputs":[],"source":"print('Creating the model...')\nmodel = combined_model()\nprint('Model created!')","cell_type":"code","execution_count":13},{"metadata":{"collapsed":true},"outputs":[],"source":"from keras.callbacks import ModelCheckpoint\nfrom keras.models import load_model\n\n\ndef combined_generator(imgen, X):\n    \"\"\"\n    A generator to train our keras neural network. It\n    takes the image augmenter generator and the array\n    of the pre-extracted features.\n    It yields a minibatch and will run indefinitely\n    \"\"\"\n    while True:\n        for i in range(X.shape[0]):\n            # Get the image batch and labels\n            batch_img, batch_y = next(imgen)\n            # This is where that change to the source code we\n            # made will come in handy. We can now access the indicies\n            # of the images that imgen gave us.\n            ############################################\n            x = X[imgen.my_index_array]\n            #############################################\n            yield [batch_img, x], batch_y","cell_type":"code","execution_count":14},{"metadata":{},"outputs":[],"source":"# autosave best Model\nbest_model_file = \"leafnet.h5\"\nbest_model = ModelCheckpoint(best_model_file, monitor='val_loss', verbose=1, save_best_only=True)\n\nprint('Training model...')\nhistory = model.fit_generator(combined_generator(imgen_train, X_num_tr),\n                              samples_per_epoch=X_num_tr.shape[0],\n                              nb_epoch=10,\n                              validation_data=([X_img_val, X_num_val], y_val_cat),\n                              nb_val_samples=X_num_val.shape[0],\n                              verbose=0,\n                              callbacks=[best_model])\n\nprint('Loading the best model...')\nmodel = load_model(best_model_file)\nprint('Best Model loaded!')","cell_type":"code","execution_count":15},{"metadata":{},"outputs":[],"source":"# Get the names of the column headers\nLABELS = sorted(pd.read_csv(os.path.join(root, 'train.csv')).species.unique())\n\nindex, test, X_img_te = load_test_data()\n\nyPred_proba = model.predict([X_img_te, test])\n\n# Converting the test predictions in a dataframe as depicted by sample submission\nyPred = pd.DataFrame(yPred_proba,index=index,columns=LABELS)\n\nprint('Creating and writing submission...')\nfp = open('submit.csv', 'w')\nfp.write(yPred.to_csv())\nprint('Finished writing submission')\n# Display the submission\nyPred.tail()","cell_type":"code","execution_count":16},{"metadata":{},"outputs":[],"source":"from math import sqrt\n\nimport matplotlib.pyplot as plt\nfrom keras import backend as K\n\nNUM_LEAVES = 3\nmodel_fn = 'leafnet.h5'\n\n# Function by gcalmettes from http://stackoverflow.com/questions/11159436/multiple-figures-in-a-single-window\ndef plot_figures(figures, nrows = 1, ncols=1, titles=False):\n    \"\"\"Plot a dictionary of figures.\n\n    Parameters\n    ----------\n    figures : <title, figure> dictionary\n    ncols : number of columns of subplots wanted in the display\n    nrows : number of rows of subplots wanted in the figure\n    \"\"\"\n\n    fig, axeslist = plt.subplots(ncols=ncols, nrows=nrows)\n    for ind,title in enumerate(sorted(figures.keys(), key=lambda s: int(s[3:]))):\n        axeslist.ravel()[ind].imshow(figures[title], cmap=plt.gray())\n        if titles:\n            axeslist.ravel()[ind].set_title(title)\n\n    for ind in range(nrows*ncols):\n        axeslist.ravel()[ind].set_axis_off()\n\n    if titles:\n        plt.tight_layout()\n    plt.show()\n\n\ndef get_dim(num):\n    \"\"\"\n    Simple function to get the dimensions of a square-ish shape for plotting\n    num images\n    \"\"\"\n\n    s = sqrt(num)\n    if round(s) < s:\n        return (int(s), int(s)+1)\n    else:\n        return (int(s)+1, int(s)+1)\n\n# Load the best model\nmodel = load_model(model_fn)\n\n# Get the convolutional layers\nconv_layers = [layer for layer in model.layers if isinstance(layer, MaxPooling2D)]\n\n# Pick random images to visualize\nimgs_to_visualize = np.random.choice(np.arange(0, len(X_img_val)), NUM_LEAVES)\n\n# Use a keras function to extract the conv layer data\nconvout_func = K.function([model.layers[0].input, K.learning_phase()], [layer.output for layer in conv_layers])\nconv_imgs_filts = convout_func([X_img_val[imgs_to_visualize], 0])\n# Also get the prediction so we know what we predicted\npredictions = model.predict([X_img_val[imgs_to_visualize], X_num_val[imgs_to_visualize]])\n\nimshow = plt.imshow #alias\n# Loop through each image disply relevant info\nfor img_count, img_to_visualize in enumerate(imgs_to_visualize):\n\n    # Get top 3 predictions\n    top3_ind = predictions[img_count].argsort()[-3:]\n    top3_species = np.array(LABELS)[top3_ind]\n    top3_preds = predictions[img_count][top3_ind]\n\n    # Get the actual leaf species\n    actual = LABELS[y_val[img_to_visualize]]\n\n    # Display the top 3 predictions and the actual species\n    print(\"Top 3 Predicitons:\")\n    for i in range(2, -1, -1):\n        print(\"\\t%s: %s\" % (top3_species[i], top3_preds[i]))\n    print(\"\\nActual: %s\" % actual)\n\n    # Show the original image\n    plt.title(\"Image used: #%d (digit=%d)\" % (img_to_visualize, y_val[img_to_visualize]))\n    # For Theano users comment the line below and\n    imshow(X_img_val[img_to_visualize][:, :, 0], cmap='gray')\n    # imshow(X_img_val[img_to_visualize][0], cmap='gray') # uncomment this\n    plt.tight_layout()\n    plt.show()\n\n    # Plot the filter images\n    for i, conv_imgs_filt in enumerate(conv_imgs_filts):\n        conv_img_filt = conv_imgs_filt[img_count]\n        print(\"Visualizing Convolutions Layer %d\" % i)\n        # Get it ready for the plot_figures function\n        # For Theano users comment the line below and\n        fig_dict = {'flt{0}'.format(i): conv_img_filt[:, :, i] for i in range(conv_img_filt.shape[-1])}\n        # fig_dict = {'flt{0}'.format(i): conv_img_filt[i] for i in range(conv_img_filt.shape[-1])} # uncomment this\n        plot_figures(fig_dict, *get_dim(len(fig_dict)))","cell_type":"code","execution_count":17}],"nbformat":4}