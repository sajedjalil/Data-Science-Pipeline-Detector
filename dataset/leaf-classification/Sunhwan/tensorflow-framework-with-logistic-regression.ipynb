{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"50bb3efc-a45e-4952-562f-56755d2a473a"},"source":"# The Idea\n\nThis notebook is created mainly to provide a framework to use Tensorflow for building a logistic regression model with the pre-extracted features. You have a room to improve the results by including more features and trying different algorithms.\n\n**Step 1:** Define paramaters for the model, read source data, and create train / test data array"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"01b742cb-803f-ee2a-89a5-7288ea5ff76f"},"outputs":[],"source":"import tensorflow as tf\nimport numpy as np\nimport pandas as pd\nimport time\nfrom sklearn.utils import shuffle\n\n# Define paramaters for the model\nlearning_rate = 0.01\nbatch_size = 33\nn_epochs = 100\n\n# Step 1.1: Read in data\ntrain_df = pd.read_csv('../input/train.csv') \ntest_df  = pd.read_csv('../input/test.csv') \n\n# Step 1.2: create train and test data array\ntrain_data  = train_df.loc[:,'margin1':'texture64']\ntest_data   = test_df.loc[:,'margin1':'texture64']\ntarget_data = pd.get_dummies(train_df.species)\n\nX_train = train_data.as_matrix()\nX_test  = test_data.as_matrix()\ny_train = target_data.as_matrix()\n\nnum_train = X_train.shape[0]\nnum_test  = X_test.shape[0]"},{"cell_type":"markdown","metadata":{"_cell_guid":"8ace9033-887e-2996-00f7-4250067368c3"},"source":"**Step 2:** Create placeholders for features and labels. Each image is represented with 1x192 tensor and there are 99 classes for each image and each lable is one hot vector."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c1a9a9e2-0ebb-e903-1d74-6f9db4a2450b"},"outputs":[],"source":"X = tf.placeholder(tf.float32, [None, 192], name='X_placeholder') \nY = tf.placeholder(tf.float32, [None, 99], name='Y_placeholder')"},{"cell_type":"markdown","metadata":{"_cell_guid":"cdcd447b-48b5-576a-a03a-ba417b157a5b"},"source":"**Step 3:** Create weights and bias\n* w is initialized to random variables with mean of 0, stddev of 0.01\n* b is initialized to 0\n* shape of w depends on the dimension of X and Y so that Y = tf.matmul(X, w)\n* shape of b depends on Y"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c1e48f8d-871c-a8cc-52e4-35f38d493cd0"},"outputs":[],"source":"w = tf.Variable(tf.random_normal(shape=[192, 99], stddev=0.01), name='weights')\nb = tf.Variable(tf.zeros([1, 99]), name=\"bias\")"},{"cell_type":"markdown","metadata":{"_cell_guid":"934fb27c-ba7a-1371-2830-2a64ad1bc692"},"source":"**Step 4:** Build model that returns the logits. This logits will be later passed through softmax layer."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f0bbd2a1-81d0-3d1f-2ae2-6034da0c90f7"},"outputs":[],"source":"logits = tf.matmul(X, w) + b "},{"cell_type":"markdown","metadata":{"_cell_guid":"83d68f71-01ab-b60b-6888-e720afd35f52"},"source":"**Step 5:** Define log loss function. Let's use cross entropy of softmax of logits as the loss function."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d2977fdc-df74-4f75-5f0c-18a5440b60cf"},"outputs":[],"source":"y = tf.nn.softmax(logits)\nloss = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(y), reduction_indices=[1]))"},{"cell_type":"markdown","metadata":{"_cell_guid":"4c8d191e-ea39-28da-802b-40beae7c6745"},"source":"**Step 6:** Define training operation using Adam optimizer with learning rate of 0.01 to minimize loss and apply a trained model to test dataset.\n\n**Note:** Because the number of features, 192, is relatively large considering the number of data points, 990, I used Adam optimizer instead of gradient descent optimizer. It turns out Adam optimizer outperforms gradient descent optimizer! Feel free to test it by replacing tf.train.AdamOptimizer by tf.train.GradientDescentOptimizer."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"aea24b92-a5f3-dc42-b9bd-f85c9edb3b31"},"outputs":[],"source":"optimizer = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n\nwith tf.Session() as sess:\n    # to visualize using TensorBoard\n    #writer = tf.summary.FileWriter('./graph/leaf_lr', sess.graph)\n\n    start_time = time.time()\n    sess.run(tf.global_variables_initializer())\t\n    n_batches = int(num_train/batch_size)\n\n    for i in range(n_epochs): # train the model n_epochs times\n\n        # shuffle X, y\n        X_train, y_train = shuffle(X_train, y_train)\n        total_loss   = 0\n\n        for j in range(n_batches):\n            X_batch, Y_batch = X_train[j*batch_size:(j+1)*batch_size], y_train[j*batch_size:(j+1)*batch_size]\n            _, loss_batch = sess.run([optimizer, loss], feed_dict={X: X_batch, Y:Y_batch}) \n            total_loss += loss_batch\n\n        print('Average loss epoch {0}: {1}'.format(i, total_loss/n_batches))\n\n    print('Total time: {0} seconds'.format(time.time() - start_time))\n\n    print('Optimization Finished!') # should be around 0.35 after 25 epochs\n\n    # test the model\n    logits_test = sess.run(logits, feed_dict={X: X_test}) \n    Y_pred = sess.run(tf.nn.softmax(logits_test))\n\n    #writer.close()"},{"cell_type":"markdown","metadata":{"_cell_guid":"839b3a5f-1c31-08f9-f208-88c183fe0695"},"source":"**Step 7:** Write to the submission file"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"92f664ea-273a-e791-ccd9-26b5c4dba1a6"},"outputs":[],"source":"sample_submission  = pd.read_csv('../input/sample_submission.csv')\ncol_idx = list(sample_submission)[1:]\nrow_idx = sample_submission.id.values\n\nsubmission = pd.DataFrame(data=Y_pred, index=row_idx, columns=col_idx)\nprint(submission.head())"},{"cell_type":"markdown","metadata":{"_cell_guid":"39a68bff-5fbe-5492-9e7f-f8d0959e47a6"},"source":"Though Tensowflow site has great tutorials, I wanted to provide a simple framework to use Tensorflow for building a loss function, selecting a optimizer, training the model, and finally testing the model. To me, Tensorflow is very flexible to experiment different optimizers and loss functions and I hope you find the same!\n\n## Thank you for reading!"}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"}},"nbformat":4,"nbformat_minor":0}