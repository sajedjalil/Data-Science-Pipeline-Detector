{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"92573c64-943b-a7af-c782-79140887a0ae"},"outputs":[],"source":"import os\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import StratifiedShuffleSplit\n\nfrom keras.utils.np_utils import to_categorical\nfrom keras.preprocessing.image import img_to_array, load_img\n\n# A large amount of the data loading code is based on najeebkhan's kernel\n# Check it out at https://www.kaggle.com/najeebkhan/leaf-classification/neural-network-through-keras\nroot = '../input'\nnp.random.seed(2016)\nsplit_random_state = 7\nsplit = .9\n\n\ndef load_numeric_training(standardize=True):\n    \"\"\"\n    Loads the pre-extracted features for the training data\n    and returns a tuple of the image ids, the data, and the labels\n    \"\"\"\n    # Read data from the CSV file\n    data = pd.read_csv(os.path.join(root, 'train.csv'))\n    ID = data.pop('id')\n\n    # Since the labels are textual, so we encode them categorically\n    y = data.pop('species')\n    y = LabelEncoder().fit(y).transform(y)\n    # standardize the data by setting the mean to 0 and std to 1\n    X = StandardScaler().fit(data).transform(data) if standardize else data.values\n\n    return ID, X, y\n\n\ndef load_numeric_test(standardize=True):\n    \"\"\"\n    Loads the pre-extracted features for the test data\n    and returns a tuple of the image ids, the data\n    \"\"\"\n    test = pd.read_csv(os.path.join(root, 'test.csv'))\n    ID = test.pop('id')\n    # standardize the data by setting the mean to 0 and std to 1\n    test = StandardScaler().fit(test).transform(test) if standardize else test.values\n    return ID, test\n\n\ndef resize_img(img, max_dim=96):\n    \"\"\"\n    Resize the image to so the maximum side is of size max_dim\n    Returns a new image of the right size\n    \"\"\"\n    # Get the axis with the larger dimension\n    max_ax = max((0, 1), key=lambda i: img.size[i])\n    # Scale both axes so the image's largest dimension is max_dim\n    scale = max_dim / float(img.size[max_ax])\n    return img.resize((int(img.size[0] * scale), int(img.size[1] * scale)))\n\n\ndef load_image_data(ids, max_dim=96, center=True):\n    \"\"\"\n    Takes as input an array of image ids and loads the images as numpy\n    arrays with the images resized so the longest side is max-dim length.\n    If center is True, then will place the image in the center of\n    the output array, otherwise it will be placed at the top-left corner.\n    \"\"\"\n    # Initialize the output array\n    # NOTE: Theano users comment line below and\n    X = np.empty((len(ids), max_dim, max_dim, 1))\n    # X = np.empty((len(ids), 1, max_dim, max_dim)) # uncomment this\n    for i, idee in enumerate(ids):\n        # Turn the image into an array\n        x = resize_img(load_img(os.path.join(root, 'images', str(idee) + '.jpg'), grayscale=True), max_dim=max_dim)\n        x = img_to_array(x)\n        # Get the corners of the bounding box for the image\n        # NOTE: Theano users comment the two lines below and\n        length = x.shape[0]\n        width = x.shape[1]\n        # length = x.shape[1] # uncomment this\n        # width = x.shape[2] # uncomment this\n        if center:\n            h1 = int((max_dim - length) / 2)\n            h2 = h1 + length\n            w1 = int((max_dim - width) / 2)\n            w2 = w1 + width\n        else:\n            h1, w1 = 0, 0\n            h2, w2 = (length, width)\n        # Insert into image matrix\n        # NOTE: Theano users comment line below and\n        X[i, h1:h2, w1:w2, 0:1] = x\n        # X[i, 0:1, h1:h2, w1:w2] = x  # uncomment this\n    # Scale the array values so they are between 0 and 1\n    return np.around(X / 255.0)\n\n\ndef load_train_data(split=split, random_state=None):\n    \"\"\"\n    Loads the pre-extracted feature and image training data and\n    splits them into training and cross-validation.\n    Returns one tuple for the training data and one for the validation\n    data. Each tuple is in the order pre-extracted features, images,\n    and labels.\n    \"\"\"\n    # Load the pre-extracted features\n    ID, X_num_tr, y = load_numeric_training()\n    # Load the image data\n    X_img_tr = load_image_data(ID)\n    # Split them into validation and cross-validation\n    sss = StratifiedShuffleSplit(n_splits=1, train_size=split, random_state=random_state)\n    train_ind, test_ind = next(sss.split(X_num_tr, y))\n    X_num_val, X_img_val, y_val = X_num_tr[test_ind], X_img_tr[test_ind], y[test_ind]\n    X_num_tr, X_img_tr, y_tr = X_num_tr[train_ind], X_img_tr[train_ind], y[train_ind]\n    return (X_num_tr, X_img_tr, y_tr), (X_num_val, X_img_val, y_val)\n\n\ndef load_test_data():\n    \"\"\"\n    Loads the pre-extracted feature and image test data.\n    Returns a tuple in the order ids, pre-extracted features,\n    and images.\n    \"\"\"\n    # Load the pre-extracted features\n    ID, X_num_te = load_numeric_test()\n    # Load the image data\n    X_img_te = load_image_data(ID)\n    return ID, X_num_te, X_img_te\n\nprint('Loading the training data...')\n(X_num_tr, X_img_tr, y_tr), (X_num_val, X_img_val, y_val) = load_train_data(random_state=split_random_state)\ny_tr_cat = to_categorical(y_tr)\ny_val_cat = to_categorical(y_val)\nprint('Training data loaded!')"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f85cf76e-4ee4-03a1-2f10-98da59bb2138"},"outputs":[],"source":""},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"0dcc5f0d-a3bd-1649-0cc5-c13b9123732a"},"outputs":[],"source":"import pandas as pd\nimport numpy as np\nimport scipy as scipy\nimport tensorflow as tf"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"2ef3f9c9-f346-4669-f3df-a7b5d14f0c3a"},"outputs":[],"source":"def iterate_minibatches(inputs, inputs_img, targets, batchsize, shuffle=False):\n    assert len(inputs) == len(targets)\n    if shuffle:\n        indices = np.arange(len(inputs))\n        np.random.shuffle(indices)\n    for start_idx in range(0, len(inputs) - batchsize + 1, batchsize):\n        if shuffle:\n            excerpt = indices[start_idx:start_idx + batchsize]\n        else:\n            excerpt = slice(start_idx, start_idx + batchsize)\n        yield inputs[excerpt], inputs_img[excerpt], targets[excerpt]\n\ndef weight_variable(shape):\n    \"\"\"Create a weight variable with appropriate initialization.\"\"\"\n    initial = tf.truncated_normal(shape, stddev=0.1)\n    return tf.Variable(initial)\n\n\ndef bias_variable(shape):\n    \"\"\"Create a bias variable with appropriate initialization.\"\"\"\n    initial = tf.constant(0.1, shape=shape)\n    return tf.Variable(initial)\n\ndef nn_layer(input_tensor, input_dim, output_dim, layer_name, act=tf.nn.relu):\n    \"\"\"Reusable code for making a simple neural net layer.\n    It does a matrix multiply, bias add, and then uses relu to nonlinearize.\n    It also sets up name scoping so that the resultant graph is easy to read,\n    and adds a number of summary ops.\n    \"\"\"\n    # Adding a name scope ensures logical grouping of the layers in the graph.\n    weights = weight_variable([input_dim, output_dim])\n    biases = bias_variable([output_dim])\n    preactivate = tf.matmul(input_tensor, weights) + biases\n    activations = act(preactivate, name='activation')\n    return activations\n\ndef conv2d(x, W):\n    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n\ndef max_pool_2x2(x):\n    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n                        strides=[1, 2, 2, 1], padding='SAME')\n"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"aa806af0-840b-ccf0-92cb-16d33b4a71f9"},"outputs":[],"source":"df = pd.read_csv('../input/train.csv')"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"78dae40f-1891-5053-8c59-1e940900d61d"},"outputs":[],"source":"num_classes = 99"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"adf4a8b9-2818-cd31-095a-94728bf3e2a6"},"outputs":[],"source":"train_df = df\nprint(train_df.shape)\ntrain_df.head()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"6bebfc3f-e06b-b931-49a2-41b5fd363540"},"outputs":[],"source":"print(train_df.species.value_counts(normalize=True))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"fab2347c-6de3-71ef-30f0-7849aaecf5b8"},"outputs":[],"source":"species = train_df['species'].values"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f0d2b57d-888c-ec99-3754-cf65caeaa749"},"outputs":[],"source":"species.shape"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c343bf35-e6ad-e617-eeda-245fb7abde66"},"outputs":[],"source":"species[1]"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"69e791ef-cc12-bcec-320f-ce680c28d09c"},"outputs":[],"source":"print(Y_train.shape)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a05b4edd-16b7-466d-cd01-d439e4ccec36"},"outputs":[],"source":"X_train = train_df.values"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"6f52509f-5038-822f-c435-8e619b6672b8"},"outputs":[],"source":"X_train = scipy.delete(X_train, 0, 1) # Remove first and second columns\nX_train = scipy.delete(X_train, 0, 1) "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"2fab3e93-cb8d-2bb6-8ec4-c8498478c4dc"},"outputs":[],"source":"print(X_train.shape)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"aa199b79-6242-ef56-a1a4-8aa4ab6668e3"},"outputs":[],"source":"X_train, X_val = X_train[:-200], X_train[-200:]\nX_train, X_test = X_train[:-100], X_train[-100:]"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"253037b0-497e-807e-1d64-7ee07165093a"},"outputs":[],"source":"Y_train, Y_val = Y_train[:-200], Y_train[-200:]\nY_train, Y_test = Y_train[:-100], Y_train[-100:]"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c28ca8a9-70af-0f95-ccc7-3b4216668436"},"outputs":[],"source":"sess = tf.InteractiveSession()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"46a42469-329b-8c54-814a-6d3cf3202fc9"},"outputs":[],"source":"## Create the model\nnum_classes = 99\nx_feature = tf.placeholder(tf.float32, [None, 192])\nx_img = tf.placeholder(tf.float32, [None, 96, 96, 1])\ny_ = tf.placeholder(tf.float32, [None, num_classes])\nkeep_prob = tf.placeholder(tf.float32)\n\nnet = tf.reshape(x_img, [-1, 96*96*1])\nnet = nn_layer(net, 96*96*1, 1024, 'layer1')\nnet = nn_layer(net, 1024, 512, 'layer2')\nnet = nn_layer(net, 512, 256, 'layer3')\nnet = tf.concat(1, [net, x_feature])\nnet = tf.nn.dropout(net, keep_prob)\nnet = nn_layer(net, 256+192, 512, 'layer4')\nnet = tf.nn.dropout(net, keep_prob)\nnet = nn_layer(net, 512, 1024, 'layer5')\ny = nn_layer(net, 1024, num_classes, 'layer6', act=tf.identity)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"9c15d3d9-128d-0a06-03e4-aa72dd27db32"},"outputs":[],"source":"cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y))\ntrain_step = tf.train.AdamOptimizer(\n    learning_rate=1e-4, beta1=0.9, beta2=0.999, epsilon=1e-08, use_locking=False, name='Adam')\\\n.minimize(cross_entropy)\n\nprediction = tf.nn.softmax(y)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"ca172a9b-763a-0999-4aab-a94352decee5"},"outputs":[],"source":"tf.global_variables_initializer().run()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"4d5d3d99-5439-c14c-63fa-339ad1094810"},"outputs":[],"source":"min_loss = 100\nsaver = tf.train.Saver()\nsave_path = ''"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e11848c5-0f81-fc6a-9d1d-a1b0525f588c"},"outputs":[],"source":"#X_num_tr, X_img_tr, y_tr, X_num_val, X_img_val, y_val\n\n\nfor i in range(50):\n    if i % 5 == 0:\n        print(i)\n    for batch in iterate_minibatches(X_num_tr, X_img_tr, y_tr_cat, 50, shuffle=True):\n        batch_xs, batch_xs_img, batch_ys = batch\n        sess.run(train_step, feed_dict={x_img: batch_xs_img, x_feature: batch_xs, y_: batch_ys, keep_prob: 0.5})\n\n        # Test trained model\n        correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n    print(\"tr:{}\".format(sess.run(accuracy, feed_dict={x_img: X_img_tr, \n                                                       x_feature: X_num_tr, y_: y_tr_cat, keep_prob: 1.0})))\n    print(\"val:{}\".format(sess.run(accuracy, feed_dict={x_img: X_img_val, \n                                                        x_feature: X_num_val, y_: y_val_cat, keep_prob: 1.0})))\n    cur_loss = sess.run(cross_entropy, feed_dict={x_img: X_img_val, \n                                                        x_feature: X_num_val, y_: y_val_cat, keep_prob: 1.0})\n    if cur_loss < min_loss:\n        min_loss = cur_loss\n        save_path = saver.save(sess, 'my-model')\n        print(\"!!!NEW MIN LOSS {}. Saved at {}\".format(cur_loss, save_path))\n\n    else: \n        print(\"val_los:{}\".format(cur_loss))\n\nprint(\"val:{}\".format(sess.run(accuracy, feed_dict={x_img: X_img_val, \n                                                        x_feature: X_num_val, y_: y_val_cat, keep_prob: 1.0})))\n    "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c57299b2-8f91-073a-0f3e-1e7b54e976a5"},"outputs":[],"source":"with tf.Session() as sess2:\n    # Initialize variables\n    #sess.run(init)\n    tf.global_variables_initializer().run()\n\n    # Restore model weights from previously saved model\n    saver.restore(sess2, save_path)\n    print(\"Model restored from file: %s\" % save_path)\n\n    loss = sess2.run(cross_entropy, feed_dict={x_img: X_img_val, \n                                                        x_feature: X_num_val, y_: y_val_cat, keep_prob: 1.0})\n    \n    print(\"val:{}\".format(loss))\n    \n    \n    LABELS = sorted(pd.read_csv(os.path.join(root, 'train.csv')).species.unique())\n\n\n    index, test, X_img_te = load_test_data()\n    yPred_proba = sess2.run(prediction, feed_dict={x_img: X_img_te, \n                                                      x_feature: test, keep_prob: 1.0})\n\n    # Converting the test predictions in a dataframe as depicted by sample submission\n    yPred = pd.DataFrame(yPred_proba,index=index,columns=LABELS)\n\n    print('Creating and writing submission...')\n    #fp = open('submit.csv', 'w')\n    #fp.write(yPred.to_csv())\n    #print('Finished writing submission')\n    # Display the submission\n    yPred.tail()\n"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"653ce417-39b0-3ed8-f26e-10e863264c96"},"outputs":[],"source":"loss = sess.run(cross_entropy, feed_dict={x_img: X_img_val, \n                                                        x_feature: X_num_val, y_: y_val_cat, keep_prob: 1.0})\nprint(\"val:{}\".format(loss))\n    "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"fcee3a3b-c105-398f-e4e1-993456275f80"},"outputs":[],"source":"LABELS = sorted(pd.read_csv(os.path.join(root, 'train.csv')).species.unique())\n\n\nindex, test, X_img_te = load_test_data()\nyPred_proba = sess.run(prediction, feed_dict={x_img: X_img_te, \n                                                  x_feature: test, keep_prob: 1.0})\n\n# Converting the test predictions in a dataframe as depicted by sample submission\nyPred = pd.DataFrame(yPred_proba,index=index,columns=LABELS)\n\nprint('Creating and writing submission...')\nfp = open('submit.csv', 'w')\nfp.write(yPred.to_csv())\nprint('Finished writing submission')\n# Display the submission\n"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"903e221f-2882-a42e-5f44-c531ee62f46c"},"outputs":[],"source":"fp = open('submit.csv', 'w')\nfp.write(yPred.to_csv())"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"5f4a71b3-9e5d-0609-5598-130cdaa25b28"},"outputs":[],"source":"yPred.head()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"cd907334-7c33-cc58-f6f1-3ee763f1aeba"},"outputs":[],"source":""}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"}},"nbformat":4,"nbformat_minor":0}