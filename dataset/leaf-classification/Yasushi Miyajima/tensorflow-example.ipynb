{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"4fc4fea1-b3b8-d8ee-a1c9-7c960643e19f"},"source":"I have implemented 2 hidden layers NN with TensorFlow. \nI haven't found the best hyper parameters.\nPlease try other combinations of the parameters.\n\nI'm studying deep learning now and not an expert of using TensorFlow and Python.\nPlease forgive me if there are redundant codes."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"bcb28afc-ad3c-4b73-166e-9b6e427d2eda"},"outputs":[],"source":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport math"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e4c54b04-18c9-20bf-bad4-f3a848e3235a"},"outputs":[],"source":"flags = tf.app.flags\nFLAGS = flags.FLAGS\n\nflags.DEFINE_integer('num_classes', 99, 'Number of classes.')\nflags.DEFINE_integer('num_variables', 192, 'Number of variables.')\n\n# Hyper Parameters\nflags.DEFINE_integer('hidden1', 2048, 'Number of units in hidden layer 1.')\nflags.DEFINE_integer('hidden2', 1024, 'Number of units in hidden layer 2.')\nflags.DEFINE_integer('num_epochs', 200, 'Number of learning epochs.')\nflags.DEFINE_integer('batch_size', 90, 'Batch size.')\nflags.DEFINE_float('keep_prob', 0.5, 'Keep probability for drop out.')\nflags.DEFINE_float('learning_rate', 0.001, 'Initial learning rate.')"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c71e05f1-4179-b656-41e5-ce2cffa070a1"},"outputs":[],"source":"def inference(data, data_size, keep_prob):\n    # Hidden layer 1\n    with tf.name_scope('hidden1'):\n        weights = tf.Variable(tf.truncated_normal([data_size, FLAGS.hidden1],\n                                                  stddev=1.0 / math.sqrt(float(data_size))), name='weights1')\n        biases = tf.Variable(tf.zeros([FLAGS.hidden1]), name='biases1')\n        hidden1 = tf.nn.relu(tf.matmul(data, weights) + biases)\n\n        # Dropout before layer 2\n        hidden1_drop = tf.nn.dropout(hidden1, keep_prob, name='layer1_dropout')\n\n    # Hidden layer 2\n    with tf.name_scope('hidden2'):\n        weights = tf.Variable(tf.truncated_normal([FLAGS.hidden1, FLAGS.hidden2],\n                                                  stddev=1.0 / math.sqrt(float(FLAGS.hidden1))), name='weights2')\n        biases = tf.Variable(tf.zeros([FLAGS.hidden2]), name='biases2')\n        hidden2 = tf.nn.sigmoid(tf.matmul(hidden1_drop, weights) + biases)\n\n        # Dropout before linear reading out\n        hidden2_drop = tf.nn.dropout(hidden2, keep_prob, name='layer2_dropout')\n\n    # Read out\n    with tf.name_scope('softmax_linear'):\n        weights = tf.Variable(tf.truncated_normal([FLAGS.hidden2, FLAGS.num_classes],\n                                                  stddev=1.0 / math.sqrt(float(FLAGS.hidden2))), name='weights')\n        biases = tf.Variable(tf.zeros([FLAGS.num_classes]), name='biases')\n        logits = tf.matmul(hidden2_drop, weights) + biases\n\n        return logits"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"8b496988-369e-872f-7161-a6c595cae249"},"outputs":[],"source":"def loss(logits, labels):\n    labels = tf.to_int64(labels)\n    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits, labels, name='xentropy')\n    loss = tf.reduce_mean(cross_entropy, name='xentropy_mean')\n    return loss"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"4c54d803-55de-dc07-6e5a-dc6cbcc306b8"},"outputs":[],"source":"def training(loss):\n    tf.summary.scalar(loss.op.name, loss)\n    optimizer = tf.train.AdamOptimizer(FLAGS.learning_rate)\n    global_step = tf.Variable(0, name='global_step', trainable=False)\n    train_op = optimizer.minimize(loss, global_step=global_step)\n\n    return train_op"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b937c289-069f-289b-6ab3-ca187803f7a8"},"outputs":[],"source":"def evaluation(logits, labels):\n    correct = tf.nn.in_top_k(logits, labels, 1)\n    return tf.reduce_sum(tf.cast(correct, tf.int32))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"49694027-f9e1-8439-e3bf-9d2679fb4b27"},"outputs":[],"source":"def preprocess_data(data):\n    _data = data.copy()\n\n    # delete id column\n    del _data['id']\n\n    # normalize float type columns \n    for column in _data:\n        if _data[column].dtypes == float:\n            _data[column] = z_score_normalization(_data[column])\n\n    # categorize 'species' column and insert 'species_cat' column\n    if 'species' in _data.columns:\n        _data.insert(2, 'species_cat', _data['species'].astype('category').cat.codes)\n        _data.drop('species', axis=1, inplace=True)\n    _data = _data.astype(float)\n\n    return _data, _data.columns.size-1"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"698ef491-6243-f241-dd42-0eb7590e61ce"},"outputs":[],"source":"def load_data_and_labels(file, is_labels_exist=True):\n    df = pd.read_csv(file)\n    processed_df, variables_size = preprocess_data(df)\n    print('Reading %s' % file)\n    print('N=%d' % len(df))\n\n    if is_labels_exist is True:\n        labels = list(processed_df['species_cat'])\n    else:\n        labels = None\n\n    if is_labels_exist is True:\n        del processed_df['species_cat']\n\n    # transform DataFrame into list \n    new_rows = []\n    for index, row in processed_df.iterrows():\n        _list_row = []\n        for col in processed_df:\n            _list_row.append(row[col])\n        new_rows.append(_list_row)\n\n    if is_labels_exist is True:\n        return new_rows, labels\n    else:\n        return new_rows"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"3f0644ea-c026-ad44-cfb9-91af3a9d9c0d"},"outputs":[],"source":"def z_score_normalization(series_of_values):\n    _series_of_values = (series_of_values - series_of_values.mean()) / series_of_values.std()\n    return _series_of_values"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"45c7a379-f75b-7734-1b6d-f4e3c5c398ee"},"outputs":[],"source":"def shuffle_data(data, labels):\n    # transform list into DataFrame \n    new_df = pd.DataFrame(data)\n    new_df['__labels__'] = labels\n    new_df = new_df.reindex(np.random.permutation(new_df.index))\n\n    new_labels = list(new_df['__labels__'])\n    del new_df['__labels__']\n\n    # transform DataFrame into list \n    new_row = []\n    for index, row in new_df.iterrows():\n        _list_row = []\n        for col in new_df:\n            _list_row.append(row[col])\n        new_row.append(_list_row)\n\n    return new_row, new_labels"},{"cell_type":"markdown","metadata":{"_cell_guid":"db148400-bfad-e54e-b0f8-475b12358128"},"source":"Training\n--------"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"0b9565ac-b380-dc48-9968-69b4c26751b4"},"outputs":[],"source":"def run_training(data, labels):\n    # Tell TensorFlow that the model will be built into the default Graph.\n    with tf.Graph().as_default():\n        data_size = FLAGS.num_variables\n        num_classes = FLAGS.num_classes\n\n        data_placeholder = tf.placeholder(\"float\", shape=(None, data_size))\n        labels_placeholder = tf.placeholder(\"int32\", shape=None)\n        keep_prob = tf.placeholder(\"float\")\n\n        # Build a Graph that computes predictions from the inference model.\n        logits = inference(data_placeholder, data_size, keep_prob)\n\n        # Add to the Graph the loss calculation.\n        loss_op = loss(logits, labels_placeholder)\n\n        # Add to the Graph operations that train the model.\n        train_op = training(loss_op)\n\n        # Add the Op to compare the logits to the labels during evaluation.\n        eval_correct = evaluation(logits, labels_placeholder)\n\n        # Build the summary Tensor based on the TF collection of Summaries.\n        summary = tf.summary.merge_all()\n\n        # The op for initializing the variables.\n        init_op = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())\n\n        # check point \n        saver = tf.train.Saver()\n\n        # Create a session for running operations in the Graph.\n        sess = tf.Session()\n\n        # Instantiate a SummaryWriter to output summaries and the Graph.\n        summary_writer = tf.summary.FileWriter('.', sess.graph)\n\n        # Initialize the variables (the trained variables and the epoch counter).\n        sess.run(init_op)\n\n        # training\n        for epoch in range(FLAGS.num_epochs):\n            for i in range(int(len(data)/FLAGS.batch_size)):\n                batch = FLAGS.batch_size*i\n                sess.run(train_op, feed_dict={\n                    data_placeholder: data[batch:batch + FLAGS.batch_size],\n                    labels_placeholder: labels[batch:batch + FLAGS.batch_size],\n                    keep_prob: FLAGS.keep_prob})\n\n            # calculate accuracy in every epoch\n            train_accuracy = sess.run(eval_correct, feed_dict={\n                data_placeholder: data,\n                labels_placeholder: labels,\n                keep_prob: FLAGS.keep_prob})\n            print(\"epoch %d, acc %g\" % (epoch, train_accuracy / len(labels)))\n\n            # update TensorBoard\n            summary_str = sess.run(summary, feed_dict={\n                data_placeholder: data,\n                labels_placeholder: labels,\n                keep_prob: 1.0})\n            summary_writer.add_summary(summary_str, epoch)\n\n            # shuffling data for next epoch\n            data, labels = shuffle_data(data, labels)\n\n        # Save a checkpoint and evaluate the model periodically.\n        # Create a saver for writing training checkpoints.\n        path = saver.save(sess, 'models.ckpt')\n        print('checkpoint is saved at ' + path)\n\n        sess.close()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e16f5863-a697-c07e-3c99-97e816245a2c"},"outputs":[],"source":"train_data, train_labels = load_data_and_labels('../input/train.csv', is_labels_exist=True)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"5414f13b-45dd-5f26-66e6-ea2ae9da3c1f"},"outputs":[],"source":"run_training(train_data, train_labels)"},{"cell_type":"markdown","metadata":{"_cell_guid":"40c6eb77-8dfb-98fd-8d55-3322084550ef"},"source":"Classifier\n----------"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"9c6fefc0-d601-c385-2e16-cf41e4ab9e64"},"outputs":[],"source":"def run_classifier(data):\n    with tf.Graph().as_default():\n        data_size = FLAGS.num_variables\n        num_classes = FLAGS.num_classes\n        data_placeholder = tf.placeholder(\"float\", shape=(None, data_size))\n\n        logits = inference(data_placeholder, data_size, 1.0)\n        init_op = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())\n\n        sess = tf.Session()\n        sess.run(init_op)\n\n        saver = tf.train.Saver()\n        saver.restore(sess, './models.ckpt')\n\n        prediction_list = []\n        total = len(data)\n        with sess.as_default():\n            for i in range(total):\n                # print('#%d : ' % i, end='')\n                v = logits.eval(feed_dict={data_placeholder: [data[i]]})\n                sm = tf.nn.softmax(v)\n                smv = sm.eval()\n                prediction_list.append(smv[0])\n\n                cls = np.argmax(smv)   # get the class number with largest value by argmax\n                # print('prediction=%d (%f)' % (cls, smv[0][cls]))\n\n        return prediction_list"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"16d756dc-1a3e-4301-0a2d-d12ca683b13e"},"outputs":[],"source":"def make_output(class_list):\n    result_df = pd.DataFrame(class_list)\n    train_df = (pd.read_csv('../input/train.csv'))\n    test_df = (pd.read_csv('../input/test.csv'))\n\n    cat_label_list = train_df['species'].astype('category').cat.categories\n    new_columns = ['id']\n    new_columns.extend(list(cat_label_list))\n\n    # insert id field on left most columns\n    result_df.insert(0, 'id', test_df['id'])\n\n    # set columns names\n    result_df.columns = new_columns\n\n    # write out csv\n    result_df.to_csv(path_or_buf='predicted.csv', index=False, encoding='utf-8')"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b221e28b-0e74-cd43-3725-d57ee16a45fa"},"outputs":[],"source":"test_data = load_data_and_labels('../input/test.csv', is_labels_exist=False)\nresult_list = run_classifier(test_data)\nmake_output(result_list)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"3b3e7322-8923-7799-fc0b-30062e9afccc"},"outputs":[],"source":"# predicted.csv is created in the current folder."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"430153ac-8212-f66e-54c0-8e29517c1cd9"},"outputs":[],"source":""}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"}},"nbformat":4,"nbformat_minor":0}