{"nbformat_minor":1,"metadata":{"language_info":{"version":"3.6.3","codemirror_mode":{"version":3,"name":"ipython"},"name":"python","nbconvert_exporter":"python","file_extension":".py","pygments_lexer":"ipython3","mimetype":"text/x-python"},"_change_revision":0,"_is_fork":false,"kernelspec":{"name":"python3","language":"python","display_name":"Python 3"}},"cells":[{"metadata":{"_uuid":"c06b3d508c59c352ef04cf22adba2f0eb2ceb623","_cell_guid":"1a0c141d-80de-fdc3-c437-20ed9e5768df"},"source":"# Visualizing K-Means with Leaf Dataset\n\nThis script is about perhaps the simplest and most popular **unsupervised learning algorithm** out there: the K-Means clustering algorithm.\n\nIn this script we will apply K-Means on a small dataset of 1600 binary leaf images with different shapes and try to get a feel for the distribution of leaf images using different visualizations that clarify different aspects about how one can interpret K-Means results.\n\nWe will then continue to see if the K-Means features (distances from cluster centers) are informative in terms of classifying leafs and determine what is the optimal K (number of clusters) for the sake of leaf type classification.\n\n**Note:** This script is a follow-up script to the [PCA script][1] which is very similar but about PCA.\n\n  [1]: https://www.kaggle.com/selfishgene/visualizing-pca-with-leaf-dataset","cell_type":"markdown"},{"metadata":{"_uuid":"594b2f2bc464474c2425896e5d3929fdeadf1285","collapsed":true,"_cell_guid":"37d454ac-ebbd-8294-d6a6-fedba7b170ac","_kg_hide-input":true},"source":"import pandas as pd\nimport numpy as np\nimport matplotlib\nimport matplotlib.image as mpimg\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\n\nfrom sklearn import model_selection\nfrom sklearn import decomposition\nfrom sklearn import linear_model\nfrom sklearn import cluster\nfrom sklearn import ensemble\nfrom sklearn import neighbors\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.neighbors import KernelDensity\nfrom sklearn.manifold import TSNE\nfrom sklearn.metrics import accuracy_score\n\nfrom skimage.transform import rescale\nfrom scipy import ndimage as ndi\n\nmatplotlib.style.use('fivethirtyeight')","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"4492f4b89343a88077ca59f77c6f9410e75097cc","_cell_guid":"7b0b700c-dc5b-88cd-e88c-2c456c91af49"},"source":"## Data loading and preparation phases\nFor the sake of the script not being too cluttered, I've hidden the code and hidden an intermediate pre-processing phase. Anyone who is interested is welcome to  unhide the code and uncomment to see what is going on.\n\nThe main assumption of this pre-processing stage is that the absolute sizes of the leafs matter, and not just their shape i.e. leafs with different sizes are most definitely different types of leafs. not sure if it's actually important, but just in case","cell_type":"markdown"},{"metadata":{"_uuid":"ad1499461a796c92e983a792ac1c57bd5461c60f","collapsed":true,"_cell_guid":"7ee1b974-5109-44a0-91c0-cafe5c84e263","_kg_hide-input":true},"source":"#%% load the data\ndataDir   = '../input/'\ntrainData = pd.read_csv(dataDir + 'train.csv')\nclassEncoder = LabelEncoder()\ntrainLabels  = classEncoder.fit_transform(trainData.loc[:,'species'])\ntrainIDs     = np.array(trainData.loc[:,'id'])\n\n# show some random images\nplt.figure(figsize=(14,12))\nplt.suptitle('Original Images (with variable image sizes)', fontsize=22)\nfor k in range(28):\n    randTrainInd = np.random.randint(len(trainIDs))\n    randomID = trainIDs[randTrainInd]\n    imageFilename = dataDir + 'images/' + str(randomID) + '.jpg'\n    plt.subplot(4,7,k+1); plt.imshow(mpimg.imread(imageFilename), cmap='gray')\n    plt.title(classEncoder.classes_[trainLabels[randTrainInd]], fontsize=10); plt.axis('off')","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_kg_hide-output":false,"collapsed":true,"_uuid":"50230b3a07ba5a304960fb7ce9a319c7c79f5cb0","_cell_guid":"e85a0f6c-e904-0f5b-2688-61b524769655","_kg_hide-input":true},"source":"#%% preprocess images\n\n# go over training images and store them in a list\nnumImages = 1584\n\nshapesMatrix = np.zeros((2,numImages))\nlistOfImages = []\nfor k in range(numImages):\n    imageFilename = dataDir + 'images/' + str(k+1) + '.jpg'\n    currImage = mpimg.imread(imageFilename)\n    shapesMatrix[:,k] = np.shape(currImage)\n    listOfImages.append(currImage)\n    \n# calculate the shape of an image that will contain all original images within it\nmaxShapeSize = shapesMatrix.max(axis=1)\nfor k in range(len(maxShapeSize)):\n    if maxShapeSize[k] % 2 == 0:\n        maxShapeSize[k] += 311\n    else:\n        maxShapeSize[k] += 310\n    \n    \n# place all original images at the center of the large reference frame\nfullImageMatrix3D = np.zeros(np.hstack((maxShapeSize, np.shape(shapesMatrix[1]))).astype(int),dtype=np.dtype('u1'))\ndestXc = (maxShapeSize[1]+1)/2; destYc = (maxShapeSize[0]+1)/2\nfor k, currImage in enumerate(listOfImages):\n    Yc, Xc = ndi.center_of_mass(currImage)\n    Xd = destXc - Xc; Yd = destYc - Yc\n    rowIndLims = (int(round(Yd)),int(round(Yd)+np.shape(currImage)[0]))\n    colIndLims = (int(round(Xd)),int(round(Xd)+np.shape(currImage)[1]))\n    fullImageMatrix3D[rowIndLims[0]:rowIndLims[1],colIndLims[0]:colIndLims[1],k] = currImage\n\n'''\n# make sure nothing was ruined in the process\nplt.figure(figsize=(14,7))\nplt.suptitle('Processed Images (fixed size)', fontsize=22)\nfor k in range(28):\n    randInd = np.random.randint(np.shape(fullImageMatrix3D)[2])\n    plt.subplot(4,7,k+1); plt.imshow(fullImageMatrix3D[:,:,randInd], cmap='gray'); plt.axis('off')\n'''\n\n# re crop according to rows and columns that don't have zeros in them in any image\nxValid = fullImageMatrix3D.mean(axis=2).sum(axis=0) > 0\nyValid = fullImageMatrix3D.mean(axis=2).sum(axis=1) > 0\nxLims = (np.nonzero(xValid)[0][0],np.nonzero(xValid)[0][-1])\nyLims = (np.nonzero(yValid)[0][0],np.nonzero(yValid)[0][-1])\nfullImageMatrix3D = fullImageMatrix3D[yLims[0]:yLims[1],xLims[0]:xLims[1],:]\n\n# make sure nothing was ruined in the process\nplt.figure(figsize=(14,7))\nplt.suptitle('Final Processed Images (with fixed image size)', fontsize=22)\nfor k in range(28):\n    randInd = np.random.randint(np.shape(fullImageMatrix3D)[2])\n    plt.subplot(4,7,k+1); plt.imshow(fullImageMatrix3D[:,:,randInd], cmap='gray'); plt.axis('off')\n    if randInd < len(trainLabels):\n        plt.title(classEncoder.classes_[trainLabels[randInd]], fontsize=10)\n    else:\n        plt.title('test data sample', fontsize=10)\n        \n# scale down all images to be in normal size\nrescaleFactor = 0.15\n\nscaledDownImage = rescale(fullImageMatrix3D[:,:,0],rescaleFactor)\nscaledDownImages = np.zeros(np.hstack((np.shape(scaledDownImage),\n                                       np.shape(fullImageMatrix3D)[2])),dtype=np.dtype('f4'))\nfor imInd in range(np.shape(fullImageMatrix3D)[2]):\n    scaledDownImages[:,:,imInd] = rescale(fullImageMatrix3D[:,:,imInd],rescaleFactor)\n    \ndel fullImageMatrix3D","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"68751dbb2f9d4811aacd61a587a362de820fe22c","_cell_guid":"08a67a0f-b2fb-768e-9a2a-5de694a4bc68"},"source":"## Define a Kmeans Model class that will help us visualize things\n\nThis is long, so I've hidden the code, but if you are intereseted in delving deeper and looking at the implementation then please unhide or better yet fork the script and try playing around by editing the code.","cell_type":"markdown"},{"metadata":{"_uuid":"e815ff5a2ef821d08c61136e6bda12ff731d167b","collapsed":true,"_cell_guid":"1e91683f-642e-ace4-e50f-0bcf2fb65f10","_kg_hide-input":true},"source":"class KmeansModel:\n\n    def __init__(self, X, numClusters=10, objectPixels=None):\n        '''\n        inputs: \n            X                       - numSamples x numDimentions matrix\n            numClusters             - number of clusters to use\n            objectPixels (optional) - an binnary mask image used for presentation\n                                      will be used as Im[objectPixels] = dataSample\n                                      must satisfy objectPixels.ravel().sum() = X.shape[1]\n        '''\n        numDataSamples = X.shape[0]\n        self.numClusters = numClusters        \n        if objectPixels is None:\n            self.objectPixels = np.ones((1,X.shape[1]),dtype=np.bool)\n        else:\n            self.objectPixels = objectPixels\n        assert(self.objectPixels.ravel().sum() == X.shape[1])\n\n        KmeansModel = cluster.KMeans(n_clusters=numClusters, n_init=5)\n        self.dataRepresentation = KmeansModel.fit_transform(X)\n        self.KmeansModel = KmeansModel\n        \n        # calculate cluster frequency\n        clusterInds = KmeansModel.labels_\n        clusterFrequency = []\n        for clusterInd in range(numClusters):\n            clusterFrequency.append((clusterInds == clusterInd).sum()/float(numDataSamples))\n        self.clusterFrequency = np.array(clusterFrequency)\n        self.sortedTemplatesByFrequency = np.flipud(np.argsort(clusterFrequency))\n\n    def RepresentUsingModel(self, X, representationMethod='distFromAllClusters'):\n        \n        if representationMethod == 'distFromAllClusters':\n            return self.KmeansModel.transform(X)\n        if representationMethod == 'clusterIndex':\n            return self.KmeansModel.predict(X)\n        if representationMethod == 'oneHotClusterIndex':\n            clusterAssignment = self.KmeansModel.predict(X)\n            X_transformed = np.zeros((X.shape[0],self.numClusters))\n            for sample in range(X.shape[0]):\n                X_transformed[sample,clusterAssignment[sample]] = 1\n            return X_transformed\n\n    def ReconstructUsingModel(self, X_transformed, representationMethod='distFromAllClusters'):\n\n        if representationMethod == 'clusterIndex':\n            clusterAssignment = X_transformed\n        if representationMethod == 'oneHotClusterIndex':\n            clusterAssignment = np.argmax(X_transformed,axis=1)\n        if representationMethod == 'distFromAllClusters':\n            clusterAssignment = np.argmin(X_transformed,axis=1)\n\n        X_reconstructed = np.zeros((X_transformed.shape[0],self.KmeansModel.cluster_centers_.shape[1]))\n        for sample in range(X_transformed.shape[0]):\n            X_reconstructed[sample,:] = self.KmeansModel.cluster_centers_[clusterAssignment[sample],:]\n                \n        return X_reconstructed\n        \n    def InterpretUsingModel(self, X, representationMethod='clusterIndex'):\n        return self.ReconstructUsingModel(\\\n                        self.RepresentUsingModel(X,representationMethod),representationMethod)\n\n    # shows the cluster centers\n    def ShowTemplates(self, numTemplatesToShow=16):\n        numTemplatesToShow = min(numTemplatesToShow, self.numClusters)\n        \n        numFigRows = np.ceil(np.sqrt(numTemplatesToShow)); \n        numFigCols = np.ceil(np.sqrt(numTemplatesToShow));\n        numTemplatesPerFigure = int(numFigRows*numFigCols)\n        numFigures = int(np.ceil(float(numTemplatesToShow)/numTemplatesPerFigure))\n                \n        for figureInd in range(numFigures):\n            plt.figure()\n            for plotInd in range(numTemplatesPerFigure):\n                templateInd = self.sortedTemplatesByFrequency[numTemplatesPerFigure*figureInd + plotInd]\n                if templateInd >= self.numClusters:\n                    break\n                templateImage = np.zeros(np.shape(self.objectPixels))\n                templateImage[self.objectPixels] = \\\n                        self.KmeansModel.cluster_centers_[templateInd,:].ravel()\n\n                plt.subplot(numFigRows,numFigCols,plotInd+1)\n                if np.shape(self.objectPixels)[0] == 1:\n                    plt.plot(templateImage)\n                else:\n                    plt.imshow(templateImage,cmap='hot'); plt.axis('off')\n                plt.title(str(100*self.clusterFrequency[templateInd])[:4] + '% frequency');\n            plt.tight_layout()\n            \n    # shows several random model reconstructions\n    def ShowReconstructions(self, X, numReconstructions=6):\n        assert(np.shape(X)[1] == self.objectPixels.ravel().sum())\n        numSamples = np.shape(X)[0]\n        numReconstructions = min(numReconstructions, numSamples)\n        \n        originalImage      = np.zeros(np.shape(self.objectPixels))\n        reconstructedImage = np.zeros(np.shape(self.objectPixels))\n        \n        numReconstructionsPerFigure = min(6, numReconstructions)\n        numFigures = int(np.ceil(float(numReconstructions)/numReconstructionsPerFigure))\n        \n        for figureInd in range(numFigures):\n            plt.figure()\n            for plotCol in range(numReconstructionsPerFigure):\n                dataSampleInd = np.random.randint(numSamples)\n                originalImage[self.objectPixels] = X[dataSampleInd,:].ravel()\n                reconstructedImage[self.objectPixels] = \\\n                        self.InterpretUsingModel(np.reshape(X[dataSampleInd,:],[1,-1])).ravel()\n                diffImage = abs(originalImage - reconstructedImage)\n                \n                # original image\n                plt.subplot(3,numReconstructionsPerFigure,0*numReconstructionsPerFigure+plotCol+1)\n                if np.shape(self.objectPixels)[0] == 1:\n                    plt.plot(originalImage); plt.title('original signal')\n                else:\n                    plt.imshow(originalImage, cmap='gray'); \n                    plt.title('original image'); plt.axis('off')\n                    \n                # reconstred image\n                plt.subplot(3,numReconstructionsPerFigure,1*numReconstructionsPerFigure+plotCol+1)\n                if np.shape(self.objectPixels)[0] == 1:\n                    plt.plot(reconstructedImage); plt.title('reconstructed signal')\n                else:\n                    plt.imshow(reconstructedImage, cmap='gray'); \n                    plt.title('reconstructed image'); plt.axis('off')\n\n                # diff image\n                plt.subplot(3,numReconstructionsPerFigure,2*numReconstructionsPerFigure+plotCol+1)\n                if np.shape(self.objectPixels)[0] == 1:\n                    plt.plot(diffImage); plt.title('abs difference signal')\n                else:\n                    plt.imshow(diffImage, cmap='gray'); \n                    plt.title('abs difference image'); plt.axis('off')\n            plt.tight_layout()\n\n\n    # shows distrbution along the distance from a particular cluster and several examples for that distance\n    def ShowSingleTemplateDistances(self, X, listOfTemplates=[0,1]):\n\n        showAsTraces = (np.shape(self.objectPixels)[0] == 1)\n        assert(all([(x in range(self.numClusters)) for x in listOfTemplates]))\n                \n        X_rep = self.RepresentUsingModel(X, representationMethod='distFromAllClusters')\n        \n        percentilesToShow = [1,5,10,30,60,99]\n        numReadDataSamplePerPercentile = 4\n        representationPercentiles = []\n        for percentile in percentilesToShow:\n            representationPercentiles.append(np.percentile(self.dataRepresentation, percentile, axis=0))\n        medianRepVec =  np.percentile(self.dataRepresentation, 50, axis=0)\n\n        for templateInd in listOfTemplates:\n            plt.figure(); gs = gridspec.GridSpec(numReadDataSamplePerPercentile+2,\n                                                 len(percentilesToShow))\n\n            # calculate the Gaussian smoothed distribution of values along the eignevector direction\n            sigmaOfKDE = (representationPercentiles[-1][templateInd] - \n                          representationPercentiles[1][templateInd])/100.0\n            pdfStart   = representationPercentiles[1][templateInd]  - 15*sigmaOfKDE\n            pdfStop    = representationPercentiles[-1][templateInd] + 15*sigmaOfKDE\n            xAxis = np.linspace(pdfStart,pdfStop,200)\n            PDF_Model = KernelDensity(kernel='gaussian', \\\n                            bandwidth=sigmaOfKDE).fit(self.dataRepresentation[:,templateInd].reshape(-1,1))\n            logPDF = PDF_Model.score_samples(xAxis.reshape(-1,1))\n            percentileValuesToShow = \\\n                [representationPercentiles[x][templateInd] for x in range(len(representationPercentiles))]\n            percentilesToShowLogPDF = \\\n                PDF_Model.score_samples(np.array(percentileValuesToShow).reshape(-1,1))\n\n            # show distribution of distance from current template and red dots at the list of precentiles to show \n            plt.subplot(gs[0,:])\n            plt.fill(xAxis, np.exp(logPDF), fc='b', alpha=0.9);\n            plt.scatter(percentileValuesToShow, np.exp(percentilesToShowLogPDF), c='r',s=40);\n            plt.title(str(100*self.clusterFrequency[templateInd])[:4] + '% assignment frequency');\n\n            for plotCol, currPrecentile in enumerate(percentilesToShow):                \n                currPrecentileRepVec              = medianRepVec.copy()\n                currPrecentileRepVec[templateInd] = representationPercentiles[plotCol][templateInd]\n                \n                currPrecentileImage = np.zeros(np.shape(self.objectPixels))\n                currPrecentileRepVec = currPrecentileRepVec[:,np.newaxis].T\n                currPrecentileImage[self.objectPixels] = \\\n                            self.ReconstructUsingModel(currPrecentileRepVec).ravel()\n                \n                # show the median image with current precentile as activation of the curr image\n                plt.subplot(gs[1,plotCol]);\n                if showAsTraces:\n                    plt.plot(currPrecentileImage); \n                    plt.title('precentile: ' + str(percentilesToShow[plotCol]) + '%')\n                else:\n                    plt.imshow(currPrecentileImage, cmap='hot'); \n                    plt.title('precentile: ' + str(percentilesToShow[plotCol]) + '%'); plt.axis('off')\n\n                # find the most suitible candidates in X for current precentile\n                distFromPercentile = abs(X_rep[:,templateInd] - \n                                         representationPercentiles[plotCol][templateInd])\n                X_inds = np.argpartition(distFromPercentile, \\\n                                         numReadDataSamplePerPercentile)[:numReadDataSamplePerPercentile]\n                for k, X_ind in enumerate(X_inds):\n                    currNearestPrecentileImage = np.zeros(np.shape(self.objectPixels))\n                    currNearestPrecentileImage[self.objectPixels]  = X[X_ind,:].ravel()\n                    \n                    plt.subplot(gs[2+k,plotCol]);\n                    if showAsTraces:\n                        plt.plot(currNearestPrecentileImage); \n                        plt.title('NN with closest percentile');\n                    else:\n                        plt.imshow(currNearestPrecentileImage, cmap='gray'); \n                        plt.title('NN with closest percentile'); plt.axis('off')\n            plt.tight_layout()\n            \n            \n    def ShowDataScatterPlotsWithTSNE(self, X=None, y=None, tSNE_perplexity=30.0, colorMap='Paired'):\n        # show the distance from 2 most frequent clusters and the tSNE of the entire \"distance form template\" space \n        \n        if X is None:\n            X_rep = self.dataRepresentation\n        else:\n            X_rep = self.RepresentUsingModel(X)\n            \n        if y is None:\n            y = np.ones(X_rep.shape[0])\n            \n        tSNE_KmeansModel = TSNE(n_components=2, perplexity=tSNE_perplexity, random_state=0)\n        X_rep_tSNE = tSNE_KmeansModel.fit_transform(X_rep)\n        \n        # take the two most frequent patterns\n        mostFrequent = self.sortedTemplatesByFrequency[:2]\n        \n        plt.figure()\n        plt.subplot(1,2,1); \n        plt.scatter(X_rep[:,mostFrequent[0]], \\\n                    X_rep[:,mostFrequent[1]],c=y,cmap=colorMap,s=10,alpha=0.9)\n        plt.title('\"distance form template\" representation'); \n        plt.xlabel('distance from template 1'); plt.ylabel('distance from template 2')\n        plt.subplot(1,2,2); \n        plt.scatter(X_rep_tSNE[:,0],X_rep_tSNE[:,1],c=y,cmap=colorMap,s=15,alpha=0.9)\n        plt.title('t-SNE of Kmeans representation'); plt.xlabel('t-SNE axis1'); plt.ylabel('t-SNE axis2')\n\n\n    def ShowTemplatesInPCASpace(self, X, y=None, tSNE_perplexity=30.0, colorMap='Paired'):\n        # show the templates in the 2PC space and the tSNE of the entire PCA space\n        \n        # build PCA model and project the data onto the PCA space\n        PCAModel = decomposition.PCA(n_components=60, whiten=False)\n        X_rep = PCAModel.fit_transform(X)\n                \n        # project the Kmeans templates onto the PCA space\n        templates_rep = PCAModel.transform(templateModel.KmeansModel.cluster_centers_)\n        \n        if y is None:\n            y = self.RepresentUsingModel(X, representationMethod='clusterIndex')\n            \n        tSNE_PCAModel = TSNE(n_components=2, perplexity=tSNE_perplexity, random_state=0)\n        X_rep_tSNE = tSNE_PCAModel.fit_transform(np.vstack((X_rep,templates_rep))) \n        \n        plt.figure()\n        plt.subplot(1,2,1); plt.scatter(X_rep[:,0],X_rep[:,1],c=y,cmap=colorMap,s=15,alpha=0.9)\n        plt.scatter(templates_rep[:,0],templates_rep[:,1],c='k',cmap=colorMap,s=50)\n        plt.title('PCA representation'); plt.xlabel('PC1 coeff'); plt.ylabel('PC2 coeff')\n        \n        nC = templates_rep.shape[0]        \n        plt.subplot(1,2,2); \n        plt.scatter(X_rep_tSNE[:-nC,0],\\\n                    X_rep_tSNE[:-nC,1],c=y,cmap=colorMap,s=15,alpha=0.9)\n        plt.scatter(X_rep_tSNE[-nC:,0],\\\n                    X_rep_tSNE[-nC:,1],c='k',cmap=colorMap,s=50)\n        plt.title('t-SNE of PCA representation'); plt.xlabel('t-SNE axis1'); plt.ylabel('t-SNE axis2')","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"7709cad9894affbc873355e4ab963339237de6a1","_cell_guid":"b9914010-eedf-a549-716b-4342684340eb"},"source":"## Now lets apply k-means and look at the cluster centers\n\nWe'll think of each image as a point in a high dimensional space, and each cluster center is a different point in the high dimensional image space.\n\n\n----------\n\nFor K = 4:\n--------------","cell_type":"markdown"},{"metadata":{"_uuid":"af9ac26333ee9db9b5864d65df7e35b3df13cbf7","collapsed":true,"_cell_guid":"ae403587-3293-8946-ef94-10d6c6da2018","_kg_hide-input":true},"source":"matplotlib.rcParams['font.size'] = 12\nmatplotlib.rcParams['figure.figsize'] = (12,9)\n\nobjectPixels = np.ones((np.shape(scaledDownImages)[0],np.shape(scaledDownImages)[1])) == 1\nsampleDim = np.shape(scaledDownImages)[0]*np.shape(scaledDownImages)[1]\nX = scaledDownImages.reshape(sampleDim,-1).T\n\ntemplateModel = KmeansModel(X, numClusters=4, objectPixels=objectPixels)\ntemplateModel.ShowTemplates(numTemplatesToShow=4)","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"fde9a98852183d7d546ccaaf415a18b8770a6b3f","_cell_guid":"0bcea19a-cdef-ab21-5bbf-deed8aeb4645"},"source":"We can see that the centers are basically just large or small leaves, elongated either vertically and horizontally\n\n----------\n\nNow let's try doing it for **K = 9**:\n-------------------------------------","cell_type":"markdown"},{"metadata":{"_uuid":"f300de7babfa9a24d0ea2a1577d914aee24b0ec7","collapsed":true,"_cell_guid":"f6839364-5611-24dd-d79b-8bfe70376809","_kg_hide-input":true},"source":"matplotlib.rcParams['font.size'] = 12\nmatplotlib.rcParams['figure.figsize'] = (12,9)\n\ntemplateModel = KmeansModel(X, numClusters=9, objectPixels=objectPixels)\ntemplateModel.ShowTemplates(numTemplatesToShow=9)","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"4d87b3f8f774f15767ae90ab3768c46e0bb843fc","_cell_guid":"01c91b3d-8387-cfcb-1437-fd5a0aabbe7a"},"source":"Oh, now we start seeing some more specificity in the cluster centers\n\n\n----------\n\nLet's try also for **K=16**:\n----------------------------","cell_type":"markdown"},{"metadata":{"_uuid":"8c06e0bef59c88b2b3588534c3fb58bf38e441da","collapsed":true,"_cell_guid":"faeb9ae4-9c49-bf98-e50f-fd48479a5fa9","_kg_hide-input":true},"source":"matplotlib.rcParams['font.size'] = 10\nmatplotlib.rcParams['figure.figsize'] = (11,9)\n\ntemplateModel = KmeansModel(X, numClusters=16, objectPixels=objectPixels)\ntemplateModel.ShowTemplates(numTemplatesToShow=16)","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"6f8a21f8b95daaf25935a5d24748d1c11654092d","_cell_guid":"c884bcac-ac39-e373-8f3a-508ae000cf1c"},"source":"OK, we can clearly see templates here that are very much leaf type specific\n\n----------\n\nFinally, let's try for **K=36**:\n--------------------------------","cell_type":"markdown"},{"metadata":{"_uuid":"abd1d7c735f864c70a84f59f310f35cd45933a68","collapsed":true,"_cell_guid":"73cfd6d9-5ae6-47f9-67ed-22f2b79359f1","_kg_hide-input":true},"source":"matplotlib.rcParams['font.size'] = 9\nmatplotlib.rcParams['figure.figsize'] = (13,10)\n\ntemplateModel = KmeansModel(X, numClusters=36, objectPixels=objectPixels)\ntemplateModel.ShowTemplates(numTemplatesToShow=36)","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"1667476eb942a5ad8b30580f5602746f4aa37902","_cell_guid":"9b47602c-a956-c5a4-f45c-8471e666e21b"},"source":"Now we can see even more clearly templates that are leaf type specific. this gives us hope regarding the possibility of using these features later for classification purposes.\n\n\n----------\nlet's see how good of an approximation can these 36 templates be for several specific leaf images. \nMeaning, we'll show a leaf image, it's closest template amount the 36 templates, and the difference between these images. \n\n## Model Reconstructions: ","cell_type":"markdown"},{"metadata":{"_uuid":"ba09967cd543e9d2b812213b8a3628983c7c37e0","collapsed":true,"_cell_guid":"7eded711-6a92-369c-ca36-cc917bdd415c","_kg_hide-input":true},"source":"matplotlib.rcParams['font.size'] = 8\nmatplotlib.rcParams['figure.figsize'] = (12,5)\n\ntemplateModel.ShowReconstructions(X, numReconstructions=6)","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"5b0659212d99ee94f07cf8d6cbac3e4fec00c9d3","_cell_guid":"1f6f6b60-6f5d-7078-3656-1de85bf4728a"},"source":"We can see that the reconstructions are OK but far from perfect\n\n----------\n\n## Now let's visualize how these cluster center look like in the original high dimensional space\n\nFor this purpose we first apply PCA to reduce dimensionality of the images and then show the data once in the space of the first two principal components and in the full PCA space as visualized by t-SNE.\n\nThe cluster centers are in black, and the data points are colored according to cluster assignment","cell_type":"markdown"},{"metadata":{"_uuid":"e26b4ae8bac8c2dd4d723e3853040fcad924bd4d","collapsed":true,"_cell_guid":"216500a8-ae94-fe97-495d-feccba5edf60","_kg_hide-input":true},"source":"matplotlib.rcParams['font.size'] = 12\nmatplotlib.rcParams['figure.figsize'] = (12,8)\n\ntemplateModel.ShowTemplatesInPCASpace(X, y=None, tSNE_perplexity=15.0, colorMap='Paired')","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"7ea007deff1d57d3f9ab45623d97076f4dba9136","_cell_guid":"8a2e4c64-4758-3eb5-36d9-51646a747bf8"},"source":"## Show the distribution of distances of data samples from the most frequent template\nThis will help us get a feel for what what different distances represent by plotting several examples that are distant from that template by approximately that amount","cell_type":"markdown"},{"metadata":{"_uuid":"4dac8d002adedfd77e5ccb2af44e1beeaaea938e","collapsed":true,"_cell_guid":"8bb26185-d19b-815b-bd5a-84d6d89aa86d","_kg_hide-input":true},"source":"matplotlib.rcParams['font.size'] = 7\nmatplotlib.rcParams['figure.figsize'] = (13,9)\n\nmostFrequentClusterInd = templateModel.sortedTemplatesByFrequency[0]\ntemplateModel.ShowSingleTemplateDistances(X, listOfTemplates=[mostFrequentClusterInd])","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"3f733377c77c50e7295886bce18ff9697ad52042","_cell_guid":"b67bbf4c-7c12-4274-759d-20d5214831be"},"source":"From here we can see that all similar patterns (more to the left) are similar in the same way, and that all dissimilar patterns (more to the right) are different in their own unique way.\nWell, even though it's a very good quote, it isn't actually what we see in this dataset.\n \nWhat we do see is something a little bit surprising, if we think about it from the geometric point of view. points (leaf images) that are far away from a specific point (a template) can theoretically be far away in many different directions and therefore one would expect large diversity among all images that are at the same distance from a specific template. This is only a little bit the case. We can also see some similarities between equally distant points relative to a template. What this means is that this feature \"distance from template i\" can be informative beyond just a binary type \"like template i\" vs \"not like template i\" feature.\n\n----------\n## Let's look at another such feature (distance from 10th most frequent template):","cell_type":"markdown"},{"metadata":{"_uuid":"a8cf9e0f2e3843bfc4e537579c7ac1bbbc928a2b","collapsed":true,"_cell_guid":"daff8f7c-7916-e307-cdec-554409b44c80","_kg_hide-input":true},"source":"matplotlib.rcParams['font.size'] = 7\nmatplotlib.rcParams['figure.figsize'] = (13,9)\n\nmediumFrequencyClusterInd = templateModel.sortedTemplatesByFrequency[10]\ntemplateModel.ShowSingleTemplateDistances(X, listOfTemplates=[mediumFrequencyClusterInd])","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"324aa07dd486f8044ee7d0e33d6bdcaf50998d7f","_cell_guid":"c494efc5-35cd-63a2-6c81-f3136da6246a"},"source":"Again, we see similar things happen in this feature as well.\n\n----------\n## Visualize \"distance from cluster centers\" feature space\nPlot the scatter of distance from the two most frequent clusters, and the low dimensional t-SNE representation of the entire \"distance from clusters\" space. the colors indicate the leaf type.","cell_type":"markdown"},{"metadata":{"_uuid":"7a8c895ff50214c094a69e9dbf0ec20c0b85d573","collapsed":true,"_cell_guid":"4f37216c-7144-649d-c390-998839abf073","_kg_hide-input":true},"source":"matplotlib.rcParams['font.size'] = 12\nmatplotlib.rcParams['figure.figsize'] = (12,8)\n\nX_train = X[trainIDs-1,:]\ny_train = trainLabels\n\ntemplateModel.ShowDataScatterPlotsWithTSNE(X=X_train, y=y_train, tSNE_perplexity=15.0, colorMap='Paired')","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"56bec4362d1539524eee18179b7889c1c5f00b70","_cell_guid":"be9b84b5-a148-b01a-44d9-a7944c7443ed"},"source":"Interesting! \n\n\n----------\n## Show Model Accuracy as function of number of clusters used\nNow, similar to what we did for PCA, let's try to see what is the classification accuracy using k-means features for several different values of K and several differnt classifiers","cell_type":"markdown"},{"metadata":{"_uuid":"bcb7de181ec716b192182a09ef6be303e5a692f6","collapsed":true,"_cell_guid":"eb1c7261-db8b-30dc-d289-4707e0b4e252","_kg_hide-input":true},"source":"matplotlib.rcParams['font.size'] = 12\nmatplotlib.rcParams['figure.figsize'] = (12,8)\n\nnumClustersToUse = [1,2,4,8,16,32,64]\n\nlogReg = linear_model.LogisticRegression(C=10.0)\nkNN = neighbors.KNeighborsClassifier(n_neighbors=7)\nRF = ensemble.RandomForestClassifier(n_estimators=100)\n\nlogRegMeanAccuracy = []; kNN_MeanAccuracy = []; RF_MeanAccuracy = []\nlogRegAccuracyStd  = []; kNN_AccuracyStd  = []; RF_AccuracyStd  = []\n\nfor k in numClustersToUse:\n    stratifiedCV = model_selection.StratifiedKFold(n_splits=5, random_state=1)\n    logRegAccuracy = []; kNN_Accuracy = []; RF_Accuracy = []\n    \n    templateModel = KmeansModel(X_train, numClusters=k)\n    X_kmeans_train = templateModel.RepresentUsingModel(X_train, representationMethod='distFromAllClusters')\n    \n    for trainInds, validInds in stratifiedCV.split(X_kmeans_train, y_train):\n        X_train_cv = X_kmeans_train[trainInds,:]\n        X_valid_cv = X_kmeans_train[validInds,:]\n\n        y_train_cv = y_train[trainInds]\n        y_valid_cv = y_train[validInds]\n\n        logReg.fit(X_train_cv, y_train_cv)\n        kNN.fit(X_train_cv, y_train_cv)\n        RF.fit(X_train_cv, y_train_cv)\n    \n        logRegAccuracy.append(accuracy_score(y_valid_cv, logReg.predict(X_valid_cv)))\n        kNN_Accuracy.append(accuracy_score(y_valid_cv, kNN.predict(X_valid_cv)))\n        RF_Accuracy.append(accuracy_score(y_valid_cv, RF.predict(X_valid_cv)))\n\n    logRegMeanAccuracy.append(np.array(logRegAccuracy).mean())\n    logRegAccuracyStd.append(np.array(logRegAccuracy).std())\n\n    kNN_MeanAccuracy.append(np.array(kNN_Accuracy).mean())\n    kNN_AccuracyStd.append(np.array(kNN_Accuracy).std())\n\n    RF_MeanAccuracy.append(np.array(RF_Accuracy).mean()) \n    RF_AccuracyStd.append(np.array(RF_Accuracy).std())\n        \nplt.figure()\nplt.errorbar(x=numClustersToUse, y=logRegMeanAccuracy, yerr=logRegAccuracyStd)\nplt.errorbar(x=numClustersToUse, y=kNN_MeanAccuracy  , yerr=kNN_AccuracyStd)\nplt.errorbar(x=numClustersToUse, y=RF_MeanAccuracy   , yerr=RF_AccuracyStd)\nplt.xlim(min(numClustersToUse)-1,max(numClustersToUse)+1); plt.legend(['Logistic Regression','k Nearest Neighbor','Random Forest'],loc=2)\nplt.xlabel('num Clusters'); plt.ylabel('validation accuracy'); plt.title('accuracy as function of num Clusters')","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"ad3df978613b22b0892cbccecb9a96eba8e0b7ac","_cell_guid":"ef2ae7a0-e37c-a5b9-3469-51024d2cd8d9"},"source":"## Summery \n\nIf we compare what we see here with what we saw in the [PCA case][1], we see two main points:\n\n 1. PCA and K-Means image features are similarly useful in terms of classification.\n 2. The order between Logistic Regression and Random Forest has switched here compared to PCA case.\n\nEven though these finding cannot be generalized because they heavily depend of this particular data distribution, we can speculate that there might be something complementary that Random Forest adds to the PCA feature representation, and that k-means features add to the classification abilities of the Logistic Regression classifier.\n\nAnyway, I hope this script has shed some light about what k-means is about for some of you.\n\n  [1]: https://www.kaggle.com/selfishgene/visualizing-pca-with-leaf-dataset","cell_type":"markdown"}],"nbformat":4}