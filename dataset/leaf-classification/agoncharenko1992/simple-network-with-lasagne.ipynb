{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"4784ef21-6cc1-9ddb-b566-ee4bb882a8b4"},"source":"Simple network"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"efb9a051-24d0-e355-f466-15764c7391f9"},"outputs":[],"source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.cross_validation import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nimport theano\nimport theano.tensor as T\nimport lasagne\nimport time\nimport pickle\nfrom lasagne.layers import DenseLayer, NonlinearityLayer, DropoutLayer\nfrom lasagne.nonlinearities import softmax\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"3cbdd97c-313b-9b86-f59c-321e03612e2d"},"outputs":[],"source":"def train_data_load():\n    Xy_train_df = pd.read_csv(\"../input/train.csv\")\n    Xy_train_df['species'] = Xy_train_df.species.astype('category')\n    cat_names = list(Xy_train_df.species.cat.categories)\n    cat_code = range(len(Xy_train_df.species.cat.categories))\n    species_dict = dict(zip(cat_code, cat_names))\n    Xy_train_df['species'] = Xy_train_df.species.cat.codes\n    y_train_df = Xy_train_df.species\n    Xy_train_df.drop(labels='species', axis=1, inplace=True)\n    X_train_df = Xy_train_df\n    del Xy_train_df\n    X = X_train_df.drop(labels='id', axis=1, inplace=False).values\n    y = y_train_df.values\n    scaler = StandardScaler()\n    X = scaler.fit_transform(X, y)\n    X, X_test, y, y_test = train_test_split(X, y, test_size=0.1)\n    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.1)\n    return X_train, y_train, X_val, y_val, X_test, y_test, scaler, species_dict \ndef gen_minibatches(X, y, batch_size, shuffle=False):\n    ex_count = X.shape[0]\n    assert ex_count==y.shape[0], \"Training data sizes don't match\"\n    if shuffle:\n        ids = np.random.permutation(ex_count)\n    else:\n        ids = np.arange(ex_count)\n    for start_idx in range(0, ex_count - batch_size + 1, batch_size):\n        ii = ids[start_idx:start_idx + batch_size]\n        yield X[ii], y[ii]"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f3465c1f-aeb6-bfc5-6e81-caf0037059f6"},"outputs":[],"source":"input_var = T.matrix('inputs', dtype=theano.config.floatX)\ndef model_construction(input_var, num_units_list, features_count, classes_count):\n    network = lasagne.layers.InputLayer(shape=(None, features_count), input_var=input_var)\n    for idx, curr_layer_num_units in enumerate(num_units_list):\n        network = DenseLayer(network, num_units=curr_layer_num_units, \n                   W=lasagne.init.HeNormal(gain='relu'), b=lasagne.init.Constant(0.), \n                   nonlinearity=lasagne.nonlinearities.rectify, name='layer' + str(idx))\n        network = DropoutLayer(network, p=0.6, name='dropout' + str(idx))\n    network = DenseLayer(network, num_units=classes_count, nonlinearity=None, name='last_layer')\n    network = NonlinearityLayer(network, softmax, name='probs')    \n    return network    "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"1a0b2e65-3bfb-573d-b57c-4cca18457579"},"outputs":[],"source":"save_path = 'super_model'\ndef train(X_train, y_train, X_val, y_val, X_test, y_test, network, num_epochs=500, \n          learning_rate=0.001, learning_rate_decay=0.95, \n          momentum=0.9, momentum_decay=0.95, \n          decay_after_epochs=10, regu=0.002, batch_size=64, updates='adam'):\n    \n    target_var = T.ivector('target')\n    prediction = lasagne.layers.get_output(network)\n    loss = lasagne.objectives.categorical_crossentropy(prediction, target_var)\n    cross_entr_loss = loss.mean()\n    regu_loss = regu * lasagne.regularization.regularize_network_params(\n        network, lasagne.regularization.l2)\n    loss = cross_entr_loss + regu_loss\n    print(updates)\n    print(\"initial learning_rate=%f, \\\ndecay_value %f per %d epoch, L2_reg coeff value=%f, batch_size=%d\" \n          % (learning_rate, learning_rate_decay, decay_after_epochs, regu, batch_size))\n    train_acc = T.mean(T.eq(T.argmax(prediction, axis=1), target_var), dtype=theano.config.floatX)\n    learning_rate_var = theano.shared(np.float32(learning_rate))\n    momentum_var = theano.shared(np.float32(momentum))\n    params = lasagne.layers.get_all_params(network, trainable=True)\n    if updates=='nesterov':\n        updates = lasagne.updates.nesterov_momentum(loss, params, learning_rate=learning_rate_var,\n                                                    momentum=momentum_var)\n    else:\n        updates = lasagne.updates.adam(loss, params)\n    test_prediction = lasagne.layers.get_output(network, deterministic=True)\n    test_loss = lasagne.objectives.categorical_crossentropy(test_prediction,\n                                                            target_var)\n    test_loss = test_loss.mean()\n    test_acc = T.mean(T.eq(T.argmax(test_prediction, axis=1), target_var),\n                      dtype=theano.config.floatX)\n    train_fn = theano.function([input_var, target_var], [cross_entr_loss, regu_loss], updates=updates)\n    train_acc_fn = theano.function([input_var, target_var], train_acc)\n    val_fn = theano.function([input_var, target_var], [test_loss, test_acc])\n    print(\"Training...\")\n    best_val_acc = 0.0\n    best_model = None\n\n    loss_history = []\n    train_acc_history = []\n    val_acc_history = []\n    test_acc_history = []\n\n    for epoch in range(num_epochs):\n        train_err = train_batches = cross_loss = weights_sums = 0\n        start_time = time.time()\n        for X_batch, y_batch in gen_minibatches(X_train, y_train, batch_size, shuffle=True):\n            cross_err, weights = train_fn(X_batch, y_batch)\n            train_err += (cross_err + weights)\n            cross_loss += cross_err\n            weights_sums += weights\n            train_batches += 1\n            loss_history.append(cross_err + weights)\n        # training accuracy\n        n_acc = len(y_val)\n        trval_err = trval_acc = trval_batches = 0\n        for X_batch, y_batch in gen_minibatches(X_train[:n_acc], y_train[:n_acc], \n                                                batch_size, shuffle=False):\n            err, acc = val_fn(X_batch, y_batch)\n            trval_err += err\n            trval_acc += acc\n            trval_batches += 1\n        trval_acc /= trval_batches\n        train_acc_history.append(trval_acc)\n        # validation accuracy\n        val_err = val_acc = val_batches = 0\n        for X_batch, y_batch in gen_minibatches(X_val, y_val, batch_size//2, shuffle=False):\n            err, acc = val_fn(X_batch, y_batch)\n            val_err += err\n            val_acc += acc\n            val_batches += 1\n        val_acc /= val_batches\n        val_acc_history.append(val_acc)\n\n        test_err = test_acc = test_batches = 0\n        for X_batch, y_batch in gen_minibatches(X_test, y_test, batch_size//2, shuffle=False):\n            err, acc = val_fn(X_batch, y_batch)\n            test_err += err\n            test_acc += acc\n            test_batches += 1\n        test_acc /= test_batches\n        test_acc_history.append(test_acc)\n        \n        # keep track of the best model based on validation accuracy\n        if val_acc > best_val_acc:\n            # make a copy of the model\n            best_val_acc = val_acc\n            best_model = lasagne.layers.get_all_param_values(network)\n        if epoch % 50 == 0:\n            print('epoch %d / %d in %.1fs: loss %f, cross_loss %f, weights_loss %f, train: %.3f, val %.3f, test %.3f, lr %e mom %e'\n                  % (epoch + 1, num_epochs, time.time() - start_time,\n                     train_err / train_batches, cross_loss / train_batches,\n                     weights_sums / train_batches, trval_acc, val_acc, test_acc, \n                     learning_rate_var.get_value(), momentum_var.get_value()))\n        # decay learning rate\n        if (epoch + 1) % decay_after_epochs == 0:\n            learning_rate_var.set_value(\n                np.float32(learning_rate_var.get_value() * learning_rate_decay))\n            momentum = (1.0 - (1.0 - momentum_var.get_value()) * momentum_decay) \\\n                       .clip(max=0.9999)\n            momentum_var.set_value(np.float32(momentum))\n        # save model snapshots\n        if save_path and (epoch + 1) % 10 == 0:\n            model = lasagne.layers.get_all_param_values(network)\n            path = '%s.pickle' % (save_path)\n            with open(path, 'wb') as f:\n                pickle.dump({'model': model}, f, -1)\n    return network          "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"cf51a774-a8ee-f886-d5f1-a7718d436950"},"outputs":[],"source":"def predict_proba(network, Xtest_vals, save_path):\n    if save_path is not None:\n        path = '%s.pickle' % (save_path)\n        with open(path, 'rb') as f:\n            data_new = pickle.load(f)\n            print(len(data_new['model']))\n            lasagne.layers.set_all_param_values(network, data_new['model'])\n    proba_tensor = lasagne.layers.get_output(network, Xtest_vals, deterministic=True)\n    proba_vals = proba_tensor.eval()\n    return proba_vals\n\ndef main():\n    X_train, y_train, X_val, y_val, X_test, y_test, scaler, species_dict = train_data_load()\n    network = model_construction(input_var=input_var, num_units_list=[128, 64], \n                                 features_count=X_train.shape[1], classes_count=99)\n    network = train(X_train, y_train, X_val, y_val, X_test, y_test, network, num_epochs=700, regu=0.002,\n                   batch_size=32)\n    X_test = pd.read_csv(\"../input/test.csv\")\n    Xtest_vals = X_test.drop(labels='id', axis=1, inplace=False).values\n    Xtest_vals = scaler.transform(Xtest_vals)\n    print(Xtest_vals.shape)\n    probs = predict_proba(network, Xtest_vals, save_path)\n    print(probs)\n    cols = list(species_dict.values())\n    predicted = pd.DataFrame(probs, columns=cols)\n    predicted = pd.concat([X_test.id, predicted], axis=1)\n    predicted.to_csv(\"sample_submission.csv\", index=False)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b96455b4-ec00-c67a-0e4f-5f4e8e2ec7f9"},"outputs":[],"source":"main()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"188a65a3-86c7-75e9-85a0-982c4984f877"},"outputs":[],"source":""}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"}},"nbformat":4,"nbformat_minor":0}