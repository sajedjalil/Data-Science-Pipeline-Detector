{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"![](https://storage.googleapis.com/kaggle-media/competitions/google-image-matching/trevi-canvas-licensed-nonoderivs.jpg)","metadata":{}},{"cell_type":"markdown","source":"# ***Install Libs***","metadata":{}},{"cell_type":"markdown","source":"**Kornia, DKM Install**","metadata":{}},{"cell_type":"code","source":"!pip install ../input/kornia-loftr/kornia-0.6.4-py2.py3-none-any.whl\n!pip install ../input/kornia-loftr/kornia_moons-0.1.9-py3-none-any.whl\n\n!mkdir -p pretrained/checkpoints\n!cp /kaggle/input/imc2022-dependencies/pretrained/dkm.pth pretrained/checkpoints/dkm_base_v11.pth\n!pip install -f /kaggle/input/imc2022-dependencies/wheels --no-index einops\n!cp -r /kaggle/input/imc2022-dependencies/DKM/ /kaggle/working/DKM/\n!cd /kaggle/working/DKM/; pip install -f /kaggle/input/imc2022-dependencies/wheels -e . ","metadata":{"execution":{"iopub.status.busy":"2022-06-04T08:34:34.820749Z","iopub.execute_input":"2022-06-04T08:34:34.821136Z","iopub.status.idle":"2022-06-04T08:36:19.951588Z","shell.execute_reply.started":"2022-06-04T08:34:34.821047Z","shell.execute_reply":"2022-06-04T08:36:19.950602Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**VSAC Install**","metadata":{}},{"cell_type":"code","source":"!cp -r ../input/pvsac-dependencies/* /kaggle/working\n!cp -r /kaggle/input/vsac-src/* /kaggle/working \n!ls /kaggle/working/vsac/build\n\nfile_path = '/kaggle/input/pvsac-dependencies/vsac_pkgs/order'\nf = open(file_path, 'r')\nlines = f.readlines()\nfor line in lines:\n    !dpkg -i /kaggle/input/pvsac-dependencies/vsac_pkgs/deps/{line}\n    \n%cd /kaggle/working/vsac\n!python3 setup.py install\n\n%cd /kaggle/working/vsac/python","metadata":{"execution":{"iopub.status.busy":"2022-06-04T08:36:19.95408Z","iopub.execute_input":"2022-06-04T08:36:19.954387Z","iopub.status.idle":"2022-06-04T08:42:57.070761Z","shell.execute_reply.started":"2022-06-04T08:36:19.954344Z","shell.execute_reply":"2022-06-04T08:42:57.069796Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ***Main Code***","metadata":{}},{"cell_type":"code","source":"%%writefile test_pvsac.py\n\n#!/usr/bin/env python\n# coding: utf-8\nimport sys, os\nimport numpy as np\nimport cv2\nimport csv\nfrom glob import glob\nimport torch\nimport matplotlib.pyplot as plt\nimport kornia\nfrom kornia_moons.feature import *\nimport kornia as K\nimport kornia.feature as KF\nimport gc\nimport math\nimport time\nfrom sklearn.cluster import KMeans\nfrom PIL import Image\nfrom scipy.spatial import cKDTree\nimport pvsac\n\nsys.path.append('/kaggle/input/imc2022-dependencies/DKM/')\n\ndry_run = False\n\n################### Model ##########################################################\n\ndevice = torch.device('cuda')\nmatcher = KF.LoFTR(pretrained=None)\nmatcher.load_state_dict(torch.load(\"/kaggle/input/kornia-loftr/loftr_outdoor.ckpt\")['state_dict'])\nmatcher = matcher.to(device).eval()\n\ntorch.hub.set_dir('/kaggle/working/pretrained/')\nfrom dkm import dkm_base\nmodel = dkm_base(pretrained=True, version=\"v11\")\n\nsrc = '/kaggle/input/image-matching-challenge-2022/'\ntest_samples = []\n\nwith open(f'{src}/test.csv') as f:\n    reader = csv.reader(f, delimiter=',')\n    for i, row in enumerate(reader):\n        # Skip header.\n        if i == 0:\n            continue\n        test_samples += [row]\n        \n#####################################################################################\ndef FlattenMatrix(M, num_digits=8):\n    '''Convenience function to write CSV files.'''\n\n    return ' '.join([f'{v:.{num_digits}e}' for v in M.flatten()])\n\ndef get_images(image, center_point, crop_size=512):\n    h, w, c = image.shape\n    cw = int(w/2)\n    ch = int(h/2)\n    cx = int(center_point[0])\n    cy = int(center_point[1])\n\n    half_crop = crop_size//2\n\n    if ch > cy:\n        ylb = max(0, cy-half_crop)\n        yub = min(h, max(0, cy-half_crop)+crop_size)\n    else:\n        ylb = max(0, min(h, cy+half_crop)-crop_size)\n        yub = min(h, cy+half_crop)\n    if cw > cx:\n        xlb = max(0, cx-half_crop)\n        xub = min(w, max(0, cx-half_crop)+crop_size)\n    else:\n        xlb = max(0, min(w, cx+half_crop)-crop_size)\n        xub = min(w, cx+half_crop)\n    d_img = image[ylb:yub, xlb:xub]\n\n    return d_img, [w, h, cx, cy, xlb, xub, ylb, yub]\n\ndef get_matches(image_1, image_2):\n    input_dict = {\"image0\": K.color.rgb_to_grayscale(image_1),\n                  \"image1\": K.color.rgb_to_grayscale(image_2)}\n    with torch.no_grad():\n        correspondences = matcher(input_dict)\n\n    mkpts0 = correspondences['keypoints0'].cpu().numpy()\n    mkpts1 = correspondences['keypoints1'].cpu().numpy()\n    conf = correspondences['confidence'].cpu().numpy()\n    return mkpts0, mkpts1, conf\n\ndef concatenate_matches(mkpts0, mkpts1, score, d_score, d_mkpts0, d_mkpts1, info_1, info_2):\n    s_mkpts0 = (d_mkpts0+ [info_1[4], info_1[6]]).astype(float)\n    s_mkpts1 = (d_mkpts1+ [info_2[4], info_2[6]]).astype(float)\n\n    f_mkpts0 = np.concatenate((mkpts0, s_mkpts0))\n    f_mkpts1 = np.concatenate((mkpts1, s_mkpts1))\n    f_score = np.concatenate((score, d_score))\n\n    return f_mkpts0, f_mkpts1, f_score\n\ndef set_torch_image(img, device, infer_size=840):\n    scale = infer_size / max(img.shape[0], img.shape[1])\n    w = int(img.shape[1] * scale)\n    h = int(img.shape[0] * scale)\n    scale_w = w/img.shape[1]\n    scale_h = h/img.shape[0]\n    img = cv2.resize(img, (w, h))\n    img = K.image_to_tensor(img, False).float() /255.\n    img = K.color.bgr_to_rgb(img)\n    return img.to(device), (scale_h, scale_w)\n\n\ndef GetLargestNumbers(arr):\n    unique_nums = set(arr)\n    sorted_nums = sorted(unique_nums, reverse=True)\n    largestIndex = np.where(arr == sorted_nums[0])[0][0]\n    secondIndex = np.where(arr == sorted_nums[1])[0][0]\n    return largestIndex, secondIndex\n\ndef GetMostDenseCenterPoint(n_clusters, mkpts0, mkpts1):\n    kmeans = KMeans(n_clusters=n_clusters, random_state=42).fit(mkpts0)\n    most_dense_center_points = []\n    second_dense_center_points = []\n    labels = kmeans.labels_\n\n    unique, counts = np.unique(labels, return_counts=True)\n    largest_count, second_count = GetLargestNumbers(counts)\n    max_counts_label = unique[largest_count]\n    second_counts_label = unique[second_count]\n\n    dense_mkpts0 = []\n    dense_mkpts1 = []\n    indices = np.where(labels==max_counts_label)\n    for matchIndex in indices:\n        dense_mkpts0.append(mkpts0[matchIndex])\n        dense_mkpts1.append(mkpts1[matchIndex])\n    most_dense_center_points.append(np.array([np.mean(dense_mkpts0[0][:,0]), np.mean(dense_mkpts0[0][:,1])]))\n    most_dense_center_points.append(np.array([np.mean(dense_mkpts1[0][:,0]), np.mean(dense_mkpts1[0][:,1])]))\n\n\n    std1 = np.std(np.linalg.norm(dense_mkpts0[0] - np.expand_dims(most_dense_center_points[0], axis=0), axis=1))\n    std2 = np.std(np.linalg.norm(dense_mkpts1[0] - np.expand_dims(most_dense_center_points[1], axis=0), axis=1))\n\n    second_mkpts0 = []\n    second_mkpts1 = []\n    indices2 = np.where(labels==second_counts_label)\n    for matchIndex2 in indices2:\n        second_mkpts0.append(mkpts0[matchIndex2])\n        second_mkpts1.append(mkpts1[matchIndex2])\n    second_dense_center_points.append(np.array([np.mean(second_mkpts0[0][:,0]), np.mean(second_mkpts0[0][:,1])]))\n    second_dense_center_points.append(np.array([np.mean(second_mkpts1[0][:,0]), np.mean(second_mkpts1[0][:,1])]))\n\n    return most_dense_center_points, second_dense_center_points, (std1, std2)\n\n\ndef find_Homography(mkpts0, mkpts1, img1_o):\n    Hmatrix, mask = cv2.findHomography(mkpts0, mkpts1, cv2.USAC_MAGSAC, 0.5, 0.99999, 10000)\n    h, w, c = img1_o.shape\n    if not homography_is_accepted(Hmatrix):\n        return None, None\n    img1_w = cv2.warpPerspective(img1_o, Hmatrix, (w, h))\n    return Hmatrix, img1_w\n\ndef GetLoFTRMatches(img1_o, img2_o, device, infer_sizes):\n    img1_tensor, (scale_h1, scale_w1) = set_torch_image(img1_o, device, infer_sizes[0])\n    img2_tensor, (scale_h2, scale_w2) = set_torch_image(img2_o, device, infer_sizes[1])\n    mkpts0, mkpts1, conf = get_matches(img1_tensor, img2_tensor)\n    mkpts0[:,0] /= scale_w1\n    mkpts0[:,1] /= scale_h1\n    mkpts1[:,0] /= scale_w2\n    mkpts1[:,1] /= scale_h2\n\n    return mkpts0, mkpts1, conf\n\ndef GetDkmMatches(crop_img1, crop_img2, infer_sizes=(512,512), num_mkpts=500):\n    h1,w1, _ = crop_img1.shape\n    scale1 = infer_sizes[0]/max(h1,w1)\n    new_h1, new_w1 = int(h1*scale1), int(w1*scale1)\n    crop_img1 = cv2.resize(crop_img1, (new_w1, new_h1))\n    scale_h1, scale_w1 = new_h1 / h1, new_w1 / w1\n\n    h2,w2, _ = crop_img2.shape\n    scale2 = infer_sizes[1]/max(h2,w2)\n    new_h2, new_w2 = int(h2*scale2), int(w2*scale2)\n    crop_img2 = cv2.resize(crop_img2, (new_w2, new_h2))\n    scale_h2, scale_w2 = new_h2 / h2, new_w2 / w2\n\n\n    crop_img1_PIL = Image.fromarray(cv2.cvtColor(crop_img1, cv2.COLOR_BGR2RGB))\n    crop_img2_PIL = Image.fromarray(cv2.cvtColor(crop_img2, cv2.COLOR_BGR2RGB))\n    dense_matches, dense_certainty = model.match(crop_img1_PIL, crop_img2_PIL)\n    sparse_matches, sparse_certainty = model.sample(dense_matches, dense_certainty, num_mkpts)\n    dkm_crop_mkpts0 = sparse_matches[:, :2]\n    dkm_crop_mkpts1 = sparse_matches[:, 2:]\n\n    h, w, c = crop_img1.shape\n    dkm_crop_mkpts0[:, 0] = (dkm_crop_mkpts0[:,0]+1)/2 * w / scale_w1\n    dkm_crop_mkpts0[:, 1] = (dkm_crop_mkpts0[:,1]+1)/2 * h / scale_h1\n\n    h, w, c = crop_img2.shape\n    dkm_crop_mkpts1[:, 0] = (dkm_crop_mkpts1[:,0]+1)/2 * w / scale_w2\n    dkm_crop_mkpts1[:, 1] = (dkm_crop_mkpts1[:,1]+1)/2 * h / scale_h2\n\n    return dkm_crop_mkpts0, dkm_crop_mkpts1, sparse_certainty\n\ndef GetWarpedLoFTRMatches(mkpts0, mkpts1, score, img1_w, img2_o, corrected_Hmatrix, device, infer_sizes=(840,840)):\n    if corrected_Hmatrix is not None:\n        corrected_Hmatrix_inv = np.linalg.inv(corrected_Hmatrix)\n        mkpts0_w, mkpts1_w, score_w = GetLoFTRMatches(img1_w, img2_o, device, infer_sizes)\n\n        if mkpts0_w.shape[0] >= 8 :\n            mkpts0_w = np.expand_dims(mkpts0_w, axis=1)\n            mkpts0_w = cv2.perspectiveTransform(mkpts0_w, corrected_Hmatrix_inv).squeeze()\n            mkpts0 = np.concatenate([mkpts0, mkpts0_w])\n            mkpts1 = np.concatenate([mkpts1, mkpts1_w])\n            score = np.concatenate([score, score_w])\n            return mkpts0, mkpts1, score\n\n    return mkpts0, mkpts1, score\n\ndef radius_NMS(kps, kps1, score, r=0.3):\n    if len(kps)==0:\n        return kps, kps1\n\n    order = np.argsort(score)[::-1]\n    kps = np.array(kps)[order]\n    kps1 = np.array(kps1)[order]\n    data_pts = kps\n\n    kd_tree = cKDTree(data_pts)\n    N = len(kps)\n    idxs_removed = set()\n\n    kd_idxs = kd_tree.query_ball_point(data_pts,r)\n\n    for i in range(N):\n        if i in idxs_removed:\n            continue\n        for j in kd_idxs[i]:\n            if j>i:\n                idxs_removed.add(j)\n    idxs_remaining = [i for i in range(N) if i not in idxs_removed]\n\n    kps_out = kps[idxs_remaining]\n    kps1_out = kps1[idxs_remaining]\n    return kps_out, kps1_out\n\ndef homography_is_accepted(H):\n    H /= H[2, 2]\n    det = H[0, 0] * H[1, 1] - H[0, 1] * H[1, 0]\n    if det < 0:\n        return False\n    N1 = math.sqrt(H[0, 0]**2 + H[1, 0]**2)\n    N2 = math.sqrt(H[0, 1]**2 + H[1, 1]**2)\n\n    if N1 > 100 or N1 < 0.001:\n        return False\n    if N2 > 100 or N2 < 0.001:\n        return False\n    return True\n\ndef cut_mkpts(f_mkpts0, f_mkpts1, f_score, img1_o, img2_o):\n    maskx_0 = (f_mkpts0[:,0]>=0) & (f_mkpts0[:,0]<=img1_o.shape[1]-1)\n    f_mkpts0, f_mkpts1, f_score = f_mkpts0[maskx_0,:], f_mkpts1[maskx_0,:], f_score[maskx_0]\n    masky_0 = (f_mkpts0[:,1]>=0) & (f_mkpts0[:,1]<=img1_o.shape[0]-1)\n    f_mkpts0, f_mkpts1, f_score = f_mkpts0[masky_0,:], f_mkpts1[masky_0,:], f_score[masky_0]\n\n    maskx_1 = (f_mkpts1[:,0]>=0) & (f_mkpts1[:,0]<=img2_o.shape[1]-1)\n    f_mkpts0, f_mkpts1, f_score = f_mkpts0[maskx_1,:], f_mkpts1[maskx_1,:], f_score[maskx_1]\n    masky_1 = (f_mkpts1[:,1]>=0) & (f_mkpts1[:,1]<=img2_o.shape[0]-1)\n    f_mkpts0, f_mkpts1, f_score = f_mkpts0[masky_1,:],f_mkpts1[masky_1,:], f_score[masky_1]\n    return f_mkpts0, f_mkpts1, f_score\n\n####################\n\nF_dict = {}\n\nfor i, row in enumerate(test_samples):\n    np.random.seed(42)\n    sample_id, batch_id, img1_id, img2_id = row\n\n    # Load the images.\n    st = time.time()\n    img1_o = cv2.imread(f'{src}/test_images/{batch_id}/{img1_id}.png')\n    img2_o = cv2.imread(f'{src}/test_images/{batch_id}/{img2_id}.png')\n    h1, w1, c1 = img1_o.shape\n    h2, w2, c2 = img2_o.shape\n\n    default_infer_sizes = (1080,1080)\n    crop_sizes = (512,512)\n    \n    ##################### GET MATCH POINTS FROM WHOLE IAMGES ############################\n    mkpts0, mkpts1, score = GetLoFTRMatches(img1_o, img2_o, device, infer_sizes=default_infer_sizes)\n    \n    ##################### GET CROP POINTS FROM MATCH POINTS  ############################\n    n_clusters = 4\n    first_center_points, second_center_points, (std1, std2) = GetMostDenseCenterPoint(n_clusters, mkpts0, mkpts1)\n    \n    ##################### GET MATCH POINTS FROM WARPING IMAGE ###########################\n    if mkpts0.shape[0]>5:\n        corrected_Hmatrix1, img1_w = find_Homography(mkpts0, mkpts1, img1_o)\n        corrected_Hmatrix2, img2_w = find_Homography(mkpts1, mkpts0, img2_o)\n        mkpts0, mkpts1, score = GetWarpedLoFTRMatches(mkpts0, mkpts1, score, img1_w, img2_o, corrected_Hmatrix1, device, infer_sizes=(1080,1080))\n        mkpts1, mkpts0, score = GetWarpedLoFTRMatches(mkpts1, mkpts0, score, img2_w, img1_o, corrected_Hmatrix2, device, infer_sizes=(1080,1080))\n    \n    ##################### GET MATCH POINTS FROM CROP IMAGE ##############################\n    if  max(h1,w1) >= 512:\n        crop_img1, info_1 = get_images(img1_o, first_center_points[0], crop_size=crop_sizes[0]) #[w, h, cx, cy, xlb, xub, ylb, yub]\n        crop_img2, info_2 = get_images(img2_o, first_center_points[1], crop_size=crop_sizes[1])\n\n        # Get LoFTR matching points\n        loftr_crop_mkpts0, loftr_crop_mkpts1, loftr_score = GetLoFTRMatches(crop_img1, crop_img2, device, infer_sizes=(1080, 1080))\n        \n        if loftr_crop_mkpts0.shape[0]<500 and loftr_crop_mkpts0.shape[0]>5:\n            corrected_Hmatrix_crop1, crop_img1_w = find_Homography(loftr_crop_mkpts0, loftr_crop_mkpts1, crop_img1)\n            corrected_Hmatrix_crop2, crop_img2_w = find_Homography(loftr_crop_mkpts1, loftr_crop_mkpts0, crop_img2)\n            loftr_crop_mkpts0, loftr_crop_mkpts1, loftr_score = GetWarpedLoFTRMatches(loftr_crop_mkpts0, loftr_crop_mkpts1, loftr_score,\n                                                                crop_img1_w, crop_img2,\n                                                                corrected_Hmatrix_crop1,\n                                                                device, infer_sizes=(840, 840))\n            loftr_crop_mkpts1, loftr_crop_mkpts0, loftr_score = GetWarpedLoFTRMatches(loftr_crop_mkpts1, loftr_crop_mkpts0, loftr_score,\n                                                                crop_img2_w, crop_img1,\n                                                                corrected_Hmatrix_crop2,\n                                                                device, infer_sizes=(840, 840))\n\n        # Get DKM matching points\n        dkm_crop_mkpts0, dkm_crop_mkpts1, dkm_score = GetDkmMatches(crop_img1, crop_img2, infer_sizes=(1080, 1080), num_mkpts=300)\n\n        # concatenate matches\n        mkpts0, mkpts1, score = concatenate_matches(mkpts0, mkpts1, score, loftr_score, loftr_crop_mkpts0, loftr_crop_mkpts1, info_1, info_2)\n        f_mkpts0, f_mkpts1, f_score = concatenate_matches(mkpts0, mkpts1, score, dkm_score, dkm_crop_mkpts0, dkm_crop_mkpts1, info_1, info_2)\n\n    else:\n        f_mkpts0, f_mkpts1 = mkpts0, mkpts1\n    \n    ##################### FILTERING MATCH POINTS#### ####################################\n    f_mkpts0, f_mkpts1, f_score = cut_mkpts(f_mkpts0, f_mkpts1, f_score, img1_o, img2_o)\n    f_mkpts0, f_mkpts1 = radius_NMS(f_mkpts0, f_mkpts1, f_score, r=0.5)\n    if len(f_mkpts0) > 7:\n        params = pvsac.Params(pvsac.EstimationMethod.Fundamental, 0.1, 0.99999, 100000, pvsac.SamplingMethod.SAMPLING_PROGRESSIVE_NAPSAC, pvsac.ScoreMethod.SCORE_METHOD_MAGSAC)\n        params.setParallel(True)\n        params.setLocalOptimization(pvsac.LocalOptimMethod.LOCAL_OPTIM_INNER_LO)\n        params.setPolisher(pvsac.PolishingMethod.MAGSAC)\n        params.setLOSampleSize(10*params.getSampleSize())\n        params.setLOIterations(20)\n\n        F, inliers = pvsac.estimate(params, f_mkpts0, f_mkpts1)\n\n        inliers = inliers > 0\n        assert F.shape == (3, 3), 'Malformed F?'\n        F_dict[sample_id] = F\n    else:\n        F_dict[sample_id] = np.zeros((3, 3))\n        continue\n\n    gc.collect()\n    nd = time.time()\n\n    ##################### SAVE DATA FOR SAMPLE DATA VISUALIZATION #######################\n    if (i < 3):\n        print(\"Running time: \", nd - st, \" s\")\n        np.save(f'/kaggle/working/f_mkpts0_{i}', f_mkpts0)\n        np.save(f'/kaggle/working/f_mkpts1_{i}', f_mkpts1)\n        np.save(f'/kaggle/working/img1_o_{i}', img1_o)\n        np.save(f'/kaggle/working/img2_o_{i}', img2_o)\n        np.save(f'/kaggle/working/inliers_{i}',inliers)\n\nwith open('/kaggle/working/submission.csv', 'w') as f:\n    f.write('sample_id,fundamental_matrix\\n')\n    for sample_id, F in F_dict.items():\n        f.write(f'{sample_id},{FlattenMatrix(F)}\\n')\n\n","metadata":{"execution":{"iopub.status.busy":"2022-06-04T09:19:49.111155Z","iopub.execute_input":"2022-06-04T09:19:49.111496Z","iopub.status.idle":"2022-06-04T09:19:49.130139Z","shell.execute_reply.started":"2022-06-04T09:19:49.111463Z","shell.execute_reply":"2022-06-04T09:19:49.129082Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!python test_pvsac.py","metadata":{"execution":{"iopub.status.busy":"2022-06-04T09:19:50.272573Z","iopub.execute_input":"2022-06-04T09:19:50.2734Z","iopub.status.idle":"2022-06-04T09:20:14.664489Z","shell.execute_reply.started":"2022-06-04T09:19:50.273346Z","shell.execute_reply":"2022-06-04T09:20:14.663514Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Check Test Sample","metadata":{}},{"cell_type":"code","source":"%cd /kaggle/working","metadata":{"execution":{"iopub.status.busy":"2022-06-04T09:20:14.666694Z","iopub.execute_input":"2022-06-04T09:20:14.667013Z","iopub.status.idle":"2022-06-04T09:20:14.674363Z","shell.execute_reply.started":"2022-06-04T09:20:14.666972Z","shell.execute_reply":"2022-06-04T09:20:14.673498Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import sys, os\nimport numpy as np\nimport cv2\nimport csv\nfrom glob import glob\nimport torch\nimport matplotlib.pyplot as plt\nimport kornia\nfrom kornia_moons.feature import *\nimport kornia as K\nimport kornia.feature as KF\nimport gc\nimport math\nimport time\nfrom sklearn.cluster import KMeans \nfrom PIL import Image\nfrom scipy.spatial import cKDTree\n\nsys.path.append('/kaggle/input/imc2022-dependencies/DKM/')\n\ndry_run = False","metadata":{"execution":{"iopub.status.busy":"2022-06-04T09:20:14.675979Z","iopub.execute_input":"2022-06-04T09:20:14.676387Z","iopub.status.idle":"2022-06-04T09:20:14.686761Z","shell.execute_reply.started":"2022-06-04T09:20:14.67626Z","shell.execute_reply":"2022-06-04T09:20:14.685739Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f_mkpts0s = []\nf_mkpts1s = []\nimg1_os = []\nimg2_os = []\ninlierss= []\n\nfor i in range(3):\n    with open(f'f_mkpts0_{i}.npy', 'rb') as f:\n        f_mkpts0s.append(np.load(f))\n    with open(f'f_mkpts1_{i}.npy', 'rb') as f:\n        f_mkpts1s.append(np.load(f))\n    with open(f'img1_o_{i}.npy', 'rb') as f:\n        img1_os.append(np.load(f)) \n    with open(f'img2_o_{i}.npy', 'rb') as f:\n        img2_os.append(np.load(f))\n    with open(f'inliers_{i}.npy', 'rb') as f:\n        inlierss.append(np.load(f))    ","metadata":{"execution":{"iopub.status.busy":"2022-06-04T09:20:14.690558Z","iopub.execute_input":"2022-06-04T09:20:14.691519Z","iopub.status.idle":"2022-06-04T09:20:14.713307Z","shell.execute_reply.started":"2022-06-04T09:20:14.691311Z","shell.execute_reply":"2022-06-04T09:20:14.7125Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in range(3):\n    f_mkpts0 = f_mkpts0s[i]\n    f_mkpts1 = f_mkpts1s[i]\n    img1_o = img1_os[i]\n    img2_o = img2_os[i]\n    inliers = inlierss[i]\n    draw_LAF_matches(\n            KF.laf_from_center_scale_ori(torch.from_numpy(f_mkpts0).view(1,-1, 2),\n                                        torch.ones(f_mkpts0.shape[0]).view(1,-1, 1, 1),\n                                        torch.ones(f_mkpts0.shape[0]).view(1,-1, 1)),\n\n            KF.laf_from_center_scale_ori(torch.from_numpy(f_mkpts1).view(1,-1, 2),\n                                        torch.ones(f_mkpts1.shape[0]).view(1,-1, 1, 1),\n                                        torch.ones(f_mkpts1.shape[0]).view(1,-1, 1)),\n            torch.arange(f_mkpts0.shape[0]).view(-1,1).repeat(1,2),\n            cv2.cvtColor(img1_o, cv2.COLOR_BGR2RGB),\n            cv2.cvtColor(img2_o, cv2.COLOR_BGR2RGB),\n            inliers,\n            draw_dict={'inlier_color': (0.2, 1, 0.2),\n                       'tentative_color': None, \n                       'feature_color': (0.2, 0.5, 1), 'vertical': False})","metadata":{"execution":{"iopub.status.busy":"2022-06-04T09:20:14.71494Z","iopub.execute_input":"2022-06-04T09:20:14.71551Z","iopub.status.idle":"2022-06-04T09:21:01.438071Z","shell.execute_reply.started":"2022-06-04T09:20:14.715468Z","shell.execute_reply":"2022-06-04T09:21:01.43733Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<center>\n    <h2 style=\"color: #022047\"> Thanks for reading ðŸ¤—  </h2>\n</center>","metadata":{}}]}