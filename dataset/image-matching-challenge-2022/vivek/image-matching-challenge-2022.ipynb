{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"#### Reference:\n    https://github.com/zju3dv/LoFTR","metadata":{}},{"cell_type":"code","source":"from IPython.display import clear_output\n!pip install kornia kornia_moons einops loguru pytorch_lightning yacs\n!git clone https://github.com/zju3dv/LoFTR.git\nimport sys\nsys.path.append('./LoFTR')\nclear_output()","metadata":{"id":"jM4DIsu4dlEr","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import sys\nsys.path.append('./LoFTR')\nfrom collections import defaultdict\nfrom loguru import logger\nfrom pathlib import Path\nimport cv2\nimport torch\nimport numpy as np\nimport pytorch_lightning as pl\nfrom matplotlib import pyplot as plt\nimport gc\nfrom src.loftr import LoFTR\nfrom src.loftr.utils.supervision import compute_supervision_coarse, compute_supervision_fine\nfrom src.losses.loftr_loss import LoFTRLoss\nfrom src.optimizers import build_optimizer, build_scheduler\nfrom src.utils.metrics import (\n    compute_symmetrical_epipolar_errors,\n    compute_pose_errors,\n    aggregate_metrics\n)\nfrom src.utils.plotting import make_matching_figures\nfrom src.utils.comm import gather, all_gather\nfrom src.utils.misc import lower_config, flattenList\nfrom src.utils.profiler import PassThroughProfiler\n\nimport os.path as osp\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset\n\nfrom src.utils.dataset import read_megadepth_gray, pad_bottom_right\n\nimport os\nfrom collections import abc\nfrom torch.utils.data.dataset import Dataset\nfrom tqdm import tqdm\nfrom joblib import Parallel, delayed\n\nimport pytorch_lightning as pl\nfrom torch import distributed as dist\nfrom torch.utils.data import (\n    Dataset,\n    DataLoader,\n    ConcatDataset,\n    DistributedSampler,\n    RandomSampler,\n    dataloader\n)\n\nfrom src.utils.augment import build_augmentor\nfrom src.utils.dataloader import get_local_split\nfrom src.utils.misc import tqdm_joblib\nfrom src.utils import comm\nfrom src.datasets.sampler import RandomConcatSampler\n\nimport math\nimport argparse\nimport pprint\nfrom distutils.util import strtobool\nfrom pathlib import Path\nfrom loguru import logger as loguru_logger\n\nimport pytorch_lightning as pl\nfrom pytorch_lightning.utilities import rank_zero_only\nfrom pytorch_lightning.loggers import TensorBoardLogger\nfrom pytorch_lightning.callbacks import ModelCheckpoint, LearningRateMonitor\nfrom pytorch_lightning.plugins import DDPPlugin\n\nfrom src.config.default import get_cfg_defaults\nfrom src.utils.misc import get_rank_zero_only_logger, setup_gpus\nfrom src.utils.profiler import build_profiler\nimport pandas as pd\nloguru_logger = get_rank_zero_only_logger(loguru_logger)\nfrom glob import glob\ndevice = torch.device('cuda')","metadata":{"id":"5Tv0UcNv32Uc","execution":{"iopub.status.busy":"2022-06-21T14:58:21.816563Z","iopub.execute_input":"2022-06-21T14:58:21.817346Z","iopub.status.idle":"2022-06-21T14:58:25.914052Z","shell.execute_reply.started":"2022-06-21T14:58:21.817238Z","shell.execute_reply":"2022-06-21T14:58:25.913183Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class PL_LoFTR(pl.LightningModule):\n    def __init__(self, config, pretrained_ckpt=None, profiler=None, dump_dir=None):\n        super().__init__()\n        # Misc\n        self.config = config  \n        _config = lower_config(self.config)\n        self.loftr_cfg = lower_config(_config['loftr'])\n        self.profiler = profiler or PassThroughProfiler()\n        self.n_vals_plot = 1 \n\n        # Matcher: LoFTR\n        self.matcher = LoFTR(config=_config['loftr'])\n        self.loss = LoFTRLoss(_config)\n\n        # Pretrained weights\n        if pretrained_ckpt:\n            state_dict = torch.load(pretrained_ckpt, map_location=device)['state_dict']\n            self.matcher.load_state_dict(state_dict, strict=True)\n            logger.info(f\"Load \\'{pretrained_ckpt}\\' as pretrained checkpoint\")\n        \n        # Testing\n        self.dump_dir = dump_dir\n        \n    def configure_optimizers(self):\n        optimizer = build_optimizer(self, self.config)\n        scheduler = build_scheduler(self.config, optimizer)\n        return [optimizer], [scheduler]\n    \n    def optimizer_step(\n            self, epoch, batch_idx, optimizer, optimizer_idx,\n            optimizer_closure, on_tpu, using_native_amp, using_lbfgs):\n        # learning rate warm up\n        warmup_step = self.config.TRAINER.WARMUP_STEP\n        if self.trainer.global_step < warmup_step:\n            if self.config.TRAINER.WARMUP_TYPE == 'linear':\n                base_lr = self.config.TRAINER.WARMUP_RATIO * self.config.TRAINER.TRUE_LR\n                lr = base_lr + \\\n                    (self.trainer.global_step / self.config.TRAINER.WARMUP_STEP) * \\\n                    abs(self.config.TRAINER.TRUE_LR - base_lr)\n                for pg in optimizer.param_groups:\n                    pg['lr'] = lr\n            elif self.config.TRAINER.WARMUP_TYPE == 'constant':\n                pass\n            else:\n                raise ValueError(f'Unknown lr warm-up strategy: {self.config.TRAINER.WARMUP_TYPE}')\n\n        # update params\n        optimizer.step(closure=optimizer_closure)\n        optimizer.zero_grad()\n    \n    def _trainval_inference(self, batch):\n        with self.profiler.profile(\"Compute coarse supervision\"):\n            compute_supervision_coarse(batch, self.config)\n        \n        with self.profiler.profile(\"LoFTR\"):\n            self.matcher(batch)\n        \n        with self.profiler.profile(\"Compute fine supervision\"):\n            compute_supervision_fine(batch, self.config)\n            \n        with self.profiler.profile(\"Compute losses\"):\n            self.loss(batch)\n    \n    def _compute_metrics(self, batch):\n        with self.profiler.profile(\"Copmute metrics\"):\n            compute_symmetrical_epipolar_errors(batch)  # compute epi_errs for each match\n            compute_pose_errors(batch, self.config)  # compute R_errs, t_errs, pose_errs for each pair\n\n            rel_pair_names = list(zip(*batch['pair_names']))\n            bs = batch['image0'].size(0)\n            metrics = {\n                # to filter duplicate pairs caused by DistributedSampler\n                'identifiers': ['#'.join(rel_pair_names[b]) for b in range(bs)],\n                'epi_errs': [batch['epi_errs'][batch['m_bids'] == b].cpu().numpy() for b in range(bs)],\n                'R_errs': batch['R_errs'],\n                't_errs': batch['t_errs'],\n                'inliers': batch['inliers']}\n            ret_dict = {'metrics': metrics}\n        return ret_dict, rel_pair_names\n    \n    def training_step(self, batch, batch_idx):\n        self._trainval_inference(batch)\n        \n        # logging\n        if self.trainer.global_rank == 0 and self.global_step % self.trainer.log_every_n_steps == 0:\n            # scalars\n            for k, v in batch['loss_scalars'].items():\n                self.logger.experiment.add_scalar(f'train/{k}', v, self.global_step)\n\n            # net-params\n            if self.config.LOFTR.MATCH_COARSE.MATCH_TYPE == 'sinkhorn':\n                self.logger.experiment.add_scalar(\n                    f'skh_bin_score', self.matcher.coarse_matching.bin_score.clone().detach().cpu().data, self.global_step)\n\n            # figures\n            if self.config.TRAINER.ENABLE_PLOTTING:\n                compute_symmetrical_epipolar_errors(batch)  # compute epi_errs for each match\n                figures = make_matching_figures(batch, self.config, self.config.TRAINER.PLOT_MODE)\n                for k, v in figures.items():\n                    self.logger.experiment.add_figure(f'train_match/{k}', v, self.global_step)\n        gc.collect()\n        torch.cuda.empty_cache()\n        return {'loss': batch['loss']}\n\n    def training_epoch_end(self, outputs):\n        avg_loss = torch.stack([x['loss'] for x in outputs]).mean()\n        if self.trainer.global_rank == 0:\n            self.logger.experiment.add_scalar(\n                'train/avg_loss_on_epoch', avg_loss,\n                global_step=self.current_epoch)\n        gc.collect()\n        torch.cuda.empty_cache()\n    \n    def validation_step(self, batch, batch_idx):\n        self._trainval_inference(batch)\n        \n        ret_dict, _ = self._compute_metrics(batch)\n        \n        val_plot_interval = max(self.trainer.num_val_batches[0] // self.n_vals_plot, 1)\n        figures = {self.config.TRAINER.PLOT_MODE: []}\n        if batch_idx % val_plot_interval == 0:\n            figures = make_matching_figures(batch, self.config, mode=self.config.TRAINER.PLOT_MODE)\n        gc.collect()\n        torch.cuda.empty_cache()\n        return {\n            **ret_dict,\n            'loss_scalars': batch['loss_scalars'],\n            'figures': figures,\n        }\n        \n    def validation_epoch_end(self, outputs):\n        # handle multiple validation sets\n        multi_outputs = [outputs] if not isinstance(outputs[0], (list, tuple)) else outputs\n        multi_val_metrics = defaultdict(list)\n        \n        for valset_idx, outputs in enumerate(multi_outputs):\n            # since pl performs sanity_check at the very begining of the training\n            cur_epoch = self.trainer.current_epoch\n            if not self.trainer.resume_from_checkpoint :\n                cur_epoch = -1\n\n            # 1. loss_scalars: dict of list, on cpu\n            _loss_scalars = [o['loss_scalars'] for o in outputs]\n            loss_scalars = {k: flattenList(all_gather([_ls[k] for _ls in _loss_scalars])) for k in _loss_scalars[0]}\n\n            # 2. val metrics: dict of list, numpy\n            _metrics = [o['metrics'] for o in outputs]\n            metrics = {k: flattenList(all_gather(flattenList([_me[k] for _me in _metrics]))) for k in _metrics[0]}\n            # NOTE: all ranks need to `aggregate_merics`, but only log at rank-0 \n            val_metrics_4tb = aggregate_metrics(metrics, self.config.TRAINER.EPI_ERR_THR)\n            for thr in [5, 10, 20]:\n                multi_val_metrics[f'auc@{thr}'].append(val_metrics_4tb[f'auc@{thr}'])\n            \n            # 3. figures\n            _figures = [o['figures'] for o in outputs]\n            figures = {k: flattenList(gather(flattenList([_me[k] for _me in _figures]))) for k in _figures[0]}\n\n            # tensorboard records only on rank 0\n            if self.trainer.global_rank == 0:\n                for k, v in loss_scalars.items():\n                    mean_v = torch.stack(v).mean()\n                    self.logger.experiment.add_scalar(f'val_{valset_idx}/avg_{k}', mean_v, global_step=cur_epoch)\n\n                for k, v in val_metrics_4tb.items():\n                    self.logger.experiment.add_scalar(f\"metrics_{valset_idx}/{k}\", v, global_step=cur_epoch)\n                \n            gc.collect()\n            torch.cuda.empty_cache()   \n            plt.close('all')\n\n        for thr in [5, 10, 20]:\n            # log on all ranks for ModelCheckpoint callback to work properly\n            self.log(f'auc@{thr}', torch.tensor(np.mean(multi_val_metrics[f'auc@{thr}'])))  # ckpt monitors on this\n\n    def test_step(self, batch, batch_idx):\n        with self.profiler.profile(\"LoFTR\"):\n            self.matcher(batch)\n\n        ret_dict, rel_pair_names = self._compute_metrics(batch)\n\n        with self.profiler.profile(\"dump_results\"):\n            if self.dump_dir is not None:\n                # dump results for further analysis\n                keys_to_save = {'mkpts0_f', 'mkpts1_f', 'mconf', 'epi_errs'}\n                pair_names = list(zip(*batch['pair_names']))\n                bs = batch['image0'].shape[0]\n                dumps = []\n                for b_id in range(bs):\n                    item = {}\n                    mask = batch['m_bids'] == b_id\n                    item['pair_names'] = pair_names[b_id]\n                    item['identifier'] = '#'.join(rel_pair_names[b_id])\n                    for key in keys_to_save:\n                        item[key] = batch[key][mask].cpu().numpy()\n                    for key in ['R_errs', 't_errs', 'inliers']:\n                        item[key] = batch[key][b_id]\n                    dumps.append(item)\n                ret_dict['dumps'] = dumps\n\n        return ret_dict\n\n    def test_epoch_end(self, outputs):\n        # metrics: dict of list, numpy\n        _metrics = [o['metrics'] for o in outputs]\n        metrics = {k: flattenList(gather(flattenList([_me[k] for _me in _metrics]))) for k in _metrics[0]}\n\n        # [{key: [{...}, *#bs]}, *#batch]\n        if self.dump_dir is not None:\n            Path(self.dump_dir).mkdir(parents=True, exist_ok=True)\n            _dumps = flattenList([o['dumps'] for o in outputs])  # [{...}, #bs*#batch]\n            dumps = flattenList(gather(_dumps))  # [{...}, #proc*#bs*#batch]\n            logger.info(f'Prediction and evaluation results will be saved to: {self.dump_dir}')\n\n        if self.trainer.global_rank == 0:\n            print(self.profiler.summary())\n            val_metrics_4tb = aggregate_metrics(metrics, self.config.TRAINER.EPI_ERR_THR)\n            logger.info('\\n' + pprint.pformat(val_metrics_4tb))\n            if self.dump_dir is not None:\n                np.save(Path(self.dump_dir) / 'LoFTR_pred_eval', dumps)","metadata":{"id":"Uvjw1vM54KGM","execution":{"iopub.status.busy":"2022-06-21T14:58:25.916014Z","iopub.execute_input":"2022-06-21T14:58:25.916848Z","iopub.status.idle":"2022-06-21T14:58:25.966444Z","shell.execute_reply.started":"2022-06-21T14:58:25.916806Z","shell.execute_reply":"2022-06-21T14:58:25.965445Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def read_megadepth_depth(path, pad_to=None):\n    depth = cv2.imread(path, 0)\n    if pad_to is not None:\n        depth, _ = pad_bottom_right(depth, pad_to, ret_mask=False)\n    depth = torch.from_numpy(depth).float()  # (h, w)\n    gc.collect()\n    return depth","metadata":{"id":"k3PV1vlC5K7c","execution":{"iopub.status.busy":"2022-06-21T14:58:25.967769Z","iopub.execute_input":"2022-06-21T14:58:25.968284Z","iopub.status.idle":"2022-06-21T14:58:25.982003Z","shell.execute_reply.started":"2022-06-21T14:58:25.968175Z","shell.execute_reply":"2022-06-21T14:58:25.981012Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from glob import glob\n\nl = glob('../input/image-matching-challenge-2022/train/*')\nl = [x for x in l if ('.csv' not in x) & ('.txt' not in x)]\n\ncal = pd.read_csv(l[0]+'/calibration.csv')\ncov = pd.read_csv(l[0]+'/pair_covisibility.csv')\ncal['path'] = l[0]+'/images/'+cal.image_id+'.jpg'\ncov['image1'] = [x.split('-')[0] for x in cov.pair]\ncov['image2'] = [x.split('-')[1] for x in cov.pair]\ncov = cov.merge(cal, left_on='image1', right_on='image_id', how='inner')\ncov = cov.merge(cal, left_on='image2', right_on='image_id', how='inner')\ncov.rename(columns={'camera_intrinsics_x':'camerainst1','rotation_matrix_x':'rot1','translation_vector_x':'trans1', \n                    'path_x':'path1', 'camera_intrinsics_y':'camerainst2','rotation_matrix_y':'rot2',\n                    'translation_vector_y':'trans2', 'path_y':'path2'}, inplace=True)\ntrain = cov[['camerainst1','rot1','trans1', 'path1', 'camerainst2','rot2', 'trans2', 'path2']]\n\nfor path in tqdm(l[1::]):\n  cal = pd.read_csv(path+'/calibration.csv')\n  cov = pd.read_csv(path+'/pair_covisibility.csv')\n  cal['path'] = path+'/images/'+cal.image_id+'.jpg'\n  cov['image1'] = [x.split('-')[0] for x in cov.pair]\n  cov['image2'] = [x.split('-')[1] for x in cov.pair]\n  cov = cov.merge(cal, left_on='image1', right_on='image_id', how='inner')\n  cov = cov.merge(cal, left_on='image2', right_on='image_id', how='inner')\n  cov.rename(columns={'camera_intrinsics_x':'camerainst1','rotation_matrix_x':'rot1','translation_vector_x':'trans1', \n                    'path_x':'path1', 'camera_intrinsics_y':'camerainst2','rotation_matrix_y':'rot2',\n                    'translation_vector_y':'trans2', 'path_y':'path2'}, inplace=True)\n  \n  train = train.append(cov[['camerainst1','rot1','trans1', 'path1', 'camerainst2','rot2', 'trans2', 'path2']])\n\ntrain.to_csv('train.csv', index=False)  ","metadata":{"id":"V0mDd5NU-sWa","execution":{"iopub.status.busy":"2022-06-21T14:58:25.983664Z","iopub.execute_input":"2022-06-21T14:58:25.984619Z","iopub.status.idle":"2022-06-21T14:58:35.116416Z","shell.execute_reply.started":"2022-06-21T14:58:25.984593Z","shell.execute_reply":"2022-06-21T14:58:35.11565Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head(2)","metadata":{"execution":{"iopub.status.busy":"2022-06-21T14:58:36.622017Z","iopub.execute_input":"2022-06-21T14:58:36.622683Z","iopub.status.idle":"2022-06-21T14:58:36.63563Z","shell.execute_reply.started":"2022-06-21T14:58:36.622644Z","shell.execute_reply":"2022-06-21T14:58:36.634744Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class MegaDepthDataset(Dataset):\n    def __init__(self,\n                 data,\n                 npz_path,\n                 mode='train',\n                 min_overlap_score=0.4,\n                 img_resize=None,\n                 df=None,\n                 img_padding=False,\n                 depth_padding=False,\n                 augment_fn=None,\n                 **kwargs):\n        \"\"\"\n        Manage one scene(npz_path) of MegaDepth dataset.\n        \n        Args:\n            root_dir (str): megadepth root directory that has `phoenix`.\n            npz_path (str): {scene_id}.npz path. This contains image pair information of a scene.\n            mode (str): options are ['train', 'val', 'test']\n            min_overlap_score (float): how much a pair should have in common. In range of [0, 1]. Set to 0 when testing.\n            img_resize (int, optional): the longer edge of resized images. None for no resize. 640 is recommended.\n                                        This is useful during training with batches and testing with memory intensive algorithms.\n            df (int, optional): image size division factor. NOTE: this will change the final image size after img_resize.\n            img_padding (bool): If set to 'True', zero-pad the image to squared size. This is useful during training.\n            depth_padding (bool): If set to 'True', zero-pad depthmap to (2000, 2000). This is useful during training.\n            augment_fn (callable, optional): augments images with pre-defined visual effects.\n        \"\"\"\n        super().__init__()\n    # self.root_dir = root_dir\n        self.mode = mode\n\n        # prepare scene_info and pair_info\n        if mode == 'test' and min_overlap_score != 0:\n            logger.warning(\"You are using `min_overlap_score`!=0 in test mode. Set to 0.\")\n            min_overlap_score = 0\n\n        # parameters for image resizing, padding and depthmap padding\n        if mode == 'train':\n            assert img_resize is not None and img_padding and depth_padding\n        self.img_resize = img_resize\n        self.df = df\n        self.img_padding = img_padding\n        self.depth_max_size = 2000 if depth_padding else None  # the upperbound of depthmaps size in megadepth.\n\n        # for training LoFTR\n        self.augment_fn = augment_fn if mode == 'train' else None\n        self.coarse_scale = getattr(kwargs, 'coarse_scale', 0.125)\n        self.path1 = data[\"path1\"].values\n        self.path2 = data[\"path2\"].values\n        self.camerainst1 = data[\"camerainst1\"].values\n        self.camerainst2 = data[\"camerainst2\"].values\n        self.rot1 = data[\"rot1\"].values\n        self.rot2 = data[\"rot2\"].values\n        self.trans1 = data[\"trans1\"].values\n        self.trans2 = data[\"trans2\"].values\n        gc.collect()\n        \n    def __len__(self):\n        return len(self.path1)\n\n    def __getitem__(self, idx):\n        # read grayscale image and mask. (1, h, w) and (h, w)\n        img_name0 = self.path1[idx]\n        img_name1 = self.path2[idx]\n        \n        # TODO: Support augmentation & handle seeds for each worker correctly.\n        image0, mask0, scale0 = read_megadepth_gray(\n            img_name0, self.img_resize, self.df, self.img_padding, None)\n        image1, mask1, scale1 = read_megadepth_gray(\n            img_name1, self.img_resize, self.df, self.img_padding, None)\n        depth_path0 = '../input/depth-masks-imc2022/depth_maps/'+img_name0.split('/')[4]+'/'+img_name0.split('/')[-1]\n        depth_path1 = '../input/depth-masks-imc2022/depth_maps/'+img_name1.split('/')[4]+'/'+img_name1.split('/')[-1]\n        \n        # read depth. shape: (h, w)\n        if self.mode in ['train', 'val']:\n            depth0 = read_megadepth_depth(\n                depth_path0, pad_to=self.depth_max_size)\n            depth1 = read_megadepth_depth(\n                depth_path1, pad_to=self.depth_max_size)\n        else:\n            depth0 = depth1 = torch.tensor([])\n\n        # read intrinsics of original size\n        K_0 = torch.tensor(np.asarray([float(x) for x in self.camerainst1[idx].split(\" \")]), dtype=torch.float).reshape(3, 3)\n        K_1 = torch.tensor(np.asarray([float(x) for x in self.camerainst2[idx].split(\" \")]), dtype=torch.float).reshape(3, 3)\n\n        # read and compute relative poses\n        R0 = self.rot1[idx].replace('{','').replace('}','').replace(\"'\", \"\")        \n        R0 = np.asarray([float(x) for x in R0.split(\" \")]).reshape(3, 3)\n        Tv0 = self.trans1[idx].replace('{','').replace('}','').replace(\"'\", \"\")\n        Tv0 = np.asarray([[float(x) for x in Tv0.split(\" \")]])\n        T0 = np.concatenate((R0, Tv0.T), axis=1)\n        T0 = np.concatenate((T0, np.asarray([[0, 0, 0, 1]])), axis=0)\n        del R0\n        del Tv0\n        R1 = self.rot2[idx].replace('{','').replace('}','').replace(\"'\", \"\")\n        R1 = np.asarray([float(x) for x in R1.split(\" \")]).reshape(3, 3)\n        Tv1 = self.trans2[idx].replace('{','').replace('}','').replace(\"'\", \"\")\n        Tv1 = np.asarray([[float(x) for x in Tv1.split(\" \")]])\n        T1 = np.concatenate((R1, Tv1.T), axis=1)\n        T1 = np.concatenate((T1, np.asarray([[0, 0, 0, 1]])), axis=0)\n        del R1\n        del Tv1\n        T_0to1 = torch.tensor(np.matmul(T1, np.linalg.inv(T0)), dtype=torch.float)[:4, :4]  # (4, 4)\n        T_1to0 = T_0to1.inverse()\n\n        data = {\n            'image0': image0,  \n            'depth0': depth0,  \n            'image1': image1,\n            'depth1': depth1,\n            'T_0to1': T_0to1,  \n            'T_1to0': T_1to0,\n            'K0': K_0,  \n            'K1': K_1,\n            'scale0': scale0,  \n            'scale1': scale1,\n            'dataset_name': 'MegaDepth',\n            'scene_id': idx,\n            'pair_id': idx,\n            'pair_names': (img_name0, img_name1),\n        }\n\n        # for LoFTR training\n        if mask0 is not None:  \n            if self.coarse_scale:\n                [ts_mask_0, ts_mask_1] = F.interpolate(torch.stack([mask0, mask1], dim=0)[None].float(),\n                                                       scale_factor=self.coarse_scale,\n                                                       mode='nearest',\n                                                       recompute_scale_factor=False)[0].bool()\n            data.update({'mask0': ts_mask_0, 'mask1': ts_mask_1})\n        del image0\n        del image1\n        del depth0\n        del depth1\n        gc.collect()\n        torch.cuda.empty_cache()\n        return data","metadata":{"id":"xQ8EVizw5VSM","execution":{"iopub.status.busy":"2022-06-21T14:58:38.284315Z","iopub.execute_input":"2022-06-21T14:58:38.284687Z","iopub.status.idle":"2022-06-21T14:58:38.316847Z","shell.execute_reply.started":"2022-06-21T14:58:38.284654Z","shell.execute_reply":"2022-06-21T14:58:38.316081Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class MultiSceneDataModule(pl.LightningDataModule):\n    \"\"\" \n    For distributed training, each training process is assgined\n    only a part of the training scenes to reduce memory overhead.\n    \"\"\"\n    def __init__(self, args, config ,data):\n        super().__init__()\n\n        self.trainval_data_source = config.DATASET.TRAINVAL_DATA_SOURCE\n        self.test_data_source = config.DATASET.TEST_DATA_SOURCE\n        self.train_data = data\n        self.train_pose_root = config.DATASET.TRAIN_POSE_ROOT  \n        self.train_npz_root = config.DATASET.TRAIN_NPZ_ROOT\n        self.train_list_path = config.DATASET.TRAIN_LIST_PATH\n        self.train_intrinsic_path = config.DATASET.TRAIN_INTRINSIC_PATH\n        self.val_data = data\n        self.val_pose_root = config.DATASET.VAL_POSE_ROOT \n        self.val_npz_root = config.DATASET.VAL_NPZ_ROOT\n        self.val_list_path = config.DATASET.VAL_LIST_PATH\n        self.val_intrinsic_path = config.DATASET.VAL_INTRINSIC_PATH\n        self.test_data = data\n        self.test_pose_root = config.DATASET.TEST_POSE_ROOT  \n        self.test_npz_root = config.DATASET.TEST_NPZ_ROOT\n        self.test_list_path = config.DATASET.TEST_LIST_PATH\n        self.test_intrinsic_path = config.DATASET.TEST_INTRINSIC_PATH\n        self.min_overlap_score_test = config.DATASET.MIN_OVERLAP_SCORE_TEST  \n        self.min_overlap_score_train = config.DATASET.MIN_OVERLAP_SCORE_TRAIN\n        self.augment_fn = build_augmentor(config.DATASET.AUGMENTATION_TYPE) \n        self.mgdpt_img_resize = config.DATASET.MGDPT_IMG_RESIZE  \n        self.mgdpt_img_pad = config.DATASET.MGDPT_IMG_PAD   \n        self.mgdpt_depth_pad = config.DATASET.MGDPT_DEPTH_PAD   \n        self.mgdpt_df = config.DATASET.MGDPT_DF  # 8\n        self.coarse_scale = 1 / config.LOFTR.RESOLUTION[0]  \n        self.batch_size = args.batch_size\n        self.data_sampler = config.TRAINER.DATA_SAMPLER\n        self.n_samples_per_subset = config.TRAINER.N_SAMPLES_PER_SUBSET\n        self.subset_replacement = config.TRAINER.SB_SUBSET_SAMPLE_REPLACEMENT\n        self.shuffle = config.TRAINER.SB_SUBSET_SHUFFLE\n        self.repeat = config.TRAINER.SB_REPEAT\n        self.parallel_load_data = getattr(args, 'parallel_load_data', False)\n        self.seed = config.TRAINER.SEED  \n\n    def setup(self, stage=None):\n        \"\"\"\n        Setup train / val / test dataset. This method will be called by PL automatically.\n        Args:\n            stage (str): 'fit' in training phase, and 'test' in testing phase.\n        \"\"\"\n\n        assert stage in ['fit', 'test'], \"stage must be either fit or test\"\n\n\n        if stage == 'fit':\n            self.train_dataset = self._setup_dataset(\n                self.train_data,\n                self.train_npz_root,\n                self.train_list_path,\n                self.train_intrinsic_path,\n                mode='train',\n                min_overlap_score=self.min_overlap_score_train,\n                pose_dir=self.train_pose_root)\n            \n            self.val_dataset = self._setup_dataset(\n                self.val_data,\n                self.val_npz_root,\n                self.val_list_path,\n                self.val_intrinsic_path,\n                mode='val',\n                min_overlap_score=self.min_overlap_score_test,\n                pose_dir=self.val_pose_root)\n            \n        else:  \n            self.test_dataset = self._setup_dataset(\n                self.test_data,\n                self.test_npz_root,\n                self.test_list_path,\n                self.test_intrinsic_path,\n                mode='test',\n                min_overlap_score=self.min_overlap_score_test,\n                pose_dir=self.test_pose_root)\n            \n\n    def _setup_dataset(self,\n                       data,\n                       split_npz_root,\n                       scene_list_path,\n                       intri_path,\n                       mode='train',\n                       min_overlap_score=0.,\n                       pose_dir=None):\n        \"\"\" Setup train / val / test set\"\"\"\n        local_npz_names = \"\"\n        dataset_builder = self._build_concat_dataset\n        return dataset_builder(data, local_npz_names, split_npz_root, intri_path,\n                                mode=mode, min_overlap_score=min_overlap_score, pose_dir=pose_dir)\n\n    def _build_concat_dataset(\n        self,\n        data,\n        npz_names,\n        npz_dir,\n        intrinsic_path,\n        mode,\n        min_overlap_score=0.,\n        pose_dir=None\n    ):\n        datasets = []\n        augment_fn = self.augment_fn if mode == 'train' else None\n        data_source = self.trainval_data_source if mode in ['train', 'val'] else self.test_data_source\n        npz_path = \"\"\n\n        datasets.append(\n            MegaDepthDataset(data,\n                             npz_path,\n                             mode=mode,\n                             min_overlap_score=min_overlap_score,\n                             img_resize=self.mgdpt_img_resize,\n                             df=self.mgdpt_df,\n                             img_padding=self.mgdpt_img_pad,\n                             depth_padding=self.mgdpt_depth_pad,\n                             augment_fn=augment_fn,\n                             coarse_scale=self.coarse_scale))\n        return ConcatDataset(datasets)\n    \n\n    def train_dataloader(self):\n        \"\"\" Build training dataloader for ScanNet / MegaDepth. \"\"\"\n\n        dataloader = DataLoader(self.train_dataset, batch_size=self.batch_size, \n                              shuffle=True, \n                              num_workers=2, pin_memory=True, drop_last=False)\n        return dataloader\n    \n    def val_dataloader(self):\n        \"\"\" Build validation dataloader for ScanNet / MegaDepth. \"\"\"\n        dataloader = DataLoader(self.val_dataset, batch_size=self.batch_size, \n                              shuffle=False, \n                              num_workers=2, pin_memory=True, drop_last=False)\n        return dataloader\n\n    def test_dataloader(self, *args, **kwargs):\n        sampler = DistributedSampler(self.test_dataset, shuffle=True)\n        return DataLoader(self.test_dataset, sampler=sampler, batch_size=self.batch_size, **self.test_loader_params)\n\n\ndef _build_dataset(dataset: Dataset, *args, **kwargs):\n    return dataset(*args, **kwargs)\n","metadata":{"id":"vG3EgaOL5rJ7","execution":{"iopub.status.busy":"2022-06-21T14:58:39.035438Z","iopub.execute_input":"2022-06-21T14:58:39.035823Z","iopub.status.idle":"2022-06-21T14:58:39.060727Z","shell.execute_reply.started":"2022-06-21T14:58:39.03579Z","shell.execute_reply":"2022-06-21T14:58:39.059693Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def parse_args():\n    \n    parser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n    parser.add_argument(\n        'data_cfg_path', type=str, help='data config path')\n    parser.add_argument(\n        'main_cfg_path', type=str, help='main config path')\n    parser.add_argument(\n        '--exp_name', type=str, default='default_exp_name')\n    parser.add_argument(\n        '--batch_size', type=int, default=4, help='batch_size per gpu')\n    parser.add_argument(\n        '--num_workers', type=int, default=4)\n    parser.add_argument(\n        '--pin_memory', type=lambda x: bool(strtobool(x)),\n        nargs='?', default=True, help='whether loading data to pinned memory or not')\n    parser.add_argument(\n        '--ckpt_path', type=str, default=\"../input/loftr-outdoor/outdoor_ds.ckpt\",   \n        help='pretrained checkpoint path, helpful for using a pre-trained coarse-only LoFTR')\n    parser.add_argument(\n        '--disable_ckpt', action='store_true',\n        help='disable checkpoint saving (useful for debugging).')\n    parser.add_argument(\n        '--profiler_name', type=str, default=None,\n        help='options: [inference, pytorch], or leave it unset')\n    parser.add_argument(\n        '--parallel_load_data', action='store_true',\n        help='load datasets in with multiple processes.')\n\n    parser = pl.Trainer.add_argparse_args(parser)\n    return parser.parse_args('LoFTR/configs/data/megadepth_trainval_840.py LoFTR/configs/loftr/outdoor/buggy_pos_enc/loftr_ds.py \\\n    --exp_name test \\\n    --accelerator gpu \\\n    --gpus 0 \\\n    --num_nodes 0 \\\n    --batch_size 1 \\\n    --max_epochs 1 \\\n    --num_workers 2 \\\n    --flush_logs_every_n_steps 1 \\\n    --limit_val_batches 128 \\\n    --limit_train_batches 16'.split())","metadata":{"id":"zbuIiuEG6Uc8","execution":{"iopub.status.busy":"2022-06-21T14:58:39.872834Z","iopub.execute_input":"2022-06-21T14:58:39.873484Z","iopub.status.idle":"2022-06-21T14:58:39.883703Z","shell.execute_reply.started":"2022-06-21T14:58:39.873444Z","shell.execute_reply":"2022-06-21T14:58:39.882843Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train():\n    args = parse_args()\n    rank_zero_only(pprint.pprint)(vars(args))\n    config = get_cfg_defaults()\n    config.merge_from_file(args.main_cfg_path)\n    config.merge_from_file(args.data_cfg_path)\n    pl.seed_everything(1)      \n    args.gpus = _n_gpus = setup_gpus(args.gpus)\n    config.TRAINER.TRUE_BATCH_SIZE = args.batch_size\n    _scaling = config.TRAINER.TRUE_BATCH_SIZE / config.TRAINER.CANONICAL_BS\n    # config.TRAINER.TRUE_LR = 1e-5 * _scaling\n    config.TRAINER.TRUE_LR = 2e-6\n    # config.TRAINER.OPTIMIZER = 'adam'\n    config.TRAINER.SEED = 1\n    config.DATASET.MGDPT_IMG_RESIZE = 840\n    # config.TRAINER.SCALING = _scaling\n    # config.TRAINER.WORLD_SIZE = _n_gpus * args.num_nodes\n    # config.TRAINER.WARMUP_STEP = math.floor(config.TRAINER.WARMUP_STEP / _scaling)\n    # config.TRAINER.SB_SUBSET_SAMPLE_REPLACEMENT = False\n    # config.TRAINER.TRUE_LR = 0.008\n    # config.TRAINER.WARMUP_STEP = 1875\n    # config.DATASET.MIN_OVERLAP_SCORE_TRAIN = 0.9\n    # config.DATASET.MGDPT_DF  # 8\n\n    profiler = build_profiler(args.profiler_name)\n    model = PL_LoFTR(config, pretrained_ckpt=args.ckpt_path, profiler=profiler)  \n    data = pd.read_csv(\"train.csv\", nrows=100)\n    data['id'] = [x.split('/')[1] for x in data.path1]\n    gc.collect()\n        \n    ckpt_dir = './checkpoints'\n    ckpt_callback = ModelCheckpoint(monitor='auc@10', verbose=True, \n                                    save_top_k=5, \n                                    mode='max',\n                                    save_last=True,\n                                    dirpath='./chk/',\n                                    filename='chk-{auc@5:.3f}-{auc@10:.3f}-{auc@20:.3f}')\n    lr_monitor = LearningRateMonitor(logging_interval='step')\n    callbacks = [lr_monitor]\n    if not args.disable_ckpt:\n        callbacks.append(ckpt_callback)\n    \n    trainer = pl.Trainer.from_argparse_args(\n        args,\n        gradient_clip_val=config.TRAINER.GRADIENT_CLIPPING,\n        callbacks=callbacks,\n        weights_summary='full',\n        profiler=profiler)\n\n    data = data.sample(frac=1)\n    data_module = MultiSceneDataModule(args, config, data)\n    trainer.fit(model, datamodule=data_module)\n","metadata":{"id":"1udwZii_6plT","execution":{"iopub.status.busy":"2022-06-21T14:58:40.812127Z","iopub.execute_input":"2022-06-21T14:58:40.812843Z","iopub.status.idle":"2022-06-21T14:58:40.825928Z","shell.execute_reply.started":"2022-06-21T14:58:40.812782Z","shell.execute_reply":"2022-06-21T14:58:40.825117Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train()","metadata":{"id":"41hvhjSYJF_z","outputId":"eac63b85-ed4c-4c90-ece4-fc7ef75785b5","execution":{"iopub.status.busy":"2022-06-21T14:58:41.891568Z","iopub.execute_input":"2022-06-21T14:58:41.892219Z","iopub.status.idle":"2022-06-21T15:01:01.104279Z","shell.execute_reply.started":"2022-06-21T14:58:41.892171Z","shell.execute_reply":"2022-06-21T15:01:01.103421Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}