{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# About this notebook\n\nIn this notebook I'm presenting a simplified version of this competition: how to match an image with a subcrop of itself.\n\nYou can find explanations about my motivations here : https://www.kaggle.com/competitions/image-matching-challenge-2022/discussion/323403. My final goal is obviously much more complex but I first need to solve the basics.\n\n# QuadTreeAttention\n\nI'm showing here how to use a pretrained version of QuadTreeAttention : https://arxiv.org/abs/2201.02767\n\n# Updated version (version 4)\n\nTraining is now working,\n\n# Weird behaviors\n\n- Pretrained or fine-tuned models are good at detecting crops, but they stop working as soon as a 90° rotation is applied.","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-05-13T12:09:29.364331Z","iopub.execute_input":"2022-05-13T12:09:29.364705Z","iopub.status.idle":"2022-05-13T12:10:56.728985Z","shell.execute_reply.started":"2022-05-13T12:09:29.364659Z","shell.execute_reply":"2022-05-13T12:10:56.727825Z"}}},{"cell_type":"code","source":"!pip -q install -U kornia\n!pip -q install kornia-moons\n!pip3 -q install torch==1.8.2+cu102 torchvision==0.9.2+cu102 torchaudio==0.8.2 -f https://download.pytorch.org/whl/lts/1.8/torch_lts.html\n!pip -q install ninja\n!pip -q install loguru\n!pip -q install einops\n!pip -q install timm","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-05-31T14:28:03.539029Z","iopub.execute_input":"2022-05-31T14:28:03.539539Z","iopub.status.idle":"2022-05-31T14:30:42.032804Z","shell.execute_reply.started":"2022-05-31T14:28:03.539466Z","shell.execute_reply":"2022-05-31T14:30:42.031911Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!cp -r ../input/quadtreeattention/ ../working/ # input folder is read only\n! cd ../working/quadtreeattention/QuadTreeAttention-master/QuadTreeAttention/ && pip install .","metadata":{"execution":{"iopub.status.busy":"2022-05-31T14:30:42.036484Z","iopub.execute_input":"2022-05-31T14:30:42.036701Z","iopub.status.idle":"2022-05-31T14:32:42.116776Z","shell.execute_reply.started":"2022-05-31T14:30:42.036674Z","shell.execute_reply":"2022-05-31T14:32:42.115935Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import cv2\nimport kornia as K\nimport kornia.feature as KF\nfrom kornia.feature.loftr import LoFTR\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport torch\nimport glob\nimport random\n\nimport torchvision\nimport kornia_moons.feature as KMF\nfrom PIL import Image\n\nimport sys\nsys.path.append('../working/quadtreeattention/QuadTreeAttention-master/')\nsys.path.append('../working/quadtreeattention/QuadTreeAttention-master/FeatureMatching/')\nsys.path.append('../working/quadtreeattention/QuadTreeAttention-master/QuadTreeAttention/')","metadata":{"execution":{"iopub.status.busy":"2022-05-31T14:32:42.120216Z","iopub.execute_input":"2022-05-31T14:32:42.120459Z","iopub.status.idle":"2022-05-31T14:32:44.283391Z","shell.execute_reply.started":"2022-05-31T14:32:42.120432Z","shell.execute_reply":"2022-05-31T14:32:44.282531Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Utilities functions","metadata":{}},{"cell_type":"code","source":"from FeatureMatching.src.utils.plotting import make_matching_figure\nfrom pathlib import Path\nimport matplotlib.cm as cm\n\ndef get_images_path(path):\n    path_to_imgs = [str(p) for p in Path(path).rglob(\"**/images/*.jpg\")]\n    # remove macros\n    return path_to_imgs\n\ndef match_and_draw_dataset(matcher, dataset, conf_thresh=0, max_img=20, device=\"cuda\", rotate=False, plot_no_match=True):\n    \"\"\"\n    Match and draw from all elements in a dataset\n    for a specific model\n\n    Parameters\n    ----------\n    matcher (torch nn module): a matcher model (LOFTR, QUADTREE)\n    dataset (torch dataset): a dataset with matching pairs\n    conf_thresh (float): between 0 and 1, confidence of shown matches\n    max_img (int) : max images to plot\n    device (str) : device to make inference\n    \"\"\"\n    matcher.eval()\n    matcher.to(device)\n\n    for idx in range(min(len(dataset), max_img)):\n        batch = dataset[idx]\n        batch[\"image0\"] = batch['image0'].unsqueeze(0).to(device)\n        batch[\"image1\"] = batch['image1'].unsqueeze(0).to(device)\n\n        img0_raw = K.tensor_to_image(batch[\"raw_image0\"]) #np.tile(batch[\"image0\"].squeeze(0).cpu().numpy().transpose(1, 2, 0), 3)\n        img1_raw = K.tensor_to_image(batch[\"raw_image1\"]) #np.tile(batch[\"image1\"].squeeze(0).cpu().numpy().transpose(1, 2, 0), 3)\n\n        with torch.no_grad():\n            matcher.eval()\n            matcher.to(device)\n            matcher(batch)\n            mconf = batch['mconf'].cpu().numpy()\n            mask_conf = mconf > conf_thresh\n            mconf = mconf[mask_conf]\n            mkpts0 = batch['mkpts0_f'].cpu().numpy()[mask_conf]\n            mkpts1 = batch['mkpts1_f'].cpu().numpy()[mask_conf]\n            \n            color = cm.jet(mconf)\n        if len(mkpts0)<=3 and not plot_no_match:\n            print(\"Not enough matches\")\n            continue\n        text = [\n            'LoFTR',\n            'Matches: {}'.format(len(mkpts0)),\n        ]\n        fig = make_matching_figure(img0_raw, img1_raw, mkpts0, mkpts1, color, text=text)\n        plt.show()\n        plt.close()\n\n        if rotate:\n            # Look at transposition\n            batch[\"image0\"] = batch['image0']\n            batch[\"image1\"] = torch.transpose(batch['image1'], 2, 3)\n            img0_raw = K.tensor_to_image(batch[\"raw_image0\"]) #np.tile(batch[\"image0\"].squeeze(0).cpu().numpy().transpose(1, 2, 0), 3)\n            img1_raw = K.tensor_to_image(batch[\"raw_image1\"]).transpose(1, 0, 2) #np.tile(batch[\"image1\"].squeeze(0).cpu().numpy().transpose(1, 2, 0), 3)\n\n            with torch.no_grad():\n                matcher(batch)\n                mconf = batch['mconf'].cpu().numpy()\n                mask_conf = mconf > conf_thresh\n                mconf = mconf[mask_conf]\n                mkpts0 = batch['mkpts0_f'].cpu().numpy()[mask_conf]\n                mkpts1 = batch['mkpts1_f'].cpu().numpy()[mask_conf]\n                \n                color = cm.jet(mconf)\n\n            text = [\n                'LoFTR transpose',\n                'Init Matches: {}'.format(len(mask_conf)),\n                'Thresh Matches: {}'.format(len(mkpts0)),\n            ]\n            fig = make_matching_figure(img0_raw, img1_raw, mkpts0, mkpts1, color, text=text)\n            plt.show()\n            plt.close()\n\n    return","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-05-31T14:32:44.285879Z","iopub.execute_input":"2022-05-31T14:32:44.286157Z","iopub.status.idle":"2022-05-31T14:32:44.307739Z","shell.execute_reply.started":"2022-05-31T14:32:44.286106Z","shell.execute_reply":"2022-05-31T14:32:44.306955Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Synthetic Dataset\n\nThe goal here is simply to pick an image as input, sample a smaller crop inside and try to match the crop with the image.\n\n\n","metadata":{"execution":{"iopub.status.busy":"2022-05-13T10:40:58.927175Z","iopub.execute_input":"2022-05-13T10:40:58.92785Z","iopub.status.idle":"2022-05-13T10:40:58.933189Z","shell.execute_reply.started":"2022-05-13T10:40:58.927812Z","shell.execute_reply":"2022-05-13T10:40:58.932275Z"}}},{"cell_type":"code","source":"from torch.utils.data import Dataset\n\nclass RotateSyntheticDataset(Dataset):\n    # TODO : Is rotation correct ?\n    def __init__(self,\n                 path_to_dermoscopies,\n                 img_size=(320, 320), # (64*6, 48*6) (640, 480)\n                 crop_range=[2],\n                 rotation_prob=0.5,\n                 augment_fn=None,\n                 light_mode=False):\n        \"\"\"\n        Creates artificial dataset.\n        \n        Args:\n            - path_to_dermoscopies (list): iterable of path to dermoscopic images\n            - img_resize (int, int): Final size of image shown to model (should be divisible by 64?)\n            - augment_fn (callable, optional): augments images with pre-defined visual effects.\n            - crop_range (list of int): ratios between original image and image crop\n        \"\"\"\n        super().__init__()\n        self.path_to_dermoscopies = path_to_dermoscopies\n        \n        self.img_size = img_size\n        self.crop_range = crop_range\n        self.rotation_prob = rotation_prob\n        self.augment_fn = augment_fn\n        self.light_mode = light_mode\n\n        # for training LoFTR\n        # self.coarse_scale = 1/8 #getattr(kwargs, 'coarse_scale', 0.125) # 0.125=1/8\n\n    def __len__(self):\n        return len(self.path_to_dermoscopies)\n    \n    def __getitem__(self, idx):\n        img_path = self.path_to_dermoscopies[idx]\n        img_name = img_path.split('/')[-1]\n        \n        image0 = self.load_torch_image(img_path) #(h, w)\n\n        rand_aug = np.random.rand()\n\n        if rand_aug > 0.5:\n            image0 = torch.rot90(image0, k=1, dims=[1, 2])\n    \n        h_init, w_init = image0.shape[1:]\n        \n        image1, crop_pos = self.basic_crop(image0)\n        hc, wc = image1.shape[1:]\n        # resize imgs\n        image0 = torchvision.transforms.Resize(self.img_size)(image0)\n        image1 = torchvision.transforms.Resize(self.img_size)(image1)\n        \n        if self.augment_fn is not None:\n            image0 = image0.numpy().transpose(1, 2, 0).astype(np.uint8)\n            image1 = image1.numpy().transpose(1, 2, 0).astype(np.uint8)\n            image0 = self.augment_fn()(image=image0)[\"image\"]\n            image1 = self.augment_fn()(image=image1)[\"image\"]\n\n            image0 = torch.tensor(image0.transpose(2, 0, 1))\n            image1 = torch.tensor(image1.transpose(2, 0, 1))\n\n        scale0 = image0.shape[1:] #(h, w)\n        h0, w0 = scale0\n        \n        depth0 = torch.ones(scale0) # everything to 1?\n        # intrisinct matrix\n        K_0 = torch.tensor([[1, 0, h0/2], # h0/h_init\n                            [0, 1, w0/2], # w0/w_init\n                            [0, 0, 1]\n                           ]) # 1 to 1 pixel, centered position\n        # rotation matrix\n        R0 = np.diag([1, 1, 1]) # no rotation\n        # translation vector\n        Tv0 = np.array([[w0/2, h0/2, 0]])\n\n        T0 = np.concatenate((R0, Tv0.T), axis=1)\n        T0 = np.concatenate((T0, np.asarray([[0, 0, 0, 1]])), axis=0)\n        \n        random_prob = np.random.rand()\n        rotate = random_prob < self.rotation_prob # 0.5\n        \n        # get a random crop\n        if rotate:\n            image1 = torch.transpose(image1, 1, 2) #torch.rot90(image1, k=1, dims=[1, 2]) #torch.transpose(image1, 1, 2)\n                \n        depth1 = torch.ones(scale0) # everything to 1?\n\n        K_1 = torch.tensor([[h_init/hc, 0, h0/2], # h0/hc  , h0/2\n                            [0,  w_init/wc, w0/2], # w0/wc , w0/2\n                            [0, 0, 1]\n                           ]) # 1 to 1 pixel, centered position\n        if rotate:\n            # image need to be switched the opposite\n            R1 = np.array([[0, 1, 0],\n                           [1, 0, 0],\n                           [0, 0, 1]])\n#             R1 = np.array([[cos(th), -sin(th), 0],\n#                           [sin(th), cos(th), 0],\n#                           [0, 0, 1]])\n        else:\n            R1 = np.diag([1, 1, 1]) # no rotation\n\n        # AXES ARE MATH BASED NOT TENSOR BASED\n        Tv1 = np.array([[w0-np.mean(crop_pos[1])*w0/w_init,\n                           h0-np.mean(crop_pos[0])*h0/h_init,\n                           0]])\n        T1 = np.concatenate((R1, Tv1.T), axis=1)\n        T1 = np.concatenate((T1, np.asarray([[0, 0, 0, 1]])), axis=0)\n        \n        T_0to1 = torch.tensor(np.matmul(T1, np.linalg.inv(T0)), dtype=torch.float)[:4, :4]  # (4, 4)\n        T_1to0 = T_0to1.inverse()\n            \n        # SCALE_0 = 1/torch.tensor([w_init/w0, h_init/h0], dtype=torch.float)\n        # SCALE_1 = 1/torch.tensor([wc/w0, hc/h0], dtype=torch.float)\n        # MAYBE try to check what happens if inverse image0 and image1\n\n        data = {\n            'image0': image0.float().mean(axis=0, keepdim=True) / 255,  # (1, h, w)\n            'depth0': depth0,  # (h, w)\n            'image1': image1.float().mean(axis=0, keepdim=True) / 255,\n            'depth1': depth1,\n            'T_0to1': T_0to1,  # (4, 4)\n            'T_1to0': T_1to0,\n            'K0': K_0,  # (3, 3)\n            'K1': K_1,\n            # 'scale0': SCALE_0,  # [scale_w, scale_h]\n            # 'scale1': SCALE_1,\n            'dataset_name': 'scannet',\n            'scene_id': idx,\n            'pair_id': idx,\n            'pair_names': (img_name, \"macro_\"+img_name),\n        }\n       \n        if self.light_mode:\n            return data\n        else:\n            data[\"raw_image0\"] = image0\n            data[\"raw_image1\"] = image1\n            return data\n\n    def load_torch_image(self, fname):\n        img = K.image_to_tensor(cv2.imread(fname), False)#.float()\n        img = K.color.bgr_to_rgb(img).squeeze()\n        c, h, w = img.shape\n\n        return img\n    \n    def basic_crop(self, img):\n        c, h, w = img.shape\n        crop_ratio = np.random.choice(self.crop_range)\n        crop_size = int(h / crop_ratio), int(w / crop_ratio)\n        \n        max_x = max(1, h - crop_size[0])\n        max_y = max(1, w - crop_size[1])\n\n        rand_x = np.random.randint(max_x)\n        rand_y = np.random.randint(max_y)\n\n        end_x = rand_x + crop_size[0]\n        end_y = rand_y + crop_size[1]\n        return img[:, rand_x:end_x, rand_y:end_y], ((rand_x,end_x), (rand_y,end_y))","metadata":{"execution":{"iopub.status.busy":"2022-05-31T14:32:44.309241Z","iopub.execute_input":"2022-05-31T14:32:44.309653Z","iopub.status.idle":"2022-05-31T14:32:44.340696Z","shell.execute_reply.started":"2022-05-31T14:32:44.309615Z","shell.execute_reply":"2022-05-31T14:32:44.339973Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# data module\n\nimport pytorch_lightning as pl\nfrom torch.utils.data import DataLoader\n\nclass BasicDataModule(pl.LightningDataModule):\n    def __init__(self, path_to_dermoscopies, transforms=None, img_size=(640,640),\n                 batch_size: int = 4, crop_range=[2, 3, 4], num_workers = 4, rotation_prob=0.5):\n        super().__init__()\n        self.batch_size = batch_size\n        self.path_to_dermoscopies = path_to_dermoscopies\n        self.crop_range = crop_range\n        self.transforms = transforms\n        self.num_workers = num_workers\n        self.rotation_prob = rotation_prob\n        self.img_size = img_size\n    \n    def setup(self, stage):\n        return\n\n    def train_dataloader(self):\n#         return DataLoader(SyntheticDataset(path_to_dermoscopies, augment_fn=non_geom_transforms),\n#                           batch_size=self.batch_size, num_workers=4)\n        return DataLoader(RotateSyntheticDataset(self.path_to_dermoscopies,\n                                                 augment_fn=self.transforms,\n                                                 crop_range=self.crop_range,\n                                                 img_size=self.img_size,\n                                                 rotation_prob=self.rotation_prob,\n                                                 light_mode=True\n                                                ),\n                          batch_size=self.batch_size,\n                          num_workers=self.num_workers)\n\n    def val_dataloader(self):\n        return DataLoader(RotateSyntheticDataset(self.path_to_dermoscopies,\n                                           augment_fn=None,\n                                           crop_range=self.crop_range,\n                                           img_size=self.img_size,\n                                           rotation_prob=self.rotation_prob,\n                                           light_mode=True\n                                           ),\n                          batch_size=self.batch_size,\n                          num_workers=self.num_workers)\n\n    def test_dataloader(self):\n        return DataLoader(RotateSyntheticDataset(self.path_to_dermoscopies,\n                                                crop_range=self.crop_range,\n                                                img_size=self.img_size,\n                                                rotation_prob=self.rotation_prob,\n                                                augment_fn=None,\n                                                light_mode=True\n                                                ),\n                          batch_size=self.batch_size, num_workers=self.num_workers)\n\n    def predict_dataloader(self):\n        return DataLoader(RotateSyntheticDataset(self.path_to_dermoscopies,\n                                                crop_range=self.crop_range,\n                                                img_size=self.img_size,\n                                                rotation_prob=self.rotation_prob,\n                                                augment_fn=None,\n                                                light_mode=True\n                                                ),\n                          batch_size=self.batch_size, num_workers=self.num_workers)","metadata":{"execution":{"iopub.status.busy":"2022-05-31T14:32:44.342227Z","iopub.execute_input":"2022-05-31T14:32:44.34254Z","iopub.status.idle":"2022-05-31T14:32:49.623227Z","shell.execute_reply.started":"2022-05-31T14:32:44.342503Z","shell.execute_reply":"2022-05-31T14:32:49.622491Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# transformations for augmentation\nimport albumentations as albu\ndef non_geom_transforms(p=0.2):\n    return albu.Compose([\n        albu.ColorJitter(p=0.5),\n        albu.RandomRain(p=0.05),  # random occlusion\n        albu.RandomSunFlare(src_radius=50, p=0.1),\n        albu.ImageCompression(p=0.25),\n        albu.ISONoise(p=0.25),\n        \n        albu.OneOf(\n            [\n                albu.MotionBlur(blur_limit=(3, 5), always_apply=True),\n                albu.GaussianBlur(blur_limit=(3, 5), always_apply=True),\n            ],\n            p=p,\n        ),\n        albu.OneOf(\n            [\n                albu.GaussNoise(var_limit=(1.0, 2.0), always_apply=True),\n                 albu.RandomFog(fog_coef_lower=0.01, fog_coef_upper=0.25, always_apply=True),\n                 albu.MultiplicativeNoise(multiplier=(0.75, 1.25), elementwise=True, always_apply=True),  # noqa\n            ],\n            p=p,\n        ),\n         albu.CoarseDropout(\n                    max_holes=8,\n                    max_height=16,\n                    max_width=32,\n                    min_holes=2,\n                    min_height=2,\n                    min_width=2,\n                    fill_value=0,\n                    always_apply=True,\n                )\n    ])","metadata":{"execution":{"iopub.status.busy":"2022-05-31T14:32:49.624672Z","iopub.execute_input":"2022-05-31T14:32:49.624906Z","iopub.status.idle":"2022-05-31T14:32:51.204299Z","shell.execute_reply.started":"2022-05-31T14:32:49.624872Z","shell.execute_reply":"2022-05-31T14:32:51.203564Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Config for pretrained Quadtree","metadata":{}},{"cell_type":"code","source":"from FeatureMatching.src.config.default import get_cfg_defaults\nconfig = get_cfg_defaults()\nCROP_RANGE = [2, 3, 4] # 2\nTRANSFORMS = non_geom_transforms # None\nNB_EPOCHS = 10\n\n\n# INDOOT lofrt_ds_quadtree config\nconfig.LOFTR.MATCH_COARSE.MATCH_TYPE = 'dual_softmax'\nconfig.LOFTR.MATCH_COARSE.SPARSE_SPVS = False\nconfig.LOFTR.RESNETFPN.INITIAL_DIM = 128\nconfig.LOFTR.RESNETFPN.BLOCK_DIMS=[128, 196, 256]\nconfig.LOFTR.COARSE.D_MODEL = 256\nconfig.LOFTR.COARSE.BLOCK_TYPE = 'quadtree'\nconfig.LOFTR.COARSE.ATTN_TYPE = 'B'\nconfig.LOFTR.COARSE.TOPKS=[32, 16, 16]\nconfig.LOFTR.FINE.D_MODEL = 128\nconfig.TRAINER.WORLD_SIZE = 1 # 8\nconfig.TRAINER.CANONICAL_BS = 32\nconfig.TRAINER.TRUE_BATCH_SIZE = 1\n_scaling = 1\nconfig.TRAINER.ENABLE_PLOTTING = False\nconfig.TRAINER.SCALING = _scaling\nconfig.TRAINER.TRUE_LR = 1e-4 # 1e-4 config.TRAINER.CANONICAL_LR * _scaling\nconfig.TRAINER.WARMUP_STEP = 0 #math.floor(config.TRAINER.WARMUP_STEP / _scaling)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-05-31T14:32:51.205438Z","iopub.execute_input":"2022-05-31T14:32:51.205674Z","iopub.status.idle":"2022-05-31T14:32:51.220976Z","shell.execute_reply.started":"2022-05-31T14:32:51.205642Z","shell.execute_reply":"2022-05-31T14:32:51.220188Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# arguments \n\nimport argparse\ndef parse_args():\n    # init a costum parser which will be added into pl.Trainer parser\n    # check documentation: https://pytorch-lightning.readthedocs.io/en/latest/common/trainer.html#trainer-flags\n    parser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n    parser.add_argument(\n        'data_cfg_path', type=str, help='data config path')\n    parser.add_argument(\n        'main_cfg_path', type=str, help='main config path')\n    parser.add_argument(\n        '--exp_name', type=str, default='default_exp_name')\n    parser.add_argument(\n        '--batch_size', type=int, default=4, help='batch_size per gpu')\n    parser.add_argument(\n        '--num_workers', type=int, default=4)\n    parser.add_argument(\n        '--pin_memory', type=lambda x: bool(strtobool(x)),\n        nargs='?', default=True, help='whether loading data to pinned memory or not')\n    parser.add_argument(\n        '--ckpt_path', type=str, default=\"../input/kornia-loftr/outdoor_ds.ckpt\",\n        help='pretrained checkpoint path, helpful for using a pre-trained coarse-only LoFTR')\n    parser.add_argument(\n        '--disable_ckpt', action='store_true',\n        help='disable checkpoint saving (useful for debugging).')\n    parser.add_argument(\n        '--profiler_name', type=str, default=None,\n        help='options: [inference, pytorch], or leave it unset')\n    parser.add_argument(\n        '--parallel_load_data', action='store_true',\n        help='load datasets in with multiple processes.')\n\n    parser = pl.Trainer.add_argparse_args(parser)\n    nb_epochs = NB_EPOCHS # 20\n    return parser.parse_args(f'../input/loftrutils/LoFTR-master/LoFTR-master/configs/data/megadepth_trainval_640.py ../input/loftrutils/LoFTR-master/LoFTR-master/configs/loftr/outdoor/loftr_ds_dense.py --exp_name test --gpus 0 --num_nodes 0 --accelerator gpu --batch_size 1 --check_val_every_n_epoch 1 --log_every_n_steps 1 --flush_logs_every_n_steps 1 --limit_val_batches 1 --num_sanity_val_steps 10 --benchmark True --max_epochs {nb_epochs}'.split())\n\nfrom pytorch_lightning.utilities import rank_zero_only\nimport pytorch_lightning as pl\nimport pprint\nargs = parse_args()\nrank_zero_only(pprint.pprint)(vars(args))","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-05-31T14:32:51.222378Z","iopub.execute_input":"2022-05-31T14:32:51.222671Z","iopub.status.idle":"2022-05-31T14:32:51.255931Z","shell.execute_reply.started":"2022-05-31T14:32:51.222636Z","shell.execute_reply":"2022-05-31T14:32:51.255289Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_images = get_images_path('../input/image-matching-challenge-2022/train/brandenburg_gate/')","metadata":{"execution":{"iopub.status.busy":"2022-05-31T14:32:51.258134Z","iopub.execute_input":"2022-05-31T14:32:51.258392Z","iopub.status.idle":"2022-05-31T14:32:51.32213Z","shell.execute_reply.started":"2022-05-31T14:32:51.25836Z","shell.execute_reply":"2022-05-31T14:32:51.321432Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Define Trainer","metadata":{}},{"cell_type":"code","source":"from FeatureMatching.src.utils.profiler import build_profiler\nfrom FeatureMatching.src.lightning.lightning_loftr import PL_LoFTR\nfrom loguru import logger as loguru_logger\nfrom pytorch_lightning.loggers import TensorBoardLogger\nfrom pytorch_lightning.callbacks import ModelCheckpoint, LearningRateMonitor\n\n\n# lightning module\ndisable_ckpt = True\nprofiler_name = None # help='options: [inference, pytorch], or leave it unset\nprofiler = build_profiler(profiler_name)\nmodel = PL_LoFTR(config,\n                 pretrained_ckpt= \"../input/quadtreecheckpoints/outdoor_quadtree.ckpt\", # args.ckpt_path, from scratch atm\n                 profiler=profiler)\nloguru_logger.info(f\"LoFTR LightningModule initialized!\")\n\n# lightning data\ndata_module = BasicDataModule(train_images, transforms=TRANSFORMS, crop_range=CROP_RANGE)\nloguru_logger.info(f\"LoFTR DataModule initialized!\")\n\n# TensorBoard Logger\nlogger = TensorBoardLogger(save_dir=\"../working/logs\",\n                           name=\"test_kaggle\",\n                           default_hp_metric=False)\nckpt_dir = Path(logger.log_dir) / 'checkpoints'\n\n# Callbacks\n# TODO: update ModelCheckpoint to monitor multiple metrics\nckpt_callback = ModelCheckpoint(monitor='auc@10', verbose=True, save_top_k=5, mode='max',\n                                save_last=True,\n                                dirpath=str(ckpt_dir),\n                                filename='{epoch}-{auc@5:.3f}-{auc@10:.3f}-{auc@20:.3f}')\nlr_monitor = LearningRateMonitor(logging_interval='step')\ncallbacks = [lr_monitor]\nif not disable_ckpt:\n    callbacks.append(ckpt_callback)\n\n# Lightning Trainer\ntrainer = pl.Trainer.from_argparse_args(\n                    args=args,\n#                     plugins=DDPPlugin(find_unused_parameters=False,\n#                                       num_nodes=num_nodes,\n#                                       sync_batchnorm=False, #config.TRAINER.WORLD_SIZE > 0\n#                                      ),\n                    gradient_clip_val=config.TRAINER.GRADIENT_CLIPPING,\n                    callbacks=callbacks,\n                    logger=logger,\n#                     sync_batchnorm=False, #config.TRAINER.WORLD_SIZE > 0,\n                    replace_sampler_ddp=False,  # use custom sampler\n#                     reload_dataloaders_every_epoch=False,  # avoid repeated samples!\n                    weights_summary='full',\n                    profiler=profiler)\n","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-05-31T14:33:04.873048Z","iopub.execute_input":"2022-05-31T14:33:04.873961Z","iopub.status.idle":"2022-05-31T14:33:06.948841Z","shell.execute_reply.started":"2022-05-31T14:33:04.873906Z","shell.execute_reply":"2022-05-31T14:33:06.948125Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# BEFORE TRAINING (rely on pretraining weights)\n\nAs you can see, the pretrained model is pretty good with basic crops.\n\nHowever, even a basic 90° rotation make things wrong.","metadata":{}},{"cell_type":"code","source":"CONF_THRESH = 0.\nMAX_IMG = 5","metadata":{"execution":{"iopub.status.busy":"2022-05-31T14:33:11.129596Z","iopub.execute_input":"2022-05-31T14:33:11.130268Z","iopub.status.idle":"2022-05-31T14:33:11.136425Z","shell.execute_reply.started":"2022-05-31T14:33:11.130231Z","shell.execute_reply":"2022-05-31T14:33:11.135668Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Have a look at training images\ndataset = RotateSyntheticDataset(train_images, augment_fn=None, crop_range=CROP_RANGE)\nmatch_and_draw_dataset(matcher=model.matcher,\n                       dataset=dataset,\n                       conf_thresh=CONF_THRESH,\n                       max_img=2,\n                       rotate=True\n                      )","metadata":{"execution":{"iopub.status.busy":"2022-05-31T14:33:12.316281Z","iopub.execute_input":"2022-05-31T14:33:12.316895Z","iopub.status.idle":"2022-05-31T14:33:19.612513Z","shell.execute_reply.started":"2022-05-31T14:33:12.316863Z","shell.execute_reply":"2022-05-31T14:33:19.611202Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Have a look at validaiton images\n\npath_to_imgs = get_images_path(\"../input/image-matching-challenge-2022/train/notre_dame_front_facade/\")\ndataset = RotateSyntheticDataset(path_to_imgs, augment_fn=None, crop_range=CROP_RANGE)\nmatch_and_draw_dataset(matcher=model.matcher,\n                       dataset=dataset,\n                       conf_thresh=CONF_THRESH,\n                       max_img=MAX_IMG,\n                       rotate=True\n                      )","metadata":{"execution":{"iopub.status.busy":"2022-05-31T14:33:28.841625Z","iopub.execute_input":"2022-05-31T14:33:28.841893Z","iopub.status.idle":"2022-05-31T14:33:35.514675Z","shell.execute_reply.started":"2022-05-31T14:33:28.841863Z","shell.execute_reply":"2022-05-31T14:33:35.514027Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Now let's train for only a few epochs\n\nQuestion : Could adding rotation during training improve this behaviour ?","metadata":{}},{"cell_type":"code","source":"trainer.running_sanity_check = False\nloguru_logger.info(f\"Trainer initialized!\")\nloguru_logger.info(f\"Start training!\")\ntrainer.fit(model, datamodule=data_module)","metadata":{"execution":{"iopub.status.busy":"2022-05-31T14:33:52.233185Z","iopub.execute_input":"2022-05-31T14:33:52.233453Z","iopub.status.idle":"2022-05-31T14:33:58.73254Z","shell.execute_reply.started":"2022-05-31T14:33:52.233424Z","shell.execute_reply":"2022-05-31T14:33:58.730979Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Have a look at training images\ndataset = RotateSyntheticDataset(train_images, augment_fn=None, crop_range=CROP_RANGE)\nmatch_and_draw_dataset(matcher=model.matcher,\n                       dataset=dataset,\n                       conf_thresh=CONF_THRESH,\n                       max_img=10,\n                       rotate=True\n                      )","metadata":{"execution":{"iopub.status.busy":"2022-05-31T13:57:55.769501Z","iopub.status.idle":"2022-05-31T13:57:55.770445Z","shell.execute_reply.started":"2022-05-31T13:57:55.770182Z","shell.execute_reply":"2022-05-31T13:57:55.77021Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}