{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"import pandas as pd\nimport xgboost as xgb\nimport operator\nfrom matplotlib import pylab as plt\nimport numpy as np\nfrom sklearn import preprocessing\nfrom sklearn.cross_validation import train_test_split\n\ndef ceate_feature_map(features):\n    outfile = open('xgb.fmap', 'w')\n    i = 0\n    for feat in features:\n        outfile.write('{0}\\t{1}\\tq\\n'.format(i, feat))\n        i = i + 1\n\n    outfile.close()\n\nseed = 260681\n\ntrain = pd.read_csv('../input/train.csv')\ntest = pd.read_csv('../input/test.csv')\n\ny = train.QuoteConversion_Flag.values\ntrain = train.drop(['QuoteNumber', 'QuoteConversion_Flag'], axis=1)\ntest = test.drop('QuoteNumber', axis=1)\n\n# Lets play with some dates\ntrain['Date'] = pd.to_datetime(pd.Series(train['Original_Quote_Date']))\ntrain = train.drop('Original_Quote_Date', axis=1)\n\ntest['Date'] = pd.to_datetime(pd.Series(test['Original_Quote_Date']))\ntest = test.drop('Original_Quote_Date', axis=1)\n\ntrain['Year'] = train['Date'].apply(lambda x: int(str(x)[:4]))\ntrain['Month'] = train['Date'].apply(lambda x: int(str(x)[5:7]))\ntrain['weekday'] = train['Date'].dt.dayofweek\n\n\ntest['Year'] = test['Date'].apply(lambda x: int(str(x)[:4]))\ntest['Month'] = test['Date'].apply(lambda x: int(str(x)[5:7]))\ntest['weekday'] = test['Date'].dt.dayofweek\n\ntrain = train.drop('Date', axis=1)\ntest = test.drop('Date', axis=1)\n\ntrain = train.fillna(-1)\ntest = test.fillna(-1)\n\n\n"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"features = list(train.columns)\nceate_feature_map(features)\n\nfor f in train.columns:\n    if train[f].dtype=='object':\n        print(f)\n        lbl = preprocessing.LabelEncoder()\n        lbl.fit(list(train[f].values) + list(test[f].values))\n        train[f] = lbl.transform(list(train[f].values))\n        test[f] = lbl.transform(list(test[f].values))\n\nclf = xgb.XGBClassifier(n_estimators=25,\n                        nthread=-1,\n                        max_depth=10,\n                        learning_rate=0.025,\n                        silent=True,\n                        subsample=0.8,\n                        colsample_bytree=0.8)\n\n\n\nxgb_model = clf.fit(train, y, eval_metric=\"auc\")\npreds = clf.predict_proba(test)[:,1]\nsample = pd.read_csv('../input/sample_submission.csv')\nsample.QuoteConversion_Flag = preds\nsample.to_csv('xgb_benchmark.csv', index=False)\n"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"\nimportance = xgb.get_fscore(fmap='xgb.fmap')\nimportance = sorted(importance.items(), key=operator.itemgetter(1))\n\ndf = pd.DataFrame(importance, columns=['feature', 'fscore'])\ndf['fscore'] = df['fscore'] / df['fscore'].sum()\n\n\nplt.figure()\ndf.plot()\ndf.plot(kind='barh', x='feature', y='fscore', legend=False, figsize=(6, 10))\nplt.title('XGBoost Feature Importance')\nplt.xlabel('relative importance')\nplt.gcf().savefig('feature_importance_xgb.png')\n\n"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":0}