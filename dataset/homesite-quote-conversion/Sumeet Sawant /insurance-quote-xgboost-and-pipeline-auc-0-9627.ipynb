{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Home Site Quite Conversion Challenge \n\nBefore asking someone on a date or skydiving, it's important to know your likelihood of success. The same goes for quoting home insurance prices to a potential customer. Homesite, a leading provider of homeowners insurance, does not currently have a dynamic conversion rate model that can give them confidence a quoted price will lead to a purchase. \n\nUsing an anonymized database of information on customer and sales activity, including property and coverage information, Homesite is challenging you to predict which customers will purchase a given quote. Accurately predicting conversion would help Homesite better understand the impact of proposed pricing changes and maintain an ideal portfolio of customer segments. \n\n## Main Challenges \n\nThis dataset was huge ~260K rows( aka samples) and 298 (features) and to add to that challenge the data was anonymized so \ndoing feature engineering would be very random and usually brute force . I though of handeling this via feature selection and boosting methodology \n\n__I implemented two feature selection stratergies__ \n\n- __Mutual information:__\nMutual information (MI) between two random variables is a non-negative value, which measures the dependency between the variables. It is equal to zero if and only if two random variables are independent, and higher values mean higher dependency.\n\n- __Reculsive Feature Elimination:__\nGiven an external estimator that assigns weights to features (e.g., the coefficients of a linear model), the goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through any specific attribute or callable. Then, the least important features are pruned from current set of features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.\n\nAfter inspecting and performing EDA on the selected features I decided to treat all featues as catergorical. \n\nOnce I have the feature selected to 50 from 298 I triend two model one simple __Logistic regression__ with one-hot encoding and other __LightGBM__ . With logistic regression I Was able to get the ROC-AUC score to 0.95 but the model took a long time to train due to large number of one-hot encoding \n\nI hyper-parameter tuned two Light GBM model with __Optuna__. Optuna is a hyperparameter framework . One feature which I like about it is that it allows us to stop the run for un-promising combination of values . This allows us to run hyper-parameter search for a larger grid.  \n\nFirst model was trained on features obtained using mutual information which gave the ROC-AUC score as 0.93 and the second model was trained with features obtained from RFE which gave me a ROC-AUC score of 0.96+  For the final private test submission I was able to get a score of 0.9627 on the private leader board. \n\nFinally I used Sklearn Pipeline to optimize the prediction workflow for the test set. This allowed me to skip storing all the feature encoding values for 50 feature columns. \n\n## Key Learning \n\n- Feature Selection Techniques \n- Sklearn Pipeline \n\n## Part1 Notebook \nI will also link to this notebook my work where I optimized and did some EDA on the dataset \n\n\n## Upvote if you like the work \nLinkedIn: https://www.linkedin.com/in/sawantsumeet/","metadata":{}},{"cell_type":"markdown","source":"## Import Library ","metadata":{}},{"cell_type":"code","source":"import pandas as pd \nimport numpy as np \nimport gc\nimport lightgbm as gbm\nfrom sklearn.pipeline import Pipeline \nfrom sklearn.preprocessing import LabelEncoder,OrdinalEncoder\nfrom sklearn import model_selection,metrics \nimport warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## File Loading ","metadata":{}},{"cell_type":"code","source":"## Import the data \nmutual_columns=pd.read_csv('../input/insurancefeatures-homesite/mutual_info_features.csv').columns\nRFE_columns=pd.read_csv('../input/insurancefeatures-homesite/RFE_features.csv').columns\n\ndf=pd.read_csv('../input/insurancefeatures-homesite/train.csv')\n\ndf_mutual=df[mutual_columns]\ndf_RFE=df[RFE_columns]\n\n## Dropping few columns from df_mutual as they have to many catergories as we are going to model the annomymus feature \n## columns as purely catergorical \n\ndf_RFE.drop(columns=['Original_Quote_Date','SalesField8'],axis=1,inplace=True)\n\n# Delete not necessary items \ndel df\ngc.collect()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Declaring the model parameters tuned using Optuna ","metadata":{}},{"cell_type":"code","source":"# Parameters for the two light GBM model used . This parameters where obtained by Hyperparameter Optimization using Optuna\n\n\n## Parameters for Light GBM using Reculsive Feature Elimination \nRFE_params={ \n    'boosting_type': 'gbdt',\n    'lambda_l1': 4.540006226304331e-08,\n    'lambda_l2': 4.715716309514142,\n    'num_leaves': 105,\n    'feature_fraction': 0.89,\n    'bagging_fraction': 1,\n    'bagging_freq': 4,\n    'min_child_samples': 65,\n    'max_bin': 20,\n    'learning_rate': 0.14, }\n\n### Parameters for Light GBM using Mutual info \nmutual_info_params={'boosting_type': 'gbdt',\n    'lambda_l1': 4.956734949314487e-08,\n    'lambda_l2': 2.278541145546624e-08,\n    'num_leaves': 131,\n    'feature_fraction': 0.6,\n    'bagging_fraction': 0.76,\n    'bagging_freq': 2,\n    'min_child_samples': 21,\n    'max_bin': 18,\n    'learning_rate': 0.15}\n\n## Intialize the models\nRFE_gbm=gbm.LGBMClassifier(**RFE_params)\nmutual_gbm=gbm.LGBMClassifier(**mutual_info_params)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Sklearn Pipline to train the model ","metadata":{}},{"cell_type":"code","source":"### Pipeline Implementation \n\n## model 1\n\nX_train,X_val,y_train,y_val=model_selection.train_test_split(df_mutual.drop('QuoteConversion_Flag',axis=1),\n                                                             df_mutual['QuoteConversion_Flag'],random_state=42,\n                                                            stratify=df_mutual['QuoteConversion_Flag'])\n\nGBM1=Pipeline([\n                ('label_encoder',OrdinalEncoder(handle_unknown='use_encoded_value',unknown_value=-99)),\n                 ('mutual_gbm',mutual_gbm)                \n            ])\n\nGBM1.fit(X_train,y_train)\ny_predict_mutual=GBM1.predict_proba(X_val)\n\n\n## Model 2 \n\nX_train,X_val,y_train,y_val=model_selection.train_test_split(df_RFE.drop('QuoteConversion_Flag',axis=1),\n                                                             df_RFE['QuoteConversion_Flag'],random_state=42,\n                                                            stratify=df_RFE['QuoteConversion_Flag'])\n\nGBM2=Pipeline([\n                ('label_encoder',OrdinalEncoder(handle_unknown='use_encoded_value',unknown_value=-99)),\n                ('RFE_gbm',RFE_gbm)                \n            ])\n\n\nGBM2.fit(X_train,y_train)\ny_predict_RFE=GBM2.predict_proba(X_val)\n\n\n\n\n## Taking the average of both predictions \ny_avg=(y_predict_mutual[:,1]+y_predict_RFE[:,1])/2\n\n","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Validation AUC \n\nprint(\"AUC score for the Light GBM ensemble is:{:.2f}\".format(metrics.roc_auc_score(y_val,y_avg)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Test Set Submission ","metadata":{}},{"cell_type":"code","source":"## Test Submission \n\n## Extract the test set \nimport zipfile\nwith zipfile.ZipFile('/kaggle/input/homesite-quote-conversion/test.csv.zip', 'r') as zip_ref:\n    zip_ref.extractall('./')\n    \n## Extract the submission file \n\nwith zipfile.ZipFile('../input/homesite-quote-conversion/sample_submission.csv.zip', 'r') as zip_ref:\n    zip_ref.extractall('./')\n    \n #Load the test set    \ndf_test=pd.read_csv('./test.csv')\n\n## just take the required columns requied for predicting on it \n\nRFE_columns=[col for col in RFE_columns if col not in 'QuoteConversion_Flag'] #  Test wont have the label column \ndf_test=df_test[RFE_columns]\ndf_test.drop(columns=['Original_Quote_Date','SalesField8'],axis=1,inplace=True)\n\n#Predict on it using the GBM2 pipeline . Here pipe line has made a task easy as we do not have to store features \ny_test= GBM2.predict_proba(df_test)\n\n# Store the values obtained on the test set into the submission file \ndf_submission=pd.read_csv('./sample_submission.csv')\n\ndf_submission['QuoteConversion_Flag']=y_test[:,1]\n\ndf_submission.to_csv('./LightGBM_RFE_Features.csv',index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}