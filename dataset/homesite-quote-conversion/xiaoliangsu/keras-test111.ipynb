{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"from keras.regularizers import l2, activity_l2\nimport numpy as np\nimport pandas as pd\nfrom keras.models import Sequential\nfrom keras.layers.core import Dense, Dropout, Activation\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.layers.advanced_activations import PReLU\nfrom keras.utils import np_utils, generic_utils\nfrom sklearn.cross_validation import train_test_split\nfrom sklearn.metrics import log_loss, auc, roc_auc_score\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt\nfrom keras.optimizers import Adagrad,SGD,Adadelta\nfrom keras.callbacks import EarlyStopping\nfrom keras.layers import containers\nfrom keras.layers.core import Dense, AutoEncoder\nfrom keras.constraints import maxnorm\n\nnp.random.seed(1778)  # for reproducibility\nneed_normalise=True\nneed_validataion=True\nneed_categorical=False\nsave_categorical_file=False\nnb_epoch=1#400\ngolden_feature=[(\"CoverageField1B\",\"PropertyField21B\"),\n                (\"GeographicField6A\",\"GeographicField8A\"),\n                (\"GeographicField6A\",\"GeographicField13A\"),\n                (\"GeographicField8A\",\"GeographicField13A\"),\n                (\"GeographicField11A\",\"GeographicField13A\"),\n                (\"GeographicField8A\",\"GeographicField11A\")]\n\ndef save2model(submission,file_name,y_pre):\n    assert len(y_pre)==len(submission)\n    submission['QuoteConversion_Flag']=y_pre\n    submission.to_csv(file_name,index=False)\n    print (\"saved files %s\" % file_name)\n\ndef getDummy(df,col):\n    category_values=df[col].unique()\n    data=[[0 for i in range(len(category_values))] for i in range(len(df))]\n    dic_category=dict()\n    for i,val in enumerate(list(category_values)):\n        dic_category[str(val)]=i\n   # print dic_category\n    for i in range(len(df)):\n        data[i][dic_category[str(df[col][i])]]=1\n\n    data=np.array(data)\n    for i,val in enumerate(list(category_values)):\n        df.loc[:,\"_\".join([col,str(val)])]=data[:,i]\n\n    return df\n\n\ndef generateFileName(model,params):\n     file_name=\"_\".join([(key+\"_\"+ str(val))for key,val in params.items()])\n     return model+\"_\"+file_name+\".csv\"\n\ndef load_data():\n    train=pd.read_csv('train.csv')\n    test=pd.read_csv('test.csv')\n    train = train.drop(['QuoteNumber','PropertyField6', 'GeographicField10A'], axis=1)\n    \n    submission=pd.DataFrame()\n    submission[\"QuoteNumber\"]= test[\"QuoteNumber\"]\n\n    train_y=train['QuoteConversion_Flag'].values\n    train=train.drop('QuoteConversion_Flag',axis=1)\n    \n    test = test.drop(['QuoteNumber','PropertyField6', 'GeographicField10A'],axis=1)\n    train['Date'] = pd.to_datetime(pd.Series(train['Original_Quote_Date']))\n    train = train.drop('Original_Quote_Date', axis=1)\n    train['Year'] = train['Date'].apply(lambda x: int(str(x)[:4]))\n    train['Month'] = train['Date'].apply(lambda x: int(str(x)[5:7]))\n    train['weekday'] = [train['Date'][i].dayofweek for i in range(len(train['Date']))]\n    \n    test['Date'] = pd.to_datetime(pd.Series(test['Original_Quote_Date']))\n    test = test.drop('Original_Quote_Date', axis=1)\n    test['Year'] = test['Date'].apply(lambda x: int(str(x)[:4]))\n    test['Month'] = test['Date'].apply(lambda x: int(str(x)[5:7]))\n    test['weekday'] = [test['Date'][i].dayofweek for i in range(len(test['Date']))]\n    \n    train = train.drop('Date', axis=1)\n    test = test.drop('Date', axis=1)\n    \n    #fill na, have no effect on need_categorical method\n    train = train.fillna(-1)\n    test = test.fillna(-1)\n    \n    for f in test.columns:#\n        if train[f].dtype=='object':\n            lbl = LabelEncoder()\n            lbl.fit(list(train[f])+list(test[f]))\n            train[f] = lbl.transform(list(train[f].values))\n            test[f] = lbl.transform(list(test[f].values))\n\n    #try to encode all params less than 100 to be category\n    if need_categorical:\n        #row bind train and test\n        x=train.append(test,ignore_index=True)\n        del train\n        del test\n        for f in x.columns:#\n            category_values= set(list(x[f].unique()))\n            if len(category_values)<4:\n                print (f)\n                x=getDummy(x,f)\n                #x.drop(f,axis=1)\n                #all_data.drop(f,axis=1)\n        test = x.iloc[260753:,]\n        train = x.iloc[:260753:,]\n\n    #save need_categorical:\n#    if save_categorical_file and need_categorical:\n#        train[\"QuoteConversion_Flag\"]=train_y\n#        train[\"QuoteNumber\"]=1\n#        test[\"QuoteNumber\"]=submission[\"QuoteNumber\"]\n#        train.to_csv(\"./data/train_category.csv\",index=False)\n#        test.to_csv(\"./data/test_category.csv\",index=False)\n\n    #add golden feature:\n\n    encoder = LabelEncoder()\n    train_y = encoder.fit_transform(train_y).astype(np.int32)\n    train_y = np_utils.to_categorical(train_y)\n    #for featureA,featureB in golden_feature:\n    #    train.loc[:,\"_\".join([featureA,featureB,\"diff\"])]=train[featureA]-train[featureB]\n    #    test.loc[:,\"_\".join([featureA,featureB,\"diff\"])]=test[featureA]-test[featureB]        \n\n    print (\"processsing finished\")\n    valid=None\n    valid_y=None\n    train = np.array(train)\n    train = train.astype(np.float32)\n    test=np.array(test)\n    test=test.astype(np.float32)\n    if need_normalise:\n        scaler = StandardScaler().fit(train)\n        train = scaler.transform(train)\n        test = scaler.transform(test)\n    \n    if need_validataion:\n        train,valid,train_y,valid_y=train_test_split(train,train_y,test_size=20000,random_state=218)\n    return [(train,train_y),(test,submission),(valid,valid_y)]\n\nprint('Loading data...')\n\n#datasets=load_data()\n\ndatasets=load_data()\n\nX_train, y_train = datasets[0]\nX_test, submission = datasets[1]\nX_valid, y_valid = datasets[2]\n\nnb_classes = y_train.shape[1]\nprint(nb_classes, 'classes')\n\ndims = X_train.shape[1]\nprint(dims, 'dims')\n\nmodel = Sequential()\n\nmodel.add(Dense(1024, input_shape=(dims,)))\nmodel.add(Dropout(0.1))#    input dropout\nmodel.add(PReLU())\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.5))\n\nmodel.add(Dense(360))\nmodel.add(PReLU())\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.5))\n\nmodel.add(Dense(420))\nmodel.add(PReLU())\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.5))\n\nmodel.add(Dense(nb_classes))\nmodel.add(Activation('softmax'))\n#opt=SGD(momentum=0.9)\nmodel.compile(loss='binary_crossentropy', optimizer=\"sgd\")\nauc_scores=[]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           \nbest_score=-1\nbest_model=None\nthreshold=0.9638\nprint('Training model...')\nif need_validataion:\n    for i in range(nb_epoch):\n    #early_stopping=EarlyStopping(monitor='val_loss', patience=0, verbose=1)\n    #model.fit(X_train, y_train, nb_epoch=nb_epoch,batch_size=256,validation_split=0.01,callbacks=[early_stopping])\n        print (\"best_score is:\",best_score)\n        model.fit(X_train, y_train, nb_epoch=1,batch_size=256)\n        y_pre = model.predict_proba(X_valid)\n        scores = roc_auc_score(y_valid,y_pre)\n        auc_scores.append(scores)\n        print (i,scores)\n        if scores>best_score:\n            best_score=scores\n            best_model=model\n            if best_score>threshold:\n                y_pre = model.predict_proba(X_test)[:,1]\n                save2model(submission,'keras_nn_test_'+str(best_score)+\".csv\", y_pre)\n    plt.plot(auc_scores)\n    plt.show()\nelse:\n    model.fit(X_train, y_train, nb_epoch=nb_epoch, batch_size=256)\n\nif need_validataion:\n    model=best_model#print('Generating submission...')\n\ny_pre = model.predict_proba(X_test)[:,1]\n#print roc_auc_score(y_test,y_pre)\nsave2model(submission, 'keras_nn_test.csv',y_pre)\n"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":0}