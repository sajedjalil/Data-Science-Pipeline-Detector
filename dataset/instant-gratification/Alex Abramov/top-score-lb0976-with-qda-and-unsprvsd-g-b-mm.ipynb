{"cells":[{"metadata":{},"cell_type":"markdown","source":"### IDEA\n    If you know about make clf module from sklearn. You see that make clf returns Gaussians.\n    This fact can help you to choose very good unsupervised algo to improve your score -> Gaussian and Bayessian Mixture Model. \n    GMM and BMM give us probability of belonging n clusters with gaussian distribution."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def permute_predict(y):\n    _y = y.copy()\n    _c1 = _y < 0.00001\n    _c2 = _y > 0.99999\n    _y[_c1] = _y[_c1].max() - _y[_c1] + _y[_c1].min()\n    _y[_c2] = _y[_c2].max() - _y[_c2] + _y[_c2].min()\n    return _y","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis, LinearDiscriminantAnalysis\nfrom sklearn.feature_selection import VarianceThreshold\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.mixture import GaussianMixture as GMM\nfrom sklearn.mixture import BayesianGaussianMixture as BGM\nfrom tqdm import tqdm_notebook\nfrom sklearn.covariance import GraphicalLasso\n\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/train.csv')\ntest = pd.read_csv('../input/test.csv')\n\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cols = [c for c in train.columns if c not in ['id', 'target']]\ncols.remove('wheezy-copper-turtle-magic')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"oof = np.zeros(len(train))\npreds = np.zeros(len(test))\n\nfor i in tqdm_notebook(range(512)):\n    train2 = train[train['wheezy-copper-turtle-magic']==i]\n    test2 = test[test['wheezy-copper-turtle-magic']==i]\n    idx1 = train2.index; idx2 = test2.index\n    train2.reset_index(drop=True,inplace=True)\n    \n    data = pd.concat([pd.DataFrame(train2[cols]), pd.DataFrame(test2[cols])])\n    data2 = VarianceThreshold(2.3).fit_transform(data[cols])\n    train3 = data2[:train2.shape[0]]; test3 = data2[train2.shape[0]:]\n    \n    for c in range(train3.shape[1]):\n        low_=np.quantile(train3[:,c] , 0.001)\n        up_=np.quantile(train3[:,c], 0.999)\n        train3[:,c]=np.clip(train3[:,c],low_, up_ )\n        test3[:,c]=np.clip(test3[:,c],low_, up_ )\n        \n#     train3 = ((train3) / data[:train2.shape[0]].std(axis=1)[:, np.newaxis])\n#     test3 = ((test3) / data[train2.shape[0]:].std(axis=1)[:, np.newaxis])\n    \n    skf = StratifiedKFold(n_splits=11, random_state=42)\n    for train_index, test_index in skf.split(train2, train2['target']):\n        gmm=GMM(n_components=5, random_state=42, covariance_type='full')\n        gmm.fit(np.vstack([train3[train_index], test3]))\n        gmm_1_train=gmm.predict_proba(train3[train_index])\n        gmm_1_val=gmm.predict_proba(train3[test_index])\n        gmm_1_test=gmm.predict_proba(test3)\n\n        gmm=GMM(n_components=4, random_state=42, covariance_type='full')\n        gmm.fit(np.vstack([train3[train_index], test3]))\n        gmm_2_train=gmm.predict_proba(train3[train_index])\n        gmm_2_val=gmm.predict_proba(train3[test_index])\n        gmm_2_test=gmm.predict_proba(test3)\n\n        gmm=GMM(n_components=6, random_state=42, covariance_type='full')\n        gmm.fit(np.vstack([train3[train_index], test3]))\n        gmm_3_train=gmm.predict_proba(train3[train_index])\n        gmm_3_val=gmm.predict_proba(train3[test_index])\n        gmm_3_test=gmm.predict_proba(test3)\n\n\n\n        bgm=BGM(n_components=5, random_state=42)\n        bgm.fit(np.vstack([train3[train_index], test3]))\n        bgm_1_train=bgm.predict_proba(train3[train_index])\n        bgm_1_val=bgm.predict_proba(train3[test_index])\n        bgm_1_test=bgm.predict_proba(test3)\n\n        bgm=BGM(n_components=4, random_state=42)\n        bgm.fit(np.vstack([train3[train_index], test3]))\n        bgm_2_train=bgm.predict_proba(train3[train_index])\n        bgm_2_val=bgm.predict_proba(train3[test_index])\n        bgm_2_test=bgm.predict_proba(test3)\n\n        bgm=BGM(n_components=6, random_state=42)\n        bgm.fit(np.vstack([train3[train_index], test3]))\n        bgm_3_train=bgm.predict_proba(train3[train_index])\n        bgm_3_val=bgm.predict_proba(train3[test_index])\n        bgm_3_test=bgm.predict_proba(test3)\n    \n        _train = np.hstack((train3[train_index],\n                            gmm_1_train, gmm_2_train, gmm_3_train,\n                            bgm_1_train, bgm_2_train, bgm_3_train))\n        _val = np.hstack((train3[test_index],\n                            gmm_1_val, gmm_2_val, gmm_3_val,\n                            bgm_1_val, bgm_2_val, bgm_3_val))\n        _test = np.hstack((test3,\n                            gmm_1_test, gmm_2_test, gmm_3_test,\n                            bgm_1_test, bgm_2_test, bgm_3_test))\n        clf = QuadraticDiscriminantAnalysis(reg_param=0.04, tol=0.01) #0.04 bst - 0.9734+\n        clf.fit(_train,train2.loc[train_index]['target'])\n        \n        oof[idx1[test_index]] = clf.predict_proba(_val)[:,1]\n        preds[idx2] += clf.predict_proba(_test)[:,1] / skf.n_splits\n    print(i, roc_auc_score(train2['target'], oof[idx1]))\nprint(roc_auc_score(train['target'], oof))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub = pd.read_csv('../input/sample_submission.csv')\nsub['target'] = permute_predict(preds)\nsub.to_csv('submission.csv',index=False)\n\nimport matplotlib.pyplot as plt\nplt.hist(preds,bins=100)\nplt.title('Final Test.csv predictions')\nplt.show()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}