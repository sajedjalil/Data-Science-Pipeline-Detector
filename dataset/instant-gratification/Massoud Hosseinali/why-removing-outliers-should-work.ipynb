{"cells":[{"metadata":{},"cell_type":"markdown","source":"I am going to describe picturially why removing outliers should work on this dataset. \n\nFirst I generate a 2-dimensional classification dataset and plot 1, 2, 3 standard deviation of their gaussian distributions based on means and covariance of each class (positive and negative). "},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn import metrics, preprocessing\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.svm import NuSVC\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import StratifiedKFold\nfrom tqdm import tqdm_notebook as tqdm\nimport matplotlib.pyplot as plt\nfrom matplotlib.patches import Ellipse\nfrom sklearn import svm, neighbors\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.feature_selection import VarianceThreshold\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\nimport seaborn as sns\nfrom sklearn.datasets import make_classification\nfrom sklearn.covariance import EllipticEnvelope","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"np.random.seed(2019)\nN_points = 512\nflip_y = 0.025\n\n\ndef plot_ellipses(inpX):\n    cov = np.cov(inpX[:,0],inpX[:, 1])\n\n    for nstd in [1, 2, 3]:\n        vals, vecs = eigsorted(cov)\n        theta = np.degrees(np.arctan2(*vecs[:,0][::-1]))\n        w, h = 2 * nstd * np.sqrt(vals)\n        ell = Ellipse(xy=(np.mean(inpX[:, 0]), np.mean(inpX[:, 1])),\n                      width=w, height=h,\n                      angle=theta, color='black', alpha=1/nstd)\n        ell.set_facecolor('none')\n        ax.add_artist(ell)\n\ndef eigsorted(cov):\n    vals, vecs = np.linalg.eigh(cov)\n    order = vals.argsort()[::-1]\n    return vals[order], vecs[:,order]\n\nX = np.random.randn(N_points, 2)\nX_pos = X[:N_points//2].dot(np.array([[.6, -.6],\n                                        [-.15, 1.5]]))  + np.array([1.5, 1.5])\n\nX_neg = X[N_points//2:] #- np.array([1, 1])\ny_pos = np.ones(len(X_pos))\ny_neg = np.zeros(len(X_neg))\n\nplt.figure(figsize=(10,10))\nax = plt.subplot(111)\n\nax.scatter(X_pos[:, 0], X_pos[:, 1], c='red', label='positive')\nax.scatter(X_neg[:, 0], X_neg[:, 1], c='blue', label='negative')\n\n\nplot_ellipses(X_pos)\nplot_ellipses(X_neg)\nplt.xlim([-4, 4])\nplt.ylim([-3, 6])\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"These ellipses are more or less the true distributions we are challenged to find in this competition. Note that each chunk of our dataset have 33 to 47 dimensions so we have hyper ellipses.\n\nNow, I generate the same thing but this time I flip some of the targets (for the sake of illustration I only flipped some of the negatives to positive and not vice versa). This is similar to what `sklearn.make_classification` does by assigning random classes to some points."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"np.random.seed(2019)\n\nX = np.random.randn(N_points, 2)\nX_pos = X[:N_points//2].dot(np.array([[.6, -.6],\n                                        [-.15, 1.5]]))  + np.array([1.5, 1.5])\n\nX_neg = X[N_points//2:] #- np.array([1, 1])\ny_pos = np.ones(len(X_pos))\ny_neg = np.zeros(len(X_neg))\n\nmask_id = np.random.randint(0,N_points//2+1,int(flip_y*N_points))\nX_pos_holdout = X_pos[mask_id, :].copy()\nX_pos[mask_id, :] = X_neg[mask_id, :]\ny_pos[mask_id] = 0\nX_neg[mask_id, :] = X_pos_holdout\nX_neg[mask_id] = 1\n\nX = np.vstack((X_pos, X_neg))\ny = np.concatenate((y_pos, y_neg))\n\n\nplt.figure(figsize=(10,10))\nax = plt.subplot(111)\n\nax.scatter(X_pos[:, 0], X_pos[:, 1], c='red', label='positive')\nax.scatter(X_neg[:, 0], X_neg[:, 1], c='blue', label='negative')\n\n\nplot_ellipses(X_pos)\nplot_ellipses(X_neg)\nplt.xlim([-4, 4])\nplt.ylim([-3, 6])\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Notice how our best fit gaussian distribution is different than the true distribution shown above. \n\nLooking at these plots, I believe finding outliers and removing them should help us find parameters of gaussian distributions better and improve our score.\n\nHow can we detect outliers? Watch https://www.coursera.org/lecture/machine-learning/anomaly-detection-using-the-multivariate-gaussian-distribution-DnNr9\n\nConsidering that we know our classes are from multivariate gaussian distributions, and by looking at our plots above one way would be to remove anything that falls outside our 3 standard deviation ellipse. \n"},{"metadata":{"trusted":true},"cell_type":"code","source":"\nnp.random.seed(2019)\nN_points = 512\nflip_y = 0.025\n\nX = np.random.randn(N_points, 2)\nX_pos = X[:N_points//2].dot(np.array([[.6, -.6],\n                                        [-.15, 1.5]]))  + np.array([1.5, 1.5])\n\nX_neg = X[N_points//2:] #- np.array([1, 1])\ny_pos = np.ones(len(X_pos))\ny_neg = np.zeros(len(X_neg))\n\nmask_id = np.random.randint(0,N_points//2+1,int(flip_y*N_points))\nX_pos_holdout = X_pos[mask_id, :].copy()\nX_pos[mask_id, :] = X_neg[mask_id, :]\ny_pos[mask_id] = 0\nX_neg[mask_id, :] = X_pos_holdout\nX_neg[mask_id] = 1\n\nX = np.vstack((X_pos, X_neg))\ny = np.concatenate((y_pos, y_neg))\n\n\nplt.figure(figsize=(10,10))\nax = plt.subplot(111)\n\nax.scatter(X_pos[:, 0], X_pos[:, 1], c='red', label='positive')\nax.scatter(X_neg[:, 0], X_neg[:, 1], c='blue', label='negative')\n\ndef plot_ellipses(inpX):\n    cov = np.cov(inpX[:,0],inpX[:, 1])\n\n    for nstd in [1, 2, 3]:\n        vals, vecs = eigsorted(cov)\n        theta = np.degrees(np.arctan2(*vecs[:,0][::-1]))\n        w, h = 2 * nstd * np.sqrt(vals)\n        ell = Ellipse(xy=(np.mean(inpX[:, 0]), np.mean(inpX[:, 1])),\n                      width=w, height=h,\n                      angle=theta, color='black')\n        ell.set_facecolor('none')\n        ax.add_artist(ell)\n        \nEE = EllipticEnvelope(contamination=0.016) # notice that I did it manually to find only those that fall outside 3 std. I have a lot of kernels and couldn't find the code I was using for this method :D so please accept this for now. \nEE.fit(X_pos)\npos_ids = (EE.predict(X_pos) == -1)\n\nplt.scatter(X_pos[pos_ids, 0], X_pos[pos_ids, 1], color='yellow', label='outliers')\n\nplot_ellipses(X_pos)\nplot_ellipses(X_neg)\nplt.xlim([-4, 4])\nplt.ylim([-3, 6])\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Another way is to use `sklearn.covariance.EllipticEnvelope()` and use `contamination==0.025` argument which is \"The amount of contamination of the data set, i.e. the proportion of outliers in the data set.\""},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"\nnp.random.seed(2019)\nN_points = 512\nflip_y = 0.025\n\nX = np.random.randn(N_points, 2)\nX_pos = X[:N_points//2].dot(np.array([[.6, -.6],\n                                        [-.15, 1.5]]))  + np.array([1.5, 1.5])\n\nX_neg = X[N_points//2:] #- np.array([1, 1])\ny_pos = np.ones(len(X_pos))\ny_neg = np.zeros(len(X_neg))\n\nmask_id = np.random.randint(0,N_points//2+1,int(flip_y*N_points))\nX_pos_holdout = X_pos[mask_id, :].copy()\nX_pos[mask_id, :] = X_neg[mask_id, :]\ny_pos[mask_id] = 0\nX_neg[mask_id, :] = X_pos_holdout\nX_neg[mask_id] = 1\n\nX = np.vstack((X_pos, X_neg))\ny = np.concatenate((y_pos, y_neg))\n\n\nplt.figure(figsize=(10,10))\nax = plt.subplot(111)\n\nax.scatter(X_pos[:, 0], X_pos[:, 1], c='red', label='positive')\nax.scatter(X_neg[:, 0], X_neg[:, 1], c='blue', label='negative')\n\ndef plot_ellipses(inpX):\n    cov = np.cov(inpX[:,0],inpX[:, 1])\n\n    for nstd in [1, 2, 3]:\n        vals, vecs = eigsorted(cov)\n        theta = np.degrees(np.arctan2(*vecs[:,0][::-1]))\n        w, h = 2 * nstd * np.sqrt(vals)\n        ell = Ellipse(xy=(np.mean(inpX[:, 0]), np.mean(inpX[:, 1])),\n                      width=w, height=h,\n                      angle=theta, color='black')\n        ell.set_facecolor('none')\n        ax.add_artist(ell)\n        \nEE = EllipticEnvelope(contamination=0.025)\nEE.fit(X_pos)\npos_ids = (EE.predict(X_pos) == -1)\n\nplt.scatter(X_pos[pos_ids, 0], X_pos[pos_ids, 1], color='yellow', label='outliers')\n\nplot_ellipses(X_pos)\nplot_ellipses(X_neg)\nplt.xlim([-4, 4])\nplt.ylim([-3, 6])\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### At the end of the day, removing outliers didn't work for me. I am still wondering why?\n\n### I appreciate your ideas. Please share."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}