{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"markdown","source":"# Does Private Dataset have same structure as Train Dataset?\n# Answer: YES\nIn normal Kaggle competitions, the `test.csv` that we download contains the private test dataset and the above question is easy to verify. In this competition, \"Instant Gratification\", the `test.csv` that we download does not contain the private test dataset. Can we still answer the question above? Yes.\n\nThe unique structure of the data has been explained [here][1]. (There are 512 partial datasets within the full dataset. And each partial dataset has a different set of approximately 40 important features as identified by different standard deviations.) We have observed that both the training dataset and public test dataset have this structure. Does the private dataset also have this structure? In this kernel, we probe the private dataset and confirm that it has the same structure.\n\nThe structure of the data encourages building 512 separate models. In this kernel we also show how to build a single model instead of 512. We will build a high scoring NN (LB 0.930) by using the wonderful starter code provided by Abhishek [here][2] (please upvote Abishek's kernel) and improved upon by Vladislav [here][3]. This is accomplished by removing the dataset's useless datablocks and converting the magic variable into categorical.\n\nWe will probe the Private LB and ask \"Does the private test dataset have the same special structure as training and public test?\". If the answer is \"yes\", this kernel will submit an ensemble of NN and SVC and score LB 0.950. If the answer is \"no\", this kernel will submit all zeros and score LB 0.500. To see the answer, view this kernel's LB score above.\n\n[1]: https://www.kaggle.com/c/instant-gratification/discussion/92930\n[2]: https://www.kaggle.com/abhishek/neural-network-with-embedding-layer\n[3]: https://www.kaggle.com/speedwagon/neural-network-baseline"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":false},"cell_type":"markdown","source":"# Load Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np, pandas as pd, os, gc\n\ntrain = pd.read_csv('../input/train.csv')\ntest = pd.read_csv('../input/test.csv')\n\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Find the useful data blocks\nAs described [here][1], the data appears to be `131072 = 512 * 256` blocks of data where some are **useful** and some are **useless**.\n\n[1]: https://www.kaggle.com/c/instant-gratification/discussion/92930"},{"metadata":{"trusted":true},"cell_type":"code","source":"# FIND STANDARD DEVIATION OF ALL 512*256 BLOCKS\nuseful = np.zeros((256,512))\nfor i in range(512):\n    partial = train[ train['wheezy-copper-turtle-magic']==i ]\n    useful[:,i] = np.std(partial.iloc[:,1:-1], axis=0)\n# CONVERT TO BOOLEANS IDENTIFYING USEFULNESS\nuseful = useful > 1.5","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# PLOT BOOLEANS OF USEFUL BLOCKS\nplt.figure(figsize=(10,20))\nplt.matshow(useful.transpose(),fignum=1)\nplt.title('The useful datablocks of dataset', fontsize=24)\nplt.xlabel('256 Variable columns', fontsize=16)\nplt.ylabel('512 Partial datasets', fontsize=16)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Remove useless datablocks\nWe will set all **useless** blocks of data to zero. There's probably a more efficient Pythonic way to do the follow..."},{"metadata":{"trusted":true},"cell_type":"code","source":"# SET MAGIC COLUMN AS ALL USEFUL\nuseful[146,:] = [True]*512\n\n# REMOVE ALL USELESS BLOCKS\nfor i in range(512):\n    idx = train.columns[1:-1][ ~useful[:,i] ]    \n    train.loc[ train.iloc[:,147]==i,idx ] = 0.0\n    test.loc[ test.iloc[:,147]==i,idx ] = 0.0 \n    #if i%25==0: print(i)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Build One NN (not 512)\nWe have removed the useless data from the dataset. Next we will convert the variable `wheezy-copper-turtle-magic` into categorical so our model can identify the partial datasets. We are now ready to build a single model using the entire dataset.\n\nThe following NN code comes from Abhishek's wonderful starter code [here][1]. (Please upvote Abishek's kernel). We also use code from Vladislav's great kernel where he adds improvements [here][2]. (The single NN below scores LB 0.930 and CV 0.904)\n\n[1]: https://www.kaggle.com/abhishek/neural-network-with-embedding-layer\n[2]: https://www.kaggle.com/speedwagon/neural-network-baseline"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import StratifiedKFold\nfrom sklearn import metrics, preprocessing\n\nimport tensorflow as tf\ntf.logging.set_verbosity(tf.logging.ERROR) #hide warnings\nfrom keras.layers import Dense, Input\nfrom keras.layers import BatchNormalization\nfrom keras.models import Model\nfrom keras import callbacks\nfrom keras import backend as K\nfrom keras.layers import Dropout\nfrom keras.callbacks import LearningRateScheduler\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# CUSTOM METRICS\ndef fallback_auc(y_true, y_pred):\n    try:\n        return metrics.roc_auc_score(y_true, y_pred)\n    except:\n        return 0.5\n\ndef auc(y_true, y_pred):\n    return tf.py_function(fallback_auc, (y_true, y_pred), tf.double)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ONE-HOT-ENCODE THE MAGIC FEATURE\nlen_train = train.shape[0]\ntest['target'] = -1\ndata = pd.concat([train, test])\ndata = pd.concat([data, pd.get_dummies(data['wheezy-copper-turtle-magic'])], axis=1, sort=False)\n\ntrain = data[:len_train]\ntest = data[len_train:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# PREPARE DATA AND STANDARDIZE\ny = train.target\nids = train.id.values\ntrain = train.drop(['id', 'target'], axis=1)\ntest_ids = test.id.values\ntest = test[train.columns]\n\nall_auc_NN = []\noof_preds_NN = np.zeros((len(train)))\ntest_preds_NN = np.zeros((len(test)))\n\nscl = preprocessing.StandardScaler()\nscl.fit(pd.concat([train, test]))\ntrain = scl.transform(train)\ntest = scl.transform(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"NFOLDS = 15\nRANDOM_STATE = 42\n\ngc.collect()\n# STRATIFIED K FOLD\nfolds = StratifiedKFold(n_splits=NFOLDS, shuffle=True, random_state=RANDOM_STATE)\nfor fold_, (trn_, val_) in enumerate(folds.split(y, y)):\n    #print(\"Current Fold: {}\".format(fold_))\n    trn_x, trn_y = train[trn_, :], y.iloc[trn_]\n    val_x, val_y = train[val_, :], y.iloc[val_]\n\n    # BUILD MODEL\n    inp = Input(shape=(trn_x.shape[1],))\n    x = Dense(2000, activation=\"relu\")(inp)\n    x = BatchNormalization()(x)\n    x = Dropout(0.3)(x)\n    x = Dense(1000, activation=\"relu\")(x)\n    x = BatchNormalization()(x)\n    x = Dropout(0.3)(x)\n    x = Dense(500, activation=\"relu\")(x)\n    x = BatchNormalization()(x)\n    x = Dropout(0.2)(x)\n    x = Dense(100, activation=\"relu\")(x)\n    x = BatchNormalization()(x)\n    x = Dropout(0.2)(x)\n    out = Dense(1, activation=\"sigmoid\")(x)\n    clf = Model(inputs=inp, outputs=out)\n    clf.compile(loss='binary_crossentropy', optimizer=\"adam\", metrics=[auc])\n    \n    # CALLBACKS\n    es = callbacks.EarlyStopping(monitor='val_auc', min_delta=0.001, patience=10,\n                verbose=0, mode='max', baseline=None, restore_best_weights=True)\n    rlr = callbacks.ReduceLROnPlateau(monitor='val_auc', factor=0.5,\n                patience=3, min_lr=1e-6, mode='max', verbose=0)\n\n    # TRAIN\n    clf.fit(trn_x, trn_y, validation_data=(val_x, val_y), callbacks=[es, rlr], epochs=100, \n                batch_size=1024, verbose=0)\n    \n    # PREDICT TEST\n    test_fold_preds = clf.predict(test)\n    test_preds_NN += test_fold_preds.ravel() / NFOLDS\n    \n    # PREDICT OOF\n    val_preds = clf.predict(val_x)\n    oof_preds_NN[val_] = val_preds.ravel()\n    \n    # RECORD AUC\n    val_auc = round( metrics.roc_auc_score(val_y, val_preds),5 )\n    all_auc_NN.append(val_auc)\n    print('Fold',fold_,'has AUC =',val_auc)\n    \n    K.clear_session()\n    gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# DISPLAY NN VALIDATION AUC\nval_auc = metrics.roc_auc_score(y, oof_preds_NN)\nprint('NN_CV = OOF_AUC =', round( val_auc,5) )\nprint('Mean_AUC =', round( np.mean(all_auc_NN),5) )\n\n# PLOT NN TEST PREDICTIONS\nplt.hist(test_preds_NN,bins=100)\nplt.title('NN test.csv predictions')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Build 512 SVM\nSVM has been shown to score LB 0.928 [here][1]. We will ensemble this with our NN.\n\n[1]: https://www.kaggle.com/cdeotte/support-vector-machine-0-925"},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/train.csv')\ntest = pd.read_csv('../input/test.csv')\n\ncols = [c for c in train.columns if c not in ['id', 'target', 'wheezy-copper-turtle-magic']]\n\nfrom sklearn.svm import SVC\nfrom sklearn.feature_selection import VarianceThreshold\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import roc_auc_score\n\n# INITIALIZE VARIABLES\noof_preds_SVM = np.zeros(len(train))\ntest_preds_SVM = np.zeros(len(test))\n\n# BUILD 512 SEPARATE MODELS\nfor i in range(512):\n    \n    # ONLY TRAIN/PREDICT WHERE WHEEZY-MAGIC EQUALS I\n    train2 = train[train['wheezy-copper-turtle-magic']==i]\n    test2 = test[test['wheezy-copper-turtle-magic']==i]\n    idx1 = train2.index; idx2 = test2.index\n    train2.reset_index(drop=True,inplace=True)\n    \n    # FEATURE SELECTION (USE SUBSET OF 255 FEATURES)\n    sel = VarianceThreshold(threshold=1.5).fit(train2[cols])\n    train3 = sel.transform(train2[cols])\n    test3 = sel.transform(test2[cols])\n        \n    # STRATIFIED K FOLD\n    skf = StratifiedKFold(n_splits=11, random_state=42)\n    for train_index, test_index in skf.split(train3, train2['target']):\n        \n        # MODEL WITH SUPPORT VECTOR MACHINE\n        clf = SVC(probability=True,kernel='poly',degree=4,gamma='auto')\n        clf.fit(train3[train_index,:],train2.loc[train_index]['target'])\n        oof_preds_SVM[idx1[test_index]] = clf.predict_proba(train3[test_index,:])[:,1]\n        test_preds_SVM[idx2] += clf.predict_proba(test3)[:,1] / skf.n_splits\n        \n    #if i%10==0: print(i)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# DISPLAY SVM VALIDATION AUC\nval_auc = roc_auc_score(train['target'],oof_preds_SVM)\nprint('SVM_CV = OOF_AUC =',round(val_auc,5))\n\n# PLOT SVM TEST PREDICTIONS\nplt.hist(test_preds_SVM,bins=100)\nplt.title('SVM test.csv predictions')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Ensemble NN and SVM"},{"metadata":{"trusted":true},"cell_type":"code","source":"# DISPLAY ENSEMBLE VALIDATION AUC\nval_auc = roc_auc_score(train['target'],oof_preds_SVM+oof_preds_NN)\nprint('Ensemble_NN+SVM_CV = OOF_AUC =',round(val_auc,5))\n\n# PLOT ENSEMBLE TEST PREDICTIONS\nplt.hist(test_preds_SVM+test_preds_NN,bins=100)\nplt.title('Ensemble NN+SVM test.csv predictions')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Private Test Dataset Probing\nThe `test.csv` file that we download only contains the public test dataset. Therefore when we run our code locally or in kaggle kernels and load `test.csv`, we only have `256*512+1` rows. When we submit our kernel for scoring at Kaggle, our same code runs a second time. When our code runs this second time and loads `test.csv`, our code has the full test dataset of `512*512` rows. \n\nAfter our code finishes running this second time, the new `submission.csv` is scored and displayed as public LB. (Note that the `submission.csv` file from the first run isn't scored, only the second `submission.csv`). Since `submission.csv` is created after our code \"sees\" the full test dataset, we can probe the full test dataset and report our findings in our LB score by altering `submission.csv`.\n\nFurthermore we can separate the public and private test dataset. The id column of the dataset was decoded by Kaggle users Linear and Yirun [here][1]. Brilliant work Linear and Yirun! The ids are just the MD5 hash of the row_number as string concatenated with \"test\" or \"train\". Therefore we can put all the public test dataset ids into one index allowing us to identify the public/private test dataset and then probe the private test dataset specifically. \n\n[1]: https://www.kaggle.com/c/instant-gratification/discussion/92634#534306"},{"metadata":{"trusted":true},"cell_type":"code","source":"import hashlib\n\n# CREATE LIST PUBLIC DATASET IDS\npublic_ids = []\nfor i in range(256*512+1):\n    st = str(i)+\"test\"\n    public_ids.append( hashlib.md5(st.encode()).hexdigest() )\n    \n# DISPLAY FIRST 5 GENERATED\npublic_ids[:5]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# DISPLAY FIRST 5 ACTUAL\ntest['id'].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# SEPARATE PUBLIC AND PRIVATE DATASETS\npublic = test[ test['id'].isin(public_ids) ].copy()\nprivate = test[ ~test.index.isin(public.index) ].copy()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Determine structure of private dataset\nAs described [here][1], both the train and public test dataset appear to be 512 datasets combined. Each partial dataset has 256 variables. Therefore there are `131072 = 512 * 216` blocks of data. Each block of data has either standard deviation approx 1.0 or approx 3.75. The blocks with standard deviation 1.0 seem to be **useless** while the blocks with standard deviation 3.75 seem to be **useful**. Does the private test dataset have this same structure?\n\n[1]: https://www.kaggle.com/c/instant-gratification/discussion/92930"},{"metadata":{"trusted":true},"cell_type":"code","source":"# DETERMINE TRAIN DATASET STRUCTURE\nuseful_train = np.zeros((256,512))\nfor i in range(512):\n    partial = train[ train['wheezy-copper-turtle-magic']==i ]\n    useful_train[:,i] = np.std(partial.iloc[:,1:-1], axis=0)\nuseful_train = useful_train > 1.5\n\n# DETERMINE PUBLIC TEST DATASET STRUCTURE\nuseful_public = np.zeros((256,512))\nfor i in range(512):\n    partial = public[ public['wheezy-copper-turtle-magic']==i ]\n    useful_public[:,i] = np.std(partial.iloc[:,1:], axis=0)\nuseful_public = useful_public > 1.5\n\n# DETERMINE PRIVATE TEST DATASET STRUCTURE\nuseful_private = np.zeros((256,512))\nfor i in range(512):\n    partial = private[ private['wheezy-copper-turtle-magic']==i ]\n    useful_private[:,i] = np.std(partial.iloc[:,1:], axis=0)\nuseful_private = useful_private > 1.5","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if np.allclose(useful_train,useful_public):\n    print('Public dataset has the SAME structure as train')\nelse:\n    print('Public dataset DOES NOT HAVE the same structure as train')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Report private dataset findings via LB score\nIf the private dataset has a different structure than the training and public test dataset, we will submit all zero predictions and achieve an LB score of 0.500. If the private dataset has the same structure as training and public test dataset, then we will submit true predictions and achieve an LB score of 0.950.\n\nWhen we run this locally or in Kaggle kernels, we will create a `submission.csv` that contains all zeros because we do not have the private dataset and therefore the logic determines that private is different (because it isn't present). However when we submit this to Kaggle, we do have the private dataset and it passes the test. As a result we see that this kernel scores LB 0.950 instead of LB 0.500."},{"metadata":{"trusted":true},"cell_type":"code","source":"sub = pd.read_csv('../input/sample_submission.csv')\n\nif np.allclose(useful_train,useful_public) & np.allclose(useful_train,useful_private):\n    print('We are submitting TRUE predictions for LB 0.950')\n    sub['target'] = (test_preds_NN + test_preds_SVM) / 2.0\nelse:\n    print('We are submitting ALL ZERO predictions for LB 0.500')\n    sub['target'] = np.zeros( len(sub) )\n    \nsub.to_csv('submission.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Conclusion\nAfter submitting this to Kaggle, we see that this kernel scored LB 0.950. That implies that when this kernel ran a second time in Kaggle's evaluation procedure, it gained access to the private test dataset and confirmed that the private test dataset has the same datablock structure of usefulness as the training data and public test dataset.\n\nAdditionally in this kernel we learned how to utilize the datablock structure to alter the entire dataset thus allowing us to make a single model instead of 512 models. We built a single high scoring NN model."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}