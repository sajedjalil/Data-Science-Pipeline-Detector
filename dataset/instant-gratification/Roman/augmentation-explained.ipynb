{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Data Augmentation\n\nIn this kernel I will show a way to augment the dataset so you will have twice as much training data. In order to do so we need to undertand how the original data was generated so first lets create some sandbox examples which will let us visualize the process of augmentation.\n\nWe are going to use [sklearn.datasets.make_classification ](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_classification.html) to generate data to play with."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.datasets import make_classification \nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\nfrom sklearn.feature_selection import VarianceThreshold\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.cluster import KMeans\nimport plotly.plotly as py\nimport plotly.graph_objs as go\nimport plotly.io as pio\nfrom plotly.offline import init_notebook_mode, iplot\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.simplefilter('ignore')\ninit_notebook_mode()\nsns.set()\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"cols = ['Real-life', 'Fantasy']\ntrain, target = make_classification(10, 2, n_redundant=0, flip_y=0.0, n_informative=2, n_clusters_per_class=1, random_state=47, class_sep=1)\ntrain = pd.DataFrame(train, columns=cols)\ntrain['target'] = target\ntrain","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(14, 6))\nsns.scatterplot(train[train['target'] == 0]['Real-life'], train[train['target'] == 0]['Fantasy'], s=150);\nsns.scatterplot(train[train['target'] == 1]['Real-life'], train[train['target'] == 1]['Fantasy'], s=150);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have two separate cluster - one cluster per class, thus data labeled with 0's are on the left cluster and data labeled with 1's are on the right one.\nNext lets find centers for both of this clusters and visualize them."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(14, 6))\nsns.scatterplot(train[train['target'] == 0]['Real-life'], train[train['target'] == 0]['Fantasy'], s=150);\nsns.scatterplot(train[train['target'] == 1]['Real-life'], train[train['target'] == 1]['Fantasy'], s=150);\nsns.scatterplot([train[train['target'] == 0][cols].mean().values[0]], [train[train['target'] == 0][cols].mean().values[1]], s=250);\nsns.scatterplot([train[train['target'] == 1][cols].mean().values[0]], [train[train['target'] == 1][cols].mean().values[1]], s=250);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now lets take a closer look to the left cluster (the one labeled with 0's) and take a center point, which is called 'centroid', as an origin for our abscissa (X axis) and ordinate (Y axis)."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(14, 6))\nsns.scatterplot(train[train['target'] == 0]['Real-life'], train[train['target'] == 0]['Fantasy'], s=150);\nsns.scatterplot([train[train['target'] == 0][cols].mean().values[0]], [train[train['target'] == 0][cols].mean().values[1]], s=250);\nplt.plot([train[train['target'] == 0][cols].min()[0], train[train['target'] == 0][cols].max()[0]], [train[train['target'] == 0][cols].mean()[1]] * 2, sns.xkcd_rgb[\"black\"]);\nplt.plot([train[train['target'] == 0][cols].mean()[0]] * 2, [-1.25, 0.5], sns.xkcd_rgb[\"green\"]);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"To augment our data we need to flip this points first around X axis (black line) and then around Y axis (green line). Let's do so."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(14, 6))\nsns.scatterplot(train[train['target'] == 0]['Real-life'], train[train['target'] == 0]['Fantasy'], s=150);\nsns.scatterplot([train[train['target'] == 0][cols].mean().values[0]], [train[train['target'] == 0][cols].mean().values[1]], s=250);\nplt.plot([train[train['target'] == 0][cols].min()[0], train[train['target'] == 0][cols].max()[0]], [train[train['target'] == 0][cols].mean()[1]] * 2, sns.xkcd_rgb[\"black\"]);\nplt.plot([train[train['target'] == 0][cols].mean()[0]] * 2, [-1.25, 0.5], sns.xkcd_rgb[\"green\"]);\nsns.scatterplot(train[train['target'] == 0]['Real-life'].mean() + (train[train['target'] == 0]['Real-life'].mean() - train[train['target'] == 0]['Real-life']), train[train['target'] == 0]['Fantasy'], s=150);\nmean = train[train['target'] == 0]['Real-life'].mean()\nfor x, y in train[train['target']==0][cols].values:\n    x_new = mean + (mean - x)\n    dx = x_new - x\n    bias = 0.004\n    if dx > 0:\n        bias *= -1\n    plt.arrow(x, y, dx + bias, 0, fc=\"k\", ec=\"k\", head_width=0.075, head_length=0.003, width=0.0025, color='green');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we need to flip those new (green) points one more time, this time around Y axis. Looks a little messy, I know."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(14, 6))\nsns.scatterplot(train[train['target'] == 0]['Real-life'], train[train['target'] == 0]['Fantasy'], s=150);\nsns.scatterplot([train[train['target'] == 0][cols].mean().values[0]], [train[train['target'] == 0][cols].mean().values[1]], s=250);\nplt.plot([train[train['target'] == 0][cols].min()[0], train[train['target'] == 0][cols].max()[0]], [train[train['target'] == 0][cols].mean()[1]] * 2, sns.xkcd_rgb[\"black\"]);\nplt.plot([train[train['target'] == 0][cols].mean()[0]] * 2, [-1.25, 0.5], sns.xkcd_rgb[\"green\"]);\nsns.scatterplot(train[train['target'] == 0]['Real-life'].mean() + (train[train['target'] == 0]['Real-life'].mean() - train[train['target'] == 0]['Real-life']), train[train['target'] == 0]['Fantasy'], s=150);\nsns.scatterplot(train[train['target'] == 0]['Real-life'].mean() + (train[train['target'] == 0]['Real-life'].mean() - train[train['target'] == 0]['Real-life']), \n                train[train['target'] == 0]['Fantasy'].mean() + (train[train['target'] == 0]['Fantasy'].mean() - train[train['target'] == 0]['Fantasy']), s=150);\nmean_x = train[train['target'] == 0]['Real-life'].mean()\nmean_y = train[train['target'] == 0]['Fantasy'].mean()\nfor x, y in train[train['target']==0][cols].values:\n    x_new = mean_x + (mean_x - x)\n    y_new = mean_y + (mean_y - y)\n    dx = x_new - x\n    dy = y_new - y\n    bias_x = 0.004\n    bias_y = 0.1\n    if dx > 0:\n        bias_x *= -1\n    if dy > 0:\n        bias_y *= -1\n    plt.arrow(x, y, dx + bias_x, 0, fc=\"k\", ec=\"k\", head_width=0.075, head_length=0.003, width=0.0025);\n    plt.arrow(x_new, y, 0, dy + bias_y, fc=\"k\", ec=\"k\", head_width=0.002, head_length=0.075, width=0.0001);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So lets cleenup a little and leave only original point and new, augmented ones. For every new point it's coordinates can be calculated using the following formula: Cluster_center * 2 - point_coordinates. So every point in the cluster will be flipped around cluster's 'origin point' (cluster's center)."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(14, 6))\nsns.scatterplot(train[train['target'] == 0]['Real-life'], train[train['target'] == 0]['Fantasy'], s=150);\nsns.scatterplot([train[train['target'] == 0][cols].mean().values[0]], [train[train['target'] == 0][cols].mean().values[1]], s=250);\nplt.plot([train[train['target'] == 0][cols].min()[0], train[train['target'] == 0][cols].max()[0]], [train[train['target'] == 0][cols].mean()[1]] * 2, sns.xkcd_rgb[\"black\"]);\nplt.plot([train[train['target'] == 0][cols].mean()[0]] * 2, [-1.25, 0.5], sns.xkcd_rgb[\"green\"]);\naugmented = 2 * train[train['target']==0][cols].mean() - train[train['target']==0][cols]\nsns.scatterplot(augmented['Real-life'], augmented['Fantasy'], s=150);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we have extended our cluster of points with new ones, which would be position no further from the center of the cluster than the maximum distance of the the most distant point to this center (I know this might be a difficult sentence to understand so spend some time and eventually this would make sencs to you).\n\nThis formula (Cluster_center * 2 - point) work in any dimensionality, but we are limited to only 3 dimensions with visualization, so let's plot 3d graph and see that it still works.\n\nHere is an original cluster:"},{"metadata":{"trusted":true},"cell_type":"code","source":"train, target = make_classification(100, 3, n_redundant=0, flip_y=0.0, n_informative=3, n_clusters_per_class=1, random_state=47, class_sep=3)\ncols = ['Real-life', 'Fantasy', 'Landslide']\ntrain = pd.DataFrame(train, columns=cols)\ntrain['target'] = target\n\ntrace1 = go.Scatter3d(\n    x=train[train['target'] == 0]['Real-life'],\n    y=train[train['target'] == 0]['Fantasy'],\n    z=train[train['target'] == 0]['Landslide'],\n    mode='markers',\n    marker=dict(\n        size=8,\n        line=dict(\n            color='rgba(217, 217, 217, 0.14)',\n            width=0.5\n        ),\n        opacity=1\n    )\n)\nlayout = go.Layout(\n    margin=dict(l=0, r=0, b=0, t=0),\n    scene = dict(\n        xaxis = dict(\n            title='Real-life'),\n        yaxis = dict(\n            title='Fantasy'),\n        zaxis = dict(\n            title='Landslide')\n    )\n)\nfig = go.Figure(data=[trace1], layout=layout);\niplot(fig, filename='simple-3d-scatter', image_width=1024, image_height=768);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"And the same cluster with augmented points added:"},{"metadata":{"trusted":true},"cell_type":"code","source":"augmented = 2 * train[train['target']==0][cols].mean() - train[train['target']==0][cols]\ntrace1 = go.Scatter3d(\n    x=train[train['target'] == 0]['Real-life'],\n    y=train[train['target'] == 0]['Fantasy'],\n    z=train[train['target'] == 0]['Landslide'],\n    mode='markers',\n    marker=dict(\n        size=8,\n        line=dict(\n            color='rgba(217, 217, 217, 0.14)',\n            width=0.5\n        ),\n        opacity=1\n    )\n)\ntrace2 = go.Scatter3d(\n    x=augmented['Real-life'],\n    y=augmented['Fantasy'],\n    z=augmented['Landslide'],\n    mode='markers',\n    marker=dict(\n        size=8,\n        line=dict(\n            color='rgba(100, 100, 100, 0.14)',\n            width=0.5\n        ),\n        opacity=1\n    )\n)\nlayout = go.Layout(\n    margin=dict(l=0, r=0, b=0, t=0),\n    scene = dict(\n        xaxis = dict(\n            title='Real-life'),\n        yaxis = dict(\n            title='Fantasy'),\n        zaxis = dict(\n            title='Landslide')\n    )\n)\nfig = go.Figure(data=[trace1, trace2], layout=layout);\niplot(fig, filename='simple-3d-scatter', image_width=1024, image_height=768);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Ok, so far so good, but everything is while we are working with a single cluster. And this is possible because we have created our dataset using **n_clusters_per_class=1** parameter. But if you'd read make_classification's documentation you would see that by default it has values of **2**. In this case we need to find mean points (centroids) from both cluster. And if there are 3, 4, p clusters we need to find 2, 3, p centroids respectively. Otherwise an augmentation would go horribly wrong.\n\nLet me show what I am talking about. Now we have almost the same dataset generated but with 2 clusters per each class. If we find only 1 single centroind and flip point around them they would be located in a wrong places.\n\nFirst original points:"},{"metadata":{"trusted":true},"cell_type":"code","source":"cols = ['Real-life', 'Fantasy']\ntrain, target = make_classification(40, 2, n_redundant=0, flip_y=0.0, n_informative=2, n_clusters_per_class=2, random_state=47, class_sep=1)\ntrain = pd.DataFrame(train, columns=cols)\ntrain['target'] = target\n\nplt.figure(figsize=(14, 8))\nsns.scatterplot(train[train['target']==0]['Real-life'], train[train['target']==0]['Fantasy'], s=150);\nsns.scatterplot([train[train['target']==0][cols].mean().values[0]], [train[train['target']==0][cols].mean().values[1]], s=250);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now adding an augmented ones"},{"metadata":{"trusted":true},"cell_type":"code","source":"cols = ['Real-life', 'Fantasy']\ntrain, target = make_classification(40, 2, n_redundant=0, flip_y=0.0, n_informative=2, n_clusters_per_class=2, random_state=47, class_sep=1)\ntrain = pd.DataFrame(train, columns=cols)\ntrain['target'] = target\naugmented = 2 * train[train['target']==0][cols].mean() - train[train['target']==0][cols]\n\nplt.figure(figsize=(14, 8))\nsns.scatterplot(train[train['target']==0]['Real-life'], train[train['target']==0]['Fantasy'], s=150);\nsns.scatterplot([train[train['target']==0][cols].mean().values[0]], [train[train['target']==0][cols].mean().values[1]], s=250);\nsns.scatterplot(augmented['Real-life'], augmented['Fantasy'], s=150);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Can you see the problem? No?"},{"metadata":{},"cell_type":"markdown","source":"By now we have been plotting only points with 0 labels. But lets add those with label 1:"},{"metadata":{"trusted":true},"cell_type":"code","source":"cols = ['Real-life', 'Fantasy']\ntrain, target = make_classification(40, 2, n_redundant=0, flip_y=0.0, n_informative=2, n_clusters_per_class=2, random_state=47, class_sep=1)\ntrain = pd.DataFrame(train, columns=cols)\ntrain['target'] = target\naugmented = 2 * train[train['target']==0][cols].mean() - train[train['target']==0][cols]\n\nplt.figure(figsize=(14, 8))\nsns.scatterplot(train[train['target']==0]['Real-life'], train[train['target']==0]['Fantasy'], s=150);\nsns.scatterplot([train[train['target']==0][cols].mean().values[0]], [train[train['target']==0][cols].mean().values[1]], s=250);\nsns.scatterplot(augmented['Real-life'], augmented['Fantasy'], s=150);\nsns.scatterplot(train[train['target']==1]['Real-life'], train[train['target']==1]['Fantasy'], s=150);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we have mixed them up more than they were before adding an augmented data and any predictive model's performance would suffer because of this. \n\nIs there a way out? Yes there is. We need to find centroind for both of this clusters and make an augmnetation for them separately. In order to find centroind let me present you **K Means Clustering** algorithm. You can find an awesome explanation of this algorithm by Andrew Ng [over here](https://www.coursera.org/learn/machine-learning/lecture/93VPG/k-means-algorithm).\n\nSo lets use it and see how it will perform on our dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"km = KMeans(n_clusters=2)\nkm.fit(train[train['target']==0][cols]);\ncenroinds = km.cluster_centers_\nprint('First cluster center:', cenroinds[0])\nprint('Second cluster center:', cenroinds[1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(14, 8))\nsns.scatterplot(train[train['target']==0]['Real-life'], train[train['target']==0]['Fantasy'], s=150);\nsns.scatterplot([cenroinds[0][0]], [cenroinds[0][1]], s=250);\nsns.scatterplot([cenroinds[1][0]], [cenroinds[1][1]], s=250);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have found a proper centroind for each cluster. Now it's time to make an augmentation for each of them separately."},{"metadata":{"trusted":true},"cell_type":"code","source":"clustered_df = train[train['target']==0][cols]\nclustered_df['cluser'] = km.predict(train[train['target']==0][cols])\naugmented_1 = 2 * clustered_df[clustered_df['cluser'] == 0][cols].mean() - clustered_df[clustered_df['cluser'] == 0][cols]\naugmented_2 = 2 * clustered_df[clustered_df['cluser'] == 1][cols].mean() - clustered_df[clustered_df['cluser'] == 1][cols]\n\nplt.figure(figsize=(14, 8))\nsns.scatterplot(train[train['target']==0]['Real-life'], train[train['target']==0]['Fantasy'], s=150);\nsns.scatterplot([cenroinds[0][0]], [cenroinds[0][1]], s=250);\nsns.scatterplot([cenroinds[1][0]], [cenroinds[1][1]], s=250);\nsns.scatterplot(augmented_1['Real-life'], augmented_1['Fantasy'], s=150);\nsns.scatterplot(augmented_2['Real-life'], augmented_2['Fantasy'], s=150);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It look much better. But now we are facing another problem - what if we don't know the number of clusters per class? Like we don't know them in this competition's data set. Well, we still can use KMean to try and find an optimal number of clusters. Let me show you how."},{"metadata":{"trusted":true},"cell_type":"code","source":"train, target = make_classification(1000, 4, n_redundant=0, flip_y=0.0, n_informative=3, n_clusters_per_class=3, random_state=47, class_sep=2)\ntrain = pd.DataFrame(train)\ntrain['target'] = target\n\ninertia = dict()\nfor k in range(1, 6):\n    km = KMeans(n_clusters=k)\n    km.fit(train[train['target']==0][[0, 1, 2, 3]])\n    inertia[k] = km.inertia_\n    \nplt.figure(figsize=(14, 8))\nplt.plot(inertia.keys(), inertia.values());","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As you can see an inertia (a cummulative distance of all points to their cluster's centroid) is droping down significantly when we increase number of clusters from 1 to 2 and from 2 to 3 (which is a ground truth in this case), but almost does not decrease with 4 clusters and 5 clusters. So the best choise is 3 clusters. This is called an **elbow rule** (if you think about this graph as an arm than en elbow would be your best choice for number of clusters)."},{"metadata":{},"cell_type":"markdown","source":"Ok, so what does this all have to do with this competition's data? Well, since we have a [good guess](https://www.kaggle.com/mhviraf/synthetic-data-for-next-instant-gratification) that data was generated using make_classification all we need to do is to find a correct number of clusters per each class (we have a binary classification) per each of 512 data sets (we have 512 data sets, enumareted with **wheezy-copper-turtle-magic** feature. And then make an augmentation. \n\nRight now, if you assume that there is only 1 cluster per class and make an augmentation this only drops CV and LB. \n\nSo maybe there are more than 1 cluster per class? Or maybe this dataset is to noisy so it is impossible to find a correct number of clusters? Looks like [Chris Deotte](https://www.kaggle.com/cdeotte) can get another golded kernel explaining this to us ;)"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}