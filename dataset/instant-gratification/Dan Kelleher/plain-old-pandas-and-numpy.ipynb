{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport scipy as sp\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import  KFold\nfrom sklearn.feature_selection import SelectKBest, VarianceThreshold\nfrom sklearn.pipeline import Pipeline\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Just Pandas and Numpy (and SciPy)\n\nOkay, okay, if you look at the imports hidden above, you will see that I also import some things from SciKitLearn, so this is cheating a bit, but I have good reasons for using those for feature selection, I promise.\n\nThe first thing to do is to import the data:"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/train.csv')\ntest = pd.read_csv(\"../input/test.csv\")\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So we see it looks like hte column names were generated by a strong password generator. So, if you need a password, don't use \"wheezy-copper-turtle-magic,\" which it turns out...."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_not_float = train.loc[:,train.dtypes != \"float64\"]\ntrain_not_float.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"is the only variable which is not a float... interesting. It turns out, that once you condition on the value of \"wheezy-copper-turtle-magic,\" then the valuea are much better corrolated with the outputs!"},{"metadata":{"trusted":true},"cell_type":"code","source":"mask = train.columns != \"wheezy-copper-turtle-magic\"\nmask[0] = False\nmask[-1] = False\n#mask[75:] = False\nmask_test = test.columns != \"wheezy-copper-turtle-magic\"\nmask_test[0] = False\n#mask_test[75:] = False","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## So what is happening?\n\nUnless I am missing something, it looks like there are for each of the 512 values of `wheezy-copper-turtle-magic` and the 2 values of the `target` variable the rest of the variables appear to be normal (we can look at that later.) Assuming this is true,  this competition is actually a souped-up version of questions which I gave students in my first semester probability course.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"train20 = train[train['wheezy-copper-turtle-magic'] == 20]\nfig, axs = plt.subplots(2,2)\naxs[0,0].scatter(train20.iloc[:,25], train20.iloc[:,100], c = train20.target, alpha = 0.5)\naxs[0,1].scatter(train20.iloc[:,12], train20.iloc[:,100], c = train20.target, alpha = 0.5)\naxs[1,0].scatter(train20.iloc[:,25], train20.iloc[:,125], c = train20.target, alpha = 0.5)\naxs[1,1].scatter(train20.iloc[:,12], train20.iloc[:,125], c = train20.target, alpha = 0.5)\nplt.show","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So I should be able to solve it!\n\nTo think of this in a Bayesian way, we are assuming that, within each state of the value of the `wheezy-copper-turtle-magic` variable, there vector means $\\mu_{0/1}$ and covariance matrices $\\Sigma_{0/1}$ such that the densities of each of the rows should be\n$$\nf(\\mathbf x ~|~ target = 0/1) =  c \\det( \\Sigma_{0/1} )^{-1/2} e^{-0.5(\\mathbf x -  \\mu_{0/1}) \\Sigma_{0/1}^{-1} (\\mathbf x - \\mu_{0/1})^T}\n$$\nhere $c = (2\\pi)^{d/2}$ doesn't depend of the parameters $\\mu$ and $\\Sigma$, so we won't need to worry about it. This means, the Baysian posterior is that\n$$\nP(target = 1 ~|~ \\mathbf x ) = \\frac{ \\det( \\Sigma_{1} )^{-1/2} e^{-0.5(\\mathbf x -  \\mu_{1}) \\Sigma_{1}^{-1} (\\mathbf x - \\mu_{1})^T}}{ \\det( \\Sigma_{0} )^{-1/2} e^{-0.5(\\mathbf x -  \\mu_{0}) \\Sigma_{0}^{-1} (\\mathbf x - \\mu_{0})^T} +  \\det( \\Sigma_{1} )^{-1/2} e^{-0.5(\\mathbf x -  \\mu_{1}) \\Sigma_{1}^{-1} (\\mathbf x - \\mu_{1})^T}}\n$$\nand below is a helper function which computes this posterior given the means and the covariance matrices."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"def MAPProba(x, mu0, mu1, sigma0, sigma1, det0, det1, sigma0_pinv, sigma1_pinv):\n    x = np.asarray(x)\n    # Recenter the x's\n    x_prime0 = x - mu0\n    x_prime1 = x - mu1\n    # Check that it is a solvable equation, and solve if it is\n    if np.allclose(x_prime0 @ sigma0 @ sigma0_pinv, x_prime0):\n        y0 = 1\n        x0 =  x_prime0 @ sigma0_pinv \n        Q0 = np.dot(x0, np.transpose(x_prime0))\n        Q0 = float(Q0)\n    else:\n        y0 = 0\n    if np.allclose(x_prime1 @ sigma1 @ sigma1_pinv, x_prime1):\n        y1 = 1\n        x1 =  x_prime1 @ sigma1_pinv \n        Q1 = np.dot(x1, np.transpose(x_prime1))\n        Q1 = float(Q1)\n    else:\n        y1 = 0\n    # output nulls if both the equations weren't solveable\n    if y1 == 0 & y0 == 0:\n        return np.asarray([np.nan, np.nan]).reshape((2))\n    # output probabilities if both the equations were\n    elif y1 == 1 & y0 == 1:\n        y1 = 1/(1+np.exp(-0.5*(Q0-Q1))*det1/det0)\n        y0 = 1 - y1\n        return np.asarray([y0, y1]).reshape((2))\n    else:\n        return np.asarray([y0, y1]).reshape((2))\n    return","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# How do we apply this?\n\nWell, in our case, we know that there is a reasonable assumptions of normality, and that this model applies. The things that we are missing are the parameters $\\mu$ and $\\Sigma$.\n\nThese can be approximated. If we use $$X = \\begin{bmatrix}\\mathbf x_1 \\\\ \\mathbf x_2 \\\\ \\vdots \\\\ \\mathbf x_n \\end{bmatrix}$$ to be the data with rows $\\mathbf x_i$, then we replace the mean $\\mu$ with the sample mean $$ \\hat{\\mathbf{x}} = \\frac{\\mathbf x_1 + \\mathbf x_2 + \\cdots + \\mathbf x_n}{n}$$, \nand the sample covatirance matrix \n$$\nS = \\frac1{n-1} (XX^T - \\mu^T\\mu)\n$$\nThese are very standard, so there is built in functions in numpy and pandas for them!\n\nI am going to make a quick class that has two methods, `fit` and `predict_proba` which will compute the parameters of the model and output the priors respectively."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"class NormalMAP:\n    def __init__(self):\n        self.mu0 = 0\n        self.mu1 = 0\n       # \n        self.sigma0 = np.empty((0,0))\n        self.sigma1 = np.empty((0,0))\n        self.shape = 0\n        self.samples = 0\n        #\n        return\n    \n    def fit(self, X_tr, y_tr):\n        self.samples, self.shape = X_tr.shape\n        masker = np.asarray(y_tr == 0).reshape(self.samples)\n        X_tr0 = X_tr[masker]\n        X_tr1 = X_tr[~masker]\n\n        self.mu0 = np.asarray(X_tr0.mean()).reshape(1,self.shape)\n        self.mu1 = np.asarray(X_tr1.mean()).reshape(1,self.shape)\n\n        self.sigma0 = np.asarray(np.cov(X_tr0, rowvar = False))\n        self.sigma1 = np.asarray(np.cov(X_tr1, rowvar = False))\n\n        self.sigma0_pinv = sp.linalg.pinv(self.sigma0)\n        self.sigma1_pinv = sp.linalg.pinv(self.sigma1)\n\n        self.w0, self.V0 = np.linalg.eig(self.sigma0)\n        self.w1, self.V1 = np.linalg.eig(self.sigma1)\n\n        self.det0 = np.real(np.sqrt(np.prod(self.w0[~np.isclose(self.w0, 0)])))\n        self.det1 = np.real(np.sqrt(np.prod(self.w1[~np.isclose(self.w1, 0)])))\n        return\n\n    def predict_proba(self, X_te):\n        x = np.asarray(X_te)\n        \n        y = np.apply_along_axis(lambda x: MAPProba(x, self.mu0, self.mu1, self.sigma0, self.sigma1, self.det0, self.det1, self.sigma0_pinv, self.sigma1_pinv),\n                               1,\n                               x)\n        return y","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## So what's the problem?\n\nOkay so I have implemented this, I should be able to apply it and win the competition! Right?\n\nWell no, there are some problems. The first one arrises completely theoretically. There are 512 wheezy-values, there are 2 target values, so that means there are 1024 different normal distributions, and thus 256 samples for each one. however, they each  have 255 total variables (not including wheezy and target), this means that there is only one more sample than variable.\n\nSo, if we wanted to do cross validation using the technique as stated, and we used all 255 variables, if we reserved more than 1 sample for testing, the sample covariance matrix would have rank less than 255... it would not be invertible! This makes the formulas gibberish (although they can sort of be salvaged).\n\nThe easiest way to fix this is with features selection. This is where the scikitlearn packages come into play... features selection! In particular, using linear regression to select the K. By selecting a handful of features this brings the size of the covariance matrix down... meaning that we will surely have a full rank matrix to use!"},{"metadata":{"trusted":true},"cell_type":"code","source":"# initialize dataframes\ncv_scores = pd.DataFrame({'train_score' : [], \n                          'test_score' : [], \n                          'fold' : []})\nsubmission = pd.DataFrame({\"id\" : [],\n                           \"target\" : []})\n#\n#loop over different values of wheezy-copper-turtle-magic\nfor wheezy_value in range(512):\n    #\n    # Subset the data by conditioning on wheezy-copper-turtle-magic\n    train_temp = train[train[\"wheezy-copper-turtle-magic\"]==wheezy_value]\n    #\n    # Break into train and target sets\n    X_mask = train_temp.iloc[:,mask]\n    y_temp = train_temp.iloc[:,-1]\n    #\n    # Initialize k fold split\n    kfold = KFold(n_splits = 32, shuffle = True, random_state = 42)\n    #\n    # Initialize test set\n    test_temp = test[test[\"wheezy-copper-turtle-magic\"] == wheezy_value]\n    predictions =np.zeros((test_temp.shape[0],2))\n    #\n    # go a layer deeper\n    for fold_, (trn_idx, val_idx) in enumerate(kfold.split(X_mask,y_temp)):\n        #\n        # Create Pipeline\n        pp_pl = Pipeline([#(\"scaler\", StandardScaler()),\n                          (\"selection\", VarianceThreshold(threshold = 1.3))])\n        #\n        # Created Classifier\n        nmap = NormalMAP()\n        \n        X_train, X_test = X_mask.iloc[trn_idx,:], X_mask.iloc[val_idx,:]\n        y_train, y_test = y_temp.iloc[trn_idx], y_temp.iloc[val_idx]\n    \n        X_train_trans = pp_pl.fit_transform(X_train, y_train)\n        X_test_trans = pp_pl.transform(X_test)\n        X_train_trans = pd.DataFrame(X_train_trans)\n        X_test_trans = pd.DataFrame(X_test_trans)\n        #\n        # log CV scores\n        nmap.fit(X_train_trans, y_train)\n        y_pred_train = nmap.predict_proba(X_train_trans)\n        y_pred_test = nmap.predict_proba(X_test_trans) \n        y_pred_train = (y_pred_train[:,1]>0.5)\n        y_pred_test = (y_pred_test[:,1]>0.5)\n        cv_train = np.mean(y_pred_train == y_train)\n        cv_test = np.mean(y_pred_test == y_test)\n        cv_scores.loc[\"wheezy_value_%s_%d\" % (wheezy_value, fold_),\n                      ['train_score', 'fold']] = [cv_train, fold_]\n        cv_scores.loc[\"wheezy_value_%s_%d\" % (wheezy_value, fold_), \n                      ['test_score', 'fold']] = [cv_test, fold_]\n        #\n        # make predictions\n\n        test_trans = pp_pl.transform(test_temp.iloc[:,mask_test])\n        index_temp = test_temp.iloc[:,0]\n        predictions += nmap.predict_proba(test_trans)/kfold.n_splits\n        #\n    # format submission\n    submission_temp = pd.DataFrame()\n    submission_temp[\"id\"] = index_temp\n    submission_temp['target'] = predictions[:,1]\n    submission = pd.concat([submission,submission_temp])\n    #","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(cv_scores.head(10))\nprint(cv_scores.mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.to_csv('submission.csv', index = False)\nplt.hist(submission.target)\nplt.show","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## So what now?\n\nWell, this isn't the state-of-the-art way to handle this probelm. Graphica Lasso provides a more sophisticated way of approximating a Gaussian distribution, and SciKitLearn has a much more sophisticated Gaussian Mixture Model class than the one I made. But this way of doing things shows a lot more of the statistics that are at play in this problem, which I personally find more rewarding :)"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}