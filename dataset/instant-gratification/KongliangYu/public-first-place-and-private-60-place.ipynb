{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport os\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.feature_selection import VarianceThreshold\nfrom sklearn.mixture import GaussianMixture\nfrom sklearn.covariance import GraphicalLasso\nfrom sklearn.preprocessing import StandardScaler\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/train.csv')\ntest = pd.read_csv('../input/test.csv')\n\ntrain['wheezy-copper-turtle-magic'] = train['wheezy-copper-turtle-magic'].astype('category')\ntest['wheezy-copper-turtle-magic'] = test['wheezy-copper-turtle-magic'].astype('category')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"magicNum = 131073\ndefault_cols = [c for c in train.columns if c not in ['id', 'target','target_pred', 'wheezy-copper-turtle-magic']]\ncols = [c for c in default_cols]\nsub = pd.read_csv('../input/sample_submission.csv')\nsub.to_csv('submission.csv',index=False)\ntrain.shape,test.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are many posts explaining in detail why the n_clusters_per_class param should be 3. Hence ideally using gaussian cluster the dataset will be divided to 6 clusters, EQUALLY! But when you do this on train+public_test, you'll not get same number clusters because the data is not full. Hence I tried it online and get perfect result there.\n\nOnce eace sub-group is divided into 6 clusters, the remaining thing is easy - for each cluster, the majoriy label is the correct label. Till now we could get ideally perfect result. And this gives you public LB 0.97443 and private LB 0.97579 - the later is pretty close to a golden medal.\n\nBut the evaluation metric is AUC, not precision score, so I am wondering whether there is any way to improve the score based on the above perfect classification. \n\n\nConsider there are 20 labels, the perfect classification is ten zeros follows ten ones. But there are 4 numbers fllipped. If we don't know which numbers are fllipped, the idea score we could get is 0.8:"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_perfect = [0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1]\ny_flliped = [1,0,0,1,0,0,0,0,0,0,1,1,1,0,1,1,1,1,0,1]\nroc_auc_score(y_perfect,y_flliped)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, consider we have info that some fllipes may happen at \"some\" places, for example, we know in the first three numbers there is a flip, in 4 to 5 numbers there is another flip, in 13-14 numbers there is a flip, and in last 3 numbers there is a flip. Then, based on this info, we set each number the converted score which reflects the flipping probability, and the AUC score improves!"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_preds = [0.33,0.33,0.33,0.5,0.5,0,0,0,0,0,1,1,0.5,0.5,1,1,1,0.66,0.66,0.66]\nroc_auc_score(y_flliped,y_preds)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Return to this topic, although we don't know the exact labels which are flliped, but if we could infer the labels with higher probablity to get flliped, we could imporve the AUC  -  for those '1's with higher probability to be flliped to 0, they should have lower score, and for those '0's with higher probability to be flliped to 1, they should have higher score. \n\nSo now it turns into a mathmatical problem: give you 1024 green balls and you will choose 51 balls with 0.5 probability to chagne the green ball to red ball, after the operation, the 1024 balls are seperated into three parts, you know the exact red ball in first part(which is our training set), what's the expected red balls on the second part and third part?\n\nSadlly this problem is not easy to solve, also the variance seems high, which means even you calculated the mathematical expectation, it is quite unstable.\n\nHence I just do it with luck: \n1. for each sub group, if in train set there are more flliped values, I guess the fllipped values will be less in test set. vice versa. \n1. for each test sub group, I guess if the group is bigger, it should have more fllipped values. \n\nBased on the above rules and with some parameters trying, I quickly reached the public top1. But sadly to say, above rules are quite un-stable, hence in private set, they fails to 60+ position. Actually some other parameters works bad in public LB works quite well in privates....\n\nAnyway, interesting puzzels and fun time really. And many many thanks to those who shared quite a lot thoughts, like Vlad, Chris... Thank you!"},{"metadata":{"trusted":true},"cell_type":"code","source":"if sub.shape[0] == magicNum:\n    [].shape   \n\npreds=np.zeros(len(test))\ntrain_err=np.zeros(512)\ntest_err=np.zeros(512)\n\nfor i in range(512):  \n    \n    X = train[train['wheezy-copper-turtle-magic']==i].copy()\n    Y = X.pop('target').values\n    X_test = test[test['wheezy-copper-turtle-magic']==i].copy()\n\n    idx_train = X.index \n    idx_test = X_test.index\n    \n    X.reset_index(drop=True,inplace=True)\n    \n    X = X[cols].values             \n    X_test = X_test[cols].values\n\n    vt = VarianceThreshold(threshold=2).fit(X)\n    \n    X = vt.transform(X)         \n    X_test = vt.transform(X_test)\n    X_all = np.concatenate([X,X_test])\n    train_size = len(X)\n    test1_size = test[:131073][test[:131073]['wheezy-copper-turtle-magic']==i].shape[0]\n    compo_cnt = 6\n    for ii in range(30):\n        gmm = GaussianMixture(n_components=compo_cnt,init_params='random',covariance_type='full',max_iter=100,tol=1e-10,reg_covar=0.0001).fit(X_all)\n        labels = gmm.predict(X_all)\n        \n        cntStd = np.std([len(labels[labels==j]) for j in range(compo_cnt)])\n        #there are chances that the clustering doesn't converge, so we only choose the case that it clustered equally\n        #in which case, the sizes are 171,170,171,170,...\n        if round(cntStd,4) == 0.4714:\n            check_labels = labels[:train_size]\n            cvt_labels=np.zeros(len(labels))\n\n            #first get the perfect classification label\n            for iii in range(compo_cnt):\n                mean_val = Y[check_labels==iii].mean()\n                mean_val = 1 if mean_val > 0.5 else 0\n                cvt_labels[labels==iii] = mean_val\n            \n            #then try to predict the expected err for the test set\n            train_err[i] = len(Y[Y != cvt_labels[:train_size]])\n            if (train_err[i] >= 10) and (train_err[i] <= 15):\n                train_err[i] = 12.5\n            exp_err = max(0,(25 - train_err[i])/(train_size + test1_size))\n\n            for iii in range(compo_cnt):\n                mean_val = Y[check_labels==iii].mean()\n                mean_val = (1-exp_err) if mean_val > 0.5 else exp_err\n                cvt_labels[labels==iii] = mean_val\n\n            preds[idx_test] = cvt_labels[train_size:]\n            break\n\nsub['target'] = preds\nsub.to_csv('submission.csv',index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}