{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport scipy.io as sio\nimport os\nfrom pathlib import Path\nimport csv\nimport pickle\nfrom joblib import Parallel, delayed\nfrom tqdm import tqdm_notebook as tqdm\nimport pdb\nimport lightgbm as lgb\nimport xgboost as xgb\nimport random\nfrom sklearn.model_selection import GroupKFold, StratifiedKFold, KFold\nimport seaborn as sn\nfrom sklearn.manifold import TSNE\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.mixture import GaussianMixture\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.covariance import GraphicalLasso\nfrom sklearn.feature_selection import VarianceThreshold\nfrom scipy import linalg\n\nPATH_BASE = Path('../input')\nPATH_WORKING = Path('../working')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv(PATH_BASE/'train.csv')\ntest = pd.read_csv(PATH_BASE/'test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_mean_cov(x,y):\n    model = GraphicalLasso(max_iter=200)\n    ones = (y==1).astype(bool)\n    x2 = x[ones]\n    model.fit(x2)\n    p1 = model.precision_\n    m1 = model.location_\n    \n    onesb = (y==0).astype(bool)\n    x2b = x[onesb]\n    model.fit(x2b)\n    p2 = model.precision_\n    m2 = model.location_\n    \n    ms = np.stack([m1,m2])\n    ps = np.stack([p1,p2])\n    return ms,ps","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def projectMeans(means):\n    means[means>0]=1\n    means[means<=0]=-1\n    return means\n\ndef _compute_precision_cholesky(covariances, covariance_type):\n    estimate_precision_error_message = (\"Hell no\")\n    \n    if covariance_type in 'full':\n        n_components, n_features, _ = covariances.shape\n        precisions_chol = np.empty((n_components, n_features, n_features))\n        for k, covariance in enumerate(covariances):\n            try:\n                cov_chol = linalg.cholesky(covariance, lower=True)\n            except linalg.LinAlgError:\n                raise ValueError(estimate_precision_error_message)\n            precisions_chol[k] = linalg.solve_triangular(cov_chol,\n                                                         np.eye(n_features),\n                                                         lower=True).T\n    \n    return precisions_chol\n\ndef _estimate_gaussian_covariances_full(resp, X, nk, means, reg_covar):\n    n_components, n_features = means.shape\n    covariances = np.empty((n_components, n_features, n_features))\n    for k in range(n_components):\n        diff = X - means[k]\n        covariances[k] = np.dot(resp[:, k] * diff.T, diff) / nk[k]\n        covariances[k].flat[::n_features + 1] += reg_covar\n    return covariances\n\ndef _estimate_gaussian_parameters2(X, resp, reg_covar, covariance_type):\n    nk = resp.sum(axis=0) + 10 * np.finfo(resp.dtype).eps\n    means = np.dot(resp.T, X) / nk[:, np.newaxis]\n    means = projectMeans(means)\n\n    covariances = {\"full\": _estimate_gaussian_covariances_full}[covariance_type](resp, X, nk, means, reg_covar)\n    return nk, means, covariances\n\nclass GaussianMixture2(GaussianMixture):\n    def _m_step(self, X, log_resp):\n        resp = np.exp(log_resp)\n        sums = resp.sum(0)\n        if sums.max() - sums.min() > 2:\n            for i in range(3):\n                resp = len(X) * resp / resp.sum(0) / len(sums)\n                resp = resp/resp.sum(1)[:,None]\n        \n        n_samples, _ = X.shape\n        self.weights_, self.means_, self.covariances_ = (\n            _estimate_gaussian_parameters2(X, resp, self.reg_covar,\n                                          self.covariance_type))\n        \n        self.weights_ /= n_samples\n        self.precisions_cholesky_ = _compute_precision_cholesky(\n            self.covariances_, self.covariance_type)\n\n        \nrandom.seed(1234)\nnp.random.seed(1234)\nos.environ['PYTHONHASHSEED'] = str(1234)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cols = [c for c in train.columns if c not in ['id', 'target']]\ncols.remove('wheezy-copper-turtle-magic')\noof = np.zeros(len(train))\npreds = np.zeros(len(test))\n\nN_RAND_INIT = 3\nN_CLUST_OPT = 3\nN_TEST = 1\n\nall_acc = np.zeros((512, N_CLUST_OPT, N_RAND_INIT))\nall_roc = np.zeros((512, N_CLUST_OPT, N_RAND_INIT))\ncluster_cnt = np.zeros((512, N_CLUST_OPT, N_RAND_INIT))\ncluster_div = np.zeros((512, N_CLUST_OPT, N_RAND_INIT))\n\nj_selection = np.zeros(N_CLUST_OPT)\n\nfor i in tqdm(range(512)):\n    train2 = train[train['wheezy-copper-turtle-magic']==i]\n    test2 = test[test['wheezy-copper-turtle-magic']==i]\n    idx1 = train2.index; idx2 = test2.index\n    train2.reset_index(drop=True,inplace=True)\n    \n    sel = VarianceThreshold(threshold=1.5).fit(train2[cols])\n    train3 = sel.transform(train2[cols])\n    test3 = sel.transform(test2[cols])\n    \n    test_index = range(len(train3))\n\n    yf = train2['target']\n    ms, ps = get_mean_cov(train3,yf)\n    \n    cc_list = []\n    nc_list = 2*(np.array(range(N_CLUST_OPT)) + 2)\n    \n    for j in range(N_CLUST_OPT):\n        cc_list.append(['cluster_' + str(i) for i in range(nc_list[j])])\n    \n    gm_list = []\n    acc = np.zeros((N_CLUST_OPT, N_RAND_INIT))\n    res_list = []\n    ctc_list = []\n    \n    for j in range(N_CLUST_OPT):\n        \n        gm_list.append([])\n        res_list.append([])\n        ctc_list.append([])\n        \n        nc = nc_list[j]\n        cl = int(0.5*nc)\n        \n        for k in range(N_RAND_INIT):\n            ps_list = np.concatenate([ps]*cl, axis=0)\n\n            th_step = 100/(cl+1)\n            th_p = np.arange(th_step,99,th_step) + 0.5*(np.random.rand(cl) - 0.5)*th_step\n            th = np.percentile(ms,th_p)\n\n            ms_list = []\n            for t in range(cl):\n                ms_new = ms.copy()\n                ms_new[ms>=th[t]]=1\n                ms_new[ms<th[t]]=-1\n                ms_list.append(ms_new)\n            ms_list = np.concatenate(ms_list, axis=0)\n            \n            perm = np.random.permutation(nc)\n            ps_list = ps_list[perm]\n            ms_list = ms_list[perm]\n            \n            gm = GaussianMixture2(n_components=nc, init_params='random', covariance_type='full', tol=0.0001,reg_covar=0.1,\n                                  max_iter=5000, n_init=1, means_init=ms_list, precisions_init=ps_list, random_state=1234)\n            gm.fit(np.concatenate([train3,test3],axis = 0))\n            \n            hh = pd.DataFrame(gm.predict_proba(train3), columns = cc_list[j])\n            \n            exp_cluster_size = 0.975*len(train3)/nc\n            class_1 = (pd.DataFrame([yf]*nc).transpose().values * hh.values).sum(0)\n            class_0 = (pd.DataFrame([1-yf]*nc).transpose().values * hh.values).sum(0)\n            class_div = (0.5*np.abs(class_0[class_0 > 0.5*exp_cluster_size] - exp_cluster_size).mean()\\\n                       + 0.5*np.abs(class_1[class_1 > 0.5*exp_cluster_size] - exp_cluster_size).mean())/exp_cluster_size\n            \n            res = pd.concat([hh, yf.to_frame().reset_index(drop=True)], sort=False, axis=1)\n            \n            cluster_to_class = res.groupby('target').agg('mean').values.argmax(0)\n            cluster_cnt[i,j,k] = cluster_to_class.sum()\n            \n            res = pd.concat([hh, pd.DataFrame(cluster_to_class, index=cc_list[j], \n                                              columns=['target']).transpose()], sort=False, axis=0).\\\n                transpose().groupby('target').agg(sum).transpose()\n            \n            cluster_div[i,j,k] = class_div\n            res_list[j].append(res[1])\n            gm_list[j].append(gm)\n            ctc_list[j].append(cluster_to_class)\n            acc[j,k] = -class_div\n            all_acc[i,j,k] = (res.values.argmax(1) == yf.values).mean()\n            all_roc[i,j,k] = roc_auc_score(yf.values, res[1])\n    \n    best_j = acc.mean(1).argmax()\n    j_selection[best_j] += 1\n    \n    for k in np.argsort(acc[best_j,:])[-N_TEST:]:\n        res2 = pd.concat([pd.DataFrame(gm_list[best_j][k].predict_proba(test3), columns = cc_list[best_j]), \n                          pd.DataFrame(ctc_list[best_j][k], index=cc_list[best_j], \n                                       columns=['target']).transpose()], sort=False, axis=0).\\\n            transpose().groupby('target').agg(sum).transpose()\n        \n        oof[idx1] += res_list[best_j][k]/N_TEST\n        preds[idx2] += res2[1]/N_TEST\n    \n    if i%10==0: print('QMM scores CV =',round(roc_auc_score(train['target'],oof),5))\n\n# PRINT CV AUC\nauc = roc_auc_score(train['target'],oof)\nprint(j_selection)\nprint('Final QMM scores CV =',round(auc,5))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for j in range(N_CLUST_OPT):\n    print(np.all(cluster_cnt[:,j,:] == 0.5*nc_list[j]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Submit Predictions"},{"metadata":{"trusted":true},"cell_type":"code","source":"sub = pd.read_csv('../input/sample_submission.csv')\nsub['target'] = preds\nsub.to_csv('submission.csv',index=False)\n\nimport matplotlib.pyplot as plt\nplt.hist(preds,bins=100)\nplt.title('Final Test.csv predictions')\nplt.show()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}