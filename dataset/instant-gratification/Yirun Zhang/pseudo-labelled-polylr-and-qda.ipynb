{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Pseudo-Labelled PolyLR and QDA \n\nThis kernel shows the potential of adding quadratic polynomial features, a simple logistic regression can learn just like QDA. I also tested pseudo labelling and blending with QDA.\n\nThanks to Chris's great kernels [LR][1], [SVC][2], [probing][3], [pseudo labelling][5] and mhviraf's kernel [make_classification][4] which shows how the dataset was generated. Please also upvote those kernels.\n\n[1]: https://www.kaggle.com/cdeotte/logistic-regression-0-800\n[2]: https://www.kaggle.com/cdeotte/support-vector-machine-0-925\n[3]: https://www.kaggle.com/cdeotte/private-lb-probing-0-950\n[4]: https://www.kaggle.com/mhviraf/synthetic-data-for-next-instant-gratification\n[5]: https://www.kaggle.com/cdeotte/pseudo-labeling-qda-0-969"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np, pandas as pd\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import StandardScaler, PolynomialFeatures\nfrom sklearn.feature_selection import VarianceThreshold\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Load Data**"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Loading Train')\ntrain = pd.read_csv('../input/train.csv')\nprint('Loading Test')\ntest = pd.read_csv('../input/test.csv')\nprint('Finish')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**PolyLR and QDA**"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"oof = np.zeros(len(train))\npreds = np.zeros(len(test))\n\noof_QDA = np.zeros(len(train))\npreds_QDA = np.zeros(len(test))\n\ncols = [c for c in train.columns if c not in ['id', 'target', 'wheezy-copper-turtle-magic']]\n\nfor i in range(512):\n    train2 = train[train['wheezy-copper-turtle-magic']==i]\n    test2 = test[test['wheezy-copper-turtle-magic']==i]\n    idx1 = train2.index; idx2 = test2.index\n    train2.reset_index(drop=True,inplace=True)\n    \n    # Adding quadratic polynomial features can help linear model such as Logistic Regression learn better\n    poly = PolynomialFeatures(degree=2)\n    sc = StandardScaler()\n    data = pd.concat([pd.DataFrame(train2[cols]), pd.DataFrame(test2[cols])])\n    data2 = sc.fit_transform(poly.fit_transform(VarianceThreshold(threshold=1.5).fit_transform(data[cols])))\n    train3 = data2[:train2.shape[0]]; test3 = data2[train2.shape[0]:]\n    \n    data = pd.concat([pd.DataFrame(train2[cols]), pd.DataFrame(test2[cols])])\n    data2 = VarianceThreshold(threshold=1.5).fit_transform(data[cols])\n    train4 = data2[:train2.shape[0]]; test4 = data2[train2.shape[0]:]\n    \n    # STRATIFIED K FOLD\n    skf = StratifiedKFold(n_splits=11, random_state=42)\n    for train_index, test_index in skf.split(train2, train2['target']):\n\n        clf = LogisticRegression(solver='saga',penalty='l2',C=0.01,tol=0.001)\n        clf.fit(train3[train_index,:],train2.loc[train_index]['target'])\n        oof[idx1[test_index]] = clf.predict_proba(train3[test_index,:])[:,1]\n        preds[idx2] += clf.predict_proba(test3)[:,1] / skf.n_splits\n        \n        clf_QDA = QuadraticDiscriminantAnalysis(reg_param=0.5)\n        clf_QDA.fit(train4[train_index,:],train2.loc[train_index]['target'])\n        oof_QDA[idx1[test_index]] = clf_QDA.predict_proba(train4[test_index,:])[:,1]\n        preds_QDA[idx2] += clf_QDA.predict_proba(test4)[:,1] / skf.n_splits\n        \n    if i%64==0:\n        print(i, 'LR oof auc : ', round(roc_auc_score(train['target'][idx1], oof[idx1]), 5))\n        print(i, 'QDA oof auc : ', round(roc_auc_score(train['target'][idx1], oof_QDA[idx1]), 5))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**PolyLR with Pseudo Labelling**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# INITIALIZE VARIABLES\ntest['target'] = preds\noof = np.zeros(len(train))\npreds = np.zeros(len(test))\n\n# BUILD 512 SEPARATE MODELS\nfor k in range(512):\n    # ONLY TRAIN WITH DATA WHERE WHEEZY EQUALS I\n    train2 = train[train['wheezy-copper-turtle-magic']==k] \n    train2p = train2.copy(); idx1 = train2.index \n    test2 = test[test['wheezy-copper-turtle-magic']==k]\n    \n    # ADD PSEUDO LABELED DATA\n    test2p = test2[ (test2['target']<=0.01) | (test2['target']>=0.99) ].copy()\n    test2p.loc[ test2p['target']>=0.5, 'target' ] = 1\n    test2p.loc[ test2p['target']<0.5, 'target' ] = 0 \n    train2p = pd.concat([train2p,test2p],axis=0)\n    train2p.reset_index(drop=True,inplace=True)\n    \n     # FEATURE SELECTION AND ADDING POLYNOMIAL FEATURES\n    sel = VarianceThreshold(threshold=1.5).fit(train2p[cols])     \n    train3p = sel.transform(train2p[cols])\n    train3 = sel.transform(train2[cols])\n    test3 = sel.transform(test2[cols])   \n    poly = PolynomialFeatures(degree=2).fit(train3p)\n    train3p = poly.transform(train3p)\n    train3 = poly.transform(train3)\n    test3 = poly.transform(test3)\n    sc2 = StandardScaler()\n    train3p = sc2.fit_transform(train3p)\n    train3 = sc2.transform(train3)\n    test3 = sc2.transform(test3)\n        \n    # STRATIFIED K FOLD\n    skf = StratifiedKFold(n_splits=11, random_state=42, shuffle=True)\n    for train_index, test_index in skf.split(train3p, train2p['target']):\n        test_index3 = test_index[ test_index<len(train3) ] # ignore pseudo in oof\n        \n        # MODEL AND PREDICT WITH LR\n        clf = LogisticRegression(solver='saga',penalty='l2',C=0.01,tol=0.001)\n        clf.fit(train3p[train_index,:],train2p.loc[train_index]['target'])\n        oof[idx1[test_index3]] = clf.predict_proba(train3[test_index3,:])[:,1]\n        preds[test2.index] += clf.predict_proba(test3)[:,1] / skf.n_splits\n        \n    if k%64==0:  \n        print(k, 'LR2 oof auc : ', round(roc_auc_score(train['target'][idx1], oof[idx1]), 5))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**QDA with Pseudo Labelling (Chris's)**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# INITIALIZE VARIABLES\ntest['target'] = preds_QDA\noof_QDA2 = np.zeros(len(train))\npreds_QDA2 = np.zeros(len(test))\n\n# BUILD 512 SEPARATE MODELS\nfor k in range(512):\n    # ONLY TRAIN WITH DATA WHERE WHEEZY EQUALS I\n    train2 = train[train['wheezy-copper-turtle-magic']==k] \n    train2p = train2.copy(); idx1 = train2.index \n    test2 = test[test['wheezy-copper-turtle-magic']==k]\n    \n    # ADD PSEUDO LABELED DATA\n    test2p = test2[ (test2['target']<=0.01) | (test2['target']>=0.99) ].copy()\n    test2p.loc[ test2p['target']>=0.5, 'target' ] = 1\n    test2p.loc[ test2p['target']<0.5, 'target' ] = 0 \n    train2p = pd.concat([train2p,test2p],axis=0)\n    train2p.reset_index(drop=True,inplace=True)\n    \n    # FEATURE SELECTION (USE APPROX 40 OF 255 FEATURES)\n    sel = VarianceThreshold(threshold=1.5).fit(train2p[cols])     \n    train3p = sel.transform(train2p[cols])\n    train3 = sel.transform(train2[cols])\n    test3 = sel.transform(test2[cols])\n        \n    # STRATIFIED K FOLD\n    skf = StratifiedKFold(n_splits=11, random_state=42, shuffle=True)\n    for train_index, test_index in skf.split(train3p, train2p['target']):\n        test_index3 = test_index[ test_index<len(train3) ] # ignore pseudo in oof\n        \n        # MODEL AND PREDICT WITH QDA\n        clf_QDA2 = QuadraticDiscriminantAnalysis(reg_param=0.5)\n        clf_QDA2.fit(train3p[train_index,:],train2p.loc[train_index]['target'])\n        oof_QDA2[idx1[test_index3]] = clf_QDA2.predict_proba(train3[test_index3,:])[:,1]\n        preds_QDA2[test2.index] += clf_QDA2.predict_proba(test3)[:,1] / skf.n_splits\n       \n    if k%64==0:\n        print(k, 'QDA2 oof auc : ', round(roc_auc_score(train['target'][idx1], oof_QDA2[idx1]), 5))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('LR auc: ', round(roc_auc_score(train['target'], oof),5))\nprint('QDA auc: ', round(roc_auc_score(train['target'], oof_QDA2),5))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Find the Best Weights**\n\nLet's see if Pseudo-Labelled PolyLR can increase the performance (if w_best > 0)"},{"metadata":{"trusted":true},"cell_type":"code","source":"w_best = 0\noof_best = oof_QDA2\nfor w in np.arange(0,0.55,0.001):\n    oof_blend = w*oof+(1-w)*oof_QDA2\n    if (roc_auc_score(train['target'], oof_blend)) > (roc_auc_score(train['target'], oof_best)):\n        w_best = w\n        oof_best = oof_blend\n        print(w_best)\nprint('best weight: ', w_best)\nprint('auc_best: ', round(roc_auc_score(train['target'], oof_best), 5))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Blending with the Best Weights**"},{"metadata":{"trusted":true},"cell_type":"code","source":"sub = pd.read_csv('../input/sample_submission.csv')\nsub['target'] = w_best*preds + (1-w_best)*preds_QDA2\nsub.to_csv('submission.csv', index=False)\nsub.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Conclusion\n\nIn this kernel, I have tested PolyLR with pseudo labelling and blending with QDA. The results show that PolyLR does not increase the prediction performance of QDA since it has a very similar decision boundary to QDA, as illustrated by Chris in [Examples of Top 6 Classifiers][1].\n\n[1]: https://www.kaggle.com/c/instant-gratification/discussion/94179"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}