{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Getting Started with LightGBM\n---\n\nThis kernel is designed for both beginners and more experienced Kagglers as a quick introduction to this intriguing challenge. It will cover:\n\n* A brief overview of the data and the target\n* A quick explanation of the AUC metric used for scoring in this competition\n* A baseline gradient-boosting trees model\n* Some experiments to improve model performance\n* Two baseline submissions\n\nFirst we load in the dataset. The training data in this competition includes the target, and the row ID. We'll separate those out and take a look at the first few rows of train."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd \ntrain = pd.read_csv('../input/train.csv')\ntest = pd.read_csv('../input/test.csv')\ny = train['target'].values.flatten()\nids = train['id']\ntrain.drop(['target', 'id'], axis=1, inplace=True)\ntest.drop('id', axis=1, inplace=True)\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A few things stand out. Firstly, the Kaggle team obviously had some fun when choosing the column names! They presumably have left some clues in here for us. The last word for each name in particular look like they are hinting at something. Words like 'sorted', 'noise' and 'gaussian' are all pertinent to machine learning. For the benefit of non-native English speakers who may be confused by the vocabulary, the four words in each column names mostly consist of:\n\n* An adjective\n* A colour\n* An animal\n* A machine-learning term, Kaggle term, or nonsense word ('pembus', fepid')\n\nFinding exceptions to this general rule may help you identify valuable features!\n\nThe variables mostly lie within a small range around 0, implying they have been already scaled with a tool like sklearn's `StandardScaler()`.\n\nThere don't seem to be any missing values in the first few rows. How large is the total training data, and how much of it is missing?"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"print('train consists of {} rows and {} columns.'.format(train.shape[0], train.shape[1]))\nprint('train contains {} missing values.'.format(train.isna().sum().sum()))\nprint('train is {}% incomplete.'.format(100*train.isna().sum().sum()/(train.shape[0]*train.shape[1])))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are no missing values to be worked around or [imputed](https://en.wikipedia.org/wiki/Imputation_(statistics). That simplifies things a little. Now we can make a baseline model to see how accurately we can make predictions without changing the data in any way. But first, we should understand how the scoring for this competition works.\n\n---\n# AUC: Area-Under-the-Curve\n\n![](https://developers.google.com/machine-learning/crash-course/images/ROCCurve.svg)\n\nReceiver-Operating-Characteristic: Area-Under-the-Curve, ROC-AUC or just AUC for short, is a common metric for assessing the performance of classification models. Note that it's improper to say AUC measures the 'accuracy' of a model, as 'accuracy' has a specific definition in statistics. The basic thing to know about AUC is that it scores your overall predictions between 0 and 1. 1 means your model predicted everything perfectly. 0.5 means it predicted no better than random selection. Anything below 0.5 and something is seriously wrong with your model, as it could be predicting opposite classes!\n\nThere are many explanations of AUC online and [this is a good place to start](https://towardsdatascience.com/understanding-auc-roc-curve-68b2303cc9c5). It can be easier to understand when you look at how classification prediction works.\n\nWhen we make a prediction with a model, we output a number between 0 and 1. Say our model predicts 0.4872 - what does this number mean? Our output has to be a zero or one, so the prediction has to be evaluated against a threshold, where the prediction becomes 1 if it is equal to or larger than the threshold. You might typically expect this to be 0.5, which makes mathematical sense. In this case, the final prediction would be 0, not 0.4872. However, this may not be best threshold for our model. If the actual value for the row was 1, we have predicted a false negative. Had we used a threshold of 0.4 to identify positive cases, our prediction would have been successful. So what is the best threshold to use overall?\n\nThis actually depends on the nature of the task, and whether you consider false positives (type 1 errors) or false negatives (type 2 errors) to be worse. For example, say you were working on a model that uses ground radar signatures to estimate the probability that a buried object is a land mine. Which type of error would have more serious consequences, a false positive or a false negative? For most situations it isn't so evident which type of error is worse so we use a metric that combines the rates of both, like AUC. AUC measures the ratio of the True Positive Rate and the False Positive Rate over the entire threshold range and computes the total area under this curve, hence the name.\n\n---\n# Starter Model\n\nLet's see how well we can model the data without modification and using default model parameters. This will give us a quick baseline score we can use to evaluate any new models against. The Out-Of-Fold (OOF) score will tell us how accurate the model was on the validation data, averaged across the 5 folds."},{"metadata":{"trusted":true},"cell_type":"code","source":"import lightgbm as lgb\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import roc_auc_score\n\nN_FOLD = 5\nfolds = StratifiedKFold(n_splits=N_FOLD, shuffle=True, random_state=42)\n\noof = np.zeros(len(train))\nimportances = np.zeros(train.shape[1])\nX = train.values\npreds = np.zeros(len(test))\n\nfor train_idx, valid_idx in folds.split(X, y):\n\n    X_train, X_valid = X[train_idx, :], X[valid_idx, :]\n    y_train, y_valid = y[train_idx], y[valid_idx]\n    \n    model = lgb.LGBMClassifier(n_estimators=10000, eval_metric='auc')\n    model.fit(X_train, y_train, eval_set=[(X_train, y_train), (X_valid, y_valid)], verbose=200,\n                      early_stopping_rounds=250, eval_metric='auc')\n    val_preds = model.predict(X_valid)\n    importances += model.feature_importances_/N_FOLD\n    oof[valid_idx] = val_preds\n\nAUC_OOF = round(roc_auc_score(y, oof), 4)\nprint('Model ensemble OOF AUC score: {}'.format(AUC_OOF))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As stated above, an AUC of 0.5 indicates that a model performs as well as random guessing. So our model performance isn't very good at all! Perhaps we should take a closer look at the features.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\n#make lgb feature importance df\nfeature_df = pd.DataFrame({'feature' : train.columns,\n                             'importance' : importances})\nfeature_df = feature_df.sort_values('importance', ascending=False)\n    \n#plot feature importances\nN_ROWS = 50\nplot_height = int(np.floor(len(feature_df.iloc[:N_ROWS, :])/5))\nplt.figure(figsize=(12, plot_height));\nsns.barplot(x='importance', y='feature', data=feature_df.iloc[:N_ROWS, :]);\nplt.title('LightGBM Feature Importance');\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The column `'wheezy-copper-turtle-magic'` is our most important feature but nothing else immediately stands out. \n\n# Cardinality\n---\n\nIt was mentioned earlier that the data seem to have been scaled before they were given to us. What if some of the variables were orginally integer values which indicated categorical data? We can explore this by examining the **cardinality** of our data. Cardinality is simply the number of unique values that are present in a variable. For a truly continuous variable we would expect the cardinality to be very high, equal or almost equal to the number of total rows in the dataframe. If a continuous-looking variable only has, say, 10 unique values, it would make more sense to treat it as a categorical variable and see if this improves the model."},{"metadata":{"trusted":true},"cell_type":"code","source":"cards = []\nfor i in range(0, train.shape[1]):\n    cards.append(len(np.unique(train.iloc[:, i].values)))\ncards = np.asarray(cards)\n\ncard_df = pd.DataFrame({'feature' : train.columns,\n                       'cardinality' : cards})\n\ncard_df.sort_values('cardinality', inplace=True)\ncard_df.head()        ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Only one feature, `'wheezy-copper-turtle-magic'` displays low cardinality. So we can try encoding it as an integer variable with `pd.factorize()`, supply it to our model as a categorical variable and see if it improves performance. It's important to combine the train and test data before factorising it so that the corresponding numbers match up in the two datasets."},{"metadata":{"trusted":true},"cell_type":"code","source":"temp = pd.concat([train['wheezy-copper-turtle-magic'], test['wheezy-copper-turtle-magic']])\ntemp = pd.factorize(temp)[0]\ntrain['wheezy-copper-turtle-magic'] = temp[:len(train)]\ntest['wheezy-copper-turtle-magic'] = temp[len(train):]\ntrain['wheezy-copper-turtle-magic'] = train['wheezy-copper-turtle-magic'].astype('category')\ntest['wheezy-copper-turtle-magic'] = test['wheezy-copper-turtle-magic'].astype('category')\ncat_feature_index = [train.columns.get_loc('wheezy-copper-turtle-magic')]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now compare the model performance:"},{"metadata":{"trusted":true},"cell_type":"code","source":"oof = np.zeros(len(train))\nimportances = np.zeros(train.shape[1])\nX = train.values\n\nfor train_idx, valid_idx in folds.split(X, y):\n\n    X_train, X_valid = X[train_idx, :], X[valid_idx, :]\n    y_train, y_valid = y[train_idx], y[valid_idx]\n    \n    model = lgb.LGBMClassifier(n_estimators=10000, eval_metric='auc')\n    model.fit(X_train, y_train, eval_set=[(X_train, y_train), (X_valid, y_valid)], verbose=200,\n                      early_stopping_rounds=250, eval_metric='auc', categorical_feature=cat_feature_index)\n    val_preds = model.predict(X_valid)\n    importances += model.feature_importances_/N_FOLD\n    oof[valid_idx] = val_preds\n\nAUC_OOF = round(roc_auc_score(y, oof), 4)\nprint('Model ensemble OOF AUC score: {}'.format(AUC_OOF))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"---\nThis has improved the model performance significantly. How can we further optimise the model?\n\n# Parameter Tuning\n\nAn important part of optimising model performance is selecting the best parameters for it, such as the learning rate and maximum tree depth. You can try modifying these individually but there are a number of tools to help adjust model parameter settings. For this kernel, I'm recycling a function of mine that uses the Hyperopt package to automatically tune parameters. The code is in the code block below."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"#import required packages\nfrom hyperopt import hp, tpe, Trials, STATUS_OK\nfrom hyperopt.fmin import fmin\nfrom hyperopt.pyll.stochastic import sample\nimport gc #garbage collection\n#optional but advised\nimport warnings\nwarnings.filterwarnings('ignore')\n\n#GLOBAL HYPEROPT PARAMETERS\nNUM_EVALS = 1000 #number of hyperopt evaluation rounds\nN_FOLDS = 5 #number of cross-validation folds on data in each evaluation round\n\n#LIGHTGBM PARAMETERS\nLGBM_MAX_LEAVES = 2**9 #maximum number of leaves per tree for LightGBM\nLGBM_MAX_DEPTH = 25 #maximum tree depth for LightGBM\nEVAL_METRIC_LGBM_REG = 'mae' #LightGBM regression metric. Note that 'rmse' is more commonly used \nEVAL_METRIC_LGBM_CLASS = 'auc'#LightGBM classification metric\n\ndef quick_hyperopt(data, labels, num_evals=NUM_EVALS, Class=True, cat_features=None):\n    \n    print('Running {} rounds of LightGBM parameter optimisation:'.format(num_evals))\n    #clear space\n    gc.collect()\n\n    integer_params = ['max_depth',\n                     'num_leaves',\n                      'max_bin',\n                     'min_data_in_leaf',\n                     'min_data_in_bin']\n\n    def objective(space_params):\n\n        #cast integer params from float to int\n        for param in integer_params:\n            space_params[param] = int(space_params[param])\n\n        #extract nested conditional parameters\n        if space_params['boosting']['boosting'] == 'goss':\n            top_rate = space_params['boosting'].get('top_rate')\n            other_rate = space_params['boosting'].get('other_rate')\n            #0 <= top_rate + other_rate <= 1\n            top_rate = max(top_rate, 0)\n            top_rate = min(top_rate, 0.5)\n            other_rate = max(other_rate, 0)\n            other_rate = min(other_rate, 0.5)\n            space_params['top_rate'] = top_rate\n            space_params['other_rate'] = other_rate\n\n        subsample = space_params['boosting'].get('subsample', 1.0)\n        space_params['boosting'] = space_params['boosting']['boosting']\n        space_params['subsample'] = subsample\n\n        if Class:                \n            if cat_features is not None:\n                cv_results = lgb.cv(space_params, train, nfold = N_FOLDS, stratified=True, categorical_feature=cat_features,\n                                    early_stopping_rounds=100, metrics=EVAL_METRIC_LGBM_CLASS, seed=42)\n                best_loss = 1 - cv_results['auc-mean'][-1]\n            else:\n                cv_results = lgb.cv(space_params, train, nfold = N_FOLDS, stratified=True,\n                                    early_stopping_rounds=100, metrics=EVAL_METRIC_LGBM_CLASS, seed=42)\n                best_loss = 1 - cv_results['auc-mean'][-1]\n\n        else:\n            if cat_features is not None:\n                cv_results = lgb.cv(space_params, train, nfold = N_FOLDS, stratified=False,\n                                    early_stopping_rounds=100, metrics=EVAL_METRIC_LGBM_REG, seed=42)\n                best_loss = cv_results['l1-mean'][-1] #'l2-mean' for rmse\n            else:\n                cv_results = lgb.cv(space_params, train, nfold = N_FOLDS, stratified=True,\n                                    early_stopping_rounds=100, metrics=EVAL_METRIC_LGBM_REG, seed=42)\n                best_loss = 1 - cv_results['auc-mean'][-1]\n\n        return{'loss':best_loss, 'status': STATUS_OK }\n\n    if cat_features is not None:\n        train = lgb.Dataset(data, labels, categorical_feature=cat_features)\n    else:\n         train = lgb.Dataset(data, labels)\n\n    #integer and string parameters, used with hp.choice()\n    boosting_list = [{'boosting': 'gbdt',\n                      'subsample': hp.uniform('subsample', 0.5, 1)},\n                     {'boosting': 'goss',\n                      'subsample': 1.0,\n                     'top_rate': hp.uniform('top_rate', 0, 0.5),\n                     'other_rate': hp.uniform('other_rate', 0, 0.5)}] #if including 'dart', make sure to set 'n_estimators'\n\n    if Class:\n        metric_list = ['auc'] #modify as required for other classification metrics\n        objective_list = ['binary', 'cross_entropy']\n\n    else:\n        metric_list = ['MAE', 'RMSE'] \n        objective_list = ['huber', 'gamma', 'fair', 'tweedie']\n\n\n    space ={'boosting' : hp.choice('boosting', boosting_list),\n            'num_leaves' : hp.quniform('num_leaves', 2, LGBM_MAX_LEAVES, 1),\n            'max_depth': hp.quniform('max_depth', 2, LGBM_MAX_DEPTH, 1),\n            'max_bin': hp.quniform('max_bin', 32, 255, 1),\n            'min_data_in_leaf': hp.quniform('min_data_in_leaf', 1, 256, 1),\n            'min_data_in_bin': hp.quniform('min_data_in_bin', 1, 256, 1),\n            'min_gain_to_split' : hp.quniform('min_gain_to_split', 0.1, 5, 0.01),\n            'lambda_l1' : hp.uniform('lambda_l1', 0, 5),\n            'lambda_l2' : hp.uniform('lambda_l2', 0, 5),\n            'learning_rate' : hp.loguniform('learning_rate', np.log(0.005), np.log(0.2)),\n            'metric' : hp.choice('metric', metric_list),\n            'objective' : hp.choice('objective', objective_list),\n            'feature_fraction' : hp.quniform('feature_fraction', 0.5, 1, 0.01),\n            'bagging_fraction' : hp.quniform('bagging_fraction', 0.5, 1, 0.01)\n        }\n\n    trials = Trials()\n    best = fmin(fn=objective,\n                space=space,\n                algo=tpe.suggest,\n                max_evals=num_evals, \n                trials=trials)\n\n    #fmin() will return the index of values chosen from the lists/arrays in 'space'\n    #to obtain actual values, index values are used to subset the original lists/arrays\n    best['boosting'] = boosting_list[best['boosting']]['boosting']#nested dict, index twice\n    best['metric'] = metric_list[best['metric']]\n    best['objective'] = objective_list[best['objective']]\n\n    #cast floats of integer params to int\n    for param in integer_params:\n        best[param] = int(best[param])\n\n    print('{' + '\\n'.join('{}: {}'.format(k, v) for k, v in best.items()) + '}')\n    return(best)    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lgbm_params = quick_hyperopt(train, y, 25, cat_features=cat_feature_index)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"With the optimised parameters we can train our final models, make the predictions on test and submit."},{"metadata":{"trusted":true},"cell_type":"code","source":"oof = np.zeros(len(train))\nX = train.values\npreds = np.zeros(len(test))\n\nfor train_idx, valid_idx in folds.split(X, y):\n\n    X_train, X_valid = X[train_idx, :], X[valid_idx, :]\n    y_train, y_valid = y[train_idx], y[valid_idx]\n    \n    train_dataset = lgb.Dataset(X_train, y_train, categorical_feature=cat_feature_index)\n    val_dataset = lgb.Dataset(X_valid, y_valid, categorical_feature=cat_feature_index)\n\n    model = lgb.train(lgbm_params, train_dataset, valid_sets=[train_dataset, val_dataset], verbose_eval=200,\n                      num_boost_round=10000, early_stopping_rounds=250, categorical_feature=cat_feature_index)\n    val_preds = model.predict(X_valid, num_iteration=model.best_iteration)\n    oof[valid_idx] = val_preds\n    preds += model.predict(test, num_iteration=model.best_iteration)/N_FOLD\n\nAUC_OOF = round(roc_auc_score(y, oof), 4)\nprint('Model ensemble OOF AUC score: {}'.format(AUC_OOF))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub = pd.read_csv('../input/sample_submission.csv')\nsub['target'] = preds\nsub.to_csv('initial_sub.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Wordplay Experiment\n\nAs a last idea, let's assume that Kaggle's columns names are designed to give us obvious hints. What happens if we drop all the columns with the words 'noise', 'distraction' and 'discard'?"},{"metadata":{"trusted":true},"cell_type":"code","source":"keep_cols = train.columns\nremove_words = ['noise', 'distraction', 'discard']\nfor keyword in remove_words:\n    keep_cols = [x for x in keep_cols if keyword not in keep_cols]\n    \ntrain_2 = train[keep_cols]\ntest_2 = test[keep_cols]\ncat_feature_index = [train_2.columns.get_loc('wheezy-copper-turtle-magic')]\n\n#get new optimised parameters\nfinal_params = quick_hyperopt(train_2, y, 25, cat_features=cat_feature_index)\n\noof = np.zeros(len(train))\nX = train_2.values\npreds = np.zeros(len(test))\n\nfor train_idx, valid_idx in folds.split(X, y):\n\n    X_train, X_valid = X[train_idx, :], X[valid_idx, :]\n    y_train, y_valid = y[train_idx], y[valid_idx]\n    \n    train_dataset = lgb.Dataset(X_train, y_train, categorical_feature=cat_feature_index)\n    val_dataset = lgb.Dataset(X_valid, y_valid, categorical_feature=cat_feature_index)\n\n    model = lgb.train(final_params, train_dataset, valid_sets=[train_dataset, val_dataset], verbose_eval=200,\n                      num_boost_round=10000, early_stopping_rounds=250, categorical_feature=cat_feature_index)\n    val_preds = model.predict(X_valid, num_iteration=model.best_iteration)\n    oof[valid_idx] = val_preds\n    preds += model.predict(test_2, num_iteration=model.best_iteration)/N_FOLD\n\nAUC_OOF = round(roc_auc_score(y, oof), 4)\nprint('Model ensemble OOF AUC score: {}'.format(AUC_OOF))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Apparently the clues aren't that obvious! So what happens if we remove the columns with positive words like 'important', 'grandmaster' and 'expert'?"},{"metadata":{"trusted":true},"cell_type":"code","source":"keep_cols = train.columns\nremove_words = ['important', 'grandmaster', 'expert']\nfor keyword in remove_words:\n    keep_cols = [x for x in keep_cols if keyword not in keep_cols]\n    \ntrain_3 = train[keep_cols]\ntest_3 = test[keep_cols]\ncat_feature_index = [train_3.columns.get_loc('wheezy-copper-turtle-magic')]\n\n#get new optimised parameters\nfinal_params = quick_hyperopt(train_3, y, 25, cat_features=cat_feature_index)\n\noof = np.zeros(len(train))\nX = train_3.values\npreds = np.zeros(len(test))\n\nfor train_idx, valid_idx in folds.split(X, y):\n\n    X_train, X_valid = X[train_idx, :], X[valid_idx, :]\n    y_train, y_valid = y[train_idx], y[valid_idx]\n    \n    train_dataset = lgb.Dataset(X_train, y_train, categorical_feature=cat_feature_index)\n    val_dataset = lgb.Dataset(X_valid, y_valid, categorical_feature=cat_feature_index)\n\n    model = lgb.train(final_params, train_dataset, valid_sets=[train_dataset, val_dataset], verbose_eval=200,\n                      num_boost_round=10000, early_stopping_rounds=250, categorical_feature=cat_feature_index)\n    val_preds = model.predict(X_valid, num_iteration=model.best_iteration)\n    oof[valid_idx] = val_preds\n    preds += model.predict(test_3, num_iteration=model.best_iteration)/N_FOLD\n\nAUC_OOF = round(roc_auc_score(y, oof), 4)\nprint('Model ensemble OOF AUC score: {}'.format(AUC_OOF))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub = pd.read_csv('../input/sample_submission.csv')\nsub['target'] = preds\nsub.to_csv('second_sub.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Looks like removing the positive descriptions actually improved results! Hopefully you can find some clues of your own in the variable names. \n\nI hope this kernel is of use to other Kagglers, all comments and questions are welcome. Good luck with the competition!"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}