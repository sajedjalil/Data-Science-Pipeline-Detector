{"cells":[{"metadata":{},"cell_type":"markdown","source":"This kernel is a combination of the following kernels:  \nhttps://www.kaggle.com/tunguz/instant-eda  \nhttps://www.kaggle.com/cdeotte/logistic-regression-0-800  \nhttps://www.kaggle.com/cdeotte/support-vector-machine-0-925  \nhttps://www.kaggle.com/cdeotte/pseudo-labeling-qda-0-969  \nhttps://www.kaggle.com/speedwagon/quadratic-discriminant-analysis  \n  \nDue Credits to [Bojan Tunguz (@tunguz)](https://www.kaggle.com/tunguz), \n[Chris Deotte (@cdeotte)](https://www.kaggle.com/cdeotte), \n[Vladislav Bakhteev (@speedwagon)](https://www.kaggle.com/speedwagon).  \nAll these kernels made me understand how to approach this competition and make a successful submission scoring decent score.  \nIf you find this kernel helpful, i would suggest upvoting all the above kernels.  "},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\n#Essential Libraries\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nfrom tqdm import tqdm_notebook as tqdm\n\n#Scikit-Learn Helpers\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.feature_selection import VarianceThreshold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import roc_auc_score\n\n#Models\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n\n#Misc \nimport warnings\nwarnings.filterwarnings(\"ignore\")\nfrom IPython.display import display\npd.set_option(\"display.max_columns\", None)\n\nscaler_type = \"no\"\nN_SPLITS = 10","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"test = pd.read_csv(\"../input/test.csv\")\ntrain = pd.read_csv(\"../input/train.csv\")\nsample_submission = pd.read_csv(\"../input/sample_submission.csv\")\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['target'].value_counts()\n# theres no class imbalance here, infact both the classes are equally represented","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"t_des = train.describe()\nt_des","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.hist(t_des.loc['mean'],bins=100);\n#Some/one variable seems to be having mean > 250","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"display(t_des.loc['mean'][t_des.loc['mean'] > 250])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.hist(t_des.loc['std'],bins=100);\n#Some/one variable seems to be having standard deviation of > ~140","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Culprit variable for abnormal mean and standard deviation\ndisplay(t_des.loc['std'][t_des.loc['std'] > 140])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nt_des['wheezy-copper-turtle-magic']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,5))\nplt.hist(train['wheezy-copper-turtle-magic'].values,bins=900);\n# this feature doesnt follow normal distribution","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def scale(train,test):\n    traintest = pd.concat([train,test],axis=0,ignore_index=True).reset_index(drop=True)\n    cols = [c for c in train.columns if c not in ['id','target','wheezy-copper-turtle-magic']]\n    scaler = StandardScaler()\n    traintest[cols] = scaler.fit_transform(traintest[cols])\n    train['wheezy-copper-turtle-magic'] = train['wheezy-copper-turtle-magic'].astype('category')\n    train = traintest[:train.shape[0]].reset_index(drop=True)\n    test = traintest[train.shape[0]:].reset_index(drop=True)\n    return train,test\nif scaler_type == \"standard\":\n    train,test = scale(train,test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Are our variable gaussians?"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,15))\n\n#DISTPLOT FOR FIRST 8 VARIABLES\nfor i in range(8):\n    plt.subplot(3,3,i+1)\n    sns.distplot(train.iloc[:,i+1],bins=200)\n    plt.title(train.columns[i+1])\n    plt.xlabel('')\n#     plt.title(train.columns[i])\n\n#DISPLOT FOR GAUSSIAN (FOR COMPARISON)\nplt.subplot(3,3,9)\nstd = round(np.std(train.iloc[:,8]),2)\ndata = np.random.normal(0,std,train.shape[0])\nsns.distplot(data,bins=200)\nplt.title(\"Gaussian with mean=0,std=\"+str(std)+\"\\n(\"+train.columns[8]+\")\")\nplt.xlabel('')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Normality Plots\n* To verify our doubts we'll use Normality Plots.\n* **It indicates whether the variables are gaussian or not. If the are gaussian, it plots out a straight line**; like in the 9th figure below.\n* However in our case, for the initial 8 variables, it plots our **piecewise straight lines indicating we may have Gaussian Mixture Models** ie. Each variable is a sum of multiple gaussians.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy import stats\nplt.figure(figsize=(15,15))\n\n#NORMALITY PLOTS FOR FIRST 8 VARIABLES\nfor i in range(8):\n    plt.subplot(3,3,i+1)\n    stats.probplot(train.iloc[:,i+1],plot=plt)\n    plt.title(train.columns[i+1])\n    plt.xlabel('')\n\n#NORMALITY PLOT FOR GAUSSIAN\nplt.subplot(3,3,9)\nstd = round(np.std(train.iloc[:,8]),2)\ndata = np.random.normal(0,std,train.shape[0])\nstats.probplot(data,plot=plt)\nplt.title(\"Gaussian with mean=0,std=\"+str(std)+\"\\n(\"+train.columns[8]+\")\")\nplt.xlabel('')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Variables with Partial Dataset are Gaussian\nIf we look at just partial dataset where 'wheezy-copper-turtle-magic' = k for 0 <= k < 512, the variables seem to be normal (see below plots). This maybe because kaggle combined 512 datasets into 1 for the competition."},{"metadata":{"trusted":true},"cell_type":"code","source":"train0 = train[train['wheezy-copper-turtle-magic'] == 0]\nplt.figure(figsize=(15,15))\n\n#DISTPLOT FOR FIRST 8 VARIABLES\nfor i in range(8):\n    plt.subplot(3,3,i+1)\n    sns.distplot(train0.iloc[:,i+1],bins=10)\n    plt.title(train0.columns[i+1])\n    plt.xlabel('')\n\n#DISPLOT FOR GAUSSIAN (FOR COMPARISON)\nplt.subplot(3,3,9)\nstd0 = round(np.std(train0.iloc[:,8]),2)\ndata0 = np.random.normal(0,std0,train0.shape[0])\nsns.distplot(data0,bins=10)\nplt.title(\"Gaussian with mean=0,std=\"+str(std)+\"\\n(\"+train0.columns[8]+\")\")\nplt.xlabel('')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Normality Plots for Partial Dataset\nplt.figure(figsize=(15,15))\n\n#NORMALITY PLOTS FOR FIRST 8 VARIABLES OF PARTIAL DATASET\nfor i in range(8):\n    plt.subplot(3,3,i+1)\n    stats.probplot(train0.iloc[:,i+1],plot=plt)\n    plt.title(train0.columns[i+1])\n    plt.xlabel('')\n\n#NORMALITY PLOT FOR GAUSSIAN\nplt.subplot(3,3,9)\nstats.probplot(data,plot=plt)\nplt.title(\"Gaussian with mean=0,std=\"+str(std)+\"\\n(\"+train0.columns[8]+\")\")\nplt.xlabel('')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Logistic Regression without interactions\nNone of the 256 variables have correlation with the target greater than absolute value 0.04. Therefore if you use LR to model target you score a low CV 0.530 because LR treats the variables as independent and doesn't utilize interactions."},{"metadata":{"trusted":true},"cell_type":"code","source":"# %%time\n# ###########################################Logistic Regression Baseline########################\n# features_to_use = [c for c in train.columns if c not in ['id','target']]\n# # X_train = train[features_to_use]\n# # y_train = train['target']\n# # lr = LogisticRegression(C=1.0,solver='sag')\n# # cv_score = cross_val_score(lr, X_train, y_train, scoring='roc_auc',cv=3)\n# # print(cv_score)\n# # print(np.mean(cv_score))\n\n# # %%time\n# kfold = KFold(n_splits=N_SPLITS,shuffle=True,random_state=42)\n# oof = np.zeros(train.shape[0]) ## Out Of Fold (predictions)\n# pred = 0\n# for fold_,(trn_idx,val_idx) in enumerate(kfold.split(X=train,y=train['target'])):\n#     print(\"fold {}\".format(fold_+1))\n#     X_train, y_train = train.iloc[trn_idx][features_to_use], train['target'].iloc[trn_idx]\n#     X_val, y_val = train.iloc[val_idx][features_to_use], train['target'].iloc[val_idx]\n#     lg = LogisticRegression(C=1.0,solver='sag')\n#     lg.fit(X_train,y_train)\n#     val_pred = lg.predict_proba(X_val)[:,1]\n#     #Each row/datapoint would require a prediction on both 0 and 1. \n#     #For example datapoint1 has 80% likelihood to belong to 0, and 20% belonging to 1. the output would be (0.8,0.2). \n#     #you need to access prediciton[:,1] to get the second column if u want the prediction for 1. \n#     #In general access prediction[:,k] if you want likelihood of the k th class\n#     oof[val_idx] = val_pred\n#     pred += lg.predict_proba(test[features_to_use])[:,1]/N_SPLITS\n#     print(roc_auc_score(y_val,val_pred))\n# print(roc_auc_score(train['target'].values,oof))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Interactions\nWith below visualizations, we can see the interactions between wheezy-copper-turtle-magic and the other variables.\n\nEach of the variables by itself cannot predict the target well, but when 'wheezy-copper-turtle-magic' equals a specific value,then the variables can predict the target well.\n\nFor Example, \nGenerally there seems to be a **NEUTRAL CORRELATION(indicating that the variables are unrelated**) b/w zippy and the target variable.\nBut when wheezy-copper-turtle-magic=0, there is positive correlation b/w zippy variable and the target variable(**Positive Correlation is indicated by the fact that the orange histogram representing target=1 is shifted to the right(positive x-axis) relative to the blue histogram representing target=0**)  \nNote: **Positive correlation means that both the variable move in the same direction**  \nThus when wheezy-magic = 0, **positive values of zippy are associated with more target=1 and negative values of zippy are associated with more target=0. Hence positive correlation.**\n\nSimilarly, \nGenerally there seems to be a neutral correlation b/w chewy and the target variable.\nBut when wheezy-copper-turtle-magic=0, there is negative correlation b/w chewy variable and the target variable(**Negative Correlation is indicated by the fact that the orange histogram representing target=1 is shifted to the left(negative x-axis)relative to the blue histogram representing target=0**)  \nNote: **Negative correlation means that when one variables value increases other one's value decreases. **"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,5))\n\n# PLOT ALL ZIPPY\nplt.subplot(1,2,1)\nsns.distplot(train[ (train['target']==0) ]['zippy-harlequin-otter-grandmaster'], label = 't=0')\nsns.distplot(train[ (train['target']==1) ]['zippy-harlequin-otter-grandmaster'], label = 't=1')\nplt.title(\"Without interaction, zippy has neutral correlation with the target variable \\n (showing all rows)\")\nplt.xlim((-4,4))\nplt.legend()\n\n# PLOT ZIPPY WHERE WHEEZY-MAGIC=0\nplt.subplot(1,2,2)\nsns.distplot(train[(train['target']==0) & (train['wheezy-copper-turtle-magic']==0)]\n             ['zippy-harlequin-otter-grandmaster'],label='t=0')\nsns.distplot(train[(train['target']==1) & (train['wheezy-copper-turtle-magic']==0)]\n             ['zippy-harlequin-otter-grandmaster'],label='t=1')\nplt.xlim(-2,2)\nplt.title(\"With Interaction, zippy has positively correlation with target variable \\n(showing only rows where 'wheezy-copper-turtle-magic'=0)\")\nplt.legend()\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,5))\n\n# PLOT ALL ZIPPY\nplt.subplot(1,2,1)\nsns.distplot(train[ (train['target']==0) ]['chewy-lime-peccary-fimbus'], label = 't=0')\nsns.distplot(train[ (train['target']==1) ]['chewy-lime-peccary-fimbus'], label = 't=1')\nplt.title(\"Without interaction, chewy has neutral correlation with the target variable \\n (showing all rows)\")\nplt.xlim((-4,4))\nplt.legend()\n\n# PLOT ZIPPY WHERE WHEEZY-MAGIC=0\nplt.subplot(1,2,2)\nsns.distplot(train[(train['target']==0) & (train['wheezy-copper-turtle-magic']==0)]\n             ['chewy-lime-peccary-fimbus'],label='t=0')\nsns.distplot(train[(train['target']==1) & (train['wheezy-copper-turtle-magic']==0)]\n             ['chewy-lime-peccary-fimbus'],label='t=1')\nplt.xlim(-2,2)\nplt.title(\"With Interaction, zippy has negative correlation with the target variable \\n (showing only rows where 'wheezy-copper-turtle-magic'=0)\")\nplt.legend()\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Logistic Regression with interactions\nUsing LR, we can build a model that includes interactions by building 512 separate models. We will build one LR (logistic regression) model for each value of wheezy-copper-turtle-magic and use the appropriate model to predict test.csv. This scores CV 0.805 and LB 0.808"},{"metadata":{"trusted":true},"cell_type":"code","source":"# %%time\n# #Bojan's Way\n# #################Logistic Regression with interaction##########################\n# #initialize-variables\n# cols = [c for c in train.columns if c not in ['id','target','wheezy-copper-turtle-magic']]\n# target = train['target']\n# NSPLITS = 25\n# oof_lr = np.zeros(train.shape[0])\n# pred_lr = np.zeros(test.shape[0])\n# # \n# skf = StratifiedKFold(n_splits=NSPLITS,random_state=42,shuffle=True)\n\n# for fold_,(train_idx, val_idx) in enumerate(skf.split(train.values,target.values)):\n#     print('Fold',fold_+1)\n#     train_f = train.iloc[train_idx]\n#     val_f,y_val = train.iloc[val_idx],target.iloc[val_idx]\n#     for i in tqdm(range(512)):\n#         train2 = train_f[train_f['wheezy-copper-turtle-magic'] == i]\n#         val2 = val_f[val_f['wheezy-copper-turtle-magic'] == i]\n#         test2 = test[test['wheezy-copper-turtle-magic'] == i]\n#         idx1 = train2.index;idx2 = val2.index;idx3 = test2.index\n#         train2.reset_index(drop=True,inplace=True); val2.reset_index(drop=True,inplace=True); test2.reset_index(drop=True,inplace=True);\n#         lg = LogisticRegression(solver='liblinear',penalty='l1',C=0.05)\n#         lg.fit(train2[cols],train2['target'])\n#         oof_lr[idx2] = lg.predict_proba(val2[cols])[:,1]\n#         pred_lr[idx3] += lg.predict_proba(test2[cols])[:,1]/NSPLITS\n# #     print(roc_auc_score(y_val,oof_lr[val_idx]))\n# print('LB with interactions',round(roc_auc_score(target,oof_lr),5))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# %%time\n# #Chris's Way\n\n# cols = [c for c in train.columns if c not in ['id','target','wheezy-copper-turtle-magic']]\n# oof_lr = np.zeros(train.shape[0])\n# pred_lr = np.zeros(test.shape[0])\n\n# #512 models for each wheezy-copper-turtle-magic value\n# for i in tqdm(range(train['wheezy-copper-turtle-magic'].nunique())):\n#     train2 = train[train['wheezy-copper-turtle-magic']==i]\n#     test2 = test[test['wheezy-copper-turtle-magic']==i]\n#     idx1 = train2.index; idx2 = test2.index\n#     train2.reset_index(drop=True,inplace=True); test2.reset_index(drop=True, inplace=True)\n    \n#     skf = StratifiedKFold(n_splits=25,random_state=42)\n#     for fold_,(trn_idx,val_idx) in enumerate(skf.split(train2,train2['target'])):\n#         X_train,y_train = train2.iloc[trn_idx], train2['target'].iloc[trn_idx]\n#         X_val,y_val = train2.iloc[val_idx], train2['target'].iloc[val_idx]\n#         lg = LogisticRegression(solver='liblinear',C=0.05,penalty='l1')\n#         lg.fit(X_train[cols],y_train)\n#         oof_lr[idx1[val_idx]] = lg.predict_proba(X_val[cols])[:,1]\n#         pred_lr[idx2] += lg.predict_proba(test2[cols])[:,1]/25.0\n# print('CV score-LR (with interaction)',round(roc_auc_score(train['target'],oof_lr),5))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Conclusion\nIn conclusion we see that the variable wheezy-copper-turtle-magic interacts with other variables to predict target. Also we see that a simple model can score a high CV and LB score.\n\n**This is similar to the classic XOR problem.** Suppose we have data with two variables and one target: (x1,x2,y) with the following 4 rows: (0,0,0), (1,0,1), (0,1,1), (1,1,0). Notice that neither x1 nor x2 correlate with target y. Also x1 and x2 do not correlate with each other. However, x1 and x2 interact. Whenever x1 is not equal to x2 then y=1 and when x1=x2 then y=0. So together they predict y but separately they cannot predict y."},{"metadata":{},"cell_type":"markdown","source":"## SVM Way\nFrom Chris Deotte's Kernel https://www.kaggle.com/cdeotte/support-vector-machine-0-925"},{"metadata":{"trusted":true},"cell_type":"code","source":"# %%time\n# cols = [c for c in train.columns if c not in ['id','target','wheezy-copper-turtle-magic']]\n# oof_svm = np.zeros(train.shape[0])\n# pred_svm = np.zeros(test.shape[0])\n# skf = StratifiedKFold(n_splits=25,random_state=42)\n# for i in tqdm(range(train['wheezy-copper-turtle-magic'].nunique())):\n#     train2 = train[train['wheezy-copper-turtle-magic']==i]\n#     test2 = test[test['wheezy-copper-turtle-magic']==i]\n#     idx1=train2.index;idx2=test2.index\n#     train2.reset_index(drop=True,inplace=True);test2.reset_index(drop=True,inplace=True)\n    \n#     #When building a model with only 512 observations and 255 features, \n#     #it is important to reduce the number of features to prevent overfitting.\n#     #255 to ~44 features\n#     sel = VarianceThreshold(threshold=1.5).fit(test2[cols])\n#     train3 = sel.transform(train2[cols])\n#     test3 = sel.transform(test2[cols])\n    \n#     for fold_, (train_idx,val_idx) in enumerate(skf.split(train3,train2['target'])):\n#         svc = SVC(kernel='poly',degree=4,gamma='auto',probability=True)\n#         svc.fit(train3[train_idx,:],train2['target'].iloc[train_idx])\n#         oof_svm[idx1[val_idx]]=svc.predict_proba(train3[val_idx,:])[:,1]\n#         pred_svm[idx2] += svc.predict_proba(test3)[:,1]/skf.n_splits\n# roc = roc_auc_score(train['target'],oof_svm)\n# print('CV score-SVM',round(roc,5))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# print('CV score-svm0.9',round(roc_auc_score(train['target'],0.9*oof_svm+0.1*oof_lr),5))\n# print('CV score-svm0.8',round(roc_auc_score(train['target'],0.8*oof_svm+0.2*oof_lr),5))\n# print('CV score-svm0.7',round(roc_auc_score(train['target'],0.7*oof_svm+0.3*oof_lr),5))\n# print('CV score-svm0.6',round(roc_auc_score(train['target'],0.6*oof_svm+0.4*oof_lr),5))\n# print('CV score-svm0.5',round(roc_auc_score(train['target'],0.5*oof_svm+0.5*oof_lr),5))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Submit Predictions"},{"metadata":{"trusted":true},"cell_type":"code","source":"# # sample_submission['target'] = pred_svm\n# # sample_submission.to_csv(\"submission_svm.csv\", index=False)\n\n# sample_submission['target'] = 0.9*pred_svm+0.1*pred_lr\n# sample_submission.to_csv(\"submission_svm1.csv\", index=False)\n# sample_submission['target'] = 0.8*pred_svm+0.2*pred_lr\n# sample_submission.to_csv(\"submission_svm2.csv\", index=False)\n# sample_submission['target'] = 0.7*pred_svm+0.3*pred_lr\n# sample_submission.to_csv(\"submission_svm3.csv\", index=False)\n# sample_submission['target'] = 0.6*pred_svm+0.4*pred_lr\n# sample_submission.to_csv(\"submission_svm4.csv\", index=False)\n# sample_submission['target'] = 0.5*pred_svm+0.5*pred_lr\n# sample_submission.to_csv(\"submission_svm5.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Quadratic Discriminant Analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"# %%time\n# cols = [c for c in train.columns if c not in ['id','target','wheezy-copper-turtle-magic']]\n\n# oof_qda = np.zeros(train.shape[0])\n# pred_qda = np.zeros(test.shape[0])\n\n# for i in tqdm(range(512)):\n#     train2 = train[train['wheezy-copper-turtle-magic']==i]\n#     test2 = test[test['wheezy-copper-turtle-magic']==i]\n#     idx1=train2.index; idx2=test2.index\n#     train2.reset_index(drop=True,inplace=True);test2.reset_index(drop=True,inplace=True)\n    \n#     data = pd.concat([pd.DataFrame(train2[cols]),pd.DataFrame(test2[cols])])\n#     data0 = VarianceThreshold(1.5).fit_transform(data)\n#     train3 = data0[:train2.shape[0]]; test3=data0[train2.shape[0]:]\n    \n#     skf = StratifiedKFold(n_splits=11,random_state=42)\n#     for trn_idx,val_idx in skf.split(train3,train2['target']):\n# #         clf = QuadraticDiscriminantAnalysis(0.1) #0.9649\n#         clf = QuadraticDiscriminantAnalysis(0.08) #\n#         clf.fit(train3[trn_idx,:],train2.loc[trn_idx]['target'])\n#         oof_qda[idx1[val_idx]] = clf.predict_proba(train3[val_idx,:])[:,1]\n#         pred_qda[idx2] += clf.predict_proba(test3)[:,1]/skf.n_splits\n# auc = roc_auc_score(train['target'],oof_qda)\n# print(f'AUC: {auc:.5}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# sample_submission['target'] = pred_qda\n# sample_submission.to_csv('sub_qda.csv',index=False)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Pseudo-Labelling QDA"},{"metadata":{},"cell_type":"markdown","source":"Step 1 and 2 - Build first QDA model and predict test"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n# INITIALIZE VARIABLES\ncols = [c for c in train.columns if c not in ['id', 'target']]\ncols.remove('wheezy-copper-turtle-magic')\noof = np.zeros(len(train))\npreds = np.zeros(len(test))\n\n# BUILD 512 SEPARATE MODELS\nfor i in range(512):\n    # ONLY TRAIN WITH DATA WHERE WHEEZY EQUALS I\n    train2 = train[train['wheezy-copper-turtle-magic']==i]\n    test2 = test[test['wheezy-copper-turtle-magic']==i]\n    idx1 = train2.index; idx2 = test2.index\n    train2.reset_index(drop=True,inplace=True)\n    \n    # FEATURE SELECTION (USE APPROX 40 OF 255 FEATURES)\n    sel = VarianceThreshold(threshold=1.5).fit(train2[cols])\n    train3 = sel.transform(train2[cols])\n    test3 = sel.transform(test2[cols])\n    \n    # STRATIFIED K-FOLD\n    skf = StratifiedKFold(n_splits=11, random_state=42, shuffle=True)\n    for train_index, test_index in skf.split(train3, train2['target']):\n        \n        # MODEL AND PREDICT WITH QDA\n        clf = QuadraticDiscriminantAnalysis(reg_param=0.5)\n        clf.fit(train3[train_index,:],train2.loc[train_index]['target'])\n        oof[idx1[test_index]] = clf.predict_proba(train3[test_index,:])[:,1]\n        preds[idx2] += clf.predict_proba(test3)[:,1] / skf.n_splits\n       \n    #if i%64==0: print(i)\n        \n# PRINT CV AUC\nauc = roc_auc_score(train['target'],oof)\nprint('QDA scores CV =',round(auc,5))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Step 3 & 4 - Add pseudo label data and build second model"},{"metadata":{"trusted":true},"cell_type":"code","source":"# INITIALIZE VARIABLES\ntest['target'] = preds\noof = np.zeros(len(train))\npreds = np.zeros(len(test))\n\n# BUILD 512 SEPARATE MODELS\nfor k in range(512):\n    # ONLY TRAIN WITH DATA WHERE WHEEZY EQUALS I\n    train2 = train[train['wheezy-copper-turtle-magic']==k] \n    train2p = train2.copy(); idx1 = train2.index \n    test2 = test[test['wheezy-copper-turtle-magic']==k]\n    \n    # ADD PSEUDO LABELED DATA\n    test2p = test2[ (test2['target']<=0.01) | (test2['target']>=0.99) ].copy()\n    test2p.loc[ test2p['target']>=0.5, 'target' ] = 1\n    test2p.loc[ test2p['target']<0.5, 'target' ] = 0 \n    train2p = pd.concat([train2p,test2p],axis=0)\n    train2p.reset_index(drop=True,inplace=True)\n    \n    # FEATURE SELECTION (USE APPROX 40 OF 255 FEATURES)\n    sel = VarianceThreshold(threshold=1.5).fit(train2p[cols])     \n    train3p = sel.transform(train2p[cols])\n    train3 = sel.transform(train2[cols])\n    test3 = sel.transform(test2[cols])\n        \n    # STRATIFIED K FOLD\n    skf = StratifiedKFold(n_splits=11, random_state=42, shuffle=True)\n    for train_index, test_index in skf.split(train3p, train2p['target']):\n        test_index3 = test_index[ test_index<len(train3) ] # ignore pseudo in oof\n        \n        # MODEL AND PREDICT WITH QDA\n        clf = QuadraticDiscriminantAnalysis(reg_param=0.5)\n        clf.fit(train3p[train_index,:],train2p.loc[train_index]['target'])\n        oof[idx1[test_index3]] = clf.predict_proba(train3[test_index3,:])[:,1]\n        preds[test2.index] += clf.predict_proba(test3)[:,1] / skf.n_splits\n       \n    #if k%64==0: print(k)\n        \n# PRINT CV AUC\nauc = roc_auc_score(train['target'],oof)\nprint('Pseudo Labeled QDA scores CV =',round(auc,5))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub = pd.read_csv('../input/sample_submission.csv')\nsub['target'] = preds\nsub.to_csv('submission.csv',index=False)\n\nimport matplotlib.pyplot as plt\nplt.hist(preds,bins=100)\nplt.title('Final Test.csv predictions')\nplt.show()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}