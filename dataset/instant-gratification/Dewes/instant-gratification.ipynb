{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":2,"outputs":[{"output_type":"stream","text":"['test.csv', 'train.csv', 'sample_submission.csv']\n","name":"stdout"}]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"# Loading the data\ntrain = pd.read_csv('../input/train.csv')\ntest = pd.read_csv('../input/test.csv')","execution_count":3,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# About the data\ntrain.shape, test.shape","execution_count":4,"outputs":[{"output_type":"execute_result","execution_count":4,"data":{"text/plain":"((262144, 258), (131073, 257))"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.info()","execution_count":5,"outputs":[{"output_type":"stream","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 262144 entries, 0 to 262143\nColumns: 258 entries, id to target\ndtypes: float64(255), int64(2), object(1)\nmemory usage: 516.0+ MB\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.info()","execution_count":6,"outputs":[{"output_type":"stream","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 131073 entries, 0 to 131072\nColumns: 257 entries, id to gamy-white-monster-expert\ndtypes: float64(255), int64(1), object(1)\nmemory usage: 257.0+ MB\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head(3).T","execution_count":7,"outputs":[{"output_type":"execute_result","execution_count":7,"data":{"text/plain":"                                                                          0                ...                                                2\nid                                         707b395ecdcbb4dc2eabea00e4d1b179                ...                 4ccbcb3d13e5072ff1d9c61afe2c4f77\nmuggy-smalt-axolotl-pembus                                         -2.07065                ...                                         -1.68047\ndorky-peach-sheepdog-ordinal                                        1.01816                ...                                         0.860529\nslimy-seashell-cassowary-goose                                     0.228643                ...                                          -1.0762\nsnazzy-harlequin-chicken-distraction                               0.857221                ...                                         0.740124\nfrumpy-smalt-mau-ordinal                                           0.052271                ...                                          3.67844\nstealthy-beige-pinscher-golden                                     0.230303                ...                                         0.288558\nchummy-cream-tarantula-entropy                                     -6.38509                ...                                         0.515875\nhazy-emerald-cuttlefish-unsorted                                   0.439369                ...                                          0.92059\nnerdy-indigo-wolfhound-sorted                                     -0.721946                ...                                         -1.22328\nleaky-amaranth-lizard-sorted                                      -0.227027                ...                                         -1.02978\nugly-tangerine-chihuahua-important                                 0.575964                ...                                          -2.2034\nshaggy-silver-indri-fimbus                                          1.54191                ...                                         -7.08872\nflaky-chocolate-beetle-grandmaster                                  1.74529                ...                                         0.438218\nsquirrely-harlequin-sheep-sumble                                  -0.624271                ...                                        -0.848173\nfreaky-tan-angelfish-noise                                          3.60096                ...                                          1.54267\nlousy-plum-penguin-sumble                                           1.17649                ...                                         -2.16686\nbluesy-rose-wallaby-discard                                       -0.182776                ...                                         -0.86767\nbaggy-copper-oriole-dummy                                         -0.228391                ...                                        -0.980947\nstealthy-scarlet-hound-fepid                                        1.68226                ...                                         0.567793\ngreasy-cinnamon-bonobo-contributor                                -0.833236                ...                                          1.32343\ncranky-cardinal-dogfish-ordinal                                    -4.37769                ...                                          -2.0767\nsnippy-auburn-vole-learn                                           -5.37241                ...                                        -0.291598\ngreasy-sepia-coral-dataset                                        -0.477742                ...                                         -1.56482\nflabby-tangerine-fowl-entropy                                     -0.179005                ...                                          -8.7187\nlousy-smalt-pinscher-dummy                                        -0.516475                ...                                         0.340144\nbluesy-brass-chihuahua-distraction                                 0.127391                ...                                        -0.566402\ngoopy-eggplant-indri-entropy                                      -0.857591                ...                                         0.844324\nhomey-sepia-bombay-sorted                                           -0.4615                ...                                         0.816421\nhomely-ruby-bulldog-entropy                                          2.1603                ...                                         -1.01911\n...                                                                     ...                ...                                              ...\ngreasy-magnolia-spider-grandmaster                                 0.729459                ...                                        -0.214285\ncrabby-carmine-flounder-sorted                                      0.38614                ...                                        -0.389428\nskimpy-copper-fowl-grandmaster                                     0.319814                ...                                         -1.00763\nhasty-seashell-woodpecker-hint                                    -0.407682                ...                                         0.336435\nsnappy-purple-bobcat-important                                    -0.170667                ...                                        -0.851292\nthirsty-carmine-corgi-ordinal                                      -1.24292                ...                                        -0.024184\nhomely-auburn-reindeer-unsorted                                    -1.71905                ...                                         0.455908\ncrappy-beige-tiger-fepid                                          -0.132395                ...                                         0.458753\ncranky-auburn-swan-novice                                         -0.368991                ...                                         -0.26723\nchewy-bistre-buzzard-expert                                        -5.11255                ...                                          -2.0324\nskinny-cyan-macaque-pembus                                         -2.08599                ...                                         0.203082\nslimy-periwinkle-otter-expert                                     -0.897257                ...                                         0.654107\nsnazzy-burgundy-clam-novice                                         1.08067                ...                                         -3.51234\ncozy-ochre-gorilla-gaussian                                       -0.273262                ...                                        -0.840937\nhomey-sangria-wolfhound-dummy                                      0.342824                ...                                         0.519407\nsnazzy-asparagus-hippopotamus-contributor                          0.640177                ...                                        -0.028053\npaltry-red-hamster-sorted                                         -0.415298                ...                                         -1.62108\nzippy-dandelion-insect-golden                                     -0.483126                ...                                         0.142132\nbaggy-coral-bandicoot-unsorted                                    -0.080799                ...                                          1.51466\ngoopy-lavender-wolverine-fimbus                                     2.41622                ...                                         0.828815\nwheezy-myrtle-mandrill-entropy                                     0.351895                ...                                         0.516422\nwiggy-lilac-lemming-sorted                                         0.618824                ...                                         0.130521\ngloppy-cerise-snail-contributor                                    -1.54242                ...                                         -0.45921\nwoozy-silver-havanese-gaussian                                     0.598175                ...                                           2.0282\njumpy-thistle-discus-sorted                                        0.611757                ...                                        -0.093968\nmuggy-turquoise-donkey-important                                   0.678772                ...                                        -0.218274\nblurry-buff-hyena-entropy                                          0.247059                ...                                        -0.163136\nbluesy-chocolate-kudu-fepid                                       -0.806677                ...                                        -0.870289\ngamy-white-monster-expert                                         -0.193649                ...                                         0.064038\ntarget                                                                    0                ...                                                1\n\n[258 rows x 3 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>id</th>\n      <td>707b395ecdcbb4dc2eabea00e4d1b179</td>\n      <td>5880c03c6582a7b42248668e56b4bdec</td>\n      <td>4ccbcb3d13e5072ff1d9c61afe2c4f77</td>\n    </tr>\n    <tr>\n      <th>muggy-smalt-axolotl-pembus</th>\n      <td>-2.07065</td>\n      <td>-0.491702</td>\n      <td>-1.68047</td>\n    </tr>\n    <tr>\n      <th>dorky-peach-sheepdog-ordinal</th>\n      <td>1.01816</td>\n      <td>0.082645</td>\n      <td>0.860529</td>\n    </tr>\n    <tr>\n      <th>slimy-seashell-cassowary-goose</th>\n      <td>0.228643</td>\n      <td>-0.011193</td>\n      <td>-1.0762</td>\n    </tr>\n    <tr>\n      <th>snazzy-harlequin-chicken-distraction</th>\n      <td>0.857221</td>\n      <td>1.07127</td>\n      <td>0.740124</td>\n    </tr>\n    <tr>\n      <th>frumpy-smalt-mau-ordinal</th>\n      <td>0.052271</td>\n      <td>-0.346347</td>\n      <td>3.67844</td>\n    </tr>\n    <tr>\n      <th>stealthy-beige-pinscher-golden</th>\n      <td>0.230303</td>\n      <td>-0.082209</td>\n      <td>0.288558</td>\n    </tr>\n    <tr>\n      <th>chummy-cream-tarantula-entropy</th>\n      <td>-6.38509</td>\n      <td>0.110579</td>\n      <td>0.515875</td>\n    </tr>\n    <tr>\n      <th>hazy-emerald-cuttlefish-unsorted</th>\n      <td>0.439369</td>\n      <td>-0.382374</td>\n      <td>0.92059</td>\n    </tr>\n    <tr>\n      <th>nerdy-indigo-wolfhound-sorted</th>\n      <td>-0.721946</td>\n      <td>-0.22962</td>\n      <td>-1.22328</td>\n    </tr>\n    <tr>\n      <th>leaky-amaranth-lizard-sorted</th>\n      <td>-0.227027</td>\n      <td>0.78398</td>\n      <td>-1.02978</td>\n    </tr>\n    <tr>\n      <th>ugly-tangerine-chihuahua-important</th>\n      <td>0.575964</td>\n      <td>-1.28058</td>\n      <td>-2.2034</td>\n    </tr>\n    <tr>\n      <th>shaggy-silver-indri-fimbus</th>\n      <td>1.54191</td>\n      <td>-1.00348</td>\n      <td>-7.08872</td>\n    </tr>\n    <tr>\n      <th>flaky-chocolate-beetle-grandmaster</th>\n      <td>1.74529</td>\n      <td>-7.7532</td>\n      <td>0.438218</td>\n    </tr>\n    <tr>\n      <th>squirrely-harlequin-sheep-sumble</th>\n      <td>-0.624271</td>\n      <td>-1.32055</td>\n      <td>-0.848173</td>\n    </tr>\n    <tr>\n      <th>freaky-tan-angelfish-noise</th>\n      <td>3.60096</td>\n      <td>0.919078</td>\n      <td>1.54267</td>\n    </tr>\n    <tr>\n      <th>lousy-plum-penguin-sumble</th>\n      <td>1.17649</td>\n      <td>-1.03607</td>\n      <td>-2.16686</td>\n    </tr>\n    <tr>\n      <th>bluesy-rose-wallaby-discard</th>\n      <td>-0.182776</td>\n      <td>0.030213</td>\n      <td>-0.86767</td>\n    </tr>\n    <tr>\n      <th>baggy-copper-oriole-dummy</th>\n      <td>-0.228391</td>\n      <td>0.910172</td>\n      <td>-0.980947</td>\n    </tr>\n    <tr>\n      <th>stealthy-scarlet-hound-fepid</th>\n      <td>1.68226</td>\n      <td>-0.905345</td>\n      <td>0.567793</td>\n    </tr>\n    <tr>\n      <th>greasy-cinnamon-bonobo-contributor</th>\n      <td>-0.833236</td>\n      <td>0.646641</td>\n      <td>1.32343</td>\n    </tr>\n    <tr>\n      <th>cranky-cardinal-dogfish-ordinal</th>\n      <td>-4.37769</td>\n      <td>-0.465291</td>\n      <td>-2.0767</td>\n    </tr>\n    <tr>\n      <th>snippy-auburn-vole-learn</th>\n      <td>-5.37241</td>\n      <td>-0.531735</td>\n      <td>-0.291598</td>\n    </tr>\n    <tr>\n      <th>greasy-sepia-coral-dataset</th>\n      <td>-0.477742</td>\n      <td>-0.756781</td>\n      <td>-1.56482</td>\n    </tr>\n    <tr>\n      <th>flabby-tangerine-fowl-entropy</th>\n      <td>-0.179005</td>\n      <td>0.193724</td>\n      <td>-8.7187</td>\n    </tr>\n    <tr>\n      <th>lousy-smalt-pinscher-dummy</th>\n      <td>-0.516475</td>\n      <td>0.224277</td>\n      <td>0.340144</td>\n    </tr>\n    <tr>\n      <th>bluesy-brass-chihuahua-distraction</th>\n      <td>0.127391</td>\n      <td>-0.474412</td>\n      <td>-0.566402</td>\n    </tr>\n    <tr>\n      <th>goopy-eggplant-indri-entropy</th>\n      <td>-0.857591</td>\n      <td>1.8858</td>\n      <td>0.844324</td>\n    </tr>\n    <tr>\n      <th>homey-sepia-bombay-sorted</th>\n      <td>-0.4615</td>\n      <td>0.205439</td>\n      <td>0.816421</td>\n    </tr>\n    <tr>\n      <th>homely-ruby-bulldog-entropy</th>\n      <td>2.1603</td>\n      <td>-6.48142</td>\n      <td>-1.01911</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>greasy-magnolia-spider-grandmaster</th>\n      <td>0.729459</td>\n      <td>-0.395342</td>\n      <td>-0.214285</td>\n    </tr>\n    <tr>\n      <th>crabby-carmine-flounder-sorted</th>\n      <td>0.38614</td>\n      <td>-0.642357</td>\n      <td>-0.389428</td>\n    </tr>\n    <tr>\n      <th>skimpy-copper-fowl-grandmaster</th>\n      <td>0.319814</td>\n      <td>-0.627209</td>\n      <td>-1.00763</td>\n    </tr>\n    <tr>\n      <th>hasty-seashell-woodpecker-hint</th>\n      <td>-0.407682</td>\n      <td>0.257271</td>\n      <td>0.336435</td>\n    </tr>\n    <tr>\n      <th>snappy-purple-bobcat-important</th>\n      <td>-0.170667</td>\n      <td>-1.46156</td>\n      <td>-0.851292</td>\n    </tr>\n    <tr>\n      <th>thirsty-carmine-corgi-ordinal</th>\n      <td>-1.24292</td>\n      <td>0.325613</td>\n      <td>-0.024184</td>\n    </tr>\n    <tr>\n      <th>homely-auburn-reindeer-unsorted</th>\n      <td>-1.71905</td>\n      <td>1.62837</td>\n      <td>0.455908</td>\n    </tr>\n    <tr>\n      <th>crappy-beige-tiger-fepid</th>\n      <td>-0.132395</td>\n      <td>0.64004</td>\n      <td>0.458753</td>\n    </tr>\n    <tr>\n      <th>cranky-auburn-swan-novice</th>\n      <td>-0.368991</td>\n      <td>0.750735</td>\n      <td>-0.26723</td>\n    </tr>\n    <tr>\n      <th>chewy-bistre-buzzard-expert</th>\n      <td>-5.11255</td>\n      <td>1.16457</td>\n      <td>-2.0324</td>\n    </tr>\n    <tr>\n      <th>skinny-cyan-macaque-pembus</th>\n      <td>-2.08599</td>\n      <td>0.900373</td>\n      <td>0.203082</td>\n    </tr>\n    <tr>\n      <th>slimy-periwinkle-otter-expert</th>\n      <td>-0.897257</td>\n      <td>0.063489</td>\n      <td>0.654107</td>\n    </tr>\n    <tr>\n      <th>snazzy-burgundy-clam-novice</th>\n      <td>1.08067</td>\n      <td>0.948158</td>\n      <td>-3.51234</td>\n    </tr>\n    <tr>\n      <th>cozy-ochre-gorilla-gaussian</th>\n      <td>-0.273262</td>\n      <td>0.273014</td>\n      <td>-0.840937</td>\n    </tr>\n    <tr>\n      <th>homey-sangria-wolfhound-dummy</th>\n      <td>0.342824</td>\n      <td>-1.26915</td>\n      <td>0.519407</td>\n    </tr>\n    <tr>\n      <th>snazzy-asparagus-hippopotamus-contributor</th>\n      <td>0.640177</td>\n      <td>-0.251101</td>\n      <td>-0.028053</td>\n    </tr>\n    <tr>\n      <th>paltry-red-hamster-sorted</th>\n      <td>-0.415298</td>\n      <td>-2.27173</td>\n      <td>-1.62108</td>\n    </tr>\n    <tr>\n      <th>zippy-dandelion-insect-golden</th>\n      <td>-0.483126</td>\n      <td>-0.044167</td>\n      <td>0.142132</td>\n    </tr>\n    <tr>\n      <th>baggy-coral-bandicoot-unsorted</th>\n      <td>-0.080799</td>\n      <td>-0.443766</td>\n      <td>1.51466</td>\n    </tr>\n    <tr>\n      <th>goopy-lavender-wolverine-fimbus</th>\n      <td>2.41622</td>\n      <td>-1.14479</td>\n      <td>0.828815</td>\n    </tr>\n    <tr>\n      <th>wheezy-myrtle-mandrill-entropy</th>\n      <td>0.351895</td>\n      <td>-0.645115</td>\n      <td>0.516422</td>\n    </tr>\n    <tr>\n      <th>wiggy-lilac-lemming-sorted</th>\n      <td>0.618824</td>\n      <td>-1.24609</td>\n      <td>0.130521</td>\n    </tr>\n    <tr>\n      <th>gloppy-cerise-snail-contributor</th>\n      <td>-1.54242</td>\n      <td>2.61336</td>\n      <td>-0.45921</td>\n    </tr>\n    <tr>\n      <th>woozy-silver-havanese-gaussian</th>\n      <td>0.598175</td>\n      <td>-0.479664</td>\n      <td>2.0282</td>\n    </tr>\n    <tr>\n      <th>jumpy-thistle-discus-sorted</th>\n      <td>0.611757</td>\n      <td>1.58129</td>\n      <td>-0.093968</td>\n    </tr>\n    <tr>\n      <th>muggy-turquoise-donkey-important</th>\n      <td>0.678772</td>\n      <td>0.931258</td>\n      <td>-0.218274</td>\n    </tr>\n    <tr>\n      <th>blurry-buff-hyena-entropy</th>\n      <td>0.247059</td>\n      <td>0.151937</td>\n      <td>-0.163136</td>\n    </tr>\n    <tr>\n      <th>bluesy-chocolate-kudu-fepid</th>\n      <td>-0.806677</td>\n      <td>-0.766595</td>\n      <td>-0.870289</td>\n    </tr>\n    <tr>\n      <th>gamy-white-monster-expert</th>\n      <td>-0.193649</td>\n      <td>0.474351</td>\n      <td>0.064038</td>\n    </tr>\n    <tr>\n      <th>target</th>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>258 rows Ã— 3 columns</p>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Distribution of the data\ntrain.describe()","execution_count":8,"outputs":[{"output_type":"execute_result","execution_count":8,"data":{"text/plain":"       muggy-smalt-axolotl-pembus      ...               target\ncount               262144.000000      ...        262144.000000\nmean                     0.005924      ...             0.500225\nstd                      1.706660      ...             0.500001\nmin                    -15.588429      ...             0.000000\n25%                     -0.765404      ...             0.000000\n50%                      0.002148      ...             1.000000\n75%                      0.769228      ...             1.000000\nmax                     15.797000      ...             1.000000\n\n[8 rows x 257 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>muggy-smalt-axolotl-pembus</th>\n      <th>dorky-peach-sheepdog-ordinal</th>\n      <th>slimy-seashell-cassowary-goose</th>\n      <th>snazzy-harlequin-chicken-distraction</th>\n      <th>frumpy-smalt-mau-ordinal</th>\n      <th>stealthy-beige-pinscher-golden</th>\n      <th>chummy-cream-tarantula-entropy</th>\n      <th>hazy-emerald-cuttlefish-unsorted</th>\n      <th>nerdy-indigo-wolfhound-sorted</th>\n      <th>leaky-amaranth-lizard-sorted</th>\n      <th>ugly-tangerine-chihuahua-important</th>\n      <th>shaggy-silver-indri-fimbus</th>\n      <th>flaky-chocolate-beetle-grandmaster</th>\n      <th>squirrely-harlequin-sheep-sumble</th>\n      <th>freaky-tan-angelfish-noise</th>\n      <th>lousy-plum-penguin-sumble</th>\n      <th>bluesy-rose-wallaby-discard</th>\n      <th>baggy-copper-oriole-dummy</th>\n      <th>stealthy-scarlet-hound-fepid</th>\n      <th>greasy-cinnamon-bonobo-contributor</th>\n      <th>cranky-cardinal-dogfish-ordinal</th>\n      <th>snippy-auburn-vole-learn</th>\n      <th>greasy-sepia-coral-dataset</th>\n      <th>flabby-tangerine-fowl-entropy</th>\n      <th>lousy-smalt-pinscher-dummy</th>\n      <th>bluesy-brass-chihuahua-distraction</th>\n      <th>goopy-eggplant-indri-entropy</th>\n      <th>homey-sepia-bombay-sorted</th>\n      <th>homely-ruby-bulldog-entropy</th>\n      <th>hasty-blue-sheep-contributor</th>\n      <th>blurry-wisteria-oyster-master</th>\n      <th>snoopy-auburn-dogfish-expert</th>\n      <th>stinky-maroon-blue-kernel</th>\n      <th>bumpy-amaranth-armadillo-important</th>\n      <th>slaphappy-peach-oyster-master</th>\n      <th>dorky-tomato-ragdoll-dataset</th>\n      <th>messy-mauve-wolverine-ordinal</th>\n      <th>geeky-pumpkin-moorhen-important</th>\n      <th>crabby-teal-otter-unsorted</th>\n      <th>flaky-goldenrod-bat-noise</th>\n      <th>...</th>\n      <th>beady-mauve-frog-distraction</th>\n      <th>surly-brass-maltese-ordinal</th>\n      <th>beady-asparagus-opossum-expert</th>\n      <th>beady-rust-impala-dummy</th>\n      <th>droopy-amethyst-dachshund-hint</th>\n      <th>homey-crimson-budgerigar-grandmaster</th>\n      <th>droopy-cardinal-impala-important</th>\n      <th>woozy-apricot-moose-hint</th>\n      <th>paltry-sapphire-labradoodle-dummy</th>\n      <th>crappy-carmine-eagle-entropy</th>\n      <th>greasy-magnolia-spider-grandmaster</th>\n      <th>crabby-carmine-flounder-sorted</th>\n      <th>skimpy-copper-fowl-grandmaster</th>\n      <th>hasty-seashell-woodpecker-hint</th>\n      <th>snappy-purple-bobcat-important</th>\n      <th>thirsty-carmine-corgi-ordinal</th>\n      <th>homely-auburn-reindeer-unsorted</th>\n      <th>crappy-beige-tiger-fepid</th>\n      <th>cranky-auburn-swan-novice</th>\n      <th>chewy-bistre-buzzard-expert</th>\n      <th>skinny-cyan-macaque-pembus</th>\n      <th>slimy-periwinkle-otter-expert</th>\n      <th>snazzy-burgundy-clam-novice</th>\n      <th>cozy-ochre-gorilla-gaussian</th>\n      <th>homey-sangria-wolfhound-dummy</th>\n      <th>snazzy-asparagus-hippopotamus-contributor</th>\n      <th>paltry-red-hamster-sorted</th>\n      <th>zippy-dandelion-insect-golden</th>\n      <th>baggy-coral-bandicoot-unsorted</th>\n      <th>goopy-lavender-wolverine-fimbus</th>\n      <th>wheezy-myrtle-mandrill-entropy</th>\n      <th>wiggy-lilac-lemming-sorted</th>\n      <th>gloppy-cerise-snail-contributor</th>\n      <th>woozy-silver-havanese-gaussian</th>\n      <th>jumpy-thistle-discus-sorted</th>\n      <th>muggy-turquoise-donkey-important</th>\n      <th>blurry-buff-hyena-entropy</th>\n      <th>bluesy-chocolate-kudu-fepid</th>\n      <th>gamy-white-monster-expert</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>262144.000000</td>\n      <td>262144.000000</td>\n      <td>262144.000000</td>\n      <td>262144.000000</td>\n      <td>262144.000000</td>\n      <td>262144.000000</td>\n      <td>262144.000000</td>\n      <td>262144.000000</td>\n      <td>262144.000000</td>\n      <td>262144.000000</td>\n      <td>262144.000000</td>\n      <td>262144.000000</td>\n      <td>262144.000000</td>\n      <td>262144.000000</td>\n      <td>262144.000000</td>\n      <td>262144.000000</td>\n      <td>262144.000000</td>\n      <td>262144.000000</td>\n      <td>262144.000000</td>\n      <td>262144.000000</td>\n      <td>262144.000000</td>\n      <td>262144.000000</td>\n      <td>262144.000000</td>\n      <td>262144.000000</td>\n      <td>262144.000000</td>\n      <td>262144.000000</td>\n      <td>262144.000000</td>\n      <td>262144.000000</td>\n      <td>262144.000000</td>\n      <td>262144.000000</td>\n      <td>262144.000000</td>\n      <td>262144.000000</td>\n      <td>262144.000000</td>\n      <td>262144.000000</td>\n      <td>262144.000000</td>\n      <td>262144.000000</td>\n      <td>262144.000000</td>\n      <td>262144.000000</td>\n      <td>262144.000000</td>\n      <td>262144.000000</td>\n      <td>...</td>\n      <td>262144.000000</td>\n      <td>262144.000000</td>\n      <td>262144.000000</td>\n      <td>262144.000000</td>\n      <td>262144.000000</td>\n      <td>262144.000000</td>\n      <td>262144.000000</td>\n      <td>262144.000000</td>\n      <td>262144.000000</td>\n      <td>262144.000000</td>\n      <td>262144.000000</td>\n      <td>262144.000000</td>\n      <td>262144.000000</td>\n      <td>262144.000000</td>\n      <td>262144.000000</td>\n      <td>262144.000000</td>\n      <td>262144.000000</td>\n      <td>262144.000000</td>\n      <td>262144.000000</td>\n      <td>262144.000000</td>\n      <td>262144.000000</td>\n      <td>262144.000000</td>\n      <td>262144.000000</td>\n      <td>262144.000000</td>\n      <td>262144.000000</td>\n      <td>262144.000000</td>\n      <td>262144.000000</td>\n      <td>262144.000000</td>\n      <td>262144.000000</td>\n      <td>262144.000000</td>\n      <td>262144.000000</td>\n      <td>262144.000000</td>\n      <td>262144.000000</td>\n      <td>262144.000000</td>\n      <td>262144.000000</td>\n      <td>262144.000000</td>\n      <td>262144.000000</td>\n      <td>262144.000000</td>\n      <td>262144.000000</td>\n      <td>262144.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>0.005924</td>\n      <td>0.000936</td>\n      <td>-0.012530</td>\n      <td>-0.005688</td>\n      <td>0.001607</td>\n      <td>0.007878</td>\n      <td>-0.003500</td>\n      <td>-0.005927</td>\n      <td>0.002664</td>\n      <td>0.020485</td>\n      <td>-0.001629</td>\n      <td>0.008115</td>\n      <td>-0.001445</td>\n      <td>0.006434</td>\n      <td>0.013150</td>\n      <td>-0.016462</td>\n      <td>0.014498</td>\n      <td>0.000990</td>\n      <td>-0.004702</td>\n      <td>-0.010490</td>\n      <td>0.006592</td>\n      <td>-0.007705</td>\n      <td>0.010857</td>\n      <td>-0.011858</td>\n      <td>-0.005172</td>\n      <td>-0.012855</td>\n      <td>-0.003606</td>\n      <td>0.001209</td>\n      <td>0.007980</td>\n      <td>-0.005911</td>\n      <td>-0.000397</td>\n      <td>-0.000490</td>\n      <td>-0.013010</td>\n      <td>-0.000040</td>\n      <td>0.003882</td>\n      <td>-0.008507</td>\n      <td>-0.009352</td>\n      <td>0.007037</td>\n      <td>0.004091</td>\n      <td>-0.011576</td>\n      <td>...</td>\n      <td>-0.002381</td>\n      <td>-0.010963</td>\n      <td>0.005069</td>\n      <td>0.001779</td>\n      <td>0.011217</td>\n      <td>-0.000718</td>\n      <td>-0.008141</td>\n      <td>0.003521</td>\n      <td>0.011593</td>\n      <td>-0.011187</td>\n      <td>0.001626</td>\n      <td>0.000656</td>\n      <td>0.007972</td>\n      <td>-0.003604</td>\n      <td>0.010641</td>\n      <td>0.008723</td>\n      <td>-0.002222</td>\n      <td>-0.000222</td>\n      <td>-0.001966</td>\n      <td>0.012452</td>\n      <td>0.010065</td>\n      <td>0.019941</td>\n      <td>0.002438</td>\n      <td>0.007421</td>\n      <td>0.006418</td>\n      <td>-0.001377</td>\n      <td>-0.005976</td>\n      <td>0.011046</td>\n      <td>-0.001058</td>\n      <td>-0.007541</td>\n      <td>-0.002670</td>\n      <td>-0.011401</td>\n      <td>0.007895</td>\n      <td>0.000651</td>\n      <td>-0.002613</td>\n      <td>0.002351</td>\n      <td>-0.011684</td>\n      <td>-0.007153</td>\n      <td>-0.004066</td>\n      <td>0.500225</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>1.706660</td>\n      <td>1.720943</td>\n      <td>1.698693</td>\n      <td>1.874875</td>\n      <td>1.726869</td>\n      <td>1.734646</td>\n      <td>1.720431</td>\n      <td>1.693276</td>\n      <td>1.872248</td>\n      <td>1.645070</td>\n      <td>1.682552</td>\n      <td>1.608536</td>\n      <td>1.720585</td>\n      <td>1.705780</td>\n      <td>1.682066</td>\n      <td>1.781459</td>\n      <td>1.702845</td>\n      <td>1.651425</td>\n      <td>1.799791</td>\n      <td>1.723219</td>\n      <td>1.727600</td>\n      <td>1.678423</td>\n      <td>1.807137</td>\n      <td>1.805204</td>\n      <td>1.778072</td>\n      <td>1.658649</td>\n      <td>1.795048</td>\n      <td>1.827458</td>\n      <td>1.725991</td>\n      <td>1.771547</td>\n      <td>1.786675</td>\n      <td>1.688791</td>\n      <td>1.770345</td>\n      <td>1.820864</td>\n      <td>1.757163</td>\n      <td>1.732845</td>\n      <td>1.885151</td>\n      <td>1.813789</td>\n      <td>1.688640</td>\n      <td>1.811376</td>\n      <td>...</td>\n      <td>1.774142</td>\n      <td>1.727096</td>\n      <td>1.710350</td>\n      <td>1.747681</td>\n      <td>1.873766</td>\n      <td>1.769533</td>\n      <td>1.734635</td>\n      <td>1.691880</td>\n      <td>1.709808</td>\n      <td>1.651178</td>\n      <td>1.747964</td>\n      <td>1.757856</td>\n      <td>1.845592</td>\n      <td>1.808498</td>\n      <td>1.806801</td>\n      <td>1.801394</td>\n      <td>1.823901</td>\n      <td>1.767555</td>\n      <td>1.696124</td>\n      <td>1.847551</td>\n      <td>1.786441</td>\n      <td>1.729407</td>\n      <td>1.820585</td>\n      <td>1.842801</td>\n      <td>1.702032</td>\n      <td>1.768896</td>\n      <td>1.744526</td>\n      <td>1.806031</td>\n      <td>1.845951</td>\n      <td>1.734388</td>\n      <td>1.799308</td>\n      <td>1.788832</td>\n      <td>1.775853</td>\n      <td>1.703296</td>\n      <td>1.692972</td>\n      <td>1.742284</td>\n      <td>1.698231</td>\n      <td>1.759408</td>\n      <td>1.661463</td>\n      <td>0.500001</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>-15.588429</td>\n      <td>-15.644144</td>\n      <td>-15.235982</td>\n      <td>-15.672966</td>\n      <td>-16.800626</td>\n      <td>-14.789422</td>\n      <td>-15.591329</td>\n      <td>-15.696478</td>\n      <td>-17.011807</td>\n      <td>-14.693510</td>\n      <td>-19.733308</td>\n      <td>-17.343910</td>\n      <td>-15.661142</td>\n      <td>-15.449736</td>\n      <td>-17.327255</td>\n      <td>-15.724992</td>\n      <td>-20.959471</td>\n      <td>-16.657590</td>\n      <td>-18.662974</td>\n      <td>-16.838221</td>\n      <td>-16.533784</td>\n      <td>-15.632537</td>\n      <td>-18.807971</td>\n      <td>-17.454867</td>\n      <td>-17.650032</td>\n      <td>-16.063910</td>\n      <td>-16.203291</td>\n      <td>-17.616064</td>\n      <td>-17.615630</td>\n      <td>-18.155357</td>\n      <td>-17.581792</td>\n      <td>-15.349092</td>\n      <td>-15.377036</td>\n      <td>-16.233520</td>\n      <td>-16.210152</td>\n      <td>-16.110597</td>\n      <td>-16.508542</td>\n      <td>-15.571176</td>\n      <td>-14.857979</td>\n      <td>-17.515148</td>\n      <td>...</td>\n      <td>-15.697323</td>\n      <td>-15.549593</td>\n      <td>-15.984477</td>\n      <td>-14.826623</td>\n      <td>-19.188606</td>\n      <td>-16.045207</td>\n      <td>-15.536694</td>\n      <td>-14.945959</td>\n      <td>-14.738427</td>\n      <td>-15.321612</td>\n      <td>-18.471287</td>\n      <td>-15.637615</td>\n      <td>-17.071420</td>\n      <td>-16.837020</td>\n      <td>-18.289074</td>\n      <td>-15.876873</td>\n      <td>-18.660168</td>\n      <td>-16.405136</td>\n      <td>-16.962770</td>\n      <td>-17.689697</td>\n      <td>-15.391184</td>\n      <td>-16.676546</td>\n      <td>-16.242346</td>\n      <td>-15.428478</td>\n      <td>-15.213663</td>\n      <td>-15.615187</td>\n      <td>-16.736959</td>\n      <td>-17.325479</td>\n      <td>-17.701414</td>\n      <td>-14.209051</td>\n      <td>-16.740804</td>\n      <td>-17.681945</td>\n      <td>-15.705197</td>\n      <td>-15.765274</td>\n      <td>-15.604682</td>\n      <td>-16.965775</td>\n      <td>-15.064518</td>\n      <td>-16.145154</td>\n      <td>-16.815640</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>-0.765404</td>\n      <td>-0.770224</td>\n      <td>-0.777421</td>\n      <td>-0.802502</td>\n      <td>-0.770838</td>\n      <td>-0.771608</td>\n      <td>-0.770634</td>\n      <td>-0.764398</td>\n      <td>-0.795475</td>\n      <td>-0.756594</td>\n      <td>-0.761005</td>\n      <td>-0.741232</td>\n      <td>-0.767934</td>\n      <td>-0.759389</td>\n      <td>-0.756116</td>\n      <td>-0.782001</td>\n      <td>-0.758484</td>\n      <td>-0.755598</td>\n      <td>-0.786833</td>\n      <td>-0.775193</td>\n      <td>-0.768002</td>\n      <td>-0.765029</td>\n      <td>-0.779992</td>\n      <td>-0.791112</td>\n      <td>-0.783849</td>\n      <td>-0.763995</td>\n      <td>-0.783554</td>\n      <td>-0.793338</td>\n      <td>-0.765906</td>\n      <td>-0.784965</td>\n      <td>-0.777341</td>\n      <td>-0.761246</td>\n      <td>-0.781898</td>\n      <td>-0.787004</td>\n      <td>-0.773300</td>\n      <td>-0.775081</td>\n      <td>-0.801363</td>\n      <td>-0.783456</td>\n      <td>-0.760404</td>\n      <td>-0.789706</td>\n      <td>...</td>\n      <td>-0.776379</td>\n      <td>-0.780408</td>\n      <td>-0.767824</td>\n      <td>-0.773896</td>\n      <td>-0.796242</td>\n      <td>-0.776701</td>\n      <td>-0.777279</td>\n      <td>-0.757839</td>\n      <td>-0.762797</td>\n      <td>-0.760570</td>\n      <td>-0.771469</td>\n      <td>-0.775721</td>\n      <td>-0.787209</td>\n      <td>-0.789172</td>\n      <td>-0.785432</td>\n      <td>-0.781371</td>\n      <td>-0.793220</td>\n      <td>-0.778822</td>\n      <td>-0.762225</td>\n      <td>-0.787743</td>\n      <td>-0.776295</td>\n      <td>-0.754395</td>\n      <td>-0.788463</td>\n      <td>-0.789934</td>\n      <td>-0.765082</td>\n      <td>-0.778429</td>\n      <td>-0.777317</td>\n      <td>-0.779243</td>\n      <td>-0.798982</td>\n      <td>-0.777831</td>\n      <td>-0.782944</td>\n      <td>-0.788437</td>\n      <td>-0.778580</td>\n      <td>-0.767367</td>\n      <td>-0.764199</td>\n      <td>-0.770457</td>\n      <td>-0.768885</td>\n      <td>-0.785702</td>\n      <td>-0.766552</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>0.002148</td>\n      <td>0.003543</td>\n      <td>-0.005771</td>\n      <td>-0.003505</td>\n      <td>0.000866</td>\n      <td>0.002830</td>\n      <td>0.001257</td>\n      <td>-0.003657</td>\n      <td>0.004936</td>\n      <td>0.004372</td>\n      <td>0.000157</td>\n      <td>0.003860</td>\n      <td>-0.001344</td>\n      <td>0.003531</td>\n      <td>0.006706</td>\n      <td>-0.003552</td>\n      <td>0.004006</td>\n      <td>-0.000431</td>\n      <td>-0.001292</td>\n      <td>-0.005718</td>\n      <td>-0.002144</td>\n      <td>-0.004066</td>\n      <td>0.005575</td>\n      <td>-0.005285</td>\n      <td>-0.001344</td>\n      <td>-0.003270</td>\n      <td>-0.002283</td>\n      <td>0.001721</td>\n      <td>0.004760</td>\n      <td>-0.001934</td>\n      <td>0.001828</td>\n      <td>-0.001682</td>\n      <td>-0.003771</td>\n      <td>0.000990</td>\n      <td>-0.000225</td>\n      <td>-0.003409</td>\n      <td>-0.004631</td>\n      <td>0.006661</td>\n      <td>0.001620</td>\n      <td>-0.003919</td>\n      <td>...</td>\n      <td>0.000937</td>\n      <td>-0.002320</td>\n      <td>0.002214</td>\n      <td>0.000339</td>\n      <td>0.004685</td>\n      <td>0.001403</td>\n      <td>-0.009198</td>\n      <td>0.003729</td>\n      <td>0.006625</td>\n      <td>-0.005897</td>\n      <td>0.001181</td>\n      <td>-0.004775</td>\n      <td>0.002720</td>\n      <td>-0.003037</td>\n      <td>0.000898</td>\n      <td>0.005414</td>\n      <td>-0.001182</td>\n      <td>-0.000443</td>\n      <td>0.000017</td>\n      <td>0.010056</td>\n      <td>0.004044</td>\n      <td>0.007277</td>\n      <td>0.002079</td>\n      <td>0.001674</td>\n      <td>0.001819</td>\n      <td>0.001125</td>\n      <td>-0.003708</td>\n      <td>-0.000665</td>\n      <td>-0.001368</td>\n      <td>-0.007404</td>\n      <td>-0.002239</td>\n      <td>-0.004543</td>\n      <td>-0.000675</td>\n      <td>0.000771</td>\n      <td>-0.000331</td>\n      <td>-0.000617</td>\n      <td>-0.005634</td>\n      <td>-0.004459</td>\n      <td>-0.004471</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>0.769228</td>\n      <td>0.773400</td>\n      <td>0.764092</td>\n      <td>0.798460</td>\n      <td>0.769954</td>\n      <td>0.776606</td>\n      <td>0.769794</td>\n      <td>0.752836</td>\n      <td>0.800376</td>\n      <td>0.764807</td>\n      <td>0.759293</td>\n      <td>0.750801</td>\n      <td>0.763828</td>\n      <td>0.770540</td>\n      <td>0.768772</td>\n      <td>0.772042</td>\n      <td>0.772029</td>\n      <td>0.759202</td>\n      <td>0.782631</td>\n      <td>0.757846</td>\n      <td>0.772787</td>\n      <td>0.759634</td>\n      <td>0.792009</td>\n      <td>0.780260</td>\n      <td>0.777918</td>\n      <td>0.750965</td>\n      <td>0.781823</td>\n      <td>0.790271</td>\n      <td>0.779128</td>\n      <td>0.775912</td>\n      <td>0.781760</td>\n      <td>0.761690</td>\n      <td>0.773938</td>\n      <td>0.783732</td>\n      <td>0.780273</td>\n      <td>0.764276</td>\n      <td>0.798681</td>\n      <td>0.793304</td>\n      <td>0.763912</td>\n      <td>0.778159</td>\n      <td>...</td>\n      <td>0.775500</td>\n      <td>0.767383</td>\n      <td>0.768818</td>\n      <td>0.772977</td>\n      <td>0.806502</td>\n      <td>0.775486</td>\n      <td>0.764319</td>\n      <td>0.762854</td>\n      <td>0.772323</td>\n      <td>0.752352</td>\n      <td>0.773328</td>\n      <td>0.776182</td>\n      <td>0.796490</td>\n      <td>0.789162</td>\n      <td>0.792034</td>\n      <td>0.788740</td>\n      <td>0.785778</td>\n      <td>0.775193</td>\n      <td>0.763919</td>\n      <td>0.798912</td>\n      <td>0.782247</td>\n      <td>0.778481</td>\n      <td>0.788068</td>\n      <td>0.796270</td>\n      <td>0.767389</td>\n      <td>0.783034</td>\n      <td>0.767407</td>\n      <td>0.786449</td>\n      <td>0.790400</td>\n      <td>0.770104</td>\n      <td>0.786180</td>\n      <td>0.776178</td>\n      <td>0.788910</td>\n      <td>0.769277</td>\n      <td>0.764115</td>\n      <td>0.773100</td>\n      <td>0.753518</td>\n      <td>0.774197</td>\n      <td>0.758012</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>15.797000</td>\n      <td>18.577178</td>\n      <td>17.205310</td>\n      <td>16.585173</td>\n      <td>16.815798</td>\n      <td>17.378922</td>\n      <td>15.202100</td>\n      <td>16.645394</td>\n      <td>19.396192</td>\n      <td>17.131375</td>\n      <td>15.558533</td>\n      <td>15.732609</td>\n      <td>16.599516</td>\n      <td>16.097210</td>\n      <td>15.756782</td>\n      <td>16.422857</td>\n      <td>16.069614</td>\n      <td>17.051746</td>\n      <td>16.286381</td>\n      <td>16.609642</td>\n      <td>15.522247</td>\n      <td>15.902420</td>\n      <td>17.171457</td>\n      <td>17.235412</td>\n      <td>15.839078</td>\n      <td>16.228652</td>\n      <td>15.947737</td>\n      <td>17.017614</td>\n      <td>16.265235</td>\n      <td>15.080431</td>\n      <td>16.664606</td>\n      <td>15.846838</td>\n      <td>18.455962</td>\n      <td>16.199981</td>\n      <td>17.143851</td>\n      <td>17.204338</td>\n      <td>18.192529</td>\n      <td>18.468209</td>\n      <td>15.085824</td>\n      <td>16.678473</td>\n      <td>...</td>\n      <td>16.176833</td>\n      <td>14.602126</td>\n      <td>15.282156</td>\n      <td>15.946309</td>\n      <td>16.399846</td>\n      <td>15.427315</td>\n      <td>14.597412</td>\n      <td>15.010216</td>\n      <td>15.931864</td>\n      <td>15.190043</td>\n      <td>18.511841</td>\n      <td>15.491450</td>\n      <td>16.462407</td>\n      <td>16.366581</td>\n      <td>15.037548</td>\n      <td>17.406346</td>\n      <td>18.428578</td>\n      <td>19.692337</td>\n      <td>15.551620</td>\n      <td>16.616120</td>\n      <td>17.121966</td>\n      <td>15.819361</td>\n      <td>18.307258</td>\n      <td>16.745289</td>\n      <td>17.215588</td>\n      <td>15.609117</td>\n      <td>16.784002</td>\n      <td>16.436113</td>\n      <td>15.832077</td>\n      <td>16.727533</td>\n      <td>15.511936</td>\n      <td>16.252503</td>\n      <td>16.387990</td>\n      <td>16.260286</td>\n      <td>14.748128</td>\n      <td>16.212146</td>\n      <td>18.866005</td>\n      <td>18.811832</td>\n      <td>17.302211</td>\n      <td>1.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Verifying for NaN\nsum(train.isna().any())","execution_count":9,"outputs":[{"output_type":"execute_result","execution_count":9,"data":{"text/plain":"0"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.concat([train, test], sort=False)","execution_count":10,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[df['wheezy-copper-turtle-magic']==1].describe()","execution_count":11,"outputs":[{"output_type":"execute_result","execution_count":11,"data":{"text/plain":"       muggy-smalt-axolotl-pembus     ...          target\ncount                  760.000000     ...      510.000000\nmean                    -0.017534     ...        0.482353\nstd                      0.979238     ...        0.500179\nmin                     -3.353590     ...        0.000000\n25%                     -0.639533     ...        0.000000\n50%                      0.008465     ...        0.000000\n75%                      0.611780     ...        1.000000\nmax                      2.885800     ...        1.000000\n\n[8 rows x 257 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>muggy-smalt-axolotl-pembus</th>\n      <th>dorky-peach-sheepdog-ordinal</th>\n      <th>slimy-seashell-cassowary-goose</th>\n      <th>snazzy-harlequin-chicken-distraction</th>\n      <th>frumpy-smalt-mau-ordinal</th>\n      <th>stealthy-beige-pinscher-golden</th>\n      <th>chummy-cream-tarantula-entropy</th>\n      <th>hazy-emerald-cuttlefish-unsorted</th>\n      <th>nerdy-indigo-wolfhound-sorted</th>\n      <th>leaky-amaranth-lizard-sorted</th>\n      <th>ugly-tangerine-chihuahua-important</th>\n      <th>shaggy-silver-indri-fimbus</th>\n      <th>flaky-chocolate-beetle-grandmaster</th>\n      <th>squirrely-harlequin-sheep-sumble</th>\n      <th>freaky-tan-angelfish-noise</th>\n      <th>lousy-plum-penguin-sumble</th>\n      <th>bluesy-rose-wallaby-discard</th>\n      <th>baggy-copper-oriole-dummy</th>\n      <th>stealthy-scarlet-hound-fepid</th>\n      <th>greasy-cinnamon-bonobo-contributor</th>\n      <th>cranky-cardinal-dogfish-ordinal</th>\n      <th>snippy-auburn-vole-learn</th>\n      <th>greasy-sepia-coral-dataset</th>\n      <th>flabby-tangerine-fowl-entropy</th>\n      <th>lousy-smalt-pinscher-dummy</th>\n      <th>bluesy-brass-chihuahua-distraction</th>\n      <th>goopy-eggplant-indri-entropy</th>\n      <th>homey-sepia-bombay-sorted</th>\n      <th>homely-ruby-bulldog-entropy</th>\n      <th>hasty-blue-sheep-contributor</th>\n      <th>blurry-wisteria-oyster-master</th>\n      <th>snoopy-auburn-dogfish-expert</th>\n      <th>stinky-maroon-blue-kernel</th>\n      <th>bumpy-amaranth-armadillo-important</th>\n      <th>slaphappy-peach-oyster-master</th>\n      <th>dorky-tomato-ragdoll-dataset</th>\n      <th>messy-mauve-wolverine-ordinal</th>\n      <th>geeky-pumpkin-moorhen-important</th>\n      <th>crabby-teal-otter-unsorted</th>\n      <th>flaky-goldenrod-bat-noise</th>\n      <th>...</th>\n      <th>beady-mauve-frog-distraction</th>\n      <th>surly-brass-maltese-ordinal</th>\n      <th>beady-asparagus-opossum-expert</th>\n      <th>beady-rust-impala-dummy</th>\n      <th>droopy-amethyst-dachshund-hint</th>\n      <th>homey-crimson-budgerigar-grandmaster</th>\n      <th>droopy-cardinal-impala-important</th>\n      <th>woozy-apricot-moose-hint</th>\n      <th>paltry-sapphire-labradoodle-dummy</th>\n      <th>crappy-carmine-eagle-entropy</th>\n      <th>greasy-magnolia-spider-grandmaster</th>\n      <th>crabby-carmine-flounder-sorted</th>\n      <th>skimpy-copper-fowl-grandmaster</th>\n      <th>hasty-seashell-woodpecker-hint</th>\n      <th>snappy-purple-bobcat-important</th>\n      <th>thirsty-carmine-corgi-ordinal</th>\n      <th>homely-auburn-reindeer-unsorted</th>\n      <th>crappy-beige-tiger-fepid</th>\n      <th>cranky-auburn-swan-novice</th>\n      <th>chewy-bistre-buzzard-expert</th>\n      <th>skinny-cyan-macaque-pembus</th>\n      <th>slimy-periwinkle-otter-expert</th>\n      <th>snazzy-burgundy-clam-novice</th>\n      <th>cozy-ochre-gorilla-gaussian</th>\n      <th>homey-sangria-wolfhound-dummy</th>\n      <th>snazzy-asparagus-hippopotamus-contributor</th>\n      <th>paltry-red-hamster-sorted</th>\n      <th>zippy-dandelion-insect-golden</th>\n      <th>baggy-coral-bandicoot-unsorted</th>\n      <th>goopy-lavender-wolverine-fimbus</th>\n      <th>wheezy-myrtle-mandrill-entropy</th>\n      <th>wiggy-lilac-lemming-sorted</th>\n      <th>gloppy-cerise-snail-contributor</th>\n      <th>woozy-silver-havanese-gaussian</th>\n      <th>jumpy-thistle-discus-sorted</th>\n      <th>muggy-turquoise-donkey-important</th>\n      <th>blurry-buff-hyena-entropy</th>\n      <th>bluesy-chocolate-kudu-fepid</th>\n      <th>gamy-white-monster-expert</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>760.000000</td>\n      <td>760.000000</td>\n      <td>760.000000</td>\n      <td>760.000000</td>\n      <td>760.000000</td>\n      <td>760.000000</td>\n      <td>760.000000</td>\n      <td>760.000000</td>\n      <td>760.000000</td>\n      <td>760.000000</td>\n      <td>760.000000</td>\n      <td>760.000000</td>\n      <td>760.000000</td>\n      <td>760.000000</td>\n      <td>760.000000</td>\n      <td>760.000000</td>\n      <td>760.000000</td>\n      <td>760.000000</td>\n      <td>760.000000</td>\n      <td>760.000000</td>\n      <td>760.000000</td>\n      <td>760.000000</td>\n      <td>760.000000</td>\n      <td>760.000000</td>\n      <td>760.000000</td>\n      <td>760.000000</td>\n      <td>760.000000</td>\n      <td>760.000000</td>\n      <td>760.000000</td>\n      <td>760.000000</td>\n      <td>760.000000</td>\n      <td>760.000000</td>\n      <td>760.000000</td>\n      <td>760.000000</td>\n      <td>760.000000</td>\n      <td>760.000000</td>\n      <td>760.000000</td>\n      <td>760.000000</td>\n      <td>760.000000</td>\n      <td>760.000000</td>\n      <td>...</td>\n      <td>760.000000</td>\n      <td>760.000000</td>\n      <td>760.000000</td>\n      <td>760.000000</td>\n      <td>760.000000</td>\n      <td>760.000000</td>\n      <td>760.000000</td>\n      <td>760.000000</td>\n      <td>760.000000</td>\n      <td>760.000000</td>\n      <td>760.000000</td>\n      <td>760.000000</td>\n      <td>760.000000</td>\n      <td>760.000000</td>\n      <td>760.000000</td>\n      <td>760.000000</td>\n      <td>760.000000</td>\n      <td>760.000000</td>\n      <td>760.000000</td>\n      <td>760.000000</td>\n      <td>760.000000</td>\n      <td>760.000000</td>\n      <td>760.000000</td>\n      <td>760.000000</td>\n      <td>760.000000</td>\n      <td>760.000000</td>\n      <td>760.000000</td>\n      <td>760.000000</td>\n      <td>760.000000</td>\n      <td>760.000000</td>\n      <td>760.000000</td>\n      <td>760.000000</td>\n      <td>760.000000</td>\n      <td>760.000000</td>\n      <td>760.000000</td>\n      <td>760.000000</td>\n      <td>760.000000</td>\n      <td>760.000000</td>\n      <td>760.000000</td>\n      <td>510.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>-0.017534</td>\n      <td>0.073963</td>\n      <td>0.015555</td>\n      <td>-0.048986</td>\n      <td>0.003776</td>\n      <td>0.032501</td>\n      <td>-0.000611</td>\n      <td>0.016311</td>\n      <td>0.024067</td>\n      <td>0.018668</td>\n      <td>-0.026617</td>\n      <td>0.062218</td>\n      <td>-0.012063</td>\n      <td>-0.052167</td>\n      <td>0.064870</td>\n      <td>-0.065495</td>\n      <td>-0.016810</td>\n      <td>0.077996</td>\n      <td>0.028765</td>\n      <td>0.001102</td>\n      <td>-0.018126</td>\n      <td>-0.009079</td>\n      <td>0.283401</td>\n      <td>-0.017859</td>\n      <td>0.402509</td>\n      <td>0.066971</td>\n      <td>-0.023475</td>\n      <td>0.060500</td>\n      <td>-0.003101</td>\n      <td>-0.012916</td>\n      <td>0.010551</td>\n      <td>-0.418386</td>\n      <td>-0.070237</td>\n      <td>-0.108400</td>\n      <td>-0.025786</td>\n      <td>0.004452</td>\n      <td>-0.024040</td>\n      <td>-0.070516</td>\n      <td>0.006503</td>\n      <td>0.005122</td>\n      <td>...</td>\n      <td>0.006389</td>\n      <td>-0.098295</td>\n      <td>0.049545</td>\n      <td>-0.002984</td>\n      <td>-0.031099</td>\n      <td>0.020168</td>\n      <td>0.000717</td>\n      <td>-0.012232</td>\n      <td>-0.007029</td>\n      <td>0.004517</td>\n      <td>0.004434</td>\n      <td>0.075591</td>\n      <td>-0.291706</td>\n      <td>0.016107</td>\n      <td>0.008259</td>\n      <td>0.662749</td>\n      <td>-0.051037</td>\n      <td>0.001546</td>\n      <td>-0.187577</td>\n      <td>0.308939</td>\n      <td>0.069833</td>\n      <td>0.024078</td>\n      <td>0.003294</td>\n      <td>-0.033126</td>\n      <td>-0.754124</td>\n      <td>0.023882</td>\n      <td>-0.024085</td>\n      <td>-0.025120</td>\n      <td>-0.040426</td>\n      <td>0.004938</td>\n      <td>-0.018121</td>\n      <td>-0.033203</td>\n      <td>-0.004182</td>\n      <td>0.037673</td>\n      <td>0.023269</td>\n      <td>0.013545</td>\n      <td>0.002931</td>\n      <td>-0.021872</td>\n      <td>0.033656</td>\n      <td>0.482353</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>0.979238</td>\n      <td>0.979928</td>\n      <td>1.064286</td>\n      <td>1.021387</td>\n      <td>1.013771</td>\n      <td>0.997054</td>\n      <td>1.015871</td>\n      <td>3.500945</td>\n      <td>0.982225</td>\n      <td>1.002488</td>\n      <td>1.000984</td>\n      <td>3.868618</td>\n      <td>0.982145</td>\n      <td>3.482347</td>\n      <td>0.981900</td>\n      <td>0.986670</td>\n      <td>1.016993</td>\n      <td>0.989651</td>\n      <td>1.012141</td>\n      <td>1.041692</td>\n      <td>0.992011</td>\n      <td>1.017644</td>\n      <td>3.387261</td>\n      <td>1.022243</td>\n      <td>3.577127</td>\n      <td>0.968978</td>\n      <td>1.019024</td>\n      <td>0.999527</td>\n      <td>1.008177</td>\n      <td>0.994641</td>\n      <td>0.992145</td>\n      <td>3.563474</td>\n      <td>0.984140</td>\n      <td>3.738636</td>\n      <td>0.979964</td>\n      <td>1.073953</td>\n      <td>0.960437</td>\n      <td>1.019438</td>\n      <td>0.996489</td>\n      <td>0.974431</td>\n      <td>...</td>\n      <td>1.064762</td>\n      <td>0.997156</td>\n      <td>1.016346</td>\n      <td>0.965575</td>\n      <td>0.975856</td>\n      <td>0.998462</td>\n      <td>0.968838</td>\n      <td>1.005924</td>\n      <td>1.041271</td>\n      <td>0.983157</td>\n      <td>1.015955</td>\n      <td>3.298638</td>\n      <td>3.611518</td>\n      <td>0.988116</td>\n      <td>1.022401</td>\n      <td>3.529806</td>\n      <td>1.005570</td>\n      <td>0.978927</td>\n      <td>3.522520</td>\n      <td>3.513477</td>\n      <td>3.457714</td>\n      <td>1.029802</td>\n      <td>0.951768</td>\n      <td>1.024759</td>\n      <td>3.683156</td>\n      <td>1.030788</td>\n      <td>1.033377</td>\n      <td>1.013512</td>\n      <td>1.056596</td>\n      <td>0.990774</td>\n      <td>1.012513</td>\n      <td>1.016890</td>\n      <td>0.987173</td>\n      <td>0.999562</td>\n      <td>1.001501</td>\n      <td>0.952444</td>\n      <td>1.012804</td>\n      <td>1.009946</td>\n      <td>0.993813</td>\n      <td>0.500179</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>-3.353590</td>\n      <td>-3.160774</td>\n      <td>-3.866968</td>\n      <td>-3.346169</td>\n      <td>-3.439449</td>\n      <td>-2.793122</td>\n      <td>-2.735746</td>\n      <td>-11.813352</td>\n      <td>-3.002428</td>\n      <td>-2.978814</td>\n      <td>-2.867628</td>\n      <td>-13.996723</td>\n      <td>-3.192481</td>\n      <td>-8.659549</td>\n      <td>-3.382631</td>\n      <td>-3.567203</td>\n      <td>-3.045319</td>\n      <td>-3.413502</td>\n      <td>-3.217337</td>\n      <td>-3.076332</td>\n      <td>-3.444899</td>\n      <td>-3.218103</td>\n      <td>-10.609104</td>\n      <td>-3.326598</td>\n      <td>-9.377291</td>\n      <td>-2.770892</td>\n      <td>-2.952473</td>\n      <td>-3.858832</td>\n      <td>-2.689016</td>\n      <td>-2.958616</td>\n      <td>-2.919922</td>\n      <td>-12.908672</td>\n      <td>-3.462722</td>\n      <td>-13.186220</td>\n      <td>-3.189770</td>\n      <td>-4.222960</td>\n      <td>-2.916117</td>\n      <td>-2.810945</td>\n      <td>-3.125293</td>\n      <td>-3.166282</td>\n      <td>...</td>\n      <td>-3.721351</td>\n      <td>-3.555924</td>\n      <td>-2.896958</td>\n      <td>-2.892889</td>\n      <td>-3.873477</td>\n      <td>-3.534105</td>\n      <td>-2.657709</td>\n      <td>-3.522369</td>\n      <td>-4.260881</td>\n      <td>-3.170835</td>\n      <td>-3.864965</td>\n      <td>-8.829655</td>\n      <td>-11.110380</td>\n      <td>-2.924996</td>\n      <td>-4.513400</td>\n      <td>-11.685521</td>\n      <td>-3.167792</td>\n      <td>-2.727484</td>\n      <td>-11.503439</td>\n      <td>-11.395058</td>\n      <td>-9.747040</td>\n      <td>-3.762917</td>\n      <td>-2.618268</td>\n      <td>-3.020348</td>\n      <td>-10.805874</td>\n      <td>-3.338991</td>\n      <td>-3.251757</td>\n      <td>-3.093280</td>\n      <td>-3.605726</td>\n      <td>-2.557082</td>\n      <td>-3.958156</td>\n      <td>-2.566585</td>\n      <td>-2.986497</td>\n      <td>-2.796218</td>\n      <td>-3.285437</td>\n      <td>-2.605237</td>\n      <td>-3.145273</td>\n      <td>-3.245244</td>\n      <td>-3.101536</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>-0.639533</td>\n      <td>-0.555628</td>\n      <td>-0.706632</td>\n      <td>-0.716309</td>\n      <td>-0.671993</td>\n      <td>-0.652227</td>\n      <td>-0.710672</td>\n      <td>-2.287451</td>\n      <td>-0.604133</td>\n      <td>-0.605484</td>\n      <td>-0.710426</td>\n      <td>-2.434227</td>\n      <td>-0.641928</td>\n      <td>-2.473051</td>\n      <td>-0.627579</td>\n      <td>-0.674657</td>\n      <td>-0.720136</td>\n      <td>-0.577443</td>\n      <td>-0.628900</td>\n      <td>-0.676548</td>\n      <td>-0.677539</td>\n      <td>-0.661798</td>\n      <td>-2.181410</td>\n      <td>-0.695865</td>\n      <td>-2.056729</td>\n      <td>-0.577446</td>\n      <td>-0.663096</td>\n      <td>-0.547184</td>\n      <td>-0.697333</td>\n      <td>-0.688149</td>\n      <td>-0.696676</td>\n      <td>-2.829543</td>\n      <td>-0.735320</td>\n      <td>-2.725583</td>\n      <td>-0.660565</td>\n      <td>-0.672438</td>\n      <td>-0.659107</td>\n      <td>-0.763837</td>\n      <td>-0.679214</td>\n      <td>-0.661138</td>\n      <td>...</td>\n      <td>-0.704473</td>\n      <td>-0.736725</td>\n      <td>-0.610839</td>\n      <td>-0.692622</td>\n      <td>-0.684050</td>\n      <td>-0.641662</td>\n      <td>-0.660172</td>\n      <td>-0.710983</td>\n      <td>-0.696019</td>\n      <td>-0.606859</td>\n      <td>-0.674952</td>\n      <td>-2.092871</td>\n      <td>-2.870155</td>\n      <td>-0.642767</td>\n      <td>-0.648420</td>\n      <td>-1.632079</td>\n      <td>-0.752215</td>\n      <td>-0.647201</td>\n      <td>-2.510389</td>\n      <td>-2.267942</td>\n      <td>-2.172349</td>\n      <td>-0.654825</td>\n      <td>-0.686959</td>\n      <td>-0.758050</td>\n      <td>-3.243446</td>\n      <td>-0.670994</td>\n      <td>-0.743005</td>\n      <td>-0.760895</td>\n      <td>-0.772873</td>\n      <td>-0.697268</td>\n      <td>-0.711532</td>\n      <td>-0.682114</td>\n      <td>-0.690329</td>\n      <td>-0.614012</td>\n      <td>-0.634060</td>\n      <td>-0.603023</td>\n      <td>-0.654529</td>\n      <td>-0.739877</td>\n      <td>-0.595388</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>0.008465</td>\n      <td>0.107037</td>\n      <td>0.040606</td>\n      <td>-0.017831</td>\n      <td>-0.002263</td>\n      <td>0.042942</td>\n      <td>0.017466</td>\n      <td>0.117571</td>\n      <td>0.060573</td>\n      <td>-0.015366</td>\n      <td>-0.040627</td>\n      <td>0.000854</td>\n      <td>0.037226</td>\n      <td>-0.084183</td>\n      <td>0.084299</td>\n      <td>-0.063468</td>\n      <td>-0.009721</td>\n      <td>0.084863</td>\n      <td>0.037971</td>\n      <td>0.023495</td>\n      <td>-0.031072</td>\n      <td>-0.032040</td>\n      <td>0.292218</td>\n      <td>-0.036090</td>\n      <td>0.471836</td>\n      <td>0.062394</td>\n      <td>-0.031362</td>\n      <td>0.097692</td>\n      <td>-0.068991</td>\n      <td>-0.013927</td>\n      <td>0.033623</td>\n      <td>-0.331220</td>\n      <td>-0.098562</td>\n      <td>-0.082625</td>\n      <td>-0.049410</td>\n      <td>-0.019560</td>\n      <td>-0.023654</td>\n      <td>-0.099602</td>\n      <td>-0.006027</td>\n      <td>0.001779</td>\n      <td>...</td>\n      <td>0.066919</td>\n      <td>-0.108828</td>\n      <td>0.073347</td>\n      <td>-0.035563</td>\n      <td>-0.082555</td>\n      <td>0.009973</td>\n      <td>-0.007164</td>\n      <td>0.009004</td>\n      <td>-0.003429</td>\n      <td>-0.004538</td>\n      <td>0.016460</td>\n      <td>0.206210</td>\n      <td>-0.324968</td>\n      <td>0.077856</td>\n      <td>-0.005758</td>\n      <td>0.707206</td>\n      <td>-0.007546</td>\n      <td>-0.007482</td>\n      <td>-0.395965</td>\n      <td>0.146864</td>\n      <td>0.229534</td>\n      <td>0.034510</td>\n      <td>0.042538</td>\n      <td>-0.058142</td>\n      <td>-0.756336</td>\n      <td>0.077919</td>\n      <td>0.014920</td>\n      <td>-0.024894</td>\n      <td>-0.041682</td>\n      <td>0.000387</td>\n      <td>-0.048119</td>\n      <td>-0.060006</td>\n      <td>-0.069657</td>\n      <td>0.006360</td>\n      <td>0.030275</td>\n      <td>-0.028358</td>\n      <td>-0.012909</td>\n      <td>0.005767</td>\n      <td>0.016637</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>0.611780</td>\n      <td>0.727279</td>\n      <td>0.791930</td>\n      <td>0.637921</td>\n      <td>0.705994</td>\n      <td>0.726681</td>\n      <td>0.702529</td>\n      <td>2.279073</td>\n      <td>0.655111</td>\n      <td>0.643781</td>\n      <td>0.674268</td>\n      <td>2.728879</td>\n      <td>0.610547</td>\n      <td>2.393464</td>\n      <td>0.750788</td>\n      <td>0.546597</td>\n      <td>0.698708</td>\n      <td>0.751341</td>\n      <td>0.725131</td>\n      <td>0.644003</td>\n      <td>0.646553</td>\n      <td>0.649946</td>\n      <td>2.611254</td>\n      <td>0.692850</td>\n      <td>2.797045</td>\n      <td>0.756151</td>\n      <td>0.641699</td>\n      <td>0.730118</td>\n      <td>0.678337</td>\n      <td>0.693063</td>\n      <td>0.663656</td>\n      <td>1.881330</td>\n      <td>0.629290</td>\n      <td>2.471784</td>\n      <td>0.655012</td>\n      <td>0.702927</td>\n      <td>0.618426</td>\n      <td>0.636471</td>\n      <td>0.690621</td>\n      <td>0.632907</td>\n      <td>...</td>\n      <td>0.713259</td>\n      <td>0.556148</td>\n      <td>0.693060</td>\n      <td>0.668890</td>\n      <td>0.556889</td>\n      <td>0.655888</td>\n      <td>0.666686</td>\n      <td>0.637547</td>\n      <td>0.668371</td>\n      <td>0.671015</td>\n      <td>0.663196</td>\n      <td>2.342890</td>\n      <td>2.277834</td>\n      <td>0.691226</td>\n      <td>0.717471</td>\n      <td>2.985548</td>\n      <td>0.632959</td>\n      <td>0.662859</td>\n      <td>2.234170</td>\n      <td>2.924644</td>\n      <td>2.404276</td>\n      <td>0.675510</td>\n      <td>0.638791</td>\n      <td>0.639747</td>\n      <td>1.774662</td>\n      <td>0.712772</td>\n      <td>0.698871</td>\n      <td>0.639766</td>\n      <td>0.660902</td>\n      <td>0.643925</td>\n      <td>0.628505</td>\n      <td>0.654921</td>\n      <td>0.677181</td>\n      <td>0.720910</td>\n      <td>0.703939</td>\n      <td>0.662624</td>\n      <td>0.690946</td>\n      <td>0.685219</td>\n      <td>0.674299</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>2.885800</td>\n      <td>3.159348</td>\n      <td>3.772958</td>\n      <td>3.151226</td>\n      <td>3.466475</td>\n      <td>3.246862</td>\n      <td>3.063217</td>\n      <td>15.163583</td>\n      <td>3.418600</td>\n      <td>3.407023</td>\n      <td>2.775425</td>\n      <td>13.053071</td>\n      <td>3.483937</td>\n      <td>10.485777</td>\n      <td>2.758156</td>\n      <td>3.256764</td>\n      <td>2.899790</td>\n      <td>2.672278</td>\n      <td>3.448294</td>\n      <td>3.430788</td>\n      <td>3.594773</td>\n      <td>3.153580</td>\n      <td>10.140796</td>\n      <td>2.568590</td>\n      <td>12.040916</td>\n      <td>3.317353</td>\n      <td>3.311362</td>\n      <td>3.234487</td>\n      <td>3.669993</td>\n      <td>2.778811</td>\n      <td>3.598962</td>\n      <td>11.818452</td>\n      <td>3.092713</td>\n      <td>10.588293</td>\n      <td>2.745876</td>\n      <td>3.378303</td>\n      <td>2.840553</td>\n      <td>3.384531</td>\n      <td>3.702715</td>\n      <td>2.668790</td>\n      <td>...</td>\n      <td>3.096701</td>\n      <td>3.183860</td>\n      <td>3.397093</td>\n      <td>3.028995</td>\n      <td>4.046307</td>\n      <td>3.155319</td>\n      <td>2.728446</td>\n      <td>3.428839</td>\n      <td>3.090594</td>\n      <td>2.943795</td>\n      <td>3.035883</td>\n      <td>8.980519</td>\n      <td>9.287152</td>\n      <td>2.703494</td>\n      <td>2.747701</td>\n      <td>12.670131</td>\n      <td>2.719868</td>\n      <td>3.215520</td>\n      <td>11.784433</td>\n      <td>10.677217</td>\n      <td>10.522188</td>\n      <td>2.952799</td>\n      <td>2.653421</td>\n      <td>2.966791</td>\n      <td>13.269246</td>\n      <td>3.554291</td>\n      <td>2.738094</td>\n      <td>3.006469</td>\n      <td>3.225383</td>\n      <td>3.392816</td>\n      <td>3.520465</td>\n      <td>3.610830</td>\n      <td>2.810355</td>\n      <td>3.181904</td>\n      <td>2.951212</td>\n      <td>3.510002</td>\n      <td>3.257977</td>\n      <td>4.236546</td>\n      <td>3.541414</td>\n      <td>1.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_selection import VarianceThreshold\nfrom sklearn.preprocessing import StandardScaler","execution_count":12,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\nfrom sklearn.ensemble import ExtraTreesClassifier, RandomForestClassifier\nfrom sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.naive_bayes import BernoulliNB\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\nimport time\n\ndef get_best_param(x, y, modelo, params):\n    grid = GridSearchCV(estimator=modelo, param_grid=params, cv=3, n_jobs=-1)\n    grid_result = grid.fit(x, y)        \n    return (grid_result.best_score_, grid_result.best_params_)\n\n# Treina tudo e acha o melhor\ndef testa_tudo(train, alvo):\n    '''\n    FunÃ§Ã£o que tenta um monte de algoritmo de classificaÃ§Ã£o e ve qual que presta. Demora um pouco.\n    '''\n    start_time = time.time()\n    melhores = dict()\n    # modelos\n    models = {\n        'QuadraticDiscriminantAnalysis': QuadraticDiscriminantAnalysis(),\n    }\n    # parametros\n    params = {\n        'QuadraticDiscriminantAnalysis': {'tol': [1,0.1,0.01,0.001,0.0001,0.00001]}\n    }\n    for nome, modelo in models.items():\n        param = params[nome]\n        melhores[nome] = get_best_param(train, alvo, modelo, param)\n        print(\"Modelo: %s, melhor classificaÃ§Ã£o: %f usando %s\" % (nome, melhores[nome][0], melhores[nome][1]))\n    print(\"Tempo rodando: \" + str((time.time() - start_time)) + ' ms')\n    return melhores","execution_count":13,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def qual_melhor(melhores):\n    melhor = {'score': 0, 'nome': '', 'params': None}\n    for nome, res in melhores.items():\n        if res[0] > melhor['score']:\n            melhor['score'] = res[0]\n            melhor['nome'] = nome\n            melhor['params'] = res[1]\n    return melhor\n\ndef roda_melhor(melhor, train, target, test):\n    models = {\n        'QuadraticDiscriminantAnalysis': QuadraticDiscriminantAnalysis(),\n    }\n    modelo = models[melhor['nome']].set_params(**melhor['params'])\n    modelo.fit(train, target)\n    return modelo.predict_proba(test)\n    ","execution_count":14,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"size_x = train.shape[0]\nsize_y = test.shape[0]\n\n#melhores = testa_tudo(novo[:size_x], train['target'])","execution_count":15,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#melhor = qual_melhor(melhores)\n#pred = roda_melhor(melhor, novo[:size_x], train['target'], novo[size_x:])","execution_count":16,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#test['target'] = pred\n#test[['id', 'target']].to_csv('submission.csv', index=False)","execution_count":17,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import roc_auc_score\n\n# wheezy-turtle is a cat variable\ncols = [c for c in df.columns if c not in ['id', 'target', 'wheezy-copper-turtle-magic']]\n\n# Train by wheezy-copper-turtle-magic group\noof = np.zeros(len(train))\npreds = np.zeros(len(test))\n\nfor i in range(df['wheezy-copper-turtle-magic'].max()+1):\n    train2 = train[train['wheezy-copper-turtle-magic']==i]\n    test2 = test[test['wheezy-copper-turtle-magic']==i]\n    idx1 = train2.index\n    idx2 = test2.index\n    train2.reset_index(drop=True, inplace=True)\n    \n    # Normalizing the data\n    df2 = pd.concat([train2[cols], test2[cols]])\n    df2 = VarianceThreshold(threshold=1.5).fit_transform(df2[cols])\n    \n    # Getting the vectors\n    train3 = df2[:train2.shape[0]]\n    test3 = df2[train2.shape[0]:]\n    \n    # CrossValidation using StratifiedKFold\n    skf = StratifiedKFold(n_splits=10, random_state=666)\n    for train_index, test_index in skf.split(train2, train2['target']):\n        clf = QuadraticDiscriminantAnalysis(tol=1)\n        clf.fit(train3[train_index,:],train2.loc[train_index]['target'])\n        oof[idx1[test_index]] = clf.predict_proba(train3[test_index,:])[:,1]\n        preds[idx2] += clf.predict_proba(test3)[:,1] / skf.n_splits\n\n\nauc = roc_auc_score(train['target'], oof)\nprint(f'AUC: {auc:.5}')","execution_count":18,"outputs":[{"output_type":"stream","text":"AUC: 0.9648\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"#test['target'] = preds\n#test[['id', 'target']].to_csv('submission.csv', index=False)","execution_count":19,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Improving the model with pseudo labels\n\n# INITIALIZE VARIABLES\ntest['target'] = preds\noof = np.zeros(len(train))\npreds = np.zeros(len(test))\n\n# BUILD 512 SEPARATE MODELS\nfor k in range(512):\n    # ONLY TRAIN WITH DATA WHERE WHEEZY EQUALS I\n    train2 = train[train['wheezy-copper-turtle-magic']==k] \n    train2p = train2.copy(); idx1 = train2.index \n    test2 = test[test['wheezy-copper-turtle-magic']==k]\n    \n    # ADD PSEUDO LABELED DATA\n    test2p = test2[ (test2['target']<=0.01) | (test2['target']>=0.99) ].copy()\n    test2p.loc[ test2p['target']>=0.5, 'target' ] = 1\n    test2p.loc[ test2p['target']<0.5, 'target' ] = 0 \n    train2p = pd.concat([train2p,test2p],axis=0)\n    train2p.reset_index(drop=True,inplace=True)\n    \n    # FEATURE SELECTION (USE APPROX 40 OF 255 FEATURES)\n    sel = VarianceThreshold(threshold=1.5).fit(train2p[cols])     \n    train3p = sel.transform(train2p[cols])\n    train3 = sel.transform(train2[cols])\n    test3 = sel.transform(test2[cols])\n        \n    # STRATIFIED K FOLD\n    skf = StratifiedKFold(n_splits=11, random_state=42, shuffle=True)\n    for train_index, test_index in skf.split(train3p, train2p['target']):\n        test_index3 = test_index[ test_index<len(train3) ] # ignore pseudo in oof\n        \n        # MODEL AND PREDICT WITH QDA\n        clf = QuadraticDiscriminantAnalysis(reg_param=0.5)\n        clf.fit(train3p[train_index,:],train2p.loc[train_index]['target'])\n        oof[idx1[test_index3]] = clf.predict_proba(train3[test_index3,:])[:,1]\n        preds[test2.index] += clf.predict_proba(test3)[:,1] / skf.n_splits\n       \n    #if k%64==0: print(k)\n        \n# PRINT CV AUC\nauc = roc_auc_score(train['target'],oof)\nprint('Pseudo Labeled QDA scores CV =',round(auc,5))","execution_count":20,"outputs":[{"output_type":"stream","text":"Pseudo Labeled QDA scores CV = 0.97036\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub = pd.read_csv('../input/sample_submission.csv')\nsub['target'] = preds\nsub.to_csv('submission.csv',index=False)","execution_count":21,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}