{"cells":[{"metadata":{},"cell_type":"markdown","source":"Thanks to great public kernels (e.g. [Synthetic data for (next?) Instant Gratification](https://www.kaggle.com/mhviraf/synthetic-data-for-next-instant-gratification) and [Quadratic Discriminant Analysis](https://www.kaggle.com/speedwagon/quadratic-discriminant-analysis)), we all know that applying quadratic discriminant analysis to data with the same values of 'wheezy-copper-turtle-magic' is a very promissing way to go in this competition.\n\nQDA has essentially only one hyperparameter, which is 'reg_param', for regularization. What I would like to try here is to see whether using the same 'reg_param', say 0.1 or 0.5, for 512 models is OK. \n\nTo this end, I simply use GridSearchCV in each model."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### libraries"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"from sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.feature_selection import VarianceThreshold\nfrom sklearn.model_selection import GridSearchCV, train_test_split\n\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nprint(\"Libraries were imported.\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### load data"},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/train.csv')\ntest = pd.read_csv('../input/test.csv')\n\nprint(\"Data were loaded.\")\ncols = [c for c in train.columns if c not in ['id', 'target', 'wheezy-copper-turtle-magic']]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Parameter tuning on QDA via GridSearchCV"},{"metadata":{"trusted":true},"cell_type":"code","source":"# parameter to be optimized\nparams = [{'reg_param': [0.1, 0.2, 0.3, 0.4, 0.5]}]\n\n# 512 models\nreg_params = np.zeros(512)\nfor i in range(512):\n    train2 = train[train['wheezy-copper-turtle-magic']==i]\n    test2 = test[test['wheezy-copper-turtle-magic']==i]\n    idx1 = train2.index; idx2 = test2.index\n    train2.reset_index(drop=True,inplace=True)\n\n    data = pd.concat([pd.DataFrame(train2[cols]), pd.DataFrame(test2[cols])])\n    data2 = VarianceThreshold(threshold=2).fit_transform(data[cols])\n\n    train3 = data2[:train2.shape[0]]; \n    \n    qda = QuadraticDiscriminantAnalysis()\n    clf = GridSearchCV(qda, params, cv=4)\n    clf.fit(train3, train2['target'])\n    \n    reg_params[i] = clf.best_params_['reg_param']\n    print(\"Best reg_param for model \" + str(i) + \" is \" + str(reg_params[i]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(reg_params)\nplt.title(\"reg_param in QDA\")\nplt.figure()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Interesting, actually optimal parameters differ across models.\n\nLet's use optimal parameters for each model and submit."},{"metadata":{"trusted":true},"cell_type":"code","source":"oof = np.zeros(len(train))\npreds = np.zeros(len(test))\n\nfor i in range(512):\n\n    train2 = train[train['wheezy-copper-turtle-magic']==i]\n    test2 = test[test['wheezy-copper-turtle-magic']==i]\n    idx1 = train2.index; idx2 = test2.index\n    train2.reset_index(drop=True,inplace=True)\n\n    data = pd.concat([pd.DataFrame(train2[cols]), pd.DataFrame(test2[cols])])\n    data2 = VarianceThreshold(threshold=2).fit_transform(data[cols])\n\n    train3 = data2[:train2.shape[0]]; test3 = data2[train2.shape[0]:]\n\n    skf = StratifiedKFold(n_splits=5, random_state=42)\n    for train_index, test_index in skf.split(train2, train2['target']):\n\n        clf = QuadraticDiscriminantAnalysis(reg_params[i])\n        clf.fit(train3[train_index,:],train2.loc[train_index]['target'])\n        oof[idx1[test_index]] = clf.predict_proba(train3[test_index,:])[:,1]\n        preds[idx2] += clf.predict_proba(test3)[:,1] / skf.n_splits\n\nauc = roc_auc_score(train['target'], oof)\nprint(f'AUC: {auc:.5}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### submission"},{"metadata":{"trusted":true},"cell_type":"code","source":"sub = pd.read_csv('../input/sample_submission.csv')\nsub['target'] = preds\nsub.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}