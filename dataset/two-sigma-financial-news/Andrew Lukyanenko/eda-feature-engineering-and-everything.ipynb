{"cells":[{"metadata":{"_uuid":"c49c4f47379d81cd7fc50faf2f482938e441dec5"},"cell_type":"markdown","source":"## General information\n\nTwo Sigma Financial News Competition is a unique competitions: not only it is a Kernel-only competition, but we aren't supposed to download data and during stage two our solutions will be used to predict future real data.\n\nI'll try to do an extensive EDA for this competition and try to find some interesting things about the data.\n\nP. S. I'l learning to use plotly, so there will be interactive charts at last!"},{"metadata":{"_uuid":"6daa1434e9a463cb819f93bb08b41602e4b1f64b"},"cell_type":"markdown","source":"![](http://fintechnews.ch/wp-content/uploads/2016/11/Deutsche-Bank-Survey-87-of-Financial-Market-Participants-Say-Blockchain-Will-Disrupt-The-Industry-1440x564_c.jpg)"},{"metadata":{"_uuid":"896f2b18bf4f32ec7dfb0196e3d718a7ae991b6a"},"cell_type":"markdown","source":"### Getting data and importing libraries"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nimport datetime\nimport lightgbm as lgb\nfrom scipy import stats\nfrom scipy.sparse import hstack, csr_matrix\nfrom sklearn.model_selection import train_test_split\nfrom wordcloud import WordCloud\nfrom collections import Counter\nfrom nltk.corpus import stopwords\nfrom nltk.util import ngrams\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.preprocessing import StandardScaler\nstop = set(stopwords.words('english'))\n\n\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.tools as tls\n\nfrom xgboost import XGBClassifier\nimport lightgbm as lgb\nfrom sklearn import model_selection\nfrom sklearn.metrics import accuracy_score","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"# official way to get the data\nfrom kaggle.competitions import twosigmanews\nenv = twosigmanews.make_env()\nprint('Done!')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1ee6824cf41c4fd03be113d54e6975cf3574c06f"},"cell_type":"code","source":"(market_train_df, news_train_df) = env.get_training_data()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"56d475175477feee4b5672285e8bccd663b5c4c6"},"cell_type":"markdown","source":"We have two datasets, let's explore them separately."},{"metadata":{"_uuid":"a21a4bd4a0be55614fcdcbc8f1812b5994dabb19"},"cell_type":"markdown","source":"## Market data\n\nWe have a really interesting dataset which contains stock prices for many companies over a decade!\n\nFor now let's have a look at the data itself and not think about the competition. We can see long-term trends, appearing and declining companies and many other things."},{"metadata":{"trusted":true,"_uuid":"cca04f17e12924251a4ea85f1ea63a81e4c1c4eb"},"cell_type":"code","source":"print(f'{market_train_df.shape[0]} samples and {market_train_df.shape[1]} features in the training market dataset.')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a10641e3f980559a25ebc3ab83082443ce344de8","scrolled":true},"cell_type":"code","source":"market_train_df.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"68399eec91a6c1e773c9534969949ca40f04f290"},"cell_type":"markdown","source":"At first let's take 10 random assets and plot them."},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"e49f9296ce4c92adc8b82bc4e96fe05d1a1845d7","scrolled":false},"cell_type":"code","source":"data = []\nfor asset in np.random.choice(market_train_df['assetName'].unique(), 10):\n    asset_df = market_train_df[(market_train_df['assetName'] == asset)]\n\n    data.append(go.Scatter(\n        x = asset_df['time'].dt.strftime(date_format='%Y-%m-%d').values,\n        y = asset_df['close'].values,\n        name = asset\n    ))\nlayout = go.Layout(dict(title = \"Closing prices of 10 random assets\",\n                  xaxis = dict(title = 'Month'),\n                  yaxis = dict(title = 'Price (USD)'),\n                  ),legend=dict(\n                orientation=\"h\"))\npy.iplot(dict(data=data, layout=layout), filename='basic-line')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dae9a9525d26e61fc597f6c107e3a0680df2384d"},"cell_type":"markdown","source":"I plot data for all periods because I'd like to show long-term trends.\nAssets are sampled randomly, but you should see that some companies' stocks started trading later, some dissappeared. Disappearence could be due to bankruptcy, acquisition or other reasons."},{"metadata":{"trusted":true,"_uuid":"62ccdf6ee7c3371c8a2079c4261b5527fac0a72d"},"cell_type":"markdown","source":"Well, these were some random companies. But it would be more interesting to see general trends of prices."},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"066d58a619d40beaf565c02064f2d1351fa717a8"},"cell_type":"code","source":"data = []\n#market_train_df['close'] = market_train_df['close'] / 20\nfor i in [0.05, 0.1, 0.25, 0.5, 0.75, 0.9, 0.95]:\n    price_df = market_train_df.groupby('time')['close'].quantile(i).reset_index()\n\n    data.append(go.Scatter(\n        x = price_df['time'].dt.strftime(date_format='%Y-%m-%d').values,\n        y = price_df['close'].values,\n        name = f'{i} quantile'\n    ))\nlayout = go.Layout(dict(title = \"Trends of closing prices by quantiles\",\n                  xaxis = dict(title = 'Month'),\n                  yaxis = dict(title = 'Price (USD)'),\n                  ),legend=dict(\n                orientation=\"h\"))\npy.iplot(dict(data=data, layout=layout), filename='basic-line')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7620510c704631bb04200c6a47ff01b0025befbd"},"cell_type":"markdown","source":"It is cool to be able to see how markets fall and rise again.\nI have shown 4 events when there were serious stock price drops on the market.\nYou could also notice that higher quantile prices have increased with time and lower quantile prices decreased.\nMaybe the gap between poor and rich increases... on the other hand maybe more \"little\" companies are ready to go to market and prices of their shares isn't very high."},{"metadata":{"_uuid":"19a6cf416e4b7d88b1dcdf5ef37565307ce5d3ce"},"cell_type":"markdown","source":"Now, let's look at these price drops in details."},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"8ca5182693d1826b459101b54dd23f6d7bf69b3d"},"cell_type":"code","source":"market_train_df['price_diff'] = market_train_df['close'] - market_train_df['open']\ngrouped = market_train_df.groupby('time').agg({'price_diff': ['std', 'min']}).reset_index()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a21c4b16636acb7cc98caa354bb06ea989d6373f","_kg_hide-input":true,"scrolled":false},"cell_type":"code","source":"print(f\"Average standard deviation of price change within a day in {grouped['price_diff']['std'].mean():.4f}.\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4be3e4c4cc550f67bb77eea9f42a935f6574d48b"},"cell_type":"code","source":"g = grouped.sort_values(('price_diff', 'std'), ascending=False)[:10]\ng['min_text'] = 'Maximum price drop: ' + (-1 * g['price_diff']['min']).astype(str)\ntrace = go.Scatter(\n    x = g['time'].dt.strftime(date_format='%Y-%m-%d').values,\n    y = g['price_diff']['std'].values,\n    mode='markers',\n    marker=dict(\n        size = g['price_diff']['std'].values,\n        color = g['price_diff']['std'].values,\n        colorscale='Portland',\n        showscale=True\n    ),\n    text = g['min_text'].values\n    #text = f\"Maximum price drop: {g['price_diff']['min'].values}\"\n    #g['time'].dt.strftime(date_format='%Y-%m-%d').values\n)\ndata = [trace]\n\nlayout= go.Layout(\n    autosize= True,\n    title= 'Top 10 months by standard deviation of price change within a day',\n    hovermode= 'closest',\n    yaxis=dict(\n        title= 'price_diff',\n        ticklen= 5,\n        gridwidth= 2,\n    ),\n    showlegend= False\n)\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig,filename='scatter2010')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c7e55d1b2db816c00d5cc66f373949ec010c0494"},"cell_type":"markdown","source":"We can see huge price fluctiations when market crashed. Just think about it... **But this is wrong!** There was no huge crash on January 2010... Let's dive into the data!"},{"metadata":{"_uuid":"f4c43e1d3dc085b540af9736043089f5fb386f6b"},"cell_type":"markdown","source":"### Possible data errors\n\nAt first let's simply sort data by the difference between open and close prices."},{"metadata":{"trusted":true,"_uuid":"fd332dac81171201abbefcf3a42cc0a2c315d895"},"cell_type":"code","source":"market_train_df.sort_values('price_diff')[:10]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"409b5a614ff8c5efd13d9a69a81a6f0b81acf3ce"},"cell_type":"markdown","source":"So price of \"Towers Watson & Co\" shares was almost 10k... I think this is simply an error in data.\n\nBut what about Bank of New York Mellon Corp?\n\nLet's see data by Yahoo:"},{"metadata":{"_uuid":"79fba7ac9a4162cc5d6ac93c7b366ffba2d350a0"},"cell_type":"markdown","source":"![](https://i.imgur.com/C3COWfe.png)"},{"metadata":{"_uuid":"e2c1089b0ba6c7dce00d398ebf5702262f98e2de"},"cell_type":"markdown","source":"There were no spikes."},{"metadata":{"trusted":true,"_uuid":"143157631b3ecefbbf8c227c7c9b5bd522df2812"},"cell_type":"markdown","source":"Another case is with cost equal to 999, such numbers are usually suspicious. Let's look at Archrock Inc - no spikes there as well.\n\n![](https://i.imgur.com/KYZKkSd.png)"},{"metadata":{"trusted":true,"_uuid":"9fb49e6cf41f70872a78276403f3fe4d9ed87ae4"},"cell_type":"markdown","source":"So, let's try to find strange cases."},{"metadata":{"trusted":true,"_uuid":"04fe6a44a65a7b66fa128f24acf6717eda1f6e20"},"cell_type":"code","source":"market_train_df['close_to_open'] =  np.abs(market_train_df['close'] / market_train_df['open'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"39bf65e0c1a6fac123f5af29a76248c21c458747"},"cell_type":"code","source":"print(f\"In {(market_train_df['close_to_open'] >= 1.2).sum()} lines price increased by 20% or more.\")\nprint(f\"In {(market_train_df['close_to_open'] <= 0.8).sum()} lines price decreased by 20% or more.\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"549aabe483a6f62dd7946020d3d8e60be89919d3"},"cell_type":"markdown","source":"Well, this isn't much considering we have more than 4 million lines and a lot of these cases are due to price falls during market crash. Well just need to deal with outliers."},{"metadata":{"trusted":true,"_uuid":"47fcd3e5635e68ea9681626bfb3bc9583b76fb00"},"cell_type":"code","source":"print(f\"In {(market_train_df['close_to_open'] >= 2).sum()} lines price increased by 100% or more.\")\nprint(f\"In {(market_train_df['close_to_open'] <= 0.5).sum()} lines price decreased by 100% or more.\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"78b680921bead3d610d3ba35f6cefa98778edeff"},"cell_type":"markdown","source":"For a quick fix I'll replace outliers in these lines with mean open or close price of this company."},{"metadata":{"_kg_hide-input":true,"trusted":true,"scrolled":true,"_uuid":"19ef8496d92912fd56dce27ea0548c8a42c92212"},"cell_type":"code","source":"market_train_df['assetName_mean_open'] = market_train_df.groupby('assetName')['open'].transform('mean')\nmarket_train_df['assetName_mean_close'] = market_train_df.groupby('assetName')['close'].transform('mean')\n\n# if open price is too far from mean open price for this company, replace it. Otherwise replace close price.\nfor i, row in market_train_df.loc[market_train_df['close_to_open'] >= 2].iterrows():\n    if np.abs(row['assetName_mean_open'] - row['open']) > np.abs(row['assetName_mean_close'] - row['close']):\n        market_train_df.iloc[i,5] = row['assetName_mean_open']\n    else:\n        market_train_df.iloc[i,4] = row['assetName_mean_close']\n        \nfor i, row in market_train_df.loc[market_train_df['close_to_open'] <= 0.5].iterrows():\n    if np.abs(row['assetName_mean_open'] - row['open']) > np.abs(row['assetName_mean_close'] - row['close']):\n        market_train_df.iloc[i,5] = row['assetName_mean_open']\n    else:\n        market_train_df.iloc[i,4] = row['assetName_mean_close']","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a3f8287435af9eecb41e7d054ec52283a3cf40eb"},"cell_type":"markdown","source":"Now let's try to build that graph again."},{"metadata":{"_kg_hide-input":true,"trusted":true,"scrolled":false,"_uuid":"ca35be72e7329e2265b885c2846cac73c68e0c5f"},"cell_type":"code","source":"market_train_df['price_diff'] = market_train_df['close'] - market_train_df['open']\ngrouped = market_train_df.groupby(['time']).agg({'price_diff': ['std', 'min']}).reset_index()\ng = grouped.sort_values(('price_diff', 'std'), ascending=False)[:10]\ng['min_text'] = 'Maximum price drop: ' + (-1 * np.round(g['price_diff']['min'], 2)).astype(str)\ntrace = go.Scatter(\n    x = g['time'].dt.strftime(date_format='%Y-%m-%d').values,\n    y = g['price_diff']['std'].values,\n    mode='markers',\n    marker=dict(\n        size = g['price_diff']['std'].values * 5,\n        color = g['price_diff']['std'].values,\n        colorscale='Portland',\n        showscale=True\n    ),\n    text = g['min_text'].values\n    #text = f\"Maximum price drop: {g['price_diff']['min'].values}\"\n    #g['time'].dt.strftime(date_format='%Y-%m-%d').values\n)\ndata = [trace]\n\nlayout= go.Layout(\n    autosize= True,\n    title= 'Top 10 months by standard deviation of price change within a day',\n    hovermode= 'closest',\n    yaxis=dict(\n        title= 'price_diff',\n        ticklen= 5,\n        gridwidth= 2,\n    ),\n    showlegend= False\n)\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig,filename='scatter2010')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6451f892eb30e9ed887e3fc3bba0e4577af87655"},"cell_type":"markdown","source":"Now the graph is much more reasonable."},{"metadata":{"trusted":true,"_uuid":"ba7e227d09cb2954e1339aab21a4c49fbe53a90e"},"cell_type":"markdown","source":"Now let's take a look at out target variable."},{"metadata":{"_kg_hide-input":true,"trusted":true,"scrolled":false,"_uuid":"fc11eb700da57c528111c751f83bb1fc67446489"},"cell_type":"code","source":"data = []\nfor i in [0.05, 0.1, 0.25, 0.5, 0.75, 0.9, 0.95]:\n    price_df = market_train_df.groupby('time')['returnsOpenNextMktres10'].quantile(i).reset_index()\n\n    data.append(go.Scatter(\n        x = price_df['time'].dt.strftime(date_format='%Y-%m-%d').values,\n        y = price_df['returnsOpenNextMktres10'].values,\n        name = f'{i} quantile'\n    ))\nlayout = go.Layout(dict(title = \"Trends of returnsOpenNextMktres10 by quantiles\",\n                  xaxis = dict(title = 'Month'),\n                  yaxis = dict(title = 'Price (USD)'),\n                  ),legend=dict(\n                orientation=\"h\"),)\npy.iplot(dict(data=data, layout=layout), filename='basic-line')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"414894c26558ba77cb7e0963fba2a31c5f0d5d87"},"cell_type":"markdown","source":"We can see that quantiles have a high deviation, but mean value doesn't change much.\n\nNow I think it is time to throw an old part of dataset. Let's leave only data since 2010 year, this way we will get rid of the data of the biggest crisis."},{"metadata":{"_uuid":"29454b6967e6d8fe0c5441435a41d31d70048a1a"},"cell_type":"markdown","source":"Let's look at the target variable now."},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"3d45a734741f7bdea69432e09a3f63c9e14132a6"},"cell_type":"code","source":"data = []\nmarket_train_df = market_train_df.loc[market_train_df['time'] >= '2010-01-01 22:00:00+0000']\n\nprice_df = market_train_df.groupby('time')['returnsOpenNextMktres10'].mean().reset_index()\n\ndata.append(go.Scatter(\n    x = price_df['time'].dt.strftime(date_format='%Y-%m-%d').values,\n    y = price_df['returnsOpenNextMktres10'].values,\n    name = f'{i} quantile'\n))\nlayout = go.Layout(dict(title = \"Treand of returnsOpenNextMktres10 mean\",\n                  xaxis = dict(title = 'Month'),\n                  yaxis = dict(title = 'Price (USD)'),\n                  ),legend=dict(\n                orientation=\"h\"),)\npy.iplot(dict(data=data, layout=layout), filename='basic-line')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ed1d2c559c69fb4f628e29423134f7e610cf3df9"},"cell_type":"markdown","source":"Fluctuations seem to be high, but in fact they are lower that 8 percent. In fact it looks like a random noise..."},{"metadata":{"_uuid":"7a3bc153d0005c67122c7835f30bbaff9002ca9f"},"cell_type":"markdown","source":"Now let's remember the description:\n```\nThe marketdata contains a variety of returns calculated over different timespans. All of the returns in this set of marketdata have these properties:\n\n    Returns are always calculated either open-to-open (from the opening time of one trading day to the open of another) or close-to-close (from the closing time of one trading day to the open of another).\n    Returns are either raw, meaning that the data is not adjusted against any benchmark, or market-residualized (Mktres), meaning that the movement of the market as a whole has been accounted for, leaving only movements inherent to the instrument.\n    Returns can be calculated over any arbitrary interval. Provided here are 1 day and 10 day horizons.\n    Returns are tagged with 'Prev' if they are backwards looking in time, or 'Next' if forwards looking.\n```\n\nLet's have a look at means of these variables."},{"metadata":{"trusted":true,"_uuid":"71abf4301d4c4cc18a5e300e7d20319560d49df1","_kg_hide-input":true},"cell_type":"code","source":"data = []\nfor col in ['returnsClosePrevRaw1', 'returnsOpenPrevRaw1',\n       'returnsClosePrevMktres1', 'returnsOpenPrevMktres1',\n       'returnsClosePrevRaw10', 'returnsOpenPrevRaw10',\n       'returnsClosePrevMktres10', 'returnsOpenPrevMktres10',\n       'returnsOpenNextMktres10']:\n    df = market_train_df.groupby('time')[col].mean().reset_index()\n    data.append(go.Scatter(\n        x = df['time'].dt.strftime(date_format='%Y-%m-%d').values,\n        y = df[col].values,\n        name = col\n    ))\n    \nlayout = go.Layout(dict(title = \"Treand of mean values\",\n                  xaxis = dict(title = 'Month'),\n                  yaxis = dict(title = 'Price (USD)'),\n                  ),legend=dict(\n                orientation=\"h\"),)\npy.iplot(dict(data=data, layout=layout), filename='basic-line')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d0e73c3ca9cae28623fb77eeb1ef4e0c5c22303d"},"cell_type":"markdown","source":"Well, for me it is difficult to interpret this, but it seems that returns for previous 10 days fluctuate the most."},{"metadata":{"_uuid":"7ce3cdada9c4560b8777dbbf3db5c67d1032caf0"},"cell_type":"markdown","source":"### News data"},{"metadata":{"trusted":true,"_uuid":"ba97b5f5630b0cf80e47b7bdf05fb8dd5ebee870"},"cell_type":"code","source":"news_train_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"746a676f5c99038eaa2675f1c829056f8a7eba71"},"cell_type":"code","source":"print(f'{news_train_df.shape[0]} samples and {news_train_df.shape[1]} features in the training news dataset.')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d999bbf073b48992439ad1ed45e4f22c6c676ef9"},"cell_type":"markdown","source":"The file is too huge to work with text directly, so let's see a wordcloud of the last 100000 headlines."},{"metadata":{"trusted":true,"_uuid":"abe755aae9cf82fd8eb687d3df7b0db1bcec622b"},"cell_type":"code","source":"text = ' '.join(news_train_df['headline'].str.lower().values[-1000000:])\nwordcloud = WordCloud(max_font_size=None, stopwords=stop, background_color='white',\n                      width=1200, height=1000).generate(text)\nplt.figure(figsize=(12, 8))\nplt.imshow(wordcloud)\nplt.title('Top words in headline')\nplt.axis(\"off\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"141ccfc6aa372c86c901e517f5fe26f2199c037d"},"cell_type":"code","source":"# Let's also limit the time period\nnews_train_df = news_train_df.loc[news_train_df['time'] >= '2010-01-01 22:00:00+0000']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4fd2fa4fddc287426bad197666b5788006bab8b4"},"cell_type":"code","source":"(news_train_df['urgency'].value_counts() / 1000000).plot('bar');\nplt.xticks(rotation=30);\nplt.title('Urgency counts (mln)');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"676e3a99d8decf96d38a3a0807e10cd5b647be2d"},"cell_type":"markdown","source":"Well, it seems that in fact urgency \"2\" is almost never used."},{"metadata":{"trusted":true,"_uuid":"103ca16b4a4431358d6b49f9013d11ddb7f9452a"},"cell_type":"code","source":"news_train_df['sentence_word_count'] =  news_train_df['wordCount'] / news_train_df['sentenceCount']\nplt.boxplot(news_train_df['sentence_word_count'][news_train_df['sentence_word_count'] < 40]);","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ada478042b03c13217820e4f1bfa7343affd241c"},"cell_type":"markdown","source":"There are some big outliers, but sentences mostly have 15-25 words in them."},{"metadata":{"trusted":true,"_uuid":"a06cf9147c8093acbac7d123432587e5d32410b2","scrolled":true},"cell_type":"code","source":"news_train_df['provider'].value_counts().head(10)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"19b0aeeb5a37e7de6d75d456aecb98514da77d29"},"cell_type":"markdown","source":"It isn't surprising that Reuters is the most common provider :)"},{"metadata":{"trusted":true,"_uuid":"d700f29e213413d5fe3acecd9bfc9605895bca60"},"cell_type":"code","source":"(news_train_df['headlineTag'].value_counts() / 1000)[:10].plot('barh');\nplt.title('headlineTag counts (thousands)');","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"dc2cad235a461e096f0076b58b913fb5abd0d31c"},"cell_type":"markdown","source":"Well, most news are tagless."},{"metadata":{"trusted":true,"_uuid":"81c132598b64437c94bbf86a41a49e1527c25848","_kg_hide-input":true,"scrolled":false},"cell_type":"code","source":"for i, j in zip([-1, 0, 1], ['negative', 'neutral', 'positive']):\n    df_sentiment = news_train_df.loc[news_train_df['sentimentClass'] == i, 'assetName']\n    print(f'Top mentioned companies for {j} sentiment are:')\n    print(df_sentiment.value_counts().head(5))\n    print('')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8d1cab6278c7fdf3b70edab0e9db751a64484690"},"cell_type":"markdown","source":"I think it is quite funny that Apple is a company with most both negative and positive sentiments."},{"metadata":{"_uuid":"6a79b9cba0ccf3d18554411cbf368305f3ba38e3"},"cell_type":"markdown","source":"At first I was sad that we don't have access to the texts of the news, but I have realized that we won't be able to use them anyway due to kernel memory limitations."},{"metadata":{"_uuid":"8e784d89a6731a06bc75b5c0ded3730ec6d43701"},"cell_type":"markdown","source":"## Modelling\n\nIt's time to build a model!\nI think that in this case we should build a binary classifier - we will simply predict whether the target goes up or down."},{"metadata":{"trusted":true,"_uuid":"57973e95286b66b3f475859c110d12db37873609","_kg_hide-input":true},"cell_type":"code","source":"#%%time\n# code mostly takes from this kernel: https://www.kaggle.com/ashishpatel26/bird-eye-view-of-two-sigma-xgb\n\ndef data_prep(market_df,news_df):\n    market_df['time'] = market_df.time.dt.date\n    market_df['returnsOpenPrevRaw1_to_volume'] = market_df['returnsOpenPrevRaw1'] / market_df['volume']\n    market_df['close_to_open'] = market_df['close'] / market_df['open']\n    market_df['volume_to_mean'] = market_df['volume'] / market_df['volume'].mean()\n    news_df['sentence_word_count'] =  news_df['wordCount'] / news_df['sentenceCount']\n    news_df['time'] = news_df.time.dt.hour\n    news_df['sourceTimestamp']= news_df.sourceTimestamp.dt.hour\n    news_df['firstCreated'] = news_df.firstCreated.dt.date\n    news_df['assetCodesLen'] = news_df['assetCodes'].map(lambda x: len(eval(x)))\n    news_df['assetCodes'] = news_df['assetCodes'].map(lambda x: list(eval(x))[0])\n    news_df['headlineLen'] = news_df['headline'].apply(lambda x: len(x))\n    news_df['assetCodesLen'] = news_df['assetCodes'].apply(lambda x: len(x))\n    news_df['asset_sentiment_count'] = news_df.groupby(['assetName', 'sentimentClass'])['time'].transform('count')\n    news_df['asset_sentence_mean'] = news_df.groupby(['assetName', 'sentenceCount'])['time'].transform('mean')\n    lbl = {k: v for v, k in enumerate(news_df['headlineTag'].unique())}\n    news_df['headlineTagT'] = news_df['headlineTag'].map(lbl)\n    kcol = ['firstCreated', 'assetCodes']\n    news_df = news_df.groupby(kcol, as_index=False).mean()\n\n    market_df = pd.merge(market_df, news_df, how='left', left_on=['time', 'assetCode'], \n                            right_on=['firstCreated', 'assetCodes'])\n\n    lbl = {k: v for v, k in enumerate(market_df['assetCode'].unique())}\n    market_df['assetCodeT'] = market_df['assetCode'].map(lbl)\n    \n    market_df = market_df.dropna(axis=0)\n    \n    return market_df\n\nmarket_train_df.drop(['price_diff', 'assetName_mean_open', 'assetName_mean_close'], axis=1, inplace=True)\nmarket_train = data_prep(market_train_df, news_train_df)\nprint(market_train.shape)\nup = market_train.returnsOpenNextMktres10 >= 0\n\nfcol = [c for c in market_train.columns if c not in ['assetCode', 'assetCodes', 'assetCodesLen', 'assetName', 'assetCodeT',\n                                             'firstCreated', 'headline', 'headlineTag', 'marketCommentary', 'provider',\n                                             'returnsOpenNextMktres10', 'sourceId', 'subjects', 'time', 'time_x', 'universe','sourceTimestamp']]\n\nX = market_train[fcol].values\nup = up.values\nr = market_train.returnsOpenNextMktres10.values\n\n# Scaling of X values\nmins = np.min(X, axis=0)\nmaxs = np.max(X, axis=0)\nrng = maxs - mins\nX = 1 - ((maxs - X) / rng)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9695eface6eee79f079e168d97ced1835e868436"},"cell_type":"code","source":"X_train, X_test, up_train, up_test, r_train, r_test = model_selection.train_test_split(X, up, r, test_size=0.1, random_state=99)\n\n# xgb_up = XGBClassifier(n_jobs=4,\n#                        n_estimators=300,\n#                        max_depth=3,\n#                        eta=0.15,\n#                        random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5190fffb5a49859564ac76fcd5cbace09fce50da"},"cell_type":"code","source":"params = {'learning_rate': 0.01, 'max_depth': 12, 'boosting': 'gbdt', 'objective': 'binary', 'metric': 'auc', 'is_training_metric': True, 'seed': 42}\nmodel = lgb.train(params, train_set=lgb.Dataset(X_train, label=up_train), num_boost_round=2000,\n                  valid_sets=[lgb.Dataset(X_train, label=up_train), lgb.Dataset(X_test, label=up_test)],\n                  verbose_eval=100, early_stopping_rounds=100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1b5b48c148b13ccaac68f6415e8da6ca4d3a9d21","_kg_hide-input":true},"cell_type":"code","source":"def generate_color():\n    color = '#{:02x}{:02x}{:02x}'.format(*map(lambda x: np.random.randint(0, 255), range(3)))\n    return color\n\ndf = pd.DataFrame({'imp': model.feature_importance(), 'col':fcol})\ndf = df.sort_values(['imp','col'], ascending=[True, False])\ndata = [df]\nfor dd in data:  \n    colors = []\n    for i in range(len(dd)):\n         colors.append(generate_color())\n\n    data = [\n        go.Bar(\n        orientation = 'h',\n        x=dd.imp,\n        y=dd.col,\n        name='Features',\n        textfont=dict(size=20),\n            marker=dict(\n            color= colors,\n            line=dict(\n                color='#000000',\n                width=0.5\n            ),\n            opacity = 0.87\n        )\n    )\n    ]\n    layout= go.Layout(\n        title= 'Feature Importance of LGB',\n        xaxis= dict(title='Columns', ticklen=5, zeroline=False, gridwidth=2),\n        yaxis=dict(title='Value Count', ticklen=5, gridwidth=2),\n        showlegend=True\n    )\n\n    py.iplot(dict(data=data,layout=layout), filename='horizontal-bar')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b7b548b08eb281b4a341d235cbeea8b7e1201a36"},"cell_type":"code","source":"days = env.get_prediction_days()\nimport time\n\nn_days = 0\nprep_time = 0\nprediction_time = 0\npackaging_time = 0\nfor (market_obs_df, news_obs_df, predictions_template_df) in days:\n    n_days +=1\n    if n_days % 50 == 0:\n        print(n_days,end=' ')\n    \n    t = time.time()\n    market_obs_df = data_prep(market_obs_df, news_obs_df)\n    market_obs_df = market_obs_df[market_obs_df.assetCode.isin(predictions_template_df.assetCode)]\n    X_live = market_obs_df[fcol].values\n    X_live = 1 - ((maxs - X_live) / rng)\n    prep_time += time.time() - t\n    \n    t = time.time()\n    lp = model.predict(X_live)\n    prediction_time += time.time() -t\n    \n    t = time.time()\n    confidence = 2 * lp -1\n    preds = pd.DataFrame({'assetCode':market_obs_df['assetCode'],'confidence':confidence})\n    predictions_template_df = predictions_template_df.merge(preds,how='left').drop('confidenceValue',axis=1).fillna(0).rename(columns={'confidence':'confidenceValue'})\n    env.predict(predictions_template_df)\n    packaging_time += time.time() - t\n    \nenv.write_submission_file()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"12a65ba92ed0f1185a8c4b30c0f8142da4ec8418"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}