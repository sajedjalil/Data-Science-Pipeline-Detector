{"cells":[{"metadata":{"_uuid":"da140a9f983b988928cd4262cdcb06e2acb4949e"},"cell_type":"markdown","source":"This notebook presents a LGBM model where both the `market` and `news` data are merged. In order to facilitate the data preparation and the subsequent update during prediction stage, the model is embedded in a python Class. In the first part of the notebook, the Class funcionalities are presented, step-by-step, by adding new methods to the class instance. In a second stage, the real training is performed.\n\n<a id=\"top\"></a> <br>\n## Notebook  Content\n1. [Utility functions](#1)\n1. [Class definition](#2)\n    1. [Selecting training dates](#3)\n    1. [Feature engineering](#4)\n        1. [Engineering the News data](#5)\n        1. [merging the News and Market data](#6)\n        1. [ Indexing categorical columns](#7)\n        1. [ Lagged information](#8)\n    1. [Feature selection](#9)\n1. [Training the model](#10)\n1. [Model pipeline](#11)\n1. [Test set predictions](#12)"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true,"_kg_hide-output":false,"scrolled":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport lightgbm as lgb\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom joblib import parallel, delayed\nimport gc\nimport sys\nimport pytz\nimport warnings\nimport time\nimport inspect\nimport datetime\nfrom itertools import chain\nfrom datetime import date, timedelta\nfrom kaggle.competitions import twosigmanews\nwarnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n#______________________________________________________________\nenv = twosigmanews.make_env()\n(market_train, news_train) = env.get_training_data()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"59b9d69a11f5f2b2e9eeb10f930e39fb2eecfce0"},"cell_type":"markdown","source":"<a id=\"1\"></a> <br>\n## 1. Utility functions\nWe first load some utility functions. The first, written by [Guillaume Martin](https://www.kaggle.com/gemartin/load-data-reduce-memory-usage#), allows to manage memory:"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() / 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() / 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n    return df","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"51a4a3bc0f1498af60a3e3aae98eb43ea61c3b9f"},"cell_type":"markdown","source":"The second helps tracking memory usage:"},{"metadata":{"trusted":true,"_uuid":"b4363ed7b70f7f911279b82bf9152806d60eb432","_kg_hide-input":true},"cell_type":"code","source":"def show_mem_usage():\n    ipython_vars = ['In', 'Out', 'exit', 'quit', 'get_ipython', 'ipython_vars']\n    list_objects=[]\n    mem = 0\n    for x in globals():\n        if x.startswith('_'): continue\n        if x in sys.modules: continue \n        if x in ipython_vars: continue\n        if isinstance(globals().get(x), pd.DataFrame):\n            mem = sys.getsizeof(globals().get(x))/1e+6\n            if mem > 1:\n                list_objects.append([x, mem ])\n        else:\n            for o in dir(globals().get(x)):\n                if o.startswith('__'): continue\n                mem = sys.getsizeof(getattr(globals().get(x), o))/1e+6\n                if mem > 1:\n                    list_objects.append(['.'.join([x,o]), mem ])\n    return sorted(list_objects, key=lambda x: x[1], reverse=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f832c994b253c1d140770057ba0666e6358fa8bf"},"cell_type":"markdown","source":"Finally, we introduce a decorator that allow to time functions:"},{"metadata":{"trusted":true,"_uuid":"ddf0082618fffb8b3f7200b7697807115256db54","_kg_hide-input":true},"cell_type":"code","source":"def timeit(method):\n    def timed(*args, **kw):\n        ts = time.time()\n        result = method(*args, **kw)\n        te = time.time()\n        if TIME_FUNCTIONS:\n            dt = (te - ts)*1000\n            if dt < 1000:\n                print ('{:<40}  {:>20.2f} ms'.format(method.__name__, dt))\n            else:\n                print ('{:<40}  {:>20.2f} s'.format(method.__name__, dt/1000))\n        return result\n    return timed","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1c284f3895011bdab85ac2a46799ea91f72efa75"},"cell_type":"markdown","source":"We can check in which objects the memory is currently allocated: "},{"metadata":{"trusted":true,"_uuid":"b8baa3aede599f0f8bed7f5088f10586bc4f324a","scrolled":true},"cell_type":"code","source":"gc.collect()\nmemory_used = show_mem_usage()\nprint(\"approximate memory usage: {:>5.2f}GB\".format(sum([s[1] for s in memory_used])/1000))\nmemory_used","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c00246f23dadde674a6d92365583f0fc26f63082"},"cell_type":"markdown","source":"<a id=\"2\"></a> <br>\n## 1. Class definition\nIn this section, I develop a class that contains the feature engineering and training of the model. For a didactic purpose, the various methods are are introduced consecutively and are thus added to the instance that contains the model. In production, all the methods would be merged while first defining the class."},{"metadata":{"trusted":true,"_uuid":"38a0fc843b53dd18f77043f0d3223f0599d72002"},"cell_type":"code","source":"class model2SigmaStockPrizes():\n    #_______________________\n    # class initialisation\n    @timeit\n    def __init__(self, market, news, verbose=False):\n        self.market = market.sort_values('time')\n        self.news = news.sort_values('time')\n        self.market = reduce_mem_usage(self.market, verbose)\n        self.news = reduce_mem_usage(self.news, verbose)\n        self.verbose = verbose\n        self._format_dates()\n        self._convert_booleans()\n    #________________________________________________________________________\n    # detection of columns with dates and times, then reduced to single dates\n    @timeit\n    def _format_dates(self):\n        for df in [self.market, self.news]:\n            datetime_cols = [c for c in df.columns if 'date' in str(df[c].dtypes)]\n            for col in datetime_cols:\n                df[col] = df[col].dt.normalize()\n                if self.verbose: \n                    print (\"Content of column:'{}' set as date\".format(col))\n    #_________________________________\n    # convert booleans columns to int\n    @timeit\n    def _convert_booleans(self):\n        for col in self.news.columns:\n            if self.news[col].dtype == bool: \n                self.news[col] = self.news[col].astype(int)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4f1b5739ec141adc1e40b94b8a18f7ee43484c04"},"cell_type":"markdown","source":"We create the class instance:"},{"metadata":{"trusted":true,"_uuid":"7940faab9c53ba3e671c17b2d14a37f9e0cb210a"},"cell_type":"code","source":"TIME_FUNCTIONS = True\ntwo_sigma_model =  model2SigmaStockPrizes(market_train, news_train, verbose=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2fab6228330660606e889537e464f129b388a80e"},"cell_type":"code","source":"del news_train\ndel market_train\ngc.collect()\nshow_mem_usage()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8d553635aafbf004799816ce96d4dee86499eabe"},"cell_type":"markdown","source":"<a id=\"3\"></a> <br>\n### 1.1 Selecting training dates\nTo reduce the time spent to pre-process the data, and ease the FE exploration, we can limit the number of dates during the exploration. Moreover, as outlined in a few discussions and kernels, the data before 2009 may false the models and it is safe to forget about this period. Hence, the following method allows to select the first date to account for:"},{"metadata":{"trusted":true,"scrolled":false,"_uuid":"c1ee9ca374d428fda3662022113d156face51736"},"cell_type":"code","source":"@timeit\ndef select_dates(self, first_date):\n    self.market = self.market.loc[self.market['time'] >= first_date]\n    self.news = self.news.loc[self.news['time'] >= first_date]\n    if self.verbose:\n        print(\"data before '{}' has been removed\".format(first_date))\n#______________________________________________________________________\ntwo_sigma_model.select_dates = select_dates.__get__(two_sigma_model)\ntwo_sigma_model.select_dates(datetime.datetime(2009, 1, 1, 0, 0, 0, 0, pytz.UTC))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e643ceda451de792fb625acf2f19cb5db4e5bcde"},"cell_type":"code","source":"gc.collect()\nshow_mem_usage()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"700a879142f161c4715671a709da9fac2a3eeb6c"},"cell_type":"markdown","source":"<a id=\"4\"></a> <br>\n### 1.2 Feature engineering\nA first step consists in performing some basic feature engineering:"},{"metadata":{"trusted":true,"_uuid":"3c2b1b6500e9ba38a43b40edc6b93f6d57125e18"},"cell_type":"code","source":"@timeit\ndef column_combinations(self):\n    self.market['price_diff'] = self.market['close'] - self.market['open']\n\n@timeit\ndef asset_codes_encoding(self):\n    self.news['assetCodesLen'] = self.news['assetCodes'].map(lambda x: len(eval(x)))\n    self.news['assetCodes'] = self.news['assetCodes'].str.findall(f\"'([\\w\\./]+)'\").apply(lambda x: x[0])\n\n@timeit\ndef news_count(self):\n    t = self.news.groupby(['time', 'assetName']).size().reset_index(name='news_count')\n    self.news = pd.merge(self.news, t, on=['time', 'assetName'])\n#______________________________________________________________________\ntwo_sigma_model.column_combinations = column_combinations.__get__(two_sigma_model)\ntwo_sigma_model.asset_codes_encoding = asset_codes_encoding.__get__(two_sigma_model)\ntwo_sigma_model.news_count = news_count.__get__(two_sigma_model)\n\ntwo_sigma_model.column_combinations()\ntwo_sigma_model.asset_codes_encoding()\ntwo_sigma_model.news_count()    ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"22eb327607f5187473744f3fc8a954dc6006fcec"},"cell_type":"markdown","source":"<a id=\"5\"></a> <br>\n#### 1.2.1 Engineering  the `News` data\nThe `news` data contains a few numerical columns. For each instrument and each day, we aggregate the corresponding variables with common statistics:"},{"metadata":{"trusted":true,"_uuid":"4979f5e1e8cb5850f65f70a7a7582e2f00b1bfeb"},"cell_type":"code","source":"@timeit\ndef aggregate_numericals(self):\n    numerics = ['int8', 'int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    self.news_numerical = self.news.select_dtypes(include=numerics).copy()\n    self.news_numerical_columns = self.news_numerical.columns\n    self.news_numerical.loc[:, 'assetCodes'] = self.news['assetCodes'] \n    self.news_numerical.loc[:, 'time'] = self.news['time']\n\n    agg_func = {\n        'takeSequence': ['sum', 'mean', 'max', 'min', 'std'],\n        'bodySize': ['sum', 'mean', 'max', 'min', 'std'],\n        'marketCommentary': ['sum', 'mean'],\n        'sentenceCount': ['sum', 'mean', 'max', 'min', 'std'],\n        'wordCount': ['sum', 'mean', 'max', 'min', 'std'],\n        'relevance': ['sum', 'mean', 'max', 'min', 'std'],\n        'firstMentionSentence': ['sum', 'mean', 'max', 'min', 'std'],\n        'sentimentNegative': ['sum', 'mean', 'max', 'min', 'std'],\n        'sentimentNeutral': ['sum', 'mean', 'max', 'min', 'std'],\n        'sentimentPositive': ['sum', 'mean', 'max', 'min', 'std'],\n        'sentimentWordCount': ['sum', 'mean', 'max', 'min', 'std'],\n        'noveltyCount12H': ['sum', 'mean', 'max', 'min', 'std'],\n        'noveltyCount24H': ['sum', 'mean', 'max', 'min', 'std'],\n        'noveltyCount3D': ['sum', 'mean', 'max', 'min', 'std'],\n        'noveltyCount5D': ['sum', 'mean', 'max', 'min', 'std'],\n        'noveltyCount7D': ['sum', 'mean', 'max', 'min', 'std'],\n        'volumeCounts12H': ['sum', 'mean', 'max', 'min', 'std'],\n        'volumeCounts24H': ['sum', 'mean', 'max', 'min', 'std'],\n        'volumeCounts3D': ['sum', 'mean', 'max', 'min', 'std'],\n        'volumeCounts5D': ['sum', 'mean', 'max', 'min', 'std'],\n        'volumeCounts7D': ['sum', 'mean', 'max', 'min', 'std'],\n        'news_count': ['max']\n        }\n\n    self.news_numerical = self.news_numerical.groupby(['assetCodes', 'time']).agg(agg_func)\n    self.news_numerical.columns = ['_'.join(col).strip()\n                                   for col in self.news_numerical.columns.values]\n    self.news_numerical.reset_index(inplace=True)\n#______________________________________________________________________\ntwo_sigma_model.aggregate_numericals = aggregate_numericals.__get__(two_sigma_model)\n#two_sigma_model.aggregate_numericals() ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"87dba90861fc10e6df956f0dc00219a0768c3cb5"},"cell_type":"markdown","source":"Concerning the categorical columns, we only consider the last entry when for an instrument, there are multiple entries on a single day:"},{"metadata":{"trusted":true,"_uuid":"7cd32f6c1926f53c4fea9baae4723d7b3e8c6776"},"cell_type":"code","source":"@timeit\ndef aggregate_categoricals(self): \n    categ_columns = [col for col in self.news.columns\n                     if col not in self.news_numerical_columns]\n    if self.verbose:\n        print(\"news's categorical columns:\\n\", categ_columns)\n    temp = self.news[self.news['news_count'] > 1][categ_columns].copy()\n    multiple_articles = temp.groupby(['assetCodes', 'time']).tail(1)\n    single_articles = self.news[self.news['news_count'] == 1][categ_columns]\n    self.news_categ = pd.concat([multiple_articles, single_articles])\n#________________________________________________________________________________________\ntwo_sigma_model.aggregate_categoricals = aggregate_categoricals.__get__(two_sigma_model)\n#two_sigma_model.aggregate_categoricals() ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"eadbca3c79945e0d9d96c20ea80fd7ecdf17dc20"},"cell_type":"markdown","source":"<a id=\"6\"></a> <br>\n#### 1.2.2 merging the `News` and `Market` data"},{"metadata":{"trusted":true,"_uuid":"8aa552741323bb7ae1626a641af9593e68283e76"},"cell_type":"code","source":"@timeit\ndef merge_news(self):\n    self.news = pd.merge(self.news_numerical,\n                         self.news_categ,\n                         on = ['assetCodes', 'time'],\n                         how='left')\n    # freeing memory\n    self.news_categ = 0\n    self.news_numerical = 0\n\n@timeit\ndef merge_market_news(self, keep_news=False):\n    if keep_news:\n        self.merged_df = pd.merge(self.market,\n                                  self.news,\n                                  left_on=['time', 'assetCode'],\n                                  right_on=['time', 'assetCodes'],\n                                  how='left')\n    else:\n        self.merged_df = self.market\n    # freeing memory\n    self.news = 0\n    self.market = 0\n    self.merged_df = reduce_mem_usage(self.merged_df, self.verbose)\n    if self.verbose:\n        print(\"merged_df's shape is: {}\".format(self.merged_df.shape))\n#_________________________________________________________________________\ntwo_sigma_model.merge_news = merge_news.__get__(two_sigma_model)\ntwo_sigma_model.merge_market_news = merge_market_news.__get__(two_sigma_model)\n\n#two_sigma_model.merge_news()\ntwo_sigma_model.merge_market_news() ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"573b9b6746e234e8b010e6e509e3324e518ef4d3"},"cell_type":"code","source":"gc.collect()\nshow_mem_usage()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b86398cd102c6c4cde57f19e4c9bbeb5ecd67639"},"cell_type":"markdown","source":"<a id=\"7\"></a> <br>\n####  1.2.3 Indexing categorical columns\nAt this stage, we define the categorical columns we will subsequently introduce in LGBM as `categorical_features`. These index is set during training and then reloaded during the prediction stage:"},{"metadata":{"trusted":true,"_uuid":"8e3cb62412f637787d1877e110bbcad27ff76f59"},"cell_type":"code","source":"@timeit\ndef set_labels(self, labeled_columns, define_index=False):\n    #____________________________________________________\n    # indexation during training stage\n    if define_index:\n        self.indexer = {}\n        for col in labeled_columns:\n            _, self.indexer[col] = pd.factorize(self.merged_df[col])\n    # label encoding\n    self.categorical_columns = labeled_columns\n    if self.verbose:\n        print(\"categorical variables: {}\".format(labeled_columns))\n    for col in labeled_columns:\n        self.merged_df[col] = self.indexer[col].get_indexer(self.merged_df[col])\n#_____________________________________________________________________________\ntwo_sigma_model.set_labels = set_labels.__get__(two_sigma_model)\n#two_sigma_model.set_labels(['headlineTag', 'provider'], define_index=True)    \ntwo_sigma_model.set_labels([], define_index=True)    ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1664bd2a8999faf2505db43910bd87aad535c1d2"},"cell_type":"markdown","source":"<a id=\"8\"></a> <br>\n#### 1.2.4 Lagged information\nA way to introduce the time is to create variables that contain past information (either time averages or lagged quantities). Below, we consider the returns on the `close` and `open` variables with respect to past values:"},{"metadata":{"trusted":true,"_uuid":"398dd86300342e1b7bb6bc923a2da6a88f3f6c70"},"cell_type":"code","source":"@timeit\ndef save_history(self):\n    self.common_lag_features = ['time', 'assetCode'] \n    self.used_for_news_lag = [] # ['sentimentNegative_mean']\n    self.used_for_lag = ['price_diff', 'close']\n    self.history_df =  self.merged_df[self.common_lag_features +\n                                      self.used_for_lag +\n                                      self.used_for_news_lag].copy()\n#_____________________________________________________________________________\ntwo_sigma_model.save_history = save_history.__get__(two_sigma_model)\ntwo_sigma_model.save_history()    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"11929ea3b42353923c73976ae36c89fc2f882489"},"cell_type":"code","source":"gc.collect()\nshow_mem_usage()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c96869e155121985ab6b6e012ffcc6a2d113794d"},"cell_type":"code","source":"def run_parallel(grouped, method, verbose):\n    with parallel.Parallel(n_jobs=-1, verbose=verbose) as par:\n        segments = par(delayed(method)(df_seg) for df_seg in grouped)\n    return segments\n\ndef account_for_past(df_code, n_lag=[7, 14, 21], shift_size=1):\n    features = [c for c in df_code.columns if c not in ['assetCode']]\n    lag_columns = []\n    for col in features:\n        for window in n_lag:\n            if LAST_DATE_ONLY:\n                tmp = df_code[df_code.index.max() - 2*timedelta(window):df_code.index.max()]\n                rolled = tmp[col].shift(shift_size, freq='D').rolling(window=window)\n            else:\n                rolled = df_code[col].shift(shift_size, freq='D').rolling(window=window)\n                \n#             for fct in [np.mean, min, max]:\n#                 col_name = '{}_{}_past{}'.format(col, fct.__name__, window)\n#                 lag_columns.append(col_name)\n#                 df_code[col_name] = rolled.apply(fct)\n\n                col_name = '{}_median_past{}'.format(col, window)\n                lag_columns.append(col_name)\n                df_code[col_name] = rolled.median()\n            \n                col_name = '{}_max_past{}'.format(col, window)\n                lag_columns.append(col_name)\n                df_code[col_name] = rolled.max()\n                \n                col_name = '{}_min_past{}'.format(col, window)\n                lag_columns.append(col_name)\n                df_code[col_name] = rolled.min()\n                \n    return lag_columns, df_code.drop(features, axis=1)\n\ndef create_returns(df_code, n_days=[3, 5, 7, 12]):\n    features = [c for c in df_code.columns if c not in ['assetCode']]\n    lag_columns = []\n    for col in features:\n        for days in n_days:\n            col_shift = 'shift_{}_{}'.format(col, days)  \n            col_name = 'returns_{}_PrevRaw{}'.format(col, days)\n            lag_columns.append(col_name)\n            df_code.loc[:, col_shift] = df_code[col].shift(days, freq='D')\n            df_code[col_name] = (df_code[col_shift] - df_code[col]) / df_code[col_shift]\n            df_code.drop(col_shift, axis=1, inplace=True)\n    return lag_columns, df_code.drop(features, axis=1)\n\n@timeit\ndef define_lagged_var(self):\n    start_time = time.time()\n    list_df = [d[1][self.common_lag_features + ['price_diff']].set_index('time').copy()\n               for d in self.history_df.groupby('assetCode', sort=False)]\n    if self.verbose:\n        print('assetCodes: {}, time to group: {:<5.2f}s'.format(len(list_df), time.time() - start_time))\n        \n    grouped = run_parallel(list_df, account_for_past, int(self.verbose))\n    self.lag_columns = grouped[0][0]\n    self.merged_df = pd.merge(self.merged_df,\n                              pd.concat([d[1] for d in grouped]).reset_index(),\n                              on=self.common_lag_features,\n                              how='left')\n    \n@timeit\ndef define_lagged_return(self):\n    start_time = time.time()\n    list_df = [d[1][self.common_lag_features + ['close']].set_index('time').copy()\n               for d in self.history_df.groupby('assetCode', sort=False)]\n    if self.verbose:\n        print('assetCodes: {}, time to group: {:<5.2f}s'.format(len(list_df), time.time() - start_time))\n        \n    grouped = run_parallel(list_df, create_returns, int(self.verbose))\n    self.lag_columns = grouped[0][0]\n    self.merged_df = pd.merge(self.merged_df,\n                              pd.concat([d[1] for d in grouped]).reset_index(),\n                              on=self.common_lag_features,\n                              how='left')\n        \n    \n#_____________________________________________________________________________\ntwo_sigma_model.define_lagged_var = define_lagged_var.__get__(two_sigma_model)\ntwo_sigma_model.define_lagged_return = define_lagged_return.__get__(two_sigma_model)\nLAST_DATE_ONLY = False\ntwo_sigma_model.define_lagged_var()      \ntwo_sigma_model.define_lagged_return()      ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a644e0a851c3bcc32ae7a9bb539aaf56088c0ff8"},"cell_type":"code","source":"@timeit\ndef news_moving_averages(self, col, window):\n    col_name = 'news_mva_'+ col + '_' + str(window) + 'days'\n    if self.verbose:\n        print(\"column '{}' has been created\".format(col_name))\n    avg_col = \\\n        self.history_df[self.common_lag_features+[col]].set_index('time').\\\n        groupby('assetCode').rolling(window=window, freq='D').mean().reset_index()\n    avg_col.rename(columns = {col: col_name}, inplace = True)\n    self.merged_df = pd.merge(self.merged_df,\n                              avg_col[self.common_lag_features + [col_name]],\n                              on=self.common_lag_features,\n                              how='left')\n    return col_name\n\n@timeit\ndef calc_news_moving_averages(self):    \n    self.news_lag_columns = []\n    for col in ['sentimentNegative_mean']:\n        new_col = self.news_moving_averages(col, 7)\n        self.news_lag_columns.append(new_col)\n#_____________________________________________________________________________\ntwo_sigma_model.news_moving_averages = news_moving_averages.__get__(two_sigma_model)\ntwo_sigma_model.calc_news_moving_averages = calc_news_moving_averages.__get__(two_sigma_model)\n#two_sigma_model.calc_news_moving_averages()   ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"69e76a7e2615de5d534e7e77219b604671110cf2"},"cell_type":"markdown","source":"<a id=\"9\"></a> <br>\n#### 1.3 Feature selection\nBefore training, we select the variables we want to keep in our model:"},{"metadata":{"trusted":true,"_uuid":"81e258b22bc940e742089d2327f88b83ef8c2233"},"cell_type":"code","source":"@timeit\ndef select_variables(self):\n    removed_columns = [\n        'assetCode', 'assetCodes', 'assetCodesLen', 'assetName_x', 'assetName_y', 'assetName',\n        'audiences', 'firstCreated', 'headline', 'returnsOpenNextMktres10',\n        'sourceId', 'subjects', 'time', 'universe','sourceTimestamp']\n    self.selected_variables = [c for c in self.merged_df.columns if c not in removed_columns]\n    \n    if self.verbose:\n        print(\"variables kept: {}\".format(self.selected_variables))\n#_____________________________________________________________________________\ntwo_sigma_model.select_variables = select_variables.__get__(two_sigma_model)\ntwo_sigma_model.select_variables()  ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b736f4f07f4fce4bcb4ee078d33b1f9d3e3f0956"},"cell_type":"markdown","source":"<a id=\"10\"></a> <br>\n## 2. Training the model\n\nAs a first step, we split the data set in order to cross-validate the model according to time:"},{"metadata":{"trusted":true,"_uuid":"cbd7d7b9051b6daf2d2ea9436adce93090c2d07b"},"cell_type":"code","source":"@timeit\ndef define_time_cv(self):\n    X = self.merged_df[self.selected_variables]\n    target = self.merged_df['returnsOpenNextMktres10']\n\n    time = self.merged_df['time']\n    universe = self.merged_df['universe']\n\n    n_train = int(X.shape[0] * 0.8)\n    self.X_train, self.y_train = X.iloc[:n_train], target[:n_train]\n    self.X_valid, self.y_valid = X.iloc[n_train:], target[n_train:]\n    self.t_valid = time.iloc[n_train:]\n\n    # For valid data, keep only those with universe > 0. This will help calculate the metric\n    u_valid = (universe.iloc[n_train:] > 0)\n    self.t_valid = time.iloc[n_train:]\n\n    self.X_valid = self.X_valid[u_valid]\n    self.y_valid = self.y_valid[u_valid]\n    self.t_valid = self.t_valid[u_valid]\n    del u_valid\n#________________________________________________________________________________\ntwo_sigma_model.define_time_cv = define_time_cv.__get__(two_sigma_model)\ntwo_sigma_model.define_time_cv()  ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1afda0d1fbff99c1423459dafdea34e81196ec0e"},"cell_type":"markdown","source":"and then prepare the data for LGBM:"},{"metadata":{"trusted":true,"_uuid":"93e1ceef5011dd1549e02b22b00788bf97f0cc1e"},"cell_type":"code","source":"@timeit\ndef prep_lgbm_data(self):\n    \n    self.dtrain = lgb.Dataset(\n        self.X_train.values, self.y_train,\n        feature_name = self.selected_variables,\n        categorical_feature = self.categorical_columns,\n        free_raw_data = False)\n    \n    self.dvalid = lgb.Dataset(\n        self.X_valid.values, self.y_valid,\n        feature_name = self.selected_variables,\n        categorical_feature = self.categorical_columns,\n        free_raw_data = False)\n    \n    self.dvalid.params = {'extra_time': self.t_valid.factorize()[0]}\n#________________________________________________________________________________\ntwo_sigma_model.prep_lgbm_data = prep_lgbm_data.__get__(two_sigma_model)\ntwo_sigma_model.prep_lgbm_data() ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2f34923c876d543c1dc2821c5b9f640590eb56ec"},"cell_type":"markdown","source":"We define a function that encodes the competition's metric:"},{"metadata":{"trusted":true,"_uuid":"f51608f97db44de99c0308f23b99b75cc638cca4"},"cell_type":"code","source":"def sigma_score(preds, valid_data):\n    df_time = valid_data.params['extra_time']\n    labels = valid_data.get_label()\n    val = pd.DataFrame()\n    val['time'] = df_time\n    val['y'] = preds * labels.values\n    output = val.groupby('time').sum()\n    score = output['y'].mean() / output['y'].std()\n    return 'sigma_score', score, True","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"13417f3169a92cca34e5456baec133b8cfa2ed1b"},"cell_type":"markdown","source":"to finally train the model:"},{"metadata":{"trusted":true,"_uuid":"bab87ba83c4934ae073ec47b3dd55a6083bb6d84"},"cell_type":"code","source":"def clean_frames(self):\n    del self.merged_df\n#________________________________________________________________________________\ntwo_sigma_model.clean_frames = clean_frames.__get__(two_sigma_model)\ntwo_sigma_model.clean_frames()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0886b8466d203a952617cebcb21e92295d492d58"},"cell_type":"code","source":"two_sigma_model.X_train[-15:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"59bf13ed2741dcf5dee0d4b58020bac8fa6269ab"},"cell_type":"code","source":"gc.collect()\nshow_mem_usage()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4c5a7da47786c944661d3a6d775239711569903b"},"cell_type":"code","source":"@timeit\ndef train_model(self, lgb_params):\n    evals_result = {}\n    self.model = lgb.train(\n        lgb_params,\n        self.dtrain,\n        num_boost_round= 10000,\n        valid_sets=(self.dvalid,),\n        valid_names=('valid',),\n        verbose_eval=100,\n        early_stopping_rounds=200,\n        feval=sigma_score,\n        evals_result=evals_result\n    )\n    df_result = pd.DataFrame(evals_result['valid'])\n    self.num_boost_round, valid_score = \\\n        df_result['sigma_score'].idxmax()+1, df_result['sigma_score'].max()\n    \n    print(f'Best score was {valid_score:.5f} on round {self.num_boost_round}')\n    \n    del self.X_train\n    del self.y_train\n    del self.X_valid\n    del self.y_valid\n#________________________________________________________________________________\ntwo_sigma_model.train_model = train_model.__get__(two_sigma_model)\n\nlgb_params = dict(\n    objective = 'regression_l1',\n    learning_rate = 0.01,\n    num_leaves = 51,\n    max_depth = 8,\n    bagging_fraction = 0.9,\n    bagging_freq = 1,\n    feature_fraction = 0.9,\n    lambda_l1 = 0.0,\n    lambda_l2 = 1.0,\n    metric = 'None', \n    seed = 42)\n\n\ntwo_sigma_model.train_model(lgb_params) ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"704a1637f0c7a081f5394090d7ac02537dae2b50"},"cell_type":"markdown","source":"We can inspect the feature importance:"},{"metadata":{"trusted":true,"_uuid":"93d31d6906ee7e82950cdeaf9b5c743f5e2faa33"},"cell_type":"code","source":"# two_sigma_model.model.feature_name()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"02e00b29f17b18e603da50f7170a07e0438f19fc"},"cell_type":"code","source":"# liste = list(zip(two_sigma_model.model.feature_name(),\n#     two_sigma_model.model.feature_importance('gain')))\n# liste.sort(key = lambda x:x[1], reverse=True)\n# [x[0] for x in liste[:50]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2fbc2d92efbf61c066102503a38ff0f7b0c54236"},"cell_type":"code","source":"def feat_importances(self):\n    fig, ax = plt.subplots(1, 1, figsize=(11, 20))\n    lgb.plot_importance(self.model, ax, importance_type='gain')\n    fig.tight_layout()\n#________________________________________________________________________________\ntwo_sigma_model.feat_importances = feat_importances.__get__(two_sigma_model)\ntwo_sigma_model.feat_importances()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1bd1ed72ff220993ae1adea5c763bb71ffcf999c"},"cell_type":"markdown","source":"<a id=\"11\"></a> <br>\n## 3. Model pipeline\nDuring the prediction stage, the observations of each day are given one after the other and we have to make the prediction before loading the data of the next day.\nHence, in order to perform time averages or calculate lagged values, we have to enrich our data while it comes. We then define a method that concatentate the whole process described above:"},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"40ba79e03b888796448b3c356157b78d3c62d10c"},"cell_type":"code","source":"@timeit\ndef data_prep(self, market, news):\n    self.verbose = False\n    self.market = market\n    self.news = news\n    self._format_dates()\n    self._convert_booleans()\n    self.column_combinations()\n    self.asset_codes_encoding()\n    self.news_count()   \n    #self.aggregate_numericals()\n    #self.aggregate_categoricals() \n    #self.merge_news()\n    self.merge_market_news()\n    #self.set_labels(['headlineTag', 'provider'])  \n    self.set_labels([])  \n#________________________________________________________________________________\ntwo_sigma_model.data_prep = data_prep.__get__(two_sigma_model)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4dc347c2c4187418070ff92c9a6f4c11dc00aca5"},"cell_type":"markdown","source":"<a id=\"12\"></a> <br>\n## 4. Test set predictions"},{"metadata":{"trusted":true,"_uuid":"8a9eaa74e98e2b807ae3de140cd881daae440442"},"cell_type":"code","source":"days = env.get_prediction_days()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1c2acf24055c4121c3798269882c90c77f66c20f"},"cell_type":"markdown","source":"Finally, we define a method that adds a new day to the data currently available:"},{"metadata":{"trusted":true,"_uuid":"340bdb169574dfcb058cf64c80eed4ff5c2e4635"},"cell_type":"code","source":"@timeit\ndef add_new_date(self):\n    self.history_df = pd.concat([self.history_df,\n                                 self.merged_df[\n                                      self.common_lag_features +\n                                      self.used_for_lag +\n                                      self.used_for_news_lag]])\n    if self.verbose:\n        print(\"data for lagged quantities from {} to {}\".format(\n            self.lag_df['time'].min().date(),\n            self.lag_df['time'].max().date()))\n#_______________________________________________________________\ntwo_sigma_model.add_new_date = add_new_date.__get__(two_sigma_model)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b396f26db3048698bffea52e0b9976fa203e0c80"},"cell_type":"markdown","source":"and a method that performs the predictions:"},{"metadata":{"trusted":true,"_uuid":"44a46933390b943f289b19bf0176987513fef405"},"cell_type":"code","source":"@timeit\ndef predict(self, date, pred_template):\n    \n    df = self.merged_df[self.merged_df['time'] == date]\n    \n    predictions = self.model.predict(\n        df[self.selected_variables].values,\n        ntree_limit = self.num_boost_round\n    )\n    \n    preds = pd.DataFrame({'assetCode': df['assetCode'],\n                          'confidence': np.clip(predictions, -1, 1)})\n    \n    self.formated_pred = \\\n        pred_template.merge(preds, how='left').\\\n        drop('confidenceValue', axis=1).\\\n        fillna(0).\\\n        rename(columns={'confidence': 'confidenceValue'})\n#_______________________________________________________________\ntwo_sigma_model.predict = predict.__get__(two_sigma_model)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8db5734c4a3bfd2398a93c78f88075e64f421972"},"cell_type":"markdown","source":"We are then able to loop over the test set days and create our prediction: "},{"metadata":{"trusted":true,"_uuid":"5ba94f72c97bd9adff77bdaed679c59670e26c70"},"cell_type":"code","source":"two_sigma_model.history_df = \\\ntwo_sigma_model.history_df[two_sigma_model.history_df['time'] > (two_sigma_model.history_df['time'].max() - timedelta(30))]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2c24507d292608737173350a1d09a319bc5da3b7","scrolled":false},"cell_type":"code","source":"TIME_FUNCTIONS = False\nn_days = 0\nfor (market_obs_df, news_obs_df, predictions_template_df) in days:\n    n_days +=1\n    start_time = time.time()\n    print(n_days, end=' ')\n    two_sigma_model.data_prep(market_obs_df, news_obs_df)\n    date = two_sigma_model.merged_df['time'].max()\n    #_________________________________\n    two_sigma_model.add_new_date()\n    two_sigma_model.define_lagged_var()\n    two_sigma_model.define_lagged_return()\n    #two_sigma_model.calc_news_moving_averages()\n    two_sigma_model.select_variables()\n    two_sigma_model.predict(date, predictions_template_df)\n    env.predict(two_sigma_model.formated_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8048e6c71bbde93cabcc3913a4d95d97fd9a7b96"},"cell_type":"code","source":"env.write_submission_file()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"434722e1e476547e22ac78d6ae6e3715a04d2c7f"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3aded53d237be7236129754ce8d07e22b109c5df"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}