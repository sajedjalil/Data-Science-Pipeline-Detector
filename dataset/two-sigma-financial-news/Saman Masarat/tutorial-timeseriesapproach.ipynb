{"cells":[{"metadata":{"_uuid":"35763080eef4e52bb1fe28107eea873ff9fdeea9"},"cell_type":"markdown","source":"<a id=\"0\"></a> <br>\n## Kernel Headlines\n1. [Introduction and RoadMap](#1)\n    1. [What makes Time Series Special](#2)\n\t2. [Be Careful about Our Approach](#3)\n\t3. [Imports and Loading Data](#3)\n\t4. [Simple Functionalities of Time Series](#4)\n\t\n2. [Stationary in Time Series](#5)\n    1. [What does stationary means in time series](#6)\n\t2. [Stationary Parameters](#7)\n\t3. [Dickey Fuller Test](#8)\n\t4. [Make a Time Serie Stationary](#9)\n\t5. [Estimating and Eliminating Trend](#10)\n\t6. [Moving Average](#11)\n\t7. [Weighted Moving Average](#12)\n\t8. [Eliminating Trend and Seasonality](#13)\n\t    1. [Differencing](#14)\n\t\t2. [Decomposition](#15)\n    9. [Forecasting TimeSeries](#16)\n       1. [Introduction To ARIMA](#17)\n\t   2. [Autocorrelation Function](#18)\n\t   3. [Partial Autocorrelation Function(PACF)](#19)\n\t   4. [AR model](#20)\n\t   5. [MA model](#21)\n\t   6. [Combined Model](#22)\n\t10. [Taking It Back to Original Scale](#23)\n\t\n\n\n5. [References](#100)"},{"metadata":{"_uuid":"d8d9f202ae4f337e8be81a64712b116b668d80b1"},"cell_type":"markdown","source":"<a id=\"0\"></a> <br>\n#  1-Introduction and RoadMap"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"markdown","source":"Our journey would go through the following steps:\n * What makes Time Series Special?\n * Loading and Handling Time Series in Pandas\n * How to Check Stationarity of a Time Series?\n * How to make a Time Series Stationary?\n * Forecasting a Time Series\n\nI have used [This Tutorial](https://www.analyticsvidhya.com/blog/2016/02/time-series-forecasting-codes-python/) as a main idea for this kernel.  You can find more details there if you are motivated. "},{"metadata":{"_uuid":"c0b9bd3895b174c7dae14c5568043a24cfe583c2"},"cell_type":"markdown","source":"<a id=\"1\"></a> <br>\n**A. WHAT MAKES TIME SERIES**\n\nAs the name suggests, TS is a collection of data points collected at constant time intervals. These are analyzed to determine the long term trend so as to forecast the future or perform some other form of analysis. But what makes a TS different from say a regular regression problem? There are 2 things:\n\n1. It is time dependent. So the basic assumption of a linear regression model that the observations are independent doesn’t hold in this case.\n2. Along with an increasing or decreasing trend, most TS have some form of seasonality trends, i.e. variations specific to a particular time frame. For example, if you see the sales of a woolen jacket over time, you will invariably find higher sales in winter seasons."},{"metadata":{"_uuid":"d081304d6e1f3e96f74f9999ab85d6195bfad74f"},"cell_type":"markdown","source":"<a id=\"2\"></a> <br>\n**B. BE CAREFUL ABOUT OUR APPROACH**\n\nPlease note that we dont want to evaluate the process to get competition answer. Main goal of this kernel is providing tutorial to represnet how TimeSeries can be modeled evaluated and also can be used in production datasets.\n\nPlease note that the aim of this kernel is to familiarize you with the various techniques used for TS in general. Our main focus is analyzing the problem with TimeSeries approches. So, the results may be a little ambiguous. Dont hesitate to share your idea in comments. Any hint will be appreciated.\n\n"},{"metadata":{"_uuid":"fa49a059e6a7f8338e0db0748f6947d9cf605f56"},"cell_type":"markdown","source":"<a id=\"3\"></a> <br>\n**C. IMPORTS AND LOADING DATA**"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pylab as plt\nfrom datetime import datetime\n%matplotlib inline\nfrom matplotlib.pylab import rcParams\nrcParams['figure.figsize'] = 15, 6\nfrom kaggle.competitions import twosigmanews","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8e599fcb559b5b3eab00f737734570dd3be50d76"},"cell_type":"code","source":"env = twosigmanews.make_env()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e7e1667c6bde18f783cf7c574d767e30e62cb469"},"cell_type":"code","source":"market_data = env.get_training_data()[0]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"06cd2333e36afcfbf20f2984de7ea7f3dc0edfec"},"cell_type":"markdown","source":"<a id=\"4\"></a> <br>\n**D. SIMPLE FUNCTIONALITIES OF TIME SERIES **"},{"metadata":{"trusted":true,"_uuid":"7f1082e5816fac69507a22abbc5d17d100adc04d","scrolled":true},"cell_type":"code","source":"fig,axes = plt.subplots(1,1,figsize=(15,10))\naxes.set_title(\"Time Distro\")\naxes.set_ylabel(\"# of records\")\naxes.set_xlabel(\"date\")\naxes.plot(market_data.time.dt.date.value_counts().sort_index().index, market_data.time.dt.date.value_counts().sort_index().values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e88ab06dd659c3ab673b127664a96a21f53ab4f9"},"cell_type":"code","source":"market_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"efe28954abeffceed9199e3038ff79405e9e727a"},"cell_type":"code","source":"time_series_df = market_data[[\"time\"]].groupby(by=[\"time\"]).size()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2efeac96fc55d0e8cf44b33d4362aa40de7ef976"},"cell_type":"code","source":"time_series_df.index","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c84c7c49612b17a88c64efed94aacbe1882273fa"},"cell_type":"markdown","source":"As you can see, the index format is 'Index'. Lets change it to datetime index to being capable of using TimeSeries package facilities."},{"metadata":{"trusted":true,"_uuid":"f7f1567084b016bed24a97e103f18a368032fa1e"},"cell_type":"code","source":"time_series_df.index = pd.to_datetime(time_series_df.index)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"204149c1ece6b58f2573c0b3069301e84f8e9f77"},"cell_type":"code","source":"time_series_df.index","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f172663b0163f4bfdd3d6f184a6393e821e53bf7"},"cell_type":"markdown","source":"Now, its type is 'DatetimeIndex. Lets move on and continue ;-)'\n\nNow you can easily access to the values by the date you want in more easily way"},{"metadata":{"trusted":true,"_uuid":"a328ee98334f74c6d573a41eeab8d66ec68a239f"},"cell_type":"code","source":"time_series_df[\"2007-02-01\"]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e2616c10da42bf8773e1f894f2d3a7d72f6438b5"},"cell_type":"markdown","source":"There is also possible to do a range query."},{"metadata":{"trusted":true,"_uuid":"c8394f22b9e9599d99d587711658efe2ab64fabb"},"cell_type":"code","source":"time_series_df[\"2007-02-01\" : \"2007-02-10\"]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ec24ec80ebb101ffe8b843d2dddd66d6297b41e9"},"cell_type":"markdown","source":"Or also customized date range queries."},{"metadata":{"trusted":true,"_uuid":"aeb8eed6a363ed81c8d60c0ba6df8ad63d267b52"},"cell_type":"code","source":"time_series_df[\"2007-02\"]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bb8365e444f4d1ba2e15f24b7dd2be8f1ca988d5"},"cell_type":"markdown","source":"<a id=\"5\"></a> <br>\n#  2-Stationary in Time Series\n\n\n<a id=\"6\"></a> <br>\n**A. WHAT DOES STATIONARY MEANS IN TIME SERIES**\n\nA TS is said to be stationary if its statistical properties such as mean, variance remain constant over time.  **If the data is stationary we can anticipate that the treatment of users in the future will be similar to the old records.** It is completely obvious that analyzing the stationary data is easier than non-stationary one.\n\nReturn back to the time plot we had in previous section. Is the data stationary ?!\n\n<a id=\"7\"></a> <br>\n**B. STATIONARY PARAMETERS**\n\nStationarity is defined using very strict criterion. However, for practical purposes we can assume the series to be stationary if it has constant statistical properties over time, ie. the following:\n\n 1. constant mean\n 2. constant variance\n 3. An autocovariance that does not depend on time.\n"},{"metadata":{"_uuid":"ade444a9d7d6f7526ef3ac4a86bc837e191579f8"},"cell_type":"markdown","source":"**As mentioned before the main purpose of the kernel is learning, so we will use part of data which is completely compatible with TimeSeries approaches. As it is obvious the range of 2007 to 2009 have periodic treatment could be analyzed with TimeSeries approaches.**"},{"metadata":{"trusted":true,"_uuid":"660907ae43e707af35d75cda9f2f54d47dd96f9e"},"cell_type":"code","source":"fig,axes = plt.subplots(1,1,figsize=(15,10))\naxes.set_title(\"Time Distro\")\naxes.set_ylabel(\"# of records\")\naxes.set_xlabel(\"date\")\naxes.plot(time_series_df[\"2007\" : \"2008-9\"])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f1f85e338ed25e1b3236e63c0df3d8c41c44e6e6"},"cell_type":"markdown","source":"Ignore the missing values we have in end period of 2015. The increasing treatment can be observed. In the other words, it is overall increasing trend.\nHowever, it might not always be possible to make such visual inferences (we’ll see such cases later). So, more formally, we can check stationarity using the following:\nPlotting Rolling Statistics:\nWe can plot the moving average or moving variance and see if it varies with time. By moving average/variance I mean that at any instant ‘t’, we’ll take the average/variance of the last year, i.e. last 12 months. But again this is more of a visual technique.\n\n<a id=\"8\"></a> <br>\n**C. DICKEY-FULLER TEST**\n\nThis is one of the statistical tests for checking stationarity. Here the null hypothesis is that the TS is non-stationary. The test results comprise of a Test Statistic and some Critical Values for difference confidence levels. If the ‘Test Statistic’ is less than the ‘Critical Value’, we can reject the null hypothesis and say that the series is stationary"},{"metadata":{"trusted":true,"_uuid":"3ec58ffdd8980f6aac63afb5bd43c91a62c062da"},"cell_type":"code","source":"from statsmodels.tsa.stattools import adfuller\ndef test_stationarity(timeseries):\n    \n    #Determing rolling statistics\n#     rolmean = pd.rolling_mean(timeseries, window=12)\n#     rolstd = pd.rolling_std(timeseries, window=12)\n    \n    rolmean = timeseries.rolling(window=120).mean()\n    rolstd = timeseries.rolling(window=120).std()\n\n    #Plot rolling statistics:\n    plt.figure(figsize=(20,10))\n    orig = plt.plot(timeseries, color='blue',label='Original')\n    mean = plt.plot(rolmean, color='red', label='Rolling Mean')\n    std = plt.plot(rolstd, color='black', label = 'Rolling Std')\n    plt.legend(loc='best')\n    plt.title('Rolling Mean & Standard Deviation')\n    plt.show(block=False)\n    \n    #Perform Dickey-Fuller test:\n    print ('Results of Dickey-Fuller Test:')\n    dftest = adfuller(timeseries, autolag='AIC')\n    dfoutput = pd.Series(dftest[0:4], index=['Test Statistic','p-value','#Lags Used','Number of Observations Used'])\n    for key,value in dftest[4].items():\n        dfoutput['Critical Value (%s)'%key] = value\n    print (dfoutput)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b9ffd76695bc5bdcd03c7ad9a3b641a2e31d7e69"},"cell_type":"markdown","source":"As you can see in the following code, we have plotted original data, mean and standar deviation of it in one figure.\nNow, lets check is our data stationary ?!"},{"metadata":{"trusted":true,"_uuid":"399eb46d1f20372a16d20fd2ec65532062ec01a4"},"cell_type":"code","source":"time_series_df = time_series_df[\"2007\" : \"2008-9\"]\ntest_stationarity(time_series_df)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a08c6809e8e3692076e2d98ab8f8d0c55d625d33"},"cell_type":"markdown","source":"Though the variation in standard deviation is small, mean is clearly increasing with time and this is not a stationary series. Also, the test statistic is way more than the critical values. Note that the signed values should be compared and not the absolute values.\n\nNext, we’ll discuss the techniques that can be used to take this TS towards stationarity."},{"metadata":{"_uuid":"97f809415729be2e6778368fda5b72878b6026c3"},"cell_type":"markdown","source":"<a id=\"9\"></a> <br>\n**D. MAKE A TIME SERIE STATIONARY** "},{"metadata":{"_uuid":"c8fe6c702174de6a521add5e3173b04db9bb9079"},"cell_type":"markdown","source":"Though stationarity assumption is taken in many TS models, almost none of practical time series are stationary. So statisticians have figured out ways to make series stationary, which we’ll discuss now. Actually, its almost impossible to make a series perfectly stationary, but we try to take it as close as possible.\n\nLets understand what is making a TS non-stationary. There are 2 major reasons behind non-stationaruty of a TS:\n1. Trend – varying mean over time. For eg, in this case we saw that on average, the number of passengers was growing over time.\n2. Seasonality – variations at specific time-frames. eg people might have a tendency to buy cars in a particular month because of pay increment or festivals.\n\nThe underlying principle is to model or estimate the trend and seasonality in the series and remove those from the series to get a stationary series. Then statistical forecasting techniques can be implemented on this series. The final step would be to convert the forecasted values into the original scale by applying trend and seasonality constraints back.\n\nNote: I’ll be discussing a number of methods. Some might work well in this case and others might not. But the idea is to get a hang of all the methods and not focus on just the problem at hand.\n\nLet’s start by working on the trend part."},{"metadata":{"_uuid":"50e0282655c2cc6340ac5254b07db58b4dcafdfb"},"cell_type":"markdown","source":"<a id=\"10\"></a> <br>\n**E. ESTIMATING AND ELIMINATING TREND**\n\nOne of the first tricks to reduce trend can be transformation. For example, in this case we can clearly see that the there is a significant positive trend. So we can apply transformation which penalize higher values more than smaller values. These can be taking a log, square root, cube root, etc. Lets take a log transform here for simplicity:\n\n(ingore the interval that there is a no data)"},{"metadata":{"trusted":true,"_uuid":"d4fa04330c69bc48668360b4a7905199053b0ddf"},"cell_type":"code","source":"fig,axes = plt.subplots(1,1,figsize=(15,10))\naxes.set_title(\"Time Distro\")\naxes.set_ylabel(\"LOG(# of records)\")\naxes.set_xlabel(\"date\")\naxes.plot(time_series_df)\nts_log = np.log(time_series_df)\naxes.plot(ts_log)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a5ac8b38c13d22f1427b28a4b0299699fc7beb4a"},"cell_type":"markdown","source":"In this simpler case, it is easy to see a forward trend in the data. But its not very intuitive in presence of noise. So we can use some techniques to estimate or model this trend and then remove it from the series. There can be many ways of doing it and some of most commonly used are:\n\n1. Aggregation – taking average for a time period like monthly/weekly averages\n2. Smoothing – taking rolling averages\n3. Polynomial Fitting – fit a regression model\n\nI will discuss smoothing here and you should try other techniques as well which might work out for other problems. Smoothing refers to taking rolling estimates, i.e. considering the past few instances. There are can be various ways but I will discuss two of those here."},{"metadata":{"_uuid":"9eeed760030e90b9967b2cc4c9a0bd34edaee2fc"},"cell_type":"markdown","source":"<a id=\"11\"></a> <br>\n**F. Moving average**\n\nIn this approach, we take average of ‘k’ consecutive values depending on the frequency of time series. Here we can take the average over the past 1 year, i.e. last 12 values. Pandas has specific functions defined for determining rolling statistics.\n"},{"metadata":{"trusted":true,"_uuid":"f00fd05208dead671581a3bf27acb5b4eb694ef4"},"cell_type":"code","source":"fig,axes = plt.subplots(1,1,figsize=(15,10))\naxes.set_title(\"Time Distro\")\naxes.set_ylabel(\"LOG(# of records)\")\naxes.set_xlabel(\"date\")\n\nmoving_avg = ts_log.rolling(6).mean()\naxes.plot(ts_log)\naxes.plot(moving_avg, color='red')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4f9a1dfa35d6daf2984b050d6ff42ebd90f45b94"},"cell_type":"markdown","source":"The red line shows the rolling mean. Lets subtract this from the original series. Note that since we are taking average of last 6 values, rolling mean is not defined for first 5 values. This can be observed as:"},{"metadata":{"trusted":true,"_uuid":"5467b21045707bdabe92d4acc0ec21fb3109f1bc"},"cell_type":"code","source":"ts_log_moving_avg_diff = ts_log - moving_avg\nts_log_moving_avg_diff.head(6)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ac3e1c30a180b3a4778ff28432876a834840511b"},"cell_type":"markdown","source":"Notice the first 6 being Nan. Lets drop these NaN values and check the plots to test stationarity."},{"metadata":{"trusted":true,"_uuid":"97da84eabbc289876f73738f7d44b1bdfeb47ec0"},"cell_type":"code","source":"ts_log_moving_avg_diff.dropna(inplace=True)\ntest_stationarity(ts_log_moving_avg_diff)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"44269442109df94b7dc225326a3977f04ff22695"},"cell_type":"markdown","source":"This looks like a much better series. The rolling values appear to be varying slightly but there is no specific trend. Also, the test statistic is smaller than the 1% critical values so we can say with 99% confidence that this is a stationary series."},{"metadata":{"_uuid":"5938ac6e7e9ccd928111f2c91f7a58ed133ce691"},"cell_type":"markdown","source":"<a id=\"12\"></a> <br>\n**G. WEIGHTED MOVING AVERAGE**\n\nHowever, a drawback in this particular approach is that the time-period has to be strictly defined. In this case we can take 6 month averages but in complex situations like forecasting a stock price, its difficult to come up with a number. So we take a ‘weighted moving average’ where more recent values are given a higher weight. There can be many technique for assigning weights. A popular one is exponentially weighted moving average where weights are assigned to all the previous values with a decay factor.  This can be implemented in Pandas as:"},{"metadata":{"trusted":true,"_uuid":"ab667ec8fdab1ee2c574b105c12786f914350e5c"},"cell_type":"code","source":"# # expwighted_avg = pd.ewma(ts_log, halflife=12)\n# expwighted_avg = pd.DataFrame(ts_log).ewm(halflife=12).mean()\nfig,axes = plt.subplots(1,1,figsize=(15,10))\naxes.set_title(\"Expolatiornay Moving Average\")\naxes.set_ylabel(\"LOG(# of records)\")\naxes.set_xlabel(\"date\")\nexpwighted_avg = ts_log.ewm(halflife=6).mean()\nplt.plot(ts_log)\nplt.plot(expwighted_avg, color='red')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7c5cc0b9df4633d13b6c7154a4896af0eae1307a"},"cell_type":"markdown","source":"Note that here the parameter ‘halflife’ is used to define the amount of exponential decay. This is just an assumption here and would depend largely on the business domain. Other parameters like span and center of mass can also be used to define decay which are discussed in the link shared above. Now, let’s remove this from series and check stationarity:"},{"metadata":{"trusted":true,"_uuid":"7a92c5ad78a0ba9dd96e76fb688b262e06857987"},"cell_type":"code","source":"ts_log_ewma_diff = ts_log - expwighted_avg\ntest_stationarity(ts_log_ewma_diff)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"204e9956506169bf66c2253372a22611d6c7315e"},"cell_type":"markdown","source":"This TS has even lesser variations in mean and standard deviation in magnitude. Also, the test statistic is smaller than the 1% critical value, which is better than the previous case. Note that in this case there will be no missing values as all values from starting are given weights. So it’ll work even with no previous values.\n\nThe simple moving average we have done in previous section return TestStatistic -3.80 but Exploationary Moving Average returns -3.94 for TestStatic. Which is compeltely better than former."},{"metadata":{"_uuid":"b31a9cbcc35cd43b911e72f41c3482a6326f734d"},"cell_type":"markdown","source":"<a id=\"13\"></a> <br>\n**H. Eliminating Trend and Seasonality**\n\nThe simple trend reduction techniques discussed before don’t work in all cases, particularly the ones with high seasonality. Lets discuss two ways of removing trend and seasonality:\n\n1. Differencing – taking the differece with a particular time lag\n2. Decomposition – modeling both trend and seasonality and removing them from the model.\n\n\n<a id=\"14\"></a> <br>\n1. Differencing\n\nOne of the most common methods of dealing with both trend and seasonality is differencing. In this technique, we take the difference of the observation at a particular instant with that at the previous instant. This mostly works well in improving stationarity. First order differencing can be done in Pandas as:\n"},{"metadata":{"trusted":true,"_uuid":"e272d2af8c76df2a88e1a8a5bbf3c62c856fa7aa"},"cell_type":"code","source":"fig, axes = plt.subplots(1,1,figsize=(20,10))\nts_log_diff = ts_log - ts_log.shift()\nplt.plot(ts_log_diff)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c127ff4dd2b44b80302d106817db06620b645ac0"},"cell_type":"markdown","source":"This appears to have reduced trend considerably. Lets verify our method using test_stationary procedure we have done in earlier sections."},{"metadata":{"trusted":true,"_uuid":"ac488db0c8cbcd92dbe005c8c76b9c4f4d49ad13"},"cell_type":"code","source":"ts_log_diff.dropna(inplace=True)\ntest_stationarity(ts_log_diff)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"eb655f1c84cf32d4c8cab4244ba34ab42d82e3df"},"cell_type":"markdown","source":"As you can compare this result with previous ones, the moving average method had a better result in comparison to our method. Another point could be mentioned is this comparasion has been revealed that the assumption of seasonality is not correct for this data shift.\n\nWe can see that the mean and std variations have small variations with time. Also, the Dickey-Fuller test statistic is less than the 5% critical value, thus the TS is stationary with 95% confidence. We can also take second or third order differences which might get even better results in certain applications. \n\nLets check the same procedure for shifting with 10 preiod."},{"metadata":{"trusted":true,"_uuid":"7db69d846a4f0522fcf7f7cee0857ae4a02acbe6"},"cell_type":"code","source":"test_stationarity((ts_log - ts_log.shift(10)).dropna())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1ddf4051aee64e767410f56c2e0ed9ef81f5ee50"},"cell_type":"markdown","source":"As you can see, with 10 period Defferencing the data will be stationary with more than 99% confidentialy.\n\nNow, lets move on to Decomposing\n\n<a id=\"15\"></a> <br>\n2. Decomposition\n\nIn this approach, both trend and seasonality are modeled separately and the remaining part of the series is returned."},{"metadata":{"trusted":true,"_uuid":"b66f1033c8b8a80d0a333e45bc055182760ae33b"},"cell_type":"code","source":"from statsmodels.tsa.seasonal import seasonal_decompose\ndecomposition = seasonal_decompose(ts_log,freq=10)\n\ntrend = decomposition.trend\nseasonal = decomposition.seasonal\nresidual = decomposition.resid\n\nfig,ax = plt.subplots(figsize=(15,20))\nplt.subplot(411)\nplt.plot(ts_log, label='Original')\nplt.legend(loc='best')\nplt.subplot(412)\nplt.plot(trend, label='Trend')\nplt.legend(loc='best')\nplt.subplot(413)\nplt.plot(seasonal,label='Seasonality')\nplt.legend(loc='best')\nplt.subplot(414)\nplt.plot(residual, label='Residuals')\nplt.legend(loc='best')\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1fbf18035bfc7ae76124da2e4f1e3b0c5ec1add1"},"cell_type":"markdown","source":"Here we can see that the trend, seasonality are separated out from data and we can model the residuals. Lets check stationarity of residuals:"},{"metadata":{"trusted":true,"_uuid":"f1a816ec4b01629fc4c21ae92b56676cdb9013cd"},"cell_type":"code","source":"ts_log_decompose = residual\nts_log_decompose.dropna(inplace=True)\ntest_stationarity(ts_log_decompose)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e5c440b152ef3fb2709ad821d68b71c5171cd65e"},"cell_type":"markdown","source":"The Dickey-Fuller test statistic is significantly lower than the 1% critical value. So this TS is very close to stationary. You can try advanced decomposition techniques as well which can generate better results. Also, you should note that converting the residuals into original values for future data in not very intuitive in this case."},{"metadata":{"_uuid":"c6d24f565ad57ba01083aecee7bce135daed519f"},"cell_type":"markdown","source":"<a id=\"16\"></a> <br>\n**I. Forecasting TimeSeries**\n\nWe saw different techniques and all of them worked reasonably well for making the TS stationary. Lets make model on the TS after differencing as it is a very popular technique. Also, its relatively easier to add noise and seasonality back into predicted residuals in this case. Having performed the trend and seasonality estimation techniques, there can be two situations:\n\n1. A strictly stationary series with no dependence among the values. This is the easy case wherein we can model the residuals as white noise. But this is very rare.\n2. A series with significant dependence among values. In this case we need to use some statistical models like ARIMA to forecast the data.\n\nLet me give you a brief introduction to ARIMA. I won’t go into the technical details but you should understand these concepts in detail if you wish to apply them more effectively. ARIMA stands for Auto-Regressive Integrated Moving Averages. The ARIMA forecasting for a stationary time series is nothing but a linear (like a linear regression) equation. The predictors depend on the parameters (p,d,q) of the ARIMA model:\n\n<a id=\"17\"></a> <br>\n1. INTRODUCTION TO ARIMA\n\nNumber of AR (Auto-Regressive) terms (p): AR terms are just lags of dependent variable. For instance if p is 5, the predictors for x(t) will be x(t-1)….x(t-5).\n\nNumber of MA (Moving Average) terms (q): MA terms are lagged forecast errors in prediction equation. For instance if q is 5, the predictors for x(t) will be e(t-1)….e(t-5) where e(i) is the difference between the moving average at ith instant and actual value.\n\nNumber of Differences (d): These are the number of nonseasonal differences, i.e. in this case we took the first order difference. So either we can pass that variable and put d=0 or pass the original variable and put d=1. Both will generate same results.\n\nAn importance concern here is how to determine the value of ‘p’ and ‘q’. We use two plots to determine these numbers. Lets discuss them first.\n\n<a id=\"18\"></a> <br>\n2. AUTOCORRELATION FUNCTION(ACF)\n\nIt is a measure of the correlation between the the TS with a lagged version of itself. For instance at lag 5, ACF would compare series at time instant ‘t1’…’t2’ with series at instant ‘t1-5’…’t2-5’ (t1-5 and t2 being end points). Partial \n\n<a id=\"19\"></a> <br>\n3. PARTIAL AUTOCORRELATION FUNCTION (PACF)\n\nThis measures the correlation between the TS with a lagged version of itself but after eliminating the variations already explained by the intervening comparisons. Eg at lag 5, it will check the correlation but remove the effects already explained by lags 1 to 4.\n"},{"metadata":{"trusted":true,"_uuid":"ea2a7b5c8ea892dafe189cd74998c2e51034ac3d"},"cell_type":"code","source":"#ACF and PACF plots:\nfrom statsmodels.tsa.stattools import acf, pacf\nlag_acf = acf(ts_log_diff, nlags=20)\nlag_pacf = pacf(ts_log_diff, nlags=20, method='ols')\n#Plot ACF: \nplt.subplot(121) \nplt.plot(lag_acf)\nplt.axhline(y=0,linestyle='--',color='gray')\nplt.axhline(y=-1.96/np.sqrt(len(ts_log_diff)),linestyle='--',color='gray')\nplt.axhline(y=1.96/np.sqrt(len(ts_log_diff)),linestyle='--',color='gray')\nplt.title('Autocorrelation Function')\n\n#Plot PACF:\nplt.subplot(122)\nplt.plot(lag_pacf)\nplt.axhline(y=0,linestyle='--',color='gray')\nplt.axhline(y=-1.96/np.sqrt(len(ts_log_diff)),linestyle='--',color='gray')\nplt.axhline(y=1.96/np.sqrt(len(ts_log_diff)),linestyle='--',color='gray')\nplt.title('Partial Autocorrelation Function')\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2ecee97c2259fc5f20df712ac850e97f53a6ab38"},"cell_type":"markdown","source":"In this plot, the two dotted lines on either sides of 0 are the confidence interevals. These can be used to determine the ‘p’ and ‘q’ values as:\n\n1. p – The lag value where the PACF chart crosses the upper confidence interval for the first time. If you notice closely, in this case p=1.5\n2. q – The lag value where the ACF chart crosses the upper confidence interval for the first time. If you notice closely, in this case q=1.25.\n\nNow, lets make 3 different ARIMA models considering individual as well as combined effects. I will also print the RSS for each. Please note that here RSS is for the values of residuals and not actual series.\n\nWe need to load the ARIMA model first"},{"metadata":{"_uuid":"2c61da427a21023b1d8e28911fd500634f31243a"},"cell_type":"markdown","source":"<a id=\"20\"></a> <br>\n4. AR MODEL\n\nThe p,d,q values can be specified using the order argument of ARIMA which take a tuple (p,d,q). Let model the 3 cases:"},{"metadata":{"trusted":true,"_uuid":"25ae4ca27ceec84762f75a34c8656374e07abe74"},"cell_type":"code","source":"from statsmodels.tsa.arima_model import ARIMA\nmodel = ARIMA(ts_log, order=(2, 1, 0))  \nresults_AR = model.fit(disp=-1)  \nfig,axes = plt.subplots(1,1,figsize=(20,10))\nplt.plot(ts_log_diff)\nplt.plot(results_AR.fittedvalues, color='red')\nplt.title('RSS: %.4f'% sum((results_AR.fittedvalues-ts_log_diff)**2))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c89d9a94ad5c1542e090f38c704454403f7a94fe"},"cell_type":"markdown","source":"<a id=\"21\"></a> <br>\n5. MA MODEL\n"},{"metadata":{"trusted":true,"_uuid":"7529a23173dc7df45ceb303e26e3c6d07d35cef9"},"cell_type":"code","source":"model = ARIMA(ts_log, order=(0, 1, 2))  \nresults_MA = model.fit(disp=-1)  \nfig,axes = plt.subplots(1,1,figsize=(20,10))\nplt.plot(ts_log_diff)\nplt.plot(results_MA.fittedvalues, color='red')\nplt.title('RSS: %.4f'% sum((results_MA.fittedvalues-ts_log_diff)**2))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"19252d72a07c40f8b5b78e848d1fe3fa32627f65"},"cell_type":"markdown","source":"<a id=\"22\"></a> <br>\n6. COMBINED MODEL"},{"metadata":{"trusted":true,"_uuid":"53593b847f41d864b7ebc156983e85ea5ddebc03"},"cell_type":"code","source":"model = ARIMA(ts_log, order=(2, 1, 2))  \nresults_ARIMA = model.fit(disp=-1)  \nfig,axes = plt.subplots(1,1,figsize=(20,10))\nplt.plot(ts_log_diff)\nplt.plot(results_ARIMA.fittedvalues, color='red')\nplt.title('RSS: %.4f'% sum((results_ARIMA.fittedvalues-ts_log_diff)**2))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"84a11e950e0b3d1e837c43510a9c6d7113107d93"},"cell_type":"markdown","source":"Here we can see that the AR and MA models have almost the same RSS but combined is significantly better. Now, we are left with 1 last step, i.e. taking these values back to the original scale."},{"metadata":{"_uuid":"70cc89161b8953d382bee17e380f00f77471c7fd"},"cell_type":"markdown","source":"<a id=\"23\"></a> <br>\n**J. TAKING IT BACK TO ORIGINAL SCALE**\n\nSince the combined model gave best result, lets scale it back to the original values and see how well it performs there. First step would be to store the predicted results as a separate series and observe it.\n"},{"metadata":{"trusted":true,"_uuid":"efbbb90a61e3d064f7ceb358ab950bd16c2dd990"},"cell_type":"code","source":"predictions_ARIMA_diff = pd.Series(results_ARIMA.fittedvalues, copy=True)\nprint(predictions_ARIMA_diff.head())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"35b13a8b509480105503a8b73d1c0f295acbf535"},"cell_type":"markdown","source":"Notice that these start from ‘2007-02-02’ and not the first month. Why? This is because we took a lag by 1 and first element doesn’t have anything before it to subtract from. The way to convert the differencing to log scale is to add these differences consecutively to the base number. An easy way to do it is to first determine the cumulative sum at index and then add it to the base number. The cumulative sum can be found as:"},{"metadata":{"trusted":true,"_uuid":"aee15b8640e1e35783cb349cf54b4c6655a18080"},"cell_type":"code","source":"predictions_ARIMA_diff_cumsum = predictions_ARIMA_diff.cumsum()\nprint(predictions_ARIMA_diff_cumsum.head())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7df86347c08a9e0962f34538a6ba99fac95b2dfd"},"cell_type":"markdown","source":"You can quickly do some back of mind calculations using previous output to check if these are correct. Next we’ve to add them to base number. For this lets create a series with all values as base number and add the differences to it. This can be done as:"},{"metadata":{"trusted":true,"_uuid":"9f9db060577f2b5e88534f03a74522e5746c972d"},"cell_type":"code","source":"predictions_ARIMA_log = pd.Series(ts_log.ix[0], index=ts_log.index)\npredictions_ARIMA_log = predictions_ARIMA_log.add(predictions_ARIMA_diff_cumsum,fill_value=0)\npredictions_ARIMA_log.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1ae6107951dec0221b8ed6472570b1d2c9dd3eaa"},"cell_type":"markdown","source":"Now, Lets do a comparison beween our estimation and main data."},{"metadata":{"trusted":true,"_uuid":"07bfebe2dbf536c5f128abe315fb452011a0197f"},"cell_type":"code","source":"pd.concat([ts_log.head(100), predictions_ARIMA_log.head(100)],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c4f25bc9a576fe09a3e2e69a155e878312fea21d"},"cell_type":"markdown","source":"Here the first element is base number itself and from there on the values cumulatively added. Last step is to take the exponent and compare with the original series."},{"metadata":{"trusted":true,"_uuid":"9f9ca23ea83fa775a16d494f1fcd15ef69d571d0"},"cell_type":"code","source":"fig,axes = plt.subplots(1,1,figsize=(15,10))\npredictions_ARIMA = np.exp(predictions_ARIMA_log)\nplt.plot(ts_log)\nplt.plot(predictions_ARIMA)\nplt.title('RMSE: %.4f'% np.sqrt(sum((predictions_ARIMA-ts_log)**2)/len(ts_log)))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"25a23a6a79296bbce2dd8f003036063dc9678eb9"},"cell_type":"markdown","source":"Finally we have a forecast at the original scale. Not a very good forecast I would say but you got the idea right? Now, I leave it upto you to refine the methodology further and make a better solution.\n"},{"metadata":{"_uuid":"a39186f2bc75f3a11710c185d6c976f912f71d1e"},"cell_type":"markdown","source":""},{"metadata":{"_uuid":"b173b72dc194197746f405e18c797220679af534"},"cell_type":"markdown","source":"\n**In progress ...**\n\n**Be in touch to get last commits ...**\n\n**I'll try to complete it as soon as possible**\n"},{"metadata":{"_uuid":"5d1452abb6b65808ecd9d0f8456e99645410a033"},"cell_type":"markdown","source":"<a id=\"100\"></a> <br>\n#  3-References\n\n[1. A comprehensive beginner’s guide to create a Time Series Forecast (with Codes in Python).](https://www.analyticsvidhya.com/blog/2016/02/time-series-forecasting-codes-python/)\n\n[2. My Kernel on Kaggle Feature Engineering in TwoSigma Competition.](https://www.kaggle.com/smasar/eda-preprocessing-processing-evaluation)\n\n3. Personal Experieces in Similar Projects.\n\n"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}