{"cells":[{"metadata":{"_uuid":"726661972b09b03a31d424ef02a9be0cd284d81b"},"cell_type":"markdown","source":" # <div style=\"text-align: center\">Linear Algebra for Data Scientists \n<div style=\"text-align: center\">One of the most common questions we get on <b>Data science</b> is:\n<br>\nHow much maths do I need to learn to be a <b>data scientist</b>?\n<br>\nIf you get confused and ask experts what should you learn at this stage, most of them would suggest / agree that you go ahead with Linear Algebra!\nin this simple tutorials you can learn all of the thing you need for being a data scientist with <b>Linear Algabra</b></div>\n<img src='http://s9.picofile.com/file/8341515150/LinearAlgabra.png'>\n<div style=\"text-align:center\">last update: <b>11/17/2018</b></div>\n\n\n\n\n\nYou can Fork code  and  Follow me on:\n\n> ###### [ GitHub](https://github.com/mjbahmani/10-steps-to-become-a-data-scientist)\n> ###### [Kaggle](https://www.kaggle.com/mjbahmani/)\n-------------------------------------------------------------------------------------------------------------\n <b>I hope you find this kernel helpful and some <font color='red'>UPVOTES</font> would be very much appreciated.<b/>\n    \n -----------"},{"metadata":{"_uuid":"2a01be35950f7a117fc6700e866de3bf5a3ea6b9"},"cell_type":"markdown","source":" <a id=\"top\"></a> <br>\n## Notebook  Content\n1. [Introduction](#1)\n1. [Basic Concepts and Notation](#2)\n1. [Notation ](#2)\n1. [Matrix Multiplication](#3)\n    1. [Vector-Vector Products](#4)\n    1. [Outer Product of Two Vectors](#5)\n    1. [Matrix-Vector Products](#6)\n    1. [Matrix-Matrix Products](#7)\n1. [Identity Matrix](#8)\n1. [Diagonal Matrix](#9)\n1. [Transpose of a Matrix](#10)\n1. [Symmetric Metrices](#11)\n1. [The Trace](#12)\n1. [Norms](#13)\n1. [Linear Independence and Rank](#14)\n    1. [Column Rank of a Matrix](#15)\n    1. [Row Rank of a Matrix](#16)\n    1. [Rank of a Matrix](#17)\n1. [Subtraction and Addition of Metrices](#18)\n    1. [Inverse](#19)\n1. [Orthogonal Matrices](#20)\n1. [Range and Nullspace of a Matrix](#21)\n1. [Determinant](#22)\n    1. [geometric interpretation of the determinant](#23)\n1. [Tensors](#24)\n1. [Hyperplane](#25)\n1. [Summary](#26)\n    1. [Dot Product](#27)\n    1. [Hadamard Product](#28)\n    1. [Outer Product](#29)\n1. [Eigenvalues and Eigenvectors](#30)\n1. [Exercise](#31)\n1. [Conclusion](#32)\n1. [References](#33)"},{"metadata":{"_uuid":"b18443661b6d30ffea2150fa74d44d62e14ae952"},"cell_type":"markdown","source":"<a id=\"1\"></a> <br>\n#  1-Introduction\n**Linear algebra** is the branch of mathematics that deals with **vector spaces**. good understanding of Linear Algebra is intrinsic to analyze Machine Learning algorithms, especially for **Deep Learning** where so much happens behind the curtain.you have my word that I will try to keep mathematical formulas & derivations out of this completely mathematical topic and I try to cover all of subject that you need as data scientist.\n"},{"metadata":{"_uuid":"aa205b8af27183f39ad0e5c9364e3560da512df3"},"cell_type":"markdown","source":"*Is there anything more useless or less useful than Algebra?*\n\n**Billy Connolly**"},{"metadata":{"_uuid":"9008e99d1ebea16694d75bfa1ba5addef515198e"},"cell_type":"markdown","source":"## 1-1 Import"},{"metadata":{"trusted":true,"_uuid":"223d7c576e665b2bbb83894e4f24346738e95877"},"cell_type":"code","source":"import matplotlib.patches as patch\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm\nfrom scipy import linalg\nfrom sklearn import svm\nimport tensorflow as tf\nimport pandas as pd\nimport numpy as np\nimport glob\nimport sys\nimport os","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"286ce03c993f8784863f6ad59298c869f8a544b0"},"cell_type":"markdown","source":"##  1-2 Setup"},{"metadata":{"trusted":true,"_uuid":"480928dbf26d5ef6ac7a1ddfe59b51a5eb95338a"},"cell_type":"code","source":"%matplotlib inline\n%precision 4\nplt.style.use('ggplot')\nnp.set_printoptions(suppress=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e6824a84cbdfb6dc17200c495101e113967bf514"},"cell_type":"markdown","source":"<a id=\"1\"></a> <br>\n# 2- Basic Concepts and Notation"},{"metadata":{"_uuid":"d46c78c9ba7287f5af049a777047621cca585e9b"},"cell_type":"markdown","source":"The following system of equations:"},{"metadata":{"_uuid":"92450d2d0c14c50b12faff1fba1a24d47f73c6fa"},"cell_type":"markdown","source":"$\\begin{equation}\n\\begin{split}\n4 x_1 - 5 x_2 & = -13 \\\\\n -2x_1 + 3 x_2 & = 9\n\\end{split}\n\\end{equation}$"},{"metadata":{"_uuid":"e7b0348e56afdf9ecaff27164052f9ad8157a355"},"cell_type":"markdown","source":"We are looking for a unique solution for the two variables $x_1$ and $x_2$.  The system can be described as:"},{"metadata":{"_uuid":"cd257c92064d3a32a783f478de4ecc6ee02eb855"},"cell_type":"markdown","source":"\\begin{align}\n\\dot{x} & = \\sigma(y-x) \\\\\n\\dot{y} & = \\rho x - y - xz \\\\\n\\dot{z} & = -\\beta z + xy\n\\end{align}"},{"metadata":{"_uuid":"6196aa1b102f2bc5baa03ab11a6f46a6334afb77"},"cell_type":"markdown","source":"$$\nAx=b\n$$"},{"metadata":{"_uuid":"62e00b4cd01db4db4eeed802bc6f873e56d44401"},"cell_type":"markdown","source":"as matrices:"},{"metadata":{"_uuid":"4d66f085637e77ab9d00fad7070d04902e06a405"},"cell_type":"markdown","source":"$$A = \\begin{bmatrix}\n       4  & -5 \\\\[0.3em]\n       -2 &  3 \n     \\end{bmatrix},\\ \n b = \\begin{bmatrix}\n       -13 \\\\[0.3em]\n       9 \n     \\end{bmatrix}$$"},{"metadata":{"_uuid":"c29e9ef072d3fe0241c29d3f1ce528acf428d50d"},"cell_type":"markdown","source":"A **scalar** is an element in a vector, containing a real number **value**. In a vector space model or a vector mapping of (symbolic, qualitative, or quantitative) properties the scalar holds the concrete value or property of a variable."},{"metadata":{"_uuid":"113ed77c0072b401987b15bf29b020b3b47f49ba"},"cell_type":"markdown","source":"A **vector** is an array, tuple, or ordered list of scalars (or elements) of size $n$, with $n$ a positive integer. The **length** of the vector, that is the number of scalars in the vector, is also called the **order** of the vector.\n<img src='https://cnx.org/resources/ba7a89a854e2336c540409615dbf47aa44155c56/pic002.png' height=400 width=400>"},{"metadata":{"trusted":true,"_uuid":"9d1e3eceee8943fb0b6086abfc68ae6634a6cac3"},"cell_type":"code","source":"#3-dimensional vector in numpy\na = np.zeros((2, 3, 4))\n#l = [[[ 0.,  0.,  0.,  0.],\n    #      [ 0.,  0.,  0.,  0.],\n     #     [ 0.,  0.,  0.,  0.]],\n     #     [[ 0.,  0.,  0.,  0.],\n    #      [ 0.,  0.,  0.,  0.],\n     #     [ 0.,  0.,  0.,  0.]]]\na","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"49c0b3e90c95512ef3733b25fd87cdef8ce31f97"},"cell_type":"markdown","source":"**Vectorization** is the process of creating a vector from some data using some process."},{"metadata":{"_uuid":"dbea06c756c0c9e398def8799d080e23b3e5f899"},"cell_type":"markdown","source":"Vectors of the length $n$ could be treated like points in $n$-dimensional space. One can calculate the distance between such points using measures like [Euclidean Distance](https://en.wikipedia.org/wiki/Euclidean_distance). The similarity of vectors could also be calculated using [Cosine Similarity](https://en.wikipedia.org/wiki/Cosine_similarity).\n###### [Go to top](#top)"},{"metadata":{"_uuid":"edaec8965119aa83192198d2d440c37546335719"},"cell_type":"markdown","source":"<a id=\"2\"></a> <br>\n## 3- Notation"},{"metadata":{"_uuid":"64af248dc35d9897a3f5bdd032850a57e4ff3876"},"cell_type":"markdown","source":"A **matrix** is a list of vectors that all are of the same length. $A$ is a matrix with $m$ rows and $n$ columns, antries of $A$ are real numbers:"},{"metadata":{"_uuid":"9bad820eed2da96e788ffcafbcf2479caee67643"},"cell_type":"markdown","source":"$A \\in \\mathbb{R}^{m \\times n}$"},{"metadata":{"_uuid":"b55c3fe6c04903ecfeeea63b7336123352acf529"},"cell_type":"markdown","source":"A vector $x$ with $n$ entries of real numbers, could also be thought of as a matrix with $n$ rows and $1$ column, or as known as a **column vector**."},{"metadata":{"_uuid":"c0b067c3a32a5104136293c0d1887b2edeca12a7"},"cell_type":"markdown","source":"$x = \\begin{bmatrix}\n       x_1 \\\\[0.3em]\n       x_2 \\\\[0.3em]\n       \\vdots \\\\[0.3em]\n       x_n\n     \\end{bmatrix}$"},{"metadata":{"_uuid":"ea3788fd5e066c884394be142580a4fdadac01fe"},"cell_type":"markdown","source":"Representing a **row vector**, that is a matrix with $1$ row and $n$ columns, we write $x^T$ (this denotes the transpose of $x$, see above)."},{"metadata":{"_uuid":"084a41970af4598e3fd2d3f6217a8695b160c9ac"},"cell_type":"markdown","source":"$x^T = \\begin{bmatrix}\n       x_1 & x_2 & \\cdots & x_n\n     \\end{bmatrix}$"},{"metadata":{"_uuid":"0a4ea55a9af6d93de973a5e87fb5c82ce1fb7206"},"cell_type":"markdown","source":"We use the notation $a_{ij}$ (or $A_{ij}$, $A_{i,j}$, etc.) to denote the entry of $A$ in the $i$th row and\n$j$th column:"},{"metadata":{"_uuid":"0503f34627c7269d302d2b3836069c8a04ab7dba"},"cell_type":"markdown","source":"$A = \\begin{bmatrix}\n       a_{11} & a_{12} & \\cdots & a_{1n} \\\\[0.3em]\n       a_{21} & a_{22} & \\cdots & a_{2n} \\\\[0.3em]\n       \\vdots & \\vdots & \\ddots & \\vdots \\\\[0.3em]\n       a_{m1} & a_{m2} & \\cdots & a_{mn} \n     \\end{bmatrix}$"},{"metadata":{"_uuid":"e8af099926ef22c12554e4a2d8820afd542ee807"},"cell_type":"markdown","source":"We denote the $j$th column of $A$ by $a_j$ or $A_{:,j}$:"},{"metadata":{"_uuid":"c17b201130a3ea73e59a50914599e7b9d1c1306d"},"cell_type":"markdown","source":"$A = \\begin{bmatrix}\n       \\big| & \\big| &  & \\big| \\\\[0.3em]\n       a_{1} & a_{2} & \\cdots & a_{n} \\\\[0.3em]\n       \\big| & \\big| &  & \\big|  \n     \\end{bmatrix}$"},{"metadata":{"_uuid":"7644f7d6386a60f8bf590d99a23d47accda09d8a"},"cell_type":"markdown","source":"We denote the $i$th row of $A$ by $a_i^T$ or $A_{i,:}$:"},{"metadata":{"_uuid":"fc9411370a353ff0e933eceadd9de277ca4b0113"},"cell_type":"markdown","source":"$A = \\begin{bmatrix}\n      -- & a_1^T  & -- \\\\[0.3em]\n       -- & a_2^T  & -- \\\\[0.3em]\n          & \\vdots &  \\\\[0.3em]\n       -- & a_m^T  & -- \n     \\end{bmatrix}$"},{"metadata":{"_uuid":"2301a92667f6fc9b26f76c216b5c8cb6e47b6343"},"cell_type":"markdown","source":""},{"metadata":{"_uuid":"c572408519a74d444556d2769694ebd9bf4d58da"},"cell_type":"markdown","source":"A $n \\times m$ matrix is a two-dimensional array with $n$ rows and $m$ columns.\n###### [Go to top](#top)"},{"metadata":{"_uuid":"41bc780d93b81da8aa1ff806c54a4791dbb2c8dc"},"cell_type":"markdown","source":"<a id=\"3\"></a> <br>\n## 4-Matrix Multiplication"},{"metadata":{"_uuid":"0573f52724da68f860328d1cc3259c215d817f80"},"cell_type":"markdown","source":"The result of the multiplication of two matrixes $A \\in \\mathbb{R}^{m \\times n}$ and $B \\in \\mathbb{R}^{n \\times p}$ is the matrix:"},{"metadata":{"trusted":true,"_uuid":"788f078e069a2ace3ca7d0aead749ead8b248c6d"},"cell_type":"code","source":"# initializing matrices \nx = np.array([[1, 2], [4, 5]]) \ny = np.array([[7, 8], [9, 10]])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bd6307c19afbda119a0dacca7e096b965889a30b"},"cell_type":"markdown","source":"$C = AB \\in \\mathbb{R}^{m \\times n}$"},{"metadata":{"_uuid":"0e0a76e86724698f241fcc74fc37e54d881622bd"},"cell_type":"markdown","source":"That is, we are multiplying the columns of $A$ with the rows of $B$:"},{"metadata":{"_uuid":"adb65bab1beb10117cbb490383cb62a9578ce62f"},"cell_type":"markdown","source":"$C_{ij}=\\sum_{k=1}^n{A_{ij}B_{kj}}$\n<img src='https://cdn.britannica.com/06/77706-004-31EE92F3.jpg'>"},{"metadata":{"_uuid":"17f45fb4f428ad87706493da0431bdce6c00b531"},"cell_type":"markdown","source":"The number of columns in $A$ must be equal to the number of rows in $B$.\n\n###### [Go to top](#top)"},{"metadata":{"trusted":true,"_uuid":"3cf4286adffc483893952a31d5a7006c462b3f60"},"cell_type":"code","source":"# using add() to add matrices \nprint (\"The element wise addition of matrix is : \") \nprint (np.add(x,y)) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"911a916a2c725232349a832769dd794956cb88cb"},"cell_type":"code","source":"# using subtract() to subtract matrices \nprint (\"The element wise subtraction of matrix is : \") \nprint (np.subtract(x,y)) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"045eba6b777e062510cfa4bf055c680830a66036"},"cell_type":"code","source":"# using divide() to divide matrices \nprint (\"The element wise division of matrix is : \") \nprint (np.divide(x,y)) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"aa8078d3057e8169d29ea9730d483ce9aebd5f2f"},"cell_type":"code","source":"# using multiply() to multiply matrices element wise \nprint (\"The element wise multiplication of matrix is : \") \nprint (np.multiply(x,y))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"aa352f5ba3a8ee911eb8f1c03749267eb4c4f76e"},"cell_type":"markdown","source":"<a id=\"4\"></a> <br>\n## 4-1 Vector-Vector Products"},{"metadata":{"_uuid":"4cf79a777e194d13bcc2dd7d164158f4390b5e8c"},"cell_type":"markdown","source":"#### Inner or Dot Product of Two Vectors"},{"metadata":{"_uuid":"3274f5ded5ea7255a4f1e24a3155a5a0af41f6c3"},"cell_type":"markdown","source":"For two vectors $x, y \\in \\mathbb{R}^n$, the **inner product** or **dot product** $x^T y$ is a real number:"},{"metadata":{"_uuid":"12af85ed7e264aebd0f5b07757bd30cebe4da1a8"},"cell_type":"markdown","source":"$x^T y \\in \\mathbb{R} = \\begin{bmatrix}\n       x_1 & x_2 & \\cdots & x_n\n     \\end{bmatrix} \\begin{bmatrix}\n       y_1 \\\\[0.3em]\n       y_2 \\\\[0.3em]\n       \\vdots \\\\[0.3em]\n       y_n\n     \\end{bmatrix} = \\sum_{i=1}^{n}{x_i y_i}$"},{"metadata":{"_uuid":"1b9b1e36e48691239719c0810b1097d4b3ffbd84"},"cell_type":"markdown","source":"The **inner products** are a special case of matrix multiplication."},{"metadata":{"_uuid":"530f02a04a9c565031e3a5f2ba01781265b59f7b"},"cell_type":"markdown","source":"It is always the case that $x^T y = y^T x$."},{"metadata":{"_uuid":"ab71687334bbaacf7f2b715b957d24a18e255e71"},"cell_type":"markdown","source":"##### Example"},{"metadata":{"_uuid":"1f11e295fe1154db1ad859ac91c55211e1f35b4c"},"cell_type":"markdown","source":"To calculate the inner product of two vectors $x = [1 2 3 4]$ and $y = [5 6 7 8]$, we can loop through the vector and multiply and sum the scalars (this is simplified code):"},{"metadata":{"_uuid":"371da89fa6d1b698c59ee82d6aa7b475fd7a5625","trusted":true},"cell_type":"code","source":"x = (1, 2, 3, 4)\ny = (5, 6, 7, 8)\nn = len(x)\nif n == len(y):\n    result = 0\n    for i in range(n):\n        result += x[i] * y[i]\n    print(result)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5dfcb47d8e3568eafc3593911ef8655762525093"},"cell_type":"markdown","source":"It is clear that in the code above we could change line 7 to `result += y[i] * x[i]` without affecting the result.\n###### [Go to top](#top)"},{"metadata":{"_uuid":"2bc1c325a7af9aa6d418474bbc59e5eb24c4652a"},"cell_type":"markdown","source":"We can use the *numpy* module to apply the same operation, to calculate the **inner product**. We import the *numpy* module and assign it a name *np* for the following code:"},{"metadata":{"_uuid":"05779f9ebb13affb22eb3f35bee252f04f7f596c","trusted":true},"cell_type":"code","source":"import numpy as np","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4d8ffa140774c6a7e2f0cd35c4809bd80069ce8b"},"cell_type":"markdown","source":"We define the vectors $x$ and $y$ using *numpy*:"},{"metadata":{"_uuid":"e6c39782297031e83d0e695fa80f9ebc2a817f4f","trusted":true},"cell_type":"code","source":"x = np.array([1, 2, 3, 4])\ny = np.array([5, 6, 7, 8])\nprint(\"x:\", x)\nprint(\"y:\", y)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ba7b567d2a4696cf0739e12cf4415ea3b8110e1e"},"cell_type":"markdown","source":"We can now calculate the $dot$ or $inner product$ using the *dot* function of *numpy*:"},{"metadata":{"_uuid":"c9fd9b61bdfa83059272f1ad61067138d0763308","trusted":true},"cell_type":"code","source":"np.dot(x, y)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fec79e95e9d5b4059f91fd69334569a6831b835b"},"cell_type":"markdown","source":"The order of the arguments is irrelevant:"},{"metadata":{"_uuid":"d3843ed486083fd994883be64136127728d09d7e","trusted":true},"cell_type":"code","source":"np.dot(y, x)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4025fa080136e50fcc4749100946148fa508ce32"},"cell_type":"markdown","source":"Note that both vectors are actually **row vectors** in the above code. We can transpose them to column vectors by using the *shape* property:"},{"metadata":{"_uuid":"f3a97f695aad46b1d848469240308024d1dcb634","trusted":true},"cell_type":"code","source":"print(\"x:\", x)\nx.shape = (4, 1)\nprint(\"xT:\", x)\nprint(\"y:\", y)\ny.shape = (4, 1)\nprint(\"yT:\", y)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6a850ef7d0e2025dd57cbf89c4bbe4146ed83ba4"},"cell_type":"markdown","source":"In fact, in our understanding of Linear Algebra, we take the arrays above to represent **row vectors**. *Numpy* treates them differently."},{"metadata":{"_uuid":"b64cdd134c53e7865a76d4efecc2ace176c664cd"},"cell_type":"markdown","source":"We see the issues when we try to transform the array objects. Usually, we can transform a row vector into a column vector in *numpy* by using the *T* method on vector or matrix objects:\n###### [Go to top](#top)"},{"metadata":{"_uuid":"bb73c80a401c89d79dbd920e5d63cf1a07b384e7","trusted":true},"cell_type":"code","source":"x = np.array([1, 2, 3, 4])\ny = np.array([5, 6, 7, 8])\nprint(\"x:\", x)\nprint(\"y:\", y)\nprint(\"xT:\", x.T)\nprint(\"yT:\", y.T)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e80fc9e4bec6d2cc26a8194e19736e1f24484d5f"},"cell_type":"markdown","source":"The problem here is that this does not do, what we expect it to do. It only works, if we declare the variables not to be arrays of numbers, but in fact a matrix:"},{"metadata":{"_uuid":"cb78424e23837608cbd597fb6fc7c3cbd99f368a","trusted":true},"cell_type":"code","source":"x = np.array([[1, 2, 3, 4]])\ny = np.array([[5, 6, 7, 8]])\nprint(\"x:\", x)\nprint(\"y:\", y)\nprint(\"xT:\", x.T)\nprint(\"yT:\", y.T)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9bed60bdfd536f42ad4a88a358e611fef07bc14a"},"cell_type":"markdown","source":"Note that the *numpy* functions *dot* and *outer* are not affected by this distinction. We can compute the dot product using the mathematical equation above in *numpy* using the new $x$ and $y$ row vectors:\n###### [Go to top](#top)"},{"metadata":{"_uuid":"df9c92e49f2dad24800996d0655caccec351c580","trusted":true},"cell_type":"code","source":"print(\"x:\", x)\nprint(\"y:\", y.T)\nnp.dot(x, y.T)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c1705fc2b1b16b9228274a2f53cd0fa59a39d8fd"},"cell_type":"markdown","source":"Or by reverting to:"},{"metadata":{"_uuid":"3a86b041668670f66b643053dfecfc46bdcd2749","trusted":true},"cell_type":"code","source":"print(\"x:\", x.T)\nprint(\"y:\", y)\nnp.dot(y, x.T)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"13ebfb31e281db7417cc3e2beb312e63f9688e28"},"cell_type":"markdown","source":"To read the result from this array of arrays, we would need to access the value this way:"},{"metadata":{"_uuid":"295be78d3c1258ec4f24579985c5f14f8746e8a9","trusted":true},"cell_type":"code","source":"np.dot(y, x.T)[0][0]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b5bda4ac75a8d11935ef765b2c869e56d9df8a56"},"cell_type":"markdown","source":"<a id=\"5\"></a> <br>\n## 4-2 Outer Product of Two Vectors"},{"metadata":{"_uuid":"600acbd91bfbb49c43541a743541fcdb43b00c1a"},"cell_type":"markdown","source":"For two vectors $x \\in \\mathbb{R}^m$ and $y \\in \\mathbb{R}^n$, where $n$ and $m$ do not have to be equal, the **outer product** of $x$ and $y$ is:"},{"metadata":{"_uuid":"8e8923ee3fd2fe565cbd45dd93eb69b99bcae973"},"cell_type":"markdown","source":"$xy^T \\in \\mathbb{R}^{m\\times n}$"},{"metadata":{"_uuid":"5d50297bd88d3a2c3c7738412e84611c119695e8"},"cell_type":"markdown","source":"The **outer product** results in a matrix with $m$ rows and $n$ columns by $(xy^T)_{ij} = x_i y_j$:"},{"metadata":{"_uuid":"24c79c28d1a44f968c54fb7f6b25f4515e48ffa2"},"cell_type":"markdown","source":"$xy^T \\in \\mathbb{R}^{m\\times n} = \\begin{bmatrix}\n       x_1 \\\\[0.3em]\n       x_2 \\\\[0.3em]\n       \\vdots \\\\[0.3em]\n       x_n\n     \\end{bmatrix} \\begin{bmatrix}\n       y_1 & y_2 & \\cdots & y_n\n     \\end{bmatrix} = \\begin{bmatrix}\n       x_1 y_1 & x_1 y_2 & \\cdots & x_1 y_n \\\\[0.3em]\n       x_2 y_1 & x_2 y_2 & \\cdots & x_2 y_n \\\\[0.3em]\n       \\vdots  & \\vdots  & \\ddots & \\vdots \\\\[0.3em]\n       x_m y_1 & x_m y_2 & \\cdots & x_m y_n \\\\[0.3em]\n     \\end{bmatrix}$"},{"metadata":{"_uuid":"cfb0c1807bfd9dee7f997d2375d065c4ecbcc9d5"},"cell_type":"markdown","source":"Some useful property of the outer product: assume $\\mathbf{1} \\in \\mathbb{R}^n$ is an $n$-dimensional vector of scalars with the value $1$. Given a matrix $A \\in \\mathbb{R}^{m\\times n}$ with all columns equal to some vector $x \\in \\mathbb{R}^m$, using the outer product $A$ can be represented as:"},{"metadata":{"_uuid":"223cb0c14b513f60b8a31d5c81fab450afc50902"},"cell_type":"markdown","source":"$A = \\begin{bmatrix}\n       \\big| & \\big| &  & \\big| \\\\[0.3em]\n       x & x & \\cdots & x \\\\[0.3em]\n       \\big| & \\big| &  & \\big|  \n     \\end{bmatrix} = \\begin{bmatrix}\n       x_1    & x_1    & \\cdots & x_1    \\\\[0.3em]\n       x_2    & x_2    & \\cdots & x_2    \\\\[0.3em]\n       \\vdots & \\vdots & \\ddots & \\vdots \\\\[0.3em]\n       x_m    &x_m     & \\cdots & x_m\n     \\end{bmatrix} = \\begin{bmatrix}\n       x_1 \\\\[0.3em]\n       x_2 \\\\[0.3em]\n       \\vdots \\\\[0.3em]\n       x_m\n     \\end{bmatrix} \\begin{bmatrix}\n       1 & 1 & \\cdots & 1\n     \\end{bmatrix} = x \\mathbf{1}^T$"},{"metadata":{"_uuid":"559fa51035235c9d1c1a2286d05610d5e51dc958"},"cell_type":"markdown","source":"##### Example"},{"metadata":{"_uuid":"79454aca18aaa191658d04a48662f75588dd6e4e"},"cell_type":"markdown","source":"If we want to compute the outer product of two vectors $x$ and $y$, we need to transpose the row vector $x$ to a column vector $x^T$. This can be achieved by the *reshape* function in *numpy*, the *T* method, or the *transpose()* function. The *reshape* function takes a parameter that describes the number of colums and rows for the resulting transposing:\n###### [Go to top](#top)"},{"metadata":{"_uuid":"31c5791cb210071f5253d0a20a7f1e2c030a48ea","trusted":true},"cell_type":"code","source":"x = np.array([[1, 2, 3, 4]])\nprint(\"x:\", x)\nprint(\"xT:\", np.reshape(x, (4, 1)))\nprint(\"xT:\", x.T)\nprint(\"xT:\", x.transpose())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0f8844018b2de3e4cd9584350a223b1e1347efe9"},"cell_type":"markdown","source":"We can now compute the **outer product** by multiplying the column vector $x$ with the row vector $y$:"},{"metadata":{"_uuid":"4744a491b80ce1e01ddc4590847c9660ea9ae14b","trusted":true},"cell_type":"code","source":"x = np.array([[1, 2, 3, 4]])\ny = np.array([[5, 6, 7, 8]])\nx.T * y","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7ee36e496adf686e1445faca7e5c9c1dee9abf35"},"cell_type":"markdown","source":"*Numpy* provides an *outer* function that does all that:"},{"metadata":{"_uuid":"47ce570eb9aa9a1173a2f30de728aba2aec3976c","trusted":true},"cell_type":"code","source":"np.outer(x, y)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b3eed151fe4f34604d1691e36a41d82b36d3fead"},"cell_type":"markdown","source":"Note, in this simple case using the simple arrays for the data structures of the vectors does not affect the result of the *outer* function:"},{"metadata":{"_uuid":"e52b29787ddf67293d7dbe6f0887cfc23fc4f11f","trusted":true},"cell_type":"code","source":"x = np.array([1, 2, 3, 4])\ny = np.array([5, 6, 7, 8])\nnp.outer(x, y)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0497e5d541a3c1ba344863a6730ea7521e65d50c"},"cell_type":"markdown","source":"<a id=\"6\"></a> <br>\n## 4-3 Matrix-Vector Products"},{"metadata":{"_uuid":"04b13cebdd26ed43bc8a0cceb68a7108d70a136a"},"cell_type":"markdown","source":"Assume a matrix $A \\in \\mathbb{R}^{m\\times n}$ and a vector $x \\in \\mathbb{R}^n$ the product results in a vector $y = Ax \\in \\mathbb{R}^m$."},{"metadata":{"_uuid":"5fd17293bb4d5d64da6b241166804fc7e04fe01f"},"cell_type":"markdown","source":"$Ax$ could be expressed as the dot product of row $i$ of matrix $A$ with the column value $j$ of vector $x$. Let us first consider matrix multiplication with a scalar:\n###### [Go to top](#top)"},{"metadata":{"_uuid":"16e79a2bad504080985d9934e8ca7715ca808ecf"},"cell_type":"markdown","source":"$A = \\begin{bmatrix}\n       1 & 2 \\\\[0.3em]\n       3 & 4\n     \\end{bmatrix}$"},{"metadata":{"_uuid":"d5ce445bbdd95aa2f30b88d50d40694c7138cd16"},"cell_type":"markdown","source":"We can compute the product of $A$ with a scalar $n = 2$ as:"},{"metadata":{"_uuid":"d4e35d8ca8587610ee6af9b9dbe5ffcb39b088e6"},"cell_type":"markdown","source":"$A = \\begin{bmatrix}\n       1 * n & 2 * n \\\\[0.3em]\n       3 * n & 4 * n\n     \\end{bmatrix} = \\begin{bmatrix}\n       1 * 2 & 2 * 2 \\\\[0.3em]\n       3 * 2 & 4 * 2\n     \\end{bmatrix} = \\begin{bmatrix}\n       2 & 4 \\\\[0.3em]\n       6 & 8\n     \\end{bmatrix} $"},{"metadata":{"_uuid":"cdf6b9087f0d650fc81e307e20ea19c80e65ca81"},"cell_type":"markdown","source":"Using *numpy* this can be achieved by:"},{"metadata":{"_uuid":"dab712cc9bfed1169b78e17b899fb51ab054323a","trusted":true},"cell_type":"code","source":"import numpy as np\nA = np.array([[4, 5, 6],\n             [7, 8, 9]])\nA * 2","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7c0f41deb55a4d276a163064a096a7265dfb6b70"},"cell_type":"markdown","source":"Assume that we have a column vector $x$:"},{"metadata":{"_uuid":"9b1ae2e8676afb189d397c7eda3ac2ff6ac7d68c"},"cell_type":"markdown","source":"$x = \\begin{bmatrix}\n       1 \\\\[0.3em]\n       2 \\\\[0.3em]\n       3 \n     \\end{bmatrix}$"},{"metadata":{"_uuid":"cc7395a031bfe1a5ef2c81742ce53402a4f4a760"},"cell_type":"markdown","source":"To be able to multiply this vector with a matrix, the number of columns in the matrix must correspond to the number of rows in the column vector. The matrix $A$ must have $3$ columns, as for example: "},{"metadata":{"_uuid":"3a64bb5508b56c69089f3f3a0ad660b7d3afcc6d"},"cell_type":"markdown","source":"$A = \\begin{bmatrix}\n       4 & 5 & 6\\\\[0.3em]\n       7 & 8 & 9\n     \\end{bmatrix}$"},{"metadata":{"_uuid":"263ba698745983ffaf1d3e8d70913655d706b7b5"},"cell_type":"markdown","source":"To compute $Ax$, we multiply row $1$ of the matrix with column $1$ of $x$:"},{"metadata":{"_uuid":"c2a029c124746ef05f143057fd2504aacbd0f420"},"cell_type":"markdown","source":"$\\begin{bmatrix}\n  4 & 5 & 6\n \\end{bmatrix}\n \\begin{bmatrix}\n 1 \\\\[0.3em]\n 2 \\\\[0.3em]\n 3 \n\\end{bmatrix} = 4 * 1 + 5 * 2 + 6 * 3 = 32 $"},{"metadata":{"_uuid":"a58cee7c07b4c95146d47bb4760cbad517f80d73"},"cell_type":"markdown","source":"We do the compute the dot product of row $2$ of $A$ and column $1$ of $x$:"},{"metadata":{"_uuid":"2b48c07ca48080c579e827df7b7b7851648d1c4e"},"cell_type":"markdown","source":"$\\begin{bmatrix}\n  7 & 8 & 9\n \\end{bmatrix}\n \\begin{bmatrix}\n 1 \\\\[0.3em]\n 2 \\\\[0.3em]\n 3 \n\\end{bmatrix} = 7 * 1 + 8 * 2 + 9 * 3 = 50 $"},{"metadata":{"_uuid":"9212e603fc8e945f1fd1d2a1155aeaad802c5008"},"cell_type":"markdown","source":"The resulting column vector $Ax$ is:"},{"metadata":{"_uuid":"91f948005340d308fa68953ce38f38e751d8711d"},"cell_type":"markdown","source":"$Ax = \\begin{bmatrix}\n       32 \\\\[0.3em]\n       50 \n     \\end{bmatrix}$"},{"metadata":{"_uuid":"2be29c242244536effade27bc0f8bbd03eddd537"},"cell_type":"markdown","source":"Using *numpy* we can compute $Ax$:"},{"metadata":{"_uuid":"00f0a534b7dae7a24ee27a74553e1f785f3714ef","trusted":true},"cell_type":"code","source":"A = np.array([[4, 5, 6],\n             [7, 8, 9]])\nx = np.array([1, 2, 3])\nA.dot(x)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2f6c7e7c90592f5d4d529798199ec7507a2752c3"},"cell_type":"markdown","source":"We can thus describe the product writing $A$ by rows as:"},{"metadata":{"_uuid":"bf34bef734155d6027fdcf8f16ae735a25ccaeef"},"cell_type":"markdown","source":"$y = Ax = \\begin{bmatrix}\n -- & a_1^T  & -- \\\\[0.3em]\n -- & a_2^T  & -- \\\\[0.3em]\n    & \\vdots &  \\\\[0.3em]\n -- & a_m^T  & -- \n\\end{bmatrix} x = \\begin{bmatrix}\n a_1^T x \\\\[0.3em]\n a_2^T x \\\\[0.3em]\n \\vdots \\\\[0.3em]\n a_m^T x \n\\end{bmatrix}$"},{"metadata":{"_uuid":"5662d8ac1faca01adf26451e5d07bf18dff21290"},"cell_type":"markdown","source":"This means that the $i$th scalar of $y$ is the inner product of the $i$th row of $A$ and $x$, that is $y_i = a_i^T x$."},{"metadata":{"_uuid":"3c2f566e74d022fcb5b8cebf8405d8f5029dd9e6"},"cell_type":"markdown","source":"If we write $A$ in column form, then:"},{"metadata":{"_uuid":"9140e355e1330614dc78af903d25155af62a3c87"},"cell_type":"markdown","source":"$y = Ax =\n\\begin{bmatrix}\n \\big| & \\big| &  & \\big| \\\\[0.3em]\n a_1 & a_2 & \\cdots & a_n \\\\[0.3em]\n \\big| & \\big| &  & \\big|  \n\\end{bmatrix}\n\\begin{bmatrix}\n x_1 \\\\[0.3em]\n x_2 \\\\[0.3em]\n \\vdots \\\\[0.3em]\n x_n\n\\end{bmatrix} =\n\\begin{bmatrix}\n a_1\n\\end{bmatrix} x_1 + \n\\begin{bmatrix}\n a_2\n\\end{bmatrix} x_2 + \\dots +\n\\begin{bmatrix}\n a_n\n\\end{bmatrix} x_n\n$"},{"metadata":{"_uuid":"6ff2956fe3dda8845ca163ddfcbf41197f00934c"},"cell_type":"markdown","source":"In this case $y$ is a **[linear combination](https://en.wikipedia.org/wiki/Linear_combination)** of the *columns* of $A$, the coefficients taken from $x$."},{"metadata":{"_uuid":"d771db142d2ea47073067f2edcae75351c2af9d4"},"cell_type":"markdown","source":"The above examples multiply be the right with a column vector. One can multiply on the left by a row vector as well, $y^T = x^T A$ for $A \\in \\mathbb{R}^{m\\times n}$, $x\\in \\mathbb{R}^m$, $y \\in \\mathbb{R}^n$. There are two ways to express $y^T$, with $A$ expressed by its columns, with $i$th scalar of $y^T$ corresponds to the inner product of $x$ and the $i$th column of $A$:"},{"metadata":{"_uuid":"084b84e5eda9db135fdec2e8118510b875bb84f9"},"cell_type":"markdown","source":"$y^T = x^T A = x^t \\begin{bmatrix}\n \\big| & \\big| &  & \\big| \\\\[0.3em]\n a_1 & a_2 & \\cdots & a_n \\\\[0.3em]\n \\big| & \\big| &  & \\big|  \n\\end{bmatrix} = \n\\begin{bmatrix}\n x^T a_1 & x^T a_2 & \\dots & x^T a_n   \n\\end{bmatrix}$"},{"metadata":{"_uuid":"7b07dc51ff2dc60fc85e6512ac9ac8171e4c15df"},"cell_type":"markdown","source":"One can express $A$ by rows, where $y^T$ is a linear combination of the rows of $A$ with the scalars from $x$."},{"metadata":{"_uuid":"00fef9e0d2286fd9e415e86c3117b14461f9dc95"},"cell_type":"markdown","source":"$\\begin{equation}\n\\begin{split}\ny^T & = x^T A \\\\\n    & = \\begin{bmatrix}\n x_1 & x_2 & \\dots & x_n   \n\\end{bmatrix}\n\\begin{bmatrix}\n -- & a_1^T  & -- \\\\[0.3em]\n -- & a_2^T  & -- \\\\[0.3em]\n    & \\vdots &  \\\\[0.3em]\n -- & a_m^T  & -- \n\\end{bmatrix} \\\\\n   & = x_1 \\begin{bmatrix}-- & a_1^T  & --\\end{bmatrix} + x_2 \\begin{bmatrix}-- & a_2^T  & --\\end{bmatrix} + \\dots + x_n \\begin{bmatrix}-- & a_n^T  & --\\end{bmatrix}\n\\end{split}\n\\end{equation}$\n\n###### [Go to top](#top)"},{"metadata":{"_uuid":"baee0e90e0271a893fa7344c7fbecb17849e24cb"},"cell_type":"markdown","source":"<a id=\"7\"></a> <br>\n## 4-4 Matrix-Matrix Products"},{"metadata":{"_uuid":"b9d6edf802b8d25e416d6bcbc05a89f232761f0c"},"cell_type":"markdown","source":"One can view matrix-matrix multiplication $C = AB$ as a set of vector-vector products. The $(i,j)$th entry of $C$ is the inner product of the $i$th row of $A$ and the $j$th column of $B$:"},{"metadata":{"trusted":true,"_uuid":"da4f41de85c7b81e7b12eb2aa5d96f36b9239795"},"cell_type":"code","source":"matrix1 = np.matrix(\n    [[0, 4],\n     [2, 0]]\n)\nmatrix2 = np.matrix(\n    [[-1, 2],\n     [1, -2]]\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8fff3d143c70a997cb601ce8440f3e98ba4be645"},"cell_type":"code","source":"matrix1 + matrix2","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ee80826d7e34cb283c5ff4ef165d05f7715fe14f"},"cell_type":"markdown","source":"### 4-4-1  Multiplication\nTo multiply two matrices with numpy, you can use the np.dot method:"},{"metadata":{"trusted":true,"_uuid":"413b954a7fce564c58d2bab2c0e48c8a268ca706"},"cell_type":"code","source":"np.dot(matrix1, matrix2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"71a943f0cf2b0a27001ed6e53a766f2626946587"},"cell_type":"code","source":"\nmatrix1 * matrix2","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4d23d2540ce269b381ed3a574afe757dd9ce0890"},"cell_type":"markdown","source":"$C = AB =\n\\begin{bmatrix}\n -- & a_1^T  & -- \\\\[0.3em]\n -- & a_2^T  & -- \\\\[0.3em]\n    & \\vdots &  \\\\[0.3em]\n -- & a_m^T  & -- \n\\end{bmatrix}\n\\begin{bmatrix}\n \\big| & \\big| &  & \\big| \\\\[0.3em]\n b_1 & b_2 & \\cdots & b_p \\\\[0.3em]\n \\big| & \\big| &  & \\big|  \n\\end{bmatrix} = \n\\begin{bmatrix}\n a_1^T b_1 & a_1^T b_2 & \\cdots & a_1^T b_p \\\\[0.3em]\n a_2^T b_1 & a_2^T b_2 & \\cdots & a_2^T b_p \\\\[0.3em]\n \\vdots    & \\vdots    & \\ddots & \\vdots    \\\\[0.3em]\n a_m^T b_1 & a_m^T b_2 & \\cdots & a_m^T b_p \n\\end{bmatrix}$"},{"metadata":{"_uuid":"92fccf9287e2092fb03bcee8f14cdc4b943493ef"},"cell_type":"markdown","source":"Here $A \\in \\mathbb{R}^{m\\times n}$ and $B \\in \\mathbb{R}^{n\\times p}$, $a_i \\in \\mathbb{R}^n$ and $b_j \\in \\mathbb{R}^n$, and $A$ is represented by rows, $B$ by columns."},{"metadata":{"_uuid":"9777cf6f6fda796af639d33530c834098dc86d8e"},"cell_type":"markdown","source":"If we represent $A$ by columns and $B$ by rows, then $AB$ is the sum of the outer products:"},{"metadata":{"_uuid":"6de77426379cc3e29abbe9323f14e95cf97286f1"},"cell_type":"markdown","source":"$C = AB =\n\\begin{bmatrix}\n \\big| & \\big| &  & \\big| \\\\[0.3em]\n a_1 & a_2 & \\cdots & a_n \\\\[0.3em]\n \\big| & \\big| &  & \\big|  \n\\end{bmatrix}\n\\begin{bmatrix}\n -- & b_1^T  & -- \\\\[0.3em]\n -- & b_2^T  & -- \\\\[0.3em]\n    & \\vdots &  \\\\[0.3em]\n -- & b_n^T  & -- \n\\end{bmatrix}\n= \\sum_{i=1}^n a_i b_i^T\n$"},{"metadata":{"_uuid":"44d6c5cb2e94ba42ba07a2c7318066cff447bd13"},"cell_type":"markdown","source":"This means that $AB$ is the sum over all $i$ of the outer product of the $i$th column of $A$ and the $i$th row of $B$."},{"metadata":{"_uuid":"bf95358be113e90861b32ddc10eb4d167970ea7c"},"cell_type":"markdown","source":"One can interpret matrix-matrix operations also as a set of matrix-vector products. Representing $B$ by columns, the columns of $C$ are matrix-vector products between $A$ and the columns of $B$:"},{"metadata":{"_uuid":"e4af158d553d4af80b571cb7792d3742d9a3933e"},"cell_type":"markdown","source":"$C = AB = A\n\\begin{bmatrix}\n \\big| & \\big| &  & \\big| \\\\[0.3em]\n b_1 & b_2 & \\cdots & b_p \\\\[0.3em]\n \\big| & \\big| &  & \\big|  \n\\end{bmatrix} = \n\\begin{bmatrix}\n \\big| & \\big| &  & \\big| \\\\[0.3em]\n A b_1 & A b_2 & \\cdots & A b_p \\\\[0.3em]\n \\big| & \\big| &  & \\big|  \n\\end{bmatrix}\n$"},{"metadata":{"_uuid":"db64a70c69c7498c0addd32920b1997ae346648f"},"cell_type":"markdown","source":"In this interpretation the $i$th column of $C$ is the matrix-vector product with the vector on the right, i.e. $c_i = A b_i$."},{"metadata":{"_uuid":"fa9aed70fa2cf022252d49b0e6cf61c2a3271938"},"cell_type":"markdown","source":"Representing $A$ by rows, the rows of $C$ are the matrix-vector products between the rows of $A$ and $B$:"},{"metadata":{"_uuid":"c031d45c3a799bfb462b54ee981cced64aeb6b46"},"cell_type":"markdown","source":"$C = AB = \\begin{bmatrix}\n -- & a_1^T  & -- \\\\[0.3em]\n -- & a_2^T  & -- \\\\[0.3em]\n    & \\vdots &  \\\\[0.3em]\n -- & a_m^T  & -- \n\\end{bmatrix}\nB = \n\\begin{bmatrix}\n -- & a_1^T B & -- \\\\[0.3em]\n -- & a_2^T B & -- \\\\[0.3em]\n    & \\vdots &  \\\\[0.3em]\n -- & a_n^T B & -- \n\\end{bmatrix}$"},{"metadata":{"_uuid":"db7e6699c9753b8bc35ddb205e39b6c0cfeaa7ba"},"cell_type":"markdown","source":"The $i$th row of $C$ is the matrix-vector product with the vector on the left, i.e. $c_i^T = a_i^T B$."},{"metadata":{"_uuid":"acb4c283712f1039fe818e9c76b2d06655df25ae"},"cell_type":"markdown","source":"#### Notes on Matrix-Matrix Products"},{"metadata":{"_uuid":"10b2cf32e0874d9d9ca346cf6e88051a74e670a0"},"cell_type":"markdown","source":"**Matrix multiplication is associative:** $(AB)C = A(BC)$"},{"metadata":{"_uuid":"336cb93754fabc5e3e870339560db9c13b7b5499"},"cell_type":"markdown","source":"**Matrix multiplication is distributive:** $A(B + C) = AB + AC$"},{"metadata":{"_uuid":"0f5a92225097ef125566df78d95c73699287388b"},"cell_type":"markdown","source":"**Matrix multiplication is, in general, not commutative;** It can be the case that $AB \\neq BA$. (For example, if $A \\in \\mathbb{R}^{m\\times n}$ and $B \\in \\mathbb{R}^{n\\times q}$, the matrix product $BA$ does not even exist if $m$ and $q$ are not equal!)\n###### [Go to top](#top)"},{"metadata":{"_uuid":"42bf81d28ea53bf258944612b436bf9a3a6b1292"},"cell_type":"markdown","source":"<a id=\"8\"></a> <br>\n## 5- Identity Matrix"},{"metadata":{"_uuid":"2cb80bc7e181a316499f1c420d6504714a887c98"},"cell_type":"markdown","source":"The **identity matrix** $I \\in \\mathbb{R}^{n\\times n}$ is a square matrix with the value $1$ on the diagonal and $0$ everywhere else:"},{"metadata":{"trusted":true,"_uuid":"842b12bf0ffff4ab4252db3134ca16eb44d2bc89"},"cell_type":"code","source":"np.eye(4)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fa7f57a6322de8459dfa3f46472a1f61acdcf31b"},"cell_type":"markdown","source":"$I_{ij} = \\left\\{\n\\begin{array}{lr}\n 1 & i = j\\\\\n 0 & i \\neq j\n\\end{array}\n\\right.\n$"},{"metadata":{"_uuid":"57cbbb9997318e430819b5e4d3accd0fd1f0a8d4"},"cell_type":"markdown","source":"For all $A \\in \\mathbb{R}^{m\\times n}$:"},{"metadata":{"_uuid":"556f89cba86ab17d19c89bce923fd09eea629b83"},"cell_type":"markdown","source":"$AI = A = IA$"},{"metadata":{"_uuid":"91306a27b6500debe6277cedd75558823466907c"},"cell_type":"markdown","source":"In the equation above multiplication has to be made possible, which means that in the portion $AI = A$ the dimensions of $I$ have to be $n\\times n$, while in $A = IA$ they have to be $m\\times m$."},{"metadata":{"_uuid":"be61bed3414ed2dccb551abbffb0a58ba270d38d"},"cell_type":"markdown","source":"We can generate an *identity matrix* in *numpy* using:"},{"metadata":{"_uuid":"29068a6e863dff19854170ea9ef701385d4ebda7","trusted":true},"cell_type":"code","source":"import numpy as np\nA = np.array([[0, 1, 2],\n              [3, 4, 5],\n              [6, 7, 8],\n              [9, 10, 11]])\nprint(\"A:\", A)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"caf0c2e4e4a417c3751daed3f3bf5e151562ab52"},"cell_type":"markdown","source":"We can ask for the shape of $A$:"},{"metadata":{"_uuid":"41d1470cef878a6ea9d6db819ca44bf5ebc7232e","trusted":true},"cell_type":"code","source":"A.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ffdd27ea7c4abf464d6ea49efbd921e68622a12f"},"cell_type":"markdown","source":"The *shape* property of a matrix contains the $m$ (number of rows) and $n$ (number of columns) properties in a tuple, in that particular order. We can create an identity matrix for the use in $AI$ by using the $n$ value: "},{"metadata":{"_uuid":"70d0df8d58e0a9209bcaa5753c51d3e8d905ce40","trusted":true},"cell_type":"code","source":"np.identity(A.shape[1], dtype=\"int\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8598fcdf9d480b28ccbfb9bde7c42297842bb590"},"cell_type":"markdown","source":"Note that we specify the *dtype* parameter to *identity* as *int*, since the default would return a matrix of *float* values."},{"metadata":{"_uuid":"6529004c2c919ec745dfe52f007ec4e90e39032c"},"cell_type":"markdown","source":"To generate an identity matrix for the use in $IA$ we would use the $m$ value:"},{"metadata":{"_uuid":"6ac179dd27c16233d91df6ef504de64e5fadb7c8","trusted":true},"cell_type":"code","source":"np.identity(A.shape[0], dtype=\"int\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bab65d4322a7f2f3f6ae4b596d67ce3643647e42"},"cell_type":"markdown","source":"We can compute the dot product of $A$ and its identity matrix $I$:"},{"metadata":{"_uuid":"bc705223af8b1d89e6c5ac665da1993145b61bb7","trusted":true},"cell_type":"code","source":"n = A.shape[1]\nI = np.array(np.identity(n, dtype=\"int\"))\nnp.dot(A, I)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"279c5e7af710a1ce530f2debc04c23001041745e"},"cell_type":"markdown","source":"The same is true for the other direction:"},{"metadata":{"_uuid":"7818e9032440e9dddcdea3839fc8ba2cbac81d90","trusted":true},"cell_type":"code","source":"m = A.shape[0]\nI = np.array(np.identity(m, dtype=\"int\"))\nnp.dot(I, A)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7f97a3211e646d8ffc467bcda3623da3b89b9202"},"cell_type":"markdown","source":"### 5-1  Inverse Matrices"},{"metadata":{"trusted":true,"_uuid":"2e0fdf7abf02064addfb5acf23b751dbf8e8fc1f"},"cell_type":"code","source":"inverse = np.linalg.inv(matrix1)\nprint(inverse)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"93c10865c2374f850dd040b8d545c226002dbb75"},"cell_type":"markdown","source":"<a id=\"9\"></a> <br>\n## 6- Diagonal Matrix"},{"metadata":{"_uuid":"ef72260c42e7c6e409b52f5b3c845e3c4a7fbe7d"},"cell_type":"markdown","source":"In the **diagonal matrix** non-diagonal elements are $0$, that is $D = diag(d_1, d_2, \\dots{}, d_n)$, with:"},{"metadata":{"_uuid":"b63515e277c941f4a6970d3a692a40cc23433d20"},"cell_type":"markdown","source":"$D_{ij} = \\left\\{\n\\begin{array}{lr}\n d_i & i = j\\\\\n 0 & i \\neq j\n\\end{array}\n\\right.\n$"},{"metadata":{"_uuid":"2a70473875d2876ab2ced96694e02da219bc6f8f"},"cell_type":"markdown","source":"The identity matrix is a special case of a diagonal matrix: $I = diag(1, 1, \\dots{}, 1)$."},{"metadata":{"_uuid":"10c74f8237e9f95bcc0e47cf5b2c0beba8b39b01"},"cell_type":"markdown","source":"In *numpy* we can create a *diagonal matrix* from any given matrix using the *diag* function:"},{"metadata":{"_uuid":"51b5323cf73f7e328f3c8c024fd634e33329235b","trusted":true},"cell_type":"code","source":"import numpy as np\nA = np.array([[0,   1,  2,  3],\n              [4,   5,  6,  7],\n              [8,   9, 10, 11],\n              [12, 13, 14, 15]])\nnp.diag(A)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0a1c89b112c3a23a7fe57eb358979e01776e55e5"},"cell_type":"markdown","source":"An optional parameter *k* to the *diag* function allows us to extract the diagonal above the main diagonal with a positive *k*, and below the main diagonal with a negative *k*:\n###### [Go to top](#top)"},{"metadata":{"_uuid":"213118a89acd75f4ca025d46b319037cd1bcbbf8","trusted":true},"cell_type":"code","source":"np.diag(A, k=1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"70526663a463f5cdb1214fcf5ea2f7f3fb9ce166","trusted":true},"cell_type":"code","source":"np.diag(A, k=-1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"87d1e66c2fdd87db8a4b6e7b2dfee28d66dfd3fa"},"cell_type":"markdown","source":"<a id=\"10\"></a> <br>\n## 7- Transpose of a Matrix"},{"metadata":{"_uuid":"c56e983d9d25ddb75355b21700f1bb8a117a3bf2"},"cell_type":"markdown","source":"**Transposing** a matrix is achieved by *flipping* the rows and columns. For a matrix $A \\in \\mathbb{R}^{m\\times n}$ the transpose $A^T \\in \\mathbb{R}^{n\\times m}$ is the $n\\times m$ matrix given by:"},{"metadata":{"_uuid":"4076483db1f8a050c8a4389e464daf3320b27353"},"cell_type":"markdown","source":"$(A^T)_{ij} = A_{ji}$"},{"metadata":{"_uuid":"8e052bc93821a8f2c38fd71079c0eb1cc4529d70"},"cell_type":"markdown","source":"Properties of transposes:"},{"metadata":{"_uuid":"cacdebb118384b89516d2531497c2c9a3ff062cd"},"cell_type":"markdown","source":"- $(A^T)^T = A$\n- $(AB)^T = B^T A^T$\n- $(A+B)^T = A^T + B^T$"},{"metadata":{"_uuid":"2862a79e4c2abaede94a473a74f5eee9c07be65d"},"cell_type":"markdown","source":"<a id=\"11\"></a> <br>\n## 8- Symmetric Matrices"},{"metadata":{"_uuid":"9ffb7cb76c38a6ea0544d7f6fc392aaf27e53db0"},"cell_type":"markdown","source":"Square metrices $A \\in \\mathbb{R}^{n\\times n}$ are **symmetric**, if $A = A^T$."},{"metadata":{"_uuid":"d9bd494a412e0aba403cd7704e750c571da15550"},"cell_type":"markdown","source":"$A$ is **anti-symmetric**, if $A = -A^T$."},{"metadata":{"_uuid":"84a715bb732f79ebab0ef8fbb0e1a9e3b1571bbb"},"cell_type":"markdown","source":"For any matrix $A \\in \\mathbb{R}^{n\\times n}$, the matrix $A + A^T$ is **symmetric**."},{"metadata":{"_uuid":"84f8f94866478a6d3488ac7c3e10bde4050282ec"},"cell_type":"markdown","source":"For any matrix $A \\in \\mathbb{R}^{n\\times n}$, the matrix $A - A^T$ is **anti-symmetric**."},{"metadata":{"_uuid":"63e98eda5728b4f2e92903de5ac143a63bff1988"},"cell_type":"markdown","source":"Thus, any square matrix $A \\in \\mathbb{R}^{n\\times n}$ can be represented as a sum of a symmetric matrix and an anti-symmetric matrix:"},{"metadata":{"_uuid":"0b576a9e0c31f5d6abfd7acd8e6bffe9a5b8fa46"},"cell_type":"markdown","source":"$A = \\frac{1}{2} (A + A^T) + \\frac{1}{2} (A - A^T)$"},{"metadata":{"_uuid":"a30d35c4e2f7dc510ab9bfdad0f95ddb331b0867"},"cell_type":"markdown","source":"The first matrix on the right, i.e. $\\frac{1}{2} (A + A^T)$ is symmetric. The second matrix $\\frac{1}{2} (A - A^T)$ is anti-symmetric."},{"metadata":{"_uuid":"ece2bb90cbe7266a0868c7aef3ebc69a2d4a87c6"},"cell_type":"markdown","source":"$\\mathbb{S}^n$ is the set of all symmetric matrices of size $n$."},{"metadata":{"_uuid":"65a01f8617d8963563fafe08a2eb7d727b747de1"},"cell_type":"markdown","source":"$A \\in \\mathbb{S}^n$ means that $A$ is symmetric and of the size $n\\times n$."},{"metadata":{"_uuid":"428183208acf9df58cd241a7cd0ede7e17baf3d1"},"cell_type":"markdown","source":"<a id=\"12\"></a> <br>\n## 9-The Trace"},{"metadata":{"_uuid":"65c8dc82a48027fe563390588cd7afded770124c"},"cell_type":"markdown","source":"The **trace** of a square matrix $A \\in \\mathbb{R}^{n\\times n}$ is $tr(A)$ (or $trA$) is the sum of the diagonal elements in the matrix:"},{"metadata":{"_uuid":"54a2ebb1d5b8770df521a93140944bade3e59f58"},"cell_type":"markdown","source":"$trA = \\sum_{i=1}^n A_{ii}$"},{"metadata":{"_uuid":"5fa504cae78b2e61af20508a743c59b73de808a4"},"cell_type":"markdown","source":"Properties of the **trace**:"},{"metadata":{"_uuid":"714ef0b38ed9ed3d965403e1a723e14755ae3578"},"cell_type":"markdown","source":"- For $A \\in \\mathbb{R}^{n\\times n}$, $\\mathrm{tr}A = \\mathrm{tr}A^T$\n- For $A,B \\in \\mathbb{R}^{n\\times n}$, $\\mathrm{tr}(A + B) = \\mathrm{tr}A + \\mathrm{tr}B$\n- For $A \\in \\mathbb{R}^{n\\times n}$, $t \\in \\mathbb{R}$, $\\mathrm{tr}(tA) = t \\mathrm{tr}A$\n- For $A,B$ such that $AB$ is square, $\\mathrm{tr}AB = \\mathrm{tr}BA$\n- For $A,B,C$ such that $ABC$ is square, $\\mathrm{tr}ABC = \\mathrm{tr}BCA = \\mathrm{tr}CAB$, and so on for the product of more matrices.\n\n###### [Go to top](#top)"},{"metadata":{"_uuid":"17f061ccf8620270700b566eea8e41b70f215960","trusted":true},"cell_type":"code","source":"a = np.arange(8).reshape((2,2,2))\nnp.trace(a)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"03ec16794b2e1b2c5204eb5da7e03769fac509bf"},"cell_type":"code","source":"print(np.trace(matrix1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e4c042172e851dc79b63674dba2751f3b742fff7"},"cell_type":"code","source":"det = np.linalg.det(matrix1)\nprint(det)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"94dd4ec936f746f76064c24599ba47f53fa4f9dc"},"cell_type":"code","source":"a = np.array([[1, 2], [3, 4]])\na","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"38279e3a50d446c93bdb7f4c40dc0b240bfb2ad8"},"cell_type":"code","source":"\na.transpose()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0b5536a2d51d252ff8ed7c972f407c1669fac6ab"},"cell_type":"markdown","source":"<a id=\"13\"></a> <br>\n# 10- Norms"},{"metadata":{"_uuid":"10c44bf3423731f3e86e0991445262aa370f84f9"},"cell_type":"markdown","source":"The **norm** of a vector $x$ is $\\| x\\|$, informally the length of a vector."},{"metadata":{"_uuid":"24a7de6bf2eca7bb43c39bb7c29b1e95a55be60d"},"cell_type":"markdown","source":"Example: the Euclidean or $\\mathscr{l}_2$ norm:"},{"metadata":{"_uuid":"5769f3e4578c931424802b0477d461186d38d7c9"},"cell_type":"markdown","source":"$\\|x\\|_2 = \\sqrt{\\sum_{i=1}^n{x_i^2}}$"},{"metadata":{"_uuid":"3a22383b644d3ecb03b2e1612e4a81e42d801198"},"cell_type":"markdown","source":"Note: $\\|x\\|_2^2 = x^T x$"},{"metadata":{"_uuid":"31936574afd9f877b78437741644f02360712dbf"},"cell_type":"markdown","source":"A **norm** is any function $f : \\mathbb{R}^n \\rightarrow \\mathbb{R}$ that satisfies the following properties:"},{"metadata":{"_uuid":"92ee01878823a54687233df4f81ad557c4b8d0f5"},"cell_type":"markdown","source":"- For all $x \\in \\mathbb{R}^n$, $f(x) \\geq 0$ (non-negativity)\n- $f(x) = 0$ if and only if $x = 0$ (definiteness)\n- For all $x \\in \\mathbb{R}^n$, $t \\in \\mathbb{R}$, $f(tx) = |t|\\ f(x)$ (homogeneity)\n- For all $x, y \\in \\mathbb{R}^n$, $f(x + y) \\leq f(x) + f(y)$ (triangle inequality)"},{"metadata":{"_uuid":"9d7f1f14c48ede7d070853d10d87f953c2e96363"},"cell_type":"markdown","source":"Norm $\\mathscr{l}_1$:"},{"metadata":{"_uuid":"d344bdc1023e51db2d40c70fc3517482ba8498c3"},"cell_type":"markdown","source":"$\\|x\\|_1 = \\sum_{i=1}^n{|x_i|}$"},{"metadata":{"_uuid":"b7f49b2eae1d711613d9c2376cbbc882432cf8cf"},"cell_type":"markdown","source":"How  calculate norm in python? **it is so easy**\n###### [Go to top](#top)"},{"metadata":{"trusted":true,"_uuid":"d8232fcb5a3b7ef9f9dab45d8d964046c584da11"},"cell_type":"code","source":"v = np.array([1,2,3,4])\nnorm.median(v)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"42d54d284146a24f5eeedffa8c53ed870359b08d"},"cell_type":"markdown","source":"<a id=\"14\"></a> <br>\n# 11- Linear Independence and Rank"},{"metadata":{"_uuid":"1b1e657d8254ffeb8d935ab3aa78d2818b6b2783"},"cell_type":"markdown","source":"A set of vectors $\\{x_1, x_2, \\dots{}, x_n\\} \\subset \\mathbb{R}^m$ is said to be **(linearly) independent** if no vector can be represented as a linear combination of the remaining vectors."},{"metadata":{"_uuid":"81303a8c87b6b3d06c670cb256a9cb8f5cd5d7d1"},"cell_type":"markdown","source":"A set of vectors $\\{x_1, x_2, \\dots{}, x_n\\} \\subset \\mathbb{R}^m$ is said to be **(lineraly) dependent** if one vector from this set can be represented as a linear combination of the remaining vectors."},{"metadata":{"_uuid":"b4c86e0d8f2a3e0577f826ef9c7c003d33fe8644"},"cell_type":"markdown","source":"For some scalar values $\\alpha_1, \\dots{}, \\alpha_{n-1} \\in \\mathbb{R}$ the vectors $x_1, \\dots{}, x_n$ are linerly dependent, if:"},{"metadata":{"_uuid":"2f069efddfa24291e1332122b0f90e1f1535c969"},"cell_type":"markdown","source":"$\\begin{equation}\nx_n = \\sum_{i=1}^{n-1}{\\alpha_i x_i}\n\\end{equation}$"},{"metadata":{"_uuid":"073368cbbc7baf69fed982710410ea8230cf7a39"},"cell_type":"markdown","source":"Example: The following vectors are lineraly dependent, because $x_3 = -2 x_1 + x_2$"},{"metadata":{"_uuid":"7b32ded623b0ec47af882f547c2b2747815ebea3"},"cell_type":"markdown","source":"$x_1 = \\begin{bmatrix}\n 1 \\\\[0.3em]\n 2 \\\\[0.3em]\n 3 \n\\end{bmatrix}\n\\quad\nx_2 = \\begin{bmatrix}\n 4 \\\\[0.3em]\n 1 \\\\[0.3em]\n 5 \n\\end{bmatrix}\n\\quad\nx_3 = \\begin{bmatrix}\n 2 \\\\[0.3em]\n -1 \\\\[0.3em]\n -1 \n\\end{bmatrix}\n$"},{"metadata":{"_uuid":"90c9bd9faf3ba0c6e8299f5c1e2495ab804a9105"},"cell_type":"markdown","source":"<a id=\"15\"></a> <br>\n## 11-1 Column Rank of a Matrix"},{"metadata":{"_uuid":"21660819816899f2b26e88d8d319bf0af58d3ef1"},"cell_type":"markdown","source":"The **column rank** of a matrix $A \\in \\mathbb{R}^{m\\times n}$ is the size of the largest subset of columns of $A$ that constitute a linear independent set. Informaly this is the number of linearly independent columns of $A$.\n###### [Go to top](#top)"},{"metadata":{"_uuid":"54eb416e6d3bf9e9aee63eb8b2dda5e935e65de9"},"cell_type":"markdown","source":"<a id=\"16\"></a> <br>\n## 11-2 Row Rank of a Matrix"},{"metadata":{"_uuid":"182d15eb4de63f174665159e1ba83fd181832a41"},"cell_type":"markdown","source":"The **row rank** of a matrix $A \\in \\mathbb{R}^{m\\times n}$ is the largest number of rows of $A$ that constitute a lineraly independent set."},{"metadata":{"_uuid":"134604c79595d4a945d8381fff3a999ba58b1f24"},"cell_type":"markdown","source":"<a id=\"17\"></a> <br>\n## 11-3 Rank of a Matrix"},{"metadata":{"_uuid":"6beff646fa29df47149aa8c55181a6a47ae6ee21"},"cell_type":"markdown","source":"For any matrix $A \\in \\mathbb{R}^{m\\times n}$, the column rank of $A$ is equal to the row rank of $A$. Both quantities are referred to collectively as the rank of $A$, denoted as $rank(A)$. Here are some basic properties of the rank:\n###### [Go to top](#top)"},{"metadata":{"_uuid":"e3ecf29883a4d9026e917936f80e9667981e0bdb"},"cell_type":"markdown","source":"- For $A \\in \\mathbb{R}^{m\\times n}$, $rank(A) \\leq \\min(m, n)$. If $rank(A) = \\min(m, n)$, then $A$ is said to be\n**full rank**.\n- For $A \\in \\mathbb{R}^{m\\times n}$, $rank(A) = rank(A^T)$\n- For $A \\in \\mathbb{R}^{m\\times n}$, $B \\in \\mathbb{R}^{n\\times p}$, $rank(AB) \\leq \\min(rank(A), rank(B))$\n- For $A,B \\in \\mathbb{R}^{m\\times n}$, $rank(A + B) \\leq rank(A) + rank(B)$"},{"metadata":{"_uuid":"e3cf40ea16ea61ebd53a5b56d1ecccf3ebfeba50"},"cell_type":"markdown","source":"<a id=\"18\"></a> <br>\n# 12-  Subtraction and Addition of Metrices"},{"metadata":{"_uuid":"a1849019d67f882bfafb7650be73b682b1a9927f"},"cell_type":"markdown","source":"Assume $A \\in \\mathbb{R}^{m\\times n}$ and $B \\in \\mathbb{R}^{m\\times n}$, that is $A$ and $B$ are of the same size, to add $A$ to $B$, or to subtract $B$ from $A$, we add or subtract corresponding entries:"},{"metadata":{"_uuid":"c85f1a6e4bfacef8def2a18b66bd2d23178ff9d6"},"cell_type":"markdown","source":"$A + B =\n\\begin{bmatrix}\n a_{11} & a_{12} & \\cdots & a_{1n} \\\\[0.3em]\n a_{21} & a_{22} & \\cdots & a_{2n} \\\\[0.3em]\n \\vdots & \\vdots & \\ddots & \\vdots \\\\[0.3em]\n a_{m1} & a_{m2} & \\cdots & a_{mn}\n\\end{bmatrix} +\n\\begin{bmatrix}\n b_{11} & b_{12} & \\cdots & b_{1n} \\\\[0.3em]\n b_{21} & b_{22} & \\cdots & b_{2n} \\\\[0.3em]\n \\vdots & \\vdots & \\ddots & \\vdots \\\\[0.3em]\n b_{m1} & b_{m2} & \\cdots & b_{mn}\n\\end{bmatrix} =\n\\begin{bmatrix}\n a_{11} + b_{11} & a_{12} + b_{12} & \\cdots & a_{1n} + b_{1n} \\\\[0.3em]\n a_{21} + b_{21} & a_{22} + b_{22} & \\cdots & a_{2n} + b_{2n} \\\\[0.3em]\n \\vdots & \\vdots & \\ddots & \\vdots \\\\[0.3em]\n a_{m1} + b_{m1} & a_{m2} + b_{m2} & \\cdots & a_{mn} + b_{mn}\n\\end{bmatrix}\n$"},{"metadata":{"_uuid":"ab774c9519a58113a305fb800ac761d7a2b8a7f2"},"cell_type":"markdown","source":"The same is applies to subtraction:"},{"metadata":{"_uuid":"fe301f7d29c3c1d88cb28ea80365146ce252b571"},"cell_type":"markdown","source":"$A - B =\n\\begin{bmatrix}\n a_{11} & a_{12} & \\cdots & a_{1n} \\\\[0.3em]\n a_{21} & a_{22} & \\cdots & a_{2n} \\\\[0.3em]\n \\vdots & \\vdots & \\ddots & \\vdots \\\\[0.3em]\n a_{m1} & a_{m2} & \\cdots & a_{mn}\n\\end{bmatrix} -\n\\begin{bmatrix}\n b_{11} & b_{12} & \\cdots & b_{1n} \\\\[0.3em]\n b_{21} & b_{22} & \\cdots & b_{2n} \\\\[0.3em]\n \\vdots & \\vdots & \\ddots & \\vdots \\\\[0.3em]\n b_{m1} & b_{m2} & \\cdots & b_{mn}\n\\end{bmatrix} =\n\\begin{bmatrix}\n a_{11} - b_{11} & a_{12} - b_{12} & \\cdots & a_{1n} - b_{1n} \\\\[0.3em]\n a_{21} - b_{21} & a_{22} - b_{22} & \\cdots & a_{2n} - b_{2n} \\\\[0.3em]\n \\vdots & \\vdots & \\ddots & \\vdots \\\\[0.3em]\n a_{m1} - b_{m1} & a_{m2} - b_{m2} & \\cdots & a_{mn} - b_{mn}\n\\end{bmatrix}\n$"},{"metadata":{"_uuid":"1203e8ce060702741dac72fa1ba8db01430d0e2a"},"cell_type":"markdown","source":"In Python using *numpy* this can be achieved using the following code:"},{"metadata":{"_uuid":"fa3526c6b6308ae79ab322ff12e6e21e45761e8a","trusted":true},"cell_type":"code","source":"import numpy as np\nprint(\"np.arange(9):\", np.arange(9))\nprint(\"np.arange(9, 18):\", np.arange(9, 18))\nA = np.arange(9, 18).reshape((3, 3))\nB = np.arange(9).reshape((3, 3))\nprint(\"A:\", A)\nprint(\"B:\", B)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0653012e78bb393232bd317d056e46ed6a96df64"},"cell_type":"markdown","source":"The *numpy* function *arange* is similar to the standard Python function *range*. It returns an array with $n$ elements, specified in the one parameter version only. If we provide to parameters to *arange*, it generates an array starting from the value of the first parameter and ending with a value one less than the second parameter. The function *reshape* returns us a matrix with the corresponding number of rows and columns."},{"metadata":{"_uuid":"2c2f223e42500cdc89887cf0c9c3a5bb2fd2497c"},"cell_type":"markdown","source":"We can now add and subtract the two matrices $A$ and $B$:"},{"metadata":{"_uuid":"3882778eea130a7cc3fd3e32d66177e6d5715223","trusted":true},"cell_type":"code","source":"A + B","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"00e586d7bdee0508f12ec92f4742994813ce0f79","trusted":true},"cell_type":"code","source":"A - B","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"94127d106aa8e05925e99e5f6c0a70f2c860af39"},"cell_type":"markdown","source":"<a id=\"19\"></a> <br>\n## 12-1 Inverse"},{"metadata":{"_uuid":"0f12955c3ddc3ddd076f4c64b30e50c952536bc9"},"cell_type":"markdown","source":"The **inverse** of a square matrix $A \\in \\mathbb{R}^{n\\times n}$ is $A^{-1}$:"},{"metadata":{"_uuid":"8dc73e363ae4a93812b6e17fe70ebc135d8ba6b4"},"cell_type":"markdown","source":"$A^{-1} A = I = A A^{-1}$"},{"metadata":{"_uuid":"18c16a6e9bb49a377274e69d40b514e956fd048f"},"cell_type":"markdown","source":"Not all matrices have inverses. Non-square matrices do not have inverses by definition. For some square matrices $A$ the inverse might not exist."},{"metadata":{"_uuid":"185dd34140399fd7da8b5e6fcc7eff1d95278cd5"},"cell_type":"markdown","source":"$A$ is **invertible** or **non-singular** if $A^{-1}$ exists."},{"metadata":{"_uuid":"a4d8f9676deb29e4782f0587649aaf517a03ac57"},"cell_type":"markdown","source":"$A$ is **non-invertible** or **singular** if $A^{-1}$ does not exist."},{"metadata":{"_uuid":"4c898f96bf3f31393cf4f99a6a36f0d9d1c2281f"},"cell_type":"markdown","source":"<font color='red'>Note: **non-singular** means the opposite of **non-invertible**!</font>"},{"metadata":{"_uuid":"6b73cfa800911706a244586d404f46151e779739"},"cell_type":"markdown","source":"For $A$ to have an inverse $A^{-1}$, $A$ must be **full rank**."},{"metadata":{"_uuid":"3ef6268b4924d7ea4fa8ba3d223b4088e935d7a1"},"cell_type":"markdown","source":"Assuming that $A,B \\in \\mathbb{R}^{n\\times n}$ are non-singular, then:"},{"metadata":{"_uuid":"156bdaac9c0ba13b5821690eab42cef6fecaa086"},"cell_type":"markdown","source":"- $(A^{-1})^{-1} = A$\n- $(AB)^{-1} = B^{-1} A^{-1}$\n- $(A^{-1})^T = (A^T)^{-1}$ (often simply $A^{-T}$)\n###### [Go to top](#top)"},{"metadata":{"_uuid":"6d67e21b6e3c3131310b7208bac880550e61ad03"},"cell_type":"markdown","source":"<a id=\"20\"></a> <br>\n## 13- Orthogonal Matrices"},{"metadata":{"_uuid":"b6fe01ce3f3ab34bc1af7161a4cc26888bafbccc"},"cell_type":"markdown","source":"Two vectors $x, y \\in \\mathbb{R}^n$ are **orthogonal** if $x^T y = 0$."},{"metadata":{"_uuid":"0da38648b240fb48345c46ebbd929d5b52de2649"},"cell_type":"markdown","source":"A vector $x \\in \\mathbb{R}^n$ is **normalized** if $\\|x\\|^2 = 1$."},{"metadata":{"_uuid":"285dac0071b9f07b4cf6eb6801ba66438eba1973"},"cell_type":"markdown","source":"A square matrix $U \\in \\mathbb{R}^{n\\times n}$ is **orthogonal** if all its columns are orthogonal to each other and are **normalized**. The columns are then referred to as being **orthonormal**."},{"metadata":{"_uuid":"1f48fa9b729d801ea41c99e9fc7c3836f003feb1"},"cell_type":"markdown","source":"It follows immediately from the definition of orthogonality and normality that:"},{"metadata":{"_uuid":"d038680211e0a4c8f3973ca6c363989092bcbc28"},"cell_type":"markdown","source":"$U^T U = I = U U^T$"},{"metadata":{"_uuid":"6d5d88f90103f834392fdc3737b153e24bc7e89f"},"cell_type":"markdown","source":"This means that the inverse of an orthogonal matrix is its transpose."},{"metadata":{"_uuid":"c6b4807c8fea87560848b8a7304e7f272e14a6cc"},"cell_type":"markdown","source":"If U is not square - i.e., $U \\in \\mathbb{R}^{m\\times n}$, $n < m$ - but its columns are still orthonormal, then $U^T U = I$, but $U U^T \\neq I$."},{"metadata":{"_uuid":"72ab7604c3c4f7375a5404a92b13a3ecc948db83"},"cell_type":"markdown","source":"We generally only use the term orthogonal to describe the case, where $U$ is square."},{"metadata":{"_uuid":"abba1ff00658101fff6b4c9c1a227f66f4025d13"},"cell_type":"markdown","source":"Another nice property of orthogonal matrices is that operating on a vector with an orthogonal matrix will not change its Euclidean norm. For any $x \\in \\mathbb{R}^n$, $U \\in \\mathbb{R}^{n\\times n}$ orthogonal."},{"metadata":{"_uuid":"de6caa369cdbd23a1284111c157ff678cb1253b7"},"cell_type":"markdown","source":"$\\|U_x\\|^2 = \\|x\\|^2$"},{"metadata":{"trusted":true,"_uuid":"fca71217a54595fb76f02e9044b3090ea75ffb80","_kg_hide-input":true},"cell_type":"code","source":"#How to create random orthonormal matrix in python numpy\ndef rvs(dim=3):\n     random_state = np.random\n     H = np.eye(dim)\n     D = np.ones((dim,))\n     for n in range(1, dim):\n         x = random_state.normal(size=(dim-n+1,))\n         D[n-1] = np.sign(x[0])\n         x[0] -= D[n-1]*np.sqrt((x*x).sum())\n         # Householder transformation\n         Hx = (np.eye(dim-n+1) - 2.*np.outer(x, x)/(x*x).sum())\n         mat = np.eye(dim)\n         mat[n-1:, n-1:] = Hx\n         H = np.dot(H, mat)\n         # Fix the last sign such that the determinant is 1\n     D[-1] = (-1)**(1-(dim % 2))*D.prod()\n     # Equivalent to np.dot(np.diag(D), H) but faster, apparently\n     H = (D*H.T).T\n     return H","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"84a7cc36920151c3166ad9036173a975077a816e"},"cell_type":"markdown","source":"<a id=\"21\"></a> <br>\n## 14- Range and Nullspace of a Matrix"},{"metadata":{"_uuid":"94a0da9d701ebbfd945f4f1d506685a84adf2bd1"},"cell_type":"markdown","source":"The **span** of a set of vectors $\\{ x_1, x_2, \\dots{}, x_n\\}$ is the set of all vectors that can be expressed as\na linear combination of $\\{ x_1, \\dots{}, x_n \\}$:"},{"metadata":{"_uuid":"383daf5739119f28efaeac95ffaadb43dd9ac053"},"cell_type":"markdown","source":"$\\mathrm{span}(\\{ x_1, \\dots{}, x_n \\}) = \\{ v : v = \\sum_{i=1}^n \\alpha_i x_i, \\alpha_i \\in \\mathbb{R} \\}$"},{"metadata":{"_uuid":"871ec420d48420bb427fb4de940bf0493161e0bf"},"cell_type":"markdown","source":"It can be shown that if $\\{ x_1, \\dots{}, x_n \\}$ is a set of n linearly independent vectors, where each $x_i \\in \\mathbb{R}^n$, then $\\mathrm{span}(\\{ x_1, \\dots{}, x_n\\}) = \\mathbb{R}^n$. That is, any vector $v \\in \\mathbb{R}^n$ can be written as a linear combination of $x_1$ through $x_n$."},{"metadata":{"_uuid":"90013aa1d12bb743454daa79b0ca39ec6de659bb"},"cell_type":"markdown","source":"The projection of a vector $y \\in \\mathbb{R}^m$ onto the span of $\\{ x_1, \\dots{}, x_n\\}$ (here we assume $x_i \\in \\mathbb{R}^m$) is the vector $v \\in \\mathrm{span}(\\{ x_1, \\dots{}, x_n \\})$, such that $v$ is as close as possible to $y$, as measured by the Euclidean norm $\\|v  y\\|^2$. We denote the projection as $\\mathrm{Proj}(y; \\{ x_1, \\dots{}, x_n \\})$ and can define it formally as:"},{"metadata":{"_uuid":"a5a99821ba988bb61e19cbea30d491010b018fa0"},"cell_type":"markdown","source":"$\\mathrm{Proj}( y; \\{ x_1, \\dots{}, x_n \\}) = \\mathrm{argmin}_{v\\in \\mathrm{span}(\\{x_1,\\dots{},x_n\\})}\\|y  v\\|^2$"},{"metadata":{"_uuid":"5a1f8588277edbd60ae162d5eb75d80477bb5536"},"cell_type":"markdown","source":"The **range** (sometimes also called the columnspace) of a matrix $A \\in \\mathbb{R}^{m\\times n}$, denoted $\\mathcal{R}(A)$, is the the span of the columns of $A$. In other words,"},{"metadata":{"_uuid":"2eed745f252be323178ea38e2a36c679b32e5a20"},"cell_type":"markdown","source":"$\\mathcal{R}(A) = \\{ v \\in \\mathbb{R}^m : v = A x, x \\in \\mathbb{R}^n\\}$"},{"metadata":{"_uuid":"24c6299fe37bf5c6cc4a6bf032c69a54b1225711"},"cell_type":"markdown","source":"Making a few technical assumptions (namely that $A$ is full rank and that $n < m$), the projection of a vector $y \\in \\mathbb{R}^m$ onto the range of $A$ is given by:"},{"metadata":{"_uuid":"317752e8f0825aa0467524c54811cb6a91a6a589"},"cell_type":"markdown","source":"$\\mathrm{Proj}(y; A) = \\mathrm{argmin}_{v\\in \\mathcal{R}(A)}\\|v  y\\|^2 = A(A^T A)^{1} A^T y$"},{"metadata":{"_uuid":"01cd617db6fe7e3ea6c26740ff2a740896c28d83"},"cell_type":"markdown","source":"<font color=\"red\">See for more details in the notes page 13.</font>"},{"metadata":{"_uuid":"1be3dda0bfa967fabd0e5b79783f79880292264b"},"cell_type":"markdown","source":"The **nullspace** of a matrix $A \\in \\mathbb{R}^{m\\times n}$, denoted $\\mathcal{N}(A)$ is the set of all vectors that equal $0$ when multiplied by $A$, i.e.,"},{"metadata":{"_uuid":"17e8400c92fc8bcfa2ae43b0b486daa39a99ac13"},"cell_type":"markdown","source":"$\\mathcal{N}(A) = \\{ x \\in \\mathbb{R}^n : A x = 0 \\}$"},{"metadata":{"_uuid":"bc996c4d7033175abf7327a96510d852956271e7"},"cell_type":"markdown","source":"Note that vectors in $\\mathcal{R}(A)$ are of size $m$, while vectors in the $\\mathcal{N}(A)$ are of size $n$, so vectors in $\\mathcal{R}(A^T)$ and $\\mathcal{N}(A)$ are both in $\\mathbb{R}^n$. In fact, we can say much more. It turns out that:"},{"metadata":{"_uuid":"b6d09d8a84f5d4f386ec4f66e02faa3fe4f8d430"},"cell_type":"markdown","source":"$\\{ w : w = u + v, u \\in \\mathcal{R}(A^T), v \\in \\mathcal{N}(A) \\} = \\mathbb{R}^n$ and $\\mathcal{R}(A^T) \\cap \\mathcal{N}(A) = \\{0\\}$"},{"metadata":{"_uuid":"d31bfe6323c97b5a80b7d8449599cc44f01b1ff2"},"cell_type":"markdown","source":"In other words, $\\mathcal{R}(A^T)$ and $\\mathcal{N}(A)$ are disjoint subsets that together span the entire space of\n$\\mathbb{R}^n$. Sets of this type are called **orthogonal complements**, and we denote this $\\mathcal{R}(A^T) = \\mathcal{N}(A)^\\perp$.\n\n###### [Go to top](#top)"},{"metadata":{"_uuid":"5168e46a20736d3815f9ac3590ab129b732bee12"},"cell_type":"markdown","source":"<a id=\"22\"></a> <br>\n# 15-  Determinant"},{"metadata":{"_uuid":"ab428148ca965168e967ead22e9a5d75d0753395"},"cell_type":"markdown","source":"The determinant of a square matrix $A \\in \\mathbb{R}^{n\\times n}$, is a function $\\mathrm{det} : \\mathbb{R}^{n\\times n} \\rightarrow \\mathbb{R}$, and is denoted $|A|$ or $\\mathrm{det}A$ (like the trace operator, we usually omit parentheses)."},{"metadata":{"_uuid":"8d506220cb4188a91add70aef133f099546c5b52"},"cell_type":"markdown","source":"<a id=\"23\"></a> <br>\n## 15-1 A geometric interpretation of the determinant"},{"metadata":{"_uuid":"9f283379fba7f2500ed5c872aefa9bca9b8e96e4"},"cell_type":"markdown","source":"Given"},{"metadata":{"_uuid":"c7b9813ca4bc5a0def37c4023f4411736f3a56f6"},"cell_type":"markdown","source":"$\\begin{bmatrix}\n -- & a_1^T  & -- \\\\[0.3em]\n -- & a_2^T  & -- \\\\[0.3em]\n    & \\vdots &  \\\\[0.3em]\n -- & a_n^T  & -- \n\\end{bmatrix}$"},{"metadata":{"_uuid":"eb5b19546fdb0385acecb393b738751993cc233e"},"cell_type":"markdown","source":"consider the set of points $S \\subset \\mathbb{R}^n$ formed by taking all possible linear combinations of the row vectors $a_1, \\dots{}, a_n \\in \\mathbb{R}^n$ of $A$, where the coefficients of the linear combination are all\nbetween $0$ and $1$; that is, the set $S$ is the restriction of $\\mathrm{span}( \\{ a_1, \\dots{}, a_n \\})$ to only those linear combinations whose coefficients $\\alpha_1, \\dots{}, \\alpha_n$ satisfy $0 \\leq \\alpha_i \\leq 1$, $i = 1, \\dots{}, n$. Formally:"},{"metadata":{"_uuid":"5472ba1f4030a4269cf126aa25e2380e0423df7e"},"cell_type":"markdown","source":"$S = \\{v \\in \\mathbb{R}^n : v = \\sum_{i=1}^n \\alpha_i a_i \\mbox{ where } 0 \\leq \\alpha_i \\leq 1, i = 1, \\dots{}, n \\}$"},{"metadata":{"_uuid":"c50a0db43e5b89a727bc197ae5ba8cbebb01d7f2"},"cell_type":"markdown","source":"The absolute value of the determinant of $A$, it turns out, is a measure of the *volume* of the set $S$. The volume here is intuitively for example for $n = 2$ the area of $S$ in the Cartesian plane, or with $n = 3$ it is the common understanding of *volume* for 3-dimensional objects."},{"metadata":{"_uuid":"c0f6744d8f272e3003d102375db1324a79e0fab2"},"cell_type":"markdown","source":"Example:"},{"metadata":{"_uuid":"a0a147d45a64ba1993de8ba8fe174926855bd107"},"cell_type":"markdown","source":"$A = \\begin{bmatrix}\n 1 & 3\\\\[0.3em]\n 3 & 2 \n\\end{bmatrix}$"},{"metadata":{"_uuid":"24a2eeb6ef0d65aceb11fcc075b1704dbc93adfe"},"cell_type":"markdown","source":"The rows of the matrix are:"},{"metadata":{"_uuid":"58f2dda1e93a143213f0c042ff45e50531dacb69"},"cell_type":"markdown","source":"$a_1 = \\begin{bmatrix}\n 1 \\\\[0.3em]\n 3 \n\\end{bmatrix}\n\\quad\na_2 = \\begin{bmatrix}\n 3 \\\\[0.3em]\n 2 \n\\end{bmatrix}$"},{"metadata":{"_uuid":"7a98353ba7d834e4e5eff9e4d47d566dd5c3bcf7"},"cell_type":"markdown","source":"The set S corresponding to these rows is shown in:"},{"metadata":{"_uuid":"b79f0b59db034dc4cae82baa909c1ecea465edde"},"cell_type":"markdown","source":"<img src=\"LinearAlgebra_Determinant_2x2.png\" style=\"max-width:100%; width: 30%; max-width: none\">"},{"metadata":{"_uuid":"22905f0fbe47a3faa1ee412af8d826b7d8e866c2"},"cell_type":"markdown","source":"The figure above is an illustration of the determinant for the $2\\times 2$ matrix $A$ above. Here, $a_1$ and $a_2$\nare vectors corresponding to the rows of $A$, and the set $S$ corresponds to the shaded region (i.e., the parallelogram). The absolute value of the determinant, $|\\mathrm{det}A| = 7$, is the area of the parallelogram."},{"metadata":{"_uuid":"3b5673d481c010c12a8d18bdca300a8edb0574ec"},"cell_type":"markdown","source":"For two-dimensional matrices, $S$ generally has the shape of a parallelogram. In our example, the value of the determinant is $|A| = 7$ (as can be computed using the formulas shown later), so the area of the parallelogram is $7$."},{"metadata":{"_uuid":"b3ee3e58537abad231076fa6dcad1f3abd68069e"},"cell_type":"markdown","source":"In three dimensions, the set $S$ corresponds to an object known as a parallelepiped (a three-dimensional box with skewed sides, such that every face has the shape of a parallelogram). The absolute value of the determinant of the $3 \\times 3$ matrix whose rows define $S$ give the three-dimensional volume of the parallelepiped. In even higher dimensions, the set $S$ is an object known as an $n$-dimensional parallelotope."},{"metadata":{"_uuid":"fde7372d4a93c088021b1b88ac7735898423d6fa"},"cell_type":"markdown","source":"Algebraically, the determinant satisfies the following three properties (from which all other properties follow, including the general formula):"},{"metadata":{"_uuid":"23fcae64aa37ed84a6623d4bbd4d3827bb4de479"},"cell_type":"markdown","source":"- The determinant of the identity is $1$, $|I| = 1$. (Geometrically, the volume of a unit hypercube is $1$).\n- Given a matrix $A \\in \\mathbb{R}^{n\\times n}$, if we multiply a single row in $A$ by a scalar $t \\in \\mathbb{R}$, then the determinant of the new matrix is $t|A|$,<br/>\n$\\left| \\begin{bmatrix}\n -- & t a_1^T  & -- \\\\[0.3em]\n -- & a_2^T  & -- \\\\[0.3em]\n    & \\vdots &  \\\\[0.3em]\n -- & a_m^T  & -- \n\\end{bmatrix}\\right|  = t|A|$<br/>\n(Geometrically, multiplying one of the sides of the set $S$ by a factor $t$ causes the volume\nto increase by a factor $t$.)\n- If we exchange any two rows $a^T_i$ and $a^T_j$ of $A$, then the determinant of the new matrix is $|A|$, for example<br/>\n$\\left| \\begin{bmatrix}\n -- & a_2^T  & -- \\\\[0.3em]\n -- & a_1^T  & -- \\\\[0.3em]\n    & \\vdots &  \\\\[0.3em]\n -- & a_m^T  & -- \n\\end{bmatrix}\\right|  = -|A|$"},{"metadata":{"_uuid":"53377e062ad34710a752ba01c2ad9b29e7a7acc1"},"cell_type":"markdown","source":"Several properties that follow from the three properties above include:"},{"metadata":{"_uuid":"adc1e33df4ba57d017b0d15b92dbed95411d0dd0"},"cell_type":"markdown","source":"- For $A \\in \\mathbb{R}^{n\\times n}$, $|A| = |A^T|$\n- For $A,B \\in \\mathbb{R}^{n\\times n}$, $|AB| = |A||B|$\n- For $A \\in \\mathbb{R}^{n\\times n}$, $|A| = 0$ if and only if $A$ is singular (i.e., non-invertible). (If $A$ is singular then it does not have full rank, and hence its columns are linearly dependent. In this case, the set $S$ corresponds to a \"flat sheet\" within the $n$-dimensional space and hence has zero volume.)\n- For $A \\in \\mathbb{R}^{n\\times n}$ and $A$ non-singular, $|A1| = 1/|A|$\n###### [Go to top](#top)"},{"metadata":{"_uuid":"b678c186a46e54adda4b3621e17d6b10a01b5104"},"cell_type":"markdown","source":""},{"metadata":{"_uuid":"c03b0d7f8c0409ac026d6c7274cfaf95b572a26c"},"cell_type":"markdown","source":"<a id=\"24\"></a> <br>\n# 16- Tensors"},{"metadata":{"_uuid":"baf22e2a7f0a839a26df2a17815b6f2867dc7c15"},"cell_type":"markdown","source":"A [**tensor**](https://en.wikipedia.org/wiki/Tensor) could be thought of as an organized multidimensional array of numerical values. A vector could be assumed to be a sub-class of a tensor. Rows of tensors extend alone the y-axis, columns along the x-axis. The **rank** of a scalar is 0, the rank of a **vector** is 1, the rank of a **matrix** is 2, the rank of a **tensor** is 3 or higher.\n\n###### [Go to top](#top)"},{"metadata":{"trusted":true,"_uuid":"3bb2dbff06ab25e05e379d45b5f529c94d2bf6aa"},"cell_type":"code","source":"A = tf.Variable(np.zeros((5, 5), dtype=np.float32), trainable=False)\nnew_part = tf.ones((2,3))\nupdate_A = A[2:4,2:5].assign(new_part)\nsess = tf.InteractiveSession()\ntf.global_variables_initializer().run()\nprint(update_A.eval())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9585bfae0dd3cd9de762cf8d5ffb801a2b24dc08"},"cell_type":"markdown","source":"<a id=\"25\"></a> <br>\n# 17- Hyperplane"},{"metadata":{"_uuid":"e689830f047dd755c68f83b0a4747928eb70c044"},"cell_type":"markdown","source":"The **hyperplane** is a sub-space in the ambient space with one dimension less. In a two-dimensional space the hyperplane is a line, in a three-dimensional space it is a two-dimensional plane, etc."},{"metadata":{"_uuid":"2f4ff05c6a2421c9e41d326d29970ff6be1b3695"},"cell_type":"markdown","source":"Hyperplanes divide an $n$-dimensional space into sub-spaces that might represent clases in a machine learning algorithm."},{"metadata":{"trusted":true,"_uuid":"43691809c6e28187520e3fce5fe89007dbda1166"},"cell_type":"code","source":"np.random.seed(0)\nX = np.r_[np.random.randn(20, 2) - [2, 2], np.random.randn(20, 2) + [2, 2]]\nY = [0] * 20 + [1] * 20\n\nfig, ax = plt.subplots()\nclf2 = svm.LinearSVC(C=1).fit(X, Y)\n\n# get the separating hyperplane\nw = clf2.coef_[0]\na = -w[0] / w[1]\nxx = np.linspace(-5, 5)\nyy = a * xx - (clf2.intercept_[0]) / w[1]\n\n# create a mesh to plot in\nx_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\ny_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\nxx2, yy2 = np.meshgrid(np.arange(x_min, x_max, .2),\n                     np.arange(y_min, y_max, .2))\nZ = clf2.predict(np.c_[xx2.ravel(), yy2.ravel()])\n\nZ = Z.reshape(xx2.shape)\nax.contourf(xx2, yy2, Z, cmap=plt.cm.coolwarm, alpha=0.3)\nax.scatter(X[:, 0], X[:, 1], c=Y, cmap=plt.cm.coolwarm, s=25)\nax.plot(xx,yy)\n\nax.axis([x_min, x_max,y_min, y_max])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5d0b6000150ad6ff588def76de5a253ede8cf2c0"},"cell_type":"markdown","source":"<a id=\"26\"></a> <br>\n# 18- Summary\nlet me summary what we read in this kernel"},{"metadata":{"_uuid":"96e75090c39b6cd532802b441136fa965ec1e23e"},"cell_type":"markdown","source":"<a id=\"27\"></a> <br>\n## 18-1 Dot Product"},{"metadata":{"_uuid":"17c6a340146ca249ecc0c705bcf1a1d3b238a41e"},"cell_type":"markdown","source":"This is also the *inner product*. It is a function that returns a number computed from two vectors of the same length by summing up the product of the corresponding dimensions."},{"metadata":{"_uuid":"568f93108472fb46ac6c1385eaf522df04b88963"},"cell_type":"markdown","source":"For two vectors $a = [a_1, a_2, \\dots{}, a_n]$ and $b = [b_1, b_2, \\dots{}, b_n]$ the dot product is:"},{"metadata":{"_uuid":"553525a7d5c8bdc0f42d15cb0c7b050b99259438"},"cell_type":"markdown","source":"$\\mathbf{a} \\cdot \\mathbf{b} = \\sum_{i=1}^{n} a_{i} b_{i} = a_{1} b_{1} + a_{2} b_{2} + \\cdots + a_{n} b_{n}$"},{"metadata":{"_uuid":"3ddded1808585f448ae581cefc5f7929dfb8b6bb"},"cell_type":"markdown","source":"If we normalize two vectors and compute the dot product, we get the *cosine similarity*, which can be used as a metric for cimilarity of vectors. Independent of the absolute length we look at the angle between the vectors, i.e. the lenght is neutralized via normalization."},{"metadata":{"_uuid":"209f09dffb7a9fd54d4f67505c9c2c99c3b9d8a1"},"cell_type":"markdown","source":"The cosine of two non-zero vectors can be derived by using the Euclidean dot product formula (see [Wikipedia: Cosine similarity](https://en.wikipedia.org/wiki/Cosine_similarity)):"},{"metadata":{"_uuid":"70b58e8c4b3a7f7dfd2c6d1ac2645d733567f695"},"cell_type":"markdown","source":"$\\mathbf{a} \\cdot \\mathbf{b} = \\left\\|\\mathbf{a}\\right\\| \\left\\|\\mathbf{b}\\right\\| \\cos\\theta$"},{"metadata":{"_uuid":"84637290dea55cd2b13490e2f909a5345e1c34e0"},"cell_type":"markdown","source":"Given two vectors of attributes, $A$ and $B$, the cosine similarity, $cos(\\theta)$, is represented using a dot product and magnitude as:"},{"metadata":{"_uuid":"b16a800c6e8a05128b743df3d2f46e9949aa0b91"},"cell_type":"markdown","source":"$\\text{similarity} = \\cos(\\theta) = \\frac{\\mathbf{A} \\cdot \\mathbf{B}}{ \\|\\mathbf{A} \\|\\|\\mathbf{B} \\| } = \\frac{\\sum \\limits_{i=1}^{n}{A_{i}B_{i}}}{{\\sqrt {\\sum \\limits _{i=1}^{n}{A_{i}^{2}}}}{\\sqrt {\\sum \\limits _{i=1}^{n}{B_{i}^{2}}}}}$, with $A_i$ and $B_i$ components of vector $A$ and $B$ respectively.\n\n###### [Go to top](#top)"},{"metadata":{"_uuid":"71b2e4d8c382f4136e42fd53b61ef88df119b080"},"cell_type":"markdown","source":"<a id=\"28\"></a> <br>\n## 18-2 Hadamard Product"},{"metadata":{"_uuid":"4c7d5009e2286222eb2cabbaed7ef566466b1750"},"cell_type":"markdown","source":"This is also known as the **entrywise product**. For two matrices $A \\in \\mathbb{R}^{m\\times n}$ and $B \\in \\mathbb{R}^{m\\times n}$ the Hadamard product $A\\circ B$ is:"},{"metadata":{"_uuid":"9a7b9033a63c6ab1f8cf966f223ade338e7246fa"},"cell_type":"markdown","source":"$(A\\circ B)_{i,j} = (A)_{i,j} (B)_{i,j}$"},{"metadata":{"_uuid":"6894eccfcce41feb652d7fc48b29d1d277600a61"},"cell_type":"markdown","source":"For example:"},{"metadata":{"_uuid":"fe03041de34c8d360f6c6dfec2203f728cd8ce14"},"cell_type":"markdown","source":"$\\begin{bmatrix}\n a_{11} & a_{12} & a_{13} \\\\[0.3em]\n a_{21} & a_{22} & a_{23} \\\\[0.3em]\n a_{31} & a_{32} & a_{33}\n\\end{bmatrix} \\circ\n\\begin{bmatrix}\n b_{11} & b_{12} & b_{13} \\\\[0.3em]\n b_{21} & b_{22} & b_{23} \\\\[0.3em]\n b_{31} & b_{32} & b_{33}\n\\end{bmatrix} = \n\\begin{bmatrix}\n a_{11}b_{11} & a_{12}b_{12} & a_{13}b_{13} \\\\[0.3em]\n a_{21}b_{21} & a_{22}b_{22} & a_{23}b_{23} \\\\[0.3em]\n a_{31}b_{31} & a_{32}b_{32} & a_{33}b_{33}\n\\end{bmatrix}$"},{"metadata":{"_uuid":"095f1aa2d9e40963f741190a0a6a27e965128b0b"},"cell_type":"markdown","source":"<a id=\"29\"></a> <br>\n## 18-3 Outer Product"},{"metadata":{"_uuid":"a5a14ff2193cead6734cd39fc259e8cb78eade8d"},"cell_type":"markdown","source":"This is also called the **tensor product** of two vectors. Compute the resulting matrix by multiplying each element from a column vector with all alements in a row vector."},{"metadata":{"_uuid":"e2c212b2e166a22e1223eb3dc8eadd8708d63da8"},"cell_type":"markdown","source":"<a id=\"30\"></a> <br>\n# 19- Eigenvalues and Eigenvectors\nAssume, we have two interest bearing accounts. The first gives an interest rate of 5%, the second a 3% interest, with annual compound.\n\nAssume that after $t$ years the amounts in the two accounts are represented by a 2-vector:\n\n$x^{(t)} = \\begin{bmatrix}\n amount in Account 1 \\\\[0.3em]\n amount in Account 2\n\\end{bmatrix}$\n\nThe growth of the amounts in one year can be described in a matrix:\n\n$x^{(t+1)} = \\begin{bmatrix}\n a_{11} & a_{12} \\\\[0.3em]\n a_{21} & a_{22}\n\\end{bmatrix} x^{(t)}$\n\nGiven the specification of the interest rate above, this simple case gives us:\n\n$x^{(t+1)} = \\begin{bmatrix}\n 1.05 & 0    \\\\[0.3em]\n 0    & 1.03\n\\end{bmatrix} x^{(t)}$\n\nLet $A$ denote the matrix: $\\begin{bmatrix}\n 1.05 & 0    \\\\[0.3em]\n 0    & 1.03\n\\end{bmatrix}$\n\n\n$A$ is a diagonal.\n\n\n###### [Go to top](#top)\n"},{"metadata":{"_uuid":"d4fcec0a1e26fc2141216557438defa7a21e2a35","trusted":true},"cell_type":"code","source":"import numpy as np\nx = np.array([[100],\n              [100]])\nA = np.array([[1.05, 0],\n              [0,    1.03]])\nA.dot(x)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1f7c6424ce119fcc291049a93fd9b2ae2f2b0d52"},"cell_type":"markdown","source":"After two years the accounts would be:"},{"metadata":{"_uuid":"01c9a78d158c1ea69aaacade830cae9064a0c8aa","trusted":true},"cell_type":"code","source":"A.dot(A.dot(x))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d7fbcdf5abef38a3e31d9862fbbbb63afe7f5aad"},"cell_type":"markdown","source":"If we might want to know how $x^{(100)}$ compares to $x^{(0)}$, we could iterate over:\n    \n    \n"},{"metadata":{"_uuid":"5c83c5721054bd3155a87fdd1aefc3709d83e5d2"},"cell_type":"markdown","source":"$\\begin{align}\nx^{(100)} & = A x^{(99)} \\\\\n          & = A(Ax^{(98)}) \\\\\n          & = A(A(Ax^{(97)})) \\\\\n          & \\vdots \\\\\n          & = \\underbrace{A \\cdot A \\dots A}_\\text{100 times} \\ x^{(0)} \n\\end{align}$\n\nWe can also write the product as $A^{100}$.\n\nNote that $A$ is a diagonal, thus the entries of $A^{100}$ are $1.05^{100}$ and $1.03^{100}$:\n\n$A^{100} = \\begin{bmatrix}\n 131.50125784630401 & 0    \\\\[0.3em]\n 0    & 19.218631980856298\n\\end{bmatrix}$\n\nWhat we can see is that account 1 dominates account 2, account 2 becoming less and less relevant over time."},{"metadata":{"_uuid":"4b488bdb25f40572d2493b54b3a60bfbaa4b0f5a"},"cell_type":"markdown","source":"<a id=\"31\"></a> <br>\n# 20- Exercises"},{"metadata":{"trusted":true,"_uuid":"73919bc844e32ce2015c4d1bebffcc41563dd854"},"cell_type":"code","source":"# Students may (probably should) ignore this code. It is just here to make pretty arrows.\n\ndef plot_vectors(vs):\n    \"\"\"Plot vectors in vs assuming origin at (0,0).\"\"\"\n    n = len(vs)\n    X, Y = np.zeros((n, 2))\n    U, V = np.vstack(vs).T\n    plt.quiver(X, Y, U, V, range(n), angles='xy', scale_units='xy', scale=1)\n    xmin, xmax = np.min([U, X]), np.max([U, X])\n    ymin, ymax = np.min([V, Y]), np.max([V, Y])\n    xrng = xmax - xmin\n    yrng = ymax - ymin\n    xmin -= 0.05*xrng\n    xmax += 0.05*xrng\n    ymin -= 0.05*yrng\n    ymax += 0.05*yrng\n    plt.axis([xmin, xmax, ymin, ymax])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"793dd7f954e8c85d121d50c7e729cb4890c146aa"},"cell_type":"code","source":"# Again, this code is not intended as a coding example.\n\na1 = np.array([3,0])         # axis\na2 = np.array([0,3])\n\nplt.figure(figsize=(8,4))\nplt.subplot(1,2,1)\nplot_vectors([a1, a2])\nv1 = np.array([2,3])\nplot_vectors([a1,v1])\nplt.text(2,3,\"(2,3)\",fontsize=16)\nplt.tight_layout()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3bf8f22bf0e5854d91f4b90469be693c2334974c"},"cell_type":"code","source":"#Matrices, Transformations and Geometric Interpretation\na1 = np.array([7,0])         # axis\na2 = np.array([0,5])\n\nA = np.array([[2,1],[1,1]])  # transformation f in standard basis\nv2 =np.dot(A,v1)\nplt.figure(figsize=(8,8))\nplot_vectors([a1, a2])\nv1 = np.array([2,3])\nplot_vectors([v1,v2])\nplt.text(2,3,\"v1 =(2,3)\",fontsize=16)\nplt.text(6,5,\"Av1 = \", fontsize=16)\nplt.text(v2[0],v2[1],\"(7,5)\",fontsize=16)\nprint(v2[1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c63ccda879138dfa233d469b4e64fbc1cb416098"},"cell_type":"code","source":"#Change to a Different Basis\ne1 = np.array([1,0])\ne2 = np.array([0,1])\nB = np.array([[1,4],[3,1]])\nplt.figure(figsize=(8,4))\nplt.subplot(1,2,1)\nplot_vectors([e1, e2])\nplt.subplot(1,2,2)\nplot_vectors([B.dot(e1), B.dot(e2)])\nplt.Circle((0,0),2)\n#plt.show()\n#plt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bff969c3b5fa1e67a13b77371f4c0b886ff58227"},"cell_type":"code","source":"#Inner Products \ne1 = np.array([1,0])\ne2 = np.array([0,1])\nA = np.array([[2,3],[3,1]])\nv1=A.dot(e1)\nv2=A.dot(e2)\nplt.figure(figsize=(8,4))\nplt.subplot(1,2,1)\nplot_vectors([e1, e2])\nplt.subplot(1,2,2)\nplot_vectors([v1,v2])\nplt.tight_layout()\n#help(plt.Circle)\nplt.Circle(np.array([0,0]),radius=1)\nplt.Circle.draw","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"36cbcb44b52b40e45aec34bac0b632183a09460a"},"cell_type":"code","source":"# using sqrt() to print the square root of matrix \nprint (\"The element wise square root is : \") \nprint (np.sqrt(x)) ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"afc2a360fedd783e5e9d7bbc975c9c6f06a2ee72"},"cell_type":"markdown","source":"<a id=\"32\"></a> <br>\n# 21-Conclusion\nIf you have made this far  give yourself a pat at the back. We have covered different aspects of **Linear algebra** in this Kernel. I have tried to give sufficient amount of information as well as keep the flow such that everybody can understand the concepts and be able to do necessary calculations. Still, if you get stuck somewhere, feel free to comment below.\n\n###### [Go to top](#top)"},{"metadata":{"_uuid":"b132163ee07917a0ab100b93f6ed5545ce0de45d"},"cell_type":"markdown","source":"you can follow me on:\n> ###### [ GitHub](https://github.com/mjbahmani/10-steps-to-become-a-data-scientist)\n> ###### [Kaggle](https://www.kaggle.com/mjbahmani/)\n\n <b>I hope you find this kernel helpful and some <font color='red'>UPVOTES</font> would be very much appreciated.<b/>\n "},{"metadata":{"_uuid":"5719a5ba111b65b20b53d538281ac773eb14471a"},"cell_type":"markdown","source":"<a id=\"33\"></a> <br>\n# 22-References"},{"metadata":{"_uuid":"aab5b3d8cb417250dc6baa081a579106900effba"},"cell_type":"markdown","source":"1. [Linear Algbra1](https://github.com/dcavar/python-tutorial-for-ipython)\n1. [Linear Algbra2](https://www.oreilly.com/library/view/data-science-from/9781491901410/ch04.html)\n1. [GitHub](https://github.com/mjbahmani/10-steps-to-become-a-data-scientist)\n\n\n"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}