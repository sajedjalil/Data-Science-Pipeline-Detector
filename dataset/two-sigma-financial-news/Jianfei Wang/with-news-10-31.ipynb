{"cells":[{"metadata":{"_uuid":"512d64e04ad0533dd72ca6bcbca4882a7f289956"},"cell_type":"markdown","source":"# Market Data Only Baseline\n\nUsing a lot of ideas from NN Baseline Kernel.\nsee. https://www.kaggle.com/christofhenkel/market-data-nn-baseline"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport time\nimport datetime\nfrom sklearn.metrics import accuracy_score\nfrom kaggle.competitions import twosigmanews\nimport gc","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true,"scrolled":true},"cell_type":"code","source":"env = twosigmanews.make_env()\n(market_train, news_train) = env.get_training_data()\ngc.enable()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9e7e45eab0af5570ca16126c8acab76e14fd090d"},"cell_type":"code","source":"#10:00之后的算成下一天，似乎有不好的影响\n# index = news_train['time'][news_train['time'].dt.hour > 22].index\n# news_train.loc[index,'time']  = news_train.loc[index,'time'].dt.ceil('d')\nnews_train['time'] = news_train['time'].dt.floor('d')\ncols = ['sentimentNegative','sentimentNeutral','sentimentPositive','relevance','companyCount','bodySize','sentenceCount','wordCount','firstMentionSentence',\n        'sentimentWordCount','takeSequence','sentimentClass','noveltyCount12H', 'noveltyCount24H','noveltyCount3D', 'noveltyCount5D', 'noveltyCount7D', \n        'volumeCounts12H','volumeCounts24H', 'volumeCounts3D', 'volumeCounts5D','volumeCounts7D']\n\nnews_total = news_train[['time','assetName'] + cols].copy()\ndel news_train\ngc.collect()\nnews_train = news_total\nprint(news_train.columns)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"04858b97eb857adcc9f398fbc65535b1cb6340b9"},"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(action ='ignore',category = DeprecationWarning)\n\n#直接相乘内存会爆掉，成之后，变成了0.66912，比最好0.66972差了一点，暂时不成\n# for col in cols:\n#     if col != 'relevance':\n#         print(col)\n#         news_train[col] = news_train[col] * news_train['relevance']\n#聚合每一个日期前三天内的新闻数据，影响股价走势\n#之前的版本，直接复制几份，然后和market_train进行join，代价较大\n#直接进行news data的join\ndef get_news_train(raw_data,days = 5):\n    news_last = pd.DataFrame()\n    #衰减系数\n    rate = 1.0\n    for i in range(days):\n        cur_train = raw_data[cols] * rate \n        rate *= 0.7\n        cur_train['time'] = raw_data['time'] + datetime.timedelta(days = i,hours=22)\n        cur_train['key'] = cur_train['time'].astype(str)+ raw_data['assetName'].astype(str)\n        #cur_train的groupby是被迫的操作，处理new_train，6天之内的，内存不足，下面的groupby\n        cur_train = cur_train[['key'] + cols].groupby('key').sum()\n        cur_train['key'] = cur_train.index.values\n        news_last = pd.concat([news_last, cur_train[['key'] + cols]])\n        del cur_train\n        gc.collect()\n        print(\"after concat the shape is:\",news_last.shape)\n        news_last = news_last.groupby('key').sum()\n        news_last['key'] = news_last.index.values\n        print(\"the result shape is:\",news_last.shape)\n       \n    del news_last['key']\n    return news_last\n\nnews_last = get_news_train(news_train)\nprint(news_last.shape)\nprint(news_last.head())\nprint(news_last.dtypes)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3f26fb818749b3de15657d8b73cd4bb75b1227e2"},"cell_type":"code","source":"market_train['key'] = market_train['time'].astype(str) + market_train['assetName'].astype(str)\nmarket_train = market_train.join(news_last,on = 'key',how='left')\nprint(market_train['sentimentNeutral'].isnull().value_counts())\nmarket_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0daf8923ed239a73cb70fc631662cfcea1fd06cb"},"cell_type":"code","source":"# print(market_train['assetName'].nunique())\n# print(news_train['assetName'].nunique())\n# 通过assetName 判断有market 中有12万个example没在 news中出现,通过时间进行join，交集太少，目前感觉使用\n# assetName比较合适\n# print(market_train['assetName'].isin(news_train['assetName']).value_counts())\n# print(market_train['time'].nunique())\n# print(news_train['time'].nunique())\n# print(news_train['time'].describe())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5dfa1843dfece6fccfca91896ef85a332b55e3e6"},"cell_type":"code","source":"cat_cols = ['assetCode','assetName']\nnum_cols = ['volume', 'close', 'open', 'returnsClosePrevRaw1', 'returnsOpenPrevRaw1', 'returnsClosePrevMktres1',\n                    'returnsOpenPrevMktres1', 'returnsClosePrevRaw10', 'returnsOpenPrevRaw10', 'returnsClosePrevMktres10',\n                    'returnsOpenPrevMktres10','sentimentNegative','sentimentNeutral','sentimentPositive','relevance','companyCount','bodySize',\n            'sentenceCount','wordCount','firstMentionSentence', 'sentimentWordCount','takeSequence','sentimentClass','noveltyCount12H', \n            'noveltyCount24H','noveltyCount3D', 'noveltyCount5D', 'noveltyCount7D', 'volumeCounts12H','volumeCounts24H', 'volumeCounts3D',\n            'volumeCounts5D','volumeCounts7D']\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e108339134e95473b4a983237d58adb64c3ef64a"},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\ntrain_indices, val_indices = train_test_split(market_train.index.values,test_size=0.25, random_state=23)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f51d00dc43857b446ae4a24b3718753f09040fd5"},"cell_type":"markdown","source":"# Handling categorical variables"},{"metadata":{"trusted":true,"_uuid":"301a65b834d8614a914883d49be1860550174f06"},"cell_type":"code","source":"def encode(encoder, x):\n    len_encoder = len(encoder)\n    try:\n        id = encoder[x]\n    except KeyError:\n        id = len_encoder\n    return id\nencoders = [{} for i in range(len(cat_cols))]\nfor i, cat in enumerate(cat_cols):\n    print('encoding %s ...' % cat, end=' ')\n    encoders[i] = {l: id for id, l in enumerate(market_train.loc[train_indices, cat].unique())}\n    market_train[cat] = market_train[cat].astype(str).apply(lambda x: encode(encoders[i], x))\n    print('Done')\n\nembed_sizes = [len(encoder) + 1 for encoder in encoders] #+1 for possible unknown assets","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"397360cbacbcb294daf5e0750f471e13ced31978"},"cell_type":"markdown","source":"# Handling numerical variables"},{"metadata":{"trusted":true,"_uuid":"0e6c878e412b3e819e056cee40181b96fbac2f78"},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler \nimport matplotlib\n# market_train[num_cols] = market_train[num_cols].fillna(0)\n#异常点过滤\n# print(market_train['close'][market_train['close'] > 1000].count())\n# print(market_train['open'][market_train['open'] > 1000].count())\n# print(market_train['volume'][market_train['volume'] > 1e+08].count())\n\nmarket_train['close'].clip(upper = 1000, inplace = True)\nmarket_train['open'].clip(upper = 1000, inplace = True)\nmarket_train['volume'].clip(upper = 1e+08, inplace = True)\n\n# matplotlib.rcParams['figure.figsize'] = (12.0, 6.0)\n# prices = pd.DataFrame({\"close\":market_train[\"close\"], \"log(close + 1)\":np.log1p(market_train[\"close\"])})\n# prices.hist(bins = 10)\n# 一定同时进行待预测数据集的对数转换\n# 开盘价，收盘价不太符合正态分布，进行一个对数转换\n# market_train['close'] = np.log1p(market_train['close'])\n# market_train['open'] = np.log1p(market_train['open'])\n\nprint('scaling numerical columns')\nscaler = StandardScaler()\ncol_mean = market_train[num_cols].mean()\nmarket_train[num_cols]=market_train[num_cols].fillna(col_mean)\n\nscaler = StandardScaler()\nmarket_train[num_cols] = scaler.fit_transform(market_train[num_cols])\n# market_train.describe()\n# market_train[num_cols].isna()\n# market_train['returnsClosePrevMktres1'].isnull().value_counts()\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"eae09951757845ad5bdf08e004e6477513ed739a"},"cell_type":"markdown","source":"# Prepare data"},{"metadata":{"trusted":true,"_uuid":"cb217fc475f418f18720f87a0596aa23eddeb209"},"cell_type":"code","source":"def get_input(market_train, indices):\n    X = market_train.loc[indices, num_cols]\n    for cat in cat_cols:\n        X[cat] = market_train.loc[indices, cat].values\n    y = (market_train.loc[indices,'returnsOpenNextMktres10'] >= 0).values\n    r = market_train.loc[indices,'returnsOpenNextMktres10'].values\n    u = market_train.loc[indices, 'universe']\n    d = market_train.loc[indices, 'time'].dt.date\n    return X,y,r,u,d\n\n# r, u and d are used to calculate the scoring metric\nX_train,y_train,r_train,u_train,d_train = get_input(market_train, train_indices)\n\nX_valid,y_valid,r_valid,u_valid,d_valid = get_input(market_train, val_indices)\nX_train.shape\nprint(X_valid.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ecd18fe6d1a0ecf20678f94d7ba4746a7103b407"},"cell_type":"markdown","source":"# Train  model using hyperopt to auto hyper_parameters turing"},{"metadata":{"trusted":true,"_uuid":"542f77f1f5675067d810614da9add097266b85a5"},"cell_type":"code","source":"\nfrom xgboost import XGBClassifier\nimport lightgbm as lgb\nfrom functools import partial\nfrom hyperopt import hp, fmin, tpe\nfrom sklearn.metrics import mean_squared_error\nalgo = partial(tpe.suggest, n_startup_jobs=10)\n# def auto_turing(args):\n#     #model = XGBClassifier(n_jobs = 4, n_estimators = args['n_estimators'],max_depth=6)\n#     model = lgb.LGBMClassifier(n_estimators=args['n_estimators'])\n#     model.fit(X_train,y_train.astype(int))\n#     confidence_valid = model.predict(X_valid)*2 -1\n#     score = accuracy_score(confidence_valid>0,y_valid)\n#     print(args,score)\n#     return -score\n# space = {\"n_estimators\":hp.choice(\"n_estimators\",range(20,200))}\n# print(fmin)\n# best = fmin(auto_turing, space, algo=algo,max_evals=30)\n# print(best)\n\n# 单机xgb程序\n# model = XGBClassifier(n_jobs = 4, n_estimators = 80, max_depth=6, subsample = 0.66,colsample_bytree = 0.66,learning_rate = 0.1)\n# model.fit(X_train,y_train.astype(int))\n# confidence_valid = model.predict(X_valid)*2 -1\n# score = accuracy_score(confidence_valid>0,y_valid)\n# print(score)\n# print(\"MSE\")\n# print(mean_squared_error(confidence_valid > 0, y_valid.astype(float)))\n# 单机lgb程序,训练比xgb快\n# import lightgbm as lgb\n# model = lgb.LGBMClassifier(num_threads = 4, n_estimators=100, feature_fraction = 0.66, bagging_fraction = 0.66,\n#                            early_stopping_rounds = 10,valid_sets = [X_valid, y_valid.astype(int)],objective = 'binary', metric='binary_logloss')\n# model.fit(X_train,y_train.astype(int))\n# confidence_valid = model.predict(X_valid)*2 -1\n# score = accuracy_score(confidence_valid>0,y_valid)\n# print(score)\n# print(\"MSE\",mean_squared_error(confidence_valid > 0, y_valid.astype(float)))\n# custom function to run light gbm model\ndef run_lgb(train_X, train_y, val_X, val_y,args):\n    params = {\n        \"objective\" : \"binary\",\n        \"metric\" : \"binary_logloss\", \n        \"num_leaves\" : args['num_leaves'],\n        \"min_child_samples\" : args['min_child_samples'],\n        \"learning_rate\" : args['learning_rate'],\n        \"bagging_fraction\" : 0.7,\n        \"feature_fraction\" : 0.66,\n        \"bagging_frequency\" : 5,\n        \"bagging_seed\" : 2018,\n        \"verbosity\" : -1\n    }\n    \n    lgtrain = lgb.Dataset(train_X, label=train_y)\n    lgval = lgb.Dataset(val_X, label=val_y)\n    model = lgb.train(params, lgtrain, args['n_estimators'], valid_sets=[lgval], early_stopping_rounds=50, verbose_eval=100)\n    \n#     pred_test_y = model.predict(test_X, num_iteration=model.best_iteration)\n    pred_val_y = model.predict(val_X, num_iteration=model.best_iteration)\n    confidence_valid = model.predict(X_valid)*2 -1\n    score = accuracy_score(confidence_valid > 0 , y_valid)\n    print(score)\n    mse = mean_squared_error(confidence_valid > 0, y_valid.astype(float))\n    print(\"MSE\", mse)\n    print(\"args\",args)\n    return model, mse\n\n# def auto_turing(args):\n#     model, mse = run_lgb(X_train, y_train.astype(int), X_valid, y_valid.astype(int),args)\n#     return mse\n# space = {\"n_estimators\":hp.choice('n_estimators',range(100,1000)),\n#          \"num_leaves\":hp.choice('num_leaves',range(20,100)),\n#          \"min_child_samples\":hp.choice(\"min_child_samples\",range(20,2000)),\n#          'learning_rate':hp.loguniform('learning_rate',0.01,0.3),\n#          'max_depth': hp.choice('max_depth', range(3,8))\n#         }\n# print(fmin)\n# best = fmin(auto_turing, space, algo=algo,max_evals=100)\n# print(best)\nargs = {'learning_rate': 1.0958730495793214, 'max_depth': 7, 'min_child_samples': 301, 'n_estimators': 439, 'num_leaves': 43}\nmodel, _ = run_lgb(X_train, y_train.astype(int), X_valid, y_valid.astype(int), args)\n\n# from sklearn.ensemble import RandomForestClassifier\n# distribution of confidence that will be used as submission\n# plt.hist(confidence_valid, bins='auto')\n# plt.title(\"predicted confidence\")\n# plt.show()\n# these are tuned params I found\ngc.collect()\n ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bcb2cb97ac4f6b797e9fe9f3cb2682090b1e6850"},"cell_type":"markdown","source":"Result validation"},{"metadata":{"trusted":true,"_uuid":"5d8097adcabf5794f04e4ec1f55f2993ef9176cb"},"cell_type":"code","source":"# calculation of actual metric that is used to calculate final score\nconfidence_valid = model.predict(X_valid)*2 -1\nr_valid = r_valid.clip(-1,1) # get rid of outliers. Where do they come from??\nx_t_i = confidence_valid * r_valid * u_valid\ndata = {'day' : d_valid, 'x_t_i' : x_t_i}\ndf = pd.DataFrame(data)\nx_t = df.groupby('day').sum().values.flatten()\nmean = np.mean(x_t)\nstd = np.std(x_t)\nscore_valid = mean / std\nprint(score_valid)\nmarket_train.describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a556452e64bef6bee50b7e281ef65f2923c554a7"},"cell_type":"markdown","source":"# Prediction"},{"metadata":{"trusted":true,"_uuid":"f57d1b1d8cf8bea0594e0d699f1b849566ebad86"},"cell_type":"code","source":"days = env.get_prediction_days()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0cea4e659153962bdf2062b0ca10943927549a26"},"cell_type":"code","source":"n_days = 0\npredicted_confidences = np.array([])\nfrom collections import deque\nnews_pre = deque()\nnews_all = pd.DataFrame()\nBaseMod = 50\nfor (market_obs_df, news_obs_df, predictions_template_df) in days:\n    n_days +=1\n    print(n_days,end=' ')\n    news_all = pd.concat([news_all,news_obs_df])\n    if n_days >= BaseMod and n_days % BaseMod >= 0 and n_days % BaseMod < 8:\n        news_pre.append(news_obs_df)\n    elif n_days >= BaseMod and n_days % BaseMod == 8:\n        del news_all\n        gc.collect()\n        news_all = pd.DataFrame()\n        for item in news_pre:\n            news_all = pd.concat([news_all,item])\n        news_pre.clear()\n    \n#     index = news_all['time'][news_all['time'].dt.hour > 22].index\n#     news_all.loc[index,'time']  = news_all.loc[index,'time'].dt.ceil('d')\n    news_all['time'] = news_all['time'].dt.floor('d')\n    news_last = pd.DataFrame()\n    \n#     for col in cols:\n#         if col != 'relevance':\n#             print(col)\n#             news_all[col] = news_all[col] * news_all['relevance']\n    #聚合每一个日期前三天内的新闻数据，影响股价走势\n    news_last = get_news_train(news_all)\n\n    market_obs_df['key'] = market_obs_df['time'].astype(str) + market_obs_df['assetName'].astype(str)\n    market_obs_df = market_obs_df.join(news_last,on = 'key',how='left')\n    \n    #异常点过滤\n    market_obs_df['close'].clip(upper = 1000, inplace = True)\n    market_obs_df['open'].clip(upper = 1000, inplace = True)\n    market_obs_df['volume'].clip(upper = 1e+08, inplace = True)\n    \n    # 对数转换\n#     market_obs_df['close'] = np.log1p(market_obs_df['close'])\n#     market_obs_df['open'] = np.log1p(market_obs_df['open'])\n    \n#     col_mean = [num_cols].mean()\n    #归一化\n    market_obs_df[num_cols]=market_obs_df[num_cols].fillna(col_mean)\n    market_obs_df[num_cols] = scaler.transform(market_obs_df[num_cols])\n    X_test = market_obs_df[num_cols]\n    X_test['assetCode'] = market_obs_df['assetCode'].apply(lambda x: encode(encoders[0], x)).values\n    X_test['assetName'] = market_obs_df['assetName'].apply(lambda x: encode(encoders[1], x)).values\n\n    \n    market_prediction = model.predict(X_test)*2 -1\n    predicted_confidences = np.concatenate((predicted_confidences, market_prediction))\n\n    preds = pd.DataFrame({'assetCode':market_obs_df['assetCode'],'confidence':market_prediction})\n    # insert predictions to template\n    predictions_template_df = predictions_template_df.merge(preds,how='left').drop('confidenceValue',axis=1).fillna(0).rename(columns={'confidence':'confidenceValue'})\n    env.predict(predictions_template_df)\n    del news_last\n    gc.collect()\n\nenv.write_submission_file()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c0b10b1ee21cfa2ff48b7ad2b7351382c03daeaa"},"cell_type":"code","source":"# distribution of confidence as a sanity check: they should be distributed as above\nplt.hist(predicted_confidences, bins='auto')\nplt.title(\"predicted confidence\")\nplt.show()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}