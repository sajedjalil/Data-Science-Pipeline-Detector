{"cells":[{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"# Dependencies\nfrom time import time\nfrom dateutil import parser\nfrom pandas.tseries.offsets import BDay\nfrom itertools import chain\n\nimport numpy as np\nimport pandas as pd\nimport random as rnd\nimport warnings\n\nwarnings.filterwarnings('ignore')\n%matplotlib inline\nnp.random.seed(42)\n\n# Metrics AND FUNCTIONS\n# Standardize the data:\nfrom sklearn.preprocessing import StandardScaler\n\n## CLASSIFIERS LIST\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import GradientBoostingRegressor\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import BaggingClassifier\n\n# Features\nf = {'firstMentionSentence':['median','std'],\n     'sentimentNeutral':['mean','std'], \n     'noveltyCount12H':['sum'],'noveltyCount24H':['sum'],'noveltyCount3D':['sum'],\n     'noveltyCount5D':['sum'],'noveltyCount7D':['sum'],\n     'relevance':['median'],  \n     'companyCount':['median'], \n     'sentimentNegative':['std'],\n     'sentimentWordCount':['median']}\n        \ndef pre_processing(mkt, nws):\n    \n## CONSOLIDATE TIME TO THE NEXT BUSINESS DAY\n\n    if mkt.time.dtype != 'datetime64[ns, UTC]':\n        mkt.time = mkt.time.apply(lambda x: parser.parse(x))\n        nws.time = nws.time.apply(lambda x: parser.parse(x))\n    nws['time'] = (nws['time'] - np.timedelta64(22,'h')).dt.ceil('1D') #.dt.date \n    mkt['time'] = mkt['time'].dt.floor('1D')\n    # Verify if business day, if not, roll to the next B day\n    offset = BDay()\n    nws.time = nws.time.apply(lambda x: offset.rollforward(x))\n    \n    ## TRIM\n    \n    mkt.drop(['assetName','open','returnsClosePrevRaw1','returnsOpenPrevRaw1',\n    'returnsClosePrevRaw10','returnsOpenPrevRaw10', \n    'returnsClosePrevMktres1'], axis=1, inplace=True)\n\n    nws.drop(['sourceTimestamp', 'firstCreated', 'sourceId', 'headline',\n    'takeSequence', 'provider', 'subjects', 'audiences','bodySize',\n    'headlineTag', 'marketCommentary', 'assetName',\n    'urgency', 'sentenceCount', 'wordCount', 'sentimentClass', 'sentimentPositive', 'volumeCounts12H',\n    'volumeCounts24H', 'volumeCounts3D', 'volumeCounts5D',\n    'volumeCounts7D'], axis=1, inplace=True)\n   \n    ## Break down the set of assets (10s)\n    \n    nws = expand_assets(nws)\n\n    ## FEATURE Engineering\n    \n    nws = nws.groupby(['time','assetCode']).agg(f)\n    # Correct the labels\n    col_name = ['_'.join(title) if isinstance(title, tuple) else title for title in nws.columns ]\n    nws.columns = col_name\n    # Regroup the noveltyCount variables into max, min, median and std. \n    current_col = [col for col in filter(lambda x: x.startswith('noveltyCount'), nws.columns)]\n    nws['novelty_median'] = nws[current_col].apply(np.median, axis=1)\n    nws['novelty_std'] = nws[current_col].apply(np.std, axis=1)\n    nws.drop(current_col, axis=1, inplace=True)\n    \n    # MERGING\n    \n    data = pd.merge(mkt, nws,  how='outer', left_on=['time','assetCode'], right_on = ['time','assetCode'])\n    del nws, mkt\n\n    # Set all Nans to 0\n    data = data.loc[(~data.volume.isnull()) & (~data.firstMentionSentence_median.isnull())].fillna(0)\n    return data\n\ndef expand_assets(nws):\n    \n    nws['assetCodes'] = nws['assetCodes'].str.findall(f\"'([\\w\\./]+)'\")   \n    assetCodes_expanded = list(chain(*nws['assetCodes']))\n    assetCodes_index = nws.index.repeat( nws['assetCodes'].apply(len) )\n    df = pd.DataFrame({'idx': assetCodes_index, 'assetCode': assetCodes_expanded})\n    # Create expandaded news (will repeat every assetCodes' row)\n    nws_expanded = pd.merge(df, nws, left_on='idx', right_index=True)\n    nws_expanded.drop(['idx','assetCodes'], axis=1, inplace=True)\n    return nws_expanded\n\ndef split_dataset(features, data):\n    \n    # Standardize\n    X = StandardScaler().fit_transform(data.loc[:, features].values)\n    y = np.array(data.target.values).reshape(X.shape[0],1)\n    training_size = np.floor(X.shape[0]*0.75).astype(int)\n    X_train = X[:training_size]\n    y_train = y[:training_size]\n    X_test = X[training_size:]\n    y_test = y[training_size:]\n    # Many training algorithms are sensitive to the order of the training instances, \n    # so it's generally good practice to shuffle them first:\n    rnd_idx = np.random.permutation(training_size)\n    X_train = X_train[rnd_idx]\n    y_train = y_train[rnd_idx]\n    return X_train, X_test, y_train, y_test\n\n# Memory saving function credit to https://www.kaggle.com/gemartin/load-data-reduce-memory-usage\ndef reduce_mem_usage(df):\n\n    for col in df.columns:\n        col_type = df[col].dtype.name\n\n        if col_type not in ['object', 'category', 'datetime64[ns, UTC]']:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n    return df\n\ndef training(X_train,y_train):\n\n    # --- SGD\n    sgd_paste_clf = BaggingClassifier(\n        SGDClassifier(random_state=42,alpha=0.01,l1_ratio=0.25,loss='log',penalty='elasticnet'),\n        n_estimators=50, n_jobs=1, random_state=40)\n    \n    # --- DT\n    dt_paste_clf = BaggingClassifier(\n        DecisionTreeClassifier(random_state=42,max_leaf_nodes=91,min_samples_leaf=7,min_weight_fraction_leaf=0.01),\n        n_estimators=50, n_jobs=1, random_state=40)\n    \n    # --- Gradient Boosting\n    gbrt_best = GradientBoostingRegressor(max_depth=2, n_estimators=35, random_state=42)\n\n    sgd_paste_clf.fit(X_train, y_train)\n    dt_paste_clf.fit(X_train, y_train)\n    gbrt_best.fit(X_train, y_train)\n\n    return sgd_paste_clf, dt_paste_clf, gbrt_best\n\ndef get_prediction(clf1, clf2, clf3, X_test):\n    yhat = {\n    'SGD': clf1.predict_proba(X_test),\n    'DT': clf2.predict_proba(X_test)} \n    yhat_GB = {'GB':clf3.predict(X_test)}\n\n    # Hard Voting with Soft Voting output\n    my_dict = pd.DataFrame({'clf': list(yhat.keys()),    # score = range(len(tst.valuesof sgd))\n            'tags': list(yhat.values())}, columns = ['clf', 'tags'])\n    tags = my_dict['tags'].apply(pd.DataFrame)\n    df_temp = pd.DataFrame()\n    for i,d in enumerate(yhat.keys()):\n        tags[i].columns = [d+'-1',d+'+1']\n        df_temp = pd.concat([df_temp, tags[i]], axis=1)\n    maxCol=lambda x: max(x.min(), x.max(), key=abs)\n\n    df_temp['SGD-1'] = df_temp['SGD-1'].apply(lambda y: y*(-1))\n    df_temp['SGD_Label'] = df_temp.loc[:,df_temp.columns.str.startswith('SGD')].apply(maxCol,axis=1)\n    df_temp['DT-1'] = df_temp['DT-1'].apply(lambda y: y*(-1))\n    df_temp['DT_Label'] = df_temp.loc[:,df_temp.columns.str.startswith('DT')].apply(maxCol,axis=1)\n\n    df_gb = pd.DataFrame(yhat_GB['GB'], columns=['GB'])\n    df_gb['GB-1'] = df_gb.GB.apply(lambda x: x-1 if x<=50 else x)\n    df_gb.columns = ['GB+1','GB-1']\n    df_gb['GB_Label'] = df_gb.loc[:,df_gb.columns.str.startswith('GB')].apply(maxCol,axis=1)\n\n    df_temp = pd.concat([df_temp,df_gb], axis=1)\n    df_temp['myHV'] = df_temp.loc[:,df_temp.columns.str.endswith('_Label')].apply(lambda l: np.bincount(l>=0).argmax(), axis=1)\n\n    return df_temp.apply(hVote, axis=1)\n\ndef hVote(r):\n    avg = []\n    for l in ['SGD_Label','DT_Label','GB_Label']:\n        if (r.myHV==0 and r[l]<0) or (r.myHV==1 and r[l]>0):\n            avg.append(r[l])\n    return np.mean(avg)\n\ndef make_predictions(predictions_template_df, market_obs_df, news_obs_df):\n    # Preprocessing\n    df = pre_processing(market_obs_df, news_obs_df)\n    df.reset_index(inplace=True)\n    df.drop(['time','index'], axis=1, inplace=True)\n    features = df.columns.difference(['assetCode'])\n        # On the iteration 386 there is so few news that when merged with the market the dataframe has no data. \n    if df.shape[0] > 0:\n        X_test = StandardScaler().fit_transform(df.loc[:, features].values)\n        # Prediction\n#         y_pred = model.predict_proba(X_test)\n#         df['confidenceValue'] = [p if p>=0.5 else 1-p for p in y_pred[:,1]]\n        df['confidenceValue'] = get_prediction(clf1, clf2, clf3, X_test)\n        # Merge prediction with the respective asset code\n        pred = pd.merge(predictions_template_df,df.loc[:,['assetCode','confidenceValue']],on='assetCode', how='outer').loc[:,[\"assetCode\", \"confidenceValue_y\"]].fillna(0)\n    else: \n        pred = predictions_template_df.copy()\n        pred.columns = ['assetCode','confidenceValue_y']\n    predictions_template_df.confidenceValue = pred.confidenceValue_y\n    \ndef get_prediction_2(clf, X_test):\n    return clf.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6f326d1cd26f67e7473dda7323d3a938ff7494d1","scrolled":false},"cell_type":"code","source":"# First let's import the module and create an environment.\nfrom kaggle.competitions import twosigmanews\n# You can only call make_env() once, so don't lose it!\nenv = twosigmanews.make_env()\n(market_train_df, news_train_df) = env.get_training_data()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4121007715b7061a2b37e64f974d042461d73040"},"cell_type":"markdown","source":"### Preprocessing"},{"metadata":{"trusted":true,"_uuid":"28a67c5f6adee226d6a84329d61d5aaf607da968","scrolled":false},"cell_type":"code","source":"# -------- ~ 700 s\nstart = time()\n\nmkt = reduce_mem_usage(market_train_df)\nnws = reduce_mem_usage(news_train_df)\ndel market_train_df, news_train_df\n\nmkt[\"returnsOpenNextMktres10\"] = mkt[\"returnsOpenNextMktres10\"] > 0 # .clip(-1, 1)\nmkt.rename(columns={'returnsOpenNextMktres10':'target'}, inplace=True)\n\ndata = pre_processing(mkt, nws)\n\n# Reset Index\ndata.reset_index(inplace=True)\ndata.drop(['time','assetCode','index','universe'], axis=1, inplace=True)\n\n# RESULTS\nprint('Preprocessing Completed:',time()-start, 'seconds')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ee048cdd3153952b2765defb4db5fa3695b3465b"},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b8179a288fb8fb98d1f7528f3f1948a3e654744a"},"cell_type":"markdown","source":"### Training"},{"metadata":{"trusted":true,"_uuid":"ec067ba0b7f1d733ad93f981aee524576965068b","scrolled":true},"cell_type":"code","source":"## ---------  ~ 1000s\nstart = time()\n\n## -------- Split dataset\nfeatures = data.columns.difference(['target'])\nX_train, X_test, y_train, y_test = split_dataset(features, data)\n\nclf1, clf2, clf3 = training(X_train, y_train)\n\nprint(time()-start)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"47a8e21918cd40678f8c86b1514d14e123c1f106"},"cell_type":"code","source":"# --- Analytics\n\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import precision_recall_curve\nfrom sklearn.metrics import precision_score, recall_score\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.dummy import DummyClassifier \n\n# Results\nres = pd.DataFrame()\n\n# Results\ndef get_res(res, clf_time, clf_name, X_train, y_test, y_pred, p):\n    return pd.concat([res,\n               pd.DataFrame({'data_size':str(X_train.shape),\n              'ETA': clf_time,\n              'Acc': accuracy_score(y_test, y_pred),\n              'Precision': precision_score(y_test, y_pred),\n              'Recall': recall_score(y_test, y_pred),\n              'F1': f1_score(y_test, y_pred),\n              'MSE': mean_squared_error(y_test, y_pred*1),\n              'AUC': roc_auc_score(y_test, y_pred), \n              'Params':p}, index=[clf_name])])\n\n# Function used in feature importance selection\ndef prep_res(r):\n    # Remove the ETA column as well as the Params and data size\n    r.drop(['ETA','Params','data_size'], inplace=True, axis=1)\n    r = r.stack().reset_index()\n    # Join the two indexes together and convert it to a df\n    # Merge \n    r['metric'] = r.apply(lambda row: row.level_0+' '+row.level_1, axis=1)\n    #res.drop(['level_0','level_1'], inplace=True, axis=1)\n    #res.set_index('metric',inplace=True)\n    r.columns = ['clf','metric','all','mix']\n    return r\n\ndef get_baseline(res, X_train, y_train, X_test, y_test, title):\n    ## BASELINE  \n    # Logistic Regression  95s\n    start = time()\n    log_clf = LogisticRegression(random_state=42)\n    log_clf.fit(X_train, y_train)\n    y_pred = log_clf.predict(X_test)\n    res = get_res(res, time()-start, 'LogReg', X_train, y_test, y_pred, title)\n\n    # SGD\n    start = time()\n    sgd_clf = SGDClassifier(random_state=42)\n    sgd_clf.fit(X_train, y_train)\n    y_pred = sgd_clf.predict(X_test)\n    res = get_res(res, time()-start, 'SGD', X_train, y_test, y_pred, title)\n\n    # Decision Tree\n    start = time()\n    tree_clf = DecisionTreeClassifier(max_depth=2, random_state=42)\n    tree_clf.fit(X_train, y_train)\n    # tree_clf.predict_proba(X_test)\n    tree_clf.predict(X_test)\n    res = get_res(res, time()-start, 'DecTree', X_train, y_test, y_pred, title)\n    return res","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"24fcd68125ae7e43ab047fff72ba65ac49eacc7d"},"cell_type":"code","source":"## --------- Results for Analysis\nstart = time()\nres = get_res(pd.DataFrame(), time()-start, 'SGD', X_train, y_test, clf1.predict(X_test), 'Final')\nstart = time()\nres = get_res(res, time()-start, 'DT', X_train, y_test, clf2.predict(X_test), 'Final')\nstart = time()\nres = get_res(res, time()-start, 'GBoost', X_train, y_test, clf3.predict(X_test)>=0.5, 'Final')\nstart = time()\nres = get_res(res, time()-start, 'HVoting', X_train, y_test, get_prediction(clf1, clf2, clf3, X_test)>0, 'Final')\n\nres.loc[:,res.columns.difference(['Params','data_size'])].head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"aed6cc4ffcaa62ba443eb0999bd50d022562368f"},"cell_type":"markdown","source":"### Testing the competition's test set"},{"metadata":{"trusted":true,"_uuid":"9f44606da364ca7e70a0dfe3912cca180bc752c9"},"cell_type":"code","source":"## ------ Train whole dataset\n# ~ 1000 s\n\nX_train = np.vstack((X_train,X_test))\ny_train = np.vstack((y_train, y_test))\n\nclf1, clf2, clf3 = training(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7562aa92bc83bd077ac11f6baf496bf9c2d66f13"},"cell_type":"code","source":"def make_predictions_2(predictions_template_df, market_obs_df, news_obs_df):\n    # Preprocessing\n    df = pre_processing(market_obs_df, news_obs_df)\n    df.reset_index(inplace=True)\n    df.drop(['time','index'], axis=1, inplace=True)\n    features = df.columns.difference(['assetCode'])\n        # On the iteration 386 there is so few news that when merged with the market the dataframe has no data. \n    if df.shape[0] > 0:\n        X_test = StandardScaler().fit_transform(df.loc[:, features].values)\n        # Prediction\n#         y_pred = model.predict_proba(X_test)\n#         df['confidenceValue'] = [p if p>=0.5 else 1-p for p in y_pred[:,1]]\n        df['confidenceValue'] = get_prediction_2(clf3, X_test)\n        # Merge prediction with the respective asset code\n        pred = pd.merge(predictions_template_df,df.loc[:,['assetCode','confidenceValue']],on='assetCode', how='outer').loc[:,[\"assetCode\", \"confidenceValue_y\"]].fillna(0)\n    else: \n        pred = predictions_template_df.copy()\n        pred.columns = ['assetCode','confidenceValue_y']\n    predictions_template_df.confidenceValue = pred.confidenceValue_y","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c4a33933fda4ec65427eb081a745e2b612518a66","scrolled":true},"cell_type":"code","source":"## ------ Test\n# ~ 510 s\n\nstart = time()\n\ndays = env.get_prediction_days()\n\nfor (market_obs_df, news_obs_df, predictions_template_df) in days:\n    make_predictions(predictions_template_df, market_obs_df, news_obs_df)\n    env.predict(predictions_template_df)\n\nenv.write_submission_file()\n\nprint('Done',time()-start)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cb76692f5ca3a516fbf5f261ba204b9eb2b11267"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}