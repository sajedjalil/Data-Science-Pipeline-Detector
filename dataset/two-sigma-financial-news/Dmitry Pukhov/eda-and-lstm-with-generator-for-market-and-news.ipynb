{"cells":[{"metadata":{"_uuid":"7d991c36c4724376252558b4e494858192a5dc1d"},"cell_type":"markdown","source":"# 1. Introduction\n\n**Main idea:** use LSTM neural network to predict stock movements.\nCode only version of this kernel on Github: https://github.com/DmitryPukhov/marketnewslstm\n\nI used ideas and sometimes copy pasted the code from kernels:\n\nEDA, outliers: https://www.kaggle.com/artgor/eda-feature-engineering-and-everything\n\nNN: https://www.kaggle.com/christofhenkel/market-data-nn-baseline#\n\nLSTM: https://www.kaggle.com/pablocastilla, https://www.kaggle.com/sergeykalutsky/lstm-model-on-market-data#,  https://www.kaggle.com/ashkaan/lstm-baseline# \n\nNews processing: https://www.kaggle.com/bguberfain/a-simple-model-using-the-market-and-news-data# \n\n**Disclaimer:** currently the model's performance is not perfect.\n\n**ToDo:**\n \n1. Experiment with news resampling on different periods - 10 days. Now many market rows have** empty news** joined\n1. Train on random time windows instead of sampling single records.\n1. Try technical indicators on market data: MACD, RSI etc. \n1. Work with residuals instead of raw data\n"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"scrolled":true},"cell_type":"code","source":"#####################################\n# Libraries\n#####################################\n# Common libs\nimport pandas as pd\nimport numpy as np\nimport sys\nimport os\nimport os.path\nimport random\nfrom pathlib import Path\nfrom time import time\nfrom itertools import chain\nimport gc\n\n# Image processing\nimport imageio\nimport skimage\nimport skimage.io\nimport skimage.transform\n#from skimage.transform import rescale, resize, downscale_local_mean\n\n# Charts\nimport matplotlib.pyplot as plt\nfrom matplotlib.ticker import MaxNLocator\nimport seaborn as sns\n\n\n# ML\nimport scipy\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\nfrom sklearn.metrics import accuracy_score\nfrom xgboost import XGBClassifier\n#from sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\n#from sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import QuantileTransformer,StandardScaler, MinMaxScaler,OneHotEncoder, LabelEncoder, RobustScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split, GridSearchCV\n#from sklearn.preprocessing import OneHotEncoder\nfrom keras.preprocessing.sequence import TimeseriesGenerator\nfrom keras import optimizers\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Conv2D, Flatten, MaxPool2D, Dropout, BatchNormalization, LSTM, Embedding\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.callbacks import ModelCheckpoint, Callback, EarlyStopping, ReduceLROnPlateau\nfrom keras.utils import to_categorical\nimport tensorflow\n\n#####################################\n# Settings\n#####################################\nplt.style.use('seaborn')\n# Set random seed to make results reproducable\nnp.random.seed(42)\ntensorflow.set_random_seed(42)\nos.environ['PYTHONHASHSEED'] = '42'\n# Improve printed df readability\npd.options.display.float_format = '{:,.4f}'.format\npd.set_option('display.max_columns', 100)\npd.set_option('display.width', 200)\n\nprint(os.listdir(\"../input\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0e4c6d73d0bd15aa0a90ba67e81cac4f3884815c"},"cell_type":"code","source":"# This competition settings\nfrom kaggle.competitions import twosigmanews\nenv = twosigmanews.make_env()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cdb8de212049feaee5ae7d97251e673167317a51"},"cell_type":"code","source":"# Read the data\n# Read market and news data\n(market, news) = env.get_training_data()\n\n# Will use daily data only\nnews.time = pd.to_datetime(news.time.astype('datetime64').dt.date, utc=True)\nmarket.time = pd.to_datetime(market.time.astype('datetime64').dt.date, utc=True)\n\n# market.time = market.time.dt.date\n# news.time = news.time.dt.date","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d14b97e70e8a58a00d9fe848209ac37aaa861d05"},"cell_type":"code","source":"toy=False\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3cbafb2d81af7efc279b3e82c12db5560c11de42"},"cell_type":"markdown","source":"# 2. Market data EDA\n\n"},{"metadata":{"_uuid":"08ee61bcfbe325cd164cb482e76dc79d4cd136c6"},"cell_type":"markdown","source":"## General view of market data"},{"metadata":{"trusted":true,"_uuid":"88b43981558d36cfc7ad8424cbab2876739a4035"},"cell_type":"code","source":"market.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"d45d4122252b45c730e1b1329d74e17f28aed2f6"},"cell_type":"code","source":"# Look at column types\nmarket.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"beb3dfd4d3bfb51440aa8a7f74e5a448b677320b"},"cell_type":"code","source":"# Look at min-max, quantiles\nmarket.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5dc1a2d927875f06b513ba9997ffdbe803c9918d"},"cell_type":"code","source":"# How many total records and assets are in the data\nnassets=len(market.assetName.unique().categories)\nnrows=market.close.count()\nprint(\"Total count: %d records of %d assets\" % (nrows, nassets))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0e706e6de7be9968c23fe9d2e03a6e0cf97cefef"},"cell_type":"markdown","source":"## Look at label values\n"},{"metadata":{"trusted":true,"_uuid":"1bf0b433b2b62fc7fecbb73f2047845bcab9421e"},"cell_type":"code","source":"# Plot label column\nmarket.returnsOpenNextMktres10.plot(figsize=(12,5))\nplt.title('Label values: returnsOpenNextMktres10')\nplt.ylabel('returnsOpenNextMktres10')\nplt.xlabel('Observation no')\nplt.show()\n\n# Look at quantiles\nmarket.returnsOpenNextMktres10.describe(percentiles=[0.01, 0.99])\n#market.returnsOpenNextMktres10.describe()\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d9d7ec6e1010b5fff31de23e1dd5c1ed0c067ece"},"cell_type":"markdown","source":"As we can see, the most of labels lay between -0.2 and 0.2 and there are otliers. "},{"metadata":{"trusted":true,"_uuid":"e4687e251755a199c0321a20706b4b514a6d2482"},"cell_type":"code","source":"sns.distplot(market.returnsOpenNextMktres10.clip(-1,1))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4ecb013e4cef2cfec05b4bf61e72fa50e1d78462"},"cell_type":"markdown","source":"## Price and volume chart of random asset."},{"metadata":{"trusted":true,"_uuid":"429383be22444b0e8ab691025d1a5f5655f4f532"},"cell_type":"code","source":"def plot_random_asset(market):\n    \"\"\"\n    Get random asset, show price, volatility and volume\n    \"\"\"\n    # Get any asset\n    ass = market.assetCode.sample(1, random_state=24).iloc[0]\n    ass_market = market[market['assetCode'] == ass]\n    ass_market.index = ass_market.time\n\n    # Plotting\n    f, axs = plt.subplots(3,1, sharex=True, figsize=(12,8))\n    # Close price \n    ass_market.close.plot(ax=axs[0])\n    axs[0].set_ylabel(\"Price\")\n\n    # Volatility (close-open)\n    volat_df = (ass_market.close - ass_market.open)\n    (ass_market.close - ass_market.open).plot(color='green', ax = axs[1])\n    axs[1].set_ylabel(\"Volatility\")\n\n    # Volume\n    ass_market.volume.plot(ax=axs[2], color='darkred')\n    axs[2].set_ylabel(\"Volume\")\n\n    # Show the plot\n    f.suptitle(\"Asset: %s\" % ass, fontsize=22)\n    plt.tight_layout()\n    plt.subplots_adjust(top=0.93)\n    plt.show()\n\nplot_random_asset(market)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6c4f895acc2ee587015e5a048e6b29c672987cb7"},"cell_type":"markdown","source":"# 3. News data EDA"},{"metadata":{"_uuid":"74af99712fedd3c3444d11dc56cc6f4c37783f13"},"cell_type":"markdown","source":"## General look"},{"metadata":{"trusted":true,"_uuid":"0f929789f26f86022c9717166fdd9065af86aebd"},"cell_type":"code","source":"news.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"218f014ba0bb68ce59c4e60a838cf58777c848ed","scrolled":true},"cell_type":"code","source":"# See column types\nnews.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dda568f22f29d229dec984c5bc9c5ab522df9253"},"cell_type":"code","source":"nnews = news.size\nnassets = len(news.assetName.cat.categories)\nprint(\"Total %d news about %d assets\" % (nnews, nassets))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ad1660ff27ae184bda5253b204ab5edcee511934"},"cell_type":"markdown","source":"## Positivity and negativity\nLet's see which attitude prevails in news."},{"metadata":{"trusted":true,"_uuid":"144e5d4a831086e37c87ee038f8ee7578a5b740a"},"cell_type":"code","source":"    # Barplot on negative, neutral and positive columns.\n    news[['sentimentNegative', 'sentimentNeutral','sentimentPositive']].mean().plot(kind='bar')\n    plt.title(\"News positivity chart\")\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"37dad8f7614beb8e9af710199a61b798a510be6f"},"cell_type":"markdown","source":"Neutral and positive a little bit higher. So according to the news the market is something like flat whith a little grow tendency."},{"metadata":{"_uuid":"6f5555947cb4d6d0e197ceba77a4c970b7387618"},"cell_type":"markdown","source":"# 4.  Preprocess the data\nWe are going to use data generator to feed the model. Generator yields data to the model batch by batch. For each batch we are doing following steps:\nPreprocess news and market separately. Then join them and yield, so it will come to the **model.fit_generate** method."},{"metadata":{"_uuid":"456914e6064c04aacf182d861dba8e2829a2ae95"},"cell_type":"markdown","source":"\n## Split to train, validation and test\n\nWe are using indices with time and asset code only. Full features and labels will be prepared in generator per batch to save memory.\n"},{"metadata":{"trusted":true,"_uuid":"c78f1666ca72f8ea6d2620f6376ab8fdb9e4ef88"},"cell_type":"code","source":"def train_test_val_split(market):\n    \"\"\"\n    Get sample of assets but each asset has full market data after 2009\n    Split to time sorted train, validation and test.\n    @return: train, validation, test df. Short variant - time and asset columns only\n    \"\"\"\n    # Work with data after 2009\n    market_idx = market[market.time > '2009'][['time', 'assetCode']]\n    if toy: market_idx = market_idx.sample(100000)\n    else: market_idx = market_idx.sample(1000000)\n    # Split to train, validation and test\n    market_idx = market_idx.sort_values(by=['time'])\n    market_train_idx, market_test_idx = train_test_split(market_idx, shuffle=False, random_state=24)\n    market_train_idx, market_val_idx = train_test_split(market_train_idx, test_size=0.1, shuffle=False, random_state=24)\n    return(market_train_idx, market_val_idx, market_test_idx)\n\n# Split\ntrain_idx, val_idx, test_idx = train_test_val_split(market)\n\n# Plot train/val/test size\nsns.barplot(['Train', 'Validation', 'Test'],[train_idx.index.size, val_idx.index.size, test_idx.index.size])\nplt.title('Train, validation, test split.')\nplt.ylabel('Count')\nplt.show()\n\ntrain_idx.tail()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"eafcd6a52d8e6375dd4d54fce2c41029d180454d"},"cell_type":"markdown","source":"## Market preprocessor\nPrepare market batch for generator - scale numeric columns, encode categorical etc."},{"metadata":{"trusted":true,"scrolled":false,"_uuid":"85edd087f2e51454ba31d540d624004af2f9e288"},"cell_type":"code","source":"class MarketPrepro:\n    \"\"\"\n    Standard way to generate batches for model.fit_generator(generator, ...)\n    Should be fit on train data and used on all train, validation, test\n    \"\"\"\n    # Features\n    assetcode_encoded = []\n    time_cols = ['year', 'week', 'day', 'dayofweek']\n    numeric_cols = ['volume', 'close', 'open', 'returnsClosePrevRaw1', 'returnsOpenPrevRaw1',\n                    'returnsClosePrevMktres1', 'returnsOpenPrevMktres1', 'returnsClosePrevRaw10',\n                    'returnsOpenPrevRaw10', 'returnsClosePrevMktres10', 'returnsOpenPrevMktres10']\n    feature_cols = ['assetCode_encoded'] + time_cols + numeric_cols\n\n    # Labels\n    label_cols = ['returnsOpenNextMktres10']\n\n    def __init__(self):\n        self.cats = {}\n        self.numeric_scaler = StandardScaler()\n\n    def fit(self, market_train_idx, market):\n        \"\"\"\n        Fit preprocessing scalers, encoders on given train df.\n        Store given indices to generate batches_from.\n        @param market_train_df: train data to fit on\n        \"\"\"\n        market_train_df = market.loc[market_train_idx.index].copy()\n        # Clean bad data. We fit on train dataset and it's ok to remove bad data\n        market_train_df = self.fix_train(market_train_df)\n\n        # Extract day, week, year from time\n        market_train_df = self.prepare_time_cols(market_train_df)\n        # Fit for numeric and time\n        # self.numeric_scaler = QuantileTransformer()\n        self.numeric_scaler.fit(market_train_df[self.numeric_cols + self.time_cols])\n\n        # Fit asset encoding\n        self.encode_asset(market_train_df, is_train=True)\n\n    def fix_train(self, train_df):\n        \"\"\"\n        Remove bad data. For train dataset only\n        \"\"\"\n        # Remove strange cases with close/open ratio > 2\n        max_ratio = 2\n        train_df = train_df[(train_df['close'] / train_df['open']).abs() <= max_ratio].loc[:]\n        # Fix outliers etc like for test set\n        train_df = self.safe_fix(train_df)\n        return train_df\n\n    def safe_fix(self, df):\n        \"\"\"\n        Fill na, fix outliers. Safe for test dataset, no rows removed.\n        \"\"\"\n        # Fill nans\n        df[self.numeric_cols] = df[self.numeric_cols].fillna(0)\n        # Fix outliers\n        df[self.numeric_cols] = df[self.numeric_cols].clip(df[self.numeric_cols].quantile(0.01),\n                                                           df[self.numeric_cols].quantile(0.99), axis=1)\n        return df\n\n    def get_X(self, df):\n        \"\"\"\n        Preprocess and return X without y\n        \"\"\"\n        df = df.copy()\n        df = self.safe_fix(df)\n\n        # Add day, week, year\n        df = self.prepare_time_cols(df)\n        # Encode assetCode\n        df = self.encode_asset(df)\n        # Scale numeric features and labels\n\n        df = df.set_index(['assetCode', 'time'], drop=False)\n        df[self.numeric_cols + self.time_cols] = self.numeric_scaler.transform(\n            df[self.numeric_cols + self.time_cols].astype(float))\n\n        # print(df.head())\n        # Return X\n        return df[self.feature_cols]\n\n    def get_y(self, df, is_raw_y=False):\n        if is_raw_y:\n            return df[self.label_cols]\n        else:\n            return (df[self.label_cols] >= 0).astype(float)\n\n    def encode_asset(self, df, is_train=False):\n        def encode(assetcode):\n            \"\"\"\n            Encode categorical features to numbers\n            \"\"\"\n            try:\n                # Transform to index of name in stored names list\n                index_value = self.assetcode_encoded.index(assetcode) + 1\n            except ValueError:\n                # If new value, add it to the list and return new index\n                self.assetcode_encoded.append(assetcode)\n                index_value = len(self.assetcode_encoded)\n\n            # index_value = 1.0/(index_value)\n            index_value = index_value / (self.assetcode_train_count + 1)\n            return (index_value)\n\n        # Store train assetcode_train_count for use as a delimiter for test data encoding\n        if is_train:\n            self.assetcode_train_count = len(df['assetCode'].unique()) + 1\n\n        df['assetCode_encoded'] = df['assetCode'].apply(lambda assetcode: encode(assetcode))\n        return (df)\n\n    @staticmethod\n    def prepare_time_cols(df):\n        \"\"\"\n        Extract time parts, they are important for time series\n        \"\"\"\n        df['year'] = pd.to_datetime(df['time']).dt.year\n        # Maybe remove month because week of year can handle the same info\n        df['day'] = pd.to_datetime(df['time']).dt.day\n        # Week of year\n        df['week'] = pd.to_datetime(df['time']).dt.week\n        df['dayofweek'] = pd.to_datetime(df['time']).dt.dayofweek\n        return df\n\n    \n# Create instance for global usage\nmarket_prepro = MarketPrepro()\nmarket_prepro.fit(train_idx, market)\nprint('market_prepro is fit')\n#prepro.get_X(train_idx.sample(10), market, news).head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d8ea2a3998e93f5a2f8d2c4a6c7aef7de737bc75"},"cell_type":"markdown","source":"### News preprocessor\nPrepare news batch for generator.\nAsset can have many news per day, so group them by asset, day and aggregate. Then normalize numerical values. News aggregation part is based on this kernel: https://www.kaggle.com/bguberfain/a-simple-model-using-the-market-and-news-data#"},{"metadata":{"trusted":true,"_uuid":"725ee92df319ffdba9d5db27ac68e08860f67f4a"},"cell_type":"code","source":"class NewsPrepro:\n    \"\"\"\n    Aggregate news by day and asset. Normalize numeric values.\n    \"\"\"\n    news_cols_numeric = ['urgency', 'takeSequence', 'wordCount', 'sentenceCount', 'companyCount',\n                         'marketCommentary', 'relevance', 'sentimentNegative', 'sentimentNeutral',\n                         'sentimentPositive', 'sentimentWordCount', 'noveltyCount12H', 'noveltyCount24H',\n                         'noveltyCount3D', 'noveltyCount5D', 'noveltyCount7D', 'volumeCounts12H',\n                         'volumeCounts24H', 'volumeCounts3D', 'volumeCounts5D', 'volumeCounts7D']\n\n    feature_cols = news_cols_numeric\n\n    def fit(self, idx, news):\n        \"\"\"\n        Fit preprocessing scalers, encoders on given train df.\n        @param idx: index with time, assetCode\n        \"\"\"\n        # Save indices[assetCode, time, news_index] for all news\n        self.all_news_idx = self.news_idx(news)\n\n        # Get news only related to market idx\n        news_idx = idx.merge(self.all_news_idx, on=['assetCode', 'time'], suffixes=['_idx', ''])[\n            ['news_index', 'assetCode', 'time']]\n        news_train_df = news_idx.merge(news, left_on='news_index', right_index=True, suffixes=['_idx', ''])[\n            self.news_cols_numeric]\n\n        # Numeric data normalization\n        self.numeric_scaler = StandardScaler()\n        news_train_df.fillna(0, inplace=True)\n\n        # Fit scaler\n        self.numeric_scaler.fit(news_train_df)\n\n    def get_X(self, idx, news):\n        \"\"\"\n        Preprocess news for asset code and time from given index\n        \"\"\"\n        news_idx = idx.merge(self.all_news_idx, on=['assetCode', 'time'], suffixes=['_idx', ''])[\n            ['news_index', 'assetCode', 'time']]\n        news_df = news_idx.merge(news, left_on='news_index', right_index=True, suffixes=['_idx', ''])[\n            ['time', 'assetCode'] + self.news_cols_numeric]\n        news_df = self.aggregate_news(news_df)\n\n        return self.safe_fix(news_df)\n\n    def safe_fix(self, news_df):\n        \"\"\"\n        Scale, fillna\n        \"\"\"\n        # Normalize, fillna etc without removing rows.\n        news_df.fillna(0, inplace=True)\n        if not news_df.empty:\n            news_df[self.news_cols_numeric] = self.numeric_scaler.transform(news_df[self.news_cols_numeric])\n        return news_df\n\n    def news_idx(self, news):\n        \"\"\"\n        Get asset code, time -> news id\n        :param news:\n        :return:\n        \"\"\"\n\n        # Fix asset codes (str -> list)\n        asset_codes_list = news['assetCodes'].str.findall(f\"'([\\w\\./]+)'\")\n\n        # Expand assetCodes\n        assetCodes_expanded = list(chain(*asset_codes_list))\n\n        assetCodes_index = news.index.repeat(asset_codes_list.apply(len))\n        assert len(assetCodes_index) == len(assetCodes_expanded)\n        df_assetCodes = pd.DataFrame({'news_index': assetCodes_index, 'assetCode': assetCodes_expanded})\n\n        # Create expanded news (will repeat every assetCodes' row)\n        #        df_expanded = pd.merge(df_assetCodes, news, left_on='level_0', right_index=True)\n        df_expanded = pd.merge(df_assetCodes, news[['time']], left_on='news_index', right_index=True)\n        # df_expanded = df_expanded[['time', 'assetCode'] + self.news_cols_numeric].groupby(['time', 'assetCode']).mean()\n\n        return df_expanded\n\n    def with_asset_code(self, news):\n        \"\"\"\n        Update news index to be time, assetCode\n        :param news:\n        :return:\n        \"\"\"\n        if news.empty:\n            if 'assetCode' not in news.columns:\n                news.columns = news.columns + 'assetCode'\n            return news\n\n        # Fix asset codes (str -> list)\n        news['assetCodesList'] = news['assetCodes'].str.findall(f\"'([\\w\\./]+)'\")\n\n        # Expand assetCodes\n        assetCodes_expanded = list(chain(*news['assetCodesList']))\n\n        assetCodes_index = news.index.repeat(news['assetCodesList'].apply(len))\n        assert len(assetCodes_index) == len(assetCodes_expanded)\n        df_assetCodes = pd.DataFrame({'level_0': assetCodes_index, 'assetCode': assetCodes_expanded})\n\n        # Create expanded news (will repeat every assetCodes' row)\n        #        df_expanded = pd.merge(df_assetCodes, news, left_on='level_0', right_index=True)\n        df_expanded = pd.merge(df_assetCodes, news, left_on='level_0', right_index=True)\n        df_expanded = df_expanded[['time', 'assetCode'] + self.news_cols_numeric].groupby(['time', 'assetCode']).mean()\n\n        return df_expanded\n\n    def aggregate_news(self, df):\n        \"\"\"\n        News are rare for an asset. We get mean value for 10 days\n        :param df:\n        :return:\n        \"\"\"\n        if df.empty:\n            return df\n\n        # News are rare for the asset, so aggregate them by rolling period say 10 days\n        rolling_days = 10\n        df_aggregated = df.groupby(['assetCode', 'time']).mean().reset_index(['assetCode', 'time'])\n        df_aggregated = df_aggregated.groupby('assetCode') \\\n            .rolling(rolling_days, on='time') \\\n            .apply(np.mean, raw=False) \\\n            .reset_index('assetCode')\n        #df_aggregated.set_index(['time', 'assetCode'], inplace=True)\n        return df_aggregated\n    \n\n# Create instance for global usage\nnews_prepro = NewsPrepro()\nnews_prepro.fit(train_idx, news)\nprint('news_prepro is fit')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7a3fa368ce3db72952421561b00ce1bbc000f3d5"},"cell_type":"markdown","source":"## Join market and news\nGenerator, tests and submission will call this facade to request joined market&news data."},{"metadata":{"trusted":true,"_uuid":"0b84e6f57dba7ed270b14ec28888d9d537c0806a"},"cell_type":"code","source":"class JoinedPreprocessor:\n    \"\"\"\n    Join market with news and preprocess\n    \"\"\"\n\n    def __init__(self, market_prepro, news_prepro):\n        self.market_prepro = market_prepro\n        self.news_prepro = news_prepro\n\n    def get_X(self, market, news):\n        \"\"\"\n        Returns preprocessed market + news\n        :return: X\n        \"\"\"\n        # Market row\n        market_X = self.market_prepro.get_X(market)\n        # One row in news contains many asset codes. Extend it to news_X with one asset code - one row\n        news_idx = self.news_prepro.news_idx(news)\n        news_X = self.news_prepro.get_X(news_idx, news)\n        #news_X.time = news_X.time.astype('datetime64')\n        # X = market X + news X\n        X = market_X.merge(news_X, how='left', on=['time', 'assetCode'], left_index=True)\n        X = X.fillna(0)\n        X = X[self.market_prepro.feature_cols + self.news_prepro.feature_cols]\n        return X\n\n    def get_Xy(self, idx, market, news, is_train=False, is_raw_y=False):\n        \"\"\"\n        Returns preprocessed features and labels for given indices\n        \"\"\"\n        # Get market data for index\n        market_df = market.loc[idx.index]\n        # We can remove bad data in train\n        if is_train:\n            market_df = self.market_prepro.fix_train(market_df)\n        market_Xy = self.market_prepro.get_X(market_df)\n        # Get news data for index\n        news_X = self.news_prepro.get_X(idx, news)\n        #news_X.time = pd.to_datetime(news_X.time, utc=True)\n        #news_X.time = news_X.time.astype('datetime64')\n        # Merge and return\n        Xy = market_Xy.merge(news_X, how='left', on=['time', 'assetCode'], left_index=True)\n        Xy = Xy.fillna(0)\n        X = Xy[self.market_prepro.feature_cols + self.news_prepro.feature_cols]\n        y = self.market_prepro.get_y(market_df, is_raw_y)\n\n        return X, y\n\n    def with_look_back(self, X, y, look_back, look_back_step):\n        \"\"\"\n        Add look back window values to prepare dataset for LSTM\n        \"\"\"\n        look_back_fixed = look_back_step * (look_back // look_back_step)\n        # Fill look_back rows before first\n        first_xrow = X.values[0]\n        first_xrow.shape = [1, X.values.shape[1]]\n        first_xrows = np.repeat(first_xrow, look_back_fixed, axis=0)\n        X_values = np.append(first_xrows, X.values, axis=0)\n\n        if y is not None:\n            first_yrow = y.values[0]\n            first_yrow.shape = [1, y.values.shape[1]]\n            first_yrows = np.repeat(first_yrow, look_back_fixed, axis=0)\n            y_values = np.append(first_yrows, y.values, axis=0)\n\n        # for i in range(0, len(X) - look_back + 1):\n        X_processed = []\n        y_processed = []\n        for i in range(look_back_fixed , len(X_values)):\n            # Add lookback to X\n            x_window = X_values[i - (look_back_fixed//look_back_step)*look_back_step:i+1:look_back_step, :]\n            X_processed.append(x_window)\n            # If input is X only, we'll not output y\n            if y is None:\n                continue\n            # Add lookback to y\n            y_window = y_values[i - (look_back_fixed//look_back_step)*look_back_step:i+1:look_back_step, :]\n            y_processed.append(y_window)\n        # Return Xy for train/test or X for prediction\n        if y is not None:\n            #return np.array(X_processed), np.array(y_processed)\n            return np.array(X_processed), y.values\n        else:\n            return np.array(X_processed)\n\n        \nprepro = JoinedPreprocessor(market_prepro, news_prepro)    \nprint('Preprocessor created')\n#Market and news preprocessor instance\n#prepro = JoinedPreprocessor(market_prepro, news_prepro)\n# prepro.fit(train_idx, market, news)\n# print('Preprocessor is fit')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5ba0bad0c479fb66301530eb12f288d926b10c02"},"cell_type":"markdown","source":"## Data generator\nKeras standard approach to generate batches for **model.fit_generator()**. \nGet market and news data together here.\n\n**Opened question:** how to better organize the data?\nIdeally we could have one trained model per asset. But there are more than 3K assets - no resources for such a big train. There also are new unseen assets in future data. Still have no clear idea how to handle this.\n\n**Current implementation: ** Each generated batch contains the data for one provider.\n\n"},{"metadata":{"trusted":true,"_uuid":"fe9e4d1c83f80d45cb7ef9baa4e3890ddf31f7e8"},"cell_type":"code","source":"class JoinedGenerator:\n    \"\"\"\n    Keras standard approach to generage batches for model.fit_generator() call.\n    \"\"\"\n\n    def __init__(self, prepro, idx, market, news):\n        \"\"\"\n        @param preprocessor: market and news join preprocessor\n        @param market: full loaded market df\n        @param news: full loaded news df\n        @param index_df: df with assetCode and time of train or validation market data. Batches will be taken from them.\n        \"\"\"\n        self.market = market\n        self.prepro = prepro\n        self.news = news\n        self.idx = idx\n\n    def flow_lstm(self, batch_size, is_train, look_back, look_back_step):\n        \"\"\"\n        Generate batch data for LSTM NN\n        Each cycle in a loop we yield a batch for one training step in epoch.\n        \"\"\"\n        while True:\n            # Get market indices of random assets, sorted by assetCode, time.\n            batch_idx = self.get_random_assets_idx(batch_size)\n\n            # Get X, y data for this batch, containing market and news, but without look back yet\n            X, y = self.prepro.get_Xy(batch_idx, self.market, self.news, is_train)\n            # Add look back data to X, y\n            X, y = self.prepro.with_look_back(X, y, look_back, look_back_step)\n            yield X, y\n\n    def get_random_assets_idx(self, batch_size):\n        \"\"\"\n        Get random asset and it's last market data indices.\n        Repeat for next asset until we reach batch_size.\n        \"\"\"\n        asset_codes = self.idx['assetCode'].unique().tolist()\n\n        # Insert first asset\n        asset = np.random.choice(asset_codes)\n        asset_codes.remove(asset)\n        #asset = 'ADBE.O'\n        batch_index_df = self.idx[self.idx.assetCode == asset].tail(batch_size)\n\n        return batch_index_df.sort_values(by=['assetCode', 'time'])\n        \n\n# Train data generator instance\njoin_generator = JoinedGenerator(prepro, train_idx, market, news)\n\n# Validation data generator instance\nval_generator = JoinedGenerator(prepro, val_idx, market, news)\nprint('Generators created')\n\n# X,y=next(join_generator.flow_lstm(20,True,10,2))\n# print(X[:1])\n# print(y[:1])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f6f84645cab4bf1b01f0fe01fc68e6817f83f73e"},"cell_type":"markdown","source":""},{"metadata":{"_uuid":"a3b7fafdd69852eac0ad94501adfe87ef4a48e7b"},"cell_type":"markdown","source":"# 5. Base LSTM model for market and news\n"},{"metadata":{"_uuid":"9ae61753db796dbbbcbbce536ade22008c7e33e4"},"cell_type":"markdown","source":"## Define the model"},{"metadata":{"trusted":true,"_uuid":"7eae329a820e1614958e031fccc9e47c065d4abd","scrolled":true},"cell_type":"code","source":"class ModelFactory:\n    \"\"\"\n    Generate different models. Actually only one of them is used in the kernel,\n    this factory is for experiments when debugging.\n    \"\"\"\n    # LSTM look back window size\n    look_back=90\n    # In windows size look back each look_back_step days\n    look_back_step=10\n\n    def lstm_128(input_size):\n        model = Sequential()\n        # Add an input layer market + news\n        #input_size = len(market_prepro.feature_cols) + len(news_prepro.feature_cols)\n        # input_shape=(timesteps, input features)\n        model.add(LSTM(units=128, return_sequences=True, input_shape=(None,input_size)))\n        model.add(LSTM(units=64, return_sequences=True ))\n        model.add(LSTM(units=32, return_sequences=False))\n\n        # Add an output layer\n        model.add(Dense(1, activation='sigmoid'))\n        model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n\n        return(model)\n\n    def train(model, toy, join_generator, val_generator):\n        weights_file='best_weights.h5'\n\n        # We'll stop training if no improvement after some epochs\n        earlystopper = EarlyStopping(patience=5, verbose=1)\n\n        # Low, avg and high scor training will be saved here\n        # Save the best model during the traning\n        checkpointer = ModelCheckpoint(weights_file\n                                       #,monitor='val_acc'\n                                       ,verbose=1\n                                       ,save_best_only=True\n                                       ,save_weights_only=True)\n\n        #reduce_lr = ReduceLROnPlateau(factor=0.2, patience=3, min_lr=0.001)\n        reduce_lr = ReduceLROnPlateau(factor=0.1, patience=2, min_lr=0.001)\n\n        # Set fit parameters\n        # Rule of thumb: steps_per_epoch = TotalTrainingSamples / TrainingBatchSize\n        #                validation_steps = TotalvalidationSamples / ValidationBatchSize\n        if toy:\n            batch_size=1000\n            validation_batch_size=1000\n            steps_per_epoch=5\n            validation_steps=2\n            epochs=3\n            look_back=10\n            look_back_step=2\n        else:\n            batch_size=1000\n            validation_batch_size=1000\n            steps_per_epoch=20\n            validation_steps=5\n            epochs=20\n            look_back=90\n            look_back_step=10\n\n        print(f'Toy:{toy}, epochs:{epochs}, steps per epoch: {steps_per_epoch}, validation steps:{validation_steps}')\n        print(f'Batch_size:{batch_size}, validation batch size:{validation_batch_size}')\n\n        # Fit\n        training = model.fit_generator(join_generator.flow_lstm(batch_size=batch_size\n                                                                , is_train=True\n                                                                , look_back=look_back\n                                                                , look_back_step=look_back_step)\n                                       , epochs=epochs\n                                       , validation_data=val_generator.flow_lstm(batch_size=validation_batch_size\n                                                                                 , is_train=False\n                                                                                 , look_back=look_back\n                                                                                 , look_back_step=look_back_step)\n                                       , steps_per_epoch=steps_per_epoch\n                                       , validation_steps=validation_steps\n                                       , callbacks=[earlystopper, checkpointer, reduce_lr])\n        # Load best weights saved\n        model.load_weights(weights_file)\n        return training\n\n\nmodel = ModelFactory.lstm_128(len(market_prepro.feature_cols) + len(news_prepro.feature_cols))\nmodel.summary()\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a06ff01bdad074a86dce8a3ef685f15eac017f4c"},"cell_type":"markdown","source":"## Train market and news model"},{"metadata":{"trusted":true,"_uuid":"e4e5814f7f8cbd1591c99e7580d16072ebf93961"},"cell_type":"code","source":"training = ModelFactory.train(model, toy, join_generator, val_generator)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"656c0f64cecd204f5fb607576ff120c9f76ce4fb"},"cell_type":"markdown","source":"## Prediction function\n We have 2 cases for preprocessing and prediction - test and new data.\n Preprocessing for new data is different from test - in new data we have no ground truth. We also have one new day in market and news dataframes - no huge data for years and no indices as for test data.\n To make prediction easy, put repeatable calls to prediction class."},{"metadata":{"trusted":true,"_uuid":"576444c01cc726ac5f362bc85e973246da8bef09"},"cell_type":"code","source":"class Predictor:\n    \"\"\"\n    Predict for test data or real prediction\n    \"\"\"\n\n    def __init__(self, prepro, market_prepro, news_prepro, model, look_back, look_back_step):\n        self.prepro = prepro\n        self.market_prepro = market_prepro\n        self.news_prepro = news_prepro\n        self.model = model\n        self.look_back = look_back\n        self.look_back_step = look_back_step\n\n    def predict(self, market, news):\n        \"\"\"\n        Predict from new received market and news data.\n        :return: predicted y\n        \"\"\"\n        X = self.prepro.get_X(market, news)\n        X = self.prepro.with_look_back(X, None, self.look_back, self.look_back_step)\n        y = self.model.predict(X) * 2 - 1\n        return y\n\n    def predict_idx(self, pred_idx, market, news):\n        \"\"\"\n        Predict for test from indices\n        :return:\n            predicted y, ground truth y\n        \"\"\"\n        # Get preprocessed X, y\n        X_test, y_test = self.prepro.get_Xy(pred_idx, market, news, is_train=False, is_raw_y=True)\n        # Add there look back rows for LSTM\n        X_test, y_test = self.prepro.with_look_back(X_test, y_test, look_back=self.look_back,\n                                                    look_back_step=self.look_back_step)\n        y_pred = self.model.predict(X_test) * 2 - 1\n        return y_pred, y_test\n\n    \npredictor = Predictor( prepro, market_prepro, news_prepro, model, ModelFactory.look_back, ModelFactory.look_back_step)    \n# pred_size=100\n# pred_idx = test_idx.tail(pred_size + ModelFactory.look_back)\n# y_pred, y_test = predictor.predict_idx(pred_idx, market,  news)\n# plt.plot(y_pred, marker='.', linestyle='none')\n# plt.plot(y_test, marker='.', linestyle='none')\n# plt.legend([\"pred\",\"test\"])\n# plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8a9aa61e067c39449c36584cc552a2a6b198007a"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8364d16fd63e4ef983573c9f4237e02ad0d004e9"},"cell_type":"markdown","source":"## Evaluate market model\n\n### Loss function by epoch "},{"metadata":{"trusted":true,"_uuid":"cc30491137c941043e2bd82f739c4147e5c1569f"},"cell_type":"code","source":"# # Plotting\n# f, axs = plt.subplots(3,1, sharex=True, figsize=(12,8))\n# # Close price \n# ass_market.close.plot(ax=axs[0])\n# axs[0].set_ylabel(\"Price\")\n\nplt.figure(1, figsize=(8,3))\nplt.subplot(121)\nplt.plot(training.history['loss'])\nplt.plot(training.history['val_loss'])\nplt.title(\"Loss and validation loss\")\nplt.legend([\"Loss\", \"Validation loss\"])\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Loss\")\n\nplt.subplot(122)\nplt.plot(training.history['acc'])\nplt.plot(training.history['val_acc'])\nplt.title(\"Acc and validation acc\")\nplt.legend([\"Acc\", \"Validation acc\"])\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Accuracy\")\nplt.suptitle('Training history', fontsize=16)\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"386c1f3461d9b9b3df18856eceb53dd2ee495579"},"cell_type":"markdown","source":"### Predict on test data"},{"metadata":{"trusted":true,"_uuid":"5f843bc7b234b61837de390722af3f191737c483"},"cell_type":"code","source":"def predict_on_test():\n    # Predict on last test data\n    pred_size=1000\n    pred_idx = test_idx.tail(pred_size + ModelFactory.look_back)\n    y_pred, y_test = predictor.predict_idx(pred_idx, market, news)\n    #market_df = market.loc[pred_idx.index]\n    #y_test = market_df['returnsOpenNextMktres10'].values\n    # Plot\n    ax1 = plt.subplot2grid((2, 2), (0, 0), rowspan=2)\n    ax1.plot(y_test, linestyle='none', marker='.', color='darkblue')\n    ax1.plot(y_pred, linestyle='none', marker='.', color='darkorange')\n    ax1.legend([\"Ground truth\",\"Predicted\"])\n    ax1.set_title(\"Both\")\n    ax1.set_xlabel(\"Epoch\")\n    ax2 = plt.subplot2grid((2, 2), (0, 1), colspan=1,rowspan=1)\n    ax2.plot(y_test, linestyle='none', marker='.', color='darkblue')\n    ax2.set_title(\"Ground truth\")\n    ax3 = plt.subplot2grid((2, 2), (1, 1), colspan=1,rowspan=1)\n    ax3.plot(y_pred, linestyle='none', marker='.', color='darkorange')\n    ax3.set_title(\"Predicted\")\n    plt.tight_layout()\n    plt.show()\n\npredict_on_test()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e34e3ee9448fcbc131830b3277ee830fa5e6ec1b"},"cell_type":"markdown","source":"### Predict on random asset"},{"metadata":{"trusted":true,"_uuid":"180acea47aa0580448e8790bf55b64d8e202013b"},"cell_type":"code","source":"def predict_random_asset():\n    \"\"\"\n    Get random asset from test set, predict on it, plot ground truth and predicted value\n    \"\"\"\n    # Get any asset\n    asset = test_idx['assetCode'].sample(1, random_state=66).values[0]\n    pred_idx = test_idx[test_idx.assetCode == asset]\n    y_pred, y_test = predictor.predict_idx(pred_idx, market, news)\n    # Plot\n    plt.plot(y_test, linestyle='none', marker='.', color='darkblue')\n    plt.plot(y_pred, linestyle='none', marker='.', color='darkorange')\n    plt.xticks(rotation=45)\n    plt.title(asset)\n    plt.legend([\"Ground truth\", \"predicted\"])\n    plt.show()\n    \npredict_random_asset()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"46dd65f9bf556765fc0447e7f57abbbc629d1deb"},"cell_type":"code","source":"def get_score():\n    \"\"\"\n    Calculation of actual metric that is used to calculate final score\n    @param r: returnsOpenNextMktres10\n    @param u: universe\n    where rti is the 10-day market-adjusted leading return for day t for instrument i, and uti is a 0/1 universe variable (see the data description for details) that controls whether a particular asset is included in scoring on a particular day.    \n    \"\"\"\n    # Get test sample to calculate score on\n    pred_idx = test_idx #.sample(10000, random_state=24)\n    y_pred, y_test = predictor.predict_idx(pred_idx, market, news)    \n    look_back=ModelFactory.look_back\n    market_df = market.loc[pred_idx.index]\n    r=market_df['returnsOpenNextMktres10'].values#.values[look_back:]\n    u=market_df['universe'].values#.values[look_back:]\n    confidence=y_pred\n    # calculation of actual metric that is used to calculate final score\n    r = r.clip(-1,1) # get rid of outliers. Where do they come from??\n    x_t_i = confidence.reshape(r.shape) * r * u\n\n    #print(x_t_i.iloc[0])\n    d = (market_df['time'].dt.day).values #[look_back:]\n    data = {'day' : d, 'x_t_i' : x_t_i}\n    df = pd.DataFrame(data)\n    x_t = df.groupby('day').sum().values.flatten()\n    mean = np.mean(x_t)\n    std = np.std(x_t)\n    score = mean / std\n    return score\n    \nprint(f\"Sigma score: {get_score()}\")    \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"46dd65f9bf556765fc0447e7f57abbbc629d1deb","scrolled":true},"cell_type":"code","source":"def calc_acc():\n    # Get X_test, y_test with look back for LSTM\n    pred_idx = test_idx.sample(10000)\n    y_pred, y_test = predictor.predict_idx(pred_idx, market, news)\n    \n    #y_pred = pd.DataFrame(market_prepro.y_scaler.inverse_transform(model.predict(X_test)))\n    print(\"Accuracy: %f\" % accuracy_score(y_test >= 0, y_pred >= 0))\n    #score = get_score(market_df, confidence, market_df.returnsOpenNextMktres10, market_df.universe)\n    print('Predictions size: ', len(y_pred))\n    print('y_test size:', len(y_test))\n     # Show distribution of confidence that will be used as submission\n    plt.hist(y_test, bins='auto', alpha=0.3)\n    plt.hist(y_pred, bins='auto', alpha=0.3, color='darkorange')\n    plt.legend(['Ground truth', 'Predicted'])\n    plt.xlabel(\"Confidence\")\n    plt.ylabel(\"Count\")\n    plt.title(\"predicted confidence\")\n    plt.show()\n\n# Call accuracy calculation and plot    \ncalc_acc()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a1089c3423417fe6d568afa4a9ca6d57dd5f449d"},"cell_type":"markdown","source":"# Submission"},{"metadata":{"trusted":true,"_uuid":"43a3295c06c520b525542183b1049d0a593e0d26"},"cell_type":"code","source":"def make_predictions(market_obs_df, news_obs_df, predictions_template_df):\n    \"\"\"\n    Predict confidence for one day and update predictions_template_df['confidenceValue']\n    @param market_obs_df: market_obs_df returned from env\n    @param predictions_template_df: predictions_template_df returned from env.\n    @return: None. prediction_template_df updated instead. \n    \"\"\"\n    # Predict\n    y_pred = predictor.predict(market_obs_df, news_obs_df)\n    confidence_df=pd.DataFrame(y_pred, columns=['confidence'])\n\n    # Merge predicted confidence to predictions template\n    pred_df = pd.concat([predictions_template_df, confidence_df], axis=1).fillna(0)\n    predictions_template_df.confidenceValue = pred_df.confidence","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"79ed08e77aa14dfb7bd811d2187b0d2059a95052"},"cell_type":"code","source":"##########################\n# Submission code\n\n# Save data here for later debugging on it\ndays_saved_data = []\n\n# Store execution info for plotting later\npredicted_days=[]\npredicted_times=[]\nlast_predictions_template_df = None\n\n# Predict day by day\ndays = env.get_prediction_days()\nlast_year=None\nfor (market_obs_df, news_obs_df, predictions_template_df) in days:\n    # Store the data for later debugging on it\n    days_saved_data.append((market_obs_df, news_obs_df, predictions_template_df))\n    # For later plotting\n    predicted_days.append(market_obs_df.iloc[0].time.strftime('%Y-%m-%d'))\n    time_start = time()\n    # For logging\n    cur_year = market_obs_df.iloc[0].time.strftime('%Y')\n    if cur_year != last_year:\n        print(f'Predicting {cur_year}...')\n        last_year = cur_year\n\n    # Call prediction func\n    make_predictions(market_obs_df, news_obs_df, predictions_template_df)\n    #!!!\n    env.predict(predictions_template_df)\n    \n    # For later plotting\n    last_predictions_template_df = predictions_template_df\n    predicted_times.append(time()-time_start)\n    #print(\"Prediction completed for \", predicted_days[-1])\n    \nprint(f\"Prediction for {len(predicted_days)} days completed\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"38ce296e50b8c15c4dde4c4626e489c474b39179"},"cell_type":"code","source":"# Plot execution time \nsns.barplot(np.array(predicted_days), np.array(predicted_times))\nplt.title(\"Execution time per day\")\nplt.xlabel(\"Day\")\nplt.ylabel(\"Execution time, seconds\")\nplt.show()\n\n# Plot predicted confidence for last day\nlast_predictions_template_df.plot(linestyle='none', marker='.', color='darkorange')\nplt.title(\"Predicted confidence for last observed day: %s\" % predicted_days[-1])\nplt.xlabel(\"Observation No.\")\nplt.ylabel(\"Confidence\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bcbe9c35e83e45f890d2450942fca4a5819b40d5"},"cell_type":"code","source":"# We've got a submission file!\n# !!! Write submission after all days are predicted\nenv.write_submission_file()\nprint([filename for filename in os.listdir('.') if '.csv' in filename])","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}