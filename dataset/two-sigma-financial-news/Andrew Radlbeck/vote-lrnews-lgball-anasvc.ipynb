{"cells":[{"metadata":{"_uuid":"3b334ba5e96ded19f69f84b57f77df1ed2e44239"},"cell_type":"markdown","source":"### A - Market\n### B - News\n### C - Combinational\n### news data needs to be combined with market first to map with date and target.\n### data_processing() outputs a df with combined data first.\n### data_slice() then outputs df_news for B and a cleaned df for C.\n### All missing data are dropped once merged. Because once mapped with data and assetCode, news_df will have lots of missing data.\n### both classifier predict() bin_target [0,1] and the predict_proba() confidence value which can be converted to competition confidence value by predict_proba()[:, 1]*2-1."},{"metadata":{"trusted":true,"_uuid":"6039cc94371e0b283ac4a0061d2baff499e7711b"},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport gc\n\nfrom sklearn import *\nimport time\nfrom lightgbm import LGBMClassifier\nfrom sklearn.linear_model import LogisticRegression\n\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import confusion_matrix\n\nimport matplotlib.pyplot as plt\nfrom mlxtend.evaluate import confusion_matrix\nfrom mlxtend.plotting import plot_confusion_matrix\nimport seaborn as sns\n%matplotlib inline\n\nimport matplotlib as mpl\nmpl.rcParams['axes.titlesize'] = 20\nmpl.rcParams['axes.labelsize'] = 16\nmpl.rcParams['xtick.labelsize'] = 16\nmpl.rcParams['ytick.labelsize'] = 16","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"from kaggle.competitions import twosigmanews\n# You can only call make_env() once, so don't lose it!\nenv = twosigmanews.make_env()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c20fa6deeac9d374c98774abd90bdc76b023ee63"},"cell_type":"code","source":"(market_train_df, news_train_df) = env.get_training_data()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a79fe114f2b3a52d387e93ab69490694ee494602"},"cell_type":"markdown","source":"## Functions for data processing."},{"metadata":{"trusted":true,"_uuid":"2ea139ebe858d5d9baef015335dc93c9e96f89c2"},"cell_type":"code","source":"### Process market data.\ndef market_process(market_train_df):\n    \n    market_train_df['time'] = market_train_df.time.dt.date\n    market_train_df['bartrend'] = market_train_df['close'] / market_train_df['open']\n    market_train_df['average'] = (market_train_df['close'] + market_train_df['open'])/2\n    market_train_df['pricevolume'] = market_train_df['volume'] * market_train_df['close']\n    \n    # drop nans or not?\n    #market_train_df.dropna(axis=0, inplace=True)\n    market_train_df.drop('assetName', axis=1, inplace=True)\n\n    # Set datatype to float32 to save space\n    float_cols = {c: 'float32' for c in market_train_df.columns if c not in ['assetCode', 'time']}\n    \n    return market_train_df.astype(float_cols)\n\n### process news data.\ndef news_process(news_train_df):\n    \n    news_train_df['time'] = news_train_df.time.dt.date\n    news_train_df['position'] = news_train_df['firstMentionSentence'] / news_train_df['sentenceCount']\n    news_train_df['coverage'] = news_train_df['sentimentWordCount'] / news_train_df['wordCount']\n    droplist_for_now = ['sourceTimestamp','firstCreated','subjects','audiences','headline','assetName']\n    news_train_df.drop(droplist_for_now, axis=1, inplace=True)\n    \n    # factorize the following three\n    for col in ['headlineTag', 'provider', 'sourceId', 'marketCommentary']:\n        news_train_df[col], uniques = pd.factorize(news_train_df[col])\n        del uniques\n    \n    # Remove {} and '' from assetCodes column\n    news_train_df['assetCodes'] = news_train_df['assetCodes'].apply(lambda x: x[1:-1].replace(\"'\", \"\"))\n    return news_train_df\n\n## Unstack assetCodes.\ndef unstack_asset_codes(news_train_df):\n    codes = []\n    indexes = []\n    for i, values in news_train_df['assetCodes'].iteritems():\n        explode = values.split(\", \")\n        codes.extend(explode)\n        repeat_index = [int(i)]*len(explode)\n        indexes.extend(repeat_index)\n    index_df = pd.DataFrame({'news_index': indexes, 'assetCode': codes})\n    del codes, indexes\n    gc.collect()\n    return index_df\n\n## Merge news on index\ndef merge_news_on_index(news_train_df, index_df):\n    news_train_df['news_index'] = news_train_df.index.copy()\n\n    # Merge news on unstacked assets\n    news_unstack_df = index_df.merge(news_train_df, how='left', on='news_index')\n    news_unstack_df.drop(['news_index', 'assetCodes'], axis=1, inplace=True)\n    return news_unstack_df\n\n## Comine multiple news reports for same assets on same day.\ndef group_news(news_frame):\n    \n    aggregations = ['mean']\n    gp = news_frame.groupby(['assetCode', 'time']).agg(aggregations)\n    gp.columns = pd.Index([\"{}_{}\".format(e[0], e[1]) for e in gp.columns.tolist()])\n    gp.reset_index(inplace=True)\n    # Set datatype to float32\n    float_cols = {c: 'float32' for c in gp.columns if c not in ['assetCode', 'time']}\n    return gp.astype(float_cols)\n\n### Merge market and news data\ndef merge(market_train_df,news_agg_df):\n    \n    df = market_train_df.merge(news_agg_df, how='left', on=['time','assetCode'])\n    # drop nans or not?\n    #df.dropna(axis=0, inplace=True)\n    \n    del market_train_df, news_agg_df\n    return df\n\n######################################################\n\ndef data_processing(market_train_df, news_train_df):\n    ## Market\n    market_train_df = market_process(market_train_df)\n    print(\"Market data shape: \", market_train_df.shape)\n    \n    ## News\n    news_train_df = news_process(news_train_df)\n    index_df = unstack_asset_codes(news_train_df)\n    news_unstack_df = merge_news_on_index(news_train_df, index_df)\n    del news_train_df, index_df\n    news_agg_df = group_news(news_unstack_df)\n    del news_unstack_df\n    print('News data shape: ', news_agg_df.shape)\n          \n    ## Merge\n    df = merge(market_train_df,news_agg_df)\n    print('Merged shape: ', df.shape)\n    \n    df.dropna(axis=0, inplace=True)\n    print('wo missing shape: ', df.shape)\n    \n    gc.collect()\n    return df","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2712c731e6b5a4c7f3c0b54b9950187b1b1d4778"},"cell_type":"markdown","source":"### Functions for A"},{"metadata":{"trusted":true,"_uuid":"69b064ca9247da8da10ef49a11f18dd8a8369876"},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import minmax_scale\nfrom sklearn import svm\n\ndef analysis_get_features(market_data, byday=False, trainInfo=None):\n    # for full training set feature creation\n    if(not byday):\n        # assign uids to each asset \n        '''\n        uAssestCode = pd.unique(market_data.assetCode)    \n        uidList     = np.linspace(1.0, uAssestCode.shape[0], num=uAssestCode.shape[0])\n\n        # feature 0 - map from assetCode to uid    \n        uidMap = {}\n        for A, B in zip(uAssestCode, uidList):\n            uidMap[A] = B\n\n        aUID = np.zeros(market_data.shape[0])\n        for i, item in enumerate(market_data.assetCode):\n            aUID[i] = uidMap[item]\n        '''\n        # feature 1, 2 - gain, gainb    \n        gain  = market_data.close - market_data.open    \n        gainb = np.zeros(gain.shape[0])\n        # classify\n        gainb[gain > 0] = 1\n\n        # feature 3 - volumeb\n        v   = market_data.volume\n        npv = np.array(v)    \n        vbins, ved = np.histogram(v, bins=20)\n        volumeb    = np.zeros(v.shape[0])\n\n        # create classes for bins\n        for i in range(1, ved.shape[0] - 1): \n            volumeb[np.logical_and(ved[i] < npv, npv < ved[i+1])] = i\n\n        # features to dataframe\n        #Xdict = {1: aUID, 2: gain, 3: gainb, 4: volumeb}\n        Xdict = {1: gain, 2: gainb, 3: volumeb}\n        X     = pd.DataFrame(Xdict)\n        \n        # save off training information\n        trainInfo = (ved)\n        \n    # for one off feature creation\n    else:                \n        # feature 0\n        '''\n        auid = np.zeros(market_data.assetCode.shape[0])\n        for i, assetCode in enumerate(market_data.assetCode):\n            # look for uid\n            if assetCode in trainInfo[1]:\n                uid = trainInfo[1][assetCode]\n            else:\n                # if its a new asset code create a new uid\n                newUID = trainInfo[0].max() + 1\n                np.append(trainInfo[0], newUID)\n                \n                # update dict\n                trainInfo[1][assetCode] = newUID\n                uid = newUID\n                \n            # set uid\n            auid[i] = uid\n        '''\n        # feature 1, 2 - gain, gainb\n        gain  = market_data.close - market_data.open    \n        gainb = np.zeros(gain.shape[0])\n        # classify\n        gainb[gain > 0] = 1\n        \n        # feature 3 - volumeb\n        v   = market_data.volume\n        npv = np.array(v)    \n        # TODO consider using the same bin alignment as the training data\n        # it may be better to leave it as-is; it would be proportionate\n        # ved = trainInfo[2][i]\n        vbins, ved = np.histogram(v, bins=20)\n        volumeb    = np.zeros(v.shape[0])\n\n        # create classes for bins\n        for i in range(1, ved.shape[0] - 1): \n            volumeb[np.logical_and(ved[i] < npv, npv < ved[i+1])] = i\n                \n        # features to dataframe\n        #Xdict = {1: auid, 2: gain, 3: gainb, 4: volumeb}\n        Xdict = {1: gain, 2: gainb, 3: volumeb}\n        X     = pd.DataFrame(Xdict)\n    \n    return X, trainInfo\n\ndef analysis_train(features, target):\n    # scale y to be max [-1,1] to represent confidence \n    y       = np.zeros(target.shape[0])\n    #y_scale = minmax_scale(list(target), feature_range=(-1, 1), axis=0, copy=True)\n    y[target >  1e-3] = 1\n    y[target < -1e-3] = -1\n\n    # implement SVM regression\n    clf = svm.LinearSVC()\n    #clf = svm.SVR(C=0.9, kernel='rbf')  \n    #clf = svm.SVR(kernel='linear', C=1e3)   \n    #clf = svm.SVC(gamma='auto')\n    clf.fit(features, y)\n    \n    return clf\n\ndef analysis_predict(market_obs, predictions, trainInfo, trainedModel, toggle=True):    \n    features, trainInfo = analysis_get_features(market_obs, toggle, trainInfo)\n    p       = trainedModel.predict(features)\n    #p_scale = minmax_scale(list(p), feature_range=(-1, 1), axis=0, copy=True)\n    #p_class = np.ones(p_scale.shape[0]) * -1\n    #p_class[p_scale > 0] = 1\n    p_class = p\n    \n    # set\n    predictions.confidenceValue = p_class","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"477ba549b5c9e58e9f5b3fa933d4388c38d399b2"},"cell_type":"markdown","source":"## Data processing.\n### Get df_news and df."},{"metadata":{"trusted":true,"_uuid":"e072e158693c59874ea414487a64db09c7c42769"},"cell_type":"code","source":"df = data_processing(market_train_df, news_train_df)\n\n# extract useful data.\ndates = df.time\nnum_target = df.returnsOpenNextMktres10.astype('float32')\nbin_target = (df.returnsOpenNextMktres10 >= 0).astype('int8')\nuniverse = df.universe.astype('int8')\n\n#Slice out df_news for LR. Clean df for lgb.\ndef data_slice(df):\n    # Drop columns that are not features\n    df.drop(['returnsOpenNextMktres10', 'universe', 'assetCode', 'time'], axis=1, inplace=True)\n    \n    market_column = df.columns.tolist()[:14] #14\n    news_column = df.columns.tolist()[14:] #29\n    \n    # df_news for B.\n    df_news = df[news_column]\n    print('df_news shape: ', df_news.shape)\n    \n    # df for C.\n    drop_list = ['takeSequence_mean','provider_mean','firstMentionSentence_mean',\n                'headlineTag_mean','marketCommentary_mean',\n                'noveltyCount12H_mean','noveltyCount24H_mean','noveltyCount3D_mean','sourceId_mean',\n                'noveltyCount5D_mean','noveltyCount7D_mean','urgency_mean','sentimentClass_mean']\n    df.drop(drop_list, axis=1, inplace=True)\n    print('df shape: ', df.shape)\n    \n    return df_news, df\n\ndf_news, df = data_slice(df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1ebc02b3affc47ed968b4066f03e48adb7d9e874"},"cell_type":"code","source":"df_news.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4d5df16caf0ea13afbeca06e07dcce9eca2f5b29"},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c71a787afc1306f129bbea8dafed5658294b37c5"},"cell_type":"markdown","source":"## Split data for training."},{"metadata":{"trusted":true,"_uuid":"aea498af5dd25013377f99f08fb2a5eac2d41ff0"},"cell_type":"code","source":"# random sample split\ntrain_index, test_index = model_selection.train_test_split(df.index.values, test_size=0.25, \n                                                           random_state = 11)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"dd489ea1039cddb82945c43970795e44f697b3e6"},"cell_type":"markdown","source":"## A - Train LinearSVC classifier for market only.\n\nCustom features are slow so here we create a test mode"},{"metadata":{"trusted":true,"_uuid":"dc446218587d1b0a5562787338fb17cc49185696"},"cell_type":"code","source":"test_mode = False\n\nif(test_mode):\n    split_train_index = 20000\n    split_test_index  = 20000\nelse:\n    split_train_index = train_index.shape[0]\n    split_test_index  = test_index.shape[0]        \n    \ntrain_market_train_df = df.loc[train_index[0:split_train_index]]\n#\ntest_market_train_df = df.loc[test_index[0:split_test_index]]\ntest_target_raw      = bin_target[test_index[0:split_test_index]]\ntest_target_class    = np.ones(test_target_raw.shape[0]) * -1\ntest_target_class[test_target_raw >  0] = 1\n\n# train analytical\nfeatures, trainInfo = analysis_get_features(train_market_train_df)\ntrainedModel_A      = analysis_train(features, bin_target[train_index[0:split_train_index]])\nprint(trainedModel_A)\n\n# predict\n#predictions = {'assetCode': test_market_train_df['assetCode'], 'confidenceValue' : np.zeros(test_market_train_df.shape[0])}\npredictions = {'confidenceValue' : np.zeros(test_market_train_df.shape[0])}\npredictions = pd.DataFrame(predictions)\n\n# get model A predictions\nanalysis_predict(test_market_train_df, predictions, trainInfo, trainedModel_A, False)        \n# save\npredA = predictions['confidenceValue'].values.copy()\npredA = predA.astype('int8')\n\nprint(\"SVC clf accuracy : %f\" % \\\n  accuracy_score(predA,\n                 test_target_raw))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cb2403ef7113794ae3203a9830ea164e211e1290"},"cell_type":"markdown","source":"## B - Train logistic regression classifier for news only.\nIt will not converge even with iteraction=400, so don't bother adding more."},{"metadata":{"trusted":true,"_uuid":"f0605b7ad72513a294369678e31707454650e127"},"cell_type":"code","source":"def train_news_model(df_news):\n    t = time.time()\n    print('Fitting Up')\n    clf = LogisticRegression(solver='sag', max_iter=200, n_jobs=4) # Stochastic Average Gradient: fast\n    clf.fit(df_news.loc[train_index],bin_target.loc[train_index])\n    print('Done')\n    print(f'Done, time = {time.time() - t}')\n    return clf\n\ntrainedModel_B = train_news_model(df_news)\nprint(trainedModel_B)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e6be89e1febed90df405d5a72c961b6a9fe8c89d"},"cell_type":"markdown","source":"## B - LR Evaluation"},{"metadata":{"trusted":true,"_uuid":"32d7e09d42d439b7301473b40697ebc75cbe35f5"},"cell_type":"code","source":"predB = trainedModel_B.predict(df_news.loc[test_index])\n\nprint(\"LR clf accuracy : %f\" % \\\n      accuracy_score(predB,\n                     bin_target.loc[test_index]))\nprint(\"LR clf AUC : %f\" % \\\n      roc_auc_score(bin_target.loc[test_index].values,\n                    trainedModel_B.predict_proba(df_news.loc[test_index])[:, 1]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e82d9c9cdaf77454053846bf9ddf6fec82c68a14"},"cell_type":"code","source":"plt.hist(trainedModel_B.predict_proba(df_news.loc[test_index])[:, 1]*2-1, \n         bins='auto', alpha=0.3, color='darkorange')\n#plt.legend(['Ground truth', 'Predicted'])\nplt.xlabel(\"Confidence\")\nplt.ylabel(\"Count\")\nplt.title(\"predicted confidence\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3e7d99f7d553b1f2e6e84336da141ef614fdb4ca"},"cell_type":"code","source":"cfm = confusion_matrix(y_target=np.array(bin_target.loc[test_index]), \n                       y_predicted=trainedModel_B.predict(df_news.loc[test_index]).tolist())\nfig, ax = plot_confusion_matrix(conf_mat=cfm)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a61ffc90bc2d1a3a9bd7f8353dec05faa34faa8e"},"cell_type":"markdown","source":"## C - Train lgb classifier for market+news combo."},{"metadata":{"trusted":true,"_uuid":"da818b8742cf6c2fc48e2f11aa565a1938c65453"},"cell_type":"code","source":"def train_combo_model(df):\n    ## best parameters for lgb.\n    lgb = LGBMClassifier(\n        objective='binary',\n        boosting='gbdt',\n        learning_rate = 0.05,\n        max_depth = 8,\n        num_leaves = 80,\n        n_estimators = 400,\n        bagging_fraction = 0.8,\n        feature_fraction = 0.9)\n\n    t = time.time()\n    print('Fitting Up')\n    lgb.fit(df.loc[train_index],bin_target.loc[train_index])\n    print('Done')\n    print(f'Done, time = {time.time() - t}')\n    return lgb\n\ntrainedModel_C = train_combo_model(df)\nprint(trainedModel_C)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"45926a64e07e6f31291e0e3810b82a64b917abde"},"cell_type":"code","source":"predC = trainedModel_C.predict(df.loc[test_index])\n\nprint(\"lgb accuracy : %f\" % \\\n      accuracy_score(predC,\n                     bin_target.loc[test_index]))\nprint(\"lgb AUC : %f\" % \\\n      roc_auc_score(bin_target.loc[test_index].values,\n                    trainedModel_C.predict_proba(df.loc[test_index])[:, 1]))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"077ca7ac7e144d167c5e55074216324393227677"},"cell_type":"markdown","source":"## Vote with all model types"},{"metadata":{"trusted":true,"_uuid":"688a3cc94ad135287bdd9b549a7859dfeb2d7db9"},"cell_type":"code","source":"# lets add some weighting to C as we know its our best model\n# A, B = 2 / 7 : C = 3 / 7\npred  = np.array([predA[0:split_test_index], predA[0:split_test_index], \n                  predB[0:split_test_index], predB[0:split_test_index], \n                  predC[0:split_test_index], predC[0:split_test_index], predC[0:split_test_index]\n                 ])\ntally = np.apply_along_axis(np.bincount, axis=0, arr=pred, minlength = np.max(pred) +1)\nvote  = np.apply_along_axis(np.argmax, axis=0, arr= tally)\n\ntest = bin_target.loc[test_index[0:split_test_index]]\n\nprint(\"total accuracy : %f\" % \\\n      accuracy_score(vote,\n                     test))\n\ncfm = confusion_matrix(y_target=np.array(test), \n                       y_predicted=vote)\nfig, ax = plot_confusion_matrix(conf_mat=cfm)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5ecfc9e6fd0ff060652eb2c378e952c1d359f2b4"},"cell_type":"markdown","source":"# The End"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}