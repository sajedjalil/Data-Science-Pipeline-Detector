{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\nimport matplotlib.pyplot as plt\n\nimport datetime \n\nimport sklearn # ML\nfrom kaggle.competitions import twosigmanews\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"# Retreive the environment of the competition\n# You can only call make_env() once, so don't lose it!\nenv = twosigmanews.make_env()\nprint('Data loaded!')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8df93b966bdb40b3a559eadc5d8c0cfd4f4a0ed5"},"cell_type":"code","source":"# Retrieve all training data\n(market_train_df, news_train_df) = env.get_training_data()\nprint(\"Fetching training data finished... \")\nprint('Data obtained!')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"99de5d73c3d49783c4c930c2fb3e7cf901ef0405"},"cell_type":"code","source":"# Market data analysis\n# Types of the columns\nprint(market_train_df.dtypes)\nmarket_train_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"463ee7d656d8a0712db41a4a5cdb7d4fcfd325f5"},"cell_type":"code","source":"# Correlation between the numericals (except universe)\n# Note that this removes the null values from the computation\nmarket_train_df.iloc[:, 3:].corr(method='pearson')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"003ececb5a7bf02859f91e82b9f479f3aba3aa7b"},"cell_type":"code","source":"# Lets analyze further the target variable\n# Very big outliers, lets see their number and distribution\nfig, axes = plt.subplots(3,2, figsize=(20, 12)) # create figure and axes\nprint(\"# Rows with |value| > 1 =\", market_train_df[market_train_df[\"returnsOpenNextMktres10\"].abs()>1].shape[0])\nprint(\"# Rows with |value| > 0.5 =\", market_train_df[market_train_df[\"returnsOpenNextMktres10\"].abs()>0.5].shape[0])\nprint(\"# Rows with |value| > 0.25 =\", market_train_df[market_train_df[\"returnsOpenNextMktres10\"].abs()>0.25].shape[0])\nprint(\"# Rows with |value| > 0.1 =\", market_train_df[market_train_df[\"returnsOpenNextMktres10\"].abs()>0.1].shape[0])\n\n# Boxplot with all values\nmarket_train_df.boxplot(column=\"returnsOpenNextMktres10\", ax=axes.flatten()[0])\naxes.flatten()[0].set_xlabel('Boxplot with all values', fontsize=18)\n# Removing rows with outliers (bigger or smaller than 1)\nmarket_train_df[market_train_df[\"returnsOpenNextMktres10\"].abs()<1].boxplot(column=\"returnsOpenNextMktres10\", ax=axes.flatten()[1])\naxes.flatten()[1].set_xlabel('Boxplot with values such that |val| < 1', fontsize=18)\n# Removing rows with outliers (bigger or smaller than 0.5)\nmarket_train_df[market_train_df[\"returnsOpenNextMktres10\"].abs()<0.5].boxplot(column=\"returnsOpenNextMktres10\", ax=axes.flatten()[2])\naxes.flatten()[2].set_xlabel('Boxplot with values such that |val| < 0.5', fontsize=18)\n# Removing rows with outliers (bigger or smaller than 0.25)\nmarket_train_df[market_train_df[\"returnsOpenNextMktres10\"].abs()<0.25].boxplot(column=\"returnsOpenNextMktres10\", ax=axes.flatten()[3])\naxes.flatten()[3].set_xlabel('Boxplot with values such that |val| < 0.25', fontsize=18)\n# Removing rows with outliers (bigger or smaller than 0.1)\nmarket_train_df[market_train_df[\"returnsOpenNextMktres10\"].abs()<0.1].boxplot(column=\"returnsOpenNextMktres10\", ax=axes.flatten()[4])\naxes.flatten()[4].set_xlabel('Boxplot with values such that |val| < 0.1', fontsize=18)\n# Distribution of the target value (not including values bigger or smaller than 1)\nmarket_train_df[market_train_df[\"returnsOpenNextMktres10\"].abs()<0.25].hist(column=\"returnsOpenNextMktres10\", bins=100, ax=axes.flatten()[5])\naxes.flatten()[5].set_xlabel('Histogram for values such that |val| < 0.25', fontsize=18)\nprint(\"The variable is actually centered in 0 and only a few outliers higher than 0.1. This makes sense considering that the returns of the \\\nmarket for 10 days are really small. Our goal then should be to detect those times in which the wins or loses are really high by making \\\nuse of the news. A good approach for this could be an algorithm to control the small temporal oscilation of the market and then use the news \\\nto detect those imprevisible changes.\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"11843ca6a23a2c5ef9efbf6ec1d434be2174c618"},"cell_type":"code","source":"# Let's analyze null values \n#print(market_train_df.isnull().sum())\nnuls = market_train_df.loc[:,['returnsClosePrevMktres1','returnsOpenPrevMktres1','returnsClosePrevMktres10','returnsOpenPrevMktres10']]\nnuls\n#print (nuls)\n#tol = 0.10\n#nuls_r1 = market_train_df.loc[market_train_df['returnsClosePrevMktres1'] <= tol,'returnsClosePrevMktres1']\n#print(nuls_r1, len(nuls_r1), 'Media--> ', nuls_r1.mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4b0fdef91afe8c5dea8392e1754f958bed2b0a87"},"cell_type":"code","source":"# Lets change null values by means without taking into account the possible outliers. \ntol = 0.15\nlista_valores_rellenar = list()\nfor col in nuls:\n    print (nuls[col], \"Media--> \", nuls[col].loc[nuls[col]<= tol].mean())\n    lista_valores_rellenar += [nuls[col].loc[nuls[col]<= tol].mean()]\nlista_valores_rellenar","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9a0d931f2e33c76e96893aed86d31c93dd510e84"},"cell_type":"code","source":"# And now we fill the nulls\nmarket_train_df.loc[pd.Series(market_train_df['returnsClosePrevMktres1'].isnull()), 'returnsClosePrevMktres1'] = lista_valores_rellenar[0]\nmarket_train_df.loc[pd.Series(market_train_df['returnsOpenPrevMktres1'].isnull()), 'returnsOpenPrevMktres1'] = lista_valores_rellenar[1]\nmarket_train_df.loc[pd.Series(market_train_df['returnsClosePrevMktres10'].isnull()), 'returnsClosePrevMktres10'] = lista_valores_rellenar[2]\nmarket_train_df.loc[pd.Series(market_train_df['returnsOpenPrevMktres10'].isnull()), 'returnsOpenPrevMktres10'] = lista_valores_rellenar[3]\nprint(lista_valores_rellenar)\n\n# Did it work?\nprint(market_train_df.isnull().sum())\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ec2bf079be77ffdd2cdf150392cae35826d8ef4f"},"cell_type":"code","source":"# Let us remove some not important columns.\n\nnews_train_df = news_train_df.drop(['sourceTimestamp','firstCreated'], axis = 1)\n#market_train_df = market_train_df.drop(['daily_diff'], axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"eae3b93d108aca2bb510c8c522a07504410f3c20"},"cell_type":"code","source":"#Para dataset market\n#creamos un nuevo campo con el formato que queremos usando strtime\nmarket_train_df['date']=market_train_df['time'].dt.strftime('%Y-%m-%d')\n#esto nos crea un nuevo campo date con el formato que queremos pero con tipo object\n#market_train_df['date'].dtypes\n#cambiamos el tipo de dato object a datetime, manteniendo el formato que queremos\nmarket_train_df['date']=pd.to_datetime(market_train_df['date'],  format='%Y/%m/%d')\n#comprobamos qe hemos cambiado el tipo de dato\nprint(market_train_df.dtypes)\n#mostramos resultados del nuevo DF\nmarket_train_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b68ae5bb99d2abd04e87ee508a74e19b25f8bcc1"},"cell_type":"code","source":"#Para dataset news\n#creamos un nuevo campo con el formato que queremos usando strtime\nnews_train_df['date']=news_train_df['time'].dt.strftime('%Y-%m-%d')\n#esto nos crea un nuevo campo date con el formato que queremos pero con tipo object\n#market_train_df['date'].dtypes\n#cambiamos el tipo de dato object a datetime, manteniendo el formato que queremos\nnews_train_df['date']=pd.to_datetime(news_train_df['date'],  format='%Y/%m/%d')\n#comprobamos qe hemos cambiado el tipo de dato\nnews_train_df.dtypes\n#mostramos resultados del nuevo DF\nnews_train_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b1537a2685cdfdb67501d28105ed8d931d09c1e3"},"cell_type":"code","source":"# Lets join both datsets. Firstly, we need to do some minor changes on dates. We know that there might be more than 1 new for a given asset in one day.\n#news_train_df['time2'] = news_train_df['time'].astype(str).str.slice(0,10)\n#market_train_df['time2'] = market_train_df['time'].astype(str).str.slice(0,10)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"adb371d9490a71ba4696be96c279a2b57ffa8365"},"cell_type":"code","source":"print(len(news_train_df), len(market_train_df))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4abb16b48b6fa7abb1c2db9828cc5c117b77b7d0"},"cell_type":"code","source":"# As after when we are training we run out of RAM, I make a random sampling of the market dataset. There is no need \n# to do that on the news, since when we do the merge we use the keys of the markets. It would be nice to be sure that\n# we have same type of distributions. \nvalores_mercado = np.random.choice(int(len(market_train_df)/4), int(len(market_train_df)/4), replace=False)\nvalores_news = np.random.choice(int(len(news_train_df)/1), int(len(news_train_df)/1), replace=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cde8a82dad7996ed29dd281d30dcb43d4fa05662"},"cell_type":"code","source":"# We select those rows of the dataframes\nprint(len(market_train_df))\nnews_train_df = news_train_df.iloc[valores_news,:]\nmarket_train_df = market_train_df.iloc[valores_mercado,:]\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b10877d8e9ab0d1fff6cdb9663b08f3824673d53"},"cell_type":"code","source":"# We do the final merge by 2 keys\ndf_final = market_train_df.merge(news_train_df, how = 'left', on = ['assetName', 'date'])\nprint(len(df_final))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"db888237c5aa1348f12b9555623da23b725f5f61"},"cell_type":"code","source":"# Posterior analysis say that the dataset may have many outliers. Most of the dataset is below 0.5 so the rest might be outliers\nprint(len(df_final), len(df_final.loc[abs(df_final['returnsOpenNextMktres10']) <0.5]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8bb959dabba911453902e3a55cc5bc38c16269e7"},"cell_type":"code","source":"df_final.head(20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"958761a864fea913d56f5c0049850cf04f6c30dc"},"cell_type":"code","source":"# Lets study a little bit more the target variable. It is a sharp gaussian!!\n\ndf_final.loc[abs(df_final['returnsOpenNextMktres10']) <0.5, 'returnsOpenNextMktres10'].hist(bins = 1000)\n# We are happy with that number\n\ndf_final = df_final.loc[abs(df_final['returnsOpenNextMktres10']) <0.5]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5b810962e1949e2eaef26a3c1f2c40002ea1316d"},"cell_type":"code","source":"print('KURTOSIS --> ', df_final['returnsOpenNextMktres10'].kurtosis(), 'SKEWNESS --> ', df_final['returnsOpenNextMktres10'].skew())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b032da3067002bbd2281183100eadf307c33b935"},"cell_type":"code","source":"# Lets make a list out of the columns related to novelty and counts.\ndf_final['list_novelties'] = df_final.reset_index()[['noveltyCount12H','noveltyCount24H','noveltyCount3D','noveltyCount5D','noveltyCount7D']].values.tolist()\ndf_final = df_final.drop(['noveltyCount12H','noveltyCount24H','noveltyCount3D','noveltyCount5D','noveltyCount7D'], axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ee44304c1d9d5e73d9873ec852fcdb8e638b282b"},"cell_type":"code","source":"df_final['list_counts']= df_final.reset_index()[['volumeCounts12H','volumeCounts24H','volumeCounts3D','volumeCounts5D','volumeCounts7D']].values.tolist()\ndf_final = df_final.drop(['volumeCounts12H','volumeCounts24H','volumeCounts3D','volumeCounts5D','volumeCounts7D'], axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"521e68a9fa18c331b6247c32144eb22231c775d9"},"cell_type":"code","source":"df_final.head(10)\nlen(df_final)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f1d8cc864b3b4e01d73571103f6e877aac5daee1"},"cell_type":"code","source":"# Lets do the splitting for ML !\nfrom sklearn.model_selection import train_test_split\ndf_final = df_final.dropna()\nX = df_final.loc[:, df_final.columns != 'returnsOpenNextMktres10']\ny = df_final.loc[:, 'returnsOpenNextMktres10']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, shuffle=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c51d1405ddbc8862b6a930ff014b6b197021b5f6"},"cell_type":"code","source":"# We need to delete datafrmes to avoid memory problems\ndel ( market_train_df, news_train_df, df_final)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"08c3b7c34f022bf3a67fa51262c2c796c1f192df"},"cell_type":"code","source":"print(len(X_train),X_train.isnull().sum(), type(X_train))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e5b5f57511b26409047611030b235c4a2cea5ee8"},"cell_type":"code","source":"# We delete nans from our datasets. THey appear after the join\nprint(len(X_train), len(X_test),len(y_train),len(y_test))\nX_train, X_test, y_train, y_test = X_train.dropna(), X_test.dropna(), y_train.dropna(), y_test.dropna() \nprint(len(X_train), len(X_test),len(y_train),len(y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"30d93134e8b850f9b96960861ac40a3cdcdbddd0"},"cell_type":"code","source":"y_train.head()\nprint(len(y_train))\nprint('KURTOSIS NO OUT--> ', y_train.loc[abs(y_train) <0.5].kurtosis(), 'SKEWNESS NO OUT--> ', y_train.loc[abs(y_train) <0.5].skew())\nprint('KURTOSIS --> ', y_train.kurtosis(), 'SKEWNESS--> ', y_train.skew())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0efcdfc456c8f2d864107f18732323a19615a56e"},"cell_type":"code","source":"print(X_train.dtypes)\nX_train.head(6)\n# Pandas already does this for us!!!!\n#cols_cod = []\n#for i in X_train:\n #   print (type(X_train[i]))\n    #if type(i) != 'float64':\n     #   cols_cod += [i]\n#print(cols_cod)\n#from sklearn.preprocessing import OneHotEncoder\n#enc =  OneHotEncoder(handle_unknown='ignore')\n#enc.fit(X)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b5b1ce324a9ac99d08e8831bef143c17235c481b"},"cell_type":"code","source":"# For some first train tests we just use the numerical data, we need to think about what we should do with the strings.\n\nx_train = X_train.select_dtypes(['float32', 'float64'])\nx_train\nx_test = X_test.select_dtypes(['float32', 'float64'])\nx_train.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6c16b5361eefbcf80808ea70a5736217c14b3816"},"cell_type":"code","source":"# Let's make a function than standarizes numerical values.\ndef normalizer(df):\n    for col in df:\n        df[col] = df[col] / df[col].max()\n    return df\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f6ba6ddcc6900f87ad28f6fb641dc88cb24888b1"},"cell_type":"code","source":"print(len(x_train), len(y_train))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3cf7969603c2152f6b8089963fe0fd94b06e395b"},"cell_type":"code","source":"x_train.head(160)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e450f52029b1a508561945eb4fe335aa4ea6b0f0"},"cell_type":"code","source":"# x_train and y_Train podrian no tener l misma longitud\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn import linear_model\nreg = linear_model.LinearRegression()\nreg.fit(x_train, y_train)\ny_predicted=reg.predict(x_test)\nprint(\"Acab贸\")\nmean_absolute_error(y_test, y_predicted)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d8f107a43c295ae95aa3aca7b6b2a8b637fd808a"},"cell_type":"code","source":"# Let us try with the standarize dataframes\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn import linear_model\nreg1 = linear_model.LinearRegression()\nreg1.fit(normalizer(x_train), y_train)\ny_predicted=reg1.predict(normalizer(x_test))\nprint(\"Acab贸\")\nmean_absolute_error(y_test, y_predicted)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ebf4a8867ae9c3d86594f375ff92552a6011d93e"},"cell_type":"code","source":"# WE NEED TO ENCODE DATA. \n#from sklearn.preprocessing import OneHotEncoder\n#enc = OneHotEncoder(handle_unknown='ignore')\n#enc.fit(X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7e135f1c5b9bae35abec1c2deadf22e079962d79"},"cell_type":"code","source":"# Now ML TIME!!\n# PODRIA HABER PROBLEMAS DE QUE LAS LISTAS NO MIDAN LO MISMO!!!\n\n\nfrom sklearn import svm\nclf = svm.SVR()\nclf.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"37a5bb27ef0edbfb6b333f45c35d653fbf4c93a2"},"cell_type":"code","source":"# Predict\nfrom sklearn.ensemble import RandomForestRegressor\nregr = RandomForestRegressor(n_jobs=-1, max_depth=20, random_state=0, n_estimators=20)\nregr.fit(x_train, y_train)\ny_predicted=regr.predict(x_test)\nprint(\"Acab贸\")\nmean_absolute_error(y_test, y_predicted)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a4260668f5ce16ff8f732983ab4c2d4cc906db68"},"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\nregr1 = RandomForestRegressor(n_jobs=-1, max_depth=20, random_state=0, n_estimators=20)\nregr1.fit(normalizer(x_train), y_train)\ny_predicted=regr1.predict(normalizer(x_test))\nprint(\"Acab贸\")\nmean_absolute_error(y_test, y_predicted)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a54aa71992827576e1f29cbdbd531e3692ebfa8d"},"cell_type":"code","source":"#from sklearn.naive_bayes import GaussianNB\n#gnb = GaussianNB()\n#gnb.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b5e1acc8418716a04d7efa055d2af4711ee115ee"},"cell_type":"code","source":"df_results = X_test\ndf_results.insert(loc=df_results.shape[1], column=\"y_real\", value=y_test)\ndf_results.insert(loc=df_results.shape[1], column=\"y_pred\", value=y_predicted)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e471a5d548a8975c23da4e2565c062483736e4ca"},"cell_type":"code","source":"days = env.get_prediction_days()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1725b3e7849caecdf8fbc1a0f7dd522850d53152"},"cell_type":"code","source":"(market_obs_df, news_obs_df, predictions_template_df) = next(days)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a3d7caa5c9ba6ab848431bac8edd7a13b9634f2e"},"cell_type":"code","source":"predictions_template_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"daa4120b7e5781aeae21d1615473f31d3dfe26da"},"cell_type":"code","source":"days.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"30d276237b81475c1920eee8ce1b1e6eeaf3b5e6"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}