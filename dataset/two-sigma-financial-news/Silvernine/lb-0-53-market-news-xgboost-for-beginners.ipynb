{"cells":[{"metadata":{"_uuid":"101a4cc2960efae072e5237982a5265c7c1e2aa9"},"cell_type":"markdown","source":"# 1.) Import Environment"},{"metadata":{"trusted":true,"_uuid":"3c0a5b5fddb1b2b068a1af2eae570a7068806825","scrolled":true},"cell_type":"code","source":"# Import some libraries\n%matplotlib inline\nimport numpy as np # linear algebra\nimport pandas as pd # data processing\nimport matplotlib.pyplot as plt # graphing\nimport os\nfrom datetime import datetime, timedelta # Used to subtract days from a date\n\nprint(os.listdir(\"../input\"))\n# Any results you write to the current directory are saved as output.\n\n# Import environment\nfrom kaggle.competitions import twosigmanews\nenv = twosigmanews.make_env()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"64111417bbfb09179e3790b3a3bae7b76571107f"},"cell_type":"markdown","source":"# 2.) Load Initial Market & News Train Data"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"# Import training dataset\n(market_train_df, news_train_df) = env.get_training_data()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ca85ae7f11f423de82cb2655363618df64cd482d"},"cell_type":"markdown","source":"# 3.) News Data Processing"},{"metadata":{"_uuid":"6aebc21a2ac2aaeaf1fd37d68fedb85784fd57ed"},"cell_type":"markdown","source":"### Function"},{"metadata":{"trusted":true,"_uuid":"3646cbc463aa6331ffe8e6d061ed7c48db6c164a","scrolled":true},"cell_type":"code","source":"# Process news data function\ndef process_news_date(news_train_df):\n    # Define which columns I don't want - I just used my intuition to select these columns\n    news_columns_to_drop = ['firstCreated','sourceId','headline','takeSequence','provider','subjects','audiences','bodySize','companyCount','headlineTag','sentenceCount','assetCodes','firstMentionSentence','noveltyCount12H','noveltyCount24H','noveltyCount3D','noveltyCount5D','noveltyCount7D','volumeCounts12H','volumeCounts24H','volumeCounts3D','volumeCounts5D','volumeCounts7D']\n    # Drop the columns chosen from above\n    news_train_df.drop(columns=news_columns_to_drop,inplace=True)\n    # Create sentiment word ratio from sentimentWordCount and wordCount <- i think this feature is helpful.\n    news_train_df['sentimentWordRatio'] = news_train_df['sentimentWordCount']/news_train_df['wordCount']\n    # Drop sentimentWordCount and wordCount since they are incorporated into the new column sentimentWordRatio now\n    news_columns_to_drop = ['wordCount','sentimentWordCount']\n    news_train_df=news_train_df.drop(columns=news_columns_to_drop)\n    #return the news dataframe\n    return news_train_df","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"485a927bf53cb987e90ae31796318cac3b91e495"},"cell_type":"markdown","source":"# 4.) Merge News Data & Market Data"},{"metadata":{"_uuid":"64fb0a50bd3245aa60b08733fd34c2768ad7ea84"},"cell_type":"markdown","source":"### Functions Used"},{"metadata":{"trusted":true,"_uuid":"e5a5e15ea148e6a06db0543c430824a86afde1c5"},"cell_type":"code","source":"# Separate 'date' into year,month, and day. Then, add year,month, and day to the 'assetName'.\n# Performing this will allow me to merge news & market data with this new 'combined_index' column\ndef combined_index(df):\n    df['combined_index'] = (df['time'].dt.year).astype(str)+(df['time'].dt.month).astype(str)+(df['time'].dt.day).astype(str)+(df['assetName']).astype(str)\n    return df\n\n# mergy market & news data by 'combined_index'\ndef merge_market_news(market_df,news_df):\n    # By having .mean(), it will take average of numeric values if there are duplicate news for the same 'combined_index'\n    news_df = news_df.groupby('combined_index').mean()\n    # merge news data to market data using the 'combined_index' we created\n    market_df=market_df.merge(news_df,how='left',on='combined_index')\n    # since there are more items in market data, ther are lots of rows with NaNs, and we fill them with 0 for training purposes.\n    fill_na_columns = ['urgency','marketCommentary','relevance','sentimentClass','sentimentNegative','sentimentNeutral','sentimentPositive','sentimentWordRatio']\n    market_df[fill_na_columns]=market_df[fill_na_columns].fillna(0)\n    return market_df","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"978513481ccf6b51a31a9ddcdea7d867f99c769a"},"cell_type":"markdown","source":"### Perform Data Processing on News Data & Merge it with Market Data"},{"metadata":{"trusted":true,"_uuid":"008e19ea0a680602b38fc095067a84f9118b0cd0"},"cell_type":"code","source":"# Process news data\nnews_train_df=process_news_date(news_train_df)\n# Create 'combined_index' for news dataframe\nnews_train_df=combined_index(news_train_df).copy()\n# Create 'combined_index' for market dataframe\nmarket_train_df=combined_index(market_train_df).copy()\n# Merge market & news data\nmarket_train_df=merge_market_news(market_train_df,news_train_df)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"031195ba97e8c9e3f89e7b2c11b34c59ce8af721"},"cell_type":"markdown","source":"# 5.) Market Data Processing"},{"metadata":{"_uuid":"8d8878eae351d0999e84f8d1438435bab5ec727e"},"cell_type":"markdown","source":"### Function"},{"metadata":{"trusted":true,"_uuid":"56dd0899b24e70d1f5bcae596f758037c4653eae"},"cell_type":"code","source":"# Pre-processes market data for training\ndef pre_process_market_data(market_train_df):\n    # Let's remove outliers based on our EDA. Remove anything outside [-1,1] for 'returnsOpenNextMktres10'\n    market_train_df = (market_train_df[(market_train_df['returnsOpenNextMktres10']<1) & (market_train_df['returnsOpenNextMktres10']>-1)]).copy()\n    # Let's choose our features\n    features = ['time','universe','volume','returnsClosePrevRaw1','returnsOpenPrevRaw1','returnsClosePrevRaw10','returnsOpenPrevRaw10','urgency','marketCommentary','relevance','sentimentClass','sentimentNegative','sentimentNeutral','sentimentPositive','sentimentWordRatio']\n    x = market_train_df[features].copy()\n    y = market_train_df[['returnsOpenNextMktres10','universe','time']].copy()\n    return x,y\n\n# Pre-processes market data for prediction for actual scoring. We are not provided with 'returnsOpenNextMktres10', hence no outlier removal is needed and we don't need to output target data.\ndef pre_process_market_data_actual_competition(market_train_df):\n    # Let's choose our features\n    features = ['volume','returnsClosePrevRaw1','returnsOpenPrevRaw1','returnsClosePrevRaw10','returnsOpenPrevRaw10','urgency','marketCommentary','relevance','sentimentClass','sentimentNegative','sentimentNeutral','sentimentPositive','sentimentWordRatio']\n    x = market_train_df[features]\n    return x","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"109076ccea721e7889d9c96bc686e47073f42a09"},"cell_type":"markdown","source":"### Perform Data Processing on Market Data"},{"metadata":{"trusted":true,"_uuid":"71545ebf108d692d7366580cd4b5613f4759a3c9"},"cell_type":"code","source":"x,y=pre_process_market_data(market_train_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9248d94520abe1e1db7994c31264b64804315ed2"},"cell_type":"markdown","source":"# 6.) Other Functions"},{"metadata":{"trusted":true,"_uuid":"9ca1d4ec3e6d3e0157ac93bd1ba7b430844c0ad9"},"cell_type":"code","source":"# Splits data for training. Takes out 30 days worth of data between training and validation set to prevent data leakage\ndef split_train_test_and_time(x,y,test_size):    \n    # Splits data as specified test_size and creates a gap of 30 days between train and test. This helps data leakage so that the model doesn't know the future when training\n    X_train = x[x['time']<(x['time'][int(len(x)*(1-test_size))]-timedelta(days=30))]\n    y_train = y[y['time']<(y['time'][int(len(x)*(1-test_size))]-timedelta(days=30))]\n    X_test = x[x['time']>x['time'][int(len(x)*(1-test_size))]]\n    y_test = y[y['time']>y['time'][int(len(y)*(1-test_size))]]   \n    # Features to be used\n    features_no_universe = ['volume','returnsClosePrevRaw1','returnsOpenPrevRaw1','returnsClosePrevRaw10','returnsOpenPrevRaw10','urgency','marketCommentary','relevance','sentimentClass','sentimentNegative','sentimentNeutral','sentimentPositive','sentimentWordRatio']\n    # Filters out data with universe==0\n    # X_train = X_train[X_train.universe==1]\n    # y_train = y_train[y_train.universe==1]  \n    # Save time for calculating score later. It is used to group and sum x_t values each day\n    train_time = X_train['time']\n    X_train = X_train[features_no_universe]\n    y_train = y_train['returnsOpenNextMktres10']    \n    # Filters out data with universe==0 for accurate scoring\n    X_test = X_test[X_test.universe==1]\n    y_test = y_test[y_test.universe==1]\n    # Save time for calculating score later. It is used to group and sum x_t values each day\n    test_time = X_test['time']\n    X_test = X_test[features_no_universe]\n    y_test = y_test['returnsOpenNextMktres10']   \n    return X_train,X_test,y_train,y_test,train_time,test_time\n\n# Draw graph of train vs eval scores. Visualize training process once it's done\ndef draw_train_eval_graph(evals_result,params):\n    x_axix = range(1,len(evals_result['train']['sigma_score'])+1)\n    train_sigma_score = evals_result['train']['sigma_score']\n    eval_sigma_score = evals_result['eval']['sigma_score']\n\n    plt.plot(x_axix,train_sigma_score,label='Train')\n    plt.plot(x_axix,eval_sigma_score,label='Eval')\n    plt.legend()\n    print(\"eta: \",params['eta'],\", max_depth: \",params['max_depth'])\n\n# This will display real target vs predictions. Kind of a sanity check..\ndef compare_real_target_with_pred(x,y):\n    # Compare predict and actual nextMKTres side by side\n    input_for_pred = xgb.DMatrix(x.values)\n    y_pred = bst.predict(input_for_pred,ntree_limit = bst.best_ntree_limit)\n    data  = {'y_real':y.values, 'y_pred': y_pred}\n    print(pd.DataFrame(data))","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":false,"_uuid":"7da7aea196751cf7cbfee14736d8f2acacfc5847"},"cell_type":"markdown","source":"# 7.) Sigma Score Function"},{"metadata":{"trusted":true,"_uuid":"8ef761f1e3eac334df5dff7a8ab93be769fa35e1"},"cell_type":"code","source":"# sigma_score function is considered as a custom evaluation metric for xgboost\n# example of how custom evaluation function is incorporated into xgboost's training can be found here : https://github.com/dmlc/xgboost/blob/master/demo/guide-python/custom_objective.py\ndef sigma_score(preds,dval):\n    # get y_target values\n    labels = dval.get_label()\n    # call time parameter to be used for grouping, so that we can add x_t values for each day\n    df_time = dval.params['extra_time']\n    # instead of making any prediction above 0 as 1, I chose anything above the mean of predictions (I call it market average) to be 1\n    preds[preds>preds.mean()]=1\n    # anything between market average(prediction mean) and 0 were given 0. \n    preds[(preds<=preds.mean())&(preds>=0)]=0\n    # any asset giving negative return...... -1 \n    preds[preds<0]=-1\n    # I assume you can take below approach too\n    #preds[preds>0]=1\n    #preds[preds<=0]=-1\n    \n    #calculate x_t and score as specified by the competition\n    x_t = pd.Series(preds*labels)\n    x_t_sum = x_t.groupby(df_time).sum()    \n    score = (x_t_sum.mean())/(x_t_sum.std())\n    return 'sigma_score', round(score,5)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c982e6d5b93ab12a174a16259b5dc6527c14e42f"},"cell_type":"markdown","source":"# 8.) Train Model - Evaluation of model"},{"metadata":{"trusted":true,"_uuid":"b2c6d880aff208b061e196c8dd3e4f29721139b9","scrolled":false},"cell_type":"code","source":"import xgboost as xgb\n\n# remember, this is not train_test_split from sklearn. This is my own function. It devides data with 30 days gap and doesn't allow the model to look into the future.\nX_train,X_val,y_train,y_val,train_time,val_time=split_train_test_and_time(x,y,test_size=0.2)\n\n# Define datasets that xgboost accepts\nxgtrain = xgb.DMatrix(X_train.values,y_train.values)\nxgval = xgb.DMatrix(X_val.values,y_val.values)\n\n# We will 'inject' an extra parameter in order to have access to df_valid['time'] inside sigma_score without globals\nxgtrain.params = {'extra_time': train_time.factorize()[0]}\nxgval.params = {'extra_time': val_time.factorize()[0]}\n\n# define parameters. I found learning rate of 0.3 and max_depth of 6 to be suitable.\nparams ={'eta':0.5, 'max_depth':5,'objective':'reg:linear','silent':1,'eval_metric':'rmse'}\n# this allows cross validation. Make sure eval data is the latter one, so that the model will do an early stopping if eval data's sigma score doesn't increase.\n# We want the training to stop when eval data's sigma score doesn't increase so that we don't overfit our model to the training data\nevallist = [(xgtrain,'train'),(xgval,'eval')]\n# Save evaluation metric scores for displaying later\nevals_result = {}\n# perform 400 rounds at maximum if early stopping doesn't happen\nnum_round = 400\n# here, one thing to note is that our custom evaluation function 'sigma_score' is passed into 'feval'.\nbst = xgb.train(params,xgtrain,num_round,evallist,evals_result=evals_result,feval=sigma_score,maximize=True,early_stopping_rounds=50,verbose_eval=10)\n# # If early stopping is enabled during training, you can get predictions from the best iteration by using this-> y_test_pred = bst.predict(xgtest,ntree_limit = bst.best_ntree_limit)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"dff5ff99670676d725f0ae1adbc1b71c7a17708b"},"cell_type":"markdown","source":"# 9.) Some Sanity Checks. Real vs Pred"},{"metadata":{"_uuid":"8d8691a19b9503ac67cd2e7eec24ff4bf32d5295"},"cell_type":"markdown","source":"\n### Val Data"},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"aa21a652348d4bbcdcf2ac80ef6bafcead44f3f1"},"cell_type":"code","source":"# Compare predict and actual nextMKTres side by side\ncompare_real_target_with_pred(X_val,y_val)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6400e1e9f744e0680a59c76c77bc4fac3b432a5b"},"cell_type":"markdown","source":"### Train Data"},{"metadata":{"trusted":true,"_uuid":"b0a6495fbc4d7d05715601a3f7b482d460bda0ef","scrolled":true},"cell_type":"code","source":"# Compare predict and actual nextMKTres side by side\ncompare_real_target_with_pred(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4adcd0f788ad5aa5ed226026278ceea08debb64d"},"cell_type":"markdown","source":"### Graph History of Learning for Val Data and Train Data"},{"metadata":{"trusted":true,"_uuid":"8cf83943b3321ef81788a92e6843bb6480b1d03b","scrolled":true},"cell_type":"code","source":"draw_train_eval_graph(evals_result,params)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"027f5e56c82218897f686314ab504cc7fbcbebd4"},"cell_type":"markdown","source":"# 10.) For Final Submission"},{"metadata":{"trusted":true,"_uuid":"c4d492fc6fc9cd71610738e5a70b7b1728622fdf"},"cell_type":"code","source":"# Use functions defined previously to process data for the final submission\ndef make_my_confidence_predictions(market_obs_df,predictions_template_df):#, news_obs_df, predictions_template_df):\n    x=pre_process_market_data_actual_competition(market_obs_df).copy()\n    x = xgb.DMatrix(x.values)\n    predictions_template_df.confidenceValue = bst.predict(x,ntree_limit = bst.best_ntree_limit)\n    predictions_template_df.confidenceValue[predictions_template_df.confidenceValue>predictions_template_df.confidenceValue.mean()]=1\n    predictions_template_df.confidenceValue[(predictions_template_df.confidenceValue<=predictions_template_df.confidenceValue.mean())&(predictions_template_df.confidenceValue>=0)]=0\n    predictions_template_df.confidenceValue[predictions_template_df.confidenceValue<0]=-1\n    return predictions_template_df\n\nfor (market_obs_df, news_obs_df, predictions_template_df) in env.get_prediction_days():    \n    # Process news data\n    news_obs_df=process_news_date(news_obs_df)\n    # Truncate date so that both news and market can be combined\n    news_obs_df=combined_index(news_obs_df).copy()\n    # Truncate date so that both news and market can be combined\n    market_obs_df=combined_index(market_obs_df).copy()\n    market_obs_df=merge_market_news(market_obs_df,news_obs_df)\n    market_obs_df=pre_process_market_data_actual_competition(market_obs_df)\n    \n    predictions_df = make_my_confidence_predictions(market_obs_df, predictions_template_df)\n    env.predict(predictions_df)\nprint('Done!')\n# Write submission file    \nenv.write_submission_file()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"56b74ba8faeaa56cdd84eeb624e394acb51ed0d3"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}