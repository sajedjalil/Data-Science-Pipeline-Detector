{"cells":[{"metadata":{"trusted":true,"_uuid":"c83e00537aba68d029ba47119d3994013ee96028"},"cell_type":"code","source":"## This is Two Merge Kernel of\n\n# https://www.kaggle.com/bguberfain/a-simple-model-using-the-market-and-news-data/\n# https://www.kaggle.com/the1owl/my-two-sigma-cents-only/","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport lightgbm as lgb\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom itertools import chain\nimport warnings\nwarnings.simplefilter(\"ignore\")\n\n%matplotlib inline\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"from kaggle.competitions import twosigmanews\n# You can only call make_env() once, so don't lose it!\nenv = twosigmanews.make_env()\nprint('Done!')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"663035547432e23a93f0b90657c94738f6f7acb7"},"cell_type":"code","source":"(market_train_df, news_train_df) = env.get_training_data()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cecf482046bba17fe8d721dcfadfa616ae8f89a8"},"cell_type":"code","source":"market_train = market_train_df.tail(2_000_000)\nmarket_train1 = market_train_df.tail(2_000_000)\nmarket_train_df = market_train_df.tail(2_000_000)\nnews_train = news_train_df.tail(6_000_000)\nnews_train1 = news_train_df.tail(6_000_000)\nnews_train_df = news_train_df.tail(6_000_000)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1ef980a1c362fbec0cc60b17b7e90a19e9c0d246"},"cell_type":"code","source":"def data_prep(market_train1,news_train1):\n    market_train1.time = market_train1.time.dt.date\n    news_train1.time = news_train1.time.dt.hour\n    news_train1.sourceTimestamp= news_train1.sourceTimestamp.dt.hour\n    news_train1.firstCreated = news_train1.firstCreated.dt.date\n    news_train1['assetCodesLen'] = news_train1['assetCodes'].map(lambda x: len(eval(x)))\n    news_train1['assetCodes'] = news_train1['assetCodes'].map(lambda x: list(eval(x))[0])\n    kcol = ['firstCreated', 'assetCodes']\n    news_train1 = news_train1.groupby(kcol, as_index=False).mean()\n    market_train1 = pd.merge(market_train1, news_train1, how='left', left_on=['time', 'assetCode'], \n                            right_on=['firstCreated', 'assetCodes'])\n    lbl = {k: v for v, k in enumerate(market_train1['assetCode'].unique())}\n    market_train1['assetCodeT'] = market_train1['assetCode'].map(lbl)\n    \n    \n    market_train1 = market_train1.dropna(axis=0)\n    \n    return market_train1\n\nmarket_train1 = data_prep(market_train1,news_train1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"44c177f78bdf459265a9cf328add89262cd6cfa3"},"cell_type":"code","source":"up = market_train.returnsOpenNextMktres10 >= 0\nfcol = [c for c in market_train if c not in ['assetCode', 'assetCodes', 'assetCodesLen', \n                                             'assetName', 'audiences', 'firstCreated', 'headline',\n                                             'headlineTag', 'marketCommentary', 'provider', 'returnsOpenNextMktres10',\n                                             'sourceId', 'subjects', 'time', 'time_x', 'universe','sourceTimestamp']]\n\n# We still need the returns for model tuning\nX = market_train[fcol].values\nup = up.values\nr = market_train.returnsOpenNextMktres10.values\n\n# Scaling of X values\n# It is good to keep these scaling values for later\nmins = np.min(X, axis=0)\nmaxs = np.max(X, axis=0)\nrng = maxs - mins\nX = 1 - ((maxs - X) / rng)\n\n# Sanity check\nassert X.shape[0] == up.shape[0] == r.shape[0]\n\nfrom sklearn import model_selection\nX_train, X_test, up_train, up_test, r_train, r_test = model_selection.train_test_split(X, up, r, test_size=0.25, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f0a14cc41f5694fdd2e4a099b77ccbc6bb35f18d"},"cell_type":"code","source":"from xgboost import XGBClassifier\nimport time\n\nxgb_up = XGBClassifier(n_jobs=4,n_estimators=200,max_depth=8,eta=0.1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a17854c66b7b509d95347787501fe77ad5be0759"},"cell_type":"code","source":"%%time\nfrom sklearn.metrics import accuracy_score\nprint('Fitting Up')\nxgb_up.fit(X_train,up_train,verbose=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f32f475e0f3c4dc446aa9d34c1f1c4143d524128"},"cell_type":"code","source":"from sklearn.metrics import accuracy_score\naccuracy_score(xgb_up.predict(X_test),up_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c5fd3293d95b2034c969c9787213bd2c23866493"},"cell_type":"code","source":"# Feature Engineering\n\nnews_cols_agg = {\n    'urgency': ['min', 'count'],\n    'takeSequence': ['max'],\n    'bodySize': ['min', 'max', 'mean', 'std'],\n    'wordCount': ['min', 'max', 'mean', 'std'],\n    'sentenceCount': ['min', 'max', 'mean', 'std'],\n    'companyCount': ['min', 'max', 'mean', 'std'],\n    'marketCommentary': ['min', 'max', 'mean', 'std'],\n    'relevance': ['min', 'max', 'mean', 'std'],\n    'sentimentNegative': ['min', 'max', 'mean', 'std'],\n    'sentimentNeutral': ['min', 'max', 'mean', 'std'],\n    'sentimentPositive': ['min', 'max', 'mean', 'std'],\n    'sentimentWordCount': ['min', 'max', 'mean', 'std'],\n    'noveltyCount12H': ['min', 'max', 'mean', 'std'],\n    'noveltyCount24H': ['min', 'max', 'mean', 'std'],\n    'noveltyCount3D': ['min', 'max', 'mean', 'std'],\n    'noveltyCount5D': ['min', 'max', 'mean', 'std'],\n    'noveltyCount7D': ['min', 'max', 'mean', 'std'],\n    'volumeCounts12H': ['min', 'max', 'mean', 'std'],\n    'volumeCounts24H': ['min', 'max', 'mean', 'std'],\n    'volumeCounts3D': ['min', 'max', 'mean', 'std'],\n    'volumeCounts5D': ['min', 'max', 'mean', 'std'],\n    'volumeCounts7D': ['min', 'max', 'mean', 'std']\n}","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2a6602f1c1e2067bc5749809762ee9845727e99a"},"cell_type":"markdown","source":"# Join Two File"},{"metadata":{"trusted":true,"_uuid":"ec2fd3dc5a5416146905bc40c6914bf60dc49690"},"cell_type":"code","source":"def join_market_news(market_train_df, news_train_df):\n    # Expand assetCodes\n    assetCodes_expanded = list(chain(*news_train_df['assetCodes']))\n    assetCodes_index = news_train_df.index.repeat( news_train_df['assetCodes'].apply(len) )\n\n    assert len(assetCodes_index) == len(assetCodes_expanded)\n    df_assetCodes = pd.DataFrame({'level_0': assetCodes_index, 'assetCode': assetCodes_expanded})\n\n    # Create expandaded news (will repeat every assetCodes' row)\n    news_cols = ['time', 'assetCodes'] + sorted(news_cols_agg.keys())\n    news_train_df_expanded = pd.merge(df_assetCodes, news_train_df[news_cols], left_on='level_0', right_index=True, suffixes=(['','_old']))\n\n    # Free memory\n    del news_train_df, df_assetCodes\n\n    # Aggregate numerical news features\n    news_train_df_aggregated = news_train_df_expanded.groupby(['time', 'assetCode']).agg(news_cols_agg)\n    \n    # Convert to float32 to save memory\n    news_train_df_aggregated = news_train_df_aggregated.apply(np.float32)\n\n    # Free memory\n    del news_train_df_expanded\n\n    # Flat columns\n    news_train_df_aggregated.columns = ['_'.join(col).strip() for col in news_train_df_aggregated.columns.values]\n\n    # Join with train\n    market_train_df = market_train_df.join(news_train_df_aggregated, on=['time', 'assetCode'])\n\n    # Free memory\n    del news_train_df_aggregated\n    \n    return market_train_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fc190c23ff9a11c41b973b6df6587aa0cbbde1c7"},"cell_type":"code","source":"def get_xy(market_train_df, news_train_df, le=None):\n    x, le = get_x(market_train_df, news_train_df)\n    y = market_train_df['returnsOpenNextMktres10'].clip(-1, 1)\n    return x, y, le\n\n\ndef label_encode(series, min_count):\n    vc = series.value_counts()\n    le = {c:i for i, c in enumerate(vc.index[vc >= min_count])}\n    return le\n\n\ndef get_x(market_train_df, news_train_df, le=None):\n    # Split date into before and after 22h (the time used in train data)\n    # E.g: 2007-03-07 23:26:39+00:00 -> 2007-03-08 00:00:00+00:00 (next day)\n    #      2009-02-25 21:00:50+00:00 -> 2009-02-25 00:00:00+00:00 (current day)\n    news_train_df['time'] = (news_train_df['time'] - np.timedelta64(22,'h')).dt.ceil('1D')\n\n    # Round time of market_train_df to 0h of curret day\n    market_train_df['time'] = market_train_df['time'].dt.floor('1D')\n\n    # Fix asset codes (str -> list)\n    news_train_df['assetCodes'] = news_train_df['assetCodes'].str.findall(f\"'([\\w\\./]+)'\")    \n    \n    # Join market and news\n    x = join_market_news(market_train_df, news_train_df)\n    \n    # If not label-encoder... encode assetCode\n    if le is None:\n        le_assetCode = label_encode(x['assetCode'], min_count=10)\n        le_assetName = label_encode(x['assetName'], min_count=5)\n    else:\n        # 'unpack' label encoders\n        le_assetCode, le_assetName = le\n        \n    x['assetCode'] = x['assetCode'].map(le_assetCode).fillna(-1).astype(int)\n    x['assetName'] = x['assetName'].map(le_assetName).fillna(-1).astype(int)\n    \n    try:\n        x.drop(columns=['returnsOpenNextMktres10'], inplace=True)\n    except:\n        pass\n    try:\n        x.drop(columns=['universe'], inplace=True)\n    except:\n        pass\n    x['dayofweek'], x['month'] = x.time.dt.dayofweek, x.time.dt.month\n    x.drop(columns='time', inplace=True)\n#    x.fillna(-1000,inplace=True)\n\n    # Fix some mixed-type columns\n    for bogus_col in ['marketCommentary_min', 'marketCommentary_max']:\n        x[bogus_col] = x[bogus_col].astype(float)\n    \n    return x, (le_assetCode, le_assetName)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3e386986c1e659b5f91fd39a04094c3e32f3c905"},"cell_type":"code","source":"X, y, le = get_xy(market_train_df, news_train_df)\nX.shape, y.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6fc25c2a976031745f34fe02b66fb82ffea5688c"},"cell_type":"code","source":"# Save universe data for latter use\nuniverse = market_train_df['universe']\ntime = market_train_df['time']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4b799b93d0738060dbfb726a85aed2335a96c68a"},"cell_type":"code","source":"n_train = int(X.shape[0] * 0.8)\n\nX_train, y_train = X.iloc[:n_train], y.iloc[:n_train]\nX_valid, y_valid = X.iloc[n_train:], y.iloc[n_train:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"98d849710267760cfd8658bd464b3dc29980635c"},"cell_type":"code","source":"# For valid data, keep only those with universe > 0. This will help calculate the metric\nu_valid = (universe.iloc[n_train:] > 0)\nt_valid = time.iloc[n_train:]\n\nX_valid = X_valid[u_valid]\ny_valid = y_valid[u_valid]\nt_valid = t_valid[u_valid]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"88b694eb21a3b633e30e7d70c30ff03c9af23a3d"},"cell_type":"code","source":"# Creat\ntrain_cols = X.columns.tolist()\ncategorical_cols = ['assetCode', 'assetName', 'dayofweek', 'month']\n\n# Note: y data is expected to be a pandas Series, as we will use its group_by function in `sigma_score`\ndtrain = lgb.Dataset(X_train.values, y_train, feature_name=train_cols, categorical_feature=categorical_cols, free_raw_data=False)\ndvalid = lgb.Dataset(X_valid.values, y_valid, feature_name=train_cols, categorical_feature=categorical_cols, free_raw_data=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a6ca13f1ee446a90fe2f77ea4faf43d700e493fc"},"cell_type":"code","source":"# We will 'inject' an extra parameter in order to have access to df_valid['time'] inside sigma_score without globals\ndvalid.params = {\n    'extra_time': t_valid.factorize()[0]\n}","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e06673154291b240bf9b5b03f9f1fb94fe5b0648"},"cell_type":"markdown","source":"# Light GBM"},{"metadata":{"trusted":true,"_uuid":"531ea9b6aebe8921980acc866c9326c5a27c7bfb"},"cell_type":"code","source":"lgb_params = dict(\n    objective = 'regression_l1',\n    learning_rate = 0.1,\n    num_leaves = 3,\n    max_depth = -1,\n    min_data_in_leaf = 1000,\n#     min_sum_hessian_in_leaf = 1000,\n    bagging_fraction = 0.5,\n    bagging_freq = 2,\n    feature_fraction = 0.75,\n    lambda_l1 = 0.0,\n    lambda_l2 = 0.0,\n    metric = 'None', # This will ignore the loss objetive and use sigma_score instead,\n    seed = 42 # Change for better luck! :)\n)\n\ndef sigma_score(preds, valid_data):\n    df_time = valid_data.params['extra_time']\n    labels = valid_data.get_label()\n    \n#    assert len(labels) == len(df_time)\n\n    x_t = preds * labels #  * df_valid['universe'] -> Here we take out the 'universe' term because we already keep only those equals to 1.\n    \n    # Here we take advantage of the fact that `labels` (used to calculate `x_t`)\n    # is a pd.Series and call `group_by`\n    x_t_sum = x_t.groupby(df_time).sum()\n    score = x_t_sum.mean() / x_t_sum.std()\n\n    return 'sigma_score', score, True\n\nevals_result = {}\nm = lgb.train(lgb_params, dtrain, num_boost_round=1000, valid_sets=(dvalid,), valid_names=('valid',), verbose_eval=50,\n              early_stopping_rounds=200, feval=sigma_score, evals_result=evals_result)\n\n\ndf_result = pd.DataFrame(evals_result['valid'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"79637df949dfd50850007d91739414747913ef31"},"cell_type":"code","source":"ax = df_result.plot(figsize=(12, 8))\nax.scatter(df_result['sigma_score'].idxmax(), df_result['sigma_score'].max(), marker='+', color='red')\n\nnum_boost_round, valid_score = df_result['sigma_score'].idxmax()+1, df_result['sigma_score'].max()\nprint(f'Best score was {valid_score:.5f} on round {num_boost_round}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"76713f06dff366d7ad17a94157f26e6210a58121"},"cell_type":"code","source":"fig, ax = plt.subplots(1, 2, figsize=(14, 5))\nlgb.plot_importance(m, ax=ax[0])\nlgb.plot_importance(m, ax=ax[1], importance_type='gain')\nfig.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"32027bae5a56f6241c8b4087d30a1527e55e8c15"},"cell_type":"code","source":"# Train full model\ndtrain_full = lgb.Dataset(X, y, feature_name=train_cols, categorical_feature=categorical_cols)\nmodel = lgb.train(lgb_params, dtrain, num_boost_round=num_boost_round)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1bc18ff3baecdf10bbdd898b51d787b9015dd9c3"},"cell_type":"markdown","source":"# ExtraTreeRegressor"},{"metadata":{"trusted":true,"_uuid":"d7fd3b77c24f2cde39950144908b574c15521848"},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn import *\nfrom kaggle.competitions import twosigmanews\n\ncol = [c for c in market_train if c not in ['assetCode', 'assetName', 'time', 'returnsOpenNextMktres10', 'universe']]\nfor c in col:\n    market_train[c] = market_train[c].fillna(0.0)\n#LabelEncode assetCode, add news based on split assetCodes and dates, aggregate news results\netr = ensemble.ExtraTreesRegressor(n_jobs=-1)\netr.fit(market_train[col], market_train['returnsOpenNextMktres10'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"847fd779894510e4d971792b4c6e06a8fa7227af"},"cell_type":"code","source":"def make_predictions(predictions_template_df, market_obs_df, news_obs_df, le):\n    x, _ = get_x(market_obs_df, news_obs_df, le)\n    predictions_template_df.confidenceValue = np.clip(model.predict(x), -1, 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d48e26bf65ed1313ffc8fb4e3e2afd594eee8709"},"cell_type":"code","source":"days = env.get_prediction_days()\n\n\nn_days = 0\nprep_time = 0\nprediction_time = 0\npackaging_time = 0\nfor (market_obs_df, news_obs_df, predictions_template_df) in days:\n    ########Light GBM\n    make_predictions(predictions_template_df, market_obs_df, news_obs_df, le) # LightGBM Prediction Function Submission\n    ########### XGB\n    market_obs_df1, news_obs_df1, predictions_template_df1 = market_obs_df, news_obs_df, predictions_template_df\n     n_days +=1\n    print(n_days,end=' ')\n    t = time.time()\n    market_obs_df1 = data_prep(market_obs_df1, news_obs_df1)\n    market_obs_df1 = market_obs_df1[market_obs_df1.assetCode.isin(predictions_template_df1.assetCode)]\n    X_live = market_obs_df1[fcol].values\n    X_live = 1 - ((maxs - X_live) / rng)\n    prep_time += time.time() - t\n    \n    t = time.time()\n    lp = xgb_up.predict_proba(X_live)\n    prediction_time += time.time() -t\n    t = time.time()\n    confidence = 2* lp[:,1] -1\n    preds = pd.DataFrame({'assetCode':market_obs_df['assetCode'],'confidence':confidence})\n    predictions_template_df1 = predictions_template_df1.merge(preds,how='left').drop('confidenceValue',axis=1).fillna(0).rename(columns={'confidence':'confidenceValue'})\n    ########EXTRA TREE REGRESSOR\n    for c in col:\n        market_obs_df[c] = market_obs_df[c].fillna(0.0)\n    market_obs_df['confidenceValue'] = etr.predict(market_obs_df[col]).clip(-1.0, 1.0)\n    sub = market_obs_df[['assetCode','confidenceValue']] # Extratreeregressor Function Submission\n    predictions_template_df.confidenceValue = predictions_template_df.confidenceValue * 0.34 + sub[\"confidenceValue\"] * 0.33 + predictions_template_df1.confidenceValue * 0.33 # Blending\n    env.predict(predictions_template_df)\nenv.write_submission_file()\nprint('Done!')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}