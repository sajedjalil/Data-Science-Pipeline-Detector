{"cells":[{"metadata":{"_uuid":"61683b3d1cba7506e7d7aa867f1d4e815818e720"},"cell_type":"markdown","source":""},{"metadata":{"_uuid":"cc28a1e7d0a98f13bc76d6172516f65e07263890"},"cell_type":"markdown","source":""},{"metadata":{"_uuid":"28f22f01fe854ef209669a33db45bd58190e2036"},"cell_type":"markdown","source":"\n\n**1. Feature Engineering**\n\n- Merge Market & News Data\n- Impute returns data using NOCB \n   https://towardsdatascience.com/how-to-handle-missing-data-8646b18db0d4\n- Create new Features to account for Time Series auto Correlation between rows.\n\n**2. Data reduction & Exploration**\n-  Subset of Data for top companies that always appear in news. \n    We considered top 15 companies with news data based on our research because we are evaluating the effect of news on Stock Movement.\n- Subset of Data from 2013\n- Use numeric news data (Novelty, Volume counts, Sentececount, Relevance, takeSequence etc.) & returns data columns\n-  Spot outliers and plot correlation\n\n**3. Split train and Test**\n1. Transform target variable to binary Stock-Movement Up/Down (0/1)\n2. Stock-Movement Up/Down will be the label for training.\n\n**4. Compare various Classifiers to find the best one.**\nhttps://www.kaggle.com/aldemuro/comparing-ml-algorithms-train-accuracy-90\n\n**5. Fit Classifier with Train**\nUsing Classifiers that work well with Mixed Data.\n\n    1. Random Forest\n    2. BaggingClassifier with DecisionTrees\n    3. XGBoost\n\n**6. Cross validation to estimate test error for this model.**\n\n**7. Use GridSearchCV to tune hyper parameters.**\n\n**8. Use the best estimator for test prediction and accuracy.**\n\n**Tips/Tricks: **\n\nMeasures to improve Test Accuracy of the models:\n1. Use companies that have data for VolumeCounts7D/NoveltyCount7D.\n\n2. Find outliers of all variables and treat them.\nhttps://www.kaggle.com/artgor/eda-feature-engineering-and-everything\n\n3 Create new features to capture autocorrelation:\n    e.g: https://www.kaggle.com/youhanlee/simple-quant-features-using-python\n    Make these specific to a particular Stock.\n    \n4. Use randomSearchCV instead of GridSearchCv\n\n5. HyperParameter Tuning\nhttps://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/\n\n6. Find means to add more data.\n\n7. Top 10 Features impacting next 10 day movement\n\n8. Splitting date into discrete components can allow decision trees were able to make better guesses.\n\n9. Make assetCode-specific datasets, train different assetCode specific models separtately on these datasets.\nCreate an ensemble of assetCode specific models?\nhttps://www.kaggle.com/arthurtok/introduction-to-ensembling-stacking-in-python\nhttps://www.analyticsvidhya.com/blog/2018/06/comprehensive-guide-for-ensemble-models/\n\n10. Overfit vs. Underfit curve plot\nhttp://scikit-learn.org/stable/modules/learning_curve.html\nhttps://www.kaggle.com/danbrice/keras-plot-history-full-report-and-grid-search\n\n11. Fit models with various subsets of the input features to find the best model.\nhttp://www.science.smith.edu/~jcrouser/SDS293/labs/lab8-py.html\n\n12. XGBoost overfit\nhttps://machinelearningmastery.com/avoid-overfitting-by-early-stopping-with-xgboost-in-python/"},{"metadata":{"_uuid":"4daf190e5465c842a9a33f77cafcd66a88171425"},"cell_type":"markdown","source":"# Imports"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_kg_hide-input":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nfrom sklearn import *\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.tools as tls\nfrom sklearn import model_selection\nfrom sklearn.metrics import accuracy_score\nfrom kaggle.competitions import twosigmanews\nenv = twosigmanews.make_env()\n(market_inputMain, news_inputMain) = env.get_training_data()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"91d3530d718108424a688de4882db8bc165e2b42","trusted":true},"cell_type":"code","source":"#print(market_inputMain.head())\n#print(news_inputMain.head())\nmarket_train_df = market_inputMain.copy(deep=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ad7f64c02457bb2bbd3091364eb8fed4803fe988"},"cell_type":"code","source":"print(len(market_train_df[\"assetCode\"].unique()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b8bc2a41a08b62450c34379407d3dcef372f8834"},"cell_type":"code","source":"data = []\nfor i in [0.05, 0.1, 0.25, 0.5, 0.75, 0.9, 0.95]:\n    price_df = market_train_df.groupby('time')['close'].quantile(i).reset_index()\n\n    data.append(go.Scatter(\n        x = price_df['time'].dt.strftime(date_format='%Y-%m-%d').values,\n        y = price_df['close'].values,\n        name = f'{i} quantile'\n    ))\nlayout = go.Layout(dict(title = \"Trends of closing prices by quantiles\",\n                  xaxis = dict(title = 'Month'),\n                  yaxis = dict(title = 'Price (USD)'),\n                  ),legend=dict(\n                orientation=\"h\"),\n    annotations=[\n        dict(\n            x='2008-09-01 22:00:00+0000',\n            y=82,\n            xref='x',\n            yref='y',\n            text='Collapse of Lehman Brothers',\n            showarrow=True,\n            font=dict(\n                family='Courier New, monospace',\n                size=16,\n                color='#ffffff'\n            ),\n            align='center',\n            arrowhead=2,\n            arrowsize=1,\n            arrowwidth=2,\n            arrowcolor='#636363',\n            ax=20,\n            ay=-30,\n            bordercolor='#c7c7c7',\n            borderwidth=2,\n            borderpad=4,\n            bgcolor='#ff7f0e',\n            opacity=0.8\n        ),\n        dict(\n            x='2011-08-01 22:00:00+0000',\n            y=85,\n            xref='x',\n            yref='y',\n            text='Black Monday',\n            showarrow=True,\n            font=dict(\n                family='Courier New, monospace',\n                size=16,\n                color='#ffffff'\n            ),\n            align='center',\n            arrowhead=2,\n            arrowsize=1,\n            arrowwidth=2,\n            arrowcolor='#636363',\n            ax=20,\n            ay=-30,\n            bordercolor='#c7c7c7',\n            borderwidth=2,\n            borderpad=4,\n            bgcolor='#ff7f0e',\n            opacity=0.8\n        ),\n        dict(\n            x='2014-10-01 22:00:00+0000',\n            y=120,\n            xref='x',\n            yref='y',\n            text='Another crisis',\n            showarrow=True,\n            font=dict(\n                family='Courier New, monospace',\n                size=16,\n                color='#ffffff'\n            ),\n            align='center',\n            arrowhead=2,\n            arrowsize=1,\n            arrowwidth=2,\n            arrowcolor='#636363',\n            ax=-20,\n            ay=-30,\n            bordercolor='#c7c7c7',\n            borderwidth=2,\n            borderpad=4,\n            bgcolor='#ff7f0e',\n            opacity=0.8\n        ),\n        dict(\n            x='2016-01-01 22:00:00+0000',\n            y=120,\n            xref='x',\n            yref='y',\n            text='Oil prices crash',\n            showarrow=True,\n            font=dict(\n                family='Courier New, monospace',\n                size=16,\n                color='#ffffff'\n            ),\n            align='center',\n            arrowhead=2,\n            arrowsize=1,\n            arrowwidth=2,\n            arrowcolor='#636363',\n            ax=20,\n            ay=-30,\n            bordercolor='#c7c7c7',\n            borderwidth=2,\n            borderpad=4,\n            bgcolor='#ff7f0e',\n            opacity=0.8\n        )\n    ])\npy.iplot(dict(data=data, layout=layout), filename='basic-line')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"495b98c3a02b74dc266b1029f16800824e223216","_kg_hide-output":false},"cell_type":"code","source":"for i, j in zip([-1, 0, 1], ['negative', 'neutral', 'positive']):\n    df_sentiment = news_inputMain.loc[news_inputMain['sentimentClass'] == i, 'assetName']\n    print(f'Top mentioned companies for {j} sentiment are:')\n    print(df_sentiment.value_counts().head(5))\n    print('')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b53769c1539e5e60cf926e71c3574a1d9a7529c5"},"cell_type":"code","source":"df_volumeCount = news_inputMain.loc[news_inputMain['volumeCounts7D'] > 0, 'assetName']\nprint(f'Top mentioned companies for {j} volumeCounts7D are:')\nprint(df_volumeCount.value_counts()[1:50])\n\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b4081c08e992e20171bb44c8218df739fb514397"},"cell_type":"markdown","source":"# 1. Feature Engineering"},{"metadata":{"_uuid":"8f20e428c48c0c20fad30685d63453920d38a2cd"},"cell_type":"markdown","source":"#### **Function to merge Market & News Datasets**"},{"metadata":{"_uuid":"dff38dc6d73fbea62c9b9a92d65e956e5d6a2c3f","trusted":true},"cell_type":"code","source":"#Make a deep copy to keep the main dataset. Environment cannot be restarted at will.\ndfm = market_inputMain.copy(deep=True)\ndfn = news_inputMain.copy(deep=True)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bc725b02f6de038b9eb175d9ee38a85da864b40d"},"cell_type":"code","source":"dfm[\"assetCode\"].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"93d54c9d9d8f9efe5ab1c896481cd793ea1ce2b9"},"cell_type":"code","source":" dfm = dfm[dfm[\"assetName\"].isin([  \n'Citigroup Inc'\\\n,'Apple Inc'\\\n,'JPMorgan Chase & Co'\\\n,'Bank of America Corp'\\\n,'HSBC Holdings PLC'\\\n,'Goldman Sachs Group Inc'\\\n,'Deutsche Bank AG'\\\n,'BHP Billiton PLC'\\\n,'BP PLC'\\\n,'Google Inc'\\\n,'Boeing Co'\\\n,'Rio Tinto PLC'\\\n,'Royal Dutch Shell PLC'\\\n,'Ford Motor Co'\\\n,'General Electric Co'\\\n,'Morgan Stanley'\\\n,'Microsoft Corp'\\\n,'Exxon Mobil Corp'\\\n,'UBS AG'\\\n,'Toyota Motor Corp'\\\n,'Royal Bank of Scotland Group PLC'\\\n,'Wal-Mart Stores Inc'\\\n,'BHP Billiton Ltd'\\\n,'General Motors Co'\\\n,'Verizon Communications Inc'\\\n,'AT&T Inc'\\\n,'Wells Fargo & Co'\\\n,'Amazon.com Inc'\\\n,'Lloyds Banking Group PLC'\\\n,'Credit Suisse Group AG'\\\n,'Chevron Corp'\\\n,'Pfizer Inc'\\\n,'American International Group Inc'\\\n,'Vodafone Group PLC'\\\n,'Federal Home Loan Mortgage Corp'\\\n,'Sony Corp'\\\n,'Federal National Mortgage Association'\\\n,'Total SA'\\\n,'Motors Liquidation Co'\\\n,'Nokia Oyj'\\\n,'Intel Corp'\\\n,'Twenty-First Century Fox Inc'\\\n,'Yahoo! Inc'\\\n,'International Business Machines Corp'\\\n,'GlaxoSmithKline PLC'\\\n,'Credit Suisse AG'\\\n,'Facebook Inc'\\\n,'HP Inc'\\\n,'Banco Santander SA'  ])]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"be86b4da74924c4b4011b288c1160ad03cc75434"},"cell_type":"code","source":"market_inputMain.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7aa2bb88383e3e78133c3ff19bdcdaaa50b66d92","trusted":true},"cell_type":"code","source":"dfm.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"89bcf16010e6130650f251eae6532e1a9c1e254e","trusted":true},"cell_type":"code","source":"market_inputMain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"59820ebe22328fddf1b6bbed237a8eff9387a6c7"},"cell_type":"code","source":"news_inputMain.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"453559e2da628c62312d277a0b49179edaaf2c4e","trusted":true},"cell_type":"code","source":"dfn.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a804644df8121dd91a463b67a8c0e6a280755d68"},"cell_type":"markdown","source":"#### Cutdown datasets"},{"metadata":{"_uuid":"3ac49e29f5c09134760207979df55140e743bbc1","trusted":true},"cell_type":"code","source":"#utc \nimport datetime\nimport pytz\n\nutc=pytz.UTC\n\n#cut down datasets to return\nstartdate = pd.to_datetime(\"2014-01-01\").replace(tzinfo=utc)\ndfm = dfm[dfm.time > startdate]\ndfn = dfn[dfn.time > startdate]\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bb9ce879de0ac1fb9433a949f23f62fc7417fa7e"},"cell_type":"markdown","source":"### EXPAND NEWS Dataset as each \"assetCodes\" field is a  list of assetCodes"},{"metadata":{"_uuid":"c3218d75cb4144a1efa280dd0569db83565c45ac","trusted":true},"cell_type":"code","source":"#News dataset shape before expanding\nnews_df = dfn\nnews_df.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"445c7013ddaed86824f010ddefea291ec4466197","trusted":true},"cell_type":"code","source":"dfm.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8077c966f019c61f5377c5beb45c3cad42fc574b","trusted":true},"cell_type":"code","source":"#First five asset codes of non-expaned News Dataset\nnews_df[\"assetCodes\"].head(5)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e5444e6efcd1084bb1c149e94e8c5d587e1a1fef","trusted":true},"cell_type":"code","source":"#Expanding assetCodes\nfrom itertools import chain\nnews_cols = news_df.columns.values\nnews_df['assetCodes'] = news_df['assetCodes'].str.findall(f\"'([\\w\\./]+)'\")  \n#print(chain(*news_df['assetCodes']))\nassetCodes_expanded = list(chain(*news_df['assetCodes']))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0e5fba709f5b6a8677f8773a66f46a44d65035fe"},"cell_type":"code","source":"assetCodeArray = dfm[\"assetCode\"].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bc00f68ca0f1f1ec3eab34300ec2808540250a7b"},"cell_type":"code","source":"#assetCodes_expanded = assetCodes_expanded\n\nassetCodes_index = news_df.index.repeat( news_df['assetCodes'].apply(len) )\nassert len(assetCodes_index) == len(assetCodes_expanded)\nassetCodes = pd.DataFrame({'level_0': assetCodes_index, 'assetCode': assetCodes_expanded})\nassetCodes = assetCodes[assetCodes[\"assetCode\"].isin(assetCodeArray)]\nnews_df_expanded = pd.merge(assetCodes, news_df[news_cols], left_on='level_0', right_index=True, suffixes=(['','_old']))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9f4686abfef744e6d5e2e9d7789c073e23519570"},"cell_type":"markdown","source":"'AAPL.O', 'BA.N', 'BAC.N', 'BBL.N', 'BCS.N', 'BP.N', 'DB.N', 'F.N',\\\n       'GE.N', 'GS.N', 'HBC.N', 'JPM.N', 'MS.N', 'MSFT.O', 'RDSa.N',\n       'RDSb.N', 'RTP.N', 'XOM.N', 'RIO.N', 'C.N', 'HSBC.N'"},{"metadata":{"trusted":true,"_uuid":"865ba5fe2692f82540a33f89d1aaf8098feb52f5"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"04328c633436531c2c5b59a2c5c77f2232837ef9","trusted":true},"cell_type":"code","source":"#Shape of news_df after expanding\nprint(news_df_expanded.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9067e8f7f1599df98e27407ec8f23e9031220046","trusted":true},"cell_type":"code","source":"news_df_expanded.iloc[:5, :10]","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"_uuid":"75dc4413973e648691d69f54a85bd8a9d1ed9f85","trusted":true},"cell_type":"code","source":"#Checking to see if there are missing values in news\nnews_df_expanded.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d16c96c8b881bcd7a42e21976598c90322327a20"},"cell_type":"markdown","source":"#### We found out that there are no missing values(NAs) in news dataset"},{"metadata":{"_uuid":"2fa06a4885e30e0222bb881e33ab7ed30dedb503"},"cell_type":"markdown","source":"#### Function to Merge Market vs. News datasets"},{"metadata":{"_uuid":"5b1a29fba1b10ca5860b4d111e52dbe5d4f9eb35","trusted":true},"cell_type":"code","source":"#\"data_prep\" will do Merge and some basic cleaning\n\ndef data_prep(market_df,news_df):\n    asset_code_dict = {k: v for v, k in enumerate(dfmI['assetCode'].unique())}\n    asset_code_dict\n    columns_tobe_retained = ['time','assetCode', 'assetName' ,'volume', 'open', 'close','returnsClosePrevRaw1',\\\n                             'returnsOpenPrevRaw1','returnsClosePrevMktres1','returnsOpenPrevMktres1',\\\n                             'returnsClosePrevRaw10','returnsOpenPrevRaw10','returnsClosePrevMktres10',\\\n                             'returnsOpenPrevMktres10','returnsOpenNextMktres10',\\\n                             'assetCodeT','urgency', 'takeSequence', 'companyCount','marketCommentary','sentenceCount',\\\n           'firstMentionSentence','relevance','sentimentClass','sentimentWordCount','noveltyCount24H',\\\n           'firstCreated',   \\\n                      # 'asset_sentiment_count', 'asset_sentence_mean','len_audiences',\\\n           'noveltyCount3D', 'noveltyCount5D', 'noveltyCount7D','volumeCounts24H','volumeCounts3D','volumeCounts5D','volumeCounts7D']\n    market_df['date'] = market_df['time'].dt.date\n    market_df['close_to_open'] = market_df['close'] / market_df['open']\n    market_df['assetCodeT'] = market_df['assetCode'].map(asset_code_dict)\n    #News data feature creation\n    #news_df['time'] = news_df.time.dt.hour\n    #news_df['sourceTimestamp']= news_df.sourceTimestamp.dt.hour\n    #news_df['firstCreated'] = news_df['firstCreated'].dt.date \n    #news_df['asset_sentiment_count'] = news_df.groupby(['assetName', 'sentimentClass'])['time'].transform('count')\n    #news_df['asset_sentence_mean'] = news_df.groupby(['assetName', 'sentenceCount'])['time'].transform('mean')\n    #news_df['len_audiences'] = news_df['audiences'].map(lambda x: len(eval(x)))\n    #kcol = ['firstCreated', 'assetCode']\n    news_df = news_df.groupby(kcol, as_index=False).mean()\n\n    # Merge news and market data. Only keep numeric columns\n    market_df_merge = pd.merge(market_df, news_df, how='left', left_on=['date', 'assetCode'], \n                            right_on=['firstCreated', 'assetCode'])\n\n    #return only data for the numeric columns + key information (assetCode, time)\n    return market_df_merge[columns_tobe_retained]\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"876854d1b5f8ad0e26e606fbb99156848588b77f","trusted":true},"cell_type":"code","source":"#Group News Data by firstCreated & assetCode\nkcol = ['firstCreated', 'assetCode']\nd = news_df_expanded.sort_values('firstCreated').copy(deep=True)\nd['firstCreated'] = d['firstCreated'].dt.date\nd = d.groupby(kcol, as_index=False).mean()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6c02372dfc697f316352aefcba5114ec04fa2328","trusted":true},"cell_type":"code","source":"d.tail(10)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6eb14516991c8ff1c54b98ca9827aa23e2c3e178","trusted":true},"cell_type":"code","source":"dfmI = dfm.copy(deep=True)\ndfnI = d.copy(deep=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8b3473f184a66b4e3fad02bc35a4054bbd3b8beb"},"cell_type":"markdown","source":"## Merge Market & News data"},{"metadata":{"trusted":true,"_uuid":"81445deb49d6b11f923b8129a9332cd04a6fc966","_kg_hide-output":true,"_kg_hide-input":true},"cell_type":"code","source":"len(dfnI)\nasset_code_dict = {k: v for v, k in enumerate(dfmI['assetCode'].unique())}\nasset_code_dict\n    ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7c9fc8c48d0447e28c09f671388bfe24f9184779","trusted":true},"cell_type":"code","source":"#\"data_prep\" will do Merge of Market & News Datasets\nmerged_dataset = data_prep(dfmI,dfnI)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"068da2e003be7728cc1e07092eab3a0dad5d9272","trusted":true},"cell_type":"code","source":"#Look at missing value summary\nmerged_dataset.count()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0b3dbeb1b017e3853c00e547d69e1831814fda13","trusted":true},"cell_type":"code","source":"#Checkingfor NAs\nmerged_dataset.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"57278ae30674d737b0c7272a77830a9e262b768c","trusted":true},"cell_type":"code","source":"# Function to plot time series data\ndef plot_vs_time(data_frame, column, calculation='mean', span=10):\n    if calculation == 'mean':\n        group_temp = data_frame.groupby('firstCreated')[column].mean().reset_index()\n    if calculation == 'count':\n        group_temp = data_frame.groupby('firstCreated')[column].count().reset_index()\n    if calculation == 'nunique':\n        group_temp = data_frame.groupby('firstCreated')[column].nunique().reset_index()\n    group_temp = group_temp.ewm(span=span).mean()\n    fig = plt.figure(figsize=(10,3))\n    plt.plot(group_temp['firstCreated'], group_temp[column])\n    plt.xlabel('Time')\n    plt.ylabel(column)\n    plt.title('%s versus time' %column)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"32629545e8632589d91fe1786adf83712cab4cfb"},"cell_type":"markdown","source":"### Look at NA values of returnsOpenPrevMktres10. We will re-verify this graph after interpolating to miss NA values in returns variables."},{"metadata":{"trusted":true,"_uuid":"e766555bb312687a064f35f7dbc074e1e229652c"},"cell_type":"code","source":"merged_dataset[merged_dataset['returnsOpenPrevMktres10'].isna()]['assetCode']","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4dcac19d781ffd6f9499667021b5d8967417f912","trusted":true},"cell_type":"code","source":"d = merged_dataset[merged_dataset['assetCode'] == 'SAN.N']\nimport matplotlib.pyplot as plt\n#print(d[d['returnsOpenPrevMktres10'].isna()]['assetCode'])\nprint(d.head())\nplt.plot(d['time'], d['returnsOpenPrevMktres10'])\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9b34bea6789024eca62338f8acfea6ff58cb7e3c"},"cell_type":"markdown","source":"### Impute Market Returns data using  (NOCB) imputation: https://towardsdatascience.com/how-to-handle-missing-data-8646b18db0d4"},{"metadata":{"_uuid":"0e18a0ca38b033db7b25b057eb8374d9895324b4"},"cell_type":"markdown","source":"### Fill nan values in MktRes columns using NOCB interpolation."},{"metadata":{"_uuid":"b18f280c956ef077ceee6d208eea009e2f691cde","trusted":true},"cell_type":"code","source":"# Fill nan values in MktRes columns using NOCB interpolation.\nmarket_fill = merged_dataset.copy(deep=True)\ncolumn_market = ['returnsClosePrevMktres1','returnsOpenPrevMktres1','returnsClosePrevMktres10', 'returnsOpenPrevMktres10']\ncolumn_raw = ['returnsClosePrevRaw1', 'returnsOpenPrevRaw1','returnsClosePrevRaw10', 'returnsOpenPrevRaw10']\nfor i in range(len(column_market)):\n    market_fill[column_market[i]].interpolate(method='nearest', inplace=True)   ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"08ad3d7b80f41cb0a8cb2f57ef65ffa74b6cacfa"},"cell_type":"markdown","source":"### Checking to make sure NAs are filled"},{"metadata":{"_uuid":"3008c223e557cdfbaa0fba8f315b2fd0953f3129","trusted":true},"cell_type":"code","source":"d = market_fill[market_fill['assetCode'] == 'SAN.N']\nimport matplotlib.pyplot as plt\n#print(d[d['returnsOpenPrevMktres10'].isna()]['assetCode'])\nprint(d.head())\nplt.plot(d['time'], d['returnsOpenPrevMktres10'])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9e4fd8c515e7cb35f389d06223844f5d9450d25a","trusted":true},"cell_type":"code","source":"#Checkingfor NAs before extracting data only for the 5 companies\nmarket_fill.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a6c51b337955c9b22c30b6a7342f8b3c0bfcb0e7","scrolled":false},"cell_type":"code","source":"#fill nulls with NAs\nimport numpy as np\nmarket_fill = market_fill.fillna(method='bfill')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9470d309e130ea148279bbbbcbfa4cbf60f0e4ee"},"cell_type":"code","source":"market_fill.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4d649a76ecd61cecc324c41ff502d1e910390b26"},"cell_type":"code","source":"market_fill.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d764a9c1c4a6769f73a1167e5095aacc7af919d9"},"cell_type":"code","source":"#Lets fill NAs with Linear Interpolation\n# Fill nan values in News Variables\n \ncolumn_market = [\"urgency\", \"takeSequence\",\"companyCount\",\"marketCommentary\",\n\"sentenceCount\",\"firstMentionSentence\",\"relevance\",\n\"sentimentClass\",\"sentimentWordCount\",\"noveltyCount24H\",\n\"firstCreated\",\"noveltyCount3D\",\"noveltyCount5D\",\n\"noveltyCount7D\",\"volumeCounts24H\",\"volumeCounts3D\"\n,\"volumeCounts5D\",\"volumeCounts7D\"]\n#column_raw = ['returnsClosePrevRaw1', 'returnsOpenPrevRaw1','returnsClosePrevRaw10', 'returnsOpenPrevRaw10']\nfor i in range(len(column_market)):\n    market_fill[column_market[i]].interpolate(method='nearest', inplace=True) ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ae8adea4f434d6a846315c703b0e740fd268a6f7"},"cell_type":"markdown","source":"# 1. Feature Engineering Contd."},{"metadata":{"_uuid":"81065902ea992c03f3140ef2d958f7a11e2ed2ca"},"cell_type":"markdown","source":"#### Bin numerical to binary when there is not much data for factors.\n"},{"metadata":{"_uuid":"6b577d6bbed1e4c20c77d3878c9f4ce4ff87f8f3"},"cell_type":"markdown","source":"### Create new Features to account for Time Series auto Correlation between rows."},{"metadata":{"trusted":true,"_uuid":"24981847ecef4b85129762a0d93611828507fa6a"},"cell_type":"code","source":"def rsiFunc(prices, n=14):\n    deltas = np.diff(prices)\n    seed = deltas[:n+1]\n    up = seed[seed>=0].sum()/n\n    down = -seed[seed<0].sum()/n\n    rs = up/down\n    rsi = np.zeros_like(prices)\n    rsi[:n] = 100. - 100./(1.+rs)\n\n    for i in range(n, len(prices)):\n        delta = deltas[i-1] # cause the diff is 1 shorter\n\n        if delta>0:\n            upval = delta\n            downval = 0.\n        else:\n            upval = 0.\n            downval = -delta\n\n        up = (up*(n-1) + upval)/n\n        down = (down*(n-1) + downval)/n\n\n        rs = up/down\n        rsi[i] = 100. - 100./(1.+rs)\n\n    return rsi","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"70fe65fdffb63fa71593fd640e208613206ab8e2","scrolled":false},"cell_type":"code","source":"#'AAPL.O',  'CSCO.O', 'IBM.N', 'INTC.O', 'MSFT.O', 'ORCL.O', 'ORCL.N'\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom datetime import datetime\nfull_dataset = pd.DataFrame()\n\n\nfor assetCode in assetCodeArray:\n   df = pd.DataFrame()\n   # Gather asset specific data\n   df = market_fill[market_fill[\"assetCode\"] == assetCode]\n   df['rsi10D'] = rsiFunc(df['close'].values, 10)\n   #Calculating all the trend variables for this assetCode\n   df['volume10DMA'] = df[\"volume\"].rolling(window=10,min_periods=1).mean() \n   df['returnsClosePrevMktres1-10DMA'] = df[\"returnsClosePrevMktres1\"].rolling(window=10,min_periods=1).mean()\n   df['returnsClosePrevMktres1-yearly'] = df[\"returnsClosePrevMktres1\"].rolling(window=365,min_periods=1).mean()\n   df['returnsClosePrevMktres1-quarterly'] = df[\"returnsClosePrevMktres1\"].rolling(window=90,min_periods=1).mean()\n\n   df['returnsOpenPrevMktres1-10DMA'] = df[\"returnsOpenPrevMktres1\"].rolling(window=10,min_periods=1).mean()\n   df['returnsOpenPrevMktres1-yearly'] = df[\"returnsOpenPrevMktres1\"].rolling(window=365,min_periods=1).mean()\n   df['returnsOpenPrevMktres1-quarterly'] = df[\"returnsOpenPrevMktres1\"].rolling(window=90,min_periods=1).mean()\n   \n   #Create new feature for close price moving average.\n   df['close10DMA'] = df['close'].rolling(window=10,min_periods=1).mean()\n   df['sentenceCount7D'] = df['sentenceCount'].rolling(window=7,min_periods=1).sum()\n   df['firstMentionSentence7D'] = df['firstMentionSentence'].rolling(window=7,min_periods=1).sum()\n   df['relevance7D'] = df['relevance'].rolling(window=7,min_periods=1).sum()\n   df['sentimentWordCount7D'] = df['sentimentWordCount'].rolling(window=7,min_periods=1).sum()\n   df['sentimentClass7D'] = df['sentimentClass'].rolling(window=7,min_periods=1).sum()\n   df['urgency7D'] = df['urgency'].rolling(window=7,min_periods=1).sum()\n   df['takeSequence7D'] = df['takeSequence'].rolling(window=7,min_periods=1).sum()\n   df['companyCount7D'] = df['companyCount'].rolling(window=7,min_periods=1).sum()\n   df['marketCommentary7D'] = df['marketCommentary'].rolling(window=7,min_periods=1).sum()\n   #df['bodySize7D'] = df['bodySize'].rolling(window=7).sum()\n\n   #Exponential Moving Average\n   ewma = pd.Series.ewm\n   df['close_10EMA'] = ewma(df[\"close\"], span=10).mean()\n   #Bollinger Bands are a type of statistical chart characterizing the prices and \n   #volatility over time of a financial instrument or commodity, using a formulaic method \n   #propounded by John Bollinger in the 1980s. Financial traders employ these charts as \n   #a methodical tool to inform trading decisions, control automated trading systems, \n   #or as a component of technical analysis. Bollinger Bands display a graphical band \n   #(the envelope maximum and minimum of moving averages, similar to\n   #Keltner or Donchian channels) and volatility (expressed by the width of the envelope) \n   #in one two-dimensional chart.\n\n   #ref. https://en.wikipedia.org/wiki/Bollinger_Bands \n   #Moving average convergence divergence (MACD) is a trend-following momentum indicator that shows the \n   #relationship between two moving averages of prices.\n   #The MACD is calculated by subtracting the 26-day exponential moving average (EMA) from the 12-day EMA\n   df['close_26EMA'] = ewma(df[\"close\"], span=26).mean()\n   df['close_12EMA'] = ewma(df[\"close\"], span=12).mean()\n   df['MACD'] = df['close_12EMA'] - df['close_26EMA']\n   no_of_std = 2\n   #ref. https://www.investopedia.com/terms/m/macd.asp\n\n   df['MA_10MA'] = df['close'].rolling(window=10,min_periods=1).mean()\n   df['MA_10MA_std'] = df['close'].rolling(window=10,min_periods=1).std() \n   df['MA_10MA_BB_high'] = df['MA_10MA'] + no_of_std * df['MA_10MA_std']\n   df['MA_10MA_BB_low'] = df['MA_10MA'] - no_of_std * df['MA_10MA_std']\n   full_dataset = full_dataset.append(df)\n\n\n\nfull_dataset[\"firstCreated\"] = full_dataset[\"time\"].dt.date\nfull_dataset['Year'] = full_dataset.time.dt.year\nfull_dataset['Month'] = full_dataset.time.dt.month\nfull_dataset['Day'] = full_dataset.time.dt.day\nfull_dataset['Week'] = full_dataset.time.dt.week\n\nimport datetime\nfull_dataset['day_of_year']  = full_dataset[\"time\"].dt.dayofyear\nfull_dataset = full_dataset.sort_values('firstCreated')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"68dc3358f46987d501c956d74ff203632b7a27c1"},"cell_type":"markdown","source":"#### Spot outlier Companies for close/open price difference"},{"metadata":{"trusted":true,"_uuid":"d601854c2eae7a24c8de981f6d51a040b4a9cf4c"},"cell_type":"code","source":"full_dataset[\"firstCreated\"] = full_dataset[\"time\"].dt.date\nfull_dataset['Year'] = full_dataset.time.dt.year\nfull_dataset['Month'] = full_dataset.time.dt.month\nfull_dataset['Day'] = full_dataset.time.dt.day\nfull_dataset['Week'] = full_dataset.time.dt.week\nimport datetime\nfull_dataset['day_of_year']  = full_dataset[\"time\"].dt.dayofyear\nfull_dataset['quarter']  = full_dataset[\"time\"].dt.quarter\nfull_dataset = full_dataset.sort_values('firstCreated')\n\nfull_dataset[\"daily_diff\"] = full_dataset[\"close\"] - full_dataset[\"open\"]\nfull_dataset['close_to_open'] =  np.abs(full_dataset['close'] / full_dataset['open'])\n    \n # determine whether the day is set on a holiday\nfrom pandas.tseries.holiday import USFederalHolidayCalendar as cal\nholidays = cal().holidays(start='2007-01-01', end='2018-09-27').to_pydatetime()\nfull_dataset['on_holiday'] = full_dataset[\"firstCreated\"].str.slice(0,10).apply(lambda x: 1 if x in holidays else 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"51652dc3e5ed95ab2bb8200d04681a8740e37b28"},"cell_type":"markdown","source":" plot_vs_time(full_dataset, 'returnsClosePrevMktres1', calculation='mean', span=30)\n plot_vs_time(full_dataset, 'returnsClosePrevMktres1', calculation='mean', span=90)"},{"metadata":{"trusted":true,"_uuid":"e566fc6f236a0cd97ab1660066492583fd70993e"},"cell_type":"code","source":"full_dataset.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cb32718e5416bb8cdf61f82da0352c3c21d624c2"},"cell_type":"code","source":"pos = len(full_dataset[full_dataset['returnsOpenNextMktres10']>0])\nneg = len(full_dataset[full_dataset['returnsOpenNextMktres10']<0])\ntot = pos + neg\n\nprint ('Positive cases %: ', pos  * 100/tot)\nprint ('Negative cases %: ', neg  * 100/tot)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2fd8d6e0db5edb35895dfcc7adaf1e37b0f843f7"},"cell_type":"code","source":"market_train_df = full_dataset.copy(deep=True)\nmarket_train_df['price_diff'] = market_train_df['close'] - market_train_df['open']\ngrouped = market_train_df.groupby('time').agg({'price_diff': ['std', 'min']}).reset_index()\ngrouped.sort_values(('price_diff', 'std'), ascending=False)[:10].head()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"21b0fee91a224426f65d00e2312cad8aa698ee4f"},"cell_type":"code","source":"grouped.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2518733432e866c0c2328d99e3a51d501aa637fc"},"cell_type":"code","source":"g = grouped.sort_values(('price_diff', 'std'), ascending=False)[:10]\ng['min_text'] = 'Maximum price drop: ' + (-1 * g['price_diff']['min']).astype(str)\ntrace = go.Scatter(\n    x = g['time'].dt.strftime(date_format='%Y-%m-%d').values,\n    y = g['price_diff']['std'].values,\n    mode='markers',\n    marker=dict(\n        size = g['price_diff']['std'].values,\n        color = g['price_diff']['std'].values,\n        colorscale='Portland',\n        showscale=True\n    ),\n    text = g['min_text'].values\n    #text = f\"Maximum price drop: {g['price_diff']['min'].values}\"\n    #g['time'].dt.strftime(date_format='%Y-%m-%d').values\n)\ndata = [trace]\n\nlayout= go.Layout(\n    autosize= True,\n    title= 'Top 10 months by standard deviation of price change within a day',\n    hovermode= 'closest',\n    yaxis=dict(\n        title= 'price_diff',\n        ticklen= 5,\n        gridwidth= 2,\n    ),\n    showlegend= False\n)\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig,filename='scatter2010')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a2f65f2db531741bc335b58f1d42a0e472369ba3"},"cell_type":"markdown","source":"### No surprising outliers to remove in terms of prices"},{"metadata":{"_uuid":"f53e81694de43d2f900420255072ba4f72823584"},"cell_type":"markdown","source":"#### Plot Correlations of Market Data"},{"metadata":{"_uuid":"b35e5abc81a614bb13a9ea5a6f1d839f2aef7dec","trusted":true},"cell_type":"code","source":"import seaborn as sns\nmarket_train_df[\"target_stockMoveUp\"] = market_train_df.returnsOpenNextMktres10 > 0\ncolumns_corr_market =  [ 'volume', 'open', 'close',\\\n       'returnsClosePrevRaw1', 'returnsOpenPrevRaw1',\\\n       'returnsClosePrevMktres1', 'returnsOpenPrevMktres1',\\\n       'returnsClosePrevRaw10', 'returnsOpenPrevRaw10',\\\n       'returnsClosePrevMktres10', 'returnsOpenPrevMktres10',\\\n       'close_10EMA', 'close_26EMA', 'close_12EMA', 'MACD',\\\n       'MA_10MA', 'MA_10MA_std', 'MA_10MA_BB_high', 'MA_10MA_BB_low','target_stockMoveUp']\ncolormap = plt.cm.RdBu\nplt.figure(figsize=(18,15))\nsns.heatmap(market_train_df[columns_corr_market].astype(float).corr(), linewidths=0.1, vmax=1.0, vmin=-1., square=True, cmap=colormap, linecolor='white', annot=True)\nplt.title('Pair-wise correlation')\nplt.rcParams['font.size'] = 10","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"253e69e46e05c97cf423456accf8996156bea468"},"cell_type":"markdown","source":"*Conclusions:**\n\n1. Stock volumes have some positive impact on the Stock movement.\n2. All of the returns variable have positive correlation with each other.\n3. Close & Open prices have strong correlation."},{"metadata":{"_uuid":"793f6db4f0b0977b1b7823f851ee1983b1165bf2","trusted":true},"cell_type":"code","source":"columns_corr_merge = [    \n       'noveltyCount3D', 'noveltyCount5D', 'noveltyCount7D', 'volumeCounts24H',\n       'volumeCounts3D', 'volumeCounts5D', 'volumeCounts7D', \n       'volume10DMA', 'close10DMA', 'sentenceCount7D',\n       'firstMentionSentence7D', 'relevance7D', 'sentimentWordCount7D',\n       'sentimentClass7D' , 'urgency7D', 'takeSequence7D', 'companyCount7D',\n       'marketCommentary7D','target_stockMoveUp']\ncolormap = plt.cm.RdBu\n# Scaling \ndf = market_train_df[columns_corr_merge]\nmins = np.min(df, axis=0)\nmaxs = np.max(df, axis=0)\nrng = maxs - mins\ndf = 1 - ((maxs - df) / rng)\n\nplt.figure(figsize=(18,15))\nsns.heatmap(df.astype(float).corr(), linewidths=0.1, vmax=1.0, vmin=-1., square=True, cmap=colormap, linecolor='white', annot=True)\nplt.title('Pair-wise correlation market and news')\nplt.rcParams['font.size'] = 10","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3cd54cb092fc7d772201d4b03310826e7946140c","trusted":false},"cell_type":"markdown","source":"**Conclusions:**\n\n1. Stock volumes have positive correlation with Stock Movement variables and the news Novelty/Volume.\n2. Novelty of the content seems to have correlation with Stock Closing Price.\n3. Novelty Indicators and Volume counts are postively correlated with each other.\n"},{"metadata":{"_uuid":"3c65113d21d67b431863885de2802e698dcd422e"},"cell_type":"markdown","source":"# Functions"},{"metadata":{"_uuid":"5dc87443bebdd2f21bd5c32ae26cff3422290d88"},"cell_type":"markdown","source":"# 3. Split Train and Test"},{"metadata":{"trusted":true,"_uuid":"964b5be4a50c8f8cc4b82a764aa0c98e70c9ab3e"},"cell_type":"code","source":"df1 = full_dataset.copy(deep=True)\ndf1 = df1.dropna()\n#create y from stock returns for next 10 days variable.\ny = df1.returnsOpenNextMktres10 > 0\n# Rest of the dataset is X\n\n#cols = [ 'volume','returnsClosePrevMktres1-20DMA',\\\n#       'returnsClosePrevRaw1', 'returnsOpenPrevRaw1',\\\n#       'returnsClosePrevMktres1', 'returnsOpenPrevMktres1',\\\n#       'returnsClosePrevRaw10', 'returnsOpenPrevRaw10',\\\n#       'returnsClosePrevMktres10', 'returnsOpenPrevMktres10',\\\n#       'assetCodeT',\\\n#       'volumeCounts7D', 'rsi20D',\\\n#       'volume10DMA', 'close10DMA', 'sentenceCount7D',\\\n#       'firstMentionSentence7D', 'relevance7D', 'sentimentWordCount7D',\\\n#       'sentimentClass7D', 'urgency7D', 'takeSequence7D', 'companyCount7D',\\\n#       'close_10EMA', 'close_26EMA', 'close_12EMA',\\\n#       'MACD', 'MA_10MA', 'MA_10MA_std', 'MA_10MA_BB_high', 'MA_10MA_BB_low',\\\n#       'Year', 'Month', 'Day', 'Week', 'day_of_year' ]\n\n\n#df1[cols].head(200)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"15913ae2c738d3ad95f2f077b4c7d6f0dcffbdc5"},"cell_type":"code","source":"cols = [\n       'volume10DMA'\n    ,'day_of_year'\\\n    ,'MA_10MA_BB_high'\\\n    ,'close_10EMA'\\\n    ,'returnsClosePrevMktres10'\\\n    ,'MACD'\\\n    ,'companyCount7D'\\\n    ,'sentimentClass7D'\\\n    ,'rsi10D'\\\n    #,'assetCodeT'\\\n    #,'close_to_open'\\\n    ,'volumeCounts7D'\\\n    #'returnsClosePrevMktres1-20DMA'\\\n    ,'returnsClosePrevMktres1-yearly'\\\n    ,'returnsClosePrevMktres1-quarterly'\\\n    #'returnsOpenPrevMktres1-20DMA'\\\n    ,'returnsOpenPrevMktres1-yearly'\\\n    ,'returnsOpenPrevMktres1-quarterly'\\\n   \n    \n]\nlen(cols)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"497e081c23b10f75312cbde22fdebac3c7889ae9","trusted":true},"cell_type":"code","source":"\n\nX = df1[cols] \n#train_size = int(len(X) * 0.66)\n#X_train, X_test = X[0:train_size], X[train_size:len(X)]\n#y_train, y_test = y[0:train_size], y[train_size:len(X)]\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.34, random_state=1)\n\n#X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.34, random_state=1)\n\nprint('Observations: %d' % (len(X)))\nprint('Training Observations: %d' % (len(X_train)))\n#print('Validation Observations: %d' % (len(X_val)))\nprint('Testing Observations: %d' % (len(X_test)))\n\n# The target is binary\nimport xgboost as xgb","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"239b8c4d46089afe645414966cc35b1ed10fdb37"},"cell_type":"markdown","source":"# Comparing AUC Scores from Classifiers to pick Top 3"},{"metadata":{"trusted":true,"_uuid":"64866a40cc46726d21438f0bf41d747aba74de0d"},"cell_type":"code","source":"# Compare Algorithms\nimport pandas\nimport xgboost as xgb\nimport matplotlib.pyplot as plt\nfrom sklearn import model_selection\nfrom sklearn.ensemble import BaggingClassifier, RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\n# prepare configuration for cross validation test harness\nseed = 7\n# prepare models\nmodels = []\nmodels.append(('LR', LogisticRegression()))\nmodels.append(('LDA', LinearDiscriminantAnalysis()))\nmodels.append(('KNN', KNeighborsClassifier()))\nmodels.append(('BAGC', BaggingClassifier()))\nmodels.append(('NB', GaussianNB()))\nmodels.append(('SVM', SVC()))\nmodels.append(('RFC', RandomForestClassifier()))\nmodels.append(('ABC', AdaBoostClassifier()))\nmodels.append(('XGBC', xgb.XGBClassifier()))\nmodels.append(('GBC', GradientBoostingClassifier()))\n# evaluate each model in turn\nresults = []\nnames = []\nscoring = 'roc_auc'\nfor name, model in models:\n    kfold = model_selection.KFold(n_splits=10, random_state=seed)\n    cv_results = model_selection.cross_val_score(model, X_train, y_train, cv=kfold, scoring=scoring)\n    results.append(cv_results)\n    names.append(name)\n    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n    print(msg)\n# boxplot algorithm comparison\nimport matplotlib as mpl\nfrom matplotlib.pyplot import figure\nfigure(num=None, figsize=(15, 15), dpi=80, facecolor='b', edgecolor='k')\n#figure(figsize=(1,1))\nfig = plt.figure()\nmpl.style.use('bmh')\nfig.set_size_inches(15, 15, forward=True)\nfig.suptitle('ML Algorithm Comparison')\nax = fig.add_subplot(111)\nplt.boxplot(results)\nax.set_xticklabels(names)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e8c3b37fc6ebe46e95e85b368188540c0e6f39cd"},"cell_type":"markdown","source":"# Picked Top 3 Classifiers - XGBoost, Bagging & RF Classifiers"},{"metadata":{"_uuid":"821544ca45840e108553120096f7c9ce4947dc5b"},"cell_type":"markdown","source":"## XGBoost - Classifier 1"},{"metadata":{"trusted":true,"_uuid":"8d634722bb7ddcddd04691d3793ae7d9594f39d0"},"cell_type":"code","source":"import xgboost as xgb\nfrom sklearn.ensemble import BaggingClassifier, RandomForestClassifier, AdaBoostClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV\nimport numpy as np\n \nxgb_model_train = xgb.XGBClassifier()\n \nimport matplotlib.pylab as plt\nfrom matplotlib import pyplot\nfrom xgboost import plot_importance\nfrom sklearn.metrics  import accuracy_score, roc_auc_score\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9591eea10cf1b491295982cbbaa1ee26f811f9cb"},"cell_type":"markdown","source":"## Random Forest - Classifier 2"},{"metadata":{"trusted":true,"_uuid":"14685dc187da3f9f63e5a433764e2c4f20b02497"},"cell_type":"code","source":"rf_obj=RandomForestClassifier()\n#rf_obj.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a65b34e40fe166734ff9f1fe3b2add61bc9f5e35"},"cell_type":"markdown","source":"## BaggingClassifier - Classifier 3"},{"metadata":{"trusted":true,"_uuid":"bc2d75d2253b4078822e49f9f01505db0333acfc"},"cell_type":"code","source":"bgc_obj=BaggingClassifier(base_estimator=None)\n#bgc_obj.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9de00c5cda6ef6fdb25b2c2bd3542f42330367c6"},"cell_type":"markdown","source":"# Plotting Underfit vs Overfit. based on Max-Depth"},{"metadata":{"trusted":true,"_uuid":"34e514be761c1b14f206e950377333933c5d41d5"},"cell_type":"code","source":"max_depths = [  3,  4,  5,  6, 7,  8,  9, 10, 11, 12,13,14,15,16,18]\n\nprint (max_depths)\ntrain_results = []\ntest_results = []\nfrom sklearn.model_selection import cross_val_score\nfor max_depth in max_depths:\n   dt = xgb.XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n       colsample_bytree=1, gamma=1, learning_rate=0.1, max_delta_step=0,\n       max_depth=max_depth, min_child_weight=1, missing=None, n_estimators=100,\n       n_jobs=1, nthread=None, objective='binary:logistic', random_state=0,\n       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n       silent=True, subsample=1 )  \n   scores = cross_val_score(dt, X_train, y_train, cv=5,scoring=\"accuracy\")\n   dt.fit(X_train,y_train)\n   #print(\"estimated AUC on the XG Boost training data-set: %0.3f (+/- %0.3f)\" % (scores.mean(), scores.std() * 2))   \n   # Add auc score to previous train results\n   train_results.append(scores.mean())\n   #y_pred = dt.predict(X_test)\n   #false_positive_rate, true_positive_rate, thresholds = metrics.roc_curve(y_test, y_pred)\n   #roc_auc = metrics.auc(false_positive_rate, true_positive_rate)\n   testAccuracy = round(dt.score(X_test, y_test), 4)\n   # Add auc score to previous test results\n   test_results.append(testAccuracy)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"75c9f0074f2e0c7f90108e42c49a176908de43b5"},"cell_type":"code","source":"\nfrom matplotlib.legend_handler import HandlerLine2D\nline1, = plt.plot(max_depths, train_results, 'b', label='Train')\nline2, = plt.plot(max_depths, test_results, 'r', label='Test')\nplt.legend(handler_map={line1: HandlerLine2D(numpoints=2)})\nplt.ylabel('Scores for XGBoost')\nplt.xlabel('Tree depth')\nplt.show()\ntrain_results = []\ntest_results = []\nfrom sklearn.ensemble import BaggingClassifier, RandomForestClassifier, AdaBoostClassifier\nfrom sklearn.model_selection import cross_val_score\nfor max_depth in max_depths:\n   dt = RandomForestClassifier(max_depth=max_depth)\n   \n   scores = cross_val_score(dt, X_train, y_train, cv=5,scoring=\"accuracy\")\n   #print(\"estimated AUC on the RF training data-set: %0.3f (+/- %0.3f)\" % (scores.mean(), scores.std() * 2))\n   train_results.append(scores.mean())\n   dt.fit(X_train,y_train)\n   #y_pred = dt.predict(X_test)\n   #false_positive_rate, true_positive_rate, thresholds = metrics.roc_curve(y_test, y_pred)\n   #roc_auc = metrics.auc(false_positive_rate, true_positive_rate)\n   # Add auc score to previous test results\n   testAccuracy = round(dt.score(X_test, y_test), 4)\n   # Add auc score to previous test results\n   test_results.append(testAccuracy)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cbd473fc1d372a5e3fe69b2c5cfa9c9984b635eb"},"cell_type":"code","source":"from matplotlib.legend_handler import HandlerLine2D\nline1, = plt.plot(max_depths, train_results, 'b', label='Train')\nline2, = plt.plot(max_depths, test_results, 'r', label='Test')\nplt.legend(handler_map={line1: HandlerLine2D(numpoints=2)})\nplt.ylabel('Score for RFClassifier')\nplt.xlabel('Tree depth')\nplt.show()\ntrain_results = []\ntest_results = []\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import cross_val_score\nfor max_depth in max_depths:\n   dt = BaggingClassifier(base_estimator=DecisionTreeClassifier(max_depth = max_depth))\n   scores = cross_val_score(dt, X_train, y_train, cv=5,scoring=\"roc_auc\")\n   #print(\"estimated AUC on the RF training data-set: %0.3f (+/- %0.3f)\" % (scores.mean(), scores.std() * 2))\n   train_results.append(scores.mean())\n   dt.fit(X_train,y_train)\n  #y_pred = dt.predict(X_test)\n   #false_positive_rate, true_positive_rate, thresholds = metrics.roc_curve(y_test, y_pred)\n   #roc_auc = metrics.auc(false_positive_rate, true_positive_rate)\n   # Add auc score to previous test results\n   testAccuracy = round(dt.score(X_test, y_test), 4)\n   # Add auc score to previous test results\n   test_results.append(testAccuracy)\nfrom matplotlib.legend_handler import HandlerLine2D\nline1, = plt.plot(max_depths, train_results, 'b', label='Train')\nline2, = plt.plot(max_depths, test_results, 'r', label='Test')\nplt.legend(handler_map={line1: HandlerLine2D(numpoints=2)})\nplt.ylabel('Score for Bagging Classifier')\nplt.xlabel('Tree depth')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"64f8145eaae6cd9ab1941dd63b70e2f7d7a6b328"},"cell_type":"markdown","source":"# 5. Using Max-Depth findings to Fit Classifier(s) with GridSearchCV"},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"3763a9f579091b17261b9fe06eff9df9fc4d8e77"},"cell_type":"code","source":"import xgboost as xgb\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n\nimport numpy as np ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"54f97baca40b109d9ab4476d23ff023c3436c34b"},"cell_type":"markdown","source":"## XGBoost - Classifier 1 - GridSearch"},{"metadata":{"trusted":true,"_uuid":"3318f15aec5cee4f26d6b1ecfdd8a0110653d00c"},"cell_type":"code","source":"#brute force scan for all parameters, here are the tricks\n#usually max_depth is 6,7,8\n#learning rate is around 0.05, but small changes may make big diff\n#tuning min_child_weight subsample colsample_bytree can have \n#much fun of fighting against overfit \n#n_estimators is how many round of boosting\n#finally, ensemble xgboost with multiple seeds may reduce variance\n#from sklearn.model_selection import TimeSeriesSplit\n#tss = TimeSeriesSplit(n_splits=10).split(X_train)\n\nparams = {\n         \n        'gamma': [0.5, 1, 1.5, 2, 5],\n        'max_depth': [6,7,8,9,10],\n        'n_estimators': [50,100  ]  \n        }\n \n#gsearch = GridSearchCV(xgb_model_train, params, n_jobs=5,cv=5, scoring='roc_auc',verbose=1, refit=True)\ngsearch = RandomizedSearchCV(xgb_model_train, params, cv = 5, scoring = 'roc_auc', verbose=1, \n                              random_state=42, n_jobs = -1)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cf4bc8cc2712ab182afed857d16df54acc1b3a78"},"cell_type":"markdown","source":"## Random Forest - Classifier 2 - GridSearch"},{"metadata":{"trusted":true,"_uuid":"03c047054284e67ab46d71a69753b263fa22739a"},"cell_type":"code","source":"params = { \n'max_depth': [14,15,16,17],\n 'max_features': ['auto', 'sqrt'],\n 'min_samples_leaf': [1, 2, 4],\n 'min_samples_split': [2, 5, 10],\n 'n_estimators': [50,100,200,300,400] \n}\n#rf_Grid = GridSearchCV(rf_obj, param_grid, cv = 5, scoring = 'roc_auc',refit = True, n_jobs=-1, verbose = 1)\nrf_Grid = RandomizedSearchCV(rf_obj, params,cv = 5, scoring = 'roc_auc', verbose=1, random_state=42, n_jobs = -1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"09164e16f1f33e61090c01ef5f02de82954327f9"},"cell_type":"markdown","source":"## BaggingClassifier - Classifier 3  - GridSearch"},{"metadata":{"trusted":true,"_uuid":"880283b7b620aacfbdc5b95ce40489fafd93adb3"},"cell_type":"code","source":"params = {\n  'max_features': [5,6, 7,8],\n  'max_samples' : [0.05, 0.1, 0.2, 0.5],\n  'n_estimators': [50,100, 200, 400] ,\n  'bootstrap' :[True,False],\n  'bootstrap_features':[True,False]\n}\n \nbgc_Grid = RandomizedSearchCV(BaggingClassifier(DecisionTreeClassifier(), max_features = 0.5), params , cv = 5, scoring = 'roc_auc', verbose=1, random_state=42, n_jobs = -1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0284161b3a820506e5a47d2e67197ad66040ace5"},"cell_type":"markdown","source":"# # 7. Use GridSearchCV to tune hyper parameters.**"},{"metadata":{"trusted":true,"_uuid":"5a141dbf21013ad1b9b5df34a7ed469237852948","scrolled":true},"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a2bb6b6bcbab37e16de727c700b264e759f398e7"},"cell_type":"code","source":"gsearch.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d864449ca83445f9f4e6dbe3040e9d5a0ca9ce30","scrolled":false},"cell_type":"code","source":"rf_Grid.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5e84e03db320d2bf1b803ba305c61c56b2bbe779"},"cell_type":"code","source":"bgc_Grid.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2a4ab8181055c2ea0730e657b42a3beb5ae0e76d"},"cell_type":"markdown","source":"## Plotting top features from XGBoost fit"},{"metadata":{"trusted":true,"_uuid":"95cc0cc0b7d8c9e2e52625e1b2528797727cfb6d"},"cell_type":"code","source":"plot_importance(gsearch.best_estimator_) \nplt.rcParams['figure.figsize'] = [12,12]\nplt.rcParams['font.size'] = 25\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c7a8bdd231e67770140c734e9186c479b988736f"},"cell_type":"code","source":"gsearch.best_estimator_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7888ede288165328b1e9ba651336a4d72ecb7e32"},"cell_type":"code","source":"xg_clf = gsearch.best_estimator_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4f92f3da8b33a1c611e1a67fc386558ad9d97f2d"},"cell_type":"code","source":"rf_clf = rf_Grid.best_estimator_\nrf_clf ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ed5c158c88dd22faeb9f0a15ef895df9c521bb14"},"cell_type":"markdown","source":"## Plotting top features from RF fit"},{"metadata":{"trusted":true,"_uuid":"05d48bb8b4051d0f407f4cde1fad40759f96e44e"},"cell_type":"code","source":"feature_importances = pd.Series(rf_clf .feature_importances_, index=X_train.columns)\nfeature_importances.nlargest(10).sort_values(ascending = True).plot(kind='barh')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b2f04d122b85d259b7adba6117dd332587ea16a1"},"cell_type":"code","source":"bgc_clf = bgc_Grid.best_estimator_","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7d3eb27032210e4c5f8308bb38cc0d328c8cd083"},"cell_type":"markdown","source":"# 8. Validation set AUC"},{"metadata":{"trusted":true,"_uuid":"66d401057d5c8ddefadf7d75e0fb941cd9a56ae9"},"cell_type":"code","source":"#from sklearn.metrics import accuracy_score\nprint('XGBoost Cross validation AUC score:', np.round(gsearch.best_score_ ,2))\n ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e315014eb858e88526439a08b715dad352a465ea"},"cell_type":"code","source":"#from sklearn.metrics import accuracy_score\n\n#best_parameters, score, _ = max(rf_Grid.grid_scores_, key=lambda x: x[1])\nprint('RandomForest Cross validation AUC score:', np.round(rf_Grid.best_score_ ,2))\n#for param_name in sorted(best_parameters.keys()):\n#    print(\"%s: %r\" % (param_name, best_parameters[param_name]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d9f1e37dda38e2fa84572c668da4a6deb25b5af9","_kg_hide-output":false},"cell_type":"code","source":"#from sklearn.metrics import accuracy_score\n\n#best_parameters, score, _ = max(bgc_Grid.grid_scores_, key=lambda x: x[1])\nprint('Bagging Classifier Cross validation AUC score:', np.round(bgc_Grid.best_score_ ,2))\n#for param_name in sorted(best_parameters.keys()):\n#    print(\"%s: %r\" % (param_name, best_parameters[param_name]))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"22163bb4774c60a29574168df1a523d74244c39c"},"cell_type":"markdown","source":""},{"metadata":{"_uuid":"383b6807b4bdde332b811b416f79f7d82c2a67f1"},"cell_type":"markdown","source":"# Train Predictions and AUC"},{"metadata":{"trusted":true,"_uuid":"e703a6c8e3b2a42e94bd790441d9bd132f88a258"},"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\nscores = cross_val_score(rf_clf, X_train, y_train, cv=5,scoring=\"roc_auc\")\nprint(\"estimated AUC on the training data-set: %0.3f (+/- %0.3f)\" % (scores.mean(), scores.std() * 2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"50f80ee9a2ce2c0621332dac9df21bcd522f8922"},"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\nscores = cross_val_score(xg_clf, X_train, y_train, cv=5,scoring=\"roc_auc\")\nprint(\"estimated AUC on the training data-set: %0.3f (+/- %0.3f)\" % (scores.mean(), scores.std() * 2))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ca1f8b0a2f5cd9488513297330e244c9308035b2"},"cell_type":"markdown","source":"# Test set AUC for XGBoost"},{"metadata":{"trusted":true,"_uuid":"7fdf7c55e88a2fcf4779feddffa9664bc1738ca1"},"cell_type":"code","source":"# calculate the fpr and tpr for all thresholds of the classification\nprobs =  gsearch.predict_proba(X_test)\npreds = probs[:,1]\nfpr, tpr, threshold = metrics.roc_curve(y_test, preds)\nroc_auc = metrics.auc(fpr, tpr)\nprint (\"Test AUC for XGBoost\", roc_auc )\n# method I: plt\nimport matplotlib.pyplot as plt\nplt.title('Receiver Operating Characteristic XGBoost Test')\nplt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\nplt.legend(loc = 'lower right')\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0, 1])\nplt.ylim([0, 1])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"34e9428abab696c28576ce910df1d52c0d2a1ac4"},"cell_type":"code","source":"y_scores = gsearch.predict(X_test)\nconfusion_matrix =  pd.crosstab(index=y_test, columns=y_scores, rownames=['Expected'], colnames=['Predicted'])\nsns.heatmap(confusion_matrix, annot=True, square=False, fmt='', cbar=False)\nplt.title(\"Classification Matrix\", fontsize = 15)\nplt.show()\n#plot_confusion_matrix_from_data(y_test,  y_scores,columns=[\"Up\",\"Down\" ], annot=True, cmap=\"Blues\",\n     # fmt='.5f', fz=20, lw=1, cbar=False, figsize=[12,12], show_null_values=0, pred_val_axis='lin')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2d802815adaeb98243c7e550b48bfdf7fdd56f45"},"cell_type":"code","source":"clfReport = metrics.classification_report(y_test, y_scores, target_names=[\"Stock-Movement-Up\", \"Stock-Movement-Down\"])\nprint(clfReport)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4cc4583ec766651e1c71d20a9f2b1b855b962a06"},"cell_type":"code","source":"# calculate the fpr and tpr for all thresholds of the classification\nprobs =  rf_clf.predict_proba(X_test)\npreds = probs[:,1]\nfpr, tpr, threshold = metrics.roc_curve(y_test, preds)\nroc_auc = metrics.auc(fpr, tpr)\nprint (\"Test AUC for Random Forest\", roc_auc )\n# method I: plt\nimport matplotlib.pyplot as plt\nplt.title('Receiver Operating Characteristic Random Forest Test')\nplt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\nplt.legend(loc = 'lower right')\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0, 1])\nplt.ylim([0, 1])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show()\n\ny_scores = rf_clf.predict(X_test)\nconfusion_matrix =  pd.crosstab(index=y_test, columns=y_scores, rownames=['Expected'], colnames=['Predicted'])\nsns.heatmap(confusion_matrix, annot=True, square=False, fmt='', cbar=False)\nplt.title(\"Classification Matrix\", fontsize = 15)\nplt.show()\n#plot_confusion_matrix_from_data(y_test,  y_scores,columns=[\"Up\",\"Down\" ], annot=True, cmap=\"Blues\",\n     # fmt='.5f', fz=20, lw=1, cbar=False, figsize=[12,12], show_null_values=0, pred_val_axis='lin')\n\n    \nclfReport = metrics.classification_report(y_test, y_scores, target_names=[\"Stock-Movement-Up\", \"Stock-Movement-Down\"])\nprint(clfReport)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9038757d901e5f2fa35325f2b9a1e429cda2ff7f"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}