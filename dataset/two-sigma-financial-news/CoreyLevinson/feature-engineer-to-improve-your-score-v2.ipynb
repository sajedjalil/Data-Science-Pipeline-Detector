{"cells":[{"metadata":{"_uuid":"cc28a1e7d0a98f13bc76d6172516f65e07263890"},"cell_type":"markdown","source":"<p>The notebook is based on:  https://www.kaggle.com/jannesklaas/lb-0-6-xgboost-baseline  </p>\n<p>Only difference:  xgb_up = XGBClassifier(n_jobs=4,n_estimators=200,max_depth=8,eta=0.1) </p>\n<p> His original model: xgb_up = XGBClassifier(n_jobs=4,n_estimators=100) </p>\n<p> Please knidly vote  if you like the parameters </p>\n    "},{"metadata":{"_uuid":"28f22f01fe854ef209669a33db45bd58190e2036"},"cell_type":"markdown","source":"# XGBoost Baseline\n\nThis notebook rephrases the challenge of predicting stock returns as the challenge of predicting whether a stock will go up. The evaluation  asks you to predict a confidence value between -1 and 1. The predicted confidence value gets then multiplied with the actual return. If your confidence is in the wrong direction (ie. you predict positive values while returns are actually negative), you loose on the metric. If your direction is right however, you want your confidence be as large as possible.\n\nStocks can only go up or down, if the stock is not going up, it must go down (at least a little bit). So if we know our model confidence in the stock going up, then our new confidence is:\n$$\\hat{y}=up-(1-up)=2*up-1$$\n\nWe are left with a \"simple\" binary classification problem, for which there are a number of good tool, here we use XGBoost, but pick your poison."},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn import *\nimport re\nfrom kaggle.competitions import twosigmanews\nenv = twosigmanews.make_env()\n(market_train, news_train) = env.get_training_data()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f2dad9717c953f794c179d40f4729e5f92dfd829","_kg_hide-input":true},"cell_type":"code","source":"def data_prep(market_train,news_train):\n    market_train.time = market_train.time.dt.date\n    news_train.time = news_train.time.dt.hour\n    news_train.sourceTimestamp= news_train.sourceTimestamp.dt.hour\n    news_train.firstCreated = news_train.firstCreated.dt.date\n    news_train['assetCodesLen'] = news_train['assetCodes'].map(lambda x: len(eval(x)))\n    news_train['assetCodes'] = news_train['assetCodes'].map(lambda x: list(eval(x))[0])\n    news_train['headlineLen'] = news_train['headline'].apply(lambda x: len(x))\n    news_train['numberUppercaseLetters'] = news_train['headline'].apply(lambda x: len(re.findall(r'[A-Z]',x)))\n    news_train['uppercaseRatio'] = news_train['numberUppercaseLetters'] / news_train['headlineLen']\n    kcol = ['firstCreated', 'assetCodes']\n    news_train = news_train.groupby(kcol, as_index=False).mean()\n    market_train = pd.merge(market_train, news_train, how='left', left_on=['time', 'assetCode'], \n                            right_on=['firstCreated', 'assetCodes'])\n    lbl = {k: v for v, k in enumerate(market_train['assetCode'].unique())}\n    market_train['assetCodeT'] = market_train['assetCode'].map(lbl)\n    \n    \n    market_train = market_train.dropna(axis=0)\n    \n    return market_train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dff38dc6d73fbea62c9b9a92d65e956e5d6a2c3f"},"cell_type":"code","source":"market_train = data_prep(market_train,news_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8fac527806f2e85323c4c23e1f7d8bbf29df4718"},"cell_type":"code","source":"market_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"016c66ecdf1e913fcf97941924c177e7764ecb0e"},"cell_type":"code","source":"from datetime import datetime, date","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"20bc8950e44b90467d3b44a8b48a0554f21f14d8"},"cell_type":"code","source":"# Remove noisy data from financial crisis of 2008 and before\n# Why? Because we have reason to believe this time is significantly different from how it acts now.\nmarket_train = market_train.loc[market_train['time_x']>=date(2009, 1, 1)]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"37659796cd0f92b229701fcec7dd138ad4040407"},"cell_type":"markdown","source":"# Now we will feature engineer to improve our score."},{"metadata":{"trusted":true,"_uuid":"f7f8195ca7952c98f9c4e1fb721fd45d381c77e6"},"cell_type":"code","source":"# Open Minus Closing Amounts\nmarket_train['open_minus_close'] = market_train['open'] - market_train['close']\nmarket_train['openprevraw1_minus_closeprevraw1'] = market_train['returnsOpenPrevRaw1'] - market_train['returnsClosePrevRaw1']\nmarket_train['openprevraw10_minus_closeprevraw10'] = market_train['returnsOpenPrevRaw10'] - market_train['returnsClosePrevRaw10']\nmarket_train['openprevmktres1_minus_closeprevmktres1'] = market_train['returnsOpenPrevMktres1'] - market_train['returnsClosePrevMktres1']\nmarket_train['openprevmktres10_minus_closeprevmktres10'] = market_train['returnsOpenPrevMktres10'] - market_train['returnsClosePrevMktres10']\n\n# Differences between opens and closes over lengths of time\nmarket_train['open_change'] = market_train['open'] - market_train['returnsOpenPrevRaw1']\nmarket_train['open_change2'] = market_train['open'] - market_train['returnsOpenPrevRaw10']\nmarket_train['open_change3'] = market_train['open'] - market_train['returnsOpenPrevMktres1']\nmarket_train['open_change4'] = market_train['open'] - market_train['returnsOpenPrevMktres10']\nmarket_train['open_change5'] = market_train['returnsOpenPrevRaw1'] - market_train['returnsOpenPrevRaw10']\nmarket_train['open_change6'] = market_train['returnsOpenPrevRaw1'] - market_train['returnsOpenPrevMktres1']\nmarket_train['open_change7'] = market_train['returnsOpenPrevRaw1'] - market_train['returnsOpenPrevMktres10']\nmarket_train['open_change8'] = market_train['returnsOpenPrevRaw10'] - market_train['returnsOpenPrevMktres1']\nmarket_train['open_change9'] = market_train['returnsOpenPrevRaw10'] - market_train['returnsOpenPrevMktres10']\nmarket_train['open_change10'] = market_train['returnsOpenPrevMktres1'] - market_train['returnsOpenPrevMktres10']\n\nmarket_train['close_change'] = market_train['close'] - market_train['returnsClosePrevRaw1']\nmarket_train['close_change2'] = market_train['close'] - market_train['returnsClosePrevRaw10']\nmarket_train['close_change3'] = market_train['close'] - market_train['returnsClosePrevMktres1']\nmarket_train['close_change4'] = market_train['close'] - market_train['returnsClosePrevMktres10']\nmarket_train['close_change5'] = market_train['returnsClosePrevRaw1'] - market_train['returnsClosePrevRaw10']\nmarket_train['close_change6'] = market_train['returnsClosePrevRaw1'] - market_train['returnsClosePrevMktres1']\nmarket_train['close_change7'] = market_train['returnsClosePrevRaw1'] - market_train['returnsClosePrevMktres10']\nmarket_train['close_change8'] = market_train['returnsClosePrevRaw10'] - market_train['returnsClosePrevMktres1']\nmarket_train['close_change9'] = market_train['returnsClosePrevRaw10'] - market_train['returnsClosePrevMktres10']\nmarket_train['close_change10'] = market_train['returnsClosePrevMktres1'] - market_train['returnsClosePrevMktres10']\n\n# Verbosity Ratios: Maybe if you have a small word count but large body size, you are using long words -> smarter words -> news impact has more impact?\nmarket_train['sentenceratio'] = market_train['bodySize'] / market_train['sentenceCount']\nmarket_train['wordratio'] = market_train['bodySize'] / market_train['wordCount']\nmarket_train['sentence_to_wordratio'] = market_train['sentenceCount'] / market_train['wordCount']\n\n# Sentiment aggregations\nmarket_train['sentiment_avg'] = np.mean(market_train.head()[['sentimentNegative','sentimentNeutral','sentimentPositive']], axis=1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3b8cce945a12abee5f39cb0245c9673af1e80c06"},"cell_type":"code","source":"# The target is binary\nup = market_train.returnsOpenNextMktres10 >= 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9ae876c4fe42afea0f8d851f9ad5f452e0e54172","_kg_hide-input":true},"cell_type":"code","source":"fcol = [c for c in market_train if c not in ['assetCode', 'assetCodes', 'assetCodesLen', 'assetName', 'audiences', \n                                             'firstCreated', 'headline', 'headlineTag', 'marketCommentary', 'provider', \n                                             'returnsOpenNextMktres10', 'sourceId', 'subjects', 'time', 'time_x', 'universe','sourceTimestamp']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bcdc25c33aa7554fa478d22a1117e9cd70f3695e"},"cell_type":"code","source":"# We still need the returns for model tuning\nX = market_train[fcol].values\nup = up.values\nr = market_train.returnsOpenNextMktres10.values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b0606838c4f3048b0bf9201a474d10141e23a71e"},"cell_type":"code","source":"# Scaling of X values\n# It is good to keep these scaling values for later\nmins = np.min(X, axis=0)\nmaxs = np.max(X, axis=0)\nrng = maxs - mins\nX = 1 - ((maxs - X) / rng)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1cbf8850869641f05fa82fa9f0b7940c7bd195cf"},"cell_type":"code","source":"# Sanity check\nassert X.shape[0] == up.shape[0] == r.shape[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4e7d6c785647616780c0d44551bc0ee9687764eb"},"cell_type":"code","source":"X_train, X_test, up_train, up_test, r_train, r_test\\\n= model_selection.train_test_split(X, up, r, test_size=0.25, random_state=99)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d0f53b50992ef3679eaa613f73886a7cd41f618e"},"cell_type":"code","source":"from xgboost import XGBClassifier\nimport time","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d7ca196ff23b888ec496a11d0e603b2f921d1e7b"},"cell_type":"code","source":"xgb_up = XGBClassifier(n_jobs=4,n_estimators=250,max_depth=8,eta=0.1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5523094226fce884c6e8ae7f10f8bdcc087b9391"},"cell_type":"code","source":"t = time.time()\nprint('Fitting Up')\nxgb_up.fit(X_train,up_train)\nprint(f'Done, time = {time.time() - t}')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9c9c3df4b4f1460a9fce79e3c0b185cce6d502ec"},"cell_type":"markdown","source":"A side effect of treating this as a binary task is that we can use a simpler metric to judge our models"},{"metadata":{"trusted":true,"_uuid":"b9704742ed38c1ea4d6cbb07e6176a8a5e06b4e0"},"cell_type":"code","source":"from sklearn.metrics import accuracy_score\naccuracy_score(xgb_up.predict(X_test),up_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"882168eb5aad93dfaa3e2122bf8bca9fe1585a1e"},"cell_type":"code","source":"days = env.get_prediction_days()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ba0b0d6c0582cfea44b849a5d454963d39613757"},"cell_type":"code","source":"n_days = 0\nprep_time = 0\nprediction_time = 0\npackaging_time = 0\nfor (market_obs_df, news_obs_df, predictions_template_df) in days:\n    n_days +=1\n    print(n_days,end=' ')\n    t = time.time()\n    market_obs_df = data_prep(market_obs_df, news_obs_df)\n    # Open Minus Closing Amounts\n    market_obs_df['open_minus_close'] = market_obs_df['open'] - market_obs_df['close']\n    market_obs_df['openprevraw1_minus_closeprevraw1'] = market_obs_df['returnsOpenPrevRaw1'] - market_obs_df['returnsClosePrevRaw1']\n    market_obs_df['openprevraw10_minus_closeprevraw10'] = market_obs_df['returnsOpenPrevRaw10'] - market_obs_df['returnsClosePrevRaw10']\n    market_obs_df['openprevmktres1_minus_closeprevmktres1'] = market_obs_df['returnsOpenPrevMktres1'] - market_obs_df['returnsClosePrevMktres1']\n    market_obs_df['openprevmktres10_minus_closeprevmktres10'] = market_obs_df['returnsOpenPrevMktres10'] - market_obs_df['returnsClosePrevMktres10']\n\n    # Differences between opens and closes over lengths of time\n    market_obs_df['open_change'] = market_obs_df['open'] - market_obs_df['returnsOpenPrevRaw1']\n    market_obs_df['open_change2'] = market_obs_df['open'] - market_obs_df['returnsOpenPrevRaw10']\n    market_obs_df['open_change3'] = market_obs_df['open'] - market_obs_df['returnsOpenPrevMktres1']\n    market_obs_df['open_change4'] = market_obs_df['open'] - market_obs_df['returnsOpenPrevMktres10']\n    market_obs_df['open_change5'] = market_obs_df['returnsOpenPrevRaw1'] - market_obs_df['returnsOpenPrevRaw10']\n    market_obs_df['open_change6'] = market_obs_df['returnsOpenPrevRaw1'] - market_obs_df['returnsOpenPrevMktres1']\n    market_obs_df['open_change7'] = market_obs_df['returnsOpenPrevRaw1'] - market_obs_df['returnsOpenPrevMktres10']\n    market_obs_df['open_change8'] = market_obs_df['returnsOpenPrevRaw10'] - market_obs_df['returnsOpenPrevMktres1']\n    market_obs_df['open_change9'] = market_obs_df['returnsOpenPrevRaw10'] - market_obs_df['returnsOpenPrevMktres10']\n    market_obs_df['open_change10'] = market_obs_df['returnsOpenPrevMktres1'] - market_obs_df['returnsOpenPrevMktres10']\n\n    market_obs_df['close_change'] = market_obs_df['close'] - market_obs_df['returnsClosePrevRaw1']\n    market_obs_df['close_change2'] = market_obs_df['close'] - market_obs_df['returnsClosePrevRaw10']\n    market_obs_df['close_change3'] = market_obs_df['close'] - market_obs_df['returnsClosePrevMktres1']\n    market_obs_df['close_change4'] = market_obs_df['close'] - market_obs_df['returnsClosePrevMktres10']\n    market_obs_df['close_change5'] = market_obs_df['returnsClosePrevRaw1'] - market_obs_df['returnsClosePrevRaw10']\n    market_obs_df['close_change6'] = market_obs_df['returnsClosePrevRaw1'] - market_obs_df['returnsClosePrevMktres1']\n    market_obs_df['close_change7'] = market_obs_df['returnsClosePrevRaw1'] - market_obs_df['returnsClosePrevMktres10']\n    market_obs_df['close_change8'] = market_obs_df['returnsClosePrevRaw10'] - market_obs_df['returnsClosePrevMktres1']\n    market_obs_df['close_change9'] = market_obs_df['returnsClosePrevRaw10'] - market_obs_df['returnsClosePrevMktres10']\n    market_obs_df['close_change10'] = market_obs_df['returnsClosePrevMktres1'] - market_obs_df['returnsClosePrevMktres10']\n\n    # Verbosity Ratios: Maybe if you have a small word count but large body size, you are using long words -> smarter words -> news impact has more impact?\n    market_obs_df['sentenceratio'] = market_obs_df['bodySize'] / market_obs_df['sentenceCount']\n    market_obs_df['wordratio'] = market_obs_df['bodySize'] / market_obs_df['wordCount']\n    market_obs_df['sentence_to_wordratio'] = market_obs_df['sentenceCount'] / market_obs_df['wordCount']\n\n    # Sentiment aggregations\n    market_obs_df['sentiment_avg'] = np.mean(market_obs_df.head()[['sentimentNegative','sentimentNeutral','sentimentPositive']], axis=1)\n    market_obs_df = market_obs_df[market_obs_df.assetCode.isin(predictions_template_df.assetCode)]\n    X_live = market_obs_df[fcol].values\n    X_live = 1 - ((maxs - X_live) / rng)\n    prep_time += time.time() - t\n    \n    t = time.time()\n    lp = xgb_up.predict_proba(X_live)\n    prediction_time += time.time() -t\n    \n    t = time.time()\n    confidence = 2* lp[:,1] -1\n    preds = pd.DataFrame({'assetCode':market_obs_df['assetCode'],'confidence':confidence})\n    predictions_template_df = predictions_template_df.merge(preds,how='left').drop('confidenceValue',axis=1).fillna(0).rename(columns={'confidence':'confidenceValue'})\n    env.predict(predictions_template_df)\n    packaging_time += time.time() - t","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"276d3a27bb4e07ee5b385c4e10a79f2d4afd707b"},"cell_type":"code","source":"env.write_submission_file()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2e66a8de2d58932d1d3df9f07794a5fa19f45725"},"cell_type":"code","source":"total = prep_time + prediction_time + packaging_time\nprint(f'Preparing Data: {prep_time:.2f}s')\nprint(f'Making Predictions: {prediction_time:.2f}s')\nprint(f'Packing: {packaging_time:.2f}s')\nprint(f'Total: {total:.2f}s')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a986b33984691203ee9d56830ccf0248baa2c9d9"},"cell_type":"markdown","source":"For good measure, we can check what XGBoost bases its decisions on"},{"metadata":{"trusted":true,"_uuid":"53e5ef548b227c146994ebb02fbd395f69c3c368"},"cell_type":"code","source":"import matplotlib.pyplot as plt\n%matplotlib inline\nfrom xgboost import plot_importance","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cb06f976b02d95bcc9bbfd6d58e3a492da1ce75e"},"cell_type":"code","source":"plt.figure(num=None, figsize=(10, 10), dpi=80, facecolor='w', edgecolor='k')\nplt.bar(range(len(xgb_up.feature_importances_)), xgb_up.feature_importances_)\nplt.xticks(range(len(xgb_up.feature_importances_)), fcol, rotation='vertical');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3cd54cb092fc7d772201d4b03310826e7946140c"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}