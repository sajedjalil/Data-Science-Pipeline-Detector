{"cells":[{"metadata":{"_uuid":"0851731b4ca10afe7622bd93aedbe97f9514a477"},"cell_type":"markdown","source":"# Created by Corey Levinson"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd # python dataframes\nimport numpy as np # python numerics\nimport matplotlib.pyplot as plt # python plotting\nimport seaborn as sns\n\n# Keras imports\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, LSTM, GRU, SpatialDropout1D, GlobalMaxPool1D\nfrom keras.layers.embeddings import Embedding\n\nfrom kaggle.competitions import twosigmanews # Needed to obtain training/test data\n\nfrom tqdm import tqdm\nimport gc","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3ce2811e84aa786433167fd7ef97bb023ba011e1"},"cell_type":"code","source":"# Change DEBUG to False when you're ready, Corey.\nDEBUG = False\n\n# Change YEARMIN to change the cutoff point for your data\n# All data must be greater than YEARMIN\nYEARMIN = 2011","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"30bf5d1ebd83fcd11af6d45d11f66d593f07a02f"},"cell_type":"code","source":"#random seeds for stochastic parts of neural network \nnp.random.seed(100)\nfrom tensorflow import set_random_seed\nset_random_seed(150)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9177d0da175cb4a6f555efd2b553e1ffc482f224"},"cell_type":"code","source":"env = twosigmanews.make_env()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true,"scrolled":true},"cell_type":"code","source":"# Load in market data, garbage collect news data\n(market_train, _) = env.get_training_data()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4bd830b6000c1388f17aa9a5874d0d1a56eb1e69"},"cell_type":"markdown","source":"# Light preprocessing:"},{"metadata":{"trusted":true,"_uuid":"c043bb57f980df8c960c58a5c59a01f5658a446b"},"cell_type":"code","source":"# Require all data to be more recent than YEARMIN\nmarket_train = market_train.loc[market_train['time'].dt.year > YEARMIN]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d0dd4982850a75d5383057ba2308b55fde80b8bc"},"cell_type":"code","source":"market_train.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7f78d839015b6351de6941c5e4477550d624b572"},"cell_type":"code","source":"# # Require all TARGETS be in range (-1, 1)\n# market_train['returnsOpenNextMktres10'] = market_train['returnsOpenNextMktres10'].clip(-1,1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"90ffe12bbd803f50ce28b9b4c92ce71e1b367ff3"},"cell_type":"code","source":"# Are there any columns that have NA's?\n# Recall Neural Networks requires all values imputed\nprint('MARKET TRAIN:')\nfor col in market_train.columns:\n    print(col+' has '+str(market_train[col].isna().sum())+' NAs')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5a849c48e1d3105af06531696fed032dbf2efce8"},"cell_type":"markdown","source":"# Four columns have NA's. Let's impute them with the median of the group"},{"metadata":{"trusted":true,"_uuid":"40f88d7507cf9a4f285cdbc40b011a04ce6cf230"},"cell_type":"code","source":"# If DEBUG, then don't read in all of the data.\nif DEBUG:\n    market_train = market_train.sample(50000, random_state=4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"42c0552d309e0e752b9d71556f4abc85b696bcd2"},"cell_type":"code","source":"# Attempt to impute by group by's median\nmarket_train['returnsClosePrevMktres1'] = market_train.groupby(['assetCode'])['returnsClosePrevMktres1'].transform(lambda x: x.fillna(x.median()))\nmarket_train['returnsOpenPrevMktres1'] = market_train.groupby(['assetCode'])['returnsOpenPrevMktres1'].transform(lambda x: x.fillna(x.median()))\nmarket_train['returnsClosePrevMktres10'] = market_train.groupby(['assetCode'])['returnsClosePrevMktres10'].transform(lambda x: x.fillna(x.median()))\nmarket_train['returnsOpenPrevMktres10'] = market_train.groupby(['assetCode'])['returnsOpenPrevMktres10'].transform(lambda x: x.fillna(x.median()))\n\n# If the assetCode has no non-null values, then impute with column median\nmarket_train = market_train.fillna(market_train.median())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a0f12294f6e60819a590ac45471f01ce39dc0186"},"cell_type":"code","source":"market_train = market_train.sort_values(['assetCode','time']) # Sort it by time for use in LSTM later\nmarket_train.reset_index(drop=True,inplace=True)\nmarket_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"26416e3b861bce330f8856dbb3b6ba477d5d88cf"},"cell_type":"code","source":"market_train.columns","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b23f4eca590477a11949690b2a48ef165274b99c"},"cell_type":"markdown","source":"# Hypothesis: I think i dont have enough RAM to construct the list. So I am reducing amount of information being fed."},{"metadata":{"trusted":true,"_uuid":"7bddadc7f43b8fb316647434a42464f339600c84"},"cell_type":"code","source":"market_train['time'] = pd.to_datetime(market_train['time'].dt.date) # Change from datetime to date for less memory and easier merge with news","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a7f1d9d93f37c174df7e0ed4c41130ada5a338ac"},"cell_type":"code","source":"# Feature Engineering\nmarket_train['margin1'] = market_train['open'] / market_train['close']\nmarket_train['TARGET'] = np.sign(market_train['returnsOpenNextMktres10'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"acf91b8321caeeced6b31c26679012330ef5eec5"},"cell_type":"code","source":"# # Keep last 30 of each asset\ntotal_market_obs_df = [market_train.loc[(market_train['time'].dt.year >= 2016) & (market_train['time'].dt.month >= 9)].groupby('assetCode').tail(30).drop(['universe','returnsOpenNextMktres10'], axis=1)]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"98385becb2c3fa7ccfb645c6b692b2aaf39b0c2c"},"cell_type":"markdown","source":"# Train LSTM model now"},{"metadata":{"trusted":true,"_uuid":"7702403b6c0c744fd3991960707c1d57791ca3ef"},"cell_type":"code","source":"LSTM_COLUMNS_TO_USE = ['time', # Time variable is necessary\n                       'assetCode', # AssetCode is necessary to perform merges/historical analysis\n                       'universe', # binary variable indicating if entry will be used in metric\n                       'returnsOpenNextMktres10',\n                       'TARGET',\n                       'volume',\n                       'returnsClosePrevRaw1', 'returnsOpenPrevRaw1',\n                       'returnsClosePrevMktres1', 'returnsOpenPrevMktres1',\n                       'returnsClosePrevRaw10', 'returnsOpenPrevRaw10',\n                       'returnsClosePrevMktres10', 'returnsOpenPrevMktres10',\n                       'margin1',\n                     ]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a39df2876c4987cfa2fd2b55857167f7f66f958a"},"cell_type":"code","source":"# Drop columns not in use\nmarket_train = market_train[LSTM_COLUMNS_TO_USE]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"900b87ec6c043ae81c80365b3a3656dce2792ccc"},"cell_type":"code","source":"gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"27de2e30ce83148ecfc69959df00cc0f15ae6ef0"},"cell_type":"code","source":"# If the assetCode has no non-null values, then impute with column median\nmarket_train = market_train.fillna(market_train.median())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d618f9659442e858f797c27ce33a6a34ebabdc7f"},"cell_type":"code","source":"INFORMATION_COLS = ['time','assetCode','universe','returnsOpenNextMktres10','TARGET']\nINPUT_COLS = [f for f in market_train.columns if f not in INFORMATION_COLS]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"021795ed71300f7dbab39d44e1c72afd7e1ebb53"},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\nmarket_train[INPUT_COLS] = scaler.fit_transform(market_train[INPUT_COLS])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"381433b942a3817d8537a34197641c8f0af30971"},"cell_type":"code","source":"market_train.head(20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"19dcff9486bc1af3fb6dd0d8d89f1e7d2dd531d9"},"cell_type":"code","source":"# Adapted from: https://machinelearningmastery.com/multivariate-time-series-forecasting-lstms-keras/\n\ndef series_to_supervised(dataset, n_in=1):\n    data_X = []\n    data_time = []\n    data_assetCode = []\n    data_universe = []\n    data_returns = []\n    data_TARGET = []\n    \n    # input sequence (t-n, ... t-1, t)\n    for i in range(0, len(dataset)):\n        data_time.append(dataset[i][0])\n        data_assetCode.append(dataset[i][1])\n        data_universe.append(dataset[i][2])\n        data_returns.append(dataset[i][3])\n        data_TARGET.append(dataset[i][4])\n        to_append = np.append(np.zeros(shape=(max(0,n_in - 1 - i), 10)),(dataset[max(0, i - n_in+1):i+1, len(INFORMATION_COLS):]), axis=0)\n        data_X.append( to_append)\n        \n    return data_X, data_time, data_assetCode, data_universe, data_returns, data_TARGET","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"863bdcfb2d5cb9e539185af3fa9e2a2827d6d7f7"},"cell_type":"code","source":"LOOK_BACK = 15","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"93eafc98f26da4577c3ea6a2e4aa72f6050ce7c0"},"cell_type":"code","source":"# Create LSTM input for each assetCode individually and store in a huge list\nlstm_df_list = np.empty(shape=(market_train.shape[0],LOOK_BACK,10))\n#lstm_df_list = []\nthe_time = []\nthe_assetCode = []\nthe_universe = []\nthe_returns = []\nthe_TARGET = []\n\nrow_at = 0\n\n#for assetCode in ['AA.N','ABAX.O']:#tqdm(market_train['assetCode'].unique()[1:3]):\nfor i in tqdm(market_train.groupby('assetCode')['time'].count().reset_index().values):\n    res = series_to_supervised(market_train.loc[market_train['assetCode']==i[0]].values, n_in=LOOK_BACK)\n    #lstm_df_list = np.append(lstm_df_list, np.array(res[0]), axis=0)\n    #lstm_df_list.append(np.array(res[0]))\n    lstm_df_list[row_at:row_at+i[1]] = np.array(res[0])\n    row_at = row_at + i[1]\n    the_time.append(res[1])\n    the_assetCode.append(res[2])\n    the_universe.append(res[3])\n    the_returns.append(res[4])\n    the_TARGET.append(res[5])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"08fa97a1496d1c341adbd260f81b842c85cfd53e"},"cell_type":"code","source":"# FLATTEN LISTS\nimport itertools\n\nthe_time = list(itertools.chain.from_iterable(the_time))\nthe_assetCode = list(itertools.chain.from_iterable(the_assetCode))\nthe_universe = list(itertools.chain.from_iterable(the_universe))\nthe_returns = list(itertools.chain.from_iterable(the_returns))\nthe_TARGET = list(itertools.chain.from_iterable(the_TARGET))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4bc6bc6423aa98573aae6a3627859811ee9e95d8"},"cell_type":"code","source":"the_TARGET[-5:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9bbb8c80ae5882f85775d089a36877cfbfe29809"},"cell_type":"code","source":"del market_train\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c9394b6dd7f5d7b331623d08dadf03b01c51d6f2"},"cell_type":"code","source":"print(lstm_df_list.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e4ef548a8925d7007ec698de88d1f36d270bbd53"},"cell_type":"code","source":"from keras import callbacks","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1de0a1dab77858b9bcaf7fbec6cec6c870ff8134"},"cell_type":"code","source":"# https://medium.com/@thongonary/how-to-compute-f1-score-for-each-epoch-in-keras-a1acd17715a2\n\nclass Metrics(callbacks.Callback):\n    def on_train_begin(self, logs={}):\n        self._data = []\n\n    def on_epoch_end(self, batch, logs={}):\n        X_val, y_val = self.validation_data[0], self.validation_data[1]\n        y_predict = (pd.DataFrame(model.predict(X_val)) * 2) - 1 # Need to convert it back to [-1, 1] instead of [0, 1]\n        _sigmascore = sigma_scorelstm(y_val, y_predict)\n        print(\" â€” sigmascore: %f\" % (_sigmascore))\n\n        self._data.append({\n            'val_sigmascore': _sigmascore\n        })\n        return\n\n    def get_data(self):\n        return self._data\n\nmetrics = Metrics()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"27384f74d2caa3f1b38db64b64964ac3a0370bf0"},"cell_type":"code","source":"train_index = [i for i in range(len(lstm_df_list))]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"823feabd7fb83110e5ba41c7266de630bc01c2b3"},"cell_type":"code","source":"def sigma_scorelstm(y_true, y_pred):\n        x_t_i = y_pred * pd.DataFrame([the_returns[i] for i in train_index]) * pd.DataFrame([the_universe[i] for i in train_index]) # Multiply my confidence by return multiplied by universe\n        data = pd.concat([pd.DataFrame([the_time[i] for i in train_index]), x_t_i], axis=1)\n        data.columns = ['day','x_t_i']\n        x_t = data.groupby('day').sum().values.flatten()\n        mean = np.mean(x_t)\n        std = np.std(x_t)\n        score_valid = mean / std\n        return score_valid","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"362254a9ac0bed25595ed6cf9c8cd891a52eb6a9"},"cell_type":"code","source":"trainX = np.array([lstm_df_list[i] for i in train_index])\ntrainY = (np.array([the_TARGET[i] for i in train_index]) + 1) / 2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"644d1c016f11368b68a682807093e0c372974554"},"cell_type":"code","source":"trainY[-5:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"04d43a3acbab111696687e4a89c22b11ebe51c79"},"cell_type":"code","source":"model = Sequential()\nmodel.add(GRU(50, return_sequences=True, input_shape=(LOOK_BACK, trainX.shape[2])))\nmodel.add(SpatialDropout1D(0.5))\nmodel.add(GlobalMaxPool1D())\nmodel.add(Dropout(0.50))\n\nmodel.add(Dense(128, activation='relu'))\nmodel.add(Dense(128, activation='relu'))\nmodel.add(Dropout(0.25))\n\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.compile(loss='binary_crossentropy', optimizer='rmsprop') # RMS prop is supposed to be better for recurrent neural networks.\n\nhistory = model.fit(trainX, trainY, epochs=2, batch_size=1028, validation_data=(trainX, trainY), verbose=2, shuffle=True, callbacks=[metrics])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"60e88f6c7c8c77729e982ff29c43ef72b55dc446"},"cell_type":"code","source":"plt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'valid'], loc='upper right')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"84be7317111beb0b070c83cc63240d5a4f1c62bb"},"cell_type":"code","source":"# Gain memory\ndel trainX, trainY\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"03dd5ed22489cf1da1d9fe86e3075861e3cc266e"},"cell_type":"markdown","source":"# Predictions:"},{"metadata":{"trusted":true,"_uuid":"8f584f77e9d00ec469b29cb1d1f7139e2f1164d5"},"cell_type":"code","source":"# You can only iterate through a result from `get_prediction_days()` once\n# so be careful not to lose it once you start iterating.\ndays = env.get_prediction_days()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"12f65f42c7cf045a5ae4dfd13ed63cc7306a6bf2"},"cell_type":"code","source":"# Correct LSTM columns to use\nLSTM_COLUMNS_TO_USE = [col for col in LSTM_COLUMNS_TO_USE if (col!='universe' and col!='returnsOpenNextMktres10' and col!='TARGET')]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"430a5a46f855d14b512694a48ef042b25b409962"},"cell_type":"code","source":"# Drop this\ntotal_market_obs_df[0].drop('TARGET', axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9b6ce1cf73139c5cc5e52bcb8df1810e0193dfe1"},"cell_type":"code","source":"# Adapted from: https://machinelearningmastery.com/multivariate-time-series-forecasting-lstms-keras/\n\ndef series_to_supervised(dataset, curdate, n_in=1):\n    data_X = []\n    \n    # input sequence (t-n, ... t-1, t)\n    for i in range(0, len(dataset)):\n        # Only create with the prediction date\n        if dataset[i][0]==curdate:\n            to_append = np.append(np.zeros(shape=(max(0,n_in - 1 - i), 10)),(dataset[max(0, i - n_in+1):i+1, 2:]), axis=0)\n            data_X.append( to_append)\n        \n    return data_X\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0d18248795e42ce53e96925b7d079b2f2e3e134e"},"cell_type":"code","source":"for (market_obs_df, _, predictions_template_df) in days:\n    #######################\n    # LGBM modeling:\n    \n    market_obs_df = market_obs_df.fillna(market_obs_df.median())\n    market_obs_df = market_obs_df.sort_values('assetCode')\n    \n    market_obs_df['time'] = pd.to_datetime(market_obs_df['time'].dt.date)\n        \n    # Feature Engineering\n    market_obs_df['margin1'] = market_obs_df['open'] / market_obs_df['close']\n    \n    # Save to history df\n    total_market_obs_df.append(market_obs_df)\n    history_df = pd.concat(total_market_obs_df[-(np.max(30)+1):]) # Store last 30 for assetCodes\n    \n    ###################################\n    # LSTM modeling:\n    \n    tmp = history_df[LSTM_COLUMNS_TO_USE]\n    \n    # If the assetCode has no non-null values, then impute with column median\n    tmp = tmp.fillna(tmp.median())\n    \n    # Scale\n    tmp[INPUT_COLS] = scaler.fit_transform(tmp[INPUT_COLS])\n    \n    # Create LSTM input for each assetCode individually and store in a huge list\n    lstm_df_list = np.empty(shape=(predictions_template_df.shape[0],LOOK_BACK,10))\n\n    row_at = 0\n\n    for asset in market_obs_df['assetCode'].unique():\n        res = series_to_supervised(tmp.loc[tmp['assetCode']==asset].values, curdate=tmp['time'].max(), n_in=LOOK_BACK)\n\n        lstm_df_list[row_at] = np.array(res)\n        row_at = row_at + 1\n        \n    trainX = np.array([lstm_df_list[i] for i in range(len(lstm_df_list))])\n    \n    yhat_lstm = model.predict(trainX)\n    yhat_lstm = yhat_lstm.flatten() # Flatten it\n    \n    yhat_lstm = pd.DataFrame(yhat_lstm)\n    preds = (yhat_lstm * 2) - 1\n    \n#     # Predict on Ensemble now\n#     ensemble = pd.concat([yhat_lgbm, yhat_goss, yhat_dart, yhat_lstm], axis=1)\n#     ensemble.columns = ['lgbm','goss','dart','lstm']\n\n#     preds = logreg.predict_proba(ensemble)[:,1]\n#     preds = (preds * 2) - 1 # Convert from [0,1] to [-1,1]\n    \n    predictions_template_df['confidenceValue'] = preds\n    env.predict(predictions_template_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f58ef910e3054d2fef037f7d4443885d2dfe7ba3"},"cell_type":"code","source":"env.write_submission_file()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a589cb120d9824873d80dd683f80f777f4b2904d"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}