{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom tqdm import tqdm\n\nfrom sklearn.model_selection import train_test_split\n\nimport transformers\nfrom transformers import BertModel, BertTokenizer, AdamW, get_linear_schedule_with_warmup","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**test whether the system has a GPU support and fix device variable accordingly**"},{"metadata":{"trusted":true},"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Set the random seeds for deterministic results.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"SEED = 1234\n\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\ntorch.cuda.manual_seed(SEED)\ntorch.backends.cudnn.deterministic = True","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = pd.read_csv(\"/kaggle/input/quora-question-pairs/train.csv.zip\")\ntrain_df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# remove Null Values\ntrain_df.dropna(inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Draw graph which shows number of words in both questions."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_sentences_lens = train_df['question1'].apply(lambda x: len(x.split(' '))).tolist()\ntrain_sentences_lens.extend(train_df['question2'].apply(lambda x: len(x.split(' '))).tolist())\nsns.distplot(train_sentences_lens)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we see from the graph, the number of cases where words counts greater than 40 is too small."},{"metadata":{"trusted":true},"cell_type":"code","source":"MAX_LEN = 40","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Draw pie chart which shows distribution of positive and negative examples"},{"metadata":{"trusted":true},"cell_type":"code","source":"def pie_chart(similar_questions_num, different_questions_num, set_type):\n    labels = 'Similiar', 'Different'\n    sizes = [similar_questions_num, different_questions_num]\n\n    fig1, ax1 = plt.subplots()\n    ax1.set_title(set_type)\n    ax1.pie(sizes, labels=labels, autopct='%1.2f%%', shadow=True, startangle=90)\n\n    plt.show()\n\nsimilar_samples_num = sum(train_df['is_duplicate'].values)\npie_chart(similar_samples_num, len(train_df['is_duplicate']) - similar_samples_num, 'train set')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The data is more or less balanced"},{"metadata":{},"cell_type":"markdown","source":"### See most frequently asked questions"},{"metadata":{"trusted":true},"cell_type":"code","source":"qids = pd.Series(list(train_df['qid1']) + list(train_df['qid2']))\n\nprint ('Unique Questions number: {}\\n'.format(len(np.unique(qids))))\n\nq_vals=qids.value_counts()[0:5]\nprint ('Top 5 most frequently asked questions: ')\n\nfor pair in q_vals.iteritems():\n    print(train_df.loc[train_df['qid2']==pair[0]]['question1'].head(1).values + \" count: \" + str(pair[1]))\n\nq_vals=q_vals.values","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Checking whether there are any repeated pair of questions"},{"metadata":{"trusted":true},"cell_type":"code","source":"duplicate_rows = train_df[train_df.duplicated(['qid1','qid2'])]\nprint (\"Number of duplicate questions : \", len(duplicate_rows))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Plot representing unique and repeated question counts"},{"metadata":{"trusted":true},"cell_type":"code","source":"x = [\"Unique\" , \"Repeated\"]\ny =  [len(np.unique(qids)), np.sum(qids.value_counts() > 1)]\n\nplt.figure(figsize=(10, 8))\nplt.title (\"Unique and Repeated questions counts\")\nsns.barplot(x,y)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Common words plots"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Number of common unique words in question1 and question2\ndef common_words(row):\n    q1_word_set = set(map(lambda word: word.lower().strip(), row['question1'].split(\" \")))\n    q2_word_set = set(map(lambda word: word.lower().strip(), row['question2'].split(\" \")))    \n    return 1.0 * len(q1_word_set & q2_word_set)\n\ntrain_df['common_words'] = train_df.apply(common_words, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15, 10))\n\nplt.subplot(1,2,2)\nsns.distplot(train_df[train_df['is_duplicate'] == 1]['common_words'][0:] , label = \"1\", color = 'red')\nsns.distplot(train_df[train_df['is_duplicate'] == 0]['common_words'][0:] , label = \"0\" , color = 'blue' )\n\nplt.subplot(1,2,1)\nsns.violinplot(x = 'is_duplicate', y = 'common_words', data = train_df[0:])\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"common words distributions more or less the same in both kind of question pairs"},{"metadata":{},"cell_type":"markdown","source":"### Shared words plots"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Number of common words between question1 and question2 divided by total words between both of them\ndef shared_words(row):\n    q1_word_set = set(map(lambda word: word.lower().strip(), row['question1'].split(\" \")))\n    q2_word_set = set(map(lambda word: word.lower().strip(), row['question2'].split(\" \")))    \n    return 1.0 * len(q1_word_set & q2_word_set) / (len(q1_word_set) + len(q2_word_set))    \n\ntrain_df['shared_words'] = train_df.apply(shared_words, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15, 10))\n\nplt.subplot(1,2,2)\nsns.distplot(train_df[train_df['is_duplicate'] == 1]['shared_words'][0:] , label = \"1\", color = 'red')\nsns.distplot(train_df[train_df['is_duplicate'] == 0]['shared_words'][0:] , label = \"0\" , color = 'green' )\n\nplt.subplot(1,2,1)\nsns.violinplot(x = 'is_duplicate', y = 'shared_words', data = train_df[0:])\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see from the graphs, the rate of shared words is on average higher in similar question pairs"},{"metadata":{},"cell_type":"markdown","source":"# Dataset Preparation"},{"metadata":{"trusted":true},"cell_type":"code","source":"BERT_VERSION = 'bert-base-uncased'\nPOOLED_OUTPUT_DIM = 768 ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer = BertTokenizer.from_pretrained(BERT_VERSION)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# split data to train and validation sets\ntrain_df, val_df = train_test_split(train_df, test_size=0.1)\ntrain_df = train_df.reset_index(drop=True)\nval_df = val_df.reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class BertDataSet:\n    def __init__(self, first_questions, second_questions, targets, tokenizer):\n        self.first_questions = first_questions\n        self.second_questions = second_questions\n        self.targets = targets\n        self.tokenizer = tokenizer\n        self.length = len(first_questions)\n        \n    def __len__(self):\n        return self.length\n\n    def __getitem__(self, item):\n        first_question = str(self.first_questions[item])\n        second_question = str(self.second_questions[item])\n\n        # removes extra white spaces from questions\n        first_question = \" \".join(first_question.split())\n        second_question = \" \".join(second_question.split())\n        \n        ### [CLS] question1 [SEP] questions2 [SEP] ... [PAD]\n        inputs = self.tokenizer.encode_plus(\n            first_question,\n            second_question,\n            add_special_tokens=True,\n            padding='max_length',\n            max_length=2 * MAX_LEN + 3, # max length of 2 questions and 3 special tokens\n            truncation=True   \n        )\n        \n        # return targets 0, when using data set in testing and targets are none\n        return {\n            \"ids\": torch.tensor(inputs[\"input_ids\"], dtype=torch.long),\n            \"mask\": torch.tensor(inputs[\"attention_mask\"], dtype=torch.long),\n            \"token_type_ids\": torch.tensor(inputs[\"token_type_ids\"], dtype=torch.long),\n            \"targets\": torch.tensor(int(self.targets[item]), dtype=torch.long) if self.targets is not None else 0\n        }\n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# creates dataset and returns dataloader of it\ndef get_data_loader(df, targets, batch_size, shuffle, tokenizer):\n    dataset = BertDataSet(\n        first_questions=df[\"question1\"].values,\n        second_questions=df[\"question2\"].values,\n        targets=targets,\n        tokenizer=tokenizer\n    )\n    \n    data_loader = torch.utils.data.DataLoader(\n        dataset,\n        batch_size = batch_size,\n        shuffle=shuffle\n    )\n    \n    return data_loader","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# training batch size we gonna use throughout this notebook.\nBS = 128","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# create data loaders of training and validation data.\ntrain_data_loader = get_data_loader(\n    df=train_df,\n    targets=train_df[\"is_duplicate\"].values,\n    batch_size=BS,\n    shuffle=True,\n    tokenizer=tokenizer\n)\n\nval_data_loader = get_data_loader(\n    df=val_df,\n    targets=val_df[\"is_duplicate\"].values,\n    batch_size=4 * BS,\n    shuffle=True,\n    tokenizer=tokenizer\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"class BertModel(nn.Module):\n    def __init__(self, bert_path):\n        super(BertModel, self).__init__()\n        self.bert_path = bert_path\n        self.bert = transformers.BertModel.from_pretrained(self.bert_path)\n        self.dropout = nn.Dropout(0.3)\n        self.out = nn.Linear(POOLED_OUTPUT_DIM, 1)\n\n    def forward(self, ids, mask, token_type_ids):\n        _, pooled = self.bert(ids, attention_mask=mask,token_type_ids=token_type_ids)\n        \n        # add dropout to prevent overfitting.\n        pooled = self.dropout(pooled) \n        return self.out(pooled)\n\nmodel = BertModel(BERT_VERSION).to(device)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Training"},{"metadata":{"trusted":true},"cell_type":"code","source":"# loss function is simple binary cross entropy loss\n# need sigmoid to put probabilities in [0,1] interval\ndef loss_fn(outputs, targets):\n    outputs = torch.squeeze(outputs)\n    return nn.BCELoss()(nn.Sigmoid()(outputs), targets)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# computes perplexity on validation data\ndef calculate_perplexity(data_loader, model, device):\n    model.eval()\n    \n    # tells Pytorch not to store values of intermediate computations for backward pass because we not gonna need gradients.\n    with torch.no_grad():\n        total_loss = 0\n        for batch in data_loader:\n            ids = batch[\"ids\"].to(device, dtype=torch.long)\n            mask = batch[\"mask\"].to(device, dtype=torch.long)\n            token_type_ids = batch[\"token_type_ids\"].to(device, dtype=torch.long)\n            targets = batch[\"targets\"].to(device, dtype=torch.float)\n\n            outputs = model(ids=ids, mask=mask, token_type_ids=token_type_ids)\n            total_loss += loss_fn(outputs, targets).item()\n            \n    model.train()\n\n    return np.exp(total_loss / len(data_loader))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_loop(epochs, train_data_loader, val_data_loader, model, optimizer, device, scheduler=None):\n    it = 1\n    total_loss = 0\n    curr_perplexity = None\n    perplexity = None\n    \n    model.train()\n    for epoch in range(epochs):\n        print('Epoch: ', epoch + 1)\n        for batch in train_data_loader:\n            ids = batch[\"ids\"].to(device, dtype=torch.long)\n            mask = batch[\"mask\"].to(device, dtype=torch.long)\n            token_type_ids = batch[\"token_type_ids\"].to(device, dtype=torch.long)\n            targets = batch[\"targets\"].to(device, dtype=torch.float)\n\n            optimizer.zero_grad()\n            \n            # do forward pass, will save intermediate computations of the graph for later backprop use.\n            outputs = model(ids, mask=mask, token_type_ids=token_type_ids)\n            \n            loss = loss_fn(outputs, targets)\n            total_loss += loss.item()\n            \n            # running backprop.\n            loss.backward()\n            \n            # doing gradient descent step.\n            optimizer.step()\n            \n            # we are logging current loss/perplexity in every 100 iteration\n            if it % 100 == 0:\n                \n                # computing validation set perplexity in every 500 iteration.\n                if it % 500 == 0:\n                    curr_perplexity = calculate_perplexity(val_data_loader, model, device)\n                    \n                    if scheduler is not None:\n                        scheduler.step()\n\n                    # making checkpoint of best model weights.\n                    if not perplexity or curr_perplexity < perplexity:\n                        torch.save(model.state_dict(), 'saved_model')\n                        perplexity = curr_perplexity\n\n                print('| Iter', it, '| Avg Train Loss', total_loss / 100, '| Dev Perplexity', curr_perplexity)\n                total_loss = 0\n\n            it += 1\n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def run(model, train_df, device, train_data_loader, val_data_loader):\n    EPOCHS = 1\n    \n    lr = 3e-5\n    num_training_steps = int(len(train_data_loader) * EPOCHS)\n    optimizer = AdamW(model.parameters(), lr=lr)\n    scheduler = get_linear_schedule_with_warmup(\n        optimizer,\n        num_warmup_steps=0,\n        num_training_steps=num_training_steps\n    )\n    \n    \n    train_loop(EPOCHS, train_data_loader, val_data_loader,  model, optimizer, device, scheduler)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"run(model, train_df, device, train_data_loader, val_data_loader)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Testing"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df = pd.read_csv(\"/kaggle/input/quora-question-pairs/test.csv\")\ntest_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# this function returns probabilities for every test case.\ndef test(model, test_df, device):\n    predictions = torch.empty(0).to(device, dtype=torch.float)\n    \n    test_dataset = BertDataSet(\n        first_questions=test_df[\"question1\"].values,\n        second_questions=test_df[\"question2\"].values,\n        targets=None,\n        tokenizer=tokenizer\n    )\n    \n    test_data_loader = torch.utils.data.DataLoader(\n        test_dataset,\n        batch_size=512\n    )\n    \n    with torch.no_grad():\n        model.eval()\n        for batch in tqdm(test_data_loader):\n            ids = batch[\"ids\"]\n            mask = batch[\"mask\"]\n            token_type_ids = batch[\"token_type_ids\"]\n\n            ids = ids.to(device, dtype=torch.long)\n            mask = mask.to(device, dtype=torch.long)\n            token_type_ids = token_type_ids.to(device, dtype=torch.long)\n\n            outputs = model(ids=ids, mask=mask, token_type_ids=token_type_ids)\n            predictions = torch.cat((predictions, nn.Sigmoid()(outputs)))\n    \n    return predictions.cpu().numpy().squeeze()\n\npredictions = test(model, test_df, device)\nlen(predictions)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# write down answers in is_duplicate column.\ntest_df['is_duplicate'] = predictions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# save results to submission.csv.\ntest_df[['test_id', 'is_duplicate']].to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Evaluation"},{"metadata":{"trusted":true},"cell_type":"code","source":"# prints if two questions is similar and score of confidence\ndef eval(model, tokenizer, first_question, second_question, device):\n    inputs = tokenizer.encode_plus(\n        first_question,\n        second_question,\n        add_special_tokens=True,\n    )\n\n    ids = torch.tensor([inputs[\"input_ids\"]], dtype=torch.long).to(device, dtype=torch.long)\n    mask = torch.tensor([inputs[\"attention_mask\"]], dtype=torch.long).to(device, dtype=torch.long)\n    token_type_ids = torch.tensor([inputs[\"token_type_ids\"]], dtype=torch.long).to(device, dtype=torch.long)\n\n    with torch.no_grad():\n        model.eval()\n        output = model(ids=ids, mask=mask, token_type_ids=token_type_ids)\n        prob = nn.Sigmoid()(output).item()\n\n        print(\"questions [{}] and [{}] are {} with score {}\".format(first_question, second_question, 'similar' if prob > 0.5 else 'not similar', prob))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# change questions to test model\nfirst_question = \"how to register on hackerrank with google account?\"\nsecond_question = \"Can I sign using google account on hackerrank?\"\n\neval(model, tokenizer, first_question, second_question, device)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}