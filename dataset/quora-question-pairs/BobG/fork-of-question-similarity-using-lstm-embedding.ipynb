{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"47f4c14e-ce61-8508-98dc-3476e71e459d"},"source":""},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"11f68ab6-9ffa-fec5-3f76-2d351c236692"},"outputs":[],"source":"import numpy as np\nimport pandas as pd"},{"cell_type":"markdown","metadata":{"_cell_guid":"5fa9104a-6208-4724-27dc-dd7ad4b48272"},"source":"## Read Data ##"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"eca0de69-55e1-1cd8-137a-537e4d2d4c10"},"outputs":[],"source":"df_train = pd.read_csv('../input/train.csv', encoding='utf-8')\ndf_train['id'] = df_train['id'].apply(str)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"4b58843b-6626-49f5-68fa-ee55ca509f98"},"outputs":[],"source":"df_all = df_train.head(10000)\nprint('only use 10K TRAINING data. Try to overfit this small amout of data (ro run the model fast)')\n\ndf_all['question1'].fillna('', inplace=True)\ndf_all['question2'].fillna('', inplace=True)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"9fc37d66-e129-2544-144d-5e9186426171"},"outputs":[],"source":""},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"5c77b206-7b3c-c59b-e6d9-f84cb1eed3c5"},"outputs":[],"source":"#df_test = pd.read_csv('../input/test.csv', encoding='utf-8')\n#df_test['test_id'] = df_test['test_id'].apply(str)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"55e0097d-08c1-5bcf-42f4-32240ee34fbb"},"outputs":[],"source":"#df_all = pd.concat((df_train, df_test))\n#df_all['question1'].fillna('', inplace=True)\n#df_all['question2'].fillna('', inplace=True)"},{"cell_type":"markdown","metadata":{"_cell_guid":"94aaae1c-c081-b41d-d1fc-e8091406c9f6"},"source":"## Create Vocab ##"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"eb60bcdc-0345-d72b-ebe2-611a964b8727"},"outputs":[],"source":"from sklearn.feature_extraction.text import CountVectorizer\nimport itertools"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"0e2695cb-4e6f-e7af-4087-82acfd8229b2"},"outputs":[],"source":"counts_vectorizer = CountVectorizer(max_features=10000-1).fit(\n    itertools.chain(df_all['question1'], df_all['question2']))\nother_index = len(counts_vectorizer.vocabulary_)"},{"cell_type":"markdown","metadata":{"_cell_guid":"f1114921-8527-437c-c980-30a04a0a9108"},"source":"##Prep Data##"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"ca2f2087-8f6c-787e-5042-c5e379c1a131"},"outputs":[],"source":"import re\nfrom keras.preprocessing.sequence import pad_sequences\nfrom sklearn.model_selection import train_test_split"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f22db32e-2d45-65fb-6847-59ad7020f854"},"outputs":[],"source":"words_tokenizer = re.compile(counts_vectorizer.token_pattern)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"1bafb51f-d0c1-1342-dc80-9225b028beec"},"outputs":[],"source":"def create_padded_seqs(texts, max_len=10):\n    seqs = texts.apply(lambda s: \n        [counts_vectorizer.vocabulary_[w] if w in counts_vectorizer.vocabulary_ else other_index\n         for w in words_tokenizer.findall(s.lower())])\n    return pad_sequences(seqs, maxlen=max_len)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"8afdab6e-f05c-4e6c-0d6d-e395861b5d4e"},"outputs":[],"source":"X1_train, X1_val, X2_train, X2_val, y_train, y_val = \\\n    train_test_split(create_padded_seqs(df_all[df_all['id'].notnull()]['question1']), \n                     create_padded_seqs(df_all[df_all['id'].notnull()]['question2']),\n                     df_all[df_all['id'].notnull()]['is_duplicate'].values,\n                     stratify=df_all[df_all['id'].notnull()]['is_duplicate'].values,\n                     test_size=0.3, random_state=1989)"},{"cell_type":"markdown","metadata":{"_cell_guid":"7e5e7c17-a66a-acd8-3bc2-a5b4c643f5b4"},"source":"##Training##"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b9da33a1-8f1c-c6a2-e1e7-40656be2cbe7"},"outputs":[],"source":"import keras.layers as lyr\nfrom keras.models import Model"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"89ea4425-bcb6-b794-784c-85824b590611"},"outputs":[],"source":"X1_train.shape[1:], X2_train.shape"},{"cell_type":"markdown","metadata":{"_cell_guid":"641a9d24-b566-21c9-f8e3-0b05b5d1f39f"},"source":"# HongBo: I was going to try the codes as follows (use inner product and Sigmoid), but there is error popping out :"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"1becec26-2df1-b38c-afc5-6a053ab47382"},"outputs":[],"source":"\n#input1_tensor = lyr.Input(X1_train.shape[1:])\n#input2_tensor = lyr.Input(X2_train.shape[1:])\n#words_embedding_layer = lyr.Embedding(X1_train.max() + 1, 100)\n#seq_embedding_layer = lyr.LSTM(256, activation='tanh')\n#seq_embedding = lambda tensor: seq_embedding_layer(words_embedding_layer(tensor))\n#tmp1 = seq_embedding(input1_tensor)\n#tmp2 = seq_embedding(input2_tensor)\n#print(tmp1.shape, tmp2.shape, input1_tensor.shape, input2_tensor.shape)\n#lyr.Dot(axes=1)([tmp1, tmp2])\n#dot_layer = lyr.Dot(axes=1)([tmp1, tmp2])\n#output_layer = lyr.Activation(\"sigmoid\")(dot_layer)\n#output_layer.shape\n#model = Model([input1_tensor, input2_tensor], ouput_layer)\n#model.compile(loss='binary_crossentropy', optimizer='adam')\n#model.summary()\n"},{"cell_type":"markdown","metadata":{"_cell_guid":"65b0e78b-ac8d-42e2-3a59-35d521963d69"},"source":"# HongBo: below is the original other people's code\n"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"8c863ad7-276d-5d38-bad5-205b29547f0d"},"outputs":[],"source":"input1_tensor = lyr.Input(X1_train.shape[1:])\ninput2_tensor = lyr.Input(X2_train.shape[1:])\nwords_embedding_layer = lyr.Embedding(X1_train.max() + 1, 100)\nseq_embedding_layer = lyr.LSTM(256, activation='tanh')\nseq_embedding = lambda tensor: seq_embedding_layer(words_embedding_layer(tensor))\nmerge_layer = lyr.multiply([seq_embedding(input1_tensor), seq_embedding(input2_tensor)])\ndense1_layer = lyr.Dense(16, activation='sigmoid')(merge_layer)\nouput_layer = lyr.Dense(1, activation='sigmoid')(dense1_layer)\nmodel = Model([input1_tensor, input2_tensor], ouput_layer)\nmodel.compile(loss='binary_crossentropy', optimizer='adam')\nmodel.summary()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"852640d6-6318-2cf6-1b1e-6409f36ec951"},"outputs":[],"source":"model.fit([X1_train, X2_train], y_train, \n          validation_data=([X1_val, X2_val], y_val), \n          batch_size=128, epochs=6, verbose=2)"},{"cell_type":"markdown","metadata":{"_cell_guid":"b5d7e836-cd1a-05cb-7741-d967a2bf34a1"},"source":"##Extract Features From Model##"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"37dbfa10-9aa3-fe0d-95f2-8b1f2aeda32f"},"outputs":[],"source":"features_model = Model([input1_tensor, input2_tensor], merge_layer)\nfeatures_model.compile(loss='mse', optimizer='adam')"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a59d052f-1eca-7693-cafe-ff4b899e469b"},"outputs":[],"source":"F_train = features_model.predict([X1_train, X2_train], batch_size=128)\nF_val = features_model.predict([X1_val, X2_val], batch_size=128)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"5d807d9c-d1a3-3b7a-445b-9415769e0235"},"outputs":[],"source":"F_train.shape, F_val.shape"},{"cell_type":"markdown","metadata":{"_cell_guid":"d47649ca-f55a-1528-86a1-bc465b85189d"},"source":"##Train XGBoost##"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a9e938f7-aa89-a0e0-298c-721763c198ae"},"outputs":[],"source":"import xgboost as xgb"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"46f28c7b-318e-fb0c-49d3-fa91b1967086"},"outputs":[],"source":"dTrain = xgb.DMatrix(F_train, label=y_train)\ndVal = xgb.DMatrix(F_val, label=y_val)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"36498229-1969-8424-79c9-6e7e9ddd7ba3"},"outputs":[],"source":"xgb_params = {\n    'objective': 'binary:logistic',\n    'booster': 'gbtree',\n    'eval_metric': 'logloss',\n    'eta': 0.1, \n    'max_depth': 9,\n    'subsample': 0.9,\n    'colsample_bytree': 1 / F_train.shape[1]**0.5,\n    'min_child_weight': 5,\n    'silent': 1\n}\nbst = xgb.train(xgb_params, dTrain, 1000,  [(dTrain,'train'), (dVal,'val')], \n                verbose_eval=10, early_stopping_rounds=10)"},{"cell_type":"markdown","metadata":{"_cell_guid":"eb0f298c-7513-d3df-0947-aa8ca6d5bd87"},"source":"##Predict Test##"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a45ad89e-d81d-26a5-48e3-4c7e7487dd5a"},"outputs":[],"source":"X1_test = create_padded_seqs(df_all[df_all['test_id'].notnull()]['question1'])\nX2_test = create_padded_seqs(df_all[df_all['test_id'].notnull()]['question2'])"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"7ebf03ed-1ccf-22f5-c296-1b307adb5de8"},"outputs":[],"source":"F_test = features_model.predict([X1_test, X2_test], batch_size=128)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"226069f1-e7e6-2559-593f-80b7326e7c1c"},"outputs":[],"source":"dTest = xgb.DMatrix(F_test)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"832684e5-e1d1-235b-bcfc-52e0c19832fa"},"outputs":[],"source":"df_sub = pd.DataFrame({\n        'test_id': df_all[df_all['test_id'].notnull()]['test_id'].values,\n        'is_duplicate': bst.predict(dTest, ntree_limit=bst.best_ntree_limit)\n    }).set_index('test_id')"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"119ad0de-8aea-b97a-7816-10b671d8fbc9"},"outputs":[],"source":"df_sub.head()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"89c55e55-46f4-8728-bd4e-7abff5ea9b9a"},"outputs":[],"source":"df_sub['is_duplicate'].hist(bins=100)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f48ad5db-a1d7-b97a-8052-269e7ce43160"},"outputs":[],"source":""},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"06353662-65fd-2447-967a-b7ba82580bd9"},"outputs":[],"source":""}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":0}