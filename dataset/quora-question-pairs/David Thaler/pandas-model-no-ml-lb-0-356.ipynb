{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"305ea1c8-ce79-7692-52ba-b644e8116ecb"},"source":"## Word Share Model\nThe word_match_share feature is surprisingly predictive. In this notebook, we make a quick Pandas-only model that gets 0.356 LB without machine learning. It just groups on binned word_match_share values and then averages the label."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"03cfee2f-da7e-31a9-a98f-b3730710de15"},"outputs":[],"source":"import numpy as np\nimport pandas as pd\ntr = pd.read_csv('../input/train.csv')\nte = pd.read_csv('../input/test.csv')\nfrom nltk.corpus import stopwords\nSCALE = 0.3627"},{"cell_type":"markdown","metadata":{"_cell_guid":"ff61eb3b-1085-d9b9-93b5-5207f5f84bb3"},"source":"As noted in the kernel [How many 1's are in the public LB][1], the mean label on the test set is about 0.175, which is quite different from the training mean of 0.369. The constant SCALE above is an odds ratio. We use it to shift the mean label of the predictions from that of the training set to that of the test set. This is needed because the evaluation metric for this competition is log loss, which is sensitive to the mean prediction.       \n$$SCALE = \\frac{\\frac{y_{te}}{1 - y_{te}}}{\\frac{y_{tr}}{1 - y_{tr}}}$$    \nWhere y_te is the mean test label and y_tr is the mean training set label.\n\n\n  [1]: https://www.kaggle.com/davidthaler/quora-question-pairs/how-many-1-s-are-in-the-public-lb"},{"cell_type":"markdown","metadata":{"_cell_guid":"bd16c407-cc9c-fd75-f5f6-c633e6ed2fb2"},"source":"This model uses only one feature, which is the ever-popular word_match_share feature. The version below is Pandas-centric, but is equivalent to other versions available in the kernels."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"16612b7a-6df3-9ab3-fe55-9e547ddf6e52"},"outputs":[],"source":"def word_match_share(x):\n    '''\n    The much-loved word_match_share feature.\n\n    Args:\n        x: source data with question1/2\n        \n    Returns:\n        word_match_share as a pandas Series\n    '''\n    stops = set(stopwords.words('english'))\n    q1 = x.question1.fillna(' ').str.lower().str.split()\n    q2 = x.question2.fillna(' ').str.lower().str.split()\n    q1 = q1.map(lambda l : set(l) - stops)\n    q2 = q2.map(lambda l : set(l) - stops)\n    q = pd.DataFrame({'q1':q1, 'q2':q2})\n    q['len_inter'] = q.apply(lambda row : len(row['q1'] & row['q2']), axis=1)\n    q['len_tot'] = q.q1.map(len) + q.q2.map(len)\n    return (2 * q.len_inter / q.len_tot).fillna(0)"},{"cell_type":"markdown","metadata":{"_cell_guid":"c78f423b-f319-7737-3396-6e56738dc054"},"source":"To make our model, all we do is group on binned values of word_match_share, and then count the number of positives and total values, scaling the positives by SCALE to adjust the mean prediction. Then we compute binned word_match_share for the test set and apply those values, with a little bit of Laplace smoothing to get rid of extreme values based on low counts."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b8107f95-a313-160a-52b3-3cce9a0be3f8"},"outputs":[],"source":"def bin_model(tr, te, bins=100, vpos=1, vss=3):\n    '''\n    Runs a Pandas table model using the word_match_share feature.\n    \n    Args:\n        tr: pandas DataFrame with question1/2 in it\n        te: test data frame\n        bins: word shares are rounded to whole numbers after multiplying by bins.\n        v_pos: number of virtual positives for smoothing (can be non-integer)\n        vss: virtual sample size for smoothing (can be non-integer)\n        \n    Returns:\n        submission in a Pandas Data Frame.\n    '''\n    tr['word_share'] = word_match_share(tr)\n    tr['binned_share'] = (bins * tr.word_share).round()\n    pos = tr.groupby('binned_share').is_duplicate.sum()\n    cts = tr.binned_share.value_counts()\n    te['word_share'] = word_match_share(te)\n    te['binned_share'] = (bins * te.word_share).round()\n    te_pos = te.binned_share.map(pos, na_action='ignore').fillna(0)\n    te_cts = te.binned_share.map(cts, na_action='ignore').fillna(0)\n    prob = (te_pos + vpos) / (te_cts + vss)\n    odds = prob / (1 - prob)\n    scaled_odds = SCALE * odds\n    scaled_prob = scaled_odds / (1 + scaled_odds)\n    sub = te[['test_id']].copy()\n    sub['is_duplicate'] = scaled_prob\n    return sub"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"796b1e89-a0d7-387d-68bf-8729e1839c25"},"outputs":[],"source":"sub = bin_model(tr, te)\nsub.to_csv('no_ml_model.csv', index=False, float_format='%.6f')\nsub.head(10)"},{"cell_type":"markdown","metadata":{"_cell_guid":"72b87d61-b1ff-c86d-11cd-dee37080a4ac"},"source":"And that should do it... :)"}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":0}