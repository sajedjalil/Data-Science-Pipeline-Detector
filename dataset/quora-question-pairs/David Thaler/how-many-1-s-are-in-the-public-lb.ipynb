{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"1a04c9fb-fab1-7fa2-8ab3-b8c2a33ed3f8"},"source":"*Note (April 16): This notebook shows that the public test set has a lower mean label than the training set, but it does not say why, because I did not know. Ash Hafez has probably now found the reason why in the kernel [Temporal pattern in train response rates?][1]*\n\n\n  [1]: https://www.kaggle.com/ashhafez/quora-question-pairs/temporal-pattern-in-train-response-rates"},{"cell_type":"markdown","metadata":{"_cell_guid":"61c7c07b-99c3-c06d-6df2-694daeee717a"},"source":"*Note 2 (April 23): When I first wrote this, I only used 2 decimal places in the calculation. As noted in the [comments below][1] and in [another kernel][2], the correct percentage is more like 0.175. I have corrected this below. I have also typeset the single equation because the number of parenthesis was causing confusion.*\n\n\n  [1]: https://www.kaggle.com/davidthaler/quora-question-pairs/how-many-1-s-are-in-the-public-lb#177070\n  [2]: https://www.kaggle.com/badat0202/quora-question-pairs/estimate-distribution-of-data-in-lb"},{"cell_type":"markdown","metadata":{"_cell_guid":"96811c29-4d24-a823-7a3e-c36933b18dd3"},"source":"In the kernel [Exploratory Data Analysis][1], Anokas noted that there are a different number of positives in the training data than on the public test set. He also reported getting a score of 0.554 on the LB with a constant prediction at the training set mean of 0.369. That is enough information to determine the fraction of 1's in the public LB. So let's do that.\n\n  [1]: https://www.kaggle.com/anokas/quora-question-pairs/exploratory-data-analysis"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c909f80d-aece-6913-3f17-f2d756b3fc90"},"outputs":[],"source":"import numpy as np\nimport pandas as pd\nfrom matplotlib import pyplot as plt\nfrom sklearn.metrics import log_loss"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b9832a8f-4ef0-76cd-5d89-ae38463749b8"},"outputs":[],"source":"l = []\np = [0.37] * 1000\nfor r in range(1, 1000):\n    y = [1]*r + [0]*(1000-r)\n    l.append(log_loss(y, p))\nl = np.array(l)\nx = np.arange(0.1, 100, 0.1)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d13c2828-a339-0083-99b6-97efb845b89b"},"outputs":[],"source":"plt.plot(x, l, '_')\nplt.title('Log Loss vs. Pct. Positve with Constant Prediction of 0.37')\nplt.xlabel('% Positve in LB')\nplt.ylabel('Log Loss for Constant Prediction 0.37')\nplt.grid()\nplt.show()"},{"cell_type":"markdown","metadata":{"_cell_guid":"10d57ca5-6633-9c46-2ec6-36d42ae6802b"},"source":"From the graph, the log loss of 0.55 that Anokas got looks like it occurs at around 0.165 or 0.17. In fact, we can compute it directly. Using the log loss formula from [here][1], and using the fact that this is a constant prediction, we get:    \n\n$$r = \\frac{logloss + log(1-p)}{log\\big( \\frac{1-p}{p}\\big)}$$\n\nwhere r is the fraction of positives. In that expression, p and the logloss are known for Anokas' constant prediction of p=0.369, which gave loss of 0.554. That yields r of 0.174, about the same as the graph.\n\n  [1]: https://www.kaggle.com/wiki/LogarithmicLoss"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"6c7efcaa-4f1b-d8e4-d42e-2616ab28dd8f"},"outputs":[],"source":"test = pd.read_csv('../input/test.csv')\nsub = test[['test_id']].copy()\nsub['is_duplicate'] = 0.174\nsub.to_csv('constant_sub.csv', index=False)"},{"cell_type":"markdown","metadata":{"_cell_guid":"99a55440-4b9e-c59a-489a-f37d547e3d8a"},"source":"So that get's about 0.463 on the LB. Now the bigger question is: How many 1's are in the private LB? Is it just a fluke that the public LB has so many fewer 1's than train?"}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":0}