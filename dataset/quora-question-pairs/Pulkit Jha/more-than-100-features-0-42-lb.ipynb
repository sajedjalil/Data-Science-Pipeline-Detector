{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"0a56a262-8015-68e8-6c7a-6cb74a64b48f"},"outputs":[],"source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"1b2baecb-79aa-77fb-7480-62b50051f778"},"outputs":[],"source":"#read data\ntrain_df = pd.read_csv('../input/train.csv')\ntest_df  = pd.read_csv('../input/test.csv')\n\n#get an idea\ntrain_df.info()\ntest_df.info()\ntrain_df.head()\ntest_df.head()\n"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"704272fe-af92-cd58-6757-8fc03dfaffcb"},"outputs":[],"source":"#use csv file directly to create features for faster operatio\n# refrences : https://www.kaggle.com/anokas/data-analysis-xgboost-starter-0-35460-lb\n# https://www.kaggle.com/the1owl/matching-que-for-quora-end-to-end-0-33719-pb\n\nimport os, sys, re, csv\nimport pandas as pd\nimport numpy as np\nfrom nltk.corpus import stopwords\nfrom collections import Counter\nfrom difflib import SequenceMatcher\nimport math\nimport nltk\nfrom datetime import datetime\n\nsw = set(stopwords.words('english'))\n\ndef get_cosine(str1, str2):\n\n    vec1 = Counter(str1.split())\n    vec2 = Counter(str2.split())\n\n    intersection = set(vec1.keys()) & set(vec2.keys())\n    numerator = sum([vec1[x] * vec2[x] for x in intersection])\n\n    sum1 = sum([vec1[x]**2 for x in vec1.keys()])\n    sum2 = sum([vec2[x]**2 for x in vec2.keys()])\n    denominator = math.sqrt(sum1) * math.sqrt(sum2)\n\n    if not denominator:\n        return 0.0\n    else:\n        return float(numerator) / denominator\n\ndef DistJaccard(str1, str2):\n    str1 = set(str1.split())\n    str2 = set(str2.split())\n    numerator = len(str1 & str2)\n    denominator = len(str1 | str2)\n    if not denominator:\n        return 0.0\n    else:\n        return float(numerator) / denominator\n\ndef similar(a, b):\n    return SequenceMatcher(None, a, b).ratio()\n\ndef getAlphabetCount(word):\n    alphabet_dict = {}\n    txt = re.sub('[^A-Za-z]','',re.sub(' ','',word))\n    for aa in txt:\n        if aa in alphabet_dict:\n            alphabet_dict[aa] += 1\n        else:\n            alphabet_dict[aa] = 1\n    list1 = list(txt)\n    list2 = list('abcdefghijklmnopqrstuvwxyz')\n    diff = set(list2).symmetric_difference(list1)\n    for bb in diff:\n        alphabet_dict[bb] = 0\n    return alphabet_dict\n\ndef decisionMaker(a,b):\n    if(a==b):\n        return 1\n    else:\n        return 0\n\ndef word_match_share(txt1, txt2):\n    q1words = {}\n    q2words = {}\n    for word in txt1:\n        q1words[word] = 1\n    for word in txt2:\n        q2words[word] = 1\n    if len(q1words) == 0 or len(q2words) == 0:\n        return 0\n    shared_words_in_q1 = [w for w in q1words.keys() if w in q2words]\n    shared_words_in_q2 = [w for w in q2words.keys() if w in q1words]\n    match_score = (len(shared_words_in_q1) + len(shared_words_in_q2))*1.0/(len(q1words) + len(q2words))\n    return match_score\n\ndef get_weight(count, eps=10000, min_count=2):\n    if count < min_count:\n        return 0\n    else:\n        return 1.0 / (count + eps)\n\neps = 5000\n\ndef getAlphabetCount(word):\n    alphabet_dict = {}\n    txt = re.sub('[^A-Za-z]','',re.sub(' ','',word))\n    for aa in txt:\n        if aa in alphabet_dict:\n            alphabet_dict[aa] += 1\n        else:\n            alphabet_dict[aa] = 1\n    list1 = list(txt)\n    list2 = list('abcdefghijklmnopqrstuvwxyz')\n    diff = set(list2).symmetric_difference(list1)\n    for bb in diff:\n        alphabet_dict[bb] = 0\n    return alphabet_dict\n\ndef decisionMaker(a,b):\n    if(a==b):\n        return 1\n    else:\n        return 0\n\ndef word_match_share(txt1, txt2):\n    q1words = {}\n    q2words = {}\n    for word in txt1:\n        q1words[word] = 1\n    for word in txt2:\n        q2words[word] = 1\n    if len(q1words) == 0 or len(q2words) == 0:\n        return 0\n    shared_words_in_q1 = [w for w in q1words.keys() if w in q2words]\n    shared_words_in_q2 = [w for w in q2words.keys() if w in q1words]\n    match_score = (len(shared_words_in_q1) + len(shared_words_in_q2))*1.0/(len(q1words) + len(q2words))\n    return match_score\n\ndef get_weight(count, eps=10000, min_count=2):\n    if count < min_count:\n        return 0\n    else:\n        return 1.0 / (count + eps)\n\neps = 5000\n\n#This function is messed up - can be written much better\ndef tfidf_word_match_share(txt1, txt2):\n    tfidf = []\n    q1words = {}\n    q2words = {}\n    for word in txt1:\n        q1words[word] = 1\n    for word in txt2:\n        q2words[word] = 1\n    if len(q1words) == 0 or len(q1words) == 0 :\n        return 0,0,0,0,0,0,0,0,0,0,0\n    words = txt1 + txt2\n    counts = Counter(words)\n    weights = {word: get_weight(count) for word, count in counts.items()}\n    q1_tfidf_weights = [weights.get(w, 0) for w in q1words.keys()]\n    q2_tfidf_weights = [weights.get(w, 0) for w in q2words.keys()]\n    shared_weights = [weights.get(w, 0) for w in q1words.keys() if w in q2words] + [weights.get(w, 0) for w in q2words.keys() if w in q1words]\n    total_weights  = [weights.get(w, 0) for w in q1words] + [weights.get(w, 0) for w in q2words]\n    tfidfRatio = np.sum(shared_weights)*1.0 / np.sum(total_weights)\n    q1_tfidf_sum     = sum(q1_tfidf_weights)\n    q2_tfidf_sum     = sum(q2_tfidf_weights)\n    q1_tfidf_mean    = np.mean(q1_tfidf_weights)\n    q2_tfidf_mean    = np.mean(q2_tfidf_weights)\n    q1_tfidf_min     = min(q1_tfidf_weights)\n    q1_tfidf_max     = min(q1_tfidf_weights)\n    q1_tfidf_range   = q1_tfidf_max - q1_tfidf_min\n    q2_tfidf_min     = min(q2_tfidf_weights)\n    q2_tfidf_max     = min(q2_tfidf_weights)\n    q2_tfidf_range   = q2_tfidf_max - q2_tfidf_min\n    return q1_tfidf_sum, q2_tfidf_sum, q1_tfidf_mean, q2_tfidf_mean, q1_tfidf_min, q1_tfidf_max, q1_tfidf_range, q2_tfidf_min, q2_tfidf_max, q2_tfidf_range, q2_tfidf_range\n    \ndef shared_2gram(q1, q2):\n    q1_2gram = set([i for i in zip(q1.split(), q1.split()[1:])])\n    q2_2gram = set([i for i in zip(q2.split(), q2.split()[1:])])\n    shared_2gram = q1_2gram.intersection(q2_2gram)\n    if len(q1_2gram)==0 or len(q2_2gram) == 0:\n        return 0\n    return len(shared_2gram)*1.0/(len(q1_2gram) + len(q2_2gram))\n\ndef shared_3gram(q1, q2):\n    q1_3gram = set([i for i in zip(q1.split(), q1.split()[1:], q1.split()[2:])])\n    q2_3gram = set([i for i in zip(q2.split(), q2.split()[1:], q2.split()[2:])])\n    shared_3gram = q1_3gram.intersection(q2_3gram)\n    if len(q1_3gram)==0 or len(q2_3gram) == 0:\n        return 0\n    return len(shared_3gram)*1.0/(len(q1_3gram) + len(q2_3gram))\n\ndef avgWordLenDiff(q1, q2):\n    len_char_q1 = len(q1.replace(' ',''))\n    len_char_q2 = len(q2.replace(' ',''))\n    len_word_q1 = len(q1.split())\n    len_word_q2 = len(q2.split())\n    if len_char_q1 == 0 or len_char_q2 == 0:\n        return 0, 0, 0\n    avg_world_len_q1 = len_char_q1*1.0/len_word_q1\n    avg_world_len_q2 = len_char_q2*1.0/len_word_q2\n    return avg_world_len_q1, avg_world_len_q2, avg_world_len_q1-avg_world_len_q2\n\nstops = set(stopwords.words(\"english\"))\ndef avgStopWords(q1, q2):\n    q1stops_len = len(set(q1.split()).intersection(stops))\n    q2stops_len = len(set(q2.split()).intersection(stops))\n    q1word_len  = len(q1.split())\n    q2word_len  = len(q2.split())\n    if q1stops_len == 0 or q2stops_len == 0:\n        return 0,0,0\n    q1_r = q1stops_len*1.0/q1word_len\n    q2_r = q2stops_len*1.0/q2word_len\n    q_diff = q1_r - q2_r\n    return q1_r, q2_r, q_diff\n\n"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e7d9a914-0f19-f588-ef0a-c147c4558a06"},"outputs":[],"source":"def create_data(infile,outfile):\n    \n    if('train' in infile):\n        with open('../input/' + infile) as file:\n            reader = csv.reader(file, delimiter = ',')\n            i = 0\n            for line in reader:\n                q1 = [re.sub('[^A-Za-z0-9]','',x).lower() for x in line[3].split() if len(x)>1 and x not in sw]\n                q2 = [re.sub('[^A-Za-z0-9]','',x).lower() for x in line[4].split() if len(x)>1 and x not in sw]\n                if len(q1) > 1 :\n                    q1 = ' '.join([re.sub('[^A-Za-z0-9]','',x).lower() for x in q1 if len(x)>1 and x not in sw])\n                else:\n                    q1 = 'blank'\n                if len(q2):\n                    q2 = ' '.join([re.sub('[^A-Za-z0-9]','',x).lower() for x in q2 if len(x)>1 and x not in sw])\n                else:\n                    q2 = 'blank'\n                q1_len_with_space, q2_len_with_space  = len(q1), len(q2)\n                q1_len_without_space, q2_len_without_space  = len(re.sub(' ','',q1)), len(re.sub(' ','',q2))\n                q1_no_of_words, q2_no_of_words = len(q1.split()), len(q2.split())\n                q1_no_of_uniq_words, q2_no_of_uniq_words = len(set(q1.split())), len(set(q2.split()))\n\n                len_with_space_ind = decisionMaker(q1_len_with_space, q2_len_with_space)\n                len_without_space_ind = decisionMaker(q1_len_without_space, q2_len_without_space)\n                no_of_words_ind = decisionMaker(q2_no_of_words, q2_no_of_words)\n                no_of_uniq_words_ind = decisionMaker(q2_no_of_uniq_words, q2_no_of_uniq_words)\n\n                diff_len_with_space = q1_len_with_space - q2_len_with_space\n                diff_len_without_space = q1_len_without_space - q2_len_without_space\n                diff_no_of_words = q1_no_of_words - q2_no_of_words\n                diff_no_of_uniq_words = q1_no_of_uniq_words - q2_no_of_uniq_words\n\n                alphabetCountDictQ1 = getAlphabetCount(q1)\n                alphabetCountDictQ2 = getAlphabetCount(q2)\n                q1_a, q2_a = alphabetCountDictQ1['a'], alphabetCountDictQ2['a']\n                q1_b, q2_b = alphabetCountDictQ1['b'], alphabetCountDictQ2['b']\n                q1_c, q2_c = alphabetCountDictQ1['c'], alphabetCountDictQ2['c']\n                q1_d, q2_d = alphabetCountDictQ1['d'], alphabetCountDictQ2['d']\n                q1_e, q2_e = alphabetCountDictQ1['e'], alphabetCountDictQ2['e']\n                q1_f, q2_f = alphabetCountDictQ1['f'], alphabetCountDictQ2['f']\n                q1_g, q2_g = alphabetCountDictQ1['g'], alphabetCountDictQ2['g']\n                q1_h, q2_h = alphabetCountDictQ1['h'], alphabetCountDictQ2['h']\n                q1_i, q2_i = alphabetCountDictQ1['i'], alphabetCountDictQ2['i']\n                q1_j, q2_j = alphabetCountDictQ1['j'], alphabetCountDictQ2['j']\n                q1_k, q2_k = alphabetCountDictQ1['k'], alphabetCountDictQ2['k']\n                q1_l, q2_l = alphabetCountDictQ1['l'], alphabetCountDictQ2['l']\n                q1_m, q2_m = alphabetCountDictQ1['m'], alphabetCountDictQ2['m']\n                q1_n, q2_n = alphabetCountDictQ1['n'], alphabetCountDictQ2['n']\n                q1_o, q2_o = alphabetCountDictQ1['o'], alphabetCountDictQ2['o']\n                q1_p, q2_p = alphabetCountDictQ1['p'], alphabetCountDictQ2['p']\n                q1_q, q2_q = alphabetCountDictQ1['q'], alphabetCountDictQ2['q']\n                q1_r, q2_r = alphabetCountDictQ1['r'], alphabetCountDictQ2['r']\n                q1_s, q2_s = alphabetCountDictQ1['s'], alphabetCountDictQ2['s']\n                q1_t, q2_t = alphabetCountDictQ1['t'], alphabetCountDictQ2['t']\n                q1_u, q2_u = alphabetCountDictQ1['u'], alphabetCountDictQ2['u']\n                q1_v, q2_v = alphabetCountDictQ1['v'], alphabetCountDictQ2['v']\n                q1_w, q2_w = alphabetCountDictQ1['w'], alphabetCountDictQ2['w']\n                q1_x, q2_x = alphabetCountDictQ1['x'], alphabetCountDictQ2['x']\n                q1_y, q2_y = alphabetCountDictQ1['y'], alphabetCountDictQ2['y']\n                q1_z, q2_z = alphabetCountDictQ1['z'], alphabetCountDictQ2['z']\n\n                a_count_ind = decisionMaker(q1_a, q2_a)\n                b_count_ind = decisionMaker(q1_b, q2_b)\n                c_count_ind = decisionMaker(q1_c, q2_c)\n                d_count_ind = decisionMaker(q1_d, q2_d)\n                e_count_ind = decisionMaker(q1_e, q2_e)\n                f_count_ind = decisionMaker(q1_f, q2_f)\n                g_count_ind = decisionMaker(q1_g, q2_g)\n                h_count_ind = decisionMaker(q1_h, q2_h)\n                i_count_ind = decisionMaker(q1_i, q2_i)\n                j_count_ind = decisionMaker(q1_j, q2_j)\n                k_count_ind = decisionMaker(q1_k, q2_k)\n                l_count_ind = decisionMaker(q1_l, q2_l)\n                m_count_ind = decisionMaker(q1_m, q2_m)\n                n_count_ind = decisionMaker(q1_n, q2_n)\n                o_count_ind = decisionMaker(q1_o, q2_o)\n                p_count_ind = decisionMaker(q1_p, q2_p)\n                q_count_ind = decisionMaker(q1_q, q2_q)\n                r_count_ind = decisionMaker(q1_r, q2_r)\n                s_count_ind = decisionMaker(q1_s, q2_s)\n                t_count_ind = decisionMaker(q1_t, q2_t)\n                u_count_ind = decisionMaker(q1_u, q2_u)\n                v_count_ind = decisionMaker(q1_v, q2_v)\n                w_count_ind = decisionMaker(q1_w, q2_w)\n                x_count_ind = decisionMaker(q1_x, q2_x)\n                y_count_ind = decisionMaker(q1_y, q2_y)\n                z_count_ind = decisionMaker(q1_z, q2_z)\n                \n                diff_a_count = q1_a - q2_a\n                diff_b_count = q1_b - q2_b\n                diff_c_count = q1_c - q2_c\n                diff_d_count = q1_d - q2_d\n                diff_e_count = q1_e - q2_e\n                diff_f_count = q1_f - q2_f\n                diff_g_count = q1_g - q2_g\n                diff_h_count = q1_h - q2_h\n                diff_i_count = q1_i - q2_i\n                diff_j_count = q1_j - q2_j\n                diff_k_count = q1_k - q2_k\n                diff_l_count = q1_l - q2_l\n                diff_m_count = q1_m - q2_m\n                diff_n_count = q1_n - q2_n\n                diff_o_count = q1_o - q2_o\n                diff_p_count = q1_p - q2_p\n                diff_q_count = q1_q - q2_q\n                diff_r_count = q1_r - q2_r\n                diff_s_count = q1_s - q2_s\n                diff_t_count = q1_t - q2_t\n                diff_u_count = q1_u - q2_u\n                diff_v_count = q1_v - q2_v\n                diff_w_count = q1_w - q2_w\n                diff_x_count = q1_x - q2_x\n                diff_y_count = q1_y - q2_y\n                diff_z_count = q1_z - q2_z\n\n                cos_sim = get_cosine(q1, q2)\n                jac_sim = DistJaccard(q1, q2)\n                seq_mat = similar(q1, q2)\n                #lav_dis = distance.levenshtein(q1, q2)\n                word_match = word_match_share(q1, q2)\n                tfidf_match = tfidf_word_match_share(q1, q2)[10]\n                shared_2grams = shared_2gram(q1, q2)\n                shared_3grams = shared_3gram(q1, q2)\n                avg_world_len_q1 = avgWordLenDiff(q1, q2)[0]\n                avg_world_len_q2 = avgWordLenDiff(q1, q2)[1]\n                avg_world_diff   = avgWordLenDiff(q1, q2)[2]\n                q1_stop_ratio    = avgStopWords(q1, q2)[0]\n                q2_stop_ratio    = avgStopWords(q1, q2)[1]\n                ratio_diff       = avgStopWords(q1, q2)[2]\n                caps_count_q1 = sum([1 for j in line[3] if j.isupper()])\n                caps_count_q2 = sum([1 for j in line[4] if j.isupper()])\n                caps_count_diff = caps_count_q1 - caps_count_q2\n                qmarks_q1 = sum([1 for j in line[3] if j=='?'])\n                qmarks_q2 = sum([1 for j in line[4] if j=='?'])\n                qmarks_diff = qmarks_q1 - qmarks_q2\n                fs_q1 = sum([1 for j in line[3] if j=='.'])\n                fs_q2 = sum([1 for j in line[4] if j=='.'])\n                fs_diff = qmarks_q1 - qmarks_q2\n                if(len(line[3])>1):\n                    first_caps_count_q1 = sum([1 if line[3][0].isupper() else 0])\n                else:\n                    first_caps_count_q1 = 0\n                if(len(line[4])>1):\n                    first_caps_count_q2 = sum([1 if line[4][0].isupper() else 0])\n                else:\n                    first_caps_count_q2 = 0\n                first_caps_count_diff = first_caps_count_q1 - first_caps_count_q2\n                numb_count_q1 = sum([1 for j in line[3] if j.isdigit()])\n                numb_count_q2 = sum([1 for j in line[4] if j.isdigit()])\n                numb_count_diff = numb_count_q1 - numb_count_q2\n                nouns_q1 = [w for w, t in nltk.pos_tag(nltk.word_tokenize(str(q1).lower())) if t[:1] in ['N']]\n                nouns_q2 = [w for w, t in nltk.pos_tag(nltk.word_tokenize(str(q2).lower())) if t[:1] in ['N']]\n                noun_count_q1 = len(nouns_q1)\n                noun_count_q2 = len(nouns_q2)\n                noun_match    = sum([1 for w in nouns_q1 if w in nouns_q2])\n                tfidf = tfidf_word_match_share(q1, q2)\n                q1_tfidf_sum = tfidf[0]\n                q2_tfidf_sum = tfidf_word_match_share(q1, q2)[1]\n                q1_tfidf_mean = tfidf_word_match_share(q1, q2)[2]\n                q2_tfidf_mean = tfidf_word_match_share(q1, q2)[3]\n                q1_tfidf_min = tfidf_word_match_share(q1, q2)[4]\n                q1_tfidf_max = tfidf_word_match_share(q1, q2)[5]\n                q1_tfidf_range = tfidf_word_match_share(q1, q2)[6]\n                q2_tfidf_min = tfidf_word_match_share(q1, q2)[7]\n                q2_tfidf_max = tfidf_word_match_share(q1, q2)[8]\n                q2_tfidf_range = tfidf_word_match_share(q1, q2)[9]\n\n    #outfile.write(line[0] + '^' + q1 + '^' + q2 + '^' + str(len_with_space_ind) + '^' + str(len_without_space_ind) + '^' + str(no_of_words_ind) + '^' + str(no_of_uniq_words_ind) + '^' + str(diff_len_with_space) + '^' + str(diff_len_without_space) + '^' + str(diff_no_of_words) + '^' +  str(diff_no_of_uniq_words) + '^' + str(a_count_ind) + '^' + str(b_count_ind) + '^' + str(c_count_ind) + '^' + str(d_count_ind) + '^' + str(e_count_ind) + '^' + str(f_count_ind) + '^' + str(g_count_ind) + '^' + str(h_count_ind) + '^' + str(i_count_ind) + '^' + str(j_count_ind) + '^' + str(k_count_ind) + '^' + str(l_count_ind) + '^' + str(m_count_ind) + '^' + str(n_count_ind) + '^' + str(o_count_ind) + '^' + str(p_count_ind) + '^' + str(q_count_ind) + '^' + str(r_count_ind) + '^' + str(s_count_ind) + '^' + str(t_count_ind) + '^' + str(u_count_ind) + '^' + str(v_count_ind) + '^' + str(w_count_ind) + '^' + str(x_count_ind) + '^' + str(y_count_ind) + '^' + str(z_count_ind) + '^' + str(diff_a_count) + '^' + str(diff_b_count) + '^' + str(diff_c_count) + '^' + str(diff_d_count) + '^' + str(diff_e_count) + '^' + str(diff_f_count) + '^' + str(diff_g_count) + '^' + str(diff_h_count) + '^' + str(diff_i_count) + '^' + str(diff_j_count) + '^' + str(diff_k_count) + '^' + str(diff_l_count) + '^' + str(diff_m_count) + '^' + str(diff_n_count) + '^' + str(diff_o_count) + '^' + str(diff_p_count) + '^' + str(diff_q_count) + '^' + str(diff_r_count) + '^' + str(diff_s_count) + '^' + str(diff_t_count) + '^' + str(diff_u_count) + '^' + str(diff_v_count) + '^' + str(diff_w_count) + '^' + str(diff_x_count) + '^' + str(diff_y_count) + '^' + str(diff_z_count) + '^' + str(cos_sim) + '^' + str(jac_sim) + '^' + str(seq_mat) + '^' + str(lav_dis) + '^' + str(word_match) + '^' + str(tfidf_match) + '^' + str(shared_2grams) + '^' + str(shared_3grams) + '^' + str(avg_world_len_q1) + '^' + str(avg_world_len_q2) + '^' + str(avg_world_diff) + '^' + str(q1_stop_ratio) + '^' + str(q1_stop_ratio) + '^' + str(ratio_diff) + '^' + str(caps_count_q1) + '^' + str(caps_count_q2) + '^' + str(caps_count_diff) + '^' + str(qmarks_q1) + '^' + str(qmarks_q2) + '^' + str(qmarks_diff) + '^' + str(fs_q1) + '^' + str(fs_q2) + '^' + str(qmarks_diff) + '^' + str(first_caps_count_q1) + '^' + str(first_caps_count_q2) + '^' + str(first_caps_count_diff) + str(numb_count_q1) + '^' + str(numb_count_q2) + '^' + str(numb_count_diff) + '^' + str(noun_count_q1) + '^' + str(noun_count_q2) + '^' + str(noun_match) + '^' + str(q1_tfidf_sum) + '^' + str(q2_tfidf_sum) + '^' + str(q1_tfidf_mean) + '^' + str(q2_tfidf_mean) + '^' + str(q1_tfidf_min) + '^' + str(q1_tfidf_max) + '^' + str(q1_tfidf_range) + '^' + str(q2_tfidf_min) + '^' + str(q2_tfidf_max) + '^' + str(q2_tfidf_range) + '^' + str(line[5]) + '\\n')              "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"4c6f89af-551b-6e27-64e9-afdcddaf4093"},"outputs":[],"source":"create_data('train.csv', 'train_ftrs.csv')"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"472c2ddd-6552-c860-57da-e3d2ee5e2c36"},"outputs":[],"source":""}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":0}