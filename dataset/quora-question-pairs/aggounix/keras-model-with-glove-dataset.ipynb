{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"613c7edb-fa0a-fc62-4822-690183dc4f14"},"outputs":[],"source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.utils.data_utils import get_file\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b7ea3ed0-05f4-a3d4-7bbb-cebf910a0779"},"outputs":[],"source":"data = pd.read_csv('../input/train.csv')\nquestion1 = []\nquestion2 = []\nis_duplicate = []\nquestion1 = data[\"question1\"].astype('str') \nquestion2 = data[\"question2\"].astype('str') \nis_duplicate = data[\"is_duplicate\"]\n\nprint (len(question1))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"ad9561d0-a49d-d157-1671-8c6f486ce24c"},"outputs":[],"source":"MAX_NB_WORDS = 200000\nMAX_SEQUENCE_LENGTH = 25\nEMBEDDING_DIM = 300\nquestions = question1 + question2\ntokenizer = Tokenizer(num_words=MAX_NB_WORDS)\ntokenizer.fit_on_texts(questions)\nquestion1_word_sequences = tokenizer.texts_to_sequences(question1)\nquestion2_word_sequences = tokenizer.texts_to_sequences(question2)\nword_index = tokenizer.word_index\n\nprint(\"Words in index: %d\" % len(word_index))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"beeb6673-724c-00dc-b6cd-d34f07caa449"},"outputs":[],"source":"from os.path import expanduser, exists\nfrom zipfile import ZipFile\nKERAS_DATASETS_DIR = expanduser('~/.keras/datasets/')\nGLOVE_ZIP_FILE_URL = 'http://nlp.stanford.edu/data/glove.840B.300d.zip'\nGLOVE_ZIP_FILE = 'glove.840B.300d.zip'\nGLOVE_FILE = 'glove.840B.300d.txt'"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"4f9b44f5-c1c7-beba-b2fc-ad4b767c9a88"},"outputs":[],"source":"# download \nif not exists(KERAS_DATASETS_DIR + GLOVE_ZIP_FILE):    \n    zipfile = ZipFile(get_file(GLOVE_ZIP_FILE, GLOVE_ZIP_FILE_URL))\n    zipfile.extract(GLOVE_FILE, path=KERAS_DATASETS_DIR)\n    \nprint(\"Processing\", GLOVE_FILE)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"fa0cd943-89b7-f9a1-fed4-df7746da16e7"},"outputs":[],"source":"from os.path import expanduser, exists\nfrom zipfile import ZipFile\nKERAS_DATASETS_DIR = expanduser('~/.keras/datasets/')\nGLOVE_ZIP_FILE_URL = 'http://nlp.stanford.edu/data/glove.840B.300d.zip'\nGLOVE_ZIP_FILE = 'glove.840B.300d.zip'\nGLOVE_FILE = 'glove.840B.300d.txt'\n"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"58d1c28b-d99f-3d37-3a84-efb4374dfb80"},"outputs":[],"source":"#Not possible to download on kaggle\n#if not exists(KERAS_DATASETS_DIR + GLOVE_ZIP_FILE):    \n#    zipfile = ZipFile(get_file(GLOVE_ZIP_FILE, GLOVE_ZIP_FILE_URL))\n#    zipfile.extract(GLOVE_FILE, path=KERAS_DATASETS_DIR)\n    \nprint(\"Processing\", GLOVE_FILE)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e563e55e-20cc-acf8-9992-7fbdf82027ad"},"outputs":[],"source":"embeddings_index = {}\n'''with open(KERAS_DATASETS_DIR + GLOVE_FILE, encoding='utf-8') as f:\n    for line in f:\n        values = line.split(' ')\n        word = values[0]\n        embedding = np.asarray(values[1:], dtype='float32')\n        embeddings_index[word] = embedding'''\n\nprint('Word embeddings: %d' % len(embeddings_index))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"35b15c79-7246-5ecf-328b-ecf6a7107060"},"outputs":[],"source":"q1_data = pad_sequences(question1_word_sequences, maxlen=MAX_SEQUENCE_LENGTH)\nq2_data = pad_sequences(question2_word_sequences, maxlen=MAX_SEQUENCE_LENGTH)\nlabels = np.array(is_duplicate, dtype=int)\nprint('Shape of question1 data tensor:', q1_data.shape)\nprint('Shape of question2 data tensor:', q2_data.shape)\nprint('Shape of label tensor:', labels.shape)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"25e664e9-11c7-830f-21b4-a3918cd936d5"},"outputs":[],"source":"nb_words = min(MAX_NB_WORDS, len(word_index))\nword_embedding_matrix = np.zeros((nb_words + 1, EMBEDDING_DIM))\nfor word, i in word_index.items():\n    if i > MAX_NB_WORDS:\n        continue\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        word_embedding_matrix[i] = embedding_vector\n\nprint('Null word embeddings: %d' % np.sum(np.sum(word_embedding_matrix, axis=1) == 0))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"98740ac4-dadc-3e68-7c0e-b53d3b4c7358"},"outputs":[],"source":"from keras.models import Model\nfrom keras.layers import Input, TimeDistributed, Dense, Lambda, concatenate, Dropout, BatchNormalization\nfrom keras.layers.embeddings import Embedding\nfrom keras.regularizers import l2\nfrom keras.callbacks import Callback, ModelCheckpoint\nfrom keras import backend as K\nfrom sklearn.model_selection import train_test_split\nX = np.stack((q1_data, q2_data), axis=1)\ny = labels\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=TEST_SPLIT, random_state=RNG_SEED)\nQ1_train = X_train[:,0]\nQ2_train = X_train[:,1]\nQ1_test = X_test[:,0]\nQ2_test = X_test[:,1]"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d4579ea9-57cd-2ad4-313a-eb6a2a2810ca"},"outputs":[],"source":"question1 = Input(shape=(MAX_SEQUENCE_LENGTH,))\nquestion2 = Input(shape=(MAX_SEQUENCE_LENGTH,))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"ce3fc7c8-3980-b220-565d-b85ea43f3f01"},"outputs":[],"source":"MAX_SEQUENCE_LENGTH = 25\nEMBEDDING_DIM = 300\nVALIDATION_SPLIT = 0.1\nTEST_SPLIT = 0.1\nRNG_SEED = 13371447\nNB_EPOCHS = 25\nDROPOUT = 0.1\nBATCH_SIZE = 32\nq1 = Embedding(nb_words + 1, \n                 EMBEDDING_DIM, \n                 weights=[word_embedding_matrix], \n                 input_length=MAX_SEQUENCE_LENGTH, \n                 trainable=False)(question1)\nq1 = TimeDistributed(Dense(EMBEDDING_DIM, activation='relu'))(q1)\nq1 = Lambda(lambda x: K.max(x, axis=1), output_shape=(EMBEDDING_DIM, ))(q1)\n\nq2 = Embedding(nb_words + 1, \n                 EMBEDDING_DIM, \n                 weights=[word_embedding_matrix], \n                 input_length=MAX_SEQUENCE_LENGTH, \n                 trainable=False)(question2)\nq2 = TimeDistributed(Dense(EMBEDDING_DIM, activation='relu'))(q2)\nq2 = Lambda(lambda x: K.max(x, axis=1), output_shape=(EMBEDDING_DIM, ))(q2)\n\nmerged = concatenate([q1,q2])\nmerged = Dense(200, activation='relu')(merged)\nmerged = Dropout(DROPOUT)(merged)\nmerged = BatchNormalization()(merged)\nmerged = Dense(200, activation='relu')(merged)\nmerged = Dropout(DROPOUT)(merged)\nmerged = BatchNormalization()(merged)\nmerged = Dense(200, activation='relu')(merged)\nmerged = Dropout(DROPOUT)(merged)\nmerged = BatchNormalization()(merged)\nmerged = Dense(200, activation='relu')(merged)\nmerged = Dropout(DROPOUT)(merged)\nmerged = BatchNormalization()(merged)\n\nis_duplicate = Dense(1, activation='sigmoid')(merged)\n\nmodel = Model(inputs=[question1,question2], outputs=is_duplicate)\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"20fe4c5c-039e-f6b1-55dd-1d87228b6300"},"outputs":[],"source":"model.summary()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"098b77db-8013-c97f-688d-bb057cf50a28"},"outputs":[],"source":"MODEL_WEIGHTS = 'question_pairs_weights.h5'\ncallbacks = [ModelCheckpoint(MODEL_WEIGHTS, monitor='val_acc', save_best_only=True)]\nhistory = model.fit([Q1_train, Q2_train],\n                    y_train,\n                    epochs=NB_EPOCHS,\n                    validation_split=VALIDATION_SPLIT,\n                    verbose=2,\n                    batch_size=BATCH_SIZE,\n                    callbacks=callbacks)\n"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"6c52c2b9-4629-0b99-df62-418c335276e2"},"outputs":[],"source":"acc = pd.DataFrame({'epoch': [ i + 1 for i in history.epoch ],\n                    'training': history.history['acc'],\n                    'validation': history.history['val_acc']})\nax = acc.iloc[:,:].plot(x='epoch', figsize={5,8}, grid=True)\nax.set_ylabel(\"accuracy\")\nax.set_ylim([0.0,1.0]);"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"2263318f-9919-63e5-fa4b-37f893e9e7b9"},"outputs":[],"source":"max_val_acc, idx = max((val, idx) for (idx, val) in enumerate(history.history['val_acc']))\nprint('Maximum accuracy at epoch', '{:d}'.format(idx+1), '=', '{:.4f}'.format(max_val_acc))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"aded5002-8dc7-0b9f-4844-8ab66a5a8186"},"outputs":[],"source":"model.load_weights(MODEL_WEIGHTS)\npredictions = model.predict([test_q1, test_q2, test_q1, test_q2], verbose = True)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c2120609-1f89-ce0d-d9fe-c24a3050e238"},"outputs":[],"source":"submission = pd.DataFrame(predictions, columns=['is_duplicate'])\nsubmission.insert(0, 'test_id', test.test_id)\nfile_name = 'submission_{}.csv'.format(min_loss)\nsubmission.to_csv(file_name, index=False)\n\nsubmission.head(10)"}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":0}