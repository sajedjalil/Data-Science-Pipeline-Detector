{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Quora Duplicate Questions: Modelling\n**This notebook covers the modelling part of this competition. For EDA & Visulalization, the notebook can be found [here](https://www.kaggle.com/mitishaagarwal/quora-duplicate-questions-eda). ****","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport re\nimport time\nimport warnings\nimport sqlite3\nfrom sqlalchemy import create_engine # database connection\nimport csv\nimport os\nwarnings.filterwarnings(\"ignore\")\nimport datetime as dt\nimport numpy as np\nfrom nltk.corpus import stopwords\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.preprocessing import normalize\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.manifold import TSNE\nimport seaborn as sns\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics.classification import accuracy_score, log_loss\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom collections import Counter\nfrom scipy.sparse import hstack\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import StratifiedKFold \nfrom collections import Counter, defaultdict\nfrom sklearn.calibration import CalibratedClassifierCV\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\nimport math\nfrom sklearn.metrics import normalized_mutual_info_score\nfrom sklearn.ensemble import RandomForestClassifier\n\n\n\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import SGDClassifier\nfrom mlxtend.classifier import StackingClassifier\n\nfrom sklearn import model_selection\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import precision_recall_curve, auc, roc_curve","metadata":{"id":"6VXPfRp-tARR","outputId":"6c27acaf-2c6a-467f-fd7a-68efce98e30b","execution":{"iopub.status.busy":"2022-01-13T08:35:29.015729Z","iopub.execute_input":"2022-01-13T08:35:29.016075Z","iopub.status.idle":"2022-01-13T08:35:29.028809Z","shell.execute_reply.started":"2022-01-13T08:35:29.016037Z","shell.execute_reply":"2022-01-13T08:35:29.027803Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1>4. Machine Learning Models </h1>","metadata":{"id":"ZihvUPvHtARd"}},{"cell_type":"markdown","source":"<h2> 4.1 Reading data from file and storing into sql table </h2>","metadata":{"id":"CtN9VBPutARf"}},{"cell_type":"code","source":"#Creating db file from csv\nif not os.path.isfile('train.db'):\n    disk_engine = create_engine('sqlite:///train.db')\n    start = dt.datetime.now()\n    chunksize = 180000\n    j = 0\n    index_start = 1\n    for df in pd.read_csv('../input/d/elemento/quora-question-pairs/final_features.csv', names=['Unnamed: 0','id','is_duplicate','cwc_min','cwc_max','csc_min','csc_max','ctc_min','ctc_max','last_word_eq','first_word_eq','abs_len_diff','mean_len','token_set_ratio','token_sort_ratio','fuzz_ratio','fuzz_partial_ratio','longest_substr_ratio','freq_qid1','freq_qid2','q1len','q2len','q1_n_words','q2_n_words','word_Common','word_Total','word_share','freq_q1+q2','freq_q1-q2','0_x','1_x','2_x','3_x','4_x','5_x','6_x','7_x','8_x','9_x','10_x','11_x','12_x','13_x','14_x','15_x','16_x','17_x','18_x','19_x','20_x','21_x','22_x','23_x','24_x','25_x','26_x','27_x','28_x','29_x','30_x','31_x','32_x','33_x','34_x','35_x','36_x','37_x','38_x','39_x','40_x','41_x','42_x','43_x','44_x','45_x','46_x','47_x','48_x','49_x','50_x','51_x','52_x','53_x','54_x','55_x','56_x','57_x','58_x','59_x','60_x','61_x','62_x','63_x','64_x','65_x','66_x','67_x','68_x','69_x','70_x','71_x','72_x','73_x','74_x','75_x','76_x','77_x','78_x','79_x','80_x','81_x','82_x','83_x','84_x','85_x','86_x','87_x','88_x','89_x','90_x','91_x','92_x','93_x','94_x','95_x','96_x','97_x','98_x','99_x','100_x','101_x','102_x','103_x','104_x','105_x','106_x','107_x','108_x','109_x','110_x','111_x','112_x','113_x','114_x','115_x','116_x','117_x','118_x','119_x','120_x','121_x','122_x','123_x','124_x','125_x','126_x','127_x','128_x','129_x','130_x','131_x','132_x','133_x','134_x','135_x','136_x','137_x','138_x','139_x','140_x','141_x','142_x','143_x','144_x','145_x','146_x','147_x','148_x','149_x','150_x','151_x','152_x','153_x','154_x','155_x','156_x','157_x','158_x','159_x','160_x','161_x','162_x','163_x','164_x','165_x','166_x','167_x','168_x','169_x','170_x','171_x','172_x','173_x','174_x','175_x','176_x','177_x','178_x','179_x','180_x','181_x','182_x','183_x','184_x','185_x','186_x','187_x','188_x','189_x','190_x','191_x','192_x','193_x','194_x','195_x','196_x','197_x','198_x','199_x','200_x','201_x','202_x','203_x','204_x','205_x','206_x','207_x','208_x','209_x','210_x','211_x','212_x','213_x','214_x','215_x','216_x','217_x','218_x','219_x','220_x','221_x','222_x','223_x','224_x','225_x','226_x','227_x','228_x','229_x','230_x','231_x','232_x','233_x','234_x','235_x','236_x','237_x','238_x','239_x','240_x','241_x','242_x','243_x','244_x','245_x','246_x','247_x','248_x','249_x','250_x','251_x','252_x','253_x','254_x','255_x','256_x','257_x','258_x','259_x','260_x','261_x','262_x','263_x','264_x','265_x','266_x','267_x','268_x','269_x','270_x','271_x','272_x','273_x','274_x','275_x','276_x','277_x','278_x','279_x','280_x','281_x','282_x','283_x','284_x','285_x','286_x','287_x','288_x','289_x','290_x','291_x','292_x','293_x','294_x','295_x','296_x','297_x','298_x','299_x','300_x','301_x','302_x','303_x','304_x','305_x','306_x','307_x','308_x','309_x','310_x','311_x','312_x','313_x','314_x','315_x','316_x','317_x','318_x','319_x','320_x','321_x','322_x','323_x','324_x','325_x','326_x','327_x','328_x','329_x','330_x','331_x','332_x','333_x','334_x','335_x','336_x','337_x','338_x','339_x','340_x','341_x','342_x','343_x','344_x','345_x','346_x','347_x','348_x','349_x','350_x','351_x','352_x','353_x','354_x','355_x','356_x','357_x','358_x','359_x','360_x','361_x','362_x','363_x','364_x','365_x','366_x','367_x','368_x','369_x','370_x','371_x','372_x','373_x','374_x','375_x','376_x','377_x','378_x','379_x','380_x','381_x','382_x','383_x','0_y','1_y','2_y','3_y','4_y','5_y','6_y','7_y','8_y','9_y','10_y','11_y','12_y','13_y','14_y','15_y','16_y','17_y','18_y','19_y','20_y','21_y','22_y','23_y','24_y','25_y','26_y','27_y','28_y','29_y','30_y','31_y','32_y','33_y','34_y','35_y','36_y','37_y','38_y','39_y','40_y','41_y','42_y','43_y','44_y','45_y','46_y','47_y','48_y','49_y','50_y','51_y','52_y','53_y','54_y','55_y','56_y','57_y','58_y','59_y','60_y','61_y','62_y','63_y','64_y','65_y','66_y','67_y','68_y','69_y','70_y','71_y','72_y','73_y','74_y','75_y','76_y','77_y','78_y','79_y','80_y','81_y','82_y','83_y','84_y','85_y','86_y','87_y','88_y','89_y','90_y','91_y','92_y','93_y','94_y','95_y','96_y','97_y','98_y','99_y','100_y','101_y','102_y','103_y','104_y','105_y','106_y','107_y','108_y','109_y','110_y','111_y','112_y','113_y','114_y','115_y','116_y','117_y','118_y','119_y','120_y','121_y','122_y','123_y','124_y','125_y','126_y','127_y','128_y','129_y','130_y','131_y','132_y','133_y','134_y','135_y','136_y','137_y','138_y','139_y','140_y','141_y','142_y','143_y','144_y','145_y','146_y','147_y','148_y','149_y','150_y','151_y','152_y','153_y','154_y','155_y','156_y','157_y','158_y','159_y','160_y','161_y','162_y','163_y','164_y','165_y','166_y','167_y','168_y','169_y','170_y','171_y','172_y','173_y','174_y','175_y','176_y','177_y','178_y','179_y','180_y','181_y','182_y','183_y','184_y','185_y','186_y','187_y','188_y','189_y','190_y','191_y','192_y','193_y','194_y','195_y','196_y','197_y','198_y','199_y','200_y','201_y','202_y','203_y','204_y','205_y','206_y','207_y','208_y','209_y','210_y','211_y','212_y','213_y','214_y','215_y','216_y','217_y','218_y','219_y','220_y','221_y','222_y','223_y','224_y','225_y','226_y','227_y','228_y','229_y','230_y','231_y','232_y','233_y','234_y','235_y','236_y','237_y','238_y','239_y','240_y','241_y','242_y','243_y','244_y','245_y','246_y','247_y','248_y','249_y','250_y','251_y','252_y','253_y','254_y','255_y','256_y','257_y','258_y','259_y','260_y','261_y','262_y','263_y','264_y','265_y','266_y','267_y','268_y','269_y','270_y','271_y','272_y','273_y','274_y','275_y','276_y','277_y','278_y','279_y','280_y','281_y','282_y','283_y','284_y','285_y','286_y','287_y','288_y','289_y','290_y','291_y','292_y','293_y','294_y','295_y','296_y','297_y','298_y','299_y','300_y','301_y','302_y','303_y','304_y','305_y','306_y','307_y','308_y','309_y','310_y','311_y','312_y','313_y','314_y','315_y','316_y','317_y','318_y','319_y','320_y','321_y','322_y','323_y','324_y','325_y','326_y','327_y','328_y','329_y','330_y','331_y','332_y','333_y','334_y','335_y','336_y','337_y','338_y','339_y','340_y','341_y','342_y','343_y','344_y','345_y','346_y','347_y','348_y','349_y','350_y','351_y','352_y','353_y','354_y','355_y','356_y','357_y','358_y','359_y','360_y','361_y','362_y','363_y','364_y','365_y','366_y','367_y','368_y','369_y','370_y','371_y','372_y','373_y','374_y','375_y','376_y','377_y','378_y','379_y','380_y','381_y','382_y','383_y'], chunksize=chunksize, iterator=True, encoding='utf-8', ):\n        df.index += index_start\n        j+=1\n        print('{} rows'.format(j*chunksize))\n        df.to_sql('data', disk_engine, if_exists='append')\n        index_start = df.index[-1] + 1","metadata":{"id":"owBQdjY1tARh","execution":{"iopub.status.busy":"2022-01-13T08:40:57.736446Z","iopub.execute_input":"2022-01-13T08:40:57.736828Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#http://www.sqlitetutorial.net/sqlite-python/create-tables/\ndef create_connection(db_file):\n    \"\"\" create a database connection to the SQLite database\n        specified by db_file\n    :param db_file: database file\n    :return: Connection object or None\n    \"\"\"\n    try:\n        conn = sqlite3.connect(db_file)\n        return conn\n    except Error as e:\n        print(e)\n \n    return None\n\n\ndef checkTableExists(dbcon):\n    cursr = dbcon.cursor()\n    str = \"select name from sqlite_master where type='table'\"\n    table_names = cursr.execute(str)\n    print(\"Tables in the databse:\")\n    tables =table_names.fetchall() \n    print(tables[0][0])\n    return(len(tables))","metadata":{"id":"4hpD3aBktARn","execution":{"iopub.status.busy":"2022-01-13T08:35:29.173715Z","iopub.status.idle":"2022-01-13T08:35:29.174091Z","shell.execute_reply.started":"2022-01-13T08:35:29.173904Z","shell.execute_reply":"2022-01-13T08:35:29.173927Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"read_db = 'train.db'\nconn_r = create_connection(read_db)\ncheckTableExists(conn_r)\nconn_r.close()","metadata":{"id":"nR8ZIUnttARs","outputId":"810fb3fb-7da2-4b78-9e29-9edabbf68cf6","execution":{"iopub.status.busy":"2022-01-13T08:35:29.175678Z","iopub.status.idle":"2022-01-13T08:35:29.176012Z","shell.execute_reply.started":"2022-01-13T08:35:29.175833Z","shell.execute_reply":"2022-01-13T08:35:29.175855Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# try to sample data according to the computing power you have\nif os.path.isfile(read_db):\n    conn_r = create_connection(read_db)\n    if conn_r is not None:\n        # for selecting first 1M rows\n        # data = pd.read_sql_query(\"\"\"SELECT * FROM data LIMIT 100001;\"\"\", conn_r)\n        \n        # for selecting random points\n        data = pd.read_sql_query(\"SELECT * From data ORDER BY RANDOM() LIMIT 100001;\", conn_r)\n        conn_r.commit()\n        conn_r.close()","metadata":{"id":"SZq5gaaztARy","execution":{"iopub.status.busy":"2022-01-13T08:35:29.17759Z","iopub.status.idle":"2022-01-13T08:35:29.177928Z","shell.execute_reply.started":"2022-01-13T08:35:29.177757Z","shell.execute_reply":"2022-01-13T08:35:29.177778Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# remove the first row \ndata.drop(data.index[0], inplace=True)\ny_true = data['is_duplicate']\ndata.drop(['Unnamed: 0', 'id','index','is_duplicate'], axis=1, inplace=True)","metadata":{"id":"ZkeBKktKtAR3","execution":{"iopub.status.busy":"2022-01-13T08:35:29.179287Z","iopub.status.idle":"2022-01-13T08:35:29.179626Z","shell.execute_reply.started":"2022-01-13T08:35:29.179445Z","shell.execute_reply":"2022-01-13T08:35:29.179476Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.head()","metadata":{"id":"QKSenpsmtAR9","outputId":"81d890ce-df79-4402-9324-84817dbd5a7d","execution":{"iopub.status.busy":"2022-01-13T08:35:29.180973Z","iopub.status.idle":"2022-01-13T08:35:29.18134Z","shell.execute_reply.started":"2022-01-13T08:35:29.181113Z","shell.execute_reply":"2022-01-13T08:35:29.181152Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2> 4.2 Converting strings to numerics </h2>","metadata":{"id":"KaWHDzqUtASD"}},{"cell_type":"code","source":"# after we read from sql table each entry was read it as a string\n# we convert all the features into numaric before we apply any model\ncols = list(data.columns)\nfor i in cols:\n    data[i] = data[i].apply(pd.to_numeric)\n    print(i)","metadata":{"id":"iLV60gkptASD","outputId":"f297e0f4-52d5-4ab4-8a43-f0ff82f63698","execution":{"iopub.status.busy":"2022-01-13T08:35:29.183567Z","iopub.status.idle":"2022-01-13T08:35:29.184657Z","shell.execute_reply.started":"2022-01-13T08:35:29.184308Z","shell.execute_reply":"2022-01-13T08:35:29.184347Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# https://stackoverflow.com/questions/7368789/convert-all-strings-in-a-list-to-int\ny_true = list(map(int, y_true.values))","metadata":{"id":"_LpfQwc9tASJ","execution":{"iopub.status.busy":"2022-01-13T08:35:29.185859Z","iopub.status.idle":"2022-01-13T08:35:29.186339Z","shell.execute_reply.started":"2022-01-13T08:35:29.18607Z","shell.execute_reply":"2022-01-13T08:35:29.186095Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2> 4.3 Random train test split( 70:30) </h2>","metadata":{"id":"CuMTqWGutASO"}},{"cell_type":"code","source":"X_train,X_test, y_train, y_test = train_test_split(data, y_true, stratify=y_true, test_size=0.3)","metadata":{"id":"3Rat2obGtASP","execution":{"iopub.status.busy":"2022-01-13T08:35:29.188107Z","iopub.status.idle":"2022-01-13T08:35:29.188587Z","shell.execute_reply.started":"2022-01-13T08:35:29.188341Z","shell.execute_reply":"2022-01-13T08:35:29.188366Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Number of data points in train data :\",X_train.shape)\nprint(\"Number of data points in test data :\",X_test.shape)","metadata":{"id":"1Iw9zCHqtASS","outputId":"910b684b-0876-4dd8-e0d9-457846236833","execution":{"iopub.status.busy":"2022-01-13T08:35:29.189634Z","iopub.status.idle":"2022-01-13T08:35:29.190097Z","shell.execute_reply.started":"2022-01-13T08:35:29.189852Z","shell.execute_reply":"2022-01-13T08:35:29.189878Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"-\"*10, \"Distribution of output variable in train data\", \"-\"*10)\ntrain_distr = Counter(y_train)\ntrain_len = len(y_train)\nprint(\"Class 0: \",int(train_distr[0])/train_len,\"Class 1: \", int(train_distr[1])/train_len)\nprint(\"-\"*10, \"Distribution of output variable in train data\", \"-\"*10)\ntest_distr = Counter(y_test)\ntest_len = len(y_test)\nprint(\"Class 0: \",int(test_distr[1])/test_len, \"Class 1: \",int(test_distr[1])/test_len)","metadata":{"id":"0oDV15LJtASY","outputId":"70a1e4eb-3f31-4f1e-a53b-ad972978505d","execution":{"iopub.status.busy":"2022-01-13T08:35:29.191862Z","iopub.status.idle":"2022-01-13T08:35:29.192384Z","shell.execute_reply.started":"2022-01-13T08:35:29.192068Z","shell.execute_reply":"2022-01-13T08:35:29.192093Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# This function plots the confusion matrices given y_i, y_i_hat.\ndef plot_confusion_matrix(test_y, predict_y):\n    C = confusion_matrix(test_y, predict_y)\n    # C = 9,9 matrix, each cell (i,j) represents number of points of class i are predicted class j\n    \n    A =(((C.T)/(C.sum(axis=1))).T)\n    #divid each element of the confusion matrix with the sum of elements in that column\n    \n    # C = [[1, 2],\n    #     [3, 4]]\n    # C.T = [[1, 3],\n    #        [2, 4]]\n    # C.sum(axis = 1)  axis=0 corresonds to columns and axis=1 corresponds to rows in two diamensional array\n    # C.sum(axix =1) = [[3, 7]]\n    # ((C.T)/(C.sum(axis=1))) = [[1/3, 3/7]\n    #                           [2/3, 4/7]]\n\n    # ((C.T)/(C.sum(axis=1))).T = [[1/3, 2/3]\n    #                           [3/7, 4/7]]\n    # sum of row elements = 1\n    \n    B =(C/C.sum(axis=0))\n    #divid each element of the confusion matrix with the sum of elements in that row\n    # C = [[1, 2],\n    #     [3, 4]]\n    # C.sum(axis = 0)  axis=0 corresonds to columns and axis=1 corresponds to rows in two diamensional array\n    # C.sum(axix =0) = [[4, 6]]\n    # (C/C.sum(axis=0)) = [[1/4, 2/6],\n    #                      [3/4, 4/6]] \n    plt.figure(figsize=(20,4))\n    \n    labels = [1,2]\n    # representing A in heatmap format\n    cmap=sns.light_palette(\"blue\")\n    plt.subplot(1, 3, 1)\n    sns.heatmap(C, annot=True, cmap=cmap, fmt=\".3f\", xticklabels=labels, yticklabels=labels)\n    plt.xlabel('Predicted Class')\n    plt.ylabel('Original Class')\n    plt.title(\"Confusion matrix\")\n    \n    plt.subplot(1, 3, 2)\n    sns.heatmap(B, annot=True, cmap=cmap, fmt=\".3f\", xticklabels=labels, yticklabels=labels)\n    plt.xlabel('Predicted Class')\n    plt.ylabel('Original Class')\n    plt.title(\"Precision matrix\")\n    \n    plt.subplot(1, 3, 3)\n    # representing B in heatmap format\n    sns.heatmap(A, annot=True, cmap=cmap, fmt=\".3f\", xticklabels=labels, yticklabels=labels)\n    plt.xlabel('Predicted Class')\n    plt.ylabel('Original Class')\n    plt.title(\"Recall matrix\")\n    \n    plt.show()","metadata":{"id":"XfxcPT6jtASg","execution":{"iopub.status.busy":"2022-01-13T08:35:29.194294Z","iopub.status.idle":"2022-01-13T08:35:29.194767Z","shell.execute_reply.started":"2022-01-13T08:35:29.194511Z","shell.execute_reply":"2022-01-13T08:35:29.194536Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2> 4.4 Building a random model (Finding worst-case log-loss) </h2>","metadata":{"id":"UStQJ5F_tASk"}},{"cell_type":"code","source":"# we need to generate 9 numbers and the sum of numbers should be 1\n# one solution is to genarate 9 numbers and divide each of the numbers by their sum\n# ref: https://stackoverflow.com/a/18662466/4084039\n# we create a output array that has exactly same size as the CV data\npredicted_y = np.zeros((test_len,2))\nfor i in range(test_len):\n    rand_probs = np.random.rand(1,2)\n    predicted_y[i] = ((rand_probs/sum(sum(rand_probs)))[0])\nprint(\"Log loss on Test Data using Random Model\",log_loss(y_test, predicted_y, eps=1e-15))\n\npredicted_y =np.argmax(predicted_y, axis=1)\nplot_confusion_matrix(y_test, predicted_y)","metadata":{"id":"qwMDqcU7tASl","outputId":"c1e90d53-25ec-445b-e33a-299538520e32","execution":{"iopub.status.busy":"2022-01-13T08:35:29.197123Z","iopub.status.idle":"2022-01-13T08:35:29.197612Z","shell.execute_reply.started":"2022-01-13T08:35:29.197357Z","shell.execute_reply":"2022-01-13T08:35:29.197382Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2> 4.4 Logistic Regression with hyperparameter tuning </h2>","metadata":{"id":"YgY29g_qtASq"}},{"cell_type":"code","source":"alpha = [10 ** x for x in range(-5, 2)] # hyperparam for SGD classifier.\n\n# read more about SGDClassifier() at http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html\n# ------------------------------\n# default parameters\n# SGDClassifier(loss=’hinge’, penalty=’l2’, alpha=0.0001, l1_ratio=0.15, fit_intercept=True, max_iter=None, tol=None, \n# shuffle=True, verbose=0, epsilon=0.1, n_jobs=1, random_state=None, learning_rate=’optimal’, eta0=0.0, power_t=0.5, \n# class_weight=None, warm_start=False, average=False, n_iter=None)\n\n# some of methods\n# fit(X, y[, coef_init, intercept_init, …])\tFit linear model with Stochastic Gradient Descent.\n# predict(X)\tPredict class labels for samples in X.\n\n\nlog_error_array=[]\nfor i in alpha:\n    clf = SGDClassifier(alpha=i, penalty='l2', loss='log', random_state=42)\n    clf.fit(X_train, y_train)\n    sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n    sig_clf.fit(X_train, y_train)\n    predict_y = sig_clf.predict_proba(X_test)\n    log_error_array.append(log_loss(y_test, predict_y, labels=clf.classes_, eps=1e-15))\n    print('For values of alpha = ', i, \"The log loss is:\",log_loss(y_test, predict_y, labels=clf.classes_, eps=1e-15))\n\nfig, ax = plt.subplots()\nax.plot(alpha, log_error_array,c='g')\nfor i, txt in enumerate(np.round(log_error_array,3)):\n    ax.annotate((alpha[i],np.round(txt,3)), (alpha[i],log_error_array[i]))\nplt.grid()\nplt.title(\"Cross Validation Error for each alpha\")\nplt.xlabel(\"Alpha i's\")\nplt.ylabel(\"Error measure\")\nplt.show()\n\n\nbest_alpha = np.argmin(log_error_array)\nclf = SGDClassifier(alpha=alpha[best_alpha], penalty='l2', loss='log', random_state=42)\nclf.fit(X_train, y_train)\nsig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\nsig_clf.fit(X_train, y_train)\n\npredict_y = sig_clf.predict_proba(X_train)\nprint('For values of best alpha = ', alpha[best_alpha], \"The train log loss is:\",log_loss(y_train, predict_y, labels=clf.classes_, eps=1e-15))\npredict_y = sig_clf.predict_proba(X_test)\nprint('For values of best alpha = ', alpha[best_alpha], \"The test log loss is:\",log_loss(y_test, predict_y, labels=clf.classes_, eps=1e-15))\npredicted_y =np.argmax(predict_y,axis=1)\nprint(\"Total number of data points :\", len(predicted_y))\nplot_confusion_matrix(y_test, predicted_y)","metadata":{"id":"Wb2tOE3GtASr","outputId":"d7e4fc88-7d4e-4313-cda7-462a2409292e","execution":{"iopub.status.busy":"2022-01-13T08:35:29.198843Z","iopub.status.idle":"2022-01-13T08:35:29.199316Z","shell.execute_reply.started":"2022-01-13T08:35:29.19905Z","shell.execute_reply":"2022-01-13T08:35:29.199074Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2> 4.5 Linear SVM with hyperparameter tuning </h2>","metadata":{"id":"ouQSEnr3tASy"}},{"cell_type":"code","source":"alpha = [10 ** x for x in range(-5, 2)] # hyperparam for SGD classifier.\n\n# read more about SGDClassifier() at http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html\n# ------------------------------\n# default parameters\n# SGDClassifier(loss=’hinge’, penalty=’l2’, alpha=0.0001, l1_ratio=0.15, fit_intercept=True, max_iter=None, tol=None, \n# shuffle=True, verbose=0, epsilon=0.1, n_jobs=1, random_state=None, learning_rate=’optimal’, eta0=0.0, power_t=0.5, \n# class_weight=None, warm_start=False, average=False, n_iter=None)\n\n# some of methods\n# fit(X, y[, coef_init, intercept_init, …])\tFit linear model with Stochastic Gradient Descent.\n# predict(X)\tPredict class labels for samples in X.\n\nlog_error_array=[]\nfor i in alpha:\n    clf = SGDClassifier(alpha=i, penalty='l1', loss='hinge', random_state=42)\n    clf.fit(X_train, y_train)\n    sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n    sig_clf.fit(X_train, y_train)\n    predict_y = sig_clf.predict_proba(X_test)\n    log_error_array.append(log_loss(y_test, predict_y, labels=clf.classes_, eps=1e-15))\n    print('For values of alpha = ', i, \"The log loss is:\",log_loss(y_test, predict_y, labels=clf.classes_, eps=1e-15))\n\nfig, ax = plt.subplots()\nax.plot(alpha, log_error_array,c='g')\nfor i, txt in enumerate(np.round(log_error_array,3)):\n    ax.annotate((alpha[i],np.round(txt,3)), (alpha[i],log_error_array[i]))\nplt.grid()\nplt.title(\"Cross Validation Error for each alpha\")\nplt.xlabel(\"Alpha i's\")\nplt.ylabel(\"Error measure\")\nplt.show()\n\n\nbest_alpha = np.argmin(log_error_array)\nclf = SGDClassifier(alpha=alpha[best_alpha], penalty='l1', loss='hinge', random_state=42)\nclf.fit(X_train, y_train)\nsig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\nsig_clf.fit(X_train, y_train)\n\npredict_y = sig_clf.predict_proba(X_train)\nprint('For values of best alpha = ', alpha[best_alpha], \"The train log loss is:\",log_loss(y_train, predict_y, labels=clf.classes_, eps=1e-15))\npredict_y = sig_clf.predict_proba(X_test)\nprint('For values of best alpha = ', alpha[best_alpha], \"The test log loss is:\",log_loss(y_test, predict_y, labels=clf.classes_, eps=1e-15))\npredicted_y =np.argmax(predict_y,axis=1)\nprint(\"Total number of data points :\", len(predicted_y))\nplot_confusion_matrix(y_test, predicted_y)","metadata":{"id":"AOFfZ5PLtAS0","outputId":"d31eb598-e275-48cb-c49b-98e9eb76d8ba","execution":{"iopub.status.busy":"2022-01-13T08:35:29.200489Z","iopub.status.idle":"2022-01-13T08:35:29.200942Z","shell.execute_reply.started":"2022-01-13T08:35:29.200691Z","shell.execute_reply":"2022-01-13T08:35:29.200715Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2> 4.6 XGBoost </h2>","metadata":{"id":"ZhTJgclztAS6"}},{"cell_type":"code","source":"import xgboost as xgb\nparams = {}\nparams['objective'] = 'binary:logistic'\nparams['eval_metric'] = 'logloss'\nparams['eta'] = 0.02\nparams['max_depth'] = 4\n\nd_train = xgb.DMatrix(X_train, label=y_train)\nd_test = xgb.DMatrix(X_test, label=y_test)\n\nwatchlist = [(d_train, 'train'), (d_test, 'valid')]\n\nbst = xgb.train(params, d_train, 400, watchlist, early_stopping_rounds=20, verbose_eval=10)\n\nxgdmat = xgb.DMatrix(X_train,y_train)\npredict_y = bst.predict(d_test)\nprint(\"The test log loss is:\",log_loss(y_test, predict_y, labels=clf.classes_, eps=1e-15))","metadata":{"id":"9U367-xetAS7","outputId":"167e8588-2ac4-4c6d-ac22-f56a2fce5657","execution":{"iopub.status.busy":"2022-01-13T08:35:29.202459Z","iopub.status.idle":"2022-01-13T08:35:29.202777Z","shell.execute_reply.started":"2022-01-13T08:35:29.202598Z","shell.execute_reply":"2022-01-13T08:35:29.202614Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predicted_y =np.array(predict_y>0.5,dtype=int)\nprint(\"Total number of data points :\", len(predicted_y))\nplot_confusion_matrix(y_test, predicted_y)","metadata":{"id":"6U5b17AatAS_","outputId":"ca83b680-023b-4bc5-f499-8d8d85c2ff5e","execution":{"iopub.status.busy":"2022-01-13T08:35:29.205642Z","iopub.status.idle":"2022-01-13T08:35:29.205957Z","shell.execute_reply.started":"2022-01-13T08:35:29.205793Z","shell.execute_reply":"2022-01-13T08:35:29.205809Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1> 5. Assignments </h1>","metadata":{"id":"WmiIgHOJtATF"}},{"cell_type":"markdown","source":"1. Try out models (Logistic regression, Linear-SVM) with simple TF-IDF vectors instead of TD_IDF weighted word2Vec.\n2. Perform hyperparameter tuning  of XgBoost models using RandomsearchCV with vectorizer as TF-IDF W2V  to reduce the log-loss.\n\n","metadata":{"id":"CWS6JoB0tATF"}}]}