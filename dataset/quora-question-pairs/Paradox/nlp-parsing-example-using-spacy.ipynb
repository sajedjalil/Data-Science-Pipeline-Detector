{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"755de377-4598-7348-4ce9-5374323356b1"},"source":"# An example of parsing text with Spacy. And using POS tags to make some fuzzy feature metrics.\n"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"5e5553f1-117d-53f6-97f5-2ca97f82ab82"},"outputs":[],"source":"import spacy\nfrom spacy.en import English\nfrom spacy.symbols import *\nnlp = English()\n\nimport pandas as pd\nimport numpy as np\nfrom fuzzywuzzy import fuzz\nfrom tqdm import tqdm, tqdm_notebook\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nroot_path = '../input/'"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a1e20f3d-db1b-bca2-6f01-8f269cb874ff"},"outputs":[],"source":"def get_distinct_questions(train, test):\n    df1 = train[['question1']].copy()\n    df2 = train[['question2']].copy()\n    df1_test = test[['question1']].copy()\n    df2_test = test[['question2']].copy()\n\n    df2.rename(columns = {'question2':'question1'},inplace=True)\n    df2_test.rename(columns = {'question2':'question1'},inplace=True)\n\n    questions = df1.append(df2)\n    questions = questions.append(df1_test)\n    questions = questions.append(df2_test)\n    \n    questions.drop_duplicates(subset = ['question1'],inplace=True)\n\n    questions.reset_index(inplace=True,drop=True)\n    del df1,df1_test, df2, df2_test\n    return questions\n\ndef parse_question(doc):\n    #doc = nlp(question)\n    \n    pobj = []\n    dobj = []\n    num = []\n    qword = []\n    noun = []\n    verb = []\n    adj = []\n    ents = []\n    ent_types = []\n    \n    \n    ent = []\n    ent_type = \"\"\n    sent_count = 0\n    for s in doc.sents:\n        sent_count+=1\n        for word in s:\n            #print(\"(\" + word.ent_type_  + \",\" + str(word.pos)  + \",\" + word.pos_  + \",\" + str(word.pos)  + \",\" + word.tag_  \\\n            #      + \",\" + str(word.tag)  + \",\" + word.dep_    + \",\" + str(word.dep)  + \",\" + word.lemma_ + \") \")\n            #ENTITIES\n            if word.ent_type == 0:\n                if len(ent) > 0:\n                    ents.append('_'.join(ent))                    \n                    ent_types.append(ent_type)\n                    ent = []\n                    ent_type = \"\"\n            elif word.ent_type > 0 and word.ent_iob == 3:\n                if len(ent) > 0:\n                    ents.append('_'.join(ent))                    \n                    ent_types.append(ent_type)\n                    ent = []\n                ent.append(word.lemma_)\n                ent_type = word.ent_type_ \n            elif word.ent_type > 0 and word.ent_iob == 1:                \n                ent.append(word.lemma_)\n                \n            #QUESTIONS\n            if word.tag_.find('W') == 0:\n                qword.append(word.lemma_)\n            #NOUNS\n            elif word.pos in [90,94]:\n                noun.append(word.lemma_)\n                \n                #pobj\n                if word.dep == 435:\n                    pobj.append(word.lemma_)\n                #dobj\n                elif word.dep == 412:\n                    dobj.append(word.lemma_)\n\n            #NUMBER\n            elif word.pos in [91]:\n                num.append(word.lemma_)\n            #ADJ\n            elif word.pos in [82]:\n                adj.append(word.lemma_)\n            #VERB\n            elif word.pos in [98]:\n                verb.append(word.lemma_)     \n            \n    if len(ent) > 0:\n        ents.append('_'.join(ent))                    \n        ent_types.append(ent_type)\n        ent = []   \n    #print(sent_count, pobj, dobj, num, qword, noun, verb, adj, ents, ent_types)\n    return sent_count, pobj, dobj, num, qword, noun, verb, adj, ents, ent_types\n\n\ndef match_count(list1, list2):\n    return len(set(list1).intersection(set(list2)))\n\ndef diff_count(list1, list2):\n    return len([obj for obj in list1 if obj not in list2] + [obj for obj in list2 if obj not in list1])\n\ndef get_nlp_features(nlp_parts1, nlp_parts2):\n\n    sent_count1, pobj1, dobj1, num1, qword1, noun1, verb1, adj1, ents1, ent_types1 = nlp_parts1\n    sent_count2, pobj2, dobj2, num2, qword2, noun2, verb2, adj2, ents2, ent_types2 = nlp_parts2\n    \n    ret = []\n    \n    f = diff_count\n    ret1 = [abs(sent_count1 - sent_count2), f(pobj1,pobj2), f(dobj1,dobj2),\\\n            f(num1,num2), f(qword1,qword2), f(noun1,noun2),\\\n            f(verb1,verb2), f(adj1,adj2), f(ents1,ents2), f(ent_types1,ent_types1)]\n    \n    f = match_count\n    ret2 = [f(pobj1,pobj2), f(dobj1,dobj2),\\\n            f(num1,num2), f(qword1,qword2), f(noun1,noun2),\\\n            f(verb1,verb2), f(adj1,adj2), f(ents1,ents2), f(ent_types1,ent_types1)]\n    ret3 = [ret2[0] * ret2[0],ret2[1] * ret2[1],ret2[2] * ret2[2],ret2[3] * ret2[3],ret2[4] * ret2[4],ret2[5] * ret2[5]\\\n            ,ret2[6] * ret2[6],ret2[7] * ret2[7],ret2[8] * ret2[8]]\n    f = fuzz.QRatio\n    ret4 = [f(pobj1,pobj2), f(dobj1,dobj2), f(num1,num2), f(qword1,qword2), f(noun1,noun2), f(verb1,verb2), f(adj1,adj2), f(ents1,ents2)]\n    \n    f = fuzz.WRatio\n    ret5 = [f(pobj1,pobj2), f(dobj1,dobj2), f(num1,num2), f(qword1,qword2), f(noun1,noun2), f(verb1,verb2), f(adj1,adj2), f(ents1,ents2)]\n          \n    f = fuzz.token_set_ratio\n    ret6 = [f(pobj1,pobj2), f(dobj1,dobj2), f(num1,num2), f(qword1,qword2), f(noun1,noun2), f(verb1,verb2), f(adj1,adj2), f(ents1,ents2)]\n    \n    f = fuzz.token_sort_ratio\n    ret7 = [f(pobj1,pobj2), f(dobj1,dobj2), f(num1,num2), f(qword1,qword2), f(noun1,noun2), f(verb1,verb2), f(adj1,adj2), f(ents1,ents2)]\n    \n    f = fuzz.partial_ratio\n    ret8 = [f(pobj1,pobj2), f(dobj1,dobj2), f(num1,num2), f(qword1,qword2), f(noun1,noun2), f(verb1,verb2), f(adj1,adj2), f(ents1,ents2)]\n    \n    f = fuzz.partial_token_sort_ratio\n    ret9 = [f(pobj1,pobj2), f(dobj1,dobj2), f(num1,num2), f(qword1,qword2), f(noun1,noun2), f(verb1,verb2), f(adj1,adj2), f(ents1,ents2)]\n    \n    f = fuzz.partial_token_set_ratio\n    ret10 = [f(pobj1,pobj2), f(dobj1,dobj2), f(num1,num2), f(qword1,qword2), f(noun1,noun2), f(verb1,verb2), f(adj1,adj2), f(ents1,ents2)]\n    \n    ret11 = [sent_count1, sent_count2, len(pobj1), len(pobj2), len(dobj1), len(dobj2), len(num1),len(num2),\\\n            len(qword1),len(qword2), len(noun1),len(noun2), len(verb1),len(verb2), len(adj1),len(adj2), len(ents1),len(ents2)]\n    \n    \n    ret.extend(ret1)\n    ret.extend(ret2)\n    ret.extend(ret3)\n    ret.extend(ret4)\n    ret.extend(ret5)\n    ret.extend(ret6)\n    ret.extend(ret7)\n    ret.extend(ret8)\n    ret.extend(ret9)\n    ret.extend(ret10)\n    ret.extend(ret11)\n    return tuple(ret)"},{"cell_type":"markdown","metadata":{"_cell_guid":"3d8828de-4e94-1386-3366-1758c7c88e78"},"source":"# Cache a parse of all the distinct questions"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"65aff1be-5b27-03cc-ff33-0ba18ac0f461","collapsed":true},"outputs":[],"source":"%%time\ntrain_data =  pd.read_csv(root_path + 'train.csv', header=0)\ntest_data =  pd.read_csv(root_path + 'test.csv', header=0)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f2a2b383-4e50-5527-5e61-33821e124ae4","collapsed":true},"outputs":[],"source":"train_questions = get_distinct_questions(train_data, test_data)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d7f6e983-44b9-474a-7f7a-0bf68322239d"},"outputs":[],"source":"nlp_parse_lookup = {}\nindex = 0\nfor doc in tqdm_notebook(nlp.pipe([str(q) for q in train_questions['question1']], n_threads=16, batch_size=10000), total = len(train_questions)):\n    nlp_parse_lookup[str(train_questions.iloc[index]['question1'])] = parse_question(doc)\n    index += 1\n    "},{"cell_type":"markdown","metadata":{"_cell_guid":"32545be8-18e2-ceac-a678-0d678c399111"},"source":"# Make some fuzzy metrics feature from the parsed content\nThese features get 0.35 LB score"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"480323c7-0aca-26ba-fd14-7b125fb2249d"},"outputs":[],"source":"type_list =['pobj', 'dobj', 'num', 'qword', 'noun', 'verb', 'adj', 'ents']\n\ncolumns=[]\ncolumns.append('sent_diff_count') \ncolumns.extend([t + \"_diff_count\" for t in type_list])  \ncolumns.append('ent_types_diff_count') \n\ncolumns.extend([t + \"_match_count\" for t in type_list])  \ncolumns.append('ent_types_match_count') \n\ncolumns.extend([t + \"_match_square\" for t in type_list])  \ncolumns.append('ent_types_match_square')                \n                                                                                                                \ncolumns.extend([t + \"_QRatio\" for t in type_list])                                                             \ncolumns.extend([t + \"_WRatio\" for t in type_list])                                                             \ncolumns.extend([t + \"_token_set_ratio\" for t in type_list])                                                             \ncolumns.extend([t + \"_token_sort_ratio\" for t in type_list])                                                             \ncolumns.extend([t + \"_partial_ratio\" for t in type_list])                                                             \ncolumns.extend([t + \"_partial_token_sort_ratio\" for t in type_list])                                                             \ncolumns.extend([t + \"_partial_token_set_ratio\" for t in type_list])\n                                                                                                              \ncolumns.extend(['sent_count1', 'sent_count2','len_pobj1', 'len_pobj2', 'len_dobj1', 'len_dobj2',\\\n                'len_num1', 'len_num2', 'len_qword1', 'len_qword2', 'len_noun1', 'len_noun2', \\\n                'len_verb1', 'len_verb2', 'len_adj1', 'len_adj2', 'len_ents1', 'len_ents2'])"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f0b1896c-0445-5b50-0167-619b241f2708"},"outputs":[],"source":"feature_list = [get_nlp_features(nlp_parse_lookup[str(q[0])], nlp_parse_lookup[str(q[1])]) \\\n                for q in tqdm_notebook(train_data[['question1','question2']].values, total = len(train_data))]\nnlp_feat = pd.DataFrame(feature_list, columns=columns)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"1b37d363-209b-74eb-d05f-0396461d3c06"},"outputs":[],"source":"feature_list = [get_nlp_features(nlp_parse_lookup[str(q[0])], nlp_parse_lookup[str(q[1])]) \\\n                for q in tqdm_notebook(test_data[['question1','question2']].values, total = len(test_data))]\nnlp_test_feat = pd.DataFrame(feature_list, columns=columns)"},{"cell_type":"markdown","metadata":{"_cell_guid":"beb3f829-cb9c-a681-9590-5d1d6044bffd"},"source":"# Save Features"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"61802df9-18c0-f392-3b7c-d99a7d5f69a7"},"outputs":[],"source":"nlp_feat.to_csv(root_path + 'quora_train_features_nlp.tsv', index=False, sep='\\t')\nnlp_test_feat.to_csv(root_path + 'quora_test_features_nlp.tsv', index=False, sep='\\t')"},{"cell_type":"markdown","metadata":{"_cell_guid":"a2e5a728-15e3-756e-7bbf-b756235ee83b","collapsed":true},"source":"# Some analysis of the features gathered"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"ca814ee6-cd9a-fdcd-2394-4b19abf92c17"},"outputs":[],"source":"nlp_feat['is_duplicate'] = train_data['is_duplicate']"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d9361c68-3644-1609-6a7e-29617e4bc248","collapsed":true},"outputs":[],"source":"mcorr = nlp_feat.corr()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"0576f4fb-ce86-0af5-3186-dcc9cd51849e"},"outputs":[],"source":"#Check\nmcorr.sort_values(['is_duplicate'])['is_duplicate']"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b34abfd6-e8c4-6ea1-1b1d-b4fd6e87ac53"},"outputs":[],"source":"for column_name in mcorr.columns:\n    index = 0\n    matches = mcorr.query('abs(' + str(column_name) + ') >= 0.995').sort_values(column_name)[column_name]    \n    if len(matches) > 1:\n        print()\n        print(column_name  + \"\\n----------------\")\n        for match in matches:            \n            if matches.index[index] != column_name:\n                print(matches.index[index] + '\\t' + str(match))\n       \n            index += 1"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"155e1bdb-a205-2562-43b4-b05482ec437e","collapsed":true},"outputs":[],"source":"def plot_real_feature(fname, train_feat):\n    fig = plt.figure()\n    ax1 = plt.subplot2grid((3, 2), (0, 0), colspan=2)\n    ax2 = plt.subplot2grid((3, 2), (1, 0), colspan=2)\n    ax3 = plt.subplot2grid((3, 2), (2, 0))\n    ax4 = plt.subplot2grid((3, 2), (2, 1))\n    ax1.set_title('Distribution of %s' % fname, fontsize=20)\n    sns.distplot(train_feat[fname], \n                 bins=50, \n                 ax=ax1)    \n    sns.distplot(train_feat[train_feat.is_duplicate == 1][fname], \n                 bins=50, \n                 ax=ax2,\n                 label='is dup')    \n    sns.distplot(train_feat[train_feat.is_duplicate == 0][fname], \n                 bins=50, \n                 ax=ax2,\n                 label='not dup')\n    ax2.legend(loc='upper right', prop={'size': 18})\n    sns.boxplot(y=fname, \n                x='is_duplicate', \n                data=train_feat, \n                ax=ax3)\n    sns.violinplot(y=fname, \n                   x='is_duplicate', \n                   data=train_feat, \n                   ax=ax4)\n    plt.show()\n\ndef plot_corr(mcorr):    \n    \n    mask = np.zeros_like(mcorr, dtype=np.bool)\n    mask[np.triu_indices_from(mask)] = True\n    cmap = sns.diverging_palette(220, 10, as_cmap=True)\n    g = sns.heatmap(mcorr, mask=mask, cmap=cmap, square=True, annot=True, fmt='0.2f')\n    g.set_xticklabels(mcorr.columns, rotation=90)\n    g.set_yticklabels(reversed(mcorr.columns))\n    plt.show()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"1df29d83-f4b1-a660-6998-7a41fee67f77"},"outputs":[],"source":"plot_real_feature('pobj_match_count', nlp_feat)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"9daac28b-de93-ca89-c79f-a3266d5f6842","collapsed":true},"outputs":[],"source":""}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":0}