{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"dd024f71-7f2a-b8d6-931e-8bed53d70230"},"outputs":[],"source":""},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"8ac6aec4-f865-a897-2acf-acce21320cc1"},"outputs":[],"source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output.\n\nimport string\nimport nltk\nfrom nltk.corpus import stopwords\ntrain_df = pd.read_csv('../input/train.csv')\n\npunctuations = list(string.punctuation)\nmystopwords=set(stopwords.words('english'))\nremovelist=[]\nremovelist.extend(punctuations)\nremovelist.extend(mystopwords)\n\n#train_df=pd.DataFrame(train_df[:200])\n#train_df['question1']=train_df['question1'].str.lower().str.split()\n#train_df['question1']=train_df['question1'].map(lambda row: row.lower())\n#train_df['question1']=train_df.apply(lambda row: nltk.word_tokenize(row['question1']), axis=1)\ntrain_df['question1']=train_df['question1'].str.lower()\ntrain_df['question2']=train_df['question2'].str.lower()\n\ntrain_df['question1']=train_df['question1'].str.split()\ntrain_df['question2']=train_df['question2'].str.split()\ntrain_df.dropna(subset=['question1'], inplace=True)\ntrain_df.dropna(subset=['question2'], inplace=True)\n#train_df['question1']=train_df['question1'].apply(lambda row: [item for item in row if item not in removelist])\n\n#train_df['question2']=train_df['question2'].str.lower().str.split()\n#train_df['question1']=train_df['question1'].map(lambda row: row.lower())\n#train_df['question1']=train_df.apply(lambda row: nltk.word_tokenize(row['question1']), axis=1)\n#train_df['question2']=train_df['question2'].apply(lambda row: [item for item in row if item not in removelist])\n#train_df['question2']=train_df['question2'].str.lower()\ntrain_df['question2']=train_df['question2'].apply(lambda row: [item for item in row if item not in removelist])\n\n#train_df['question2']\n#train_df.isnull().sum()\ntrain_df['question1']=train_df['question1'].apply(lambda row: [item for item in row if item not in removelist])\ntrain_df['question2']=train_df['question2'].apply(lambda row: [item for item in row if item not in removelist])\n"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"224dfa45-2776-6921-c93b-ce485e2450e4"},"outputs":[],"source":"q1"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"1823a5e3-4ae0-754a-89a0-0bdf24e2f5eb"},"outputs":[],"source":"\n"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"1cc75389-5d80-2c2d-6120-ccbfff6885df"},"outputs":[],"source":""},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f83685b9-4f58-9280-37fb-9ec547011057"},"outputs":[],"source":""}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":0}