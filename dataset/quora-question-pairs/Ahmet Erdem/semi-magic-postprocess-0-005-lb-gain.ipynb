{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"9d8772a5-34de-8a7e-8efd-7cad86867d44"},"source":"With an unsupervised postprocessing technique I could manage to gain around 0.005 in my best ensemble model, which made ~0.1325 to ~1275. (Maybe I could stay in top 1% thanks to it.)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e1ef89ce-e478-5488-10e2-b4c9c305ab7a"},"outputs":[],"source":"import numpy as np\nimport pandas as pd\n\ndf_train =  pd.read_csv('../input/train.csv')\ndf_test =  pd.read_csv('../input/test.csv')"},{"cell_type":"markdown","metadata":{"_cell_guid":"941f2039-337a-ff0f-d084-d997c8dc3e9d"},"source":"After you obtain your best model, use your submission for a better submission:)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a74ea270-614e-a27c-8d3a-e96382776069"},"outputs":[],"source":"#REPLACE it with:\n#test_label = np.array(pd.read_csv('your_best_solution.csv')[\"is_duplicate\"])\ntest_label = np.random.rand(len(df_test))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"332a949a-ecca-c3fc-6925-47aea2c68ff9"},"outputs":[],"source":"from collections import defaultdict\n\nREPEAT = 2 #a reasonable number which can consider your updates iteratively but not ruin the predictions\n\nDUP_THRESHOLD = 0.5 #classification threshold for duplicates\nNOT_DUP_THRESHOLD = 0.1 #classification threshold for non-duplicates\n#Since the data is unbalanced, our mean prediction is around 0.16. So this is the reason of unbalanced thresholds\n\nMAX_UPDATE = 0.2 # maximum update on the dup probability (a high choice may ruin the predictions)\nDUP_UPPER_BOUND = 0.98 # do not update dup probabilities above this threshold\nNOT_DUP_LOWER_BOUND = 0.01 # do not update dup probabilities below this threshold\n# There is no significant gain between 0.98 and 1.00 for a dup \n# but there is significant loss if it is not really a dup"},{"cell_type":"markdown","metadata":{"_cell_guid":"c6ddb218-2333-0faf-4727-0bc330fcc991"},"source":"This part is nothing magic but basic logic. If A is a duplicate of B and C is a duplicate of B, then A is a duplicate of C."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d41d7f5e-7227-ff1b-b67e-9b36b54cfb51"},"outputs":[],"source":"for i in range(REPEAT):\n    dup_neighbors = defaultdict(set)\n\n    for dup, q1, q2 in zip(df_train[\"is_duplicate\"], df_train[\"question1\"], df_train[\"question2\"]): \n        if dup:\n            dup_neighbors[q1].add(q2)\n            dup_neighbors[q2].add(q1)\n    \n    for dup, q1, q2 in zip(test_label, df_test[\"question1\"], df_test[\"question2\"]): \n        if dup > DUP_THRESHOLD:\n            dup_neighbors[q1].add(q2)\n            dup_neighbors[q2].add(q1)\n\n    count = 0\n    for index, (q1, q2) in enumerate(zip(df_test[\"question1\"], df_test[\"question2\"])): \n        dup_neighbor_count = len(dup_neighbors[q1].intersection(dup_neighbors[q2]))\n        if dup_neighbor_count > 0 and test_label[index] < DUP_UPPER_BOUND:\n            update = min(MAX_UPDATE, (DUP_UPPER_BOUND - test_label[index])/2)\n            test_label[index] += update\n            count += 1\n\n    print(\"Edited:\", count)"},{"cell_type":"markdown","metadata":{"_cell_guid":"b9c553f1-7d34-815b-5c8b-942eea3015e8"},"source":"This part is the magic part, because having a non-duplicate common neighbor does not mean that these questions are not duplicates but if you read https://data.quora.com/First-Quora-Dataset-Release-Question-Pairs, you may find some insights.\n\n> Our original sampling method returned an imbalanced dataset with many more true examples of duplicate pairs than non-duplicates. **Therefore, we supplemented the dataset with negative examples.** One source of negative examples were pairs of “related questions” which, although **pertaining to similar topics**, are not truly semantically equivalent."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"18954434-8c3b-7edf-63f9-259773f8cecd"},"outputs":[],"source":"for i in range(REPEAT):\n    not_dup_neighbors = defaultdict(set)\n\n    for dup, q1, q2 in zip(df_train[\"is_duplicate\"], df_train[\"question1\"], df_train[\"question2\"]): \n        if not dup:\n            not_dup_neighbors[q1].add(q2)\n            not_dup_neighbors[q2].add(q1)\n    \n    for dup, q1, q2 in zip(test_label, df_test[\"question1\"], df_test[\"question2\"]): \n        if dup < NOT_DUP_THRESHOLD:\n            not_dup_neighbors[q1].add(q2)\n            not_dup_neighbors[q2].add(q1)\n\n    count = 0\n    for index, (q1, q2) in enumerate(zip(df_test[\"question1\"], df_test[\"question2\"])): \n        dup_neighbor_count = len(not_dup_neighbors[q1].intersection(not_dup_neighbors[q2]))\n        if dup_neighbor_count > 0 and test_label[index] > NOT_DUP_LOWER_BOUND:\n            update = min(MAX_UPDATE, (test_label[index] - NOT_DUP_LOWER_BOUND)/2)\n            test_label[index] -= update\n            count += 1\n\n    print(\"Edited:\", count)"},{"cell_type":"markdown","metadata":{"_cell_guid":"a61def78-686a-7b59-626c-62b3bbd6bc70"},"source":"Prepare the submission"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d9e64f54-fb45-3735-afea-c216fbcb189c"},"outputs":[],"source":"submission = pd.DataFrame({'test_id':df_test[\"test_id\"], 'is_duplicate':test_label})\n#submission.to_csv('submission.csv', index=False)"},{"cell_type":"markdown","metadata":{"_cell_guid":"357ad942-910a-b549-450d-853db3ea4636"},"source":"I will also provide the repository of my relatively lightweight solution when I have time."}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":0}