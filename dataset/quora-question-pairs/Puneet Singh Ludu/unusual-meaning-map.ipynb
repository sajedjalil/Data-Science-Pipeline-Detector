{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"97a4020f-95de-c0de-9ce2-b8e2307b4906"},"source":"Unusual meaning map: Treating question pairs as image / surface\n---------------------------------------------------------------\n"},{"cell_type":"markdown","metadata":{"_cell_guid":"97a5d3c2-fa3b-452d-d629-3116968b6c1c"},"source":"Other people have already written really nice exploratory kernels which helped me to write the minimal code myself. \n\nIn this kernel, I have tried to extract a different type of feature from which we can learn using any algorithm which can learn via image. The basic assumption behind this exercise is to capture non-sequential closeness between words.\n\nFor example:\nA Question pair has pointing arrows from each of the words of one sentence to each of the words from another sentence\n![A Question pair has pointing arrows from each of the words of one sentence to each of the words from another sentence][1]\n\n  [1]: http://image.prntscr.com/image/97e92b0357a843078b61eef5ad8a183b.png\n\nTo capture this we can create NxM matrix with Word2Vec distance between each word with other. and resize the matrix just like an image to a 10x10 matrix and use this as a feature to xgboost."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"9a81d538-d1a1-358a-8c34-7670147aeaec"},"outputs":[],"source":"import csv\nimport pip\nfrom gensim import corpora, models, similarities\nimport pandas as pd\nimport numpy as np\ntrain_file = \"../input/train.csv\"\ndf = pd.read_csv(train_file, index_col=\"id\")\ndf"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"90d844ec-5dae-bafc-ee58-79397f9dd6d5"},"outputs":[],"source":"import matplotlib.pylab as plt"},{"cell_type":"markdown","metadata":{"_cell_guid":"20a5a730-45be-6880-0191-20347be4788c"},"source":"**Extracting unique questions**"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"38f3883f-8258-97c5-9b35-dc9199d86fb5"},"outputs":[],"source":"questions = dict()\n\nfor row in df.iterrows():\n    questions[row[1]['qid1']] = row[1]['question1']\n    questions[row[1]['qid2']] = row[1]['question2']"},{"cell_type":"markdown","metadata":{"_cell_guid":"e7f1e6ae-52f1-d715-b7e6-276edfa278f0"},"source":"**Creating a simple tokenizer**"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"bf70144f-d19d-d6ab-3160-b2b666eef5f9"},"outputs":[],"source":"import re\nimport nltk\ndef basic_cleaning(string):\n    string = str(string)\n    try:\n        string = string.decode('unicode-escape')\n    except Exception:\n        pass\n    string = string.lower()\n    string = re.sub(' +', ' ', string)\n    return string\nsentences = []\nfor i in questions:\n    sentences.append(nltk.word_tokenize(basic_cleaning(questions[i])))"},{"cell_type":"markdown","metadata":{"_cell_guid":"7147800f-b006-e754-afe6-057975bdffba"},"source":"**Creating a simple Word2Vec model from the question pair, we can use a pre-trained model instead to get better results**"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a80b9de3-48df-b6b4-2701-554f53d31d84"},"outputs":[],"source":"import gensim\nmodel = gensim.models.Word2Vec(sentences, size=100, window=5, min_count=5, workers=4)"},{"cell_type":"markdown","metadata":{"_cell_guid":"fe010b8f-1fea-d87a-5733-8cfecc960157"},"source":"**A very simple term frequency and document frequency extractor** "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c5784c45-78b4-f124-5452-405777f0500e"},"outputs":[],"source":"tf = dict()\ndocf = dict()\ntotal_docs = 0\nfor qid in questions:\n    total_docs += 1\n    toks = nltk.word_tokenize(basic_cleaning(questions[qid]))\n    uniq_toks = set(toks)\n    for i in toks:\n        if i not in tf:\n            tf[i] = 1\n        else:\n            tf[i] += 1\n    for i in uniq_toks:\n        if i not in docf:\n            docf[i] = 1\n        else:\n            docf[i] += 1"},{"cell_type":"markdown","metadata":{"_cell_guid":"a7e3ce1a-4cfa-ea08-8703-0659ac38b4a3"},"source":"Mimic the IDF function but penalize the words which have fairly high score otherwise, and give a strong boost to the words which appear sporadically."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"831e8912-bfed-8bc0-478d-714286d8b618"},"outputs":[],"source":"from __future__ import division\nimport math\ndef idf(word):\n    return 1 - math.sqrt(docf[word]/total_docs)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"146df649-d4c4-434b-78f7-32c1c9299b37"},"outputs":[],"source":"print(idf(\"kenya\"))"},{"cell_type":"markdown","metadata":{"_cell_guid":"e8693960-2062-3042-6cb4-f1affe4e4986"},"source":"A simple cleaning module for feature extraction"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"01a2e717-4c55-0d11-b1a0-68c62a934f69"},"outputs":[],"source":"import re\nimport nltk\ndef basic_cleaning(string):\n    string = str(string)\n    string = string.lower()\n    string = re.sub('[0-9\\(\\)\\!\\^\\%\\$\\'\\\"\\.;,-\\?\\{\\}\\[\\]\\\\/]', ' ', string)\n    string = ' '.join([i for i in string.split() if i not in [\"a\", \"and\", \"of\", \"the\", \"to\", \"on\", \"in\", \"at\", \"is\"]])\n    string = re.sub(' +', ' ', string)\n    return string"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c19639c7-73ce-115d-ece0-afad6f390c21"},"outputs":[],"source":"def w2v_sim(w1, w2):\n    try:\n        return model.similarity(w1, w2)*idf(w1)*idf(w2)\n    except Exception:\n        return 0.0"},{"cell_type":"markdown","metadata":{"_cell_guid":"9f9e1c16-d62d-ead9-395a-ed698eafcca2"},"source":"**Visualizing features**\n\nThis function will create a 10x10 matrix using MxN word pairs among the words of question pair"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b9bd9077-7147-5649-7aa5-e3e170a7d57c"},"outputs":[],"source":"\nfrom mpl_toolkits.mplot3d.axes3d import Axes3D\nimport matplotlib.cm as cm\nfrom scipy import *\ndf = df.sample(n=30000)\ndef imagify(row):\n    s1 = row['question1']\n    s2 = row['question2']\n    t1 = list((basic_cleaning(s1)).split())\n    t2 = list((basic_cleaning(s2)).split())\n    print(\"Q1: \"+ s1)\n    print(\"Q2: \"+ s2)\n    print(\"Duplicate: \" + str(row['is_duplicate']))\n    \n    img = [[w2v_sim(x, y) for x in t1] for y in t2] \n    a = np.array(img, order='C')\n    img = np.resize(a,(10,10))\n    # print img\n    fig = plt.figure()\n    # tell imshow about color map so that only set colors are used\n    image = plt.imshow(img,interpolation='nearest')\n    # make a color bar\n    plt.colorbar(image)\n    plt.show()\ns = df.sample(n=3)\nplt.close()\ns.apply(imagify, axis=1, raw=True)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"08d3a251-1858-dbd9-58c1-b959b11fb932"},"outputs":[],"source":"from mpl_toolkits.mplot3d import Axes3D\nfrom matplotlib import cm\nimport matplotlib.pyplot as plt\nfrom numpy import *\n\nplt.close()\ndef surface(row):\n    s1 = row['question1']\n    s2 = row['question2']\n    t1 = list((basic_cleaning(s1)).split())\n    t2 = list((basic_cleaning(s2)).split())\n    print(\"Q1: \"+ s1)\n    print(\"Q2: \"+ s2)\n    print(\"Duplicate: \" + str(row['is_duplicate']))\n    \n#     img = [[w2v_sim(x, y) for x in t1] for y in t2] \n\n    fig = plt.figure()\n    ax = Axes3D(fig)\n    X = linspace(0,10,10)\n    Y = linspace(0,10,10)\n    X, Y = meshgrid(X, Y)\n    Z = [[w2v_sim(x, y) for x in t1] for y in t2] \n    a = np.array(Z, order='C')\n    Z = np.resize(a,(10,10))\n    \n    ax.plot_surface(Y, X, Z, rstride=1, cstride=1, cmap=cm.jet)\n    ax.set_xlabel(\"X Axis\")\n    ax.set_ylabel(\"Y Axis\")\n    ax.set_zlabel(\"Z Axis\")\n    plt.show()\n    \ns = df.sample(n=3)\nplt.close()\ns.apply(surface, axis=1, raw=True)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"6ea1b1e7-1d71-abeb-b6c1-eb27649c23cb"},"outputs":[],"source":"def img_feature(row):\n    s1 = row['question1']\n    s2 = row['question2']\n    t1 = list((basic_cleaning(s1)).split())\n    t2 = list((basic_cleaning(s2)).split())\n    Z = [[w2v_sim(x, y) for x in t1] for y in t2] \n    a = np.array(Z, order='C')\n    return [np.resize(a,(10,10)).flatten()]\ns = df\n\nimg = s.apply(img_feature, axis=1, raw=True)\npix_col = [[] for y in range(100)] \nfor k in img.iteritems():\n        for f in range(len(list(k[1][0]))):\n           pix_col[f].append(k[1][0][f])"},{"cell_type":"markdown","metadata":{"_cell_guid":"3f70ee65-961b-e504-6319-1dc2f1fbdb9b"},"source":"**Extracting Features**"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"0dc37757-8f09-a77a-0219-d75f6ff4b3dc"},"outputs":[],"source":"from nltk.corpus import stopwords\nfrom __future__ import division\nstops = set(stopwords.words(\"english\"))\n\ndef word_match_share(row):\n    q1words = {}\n    q2words = {}\n    for word in str(row['question1']).lower().split():\n        if word not in stops:\n            q1words[word] = 1\n    for word in str(row['question2']).lower().split():\n        if word not in stops:\n            q2words[word] = 1\n    if len(q1words) == 0 or len(q2words) == 0:\n        # The computer-generated chaff includes a few questions that are nothing but stopwords\n        return 0\n    shared_words_in_q1 = [w for w in q1words.keys() if w in q2words]\n    shared_words_in_q2 = [w for w in q2words.keys() if w in q1words]\n    R = (len(shared_words_in_q1) + len(shared_words_in_q2))/(len(q1words) + len(q2words))\n    return R\n\ntrain_word_match = df.apply(word_match_share, axis=1, raw=True)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"734650f7-3d1b-3368-e7d0-7177e77f0927"},"outputs":[],"source":"from __future__ import division\nx_train = pd.DataFrame()\n\nfor g in range(len(pix_col)):\n    x_train['img'+str(g)] = pix_col[g]\n\n    \nx_train['word_match'] = train_word_match\n\ny_train = s['is_duplicate'].values\npos_train = x_train[y_train == 1]\nneg_train = x_train[y_train == 0]\n# Now we oversample the negative class\n# There is likely a much more elegant way to do this...\np = 0.165\nscale = ((len(pos_train) / (len(pos_train) + len(neg_train))) / p) - 1\nwhile scale > 1:\n    neg_train = pd.concat([neg_train, neg_train])\n    scale -=1\nneg_train = pd.concat([neg_train, neg_train[:int(scale * len(neg_train))]])\nprint(len(pos_train) / (len(pos_train) + len(neg_train)))\n\nx_train = pd.concat([pos_train, neg_train])\ny_train = (np.zeros(len(pos_train)) + 1).tolist() + np.zeros(len(neg_train)).tolist()\ndel pos_train, neg_train"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e689a400-39d0-02b9-54ab-21b0048193a8"},"outputs":[],"source":"from sklearn.cross_validation import train_test_split\n\nx_train, x_valid, y_train, y_valid = train_test_split(x_train, y_train, test_size=0.2, random_state=4242)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b3ad6c45-cfb6-2af7-2d0a-ac832e548349"},"outputs":[],"source":"import xgboost as xgb\n\n# Set our parameters for xgboost\nparams = {}\nparams['objective'] = 'binary:logistic'\nparams['eval_metric'] = 'logloss'\nparams['eta'] = 0.02\nparams['max_depth'] = 7\n\nd_train = xgb.DMatrix(x_train, label=y_train)\nd_valid = xgb.DMatrix(x_valid, label=y_valid)\n\nwatchlist = [(d_train, 'train'), (d_valid, 'valid')]\n\nbst = xgb.train(params, d_train, 500, watchlist, early_stopping_rounds=100, verbose_eval=10)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"6b4e1dfc-ad3c-569a-b012-c6873f9bb978"},"outputs":[],"source":"import matplotlib.pyplot as plt\n%matplotlib inline\n\nplt.rcParams['figure.figsize'] = (12.0, 30.0)\nxgb.plot_importance(bst); plt.show()"},{"cell_type":"markdown","metadata":{"_cell_guid":"bbc9ef3a-d2a9-5a76-ff82-66c836e58093"},"source":"Using this technique and combining it with word match features I got log loss of **0.31858** on test dataset. \n\nI thought this feature can be of some help to others hence shared. Enjoy :)"}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":0}