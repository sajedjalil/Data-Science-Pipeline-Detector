{"metadata":{"language_info":{"codemirror_mode":{"version":3,"name":"ipython"},"nbconvert_exporter":"python","file_extension":".py","version":"3.6.1","name":"python","pygments_lexer":"ipython3","mimetype":"text/x-python"},"kernelspec":{"language":"python","name":"python3","display_name":"Python 3"}},"nbformat_minor":1,"cells":[{"metadata":{"_uuid":"a015673c7c6bd389f1f0ce298c8d5937bc3712ba","_cell_guid":"0d43fb36-23f2-404a-9440-b207bffbc782"},"cell_type":"markdown","source":"# Quora Question Pairs 69% Accuracy\nThe Kernel is based on the reasearch paper \"A simple but tough to beat baseline for sentence embeddings\" by princeton university so we would like to thank for their great research on sentence embeddings.\nlink : https://openreview.net/forum?id=SyK00v5xx"},{"metadata":{"_uuid":"aedf470ffbd958a152a546350e4c691b86cb4298","_kg_hide-input":false,"collapsed":true,"_kg_hide-output":true,"_cell_guid":"d7915bb7-36ea-4922-9322-2e5a1edbcc05"},"outputs":[],"cell_type":"code","source":"from __future__ import division\nimport gensim\nimport numpy as np\nfrom sklearn.decomposition import PCA\nimport gensim.models.word2vec\nfrom collections import Counter\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport itertools\nfrom nltk.tokenize import word_tokenize\nimport re\nfrom nltk.corpus import stopwords\nfrom nltk.stem import SnowballStemmer\nfrom string import punctuation\nimport csv\nprint(\"All Libraries Successfully imported\")","execution_count":null},{"metadata":{"_uuid":"2a0c86b01bdd6db54b5505f89a45986e6d643aed","collapsed":true,"_cell_guid":"7c642534-04d1-468f-98e1-06fb573ceff2"},"cell_type":"markdown","source":"## The function \"text_to_wordlist\" is from\n## https://www.kaggle.com/currie32/quora-question-pairs/the-importance-of-cleaning-text"},{"metadata":{"_uuid":"381cc81031800b8957fe39bfe7999aec3853bd3d","collapsed":true,"_cell_guid":"2f8ad94b-5cdd-41dc-a1dd-e7e86f190c92"},"outputs":[],"cell_type":"code","source":"def text_to_wordlist(text, remove_stopwords=False, stem_words=False):\n    # Clean the text, with the option to remove stopwords and to stem words.\n    \n    # Convert words to lower case and split them\n    text = text.lower().split()\n\n    # Optionally, remove stop words\n    if remove_stopwords:\n        stops = set(stopwords.words(\"english\"))\n        text = [w for w in text if not w in stops]\n    \n    text = \" \".join(text)\n\n    # Clean the text\n    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n    text = re.sub(r\"what's\", \"what is \", text)\n    text = re.sub(r\"\\'s\", \" \", text)\n    text = re.sub(r\"\\'ve\", \" have \", text)\n    text = re.sub(r\"can't\", \"cannot \", text)\n    text = re.sub(r\"n't\", \" not \", text)\n    text = re.sub(r\"i'm\", \"i am \", text)\n    text = re.sub(r\"\\'re\", \" are \", text)\n    text = re.sub(r\"\\'d\", \" would \", text)\n    text = re.sub(r\"\\'ll\", \" will \", text)\n    text = re.sub(r\",\", \" \", text)\n    text = re.sub(r\"\\.\", \" \", text)\n    text = re.sub(r\"!\", \" ! \", text)\n    text = re.sub(r\"\\/\", \" \", text)\n    text = re.sub(r\"\\^\", \" ^ \", text)\n    text = re.sub(r\"\\+\", \" + \", text)\n    text = re.sub(r\"\\-\", \" - \", text)\n    text = re.sub(r\"\\=\", \" = \", text)\n    text = re.sub(r\"'\", \" \", text)\n    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n    text = re.sub(r\":\", \" : \", text)\n    text = re.sub(r\" e g \", \" eg \", text)\n    text = re.sub(r\" b g \", \" bg \", text)\n    text = re.sub(r\" u s \", \" american \", text)\n    text = re.sub(r\"\\0s\", \"0\", text)\n    text = re.sub(r\" 9 11 \", \"911\", text)\n    text = re.sub(r\"e - mail\", \"email\", text)\n    text = re.sub(r\"j k\", \"jk\", text)\n    text = re.sub(r\"\\s{2,}\", \" \", text)\n    \n    # Optionally, shorten words to their stems\n    if stem_words:\n        text = text.split()\n        stemmer = SnowballStemmer('english')\n        stemmed_words = [stemmer.stem(word) for word in text]\n        text = \" \".join(stemmed_words)\n    \n    # Return a list of words\n    return(text)","execution_count":null},{"metadata":{"_uuid":"0bfd4b9618ce8b45ea6cfda030cc857bab2989a3","_cell_guid":"b7efba69-9b70-4440-9c89-fd69e405a4ed"},"cell_type":"markdown","source":"## Loading word vectors of Google to get Sentence Embeddings "},{"metadata":{"_uuid":"e7c92699d32a4b13c1692f3150702f217c2fca36","collapsed":true,"_cell_guid":"647b9c29-15df-4853-ab5f-882b1e2e1944"},"outputs":[],"cell_type":"code","source":"#Initializing word2vec\n\ndef gensim_load_vec(path=\"GoogleNews-vectors-negative300.bin\"):\n    #use gensim_emb.wv.index2word if used this way to load vectors\n    #gensim_emb = gensim.models.word2vec.Word2Vec.load(path)\n    gensim_emb =  gensim.models.KeyedVectors.load_word2vec_format(path, binary=True,limit=500000)\n    vocab = gensim_emb.index2word\n    vec = gensim_emb.syn0\n    shape = gensim_emb.syn0.shape\n    return gensim_emb, vec, shape, vocab","execution_count":null},{"metadata":{"_uuid":"84eb4c9854a63e66705d2189cd0cc543e6909305","collapsed":true,"_cell_guid":"48571027-3e89-4354-bb38-f1ed7597a2ee"},"outputs":[],"cell_type":"code","source":"def map_word_frequency(document):\n    return Counter(itertools.chain(*document))","execution_count":null},{"metadata":{"_uuid":"fa8125fd4b48d1c5ec93e26e196ae8e52d7cf801","_cell_guid":"c28e2c28-e7dc-41c3-a530-76fd7ff037fb"},"cell_type":"markdown","source":"# Sentence Embedding Calculation as Described in the Research Paper"},{"metadata":{"_uuid":"a452b99ac8509d7699b0b2ab28cfcc33483266cf","collapsed":true,"_cell_guid":"82be2e61-798c-452e-b002-efb80210e8a4"},"outputs":[],"cell_type":"code","source":"\ndef sentence2vec(tokenised_sentence_list, embedding_size, word_emb_model, a = 1e-3):\n    sentence_vecs = []\n    try:\n        word_counts = map_word_frequency(tokenised_sentence_list)\n        sentence_set=[]\n        for sentence in tokenised_sentence_list:\n            vs = np.zeros(embedding_size)\n            sentence_length = len(sentence)\n            for word in sentence:\n                a_value = a / (a + word_counts[word]) # smooth inverse frequency, SIF\n            try:\n                vs = np.add(vs, np.multiply(a_value, word_emb_model[word])) # vs += sif * word_vector\n            except Exception as e:\n                pass\n            vs = np.divide(vs, sentence_length) # weighted average\n            sentence_set.append(vs)\n    except Exception as e:\n           print(\"Exception in sentence embedding\")\n    return sentence_set","execution_count":null},{"metadata":{"_uuid":"fc20d7232cab825d08f335d8fa51e60f4ed4b047","_kg_hide-output":true,"collapsed":true,"_cell_guid":"dd321fdd-e87e-422b-bd20-6500e271c700"},"outputs":[],"cell_type":"code","source":"#Testing with one pair\n\ntext1 = \"How can I be good geologist\"\ntext2 = \"should I do be great geologist\"\ntoken1 = text_to_wordlist(text1)\ntoken2 = text_to_wordlist(text2)\ntoken1 = word_tokenize(token1)\ntoken2 = word_tokenize(token2)\nsent_list=[]\nsent_list.append(token1)\nsent_list.append(token2)\ngensim_emb, vec, shape, vocab = gensim_load_vec()","execution_count":null},{"metadata":{"_uuid":"781354d2f73284637af809f52201654f57b75f8d","_kg_hide-output":true,"collapsed":true,"_cell_guid":"072c2556-6d92-4957-af8b-925942a93634"},"outputs":[],"cell_type":"code","source":"sent_emb = sentence2vec(sent_list,300,gensim_emb)","execution_count":null},{"metadata":{"_uuid":"539d5b416769b1de81eea124804406d2f7c2aa80","collapsed":true,"_cell_guid":"cb6c00a1-9470-4608-83cd-3b85d230e670"},"outputs":[],"cell_type":"code","source":"score=float(cosine_similarity([sent_emb[0]],[sent_emb[1]]))\nprint(score)","execution_count":null}],"nbformat":4}