{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"88c555ce-b0ab-6785-922a-a1360ee1af95"},"source":"# Using"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"2ac06317-c4d6-4c31-470b-92f6185aa633"},"outputs":[],"source":"import numpy as np\nimport pandas as pd\nimport xgboost as xgb\nimport datetime\nimport operator\nfrom sklearn.cross_validation import train_test_split\nfrom collections import Counter\nfrom nltk.corpus import stopwords\nimport matplotlib.pyplot as plt\nfrom pylab import plot, show, subplot, specgram, imshow, savefig\n\nRS = 12357\nROUNDS = 200\n\nprint(\"Started\")\nnp.random.seed(RS)\ninput_folder = '../input/'"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"ab873612-eff0-bc36-877f-d7ae68228278"},"outputs":[],"source":"def train_xgb(X, y, params):\n\tprint(\"Will train XGB for {} rounds, RandomSeed: {}\".format(ROUNDS, RS))\n\tx, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=RS)\n\n\txg_train = xgb.DMatrix(x, label=y_train)\n\txg_val = xgb.DMatrix(X_val, label=y_val)\n\n\twatchlist  = [(xg_train,'train'), (xg_val,'eval')]\n\treturn xgb.train(params, xg_train, ROUNDS, watchlist)\n\ndef predict_xgb(clr, X_test):\n\treturn clr.predict(xgb.DMatrix(X_test))\n\ndef create_feature_map(features):\n\toutfile = open('xgb.fmap', 'w')\n\ti = 0\n\tfor feat in features:\n\t\toutfile.write('{0}\\t{1}\\tq\\n'.format(i, feat))\n\t\ti = i + 1\n\toutfile.close()\n\ndef add_word_count(x, df, word):\n\tx['q1_' + word] = df['question1'].apply(lambda x: (word in str(x).lower())*1)\n\tx['q2_' + word] = df['question2'].apply(lambda x: (word in str(x).lower())*1)\n\tx[word + '_both'] = x['q1_' + word] * x['q2_' + word]"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"2a5bc334-bec2-d0cd-8875-c290db70e76f"},"outputs":[],"source":"%%time\n# If a word appears only once, we ignore it completely (likely a typo)\n# Epsilon defines a smoothing constant, which makes the effect of extremely rare words smaller\ndef get_weight(count, eps=10000, min_count=2):\n    return 0 if count < min_count else 1 / (count + eps)\n\ndef word_shares(row):\n    q1_list = str(row['question1']).lower().split()\n    q1 = set(q1_list)\n    q1words = q1.difference(stops)\n    if len(q1words) == 0:\n        return '0:0:0:0:0:0:0:0'\n\n    q2_list = str(row['question2']).lower().split()\n    q2 = set(q2_list)\n    q2words = q2.difference(stops)\n    if len(q2words) == 0:\n        return '0:0:0:0:0:0:0:0'\n\n    words_hamming = sum(1 for i in zip(q1_list, q2_list) if i[0]==i[1])/max(len(q1_list), len(q2_list))\n\n    q1stops = q1.intersection(stops)\n    q2stops = q2.intersection(stops)\n\n    q1_2gram = set([i for i in zip(q1_list, q1_list[1:])])\n    q2_2gram = set([i for i in zip(q2_list, q2_list[1:])])\n\n    shared_2gram = q1_2gram.intersection(q2_2gram)\n\n    shared_words = q1words.intersection(q2words)\n    shared_weights = [weights.get(w, 0) for w in shared_words]\n    q1_weights = [weights.get(w, 0) for w in q1words]\n    q2_weights = [weights.get(w, 0) for w in q2words]\n    total_weights = q1_weights + q1_weights\n\n    R1 = np.sum(shared_weights) / np.sum(total_weights) #tfidf share\n    R2 = len(shared_words) / (len(q1words) + len(q2words) - len(shared_words)) #count share\n    R31 = len(q1stops) / len(q1words) #stops in q1\n    R32 = len(q2stops) / len(q2words) #stops in q2\n    Rcosine_denominator = (np.sqrt(np.dot(q1_weights,q1_weights))*np.sqrt(np.dot(q2_weights,q2_weights)))\n    Rcosine = np.dot(shared_weights, shared_weights)/Rcosine_denominator\n    if len(q1_2gram) + len(q2_2gram) == 0:\n        R2gram = 0\n    else:\n        R2gram = len(shared_2gram) / (len(q1_2gram) + len(q2_2gram))\n    return '{}:{}:{}:{}:{}:{}:{}:{}'.format(R1, R2, len(shared_words), R31, R32, R2gram, Rcosine, words_hamming)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"3ecc55fb-c699-91dc-a1ad-7240afcccf8b"},"outputs":[],"source":"%%time\n\nparams = {}\nparams['objective'] = 'binary:logistic'\nparams['eval_metric'] = 'logloss'\nparams['eta'] = 0.41\nparams['max_depth'] = 6\nparams['silent'] = 1\nparams['seed'] = RS\n\ndf_train = pd.read_csv(input_folder + 'train.csv')\ndf_test  = pd.read_csv(input_folder + 'test.csv')\n\nprint(\"Original data: X_train: {}, X_test: {}\".format(df_train.shape, df_test.shape))\nprint(\"Features processing, be patient...\")\n\n\ntrain_qs = pd.Series(df_train['question1'].tolist() + \\\n                     df_train['question2'].tolist()).astype(str)\nwords = (\" \".join(train_qs)).lower().split()\ncounts = Counter(words)\nweights = {word: get_weight(count) for word, count in counts.items()}\n\nstops = set(stopwords.words(\"english\"))\n\nques = pd.concat([df_train[['question1', 'question2']], \\\n        df_test[['question1', 'question2']]], axis=0).reset_index(drop='index')\nques.shape\nfrom collections import defaultdict\nq_dict = defaultdict(set)\nfor i in range(ques.shape[0]):\n        q_dict[ques.question1[i]].add(ques.question2[i])\n        q_dict[ques.question2[i]].add(ques.question1[i])\n        \ndef q1_q2_intersect(row):\n    return(len(set(q_dict[row['question1']]).intersection(set(q_dict[row['question2']]))))\ndf_train['q1_q2_intersect'] = df_train.apply(q1_q2_intersect, axis=1, raw=True)\ndf_test['q1_q2_intersect'] = df_test.apply(q1_q2_intersect, axis=1, raw=True)\n\ndf = pd.concat([df_train, df_test])\ndf.to_csv(\"xgb_seed{}_n{}.csv\".format(RS, ROUNDS), index=False)\ndf['word_shares'] = df.apply(word_shares, axis=1, raw=True)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"8145a095-7274-4b46-2a85-2d7fdb6e0dfe"},"outputs":[],"source":"%%time\n\nx = pd.DataFrame()\n\nx['word_match']       = df['word_shares'].apply(lambda x: float(x.split(':')[0]))\nx['word_match_2root'] = np.sqrt(x['word_match'])\nx['tfidf_word_match'] = df['word_shares'].apply(lambda x: float(x.split(':')[1]))\nx['shared_count']     = df['word_shares'].apply(lambda x: float(x.split(':')[2]))\n\nx['stops1_ratio']     = df['word_shares'].apply(lambda x: float(x.split(':')[3]))\nx['stops2_ratio']     = df['word_shares'].apply(lambda x: float(x.split(':')[4]))\nx['shared_2gram']     = df['word_shares'].apply(lambda x: float(x.split(':')[5]))\nx['cosine']           = df['word_shares'].apply(lambda x: float(x.split(':')[6]))\nx['words_hamming']    = df['word_shares'].apply(lambda x: float(x.split(':')[7]))\nx['diff_stops_r']     = abs(x['stops1_ratio'] - x['stops2_ratio'])\n\nx['len_q1'] = df['question1'].apply(lambda x: len(str(x)))\nx['len_q2'] = df['question2'].apply(lambda x: len(str(x)))\nx['diff_len'] = abs(x['len_q1'] - x['len_q2'])\n\nx['caps_count_q1'] = df['question1'].apply(lambda x:sum(1 for i in str(x) if i.isupper()))\nx['caps_count_q2'] = df['question2'].apply(lambda x:sum(1 for i in str(x) if i.isupper()))\nx['diff_caps'] = abs(x['caps_count_q1'] - x['caps_count_q2'])\n\nx['len_char_q1'] = df['question1'].apply(lambda x: len(str(x).replace(' ', '')))\nx['len_char_q2'] = df['question2'].apply(lambda x: len(str(x).replace(' ', '')))\nx['diff_len_char'] = abs(x['len_char_q1'] - x['len_char_q2'])\n\nx['len_word_q1'] = df['question1'].apply(lambda x: len(str(x).split()))\nx['len_word_q2'] = df['question2'].apply(lambda x: len(str(x).split()))\nx['diff_len_word'] = abs(x['len_word_q1'] - x['len_word_q2'])\n\nx['avg_world_len1'] = x['len_char_q1'] / x['len_word_q1']\nx['avg_world_len2'] = x['len_char_q2'] / x['len_word_q2']\nx['diff_avg_word'] = abs(x['avg_world_len1'] - x['avg_world_len2'])\n\nx['exactly_same'] = (df['question1'] == df['question2']).astype(int)\nx['duplicated'] = df.duplicated(['question1','question2']).astype(int)\n\nx['q1_q2_intersect']=df['q1_q2_intersect']\nx=x.drop(['len_word_q2','len_word_q1','avg_world_len1','avg_world_len2','len_char_q1','len_char_q2'],axis=1)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"cfd33189-5847-7a78-560d-ce0479fc6ccf"},"outputs":[],"source":"%%time\n\nprint(x.columns)\nprint(x.describe())"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"8542c36f-cb2f-d031-eb66-0c0ba926ab41"},"outputs":[],"source":"%%time\n\nfeature_names = list(x.columns.values)\ncreate_feature_map(feature_names)\nprint(\"Features: {}\".format(feature_names))\n\nx_train = x[:df_train.shape[0]]\nx_test  = x[df_train.shape[0]:]\ny_train = df_train['is_duplicate'].values\ndel x, df_train\n\nif 1: # Now we oversample the negative class - on your own risk of overfitting!\n    pos_train = x_train[y_train == 1]\n    neg_train = x_train[y_train == 0]\n\n    print(\"Oversampling started for proportion: {}\".format(len(pos_train) / (len(pos_train) + len(neg_train))))\n    p = 0.165\n    scale = ((len(pos_train) / (len(pos_train) + len(neg_train))) / p) - 1\n    while scale > 1:\n        neg_train = pd.concat([neg_train, neg_train])\n        scale -=1\n    neg_train = pd.concat([neg_train, neg_train[:int(scale * len(neg_train))]])\n    print(\"Oversampling done, new proportion: {}\".format(len(pos_train) / (len(pos_train) + len(neg_train))))\n\n    x_train = pd.concat([pos_train, neg_train])\n    y_train = (np.zeros(len(pos_train)) + 1).tolist() + np.zeros(len(neg_train)).tolist()\n    del pos_train, neg_train"},{"cell_type":"markdown","metadata":{"_cell_guid":"675ab2dd-36d9-09e4-dddb-528009fba42e"},"source":"# Training XGBoost"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"3d04512b-1622-9e53-ab34-e7638b4a29e3"},"outputs":[],"source":"print(\"Training data: X_train: {}, Y_train: {}, X_test: {}\".format(x_train.shape, len(y_train), x_test.shape))\nclr = train_xgb(x_train, y_train, params)\npreds = predict_xgb(clr, x_test)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"ef069c1a-c0c6-19e2-08f8-f2631b4284f1"},"outputs":[],"source":"print(\"Writing output...\")\nsub = pd.DataFrame()\nsub['test_id'] = df_test['test_id']\nsub['is_duplicate'] = preds *.75\nsub.to_csv(\"xgb_seed{}_n{}.csv\".format(RS, ROUNDS), index=False)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"9ad0149f-8a8e-f475-8348-ce63c67554fc"},"outputs":[],"source":"print('over')"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"5b82eb23-fa54-ed7e-79ab-c7bda9851e55"},"outputs":[],"source":""}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":0}