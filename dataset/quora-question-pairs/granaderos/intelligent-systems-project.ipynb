{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"171f1c05-31f1-0db5-328b-999b94f48276"},"outputs":[],"source":"import numpy as np\nimport pandas as pd\nimport xgboost as xgb\nimport datetime\nimport operator\nfrom sklearn.cross_validation import train_test_split\nfrom collections import Counter\nfrom nltk.corpus import stopwords\nimport matplotlib.pyplot as plt\nfrom pylab import plot, show, subplot, specgram, imshow, savefig\nfrom collections import defaultdict\n\nRS = 12357\nROUNDS = 315\n#ROUNDS = 75\n\nprint(\"Started\")\nnp.random.seed(RS)\ninput_folder = '../input/quora-question-pairs/'\n\ndef train_xgb(X, y, params):\n\tprint(\"Will train XGB for {} rounds, RandomSeed: {}\".format(ROUNDS, RS))\n\tx, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=RS)\n\n\txg_train = xgb.DMatrix(x, label=y_train)\n\txg_val = xgb.DMatrix(X_val, label=y_val)\n\n\twatchlist  = [(xg_train,'train'), (xg_val,'eval')]\n\treturn xgb.train(params, xg_train, ROUNDS, watchlist)\n\ndef predict_xgb(clr, X_test):\n\treturn clr.predict(xgb.DMatrix(X_test))\n\ndef create_feature_map(features):\n\toutfile = open('xgb.fmap', 'w')\n\ti = 0\n\tfor feat in features:\n\t\toutfile.write('{0}\\t{1}\\tq\\n'.format(i, feat))\n\t\ti = i + 1\n\toutfile.close()\n\ndef add_word_count(x, df, word):\n\tx['q1_' + word] = df['question1'].apply(lambda x: (word in str(x).lower())*1)\n\tx['q2_' + word] = df['question2'].apply(lambda x: (word in str(x).lower())*1)\n\tx[word + '_both'] = x['q1_' + word] * x['q2_' + word]\n\ndef main():\n\tparams = {}\n\tparams['objective'] = 'binary:logistic'\n\tparams['eval_metric'] = 'logloss'\n\tparams['eta'] = 0.11\n\tparams['max_depth'] = 5\n\tparams['silent'] = 1\n\tparams['seed'] = RS\n\n\tdf_train = pd.read_csv(input_folder + 'train.csv')\n\tdf_test  = pd.read_csv(input_folder + 'test.csv')\n\n\tprint(\"Original data: X_train: {}, X_test: {}\".format(df_train.shape, df_test.shape))\n\n\tprint(\"Features processing, be patient...\")\n\n\t# If a word appears only once, we ignore it completely (likely a typo)\n\t# Epsilon defines a smoothing constant, which makes the effect of extremely rare words smaller\n\tdef get_weight(count, eps=10000, min_count=2):\n\t\treturn 0 if count < min_count else 1 / (count + eps)\n\n\ttrain_qs = pd.Series(df_train['question1'].tolist() + df_train['question2'].tolist()).astype(str)\n\twords = (\" \".join(train_qs)).lower().split()\n\tcounts = Counter(words)\n\tweights = {word: get_weight(count) for word, count in counts.items()}\n\n\tstops = set(stopwords.words(\"english\"))\n\tdef word_shares(row):\n\t\tq1_list = str(row['question1']).lower().split()\n\t\tq1 = set(q1_list)\n\t\tq1words = q1.difference(stops)\n\t\tif len(q1words) == 0:\n\t\t\treturn '0:0:0:0:0:0:0:0'\n        \n\t\tq2_list = str(row['question2']).lower().split()\n\t\tq2 = set(q2_list)\n\t\tq2words = q2.difference(stops)\n\t\tif len(q2words) == 0:\n\t\t\treturn '0:0:0:0:0:0:0:0'\n\n\t\twords_hamming = sum(1 for i in zip(q1_list, q2_list) if i[0]==i[1])/max(len(q1_list), len(q2_list))\n\n\t\tq1stops = q1.intersection(stops)\n\t\tq2stops = q2.intersection(stops)\n\n\t\tq1_2gram = set([i for i in zip(q1_list, q1_list[1:])])\n\t\tq2_2gram = set([i for i in zip(q2_list, q2_list[1:])])\n\n\t\tshared_2gram = q1_2gram.intersection(q2_2gram)\n\n\t\tshared_words = q1words.intersection(q2words)\n\t\tshared_weights = [weights.get(w, 0) for w in shared_words]\n\t\tq1_weights = [weights.get(w, 0) for w in q1words]\n\t\tq2_weights = [weights.get(w, 0) for w in q2words]\n\t\ttotal_weights = q1_weights + q1_weights\n\t\t\n\t\tR1 = np.sum(shared_weights) / np.sum(total_weights) #tfidf share\n\t\tR2 = len(shared_words) / (len(q1words) + len(q2words) - len(shared_words)) #count share\n\t\tR31 = len(q1stops) / len(q1words) #stops in q1\n\t\tR32 = len(q2stops) / len(q2words) #stops in q2\n\t\tRcosine_denominator = (np.sqrt(np.dot(q1_weights,q1_weights))*np.sqrt(np.dot(q2_weights,q2_weights)))\n\t\tRcosine = np.dot(shared_weights, shared_weights)/Rcosine_denominator\n\t\tif len(q1_2gram) + len(q2_2gram) == 0:\n\t\t\tR2gram = 0\n\t\telse:\n\t\t\tR2gram = len(shared_2gram) / (len(q1_2gram) + len(q2_2gram))\n\t\treturn '{}:{}:{}:{}:{}:{}:{}:{}'.format(R1, R2, len(shared_words), R31, R32, R2gram, Rcosine, words_hamming)\n\n\tdf = pd.concat([df_train, df_test])\n\tdf['word_shares'] = df.apply(word_shares, axis=1, raw=True)\n\n\tx = pd.DataFrame()\n\n\tx['word_match']       = df['word_shares'].apply(lambda x: float(x.split(':')[0]))\n\tx['word_match_2root'] = np.sqrt(x['word_match'])\n\tx['tfidf_word_match'] = df['word_shares'].apply(lambda x: float(x.split(':')[1]))\n\tx['shared_count']     = df['word_shares'].apply(lambda x: float(x.split(':')[2]))\n\n\tx['stops1_ratio']     = df['word_shares'].apply(lambda x: float(x.split(':')[3]))\n\tx['stops2_ratio']     = df['word_shares'].apply(lambda x: float(x.split(':')[4]))\n\tx['shared_2gram']     = df['word_shares'].apply(lambda x: float(x.split(':')[5]))\n\tx['cosine']           = df['word_shares'].apply(lambda x: float(x.split(':')[6]))\n\tx['words_hamming']    = df['word_shares'].apply(lambda x: float(x.split(':')[7]))\n\tx['diff_stops_r']     = x['stops1_ratio'] - x['stops2_ratio']\n\n\tx['len_q1'] = df['question1'].apply(lambda x: len(str(x)))\n\tx['len_q2'] = df['question2'].apply(lambda x: len(str(x)))\n\tx['diff_len'] = x['len_q1'] - x['len_q2']\n\t\n\tx['caps_count_q1'] = df['question1'].apply(lambda x:sum(1 for i in str(x) if i.isupper()))\n\tx['caps_count_q2'] = df['question2'].apply(lambda x:sum(1 for i in str(x) if i.isupper()))\n\tx['diff_caps'] = x['caps_count_q1'] - x['caps_count_q2']\n\n\tx['len_char_q1'] = df['question1'].apply(lambda x: len(str(x).replace(' ', '')))\n\tx['len_char_q2'] = df['question2'].apply(lambda x: len(str(x).replace(' ', '')))\n\tx['diff_len_char'] = x['len_char_q1'] - x['len_char_q2']\n\n\tx['len_word_q1'] = df['question1'].apply(lambda x: len(str(x).split()))\n\tx['len_word_q2'] = df['question2'].apply(lambda x: len(str(x).split()))\n\tx['diff_len_word'] = x['len_word_q1'] - x['len_word_q2']\n\n\tx['avg_world_len1'] = x['len_char_q1'] / x['len_word_q1']\n\tx['avg_world_len2'] = x['len_char_q2'] / x['len_word_q2']\n\tx['diff_avg_word'] = x['avg_world_len1'] - x['avg_world_len2']\n\n\tx['exactly_same'] = (df['question1'] == df['question2']).astype(int)\n\tx['duplicated'] = df.duplicated(['question1','question2']).astype(int)\n\tadd_word_count(x, df,'how')\n\tadd_word_count(x, df,'what')\n\tadd_word_count(x, df,'which')\n\tadd_word_count(x, df,'who')\n\tadd_word_count(x, df,'where')\n\tadd_word_count(x, df,'when')\n\tadd_word_count(x, df,'why')\n\n\n\n\n\tdf1 = df_train[['question1']].copy()\n\tdf2 = df_train[['question2']].copy()\n\tdf1_test = df_test[['question1']].copy()\n\tdf2_test = df_test[['question2']].copy()\n\n\tdf2.rename(columns = {'question2':'question1'},inplace=True)\n\tdf2_test.rename(columns = {'question2':'question1'},inplace=True)\n\n\ttrain_questions = df1.append(df2)\n\ttrain_questions = train_questions.append(df1_test)\n\ttrain_questions = train_questions.append(df2_test)\n\t#train_questions.drop_duplicates(subset = ['qid1'],inplace=True)\n\ttrain_questions.drop_duplicates(subset = ['question1'],inplace=True)\n\n\ttrain_questions.reset_index(inplace=True,drop=True)\n\tquestions_dict = pd.Series(train_questions.index.values,index=train_questions.question1.values).to_dict()\n\ttrain_cp = df_train.copy()\n\ttest_cp = df_test.copy()\n\ttrain_cp.drop(['qid1','qid2'],axis=1,inplace=True)\n\n\ttest_cp['is_duplicate'] = -1\n\ttest_cp.rename(columns={'test_id':'id'},inplace=True)\n\tcomb = pd.concat([train_cp,test_cp])\n\n\tx['q1_hash'] = comb['question1'].map(questions_dict)\n\tx['q2_hash'] = comb['question2'].map(questions_dict)\n\n\tq1_vc = x.q1_hash.value_counts().to_dict()\n\tq2_vc = x.q2_hash.value_counts().to_dict()\n\n\tdef try_apply_dict(x,dict_to_apply):\n\t\ttry:\n\t\t\treturn dict_to_apply[x]\n\t\texcept KeyError:\n\t\t\treturn 0\n\t#map to frequency space\n\tx['q1_freq'] = x['q1_hash'].map(lambda x: try_apply_dict(x,q1_vc) + try_apply_dict(x,q2_vc))\n\tx['q2_freq'] = x['q2_hash'].map(lambda x: try_apply_dict(x,q1_vc) + try_apply_dict(x,q2_vc))\n\tdel q1_vc, q2_vc, comb, test_cp, train_cp, df1, df2, df1_test, df2_test, train_questions, questions_dict\n\t\n\tfeature_names = list(x.columns.values)\n\tcreate_feature_map(feature_names)\n\tprint(\"Features: {}\".format(feature_names))\n\n\tx_train = x[:df_train.shape[0]]\n\tx_test  = x[df_train.shape[0]:]\n\ty_train = df_train['is_duplicate'].values\n\tdel x, df_train\n\n\tif 1: # Now we oversample the negative class - on your own risk of overfitting!\n\t\tpos_train = x_train[y_train == 1]\n\t\tneg_train = x_train[y_train == 0]\n\n\t\tprint(\"Oversampling started for proportion: {}\".format(len(pos_train) / (len(pos_train) + len(neg_train))))\n\t\tp = 0.165\n\t\tscale = ((len(pos_train) / (len(pos_train) + len(neg_train))) / p) - 1\n\t\twhile scale > 1:\n\t\t\tneg_train = pd.concat([neg_train, neg_train])\n\t\t\tscale -=1\n\t\tneg_train = pd.concat([neg_train, neg_train[:int(scale * len(neg_train))]])\n\t\tprint(\"Oversampling done, new proportion: {}\".format(len(pos_train) / (len(pos_train) + len(neg_train))))\n\n\t\tx_train = pd.concat([pos_train, neg_train])\n\t\ty_train = (np.zeros(len(pos_train)) + 1).tolist() + np.zeros(len(neg_train)).tolist()\n\t\tdel pos_train, neg_train\n\t\n\tprint(\"Training data: X_train: {}, Y_train: {}, X_test: {}\".format(x_train.shape, len(y_train), x_test.shape))\n\tclr = train_xgb(x_train, y_train, params)\n\tdel x_train, y_train\n\tpreds = predict_xgb(clr, x_test)\n\n\tprint(\"Writing output...\")\n\tsub = pd.DataFrame()\n\tsub['test_id'] = df_test['test_id']\n\tsub['is_duplicate'] = preds *.75\n\tsub.to_csv(\"xgb_seed{}_n{}.csv\".format(RS, ROUNDS), index=False)\n\n\tprint(\"Features importances...\")\n\timportance = clr.get_fscore(fmap='xgb.fmap')\n\timportance = sorted(importance.items(), key=operator.itemgetter(1))\n\tft = pd.DataFrame(importance, columns=['feature', 'fscore'])\n\n\tft.plot(kind='barh', x='feature', y='fscore', legend=False, figsize=(10, 25))\n\tplt.gcf().savefig('features_importance.png')\n\nmain()\nprint(\"Done.\")"}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":0}