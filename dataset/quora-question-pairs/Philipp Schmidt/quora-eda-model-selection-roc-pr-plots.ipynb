{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"d68e8e76-35a8-3224-da55-3512973082c3"},"source":"# Quora Question-pair classification\n\nThis competition is about modelling whether a pair of questions on Quora is asking the same question. For this problem we have about **400.000** training examples. Each row consists of two sentences and a binary label that indicates to us whether the two questions were the same or not.\n\nInspired by this nice [kernel](https://www.kaggle.com/arthurtok/d/mcdonalds/nutrition-facts/super-sized-we-macdonald-s-nutritional-metrics) from [Anisotropic](https://www.kaggle.com/arthurtok) I've added a few interactive 2D and 3D scatter plots.\nTo get an insight into how the duplicates evolve over the number of words in the questions, I've added a plotly animation that encodes number of words and word share similarity in a scatter plot.\n\n**We will be looking in detail at:**\n\n* question pair TF-IDF encodings\n* basic feature engineering and their embeddings in lower dimensional spaces\n* parallel coordinates visualization\n* model selection and evaluation + sample submission.\n\nIf you like this kernel, please upvote it :D, thanks!\n\n\n----------\n\n\nAdded a final section for cross-validated model selection and evaluation. We will look at standard binary classification metrics, like ROC and PR curves and their AUCs. The best (linear) model that we found then generates a submission."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a93da02c-4974-9116-74ce-3c8c45c3d581"},"outputs":[],"source":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom subprocess import check_output\n\n%matplotlib inline\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.tools as tls\n\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\ndf = pd.read_csv(\"../input/train.csv\").fillna(\"\")\ndf.head() "},{"cell_type":"markdown","metadata":{"_cell_guid":"0f3b994b-f2f3-2847-c635-f28672735666"},"source":"So we have six columns in total one of which is the label."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c2bc3874-d7cd-faa8-037f-dccdb4fc8e7c"},"outputs":[],"source":"df.info()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"8234a5a5-822a-6f9e-f4a4-72464f5c2b33"},"outputs":[],"source":"df.shape"},{"cell_type":"markdown","metadata":{"_cell_guid":"53d5b1fa-4cb4-7217-ee75-797cfdaedfae"},"source":"We have a fairly balanced dataset here."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f3e4d73e-ee05-ded2-2c86-962164638e58"},"outputs":[],"source":"df.groupby(\"is_duplicate\")['id'].count().plot.bar()"},{"cell_type":"markdown","metadata":{"_cell_guid":"90970074-a8d1-78b2-e363-9c16045e0bb0"},"source":"# Feature construction\n\nWe will now construct a basic set of features that we will later use to embed our samples with.\n\nThe first we will be looking at is rather standard TF-IDF encoding for each of the questions. In order to limit the computational complexity and storage requirements we will only encode the top terms across all documents with TF-IDF and also look at a subsample of the data."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"20befdf0-f412-1df5-071c-4bb0c688a50c"},"outputs":[],"source":"dfs = df[0:2500]\ndfs.groupby(\"is_duplicate\")['id'].count().plot.bar()"},{"cell_type":"markdown","metadata":{"_cell_guid":"88a6100d-4b89-28b2-004f-2264fceb35e5"},"source":"The subsample still has a very similar label distribution, ok to continue like that, without taking a deeper look how to achieve better sampling than just taking the first rows of the dataset.\n\nCreate a dataframe where the top 50% of rows have only question 1 and the bottom 50% have only question 2, same ordering per halve as in the original dataframe."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a2dc8cde-e352-0af0-0852-82e08ce20aac"},"outputs":[],"source":"dfq1, dfq2 = dfs[['qid1', 'question1']], dfs[['qid2', 'question2']]\ndfq1.columns = ['qid1', 'question']\ndfq2.columns = ['qid2', 'question']\n\n# merge two two dfs, there are two nans for question\ndfqa = pd.concat((dfq1, dfq2), axis=0).fillna(\"\")\nnrows_for_q1 = dfqa.shape[0]/2\ndfqa.shape"},{"cell_type":"markdown","metadata":{"_cell_guid":"76af0da5-b0b5-88f2-f75f-3eaba1a7ccf1"},"source":"Transform questions by TF-IDF."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c2a3fbb9-af84-ee0b-be83-d0637de08f4f"},"outputs":[],"source":"from sklearn.feature_extraction.text import TfidfVectorizer, HashingVectorizer\nmq1 = TfidfVectorizer(max_features = 256).fit_transform(dfqa['question'].values)\nmq1"},{"cell_type":"markdown","metadata":{"_cell_guid":"5205ceed-a6e8-c809-a603-2dba38135cf0"},"source":"Since we are looking at pairs of data, we will be taking the difference of all question one and question two pairs with this. This will result in a matrix that again has the same number of rows as the subsampled data and one vector that describes the relationship between the two questions."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c718befc-4a6d-8fe9-a1fc-01cb04aa56b8"},"outputs":[],"source":"diff_encodings = np.abs(mq1[::2] - mq1[1::2])\ndiff_encodings"},{"cell_type":"markdown","metadata":{"_cell_guid":"855c7369-d5a7-0f80-0bd7-6eb5077b7466"},"source":"# 3D t-SNE embedding\n\nWe will use t-SNE to embed the TF-IDF vectors in three dimensions and create an interactive scatter plot with them."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f41740ed-e0b3-d455-3faf-810957ed0f35"},"outputs":[],"source":"from sklearn.manifold import TSNE\ntsne = TSNE(\n    n_components=3,\n    init='random', # pca\n    random_state=101,\n    method='barnes_hut',\n    n_iter=200,\n    verbose=2,\n    angle=0.5\n).fit_transform(diff_encodings.toarray())"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"85875ea8-2806-0eb6-d294-2815f7648802"},"outputs":[],"source":"trace1 = go.Scatter3d(\n    x=tsne[:,0],\n    y=tsne[:,1],\n    z=tsne[:,2],\n    mode='markers',\n    marker=dict(\n        sizemode='diameter',\n        color = dfs['is_duplicate'].values,\n        colorscale = 'Portland',\n        colorbar = dict(title = 'duplicate'),\n        line=dict(color='rgb(255, 255, 255)'),\n        opacity=0.75\n    )\n)\n\ndata=[trace1]\nlayout=dict(height=800, width=800, title='test')\nfig=dict(data=data, layout=layout)\npy.iplot(fig, filename='3DBubble')"},{"cell_type":"markdown","metadata":{"_cell_guid":"56c3d495-26a2-b19f-abc4-1a9b853f8ea9"},"source":"That three dimensional embedding looks nice, but is not telling us much about the structure of the space that we created. There seem to be no clusters of either class present, so let's go on to the next section."},{"cell_type":"markdown","metadata":{"_cell_guid":"f6dee4a2-2c46-d87f-fd9e-12305fd4244a"},"source":"## Feature EDA\n\nLet us now construct a few features\n\n* character length of questions 1 and 2\n* number of words in question 1 and 2\n* normalized word share count.\n\nWe can then have a look at how well each of these separate the two classes."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"7a33cec5-f30a-6801-499e-86f1ac0389df"},"outputs":[],"source":"df['q1len'] = df['question1'].str.len()\ndf['q2len'] = df['question2'].str.len()\n\ndf['q1_n_words'] = df['question1'].apply(lambda row: len(row.split(\" \")))\ndf['q2_n_words'] = df['question2'].apply(lambda row: len(row.split(\" \")))\n\ndef normalized_word_share(row):\n    w1 = set(map(lambda word: word.lower().strip(), row['question1'].split(\" \")))\n    w2 = set(map(lambda word: word.lower().strip(), row['question2'].split(\" \")))    \n    return 1.0 * len(w1 & w2)/(len(w1) + len(w2))\n\n\ndf['word_share'] = df.apply(normalized_word_share, axis=1)\n\ndf.head()"},{"cell_type":"markdown","metadata":{"_cell_guid":"5658f91c-2407-89e5-c93f-d84b8dbe0a49"},"source":"The distributions for normalized word share have some overlap on the far right hand side, meaning there are quite a lot of questions with high word similarity but are both duplicates and non-duplicates."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"173d90c7-5aa8-5f8e-0f13-b7e13bdcb70f"},"outputs":[],"source":"plt.figure(figsize=(12, 8))\nplt.subplot(1,2,1)\nsns.violinplot(x = 'is_duplicate', y = 'word_share', data = df[0:50000])\nplt.subplot(1,2,2)\nsns.distplot(df[df['is_duplicate'] == 1.0]['word_share'][0:10000], color = 'green')\nsns.distplot(df[df['is_duplicate'] == 0.0]['word_share'][0:10000], color = 'red')"},{"cell_type":"markdown","metadata":{"_cell_guid":"129a284f-a312-80b9-dd80-c45bb704b6a6"},"source":"Scatter plot of question pair character lengths where color indicates duplicates and the size the word share coefficient we've calculated earlier."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d5392b7d-fe9d-1a85-d1a0-6ebabf2724f7"},"outputs":[],"source":"df_subsampled = df[0:2000]\n\ntrace = go.Scatter(\n    y = df_subsampled['q2len'].values,\n    x = df_subsampled['q1len'].values,\n    mode='markers',\n    marker=dict(\n        size= df_subsampled['word_share'].values * 60,\n        color = df_subsampled['is_duplicate'].values,\n        colorscale='Portland',\n        showscale=True,\n        opacity=0.5,\n        colorbar = dict(title = 'duplicate')\n    ),\n    text = np.round(df_subsampled['word_share'].values, decimals=2)\n)\ndata = [trace]\n\nlayout= go.Layout(\n    autosize= True,\n    title= 'Scatter plot of character lengths of question one and two',\n    hovermode= 'closest',\n        xaxis=dict(\n        showgrid=False,\n        zeroline=False,\n        showline=False\n    ),\n    yaxis=dict(\n        title= 'Question 2 length',\n        ticklen= 5,\n        gridwidth= 2,\n        showgrid=False,\n        zeroline=False,\n        showline=False,\n    ),\n    showlegend= False\n)\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig,filename='scatterWords')"},{"cell_type":"markdown","metadata":{"_cell_guid":"c541ab22-bd0d-46b2-c3fd-22fcca348292"},"source":"# Animation over average number of words\n\nFor that we will calculate the average number of words in both questions for each row.\n\nIn the end we want to have a scatter plot, just like the one above, but giving us one more dimension, in that case the average number of words in both questions. That will allow us to see the dependence on that variable. We also expect that as the number of words is increased, the character lengths of Q1 and Q2 will increase."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"4d972d6b-49fb-6d9c-87ea-4711a4399eea"},"outputs":[],"source":"from IPython.display import display, HTML\n\ndf_subsampled['q_n_words_avg'] = np.round((df_subsampled['q1_n_words'] + df_subsampled['q2_n_words'])/2.0).astype(int)\nprint(df_subsampled['q_n_words_avg'].max())\ndf_subsampled = df_subsampled[df_subsampled['q_n_words_avg'] < 20]\ndf_subsampled.head()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"0d0004e1-baf3-b186-2e2b-6b28b1fd1551"},"outputs":[],"source":"word_lens = sorted(list(df_subsampled['q_n_words_avg'].unique()))\n# make figure\nfigure = {\n    'data': [],\n    'layout': {\n        'title': 'Scatter plot of char lenghts of Q1 and Q2 (size ~ word share similarity)',\n    },\n    'frames': []#,\n    #'config': {'scrollzoom': True}\n}\n\n# fill in most of layout\nfigure['layout']['xaxis'] = {'range': [0, 200], 'title': 'Q1 length'}\nfigure['layout']['yaxis'] = {\n    'range': [0, 200],\n    'title': 'Q2 length'#,\n    #'type': 'log'\n}\nfigure['layout']['hovermode'] = 'closest'\n\nfigure['layout']['updatemenus'] = [\n    {\n        'buttons': [\n            {\n                'args': [None, {'frame': {'duration': 300, 'redraw': False},\n                         'fromcurrent': True, 'transition': {'duration': 300, 'easing': 'quadratic-in-out'}}],\n                'label': 'Play',\n                'method': 'animate'\n            },\n            {\n                'args': [[None], {'frame': {'duration': 0, 'redraw': False}, 'mode': 'immediate',\n                'transition': {'duration': 0}}],\n                'label': 'Pause',\n                'method': 'animate'\n            }\n        ],\n        'direction': 'left',\n        'pad': {'r': 10, 't': 87},\n        'showactive': False,\n        'type': 'buttons',\n        'x': 0.1,\n        'xanchor': 'right',\n        'y': 0,\n        'yanchor': 'top'\n    }\n]\n\nsliders_dict = {\n    'active': 0,\n    'yanchor': 'top',\n    'xanchor': 'left',\n    'currentvalue': {\n        'font': {'size': 20},\n        'prefix': 'Avg. number of words in both questions:',\n        'visible': True,\n        'xanchor': 'right'\n    },\n    'transition': {'duration': 300, 'easing': 'cubic-in-out'},\n    'pad': {'b': 10, 't': 50},\n    'len': 0.9,\n    'x': 0.1,\n    'y': 0,\n    'steps': []\n}\n\n# make data\nword_len = word_lens[0]\ndff = df_subsampled[df_subsampled['q_n_words_avg'] == word_len]\ndata_dict = {\n    'x': list(dff['q1len']),\n    'y': list(dff['q2len']),\n    'mode': 'markers',\n    'text': list(dff['is_duplicate']),\n    'marker': {\n        'sizemode': 'area',\n        #'sizeref': 200000,\n        'colorscale': 'Portland',\n        'size': dff['word_share'].values * 120,\n        'color': dff['is_duplicate'].values,\n        'colorbar': dict(title = 'duplicate')\n    },\n    'name': 'some name'\n}\nfigure['data'].append(data_dict)\n\n# make frames\nfor word_len in word_lens:\n    frame = {'data': [], 'name': str(word_len)}\n    dff = df_subsampled[df_subsampled['q_n_words_avg'] == word_len]\n\n    data_dict = {\n        'x': list(dff['q1len']),\n        'y': list(dff['q2len']),\n        'mode': 'markers',\n        'text': list(dff['is_duplicate']),\n        'marker': {\n            'sizemode': 'area',\n            #'sizeref': 200000,\n            'size': dff['word_share'].values * 120,\n            'colorscale': 'Portland',\n            'color': dff['is_duplicate'].values,\n            'colorbar': dict(title = 'duplicate')\n        },\n        'name': 'some name'\n    }\n    frame['data'].append(data_dict)\n\n    figure['frames'].append(frame)\n    slider_step = {'args': [\n        [word_len],\n        {\n            'frame': {'duration': 300, 'redraw': False},\n            'mode': 'immediate',\n            'transition': {'duration': 300}\n        }\n     ],\n     'label': word_len,\n     'method': 'animate'}\n    sliders_dict['steps'].append(slider_step)\n\n    \nfigure['layout']['sliders'] = [sliders_dict]\n\npy.iplot(figure)"},{"cell_type":"markdown","metadata":{"_cell_guid":"87d50db8-0cbc-4e48-cbf6-b7fe3415e564"},"source":"What is interesting about that, is that as the number of words increases, the distribution of character lengths of the first and second question becomes less and less spherical."},{"cell_type":"markdown","metadata":{"_cell_guid":"bd578f02-7559-84e9-e0ab-e00768579a07"},"source":"# Embedding with engineered features\n\nWe will now revisit the t-SNE embedding with the manually engineered features.\n\nFor that we use the number of words in both questions, character lengths and their word share coefficient. t-SNE is sensitive to scaling of different dimensions and we want all of the dimensions to contribute equally to the distance measure that t-SNE is trying to preserve."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"691c5d85-fda7-0558-85ef-72eda912c297"},"outputs":[],"source":"from sklearn.preprocessing import MinMaxScaler\n\ndf_subsampled = df[0:3000]\nX = MinMaxScaler().fit_transform(df_subsampled[['q1_n_words', 'q1len', 'q2_n_words', 'q2len', 'word_share']])\ny = df_subsampled['is_duplicate'].values"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"1a38c7e4-138a-7930-c77d-41ddfeca1beb"},"outputs":[],"source":"tsne = TSNE(\n    n_components=3,\n    init='random', # pca\n    random_state=101,\n    method='barnes_hut',\n    n_iter=200,\n    verbose=2,\n    angle=0.5\n).fit_transform(X)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"8ba80f13-7e14-21b7-d838-e3eb9ae9abf2"},"outputs":[],"source":"trace1 = go.Scatter3d(\n    x=tsne[:,0],\n    y=tsne[:,1],\n    z=tsne[:,2],\n    mode='markers',\n    marker=dict(\n        sizemode='diameter',\n        color = y,\n        colorscale = 'Portland',\n        colorbar = dict(title = 'duplicate'),\n        line=dict(color='rgb(255, 255, 255)'),\n        opacity=0.75\n    )\n)\n\ndata=[trace1]\nlayout=dict(height=800, width=800, title='3d embedding with engineered features')\nfig=dict(data=data, layout=layout)\npy.iplot(fig, filename='3DBubble')"},{"cell_type":"markdown","metadata":{"_cell_guid":"88b309db-3b34-b73d-40b5-c9a0c7bf349e"},"source":"The embedding of the engineered features has much more structure than the previous one where we were only computing differences of TF-IDF encodings.\n\nIn the cluster of the negatives we have few positives whereas in the cluster of positives we have a lot more negatives. That matches our observation from the boxplot of word share coefficient above, where we could see that the negative class has a lot of overlap with the positive class for high word share coefficients."},{"cell_type":"markdown","metadata":{"_cell_guid":"292eba08-4035-b446-2477-472555250cca"},"source":"# Parallel Coordinates\n\nWe now want to get another perspective on high dimensional data, such as the TF-IDF encoded questions. For that purpose I'll encode the concatenated questions into a set of N dimensions, s.t. each row in the dataframe then has one N dimensional vector associated to it.\nWith this we can then have a look at how these coordinates (or TF-IDF dimensions) vary by label.\n\nThere are many EDA methods to visualize high dimensional data, I'll show parallel coordinates here.\n\nTo make a nice looking plot, I've chosen N to be quite small, much smaller actually than you would encode it in a machine learning algorithm."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"fa6b4677-e7b7-b3a1-7be3-e8df12f6de63"},"outputs":[],"source":"from pandas.tools.plotting import parallel_coordinates\n\ndf_subsampled = df[0:500]\n\nN = 64\n\n#encoded = HashingVectorizer(n_features = N).fit_transform(df_subsampled.apply(lambda row: row['question1']+' '+row['question2'], axis=1).values)\nencoded = TfidfVectorizer(max_features = N).fit_transform(df_subsampled.apply(lambda row: row['question1']+' '+row['question2'], axis=1).values)\n# generate columns in the dataframe for each of the 32 dimensions\ncols = ['hashed_'+str(i) for i in range(encoded.shape[1])]\nfor idx, col in enumerate(cols):\n    df_subsampled[col] = encoded[:,idx].toarray()\n\nplt.figure(figsize=(12,8))\nkws = {\n    'linewidth': 0.5,\n    'alpha': 0.7\n}\nparallel_coordinates(\n    df_subsampled[cols + ['is_duplicate']],\n    'is_duplicate',\n    axvlines=False, colormap=plt.get_cmap('plasma'),\n    **kws\n)\n#plt.grid(False)\nplt.xticks([])\nplt.xlabel(\"encoded question dimensions\")\nplt.ylabel(\"value of dimension\")"},{"cell_type":"markdown","metadata":{"_cell_guid":"08a730bc-08b5-ccb0-4dd9-2ec7c8d15954"},"source":"In the parallel coordinates we can see that there are some dimensions that have high TF-IDF features values for duplicates and others high values for non-duplicates."},{"cell_type":"markdown","metadata":{"_cell_guid":"d4f03ac1-40ca-c3ca-2535-a7c27140fbda"},"source":"# Question character length correlations by duplication label\n\nThe pairplot of character length of both questions by duplication label is showing us that, duplicated questions seem to have a somewhat similar amount of characters in them.\n\nAlso we can see something quite intuitive, that there is rather strong correlation in the number of words and the number of characters in a question."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"4f9fdcb0-21b9-928d-fa8a-527c0d126952"},"outputs":[],"source":"n = 10000\nsns.pairplot(df[['q1len', 'q2len', 'q1_n_words', 'q2_n_words', 'is_duplicate']][0:n], hue='is_duplicate')"},{"cell_type":"markdown","metadata":{"_cell_guid":"422ea38c-ebfe-4e64-5809-a3a714ce177c"},"source":"# Model starter\n\nTrain a model with the basic feature we've constructed so far.\n\nFor that we will use Logisitic regression, for which we will do a quick parameter search with CV, plot ROC and PR curve on the holdout set and finally generate a submission."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"87d7ba4f-d876-fea8-697b-258465b2bff8"},"outputs":[],"source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import precision_recall_curve, auc, roc_curve\nfrom sklearn.model_selection import GridSearchCV, train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\n\nscaler = MinMaxScaler().fit(df[['q1len', 'q2len', 'q1_n_words', 'q2_n_words', 'word_share']])\n\nX = scaler.transform(df[['q1len', 'q2len', 'q1_n_words', 'q2_n_words', 'word_share']])\ny = df['is_duplicate']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n\nX_train.shape, X_test.shape, y_train.shape, y_test.shape\n"},{"cell_type":"markdown","metadata":{"_cell_guid":"d5fe273b-a27b-4f1e-1600-aeff93577d96"},"source":"Run cross-validation with a few hyper parameters."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"61974cf0-a794-1339-8c0b-6a4fd65a72df"},"outputs":[],"source":"clf = LogisticRegression()\ngrid = {\n    'C': [1e-6, 1e-3, 1e0],\n    'penalty': ['l1', 'l2']\n}\ncv = GridSearchCV(clf, grid, scoring='neg_log_loss', n_jobs=-1, verbose=1)\ncv.fit(X_train, y_train)"},{"cell_type":"markdown","metadata":{"_cell_guid":"d1a2c485-4ff4-98bc-a0d4-56f3f0428d74"},"source":"Print validation results. Here we see that the strongly regularized model has much worse negative log loss than the other two models, regardless of which regularizer we've used."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f4622488-59a3-034b-16dc-cd613389efe1"},"outputs":[],"source":"for i in range(1, len(cv.cv_results_['params'])+1):\n    rank = cv.cv_results_['rank_test_score'][i-1]\n    s = cv.cv_results_['mean_test_score'][i-1]\n    sd = cv.cv_results_['std_test_score'][i-1]\n    params = cv.cv_results_['params'][i-1]\n    print(\"{0}. Mean validation neg log loss: {1:.3f} (std: {2:.3f}) - {3}\".format(\n        rank,\n        s,\n        sd,\n        params\n    ))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e13919c4-dcee-d2ba-0d51-cc8763c666a9"},"outputs":[],"source":"print(cv.best_params_)\nprint(cv.best_estimator_.coef_)"},{"cell_type":"markdown","metadata":{"_cell_guid":"3997c4e4-aeb8-2373-b15b-fab1dab83766"},"source":"### ROC\n\nReceiver operator characteristic, used very commonly to assess the quality of models for binary classification.\n\nWe will look at at three different classifiers here, a strongly regularized one and two with weaker regularization. The heavily regularized model has parameters very close to zero and is actually worse than if we would pick the labels for our holdout samples randomly."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"64932d89-e554-1405-150b-f1d1cd34d936"},"outputs":[],"source":"colors = ['r', 'g', 'b', 'y', 'k', 'c', 'm', 'brown', 'r']\nlw = 1\nCs = [1e-6, 1e-4, 1e0]\n\nplt.figure(figsize=(12,8))\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curve for different classifiers')\n\nplt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n\nlabels = []\nfor idx, C in enumerate(Cs):\n    clf = LogisticRegression(C = C)\n    clf.fit(X_train, y_train)\n    print(\"C: {}, parameters {} and intercept {}\".format(C, clf.coef_, clf.intercept_))\n    fpr, tpr, _ = roc_curve(y_test, clf.predict_proba(X_test)[:,1])\n    roc_auc = auc(fpr, tpr)\n    plt.plot(fpr, tpr, lw=lw, color=colors[idx])\n    labels.append(\"C: {}, AUC = {}\".format(C, np.round(roc_auc, 4)))\n\nplt.legend(['random AUC = 0.5'] + labels)"},{"cell_type":"markdown","metadata":{"_cell_guid":"0735c8d1-9f85-fed0-a6dc-0a566a52005d"},"source":"# Precision-Recall Curve\n\nAlso used very commonly, but more often in cases where we have class-imbalance. We can see here, that there are a few positive samples that we can identify quite reliably. On in the medium and high recall regions we see that there are also positives samples that are harder to separate from the negatives."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"4d8124b5-4310-5b31-e1d3-f6946db5817c"},"outputs":[],"source":"pr, re, _ = precision_recall_curve(y_test, cv.best_estimator_.predict_proba(X_test)[:,1])\nplt.figure(figsize=(12,8))\nplt.plot(re, pr)\nplt.title('PR Curve (AUC {})'.format(auc(re, pr)))\nplt.xlabel('Recall')\nplt.ylabel('Precision')"},{"cell_type":"markdown","metadata":{"_cell_guid":"1536c175-139b-6759-f46e-f39ab5c0be01"},"source":"# Prepare submission\n\nHere we read the test data and apply the same transformations that we've used for the training data. We also need to scale the computed features again."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"edb6fe20-ba42-2f53-1014-021e22d38441"},"outputs":[],"source":"dftest = pd.read_csv(\"../input/test.csv\").fillna(\"\")\n\ndftest['q1len'] = dftest['question1'].str.len()\ndftest['q2len'] = dftest['question2'].str.len()\n\ndftest['q1_n_words'] = dftest['question1'].apply(lambda row: len(row.split(\" \")))\ndftest['q2_n_words'] = dftest['question2'].apply(lambda row: len(row.split(\" \")))\n\ndftest['word_share'] = dftest.apply(normalized_word_share, axis=1)\n\ndftest.head()\n"},{"cell_type":"markdown","metadata":{"_cell_guid":"d69a9206-b624-315f-7a61-d708d0eb91f2"},"source":"We use the best estimator found by cross-validation and retrain it, using the best hyper parameters, on the whole training set."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"7676e3d8-78d2-a2c4-8851-7450da916ffb"},"outputs":[],"source":"retrained = cv.best_estimator_.fit(X, y)\n\nX_submission = scaler.transform(dftest[['q1len', 'q2len', 'q1_n_words', 'q2_n_words', 'word_share']])\n\ny_submission = retrained.predict_proba(X_submission)[:,1]\n\nsubmission = pd.DataFrame({'test_id': dftest['test_id'], 'is_duplicate': y_submission})\nsubmission.head()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"fab27d1d-01a0-2aa2-9e69-9ddc9f8c0ccf"},"outputs":[],"source":"sns.distplot(submission.is_duplicate[0:2000])"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f333d35c-15f8-405a-bdcb-465ad459deb1"},"outputs":[],"source":"submission.to_csv(\"submission.csv\", index=False)"},{"cell_type":"markdown","metadata":{"_cell_guid":"59b017bb-100c-aa7b-ed18-e929307b690e"},"source":"Most likely this submission will not score very good, we've only used a small set of features and didn't really dive into feature engineering that much. :)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"16053eb4-b499-32d9-ed9f-4d218d88c070"},"outputs":[],"source":""}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":0}