{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"c874abf2-a4a2-fd09-65cf-1e4173eaf399"},"source":"This notebook is written with reference to the following great kernels:\n\n[Data Analysis & XGBoost Starter (0.35460 LB) by anokas][1]\n\n[Quora EDA & Model selection (ROC, PR plots) by Philipp Schmidt][2]\n\n  [1]: https://www.kaggle.com/anokas/data-analysis-xgboost-starter-0-35460-lb\n  [2]: https://www.kaggle.com/philschmidt/quora-eda-model-selection-roc-pr-plots"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"6a040030-97ca-e9a6-b8e6-4f6554bb206e"},"outputs":[],"source":"# initial setup\n\nimport numpy as np\nimport pandas as pd\nimport os\nimport gc\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\npal = sns.color_palette()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f30badbd-b2ce-f984-840e-97080a6e2866"},"outputs":[],"source":"# check the input data\n\nfor f in os.listdir('../input'):\n    if 'zip' not in f:\n        print(f.ljust(30) + str(round(os.path.getsize('../input/' + f) / 1000000, 2)) + ' MB')"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"79192515-91fe-c811-e5d9-4f5fb9fcbd8f"},"outputs":[],"source":"# check the fields in the dataset\n\ndf_train = pd.read_csv('../input/train.csv')\ndf_train.head()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"8119e351-94e6-df8b-0847-17317e8acbfe"},"outputs":[],"source":"print('Total number of question pairs for training: {}'.format(len(df_train)))\nprint('Duplicate pairs: {}%'.format(round(df_train['is_duplicate'].mean()*100, 2)))\nqids = pd.Series(df_train['qid1'].tolist() + df_train['qid2'].tolist())\nprint('Total number of questions in the training data: {}'.format(len(\n    np.unique(qids))))\nprint('Number of questions that appear multiple times: {}'.format(np.sum(qids.value_counts() > 1)))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"0f15bab9-90b4-a3de-a7a3-5df2f7bb7c0c"},"outputs":[],"source":"# basic statistics\n\nprint('Total number of question pairs for training: {}'.format(len(df_train)))\nprint('Postive Class (Duplicate pairs): {}%'.format(round(df_train['is_duplicate'].mean()*100, 2)))\nqids = pd.Series(df_train['qid1'].tolist() + df_train['qid2'].tolist())\nprint('Total number of questions in the training data: {}'.format(len(\n    np.unique(qids))))\nprint('Number of questions that appear multiple times: {}'.format(np.sum(qids.value_counts() > 1)))\n\nplt.figure(figsize=(12, 5))\nplt.hist(qids.value_counts(), bins=50)\nplt.yscale('log', nonposy='clip')\nplt.title('Log-Histogram of question appearance counts')\nplt.xlabel('Number of occurences of question')\nplt.ylabel('Number of questions')\nprint()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f481933c-f3f7-c2dd-2823-bcf852e0c266"},"outputs":[],"source":"# check the fields in the test set\n\ndf_test = pd.read_csv('../input/test.csv')\ndf_test.head()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"60151720-8a5c-4dac-90dc-1c1b2303f336"},"outputs":[],"source":"print('Total number of question pairs for testing: {}'.format(len(df_test)))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"7828f7ef-194e-f319-de42-1ee2ca03ca84"},"outputs":[],"source":"train_qs = pd.Series(df_train['question1'].tolist() + df_train['question2'].tolist()).astype(str)\ntest_qs = pd.Series(df_test['question1'].tolist() + df_test['question2'].tolist()).astype(str)\n\ndist_train = train_qs.apply(len)\ndist_test = test_qs.apply(len)\nplt.figure(figsize=(15, 10))\nplt.hist(dist_train, bins=200, range=[0, 200], color=pal[2], normed=True, label='train')\nplt.hist(dist_test, bins=200, range=[0, 200], color=pal[1], normed=True, alpha=0.5, label='test')\nplt.title('Normalised histogram of character count in questions', fontsize=15)\nplt.legend()\nplt.xlabel('Number of characters', fontsize=15)\nplt.ylabel('Probability', fontsize=15)\n\nprint('mean-train {:.2f} std-train {:.2f} mean-test {:.2f} std-test {:.2f} max-train {:.2f} max-test {:.2f}'.format(dist_train.mean(), \n                          dist_train.std(), dist_test.mean(), dist_test.std(), dist_train.max(), dist_test.max()))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"ed05aac1-fb81-a739-6971-e2e7888c1bd1"},"outputs":[],"source":"# naive method for splitting words (splitting on spaces instead of using a serious tokenizer):\ndist_train = train_qs.apply(lambda x: len(x.split(' ')))\ndist_test = test_qs.apply(lambda x: len(x.split(' ')))\n\nplt.figure(figsize=(15, 10))\nplt.hist(dist_train, bins=50, range=[0, 50], color=pal[2], normed=True, label='train')\nplt.hist(dist_test, bins=50, range=[0, 50], color=pal[1], normed=True, alpha=0.5, label='test')\nplt.title('Normalised histogram of word count in questions', fontsize=15)\nplt.legend()\nplt.xlabel('Number of words', fontsize=15)\nplt.ylabel('Probability', fontsize=15)\n\nprint('mean-train {:.2f} std-train {:.2f} mean-test {:.2f} std-test {:.2f} max-train {:.2f} max-test {:.2f}'.format(dist_train.mean(), \n                          dist_train.std(), dist_test.mean(), dist_test.std(), dist_train.max(), dist_test.max()))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e1f9b83d-a1b8-5b2f-9f00-83426fe14c90"},"outputs":[],"source":"from wordcloud import WordCloud\ncloud = WordCloud(width=1440, height=1080).generate(\" \".join(train_qs.astype(str)))\nplt.figure(figsize=(20, 15))\nplt.imshow(cloud)\nplt.axis('off')"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f37f1dd1-2013-c2ad-1b94-1e3833905fd7"},"outputs":[],"source":"qmarks = np.mean(train_qs.apply(lambda x: '?' in x))\nmath = np.mean(train_qs.apply(lambda x: '[math]' in x))\nfullstop = np.mean(train_qs.apply(lambda x: '.' in x))\ncapital_first = np.mean(train_qs.apply(lambda x: x[0].isupper()))\ncapitals = np.mean(train_qs.apply(lambda x: max([y.isupper() for y in x])))\nnumbers = np.mean(train_qs.apply(lambda x: max([y.isdigit() for y in x])))\n\nprint('Questions with question marks: {:.2f}%'.format(qmarks * 100))\nprint('Questions with [math] tags: {:.2f}%'.format(math * 100))\nprint('Questions with full stops: {:.2f}%'.format(fullstop * 100))\nprint('Questions with capitalised first letters: {:.2f}%'.format(capital_first * 100))\nprint('Questions with capital letters: {:.2f}%'.format(capitals * 100))\nprint('Questions with numbers: {:.2f}%'.format(numbers * 100))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"085e1d79-3eba-7709-718b-3abce7c50666"},"outputs":[],"source":"from nltk.corpus import stopwords\n\nstops = set(stopwords.words(\"english\"))\n\ndef word_match_share(row):\n    q1words = {}\n    q2words = {}\n    for word in str(row['question1']).lower().split():\n        if word not in stops:\n            q1words[word] = 1\n    for word in str(row['question2']).lower().split():\n        if word not in stops:\n            q2words[word] = 1\n    if len(q1words) == 0 or len(q2words) == 0:\n        # The computer-generated chaff includes a few questions that are nothing but stopwords\n        return 0\n    shared_words_in_q1 = [w for w in q1words.keys() if w in q2words]\n    shared_words_in_q2 = [w for w in q2words.keys() if w in q1words]\n    R = (len(shared_words_in_q1) + len(shared_words_in_q2))/(len(q1words) + len(q2words))\n    return R\n\nplt.figure(figsize=(15, 5))\ntrain_word_match = df_train.apply(word_match_share, axis=1, raw=True)\nplt.hist(train_word_match[df_train['is_duplicate'] == 0], bins=20, normed=True, label='Not Duplicate')\nplt.hist(train_word_match[df_train['is_duplicate'] == 1], bins=20, normed=True, alpha=0.7, label='Duplicate')\nplt.legend()\nplt.title('Label distribution over fraction of words shared by questions pairs', fontsize=15)\nplt.xlabel('Fraction of words shared', fontsize=15)"}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":0}