{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"402f6aed-475f-d967-6343-bf09e281d95c"},"source":"Quora question Pairs"},{"cell_type":"markdown","metadata":{"_cell_guid":"ea454caf-4755-81f9-9214-1dedfff36949"},"source":"To check the question is duplicate or not ?"},{"cell_type":"markdown","metadata":{"_cell_guid":"c3ca9b0d-8051-8b6a-17d6-518b6ae70598"},"source":"Importing the modules which are needed"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"49f8e7c3-9a8f-8d0e-9f80-736510811e59"},"outputs":[],"source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom nltk.corpus import stopwords\nfrom nltk import word_tokenize, ngrams\nfrom sklearn import ensemble\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import log_loss\nimport xgboost as xgb\n\neng_stopwords = set(stopwords.words('english'))\ncolor = sns.color_palette()\n\n%matplotlib inline\n\npd.options.mode.chained_assignment = None  # default='warn'"},{"cell_type":"markdown","metadata":{"_cell_guid":"b8f37375-0778-0d66-a8ae-266bba7e0212"},"source":"Reading the both input files"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"638b3afa-34a1-8bb4-f6db-52df1c5f109a"},"outputs":[],"source":"train_df = pd.read_csv(\"../input/train.csv\")\ntest_df = pd.read_csv(\"../input/test.csv\")"},{"cell_type":"markdown","metadata":{"_cell_guid":"c5a31b70-1658-a156-5f88-484652bd8629"},"source":"checking the number of row and columns present in the input file"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b673e64c-3f89-276d-9968-651e2a13ebd7"},"outputs":[],"source":"print(train_df.shape)\nprint(test_df.shape)"},{"cell_type":"markdown","metadata":{"_cell_guid":"36acc92d-9c9e-b71b-b3c8-9f09d1a7cc08"},"source":"Understand the data from the given input files "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"dfe5c30c-d14b-d5e3-bdf2-d55654e46809"},"outputs":[],"source":"train_df.describe()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"2cdc21d0-e3cc-e9e9-ab49-e8bad4c0f0f6"},"outputs":[],"source":"test_df.describe()"},{"cell_type":"markdown","metadata":{"_cell_guid":"a2cf71d4-5deb-07e8-4762-5c8431615a8e"},"source":"checking the values"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"9f7bb197-dbbf-0535-4f61-727c8a680560"},"outputs":[],"source":"train_df.head(5)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b0cd0065-4087-7c7f-46eb-a44057346f84"},"outputs":[],"source":"test_df.head(5)"},{"cell_type":"markdown","metadata":{"_cell_guid":"7b7d2c2c-d5dc-9069-7b51-99ef800324ce"},"source":"**Fields in both input files are :**\n1) Train dataframe\nid,qid1,qid2,question1,question2,is_duplicate\n\nid - It has set question pair numbers\n\nqid1,qid2 - has unique id numbers respective to the question1,question2 columns\n\nis_duplicate - it has zero , if the question1 and question2 has the same meaning then set 1 else 0\n\n\n2) Test data \ntest_id,question1,question2\n\ntest_id - it has set question pair numbers"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"1526fb73-902e-1ac9-1b77-00355e0027bd"},"outputs":[],"source":"dup = train_df['is_duplicate'].value_counts()\nplt.figure(figsize=(6,3))\nsns.barplot(dup.index,dup.values,color=color[3])\nplt.ylabel(\"No of Occurences\",fontsize=14)\nplt.xlabel(\"Duplicate\",fontsize=14)\nplt.show()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"5cef0481-e60b-1487-1fa8-b3e97f8488d5"},"outputs":[],"source":"dup\ndup/dup.sum()"},{"cell_type":"markdown","metadata":{"_cell_guid":"29e12ed5-402b-cbd7-a3e1-1cd38f32b763"},"source":"combining the question1 and question2 column as a single column"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"56282e2d-5e4e-9cf4-b923-902e7a1f8ec0"},"outputs":[],"source":"question_df=pd.DataFrame(pd.concat([train_df['question1'],train_df['question2']]))\nquestion_df.columns=[\"questions\"]"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"8b9d2aa4-9fac-b569-aa5f-18f83618fa30"},"outputs":[],"source":"question_df = pd.DataFrame(pd.concat([train_df['question1'], train_df['question2']]))\nquestion_df.columns = [\"questions\"]\nquestion_df[\"No_of_Words\"] = question_df[\"questions\"].apply(lambda x : len(str(x).split()))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"86eb4ca3-23a8-235e-55a4-ad1335e5a7fd"},"outputs":[],"source":"count =question_df['No_of_Words'].value_counts()\n\nplt.figure(figsize=(12,6))\nsns.barplot(count.index, count.values, alpha=0.6, color=color[3])\nplt.ylabel('Number of Occurrences', fontsize=12)\nplt.xlabel('Number of words in the question Column', fontsize=12)\nplt.xticks(rotation='vertical')\nplt.show()"},{"cell_type":"markdown","metadata":{"_cell_guid":"7f16afc1-2917-d3c2-1a66-121db5122488"},"source":"Checking the character in question"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"2504d662-b54a-5acd-04e3-1e8f3d388c6e"},"outputs":[],"source":"question_df[\"no_of_characters\"]=question_df[\"questions\"].apply(lambda x : len(str(x)))\ncount = question_df['no_of_characters'].value_counts()\n\nplt.figure(figsize=(50,8))\nsns.barplot(count.index,count.values,alpha=0.6,color=color[3])\nplt.ylabel('Number of occurences',fontsize=12)\nplt.xlabel('Number of characters',fontsize=12)\nplt.xticks(rotation='vertical')\nplt.show()"},{"cell_type":"markdown","metadata":{"_cell_guid":"4370ba90-ac21-8106-fb9f-2cde861eada6"},"source":"Getting common words from question1 and question2 in dataset"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"2d562c37-7ebc-57e1-1ef6-edd6701d04d4"},"outputs":[],"source":"def get_unigrams(que):\n    return [word for word in word_tokenize(que.lower()) if word not in eng_stopwords]\n\ndef get_common_unigrams(row):\n    return len( set(row[\"unigrams_ques1\"]).intersection(set(row[\"unigrams_ques2\"])) )\n\ndef get_common_unigram_ratio(row):\n    return float(row[\"unigrams_common_count\"]) / max(len( set(row[\"unigrams_ques1\"]).union(set(row[\"unigrams_ques2\"])) ),1)\n\ntrain_df[\"unigrams_ques1\"] = train_df['question1'].apply(lambda x: get_unigrams(str(x)))\ntrain_df[\"unigrams_ques2\"] = train_df['question2'].apply(lambda x: get_unigrams(str(x)))\ntrain_df[\"unigrams_common_count\"] = train_df.apply(lambda row: get_common_unigrams(row),axis=1)\ntrain_df[\"unigrams_common_ratio\"] = train_df.apply(lambda row: get_common_unigram_ratio(row), axis=1)\n\n"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"8f5939f1-340d-9da6-bca3-f012fa4342a1"},"outputs":[],"source":"count = train_df['unigrams_common_count'].value_counts()\n\nplt.figure(figsize=(12,6))\nsns.barplot(count.index, count.values, alpha=0.8)\nplt.ylabel('Number of Occurrences', fontsize=12)\nplt.xlabel('Common unigrams count', fontsize=12)\nplt.show()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b8f98b32-fd6d-d80e-b387-9b3cd596985d"},"outputs":[],"source":"plt.figure(figsize=(12,6))\nsns.violinplot(x=\"is_duplicate\", y=\"unigrams_common_count\", data=train_df, palette=\"muted\")\nplt.xlabel('Is duplicate', fontsize=12)\nplt.ylabel('Common unigrams count', fontsize=12)\nplt.show()\n"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"623c875d-6e33-acaf-67af-69467ea0f515"},"outputs":[],"source":"plt.figure(figsize=(12,6))\nsns.violinplot(x=\"is_duplicate\", y=\"unigrams_common_ratio\", data=train_df, palette=\"muted\")\nplt.xlabel('Is duplicate', fontsize=12)\nplt.ylabel('Common unigrams ratio', fontsize=12)\nplt.show()"},{"cell_type":"markdown","metadata":{"_cell_guid":"ce9883d8-2449-306f-db3f-97d36b32b440"},"source":"using Bigrams"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"0f69993a-c331-757a-b297-78ccc9e4bdea"},"outputs":[],"source":"def get_bigrams(que):\n    return [word for word in word_tokenize(que.lower()) if word not in eng_stopwords]\n\ndef get_common_bigrams(row):\n    return len( set(row[\"bigrams_ques1\"]).intersection(set(row[\"bigrams_ques2\"])) )\n\ndef get_common_bigram_ratio(row):\n    return float(row[\"bigrams_common_count\"]) / max(len( set(row[\"bigrams_ques1\"]).union(set(row[\"bigrams_ques2\"])) ),1)\n\ntrain_df[\"bigrams_ques1\"] = train_df['unigrams_ques1'].apply(lambda x: get_unigrams(str(x)))\ntrain_df[\"bigrams_ques2\"] = train_df['unigrams_ques2'].apply(lambda x: get_unigrams(str(x)))\ntrain_df[\"bigrams_common_count\"] = train_df.apply(lambda row: get_common_unigrams(row),axis=1)\ntrain_df[\"bigrams_common_ratio\"] = train_df.apply(lambda row: get_common_unigram_ratio(row), axis=1)\n\n"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"260e4f9c-b8cb-92ec-dfb3-01a541237409"},"outputs":[],"source":"count = train_df['bigrams_common_count'].value_counts()\n\nplt.figure(figsize=(12,6))\nsns.barplot(count.index, count.values, alpha=0.8)\nplt.ylabel('Number of Occurrences', fontsize=12)\nplt.xlabel('Common bigrams count', fontsize=12)\nplt.show()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c40a4565-6ce2-aab5-7068-a0d7d2aa4280"},"outputs":[],"source":"plt.figure(figsize=(12,6))\nsns.violinplot(x=\"is_duplicate\", y=\"bigrams_common_count\", data=train_df, palette=\"muted\")\nplt.xlabel('Is duplicate', fontsize=12)\nplt.ylabel('Common bigrams count', fontsize=12)\nplt.show()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"24f7fc6c-f4bf-3aa2-48bc-ffc849cf1b35"},"outputs":[],"source":"plt.figure(figsize=(12,6))\nsns.violinplot(x=\"is_duplicate\", y=\"bigrams_common_ratio\", data=train_df, palette=\"muted\")\nplt.xlabel('Is duplicate', fontsize=12)\nplt.ylabel('Common bigrams ratio', fontsize=12)\nplt.show()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"ab0cf1c3-a1af-eab3-799d-fd13b9a07949"},"outputs":[],"source":"def feature_extraction(row):\n    que1 = str(row['question1'])\n    que2 = str(row['question2'])\n    out_list = []\n    # get unigram features #\n    unigrams_que1 = [word for word in que1.lower().split() if word not in eng_stopwords]\n    unigrams_que2 = [word for word in que2.lower().split() if word not in eng_stopwords]\n    common_unigrams_len = len(set(unigrams_que1).intersection(set(unigrams_que2)))\n    common_unigrams_ratio = float(common_unigrams_len) / max(len(set(unigrams_que1).union(set(unigrams_que2))),1)\n    out_list.extend([common_unigrams_len, common_unigrams_ratio])\n\n    # get bigram features #\n    bigrams_que1 = [i for i in ngrams(unigrams_que1, 2)]\n    bigrams_que2 = [i for i in ngrams(unigrams_que2, 2)]\n    common_bigrams_len = len(set(bigrams_que1).intersection(set(bigrams_que2)))\n    common_bigrams_ratio = float(common_bigrams_len) / max(len(set(bigrams_que1).union(set(bigrams_que2))),1)\n    out_list.extend([common_bigrams_len, common_bigrams_ratio])\n\n    # get trigram features #\n    trigrams_que1 = [i for i in ngrams(unigrams_que1, 3)]\n    trigrams_que2 = [i for i in ngrams(unigrams_que2, 3)]\n    common_trigrams_len = len(set(trigrams_que1).intersection(set(trigrams_que2)))\n    common_trigrams_ratio = float(common_trigrams_len) / max(len(set(trigrams_que1).union(set(trigrams_que2))),1)\n    out_list.extend([common_trigrams_len, common_trigrams_ratio])\n    return out_list"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f385f7b4-acf7-9312-e0e5-508050e75ab2"},"outputs":[],"source":"def runXGB(train_X, train_y, test_X, test_y=None, feature_names=None, seed_val=0):\n        params = {}\n        params[\"objective\"] = \"binary:logistic\"\n        params['eval_metric'] = 'logloss'\n        params[\"eta\"] = 0.02\n        params[\"subsample\"] = 0.7\n        params[\"min_child_weight\"] = 1\n        params[\"colsample_bytree\"] = 0.7\n        params[\"max_depth\"] = 4\n        params[\"silent\"] = 1\n        params[\"seed\"] = seed_val\n        num_rounds = 300 \n        plst = list(params.items())\n        xgtrain = xgb.DMatrix(train_X, label=train_y)\n\n        if test_y is not None:\n                xgtest = xgb.DMatrix(test_X, label=test_y)\n                watchlist = [ (xgtrain,'train'), (xgtest, 'test') ]\n                model = xgb.train(plst, xgtrain, num_rounds, watchlist, early_stopping_rounds=100, verbose_eval=10)\n        else:\n                xgtest = xgb.DMatrix(test_X)\n                model = xgb.train(plst, xgtrain, num_rounds)\n                \n        pred_test_y = model.predict(xgtest)\n\n        loss = 1\n        if test_y is not None:\n                loss = log_loss(test_y, pred_test_y)\n                return pred_test_y, loss, model\n        else:\n            return pred_test_y, loss, model"},{"cell_type":"markdown","metadata":{"_cell_guid":"a900c8f3-001c-f330-dfaa-8cb2b7c6fc94"},"source":"Thanks to [SRK Notebook][1]\n\n\n  [1]: https://www.kaggle.com/sudalairajkumar/quora-question-pairs/simple-exploration-notebook-quora-ques-pair"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"209265b5-99f2-911b-c4a0-c66164523e06"},"outputs":[],"source":"train_X = np.vstack( np.array(train_df.apply(lambda row: feature_extraction(row), axis=1)) ) \ntest_X = np.vstack( np.array(test_df.apply(lambda row: feature_extraction(row), axis=1)) )\ntrain_y = np.array(train_df[\"is_duplicate\"])\ntest_id = np.array(test_df[\"test_id\"])"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"6f91ac40-7402-ae4e-ac1e-7154e5e45617"},"outputs":[],"source":"train_X_dup = train_X[train_y==1]\ntrain_X_non_dup = train_X[train_y==0]\n\ntrain_X = np.vstack([train_X_non_dup, train_X_dup, train_X_non_dup, train_X_non_dup])\ntrain_y = np.array([0]*train_X_non_dup.shape[0] + [1]*train_X_dup.shape[0] + [0]*train_X_non_dup.shape[0] + [0]*train_X_non_dup.shape[0])\ndel train_X_dup\ndel train_X_non_dup\nprint(\"Mean target rate : \",train_y.mean())"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f28aa8f3-331b-00b9-8673-1a101b4b74d6"},"outputs":[],"source":"kf = KFold(n_splits=5, shuffle=True, random_state=2016)\nfor dev_index, val_index in kf.split(range(train_X.shape[0])):\n    dev_X, val_X = train_X[dev_index,:], train_X[val_index,:]\n    dev_y, val_y = train_y[dev_index], train_y[val_index]\n    preds, lloss, model = runXGB(dev_X, dev_y, val_X, val_y)\n    break"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"ab815362-7f46-e124-0670-2e4190c91696"},"outputs":[],"source":"xgtest = xgb.DMatrix(test_X)\npreds = model.predict(xgtest)\n\nout_df = pd.DataFrame({\"test_id\":test_id, \"is_duplicate\":preds})\nout_df.to_csv(\"xgb_starter.csv\", index=False)"}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":0}