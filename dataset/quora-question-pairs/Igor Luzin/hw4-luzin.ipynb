{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"58b18843-8df0-6903-e797-317b105e40bb"},"source":"# Домашнее задание 4. Quora Question Pairs"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"3d57343c-7cdb-fb75-ac20-d76d3166edcd"},"outputs":[],"source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c8ecb3e5-3a2b-53cd-020d-487cb851ad12"},"outputs":[],"source":"df_train = pd.read_csv('../input/train.csv', low_memory=False)\ndf_train.set_index('id', inplace=True)\ndf_train.head()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"691974fa-c58d-18d3-6d2d-82455808d928"},"outputs":[],"source":"from sklearn.feature_extraction.text import CountVectorizer\n\ndef texts_vectorization(vectorizer, texts_1, texts_2):\n    vectorizer.fit(pd.concat([texts_1.fillna(''), texts_2.fillna('')]))\n    vect_1 = vectorizer.transform(texts_1.fillna(''))\n    vect_2 = vectorizer.transform(texts_2.fillna(''))\n    return vect_1, vect_2\n\nvect = CountVectorizer(analyzer='char', ngram_range=(1, 1), min_df=0.001, binary=True)"},{"cell_type":"markdown","metadata":{"_cell_guid":"1d33b3d8-b907-993c-f0ed-eab9993b2087"},"source":"We vectorize the training sample by counting enough frequently occurring 1-grams, 2-grams, 3-grams, 4-grams and 5-grams. The value of tf will reflect only the presence of N-grams in the question under consideration but not their amount. Here we are having to use 1-grams only because of low performance and established resource limits within the kaggle kernels."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"6f58ffc6-394f-773d-79f7-f7e6ed4b736f"},"outputs":[],"source":"bag_question1, bag_question2 = texts_vectorization(vect, df_train.question1, df_train.question2)\nbag_question1.shape, bag_question2.shape"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"fb7998c6-9fa5-8aae-a9cc-6d4f63bada7e"},"outputs":[],"source":"X = bag_question1 != bag_question2\nX.shape"},{"cell_type":"markdown","metadata":{"_cell_guid":"5be318db-6da7-b6e8-bd0c-ee1abd73f7c9"},"source":"We shall divide the initial sample into a training sample and a control sample to prevent re-training."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"0fbc4b4d-97ac-bf2e-f6cb-09314845aa32"},"outputs":[],"source":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, df_train.is_duplicate, test_size=0.1)"},{"cell_type":"markdown","metadata":{"_cell_guid":"deda1138-c881-f3c8-2629-720006f8ba94"},"source":"To predict probabilities we will use a multi-layer perceptron classifier with two inner layers of 1000 and 50 neurons, respectively. In order for monitoring the quality of the model on the control subsample, we will limit the training of the model to one iteration, additionally permitting reuse of the solution of the previous call to fit as initialization. Here we are having to set the layers sizes to 100 and 5 respectively."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"cae89ead-413c-f52b-374d-8785cb1f7712"},"outputs":[],"source":"from sklearn.neural_network import MLPClassifier\n\nclf = MLPClassifier(hidden_layer_sizes=(100, 5), activation='logistic', max_iter=1, warm_start=True, verbose=True)"},{"cell_type":"markdown","metadata":{"_cell_guid":"6ca65c0c-acfa-a9c8-1506-68bd92ed41e0"},"source":"By choosing a threshold for a priori score, we must take into account that the maximum allowable value is limited by the parameters of the model and the number of N-grams used."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"0b89577a-87c4-de14-5cf2-5818029fbf49"},"outputs":[],"source":"from sklearn.metrics import log_loss\n\nscore = np.inf\nwhile score > 0.58:\n    clf.fit(X_train, y_train)\n    score = log_loss(y_test, clf.predict_proba(X_test))\n    print('score = {}'.format(score))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"2110b833-9a23-123e-2d35-631cf90b87c1"},"outputs":[],"source":"df_test = pd.read_csv('../input/test.csv', low_memory=False)\ndf_test.set_index('test_id', inplace=True)\ndf_test.head()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"38d2496b-af18-2255-5c05-812b1162c509"},"outputs":[],"source":"bag_out_question1 = vect.transform(df_test.question1.fillna(''))\nbag_out_question2 = vect.transform(df_test.question2.fillna(''))\nbag_out_question1.shape, bag_out_question2.shape"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"28b7d112-1840-ce38-7c5b-6bdb7b7dc14d"},"outputs":[],"source":"X_out = bag_out_question1 != bag_out_question2\nX_out.shape"},{"cell_type":"markdown","metadata":{"_cell_guid":"b703ad1e-651b-8869-ac89-19e411e37bc3"},"source":"Let's predict the probabilities of identity questions in pairs of the test sample."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"be47afae-f3db-3cd6-f958-721390ae0f96"},"outputs":[],"source":"y_out_proba = clf.predict_proba(X_out)\ny_out_proba.shape"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"45caab39-8409-0415-6e20-f13f204ebe81"},"outputs":[],"source":"df_submission = df_test.drop(df_test.columns, axis='columns')\ndf_submission['is_duplicate'] = y_out_proba[:, 1]\ndf_submission.to_csv('submission.csv')"},{"cell_type":"markdown","metadata":{"_cell_guid":"405b205d-3b7f-f565-117b-6e2c62697301"},"source":"Our approach allows to achieve a better score than 0.35 with sufficient amount of computing resources."}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":0}