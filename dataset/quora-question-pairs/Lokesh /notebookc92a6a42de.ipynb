{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"fc7f6538-e457-4f87-b353-36442684a6e1"},"outputs":[],"source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"9f1a0ac5-4027-9f1f-bb71-087bc0e3627b"},"outputs":[],"source":"import argparse\nimport functools\nfrom collections import defaultdict\n\nimport numpy as np\nimport pandas as pd\nimport xgboost as xgb\n\nfrom nltk.corpus import stopwords\nfrom collections import Counter\nfrom sklearn.metrics import log_loss\nfrom sklearn.cross_validation import train_test_split\n\nfrom xgboost import XGBClassifier\n\n\ndef word_match_share(row, stops=None):\n    q1words = {}\n    q2words = {}\n    for word in row['question1']:\n        if word not in stops:\n            q1words[word] = 1\n    for word in row['question2']:\n        if word not in stops:\n            q2words[word] = 1\n    if len(q1words) == 0 or len(q2words) == 0:\n        # The computer-generated chaff includes a few questions that are nothing but stopwords\n        return 0\n    shared_words_in_q1 = [w for w in q1words.keys() if w in q2words]\n    shared_words_in_q2 = [w for w in q2words.keys() if w in q1words]\n    R = (len(shared_words_in_q1) + len(shared_words_in_q2))/(len(q1words) + len(q2words))\n    return R\n\ndef jaccard(row):\n    wic = set(row['question1']).intersection(set(row['question2']))\n    uw = set(row['question1']).union(row['question2'])\n    if len(uw) == 0:\n        uw = [1]\n    return (len(wic) / len(uw))\n\ndef common_words(row):\n    return len(set(row['question1']).intersection(set(row['question2'])))\n\ndef total_unique_words(row):\n    return len(set(row['question1']).union(row['question2']))\n\ndef total_unq_words_stop(row, stops):\n    return len([x for x in set(row['question1']).union(row['question2']) if x not in stops])\n\ndef wc_diff(row):\n    return abs(len(row['question1']) - len(row['question2']))\n\ndef wc_ratio(row):\n    l1 = len(row['question1'])*1.0 \n    l2 = len(row['question2'])\n    if l2 == 0:\n        return np.nan\n    if l1 / l2:\n        return l2 / l1\n    else:\n        return l1 / l2\n\ndef wc_diff_unique(row):\n    return abs(len(set(row['question1'])) - len(set(row['question2'])))\n\ndef wc_ratio_unique(row):\n    l1 = len(set(row['question1'])) * 1.0\n    l2 = len(set(row['question2']))\n    if l2 == 0:\n        return np.nan\n    if l1 / l2:\n        return l2 / l1\n    else:\n        return l1 / l2\n\ndef wc_diff_unique_stop(row, stops=None):\n    return abs(len([x for x in set(row['question1']) if x not in stops]) - len([x for x in set(row['question2']) if x not in stops]))\n\ndef wc_ratio_unique_stop(row, stops=None):\n    l1 = len([x for x in set(row['question1']) if x not in stops])*1.0 \n    l2 = len([x for x in set(row['question2']) if x not in stops])\n    if l2 == 0:\n        return np.nan\n    if l1 / l2:\n        return l2 / l1\n    else:\n        return l1 / l2\n\ndef same_start_word(row):\n    if not row['question1'] or not row['question2']:\n        return np.nan\n    return int(row['question1'][0] == row['question2'][0])\n\ndef char_diff(row):\n    return abs(len(''.join(row['question1'])) - len(''.join(row['question2'])))\n\ndef char_ratio(row):\n    l1 = len(''.join(row['question1'])) \n    l2 = len(''.join(row['question2']))\n    if l2 == 0:\n        return np.nan\n    if l1 / l2:\n        return l2 / l1\n    else:\n        return l1 / l2\n\ndef char_diff_unique_stop(row, stops=None):\n    return abs(len(''.join([x for x in set(row['question1']) if x not in stops])) - len(''.join([x for x in set(row['question2']) if x not in stops])))\n\n\ndef get_weight(count, eps=10000, min_count=2):\n    if count < min_count:\n        return 0\n    else:\n        return 1 / (count + eps)\n    \ndef tfidf_word_match_share_stops(row, stops=None, weights=None):\n    q1words = {}\n    q2words = {}\n    for word in row['question1']:\n        if word not in stops:\n            q1words[word] = 1\n    for word in row['question2']:\n        if word not in stops:\n            q2words[word] = 1\n    if len(q1words) == 0 or len(q2words) == 0:\n        # The computer-generated chaff includes a few questions that are nothing but stopwords\n        return 0\n    \n    shared_weights = [weights.get(w, 0) for w in q1words.keys() if w in q2words] + [weights.get(w, 0) for w in q2words.keys() if w in q1words]\n    total_weights = [weights.get(w, 0) for w in q1words] + [weights.get(w, 0) for w in q2words]\n    \n    R = np.sum(shared_weights) / np.sum(total_weights)\n    return R\n\ndef tfidf_word_match_share(row, weights=None):\n    q1words = {}\n    q2words = {}\n    for word in row['question1']:\n        q1words[word] = 1\n    for word in row['question2']:\n        q2words[word] = 1\n    if len(q1words) == 0 or len(q2words) == 0:\n        # The computer-generated chaff includes a few questions that are nothing but stopwords\n        return 0\n    \n    shared_weights = [weights.get(w, 0) for w in q1words.keys() if w in q2words] + [weights.get(w, 0) for w in q2words.keys() if w in q1words]\n    total_weights = [weights.get(w, 0) for w in q1words] + [weights.get(w, 0) for w in q2words]\n    \n    R = np.sum(shared_weights) / np.sum(total_weights)\n    return R\n\n\ndef build_features(data, stops, weights):\n    X = pd.DataFrame()\n    f = functools.partial(word_match_share, stops=stops)\n    X['word_match'] = data.apply(f, axis=1, raw=True) #1\n\n    f = functools.partial(tfidf_word_match_share, weights=weights)\n    X['tfidf_wm'] = data.apply(f, axis=1, raw=True) #2\n\n    f = functools.partial(tfidf_word_match_share_stops, stops=stops, weights=weights)\n    X['tfidf_wm_stops'] = data.apply(f, axis=1, raw=True) #3\n\n    X['jaccard'] = data.apply(jaccard, axis=1, raw=True) #4\n    X['wc_diff'] = data.apply(wc_diff, axis=1, raw=True) #5\n    X['wc_ratio'] = data.apply(wc_ratio, axis=1, raw=True) #6\n    X['wc_diff_unique'] = data.apply(wc_diff_unique, axis=1, raw=True) #7\n    X['wc_ratio_unique'] = data.apply(wc_ratio_unique, axis=1, raw=True) #8\n\n    f = functools.partial(wc_diff_unique_stop, stops=stops)    \n    X['wc_diff_unq_stop'] = data.apply(f, axis=1, raw=True) #9\n    f = functools.partial(wc_ratio_unique_stop, stops=stops)    \n    X['wc_ratio_unique_stop'] = data.apply(f, axis=1, raw=True) #10\n\n    X['same_start'] = data.apply(same_start_word, axis=1, raw=True) #11\n    X['char_diff'] = data.apply(char_diff, axis=1, raw=True) #12\n\n    f = functools.partial(char_diff_unique_stop, stops=stops) \n    X['char_diff_unq_stop'] = data.apply(f, axis=1, raw=True) #13\n\n#     X['common_words'] = data.apply(common_words, axis=1, raw=True)  #14\n    X['total_unique_words'] = data.apply(total_unique_words, axis=1, raw=True)  #15\n\n    f = functools.partial(total_unq_words_stop, stops=stops)\n    X['total_unq_words_stop'] = data.apply(f, axis=1, raw=True)  #16\n    \n    X['char_ratio'] = data.apply(char_ratio, axis=1, raw=True) #17    \n\n    return X\n\n\ndef main():\n    parser = argparse.ArgumentParser(description='XGB with Handcrafted Features')\n    parser.add_argument('--save', type=str, default='XGB_leaky',\n                        help='save_file_names')\n    args = parser.parse_args()\n\n    df_train = pd.read_csv('../data/train_features.csv', encoding=\"ISO-8859-1\")\n    X_train_ab = df_train.iloc[:, 2:-1]\n    X_train_ab = X_train_ab.drop('euclidean_distance', axis=1)\n    X_train_ab = X_train_ab.drop('jaccard_distance', axis=1)\n\n    df_train = pd.read_csv('../data/train.csv')\n    df_train = df_train.fillna(' ')\n\n    df_test = pd.read_csv('../data/test.csv')\n    ques = pd.concat([df_train[['question1', 'question2']], \\\n        df_test[['question1', 'question2']]], axis=0).reset_index(drop='index')\n    q_dict = defaultdict(set)\n    for i in range(ques.shape[0]):\n            q_dict[ques.question1[i]].add(ques.question2[i])\n            q_dict[ques.question2[i]].add(ques.question1[i])\n\n    def q1_freq(row):\n        return(len(q_dict[row['question1']]))\n        \n    def q2_freq(row):\n        return(len(q_dict[row['question2']]))\n        \n    def q1_q2_intersect(row):\n        return(len(set(q_dict[row['question1']]).intersection(set(q_dict[row['question2']]))))\n\n    df_train['q1_q2_intersect'] = df_train.apply(q1_q2_intersect, axis=1, raw=True)\n    df_train['q1_freq'] = df_train.apply(q1_freq, axis=1, raw=True)\n    df_train['q2_freq'] = df_train.apply(q2_freq, axis=1, raw=True)\n\n    df_test['q1_q2_intersect'] = df_test.apply(q1_q2_intersect, axis=1, raw=True)\n    df_test['q1_freq'] = df_test.apply(q1_freq, axis=1, raw=True)\n    df_test['q2_freq'] = df_test.apply(q2_freq, axis=1, raw=True)\n\n    test_leaky = df_test.loc[:, ['q1_q2_intersect','q1_freq','q2_freq']]\n    del df_test\n\n    train_leaky = df_train.loc[:, ['q1_q2_intersect','q1_freq','q2_freq']]\n\n    # explore\n    stops = set(stopwords.words(\"english\"))\n\n    df_train['question1'] = df_train['question1'].map(lambda x: str(x).lower().split())\n    df_train['question2'] = df_train['question2'].map(lambda x: str(x).lower().split())\n\n    train_qs = pd.Series(df_train['question1'].tolist() + df_train['question2'].tolist())\n\n    words = [x for y in train_qs for x in y]\n    counts = Counter(words)\n    weights = {word: get_weight(count) for word, count in counts.items()}\n\n    print('Building Features')\n    X_train = build_features(df_train, stops, weights)\n    X_train = pd.concat((X_train, X_train_ab, train_leaky), axis=1)\n    y_train = df_train['is_duplicate'].values\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.1, random_state=4242)\n\n    #UPDownSampling\n    pos_train = X_train[y_train == 1]\n    neg_train = X_train[y_train == 0]\n    X_train = pd.concat((neg_train, pos_train.iloc[:int(0.8*len(pos_train))], neg_train))\n    y_train = np.array([0] * neg_train.shape[0] + [1] * pos_train.iloc[:int(0.8*len(pos_train))].shape[0] + [0] * neg_train.shape[0])\n    print(np.mean(y_train))\n    del pos_train, neg_train\n\n    pos_valid = X_valid[y_valid == 1]\n    neg_valid = X_valid[y_valid == 0]\n    X_valid = pd.concat((neg_valid, pos_valid.iloc[:int(0.8 * len(pos_valid))], neg_valid))\n    y_valid = np.array([0] * neg_valid.shape[0] + [1] * pos_valid.iloc[:int(0.8 * len(pos_valid))].shape[0] + [0] * neg_valid.shape[0])\n    print(np.mean(y_valid))\n    del pos_valid, neg_valid\n\n\n    params = {}\n    params['objective'] = 'binary:logistic'\n    params['eval_metric'] = 'logloss'\n    params['eta'] = 0.02\n    params['max_depth'] = 7\n    params['subsample'] = 0.6\n    params['base_score'] = 0.2\n    # params['scale_pos_weight'] = 0.2\n\n    d_train = xgb.DMatrix(X_train, label=y_train)\n    d_valid = xgb.DMatrix(X_valid, label=y_valid)\n\n    watchlist = [(d_train, 'train'), (d_valid, 'valid')]\n\n    bst = xgb.train(params, d_train, 2500, watchlist, early_stopping_rounds=50, verbose_eval=50)\n    print(log_loss(y_valid, bst.predict(d_valid)))\n    bst.save_model(args.save + '.mdl')\n\n\n    print('Building Test Features')\n    df_test = pd.read_csv('../data/test_features.csv', encoding=\"ISO-8859-1\")\n    x_test_ab = df_test.iloc[:, 2:-1]\n    x_test_ab = x_test_ab.drop('euclidean_distance', axis=1)\n    x_test_ab = x_test_ab.drop('jaccard_distance', axis=1)\n    \n    df_test = pd.read_csv('../data/test.csv')\n    df_test = df_test.fillna(' ')\n\n    df_test['question1'] = df_test['question1'].map(lambda x: str(x).lower().split())\n    df_test['question2'] = df_test['question2'].map(lambda x: str(x).lower().split())\n    \n    x_test = build_features(df_test, stops, weights)\n    x_test = pd.concat((x_test, x_test_ab, test_leaky), axis=1)\n    d_test = xgb.DMatrix(x_test)\n    p_test = bst.predict(d_test)\n    sub = pd.DataFrame()\n    sub['test_id'] = df_test['test_id']\n    sub['is_duplicate'] = p_test\n    sub.to_csv('../predictions/' + args.save + '.csv')\n\nif __name__ == '__main__':\n    main()"}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":0}