{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"037b4592-b1cb-9a5d-fe44-0904afe3ab88","_uuid":"7f7880d6bbc73d8f019f937fe0b9a6dfa03e8fbe"},"source":"Hey dear Kagglers, I'm excited to share with you my very first notebook and I'll be very happy to get some advice on the many things I can improve in my investigation into the Quora dataset. Here goes...\n\nI decided to take a hybrid approach (including naive as well as tf-idf features).\n\nWe start by first deriving the naive features:\n\n - Similarity: basic similarity ratio between the two question strings\n - Pruned similarity: similarity of the two question strings excluding the stopwords","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"_cell_guid":"6954c41e-07f7-0a8d-af2f-1e08bd161b28","_uuid":"f799969a3e025a94b299bfb99fc494809aa56a52","_execution_state":"idle","trusted":false},"source":"import pandas as pd\npd.set_option('max_colwidth', 250) #so that the full column of tagged sentences can be displayed\nimport numpy as np\nimport nltk\nfrom nltk.corpus import stopwords\nfrom difflib import SequenceMatcher\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import log_loss\n\nimport warnings\nwarnings.filterwarnings(\"ignore\", category = DeprecationWarning) #to stop the annoying deprecation warnings from sklearn\n\n#Some simple functions\ndef remove_stopwords(tokenized_sent):\n    unique_stopwords = set(stopwords.words('english'))\n    return [word for word in tokenized_sent if word.lower() not in unique_stopwords]\n\ndef concatenate_tokens(token_list):\n    return str(' '.join(token_list))\n\ndef find_similarity(sent1, sent2):\n\treturn SequenceMatcher(lambda x: x in (' ', '?', '.', '\"\"', '!'), sent1, sent2).ratio()\n\ndef return_common_tokens(sent1, sent2):\n    return \" \".join([word.lower() for word in sent1 if word in sent2])\n\ndef convert_tokens_lower(tokens):\n    return [token.lower() for token in tokens]\n\n#Reading the train file\ntrain_sample = pd.read_csv('../input/train.csv', encoding = 'utf-8', index_col = 0, header = 0, iterator = True).get_chunk(100000)\n","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"_cell_guid":"973f9fd1-1e5e-4486-8505-ba2b84aad5a9","_uuid":"7f8260cd5c6cfcde18a46f48da609a56a8b26e18","collapsed":false,"_execution_state":"idle"},"source":"Nandu ","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"_cell_guid":"3b57efdc-58a6-4f8c-b0e2-5fea196cc10b","_uuid":"b774c607cb575b59222222e9f7f649eef55dbe20","collapsed":false,"_execution_state":"idle","trusted":false},"source":"#train_sample =train_sample[:100]","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"_cell_guid":"91872c53-72b9-43ec-95f6-95c9544ae6c5","_uuid":"10d2a6d40d3da081fd1d24802169bb48d89018a3","collapsed":false,"_execution_state":"idle","trusted":false},"source":"train_sample.head(2)\n","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"_cell_guid":"5313dd92-9536-4b56-9e44-9c0de781e1a3","_uuid":"4b9102ca53fc86ba076d0ebd8ca04c0c1a22ea09","collapsed":false,"_execution_state":"idle","trusted":false},"source":"train_sample.size","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"_cell_guid":"7f476768-bcf5-4d96-86b1-6d22c3c3590d","_uuid":"2fb8f7012a028d15537c906a3bdd92664eb262c8","collapsed":false,"_execution_state":"idle","trusted":false},"source":"\n\ntransformed_sentences_train = pd.DataFrame(index = train_sample.index)\nnaive_similarity = pd.DataFrame()\ntemp_features = pd.DataFrame()\ndictionary = pd.DataFrame()\ntemp_isDuplicate = pd.DataFrame()\n\n#Deriving the naive features\nfor i in (1, 2):\n        transformed_sentences_train['question%s_tokens' % i] = train_sample['question%s' % i].apply(nltk.word_tokenize)\n        transformed_sentences_train['question%s_lowercase_tokens' % i] = transformed_sentences_train['question%s_tokens' % i].apply(convert_tokens_lower)\n        transformed_sentences_train['question%s_lowercase' % i] = transformed_sentences_train['question%s_lowercase_tokens' % i].apply(concatenate_tokens)\n        transformed_sentences_train['question%s_words' % i] = transformed_sentences_train['question%s_tokens' % i].apply(remove_stopwords)\n        transformed_sentences_train['question%s_pruned' % i] = transformed_sentences_train['question%s_words' % i].apply(concatenate_tokens)\nnaive_similarity['similarity'] = np.vectorize(find_similarity)(train_sample['question1'], train_sample['question2'])\nnaive_similarity['pruned_similarity'] = np.vectorize(find_similarity)(transformed_sentences_train['question1_pruned'], transformed_sentences_train['question2_pruned'])\ntemp_features['common_tokens'] = np.vectorize(return_common_tokens)(transformed_sentences_train['question1_tokens'], transformed_sentences_train['question2_tokens'])\n\nnaive_similarity['is_duplicate'] = train_sample['is_duplicate']\nnaive_similarity['question1'] = train_sample['question1']\nnaive_similarity['question2'] = train_sample['question2']\n#temp_isDuplicate = train_sample['is_duplicate']\n\n#print (naive_similarity[:20])\n#naive_similarity.head(18)","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"_cell_guid":"89db7e3a-f31a-45b4-927a-66e8c9e9498e","_uuid":"9af4ce9e229ef19c4d2f60a2155837689d1b07d4","collapsed":false,"_execution_state":"idle","trusted":false},"source":"naive_similarity['Both'] = ((naive_similarity['similarity']>0.66 ) & (naive_similarity['is_duplicate']))","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"_cell_guid":"501e5b2b-fdfb-4e3b-a2c3-db0f217bd1ea","_uuid":"4e58c4025076b74d8bd464559bbb8a680ffa1fa9","collapsed":false,"_execution_state":"idle"},"source":"naive_similarity.head(20)","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"_cell_guid":"a92d7ea5-f812-4f31-b0f7-9843bd669c08","_uuid":"40a760897f18ff78a57b43d50f00b86efe760e23","collapsed":false,"_execution_state":"idle","trusted":false},"source":"type(tempdf)","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"_cell_guid":"09529bb5-707e-4a6a-949d-6bb8c468a0b4","_uuid":"04da56392415196b2b1eaf77fce048995dc26689","collapsed":false,"_execution_state":"idle"},"source":"tempdf.size","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"_cell_guid":"a1436588-3725-42ab-80f6-f88086ce55ff","_uuid":"a619a6c71d9ab7d06fbc0ec7072d4cc74258ac23","collapsed":false,"_execution_state":"idle"},"source":"naive_similarity.size","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"_cell_guid":"d6bbe6f1-991e-45c6-99c1-7b66a2c75291","_uuid":"8669965c1cdf970bceace738435426347770de9f","collapsed":false,"_execution_state":"idle","trusted":false},"source":"naive_similarity.head(3)","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"_cell_guid":"113bde77-c110-45f0-ac8d-93b3dc572d10","_uuid":"08b852e7ce3c82e458f5a04aa843d713cde07cc9","collapsed":false,"_execution_state":"idle","trusted":false},"source":"√ç#tempdf.groupby('col1').count().size\ncounts = naive_similarity.groupby('Both').size(); counts\n#tempdf.value_counts()","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"_cell_guid":"54532d80-7f77-45c1-a233-5248187e6708","_uuid":"d7e3cdf52b54c88ad088aa2208d8a455d6edcf20","collapsed":false,"_execution_state":"idle","trusted":false},"source":"#test = np.vectorize(find_similarity)('good geologist ?', 'great geologist ?')\ntest = np.vectorize(find_similarity)(\"how can i be a good geologist \", \"what should i do to be a great geologist ?\")   \n#test = np.vectorize(find_similarity)(\"howcanibeagoodgeologist?\", \"whatshouldidotobeagreatgeologist?\")    \nprint (test)","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"_cell_guid":"c31f1693-880d-4303-8b51-aacc83bf050a","_uuid":"fee20401c67ef86060ea5304f7f63388b93271cb","collapsed":false,"_execution_state":"idle","trusted":false},"source":"transformed_sentences_train.head(8)","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"_cell_guid":"b93de2fe-25cd-4bf1-a5f3-fe31c1ac9d58","_uuid":"08810774d5c8b72d2b3a7bc8d73e6abcae57e36d"},"source":"This is supposed to catch the most elementary non-duplicates (where the questions are obviously different), e.g. question id 3:\n\n - Why am I mentally very lonely? How can I solve it?\n - Find the remainder when [math]23^{24}[/math] is divided by 24,23?\n\nAs we can see from the output, the similarity there is 14% and the pruned similarity is 11%\n\nNext, we can enrich the feature set by adding the term frequency inverse dictionary frequency measure (tf-idf). The term frequency is the count of a term in a specific question, the inverse document frequency is the log of the total number of questions divided by the number of questions containing the term. Here is the derivation using scikit-learn's library:","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"_cell_guid":"88979f8c-f8e8-139e-5c3e-4d6180025815","_uuid":"12de2643a44962b8e685deb1ac6f067f9a3df48f","_execution_state":"idle","trusted":false},"source":"dictionary = pd.DataFrame()\n\n#Deriving the TF-IDF\ndictionary['concatenated_questions'] = transformed_sentences_train['question1_lowercase'] + transformed_sentences_train['question2_lowercase']\n\nvectorizer = CountVectorizer()\nterms_matrix = vectorizer.fit_transform(dictionary['concatenated_questions'])\nterms_matrix_1 = vectorizer.transform(transformed_sentences_train['question1_lowercase'])\nterms_matrix_2 = vectorizer.transform(transformed_sentences_train['question2_lowercase'])\ncommon_terms_matrx = vectorizer.transform(temp_features['common_tokens'])\n\ntransformer = TfidfTransformer(smooth_idf = False)\nweights_matrix = transformer.fit_transform(terms_matrix)\nweights_matrix_1 = transformer.transform(terms_matrix_1)\nweights_matrix_2 = transformer.transform(terms_matrix_2)\ncommon_weights_matrix = transformer.transform(common_terms_matrx)\n\n#Converting the sparse matrices into dataframes\ntransformed_matrix_1 = weights_matrix_1.tocoo(copy = False)\ntransformed_matrix_2 = weights_matrix_2.tocoo(copy = False)\ntransformed_common_weights_matrix = common_weights_matrix.tocoo(copy = False)\n\nweights_dataframe_1 = pd.DataFrame({'index': transformed_matrix_1.row, 'term_id': transformed_matrix_1.col, 'weight_q1': transformed_matrix_1.data})[['index', 'term_id', 'weight_q1']].sort_values(['index', 'term_id']).reset_index(drop = True)\nweights_dataframe_2 = pd.DataFrame({'index': transformed_matrix_2.row, 'term_id': transformed_matrix_2.col, 'weight_q2': transformed_matrix_2.data})[['index', 'term_id', 'weight_q2']].sort_values(['index', 'term_id']).reset_index(drop = True)\nweights_dataframe_3 = pd.DataFrame({'index': transformed_common_weights_matrix.row, 'term_id': transformed_common_weights_matrix.col, 'common_weight': transformed_common_weights_matrix.data})[['index', 'term_id', 'common_weight']].sort_values(['index', 'term_id']).reset_index(drop = True)\n\n#Summing the weights of each token in each question to get the summed weight of the question\nsum_weights_1, sum_weights_2, sum_weights_3 = weights_dataframe_1.groupby('index').sum(), weights_dataframe_2.groupby('index').sum(), weights_dataframe_3.groupby('index').sum()\n\nweights = sum_weights_1.join(sum_weights_2, how = 'outer', lsuffix = '_q1', rsuffix = '_q2').join(sum_weights_3, how = 'outer', lsuffix = '_cw', rsuffix = '_cw').join(train_sample['is_duplicate'])\nweights = weights.fillna(0)\ndel weights['term_id_q1'], weights['term_id_q2'], weights['term_id']\n#weights[is_dup] = train_sample['is_duplicate']\n\n\nprint (weights[:20])","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"_cell_guid":"83991ad1-9869-3579-e203-9120ccf49191","_uuid":"df399901fb874a1831010e0daf9cc3d81e0a21f3"},"source":"This feature is designed to account for questions that are quite similar as strings but are different in meaning. The difference usually comes from a small amount of very significant terms. Example pair id 0:\n\n - What is the step by step guide to invest in share market in india?\n - What is the step by step guide to invest in share market?\n\nAs is obvious from the data, these two questions have a 91% similarity and 90% pruned similarity. However, the one word that significantly differentiates them is 'india.' The way tf-idf is supposed to address this issue is by applying a larger weight to the 'india' term than to the others. This changes significantly the weight sum of the first and second questions (as is evident from the data above).\n\nIn addition, we also derive the 'common weight' of the two questions, i.e. the sum of the weight of all the tokens that the two questions share. As we can see this weight is very similar to the weight of the second question which also agrees with our observations.\n\nNext, we'll join the features we derived, shuffle and scale them:","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"_cell_guid":"7caf8fb7-27f5-edac-8383-7af5de5727ec","_uuid":"fbd788179ba541d86693cf0c7f7ef2a6c4839e75","_execution_state":"idle","trusted":false},"source":"X = naive_similarity.join(weights, how = 'inner')\n\n#Creating a random train-test split\ny = train_sample['is_duplicate']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.5, random_state = 42)\n\n#Scaling the features\nsc = StandardScaler()\nfor frame in (X_train, X_test):\n    sc.fit(frame)\n    frame = pd.DataFrame(sc.transform(frame), index = frame.index, columns = frame.columns)\n\nprint (X_train[:20])","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"_cell_guid":"e989eb4a-f429-6e49-e9fb-8721a9b6cb0d","_uuid":"29d715c62a7c99767e600216b232b5112460adda"},"source":"We train our algorithm (gradient boosting classifier) and print the logarithmic loss:","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"_cell_guid":"118bc787-8ef3-709a-1175-978736110867","_uuid":"7f877344ead32edf00b3ff4022b59c7e19387082","trusted":false},"source":"#Training the algorithm and making a prediction\ngbc = GradientBoostingClassifier(n_estimators = 8000, learning_rate = 0.3, max_depth = 3)\ngbc.fit(X_train, y_train.values.ravel())\nprediction = pd.DataFrame(gbc.predict(X_test), columns = ['is_duplicate'], index = X_test.index)\n\n#Inspecting our mistakes\nprediction_actual = prediction.join(y_test, how = 'inner', lsuffix = '_predicted', rsuffix = '_actual').join(train_sample[['question1', 'question2']], how = 'inner').join(X_test, how = 'inner')\n\nprint ('The log loss is %s' % log_loss(y_test, prediction))","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"_cell_guid":"5241c582-9b55-1746-db1e-9f8b85205b22","_uuid":"ef118a69e705e1a1762a8be082fabbb7115295be"},"source":"As we can see, the log loss is abysmal for the 30 question pairs in the sample, but it actually goes down substantially if the algorithm is trained over most of the training data.\n\nFinally, we evaluate our mistakes:","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"_cell_guid":"72baef14-d622-f8d2-7eba-3c7d686f8d1a","_uuid":"0d63a39d27a31dec284fbd4476ea26472b681751","trusted":false},"source":"print (prediction_actual[prediction_actual['is_duplicate_predicted'] != prediction_actual['is_duplicate_actual']][:10])","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"_cell_guid":"c4aafd87-e09f-54b4-513e-70bef6cec118","_uuid":"c3b9bff4de53484d875b7b72b031118557237f06"},"source":"As we can see, this approach needs to be supplemented by other metrics. The types of errors we are likely to encounter are:\n\n - Cases where the weights of two contextually different expressions are similar (e.g. pair 28). In this case 'ask for' and 'make' may have very similar weights due to similar counts of the term throughout the corpus, but have a fundamentally different meaning.\n - Algorithmic errors - where the features indicate difference to an observer but not to the algorithm (e.g. pair 24 where the similarity is 48% and the weight ratio is 75%). This could potentially be improved by tweaking the training parameters, adding more training data and executing more epochs.\n\nIn addition, our data derivation has several shortcomings. Namely: we have done no canonization of the terms in the corpus. This means that the following terms will be considered different (and have different counts and weights according to the tf-idf):\n\n - 2016-12-01 and 1st of December 2016\n - Youtube and YouTube\n - india and India\n\nThis problem can be solved through a similarity matching and some regular expressions.\n\nAnother issue we haven't addressed is the semantic closeness of terms in the question pairs for cases like:\n\n - Holland and The Netherlands\n - Holland and France (both may have equal frequency in the corpus and equal weights but have different meaning)\n\nThis problem can be resolved through vectorization of the terms and taking cosine of their values.\n\nUnfortunately those tasks are beyond the allocated time or hardware of my current participation (30 hours and Acer Revo One, respectively), but had time been abundantly available, I would work on the following additional features:\n\n - Regular expression parser to canonize the training and test corpus\n - Cosine of the terms of each question pair\n - N-gram derivation and comparison\n\nI'm eager to hear your constructive criticism and suggestions for improvement!","execution_count":null,"outputs":[]}],"nbformat":4,"nbformat_minor":0,"metadata":{"_is_fork":false,"language_info":{"file_extension":".py","nbconvert_exporter":"python","name":"python","version":"3.6.1","mimetype":"text/x-python","pygments_lexer":"ipython3","codemirror_mode":{"version":3,"name":"ipython"}},"_change_revision":0,"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"}}}