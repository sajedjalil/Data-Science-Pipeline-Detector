{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Quora Question Pairs Similarity","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## 1. Business Problem","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Quora is a place to gain and share knowledge—about anything. It’s a platform to ask questions and connect with people who contribute unique insights and quality answers. This empowers people to learn from each other and to better understand the world.\n\nOver 100 million people visit Quora every month, so it's no surprise that many people ask similarly worded questions. Multiple questions with the same intent can cause seekers to spend more time finding the best answer to their question, and make writers feel they need to answer multiple versions of the same question. Quora values canonical questions because they provide a better experience to active seekers and writers, and offer more value to both of these groups in the long term.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Problem Statement \n\n\n\n*   Identify which questions asked on Quora are duplicates of questions that  \n    have already been asked.\n\n*   This could be useful to instantly provide answers to questions that have already been answered.\n\n \n\n*   We are tasked with predicting whether a pair of questions are duplicates or not.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Data Overview\n\n- Data will be in a file Train.csv\n- Train.csv contains 5 columns : qid1, qid2, question1, question2, is_duplicate\n- Size of Train.csv - 60MB\n- Number of rows in Train.csv = 404,290","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom subprocess import check_output\n%matplotlib inline\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.tools as tls\nimport os\nimport gc\nimport re\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\nfrom bs4 import BeautifulSoup\nfrom fuzzywuzzy import fuzz\nfrom sklearn.manifold import TSNE\nfrom wordcloud import WordCloud, STOPWORDS\nfrom os import path\nfrom PIL import Image\nimport nltk\nfrom nltk.stem.porter import PorterStemmer\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\nfrom nltk.stem.wordnet import WordNetLemmatizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import cross_val_score\nfrom collections import Counter\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics.classification import accuracy_score, log_loss\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom collections import Counter\nfrom scipy.sparse import hstack\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.svm import SVC\nfrom collections import Counter, defaultdict\nfrom sklearn.calibration import CalibratedClassifierCV\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\nimport math\nfrom sklearn.metrics import normalized_mutual_info_score\nfrom sklearn.ensemble import RandomForestClassifier\n\n\n\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import SGDClassifier\nfrom mlxtend.classifier import StackingClassifier\n\nfrom sklearn import model_selection\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import precision_recall_curve, auc, roc_curve\nfrom sklearn.preprocessing import normalize\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport sys\nimport os \nfrom tqdm import tqdm\nimport spacy","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import zipfile\n\nzf = zipfile.ZipFile('../input/quora-question-pairs/train.csv.zip')\nquora_df = pd.read_csv(zf.open('train.csv'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"quora_df.head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"quora_df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"quora_df.groupby(\"is_duplicate\")['id'].count().plot.bar()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"quora_df['is_duplicate'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Question pairs are not Similar (is_duplicate = 0):\\n   {}%'.format(100 - round(quora_df['is_duplicate'].mean()*100, 2)))\nprint('Question pairs are Similar (is_duplicate = 1):\\n   {}%'.format(round(quora_df['is_duplicate'].mean()*100, 2)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"question_ids=pd.Series(quora_df['qid1'].tolist() + quora_df['qid2'].tolist())\n\nunique_questions=len(np.unique(question_ids))\nquestions_morethan1=np.sum(question_ids.value_counts() > 1)\n\n\nprint('Total No of Unique questions :{} \\n'.format(unique_questions))\n\nprint ('Number of unique questions that appear more than one time: {} ({}%)\\n'.format(questions_morethan1,questions_morethan1/unique_questions*100))\n\nprint ('Max number of times a single question is repeated: {}\\n'.format(max(question_ids.value_counts()))) \n\nq_vals=question_ids.value_counts()\n\nq_vals=q_vals.values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"type(question_ids)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"question_ids[:5]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nan_rows=quora_df[quora_df.isnull().any(1)]\n\nprint (nan_rows)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"quora_df=quora_df.fillna('')\n\nnan_rows=quora_df[quora_df.isnull().any(1)]\n\nprint (nan_rows)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3.3 Basic Feature Extraction (before cleaning)\n\n\n\n1.   freq_qid1 = Frequency of qid1's\n2.   freq_qid2 = Frequency of qid2's\n3.   q1len = Length of q1\n4.   q2len = Length of q2\n5.   q1_n_words = Number of words in Question 1\n6.   q2_n_words = Number of words in Question 2\n7.   word_Common = (Number of common unique words in Question 1 and Question 2)\n8.   word_Total =(Total num of words in Question 1 + Total num of words in Question 2)\n9.   word_share = (word_common)/(word_Total)\n10.  freq_q1+freq_q2 = sum total of frequency of qid1 and qid2\n11.  freq_q1-freq_q2 = absolute difference of frequency of qid1 and qid2","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"if os.path.isfile('feature_engg_preprocessing_train.csv'):\n  quora_df = pd.read_csv(\"feature_engg_preprocessing_train\",encoding='latin-1')\nelse:\n\n  quora_df['freq_qid1'] = quora_df.groupby('qid1')['qid1'].transform('count')\n  quora_df['freq_qid2'] = quora_df.groupby('qid2')['qid2'].transform('count')\n  quora_df['q1len'] = quora_df['question1'].str.len()\n  quora_df['q2len'] = quora_df['question2'].str.len()\n  quora_df['q1_n_words'] = quora_df['question1'].apply(lambda row: len(row.split(\" \")))\n  quora_df['q2_n_words'] = quora_df['question2'].apply(lambda row: len(row.split(\" \")))\n\n  def normalized_word_Common(row):\n     w1 = set(map(lambda word: word.lower().strip(), row['question1'].split(\" \")))\n     w2 = set(map(lambda word: word.lower().strip(), row['question2'].split(\" \")))    \n     return 1.0 * len(w1 & w2)\n  quora_df['word_Common'] = quora_df.apply(normalized_word_Common, axis=1)\n\n  def normalized_word_Total(row):\n    w1 = set(map(lambda word: word.lower().strip(), row['question1'].split(\" \")))\n    w2 = set(map(lambda word: word.lower().strip(), row['question2'].split(\" \")))    \n    return 1.0 * (len(w1) + len(w2))\n  quora_df['word_Total'] = quora_df.apply(normalized_word_Total, axis=1)\n\n  def normalized_word_share(row):\n    w1 = set(map(lambda word: word.lower().strip(), row['question1'].split(\" \")))\n    w2 = set(map(lambda word: word.lower().strip(), row['question2'].split(\" \")))    \n    return 1.0 * len(w1 & w2)/(len(w1) + len(w2))\n  quora_df['word_share'] = quora_df.apply(normalized_word_share, axis=1)\n\n  quora_df['freq_q1+q2'] = quora_df['freq_qid1']+quora_df['freq_qid2']\n  quora_df['freq_q1-q2'] = abs(quora_df['freq_qid1']-quora_df['freq_qid2'])\n\n  quora_df.to_csv(\"feature_engg_preprocessing_train.csv\", index=False)\n\nquora_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print (\"Minimum length of the questions in question1 : \" , min(quora_df['q1_n_words']))\n\nprint (\"Minimum length of the questions in question2 : \" , min(quora_df['q2_n_words']))\n\nprint (\"Number of Questions with minimum length [question1] :\", quora_df[quora_df['q1_n_words']== 1].shape[0])\nprint (\"Number of Questions with minimum length [question2] :\", quora_df[quora_df['q2_n_words']== 1].shape[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12, 8))\n\nplt.subplot(1,2,1)\nsns.violinplot(x = 'is_duplicate', y = 'word_share', data = quora_df[0:])\n\nplt.subplot(1,2,2)\nsns.distplot(quora_df[quora_df['is_duplicate'] == 1.0]['word_share'][0:] , label = \"1\", color = 'red')\nsns.distplot(quora_df[quora_df['is_duplicate'] == 0.0]['word_share'][0:] , label = \"0\" , color = 'blue' )\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### By looking at the Violenplot , below are the 2 observations :\n\n1. The distributions for normalized word_share have some overlap on the far right-hand side, i.e., there are quite a lot of questions with high word similarity\n2. The average word share and Common no. of words of qid1 and qid2 is more when they are duplicate(Similar)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"pip install fuzzywuzzy","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if os.path.isfile('feature_engg_preprocessing_train.csv'):\n    quora_df = pd.read_csv(\"feature_engg_preprocessing_train.csv\",encoding='latin-1')\n    quora_df = quora_df.fillna('')\n    quora_df.head()\nelse:\n    print(\"get feature_engg_preprocessing_train.csv from drive or run the previous notebook\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"quora_df.head(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nltk.download('stopwords')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.corpus import stopwords\nSAFE_DIV = 0.0001 \n\nSTOP_WORDS = stopwords.words(\"english\")\n\n\ndef preprocess(x):\n    x = str(x).lower()\n    x = x.replace(\",000,000\", \"m\").replace(\",000\", \"k\").replace(\"′\", \"'\").replace(\"’\", \"'\")\\\n                           .replace(\"won't\", \"will not\").replace(\"cannot\", \"can not\").replace(\"can't\", \"can not\")\\\n                           .replace(\"n't\", \" not\").replace(\"what's\", \"what is\").replace(\"it's\", \"it is\")\\\n                           .replace(\"'ve\", \" have\").replace(\"i'm\", \"i am\").replace(\"'re\", \" are\")\\\n                           .replace(\"he's\", \"he is\").replace(\"she's\", \"she is\").replace(\"'s\", \" own\")\\\n                           .replace(\"%\", \" percent \").replace(\"₹\", \" rupee \").replace(\"$\", \" dollar \")\\\n                           .replace(\"€\", \" euro \").replace(\"'ll\", \" will\")\n    x = re.sub(r\"([0-9]+)000000\", r\"\\1m\", x)\n    x = re.sub(r\"([0-9]+)000\", r\"\\1k\", x)\n    \n    \n    porter = PorterStemmer()\n    pattern = re.compile('\\W')\n    \n    if type(x) == type(''):\n        x = re.sub(pattern, ' ', x)\n    \n    \n    if type(x) == type(''):\n        x = porter.stem(x)\n        example1 = BeautifulSoup(x)\n        x = example1.get_text()\n               \n    \n    return x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_token_features(q1, q2):\n    token_features = [0.0]*10\n    \n    # Converting the Sentence into Tokens: \n    q1_tokens = q1.split()\n    q2_tokens = q2.split()\n\n    if len(q1_tokens) == 0 or len(q2_tokens) == 0:\n        return token_features\n    # Get the non-stopwords in Questions\n    q1_words = set([word for word in q1_tokens if word not in STOP_WORDS])\n    q2_words = set([word for word in q2_tokens if word not in STOP_WORDS])\n    \n    #Get the stopwords in Questions\n    q1_stops = set([word for word in q1_tokens if word in STOP_WORDS])\n    q2_stops = set([word for word in q2_tokens if word in STOP_WORDS])\n    \n    # Get the common non-stopwords from Question pair\n    common_word_count = len(q1_words.intersection(q2_words))\n    \n    # Get the common stopwords from Question pair\n    common_stop_count = len(q1_stops.intersection(q2_stops))\n    \n    # Get the common Tokens from Question pair\n    common_token_count = len(set(q1_tokens).intersection(set(q2_tokens)))\n    \n    \n    token_features[0] = common_word_count / (min(len(q1_words), len(q2_words)) + SAFE_DIV)\n    token_features[1] = common_word_count / (max(len(q1_words), len(q2_words)) + SAFE_DIV)\n    token_features[2] = common_stop_count / (min(len(q1_stops), len(q2_stops)) + SAFE_DIV)\n    token_features[3] = common_stop_count / (max(len(q1_stops), len(q2_stops)) + SAFE_DIV)\n    token_features[4] = common_token_count / (min(len(q1_tokens), len(q2_tokens)) + SAFE_DIV)\n    token_features[5] = common_token_count / (max(len(q1_tokens), len(q2_tokens)) + SAFE_DIV)\n    \n    # Last word of both question is same or not\n    token_features[6] = int(q1_tokens[-1] == q2_tokens[-1])\n    \n    # First word of both question is same or not\n    token_features[7] = int(q1_tokens[0] == q2_tokens[0])\n    \n    token_features[8] = abs(len(q1_tokens) - len(q2_tokens))\n    \n    #Average Token Length of both Questions\n    token_features[9] = (len(q1_tokens) + len(q2_tokens))/2\n    return token_features\n\n# get the Longest Common sub string\n\ndef get_longest_substr_ratio(a, b):\n    strs = list(distance.lcsubstrings(a, b))\n    if len(strs) == 0:\n        return 0\n    else:\n        return len(strs[0]) / (min(len(a), len(b)) + 1)\n\ndef extract_features(df):\n    # preprocessing each question\n    df[\"question1\"] = df[\"question1\"].fillna(\"\").apply(preprocess)\n    df[\"question2\"] = df[\"question2\"].fillna(\"\").apply(preprocess)\n\n    print(\"token features...\")\n    \n    # Merging Features with dataset\n    \n    token_features = df.apply(lambda x: get_token_features(x[\"question1\"], x[\"question2\"]), axis=1)\n    \n    df[\"cwc_min\"]       = list(map(lambda x: x[0], token_features))\n    df[\"cwc_max\"]       = list(map(lambda x: x[1], token_features))\n    df[\"csc_min\"]       = list(map(lambda x: x[2], token_features))\n    df[\"csc_max\"]       = list(map(lambda x: x[3], token_features))\n    df[\"ctc_min\"]       = list(map(lambda x: x[4], token_features))\n    df[\"ctc_max\"]       = list(map(lambda x: x[5], token_features))\n    df[\"last_word_eq\"]  = list(map(lambda x: x[6], token_features))\n    df[\"first_word_eq\"] = list(map(lambda x: x[7], token_features))\n    df[\"abs_len_diff\"]  = list(map(lambda x: x[8], token_features))\n    df[\"mean_len\"]      = list(map(lambda x: x[9], token_features))\n   \n    #Computing Fuzzy Features and Merging with Dataset       \n    print(\"fuzzy features..\")\n\n    df[\"token_set_ratio\"]       = df.apply(lambda x: fuzz.token_set_ratio(x[\"question1\"], x[\"question2\"]), axis=1)\n    # The token sort approach involves tokenizing the string in question, sorting the tokens alphabetically, and \n    # then joining them back into a string We then compare the transformed strings with a simple ratio().\n    df[\"token_sort_ratio\"]      = df.apply(lambda x: fuzz.token_sort_ratio(x[\"question1\"], x[\"question2\"]), axis=1)\n    df[\"fuzz_ratio\"]            = df.apply(lambda x: fuzz.QRatio(x[\"question1\"], x[\"question2\"]), axis=1)\n    df[\"fuzz_partial_ratio\"]    = df.apply(lambda x: fuzz.partial_ratio(x[\"question1\"], x[\"question2\"]), axis=1)\n    df[\"longest_substr_ratio\"]  = df.apply(lambda x: get_longest_substr_ratio(x[\"question1\"], x[\"question2\"]), axis=1)\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pip install distance","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import distance\nif os.path.isfile('nlp_features_train.csv'):\n    quora_df = pd.read_csv(\"nlp_features_train.csv\",encoding='latin-1')\n    quora_df.fillna('')\nelse:\n    print(\"Extracting features for train:\")\n    quora_df = pd.read_csv('../input/quora-question-pairs/train.csv.zip')\n    quora_df = extract_features(quora_df)\n    quora_df.to_csv(\"nlp_features_train.csv\", index=False)\nquora_df.head(2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## GENERATING WORD CLOUD OF DUPLICATES AND NON DUPLICATE QUESTION PAIRS. WE CAN OBSERVE MOST FREQUENT OCCURING WORDS","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_duplicate = quora_df[quora_df['is_duplicate'] == 1]\ndf_nonduplicate = quora_df[quora_df['is_duplicate'] == 0]\n\n# Converting 2d array of q1 and q2 and flatten the array: like {{1,2},{3,4}} to {1,2,3,4}\np = np.dstack([df_duplicate[\"question1\"], df_duplicate[\"question2\"]]).flatten()\nn = np.dstack([df_nonduplicate[\"question1\"], df_nonduplicate[\"question2\"]]).flatten()\n\nprint (\"Number of data points in class 1 (duplicate pairs) :\",len(p))\nprint (\"Number of data points in class 0 (non duplicate pairs) :\",len(n))\n\n#Saving the np array into a text file\nnp.savetxt('train_p.txt', p, delimiter=' ', fmt='%s')\nnp.savetxt('train_n.txt', n, delimiter=' ', fmt='%s')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"d = path.dirname('.')\n\ntextp_w = open(path.join(d, 'train_p.txt')).read()\ntextn_w = open(path.join(d, 'train_n.txt')).read()\nstopwords = set(STOPWORDS)\nstopwords.add(\"said\")\nstopwords.add(\"br\")\nstopwords.add(\" \")\nstopwords.remove(\"not\")\n\nstopwords.remove(\"no\")\nstopwords.remove(\"like\")\n\nprint (\"Total number of words in duplicate pair questions :\",len(textp_w))\nprint (\"Total number of words in non duplicate pair questions :\",len(textn_w))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"wc = WordCloud(background_color=\"white\", max_words=len(textp_w), stopwords=stopwords)\nwc.generate(textp_w)\nprint (\"Word Cloud for Duplicate Question pairs\")\nplt.imshow(wc, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"wc = WordCloud(background_color=\"white\", max_words=len(textn_w),stopwords=stopwords)\n# generate word cloud\nwc.generate(textn_w)\nprint (\"Word Cloud for non-Duplicate Question pairs:\")\nplt.imshow(wc, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## TAKE 100K DATAPOINTS AND SPLIT THEM INTO INTO TEST AND TRAIN ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## FEATURIZING TEXT DATA USING TF-IDF ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"quora_df['question1']=quora_df['question1'].apply(lambda x:str(x))\nquora_df['question2']=quora_df['question2'].apply(lambda x:str(x))\n\nquora_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if os.path.isfile('nlp_features_train.csv'):\n    dfnlp = pd.read_csv(\"nlp_features_train.csv\",encoding='latin-1')\nelse:\n    print(\"download nlp_features_train.csv from drive or run previous notebook\")\n\nif os.path.isfile('feature_engg_preprocessing_train.csv'):\n    dfppro = pd.read_csv(\"feature_engg_preprocessing_train.csv\",encoding='latin-1')\nelse:\n    print(\"download ./feature_engg_preprocessing_train.csv from drive or run previous notebook\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df1 = dfnlp.drop(['qid1','qid2','question1','question2','is_duplicate'],axis=1)\ndf2 = dfppro.drop(['qid1','qid2','question1','question2','is_duplicate'],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df3=dfnlp[['id','question1','question2']]\nduplicate=dfnlp.is_duplicate","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df3 = df3.fillna(' ')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"new_dataframe = pd.DataFrame()\n\nnew_dataframe['questions']=df3.question1 + ' ' + df3.question2\nnew_dataframe['id']=df3.id\ndf2['id']=df1['id']\nnew_dataframe['id']=df1['id']\nfinal_df = df1.merge(df2, on='id',how='left')\nX_Final  = final_df.merge(new_dataframe, on='id',how='left')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_Final=X_Final.drop('id',axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_Final.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_Final.head(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_Final.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_Final.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Y_Final=np.array(duplicate)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_Final_100K = X_Final[0:100000]\nY_Final_100K = Y_Final[0:100000]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_Train,X_Test,Y_Train,Y_Test = train_test_split(X_Final_100K,Y_Final_100K,test_size=0.2,random_state=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(X_Train.shape)\nprint(X_Test.shape)\nprint(Y_Train.shape)\nprint(Y_Test.shape)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_ques=X_Train['questions']\nX_test_ques=X_Test['questions']\n\nX_Train=X_Train.drop('questions',axis=1)\nX_Test=X_Test.drop('questions',axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## FEATURIZATION DATA USING TF-IDF WEIGHTED WORD2VEC","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_extraction.text import CountVectorizer\n\ntfidf_vector=TfidfVectorizer(lowercase=False)\ntfidf_vector.fit_transform(X_train_ques)\n\nword2Vectfidf = dict(zip(tfidf_vector.get_feature_names(), tfidf_vector.idf_))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"pip install spacy","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nlp = spacy.load('en_core_web_sm')\n\nvecs1 = []\n\nfor qu1 in tqdm(list(X_train_ques)):\n    doc1 = nlp(qu1)    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"type(doc1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(doc1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"doc1[0].vector","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(doc1[0].vector)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"nlp = spacy.load('en_core_web_sm')\n\nvecs1 = []\n\nfor qu1 in tqdm(X_train_ques):\n    #doc1 = nlp(qu1)      \n    mean_vec1 = np.zeros([len(doc1), len(doc1[0].vector)])\n    for word1 in doc1:\n        # word2vec\n        vec1 = word1.vector\n        # fetch df score\n        try:\n            idf = word2Vectfidf[str(word1)]\n        except:\n            idf = 0\n        # compute final vec\n        mean_vec1 += vec1 * idf\n    mean_vec1 = mean_vec1.mean(axis=0)\n    vecs1.append(mean_vec1)\n#X_train_ques['q1_feats_m'] = list(vecs1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for qu2 in tqdm(list(X_test_ques)):    \n    doc2 = nlp(qu2) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"type(doc2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(doc2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"doc2[0].vector","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(doc2[0].vector)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vecs2 = []\n\nfor qu2 in tqdm(list(X_test_ques)):\n    #doc2 = nlp(qu2) \n    mean_vec2 = np.zeros([len(doc2), len(doc2[0].vector)])\n    for word2 in doc2:        \n        # word2vec\n        vec2 = word2.vector\n        # fetch df score\n        try:\n            idf = word2Vectfidf[str(word2)]\n        except:\n            #print word\n            idf = 0\n        # compute final vec\n        mean_vec2 += vec2 * idf\n    mean_vec2 = mean_vec2.mean(axis=0)\n    vecs2.append(mean_vec2)\n#X_Test['q2_feats_m'] = list(vecs2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df=pd.DataFrame(vecs1)\ntest_df = pd.DataFrame(vecs2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_Train.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_Train.values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.head(4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy.sparse import hstack\nX_Train = hstack((X_Train.values,train_df))\nX_Test= hstack((X_Test.values,test_df))\nprint(X_Train.shape)\nprint(X_Test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"type(X_Train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# This function plots the confusion matrices given y_i, y_i_hat.\ndef plot_confusion_matrix(test_y, predict_y):\n    C = confusion_matrix(test_y, predict_y)\n    # C = 9,9 matrix, each cell (i,j) represents number of points of class i are predicted class j\n    \n    A =(((C.T)/(C.sum(axis=1))).T)\n    #divid each element of the confusion matrix with the sum of elements in that column\n    \n    # C = [[1, 2],\n    #     [3, 4]]\n    # C.T = [[1, 3],\n    #        [2, 4]]\n    # C.sum(axis = 1)  axis=0 corresonds to columns and axis=1 corresponds to rows in two diamensional array\n    # C.sum(axix =1) = [[3, 7]]\n    # ((C.T)/(C.sum(axis=1))) = [[1/3, 3/7]\n    #                           [2/3, 4/7]]\n\n    # ((C.T)/(C.sum(axis=1))).T = [[1/3, 2/3]\n    #                           [3/7, 4/7]]\n    # sum of row elements = 1\n    \n    B =(C/C.sum(axis=0))\n    #divid each element of the confusion matrix with the sum of elements in that row\n    # C = [[1, 2],\n    #     [3, 4]]\n    # C.sum(axis = 0)  axis=0 corresonds to columns and axis=1 corresponds to rows in two diamensional array\n    # C.sum(axix =0) = [[4, 6]]\n    # (C/C.sum(axis=0)) = [[1/4, 2/6],\n    #                      [3/4, 4/6]] \n    plt.figure(figsize=(20,4))\n    \n    labels = [1,2]\n    # representing A in heatmap format\n    cmap=sns.light_palette(\"blue\")\n    plt.subplot(1, 3, 1)\n    sns.heatmap(C, annot=True, cmap=cmap, fmt=\".3f\", xticklabels=labels, yticklabels=labels)\n    plt.xlabel('Predicted Class')\n    plt.ylabel('Original Class')\n    plt.title(\"Confusion matrix\")\n    \n    plt.subplot(1, 3, 2)\n    sns.heatmap(B, annot=True, cmap=cmap, fmt=\".3f\", xticklabels=labels, yticklabels=labels)\n    plt.xlabel('Predicted Class')\n    plt.ylabel('Original Class')\n    plt.title(\"Precision matrix\")\n    \n    plt.subplot(1, 3, 3)\n    # representing B in heatmap format\n    sns.heatmap(A, annot=True, cmap=cmap, fmt=\".3f\", xticklabels=labels, yticklabels=labels)\n    plt.xlabel('Predicted Class')\n    plt.ylabel('Original Class')\n    plt.title(\"Recall matrix\")\n    \n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### BUILDING A RANDOM MODEL AND FINDING THE WORST CASE LOG LOSS","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"test_len = len(Y_Test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predicted_y = np.zeros((test_len,2))\nfor i in range(test_len):\n    rand_probs = np.random.rand(1,2)\n    predicted_y[i] = ((rand_probs/sum(sum(rand_probs)))[0])\nprint(\"Log loss on Test Data using Random Model\",log_loss(Y_Test, predicted_y, eps=1e-15))\n\npredicted_y =np.argmax(predicted_y, axis=1)\nplot_confusion_matrix(Y_Test, predicted_y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### LOGISTIC REGRESSION TO FIND HYPERPARAMETER","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"alpha = [10 ** x for x in range(-5, 2)] # hyperparam for SGD classifier.\nlog_error_array=[]\nfor i in alpha:\n    clf = SGDClassifier(alpha=i, penalty='l2', loss='log', random_state=42)\n    clf.fit(X_Train, Y_Train)\n    sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n    sig_clf.fit(X_Train, Y_Train)\n    predict_y = sig_clf.predict_proba(X_Test)\n    log_error_array.append(log_loss(Y_Test, predict_y, labels=clf.classes_, eps=1e-15))\n    print('For values of alpha = ', i, \"The log loss is:\",log_loss(Y_Test, predict_y, labels=clf.classes_, eps=1e-15))\n\nfig, ax = plt.subplots()\nax.plot(alpha, log_error_array,c='g')\nfor i, txt in enumerate(np.round(log_error_array,3)):\n    ax.annotate((alpha[i],np.round(txt,3)), (alpha[i],log_error_array[i]))\nplt.grid()\nplt.title(\"Cross Validation Error for each alpha\")\nplt.xlabel(\"Alpha i's\")\nplt.ylabel(\"Error measure\")\nplt.show()\n\n\nbest_alpha = np.argmin(log_error_array)\nclf = SGDClassifier(alpha=alpha[best_alpha], penalty='l2', loss='log', random_state=42)\nclf.fit(X_Train, Y_Train)\nsig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\nsig_clf.fit(X_Train, Y_Train)\n\npredict_y = sig_clf.predict_proba(X_Train)\nprint('For values of best alpha = ', alpha[best_alpha], \"The train log loss is:\",log_loss(Y_Train, predict_y, labels=clf.classes_, eps=1e-15))\npredict_y = sig_clf.predict_proba(X_Test)\nprint('For values of best alpha = ', alpha[best_alpha], \"The test log loss is:\",log_loss(Y_Test, predict_y, labels=clf.classes_, eps=1e-15))\npredicted_y =np.argmax(predict_y,axis=1)\nprint(\"Total number of data points :\", len(predicted_y))\nplot_confusion_matrix(Y_Test, predicted_y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### LINEAR SVM WITH HYPERPARAMETER TUNING ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"alpha = [10 ** x for x in range(-5, 2)] # hyperparam for SGD classifier.\nlog_error_array=[]\nfor i in alpha:\n    clf = SGDClassifier(alpha=i, penalty='l1', loss='hinge', random_state=42)\n    clf.fit(X_Train, Y_Train)\n    sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n    sig_clf.fit(X_Train, Y_Train)\n    predict_y = sig_clf.predict_proba(X_Test)\n    log_error_array.append(log_loss(Y_Test, predict_y, labels=clf.classes_, eps=1e-15))\n    print('For values of alpha = ', i, \"The log loss is:\",log_loss(Y_Test, predict_y, labels=clf.classes_, eps=1e-15))\n\nfig, ax = plt.subplots()\nax.plot(alpha, log_error_array,c='g')\nfor i, txt in enumerate(np.round(log_error_array,3)):\n    ax.annotate((alpha[i],np.round(txt,3)), (alpha[i],log_error_array[i]))\nplt.grid()\nplt.title(\"Cross Validation Error for each alpha\")\nplt.xlabel(\"Alpha i's\")\nplt.ylabel(\"Error measure\")\nplt.show()\n\n\nbest_alpha = np.argmin(log_error_array)\nclf = SGDClassifier(alpha=alpha[best_alpha], penalty='l1', loss='hinge', random_state=42)\nclf.fit(X_Train, Y_Train)\nsig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\nsig_clf.fit(X_Train, Y_Train)\n\npredict_y = sig_clf.predict_proba(X_Train)\nprint('For values of best alpha = ', alpha[best_alpha], \"The train log loss is:\",log_loss(Y_Train, predict_y, labels=clf.classes_, eps=1e-15))\npredict_y = sig_clf.predict_proba(X_Test)\nprint('For values of best alpha = ', alpha[best_alpha], \"The test log loss is:\",log_loss(Y_Test, predict_y, labels=clf.classes_, eps=1e-15))\npredicted_y =np.argmax(predict_y,axis=1)\nprint(\"Total number of data points :\", len(predicted_y))\nplot_confusion_matrix(Y_Test, predicted_y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### XGBoost","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import xgboost as xgb\nparams = {}\nparams['objective'] = 'binary:logistic'\nparams['eval_metric'] = 'logloss'\nparams['eta'] = 0.02\nparams['max_depth'] = 4\n\nd_train = xgb.DMatrix(X_Train, label=Y_Train)\nd_test = xgb.DMatrix(X_Test, label=Y_Test)\n\nwatchlist = [(d_train, 'train'), (d_test, 'valid')]\n\nbst = xgb.train(params, d_train, 400, watchlist, early_stopping_rounds=20, verbose_eval=10)\n\nxgdmat = xgb.DMatrix(X_Train,Y_Train)\npredict_y = bst.predict(d_test)\nprint(\"The test log loss is:\",log_loss(Y_Test, predict_y, labels=clf.classes_, eps=1e-15))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predicted_y =np.array(predict_y>0.5,dtype=int)\nprint(\"Total number of data points :\", len(predicted_y))\nplot_confusion_matrix(Y_Test, predicted_y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### TF-IDF VECTORIZATION ON QUORA QUESTION PAIR SIMILARITY","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"if os.path.isfile('nlp_features_train.csv'):\n    dfnlp = pd.read_csv(\"nlp_features_train.csv\",encoding='latin-1')\nelse:\n    print(\"download nlp_features_train.csv from drive or run previous notebook\")\n\nif os.path.isfile('feature_engg_preprocessing_train.csv'):\n    dfppro = pd.read_csv(\"feature_engg_preprocessing_train.csv\",encoding='latin-1')\nelse:\n    print(\"download ./feature_engg_preprocessing_train.csv from drive or run previous notebook\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_Final.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_final_100K = X_Final[0:100000]\nY_final_100K = Y_Final[0:100000]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train,X_test,Y_train,Y_test = train_test_split(X_final_100K,Y_final_100K,test_size=0.2,random_state=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_questions = X_train['questions']\nX_test_questions = X_test['questions']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = X_train.drop('questions',axis=1)\nX_test = X_test.drop('questions',axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\n\ntfidf_vector=TfidfVectorizer(ngram_range=(1,3),min_df=5)\n\nX_train_data_tfidf= tfidf_vector.fit_transform(X_train_questions)\nX_test_data_tfidf= tfidf_vector.transform(X_test_questions)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = hstack((X_train.values,X_train_data_tfidf))\nX_test= hstack((X_test.values,X_test_data_tfidf))\nprint(X_train.shape)\nprint(X_test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Logistic Regression to find Hyperparameter","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"alpha = [10 ** x for x in range(-5, 2)] # hyperparam for SGD classifier.\nlog_error_array=[]\nfor i in alpha:\n    clf = SGDClassifier(alpha=i, penalty='l2', loss='log', random_state=42)\n    clf.fit(X_train, Y_train)\n    sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n    sig_clf.fit(X_train, Y_train)\n    predict_y = sig_clf.predict_proba(X_test)\n    log_error_array.append(log_loss(Y_test, predict_y, labels=clf.classes_, eps=1e-15))\n    print('For values of alpha = ', i, \"The log loss is:\",log_loss(Y_test, predict_y, labels=clf.classes_, eps=1e-15))\n\nfig, ax = plt.subplots()\nax.plot(alpha, log_error_array,c='g')\nfor i, txt in enumerate(np.round(log_error_array,3)):\n    ax.annotate((alpha[i],np.round(txt,3)), (alpha[i],log_error_array[i]))\nplt.grid()\nplt.title(\"Cross Validation Error for each alpha\")\nplt.xlabel(\"Alpha i's\")\nplt.ylabel(\"Error measure\")\nplt.show()\n\n\nbest_alpha = np.argmin(log_error_array)\nclf = SGDClassifier(alpha=alpha[best_alpha], penalty='l2', loss='log', random_state=42)\nclf.fit(X_train, Y_train)\nsig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\nsig_clf.fit(X_train, Y_train)\n\npredict_y = sig_clf.predict_proba(X_train)\nprint('For values of best alpha = ', alpha[best_alpha], \"The train log loss is:\",log_loss(Y_train, predict_y, labels=clf.classes_, eps=1e-15))\npredict_y = sig_clf.predict_proba(X_test)\nprint('For values of best alpha = ', alpha[best_alpha], \"The test log loss is:\",log_loss(Y_test, predict_y, labels=clf.classes_, eps=1e-15))\npredicted_y =np.argmax(predict_y,axis=1)\nprint(\"Total number of data points :\", len(predicted_y))\nplot_confusion_matrix(Y_test, predicted_y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Linear SVM","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"alpha = [10 ** x for x in range(-5, 2)] # hyperparam for SGD classifier.\nlog_error_array=[]\nfor i in alpha:\n    clf = SGDClassifier(alpha=i, penalty='l1', loss='hinge', random_state=42)\n    clf.fit(X_train, Y_train)\n    sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n    sig_clf.fit(X_train, Y_train)\n    predict_y = sig_clf.predict_proba(X_test)\n    log_error_array.append(log_loss(Y_test, predict_y, labels=clf.classes_, eps=1e-15))\n    print('For values of alpha = ', i, \"The log loss is:\",log_loss(Y_test, predict_y, labels=clf.classes_, eps=1e-15))\n\nfig, ax = plt.subplots()\nax.plot(alpha, log_error_array,c='g')\nfor i, txt in enumerate(np.round(log_error_array,3)):\n    ax.annotate((alpha[i],np.round(txt,3)), (alpha[i],log_error_array[i]))\nplt.grid()\nplt.title(\"Cross Validation Error for each alpha\")\nplt.xlabel(\"Alpha i's\")\nplt.ylabel(\"Error measure\")\nplt.show()\n\n\nbest_alpha = np.argmin(log_error_array)\nclf = SGDClassifier(alpha=alpha[best_alpha], penalty='l1', loss='hinge', random_state=42)\nclf.fit(X_train, Y_train)\nsig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\nsig_clf.fit(X_train, Y_train)\n\npredict_y = sig_clf.predict_proba(X_train)\nprint('For values of best alpha = ', alpha[best_alpha], \"The train log loss is:\",log_loss(Y_train, predict_y, labels=clf.classes_, eps=1e-15))\npredict_y = sig_clf.predict_proba(X_test)\nprint('For values of best alpha = ', alpha[best_alpha], \"The test log loss is:\",log_loss(Y_test, predict_y, labels=clf.classes_, eps=1e-15))\npredicted_y =np.argmax(predict_y,axis=1)\nprint(\"Total number of data points :\", len(predicted_y))\nplot_confusion_matrix(Y_test, predicted_y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Hyperparameter tuning using RandomSearchCV","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import RandomizedSearchCV\nimport xgboost as xgb\n\nparam = {\"max_depth\":[1,5,10,50,100,500,1000],\"n_estimators\":[20,40,60,80,100]}\n\nxgb_classifier=xgb.XGBClassifier(n_jobs=-1,random_state=25)\n\nmodel = RandomizedSearchCV(xgb_classifier,param,n_iter=30,scoring='neg_log_loss',cv=3,n_jobs=-1)\n\nmodel.fit(X_train,Y_train)\nmodel.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import RandomizedSearchCV\nimport xgboost as xgb\n\nclf=xgb.XGBClassifier(n_jobs=-1,random_state=25,max_depth=10,n_estimators=100)\nclf.fit(X_train,Y_train)\ny_pred_test=clf.predict_proba(X_test)\ny_pred_train=clf.predict_proba(X_train)\nlog_loss_train = log_loss(Y_train, y_pred_train, eps=1e-15)\nlog_loss_test=log_loss(Y_test,y_pred_test,eps=1e-15)\nprint('Train log loss = ',log_loss_train,' Test log loss = ',log_loss_test)\npredicted_y=np.argmax(y_pred_test,axis=1)\nplot_confusion_matrix(Y_test,predicted_y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Pretty Table ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"pip install -U PTable","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# DRAW CONCLUSION","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from prettytable import PrettyTable\nx= PrettyTable()\n\nx.field_names = [\"VECTORIZER\",\"TYPE OF MODEL\",\"TRAIN LOG LOSS\",\"TEST LOG LOSS\"]\nx.add_row(['TF-IDF WEIGHTED W2V','LOGISTIC REGRESSION(ALPHA=0.001)','0.4314','0.9163'])\nx.add_row(['TF-IDF WEIGHTED W2V','LINEAR SVM(ALPHA = 0.1)','0.5209','0.5377'])\nx.add_row(['TF-IDF WEIGHTED W2V','XGBOOST','0.3545','0.3532'])\nx.add_row(['TF-IDF','LOGISTIC REGRESSION(ALPHA=0.0001)','0.4034 ','0.4014'])\nx.add_row(['TF-IDF','LINEAR SVM(ALPHA=0.00001)','0.4336','0.4329'])\nx.add_row(['TF-IDF','XGBOOST','0.2131 ','0.3163'])\n\nprint(x)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### LOOKING AT THE PRETTY TABLE , THE TF-IDF VECTORIZER USING XG-BOOST PERFORM WELL WITH LESS TRAIN LOSS = 0.2131 AND TEST LOSS = 0.3163","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}