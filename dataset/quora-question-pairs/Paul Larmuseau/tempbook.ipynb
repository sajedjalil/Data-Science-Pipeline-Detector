{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"23de6269-51ba-2d76-2ed4-bf56f0020ed8"},"outputs":[],"source":"import numpy as np\nimport pandas as pd\nfrom collections import Counter\nimport os\nimport gc\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\n# timing function\nimport time   \nstart = time.clock() #_________________ measure efficiency timing\n\ninput_folder='../input/'\ntrain = pd.read_csv(input_folder + 'train.csv',encoding='utf8')[:5000]\ntest  = pd.read_csv(input_folder + 'test.csv',encoding='utf8')[:5000]\n\n# lege opvullen\ntrain.fillna(value='leeg',inplace=True)\ntest.fillna(value='leeg',inplace=True)\n\nprint(\"Original data: trainQ: {}, testQ: {}\".format(train.shape, test.shape) )\nend = time.clock()\nprint('open:',end-start)\n\ndef cleantxt(x):   \n    # Pad punctuation with spaces on both sides\n    for char in ['.', '\"', ',', '(', ')', '!', '?', ';', ':' , '[' ,']' ,'{' ,'}' ,'^']:\n        x = x.replace(char, ' ' + char + ' ')\n    return x\ntrain['question1']=train['question1'].map(cleantxt)\ntrain['question2']=train['question2'].map(cleantxt)\ntest['question1']=test['question1'].map(cleantxt)\ntest['question2']=test['question2'].map(cleantxt)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c6a091ea-6e57-706a-7e72-461d961d68d2"},"outputs":[],"source":"from nltk.tokenize import TreebankWordTokenizer\nfrom math import*\nprint(train.head())\n\nimport re\nr = re.compile(\"[ ,.?|']\")\n\nfrom nltk.corpus import stopwords\nstops = set(stopwords.words('english'))\n\n\nnr_tr=len(train)\nnr_te=len(test)\n\ntrain_qs = pd.Series(train['question1'].tolist() + train['question2'].tolist())\ntest_qs = pd.Series(test['question1'].tolist() + test['question2'].tolist())\nall_qs = pd.DataFrame(train_qs.append(test_qs) )\nall_qs = all_qs.reset_index()\nall_qs.columns=['index','Q']\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\ntfidf = TfidfVectorizer( ngram_range=(1,1),use_idf=True, tokenizer=TreebankWordTokenizer().tokenize)\ntfidf2 = TfidfVectorizer( ngram_range=(2,2),use_idf=True, tokenizer=TreebankWordTokenizer().tokenize)\ntfidfall=tfidf.fit_transform(all_qs['Q'])\ntfidfall2=tfidf2.fit_transform(all_qs['Q'])#print(tfidf)#print(tfidf.stop_words_ )\nwordnr = pd.DataFrame(pd.Series(list(tfidf.vocabulary_.keys()), index=tfidf.vocabulary_.values()),columns=['woord']) # #print(words)  #woordenboek !\nnrword = pd.DataFrame.from_dict(tfidf.vocabulary_,orient='index')\nnrword.columns=['tfnr']#nrword['idf']=tfidf.idf_#print(nrword)\nend = time.clock()\nprint('__________tfidf:',end-start)\n\nprint('Test examples for first Q pair')\nQnr=0\nprint('cosinesim',  (tfidfall.getrow(Qnr)*tfidfall.getrow(Qnr+nr_tr).T).toarray()[0][0]     )\nprint('cosinesim2',tfidfall[Qnr:Qnr+1].todense()*tfidfall[nr_tr+Qnr:nr_tr+Qnr+1].todense().T)\n\ncoo = tfidfall[Qnr:Qnr+1].tocoo(copy=False)\nvector1=pd.DataFrame(data=coo.data,index=coo.col,columns=['tfidf']) #print(vector1.T)\ncoo2 = tfidfall[Qnr+nr_tr:Qnr+nr_tr+1].tocoo(copy=False)\nvector2=pd.DataFrame(data=coo2.data,index=coo2.col,columns=['tfidf']) #print(vector2.T)\ncosin= vector1*vector2\nvector1['tfidf2']=vector2['tfidf']\nvector1['woord']=wordnr['woord'] #print(vector1)\nvector1=vector1.fillna(0) #print(vector1.T)\neucl = (vector1['tfidf']-vector1['tfidf2'])**2\nprint('cosin',cosin.sum())\nprint('eucl',sqrt(eucl.sum()))\nvector1['woord']=wordnr['woord'] #print('vector',vector1.T)\nq1_word = train.ix[Qnr]['question1'].lower().split()\nq2_word = train.ix[Qnr]['question2'].lower().split()\nalien_word = [list(vector1[vector1['woord']==w]['tfidf'])[0] for w in q1_word if w not in q2_word]\nprint ('weighted sumdiff',sum(alien_word))\nprint(q1_word,q2_word,alien_word)\n\ndef word_match(row):\n    #print(row)\n    q1_word = {}\n    q2_word = {}\n    q1_word=TreebankWordTokenizer().tokenize(row['question1'].lower())\n    q2_word=TreebankWordTokenizer().tokenize(row['question2'].lower())        \n    q1_word = [w for w in q1_word if w not in stops]\n    q2_word = [w for w in q2_word if w not in stops]\n\n    if len(q1_word) == 0 or len(q2_word)==0:\n        return 0,0,0,0,0,0,0,0,0,0,0,0,0,0\n    shared_word_in_q1 = [w for w in q1_word if w in q2_word]\n    shared_word_in_q2 = [w for w in q2_word if w in q1_word]\n    alien_word_in_q1 = [w for w in q1_word if w not in q2_word]\n    alien_word_in_q2 = [w for w in q2_word if w not in q1_word] \n    stop_word_in_q1 = [w for w in q1_word if w in stops]\n    stop_word_in_q2 = [w for w in q2_word if w in stops] \n\n    nr_rij=row['id']\n    #T= (tfidfall[nr_rij:nr_rij+1]*tfidfall[nr_rij+nr_tr:nr_rij+nr_tr+1].T).toarray()[0][0]      \n    AD = len(shared_word_in_q2)/(len(shared_word_in_q2)+len(alien_word_in_q1)*2+len(alien_word_in_q2)*2)\n    D = (len(shared_word_in_q1)+len(shared_word_in_q2))/(len(q1_word)+len(q2_word))\n    K = len(shared_word_in_q1)/len(q1_word)*0.5+len(shared_word_in_q2)/len(q2_word)*0.5\n    O = len(shared_word_in_q2)/sqrt(len(q1_word)*len(q2_word))\n    S = (len(shared_word_in_q1)-len(stop_word_in_q1)+len(shared_word_in_q2)-len(stop_word_in_q2))/(len(q1_word)-len(stop_word_in_q1)+len(q2_word)-len(stop_word_in_q2))\n    A1 = (len(alien_word_in_q1)-len(stop_word_in_q1))/(len(q1_word)-len(stop_word_in_q1))    \n    A2 = (len(alien_word_in_q2)-len(stop_word_in_q2))/(len(q2_word)-len(stop_word_in_q2))\n    coo1 = tfidfall.getrow(nr_rij).tocoo(copy=False)\n    coo2 = tfidfall.getrow(nr_rij+nr_tr).tocoo(copy=False)\n    vector1=pd.DataFrame(data=coo1.data,index=coo1.col,columns=['tfidf']) \n    vector2=pd.DataFrame(data=coo2.data,index=coo2.col,columns=['tfidf']) \n    #print('v1',vector1.T)\n    cosin= vector1*vector2\n    vector1['tfidf2']=vector2['tfidf']\n    eucl = (vector1['tfidf']-vector1['tfidf2'])**2    \n    T=(cosin.sum())[0]\n    E=sqrt(eucl.sum())\n    vector1['woord']=wordnr['woord']\n    vector1=vector1.fillna(0)\n    vector2['woord']=wordnr['woord']\n    shared_word_weight1 = [list(vector1[vector1['woord']==w]['tfidf'])[0] for w in shared_word_in_q1]\n    alien_word_weight1 = [list(vector1[vector1['woord']==w]['tfidf'])[0] for w in alien_word_in_q1]\n    #print('aliens',alien_word_in_q2,'vector2',vector2.T)\n    alien_word_weight2 = [list(vector2[vector2['woord']==w]['tfidf'])[0] for w in alien_word_in_q2]\n    eucl_word_weight2 = [list( (vector2[vector2['woord']==w]['tfidf']-0)**2 ) [0] for w in alien_word_in_q2]\n    ADw = sum(shared_word_weight1)/(sum(shared_word_weight1)+sum(alien_word_weight1*2)+sum(alien_word_weight2*2))\n    ED=sqrt(sum(eucl_word_weight2))\n    WK = sum(shared_word_weight1)/sum(vector1['tfidf'])*0.5+sum(shared_word_weight1)/sum(vector2['tfidf'])*0.5\n    WO = sum(shared_word_weight1)/sqrt(sum(vector1['tfidf'])*sum(vector2['tfidf']))\n    #print(eucl_word_weight2)\n    return D,A1,A2,S,T,sum(alien_word_weight1),sum(alien_word_weight2),E,ED,AD,O,K,ADw,WK,WO\n\n\ntemp = train.apply(word_match,axis=1)\ntrain_wm = pd.DataFrame(temp.tolist(), columns=['match', 'alien1','alien2','matchS','cosin','difw1','difw2','Eucl','Eudiff','AntiDice','Ochia','Kulz','WeightADice','WeightKulz','WeightOchia'])\nend = time.clock()\nprint('wm:',len(train_wm)/end-start)\ntrain[['match','alien1','alien2','matchS','cosin','difw1','difw2','Eucl','Eudiff','AntiDice','Ochia','Kulz','WeightADice','WeightKulz','WeightOchia']]=train_wm\nend = time.clock()\nprint('____different similarities:',end-start)\ndef plotter(kolom):\n    plt.figure(figsize=(15,5))\n    plt.hist(train[train['is_duplicate'] == 0][kolom].fillna(0), bins=20, normed=False, label='Not Duplicate')\n    plt.hist(train[train['is_duplicate'] == 1][kolom].fillna(0), bins=20, normed=False, alpha=0.7, label='Duplicate')\n    plt.legend()\n    plt.title('Label distribution over', fontsize=15)\n    plt.xlabel(kolom, fontsize=15)\n\nplotter('match')    \nplotter('alien1')    \nplotter('alien2')    \nplotter('matchS')\nplotter('cosin')\nplotter('difw1')\nplotter('difw2')\nplotter('Eucl')\nplotter('Eudiff')\nplotter('AntiDice')\nplotter('Ochia')\nplotter('Kulz')\nplotter('WeightADice')\nplotter('WeightKulz')\nplotter('WeightOchia')\nend = time.clock()\nprint('clean and make freq word dict:',end-start)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"2706363a-8aa1-8f02-f29a-ff66c6e69c5a"},"outputs":[],"source":"print(train[train['Ochia']>0.98])\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import log_loss\nimport scipy\nimport xgboost as xgb\nimport difflib\n\ny=train['is_duplicate']     \nfeats = train.columns.values.tolist()\nfeats=[x for x in feats if x not in ['question1','question2','id','qid1','qid2','is_duplicate']]\nprint(\"features\",feats)\n\nx_train, x_valid, y_train, y_valid = train_test_split(train[feats], y, test_size=0.1, random_state=0)\n#XGBoost model\nparams = {\"objective\":\"binary:logistic\",'eval_metric':'logloss',\"max_depth\":7}\n\nd_train = xgb.DMatrix(x_train, label=y_train)\nd_valid = xgb.DMatrix(x_valid, label=y_valid)\nwatchlist = [(d_train, 'train'), (d_valid, 'valid')]\nbst = xgb.train(params, d_train, 2000, watchlist, early_stopping_rounds=200,verbose_eval=25) #change to higher #s\nprint('training done')\n\nend = time.clock()\nprint('____________trained:',end-start)\nprint(\"log loss for training data set\",log_loss(y, bst.predict(xgb.DMatrix(train[feats]))))\n#Predicting for test data set\nsub = pd.DataFrame() # Submission data frame\nsub['test_id'] = []\nsub['is_duplicate'] = []\nheader=['test_id','question1','question2','id','qid1','qid2','is_duplicate']"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"202c1e2f-6e7b-72cf-7ca8-564b2938c55e"},"outputs":[],"source":"\ndef word_match_test(row):\n    #print(row)\n    q1_word = {}\n    q2_word = {}\n    q1_word=TreebankWordTokenizer().tokenize(row['question1'].lower())\n    q2_word=TreebankWordTokenizer().tokenize(row['question2'].lower())        \n    q1_word = [w for w in q1_word if w not in stops]\n    q2_word = [w for w in q2_word if w not in stops]\n\n    if len(q1_word) == 0 or len(q2_word)==0:\n        return 0,0,0,0,0,0,0,0,0,0,0,0,0,0\n    shared_word_in_q1 = [w for w in q1_word if w in q2_word]\n    shared_word_in_q2 = [w for w in q2_word if w in q1_word]\n    alien_word_in_q1 = [w for w in q1_word if w not in q2_word]\n    alien_word_in_q2 = [w for w in q2_word if w not in q1_word] \n    stop_word_in_q1 = [w for w in q1_word if w in stops]\n    stop_word_in_q2 = [w for w in q2_word if w in stops] \n\n    nr_rij=row['test_id']+nr_tr*2\n    #T= (tfidfall[nr_rij:nr_rij+1]*tfidfall[nr_rij+nr_tr:nr_rij+nr_tr+1].T).toarray()[0][0]      \n    AD = len(shared_word_in_q2)/(len(shared_word_in_q2)+len(alien_word_in_q1)*2+len(alien_word_in_q2)*2)\n    D = (len(shared_word_in_q1)+len(shared_word_in_q2))/(len(q1_word)+len(q2_word))\n    K = len(shared_word_in_q1)/len(q1_word)*0.5+len(shared_word_in_q2)/len(q2_word)*0.5\n    O = len(shared_word_in_q2)/sqrt(len(q1_word)*len(q2_word))\n    S = (len(shared_word_in_q1)-len(stop_word_in_q1)+len(shared_word_in_q2)-len(stop_word_in_q2))/(len(q1_word)-len(stop_word_in_q1)+len(q2_word)-len(stop_word_in_q2))\n    A1 = (len(alien_word_in_q1)-len(stop_word_in_q1))/(len(q1_word)-len(stop_word_in_q1))    \n    A2 = (len(alien_word_in_q2)-len(stop_word_in_q2))/(len(q2_word)-len(stop_word_in_q2))\n    coo1 = tfidfall.getrow(nr_rij).tocoo(copy=False)\n    coo2 = tfidfall.getrow(nr_rij+nr_te).tocoo(copy=False)\n    vector1=pd.DataFrame(data=coo1.data,index=coo1.col,columns=['tfidf']) \n    vector2=pd.DataFrame(data=coo2.data,index=coo2.col,columns=['tfidf']) \n    #print('v1',vector1.T)\n    cosin= vector1*vector2\n    vector1['tfidf2']=vector2['tfidf']\n    eucl = (vector1['tfidf']-vector1['tfidf2'])**2    \n    T=(cosin.sum())[0]\n    E=sqrt(eucl.sum())\n    vector1['woord']=wordnr['woord']\n    vector1=vector1.fillna(0)\n    vector2['woord']=wordnr['woord']\n    shared_word_weight1 = [list(vector1[vector1['woord']==w]['tfidf'])[0] for w in shared_word_in_q1]\n    alien_word_weight1 = [list(vector1[vector1['woord']==w]['tfidf'])[0] for w in alien_word_in_q1]\n    #print('aliens',alien_word_in_q2,'vector2',vector2.T)\n    alien_word_weight2 = [list(vector2[vector2['woord']==w]['tfidf'])[0] for w in alien_word_in_q2]\n    eucl_word_weight2 = [list( (vector2[vector2['woord']==w]['tfidf']-0)**2 ) [0] for w in alien_word_in_q2]\n    ADw = sum(shared_word_weight1)/(sum(shared_word_weight1)+sum(alien_word_weight1*2)+sum(alien_word_weight2*2))\n    ED=sqrt(sum(eucl_word_weight2))\n    WK = sum(shared_word_weight1)/sum(vector1['tfidf'])*0.5+sum(shared_word_weight1)/sum(vector2['tfidf'])*0.5\n    WO = sum(shared_word_weight1)/sqrt(sum(vector1['tfidf'])*sum(vector2['tfidf']))\n    #print(eucl_word_weight2)\n    return D,A1,A2,S,T,sum(alien_word_weight1),sum(alien_word_weight2),E,ED,AD,O,K,ADw,WK,WO\n\n\ntemp = test.apply(word_match_test,axis=1)\ntrain_wm = pd.DataFrame(temp.tolist(), columns=['match', 'alien1','alien2','matchS','cosin','difw1','difw2','Eucl','Eudiff','AntiDice','Ochia','Kulz','WeightADice','WeightKulz','WeightOchia'])\nend = time.clock()\nprint('wm:',len(train_wm)/end-start)\ntest[['match','alien1','alien2','matchS','cosin','difw1','difw2','Eucl','Eudiff','AntiDice','Ochia','Kulz','WeightADice','WeightKulz','WeightOchia']]=train_wm\nend = time.clock()\n\ndef plottest(kolom):\n    plt.figure(figsize=(15,5))\n    plt.hist(test[kolom].fillna(0), bins=20, normed=False)\n    plt.legend()\n    plt.title('Label distribution over', fontsize=15)\n    plt.xlabel(kolom, fontsize=15)\n    \nplottest('match')    \nplottest('alien1')    \nplottest('alien2')    \nplottest('matchS')\nplottest('cosin')\nplottest('difw1')\nplottest('difw2')\nplottest('Eucl')\nplottest('Eudiff')\nplottest('AntiDice')\nplottest('Ochia')\nplottest('Kulz')\nplottest('WeightADice')\nplottest('WeightKulz')\nplottest('WeightOchia')\n\nsub=pd.DataFrame({'test_id':test['test_id'], 'is_duplicate':bst.predict(xgb.DMatrix(test[feats]))})\nprint(sub.head())\nsub.to_csv('quora_xgb.csv', index=False)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"36a40ed1-81ec-317b-a7f1-cefe1697f632"},"outputs":[],"source":""},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c2c7ef04-fc3b-1ee3-6f5e-8fc25ae0c398"},"outputs":[],"source":""}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":0}